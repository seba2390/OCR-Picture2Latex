{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def math_scraper(input_names: List[str], output_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads the input LaTeX file and extracts all equations, align, and $$ environments\n",
    "    into the output file, with each math environment on a separate line.\n",
    "\n",
    "    Args:\n",
    "        input_names (List[str]): A list of the names of the input LaTeX files.\n",
    "        output_name (str): The name of the output text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example Usage:\n",
    "        >>> math_scraper(['input1.tex', 'input2.tex'], 'output.txt')\n",
    "    \"\"\"\n",
    "    # Open the output file for writing\n",
    "    with open(output_name, 'w') as output_file:\n",
    "\n",
    "        # Define regular expressions for equation, align, and $$ environments\n",
    "        equation_re = re.compile(r'\\\\begin{equation\\*?}(.*?)\\\\end{equation\\*?}', re.DOTALL)\n",
    "        align_re = re.compile(r'\\\\begin{align\\*?}(.*?)\\\\end{align\\*?}', re.DOTALL)\n",
    "        dollar_re = re.compile(r'\\\\begin{equation\\*?}(.*?)\\\\end{equation\\*?}|'\n",
    "                               r'\\\\begin{align\\*?}(.*?)\\\\end{align\\*?}|'\n",
    "                               r'\\$\\$(.*?)\\$\\$', re.DOTALL)\n",
    "\n",
    "        # Find matches within each input file and write them to the output file\n",
    "        for input_name in input_names:\n",
    "            with open(input_name, 'r') as input_file:\n",
    "                input_contents = input_file.read()\n",
    "                for match in equation_re.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n\\n')\n",
    "\n",
    "                for match in align_re.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n\\n')\n",
    "\n",
    "                for match in dollar_re.findall(input_contents):\n",
    "                    output_file.write(match[1].strip() + '\\n\\n')\n",
    "\n",
    "        input_file.close()\n",
    "    output_file.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def file_scraper(directory_name: str, output_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads all files in the specified directory and writes their contents\n",
    "    to the specified output file, with each file's contents separated by a newline.\n",
    "\n",
    "    Args:\n",
    "        directory_name (str): The name of the directory containing the input files.\n",
    "        output_name (str): The name of the output text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example Usage:\n",
    "        >>> file_scraper('my_directory', 'output.txt')\n",
    "    \"\"\"\n",
    "    # Create a list of all files in the directory\n",
    "    file_list = [f for f in os.listdir(directory_name) if os.path.isfile(os.path.join(directory_name, f))]\n",
    "\n",
    "    # Check if the output file already exists in the parent directory\n",
    "    parent_directory = os.path.dirname(directory_name)\n",
    "    file_path = os.path.join(parent_directory, output_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Add a suffix to the output file name if it already exists\n",
    "        filename, extension = os.path.splitext(output_name)\n",
    "        i = 1\n",
    "        while os.path.isfile(os.path.join(parent_directory, f\"{filename}_{i}{extension}\")):\n",
    "            i += 1\n",
    "        output_name = f\"{filename}_{i}{extension}\"\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(os.path.join(parent_directory, output_name), 'w') as output_file:\n",
    "        # Write the contents of each file to the output file\n",
    "        for file_name in file_list:\n",
    "            with open(os.path.join(directory_name, file_name), 'r') as input_file:\n",
    "                output_file.write(input_file.read().strip() + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "def unpack_tex_files(download_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified directory that are not '.tex' files or compressed '.pdf' files containing '.tex' files.\n",
    "    Extracts all '.tex' files from compressed '.tar.gz' or '.tar' files to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        download_directory: The path of the directory containing the downloaded files.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the 'file' command fails to execute properly.\n",
    "\n",
    "    \"\"\"\n",
    "    file_names = os.listdir(path=download_directory)\n",
    "\n",
    "    for idx, filename in enumerate(file_names):\n",
    "        if filename[-4:] != \".tex\":\n",
    "            print(f\" ===== Handling file nr: {idx+1} with id: {filename} =====\")\n",
    "            full_filename = os.path.join(download_directory, filename)\n",
    "            file_type_info_list = subprocess.check_output([\"file\", full_filename]).decode().strip().split()\n",
    "            file_removed = False\n",
    "\n",
    "            # If file_type_info_list contains 'PDF', file is either a '.pdf' or a compressed '.pdf' file.\n",
    "            # We want only '.tex' files, so we delete it.\n",
    "            if \"PDF\" in file_type_info_list:\n",
    "                print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                file_removed = True\n",
    "                os.remove(full_filename)\n",
    "\n",
    "            # If file_type_info_list contains 'gzip' and 'compressed', file is a compressed '.tar.gz' file.\n",
    "            # We extract all '.tex' files to the specified directory and delete the original file.\n",
    "            elif \"gzip\" in file_type_info_list and \"compressed\" in file_type_info_list:\n",
    "                try:\n",
    "                    tar = tarfile.open(full_filename, 'r:gz')\n",
    "                    file_types = [os.path.splitext(os.path.basename(member.name))[1] for member in tar.getmembers()]\n",
    "                    tar.close()\n",
    "                    if \".tex\" not in file_types:\n",
    "                        print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                        file_removed = True\n",
    "                        os.remove(full_filename)\n",
    "                    else:\n",
    "                        tar = tarfile.open(full_filename, 'r:gz')\n",
    "                        tar.extractall(path=download_directory, members=[m for m in tar.getmembers() if os.path.splitext(m.name)[1] == \".tex\"])\n",
    "                        tar.close()\n",
    "                except tarfile.TarError as e:\n",
    "                    print(f\"{e} - file nr: {idx+1} with id: {filename} skipped an deleted...\")\n",
    "                    os.remove(full_filename)\n",
    "\n",
    "            # If file_type_info_list contains 'tar', 'POSIX' and 'archive', file is a '.tar' file.\n",
    "            # We extract all '.tex' files to the specified directory and delete the original file.\n",
    "            elif \"tar\" in file_type_info_list and \"POSIX\" in file_type_info_list and \"archive\" in file_type_info_list:\n",
    "                try:\n",
    "                    tar = tarfile.open(full_filename, 'r:')\n",
    "                    file_types = [os.path.splitext(os.path.basename(member.name))[1] for member in tar.getmembers()]\n",
    "                    tar.close()\n",
    "                    if \".tex\" not in file_types:\n",
    "                        print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                        file_removed = True\n",
    "                        os.remove(full_filename)\n",
    "                    else:\n",
    "                        tar = tarfile.open(full_filename, 'r:')\n",
    "                        tar.extractall(path=download_directory, members=[m for m in tar.getmembers() if os.path.splitext(m.name)[1] == \".tex\"])\n",
    "                        tar.close()\n",
    "                except tarfile.TarError as e:\n",
    "                    print(f\"{e} - file nr: {idx+1} with id: {filename} skipped an deleted...\")\n",
    "                    os.remove(full_filename)\n",
    "            # Always remove original after.\n",
    "            if not file_removed:\n",
    "                os.remove(full_filename)\n",
    "\n",
    "    # Move files from created sub-folders to parent folder\n",
    "    move_files_to_parent(download_directory=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def download_arxiv_papers(category: str, num_papers: int, download_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads STEM (Science, Technology, Engineering and Math) category arXiv papers in the specified category and saves them to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        category (str): The arXiv category to download papers from.\n",
    "        num_papers (int): The number of papers to download.\n",
    "        download_dir (str): The directory to save the downloaded papers to.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Physics, Mathematics, Computer Science, Quantitative biology, Electrical Engineering and Systems Science\n",
    "    # Quantitative finance, Economics, Statistics\n",
    "    STEM = ['physics','math', 'cs', 'eess', 'q-bio', 'q-fin', 'econ', 'stat']\n",
    "    if category not in STEM:\n",
    "        raise Exception(f'Category: {category} is not in the defined STEM categories: {STEM}')\n",
    "\n",
    "    # Create the download directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        # Notify the user that the download directory was created\n",
    "        print(f\"N.B.: Path '{download_dir}' didn't already exist, so it was created... \")\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Request the arXiv category page and parse it with BeautifulSoup\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "    # Format the date as YY/MM\n",
    "    current_formatted_date = now.strftime(\"%y%m\")\n",
    "\n",
    "    # Construct the URL of the arXiv category page for the current date and specified category\n",
    "    url = f\"https://arxiv.org/list/{category}/{current_formatted_date}\"\n",
    "    # Send an HTTP request to the URL and get the response\n",
    "    response_1 = requests.get(url)\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_1.content, 'html.parser')\n",
    "    # Construct a regular expression that matches links to the arXiv category page for the current date and specified category\n",
    "    regex = re.compile(re.escape(f'/list/{category}/{current_formatted_date}'))\n",
    "    # Find all links on the page that match the regular expression\n",
    "    links = soup.find_all('a', href=regex)\n",
    "    link = None\n",
    "    # Loop over the links and find the link with the text \">all</a>\"\n",
    "    for l in soup.find_all('a', href=regex):\n",
    "        if str(l)[-8:] == \">all</a>\":\n",
    "            link = l\n",
    "    # Send an HTTP request to the URL of the page that contains links to individual papers in the category\n",
    "    response_2 = requests.get(url+link['href'])\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_2.content, 'html.parser')\n",
    "\n",
    "    # Extract the links to the individual papers from the page\n",
    "    paper_links = soup.find_all('a', title='Abstract')\n",
    "\n",
    "    # Raise an exception if the number of papers requested is greater than the number of papers available\n",
    "    if num_papers > len(paper_links):\n",
    "        raise Exception(f\"{num_papers} papers requested, but only: {len(paper_links)} available in category: '{category}' in YY/MM: {current_formatted_date} ...\")\n",
    "    else:\n",
    "        # Notify the user of the number of papers being downloaded\n",
    "        print(f\"======= Downloading: {num_papers} (out of {len(paper_links)}) papers in category: '{category}' in YY/MM: {current_formatted_date} =======\")\n",
    "\n",
    "    # Loop over the paper links and download the source files for each paper\n",
    "    for i, link in enumerate(paper_links):\n",
    "        if i >= num_papers:\n",
    "            break\n",
    "\n",
    "        # Get the URL of the paper page\n",
    "        paper_url = \"https://arxiv.org\" + link['href']\n",
    "\n",
    "        # Extract the paper ID from the URL\n",
    "        paper_id = paper_url.split('/')[-1]\n",
    "\n",
    "        # Construct the URL of the source files\n",
    "        source_url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "\n",
    "        print(\"Downloading paper: \", i+1, \" with id: \", paper_id, \" ...\")\n",
    "        # Download the source files and save them to the download directory\n",
    "        response_3 = requests.get(source_url)\n",
    "\n",
    "        with open(download_dir+str(paper_id), \"wb\") as f:\n",
    "            f.write(response_3.content)\n",
    "        f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Downloading: 50 (out of 1360) papers in category: 'physics' in YY/MM: 2303 =======\n",
      "Downloading paper:  1  with id:  2303.00033  ...\n",
      "Downloading paper:  2  with id:  2303.00036  ...\n",
      "Downloading paper:  3  with id:  2303.00037  ...\n",
      "Downloading paper:  4  with id:  2303.00041  ...\n",
      "Downloading paper:  5  with id:  2303.00059  ...\n",
      "Downloading paper:  6  with id:  2303.00061  ...\n",
      "Downloading paper:  7  with id:  2303.00072  ...\n",
      "Downloading paper:  8  with id:  2303.00081  ...\n",
      "Downloading paper:  9  with id:  2303.00104  ...\n",
      "Downloading paper:  10  with id:  2303.00114  ...\n",
      "Downloading paper:  11  with id:  2303.00121  ...\n",
      "Downloading paper:  12  with id:  2303.00127  ...\n",
      "Downloading paper:  13  with id:  2303.00166  ...\n",
      "Downloading paper:  14  with id:  2303.00176  ...\n",
      "Downloading paper:  15  with id:  2303.00184  ...\n",
      "Downloading paper:  16  with id:  2303.00189  ...\n",
      "Downloading paper:  17  with id:  2303.00195  ...\n",
      "Downloading paper:  18  with id:  2303.00197  ...\n",
      "Downloading paper:  19  with id:  2303.00224  ...\n",
      "Downloading paper:  20  with id:  2303.00225  ...\n",
      "Downloading paper:  21  with id:  2303.00239  ...\n",
      "Downloading paper:  22  with id:  2303.00248  ...\n",
      "Downloading paper:  23  with id:  2303.00266  ...\n",
      "Downloading paper:  24  with id:  2303.00287  ...\n",
      "Downloading paper:  25  with id:  2303.00290  ...\n",
      "Downloading paper:  26  with id:  2303.00352  ...\n",
      "Downloading paper:  27  with id:  2303.00361  ...\n",
      "Downloading paper:  28  with id:  2303.00365  ...\n",
      "Downloading paper:  29  with id:  2303.00395  ...\n",
      "Downloading paper:  30  with id:  2303.00415  ...\n",
      "Downloading paper:  31  with id:  2303.00416  ...\n",
      "Downloading paper:  32  with id:  2303.00425  ...\n",
      "Downloading paper:  33  with id:  2303.00436  ...\n",
      "Downloading paper:  34  with id:  2303.00444  ...\n",
      "Downloading paper:  35  with id:  2303.00469  ...\n",
      "Downloading paper:  36  with id:  2303.00478  ...\n",
      "Downloading paper:  37  with id:  2303.00512  ...\n",
      "Downloading paper:  38  with id:  2303.00526  ...\n",
      "Downloading paper:  39  with id:  2303.00548  ...\n",
      "Downloading paper:  40  with id:  2303.00568  ...\n",
      "Downloading paper:  41  with id:  2303.00622  ...\n",
      "Downloading paper:  42  with id:  2303.00640  ...\n",
      "Downloading paper:  43  with id:  2303.00643  ...\n",
      "Downloading paper:  44  with id:  2303.00648  ...\n",
      "Downloading paper:  45  with id:  2303.00657  ...\n",
      "Downloading paper:  46  with id:  2303.00661  ...\n",
      "Downloading paper:  47  with id:  2303.00669  ...\n",
      "Downloading paper:  48  with id:  2303.00675  ...\n",
      "Downloading paper:  49  with id:  2303.00679  ...\n",
      "Downloading paper:  50  with id:  2303.00681  ...\n"
     ]
    }
   ],
   "source": [
    "nr_papers = 50\n",
    "STEM_category = 'physics'\n",
    "download_directory = \"arxiv_papers/\"\n",
    "download_arxiv_papers(STEM_category, nr_papers, download_dir=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== Handling file nr: 1 with id: 2303.00361 =====\n",
      " Deleting file nr: 1 with id: 2303.00361 - no '.tex' file found...\n",
      " ===== Handling file nr: 2 with id: 2303.00395 =====\n",
      " ===== Handling file nr: 3 with id: 2303.00197 =====\n",
      " ===== Handling file nr: 4 with id: 2303.00622 =====\n",
      " ===== Handling file nr: 5 with id: 2303.00248 =====\n",
      " Deleting file nr: 5 with id: 2303.00248 - no '.tex' file found...\n",
      " ===== Handling file nr: 6 with id: 2303.00224 =====\n",
      " ===== Handling file nr: 7 with id: 2303.00640 =====\n",
      " ===== Handling file nr: 8 with id: 2303.00444 =====\n",
      " ===== Handling file nr: 9 with id: 2303.00072 =====\n",
      " ===== Handling file nr: 10 with id: 2303.00416 =====\n",
      " Deleting file nr: 10 with id: 2303.00416 - no '.tex' file found...\n",
      " ===== Handling file nr: 11 with id: 2303.00081 =====\n",
      " ===== Handling file nr: 12 with id: 2303.00648 =====\n",
      " ===== Handling file nr: 13 with id: 2303.00679 =====\n",
      " ===== Handling file nr: 14 with id: 2303.00225 =====\n",
      " ===== Handling file nr: 15 with id: 2303.00469 =====\n",
      " ===== Handling file nr: 16 with id: 2303.00059 =====\n",
      " ===== Handling file nr: 17 with id: 2303.00061 =====\n",
      " Deleting file nr: 17 with id: 2303.00061 - no '.tex' file found...\n",
      " ===== Handling file nr: 18 with id: 2303.00033 =====\n",
      " ===== Handling file nr: 19 with id: 2303.00239 =====\n",
      " Deleting file nr: 19 with id: 2303.00239 - no '.tex' file found...\n",
      " ===== Handling file nr: 20 with id: 2303.00290 =====\n",
      " ===== Handling file nr: 21 with id: 2303.00184 =====\n",
      " ===== Handling file nr: 22 with id: 2303.00548 =====\n",
      " ===== Handling file nr: 23 with id: 2303.00114 =====\n",
      " Deleting file nr: 23 with id: 2303.00114 - no '.tex' file found...\n",
      " ===== Handling file nr: 24 with id: 2303.00176 =====\n",
      " ===== Handling file nr: 25 with id: 2303.00512 =====\n",
      " Deleting file nr: 25 with id: 2303.00512 - no '.tex' file found...\n",
      " ===== Handling file nr: 26 with id: 2303.00104 =====\n",
      " Deleting file nr: 26 with id: 2303.00104 - no '.tex' file found...\n",
      " ===== Handling file nr: 27 with id: 2303.00195 =====\n",
      " ===== Handling file nr: 28 with id: 2303.00166 =====\n",
      " Deleting file nr: 28 with id: 2303.00166 - no '.tex' file found...\n",
      " ===== Handling file nr: 29 with id: 2303.00365 =====\n",
      " ===== Handling file nr: 30 with id: 2303.00568 =====\n",
      " ===== Handling file nr: 31 with id: 2303.00352 =====\n",
      " ===== Handling file nr: 32 with id: 2303.00643 =====\n",
      " ===== Handling file nr: 33 with id: 2303.00478 =====\n",
      " ===== Handling file nr: 34 with id: 2303.00675 =====\n",
      " ===== Handling file nr: 35 with id: 2303.00681 =====\n",
      " ===== Handling file nr: 36 with id: 2303.00041 =====\n",
      " Deleting file nr: 36 with id: 2303.00041 - no '.tex' file found...\n",
      " ===== Handling file nr: 37 with id: 2303.00425 =====\n",
      " ===== Handling file nr: 38 with id: 2303.00287 =====\n",
      " ===== Handling file nr: 39 with id: 2303.00415 =====\n",
      " ===== Handling file nr: 40 with id: 2303.00266 =====\n",
      " Deleting file nr: 40 with id: 2303.00266 - no '.tex' file found...\n",
      " ===== Handling file nr: 41 with id: 2303.00036 =====\n",
      " ===== Handling file nr: 42 with id: 2303.00669 =====\n",
      " ===== Handling file nr: 43 with id: 2303.00436 =====\n",
      " ===== Handling file nr: 44 with id: 2303.00657 =====\n",
      " ===== Handling file nr: 45 with id: 2303.00661 =====\n",
      " Deleting file nr: 45 with id: 2303.00661 - no '.tex' file found...\n",
      " ===== Handling file nr: 46 with id: 2303.00037 =====\n",
      " ===== Handling file nr: 47 with id: 2303.00189 =====\n",
      " ===== Handling file nr: 48 with id: 2303.00526 =====\n",
      " ===== Handling file nr: 49 with id: 2303.00127 =====\n",
      " ===== Handling file nr: 50 with id: 2303.00121 =====\n",
      "Destination path 'arxiv_papers/main_ver3.tex' already exists - file deleted and skipped..\n",
      "Destination path 'arxiv_papers/articulo.tex' already exists - file deleted and skipped..\n"
     ]
    }
   ],
   "source": [
    "unpack_tex_files(download_directory=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "file_names = [download_directory+file_name for file_name in os.listdir(download_directory) if file_name != \".DS_Store\"]\n",
    "output_file_name = \"equations.txt\"\n",
    "math_scraper(input_names=file_names,output_name=output_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}