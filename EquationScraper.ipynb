{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def math_scraper(input_names: List[str], output_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads the input LaTeX file and extracts all equations, align, and $$ environments\n",
    "    into the output file, with each math environment on a separate line.\n",
    "\n",
    "    Args:\n",
    "        input_names (List[str]): A list of the names of the input LaTeX files.\n",
    "        output_name (str): The name of the output text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example Usage:\n",
    "        >>> math_scraper(['input1.tex', 'input2.tex'], 'output.txt')\n",
    "    \"\"\"\n",
    "    # Open the output file for writing\n",
    "    with open(output_name, 'w') as output_file:\n",
    "\n",
    "        # Define regular expressions for equation, align, and $$ environments\n",
    "        equation_regex = re.compile(r'\\\\begin\\{equation\\}(.*?)\\\\end\\{equation\\}', re.DOTALL)\n",
    "        equation_star_regex = re.compile(r'\\\\begin\\{equation\\*\\}(.*?)\\\\end\\{equation\\*\\}', re.DOTALL)\n",
    "        align_regex = re.compile(r'\\\\begin\\{align\\}(.*?)\\\\end\\{align\\}', re.DOTALL)\n",
    "        align_star_regex = re.compile(r'\\\\begin\\{align\\*\\}(.*?)\\\\end\\{align\\*\\}', re.DOTALL)\n",
    "\n",
    "        # Find matches within each input file and write them to the output file\n",
    "        for idx, input_name in enumerate(input_names):\n",
    "            print(f\"Extracting latex equations from file nr: {idx+1}, named: {input_name}...\")\n",
    "            with open(input_name, 'r') as input_file:\n",
    "                input_contents = input_file.read()\n",
    "                for match in equation_regex.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n')\n",
    "                for match in equation_star_regex.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n')\n",
    "                for match in align_regex.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n')\n",
    "                for match in align_star_regex.findall(input_contents):\n",
    "                    output_file.write(match.strip() + '\\n')\n",
    "\n",
    "\n",
    "            input_file.close()\n",
    "    output_file.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def move_files_to_parent(download_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Move all files in subdirectories of `download_directory` to the parent directory.\n",
    "\n",
    "    Args:\n",
    "        download_directory: The path to the directory containing the files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the `download_directory` does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(download_directory):\n",
    "        raise FileNotFoundError(f\"{download_directory} does not exist.\")\n",
    "\n",
    "    for item in os.listdir(download_directory):\n",
    "        item_path = os.path.join(download_directory, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            for file_name in os.listdir(item_path):\n",
    "                file_path = os.path.join(item_path, file_name)\n",
    "                new_file_path = os.path.join(download_directory, file_name)\n",
    "                i = 1\n",
    "                while os.path.exists(new_file_path):\n",
    "                    file_name_split = os.path.splitext(file_name)\n",
    "                    new_file_name = f\"{file_name_split[0]}_{i}{file_name_split[1]}\"\n",
    "                    new_file_path = os.path.join(download_directory, new_file_name)\n",
    "                    i += 1\n",
    "                try:\n",
    "                    shutil.move(file_path, new_file_path)\n",
    "                except shutil.Error as e:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"{e} - file deleted and skipped..\")\n",
    "            os.rmdir(item_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "def unpack_tex_files(download_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified directory that are not '.tex' files or compressed '.pdf' files containing '.tex' files.\n",
    "    Extracts all '.tex' files from compressed '.tar.gz' or '.tar' files to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        download_directory: The path of the directory containing the downloaded files.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the 'file' command fails to execute properly.\n",
    "\n",
    "    \"\"\"\n",
    "    file_names = os.listdir(path=download_directory)\n",
    "\n",
    "    for idx, filename in enumerate(file_names):\n",
    "        if filename[-4:] != \".tex\":\n",
    "            print(f\" ===== Handling file nr: {idx+1} with id: {filename} =====\")\n",
    "            full_filename = os.path.join(download_directory, filename)\n",
    "            file_type_info_list = subprocess.check_output([\"file\", full_filename]).decode().strip().split()\n",
    "            file_removed = False\n",
    "\n",
    "            # If file_type_info_list contains 'PDF', file is either a '.pdf' or a compressed '.pdf' file.\n",
    "            # We want only '.tex' files, so we delete it.\n",
    "            if \"PDF\" in file_type_info_list:\n",
    "                print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                file_removed = True\n",
    "                os.remove(full_filename)\n",
    "\n",
    "            # If file_type_info_list contains 'gzip' and 'compressed', file is a compressed '.tar.gz' file.\n",
    "            # We extract all '.tex' files to the specified directory and delete the original file.\n",
    "            elif \"gzip\" in file_type_info_list and \"compressed\" in file_type_info_list:\n",
    "                try:\n",
    "                    tar = tarfile.open(full_filename, 'r:gz')\n",
    "                    file_types = [os.path.splitext(os.path.basename(member.name))[1] for member in tar.getmembers()]\n",
    "                    tar.close()\n",
    "                    if \".tex\" not in file_types:\n",
    "                        print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                        file_removed = True\n",
    "                        os.remove(full_filename)\n",
    "                    else:\n",
    "                        tar = tarfile.open(full_filename, 'r:gz')\n",
    "                        tar.extractall(path=download_directory, members=[m for m in tar.getmembers() if os.path.splitext(m.name)[1] == \".tex\"])\n",
    "                        tar.close()\n",
    "                except tarfile.TarError as e:\n",
    "                    print(f\"{e} - file nr: {idx+1} with id: {filename} skipped an deleted...\")\n",
    "                    os.remove(full_filename)\n",
    "\n",
    "            # If file_type_info_list contains 'tar', 'POSIX' and 'archive', file is a '.tar' file.\n",
    "            # We extract all '.tex' files to the specified directory and delete the original file.\n",
    "            elif \"tar\" in file_type_info_list and \"POSIX\" in file_type_info_list and \"archive\" in file_type_info_list:\n",
    "                try:\n",
    "                    tar = tarfile.open(full_filename, 'r:')\n",
    "                    file_types = [os.path.splitext(os.path.basename(member.name))[1] for member in tar.getmembers()]\n",
    "                    tar.close()\n",
    "                    if \".tex\" not in file_types:\n",
    "                        print(f\" Deleting file nr: {idx+1} with id: {filename} - no '.tex' file found...\")\n",
    "                        file_removed = True\n",
    "                        os.remove(full_filename)\n",
    "                    else:\n",
    "                        tar = tarfile.open(full_filename, 'r:')\n",
    "                        tar.extractall(path=download_directory, members=[m for m in tar.getmembers() if os.path.splitext(m.name)[1] == \".tex\"])\n",
    "                        tar.close()\n",
    "                except tarfile.TarError as e:\n",
    "                    print(f\"{e} - file nr: {idx+1} with id: {filename} skipped an deleted...\")\n",
    "                    os.remove(full_filename)\n",
    "            # Always remove original after.\n",
    "            if not file_removed:\n",
    "                os.remove(full_filename)\n",
    "\n",
    "    # Move files from created sub-folders to parent folder\n",
    "    move_files_to_parent(download_directory=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def download_arxiv_papers(category: str, num_papers: int, download_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads STEM (Science, Technology, Engineering and Math) category arXiv papers in the specified category and saves them to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        category (str): The arXiv category to download papers from.\n",
    "        num_papers (int): The number of papers to download.\n",
    "        download_dir (str): The directory to save the downloaded papers to.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Physics, Mathematics, Computer Science, Quantitative biology, Electrical Engineering and Systems Science\n",
    "    # Quantitative finance, Economics, Statistics\n",
    "    STEM = ['physics','math', 'cs', 'eess', 'q-bio', 'q-fin', 'econ', 'stat']\n",
    "    if category not in STEM:\n",
    "        raise Exception(f'Category: {category} is not in the defined STEM categories: {STEM}')\n",
    "\n",
    "    # Create the download directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        # Notify the user that the download directory was created\n",
    "        print(f\"N.B.: Path '{download_dir}' didn't already exist, so it was created... \")\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Request the arXiv category page and parse it with BeautifulSoup\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "    # Format the date as YY/MM\n",
    "    current_formatted_date = now.strftime(\"%y%m\")\n",
    "\n",
    "    # Construct the URL of the arXiv category page for the current date and specified category\n",
    "    url = f\"https://arxiv.org/list/{category}/{current_formatted_date}\"\n",
    "    # Send an HTTP request to the URL and get the response\n",
    "    response_1 = requests.get(url)\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_1.content, 'html.parser')\n",
    "    # Construct a regular expression that matches links to the arXiv category page for the current date and specified category\n",
    "    regex = re.compile(re.escape(f'/list/{category}/{current_formatted_date}'))\n",
    "    # Find all links on the page that match the regular expression\n",
    "    links = soup.find_all('a', href=regex)\n",
    "    link = None\n",
    "    # Loop over the links and find the link with the text \">all</a>\"\n",
    "    for l in soup.find_all('a', href=regex):\n",
    "        if str(l)[-8:] == \">all</a>\":\n",
    "            link = l\n",
    "    # Send an HTTP request to the URL of the page that contains links to individual papers in the category\n",
    "    response_2 = requests.get(url+link['href'])\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_2.content, 'html.parser')\n",
    "\n",
    "    # Extract the links to the individual papers from the page\n",
    "    paper_links = soup.find_all('a', title='Abstract')\n",
    "\n",
    "    # Raise an exception if the number of papers requested is greater than the number of papers available\n",
    "    if num_papers > len(paper_links):\n",
    "        raise Exception(f\"{num_papers} papers requested, but only: {len(paper_links)} available in category: '{category}' in YY/MM: {current_formatted_date} ...\")\n",
    "    else:\n",
    "        # Notify the user of the number of papers being downloaded\n",
    "        print(f\"======= Downloading: {num_papers} (out of {len(paper_links)}) papers in category: '{category}' in YY/MM: {current_formatted_date} =======\")\n",
    "\n",
    "    # Loop over the paper links and download the source files for each paper\n",
    "    for i, link in enumerate(paper_links):\n",
    "        if i >= num_papers:\n",
    "            break\n",
    "\n",
    "        # Get the URL of the paper page\n",
    "        paper_url = \"https://arxiv.org\" + link['href']\n",
    "\n",
    "        # Extract the paper ID from the URL\n",
    "        paper_id = paper_url.split('/')[-1]\n",
    "\n",
    "        # Construct the URL of the source files\n",
    "        source_url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "\n",
    "        print(\"Downloading paper: \", i+1, \" with id: \", paper_id, \" ...\")\n",
    "        # Download the source files and save them to the download directory\n",
    "        response_3 = requests.get(source_url)\n",
    "\n",
    "        with open(download_dir+str(paper_id), \"wb\") as f:\n",
    "            f.write(response_3.content)\n",
    "        f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Downloading: 50 (out of 1360) papers in category: 'physics' in YY/MM: 2303 =======\n",
      "Downloading paper:  1  with id:  2303.00033  ...\n",
      "Downloading paper:  2  with id:  2303.00036  ...\n",
      "Downloading paper:  3  with id:  2303.00037  ...\n",
      "Downloading paper:  4  with id:  2303.00041  ...\n",
      "Downloading paper:  5  with id:  2303.00059  ...\n",
      "Downloading paper:  6  with id:  2303.00061  ...\n",
      "Downloading paper:  7  with id:  2303.00072  ...\n",
      "Downloading paper:  8  with id:  2303.00081  ...\n",
      "Downloading paper:  9  with id:  2303.00104  ...\n",
      "Downloading paper:  10  with id:  2303.00114  ...\n",
      "Downloading paper:  11  with id:  2303.00121  ...\n",
      "Downloading paper:  12  with id:  2303.00127  ...\n",
      "Downloading paper:  13  with id:  2303.00166  ...\n",
      "Downloading paper:  14  with id:  2303.00176  ...\n",
      "Downloading paper:  15  with id:  2303.00184  ...\n",
      "Downloading paper:  16  with id:  2303.00189  ...\n",
      "Downloading paper:  17  with id:  2303.00195  ...\n",
      "Downloading paper:  18  with id:  2303.00197  ...\n",
      "Downloading paper:  19  with id:  2303.00224  ...\n",
      "Downloading paper:  20  with id:  2303.00225  ...\n",
      "Downloading paper:  21  with id:  2303.00239  ...\n",
      "Downloading paper:  22  with id:  2303.00248  ...\n",
      "Downloading paper:  23  with id:  2303.00266  ...\n",
      "Downloading paper:  24  with id:  2303.00287  ...\n",
      "Downloading paper:  25  with id:  2303.00290  ...\n",
      "Downloading paper:  26  with id:  2303.00352  ...\n",
      "Downloading paper:  27  with id:  2303.00361  ...\n",
      "Downloading paper:  28  with id:  2303.00365  ...\n",
      "Downloading paper:  29  with id:  2303.00395  ...\n",
      "Downloading paper:  30  with id:  2303.00415  ...\n",
      "Downloading paper:  31  with id:  2303.00416  ...\n",
      "Downloading paper:  32  with id:  2303.00425  ...\n",
      "Downloading paper:  33  with id:  2303.00436  ...\n",
      "Downloading paper:  34  with id:  2303.00444  ...\n",
      "Downloading paper:  35  with id:  2303.00469  ...\n",
      "Downloading paper:  36  with id:  2303.00478  ...\n",
      "Downloading paper:  37  with id:  2303.00512  ...\n",
      "Downloading paper:  38  with id:  2303.00526  ...\n",
      "Downloading paper:  39  with id:  2303.00548  ...\n",
      "Downloading paper:  40  with id:  2303.00568  ...\n",
      "Downloading paper:  41  with id:  2303.00622  ...\n",
      "Downloading paper:  42  with id:  2303.00640  ...\n",
      "Downloading paper:  43  with id:  2303.00643  ...\n",
      "Downloading paper:  44  with id:  2303.00648  ...\n",
      "Downloading paper:  45  with id:  2303.00657  ...\n",
      "Downloading paper:  46  with id:  2303.00661  ...\n",
      "Downloading paper:  47  with id:  2303.00669  ...\n",
      "Downloading paper:  48  with id:  2303.00675  ...\n",
      "Downloading paper:  49  with id:  2303.00679  ...\n",
      "Downloading paper:  50  with id:  2303.00681  ...\n"
     ]
    }
   ],
   "source": [
    "nr_papers = 50\n",
    "STEM_category = 'physics'\n",
    "download_directory = \"arxiv_papers/\"\n",
    "download_arxiv_papers(STEM_category, nr_papers, download_dir=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== Handling file nr: 1 with id: 2303.00361 =====\n",
      " Deleting file nr: 1 with id: 2303.00361 - no '.tex' file found...\n",
      " ===== Handling file nr: 2 with id: 2303.00395 =====\n",
      " ===== Handling file nr: 3 with id: 2303.00197 =====\n",
      " ===== Handling file nr: 4 with id: 2303.00622 =====\n",
      " ===== Handling file nr: 5 with id: 2303.00248 =====\n",
      " Deleting file nr: 5 with id: 2303.00248 - no '.tex' file found...\n",
      " ===== Handling file nr: 6 with id: .DS_Store =====\n",
      " ===== Handling file nr: 7 with id: 2303.00224 =====\n",
      " ===== Handling file nr: 8 with id: 2303.00640 =====\n",
      " ===== Handling file nr: 9 with id: 2303.00444 =====\n",
      " ===== Handling file nr: 10 with id: 2303.00072 =====\n",
      " ===== Handling file nr: 11 with id: 2303.00416 =====\n",
      " Deleting file nr: 11 with id: 2303.00416 - no '.tex' file found...\n",
      " ===== Handling file nr: 12 with id: 2303.00081 =====\n",
      " ===== Handling file nr: 13 with id: 2303.00648 =====\n",
      " ===== Handling file nr: 14 with id: 2303.00679 =====\n",
      " ===== Handling file nr: 15 with id: 2303.00225 =====\n",
      " ===== Handling file nr: 16 with id: 2303.00469 =====\n",
      " ===== Handling file nr: 17 with id: 2303.00059 =====\n",
      " ===== Handling file nr: 18 with id: 2303.00061 =====\n",
      " Deleting file nr: 18 with id: 2303.00061 - no '.tex' file found...\n",
      " ===== Handling file nr: 19 with id: 2303.00033 =====\n",
      " ===== Handling file nr: 20 with id: 2303.00239 =====\n",
      " Deleting file nr: 20 with id: 2303.00239 - no '.tex' file found...\n",
      " ===== Handling file nr: 22 with id: 2303.00290 =====\n",
      " ===== Handling file nr: 23 with id: 2303.00184 =====\n",
      " ===== Handling file nr: 24 with id: 2303.00548 =====\n",
      " ===== Handling file nr: 25 with id: 2303.00114 =====\n",
      " Deleting file nr: 25 with id: 2303.00114 - no '.tex' file found...\n",
      " ===== Handling file nr: 26 with id: 2303.00176 =====\n",
      " ===== Handling file nr: 27 with id: 2303.00512 =====\n",
      " Deleting file nr: 27 with id: 2303.00512 - no '.tex' file found...\n",
      " ===== Handling file nr: 28 with id: 2303.00104 =====\n",
      " Deleting file nr: 28 with id: 2303.00104 - no '.tex' file found...\n",
      " ===== Handling file nr: 29 with id: 2303.00195 =====\n",
      " ===== Handling file nr: 30 with id: 2303.00166 =====\n",
      " Deleting file nr: 30 with id: 2303.00166 - no '.tex' file found...\n",
      " ===== Handling file nr: 31 with id: 2303.00365 =====\n",
      " ===== Handling file nr: 32 with id: 2303.00568 =====\n",
      " ===== Handling file nr: 33 with id: 2303.00352 =====\n",
      " ===== Handling file nr: 34 with id: 2303.00643 =====\n",
      " ===== Handling file nr: 35 with id: 2303.00478 =====\n",
      " ===== Handling file nr: 36 with id: 2303.00675 =====\n",
      " ===== Handling file nr: 37 with id: 2303.00681 =====\n",
      " ===== Handling file nr: 38 with id: 2303.00041 =====\n",
      " Deleting file nr: 38 with id: 2303.00041 - no '.tex' file found...\n",
      " ===== Handling file nr: 39 with id: 2303.00425 =====\n",
      " ===== Handling file nr: 40 with id: 2303.00287 =====\n",
      " ===== Handling file nr: 41 with id: 2303.00415 =====\n",
      " ===== Handling file nr: 42 with id: 2303.00266 =====\n",
      " Deleting file nr: 42 with id: 2303.00266 - no '.tex' file found...\n",
      " ===== Handling file nr: 43 with id: 2303.00036 =====\n",
      " ===== Handling file nr: 44 with id: 2303.00669 =====\n",
      " ===== Handling file nr: 45 with id: 2303.00436 =====\n",
      " ===== Handling file nr: 46 with id: 2303.00657 =====\n",
      " ===== Handling file nr: 47 with id: 2303.00661 =====\n",
      " Deleting file nr: 47 with id: 2303.00661 - no '.tex' file found...\n",
      " ===== Handling file nr: 48 with id: 2303.00037 =====\n",
      " ===== Handling file nr: 49 with id: 2303.00189 =====\n",
      " ===== Handling file nr: 50 with id: 2303.00526 =====\n",
      " ===== Handling file nr: 51 with id: 2303.00127 =====\n",
      " ===== Handling file nr: 52 with id: 2303.00121 =====\n"
     ]
    }
   ],
   "source": [
    "unpack_tex_files(download_directory=download_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting latex equations from file nr: 1, named: arxiv_papers/monitoring.tex...\n",
      "Extracting latex equations from file nr: 2, named: arxiv_papers/Instrumentalpaper.tex...\n",
      "Extracting latex equations from file nr: 3, named: arxiv_papers/jfm-instructions.tex...\n",
      "Extracting latex equations from file nr: 4, named: arxiv_papers/PoP_LRRC.tex...\n",
      "Extracting latex equations from file nr: 5, named: arxiv_papers/articulo_1.tex...\n",
      "Extracting latex equations from file nr: 6, named: arxiv_papers/main_ver3_1.tex...\n",
      "Extracting latex equations from file nr: 7, named: arxiv_papers/main_for_arxiv.tex...\n",
      "Extracting latex equations from file nr: 8, named: arxiv_papers/laser.tex...\n",
      "Extracting latex equations from file nr: 9, named: arxiv_papers/elasticity_and_units.tex...\n",
      "Extracting latex equations from file nr: 10, named: arxiv_papers/main.tex...\n",
      "Extracting latex equations from file nr: 11, named: arxiv_papers/tilecal.tex...\n",
      "Extracting latex equations from file nr: 12, named: arxiv_papers/final.tex...\n",
      "Extracting latex equations from file nr: 13, named: arxiv_papers/paper.tex...\n",
      "Extracting latex equations from file nr: 14, named: arxiv_papers/Article.tex...\n",
      "Extracting latex equations from file nr: 15, named: arxiv_papers/neurips_2022.tex...\n",
      "Extracting latex equations from file nr: 16, named: arxiv_papers/introduction.tex...\n",
      "Extracting latex equations from file nr: 17, named: arxiv_papers/AdjointMethod.tex...\n",
      "Extracting latex equations from file nr: 18, named: arxiv_papers/calibration.tex...\n",
      "Extracting latex equations from file nr: 19, named: arxiv_papers/DRAFT.tex...\n",
      "Extracting latex equations from file nr: 20, named: arxiv_papers/Optica-template.tex...\n",
      "Extracting latex equations from file nr: 21, named: arxiv_papers/main-arxiv.tex...\n",
      "Extracting latex equations from file nr: 22, named: arxiv_papers/NodesMetricsCSF.tex...\n",
      "Extracting latex equations from file nr: 23, named: arxiv_papers/Piezoelectric_cement_clean.tex...\n",
      "Extracting latex equations from file nr: 24, named: arxiv_papers/articulo.tex...\n",
      "Extracting latex equations from file nr: 25, named: arxiv_papers/underprediction.tex...\n",
      "Extracting latex equations from file nr: 26, named: arxiv_papers/experimentaldynamicsballchainarxiv.tex...\n",
      "Extracting latex equations from file nr: 27, named: arxiv_papers/CarbonSPcomponents.tex...\n",
      "Extracting latex equations from file nr: 28, named: arxiv_papers/Quantum_Phase_Diagram_of_PT-Symmetry_or_Broken_in_a_Non-Hermitian_Photonic_Structure.tex...\n",
      "Extracting latex equations from file nr: 29, named: arxiv_papers/laseringap.tex...\n",
      "Extracting latex equations from file nr: 30, named: arxiv_papers/isingKnowledgeForArXivBBL.tex...\n",
      "Extracting latex equations from file nr: 31, named: arxiv_papers/main_ver3.tex...\n",
      "Extracting latex equations from file nr: 32, named: arxiv_papers/JPCA_ver1.tex...\n",
      "Extracting latex equations from file nr: 33, named: arxiv_papers/Electro_Acoustic_Noise.tex...\n",
      "Extracting latex equations from file nr: 34, named: arxiv_papers/references.tex...\n"
     ]
    }
   ],
   "source": [
    "file_names = [download_directory+file_name for file_name in os.listdir(download_directory) if file_name != \".DS_Store\"]\n",
    "output_file_name = \"equations.txt\"\n",
    "math_scraper(input_names=file_names,output_name=output_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def download_paper(download_dir: str, paper_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a single arXiv paper with the given ID and saves it to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        download_dir (str): The directory to save the downloaded paper to.\n",
    "        paper_id (str): The ID of the paper to download.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Construct the URL of the source files\n",
    "    source_url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "\n",
    "    # Download the source files and save them to the download directory\n",
    "    response = requests.get(source_url)\n",
    "\n",
    "    with open(download_dir + str(paper_id), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def download_arxiv_papers(category: str, num_papers: int, download_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads STEM (Science, Technology, Engineering and Math) category arXiv papers in the specified category and saves them to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        category (str): The arXiv category to download papers from.\n",
    "        num_papers (int): The number of papers to download.\n",
    "        download_dir (str): The directory to save the downloaded papers to.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Physics, Mathematics, Computer Science, Quantitative biology, Electrical Engineering and Systems Science\n",
    "    # Quantitative finance, Economics, Statistics\n",
    "    STEM = ['physics','math', 'cs', 'eess', 'q-bio', 'q-fin', 'econ', 'stat']\n",
    "    if category not in STEM:\n",
    "        raise Exception(f'Category: {category} is not in the defined STEM categories: {STEM}')\n",
    "\n",
    "    # Create the download directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        # Notify the user that the download directory was created\n",
    "        print(f\"N.B.: Path '{download_dir}' didn't already exist, so it was created... \")\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Request the arXiv category page and parse it with BeautifulSoup\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "    # Format the date as YY/MM\n",
    "    current_formatted_date = now.strftime(\"%y%m\")\n",
    "\n",
    "    # Construct the URL of the arXiv category page for the current date and specified category\n",
    "    url = f\"https://arxiv.org/list/{category}/{current_formatted_date}\"\n",
    "    # Send an HTTP request to the URL and get the response\n",
    "    response_1 = requests.get(url)\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_1.content, 'html.parser')\n",
    "    # Construct a regular expression that matches links to the arXiv category page for the current date and specified category\n",
    "    regex = re.compile(re.escape(f'/list/{category}/{current_formatted_date}'))\n",
    "    # Find all links on the page that match the regular expression\n",
    "    links = soup.find_all('a', href=regex)\n",
    "    link = None\n",
    "    # Loop over the links and find the link with the text \">all</a>\"\n",
    "    for l in soup.find_all('a', href=regex):\n",
    "        if str(l)[-8:] == \">all</a>\":\n",
    "            link = l\n",
    "    # Send an HTTP request to the URL of the page that contains links to individual papers in the category\n",
    "    response_2 = requests.get(url+link['href'])\n",
    "    # Parse the HTML content of the response with BeautifulSoup\n",
    "    soup = BeautifulSoup(response_2.content, 'html.parser')\n",
    "\n",
    "        # Extract the URLs of the first num_papers papers in the category\n",
    "    paper_urls = []\n",
    "    for i, paper in enumerate(soup.find_all('dt')):\n",
    "        if i >= num_papers:\n",
    "            break\n",
    "        paper_id = paper.find('a')['href'].split('/')[-1]\n",
    "        paper_urls.append(paper_id)\n",
    "\n",
    "    # Download the papers using multiprocessing\n",
    "    with mp.Pool() as pool:\n",
    "        pool.starmap(download_paper, [(download_dir, paper_id) for paper_id in paper_urls])\n",
    "\n",
    "    # Notify the user that the papers have been downloaded\n",
    "    print(f\"Downloaded {len(paper_urls)} papers to {download_dir}.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}