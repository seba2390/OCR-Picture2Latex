\section{Preliminaries}
\noindent\textbf{Notation:} We denote by $\P(x)$ and $p(x)$ the cumulative distribution function and the probability density function defined on $x\in\mathcal{X}$, respectively. We define $[n]=\{1,2,\ldots,n\}$. The cardinality of set $\mathcal{A}$ is denoted by $|\mathcal{A}|$. Let $\|\cdot\|_2$ be the $\ell_2$ norm of a vector or the spectral norm of a matrix. We denote by $\|\cdot\|_{\rF}$ the Frobenius norm of a matrix. For a matrix $M\in\R^{m\times n}$, we denote by $\sigma_{\min}(M)$ and $\sigma_{\max}(M)$ the smallest non-zero singular value and the largest singular value of $M$, respectively. 


% We use $\tilde{\mathcal{O}}(\cdot)$ and $\tilde{\Omega}(\cdot)$ notation to hide polylogarithmic factors. The use of $\mathcal{O}$, $\Omega$ and $\Theta$ is standard.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Basic Definitions}

To measure the difference between two probability distributions, we give the formal definition of the total variation distance and the Hellinger Distance as follows. 
\begin{definition}[Total Variation Distance and Hellinger Distance]\label{def_tv}
Suppose that $\P_1$ and $\P_2$ are two probability measures defined on a measurable space $(\mathcal{X},\mathcal{F})$. The total variation distance between $\P_1$ and $\P_2$ is defined as,
\$
\TV(\P_1,\P_2):=\sup_{A\in\mathcal{F}}|\P_1(A)-\P_2(A)|.
\$
Let $\mu$ be a measure defined on $(\mathcal{X},\mathcal{F})$ such that $\P_1\ll\mu$, $\P_2\ll\mu$. It then holds that
\$
\TV(\P_1,\P_2)=\frac{1}{2}\int_{\mathcal{X}} |p_1-p_2|\, d\mu.
\$
Here 
\$
p_1:=\frac{d\P_1}{d\mu},\quad p_2:=\frac{d\P_2}{d\mu}.
\$
The square of Hellinger distance between $\P_1$ and $\P_2$ is defined as
\$
H^2(\P_1,\P_2)=\frac{1}{2}\int_{\mathcal{X}}(\sqrt{p_1}-\sqrt{p_2})^2\,d\mu.
\$
\end{definition}

Note that if $\mathcal{X}\subset\R^d$, then $p_1$ and $p_2$ are the probability density functions of the cumulative distributions $\P_1$ and $\P_2$. In the following, we focus on the case where $\mathcal{X}\subset\R^{d}$. We denote by $\mP_{\mathcal{X}}(\Phi)$ a set of parameterized density functions $p_{\phi}(x)$ defined on $x\in\mathcal{X}$
\$
\mP_{\mathcal{X}}(\Phi):=\{p_{\phi}(x)\,|\,\phi\in\Phi\},
\$
where $\phi\in\Phi$ is the parameter. Our main results will be phrased in terms of bracketing number for the class $\mP_{\mathcal{X}}(\Phi)$ under $\|\cdot\|_1$ distance, which is defined as follows.


\begin{definition}[$\epsilon$-Bracket and Bracketing Number]
Let $\epsilon>0$. Under $\|\cdot\|_1$ distance, a set of functions $\mN_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ is an $\epsilon$-bracket of $\mP_{\mathcal{X}}(\Phi)$ if for any $p_{\phi}(x)\in\mP_{\mathcal{X}}(\Phi)$, there exists a function $\bar p_{\phi}(x)\in\mN_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ such that the following two properties hold:
\begin{itemize}
    \item $\bar p_{\phi}(x)\geq p_{\phi}(x),~\forall x\in\mathcal{X}$
    \item $\|\bar p_{\phi}(x)-p_{\phi}(x)\|_1=\int |\bar p_{\phi}(x)- p_{\phi}(x)|\, dx\leq \epsilon$
\end{itemize}
Note that $\bar p_{\phi}(x)$ need not to belong to $\mP_{\mathcal{X}}(\Phi)$. The bracketing number $N_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ is the cardinality of the smallest $\epsilon$-bracket needed to cover $\mP_{\mathcal{X}}(\Phi)$. The entropy is defined as the logarithm of the bracketing number.
\end{definition}

To measure the complexity of a function class, we consider the Rademacher complexity defined as follows.
\begin{definition}[Rademacher Complexity]
Suppose that $x_1,\ldots,x_n$ are sampled i.i.d from a probability distribution defined on a set $\mathcal{X}$. Let $\mathcal{G}$ be a class of functions mapping from $\mathcal{X}$ to $\R$. The empirical Rademacher complexity of $\mathcal{G}$ is defined as follows,
\$
\hat R_n(\mathcal{G}):=\E_{\sigma_i}\bigg[\sup_{g\in\mathcal{G}}\frac{2}{n}\sum^{n}_{i=1}\sigma_i g(x_i)\bigg],
\$
where $\{\sigma_i\}^n_{i=1}$ are independent random variables drawn from the Rademacher distribution and the expectation is taken over the randomness of $\{\sigma_i\}^n_{i=1}$. The Rademacher complexity of $\mathcal{G}$ is defined as
\$
R_n(\mathcal{G}):=\E_{x_i}[\hat R_n(\mathcal{G})],
\$
where the expectation is taken over the randomness of samples $\{x_i\}^n_{i=1}$.
\end{definition}

