In this section, we present a dimension-reduced KRnet map approach (DR-KRnet) in detail, which consists of three parts (choices of prior, likelihood computation, and posterior approximation).
%since we employ the Bayesian framework. 
First, the VAE prior is introduced to capture the features of $\{y^{(i)}\}_{i=1}^N$. Next, the KRnet map is adopted for pushing forward the prior to the posterior in the low-dimensional latent space. In addition, physics-constrained surrogate modeling is used to compute the likelihood function efficiently.
% \subsection{VAE-GAN priors for dimension reduction}\label{vae_gan_section}
\subsection{VAE priors for dimension reduction}\label{vae_gan_section}
As a dimension reduction method, variational autoencoder (VAE) builds the relationship between the latent space and the original high-dimensional parameter. We briefly recall the VAE. Assume that there exists a latent random variable $x\in \mathbb{R}^d$ ($d\ll n$) with a marginal distribution $p_{x,\theta}$, where $\theta$ includes the model parameters. The joint distribution $p_{x,y,\theta}$ of $x$ and $y$ is then described by the conditional distribution $p_{y|x,\theta}$, i.e., $p_{x,y,\theta}=p_{y|x,\theta}p_{x,\theta}$.
% The main task of VAE is to generate $y$ from $p_y$.
 According to Bayes' rule,
\begin{align}
    p_{y,\theta}=\frac{p_{x,y,\theta}}{p_{x|y,\theta}}=\frac{p_{y|x,\theta}p_{x,\theta}}{p_{x|y,\theta}}.
\end{align}
The posterior distribution $p_{x|y,\theta}$ is in general
intractable, and then an approximation model $q_{x|y,\phi}$ is needed, where $\phi$ includes the model parameters. The optimal parameters $\theta$ and $\phi$ are determined by minimizing the KL divergence
\begin{align}
    D_{KL}(q_{x|y,\phi}||p_{x|y,\theta})=D_{KL}(q_{x|y,\phi}||p_{x,\theta})-\mathbb{E}_{q_{x|y,\phi}}[\log p_{y|x,\theta} ]+\log p_{y,\theta}\geq 0.
\end{align}
The minimization of $D_{KL}(q_{x|y,\phi}||p_{x|y,\theta})$ is equivalent to the maximization of
the variational lower bound of $\log p_{y,\theta}$, which is defined as 
\begin{align}\label{eq_vae_loss}
    \mathcal{L}_{\theta,\phi}(y)=\mathbb{E}_{q_{x|y,\phi}}[\log p_{y|x,\theta} ]-D_{KL}(q_{x|y,\phi}||p_{x,\theta}).
\end{align}
% A VAE consists of two networks that encode a data sample $y\in \mathbb{R}^m$
% to a latent representation $x\in \mathbb{R}^n$ and decode the latent representation back to data space,
% respectively:
% \begin{align}
%     x\sim \text{Enc}(y)=q_{x|y,\phi}, \quad, \Tilde{y}\sim\text{Dec}(x)=p_{y|x,\theta},
% \end{align}
% where $\phi$ and $\theta$ denote model parameters for the encoder and the decoder respectively.
% The VAE regularizes the encoder by imposing a prior over the latent distribution $p_{x,\theta}$.  
In the canonical VAE, we specify the PDF models respectively for $p_{y|x,\theta},\,q_{x|y,\phi}$ and $p_{x,\theta}$ as follows:
\begin{equation}
	\begin{aligned}
		p_{y|x,\theta}&=\mathcal{N}\left(\mu_{de,\theta}\left(x\right), \text{diag}\left(\sigma_{de,\theta}^{\odot 2}\left(x\right)\right)\right),\\
		q_{x|y,\phi}&=\mathcal{N}\left(\mu_{en,\phi}\left(y\right), \text{diag}\left(\sigma_{en,\phi}^{\odot 2}\left(y\right)\right)\right), \label{caen}\\
		p_{x,\theta}&=\mathcal{N}(0,\mathbf{I}),
	\end{aligned}
\end{equation}
where ${*}^{\odot2}$ means the component-wise square operation. The tuples $(\mu_{en,\theta}(y), \sigma_{en,\theta}(y))$ and $\left(\mu_{de,\theta}(x), \sigma_{de,\theta}(x)\right)$ are modeled via neural networks, i.e.,
\begin{align}
&\left(\mu_{en,\theta}(y), \sigma_{en,\theta}(y)\right)=\text{NN}_{en}(y;\theta),\label{vae1}\\
&x=\mu_{en,\phi}(y)+\sigma_{en,\phi}(y)\odot \varepsilon,\quad \varepsilon \sim \mathcal{N}(0,\mathbf{I}),\label{vae2}\\
&\left(\mu_{de,\theta}(x), \sigma_{de,\theta}(x)\right)=\text{NN}_{de}(x;\phi),\label{vae3}\\
&\hat{y}=\mu_{de,\theta}(x),
\end{align}
where $\text{NN}_{de}$ and $\text{NN}_{en}$ characterize the encoder and decoder neural networks to describe the relation between a data sample $y\in\mathbb{R}^n$ and a latent representation $x\in\mathbb{R}^d$, and $\hat{y}$ is the reconstruction of  $y$. 
% Figure \ref{vae_gan} illustrates the structure of VAE. 

Given a prior dataset $Y:=\{y^{(i)}\}_{i=1}^N$, the expectation of the variational lower bound  \eqref{eq_vae_loss} can be approximated via 
the Monte Carlo estimation
\begin{align}
    \mathbb{E}_{p_{y,\theta}}\left[\mathcal{L}_{\theta,\phi}(y)\right]&\approx \frac{1}{N}\sum_{i=1}^N\mathcal{L}_{\theta,\phi}\left(y^{(i)}\right)\nonumber\\
    &\approx\underbrace{\frac{1}{N}\sum_{i=1}^N\left[\log p_{y^{(i)}|x^{(i)},\theta}-(\log q_{x^{(i)}|y^{(i)},\phi}-\log p_{x^{(i)},\theta})\right]}_{\hat{\mathcal{L}}_{\theta,\phi}(Y)}, \label{vae_loss_discri}
\end{align}
where $x^{(i)}$ can be generated by substituting $y^{(i)}$ into  \eqref{vae2}.
% which are modeled by neural networks, characterize the encoder and decoder to describe the relation between a data sample $y\in\mathbb{R}^n$ and a latent representation $x\in\mathbb{R}^d$:
%In all, a VAE consists of two networks that encode a data sample $y\in \mathbb{R}^n$
%to a latent representation $x\in \mathbb{R}^d$ and decode the latent representation back to data space,
%respectively:
%\begin{equation}
% \begin{align}
% 	x&=\text{Enc}(y)=\mu_{en,\phi}(y)+\sigma_{en,\phi}(y)\odot \epsilon_1,\label{vae_first}\\
% 	\hat{y}&=\text{Dec}(x)=\mu_{de,\phi}(x)+\sigma_{de,\phi}(x)\odot \epsilon_2,\label{vae_second}
% \end{align}
% %\end{equation}
% where $\epsilon_1\sim \mathcal{N}(0,\mathbf{I})$, $\epsilon_2\sim \mathcal{N}(0,\mathbf{I})$, and $\hat{y}$ is the reconstruction of  $y$.
% The VAE loss is minus the sum of the expected log likelihood (the reconstruction error) and a prior regularization term:
% \begin{align}
%     \mathcal{L}_{VAE}&=-\mathbb{E}_{q_{x|y,\phi}}\Big[\log\frac{p_{y|x,\theta}p_{x,\theta}}{q_{x|y,\phi}}\Big] \nonumber\\
%     &=-\mathbb{E}_{q_{x|y,\phi}}[\log p_{y|x,\theta} ]+D_{KL}(q_{x|y,\phi}||p_{x,\theta})
% \end{align}
% The main drawback of VAE model is that it tends to produce unrealistic, blurry samples, which seriously influences the accuracy of estimating posterior mean in inverse problems. As we all know, an appealing property of GAN is that its discriminator contributes to generating realistic data that resembles the prior datasets $\{y^{(i)}\}_{i=1}^N$. 
% A GAN consists of two neural networks: the generator network $y=\text{Gen}(x)$ while the discriminator network assigns probability $p=\text{Dis}(y)\in [0,1]$ that $y$ is an actual training sample and probability $1-p$ that $y$ is generated by the generator network. Appplying the advantages of VAE and GAN, we propose a more accuracy method---VAE-GAN prior for characterizing the prior datasets, which combines VAE and GAN by replacing the generator of GAN by VAE. The network structure of VAE-GAN priors is shown in Fig.\ \ref{vae_gan}. The parameters of VAE is updated by minimizing the following loss
% \begin{align}\label{gen_loss}
% 	\mathcal{L}_{\text{gen}}=\mathcal{L}_{VAE}+\beta \log (\text{Dis}(\text{Dec}(x))),
% \end{align}
% where $\mathcal{L}_{VAE}=-\mathcal{L}_{\theta,\phi}(y)$, $\beta$ is a tunable parameter and minimizing the second term aims to generate realistic samples. The parameters of discriminator is updated by minimizing the following loss
% \begin{align}
% \mathcal{L}_{\text{GAN}}=\log(\text{Dis}(y))+\log (1-\text{Dis}(\text{Dec}(x))).
% \end{align}
% The training process of VAE-GAN priors is illustrated in Algorithm \ref{alg_vae_gan}.
%where the first is to enforce that discrimininator views the actual training sample as 
We pre-train the VAE priors by Algorithm \ref{alg_vae_gan}, of which the output  is the pre-trained decoder $p_{y|x,\theta^*}$. Here $\theta^*$ consists of the optimal parameters of the decoder.
The inference over $y$ in  \eqref{yposterior} is replaced by infering the latent variable $x$ from the observations, formulated as
\begin{align}
	\pi(x|\mathcal{D}_{obs})\propto &{\pi(\mathcal{D}_{obs}|x)\pi(x)}, \nonumber\\
	&=\left(\int \pi(\mathcal{D}_{obs}|y,x) \pi(y|x) dy\right) \pi(x)\nonumber\\
	&= \underbrace{\left(\int \pi(\mathcal{D}_{obs}|y,x) p_{y|x,\theta^*} dy\right) p_{x,\theta^*}}_{\hat{\pi}(x)}, \label{xposterior}
\end{align}
where $\pi(\mathcal{D}_{obs}|y,x) $ is the likelihood function, $p_{y|x,\theta^*}$ is the pre-trained decoder, and $p_{x,\theta^*}$ is a simple prior distribution of VAE, e.g., the standard Gaussian. % priors.
% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.8\textwidth]{image/351676253784_.pic.jpg}
% 	\caption{The illustration of VAE.}
% 	\label{vae_gan}
% \end{figure}
\begin{algorithm}[H]
	\caption{Training the VAE priors}
	\label{alg_vae_gan}
	\begin{algorithmic}[1]
		\Require The prior dataset $Y:=\{y^{(i)}\}_{i=1}^N$, maximum epoch number $E$, batch size $n_{batch}$, learning rate $\eta$.
           \State Divide $Y$ into $N_b$ mini-batches $\{Y_j\}_{j=1}^{N_b}$ where $N_b=\frac{N}{n_{batch}}$.
           \State Initialize $\theta$ and $\phi$ for the encoder and decoder networks.		
           \For {$i = 1:E$}
		\For {$j=1:N_b$}
		%\State Sample $n_{batch}$ datapoints $Y$ from $\{y^{(i)}\}_{i=1}^N$
            \State Construct the noise set $S_j=\{\varepsilon^k\sim \mathcal{N}(0,\mathbf{I}),k=1,2,\dots,n_{batch}\}$.
            \State Apply $Y_j$ and $S_j$ to compute  \eqref{vae1}--\eqref{vae3}.
	    % \State Obtain $\mu_{en,\phi}$ and $\sigma_{en,\phi}$ through the encoder
		\State Compute  $-\hat{\mathcal{L}}_{\theta,\phi}(Y_j)$ in  \eqref{vae_loss_discri} and its gradients $-\nabla_{\theta}\hat{\mathcal{L}}_{\theta,\phi}(Y_j),\,-\nabla_{\phi}\hat{\mathcal{L}}_{\theta,\phi}(Y_j)$.
		% \State $\mathcal{L}_{\text{GAN}}\leftarrow \log(\text{Dis}(Y))+\log (1-\text{Dis}(\Tilde{Y})).$
		\State Update the parameters $(\theta,\phi)$ using gradient-based optimization algorithm (e.g., Adam optimizer \cite{kingma2014adam} with learning rate $\eta$).
		\EndFor
		\EndFor
         \State Let $\theta^*=\theta$, where $\theta$ includes the parameters of the decoder networks at the last epoch.
		\Ensure The probabilistic decoder $p_{y|x,\theta^*}$.
%		\Ensure probabilistic encoder $q_{x|y,\phi^*}$, probabilistic decoder $p_{y|x,\theta^*}$.
	\end{algorithmic}
\end{algorithm}
% \begin{rmk}[Some thoughts about VAE-GAN]
% {\color{red}The loss of VAE has two terms: for any $y$,
% \[
% \mathcal{L}_{VAE}=D_{KL}(q_{x|y}\|p_x)+\mathbb{E}_{q_{x|y}}[\log p_{y|x}].
% \]
% The second term is a reconstruction error: If we let $p_{y|x}\sim \mathcal{N}(f(x),c\mathbf{I})$, the second term is 
% \[
% \frac{1}{2c}\|y-\text{Dec}(\text{Enc}(y))\|^2,
% \]
% i.e., the reconstruction error is measured in terms of the $L_2$ norm. The first term of $\mathcal{L}_{VAE}$ force the posterior $q_{x|y}$ close to $p_x$, which can be regarded as a regularization term. In equation (15), if we let 
% \[
% \mathcal{L}_{GAN}=\mathbb{E}[\log(\text{Dis}(y))]+\mathbb{E}[\log(1-\text{Dis}(\text{Dec}(\text{Enc}(y))))]
% \]
% it can be regarded as an extra reconstruction error by minimizing the difference between the distributions of $y$ and $\text{Dec}(\text{Enc}(y))$ using the Jensen-Shannon divergence. This is the current choice. 

% Since we only need the decoder, we may emphasize the prior distribution by simply replacing the generator of GAN with the decoder such that
% \[
% \mathcal{L}_{GAN}=\mathbb{E}[\log(\text{Dis}(y))]+\mathbb{E}[\log(1-\text{Dis}(\text{Dec}(x)))],
% \]
% where $x\sim p_x(x)$. This will force the joint distribution $p_{y|x}p_x$ to be more consistent with the data distribution. The loss of VAE plays a role of regularization. 

% Another idea to make the decoder sharper is to modify the loss of VAE as
% \[
% \mathcal{L}_{VAE}=D_{KL}(q_{x|y}\|p_x)+\mathbb{E}_{q_{x|y}}[\log p_{y|x}]+\lambda\|y-\mu_{de}(\text{Enc}(y))\|^2,
% \]
% where the extra penalty term ignores the noise in the decoder. Minimizing the noise in the decoder corresponds to maximizing the mutual information between $y$ and $x$. 

% I am not sure which one works better since the modeling capability of VAE is limited. 
% }
% \end{rmk}

\subsection{KRnet map}
In  \eqref{xposterior}, let $\pi(x|\mathcal{D}_{obs})=C^{-1}\hat{\pi}(x)$, $x\in \mathbb{R}^d$, $d\ll n$. In the low-dimensional latent space of the pre-trained VAE prior, we intend to approximate the posterior $\pi(x|\mathcal{D}_{obs})$ by constructing a map that pushes forward the prior to the posterior. In other words, we seek a transport map $\mathcal{T}$: $z \mapsto x$ such that $\mathcal{T}_{\#} \mu_z = \mu_x$, where $d\mu_z=p_{z,\theta^*}dz$ and $d\mu_x=\pi(x|\mathcal{D}_{obs})dx$ are the probability measures of $z$ and $x$ respectively, and $\mathcal{T}_{\#} \mu_{z}$ is the push-forward of $\mu_{z}$ satisfying $\mu_{x}(B) = \mu_{z}(\mathcal{T}^{-1}(B))$ for every Borel set $B$. The Knothe-Rosenblatt rearrangement tells us that the transport map $\mathcal{T}$ may have a lower-triangular structure 
\begin{equation}
    {z} = \mathcal{T}^{-1}({x}) = \left[ 
    \begin{array}{l}
    \mathcal{T}_1(x_1) \\
    \mathcal{T}_2(x_1, x_2) \\
    \vdots \\
    \mathcal{T}_{d}(x_1, \ldots, x_d)
    \end{array}
    \right].
\end{equation} 
This mapping can be regarded as a limit of sequence of optimal transport maps when the quadratic cost degenerates \cite{carlier2010knothe}. 

%With the Knothe-Rosenblatt rearrangement, our proposed flow-based generative model, called KRnet, provides an expressive model for density  approximation. KRnet provides an invertible mapping $x=f^{-1}(z)$, which can push the prior $z\sim p_{z,\theta^*}$ towards the posterior $x\sim \pi(x|\mathcal{D}_{obs})$. 


 %Let $\mu_Z$ and $\mu_X$ be the probability measures of two random variables $X,Z\in\xs{R}^d$ respectively. A mapping $\mathcal{T}$: $Z \mapsto X$ is called a transport map such that $\mathcal{T}_{\#} \mu_Z = \mu_X$, where $\mathcal{T}_{\#} \mu_{Z}$ is the push-forward of $\mu_{Z}$ such that $\mu_{X}(B) = \mu_{Z}(\mathcal{T}^{-1}(B))$ for every Borel set $B$.

%Noticing that the invertible mapping $f(x)$ also defines a transport map, we then incorporate the triangular structure of the Knothe-Rosenblatt rearrangement into the definition of $f(x)$ which results in KRnet as a generalization of real NVP \cite{dinh2016density}. 
The basic idea of KRnet is to define the structure of a normalizing flow $f({x})$ in terms of the Knothe-Rosenblatt rearrangement which results in KRnet as a generalization of real NVP \cite{dinh2016density}. Let ${x} = \left[{x}^{(1)}, \ldots, {x}^{(K)}\right]^\mathsf{T}$ be a partition of ${x}$, where ${x}^{(i)} =\left[x_{1}^{(i)}, \ldots, x_{m}^{(i)}\right]^\mathsf{T}$ with $1 \leq K \leq d, 1 \leq m \leq d$, and $\sum_{i=1}^K \mathrm{dim}\left({x}^{(i)}\right) = d$. Our KRnet takes an overall form
\begin{equation} \label{eqn_KR}
  {z} = f({x}) = \left[ 
    \begin{array}{l}
    f_1\left({x}^{(1)}\right) \\
    f_2\left({x}^{(1)}, {x}^{(2)}\right) \\
    \vdots \\
    f_{K}\left({x}^{(1)}, \ldots, {x}^{(K)}\right)
    \end{array}
    \right].
\end{equation}
Each $f_{i}$, $i=2,\ldots,K$, is constructed with real NVP by stacking a sequence of simple bijections. KRnet provides a more expressive density model than real NVP for the same model size. More details about KRnet can be found in \cite{tang2020deep,adda_2022}.

%KRnet consists of one  outer loop and $K-1$ inner loops. The outer loop has $K-1$ stages, corresponding to the $K-1$ mappings $f_{i}$ in equation \eqref{eqn_KR} with $i=2,\ldots,K$, and for each stage, an inner loop of $L$ affine coupling layers is defined. More specifically, we have
%\begin{equation}\label{krnet_expr}
%z = f(x) = L_{N} \circ f_{[K-1]}^{\textsf{outer}} \circ \cdots \circ f_{[1]}^{\textsf{outer}} (x),
%\end{equation}
%where $f_{[i]}^{\textsf{outer}}$ is defined as
%\begin{equation}
%f_{[k]}^{\textsf{outer}} = L_S \circ f_{[k, L]}^{\textsf{inner}} \circ \cdots \circ %f_{[k,1]}^{\textsf{inner}} \circ L_R.
%\end{equation}
%Here $f_{[k,i]}^{\textsf{inner}}$ indicates a combination of one affine coupling layer and one scale and bias layer, and $L_N$, $L_S$ and $L_R$ indicate the nonlinear layer, the squeezing layer and the rotation layer, respectively.
%More details about KRnet can be found in \cite{tang2020deep,adda_2022}.

%\input{krnet}

Let $q_{x,\alpha}$ be the PDF model induced by a KRnet with model parameters $\alpha$, and then  \eqref{eqn_KR} is reformulated into
\begin{equation}\label{eqn_KR_alpha}
    {z} = f_{\alpha}({x}).
\end{equation}
To approximate $\pi(x|\mathcal{D}_{obs})$ in  \eqref{xposterior}, we minimize the KL divergence between $q_{x,\alpha}$ and $\pi(x|\mathcal{D}_{obs})$
\begin{align*}
	D_{KL}\left(q_{x,\alpha}||\pi\left(x|\mathcal{D}_{obs}\right)\right)=\int q_{x,\alpha}\log \frac{q_{x,\alpha}}{\pi(x|\mathcal{D}_{obs})} dx
	=\int q_{x,\alpha}\log \frac{q_{x,\alpha}}{\hat{\pi}(x)} dx+\log C,
\end{align*}
which is equivalent to minimize the following functional
\begin{align}
	\int q_{x,\alpha}\log \frac{q_{x,\alpha}}{\hat{\pi}(x)} dx 
	&=\int p_{z,\theta^*}\log \frac{q_{x,\alpha}\left(f_{\alpha}^{-1}(z)\right)}{\hat{\pi}\left(f_{\alpha}^{-1}(z)\right)} dz \nonumber\\
	&\approx\frac{1}{I}\sum_{i=1}^I\log q_{x,\alpha}\left(f_{\alpha}^{-1}\left(z^{(i)}\right)\right)-\frac{1}{I}\sum_{i=1}^I\log \hat{\pi}\left(f_{\alpha}^{-1}\left(z^{(i)}\right)\right),\quad z^{(i)} \sim p_{z,\theta^*}. \label{firstloss}
\end{align}
Let $x^{(i)}=f_{\alpha}^{-1}\left(z^{(i)}\right)$. The second term of the right hand of  \eqref{firstloss} is obtained as
\begin{align*}
	-\frac{1}{I}\sum_{i=1}^I\log \hat{\pi}\left(x^{(i)}\right)
	&=-\frac{1}{I}\sum_{i=1}^I\log \left(\int \pi\left(\mathcal{D}_{obs}|y,x^{(i)}\right) p_{y|x^{(i)},\theta^*} dy\right) -\frac{1}{I}\sum_{i=1}^I\log p_{x^{(i)},\theta^*}\\
	&\leq -\frac{1}{I}\sum_{i=1}^I\int p_{y|x^{(i)},\theta^*} \log \pi\left(\mathcal{D}_{obs}|y,x^{(i)}\right) dy-\frac{1}{I}\sum_{i=1}^I\log p_{x^{(i)},\theta^*}\\
	&\approx -\frac{1}{I}\frac{1}{J}\sum_{i=1}^I\sum_{j=1}^J \log \pi\left(\mathcal{D}_{obs}|y^{(i,j)},x^{(i)}\right)-\frac{1}{I}\sum_{i=1}^I\log p_{x^{(i)},\theta^*},\quad y^{(i,j)}\sim p_{y|x^{(i)},\theta^*}, 
\end{align*}
where the Jensen's inequality is applied and $\pi\left(\mathcal{D}_{obs}|y^{(i,j)},x^{(i)}\right)$ is the likelihood function. Since the first term on the right-hand side corresponds to the expectation of $\log\pi(\mathcal{D}_{obs}|y,x)$ with respect to the joint distribution given by $p_{y|x,\theta^*}p_{x,\theta^*}$, we may simply let $J=1$.  
%For simplify, we can just let $J=1$ by noting that
%\begin{align}
%	&-\frac{1}{I}\sum_{i=1}^I\log \hat{\pi}\left(x^{(i)}\right) \nonumber\\
%	&\leq -\frac{1}{I}\sum_{i=1}^I\log \pi(\mathcal{D}_{obs}|y^{(i)},x^{(i)})-\frac{1}{I}\sum_{i=1}^I\log p_{x^{(i)},\theta^*},\quad y^{(i)}\sim p_{y|x^{(i)},\theta^*}. \label{secondloss}
%\end{align}
%Substituting \eqref{secondloss} into \eqref{firstloss}, we need to minimize the following objective function
We then reach our objective function for minimization
\begin{align}
	\mathcal{L}_{KRnet}&=\frac{1}{I}\sum_{i=1}^I\log q_{x^{(i)},\alpha}-\frac{1}{I}\sum_{i=1}^I \log \pi\left(\mathcal{D}_{obs}|y^{(i)},x^{(i)}\right)-\frac{1}{I}\sum_{i=1}^I\log p_{x^{(i)},\theta^*}, \label{krnet_loss}
 %\\
%		&=\frac{1}{I}\sum_{i=1}^I\Big[\log q_{x^{(i)},\alpha}-\log %\pi(\mathcal{D}_{obs}|y^{(i)},x^{(i)})-\log p_{x^{(i)},\theta^*}\Big], \label{krnet_loss}
\end{align}
where $x^{(i)}=f_{\alpha}^{-1}\left(z^{(i)}\right),\, z^{(i)} \sim p_{z,\theta^*}$ and $y^{(i)}\sim p_{y|x^{(i)},\theta^*} $.

Once KRnet has been trained by minimizing $\mathcal{L}_{KRnet}$, we can estimate the moments of the posterior  $\pi(y|\mathcal{D}_{obs})$ through the pre-trained decoder,
\begin{align}
	\mathbb{E}[y]&=\int y \left(\int p_{y|x,\theta^*}q_{x,\alpha^*}dx \right) dy
	\approx \frac{1}{N_s}\sum_{i=1}^{N_s}\int y p_{y|x^{(i)},\theta^*} dy\nonumber\\
	&\approx \frac{1}{N_s}\sum_{i=1}^{N_s} \mu_{de,\theta^*}\left(x^{(i)}\right),\quad x^{(i)}=f_{\alpha^*}^{-1}\left(z^{(i)}\right),\, z^{(i)} \sim p_{z,\theta^*}, \label{mean_compute}
\end{align}
\begin{align}
	\mathbb{V}[y]&=\mathbb{E}\left[\left(y-\mathbb{E}[y]\right)\left(y-\mathbb{E}[y]\right)^\mathsf{T}\right]\nonumber\\
	&=\int (y-\mathbb{E}[y])(y-\mathbb{E}[y])^\mathsf{T}\left(\int p_{y|x,\theta^*}q_{x,\alpha}dx \right) dy\nonumber\\
	&\approx \frac{1}{N_s}\sum_{i=1}^{N_s}\int (y-\mathbb{E}[y])(y-\mathbb{E}[y])^\mathsf{T} p_{y|x^{(i)},\theta^*} dy\nonumber\\
	&\approx \frac{1}{N_s}\sum_{i=1}^{N_s} \text{diag}\left(\sigma_{de,\theta^*}^{\odot 2}\left(x^{(i)}\right)\right),\quad x^{(i)}=f_{\alpha^*}^{-1}\left(z^{(i)}\right),\, z^{(i)} \sim p_{z,\theta^*},\label{variance_compute}
\end{align}
where $ p_{y|x^{(i)},\theta^*}=\mathcal{N}\left(\mu_{de,\theta^*}\left(x^{(i)}\right), \text{diag}\left(\sigma_{de,\theta^*}^{\odot 2}\left(x^{(i)}\right)\right)\right)$ is the pre-trained decoder given in section \ref{vae_gan_section}, $\alpha^*$ represents the optimal parameters of KRnet, and $N_s$ is the number of posterior samples.
%and ${*}^{\odot2}$ means the component-wise square operation. 
\subsection{Physics-constrained surrogate modeling}
Asides from the pre-trained decoder, we need to pre-train a surrogate model for the forward problem such that we may efficiently minimize $\mathcal{L}_{KRnet}$ given in  \eqref{krnet_loss} by stochastic gradient-based optimization \cite{bottou2018optimization}.
%for efficiently achieving the forward computation and the adjoint method at the optimization step in \eqref{krnet_loss}.
Assume that the governing %the physical system described in \eqref{physical_problem} 
equations are defined on a two-dimensional regular $H\times W$ grid, where $H$ and $W$ denote the number of grid points along the two axes of the spatial domain. We transform the surrogate modeling problem into an image-to-image regression problem through a mapping 
%, with the regression
%function defined as
\begin{align}\label{surroagte_map}
	\hat{\mathcal{F}}:y\in \mathbb{R}^{d_y\times H\times W }\to u\in \mathbb{R}^{d_u\times H\times W }.
\end{align}
Here $d_y$ and $d_u$ are treated as the number of channels in the input and output images, similar to the RGB channels in natural images. More specifically, the surrogate model $u=\hat{\mathcal{F}}_{\Theta}(y)$ with model parameters $\Theta$ is composed of convolutional encoder and decoder networks, i.e.,\ $u=\text{decoder}\circ\text{encoder}(y)$. The surrogate model is trained without labeled data, in other words, the PDE will not be simulated for some chosen $y$. Similar to PINN, it is trained \cite{raissi2019physics} by enforcing the constraints given by  \eqref{physical_problem}, i.e., we minimize the following objective function:
%learning to solve the PDE with given boundary conditions, using the following loss function
\begin{align}\label{surrogate_loss}
	\mathcal{J}\left(\Theta;\{y^{(i)}\}_{i=1}^N\right)=\frac{1}{N}\sum_{i=1}^N\left[ {\left\Arrowvert \mathcal{R}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right),y^{(i)}\right)\right\Arrowvert}_2^2+\beta{\left\Arrowvert\mathcal{B}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right)\right)\right\Arrowvert}_2^2\right],
\end{align}
where 
$\mathcal{R}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right),y^{(i)}\right)=\mathcal{L}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right);y^{(i)}\right)-h$ and $\mathcal{B}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right)\right)=\pB\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right);y^{(i)}\right)-g$ measure how well $\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right)$ satisfies the partial differential equations and the boundary conditions, respectively, and $\beta>0$ is a penalty parameter. %the weight (Lagrange multiplier) to softly enforce the boundary conditions. 
Both $\mathcal{R}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right),y^{(i)}\right)$ and $\mathcal{B}\left(\hat{\mathcal{F}}_{\Theta}\left(y^{(i)}\right)\right)$ may involve
integration and differentiation with respect to the spatial coordinates, which are approximated with highly efficient discrete operations, e.g.,\ Sobel filters \cite{ma2004invitation,zhu2019physics}. The surrogate trained with the loss function  \eqref{surrogate_loss} is called
physics-constrained surrogate. The %physics constrained model is implemented by 
training process is summarized in Algorithm \ref{alg_surrogate}.

Once we obtain the pre-trained decoder $p_{y|x,\theta^*}$ and the pre-train surrogate model $\hat{\mathcal{F}}_{\Theta^*}(y)$, we can find the transport map from the prior to the posterior in the low-dimensional latent space, 
%and then compute the posterior moments through pre-trained decoder, 
which is implemented in Algorithm \ref{alg_krnet}. The whole process of seeking the dimension-reduced KRnet map (DR-KRnet) is shown in Figure \ref{krnet_flow}.

\begin{algorithm}[H]
	\caption{Training the physics-constrained surrogate model}
	\label{alg_surrogate}
	\begin{algorithmic}[1]
		\Require The prior dataset $Y:=\{y^{(i)}\}_{i=1}^N$, maximum epoch number $E$, batch size $n_{batch}$, and learning rate $\eta$.
            \State Divide $Y$ into $N_b$ mini-batches $\{Y_j\}_{j=1}^{N_b}$ where $N_b=\frac{N}{n_{batch}}$.
            \State Initialize $\Theta$ for the surrogate networks.
		\For {$i = 1:E$}
		\For {$j=1:N_b$}
            \State Compute the objective function $\mathcal{J}(\Theta; Y_j)$ in  \eqref{surrogate_loss} and its gradient $\nabla_{\Theta}\mathcal{J}(\Theta; Y_j)$.
		\State Update the parameters $\Theta$ using gradient-based optimization algorithm (e.g., Adam optimizer \cite{kingma2014adam} with learning rate $\eta$).
		\EndFor
		\EndFor
  \State Let $\Theta^*=\Theta$, where $\Theta$ includes the parameters of the surrogate networks at the last epoch.
		\Ensure The surrogate model $u=\hat{\mathcal{F}}_{\Theta^*}(y)$ with optimal parameters $\Theta^*$.
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
	\caption{Dimension-reduced KRnet maps (DR-KRnet)}
	\label{alg_krnet}
	\begin{algorithmic}[1]
		\Require Pre-trained decoder $p_{y|x,\theta^*}=\mathcal{N}\left(\mu_{de,\theta^*}\left(x\right), \text{diag}\left(\sigma_{de,\theta^*}^{\odot 2}\left(x\right)\right)\right)$, pre-trained surrogate model $\hat{\mathcal{F}}_{\Theta^*}$, sample size from $\mathcal{N}(0,\mathbf{I})$ $I$, sample size for posterior distribution $N_s$, batch size $n_{batch}$, maximum epoch number $E$, learning rate $\eta$.
  \State Generate the training dataset $Z:=\{z^{(i)}\}_{i=1}^I$ where $z^{(i)}\sim \mathcal{N}(0,\mathbf{I})$.
  \State Divide $Z$ into $N_b$ mini-batches $\{Z_j\}_{j=1}^{N_b}$ where $N_b=\frac{I}{n_{batch}}$.
  \State Initialize $\alpha$ of the KRnet map.
		\For {$i = 1:E$}
		\For {$j=1:N_b$}
		\State Compute $X_j=f_{\alpha}^{-1}(Z_j)$ in  \eqref{eqn_KR_alpha}.
            \State Compute the high-dimensional parameters: $Y_j=\mu_{de,\theta^*}(X_j).$
		\State Compute the surrogate model: $U_j=\hat{\mathcal{F}}_{\Theta^*}(Y_j).$
		\State Compute the loss function $\mathcal{L}_{KRnet}$ in  \eqref{krnet_loss} and its gradient $\nabla_{\alpha}\mathcal{L}_{KRnet}$.
		\State Update the parameters $\alpha$ using gradient-based optimization algorithm (e.g., Adam optimizer \cite{kingma2014adam} with learning rate $\eta$).
		% $\alpha \leftarrow\alpha+\eta\nabla_{\alpha}\mathcal{L}_{KRnet}$
		\EndFor
		\EndFor
  \State Let $\alpha^*=\alpha$, where $\alpha$ includes the parameters of the KRnet map at the last epoch.
		\State Sample $\{z^{(i)}\}_{i=1}^{N_s} $ where $z^{(i)}\sim \mathcal{N}(0,\mathbf{I})$.
		\State $x^{(i)}=f_{\alpha^*}^{-1}\left(z^{(i)}\right)$, for $i=1,2,\dots,N_s$.
		\State Compute the posterior mean $\hat{\mathbb{E}}[y]=\frac{1}{N_s}\sum_{i=1}^{N_s} \mu_{de,\theta^*}\left(x^{(i)}\right)$ in  \eqref{mean_compute}.
             \State Compute the posterior variance $\hat{\mathbb{V}}[y]=\frac{1}{N_s}\sum_{i=1}^{N_s} \text{diag}\left(\sigma_{de,\theta^*}^{\odot 2}\left(x^{(i)}\right)\right)$ in  \eqref{variance_compute}.
		\Ensure The posterior mean $\hat{\mathbb{E}}[y]$ and the posterior variance $\hat{\mathbb{V}}[y]$.
	\end{algorithmic}
\end{algorithm}
\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{image/KRnet_NN.png}
	\caption{The full workflow of seeking the dimension-reduced KRnet map.}
	\label{krnet_flow}
\end{figure}