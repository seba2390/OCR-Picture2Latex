\renewcommand\thefigure{\thesection.\arabic{figure}}
\counterwithin{figure}{section}

\section{Technical proofs}\label{sec:appendix}

% proofs

\subsection{Results in \Cref{sec:pval}}


\begin{proof}[Proof of \Cref{prop:pvalue.validity}]
\textbf{Step 1}. We prove the validity if $\cdfTestStatistics{i}(t) = \cdfTestStatistics{j}(t)$ for all $j \in \hypothesisIndex{\text{nc}}$ and $t \in \RR$.
By condition~\ref{vali:assu:one.exchangeable.general}, $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is exchangeable.
Since there are no ties among $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ a.s., then the rank of $\testStatistics{i}$ among $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is uniformly distributed on $\{1, 2, \ldots, 1+\NoNc\}$. By Definition~\eqref{eq:pval},
% \begin{align*}
%     \PP\left(\pval{i} = \frac{1+k}{1+\NoNc}\right)
%     = \PP\left(\sum_{j=1}^{\NoNc} \1_{\{\ncTestStatistics{j} \le \testStatistics{i}\}} = k\right)
%     = \frac{1}{1+\NoNc}, \quad 0 \le k \le \NoNc.
% \end{align*}
for $0 \le t \le 1$,
\begin{align*}
    \PP(\pval{i} \le t)
    &= \PP\left(\sum_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j} \le \testStatistics{i}\}}
    \le t(1+\NoNc)\right) \\
    &= \sum_{k=1}^{\lfloor t(1+\NoNc) \rfloor}\PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j}
    \le \testStatistics{i}\}}
    = k \right)
    = \frac{\lfloor t(1+\NoNc)\rfloor}{1+\NoNc}
    \le t.
\end{align*}
If we break ties randomly, $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is still exchangeable and the rank of $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is a permutation of $1$, $2$, $\ldots$, $\NoNc+1$.
The marginal distribution of the rank of $\testStatistics{i}$ is still uniform and the proof holds.


\textbf{Step 2}. We prove the validity if $\cdfTestStatistics{i}(t) \le \cdfTestStatistics{j}(t)$ for all $j \in \hypothesisIndex{\text{nc}}$ and $t \in \RR$.
% Since $\cdfTestStatistics{i}(t)$, $\cdfTestStatistics{j}(t)$, $j \in \hypothesisIndex{\text{nc}}$ are continuous, then
% the inverses $\cdfTestStatistics{i}^{-1}$, $\ncCdfTestStatistics{j}^{-1}$ exist and
% \begin{align*}
%      \cdfTestStatistics{i}(\testStatistics{i})
%      \stackrel{d}{=}
%      \cdfTestStatistics{j}(\testStatistics{j}) \stackrel{d}{=} U[0,1].
% \end{align*}
By condition~\ref{vali:assu:one.exchangeable.general} and step 1, the \nickname~p-value $\pval{i}'$ based on $(\cdfTestStatistics{j}(\testStatistics{j}))_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is valid.
Define $\testStatistics{j}' = \cdfTestStatistics{j}^{-1}(\cdfTestStatistics{j}(\testStatistics{j}))$,
where $\cdfTestStatistics{j}^{-1}(p) := \inf\{t: \cdfTestStatistics{j}(t) \ge p\}$.
Then $\testStatistics{j}' = \cdfTestStatistics{j}^{-1}(\cdfTestStatistics{j}(\testStatistics{j}'))$, $\testStatistics{j}' \le \testStatistics{j}$, and $\PP(\testStatistics{j}' < \testStatistics{j}) = 0$.
On $\{\testStatistics{j}' = \testStatistics{j}\}$, by condition~\ref{vali:assu:null.nc.conservative.general},
\begin{align*}
    \cdfTestStatistics{j}(\testStatistics{j}') \le \cdfTestStatistics{i}(\testStatistics{i})
    \implies
    \cdfTestStatistics{j}(\testStatistics{j}') \le \cdfTestStatistics{j}(\testStatistics{i})
    \implies
    \testStatistics{j}'
    = \cdfTestStatistics{j}^{-1}(\cdfTestStatistics{j}(\testStatistics{j}')) \le \cdfTestStatistics{j}^{-1}\cdfTestStatistics{j}(\testStatistics{i})
    \le \testStatistics{i}.
\end{align*}
Therefore,
\begin{align*}
    \PP(\pval{i} \le t)
    &= \PP\left(\sum_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j}' \le \testStatistics{i}\}}
    \le t(1+\NoNc)\right)
    % = \PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\cdfTestStatistics{i}(\testStatistics{j}) \le \cdfTestStatistics{i}(\testStatistics{i}')\}}
    % \le t(1+\NoNc)\right)
    \\
    &\le \PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\cdfTestStatistics{j}(\testStatistics{j}') \le \cdfTestStatistics{i}(\testStatistics{i})\}}
    \le t(1+\NoNc)\right)
    =  \PP\left(\pval{i}' \le t\right)
    \le t.
\end{align*}
% Note that breaking ties randomly is equivalent to first randomly permuting the indices $\{i\} \cup \hypothesisIndex{\text{nc}}$ and breaking ties according to the permuted indices.
% Denote the permutation mapping by $\sigma: \{i\} \cup \hypothesisIndex{\text{nc}} \to [\NoNc+1]$, then by $\cdfTestStatistics{j}(\testStatistics{j}') < \cdfTestStatistics{i}(\testStatistics{i})
    % \implies \testStatistics{j}' < \testStatistics{i}$, $\cdfTestStatistics{j}(\testStatistics{j}') = \cdfTestStatistics{i}(\testStatistics{i})
    % \implies \testStatistics{j}' \le \testStatistics{i}$, we have
% \begin{align*}
%     &\quad~\1_{\{\cdfTestStatistics{j}(\testStatistics{j}') < \cdfTestStatistics{i}(\testStatistics{i})\}} + \1_{\{\cdfTestStatistics{j}(\testStatistics{j}') = \cdfTestStatistics{i}(\testStatistics{i}), \sigma(j) < \sigma(i)\}}\\
%     &\le \1_{\{\testStatistics{j}' < \testStatistics{i}, \cdfTestStatistics{j}(\testStatistics{j}') < \cdfTestStatistics{i}(\testStatistics{i})\}}
%     + \1_{\{\testStatistics{j}' < \testStatistics{i}, \cdfTestStatistics{j}(\testStatistics{j}') = \cdfTestStatistics{i}(\testStatistics{i})\}}+ \1_{\{\testStatistics{j}' = \testStatistics{i}, \cdfTestStatistics{j}(\testStatistics{j}') = \cdfTestStatistics{i}(\testStatistics{i}), \sigma(j) < \sigma(i)\}}\\
%     &\le \1_{\{\testStatistics{j}' < \testStatistics{i}\}}
%     + \1_{\{\testStatistics{j}' = \testStatistics{i}, \sigma(j) < \sigma(i)\}},
% \end{align*}
% and the above proof of the validity holds.
\end{proof}

% \begin{proof}[Proof of \Cref{prop:pvalue.validity}]
% \textbf{Step 1}. We prove the validity if $\cdfTestStatistics{i}(t) = \cdfTestStatistics{j}(t)$ for all $j \in \hypothesisIndex{\text{nc}}$ and $t \in \RR$.
% By condition~\ref{vali:assu:one.exchangeable.general}, $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is exchangeable.
% Since there are no ties among $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ a.s., then the rank of $\testStatistics{i}$ among $(\testStatistics{j})_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is uniformly distributed on $\{1, 2, \ldots, 1+\NoNc\}$. By Definition~\eqref{eq:pval},
% % \begin{align*}
% %     \PP\left(\pval{i} = \frac{1+k}{1+\NoNc}\right)
% %     = \PP\left(\sum_{j=1}^{\NoNc} \1_{\{\ncTestStatistics{j} \le \testStatistics{i}\}} = k\right)
% %     = \frac{1}{1+\NoNc}, \quad 0 \le k \le \NoNc.
% % \end{align*}
% for $0 \le t \le 1$,
% \begin{align*}
%     \PP(\pval{i} \le t)
%     &= \PP\left(\sum_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j} \le \testStatistics{i}\}}
%     \le t(1+\NoNc)\right) \\
%     &= \sum_{k=1}^{\lfloor t(1+\NoNc) \rfloor}\PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j}
%     \le \testStatistics{i}\}}
%     = k \right)
%     = \frac{\lfloor t(1+\NoNc)\rfloor}{1+\NoNc}
%     \le t.
% \end{align*}

% \textbf{Step 2}. We prove the validity if $\cdfTestStatistics{i}(t) \le \cdfTestStatistics{j}(t)$ for all $j \in \hypothesisIndex{\text{nc}}$ and $t \in \RR$.
% Since $\cdfTestStatistics{i}(t)$, $\cdfTestStatistics{j}(t)$, $j \in \hypothesisIndex{\text{nc}}$ are continuous, then
% % the inverses $\cdfTestStatistics{i}^{-1}$, $\ncCdfTestStatistics{j}^{-1}$ exist and
% \begin{align*}
%      \cdfTestStatistics{i}(\testStatistics{i})
%      \stackrel{d}{=}
%      \cdfTestStatistics{j}(\testStatistics{j}) \stackrel{d}{=} U[0,1].
% \end{align*}
% By condition~\ref{vali:assu:one.exchangeable.general} and step 1, the \nickname~p-value $\pval{i}'$ based on $(\cdfTestStatistics{j}(\testStatistics{j}))_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}}$ is valid.
% Define $\testStatistics{i}' = \max_{t} \{t: \cdfTestStatistics{i}(\testStatistics{i})  = \cdfTestStatistics{i}(t) \}$, then $\testStatistics{i}' = \testStatistics{i}$ a.s., and $t \le \testStatistics{i}'$ is equivalent to $\cdfTestStatistics{i}(t) \le \cdfTestStatistics{i}(\testStatistics{i}')$.
% By condition~\ref{vali:assu:null.nc.conservative.general},
% \begin{align*}
%     \PP(\pval{i} \le t)
%     &= \PP\left(\sum_{j \in \{i\} \cup \hypothesisIndex{\text{nc}}} \1_{\{\testStatistics{j} \le \testStatistics{i}'\}}
%     \le t(1+\NoNc)\right)
%     = \PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\cdfTestStatistics{i}(\testStatistics{j}) \le \cdfTestStatistics{i}(\testStatistics{i}')\}}
%     \le t(1+\NoNc)\right) \\
%     &\le \PP\left(\sum_{j \in \{i\} \cup\hypothesisIndex{\text{nc}}} \1_{\{\cdfTestStatistics{j}(\testStatistics{j}) \le \cdfTestStatistics{i}(\testStatistics{i}')\}}
%     \le t(1+\NoNc)\right)
%     =  \PP\left(\pval{i}' \le t\right)
%     \le t.
% \end{align*}
% \end{proof}




\begin{proof}[Proof of \Cref{prop:PRDS}]

Without loss of generality, we assume $\nullHypothesisIndex = \{1,
\dotsc, \NoNull\}$ and $\hypothesisIndex{\text{nc}} = \{\No+1, \dotsc,
\No+\NoNc\}$.
When $\NoNull > 0$, $\testStatistics{1}$ corresponds to a true null.
Under both conditions in \Cref{prop:PRDS}, $\cdfTestStatistics{i} =
\cdfTestStatistics{j}$ for all $i,j \in  \nullHypothesisIndex \cup
\hypothesisIndex{\text{nc}}$. Without loss of generality, we assume
the null distribution is $U[0,1]$ so $\cdfTestStatistics{i}(t) = t$
for all $0 \leq t \leq 1$ and $i \in  \nullHypothesisIndex \cup
\hypothesisIndex{\text{nc}}$; otherwise, we can replace all statistics
$\testStatistics{i}$ by $\cdfTestStatistics{1}(\testStatistics{i})$.
Since the \PRDS~property is invariant by co-monotone transformations,
with a slight abuse of notation, it suffices to prove the
\PRDS~property is satisfied by the unnormalized ranks
\begin{align*}
    \qval{i} := \sum_{j \in \hypothesisIndex{\text{nc}}}
  \1_{\{\testStatistics{j} \le \testStatistics{i}\}},~i \in \hypothesisIndex{}.
\end{align*}
% We use $\qvalAll(\boldsymbol{t})$ to denote the vector if $\testStatisticsAll = \boldsymbol{t}$.
Under both sets of conditions, $(\testStatistics{i})_{i \in \{1\} \cup \hypothesisIndex{\text{nc}}}$ is exchangeable.
Therefore, for any $0 \le k \le \NoNc - 1$, $\PP(\qval{1} = k) =
\PP(\qval{1} = k+1) = 1/(1+\NoNc)$. Then
it suffices to show
\begin{align*}
    \PP\left(\qvalAll \in \calD, ~\qval{1} = k\right) \le \PP\left(\qvalAll \in \calD, ~\qval{1} = k+1\right)
\end{align*}
for all fixed $0 \le k \le \NoNc - 1$ and increasing set $\calD
\subseteq \RR^{\No+\NoNc}$.

Let $\bm C = (C_1,\dotsc,C_{\NoNc}) = (T_{\No+1}, \dotsc,
T_{\No+\NoNc})$ denote all negative control statistics. Let $C_{(1)} <
\dotsb < C_{(\NoNc)}$ be the order statistics. Let
$\boldsymbol{C}_{-(k+1)}$ denote all negative control test
statistics excluding $C_{(k+1)}$. It suffices to show that
\begin{align} \label{eq:intermediate-step-1}
    \PP\left(\qvalAll \in \calD, ~\qval{1} = k \mid \boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}\right) \le \PP\left(\qvalAll \in \calD, ~\qval{1} =
  k+1 \mid \boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}\right)
\end{align}
for all $\bm c_{-(k+1)}$. As a convention, let $c_{(0)} = 0$ and
$c_{(\NoNc+1)} = 1$. Define the bi-variate function
\begin{align*}
    f(t, c) = \PP\left(\pvalAll \in \calD \mid \boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}, C_{(k+1)} = c, \testStatistics{1} = t
  \right) \quad \text{for}~c_{(k)} < c, t < c_{(k+2)}.
\end{align*}
Let $g(c,t)$ denote the density function of $(C_{(k+1)}, T_1)$ given
$\boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}$ and $c_{(k)} < T_1
< c_{(k+2)}$.
To prove \eqref{eq:intermediate-step-1}, it suffices to show that
\[
  \int_{c_{(k)}}^{c_{(k+2)}} \int_{c_{(k)}}^{c_{(k+2)}} f(t,c) g(t,c) \1_{\{t
    < c\}} \,dt\, dc \leq \int_{c_{(k)}}^{c_{(k+2)}}
  \int_{c_{(k)}}^{c_{(k+2)}} f(t,c) g(t,c) \1_{\{t > c\}} \,dt\,dc.
\]
This follows from the next three Lemmas.

\begin{lemma} \label{lemm:symmetric-g}
The function $g$ is symmetric, i.e.\ $g(t,c) = g(c,t)$ for all
$c_{(k)} < c < t < c_{(k+2)}$.
\end{lemma}

\begin{lemma} \label{lemm:symmetric-inequality}
  $f(c,t) \leq f(t,c)$ for all $c_{(k)} < c < t < c_{(k+2)}$.
\end{lemma}

\begin{lemma}\label{lemm:PRDS}
Let $h: [0,1]^2 \to \RR$ be a bi-variate function and $h(x,y) \leq
h(y,x)$ for all $0 < x < y < 1$. Then
\begin{align*}
  \int_0^1 \int_0^1 h(x,y) \1_{\{x < y\}} \, dx \, dy \leq \int_0^1
  \int_0^1 h(x,y) \1_{\{x > y\}} \, dx \, dy
\end{align*}
\end{lemma}

% This inequality follows from \Cref{lemm:PRDS} after this paragraph and
% two observations:
% , $f(t, c)$ is increasing in $t$.
% By the definition of $\pvalAll$, $f(t, c)$ is decreasing in $c$.
% By condition~\ref{PRDS:assu:null.nc.identical}, we can assume all test statistics are marginally $U[0,1]$.
% Further, conditional on $\boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}$ and the event that $k \le \pval{1} \le k+1$, by condition~\ref{PRDS:assu:all.nc.set.independent} and condition~\ref{PRDS:assu:nc}, $\testStatistics{1}$ and $C_{(k+1)}$ are independently uniformly distributed on $\left[c_{(k)}, c_{(k+2)}\right]$ (here we denote $c_{(0)} = 0$, $c_{(\NoNc+1)} = 1$).
% Denote the expectation regarding $\testStatistics{1}$ and $C_{(k+1)}$ with this distribution by $\tilde{\EE}$, then
% \begin{align*}
%     &\quad~\PP\left(\pvalAll \in \calD, \pval{1} = k \mid \boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}, k \le \pval{1} \le k+1 \right)\\
%     &= \PP\left(\pvalAll \in \calD, \testStatistics{1} < C_{(k+1)} \mid \boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}, k \le \pval{1} \le k+1 \right)\\
%     &= \tilde{\EE}\left[\PP\left(\pvalAll \in \calD \mid \boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)}, k \le \pval{1} \le k+1, C_{(k+1)}, \testStatistics{1}\right), \testStatistics{1} < C_{(k+1)}\right]\\
%     &= \tilde{\EE}\left[f\left(\testStatistics{1}, C_{(k+1)}\right), \testStatistics{1} < C_{(k+1)}\right].
% \end{align*}
% Similarly, $\PP\left(\pvalAll \in \calD,  \pval{1} = k+1 \mid \boldsymbol{C}_{-(k+1)} = \boldsymbol{c}_{-(k+1)},  k \le \pval{1} \le k+1\right) = \tilde{\EE}\left[f(\testStatistics{1}, C_{(k+1)}), \testStatistics{1} > C_{(k+1)}\right]$.
% By \Cref{lemm:PRDS}, $\tilde{\EE}\left[f(\testStatistics{1}, C_{(k+1)}), \testStatistics{1} < C_{(k+1)}\right] \le \tilde{\EE}\left[f(\testStatistics{1}, C_{(k+1)}), \testStatistics{1} > C_{(k+1)}\right]$.
% We use ${\EE}'$ to denote the expectation regarding $\boldsymbol{C}_{-(k+1)}$ with the distribution $\boldsymbol{C}_{-(k+1)} \mid k \le \pval{1} \le k+1$, then
% \begin{align*}
%    &\quad~\PP\left(\pvalAll \in \calD, \pval{1} = k \mid k \le \pval{1} \le k+1 \right)\\
%    &= \EE'\left[\PP\left(\pvalAll \in \calD, \pval{1} = k \mid k \le \pval{1} \le k+1, \boldsymbol{C}_{-(k+1)} \right)\right]\\
%    &\le \EE'\left[\PP\left(\pvalAll \in \calD, \pval{1} = k+1 \mid k \le \pval{1} \le k+1, \boldsymbol{C}_{-(k+1)} \right)\right]\\
%    &=\PP\left(\pvalAll \in \calD, \pval{1} = k+1 \mid k \le \pval{1} \le k+1 \right).
% \end{align*}
% By $\PP\left(\pvalAll \in \calD, \pval{1} = k \mid k \le \pval{1} \le k+1 \right) = \PP\left(\pvalAll \in \calD, \pval{1} = k \right)/\PP(k \le \pval{1} \le k+1)$, we finish the proof.

\begin{proof}[Proof of \Cref{lemm:symmetric-g}]
  This is obviously true under the exchangeability condition in
  \ref{PRDS:assu:exchangeable}.
  For the first set of conditions, the
  independence conditions in \ref{PRDS:assu:all.nc.set.independent} and
  \ref{PRDS:assu:nc} imply that
\[
  \left(C_{(k+1)}, T_1\right) \mid \boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)} \sim U\left(\left[c_{(k)}, c_{(k+2)}\right]^2\right),
\]
where $U\left(\left[c_{(k)}, c_{(k+2)}\right]^2\right)$ is the uniform distribution over the
square $\left[c_{(k)}, c_{(k+2)}\right]^2$.
% For the second exchangeability condition in
% \ref{PRDS:assu:exchangeable}, let $\Sigma(\NoNc)$ denote the
% permutation group of $\{1,\dotsc,\NoNc\}$.
% \[
%   \PP(C_{(k+1)} = c, T_1 = t \mid \bm C_{-(k+1)} = \bm c_{-(k+1)}) =
%   \PP(C_{k+1} = c, T_1 = t \mid \bm
%   C_{l} = c_l, l \in \{1,\dotsc,\NoNc\} \setminus \{k+1\}).
% \]
\renewcommand{\qedsymbol}{}
\end{proof}


\begin{proof}[Proof of \Cref{lemm:symmetric-inequality}]
  Under the first set of conditions, we claim that $f(t,c)$ is
  decreasing in $c$ and increasing in $t$, so the desired conclusion
  follows. The first observation follows from the definition of $\bm
  p$ and the assumption that $\mathcal{D}$ is increasing. The second
  claim follows from the PRDS property in
  condition~\ref{PRDS:assu:PRDS}. To see this, given $\bm C = \bm c$,
  the event $\bm p \in \mathcal{D}$ can be rewritten as an event $\bm
  T \in \mathcal{D}'$ where $\mathcal{D}' = \mathcal{D}'(\bm c)$ is an
  increasing set that depends on $\bm c$. Using conditions
  \ref{PRDS:assu:all.nc.set.independent} and \ref{PRDS:assu:PRDS}, we
  have
\begin{align*}
  f(t,c)=&\PP\left(\pvalAll \in \calD \mid \boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}, C_{(k+1)} = c, \testStatistics{1} = t
  \right) \\
  =& \PP\left(\bm T \in \calD'(\bm c) \mid \boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}, C_{(k+1)} = c, \testStatistics{1} = t
     \right) \\
  =& \PP\left(\bm T \in \calD'(\bm c) \mid \testStatistics{1} = t
  \right)
\end{align*}
is increasing in $t$.
Now consider the exchangeability condition
\ref{PRDS:assu:exchangeable}, which implies that $(T_2,\dotsc,T_{\No})$ has
the same conditional distribution given
\[
\boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}, C_{(k+1)} = c, \testStatistics{1} = t,
\]
and given
\[
\boldsymbol{C}_{-(k+1)}
  = \boldsymbol{c}_{-(k+1)}, C_{(k+1)} = t, \testStatistics{1} = c,
\]
for all $c_{(k)} < c, t < c_{(k+2)}$. The conclusion then follows from
the fact that the \nickname~p-values $\bm p$ only become smaller when
we swap $C_{(k+1)} = c$ with $T_1 = t$ if $c < t$ and the assumption
that $\mathcal{D}$ is increasing.
\renewcommand{\qedsymbol}{}
\end{proof}

\begin{proof}[Proof of \Cref{lemm:PRDS}]
  The conclusion follows from rewritting one of the
  integrals as follows,
  \begin{align*}
    &\int_0^1 \int_0^1 h(x,y) \1_{\{x < y\}} \, dx \, dy - \int_0^1
      \int_0^1 h(x,y) \1_{\{x > y\}} \, dx \, dy \\
    =& \int_0^1 \int_0^1 h(x,y) \1_{\{x < y\}} \, dx \, dy - \int_0^1
       \int_0^1 h(y,x) \1_{\{y > x\}} \, dx \, dy \\
    =& \int_0^1 \int_0^1 (h(x,y) -h(y,x)) \1_{\{x < y\}} \, dx \, dy
    \\
    \leq& 0.
  \end{align*}
\renewcommand{\qedsymbol}{}
\end{proof}
\vspace{-2em}
\end{proof}

% \textbf{Second set of conditions}.
% Define $\calD'_k := \{\testStatisticsAll: \qvalAll(\testStatisticsAll) \in \calD,  ~\qval{1} = k\}$.
% Define the mapping $\phi: \RR^{\No + \NoNc} \to \RR^{\No + \NoNc}$ that swaps $t_{1}$ and $c_{(k)}$,
% \begin{align*}
%     \phi_i(\boldsymbol{t}) =
%     \begin{cases}
%     c_{(k)}, & i = 1, \\
%     t_i, & 2 \le i \le \No,\\
%     t_1, & i > \No,~ t_{i} = c_{(k)}, \\
%     t_i, & i > \No,~ t_{i} \neq c_{(k)}.
%     \end{cases}
% \end{align*}
% For $\boldsymbol{t} \in \calD'_k$, since $c_{(k-1)} < t_1 < c_{(k)}$, $\phi$ does not change the order of  $\boldsymbol{c}$, $\phi \circ \phi$ is the identity function, and $\qval{1}(\phi(\boldsymbol{t})) = k+1$.
% Moreover, $\boldsymbol{t}_{\hypothesisIndex{}} \succeq  \phi_{\hypothesisIndex{}}(\boldsymbol{t})$, $\phi_{\hypothesisIndex{\text{nc}}}(\boldsymbol{t}) \preceq  \phi_{\hypothesisIndex{\text{nc}}}(\boldsymbol{t})$, and
% $\qvalAll(\phi(\testStatisticsAll)) \succeq \qvalAll(\testStatisticsAll)$.
% Then for any $\boldsymbol{t} \in \calD'_k$, we have $\qvalAll(\phi(\boldsymbol{t})) \in \calD$ and thus $\phi(\boldsymbol{t}) \in \calD_{k+1}'$.
% Let $\testStatisticsAll_{i \leftrightarrow j}$, $1 \le i \le \No$, $1 \le j \le \NoNc$ be swapping $\testStatistics{i}$ and $\ncTestStatistics{j}$.
% By condition~\ref{PRDS:assu:exchangeable},
% \begin{align*}
%     &\quad~\PP(\qvalAll \in \calD, ~\qval{1} = k)
%     = \PP(\testStatisticsAll \in \calD'_k)
%     =  \sum_{j \in \hypothesisIndex{\text{nc}}} \PP\left(\testStatisticsAll \in \calD'_k \cap \{\boldsymbol{t}: c_{j} = c_{(k)} \}\right)\\
%     &= \sum_{j \in \hypothesisIndex{\text{nc}}} \PP\left(\testStatisticsAll_{1 \leftrightarrow j} \in \phi(\calD'_k \cap \{\boldsymbol{t}: c_{j} = c_{(k)} \})\right)
%     = \sum_{j \in \hypothesisIndex{\text{nc}}} \PP\left(\testStatisticsAll \in \phi(\calD'_k \cap \{\boldsymbol{t}: c_{j} = c_{(k)} \})\right)\\
%     &\le \sum_{j \in \hypothesisIndex{\text{nc}}} \PP\left(\testStatisticsAll \in \calD'_{k+1} \cap \{\boldsymbol{t}: c_{j} = c_{(k)} \}\right)
%     = \PP(\testStatisticsAll \in \calD'_{k+1})
%     = \PP(\qvalAll \in \calD, ~\qval{1} = k+1).
% \end{align*}

% If we break ties randomly, the proof for the second set of conditions holds if we
% define $\phi_i$ as swapping both the value of the test statistic and the permuted index.
% However, the proof of the first set of conditions relies on $\cdfTestStatistics{j}(\testStatistics{j}) \stackrel{d}{=} U[0,1]$ and does not straightforwardly extend to non-continuous CDF.  % In fact, if $t_1 = c_{(k)}$, then $\sigma(1) > \sigma(j_k)$ where $j_k$ is the index of the $k$-th smallest test statistics after breaking the ties.



\begin{proposition}\label{prop:permutation}
    If the permutation test statistic $\permutationFunction{\cdot}$ satisfies
    \begin{enumerate}
        \item \label{prop:assu:monotone.invariance}$\permutationFunction{\testStatisticsAll} = \permutationFunction{g(\testStatisticsAll)}$ for arbitrary strictly increasing function $g(\cdot): \RR \to \RR$,  $g(\testStatisticsAll) = (g(\testStatistics{i}))_{i \in  \hypothesisIndex{} \cup \hypothesisIndex{\text{nc}}}$;
        \item \label{prop:assu:permutation.invariance} $\permutationFunction{\testStatisticsAll} = \permutationFunction{\testStatisticsAll_{P}}$, where $\testStatisticsAll_{P}$ is a permutation of $\testStatisticsAll$ where $(\testStatistics{i})_{i \in  \hypothesisIndex{} }$ are not exchanged with $(\testStatistics{j})_{j \in \hypothesisIndex{\text{nc}}}$,
    \end{enumerate}
    then
    $\permutationFunction{\testStatisticsAll}$ is a function of the set of \nickname~p-values $\{\pval{i}\}_{i \in \hypothesisIndex{}}$.
\end{proposition}
% \noindent The proof of \Cref{prop:permutation} relies on two facts: (1) the mapping from $\testStatisticsAll$ to its rank vector $R(\testStatisticsAll)$ is strictly increasing;
% (2) for $\testStatisticsAll$ and $\testStatisticsAll'$, if $\{\pval{i}\}_{i \in \hypothesisIndex{}} = \{\pval{i}'\}_{i \in \hypothesisIndex{}}$, then we can always permute $(\testStatistics{i})_{i \in \hypothesisIndex{}}$, $(\testStatistics{i})_{i \in \hypothesisIndex{\text{nc}}}$ separately such that $R(\testStatisticsAll_{P}) = R(\testStatisticsAll')$.

\begin{proof}[Proof of \Cref{prop:permutation}]
The mapping from $\testStatisticsAll$ with no ties to its rank $R(\testStatisticsAll)$ is a strictly increasing function.
% Here $\symmetricGroup{[\No + \NoNc]}$ denotes the symmetric group of $[\No + \NoNc]$ consisting of all permutations of $[\No + \NoNc]$.
Since $\permutationFunction{\testStatisticsAll} = \permutationFunction{g(\testStatisticsAll)}$ for arbitrary strictly increasing function $g(\cdot)$,
% we let $g(\cdot) = R(\cdot)$ and
then $\permutationFunction{\testStatisticsAll} = \permutationFunction{R(\testStatisticsAll)}$.
% i.e., $\permutationFunction{\cdot}$ depends on $[\testStatistics{}, \ncTestStatistics{}]$ only through their ranks.
For $\testStatisticsAll$ and $\testStatisticsAll'$, we can always permute $(\testStatistics{i})_{i \in  \hypothesisIndex{}}$ so that the ordering within $(\testStatistics{i})_{i \in  \hypothesisIndex{} }$ equals that of $(\testStatistics{i}')_{i \in  \hypothesisIndex{} }$. Similarly for $(\testStatistics{i})_{i \in  \hypothesisIndex{\text{nc}} }$.
Denote the permuted test statistics by $\testStatisticsAll_P$, and then $\{\pval{i}\}_{i \in \hypothesisIndex{}} = \{\pval{P,i}\}_{i \in \hypothesisIndex{}}$.
Since $\permutationFunction{\cdot}$ is invariant to permutations within $(\testStatistics{i})_{i \in  \hypothesisIndex{} }$ and $(\testStatistics{i})_{i \in  \hypothesisIndex{\text{nc}} }$, then $\permutationFunction{\testStatisticsAll} = \permutationFunction{\testStatisticsAll_P}$.
Note that $\{\pval{i}\}_{i \in \hypothesisIndex{}} = \{\pval{P,i}\}_{i \in \hypothesisIndex{}} = \{\pval{i}'\}_{i \in \hypothesisIndex{}}$ implies $R(\testStatisticsAll_P) = R(\testStatisticsAll')$, and further $\permutationFunction{\testStatisticsAll}
= \permutationFunction{\testStatisticsAll_P}
= \permutationFunction{R(\testStatisticsAll_P)}
= \permutationFunction{R(\testStatisticsAll')}
= \permutationFunction{\testStatisticsAll'}$.
Therefore,   $\permutationFunction{\testStatisticsAll}$ is a function of $\{\pval{i}\}_{i \in \hypothesisIndex{}}$.
\end{proof}

\subsection{Results in \Cref{sec:empirical.process}}\label{sec:results-empirical-process}

The next proposition suggests that $\widehat{\FDR}_{\lambda}(t)$ is a
conservative estimate of $\FDR(t)$. The proposition is valid for replacing the $+2$ by $+1$ in the numerator of $\widehat{\FDR}_{\lambda}(t)$.
\begin{proposition}\label{prop:FDR.estimate}
    Assume $(\testStatistics{i})_{i \in \hypothesisIndex{} \cup \hypothesisIndex{\text{nc}}}$ are independent and $\testStatistics{i} \stackrel{d}{=} \ncTestStatistics{j}$ for $i \in \nullHypothesisIndex$, $j \in \hypothesisIndex{\text{nc}}$, then
    $\EE\left[\widehat{\FDR}_{\lambda}(t) \right] \ge \FDR(t)$ when
    $\lambda = 1$.
\end{proposition}
\noindent


\begin{proof}[Proof of \Cref{prop:FDR.estimate}]

For $0 < \lambda < 1$, $\PP(V_{\text{nc}}(\lambda) = \NoNc) > 0$ and $\EE\left[\widehat{\FDR}_\lambda (t)\right] = \infty > \FDR(t)$.
Thus, it is left to show the inequality is true for $\lambda = 1$. For simplicity, we write $\widehat{\FDR}_1 (t)$ as $\widehat{\FDR} (t)$.
We denote the distribution of true null and negative control test statistics by $\cdfTestStatisticsNull(t)$.
Since $V_{\text{nc}}(t) \le \NoNc$,
\begin{align}\label{proof:eq:cdf.estimate}
    \EE\left[\frac{V_{\text{nc}}(t)+1}{\NoNc+1}\right]  \ge \EE\left[\frac{V_{\text{nc}}(t)}{\NoNc}\right] = \cdfTestStatisticsNull(t).
\end{align}
The result in \cite[Theorem 1]{storey2004strong} of p-values following $U[0,1]$ can extend to test statistics of arbitrary distribution with continuous CDF,
\begin{align}\label{proof:eq:FDR.estimate}
     \EE\left[\frac{\NoNull \cdfTestStatisticsNull(t) - V(t)}{R(t) \vee 1} \right] \ge 0.
\end{align}
Since the negative control test statistics are independent of the test statistics under investigation and $\No \ge \NoNull$,
% we plug Eq.~\eqref{proof:eq:cdf.estimate} into Eq.~\eqref{proof:eq:FDR.estimate}
\begin{align*}
    \EE\left[\widehat{\FDR}(t)\right]
    &=\EE\left[\EE\left[\widehat{\FDR}(t) \mid R(t)\right] \right]
    =\EE\left[\frac{\No \EE\left[\frac{V_{\text{nc}}(t)+1}{\NoNc+1} \mid  R(t)\right]}{R(t) \vee 1}  \right] \\
    &\ge \EE\left[\frac{\No \cdfTestStatisticsNull(t)}{R(t) \vee 1} \right]
    \ge \EE\left[\frac{V(t)}{R(t) \vee 1} \right] = \FDR(t).
\end{align*}
\end{proof}



\begin{proof}[Proof of \Cref{prop:FDR.equivalence}]

Let $\testStatistics{q}'$, $\testStatistics{q}$ be the largest test statistic under investigation rejected by applying the \BH~procedure to the \nickname~p-values and the empirical-process-based step-up procedure, respectively.
It is straightforward to show the event that $\testStatistics{q}'$ does not exist is equivalent to  the event that $\testStatistics{q}$ does not exist.
It is left to show the equivalence holds when both $\testStatistics{q}'$ and $\testStatistics{q}$ are well-defined.
On one hand,
\begin{align*}
    % \pval{i} =
    \frac{V_{\text{nc}}(\testStatistics{q}')+2}{\NoNc+1}  \wedge 1 \le \frac{R(\testStatistics{q}')}{\No} q
    \implies
    \frac{\No\frac{V_{\text{nc}}(\testStatistics{q}')+2}{\NoNc+1}}{R(\testStatistics{q}') \vee 1} \le q
    \implies \stoppingTime \ge \testStatistics{q}',
\end{align*}
which further implies $\testStatistics{q} \ge \testStatistics{q}'$ by the definition of $\testStatistics{q}$.
On the other hand, by the discussion before \Cref{prop:FDR.equivalence},
% $T_q$ is a negative control test statistics, and
there exists $\varepsilon > 0$ such that $\stoppingTime - \varepsilon > \testStatistics{q}$, $V_{\text{nc}}(\stoppingTime - \varepsilon) \ge V_{\text{nc}}(\testStatistics{q})$,
$R(\stoppingTime - \varepsilon) = R(\testStatistics{q})$,
and $\widehat{\FDR}(\stoppingTime - \varepsilon) \le q$.
Then,
\begin{align*}
    \widehat{\FDR}(\testStatistics{q})
    = \frac{\No \cdot \frac{V_{\text{nc}}(\testStatistics{q})+2}{\NoNc+1} }{R(\testStatistics{q}) \vee 1}
    % \le \frac{\No \cdot \frac{V_{\text{nc}}(T_q - \varepsilon)+1}{\NoNc+1} }{R(\testStatistics{q}) \vee 1}
    \le \frac{\No \cdot \frac{V_{\text{nc}}(\stoppingTime - \varepsilon)+2}{\NoNc+1} }{R(\stoppingTime - \varepsilon) \vee 1}
    = \widehat{\FDR}(\stoppingTime - \varepsilon)
    \le q.
\end{align*}
Therefore,  $\testStatistics{q} \le \testStatistics{q}'$.

\end{proof}



\begin{proof}[Proof of \Cref{prop:FDR}]
% backward martingale intro: https://www.randomservices.org/random/martingales/Backwards.html
Without loss of generality, we assume the test statistics are supported on $(0,1)$.
Otherwise, we can always apply the transformation $t \to \frac{e^t}{1 + e^t}$.
Define a decreasing family of $\sigma$-algebras $\calF_t = \sigma(\falsePositive(s), \truePositive(s), \ncFalsePositive(s), ~t \le s \le 1)$ for $0 \le t \le 1$.
% Define the random process
% \begin{align*}
%     M(t) = \frac{\falsePositive(t)}{\ncFalsePositive(t) + 1},\quad {\FDP}(t) = \frac{\falsePositive(t)}{(\falsePositive(t) + \truePositive(t)) \vee 1}.
% \end{align*}
By definition, $M(t)$ in Eq.~\eqref{eq:super.martingale} is measurable with respect to $\calF_t$, $0 \le M(t) \le (\NoNc+1)\NoNull$, $M(0) = 0$, and $M(1) = \NoNull$.


We show $M(t)$ is a backward super-martingale with respect to $\{\calF_t\}$.
For $0 \le s < t$, by condition~\ref{FDR:assu:independent}, $(\falsePositive(s), \falsePositive(t))$, $(\falsePositive_{\text{nc}}(s), \falsePositive_{\text{nc}}(t))$, and $(\truePositive(s), \truePositive(t))$ are mutually independent.
On $\left\{\falsePositive(t) = ck, \ncFalsePositive(t) + 1 = c\right\}$ with non-zero probability,
% we aim to prove
% \begin{align}\label{proof:eq:mtg1}
%     \EE\left[ \frac{V(s)}{V_{\text{nc}}(s) + 1} \mid V(t) = ck, ~V_{\text{nc}}(t) + 1 = c \right]
%     \le k.
% \end{align}
% In fact,
\begin{align}\label{proof:eq:mtg2}
\begin{split}
    \EE\left[ \frac{\falsePositive(s)}{\ncFalsePositive(s) + 1} \mid \calF_t \right]
    &= \EE\left[ \frac{\falsePositive(s)}{\ncFalsePositive(s) + 1} \mid \falsePositive(t) = ck, ~\ncFalsePositive(t) + 1 = c \right]\\
    &=  \underbrace{\EE\left[V(s) \mid V(t) = ck\right]}_{:=\text{(I)}} \cdot \underbrace{\EE\left[\frac{1}{\ncFalsePositive(s) + 1} \mid \ncFalsePositive(t) + 1 = c \right]}_{:=\text{(II)}}.
\end{split}
\end{align}
By condition~\ref{FDR:assu:null.nc.uniformly.conservative}, there exists $0 \le p_{t}^s \le 1$ such that
\begin{align}\label{proof:eq:prob}
    \max_{i \in \nullHypothesisIndex }\frac{\cdfTestStatistics{i}(s)}{\cdfTestStatistics{i}(t)}
    \le  p_{t}^s
    \le \min_{j \in \hypothesisIndex{\text{nc}} }\frac{\cdfTestStatistics{j}(s)}{\cdfTestStatistics{j}(t)}.
\end{align}
By condition~\ref{FDR:assu:independent}, and Eq.~\eqref{proof:eq:prob},
% \zg{To check},
\begin{align}\label{proof:eq:stochastic.dominance}
    \falsePositive(s) \mid \falsePositive(t) &\lesssim Z, \quad Z \sim \text{Binomial}(\falsePositive(t), p_t^s),\\ \label{proof:eq:stochastic.dominance.2}
    \frac{1}{1+\ncFalsePositive(s)} \mid \frac{1}{1+\ncFalsePositive(t)} &\lesssim \frac{1}{1+Z^{\text{nc}}}, \quad Z^{\text{nc}} \sim \text{Binomial}(\ncFalsePositive(t), p_t^s).
\end{align}
For term (I) in Eq.~\eqref{proof:eq:mtg2}, by Eq.~\eqref{proof:eq:stochastic.dominance},
\begin{align}\label{proof:eq:mtg(I)}
    \text{(I)}
    \le \falsePositive(t) \cdot p_t^s
    = ck p_t^s.
\end{align}
For term (II) in Eq.~\eqref{proof:eq:mtg2}, by Eq.~\eqref{proof:eq:stochastic.dominance.2},
\begin{align}\label{proof:eq:mtg(II)}
    \begin{split}
        \text{(II)}
        &\le \sum_{i=0}^{c-1} \frac{1}{i+1} \binom{c-1}{i} (p_t^s)^{i} (1-p_t^s)^{c-1-i}\\
        &= \frac{1}{cp_{t}^s}\sum_{i=0}^{c-1} \binom{c}{i+1} (p_t^s)^{i+1} (1-p_t^s)^{c-1-i}
        =\frac{1}{c p_{t}^s} \left(1 - (1-p_t^s)^{c} \right).
    \end{split}
\end{align}
Plug Eq.~\eqref{proof:eq:mtg(I)}, Eq.~\eqref{proof:eq:mtg(II)} into Eq.~\eqref{proof:eq:mtg2},
\begin{align*}
    \EE\left[ \frac{\falsePositive(s)}{\ncFalsePositive(s) + 1} \mid \calF_t \right]
    \le ck p_t^s \cdot \frac{1}{c p_{t}^s} \left(1 - (1-p_t^s)^{c} \right)
    \le k
    = \frac{\falsePositive(t)}{\ncFalsePositive(t) + 1}.
\end{align*}


Next, we show $\stoppingTimeLambda$ is a stopping time with respect to $\{\calF_t\}$.
By definition, if $\lambda = 1$, then for $0 \le t \le 1$,
\begin{align*}
    \{\stoppingTime > t\}
    = \left\{\sup\left\{s \ge 0: \frac{\frac{\No}{\NoNc+1}(\ncFalsePositive(s) + 2)}{(\falsePositive(s) + \truePositive(s)) \vee 1} \le q \right\} > t \right\},
\end{align*}
is $\calF_t$ measurable.
If $0 < \lambda < 1$, for $t \ge \lambda$, $\{\stoppingTimeLambda > t\} = \emptyset$; for $t < \lambda$, $\hat{\pi}(\lambda)$ is $\calF_t$ measurable.
Thus, $\stoppingTimeLambda$ is a stopping time regarding $\{\calF_t\}$.


Since $M(t)$ is bounded and thus uniformly integrable, we can apply the optional stopping time theorem
% By the upcrossing inequality \zg{add reference}, $\lim_{t \to \infty} M(t)$ exists a.s.. Since $M(t)$ is u.i., it also converges in $L_1$.
% (Theorem 4.8.2 of Durrett)
 \begin{align}\label{proof:eq:super.martingale}
    \EE\left[M\left(\stoppingTimeLambda\right) \mid \calF_1\right]
    \le M(1)
    = \NoNull.
\end{align}

We next show
\begin{align}\label{proof:eq:compare1}
    \frac{\falsePositive(\stoppingTimeLambda)}{(\falsePositive(\stoppingTimeLambda) + \truePositive(\stoppingTimeLambda)) \vee 1}
    \le \frac{q}{\hat{\pi}(\lambda)} \cdot \frac{\NoNc+1}{\No} \cdot \frac{\falsePositive(\stoppingTimeLambda)}{\ncFalsePositive(\stoppingTimeLambda) + 1}.
    % (V(\stoppingTimeLambda) + S(\stoppingTimeLambda)) \vee 1 \ge \frac{\hat{\pi}(\lambda)(V_{\text{nc}}(\stoppingTimeLambda) + 1)}{q}
\end{align}
It suffices to discuss the case where $\stoppingTimeLambda > 0$.
Let $\calS_q = \left\{0 \le t \le \lambda: \frac{\hat{\pi}(\lambda)\No\frac{V_{\text{nc}}(t) + 2}{\NoNc+1}}{(V(t) + S(t)) \vee 1} \le q \right\}$, then $\stoppingTimeLambda = \sup \calS_q$.
If $\stoppingTimeLambda \in \calS_q$, then Eq.~\eqref{proof:eq:compare1} is obviously true.
If $\stoppingTimeLambda \notin \calS_q$, then there exists a sequence $(t_k) \subseteq \calS_q$ such that $t_k \to \stoppingTimeLambda$ as $k \to \infty$, and $\falsePositive(t_k) + \truePositive(t_k) \le \falsePositive(\stoppingTimeLambda) + \truePositive(\stoppingTimeLambda)$.
Since a Poisson process increases at most by one at a time, then $\lim_{k\to \infty}\ncFalsePositive(t_k) + 2 \ge \ncFalsePositive(\stoppingTimeLambda) + 1$. Therefore,
\begin{align*}
 q \cdot \frac{\NoNc+1}{\No} &\ge \lim_{k \to \infty}\frac{\hat{\pi}(\lambda)(\ncFalsePositive(t_k) + 2)}{(\falsePositive(t_k) + \truePositive(t_k)) \vee 1}
 \ge \frac{\hat{\pi}(\lambda)
 \left(\ncFalsePositive\left(\stoppingTimeLambda\right) + 1\right)}{\left(\falsePositive\left(\stoppingTimeLambda\right) + \truePositive\left(\stoppingTimeLambda\right)\right) \vee 1}.
\end{align*}


Finally, we compute the \FDR. By Eq.~\eqref{proof:eq:compare1},
\begin{align*}
    \FDR
    % = \EE\left[\widehat{\FDP}(\stoppingTimeLambda) \mid \calF_1\right]
    = \EE\left[\frac{\falsePositive(\stoppingTimeLambda)}{(\falsePositive(\stoppingTimeLambda) + \truePositive(\stoppingTimeLambda)) \vee 1} \mid \calF_1 \right]
    \le q \cdot \frac{\NoNc+1}{\No} \cdot \EE\left[\frac{1}{\hat{\pi}(\lambda)} \cdot \frac{\falsePositive(\stoppingTimeLambda)}{\ncFalsePositive(\stoppingTimeLambda) + 1} \mid \calF_1 \right].
\end{align*}
By the tower property and Eq.~\eqref{proof:eq:super.martingale},
\begin{align*}
    \EE\left[\frac{1}{\hat{\pi}(\lambda)} \cdot \frac{\falsePositive(\stoppingTimeLambda)}{\ncFalsePositive(\stoppingTimeLambda) + 1} \mid \calF_1 \right]
    = \EE\left[\frac{1}{\hat{\pi}(\lambda)} \cdot \EE\left[\frac{\falsePositive(\stoppingTimeLambda)}{\ncFalsePositive(\stoppingTimeLambda) + 1} \mid \calF_\lambda\right]\mid \calF_1 \right]
    \le \EE\left[\frac{1}{\hat{\pi}(\lambda)} \cdot \frac{\falsePositive(\lambda)}{\ncFalsePositive(\lambda) + 1} \mid \calF_1 \right].
\end{align*}
If $\lambda = 1$, then
\begin{align*}
    \FDR \le q \cdot \frac{\NoNc + 1}{\No} \cdot \frac{\NoNull}{\NoNc + 1} \le q.
\end{align*}
If $0 < \lambda < 1$, we plug in the definition of $\hat{\pi}(\lambda)$ and by condition~\ref{FDR:assu:independent},
    \begin{align*}
        % q \cdot \EE\left[\frac{1}{\hat{\pi}} \cdot \frac{\falsePositive(\lambda)}{\ncFalsePositive(\lambda) + 1} \mid \calF_1 \right]
        \FDR
        &\le q \cdot \EE\left[\frac{\NoNc - \ncFalsePositive(\lambda)}{\No - \truePositive(\lambda) - \falsePositive(\lambda) + 1} \cdot \frac{\falsePositive(\lambda)}{\ncFalsePositive(\lambda) + 1} \mid \calF_1 \right]\\
        &= q \cdot \EE\left[\frac{\NoNc - \ncFalsePositive(\lambda)}{\ncFalsePositive(\lambda) + 1} \mid \calF_1 \right] \cdot \EE\left[\frac{\falsePositive(\lambda)}{\No - \truePositive(\lambda) - \falsePositive(\lambda) + 1} \mid \calF_1 \right].
    \end{align*}
    By a similar argument of Eq.~\eqref{proof:eq:mtg(II)},
%     Since $V_{\text{nc}}(\lambda) \mid m_{\text{nc}} \sim \text{Binomial}(m_{\text{nc}}, p_\infty^\lambda)$,
% \begin{align*}
%     \EE\left[\frac{m_{\text{nc}} - V_{\text{nc}}(\lambda)}{V_{\text{nc}}(\lambda) + 1} \mid m_{\text{nc}} \right]
%     &= \sum_{i=0}^{m_{\text{nc}}} \frac{m_{\text{nc}}-i}{i+1} \binom{m_{\text{nc}}}{i} (p_\infty^\lambda)^{i} (1-p_\infty^\lambda)^{m_{\text{nc}}-i}\\
%         &= \frac{1-p_\infty^\lambda}{p_{\infty}^\lambda}\sum_{i=0}^{m_{\text{nc}}-1}  \binom{m_{\text{nc}}}{i+1} (p_\infty^\lambda)^{i+1} (1-p_\infty^\lambda)^{m_{\text{nc}}-1-i}\\
%         &=\frac{1-p_\infty^\lambda}{ p_\infty^\lambda} \left(1 - (1-p_\infty^\lambda)^{m_{\text{nc}}} \right).
% \end{align*}
% Since $m - S(\lambda) - V(\lambda) \le m_0 - V(\lambda)$,
% \begin{align*}
%     \EE\left[\frac{V(\lambda)}{m - S(\lambda) - V(\lambda) + 1} \mid m_1, m_0 \right] \le \EE\left[\frac{V(\lambda)}{m_0 - V(\lambda) + 1} \mid m_0 \right]
%     = \frac{ p_\infty^\lambda}{1-p_\infty^\lambda} \left(1 - (p_\infty^\lambda)^{m_0} \right).
% \end{align*}
    \begin{align*}
        % \EE\left[\frac{1}{\hat{\pi}} \cdot \frac{V(\lambda)}{V_{\text{nc}}(\lambda) + 1} \mid \calF_\infty \right]
        \FDR
        \le q \cdot \frac{1-p_1^\lambda}{p_1^\lambda} (1 - (1 - p_1^\lambda)^{\NoNc}) \cdot \frac{p_1^\lambda}{1-p_1^\lambda} (1 - (p_1^\lambda)^{\NoNull})
        \le q.
    \end{align*}
    % Finally, by the tower property,
    % \begin{align*}
    %     \EE[r(\stoppingTimeLambda) \mid m, m_{\text{nc}}]
    %     =\EE[\EE[r(\stoppingTimeLambda) \mid \calF_\infty] \mid m, m_{\text{nc}}]
    %     \le q.

\end{proof}

\begin{remark}
In the proof of \Cref{prop:FDR}, it may be tempting to use the alternative process
\[
  \frac{\falsePositive(t)}{(\ncFalsePositive(t) \vee 1)/\NoNc}
\]
as $M(t)$. However, this is not a super-martingale.
\end{remark}

\subsection{Results in
  \Cref{sec:localFDR}}\label{sec:results-localFDR}

\begin{proposition}\label{prop:convergence.rate.weak}
For any $\cdfTestStatistics{}$, $\cdfTestStatisticsNull$, and $0 < \varepsilon < 1$,
\begin{align*}
    \PP\left(\BayesRiskLambda(\EmpiricalThreshold) - \BayesRiskLambda(\trueThreshold) > 2(1+\lambda)\sqrt{\frac{\log(4/\varepsilon)}{2 (\NoNc \wedge \No)}} \right)
    \le \varepsilon.
\end{align*}
\end{proposition}

\begin{proof}[Proof of \Cref{prop:convergence.rate.weak}]
    By the triangle inequality and the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality \cite{dvoretzky1956asymptotic, massart1990tight}, for any $\varepsilon' > 0$,
    \begin{align}\label{proof:eq:weak.1}
    \begin{split}
        &\quad~\PP\left(\sup_{\threshold \in \RR}\left|
        \EmpiricalBayesRiskLambda(\threshold)
        - \BayesRiskLambda(\threshold)\right| > (1+\lambda) \varepsilon' \right)\\
        &\le \PP\left(\sup_{\threshold \in \RR}\left|
        \cdfTestStatisticsNull(\threshold)
        - \EmpiricalCdfTestStatisticsNull(\threshold)\right| > \varepsilon' \right) +  \PP\left(\sup_{\threshold \in \RR}\left|
        \lambda\cdfTestStatistics{}(\threshold)
        - \lambda\EmpiricalCdfTestStatistics(\threshold)\right| > \lambda \varepsilon' \right)\\
        &\le 2 e^{-2 \NoNc\varepsilon'^2} + 2 e^{-2\No\varepsilon'^2}
        \le  4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}.
        \end{split}
    \end{align}
    Since $\EmpiricalThreshold$ minimizes $\EmpiricalBayesRiskLambda$,
    \begin{align}\label{proof:eq:weak.2}
    \begin{split}
        &\quad~\BayesRiskLambda(\EmpiricalThreshold) - \BayesRiskLambda(\trueThreshold) \\
        &= \left(\BayesRiskLambda(\EmpiricalThreshold) - \EmpiricalBayesRiskLambda(\EmpiricalThreshold)\right) +
        \left(\EmpiricalBayesRiskLambda(\EmpiricalThreshold) -  \EmpiricalBayesRiskLambda(\trueThreshold)\right) +  \left(\EmpiricalBayesRiskLambda(\trueThreshold) -  \BayesRiskLambda(\trueThreshold)\right)  \\
        &\le 2 \sup_{\threshold \in \RR}\left|
        \EmpiricalBayesRiskLambda(\threshold)
        - \BayesRiskLambda(\threshold)\right|.
    \end{split}
    \end{align}
    Therefore, we combine Eq.~\eqref{proof:eq:weak.1}, Eq.~\eqref{proof:eq:weak.2}, and take $\varepsilon' = \sqrt{\frac{\log(4/\varepsilon)}{2 (\NoNc \wedge \No)}}$,
    \begin{align*}
        &\quad~\PP\left(\BayesRiskLambda(\EmpiricalThreshold) - \BayesRiskLambda(\trueThreshold) > 2 (1+\lambda)\sqrt{\frac{\log(4/\varepsilon)}{2 \NoNc \wedge \No}} \right) \\
        &\le \PP\left(\sup_{\threshold \in \RR}\left|
        \EmpiricalBayesRiskLambda(\threshold)
        - \BayesRiskLambda(\threshold)\right| > (1+\lambda) \sqrt{\frac{\log(4/\varepsilon)}{2 (\NoNc \wedge \No)}}  \right)
        = \varepsilon.
    \end{align*}
\end{proof}

\begin{proposition}\label{prop:convergence.rate}
   Consider the setting in \Cref{sec:localFDR} and assume the following assumptions hold:
    \begin{enumerate}
        \item \label{prop:assu:density.derivative} $\pdfTestStatistics{}(\trueThreshold)$, $\pdfTestStatisticsNull(\trueThreshold) > 0$ and $\pdfTestStatistics{}'(\trueThreshold)$, $\pdfTestStatisticsNull'(\trueThreshold)$ exist;
        \item \label{prop:assu:density.upper.bound}
         $\pdfTestStatistics{}(\threshold) \le \pdfTestStatistics{\max}$, $\pdfTestStatisticsNull(\threshold) \le \pdfTestStatistics{0,\max}$, in $|\threshold - \trueThreshold| \le D$ for some $\pdfTestStatistics{\max}$, $\pdfTestStatistics{0,\max}$, $D > 0$;
        \item \label{prop:assu:MLR} the derivative $(\pdfTestStatisticsNull/\pdfTestStatistics{})'(\trueThreshold)$ is positive;
        \item \label{prop:assu:Bayes.risk} for any $\delta > 0$, there exists $\varepsilon_\delta > 0$ such that $\cdfTestStatisticsNull(\threshold) - \lambda \cdfTestStatistics{}(\threshold) > \cdfTestStatisticsNull(\trueThreshold) - \lambda \cdfTestStatistics{}(\trueThreshold) + \varepsilon_\delta$ for $|\threshold - \trueThreshold| > \delta$.
    \end{enumerate}
    Then there exists a constant $C^*>0$ such that with
    probability at least $1 - C^*/\log(\No \wedge \NoNc)$,
    \begin{align}\label{eq:prop:convergence.rate}
        \left|\EmpiricalThreshold - \trueThreshold\right|
        \le \frac{\log(\No \wedge \NoNc)}{(\No \wedge
      \NoNc)^{1/3}}\quad \text{for all}~\No, \NoNc \ge 5.
    \end{align}
\end{proposition}


% point-wise
\begin{lemma}\label{lemm:risk.valley}
Under Assumptions~\ref{prop:assu:density.derivative}, \ref{prop:assu:MLR}, there exists $C' > 0$, $D' > 0$ such that for any $\threshold$ satisfying $\left|\threshold - \trueThreshold \right| \le D'$,
\begin{align}\label{eq:lemm:risk.valley}
    \BayesRiskLambda(\trueThreshold) \le \BayesRiskLambda(\threshold) - C' (\threshold - \trueThreshold)^2.
\end{align}
\end{lemma}


\begin{proof}[Proof of \Cref{lemm:risk.valley}]

We perform Taylor expansion of $\BayesRiskLambda(\threshold)$ at $\trueThreshold$ with the Peano's form of remainder,    \begin{align}\label{proof:lemm:eq:taylor.expansion}
        \BayesRiskLambda(\threshold)
        &= \BayesRiskLambda(\trueThreshold) + \BayesRiskLambda'(\trueThreshold) (\threshold - \trueThreshold) + \frac{\BayesRiskLambda''(\trueThreshold)}{2}(\threshold - \trueThreshold)^2
        + o((\threshold - \trueThreshold)^2).
    \end{align}
    Since $\trueThreshold$ minimizes $\BayesRiskLambda(\threshold)$, by the KKT condition,    \begin{align}\label{proof:lemm:eq:first.order.derivative}
        \BayesRiskLambda'(\trueThreshold)
        &= \cdfTestStatisticsNull'(\trueThreshold) - \lambda \cdfTestStatistics{}'(\trueThreshold)
        = \pdfTestStatisticsNull(\trueThreshold) - \lambda \pdfTestStatistics{}(\trueThreshold)
        = 0.
    \end{align}
    Note that
    \begin{align}\label{proof:lemm:eq:MLR}
        0 < \left(\frac{\pdfTestStatisticsNull}{\pdfTestStatistics{}}\right)'(\trueThreshold)
        = \frac{\pdfTestStatisticsNull'{}(\trueThreshold) \pdfTestStatistics{}(\trueThreshold) - \pdfTestStatisticsNull{}(\trueThreshold) \pdfTestStatistics{}'(\trueThreshold)}{\pdfTestStatistics{}^2(\trueThreshold)}.
    \end{align}
    By Eq.~\eqref{proof:lemm:eq:first.order.derivative} and \eqref{proof:lemm:eq:MLR},
    \begin{align}\label{proof:lemm:eq:second.order.derivative}
    \begin{split}
    \BayesRiskLambda''(\trueThreshold)
    &= \pdfTestStatisticsNull'(\trueThreshold) - \lambda \pdfTestStatistics{}'(\trueThreshold)
    = \pdfTestStatisticsNull'(\trueThreshold)
    - \frac{\pdfTestStatisticsNull(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)} \pdfTestStatistics{}'(\trueThreshold)
    = \frac{\pdfTestStatisticsNull'(\trueThreshold)
     \pdfTestStatistics{}(\trueThreshold) - \pdfTestStatisticsNull(\trueThreshold) \pdfTestStatistics{}'(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)}
     >0.
     \end{split}
    \end{align}
    Plug Eq.~\eqref{proof:lemm:eq:first.order.derivative} and \eqref{proof:lemm:eq:second.order.derivative} into Eq.~\eqref{proof:lemm:eq:taylor.expansion} and we have proved that there exists $D' > 0$, $C' = \BayesRiskLambda''(\trueThreshold)/4 > 0$, such that Eq.~\eqref{eq:lemm:risk.valley} is valid.
\end{proof}

\begin{lemma}\label{lemm:consistency}
Under the assumptions in \Cref{prop:convergence.rate},
$\EmpiricalThreshold \to \trueThreshold$ in probability as $\No$, $\NoNc \to \infty$.
\end{lemma}


\begin{proof}[Proof of \Cref{lemm:consistency}]
    % By Assumption~\ref{prop:assu:Bayes.risk} and \Cref{lemm:risk.valley}, there exists $D'' > 0$ such that for any $\delta < D''$,
    % \begin{align*}
    %     \BayesRiskLambda(\trueThreshold) < \BayesRiskLambda(\threshold) - C' \delta^2, \quad |\threshold - \trueThreshold| \ge \delta.
    % \end{align*}
    Let $\EmpiricalBayesRiskLambda(\threshold):= \EmpiricalCdfTestStatisticsNull(\threshold) - \lambda \EmpiricalCdfTestStatistics(\threshold)$.
    For any $\delta > 0$, there exists $\varepsilon > 0$ such that $\BayesRiskLambda(\trueThreshold) < \BayesRiskLambda(\threshold) - \varepsilon$ for any $|\threshold - \trueThreshold| > \delta$.
    % By the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality \cite{dvoretzky1956asymptotic, massart1990tight}, for any $\varepsilon' > 0$,
    % \begin{align*}
    %     &\quad~\PP\left(\sup_{\threshold \in \RR}\left|
    %     \EmpiricalBayesRiskLambda(\threshold)
    %     - \BayesRiskLambda(\threshold)\right| > (1+\lambda) \varepsilon' \right)\\
    %     &\le \PP\left(\sup_{\threshold \in \RR}\left|
    %     \cdfTestStatisticsNull(\threshold)
    %     - \EmpiricalCdfTestStatisticsNull(\threshold)\right| > \varepsilon' \right) +  \PP\left(\sup_{\threshold \in \RR}\left|
    %     \lambda\cdfTestStatistics{}(\threshold)
    %     - \lambda\EmpiricalCdfTestStatistics(\threshold)\right| > \lambda \varepsilon' \right)\\
    %     &\le 2 e^{-2 \NoNc\varepsilon'^2} + 2 e^{-2\No\varepsilon'^2}
    %     \le  4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}.
    % \end{align*}
    By Eq.~\eqref{proof:eq:weak.1} with $\varepsilon' = \varepsilon/2(1+\lambda)$, we have with probability at least $1 - 8 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}$, for any $|\threshold - \trueThreshold| > \delta$,
    \begin{align*}
    &\quad~\EmpiricalBayesRiskLambda(\trueThreshold)  - \EmpiricalBayesRiskLambda(\threshold)\\
          &= (\EmpiricalBayesRiskLambda(\trueThreshold)  - \BayesRiskLambda(\trueThreshold))
          + (\BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\threshold))
          + (\BayesRiskLambda(\threshold) - \EmpiricalBayesRiskLambda(\threshold))\\
          &< -\varepsilon + 2 (1+\lambda)\varepsilon'
          = 0.
    \end{align*}
   Therefore, $\PP(|\EmpiricalThreshold - \trueThreshold| \le \delta) \to 1$.
\end{proof}


\begin{lemma}\label{lemm:maximal.inequality}
Under the assumptions in \Cref{prop:convergence.rate},
there exists $C > 0$ such that for any $0 < \delta \le D$, $\No \wedge \NoNc \ge 5$,
    \begin{align}\label{lemm:eq:M.estimator}
        &\quad~\EE\left[\sup_{|\threshold - \trueThreshold| \le \delta} \left|
        \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
        \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
        \right|\right] \\
        &\le C (1+\lambda) \left(\sqrt{\frac{\delta \left(\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max}\right) \log(\No \wedge \NoNc)}{\No \wedge \NoNc}} + \frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}\right).
    \end{align}
\end{lemma}


\begin{proof}[Proof of \Cref{lemm:maximal.inequality}]
% Notice that $\BayesRisk(\threshold) = \cdfTestStatisticsNull(\threshold) - \lambda \cdfTestStatistics{}(\threshold)$,
By the triangle inequality,
\begin{align*}
    &\quad~\sup_{|\threshold - \trueThreshold| \le \delta}\left|
    \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
    \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
    \right|\\
    &\le \sup_{|\threshold - \trueThreshold| \le \delta}\left|
    \left(\EmpiricalCdfTestStatisticsNull(\threshold) - \cdfTestStatisticsNull(\threshold)\right) -
    \left(\EmpiricalCdfTestStatisticsNull(\trueThreshold) - \cdfTestStatisticsNull(\trueThreshold)\right)
    \right| \\
    &\quad~+ \sup_{|\threshold - \trueThreshold| \le \delta}\lambda\left|
    \left(\EmpiricalCdfTestStatistics{}(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
    \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}{}(\trueThreshold)\right)
    \right|.
\end{align*}
We show there exists $C > 0$, for any $0 < \delta \le D$, $\No \ge 5$,
\begin{align}\label{proof:lemm:eq:M.estimator}
    \EE\left[\sup_{|\threshold - \trueThreshold| \le \delta} \left|
        \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
        \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
        \right|\right] \le C \left(\sqrt{\frac{\delta \pdfTestStatistics{\max} \log(\No)}{\No}} + \frac{\log(\No)}{\No}\right).
\end{align}
The analysis also applies to the null CDF part and combining the two parts yields the desired result.

For $\threshold < \trueThreshold$,
    \begin{align*}
        \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
        \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
        = \frac{1}{n} \sum_{i=1}^\No \1_{\{\testStatistics{i} \in (\threshold, \trueThreshold]\}} - \PP\left(\testStatistics{i} \in (\threshold, \trueThreshold]\right),
    \end{align*}
    and similarly for $\threshold > \trueThreshold$. To bound the left hand side of Eq.~\eqref{proof:lemm:eq:M.estimator}, we define   \begin{align*}
        &\sigma_\delta
        := \sup_{|\threshold - \trueThreshold| \le \delta} \sqrt{\var\left(\1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right)},\\
        N_\delta :=& \left\{\left(i, \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right), ~|\threshold - \trueThreshold| < \delta \right\}, \quad
        \pi_\delta
        := \EE \left[\log(2 |N_\delta|)\right].
    \end{align*}
    We bound $\sigma_\delta$ and $ \pi_\delta $ separately. For $\sigma_\delta$, by Assumption~\ref{prop:assu:density.upper.bound},
    \begin{align}\label{proof:lemm:eq:bound.sigma}
        \sigma_\delta
        \le \sqrt{\PP\left(\testStatistics{i} \in [\trueThreshold - \delta, \trueThreshold + \delta]\right)}
        \le \sqrt{2 \delta \pdfTestStatistics{\max}}.
    \end{align}
    For $\pi_\delta$, notice that $\1_{\{\testStatistics{i} \in [\trueThreshold, \threshold]\}}$ is binary and increases with regard to $\threshold$, then
    \begin{align*}
         \left|\left\{\left(i, \1_{\{\testStatistics{i} \in [\trueThreshold, \threshold]\}}\right), ~0 \le \threshold - \trueThreshold \le \delta \right\}\right| \le \No+1,
    \end{align*}
    and similarly for $-\delta \le \threshold - \trueThreshold \le 0$. Combine the two parts, $ \left|N_\delta\right| \le 2 \No+2$,
    % \begin{align}\label{proof:lemm:eq:bound.N}
    %      \left|N_\delta\right|
    %      \le \left|\{\{i, \testStatistics{i} \in [\trueThreshold, \threshold]\}, ~0 \le \threshold - \trueThreshold \le \delta \} \right| +
    %      \left|\{\{i, \testStatistics{i} \in [\threshold, \trueThreshold]\}, ~ -\delta \le \threshold - \trueThreshold \le 0 \} \right|
    %      \le 2 \No,
    % \end{align}
    and $\pi_\delta \le \log(4\No+4)$.
    By \cite[Lemma 6.4]{massart2007concentration} or \cite[Theorem 3.1]{baraud2016bounding},
    % and Eq.~\eqref{proof:lemm:eq:bound.sigma}, \eqref{proof:lemm:eq:bound.N},
    \begin{align*}
        &\quad~\EE\left[\sup_{|\threshold - \trueThreshold| \le \delta} \left|
        \frac{1}{n} \sum_{i=1}^\No \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}} - \PP\left(\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\right)
        \right|\right]\\
        &\le 2 \sigma_\delta \sqrt{\frac{2 \pi_\delta}{\No}} + \frac{8\pi_\delta}{\No}
        \le 4 \sqrt{\frac{\delta \pdfTestStatistics{\max} \log(4\No+4)}{\No}} + \frac{8\log(4\No+4)}{\No}.
    \end{align*}
    Take $C = 16$, Eq.~\eqref{proof:lemm:eq:M.estimator} is valid for $n \ge 5$.
\end{proof}



\begin{proof}[Proof of \Cref{prop:convergence.rate}]
By the proof of \Cref{lemm:consistency}, for $\No \wedge \NoNc$ large enough,  $\PP(|\EmpiricalThreshold - \trueThreshold| > D'/2) \le 1/\log(\No \wedge \NoNc)$ for the $D'$ in \Cref{lemm:risk.valley}.
Define the rate
$r_{\No \wedge\NoNc} = ({\No \wedge \NoNc}/\log({\No \wedge \NoNc}))^{-1/3}$, and shells
\begin{align*}
 S_{{\No \wedge\NoNc},j} = \left\{\tau: \frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le  |\threshold - \trueThreshold| \le \frac{2^j}{r_{\No \wedge\NoNc}} \right\}, \quad j \ge 1.
\end{align*}
If the event $\{|\EmpiricalThreshold- \trueThreshold| \ge 2^t/r_{\No \wedge\NoNc}\}$ happens for some $t \ge 0$, then there exists $j \ge t$ such that $\EmpiricalThreshold \in S_{{\No \wedge\NoNc}, j}$. Therefore, for $\No \wedge \NoNc$ large enough,
\begin{align*}
    &\quad~\PP\left(
    |\EmpiricalThreshold- \trueThreshold| \ge \frac{2^t}{r_{\No \wedge\NoNc}}
    \right)\\
    &\le \PP\left(
    |\EmpiricalThreshold - \trueThreshold| \ge \frac{2^t}{r_{\No \wedge\NoNc}},  ~|\EmpiricalThreshold - \trueThreshold|\le D'/2
    \right) + \PP\left(
    |\EmpiricalThreshold- \trueThreshold| > D'/2
    \right) \\
    &\le \sum_{j \ge t, ~\frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le D'/2} \PP\left(
    \EmpiricalThreshold \in S_{\No \wedge\NoNc,j}
    \right)  + \frac{1}{\log(\No \wedge\NoNc)} \\
    &\le \sum_{j \ge t, ~\frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le D'/2} \PP\left(
    \exists~\threshold \in  S_{\No \wedge\NoNc,j},~ \EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)
    \right)  + \frac{1}{\log(\No \wedge\NoNc)}.
\end{align*}
Notice that if $\EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)$
% for $\threshold \in S_{\No \wedge\NoNc,j}$ with $\frac{2^{j}}{r_{\No \wedge\NoNc}} \le D'$, then
and $|\threshold - \trueThreshold| \le D'$, by \Cref{lemm:risk.valley},
\begin{align*}
    \EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))
    &\le \BayesRisk(\trueThreshold) - \BayesRisk(\threshold)
    \le -C'(\threshold - \trueThreshold)^2
    \le  -C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}.
\end{align*}
Then by Markov inequality and \Cref{lemm:maximal.inequality},
\begin{align*}
    &~\quad\PP\left(
    \exists~\threshold \in  S_{\No \wedge\NoNc,j},~ \EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)
    \right)\\
    &\le \PP\left(\sup_{\threshold \in S_{\No \wedge\NoNc,j}}\left|\EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))\right| \ge C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2} \right)\\
    &\le \frac{\EE\left[\sup_{\threshold \in S_{\No \wedge\NoNc,j}}\left|\EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))\right| \right]}{C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}}\\
    &\le \frac{C (1+\lambda) \left(\sqrt{\frac{(2^j/r_{\No \wedge \NoNc}) (\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max})\log(\No \wedge \NoNc)}{\No \wedge \NoNc}} + \frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}\right)}{C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}}\\
    &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot \sqrt{\frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}} \cdot r_{\No \wedge\NoNc}^{3/2} \cdot  2^{-3j/2} \\
    &= \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot 2^{-3j/2},
\end{align*}
where we use $\sqrt{\frac{(2^j/r_{\No \wedge\NoNc})\log(\No \wedge \NoNc)}{\No \wedge \NoNc}} \ge \frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}$.
Finally, we sum over shells $S_{\No \wedge \NoNc, j}$,
\begin{align*}
    \PP\left(|\EmpiricalThreshold- \trueThreshold| \ge \frac{2^t}{r_{\No \wedge\NoNc}}\right)
    &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'}  \sum_{j \ge t}^\infty  2^{-3j/2} + \frac{1}{\log(\No \wedge \NoNc)}\\
    &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot \frac{2^{-3t/2}}{1-2^{-3/2}} + \frac{1}{\log(\No \wedge \NoNc)}.
\end{align*}
Take $2^{3t/2} = \log(\No \wedge \NoNc)$,
\begin{align*}
    \PP\left(|\EmpiricalThreshold- \trueThreshold| \ge \frac{2^t}{r_{\No \wedge\NoNc}}\right)
    = \PP\left(|\EmpiricalThreshold- \trueThreshold| \ge \frac{\log(\No \wedge\NoNc)}{(\No \wedge\NoNc)^{1/3}}\right)
    \le \frac{C^*}{\log(\No \wedge\NoNc)},
\end{align*}
where we let $C^* =  \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{(1-2^{-3/2})C'} + 1$.
\end{proof}




% uniform
\begin{proposition}\label{prop:convergence.rate.uniform}
   Let $\Lambda = (a, b) \subseteq (0,\probNull)$ and $\calT =
   \{\trueThreshold: \lambda \in \Lambda\}$. Suppose there exists
   a neighbourhood $\calT_\Delta = (\inf {\calT} - \Delta, \sup
   {\calT} + \Delta)$ for some $\Delta > 0$ and some $0 <
   \pdfTestStatistics{}_{\min} < \pdfTestStatistics{}_{\max} < \infty$
   such that
   \begin{enumerate}
        \item \label{prop:assu:Bayes.risk.uniform}  $\inf_{\lambda \in
            \Lambda}\inf_{\threshold \notin
            \calT_\Delta}(\cdfTestStatisticsNull(\threshold) - \lambda
          \cdfTestStatistics{}(\threshold)) -
          (\cdfTestStatisticsNull(\trueThreshold) - \lambda
          \cdfTestStatistics{}(\trueThreshold)) > 0$;
        \item \label{prop:assu:density.upper.bound.uniform}
          $0 < \pdfTestStatistics{\min} \leq \pdfTestStatistics{}(t),\pdfTestStatistics{0}(t) \leq  \pdfTestStatistics{\max} < \infty$ for
          all $\threshold \in \calT_{\Delta}$;
        % \item \label{prop:assu:density.derivative}
        % $f'$, $\pdfTestStatisticsNull'$ exist on $\calT$;
        \item \label{prop:assu:MLR.uniform}
           $\pdfTestStatisticsNull'$, $f'$, $(\pdfTestStatisticsNull/\pdfTestStatistics{})'$ exist and $0 < f_{\min}'
     \le \pdfTestStatisticsNull'(\threshold), \pdfTestStatistics{}'(\threshold), (\pdfTestStatisticsNull/\pdfTestStatistics{})'(\threshold) \le f_{\max}' < \infty$  for
          all $\threshold \in \calT_{\Delta}$.
    \end{enumerate}
    Then there exists a constant $C^* > 0$ such that with probability at least $1 - C^*/\log(\No \wedge \NoNc)$,
    \begin{align}\label{eq:prop:convergence.rate}
        \sup_{\lambda \in \Lambda}\left|\EmpiricalThreshold - \trueThreshold\right|
        \le \frac{\log(\No \wedge \NoNc)}{(\No \wedge \NoNc)^{1/3}}, \quad \text{for all} \No \wedge \NoNc \ge 5.
    \end{align}
\end{proposition}


\begin{proof}[Proof of \Cref{prop:convergence.rate.uniform}]
We can follow the proof of \Cref{prop:convergence.rate} and substitute \Cref{lemm:risk.valley}, \Cref{lemm:consistency}, and \Cref{lemm:maximal.inequality} with their corresponding uniform results provided below.

Analogous to \Cref{lemm:risk.valley}, we have under Assumption~\ref{prop:assu:density.upper.bound.uniform} and Assumption~\ref{prop:assu:MLR.uniform}, there exists $C' > 0$, $D' > 0$ such that for any $\lambda \in \Lambda$, any $\threshold$ satisfying $\left|\threshold - \trueThreshold \right| \le D'$,
\begin{align}\label{eq:lemm:risk.valley.uniform}
    \BayesRiskLambda(\trueThreshold) \le \BayesRiskLambda(\threshold) - C' (\threshold - \trueThreshold)^2.
\end{align}
In fact, by Assumption~\ref{prop:assu:MLR.uniform}, we perform Taylor expansion of $\BayesRiskLambda(\threshold)$ at $\trueThreshold$. For $|\threshold - \trueThreshold| <  \Delta \wedge (f_{\min}' f_{\min}/2(f_{\max}')^2)$, \begin{align}\label{proof:lemm:eq:taylor.expansion.uniform}
        \BayesRiskLambda(\threshold)
        &= \BayesRiskLambda(\trueThreshold) + \BayesRiskLambda'(\trueThreshold) (\threshold - \trueThreshold) + \frac{\BayesRiskLambda''(\threshold')}{2}(\threshold - \trueThreshold)^2,
    \end{align}
    for some $\threshold' \in [t \wedge \trueThreshold, t \vee \trueThreshold]$.
    Since $\trueThreshold$ minimizes $\BayesRiskLambda(\threshold)$, by the KKT condition,    \begin{align}\label{proof:lemm:eq:first.order.derivative.uniform}
        \BayesRiskLambda'(\trueThreshold)
        &= \cdfTestStatisticsNull'(\trueThreshold) - \lambda \cdfTestStatistics{}'(\trueThreshold)
        = \pdfTestStatisticsNull(\trueThreshold) - \lambda \pdfTestStatistics{}(\trueThreshold)
        = 0.
    \end{align}
    By Assumption~\ref{prop:assu:MLR.uniform},
    \begin{align}\label{proof:lemm:eq:MLR.uniform}
        f_{\min}' < \left(\frac{\pdfTestStatisticsNull}{\pdfTestStatistics{}}\right)'(\threshold')
        = \frac{\pdfTestStatisticsNull'{}(\threshold') \pdfTestStatistics{}(\threshold') - \pdfTestStatisticsNull{}(\threshold') \pdfTestStatistics{}'(\threshold')}{\pdfTestStatistics{}^2(\threshold')}.
    \end{align}
    By Eq.~\eqref{proof:lemm:eq:first.order.derivative.uniform}, \eqref{proof:lemm:eq:MLR.uniform}, and Assumption~\ref{prop:assu:density.upper.bound.uniform}, Assumption~\ref{prop:assu:MLR.uniform},
    \begin{align}\label{proof:lemm:eq:second.order.derivative.uniform}
    \begin{split}
    \BayesRiskLambda''(\threshold')
    &= \pdfTestStatisticsNull'(\threshold') - \lambda \pdfTestStatistics{}'(\threshold')
    = \pdfTestStatisticsNull'(\threshold')
    - \frac{\pdfTestStatisticsNull(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)} \pdfTestStatistics{}'(\threshold') \\
    &= \pdfTestStatisticsNull'(\threshold') - \frac{\pdfTestStatisticsNull(\threshold')}{\pdfTestStatistics{}(\threshold')} \pdfTestStatistics{}'(\threshold') + \frac{\pdfTestStatisticsNull(\threshold')}{\pdfTestStatistics{}(\threshold')} \pdfTestStatistics{}'(\threshold')
    - \frac{\pdfTestStatisticsNull(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)} \pdfTestStatistics{}'(\threshold') \\
    &> \frac{\pdfTestStatisticsNull'(\threshold')
     \pdfTestStatistics{}(\threshold') - \pdfTestStatisticsNull(\threshold') \pdfTestStatistics{}'(\threshold')}{\pdfTestStatistics{}(\threshold')} - |\threshold' - \trueThreshold|\sup_{\threshold \in \calT_\Delta} \left(\frac{\pdfTestStatisticsNull}{\pdfTestStatistics{}}\right)'(\threshold) \sup_{\threshold \in \calT_\Delta} \pdfTestStatistics{}'(\threshold)\\
     &> f_{\min}' f_{\min} - |\threshold' - \trueThreshold| (f_{\max}')^2.
     \end{split}
    \end{align}
    Plug Eq.~\eqref{proof:lemm:eq:first.order.derivative.uniform} and \eqref{proof:lemm:eq:second.order.derivative.uniform} into Eq.~\eqref{proof:lemm:eq:taylor.expansion.uniform} and we have proved that Eq.~\eqref{eq:lemm:risk.valley.uniform} is valid for $C' =  f_{\min}' f_{\min}/4 > 0$ and $D' < \Delta \wedge (f_{\min}' f_{\min}/2(f_{\max}')^2)$.


Analogous to \Cref{lemm:consistency}, under the assumptions in \Cref{prop:convergence.rate.uniform}, $\sup_{\lambda \in \Lambda} |\EmpiricalThreshold - \trueThreshold| \to 0$ in probability as $\No$, $\NoNc \to \infty$.
    % By Assumption~\ref{prop:assu:Bayes.risk} and \Cref{lemm:risk.valley}, there exists $D'' > 0$ such that for any $\delta < D''$,
    % \begin{align*}
    %     \BayesRiskLambda(\trueThreshold) < \BayesRiskLambda(\threshold) - C' \delta^2, \quad |\threshold - \trueThreshold| \ge \delta.
    % \end{align*}
    % We prove the result for $\EmpiricalThreshold$, and the argument also applies to $\estimatedThreshold$.
    In fact, by Assumption~\ref{prop:assu:MLR.uniform}, $\BayesRiskLambda(\threshold)$ decreases on $(\inf \calT - \Delta, \trueThreshold]$ and increases on $[\trueThreshold, \sup \calT + \Delta)$.
    By Assumption~\ref{prop:assu:Bayes.risk.uniform} and \eqref{eq:lemm:risk.valley.uniform}, for any $0 < \delta < D'$, $\lambda \in \Lambda$, $\threshold - \trueThreshold > \delta$,
    \begin{align*}
        \BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\threshold)
        \le \BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\trueThreshold + \delta)
        \le -C'\delta^2.
    \end{align*}
    Similarly for $\threshold - \trueThreshold < -\delta$. The rest of the proof is similar to that of \Cref{lemm:consistency}.
%     For simplicity, we let $\varepsilon:= C'\delta^2$.
%     By the triangle inequality and the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality \cite{dvoretzky1956asymptotic, massart1990tight}, for any $\varepsilon' > 0$,
%     \begin{align*}
%         &\quad~\PP\left(\sup_{\lambda \in \Lambda, \threshold \in \RR}\left|
%         \EmpiricalBayesRiskLambda(\threshold)
%         - \BayesRiskLambda(\threshold)\right| - (1+\lambda) \varepsilon' > 0\right)\\
%         &\le \PP\left(\sup_{\threshold \in \RR}\left|
%         \cdfTestStatisticsNull(\threshold)
%         - \EmpiricalCdfTestStatisticsNull(\threshold)\right| - \varepsilon' > 0\right) +  \PP\left(\sup_{\lambda \in \Lambda, \threshold \in \RR}\left|
%         \lambda\cdfTestStatistics{}(\threshold)
%         - \lambda\EmpiricalCdfTestStatistics(\threshold)\right| - \lambda \varepsilon' > 0 \right)\\
%         &\le 2 e^{-2 \NoNc\varepsilon'^2} + 2 e^{-2\No\varepsilon'^2}
%         \le  4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}.
%     \end{align*}
%     Let $\varepsilon' = \varepsilon/2(1+\lambda)$. With probability at least $1 - 4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}$, for any $\lambda \in \Lambda$, $|\threshold - \trueThreshold| > \delta$,
%     \begin{align*}
%     &\quad~\EmpiricalBayesRiskLambda(\trueThreshold)  - \EmpiricalBayesRiskLambda(\threshold)\\
%           &= (\EmpiricalBayesRiskLambda(\trueThreshold)  - \BayesRiskLambda(\trueThreshold))
%           + (\BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\threshold))
%           + (\BayesRiskLambda(\threshold) - \EmpiricalBayesRiskLambda(\threshold))\\
%           &< -\varepsilon + 2 (1+\lambda)\varepsilon'
%           = 0.
%     \end{align*}
%   Therefore, $\PP(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold - \trueThreshold| \le \delta) \to 1$.

Analogous to \Cref{lemm:maximal.inequality}, we have under Assumption~\ref{prop:assu:density.upper.bound.uniform},
there exists $C > 0$, $\delta > 0$, such that for $\No \wedge \NoNc \ge 5$,
    \begin{align*}
        \begin{split}
        &\quad~\EE\left[\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \left|
        \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
        \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
        \right|\right] \\
        &\le C (1+\lambda) \left(\sqrt{\frac{\delta \pdfTestStatistics{\max} \log(\No \wedge \NoNc)}{\No \wedge \NoNc}} + \frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}\right).
        \end{split}
    \end{align*}
% \begin{proof}[Proof of \Cref{lemm:maximal.inequality.uniform}]
% Notice that $\BayesRisk(\threshold) = \cdfTestStatisticsNull(\threshold) - \lambda \cdfTestStatistics{}(\threshold)$,
% We prove the result for $\EmpiricalBayesRiskLambda(\trueThreshold)$ and $\No \wedge \NoNc$, and the argument also applies to $\EmpiricalBayesRiskLambdaSecond(\trueThreshold)$ and $\No$.
In fact, the outline of the proof is similar to that of \Cref{lemm:maximal.inequality}.
% By the triangle inequality,
% \begin{align*}
%     &\quad~\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\left|
%     \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
%     \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
%     \right|\\
%     &\le \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\left|
%     \left(\EmpiricalCdfTestStatisticsNull(\threshold) - \cdfTestStatisticsNull(\threshold)\right) -
%     \left(\EmpiricalCdfTestStatisticsNull(\trueThreshold) - \cdfTestStatisticsNull(\trueThreshold)\right)
%     \right| \\
%     &\quad~+ \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\lambda\left|
%     \left(\EmpiricalCdfTestStatistics{}(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
%     \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}{}(\trueThreshold)\right)
%     \right|.
% \end{align*}
% We show there exists $C > 0$, $\delta > 0$, $\No \ge 5$,
% \begin{align}\label{proof:lemm:eq:M.estimator}
%     \EE\left[\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \left|
%         \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
%         \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
%         \right|\right] \le C \left(\sqrt{\frac{\delta \pdfTestStatistics{\max} \log(\No)}{\No}} + \frac{\log(\No)}{\No}\right).
% \end{align}
% The analysis also applies to the null CDF part and combining the two parts yields the desired result.
% For any $\lambda \in \Lambda$,
%     \begin{align*}
%         \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
%         \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
%         = \frac{1}{n} \sum_{i=1}^\No \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}} - \PP\left(\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\right).
%     \end{align*}
    It is left to characterize the complexity of the function class $\left\{\1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}, \lambda \in \Lambda, |t - \trueThreshold| < \delta\right\}$. We define
        \begin{align*}
        &\sigma_\delta
        := \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \sqrt{\var\left(\1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right)},\\
        N_\delta :=& \left\{\left\{i: \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right\}: ~\lambda \in \Lambda, ~|\threshold - \trueThreshold| < \delta \right\}, \quad
        \pi_\delta
        := \EE \left[\log(2 |N_\delta|)\right].
    \end{align*}
    We bound $\sigma_\delta$ and $ \pi_\delta $ separately. For $\sigma_\delta$, by Assumption~\ref{prop:assu:density.upper.bound.uniform},
    \begin{align*}
        \sigma_\delta
        \le \sqrt{\PP\left(\testStatistics{i} \in [\trueThreshold - \delta, \trueThreshold + \delta]\right)}
        \le \sqrt{2 \delta \pdfTestStatistics{\max}}.
    \end{align*}
    For $\pi_\delta$, notice that $\{[\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]: \lambda \in \Lambda, |\threshold - \trueThreshold| < \delta\}$ is a subset of all intervals of $\RR$, then
    \begin{align*}
         \left|\left\{\left\{i: \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right\}: ~\lambda \in \Lambda, ~|\threshold - \trueThreshold| < \delta \right\}\right|
         \le 1 + \binom{\No+1}{2}
         \le \No^2,
    \end{align*}
    for $\No \ge 5$, and immediately $\pi_\delta \le \log(2\No^2)$. % $\No \ge 2$
    % Again by \cite[Lemma 6.4]{massart2007concentration} or \cite[Theorem 3.1]{baraud2016bounding}, we have that \eqref{lemm:eq:M.estimator} is true.
\end{proof}

% \begin{lemma}\label{lemm:risk.valley.uniform}
% Under Assumption~\ref{prop:assu:density.upper.bound.uniform} and Assumption~\ref{prop:assu:MLR.uniform}, there exists $C' > 0$, $D' > 0$ such that for any $\lambda \in \Lambda$, any $\threshold$ satisfying $\left|\threshold - \trueThreshold \right| \le D'$,
% \begin{align}\label{eq:lemm:risk.valley.uniform}
%     \BayesRiskLambda(\trueThreshold) \le \BayesRiskLambda(\threshold) - C' (\threshold - \trueThreshold)^2.
% \end{align}
% \end{lemma}

% \begin{proof}[Proof of \Cref{lemm:risk.valley.uniform}]
%     By Assumption~\ref{prop:assu:MLR.uniform}, we perform Taylor expansion of $\BayesRiskLambda(\threshold)$ at $\trueThreshold$. For $|\threshold - \trueThreshold| <  \Delta \wedge (f_{\min}' f_{\min}/2(f_{\max}')^2)$, \begin{align}\label{proof:lemm:eq:taylor.expansion.uniform}
%         \BayesRiskLambda(\threshold)
%         &= \BayesRiskLambda(\trueThreshold) + \BayesRiskLambda'(\trueThreshold) (\threshold - \trueThreshold) + \frac{\BayesRiskLambda''(\threshold')}{2}(\threshold - \trueThreshold)^2,
%     \end{align}
%     for some $\threshold' \in [t \wedge \trueThreshold, t \vee \trueThreshold]$.
%     Since $\trueThreshold$ minimizes $\BayesRiskLambda(\threshold)$, by the KKT condition,    \begin{align}\label{proof:lemm:eq:first.order.derivative.uniform}
%         \BayesRiskLambda'(\trueThreshold)
%         &= \cdfTestStatisticsNull'(\trueThreshold) - \lambda \cdfTestStatistics{}'(\trueThreshold)
%         = \pdfTestStatisticsNull(\trueThreshold) - \lambda \pdfTestStatistics{}(\trueThreshold)
%         = 0.
%     \end{align}
%     By Assumption~\ref{prop:assu:MLR.uniform},
%     \begin{align}\label{proof:lemm:eq:MLR.uniform}
%         f_{\min}' < \left(\frac{\pdfTestStatisticsNull}{\pdfTestStatistics{}}\right)'(\threshold')
%         = \frac{\pdfTestStatisticsNull'{}(\threshold') \pdfTestStatistics{}(\threshold') - \pdfTestStatisticsNull{}(\threshold') \pdfTestStatistics{}'(\threshold')}{\pdfTestStatistics{}^2(\threshold')}.
%     \end{align}
%     By Eq.~\eqref{proof:lemm:eq:first.order.derivative.uniform}, \eqref{proof:lemm:eq:MLR.uniform}, and Assumption~\ref{prop:assu:density.upper.bound.uniform}, Assumption~\ref{prop:assu:MLR.uniform},
%     \begin{align}\label{proof:lemm:eq:second.order.derivative.uniform}
%     \begin{split}
%     \BayesRiskLambda''(\threshold')
%     &= \pdfTestStatisticsNull'(\threshold') - \lambda \pdfTestStatistics{}'(\threshold')
%     = \pdfTestStatisticsNull'(\threshold')
%     - \frac{\pdfTestStatisticsNull(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)} \pdfTestStatistics{}'(\threshold') \\
%     &= \pdfTestStatisticsNull'(\threshold') - \frac{\pdfTestStatisticsNull(\threshold')}{\pdfTestStatistics{}(\threshold')} \pdfTestStatistics{}'(\threshold') + \frac{\pdfTestStatisticsNull(\threshold')}{\pdfTestStatistics{}(\threshold')} \pdfTestStatistics{}'(\threshold')
%     - \frac{\pdfTestStatisticsNull(\trueThreshold)}{\pdfTestStatistics{}(\trueThreshold)} \pdfTestStatistics{}'(\threshold') \\
%     &> \frac{\pdfTestStatisticsNull'(\threshold')
%      \pdfTestStatistics{}(\threshold') - \pdfTestStatisticsNull(\threshold') \pdfTestStatistics{}'(\threshold')}{\pdfTestStatistics{}(\threshold')} - |\threshold' - \trueThreshold|\sup_{\threshold \in \calT_\Delta} \left(\frac{\pdfTestStatisticsNull}{\pdfTestStatistics{}}\right)'(\threshold) \sup_{\threshold \in \calT_\Delta} \pdfTestStatistics{}'(\threshold)\\
%      &> f_{\min}' f_{\min} - |\threshold' - \trueThreshold| (f_{\max}')^2.
%      \end{split}
%     \end{align}
%     Plug Eq.~\eqref{proof:lemm:eq:first.order.derivative.uniform} and \eqref{proof:lemm:eq:second.order.derivative.uniform} into Eq.~\eqref{proof:lemm:eq:taylor.expansion.uniform} and we have proved that Eq.~\eqref{eq:lemm:risk.valley.uniform} is valid for $C' =  f_{\min}' f_{\min}/4 > 0$ and $D' < \Delta \wedge (f_{\min}' f_{\min}/2(f_{\max}')^2)$.
% \end{proof}


% \begin{lemma}\label{lemm:consistency.uniform}
% Under the assumptions in \Cref{prop:convergence.rate.uniform}, $\sup_{\lambda \in \Lambda} |\EmpiricalThreshold - \trueThreshold| \to 0$ in probability as $\No$, $\NoNc \to \infty$.
% \end{lemma}

% \begin{proof}[Proof of \Cref{lemm:consistency.uniform}]
%     % By Assumption~\ref{prop:assu:Bayes.risk} and \Cref{lemm:risk.valley}, there exists $D'' > 0$ such that for any $\delta < D''$,
%     % \begin{align*}
%     %     \BayesRiskLambda(\trueThreshold) < \BayesRiskLambda(\threshold) - C' \delta^2, \quad |\threshold - \trueThreshold| \ge \delta.
%     % \end{align*}
%     % We prove the result for $\EmpiricalThreshold$, and the argument also applies to $\estimatedThreshold$.
%     By Assumption~\ref{prop:assu:MLR.uniform}, $\BayesRiskLambda(\threshold)$ decreases on $(\inf \calT - \Delta, \trueThreshold]$ and increases on $[\trueThreshold, \sup \calT + \Delta)$.
%     By Assumption~\ref{prop:assu:Bayes.risk.uniform} and \Cref{lemm:risk.valley.uniform}, for any $0 < \delta < D'$, $\lambda \in \Lambda$, $\threshold - \trueThreshold > \delta$,
%     \begin{align*}
%         \BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\threshold)
%         \le \BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\trueThreshold + \delta)
%         \le -C'\delta^2.
%     \end{align*}
%     Similarly for $\threshold - \trueThreshold < -\delta$. The rest of the proof is similar to that of \Cref{lemm:consistency}.
% %     For simplicity, we let $\varepsilon:= C'\delta^2$.
% %     By the triangle inequality and the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality \cite{dvoretzky1956asymptotic, massart1990tight}, for any $\varepsilon' > 0$,
% %     \begin{align*}
% %         &\quad~\PP\left(\sup_{\lambda \in \Lambda, \threshold \in \RR}\left|
% %         \EmpiricalBayesRiskLambda(\threshold)
% %         - \BayesRiskLambda(\threshold)\right| - (1+\lambda) \varepsilon' > 0\right)\\
% %         &\le \PP\left(\sup_{\threshold \in \RR}\left|
% %         \cdfTestStatisticsNull(\threshold)
% %         - \EmpiricalCdfTestStatisticsNull(\threshold)\right| - \varepsilon' > 0\right) +  \PP\left(\sup_{\lambda \in \Lambda, \threshold \in \RR}\left|
% %         \lambda\cdfTestStatistics{}(\threshold)
% %         - \lambda\EmpiricalCdfTestStatistics(\threshold)\right| - \lambda \varepsilon' > 0 \right)\\
% %         &\le 2 e^{-2 \NoNc\varepsilon'^2} + 2 e^{-2\No\varepsilon'^2}
% %         \le  4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}.
% %     \end{align*}
% %     Let $\varepsilon' = \varepsilon/2(1+\lambda)$. With probability at least $1 - 4 e^{-2 (\NoNc \wedge \No)\varepsilon'^2}$, for any $\lambda \in \Lambda$, $|\threshold - \trueThreshold| > \delta$,
% %     \begin{align*}
% %     &\quad~\EmpiricalBayesRiskLambda(\trueThreshold)  - \EmpiricalBayesRiskLambda(\threshold)\\
% %           &= (\EmpiricalBayesRiskLambda(\trueThreshold)  - \BayesRiskLambda(\trueThreshold))
% %           + (\BayesRiskLambda(\trueThreshold) - \BayesRiskLambda(\threshold))
% %           + (\BayesRiskLambda(\threshold) - \EmpiricalBayesRiskLambda(\threshold))\\
% %           &< -\varepsilon + 2 (1+\lambda)\varepsilon'
% %           = 0.
% %     \end{align*}
% %   Therefore, $\PP(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold - \trueThreshold| \le \delta) \to 1$.
% \end{proof}


% \begin{lemma}\label{lemm:maximal.inequality.uniform}
% Under Assumption~\ref{prop:assu:density.upper.bound.uniform},
% there exists $C > 0$, $\delta > 0$, such that for $\No \wedge \NoNc \ge 5$,
%     \begin{align*}
%         \begin{split}
%         &\quad~\EE\left[\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \left|
%         \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
%         \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
%         \right|\right] \\
%         &\le C (1+\lambda) \left(\sqrt{\frac{\delta \pdfTestStatistics{\max} \log(\No \wedge \NoNc)}{\No \wedge \NoNc}} + \frac{\log(\No \wedge \NoNc)}{\No \wedge \NoNc}\right).
%         \end{split}
%     \end{align*}
% \end{lemma}


% \begin{proof}[Proof of \Cref{lemm:maximal.inequality.uniform}]
% % Notice that $\BayesRisk(\threshold) = \cdfTestStatisticsNull(\threshold) - \lambda \cdfTestStatistics{}(\threshold)$,
% % We prove the result for $\EmpiricalBayesRiskLambda(\trueThreshold)$ and $\No \wedge \NoNc$, and the argument also applies to $\EmpiricalBayesRiskLambdaSecond(\trueThreshold)$ and $\No$.
% The outline of the proof is similar to that of \Cref{lemm:maximal.inequality}.
% % By the triangle inequality,
% % \begin{align*}
% %     &\quad~\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\left|
% %     \left(\EmpiricalBayesRiskLambda(\threshold) - \BayesRiskLambda{}(\threshold)\right) -
% %     \left(\EmpiricalBayesRiskLambda(\trueThreshold) - \BayesRiskLambda{}(\trueThreshold)\right)
% %     \right|\\
% %     &\le \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\left|
% %     \left(\EmpiricalCdfTestStatisticsNull(\threshold) - \cdfTestStatisticsNull(\threshold)\right) -
% %     \left(\EmpiricalCdfTestStatisticsNull(\trueThreshold) - \cdfTestStatisticsNull(\trueThreshold)\right)
% %     \right| \\
% %     &\quad~+ \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta}\lambda\left|
% %     \left(\EmpiricalCdfTestStatistics{}(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
% %     \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}{}(\trueThreshold)\right)
% %     \right|.
% % \end{align*}
% % We show there exists $C > 0$, $\delta > 0$, $\No \ge 5$,
% % \begin{align}\label{proof:lemm:eq:M.estimator}
% %     \EE\left[\sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \left|
% %         \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
% %         \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
% %         \right|\right] \le C \left(\sqrt{\frac{\delta \pdfTestStatistics{\max} \log(\No)}{\No}} + \frac{\log(\No)}{\No}\right).
% % \end{align}
% % The analysis also applies to the null CDF part and combining the two parts yields the desired result.
% % For any $\lambda \in \Lambda$,
% %     \begin{align*}
% %         \left(\EmpiricalCdfTestStatistics(\threshold) - \cdfTestStatistics{}(\threshold)\right) -
% %         \left(\EmpiricalCdfTestStatistics(\trueThreshold) - \cdfTestStatistics{}(\trueThreshold)\right)
% %         = \frac{1}{n} \sum_{i=1}^\No \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}} - \PP\left(\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\right).
% %     \end{align*}
%     It is left to characterize the complexity of the function class $\left\{\1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}, \lambda \in \Lambda, |t - \trueThreshold| < \delta\right\}$. We define
%         \begin{align*}
%         &\sigma_\delta
%         := \sup_{\lambda \in \Lambda, |\threshold - \trueThreshold| \le \delta} \sqrt{\var\left(\1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right)},\\
%         N_\delta :=& \left\{\left\{i: \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right\}: ~\lambda \in \Lambda, ~|\threshold - \trueThreshold| < \delta \right\}, \quad
%         \pi_\delta
%         := \EE \left[\log(2 |N_\delta|)\right].
%     \end{align*}
%     We bound $\sigma_\delta$ and $ \pi_\delta $ separately. For $\sigma_\delta$, by Assumption~\ref{prop:assu:density.upper.bound.uniform},
%     \begin{align*}
%         \sigma_\delta
%         \le \sqrt{\PP\left(\testStatistics{i} \in [\trueThreshold - \delta, \trueThreshold + \delta]\right)}
%         \le \sqrt{2 \delta \pdfTestStatistics{\max}}.
%     \end{align*}
%     For $\pi_\delta$, notice that $\{[\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]: \lambda \in \Lambda, |\threshold - \trueThreshold| < \delta\}$ is a subset of all intervals of $\RR$, then
%     \begin{align*}
%          \left|\left\{\left\{i: \1_{\{\testStatistics{i} \in [\threshold \wedge \trueThreshold, \threshold \vee \trueThreshold]\}}\right\}: ~\lambda \in \Lambda, ~|\threshold - \trueThreshold| < \delta \right\}\right|
%          \le 1 + \binom{\No+1}{2}
%          \le \No^2,
%     \end{align*}
%     for $\No \ge 2$, and immediately $\pi_\delta \le \log(2\No^2)$.
%     % Again by \cite[Lemma 6.4]{massart2007concentration} or \cite[Theorem 3.1]{baraud2016bounding}, we have that \eqref{lemm:eq:M.estimator} is true.
% \end{proof}



% \begin{proof}[Proof of \Cref{prop:convergence.rate.uniform}]
% By the proof of \Cref{lemm:consistency}, for $\No \wedge \NoNc$ large enough,  $\PP(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold - \trueThreshold| > D'/2) \le 1/\log(\No \wedge \NoNc)$ for the $D'$ in \Cref{lemm:risk.valley}.
% Define the rate
% $r_{\No \wedge\NoNc} = ({\No \wedge \NoNc}/\log({\No \wedge \NoNc}))^{-1/3}$, and shells
% \begin{align*}
%  S_{{\lambda, \No \wedge\NoNc},j} = \left\{\tau: \frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le  |\threshold - \trueThreshold| \le \frac{2^j}{r_{\No \wedge\NoNc}} \right\}, \quad j \ge 1.
% \end{align*}
% If the event $\{|\EmpiricalThreshold- \trueThreshold| \ge 2^t/r_{\No \wedge\NoNc}\}$ happens for some $t \ge 0$, then there exists $j \ge t$ such that $\EmpiricalThreshold \in S_{{\lambda, \No \wedge\NoNc}, j}$. Therefore, for $\No \wedge \NoNc$ large enough,
% \begin{align*}
%     &\quad~\PP\left(
%     \sup_{\lambda \in \Lambda}|\EmpiricalThreshold- \trueThreshold| > \frac{2^t}{r_{\No \wedge\NoNc}}
%     \right)\\
%     &\le \PP\left(
%     \exists {\lambda \in \Lambda}, |\EmpiricalThreshold - \trueThreshold| \ge \frac{2^t}{r_{\No \wedge\NoNc}},  ~|\EmpiricalThreshold - \trueThreshold|\le D'/2
%     \right) + \PP\left(
%   \exists {\lambda \in \Lambda}, |\EmpiricalThreshold- \trueThreshold| > D'/2
%     \right) \\
%     &\le \sum_{j \ge t, ~\frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le D'/2} \PP\left(\exists {\lambda \in \Lambda}, ~
%     \EmpiricalThreshold \in S_{\lambda, \No \wedge\NoNc,j}
%     \right)  + \frac{1}{\log(\No \wedge\NoNc)} \\
%     &\le \sum_{j \ge t, ~\frac{2^{j-1}}{r_{\No \wedge\NoNc}} \le D'/2} \PP\left(
%     \exists~ \lambda \in \Lambda, \threshold \in  S_{\lambda, \No \wedge\NoNc,j},~ \EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)
%     \right)  + \frac{1}{\log(\No \wedge\NoNc)}.
% \end{align*}
% Notice that if $\EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)$
% % for $\threshold \in S_{\No \wedge\NoNc,j}$ with $\frac{2^{j}}{r_{\No \wedge\NoNc}} \le D'$, then
% and $|\threshold - \trueThreshold| \le D'$, by \Cref{lemm:risk.valley},
% \begin{align*}
%     \EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))
%     &\le \BayesRisk(\trueThreshold) - \BayesRisk(\threshold)
%     \le -C'(\threshold - \trueThreshold)^2
%     \le  -C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}.
% \end{align*}
% Then by the Markov inequality and \Cref{lemm:maximal.inequality},
% \begin{align*}
%     &~\quad\PP\left(
%     \exists~\lambda \in \Lambda, \threshold \in  S_{\lambda, \No \wedge\NoNc,j},~ \EmpiricalBayesRisk(\threshold) < \EmpiricalBayesRisk(\trueThreshold)
%     \right)\\
%     &\le \PP\left(\sup_{\lambda \in \Lambda, \threshold \in S_{\lambda, \No \wedge\NoNc,j}}\left|\EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))\right| \ge C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2} \right)\\
%     &\le {\EE\left[\sup_{\lambda \in \Lambda, \threshold \in S_{\lambda, \No \wedge\NoNc,j}}\left|\EmpiricalBayesRisk(\threshold) - \BayesRisk(\threshold) - (\EmpiricalBayesRisk(\trueThreshold) - \BayesRisk(\trueThreshold))\right| \right]}\left({C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}}\right)^{-1}\\
%     &\le {C (1+\lambda) \left(\sqrt{\frac{(2^j/r_{\No \vee \NoNc}) (\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max})\log(\No \vee \NoNc)}{\No \vee \NoNc}} + \frac{\log(\No \vee \NoNc)}{\No \vee \NoNc}\right)}\left({C'\frac{4^{j-1}}{r_{\No \wedge\NoNc}^2}}\right)^{-1}\\
%     &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot \sqrt{\frac{\log(\No \vee \NoNc)}{\No \vee \NoNc}} \cdot r_{\No \wedge\NoNc}^{3/2} \cdot  2^{-3j/2} \\
%     &= \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot 2^{-3j/2},
% \end{align*}
% where we use $\sqrt{\frac{(2^j/r_{\No \wedge\NoNc})\log(\No \vee \NoNc)}{\No \vee \NoNc}} \ge \frac{\log(\No \vee \NoNc)}{\No \vee \NoNc}$.
% Finally, we sum over shells $S_{\lambda, \No \wedge \NoNc, j}$,
% \begin{align*}
%     \PP\left(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold- \trueThreshold| > \frac{2^t}{r_{\No \wedge\NoNc}}\right)
%     &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'}  \sum_{j \ge t}^\infty  2^{-3j/2} + \frac{1}{\log(\No \wedge \NoNc)}\\
%     &\le \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{C'} \cdot \frac{2^{-3t/2}}{1-2^{-3/2}} + \frac{1}{\log(\No \wedge \NoNc)}.
% \end{align*}
% Take $2^{3t/2} = \log(\No \wedge \NoNc)$,
% \begin{align*}
%     \PP\left(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold- \trueThreshold| > \frac{2^t}{r_{\No \wedge\NoNc}}\right)
%     = \PP\left(\sup_{\lambda \in \Lambda}|\EmpiricalThreshold- \trueThreshold| \ge \frac{\log(\No \wedge\NoNc)}{(\No \wedge\NoNc)^{1/3}}\right)
%     \le \frac{C^*}{\log(\No \wedge\NoNc)},
% \end{align*}
% where we let $C^* =  \frac{8C(1+\lambda) \sqrt{\pdfTestStatistics{\max} \vee \pdfTestStatistics{0,\max} \vee 1})}{(1-2^{-3/2})C'} + 1$.
% \end{proof}


\begin{remark}\label{rmk:non.monotone}
    In the cases with non-monotone likelihood ratios, we identify the test
    statistic values $t$ that minimize the objective function
    $\EmpiricalCdfTestStatisticsNull(\threshold) - \lambda
\EmpiricalCdfTestStatistics(\threshold)$ within a neighborhood $[t -h_n, t+h_n]$, where $h_n$ denotes a vanishing bandwidth. If there are a finite number of test statistic values $\tau_{\lambda,l}^*$ such that $\localFDR(\tau_{\lambda,l}^*) = q$, an analogous convergence result to \Cref{prop:convergence.rate} can be obtained.
\end{remark}


\section{Counter-examples}\label{sec:example}

\subsection{Stochastic dominance and \PRDS}\label{sec:example.PRDS}

We provide an example to show that condition~\ref{PRDS:assu:null.nc.identical} in
\Cref{prop:PRDS} cannot be relaxed to the
stochastic dominance
condition~\ref{vali:assu:null.nc.conservative.general} in
\Cref{prop:pvalue.validity}.
Consider two true null test statistics $\testStatistics{1}$, $\testStatistics{2} \sim U[0,1]$, two internal negative control test statistics $\testStatistics{3}$, $\testStatistics{4} \sim \text{Beta}(1,2)$, and assume all variables are independent.
Then $\testStatistics{1}$, $\testStatistics{2}$ uniformly stochastically dominate $\testStatistics{3}$, $\testStatistics{4}$, but
$(\pval{i})_{1 \le i \le 2}$ is not PRD because
$\PP\left(\pval{2} = 1 \mid \pval{1} = \frac{1}{3}\right)
= \frac{4}{9}
> \frac{5}{12}
= \PP\left(\pval{2} = 1 \mid \pval{1} = \frac{2}{3}\right)$.
% Note that the super-martingale proof still applies to such non-\PRDS~\nickname~p-values.



% \subsection{Increased p-values and \BH~procedure}\label{sec:example.BH}

% We provide an example to show that the \BH~procedure applied to $(p_i)_{i \in \hypothesisIndex{}}$ controlling \FDR~does not imply the \FDR~control on a sequence of larger p-values $(p_i')_{i \in \hypothesisIndex{}}$, $p_i' \ge p_i$ for all $i$.
% Consider one true null p-value $\pval{1} \sim U[0,1]$, two non-null p-values $\pval{2}$, $\pval{3} \sim \text{Beta}(1,2)$, and assume all p-values are independent.
% The \BH~procedure augmented with the true proportion of nulls controls \FDR~at the exact target level $\alpha$ \cite{storey2004strong}.
% Define the event $\calA_\alpha
% := \left\{p_1 \le p_2 \le p_3,
% ~\pval{1} \le \alpha,
% ~\pval{2} > 2\alpha,
% ~\pval{3} \le 3\alpha \right\}$.
% Define another set of p-values as $(\pval{1}', \pval{2}', \pval{3}')
% = (\alpha, \pval{2}, 1)$ on $\calA_\alpha$ and $(\pval{1}', \pval{2}', \pval{3}')
% = (\pval{1}, \pval{2}, \pval{3})$ otherwise.
% Obviously, $\pval{i}' \ge \pval{i}$ for all $i$.
% On $\calA_\alpha$, the \BH~procedure applied to $(p_i)_{1 \le i \le 3}$ yields $R = 3$ rejections with $V = 1$ false positive, while the procedure applied to the larger sequence of p-values $(p_i')_{1 \le i \le 3}$ only yields $R = 1$ rejection which is false $(V=1)$. Therefore, $\FDR' = \FDR +  \frac{2}{3}\PP(\calA_\alpha) > \alpha$.
% \begin{align*}
%     \text{FDR}'
%     &= \EE\left[\frac{V'}{R' \vee 1} \right]
%     = \EE\left[\frac{V}{R \vee 1}, ~\calA_\alpha^c \right]
%     + \EE\left[\frac{V}{R \vee 1} + \frac{2}{3}, ~\calA_\alpha \right]\\
%     &= \text{FDR}\left((p_i)\right) +  \frac{2}{3}\PP(\calA_\alpha)
%     = \alpha +  \frac{2}{3}\PP(\calA_\alpha) > \alpha.
% \end{align*}


% To prove the \FDR~control of BH procedure applied to a certain type of p-values, we can not resort to constructing another set of p-values that are individually smaller p and on which BH controls \FDR.


% We discuss the equicorrelated multivariate normal (EMN) model, which is typical case of factor model with one latent factors. The analysis can be generalized to multiple factors straightforwardly.
% Suppose $\{\testStatistics{i}\} \cup \{\ncTestStatistics{j}\}_{1 \le j \le \NoNc}$ are jointly normal with the covariance matrix having one on the diagonal and $-1/(1 + \NoNc) \le \rho \le 1$ elsewhere.
% Assume that the means satisfy
% \begin{align*}
%     \mu_i = 0, \quad \hypothesis{i} \in \nullHypothesisSet, \quad \quad
%      \mu_i~\text{arbitrary}, \quad \hypothesis{i} \in \nonNullHypothesisSet, \quad\quad
%     \mu_j^{\text{nc}} = 0, \quad 1 \le j \le \NoNc.
% \end{align*}
% Then $\{\cdfTestStatistics{i}(\testStatistics{i})\} \cup \{\ncCdfTestStatistics{j}(\ncTestStatistics{j})\}_{1 \le j \le \NoNc}$ are dependent but exchangeable. Therefore, the \nickname~p-values are valid.
% The EMN model can be rewritten as a factor model: $\testStatistics{i} = \sqrt{\rho}Z + \sqrt{1-\rho}X_i$, $\ncTestStatistics{j} = \sqrt{\rho}Z + \sqrt{1-\rho}X_j$, where $Z$, $X_i$, and $\{X_j\}_{1 \le j \le \NoNc}$ are mutually independent standard normal random variables.


% Furthermore, suppose the means fulfill
% \begin{align*}
%     \mu_i = 0, \quad \hypothesis{i} \in \nullHypothesisSet, \quad \quad
%      \mu_i~\text{arbitrary}, \quad \hypothesis{i} \in \nonNullHypothesisSet, \quad\quad
%     \mu_j^{\text{nc}} = 0, \quad 1 \le j \le \NoNc.
% \end{align*}
% Then the \nickname~p-values $\{\pval{i}\}_{1 \le i \le \No}$ induced by $\{\testStatistics{i}\}_{1 \le i \le \No} \cup \{\ncTestStatistics{j}\}_{1 \le j \le \NoNc}$ are \PRDS~on $\left\{\pval{i}: \hypothesis{i} \in \nullHypothesisSet\right\}$. By \Cref{coro:FDR} we can combine the \BH~procedure with the \nickname~p-values.



\subsection{Fisher's method and permutation test}\label{sec:example.fisher}

We show directly combining Fisher's method with \nickname~p-values fails to control the type I error.
In \Cref{fig:fisher.test} panel (a), the theoretical null distribution $\chi_{2 \No}^2$ assuming independent p-values is inappropriate.
In contrast, the simulated null based on $1000$ permutations is
reasonably accurate (\Cref{fig:fisher.test} panel (b)).


\begin{figure}[tbp]
    \centering
    \begin{minipage}{8cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width  = 7cm]{plot/chisq.pdf}
    \subcaption{theoretical null}
    \end{minipage}
    \begin{minipage}{8cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width  = 7cm]{plot/ref.pdf}
    \subcaption{simulated null}
    \end{minipage}
    \caption{{Combining Fisher's method with \nickname~p-values.
    We generate $400$ test statistics for investigation and $400$ internal negative control test statistics independently from $\mathcal{N}(0,1)$, compute the \nickname~p-values and further the Fisher's test statistic.
    % To simulate the null distribution, we generate $400$ test statistics for investigation and $400$ internal negative control test statistics following i.i.d. $U[0,1]$, compute the test statistics of Fisher's method, and repeat $1000$ times.
    Panel (a) displays the quantile-quantile plot based on the theoretical null $\chi^2_{2\No}$ and panel (b) depicts the simulated null ($10^3$ permutations).
}}
    \label{fig:fisher.test}
\end{figure}



\subsection{\textcite[thm.\
  2]{bates21_testin_outlier_with_confor_p_values}}
\label{sec:bates-typo}

Theorem 2 in \cite{bates21_testin_outlier_with_confor_p_values} states that if $(\testStatistics{i})_{i \in \nullHypothesisIndex}$ is jointly independent and $(\testStatistics{i})_{i \in \nullHypothesisIndex} \independent (\testStatistics{j})_{j \in \hypothesisIndex{\text{nc}}}$, then $(\pval{i})_{i \in \hypothesisIndex{}}$ is \PRDS~on $(\pval{i})_{i \in \nullHypothesisIndex}$.
Surprisingly, this theorem does not impose any assumptions on
$(\testStatistics{i})_{i \in \hypothesisIndex{1}}$ (what
\textcite{bates21_testin_outlier_with_confor_p_values} call outliers). The
next simple counter-example shows that some assumptions on
$(\testStatistics{i})_{i \in \hypothesisIndex{1}}$ are indeed
necessary.   % with $\No = 2$ test statistics under investigation and
% $\NoNc = 1$ internal negative control.
In this example, $\nullHypothesisIndex = \{1\}$, $ \hypothesisIndex{1} =
\{2\}$, $\hypothesisIndex{\text{nc}} = \{3\}$,
$\testStatistics{1}$, $\testStatistics{3}$ are i.i.d.\ $U(0,1)$,
and $\testStatistics{2} = \1_{\{\testStatistics{1} < 0.5\}}$. For the
increasing set $\calD = \{\pvalAll: \pval{2} > 0.5\}$, we have
\begin{align*}
    \PP(\pvalAll \in \calD \mid \pval{1} = 0.5)
    &= \PP(\testStatistics{1} < 0.5 \mid \testStatistics{1} <
      \testStatistics{3}) = 2 \PP(\testStatistics{1} < 0.5, \testStatistics{1} < \testStatistics{3})
    = \frac{3}{4}, \\
    \PP(\pvalAll \in \calD \mid \pval{1} = 1)
    &= \PP(\testStatistics{1} < 0.5 \mid \testStatistics{1} \ge
      \testStatistics{3}) = 2 \PP(\testStatistics{1} < 0.5, \testStatistics{1} \ge \testStatistics{3})
    = \frac{1}{4}.
\end{align*}
Thus, the desired \PRDS~property is not true in this case.

A closer examination of Theorem 2 in
\cite{bates21_testin_outlier_with_confor_p_values} shows that a key
step in their proof (reproduced below using our notation) directly drops the
conditioning event $p_i = p$ without any justification:
\begin{align*}
    \PP\left(\pvalAll \in \calD \mid \pval{i} = p \right)
    = \EE_{\boldsymbol{Z}| \pval{i} = p}\left[ \PP\left(\pvalAll \in \calD \mid \boldsymbol{Z} \right)\right],
\end{align*}
where $\boldsymbol{Z}$ is the set of order statistics of
$(\testStatistics{j})_{j \in\hypothesisIndex{ \text{nc}}}$. This step
can be rectified by assuming that $(\testStatistics{i})_{i \in
  \hypothesisIndex{1}}$ is mutually independent and is independent of
the remaining test statistics.



\section{Additional numerical results}\label{sec:additional.simulation}

We report in \Cref{fig:nc.sample.size} the power of
\BH~\nickname~for $\No \in \{100, 200\}$, $\NoNonNull
\in \{10, 20\}$, $q \in \{0.1, 0.2\}$, and $\NoNc \in \{50, 100, 150,
\dotsc, 500\}$. We adopt the setting of independent p-values with
exact baseline p-values in \Cref{sec:simulation}. We do not report
the power of \BH, as it is numerically identical to \BH~oracle in this
case.


\begin{figure}[tbp]
    \centering
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-100-n1-20-q-20.pdf}
    \subcaption{$\No = 100, \NoNonNull = 20, q = 0.2, \No/(\NoNonNull q) = 25$}
    \end{minipage}
        \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-100-n1-20-q-10.pdf}
    \subcaption{$\No = 100, \NoNonNull = 20, q = 0.1, \No/(\NoNonNull q) = 50$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-100-n1-10-q-20.pdf}
    \subcaption{$\No = 100, \NoNonNull = 10, q = 0.2, \No/(\NoNonNull q) = 50$}
    \end{minipage}
            \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-100-n1-10-q-10.pdf}
    \subcaption{$\No = 100, \NoNonNull = 10, q = 0.1, \No/(\NoNonNull q) = 100$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-200-n1-20-q-20.pdf}
    \subcaption{$\No = 200, \NoNonNull = 20, q = 0.2, \No/(\NoNonNull q) = 50$}
    \end{minipage}
            \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-200-n1-20-q-10.pdf}
    \subcaption{$\No = 200, \NoNonNull = 20, q = 0.1, \No/(\NoNonNull q) = 100$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-200-n1-10-q-20.pdf}
    \subcaption{$\No = 200, \NoNonNull = 10, q = 0.2, \No/(\NoNonNull q) = 100$}
\end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-n-200-n1-10-q-10.pdf}
    \subcaption{$\No = 200, \NoNonNull = 10, q = 0.1, \No/(\NoNonNull q) = 200$}
\end{minipage}
    \caption{{Power analysis of the \BH~\nickname~with different numbers of internal negative control units. We vary $\No \in \{100, 200\}$, $\NoNonNull  \in \{10, 20\}$, and $q \in \{0.1, 0.2\}$. The number of internal negative controls varies from $50$ to $500$. The results are aggregated over $10^3$ trials. }}
    \label{fig:nc.sample.size}
  \end{figure}

Next, we carry out this power analysis with weaker non-nulls.
In particular, we adopt the setting of independent p-values with exact
baseline p-values in \Cref{sec:simulation} but set the marginal
distribution of the non-nulls to $\Phi(Z - 2)$. We vary $\No \in
\{100, 200\}$, $\NoNonNull  \in \{10, 20\}$, and $q \in \{0.1,
0.2\}$. We increase the internal negative control sample size from
$50$ to $1000$. The results are reported in
\Cref{fig:nc.sample.size.threshold.weak}. In this setting, we find
that the \BH~\nickname~requires $\NoNc \ge
5 \No/(\FDRLevel \NoNonNull)$ internal negative controls to achieve
comparable power as \BH~oracle.

\begin{figure}[tbp]
    \centering
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-100-n1-20-q-20.pdf}
    \subcaption{$\No = 100, \NoNonNull = 20, q = 0.2, \frac{\No}{\NoNonNull q} = 25$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-100-n1-20-q-10.pdf}
    \subcaption{$\No = 100, \NoNonNull = 20, q = 0.1, \frac{\No}{\NoNonNull q} = 50$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-100-n1-10-q-20.pdf}
    \subcaption{$\No = 100, \NoNonNull = 10, q = 0.2, \frac{\No}{\NoNonNull q} = 50$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-100-n1-10-q-10.pdf}
    \subcaption{$\No = 100, \NoNonNull = 10, q = 0.1, \frac{\No}{\NoNonNull q} = 100$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-200-n1-20-q-20.pdf}
    \subcaption{$\No = 200, \NoNonNull = 20, q = 0.2, \frac{\No}{\NoNonNull q} = 50$}
    \end{minipage}
            \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-200-n1-20-q-10.pdf}
    \subcaption{$\No = 200, \NoNonNull = 20, q = 0.1, \frac{\No}{\NoNonNull q} = 100$}
    \end{minipage}
    \begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width  = 5cm]{plot/m-nc-independent-power-weak-n-200-n1-10-q-20.pdf}
    \subcaption{$\No = 200, \NoNonNull = 10, q = 0.2, \frac{\No}{\NoNonNull q} = 100$}
\end{minipage}
\begin{minipage}{6.7cm}
    \centering
\includegraphics[clip, trim = 0cm 0cm 0cm 1cm, width = 5.6cm]{plot/m-nc-independent-power-weak-n-200-n1-10-q-10.pdf}
    \subcaption{$\No = 200, \NoNonNull = 10, q = 0.1, \frac{\No}{\NoNonNull q} = 200$}
\end{minipage}
    \caption{{Power analysis of the \BH~\nickname~with weaker non-nulls and different numbers of internal negative control units. We vary $\No \in \{100, 200\}$, $\NoNonNull \in \{10, 20\}$, and $q \in \{0.1, 0.2\}$. The internal negative control sample size varies from $50$ to $1000$. The results are aggregated over $10^3$ trials.}}
    \label{fig:nc.sample.size.threshold.weak}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width = \textwidth]{plot/empirical-null.pdf}
  \caption{A comparison of the distribution of p-values for the
    proteomic application obtained from the empirical null
    distributions described in \Cref{sec:choice-empir-null}. It is
    expected that, when using the correct null distribution, p-values
    between 0.5 and 1 should be nearly uniformly distributed.}
  \label{fig:empirical-null}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.5\textwidth]{plot/qq-efron.pdf}
  \caption{A quantile-quantile plot of the negative control statistics
    against the empirical null distribution $\text{N}(-0.02, 0.18)$
    obtained by Efron's method. The solid line corresponds to the diagonal.}
  \label{fig:qq-efron}
\end{figure}