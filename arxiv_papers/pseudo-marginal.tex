While the particle Gibbs approach to sampling from \eqref{eq:auxiliary-feynman-kac} is perhaps the most natural, it is also possible to instead consider a pseudo-marginal approach~\citep{andrieu2009pseudomarginal} as given by the particle marginal Metropolisâ€“Hastings (PMMH) sampler of \citet{Andrieu2010particle}. Consider a proposal distribution $q(\dd{u}'_{0:T} \mid u_{0:T})$, for example, $\prod_{0}^T \mathcal{N}(u'_t; u_t, \frac{\delta}{2}\Sigma_t)$. Similarly to PMMH, because sequential Monte Carlo provides an unbiased estimate $\hat{\mathcal{Z}}_N(u_{0:T)}$ of the normalising constant for $\pi(x_{0:T} \mid u_{0:T})$, we can marginally target $\pi(x_{0:T})$ using a PMMH methodology. We succintly summarise this extension in Algorithm~\ref{alg:auxiliary-pmcmc}.
\begin{algorithm}[!htb]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Auxiliary pseudo-marginal sampler}\label{alg:auxiliary-pmcmc}
    \KwResult{An updated trajectory $x^{k+1}_{0:T}$}
    \Fn{\textsc{aux-pm}$\big(x^k_{0:T}$, $u^k_{0:T},$ $\hat{\mathcal{Z}}^k_N\big)$}{
        Sample $u'_{0:T} \sim q(\cdot \mid u^k_{0:T})$\label{step:pmcmc-1}\;
        Sample $x'_{0:T}$ and $\hat{\mathcal{Z}}'_N$ using a particle filter targeting $\pi(\ft{x}{0}{T} \mid \ft{u'}{0}{T})$\label{step:pmcmc-2}\;
        Set $x^{k+1}_{0:T}$ to $x'_{0:T}$ with probability $\frac{\hat{\mathcal{Z}}'_N q(u^k_{0:T} \mid u'_{0:T})}{\hat{\mathcal{Z}}^k_N q(u'_{0:T} \mid u_{0:T})}$, otherwise, set it to $x^k_{0:T}$\;
        \Return{$x^{k+1}_{0:T}$}
    }
\end{algorithm}

This method is related to the correlated pseudo-marginal method of \citet{Deligiannidis2018correlated}, which instead considers the extended distribution
\begin{equation}
    \label{eq:other-auxiliary-feynman-kac}
    \pi(\ft{x}{0}{T}, \ft{u}{0}{T})
    \propto g_0(x_0) p_0(x_0) \left\{\prod_{t=1}^T g_t(x_t, x_{t-1}) \, p_t(x_t \mid x_{t-1}) \right\}\left\{\prod_{t=0}^T \mathcal{N}\left(u_t; 0, I\right)\right\},
\end{equation}
where the $u_t$s represent Gaussian variables used to generate particles (and sample from the ancestors) within the sequential Monte Carlo routine. By correlating these over time, they show that the pseudo-marginal algorithm can be made to scale better with time series of increasing lengths $T$. This is because it results in correlated likelihood ratios $\frac{\hat{\mathcal{Z}}'_N}{\hat{\mathcal{Z}}^k_N}$ which exhibit lower variance than they would have otherwise.

By using a proposal distribution adapted to the auxiliary target at hand, in a similar spirit as for the auxiliary particle Gibbs sampler of Algorithm~\ref{alg:aux-pgibbs}, we can hope to also benefit from a reduced variance of the likelihood estimates ratio in Algorithm~\ref{alg:auxiliary-pmcmc}. This, however, is not because the two estimates are correlated, but rather because they will both exhibit lower variance individually than their non-augmented counterparts.
Contrary to \citet{Deligiannidis2018correlated}, this method necessitates evaluation of the full (unnormalised) density of the Feynman--Kac model at hand, and will likely not perform well for a very large $T$. On the other hand, and in contrast to the correlated pseudo-marginal method~\citep[see the comments in Theorem 3 and Section 5.3]{Deligiannidis2018correlated}, Algorithm~\ref{alg:auxiliary-pmcmc} is likely to perform well in higher dimensions, due to the localisation of the proposals. We leave the study of this question open for future work and focus on the auxiliary particle Gibbs algorithm in the remainder of this article.
