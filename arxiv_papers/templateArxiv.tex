\pdfoutput=1
\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}% graphics
\usepackage{verbatim,listings} 
%% recommended packages
% \usepackage{orcidlink}
\usepackage{xcolor,thumbpdf,lmodern,rotating}
% for \mathbb
\usepackage{amsfonts,amssymb,pifont}
%% another package (only for this demo article)
\usepackage{amsmath,mathtools}
%% packages I added myself
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{cleveref}
\usepackage{float}
\graphicspath{{images/}}     % organize your images and other figures under media/ folder
\usepackage[]{apacite}
\bibliographystyle{apacite}
\providecommand{\BIBand}{and}
 
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 


%% new custom commands
\newcommand{\class}[1]{`\textbf{#1}'}
\newcommand{\fct}[1]{\textbf{#1()}}

% Update your Headers here
\fancyhead[LO]{GEMAct: a Python package for non-life (re)insurance modeling}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}

  
%% Title
\title{GEMAct: a Python package for non-life (re)insurance modeling
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Gabriele Pittarello \\
  Universit\`{a} `La Sapienza'\\
  Roma\\
  \texttt{gabriele.pittarello@uniroma1.it} \\
  %% examples of more authors
   \And
 Edoardo Luini\\
 Universit\`{a} Cattolica del Sacro Cuore\\
  Milano\\
  \texttt{edoardo.glaucoluini@unicatt.it} \\
     \And
  Manfred Marvin Marchione\\
 Universit\`{a} `La Sapienza'\\
  Roma\\
  \texttt{manfredmarvin.marchione@uniroma1.it} \\
}


\begin{document}
\maketitle

\begin{abstract}
This paper introduces \textbf{gemact}, a \textbf{Python} package for actuarial modeling based on the collective risk model. The library supports applications to costing and risk transfers, risks aggregation, and claims reserving. We add new probability distributions to those available in \textbf{scipy}, including the (a,b,0) and (a,b,1) discrete distributions, the Archimedean and the Elliptical classes of copulas.
We provide a software implementation of the AEP algorithm for calculating the cumulative distribution function of the sum of dependent, non-negative random variables, given their dependency structure specified with a copula. The theoretical framework is introduced in brief at the beginning of each section to provide the reader with a sufficient understanding of the underlying actuarial models.
\end{abstract}

\keywords{insurance, collective risk model, costing, claims reserving, risk aggregation, Python}


\section[Introduction]{Introduction} \label{sec:intro}

In non-life insurance, accurately representing and quantifying future losses is a foundational task, central to several areas ranging from pricing, reserving to risk management. Indeed, the actuarial literature is rich in models that are relevant in such applications. Among those, the collective risk model has been widely studied as it is mathematically tractable, it requires little and general information, and it can be implemented efficiently (\cite{klugman98}, \cite{embrechts09} and \cite{parodi14}). In particular, by knowing the frequency and severity distributions of the losses, it is possible to compute the distribution of the aggregate (total) losses.
Being the collective risk model the common thread of this work, we developed \textbf{gemact} to provide a collection of tools for (re)insurance modeling under a unified formal framework. The actuarial theory required to understand the package applications is introduced in brief at the beginning of each section with a sufficient number of references for the reader interested in an in-depth analysis.

In the recent years, programming languages and statistical computing environments, such as \textbf{Python} \cite{python} and \textbf{R} \cite{R}, have become increasingly popular, see \cite{ozgur22}. Currently, coding skills form part of the body of knowledge of actuaries and actuarial science researchers.
In \textbf{R}, an extensive implementation for aggregate loss modeling based upon the collective risk theory is the \textbf{actuar} package, see \cite{actuar1} and \cite{actuar2}.
Conversely, no comprehensive library based upon the collective risk model is still available in \textbf{Python}, despite its growing user base and the practical relevance of such model.
At the same time, an available solution in Python is the \textbf{aggregate} package, which implements fast distributions computations via Fast Fourier Transform (FFT) \cite{aggregatepackage}.

With regards to claims reserving, \textbf{chainladder} offers in \textbf{Python} standard aggregate reserving techniques, like deterministic and stochastic chain ladder methods, the Bornhuetter-Ferguson model, and the CapeCod model, see \cite{chainladderPypackage}. In addition, an extensive \textbf{R} library for claims reserving is the \textbf{ChainLadder} package from \cite{chainladderR}. Furthermore, \textbf{apc} provides the family of age-period-cohort for reserving. This package is available in both the above-mentioned programming languages \cite{apcpackage}. 

At last, when it comes to the topic of dependence modeling via copulae, in \textbf{Python} one can use \textbf{copulas} and \textbf{copulae} packages (see \cite{copulaepackage} and \cite{copulaspackage}). Similarly, copula features in \textbf{R} are implemented in \textbf{copula}, see the package and its extensions in \cite{copulaR1}, \cite{copulaR2} and \cite{copulaR3}. 

This paper presents the contribution of the \textbf{gemact} package to the \textbf{Python} community.
In \Cref{sec:lossmodel} we introduce the statistical framework of the collective risk model. There, we define the aggregate loss distribution as a random sum of i.i.d. random variables that can be computed using the recursive formula in \cite{panjer81}, the Fast Fourier Transform in \cite{heckman83} or via a Monte Carlo simulation approach \cite[p.~467]{klugman98}. Once the aggregate distribution is available, its expected value can be used for risk costing and pure premium calculation. In this respect, the package supports typical coverage modifiers like (individual and aggregate) deductibles, limits, and reinstatements, see \cite{Sundt93}. In this section we also introduce new distributions added to those of \textbf{scipy} \cite{scipy}. In particular, we implemented the (a,b,0) and (a,b,1) families of distributions for describing the loss frequency \cite[p.~505]{klugman98}, and further continuous distributions to model the loss severity, such as the Generalized Beta \cite[p.~493]{klugman98}. Also, we consider different methods for the discretization of continuous distributions \cite{gerber82}.
\newline
Often, it is necessary to model the sum of a fixed number of dependent random variables. In order to do so, as detailed in \Cref{sec:lossaggregation}, we provide the first open-source software implementation of the AEP algorithm \cite{arbenz11} for evaluating the cumulative distribution function of a sum of random variables with a given dependency structure. This can be specified with the copulas we implemented \cite{nelsen07}. 
\newline
Lastly, assuming a collective risk model holds for the cells of a run-off triangle, it is possible to define the stochastic claims reserving model in \cite{ricotta16} and \cite{clemente19}. In this case, the user gets the information about the frequency and severity parameters of the cells from the Fisher-Lange method \cite{fisher99}. Both these approaches are described in \Cref{sec:lossreserve} and implemented in our library accordingly.


\section{Loss model}
\label{sec:lossmodel}

Within the framework of the collective risk model \cite{embrechts09}, all random variables are defined on some fixed probability space $(\Omega, \mathcal{F}, P)$. Let
\begin{itemize}
    \item $N$ be a random variable taking values in $\mathbb{N}_0$ representing the claim frequency.
    \item $\left\{ Z_i\right\}_{i \in \mathbb{N}}$ be a sequence of i.i.d non-negative random variables independent of $N$; $Z$ is the random variable representing the individual (claim) loss.
\end{itemize}

The aggregate loss $X$, also referred to as aggregate claim cost, is

\begin{equation}
\label{eq:lossmodel}
X=\sum_{i=1}^{N} Z_i,
\end{equation}

with $\sum_{i=1}^{0} Z_i=0$. Details on the distribution functions of $N$, $Z$ and $X$ are discussed in the next sections.
\Cref{eq:lossmodel} is often referred to as the frequency-severity loss model representation. This can encompass common coverage modifications present in (re)insurance contracts \cite[p.~50]{parodi14}. More specifically, let us consider:

\begin{itemize}

    \item For $a \in [0, 1]$, the function $Q_a$ apportioning  the aggregate loss amount: 
    \begin{equation}
        \label{eq:qs}
       Q_a (X)= a X.
    \end{equation}
    
    \item  For $c,d \geq 0$, the function $L_{c, d}$ applied to the individual claim loss: 
    
     \begin{equation}
         \label{eq:xl}
             L_{c, d} (Z_i) = \min \left\{\max \left\{0, Z_i-d\right\}, c\right\}. %, \qquad c,d \geq 0. 
         \end{equation}
    
    Herein, for each and every loss, the excess to a \textit{deductible} $d$ (sometimes referred to as \textit{priority}) is considered up to a \textit{cover} or \textit{limit} $c$. Following the notation in \cite[p.~34]{albrecher17}, we denote $ [d, d+c ]$ as layer, a similar notation can be found in \cite{ladoucette06} and \cite[p.~46]{parodi14}. Similarly to the individual loss $Z_i$, \Cref{eq:xl} can be applied to the aggregate loss $X$.

  
    
\end{itemize}

Computing the aggregate loss distribution is relevant for several actuarial applications \cite[p.~93]{parodi14}. The \textbf{gemact} package provides a set of tools for calculating the distribution of the loss model in \Cref{eq:lossmodel}, including the above-mentioned transformations of \Cref{eq:xl} and \Cref{eq:qs}, and their combinations. 

\subsection{Risk Costing}

In this section, we describe an application of the collective risk model in \Cref{eq:lossmodel}. The expected value of the aggregate loss of a portfolio constitutes the building block of an insurance tariff. This expected amount is called pure premium and its calculation is referred as risk costing \cite[p.~282]{parodi14}. Insurers frequently cede parts of their losses to reinsurers, and risk costing takes this transfers into account. Listed below are some examples of basic reinsurance contracts whose pure premium can be computed using \textbf{gemact}.

\begin{itemize}
    \item The \textit{Quota Share (QS)}, where a share $a$ of the aggregate loss ceded to the reinsurance (along with the respective premium) and the remaining part is retained.
    
    \begin{equation}
        P^{QS} = \mathbb{E}\left( Q_a \left( X \right)\right)
    \end{equation}
    
    \item The \textit{Excess-of-loss (XL)}, where the insurer cedes to the reinsurer each and every loss exceeding a deductible $d$, up to an agreed limit or cover $c$, with $c,d \geq 0$: 
    \begin{equation}
        P^{XL} =  \mathbb{E}\left( \sum_{i=1}^{N} L_{c,d} (Z_i) \right).
    \end{equation}
    
\item The \textit{Stop Loss (SL)}, where the reinsurer covers the aggregate loss exceedance of a (aggregate) deductible $v$, up to a (aggregate) limit of cover $u$, with $u,v \geq 0$:
    \begin{equation}
    \label{eq:sl}
        P^{SL} = \mathbb{E}\left( L_{v, u} (X) \right).
    \end{equation} 
\end{itemize}

The model introduced by \Cref{eq:lossmodel} and implemented in \textbf{gemact} can be used for costing contracts like the \textit{excess-of-loss with reinstatements (RS)} in \cite{Sundt93}. Assuming the aggregate cover equal to $ (K + 1) c $, with $K \in \mathbb{Z}^+$:

\begin{equation}
\label{eq:reinstatementsP}
    P^{RS} =  \frac{\mathbb{E}\left( L_{u, v} (X) \right)}{1+\frac{1}{c} \sum_{k=1}^K l_k \mathbb{E}\left( L_{c, (k-1)c+v}(X) \right)  },
\end{equation}


where $K$ is the number of reinstatements layers and $l_k \in [0, 1]$ is the reinstatement layer loading, with $k=1, \ldots, K$. When $l_k = 0$, the $k$-th resinstatement is free. 
\newline

\subsection{Computational Methods}
\label{sec:computationalmethods}

The distribution function of the aggregate loss in \Cref{eq:lossmodel} is:
\begin{align}
\label{eq:lossmodelcdf}
    F_X(x)=\operatorname{Pr}(X \leq x)=\sum_{k=0}^{\infty} p_k F_Z^{* k}(x)
\end{align}
where $p_k=P(N=k)$, $F_Z(x)=P\left[ Z \leq x \right]$ and $F_Z^{* k}(x)=P\left[ Z_1 + \ldots + Z_n \leq x \right]$. 
\newline

Moreover, the characteristic function of the aggregate loss $\phi_{X}: \mathbb{R} \rightarrow \mathbb{C}$ can be expressed in the form:

\begin{equation}\label{eq:aggchar}\phi_{X}(t)=\mathcal{P}_{N}\left(\phi_{Z}(t)\right),\end{equation}

where $\mathcal{P}_{N}(t)=\mathrm{E}\left[t^N\right]$ is the probability generating function of the frequency $N$ and  $\phi_{Z}(t)$ is the characteristic function of the severity $Z$ \cite[p.~153]{klugman98}.
\newline 

The distribution in Equation \ref{eq:lossmodelcdf}, except in a few cases, cannot be computed analytically and its direct calculation is numerically expensive \cite[p.~239]{parodi14}. For this reason, different approaches have been analyzed to approximate the distribution function of the aggregate loss including Parametric and Numerical Quasi-Exact Methods (for a detailed treatment see \cite{Shevchenko:2010}). Amongst the latter, \textbf{gemact} implements \textit{Monte Carlo} simulation \cite[p.~467]{klugman98}, \textit{Discrete Fourier Transform} \cite{heckman83} and the so-called \textit{Recursive Formula} \cite{panjer81}. 
This section details these last two computational methods based on discrete mathematics to approximate the aggregate loss distribution.

Henceforth, let us assume, for $j=0,1,2,\ldots,m-1$ and $h>0$, an arithmetic severity distribution with probability sequence:

\begin{align*}
    \mathbf{\{f\}}=\{f_0, f_1, \ldots , f_{m-1}\},
\end{align*}


where $f_j=P(Z=j\cdot h)$. 
The discrete version of \Cref{eq:lossmodelcdf} becomes

$$
g_s=\sum_{k=0}^{\infty} p_k f_s^{* k}
$$

where $g_s = P(X=s)$ and 

\[
f_s^{* j}:= \begin{cases}1 & \text { if } k=0 \text { and } s=0 \\ 0 & \text { if } k=0 \text { and } s \in \mathbb{N} \\ \sum_{i=0}^s f_{s-i}^{*(k-1)} f_i & \text { if } k > 0\end{cases}
\]


\subsubsection{Discrete Fourier Transform}

The \textit{Discrete Fourier transform} (DFT) of the severity $\mathbf{\{f\}}$ is, for $k=0,...,m-1$, the sequence

$$\mathbf{\{\hat{f}\}}= \{\hat{f}_0, \hat{f}_1, \ldots, \hat{f}_{m-1} \},$$

where

\begin{equation}
\label{eq:DFT}
\widehat{f}_k=\sum_{j=0}^{m-1}f_j\cdot e^{-\frac{2\pi i k j}{m}}.
\end{equation}

The original sequence can be reconstructed with the inverse DFT:

$$f_j=\frac{1}{m} \sum_{k=0}^{m-1} \widehat{f_k} e^{-i 2 \pi j k / m}$$

The sequence of probabilities $\mathbf{\{g\}}=\{g_0, g_1, \ldots, g_{m-1}\}$ can be approximated taking the inverse DFT of 

\begin{equation}
\mathbf{\{ \hat{g}\}}\coloneqq \mathcal{P}_{N}\left(\mathbf{\{\hat{f}\}}\right).
\end{equation}

\subsubsection{Recursive Formula}

Assume that the frequency distribution belongs to the $(a,b,0)$ family, i.e. for $k \geq 1$ and $a,b\in\mathbb{R}$:

\begin{equation}
    \label{eq:ab0def}
    p_k=\left(a+\frac{b}{k}\right)p_{k-1}
\end{equation}

Here, $p_0$ is an additional parameter of the distribution \cite[p.~505]{klugman98}. The $(a,b,0)$ class can be generalized to the $(a,b,1)$ class assuming that the recursion \eqref{eq:ab0def} holds for $k=2,3,\ldots$.
\newline

The recursive formula was developed to efficiently compute the distribution of the aggregate loss when the frequency distribution belongs to the $(a,b,0)$ or the $(a,b,1)$ class. And herein the sequence of probabilities $\mathbf{\{{g}\}}=g_0, g_1, \ldots , g_{m-1}$ can obtained recursively:

\begin{equation}
\label{eq:rec}
    g_s=\frac{\left[p_1-(a+b) p_0\right] f_s+\sum_{j=1}^s(a+b j / s) f_j g_{s-1}}{1-a f_0},
\end{equation}

with $1 \leq s \leq m-1$ and given the initial condition $g_0=\mathcal{P}_{N}\left(f_0\right)$.


\subsection{Severity Discretization}

The calculation of the aggregate loss with DFT or the \textit{recursive formula} requires an arithmetic severity distribution \cite{embrechts09}. Conversely, the severity distribution in \Cref{eq:lossmodel} is often calibrated on a continuous support. This section illustrates the methods for the discretization of a continuous severity distribution available in the \textbf{gemact} package. 

Let $F_z: \mathbb{R^+} \rightarrow [0, 1]$ be the cumulative distribution function (cdf) of the distribution to be discretized. 
For a span $h>0$ and an integer $m$, a probability $f_j$ is assigned to each point $j \cdot h$, $j= 0, \ldots ,m$. Four alternative methods for determining the values for $f_j$ are implemented in \textbf{gemact}.

\begin{enumerate}
    \item The method of \textit{mass dispersal}:
    

    \begin{align*}
    f_j= \begin{cases}F_Z \left( \frac{h}{2} \right), & j=0 \\ F_Z\left( hj+ \frac{h}{2} \right)-F_Z\left(hj-\frac{h}{2}\right) &  j=1,\ldots,m-2\\
     1-F_Z\left(hj-\frac{h}{2}\right) &  j=m-1 \end{cases}
    \end{align*}

    \item The method of the \textit{lower discretization}:
    

    \begin{align*}
    f_j= \begin{cases}F_Z \left( 0 \right), & j=0 \\ F_Z\left( hj\right)-F_Z\left(hj-h\right) &  j=1,\ldots,m-2\\
     1-F_Z\left(hj\right) &  j=m-1 \end{cases}
    \end{align*}

\item The method of the \textit{upper discretization}:

    \begin{align*}
    f_j= \begin{cases}F_Z\left( hj+ h \right)-F_Z\left(hj\right) &  j=1,\ldots,m-1\\
     1-F_Z\left(hj\right) &  j=m \end{cases}
    \end{align*}
    
    \item The method of \textit{local moment matching}: 
    
    \begin{align*}
    f_j= \begin{cases}1-\frac{\mathrm{E}[Z\wedge h] }{h}, & j=0 \\ \frac{2 \mathrm{E}[Z \wedge h j]-\mathrm{E}[Z \wedge h (j-1) ]-\mathrm{E}[Z \wedge h (j+1) ]}{h} &  j=1,\ldots,m-1.\end{cases}
    \end{align*}

    where $\mathrm{E}[Z\wedge h] = \int_{-\infty}^h t \; dF_z(t)+h[1-F_z(h)]$.


\end{enumerate}

The above-mentioned discretization methods are modified accordingly to reflect the cases where the transformation of \Cref{eq:xl} is applied to the severity \cite[p.~517]{klugman98}. Below, we illustrate how to implement the  severity discretization in \textbf{gemact}.

Once a continuous distribution is selected from those supported in our package (see \Cref{app:alldists}) the severity distribution is defined via the \texttt{Severity} class. 

\begin{lstlisting}
Python> from gemact.lossmodel import Severity
Python> severity = Severity(
... dist='gamma',
... par={'a': 5}
... )
\end{lstlisting}

It can be noted that the \texttt{dist} argument contains the name of the distribution, and 
the \texttt{par} argument specifies, as a dictionary, the distribution parameters. In the latter, each item key-value pair represents a distribution parameter name and its value. Refer to \texttt{distributions} module for a list of the distribution names and their parameter specifications.

The \texttt{discretize} method of the \texttt{Severity} class allows to return the discrete severity probability sequence according to the approaches described above. Below, we provide an example for mass dispersal. After the discretization is achieved, we calculate the mean of the discretized distribution.

\begin{lstlisting}
Python> import numpy as np
Python> massdispersal = severity.discretize(
... discr_method='massdispersal',
... n_discr_nodes=50000,
... discr_step=.01,
... deductible=0,
... cover=100
... )
... discrete_mean = np.sum(massdispersal['nodes']*massdispersal['fj'])
... print('Mean of the discretized distribution:', discrete_mean)
Mean of the discretized distribution: 4.999999999999998
\end{lstlisting}

In order to perform the discretization the following arguments are needed: 

\begin{itemize}
    \item The number of nodes ($m$) set in the \texttt{n\_discr\_nodes} argument. 
    \item The severity discretization step ($h$) is in the \texttt{discr\_step} argument. 
    \item Cover modifications when necessary. In the example, the \texttt{deductible} ($d$) is set to zero, and the the \texttt{cover} ($c$) is set to 100.
\end{itemize}

When distribution modifications are present, \textbf{gemact} automatically adjusts the discretization step parameter to $h=\frac{c}{m}$ to have the correct number of nodes in the new distribution support.


Additionally, using the \texttt{plot\_discretized\_severity\_cdf} method, one could visually examine the discretized severity.
Further graphical parameters of \textbf{matplotlib} \cite{Hunter:2007}  \texttt{pyplot} collection are admissible. An example of output is shown in \Cref{fig:discreteseverity}.

\begin{lstlisting}
Python> discr_sev_plot= severity.plot_discr_sev_cdf(discr_method = 'massdispersal',
... n_discr_nodes = 50,
... discr_step = 1,
... deductible = 0,
... cover = 40)
\end{lstlisting}

\subsection{Supported Distributions}

The \textbf{gemact} package makes for the first time the $(a,b,0)$ and $(a,b,1)$ distribution families \cite[p~81]{klugman98} available in \textbf{Python}.
In the following code chunk we show how to use our implementation of the Zero-truncated Poisson, by importing the \texttt{distributions} module from \textbf{gemact}.

\begin{lstlisting}
Python> from gemact import distributions
Python> ztpois = distributions.ZTPoisson(mu=2)
\end{lstlisting}

Each distribution supported in \textbf{gemact} has various methods and can be used in a similar fashion to any \textbf{scipy} distribution.
Next, it is shown how to compute the Monte Carlo based approximated mean, with the random generator method fot the \texttt{ZTPoisson} class.

\begin{lstlisting}
Python> import numpy as np
Python> random_variates = ztpois.rvs(int(1e+05), random_state=1)
Python> print('Mean of the simulated values: ', np.mean(random_variates))

Mean of the simulated values: 2.3095
\end{lstlisting}

Furthermore, supported copula functions can be accessed via the \texttt{copulas} module. Below, we compute the cumulative density function of a two-dimensional Clayton copula.

\begin{lstlisting}
Python> from gemact import copulas
Python> import numpy as np
Python> clayton_copula = copulas.ClaytonCopula(par=1.2, dim=2)
Python> values = np.array([[.5, .5]])
Python> print('Clayton copula cdf ', clayton_copula.cdf(values)[0])


Clayton copula cdf  0.3443010799531017
\end{lstlisting}

In the above example, it is noted that the copula parameter and dimension are defined by means of the \texttt{par} and the \texttt{dim} arguments correspondingly.
The argument of the \texttt{cdf} method must be a \textbf{numpy} array whose dimensions meet the following requirements. Its first component is the number of points where the function shall be evaluated, its second component equals the copula dimension (\texttt{values.shape} of the example is in fact \texttt{(1,2)}).

The complete list of the distributions supported by \textbf{gemact} is available in \Cref{app:alldists}, \Cref{tab:alldists} and \Cref{tab:allcopulas}.

\subsection{Illustration}

This subsection continues and complements the illustration of \textbf{gemact} implementations concerning the collective risk model. As an overview, \Cref{fig:lossmodeldiagram} schematises the class diagram of the \texttt{lossmodel} module, by highlighting the structure and the dependencies of the \texttt{LossModel} class.

\begin{figure}
\centering
    \includegraphics[width=15cm]{images/lossmodel_diagram_2.png}
    \caption{Class diagram of the \textbf{gemact} \texttt{lossmodel} module. A rectangle represents a class; an arrow connecting two classes indicates that the target class uses the origin class. In this case, a \texttt{LossModel} object entails  \texttt{Frequency}, \texttt{Severity} and \texttt{PolicyStructure} class instances. These correspond to the frequency model, the severity model and the policy structure, respectively. The latter, in particular, is in turn specified via one or more \texttt{Layer} objects, which include cover modifiers of each separate policy component.}
    \label{fig:lossmodeldiagram}
\end{figure}

In this respect, the \texttt{Frequency} class is included in the initialization of the \texttt{LossModel} class and defines the frequency component.
%
As per the \texttt{Severity} class, \texttt{dist} identifies the name of the distribution, and 
\texttt{par} specifies, as a dictionary, its parameters. In the latter, each item key-value pair represents a frequency distribution parameter name and value. Again, please refer to \texttt{distributions} module for a list of the distribution names and their parameter specifications.
Below, we use the \texttt{severity} object we instanced in the previous sections.

\begin{lstlisting}
Python> from gemact.lossmodel import Frequency
Python> frequency = Frequency(
...  dist='poisson',
...  par={'mu': 4}
...  )
\end{lstlisting}

The loss model, once defined the parametric assumptions on frequency and severity component, is determined and computed via the \texttt{LossModel} class. Specifically, \texttt{Frequency} and \texttt{Severity} objects are assigned to \texttt{frequency} and \texttt{severity} arguments of \texttt{LossModel} as follows:

\begin{lstlisting}
Python> from gemact import LossModel
Python> lm_mc = LossModel(
... frequency=frequency,
... severity=severity,
... aggr_loss_dist_method='mc',
... n_sim=1e+04,
... random_state=1
... )

INFO:lossmodel|Approximating aggregate loss distribution
               via Monte Carlo simulation
INFO:lossmodel|MC simulation completed
\end{lstlisting}

In the previous example, in more detail, \texttt{lm\_mc}  involves a Monte Carlo based approach for the calculation of the aggregate loss distribution. This is set via the \texttt{aggr\_loss\_dist\_method} equal to \texttt{`mc'}.
The additional parameters required for the simulation are: 

\begin{itemize}
    \item the number of simulations \texttt{n\_sim},
    
    \item the (pseudo-) random number generator initializer \texttt{random\_state}.
\end{itemize}

The cumulative distribution function of the aggregate distribution can be inspected with the \texttt{plot\_dist\_cdf} method. Its output is visualized in \Cref{fig:aggrloss}.

\begin{lstlisting}
Python> agg_dist_plot= lm_mc.plot_dist_cdf(color='orange')
\end{lstlisting}

\begin{figure}
\centering
\caption{Illustration of the discretized severity and the aggregate loss pertaining to a loss model with Poisson frequency ($\text{\texttt{mu}}=4$) and Gamma severity ($\text{\texttt{a}}=5$). Plot (a) shows the cdf of the discretized severity, assuming $d=0, c= 50$. Plot (b) displays the aggregate loss cdf. No aggregate coverage modifiers are present.}
\begin{subfigure}[b]{0.45\textwidth}
    \centering\includegraphics[width=8cm]{images/discretized_severity_cdf.png}
    \caption{}
    \label{fig:discreteseverity}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
    \centering\includegraphics[width=8cm]{images/agg_loss_cdf.png}
    \caption{}
    \label{fig:aggrloss}
\end{subfigure}

\end{figure}


Moreover, a recap of the loss model computation can be printed with the \texttt{print\_aggr\_loss\_specs} method.

\begin{lstlisting}
Python> lm_mc.print_aggr_loss_method_specs()
\end{lstlisting}

\begin{lstlisting}
Aggregate Loss Distribution: layer 1 
====================================================================
                          Quantity           Value 
====================================================================
       Aggregate loss dist. method              mc
              Number of simulation          100000
                      Random state               1
\end{lstlisting}

It is possible to describe the moments of the aggregate loss distribution with the \texttt{moment} method. The \texttt{central} and \texttt{n} arguments specify, respectively, whether the moment is central and the order of the moment. 

\begin{lstlisting}
Python> lm_mc.moment(central=False, n=1)

19.950968442875176
\end{lstlisting}

The aggregate loss mean, standard deviation and skewness can be accessed with the \texttt{mean}, \texttt{std}, and \texttt{skewness} methods, respectively. The code below shows that the result for the mean is consistent with the previous example.

\begin{lstlisting}
Python> lm_mc.mean()

19.963155575580227
\end{lstlisting}


Furthermore, for the aggregate loss distribution, it is possible to simulate random variates via \texttt{rvs} method, and compute the quantile function via the \texttt{ppf} method. Equally, the cumulative distribution function is computed with the \texttt{cdf} method.
Below, an example of the \texttt{ppf} method.

\begin{lstlisting}
Python> lm_mc.ppf(q=[.8,.7])

array([28.83665586, 24.85836788])
\end{lstlisting}

The code below shows the costing of a XL reinsurance contract. Policy modifiers are set in the \texttt{PolicyStructure} class.

\begin{lstlisting}
from gemact.lossmodel import PolicyStructure, Layer

Python> policystructure = PolicyStructure(
... layers=Layer(
... cover=50,
... deductible=2)
... )
\end{lstlisting}

More precisely, in the \texttt{Layer} class the contract \texttt{cover} ($c$) and the \texttt{deductible} ($d$) are provided.
Once the model assumptions are defined and the policy structure is specified, the aggregate loss distribution can be computed.

\begin{lstlisting}
Python> lm_XL = LossModel(
... frequency=frequency,
... severity=severity,
... policystructure=policystructure,
... aggr_loss_dist_method='fft',
... sev_discr_method='massdispersal',
... n_sev_discr_nodes=int(10000),
... sev_discr_step=.01,
... n_aggr_dist_nodes=int(100000)
... )

INFO:lossmodel|Approximating aggregate loss distribution via FFT
INFO:lossmodel|FFT completed
\end{lstlisting}

It can be noted that, in the previous code chunk, we determined the aggregate loss distribution with \texttt{`fft'} as \texttt{aggr\_loss\_dist\_method}. In such case, as already described, \texttt{LossModel} requires additional arguments defining the computation process. Namely:
\begin{itemize}
    \item The number of nodes of the aggregate loss distribution \texttt{n\_aggr\_dist\_nodes}.
    \item The method to discretize the severity distribution \texttt{sev\_discr\_method}. Above, we opted for the method of mass dispersal (\texttt{`massdispersal'}). 
    \item The number of nodes of the discretized severity \texttt{n\_sev\_discr\_nodes}.
    \item The discretization step \texttt{sev\_discr\_step}.
\end{itemize}

The same arguments shall be specified when computing the aggregate loss distribution with the recursive formula, i.e. \texttt{aggr\_loss\_dist\_method} set to \texttt{`recursive'}.

The costing specifications of a calculated \texttt{LossModel} object can be accessed with the method \texttt{print\_costing\_specs()}.

\begin{lstlisting}
Python> lm_XL.print_costing_specs()

                               Costing Summary 
====================================================================
                          Quantity           Value 
====================================================================
                             Cover            50.0
                        Deductible             2.0
                   Aggregate cover             inf
              Aggregate deductible               0
Pure premium before share partecip.          12.09
                   Share partecip.               1
                      Pure premium           12.09

\end{lstlisting}

The previous output exhibits a summary of the contract structure (cover, deductible, aggregate cover, aggregate deductible) and details about the costing results. It is noted that there is no shared participation, i.e. $a$ in \Cref{eq:qs} is equal to $1$.

Lastly, we provide an example of costing an XL with reinstatements. The specification of the layer coverage modifiers, contained in \texttt{PolicyStructure}, is adjusted accordingly as follows.


\begin{lstlisting}
Python> from gemact.lossmodel import PolicyStructure, Layer
Python> policystructure_RS = PolicyStructure(
... layers=Layer(
... cover=100,
... deductible=0,
... aggr_deductible=100,
... reinst_loading=1,
... n_reinst=2
... ))
\end{lstlisting}

The parameters utilized in the above example and relevant to the XL layer are:

\begin{itemize}
    \item the aggregate deductible parameter \texttt{aggr\_deductible},
    \item the number of reinstatements \texttt{n\_reinst},
    \item  the reinstatement layer loading \texttt{reinst\_loading}.
    
\end{itemize}

Below, we compute the pure premium of \Cref{eq:reinstatementsP}, given the parametric assumptions on the frequency and severity component of the loss model.

\begin{lstlisting}
Python> from gemact.lossmodel import LossModel
Python> lm_RS = LossModel(
... frequency=Frequency(
... dist='poisson',
... par={'mu': .5}
... ),
... severity=Severity(
... par= {'loc': 0,
... 'scale': 83.34,
... 'c': 0.834},
... dist='genpareto'
... ),
... policystructure = policystructure_RS,
... aggr_loss_dist_method='fft',
... sev_discr_method='massdispersal',
... n_sev_discr_nodes=int(10000),
... sev_discr_step=.01,
... n_aggr_dist_nodes=int(100000)
... )
... pure_premium= lm_RS.pure_premium[0]
... print('Pure premium (RS): ', pure_premium)

Pure premium (RS): 4.318987283037917
\end{lstlisting}

\section{Loss aggregation}
\label{sec:lossaggregation}

In insurance and finance, the study of the sum of dependent random variables is a central topic. A notable example is risk management, where the distribution of the sum of certain risks needs to be approximated and analyzed for solvency purposes (e.g. see \cite{Wilhelmy2010} and \cite[p.~471]{hull11}).
Another application is the pricing of financial and (re)insurance contracts where the payout depends on the aggregation of two or more dependent outcomes (e.g. see \cite{Cummins1999} and \cite{Wang2013}). In this section, in contrast with the collective risk theory in \Cref{sec:lossmodel}, we model the sum of a given number of random variables $d > 1$ that are not identically distributed and are dependent.
\newline

More specifically, consider now the random vector

$$\left(X_1, \ldots, X_d\right): \Omega \rightarrow \mathbb{R}^d,$$ 

whose joint cumulative distribution function 

\begin{equation}
\label{eq:jointcdf}
    H\left(x_1,\ldots,x_d\right)=P\left[X_1\le x_1,\ldots,X_d\le x_d\right]  
\end{equation} 

is known analytically or can be numerically evaluated in an efficient way. For a real threshold $s$, the \texttt{gemact} package implements the AEP algorithm \cite{arbenz11} and a Monte Carlo based approach to model 

$$P\left[X_1 + \ldots + X_d \leq s\right]$$

given a set of parametric assumptions on the one-dimensional marginals $X_1, ..., X_d$ and their copula in \Cref{eq:jointcdf}. The moments and the quantiles of the distribution of the sum $X_1 + \ldots + X_d$ can be computed with a Monte Carlo based approach.


\subsection{Illustration}

In order to compute the random variable sum distribution, \textbf{gemact} requires the user to define a \texttt{LossAggregation} object.

\begin{lstlisting}
Python> from gemact import LossAggregation
Python> lossaggregation = LossAggregation(
...  margins=[
...  'genpareto',
...  'genpareto'],
...  margins_pars=[
...  {'loc':0,
...  'scale':1/.8,
...  'c':1/.8},
...  {'loc':0,
...  'scale':1/2,
...  'c':1/2}
...  ],
...  copula='clayton',
...  copula_par={'par':.4,
...  'dim':2})
\end{lstlisting}

Consistently with the \textbf{gemact} framework, the distribution can be computed based on margin and copula assumptions as follows: 

\begin{itemize}
    \item The  marginal distributions assumptions are listed in the parameter \texttt{margins}. Refer to \Cref{app:alldists} for the complete list of the supported distributions.
    
    \item Similarly, the parameters of the marginal distributions must be defined as a \texttt{dictionary} in the \texttt{margins\_pars} argument.
    
    \item The \texttt{copula} argument is one of the copulas listed in \Cref{app:alldists}.
    
    \item The \texttt{copula\_par} argument takes the copula parameters from the copula assumption as a \texttt{dictionary}.
    
\end{itemize}

Finally, in the following code chunk we compute with the AEP algorithm the probability that the sum of two Generalized Pareto Distributions with given parameters is lower than $1$. The underlying dependency structure is a Clayton copula.

\begin{lstlisting}
Python> p_aep = lossaggregation.cdf(1,
...  method='aep',
...  n_iter=8)
...  print('P(X1+X2 <= 1) = ', p_aep)

P(X1+X2 <= 1) = 0.2697201526525709
\end{lstlisting}

\section{Loss reserve}
\label{sec:lossreserve}

In non-life insurance, contracts do not settle when the insured event occurs. At the accident date, the event triggers a claim that will originate payments in the future. The task of predicting these liabilities is called claims reserving and it assesses the outstanding loss liabilities of the claims see \cite[p~11]{wuthrich15}. In the present work, we refer to the total outstanding loss liabilities of past claims as the loss reserve. In \Cref{fig:timeline}, we follow an individual non-life insurance claim evolution. The insured event occurs within the insured period but the claim settles after several years. In particular, after the claim is reported to the insurance company, an expert called claim adjuster provides the insurance company with a quantification of the claim payment size, namely the case estimate. Afterward, two payments occur. These payments are not known at evaluation date and they require to be estimated. In between the two payments, the case estimate is updated. Until the claims settles, the insurance company refers to it as an open claim. In certain circumstances, settled claims can be reopened (see \cite[p~431]{friedland10}).

\begin{figure}
\centering
    \centering\includegraphics[width=13cm]{images/timeline.jpg}
    \caption{Example of events of a non-life insurance claim.}
    \label{fig:timeline}
\end{figure}

Within this section, we will first define the data structure commonly used by actuarial departments for claims reserving, i.e. the run-off triangles. As we explain in the following section, these are aggregate representations of the individual data we just introduced. Afterwards, we present the Fisher-Lange model in \cite{fisher99} as proposed in \cite{savelliDATA}. This model is the starting point for the definition of the collective risk model for claims reserving in \cite{ricotta16} and \cite{clemente19}.

\subsection{Problem Framework}

Let the index $i=0,\ldots,\mathcal{J}$ denote the claim accident period, and let the index $j=0,\ldots,\mathcal{J}$ represent the claim development period, over the time horizon $\mathcal{J}>0$.
The so-called run-off triangles are defined starting from

$$
\mathcal{T}=\{(i, j): \; i=0, \ldots, \mathcal{J}, \; j=0, \ldots, \mathcal{J}; \; i+j \leq \mathcal{J}\}, \quad \mathcal{J}>0.
$$

Below is the list of the run-off triangles used in this section.

\begin{itemize}
    \item The triangle of \textbf{incremental paid claims}:
    $$X^{\mathcal{(T)}} =\left\{x_{i, j} \in \mathcal{T} \right\}$$
    with $x_{i, j}$ being the total payments from the insurance company for claims occurred at accident period $i$ and paid in period $i+j$.
    \item The triangle of the \textbf{number of paid claims}:
    $$N^{\mathcal{(T)}} =\left\{n_{i, j} \in \mathcal{T} \right\}.$$
    with $n_{i, j}$ being the number of claim payments occurred in accident period $i$ and paid in period $i+j$. The triangle of average claim cost can be derived from the incremental paid claims triangle and the number of paid claims. Indeed, let us define:
    
    $$m_{i,j}=\frac{x_{i,j}}{n_{i, j}},$$ 
    
    where $m_{i,j}$ is the average claim cost for accident period $i$ and development period $j$.
    
    \item The triangle of \textbf{incremental amounts at reserve}:
    $$R^{\mathcal{(T)}} =\left\{r_{i, j} \in \mathcal{T} \right\}.$$
    with $r_{i, j}$ being the amount booked in period $i+j$ for claims occurred in accident period $i$.
    \item The triangle of the \textbf{number of open claims}:
    $$O^{\mathcal{(T)}} =\left\{o_{i, j} \in \mathcal{T} \right\}.$$
    with $o_{i, j}$ being the number of claims that are open in period $i+j$ for claims occurred in accident period $i$.
    %$$O^{\mathcal{(T)}} =\left\{O_{i j} \in \mathcal{T} \right\}.$$
    %with $O_{i j}$ being the number of claims that are open in period $i+j$ for claims occurred in accident period $i$.
    \item The triangle of the \textbf{number of reported claims}:
    $$D^{\mathcal{(T)}} =\left\{d_{i, j}  \in \mathcal{T} \right\}.$$
    with $d_{i, j}$ being the claims reported in period $j$ and belonging to accident period $i$. Often, the number of reported claims is aggregated by accident period $i$. 
     %$$D^{\mathcal{(T)}} =\left\{D_{i j}  \in \mathcal{T} \right\}.$$
    %with $D_{i j}$ being the claims reported in period $j$ and belonging to accident period $i$. Often, the number of reported claims is aggregated by accident period $i$. 
\end{itemize}

The claims reserve is the sum of the future incremental payments for all accident periods on the time horizon $\mathcal{J}$:

\begin{equation}
    R = \sum_{i+j>\mathcal{J}}X_{i,j}.
\end{equation}

% Let:

% \begin{equation}
% \label{eq:acmethods}
% X_{i, j}=n_{i, j} \cdot m_{i, j} 
% \end{equation}

% We denote the incremental payments $X_{i,j}$ in each $i,j$ cell as the product between the number of claims $n_{i,j}$ cell and the severity average cost $m_{i,j}$.
% It is worth noticing that the \textbf{gemact} apparatus allows practitioners to model the so-called tail of the triangle: when the claims development is not completed at year $\mathcal{J}$ it is possible to add an additional column and exhaust the future claims development.

The \textbf{gemact} package offers the implementation of:

\begin{itemize}

    \item The \textit{Fisher-Lange} model in \cite{fisher99} and \cite{savelliDATA}. 
    The average claim cost in the lower triangle is easily forecasted as a projection of the inflated average claim cost. 
    \begin{equation}
        \widehat{m}_{i, j}=m_{\mathcal{J}-j, j} \prod_{h=\mathcal{J}+1}^{i+j}\left(1+ir_h\right),
    \end{equation}
    where $ir_h$ represents the claims inflation for calendar period $h$.
    \newline
    As far the claim numbers are concerned, this method assumes that the future number of paid claims is related to the percentage of open claims at the evaluation date and to the claims settlement speed. Indeed, at the evaluation date, the lower triangle is estimated as:
    \begin{equation}
        \hat{n}_{i, j}=o_{i, \mathcal{J}-i} \cdot \alpha_{\mathcal{J}-i} \cdot v_j^{(i)},
    \end{equation}
    where $\left[\alpha_j\right]_{j=0, \ldots,\mathcal{J}-1}$ is the vector of open claims given by $\alpha_j = E\left( \tau_{i, j}\right)$, and
    \begin{equation}
        \tau_{i, j}=\frac{\sum_{h=j+1}^{\mathcal{J}-i} n_{i, h}+o_{i, \mathcal{J}-i}}{o_{i, j}},
    \end{equation}
    for $i=0, \ldots, \mathcal{J}-1$ and $j=0, \ldots, \mathcal{J}-i-1$. It is assumed that $\alpha_\mathcal{J}=1$.
    The claim settlement speed is then computed for each accident year. The settlement speed for accident period $\mathcal{J}$ is
    
    \begin{equation}
        v_j^{(\mathcal{J})}=\frac{n_{\mathcal{J}-j, j} \cdot \frac{d_\mathcal{J}}{d_{\mathcal{J}-j}}}{\sum_{j=1}^\mathcal{J} n_{\mathcal{J}-j, j} \cdot \frac{d_\mathcal{J}}{d_{\mathcal{J}-j}}},
    \end{equation}
    where $d_i$ represents the number of reported claims for accident period $i$, with $i=0,\ldots,\mathcal{J}$. The formula is corrected for other accident years following the approach in \cite[p.~141]{savelliDATA}.
    \item The \textit{collective risk model} in \cite{ricotta16, clemente19}. Each incremental payment cell in the run-off triangle is modeled with a collective risk model:
    
    \begin{equation}
    \label{eq:crmreserving}
        X_{i,j}=\sum_{h=1}^{N_{i,j}} Z_{h;i,j},
    \end{equation}
    
    where the index $h=1, \ldots$ is referred to the individual claim severity.
    This reserving method allows to determine the claims reserve either via simulation or in closed form. On the other hand, it requires distributional assumptions about \textit{frequency} and \textit{severity}. In particular:
    
\begin{itemize}
    \item The \textit{frequency} of the $i,j$ cell: $N_{i,j} \sim \text{Poisson}\left(n_{i,j}\cdot \zeta \right)$ with $\zeta$ distributed as a Gamma with $\mathrm{E}\left(\zeta \right)=1$.
    
    \item The \textit{severity}: $\mathrm{E}[Z_{h;i,j}]=m_{i,j}$ and $c_{Z_{h;i,j}}=c_{z_j} \cdot \psi$ with $\psi$ distributed as a Gamma with $\mathrm{E}\left(\psi\right)=1$.
\end{itemize}
    The parameters for the simulation of \textit{frequency} and \textit{severity} are obtained from the computation of the Fisher-Lange.
    
\end{itemize}

\subsection{Illustration}

This subsection shows in more details examples regarding \texttt{lossreserve} model. In terms of its class design and interconnections, the general context is outlined in \Cref{fig:lossreservediagram}.

\begin{figure}
\centering
    \includegraphics[width=14cm]{images/lossreserve_diagram.png}
    \caption{Class diagram of the \textbf{gemact} \texttt{lossreserve} module. A rectangle represents a class; an arrow connecting two classes indicates that the target class uses the origin class. In this case, a \texttt{LossReserve} object entails \texttt{ReserveModel} and \texttt{AggregateData} class instances.}
    \label{fig:lossreservediagram}
\end{figure}

The \texttt{gemdata} sub-module provides the data sets from \cite{savelliDATA} to test the Fisher-Lange and the collective risk model. 

\begin{lstlisting}
Python> from gemact import gemdata
Python> ip_ = gemdata.incremental_payments
Python> in_ = gemdata.incurred_number
Python> cp_ = gemdata.cased_payments
Python> cn_= gemdata.cased_number
Python> reported_ = gemdata.reported_claims
Python> claims_inflation = gemdata.claims_inflation
\end{lstlisting}

The incremental paid claim triangle $X^{\mathcal{(T)}}$
is stored in \texttt{IPtriangle}, whereas the number of paid claims $N^{\mathcal{(T)}}$ is stored in \texttt{in\_triangle}. 
The case estimates for the outstanding liability amounts $R^{\mathcal{(T)}}$ are in \texttt{cased\_amount\_triangle}, while the open claim information $O^{\mathcal{(T)}}$ is stored in \texttt{cased\_amount\_triangle}. \texttt{reported\_} and \texttt{claims\_inflation} contains the reported number of claims for each accident period and an example vector of claims inflation.

The data are processed and prepared via the \texttt{AggregateData} class.

\begin{lstlisting}
Python> from gemact.lossreserve import AggregateData
Python> ad = AggregateData(
...  incremental_payments=ip_,
...  cased_payments=cp_,
...  cased_number=cn_,
...  reported_claims=reported_,
...  incurred_number=in_)
\end{lstlisting}

Alongside this, the implementation of the Fisher-Lange method is obtained by means of the \texttt{ReservingModel} class.

\begin{lstlisting}
Python> from gemact.lossreserve import ReservingModel
Python> resmodel = ReservingModel(
...  tail=True,
...  reserving_method='fisher_lange',
...  claims_inflation=claims_inflation)
\end{lstlisting}

In more detail, the \texttt{ReservingModel} class arguments concern:

\begin{itemize}
    \item The \texttt{tail} parameter (boolean) specifying whether the triangle tail is to be modeled.
    \item The \texttt{reserving\_method} parameter stating the reserving mechanism (\texttt{`fisher\_lange'} in this case). 
    \item The \texttt{claims\_inflation} parameter indicating the vector of claims inflation.
\end{itemize}

Thereafter, the actual computation of the loss reserve is performed within the \texttt{LossReserve} class:

\begin{lstlisting}
Python> from gemact.lossreserve import LossReserve
Python> lossreserve = LossReserve(
...  data=ad,
...  reservingmodel=resmodel)
\end{lstlisting}

Herein, \texttt{AggregateData} and \texttt{ReservingModel} objects are required, as arguments, to instantiate the \texttt{LossReserve} class.

Similarly to \texttt{LossModel}, also \texttt{LossReserve} comes with a summary view of the estimated reserve per each accident period, as displayed below.

\begin{lstlisting}
Python> lossreserve.print_loss_reserve()

 Accident period          Ultimate FL           Reserve FL
=============================================================================
              0.0              86841.0               1068.0
              1.0             98161.95              1837.95
              2.0            107140.17              2133.17
              3.0            117422.59              2747.59
              4.0            122773.46              4485.46
              5.0            138139.76              6000.76
              6.0            148047.84              9756.84
              7.0            145115.35             15214.35
              8.0            146550.94             21764.94
              9.0            155700.08             33900.08
             10.0            164364.24             52022.24
             11.0            164403.34            104042.34

 FL reserve:  254973.71
\end{lstlisting}

The table reports the ultimate cost (second column) and the reserve (third column) by accident period (first column).
On top of this, insights on the behavior of Fisher-Lange $\left[\alpha_j\right]_{j=0, \mathcal{J}-1}$ and settlement speed $\left[v^{(i)}_j\right]_{j=0, \mathcal{J}-1}$ are given by the \texttt{ss\_plot} method output, depicted in \Cref{fig:sspeed}. It should be noted that the argument \texttt{start} of the \texttt{LossReserve} method \texttt{ss\_plot()} specifies the accident period from which the settlement speed plot (\Cref{fig:sspeedb}) begins.

\begin{figure}
\centering
\caption{
Illustration of the behavior of the settlement speed by development period. Plot (a) shows Fisher-Lange $\left[\alpha_j\right]_{j=0, \ldots, \mathcal{J}-1}$, Plot (b) displays settlement speed $\left[v^{(i)}_j\right]_{j=0,\ldots, \mathcal{J}-1}$ for $i=0,\ldots, \mathcal{J}-1$.}
\begin{subfigure}[b]{0.45\textwidth}
    \centering\includegraphics[width=8cm]{images/lr_alphaplot.png}
    \caption{}
    \label{fig:sspeeda}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
    \centering\includegraphics[width=8cm]{images/lr_settlement_speed.png}
    \caption{}
    \label{fig:sspeedb}
\end{subfigure}
 \label{fig:sspeed}
\end{figure}

Adjusting the model definition in the \texttt{ReservingModel} class will allow to compute the collective risk model.

\begin{lstlisting}
Python> from gemact.lossreserve import ReservingModel
Python> mixing_fq_par = {
...  'a': 1 / .08 ** 2, 
...  'scale': .08 ** 2
...  }
Python> mixing_sev_par = {
...  'a': 1 / .08 ** 2, 
...  'scale': .08 ** 2
...  } 
Python> czj = gemdata.czj
Python> claims_inflation = gemact.gemdata.claims_inflation
Python> resmodel_crm = gemact.ReservingModel(
...  tail=True,
...  reserving_method="crm",
...  claims_inflation=claims_inflation,
...  mixing_fq_par=mixing_fq_par,
...  mixing_sev_par=mixing_sev_par,
...  czj=czj
...  )
\end{lstlisting}

The additional parameters required for the computation of the collective risk model in \cite{ricotta16} are: 

\begin{itemize}
    \item The \texttt{reserving\_method} set to \texttt{`crm'}.
    \item The mixing frequency and severity parameters \texttt{mixing\_fq\_par} and \texttt{mixing\_sev\_par}.
    \item The coefficient of variation of the severity \texttt{czj}.
\end{itemize}

In a similar fashion to the Fisher-Lange illustration above, it is possible to use the \texttt{LossReserve} class for computing the loss reserve and printing the  results obtained.
\clearpage

\begin{sidewaysfigure}
\small
\begin{lstlisting}
Python> lossreserve_crm = LossReserve(
...  data=ad,
...  reservingmodel=resmodel_crm,
...  ntr_sim=100,
...  random_state=1
...  )
Python> lossreserve_crm.print_loss_reserve()
[============================================================================= ] 100%
  accident period          ultimate FL           reserve FL         ultimate CRM          reserve CRM            m_sep CRM
========================================================================================================================
              0.0              86841.0               1068.0             86562.97              1516.97               281.89
              1.0             98161.95              1837.95             97709.51              1841.51                394.6
              2.0            107140.17              2133.17            106592.07              2144.07               381.08
              3.0            117422.59              2747.59            116607.24              2805.24                404.3
              4.0            122773.46              4485.46            121681.79              4517.79                480.2
              5.0            138139.76              6000.76            135938.32              6066.32               481.17
              6.0            148047.84              9756.84             144690.4               9712.4               740.44
              7.0            145115.35             15214.35            140371.48             15184.48              1202.89
              8.0            146550.94             21764.94            137808.69             21855.69              1852.65
              9.0            155700.08             33900.08            136947.61             33751.61              3469.81
             10.0            164364.24             52022.24            112067.19             53468.19              6732.36
             11.0            164403.34            104042.34            164836.79            104475.79             15494.96

 FL reserve:  254973.71

 CRM reserve:  257340.06

 CRM m_sep:  10159.51
\end{lstlisting}
\end{sidewaysfigure}
\clearpage

The additional arguments to specify are the number of simulations \texttt{ntr\_sim} and the random state \texttt{random\_state}. 

The \texttt{print\_loss\_reserve} method provides the user with a direct comparison between the Fisher-Lange and the Collective Risk Model. The output includes the ultimate cost (columns 2 and 4), the reserve (columns 3 and 5), and the mean squared error of prediction (Collective Risk Model only, column 6).
The total reserve of the collective risk model and the total mean squared error of prediction are at the bottom of the model output.
\newline

Finally, the \texttt{LossReserve} object supplies information about the distribution of the reserve. As an example, its mean can be accessed via the \texttt{mean} method.

\begin{lstlisting}
Python>lossreserve_crm.mean()

257340.0609855847
\end{lstlisting}

Analogously, it is possible to retrieve standard deviation and skewness of the the reserve distribution with \texttt{std} and \texttt{skewness} methods, respectively.


\section{Conclusions}

This paper introduces \textbf{gemact}, a \textbf{Python} package for non-life actuarial modeling based on the collective risk theory.

Our package comes with a variety of tools for loss modeling, loss aggregation, and loss reserving. Hence, it can be applied in different fields of actuarial sciences, including pricing, reserving and risk management.

\textbf{gemact} expands the reach of actuarial sciences within the growing community of \textbf{Python} programming language. Indeed, it introduces new functionalities, probability distributions, and, to the knowledge of the authors, the first open-source software implementation of the AEP algorithm.

Finally, the structure of our package is designed to guarantee flexiblility and scalability, as we thought of \textbf{gemact} as an evolving and growing project in terms of increase of features, advancements in methodologies, and extension of its scopes.

Possible future enhancements may involve the expansion of the number of available probability distribution families, the introduction of supplementary methodologies for the approximation of quantiles of the sum of random variables, and the addition of costing procedures for exotic and non-traditional reinsurance solutions.

\section*{Acknowledgments}

Previous versions of \textbf{gemact} were presented at the Mathematical and Statistical Methods for Actuarial Sciences and Finance 2022, and at the Actuarial Colloquia 2022, in the ASTIN section. We would like to thank all the people who gave us feedbacks and suggestions about the project. 


\bibliography{references}


\newpage

\begin{appendix}

\section{List of the Supported Distributions} \label{app:alldists}

\Cref{tab:alldists} provides an overview of the distributions available in \textbf{gemact}. In particular, the table presents the distribution name (column one) and its naming within \textbf{gemact} apparatus (column two). Moreover, it shows the distribution support (column three) and whether the distribution is supported in \textbf{scipy} (column four). 

%  \hline
% Pareto&\texttt{genpareto} &continuous & $\checkmark$ &  $\checkmark$   \\   
%  \hline
% Single-parameter Pareto&\texttt{genpareto} &continuous  & $\checkmark$ &  $\checkmark$   \\                   
%  \hline
% Pareto II&\texttt{genpareto} &continuous  & $\checkmark$ &  $\checkmark$   \\   
%  \hline
% Pareto III&\texttt{genpareto} &continuous  & $\checkmark$ &  $\checkmark$   \\   
%  \hline
% Pareto IV&\texttt{genpareto} & continuous  & $\checkmark$ &  $\checkmark$   \\       

\begin{table}[H]
\centering
\begin{tabular}{llll}
\hline
\textbf{Distribution}& \textbf{gemact} \textbf{name}  & \textbf{Support}    & \textbf{scipy} \\ \hline
Poisson&\texttt{poisson} &discrete  &  $\checkmark$   \\   
 \hline
Binomial&\texttt{binom} & discrete &   $\checkmark$   \\   
 \hline
Geometric&\texttt{geom} & discrete &  $\checkmark$   \\   
 \hline
Negative Binomial&\texttt{nbinom}  &discrete   &  $\checkmark$   \\   
 \hline
 Log-Series&\texttt{logser} &discrete  & $\checkmark$   \\   
 \hline
 Zero-Truncated Poisson &\texttt{ztpoisson} & discrete &  \textcolor{red}{\ding{55}}   \\   
 \hline
 Zero-Truncated Binomial&\texttt{ztbinom}  &discrete &  \textcolor{red}{\ding{55}}  \\   
 \hline
Zero-Truncated Geometric& \texttt{ztgeom} & discrete  &   \textcolor{red}{\ding{55}}   \\   
 \hline
Zero-Truncated Negative Binomial&\texttt{ztnbinom} &discrete &   \textcolor{red}{\ding{55}}   \\                         
 \hline
 Zero-Modified Poisson&\texttt{zmpoisson} &discrete&  \textcolor{red}{\ding{55}}  \\
 \hline
 Zero-Modified Binomial&\texttt{zmbinom} & discrete  &  \textcolor{red}{\ding{55}}   \\   
 \hline
Zero-Modified Geometric&\texttt{zmgeom} & discrete &  \textcolor{red}{\ding{55}}   \\   
 \hline
Zero-Modified Negative Binomial&\texttt{zmnbinom} & discrete &   \textcolor{red}{\ding{55}}   \\            
 \hline
 Zero-Modified Logarithmic&\texttt{zmlogser} &discrete & \textcolor{red}{\ding{55}}   \\  
 \hline
 Beta&\texttt{beta} &continuous  &   $\checkmark$   \\   
 \hline
Exponential&\texttt{exponential} & continuous &  $\checkmark$   \\       
 \hline
  Gamma&\texttt{gamma} & continuous &  $\checkmark$   \\       
 \hline
 Inverse Gamma&\texttt{invgamma} & continuous &  $\checkmark$   \\
  \hline
Log Gamma&\texttt{loggamma} & continuous &  $\checkmark$   \\
 \hline
Generalized Pareto& \texttt{genpareto} &continuous  &  $\checkmark$   \\   
 \hline
 Pareto& \texttt{pareto2} &continuous  &  $\checkmark$   \\   
 \hline
 Single-Parameter Pareto& \texttt{pareto1} &continuous  &  \textcolor{red}{\ding{55}}   \\   
 \hline
Lognormal& \texttt{lognormal} &continuous  &  $\checkmark$   \\  
 \hline
 Generalized Beta&\texttt{genbeta} &continuous  &  \textcolor{red}{\ding{55}}  \\   
 \hline
 Burr & \texttt{burr12} &continuous  &  $\checkmark$   \\        
 \hline
 Paralogistic & \texttt{paralogistic}&continuous &  \textcolor{red}{\ding{55}}  \\ 
\hline
Inverse Burr (Dagum)&\texttt{dagum}&  continuous &  $\checkmark$   \\   
 \hline
Inverse Paralogistic &\texttt{invparalogistic} & continuous &   \textcolor{red}{\ding{55}}   \\  
 \hline
Weibull&\texttt{weibull}  & continuous &  $\checkmark$   \\   
 \hline
Inverse Weibull&\texttt{invweibull} &continuous  &  $\checkmark$   \\    
\hline 
Inverse Gaussian&\texttt{invgauss} & continuous  &  $\checkmark$   \\   
 \hline
Fisk&\texttt{fisk} & continuous  &  $\checkmark$   \\   
 \hline
\end{tabular}
\caption{\label{tab:alldists} List of distributions supported by \textbf{gemact}.}
\end{table}

Lastly, the list of available copulas is provided in \Cref{tab:allcopulas}.

\begin{table}[H]
\centering
\begin{tabular}{lll}
\hline
\textbf{Copula} & \textbf{gemact} \textbf{name}  & \textbf{Copula family} \\
\hline
Clayton&\texttt{clayton} & Archimedean \\ 
\hline
Gumbel&\texttt{gumbel} &  Archimedean\\ 
\hline
Frank&\texttt{frank} & Archimedean \\ 
\hline
Gaussian&\texttt{gaussian} & Elliptical \\ 
\hline
T-student&\texttt{tstudent} & Elliptical \\ 
\hline
Independent&\texttt{independent} & Fundamental \\ 
\hline
Fréchet-Hoeffding upper bound (M)&\texttt{frechet–hoeffding-lower}& Fundamental \\ 
\hline
Fréchet-Hoeffding lower bound (W)&\texttt{frechet–hoeffding-upper}& Fundamental \\ 
\hline
\end{tabular}
\caption{\label{tab:allcopulas} List of copulas supported by \textbf{gemact}.}
\end{table}


\end{appendix}



\end{document}
