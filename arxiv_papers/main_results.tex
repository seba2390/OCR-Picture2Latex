\section{Main Results}\label{main_results}
In this section, we first introduce a mild ``informative'' condition for unsupervised pretraining. We show this ``informative'' condition is necessary for pretraining to benefit downstream tasks. We then provide our main results---statistical guarantees for unsupervised pretraining and downstream tasks for Algorithm \ref{mle+erm}. Finally, in Section \ref{weak_informative_model}, we generalize our results to a more technical but weaker version of the ``informative'' condition, which turns out to be useful in capturing our third example of contrastive learning (Section \ref{contrastive_learning}).


% the notion of $\kappa^{-1}$-informative model, which guarantees the utility of unsupervised pretraining for downstream tasks learning. We then present a statistical guarantee for the MLE step in line 2 of Algorithm \ref{mle+erm} and upper bound excess risk of Algorithm \ref{mle+erm}. Our theoretical result rigorously shows the advantage of unsupervised pretraining in the sense of reducing the sample complexity. In Section \ref{weak_informative_model}, we show that the same theoretical guarantee can be achieved under a weaker version of $\kappa^{-1}$-informative model assumption.

% In Section \ref{informative_model}, we first introduce the notion of $\kappa^{-1}$-informative model, which guarantees the utility of unsupervised pretraining for downstream tasks learning. We then present a statistical guarantee for the MLE step in line 2 of Algorithm \ref{mle+erm} and upper bound excess risk of Algorithm \ref{mle+erm}. Our theoretical result rigorously shows the advantage of unsupervised pretraining in the sense of reducing the sample complexity. In Section \ref{weak_informative_model}, we show that the same theoretical guarantee can be achieved under a weaker version of $\kappa^{-1}$-informative model assumption.


% \subsection{Theoretical Guarantee for \texorpdfstring{$\kappa^{-1}$}{Lg}-Informative Model}\label{informative_model}

\paragraph{Informative pretraining tasks.}
We first note that under our generic setup, unsupervised pretraining may not benefit downstream tasks at all in the worst case if no further conditions are assumed.
%\paragraph{$\kappa^{-1}$-Informative}
% Notice that, further suitable assumption is necessary for revealing the nature of unsupervised pretraining.
\begin{proposition} \label{prop:informative_necessary}
There exist classes $(\Phi, \Psi)$ as in Section \ref{sec:prob_setup} such that, regardless of unsupervised pretraining algorithms used, pretraining using unlabeled data provides no additional information towards learning predictor $g_{\phi^* , \psi^* }$.
\end{proposition}
Consider the latent variable model $z=Ax$, where $x \sim \mathcal{N}(0,I_{d})$, $A\in \Phi$ is the parameter of the model. Then, no matter how many unlabeled $\{x_{i}\}$ we have, we can gain no information of $A$ from the data! In this case, unsupervised pretraining is not beneficial for any downstream task. 

Therefore, it's crucial to give an assumption that guarantees our unsupervised pretraining is informative. As a thought experiment, suppose that in the pretraining step, we find an exact density estimator $\hat{\phi}$ for the marginal distribution of $x,s$ , i.e., $p_{\hat{\phi}}(x,s)=p_{\phi^{*}}(x,s)$ holds for every $x,s$. We should expect that this estimator also fully reveals the relationship between $x$ and $z$, i.e., $p_{\hat{\phi}}(x,z)=p_{\phi^{*}}(x,z)$ holds for every $x,z$. Unfortunately, this condition does not hold in most practical setups and is often too strong. As an example, consider Gaussian mixture models, where $z \in [K]$ is the cluster that data point $x \in \mathbb{R}^{d}$ belongs to. Then in this case, it is impossible for us to ensure $p_{\hat{\phi}}(x,z)=p_{\phi^{*}}(x,z)$, since a permutation of $z$ makes no difference in the marginal distribution of $x$. However, notice that in many circumstances, a permutation of the class label will not affect the downstream task learning. In these cases, a permutation of the clusters is allowed. Motivated by this observation, we introduce the following informative assumption which allows certain ``transformation'' induced by the downstream task:

%As a thought experiment, suppose that in the pretraining step, we find an exact density estimatior $\hat{\phi}$ for marginal distribution of $x$ , i.e., $p_{\hat{\phi}}(x)=p_{\phi^{*}}(x)$ holds for every $x$. We should expect that this estimator $\hat{\phi}$ can serve as the ground truth $\phi^{*}$ in our second step. Notice that the notion "serve" is related to the structure of the downstream task: if for every $\psi \in \Psi$, there exists $\tilde{\psi}$ such that $p_{\hat{\phi}, \tilde{\psi}}(x,y)=p_{\phi^{*},\psi}(x,y)$ holds for every $x,y$, then $\hat{\phi}$ and $\phi^{*}$ can be treated as the same in the downstream task learning. In fact, here we are assuming that our model allows some kinds of transferability. For example, in Gaussian mixture models, we often want to find the clusters for unlabeled data. Permutation of the clusters are allowed, since a permutation of class label will not affect the downstream task learning in many circumstances.  We formalize this idea, by assuming our model satisfies the following transferability assumption.
%\jiawei{Add $z=Ax$ does not work.}



\begin{assumption}[$\kappa^{-1}$-informative condition]\label{invariance}
We assume that the model class $\Phi$ is $\kappa^{-1}$-informative with respect to a transformation group $\mathcal{T}_{\Phi}$. That is, for any $\phi\in\Phi$, there exists $T_1\in\mT_{\Phi}$ such that
\%\label{informative}
\TV\big(\P_{T_1\circ\phi}(x,z),\P_{\phi^* }(x,z)\big)\leq  \kappa \cdot\TV\big(\P_{\phi}(x,s),\P_{\phi^* }(x,s)\big).
\%
Here $\phi^* $ is the ground truth parameter. Furthermore, we assume that $\mathcal{T}_{\Phi}$ is induced by transformation group $\mathcal{T}_{\Psi}$ on $\Psi$, i.e., for any $T_1\in\mT_{\Phi}$, there exists $T_2\in\mT_{\Psi}$ such that for any $(\phi,\psi)\in\Phi\times\Psi$,
\%\label{101701}
\P_{\phi,\psi}(x,y)=\P_{T_1\circ\phi,T_2\circ\psi}(x,y).
\%
\end{assumption}


% \chijin{think about alternative name.}

% \jiawei{$\kappa^{-1}$-informative, after the assumption add: sometimes $x$ is not informative but adding $s$ makes $(x,s)$ informative}

% \begin{assumption}[$\kappa^{-1}$-Informative]\label{invariance}
% We say the model is $\kappa^{-1}$-informative iff for any $\phi\in\Phi$, there exists $\psi\in\Psi$ such that
% \%
% \TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\leq \kappa \cdot\TV\big(\P_{\phi}(x),\P_{\phi^* }(x)\big).
% \%
% Here we denote by $\phi^* ,\psi^* $ the ground truth parameters.
% \end{assumption}
% \chijin{think about alternative name.}

% \jiawei{$\kappa^{-1}$-informative, after the assumption add: sometimes $x$ is not informative but adding $s$ makes $(x,s)$ informative}


% We make the following remarks on the above transferability assumption:
% \begin{itemize}
%     \item Suppose that $\mT_{\Phi}$ and $\mT_{\Psi}$ are two transformation groups defined on $\Phi$ and $\Psi$, respectively. And for any $T_1\in\mT_{\Phi}$, there exists $T_2\in\mT_{\Psi}$ such that for any $(\phi,\psi)\in\Phi\times\Psi$,
%     \%\label{101701}
%     \P_{\phi,\psi}(x,y)=\P_{T_1\circ\phi,T_2\circ\psi}(x,y).
%     \%
%     Then the transferability in $z$ implies the transferability in the model. To be specific, if we further have for any $\phi\in\Phi$, there exits a transformation $T_1\in\mT_{\Phi}$ such that 
%     \%\label{101702}
%     \TV\big(\P_{T_1\circ\phi}(x,z),\P_{\phi^* }(x,z)\big)\leq  \kappa \cdot\TV\big(\P_{\phi}(x),\P_{\phi^* }(x)\big),
%     \%
%     then the model has $\kappa$-transferability.
%     \jiawei{Assumption 4.1: (8), the transformation group we consider is defined as (7)}. To see this, for any $\phi\in\Phi$, we choose $T_1$ that satisfies \eqref{101702} and $T_2$ that satisfies \eqref{101701}. Let $\psi=T^{-1}_2\circ\psi^* $. It then holds that
%     \$
%     \TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)&=\TV\big(\P_{T_1\circ\phi,\psi^* }(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\\
%     &\leq\TV\big(\P_{T_1\circ\phi}(x,z),\P_{\phi^* }(x,z)\big)\leq \kappa \cdot\TV\big(\P_{\phi}(x),\P_{\phi^* }(x)\big).
%     \$
%     \item Note that the MLE step actually gives us a theoretical guarantee on the hellinger distance between the estimated distribution and the ground truth distribution, which is stronger than the TV distance guarantee. Thus, we can weaken our $\kappa$-transferability assumption to a hellinger distance version, i.e., for any $\phi\in\Phi$, there exists $\psi\in\Psi$ such that
%     \%\label{weak_ti}
%     \TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\leq \kappa \cdot H\big(\P_{\phi}(x),\P_{\phi^* }(x)\big).
%     \%
% %     We refer to \eqref{weak_ti} as weak $\kappa$-transferability. Similarly, weak $\kappa$-transferability in $z$ implies weak $\kappa$-transferability in the model. See Appendix \ref{proof_contrastive_learning} for the details.
%     \item For settings with side information, we say the model has weak $\kappa$-transferability with side information iff for any $\phi\in\Phi$ there exists $\psi\in\Psi$ such that
%     \%\label{weak_ti_side}
%     \TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\leq \kappa \cdot H\big(\P_{\phi}(x,s),\P_{\phi^* }(x,s)\big).
%     \%
%     See section \ref{contrastive_learning} for further details.
% \end{itemize}


% \begin{assumption}[Translation Invariance]\label{invariance}
% There exists transformation groups $\mT_{\Phi}$ and $\mT_{\Psi}$ defined on $\Phi$ and $\Psi$ such that the following properties hold:
% \begin{itemize}
%      \item For any $(\phi,\psi,T_1)\in\Phi\times\Psi\times\mT_{\Phi}$, there exists $T_2\in\mT_{\Psi}$ 
%     \$
%     \P_{\phi,\psi}(x,y)=\P_{T_1\circ\phi,T_2\circ\psi}(x,y).
%     \$
%     \item For any $\phi\in\Phi$, there exits a transformation $T_1\in\mT_{\Phi}$ such that 
%     \$
%     \TV\big(\P_{T_1\circ\phi}(x,z),\P_{\phi^* }(x,z)\big)\leq  \kappa \cdot\TV\big(\P_{\phi}(x),\P_{\phi^* }(x)\big)
%     \$ 
%     for some model-related parameters $c$.
% \end{itemize}
% \end{assumption}
% Assumption \ref{invariance} implies, for any $\phi\in\Phi$, there exists $\psi \in \Psi$ such that
% \$
% \TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\leq \kappa \cdot\TV\big(\P_{\phi}(x),\P_{\phi^* }(x)\big).    
% \$
% \chijin{make this equation as our main assumption. Remark 1: often it suffices to check transferrability in $z$; Remark 2: Hellinger}
Under Assumption \ref{invariance}, if the pretrained $\hat{\phi}$ accurately estimates the marginal distribution of $x,s$ up to high accuracy, then it also reveals the correct relation between $x$ and representation $z$ up to some transformation $\mathcal{T}_{\Phi}$ which is allowed by the downstream task, which makes it possible to learn the downstream task using less labeled data. 

Proposition \ref{prop:informative_necessary} shows that the informative condition is necessary for pretraining to bring advantage since the counter example in the proposition is precisely $0$-informative. We will also show this informative condition is rich enough to capture a wide range of unsupervised pretraining methods in Section \ref{factor_model}, \ref{gmm}, \ref{contrastive_learning}, including factor models, Gaussian mixture models, and contrastive learning models.

% Therefore it is possible to learn a good $\hat{\psi}$ that captures the relationship between $x$ and $y$ based on the pretrained $\hat{\phi}$. We will show that this informative assumption holds for widely used models such as factor models, Gaussian mixture models and contrastive learning models.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \subsection{Unsupervised Pretraining}
\paragraph{Guarantees for unsupervised pretraining.}
% Given unlabeled data $\{x_i\}^m_{i=1}$, we estimate $\phi^* $ by $\hat\phi$ via MLE, i.e.,
% \%\label{mle}
% \hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i).
% \%
Recall that $\mP_{\mathcal{X}\times\mathcal{S}}(\Phi):=\{p_{\phi}(x,s)\,|\,\phi\in\Phi\}$. We have the following guarantee for the MLE step (line 2) of Algorithm \ref{mle+erm}.

\begin{theorem}\label{tv_mle}
Let $\hat\phi$ be the maximizer defined in \eqref{mle}. Then, with probability at least $1-\delta$, we have
\$
\TV\big(\P_{\hat\phi}(x,s),\P_{\phi^* }(x,s)\big)\leq 3\sqrt{\frac{1}{m}\log\frac{N_{\b}(\mP_{\mathcal{X}\times\mathcal{S}}(\Phi),\frac{1}{m})}{\delta}},
\$
where $N_{\b}$ is the bracketing number as in Definition \ref{def:bracketing}.
\end{theorem}

Theorem \ref{tv_mle} claims that the TV error in estimating the joint distribution of $(x, s)$ decreases as $\mathcal{O}(\mathcal{C}_\Phi/m)$ where $m$ is the number of unlabeled data, and $\mathcal{C}_\Phi = \log N_{\b}(\mP_{\mathcal{X}\times\mathcal{S}}(\Phi), 1/m)$ measures the complexity of learning the latent variable models $\Phi$. 
This result mostly follows from standard analysis of MLE \citep{van2000empirical}. We include the proof in Appendix \ref{main1} for completeness.  If the model is $\kappa^{-1}$-informative, Theorem \ref{tv_mle} further implies that with probability at least $1-\delta$,
\$
&\min_{\psi}\E_{\P_{\phi^* ,\psi^* }}\big[\ell\big(g_{\hat\phi,\psi}(x),y\big)\big]-\E_{\P_{\phi^* ,\psi^* }}\big[\ell\big(g_{\phi^* ,\psi^* }(x),y\big)\big]\leq 12\kappa L\sqrt{\frac{1}{m}\log\frac{N_{\b}(\mP_{\mathcal{X}\times\mathcal{S}}(\Phi),1/m)}{\delta}}.
\$
See Lemma \ref{error_dtv} for the details. This inequality claims that if we learn a \emph{perfect} downstream predictor using the estimated representation $\hat{\phi}$, excess risk is small.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Guarantees for downstream task learning.}
% Let $\hat\phi$ be the pretrained estimator. In the learning of downstream tasks, we fix $\hat\phi$ and estimate $\psi^* $ using the labeled data $\{x_j,y_j\}^n_{j=1}$ via ERM, i.e.,
% \$
% \hat\psi\leftarrow\argmin_{\psi\in\Psi}\sum^n_{j=1}\ell\big(g_{\hat\phi,\psi}(x_j),y_j\big).
% \$
In practice, we can only learn an approximate downstream predictor using a small amount of labeled data. We upper bound the excess risk of Algorithm \ref{mle+erm} as follows.
\begin{theorem}\label{error_bound}
Let $\hat\phi$ and $\hat\psi$ be the outputs of Algorithm \ref{mle+erm}. Suppose that the loss function $\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow \R$ is $L$-bounded and our model is $\kappa^{-1}$-informative. Then, with probability at least $1-\delta$, the excess risk of Algorithm \ref{mle+erm} is bounded as:
\$
{\rm Error}_{\ell}(\hat\phi,\hat\psi)&\leq 2\max_{\phi\in\Phi} R_n(\ell\circ \mathcal{G}_{\phi,\Psi})+12\kappa L\cdot\sqrt{\frac{1}{m}\log\frac{2N_{\b}(\mP_{\mathcal{X}\times\mathcal{S}}(\Phi),1/m)}{\delta}}+2L\cdot\sqrt{\frac{2}{n}\log\frac{4}{\delta}}.
\$
Here $R_n(\cdot)$ denotes the Rademacher complexity, and
\$
\ell\circ \mathcal{G}_{\phi,\Psi}:=\big\{\ell\big( g_{\phi,\psi}(x),y\big):\mathcal{X}\times\mathcal{Y}\rightarrow [-L,L]\,\big|\,\psi\in\Psi\big\}.
\$
\end{theorem}

Note that the Rademacher complexity of a function class can be bounded by its metric entropy. We then have the following corollary.
\begin{corollary}\label{rc_covering}
Under the same preconditions as Theorem \ref{error_bound}, we have:
% Let $\hat\phi$ and $\hat\psi$ be the outputs of Algorithm \ref{mle+erm}. Suppose that the loss function $\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow \R$ is $L$-bounded and our model is $\kappa^{-1}$-informative. Then, with probability at least $1-\delta$, the excess risk of Algorithm \ref{mle+erm} can be bounded as follows,
\$
{\rm Error}_{\ell}(\hat\phi,\hat\psi)&\leq \tilde{c} \max_{\phi\in\Phi} L \sqrt{\frac{\log N (\ell\circ \mathcal{G}_{\phi,\Psi},L/\sqrt{n},\|\cdot\|_{\infty})}{n}}+2L\sqrt{\frac{2}{n}\log\frac{4}{\delta}}\notag\\
 &~+12\kappa L\sqrt{\frac{1}{m}\log\frac{2N_{\b}(\mP_{\mathcal{X}\times\mathcal{S}}(\Phi),1/m)}{\delta}}\notag,
\$
where $\tilde{c}$ is an absolute constant, $N(\mathcal{F}, \delta, \| \cdot \|_{\infty})$ is the $\delta-$covering number of function class $\mathcal{F}$ with respect to the metric $\| \cdot \|_{\infty}$.
\end{corollary} 
By Corollary \ref{rc_covering}, 
%we can see that our excess risk bound can be decomposed into two parts. The first term is $\tilde{\mathcal{O}}(\sqrt{\log N (\ell\circ \mathcal{G}_{\phi,\Psi})/n}) $, and the second term is $\tilde{\mathcal{O}}(\sqrt{\log N_{[]} (\mP(\Phi))/n}) $. Notice that $\ell\circ \mathcal{G}_{\phi,\Psi}$ is a function class that only the index $\psi$ varies, $\log N (\ell\circ \mathcal{G}_{\phi,\Psi})$ can roughly be view as the complexity measure of class $\Psi$. Similarly $\log N_{[]} (\mP(\Phi))$ can be view as the complexity of $\Phi$. Therefore, our results shows that
the excess risk of our Algorithm \ref{mle+erm} is approximately $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_\Phi/m} + \sqrt{\mathcal{C}_\Psi/n})$, where $\mathcal{C}_\Phi$ and $\mathcal{C}_\Psi$ are roughly the log bracketing number of class $\Phi$ and the log covering number of $\Psi$. Note that excess risk for the baseline algorithm that learns downstream task using only labeled data is $\tilde{\mathcal{O}}( \sqrt{\mathcal{C}_{\Phi \circ \Psi}/n})$, where $\mathcal{C}_{\Phi \circ \Psi}$ is the log covering number of composite function class $\Phi \circ \Psi$.  In many practical scenarios such as training a linear predictor on top of a pretrained deep neural networks, the complexity $\mathcal{C}_{\Phi \circ \Psi}$ is much larger than $\mathcal{C}_{\Psi}$. We also often have significantly more unlabeled data than labeled data ($m \gg n$). In these scenarios, our result rigorously shows the significant advantage of unsupervised pretraining compared to the baseline algorithm which directly performs supervised learning without using unlabeled data.
% the performance of downstream task learning with unsupervised pretraining is significantly better than the baseline of supervised learning with no pretraining.

%\jiawei{Add subsection: weak $\kappa$-informative (10), argue why this is weaker than Assumption 4.1 (Hellinger weaker than TV, $(x,y)$ better than $(x,z)$ guarantee)}

% Given unlabeled data $\{x_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, another natural scheme is to use a two-phase MLE. To be specific, in the first phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i\}^m_{i=1}$. In the second phase, we use MLE again to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$. However, we can show that this two-phase MLE scheme fails in some cases. See Appendix \ref{counter_example} for the details.


% To see what's the value of this risk bound, let's compare it to the usual bound for learning downstream task using only labeled data. The standard risk bound for supervised learning is given by $\tilde{\mathcal{O}}( \sqrt{\mathcal{C}_{\Phi \circ \Psi}/n})$. In some circumstances $\mathcal{C}_{\Phi \circ \Psi}$ is much larger than $\mathcal{C}_{\Psi}$. Therefore, when $m$ is sufficiently large and $n$ is relatively small (i.e., unlabeled data is abundant, while labeled data is scarce), the performance of downstream task learning with unsupervised pretraining is significantly better than the baseline of supervised learning with no pretraining.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Guarantees for weakly informative models}\label{weak_informative_model}

%\chijin{whole paper: weak informative $\rightarrow$ weakly informative}
%\subsection{Weak $\kappa^{-1}$-Informative}

We introduce a relaxed version of Assumption \ref{invariance}, which allows us to capture a richer class of examples.
% such as contrastive learning.
\begin{assumption}[$\kappa^{-1}$-weakly-informative condition]\label{weak_invariance}
We assume model $(\Phi, \Psi)$ is $\kappa^{-1}$-weakly-informative, that is, for any $\phi\in\Phi$, there exists $\psi\in\Psi$ such that
\%\label{weak_informative}
\TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\leq \kappa \cdot H\big(\P_{\phi}(x,s),\P_{\phi^* }(x,s)\big).
\%
Here we denote by $\phi^{*}, \psi^{*}$ the ground truth parameters.
\end{assumption}

Assumption \ref{weak_invariance} relaxes Assumption \ref{invariance} by making two modifications: (i) replace the LHS of \eqref{informative} by the TV distance between the joint distribution of $(x, y)$; (ii) replace the TV distance on the RHS by the Hellinger distance. See more on the relation of two assumptions in Appendix \ref{relation}.

% \chijin{Move these justifications to appendix}
% {\color{blue}
% Assumption \ref{weak_invariance} is actually a relaxation of Assumption \ref{invariance}. To see this, by Assumption \ref{invariance}, for any $\phi\in\Phi$, we choose $T_1$ that satisfies \eqref{informative} and $T_2$ that satisfies \eqref{101701}. Let $\psi=T^{-1}_2\circ\psi^* $. It then holds that
% \$
% &\TV\big(\P_{\phi,\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\notag\\
% &=\TV\big(\P_{T_1\circ\phi,\psi^* }(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\\
% &\leq\TV\big(\P_{T_1\circ\phi}(x,z),\P_{\phi^* }(x,z)\big)\notag\\
% &\leq \kappa \cdot\TV\big(\P_{\phi}(x,s),\P_{\phi^* }(x,s)\big).
% \$
% Note that the TV distance can be upper bounded by the Hellinger distance. Thus, Assumption \ref{invariance} directly implies Assumption \ref{weak_invariance}.}

% It can be seen that the MLE step in line 2 of Algorithm \ref{mle+erm} actually gives us a theoretical guarantee on the Hellinger distance between the estimated distribution and the ground truth distribution. Thus, 
In fact, Assumption \ref{weak_invariance} is sufficient for us to achieve the same theoretical guarantee as that in Theorem \ref{error_bound}.

\begin{theorem}\label{weak_error_bound}
Theorem \ref{error_bound} still holds under the $\kappa^{-1}$-weakly-informative assumptions.
\end{theorem}

The proof of Theorem \ref{weak_error_bound} requires a stronger version of MLE guarantee than Theorem \ref{tv_mle}, which guarantees the closeness in terms of Hellinger distance. We leave the details in Appendix \ref{main4}.

%\chijin{Appendix ???, cite the place for the proof of this theorem}.