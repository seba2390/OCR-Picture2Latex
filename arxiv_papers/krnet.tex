The basic idea of KRnet is to define the structure of $f({x})$ in terms of the Knothe-Rosenblatt rearrangement. Let $\mu_Z$ and $\mu_X$ be the probability measures of two random variables $X,Z\in\xs{R}^d$ respectively. A mapping $\mathcal{T}$: $Z \mapsto X$ is called a transport map such that $\mathcal{T}_{\#} \mu_Z = \mu_X$, where $\mathcal{T}_{\#} \mu_{Z}$ is the push-forward of $\mu_{Z}$ such that $\mu_{X}(B) = \mu_{Z}(\mathcal{T}^{-1}(B))$ for every Borel set $B$. The Knothe-Rosenblatt rearrangement tells us that the transport map $\mathcal{T}$ may have a lower-triangular structure 
\begin{equation}
    {z} = \mathcal{T}^{-1}({x}) = \left[ 
    \begin{array}{l}
    \mathcal{T}_1(x_1) \\
    \mathcal{T}_2(x_1, x_2) \\
    \vdots \\
    \mathcal{T}_{d}(x_1, \ldots, x_d)
    \end{array}
    \right].
\end{equation}
This mapping can be regarded as a limit of sequence of optimal transport maps when the quadratic cost degenerates \cite{carlier2010knothe}. Noticing that the invertible mapping $f(x)$ also defines a transport map, we then incorporate the triangular structure of the Knothe-Rosenblatt rearrangement into the definition of $f(x)$ which results in KRnet as a generalization of real NVP \cite{dinh2016density}. Let ${x} = [{x}^{(1)}, \ldots, {x}^{(K)}]^\mathsf{T}$ be a partition of ${x}$, where ${x}^{(i)} = [x_{1}^{(i)}, \ldots, x_{m}^{(i)}]^\mathsf{T}$ with $1 \leq K \leq d, 1 \leq m \leq d$, and $\sum_{i=1}^K \mathrm{dim}({x}^{(i)}) = d$. Our KRnet takes an overall form
\begin{equation} \label{eqn_KR}
  {z} = f({x}) = \left[ 
    \begin{array}{l}
    f_1({x}^{(1)}) \\
    f_2({x}^{(1)}, {x}^{(2)}) \\
    \vdots \\
    f_{K}({x}^{(1)}, \ldots, {x}^{(K)})
    \end{array}
    \right].
\end{equation}
Here each $f_{i}$ is an invertible mapping  for $ i = 2, \ldots, K$. Each $f_{i}$ is constructed by stacking a sequence of simple bijections, each of which is a shallow neural network, and thus the overall mapping $f$ is a deep neural network. KRnet consists of one  outer loop and $K-1$ inner loops. The outer loop has $K-1$ stages, corresponding to the $K-1$ mappings $f_{i}$ in equation \eqref{eqn_KR} with $i=2,\ldots,K$, and for each stage, an inner loop of $L$ affine coupling layers is defined. More specifically, we have
\begin{equation}\label{krnet_expr}
z = f(x) = L_{N} \circ f_{[K-1]}^{\textsf{outer}} \circ \cdots \circ f_{[1]}^{\textsf{outer}} (x),
\end{equation}
where $f_{[i]}^{\textsf{outer}}$ is defined as
\begin{equation}
f_{[k]}^{\textsf{outer}} = L_S \circ f_{[k, L]}^{\textsf{inner}} \circ \cdots \circ f_{[k,1]}^{\textsf{inner}} \circ L_R.
\end{equation}
Here $f_{[k,i]}^{\textsf{inner}}$ indicates a combination of one affine coupling layer and one scale and bias layer, and $L_N$, $L_S$ and $L_R$ indicate the nonlinear layer, the squeezing layer and the rotation layer, respectively.
More details about KRnet can be found in \cite{tang2020deep,adda_2022}.
