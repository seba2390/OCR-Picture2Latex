We consider single-phase, steady-state Darcy flows. Let $\alpha(s)$ denote an unknown permeability field. The pressure field $u(s,y(s))$ is defined by the following diffusion equation
\begin{align}\label{diffusion}
    -\nabla \cdot\left(\alpha(s)\nabla u(s,\alpha(s))\right)=h(s),\quad s\in \mathcal{S},
\end{align}
where the physical domain $\mathcal{S}=(0,1)^2\in\mathbb{R}^2$ is considered. We use homogeneous Dirichlet boundary conditions on the left and right boundaries and homogeneous Neumann boundary conditions on the top
and bottom boundaries, i.e.,
\begin{align*}
    &u(s,\alpha(s))=0,\quad s\in  \{0\} \times [0, 1],\\
     &u(s,\alpha(s))=0,\quad s\in  \{1\} \times [0, 1],\\
    &\alpha(s)\nabla u(s,\alpha(s))\cdot \mathbf{n}=0,\quad s\in \{(0, 1) \times \{0\}\} \cup \{(0, 1) \times \{1\}\},
\end{align*}
where $\mathbf{n}$ is the outward-pointing normal to the Neumann boundary. The source term is specified as $h(s)=3$.
In the following numerical experiments, the computation domain $\mathcal{S}$ is discretized by a uniform 64×64 grid, i.e., $H=64,W=64$ in \eqref{surroagte_map}. The goal in this paper is to infer the log-permeability field $y(s)=\log \alpha(s)$ from noisy and incomplete observations.

%To enforce the non-negative permeability constraint, we consider the log-permeability as the parameter of interest
%in the inverse problem with $y(s,\xi)=\log \alpha(s,\xi)$. Thus the inverse problem for the above model is to infer the unknown log-permeability field from given noisy pressure measurements. 
% In this paper, the exact log-permeability field $x_{exact}$ is also defined on a 64×64 uniform grid. 
% We consider 64 pressure observations that are uniformly located in $[0.0625 + 0.125i, 0.0625 +
% 0.125i],i = 0, 1, 2,\dots, 7$. 
% Noisy observations are formulated by adding a $5\%$ independent additive Gaussian random noise to the simulation pressure field which is obtained by a mixed finite element formulation
% implemented in FEniCS \cite{langtangen2017solving}.

We assume that the log-permeability field $y(s)$ is a Gaussian random field (GRF), i.e.,\,$y(s)\sim \mathcal{GP}\left(m(s),k(s_1,s_2)\right)$, where $m(s)$ and $k(s_1,s_2)$ are the mean and covariance functions, respectively. Let $s_1=[s_{1,1},s_{1,2}]^\mathsf{T}$ and $s_2=[s_{2,1},s_{2,2}]^\mathsf{T}$ denote two arbitrary spatial locations. The covariance function $k(s_1,s_2)$ is taken as
\begin{align}\label{covariance}
    k(s_1,s_2)=\sigma^2\exp\left(-\sqrt{\left(\frac{s_{1,1}-s_{2,1}}{l_1}\right)^2+\left(\frac{s_{1,2}-s_{2,2}}{l_2}\right)^2}\right),
\end{align}
where $\sigma^2$ is the variance, and $l_1,\,l_2$ are the length scales. 
This random field can be approximated by
a truncated Karhunen-Lo{\`e}ve expansion (KLE),
\begin{align}
    y(s)\approx m(s)+\sum_{k=1}^{d_{KL}}\sqrt{\lambda_k}y_k(s)\xi_k,
\end{align}
where $d_{KL}\in\mathbb{N}_+$, $y_k(s)$ and $\lambda_k$ are the eigenfunctions and
eigenvalues of $k(s_1,s_2)$ and $\{\xi_k\}_{k=1}^{d_{KL}}$ are i.i.d. Gaussian random variables of zero mean and unit variance. 
%The prior of $\{\xi_k\}_{k=1}^{d_{KL}}$ are set to be independent standard normal distributions. 
We set $m(s)=1$ and $\sigma^2=0.5$ in the numerical experiments. %As usual, 
We set $d_{KL}$ large enough such that $95\%$ of the total variance of the exponential covariance function are captured. 

We now generate the datasets as historical data for the training of VAE priors. 
 One can assume that the data are from random fields of different length scales. 
 %are not fixed in the prior distribution. In this work, 
 More specifically, we consider two different experimental setups with an increasing difficulty. The length scales are set to be $l_1 = l_2 = 0.2 + 0.01i,i = 0, 1, 2,\dots, 9$ in test problem 1 and 
% and the corresponding $d_{KL}\in [387,737]$. 
 $l_1 = l_2 = 0.1 + 0.01i,i = 0, 1, 2,\dots, 9$ in test problem 2.
% and the corresponding $d_{KL}\in [800,1800]$. 
%The parameter $y$ is uniformly discretized into a 64×64 grid.
We generate 2000 samples for each length scale and combine them to obtain the prior datasets $\{y^{(i)}\}_{i=1}^N$ for training VAE priors, where $N=20000$. Note that the KLE method
is inappropriate for dealing with varying correlation lengths but the VAE priors in section \ref{vae_gan_section} for characterizing the prior do not have such a limitation.  The architectures of the neural networks used in this paper have been summarized in Appendix. All neural network models are trained on a single NVIDIA GeForce GTX 1080Ti GPU card.

%  Given posterior samples $\{y^{(i)}\}_{i=1}^{N_s}$, the posterior
% mean and variance estimates are computed through \eqref{mean_compute}--\eqref{variance_compute}.
To access the accuracy of the estimated posterior mean field, relative errors are defined as
\begin{align}\label{error_compute}
    \epsilon_{relative}:={\left\Arrowvert\mathbb{E}[y]-y_{exact}\right\Arrowvert}_2/{\Arrowvert y_{exact}\Arrowvert}_2,
\end{align}
where $y_{exact}$ is the exact log-permeability field, and $\mathbb{E}[y]$ can be approximated by computing the posterior mean through \eqref{mean_compute}.

\subsection{Test problem 1}
Given the prior dataset $\{y^{(i)}\}_{i=1}^N$ with $y^{(i)}\in\mathbb{R}^{ 64\times 64}$, the latent variable is set to $x\in \mathbb{R}^{36}$ and then we train the corresponding VAE prior. 
%then train the VAE priors with Algorithm \ref{alg_vae_gan}. For the training hyperparameters 
In Algorithm \ref{alg_vae_gan}, we assign the batch size $n_{batch} = 100$ and the maximum epoch number $E=200$, and 
%. For the optimization in Algorithm \ref{alg_vae_gan}, 
employ the Adam optimizer with a learning rate $\eta=0.0001$. 
The architecture of the VAE prior is given in \ref{vae_nn}. 
%The VAE prior is trained 
%The neural networks are trained 
%with $E=200$ epochs. 
%The training procedure takes about 5 minutes. 
To generate samples that are consistent with the prior dataset,  one can sample a latent variable $x$ from Gaussian distribution $\mathcal{N}(0,\mathbf{I})$, and then generate the samples of $y$ by the learned decoder $p_{y|x,\theta^*}$ , i.e., $y=\mu_{de,\theta^*}(x)$. Some samples generated by the VAE prior are shown in Figure \ref{samplesVAE}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{Bayesian_inference_with_DGM/image/vae_36dim_gaussian_samples.png}
%     \caption{ 64×64 resolution random samples using the decoder $p_{y|x,\theta^*}$ of VAE prior, where $x\in \mathbb{R}^{36}$ is sampled from $\mathcal{N} (0, \mathbf{I})$, test problem 1.}
%     \label{samplesVAE}
% \end{figure}
\begin{figure}
	\centering
	\subfloat[][Prior sample 1]{\includegraphics[width=.28
 \textwidth]{image/Sample_prior1.png}}\quad
	\subfloat[][Prior sample 2]{\includegraphics[width=.28\textwidth]{image/Sample_prior2.png}}\quad
	\subfloat[][Prior sample 3]{\includegraphics[width=.28\textwidth]{image/Sample_prior3.png}}
	\\
	\subfloat[][Prior sample 4]{\includegraphics[width=.28\textwidth]{image/Sample_prior4.png}}\quad
	\subfloat[][Prior sample 5]{\includegraphics[width=.28\textwidth]{image/Sample_prior5.png}}
   \quad \subfloat[][Prior sample 6]{\includegraphics[width=.28\textwidth]{image/Sample_prior6.png}}
	%\caption{Isometry and variance quality for synthetic data.}
	\caption{64×64 resolution random samples generated from $p_{y|x,\theta^*}p_x$ for test problem 1, where $x\in \mathbb{R}^{36}$, $p_{y|x,\theta^*}$ is the decoder of the VAE prior and  $p_x=\mathcal{N} (0, \mathbf{I})$.}
    \label{samplesVAE}
\end{figure}
%It is easy to see that the VAE prior has successfully captured the prior datasets $\{y^{(i)}\}_{i=1}^N$.
%because the patterns and features of the new prior samples are similar to that of the prior datasets.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/prior_data_gaussian_lowwdim11_12.png}
%     \caption{The randomly sampled realizations in the training dataset $\{y^{(i)}\}_{i=1}^N$, test problem 1 .}
%     \label{priordata_lowdim}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/vae_gan_16dim_gaussian_lowwdim11_12_samples.png}
%     \caption{ 64×64 resolution random samples using the decoder $p_{y|x,\theta^*}$, where $x\in \mathbb{R}^{16}$ is sampled from $\mathcal{N} (0, I)$, test problem 1.}
%     \label{samplesVAEGAN_lowdim}
% \end{figure}

% As mentioned above, we only obtain the datasets $\{y^{(i)}\}_{i=1}^N$ from historical sample
% information. Variational Auto-Encoders (VAEs) are powerful models for learning low-dimensional representations of datasets.
%\subsubsection{Surrogate results}
%The most computational cost in the Bayesian inference involves the forward computation and at each optimization step the adjoint method. In order to accelerate the inference, we propose learning the neural networks surrogate
%to replace the adjoint method, which is not only automatic differentiation but also extremely fast with the deep learning frameworks like Tensorflow 2.0. 
% The 2D unit square domain $\mathcal{S}=(0,1)^2$ is discretized into a
% uniform 64x64 grid, i,e., $H=W=64$.
With the dataset $\{y^{(i)}\}_{i=1}^N$, we conduct Algorithm \ref{alg_surrogate} to train the surrogate model.  
%only the input data, i.e.,
%the log-permeability prior 
The loss function for \eqref{diffusion} and the architecture of the surrogate model are given in \ref{surrogate_nn}. In Algorithm \ref{alg_vae_gan}, the batch size and 
the maximum epoch number
are set to $n_{batch}=100$ and  $E=100$ respectively, and the Adam optimizer is employed
%For the optimization , 
 with a learning rate $\eta=0.001$. 
%The networks are trained for $E=100$ epochs.
Figure \ref{surrogate_plot} shows the performance of the trained surrogate model by comparing the prediction of the surrogate model with the simulation given by the finite element method implemented in FEniCS \cite{langtangen2017solving}. 
The difference between the surrogate pressure $\hat{u}$ and the simulation pressure $u$ is defined by $\hat{u}-u$ (see Figure \ref{surrogate_plot}(d)). The relative errors ($\Arrowvert \hat{u}-u \Arrowvert_2/\Arrowvert u \Arrowvert_2$) is 0.05694.
%Using the given true log-permeability as the input of the learned surrogate, we predict the corresponding surrogate pressure field, as shown in the third plot of Figure \ref{surrogate_plot}.  The simulation pressure field illustrated in the second plot of Figure \ref{surrogate_plot} is obtained by a mixed finite element formulation
%implemented in FEniCS \cite{langtangen2017solving}.
% It is seen that the maximum absolute error between the finite element simulation and the surrogate prediction is only about 0.01 ({\color{red}Can you normalize it using the $L_2$ norm of the finite element simulation}). 
%It indicates that the learned surrogate can make a good prediction for the forward computation.
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\textwidth]{image/surrogate_gaussian_plot.png}
%     \caption{Illustration of surrogate results, test problem 1. The four figures from left to right are the exact log-permeability to be estimated, the corresponding pressure computed by the simulator, the corresponding pressure computed by the surrogate model, and their difference.}
%     \label{surrogate_plot}
% \end{figure}
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability]{\includegraphics[width=.28
 \textwidth]{image/Exact.png}}\quad
	\subfloat[][Simulation pressure]{\includegraphics[width=.28\textwidth]{image/Simulation.png}}\\
	\subfloat[][Surrogate pressure]{\includegraphics[width=.28\textwidth]{image/Surrogate.png}}\quad
	\subfloat[][Difference between simulation pressure and surrogate pressure]{\includegraphics[width=.28\textwidth]{image/Difference.png}}
	\caption{Illustration of surrogate results for test problem 1.}
    \label{surrogate_plot}
\end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\textwidth]{image/lowdim_surrogate_plot_1900.png}
%     \caption{Illustration of surrogate results, test problem 1. The four figures from left to right are the true log-permeability to be estimated, the corresponding
% pressure computed by the simulator, the corresponding pressure computed by the surrogate model, and their difference.}
%     \label{lowdim_surrogate_plot}
% \end{figure}
%\subsubsection{Bayesian inverse results}

% Our strategy consists of two main components: data-driven VAE prior and KRnet, which is referred as DR-KRnet. 
Our DR-KRnet is compared with VAEprior-MCMC    
%the performance of the proposed method is compared with VAEprior-MCMC 
which uses MCMC to sample the posterior of the latent variable given by the same VAE prior as in DR-KRnet. We consider 64 pressure observations from locations $[0.0625 + 0.125i, 0.0625 +
0.125i],i = 0, 1, 2,\dots, 7$. 
Noisy observations are formulated by adding $5\%$ independent additive Gaussian noise to the simulated pressure field (see Figure\ \ref{surrogate_plot}(b)).

Using the pre-trained decoder and the surrogate, we seek a KRnet with Algorithm \ref{alg_krnet} to approximate the posterior in the latent space.  For KRnet, we partition the components of $x\in\mathbb{R}^{36}$ to 6 equal groups and deactivate one group after 8 affine coupling layers, where the bijection given by each coupling layer is based on the outputs of a neural network with two fully connected hidden layers of 48 neurons (More detailed about the structure of KRnet can be found in \cite{tang2020deep,adda_2022}). In Algorithm \ref{alg_krnet}, the batch size and 
the maximum epoch number
are set to $n_{batch}=100$ and  $E=5$ respectively, and the Adam optimizer is applied
 with a learning rate $\eta=0.01$. 
 % The Adam optimizer is employed with a learning
% rate $\eta=0.01$. 
The sample size from standard Gaussian distribution is $I=5000$. The sample size for posterior distribution is $N_s=2000$. 
%The loss function of KRnet converges at the third epoch. 
For VAEprior-MCMC, we consider the preconditioned Crank Nicolson MCMC (pCN-MCMC) method \cite{beskos2008mcmc,cotter2013mcmc} 
%in \ref{pcn_alg} $N_{pcn}=10000$ $N_s=2000$
and then run 10000 iterations to ensure its convergence. 
% The step size of the random movement from the current state to a new position is controlled by the free parameter $\gamma$ (see \ref{pcn_alg}). The free parameter is set to $\gamma=0.08$. 
For all implementations of the MCMC algorithm, the last 2000 states are retained and regarded as the posterior samples.  The corresponding acceptance rate (numbers of accepted samples
divided by the total sample size) is $30.64\%$. 

Figure \ref{inverse_vae} and Figure \ref{inverse_vae_mcmc} provide the inversion results given by DR-KRnet and VAEprior-MCMC respectively. It is seen that the two strategies yield consistent mean and variance and posterior samples. %The detailed results of the two methods regarding 
More details about accuracy and efficiency are presented in Table~\ref{test1}, where the relative errors are computed through \eqref{error_compute}. Time consumption for DR-KRnet (see Algorithm \ref{alg_krnet}) and VAEprior-MCMC is the computational cost of approximating the posterior of the latent variable by KRnet and MCMC respectively.
%(corresponding to Algorithm \ref{alg_krnet} and Algorithm \ref{pcn_compute}). 
DR-KRnet yields a smaller relative error than VAEprior-MCMC with a computational cost reduced by half.
%The time consumption for DR-KRnet is about half of that for VAEprior-MCMC, and DR-KRnet can give smaller relative errors.

% he peak-signal-to-noise (PSNR) and the structural similarity index measure (SSIM) \cite{hore2010image} are two well-known image quality metrics. The PSNR and SSIM are shown in Table \ref{t1}. Compared with inversion results through VAE priors, the posterior means through VAE-GAN priors are of better image quality in terms of the PSNR and SSIM. 
% \begin{figure}
%     \centering
%      \includegraphics[width=1.0\textwidth]{Bayesian_inference_with_DGM/image/agaussian_inverse_36dim_xia_vae.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and variance of the estimated posterior distribution through DR-KRnet in the first row, test problem 1. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae}
% \end{figure}
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability field]{\includegraphics[width=.28
 \textwidth]{image/KRnet_exact_lowdim.png}}\quad
	\subfloat[][Posterior mean]{\includegraphics[width=.28\textwidth]{image/KRnet_mean_lowdim.png}}\quad
	\subfloat[][Posterior variance]{\includegraphics[width=.28\textwidth]{image/KRnet_variance_lowdim.png}}
	\\
	\subfloat[][Posterior sample 1]{\includegraphics[width=.28\textwidth]{image/KRnet_p1_lowdim.png}}\quad
	\subfloat[][Posterior sample 2]{\includegraphics[width=.28\textwidth]{image/KRnet_p2_lowdim.png}}
   \quad \subfloat[][Posterior sample 3]{\includegraphics[width=.28\textwidth]{image/KRnet_p3_lowdim.png}}
	%\caption{Isometry and variance quality for synthetic data.}
	\caption{The inversion results of DR-KRnet for test problem 1. }
    \label{inverse_vae}
\end{figure}
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability field]{\includegraphics[width=.28
 \textwidth]{image/MCMC_exact.png}}\quad
	\subfloat[][Posterior mean]{\includegraphics[width=.28\textwidth]{image/MCMC_mean.png}}\quad
	\subfloat[][Posterior variance]{\includegraphics[width=.28\textwidth]{image/MCMC_variance.png}}
	\\
	\subfloat[][Posterior sample 1]{\includegraphics[width=.28\textwidth]{image/MCMC_p1.png}}\quad
	\subfloat[][Posterior sample 2]{\includegraphics[width=.28\textwidth]{image/MCMC_p2.png}}
   \quad \subfloat[][Posterior sample 3]{\includegraphics[width=.28\textwidth]{image/MCMC_p3.png}}
	%\caption{Isometry and variance quality for synthetic data.}
	\caption{The inversion results of VAEprior-MCMC for test problem 1. }
    \label{inverse_vae_mcmc}
\end{figure}

% \begin{figure}
%     \centering
%     % \includegraphics[width=1.0\textwidth]{image/show_darcy_flow_cnn_xia_8_48_1000_10_8_72_36dim_88.png}
%      \includegraphics[width=1.0\textwidth]{Bayesian_inference_with_DGM/image/a_inverse36dim_xia_baseline_0.02.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and variance of the estimated posterior distribution through VAEprior-MCMC in the first row, test problem 1. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_mcmc}
% \end{figure}

\begin{table}
		\caption{Comparisons of DR-KRnet and VAEprior-MCMC, test problem 1.}
		\label{test1}
		\centering
		\begin{tabular}{lll}
			\toprule
			%\cmidrule(r){1-2}
			 Model&  $\epsilon_{relative}$ &Time consumption\\
			\midrule
			VAEprior-MCMC &0.4014& 5.2224 minutes\\
			DR-KRnet & 0.3914&1.9608 minutes\\
			\bottomrule
		\end{tabular}
\end{table}
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\textwidth]{image/lowdim_vae_1900.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and standard deviation of the estimated posterior distribution through VAE in the first row. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_lowdim}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\textwidth]{image/lowdim_gan_1900.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and standard deviation of the estimated posterior distribution through VAE-GAN in the first row. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_gan_lowdim}
% \end{figure}

\subsection{Test problem 2}
%\subsubsection{Generative results}
In this case, we consider a larger $d_{KL}$ subject to smaller correlation lengths when generating the prior dataset. 
%is more difficult than test problem 1. 
Using $N=20000$ images as the prior dataset, we train the VAE priors with Algorithm \ref{alg_vae_gan}, where the architecture of the neural
networks is described in \ref{vae_nn}. Here the hyperparameters are the same as those of test problem 1 %. The only difference is the dimensionality 
except that the dimension of the latent variable is increased to 64. 
%$x$, where $x\in \mathbb{R}^{64}$. Since the problem becomes more difficult, we choose a higher dimension to relieve the information compression. 
% Fig.\ \ref{samplesVAEGAN} shows 6 random samples generated by the learned VAE-GAN priors. 
Figure \ref{samplesVAE_highdim} includes 6 realizations given by the decoder of the trained VAE prior. 
%with a 64×64 resolution that are output of the trained decoder network in VAE priors. 
%We can see that the trained VAE prior provides enough prior information and especially, has a great generative ability. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{Bayesian_inference_with_DGM/image/samples_highdim_gaussian_vae_64dim.png}
%     \caption{ 64×64 resolution random samples using the decoder $p_{y|x,\theta^*}$ of VAE prior, where $x\in \mathbb{R}^{64}$ is sampled from $\mathcal{N}(0, \mathbf{I})$, test problem 2.}
%     \label{samplesVAE_highdim}
% \end{figure}
\begin{figure}
	\centering
	\subfloat[][Prior sample 1]{\includegraphics[width=.28
 \textwidth]{image/Sample_prior1_highdim.png}}\quad
	\subfloat[][Prior sample 2]{\includegraphics[width=.28\textwidth]{image/Sample_prior2_highdim.png}}\quad
	\subfloat[][Prior sample 3]{\includegraphics[width=.28\textwidth]{image/Sample_prior3_highdim.png}}
	\\
	\subfloat[][Prior sample 4]{\includegraphics[width=.28\textwidth]{image/Sample_prior4_highdim..png}}\quad
	\subfloat[][Prior sample 5]{\includegraphics[width=.28\textwidth]{image/Sample_prior5_highdim.png}}
   \quad \subfloat[][Prior sample 6]{\includegraphics[width=.28\textwidth]{image/Sample_prior6_highdim.png}}
	
	\caption{64×64 resolution random samples generated from $p_{y|x,\theta^*}p_x$ for test problem 2, where $x\in \mathbb{R}^{64}$, $p_{y|x,\theta^*}$ is the decoder of the VAE prior and  $p_x=\mathcal{N} (0, \mathbf{I})$.}
    \label{samplesVAE_highdim}
\end{figure}
% We apply the datasets to pre-train the VAE for dimension reduction, where the latent variables $x\in \mathbb{R}^{36}$. For the pre-trained decoder $p_{y|x,\theta^*}$, we can sample the latent variables
% $x$ from $\mathcal{N} (0, I)$, and use these latent variables as input to the model $p_{y|x,\theta^*}$. Fig.\  \ref{samplesVAE} shows 6 realizations
% with 64×64 resolution that are output of this decoder network. Fig.\  \ref{reconVAE} illustrates the reconstruction quality of VAE. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/prior_data_gaussian.png}
%     \caption{The randomly sampled realizations in the training dataset $\{y^{(i)}\}_{i=1}^N$, test problem 2.}
%     \label{priordata}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/vae_gan_36dim_gaussian_samples.png}
%     \caption{ 64×64 resolution random samples using the decoder $p_{y|x,\theta^*}$ of VAE-GAN, where $x\in \mathbb{R}^{36}$ is sampled from $\mathcal{N} (0, I)$, test problem 2.}
%     \label{samplesVAEGAN}
% \end{figure}


%%\subsubsection{Surrogate results}
The setups and hyperparameters of the surrogate model are the same as those of test problem 1. The performance of the surrogate model is illustrated in Figure \ref{highdim_surrogate_plot}, where the simulated pressure field given by the finite element method and the predicted pressure field given by the surrogate model are shown in Figure \ref{highdim_surrogate_plot}(b)--(c) respectively for the log-permeability shown in Figure \ref{highdim_surrogate_plot}(a). 
%The given true log-permeability shown in the first plot of Figure \ref{highdim_surrogate_plot} is regarded as the input of the learned surrogate and then the surrogate pressure field is achieved, as shown in the third images of Figure \ref{highdim_surrogate_plot}. 
The difference between the surrogate pressure $\hat{u}$ and the simulation pressure $u$ is defined by $\hat{u}-u$ shown in Figure \ref{highdim_surrogate_plot}(d).
% The maximum absolute error between the finite element simulation and the surrogate prediction is only about 0.03 {\color{red} Can you normalized it?}. %It implies that the learned surrogate can efficiently approximate the forward model with little loss in accuracy.
The relative errors ($\Arrowvert \hat{u}-u \Arrowvert_2/\Arrowvert u \Arrowvert_2$) is 0.08260.
Compared to test problem 1, the prediction of the surrogate model captures the solution sufficiently well with a slight loss in accuracy.    
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\textwidth]{image/surrogate_gaussian_plot_highdim.png}
%     \caption{Illustration of surrogate results, test problem 2. The four figures from left to right are the exact log-permeability to be estimated, the corresponding
% pressure computed by the simulator, the corresponding pressure computed by the surrogate model, and their difference.}
%     \label{highdim_surrogate_plot}
% \end{figure}
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability]{\includegraphics[width=.28
 \textwidth]{image/Exact_highdim.png}}\quad
	\subfloat[][Simulation pressure]{\includegraphics[width=.28\textwidth]{image/Simulation_highdim.png}}\\
	\subfloat[][Surrogate pressure]{\includegraphics[width=.28\textwidth]{image/Surrogate_highdim.png}}\quad
	\subfloat[][Difference between simulation pressure and surrogate pressure]{\includegraphics[width=.28\textwidth]{image/Difference_highdim.png}}
	\caption{Illustration of surrogate results for test problem 2.}
    \label{highdim_surrogate_plot}
\end{figure}
%\subsubsection{Bayesian inversion results}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/vae_36dim_cnn_8_48_xia_nchw.png}
%     \caption{Sample 1-3 are the reconstruction results of True 1-3 through VAE, respectively.}
%     \label{reconVAE}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/vae_gan_vae_reconstru.png}
%     \label{reconVAE_GAN}
%     \caption{Sample 1-3 are the reconstruction results of True 1-3 through VAE-GAN, respectively.}
% \end{figure}

%We apply the DR-KRnet for high-dimensional Bayesian inference and provide a comparison with VAEprior-MCMC. 
Since test problem 2 is more challenging than test problem 1, we consider 225 pressure observations that are uniformly located in $[0.0625 + 0.125i, 0.0625 + 0.0625i],\,i = 0, 1, 2,\dots, 14$. The observations are generated from the simulated pressure field by adding $1\%$ independent additive Gaussian noise. 
%A$1\%$ independent additive Gaussian random noise is added to the simulation pressure field which is obtained by a mixed finite element formulation
%implemented in FEniCS \cite{langtangen2017solving}.

%Based on the pretrained decoder and surrogate model, we use KRnet to approximate $\pi(x|\mathcal{D}_{obs})$ directly with Algorithm \ref{alg_krnet}. 
For DR-KRnet, the architecture of the KRnet is the same as that in test problem 1 except that the components of $x\in\mathbb{R}^{64}$ are divided into 8 even groups. %The loss function of KRnet converges at the third epoch. 
For VAEprior-MCMC, %, we use preconditioned Crank-Nicholson (pCN) algorithm in \ref{pcn_alg} to sample the posterior of the latent variables. 
 we run 10000 iterations and then set the last 2000 states as posterior samples,
 %$\gamma=0.02$, 
 %$N_s=2000$ and $N_{pcn}=10000$, 
 and the acceptance rate is $24.07\%$. The inversion results for the two methods are shown in Figures \ref{inverse_vae_highdim}--\ref{inverse_vae_mcmc_highdim}. It is seen that for this case the inversion result of DR-KRnet is consistent with the exact log-permeability but VAEprior-MCMC fails to approximate the posterior of the latent variables.  
 %and hence DR-KRnet is more reliable than VAEprior-MCMC, especially in high-dimensional cases. 
 In Table \ref{test22}, more scenarios are considered in terms of the dimension of the latent variable $d$. It is seen that 
% different dimensions of the latent variable are considered and then 
DR-KRnet outperforms VAEprior-MCMC in terms of both accuracy and computational cost.
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability field]{\includegraphics[width=.28
 \textwidth]{image/KRnet_exact.png}}\quad
	\subfloat[][Posterior mean]{\includegraphics[width=.28\textwidth]{image/KRnet_mean.png}}\quad
	\subfloat[][Posterior variance]{\includegraphics[width=.3\textwidth]{image/KRnet_variance.png}}
	\\
	\subfloat[][Posterior sample 1]{\includegraphics[width=.28\textwidth]{image/KRnet_p1.png}}\quad
	\subfloat[][Posterior sample 2]{\includegraphics[width=.28\textwidth]{image/KRnet_p2.png}}
   \quad \subfloat[][Posterior sample 3]{\includegraphics[width=.28\textwidth]{image/KRnet_p3.png}}
	%\caption{Isometry and variance quality for synthetic data.}
	\caption{The inversion results of DR-KRnet for test problem 2. }
    \label{inverse_vae_highdim}
\end{figure}
% \begin{figure}
%     \centering
%      \includegraphics[width=1.0\textwidth]{Bayesian_inference_with_DGM/image/ahighdim_gaussian_inverse_64dim_xia_vae.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and variance of the estimated posterior distribution through DR-KRnet in the first row. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_highdim}
% \end{figure}

% Compared with inversion results through VAE priors, the posterior means through VAE-GAN priors perform a better image quality in terms of the PSNR and SSIM.
\begin{figure}
	\centering
	\subfloat[][The exact log-permeability field]{\includegraphics[width=.28
 \textwidth]{image/MCMC_exact_highdim.png}}\quad
	\subfloat[][Posterior mean]{\includegraphics[width=.28\textwidth]{image/MCMC_mean_highdim.png}}\quad
	\subfloat[][Posterior variance]{\includegraphics[width=.3\textwidth]{image/MCMC_variance_highdim.png}}
	\\
	\subfloat[][Posterior sample 1]{\includegraphics[width=.28\textwidth]{image/MCMC_p1_highdim.png}}\quad
	\subfloat[][Posterior sample 2]{\includegraphics[width=.28\textwidth]{image/MCMC_p2_highdim.png}}
   \quad \subfloat[][Posterior sample 3]{\includegraphics[width=.28\textwidth]{image/MCMC_p3_highdim.png}}
	%\caption{Isometry and variance quality for synthetic data.}
	\caption{The inversion results of VAEprior-MCMC for test problem 2. }
    \label{inverse_vae_mcmc_highdim}
\end{figure}
% \begin{figure}
%     \centering
%     % \includegraphics[width=1.0\textwidth]{image/darcy_flow_cnn_xia_8_48_1000_10_8_48_36dim_gan.png}
%      \includegraphics[width=1.0\textwidth]{Bayesian_inference_with_DGM/image/ahighdim_gaussian_inverse_64dim_xia_vae.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and variance of the estimated posterior distribution through VAEprior-MCMC in the first row. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_mcmc_highdim}
% \end{figure}
% \begin{table}
% 		\caption{Comparisons of DR-KRnet and VAEprior-MCMC, test problem 2.}
% 		\label{test2}
% 		\centering
% 		\begin{tabular}{lll}
% 			\toprule
% 			%\cmidrule(r){1-2}
% 			 Model& Time consumption (hours) &Relative errors\\
% 			\midrule
% 			VAEprior-MCMC & 0.1303&2.2340\\
% 			DR-KRnet &  0.06675&0.4171 \\
% 			\bottomrule
% 		\end{tabular}
% \end{table}
\begin{table}
		\caption{Comparisons of DR-KRnet and VAEprior-MCMC for test problem 2.}
		\label{test22}
		\centering
		\begin{tabular}{ccccc}
			\toprule
			%\cmidrule(r){1-2}
			 Model&$d$ & $\epsilon_{relative}$ &Time consumption&Acceptance rate\\
			\midrule
			DR-KRnet &36 &0.3854&1.9842 minutes&-\\
			DR-KRnet & 64& 0.4206&2.6844 minutes &- \\
                \midrule
                VAEprior-MCMC &36 &2.9391&4.9152 minutes&$25.64\%$ \\
			VAEprior-MCMC &64& 2.0616&5.4462 minutes&$24.07\%$\\
                VAEprior-MCMC &128 &2.0760&5.1006 minutes&$27.97\%$\\
			VAEprior-MCMC &256& 2.6581&4.824 minutes& $30.5\%$ \\ 
                VAEprior-MCMC &512& 2.2229&5.388 minutes&$31.8\%$\\
                
   \bottomrule
		\end{tabular}
\end{table}
% \begin{table}
% 		\caption{PSNR and SSIM of the resulting posterior mean through VAE and VAE-GAN, test problem 2.}
% 		\label{t1}
% 		\centering
% 		\begin{tabular}{lll}
% 			\toprule
% 			%\cmidrule(r){1-2}
% 			 Model& VAE &VAE-GAN\\
% 			\midrule
% 			PSNR & 10.637803&11.463784\\
% 			SSIM &  0.7434861& 0.75867176  \\
% 			\bottomrule
% 		\end{tabular}
% \end{table}
%right color map
% \begin{table}
% 		\caption{PSNR and SSIM of the resulting posterior mean through VAE and VAE-GAN, test problem 2.}
% 		\label{t1}
% 		\centering
% 		\begin{tabular}{lll}
% 			\toprule
% 			%\cmidrule(r){1-2}
% 			 Model& VAE &VAE-GAN\\
% 			\midrule
% 			PSNR & 0.78&0.74\\
% 			SSIM &  12.39& 10.93  \\
% 			\bottomrule
% 		\end{tabular}
% \end{table}
% \subsection{Test problem 2: $d_{KL}=(800,1800)$}
% \subsubsection{Generative results}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/prior_data_gaussian_highdim.png}
%     \caption{The randomly sampled realizations in the training dataset $\{y^{(i)}\}_{i=1}^N$, test problem 1 .}
%     \label{priordata_highdim}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{image/samples_highdim_gaussian_vaegan_64dim.png}
%     \caption{ 64×64 resolution random samples using the decoder $p_{y|x,\theta^*}$, where $x\in \mathbb{R}^{16}$ is sampled from $\mathcal{N} (0, I)$, test problem 1.}
%     \label{samplesVAEGAN_highdim}
% \end{figure}


%\subsubsection{Bayesian inverse results}

% \begin{figure}
%     \centering
%     % \includegraphics[width=1.0\textwidth]{image/darcy_flow_cnn_xia_8_48_1000_10_8_48_36dim_gan.png}
%      \includegraphics[width=1.0\textwidth]{image/highdim_gaussian_inverse_64dim_xia_39012_64obs_gan.png}
%     \caption{The exact log-permeability field with a uniform 64 × 64 grid, the mean and standard deviation of the estimated posterior distribution through VAE-GAN in the first row. Three posterior samples from the posterior in the second row.}
%     \label{inverse_vae_gan_highdim}
% \end{figure}