\section{Problem Setup} \label{sec:prob_setup}

\paragraph{Notation.} We denote by $\P(x)$ and $p(x)$ the cumulative distribution function and the probability density function defined on $x\in\mathcal{X}$, respectively. We define $[n]=\{1,2,\ldots,n\}$. The cardinality of set $\mathcal{A}$ is denoted by $|\mathcal{A}|$. Let $\|\cdot\|_2$ be the $\ell_2$ norm of a vector or the spectral norm of a matrix. We denote by $\|\cdot\|_{\rF}$ the Frobenius norm of a matrix. For a matrix $M\in\R^{m\times n}$, we denote by $\sigma_{\min}(M)$ and $\sigma_{\max}(M)$ the smallest singular value and the largest singular value of $M$, respectively. For two probability distributions $\P_1$ and $\P_2$, we denote the Total Variation (TV) distance and the Hellinger distance between these two distributions by $\TV(\P_1,\P_2)$ and $H(\P_1,\P_2)$, respectively. 
% Here the definition of TV distance and the Hellinger distance is standard.

%\chijin{Is $\sigma_{\min}(M)$ the smallest non-zero xxx or just the smallest xxx (which can be zero)?}

We denote by $x\in\mathcal{X}$ and $y\in\mathcal{Y}$ the input data and the objective of the downstream tasks, respectively. Our goal is to predict $y$ using $x$. We assume that $x$ is connected to $y$ through an unobserved latent variable $z\in\mathcal{Z}$ (which is also considered as a representation of $x$). Given the latent variable $z$, the data $x$ and the objective $y$ are independent of each other. To incorporate a large class of real-world applications, such as contrastive learning, we consider the setup where learning can possibly have access to some side information $s\in\mathcal{S}$. We assume that $(x,s,z)\sim\P_{\phi^* }(x,s,z)$ and $y|z\sim\P_{\psi^* }(y|z)$, where $\P_{\phi^* }$ and $\P_{\psi^* }$ are distributions 
indexed by $\phi^* \in\Phi$ and $\psi^* \in\Psi$. It then holds that 
\$
\P_{\phi^* ,\psi^* }(x,y)=\int \P_{\phi^* }(x,z)\P_{\psi^* }(y|z)\, dz,
\$ 
which implies the probability distribution of $(x,y)$ depends on both $\phi^* $ and $\psi^* $. Our setting includes the special case where $y=f^* (z)+\varepsilon$. Function $f^* \in\mathcal{F}$ is the ground truth function  and $\varepsilon\sim\mathcal{N}(0,\sigma^2)$ is a Gaussian noise independent of $z$. In this case, the conditional random variable $y|z\sim\mathcal{N}(f^* (z),\sigma^2)$, whose probability distribution $\P_{f^* }(y|z)$ is parameterized by $f^* \in\mathcal{F}$.



%\jiawei{Add $y=f^* (z)+\varepsilon$}
% Throughout this paper, we make the following mild assumption on the loss function.
% \begin{assumption}\label{loss_bound}
% The loss function $\ell(\cdot,\cdot)$ is $L$-bounded.
% \end{assumption}

Let $\ell(\cdot,\cdot)$ be a loss function. For any pair $(\phi,\psi)\in\Phi\times\Psi$, the optimal predictor $g_{\phi,\psi}$ is defined as follows,
\%\label{opt_est}
g_{\phi,\psi}\leftarrow\argmin_{g}\E_{\P_{\phi,\psi}}\big[\ell\big(g(x),y\big)\big],
\%
where the minimum is taken on all the possible functions and $\E_{\P_{\phi,\psi}}:=\E_{(x,y)\sim\P_{\phi,\psi}(x,y)}$. Our prediction function class is therefore given by
\$
\mathcal{G}_{\Phi,\Psi}:=\big\{ g_{\phi,\psi} \big| \phi \in \Phi, \psi \in \Psi \big\}.
\$
%\chijin{remove the unnecessarily equation number. Only leave the equation number there if we need to refer the equation later.}
In particular, if $\ell(\cdot,\cdot)$ is the squared loss function, then the optimal predictor has a closed form solution $g_{\phi,\psi}(x)=\E_{\P_{\phi,\psi}}[y|x]$ and the prediction function class $\mathcal{G}_{\Phi,\Psi}=\{\E_{\P_{\phi,\psi}}[y|x]\,|\, \phi \in \Phi, \psi \in \Psi \}$.

Given an estimator pair  $(\hat\phi,\hat\psi)$, we define the excess risk with respect to loss $\ell(\cdot,\cdot)$ as
\%\label{error}
{\rm Error}_{\ell}(\hat\phi,\hat\psi):=\E_{\P_{\phi^* ,\psi^* }}\big[\ell\big(g_{\hat\phi,\hat\psi}(x),y\big)\big]-\E_{\P_{\phi^* ,\psi^* }}\big[\ell\big(g_{\phi^* ,\psi^* }(x),y\big)\big],
\%
%\chijin{can we change the name of this quantity from prediction error to excess risk? I guess this does not look like prediction error.}
where $\phi^* $ and $\psi^* $ are the ground truth parameters. By the definition of $g_{\phi^* ,\psi^* }$, we have ${\rm Error}(\hat\phi,\hat\psi)\geq0$. We aim to learn an estimator pair $(\hat\phi,\hat\psi)$ from data that achieves smallest order of the excess risk.



% In many real world problems, learning the latent variable models $\Phi$ is far more complex than learning the prediction functions $\Psi$. 


We consider the setting where the latent variable $z$ cannot be observed. Specifically, we are given many unlabeled data and its corresponding side information $\{x_i,s_i\}^m_{i=1}$ that are sampled i.i.d from an unknown distribution $\P_{\phi^* }(x,s)$ and only a few labeled data $\{x_j,y_j\}^n_{j=1}$ that are sampled i.i.d from an unknown distribution $\P_{\phi^* ,\psi^* }(x,y)$. Here we assume that the labeled data $\{x_j,y_j\}^n_{j=1}$ is independent with the unlabeled data $\{x_i,s_i\}^m_{i=1}$ with understanding $m\gg n$. 


\paragraph{Learning algorithm.} We consider a natural learning algorithm consisting of two phases (Algorithm \ref{mle+erm}). In the unsupervised pretraining phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i,s_i\}^m_{i=1}$. In the downstream tasks learning phase, we use ERM to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$. See algorithm \ref{mle+erm} for details.

\begin{algorithm}[t]
\caption{Two-Phase MLE+ERM}\label{mle+erm}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\{x_i,s_i\}^m_{i=1}$, $\{x_j,y_j\}^n_{j=1}$
\STATE Use unlabeled data and its corresponding side information $\{x_i,s_i\}^m_{i=1}$ to learn $\hat\phi$ via MLE:
\begin{equation}\label{mle}
\hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i,s_i).
\end{equation}
\STATE Fix $\hat\phi$ and use labeled data $\{x_j,y_j\}^n_{j=1}$ to learn $\hat\psi$ via ERM:
\begin{equation}\label{erm}
\hat\psi\leftarrow\argmin_{\psi\in\Psi}\sum^n_{j=1}\ell\big(g_{\hat\phi,\psi}(x_j),y_j\big).
\end{equation}
\STATE {\bf Output:} $\hat\phi$ and $\hat\psi$.
\end{algorithmic}
\end{algorithm}


% Given unlabeled data $\{x_i,s_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, 
We remark that another natural learning algorithm in our setting is to use a two-phase MLE. To be specific, in the unsupervised pretraining phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i,s_i\}^m_{i=1}$ as \eqref{mle}. In the downstream tasks learning phase, we again use MLE to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$. However, we can show that this two-phase MLE scheme fails in the worst case. See Appendix \ref{counter_example} for the details.

%\jiawei{counter example without side information?}
% We make some remarks on our problem setup.
% \begin{itemize}
% \item In some settings (e.g., contrastive learning), there exists some side information,  i.e., we assume that $(x,s,z)\sim\P_{\phi^* }(x,s,z)$ and $y|z\sim\P_{\psi^* }(y|z)$, where $s$ represents the side information. Given unlabeled data $\{x_i,s_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, we can use a variant of Algorithm \ref{mle+erm} to deal with the problem with side information. See Section \ref{contrastive_learning} for the details. 
% \chijin{think about this $x, s, z$ formulation.}.
% \item Given unlabeled data $\{x_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, another natural scheme is to use a two-phase MLE. To be specific, in the unsupervised pretraining phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i\}^m_{i=1}$ as \eqref{mle}. In the downstream tasks learning phase, we again use MLE to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$. However, we can show that this two-phase MLE scheme fails in some cases. See Appendix \ref{counter_example} for the details.
% \end{itemize}


\paragraph{Complexity measures.} Sample complexity guarantee for Algorithm \ref{mle+erm} will be phrased in terms of three complexity measurements, i.e., bracketing number, covering number and the Rademacher complexity, which are defined as follows. We denote by $\mP_{\mathcal{X}}(\Phi)$ a set of parameterized density functions $p_{\phi}(x)$ defined on $x\in\mathcal{X}$
\$
\mP_{\mathcal{X}}(\Phi):=\{p_{\phi}(x)\,|\,\phi\in\Phi\},
\$
where $\phi\in\Phi$ is the parameter.


\begin{definition}[$\epsilon$-Bracket and Bracketing Number] \label{def:bracketing}
Let $\epsilon>0$. Under $\|\cdot\|_1$ distance, a set of functions $\mN_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ is an $\epsilon$-bracket of $\mP_{\mathcal{X}}(\Phi)$ if for any $p_{\phi}(x)\in\mP_{\mathcal{X}}(\Phi)$, there exists a function $\bar p_{\phi}(x)\in\mN_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ such that the following two properties hold:
\begin{itemize}
    \item $\bar p_{\phi}(x)\geq p_{\phi}(x),~\forall x\in\mathcal{X}$
    \item $\|\bar p_{\phi}(x)-p_{\phi}(x)\|_1=\int |\bar p_{\phi}(x)- p_{\phi}(x)|\, dx\leq \epsilon$
\end{itemize}
Note that $\bar p_{\phi}(x)$ need not to belong to $\mP_{\mathcal{X}}(\Phi)$. The bracketing number $N_{\b}(\mP_{\mathcal{X}}(\Phi),\epsilon)$ is the cardinality of the smallest $\epsilon$-bracket needed to cover $\mP_{\mathcal{X}}(\Phi)$. The entropy is defined as the logarithm of the bracketing number.
\end{definition}

To measure the complexity of a function class, we consider the covering number and the Rademacher complexity defined as follows.

\begin{definition}[$\epsilon$-Cover and Covering Number]
Let $\mathcal{F}$ be a function class and $(\mathcal{F},\|\cdot\|)$ be a metric space. For each $\epsilon>0$, a set of functions $\mathcal{N}(\mathcal{F},\epsilon,\|\cdot\|)$ is called an $\epsilon$-cover of $\mathcal{F}$ if for any $f\in\mathcal{F}$, there exists a function $g\in\mathcal{N}(\mathcal{F},\epsilon,\|\cdot\|)$ such that $\|f-g\|\leq \epsilon$. The covering number $N(\mathcal{F},\epsilon,\|\cdot\|)$ is defined as the cardinality of the smallest $\epsilon$-cover needed to cover $\mathcal{F}$.
\end{definition}


\begin{definition}[Rademacher Complexity] \label{def:Rademacher}
Suppose that $x_1,\ldots,x_n$ are sampled i.i.d from a probability distribution $\mathcal{D}$ defined on a set $\mathcal{X}$. Let $\mathcal{G}$ be a class of functions mapping from $\mathcal{X}$ to $\R$. The empirical Rademacher complexity of $\mathcal{G}$ is defined as follows,
\$
\hat R_n(\mathcal{G}):=\E_{\{\sigma_i\}_{i=1}^n \sim \text{Unif}\{\pm 1\}}\bigg[\sup_{g\in\mathcal{G}}\frac{2}{n}\sum^{n}_{i=1}\sigma_i g(x_i)\bigg],
\$
where $\{\sigma_i\}^n_{i=1}$ are independent random variables drawn from the Rademacher distribution and the expectation is taken over the randomness of $\{\sigma_i\}^n_{i=1}$. The Rademacher complexity of $\mathcal{G}$ is defined as
\$
R_n(\mathcal{G}):=\E_{\{x_i\}_{i=1}^n \sim \mathcal{D}}[\hat R_n(\mathcal{G})].
\$
% where the expectation is taken over the randomness of samples $\{x_i\}^n_{i=1}$.
\end{definition}
