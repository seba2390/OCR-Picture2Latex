\section{Introduction}
Unsupervised pretraining aims to efficiently use a large amount of unlabeled data to learn a useful representation that facilitates the learning of downstream tasks. This technique has been widely used in modern machine learning systems including computer vision \citep{caron2019unsupervised,dai2021up}, natural language processing \citep{radford2018improving, devlin2018bert,song2019mass} and speech processing \citep{schneider2019wav2vec,baevski2020wav2vec}. 
% These systems first pretrain a model to extract representations using large-scale of unlabeled data, and then use a small amount of labeled data to either train a simple prediction rule directly on the top of pretrained representation (feature-based approaches), or fine-tune all the parameters within a small neighborhood of pretrained models (fine-tuning based approaches). 
Despite its tremendous empirical success, it remains elusive why pretrained representations, which are learned without the information of downstream tasks, often help to learn the downstream tasks.

There have been several recent efforts trying to understand various approaches of unsupervised pretraining from theoretical perspectives, including language models \cite{saunshi2020mathematical, wei2021pretrained}, contrastive learning \cite{arora2019theoretical, tosh2021contrastive2, tosh2021contrastive, haochen2021provable, saunshi2022understanding}, and reconstruction-based self-supervised learning \cite{lee2021predicting}. While this line of works justifies the use of unsupervised pretraining in the corresponding regimes, many of them do not prove the advantage of unsupervised learning, in terms of sample complexity, even when compared to the naive baseline of performing supervised learning purely using the labeled data. Furthermore, these results only apply to particular approaches of unsupervised pretraining considered in their papers, and crucially rely on the specialized structural assumptions, which do not generalize beyond the settings they studied. Thus, we raise the following question:

\textbf{Can we develop a generic framework which provably explains the advantage of unsupervised pretraining?}

% With , they first pretrain a model to extract representations. 

% Then, with a small amount of labeled data, they either train task-specific heads on the top of the frozen pretrained model (feature-based approaches) or fine-tune all the parameters based on the labeled examples (fine-tuning based approaches). Despite the great empirical success, it is unclear why representations, which are learned without the information of downstream tasks, can help the downstream tasks learning. Thus, we raise the following question:

% \textbf{Is there a general theoretical framework for understanding the advantage of unsupervised pretraining?}



% With large-scale of unlabeled data and only few labeled data, they first train a neural network that outputs a valid representation and then fine-tune the parameters on the downstream tasks. However, it is unclear why the representation, which is learned without the information of downstream tasks, can help the downstream task learning. Thus, we raise the following question:

% \textbf{Is there a general theoretical framework for understanding the advantage of unsupervised pretraining?}

% Nowadays, many attempts have been made to theoretically understand the advantage of unsupervised pretraining under specific models \citep{arora2019theoretical,saunshi2020mathematical,wang2020understanding}. However, provably guarantees under general frameworks are still relatively limited.


% Recently, some attempts have been made to theoretically understand why unsupervised pretraining helps solve downstream tasks. \citet{saunshi2020mathematical} shows $\epsilon$-optimal language models learn features that can linearly solve downstream text classification problems with $\mathcal{O}(\epsilon)$ error. \citet{wei2021pretrained} proves that the pretrained model can recover certain downstream tasks under some non-degeneracy assumptions. \citet{lee2021predicting} studies reconstruction-based self-supervised learning methods and shows that under certain conditional independence assumptions, downstream tasks can be recovered by a linear head on top of representations that are learned based on some auxiliary tasks. However, these works either restrict to specific downstream tasks, fail to show explicit sample complexity, or posit strong assumptions on the structure of the models.

% including  sufficient dimension reduction \citep{fukumizu2004dimensionality,fukumizu2009kernel}, semi-supervised learning \citep{10.1007/11503415_8}, self-supervised learning \citep{10.1145/279943.279962,lee2021predicting} and contrastive learning \citep{arora2019theoretical,wang2020understanding}. However, these works either posit strong assumptions on the structure of the models or fail to give the concrete sample complexity for learning downstream tasks with unsupervised pretraining. 

% Additionally, some theoretical works focus on more practical models, such as random walk model for word embeddings \citep{arora2015latent,hashimoto2016word} and hidden Markov model  \citep{saunshi2020mathematical}. 


This paper answers the above question positively.  

We consider the generic setup where the data $x$ and its label $y$ are connected by an unobserved representation $z$.  Concretely, we assume $(x,z)$ is sampled from a latent variable model $\phi^* $ in an abstract class $\Phi$, and the distribution of label $y$ conditioned on representation $z$ is drawn from distributions $\psi^* $ in class $\Psi$. We considers a natural approach of using Maximum Likelihood Estimation (MLE) for unsupervised pretraining, which approximately learns the latent variable model $\phi^* $ using $m$ unlabeled data. We then use the results of representation learning and Empirical Risk Minimization (ERM) to learn the downstream predictor $\psi^* $ using $n$ labeled data. Investigating this generic setup allows us to bypass the limitation of prior works that are restrictied to the specific approaches for unsupervised pretraining.

We prove that, under a mild ``informative'' condition (Assumption \ref{invariance}), our algorithm achieves a excess risk of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_\Phi/m} + \sqrt{\mathcal{C}_\Psi/n})$ for downstream tasks, where $\mathcal{C}_\Phi, \mathcal{C}_\Psi$ are complexity measures of function classes $\Phi, \Psi$, and $m, n$ are the number of unlabeled and labeled data respectively. Comparing to the baseline of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_{\Phi \circ \Psi}/n})$ achieved by performing supervised learning using only the labeled data, our result rigorously shows the benefit of unsupervised pretraining when we have abundant unlabeled data $m \gg n$ and when the complexity of composite class $\mathcal{C}_{\Phi\circ \Psi}$ is much greater than the complexity of downstream task alone $\mathcal{C}_\Psi$.

Finally, this paper proves that our generic framework (including the ``informative'' condition) captures a wide range of setups for unsupervised pretraining, including (1) factor models with linear regression as downstream tasks; (2) Gaussian mixture models with classification as downstream tasks; and (3) Contrastive learning with linear regression as downstream tasks.

% It is natural to use Maximum Likelihood Estimation (MLE) to estimate $\P_{\hat{\phi}}\approx \P_{\phi^{*}}$ based on $m$ unlabeled data, and utilize this estimator along with $n$ labeled data to learn a prediction function via Empirical Risk Minimization (ERM). We show that this two-phase procedure achieves the following prediction error


% In this paper, we propose a general framework for learning downstream tasks with unsupervised pretraining. We consider a latent variable model where the data $x$ and its label $y$ are connected by a shared representation $z$. To be specific, we assume that $(x,z)$ and $y|z$ are drawn from distributions parameterized by $\Phi$ and $\Psi$, respectively. It is natural to use Maximum Likelihood Estimation (MLE) to estimate $\P_{\hat{\phi}}\approx \P_{\phi^{*}}$ based on $m$ unlabeled data, and utilize this estimator along with $n$ labeled data to learn a prediction function via Empirical Risk Minimization (ERM). We show that this two-phase procedure achieves the following prediction error
% \$
% \tilde{\mathcal{O}}\bigg(\sqrt{\frac{\mathcal{C}_{\Phi}}{m}}+\sqrt{\frac{\mathcal{C}_{\Psi}}{n}}\bigg),
% \$
% where $\mathcal{C}_\Phi, \mathcal{C}_\Psi$ are complexity measures for function classes $\Phi, \Psi$ respectively. With abundant unlabeled data (i.e., $m \rightarrow \infty$), our prediction error scale as $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_{\Psi}/n})$. Compared to the standard error of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_{\Phi \circ \Psi}/n})$ achieved by directly performing supervised learning on labeled data, our result rigorously shows the benefit of unsupervised pretraining when $\mathcal{C}_{\Phi \circ \Psi}$ is much greater than $\mathcal{C}_{\Psi}$.

% Our concrete contributions are as follows:
% \begin{itemize}
%     \item We propose a general framework for learning downstream tasks with unsupervised pretraining. Our model does not posit strong assumptions on data structure, therefore may accommodates more scenarios.
%     \item We propose Algorithm \ref{mle+erm} and give an explicit prediction error bound, which decouple the complexity of learning the shared representation and the complexity of learning the downstream tasks. Our result rigorously shows the advantage of unsupervised pretraining in the sense of reducing the sample complexity.
%     \item We instantiate our framework in three examples, factor models with linear regression as downstream tasks ,Gaussian mixture model with classification as downstream tasks and contrastive learning with linear regression as downstream tasks.
%     %\item We construct a counter example which shows the failure of two-phase MLE.
% \end{itemize}

\subsection{Related work}

\paragraph{Applications and methods for unsupervised pretraining.}
Unsupervised pretraining has achieved tremendous success in image recognition \citep{caron2019unsupervised}, objective detection \citep{dai2021up}, natural language processing \citep{devlin2018bert,radford2018improving,song2019mass} and speech recognition \citep{schneider2019wav2vec,baevski2020wav2vec}. Two most widely-used pretraining approaches are (1) feature-based approaches \citep{brown1992class,mikolov2013distributed,melamud2016context2vec,peters1802deep}, which pretrains a model to extract representations and directly uses the pretrained representations as inputs for the downstream tasks; (2) fine-tuning based approaches, \citep[see, e.g.,][]{devlin2018bert}, which fine-tunes all the model parameters in the neighborhood of pretrained representations based on downstream tasks. \citet{erhan2010does} provides the first empirical understanding on the role of pretraining. They argue that pretraining serves as a form of regularization that effectively guides the learning of downstream tasks.

% To understand why unsupervised pretraining help in deep learning, \citet{erhan2010does} empirically show that pretraining serves as a form of regularization that effectively guides the learning procedure. Though the empirical results are convincing, further theoretical work is still needed.


% Unsupervised pretraining has achieved tremendous success in image recognition \citep{caron2019unsupervised}, objective detection \citep{dai2021up}, natural language processing \citep{devlin2018bert,radford2018improving,song2019mass} and speech recognition \citep{schneider2019wav2vec,baevski2020wav2vec}. Two most widely-used pretraining approaches are (1) feature-based approaches \citep{brown1992class,mikolov2013distributed,melamud2016context2vec,peters1802deep}, which pretrains a model to extract features and uses the pretrained features as inputs for the downstream tasks; (2) fine-tuning based approaches, for example BERT \citep{devlin2018bert}, which pretrains a bidirectional Transformer encoder by artificially designed tasks and then fine-tuning all the model parameters based on downstream tasks. To understand why unsupervised pretraining help in deep learning, \citet{erhan2010does} empirically show that pretraining serves as a form of regularization that effectively guides the learning procedure. Though the empirical results are convincing, further theoretical work is still needed.



A majority of settings where pretraining is used fall into the category of semi-supervised learning \citep[see, e.g., ][]{zhu2005semi}, where a large amount of unlabeled data and a small amount of labeled data are observed during the training process. Semi-supervised learning methods aim to build a better predictor by efficiently utilizing the unlabeled data. Some traditional methods include: generative models \citep[e.g.][]{ratsaby1995learning}, low-density separation \citep{joachims1999transductive,lawrence2004semi,szummer2002information}, and graph-based methods \citep{belkin2006manifold}. While most works in this line propose new methods and show favorable empirical performance, they do not provide rigorous theoretical understanding on the benefit of unsupervised pretraining.

% However, there is no guarantee that the learned classifier behaves near optimal.

% Most of the pretraining setups fall into the semi-supervised learning setting (see \citet{zhu2005semi} for a survey), where a large amount of unlabeled data and a small amount of labeled data are observed during the training process. Semi-supervised learning methods aim to build a better classifier by efficiently utilizing the unlabeled data. Some traditional methods include: generative models \citep[e.g.][]{ratsaby1995learning}, low-density separation \citep{joachims1999transductive,lawrence2004semi,szummer2002information}, and graph-based methods \citep{belkin2006manifold}. However, there is no guarantee that the learned classifier behaves near optimal.

% self-training \citep{amini2022self} and co-training \citep{blum1998combining}. 


% Most of the pretraining works fall into the semi-supervised learning setting (see \citet{zhu2005semi} for a survey), where a large amount of unlabeled data and a small amount of labeled data are observed during the training process. One of the oldest semi-supervised learning methods is generative models \citep[e.g.][]{ratsaby1995learning}, which assumes the unlabeled examples are drawn from some mixture models. Another class of methods called low-density separation aims to place label boundaries in regions with few data points. Algorithms such as transductive support vector machine \citep{joachims1999transductive}, Gaussian processes approach \citep{lawrence2004semi} and information regularization \citep{szummer2002information} are proposed to solve the corresponding problem. Graph-based methods, which define a graph whose nodes are labeled and unlabeled data and edges reflect the similarity of data, attempts to find a smooth function on the graph that gives correct labels on the labeled nodes. Other methods include self-training (see \citet{amini2022self} for a survey), which iteratively updates the classifier and adds the most confidence unlabeled examples and its predicted labels into the training set, and co-training \citep{blum1998combining}, which assumes the existence of two conditional independent features and trains separate classifiers based on different features.

\paragraph{Theoretical understanding of unsupervised pretraining.}
Recent years witness a surge of theoretical results that provide explanations for various unsupervised pretraining methods that extract representations from unlabeled data. For example, \citep{saunshi2020mathematical,wei2021pretrained} considers pretraining vector embeddings in the language models, 
%\chijin{are \citep{saunshi2020mathematical,wei2021pretrained} also contrastive learning? so that it belongs to SSL?}
while \citep{arora2019theoretical,tosh2021contrastive2,tosh2021contrastive,haochen2021provable,saunshi2022understanding,lee2021predicting} consider several Self-Supervised Learning (SSL) approaches for pretraining. In terms of results, \citet{wei2021pretrained} shows that linear predictor on the top of pretrained languange model can recover their ground truth model; \citet{arora2019theoretical, saunshi2020mathematical,tosh2021contrastive2,tosh2021contrastive,saunshi2022understanding} show that the prediction loss of downstream task can be bounded by the loss of unsupervised pretraining tasks. These two lines of results do not prove the sample complexity advantage of unsupervised learning when compared to the baseline of performing supervised learning purely using the labeled data.

% under language model, representations of different words are pretrained and then used for downstream tasks such as emotional analysis \citep{saunshi2020mathematical,wei2021pretrained}; under self-supervised learning, representations are learned based on some auxiliary tasks \citep{arora2019theoretical,tosh2021contrastive2,tosh2021contrastive,haochen2021provable,saunshi2022understanding,lee2021predicting}. Among these works, \citet{wei2021pretrained} proves that the pretrained
% model can recover certain downstream tasks through a linear head under some
% non-degeneracy assumptions. But they treat the pretraining as a blackbox and does not reveal the corresponding sample complexity. \citet{arora2019theoretical, saunshi2020mathematical,tosh2021contrastive2,tosh2021contrastive,saunshi2022understanding} show that one can achieve small downstream population loss using an near optimal representation together with the best possible downstream (linear) classifier, which means learning a good representation is somehow meaningful. However, their work do not explicitly show the advantage of unsupervised pretraining in the sense of reducing the sample complexity. 

The most related results are \citet{lee2021predicting,haochen2021provable}, which explicitly show the sample complexity advantage of certain unsupervised pretraining methods. However, \citet{lee2021predicting} focuses on reconstruction-based SSL, and critically relies on a conditional independency assumption on the feature and its reconstruction conditioned on the label; 
\citet{haochen2021provable} considers contrastive learning, and their results relies on deterministic feature map and the spectral conditions of the normalized adjacency matrix.
% a novel concept of augmentation graph and their result strongly depends on the eigenvalues of the normalized adjacency matrix. 
% \citet{haochen2021provable} proposes a novel concept of augmentation graph and their result strongly depends on the eigenvalues of the normalized adjacency matrix. 
Both results only apply to the specific setups and approaches of unsupervised pretraining in their papers, which do not apply to other setups in general (for instance, the three examples in Section \ref{factor_model}, \ref{gmm}, \ref{contrastive_learning}). On the contrary, this paper develops a generic framework for unsupervised pretraining using only abstract function classes, which applies to a wide range of setups.


% learning a linear predictor on the top of the pretrained representation. Though their work reveal the sample complexity needed for the unsupervied pretraining and the downstream tasks learning, their results strongly rely on either the conditional independency of the data or the augmentation distributions.

% Under the semi-supervised learning setting, a line of works attempt to extract representations from unlabeled data hoping the learned representations could facilitate the downstream tasks. To fill this gap, \citet{saunshi2020mathematical} shows that features learned by $\epsilon$-optimal language models can solve downstream text classification problem with $\mathcal{O}(\sqrt{\epsilon})$ error. However, their analysis cannot be directly applied to more diverse downstream tasks. Later, \citet{wei2021pretrained} proposes an analysis framework to link the pretraining and downstream tasks through a hidden Markov model under certain non-degeneracy conditions. However, their work treats the pretraining as a blackbox and does not reveal the corresponding sample complexity. 



%Instead of directly learning a classifier to classify the data, some methods attempt to extract representations from unlabeled data hoping the learned representations could facilitate the downstream tasks. Theoretical works on learning representation can be dated back to dimensionality reduction \citep{li1991sliced}, which aims to project the high-dimensional input into a low-dimensional subspace that still captures the statistical dependency of the input and its label.  Later works \citep{fukumizu2004dimensionality,fukumizu2009kernel} use kernel method for sufficient dimension reduction and achieve some asymptotic theoretical guarantees. Word embedding, which aims to low-dimensional vector representations for words, achieves great empirical success based on algorithms such as word2vec \citep{mikolov2013efficient}. And many works \citep[e.g.][]{levy2014neural,arora2016latent} have been done to theoretically understand word embedding algorithms. However, these works do not show the efficacy of learned representations on downstream tasks. To fill this gap, \citet{saunshi2020mathematical} shows that features learned by $\epsilon$-optimal language models can solve downstream text classification problem with $\mathcal{O}(\sqrt{\epsilon})$ error. However, their analysis cannot be directly applied to more diverse downstream tasks. Later, \citet{wei2021pretrained} proposes an analysis framework to link the pretraining and downstream tasks through a hidden Markov model under certain non-degeneracy conditions. However, their work treats the pretraining as a blackbox and does not reveal the corresponding sample complexity.


% This work views learning representation as a problem of sufficient dimension reduction for regression, which aims to find a low-dimensional subspace such that the projection of
% the input data (on to the subspace) captures the statistical dependency of the input and its label. Later works \citep{fukumizu2004dimensionality,fukumizu2009kernel} use kernel method for sufficient dimension reduction and achieve some asymptotic results. 

%However, these works do not show how the learned representation facilitate the learning of downstream tasks.

% In order to take the advantage of the learned representations, researchers came up with the idea of unsupervised pretraining. Semi-supervised learning, which aims to make use of both labeled and unlabeled data in machine learning, has been the subject of extensive empirical investigation in the literature (see \citet{zhu2005semi} for survey in this direction). However, theoretical works are relatively limited. Some works, including \citet{Leskes2005TheVO},  \citet{kaariainen2005generalization} present generalization error bounds based on specific settings and assumptions. \citet{10.1007/11503415_8} provide the first PAC-style model for semi-supervised learning, by defining the "compatibility" of a hypothesis. However, the generalization bound given by this work is valid only when the target function has high compatibility (where the measure of compatibility is picked by human) with unlabeled data.

% Another line of theoretical research, which studies unsupervised pretraining under the framework of self-supervised learning, considers the co-training setting \citep{10.1145/279943.279962} under which the input variable can be partitioned into two distinct views.  \citet{10.5555/2980539.2980589} provides a PAC-style analysis for this setting. \citet{10.1007/978-3-540-72927-3_8} provides a semi-supervised algorithm and analyzes its expected regret, which is related to the notion of intrinsic dimensionality. Recently \citet{lee2021predicting} provides theoretical guarantees for self-supervised learning under the co-training setting, assuming approximate conditional independent of the two views given the label. 


%Another well-studied framework for unsupervised pretraining is contrastive learning \citep{arora2019theoretical,wang2020understanding,tosh2021contrastive,saunshi2022understanding}, which assumes access to pairs of data that are more similar than randomly sampled ones and tries to minimize a carefully designed loss function that captures the similarity of the data. They give provably guarantees that shows the learned representations facilitate downstream tasks learning. However, their works do not explicitly reveal the sample complexity needed for representation learning and downstream tasks learning.

% Another well-studied framework for unsupervised pretraining is contrastive learning \citep{arora2019theoretical,wang2020understanding,tosh2021contrastive,saunshi2022understanding}, which assumes access to pairs of data that are more similar than randomly sampled ones and tries to minimize a carefully designed loss function (known as contrastive loss) that captures the similarity of the data. They give provably guarantees showing that by minimizing the contrastive loss, the loss of downstream task can also be minimized. However, these works can only be applied to specific contrastive loss and downstream task. Another shortcoming of this line of research is that, they do not explicitly reveal how pretraining step in contrastive learning help to reduce the sample complexity of the whole learning task.


% More recently, researchers *t to focus on self-supervised learning, which learns useful representations based on auxiliary prediction tasks. A well-studied type of self-supervised learning is contrastive learning \citep{arora2019theoretical,wang2020understanding,tosh2021contrastive2,tosh2021contrastive,haochen2021provable,saunshi2022understanding}, which assumes access to positive and negative pairs and aims to learn a useful representation by minimizing some carefully designed loss functions (known as contrastive loss). \citet{arora2019theoretical} theoretically show that the best linear head on the learned representation gives small downstream task error if the contrastive loss of the learned representation is small. However, their work do not explicitly reveal how pretraining step in contrastive learning help to reduce the sample complexity of the whole learning task. \citet{haochen2021provable} propose a novel concept of augmentation graph and give sample complexity guarantees for learning a linear head on the pretrained feature extractor with labeled data. Their result strongly depends on the properties of the augmentation. For non-contrastive self-supervised learning, \citet{lee2021predicting} study reconstruction-based self-supervised learning method under certain conditional independence assumptions. They show that downstream tasks can be solved by just training a linear head on the top of learned representations, and the sample complexity can be reduced through this procedure. Theoretical works concerning self-supervised learning either make assumptions about the data (for example, conditional independency) and the augmentation distributions or fail to explicitly show the advantage of unsupervised pretraining in the sense of reducing the sample complexity.





% There also exists a flurry of works focusing on learning representations of words (word embeddings) and understanding why the pretrained language models help solve downstream tasks. \citet{arora2015latent} treats the corpus generation as a dynamic process, which is driven by the random walk of a discourse vector, and gives theoretical justifications for models such as word2vec. However, their work does not link the learned representations to downstream tasks. Later, \citet{saunshi2020mathematical} shows that features learned by $\epsilon$-optimal langauge models can solve downstream text classification problem with $\mathcal{O}(\sqrt{\epsilon})$ error. However, their analysis cannot be directly applied to more diverse downstream tasks. \citet{wei2021pretrained} proposes an analysis framework to link the pretraining and downstream tasks through a hidden Markov model under certain non-degeneracy conditions. However, their work treats the pretraining as a blackbox and does not reveal the corresponding sample complexity.

% There also exists a flurry of works focusing on theoretically understanding unsupervised pretraining under practical models. A line of research studies word embedding \citep{arora2015latent,hashimoto2016word}, which aims to learn vector representation of words that captures its meaning. Their works link semantic spaces to word
% co-occurrences through probabilistic semantic analysis and give rigorous justifications for models such as word2vec. Later work on language model \citep{saunshi2020mathematical} uses neural networks to compute low-dimensional features for contexts and mathematically shows its optimality.

% Much research has been struggling to understand why unsupervised pretraining (learning representations) helps through empirical investigation in the field of signal processing, nature language processing and transfer learning \citep{erhan2009difficulty,erhan2010does,bengio2013representation}. However, general theoretical framework for unsupervised pretraining is still relatively limited.


%Recent years, some theoretical frameworks have been proposed for learning particular models. \citet{arora2017provable} assumes the unlabeled data is probabilistically sampled from a deterministic function of the representation and a random seed, and provably learns a valid representation function that maps from the data to the representation. \citet{lee2021predicting} provides theoretical guarantees for self-supervised learning under the approximate conditional independence assumption.


\paragraph{Other approaches for representation learning.} There is another line of recent theoretical works that learn representation via multitask learning. \citet{Baxter_2000} provides generalization bounds for multitask transfer learning assuming a generative model and a shared representation among tasks. \citet{JMLR:v17:15-242} theoretically analyses a general method for learning representations from multitasks and illustrates their method in a linear feature setting. \citet{tripuraneni2021provable,du2020few} provide sample efficient algorithms that solve the problem of multitask linear regression. \citet{tripuraneni2020theory} further considers generic nonlinear feature representations and shows sample complexity guarantees for diverse training tasks. Their results differ from our work because they learn representations by supervised learning using labeled data of other tasks, while our work learns representations by unsupervised learning using unlabeled data.

% Other than working under the framework of semi-supervised learning, the utility of learning representations has also been shown in multitask learning, which aims to learn a shared representation across different tasks to facilitate learning on new tasks. \citet{Baxter_2000} provides generalization bounds for multitask transfer learning assuming a generative model and a shared representation among tasks. \citet{JMLR:v17:15-242} theoretically analyses a general method for learning representations from multitasks and illustrates their method in a linear feature setting. \citet{tripuraneni2021provable,du2020few} provide sample efficient algorithms that solves the problem of multitask linear regression. \citet{tripuraneni2020theory} further considers generic nonlinear feature representations and shows sample complexity guarantees for diverse training tasks. However, these works are not directly relevant for our setting.




