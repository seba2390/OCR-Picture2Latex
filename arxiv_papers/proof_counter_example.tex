\section{Failure of Two-Phase MLE}\label{counter_example}

For simplicity, in the sequel, we consider the case where no side information is available, i.e., we have access to unlabeled data $\{x_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$.  Another natural scheme is to use a two-phase MLE (Algorithm \ref{mle+mle}). To be specific, in the first phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i\}^m_{i=1}$. In the second phase, we use MLE again to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$. 

% \$
% \hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i).
% \$
% In the second phase, we fix $\hat\phi$ learned in the first phase and estimate $\psi^* $ by $\hat\psi$ using data $\{x_j,y_j\}^n_{j=1}$ via MLE, i.e
% \$
% \hat\psi\leftarrow\argmax_{\psi\in\Psi}\sum^n_{j=1}\log p_{\hat\phi,\psi}(x_j,y_j).
% \$

\begin{algorithm}[H]
\caption{Two-phase MLE}\label{mle+mle}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\{x_i\}^m_{i=1}$, $\{(x_j,y_j)\}^n_{j=1}$
\STATE Use unlabeled data $\{x_i\}^m_{i=1}$ to learn $\hat\phi$ via MLE:
\$
\hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i).
\$
\STATE Fix $\hat\phi$ and use labeled data $\{(x_j,y_j)\}^n_{j=1}$ to learn $\hat\psi$ via MLE:
\$
\hat\psi\leftarrow\argmax_{\psi\in\Psi}\sum^n_{j=1}\log p_{\hat\phi,\psi}(x_j,y_j).
\$
\STATE {\bf Output:} $\hat\phi$ and $\hat\psi$.
\end{algorithmic}
\end{algorithm}


Note that the two-phase MLE does not directly associate the learning process with the loss function. Thus, the only way to evaluate the excess risk is to study the total variation distance between $\P_{\hat\phi, \hat\psi}(x,y)$ and $\P_{\phi^* ,\psi^* }(x,y)$. In the pretraining phase, MLE guarantees that the estimator $\mathbb{P}_{\hat{\phi}}$ is close to $\mathbb{P}_{\phi^{*}}$ in the sense of total variation distance (Theorem \ref{tv_mle}). However, it's still possible that for some $x$, $\mathbb{P}_{\hat{\phi}}(x)=0$ while $\mathbb{P}_{\phi^{*}}(x) \neq 0$. This phenomenon may result in $\log p_{\hat\phi,\psi^* }(x_j,y_j)=-\infty$ for some labeled data in the learning of downstream tasks, which will dramatically influence the behaviour of MLE for estimating $\psi^{*}$ and finally lead to the failure of the second phase. Inspired by this idea, we give the following theorem.



\begin{theorem}\label{counter}
There exists $\Phi, \Psi, \phi^{*}\in{\Phi}, \psi^{*}\in{\Psi}$, such that for any constant $c>0$, there exists $m,n \geq c$ such that with probability at least $\frac{1}{2}(1-e^{-1})e^{-1}$, we have
\$
\TV\big(\P_{\hat\phi,\hat\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\geq \frac{1}{8},
\$
where $\hat\phi$ and $\hat\psi$ are the outputs of Algorithm \ref{mle+mle}.
\end{theorem}

\begin{proof}[Proof of Theorem \ref{counter}]
We construct the counter example as follows. Let $(x,y,z)\in\N_{+}\times\N_{+}\times\N_{+}$. We assume that the true parameter $(\phi^* ,\psi^* )=(\phi_1,\psi_1)$, which satisfies
\$
&\P_{\phi_1}(x=k,z=k)=\frac{1}{2^{k}}~~\forall k\in\N_+,\quad \P_{\phi_1}(x=m,z=n)=0 ~~\forall m\neq n,\notag\\
&\P_{\psi_1}(y=k|z=k)=1,~\forall k\in\N_{+}.
\$
% It then holds for all $k\in\N_+$ that
% \$
% \P_{\phi_1}(x=k)=\frac{1}{2^k},\quad \P_{\phi_1,\psi_1}(y=k)=\frac{1}{2^k}.
% \$
For $i\geq 2$, we define $\P_{\phi_i}$ as follows,
\$
&\P_{\phi_i}(x=1,z=1)=\frac{1}{2}+\frac{1}{2^i},\quad\P_{\phi_i}(x=k,z=k)=\frac{1}{2^k}~\forall k\notin\{1,i\}\notag\\
&\P_{\phi_i}(x=m,z=n)=0~\forall m\neq n~{\rm or}~ m=n=i.
\$
We define $\P_{\psi_2}$ as follows, for any $k\in\N_+$,
\$
&\P_{\psi_2}(y=1|z=k)=\frac{1}{4},\quad\P_{\psi_2}(y=2|z=k)=\frac{1}{2}\notag\\
&\P_{\psi_2}(y=j|z=k)=\frac{1}{2^j}~\forall j\notin\{1,2\}.
\$
We denote $
\Phi:=\{\phi_i\,|\, i\in\N_+\}$ and $\Psi:=\{\psi_1,\psi_2\}$. In the sequel, we show that Algorithm \ref{mle+mle} fails on this case. Recall that we denote by $\{x_i\}^m_{i=1}$ and $\{x_j,y_j\}^n_{j=1}$ the unlabeled data and labeled data, respectively. We have the following observations:
\begin{itemize}
    \item We define $i:=\min\{k\neq 1\,|\, k\notin\{x_i\}^m_{i=1}\}$. If we have $1\in\{x_i\}^m_{i=1}$, then the maximizer of likelihood function $\hat\phi$ satisfies $\hat\phi=\phi_i$.
    \item Suppose that $\hat\phi=\phi_i$ for some $i\neq 1$ and $i\in\{y_j\}^n_{j=1}$. We then have $\hat\psi=\psi_2$.
\end{itemize}

We define the event $\mathcal{E}:=\{\exists i\neq 1, \text{ such that } \hat\phi=\phi_i\text{ and }i\in\{y_j\}^n_{j=1}\}$.
Under event $\mathcal{E}$, we have $\hat\phi=\phi_i$ for some $i\neq 1$ and $\hat\psi=\psi_2$, which implies
\%
\TV\big(\P_{\hat\phi,\hat\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)&=\frac{1}{2}\int\int |p_{\phi_i,\psi_2}(x,y)-p_{\phi_1,\psi_1}(x,y)|\,dxdy\notag\\
&\geq \frac{1}{2}\int\bigg|\int p_{\phi_i,\psi_2}(x,y)-p_{\phi_1,\psi_1}(x,y)\,dx\bigg|\,dy\notag\\
&=\frac{1}{2}\int |p_{\phi_i,\psi_2}(y)-p_{\phi_1,\psi_1}(y)|\,dy\notag\\
&\geq \frac{1}{2}|\P_{\phi_i,\psi_2}(y=2)-\P_{\phi_1,\psi_1}(y=2)|=\frac{1}{8}
\%
In the following, we only need to lower bound the probability of event $\mathcal{E}$. Note that
\%
\P(\mathcal{E})&=\P\big(\cup^{\infty}_{i=2}\big\{\hat\phi=\phi, i\in\{y_j\}^n_{j=1}\big\}\big)\notag\\
&=\sum^{\infty}_{i=2}\P\big(\hat\phi=\phi_i, i\in\{y_j\}^n_{j=1}\big)\notag\\
&=\sum^{\infty}_{i=2}\P(\hat\phi=\phi_i)\cdot\P\big(i\in\{y_j\}^n_{j=1}\big)\notag\\
&=\sum^{\infty}_{i=2}\bigg(1-\bigg(1-\frac{1}{2^i}\bigg)^n\bigg)\cdot\P(\hat\phi=\phi_i).
\%
Thus, it holds for any $L\geq 2$ that
\%\label{092110}
\P(\mathcal{E})&\geq\sum^{L}_{i=2}\bigg(1-\bigg(1-\frac{1}{2^i}\bigg)^n\bigg)\cdot\P(\hat\phi=\phi_i)\notag\\
&\geq \bigg(1-\bigg(1-\frac{1}{2^L}\bigg)^n\bigg)\cdot\P\big(\exists 2\leq i\leq L, \hat\phi=\phi_i\big).
\%
Note that
\%\label{092111}
&\P\big(\exists 2\leq i\leq L, \hat\phi=\phi_i\big)\notag\\
&=\P\Big(\big\{1\in\{x_i\}^m_{i=1}\big\}\cap\big\{\exists 2\leq i\leq L, i\notin\{x_i\}^m_{i=1}\big\}\Big)\notag\\
&\geq\P\Big(\big\{1\in\{x_i\}^m_{i=1}\big\}\cap\big\{ L\notin\{x_i\}^m_{i=1}\big\}\Big)\notag\\
&\geq \P\big(1\in\{x_i\}^m_{i=1}\big)+\P\big(L\notin\{x_i\}^m_{i=1}\big)-1\notag\\
&=\P\big(L\notin\{x_i\}^m_{i=1}\big)-\P\big(1\notin\{x_i\}^m_{i=1}\big)\notag\\
&=\bigg(1-\frac{1}{2^L}\bigg)^m-\frac{1}{2^m}.
\%
Combining \eqref{092110} and \eqref{092111}, we have for any $L\geq 2$ 
\%
\P(\mathcal{E})\geq\bigg(1-\bigg(1-\frac{1}{2^L}\bigg)^n\bigg)\cdot\bigg(\bigg(1-\frac{1}{2^L}\bigg)^m-\frac{1}{2^m}\bigg).
\%
Setting $2^L=m=n$, we obtain that
\%
\P(\mathcal{E})\geq\bigg(1-\bigg(1-\frac{1}{m}\bigg)^m\bigg)\cdot\bigg(\bigg(1-\frac{1}{m}\bigg)^m-\frac{1}{2^m}\bigg)\rightarrow (1-e^{-1})\cdot e^{-1},\text{ as }m\rightarrow \infty.
\%
Thus, for any $c>0$, there exists $m,n\geq c$ such that 
\$
\P(\mathcal{E})\geq\frac{1}{2}(1-e^{-1})\cdot e^{-1}.
\$
\end{proof}