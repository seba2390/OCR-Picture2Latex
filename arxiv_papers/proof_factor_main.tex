\subsection{Proofs for Theorem \ref{factor_main}}\label{factor4}
In this section, we verify the utility of Algorithm \ref{mle+erm} by proving Theorem \ref{factor_main}. Recall that the truncated squared loss is defined as 
\begin{align}
\tilde{\ell} (x,y) := (y-x)^{2} \mathbb{I}_{\{(y-x)^2\leq L\}} + L \cdot \mathbb{I}_{\{(y-x)^2 > L\}},
\end{align}
which is $L-$bounded and $2\sqrt{L}-$Lipschitz w.r.t. the first argument. Before proving Theorem \ref{factor_main}, we need to state some core lemmas. 
Recall the definition of $g_{B,\beta}(x)$: 
\begin{align}
g_{B,\beta}(x):=\argmin_{g}\mathbb{E}_{B,\beta} [\ell(g(x),y)].
\end{align}
Since $\ell$ is the squared loss, it's obvious that 
\begin{align}
g_{B, \beta}(x):=\argmin_{g}\mathbb{E}_{B,\beta}[\ell(g(x),y)]=\mathbb{E}_{\P_{B, \beta}(x, y)}[y \mid x]=\beta^{T}B^{T}(BB^{T}+I_{d})^{-1}x.    
\end{align}
The next lemma shows that the optimal predictor under the squared loss $\ell$ and the truncated squared loss $\tilde{\ell}$ stays the same.
% The next lemma shows that the prediction function $g_{B,\beta}$ we defined for the square loss $\ell$ is also suitable for truncated square loss $\tilde{\ell}$.
\begin{lemma} \label{same_prediction}
We denote by $\tilde{g}_{B,\beta}$ the optimal predictor under truncated squared loss, i.e.,
\%
\tilde{g}_{B,\beta}\leftarrow\argmin_{g}\mathbb{E}_{B,\beta}[\tilde{\ell}(g(x),y)].
\%
It then holds that
\begin{align}
\tilde{g}_{B,\beta}(x)=\mathbb{E}_{\P_{B, \beta}(x, y)}[y \mid x]={g}_{B,\beta}(x).
\end{align}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{same_prediction}]
% Let 
% \begin{align}
% h_{B, \beta}=\argmin_{g}\mathbb{E}_{B,\beta}[\tilde{\ell}(g(x),y)].
% \end{align}
% Then, we have 
% \begin{align}
% h_{B, \beta}(x)=\argmin_{a}\mathbb{E}_{B,\beta}[\tilde{\ell}(a,y) \mid x].
% \end{align}
Notice that, the distribution (under parameter $B,\beta$) of $y$ given $x$ is a Gaussian distribution with mean $\mu=\mathbb{E}_{P_{B, \beta}(x, y)}[y \mid x]$ and variance $v^{2}$ (which is of no importance). We define function $f$ as
\begin{align}
f(a )&:=\mathbb{E}_{B,\beta}[\tilde{\ell}(a,y) \mid x]\notag \\
&=  \int_{a-\sqrt{L}}^{a+\sqrt{L}}(y-a)^{2}  \frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y +  \int_{a+\sqrt{L}}^{+\infty} L \frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y \notag\\
&\quad+ \int_{-\infty}^{a-\sqrt{L}} L \frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y.
\end{align}
Then, it holds that
\begin{align}
f'(a)&=\frac{L}{v\sqrt{2\pi}}e^{-\frac{(a-\mu+\sqrt{L})^{2}}{2v^{2}}} - \frac{L}{v\sqrt{2\pi}}e^{-\frac{(a-\mu-\sqrt{L})^{2}}{2v^{2}}} + \int_{a-\sqrt{L}}^{a+\sqrt{L}}2(a-y)\frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y \notag \\
&\quad - \frac{L}{v\sqrt{2\pi}}e^{-\frac{(a-\mu+\sqrt{L})^{2}}{2v^{2}}} + \frac{L}{v\sqrt{2\pi}}e^{-\frac{(a-\mu-\sqrt{L})^{2}}{2v^{2}}} \notag \\
& = \int_{a-\sqrt{L}}^{a+\sqrt{L}}2(a-y)\frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y \notag \\
&= \int_{a-\sqrt{L}}^{a}2(a-y)\frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y + \int_{a}^{a+\sqrt{L}}2(a-y)\frac{1}{v\sqrt{2 \pi}}e^{-\frac{(y-\mu)^{2}}{2v^{2}}}\mathrm{d}y \notag \\
&= \int_{0}^{\sqrt{L}}2z \frac{1}{v\sqrt{2 \pi}}e^{-\frac{(a-z-\mu)^{2}}{2v^{2}}}\mathrm{d}z - \int_{0}^{\sqrt{L}}2z \frac{1}{v\sqrt{2 \pi}}e^{-\frac{(a+z-\mu)^{2}}{2v^{2}}}\mathrm{d}z \notag \\
&= \int_{0}^{\sqrt{L}} \frac{2z}{v\sqrt{2 \pi}}(e^{-\frac{(a-z-\mu)^{2}}{2v^{2}}}-e^{-\frac{(a+z-\mu)^{2}}{2v^{2}}}) \mathrm{d}z.
\end{align}
Notice that for $z\in [0,\sqrt{L}]$,
\begin{align}
e^{-\frac{(a-z-\mu)^{2}}{2v^{2}}}-e^{-\frac{(a+z-\mu)^{2}}{2v^{2}}}>0 \text{ when } a>\mu,
\end{align}
\begin{align}
e^{-\frac{(a-z-\mu)^{2}}{2v^{2}}}-e^{-\frac{(a+z-\mu)^{2}}{2v^{2}}}<0 \text{ when } a<\mu.
\end{align}
Therefore, we have $f'(a)<0$ when $a<\mu$, $f'(a)>0$ when $a>\mu$, which implies that $a=\mu$ is the unique minimizer of $f(a)$, i.e.,
\begin{align}
\tilde{g}_{B,\beta}(x)=\mathbb{E}_{\P_{B, \beta}(x, y)}[y \mid x]={g}_{B,\beta}(x).
\end{align}
\end{proof}


The following lemma shows that the truncation has no significant influence on the excess risk.
\begin{lemma} \label{truncation}
There exist $c_{2}=(D^{2}+1)^3$, such that 
\begin{align}
\operatorname{Error}_{\ell}(\hat{B}, \hat{\beta}) \leq  
\mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)] + \sqrt{\frac{2Lc_{2}}{\pi}}e^{-\frac{L}{2c_{2}}}.
\end{align}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{truncation}]
\begin{align}\label{092720}
\operatorname{Error}_{\ell}(\hat{B}, \hat{\beta}) &= \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{B^{*},\beta^{*}}(x),y)] \notag \\
& =  \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)]\notag \\
& \quad +\mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)] \notag \\
& \quad + \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{B^{*},\beta^{*}}(x),y)] \quad (\leq 0 \text{ since } \tilde{\ell} \leq \ell ) \notag \\
& \leq \sup_{B,\beta} \{ \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{B,\beta}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B,\beta}(x),y)] \} \notag \\ 
& \quad + \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)]
\end{align}
For the first term, we have
\begin{align}\label{092721}
& \quad \sup_{B,\beta} \{ \mathbb{E}_{B^{*},\beta^{*}}[\ell (g_{B,\beta}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B,\beta}(x),y)] \} \notag \\
&= \sup_{B,\beta} \{\mathbb{E}_{B^{*},\beta^{*}} ((g_{B,\beta}(x)-y)^{2}-L) \mathds{1}_{\{(g_{B,\beta}(x)-y)^{2} \geq L\}}  \}.
\end{align}
Notice that
\begin{align}
g_{B,\beta}(x)-y = \beta^{T}B^{T}(BB^{T}+I_{d})^{-1}x-y \sim \mathcal{N}(0, \lambda^{2}),
\end{align}
where 
\begin{align}
\lambda^{2}&=  Var_{B^{*},\beta^{*}} [g_{B,\beta}(x)-y]  \notag \\
&=\mathbb{E}_{B^{*},\beta^{*}} (\beta^{T}B^{T}(BB^{T}+I_{d})^{-1}x-y)^{2} \notag \\
&= \epsilon^{2}+\beta^{T}B^{T}(BB^{T}+I_{d})^{-1} (B^{*}B^{* T}+I_{d})(BB^{T}+I_{d})^{-1}B\beta \notag \\
&\quad + \beta^{* T}\beta^{*}- 2\beta^{T}B^{T}(BB^{T}+I_{d})^{-1}B^{*}\beta^{*} \notag \\
& \leq \epsilon^{2}+\beta^{* T}\beta^{*} + \|(BB^{T}+I_{d})^{-1}\|_{2}^{2} \cdot \|B^{*}B^{* T}+I_{d} \|_{2} \cdot \|B\beta\|_{2}^{2} \notag \\
&\quad + 2 \|(BB^{T}+I_{d})^{-1}\|_{2} \cdot \|B^{*}\beta^{*}\|_{2} \cdot  \|B\beta\|_{2} \notag \\
& \leq \epsilon^{2}+ \beta^{* T}\beta^{*} + D^{4} \|B^{*}B^{* T}+I_{d} \|_{2} + 2D^{2} \|B^{*}\beta^{*}\|_{2} \notag \\
& \leq 1+ D^{2} + D^{4}(D^{2}+1)+2D^{4} \notag \\
& \leq c_{2}.
\end{align}
Therefore 
\begin{align}\label{092722}
& \quad \sup_{B,\beta} \{\mathbb{E}_{B^{*},\beta^{*}} ((g_{B,\beta}(x)-y)^{2}-L) \mathds{1}_{\{(g_{B,\beta}(x)-y)^{2} \geq L\}}  \}   \notag \\
&= \sup_{\lambda} 2\int_{\sqrt{L}}^{+\infty}\frac{1}{\lambda\sqrt{2\pi}}(x^{2}-L)e^{-\frac{x^{2}}{2\lambda^{2}}} \mathrm{d}x \notag \\
&= 2\sup_{\lambda} \bigg\{ -\frac{\lambda}{\sqrt{2\pi}}xe^{-\frac{x^{2}}{2\lambda^{2}}} \bigg|_{\sqrt{L}}^{+\infty} + (\lambda^{2}-L) \int_{\sqrt{L}}^{+\infty}\frac{1}{\lambda\sqrt{2\pi}}e^{-\frac{x^{2}}{2\lambda^{2}}}  \mathrm{d}x \bigg\} \notag \\
& = 2\sup_{\lambda} \bigg\{ \sqrt{\frac{L}{2\pi}} \lambda e^{-\frac{L}{2\lambda^{2}}} + (\lambda^{2}-L) \int_{\sqrt{L}}^{+\infty}\frac{1}{\lambda\sqrt{2\pi}}e^{-\frac{x^{2}}{2\lambda^{2}}}  \mathrm{d}x \bigg\} \notag \\
& \leq 2\sup_{\lambda} \bigg\{ \sqrt{\frac{L}{2\pi}} \lambda e^{-\frac{L}{2\lambda^{2}}} \bigg\} \quad (\text{since }L \geq c_{2} \geq \lambda^{2}) \notag \\
&= \sqrt{\frac{2L c_{2}}{\pi}} e^{-\frac{L}{2c_{2}}}.
\end{align}
The last equation holds since $\lambda e^{-\frac{L}{2\lambda^{2}}}$ monotone increases w.r.t. $\lambda$, and $\lambda \leq \sqrt{c_{1}}$. Combining \eqref{092720}, \eqref{092721} and \eqref{092722}, we finish the proof.
\end{proof}



Now we are ready to prove Theorem \ref{factor_main}.
\begin{proof}[Proof of Theorem \ref{factor_main}]
Note that $\tilde{l}$ is $L-$bounded. By Lemma \ref{same_prediction},  we can apply Theorem \ref{error_bound} to $\tilde{l}$, which gives
\begin{align} \label{apply_main}
&\mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)] \notag \\
&\leq 2 \max _{B \in \mathcal{B}} R_n\left(\tilde{\ell} \circ \mathcal{G}_{B , \mathcal{C}}\right)+L \cdot \sqrt{\frac{2}{n} \log \frac{4}{\delta}}+12 \kappa L \cdot \sqrt{\frac{1}{m} \log \frac{2N_{\b}(\mathcal{P}_{\mathcal{X}}(\mathcal{B}), 1 / m)}{\delta}} . 
\end{align}
Here $\kappa={c_{1}(\sigma_{max}^{*}+1)^{4}}/{\sigma_{min}^{*3}}$ is the transferability defined in Lemma \ref{factor_ti}.


By Lemma \ref{factor_bn}, we have
\begin{align} \label{bracketing_bound}
\log N_{\b}(\mathcal{P}(\mathcal{B}), 1 / m) \leq 4dr \log (24mdr(D^{2}+1)).
\end{align}



Since $\tilde{l}$ is $2\sqrt{L}-$Lipschitz w.r.t. the first argument, the contraction principle (Theoerem 4.12 in \cite{ledoux1991probability}) gives
\begin{align}
R_n\left(\tilde{\ell} \circ \mathcal{G}_{B , \mathcal{C}}\right) \leq 2\sqrt{L} R_n\left(\mathcal{G}_{B , \mathcal{C}}\right).
\end{align}
Therefore it remains to bound $R_n\left(\mathcal{G}_{B , \mathcal{C}}\right).$ By Lemma \ref{factor_rc}, for fixed $B$,
\begin{align}\label{092730}
R_n\left(\mathcal{G}_{B, \mathcal{C}}\right) &= \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} \mathbb{E}_{\{\sigma_{j}\}_{j=1}^{n}}  [\sup_{\beta}\frac{2}{n}\sum_{j=1}^{n} \sigma_{j} g_{B,\beta}(x_{j})] \notag \\
&= \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} \mathbb{E}_{\{\sigma_{j}\}_{j=1}^{n}} [\sup_{\beta}\frac{2}{n}\sum_{j=1}^{n} \sigma_{j} \beta^{T} B^{T} (BB^{T}+I_{d})^{-1} x_{j}] \notag \\
& \leq \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [\frac{2D}{\sqrt{n}} \sup_{j} \|B^{T} (BB^{T}+I_{d})^{-1} x_{j}\|_{2}]  \quad \text{(By Lemma \ref{factor_rc}, since }\|\beta\|_2\leq D ) \notag \\
&= \frac{2D}{\sqrt{n}} \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [ \sup_{j} \|B^{T} (BB^{T}+I_{d})^{-1} x_{j}\|_{2}].
\end{align}
Note that $x_{j}\sim \mathcal{N}(0,B^{*}B^{* T}+ I_{d})$. Therefore $B^{T} (BB^{T}+I_{d})^{-1} x_{j} \sim \mathcal{N}(0,\Sigma)$, where
\begin{align}
\Sigma:= B^{T} (BB^{T}+I_{d})^{-1}  (B^{*}B^{* T}+ I_{d}) (BB^{T}+I_{d})^{-1} B.
\end{align}
Thus, we have
\begin{align}
\Sigma^{-\frac{1}{2}} B^{T} (BB^{T}+I_{d})^{-1} x_{j} \sim \mathcal{N}(0,I_{r}).
\end{align}
Let $u_{j}:=\Sigma^{-\frac{1}{2}} B^{T} (BB^{T}+I_{d})^{-1} x_{j}$, then
\begin{align}\label{092731}
&\mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [ \sup_{j} \|B^{T} (BB^{T}+I_{d})^{-1} x_{j}\|_{2}] \notag \\
&= \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [ \sup_{j} \|\Sigma^{\frac{1}{2} }u_{j}\|_{2}] \notag \\
&\leq \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [  \sup_{j} \|\Sigma^{\frac{1}{2}}\|_{2} \| u_{j}\|_{2}] \notag \\ &\leq \sup \|\Sigma^{\frac{1}{2}}\|_{2}\mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [  \sup_{j}  \| u_{j}\|_{2}] .
\end{align}
By the Theorem 3.1.1 in \cite{vershynin2018high}, $\|u_{j}\|-\sqrt{r}$ is $c_{4}-$subGaussian for some absolute constant $c_{4}$. Therefore, for any $t>0$,
\begin{align}
e^{\mathbb{E}[t \sup_{j}\|u_{j}\|_{2}]} &\leq \mathbb{E} [e^{t \sup_{j}\|u_{j}\|_{2}}] \quad \text{(by Jensen's inequality)}\notag \\ 
& \leq \sum_{j=1}^{n} \mathbb{E} [e^{t \|u_{j}\|_{2}}] \notag \\
& = \sum_{j=1}^{n} \mathbb{E} [e^{t \|u_{j}\|_{2}-\sqrt{r}}]e^{t\sqrt{r}} \notag \\
& \leq \sum_{j=1}^{n} e^{\frac{t^{2}}{2}c_{4}}e^{t\sqrt{r}} \notag \\
&= n e^{t\sqrt{r}+\frac{t^{2}}{2}c_{4}}.
\end{align}
Taking log on both sides, we have
\begin{align}
\mathbb{E}[\sup_{j}\|u_{j}\|_{2}]    \leq \frac{\log n}{t} + \sqrt{r} + \frac{t}{2}c_{4},
\end{align}
which holds for any $t>0$. Take $t=\sqrt{\frac{2 \log n}{c_{4}}}$, we get
\begin{align} \label{092732}
\mathbb{E}[\sup_{j}\|u_{j}\|_{2}]    \leq \sqrt{2c_{4} \log n} + \sqrt{r}.    
\end{align}
Note that
\begin{align}\label{092733}
\|\Sigma\|_{2}&=\|B^{T} (BB^{T}+I_{d})^{-1}  (B^{*}B^{* T}+ I_{d}) (BB^{T}+I_{d})^{-1} B\|_{2} \notag \\
& \leq \|B\|_{2}^{2} \cdot \|(BB^{T}+I_{d})^{-1}\|_{2}^{2} \cdot \|B^{*}B^{* T}+ I_{d}\| \notag \\
& \leq (D^{2} +1)^{2},
\end{align}
i.e., $\sup \|\Sigma^{\frac{1}{2}}\|_{2} \leq (D^{2} +1)$.
Combining \eqref{092730}, \eqref{092731}, \eqref{092732} and \eqref{092733}, we have
\begin{align}
R_n\left(\mathcal{G}_{\phi, \Psi}\right) &\leq \frac{2D}{\sqrt{n}} \mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [ \sup_{j} \|B^{T} (BB^{T}+I_{d})^{-1} x_{j}\|_{2}] \notag    \\
&\leq \frac{2D}{\sqrt{n}} \sup \|\Sigma^{\frac{1}{2}}\|_{2}\mathbb{E}_{\{x_{j}\}_{j=1}^{n}} [  \sup_{j}  \| u_{j}\|_{2}] \notag \\
& \leq \frac{2D}{\sqrt{n}}  (D^{2} +1) (\sqrt{2c_{4} \log n} + \sqrt{r}) ,
\end{align}
which implies
\begin{align} \label{R_bound}
\max_{\phi \in \Phi}R_n\left(\tilde{\ell} \circ \mathcal{G}_{\phi, \Psi}\right) \leq 2\sqrt{L} \max_{\phi in \Phi} R_n\left(\mathcal{G}_{\phi, \Psi}\right) \leq 2\sqrt{L}\frac{2D}{\sqrt{n}}  (D^{2} +1) (\sqrt{2c_{4} \log n} + \sqrt{r})
\end{align}
We are now ready to bound the excess risk.
By Lemma \ref{truncation}, we have
\begin{align}
\operatorname{Error}_{\ell}(\hat{B}, \hat{\beta}) &\leq  
\mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{\hat{B},\hat{\beta}}(x),y)] - \mathbb{E}_{B^{*},\beta^{*}}[\tilde{\ell} (g_{B^{*},\beta^{*}}(x),y)] + \sqrt{\frac{2Lc_{2}}{\pi}}e^{-\frac{L}{2c_{2}}} \notag \\
& \leq 2 \max _{\phi \in \Phi} R_n\left(\tilde{\ell} \circ \mathcal{G}_{\phi, \Psi}\right)+L \cdot \sqrt{\frac{2}{n} \log \frac{4}{\delta}}\notag\\
&\quad+12 \kappa L \cdot \sqrt{\frac{1}{m} \log \frac{2N_{\b}(\mathcal{P}_{\mathcal{X}}(\mathcal{B}), 1 / m)}{\delta}} + \sqrt{\frac{2Lc_{2}}{\pi}}e^{-\frac{L}{2c_{2}}} \notag \\
& \leq 4\sqrt{L}\frac{2D}{\sqrt{n}}  (D^{2} +1) (\sqrt{2c_{4} \log n} + \sqrt{r})+L \cdot \sqrt{\frac{2}{n} \log \frac{4}{\delta}}\notag \\
&\quad+ 12 \kappa L \sqrt{\frac{1}{m}(4dr \log (24mdr(D^{2}+1)) + \log(2/\delta))}  + \sqrt{\frac{2Lc_{2}}{\pi}}e^{-\frac{L}{2c_{2}}},
\end{align}
where the second inequality follows from \eqref{apply_main} and the last inequality follows from \eqref{bracketing_bound}, \eqref{R_bound}. Here $c_4$ is an absolute constant. Note that $c_2=(D^2+1)^3$ and $L=c_{2}\log n$. Thus, we have
\begin{align}
\operatorname{Error}_{\ell}(\hat{B}, \hat{\beta}) &\leq 8\sqrt{2c_{4}}L \sqrt{\frac{1}{n}} + 8L\sqrt{\frac{r}{n}} +L \cdot \sqrt{\frac{2}{n} \log \frac{4}{\delta}}\notag \\ 
&\quad + 12 \kappa L \sqrt{\frac{1}{m}(4dr \log (24mdr(D^{2}+1)) + \log(2/\delta))}  + L\sqrt{\frac{2}{\pi n}}  \notag \\
& \leq \tilde{\mathcal{O}}\bigg(\kappa L \sqrt{\frac{dr}{m}}+ L\sqrt{\frac{r}{n}}\bigg),
\end{align}
where $L=(D^2+1)^3\log n$ and $\kappa={c_{1}(\sigma_{max}^{*}+1)^{4}}/{\sigma_{min}^{*3}}$ for some absolute constants $c_1$.

% \begin{align}
% \operatorname{Error}(\hat{B}, \hat{\beta}) &\leq 8\sqrt{2c_{4}}(D^{2}+1)^{3} \sqrt{\frac{(\log n)^{2}}{n}} + 8(D^{2}+1)^{3} \sqrt{\frac{r \log n}{n}} \notag \\
% &\quad+(D^{2}+1)^{3}\sqrt{\frac{2(\log n)^{2}\log \frac{4}{\delta}}{n}} + \sqrt{\frac{2}{\pi}}(D^{2}+1)^{3}\sqrt{\frac{\log n}{n}} \notag \\ 
% &\quad + 12 c_{1}(D^{2}+1)^{3} \frac{(\sigma_{max}^{*}+1)^{4}}{\sigma_{min}^{*3}} \sqrt{\frac{(\log n)^{2}}{m}(4dr \log (24mdr(D^{2}+1)) + \log(2/\delta))} \notag \\
% & \leq \tilde{\mathcal{O}}(\sqrt{\frac{dr}{m}}+ \sqrt{\frac{r}{n}}).
% \end{align}

% Plugging in $c_2=(D^2+1)^3$, $L=c_{2}\log n$ and $\kappa={c_{1}(\sigma_{max}^{*}+1)^{4}}/{\sigma_{min}^{*3}}$ for some absolute constants $c_1$, we have

\end{proof}


\subsection{Proofs for Theorem \ref{factor_fast_rate}} \label{fast_rate}
In this section, we provide a refined analysis for proving Theorem \ref{factor_fast_rate}. First notice that we can rewrite our model (without $z$) as 
\begin{align}
y=\beta^{* T}C^{*}x+w,   
\end{align}
where $\beta^{*} \in \mathbb{R}^{r \times 1}$, $C^{*}=B^{* T}(B^{*}B^{* T}+I_{d})^{-1}\in \mathbb{R}^{r \times d}$, $x \sim N(0, B^{*}B^{* T}+I_{d})$, $w \sim N(0,\epsilon^{2}+\|\beta^{*}\|_{2}^{2}-\beta^{* T}B^{* T}(B^{*}B^{* T}+I_{d})^{-1}B^{*}\beta^{*})$. Here $w$ and $x$ are independent. Therefore we can write our data as 
\begin{align}
Y=XC^{* T}\beta^{*} + W, 
\end{align}
where $Y=(y_1,\cdots,y_n)^{T}\in \mathbb{R}^{n \times 1}$, $X=(x_{1},\cdots,x_{n})^{T}\in \mathbb{R}^{n \times d}$, $W=(w_1,\cdots,w_n)^{T}\in \mathbb{R}^{n \times 1}$. 

In the first step (MLE), we obtain an estimator $\hat{B}$ and the corresponding estimator $\hat{C}=\hat{B}^{T}(\hat{B}\hat{B}^{T}+I_{d})^{-1}$. Then our estimator $\hat{\beta}$ for the second step (ERM) is given by 
\begin{align}
\hat{\beta} &= \argmin_{\beta} \|Y-X\hat{C}^{T}\beta\|_{2}^{2} \notag \\
&= ((X\hat{C}^{T})^{T}(X\hat{C}^{T}))^{-1}(X\hat{C}^{T})^{T}Y \notag \\
&= (\hat{C}X^{T}X\hat{C}^{T})^{-1}\hat{C}X^{T}Y.
\end{align}
Then our risk is given by
\begin{align}
{\rm Error}_{\ell}(\hat B,\hat\beta)&= \mathbb{E}_{\P_{B^* ,\beta^* }(x,y)}\big[\big(y-g_{\hat B,\hat\beta}(x)\big)^2\big]-\mathbb{E}_{\P_{B^* ,\beta^* }(x,y)}\big[\big(y-g_{B^* ,\beta^* }(x)\big)^2\big] \notag \\
&= \mathbb{E}[(\beta^{* T}C^{*}x + w - \hat{\beta}^{T}\hat{B}^{T}(\hat{B}\hat{B}^{T}+ I_d)^{-1}x)^{2}] - \mathbb{E}[w^{2}] \notag \\
&= \mathbb{E} [(\beta^{* T}C^{*}x - \hat{\beta}^{T}\hat{C}x)^2] \notag \\
&= (\beta^{* T}C^{*}-\hat{\beta}^{T}\hat{C}) (B^{*}B^{* T}+I_{d})     (\beta^{* T}C^{*}-\hat{\beta}^{T}\hat{C})^{T} \notag \\
& \leq \|B^{*}B^{* T}+I_{d}\|_{2} \|\hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*}\|_{2}^{2}
\end{align}
Our goal is to bound $\|\hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*}\|_{2}^{2}$. Consider the SVD of $C^{* T}$ and $\hat{C}^{T}$, i.e., $C^{* T}=U^{*}\Lambda^{*}V^{* T}$, $\hat{C}^{T}=\hat{U}\hat{\Lambda}\hat{V}^T$. Then, we have
% Consider the SVD of $C^{*}$ and $\hat{C}$, i.e., $C^{* T}C^{*} = U^{*} \Lambda^{*} U^{* T}$, $\hat{C}^{T}\hat{C} = \hat{U} \hat{\Lambda} \hat{U}^{T}$, which gives $C^{* T}=U^{*} \Lambda^{* \frac{1}{2}}$, $\hat{C}^{T}=\hat{U} \hat{\Lambda}^{\frac{1}{2}}$. Then we have 
\begin{align}
& \quad \hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*} \notag \\  
&= \hat{C}^{T}(\hat{C}X^{T}X\hat{C}^{T})^{-1}\hat{C}X^{T}Y-C^{* T}\beta^{*} \notag \\ 
& = \hat{C}^{T}(\hat{C}X^{T}X\hat{C}^{T})^{-1}\hat{C}X^{T}(X C^{* T} \beta^{*} + W)-C^{* T}\beta^{*} \notag \\
& = (\hat{C}^{T}(\hat{C}X^{T}X\hat{C}^{T})^{-1}\hat{C}X^{T}X C^{* T} - C^{* T})\beta^{*} + \hat{C}^{T}(\hat{C}X^{T}X\hat{C}^{T})^{-1}\hat{C} X^{T} W \notag \\
&= (\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*})\Lambda^{*}V^{* T} \beta^{*} + \hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W.
\end{align}
Therefore 
\begin{align} \label{bias_variance_decom}
\|\hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*}\|_{2}^{2} &\leq 2 \|(\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*})\|_{2}^{2} \|\Lambda^{*}\|^2_2 \|\beta^{*}\|_{2}^{2}   \notag \\
& \quad + 2\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W\|_{2}^{2}
\end{align}
We give two lemmas for bounding the related terms. The first lemma considers the bias term: 
\begin{lemma} \label{bias_term}
Let $\Sigma :=B^{*}B^{* T}+I_{d} $. If $n \gtrsim \|\Sigma\|^{2} r \log (1/\delta)$, then with probability at least $1-\delta$, 
\begin{align}
\|(\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*})\|_{2}^{2} \leq \mathcal{O} (\|\Sigma\|^{2} \Delta^{2}),    
\end{align}
where $\Delta=dist(\hat{U},U^{*}):=\|\hat{U}\hat{U}^{T}-U^{*}U^{* T} \| $. 
\end{lemma}

The second lemma considers the variance term:
\begin{lemma} \label{variance_term}
Let $\Sigma :=B^{*}B^{* T}+I_{d} $. If $n \gtrsim \|\Sigma\|^{2} r \log (1/\delta)$, then with probability at least $1-\delta$, 
\begin{align}
\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W\|_{2}^{2} \leq \mathcal{O} \bigg(\frac{\sigma^{2}r \log(4/\delta)}{n}\bigg),    
\end{align}
where $\sigma^{2}:=\mathbb{E}(w^{2})=\epsilon^{2}+\|\beta^{*}\|_{2}^{2}-\beta^{* T}B^{* T}(B^{*}B^{* T}+I_{d})^{-1}B^{*}\beta^{*}$ is the variance of $w$.
\end{lemma}
Using this two lemmas together with the decomposition (\ref{bias_variance_decom}), we have 
\begin{align}
\|\hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*}\|_{2}^{2} \leq \mathcal{O}\bigg(\|\beta^{*}\|^{2}\|\Lambda^{*}\|^2\|\Sigma\|^{2} \Delta^{2} + \frac{\sigma^{2}r \log(4/\delta)}{n } \bigg).    
\end{align}
Now it remains to control $\Delta$, which is related to the estimation error of the first step (MLE). The following lemma gives an upper bound for $\Delta$. 
\begin{lemma} \label{Delta_lemma}
If $m \gtrsim \|\Sigma\|^{2} d \log (1/\delta)$, then with probability at least $1-\delta$, 
\begin{align}
\Delta^{2} &\leq \mathcal{O} \bigg(\|\Sigma\|^{2}\frac{d \log (1/\delta)}{m} \lambda_{r}^{-2}(C^{* T}C^{*})\bigg),
\end{align}where $\lambda_{r}(C^{* T}C^{*})$ is the $r$-th (smallest) nonzero eigenvalue of $C^{* T}C^{*}$.
\end{lemma}
\begin{proof}[Proof for Theorem \ref{factor_fast_rate}]
By Lemma \ref{bias_term}, \ref{variance_term}, \ref{Delta_lemma}, we have 
\begin{align}
{\rm Error}_{\ell}(\hat B,\hat\beta) &\leq  \|\Sigma\|  \|\hat{C}^{T}\hat{\beta}- C^{* T}\beta^{*}\|_{2}^{2} \notag \\
&\leq \mathcal{O}(\|\beta^{*}\|^{2}\|\Lambda^{*}\|^2\|\Sigma\|^{3} \Delta^{2} + \|\Sigma\|\frac{\sigma^{2}r \log(4/\delta)}{n } ).    \notag \\
&\leq \mathcal{O}(\|\beta^{*}\|^{2}\|\Lambda^{*}\|^2\|\Sigma\|^{5} \lambda_{r}^{-2}(C^{* T}C^{*}) \frac{d \log (1/\delta)}{m} + \|\Sigma\|\frac{\sigma^{2}r \log(4/\delta)}{n } ).    \notag \\
\end{align}
Using the assumptions that $\|\beta^{*}\| \leq D$ and $\|B^{*}\| \leq D$, we can bound these terms by $D$ and quantities related to ground truth. First notice that
$\Sigma$ have eigenvalues $\sigma_{1}^{* 2}+1 \geq \sigma_{2}^{* 2}+1 \geq \cdots \geq \sigma_{r}^{* 2}+1 \geq 1 = \cdots = 1$, where $\sigma_{i}^{*}$ are singular values of $B^{*}$, therefore $\|\Sigma\| \leq D^{2}+1$. Also, since
\begin{align}
C^{* T}C^{*} &= (B^{*}B^{* T} + I_{d})^{-1} B^{*}B^{* T}   (B^{*}B^{* T} + I_{d})^{-1} \notag \\
&= (B^{*}B^{* T} + I_{d})^{-1} - (B^{*}B^{* T} + I_{d})^{-2} \notag \\
&= \Sigma^{-1} - \Sigma^{-2},
\end{align} 
we know that $C^{* T}C^{*}$ has $r$ nonzero eigenvalues $\{(\sigma_{i}^{*}+\sigma_{i}^{* -1})^{-2}\}_{i=1}^{r}$. Therefore $\|\Lambda^{*}\|^2 = \|C^{* T}C^{*}\| \leq 1/4$, 
\begin{align}
\lambda_{r}^{-2}(C^{* T}C^{*}) &\leq \max ((\sigma_{1}^{*}+\sigma_{1}^{* -1})^{4},(\sigma_{r}^{*}+\sigma_{r}^{* -1})^{4}) \notag \\
& \leq \mathcal{O} (D^{4} + \sigma_{r}^{* -4}).
\end{align}
For $\sigma^{2}$, we have
\begin{align}
\sigma^{2}&=\epsilon^{2}+\|\beta^{*}\|_{2}^{2}-\beta^{* T}B^{* T}(B^{*}B^{* T}+I_{d})^{-1}B^{*}\beta^{*} \notag \\
&\leq 1 +\|\beta^{*}\|^{2} \|I_{r}- B^{* T}(B^{*}B^{* T}+I_{d})^{-1}B^{*}\| \notag \\
&\leq 1 +D^{2}.
\end{align}
Combine all this bounds, we have
\begin{align}
{\rm Error}_{\ell}(\hat B,\hat\beta) &\leq \mathcal{O}(\|\beta^{*}\|^{2}\|\Lambda^{*}\|^2\|\Sigma\|^{5} \lambda_{r}^{-2}(C^{* T}C^{*}) \frac{d \log (1/\delta)}{m} + \|\Sigma\|\frac{\sigma^{2}r \log(4/\delta)}{n } ).    \notag \\
&\leq \mathcal{O} ((D^{2}+1)^{6}(D^{4}+\sigma_{min}^{* -4})\frac{d \log (1/\delta)}{m} + (D^{2}+1)^{2}\frac{r \log(4/\delta)}{n }).
\end{align}
\end{proof}

In the sequel, we give the proofs of Lemma \ref{bias_term}, \ref{variance_term} and \ref{Delta_lemma}. We first prove some additional technical lemmas. The following lemma, which is a simple corollary of \citet{tripuraneni2021provable} Lemma 20, shows the concentration property of empirical covariance matrix.
\begin{lemma} \label{concentration}
Let $\Sigma \in \mathbb{R}^{d}$ be a positive definite matrix. Let $\{x_{i}\}_{i=1}^{n}$ be $d-$dimensional Gaussian random vectors i.i.d. sample from $ N(0,\Sigma)$, $X=(x_{1},\cdots,x_{n})^{T}\in \mathbb{R}^{n \times d}$. Then for any $A,B \in \mathbb{R}^{d \times r}$, we have with probability at least $1-\delta$
\begin{align}
\|A^{T}(\frac{X^{T}X}{n})B - A^{T} \Sigma B \|_{2} \leq \mathcal{O} (\|A\| \|B\| \|\Sigma\| (\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n}).
\end{align}
\end{lemma}
\begin{proof}
We write the SVD of $A$ and $B$: $A=U_{1}\Lambda_{1}V_{1}^{T}$, $B=U_{2}\Lambda_{2}V_{2}^{T}$, where $U_{1}, U_{2} \in \mathbb{R}^{d \times r}$, $\Lambda_{1},\Lambda_{2},V_{1},V_{2} \in \mathbb{R}^{r \times r}$. Then 
\begin{align}
\|A^{T}(\frac{X^{T}X}{n})B - A^{T} \Sigma B \|_{2} &= \|V_{1}\Lambda_{1}U_{1}^{T}(\frac{X^{T}X}{n})U_{2}\Lambda_{2}V_{2}^{T} - V_{1}\Lambda_{1}U_{1}^{T}\Sigma U_{2}\Lambda_{2}V_{2}^{T} \|_{2}    \notag \\
&\leq \|V_{1}\Lambda_{1}\| \|U_{1}^{T}(\frac{X^{T}X}{n})U_{2}-U_{1}^{T}\Sigma U_{2}\| \|\Lambda_{2}V_{2}^{T}\| \notag \\
& \leq \|A\| \|B\| \|U_{1}^{T}(\frac{X^{T}X}{n})U_{2}-U_{1}^{T}\Sigma U_{2}\|.
\end{align}
Now since $U_{1},U_{2} \in \mathbb{R}^{d \times r}$ are projection matrices, we can apply \citet{tripuraneni2021provable} Lemma 20, therefore
\begin{align}
\|U_{1}^{T}(\frac{X^{T}X}{n})U_{2}-U_{1}^{T}\Sigma U_{2}\| \leq \mathcal{O} (\|\Sigma\| (\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n}))    
\end{align}
which gives what we want.
\end{proof}
The following lemma is a basic matrix perturbation result (see \citet{tripuraneni2021provable} Lemma 25).
\begin{lemma} \label{perturbation}
Let $A$ be a positive definite matrix and $E$ another matrix which satisfies $\|EA^{-1}\|\leq \frac{1}{4}$, then $F:=(A+E)^{-1}-A^{-1}$ satisfies $\|F\| \leq \frac{4}{3} \|A^{-1}\| \|EA^{-1}\|$.
\end{lemma}
With these two technical lemmas, we are able to prove Lemma \ref{bias_term}, \ref{variance_term}.
\begin{proof}[Proof of Lemma \ref{bias_term}] 
We consider $\hat{U} \in \mathbb{R}^{d \times r}$ and $\hat{U}_{\perp}^{T} \in \mathbb{R}^{d \times (d-r)}$ be orthonormal projection matrices spanning orthogonal subspaces which are rank $r$ and rank $d-r$ respectively, so that $ \operatorname{range}(\hat{U}) \oplus \operatorname{range}(\hat{U}_{\perp})=\mathbb{R}^d.$ Then $\Delta = dist (\hat{U},U^{*}) = \|\hat{U}_{\perp}^{T}U^{*}\|_{2}$ (see \citet{9584021} Lemma 2.5). Notice that $I_{d}=\hat{U}\hat{U}^{T} + \hat{U}_{\perp}\hat{U}_{\perp}^{T}$, we have 
\begin{align}
& \quad \hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*} \notag \\ 
&= \hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X (\hat{U}\hat{U}^{T} + \hat{U}_{\perp}\hat{U}_{\perp}^{T})U^{*} - U^{*} \notag \\
&= \hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X  \hat{U}\hat{U}^{T}U^{*} + \hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*} - U^{*} \notag \\
&=\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*} + \hat{U}\hat{U}^{T}U^{*} - U^{*} \notag \\
&=\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*} -\hat{U}_{\perp}\hat{U}_{\perp}^{T} U^{*} 
\end{align}
Therefore 
\begin{align} \label{decomposition2}
\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*}\|_{2}^{2} \leq 2\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\|_{2}^{2} + 2\|\hat{U}_{\perp}\hat{U}_{\perp}^{T} U^{*} \|_{2}^{2}.
\end{align}
For the second term, 
\begin{align} \label{second_term}
\|\hat{U}_{\perp}\hat{U}_{\perp}^{T} U^{*} \|_{2}^{2} \leq \|\hat{U}_{\perp}|^{2}   \|\hat{U}_{\perp}^{T} U^{*}\|^{2} \leq \Delta^{2}.
\end{align}
For the first term,
\begin{align} \label{decomposition1}
& \quad \|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| \notag \\    
& = \|\hat{U}(\hat{U}^{T}\frac{X^{T}X}{n}\hat{U})^{-1}\hat{U}^{T}\frac{X^{T}X}{n}\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| \notag \\ 
& = \|\hat{U}((\hat{U}^{T}\Sigma\hat{U})^{-1}+F)(\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}+E_{1})\| \notag \\ 
& \leq \|(\hat{U}^{T}\Sigma\hat{U})^{-1}(\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*})\|+\|(\hat{U}^{T}\Sigma\hat{U})^{-1}E_{1}\|+ \|F\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| + \|FE_{1}\|,
\end{align}
where $E_{1}=\hat{U}^{T}\frac{X^{T}X}{n}\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*} - \hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}$, $F =(\hat{U}^{T}\frac{X^{T}X}{n}\hat{U})^{-1} -(\hat{U}^{T}\Sigma\hat{U})^{-1}$. In order to bound $\|F\|$, let $E=\hat{U}^{T}\frac{X^{T}X}{n}\hat{U} -\hat{U}^{T}\Sigma\hat{U}$, then by Lemma \ref{concentration}, with probability at least $1-\delta$, \begin{align}
\|E\|\leq \mathcal{O}(\|\Sigma\|(\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n})).    
\end{align}
Therefore, since $\lambda_{min}(\Sigma)=1$,
\begin{align}
\|E(\hat{U}^{T}\Sigma\hat{U})^{-1}\| &\leq \|E\|  \|(\hat{U}^{T}\Sigma\hat{U})^{-1}\|   \notag \\
& \leq \|E\| \lambda_{min}(\Sigma)^{-1}\notag \\
& \leq \mathcal{O}(\|\Sigma\|(\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n}))
\end{align}
Notice that $n \gtrsim \|\Sigma\|^{2} r \log (1/\delta)$ implies $\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n}\lesssim \|\Sigma\|^{-1}$. Thus, we show that when $n$ is large enough, we have $\|E(\hat{U}^{T}\Sigma\hat{U})^{-1}\| \leq \frac{1}{4}$. Therefore we can apply Lemma \ref{perturbation}, which gives 
\begin{align} \label{F}
\|F\| &\leq \frac{4}{3} \|E(\hat{U}^{T}\Sigma\hat{U})^{-1}\|  \|(\hat{U}^{T}\Sigma\hat{U})^{-1}\|   \notag \\
&\leq \frac{4}{3} \times \frac{1}{4} \|(\hat{U}^{T}\Sigma\hat{U})^{-1}\| \notag \\
&\leq \frac{1}{3}.
\end{align}
As for $\|E_{1}\|$, directly applying Lemma \ref{concentration}, using $n \gtrsim \|\Sigma\|^{2} r \log (1/\delta)$, we get 
\begin{align} \label{E_1}
\|E_{1}\| &\leq \mathcal{O}(\|\Sigma\|\|\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\|(\sqrt{\frac{r}{n}}+ \frac{r}{n} +\sqrt{ \frac{\log(1/\delta)}{n}}+ \frac{\log(1/\delta)}{n})) \notag \\
& \leq \mathcal{O}(\|\Sigma\|\Delta \|\Sigma\|^{-1}) \notag \\
& \leq \mathcal{O} (\Delta)
\end{align}
Combining (\ref{decomposition1}),(\ref{F})and(\ref{E_1}), we have 
\begin{align} \label{first_term}
& \quad \|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| \notag \\
&\leq  \|(\hat{U}^{T}\Sigma\hat{U})^{-1}(\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*})\|+\|(\hat{U}^{T}\Sigma\hat{U})^{-1}E_{1}\|+ \|F\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| + \|FE_{1}\|   \notag \\
&\leq  \|(\hat{U}^{T}\Sigma\hat{U})^{-1}\|\|(\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*})\|+\|(\hat{U}^{T}\Sigma\hat{U})^{-1}\|\|E_{1}\|+ \|F\|\|\hat{U}^{T}\Sigma\hat{U}_{\perp}\hat{U}_{\perp}^{T}U^{*}\| + \|F\|\|E_{1}\|   \notag \\
&\leq  \lambda_{min}(\Sigma)^{-1}\|\Sigma\|\|\hat{U}_{\perp}^{T}U^{*}\|+\lambda_{min}(\Sigma)^{-1}\|E_{1}\|+ \|F\|\|\Sigma\|\|\hat{U}_{\perp}^{T}U^{*}\| + \|F\|\|E_{1}\|   \notag \\
&\leq  \lambda_{min}(\Sigma)^{-1}\|\Sigma\| \Delta +\lambda_{min}(\Sigma)^{-1}\mathcal{O} (\lambda_{min}(\Sigma) \Delta)+ \frac{1}{3} \lambda_{min}(\Sigma)^{-1}\|\Sigma\|\Delta + \frac{1}{3} \lambda_{min}(\Sigma)^{-1}\mathcal{O} (\lambda_{min}(\Sigma) \Delta)   \notag \\
&\leq \mathcal{O} (\|\Sigma\| \Delta)
\end{align}
Finally, combining (\ref{decomposition2}),(\ref{second_term}) and (\ref{first_term}), we get 
\begin{align}
\|(\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T}X U^{*} - U^{*})\|_{2}^{2} \leq \mathcal{O} (\|\Sigma\|^2\Delta^{2}),    
\end{align}
with probability at least $1-\delta$, which is what we want.
\end{proof}

\begin{proof}[Proof of Lemma \ref{variance_term}]
\begin{align}
\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W\|_{2}^{2} &\leq \|(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W\|_{2}^{2} \notag \\
&=((\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W)^{T}((\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W) \notag \\
&=W^{T} (\frac{1}{n}\frac{X\hat{U}}{\sqrt{n}} (\hat{U}^{T}\frac{X^{T}X}{n}\hat{U} )^{-2}\frac{\hat{U}^{T}X^{T}}{\sqrt{n}})W.
\end{align}
Let $A=\frac{1}{n}\frac{X\hat{U}}{\sqrt{n}} (\hat{U}^{T}\frac{X^{T}X}{n}\hat{U} )^{-2}\frac{\hat{U}^{T}X^{T}}{\sqrt{n}}$, $W=\sigma V$, then $V \sim N(0,I_{n})$. By Hanson-Wright inequality (see \citet{vershynin2018high} Theorem 6.2.1), 
\begin{align}
\mathbb{P}(|V^{T}AV-\mathbb{E}[V^{T}AV]| \geq t) \leq 2 \exp (-c \min(\frac{t^{2}}{\|A\|_{F}^{2}},\frac{t}{\|A\|_{2}})).   
\end{align}
Hence with probability at least $1-\delta$,
\begin{align}
V^{T}AV \leq \mathbb{E}[V^{T}AV] + \mathcal{O}(\|A\|_{F}\sqrt{\log\frac{2}{\delta}})  + \mathcal{O}(\|A\|_{2}\log\frac{2}{\delta}).
\end{align}
Notice that $\mathbb{E}[V^{T}AV]=\text{Tr}(A)$, therefore it remains to bound $\text{Tr}(A)$, $\|A\|_{F}$ and $\|A\|_{2}$. If we define $B=\frac{X\hat{U}}{\sqrt{n}} \in \mathbb{R}^{n \times r}$, then $A=\frac{1}{n}B(B^{T}B)^{-2}B^{T}$. Therefore 
\begin{align}
\text{Tr}(A) &= \text{Tr}(\frac{1}{n}B(B^{T}B)^{-2}B^{T}) \notag \\   
&= \frac{1}{n}\text{Tr}((B^{T}B)^{-2}B^{T}B) \notag \\
&= \frac{1}{n}\text{Tr}((B^{T}B)^{-1}) \notag \\
& \leq \frac{r}{n} \|(B^{T}B)^{-1}\|_{2}
\end{align}
Let the SVD of $B$ be $B=PMQ^{T}$, where $P \in \mathbb{R}^{n \times r}$, $M,Q \in \mathbb{R}^{r \times r}$, then
\begin{align}
\|A\|_{2} &=   \frac{1}{n} \|B(B^{T}B)^{-2}B^{T}\|_{2} \notag \\  
&=   \frac{1}{n} \|PMQ^{T}(QM^{2}Q^{T})^{-2}QMP^{T}\|_{2} \notag \\ 
&=   \frac{1}{n} \|PM^{-2}P^{T}\|_{2} \notag \\ 
& \leq \frac{1}{n}  \|M^{-2}\|_{2} \notag \\
&= \frac{1}{n} \|(B^{T}B)^{-1}\|_{2}
\end{align}
Also notice that $A$ is rank $r$, therefore $\|A\|_{F} \leq \sqrt{r} \|A\|_{2}$. Thus it remains to bound $\|(B^{T}B)^{-1}\|_{2} = \|(\hat{U}^{T}\frac{X^{T}X}{n}\hat{U} )^{-1}\|_{2}$. Let $F =(\hat{U}^{T}\frac{X^{T}X}{n}\hat{U})^{-1} -(\hat{U}^{T}\Sigma\hat{U})^{-1}$. Recall (\ref{F}), which states that with probability at least $1-\delta$, we have 
$\|F\|\leq \frac{1}{3}\lambda_{min}(\Sigma)^{-1}$. Therefore 
\begin{align}
\|(\hat{U}^{T}\frac{X^{T}X}{n}\hat{U} )^{-1}\|  &= \|(\hat{U}^{T}\Sigma \hat{U})^{-1}+F\|  \notag \\
&\leq \|(\hat{U}^{T}\Sigma \hat{U})^{-1}\|+\|F\|  \notag \\
&\leq \mathcal{O}(\lambda_{min}(\Sigma)^{-1}). 
\end{align}
Thus $\|A\| \leq \mathcal{O}(\frac{1}{n}\lambda_{min}(\Sigma)^{-1})$, $\|A\|_{F} \leq \mathcal{O}(\frac{\sqrt{r}}{n}\lambda_{min}(\Sigma)^{-1})$, 
$\text{Tr}(A) \leq \mathcal{O}(\frac{r}{n}\lambda_{min}(\Sigma)^{-1})$. Therefore with probability at least $1-2\delta$,
\begin{align}
V^{T}AV &\leq \mathbb{E}[V^{T}AV] + \mathcal{O}(\|A\|_{F}\sqrt{\log\frac{2}{\delta}})  + \mathcal{O}(\|A\|_{2}\log\frac{2}{\delta})  \notag \\
& \leq \mathcal{O}(\frac{r}{n}\lambda_{min}(\Sigma)^{-1}) + \mathcal{O}(\frac{\sqrt{r}}{n}\lambda_{min}(\Sigma)^{-1}\sqrt{\log \frac{2}{\delta}}) + \mathcal{O}(\frac{1}{n}\lambda_{min}(\Sigma)^{-1}\log \frac{2}{\delta}) \notag \\
& \leq \mathcal{O}(\frac{r}{n}\lambda_{min}(\Sigma)^{-1}\log \frac{2}{\delta}) \notag \\
&= \mathcal{O}(\frac{r}{n}\log \frac{2}{\delta}).
\end{align}
The last line holds since $\lambda_{min}(\Sigma)=1$.
Recall 
\begin{align}
\|\hat{U}(\hat{U}^{T}X^{T}X\hat{U})^{-1}\hat{U}^{T}X^{T} W\|_{2}^{2} = W^{T}AW =\sigma^{2} V^{T}AV,
\end{align}
combining this with the above bound for $V^{T}AV$ yields our desired result.
\end{proof}

Finally we prove Lemma \ref{Delta_lemma} in the following.
\begin{proof}[Proof of Lemma \ref{Delta_lemma}]
In the first step, we have $m$ unlabeled data $\{x_{i}\}_{i=1}^{m}$ i.i.d. sample from $N (0,\Sigma)$. Let $\hat{\Sigma}=\frac{1}{m}\sum_{i=1}^{m}x_{i}x_{i}^{T}$ be the empirical covariance matrix. Then by Lemma \ref{concentration}, with probability at least $1-\delta$, 
\begin{align}
\|\Sigma- \hat{\Sigma}\| \leq \mathcal{O}(\|\Sigma\|(\sqrt{\frac{d}{m}}+\frac{d}{m}+ \sqrt{\frac{\log (1/\delta)}{m}} + \frac{\log (1/\delta)}{m}))    
\end{align}
We claim that
\begin{align}\label{claim_rankk_approx}
\|\hat{B}\hat{B}^{T} - (\hat{\Sigma}-I_{d})\|_{2} \leq \|\hat{\Sigma}-\Sigma\|,   
\end{align}
and the proof of this claim will be at the end of this section.
With the claim,
\begin{align}
\|\hat{B}\hat{B}^{T} - B^{*}B^{* T}\| &= \|\hat{B}\hat{B}^{T} - (\hat{\Sigma}-I_{d}) + (\hat{\Sigma}-I_{d}) - (\Sigma - I_{d})\|  \notag \\
& \leq \|\hat{B}\hat{B}^{T} - (\hat{\Sigma}-I_{d})\| + \|\Sigma - \hat{\Sigma}\| \notag \\
& \leq 2\|\Sigma-\hat{\Sigma}\|.
\end{align}
Notice that
\begin{align}
C^{* T}C^{*} &= (B^{*}B^{* T} + I_{d})^{-1} B^{*}B^{* T}   (B^{*}B^{* T} + I_{d})^{-1} \notag \\
&= (B^{*}B^{* T} + I_{d})^{-1} - (B^{*}B^{* T} + I_{d})^{-2}
\end{align}
Similarly
\begin{align}
\hat{C}^{T}\hat{C} = (\hat{B}\hat{B}^{T} + I_{d})^{-1} - (\hat{B}\hat{B}^{T} + I_{d})^{-2}.
\end{align}
Let $E_{2}=(\hat{B}\hat{B}^{T} + I_{d}) - (B^{*}B^{* T} + I_{d})$, $F_{2} = (\hat{B}\hat{B}^{T} + I_{d})^{-1} - (B^{*}B^{* T} + I_{d})^{-1}$. Then 
\begin{align}
\|E_{2}\| \leq 2\|\Sigma-\hat{\Sigma}\| \leq \mathcal{O}(\|\Sigma\|(\sqrt{\frac{d}{m}}+\frac{d}{m}+ \sqrt{\frac{\log (1/\delta)}{m}} + \frac{\log (1/\delta)}{m})).
\end{align}
Therefore when $m \gtrsim \|\Sigma\|^{2} d \log (1/\delta)$, $\|E_{2}\| \leq \mathcal{O}(\|\Sigma\|\sqrt{\frac{d \log (1/\delta)}{m}})$, $\|E_{2}\Sigma^{-1}\| \leq \|E_{2}\|\|\Sigma^{-1}\| \leq 1/4$. Then we can apply Lemma \ref{perturbation}, which gives 
\begin{align}
\|F_{2}\| &\leq \frac{4}{3}    \|\Sigma^{-1}\| \|E_{2}\Sigma^{-1}\| \notag \\
& \leq \frac{4}{3}    \|\Sigma^{-1}\|^{2} \|E_{2}\| \notag \\
&\leq \mathcal{O}(\lambda_{min}^{-2}(\Sigma)\|\Sigma\|\sqrt{\frac{d \log (1/\delta)}{m}}) \notag \\
&= \mathcal{O}(\|\Sigma\|\sqrt{\frac{d \log (1/\delta)}{m}}).
\end{align}
The last line holds since $\lambda_{min}(\Sigma)=1$.
Thus 
\begin{align}
\|C^{* T}C^{*}- \hat{C}^{T}\hat{C}\| &= \|(\Sigma^{-1}+F_{2})-(\Sigma^{-1}+F_{2})^{2} - (\Sigma^{-1}-\Sigma^{-2})\| \notag \\
&= \|F_{2}- \Sigma^{-1}F_{2}-F_{2}\Sigma^{-1} - F_{2}^{2}\| \notag \\
& \leq \|F_{2}\| + 2\|\Sigma^{-1}\|\|F_{2}\| + \|F_{2}\|^{2} \notag \\
& \leq \mathcal{O}(\|\Sigma\|\sqrt{\frac{d \log (1/\delta)}{m}}).
\end{align}
Therefore by Davis-Kahan theorem, 
\begin{align}
\Delta = dist (U^{*},\hat{U}) &\leq \mathcal{O} (\lambda_{r}^{-1}(C^{* T}C^{*})\|C^{* T}C^{*}- \hat{C}^{T}\hat{C}\|).
\end{align}
Combining the above three inequalities, we have 
\begin{align}
\Delta^{2} &\leq \mathcal{O} (\|\Sigma\|^{2}\frac{d \log (1/\delta)}{m} \lambda_{r}^{-2}(C^{* T}C^{*})). 
\end{align}
Finally we will need to prove the claim (\ref{claim_rankk_approx}).
Notice that the MLE estimator $\hat{B}$ is given by 
\begin{align}
\hat{B} &= \argmax_{B\in \mathbb{R}^{d \times r}} \sum_{i=1}^{m}p_{B}(x_{i})    \notag \\
& =  \argmax_{B\in \mathbb{R}^{d \times r}} (-\log \det (BB^{T}+I_{d})-\text{Tr}(\hat{\Sigma}(BB^{T}+I_{d})^{-1})) \notag \\
& = \argmin_{B\in \mathbb{R}^{d \times r}} (\log \det (BB^{T}+I_{d})+\text{Tr}(\hat{\Sigma}(BB^{T}+I_{d})^{-1}))
\end{align}
Let $\hat{\Sigma}=\hat U\hat{\Lambda}\hat{U}^{T}$ and  $(BB^{T}+I_{d})=U \Lambda U^{T}$, where $\hat U$ and $U$ are orthogonal matrices, $\hat{\Lambda}=\text{diag}(\hat{\lambda}_{1},\cdots, \hat{\lambda}_{d})$, $\Lambda=\text{diag}(\lambda_{1},\cdots, \lambda_{d})$ and $\hat\lambda_1\geq\ldots\geq\hat\lambda_d$, $\lambda_1\geq\ldots\geq\lambda_d$. Since $\operatorname{rank}(BB^T)\leq r$, we have $\lambda_{r+1}= \ldots \lambda_{d}=1$. By Ruhe’s trace inequality (see P341 of \citet{marshall11}), we have 
\begin{align}
\text{Tr}(\hat{\Sigma}(BB^{T}+I_{d})^{-1})) \geq \  \sum_{j=1}^{d}   \lambda_{j}^{-1}\hat{\lambda}_{j}, 
\end{align}
and the equality holds only when the two matrices have simultaneous ordered spectral decomposition, i.e., $U=\hat{U}$. Therefore
\begin{align}
& \quad \min_{B\in \mathbb{R}^{d \times r}} (\log \det (BB^{T}+I_{d})+\text{Tr}(\hat{\Sigma}(BB^{T}+I_{d})^{-1})) \notag \\
&= \min_{\{\lambda_{j}\}_{j=1}^{d}} \sum_{j=1}^{d}( \log \lambda_{j} + \lambda_{j}^{-1}\hat{\lambda}_{j} ) \quad \text{subject to } \lambda_{1} \geq  \cdots \geq \lambda_{r} \geq \lambda_{r+1}=\cdots =\lambda_{d}=1
\end{align}
and the minimum is achieved when $\lambda_{j}=\hat{\lambda}_{j}$, for $j=1, \cdots, r$. Therefore the MLE estimator $\hat{B}$ satisfies $(\hat{B}\hat{B}^{T}+I_{d})=\hat U \Lambda \hat{U}^{T}$ 
where $\Lambda=\text{diag}(\hat{\lambda}_{1},\cdots, \hat{\lambda}_{r},1,\cdots,1)$. Thus, we have $\hat{B}\hat{B}^{T}=\hat U (\Lambda-I_{d}) \hat{U}^{T}$, which implies
\begin{align}
& \quad \|\hat{B}\hat{B}^{T} - (\hat{\Sigma}-I_{d})\|_{2} \notag \\
&= \|\hat U (\Lambda-I_{d}) \hat{U}^{T} - \hat U (\hat{\Lambda}-I_{d}) \hat{U}^{T}\|  \notag \\
& \leq \|\Lambda-\hat{\Lambda}\| \notag \\
& = \max_{j=r+1,\cdots, d} |\hat{\lambda}_{j}-1|\notag \\
& \leq \max_{j=1,\cdots, d} |\hat{\lambda}_{j}-\lambda_{j}(\Sigma)|\notag \\
&\leq \|\hat{\Sigma}-\Sigma\|.
\end{align}
Here the last inequality follows from Weyl's Theorem. Thus, we prove claim (\ref{claim_rankk_approx}).
\end{proof}