\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{blindtext}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[shortlabels]{enumitem}
\usepackage[breakable]{tcolorbox}
% \usepackage{showframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\mythanks}[1]{\thanks{\parbox[t]{0.95\textwidth}{#1}}}

\newcommand\myshade{85}
\colorlet{mylinkcolor}{YellowOrange}
\colorlet{myurlcolor}{violet}
\colorlet{mycitecolor}{Aquamarine}

\newtcolorbox{argbox}{colback=orange!5!white, colframe=orange!99!black, breakable}

\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\Reals}{\mathbb R}
\newcommand{\Identity}{\mathrm I}
\newcommand{\iid}{\stackrel{\textnormal{iid}}{\sim}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\Bernoulli}{\operatorname{Bernoulli}}
\newcommand{\sA}{\mathscr A}
\newcommand{\GP}{\operatorname{GP}}
\newcommand{\zeros}{\bm 0}
\newcommand{\E}{\mathbb E}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\asim}{\stackrel{{}_{\bullet}}{\sim}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Ell}{\mathscr L}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Span}{\operatorname{span}}
\newcommand{\indep}{\stackrel{\textnormal{indep}}{\sim}}
\newcommand{\sM}{\mathscr M}
\newcommand{\LS}{\textnormal{LS}}
\newcommand{\bX}{\bm X}
\newcommand{\bY}{\bm Y}
\newcommand{\bA}{\bm A}
\newcommand{\bZ}{\bm Z}
\newcommand{\bE}{\bm E}
\newcommand{\thetat}{\widetilde \theta}
\newcommand{\Psit}{\widetilde \Psi}
\newcommand{\phit}{\widetilde\phi}
\newcommand{\Ct}{\widetilde C}
\newcommand{\bC}{\bm C}
\newcommand{\uS}{\underline{S}}
\newcommand{\dfrak}{\mathfrak d}
\newcommand{\psihat}{\widehat\psi}

% \usepackage{hyperref}
% \hypersetup{
%   linkcolor  = mylinkcolor!\myshade!black,
%   citecolor  = mycitecolor!\myshade!black,
%   urlcolor   = myurlcolor!\myshade!black,
%   colorlinks = true,
% }

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{principle}{Principle}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{definition}
\newtheorem{example}{Example}

% \definecolor{pastelgreen}{rgb}{0.47, 0.87, 0.47}
% \definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}

\newcommand{\DONE}{\textcolor{green}{\textbf{DONE}}}
\usepackage[colorlinks = true, citecolor = NavyBlue, linkcolor = YellowOrange]{hyperref}
\newcommand{\alert}[1]{\textcolor{YellowGreen}{\textbf{#1}}}

\newcommand{\Data}{\mathcal D}
\newcommand{\Complete}{\mathcal C}
\newcommand{\Beta}{\mathfrak B}

\usepackage{xr}
\usepackage{fancyhdr}
\externaldocument{main}

\begin{document}

% \author{Antonio R. Linero\mythanks{Department of Statistics and Data Sciences, University of Texas at Austin, email: \href{mailto:antonio.linero@austin.utexas.edu}{antonio.linero@austin.utexas.edu}}}
\date{}
\title{Supplement to Estimating Heterogeneous Causal Mediation Effects with Bayesian Decision Tree Ensembles}

\author{
  \normalsize Angela Ting and Antonio R. Linero \\
  \normalsize \emph{Department of Statistics and Data Sciences} \\
  \normalsize \emph{University of Texas at Austin}
}

\maketitle
\doublespacing

\renewcommand{\thesection}{S.\arabic{section}}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thepage}{S.\arabic{page}}
\renewcommand{\thetheorem}{S.\arabic{theorem}}
\renewcommand{\theproposition}{S.\arabic{proposition}}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thefigure}{S.\arabic{figure}}
\renewcommand{\thelemma}{S.\arabic{lemma}}
\thispagestyle{fancy}
\pagestyle{fancy}
\rhead{\textbf{Not for Publication Supplementary Material}}

\section{Bayesian Backfitting Algorithm}

We now present a Bayesian backfitting algorithm for fitting the BCMF model of Section~\ref{sec:bart_mediation}. From Section \ref{sec:bart}, BART induces a posterior on $(T_1, M_1),..., (T_m, M_m)$. We use a Bayesian backfitting algorithm \citep{chipman2010bart} to update each $(T_j, M_j)$, which is based on treating the partial residuals excluding the $j^{\text{th}}$ tree as the response. Let $T_{(j)}$ denote the set of all trees in the sum except tree $T_j$. Similarly, let $M_{(j)}$ denote the set of all terminal nodes except $M_j$. Then, for a sum of trees model $Y_i = \sum_{j=1}^m g(X_i; T_j, M_j) + \epsilon_i$, the vector of partial residuals which excludes the $j$th tree is
\begin{align*}
  R_{ij} \equiv Y_i - \sum_{k \neq j} g(X_i; T_k, M_k).
\end{align*}
$R_{ij}$ can be used to update $(T_j, M_j)$ using the single-tree Metropolis-Hastings algorithm described by \citet{chipman1998bayesian} with $R_{ij}$ now taking the place of the data $Y_i$ since $R_{ij} = g(X_i; T_j, M_j) + \epsilon$ corresponds to a single-tree model.

In our model, $\mu, \zeta, d, \mu_m$, and $\tau_m$ are given BART priors. The mediation model parameters $\mu_m$ and $\tau_m$ can be updated using the Bayesian backfitting algorithm given in \citet{hahn2020bayesian}. To handle the outcome model parameters, let
\begin{align*}
    \mu(x) &= \sum_j g(x; T_j^{\mu}, M_j^{\mu}) \\
    \zeta(x) &= \sum_j g(x; T_j^{\zeta}, M_j^{\zeta}) \\
    d(x) &= \sum_j g(x; T_j^{d}, M_j^{d}).
\end{align*}
Then we define the partial residual for $\mu$ as
\begin{align*}
    R_{i}^{\mu} &= Y_i - A_i \, \zeta(X_i) - M_i \, d(X_i) - \sum_{k \neq j} g(X_i; T_k^{\mu}, M_k^{\mu}) \\
    &=  g(X_i; T_j^{\mu}, M_j^{\mu}) + \epsilon_i
\end{align*}
where $\epsilon_i\sim N(0, \sigma^2)$. Therefore, as in \citet{chipman1998bayesian}, $(T_j^{\mu_Y}, M_j^{\mu_Y})$ can be updated with BART using the above residual as the new response. Similarly, $(T_j^{\zeta}, M_j^{\zeta})$ can be updated with BART using the following partial residual
\begin{align*}
  R_i^\zeta
  &= Y_i - \mu(X_i) - M_i \, d(X_i)
    - A_i \, \sum_{k \ne j} g(X_i; T_k^\zeta, M_k^\zeta) \\
  &= A_i \, g(X_i ; T_j^\zeta, M_j^\zeta) + \epsilon_i.
\end{align*}
Because $A_i$ is binary, this implies that $(T_j^\zeta, M_j^\zeta)$ can be updated using the Bayesian CART algorithm proposed by \citet{chipman1998bayesian} applied to the observations $R_i^\zeta$ such that $A_i = 1$. Finally, let the partial residuals for $d$ be given by
\begin{align*}
  R_i^{d} &= \frac{Y_i - \mu(X_i) - A_i\zeta(X_i)}{M_i}
            - \sum_{k \neq j} g(X_i; T_k^{d}, M_k^{d}) \\
    &= g(X_i; T_j^d, M_j^d) + \eta_i
\end{align*}
where $\eta_i = \frac{\epsilon_i}{M_i} \sim N\left( 0, \frac{\sigma^2}{M_i^2} \right)$. Hence, $(T_j^d, M_j^d)$ can be updated using an update for the heteroskedastic variant of the Bayesian CART with known weights; an algorithm for performing an update for this model is given, for example, by \citet{pratola2020heteroscedastic}.
% where $\frac{\epsilon_i}{M_i(A_i)} \sim N\left(0, \frac{\sigma^2}{M_i(A_i)^2}\right)$. This update requires a heteroskedastic BART model rather than the standard BART model which takes the variance to be constant.


\section{Additional Simulation Results}

\begin{table}[p]
  \centering
  \begin{tabular}{llrrrr}
    \toprule
    Setting & Method & Coverage & RMSE & Bias & Length \\
    \midrule
    BART & BART & 0.96 & 0.01 & 0.00 & 0.05\\
    BART & LSEM & 0.96 & 0.01 & 0.01 & 0.05\\
    LSEM & BART & 0.98 & 0.01 & 0.00 & 0.06\\
    LSEM & LSEM & 0.96 & 0.01 & 0.00 & 0.06\\
    RLEARN & BART & 0.98 & 0.01 & 0.00 & 0.06\\
    RLEARN & LSEM & 0.96 & 0.01 & 0.00 & 0.05\\
    \bottomrule
  \end{tabular}
  \caption{Coverage probability, root mean square error, and absolute bias for $\bar \delta$.} \label{table:simulation_delta_avg}
\end{table}

\begin{table}[p]
  \begin{center}
    \begin{tabular}{llrrrr}
      \toprule
      Setting & Method & Coverage & RMSE & Bias & Length \\
      \midrule
      BART & BART & 0.90 & 0.06 & 0.02 & 0.21\\
      BART & LSEM & 0.96 & 0.06 & 0.01 & 0.21\\
      LSEM & BART & 0.97 & 0.06 & 0.02 & 0.24\\
      LSEM & LSEM & 0.96 & 0.05 & 0.00 & 0.22\\
      RLEARN & BART & 0.90 & 0.05 & 0.02 & 0.20\\
      RLEARN & LSEM & 0.94 & 0.05 & 0.01 & 0.20\\
      \bottomrule
    \end{tabular}
    \caption{Coverage probability, root mean square error, and bias for $\bar \zeta$.} \label{table:simulation_zeta_avg}
  \end{center}
\end{table}


In this section we provide additional results from the simulation study of Section~\ref{sec:simulation}. In Table~\ref{table:simulation_delta_avg} and Table~\ref{table:simulation_zeta_avg} we give the coverage for the parameters $\bar \delta$ and $\bar \zeta$ respectively under all combinations of ground truth (BART, LSEM, and the R-Learner) and estimation method (BART or LSEM). We see in Table~\ref{table:simulation_delta_avg} that the BCMF and LSEM models perform essentially the same, with BART having marginally higher coverage and comperable RMSE, absolute bias, and interval length. In Table~\ref{table:simulation_zeta_avg} we find that the BCMF produces slightly more biased estimates of the direct effect, leading to lower coverage overall.

\begin{figure}
  \centering
  \includegraphics[height= 1\textwidth]{figures/12_coverage_by_size_delta}
  \caption{Coverage of the BCMF (left) and LSEM (right) models for $\delta(X_i)$ as a function of $|\delta(X_i) - \bar \delta|$, displayed as a two-dimensional hexogonal histogram. The proportion of individuals falling in each bin is given by the bin label.}
  \label{fig:coverage_by_size_delta}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height= 1\textwidth]{figures/12_coverage_by_size_zeta}
  \caption{Coverage of the BCMF (left) and LSEM (right) models for $\zeta(X_i)$ as a function of $|\zeta(X_i) - \bar \zeta|$, displayed as a two-dimensional hexogonal histogram. The proportion of individuals falling in each bin is given by the bin label.}
  \label{fig:coverage_by_size_zeta}
\end{figure}

In Figure~\ref{fig:coverage_by_size_delta} and Figure~\ref{fig:coverage_by_size_zeta} we validate the claim in the manuscript that the coverage of the BCMF tends to be worse for individuals whose conditional average effects differ greatly from the average effect. Because the LSEM does not shrink towards homogeneity, we see that the coverage for both the direct and indirect effects is relatively constant as a function of $|\zeta(X_i) - \bar \zeta|$ and $|\delta(X_i) - \bar \delta|$, while the BCMF tends to have poorer coverage as these quantities increase.

\bibliographystyle{apalike}
\bibliography{references.bib}

\end{document}

% LocalWords:  Stieltjes Lyapounov billingsley Chebychev's Slutsky's HDA numden
% LocalWords:  dobriban invertible gia Wishart longref Minkowski's marcus
