%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{algorithm2e}
% for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2022}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{verbatim}

\usepackage{bbm}
\usepackage{color}
\usepackage{xspace}

\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\eqnum}{\leavevmode\hfill\refstepcounter{equation}\textup{(\theequation)}}
\def\R{\mathbb{R}}
\newcommand{\iid}[0]{i.i.d.\xspace}
\newcommand{\one}[1]{{\mathbbm{1}}_{{#1}}}
\newcommand{\One}[1]{{\mathbbm{1}}\left\{{#1}\right\}}
\newcommand{\inner}[2]{\langle{#1},{#2}\rangle} % Inner product
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\Norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\PP}[1]{\mathbb{P}\left\{{#1}\right\}} % Probability
\newcommand{\Pp}[2]{\mathbb{P}_{#1}\left\{{#2}\right\}} % Probability
\newcommand{\EE}[1]{\mathbb{E}\left[{#1}\right]} % Expectation
\newcommand{\Ep}[2]{\mathbb{E}_{#1}\left[{#2}\right]}
\newcommand{\EEst}[2]{\mathbb{E}\left[{#1}\ \middle| \ {#2}\right]} % Conditional expectation
\newcommand{\PPst}[2]{\mathbb{P}\left\{{#1}\ \middle| \ {#2}\right\}} % Conditional probability
\newcommand{\VV}[1]{\var\left({#1}\right)} % Variance
\newcommand{\Vp}[2]{\var_{#1}\left[{#2}\right]}
\newcommand{\VVst}[2]{\var\left({#1}\ \middle|\ {#2}\right)}
\renewcommand{\O}[1]{\mathcal{O}\left({#1}\right)}
\def\R{\mathbb{R}}
\def\Z{\mathbf{Z}}
\def\W{\mathbf{W}}
\newcommand{\ee}[1]{\mathbf{e}_{{#1}}}
\newcommand{\ident}{\mathbf{I}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\zeros}{\mathbf{0}}
\newcommand\eqd{\stackrel{\mathclap{\normalfont\mbox{d}}}{=}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\J}{\mathbf{J}}

\newcommand{\bma}{\mathbf{a}}
\newcommand{\bms}{\mathbf{s}}
\newcommand{\bmbeta}{\bm{\beta}}
\newcommand{\bmepsilon}{\bm{\epsilon}}
\newcommand{\bmTheta}{\bm{\Theta}}
\newcommand{\bmvarepsilon}{\bm{\varepsilon}}
\newcommand{\bbSig}{\Sigma\hspace{-3.5pt}{\color{white}1}\hspace{-7pt}\Sigma}

  
\newcommand{\normal}{\mathcal{N}}
\newcommand{\fronorm}[1]{\norm{{#1}}_{\text{Fro}}}
\newcommand{\grpnorm}[1]{\norm{{#1}}_{\text{group}}}
\newcommand{\Xp}{\widetilde{\X}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbXp}{\widetilde{\bbX}}
\newcommand{\fdr}{\textnormal{FDR}}
\newcommand{\mfdr}{\textnormal{mFDR}}
\newcommand{\gfdr}{\fdr_{\textnormal{group}}}
\newcommand{\mgfdr}{\mfdr_{\textnormal{group}}}
\newcommand{\fdp}{\textnormal{FDP}}
\newcommand{\fdph}{\widehat{\fdp}}
\newcommand{\Sh}{\widehat{S}}
\newcommand{\St}{\widetilde{S}}
\newcommand{\betah}{\widehat{\beta}}
\newcommand{\tlam}{\widetilde{\lambda}}
\newcommand{\hlam}{\widehat{\lambda}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\iidsim}{\stackrel{\mathrm{iid}}{\sim}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Generalized Simultaneous Knockoffs}
\title{Controlling FDR in selecting group-level simultaneous signals from multiple data sources with application to the National Covid Collaborative Cohort data}

 \author{Runqiu Wang\thanks{Department of Biostatistics, University of Nebraska Medical Center, Omaha, Nebraska, U.S.A., runqiu.wang@unmc.edu}, Ran Dai\thanks{Department of Biostatistics, University of Nebraska Medical Center, Omaha, Nebraska, U.S.A., ran.dai@unmc.edu},  Cheng Zheng\thanks{Department of Biostatistics, University of Nebraska Medical Center, Omaha, Nebraska, U.S.A., cheng.zheng@unmc.edu}, on behalf of the N3C consortium}

\begin{document}


\maketitle

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}



%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% otherwise use the standard text.

\begin{abstract}

%Large-scale electronic health record (EHR) data provide valuable opportunities to identify risk factors for disease outcomes from a long list of demographic and comorbidity features. 
One challenge in exploratory association studies using observational data is that the signals are potentially weak and the features have complex correlation structures. False discovery rate (FDR) controlling procedures can provide important statistical guarantees for replicability in risk factor identification in exploratory research. In the recently established National COVID Collaborative Cohort (N3C), electronic health record (EHR) data on the same set of candidate features are independently collected in multiple different sites, offering opportunities to identify signals by combining information from different sources. This paper presents a general knockoff-based variable selection algorithm to identify mutual signals from unions of group-level conditional independence tests with exact FDR control guarantees under finite sample settings. This algorithm can work with general regression settings, allowing heterogeneity of both the predictors and the outcomes across multiple data sources. We demonstrate the performance of this method with extensive numerical studies and an application to the N3C data.
\end{abstract}

\section{Introduction}
\label{sec:intro}

 With recent advances in scientific research, data on the same set of candidate predictors are often collected independently from multiple sources, and there is a challenge in making reliable discoveries from such data jointly. In this paper, we introduce a knockoff-based framework to identify mutual signals from multiple independent studies and provide variable selection accuracy guarantees under mild design and model assumptions.
 
 To formulate the mutual signal identification problem mathematically, for a number $N \in \mathbb{N}$, denote $[N] = \{1, \cdots, N\}$. Suppose we have data from $K$ independent datasets $(\Y^1,\X^1),$$\cdots,$$ (\Y^K,\X^K)$, where $\Y^{k}\in \R^{n_k}$ and $\X^k\in \R^{n_k\times p_k}$ for $k \in [K]$. Within the $k$-th dataset, there are $p_k$ variables, i.e. 
 \[(Y^k_{i},X^k_{i1},\cdots,X^k_{ip_k}) \iidsim \mathcal{D}_k, ~\text{for}~ i \in [n_k].\] 
 
 Across the $K$ experiments, both the outcome variables $\Y^k$s and the $\X_j^k$s for $k \in [K]$ and $j \in [p_k]$ can be of different data types, and $(\Y^k, \X_1^k,\cdots, \X_{p_k}^k)$ can have different distributions (heterogeneous). %\footnote{We denote $Y^k$s and $X^k$s as continuous variables throughout the paper for the simplicity of notation. In practice, they can be of other data types (continuous/count/nominal/ordinal/mixed).} 
 For example, $\Y^k$s can be continuous or binary disease outcomes and $\X^k$s can be a mixture of continuous and categorical medical records from the electronic health record (EHR) data. Furthermore, we do not assume $p_k$s to be identical across the $K$ datasets. For $k\in [K]$, there are $M$ mutually exclusive groups of variables, we denote their index set as $G_{k1}, \cdots, G_{kM}$, where $G_{km}\subseteq [p_k]$ for all $k \in [K]$ and $m \in [M]$. For any $m \in [M]$, we allow different group sizes ($|G_{km}|$s) across the K datasets. For example, in dataset $k$, $\X^k_{G_{km}}$ can be a group of dummy variables created for the categorical obesity level, and in dataset $l$, $\X^l_{G_{lm}}$ can be the continuously measured body mass index.  
 
 Define the null hypothesis for the following test of group $m$ in dataset $k$ as $H_{0m}^k:= Y^k \indep \X^k_{G_{km}}|\X^k_{-G_{km}}$ where $\X^k_{-G_{km}} := \X^k \setminus \X^k_{G_{km}}$, and the union null hypothesis $H_{0m} :=\cup_{k=1}^KH_{0m}^k$. Our goal is to control the FDR for the $M$ tests for the $H_{0m}$s. with the group-level hypotheses, we define
%between $Y^k$ and $X^k$, i.e,
%\begin{eqnarray*}
%Y^k_i=\beta_{0k}+\sum_{j=1}^{p_k}\beta_{jk}X^k_{ij}+\epsilon_i^k
%\end{eqnarray*}
%where $\epsilon_i^k\sim N(0,\sigma_k^2)$, then we have
%$H_{0m}^k=\{\beta_{jk}=0, \forall j\in G_{km}\}$. Notice here we do not restrict $G_{km}$ to be mutually exclusive so that we can have overlapping groups. 
\begin{multline}\label{eqn:H}
    \mathcal{S} = \{m\in [M]: H_{0m} ~\text{is false}\}~, \\ \text{and  $\mathcal{H} = \mathcal{S}^c = \{m\in [M]: H_{0m} ~\text{is true}\}$}.
\end{multline} 
We aim at developing a selection procedure returning a selection set of groups $\widehat{\mathcal{S}} \subseteq [M]$ with a controlled group level FDR:  \begin{equation}\label{eqn:fdr}
\gfdr(\widehat{\mathcal{S}}) = \EE{\frac{|\widehat{\mathcal{S}} \cap \mathcal{H}|}{|\widehat{\mathcal{S}}|\vee 1}}.
\end{equation}

To motivate our work, we give a data example of the long-term coronavirus disease 2019 (COVID-19).

\subsection{The National COVID Collaborative Cohort example} 

The N3C offers one of the largest collections of secure and de-identified clinical data in the United States for COVID-19 research \citep{Haendel-N3C}. Up to January 10, 2023, N3C has EHR information on over 17 million patients from 77 data-contributing sites, with over 6 million confirmed COVID patients. With the accumulated COVID cohort data over time, long-term effects from SARS-CoV-2 infection have been identified and brought to attention. Some COVID-19 survivors present with persistent neurological, respiratory, or cardiovascular symptoms after the acute phase of the infection, regardless of the initial disease severity, vaccination status, and demographic and comorbidity status \citep{montani2022}. The identification of risk factors for long COVID becomes an important question. Information from multiple data sites brings us both opportunities and challenges. On one hand, selecting mutual predictors from multiple data sources helps us to identify reproducible risk factors for long COVID. On the other hand, the data from different contributing sites are heterogeneous with different data types in $\X$ from different data contributing sites, and there are also different long COVID outcomes across the data sites \citep{Pfaff2022}; novel FDR controlling methods are needed for tackling these data challenges. Our proposed method will be used to identify mutual risk factor signals for long COVID from patients' demographic and comorbidity information with an FDR control guarantee.     

\subsection{Related prior work}
For risk factor identification using a single data source, knockoff-based methods have been developed for exact FDR control in selecting features with conditional associations with the response \citep{barber2015, candes2018}. The original knockoff filter \citep{barber2015,barber2019} works on linear models assuming no knowledge of the design of covariates, the signal amplitude, or the noise level. It achieves exact FDR control under finite sample settings. For more general nonlinear models, \citet{candes2018} proposed the Model-X knockoff method, which allows the conditional distribution of the response to be arbitrary and completely unknown but requires some knowledge of the distribution of $\X$ \citep{huang2020}. Model-X knockoff method is also robust against errors in the estimation of the distribution of $\X$ \citep{barber2020}. There are also abundant publications on the construction of knockoffs with an approximated distribution of $\X$. \citet{romano2019} developed a Deep knockoff machine using deep generative models. \citet{liu2019} developed a Model-X generating method using deep latent variable models. \citet{bates2020} proposed an efficient general metropolized knockoff sampler. \citet{spector2020} proposed to construct knockoffs by minimizing the reconstructability of the features. Model-specific Knockoff methods have been proposed.  \citet{dai2022kernel} proposed a kernel knockoff selection procedure for the nonparametric additive model. \citet{kormaksson2021} proposed the sequential knockoffs for continuous and categorical $\X$ variables. Knockoff-based methods have also been extended to test the null hypotheses at the group level. In this direction, group and multitask knockoff methods \citep{dai2016}, and prototype group knockoff methods \citep{chen2020} have been proposed. These group knockoff methods can also be used when there are categorical variables in $\X$ (see details in Section \ref{sec:groupknockoff}). Variants of knockoff methods have become useful tools in scientific research. For example, to identify the variations across the whole genome associated with a disease, \citet{sesia2019} developed a hidden Markov model knockoff method for FDR control in the genome-wide association study (GWAS). \citet{srinivasan2022} proposed a compositional knockoff filter for the analysis of microbiome compositional data.

%{\color{red}Add citation for Kernel Knockoffs Selection for Nonparametric Additive Models for method and add a citation for compositional knockoff for microbiome study for application.}

For simultaneous signal detection from multiple research, methods based on the BH procedure \citep{heller2014, bogomolov2013, bogomolov2018}, the local FDR as a summary of the multivariate test statistics \citep{chi2008, heller2014b} and some nonparametric method \citep{zhao2020} have been proposed. However, all methods above assume not only the independence of the experiments but also the independence (or PRDS) of the p-values for the features within each experiment, which is not realistic for the patient demographic and comorbidity data in N3C. More recently, \citet{dai2021multiple} proposed a simultaneous knockoff method for testing the union null hypotheses for feature selection at the individual level in $\X$, with exact FDR control at finite sample settings. The simultaneous knockoff method can not be directly used when a group of $\X$ variables (for example multiple cardiovascular disease variables) needs to be selected together or categorical $\X$ variables with more than 2 categories are present.


%Simultaneous knockoffs is a method to control FDR for the tests of union null hypothesis of conditional independence\cite{dai2021multiple}. This method can be applied to many important scientific areas such as the repeatability analysis in GWAS, mutual features across population with heterogeneity, high-dimensional mediator selection, etc. In addition, under the condition that K experiments are independent, simultaneous knockoffs can allow for very general conditional models and covariate structures.

%The simultaneous knockoff procedure contains four steps. The first step is to construct knockoff for the individual experiments. Suppose there are K experiments, a set of knockoff features $\widetilde{X} = [\widetilde{X}^1, \dots, \widetilde{X}^K]$ are constructed by either the Fixed-X knockoff\cite{barber2015controlling} or the Model-X knockoff\cite{candes2018panning}. The second step is to calculate test statistics for the individual experiments. For each experiment $k \in [K]$, we choose and calculate statistics $[Z^k,\widetilde{Z}^k] \in R^{2p}$ that are compatible with the knockoff construction method for experiment k. The third step is to calculate the filter statistics $W=f([Z^1,\widetilde{Z}^1],\dots,[Z^K,\widetilde{Z}^K])$, where $f$ is a one swap flip sign function (OSFF). Finally, we use the filter statistics W to calculate the threshold and do the feature selection. \citet{dai2021multiple} have proved that with the individual experiments satisfying the Fixed-X or the Mode-X knockoff model settings, the simultaneous knockoff procedure controls the modified
%FDR and the simultaneous knockoff+ (more conservative) procedure.
%\subsection{Group knockoff review}
%Group knockoff filter is a method for controlling false discovery rate in linear models with grouped features \cite{dai2016knockoff}. This method is a extension of individual knockoff method \cite{barber2015controlling} by modifying the construction of the knockoffs at group-wise level and the filter step and can also improve the power when the features are highly correlated by considering the set of true and false discoveries at the group level.

%Considering a group setting of linear regression model, $Y=X\beta+\epsilon$, where $y \in R^{n}$ is a vector of responses, $X \in R^{n \times p}$ and $\epsilon\sim N(0, \sigma^2I_{n})$, the p features are divided into m groups, $G_{1}, \dots, G_{m} \in \{1,\dots,p\}$ with group sizes $p_{1}, \dots, p_{m}$. In the sparsity setting, only a small portion of $\beta_{G_{i}}$'s are nonzero, where $\beta_{G_{i}} \in R^{p_{i}}$ is a subvector of $\beta$ in the ith group of features. To run group knockoff, two main steps are included: constructing group knockoffs and group filtering the results. First, a set of p knockoff features is constructed by the following condition on the matrix $\widetilde{X} \in R^{n \times p}$:\\
%$\widetilde{X}^{T}\widetilde{X}=\Sigma=X^{T}X$ and $\widetilde{X}^{T}X=\Sigma - S$, \\
%where S is group-block-diagonal matrix, that is S = diag$\{S_{1},\dots,S_{m}\}$ where $S_{i} \geq 0$ is the $p_{i} \times p_{i}$ matrix for the ith group block. A strategy for constructing $\widetilde{X}$ is to choose S=diag$\{S_{1}, \dots, S_{m}\}$ satisfying the condition $S \leq 2\Sigma$ and construct $\widetilde{X}$ as
%$\widetilde{X}=X(I_{p}-\Sigma^{-1}S)+\widetilde{U}C$,\\
%where $\widetilde{U}$ is a $n \times p$ orthonormal matrix and orthogonal to the span of X, while $C^{T}C=2S-S\Sigma^{-1}S$ is a Cholesky decomposition.

%The second step is the filter step, which proceeds exactly the same as the individual knockoff method except we use groups of features instead of individual features. Considering the group lasso model:\\ $\widehat{\beta}=arg min\{||y-\left[X \widetilde{X}\right]b||^{2}_{2}+\lambda||b||_{group}\}$,\\
%$\lambda_{i}$ and $\widetilde{\lambda_{i}}$ be the time of entry into the lasso path for each feature and knockoff,\\
%$\lambda_{i}=sup\{:\widehat{\beta(\lambda)}_{G_{i}}\neq0\}$,
%$\widetilde{\lambda_{i}}=sup\{:\widehat{\beta(\lambda)}_{\widetilde{G_{i}}}\neq0\}$,\\
%and $\widehat{S(\lambda)}=\{j: \lambda_{j} > \widetilde{\lambda}_{j}\ \vee \lambda\}$ and
%$\widetilde{S(\lambda)}=\{j: \widetilde{\lambda}_{j} > \lambda_{j}\ \vee \lambda\}$ be the sets of original features and knockoff features, respectively. The false discovery proportion is estimated by 
%FDP$(\lambda)=\frac{\widetilde{S}(\lambda)}{\widehat{S}(\lambda) \vee 1}$. Finally, the knockoff filter selects $\widehat{\lambda}= \min \{ \lambda: \widehat{FDP}(\lambda) \leq q$, where q is the desire bound on FDR level.


\subsection{Our contribution}
In this paper, we propose a generalized simultaneous knockoff (\textit{GS knockoff}) framework, to establish exact FDR control in selecting mutual signals at the group level from multiple conditional independence tests, assuming very general conditional models. This extension is especially useful when using a general machine-learning model to select important groups of variables or categorical variables. The main contributions of this paper are summarized below:

\begin{itemize}
    \item We present a general knockoff-based algorithm for selecting simultaneous group-level features from multiple data sources.
    \item The proposed algorithm controls the exact group level FDR under mild conditions for $\X$ and $Y|\X$.
    \item We provide a collection of easy-to-implement group knockoff construction methods that are compatible with our framework, as well as simple and powerful filter statistics.
    \item We demonstrate the FDR control property and the power of our method with extensive simulation settings. We also illustrate the application with the N3C data.
    \item Our method only requires summary statistics from the individual datasets, leading to the advantages of privacy-preserving, efficient distributed learning algorithms with the potential to work on online stream data. 
\end{itemize}
%{\color{red}[Highlight that our method only requires summary statistics from each site and thus can preserve privacy and easy to accommodate distributed learning and maybe online stream data.]}
\begin{comment}
\subsection{Group knockoff construction}
The original knockoff construction requires that $\Xp^\top X = X^\top X - diag\{s\}$, that is, all off-diagonal entries are equal. When the features are highly correlated, 
this construction is only possible for vectors $s$ with extremely small entries; that is, $\Xp_j$ and $X_j$ are themselves highly correlated, and the knockoff filter
then loses power as it is hard to distinguish between a real signal $X_j$ and its knockoff copy $\Xp_j$. 

In a group-sparse setting, we will see that we can relax this requirement on $\Xp^\top X$, thereby improving our power. In particular, the best gain will be in 
situations where within-group correlations are high but between-group correlations are low; this may arise in many applications, for example, when genes related
to the same biological pathways are grouped together, we expect to see the largest correlations occuring within groups rather than between genes in different groups.
\end{comment}
%In many situations, highly correlated variables are more likely to be in the same clusters with scientific meanings, for example, genes related to the same biological pathways can be considered as a group, and we expect higher correlations within groups than between groups. If we are more interested in the effects of the whole groups of genes, rather than individual genes, we can select groups of genes based on the pathway information. To control FDR in such scenarios, we extend the knockoff idea to group sparsity problems and propose to construct ``group knockoff'' features that retain the correlation structures of the original variables at a group level.
\section{Group knockoff construction methods}\label{sec:groupknockoff}
In this section, we present a collection of methods for generating group knockoffs for an individual dataset. For notation simplicity, we omit the superscript $k$ in this section. We begin with some definitions.

\begin{definition}
(Swapping) For a set $S \subseteq [M]$, and for a vector $\mathbf{V} = (V_1, \cdots, V_{2M}) \in \R^{2M}$, $\mathbf{V}_{\textnormal{Swap}(S)}$ indicates the swapping of $V_j$ with $V_{j+M}$ for all $j \in S$.
\end{definition}
\begin{definition}
(Group Swapping) For a set $S \subseteq [M]$ and a group partition $G=\{G_1,\dots,G_M\}$ with $G_m\subseteq[p]$, and for a vector $\mathbf{V} = (V_1, \cdots, V_{2p}) \in \R^{2p}$, $\mathbf{V}_{\textnormal{GSwap}(S,G)}=\mathbf{V}_{\textnormal{Swap}(\cup_{m\in S}G_m)}$.
\end{definition}

\subsection{Group knockoff construction for fixed design}
\begin{definition} 
A Fixed-X group knockoff for a fixed design matrix $\X=(\X_1,\cdots,\X_p)$ with group partition $G=\{G_1,\cdots,G_M\}$ is a new design matrix $\widetilde{\X}=(\widetilde{\X}_1,\cdots,\widetilde{\X}_p)$ constructed with the following two properties:
\begin{enumerate}
\item $\widetilde{\X}^\top \widetilde{\X} = \Sigma\coloneqq \X^\top \X$
\item $\widetilde{\X}^\top \X = \Sigma - \B$, where $\B\succeq 0$ is group-block-diagonal meaning that $\B_{G_i,G_j} = 0$ for any two distinct groups $i\neq j$. 
\end{enumerate}
\end{definition}
The group knockoff construction methods for fixed design have been proposed by \cite{dai2016}. Specifically, write $\B = diag\{\B_1,\dots,\B_M\}$ where $\B_m=\B_{G_m,G_m}$ and $\0\preceq \B \preceq 2\Sigma$. Here for $\J, \K \in \R^{m\times m}$, $\J \preceq \K$ if and only if $\K -\J$ is positive semidefinite. We can construct the fixed group knockoffs by setting \[\widetilde{\X} = \X(\ident_p -\Sigma^{-1}\B)+\widetilde{\U}\C\]
where $\widetilde{\U}$ is a $n\times p$ matrix orthogonal to the span of $\X$, while $\C^\top \C = 2\B - \B \Sigma^{-1}\B$ is a Cholesky decomposition. The condition $\0\preceq\B\preceq 2\Sigma$ guarantees the existence of such a Cholesky decomposition. We can select $\B$ using either the equivariant approach or the semidefinite programming (SDP) approach. For equivariant approach, we have $\B_m = b \cdot \Sigma_{G_m,G_m}$ where \[b = \min\left\{1, 2\lambda_{\min}\left(\D\Sigma \D\right)\right\}\] where $\lambda_{min}(\cdot)$ means the minimum eigen value and $\D = diag\{\Sigma_{G_1,G_1}^{-\frac{1}{2}},\dots,\Sigma_{G_M,G_M}^{-\frac{1}{2}}\}$. For SDP approach, we have $\B_m = b_m \cdot \Sigma_{G_m,G_m}$ and we can find $(b_1,\dots,b_M)$ that minimize $\sum_{m=1}^M (1-b_m)$ with the constraint $\B\preceq 2\Sigma$. In the non-group setting, it has been shown that the SDP approach can lead to a slight power increase.

We can also use an individual-level fixed knockoff matrix which automatically satisfies the fixed group knockoff matrix requirement. However, the group-level condition is weaker and it allows more flexibility in constructing $\widetilde{\X}$. Such flexibility will enable more separation between a feature $X_j$ and its knockoff $\widetilde{\X}_j$, which in turn can increase the power to detect true signals.

\subsection{Group knockoff construction for Model-X approach}
\begin{definition} \label{def:group_modelX}
A group Model-X knockoffs for the family of random variables $\X=(\X_1,\cdots,\X_p)$ with group partition $G=\{G_1,\cdots,G_M\}$ are a new family of random variables $\widetilde{\X}=(\widetilde{\X}_1,\cdots,\widetilde{\X}_p)$ constructed with the following two properties:
\begin{enumerate}
\item for any subset $S\subseteq[M]$, $(\X,\widetilde{\X})_{\text{GSwap}(S)}\eqd (\X,\widetilde{\X})$
\item $\widetilde{\X}\indep Y| \X$ if there is a response $Y$
\end{enumerate}
\end{definition}

\citet{candes2018panning} proposed a general algorithm to sample the model-X knockoff when each column is a single variable. We can extend it to allow each variable to be a multivariate random vector and thus the general algorithm to sample group knockoff can be given as below:\\
\begin{algorithm}[H]

$m=1$ \; 
\While{$m\leq M$}{
Sample $\widetilde{\X}_{G_m}$ from distribution $\mathcal{L}(\X_{G_m}|\X_{-G_m},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})$\;

$m=m+1$\;
}
\caption{Model-X Group Knockoff construction}
\label{alg:groupModelX}
\end{algorithm}

The proof that this algorithm leads to knockoffs that satisfy the group Model-X knockoff properties (Definition \ref{def:group_modelX}) is given in Lemma \ref{lem:gsknock} in the Appendix.

\subsubsection{Second-order group Model-X Knockoff}
When $\X$ follows the multivariate normal distribution $\mathcal{N}(\mu,\Sigma)$, we just need to sample $\widetilde{\X}$ such that:
\begin{eqnarray*}
Cov([\X, \widetilde{\X}])=\left(\begin{array}{cc}\Sigma &\Sigma-\B\\\Sigma-\B&\Sigma\end{array}\right),
\end{eqnarray*}
where $\B\succeq 0$ is group-block-diagonal. Or equivalently under the multivariate normal assumption, 
\begin{eqnarray*}
\widetilde{\X}|\X\sim \mathcal{N}(\widetilde{\mu},\widetilde{\Sigma}),
\end{eqnarray*}
where $\widetilde{\mu}=\X-\X\Sigma^{-1}\B$ and $\widetilde{\Sigma}=2\B-\B\Sigma^{-1}\B$. The optimal $\B$ can be obtained using the equivariant group knockoff method as the fixed design or using the SDP method for improved power. When $\Sigma$ is unknown or the continuous $\X$ does not follow a normal distribution, a construction by using $\widehat{\Sigma}$ and Gaussian working model is denoted as the second-order group-knockoff construction method.

\subsubsection{Sequential group knockoff construction}
When there are categorical components in $\X$, using second-order knockoff construction faces the problem of model mis-specification. For individual features, sequential knockoff construction has been proposed \citep{kormaksson2021}. Here we extend it to the group setting. Without loss of generality, we can assume that for each $\X_{G_m}$, it contains two parts, the continuous part $\X_{G_m}^{con}$ and the categorical part $\X_{G_m}^{cat}$. We summarize the algorithm below:


  
        
            
\begin{algorithm}
                
$m=1$, 
\textbf{While} $m\leq M$, \textbf{do}
\begin{itemize}
    \item $\widetilde{\X}^{con}_{G_m}$ construction: sample $\widetilde{\X}^{con}_{G_m} \sim \mathcal{N}(\widehat{\mu}_m,\widehat{\Sigma}_m)$ where $\widehat{\mu}_m$,$\widehat{\Sigma}_m$ are obtained by fitting a penalized multi-task linear regression of $\X^{con}_{G_m}$ on $[\X_{-G_m},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j}]$.
    \item $\widetilde{\X}^{cat}_{G_m}$ construction: sample $\widetilde{\X}^{cat}_{G_m} \sim \text{Multinom}(\widehat{\pi})$, where $\widehat{\pi}$ are obtained by fitting a penalized multinomial logistic regression of $\X^{cat}_{G_m}$ on $[\X^{con}_{G_m},\X_{-G_m},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j}]$ with predictions made on $[\widetilde{\X}^{con}_{G_m},\X_{-G_m},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j}]$.
    \item $m=m+1$
\end{itemize}
\textbf{end}
        
\caption{Sequential Group Knockoff construction
        }
    \label{alg:seqknockoff}
\end{algorithm}
In Lemma \ref{lem:gseqknock} in the Appendix, we show that when the model is correct, this satisfies the general Group Model-X Knockoff generation procedure (Algorithm \ref{alg:groupModelX}).


\section{Method}
\begin{comment}
\subsection{Settings}
Consider $K$ independent datasets $(\Y^1,\X^1),$$\cdots,$$ (\Y^K,\X^K)$, where $\Y^{k}\in \R^{n_k}$ and $\X^k\in \R^{n_k\times p_k}$ for $k \in [K]$ where $(Y^k_{i},X^k_{i1},\cdots,X^k_{ip_k})\iidsim \mathcal{D}^k$ for $k \in [K]$ and $i \in [n_k]$. Without loss of generality, assume $X^k_{ij}$ is either continuous or binary (one dummy variable for a categorical variable) for $k \in [K]$ and $j \in [p_k]$. Within dataset $k$, there are $M$ mutually exclusive groups, denoted as $G_{k1}, \cdots, G_{kM}$ where $G_{km}\subseteq [p_k]$ for all $k \in [K]$ and $m \in [M]$. Define the null hypothesis for the following test of group $m$ in dataset $k$ as $H_{0m}^k:= Y^k \indep X^k_{G_{km}}|X^k_{-G_{km}}$, and the union null hypothesis $H_{0m} :=\cup_{k=1}^KH_{0m}^k$. Our goal is to control the FDR for the $M$ tests for $H_{0m}$s.
\end{comment}

Our proposed \textit{GS knockoff} framework can work with general regression models as long as the settings for the individual datasets satisfy the Fixed-X or Model-X knockoff assumptions \citep{barber2015, candes2018}. Therefore it can work with a large spectrum of models, from linear regression models with very weak assumptions on $\X$, to machine learning models with some knowledge of the $\X$ distribution. For the group settings, we only assume in for all the $K$ dataset there are $M$ groups, but we do not require the group sizes to be the same across the datasets. Also, we do not require $\cup_{m=1}^M G_{km}=[M]$ so that we can adjust for confounding variables in the models. 

\subsection{Preliminaries}
\begin{definition}
A test statistics $[\Z, \widetilde{\Z}]$ is called group knockoff compatible with the group partition $G_1,\dots,G_M\subseteq[p]$ if it can be written as $[\Z, \widetilde{\Z}]=t([\X,\widetilde{\X}],Y])$ for some function $t(\cdot)$ such that for any $S\subseteq[M]$, $[\Z, \widetilde{\Z}]_{\text{Swap}(S)}=t([\X,\widetilde{\X}]_{\text{GSwap}(S,G)},Y])$
\end{definition}
\begin{definition}
A test statistics $[\Z, \widetilde{\Z}]$ satisfies sufficiency requirement if it can be written as a function of $[\X,\widetilde{\X}]^\top [\X,\widetilde{\X}]$ and $[\X,\widetilde{\X}]^\top Y$.
\end{definition}
\begin{definition}\label{def:one-swap-flip}
(One swap flip sign function (OSFF)) A function $f: \R^{2mK}\rightarrow \R^{m}$ is called a one swap flip sign function (OSFF) if it satisfies that for all $k\in [K]$ and all $S \subseteq[m]$, \begin{multline*}
    f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k]_{\textnormal{Swap}(S)},\cdots,[\Z^K,\widetilde{\Z}^K])\\=f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k],\cdots,[\Z^K,\widetilde{\Z}^K])\odot \epsilon(S),
\end{multline*}
where $\Z^k,\widetilde{\Z}^k \in \R^p$ for $k \in [K]$.
\end{definition}

\subsection{Algorithm}\label{sec:algorithm}

The \textit{GS knockoff} procedure is described below:
\begin{itemize}
    
    \item \textit{Step 1: Group knockoff construction for the individual experiments.} Denote the knockoff matrices for $\X^1,\cdots,\X^K$ as $\widetilde{\X}^1, \cdots,\widetilde{\X}^K$. The $\widetilde{\X}^k$ matrices can be generated using the group knockoff construction methods as described in Section \ref{sec:groupknockoff}. When only individual features exist, methods for generating individual knockoffs \citep{barber2015, candes2018, romano2019, bates2020, spector2020} can also be used since satisfying individual knockoff requirements implies satisfying group knockoff requirements. However, using individual knockoff might cause the knockoff to be very similar to the original feature and thus has less power when the within-group variables are highly correlated. 
    
    \item \textit{Step 2: Test statistics calculation for the individual experiments.} For each experiment $k \in [K]$, choose and calculate statistics $[\Z^k, \widetilde{\Z}^k] \in \R^{2M}$ that are group knockoff compatible with the group partition $G$ (and satisfy the sufficiency requirement when fixed group knockoff construction is used). 
    For our analysis, we assume the true model is
    \begin{equation}
    g_k(\EE{Y_{i}^k})=\beta_0^k+\beta^{k\top}\X^k_{i},
    \end{equation}
    and fit the working model
    \begin{multline} g_k(\EE{Y_{i}^k})=\beta_0^k+\beta^{k\top}[X_i^k \widetilde{X}_i^k]
%\alpha^k+\beta^{k\top}\X^k_{i}+\tilde{\beta}^{k\top}\widetilde{\X}^k\\=\alpha^k+(\beta^{k\top}, \tilde{\beta}^{k\top})[X^k \widetilde{X}^k].
    \end{multline}    
    by defining 
    \begin{eqnarray*}
    \widehat{\beta}_k(\lambda)&=&\arg\min_{\beta} \sum_{i=1}^{n_k}\frac{(Y_i^k-\beta_0^k-\beta^{k\top}[X_i^k \widetilde{X}_i^k])^2}{V^k_i}\\&&+\lambda \sum_{m=1}^M \sqrt{\sum_{j\in G_{km}}(\beta_{j}^k)^2},
    \end{eqnarray*}
    where $V^k_i=V^k(g_k^{-1}(\beta_0^k+\beta^{k\top}[X_i^k \widetilde{X}_i^k]))$ and $V^k(\cdot)$ is the variance function specified for the generalized linear model (GLM) for $Y^k$. Then we define 
    \begin{equation}
    Z^k_m=\sup\{\lambda: \sum_{j\in G_{km}}\beta^k_j(\lambda)^2>0\}
    \end{equation}
   \begin{equation}
   \widetilde{Z}^k_m=\sup\{\lambda: \sum_{j\in G_{km}}\beta^k_{j+p_k}(\lambda)^2>0\}.
   \end{equation}
Denote $\Z^k = (Z_1^k,\cdots,Z_M^k)$ and $\widetilde{\Z}^k = (\widetilde{Z}_1^k,\cdots,\widetilde{Z}_M^k)$.
    
 %{\color{red}[Do we use any scaling when fitting use grpreg?]}
    \item \textit{Step 3: Calculation of the filter statistics $\W$.} Choose an arbitrary OSFF $f$ as defined in Definition \ref{def:one-swap-flip} and calculate $\W=f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K])$. In our analysis, we use 
    \begin{equation}
    \W=\odot_{k=1}^K [\Z^k-\widetilde{\Z}^k],
    \end{equation}
    where $\odot$ represents the Hadamard product (elementwise product).
  
    \item \textit{Step 4: Threshold calculation and feature selection.} Using the filter statistics $\W$ from Step 3, we apply the knockoff+ filter \eqref{eqn:knockoff+} to obtain the selection set $\widehat{S}_+$ under the \textit{Generalized Simultaneous knockoff}+ procedure; or apply the knockoff filter \eqref{eqn:knockoff} to obtain $\widehat{S}$ under the \textit{Generalized Simultaneous knockoff} procedure.
    \begin{multline}\label{eqn:knockoff}
   \widehat{S} = \{j: W_j \geq \tau\},~\text{where}~\\ \tau = \min\left\{t \in \mathcal{W}_+: \frac{\#\{j:W_j \leq -t\}}{\#\{j:W_j\geq t\}\vee 1}\leq q\right\} .
\end{multline}
\begin{multline}\label{eqn:knockoff+}
   \widehat{S}_+ = \{j: W_j \geq \tau_+\}, ~\text{where}~ \\ \tau_+ = \min\left\{t \in \mathcal{W}_+: \frac{1+\#\{j:W_j \leq -t\}}{\#\{j:W_j\geq t\}\vee 1}\leq q\right\} .
\end{multline}
\end{itemize}

 
\section{Main results}
\begin{theorem}\label{thm:1}
With the test statistics $[\Z^k,\widetilde{\Z}^k]$ for $k\in [K]$ satisfy the property that $[\Z^k,\widetilde{\Z}^k]\eqd [\Z^k,\widetilde{\Z}^k]_{Swap(S)}$ for any $S\in \mathcal{H}$ and $W=f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K])$ for an OSFF function $f$, the \textit{GS knockoff} procedure \eqref{eqn:knockoff} 
controls the modified group FDR defined as
\begin{equation}\label{eqn:mfdr}
\textnormal{m}\gfdr=\EE{\frac{|\widehat{S}\cap \mathcal{H}|}{|\widehat{S}|+1/q}}\leq q, \end{equation}
and the \textit{GS knockoff}+ procedure \eqref{eqn:knockoff+} controls the group FDR as defined in \eqref{eqn:fdr}.
\end{theorem}


\begin{corollary} Under the specific choice of $[\Z^k,\widetilde{\Z}^k]$ in equations (5) and (6) as in and the choice of $W$ as in equation (7), we have that the \textit{GS knockoff} procedure controls the modified group FDR and the \textit{GS knockoff}+ procedure controls the group FDR as defined in \eqref{eqn:mfdr}.
\end{corollary}

\section{Simulation}

\subsection{Simulation settings}
To evaluate the performance of our proposed method in the group sparse setting, we simulate the K=2 independent datasets with $n_k = 1000$ in both datasets. We consider the following three data settings:
\begin{itemize}
\item Continuous: $Y^k$s and $\X^k$s are both continuous and $Y^k|\X^k$s follow linear regression models in both datasets.
\item Binary: $Y^k$s are binary and $\X^k$s are mixtures of continuous and categorical variables; $Y^k|\X^k$s follow logistic regression models for both datasets.
\item Mixed: $\X^k$s are mixtures of continuous and categorical variables, $Y^1$ is continuous and $Y^1|\X^1$ follows a linear regression model; $Y^2$ is binary and $Y^2|\X^2$ follows a probit regression model.
\end{itemize}


We generate the \textit{GS knockoff} procedure based on the algorithm described in Section \ref{sec:algorithm}. In Step 1, for the Continuous setting, we use the equivariant approach. For the Binary and Mixed settings, we use the sequential group knockoff (Algorithm \ref{alg:seqknockoff}). In Step 4, We choose the knockoff+ filter for both the \textit{GS knockoff} and \textit{individual} methods.

We compare the proposed method with the two alternative strategies for combining information from multiple datasets: 
\begin{itemize}
    \item \textit{pooling:} The multiple datasets are  first pooled together and the tests of the conditional associations are performed using the group knockoff methods for a single dataset.
    \item \textit{intersection:} First, the group knockoff methods for single datasets are used to select signals from individual datasets. Then the intersection set of the selected signals from the multiple datasets is constructed as the simultaneous signal set.
\end{itemize}
 In addition, for the Continuous setting, we also compare our \textit{GS knockoff} method with the $\textit{Simultaneous Knockoff}$ \cite{dai2021multiple} where the individual knockoff method \citep{barber2015} is used in Steps 1 and 2 in the algorithm (\textit{individual}). 

We conduct simulations to demonstrate the performance of our proposed $\textit{GS knockoff}$ method under various data settings. We first test the effect of varied signal sparsity levels of the mutual signals $s_0$ and the non-mutual signals ($s_k, ~\text{for}~ k \in [K]$) in the following cases: 1. $s_1=s_2=0$; 2. $s_1=s_2 \neq 0$; 3. $s_1=0, s_2 \neq 0$. Next, we study the effect of the within-group feature correlations $\rho_k$, and the ratio between the between-group correlations and within-group correlations $\gamma_k$. We consider two scenarios for the signal strengths: (1) both directions and strengths of the mutual signals are the same among the $K$ datasets, and (2) only the directions of the mutual signals are the same but the signal strengths are different among the $K$ datasets. More details on the simulation settings can be found in Appendix \ref{app:sim}.

\subsection{Results} %{\color{red}[In general, it is hard to compare power when all power are too close to 1, might want to reduce effect magnitude a bit in future.]}
Figure \ref{fig:figure1} shows the power and the group FDR of the four methods (\textit{GS knockoff}, \textit{pooling}, \textit{intersection}, and \textit{individual}) for the continuous setting when we vary $s_1=s_2$, within-group correlation $\rho_1=\rho_2$ (non-mutual signals exist in both groups: $s_1=s_2=16$) and correlation ratio $\gamma_1=\gamma_2$ (non-mutual signals exist in both groups: $s_1=s_2=16$). Only the \textit{GS knockoff} method controls the group FDR all the time while the other three methods (\textit{pooling}, \textit{intersection}, and \textit{individual}) fail in the FDR control. Besides, the \textit{GS knockoff} method almost has the same power as the other three methods when the dataset is sparse ($s_1=s_2\leq 12$, i.e., the proportion of true signal is $<35\%$). For the non-sparse experiments with more true signals, the \textit{GS knockoffs} method has slightly lower power than the other three methods (row 1). As within-group and between-group correlation increases, our \textit{GS knockoffs} method can still control FDR under 0.2. Although there is some power loss for our method when the correlation is high, the gap is moderate (row 2). The \textit{individual} method fails to control FDR across all the continuous settings.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.22]{images/Continuous_s1_s2_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.22]{images/Continuous_s1_s2_scale=0.6_samesig=0.pdf}\\
    \includegraphics[scale=0.22]{images/Continuous_rho_s1=16s2=16_scale=0.6_samesig=0.pdf}
    \includegraphics[scale=0.22]{images/Continuous_gamma_s1=16s2=16_scale=0.6_samesig=0.pdf}
    \caption{The power and the FDR of generalized simultaneous, pooling, intersection and individual methods for continuous data setting when varying $s_1=s_2$ in scenario 1 (row1, left) and scenario 2 (row1, right), within-group correlations $\rho_1=\rho_2$ (row 2, left) and correlation ratio $\gamma_1=\gamma_2$ (row2, right) for $s_1=s_2=16$ case in Scenario 2. }
    \label{fig:figure1}
\end{figure}

Figure \ref{fig:figure2} compares the power and the FDR on binary and mixed data settings for three cases (1. $s_1=s_2=0$; 2. $s_1=s_2 \neq 0$; 3. $s_1=0, s_2 \neq 0$) among three methods: \textit{GS knockoffs}, \textit{pooling}, and \textit{intersection}. The results are consistent with continuous data settings. Our \textit{GS knockoff} method controls FDR all the time in those settings. The \textit{intersection} method controls FDR but has less power than our method there are only simultaneous signals ($s_1=s_2=0$). The \textit{pooling} method always has the highest power among the three methods but it cannot control FDR for two samples with a large number of true signals.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.22]{images/Binary_s0_scale=3.5_samesig=1.pdf}
    \includegraphics[scale=0.22]{images/Mixed_s0_scale=2_samesig=1.pdf}\\
    \includegraphics[scale=0.22]{images/Binary_s1_s2_scale=3.5_samesig=1.pdf}
    \includegraphics[scale=0.22]{images/Mixed_s1_s2_scale=2_samesig=1.pdf}\\
    \includegraphics[scale=0.22]{images/Binary_s2_scale=3.5_samesig=1.pdf}
    \includegraphics[scale=0.22]{images/Mixed_s2_scale=2_samesig=1.pdf}
    \caption{The power and the FDR of generalized simultaneous, pooling and intersection methods when varying $s_0$ (row 1), $s_1=s_2$ (row 2), and $s_2$ (row 3) for Scenario 1 (same strengths) under binary (left) and mixed (right) data settings.}
    \label{fig:figure2}
\end{figure}

The simulation results are consistent with our theoretical expectations. In terms of FDR, the proposed \textit{GS knockoff} method controls FDR across all designed settings while the other methods fail. The \textit{pooling} method can control FDR only when there are only simultaneous signals exist ($s_1=s_2=0$). The \textit{intersection} method controls FDR when only simultaneous signals exist($s_1=s_2=0$) and non-mutual signals exist in one group ($s_1=0, s_2\neq0$). In terms of power, the \textit{GS knockoff} method has good power, which is similar to the \textit{pooing} method and is slightly higher than the \textit{intersection} method when only simultaneous signals exist($s_1=s_2=0$). When non-mutual signals exist in one group ($s_1=0, s_2\neq0$), \textit{GS knockoff}, \textit{pooling}, and \textit{intersection} methods have similar power. And the performance between scenario 1 (same strengths) and scenario 2 (different signal strengths) of our method are very similar, with all the FDR being controlled. 

More results for both Scenario 1 and 2 in three data settings: continuous, binary, and mixed are shown in Appendix \ref{app:sim-res}.

\section{The N3C data analysis}
%{\color{red}[Does N3C provide meaning of the site number? Might want to mention how many sites are available and why we choose these two sites. Also, the sample size is not for the cohort, it looks like the matched case-control sample, right?]}
In this section, we demonstrate the application of our proposed \textit{GS knockoff} method to the N3C data for the selection of risk factors of long COVID from a collection of baseline patient demographic, comorbidity and medication information. Our data is from the N3C Knowledge Store Shared Project. The N3C enclave consists of EHR data for over 6 million patients with confirmed COVID infection. It also contains high-dimensional patient demographic, comorbidity, medication, social economical information. As of January 25, 2023, there are over 77 data-contributing sites to the N3C data enclave. In this application, to avoid demanding computing time and memory, we construct a toy example with a moderate sample size and a moderate number of candidate features to demonstrate our method. The cohort is constructed by a matched case-control sampling of patients with confirmed COVID infection from two data partners ($n_1 = 12982$ in site 124 and $n_2=3100$ in site 967). Information on whether the patient has developed long COVID after the acute COVID has been recorded in the two data sites differently. In data site 124, a binary long COVID U9.09 diagnosis is provided as the long COVID outcome, whereas in site 967, a binary long COVID clinical visit index is recorded as the long COVID outcome. These two long COVID indicators are highly related but not the same. A list of patient baseline information has been extracted as (group level) candidate risk factors ($M = 43$). For some of the candidate variables, the data from the two sites are recorded differently (for example, the "race-ethnicity" variable has different numbers of levels). Our goal for this analysis is to identify mutual risk factors for these two outcomes. Details on the cohort construction and candidate risk factors can be found in Appendix \ref{app:real}. 

We create dummy variables for the categorical variables and use the \textit{GS knockoff} method with the second order method for the knockoff construction in Step 1, the group knockoff+ filter with the $\gfdr$ controlled at 0.2. We also compare the result with the selection using the group knockoff filter. 
%{\color{red} [Is it meaningful to compare the knockoff vs knockoff+? Since the statistics are the same, the selected set from knockoff+ will always be a subset from knockoff.]}

Using the group knockoff+ filter, 16 risk factors are selected: sex, age at covid, race, dementia, obesity, coronary-artery disease, systemic corticosteroids, depression, metastatic solid tumor cancers, chronic lung disease, myocardial infarction, cardiomyopathies, hypertension, negative antibody, number of covid vaccine doses and covid associated emergency department visit. Using the group knockoff filter, 17 risk factors are selected, including 16 risk factors the same as risk factors selected using group knockoff+ and sickle-cell disease. 





\section{Discussion}
In this paper, we present a novel \textit{GS knockoff} method, which allows us to control FDR in testing the union null hypotheses on conditional associations between group-level candidate features and outcomes. Like other knockoff-based methods, the \textit{GS knockoffs} can work with very general conditional model settings and covariate structures within the individual datasets, assuming the independence between the datasets. This method allows us to collectively use information from datasets with different dependencies of $Y|\X$, different outcomes $Y$, and heterogeneous $\X$ structures, allowing for different data types and different group sizes across multiple datasets. The FDR control guarantee is exact for finite sample settings. 

This method has broad applications beyond the N3C long COVID real data example. For example, in EHR data from multiple data centers, some covariates are recorded differently among the centers (for example, some centers record the body mass index (BMI) as a continuous variable, but others as an ordinal categorical variable); some groups of variables are of different group sizes (for example, for a solid organ transplant, different centers report different lists of organs); and the demographic distributions are different across the data sources. The \textit{GS knockoff} method can be used to identify mutual signals with more reliable associations. This method also requires very limited information (only the test statistics) to be shared among the data centers, which benefits data collaboration under privacy protections.

Under the \textit{GS knockoff} framework, both the Fixed-X and Model-X knockoff approaches can be used for the individual datasets. The framework is compatible with all the existing knockoff and group knockoff construction methods. In this work, we develop a list of group knockoff construction methods to work with both the Fixed-X and Model-X knockoff approaches. 
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}


\section*{Acknowledgements}
Ran Dai is partly supported by the National Institute of General Medical Sciences, U54 GM115458 and U54GM104942, which funds the Great Plains IDeA-CTR Network, GP-CTR Health Informatics Research Scholar Program, and the West Virginia CTSI. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. The analyses described in this publication were conducted with data or tools accessed through the NCATS N3C Data Enclave \url{covid.cd2h.org/enclave} and supported by CD2H - The National COVID Cohort Collaborative (N3C) IDeA CTR Collaboration 3U24TR002306-04S2 NCATS U24 TR002306. This research was possible because of the patients whose information is included within the data from participating organizations (\url{covid.cd2h.org/dtas}) and the organizations and scientists (\url{covid.cd2h.org/duas}) who have contributed to the on-going development of this community resource\citep{Haendel-N3C}. Authorship was determined using ICMJE recommendations. We gratefully acknowledge the following core contributors to N3C:
Adam B. Wilcox, Adam M. Lee, Alexis Graves, Alfred (Jerrod) Anzalone, Amin Manna, Amit Saha, Amy Olex, Andrea Zhou, Andrew E. Williams, Andrew Southerland, Andrew T. Girvin, Anita Walden, Anjali A. Sharathkumar, Benjamin Amor, Benjamin Bates, Brian Hendricks, Brijesh Patel, Caleb Alexander, Carolyn Bramante, Cavin Ward-Caviness, Charisse Madlock-Brown, Christine Suver, Christopher Chute, Christopher Dillon, Chunlei Wu, Clare Schmitt, Cliff Takemoto, Dan Housman, Davera Gabriel, David A. Eichmann, Diego Mazzotti, Don Brown, Eilis Boudreau, Elaine Hill, Elizabeth Zampino, Emily Carlson Marti, Emily R. Pfaff, Evan French, Farrukh M Koraishy, Federico Mariona, Fred Prior, George Sokos, Greg Martin, Harold Lehmann, Heidi Spratt, Hemalkumar Mehta, Hongfang Liu, Hythem Sidky, J.W. Awori Hayanga, Jami Pincavitch, Jaylyn Clark, Jeremy Richard Harper, Jessica Islam, Jin Ge, Joel Gagnier, Joel H. Saltz, Joel Saltz, Johanna Loomba, John Buse, Jomol Mathew, Joni L. Rutter, Julie A. McMurry, Justin Guinney, Justin Starren, Karen Crowley, Katie Rebecca Bradwell, Kellie M. Walters, Ken Wilkins, Kenneth R. Gersing, Kenrick Dwain Cato, Kimberly Murray, Kristin Kostka, Lavance Northington, Lee Allan Pyles, Leonie Misquitta, Lesley Cottrell, Lili Portilla, Mariam Deacy, Mark M. Bissell, Marshall Clark, Mary Emmett, Mary Morrison Saltz, Matvey B. Palchuk, Melissa A. Haendel, Meredith Adams, Meredith Temple-O'Connor, Michael G. Kurilla, Michele Morris, Nabeel Qureshi, Nasia Safdar, Nicole Garbarini, Noha Sharafeldin, Ofer Sadan, Patricia A. Francis, Penny Wung Burgoon, Peter Robinson, Philip R.O. Payne, Rafael Fuentes, Randeep Jawa, Rebecca Erwin-Cohen, Rena Patel, Richard A. Moffitt, Richard L. Zhu, Rishi Kamaleswaran, Robert Hurley, Robert T. Miller, Saiju Pyarajan, Sam G. Michael, Samuel Bozzette, Sandeep Mallipattu, Satyanarayana Vedula, Scott Chapman, Shawn T. O'Neil, Soko Setoguchi, Stephanie S. Hong, Steve Johnson, Tellen D. Bennett, Tiffany Callahan, Umit Topaloglu, Usman Sheikh, Valery Gordon, Vignesh Subbian, Warren A. Kibbe, Wenndy Hernandez, Will Beasley, Will Cooper, William Hillegass, Xiaohan Tanner Zhang. Details of contributions available at \url{covid.cd2h.org/core-contributors}.


\bibliography{example_paper}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Technical Lemmas}
\begin{lemma}\label{lem:gsknock}
For the $\widetilde{X}$ generated from Algorithm \ref{alg:groupModelX}, it satisfies the group model-X knockoff requirements.
\end{lemma}
\begin{proof}
Since the generation is without looking at $Y$, so the second condition holds automatically. Here we just need to verify the first condition. Since $(\X_{G_1},\widetilde{\X}_{G_1})|\X_{-G_1}\eqd (\widetilde{\X}_{G_1},\X_{G_1})|\X_{-G_1}$, we have $(\X,\widetilde{\X}_{G_1})\eqd (\X,\widetilde{\X}_{G_1})_{GSwap(1)}$. Now, we use proof by induction similar as \cite{candes2018panning} to show that $(\X,\widetilde{X}_{G_1},\dots,\widetilde{X}_m)\eqd (\X,\widetilde{X}_{G_1},\widetilde{X}_{G_m})_{GSwap(S)}$ for any $S\in [m]$ after $m$ steps. Since the swap of multiple groups can be performed in multiple steps with a swap of one group at a time, we just need to prove the setting for $S=\{j\}$ for $j\in [m]$. Notice the measure for the joint distribution can be written as
\begin{eqnarray*}
dP(\X,\widetilde{\X}_{\cup_{j=1}^m G_j})&=&dP(\X,\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})\frac{dP(\widetilde{\X}_{G_j},\X,\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})}{dP(\X,\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})}\\
&=&dP(\X_{-G_j},\X_{-G_j},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})\frac{dP(\X_{-G_{j}},\widetilde{\X}_{G_j},\widetilde{\X}_{\cup_{j=1}^{m-1} G_j}))}{\int dP(\X_{-G_{j}},du,\widetilde{\X}_{\cup_{j=1}^{m-1} G_j})}
\end{eqnarray*}
Changing $\X_{G_j}$ and $\widetilde{X}_{G_j}$ will lead to the same numerator and when $j<m$, by induction, the denominator does not change and when $j=m$, the denominator does not depend on $\X_{G_j}$ and $\tilde{X}_{G_j}$, so the group exchangeability holds.

Now after the $M$ steps, we get the knockoff that satisfies condition 1 of the group model-X knockoff construction.
\end{proof}

\begin{lemma}\label{lem:gseqknock}
For the $\widetilde{X}$ generated from the sequential group knockoff, it satisfies the group model-X knockoff requirements when the model is correctly specified and the true parameters are used.
\end{lemma}
\begin{proof}
When the model is correctly specified and true parameters are used, we have
\begin{eqnarray*}
\widetilde{\X}_{G_m}^{con}|\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j}\eqd \X_{G_m}^{con}|\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j}\\
\widetilde{\X}_{G_m}^{cat}|\widetilde{\X}_{G_m}^{cat},\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j}\eqd \X_{G_m}^{con}|\widetilde{\X}_{G_m}^{cat},\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j}
\end{eqnarray*}
which together implies
\begin{eqnarray*}
\widetilde{\X}_{G_m}|\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j}\eqd \X_{G_m}|\X_{-G_m}, \widetilde{\X}_{\cup_{j=1}^{m-1}G_j},
\end{eqnarray*}
and this follows the general group model-X knockoff generation procedure. So applying Lemma \ref{lem:gsknock} finishes the proof.
\end{proof}

The \textit{GS knockoff} procedure robustness against the misspecification of the distribution of $\X$. In real applications, when we have additional samples of $\X$ (for estimating the distribution of $\X$), we will be able to approximate the $\X$ distribution well. Theorem 2 of \cite{dai2021multiple} can be easily extended to show an FDR upper bound result for \textit{GS knockoffs}. 


%{\color{red}Add comment on robustness? Add robustness result?}


\begin{lemma}\label{lem:1}
Let $\W = f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K])$ where $f$ is an OSFF. Let $\epsilon \in \{\pm 1\}^M$ be a sign sequence independent of $\W$, with $\epsilon_j = +1$ for all $j \in \mathcal{S}$ and $\epsilon_j \sim \{\pm 1\}$ for all $j \in \mathcal{H}$. Then
$(W_1, \cdots, W_M) \eqd (W_1\cdot \epsilon_1, \cdots, W_M\cdot\epsilon_M)$. 
\end{lemma}
\begin{proof}
For any $S\subseteq \mathcal{H}$, we can write it as the union of $K$ subsets $S=\cup_{k=1}^K S_k$, where $S_k\subseteq \mathcal{H}_k$ for $k = 1,\cdots,K$, and $S_{k1}\cap S_{k2} = \emptyset$ for all $k_1 \neq k_2$. In particular, we can let $S_k=S\cap \mathcal{H}_k\cap (\cup_{j=1}^{k-1}\mathcal{H}_j)^c$. 
Since $S_k\subseteq \mathcal{H}_k$, for $k\in [K]$, any statistics $[\Z^k, \widetilde{\Z}^k]=w([\X^k,\widetilde{\X}^k],\Y^k)$, by the construction of knockoffs, satisfy 
$[\Z^k, \widetilde{\Z}^k] \eqd [\Z^k, \widetilde{\Z}^k]_{\textnormal{Swap}(S_k)} $. By the mutually independence between $[Z^1,\widetilde{Z}^1],\cdots, [Z^K,\widetilde{Z}^K]$, we have 

\[f\left([\Z^1,\widetilde{\Z}^1]_{\textnormal{Swap}(S_1)},\cdots,[\Z^K,\widetilde{\Z}^K]_{\textnormal{Swap}(S_K)}\right)\eqd f\left([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K]\right).\] 

Using the definition of the OSFF, we have
\begin{eqnarray*}
&&f\left([\Z^1,\widetilde{\Z}^1]_{\textnormal{Swap}(S_1)},[\Z^2,\widetilde{\Z}^2]_{\textnormal{Swap}(S_2)},\cdots,[\Z^K,\widetilde{\Z}^K]_{\textnormal{Swap}(S_K)}\right)\\
&=&f\left([\Z^1,\widetilde{\Z}^1],[\Z^2,\widetilde{\Z}^2]_{\textnormal{Swap}(S_2)},\cdots,[\Z^K,\widetilde{\Z}^K]_{\textnormal{Swap}(S_K)}\right)\odot \epsilon(S_1)\\
&=&\cdots\\
&=&f\left([\Z^1,\widetilde{\Z}^1],[\Z^2,\widetilde{\Z}^2],\cdots,[\Z^K,\widetilde{\Z}^K]\right)\odot_{k=1}^K \epsilon(S_k)\\
&=&f\left([\Z^1,\widetilde{\Z}^1],[\Z^2,\widetilde{\Z}^2],\cdots,[\Z^K,\widetilde{\Z}^K]\right)\odot\epsilon(S).
\end{eqnarray*}

So we obtain \[\W=f\left([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K]\right)\eqd f\left([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K]\right)\odot\epsilon(S)=\W\odot\epsilon(S).\] for any $S\subseteq \mathcal{H}$. Therefore, by choosing $S$ as the set $\{j:\epsilon_j=-1\}$, we have \[(W_1, \cdots, W_M) \eqd (W_1\cdot \epsilon_1, \cdots, W_M\cdot\epsilon_M).\] and thus we finish the proof of the lemma.
\end{proof}


\begin{lemma}\label{lem:mar}
For $k=m,m-1,\cdots,1,0$, put $V^{+}(k)=\#\{j:1\leq j\leq k, p_j\leq 1/2, j\in \mathcal{H}\}$ and $V^{-}(k)=\#\{ j:1\leq j\leq k, p_j> 1/2, j\in \mathcal{H}\}$ with the convention that $V^{\pm}(0)=0$. Let $\mathcal{F}_k$ be the filtration defined by knowing all the non-null $p$-values, as well as $V^{\pm}(k')$ for all $k'\geq k$. Then the process $M(k)=\frac{V^{+}(k)}{1+V^{-}(k)}$ is a super-martingale running backward in time with respect to $\mathcal{F}_k$. For any fixed $q$, $\widehat{k}=\widehat{k}_{+}$ or $\widehat{k}=\widehat{k}_{0}$ as defined in the proof of theorem 1 are stopping times, and as consequences
\begin{eqnarray*}
\EE{\frac{\#\{j\leq \widehat{k}:p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{j\leq \widehat{k}:p_j>1/2, j\in \mathcal{H}\}}}\leq 1
\end{eqnarray*}
\end{lemma}

\begin{proof}
The filtration $\mathcal{F}_k$ contains the information of whether $k$ is null and non-null process is known exactly. If $k$ is non-null, then $M(k-1)=M(k)$ and if $k$ is null, we have
\begin{eqnarray*}
M(k-1)=\frac{V^{+}(k)-\mathbbm{1}_{p_k\leq 1/2}}{1+V^{-}(k)-(1-\mathbbm{1}_{p_k\leq 1/2})}=\frac{V^{+}(k)-\mathbbm{1}_{p_k\leq 1/2}}{\left(V^{-}(k)+\mathbbm{1}_{p_k\leq 1/2}\right)\vee 1}
\end{eqnarray*}
Given that nulls are \iid, we have
\begin{eqnarray*}
\PP{\mathbbm{1}_{p_k\leq 1/2}|\mathcal{F}_k}=\frac{V^{+}(k)}{\left(V^{+}(k)+V^{-}(k)\right)}.
\end{eqnarray*}
So when $k$ is null, we have
\begin{eqnarray*}
\EE{M(k-1)|\mathcal{F}_k}&=&\frac{1}{V^{+}(k)+V^{-}(k)}\left[V^{+}(k)\frac{V^{+}(k)-1}{V^{-}(k)+1}+V^{-}(k)\frac{V^{+}(k)}{V^{-}(k)\vee 1}\right]\\
&=&\frac{V^{+}(k)}{1+V^{-}(k)}\mathbbm{1}_{V^{-}(k)>0}+(V^{+}(k)-1)\mathbbm{1}_{V^{-}(k)=0}\\
&\leq &M(k)
\end{eqnarray*}
This finishes the proof of super-martingale property. $\widehat{k}$ is a stopping time with respect to $\{\mathcal{F}_k\}$ since $\{\widehat{k}\geq k\}\in \mathcal{F}_k$. So we have $\EE{M(\widehat{k})}\leq \EE{M(m)}=\EE{\frac{\#\{j:p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{j:p_j>1/2, j\in \mathcal{H}\}}}$.

Let $X=\#\{j:p_j\leq 1/2, j\in \mathcal{H}\}$, given that $p_j\geq Unif[0,1]$ independently for all nulls, we have $X\leq_d Binomial(N,1/2)$. Let $Y\sim Binomial(N,1/2)$ where $N$ is the total number of nulls. Given that $f(x)=\frac{x}{1+N-x}$ is non-decreasing, we have
\begin{eqnarray*}
\EE{\frac{X}{1+N-X}}&\leq &\EE{\frac{Y}{1+N-Y}}\\
&=&\sum_{i=1}^N (1/2)^{N}\frac{N!}{i!(N-i)!}\frac{i}{1+N-i}\\
&=&\sum_{i=1}^N \PP{Y=i-1}\\
&\leq &1.
\end{eqnarray*}
\end{proof}


\section{Proof of the main theorem (Theorem \ref{thm:1})}
The proof of Theorem \ref{thm:1} follows the proof idea in \cite{barber2015}.
Let $m=\#\{j:W_j\neq 0\}$ and assume without loss of generality that $|W_1|\geq |W_2|\geq\cdots\geq |W_m|>0$. Define p-values $p_j=1$ if $W_j<0$ and $p_j=1/2$ if $W_j>0$, then Lemma 1 implies that null $p$-values are $i.i.d.$ with $p_j\geq \text{Unif}[0,1]$ and are independent from nonnulls. 

We first show the result for the knockoff+ threshold. Define $V=\#\{j\leq \widehat{k}_{+}: p_j\leq 1/2, j\in \mathcal{H}\}$ and $R=\#\{j\leq \widehat{k}_{+}:p_j\leq 1/2\}$ where $\widehat{k}_{+}$ satisfy that $|W_{\widehat{k}_{+}}|=\tau_{+}$ where $\tau_{+}$ is defined in theorem 1, we have
\begin{eqnarray*}
&&\EE{\frac{V}{R\vee 1}}=\EE{\frac{V}{R\vee 1}\mathbbm{1}_{\widehat{k}_{+}>0}}\\
&=&\EE{\frac{\#\{j\leq \widehat{k}_{+}: p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{j\leq \widehat{k}_{+}: p_j> 1/2, j\in \mathcal{H}\}}\left(\frac{1+\#\{j\leq \widehat{k}_{+}: p_j> 1/2, j\in \mathcal{H}\}}{\#\{j\leq \widehat{k}_{+}:p_j\leq 1/2\}\vee 1}\right)\mathbbm{1}_{\widehat{k}_{+}>0}}\\
&\leq&\EE{\frac{\#\{j\leq \widehat{k}_{+}: p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{ j\leq \widehat{k}_{+}: p_j> 1/2, j\in \mathcal{H}\}}}q\leq q,
\end{eqnarray*}
where the first inequality holds by the definition of $\widehat{k}_{+}$ and the second inequality holds by Lemma \ref{lem:mar}.

Similarly, for the knockoff threshold, we have $V=\#\{j\leq \widehat{k}_0: p_j\leq 1/2, j\in \mathcal{H}\}$ and $R=\#\{j\leq \widehat{k}_0:p_j\leq 1/2\}$ where $\widehat{k}_0$ satisfies that $|W_{\widehat{k}_0}|=\tau$ where $\tau$ is defined as in theorem 1, then
\begin{eqnarray*}
&& \EE{\frac{V}{R+q^{-1}}}\\
&=&\EE{\frac{\#\{ j\leq \widehat{k}_0: p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{j\leq \widehat{k}_0: p_j> 1/2, j\in \mathcal{H}\}}\left(\frac{1+\#\{ j\leq \widehat{k}_0: p_j> 1/2, j\in \mathcal{H}\}}{\#\{j\leq \widehat{k}_0:p_j\leq 1/2\}+q^{-1}}\right)\mathbbm{1}_{\widehat{k}_0>0}}\\
&\leq&\EE{\frac{\#\{j\leq \widehat{k}_0: p_j\leq 1/2, j\in \mathcal{H}\}}{1+\#\{ j\leq \widehat{k}_0: p_j> 1/2, j\in \mathcal{H}\}}}q\leq q,
\end{eqnarray*}
where the first inequality holds by the definition of $\widehat{k}_0$ and the second inequality holds by Lemma \ref{lem:mar}.


\section{Proof of the corollary}
\begin{proof}
First, we noticed that for $m\in S$,
\begin{eqnarray*}
W_m&=&f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k]_{\textnormal{Swap}(S)},\cdots,[\Z^K,\widetilde{\Z}^K])_m\\
&=&\prod_{j\in [K]\backslash k} (Z^j_m-\widetilde{Z}^j_m)(\widetilde{Z}_m^k-Z_m^k)\\
&=&-\prod_{j\in [K]} (Z^j_m-\widetilde{Z}^j_m)\\
&=&-f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K])_m,
\end{eqnarray*}
and for $m\notin S$
\begin{eqnarray*}
W_m&=&f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k]_{\textnormal{Swap}(S)},\cdots,[\Z^K,\widetilde{\Z}^K])_m\\
&=&\prod_{j\in [K]\backslash k} (Z^j_m-\widetilde{Z}^j_m)(Z_m^k-\widetilde{Z}_m^k)\\
&=&\prod_{j\in [K]} (Z^j_m-\widetilde{Z}^j_m)\\
&=&f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^K,\widetilde{\Z}^K])_m,
\end{eqnarray*}
So we have
\begin{eqnarray*}
W&=&f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k]_{\textnormal{Swap}(S)},\cdots,[\Z^K,\widetilde{\Z}^K])\\
&=&f([\Z^1,\widetilde{\Z}^1],\cdots,[\Z^k,\widetilde{\Z}^k],\cdots,[\Z^K,\widetilde{\Z}^K])\odot \epsilon(S)
\end{eqnarray*}
and thus the OSFF assumption is satisfied. 

When Model-X group knockoff construction is used, we notice that based on the construction of group Model-X knockoff, we have $[\X^k \widetilde{\X}^k]\eqd [\X^k \widetilde{\X}^k]_{GSwap(S)}$, $\widetilde{\X}^k\indep Y^k | \X^k$. For any $S\subseteq\mathcal{H}$, we have $[\X^k, \widetilde{\X}^k]|Y^k\eqd [\X^k, \widetilde{\X}^k]_{GSwap(S)}|Y^k$. By the definition of knockoff-compatible statistics, we have 
\begin{eqnarray*}
&&[\Z^k,\widetilde{\Z}^k]_{Swap(S)}=t([\X^k, \widetilde{\X}^k]_{GSwap(S)}, Y^k)\\
&=^d&t([\X^k, \widetilde{\X}^k], Y^k)\\
&=&[\Z^k,\widetilde{\Z}^k]
\end{eqnarray*}
 for any $S\subseteq\mathcal{H}$. When fixed group knockoff construction is used, by the definition of knockoff compatible statistics and sufficiency requirement, we have
 \begin{eqnarray*}
&&[\Z^k,\widetilde{\Z}^k]_{Swap(S)}=t([\X^k, \widetilde{\X}^k]^\top_{GSwap(S)}[\X^k, \widetilde{\X}^k]_{GSwap(S)}, [\X^k, \widetilde{\X}^k]_{GSwap(S)}^\top Y^k)\\
&=^d&t([\X^k, \widetilde{\X}^k]^\top[\X^k, \widetilde{\X}^k], [\X^k, \widetilde{\X}^k]^\top Y^k)\\
&=&[\Z^k,\widetilde{\Z}^k].
\end{eqnarray*}

 Applying Theorem \ref{thm:1}, we obtain the conclusion for this corollary.
\end{proof}

\section{Additional simulation details} \label{app:sim}

\subsection{Data Generation}

\paragraph{Continuous.} We set both experiments to have the same sample size $n_k=1000$ and the same number of groups of features $M=80$. We have the 5 features per group for each group (i.e, $G_{km} = \{5m-4, \cdots, 5m\}$ for $k \in [K], m \in [M]$ and $p_1=p_2=400$ features in total for each experiment). We simulate independent $X^k$s for $k \in [K]$ such that
\[\X_i^k \sim \mathcal{N} (\mathbf{0}, \Sigma^{k}) ~\text{for}~ i \in [n_k],\]
where $\Sigma^k \in \R^{p_k \times p_k}$ with diagonal elements $\Sigma_{jj}^{k}=1$ for $j \in [p_k]$, within-group correlations $\Sigma_{ji}^{k}=\rho_{k}$ for $i \neq j$ in the same group (i.e., $\{i,j\}\subset G_{km}$ for some $m\in [M]$) and between-group correlations $\Sigma_{ji}^{k}=\gamma_{k}\rho_{k}$ for $j	
\neq i$ in different groups (i.e., There is no $m\in [M]$ such that $\{i,j\}\subset G_{km}$). 

Next, we generate the coefficients $\beta^1, \cdots,\beta^K$ for the $K$ experiments. %In a group setting, $\beta^{k\top}=(\beta_{G_{k1}}^{k\top},\cdots, \beta_{G_{kM}}^{k\top})=((\beta_{1}^k,\beta_{2}^k,\dots,\beta_{5}),\dots,(\beta_{5M-4}^k,\beta_{5M-3}^k,\dots,\beta_{5M}^k))$. 
We denote $s_0$ as the number of groups of simultaneous signals among the $K$ datasets, $s_k$ as the number of groups of signals specifically for the $k$-th datasets. We consider two scenarios: (1) both directions and strengths of the mutual signals are the same among the $K$ datasets, and (2) only the directions of the mutual signals are the same but the signal strengths are different among the K datasets. For both datasets, the signal strengths within each group $m \in [M]$ are identical.

For Scenario 1, we sample $\omega_{j} \in \mathbb{R}^{s_j}, j \in \{0,1,2\}$, with their elements $\omega_{ji}\sim \textnormal{Uniform}[0,A]$ independent for $ i=1,\cdots, s_j$. Then we sample $\epsilon \in \{-1,1\}^M$ where $\epsilon_m$ are independently sampled from Rademacher distribution for $l=1,\cdots,M$. With $K=2$, the coefficients $\beta^1, \beta^2$ are determined by:

\begin{eqnarray*}
\beta^1&=&((\omega_0^\top , \omega_1^\top, \mathbf{0}_{M-s_0-s_1}^\top)^\top  \odot \epsilon ) \otimes \mathbf{1}_5^\top,\\
\beta^2&=&((\omega_0^\top, \mathbf{0}_{s_1}^\top,\omega_2^\top, \mathbf{0}_{M-s_0-s_1-s_2}^\top)^\top \odot \epsilon ) \otimes \mathbf{1}_5^\top, 
\end{eqnarray*}
where $\otimes$ is the Kronecker product and $\odot$ is the Hadamard product.\\

For Scenario 2, we generate $\omega_{0k} \in \mathbb{R}^{s_0}$ for $k\in [K]$ independently; i.e., we sample $\omega_{0ki} \sim \textnormal{Uniform}[0,A]$ independently for $k \in [K]$ and $i=1,\cdot, s_0$. We generate $\omega_{1}$, $\omega_2$, and $\epsilon$ the same way as described in Scenario 1. The coefficients $\beta^1, \beta^2$ are determined by:
\begin{eqnarray*}
\beta^1&=&((\omega_{01}^\top, \omega_1^\top, \mathbf{0}_{M-s_0-s_1}^\top)^\top\odot \epsilon) \otimes \mathbf{1}_5^\top\\
\beta^2&=&((\omega_{02}^\top, \mathbf{0}_{s_1}^\top,\omega_2^\top, \mathbf{0}_{M-s_0-s_1-s_2}^\top)^\top\odot \epsilon)\otimes \mathbf{1}_5^\top.
\end{eqnarray*}


Then $Y_i^k$s are obtained from the following linear model:
\begin{eqnarray*}\label{eqn:linmod}
Y^k_{i}&=&\beta^{k\top}\X^k_{i}+\varepsilon^k_{i},
\end{eqnarray*}
where $\varepsilon_i^k\sim \mathcal{N}(0,\sigma_k^2)$ for $k=1,2$, and $i=1,\dots,n_k$, $\sigma_k$ is the signal noise ratio. %In grouped setting, the features are partitioned into $M$ groups of variables for each experiment, the group size for each group is denoted as $p_i$. 

%We conduct simulations to demonstrate the performance of our proposed $\textit{GS knockoff}$ method on data settings with various sparsity level of the mutual signals ($s_0$) and non-mutual signals ($s_1$, $s_2$); different within group feature correlations $\rho_k$, and ratio between the between-group correlations and within-group correlations $\gamma_k$. 

\paragraph{Binary.} Our design matrices $\X^k$s are composed by a mixture of $M=80$ categorical and continuous variables. For both data sets, without loss of generality, we set the first 20 features as independent categorical variables with 5 categories (each level with probability 0.2) and generate the other 60 features from a multivariate normal distribution with mean 0 and covariance $\Sigma(\gamma)$, where $\Sigma(\gamma)$ is an auto-correlation matrix with its (i,j)-th element equals to $\gamma^{|i-j|}$. For each categorical variable, we create 4 dummy variables and consider them as a group (i.e. $p_k = 140$ for $k \in [K]$). For continuous variables, each of them is considered as a group.

Next, we generate the coefficients $\beta^1, \cdots, \beta^K \in \R^{p_k}$. We randomly select an index set $S_0 \subseteq[M]$ with length $|S_0|=s_0$ to indicate the mutual group-level signals among the $K$ datasets and mutually exclusive sets $S_k \subseteq [M]\setminus S_0, ~\text{for}~ k \in K$ with $|S_k| = s_k$ as the indices for the exclusive signals for the $k$-th dataset. For the random selection, we stratified by the type of variable (categorical vs continuous) to keep the ratio between categorical variables and continuous variables in $S_0$, $\dots$, $S_K$ to be 1:3. Then we randomly generate the magnitudes and directions of the $\beta$ elements independently in the signal sets. %{\color{red} Verify with Runqiu on the final settings}{\color{red}[Need provide formula for $\beta^1$ and $\beta^2$ since they have different dimension from continuous setting.]}


Let $\bar{\X}^k$ denote the expanded design matrix of $\X^k$ after replacing each categorical variable with dummy variables. Then we generate $Y^k$ from logistic models: 
\begin{eqnarray*}
Y^k_{i}\sim \textnormal{Bernoulli} \left(\frac{\exp(\alpha_k+\beta^{k\top}\bar{\X}^k_{i})}{1+\exp(\alpha_k+\beta^{k\top}\bar{\X}^k_{i})}\right),
\end{eqnarray*}
where $k=1,2~\text{and}~ i=1,\cdots, n_k$. 

\paragraph{Mixed.} We first generate $\X^k$s and $\beta^k$s the same as the binary case and expand the design matrix to obtain $\bar{\X}^k$. Then we generate the latent outcome $\overline{Y}^k$s for $k \in [K]$ from the linear models:
\begin{eqnarray*}
\overline{Y}^k_{i}&=&\beta^{k\top}\bar{\X}^k_{i}+\varepsilon^k_{i},
\end{eqnarray*}
where $\varepsilon_i^k\sim \mathcal{N}(0,\sigma_k^2)$ for $k=1,2$, and $i=1,\dots,n_k$, $\sigma_k$ is the signal noise ratio. Then we set $Y_i^1=\overline{Y}_i^1$ and set a threshold for $\overline{Y}_i^2$ to construct the binary $Y_i^2$: \[Y^2_{i} = \mathbbm{1}\{\overline{Y}^2_{i} \geq 0\}, ~\text{where}~i=1,\cdots,n_2.\]. 

\subsection{Simulation settings}
We conduct simulations to check the effects of sparsity levels $s_0, s_1, s_2$ and different correlation structures. To be specific, for the continuous case, we set the amplitude of signals $A=0.6$, the within-group correlations $\rho_k=0.5, ~\text{for}~ k \in [K]$, and the between-group correlations are set to be $\gamma_k\rho_k, ~\text{for}~ k \in [K]$, with the default correlation ratio $\gamma_k=0.1$. To understand the effects of sparsity levels, within and between group correlations respectively, we vary one of three kinds of parameters (sparsity levels parameters, within-group correlation parameters, and correlation ratio parameters) in each simulation study and fix the other two kinds of parameters. 
\begin{itemize}
\item Sparsity level parameters: $s_0, s_1, s_2.$ We fix $\gamma_k=0.1$, and $\rho_k=0.5$.
 \begin{align*}
        &\text{1. Fixing $s_1 = s_2 =0$, we vary $s_0 = 8,12,16,20,24$;}\\
        &\text{2. Fixing $s_0 =16$, we vary $s_1 = s_2= 4,8,12,16,20,24$;}\\
        &\text{3. Fixing $s_0 =16$ and $s_1=0$, we vary $s_2= 4,8,12,16,20,24$.}\\
    \end{align*}
\item Within-group correlation parameters: $\rho_1, \rho_2$. We fix $\gamma_k=0.1$ and vary within-group correlations $\rho_{1}=\rho_2 \in \{0, 0.1, \dots, 0.9\}$ for the following three choice of $s_0, s_1, s_2$.
\begin{align*}
        & s_0 =16,s_1=0,s_2=0;\\
        & s_0 =16,s_1=16,s_2=16;\\
        & s_0=16,s_1=0,s_2=16.
    \end{align*}

\item Correlation ratio parameters: $\gamma_1, \gamma_2$. We fix $\rho_k=0.5$ and vary correlation ratio $\gamma_1=\gamma_2 \in \{0,0.05,0.1,\dots,0.5\}$ for the following three choice of $s_0, s_1, s_2$. Then the between-group correlations are calculated as $\rho_k\gamma_k$.
\begin{align*}
        & s_0 =16,s_1=0,s_2=0;\\
        & s_0 =16,s_1=16,s_2=16;\\
        & s_0=16,s_1=0,s_2=16.
    \end{align*}
For the binary and mixed cases, we only vary the sparsity levels. We fixed the auto-correlation parameter $r_1=r_2=0.5$, $\alpha_1=1, \alpha_2=-1$, and vary the sparsity level $s_0, s_1, s_2.$ as following:
 \begin{align*}
        &\text{1. Fixing $s_1 = s_2 =0$, we vary $s_0 = 8,12,16,20,24$;}\\
        &\text{2. Fixing $s_0 =16$, we vary $s_1 = s_2= 4,8,12,16,20,24$;}\\
        &\text{3. Fixing $s_0 =16$ and $s_1=0$, we vary $s_2= 4,8,12,16,20,24$.}\\
    \end{align*}
\end{itemize}



\begin{comment}
\subsection{Continuous}
Let $X^k=(X_{i1}^k, X_{i2}^k, \dots, X_{iM}^k)$, where $X_{im}^k$ contains the features from group $m$. We generate coefficients $\bmbeta^1, \cdots,\bmbeta^K$ for the $K$ experiments. In a group setting, $\beta^{k\top}=(\beta_1^{k\top},\dots, \beta_M^{k\top})$$=((\beta_{11}^k,\beta_{12}^k,\dots,\beta_{15}),\dots,(\beta_{M1}^k,\beta_{M2}^k,\dots,\beta_{M5}^k))$. Besides, we denoted $s_0$ as the number of groups of simultaneous signals among the $K$ experiments, $s_k$ as the number of groups of simultaneous signals which are only shown in the $k$-th experiments. We consider two scenarios for the signals' strengths: one is both directions and strengths of the mutual signals are the same among the $K$ experiments (scenario 1), and the other one is only the directions of the mutual signals are the same but the signal strengths are different among the K experiments (scenario 2). For each experiment, $\beta^k$ is divided into $M$ groups, the strengths and direction of signals will be the same within group.

For Scenario 1, we sample $\omega_{j} \in \mathbb{R}^{s_j}, j \in [K]$, where $\omega_{ji}\sim \textnormal{Uniform}[0,A]$ are independent for $j=[K]$ and $i=1,\cdots, s_j$. Then we sample $\bmepsilon \in \{-1,1\}^p$ where $\epsilon_l$ are independently sampled from rademacher distribution for $l=1,\cdots,M$. With $K=2$, the coefficients $\beta^1, \bmbeta^2$ are determined by:
\begin{eqnarray*}
\bmbeta^1&=&(\omega_0^\top, \omega_1^\top, \mathbf{0}_{M-s_0-s_1}^\top)^\top\odot \bmepsilon,\\
\bmbeta^2&=&(\omega_0^\top, \mathbf{0}_{s_1}^\top,\omega_2^\top, \mathbf{0}_{M-s_0-s_1-s_1}^\top)^\top\odot \bmepsilon, 
\end{eqnarray*}
~\text{where $\odot$ is the Hadamard product}.\\

For Scenario 2, we generate $\omega_{0k} \in \mathbb{R}^{s_0}$ for $k=1,\cdots,K$ independently; i.e., we sample $\omega_{0ki} \sim \textnormal{Uniform}[0,A]$ independently for $k=1,\cdots,K$ and $i=1,\cdots, s_0$. We generate $\omega_{1}$, $\bmeta_2$, and $\bmepsilon$ the same way as described in Scenario 1. The coefficients $\bmbeta^1, \bmbeta^2$ are determined by:
\begin{eqnarray*}
\bmbeta^1&=&(\omega_{01}^\top, \omega_1^\top, \mathbf{0}_{M-s_0-s_1}^\top)^\top\odot \bmepsilon\\
\bmbeta^2&=&(\omega_{02}^\top, \mathbf{0}_{s_1}^\top,\omega_2^\top, \mathbf{0}_{M-s_0-s_1-s_1}^\top)^\top\odot \bmepsilon.
\end{eqnarray*}
{\color{red}Add $\omega_{0}$ is a vector, $\bmbeta^1$ is not dimension p but p*pi.}\\
For the continuous setting, we choose the amplitude of signals as $A=0.6$.

As a default, we use within-group correlations $\rho_k=0.5$, between-group correlations are set to be $\gamma_k\rho_k$, with the default correlation ratio $\gamma_k=0.1$. 
\begin{itemize}
\item Varying sparsity Level $s_0, s_1, s_2.$ We fix $\gamma_k=0.1$, and $\rho_k=0.5$.
 \begin{align*}
        &\text{1. Fixing $s_1 = s_2 =0$, we vary $s_0 = 8,12,16,20,24$;}\\
        &\text{2. Fixing $s_0 =16$, we vary $s_1 = s_2= 4,8,12,16,20,24$;}\\
        &\text{3. Fixing $s_0 =16$ and $s_1=0$, we vary $s_2= 4,8,12,16,20,24$.}\\
    \end{align*}
\item Varying Within-group correlation $\rho_1, \rho_2$. We fix $\gamma_k=0.1$ and vary within-group correlations $\rho_{1}=\rho_2 \in \{0, 0.1, \dots, 0.9\}$ for the following three choice of $s_0, s_1, s_2$.
\begin{align*}
        & s_0 =16,s_1=0,s_2=0;\\
        & s_0 =16,s_1=16,s_2=16;\\
        & s_0=16,s_1=0,s_2=16.
    \end{align*}

\item Varying between-group correlation $\gamma_1, \gamma_2$. We fix $\rho_k=0.5$ and vary correlation ratio $\gamma_1=\gamma_2 \in \{0,0.05,0.1,\dots,0.5\}$ for the following three choice of $s_0, s_1, s_2$. Then the between-group correlations are calculated as $\rho_k\gamma_k$.
\begin{align*}
        & s_0 =16,s_1=0,s_2=0;\\
        & s_0 =16,s_1=16,s_2=16;\\
        & s_0=16,s_1=0,s_2=16.
    \end{align*}
\end{itemize}

\subsection{binary}
 $\bmbeta^1,\bmbeta^2$ are generated the same way as the continuous setting. As a default, we choose $\alpha_1=1, \alpha_2=-1$. The amplitude of signals is chose as $A=3.5$.
 The coefficients for all dummy variables are the same. 
 
\subsection{mixed}
\end{comment}

\section{Additional simulation results} \label{app:sim-res}
In Figure \ref{fig:figure3} and Figure \ref{fig:figure4}, we show results for continuous setting with Scenario 1 (same strengths) and Scenario 2 (different signal strengths). 

For the continuous settings, when only simultaneous signals exist ($s_1=s_2=0$), three methods \textit{GS knockoffs}, \textit{pooling} and \textit{intersection} control FDR with similar powers for both the Scenario 1(same strengths) and the Scenario 2(different signal strengths) in general. As within group correlation increases, \textit{GS knockoff method} still has similar power with \textit{pooling} and is slightly higher than the \textit{intersection method} ($s_1=s_2=0$). When there exist non-mutual signals in one group ($s_1=0,s_2 \neq0$), \textit{pooling} methods fail to control FDR but the \textit{GS knockoffs} and \textit{intersection} still control FDR. When non-mutual signals exist in both groups ($s_1=s_2 \neq 0$), only \textit{GS knockoffs} can control FDR. The \textit{individual} method fails to control FDR across all the continuous settings. And the performance between scenario 1(same strengths) and scenario 2(different signal strengths) of our method are very similar, with all the FDR being controlled. 

Figure \ref{fig:figure5} shows the results for binary and mixed data settings with Scenario 2 (different signal strengths). The results are similar to Scenario 1 (same strengths). Our \textit{GS knockoffs} method controls FDR across all the binary and mixed settings. \textit{pooling} method can only control FDR when only simultaneous signals exist ($s_1=s_2=0$) but fail in ($s_1=0,s_2 \neq0$) and ($s_1=s_2 \neq0$) cases. The \textit{intersection} method fails to control FDR when non-mutual signals exist in both groups ($s_1=s_2 \neq 0$). In addition, when only simultaneous signals exist ($s_1=s_2=0$), our \textit{GS knockoffs} has similar power with \textit{pooling} and is slightly higher than \textit{intersection}. when there exist non-mutual signals in one group ($s_1=0,s_2 \neq0$), \textit{GS knockoffs} has similar power with \textit{intersection} and is slightly lower power than the \textit{pooling} method.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.31]{images/Continuous_s0_scale=0.6_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Continuous_rho_s1=0s2=0_scale=0.6_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Continuous_gamma_s1=0s2=0_scale=0.6_samesig=0.pdf}\\
    \includegraphics[scale=0.31]{images/Continuous_s2_scale=0.6_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Continuous_rho_s1=0s2=16_scale=0.6_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Continuous_gamma_s1=0s2=16_scale=0.6_samesig=0.pdf}
    \caption{The power and the FDR of generalized simultaneous, pooling, intersection and individual methods when varying sparsity level (left), within-group correlation (middle) and correlation ratio (right) in two cases: 1. $s_1=s_2=0$ (row 1); 2. $s_1=0, s_2 \neq 0$ (row 2)  for Scenario 2 (different signal strengths) under continuous data setting.}
    \label{fig:figure3}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.31]{images/Continuous_s0_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.31]{images/Continuous_s1_s2_scale=0.6_samesig=11.pdf}
    \includegraphics[scale=0.31]{images/Continuous_s2_scale=0.6_samesig=1.pdf}\\
    \includegraphics[scale=0.31]{images/Continuous_rho_s1=0s2=0_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.31]{images/Continuous_rho_s1=16s2=16_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.31]{images/Continuous_rho_s1=0s2=16_scale=0.6_samesig=1.pdf}\\
    \includegraphics[scale=0.31]{images/Continuous_gamma_s1=0s2=0_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.31]{images/Continuous_gamma_s1=16s2=16_scale=0.6_samesig=1.pdf}
    \includegraphics[scale=0.31]{images/Continuous_gamma_s1=0s2=16_scale=0.6_samesig=1.pdf}
    \caption{The power and the FDR of generalized simultaneous, pooling, intersection and individual methods when varying sparsity level (row 1), within-group correlation (row 2) and correlation ratio (row 3) in three cases: 1. $s_1=s_2=0$ (left); 2. $s_1=s_2 \neq 0$ (middle); 3. $s_1=0, s_2 \neq 0$ (right)  for Scenario 1 (same strengths) under continuous data setting.}
    \label{fig:figure4}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.31]{images/Binary_s0_scale=3.5_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Binary_s1_s2_scale=3.5_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Binary_s2_scale=3.5_samesig=0.pdf}\\
    \includegraphics[scale=0.31]{images/Mixed_s0_scale=2_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Mixed_s1_s2_scale=2_samesig=0.pdf}
    \includegraphics[scale=0.31]{images/Mixed_s2_scale=2_samesig=0.pdf}
    \caption{The power and the FDR of generalized simultaneous, pooling and intersection methods when varying $s_0$ (left), $s_1=s_2$ (middle), and $s_2$ (right) for Scenario 2 (different signal strengths) under binary (row1) and mixed (row2) data settings.}
    \label{fig:figure5}
\end{figure}

\section{Additional information for real data analysis} \label{app:real}
%{\color{red}[It can be useful to provide the levels of each categorical variable: added]}
The detailed cohort generating inclusion and exclusion steps are illustrated in Figure \ref{fig:figure6}.

There are 43 risk factors included in this analysis: demographic information include sex ("Male", "Felmale"), age at covid, race ("Hispanic or Latino Any Race", "Others"), indicators include tuberculosis, mild liver disease, moderate severe liver disease, thalassemia, rheumatologic disease, dementia, congestive heart failure, substanceuse disorder, kidney disease, malignant cancer, diabetes complication, cerebrovascular disease, peripheralvascular disease, heart failure, hemiplegia or paraplegia, psychosis, obesity, coronaryartery disease, systemic corticosteroids, depression, metastatic solid tumor cancers, HIV infection, chronic lung disease, peptic ulcer, sickle cell disease, myocardial infarction, diabetes uncomplicated, cardiomyopathies, hypertension, other immunocompromised, negative antibody, pulmonary embolism, tobacco smoker, solid organ or blood stem cell transplant, and some covid related information include number of covid vaccine dose, covid regimen corticosteroids ("Yes" or "No"), remdesivir usage during covid ("Yes" or "No"), extracorporeal membrane oxygenation (ECMO) machine usage during covid ("Yes" or "No"), covid associated hospitalization indicator ("Yes" or "No"), covid associated emergency department visit ("Yes" or "No"), and severity type ("Asymptomatic", "Mild", "Moderate", "Severe"). Those indicators indicate patients have been diagnosed with those diseases or symptoms before or on the day they were confirmed to get an acute covid infection (i.e. all the indicators are in two categories: "Yes", "No"). The full data dictionary can be found in \url{https://unite.nih.gov/workspace/report/ri.report.main.report.855e1f58-bf44-4343-9721-8b4c878154fe}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{images/flow_chart.png}
    \caption{Cohort construction for N3C Knowledge Store Shared Project.}
    \label{fig:figure6}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
