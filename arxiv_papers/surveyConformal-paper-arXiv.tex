\documentclass[10.5pt, letterpaper]{article}

\usepackage[letterpaper, portrait, margin=1.5in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage{amsmath}
\numberwithin{table}{section}
\numberwithin{figure}{section}
\numberwithin{equation}{section}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\newtheorem*{corollary1}{Corollary 1}
\newtheorem*{definition1}{Definition 1}
\newtheorem*{lemma1}{Lemma 1}
\newtheorem*{lemma3}{Lemma 3}
\newtheorem*{theorem2}{Theorem 2}


\begin{document}

\title{Design-based conformal prediction}
\author{Jerzy Wieczorek\footnote{Jerzy Wieczorek, Department of Statistics, Colby College, Waterville, Maine, USA. E-mail: jawieczo@colby.edu.}}

\maketitle

\doublespacing

\begin{abstract}
Conformal prediction is an assumption-lean approach to generating distribution-free prediction intervals or sets, for nearly arbitrary predictive models, with guaranteed finite-sample coverage.
Conformal methods are an active research topic in statistics and machine learning, but only recently have they been extended to non-exchangeable data. In this paper, we invite survey methodologists to begin using and contributing to conformal methods. We introduce how conformal prediction can be applied to data from several common complex sample survey designs, under a framework of design-based inference for a finite population, and we point out gaps where survey methodologists could fruitfully apply their expertise. Our simulations empirically bear out the theoretical guarantees of finite-sample coverage, and our real-data example demonstrates how conformal prediction can be applied to complex sample survey data in practice.
\end{abstract}

\paragraph{Keywords:}
Conformal prediction; Machine learning; Cross validation; Predictive modeling;
Complex sample survey designs.


\section{Introduction}\label{sec:Introduction}

% [TODO:
% Use same notation in the intuition subsecs.
% Is $n$ the calibration set size only, or the total (propertrain+calib) set size, or can it vary by context? I've decided $n$ SHOULD be EITHER calib-set only for split conf, OR full training set for full conf, but NOT split conf's combined prop.train+calib size.
% Be consistent about ``training'' vs ``proper training'' vs ``calibration.''
% When to use $\hat f$ vs $\hat\mu$?
% When to use $n \in 1,\ldots,N$ vs $n=1{:}N$?
% ``Conformal prediction'' vs ``conformal inference''?
% Also---explain in Sec 4 EXACTLY what we did for each conformal method in sims---e.g. the MEPS used weights but ignored clus/strat in the conf quantiles? But the API sims *did* use clus/strat approaches of Sec 3? Cite exactly which approach and which subsection when we describe sims.
% ]

\paragraph{What is conformal prediction?}
Conformal prediction, also called conformal inference, is a family of general-purpose approaches to constructing prediction intervals or prediction sets, which can be wrapped around almost any predictive modeling algorithm. Specifically, imagine that we wish to fit a model $\hat f_n$ to a set of training data $(X_i,Y_i)$ for $i=1,\ldots,n$, and then to report a level $1-\alpha$ prediction set for a new observation's response value $Y_{n+1}$ when $\hat f_n$ is evaluated at $X_{n+1}$. Let us require only that all $n+1$ of the points $(X_i,Y_i)$ are an
exchangeable\footnote{A sequence of random variables is exchangeable when its joint probability distribution would not change if you permuted the random variables. More precisely, a sequence $X_1,X_2,\ldots$ is exchangeable if for each $n$ and each permutation $\pi$ of $\{1,\ldots,n\}$, the joint distribution of $(X_{\pi(1)}, \ldots, X_{\pi(n)})$ is just the same as the distribution of $(X_1,\ldots,X_n)$ \citep{durrett2019probability}. Examples of exchangeability include iid sequences as well as simple random sampling (with or without replacement).}
sample from some common distribution $P$, and that the algorithm used to fit $\hat f_n$ treats these points symmetrically.
If so, we can use standard conformal prediction methods to construct $\hat C_n$ which are either prediction intervals (for a regression problem) or prediction sets (for a classification problem), such that $\mathbb{P}[Y_{n+1} \in \hat C_n(X_{n+1})] \geq 1-\alpha$,
where the probability is taken over repeated sampling of all $n+1$ points. In Section~\ref{sec:Intuition} we describe two standard approaches to conformal prediction and describe the intuition behind how they work.

\paragraph{Why is this interesting to survey methodologists?}
The principles behind conformal methods are a natural fit for the complex survey sampling toolbox. Notably, [a] conformal methods provide a finite-sample coverage guarantee, not an asymptotic or approximate guarantee.
Furthermore, unlike traditional model-based approaches (such as Gaussian-errors prediction intervals for regression), [b] conformal prediction does not require any assumptions about the distribution $P$---only an exchangeable sampling design.
Finally, [c] the guarantee holds even if the predictive model is not a good fit to $P$ (though of course the prediction intervals or sets may be larger then). We only need to treat each data point symmetrically, which rules out e.g.\ algorithms that give more weight to more-recent data over time.

These three features of conformal prediction have natural parallels in design-based inference for finite-population sampling. [a] Many fundamental methods in design-based inference provide probabilistic guarantees that are not asymptotic or approximate---they are exact finite-sample guarantees. [b] They also do not require assumptions about how the data values are distributed in the population---only knowledge of the sampling design. Finally, [c] many model-assisted methods in design-based inference have guarantees that hold whether or not the model is a good fit to the population.

From a pragmatic standpoint, users of survey data could apply conformal methods to provide design-based prediction intervals (or sets) for predictive regression (or classification) models built on survey data (Section~\ref{sec:Methods}). Such prediction intervals or sets would have guaranteed coverage---not only for traditional prediction models like linear regression, but for novel machine learning algorithms as well.
For instance, linear regression already has well-known prediction intervals when assuming Gaussian errors; and linear regression has already been adapted to account for complex survey designs; so conformal methods may add limited value there. But for newer methods---such as prediction algorithms that have not yet been adapted to work optimally with survey designs, or non-probabilistic algorithms that do not come with ``built-in'' prediction intervals---we can apply conformal prediction methods that guarantee coverage based on the sampling design alone.
Those who collect and pre-process the survey data could also find conformal methods useful, for instance in nonresponse prediction, imputation, or data cleaning (Section~\ref{sec:Extensions}).

Finally, we believe this research direction is a good opportunity for survey methodologists to contribute to the machine learning community. The popularity of conformal prediction indicates that machine learning practitioners are seeking methods that are guaranteed to work based only on the sampling design, not on the distribution of the data values. Survey methodologists have the right expertise to meet that demand.

\paragraph{Why is this not already in use for survey sampling?}
Despite the apparent affinities, conformal methods have not previously been studied under a complex sample survey framework. Conformal prediction has been under development for several decades, initially by Vladimir Vovk and colleagues \citep{vovk2005algorithmic} and more recently by a wide range of statistical and machine learning researchers. Conformal methods are starting to be deployed in real-world settings, such as the Washington Post's 2020 presidential election tracker, which reported conformal prediction intervals of the votes for each party that were updated in real time as voting districts slowly reported their results \citep{cherian2020washington}. Yet the requirement of exchangeability has made traditional conformal methods inapplicable for complex sampling designs other than simple random sampling (with or without replacement).

However, recently \cite{tibshirani2019conformal} extended conformal prediction to what they call ``weighted exchangeable'' sampling, while \cite{dunn2022distribution} derived several ``hierarchical'' conformal methods that can apply to cluster sampling. In the present work, we use these two papers as a foundation to provide conformal guarantees for more complex sampling designs.

Briefly, instead of assuming that all $n+1$ data points are exchangeable, we will assume that the $n$ training points came from a known complex sampling design on a finite population, and the test point was selected uniformly at random from the same finite population. In Section~\ref{sec:Methods}, we summarize the complex survey settings in which such conformal prediction methods are currently known to have coverage guarantees.


\subsection{Related work}

In survey sampling, the closest related topic is quantile estimation \citep{woodruff1952confidence}. However, this has typically been framed in terms of finding unbiased or low-variance estimates of the quantiles, rather than choosing a quantile estimate which guarantees exact finite-sample coverage as in conformal prediction.

More generally, prediction intervals or tolerance regions have a long history in statistics \citep{fraser1967nonparametric, guttman1970statistical}; for a recent review, see \cite{tian2022methods}. Pages 256--257 of \cite{vovk2005algorithmic} connect conformal methods to much of this earlier work. However, conformal prediction is currently a broad and very active research area, and we encourage interested readers to look into the following general resources: \cite{angelopoulos2022gentle} provide an introductory review article; Vovk and colleagues maintain a list of their own recent work at \url{http://alrw.net/}; \cite{manokhin2022awesome} maintains a GitHub repository frequently updated with new papers, tutorials, and software for conformal methods; and COPA, the Symposium on Conformal and Probabilistic Prediction with Applications, is an annual conference on the theory and practice of conformal prediction and its extensions, with a website at \url{https://copa-conference.com/}.



For dependent data, our paper builds largely on the work of \cite{dunn2022distribution} and \cite{tibshirani2019conformal}.
Besides the first paper, another approach to ``group conformal prediction'' for hierarchical data is found in \cite{fong2021conformal}, but their Bayesian setting is not a natural fit for design-based inference.
Besides the second paper, the same authors have contributed further work on conformal inference without exchangeability in \cite{barber2022conformal}. For instance, they relax the usual conformal-prediction requirement that the fitted prediction model must treat data points symmetrically. \cite{fannjiang2022conformal} address another kind of dependence: ``feedback covariate shift,'' where the choice of which test data to sample depends on results from the training data. This problem has affinities with adaptive sampling for surveys \citep{thompson1997adaptive}.


Time series methods typically address another kind of dependence: nonstationarity over time. Early work in conformal inference was designed for online prediction (where data are observed one at a time and each successive data point must be predicted before the next one is sampled) rather than for batch prediction (where the training data and test cases are all made available at once); but cases were usually assumed to be exchangeable, with no dependence over time \citep{vovk2005algorithmic}.
Recently, \cite{chernozhukov2018exact} and \cite{oliveira2022split} show that conformal methods based on block permutations and other data splitting can retain at least approximate validity for certain kinds of non-stationary time series. A related line of work is framed in terms of ``distribution drift,'' where the way that the test distribution differs from the training distribution changes slowly over time \citep{gibbs2021adaptive}.
While the time series setting differs from the dependence introduced by sampling designs, panel surveys are a common form of longitudinal data. Conformal methods that account for both sampling-design and time-series constraints have not yet been developed for panel data.

In terms of applying conformal methods to survey samples, \cite{bersson2022optimal} do use conformal prediction for small area estimation problems which rely on survey data. However, they work in a model-based framework and assume that sampled units are exchangeable within each small area, rather than focusing on design-based inference under a complex sampling design as we do in the present paper.
\cite{romano2019conformalized} and several followup papers \citep{sesia2020comparison, sesia2021conformal, feldman2021improving, bai2022efficient} illustrate conformal prediction methods using unit-level records from a complex survey dataset: the Medical Expenditure Panel Survey \citep{ahrq2017meps}. However, they appear to ignore the documented sampling design and treat it as exchangeable, counter to the design-based framework of the present paper.

Apart from conformal methods, there has been increasing interest in applying machine learning methods to complex survey data. In a high-dimensional setting, \cite{dagdoug2022model} summarize the current state of the art for using survey data with many machine learning algorithms: penalized regression \citep{mcconville2017model}, k nearest neighbors \citep{yang2019nearest}, random forests \citep{dagdoug2021model}, and others. However, they focus on using these algorithms in model-assisted estimators for means and totals, rather than on predicting individual response values. Similarly, \cite{sande2021design} show how to adapt such machine learning algorithms for design-unbiased estimation of means and totals.
Even though some of these algorithms have finally been adapted to make individual or mean predictions that account for the survey design, many of them still do not have built-in procedures for creating prediction intervals, which design-based conformal methods could provide.
Meanwhile, outside the world of survey methodology, machine learning practitioners may carry out complex sampling schemes without necessarily being aware of the literature on sampling designs. For instance, in training and evaluating a computer vision model for autonomous vehicle research, \cite{jund2021scalable} subsample a video dataset by object class (pedestrian, cyclist, or vehicle), and they subsample at the level of video segments rather than individual frames. This appears to be stratified cluster sampling, but their paper makes no such connection to the sampling design literature though it could perhaps be useful in deciding how much data to sample and how to analyze it.

Closely related is the issue of model assessment and model selection with survey data. \cite{lumley2015aic} derive a design-based AIC; \cite{holbrook2020estimating} use a parametric bootstrap to construct a design-consistent estimate of generalization error; and \cite{wieczorek2022kfold} adapt cross-validation to account for the survey design. However, each of these is focused on overall estimates of the prediction error---one estimate per model---rather than on prediction intervals for individual observations. In addition, \cite{holbrook2020estimating} allow the prediction model to be wrong but do require a correctly-specified model for the errors, while conformal methods need no model for the errors.


\subsection{Our contributions}

We introduce survey methodologists and statisticians to the key ideas behind conformal inference (Section~\ref{sec:Intuition}). We provide necessary definitions, and we share some of the intuition behind standard conformal methods under exchangeable sampling as well as behind their extension to complex sampling designs.
Furthermore, we derive exact, finite-sample, design-based coverage guarantees for applying conformal inference to data from several fundamental sampling designs, building on results from the weighted exchangeable or hierarchical frameworks (Section~\ref{sec:Methods}).

Next, we illustrate our design-based guarantees through simulations and a real-data example (Section~\ref{sec:Examples}). We use an extract from the Medical Expenditure Panel Survey or MEPS \citep{ahrq2017meps} to show that results can differ depending on whether or not conformal methods account for the survey design. We also simulate repeated sampling from a population of California schools using the Academic Performance Index or API dataset \citep{cdoe2018academic} to show how conformal methods are affected by different sampling designs.
We then discuss several practical considerations and future challenges to be addressed in applying conformal methods to survey data (Section~\ref{sec:Extensions}). We also suggest other ways that conformal methods might be useful to survey methodologists, such as in predicting nonresponse or generating synthetic microdata.

Finally, we invite survey researchers to contribute to the literature on conformal methods (Section~\ref{sec:Conclusion}). We anticipate that advances from the design-based perspective will turn out to be useful to the general community of conformal researchers.




\section{Introduction to conformal inference}\label{sec:Intuition}

\subsection{Definitions}

Because one of our main contributions is a corollary of results in \cite{tibshirani2019conformal}, we will restate some of their notation, definitions, and results here and in Section~\ref{sec:Methods}. We refer readers to their paper for proofs.

Let $\mathrm{Quantile}(\beta; F)$ denote the level $\beta$ quantile of distribution $F$, so that for $Y\sim F$,
\[\mathrm{Quantile}(\beta;F) = \inf \{y: \mathbb{P}\{Y \leq y\} \geq \beta\}. \]
We allow for distributions on the augmented real line, $\mathbb{R} \cup \{\infty\}$. We use $v_{1{:}n}=\{v_1,\ldots,v_n\}$ to denote a multiset, meaning that it is unordered and can allow the same element to appear several times. We use $\delta_a$ to denote a distribution that places all mass at the value $a$. If $v_{1{:}n}$ is an exchangeable sample, its empirical CDF is $n^{-1}\sum_{i=1}^n \delta_{v_i}$, and its level $\beta$ quantile is $\mathrm{Quantile}(\beta; v_{1{:}n})$ which is the $\lceil \beta n \rceil$ smallest value in $v_{1{:}n}$, where $\lceil \cdot \rceil$ is the ceiling function.

Now we can state the general-purpose ``quantile lemma'' that forms the foundation for conformal inference in the exchangeable setting. For instance, if we apply this lemma directly to data from an exchangeable sample of scalar random variables $V_1,\ldots,V_n$, we obtain a level $\beta$ one-sided prediction interval for a new observation $V_{n+1}$.

\begin{lemma1}[\cite{tibshirani2019conformal}]
If $V_1,\ldots, V_{n+1}$ are exchangeable random variables, then
for any $\beta\in(0,1)$, we have
\[\mathbb{P}\left\{ V_{n+1} \leq \mathrm{Quantile}\left(
\beta; V_{1{:}n} \cup {\infty}
\right) \right\} \geq \beta.\]
Furthermore, if ties between $V_1,\ldots, V_{n+1}$ occur with probability zero, then the above probability is upper bounded by $\beta + 1/(n+1)$.
\end{lemma1}


In order to use this lemma to do conformal prediction for nontrivial regression or classification problems, we must also choose a \emph{score function} $\mathcal{S}$, which takes the following arguments: a point $(x,y)$, and a multiset $Z$ (meaning that $\mathcal{S}$ must treat the points in $Z$ as unordered). The points in $Z$ will typically be the $n$ observations $Z_i=(X_i,Y_i)$ for $i=1,\ldots,n$, possibly along with one additional hypothetical observation; $(x,y)$ is typically one of the points in $Z$. $\mathcal{S}$ should return a real value, such that lower values indicate that $(x,y)$ ``conforms'' to $Z$ better.

For instance, in a regression context, we might choose
$\mathcal{S}\left((x,y),Z\right) = |y-\hat{f}(x)|$ where $\hat f$ is some regression function fitted using all of $Z$, including $(x,y)$. A small absolute residual like this suggests that $(x,y)$ conforms well to the overall trend in $Z$.

In a classification setting, we might choose $\mathcal{S}\left((x,y),Z\right) = 1-\hat{f}(x)_y$ where $\hat f$ is the probability of class $y$ estimated by some classification function fitted using all of $Z$, again including $(x,y)$. A small value of this score suggests that $(x,y)$ conforms well to $Z$, in the following sense: This observation with covariates $x$ has a high estimated probability of being in class $y$ based on trends in $Z$, and it does indeed belong to class $y$.

Finally, use $\mathcal S$ to define \emph{nonconformity scores}
\begin{equation}\label{eqn:Tibs3}
V_i^{(x,y)} = \mathcal{S}\left(Z_i,Z_{1{:}n} \cup \{(x,y)\}\right),\ i=1,\ldots,n,
\ \mathrm{and} \
V_{n+1}^{(x,y)} = \mathcal{S}\left((x,y), Z_{1{:}n}\cup\{(x,y)\}\right).
\end{equation}

We will also use following common abbreviations: SRS (simple random sampling), WR (with replacement), WOR (without replacement), PPS (probability proportional to size sampling). In Section~\ref{sec:SplitFull} we introduce a distinction between ``full conformal'' and ``split conformal'' approaches; for full conformal, we will use $n$ as the size of the training set, while for split conformal, $m$ will be the size of the proper training set and $n$ will be the size of the calibration set.




\subsection{Intuition under exchangeable sampling}\label{sec:IntuitionIid}

\paragraph{Quantile lemma:} The lower bound in Lemma~1 has a simple rationale. First, let $\hat q_{n+1}$ be the $\lceil \beta (n+1) \rceil$ smallest value in $V_{1{:}{(n+1)}}$, and let $\hat q_{conf}$ be the $\lceil \beta (n+1) \rceil$ smallest value in $V_{1{:}n}\cup\infty$.
By exchangeability, the rank of $V_{n+1}$ among all the $V_i$ is uniformly distributed over $\{1,\ldots,n+1\}$, so $V_{n+1}$ is at or below $\hat q_{n+1}$ with probability exactly $\beta$.
% If we swap out $V_{n+1}$ for a value larger than all of the $V_i$ (such as $\infty$), then the bound becomes conservative:
Then since $\hat q_{conf} \geq \hat q_{n+1}$, $V_{n+1}$ is at or below $\hat q_{conf}$ with probability \emph{at least} $\beta$. Note that these probabilities are marginal (over exchangeable sampling of all the $V_1,\ldots,V_{n+1}$ together)---not conditional on the first $n$ observations nor conditional on the test observation.

This is an exact finite-sample result that only relies on exchangeability. By contrast, if we used instead $\hat q_n$, the $\lceil \beta n \rceil$ smallest value in $V_{1{:}n}$, then it may be an asymptotically good estimate of the population quantile, but it would require stronger conditions on the data distribution. Even then, the probability that $V_{n+1}$ is at or below $\hat q_n$ would be only \emph{approximately} $\beta$.




% [TODO: Change $\beta$ to $\alpha$ throughout, or otherwise adjust to the fact that the orig Tibs paper used LOWER $\beta$ quantile to define ``Quantile()'' function.]
%
% First, we explore the intution behind a fundamental ``quantile lemma'' under the usual setup for conformal prediction. Imagine that we have drawn an exchangeable sample of $n$ real-valued observations from some population, and that we are also going to sample one additional observation (exchangeably). We want to choose a threshold $q$ such that this new observation will be above $q$ with probability $\beta$ or less. How can we choose $q$?
%
% If $q$ is the upper $\beta$ quantile of the full population, the probability that the new observation will be above it is by definition $\beta$, but typically this is an unknown population parameter. What about using the upper $\beta$ quantile of the first $n$ observations (rounding up to be conservative)? That is, what if we let $q$ be the $\lceil (1-\beta)n \rceil$ smallest of our first $n$ observations? The probability of this is not exactly $\beta$ in a finite sample, though under standard regularity conditions it should get asymptotically close as $n$ grows.
%
% However, let us pretend for a moment that we already had this last observation and we added it to the sample of our first $n$ observations. Then by symmetry, the probability that this last observation is among the top $\beta (n+1)$ of these observations is guaranteed to be exactly $\beta$ if $\beta (n+1)$ is an integer. Otherwise, we can round so that the probability is strictly less than $\beta$.
% % Now, we don't know what the $n+1$st observation's value is yet, so we can't find this quantile\ldots but in the ``worst case'' it would be the largest of these $n+1$ combined datapoints. If we assume it will be the largest, then the resulting quantile will be conservative. [[JUSTIFY WHY]]
% Concretely, if we have $n+1$ exchangeable observations, and we let $q_1$ be the $\lceil (1-\beta) (n+1) \rceil$ smallest of them, then there is a probability of exactly $\beta$ or less that the last of these observations is greater than $q_1$.
%
% The probability continues to be $\beta$ or less if we use any threshold $q$ that is larger than $q_1$. Now, if we have only seen the first $n$ observations, and we pretend the last one is $\infty$, and we let $q_2$ be the $\lceil (1-\beta) (n+1) \rceil$ smallest of this set instead, then $q_2 \geq q_1$. Hence, the probability that the last observation is greater than $q_2$ cannot be more than $\beta$, so we can safely use $q_2$ as our threshold $q$.


To see why $\hat q_{conf} \geq \hat q_{n+1}$, imagine an empirical CDF (eCDF) plot of just the first $n$ datapoints, with step heights of $1/n$ at each observed value.
(See the top subplot of Figure~\ref{fig:eCDFs-iid} for an illustration.)
If we add one more datapoint, the step heights of the eCDF will be $1/(n+1)$ instead, and the lower $\beta$ quantile will be $\hat q_{n+1}$ as above. It may be larger or smaller than the lower $\beta$ quantile of the first $n$ datapoints, depending on how large the last datapoint is. The lower $\beta$ quantile is largest if we choose a $(n+1)^{th}$ value that is larger than any of the first $n$ observed values, say at $\infty$. That is because if the added datapoint is larger than any of the others, it pushes the rest of the eCDF down, so the horizontal line with $y$-intercept of $\beta$ crosses the eCDF at an $x$ value further to the right. In this most extreme case, the lower $\beta$ quantile is $\hat q_{conf}$ as above, and therefore $\hat q_{conf} \geq \hat q_{n+1}$.
(See the bottom subplot of Figure~\ref{fig:eCDFs-iid}.)
By contrast, if instead of $\infty$ we had chosen to add a value smaller than some of the first $n$ observations, it would have pushed part of the eCDF upward and may have caused the lower $\beta$ quantile to become smaller than $\hat q_{n+1}$.

\begin{figure}[h!]
\includegraphics[width=0.6\textwidth]{eCDFs-iid}
\centering
\caption{Top: eCDF for an iid sample with values \{1,2,3,4\}. At each value there is a vertical jump of $\frac{1}{n}$=0.25. The 75th percentile is at 3. \\ Bottom: same eCDF but padded with an extra value at $\infty$. Now at each value there is a vertical jump of $\frac{1}{n+1}$=0.20. The 75th percentile is between 3 and 4, so we round up to 4.}
\label{fig:eCDFs-iid}
\end{figure}

Put another way, the probability that \{we take an exchangeable sample of size $n+1$ in which the last observation is greater than the $\lceil (1-\alpha)(n+1) \rceil$ smallest of the first $n$ observations\} is at most $\alpha$. This is not approximate or asymptotic, but a strict inequality that holds at any finite sample size---relying only on the assumption of exchangeable data.

\paragraph{Regression prediction intervals:} Next, we can apply this quantile lemma in a regression setting. (Later we describe an approach for classification problems.) Imagine that we are given a fixed regression model $\hat f$ for making predictions of some real-valued random variable $Y$ from some covariate vectors $X$ for data from this population. (For the moment, assume $\hat f$ is not data-dependent; perhaps it was chosen a priori.) Also imagine that we have some residuals from applying $\hat f$ to a ``calibration set''\footnote{We recognize that the term ``calibration'' traditionally has a completely different meaning in survey sampling. In this paper, we will use ``calibration'' only in this conformal inference sense.} of $n$ different data points $(X_1,Y_1),\ldots,(X_n,Y_n)$.
Finally, we have one other covariate vector $X_{n+1}$, drawn exchangeably from the same population, but we do not know its $Y$ value. We want to get a level $(1-\alpha)$ prediction interval for $Y_{n+1}$ at this test-case $X_{n+1}$.

We can simply apply the ``quantile lemma'' logic to the $n$ absolute residuals from the calibration set. These absolute residuals are our nonconformity scores. The probability is at least $1-\alpha$ that the distance between $Y_{n+1}$ and its regression prediction is no greater than the $\lceil(1-\alpha)(n+1)\rceil$ smallest absolute residual from the $n$ calibration cases.
So if we let $\hat q$ be the $\lceil(1-\alpha)(n+1)\rceil$ smallest absolute residual,
then $\hat C(X_{n+1}) = \hat f (X_{n+1}) \pm \hat q$ is a prediction interval for $Y_{n+1}$ with
guaranteed coverage of at least $1-\alpha$.


\paragraph{Marginal vs.\ conditional coverage:} This coverage is marginal across all samples of size $(n+1)$, with a size-$n$ calibration set plus one new test case. It is not conditional on the specific test case $X_{n+1}$ we chose; it assumes that both the calibration set and the test case together are exchangeable. Nonetheless, it is an exact finite-sample result, with almost no assumptions on the regression model or fitting algorithm---only on the sampling design (exchangeable). \cite{barber2021limits} show that although no conformal method could generally guarantee ``exact conditional coverage'' (conditioning on the exact value of $X$), certain relaxed versions of conditional coverage are achievable. \cite{angelopoulos2022gentle} review several approaches to assessing and controlling various forms of conditional coverage, noting that marginal coverage alone may be insufficient if happens to be achieved by, for instance, 0\% coverage in some rare but important subpopulations and 100\% coverage elsewhere.


\subsection{Intuition under complex sampling}\label{sec:IntuitionWtd}

How does this change when the data are not exchangeable?
\cite{tibshirani2019conformal} have extended conformal prediction to a setting called ``covariate shift,'' in which the distribution of $X$ is differerent for the training and/or calibration sets than for the test set, but the conditional distribution of $Y|X$ remains the same.
They define a condition called ``weighted exchangeability'' and show how to make conformal inference work for such data. In the present paper, we will show that certain classic finite-population sampling designs can be treated as a special case of covariate shift, and therefore the results of \cite{tibshirani2019conformal} apply to such sampling designs too.

Specifically, imagine we sample $n$ cases with replacement from a finite population with known but unequal sampling probabilities. For instance, we may be using probability proportional to size (PPS) sampling. Each unit in the population has a sampling probability and a corresponding inverse-probability sampling weight. Assume also that we take a SRS of just one test case from the full finite population; that is, we have equal probability $1/N$ of sampling any population unit. Now, we need a ``survey weighted quantile lemma'' that tells us how to find an adjusted quantile $\hat q$ of the complex sample, such that a test case nonconformity score is no larger than $\hat q$ with probability at least $1-\alpha$. We will plug the $n$ complex sample observations from our calibration set into a regression model $\hat f$ and find $\hat q$ for the absolute residuals. Then the probability of our test case $(X_{n+1},Y_{n+1})$ having a larger absolute residual is at most $\alpha$, and so $\hat f(X_{n+1}) \pm \hat q$ is a level $1-\alpha$ prediction interval for $Y_{n+1}$.

In Section~\ref{sec:Methods} we will show that in this unequal-probabilities setting, it is enough to mimic the exchangeable setting, except that instead of using an equal-weights eCDF to get the quantiles, we use a survey-weighted eCDF; see e.g.\ Section 5.11 of \cite{sarndal1992model}. The additional observation at $\infty$ will simply be assigned the known sampling weight that the test case $(X_{n+1},Y_{n+1})$ would have had, under the complex sampling design used to sample the first $n$ units.
See Figure~\ref{fig:eCDFs-survey} for an illustration.


\begin{figure}[h!]
\includegraphics[width=0.6\textwidth]{eCDFs-survey}
\centering
\caption{Top: eCDF for a survey sample with values \{1,2,3,4\} and corresponding survey weights \{4,3,2,1\}. At each value there is a vertical jump proportional to its survey weight. The 75th percentile is between 2 and 3, so we round up to 3. \\ Bottom: same eCDF but padded with an extra value at $\infty$, corresponding to a not-yet-observed unit in the population that would have a survey weight of 3 if it were to be sampled. The vertical jumps at each value are still proportional to the survey weights, but now rescaled to make room for the weight of the extra value at $\infty$. The 75th percentile is between 3 and 4, so we round up to 4.}
\label{fig:eCDFs-survey}
\end{figure}

We acknowledge that there is a long-running debate in the survey sampling literature about whether and how to use sampling weights for model fitting and inference \citep{pfeffermann1993role, fienberg2010relevance, lumley2017fitting}. From the model-based perspective, a well-specified model should remain unbiased but be more efficient without using the weights. Our goal in the present paper is not to take a stance in this debate, but simply to show how conformal inference can be applied \emph{if} the data analyst is taking a design-based perspective and using sampling weights. In a model-based analysis, if the sampling weights and other design features (such as clustering and stratification) can justifiably be ignored, standard conformal inference methods may be used.

% Also cite the papers referenced in \url{https://grbio.upc.edu/en/shared/material-seminaris/20210608_TALKAIparragirre.pdf} about design- vs model-based approaches. Obvs THIS paper DOES use survey weights. Will have to admit that some others would rather not---but OUR title is ``design-based conformal inference'' so naturally we're gonna focus on design-based here.



% \clearpage




\subsection{Split vs.\ full conformal}\label{sec:SplitFull}

Above, for simplicity we have assumed that a $\hat f$ has been provided for us. More typically, we will need to fit $\hat f$ using data from the sample at hand. In the ``split conformal'' approach \citep{lei2018distribution}, also called ``inductive conformal'' \citep{papadopoulos2002inductive}, we start with an exchangeable dataset of $m+n$ observations, and we split them at random into a ``proper training set'' of size $m$ to be used for training $\hat f$, plus a calibration set of size $n$ as described above. In this situation, coverage is still marginal over the calibration set plus one test case, but now it is conditional on the proper training set.

An optimal sample splitting ratio $m/n$ for split conformal is not known. Larger $m/n$ leads to better prediction function estimates $\hat f$, so the calibration set residuals and therefore the conformal prediction interval lengths should be smaller on average. On the other hand, larger $m/n$ also leads to a smaller calibration set and therefore more-variable estimates of their conformal quantiles, so the conformal prediction interval lengths will fluctuate more from sample to sample. We note that the optimal sample splitting ratio for conformal inference may differ from that for cross-validation \citep{shao1993linear, wieczorek2022model}. In cross-validation the test size must dominate the training size asymptotically to achieve consistent model selection, while conformal inference has guaranteed coverage at any sample size and any splitting ratio.

If we do not wish to lose statistical efficiency by splitting our data, we can use a more computationally-intensive ``full conformal'' approach \citep{vovk2005algorithmic}. In that approach, we no longer need a separate calibration set. Instead, we let $n$ denote the total number of our complete-data training cases $(X_i,Y_i)$ for $i=1,\ldots,n$. We also have one test case with only the covariates $X_{n+1}$ known. Then we repeat the following process:

\begin{itemize}
  \item Choose a hypothetical response value $y \in \mathbb{R}$.
  \item Fit a regression model $\hat f_y$ to an augmented dataset in which we pretend this $y$-value is correct: $(X_1,Y_1),\ldots,(X_n,Y_n),(X_{n+1},y)$.
  \item Find the set of $n+1$ nonconformity scores, such as the absolute residuals: $R_{y,i} = |Y_i-\hat f_y(X_i)|$ for $i=1,\ldots,n$ and  $R_{y,n+1}=|y-\hat f_y(X_{n+1})|$.
  \item Find the $1-\alpha$ quantile of these residuals: $\hat q_y$ is the $\lceil (1-\alpha)(n+1) \rceil$ smallest value of $R_{y,i}$ for $i=1,\ldots,n+1$.
  \item If $R_{y,n+1} \leq \hat q_y$, we say $y$ ``conforms'' to the rest of the data, and we add $y$ to our prediction interval.
\end{itemize}
Repeat this process for many $y$ values. In practice, we typically choose a grid of reasonable candidate $y$ values, and the range from the smallest to largest ``conforming'' $y$ values is reported as the prediction interval. One way to choose this grid could be to use split conformal as an initial estimate of the prediction interval endpoints, then search the $y$ values around each endpoint.

In terms of notation, refer back to \eqref{eqn:Tibs3}. Full conformal nonconformity scores are calculated using $Z = Z_{1{:}n} \cup \{(X_{n+1},y)\}$ and refitting a new prediction model $\hat f_y$ for each new $y$ or $X_{n+1}$. By contrast, split conformal nonconformity scores use a fixed $\hat f$ conditional on the proper training set. Each of the first $n$ nonconformity scores is calculated only using that data point $(X_i,Y_i)$ and the fixed $\hat f$, and there is no need to calculate a $(n+1)^{th}$ nonconformity score. Under the notation in \eqref{eqn:Tibs3}, we allow $Z$ to be ignored and $V_{n+1}^{(x,y)}$ to be undefined for split conformal.


The full conformal procedure is more statistically efficient than split conformal: full conformal uses all training observations to estimate each prediction interval endpoint, which reduces the variance in the reported endpoints. Split conformal separates out the ``proper training set'' and only uses a smaller calibration subsample to estimate these endpoints.

On the other hand, full conformal is far more computationally intensive than split conformal, for two reasons: $\hat f_y$ has to be refit for every candidate $y$ at a given test point $X_{n+1}$, and the entire interval needs to be refit for every test point.
Split conformal only fits $\hat f$ once, on the proper training set; and the conformal quantile from the calibration set can be applied to any new test point $X_{n+1}$ without refitting anything.

In empirical comparisons, split conformal and full conformal often produce very similar prediction intervals \citep{lei2018distribution}, so we typically recommend split conformal because it is simpler to implement in code and faster to compute.

Other variants besides split and full conformal have been developed, such as cross-conformal prediction \citep{vovk2015cross} or CV+ and Jackknife+ \citep{barber2021predictive}, each with their own statistical guarantees and computational properties.
% [?? Is Jackknife really the same as LOOCV? I am certain it isn't, but then why does Barber seem to think so?!?]


\subsection{Classification problems and prediction sets}

The present paper is focused on regression prediction intervals, but classification prediction sets are another common use case.
% Then it'll produce a set of plausible classes for a given test case, instead of an interval of plausible y-values for a test case. In the case of binary classification this might just be \{0,1\} for lots and lots of test cases, so not very useful; it's more useful for multiclass situations, like deep learning's 100-class image classification? But in any case, mention BRIEFLY and then cite other papers where folks could read more on that topic. \cite{vovk2005algorithmic} Section 4.2, page 106, describes a nonconformity score for logistic regression as $-\hat X\beta$ if $y=1$ and $\hat X\beta$ if $y=0$. So in other words, larger (more-positive) scores when the direction of prediction doesn't match true label. HOWEVER this seems to be for what Ryan calls ``full conformal'' and not for split conformal...]
% [See Ryan's slide deck, slide 39:
For instance, consider split conformal inference for a multi-class probabilistic classifier. First, fit the classifier to the proper training set to get a model $\hat f(x)_y$, whose outputs are the estimated probabilitity of class $y$ for an input $x$. Examples of such classifiers range from simple logistic regression to deep neural networks. Then, using the calibration set, each calibration ``residual'' or nonconformity score $V_i$ is calculated as $1-\hat{f}(X_i)_{Y_i}$ for each calibration observation $i=1,\ldots,n$.
Next, find the corrected $1-\alpha$ quantile of these probabilities: let $\hat q$ be the $\lceil (1-\alpha)(n+1)\rceil$ smallest value of $V_i$ for $i=1,\ldots,n$.
Finally, for a new test case $X_{n+1}$, find its estimated probability for each class $y$, and choose all the classes whose nonconformity scores are below the corrected quantile:
\[ \hat{C}(X_{n+1}) = \{y: 1-\hat f(X_{n+1})_y \leq \hat q \}. \]

Even in this simple approach, the prediction sets are adaptive: for harder test cases where $\hat f$ is more uncertain about the right class, the prediction sets will be larger than for easy test cases where $\hat f$ confidently assigns most of the probability to just one. \cite{romano2020classification} and \cite{angelopoulos2022gentle} discuss this approach and several refinements.







\section{Methods}\label{sec:Methods}

For the reader's convenience, we restate several of the key results from \cite{tibshirani2019conformal} below. Next we prove that we can apply these results to unequal-probability sampling with replacement. We follow by discussing methods for sampling without replacement, cluster sampling based on \cite{dunn2022distribution}, stratified sampling, and post-stratification.

Most of the methods below apply equally well to full or split conformal. However, for split conformal, when we are splitting survey data into a proper training set and a calibration set, we recommend using the design-based approach to data splitting in \cite{wieczorek2022kfold}. If the design was stratified, allocate units to each subset independently within each stratum. If the design involved clustering, allocate entire clusters to each subset---do not split up a cluster. Finally, if units within a stratum were sampled with unequal probabilities, allocate them to each subset at random, but make sure each observation keeps its original inverse-probability sampling weights.
By this approach, both the proper training set and the calibration set will mimic the original sampling design, which ensures that the methods below are safe to apply to your calibration set. By contrast, a simple random split can cause the calibration set to have different properties than a clustered or stratified sampling design.


\subsection{Previous results for covariate shift  \citep{tibshirani2019conformal}}\label{sec:CovShift}

Here we restate earlier key results. In subsequent sections, we will apply these results to the case of survey sampling.

For the ``covariate shift'' setting, we assume that
\begin{align}\label{eqn:Tibs5}
(X_i, Y_i) &\stackrel{iid}{\sim} P = P_X \times P_{Y|X},\ i=1,\ldots,n, \nonumber \\
(X_{n+1}, Y_{n+1}) &\sim \tilde{P} = \tilde{P}_X \times P_{Y|X},\ \textrm{independently.}
\end{align}
Note that the conditional distribution of $Y|X$ remains the same as the marginal distribution of $X$ changes.

Assuming that $P_X$ and $\tilde{P}_X$ are known, we can define likelihood ratio weight functions $w(X_i) = \mathrm{d}\tilde{P}_X(X_i) / \mathrm{d}P_X(X_i)$.
We use these to define a second set of weights:
\begin{equation}\label{eqn:Tibs6}
p_i^w(x) = \frac{w(X_i)}{\sum_{j=1}^n w(X_j) + w(x)},\ i=1,\ldots,n,
\quad \mathrm{and} \quad
p_{n+1}^w(x) = \frac{w(x)}{\sum_{j=1}^n w(X_j) + w(x)}.
\end{equation}

In this setting, we can state a weighted counterpart to Lemma~1, the exchangeable quantile lemma.
Although \cite{tibshirani2019conformal} give and prove a more general version, for simplicity here we only state a version tailored to the covariate shift setting above.

\setcounter{lemma}{1}
\begin{lemma}\label{lem:weightedquantile}
Assume data from the model~\eqref{eqn:Tibs5}.
Assume $\tilde{P}_X$ is absolutely continuous with respect to ${P}_X$, and denote $w = \mathrm{d}\tilde{P}_X/\mathrm{d}P_X$.
For any $\beta\in(0,1)$,
\[\mathbb{P}\left\{ V_{n+1} \leq \mathrm{Quantile}\left(
\beta; \sum_{i=1}^n p_i^w(x) \delta_{V_i} + p_{n+1}^w(x) \delta_\infty
\right) \right\} \geq \beta,\]
where $V_i^{(x,y)}$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs3}, and $p_i^w$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs6}.
\end{lemma}
\begin{proof}
Apply Lemma~3 of \cite{tibshirani2019conformal} in the covariate shift setting of \eqref{eqn:Tibs5}.
\end{proof}

Now we can use this weighted quantile lemma to carry out full or split conformal inference in the covariate shift setting.

\begin{corollary1}[\cite{tibshirani2019conformal}]
Assume data from the model~\eqref{eqn:Tibs5}.
Assume $\tilde{P}_X$ is absolutely continuous with respect to ${P}_X$, and denote $w = \mathrm{d}\tilde{P}_X/\mathrm{d}P_X$.
For any score function $\mathcal{S}$, and any $\alpha\in(0,1)$, define for $x \in \mathbb{R}^d$,
\begin{equation*}\label{eqn:Tibs7}
\hat{C}_n(x) = \biggl\{ y\in\mathbb{R}:V_{n+1}^{(x,y)} \leq \mathrm{Quantile} \biggl( 1-\alpha; \sum_{i=1}^n p_i^w(x) \delta_{V_i^{(x,y)}} + p_{n+1}^w(x) \delta_\infty \biggr) \biggr\},
\end{equation*}
where $V_i^{(x,y)}$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs3}, and $p_i^w$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs6}.
Then $\hat{C}_n$ satisfies
\[\mathbb{P}\left\{ Y_{n+1} \in \hat{C}_n(X_{n+1})\right\} \geq 1-\alpha.\]
\end{corollary1}

This is the weighted analogue of the ``full conformal'' approach. For the ``split conformal'' approach described earlier, we restate part of Section~A.3 from the supplement to \cite{tibshirani2019conformal}. Let $(X_1^0,Y_1^0),\ldots,(X_{m}^0,Y_{m}^0)$ be a proper training set of size $m$, used for fitting the regression function $\hat f_0$. Also let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be the calibration set of size $n$ and let $(X_{n+1},Y_{n+1})$ be the test case. Then weighted split conformal prediction is a special case of Corollary~1
%of \cite{tibshirani2019conformal}
in which $\hat f_0$ is treated as fixed, and the prediction interval simplifies to
\begin{equation*}
\hat{C}_n(x) = \hat f_0(x) \pm \mathrm{Quantile} \biggl( 1-\alpha; \sum_{i=1}^n p_i^w(x) \delta_{|Y_i-\hat f_0(X_i)|} +
p_{n+1}^w(x) \delta_\infty \biggr),
\end{equation*}
with weights $p_i^w$ as defined in \eqref{eqn:Tibs6}. By Corollary~1, this has coverage at least $1-\alpha$ conditional on the proper training set $(X_1^0,Y_1^0),\ldots,(X_{m}^0,Y_{m}^0)$.

Finally, Remark 3 of \cite{tibshirani2019conformal} notes that the results above still hold if the likelihood ratio weights have an unknown normalization constant, i.e.\ if $w \propto \mathrm{d}\tilde{P}_X/\mathrm{d}P_X$, because this constant cancels out in the final weights in \eqref{eqn:Tibs6}.






% \begin{definition1}
% \citep{tibshirani2019conformal}.
% We call random variables $V_1,\ldots,V_n$ \emph{weighted exchangeable}, with weight functions $w_1,\ldots,w_n$, if the density $f$ of their joint distribution can be factorized as
% \[f(v_1,\ldots,v_n) = \prod_{i=1}^n w_i(v_i) \cdot g(v_1,\ldots,v_n),\]
% where $g$ does not depend on the ordering of its inputs, i.e., $g(v_{\sigma(1)},\ldots,v_{\sigma(n)}) = g(v_1,\ldots,v_n)$ for any permutation $\sigma$ of $1,\ldots,n$.
% \end{definition1}

% \begin{lemma3}
% \citep{tibshirani2019conformal}.
% Let $Z_i$, $i=1\ldots,n+1$ be weighted exchangeable, with weight functions $w_1,\ldots,w_{n+1}$. Let $V_i=\mathcal{S}(Z_i,Z_{1{:}(n+1)})$, for $i=1,\ldots,n+1$, and $\mathcal{S}$ is an arbitrary score function. Define
% \begin{equation}\label{eqn:Tibs10}
% p_i^w(z_1,\ldots,z_{n+1}) =
% \frac{\sum_{\sigma:\sigma(n+1)=i} \prod_{j=1}^{n+1} w_j(z_{\sigma(j)})}{\sum_\sigma \prod_{j=1}^{n+1} w_j(z_{\sigma(j)})},
% \ i=1,\ldots,n+1,
% \end{equation}
% where the summations are taken over permutations $\sigma$ of the numbers $1,\ldots,n+1$.
% Then for any $\beta \in (0,1)$,
% \[\mathbb{P}\left\{ V_{n+1} \leq \mathrm{Quantile}\left(
% \beta; \sum_{i=1}^n p_i^w(Z_1,\ldots,Z_{n+1}) \delta_{V_i} + p_{n+1}^w(Z_1,\ldots,Z_{n+1}) \delta_\infty
% \right) \right\} \geq \beta.\]
% \end{lemma3}

% This is the weighted analogue of the ``quantile lemma'' informally introduced in Section~\ref{sec:IntuitionIid}.

% \begin{theorem2}
% \citep{tibshirani2019conformal}.
% Assume that $Z_i = (X_i,Y_i) \in \mathbb{R}^d \times \mathbb{R}$, $i=1,\ldots,n+1$ are weighted exchangeable with weight functions $w_1,\ldots,w_{n+1}$. For any score function $\mathcal{S}$, and any $\alpha\in(0,1)$, define the weighted conformal band (based on the first $n$ samples) at a point $x \in \mathbb{R}^d$ by
% \begin{align*}
% \hat{C}_n(x) = \biggl\{ y\in\mathbb{R}:V_{n+1}^{(x,y)} \leq \mathrm{Quantile} \biggl( 1-\alpha; \sum_{i=1}^n &p_i^w(Z_1,\ldots,Z_n,(x,y)) \delta_{V_i^{(x,y)}}\ + \\
% &p_{n+1}^w(Z_1,\ldots,Z_n,(x,y)) \delta_\infty \biggr) \biggr\},
% \end{align*}
% where $V_i^{(x,y)}$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs3}, and $p_i^w$, $i=1,\ldots,n+1$ are as defined in \eqref{eqn:Tibs10}.
% Then $\hat{C}_n$ satisfies
% \[\mathbb{P}\left\{ Y_{n+1} \in \hat{C}_n(X_{n+1})\right\} \geq 1-\alpha.\]
% \end{theorem2}

% This is the weighted analogue of the ``full conformal'' approach. For the ``split conformal'' approach described earlier, we restate part of Section~A.3 from the supplement to \cite{tibshirani2019conformal}. Let $(X_1^0,Y_1^0),\ldots,(X_{n_0}^0,Y_{n_0}^0)$ be a training set of size $n_0$, used for fitting the regression function $\mu_0$. Also let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be the calibration set of size $n$ and let $(X_{n+1},Y_{n+1})$ be the test case. Then weighted split conformal prediction is a special case of Theorem~2
% %of \cite{tibshirani2019conformal}
% in which the prediction interval simplifies to
% \begin{align*}
% \hat{C}_n(x) = \mu_0(x) \pm \mathrm{Quantile} \biggl( 1-\alpha; \sum_{i=1}^n &p_i^w(Z_1,\ldots,Z_n,(x,y)) \delta_{|Y_i-\mu_o(X_i)|}\ + \\
% &p_{n+1}^w(Z_1,\ldots,Z_n,(x,y)) \delta_\infty \biggr),
% \end{align*}
% with probabilities as defined in \eqref{eqn:Tibs10}. By Theorem~2, this has coverage at least $1-\alpha$ conditional on the training set $(X_1^0,Y_1^0),\ldots,(X_{n_0}^0,Y_{n_0}^0)$.



\subsection{Unequal probability sampling with replacement}\label{sec:PPS}

Now, consider independently sampling \emph{with replacement} from a finite
population of size $N$, with unequal but known nonzero sampling probabilities $\pi_i$ for each population unit $i$.
We assume no stratification, clustering, or other design constraints.

For example, consider probability proportional to size (PPS) sampling. We may have an auxiliary variable $A$ that is available for every unit from some non-survey data source, such as tax records that list the number of employees in every company in our target population. But a survey may still be needed to learn about some other variable $Y$ that is not in the auxiliary records.
In this case, we can set sampling probabilities proportional to $A_i$, and these probabilities would be known for all population units $i=1,\ldots,N$ before the survey is carried out.

Our goal is to use this survey sampling design to get prediction intervals that usually cover $Y_i$ for most of the $N$ units in the finite population.

Below, we show that this is a special case of the covariate shift setting of \cite{tibshirani2019conformal}. The distribution $P_X$ corresponds to the sampling probabilities for each population unit, and $\tilde{P}_X$ becomes a uniform distribution over the same population units.

\begin{lemma}\label{lem:PPSWR}
Assume our ``training sample'' is a sample with replacement of size $n$ where each unit is drawn independently from a finite population of size $N$, with possibly-unequal but nonzero and known sampling probabilities $\pi_i$ for each population unit $i \in 1,\ldots,N$. Also assume our ``test case'' is a single observation sampled uniformly at random from the population. Then this is a special case of the covariate shift setting defined in \eqref{eqn:Tibs5}.
\begin{itemize}
  \item Let $J = \{1,\ldots,N\}$ be the vector of distinct unit IDs for the finite population. When we use $i$ to index our training sample and test units for $i=1,\ldots,(n+1)$, then $J_i$ tells us the population unit ID of the $i^{th}$ unit in our sample. (For example, $J_1=4$ would mean that the first unit we sampled was the fourth member of the population.)
  \item Let our finite population consist of $N$ units, each with its own fixed data vector $(X_i,Y_i)$ for $i=1,\ldots,n$. The covariate data $X_i$ for unit $i$ might be a single value or a vector; it might contain auxiliary variables such as $A_i$ or not; but in any case, for sampling purposes we treat $J_i$ as one of the covariates available in $X_i$.
  \item The training distribution $P_X$ corresponds to our complex survey design, which we can treat as $n$ iid draws from a multinomial distribution whose categories are simply the unit IDs $J=\{1,\ldots,N\}$, with known multinomial probabilities $\pi_{J_i}$.
  \item The ``covariate-shifted'' test distribution $\tilde{P}_X$ consists of just one draw from the same set of unit IDs as the training distribution, but with uniform probability: let $\tilde{\pi}_{J_i} = 1/N$ for all $i=1,\ldots,N$.
  \item The likelihood ratio weighting function has the simple form $w(X_i) = 1/\pi_{J_i}$.
\end{itemize}
\end{lemma}
\begin{proof}
In the training data sampling design, units are sampled with replacement from a fixed population, and thus they are independently and identically distributed. Even though individual population units have different sampling probabilities, each unit in a training sample is drawn from the same multinomial distribution. Also, because each $Y_i$ is paired with a fixed $X_i$ for all $i=1,\ldots,N$, the conditional distribution $P_{Y|X}$ is the same for training and test data.
Hence, these training and test distributions match the requirements in \eqref{eqn:Tibs5}.

Furthermore, since $\pi_i$ is nonzero for all population units, we have $w(X_i) = \mathrm{d}\tilde{P}_X(X_i) / \mathrm{d}P_X(X_i) = (1/N) / \pi_{J_i} \propto 1/\pi_{J_i}$. By Remark~3 of \cite{tibshirani2019conformal}, it is safe to ignore the normalization constant and set $w(X_i) = 1/\pi_{J_i}$ directly.
\end{proof}

\setcounter{corollary}{1}
\begin{corollary}
Assume that $n$ training cases and one test case are drawn as described in Lemma~\ref{lem:PPSWR}. Then the weighted full conformal results from Corollary~1 and the weighted split conformal results from Section~A.3 of \cite{tibshirani2019conformal} hold, with $p_i^w$ defined by using the inverse-probability sampling weights $1/\pi_{J_i}$ for the likelihood ratio weight function $w$ in \eqref{eqn:Tibs6}.
\end{corollary}
\begin{proof}
In this setting, ${P}_X$ and $\tilde{P}_X$ are discrete distributions with identical support, so $\tilde{P}_X$ is absolutely continuous with respect to ${P}_X$. All other conditions of Corollary~1 and Section~A.3 of \cite{tibshirani2019conformal} are met by Lemma~\ref{lem:PPSWR}.
\end{proof}

In other words, we can carry out conformal inference as usual, but replacing the exchangeable eCDF with a survey-weighted eCDF in which the first $n$ observations (the training sample) have standard inverse-probability sampling weights. The $(n+1)^{th}$ sample (the test case) is assigned the known sampling weight that population unit $J_{n+1}$ would have had under the original sampling design. Even though the test case is actually sampled uniformly at random, we assume we know what the complex-design sampling weight would have been for every unit in the population.

The resulting weighted ``full conformal'' prediction intervals have guaranteed marginal coverage over repeated sampling under the following procedure:
``Take an unequal-probabilities sample with replacement of size $n$ from the population $\{1,\ldots,N\}$ with these known probabilities, then take one test case uniformly from the same population.''

For ``split conformal,'' the resulting prediction intervals have guaranteed coverage over repeated sampling of the calibration set and the test case, conditional on the proper training set. In practice, we will typically collect one combined sample of size $m+n$ under the complex sampling design, so then we will need to partition it somehow into a proper training set of size $m$ and a calibration set of size $n$. We recommend using a simple random split (not using the sampling probabilities to perform the split). This way, the two resulting subsamples have the same properties as the original sample except for the smaller sample size; see Section~2.2.1 of \cite{wieczorek2022kfold}.

\subsection{Unequal probability sampling without replacement}

Simple random sampling without replacement is exchangeable, so the usual conformal methods apply directly.

But other kinds of sampling \emph{without} replacement are not a special case of the covariate shift setting above, because the training data are no longer independent. While \cite{tibshirani2019conformal} do provide more general results under a relaxed condition they call ``weighted exchangeability,'' it is not immediately clear that this condition can account for sampling without replacement.
% It may turn out that similar arguments to those of \cite{tibshirani2019conformal} can work for sampling without replacement.

However, if $n \ll N$, then statistical properties derived under sampling with replacement are often fairly good approximations to the actual properties under assuming sampling without replacement. Our simulations in Section~\ref{sec:Sims}, in which we do sample without replacement, suggest that this is likely to hold true for conformal methods as well.




\subsection{Cluster sampling}\label{sec:Cluster}

Much like for sampling without replacement, we cannot apply the covariate shift results to cluster sampling, because the data are not independent. Cluster sampling with unequal probabilities will require further research.

However, in the special case where the clusters themselves are sampled by SRS and the ultimate units are sampled by SRS within each cluster, then we can apply the methods of \cite{dunn2022distribution}. Their paper is framed in terms of a more general two-layer hierarchical setting. They do not explicitly consider a finite-population setting, but their assumptions do allow for it (except for some restrictions in their CDF pooling method).

In the framework of \cite{dunn2022distribution}, let $P_1,\ldots,P_k \sim \Pi$ be $k$ random distributions drawn iid from $\Pi$. From each of the sampled distributions $P_j$ for $j=1,\ldots,k$, we draw $n_j$ iid observations $(X_{j1}, Y_{j1}),\ldots,(X_{jn_j}, Y_{jn_j})$.

The corresponding setup in survey sampling would be cluster sampling, where our finite population of size $N$ is partitioned into a fixed number $K$ of clusters, also called Primary Sampling Units (PSUs), and we take a sample of these clusters. Simple random sampling with replacement (SRSWR) from a finite population is a special case of iid sampling. Hence, if we take a SRSWR of $k<K$ clusters $P_1,\ldots,P_k$ from the finite population, and then take a SRSWR of $n_j$ ultimate units from cluster $j$ for each $j=1,\ldots,k$, then this is a special case of the setup above, and we can apply most of the results in \cite{dunn2022distribution}. We outline their approaches briefly here, but refer readers to their full paper for details.

\paragraph{Prediction for an observed cluster:} If we only need a prediction interval for new observations from one of the clusters we already sampled, we can simply apply standard conformal methods by only using that cluster's data, which will be exchangeable.

\paragraph{Double conformal:} For unsupervised prediction (where we only want a prediction interval for $Y$ without conditioning on covariates $X$) for a new cluster, one approach is to create conformal prediction intervals separately within each cluster, then combine their endpoints in some appropriate way to form a single interval across clusters. This idea lines up with the design-based spirit: construct valid estimates within each cluster, then combine them sensibly across clusters. \cite{dunn2022distribution} derive a method like this that is guaranteed to have coverage at least $1-\alpha$. However, in simulations it appears to overcover, with coverage of nearly 1 and wider intervals than the other approaches below.

\paragraph{Subsampling:} By subsampling, we can change the sampling design and force it to become exchangeable. If we start with the design above, but then subsample our dataset by choosing just one unit at random from each cluster, then any test case from any new cluster is exchangeable with our subsample. In other words, we treat each of the $k$ subsampled training cases and the one test case as being generated exchangeably by the following process: ``Take a cluster at random, then take one observation at random from that cluster.''

By subsampling like this once, we get a single exchangeable dataset, on which it is valid to use standard conformal methods. However, although this process guarantees exact marginal coverage $1-\alpha$ across training sets, it ignores most of the data and leads to wider variability in achieved coverage from training set to training set. An alternative is to carry out repeated subsampling $B$ times and combine the results appropriately across subsamples. \cite{dunn2022distribution} show how to do this both for supervised and unsupervised prediction problems in a way that is guaranteed to have coverage of $1-2\alpha$, but in practice tends to achieve coverage close to $1-\alpha$.

\paragraph{Pooling CDFs:} Instead of combining conformal interval endpoints across clusters, or subsampling from each cluster, we could first construct eCDFs within each cluster and average them together into one pooled eCDF. Then we could apply standard conformal methods using this pooled eCDF. \cite{dunn2022distribution} only prove that this is asymptotically valid, requiring a continuous distribution for $Y$ as well as a growing number of sampled clusters $k \rightarrow \infty$. This is not possible in the traditional design-based setting of a fixed finite population, although we could construct a superpopulation model and a sequence of growing finite populations that satisfies their requirements.

In simulations, \cite{dunn2022distribution} find that CDF pooling tends to have coverage closest to nominal as well as shortest length of prediction intervals across most settings. However, if we do not wish to rely on asymptotic arguments and continuous $Y$ data, the repeated subsampling approach appears to work better than single subsampling or the double conformal method.

\paragraph{Possible extensions:} Although \cite{dunn2022distribution} state their results in terms of iid sampling, it seems likely that some of them could be relaxed to exchangeable sampling of the distributions $P_j$ as well as exchangeable sampling within each $P_j$. If so, these results would also apply to simple random sampling \emph{without} replacement (SRSWOR), not just SRSWR. Similarly, we conjecture that some of their results (such as CDF pooling) could be extended to the ``weighted exchangeable'' setting of \cite{tibshirani2019conformal}.



\subsection{Stratified sampling}\label{sec:Strata}

Once again, we cannot apply the covariate shift results to stratified sampling, because the data are not independent. Although strata are independent of each other, the combined set of $n$ samples is not independent.

However, we can safely apply conformal methods within each stratum separately. This only requires that, within each stratum independently, we have used one of the sampling methods with conformal guarantees: SRSWOR, SRSWR, unequal probability sampling WR, or equal-probability cluster sampling. In other words, if the full population is partitioned into $H$ strata, we can treat each stratum $h=1,\ldots,H$ as its own population. Then, when we wish to form a prediction interval for a test case from stratum $h$, we apply the results from previous subsections to only the $n_h$ training cases from that stratum. Clearly this will guarantee conditional coverage by stratum. If the same coverage level is used simultaneously across all strata, it will also guarantee marginal coverage. This is an example of ``group-balanced conformal prediction'' or ``attribute-conditional Mondrian conformal prediction'' \citep{vovk2005algorithmic, vovk2013conditional, angelopoulos2022gentle}, although in earlier work typically the data are assumed to be exchangeable. In these earlier settings, the groups are based on some attribute of the data (often a covariate, but sometimes the label itself in ``class-conditional conformal prediction'' for classification problems); but the groups have not been used to impose dependence in the sampling process the way that strata do.

We may worry that this stratum-by-stratum approach will lead to a loss of statistical efficiency, since each stratum's conformal quantiles will be estimated using a sample size $n_h$ that may be much smaller than $n$. On the other hand, a common use case for stratified sampling is when we expect units to have substantially different distributions for the data in each stratum. Data may be more variable or otherwise harder to predict in some strata than in others. In such situations, it may be reasonable to expect that prediction intervals will be more useful if we allow their sizes to vary by stratum than if their size has to be constant across strata. Further, guaranteeing coverage conditional on stratum may be more desirable than only guaranteeing marginal coverage, which could be achieved by overcoverage in some strata at the expense of undercoverage in others.

Additionally, sampling designs often call for large sample sizes in each stratum, in order to have adequate precision for reporting means or totals by stratum. In such cases, each stratum is more likely to have enough training data for conformal prediction as well.


\subsection{Post-stratification}\label{sec:Poststrat}

In the simplest case, imagine our first $n$ observations were drawn using a SRSWR sampling design, but we wish to post-stratify after data collection. Perhaps some stratum-like variable was not available before sampling, but we have that variable for all survey respondents, and the population size $N_h$ of each post-stratum is known. Then we can reweight each sampled observation by the relative stratum sizes: unit-level post-stratification weights are proportional to $N_h/n_h$, where $n_h$ is random rather than fixed in advance \citep{lohr2021sampling}.

We cannot use such weights for conformal inference and retain our exact finite-sample guarantees under the justifications in the present paper, because it would induce dependence between the training set and test case, while the covariate shift results of \cite{tibshirani2019conformal} assume the test case is independent of the training cases. However, \cite{fannjiang2022conformal} extend conformal methods to allow for ``feedback covariate shift,'' where the test distribution is allowed to depend on the observed training data, and this may be a promising direction for future work on conformal prediction with post-stratification.

In the meantime, we can treat post-stratification as an approximation to estimating the covariate-shift likelihood ratio weights. Although we lose the exact conformal guarantees, using such an approximation would be just as reasonable as the estimation of covariate-shift weights in general. Specifically, imagine we are sampling SRSWR from two different finite populations: a training population of size $M$, and a test population of size $N$. Both populations have the same $P(Y|X)$ and the same set of post-strata $1,\ldots,H$, but different (and known) post-stratum sizes $M_h,N_h$ for $h=1,\ldots,H$. In both cases, our sampling design is equivalent to first choosing a post-stratum at random with probability proportional to post-stratum size, then a unit from within that post-stratum at random.

Now, we apply Lemma~\ref{lem:PPSWR}---except that for our ``covariate'' we use the post-stratum ID $h \in 1,\ldots,H$, not the unit ID. Then $dP_X(X_i=h)=M_h/M$, and $d\tilde{P}_X(X_i=h) = N_h/N$, so $w(X_i) \propto N_h/M_h$. So far, we can have exact guarantees. But if we assume that we do not actually know the true post-stratum sizes for the training population, we can replace $M_h/M$ with training-sample estimates $n_h/n$ and get post-stratification weights $w(X_i) \propto N_h/n_h$. If we further assume that both populations are actually the same, we are now in the standard post-stratification setting, and we have a reasonable justification for carrying out conformal methods using the standard post-stratification weights. We can carry out the weighted full conformal approach from Corollary~1 and the weighted split conformal approach from Section~A.3 of \cite{tibshirani2019conformal}, and our guarantees are approximate only because we estimated the ``training'' post-stratum sizes.







\section{Examples}\label{sec:Examples}


\subsection{Real data}

% Other real-dataset ideas:
% Use NHANES or similar, to create conf bands for pediatric growth charts?
%
% Admit that this wouldn't be quite practical! For pediatric charts, we want to allow for different PI widths at different ages. So it seems like they usually *start* with getting quantiles at each age, *then* smooth those est'ed quantiles:
% \url{https://www.cdc.gov/nchs/data/series/sr_11/sr11_246.pdf}
%
% Whereas conformal would fit a model for the mean-by-age first, then find residual quantiles, which is NOT quite the same thing.
%
% AHA---of course Y. Romano's Conformalized Quantile Regr does indeed do that: start with a model for quantiles-by-age, then adjust those quantiles a bit using conformal approach.

We have claimed that conformal methods may work better when they account for the sampling design of the data. As a simple demonstration, we turn to an extract of the Medical Expenditure Panel Survey or MEPS \citep{ahrq2017meps}, which is a nationally representative survey about the cost and use of health care among the U.S.\ civilian noninstitutionalized population.

We chose the MEPS because it has already been used as a benchmark dataset in several conformal inference papers, starting with \cite{romano2019conformalized} and followed by others \citep{sesia2020comparison, sesia2021conformal, feldman2021improving, bai2022efficient}. In each of these papers, the authors randomly partition MEPS data into proper training, calibration, and test sets, then report the coverage and length of conformal PIs for various models across many such random partitions.
However, all of these papers appear to assume exchangeability of the rows in the MEPS data. None of them report accounting for the complex sampling design of MEPS, which includes stratification, clustering, and oversampling of selected subgroups.

At present, we do not attempt a full correction of these earlier analyses of MEPS. We only wish to illustrate that there can be noticeable differences in the conformal PIs depending on whether or not we account for the sampling design, even in a very simple analysis. We use a portion of the public-use dataset for calendar year 2015. We subset to only those respondents who filled out the self-administered questionnaire (SAQ) portion of the survey, and we use the person-level weight variable designed to be used with the SAQ for persons age 18 and older during the interview.


In the poster associated with \cite{romano2019conformalized}, available at
\url{https://github.com/yromano/cqr/blob/master/poster/CQR_Poster.pdf},
the authors explain that they are predicting ``health care utilization, reflecting \# visits to doctor's office / hospital.''
Following their GitHub code, we define a ``utilization'' response variable as the sum of five counts for 2015:
total number of office-based visits reported;
total number of reported visits to hospital outpatient departments;
count of all emergency room visits reported;
total number of nights associated with hospital discharges; and
total number of days where home health care was received from any type of paid or unpaid caregiver.


Unlike the earlier conformal analyses of MEPS, we do take into account the public-use variables for strata, PSUs, and person-level weights. In the 2015 SAQ-eligible subset that we work with, there are 165 strata, and most have 2 or 3 PSUs. First we drop the 4 strata which had no observations in either PSU 1 or PSU 2. Next, for simplicity, we set aside every observation whose PSU is labeled 3 (regardless of stratum) and treat them as our test set. We treat the rest (PSUs 1 and 2) as our overall training set. We split this training data into proper-training and calibration sets under two different approaches. The first approach is a 50/50 SRS split that ignores the survey design. The second approach is to form a random split by PSU within each stratum, so that in each stratum independently, we randomly assign either PSU 1 to proper-training and PSU 2 to calibration or vice versa. These approaches result in proper-training and calibration sets with around 10,000 people each and a test set with 1659 people.

For each split, we fit a linear regression model to the proper-training set to predict utilization from a subset of the explanatory variables used by \cite{romano2019conformalized}: age; sex; indicators for diabetes diagnosis, private insurance coverage, and public insurance coverage; and quantitative summaries of answers to the Physical Component Summary (PCS), the Mental Component Summary (MCS), and the Kessler Index (K6) of non-specific psychological distress. For the PCS and MCS, higher scores represent better health, while lower scores for the K6 represent less distress. Then we calculate PIs for each test case by combining it with the calibration set and finding conformal quantiles. For simplicity, our quantiles do not use the methods of Sections~\ref{sec:Cluster} and \ref{sec:Strata} to handle the clustering and stratification, but they do apply the survey weights as in Section~\ref{sec:PPS}.


% This same dataset is used for other ML papers such as \cite{chester2021understanding} (on de-identified data?) or \cite{singh2019understanding} (on bias mitigation), and in those 2 papers they are SPECIFICALLY looking at MEPS rather than using it as one of many many ``example'' datasets.  So it's not ONLY this Candes group (and others inspired by it).

Because we ended up with such large proper-training and calibration sets, relative to the smaller test set, we only saw small differences between the exchangeable and design-based conformal approaches at moderate PI levels. On the other hand, we have enough proper-training and calibration data to estimate 99\% PI levels too, and there we do see substantial differences in the average PI length.

Our main takeaways, based on Tables~\ref{table:meps_covg} and \ref{table:meps_len}:

\begin{enumerate}
  \item When we used a conformal pipeline that assumed exchangeability (data splits at random; no weights in model-fitting; no weights in the conformal quantiles on the calibration set), we tended to over-cover. If we also ignored the weights when using the test set to estimate coverage, these calculations under-estimated just how much over-coverage there was. By taking survey-weighted means on the test set, we found higher estimates of coverage. We believe these higher estimates are more appropriate, since the survey-weighted means ought to generalize to the rest of the population better than un-weighted means do.
  \item When we did use design-based methods (design-based splits; design-based and survey-weighted model fits; and survey-weighted conformal quantiles on the calibration set), this reduced our over-coverage a little, according to the survey-weighted means of coverage on the test set. Similarly, it also made our PIs a little narrower (around one fewer utilization/year) for moderate PI levels, and substantially narrower (around 30 fewer utilizations/year) for 99\% PIs.
\end{enumerate}

This brief MEPS example demonstrates that the estimated PI coverages and lengths can differ when conformal methods account for the survey design. In the next subsection, we study these effects in more detail, by repeatedly sampling under known sampling designs from a complete finite population. We also use smaller sample sizes, to see more pronounced differences between using vs.\ ignoring the survey design.






\begin{table}[t!]
\centering
\begin{tabular}{rllllll}
  \hline
  & Split/fit/conformal & Test set & 80\% PI covg & 90\% PI covg  & 95\% PI covg & 99\% PI covg\\
  \hline
  & SRS & SRS & 0.825 & 0.909 & 0.951 & 0.994 \\
  & SRS & Svy-wtd & 0.837 & 0.913 & 0.951 & 0.995 \\
  & Svy-wtd & Svy-wtd & 0.830 & 0.912 & 0.950 & 0.992 \\
  \hline
\end{tabular}
\caption{Linear regression models' PI coverage, estimated on the MEPS dataset. Coverages reported as averages over 20 random proper-training/calibration splits at each setting, using the same test set each time. When data splits, proper-training-set model fits, and calibration-set conformal quantiles ignored the survey design, we over-covered (especially for lower PI levels); but when test-set estimates of coverage also ignored the sampling design, they \emph{underestimated} the amount of overcoverage, compared to test-set estimates that did account for the sampling design. However, when splits, fits, and conformal quantiles accounted for the survey design, there was less over-coverage.}
\label{table:meps_covg}
\end{table}

\begin{table}[t!]
\centering
\begin{tabular}{rllllll}
  \hline
  & Split/fit/conformal & Test set & 80\% PI length & 90\% PI length  & 95\% PI length & 99\% PI length\\
  \hline
  & SRS & Either & 14.4 & 22.0 & 30.2 & 128.0 \\
  & Svy-wtd & Svy-wtd & 14.1 & 21.0 & 29.3 & 97.3 \\
  \hline
\end{tabular}
\caption{Linear regression models' PI lengths, estimated on the MEPS dataset. PI lengths reported as averages over 20 random proper-training/calibration splits at each setting, using the same test set each time. When data splits, proper-training-set model fits, and calibration-set conformal quantiles ignored the survey design, our PI lengths tended to be larger than when splits, fits, and conformal quantiles did account for the survey design---especially at very high PI levels. In the former case, it does not matter whether or not test-set estimates were survey-weighted, because these PI lengths are constant across test-set cases for a given data split and PI level.}
\label{table:meps_len}
\end{table}










\subsection{Simulations}\label{sec:Sims}

For our design-based simulations, we used the venerable Academic Performance Index (API) data \citep{cdoe2018academic} from \texttt{R}'s \texttt{survey} package \citep{lumley2021survey}. The \texttt{apipop} dataset contains information on 37 variables for all 6194 California schools (elementary, middle, or high school) with at least 100 students. The dataset vintage is not documented, but appears to be the 1999-2000 academic year, since the data includes API scores for each school for 1999 and 2000.

We used the \texttt{apipop} dataset as the finite population, and repeatedly took samples (with around $n=200$ ultimate sampling units) using various designs. When we evaluated our results on test sets, we used the entire finite population---including those cases that had already been used to fit models or find conformal quantiles---because this corresponds to the kind of guarantees that our paper makes in earlier Sections.
Our main takeaways:

\begin{enumerate}
  \item Across many settings, using the naive quantile (the $\lceil n\alpha\rceil$ order statistic) instead of the conformal quantile (the $\lceil (n+1)\alpha\rceil$ order statistic) tended to give slight undercoverage. The conformal quantile helped partly to fix this; but in non-SRS settings it was not enough of a fix on its own.
  \item For SRS designs, the conformal quantile lemma worked as advertised. See Table~\ref{table:SRS}.
  \item For PPS designs, ignoring the weights gave slight undercoverage when weights were not highly informative about the response variable. On the other hand, ignoring the weights led to extreme \emph{over}-coverage when weights were highly informative. In both cases, weighted conformal quantiles fixed the problem. See Tables~\ref{table:PPS-api00} and \ref{table:PPS-enroll}.
  \item For clustered designs, as well as for stratified designs, ignoring the design undercovered but accounting for the design (including conformal-quantile padding) did fix it. The one exception was for one of the cluster-design simulations, where the design-based conformal PIs did not reach the target coverage. This might have been due to the small number of clusters, large variation in cluster sizes, and our choice of ``subsampling once'' as the conformal method. See Tables~\ref{table:clus} and \ref{table:strat}.
  \item For simple models and split-conformal inference,
if the weights were not highly informative about model variables or the fit of the model,
then it did not make much difference whether or not the weights were used for quantiles.
But when the weights were informative,
we saw that unweighted conformal quantiles \emph{over}-covered (and PIs were too wide).
Using survey-weights in model-fitting was not enough to fix it,
but weighting the quantiles was. See Tables~\ref{table:model_covg} and \ref{table:model_len}.
\end{enumerate}

Overall, we saw that the survey-conformal quantiles
we proposed mathematically in Section~\ref{sec:Methods}
also appear to work empirically.
There is more work to be done on special cases like clustering or post-stratification, to be sure.
But for simply survey-weighted data, there is no reason to treat it as SRS;
data analysts will most likely get coverage closer to nominal when they account for the weights or other survey design features.

Simulation details:
\begin{itemize}
  \item In all simulations, sampling was done without replacement. Although our results in Section~\ref{sec:Methods} assume sampling with replacement, we conjectured that sampling without replacement would still lead to conformal coverage close to nominal, and we wanted to check this empirically.
  \item All simulations were run 1000 times. All 95\% confidence intervals are calculated as the estimate $\pm$ 2 times the SD over $\sqrt{1000}$.
  \item After dropping the rows with missing values for \texttt{enroll} and \texttt{mobility}, the full ``finite population'' consisted of the 6153 schools without missing values for any variables used in the simulations.
  \item Most simulations used a sample size of $n=200$ schools. However, the cluster samples had $n \approx 200$ schools on average but varied across samples. The regression model simulations took PPS samples of size $m+n=400$, then split the samples at random into a proper training set of $m=200$ and a calibration set of $n=200$.
  \item The response variable was usually \texttt{api00}, the school's API in 2000. The exception is Table~\ref{table:PPS-enroll}, where the response variable was \texttt{enroll}, the same variable used to create the PPS weights. For the non-regression simulations, we simply sought ``unsupervised'' prediction intervals for the marginal distribution of the response variable (with no explanatory variable). For the regression simulations, we found quantiles of $|y - \hat f(x)|$ on the calibration set and sought prediction intervals for the response variable at the explanatory variable values for each unit in the population.
  \item PPS sampling probabilities (if used) were usually proportional to \texttt{enroll}, the number of students enrolled at the school. The exception is parts of Tables~\ref{table:model_covg} and \ref{table:model_len}, where PPS probabilities were proportional to 1 plus the square root of the residuals from the full-population linear regression model, in order to see the effects of over-sampling cases that are hard to fit well. Conformal quantiles for the PPS simulations were calculated as in Section~\ref{sec:PPS}.
  \item Clusters (if used) were based on \texttt{dnum}, the school district number. Cluster samples always took a SRS of 24 school districts. 24 was chosen because it led to an average of $n=198$ schools (close to the $n=200$ used in other sampling designs). For the ``survey-design-aware'' results in Table~\ref{table:clus}, we calculated quantiles using the ``subsampling once'' method as in Section~\ref{sec:Cluster}, while the design-unaware results ignored clustering and calculated quantiles on the whole dataset.
  \item Strata (if used) were based on \texttt{stype}, the school type. Stratified samples always took 100 elementary, 50 middle, and 50 high schools, with an SRS within each school type. For the ``survey-design-aware'' results in Table~\ref{table:strat}, we calculated quantiles separately by stratum as in Section~\ref{sec:Strata}, while the design-unaware results ignored strata and calculated quantiles on the whole dataset.
  \item Linear regression models always predicted \texttt{api00} using a linear combination of \texttt{ell} (the percentage of English language learners), \texttt{meals} (the percentage of students eligible for subsidized meals), and \texttt{mobility} (the percentage of students for whom this is the first year at the school).
\end{itemize}





\begin{table}[ht!]
\centering
\begin{tabular}{rlll}
  \hline
 & Conformal? & 80\% PI coverage & 90\% PI coverage \\
  \hline
  & no & (0.794, 0.801) & (0.945, 0.949) \\
  & yes & (0.800, 0.807) & (0.949, 0.953) \\
   \hline
\end{tabular}
\caption{SRS. Average PI coverage of \texttt{api00}, at two different PI levels, under 1000 SRS samples of $n=200$ each from API dataset. Coverages reported as 95\% confidence intervals. Naive quantiles undercover, but conformal quantiles achieve target coverage.}
\label{table:SRS}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{rllll}
  \hline
 & Survey-weighted? & Conformal? & 80\% PI coverage & 90\% PI coverage \\
  \hline
  & no & no & (0.752, 0.760) & (0.927, 0.932) \\
  & no & yes & (0.759, 0.767) & (0.935, 0.940) \\
  & yes & no & (0.795, 0.803) & (0.944, 0.949) \\
  & yes & yes & (0.803, 0.811) & (0.953, 0.958) \\
   \hline
\end{tabular}
\caption{Uninformative PPS. Average PI coverage of \texttt{api00}, at two different PI levels, under 1000 PPS samples of $n=200$ each from API dataset where probability $\propto$ \texttt{enroll}. Coverages reported as 95\% confidence intervals. Naive quantiles \emph{under}-cover; conformal quantiles alone or survey-weighting alone do not fix it; but survey-weighted conformal quantiles achieve target coverage.}
\label{table:PPS-api00}
\end{table}



\begin{table}[ht!]
\centering
\begin{tabular}{rllll}
  \hline
 & Survey-weighted? & Conformal? & 80\% PI coverage & 90\% PI coverage \\
  \hline
  & no & no & (0.933, 0.935) & (0.986, 0.987) \\
  & no & yes & (0.934, 0.937) & (0.988, 0.989) \\
  & yes & no & (0.792, 0.798) & (0.946, 0.949) \\
  & yes & yes & (0.796, 0.802) & (0.948, 0.951) \\
   \hline
\end{tabular}
\caption{Informative PPS. Average PI coverage of \texttt{enroll}, at two different PI levels, under 1000 PPS samples of $n=200$ each from API dataset where probability $\propto$ \texttt{enroll}. Coverages reported as 95\% confidence intervals. Naive quantiles \emph{over}-cover; conformal quantiles alone do not fix it, while survey-weighting alone \emph{under}-covers; but survey-weighted conformal quantiles achieve target coverage.}
\label{table:PPS-enroll}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{rllll}
  \hline
 & Survey-design? & Conformal? & 80\% PI coverage & 90\% PI coverage \\
  \hline
  & no & no & (0.785, 0.805) & (0.934, 0.943) \\
  & no & yes & (0.791, 0.811) & (0.940, 0.949) \\
  & yes & no & (0.799, 0.817) & (0.916, 0.930) \\
  & yes & yes & (0.799, 0.817) & (0.959, 0.968) \\
   \hline
\end{tabular}
\caption{Clustering. Average PI coverage of \texttt{api00}, at two different PI levels, under 1000 clustered samples of 24 clusters ($n\approx 200$) each from API dataset. Coverages reported as 95\% confidence intervals. Due to high variability in cluster sizes, these 95\% CIs are wider than in previous tables, but overall trend is generally similar to other tables: Naive quantiles appear likely to \emph{under}-cover; conformal quantiles alone or survey-design-aware analyses alone do not necessarily fix it; but survey-design-aware conformal quantiles achieve target coverage.}
\label{table:clus}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{rllll}
  \hline
 & Survey-design? & Conformal? & 80\% PI coverage & 90\% PI coverage \\
  \hline
  & no & no & (0.769, 0.777) & (0.933, 0.938) \\
  & no & yes & (0.775, 0.783) & (0.940, 0.944) \\
  & yes & no & (0.787, 0.795) & (0.939, 0.944) \\
  & yes & yes & (0.800, 0.808) & (0.953, 0.956) \\
   \hline
\end{tabular}
\caption{Stratification. Average PI coverage of \texttt{api00}, at two different PI levels, under 1000 stratified samples of $n=200$ each from API dataset. Coverages reported as 95\% confidence intervals. Naive quantiles \emph{under}-cover; conformal quantiles alone or survey-design-aware analyses alone do not fix it; but survey-design-aware conformal quantiles achieve target coverage.}
\label{table:strat}
\end{table}



\begin{table}[ht!]
\centering
\begin{tabular}{rlllll}
  \hline
  & PPS probs & Svy-wtd conformal? & Svy-wtd regression? & 80\% PI covg & 90\% PI covg \\
  \hline
  & \texttt{enroll} & no & no & (0.807, 0.810) & (0.955, 0.957) \\
  & \texttt{enroll} & yes & no & (0.803, 0.807) & (0.954, 0.956) \\
  \hline
  & residuals & no & no & (0.874, 0.876) & (0.973, 0.974) \\
  & residuals & no & yes & (0.875, 0.878) & (0.973, 0.974) \\
  & residuals & yes & no & (0.798, 0.802) & (0.950, 0.951) \\
  & residuals & yes & yes & (0.798, 0.801) & (0.950, 0.951) \\
  \hline
\end{tabular}
\caption{Linear regression models' PI coverage. Average PI coverage of \texttt{api00}, at two different PI levels, under 1000 PPS samples of $n=200$ each from API dataset. Coverages reported as 95\% confidence intervals. For weights proportional to \texttt{enroll} (uninformative for the regression), it makes little difference whether or not we weight the conformal quantiles. For informative weights proportional to full-pop residuals, un-weighted conformal quantiles \emph{over}-cover, and this is fixed by survey-weighted conformal quantiles; but it makes little difference whether or not we fit survey-weighted regression models.}
\label{table:model_covg}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{rlllll}
  \hline
  & PPS probs & Svy-wtd conformal? & Svy-wtd regression? & 80\% PI length & 90\% PI length \\
  \hline
  & \texttt{enroll} & no & no & (192.6, 194.1) & (294.5, 297.3) \\
  & \texttt{enroll} & yes & no & (190.8, 192.7) & (293.7, 297.2) \\
  \hline
  & residuals & no & no & (214.1, 215.6) & (328.2, 331.5) \\
  & residuals & no & yes & (211.8, 213.3) & (331.4, 334.8) \\
  & residuals & yes & no & (179.4, 180.8) & (285.4, 287.7) \\
  & residuals & yes & yes & (175.1, 176.3) & (286.1, 288.3) \\
  \hline
\end{tabular}
\caption{Linear regression models' PI lengths. Average length of PIs for \texttt{api00}, at two different PI levels, under 1000 PPS samples of $n=200$ each from API dataset. Lengths reported as 95\% confidence intervals. For weights proportional to \texttt{enroll} (uninformative for the regression), it makes little difference whether or not we weight the conformal quantiles. For informative weights proportional to full-pop residuals, PIs from un-weighted conformal quantiles are much wider than PIs from survey-weighted conformal quantiles; but it makes little difference whether or not we fit survey-weighted regression models.}
\label{table:model_len}
\end{table}





\section{Extensions}\label{sec:Extensions}

We discuss several practical considerations: What to do if the sampling design does not quite match the situations above? What if the sampling probabilities are not all known? How can we handle a regression problem where constant-width prediction intervals are not appropriate?

We also suggest other possible use cases for conformal inference in survey methodology, such as during data collection or pre-processing.





\subsection{Practical considerations}

\paragraph{Weighting adjustments:} Most surveys are not released with inverse-probability sampling weights alone. Typically, the final obtained sample does not include all of the units originally selected (due to issues like nonresponse), so the final survey weights have been adjusted for nonresponse, post-stratification, and other considerations.

Of course, these same issues arise with design-based inference in general. It takes expertise and professional judgment to decide whether we can reasonably trust design-based means and confidence intervals under nonresponse etc. Conformal inference with real survey data will require similar judgment.

\cite{tibshirani2019conformal} found that their conformal methods still maintained coverage close to nominal even when they only estimated the likelihood ratio weights, instead of using true likelihood ratios. We anticipate similar results for conformal methods that use approximate sampling weights instead of the true inverse-probability sampling weights.


\paragraph{Distribution shift:} Surveys are typically assumed to be sampled from a well-defined finite population, in a specific time and place. Hence, we may not be guaranteed coverage if we make predictions for future units that were not in the finite population at the time it was sampled, nor for units from distinct populations.

Once again, these same problems arise with design-based inference in general, and deciding how safely we can extrapolate beyond the original finite population becomes a matter of professional judgment by statisticians, plus subject-matter expertise around the topic of the survey. If model-based (rather than design-based) inference makes more sense for the application at hand, we may still be able to estimate the covariate shift likelihood ratio and apply the conformal techniques of \cite{tibshirani2019conformal}; it will simply no longer be design-based inference.

% Same thing with non-prob-sampled data that is post-stratified to make it look more representative: these methods are only good as long as that is a good approximation.
%
% [??] If so, using the same design-based weighting scheme as above would just be treated as an approximation, just as reasonably as estimating covariate-shift weights in a Tibshirani-paper-like situation.


\paragraph{Unknown sampling probabilities for test cases:} A distinct issue arises because our conformal methods require the sampling probabilities for each test case. However, in practice the sampling probabilities are not typically known for every element in the population. They may be known internally within the organization that planned and carried out the survey, but not necessarily available to the public.

If survey organizations were to publicly release the known sampling probabilities for every unit in the population, this could increase the risk of unit re-identification and privacy breaches.
Instead, we suggest several ideas for ways to approximate the sampling probabilities for our test cases.

\begin{itemize}
  \item The survey organization could report a set of population categories for which the sampling probabilities are approximately equal within each category, along with their approximate probabilities. It may be possible to find categories that are broad enough to minimize privacy risks, but fine enough to allow a good approximation to the real sampling probabilities.
  \item Instead of discrete categories, the survey organization could estimate and report something like a generalized variance function or GVF \citep{wolter2007generalized}, but used to estimate sampling probabilities rather than variances for each population unit based on some covariates available for each unit. Or users of a public dataset could attempt to estimate their own such function to predict the sampling probability for out-of-sample population units, based on the survey documentation and the inverse-probability sampling weights reported for the in-sample units. This may be reasonable for PPS, where the probabilities really are a simple function of some covariate that is available for all population units.
  \item The survey organization could release only the value of the single largest inverse-probability sampling weight for the whole population. Then, using that one weight for every test case during conformal prediction would be strictly conservative. As an approximation, users could use the largest sampling weight in the public-use dataset for every test case.
  \item Users could try a range of plausible weights for a desired test case, based on the weights of similar in-sample units, and report a sensitivity analysis.
  \item If a user is only interested in predictions for population units that happen to have the same vector of covariates $X$ as one of the existing cases in the dataset, they can simply use that case's survey weights.
\end{itemize}



\paragraph{Prediction intervals with adaptive width:} In the simple approach to conformal prediction for regression described above, we have used a constant-width prediction band everywhere, which might be unrealistic for many scenarios. If the true conditional distribution of $Y|X$ is heteroscedastic for instance, marginal coverage will still be correct, but it will be achieved by overcovering at some regions of $X$ and undercovering at others. For better alternatives, see recent work on locally-weighted conformal inference \citep{lei2018distribution} and conformalized quantile regression \citep{romano2019conformalized}. Alternately, it may be possible to model the data on a scale where homoscedastic residuals are reasonable, such as making predictions on the scale of $\log(Y)$ rather than $Y$.

Note that in the survey-weighted setting, even without any of these extensions we will actually get slightly different prediction interval widths at different test cases, because the survey weights can differ for each unit being predicted. However, the effect of survey weighting is to widen slightly the prediction intervals for regions of $X$ with a smaller effective sample size. This is distinct from adaptivity to regions with more variability in $Y$.

Speaking loosely, a sampled unit with a large sampling weight represents more population units than a sampled unit with a smaller weight. Thus we are more uncertain about units ``like'' the one with a larger weight than about units ``like'' the one with a smaller weight, and this intuition bears out in conformal prediction. When we apply the methods of Lemma~\ref{lem:PPSWR} to a test case with a larger weight, its survey-weighted eCDF is pushed down farther than if it had a small weight; so the nonconformity score quantile is estimated farther to the right; so the conformal prediction interval is wider for units with larger weights, all else being equal.

On the other hand, consider optimal allocation designs, in which strata with higher variance of $Y$ are oversampled. When we construct conformal intervals separately within each stratum, the high-variance strata will have their eCDFs padded by a smaller $1/n_h$ than low-variance strata do, so the quantile adjustment is less conservative. But this should be more than offset by the tautological fact that high-variance strata also have wider spread in $Y$, so that ultimately their prediction intervals will end up wider than those of low-variance strata.

% % Reminder of why I thought (earlier) that this was reversed: my earlier thoughts were based on reading Dagdoug 2022 and thinking of OPTIMAL ALLOCATION designs, where high-variance strata are oversampled. So higher-var strata have larger probs, so smaller weights, so eCDF doesn't get pushed down as much, so conformal PI is narrower. Then yes, indeed, we have NARROWER PIs in strata where we know Y is more variable---which seems backwards. But that's because we deliberately oversampled those strata! Whereas in other situations, small samp prob = large weight happens in ``undersampled'' regions (strata or otherwise), so we get WIDER PIs because we have TOO LITTLE DATA. OK, that makes sense. Our PIs get padded to be wider when there's less data, NOT when the data values themselves are more variable---I can see how that's in the conformal spirit. And frankly, if we DO optim alloc but then we do conformal separately within each stratum, this will still end up with wider PIs in high-variance regions---it's just that they'll be wider b/c the Ys are more variable, NOT b/c of added padding due to smaller sample size (since it's not smaller there).]


\subsection{Other uses for conformal methods in survey methodology}

So far we have focused on the users of published survey microdata. Conformal methods can allow them to fit models to microdata and create prediction intervals or sets around the model's predictions.

However, survey collectors (such as national statistical offices, polling agencies, and any others who collect and pre-process survey data) could find their own use cases for conformal methods:

\begin{itemize}
  \item Build a response propensity model based on internal metadata from past surveys. For the next survey, build conformal prediction sets for each sampled unit's most likely mode of response (or ultimate status of nonresponse). Use these sets to help choose the mode of initial contact for each sampled unit.
  \item Update internal dashboards about the survey data collection process in real time in an online manner, similarly to the Washington Post's 2020 presidential election tracker \citep{cherian2020washington}.
  \item As part of automated quality control checks, build conformal prediction intervals or sets for each variable. If any cases fall outside the prediction interval or set, flag them as potential outliers for followup.
  \item To impute for item nonresponse, or to generate synthetic microdata, draw at random from a conformal prediction interval or set for that variable.
\end{itemize}

We do not expect such conformal methods to replace but rather to supplement traditional methods for these aspects of survey processing. For instance, subject-matter expertise is incredibly important in designing quality-control checks to detect measurement errors or data-processing errors. At the same time, conformal methods for unsupervised outlier detection could be a secondary ``sanity check'' that supplements the checks designed by subject-matter experts.



\section{Conclusion}\label{sec:Conclusion}

There is growing interest in extending machine learning (ML) models to complex survey data, as well as a distinct body of work around tailoring confidence intervals and prediction intervals to specific ML models. While this work is ongoing and valuable, design-based conformal inference provides a way to jump ahead of both of these trends: We can apply a ML model to complex survey data (even if that model has not been specifically adapted to account for the survey design yet), and automatically get conformal prediction intervals for the fitted model (even if native prediction-interval methods have not been developed for that model yet), and still manage to provide exact, finite-sample, design-based coverage guarantees.

% [?? Are conformal PIs wider than ``native'' PIs? E.g. if I run linear regression and get PIs based on Normality assumptions, are the conformal PIs much wider? Lei 2018 shows this is NOT true in high dimensions, but I don't recall if it happens for low-dim problems. You'd think it would; but maybe the conformal PIs are not **wider** so much as **more variable**?]

Of course, this is not a panacea.
First, for models that do have well-understood design-based adaptations, the design-based version fitted to complex-survey-design training data is likely to be a better model for the mean (important in its own right) and consequently to have narrower prediction intervals (because the residuals will tend to be smaller). As a simple example, in certain cases a survey-weighted linear model may fit better and generalize better than an unweighted linear model fit to the same data. In this sense, it is still well worth taking the time to develop survey-weighted equivalents of ML models that have not yet received adequate attention among survey statisticians.

Second, for models that do have native prediction intervals, if we can justifiably trust that the underlying model assumptions are met, then we may be able to get narrower (or at least less-variable) prediction intervals natively than from conformal inference. And in some cases, native methods can give us conditional prediction intervals rather than marginal ones.
At the very least, we may get stabler prediction intervals by using native methods on the full dataset, rather than split conformal where the prediction intervals are based on only a subset of the data.
% and therefore may have higher variability in PI-width from sample to sample even if a constant PI level of $1-\beta$ is guaranteed **on average**.
In this sense, it is still well worth taking the time to develop native prediction intervals for specific models.

But for the cases where either of these conditions is not met, conformal inference is a practical and assumption-lean way to fill in the gap. We also note that \cite{lei2018distribution} found in simulations that conventional methods are so noisy for high-dimensional regression that conformal methods can yield even narrower prediction intervals than native methods, while maintaining the same coverage guarantees.

We have also discussed several pragmatic limitations to conformal inference with survey data: Sampling weights are not truly known in advance (due to nonresponse and other adjustments). Nor can they be reported for every population unit where we may want a prediction interval.
% Many (probably most) public-use survey datasets do not report sampling weights for out-of-sample units, nor (in order to protect respondent privacy, which is a very admirable and important goal!) do they publish enough details of the sampling design to allow the public to estimate sampling weights based solely on the sample-design documentation.
Above we suggest several ways to estimate these weights, but we recognize that this may be a real hurdle for many practitioners.

Still, we hope that this paper spurs interest in conformal inference among survey statisticians. Survey data analysts might find these methods directly applicable. Researchers within survey organizations may also find these methods particularly useful in settings where out-of-sample sampling probabilities can be known.
% At least some public-use datasets might begin to be released with documentation that lets us estimate sampling weights for out-of-sample units, so that conformal methods can be more widely used with public-use data.

Furthermore, we also believe that research into conformal methods from the design-based perspective could bring new insights back to the wider statistics/ML community of conformal inference researchers.
For instance, if survey statisticians are able to develop conformal methods for more general cluster sampling designs than the one in Section~\ref{sec:Cluster}, this could translate into useful results for conformal inference with hierarchical models more generally.

As another example, \cite{vovk2013conditional} cites earlier results by \cite{guttman1970statistical} to show that the distribution of achieved coverage levels has a Beta distribution across calibration sets when data are exchangeable. \cite{angelopoulos2022gentle} do suggest an estimate of the effective sample size $n_\mathrm{eff}$ for weighted conformal methods, so we can gauge how much variability to expect from this Beta distribution; but their estimate appears to be based on (4.3) of \cite{kish1992weighting}, which is not appropriate for all sampling designs nor for all estimators. Survey statisticians may be able to suggest better estimates of $n_\mathrm{eff}$ or recommend other ways to study and control the variability in achieved coverage for non-exchangeable data.
% Can we get a fpc version of this for finite pop sampling? Also, discuss approaches to estimating $n_{eff}$ that would let us do this for unequal probs, strata, and clusters. Cite (11.7.6$^\prime$) of \cite{kish1965survey} and (4.3) of \cite{kish1992weighting} as sources for a starting point approx $n_{eff}$, but also how it isn't optimal for all sampling designs nor for all estimators.

As another example, post-stratification is commonly used outside of survey sampling. Conformal methods that provide exact coverage guarantees with post-stratification---perhaps based on approaches similar to that of \cite{fannjiang2022conformal}---would be widely welcomed.

Other open problems include conformal methods for unequal-probability sampling WOR, panel survey designs, or combining strata rather than analyzing each stratum separately. We encourage survey researchers to contribute to conformal methodology and find new areas of application for these methods.

\bibliographystyle{apalike}
\bibliography{conformal-refs}

\end{document}
