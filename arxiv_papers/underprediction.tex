\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts, xcolor}
\usepackage{cprotect}

\title{Underpredictions}

\begin{document}


\section{The Bayes optimal predictor}

Consider a loss function of the form 

\begin{align*}
L(\hat F) &= \mathbb{E}_{(F,X)}\left[(\hat F - F)^2|X\right]
\\
&= \mathbb{E}_{X}\mathbb{E}_{F}\left[(\hat F - F)^2|X\right]
\end{align*}
The (Bayes) optimal predictor is given by 
\begin{align*}
    F^{*} &= \arg \underset{\hat F}{\min}\,\, L(\hat F)
\end{align*}
We consider $F^{*}(X)$ for one particular value of $X$ (in the following all expectations are to be taken over $F$ alone):
\begin{align*}
    E[(\hat F - F)^2|X] &=E[(\hat F - E[F|X]- F + E[F|X])^2|X] 
    \\
    &=E[(\hat F - E[F|X]- F + E[F|X])^2|X] 
    \\
    &=E[(\hat F - E[F|X])^2|X] + E[(E[F|X] - F)^2|X] + 2E[(\hat F - E[F|X])(F - E[F|X])] 
    \\
    &=E[(\hat F - E[F|X])^2|X] + \text{Var}(F|X) + \\ &\quad 2 E[\hat F F|X] - 2 E[F|X]^2 + 2 E[F|X]^2 - E[F|X]E[\hat F|X] 
    \\
    &\text{Assumption: $\hat F$ deterministic fct. of $X$}
    \\
    &=E[(\hat F - E[F|X])^2|X] + \text{Var}(F|X) + 
    \\
    &\quad 2 \hat F E[F|X] - 2 \hat F E[F|X] 
    \\
    & =(\hat F - E[F|X])^2 + \text{Var}(F|X) 
\end{align*}
This is a convex function, with optimum at $F^{*}(X)=\mathbb{E}_F [F|X] $. Note that for this to be valid for all $X$, our network must be sufficiently expressive, else our model would be constrained and we would not (necessarily) be able to satisfy this condition for all $X$ independently. If we do indeed have sufficient (infinite) expressivity, this is the {\color{blue} Bayes optimal predictor}.
\section{Expectations for the Bayes optimal predictor}
We want to explain the observed curve 
\begin{align}
    C(F)=E_{\hat F}[\hat F | F]
\end{align}
where $\hat F$ denotes a predicted force, and $F$ an observed force (each for one particular pixel).
Starting from the joint distribution $p(\hat F, F) = \int p(\hat F, F|X)p(X)dX$, we have
$E[\hat F | F] = \int d\hat F \hat F p(\hat F | F)$ where this conditional distribution is given by:
\begin{align}
    p(\hat F| F) &= \frac{p(\hat F, F)}{\int d\hat F p(\hat F, F)}
    \\
    &= \frac{\int dX p(\hat F, F|X)p(X)}{\int d\hat F\int dX  p(\hat F, F|X)p(X)}
    \\
    &= \frac{1}{p(F)}\int dX p(\hat F, F|X)p(X)
\end{align}
[In the following I will use $\hat F$ to denote the random variable output by the network, and $\hat F_{NN}$ to denote the function of $X$. Since the network is deterministic, they coincide for the most part, but this will make some things notationally more clear.]
The joint distribution conditioned on $X$ can be written as $p(\hat F, F|X)=p(F |\hat F, X)p(\hat F|X) $.
Putting it together, we express
\begin{align*}
    E[\hat F | F] &= \int d\hat F \,\, \hat F p(\hat F| F) 
    \\
    &= \frac{\int dX\int d\hat F \, \hat F\,p(\hat F, F|X)p(X)}{\int d\hat F\int dX  p(\hat F, F|X)p(X)}
    \\
    &= \frac{1}{p(F)}\int dX\int d\hat F \, \hat F\,p(F |\hat F,X)p(\hat F|X)p(X)
    \intertext{{\color{blue} Assumption $H_0$: The network is deterministic, so that $p(\hat F|X) = \delta(\hat F = \hat F_{NN}(X))$ (Note: this does not imply that $\hat{F}_{NN}$ is bijective. Several values of $X$ could still map to one value of $\hat F_{NN}$.)}}
    &= \frac{1}{p(F)}\int dX\int d\hat F \, \hat F\,p(F |\hat F,X){\color{blue}\delta(\hat F = \hat F_{NN}(X))} p(X)
    \intertext{(Do integration over delta)}
    &= \frac{1}{p(F)}\int dX \, \hat F_{NN}(X) \,p(F |\hat F = \hat F_{NN}(X), X)  p(X)
    \intertext{{\color{blue} Assumption  $H_1$: we are close to Bayes optimum: $\hat F_{NN}(X)\approx F^{*}(X)=\int dF \, F \, p(F|X)$. This is equivalent to assuming that we have a sufficiently expressive network to attain the Bayes optimum}}
    &= \frac{1}{p(F)}\int dX {\color{blue} \int dF' \, F' \, p(F'|X)} \,p(F |\hat F = \hat F_{NN}(X), X)  p(X)
    \\
    \intertext{With Bayes' rule $p(x|y)=p(y|x)\frac{p(x)}{p(y)}$:}
    &= \frac{1}{p(F)}\int dF' \int dX \, F' \, p(F'|X) \,p(\hat F = \hat F_{NN}(X)|F,X)\frac{p(F|X)}{p(\hat F = \hat F_{NN}(X)|X)}  p(X)
    \intertext{Finally, we use that $p(\hat F = \hat F_{NN}(X)|X)=1$}
    &= \frac{1}{p(F)}\int dF' \int dX \, F' \, p(F'|X) \,\underbrace{p(\hat F = \hat F_{NN}(X)|F,X)}_{1}\frac{p(F|X)}{\underbrace{p(\hat F = \hat F_{NN}(X)|X)}_{1}}  p(X)
    \\
    &= \int dX \int dF' \, F' \, p(F'|X) \,\frac{p(F|X) p(X)}{p(F)}
    \\
    &= \int dX \int dF' \, F' \, p(F'|X) \,p(X|F).
    \intertext{Defining the posterior predictive distribution $p_{pp}(F'|F)\equiv \int dX\,p(F'|X)p(X|F)$, 
    }
    &= \int dF' \, F' \, p_{pp}(F'|F).
\end{align*}
In summary,
$$ \Rightarrow  \mathbb{E}[\hat{F}|F] =  \mathbb{E}_{F'\sim p_{pp}(F'|F)}[F'].$$
\section{The Gaussian case}
We can then ask: in what cases is $\mathbb{E}[\hat F|F]$ larger than, less than, or equal to $F$? 
We consider the following example: 
\begin{itemize}
    \item $X$ is Gaussian, $p(X)=\mathcal{N}(X| \mu_X, \sigma_X^2)$
    \item $\hat F$ is Gaussian. I.e. $\hat F_{NN}:\mathcal{X}\rightarrow \mathbb{R}$ is such that $p(\hat F)=\int dX \, p(X)\delta(\hat F = \hat F_{NN}(X)) = \mathcal{N}(\hat F| \hat \mu, \hat \sigma^2)$.
    \item The force $F$ depends on $\hat F$ only as the mean (white noise assumption): $p(F | \hat F)\sim \mathcal{N}( F| \mu=\hat F,  \sigma^2_F)$, and depends on $X$ only through $\hat F_{NN}(X)$.
\end{itemize}
This setup is meant to mimic our setup: we assume that we have some data which deterministically determines a force $\hat F\in \mathbb{R}$. However, the $F$ that we measure is subject to either biological or measurement noise.
\par
We now want to calculate the posterior predictive distribution. To do so, we will need to calculate $p(F'|X)$ and $p(X|F)$. The former is simply $\mathcal{N}(F'|\mu=\hat{F}_{NN}(X), \sigma_F^2)$, while the latter we can compute with Bayes' rule, $p(X|F)=p(F|X)p(X)/p(F)$. In total, the posterior predictive becomes
\begin{align*}
    p_{pp}(F'|F) = \int dX \frac{1}{2\pi \sigma_F^2}\exp\left(\frac{-(F'-\hat{F}_{NN}(X))^2 -(F-\hat{F}_{NN}(X))^2}{2 \sigma_F^2}\right)\frac{p(X)}{p(F)}
\end{align*}
For simplicity, we now consider $\hat{F}_{NN}(X)=\alphaX$.
Then, the above becomes
\begin{align*}
    p_{pp}(F'|F) = \frac{1}{p(F)2\pi \sigma_F^2\sqrt{2\pi \sigma_X^2}}\int dX \exp\left(\frac{-(F'- \alpha X)^2 -(F-\alpha X)^2}{2 \sigma_F^2}
    +
    \frac{-(X-\mu)^2}{2 \sigma_X^2}\right)
\end{align*}
The term in the exponent can be simplified to
\begin{align*}
    X^2 \underbrace{\left(\frac{-\alpha^2}{\sigma_F^2} - \frac{1}{2\sigma_X^2}\right) }_{a}
    + X\underbrace{\left(\frac{\alpha(F'+F)}{\sigma_F^2} + \frac{\mu}{\sigma_X^2}\right)}_{b}
    + \underbrace{\left(\frac{-(F'^2+F^2)}{2\sigma_F^2} - \frac{\mu}{\sigma_X^2}\right)}_{c}
\end{align*}
and subsequently brought into the form (by completing the square):
\begin{align*}
    a\left(X + \frac{b}{2a}\right)^2 + \left(c - \frac{b^2}{4a}\right).
\end{align*}
After integration and ignoring prefactors (which go into the normalization), the only relevant terms are those involving $F'$. The distribution is again Gaussian\footnote{We find precisely the same formula as found elsewhere, for example {\verb |https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf}, where we have an additional scaling of $\sigma_X\rightarrow\alpha\sigma_X$. In our case, the ``measurement'' corresponds to $F$, while the parameter distribution is $X$.},
\begin{align}
    p_{pp}(F'|F)\propto \exp\left(\frac{-(F' - \mu(F))^2}{2\sigma(F)^2}\right)
\end{align}
with 
\begin{align}
    \mu(F) &= \frac{\alpha (F\alpha \sigma_X^2 + \mu \sigma_F^2)}{\alpha^2 \sigma_X^2 + \sigma_F^2}
    \\
    \sigma(F)^2 &= \sigma_F^2 + \frac{\alpha^2 \sigma_X^2 }{\alpha^2 \sigma_X^2 + \sigma_F^2}
\end{align}
We can read off from here the mean
\begin{align*}
    \mathbb{E}[F '|F] = F \frac{\alpha^2 \sigma_X^2}{\alpha^2 \sigma_X^2 + \sigma_F^2}
    +
    \frac{\mu \sigma_F^2}{\alpha^2 \sigma_X^2 + \sigma_F^2}.
\end{align*}
It is now clear that for $\mu=0$ (reflecting a centered distribution of $X$), this average will always be less than $F$, $\mathbb{E}[\hat{F}|F]<F$, in the presence of non-zero noise $\sigma_F$. 
\section{Extrapolation to a more general case}
While our scenario is certainly not Gaussian, we expect the above conclusion to be relevant nevertheless. 
In the following we make the following assumptions:
\begin{enumerate}
    \item The true force magnitude $\bar F$ can be in principle determined from the zyxin (measured image $X$) through some deterministic function $f$, $\bar F=f(X)$.
    \item Due to experimental or biological noise, the measured force magnitude $F$ is scattered around the true value $\bar F$ according to $p(F|\bar F)$. Here we make the simplifying assumption that $p(F|\bar F)=K(F-\bar F)$, which is true in the case of Gaussian noise, for example.
\end{enumerate}
With these assumptions, we now proceed to investigate the behavior of the posterior predictive mean.
Due to the first assumption, we replace $X$ in the posterior predictive distribution with $\bar F$:
\begin{align}
    p_{pp}(F'|F) &= \int dX\,p(F'|X)p(X|F)
    \\
    &= \int d\bar F \,p(F'|\bar F)p(\bar F|F)
\end{align}
keeping in mind that the true force $\bar F$ should be thought of as a parameter (in particular, the mean) of the distribution of the measured forces $F, F'$.
By the second assumption, we can write
\begin{align}
    p_{pp}(F'|F) &= \int d\bar F \,K(F'-\bar F)p(\bar F|F)
    \\
    &= (K \star p(\cdot|F))(F')
\end{align}
where $\star$ denotes convolution.
\par
We now recall the characteristic function (or Fourier transform) of a distribution $p(x)$, 
\begin{align*}
    \phi(k)=\int dk e^{ikx} p(x).
\end{align*}
The mean is given as a derivative of the characteristic function
\begin{align*}
    \mathbb{E}_p (x) = -i\partial_k\phi(k)\big |_{k=0}.
\end{align*}
We can use this to compute the mean of a convolution between a distribution $P(x)$ and an unnormalized function (or kernel) $K(x)$.
\begin{align*}
    \mathbb{E}_{P\star K} (x) &= -i\partial_k(\phi_P\cdot\phi_K)\big |_{k=0}
    \\
    &= -i(\partial_k\phi_P)\phi_K\big |_{k=0} -i\phi_P(\partial_k\phi_K)\big |_{k=0}
\end{align*}
Since $P$ is a normalized distribution, $\phi_P(k=0)=1$. If $K$ can be normalized we can view it as a probability distribution, and then
\begin{align*}
    \mathbb{E}_{P\star K} (x) &= N_K \mathbb{E}_{P} (x)  + N_K\mathbb{E}_{\tilde K}(x).
\end{align*}
where $N_K=\int dx K(x)=\phi_K(k=0)$, and $\tilde{K}=K/N_K$.
In the case where we know $(P\star K)$ is again a probability distribution, it follows that $N_K=1$.
\par 
Again invoking a situation where the noise is Gaussian, we make the  assumption that $\mathbb{E}_{\tilde K} = 0$. This is true when $p(F|\bar F) = K(F'-F)\propto \exp( -(F - \bar F)^2/2\sigma^2)$ and is generally true for distributions symmetric about the mean.
In this scenario, 
\begin{align*}
    \mathbb{E}_{p_{pp}(\cdot | F)} (F') &=\mathbb{E}_{p(\bar F | F)} (\bar F).
\end{align*}
In other words, the posterior predictive mean is the same as the posterior mean. 
Given some measurement $F$, when is the mean of the posterior $\mu_{\bar F | F}$ less than $F$?
If the prior distribution $p_0(\bar F)$ has mean $\mu_0$, the effect of a new measurement $F$ is to drag the mean of the posterior to some value between the prior mean $\mu_0$, and the new measurement $F$: $\mu_0 < \mu_{\bar F | F} < F$ (true if $F>\mu_0$, else the inequalities are switched).
While we have no proof for this statement, it seems to hold in many cases.
\par
In summary, we expect the the quantity $\mathbb{E}[\hat{F}|F]=\mathbb{E}_{p_{pp}(\cdot|F)}[F']$ to be less than $F$ when $\mu_0 < F$, where $\mu_0$ is the mean of the prior distribution $p(\bar F)$, which we can approximate by the mean of the measured distribution. 
While most of the assumptions we made in this section would likely be violated in our case, we nevertheless expect the same qualitative behavior. 
First, the posterior distribution for $\bar F$, given some measurement $F$, tends to shift the mean to some value between the prior mean and $F$.
The posterior $predictive$ distribution can be roughly understood as a convolution between the posterior distribution for $\bar F$ and the distribution of noise $p(F'|F)$. 
For ``noise'' distributions $p(F|\bar F)$ which don't change their shape and are only translated by $\bar F$, the posterior predictive mean corresponds to the posterior mean.
For more exotic forms of noise, we cannot make any conclusions. 


\iffalse
We summarize the assumptions that were made to reach this conclusion:
\begin{enumerate}
    \item The true force magnitude $\bar F$ is given exactly by a deterministic function of the measured image $X$. This is certainly not exactly true, as we expect that no one protein is sufficient to exactly predict forces. However, based on the excellent performance of zyxin for predicting forces, we conclude that this may be approximately true. Even if not, we can say that there is a well defined mean $\bar F$ (without naming it the ``true'' force), and any stochasticity we absorb in the noise.
    \item We made several assumptions on the nature of the noise to reach our conclusion. 
    By ``noise'', here we mean anything that causes the measured forces $F$ to deviate from the mean $\bar F$.
    The central assumption we made was that $p(F|\bar F )$ depends only on the distance $F - \bar F$, which allowed us to write the posterior predictive distribution as a convolution. This is equivalent to saying that the $shape$ of the distribution $p(F|\bar F )$ does not change with $\bar F$, and is only translated according to the $\bar F$.
    This is certainly not true in our case, at the very least because the magnitudes are bounded from below by $0$ so that for $\bar F\rightarrow 0$, the shape of the distribution would necessarily change. 
    However, if the noise is mostly due to measurement error, it is plausible that the shape may be mostly independent of $\bar F$. 
\end{enumerate}
\fi

\end{document}
