\documentclass[review,12pt]{elsarticle}
\usepackage{amsthm}
\usepackage{lineno,hyperref}
\usepackage{moreverb}
\usepackage{graphicx,subfig,wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{txfonts}
\usepackage{afterpage}
\usepackage[margin=0.8in]{geometry}
\usepackage{mathrsfs}
\usepackage{tikz,amsmath}
\usepackage{pstricks}
\usepackage{diagbox}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{smartdiagram}
%\usepackage{ulem}
\usepackage[normalem]{ulem}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{verbatim}
%\usepackage{smartdiagram}

\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}

\tikzset{
>=stealth',
  punktchain/.style={
    rectangle, 
    rounded corners, 
    % fill=black!10,
    draw=black, very thick,
    text width=10em, 
    minimum height=2em, 
    text centered, 
    on chain},
  line/.style={draw, thick, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=6em,
    draw=blue!40!black!90, very thick,
    text width=10em, 
    minimum height=3.5em, 
    text centered, 
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

\tikzstyle{arrow} = [thick,->,>=stealth]
% adding line numbers
%\usepackage{lineno}
%\linenumbers %Adding line numbers to documents


\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newdefinition{rmk}{Remark}
\newproof{pf}{Proof}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\Q}{\boldsymbol{Q}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\mx}{\mb{x}}
\newcommand{\pB}{\mathfrak{b}}


\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bmu}{\boldsymbol{\mu}}

\newcommand{\mbI}{\mathbf{I}}
\newcommand{\mbT}{\mathbf{T}}
\newcommand{\mbbR}{\mathbb{R}}
\newcommand{\mbbE}{\mathbb{E}}
\newcommand{\mbv}{\mathbf{v}}
\newcommand{\mbW}{\mathbf{W}}
\newcommand{\mbLambda}{\mathbf{\Lambda}}
\newcommand{\mbSigma}{\mathbf{\Sigma}}
\newcommand{\mbU}{\mathbf{U}}
\newcommand{\mbQ}{\mathbf{Q}}
\newcommand{\mbP}{\mathbf{P}}
\newcommand{\norm}[2]{\left\| #1 \right\|_{#2}}

\newcommand{\mb}{\mathbf}

%
%\renewcommand{\baselinestretch}{1.0}
%
%\newcommand{\ql}[1]{{\color{magenta}{#1}}}
\newcommand{\ql}[1]{{\color{violet}{#1}}}
\newcommand{\xw}[1]{{\color{blue}{#1}}}
\newcommand{\revs}[1]{{\color{blue}{#1}}}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\renewcommand{\mathbf}{\boldsymbol}
\newcommand{\xs}{\mathbb}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\hyphenation{MATLAB}
\newcommand{\comm}[1]{}



% remove priprint footnote
\makeatletter
\def\ps@pprintTitle{%
   \let\@oddhead\@empty
   \let\@evenhead\@empty
   \let\@oddfoot\@empty
   \let\@evenfoot\@oddfoot
}
\makeatother


\modulolinenumbers[5]

\journal{Journal of Computational Physics}
%\journal{}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-name\usepackage{tikz,amsmath}s}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}
		
		%\title{Deep density approximation with dimension reduction for high-dimensional Bayesian inverse problems}
            \title{Dimension-reduced KRnet maps for high-dimensional Bayesian inverse problems}
		%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}
		
		%%% Group authors per affiliation:
		%\author{Elsevier\fnref{myfootnote}}
		%\address{Radarweg 29, Amsterdam}
		%\fntext[myfootnote]{Since 1880.}
		
		%% or include affiliations in footnotes:
		\author[mymainaddress]{Yani Feng}
		\ead{fengyn@shanghaitech.edu.cn}
		\author[mysecondaryaddress]{Kejun Tang}
		\ead{tangkejun@icode.pku.edu.cn}
		
		\author[mythirdaddress]{Xiaoliang Wan}
		\ead{xlwan@lsu.edu}
		
		\author[mymainaddress]{Qifeng Liao\corref{mycorrespondingauthor}}
		\cortext[mycorrespondingauthor]{Corresponding author}
		\ead{liaoqf@shanghaitech.edu.cn}
		
		\address[mymainaddress]{School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China}
		\address[mysecondaryaddress]{Changsha Institute for Computing and Digital Economy, Peking University, Changsha 410205, China}
		\address[mythirdaddress]{Department of Mathematics and Center for Computation 
			and Technology, 
			Louisiana State University, Baton Rouge 70803, USA}
	
		
		\begin{abstract}
We present a dimension-reduced KRnet map approach (DR-KRnet) for high-dimensional Bayesian inverse problems, which is based on an explicit construction of a map that pushes forward the prior measure to the posterior measure in the latent space. 
% We present a new Bayesian inference approach for high-dimensional inverse problems, which is based on an explicit construction of a map that pushes forward the prior measure to the posterior measure in the latent space.  
Our approach consists of two main components: data-driven VAE prior and density approximation of the posterior of the latent variable. 
In reality, it may not be trivial to initialize a prior distribution that is consistent with available prior data; in other words, the complex prior information is often beyond simple hand-crafted priors. 
We employ variational autoencoder (VAE) to approximate the underlying distribution of the prior dataset, which is achieved through a  latent variable and a decoder. Using the decoder provided by the VAE prior, we reformulate the problem in a low-dimensional latent space. In particular, we seek an invertible transport map given by KRnet to approximate the posterior distribution of the latent variable. 
%In order to obtain a more expressive map, the normalizing flows are considered. However, the parameters of interest can be of high dimensionality in many practical problems, which renders the direct application of normalizing flows less effective. 
%Aside from the computational considerations, it is not trivial to initialize a prior distribution that is consistent with available prior datasets, i.e., the complex prior information cannot be easily captured by hand-crafted priors. 
%A data-driven prior called the VAE prior is built based on the datasets to address the challenge of choosing an appropriate prior.
%The VAE prior incorporates the dimension reduction given by VAE, which provides a general prior represented by deep neural networks instead of hand-crafted priors.
%Especially, the VAE-GAN priors can be viewed as a dimension reduction technique and generate more realistic samples %than VAE-priors. 
%Then in the low-dimensional latent space, an invertiable transport map, called KRnet map, is established to approximate the posterior distribution of the latent variable. 
Moreover, an efficient physics-constrained surrogate model without any labeled data is constructed to reduce the computational cost of solving both forward and adjoint problems involved in likelihood computation.
%at each optimization step. 
With numerical experiments, we demonstrate the accuracy and efficiency of DR-KRnet for high-dimensional Bayesian inverse problems.
		\end{abstract}
		
		\begin{keyword}
   dimension reduction; 
  KRnet;
			Bayesian inference;  VAE priors. 
		\end{keyword}
		
	\end{frontmatter}


\section{Introduction}\label{section_intro}
\input{section11}
\section{Bayesian inverse problems}\label{section_problem}
\input{section22}
\section{Dimension-reduced KRnet maps}\label{section_method}
\input{section33}
\section{Numerical experiments}\label{section_experiments}
\input{section44}
\section{Conclusions}\label{section_conclude}
We have presented a dimension-reduced KRnet map approach (DR-KRnet) for high-dimensional Bayesian inverse problems, which applies the KRnet to construct an invertible transport map from the prior to the posterior in the low-dimensional latent space of a VAE prior. The key idea of our approach is to employ a deep generative model, called KRnet, to approximate the posterior distribution in the latent space, which allows this approach to incorporate the dimension reduction technique into the Bayesian framework. In this way, the proposed approach can be suitable for practical problems when we only have access to high-dimensional prior data. With the aid of KRnet, our approach can provide an effective and efficient algorithm for both probability approximation and sample generation of posterior distributions. Numerical experiments illustrate that DR-KRnet can solve high-dimensional Bayesian inverse problems. Overall, inference with KRnet maps conducts with greater reliability and efficiency than MCMC, particularly in high-dimensional Bayesian inverse problems. Several promising avenues exist for future work. First, VAE is easy to train and we can couple DR-KRnet with information theory to design new data-driven priors. Second, we can apply our approach to more challenging problems such as petroleum reservoir simulation.


\bigskip
\textbf{Acknowledgments:}
The authors thank Yingzhi Xia for helpful suggestions and
discussions.

\bigskip
\textbf{Funding:}
Y. Feng and Q. Liao are supported by the National Natural Science Foundation of China (No. 12071291), 
the Science and Technology Commission of Shanghai Municipality (No. 20JC1414300), and the Natural Science Foundation of Shanghai (No. 20ZR1436200). K. Tang is supported by the China Postdoctoral Science Foundation under grant 2022M711730,
and X. Wanâ€™s work was supported by the National Science Foundation under grant DMS-1913163.

\begin{appendix}
\section{The neural network architecture of VAE priors}\label{vae_nn}
In section \ref{vae_gan_section}, convolutional neural networks (CNN) are applied to construct the encoder and decoder of VAE priors. Table \ref{vae_ar} presents the neural network architectures of VAE priors, where the dimension of latent variables in test problem 1-2 is $d=36$ and $d=64$, respectively.
\begin{table}[h]
    \centering
    \small
    \caption{The neural network architecture of VAE priors for test problem 1--2.}
     \label{vae_ar}
    \begin{tabular}{|c|c|}
    \hline
       Encoder  & Decoder \\
    \hline
       Input: $y$  & Input: $x$ \\
    \hline
    BatchNormalization&Dense($8*8*48$, activation=`relu')\\
    \hline
    Conv2D(16,2,2,activation=`relu')&Reshape((48, 8, 8))\\
    \hline
    BatchNormalization&BatchNormalization\\
    \hline
    Conv2D(16,3,1,padding=`same',activation=`relu')&Conv2DTranspose(64,3,2,padding=`same',activation=`relu')\\
    \hline
    BatchNormalization&BatchNormalization\\
    \hline
    Conv2D(32,2,2,activation=`relu')&Conv2DTranspose(64,3,1,padding=`same',activation=`relu')\\
    \hline
    BatchNormalization&BatchNormalization\\
    \hline
    Conv2D(32,3,1,padding=`same',activation=`relu')&Conv2DTranspose(32,3,2,padding=`same',activation=`relu')\\
    \hline
    BatchNormalization&BatchNormalization\\
    \hline
    Conv2D(64,2,2,activation=`relu')&Conv2DTranspose(32,3,1,padding=`same',activation=`relu')\\
     \hline
    BatchNormalization&BatchNormalization\\
    \hline
Conv2D(64,3,1,padding=`same',activation=`relu')&Conv2DTranspose(16,3,2,padding=`same',activation=`relu')\\
    \hline 
    Flatten&BatchNormalization\\
    \hline 
Dense($2d$)&Conv2DTranspose(16,3,1,padding=`same',activation=`relu')\\
    \hline
    \multirow{3}{*}{
    Output: $\left(\mu_{en},\log\left(\sigma_{en}^2\right)\right)$}&BatchNormalization\\
    %\hline 
    &Conv2DTranspose(2,3,1,padding=`same')\\
    %\hline 
    &Output: $\left(\mu_{de},\log\left(\sigma_{de}^2\right)\right)$\\
    \hline
    \end{tabular}
   
\end{table}
\section{The neural network architecture of physics-constrained surrogate model}\label{surrogate_nn}
In this paper, we apply convolutional neural networks (CNN) for physics-constrained surrogate model. The neural network architectures of the surrogate are listed in Table \ref{surrogate_archi_table}. For Darcy flows, the equation loss and boundary loss of the loss function \eqref{surrogate_loss} are defined as:
\begin{align}
    {\left\Arrowvert \mathcal{R}\left(\hat{\mathcal{F}}_{\theta}\left(y^{(i)}\right),y^{(i)}\right)\right\Arrowvert}_2^2={\left\Arrowvert \nabla \cdot\tau\left(y^{(i)}\right)-h\right\Arrowvert}_2^2 +  {\left\Arrowvert\tau\left(y^{(i)}\right)+\exp\left(y^{(i)}\right)\nabla u\left(y^{(i)}\right)\right\Arrowvert}_2^2,\\
    {\left\Arrowvert\mathcal{B}\left(\hat{\mathcal{F}}_{\theta}\left(y^{(i)}\right)\right)\right\Arrowvert}_2^2 =  {\left\Arrowvert u\left(y^{(i)}\right)\right\Arrowvert}_2^2 +  {\left\Arrowvert \exp\left(y^{(i)}\right)\nabla u\left(y^{(i)}\right)\cdot \mathbf{n}\right\Arrowvert}_2^2.    
\end{align}
In addition, the weight $\beta$ in  \eqref{surrogate_loss} set to 100 for test problem 1--2.
\begin{table}[H]
    \centering
    \caption{The neural network architecture of PDE surrogate for test problem 1--2.}
     \label{surrogate_archi_table}
    \begin{tabular}{|c|c|}
    \hline
        Networks& Feature maps  \\
    \hline 
    Input: $y$ &$(1,64,64)$\\
    \hline
    Conv2D&(48,2,2,activation=`relu')\\
    \hline 
    Conv2D&(144,3,1,padding=`same',activation=`relu')\\
    \hline 
    Conv2D&(72,2,2,activation=`relu')\\
    \hline 
    Conv2D&(200,3,1,padding=`same',activation=`relu')\\
    \hline 
    UpSampling2D&2\\
    \hline 
    Conv2D&(100,3,1,padding=`same',activation=`relu')\\
    \hline 
    Conv2D&(196,3,1,padding=`same',activation=`relu')\\
    \hline 
    UpSampling2D&2\\
    \hline 
    Conv2D&(3,3,1,padding=`same',activation=`relu')\\
    \hline
    Output: $(u(y),\tau_1,\tau_2)$ &$(3,64,64)$   \\ \hline
    \end{tabular}
\end{table}

% \section{MCMC algotithm}\label{pcn_alg}
% In this paper, we apply the following pCN-MCMC algorithm to sample the posterior of the latent variables, which is the baseline for our DR-KRnet. More details can be found in Algorithm \ref{pcn_compute}.
% \begin{algorithm}[h]
% 	\caption{The pCN-MCMC algorithm with VAE priors}
%  \label{pcn_compute}
% 	\begin{algorithmic}[1]
% 		\Require The likelihood $\pi(\mathcal{D}_{obs}|x)$, pre-trained decoder $p_{y|x,\theta^*}=\mathcal{N}\left(\mu_{de,\theta^*}\left(x\right), \text{diag}\left(\sigma_{de,\theta^*}^{\odot 2}\left(x\right)\right)\right)$, pre-trained surrogate model $\hat{\mathcal{F}}_{\Theta^*}$, chain length $N_{pcn}$, the number of samples from the posterior $N_s$, the hyperparameter $\gamma$.
%             \State Draw a sample $x^{(1)}$ from  $\mathcal{N}(0,\mathbf{I})$.
% 		\For {$i = 1:N_{pcn}$}
% 		\State Given an appropriate step size $\gamma$, propose 
%   $$
%   x^*=\sqrt{1-\gamma^2}x^{(i)}+\gamma \zeta, \quad, \zeta \sim \mathcal{N}(0,\mathbf{I}).
%   $$
% 		\State Compute the acceptance ratio
%   $$
%   \alpha=\text{min}\left(1,\frac{\pi(\mathcal{D}_{obs}|x^*)}{\pi(\mathcal{D}_{obs}|x^{(i)})}\right),
%   $$
%   where the likelihood $\pi(\mathcal{D}_{obs}|\cdot)\approx \pi(\mathcal{D}_{obs}|\mu_{de,\theta^*}(\cdot),\cdot)$ is computed through the decoder and the forward model.
% 		\State Draw $\rho\sim U[0,1]$.
%          \If {$\rho\leq \alpha$}
% 		\State Let $x^{(i+1)}=x^*$.
% 		\Else
% 		\State Let $x^{(i+1)}=x^{(i)}$.
% 		\EndIf
% 		\EndFor
%             \State Posterior samples $y^{(i-N_{pcn}+N_s)}=\mu_{de,\theta^*}\left(x^{(i)}\right)$, for $i=N_{pcn}-N_s+1,\dots,N_{pcn}$ .
% 		\Ensure posterior samples $\{y^{(i)}\}_{i=1}^{N_s}$.
% 	\end{algorithmic}
%\end{algorithm}
\end{appendix}

%\section*{References}

\bibliography{feng}

\end{document}