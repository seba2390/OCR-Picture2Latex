\section{Pretraining via Gaussian Mixture Models}\label{gmm}
% In statistics, a mixture model is used for modeling subpopulations within an overall population. For example, when we consider the weight of dogs, we may expect that the dogs of the same breed have similar weights. It's very natural to do clustering based on weights first which may imply the breed of the dog. Then this prediction of the breed may contribute a lot to the downstream task learning. 
% \chijin{If we run out of space, we don't need to introduce GMM using the examples.}
In this section, we show how pretraining using Gaussian Mixture Models (GMMs) can benefit the downstream classification tasks, under our theoretical framework.

\paragraph{Model setup.}
For the latent variable model, we consider a $d$-dimensional GMM with $K$ components and equal weights. To be specific, the latent variable $z$ that represents the cluster is sampled uniformly from $[K]$. In each cluster, the data is sampled from a standard Gaussian distribution, i.e., $x|z=i\sim\mN(u^* _i,I_d)$ for any $i\in[K]$. It then holds that
\begin{equation*}
x\sim\sum^K_{i=1}\frac{1}{K}\mN(u^* _i,I_d).
\end{equation*}
% which is a $d$-dimensional, $K$-component, equally weighted standard Gaussian mixture model (GMM).
We denote by $\mathcal{U}$ the parameter space with each element consisting of $K$ centers ($d$-dimensional vectors). 

We assume that the set of parameters $\mathcal{U}$ satisfies the normalization condition---there exists $D>0$ such that for any $\mathbf{u}=\{u_i\}^K_{i=1}\in\mathcal{U}$, we have $\|u_i\|_2\leq D\sqrt{d\log K},~\forall i\in[K]$. We further assume the ground-truth centers $\{u^* _i\}^K_{i=1}\in\mathcal{U}$ satisfy the following separation condition.
% \chijin{missing definition of $\mathcal{U}$.}
% \begin{assumption}\label{gmm_bound}
% There  exists $D$ such that for any $\mathbf{u}=\{u_i\}^K_{i=1}\in\mathcal{U}$, it holds that
% \$
% \|u_i\|_2\leq D\sqrt{d\log K},~\forall i\in[K].
% \$
% \end{assumption}

\begin{assumption}[Separation condition]\label{gmm_separation}
The true parameters $\{u^* _i\}^K_{i=1}\in\mathcal{U}$ satisfies
\$
\|u^* _i-u^* _j\|_2\geq 100\sqrt{d\log K},~\forall i\neq j.
\$
% We also assume there exists $D>0$ such that for any $\mathbf{u}=\{u_i\}^K_{i=1}\in\mathcal{U}$, it holds that
% \$
% \|u_i\|_2\leq D\sqrt{d\log K},~\forall i\in[K].
% \$
\end{assumption}

For the downstream task, we consider the binary classification problems with label $y\in\{0,1\}$. We denote by $\Psi$ the set of $2^K$ classifiers such that for each $\psi\in\Psi$, and any $i \in [K]$, we have either $\P_{\psi}(y=1|z=i)=1-\varepsilon$ or $\P_{\psi}(y=0|z=i)=1-\varepsilon$, 
% we have for any $i\in[K]$
% \$
% &\P_{\psi}(y=1|z=i)=1-\varepsilon, ~~\P_{\psi}(y=0|z=i)=\varepsilon\notag\\
% {\rm or}\quad&\P_{\psi}(y=0|z=i)=1-\varepsilon, ~~\P_{\psi}(y=1|z=i)=\varepsilon,
% \$
where $\varepsilon$ represents the noise. Then, the latent variable model and the prediction class are represented by $\mathcal{U}$ and $\Psi$, respectively. In the sequel, we consider the case where no side information is available, i.e., we only have access to i.i.d unlabeled data $\{x_i\}^m_{i=1}$ and i.i.d labeled data $\{x_j,y_j\}^n_{j=1}$. For classification problems, it is natural to consider the $0-1$ loss function $\ell(x,y):=\mathds{1}_{\{x\neq y\}}$ which is bounded by $1$.

% In total, we have $2^K$ such classifications. We denote by $\Psi$ the set of all these $2^K$ classifications. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\paragraph{Informative condition.} We prove that Assumption \ref{invariance} for the above model. We have the following guarantee.

\begin{lemma}\label{gmm_ti} 
Let $\tilde{\mathcal{U}} = \{\bu\in\mathcal{U} ~|~ \TV(p_{\bu}(x),p_{\bu^* }(x))\leq 1/(4K)\}$.
% Then, 
% For any $\bu\in\mathcal{U}$ such that $\TV(p_{\bu}(x),p_{\bu^* }(x))\leq 1/4K$, 
Under Assumption \ref{gmm_separation}, GMMs with parameters in $\tilde{\mathcal{U}}$ is $\mathcal{O}(1)$-informative with respect to the transformation group induced by downstream classification tasks.
% GMMs with classification as downstream tasks is $\mathcal{O}(1)$-informative.
\end{lemma}

% \chijin{This lemma is a bit sloppy on its claim. I don't understand what it precisely means. Is $\TV(p_{\bu}(x),p_{\bu^* }(x))\leq 1/4K$ the definition of $\mathcal{U}$, how do we ensure this precondition holds?}


% \begin{lemma}\label{gmm_ti}
% Let $\mathcal{T}:=\{\sigma:[K]\rightarrow [K]\,|\, \sigma{\rm  ~is~ a ~permutation~ of~ the~ set ~}[K]\}$. Suppose that Assumption \ref{gmm_separation} holds. Then, for any $\bu\in\mathcal{U}$ that satisfies $\TV(p_{\bu}(x),p_{\bu^* }(x))\leq 1/4K$, there exist $\sigma\in\mathcal{T}$ such that
% \$
% \TV\big(\P_{\sigma(\bu)}(x,z),\P_{\bu^* }(x,z)\big)\leq c_2\cdot\TV\big(\P_{\bu}(x),\P_{\bu^* }(x)\big).
% \$
% Here $c_2$ is an absolute constant and $\sigma(\bu):=\{u_{\sigma(1)}, u_{\sigma(2)},\ldots,u_{\sigma(K)}\}.$ More specifically, we have $\P_{\sigma(\bu)}(x|z=i)\sim\mN(u_{\sigma(i)},I_d)$ for any $i\in[K]$.
% \end{lemma}
% The translation invariance property for GMM (Lemma \ref{gmm_ti}) mainly shows that, if the density estimation task of GMM is done well, then in fact we achieve success on parameter estimation as well (up to permutation of parameters).


% We have the following bracketing number bound.
% \begin{lemma}\label{gmm_bn}
% Let
% \$
% \mP(\mathcal{U}):=\bigg\{\sum^K_{i=1}\frac{1}{K}\mN(u_i,I_d)\,\bigg|\,\bu=\{u_i\}^K_{i=1}\in\mathcal{U}\bigg\}.
% \$
% Under Assumption \ref{gmm_bound}, the entropy can be bounded as follows,
% \$
% \log N\big(\mP(\mathcal{U}),1/m\big)\leq 2dK\log(6mdKD).
% \$
% \end{lemma}

% Given labeled data $\{x_j,y_j\}^{n}_{j=1}$ and the pretrained $\hat \bu$, the function class
% \$
% \big\{(\mathds{1}_{g_{\hat\bu,\psi}(x_1)\neq y_1},\ldots,\mathds{1}_{g_{\hat\bu,\psi}(x_n)\neq y_n})\,\big|\,\psi\in\Psi\big\}
% \$
% is a finite function class, whose Rademacher complexity can be bounded by the following lemma.

% \begin{lemma}\label{gmm_rc}
% Let $A = \{a^{1},\ldots,a^{N}\}$ be a finite set of vectors in $\R^n$. Then, the Rademacher complexity can be bounded as follows,
% \$
% R_n(A)\leq \max_{a\in A}\|a\|_2\cdot\frac{2\sqrt{2\log N}}{n}.
% \$
% \end{lemma}

% Lemma \ref{gmm_rc} upper bounds the empirical Rademacher complexity of the function class
% $\ell\circ\mathcal{G}_{\bu,\Psi}$ defined in Theorem \ref{error_bound}, which leads to the upper bound of Rademacher complexity for $\ell\circ\mathcal{G}_{\bu,\Psi}$. By Theorem \ref{error_bound}, 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Theoretical results}
We have the following theoretical guarantee.
\begin{theorem}\label{gmm_main}
Let $\hat\bu,\hat\psi$ be the outputs of Algorithm \ref{mle+erm}. Suppose that Assumption \ref{gmm_separation} holds and $m=\tilde{\Omega}(dK^3)$. Then, for the Gaussian mixture model with classification as downstream tasks, with probability at least $1-\delta$, the excess risk can be bounded as follows,
\begin{equation*}
{\rm Error}_{\ell}(\hat\bu,\hat\psi)\leq\tilde{\mathcal{O}}\bigg(\sqrt{\frac{dK}{m}}+\sqrt{\frac{K}{n}}\bigg),
\end{equation*}
Here $\tilde{\mathcal{O}}(\cdot)$ omits some constants and the polylogarithmic factors in $m, d, K, D, 1/\delta$.
\end{theorem}

Theorem \ref{gmm_main} shows the power of unsupervised pretraining under this setting in the following sense: Note that the number of parameters of a GMM is $dK$, therefore if we directly do classification without unsupervised pretraining, the risk will scale as $\tilde{\mathcal{O}}(\sqrt{dK/n})$. When $d$ is large and $m \gg n$, we achieve a better risk bound than supervised learning that only uses the labeled data.

%\chijin{I removed $\kappa$ bc it's constant.}