\section{Pretraining via Factor Models}\label{factor_model}

High-dimensional data is very common in modern statistics and machine learning, and we often suffer from the curse of dimensionality when directly analyzing data in high-dimensional spaces. To tackle the problem, we usually assume that high-dimensional data has some low-dimensional structures. One of the widely studied models in this setting is the factor model, which models the high-dimensional measurements by low-rank plus sparse structures in data matrices to decorrelate the covariates. Learning this latent structure falls into the framework of unsupervised statistical learning. In this section, we instantiate our theoretical framework using the factor model with linear regression as a downstream task. We rigorously show how unsupervised pretraining can help reduce sample complexity in this case.

\paragraph{Model Setup.}
Factor model \citep[see, e.g.,][]{lawley1971factor,bai2002determining,forni2005generalized,fan2021robust} is widely used in finance, computational biology, and sociology, where the high-dimensional measurements are strongly correlated. For the latent variable model, we consider the factor model with standard Gaussian components, which is defined as follows.
\begin{definition}[Factor Model]
Suppose that we have $d$-dimensional random vector $x$, whose dependence is driven by $r$ factors $z$. The factor model assumes 
%\%\label{092601}
\$
x=B^* z+\mu,
\$
where $B^* $ is a $d\times r$ factor loading matrix. Here $\mu\sim N(0, I_d)$ is the idiosyncratic component that is uncorrelated with the common factor $z\sim N(0,I_r)$. We assume that the ground truth parameters $B^* \in\mathcal{B}$, where $\mathcal{B}:=\{B\in\R^{d\times r}\,|\,\|B\|_2\leq D\}$ for some $D>0$.
\end{definition}
%\chijin{change $^* $ across the paper to $^* $.}

For the downstream task, we assume that the latent factors $z$ influence on the response $y$ in a similar manner as on $x$ and hence consider the following linear regression problem
%\%\label{092602}
\$
y = \beta^{* T}z+\nu,
\$
where $\nu\sim N(0,\varepsilon^2)$  is a Gaussian noise that is uncorrelated with the factor $z$ and the idiosyncratic component $\mu$. We assume that the ground truth parameters $\beta^* \in\mathcal{C}$, where $\mathcal{C}:=\{\beta\in\R^r\,|\,\|\beta\|_2\leq D\}$ for some $D>0$. The latent variable model (i.e., $\Phi$) and the the prediction class (i.e.,$\Psi$) are then represented by $\mathcal{B}$ and $\mathcal{C}$, respectively. In the sequel, we consider the case where no side information is available, i.e., we only have access to i.i.d unlabeled data $\{x_i\}^m_{i=1}$ and i.i.d labeled data $\{x_j,y_j\}^n_{j=1}$.

For regression models, it is natural to consider the squared loss function $\ell (x,y):=(y-x)^2$.
Then, the optimal predictor $g_{B,\beta}$ under the distribution $\P_{B,\beta}$ has the following closed form solution,
\$
g_{B,\beta}(x)=\E_{\P_{B,\beta}}[y\,|\,x]=\beta^{T}B^{T}(BB^T+ I_d)^{-1}x.
\$
And the excess risk is now defined as
%\%\label{092604}
\$
{\rm Error}_{\ell}(\hat B,\hat\beta):=\E_{\P_{B^* ,\beta^* }}\big[\big(y-g_{\hat B,\hat\beta}(x)\big)^2\big]-\E_{\P_{B^* ,\beta^* }}\big[\big(y-g_{B^* ,\beta^* }(x)\big)^2\big].
\$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Informative condition.}
We first show that Assumption \ref{invariance} holds for the factor model with linear regression as downstream tasks. The idea of the factor model is to learn a low-dimensional representation $z$, where a rotation over $z$ is allowed since in the downstream task, we can also rotate $\beta$ to adapt to the rotated $z$. 


\begin{lemma}\label{factor_ti}
Factor model with linear regression as downstream tasks is $\kappa^{-1}$-informative, where
\$
\kappa=\frac{c_1(\sigma^* _{\max}+1)^4}{(\sigma^* _{\min})^3}.
\$
Here $c_1$ is some absolute constants, $\sigma^* _{\max}$ and $\sigma^* _{\min}$ are the largest and smallest singular value of $B^* $, respectively.
\end{lemma}


% Translation invariance (Assumption \ref{invariance}) can be verified for factor model through the following lemma.

% \begin{lemma}\label{factor_ti}
% Let $\mathcal{O}:=\{O\in\R^{r\times r}\,|\, O^T O=OO^T=I_r\}$, which can be seen as a transformation group defined on $\mathcal{B}$ and $\mathcal{C}$. Then, the following properties hold:
% \begin{itemize}
%     \item For any $(B,\beta,O)\in\mathcal{B}\times\mathcal{C}\times\mathcal{O}$, it holds that
%     \$
%     \P_{B,\beta}(x,y)=\P_{BO,O^T\beta}(x,y).
%     \$
%     \item For any $B\in\mathcal{B}$, there exists $O\in\mathcal{O}$ such that
%     \$
%     \TV\big(\P_{BO}(x,z),\P_{B^* }(x,z)\big)\leq \frac{c_1(\sigma^* _{\max}+1)^4}{(\sigma^* _{\min})^3}\cdot \TV(p_{B}(x),p_{B^* }(x)),
%     \$
%     where $c_1$ is an absolute constant. Here we denote by $\sigma^* _{\max}$ and $\sigma^* _{\min}$ the largest and smallest singular value of $B^* $, respectively.
% \end{itemize}
% \end{lemma}



% By \eqref{092601}, we have $x\sim\mN(0,BB^T+I_d)$. Under Assumption \ref{factor_bound}, it holds that $1\leq\|BB^T+I_d\|_2\leq D^2+1$. Based on this observation, we can bound the bracketing number as follows.



% \begin{lemma}\label{factor_bn}
% Let $\mP (\mathcal{B}):=\{\mN (0,BB^T+ I_d)\,|\, B\in\mathcal{B}\}$. Under Assumption \ref{factor_bound}, the entropy can be bounded as follows, 
% \$
% \log N_{\b}(\mP(\mathcal{B}),1/m)\leq 4dr\log\big(24mdr(D^2+1)\big).
% \$
% \end{lemma}


% Note that for fixed $B$ the prediction function class 
% \$
% \mathcal{G}_{B,\mathcal{C}}:=\big\{g_{B,\beta}(x)=\beta^{T}B^{T}(BB^T+\sigma^2 I_d)^{-1}x\,\big|\,\beta\in\mathcal{C}\big\}
% \$
% belongs to a linear hypothesis class. Thus, its Rademacher complexity can be bounded by the following lemma.
% \begin{lemma}\label{factor_rc}
% For a linear hypothesis class $\mathcal{H}=\{h_{\beta}(x)=\beta^{T}x\,|\,\beta\in\R^r, \|\beta\|_2\leq D\}$, where $x\in\R^r$ and $\|x\|_2\leq X$, the empirical Rademacher complexity can be bounded as follows,
% \$
% \hat R_n(\mathcal{H})\leq \frac{2DX}{\sqrt{n}}.
% \$
% \end{lemma}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Theoretical results.}
Recall that in Theorem \ref{error_bound}, we assume a $L$-bounded loss function to guarantee the performance of Algorithm \ref{mle+erm}. Thus, instead of directly applying Algorithm \ref{mle+erm} to the squared loss function, we consider Algorithm \ref{mle+erm} with truncated squared loss, i.e.,
\begin{equation}\label{eq:truncated_loss}
\tilde{\ell} (x,y) := (y-x)^{2}\cdot \mathds{1}_{\{(y-x)^2\leq L\}} + L \cdot \mathds{1}_{\{(y-x)^2 > L\}}.
\end{equation}
Here $L$ is a carefully chosen truncation level. To be more specific, in the first phase, we still use MLE to learn an estimator $\hat B$ as that in line 2 of Algorithm \ref{mle+erm}. In the second phase, we apply ERM to the truncated squared loss to learn an estimator $\hat\beta$, i.e.,
\$
\hat\beta\leftarrow\argmin_{\beta\in\mathcal{C}}\sum^n_{j=1}\tilde{\ell}\big(g_{\hat B,\beta}(x_j),y_j\big).
\$
We then have the following theoretical guarantee.
\begin{theorem}\label{factor_main}
We consider Algorithm \ref{mle+erm} with truncated squared loss \eqref{eq:truncated_loss}
% \begin{align*}
% \tilde{\ell} (x,y) := (y-x)^{2} \cdot\mathds{1}_{\{(y-x)^2\leq L\}} + L \cdot \mathds{1}_{\{(y-x)^2 > L\}},
% \end{align*}
with $L=(D^{2}+1)^{3}\log n$. Let $\hat B, \hat\beta$ be the outputs of Algorithm \ref{mle+erm}. Then, for factor models with linear regression as downstream tasks, with probability at least $1-\delta$, the excess risk can be bounded as follows,
\$
{\rm Error}_{\ell}(\hat B,\hat\beta)\leq \Tilde{\mathcal{O}}\bigg(\kappa L \sqrt{\frac{dr}{m}}+L\sqrt{\frac{r}{n}}\bigg),
\$
where $D$ is defined in the sets ${\cal B}$ and ${\cal C}$, and $\kappa$ is specified in Lemma \ref{factor_ti}.
% $\kappa={c_1(\sigma^* _{\max}+1)^4}/{(\sigma^* _{\min})^3}$ for some absolute constants $c_1$. 
Here $\tilde{\mathcal{O}}(\cdot)$ omits absolute constants and the polylogarithmic factors in $m, d, r, D, 1/\delta$.
\end{theorem}

Notice that the rate we obtain in Theorem \ref{factor_main} is not optimal for this specific task: by the nature of squared loss, if we consider a direct $d-$dimensional linear regression (from $x$ to $y$) with $n$ data, we can usually achieve the fast rate, where excess risk decreases as $\tilde{\mathcal{O}}(d/n)$. To fill this gap, we consider Algorithm \ref{mle+erm} with $\Phi=\R^{d\times r}$ and $\Psi=\R^{r}$ and denote $D:=\max\{\|B^* \|_2,\|\beta^* \|_2\}$. Following a more refined analysis other than using a uniform concentration technique (which is suitable for general problems but not optimal in this specific task), we achieve the following theoretical guarantee with a sharper risk rate: 
%Theorem \ref{factor_main} shows the benefit of unsupervised pretraining in the following sense. The price paid for learning the loading matrix is $\Tilde{\mathcal{O}}(\sqrt{dr/m}) $, which is small when $m$ is very large. Notice that, since $x$ is a $d$-dimensional vector, the usual linear regression will have an error of $\Tilde{\mathcal{O}}(\sqrt{d/n})$. In the error bound provided by Theorem \ref{factor_main},  the error related to $n$ scales as $\Tilde{\mathcal{O}}(\sqrt{r/n})$. Usually $d \gg r$, since $r$ is the factor dimension which is assumed to be low-dimensional. Then when $m \gg n$, the error bound $\Tilde{\mathcal{O}}(\sqrt{dr/m} + \sqrt{r/n})$ is much better than $\Tilde{\mathcal{O}}(\sqrt{d/n})$.


\begin{theorem}[Fast rate] \label{factor_fast_rate}
Let $\hat B, \hat\beta$ be the outputs of Algorithm \ref{mle+erm}. Then, if $m \gtrsim (D^{2}+1)^{2} d \log (1/\delta)$, $n \gtrsim (D^{2}+1)^{2} r \log (1/\delta)$, for factor models with linear regression as downstream tasks, with probability at least $1-\delta$, the excess risk can be bounded as follows, 
\begin{align*}
{\rm Error}_{\ell}(\hat B,\hat\beta) \leq \mathcal{O} \bigg((D^{2}+1)^{6}(D^{4}+\sigma_{\min}^{* -4})\frac{d \log (1/\delta)}{m}+ (D^{2}+1)^{2}\frac{r \log(4/\delta)}{n }\bigg).
\end{align*}
Here $\mathcal{O}(\cdot)$ omits some absolute constants.
\end{theorem}

%\chijin{remember to change prediciton error in all texts to excess risk}

Theorem \ref{factor_fast_rate} shows the benefit of unsupervised pretraining in the following sense. Assuming $D$ and $\sigma^{*}_{\min}$ are both constants. The price paid for learning the loading matrix is $\Tilde{\mathcal{O}}(d/m) $, which is small when $m$ is very large. Notice that, since $x$ is a $d$-dimensional vector, the usual linear regression will have a risk of $\Tilde{\mathcal{O}}(d/n)$. In the risk bound provided by Theorem \ref{factor_fast_rate},  the risk related to $n$ scales as $\Tilde{\mathcal{O}}(r/n)$. Usually, the factor is assumed to be low-dimensional compared with the input ($d \gg r$). Then when $m \gg n$, the risk bound $\Tilde{\mathcal{O}}(d/m + r/n)$ is much better than $\Tilde{\mathcal{O}}(d/n)$.


% Usually $d \gg r$, since $r$ is the factor dimension which is assumed to be low-dimensional.