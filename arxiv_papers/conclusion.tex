In this article, we have presented a principled approach to doing MCMC-based inference in general tractable Feynman--Kac models. At core, the method corresponds to augmenting the model by introducing an artificial observation model, and then proceeding to sample from the augmented model using a two step approach: first sample the observations conditionally on a trajectory, and second, sample from a MCMC kernel keeping the trajectory conditional distribution invariant.

To summarise, we have described two versions of this class of samplers. A first one, which we coined auxiliary Kalman sampler can be seen as an extension/specialisation of \citet{titsias2018} to models with latent dynamics, and is particularly useful when the latent model is quasi-Gaussian and of relatively small dimension. The second class, which considers using conditional SMC to sample the trajectory conditional to the auxiliary observations, can be seen as a generalisation of \citet{finke2021csmc}. Importantly, we have shown that both methods introduced could be parallelised across time steps on hardware such as GPUs, while retaining good statistical properties: the sequential and parallel versions of the auxiliary Kalman sampler are fully statistically equivalent, while the particle Gibbs ones are not, but the parallel-in-time auxiliary particle Gibbs does not suffer from worse mixing properties, in particular when run time is taken into account.

At least three classes of latent Markovian models elude our auxiliary Kalman samplers:
\begin{enumerate}
    \item Models with intractable transition or observation density, for which we cannot compute the unnormalised smoothing density.
    \item Models with multi-modal posteriors, which are hard for MCMC methods in general due to the ``local'' perspective they take. This can, however, be handled by combining the method with meta-algorithms, such as parallel tempering~\citep{geyer1991markov}.
    \item Models with very non-Gaussian latent dynamics or observations, such as those exhibiting multiplicative noise or presenting boundary constraints akin to discontinuities.
\end{enumerate}
Another, softer, issue is concerned with the computational complexity of the method with respect to the dimension of the latent space $d_X$. Indeed, because Kalman filtering and backward sampling relies on recursive Gaussian conditioning, it requires to compute matrices inverses of size $d_X \times d_X$ (or more precisely, to solve systems of the same size). In models where no specific structure alleviates these computations, they can quickly become computationally overwhelming as the dimension of the latent space increases.

Replacing the LGSSM proposal of Section~\ref{subsec:auxiliary-lgssm} by a local conditional SMC update as per Section~\ref{sec:pgibbs_samplers} allowed us to trade the single expensive computation for a quick computation across several particles. This also partially alleviates the other issues with Kalman approximations. However, the usual issues with cSMC remain: several trajectories need to be simulated, and the fully adapted auxiliary cSMCs of Section~\ref{subsec:guided-smc-sampler} cannot be parallelised-in-time. They however do not solve the problem of intractable densities, or multimodality.

While the complexity issue is somewhat mitigated by the auxiliary particle Gibbs samplers, it still remains to some extent, in particular when using dynamics-adapted proposals, which require computing a Kalman gain matrix, thereby involving an inversion of a $d_x \times d_x$ matrix. A possibility, not explored in this article, is to consider other families of proposal distributions than LGSSMs -- either in the auxiliary Kalman or the auxiliary particle Gibbs case. In particular, ensemble Kalman~\citep{frei2013bridging} approximations can be used as a natural proposal substitute to breach the computational barrier while still retaining asymptotic Monte Carlo exactness. It is likely that such proposals would perform particularly well when they form reasonable local approximations in the first place.

The reformulation of \citet{finke2021csmc} as a conditional SMC within a Gibbs sampler is a particularly promising avenue as it invites the direct application of the many cSMC practical and theoretical technologies developed over the past decade. Our experiments showed that leveraging this representation to design better auxiliary proposal distributions already largely improved the statistical properties of the algorithm at a very a low additional computational cost. Interestingly, the main benefit came from introducing gradients in the proposal, rather than incorporating the dynamics themselves. This is likely because the potentials have been designed to be particularly informative compared to the dynamics (in order to increase the variance of the underlying particle filter). We believe that this can still be improved upon many-fold in a number of settings. A natural first step would be to combine these with methods developed to tackle degeneracy in particle Gibbs~\citep[e.g.][]{lindsten2015degenerate} or very long time series~\citep{Karppinen2022bridge}.


A classical way to improve mixing in particle Gibbs is by means of blocking~\citep[see, e.g.][]{Singh2017blocking}. This is typically done in the time dimension, where the Markovian structure is leveraged to sample independent blocks $(t_1, t_2)$, $(t_3, t_4)$, $\ldots$, conditionally on their extremities. In the case of our auxiliary Kalman samplers, another type of blocking is possible, and consists in implementing a coordinate-wise blocking which is akin to introducing an additional level in the proposed Hastings-within-Gibbs sampler. This is possible due to the fact that, for a coordinate $d$, the posterior distribution of the coordinate path conditionally on the rest of the coordinates is still of the form \eqref{eq:gen-ssm}.

A final remark is concerned with the implementation of the prefix-sum algorithm~\citet{blelloch1989scans} in the JAX library~\citep{jax2018github}. At the time of writing this article, the JAX implementation can be considered high-level, by which we mean that the algorithm is implemented in Python~\citep{Rossum2009Python} rather than natively using the CUDA~\citep{cuda} GPU backend. This is in contrast to other control flow primitives such as loops and "if-else" branching. We do expect that a native implementation of the algorithm, fully GPU-focused would improve the time-performance of the Kalman samplers. 