\section{Conclusions}
This paper proposes a generic theoretic framework for explaining the statistical benefits of unsupervised pretraining.
% helps the learning of downstream tasks. 
We study the natural scheme of using MLE for unsupervised pretraining and ERM for downstream task learning. We identify a natural ``informative'' condition, under which our algorithm achieves an excess risk bound that significantly improves over the baseline achieved by purely supervised learning in the typical practical regimes.
% decouples the complexity of learning representations and the complexity of learning the downstream tasks. 
We further instantiate our theoretical framework with three concrete approaches for unsupervised pretraining and provide corresponding guarantees.
% with three specific problems, factor model with linear regression as downstream tasks, Gaussian mixture models with classification as downstream tasks and contrastive learning with linear regression as downstream tasks.