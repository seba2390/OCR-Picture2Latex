\documentclass[12pt]{article}
\usepackage{times,epsfig,graphics,amssymb,latexsym,amsmath,setspace,fullpage,algorithm,algorithmic,subcaption}
\usepackage{array,url}
\usepackage[usenames]{color}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage[comma, numbers]{natbib}

\usepackage{bm} 
\usepackage{appendix,comment}
%\usepackage[colorlinks,linkcolor=black]{hyperref}
\usepackage{amsthm}
\usepackage{makecell}



\usepackage{xr}
\externaldocument{supplement}

\newcommand{\eA}[1]{\textcolor{red}{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition}
\newtheorem{mydef}{Definition}
\newtheorem{remark}{Remark}


\newcommand{\keywords}[1]{\textbf{Keywords:} #1}
\newcommand{\xc}[1]{\textcolor{blue}{#1}}
\newcommand{\yc}[1]{\textcolor{red}{#1}}


\newcommand{\PA}{{\mathcal Q}}
\newcommand{\OS}{{\mathcal O}}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\arginf{\mathop{\rm arginf}}
\def\Cov{\mathop{\rm Cov}}
\def\Var{\mathop{\rm Var}}
\def\sign{\mathop{\rm sign}}
\def\rank{\mathop{\rm rank}}
\def\Sgn{\mathop{\rm Sgn}}
\def\arg{\mathop{\rm arg}}
\def\err{\mathop{\rm Err}}
\def\E{\mathop{\rm E}}
\def\diag{\mathop{\rm diag}}
\newtheorem{Lemma}{Lemma}
\newtheorem{Definition}{Definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Corollary}{Corollary}
\newtheorem{Assumption}{Assumption}
\newtheorem{Remark}{Remark}
\newcommand {\bfxi} {\mbox{\boldmath $\xi$}}
\newcommand {\bfeta} {\mbox{\boldmath $\eta$}}
\newcommand {\bfphi} {\mbox{\boldmath $\phi$}}
\newcommand {\bfbeta} {\mbox{\boldmath $\beta$}}
\newcommand {\bfalpha} {\mbox{\boldmath $\alpha$}}
\newcommand {\bfdelta} {\mbox{\boldmath $\delta$}}
\newcommand {\bfsigma} {\mbox{\boldmath $\sigma$}}
\newcommand {\bfSigma} {\mbox{\boldmath $\Sigma$}}
\newcommand {\bfDelta} {\mbox{\boldmath $\Delta$}}
\newcommand {\bftheta} {\mbox{\boldmath $\theta$}}
\newcommand {\bfgamma} {\mbox{\boldmath $\gamma$}}
\newcommand {\bflambda} {\mbox{\boldmath $\lambda$}}
\newcommand {\bfepsilon} {\mbox{\boldmath $\epsilon$}}
\newcommand {\bfOmega} {\mbox{\boldmath $\Omega$}}
\newcommand {\bftau} {\mbox{\boldmath $\tau$}}
\newcommand {\bfmu} {\mbox{\boldmath $\mu$}}
\def\ba{\mathop{\bf a}}
\def\bv{\mathop{\bf v}}
\def\bV{\mathop{\bf V}}
\def\bc{\mathop{\bf c}}
\def\bk{\mathop{\bf k}}
\def\bx{\mathop{\bf x}}
\def\bX{\mathop{\bf X}}
\def\bF{\mathop{\bf F}}
\def\bK{\mathop{\bf K}}
\def\bL{\mathop{\bf L}}
\def\by{\mathop{\bf y}}
\def\bY{\mathop{\bf Y}}
\def\bz{\mathop{\bf z}}
\def\bn{\mathop{\bf n}}
\def\bZ{\mathop{\bf Z}}
\def\bs{\mathop{\bf s}}
\def\bt{\mathop{\bf t}}
\def\bS{\mathop{\bf S}}
\def\bg{\mathop{\bf g}}
\def\bB{\mathop{\bf B}}
\def\bA{\mathop{\bf A}}
\def\bC{\mathop{\bf C}}
\def\bG{\mathop{\bf G}}
\def\bW{\mathop{\bf W}}
\def\be{\mathop{\bf e}}
\def\bI{\mathop{\bf I}}
\def\bH{\mathop{\bf H}}
\def\bM{\mathop{\bf M}}
\def\bT{\mathop{\bf T}}
\def\bu{\mathop{\bf u}}
\def\bv{\mathop{\bf v}}
\def\bU{\mathop{\bf U}}
\def\bz{\mathop{\bf z}}
\def\bQ{\mathop{\bf Q}}

\newcommand{\aaa}{{\boldsymbol \alpha}}
\newcommand{\bbb}{{\boldsymbol \beta}}
\newcommand{\ddd}{\boldsymbol \delta}
\newcommand{\eee}{\boldsymbol \epsilon}
\newcommand{\rrr}{\boldsymbol \gamma}
\newcommand{\llll}{\boldsymbol \lambda}
\newcommand{\LLLL}{\boldsymbol \Lambda}
\newcommand{\ttt}{\boldsymbol \theta}
\newcommand{\OOO}{\boldsymbol \Omega}
\newcommand{\SG}{\boldsymbol \Sigma}

\newcommand{\TTT}{\boldsymbol \Theta}

\newcommand{\aaaa}{\mathbf a}
\newcommand{\A}{\mathbf A}
\newcommand{\AAA}{\mathcal A}

\newcommand{\BB}{\mathbf B}
\newcommand{\BBB}{\mathcal B}

\newcommand{\OO}{\mbox{$\mathbf O$}}
\newcommand{\bb}{\mbox{$\mathbf b$}}
\newcommand{\CC}{\mathbf C}
\newcommand{\CCC}{\mathcal C}

\newcommand{\dd}{\mathbf d}
\newcommand{\DD}{\mathbf D}
\newcommand{\DDD}{\mathbf \Delta}
\newcommand{\DDDD}{\mathcal D}


\newcommand{\EE}{\mathbf 1}
\newcommand{\ee}{\mathbf e}
\newcommand{\EEE}{\mathbb E}

\newcommand{\FF}{\mbox{$\mathbf F$}}
\newcommand{\ff}{\mbox{$\mathbf f$}}
\newcommand{\GG}{\mathbf G}

\newcommand{\KK}{\mathbf K}

\newcommand{\MM}{\mathbf M}
\newcommand{\MMM}{\mathcal M}
\newcommand{\YY}{\mbox{$\mathbf Y$}}
\newcommand{\yy}{\mbox{$\mathbf y$}}

\newcommand{\NN}{\mathbf N}
\newcommand{\NNN}{\mathcal N}

\newcommand{\g}{\mathbf g}

\newcommand{\HH}{\mathbf H}
\newcommand{\hh}{\mathbf h}

\newcommand{\II}{\mathbf I}
\newcommand{\III}{\mathcal I}

\newcommand{\LL}{\mathbf L}
\newcommand{\LLL}{\mathcal L}

\newcommand{\D}{\mathbf D}

\newcommand{\cc}{\mathbf c}

\newcommand{\pp}{\mathbf p}
\newcommand{\PP}{\mathbf P}
\newcommand{\PPP}{\mathcal P}

\newcommand{\R}{\mathbb R}
\newcommand{\RR}{\mathbf R}

\newcommand{\s}{\mathbf s}
\newcommand{\SSS}{\mathbf S}
\newcommand{\SSSS}{\mathcal S}

\newcommand{\tttt}{\mathbf t}
\newcommand{\TT}{\mathcal T}
\newcommand{\T}{\mathbf T}

\newcommand{\uu}{\mathbf u}
\newcommand{\UU}{\mathbf U}

\newcommand{\VV}{\mathbf V}

\newcommand{\WW}{\mathbf W}
\newcommand{\ww}{\mathbf w}

\newcommand{\xx}{\mathbf x}
\newcommand{\XX}{\mathbf X}
\newcommand{\XXX}{\mathcal X}

\newcommand{\z}{\mathbf z}
\newcommand{\Z}{\mathbf Z}

\newcommand{\qq}{\mbox{$\mathbf q$}}
\newcommand{\QQ}{\mathbf Q}

\newcommand{\tl}{\text{l}}
\newcommand{\tn}{\text{n}}
\newcommand{\tr}{\text{tr}}
\newcommand{\pa}{\text{pa}}


\newcommand{\1}{\uppercase\expandafter{\romannumeral1}}
\newcommand{\2}{\uppercase\expandafter{\romannumeral2}}



\def\r{\mathbf r}

\newcommand{\off}{\text{off}}
\newcommand{\supp}{\text{supp}}
\newcommand{\var}{\text{var}}
\newcommand{\0}{\textbf{0}}

\allowdisplaybreaks[4]

\begin{document}
	
\title{Identifiability and Consistent Estimation of the Gaussian Chain Graph Model}
\author{Ruixuan Zhao$^{\dag}$, Haoran Zhang$^\ddag$ and Junhui Wang$^\ddag$\\ [10pt]
	$^\dag$School of Data Science \\
	City University of Hong Kong 
	\and
	$^\ddag$Department of Statistics \\
	The Chinese University of Hong Kong
}
\date{ }

\maketitle
	
\onehalfspacing
\begin{abstract}
The chain graph model admits both undirected and directed edges in one graph, where symmetric conditional dependencies are encoded via undirected edges and asymmetric causal relations are encoded via directed edges. Though frequently encountered in practice, the chain graph model has been largely under investigated in literature, possibly due to the lack of identifiability conditions between undirected and directed edges. In this paper, we first establish a set of novel identifiability conditions for the Gaussian chain graph model, exploiting a low rank plus sparse decomposition of the precision matrix. Further, an efficient learning algorithm is built upon the identifiability conditions to fully recover the chain graph structure. Theoretical analysis on the proposed method is conducted, assuring its asymptotic consistency in recovering the exact chain graph structure. The advantage of the proposed method is also supported by numerical experiments on both simulated examples and a real application on the Standard \& Poor 500 index data.
\end{abstract}
	
\begin{keywords}
Causal inference, tangent space, directed acyclic graph, Gaussian graphical model, low-rank plus sparse decomposition
\end{keywords}
	

\doublespacing

\section{Introduction}	\label{sec:intro}

Graphical model has attracted tremendous attention in recent years, which provides an efficient modeling framework to characterize various relationships among multiple objects of interest. It finds applications in a wide spectrum of scientific domains, ranging from finance\cite{Sanford2012}, information system \cite{Stanton1995}, genetics \cite{Friedman2008}, neuroscience \cite{Cole2013} to public health \cite{Luke2007}.
%economics and finance \cite{Sanford2012} and information system  

In literature, two types of graphical models have been extensively studied. The first type is the undirected graphical model, which encodes conditional dependences among collected nodes via undirected edges. Various learning methods have been proposed to reconstruct the undirected graph, especially under the well-known Gaussian graphical model \cite{Friedman2008, Cai2011} where conditional dependences are encoded via the zero pattern of the precision matrix. Another well-studied graphical model is the directed acyclic graphical model, which uses directed edges to represent causal relationships among collected nodes in a directed acyclic graph (DAG).
To reconstruct the DAG structure, linear Gaussian structural equation model (SEM) has been popularly considered in literature where causal relations are encoded via the sign pattern of the coefficient matrix. Various identifiability conditions \cite{Peters2014, Park2020} have been established for the linear Gaussian SEM, leading to a number of DAG learning methods \cite{ChenW2019, Park2020}.

Another more flexible graphical model, known as the chain graph model, can be traced back to the early work in \cite{Lauritzen1989, Wermuth1990}. It admits both undirected and directed edges in one graph, where symmetric conditional dependencies are encoded via undirected edges and asymmetric causal relations are encoded via directed edges. Further, it is often assumed that no semi-directed cycles are allowed in the chain graph. As a direct consequence, the chain graph model can be seen as a special DAG model with multiple chain components, where each chain component is a subset of nodes connected via undirected edges, and directed edges are only allowed across different chain components.

The chain graph model has been frequently encountered in practice \cite{Chen2018, Ha2021}, but largely under-investigated in literature. In fact, the chain graph model may have various interpretations, including the Lauritzen–Wermuth–Frydenberg (LWF) interpretation \cite{Lauritzen1989, Frydenberg1990}, the multivariate regression (MVR) interpretation \cite{Cox1993} and  the Andersson-Madigan-Perlman (AMP) interpretation \cite{Andersson2001}. Each interpretation implies a different independence relationship from the chain graph structure and leads to several structure learning methods, including the IC like algorithm \cite{Studeny1997}, the CKES algorithm \cite{Pena2014} and the decomposition-based algorithm \cite{Ma2008}  for LWF chain graphs, the PC like algorithm \cite{Sonntag2012} and the decomposition-based algorithm \cite{Javidian2018} for MVR chain graphs, and the PC like algorithm \cite{Pena20142} and the decomposition-based algorithm \cite{Javidian2020} for AMP chain graphs. Yet, all these methods can only estimate some Markov equivalence classes of the chain graph model, and provide no guarantee for the reconstruction of the exact chain graph structure, mostly due to the lack of identifiability conditions between undirected and directed edges. It was until very recently that \cite{Wang2021} extends the equal noise variance assumption for DAG \cite{Peters2014} to establish the identifiability of the chain graph model under the AMP interpretation. Yet, the extended identifiability condition in \cite{Wang2021} is rather artificial and difficult to verify in practice. It is also worth mentioning that if the chain components and their causal ordering are known a priori, then the chain graph model degenerates to a sequence of multivariate regression models, and various methods \cite{Drton2006, Mccarter2014, Ha2021} have been developed to recover the graphical structure.

In this paper, we establish a set of novel identifiability conditions for the Gaussian chain graph model under AMP interpretation, exploiting a low rank plus sparse decomposition of the precision matrix. Further, an efficient learning algorithm is developed to recover the exact chain graph structure, including both undirected and directed edges. Specifically, we first reconstruct the undirected edges by estimating the precision matrix of the noise vector through a regularized likelihood optimization.  Then, we identify each chain component and determine its causal ordering based on the conditional variances of its nodes. Finally, the directed edges are reconstructed via multivariate regression coupled with truncated singular value decomposition (SVD). Theoretical analysis shows that the proposed method consistently reconstructs the exact chain graph structure, which to the best of our knowledge, is the first asymptotic consistency result for the chain graph model in literature. The advantage of the proposed method is supported by numerical experiments on both simulated examples and a real application on the Standard \& Poor 500 index data, which reveals some interesting impacts of the COVID-19 pandemic on the stock market.

The rest of the paper is organized as following. Section \ref{sec:model} introduces some preliminaries on the chain graph model. Section \ref{sec::method} proposes the identifiability conditions for linear Gaussian chain graph model, and develops an efficient learning algorithm to reconstruct the exact chain graph structure. The asymptotic consistency of the proposed method are established in Section \ref{sec:theory}. Numerical experiments of the proposed method on both simulated and real examples are included in Section \ref{sec:experiment}. Section \ref{sec:conclusion} concludes the paper, and technical proofs are provided in the Appendix. Auxiliary lemmas and further computational details are deferred to a separate Supplementary File.

Before moving to Section \ref{sec:model}, we define some notations. For an integer $m$, denote $[m] = \{1,...,m\}$.
For a real value $x$, denote $\lceil x\rceil$ as the largest integer less than or equal to $x$.
For two nonnegative sequences $a_n$ and $b_n,~a_n\lesssim b_n$ means there exists a constant $c>0$ such that $a_n\leq cb_n$ when $n$ is sufficiently large. Further, $a_n\lesssim_P b_n$ means there exists a constant $c>0$ such that $\Pr(a_n\leq cb_n) \to 1$ as $n$ grows to infinity. 
For a vector $\xx$, the sub-vector corresponding to an index subset $S$ is denoted as $\xx_S=(\xx_i)_{i\in S}$. For a matrix $\mathbf{A}=(a_{ij})_{p\times p}$, the sub-matrix corresponding to rows in $S_1$ and columns in $S_2$ is denoted as $\mathbf{A}_{S_1,S_2}=(a_{ij})_{i\in S_1,j\in S_2}$, and let $\mathbf{A}_{S_1,S_2}^{-1}$ denote the corresponding sub-matrix of $\A^{-1}.$  
Also, let $\|\mathbf{A}\|_{1,\text{off}}=\sum_{i\neq j} |a_{ij}|$, $\|\A\|_{\max} = \max_{ij} |a_{ij}|$, $\|\mathbf{A}\|_2$ denote the spectrum norm, $\|\mathbf{A} \|_*$ denote the nuclear norm and $v(\A)\in\R^{p^2}$ denote the vectorization of $\A$.



%It is interesting to point out that a chain graph ${\cal G}$ corresponding to SEM \eqref{eq:model} can be seen as a directed acyclic graph (DAG) \cite{Lauritzen1996,Peters2017, Maathuis2018} on multiple chain components and \eqref{eq:cond-CC} implies that each chain component is corresponding to the Gaussian graphical model (GGM) \cite{Lauritzen1996} when removing the directed effects from its parents. Therefore, a chain graph ${\cal G}$ is a hybrid graph which captures the properties from both DAGs and GGMs. \xc{(Move to introduction.)}

%The low-rank plus sparse matrix decomposition has been well studied in literature \cite{Candes2011, Chandrasekaran2011}. 

%structure learning methods \cite{Kalisch2007, Chickering2002, Tsamardinos2006} have been developed to recover the Markov equivalence class \yc{[reference]} of the directed acyclic graph (DAG).

%especially when the underlying joint distribution is multivariate Gaussian \cite{Yuan2006, Friedman2008, Cai2011}. The other popular type of graphical model is the directed acyclic graphical model, where directed edges are employed to represent causal relationships among collected nodes. Structure learning methods \cite{Kalisch2007, Chickering2002, Tsamardinos2006} have been developed to recover the Markov equivalence class of the directed acyclic graph (DAG). To further  reconstruct the exact DAG structure, a number of efficient learning methods \cite{ChenW2019, Park2021} have also been proposed by fully exploiting the established identifiability conditions \cite{Peters2014, Shimizu2006, Park2020}. 

%The main contributions of this paper are as follows. First, a novel set of identifiability conditions are proposed for the linear Gaussian CG model under the AMP interpretation. The conditions are motivated from the graphical structure in a CG model, and appear much more natural than the condition in \cite{Wang2021}. Second, a computationally efficient structural learning algorithm is developed to first recover both undirected and directed edges in a CG model. Third, we establish the asymptotic consistency of the proposed method in reconstructing the exact CG structure, which to be best of our knowledge, is the first asymptotic consistency result for the CG model in literature. 
%Finally, the propose method is applied to analyze the Standard \& Poor 500 index data, which reveals some interesting impacts of the COVID-19 pandemic on the stock market.



\section{Chain graph model}\label{sec:model}

Suppose the joint distribution of $\xx=(x_1,...,x_p)^\top$ can be depicted as a chain graph ${\cal G}=({\cal N}, {\cal E})$, where ${\cal N}=\{1,...,p\}$ represents the node set and ${\cal E} \subset {\cal N} \times {\cal N}$ represents the edge set containing all undirected and directed edges. To differentiate, we denote $(i - j)$ for an undirected edge between nodes $i$ and $j$, $(i \rightarrow j)$ for a directed edge pointing from node $i$ to node $j$, and suppose that at most one edge is allowed between two nodes.
Then, there exists a positive integer $m$ such that ${\cal N}$ can be uniquely partitioned into $m$ disjoint chain components $\NNN = \bigcup_{k=1}^m \tau_k,$ where each $\tau_k$ is a connected component of nodes via undirected edges.
Suppose that only undirected edges exist within each chain component, and directed edges are only allowed across different chain components \cite{Maathuis2018, Drton2006}. Further suppose that there exists a permutation $\boldsymbol{\pi}=(\pi_1,...,\pi_m)$ such that for $i \in \tau_{\pi_k}$ and $j \in \tau_{\pi_l}$, if $(i\rightarrow j)$, then $k<l$. 
This excludes the existence of semi-directed cycle in $\cal G$ \cite{Maathuis2018}.
We call such permutation $\boldsymbol{\pi}$ as the causal ordering of the chain components, and directed edges could only point from nodes in higher-order $\tau_k$ to nodes in lower-order one.

Let $\pa(i) = \{ j\in {\cal N} : (j\rightarrow i) \in {\cal E}\},~\mbox{ch}(i)=\{j\in {\cal N} : (i\rightarrow j) \in {\cal E}\}$ and $\mbox{ne}(i) = \{ j\in {\cal N} :  (j-i) \in {\cal E}\}$ denote the parents, children and neighbors of node $i,$ respectively. Further, let $\pa(\tau_k)=\bigcup_{i \in \tau_k} \pa(i)$ be the parent set of chain component $\tau_k$. Suppose the joint distribution of $\xx$ satisfies the Andersson–Madigan–Perlman (AMP) Markov property \citep{Andersson2001, Levitz2001} with respect to ${\cal G},$ and follows the linear structural equation model (SEM),
\begin{align}\label{eq:model}
	\bx = \mathbf{B} \bx + \eee,
\end{align}
where $\mathbf{B} = (\beta_{ij})_{p\times p}$ is the coefficient matrix, $\eee = (\epsilon_1,...,\epsilon_p)^\top \sim {\cal N} (\mathbf{0}, \mathbf{\Omega}^{-1})$, and $\mathbf{\Omega}=(\omega_{ij})_{p\times p}$ is the precision matrix of $\eee$. 
Further, suppose that $\beta_{ji} \neq 0$ if and only if $j\in \pa(i)$, and $\omega_{ij}\neq 0$ if and only if $j\in \mbox{ne}(i)$.
Therefore, the undirected and directed edges in $\cal G$ can be directly implied by the zero patterns in $\OOO$ and $\BB$, respectively. 
The joint density of $\xx$ can then be factorized as 
\begin{equation}\label{eq:factorization}
P(\xx)=\prod_{k=1}^m P(\xx_{\tau_k} | \xx_{\pa(\tau_k)}),
\end{equation}
where $\mathbf{x}_{\tau_k} | \mathbf{x}_{\pa(\tau_k)}   \sim {\cal N} (\mathbf{B}_{\tau_k, \pa(\tau_k)} \mathbf{x}_{\pa(\tau_k)}, \mathbf{\Omega}_{\tau_k,\tau_k}^{-1})$ for $k\in[m]$, and  $\mathbf{\Omega}_{\tau_k,\tau_k}$ is not necessarily a diagonal matrix. This is a key component of the chain graph model to allow undirected edges within each $\tau_k$, and thus differs from most existing SEM models with diagonal $\mathbf{\Omega}$ in literature \cite{Peters2017, Peters2014, Park2020, ChenW2019}. 


To assure the acyclicity among chain components in $\cal G$, we say $(\OOO,\BB)$ is CG-feasible if there exists a permutation matrix $\PP$ such that both $\PP\OOO\PP^\top$ and $\PP\BB\PP^\top$ share the same block structure, where $\PP\OOO\PP^\top$ is a block diagonal matrix and $\PP\BB\PP^\top$ is a block lower triangular matrix with zero diagonal blocks. Figure \ref{ToyExample} shows a toy chain graph, as well as the supports of the original and permuted $(\OOO,\BB)$. Let $\TTT$ denote the precision matrix for $\xx$, then it follows from \eqref{eq:model} that 
\begin{align} \label{eq::decom}
	\mathbf{\Theta}=(\mathbf{I}_p-\mathbf{B})^\top \mathbf{\Omega} (\mathbf{I}_p-\mathbf{B}) =: \mathbf{\Omega} + \mathbf{L},
\end{align}
where $\mathbf{L} = \mathbf{B}^\top \mathbf{\Omega}\mathbf{B} - \mathbf{B}^{\top} \mathbf{\Omega} - \mathbf{\Omega}\mathbf{B}$. 

%Let $\mathbf{\Sigma}$ and $\TTT$ denote the covariance and precision matrix for $\xx$, then it follows from \eqref{eq:model} that 
%\begin{align} \label{eq::decom}
%	\mathbf{\Theta}=(\mathbf{I}_p-\mathbf{B})^\top \mathbf{\Omega} (\mathbf{I}_p-\mathbf{B}) =: \mathbf{\Omega} + \mathbf{L},
%\end{align}
%and $\mathbf{\Sigma}=\TTT^{-1}$, where $\mathbf{L} = \mathbf{B}^\top \mathbf{\Omega}\mathbf{B} - \mathbf{B}^{\top} \mathbf{\Omega} - \mathbf{\Omega}\mathbf{B}$. 

\begin{figure}[!htb]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/toy_example.PNG}
	\end{minipage}
	\begin{minipage}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/matrix.PNG}
	\end{minipage}
	\caption{The left panel displays a toy chain graph with colors indicating different chain components, and the right panel displays the supports of the original $(\OOO,\BB)$ in the first column and the permuted $(\OOO,\BB)$ in the second column.}\label{ToyExample}
\end{figure}


\section{Proposed method}\label{sec::method}


\subsection{Identifiability of $\cal G$}\label{sec::identi}

A key challenge in the chain graph model is the identifiability of the graph structure $\cal G$, due to the fact that $\OOO$ and $\BB$ are intertwined in the SEM model in \eqref{eq:model}. To proceed, we assume that $\OOO$ is sparse with $\|\OOO\|_0 = S$ and $\LL$ is a low-rank matrix with $\rank(\LL) = K$. The sparseness of $\OOO$ implies the sparseness of undirected edges in ${\cal G}$, which has been well adopted in the literature of Gaussian  graphical model \cite{Meinshausen2006, Friedman2008, Cai2011}. The low-rank of $\LL$ inherits from that of $\BB$ \cite{Fang2020}, which essentially assumes the presence of hub nodes in $\cal G$, i.e. nodes with multiple children or parents.

Let $\LL = \UU_1 \DD_1 \UU_1^\top$ be the eigen decomposition of $\LL,$ where $\UU_1^\top \UU_1 = \II_K$ and $\DD_1$ is a $K\times K$ diagonal matrix. We define two linear subspaces, 
\begin{eqnarray}
{\cal S}(\OOO) &=& \{\SSS\in\R^{p\times p}: \SSS^\top = \SSS,~\text{and}~s_{ij} = 0~\text{if}~\omega_{ij} = 0\}, \label{eq:tangent S} \\
{\cal T}(\LL) &=& \left\{ \UU_1\YY+\YY^\top\UU_1^\top:~ \YY \in \R^{K\times p} \right\}, \label{eq:tangent L}
\end{eqnarray}
where ${\cal S}(\OOO)$ is the tangent space, at point $\OOO$, of the manifold containing symmetric matrices with at most $S$ non-zero entries, and ${\cal T}(\LL)$ 
is the tangent space, at point $\LL$, of the manifold containing symmetric matrices with rank at most $K$ \citep{Chandrasekaran2011}. 

\begin{Assumption}\label{ass:tangent}
${\cal S}(\OOO)$ and ${\cal T}(\LL)$ intersect at the origin only; that is, ${\cal S}(\OOO) \cap {\cal T}(\LL) = \{\mathbf{0}_{p\times p}\}$.	
\end{Assumption}

Assumption \ref{ass:tangent} is the same as the transversality condition in \cite{Chandrasekaran2011}, which assures the identifiability of $(\OOO,\LL)$ in the sense that $\TTT$ can be uniquely decomposed as the sum of a matrix in ${\cal S}(\OOO)$ and the other one in ${\cal T}(\LL)$.  

\begin{Assumption}\label{ass:diff}
The $K$ eigenvalues of $\LL$ are distinct.
\end{Assumption}

Assumption~\ref{ass:diff} is necessary to identify the eigen space of the low-rank matrix $\LL,$ which has been commonly assumed in the literature of matrix perturbation \cite{yu2015useful}. 
Let $\PA$ be the parameter space of CG-feasible $(\OOO,\BB)$, where $\OOO\succ0,~\OOO+\LL\succ0,~\|\OOO\|_0\leq S,~\rank(\LL)\leq K,$ and Assumptions~\ref{ass:tangent} and \ref{ass:diff} are met. Let $(\OOO^*,\BB^*)$ denote the true parameters of the linear SEM model \eqref{eq:model} satisfying $\|\OOO^*\|_0 = S$ and $\rank(\LL^*)=K.$

\begin{theorem}\label{thm:ident}
Suppose $(\OOO^*,\BB^*)\in\PA.$ Then, there exists a small $\epsilon>0$ such that for any $(\OOO, \BB)\in\PA$ satisfying $\| \OOO-\OOO^*\|_{\max}<\epsilon$ and $\|\BB-\BB^*\|_{\max}<\epsilon,$ if $$
(\mathbf{I}_p-\BB)^\top\OOO (\mathbf{I}_p-\BB)=(\mathbf{I}_p-\BB^*)^\top \OOO^* (\mathbf{I}_p-\BB^*),
$$ 
it holds true that $(\OOO, \BB) = (\OOO^*,\BB^*).$
\end{theorem}

Theorem \ref{thm:ident} establishes the local identifiability of $(\OOO^*,\BB^*)$ in \eqref{eq:model}, which further implies the local identifiability of the chain graph $\cal G$. It essentially states that $(\OOO^*,\BB^*)\in\PA$ can be uniquely determined, within its neighborhood in $\PA$, by the precision matrix $\TTT^*=(\mathbf{I}_p-\BB^*)^\top \OOO^* (\mathbf{I}_p-\BB^*),$ which can be consistently estimated from the observed sample. 

\begin{remark}[{\bf Identifiability for DAG}]
	When $\OOO$ is indeed a diagonal matrix, each chain component contains exactly one node and thus $\cal G$ reduces to a DAG. By Theorem \ref{thm:ident}, it is identifiable as long as the eigenvalues of $\LL^*$ are distinct and $\ee_j\notin\text{span}(\UU_1^*)$ for any $j \in [p]$, where $\UU_1^*\in\R^{p\times K}$ contains eigenvectors of $\LL^*$ and $\{\ee_j\}_{j=1}^p$ are the standard basis of $\R^p.$ This provides an alternative identifiability condition for DAG, in contrast to the popularly-employed equal error variance condition \cite{Peters2014}.
\end{remark}



\subsection{Learning algorithm}\label{sec:est}

We now develop a learning algorithm to estimate $(\mathbf{\Omega}^*, \mathbf{B}^*)$ and reconstruct the chain graph ${\cal G}$. Suppose we observe independent copies $\xx_1,...,\xx_n\in\R^p$ and denote $\mathbf{X}=(\mathbf{x}_1,...,\mathbf{x}_p)^\top\in \mathbb{R}^{n\times p}$.
We first estimate $\mathbf{\Omega}^*$ via the following regularized likelihood,
\begin{align} \label{eq:opti1}
(\widehat\OOO,\widehat\LL) = \argmin_{\OOO,\LL} & \ \big \{-l(\OOO+\LL) + \lambda_n  (\|\OOO\|_{1,\off} + \gamma\|\LL\|_* ) \big \} \\
\mbox{subject to} &~~ \OOO \succ 0 \mbox{~~and~~}\OOO+\LL \succ0, \nonumber
\end{align}
where $l(\TTT) = -\text{tr}(\mathbf{\Theta} \widehat \SG) + \log\{\det (\mathbf{\Theta})\}$ is the Gaussian log-likelihood with $\widehat\SG=\frac{1}{n}\mathbf{X}^\top \mathbf{X}$, and $\lambda_n$ and $\gamma$ are tuning parameters. Here $\|\OOO\|_{1,\off} = \sum_{i\neq j}|\omega_{ij}|$ induces sparsity in $\OOO$, $\|\LL\|_*$ induces low-rank of $\LL$, and the constraints are due to the fact that both $\OOO$ and $\mathbf{\Theta}=\OOO+\LL$ are precision matrices. Note that the optimization task in \eqref{eq:opti1} is convex, and can be efficiently solved via the alternative direction method of multipliers (ADMM; \cite{BoydS2011}). More computational details are deferred to the Supplementary Files. 

Once $\widehat\OOO$ is obtained, we connect nodes $i$ and $j$ with undirected edges if $\widehat\omega_{ij}\neq 0$, which leads to multiple estimated chain components, denoted as $\widehat\tau_1,...,\widehat\tau_{\widehat m}$.
To determine the causal ordering of the estimated chain components, for each $\widehat\tau_k$ and any $\CCC\subset[p]\backslash\widehat\tau_k$, we define 
$$
\widehat{\DDDD}(\widehat{\tau}_k,\CCC)= \max_{i\in \widehat{\tau}_k} \Big \{ \widehat{\mathbf{\Sigma}}_{ii} - \widehat{\mathbf{\Sigma}}_{i {\CCC}} \widehat{\mathbf{\Sigma}}_{{\CCC}{\CCC}}^{-1} \widehat{\mathbf{\Sigma}}_{ {\CCC}i} - \widehat{\OOO}_{ii}^{-1} \Big \},
$$
where $\widehat{\mathbf{\Sigma}}_{ii} - \widehat{\mathbf{\Sigma}}_{i {\CCC}} \widehat{\mathbf{\Sigma}}_{{\CCC}{\CCC}}^{-1} \widehat{\mathbf{\Sigma}}_{ {\CCC}i}$ is the estimated conditional variance for node $i\in \widehat\tau_k$ given nodes in $\CCC$, and $\widehat{\OOO}_{ii}^{-1}$ is the estimated variance for node $i\in\widehat\tau_k$ given its parent chain components. It is thus clear that $\widehat{\DDDD}(\widehat{\tau}_k,\CCC)$ shall be close to 0 if $\CCC$ consists of all upper chain components of $\widehat\tau_k$. We start with $\widehat{\cal D}(\widehat{\tau}_k, \emptyset)= \max_{i\in \widehat{\tau}_k} \big \{ \widehat{\mathbf{\Sigma}}_{ii} - \widehat{\OOO}_{ii}^{-1} \big \}$ for each chain component $\widehat{\tau}_k$, and select the first chain component by $\widehat{\pi}_1 = \argmin_{l\in [\widehat m]}\widehat{\cal D}(\widehat{\tau}_l,\emptyset)$.
Suppose the first $s$ chain components $\widehat{\tau}_{\widehat{\pi}_1},...,\widehat{\tau}_{\widehat{\pi}_s}$ have been selected, let $\widehat{\cal C}_s = \cup_{k=1}^s \widehat{\tau}_{\widehat{\pi}_k}$ and $\widehat{\pi}_{s+1} = \argmin_{l\in [\widehat m] \setminus \cup_{k=1}^s \widehat{\pi}_k}\widehat{\cal D}(\widehat{\tau}_l, \widehat{\cal C}_s)$.
We repeat this procedure until the causal orderings of all $\widehat{\tau}_k$'s are determined, which are denoted as $\widehat{\boldsymbol\pi}=(\widehat{\pi}_1,...,\widehat{\pi}_{\widehat{m}})$.

Finally, to estimate $\BB$, we first give an intermediate estimate $\widehat{\BB}^{\text{reg}}$, whose submatrix $\widehat{\BB}^{\text{reg}}_{\widehat{\tau}_{\widehat{\pi}_k}, \widehat{\cal C}_{k-1}}$ is obtained via a multivariate regression of $\xx_{\widehat{\tau}_{\widehat{\pi}_k}}$ on $\xx_{\widehat{\cal C}_{k-1}}$, as the directed edges are only allowed from upper chain components to lower ones. Given $\widehat{\BB}^{\text{reg}}$, we conduct singular value decomposition (SVD) as $\widehat{\BB}^{\text{reg}}=\widehat{\UU}^{\text{reg}}\widehat{\DD}^{\text{reg}}(\widehat{\VV}^{\text{reg}})^\top$ with $\widehat{\DD}^{\text{reg}}=\diag(\widehat{\sigma}_1^{\text{reg}},...,\widehat{\sigma}_p^{\text{reg}})$, and then truncate the small singular values to obtain $\widehat{\BB}^{\text{svd}}=\widehat{\UU}^{\text{reg}}\widehat{\DD}^{\text{svd}}(\widehat{\VV}^{\text{reg}})^\top$, where   $\widehat{\DD}^{\text{svd}}=\diag(\widehat{\sigma}_1^{\text{svd}},...,\widehat{\sigma}_p^{\text{svd}})$ with $\widehat{\sigma}_j^{\text{svd}}=0$ if $\widehat{\sigma}_j^{\text{reg}}\leq\kappa_n$ and $\widehat{\sigma}_j^{\text{svd}}=\widehat{\sigma}_j^{\text{reg}}$ if $\widehat{\sigma}_j^{\text{reg}}>\kappa_n$, for some pre-specified $\kappa_n>0$. 
The final estimate $\widehat{\BB}=(\widehat{\beta}_{ij})_{p\times p}$ is obtained by truncating the diagonal and upper triangular blocks to 0, and conducting a hard thresholding to the lower triangular blocks with some pre-specified $\nu_n>0$, where $\widehat{\beta}_{ij}=0$ if $|\widehat{\beta}_{ij}^{\text{svd}}|\leq \nu_n$ and $\widehat{\beta}_{ij}=\widehat{\beta}_{ij}^{\text{svd}}$ if $|\widehat{\beta}_{ij}^{\text{svd}}|> \nu_n$. 
The nonzero elements of $\widehat{\BB}$ then lead to the estimated directed edges. 


\begin{comment}
Details of the learning algorithm is summarized in Algorithm \ref{alg:1}.
\begin{singlespace}
	\begin{algorithm}[!htb]
		\caption{}
		\label{alg:1}
		
		\begin{algorithmic}[1]
			\STATE \textbf{Input: } Data matrix $\bX \in {\R}^{n \times p}$.
			\STATE Compute $\widehat{\OOO}$ by solving optimization problem (\ref{eq:opti1});
			\STATE Read off chain components $\{\widehat{\tau}_1,...,\widehat{\tau}_{\widehat{m}}\}$ from $\widehat{\OOO}$;
			\STATE Set $\widehat{I}=\emptyset$ and $\widehat{\cal C}=\emptyset$;
			\STATE  {\bf For} $k \in \{1,...,\widehat{m} \}$ {\bf do}:
			\begin{itemize}
				\item[a.]  $\widehat{\pi}_k = \argmin_{i \in \{1,..,\widehat{m}\}\backslash \widehat{I}} {\cal D} (\widehat{\tau}_i, \widehat{\cal C})$;
				\item[b.]  $\widehat{I}=\widehat{I} \cup \widehat{\pi}_k$ and $\widehat{\cal C} = \widehat{\cal C} \cup \widehat{\tau}_{\widehat{\pi}_k}$.
			\end{itemize}
			\STATE Obtain $\widehat{\BB}$ via multivariate regressions, sparse  SVD and element-wise thresholding as stated above.
			\STATE	{\bf Output:} $\widehat{\boldsymbol\pi}$ and  ($\widehat{\OOO}, \widehat{\BB}$).
		\end{algorithmic}
	\end{algorithm}
\end{singlespace}
\end{comment}


\section{Asymptotic theory}\label{sec:theory}

This section quantifies the asymptotic behavior of $(\widehat\OOO,\widehat\BB)$, and establishes its consistency for reconstructing the chain graph ${\cal G}^* = ({\cal N}, {\cal E}^*)$.
Let $\TTT^* = (\mathbf{I}_p-\BB^*)^\top \OOO^* (\mathbf{I}_p-\BB^*),$ and the Fisher information matrix takes the form 
$$
\III^* = - \mathbb{E} \Big[\frac{\partial^2 l (\mathbf{\Theta}^*)}{\partial \mathbf{\Theta}^2} \Big]  =  (\mathbf{\Theta}^*)^{-1} \otimes (\mathbf{\Theta}^*)^{-1},
$$
where $\otimes$ denotes the Kronecker product. For a linear subspace $\MMM,$ let ${\cal P}_{{\cal M}}$ denote the projection onto ${\cal M},$ and ${\cal M}^{\perp}$ denote the orthogonal complement of ${\cal M}$. We further define two linear operators $F : {\cal S}(\OOO^*) \times {\cal T}(\LL^*) \rightarrow {\cal S}(\OOO^*) \times {\cal T}(\LL^*)$ and $F^\perp : {\cal S}(\OOO^*) \times {\cal T}(\LL^*) \rightarrow {\cal S}(\OOO^*)^\perp \times {\cal T}(\LL^*)^\perp$ such that 
\begin{align*}
F(\DDD_{\OOO},\DDD_{\LL}) &= ({\cal P}_{{\cal S}(\OOO^*)} ( \III^*v(\DDD_{\OOO} + \DDD_{\LL})), {\cal P}_{{\cal T}(\LL^*)} ( \III^* v(\DDD_{\OOO} + \DDD_{\LL}))), \\
F^\perp(\DDD_{\OOO},\DDD_{\LL}) &= ({\cal P}_{{\cal S}(\OOO^*)^{\perp}} ( \III^*v(\DDD_{\OOO} + \DDD_{\LL})), {\cal P}_{{\cal T}(\LL^*)^\perp} ( \III^*v(\DDD_{\OOO} + \DDD_{\LL}))).
\end{align*}
According to Assumption \ref{ass:tangent} and Lemma~\ref{lem:invertible} in the Supplementary Files, $F$ is invertible and thus $F^{-1}$ is well defined. 

Let $g_{\gamma}(\OOO,\LL)=\max\{\|\OOO\|_{\max}, \|\LL\|_2/\gamma\},$ where $\gamma>0$ is the same as that in \eqref{eq:opti1}. Let $\LL^* = \UU_1^*\DD_1^*(\UU_1^*)^\top$ be the eigen decomposition of $\LL^*.$ 


%$\|\OOO\|_{\max}$ denotes the maximal absolute entries of $\OOO$. 



\begin{Assumption}\label{ass:incor}
$g_{\gamma}(F^{\perp} F^{-1}(\sign(\OOO^*), \gamma \UU_1^*\sign(\DD_1^*) (\UU_1^*)^\top)) < 1$.
\end{Assumption}

Assumption~\ref{ass:incor} is essential for establishing the selection and rank consistency of $(\widehat\OOO,\widehat\LL)$ through penalties in \eqref{eq:opti1}. For example, in a special case when $\TTT^* = \II_p$ and ${\cal S}(\OOO^*) \perp {\cal T}(\LL^*)$, Assumption~\ref{ass:incor} simplifies to $\max\{\gamma\|\UU_1^*\sign(\DD_1^*) (\UU_1^*)^\top\|_{\max},\|\sign(\OOO^*)\|_2/\gamma\}<1$, implying that $\UU_1^*$ is not sparse and $\sign(\OOO^*)$ is not a low-rank matrix. Similar technical conditions have also been assumed in literature \cite{Chandrasekaran2011, chen2016fused}.

%It generalizes the irrepresentable condition \cite{zhao2006model, Chandrasekaran2011, chen2016fused} in the variable selection literature. 
%\yc{[can we give a simple case when it degenerates to the irrepresentable condition?]}


\begin{theorem}\label{thm:Omega}
Suppose $(\OOO^*,\BB^*)\in\PA$ and Assumption~\ref{ass:incor} holds. Let $\lambda_n = n^{-1/2+\eta}$ with a sufficiently small positive constant $\eta$. Then, with probability approaching 1, \eqref{eq:opti1} has a unique solution $(\widehat\OOO,\widehat\LL).$ Furthermore, we have
\begin{equation}\label{eq:consis}
\begin{aligned}
&\|\widehat\OOO - \OOO^*\|_{\max}  \lesssim_P n^{-\frac{1}{2}+2\eta},~~~~ &&\Pr(\sign(\widehat\OOO) = \sign(\OOO^*)) \to 1, \\
&\|\widehat\LL - \LL^*\|_{\max} \lesssim_Pn^{-\frac{1}{2}+2\eta},~~~~&&\Pr(\rank(\widehat\LL) = \rank(\LL^*))) \to 1.
\end{aligned}
\end{equation}
\end{theorem}

Theorem \ref{thm:Omega} shows that $\widehat\OOO$ has both estimation and sign consistency, which implies that the undirected edges in ${\cal G}^*$ could be exactly reconstructed with high probability. It can also be shown that $\widehat\BB$ attains both estimation and selection consistency, implying the exact recovery of the directed edges in ${\cal G}^*$. Furthermore, given $(\widehat\OOO,\widehat\BB)$, we reconstruct the chain graph as $\widehat{\cal G}=({\cal N}, \widehat{\cal E})$, where $(i\rightarrow j) \in \widehat{\cal E}$ if and only if $\widehat{\beta}_{ji} \neq 0$, and $(i - j) \in \widehat{\cal E}$ if and only if $\widehat{\omega}_{ij} \neq 0$. The following Theorem~\ref{thm:Graph} establishes the consistency of $\widehat{\cal G}$.


%\yc{[Results on $\widehat\BB$. The def of $\widehat{\cal G} = {\cal G}^*$. Use $\eta$ instead of $\psi$.]}
%After recovering the chain components, the selection consistency of the parent set for each chain component can also be established, and thus ${\cal G}^*$ can be consistently reconstructed with high probability.
%After recovering the chain components, the selection consistency of the parent set for each chain component can also be obtained, and thus we establish the following theorem on the asymptotic reconstruction consistency of the estimated chain graph $\widehat{\cal G}=(\widehat{\cal N}, \widehat{\cal E})$, where $\widehat{\cal N} = {\cal N}^*$, $(i\rightarrow j) \in \widehat{\cal E}$ if $\widehat{\beta}_{ji} \neq 0$ and $(i - j) \in \widehat{\cal E}$ if  $\widehat{\omega}_{ij} \neq 0$.


\begin{theorem}\label{thm:Graph}
Suppose all the conditions in Theorem \ref{thm:Omega} are satisfied, and we set $\kappa_n = n^{-1/2+\eta}$ and $\nu_n = n^{-1/2+2\eta}$.
Then, $\Pr ( \widehat{\cal G} = {\cal G}^* ) \to 1 \ \text{as} \ n\to \infty.$
\end{theorem}

Theorem \ref{thm:Graph} shows that the proposed method gives an exact recovery of the chain graph with high probability, which is in sharp contrast to the existing methods only recovering some Markov equivalent class of the chain graph \citep{Studeny1997, Pena2014, Ma2008, Sonntag2012, Javidian2018, Pena20142, Javidian2020}.



%guarantees that the true chain graph ${\cal G}^*$ can be exactly reconstructed by the proposed method with proper choices of tuning parameters. 

%To be best of our knowledge, it is the first consistency result of recovering chain graph in literature.

\section{Numerical experiments}\label{sec:experiment}

\subsection{Simulated examples}

We examine the numerical performance of the proposed method and compare it against existing structural learning methods for chain graph, including the decomposition-based algorithm (LCD, \cite{Javidian2020}) and PC like algorithm (PC-like, \cite{Pena20142, Javidian2020}), as well as the PC algorithm for DAG (PC, \cite{Kalisch2007}). 
The implementations of LCD and PC-like are available at \url{https://github.com/majavid/AMPCGs2019}. We implement PC algorithm through R packages \texttt{pcalg} and then convert the resulted partial DAG to DAG by \texttt{pdag2dag}. The significance level of all tests in LCD, PC-like and PC is set as $\alpha = 0.05$.

%Also, we implement PC by using an \texttt{R} package \texttt{pcalg} and then convert the output of partial DAG to DAG by using another\texttt{R} package  \texttt{pdag2dag}. 
%Specifically, the proposed method is applied with parameters $\lambda_n$, $\gamma$, $\kappa_n$ and $\nu_n$ as suggested in Section \ref{sec:theory}. 

We evaluate the numerical performances of all four methods in terms of the estimation accuracy of undirected edges, directed edges and the overall chain graph. Specifically, 
we report recall, precision and Matthews correlation coefficient (MCC) as evaluation metrics for the estimated undirected edges and directed edges, respectively. Furthermore, we employ Structural Hamming Distance (SHD) \cite{Tsamardinos2006, Javidian2020} to evaluate the estimated chain graph, which is the number of edge insertions, deletions or flips to change the estimated chain graph to the true one. Note that large values of recall, precision and MCC and small values of SHD indicate good estimation performance.

%Two simulated examples are considered to evaluate the estimation performance.

{\bf Example 1.} We consider a classic two-layer Gaussian graphical model  \cite{Lin2016, Mccarter2014} with two layers ${\cal A}_1 = \{1,..., \lceil 0.1p\rceil \}$ and ${\cal A}_2 = \{\lceil 0.1p\rceil +1, ..., p\}$, 
whose structure is illustrated in Figure \ref{SimulExample}(a). 
Within each layer, we randomly connect each pair of nodes by an undirected edge with probability $0.02$, and note that one layer may contain multiple chain components. Then, we generate the directed edges from nodes in ${\cal A}_1$ to nodes in ${\cal A}_2$ with probability $0.8$. Furthermore, the non-zero values of $\omega_{ij}$ and $\beta_{ij}$ are uniformly generated from $[-1.5,-0.5]\cup[0.5,1.5]$. To guarantee the positive definiteness of $\OOO$, each diagonal element is set as $\omega_{ii}=\sum_{j=1,j\neq i}^p \omega_{ji}+0.1$.

{\bf Example 2.} The structure of the second chain graph is illustrated in Figure \ref{SimulExample}(b). Particularly, we  randomly connect each pair of nodes by an undirected edge with probability $0.03$, and read off multiple chain components $\{\tau_1,..., \tau_m\}$ from the set of undirected edges. Then, we set the causal ordering of the chain components as $(\pi_1,...,\pi_m) = (1,...,m)$. For each chain component $\tau_k$, we randomly select the nodes as hubs with probability $0.2$, and let each hub node points to the nodes in $\cup_{i=k+1}^m \tau_i$ with probability $0.8$. Similarly, the non-zero values of $\omega_{ij}$ and $\beta_{ij}$  are uniformly generated from $[-1.5,-0.5]\cup[0.5,1.5]$, and $\omega_{ii}=\sum_{j=1,j\neq i}^p \omega_{ji}+0.1$.

\begin{figure}[!htb]
	\centering
	\begin{minipage}[b]{0.55\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/simul_CG_ex1.PNG}
		\subcaption{}
	\end{minipage}
	\begin{minipage}[b]{0.44\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/simul_CG_ex2.PNG}
		\subcaption{}
	\end{minipage}
	\caption{The chain graph structures in Examples 1 and 2.}\label{SimulExample}
\end{figure}

For each example, we consider four cases with $(p,n) = (50,500), (50,1000), (100,500)$ and $(100,1000)$, and the averaged performance of all four methods over 50 independent replications are summarized in Tables \ref{tab:ex1} and \ref{tab:ex2}. As PC only outputs the DAGs with no undirected edges, its evaluation metrics on $\widehat{\OOO}$ are all NA.

\begin{table}[!htb]
		\caption{The averaged evaluation metrics of all the methods in Example 1 together with
		their standard errors in parentheses.}
	\tiny
	\centering
	\begin{tabular}{ccccccccc}
		\hline 
		$ (p, n)$& Method & Recall($\widehat{\Omega}$) & Precision($\widehat{\Omega}$) & MCC($\widehat{\Omega}$) & Recall($\widehat{B}$) & Precision($\widehat{B}$) &  MCC($\widehat{B}$)   & SHD   \\\hline  
		$(50,500)$ & Proposed &\makecell[c]{\textbf{0.6482}  (0.0255)} & \makecell[c]{\textbf{0.8493}  (0.0195)} & \makecell[c]{\textbf{0.7327}  (0.0203)}  & \makecell[c]{\textbf{0.2789}  (0.0080)} & \makecell[c]{0.4971  (0.0133)} & \makecell[c]{\textbf{0.3361}  (0.0097)}  & \makecell[c]{\textbf{187.8600}  (2.4026)} \\
		& LCD  &\makecell[c]{0.1000  (0.0120)} & \makecell[c]{0.1174  (0.0128)} & \makecell[c]{0.0934  (0.0118)}  & \makecell[c]{0.1350  (0.0047)} & \makecell[c]{\textbf{0.5862}  (0.0125)} & \makecell[c]{0.2579  (0.0077)}  & \makecell[c]{192.7600  (1.5514)} \\
		&PC-like &\makecell[c]{0.0441  (0.0075)} & \makecell[c]{0.0424  (0.0084)} & \makecell[c]{0.0270  (0.0076)}  & \makecell[c]{0.0055  (0.0007)} & \makecell[c]{0.0679  (0.0090)} & \makecell[c]{-0.0006  (0.0025)}  & \makecell[c]{225.9800  (1.1569)} \\
		& PC & NA & NA & NA & \makecell[c]{0.0244  (0.0019)} & \makecell[c]{0.0835  (0.0059)} & \makecell[c]{0.0067  (0.0033)}  & \makecell[c]{238.1600  (1.1691)} \\
		%& Glasso  &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} \\
		\hline 
		$(50,1000)$ & Proposed &\makecell[c]{\textbf{0.6729}  (0.0237)} & \makecell[c]{\textbf{0.8380}  (0.0208)} & \makecell[c]{\textbf{0.7425}  (0.0196)}  & \makecell[c]{\textbf{0.3060}  (0.0095)} & \makecell[c]{0.4663  (0.0124)} & \makecell[c]{\textbf{0.3386}  (0.0107)}  & \makecell[c]{194.1200  (2.9303)} \\
		& LCD  &\makecell[c]{0.1174  (0.0139)} & \makecell[c]{0.1394  (0.0113)} & \makecell[c]{0.1122  (0.0115)}  & \makecell[c]{0.1869  (0.0054)} & \makecell[c]{\textbf{0.6762}  (0.0113)} & \makecell[c]{0.3324  (0.0078)}  & \makecell[c]{\textbf{181.1600}  (1.8179)} \\
		&PC-like &\makecell[c]{0.0391  (0.0067)} & \makecell[c]{0.0303  (0.0062)} & \makecell[c]{0.0161  (0.0063)}  & \makecell[c]{0.0076  (0.0009)} & \makecell[c]{0.0909  (0.0101)} & \makecell[c]{0.0054  (0.0029)}  & \makecell[c]{231.7400  (1.0909)} \\
		& PC & NA & NA & NA & \makecell[c]{0.0296  (0.0019)} & \makecell[c]{0.0898  (0.0051)} & \makecell[c]{0.0109  (0.0031)}  & \makecell[c]{242.6000  (1.3613)} \\
		%& Glasso  &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} \\
		\hline 
		$(100,500)$ & Proposed &\makecell[c]{\textbf{0.3475}  (0.0092)} & \makecell[c]{\textbf{0.9987}  (0.0009)} & \makecell[c]{\textbf{0.5832}  (0.0080)}  & \makecell[c]{\textbf{0.3480}  (0.0051)} & \makecell[c]{0.5383  (0.0068)} & \makecell[c]{\textbf{0.3984}  (0.0059)}  & \makecell[c]{\textbf{732.9800}  (5.7756)} \\
		& LCD  &\makecell[c]{0.0279  (0.0028)} & \makecell[c]{0.0860  (0.0089)} & \makecell[c]{0.0396  (0.0048)}  & \makecell[c]{0.0751  (0.0019)} & \makecell[c]{\textbf{0.5693}  (0.0093)} & \makecell[c]{0.1882  (0.0043)}  & \makecell[c]{794.2800  (3.0063)} \\
		&PC-like &\makecell[c]{0.0167  (0.0020)} & \makecell[c]{0.0411  (0.0058)} & \makecell[c]{0.0152  (0.0034)}  & \makecell[c]{0.0011  (0.0001)} & \makecell[c]{0.0307  (0.0039)} & \makecell[c]{-0.0084  (0.0008)}  & \makecell[c]{859.3400  (2.4988)} \\
		& PC & NA & NA & NA & \makecell[c]{0.0057  (0.0004)} & \makecell[c]{0.0421  (0.0030)} & \makecell[c]{-0.0114  (0.0011)}  & \makecell[c]{885.5600  (2.5778)} \\
		%& Glasso  &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} \\
		\hline
		$(100,1000)$ & Proposed &\makecell[c]{\textbf{0.4088}  (0.0101)} & \makecell[c]{\textbf{0.9988}  (0.0008)} & \makecell[c]{\textbf{0.6334}  (0.0081)}  & \makecell[c]{\textbf{0.3631}  (0.0053)} & \makecell[c]{0.4876  (0.0066)} & \makecell[c]{\textbf{0.3825}  (0.0061)}  & \makecell[c]{\textbf{775.5400}  (7.2310)} \\
		& LCD  &\makecell[c]{0.0236  (0.0022)} & \makecell[c]{0.0780  (0.0082)} & \makecell[c]{0.0340  (0.0040)}  & \makecell[c]{0.0900  (0.0022)} & \makecell[c]{\textbf{0.6349}  (0.0088)} & \makecell[c]{0.2210  (0.0045)}  & \makecell[c]{779.8200  (3.0436)} \\
		&PC-like &\makecell[c]{0.0193  (0.0020)} & \makecell[c]{0.0349  (0.0037)} & \makecell[c]{0.0131  (0.0026)}  & \makecell[c]{0.0016  (0.0002)} & \makecell[c]{0.0377  (0.0042)} & \makecell[c]{-0.0072  (0.0009)}  & \makecell[c]{872.3000  (2.4148)} \\
		& PC & NA & NA & NA & \makecell[c]{0.0077  (0.0005)} & \makecell[c]{0.0500  (0.0032)} & \makecell[c]{-0.0090  (0.0013)}  & \makecell[c]{896.0600  (2.5596)} \\
		%& Glasso  &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} &\makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} & \makecell[c]{  ()} \\
		\hline
	\end{tabular}
	\label{tab:ex1}
\end{table}


\begin{table}[!h]
	\caption{ The averaged evaluation metrics of all the  methods in Example 2 together with
		their standard errors in parentheses. Here ** denotes the fact that the corresponding methods take too long to produce any results.}
	\tiny
	\centering
	\begin{tabular}{ccccccccc}
		\hline 
		$ (p, n)$& Method & Recall($\widehat{\Omega}$) & Precision($\widehat{\Omega}$) & MCC($\widehat{\Omega}$) & Recall($\widehat{B}$) & Precision($\widehat{B}$) &  MCC($\widehat{B}$)   & SHD   \\\hline   
		$(50,500)$ & Proposed & \makecell[c]{\textbf{0.5229} (0.0263)} & \makecell[c]{\textbf{0.7843}  (0.0157)} & \makecell[c]{\textbf{0.6255}  (0.0215)} & \makecell[c]{\textbf{0.3272} (0.0178)} & \makecell[c]{\textbf{0.5651}  (0.0249)} & \makecell[c]{\textbf{0.4063}  (0.0210)}  & \makecell[c]{\textbf{128.8400}  (7.2189)}  \\
		& LCD  &\makecell[c]{0.4510  (0.0290)} & \makecell[c]{0.5184  (0.0261)} & \makecell[c]{0.4662  (0.0271)} & \makecell[c]{0.1646  (0.0102)} & \makecell[c]{0.5469  (0.0192)} & \makecell[c]{0.2764  (0.0108)} & \makecell[c]{132.3600  (7.9336)} \\
		&PC-like & \makecell[c]{0.4548  (0.0331)} & \makecell[c]{0.4296  (0.0257)} & \makecell[c]{0.4225  (0.0292)} & \makecell[c]{0.0231  (0.0024)} & \makecell[c]{0.1668  (0.0146)} & \makecell[c]{0.0439  (0.0052)}  & \makecell[c]{149.3600  (8.9086)} \\
		& PC & NA & NA & NA & \makecell[c]{0.1637  (0.0149)} & \makecell[c]{0.2490  (0.0140)} & \makecell[c]{0.1655  (0.0130)}  & \makecell[c]{160.1200  (8.2415)} \\
		%& Glasso  &  & & &  & () &  ()&  () &  ()\\
		\hline 
		$(50,1000)$ & Proposed & \makecell[c]{\textbf{0.5704} (0.0293)} & \makecell[c]{\textbf{0.7719}  (0.0166)} & \makecell[c]{\textbf{0.6481}  (0.0231)} & \makecell[c]{\textbf{0.3568} (0.0204)} & \makecell[c]{ 0.5571  (0.0252)} & \makecell[c]{\textbf{0.4195}  (0.0225)} & \makecell[c]{\textbf{128.7800}  (7.8260)}  \\
		& LCD  &\makecell[c]{0.4723  (0.0301)} & \makecell[c]{0.4873  (0.0218)} &  \makecell[c]{0.4609  (0.0255)} & \makecell[c]{0.1885  (0.0110)} & \makecell[c]{\textbf{0.5721}  (0.0173)} &  \makecell[c]{0.3052  (0.0119)} & \makecell[c]{\textbf{128.7800}  (7.8959)} \\
		&PC-like &\makecell[c]{0.4584  (0.0309)} & \makecell[c]{0.4108  (0.0239)} & \makecell[c]{0.4138  (0.0269)} & \makecell[c]{ 0.0205  (0.0025)} & \makecell[c]{ 0.1529  (0.0181)} & \makecell[c]{0.0379  (0.0059)} & \makecell[c]{149.9800  (8.8576)} \\
		& PC & NA & NA & NA & \makecell[c]{0.1848  (0.0176)} & \makecell[c]{0.2599  (0.0142)} & \makecell[c]{0.1828  (0.0156)} & \makecell[c]{159.1600  (8.5205)} \\
		%& Glasso  & & & & & () &  ()&  () &  ()\\
		\hline 
		$(100,500)$ & Proposed &\makecell[c]{\textbf{0.3560}  (0.0090)} & \makecell[c]{ \textbf{0.9984}  (0.0010)} & \makecell[c]{\textbf{0.5880}  (0.0076)} & \makecell[c]{\textbf{0.4124}  (0.0255)} & \makecell[c]{\textbf{0.7447}  (0.0270)} & \makecell[c]{\textbf{0.5438}  (0.0256)} & \makecell[c]{\textbf{155.0600}  (4.4310)} \\
		& LCD  & ** & ** & ** & ** & ** & ** &  **  \\
		&PC-like & ** & ** & ** & ** & ** & ** &  **  \\
		& PC & NA & NA & NA & \makecell[c]{0.2710  (0.0269)} & \makecell[c]{0.1013  (0.0087)} & \makecell[c]{0.1472  (0.0125)} & \makecell[c]{231.6000  (5.1892)} \\
		%& Glasso  & & & & & () &  ()&  () &  ()\\
		\hline
		$(100,1000)$ & Proposed &\makecell[c]{\textbf{0.5035}  (0.0103)} & \makecell[c]{\textbf{0.9977}  (0.0009)} & \makecell[c]{\textbf{0.7016}  (0.0073)} & \makecell[c]{\textbf{0.5882}  (0.0262)} & \makecell[c]{\textbf{0.8817}  (0.0155)} & \makecell[c]{\textbf{0.7115}  (0.0218)} & \makecell[c]{\textbf{114.5800}  (4.1264)} \\
		& LCD  & ** & ** & ** & ** & ** & ** &  ** \\
		&PC-like & ** & ** & ** & ** & ** & ** &  **  \\
		& PC & NA & NA & NA & \makecell[c]{0.2875  (0.0264)} & \makecell[c]{0.1065  (0.0091)} & \makecell[c]{0.1562  (0.0127)} & \makecell[c]{228.1400  (4.9927)} \\
		%& Glasso  & & & & & () &  ()&  () &  ()\\
		\hline
	\end{tabular}
	\label{tab:ex2}
\end{table}

From Tables \ref{tab:ex1} and \ref{tab:ex2}, it is clear that the proposed method outperforms all competitors in most scenarios.  
In Example 1, the proposed method produces a much better estimation of the undirected edges than all other methods. For directed edges, the proposed method achieves the highest Recall$(\widehat{\BB})$ and MCC$(\widehat{\BB})$. It is interesting to note that LCD gets higher Precision$(\widehat{\BB})$ than the proposed method, possibly due to the fact that LCD tends to produce fewer estimated directed edges, resulting in large Precision$(\widehat{\BB})$ but small Recall$(\widehat{\BB})$. In Example 2, the proposed method outperforms all competitors in terms of almost all the evaluation metrics. Note that LCD and PC-like take too long to produce any results when $p=100$, due to their expensive computational cost when there exist many hub nodes.

%Furthermore, if we tune the parameters in the proposed method, its performance can be further improved but at the cost of more computation.

\subsection{Standard \& Poor index data}

We apply the proposed method to study the relationships among stocks in the Standard \& Poor's 500 index, and analyze the impact of the COVID-19 pandemic on the stock market. Chain graph can accurately reveal various relationships among stocks, with undirected edges for symmetric competitive or cooperative relationship between stocks and directed edges for asymmetric causal relation from one stock to the another. %The impact of the COVID-19 pandemic is also evident from the substantially different chain graphs constructed from pre-pandemic and post-pandemic.

To proceed, we select $p = 100$ stocks with the largest market sizes in the Standard \& Poor's 500 index, and retrieve their adjusted closing prices during the pre-pandemic period, August 2017-February 2020, and the post-pandemic period, March 2020-September 2022.  The data is publicly available on many finance websites and has been packaged in some standard softwares, such as the R package \texttt{quantmod}. For each period, we first calculate the daily returns of each stock based on its adjusted closing prices, and then apply the proposed method to construct the corresponding chain graph. 

Figure \ref{fig:financeUG} displays the undirected edges between stocks in both estimated chain graphs, which consist of $39$ and $21$ undirected edges in pre-pandemic and post-pandemic, respectively. It is clear that 
there are more estimated undirected edges in the 
pre-pandemic chain graph than in the post-pandemic one, which echos the empirical findings that business expansion were more active, company cooperations were closer and competition were fiercer before the COVID-19 pandemic. Furthermore, there are $13$ common undirected edges in both chain graphs, and all these 13 connected stock pairs are from the same sector, including VISA(V) and MASTERCARD(MA), JPMORGAN CHASE(JPM) and BANK OF AMERICA(BAC), MORGAN STANLEY(MS) and GOLDMAN SACHS(GS), and HOME DEPOT(HD) and LOWE'S(LOW).  All these pairs share the same type of business, and their competition or cooperation receive less impact by the COVID-19 pandemic. In Figure \ref{fig:financeUG}, it is also interesting to note that the number of the undirected edges between stocks from different sectors have reduced in the post-pandemic chain graph. This concurs with the fact that diversified business transactions between companies have been decreased and only essential business contacts have been maintained during the COVID-19 pandemic.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{figures/finance_UG1.PNG}
	\caption{The left and right panel display all the estimated undirected edges for pre-pandemic and post-pandemic, respectively. Stocks from the same sectors are dyed with the same color, and the common undirected edges in both chain graphs are boldfaced.}\label{fig:financeUG}
\end{figure}

Figure \ref{fig:financeOrder} displays the boxplots of causal orderings of all stocks within each sector in both pre-pandemic and post-pandemic, where the causal ordering of a stock is set as that of the corresponding chain component. It is generally believed that causal ordering implies the imbalance of social demand and supply; that is, if a sector is getting more demanded, its causal ordering is inclined to move up to upstream. Evidently, Energy and Materials are always at the top of the causal ordering in both periods, as they are upstream industries and provide inputs for most other sectors. The median causal ordering of Telecommunication Services goes from downstream to upstream after the outbreak of the COVID-19 pandemic, since people travel less and rely more on telecommunication for business communication. The median causal ordering of Finances goes down during the pandemic, as commercial entities are more cautious about credit expansion and demand for financial services is likely to decline to battle the financial uncertainty. It is somewhat surprising that the median causal ordering of Healthcares appears invariant, but many pharmaceutical and biotechnology corporation in this section actually have changed from downstream to upstream, due to the rapid development vaccine and treatment during the pandemic. 

\begin{figure}[!htb]
	\centering
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/finance_beforeOrder1.PNG}
	\end{minipage}
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{figures/finance_afterOrder1.PNG}
	\end{minipage}
	\caption{The left and right panel display the boxplots of the estimated causal ordering of the top 100 stocks in each sector for pre-pandemic and post-pandemic, respectively. The sectors are ordered according to the median causal ordering of stocks in post-pandemic.}\label{fig:financeOrder}
\end{figure}

In addition, the estimated chain graphs in pre-pandemic and post-pandemic consist of $149$ and $190$ directed edges, respectively. While many directed edges remain unchanged, there are some stocks whose roles have changed dramatically in the chain graphs. In particular, some stocks with no child but multiple parents in the pre-pandemic chain graph become ones with no parent but multiple children in the post-pandemic chain graph, such as COSTCO(COST), APPLE(AAPL), ACCENTURE(ACN), INTUIT(INTU), AT\&T(T) and CHUBB(CB). This finding appears reasonable, as most of these stocks correspond to the high demanded industries during pandemic, such as COSTCO for stocking up groceries, AT\&T for remote communication, and APPLE for providing communication and online learning equipment. On the other hand, there are some other stocks with no parent but multiple children in the pre-pandemic chain component  becoming
ones with no child but multiple parents in the post-pandemic chain component, including TESLA(TSLA), TJX(TJX), BRISTOL-MYERS SQUIBB(BMY), PAYPAL(PYPL), AUTOMATIC DATA PROCESSING(ADP) and BOEING(BA). Many of these companies have been severely impacted during pandemic, such as BOEING due to minimized travels and TESLA due to shrunk consumer purchasing power.

\section{Conclusion}\label{sec:conclusion}

In this paper, we establish a set of novel identifiability conditions for the Gaussian chain graph model under AMP interpretation, exploiting a low rank plus sparse decomposition of the precision matrix.
An efficient learning algorithm is developed to recover the exact chain graph structure, including both undirected and directed edges. Theoretical analysis shows that the proposed method consistently reconstructs
the exact chain graph structure. Its advantage is also supported by various numerical experiments on both simulated and real examples. It is also interesting to extend the proposed identifiability conditions and learning algorithm to accommodate non-linear chain graph model with non-Gaussian noise.

\section*{Acknowledgment}

This work is supported in part by HK RGC Grants GRF-11304520, GRF-11301521 and GRF-11311022.







\bibliography{ref} 
%\bibliographystyle{plain}{}
\bibliographystyle{apalike}

\end{document}





















