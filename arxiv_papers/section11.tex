Bayesian inverse problems arise frequently in science and engineering, with applications ranging from subsurface and atmospheric transport to chemical kinetics. The primary task of such problems is to recover spatially varying unknown parameters from noisy and incomplete observations. Quantifying the uncertainty in the unknown parameters \cite{efendiev2006preconditioning,marzouk2007stochastic,wang2016gaussian,huan2013simulation,li2015adaptive,lieberman2010parameter,cui2016dimension} is then essential for predictive modeling and simulation-based decision-making.

The Bayesian statistical approach provides a foundation for inference from data and past knowledge. Indeed, the Bayesian setting casts the inverse solution as a posterior probability distribution over the unknown parameters. Though conceptually straightforward, characterizing the posterior, e.g., sample generation, marginalization, computation of moments, etc., is often computationally challenging especially when the dimensionality of the unknown parameters is large. 
% marginalizing; evaluating moments---faces the computational challenges.
%In most practical problems, the posterior distributions do not admit an analytical form and particularly when complex physical models enter the likelihood function---numerical approaches are needed. 
The most commonly used method for posterior simulation is Markov Chain Monte Carlo (MCMC) \cite{robert1999monte}. MCMC is an exact inference method and easy to implement. However, MCMC suffers from many limitations. An efficient MCMC algorithm depends on the design of effective proposal distributions, which becomes difficult when the target distribution contains strong correlations, particularly in high-dimensional cases.  Moreover, MCMC often requires a large number of iterations, where the forward model needs to be solved at each iteration. If the model is computationally intensive, e.g., a PDE with high-dimensional spatially-varying parameter, MCMC becomes prohibitively expensive.  
While considerable efforts have been
devoted to reducing the computational cost, e.g., \cite{lieberman2010parameter,li2014adaptive,cui2015data,jiang2017multiscale,liao2019adaptive,wang2018adaptive}, many challenges still remain in inverse problems.
Furthermore, the iteration process of MCMC is not associated with a clear convergence criterion to imply when the process has adequately captured the posterior.

As an alternative strategy to MCMC sampling, variational inference (VI) is widely used to approximate posterior distributions in Bayesian inference. Compared to MCMC, VI tends to be faster and easier to scale to large data. The idea of VI is to seek the best approximation of the posterior distribution within a family of 
parameterized density models. In \cite{goh2022solving, xia2023vi, tewari-deeppriors-2022}, coupling deep generative priors with VI to solve Bayesian inverse problems is studied.  
%first posit a family of densities and then to find a member of that family
%which is close to the posterior. Closeness is measured by Kullback–Leibler divergence. 
However, for high-dimensional distributions, it is still very challenging to
obtain an accurate posterior approximation due to the curse of dimensionality. For instance, the commonly used mean-field approach \cite{blei2017variational} assumes mutual independence between dimensions to achieve a tractable density model, which in general results in underestimated second-order moments.
%is usually under-parameterized and thus not sufficiently flexible to capture the full complexity of the true posterior.   
%where
%extra assumptions are often introduced for efficiency. For instance, a mean field approximation  
To remedy the issue, more capable density models are needed, where the mutual-independence assumption is relaxed. One strategy to do this is to seek 
%some researchers are interested in explicitly constructing 
a map that pushes the prior to the posterior, where the conditional dependence structure can be exploited and encoded into the map for more efficiency \cite{spantini2018inference}.  
%which has the more expressive power than classical VI. 
More specifically, the map transforms a random variable $z$, distributed according to the prior, into a random variable $y$, distributed according to the posterior. Such transformations can be viewed as transport maps between probability measures, %. The existence of a transport map is guaranteed, but such a map is seldom unique. 
whose existence is not unique. A certain structure needs to be introduced to determine a map. 
%Obtaining a particular map needs imposing additional particular structures. 
%Several methods are used to construct the map, e.g., parametric approximations of 
Some typical structures include the Knothe–Rosenblatt (K-R) rearrangement \cite{el2012bayesian}, neural ODE \cite{chen2018neural} and the composition of many simple maps used in flow-based deep generative models such as NICE \cite{Dinh_2014}, real NVP \cite{dinh2016density}, KRnet \cite{tang2020deep,adda_2022,wan2022vae}, to name a few.
%, including normalizing flow \cite{rezende2015variational}, real NVP \cite{dinh2016density}, KRnet %\cite{tang2020deep}. 
%The expressiveness of the map is essential for good performance, with under-expressive models leading to both %increased bias and under-estimation of posterior variance.
%In this work, we employ KRnet \cite{tang2020deep,adda_2022}, which adapts the structure of K-R rearrangement into a deep generative model and provides a better modeling capability than real NVP.
%richer, more faithful posterior approximations than real NVP. 

Two challenges need to be addressed for many practical Bayesian inverse problems. 
%when employing the map-based variational Bayes approach. 
%However, the application of Bayesian inference based on the construction of maps to practical inverse problems %leads to two significant challenges.  The first is that 
%First, Bayesian inference requires an explicit formula of prior, however, the prior knowledge may be only %available implicitly in terms of historical data or previously acquired solutions. Most problems use simple %distribution as the prior density. However, the true prior may be much more complex and then using the %handcrafted prior density may cause biased posterior. 
First, prior knowledge is often available in terms of historical data or previously acquired solutions, which should be consistent with the prior distribution of Bayesian inference. Unfortunately, the true prior may be much more complex than any commonly used explicit density models. The second challenge is the curse of dimensionality, which demands a trade-off between density approximation and sample generation for high-dimensional cases. 
%Especially, the parameter of interest suffers the curse of dimensionality in many practical problems, which renders the construction, representation and evaluation of KRnet intractable.
% Aside from the computational considerations, the prior does not have a closed form, but we only have access to the prior data. 
To deal with these two challenges, data-driven priors and dimension reduction can be incorporated.  For example, VAE-priors \cite{goh2022solving, xia2023vi, tewari-deeppriors-2022, xia2022bayesian,zhihang2023domain} and GAN priors \cite{patel2022solution} are proposed to learn the prior distribution from data, where a mapping from the low-dimensional latent space to the high-dimensional parameter space is established and MCMC or VI is subsequently implemented in terms of the latent random variable. To further increase efficiency, a surrogate model can be employed to avoid the expensive forward problem and the adjoint problem  \cite{tromp2005seismic,cao2003adjoint} that are needed for either sampling or variational inference approaches. One popular choice for surrogate modeling is the deep neural network which is able to provide a good approximation of high-dimensional parametric PDEs. For example, physics-informed neural networks (PINN) \cite{raissi2019physics} has attracted broad attention for solving PDEs, which embeds the laws of physics into the loss function. Zhu et al. \cite{zhu2018bayesian,zhu2019physics} propose a dense convolutional encoder-decoder network for PDEs with high-dimensional random inputs.

% In this work, we develop a variational Bayes approach in the latent space for both density approximation and sample generation. 
In this work, we propose a dimension-reduced KRnet map approach (DR-KRnet) for high-dimensional Bayesian inverse problems.
The main idea is to approximate the posterior distribution in terms of the latent variable that is learned from historical data, where VAE is used for dimension reduction and KRnet is used for density approximation. We first use abundant historical data to train a VAE prior, where we use the learned decoder to transfer the inference to the latent variable. We then minimize the Kullback-Leibler divergence between the posterior for the latent variable and the density model induced by KRnet. Using the decoder and the approximate posterior of the latent variable, we can compute the desired statistics efficiently because KRnet defines a transport map that provides exact samples with neglected costs. To further increase the efficiency, we also develop a convolutional encoder-decoder network as the surrogate model \cite{zhu2018bayesian,zhu2019physics}. Compared to sampling-based approaches, the main advantages of our strategy are twofold: First, we take advantages of two capable deep generative models, i.e., VAE and KRnet, to obtain an explicit density model that is sufficiently expressive for a high-dimensional posterior distribution. Second, KRnet, a normalizing flow model, is able to effectively deal with a moderately large number of dimensions and can be more robust than MCMC. 
%by approximating the posterior distribution of the latent random variable instead of sampling it.  
% To address these two challenges, data-driven priors are expressed by well-traineVariational Autoencoder (VAE) with prior data. 
%The main advantages are twofold: KRnet can be more effective and robust than MCMC when the number of dimensions is moderately large, and our approach results in an explicit density model for the posterior distribution of the latent random variable. 
%Combining the dimension reduction from VAE, numerical efficiency from the surrogate model, and the transport map from KRnet, we propose an explicit density approximation of the posterior, where the likelihood and exact samples can be efficiently obtained.  We first learn a VAE prior from the data. Using the obtained decoder, we use the KRnet-induced density model to approximate the posterior distribution of the latent variable. The approximate distribution is then used to compute the desired statistics such that MCMC is avoided. 

This paper is organized as follows. In section \ref{section_problem}, we describe the formulation of the Bayesian inverse problems that will be considered in this work. In section \ref{section_method}, our dimension-reduced KRnet map approach (DR-KRnet) for high-dimensional Bayesian inverse problems is presented, where we provide a scheme for building the neural network structure of VAE priors, introduce the KRnet to construct the map between the prior and posterior and build the physics-constrained surrogate model. In section \ref{section_experiments}, with two numerical experiments we demonstrate that our DR-KRnet can infer high-dimensional parameters efficiently. The paper is concluded in section \ref{section_conclude}.