% Preamble ------------------------------------------------------
% Page limit for JASA is about 35 pages, INCLUSIVE of
% title, abstract, reference, tables, figures

% the references alone take up around 5 pages, so this leaves us with 30 
% to work with

% We are now currently at 37 pages, almost there!

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt]{article}

% JASA margins
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\addtolength{\textheight}{.5in}%
\addtolength{\textheight}{-.3in}%

% allow affiliations
\usepackage{authblk}
% Double spacing for draft paper, remove for tighter spacing
\usepackage{setspace}
% fancy W font
\usepackage{mathrsfs}
\doublespacing

% add in marginnotes
\usepackage{marginnote}
\reversemarginpar
% make margin notes small
\renewcommand\marginfont{%
        \normalfont\scriptsize
    }
% cheap way to disable margin notes
% redfine it so that it does nothing
\renewcommand{\marginnote}[2][]{}

\usepackage{graphicx}
% it seems that amsthm is the defualt, and most robust theorem package
\usepackage{amsmath, amssymb, amsthm}

% new theorem is created, whose counter is reset with every section
\newtheorem{theorem}{Theorem}[section]
% new corollary environemnt is created, whose counter resets with every call of 
% theorem
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}
\newtheorem{sublemma}{Lemma}[lemma]
\newtheorem{proposition}{Proposition}
% remarks are typically not numbered
\newtheorem{remark}{Remark}
\newtheorem{assump}{Assumption}
% this requires the use of the amssymb
\renewcommand\qedsymbol{$\blacksquare$}

% Customize tranpose macro here
\newcommand*{\tran}{\intercal}

\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Literature Review},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs

% Make margins larger so that marginnote works properly
%\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm, heightrounded, marginparwidth=2.5cm, marginparsep=3mm]{geometry}
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{afterpage}
\usepackage{calc} % for calculating minipage widths
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

% Setup hyperref so that you can use \autoref
\usepackage{hyperref}
% hyperref^\trans autoref is good, but apparently cleveref is even better
\usepackage{cleveref}
% Fix up plurals so that it works for assumptions
\crefname{assump}{Assumption}{Assumptions}
\Crefname{figure}{Figure}{Figures}

% JASA spacing
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

% enumerate package for customizing lists --------------------------------
\usepackage{bm}
% change font here
% \usepackage[charter]{mathdesign}
\usepackage[shortlabels]{enumitem}

% \usepackage[thmmarks, amsmath, thref]{ntheorem}

% Landscape
\usepackage{pdflscape}

\theoremstyle{plain}
% \theorembodyfont{\itshape}

%\theoremheaderfont{\itshape}

\numberwithin{equation}{section}

% Set watermark for DRAFT status 
% neither of my supervisors are happy about circulating the draft before 
% proper submission/release as a working paper
% \usepackage{draftwatermark}
% \SetWatermarkText{DRAFT}
% \SetWatermarkScale{1}

\usepackage{physics}
% uniquename = false gets rid of first names, apparently this is what Elsevier likes
\usepackage[style=apa,uniquename=false,uniquelist=false,eprint=false,url=false,annotation=false]{biblatex}
\DeclareDelimFormat{nameyeardelim}{\addcomma\space}
% not sure why, but only bibtex format works, 
% even thouygh I tried to specify biblatex
\addbibresource{references2.bib}

% Add some custome enumitem propositions, lemmas, theorems, etc
\newlist{propenum}{enumerate}{1} % also creates a counter called 'propenumi'
\setlist[propenum]{label=(\alph*), ref=extcheproposition~(\alph*)}
\crefalias{propenumi}{proposition} 

\newlist{lemenum}{enumerate}{1} % also creates a counter called 'propenumi'
\setlist[lemenum]{label=(\alph*), ref=\thelemma~(\alph*)}
\crefalias{lemenumi}{lemma} 

\newlist{thmenum}{enumerate}{1} % also creates a counter called 'propenumi'
\setlist[thmenum]{label=(\alph*), ref=\thetheorem~(\alph*)}
\crefalias{thmenumi}{theorem} 

\newlist{assumpenum}{enumerate}{1} % also creates a counter called 'propenumi'
\setlist[assumpenum]{label=(\alph*), ref=\theassump~(\alph*)}
\crefalias{assumpenumi}{assump} 

% Reverse specify supplementary material to get referencing
\usepackage{xr}
\externaldocument[ext:]{supplementary_paper}
\pdfminorversion=4

%% Begin Documents ======================================================
% For the purposes of anonymization, we will need to get rid of the \href email

%\footnote{We thank Xu Han and Fa Wang for making their code available.}}
%\author{Bonsoo Koo \thanks{Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia. Email: \href{mailto:bonsoo.koo@monash.edu}{bonsoo.koo@monash.edu}}}
%\author{Benjamin Wong \thanks{Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia. Email: \href{mailto:benjamin.wong@monash.edu}{benjamin.wong@monash.edu}}}
%\author{\anonymize{Ze Yu Zhong} \thanks{
%\anonymize{Corresponding author. Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia. Email:
%\href{mailto:ze.zhong@monash.edu}{ze.zhong@monash.edu}}}}
\begin{document}
% JASA Blinding
% Adjust the numebr here to turn on/off blinding
\newcommand{\blind}{1}

\if1\blind
{
  \title{\bf Disentangling Structural Breaks in High Dimensional Factor Models \thanks{We acknowledge that this research was supported in part by the Monash eResearch Centre and eSolutions Research Support Services through the use of the MonARCH HPC Cluster.} \thanks{We acknowledge the financial support of the Australian Research Council under Grants LP160101038, DP210101440, and DE200100693.} \thanks{We thank Xu Han and Fa Wang for making their code available, in addition to the comments by Xu Han, Daniele Massacci, Mirco Rubin, Matteo Barigozzi, Serena Ng, and other participants at the 2022 EC-squared conference.}}
\author{Bonsoo Koo \thanks{Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia and Centre for Applied Macroeconomic Analysis, Australian National University Email: bonsoo.koo@monash.edu}}
\author{Benjamin Wong \thanks{Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia and Centre for Applied Macroeconomic Analysis, Australian National University. Email: benjamin.wong@monash.edu}}
\author{Ze Yu Zhong \thanks{
Corresponding author. Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia. Email:
ze.zhong@monash.edu}}
  \affil{Monash University}
	% Add date that the document was compiled
  \date{Dated: Feb. 2023}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Disentangling Structural Breaks in High Dimensional Factor Models}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
We disentangle structural breaks in dynamic factor models by establishing a \emph{projection based equivalent representation theorem} which decomposes any break into a rotational change and orthogonal shift. Our decomposition leads to the natural interpretation of these changes as a change in the factor variance and loadings respectively, which allows us to formulate two separate tests to differentiate between these two cases, unlike the pre-existing literature at large. We derive the asymptotic distributions of the two tests, and demonstrate their good finite sample performance. We apply the tests to the FRED-MD dataset focusing on the Great Moderation and Global Financial Crisis as candidate breaks, and find evidence that the Great Moderation may be better characterised as a break in the factor variance as opposed to a break in the loadings, whereas the Global Financial Crisis is a break in both. Our empirical results highlight how distinguishing between the breaks can nuance the interpretation attributed to them by existing methods.
\end{abstract}

\noindent%
{\it Keywords:} factor space, structural instability, breaks, principal components, dynamic factor models
\vfill

%% Declare Macros -------------------------------------------------------

% Basic statistics
\newcommand{\convp}{\overset{p}{\to}}
\newcommand{\convd}{\overset{d}{\to}}
\newcommand{\limN}{\lim_{N \to \infty}}
\newcommand{\limT}{\lim_{T \to \infty}}
\newcommand{\plim}{\operatorname{plim}}
\newcommand{\plimT}{\operatorname{plim}_{T \to \infty}}
\newcommand{\plimN}{\operatorname{plim}_{N \to \infty}}
\newcommand{\plimNT}{\operatorname{plim}_{N,T \to \infty}}
% Fractions
\newcommand{\fracT}{\frac{1}{T}}
\newcommand{\fracTone}{\frac{1}{T_1}}
\newcommand{\fracTtwo}{\frac{1}{T_2}}
\newcommand{\fracTm}{\frac{1}{T_m}}
\newcommand{\fracN}{\frac{1}{N}}
\newcommand{\fracTN}{\frac{1}{TN}}

% Sums
\newcommand{\sumT}{\sum_{t = 1}^{T}}
\newcommand{\sumTs}{\sum_{s = 1}^{T}}
\newcommand{\sumTj}{\sum_{j = 1}^{T}}
\newcommand{\sumN}{\sum_{i = 1}^{N}}
\newcommand{\sumNj}{\sum_{j = 1}^{N}}
\newcommand{\sumNk}{\sum_{k = 1}^{N}}

\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil }
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor }
\newcommand{\sumTfloor}{\sum_{t = 1}^{\floor{\pi T}}}
\newcommand{\sumTfloort}{\sum_{t = \floor{\pi T + 1}}^{T}}

\newcommand{\sumTfloors}{\sum_{s = 1}^{\floor{\pi T}}}
\newcommand{\sumTfloorts}{\sum_{s = \floor{\pi T + 1}}^{T}}
% \newcommand{\rank}[1]{\operatorname{rank} #1 }
\newcommand{\gt}{>}
\newcommand{\lt}{<}

% Op term macros
\newcommand{\Opdelta}{O_p\left( \frac{1}{\delta_{NT}}\right) }
\newcommand{\Opdeltasq}{O_p\left( \frac{1}{\delta_{NT}^2}\right)}
\newcommand{\Opdeltaquart}{O_p\left( \frac{1}{\delta_{NT}^4}\right)}

\newcommand{\OprtTdeltasq}{O_p\left( \frac{\sqrt{T}}{\delta_{NT}^2}\right)}
\newcommand{\OprtNdeltasq}{O_p\left( \frac{\sqrt{N}}{\delta_{NT}^2}\right)}

\newcommand{\OprtT}{O_p\left( \frac{1}{\sqrt{T}}\right)}
\newcommand{\OprtN}{O_p\left( \frac{1}{\sqrt{N}}\right)}
\newcommand{\OpT}{O_p\left( \frac{1}{T}\right)}
\newcommand{\OpN}{O_p\left( \frac{1}{N}\right)}
% General Op macro for anything arbitrary
\newcommand{\Op}[1]{O_p \left( #1 \right) }

% Indicator function
\newcommand{\indicator}[1]{\mathbf{1} \left\lbrace #1 \right\rbrace }

% Abstract ---------------------------------------------------------------
% Bonsoo says introduction could be a little longer
% but Ben says skimming through this it is good for a conference submission already
% Bonsoo thinks there is no discussion of the use of break points, by earlier results
% also, need to differentiate our appraoch from existing ones (Bonsoo still thinks it is not clear enough)

% Bonsoo says to disregard strict apa style (putting first name letter for disambiguity)
% not sure how to implement this...
\newpage
\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}
High dimensional factor models are widely used in empirical macroeconomics and finance, and assume that a large panel of time series are generated according to some small number of latent factors. Thus, a large dataset can be effectively parameterized by a set of individual ``loadings'' and a set of common ``factors,'' as a means of dimensional reduction, where one subsequently uses these factors for both forecasting (\textcite{stock_forecasting_2002}) and structural analysis (\textcite{bernanke_measuring_2005}). However, the theory underlying factor models assumes at least ``mild'' stability in model parameters (\textcite{stock_diffusion_1998}, \textcite{bai_inferential_2003}). In reality, empirical data is unlikely to maintain parameter stability, and the analysis of structural breaks in factor models presents a unique identification issue wherein breaks in the loadings and breaks in the factors cannot be easily disentangled.

Consider a factor model for $x_{it}, t = 1, \dots, T, i = 1, \dots, N$:
\begin{align}
x_{it} = \lambda_{i}^\tran f_t + e_{it},
\end{align}
where $\lambda_{i}$ is an $r \times 1$ vector of individual loadings, $f_t$ is an $r \times 1$ vector of common factors, and $e_{it}$ the idiosyncratic error. Suppose that there exists a structural break such that $\lambda_i$ doubles in value. Because both the factors and loadings are unobserved and enter in a multiplicative relationship, this is observationally equivalent to $f_t$ doubling in value. Consequently, it is typical for the literature to assume ``strict stationarity'' in the factors as an identification condition in order to pin down changes in the loadings, and necessarily interpret all ``breaks'' as occurring in the loadings (e.g. \textcite{chen_detecting_2014}, \textcite{han_tests_2015}, \textcite{baltagi_identification_2017}, and others).
% 
Such an interpretation could be misleading, because the literature has typically identified periods such as the Great Moderation (\textcite{stock_forecasting_2009}, \textcite{breitung_testing_2011}, \textcite{baltagi_estimating_2021}), the Global Financial Crisis (\textcite{ma_estimation_2018}, \textcite{barigozzi_sequential_2020}, \textcite{ma_group_2022}), and more recently the COVID-19 Pandemic (\textcite{bai_likelihood_2022}) as evidence of structural breaks, all of which are periods well known for the data displaying heteroscedasticity. Hence, it is unclear whether these results are capturing genuine breaks in the loadings, or simply picking up factor heteroscedasticity, two different cases with very different economic narratives, a concern initially raised by \textcite
{stock_chapter_2016}. In addition to their economic interpretations, differentiating these two cases is important from a mechanical viewpoint: breaks in the loadings can lead to the incorrect over-estimation of the number of factors if ignored, whereas breaks in the factors do not have this effect, a refinement of an existing concern put forth by \textcite{breitung_testing_2011}.

\marginnote[Added in clearer contribution, emphasizing that we need an (estimated) break fraction. Suggestion made by some people, particularly Fabio]{right}
Our contribution to the literature is a method testing whether these estimated breaks are breaks in the loadings, breaks in the factor variance, or both, thus disentangling the source of structural breaks. To this end, we propose a new \emph{projection based} equivalent representation theorem, which decomposes any change in the factor loading matrix into a rotational break common across the entire panel, and a leftover orthogonal shift component idiosyncratic to each series. Our \emph{projection-based} decomposition approach is motivated by the important mechanical differences of these breaks. Specifically, we observe that breaks in the factors can always be viewed as a twisting (rotation) of the same underlying factor space and are therefore absorbed into the factor estimates of the principal components (PC) estimator. Hence, this does not pose any issue for the purposes of determining the number of factors (such as using the criteria of \textcite{bai_determining_2002}), or applying the inferential results of \textcite{bai_inferential_2003}. Economically, such a break could be associated with the aforementioned Great Moderation, where the overall volatility of all series in the economy was observed to decrease. In contrast, due to their idiosyncratic nature, breaks in the factor loadings lie outside and are therefore orthogonal to the underlying factor space, and it is this orthogonality which leads to a so-called ``augmentation'' effect where the number of factors will be overestimated if ignored, as noted by \textcite{breitung_testing_2011}. This incorrect overestimation of the number of factors can have many serious consequences, including worsening factor based forecasts in the setup of \textcite{stock_forecasting_2002} as noted by \textcite{baltagi_estimating_2021}, or incorrect specification of factor models in a state space setup, which rely on PC based methods to estimate the number of factors. 

By interpreting the rotational change as a break in factor variance, and the orthogonal shift as a break in factor loadings, we are thus able to disentangle these two effects. We emphasize that this is in contrast to other similar equivalent representation theorem based approaches in the literature, who typically assume ``strict'' stationarity in the factors and interpret all breaks as breaks in the loadings (\textcite{chen_detecting_2014}, \textcite{han_tests_2015}, \textcite{baltagi_identification_2017}). Thus, although these have similar model setups, our \emph{projection based} equivalent representation theorem is a further refinement of existing methods. Based on this decomposition, we then propose two separate tests: 1) a test for any evidence of rotational change, or factor variance, and 2) a test for any evidence of orthogonal shifts, or breaks in the loadings. We establish the asymptotic distributions of these two test statistics, and show that standard critical values can be used, leading to their easy implementation. Monte Carlo studies demonstrate that the tests have good size and power, and highlight the inability of existing tests to differentiate between these two types of breaks. 

% Closest literature to our work
% rewrite this in a better way
% Bonsoo says to rewrite it so instead of criticising others, we are promoting ours
To the best of our knowledge, only a few contributions similarly try to disentangle structural breaks in factor models.  \textcite{wang_identification_2021} proposes an estimator for the number of breaks using eigenvalue ratios which is robust to changes in factor variance, but do not consider testing due to the difficulty in working with the distribution of eigenvalues. Our test statistics do not rely on eigenvalues, and hence we are able to derive standard asymptotic results and avoid this issue. Indeed, our test statistics converge to conventional Chi-squared distributions, making them easy to implement for practitioners. \textcite{pelger_state-varying_2022} propose a general time varying framework, and construct a test statistic which tests whether the factor loadings in two regimes can be fully explained by a rotation via canonical correlations, essentially amounting to a test for evidence of orthogonal shifts, or legitimate breaks in loadings. Compared to their test statistic, our tests do not require the difficult estimation of a de-bias term, which is known to be non-robust to the specification of the error structure (e.g. \textcite{su_time-varying_2017} and \textcite{su_testing_2020}). \textcite{massacci_testing_2021} proposes a test statistic for evidence of regime dependent loadings in a threshold setup that is robust against factor heteroscedasticity. All of these contributions similarly recognise that legitimate breaks in the loadings should be orthogonal to the original factor space, but their model setups, test statistics, and resulting asymptotics are all quite different. Compared to these existing papers, our setup is the only one which considers separately testing for evidence in the factor variance and loadings, and hence provides the most comprehensive framework to accurately pin down the \emph{source} of structural breaks. 

We apply our tests to the FRED-MD dataset of \textcite{mccracken_fred-md_2015} and focus on the two estimated break dates put forth by the literature: the Great Moderation, dated to be around 1984, and the Global Financial Crisis, dated to be around late 2008. We find that for the case of only one factor, the Great Moderation only rejects on the rotational test. Our orthogonal shift test could be interpreted as a test for evidence of breaks in the loadings while controlling for changes in the factor variance, and in this vein when compared to the tests of \textcite{breitung_testing_2011}, these results suggest that for the case of one factor (supported by the estimators of \textcite{onatski_determining_2010} and \textcite{ahn_eigenvalue_2013}), the Great Moderation is more accurately described as a break in the factor variance, as opposed to a break in the loadings. Although we reject both the rotational and orthogonal shift test for the case of two or more factors, for the case of two factors, we find that most of the evidence for breaks in loadings is isolated to price series, and hence depending on the application, may not pose issues for the practitioner. In contrast, the evidence for the Global Financial Crisis tends to favour a break in both the factor variance and the loadings. These results bring nuance to how these different periods of instability can be characterised, which could lead to different implications. 

% Technical notes
In this paper, all limits are taken as both $N, T$ tend to infinity simultaneously, and $\delta_{NT}$ is defined as $\min(\sqrt{T}, \sqrt{N})$. We use $\norm{\cdot}$ to denote the Frobenius norm of a vector or matrix, $\convp$ denotes convergence in probability, $\Rightarrow$ denotes weak convergence of stochastic processes, $\convd$ denotes convergence in distribution, $vech()$ denotes the column-wise vectorisation of a square matrix with the upper triangle excluded, and $\floor{\cdot}$ denotes the floor or integer part operator. We use $M$ to denote generic constants which may take different values, and $A^{-\tran}$ denotes the inverse transpose of any invertible matrix $A$.
% Main section to introduce model ------------------------------------------
\section{Disentangling Structural Breaks in Dynamic Factor Models}
% Use pi instead of tau, because tau is reserved for the error serial correlation quantities for the assumptions...
% Bonsoo says in the future to redo the strucute of this a bit
% provide some intuition behind *why* we wish to have the projection based ERT
\subsection{A Projection Based Equivalent Representation Theorem for Structural Breaks}
Let $x_{it}$ denote the observation for the $i$th cross section at period $t$ for $i = 1, \dots, N$ and $t = 1, \dots, T$. Let $\floor{\pi T}$ denote the break date, where $\pi$ is the break fraction which splits the data into subsample sizes of $T_1 = \floor{\pi T}$ and $T_2 = T - \floor{\pi T}$ respectively. Suppose that $x_{it}$ is generated from $r$ common factors with the following static factor representation:
\begin{align}
\label{eqn:static_factor_rep}
x_{it} &= 
\begin{cases}
\lambda_{1, i}^\tran f_t + e_{it}, \quad \text{for} \quad t = 1, \dots, \floor{\pi T}, \\
\lambda_{2, i}^\tran f_t + e_{it}, \quad \text{for} \quad t = \floor{\pi T} + 1, \dots, T,
\end{cases}
\end{align}
where $f_t$ is a $r \times 1$ vector of factors, $\lambda_{1, i}, \lambda_{2, i}$ are the corresponding $r \times 1$ loadings for series $i$ before and after the break respectively, and $e_{it}$ is the idiosyncratic shock. We require the number of factors $r$ to be identical before and after the break, because our method relies on subsample estimates after splitting the sample, a common regularity condition found in many other methods utilizing subsample estimates (e.g. \textcite{ma_estimation_2018} and \textcite{bai_estimation_2020}). Throughout the paper, we treat both the number of factors $r$ and the break fraction $\pi$ as known, as both of these can be consistently estimated (see \Cref{rem:r_estimation,rem:k_estimation}) without affecting any of our asymptotic results.

\Cref{eqn:static_factor_rep} can also be written using matrix form:
\begin{align}
\label{eqn:matrix_form}
X = 
\begin{bmatrix}
X_1 \\
X_2
\end{bmatrix} 
= \begin{bmatrix}
F_{1} \Lambda_1^\tran \\
F_{2} \Lambda_2^\tran 
\end{bmatrix} +
e,
\end{align}
where $F_{1} = (f_{1}, \dots, f_{\floor{\pi T}})^\tran$ is a $\floor{\pi T} \times r$ matrix of factors before the break, $F_{2} = (f_{\floor{\pi T} + 1}, \dots, f_{T})^\tran$ is a $(T - \floor{\pi T}) \times r$ matrix of factors after the break, and $\Lambda_1 = (\lambda_{1, 1}, \dots, \lambda_{1, N})^\tran$, $\Lambda_2 = (\lambda_{2, 1}, \dots, \lambda_{2, N})^\tran$ are both $N \times r$ matrices of respective loadings, and $X_1$, $X_2$ denote the respective partitions of $X$ based on the break fraction. The matrices $F_1, F_2, \Lambda_1, \Lambda_2, e$ are all unknown. 
% use \no number to specifically reference a line in align
To disentangle breaks in the factor loadings from breaks in the factors, we decompose $\Lambda_2 = \Lambda_1 Z + W$, where $Z$ represents an $r \times r$ nonsingular rotational change, and $W = (w_1, \dots, w_N)^\tran$ is an $N \times r$ matrix representing the orthogonal shift that is idiosyncratic across the cross section. It follows that this decomposition can be used to yield the following equivalent representation theorem: 
% Bonsoo says this layout of the matrices is confusing, and could be understood to be a partitioned matrix
\begin{align}
\label{eqn:projection_ert:1}
X &= 
\begin{bmatrix}
F_1 \Lambda_1^\tran \\
F_2 [\Lambda_1 Z + W]^\tran 
\end{bmatrix} + 
\begin{bmatrix}
e_{(1)} \\
e_{(2)}
\end{bmatrix} \\
&= \begin{bmatrix}
F_1 & 0 \nonumber \\
F_2 Z^\tran & F_2 
\end{bmatrix}
\begin{bmatrix}
\Lambda_1^\tran \\
W^\tran
\end{bmatrix} +
\begin{bmatrix}
e_{(1)} \\
e_{(2)}
\end{bmatrix} \nonumber \\
\label{eqn:projection_ert}
X &= G \Xi^\tran + e
\end{align}
where $e_{(1)} = (e_{1}, \dots, e_{\floor{\pi T}})$ and $e_{(2)} = (e_{\floor{\pi T} + 1}, \dots, e_T)$, and each $e_t = (e_{1t}, \dots, e_{Nt})^\tran$. \Cref{eqn:projection_ert} shows that any rotational changes induced by a non-identity $Z$ are absorbed into the factors, and any orthogonal shifts $W$ will result in the augmentation of the factor space. \Cref{eqn:projection_ert} is a version of an equivalent representation theorem (ERT), and re-expresses a factor model with structural breaks in its loadings into an observationally equivalent model with time invariant loadings. ERTs were initially formulated by \textcite{han_tests_2015}, \textcite{baltagi_identification_2017} and others, and \Cref{eqn:projection_ert} aims to complement these. If one were to ignore the break and naively use the PC estimator over the whole subsample, it will instead be consistent for an observationally equivalent model with so called \emph{pseudo} factors $G$ and time invariant loadings $\Xi$.
% Change notation of pseudo loadings ot be Xi because Theta is used later on somewhere
% The wording is bad here, but I can't think of a better way to express it
% discuss with Han about ``augmentation'' and whether this is the best terminology

Previously, it has been thought that changes in the loadings cannot be separately identified from changes in the variance of the factors, which can be equivalently be represented as a rotational change common to all loadings. This is because existing methods used the pseudo factors $G$ in order to either test for existence of any breaks (\textcite{han_tests_2015}, \textcite{chen_detecting_2014}), and/or estimate the break fraction (\textcite{baltagi_identification_2017}, \textcite{baltagi_estimating_2021}, \textcite{duan_quasi-maximum_2022}). 
% This part is not clear, need to elbaorate more on why the use of pseudo factors is limiting
Methods utilizing the estimated pseudo factors from the whole sample will necessarily have power against heteroscedasticity in the factors, even if the loadings are actually time invariant. 

Our projection based formulation aims to differentiate between breaks in the factor variance versus breaks in the factor loadings, and is motivated by the mechanical properties of the PC estimator. It is well known that the PC estimator only estimates the underlying factor space up to an arbitrary rotation. However, any break in the factor variance can always be thought of some suitable twisting or stretching of the factors themselves, i.e. a rotation. Because the factors still span the same underlying space, breaks in the factor variance will be absorbed by the PC estimator. In contrast, if there are breaks in the loadings, due to their idiosyncratic nature the changes in each series cannot be explained by the existing factors, and therefore lie outside the space spanned by the factors. Hence, if one were to ignore the break, extra factors need to be estimated in order to capture the same information over the whole sample. We formalize these different mechanical effects of the PC estimator of these breaks by classifying them accordingly. 

% These classifications need to link back to the Projection based ERT more clearly
% Remember, the projection based ERT is the main sell of this paper (really)
We first define a ``type 1'' break as presence of orthogonal shifts where $W \neq \mathbf{0}$ and $Z = I_r$, and this corresponds to the type 1 break as defined by \textcite{han_tests_2015}, \textcite{baltagi_identification_2017}, and type A break by \textcite{duan_quasi-maximum_2022} as per respective nomenclatures. Due to the orthogonality of $W$ to the original factor space, these breaks cannot be absorbed into the factors, and hence can only be interpreted as a legitimate break in the loadings. With our setup, it is now more clearly understood that it is the orthogonality induced by breaks in the loadings that lead to the factor ``augmentation'' effect raised by \textcite{breitung_testing_2011}.

We next define a ``type 2'' break as a rotational break where $Z \neq I_r$ and $W = \mathbf{0}$. Type 2 breaks occur when all of the loadings across the cross section are rotated in a homogeneous way, or more naturally, a change in the factor variance. Indeed, it is quite difficult to imagine or justify such changes in the loadings practically, and they are often ruled out by assumption (e.g. \textcite{chen_detecting_2014} and \textcite{ma_group_2022}), or similarly to us, interpreted as a change in the factor variance (\textcite{wang_identification_2021} and \textcite{pelger_state-varying_2022}). Rotational breaks correspond to type 2 breaks of \textcite{han_tests_2015} and \textcite{baltagi_identification_2017}, and the type B breaks of \textcite{duan_quasi-maximum_2022}.
% Bonsoo says that nonsingular Z needs more elaboration/remark...
% For now, just keep it in the paper, and discuss further with Han
We require $Z$ to be non-singular, and hence this rules out the case of disappearing factors, which could be viewed as somewhat limiting compared to earlier literature which used methods based on the pseudo factors. This is because we require inferential results as estimated in each subsample before and after the break, and presently it is not clear how to do this in the presence of disappearing factors. Such a regularity assumption is quite common in the literature (e.g. see \textcite{chen_detecting_2014}, \textcite{su_time-varying_2017}, \textcite{ma_estimation_2018} and \textcite{bai_estimation_2020}).\footnote{Similar to methods that rule out disappearing factors, our method seems to have power against the case where there is a disappearing factor, but the theoretical results are unclear, similar to \textcite{chen_detecting_2014}.} 

Finally, we define a ``type 3'' break as simply a combination of the two breaks where there is both a rotation and orthogonal shift, i.e. $Z \neq I_r$ and $W \neq \mathbf{0}$.

Thus, the task of disentangling breaks in the factor variance from breaks in the loadings can be expressed in the form of two hypothesis tests: 1) a test for any evidence of rotation
\begin{align}
\label{eqn:rotation_null}
\mathcal{H}_0: Z = I_r, \quad \mathcal{H}_1: Z \neq I_r,
\end{align}
which does not maintain any conditions on $W$, and 2) a test for any evidence of orthogonal shifts:
\begin{align}
\label{eqn:orthogonal_null}
\mathcal{H}_0: W_{(N \times r)} = \mathbf{0}, \quad \mathcal{H}_1: W_{(N \times r)} \neq \mathbf{0},
\end{align}
which in turn does not maintain any conditions on $Z$. We emphasise that because these two types of breaks can occur together, \emph{both} tests need to be run in order to tease out which \emph{type} of break has occurred. 
% Our classification is more precise compared to previous formulations in the literature. For example, the often cited example of a type 1 break is when all the loadings shift in a certain direction (\textcite{han_tests_2015}), but such an example would typically also induce a non-identity $Z$ to exist, and thus is more accurately a type 3 break. 
% Present some estimation results 
\subsection{Estimation and Consistency Results}
\subsubsection{Estimation}
We now discuss estimation and the asymptotic properties of the estimators. Define $\tilde{\Lambda}_1$ and $\tilde{\Lambda}_2$, the OLS fits from the estimates using the PC estimates $\tilde{F}_1 = (\tilde{f}_{1, t}, \dots, \tilde{f}_{1, \floor{\pi T}})^\tran$ and $\tilde{F}_2 = (\tilde{f}_{2, \floor{\pi T} + 1}, \dots, \tilde{f}_{2, T})$, which are $\sqrt{T_1}$ and $\sqrt{T_2}$ times the eigenvectors corresponding to the $r$ largest eigenvalues of $X_1 X_1^\tran$ and $X_2 X_2^\tran$ respectively. We define the feasible estimators for $Z$ and $W$ as
\begin{align}
\label{eqn:Z_W_estimation}
\tilde{Z} &= (\tilde{\Lambda}_1^\tran \tilde{\Lambda}_1)^{-1} \tilde{\Lambda}_1^\tran \tilde{\Lambda}_2, \\
\label{eqn:W_estimation}
\tilde{W} &= \tilde{\Lambda}_2 - \tilde{\Lambda}_1 \tilde{Z}.
\end{align}
Because $\tilde{\Lambda}_1$ and $\tilde{\Lambda}_2$ are estimates of $\Lambda_1$ and $\Lambda_2$ up to arbitrary rotations, $\tilde{Z}$ and $\tilde{W}$ cannot be directly interpreted, and the task of disentanglement is not straightforward. However, it turns out that $\tilde{Z}$ and $\tilde{W}$ are able to recover the true $Z$ and $W$ up to arbitrary rotations as well. To analyze them, we make the following assumptions. Let $\iota_{1t} \equiv \indicator{t \leq \floor{\pi T}}$ and $\iota_{2t} \equiv \indicator{t \geq \floor{\pi T} + 1}$. 
\subsubsection{Estimation Assumptions}
% Assumption 1 Factors
\begin{assump}
\label{ass:1_factors}
$E\norm{f_t}^4 \lt \infty$, $E(f_t f_t^\tran) = \Sigma_F$ and $\fracT \sumT f_t f_t^\tran \convp \Sigma_F$ for some positive definite $\Sigma_F$.
\end{assump}
% Assumption 2 Loadings
\begin{assump}
\label{ass:2_loadings}
% Not entirely sure if this needs to be redone - currently the loadings are viewed as fixed in the first section, but later they are necessairly viewed as random
For $m = 1, 2$, 
there exists a positive constant $M$ such that
$E \norm{\lambda_{m, i}}^4 \leq M$, $\norm{ \Lambda_m^{ \tran} \Lambda_m/N} - \Sigma_{{\Lambda_m}} \convp 0$ for some $\Sigma_{\Lambda_m} > 0$, and $\norm{\Lambda_{m}^\tran \Lambda_m / N - \Sigma_{\Lambda_m}} = O_p\left( N^{-1/2}\right) $. Analogously, when $W \neq 0$, $\norm{ W^{ \tran} W/N - \Sigma_{{W}} } \convp 0$ for some $\Sigma_{W} > 0$, and $\norm{W^\tran W / N - \Sigma_{W}} = O_p\left(N^{-1/2}\right) $.
\end{assump}
% Assumption 3 Errors
\begin{assump}
\label{ass:3_errors}
There exists some positive constant $M \lt \infty$ such that for all $N$ and $T$:
\begin{assumpenum}
\item \label{ass:3_errors:1} 
$E(e_{it}) = 0, E\abs{e_{it}}^8 \leq M$
\item 
$E(e_s^\tran e_t /N) = E(N^{-1} \sumN e_{is}e_{it}) = \gamma_{N}(s, t)$, $\left| \gamma_{N}(s, s) \right| \leq M $ for all $s$, and \\
$T^{-1}\sumT \sumTs \left| \gamma_{N}(s, t) \right| \leq M$.
\item \label{ass:3_errors:2} 
$E(e_{it}e_{jt}) = \tau_{ij, t}$, with $\left| \tau_{ij, t} \right| \lt \tau_{ij}$ for some $\tau_{ij}$ and for all $t$. In addition, \\
$N^{-1} \sumN \sumNj \left| \tau_{ij}\right| \leq M$.
\item \label{ass:3_errors:3} 
$E(e_{it}e_{js}) = \tau_{ij, ts}$, and $(NT)^{-1} \sumN \sumNj \sumT \sumTs \left| \tau_{ij, ts} \right| \leq M$.
\item \label{ass:3_errors:4} 
For every $(t, s)$, $E\left| N^{-1/2} \sumN [e_{is}e_{it} - E(e_{is}e_{it})]\right|^4 \leq M$.
\end{assumpenum}
\end{assump}
% Assumption 4 weak dependence between factors and errors
\begin{assump}
\label{ass:4_ind_groups}
% Replaced this with independent groups assumption
%$E \left( \frac{1}{N} \sumN \norm{T^{-1/2} \sumT f_t e_{it} \cdot \iota_{mt}) }^2 \right) \leq M$ for $m = 1, 2$.
For $m = 1, 2$, the variables $\left\lbrace \lambda_{m, i} \right\rbrace$, $\left\lbrace f_t \right\rbrace $ and $\left\lbrace e_{it}\right\rbrace $ are  mutually independent groups.
\end{assump}
% Assumption 5 (assumption E from Bai, weak dependence)
\begin{assump}
\label{ass:5_error_corr}
There exists an $M \lt \infty$ such that for all $T$ and $N$, and for every $t \leq T$ and $i \leq N$ such that:
\begin{assumpenum}[wide = 0pt, leftmargin = 3em]
\item \label{ass:5_error_corr:1} 
$\sumTs \abs{\gamma_{N}(s, t)} \leq M$ 
\item \label{ass:5_error_corr:2}
$\sumNk \abs{\tau_{ki}} \leq M$
\end{assumpenum}
\end{assump}
% Assymption 6 Moments
% firs two are just existence of moments and follow from Bai, third one is new in HI but not restrictive
\begin{assump}
\label{ass:6_moments}
There exists an $M \lt \infty$ such that for all $N, T$ and $m = 1, 2$:
\begin{assumpenum}
\item \label{ass:6_moments:1} 
$E \norm{\frac{1}{NT} \sumTs \sumNk f_s [e_{ks} e_{kt} - E(e_{ks} e_{kt})] \cdot \iota_{ms} }^2 \leq M$ for each $t$,
\item \label{ass:6_moments:2} 
$E \norm{\frac{1}{\sqrt{NT}} \sumT \sumNk f_t \lambda_{m, k}^\tran e_{kt} \cdot \iota_{mt} }^2 \leq M$,
\item \label{ass:6_moments:3}
For each $t$
$E \norm{\frac{1}{\sqrt{N}} \sumN \lambda_{m, i} e_{it}}^4 \leq M$.
\end{assumpenum}
\end{assump}
% Assymption 7 Distinct Eigenvalues
\begin{assump}
\label{ass:7_eigen_distinct}
The eigenvalues of $(\Sigma_{\Lambda_1} \Sigma_F)$ and $(\Sigma_{\Lambda_2} \Sigma_F)$ are distinct.
\end{assump}
\begin{assump}
% Assumption 8 Break Fraction
\label{ass:8_break_fraction}
The break fraction $\pi$ is bounded away from 0 and 1, and
\begin{assumpenum}
\item \label{ass:8_break_fraction:1}
$
\norm{\frac{1}{\sqrt{NT}} \sumTfloor \sumNk f_t \lambda_{m, k}^\tran e_{kt} \iota_{mt}}^2 = \Op{1}$, 
$\norm{\frac{1}{\sqrt{NT}} \sumTfloort \sumNk f_t \lambda_{m, k}^\tran e_{kt} \iota_{mt}}^2 = \Op{1}
$
for $m = 1, 2$, and
\item \label{ass:8_break_fraction:2}
$\norm{\frac{\sqrt{T}}{\floor{\pi T}} \sumTfloor (f_t f_t^\tran - \Sigma_F)} = \Op{1}$, and $\norm{\frac{\sqrt{T}}{T - \floor{\pi T}} \sumTfloort (f_t f_t^\tran - \Sigma_F)} = \Op{1}$
\end{assumpenum}
\end{assump}
% Some comments on explaining these assumptions
% Bonsoo says that these are not enough, and all papers should have some degree of self containment
% i.e. need to briefly explain what each assumption is doing, and why is it necessary
% Just referencing existing work is not enough
\Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct} are either straight from or slight modifications of those in \textcite{bai_inferential_2003}. \Cref{ass:1_factors} is the same as Assumption A in \textcite{bai_inferential_2003}, except that we require the second moment of $f_t$ to be time invariant. This additional ``strict'' stationarity assumption is common as an identification condition (e.g. \textcite{han_tests_2015}, \textcite{baltagi_identification_2017} and others) and necessarily limited the factors to exhibit no heteroscedasticity, but this is not restrictive in our case as all changes in $\Sigma_F$ are characterized by $Z$. \Cref{ass:2_loadings} is the same as Assumption B in \textcite{bai_inferential_2003}, except that it specifies the convergence speed of $\Lambda_m^\tran \Lambda_m/N$ to be no slower than $1/\sqrt{N}$ for $m = 1, 2$. \Cref{ass:2_loadings} allows for the loadings to be random, and although this is not required for the purposes of estimation and the $Z$ rotation test, it is required for the $W$ orthogonal shift tests, and we therefore combine this assumption for simplicity. \Cref{ass:3_errors,ass:5_error_corr} correspond exactly to Assumptions C and E in \textcite{bai_inferential_2003}. 
% Bonsoo is uncertain about the wording here, even though it is standard in this literature...
\Cref{ass:3_errors} allows for weak serial and cross sectional correlation and define the \emph{approximate} factor model. \Cref{ass:5_error_corr} is a strengthened version of \Cref{ass:3_errors}, but still allows for heterogeneity in time and cross-sectional dimensions. \Cref{ass:4_ind_groups} is standard in the factor modeling literature, and is the subsample version of Assumption D of \textcite{bai_confidence_2006}. \Cref{ass:7_eigen_distinct} corresponds to Assumption G in \textcite{bai_inferential_2003}. \Cref{ass:6_moments} corresponds to Assumptions F1-F2 in \textcite{bai_inferential_2003}. Although we require \Cref{ass:6_moments} which are moment conditions in \textcite{bai_inferential_2003}, asymptotic normality of $N^{-1/2} \sumN \lambda_i e_{it}$ are not required for the purposes of estimation. Also, \Cref{ass:6_moments:3} is slightly stronger that Assumption F3 of \textcite{bai_inferential_2003}, which only requires the existence of the second moments. \Cref{ass:8_break_fraction} requires that the sample sizes before and after the potential break date go to infinity. It is a weaker version of Assumption 8 in \textcite{han_tests_2015}, who assumes that the terms are bounded uniformly in a range of potential $\pi$.

% Formally define the estimated factors ftilde here, along with the H matrices
% define rotation matrices here, and make a remark about there being another observationally equivalent formulation
Recall that $\tilde{F}_1$ and $\tilde{F}_2$ are estimates of $F_1$ and $F_2$ up to two different arbitrary rotations. Specifically, we define the rotational basis in the first subsample as $H_1 = \left( \frac{ \Lambda_1^\tran \Lambda_1}{N} \right) \left( \frac{F_1^\tran \tilde{F}_1}{T_1} \right) V_{NT, 1}^{-1}$, and in the second subsample as $H_2 = \left( \frac{Z^\tran \Lambda_1^\tran \Lambda_1 Z}{N} + \frac{W^\tran W}{N} \right) \left( \frac{F_2^\tran \tilde{F}_2}{T_2} \right) V_{NT, 2}^{-1}$, where $V_{NT, 1}, V_{NT, 2}$ denote the diagonal matrix of eigenvalues of the first $r$ eigenvalues of $(NT_1)^{-1}X_1 X_1^\tran$ and $(NT_2)^{-1} X_2 X_2^\tran$ respectively.\footnote{There exists another observationally equivalent parameterization \\
$H_2 = \left(\Lambda_1^\tran \Lambda_1 + Z^{-\tran}W^\tran W Z^{-1} \right) \left( ZF_2^\tran \tilde{F}_2 \right)/(NT_2) V_{NT, 2}^{-1}$, 
where the rotation $Z$ is parameterized as part of the factors. It is straightforward to verify that either parameterization leads to same result stated in \Cref{thm:2:Z_consistency}. For more details, see \Cref{ext:prf:alternative_rotation} in the Supplementary Material.}
\begin{theorem}
\label{thm:1:Ztilde_consistency}
Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction}, $
\norm{\tilde{Z} - H_1^\tran Z H_2^{-\tran}} = \Opdeltasq$.  
\end{theorem}
Although \Cref{thm:1:Ztilde_consistency} shows that $\tilde{Z}$ itself is estimated up to a rotation and cannot be directly interpreted, the specific formulation of $\tilde{Z}$ allows us to present the following result.
% This is the main link and result, and is the crux of why the rotational test works
\begin{theorem}
\label{thm:2:Z_consistency}
Under  \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction}, and as $\frac{\sqrt{N}}{T} \to \infty$, for each $t$: $
\norm{\tilde{Z} \tilde{f}_{2, t} - H_1^\tran Z f_{t}} = o_p(1)$. 
\end{theorem}
\Cref{thm:2:Z_consistency} is a direct consequence of $\norm{\tilde{f}_{2, t} - H_2^\tran f_{t}} = o_p(1)$ from Lemma 1 of \textcite{bai_inferential_2003}, and \Cref{thm:1:Ztilde_consistency}. \Cref{thm:2:Z_consistency} shows that post multiplying $\tilde{f}_2$ by $\tilde{Z}$ rotates it back to the same rotational basis as $\tilde{f}_1$, and maintains the rotation $Z$, if any. Thus, if we combine $\tilde{f}_{1, t}$ and $\tilde{Z} \tilde{f}_{2, t}$ together, we have the following.
\begin{corollary}
\label{cor:Z_cor}
Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction}, and as $\frac{\sqrt{N}}{T} \to 0$, for each $t$:
\begin{align}
\widehat{f}_t = 
\begin{cases}
\tilde{f}_{1, t} \convp H_1^\tran f_t \quad &\text{for} \quad t = 1, 2, \dots, \floor{\pi T}, \\
\tilde{Z} \tilde{f}_{2, t} \convp H_1^\tran Z f_t \quad &\text{for} \quad t = \floor{\pi T} + 1, \dots, T.
\end{cases}
\end{align}
\end{corollary}
% Bonsoo says this bit is a bit confusing
\Cref{cor:Z_cor} shows that the combined series $\widehat{F} = (\widehat{f}_1, \dots, \widehat{f}_T)^\tran$ is on the same rotational basis both before and after the break, and can thus form the basis of a test for evidence of rotational breaks. Importantly, $\widehat{f}_t$ is free from the effects of any possible orthogonal shifts induced by $W$, and thus isolates the rotational change in the factor variance. 
\begin{theorem}
\label{thm:3:Wtilde_consistency}
Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction},
$\frac{1}{N} \norm{\tilde{W} - W H_{2}^{-\tran}}^2 = 
\Opdeltasq.
$
\end{theorem}
\Cref{thm:3:Wtilde_consistency} shows that $\tilde{W}$ estimates the true $W$ up to an arbitrary rotation. Thus, if the true $W = \mathbf{0}$, then $\tilde{W}$ should also be close to zero, and can serve as a foundation for statistical tests.

% New section for the actual test statistics
% Z test ---------------------------------------------------------
\subsection{Z Test for Rotational Changes}
% Follow the literature and just present the actual formula for the test statistic
% first, then introduce additional assumptions to show consistency results
We first present the test statistic for evidence of rotational change $\mathcal{H}_0: Z = I$ against $\mathcal{H}_1: Z \neq I$. Recall that by combining $\tilde{F}_{1}$ and $\tilde{F}_{2} \tilde{Z}^\tran$, we have $\widehat{F}$, an estimate of the true factors and any rotation they undergo. This motivates a Wald test statistic based on whether the subsample means of $\widehat{f}_t \widehat{f}_t^\tran$ are equal at a predetermined\footnote{We treat the break fraction as known \emph{a priori} for simplicity, but any consistent estimate of this can be used instead without affecting any of the results, as noted in \Cref{rem:k_estimation}.} break date $\floor{\pi T}$:
\begin{align}
\label{eqn:Z_test_stats}
\mathscr{W}_Z(\pi, \widehat{F}) = A_Z(\pi, \widehat{F})^\tran \widehat{S}_Z (\pi, \widehat{F})^{-1} A_Z(\pi, \widehat{F}),
\end{align}
where $A_Z(\pi, \widehat{F}) = 
vech\left( 
	\sqrt{T} \left( \frac{1}{\floor{\pi T}} \sumTfloor \widehat{f}_t \widehat{f}_t^\tran - \frac{1}{T - \floor{\pi T}} \sumTfloort \widehat{f}_t \widehat{f}_t^\tran \right)
\right)$. Its long run variance estimate is defined as
$
\widehat{S}_Z(\pi, \widehat{F}) = \frac{1}{\pi} \widehat{\Omega}_{Z, (1)}(\pi \widehat{F}) + \frac{1}{1 - \pi} \widehat{\Omega}_{Z, (2)}(\pi, \widehat{F}),
$
a weighted average of the variance from pre and post break data ($m = 1, 2$ respectively)
\begin{align}
\widehat{\Omega}_{Z, (m)}(\pi, \widehat{F}) &= \widehat{\Gamma}_{(m), 0} (\pi, \widehat{F}) \nonumber + 
\sum_{j = 1}^{T_m - 1} k \left( \frac{j}{b_{T_m}} \right) \left( \widehat{\Gamma}_{(m), j} (\pi, \widehat{F}) + \widehat{\Gamma}_{(m), j} (\pi, \widehat{F}))^\tran \right), \\
\widehat{\Gamma}_{(1), j} (\pi, \widehat{F}) &=  
\frac{1}{T_1} \sum_{t = j + 1}^{T_1}
vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)
vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)^\tran, \\
\widehat{\Gamma}_{(2), j} (\pi, \widehat{F}) &= 
\frac{1}{T_2} \sum_{t = j + T_1 + 1}^{T}
vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)
vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)^\tran,
\end{align}
where $k(.)$ is a real valued kernel, and $b$ is the bandwidth, and its subscripts denotes the size of the (sub)samples used to estimate the long run variance.
% Now discuss asymptotics
\subsubsection{Z Test Asymptotics under the Null Hypothesis}
We define $\mathscr{W}_Z(\pi, FH_{0, 1}) = A_Z(\pi, FH_{0, 1})^\tran \widehat{S}_Z (\pi, FH_{0, 1})^{-1} A_Z(\pi, FH_{0, 1})$ as the infeasible analog of $\mathscr{W}_Z(\pi, \widehat{F})$, and make the following assumptions.
%Before discussing the properties of the test statistic, it is helpful to describe some notations and existing results. 
%%Recall that $V_{NT, 1}, V_{NT, 2}$ denote the $r \times r$ diagonal matrices of the first $r$ eigenvalues of $(1/NT_1) X_1 X_1^\tran$ and $(1/NT_2) X_2 X_2^\tran$ respectively. 
%% Redone, second subsample notation not needed and used ever again
%Lemma A3 of \textcite{bai_inferential_2003} shows that $V_{NT, 1}, V_{NT, 2}$ converge to $V_1$, a diagonal matrix consisting of the eigenvalues of $\Sigma_{\Lambda_1}^{1/2} \Sigma_F \Sigma_{\Lambda_1}^{1/2}$. Let $\Upsilon_1$ denote its eigenvectors such that $\Upsilon_1^\tran \Upsilon_1 = I_r$. 
%Recall that by the property of principal components, $\tilde{F}_1$ is $\sqrt{T_1}$ times the eigenvectors corresponding to the $r$ largest eigenvalues of $X_1 X_1^\tran$ and estimates $F_1$ up to the rotation matrix $H_1 = \left( \frac{ \Lambda_1^\tran \Lambda_1}{N} \right) \left( \frac{F_1^\tran \tilde{F}_1}{T_1} \right) V_{NT, 1}^{-1}$ as defined earlier.  Proposition 1 of \textcite{bai_inferential_2003} shows that $F_1^\tran \tilde{F}_1/ T_1$ converges to $\Sigma_{\Lambda_1}^{-1/2}\Upsilon_1 V_1^{1/2} = H_{0, 1}^{-\tran}$. It follows that $H_1 \convp \Sigma_{\Lambda_1}^{1/2} \Upsilon_1 V_1^{-1/2} = H_{0, 1}$, its probability limit. Under the null hypothesis of no rotational change where $Z = I$, we have:
%\begin{align}
%\label{eqn:rotation-expectation}
%E(H_{0, 1}^\tran Z f_t f_t^{\tran} Z^\tran H_{0, 1}) = H_{0, 1}^\tran \Sigma_F H_{0, 1} = I_r,
%\end{align}
%which is implied by the definition of $H_{0, 1}$, and the fact that $V_{1}^{-1/2} \Upsilon_1^\tran \Sigma_{\Lambda_1}^{1/2} \Sigma_F \Sigma_{\Lambda_1}^{1/2} \Upsilon_1 V_1^{-1/2} = V_1^{-1/2}V_1V_{1}^{-1/2} = I_r$.
%% This wording here is a bit strange, according to Bonsoo
%Although $F$ and $H_{0, 1}$ are both unobservable, by \Cref{cor:Z_cor} $\widehat{F}$ is an estimate of $F H_1$, and $H_1 \convp H_{0, 1}$ (see \textcite{bai_inferential_2003}). Therefore, \Cref{eqn:rotation-expectation} is the link between the feasible test statistics using the estimated factors and the infeasible test statistics using the true factors. We make the following additional assumptions. 
\begin{assump}
\label{ass:9:hac_conditions}
\begin{assumpenum}
\item \label{ass:9:hac_conditions:1}
The Bartlett kernel of \textcite{newey_simple_1987} is used, and there exists a constant $K \gt 0$ such that $b_T, b_{\floor{\pi T}}$ and $b_{T - \floor{\pi T}}$ are less than $K T^{1/3}$; and
\item \label{ass:9:hac_conditions:2}
$\frac{T^{2/3}}{N} \to 0$ as $N, T \to \infty$.
\end{assumpenum}
\end{assump}
\begin{assump}
\label{ass:10:HAC_assump}
\begin{assumpenum}
\item \label{ass:10:HAC_assump:1}
$\Omega_Z = \limT Var\left( vech\left(  \frac{1}{\sqrt{T}} \sumT H_{0, 1}^{\tran} f_t f_t^\tran H_{0, 1} - I_r \right) \right)$ is positive definite, and $\norm{\Omega_Z} < \infty$. Its estimators $\widehat{\Omega}_{Z, (m)}(\pi, FH_{0, 1})$ for $m = 1, 2$ are consistent such that $\norm{\widehat{\Omega}_{Z, (m)}(\pi, FH_{0, 1}) - \Omega_Z} = o_p(1)$,
\item \label{ass:10:HAC_assump:2} 
$\mathscr{W}_Z(\pi, FH_{0, 1}) \Rightarrow Q_p(\pi)$, where $Q_p(\pi) = [B_p(\pi) - \pi B_p(1)]^\tran [B_p (\pi) - \pi B_p(1)]/(\pi(1 - \pi))$, and $B_p(\cdot)$ is a $p = r(r + 1)/2$ vector of independent Brownian motions on $[0, 1]$. 
\end{assumpenum}
\end{assump}
\Cref{ass:9:hac_conditions} specifies conditions for the Bartlett kernel.
\Cref{ass:10:HAC_assump:1} is a standard HAC assumption, and states that the infeasible estimators $\widehat{\Omega}_{Z, (1)}(\pi, FH_{0, 1})$, $\widehat{\Omega}_{Z, (2)}(\pi, FH_{0, 1})$ and $\widehat{\Omega}_{Z}(\pi, FH_{0, 1})$ converge to their population counterpart $\Omega_Z$.
\Cref{ass:10:HAC_assump:2} is the main result of Theorem 3 of \textcite{andrews_tests_1993}, and is necessary to establish the asymptotic distributions of the test statistics. As stated by \textcite{andrews_tests_1993}, for any fixed $\pi$, $Q_p(\pi)$ is distributed as a $\chi^2_{p = r(r + 1)/2}$ random variable, and therefore standard critical values can be used. \Cref{ass:10:HAC_assump:2} has been used in \textcite{han_tests_2015}, and one can refer to \textcite{chen_detecting_2014} for more primitive assumptions under which similar assumptions to \Cref{ass:10:HAC_assump:2} hold. Note that we do not require the convergence of $\sup_{\pi} \mathscr{W}_Z(\pi, FH_{0, 1})$ to $\sup_{\pi} Q_p(\pi)$, as we are focusing on a pre-known (or estimated) break fraction $\pi$.
\begin{theorem}
\label{thm:6:Z_Wald_consistency}
Under  \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:9:hac_conditions,ass:10:HAC_assump}, and if $\frac{\sqrt{T}}{N} \to 0$, then $\mathscr{W}_Z(\pi, \widehat{F}) \convd \chi^2_{r(r + 1)/2}$.
\end{theorem}
The proof of \Cref{thm:6:Z_Wald_consistency} is provided in the Supplementary Material, and involves proving the convergence of $\widehat{\Omega}_{Z, (1)}(\pi, \widehat{F})$, $\widehat{\Omega}_{Z, (2)}(\pi, \widehat{F})$, $\widehat{S}_Z(\pi, \widehat{F})$ and $A_Z(\pi, \widehat{F})$ to their infeasible counterparts. 
\Cref{thm:6:Z_Wald_consistency} shows that the feasible Wald test statistic converges to a Chi-squared random variable, and conventional critical values can be used.\footnote{It is also possible to construct an LM-like statistic with a restricted estimate of the variance using all of the data. However, as noted by \textcite{chen_detecting_2014} and \textcite{han_tests_2015}, such LM-like statistics have much smaller power than their Wald-type counterparts. Therefore, we focus on the Wald test.}
% Theorem A_Z
%\begin{theorem}
%\label{thm:4:A_Z}
%Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction}, if $\frac{\sqrt{T}}{N} \to \infty$, then
%\begin{align}
%\norm{A_Z(\pi, (\widehat{F})) - A_Z(\pi, FH_{0, 1})} \convp 0.
%\end{align}
%\end{theorem}
%\Cref{thm:4:A_Z} shows that $A_Z(\pi, \widehat{F})$ converges to its true population counterpart $A_Z(\pi, FH_{0, 1})$, and therefore will have the same asymptotic distribution. We next define the sample variance of $A(\pi, FH_{0, 1})$. 
%%\begin{align}
%%\widehat{\Omega}_{Z, (1)}(\pi, FH_{0, 1}) &= \widehat{\Gamma}_{(1), 0} (\pi, FH_{0, 1})) + 
%%\sum_{j = 1}^{\floor{\pi T} - 1} k \left( \frac{j}{b_{\floor{\pi T}}} \right) \left( \widehat{\Gamma}_{(1), j} (\pi, FH_{0, 1}) + \widehat{\Gamma}_{(1), j} (\pi, FH_{0, 1}))^\tran \right) , \\
%%\widehat{\Omega}_{Z, (2)}(\pi, FH_{0, 1}) &= \widehat{\Gamma}_{(2), 0} (\pi, FH_{0, 1})) + 
%%\sum_{j = \floor{\pi T} + 1}^{T - \floor{\pi T} - 1} k \left( \frac{j}{b_{T - \floor{\pi T}}} \right) \left( \widehat{\Gamma}_{(2), j} (\pi, FH_{0, 1}) + \widehat{\Gamma}_{(2), j} (F^\tran H_{0, 1}))^\tran \right) ,
%%\end{align}
%
%% Theorem for the consistency of the variance estimates
%\begin{theorem}
%\label{thm:5:Z_var_consistency}
%Under  \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:9:hac_conditions}, 
%\begin{align}
%\norm{\widehat{S}(\pi, \widehat{F}) - \widehat{S}(\pi, FH_{0, 1})} \convp 0.
%\end{align}
%\end{theorem}
%\Cref{thm:5:Z_var_consistency} shows that the infeasible sample variance can be replaced with the estimates provided by $\widehat{F}$. Next, we define the infeasible analogues of the Wald test statistics in \Cref{eqn:Z_test_stats}:
%\begin{align}
%\mathscr{W}_Z(\pi, FH_{0, 1}) = A(\pi, FH_{0, 1})^\tran \widehat{S}_Z (\pi, FH_{0, 1})^{-1} A(\pi, FH_{0, 1})
%\end{align}

% Remark on LM like statistic -------------------------------------------
% but ultimately not used because this is less powerful
% Maybe get rid of this remark, relegate to footnote
%\begin{remark}
%It is also possible to define an LM-like statistic:
%\begin{align}
%\label{eqn:Z_test_LM}
%LM_Z(\pi, \widehat{F}) = A_Z(\pi, \widehat{F})^\tran \tilde{S}_Z (\pi, \widehat{F})^{-1} A_Z(\pi, \widehat{F}),
%\end{align}
%where $\tilde{S}_Z(\pi, \widehat{F})$ is a restricted estimate of the variance covariance matrix using the whole sample of data without imposing a break:
%\begin{align}
%\label{eqn:lm_variance}
%\tilde{S}_Z(\pi, \widehat{F}) &= \left( \frac{1}{\pi} + \frac{1}{1 - \pi} \right) \widehat{\Omega}(\pi, \widehat{F}), \\
%\widehat{\Omega}(\pi, \widehat{F}) &= \widehat{\Gamma}_{0} (\pi, \widehat{F}) + 
%\sumTj k \left( \frac{j}{B_{T}} \right) (\widehat{\Gamma}_{j} (\pi, \widehat{F}) + \widehat{\Gamma}_{j} (\pi, \widehat{F})^\tran),
%\end{align}
%where similarly 
%\begin{align*}
%\widehat{\Gamma}_{j} (\pi, \widehat{F}) = \frac{1}{T} \sum_{t = j + 1}^T 
%vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)
%vech( \widehat{f}_t \widehat{f}_t^\tran - I_r)^\tran. 
%\end{align*}
%However, as noted by \textcite{chen_detecting_2014} and \textcite{han_tests_2015}, such LM-like statistics have much smaller power than their Wald-type counterparts. Therefore, we focus on the Wald test.
%\end{remark}

%% Some assumptions required for the HAC to work
\subsubsection{Z Test Asymptotics under the Alternative Hypothesis}
To analyse the power of the $Z$ test under the alternative, we make the following additional assumptions on the break:
% Additional assumptions on the rotational break
\begin{assump}
\label{ass:11:Z_test_alter}
$Z$ is a non-singular matrix, and $Z \Sigma_F Z^\tran \neq \Sigma_F$.
\end{assump}
\begin{assump}
\label{ass:12:z_alter_variance} $
plim_{T \to \infty} \operatorname{inf} 
\left( vech(C)^\tran \left[ \operatorname{max}(b_{\floor{\pi T}}, b_{T - \floor{\pi T}} ) \widehat{S}(F^* H_{0, 1})^{-1} \right] vech(C) \right) \gt 0$, 
where $C \equiv H_{0, 1}^\tran (\Sigma_F - Z \Sigma_F Z^\tran) H_{0, 1}$.
\end{assump}
% talk a little bit more about the non-singular requirement of Z here...
\Cref{ass:11:Z_test_alter} ensures that the test statistic diverges under the alternative hypothesis. It rules out the unlikely scenario where $Z = -1$, i.e. all of the loadings switch their signs after the break, and is commonly assumed (see \textcite{han_tests_2015}, \textcite{baltagi_identification_2017}, \textcite{baltagi_estimating_2021}, and others).
We require $Z$ to be non-singular, which implicitly rules out the case of disappearing/emerging factors. This is because unlike \textcite{han_tests_2015}, \textcite{baltagi_estimating_2021}, and \textcite{duan_quasi-maximum_2022} who work with the \emph{pseudo} factors as estimated over the entire sample, we work with the subsample estimates of the factors and the appropriate definition of rotation matrices is not clear in such cases (other methods using subsample estimates similarly rule this out, see \textcite{chen_detecting_2014}, \textcite{massacci_least_2017}, \textcite{ma_estimation_2018}, and others).
\Cref{ass:12:z_alter_variance} regulates the asymptotic property of the variance matrices of the statistics, ensures that $\mathscr{W}_Z(\pi, \widehat{F})$ diverges under the alternative.
%Define $F^* = [f_1, \dots, f_{\floor{\pi T}}, Z f_{\floor{ \pi T} + 1}, \dots, Z f_T]^\tran = [f_1^*, \dots, f_T^*]^\tran$ for notational simplicity. Under the alternative hypothesis, $E(H_{0, 1}^\tran f_t^* f_t^{*\tran} H_{0, 1}) \neq I$, so the HAC estimators are not properly demeaned, and thus the test statistic will diverge as $N, T \to \infty$ (see the discussion in \textcite{hall_covariance_2000}).
% LM test ver
% plim_{T \to \infty} \operatorname{inf} 
% \left( vech(C)^\tran \left( S_T \tilde{S}(F^*H_{0, 1})^{-1} \right) vech(C) \right) \gt 0
\begin{theorem}
\label{thm:7:z_alter}
Under  \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:9:hac_conditions,ass:12:z_alter_variance}, and if $Z$ satisfies \Cref{ass:11:Z_test_alter}, then
\begin{enumerate}
\item there exists some non-random matrix $C \neq 0$ such that \\
$\frac{1}{\pi T} \sumTfloor \widehat{f}_t \widehat{f}_t^\tran \allowbreak - \frac{1}{T - \floor{\pi T}} \sumTfloort \widehat{f}_t \widehat{f}_t^\tran \convp C$,
\item the test statistic $\mathscr{W}_Z(\widehat{F})$ is consistent under the alternative hypothesis that $Z \neq I$.
\end{enumerate}
\end{theorem}
\Cref{thm:7:z_alter} shows that the subsample means of $\widehat{f}_t \widehat{f}_t^\tran$ converge to different limits under the alternative, and thus result in a consistent test.
% remark on r estimation
\begin{remark}
\label{rem:r_estimation}
The number of factors $r$ is assumed to be known, and constant before and after the break due to $Z$ being non-singular. Consistent estimation of the number of factors in each subsample is possible conditional on consistent estimate of $\pi$ using any pre-existing estimator (e.g. \textcite{bai_determining_2002}, \textcite{onatski_determining_2010}, or \textcite{ahn_eigenvalue_2013}) and $\pi$ as shown in \textcite{baltagi_identification_2017}. In practice, the estimate $\widehat{r}$ may not be same in either regime, and the practitioner will need to set the number of factors to be identical. Underestimation of $r$ could omit information in rotational changes, whereas overestimation of $r$ could result in extra noise brought about by the extra estimated factors, (\textcite{baltagi_identification_2017}). In practice, overestimation of $r$ tends to lead to oversizing (see \Cref{sec:monte_carlo}), so we advise a conservative estimate of $r$.
\end{remark}
% break fraction estimation
\begin{remark}
\label{rem:k_estimation}
Similarly, the break fraction $\pi$ needs to be estimated using a method such as \textcite{baltagi_identification_2017}, \textcite{chen_estimating_2015}, or \textcite{duan_quasi-maximum_2022}. Theorem 3 of \textcite{baltagi_identification_2017} shows that such consistent estimators of $\pi$ are sufficient to obtain the usual $\Opdeltasq$ consistency rate of the estimated factors and loadings, and therefore our test statistics remain valid.
\end{remark}
% W test ---------------------------------------------------------
% Follow the literature convention and just present the test statistics first, then
% introduce additional assumptions to 
% Change and re-word the alternative hypothesis so that it is in terms of the number of augmented factors
\subsection{W test for Orthogonal Shifts}
% I use the subscripts here to emphasize that this hypothesis in infeasible
Next, we consider testing the null hypothesis $\mathcal{H}_0: W_{N \times r} = \mathbf{0}$, against the alternative hypothesis $\mathcal{H}_1: W_{N \times r} \neq \mathbf{0}$. Note that because $W$ contains $N$ rows and $N \to \infty$, traditional tests are infeasible. Similar to \textcite{han_tests_2015-1}, we re-state our null and alternative hypotheses for the type 1 break as:
\begin{align}
\label{eqn:W_null_alter_pool}
\mathcal{H}_0: r_w = 0, \quad \mathcal{H}_1: r_w \neq 0,
\end{align}
where $r_w$ is the number of extra factors augmented by the presence of orthogonal shifts. Although \Cref{eqn:W_null_alter_pool} is essentially a problem for testing the number of factors, existing tests such as \textcite{onatski_testing_2009} cannot be used without imposing further restrictive assumptions on the approximate factor model errors. Our strategy is to present an individual test for each $i$, then pool them across the cross section, thus overcoming the infinite dimensionality problem. 

We define the Wald test statistic for orthogonal shifts in any individual series as:
\begin{align}
\label{eqn:w_ind_test}
\mathscr{W}_{W, i} = T \tilde{w}_i^\tran \tilde{\Omega}_{W, i}^{-1} \tilde{w}_i,
\end{align}
where $\tilde{w}_i$ denotes the transpose of the $i$th row of $\tilde{W}$, and $\tilde{\Omega}_{W, i} = \frac{1}{1 - \pi}\tilde{\Theta}_{1, i} + \frac{1}{\pi} \tilde{\Theta}_{2, i}$ is a HAC estimate of the asymptotic variance. The covariance matrices $\tilde{\Theta}_{1, i}$ and $\tilde{\Theta}_{2, i}$ are constructed using the estimated residuals $\tilde{e}_{(1), it} = x_{it} - \tilde{\lambda}_{2, i}^\tran \tilde{f}_{1, t}$ and $\tilde{e}_{(2), it} = x_{it} - \tilde{\lambda}_{2, i}^\tran \tilde{f}_{2, t}$ in the series $\tilde{Z}^\tran \tilde{f}_{1, t} \cdot \tilde{e}_{(1), it}$ and $\tilde{f}_{2, t} \cdot \tilde{e}_{(2), it}$ respectively, and are detailed in the Supplementary Material. We define the joint Wald test statistic as:
\begin{align}
\label{eqn:W_pool_test}
\mathscr{W}_{W} = (TN) \left( \frac{\sumN \tilde{w}_i}{N}\right)^\tran \tilde{\Omega}_{W}^{-1} \left( \frac{\sumN \tilde{w}_i}{N}\right),
\end{align}
where $\tilde{\Omega}_{W} = N^{-1} \sumN \tilde{\Omega}_{W, i}$ is an estimate of the asymptotic pooled variance, detailed in the Supplementary Material.
%\begin{align}
%\label{eqn:w_ind_HAC}
%\tilde{\Theta}_{1, i} &= 
%D_{0, 1, i} + \sum_{v = 1}^{\floor{\pi T} - 1} k\left( \frac{v}{b_{\floor{\pi T}}} \right) (D_{1, vi} + D_{1, vi}^\tran), \\
%\tilde{\Theta}_{2, i} &= 
%D_{0, 2, i} + \sum_{v = 1}^{T - \floor{\pi T} - 1} k\left( \frac{v}{b_{T - \floor{\pi T}}} \right) (D_{2, vi} + D_{2, vi}^\tran), \\
%D_{1, vi} &= (\floor{\pi T})^{-1} \sum_{t = v + 1}^{\floor{\pi T}} 
%\tilde{Z}^\tran \tilde{f}_{1, t} \tilde{e}_{it} \tilde{e}_{i, t - v} \tilde{f}_{1, t - v}^\tran \tilde{Z}, \\
%D_{2, vi} &= (T - \floor{\pi T})^{-1} \sum_{t = \floor{\pi T} + v + 1}^{T} \tilde{f}_{2, t} \tilde{e}_{it} \tilde{e}_{i, t - v} \tilde{f}_{2, t - v}^\tran,
%\end{align} 
%where $k(.)$ is a real valued kernel, such as the Bartlett kernel, satisfying \Cref{ass:9:hac_conditions}. 
\subsubsection{W Test Asymptotics under the Null Hypothesis}
% Rewrite this section of the paper, and just vomit all the formulas 
% for all the test statistics here
To derive the properties of the test statistics, we make the following additional assumptions. 
% W Test assumptions -------------------------------------------------------
% \begin{assump}
% Assumption 13
% \label{ass:2_loadings}
% For $m = 1, 2$, there exists a positive constant $M$ such that
% $E \norm{\lambda_{m, i}}^4 \leq M$.
% \end{assump}
\begin{assump}
% Assumption 14
\label{ass:14:W_test_error_assump}
There exists a positive constant $M \lt \infty$ such that for all $N, T$, and  $m = 1, 2$:
\begin{assumpenum}
\item \label{ass:14:W_test_error_assump:1}
For each $t$, $E(N^{-1/2} \sumN e_{it})^2 \leq M$.
% \item \label{ass:14:W_test_error_assump:2}
% for each $t$, $E \norm{\frac{1}{\sqrt{NT}} \sumT \sumN f_t w_i^\tran e_{kt} \cdot \iota_{mt}}^2 \leq M$,
% \item \label{ass:14:W_test_error_assump:3}
% additionally, $E \norm{\frac{1}{\sqrt{NT}} \sumT \sumN f_t \lambda_{1i}^\tran e_{kt} \cdot \iota_{mt}} \leq M$, $E \norm{\frac{1}{\sqrt{NT}} \sumT \sumN f_t w_i^\tran e_{kt} \cdot \iota_{mt}} \leq M$
\end{assumpenum}
\end{assump}
\begin{assump}
\label{ass:15}
There exists a positive constant $M < \infty$ such that for all $N, T$:
\begin{assumpenum}
\item \label{ass:15:1}
For each $i$ and $m = 1, 2$,
$E\norm{
\frac{1}{\sqrt{N T_m}} \sumT \sumNk \left( \lambda_{m, k}
[e_{kt}e_{it} - E[e_{kt}e_{it}]] \right) \iota_{mt}
}^2 \leq M$,
\item \label{ass:15:2}
For each $t$ and $m = 1, 2$, $E \norm{\frac{1}{\sqrt{N}} \sumN \lambda_{m, i} e_{it} }^2 \leq M$,
\item \label{ass:15:3}
For each $i$, $E \norm{\frac{1}{\sqrt{T_m}} \sumT f_t e_{it} \iota_{mt}}^4 \leq M$,
\item \label{ass:15:4} 
$E \norm{
\frac{1}{\sqrt{N T_m}} \sumT \sumN f_t e_{it} \iota_{mt}
}^2 \leq M$.
\end{assumpenum}
\end{assump}
% Assumption 16 Pooled assumptions
\begin{assump}
\label{ass:16:sum_loadings}
For $m = 1, 2$:
\begin{assumpenum}
\item \label{ass:16:sum_loadings:1}
$\frac{1}{\sqrt{N}} \sumN \lambda_{m, i} = \Op{1}$,
\item \label{ass:16:sum_loadings:2}
$E \norm{\frac{1}{\sqrt{N}} \sumN \lambda_{m, i} e_{it}^2}^2 \leq M$ for each $t$,
\item \label{ass:16:sum_loadings:3}
$E\norm{\frac{1}{N \sqrt{T_m}} \sumT \sum_{k \neq i} \sumN \lambda_{m, k} e_{kt} e_{it} \cdot \iota_{mt}}^2 \leq M$.
\end{assumpenum}
\end{assump}
% Assumption 17 Central limit theorems
\begin{assump}
\label{ass:17:clt}
% Under the null hypothesis of no orthogonal shifts,
\begin{assumpenum}
\item \label{ass:17:clt:1} 
$\frac{1}{\sqrt{T}} \sumT f_t e_{it} \convd N(0, \Phi_{i})$, $(T)^{-1} \sumT f_t f_t^\tran e_{it}^2 \convp \Phi_{i}$, each $\Phi_{i} > 0$.
\item \label{ass:17:clt:2}
$\frac{1}{\sqrt{TN}} \sumT \sumN f_t e_{it} \convd N(0, \Phi_W)$, $(TN)^{-1} \sumT \sumN f_t f_t^\tran e_{it}^2 \convp \Phi_W$, $\Phi_W > 0$.
\end{assumpenum}
\end{assump}
% some comments/explanations of the assumptions
\Cref{ass:14:W_test_error_assump} is simply the pooled version of \Cref{ass:3_errors}. \Cref{ass:15:1} is simply \Cref{ass:6_moments:1} but corresponding to the loadings, \Cref{ass:15:2} is already implied by  \Cref{ass:6_moments:3}, and \Cref{ass:15:3} is a strengthened version of \Cref{ass:6_moments:1}. These correspond to Assumptions 6 b), 6 d) and 6 e) in \textcite{han_tests_2015-1}, and are not restrictive because they involve zero mean random variables. \Cref{ass:16:sum_loadings} requires that the sum of factor loadings is $\Op{\sqrt{N}}$, and is a slightly modified version of the assumption initially considered by \textcite{han_tests_2015-1}. As explained by \textcite{han_tests_2015-1}, this will hold if the loadings are centered around zero, such that the sum of the loadings diverge at the rate of $\sqrt{N}$ by the central limit theorem. Although this imposes somewhat stricter restrictions compared to a conventional factor model setup, it seems to hold for empirically used datasets, as noted by \textcite{han_tests_2015-1}.
\Cref{ass:17:clt:1,ass:17:clt:2} are simply central limit theorems. The latter assumption somewhat strengthens the restriction on the cross sectional correlation in $e_{it}$, and is simply the cross sectional averaged version of the CLT assumptions introduced by \textcite{bai_inferential_2003}. 
%\Cref{ass:17:clt} is necessary to establish the asymptotic null distributions of $\mathscr{W}_{W, i}$ and $\mathscr{W}_{W}$, as shown in the following theorem.
% Redo this section
% Just show the final Wald distribution
% and make a small note of how the proof is done
\begin{theorem}
\label{thm:W_test_distribution}
% not to Bonsoo's taste to state the null hypothesis again here
If $\frac{\sqrt{T}}{N} \to 0$, then:
\begin{enumerate}
\item \label{thm:W_test_distribution:1}
Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:9:hac_conditions}, and additionally \Cref{ass:14:W_test_error_assump,ass:15,ass:17:clt}, $\mathscr{W}_{W, i} \convd \chi^2_{r}$ for each $i$, and
\item \label{thm:W_test_distribution:2} 
Under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:9:hac_conditions}, and additionally \Cref{ass:14:W_test_error_assump,ass:15,ass:16:sum_loadings,ass:17:clt},
$\mathscr{W}_{W} \convd \chi^2_{r}$.
\end{enumerate}
\end{theorem}
\Cref{thm:W_test_distribution} shows that the Wald\footnote{It is also possible to construct an LM-like test statistic by imposing the null hypothesis of no break, but this results in a statistic with lower power, so we focus on the Wald test again.} test statistics converge to conventional Chi-squared random variables. The detailed proof of \Cref{thm:W_test_distribution} is provided in the Supplementary Material. The basic idea of the proof is to recognize that $\tilde{w}_i$ is a suitable weighted average of $\tilde{Z}^\tran \tilde{\lambda}_{1, i}$, and $\tilde{\lambda}_{2, i}$, both of which have asymptotic normal expansions following Theorem 2 of \textcite{bai_inferential_2003}. The asymptotic normality of $\tilde{w}_i$ and $N^{-1} \sumN \tilde{w}_i$ follows, implying the form of the Wald tests under the null hypothesis.
% Power Analysis
\subsubsection{W test Asymptotics under the Alternative Hypothesis}
To analyze the behavior of the pooled $W$ test under the alternative hypothesis, we introduce some further assumptions.
% Need to make this more precise, Han's proof is very strange
\begin{assump}
% Assumption 17
\label{ass:18:W_test_alter_assump}
There exist constants $0 < \alpha \leq 0.5$ and $C > 0$ such that as $N, T \to \infty$,
$ 
Pr\left( \norm{\frac{T^{\alpha/2}}{\sqrt{N}} \sumN w_i} > C\right) \to 1
$.
\end{assump}
\Cref{ass:18:W_test_alter_assump} requires $\norm{\frac{T^{\alpha/2}}{\sqrt{N}} \sumN w_i}$ to be bounded away from zero asymptotically. Note that if $N^{-1} \sumN w_i \convp 0$ under the alternative, then $N^{-1/2} \sumN w_i$ converges in distribution to some Gaussian random variable by the Law of Large Numbers, and hence $\norm{N^{-1/2 + \epsilon} \sumN w_i}$ is diverging as $N \to \infty$ for any positive $\epsilon$. In order for $\norm{\frac{T^{\alpha/2}}{\sqrt{N}} \sumN w_i}$ to be bounded away from zero, any $\alpha \in (0, 0.5]$ such that $T^{\alpha/2} \geq N$ is required, which is not difficult. \Cref{ass:18:W_test_alter_assump} therefore ensures that the joint test statistic diverges to infinity under the alternative hypothesis, even when if $N^{-1} \sumN w_i \convp 0$. 
\begin{theorem}
\label{thm:11:W_test_alter_cons}
Suppose that $\frac{\sqrt{T}}{N} \to 0$, and the alternative hypothesis $\mathcal{H}_1: r_w \neq 0$ holds. Then:
\begin{thmenum}
\item \label{thm:11:W_test_alter_cons:1} 
under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:14:W_test_error_assump,ass:15,ass:17:clt}, and if $w_i \neq 0$, then $\mathscr{W}_{W, i} \to \infty$ as $N, T \to \infty$,
\item \label{thm:11:W_test_alter_cons:2}
under \Cref{ass:1_factors,ass:2_loadings,ass:3_errors,ass:4_ind_groups,ass:5_error_corr,ass:6_moments,ass:7_eigen_distinct,ass:8_break_fraction,ass:14:W_test_error_assump,ass:15,ass:16:sum_loadings,ass:17:clt,ass:18:W_test_alter_assump}, $\mathscr{W}_{W} \to \infty$ if $\frac{\sqrt{N}}{T^{1 - \alpha/2}} \to 0$ as $N, T \to \infty$.
\end{thmenum}
\end{theorem}
\Cref{thm:11:W_test_alter_cons} shows that both the individual test $\mathscr{W}_{W, i}$ and joint test $\mathscr{W}_{W}$ diverge to infinity asymptotically under the alternative, and are thus consistent tests. 
% ----------------------------------------------------------------------
\section{Monte Carlo Simulations}
\label{sec:monte_carlo}
\subsection{Simulation Specification}
%Our simulation procedure for the loadings and structural break differs somewhat from the existing literature due to the new orthogonal projection model. 
We first simulate two sets of arbitrary loadings, $\Lambda_1, \Lambda_2$ both of which are distributed as a multivariate $N(\mathbf{0}_3, I_3)$, focusing on the case of $r = 3$ factors. Then, we set $W$ to be the residuals of the projection $\Lambda_2 - (\Lambda_1^\tran \Lambda_1)^{-1} \Lambda_1^\tran \Lambda_2$ to ensure that it is orthogonal to $\Lambda_1$. The rotation break $Z$ is set to the identity matrix in the case of no break, or a lower triangular matrix with $[2.5, 1.5, 0.5]$ on the main diagonal and its lower triangular entries drawn from $N(0, 1)$, as in \textcite{duan_quasi-maximum_2022}. The overarching model from we simulate can then be formulated as below.
\begin{align}
\label{eqn:overall_sim}
x_{it} = 
\begin{cases}
\lambda_{1, i}^\tran f_t + \sqrt{\theta} e_{it}, &\quad t = 1, \dots, \floor{\pi T} \\
(Z \lambda_{1, i} + \omega w_{i})^\tran f_t + \sqrt{\theta} e_{it}, &\quad t = \floor{\pi T} + 1, \dots, T,
\end{cases}
\end{align}
for $i = 1, \dots, N$ and $t = 1, \dots, T$. The parameter $\theta$ is set to $3$ in order to calibrate the signal to noise ratio to be 50\%, and the scalar $\omega$ controls the ``size'' of the orthogonal shifts.

The factors and errors are generated as follows:
\begin{align}
f_{k, t} &= \rho f_{k, t - 1} + \mu_{it}, \mu_{it} \sim i.i.d. N(0, 1 - \rho^2), \\
e_{it} &= \alpha e_{i, t - 1} + v_{it},
\end{align}
where $\rho \in \left\lbrace 0, 0.7 \right\rbrace$ captures the serial correlation in the factors, and $\mu_{it}$, $v_{it}$ are mutually independent with $v_{t} = (v_{1, t}, \dots, v_{N, t})^\tran$ being i.i.d. $N(0, \Omega)$ for $t = 1, \dots, T$. For $t = 1$, $e_{.t} = (e_{1, 1}, \dots, e_{N, 1})^\tran$ is $N(0, \frac{1}{1 - \alpha^2} \Omega)$ to initialize the errors at their stationary distributions. The scalar $\alpha$ captures the serial correlation in the errors, and as in \textcite{bates_consistent_2013} and \textcite{baltagi_identification_2017}, $\Omega_{ij} = \beta^{\abs{i - j}}$ captures the cross sectional correlation in the errors. We consider $\alpha \in \left\lbrace 0, 0.3 \right\rbrace $ and $\beta \in \left\lbrace 0, 0.3 \right\rbrace $ to consider up to mild serial and cross sectional correlation. The true break fraction is set to $0.5$ and treated as known. 

Disentanglement necessitates the practitioner running both the $Z$ and $W$ tests, which could lead to a higher family wise error rate, and to this end we report the unadjusted $p$ values, in addition to the adjusted $p$ values using \textcite{holm_simple_1979}. 
% NW suggest pre whitening, but in practice this did not affect the results much
% and at worst actually result in some singularity issues
% because the OLS step to calculate the pre-whiten sometimes failed
\subsection{Simulation Results}
% Size tables
% Currently this is comprehensive, but will trim this down to a smaller version
% and shove the rest into the appendix
% Z test has an oversizing issue without Bonferroni Holm correction
% This is alleviated with larger T (again, nothing we can do)
\begin{table}
\centering
\caption{\label{tab:size_small}Size of Rotation and Orthogonal Shift Tests, $N = 200$}
\begin{scriptsize}
\begin{tabular}[t]{ccccccccc}
\toprule
\multicolumn{4}{c}{ } & \multicolumn{2}{c}{Z Test} & \multicolumn{2}{c}{W Test} & \multicolumn{1}{c}{W Individual} \\
\cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8} \cmidrule(l{3pt}r{3pt}){9-9}
$T$ & $\rho$ & $\alpha$ & $\beta$ & Unadj. & Adj. & Unadj. & Adj. &  \\
\midrule
 &  & 0.0 & 0.0 & 0.108 & 0.072 & 0.005 & 0.003 & 0.013\\
\nopagebreak
\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 0.115 & 0.076 & 0.088 & 0.053 & 0.014\\
\nopagebreak
 &  & 0.0 & 0.0 & 0.068 & 0.031 & 0.003 & 0.001 & 0.004\\
\nopagebreak
\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.0} & 0.3 & 0.3 & 0.070 & 0.037 & 0.061 & 0.027 & 0.004\\
\cmidrule{1-9}\pagebreak[0]
 &  & 0.0 & 0.0 & 0.243 & 0.160 & 0.003 & 0.002 & 0.017\\
\nopagebreak
\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 0.239 & 0.174 & 0.096 & 0.066 & 0.027\\
\nopagebreak
 &  & 0.0 & 0.0 & 0.151 & 0.093 & 0.000 & 0.000 & 0.005\\
\nopagebreak
\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.7} & 0.3 & 0.3 & 0.144 & 0.095 & 0.074 & 0.047 & 0.009\\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{table}
We present the size analysis in \Cref{tab:size_small}. In the case of no serial correlation in the factors, and large $T$ relative to $N$, the $Z$ test has a nominal size close to the desired 5\%, and this tends to hold regardless of the serial or cross sectional correlation in the errors. The $Z$ test seems to be oversized when there is serial correlation in the factors, but this issue is alleviated and approaches a rejection rate of 0.15 as $T$ increases.\footnote{Increasing $T$ further does seem to make the size approach 5\% (see \Cref{ext:tab:size} in Supplementary Material).} The $W$ test does not seem to be affected by serial correlation in the factors, and also seems to be overly conservative when there is no serial correlation in the error, but otherwise seems to have good size. Implementation of the Bonferroni-Holm procedure to adjust the $p$ values also seems to correct the oversizing issue, so we advocate for its use. 
%
%% Z break
%\begin{footnotesize}
%\begin{longtable}[t]{cccccccccc}
%\caption{\label{tab:z_power_small}Power of $Z$ Rotation Test, $N = 200$}\\
%\toprule
%\multicolumn{4}{c}{ } & \multicolumn{2}{c}{Z Test} & \multicolumn{2}{c}{W Test} & \multicolumn{2}{c}{ } \\
%\cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8}
%$T$ & $\rho$ & $\alpha$ & $\beta$ & Unadj. & Adj. & Unadj. & Adj. & HI Test & BKW Test\\
%\midrule
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.004 & 0.004 & 1.000 & 1.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 1.000 & 1.000 & 0.100 & 0.100 & 1.000 & 1.000\\
%\nopagebreak
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.002 & 0.002 & 1.000 & 1.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.0} & 0.3 & 0.3 & 1.000 & 1.000 & 0.094 & 0.094 & 1.000 & 1.000\\
%\cmidrule{1-10}\pagebreak[0]
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.003 & 0.003 & 1.000 & 1.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 1.000 & 1.000 & 0.106 & 0.106 & 1.000 & 1.000\\
%\nopagebreak
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.001 & 0.001 & 1.000 & 1.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.7} & 0.3 & 0.3 & 1.000 & 1.000 & 0.096 & 0.096 & 1.000 & 1.000\\
%\bottomrule
%\end{longtable}
%\end{footnotesize}
%\Cref{tab:z_power_small} presents the simulation results when there is a rotational break, but no orthogonal shifts, and thus explores the finite sample power of the $Z$ test. We present the results of the proposed rotational and orthogonal shift tests, as well as the tests of \textcite{han_tests_2015} (HI) and \textcite{baltagi_estimating_2021} (BKW) based on a known break date. In general, it can be seen that the $Z$ test exhibits very good power, similar to the HI and BKW tests.
%
%% W break
%\begin{footnotesize}
%\begin{longtable}[t]{cccccccccccc}
%\caption{\label{tab:w_power_small}Power of $W$ Orthogonal Shift Test, $N = 200$}\\
%\toprule
%\multicolumn{4}{c}{ } & \multicolumn{2}{c}{Z Test} & \multicolumn{3}{c}{W Test} & \multicolumn{2}{c}{ } \\
%\cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-9}
%$T$ & $\rho$ & $\alpha$ & $\beta$ & Unadj. & Adj. & Unadj. & Adj. & Individual & HI & BKW & $\tilde{r}$\\
%\midrule
% &  & 0.0 & 0.0 & 0.134 & 0.124 & 0.870 & 0.835 & 0.866 & 1.000 & 1.000 & 5.980\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 0.136 & 0.129 & 0.860 & 0.821 & 0.849 & 1.000 & 1.000 & 5.928\\
%\nopagebreak
% &  & 0.0 & 0.0 & 0.078 & 0.074 & 0.955 & 0.951 & 0.953 & 1.000 & 1.000 & 6.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.0} & 0.3 & 0.3 & 0.079 & 0.076 & 0.950 & 0.939 & 0.947 & 1.000 & 1.000 & 6.000\\
%\cmidrule{1-12}\pagebreak[0]
% &  & 0.0 & 0.0 & 0.249 & 0.242 & 0.940 & 0.927 & 0.941 & 1.000 & 1.000 & 6.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 0.244 & 0.233 & 0.916 & 0.896 & 0.908 & 1.000 & 1.000 & 6.000\\
%\nopagebreak
% &  & 0.0 & 0.0 & 0.147 & 0.146 & 0.981 & 0.977 & 0.981 & 1.000 & 1.000 & 6.000\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.7} & 0.3 & 0.3 & 0.146 & 0.144 & 0.968 & 0.965 & 0.968 & 1.000 & 1.000 & 6.000\\
%\bottomrule
%\end{longtable}
%\end{footnotesize}
%\Cref{tab:w_power_small} presents the simulation results when there is no rotational break, and only orthogonal shifts. Similar to \Cref{tab:z_power}, we present the results of the proposed rotational, orthogonal shift, HI and BKW tests. The last column of \Cref{tab:w_power_small} shows $\tilde{r}$, the number of factors as estimated by the information criteria of \textcite{bai_determining_2002}. We emphasize that the HI and BKW tests have power against this case, and therefore combined with the results in \Cref{tab:z_power}, they are unable to discern which \emph{type} of break as occurred, as opposed to our tests. For smaller values of $T$, the $W$ test seems to be somewhat under-rejecting - this is because the remainder term in the asymptotic expansion is $\Op{\sqrt{T}/\delta_{NT}^{2}}$, which may be non-negligible in small finite samples, but the power improves as $T$ increases.
%
%%Note that all tests have low power for low values of $w$ and $\tilde{r}$ - this corresponds to the theoretical results of \textcite{stock_forecasting_2002} and \textcite{bates_consistent_2013}, who show that the PC estimator is robust to ``small'' shifts in the loadings.
%
%% Redo all of these tables without w = 0.5, to make them smaller
%% Z and W break
%\begin{footnotesize}
%\begin{longtable}[t]{cccccccccccc}
%\caption{\label{tab:z_w_power_small}Power of $Z$ and $W$ Tests, $N = 200$}\\
%\toprule
%\multicolumn{4}{c}{ } & \multicolumn{2}{c}{Z Test} & \multicolumn{3}{c}{W Test} & \multicolumn{2}{c}{ } \\
%\cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-9}
%$T$ & $\rho$ & $\alpha$ & $\beta$ & Unadj. & Adj. & Unadj. & Adj. & Individual & HI & BKW & $\tilde{r}$\\
%\midrule
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.801 & 0.800 & 0.785 & 1.000 & 1.000 & 4.310\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 1.000 & 1.000 & 0.804 & 0.803 & 0.765 & 1.000 & 1.000 & 4.206\\
%\nopagebreak
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.924 & 0.924 & 0.911 & 1.000 & 1.000 & 4.858\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.0} & 0.3 & 0.3 & 1.000 & 1.000 & 0.919 & 0.919 & 0.901 & 1.000 & 1.000 & 4.772\\
%\cmidrule{1-12}\pagebreak[0]
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.899 & 0.899 & 0.894 & 1.000 & 1.000 & 5.046\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 200} &  & 0.3 & 0.3 & 1.000 & 1.000 & 0.867 & 0.867 & 0.846 & 1.000 & 1.000 & 5.047\\
%\nopagebreak
% &  & 0.0 & 0.0 & 1.000 & 1.000 & 0.964 & 0.964 & 0.962 & 1.000 & 1.000 & 5.568\\
%\nopagebreak
%\multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-4}{*}{\centering\arraybackslash 0.7} & 0.3 & 0.3 & 1.000 & 1.000 & 0.946 & 0.946 & 0.938 & 1.000 & 1.000 & 5.511\\
%\bottomrule
%\end{longtable}
%\end{footnotesize}
\begin{table}
\centering
\begin{scriptsize}
\caption{\label{tab:z_w_power}Power of $Z$ and $W$ Tests, $r = 3$, $\alpha = \beta = 0.3$}
\begin{tabular}[t]{ccccccccccccc}
\toprule
\multicolumn{5}{c}{ } & \multicolumn{2}{c}{Z Test} & \multicolumn{3}{c}{W Test} & \multicolumn{2}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-10}
Type & $T$ & $N$ & $\omega$ & $\rho$ & Unadj. & Adj. & Unadj. & Adj. & Individual & HI & BKW & $\tilde{r}$\\
\midrule
 &  &  &  & 0.0 & 0.136 & 0.129 & 0.860 & 0.821 & 0.849 & 1.000 & 1.000 & 5.928\\
\nopagebreak
 & \multirow{-2}{*}{\centering\arraybackslash 200} &  &  & 0.7 & 0.244 & 0.233 & 0.916 & 0.896 & 0.908 & 1.000 & 1.000 & 6.000\\
\nopagebreak
 &  &  &  & 0.0 & 0.079 & 0.076 & 0.950 & 0.939 & 0.947 & 1.000 & 1.000 & 6.000\\
\nopagebreak
\multirow{-4}{*}{\centering\arraybackslash Type 1} & \multirow{-2}{*}{\centering\arraybackslash 500} &  & \multirow{-4}{*}{\centering\arraybackslash 1} & 0.7 & 0.146 & 0.144 & 0.968 & 0.965 & 0.968 & 1.000 & 1.000 & 6.000\\
\nopagebreak
 &  &  &  & 0.0 & 1.000 & 1.000 & 0.100 & 0.100 & 0.026 & 1.000 & 1.000 & 3.000\\
\nopagebreak
 & \multirow{-2}{*}{\centering\arraybackslash 200} &  &  & 0.7 & 1.000 & 1.000 & 0.106 & 0.106 & 0.035 & 1.000 & 1.000 & 3.000\\
\nopagebreak
 &  &  &  & 0.0 & 1.000 & 1.000 & 0.094 & 0.094 & 0.009 & 1.000 & 1.000 & 3.000\\
\nopagebreak
\multirow{-4}{*}{\centering\arraybackslash Type 2} & \multirow{-2}{*}{\centering\arraybackslash 500} &  & \multirow{-4}{*}{\centering\arraybackslash 0} & 0.7 & 1.000 & 1.000 & 0.096 & 0.096 & 0.012 & 1.000 & 1.000 & 3.000\\
\nopagebreak
 &  &  &  & 0.0 & 1.000 & 1.000 & 0.804 & 0.803 & 0.765 & 1.000 & 1.000 & 4.206\\
\nopagebreak
 & \multirow{-2}{*}{\centering\arraybackslash 200} &  &  & 0.7 & 1.000 & 1.000 & 0.867 & 0.867 & 0.846 & 1.000 & 1.000 & 5.047\\
\nopagebreak
 &  &  &  & 0.0 & 1.000 & 1.000 & 0.919 & 0.919 & 0.901 & 1.000 & 1.000 & 4.772\\
\nopagebreak
\multirow{-4}{*}{\centering\arraybackslash Type 3} & \multirow{-2}{*}{\centering\arraybackslash 500} & \multirow{-12}{*}{\centering\arraybackslash 200} & \multirow{-4}{*}{\centering\arraybackslash 1} & 0.7 & 1.000 & 1.000 & 0.946 & 0.946 & 0.938 & 1.000 & 1.000 & 5.511\\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{table}
\Cref{tab:z_w_power} presents the power of the $Z$ and $W$ tests across all types of breaks. It can be seen that both the $Z$ and $W$ test have good power and are rejecting correctly only on their respective break types. This is in contrast to HI and BKW tests, which consistently reject across all break types, and are thus unable to discern which type of break has occurred. 
% ----------------------------------------------------------------------
\section{Empirical Application}
\subsection{Data and Methodology}
For our empirical application, we apply our tests to the FRED-MD dataset (see \textcite{mccracken_fred-md_2015} for data cleaning and preparation). We focus on two candidate break dates: 1984 February, corresponding to the Great Moderation (\textcite{baltagi_estimating_2021}, \textcite{ma_estimation_2018} and \textcite{breitung_testing_2011}); and 2008 November, corresponding to the Global Financial Crisis (\textcite{baltagi_estimating_2021}, \textcite{ma_estimation_2018} and \textcite{duan_quasi-maximum_2022}). 

% re word this a bit
Our tests aim to \emph{differentiate} the type of break once they have been estimated, and were formulated under the assumption that there is only one break. As argued by \textcite{bai_estimation_1997}, \textcite{bai_estimating_1998} and others, tests formulated for the case of one break can be expected to have power against multiple breaks. To this end, we consider a sample of 1975 January to 2000 January for the Great Moderation (GM) break, and a sample of 2003 January to 2013 January for the Global Financial Crisis (GFC) break, in order to ensure that there is only one break in each sample. These samples were chosen because there is mixed evidence that there could be a break in the mid to late 1990s (\textcite{hansen_new_2001} and \textcite{ma_group_2022}), or in 2000 associated with the early 2000s recession (\textcite{ma_estimation_2018} and \textcite{ma_group_2022}). This is not restrictive, because the case of multiple breaks can be dealt with partitioning the data, and running the tests on each break separately. 

The number of factors in each subsample is estimated by the eigenvalue edge distribution estimator of \textcite{onatski_determining_2010}, and the eigenvalue ratio estimator of \textcite{ahn_eigenvalue_2013}.\footnote{We also consider using the information criteria $IC_{p1}$ and $IC_{p2}$ of \textcite{bai_determining_2002}, but do not report them here as they are well known to overestimate the number of factors. \textcite{ahn_eigenvalue_2013}'s eigenvalue growth ratio estimator also tends to produce similar results to their eigenvalue ratio and is hence omitted. For more comprehensive results, see \Cref{ext:tab:rtilde_full} in \Cref{ext:app:additional_empirical_results}.} As seen in \Cref{tab:rtilde}, these estimators do not typically agree with one another on empirical data, and we therefore report the results of the tests for $\tilde{r} = 1, \dots 4$, which is the maximum subsample $r$ estimated.

% subsample rtilde tables
% maybe update these tables with the test statistics of HI and BKW as well, and show that higher r actually leads to mixed evidence of there being a break at all
\begin{table}
\centering
\caption{\label{tab:rtilde}Subsample $\tilde{r}$ Estimates}
\begin{scriptsize}
\begin{tabular}[t]{ccc}
\toprule
Sample & Onatski (2010) & Ahn and Horenstein (2013) Eigenvalue Ratio\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Great Moderation (1984 February) Sample}}\\
\hspace{1em}Whole & 6 & 1\\
\hspace{1em}Pre-break & 4 & 1\\
\hspace{1em}Post-break & 3 & 3\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Global Financial Crisis (2008 November) Sample}}\\
\hspace{1em}Whole & 4 & 1\\
\hspace{1em}Pre-break & 2 & 1\\
\hspace{1em}Post-break & 2 & 1\\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{table}

\subsection{Joint Test Results}
\Cref{tab:joint} reports the results of the $Z$ and pooled $W$ tests when the Great Moderation and the Global Financial Crisis are candidate break dates. For the Great Moderation, a higher $r$ leads to significant rejection on both tests. However, for the case of only one factor, we fail to reject the null of orthogonal shifts. This is in stark contrast to the tests of \textcite{han_tests_2015} and \textcite{baltagi_estimating_2021}, which strongly reject for all values of $r$. In contrast, for the Global Financial Crisis, we see mixed evidence: in the case of one factor, we only reject the $W$ test; in the case of two factors, we only reject the $Z$ test; and we reject both tests when the number of factors is three or more. At present, it is unclear why this mixed evidence occurs, but given that the tests of \textcite{han_tests_2015} and \textcite{baltagi_estimating_2021} fail to reject for the case of one to two factors, we therefore conclude that the Global Financial Crisis is best characterized as a type 3 break for the case of three factors, and would lead to a factor augmentation effect if ignored. This is most clearly seen with \textcite{onatski_determining_2010}'s estimator, which estimates exactly double the number of factors over the whole sample, compared to each subsample.
\begin{table}
\centering
\begin{scriptsize}
\caption{\label{tab:joint}Joint Test Results}
\begin{tabular}[t]{ccccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{$Z$ Test $p$ values} & \multicolumn{2}{c}{$W$ Test $p$ values} & \multicolumn{2}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
$\tilde{r}$ & Unadj. & Adj. & Unadj. & Adj. & Han and Inoue (2015) & Baltagi et al. (2021)\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Great Moderation (1984 February) Sample}}\\
\hspace{1em}1 & 0.000 & 0.001 & 0.596 & 0.596 & 0.001 & 0.000\\
\hspace{1em}2 & 0.000 & 0.000 & 0.000 & 0.000 & 0.001 & 0.000\\
\hspace{1em}3 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
\hspace{1em}4 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.001\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Global Financial Crisis (2008 November) Sample}}\\
\hspace{1em}1 & 0.175 & 0.175 & 0.005 & 0.011 & 0.688 & 0.116\\
\hspace{1em}2 & 0.009 & 0.019 & 0.486 & 0.486 & 0.354 & 0.116\\
\hspace{1em}3 & 0.010 & 0.010 & 0.002 & 0.005 & 0.009 & 0.004\\
\hspace{1em}4 & 0.021 & 0.021 & 0.000 & 0.000 & 0.000 & 0.007\\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{table}
%
\subsection{Individual Test Results}
In order to aid economic interpretation in the precise nature of the breaks, we also report the results for the individual $w_i$ test results. For comparative purposes, we also report the individual loading break tests of \textcite{breitung_testing_2011} (BE) using the same candidate break dates. The rejection frequencies are visualized in \Cref{fig:gfc_w_ind}. 
For the Great Moderation, the $w_i$ test rejects far less often than BE's tests for the case of 1-2 factors. An interpretation of this result is that our $w_i$ test controls for possible changes in the factor variance, as opposed to BE, whose test statistics are constructed used the \emph{pseudo} factors and therefore do not control for the possibility of the factor variance breaking.

Further inspection of which \emph{specific} series are breaking in \Cref{fig:gfc_w_ind} reveals that for the case of two factors, the statistically significant joint break is actually mostly isolated to variables in the Prices group. For the case of three factors, the $W$ test rejects a much smaller fraction of Interest and Exchange Rate variables compared to BE. The precise implications of this are beyond the scope of this paper, but this suggests that if the practitioner is not concerned with price series, the augmentation effect could be safely ignored. 
% Not sure if the rejection rates are even necessary...
% The figure should be good enough

%\begin{small}
%\begin{longtable}[t]{ccc}
%\caption{\label{tab:w_ind}Individual Factor Loading Break Rejection Rates}\\
%\toprule
%$\tilde{r}$ & Individual $w$ & Breitung and Eickmeier (2011)\footnote{Only Wald test of Breitung and Eickmeier (2011) presented, as LM test provides similar results.}\\
%\midrule
%\addlinespace[0.3em]
%\multicolumn{3}{l}{\textbf{Great Moderation (1984 February) Sample}}\\
%\hspace{1em}1 & 0.024 & 0.205\\
%\hspace{1em}2 & 0.189 & 0.307\\
%\hspace{1em}3 & 0.323 & 0.386\\
%\hspace{1em}4 & 0.480 & 0.378\\
%\addlinespace[0.3em]
%\multicolumn{3}{l}{\textbf{Global Financial Crisis (2008 November) Sample}}\\
%\hspace{1em}1 & 0.441 & 0.409\\
%\hspace{1em}2 & 0.512 & 0.291\\
%\hspace{1em}3 & 0.535 & 0.291\\
%\hspace{1em}4 & 0.575 & 0.433\\
%\bottomrule
%\end{longtable}
%\end{small}
\marginnote[Updated these plots, streamlined to just 1-4 factors as well.]{right}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{W_ind_bar_plot_paper.pdf}
\caption{Individual Loading Rejection Proportions for Great Moderation Sample (1975 to 2000, break date of 1984 February) and Global Financial Crisis Samples (2003 to 2013, break date of 2008 November)}
\label{fig:gfc_w_ind}
\end{figure}
This is in contrast to the results when the GFC is used as a candidate break. Instead, we are in general able to reject a higher proportion of series than BE. Although BE did not consider the GFC as a common break date, it is interesting to see that the use of pseudo factors seems to be confounding and reducing the power in detecting legitimate breaks in the loadings. A specific look into \emph{which} specific variables are breaking reveals some differences across groups, for the case of one factor. Our orthogonal shift test fails to reject any housing or stock market variables, compared to BE, which report a rejection of a significant fraction in both of these groups. 
\subsection{Variance Decomposition}
% Shove the technical details into the supplementary material
Our projection based equivalent representation theorem provides a natural framework to decompose structural breaks and quantify the proportion of variance change due to a change in factor variance and the change in factor loadings. We relegate the technical details of this to the Supplementary Material, and report the various restricted and unrestricted $R^2$ values in \Cref{tab:rsquared}.\footnote{For results with higher $r$, see \Cref{ext:tab:rsquared_full} in \Cref{ext:app:additional_empirical_results}.} The results for Great Moderation match up with the results of the formal joint tests - for the case of $r = 1$ factor, restricting $Z = I$ results in a decrease of in sample $R^2$ from 17.8\% to 12.6\%, compared to the the restriction of $W = 0$, which only decreases in sample $R^2$ to 16.9\%. 
The results for the Global Financial Crisis at first glance seem contradictory to the results of the joint test - there appears to be negligible decreases in the in sample $R^2$ from imposing $Z = I$. However, this is because the nature of rotational change during the Global Financial Crisis is that the ordering of the factors has changed.\footnote{See \Cref{ext:fig:gfc_r2_plot1,ext:fig:gfc_r2_plot2} in Supplementary Material.} Due to the limitation of our variance decomposition methodology in controlling for this, we interpret this this as meaning that the statistical evidence from the joint test was simply picking up on this ``re-ordering'' of the factors. The restriction of $W = 0$ results in large decreases $R^2$ and in consistent with the results of the joint tests. 
% Rsquared tables
\begin{table}
\centering
\begin{scriptsize}
\caption{\label{tab:rsquared}$R^2$ Comparisons for $Z = I, W = 0$, $Z = I$ and $W = 0$ restrictions. $R^2$ values for Whole Sample PCA represent the fit from the \emph{pseudo} factors and loadings and are thus not directly comparable.}
\begin{tabular}[t]{cccccc}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Restricted $R^2$} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){3-5}
$\tilde{r}$ & Unrestricted $R^2$ & $Z = I$ & $W = 0$ & $Z= I, W = 0$ & Whole Sample PCA\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Great Moderation (1984 February) Sample}}\\
\hspace{1em}1 & 0.178 & 0.126 & 0.169 & 0.117 & 0.172\\
\hspace{1em}2 & 0.274 & 0.221 & 0.225 & 0.173 & 0.241\\
\hspace{1em}3 & 0.344 & 0.289 & 0.277 & 0.222 & 0.302\\
\hspace{1em}4 & 0.398 & 0.342 & 0.313 & 0.257 & 0.359\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Global Financial Crisis (2008 November) Sample}}\\
\hspace{1em}1 & 0.228 & 0.228 & 0.141 & 0.140 & 0.182\\
\hspace{1em}2 & 0.342 & 0.341 & 0.223 & 0.222 & 0.291\\
\hspace{1em}3 & 0.424 & 0.422 & 0.309 & 0.307 & 0.370\\
\hspace{1em}4 & 0.489 & 0.487 & 0.372 & 0.371 & 0.434\\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{table}
% Our results highlight the importance of controlling for changes in factor variance, as these could lead to misleading over or under rejection of breaks in factor loadings, for the Great Moderation and Global Financial Crisis respectively. Furthermore, we provide evidence that legitimate breaks in factor loadings occur differently across variable groups on the FRED-MD dataset, and that these can be substantially different to the results of BE. 
% -----------------------------------------------------------------------
\marginnote[Re-written and streamlined conclusion]{}
\section{Conclusion}
% Bonsoo wants me to re-write this, curently it is very similar to the abstract
% Slight summary of everything thus far in the paper
% This is too long at the moment
We propose a \emph{projection based equivalent representation theorem} to decompose any structural break in dynamic factors into a rotational change and orthogonal shift. By interpreting these two changes as a break in factor variance and a break in factor loadings respectively, we are able to subsequently propose two separate tests: 1) a test for evidence of rotational change, and 2) a test for evidence of orthogonal shifts. Monte Carlo studies demonstrate their good finite sample performance, as well as the inability of existing methods to differentiate between these different break types. We apply the tests to the FRED-MD dataset using the Great Moderation and Global Financial Crisis as candidate break dates, and find evidence that the Great Moderation may be better characterised as a break in the factor variance, as opposed to a break in the loadings, whereas the Global Financial Crisis is a break in both. Our results highlight the limitations of existing methods in differentiating between these break types and nuance the discussion surrounding structural breaks in dynamic factor models. 

Our framework provides a potential foundation to explore the precise practical and theoretical implications of structural breaks in dynamic factor models. For example, a natural question to consider is how the different break types can affect the estimation and subsequent use of factors, such as the factor augmented forecasts of \textcite{stock_forecasting_2002} and \textcite{bai_confidence_2006}, and factor augmented vector auto-regressions of \textcite{bernanke_measuring_2005}. Indeed, although there have been many suggestions for how to use factors in forecasting when a structural break is present (see \textcite{stock_forecasting_2009} and \textcite{baltagi_estimating_2021}), there is still no formal treatment of this in the literature. 
%Alternatively, our framework is a natural fit for analysing common and idiosyncratic sources of variation in large datasets. For example, \textcite{luciani_common_2020} analyses common and idiosyncratic sources of inflation using a quasi maximum likelihood approach. Instead of analysing the sources of variation arising from the common and idiosyncratic noise component, it could be interesting to revisit this problem through the lens of a structural break framework, where our projection based decomposition would offer a more natural characterization of how a ``pure inflation'' factor could change.
% List all supplementary materials
\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}
\item[R Code] R Code including FREDMD vintage available on request (.zip file)
\end{description}
% References -------------------------------------------------------------
\printbibliography

\end{document}