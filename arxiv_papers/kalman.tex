Gaussian approximations rely on the fact that when the SSM at hand is linear Gaussian (LGSSM), then the filtering and marginal smoothing distributions are Gaussian as well, and their means and covariances can be computed sequentially and in closed form~\citep[see, e.g.,][]{sarkka2013bayesian,barfoot2017state}. This is leveraged in Gaussian approximations to the filtering and marginal smoothing solutions of general SSMs. Typically, such approximations rely on Taylor linearisation, leading to the classical extended Kalman filtering \citep[see, e.g.,][]{Jazwinski:1970}, or on sigma-point linearisations, first introduced in \citet{julier2004unscented,wan2000unscented}. In this work we will focus on a more general posterior linearisation framework encompassing both methods and recently introduced in \citet[][]{Garcia:2017,Tronarp2018iterative}. See Section~\ref{subsec:auxiliary-ssm-sampler} for a short review of these methods in the context of this work.

The state of the art for these methods consists in iteratively reusing the approximated marginal smoothing distributions to refine the Gaussian approximation of the SSM at hand~\citep{bell1994iterated,Garcia:2017,Tronarp2018iterative}. Doing so makes it possible to handle SSMs for which the reverse Markov chain representing the smoothing distribution is a slow-mixing process, that is, SSMs which have ``sticky'' transitions kernels and for which the filtering transition largely differs from the smoothing one. These recursive methods have been shown to be equivalent to certain minimisation programs (such as Gauss--Newton) for some given loss functions and to be (locally) convergent. For a review of these methods we refer the reader to the doctoral thesis of \citet{tronarp2020iterative}.

Furthermore, it has been recently shown \citep{Sarkka2021temporal,yaghoobi2021parallel,Yaghoobi2022sqrt} that these iterative methods can be parallelised in time (PIT), resulting in a computational complexity of $\bigO(\log(T))$ on parallel hardware such as graphics processing units (GPUs), comparing to their classical $\bigO(T)$ complexity on sequential hardware.

An important drawback of all the Gaussian approximation based methods is that they (in all but the LGSSM case) result in biased estimates of the true filtering distribution as well as marginal and pathwise smoothing distributions. This bias is also present in the normalisation constant estimate (marginal likelihood of the observations) of the model, which makes the parameter estimation biased as well. 
