Finally, we consider the spatio-temporal model of \citet[Section 4.2]{cruscino2022highdim} which was recently introduced as a benchmark for high-dimensional state inference in non-linear systems. It consists in independent latent dynamics for a state $X_t(i, j)$ located on a two-dimensional lattice $\{1, \ldots, d\} \times \{1, \ldots, d\} \ni (i,j)$, for $d=8$, with an observation model that does not factorise over the nodes of the lattice, thereby creating non-trivial posterior structure between the states. We are given a $8^2 = 64$ dimensional model
\begin{equation}
    \label{eq:spatio-temporal}
    \begin{split}
        X_t(i, j) &= X_{t-1}(i, j) + U_t(i, j), \quad i, j = 1, \ldots, d, \\
        Y_t(i, j) &= X_{t}(i, j) + V_t(i, j), \quad i, j = 1, \ldots, d,
    \end{split}
\end{equation}
where, for all $t, i, j$, the $U_t(i, j)$ are i.i.d. according to $\mathcal{N}(0, \sigma_X^2)$, and for all $t$, the $V_t$'s are i.i.d. according to a multivariate t-distribution with $\nu$ degrees of freedom centred on $0$. The precision matrix of the $V_t$'s is given by $\Sigma^{-1} = \tau^{D[(i, j), (i', j')]}$ if $D[(i, j), (i', j')] \leq r_y$ and $0$ otherwise, where $D[(i, j), (i', j')]$ is a graph distance, and $\tau < 0$ a given parameter.

In \citet{cruscino2022highdim}, the parameters are chosen to be $\sigma_X = 1$, $\nu = 10$, $\tau = -1/4$, $r_y = 1$, and $D[(i, j), i', j')] = \abs{i - i'} + \abs{j - j'}$, so that an observation is mostly corrupted by its direct neighbours. We keep all the parameters unchanged, with the exception that, in order to make the problem more difficult, we take $\nu=1$ so that the observation model does not have first or second moments, and to showcase the parallelisation in time, we also consider a substantially higher number of time steps $T=1\,024$~\citep[vs. $T=10$ in][]{cruscino2022highdim}. Overall, the total dimension of the target model is therefore of the order of $65\,000$.
The first order auxiliary Kalman sampler is particularly suited to this type of model, even if the underlying state dimension is large. This is due to the fact that the prior factorises across all dimensions, so that the auxiliary LGSSM proposal \eqref{eq:auxiliary-LGSSM} factorises too, even if the target $\pi(x_{0:T})$ does not. As a consequence, we are left with sampling from $d \times d$ independent one-dimensional LGSSMs rather than a $d\times d$ dimensional one. This means that, instead of needing to compute conditional Gaussian distributions of dimension $d \times d$, and therefore needing to solve systems of size $d \times d$, we only need to solve one-dimensional systems, that is, divide by scalars. This property extends to some extent to auxiliary cSMC samplers where the proposal is chosen to factorise across dimensions too. This means that the cost will be dominated by the computation of the log-likelihood of the multivariate t-distribution at each time step and (for specialised implementations) the complexity of the auxiliary cSMC will then be a direct multiple of the complexity of the auxiliary Kalman sampler. This property is not verified in the case of ``full'' prior dynamics as we will see in Section~\ref{subsec:stoch-vol}.

The experiment design is as follows: we simulate $20$ datasets from \eqref{eq:spatio-temporal}. For each of these, we set the initial trajectory of the MCMC chain to be the result of a single trajectory formed from the backward sampling~\citep{Godsill2004FFBS} of a bootstrap filter algorithm with $1\,000$ particles (this gives bad smoothing statistics but is a good starting point for a MCMC chain) and run $A=5\,000$ adaptation steps, after which the statistics of the chain are collected over $L = 20\,000$ iterations.
For this experiment, all the sequential versions of the auxiliary Kalman and cSMC samplers were dramatically slower than the parallel-in-time alternatives: they took in the order of a second per iteration, both on CPU and GPU, compared to the PIT versions that took in the order of a millisecond per iteration on GPU. As a consequence, we do not report their results here. Instead, we focus on (i) the parallel-in-time version of \citet{finke2021csmc} given by using \citet{corenflos2022sequentialized} on step~\ref{step:pmcmc-2} of Algorithm~\ref{alg:aux-pgibbs}, (ii) the parallel-in-time version of the gradient auxiliary proposal \eqref{eq:differentiable-model} of Section~\ref{subsec:guided-smc-sampler}, which we refer to as gradient-informed, and finally (iii) the auxiliary Kalman sampler \eqref{eq:gaussian-FK-proposal} of Section~\ref{subsec:auxiliary-lgssm}, with first order linearisation only, noting that the second order would remove the benefits of having a separable prior.

For the gradient-informed and standard parallel-in-time cSMC samplers we pick $N=25$ particles and we target an acceptance rate per time step of $25\%$. This is more conservative than in \citet{finke2021csmc} as we found that the samplers struggled to calibrate themselves when too high values were picked. As per \citet{titsias2018}, we target a $50\%$ acceptance rate for the auxiliary Kalman sampler.
The final average acceptance rates were a little lower, with the auxiliary Kalman sampler accepting $34\%$ of the trajectories, and the two versions of the auxiliary cSMC around $23\%$. This is most likely due to our calibration algorithm being too optimistic, but did not seem to impact the final results beyond reason and therefore did not, in our opinion, warrant further investigation.

The RESJD is shown, averaged over all experiments, in Figure~\ref{fig:esjd-spatio-temporal}, while the time scaled RESJD, namely RESJD divided by the number of seconds taken to run one step of the sampler is shown, averaged over all experiments, in Figure~\ref{fig:esjd-spatio-temporal-time}.
\begin{figure}[htb!]
    \centering
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-spatial}}
        \caption{Root expected squared jump distance for the auxiliary Kalman sampler~\ref{line:ESJD-Kalman}, the auxiliary cSMC sampler~\ref{line:ESJD-cSMC}, and the auxiliary cSMC sampler with gradient-informed proposals~\ref{line:ESJD-cSMC-grad}. Kalman~\ref{line:ESJD-Kalman} shows as a roughly horizontal line at the bottom.}
        \label{fig:esjd-spatio-temporal}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-spatial-time}}
        \caption{Root expected squared jump distance per second for the auxiliary Kalman sampler~\ref{line:ESJD-Kalman}, the auxiliary cSMC sampler~\ref{line:ESJD-cSMC}, and the auxiliary cSMC sampler with gradient-informed proposals~\ref{line:ESJD-cSMC-grad}.}
        \label{fig:esjd-spatio-temporal-time}
    \end{subfigure}
    \caption{Average (across 20 different experiments) root expected squared jump distance per iteration and second for all the samplers considered on the spatio-temporal model~\eqref{eq:spatio-temporal}.}
\end{figure}

As can be seen in the figure, the gradient-enhanced PIT auxiliary cSMC has a better RESJD than the basic PIT auxiliary cSMC which in turn has a better RESJD than the auxiliary Kalman sampler. The ordering of these methods however changes if one takes into account the additional complexity incurred by SMC, and after rescaling by the time taken by iteration, the auxiliary Kalman sampler dominates the gradient-enhanced PIT auxiliary cSMC which still dominates its basic counterpart.

In practice, the auxiliary Kalman, conditional SMC, and gradient-enhanced conditional SMC samplers took respectively in average $0.52$, $2.1$, and $2.2$ milliseconds per iteration. While some idiosyncrasies may be present due to the Hastings acceptance step present in Algorithm~\ref{alg:gslr-sampler} and not in the cSMC algorithm, we believe that this performance gap could be further improved by a careful consideration of the structure of the model in the Kalman sampler -- which we have not undertaken here in order to preserve the general applicability of our implementation.


