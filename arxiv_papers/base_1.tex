In this section, we aim to empirically evaluate the statistical and computational behaviours of our proposed methods. To this end, we consider three sets of examples.
\begin{itemize}
    \item The first model performs joint parameter and state estimation for a discretely observed stochastic differential equation. This model was used in \citet{mider2021continuous} to assess the performance of their forward-guiding backward-filtering method. We demonstrate here how to use auxiliary samplers for the same purpose and show the competitiveness of our approach.
    \item The second one is a multivariate stochastic volatility model known to be challenging for Gaussian approximations and used as a benchmark in, for example, \citet{guarniero2017iterated,finke2021csmc}. This model has latent Gaussian dynamics, and an observation model which happen to both be differentiable with respect to the latent state, so that all the methods of Section~\ref{subsec:auxiliary-lgssm} and Section~\ref{sec:pgibbs_samplers} apply. We consider the same parametrisation as in \citet{finke2021csmc}, which makes the system lack ergodicity and the standard particle Gibbs samplers not converge.
    \item The last one is a spatio-temporal model with independent latent Gaussian dynamics and is used in \citet{cruscino2022highdim} as a benchmark for high dimensional filtering. This model is akin to a type of dynamic random effect model in the sense that the latent states only interact at the level of the observations. This model is used to illustrate how latent structure can be used to design computationally efficient Kalman samplers that beat cSMC ones when runtime is taken into account.
\end{itemize}


Throughout this section, when using an auxiliary cSMC sampler, be it the sequential or the parallel-in-time formulation, we use $N=25$ particles, and a target acceptance rate of $25\%$ across all time steps. This is more conservative than the recommendation of \citet{finke2021csmc}, corresponding to $1 - (1 + N)^{-1/3}\approx 66\%$. The difference stems from the fact that it may happen that the methods do not reach the relatively high acceptance rate implied by the more optimistic target for all time steps, even with very small $\delta$ values. As a consequence, the sampler is ``stuck'' by only proposing very correlated trajectories in some places. This is mostly due to the largely longer time series considered here. Softening this resulted in empirically better mixing. Furthermore, for all the experiments, and following \citet[][]{titsias2018,finke2021csmc}, we consider $\delta \Sigma_t = \delta_t I$, with $\delta_t$ being constant across time steps for the Kalman samplers. Choosing a specific form for $\Sigma_t$ is akin to pre-conditioning and is left for future works. We then calibrate $\delta_t$ to achieve the desired acceptance rate (globally for Kalman samplers or per time step for the cSMC samplers) and the actual acceptance rate is reported in the relevant sections.

The implementation details for all the experiments are as follows: whenever we say that a method was run on a CPU, we have used an AMD\textsuperscript{\textregistered} Ryzen Threadripper 3960X with 24 cores, and whenever the method has been run on a GPU, we used a Nvidia\textsuperscript{\textregistered} GeForce RTX 3090 GPU with 24GB memory. All the experiments were implemented in Python~\citep{Rossum2009Python} using the JAX library~\citep{jax2018github} which natively supports CPU and GPU backends as well as automatic differentiation that we use to compute all the gradients we required. The code to reproduce the experiments listed below can be found at the following address:~\url{https://github.com/AdrienCorenflos/aux-ssm-samplers}.

