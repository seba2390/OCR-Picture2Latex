To begin with, details of the forward  model considered in this paper are addressed as follows. Let $\mathcal{S}$ denote a spatial domain that is bounded, connected and with a polygonal boundary $\partial \mathcal{S}$, and $s\in \mathcal{S}$ is  a spatial variable. The physics of the problem considered is governed by a PDE over the spatial domain $\mathcal{S}$: find $u(s,y(s))$ such that
\begin{equation}\label{physical_problem}
	\begin{aligned}
		&\mathcal{L}\Big(s,u(s,y(s));y(s)\Big)=h(s),\quad \forall s\in \mathcal{S},\\
		&\pB\Big(s,u(s,y(s));y(s)\Big)=g(s),\quad \forall s\in \partial\mathcal{S},
	\end{aligned}
\end{equation}
where $\mathcal{L}$ is a partial differential operator and $\pB$ is a boundary operator, both of which can depend on the unknown spatial-varying parameter $y(s)$, $h(s)$ is the source function, and  $g(s)$ specifies boundary conditions.

\subsection{Bayesian framework}
We consider the task of inferring the parameter $y\in \mathbb{R}^n$ from observations $\mathcal{D}_{obs}\in \mathbb{R}^m$ under the assumption that there exists a forward model $\mathcal{F}$ determined by \eqref{physical_problem} that maps the unknown parameter $y$ to the observations $\mathcal{D}_{obs}$:
\begin{align}
    \mathcal{D}_{obs}=\mathcal{F}(y)+\epsilon,
\end{align}
where $\epsilon\in \mathbb{R}^m$ is the measurement noise. Let $\pi_{\epsilon}(\epsilon)$ be the distribution of $\epsilon$, and one can obtain the distribution of $\mathcal{D}_{obs}$ conditioned on $y$:
\begin{align}
    \pi(\mathcal{D}_{obs}|y)=\pi_{\epsilon}(\mathcal{D}_{obs}-\mathcal{F}(y)).
\end{align}

Since often $m\ll n$, inverse problems are in general ill-posed, i.e.,\ one may not be able to uniquely recover the parameter $y$ given the noisy observations $\mathcal{D}_{obs}$. In the Bayesian setting, the parameters to be inferred are treated as random variables. Given the observations $\mathcal{D}_{obs}$, one assigns a prior distribution $\pi(y)$ encoding the prior information on the parameter of interest, and the posterior $\pi(y|\mathcal{D}_{obs})$ can then be calculated via the Bayes' rule:
\begin{equation}\label{yposterior}
\pi(y|\mathcal{D}_{obs})=\frac{\pi(\mathcal{D}_{obs}|y)\pi(y)}{C} \,\,\,
	\propto \,\,\,\underbrace{\pi(\mathcal{D}_{obs}|y)\pi(y)}_{\hat{\pi}(y)},
\end{equation}
where $\pi(\mathcal{D}_{obs}|y)$ is the likelihood function, and the evidence or marginal likelihood $C=\int \pi(\mathcal{D}_{obs}|y)\pi(y) dy$ is a normalization constant. 
% and $C$ is the evidence or marginal likelihood $C=\int \pi(\mathcal{D}_{obs}|y)\pi(y) d_y$, which is often intractable because of computing a high-dimensional integral.

Since the map $\mathcal{F}$ from $y$ to $\mathcal{D}_{obs}$ is typically nonlinear and the evidence is often intractable, especially for high-dimensional problems, the posterior, in general, cannot be obtained in a closed form. Therefore, the Bayesian inference needs to characterize the unnormalized posterior, which is usually achieved by variational inference (VI) or sampling approaches such as MCMC. However, extra assumptions are often introduced in VI, e.g., the family of parameterized density models for approximating the posterior distribution is a diagonal covariance Gaussian distribution, and sampling approaches such as MCMC become less efficient for sufficiently large $n$ and $m$. To improve efficiency, dimension reduction can be introduced such that VI \cite{goh2022solving,xia2023vi} or MCMC \cite{xia2022bayesian,patel2022solution} can be implemented in a low-dimensional latent space, where the dimension reduction is achieved, either explicitly or implicitly, by deep generative models. In this work we replace MCMC or VI with a normalizing flow to develop a dimension-reduced KRnet map approach (DR-KRnet) that is completely based on deep generative modeling.
%instead of a coupling between density approximation (VAE prior) and exact sampling (MCMC) or VI.

\subsection{Inference with a map}
The core idea of our approach is to find a map that pushes forward the prior to the posterior in the latent space. Before taking into account dimension reduction and surrogate modeling, we look at how normalizing flows approximate the posterior of $y$. 
Let $z \in \xs{R}^n$ be a random variable that has a known distribution, e.g., the standard Gaussian. 
%and $y$ be two random variables distributed according to the prior and the posterior respectively. 
We seek an invertible map $f: \mathbb{R}^n\to\mathbb{R}^n$
\begin{align}
	z=f(y),
\end{align}
which depends on the observations $\mathcal{D}_{obs}$, the forward model $\mathcal{F}$, and the distribution of the measurement noise $\epsilon$. Assuming the map $f$ exists, we have the posterior by the change of variables 
\begin{align}
	p_y(y)=p_{z}(f(y)) \left |\det\nabla_{y} f \right|.
\end{align}
In practice, we will learn the map $f(\cdot)$ by minimizing the Kullback-Leibler divergence between $p_y(y)$ and the posterior:
%To find the optimal map $f$, we minimize the following Kullback-Leibler divergence,
\begin{align}
	D_{KL}\left(p_y||\pi\left(y|\mathcal{D}_{obs}\right)\right)&=\int p_{y}\log \frac{p_{y}}{\pi(y|\mathcal{D}_{obs})} dy \nonumber\\
	&=\int p_{y}\log \frac{p_{y}}{\hat{\pi}(y)} dy+\log C \nonumber\\
	&=\int p_{z}\log \frac{p_{y}(f^{-1}(z))}{\hat{\pi}(f^{-1}(z))} dz +\log C\nonumber\\
	&\approx\frac{1}{I}\sum_{i=1}^I\log p_{y}\left(f^{-1}\left(z^{(i)}\right)\right)-\frac{1}{I}\sum_{i=1}^I\log \hat{\pi}\left(f^{-1}\left(z^{(i)}\right)\right)+\log C,\quad z^{(i)} \sim p_{z}.\label{highkl}
\end{align}
%One method for guaranteeing a unique map is to enforce a "triangular" structure for the map.
It is noted that normalizing flows provide an explicit density model and an efficient way to generate exact samples of $y$ through the invertible map $y=f^{-1}(z)$. Normalizing flows can be much more expressive than classical density models, e.g., the Gaussian model subject to a diagonal covariance matrix used in the mean-field approach. Yet the construction, representation, and evaluation of these generative models grow challenging in high-dimensional cases.
%  In particular, with the Knothe-Rosenblatt rearrangement, our newly proposed flow-based generative model, called KRnet map, provides the richer, more faithful posterior approximations than real NVP. However, the computational cost of Bayesian inference with KRnet map has a square growth with the dimensionality. 
%  Especially, the parameter of interest can be of high dimensionality in many practical problems, which renders these models infeasible. 
Moreover, the prior $\pi(y)$ is often provided through historical data $\{y^{(i)}\}_{i=1}^N$, which may be significantly different from the commonly used prior such as the Gaussian distribution and needs to be modeled explicitly. Furthermore, 
%it can be seen from \eqref{highkl} that the prior in $\hat{\pi}(f^{-1}\left(z^{(i)}\right))$ should be a closed form, and 
each sample $z$ requires an evaluation of the computationally expensive forward function $\mathcal{F}$. 
%In practice, the prior information $\pi(y)$ doesn't have a closed form but one often has access to a historical dataset $\{y^{(i)}\}_{i=1}^N$, where $y^{(i)}$ can be regarded as the samples from the prior $\pi(y)$.
To address these problems, we use a dimension-reduced VAE prior to model $\pi(y)$ through  historical data and then, in the low-dimensional latent space, apply KRnet to seek an invertible map $f$ with respect to a surrogate model for the forward problem.