In this section, we consider the same experiment as in \citet[Section 6.1]{mider2021continuous}, which consists in joint sampling of the state of a discretely observed chaotic Lorenz stochastic differential equation, and of the parameter defining its drift.
The SDE is given, conditionally on a parameter $\theta = (\theta_1, \theta_2, \theta_3)$ as a three-dimensional SDE $\dd{x} = \beta_{\theta}(x) \dd{t} + \sigma \dd{W_t}$,
where $W$ is a three-dimensional standard Wiener process and
\begin{equation}
    \beta_{\theta}(x) = \begin{pmatrix}
                            \theta_1 (x_2 - x_1) \\ \theta_2 x_1 - x_2 - x_1 x_3 \\ x_1 x_2 - \theta_3 x_3
    \end{pmatrix}.
\end{equation}
The state is then observed at regular intervals (every $t_0=0.01, t_1=0.02, \ldots, t_K=2$) through its second and third component only, giving an observation model $Y_k \sim \mathcal{N}(H x(t_k), 5 I_2)$, for $H=\begin{pmatrix}
                                                                                                                                                                                                                         0 & 1 & 0 \\ 0 & 0 & 1
\end{pmatrix}$. In order to provide comparable results to \citet{mider2021continuous}, we use the code they provided to generate the same dataset, and pick the same parametrisation of the model, including the same prior for the parameters. The Markov chain is then initialised according to the prior dynamics conditionally on the same initial parameter values as in \citet{mider2021continuous}. As per their experiment, we sample from the joint distribution $\pi(x(t'_0), \ldots, x(t'_L), \theta \mid y_{0:K})$, where $t'_0=0, t'_1=2e-4, \ldots, t'_L=2$ is a finer grid, making for a total sampling space dimension of $3 + 30\,000$. To do so, we too use the conjugacy relationship of $\theta$ given the full path for $x$, implementing a Hastings-within-Gibbs routine which samples $\theta$ conditionally on $x(t'_0), \ldots, x(t'_L)$ using its closed-form Gaussian posterior~\citet[Proposition 4.5]{mider2021continuous}, and then the auxiliary Kalman sampler to sample $x(t'_0), \ldots, x(t'_L)$ conditionally on $\theta$.

In our case, because the observation model is linear, we use the following proposal in the Kalman sampler: first, given the current trajectory $(x^k(t'_l))_{l=0}^L$ and parameter $\theta^k$ state of the MCMC chain we linearise $\mathbb{E}[x(t'_l) \mid x(t'_{l-1})] = \beta_{\theta^k}(x(t'_{l-1})) (t'_l - t'_{l-1})$ around $x^k(t'_{l-1})$ using the method of Section~\ref{subsec:auxiliary-lgssm} with extended linearisation, obtaining approximations $p(x(t'_l) \mid x(t'_{l-1})) \approx \mathcal{N}(x(t'_l); F_{l-1} x(t'_{l-1}) + b_{l-1}, Q_{l-1})$. For $l=0, \ldots, L$ we then sample $u_l \sim \mathcal{N}(x(t'_l), \frac{\delta}{2} I_3)$, and then form the proposal
\begin{equation}
    \label{eq:proposal-lorenz}
    \begin{split}
        q\left((z(t'_l))_{l=0}^L \mid u_{0:L}, (x^k(t'_l))_{l=0}^L, y_{0:L}\right)
        &\propto \mathcal{N}\left(z(t'_0); m_0, P_0\right) \left\{\prod_{l=1}^L \mathcal{N}\left(z(t'_l) ; F_{l-1}z(t'_{l-1}) + b_{l-1}, Q_{l-1}\right)\right\} \\
        &\left\{\prod_{k=0}^K \mathcal{N}\left(y_k ; H z(t_k), 5 I_2\right)\right\} \left\{\prod_{l=0}^L\mathcal{N}\left(u_l ; z(t'_l), \frac{\delta}{2} I_3\right)\right\}
    \end{split}
\end{equation}
targeting the augmented model
\begin{equation}
    \begin{split}
        \pi\left((z(t'_l))_{l=0}^L \mid u_{0:L}, y_{0:K}\right)
        &\propto \mathcal{N}\left(z(t'_0); m_0, P_0\right) \left\{\prod_{l=1}^L p\left(z(t'_l) \mid z(t'_{l-1})\right)\right\} \\
        &\left\{\prod_{k=0}^K \mathcal{N}\left(y_k ; H z(t_k), 5 I_2\right)\right\}
        \left\{\prod_{l=0}^L\mathcal{N}\left(u_l ; z(t'_l), \frac{\delta}{2} I_3\right)\right\}.
    \end{split}
\end{equation}
We run $2\,500$ adaptation steps, during which we modify $\delta$ to target an average acceptance rate of $23.4\%$ (as per \citet{mider2021continuous}). Interestingly, our actual acceptance rate after adaptation was closer to $70\%$, and the resulting $\delta$ virtually infinite. This means that the proposal distribution is almost reversible with respect to the target distribution. This high acceptance rate did not negatively impact the convergence of our algorithm. In fact, our resulting effective sampling size was larger than the best one reported by \citet{mider2021continuous} for both the parameters and the smoothing marginals (while the posterior distributions were similar). We report this in Table~\ref{tab:ess_lorenz}.
\begin{table}
    \centering
    \begin{tabular}{ lrr }
        \toprule
        & This paper & \citet{mider2021continuous} \\
        \midrule
        $X_{1,1.5}$ & 31254.0    & 10480.3                     \\
        $X_{2,1.5}$ & 35469.9    & 22890.5                     \\
        $X_{3,1.5}$ & 36584.7    & 24070.2                     \\
        \midrule
        $\theta_1$  & 11850.0    & 4592.4                      \\
        $\theta_2$  & 22960.5    & 15379.5                     \\
        $\theta_3$  & 12240.5    & 10917.7                     \\
        \bottomrule
    \end{tabular}
    \caption{Effective sample size (ESS) for the auxiliary sampler, computed using chains of length $10^5$. The results for \citet{mider2021continuous} are reported for ease of comparison.}
    \label{tab:ess_lorenz}
\end{table}

In practice, our sampler took $3\,149$ seconds (52 minutes) to run on the GPU, and $9\,424$ seconds (2h30mn) on the CPU. \citet{mider2021continuous}, on the other hand, report much faster run times (roughly 3-4 minutes). While this difference may seem massive, it can be imputed in totality to the difference in software for this experiment. Indeed, because they too rely on Gaussian filtering, the theoretical serial complexity of the two methods (when run on CPU) are exactly the same. While they use the programming language Julia~\citep{Julia-2017}, we use the JAX library~\citep{jax2018github} written in the Python language. Our choice comes with the benefit of direct GPU support, but also presents the inconvenience of not supporting varying size arrays. Consequently, rather than running Kalman filtering on the proposal LGSSM~\eqref{eq:proposal-lorenz} optimally by alternatively considering independent observations of size $3$ ($u_l$) and $2$ ($y_k$), we have to consider stacked observations $(u_l, y'_l)$ of dimension 5 and treat the $y'_l$ as being missing when $t'_l$ is not part of the $t_k'$. This technical limitation would be removed by considering instead a specialised implementation in a framework allowing for such optimisations.