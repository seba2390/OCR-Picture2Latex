\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
%\usepackage{neurips_2022}




% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2022_ml4ps}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{comment}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{float}
\usepackage{comment}
\usepackage{bm}
\bibliographystyle{abbrvnat}

\hyphenation{bet-ween}


%\title{Normalizing Flows For Electron Bunch Transformation In a Beamline}
\title{Learning Electron Bunch Distribution along a FEL Beamline by Normalising Flows}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
Anna Willmann \\
Helmholtz Zentrum Dresden-Rossendorf \\
Institute of Radiation Physics\\
\texttt{a.willmann@hzdr.de} \\
\And
    Jurjen Couperus Cabadağ \\
    Institute of Radiation Physics\\
    Helmholtz Zentrum Dresden-Rossendorf \\
\And
    Yen-Yu Chang \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
\And
    Richard Pausch \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
\And
    Amin Ghaith \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
Synchrotron SOLEIL \\
\And
    Alexander Debus \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
\And
    Arie Irman \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
\And
    Michael Bussmann\\
    Center for Advanced Systems Understanding\\
    Görlitz\\
\And
    Ulrich Schramm \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\
Technische Universität Dresden\\
\And
    Nico Hoffmann \\
    Institute of Radiation Physics\\
Helmholtz Zentrum Dresden-Rossendorf \\}
    
\begin{document}


\maketitle

\begin{abstract}
    Understanding and control of Laser-driven Free Electron Lasers remain to be difficult problems that require highly intensive experimental and theoretical research. 
    The gap between simulated and experimentally collected data might complicate studies and interpretation of obtained results. 
    In this work we developed a deep learning based surrogate that could help to fill in this gap.
    We introduce a surrogate model based on normalising flows for conditional phase-space representation of electron clouds in a FEL beamline. Achieved results let us discuss further benefits and limitations in exploitability of the models to gain deeper understanding of fundamental processes within a beamline.
    %(conditional normalizing flows and masked autoregressive flows)
\end{abstract}

\section{Introduction and Motivation}
Plasma acceleration processes are comprehensively studied in the recent years as high-energy X-rays find many applications such as medical diagnostics and treatment, chip manufacturing or hardening of material surfaces. X-rays are used in research as well as they can provide atomic scale imaging.

One source of high-brilliance coherent X-rays is free-electron lasers(FELs), where a bunch of relativistic electrons is transported through a sequence of optical elements and passed through an undulator - a magnetic structure with varying polarity, where electrons emmit photons coherently.

Application of conventional particle accelerators to generate relativistic electrons might end up in kilometers in size, what makes them expensive to keep and maintain. 
Laser-plasma accelerators(injectors) are compact and can reduce FEL's costs, however successful application of them requires a precise understanding and control over the process.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{beamline.png}
  \caption{Surrogate model application in laser plasma acceleration research}
  \label{fig:beamline}
\end{figure}

To achieve clear understanding of the process there are performed many experiments supported by theoretical research.
Numerical modeling of a free electron laser consists of multiple parts: simulations of electron acceleration, transport of an electron bunch and radiation in an undulator.
In-situ analysis of a FEL by simulation is limited due to complex workflow and potentially long simulation time.
Detailed description of used equipment of the beamline together with achieved experimental results is given in \citet{hzdr_fel}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{transformed_clouds.png}
  \caption{Electron bunch 1 is passed through a sequence of optical elements (fig. \ref{fig:beamline}), each of them is changing the shape of a distribution in order to achieve high coherence of emission in the undulator.}
  \label{fig:tranclouds}
\end{figure}

Figure \ref{fig:beamline} represents a beamline from \citet{coxinel} for a free electron laser with a scatch of the research pipeline.
Electrons are accelerated inside plasma and create a bunch(figure \ref{fig:beamline}, diagnostic screen 1) that is propagated through the manipulation line(figure \ref{fig:beamline}, 2-8) and being transformed as it is shown at figure \ref{fig:tranclouds}.

The operation of the beamline can be monitored by different diagnostics and quantities, among others, spectrometer measurements at positions of diagnostic screens(e.g. diagnostic screen 2 at figure \ref{fig:beamline}) and intensity of radiation. 
We can now tune certain performance criteria of the beamline, e.g. brilliance, stability, higher intensity, by adjusting certain parameters of the injector as well as the facility. 
Alternatively, simulation-based inference(\citet{sbinference}) or data-driven methods for one-step inversion(\citet{Hoffmann2016}) can be used to recover the matching phase-space representation to provide explanation of covered internals of the beamline to advance fundamental research. 
Simulation-based inference, however, relies on a fast surrogate model that can be used for Bayesian inversion by e.g. rejection-sampling (Approximate Bayesian Computation) or Markov Chain Monte Carlo. Advanced techniques in machine learning based surrogate modeling also promises simulation-based design of accelerators which is currently limited due computational challenges for simulation of the governing system(\cite{lehe2020machine}).

Main contribution of this work is therefore a surrogate model that allows for reconstruction of the phase space distribution at different locations of the beamline conditioned on parameters of an accelerated electron bunch by an injector. 

\section{Method}
The non-linear transformation of electrons along a beamline makes it hard to model the corresponding electron distribution determenistically, i.e. by tracking each particle. We therefore propose to learn the distribution of electrons in data-driven fashion by a generative model. There also is an increasing interest in the application of physics-informed neural networks (PINN) for training of surrogate models in physics as demonstrated by (\citet{Stiller2020}). However, we focus on a data-driven surrogate model since PINN increase the computational complexity of the training due to heavy usage of automatic differentiation. There are certain orthogonal state-of-the-art generative models such as GANs(\citet{goodfellow2014generative}), variational autoencoders(\citet{kingma2019introduction}) and normalizing flows(\citet{dinhnice}) while we concentrate on normalizing flows. Compared to variational autoencoders, it has simpler architecture(only one network instead of encoder and decoder) and does not have a loss of information due to compression. 
Negative log-likelihood loss function is easier to balance and optimize compared to the adversarial GANs' loss function.

\textbf{Data.}
We are working on simulated electron bunches propagated through the beamline and observed at 8 diagnostic screens(figure \ref{fig:beamline}), computed by ELEGANT software package(\citet{elegant}).

An electron bunch is represented in phase space, where each particle $\bm{p} \in \mathbb{R}^6$ is described by 6 scalars: 3 spatial coordinates ($x_c,y_c,z_c$), index $c$ is added to indicate a notation as related to the coordinate space, its energy $\gamma$ and per-particle divergence in $X$ and $Y$ dimensions(transverse plane). 
Further we call phase space representation of electron bunch an electron/particle/point cloud, where each electron is characterized by 6 coordinates.
Dimension $Z$(longitudinal coordinates) is a direction of bunch propagation, the bunch is centered around $z_c=0$. 

Each electron bunch consists of approximately 100.000 - 200.000 particles. 
During the transport particles with only a limited range of energies are propagated to the undulator in order to gain coherent emission, for this reason some particles will be filtered out in the beamline. 

In total there are 32 simulations, each simulation consists of 8 electron clouds(1 per diagnostic screen) and identified by statistical emmitance(area occupied by a particle cloud) - $\epsilon_x, \epsilon_y$ and beta function(transverse size of the particle cloud) - $\beta_x, \beta_y$ of the initial electron cloud(figure \ref{fig:beamline}, screen 1, before passing any optical element in the line).

An initial distribution for all simulations defined to be a normal distribution for simplicity of method evaluation.
In future the initial distribution will be extended to more complex shapes.

\textbf{Normalizing Flows.}\label{nfs}
Normalizing Flows(NFs) is a family of generative models, first introduced as a non-parametric method to estimate probability density by \citet{tabak1}. 
Further development of the approach into a deep learning based method was done by L. Dinh, who suggested suitable architectures (\citet{dinhnice}, \citet{dinhrealnvp}) to apply it to higher dimensional data without limitation of network's complexity. 



The goal is to find an invertible mapping between an electron $\bm{p}$ and a sampled point $\bm{z} \sim \mathcal{N}(0, 1)$: 


$\bm{p}=f^{-1}(\bm{z}, \epsilon_x, \epsilon_y, \beta_x, \beta_y, \bm{s}; \theta)$,

\begin{wrapfigure}[5]{r}{5.5cm}
    \begin{center}
    \vspace*{-12mm}
    \includegraphics[scale=0.2]{NF_application.png}
    \caption{Scheme of normalizing flows}
    \label{fig:nf_appl}
    \end{center}
\end{wrapfigure}

where $\bm{x}=(x_c,y_c,z_c,x_p,y_p,\gamma)$ - phase-space coordinates of each particle 
and $\bm{z}=(z_1, ..., z_6), z_k \sim \mathcal{N}(0, 1), \\ k=1...6$,
$\bm{s}$ - number of a diagnostic screen.
Trainable parameters of the network are defined by $\theta$.
In our work we have a fixed finite number of diagnostic screens and we pass it in the format of a one-hot vector. 
Further $(\epsilon_x, \epsilon_y, \beta_x, \beta_y, \bm{s})$ will be called a condition and denoted by $\bm{c}$. 

Mapping $f$ is trained in the forward direction to minimize the
negative log-likelihood loss function(\citet{cinn}):
$L = max_{\theta}\sum_{i=1}^N -\frac{||f(\bm{p}_i, \bm{c};\theta)||^2_2}{2}+log\left |det\frac{\partial f(\bm{p}_i, \bm{c};\theta)}{\partial \bm{p}_i^T} \right |$

Enhancement of normalizing flows by a masked autoregressive model was suggested by \citet{maf}, where instead of pure samples $\bm{z}$ authors suggest to add dependence on all previous samples:
$\bm{p}_i = \bm{z}_i exp(\mu_i)+\alpha_i$, $\mu_i=f_{\mu_i}(\bm{p}_{1:i-1}, \bm{c};\theta)$, $\alpha_i=f_{\alpha_i}(\bm{p}_{1:i-1}, \bm{c};\theta)$,
where $i$ is a number of a sampled point, 
${f_{\mu_i}, f_{\alpha_i}}$ - MADE networks from \citet{made}.
This architecture allows not to compute $\mu_i$ and $\alpha_i$ recurrently but by a single forward pass for each $i$. $N$ is a total number of samples in a training set.

This re-formulation generalizes the original approach and has a potential to higher expressiveness than conventional NFs.

\begin{wrapfigure}[16]{l}{6.5cm}
    \begin{center}
    \vspace*{-8mm}
    \includegraphics[scale=0.18]{reconstruction.png}
    \caption{Approximation of point clouds from validation set}
    \label{fig:3d}
    \end{center}
\end{wrapfigure}

\textbf{Model training.} In this work we used two models: a conditional normalizing flow(from \citet{cinn}) and masked autoregressive flow(MAF) from \citet{maf} with a condition. For the conditional NF we use implementation of GLOW blocks(\citet{glow}) provided by FrEIA package(\citet{freia}) and masked autoregressive flow with affine blocks is implemented in \textit{nflows}(\citet{nflows}). Both models consist of 7 coupling blocks with MLP subnetworks.

All passed data are normalized to lie within $[0, 1]$. 
We sample 10.000 particles from each point cloud per iteration, 70 point clouds are processed per batch.
Conditional NF model is trained until convergence over 10.000 iterations using Adam optimizer with learning rate of $10^{-5}$. 
The reached loss on training data is $-14.474$, and on validation data: $-11.705$. The model was trained on a single NVIDIA Tesla P100 16Gb node over 26 hours. 

Masked autoregressive model was trained over 13.750 iterations using Adam optimizer with learning rate of $10^{-3}$. 
The training was distributed(horovod package for distributed learning: \citet{horovod}) over 4 NVIDIA Tesla V100 32Gb nodes. 
Training took 16 hours, reached training loss of $-18.657$ and loss on validation data: $-18.485$. 
Overfitting was not observed.

\begin{wrapfigure}[11]{r}{8.0cm}
    \begin{center}
    \vspace*{-20mm}
    \includegraphics[scale=0.28]{slicewise_small.png}
    \caption{Projections of an approximated point cloud: left column - conditional NFs, right column - MAF}
    \label{fig:slw}
    \end{center}
\end{wrapfigure}

\section{Numerical results} \label{sec:results}

A disadvantage of MAF is a high computational demand, though in the context of our problem input and output have low enough dimensionality.

The models are compared by the following metrics: MMD(\citet{mmd}) and Sinkhorn distance(upper bound of EMD, introduced in \citet{emd}), quantitative results are presented in table \ref{tab:metrics}. Metrics are given in average value(among all clouds in validation set) and the worst reconstructed cloud. 
The masked autoregressive model has shown significantly better reconstruction of point clouds, even the lowest reconstruction quality is higher than a corresponding average value for conditional normalizing flows. 

Figure \ref{fig:3d} demonstrates reconstructed point clouds made by MAF model. 
Shapes of distributions at different positions in the beamline were recongnized and approximated correctly.
Figure \ref{fig:slw} shows projections of point cloud on different planes. 
The top row corresponds to the number of particles per a slice: slices are taken in direction $Z$, each slice has the same width, bottom - 2D projection of particles on plane $Z$ against energy.
As we see, approximated distribution by the conditional NF model does not cover the complete volume of the cloud and most of particles are located around 0, while the MAF model successfully tackles this issue.

\textbf{Limitations.} The dataset has to contain smoothly changing electron clouds to achieve meaningful interpolation.
The number of points in each cloud has to be large enough to represent distribution and allow to sample at each training iteration. 
A low number of particles per cloud cause not stable approximation in a point-wise mapping manner. 
Optimization of hyperparameters plays significant role due to invertibility of the model: 
parameters of subnetworks have to lie within a given range such that results of forward and backward passes do not explode or vanish. 
This limits complexity of transformations, the balance between width of that range and model complexity has to be found. 
An example of this problem one can observe at figure \ref{fig:3d}. 
The reconstruction has certain outliers e.g. in YX-plane: range of values in y axes for ground truth data is around $[-4, 4]$, while in a reconstructed one: $[-20, 20]$. 

\begin{table}
  \caption{Statistical distance on validation data}
  \label{tab:metrics}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Model     & MMD, average & MMD, worst & Sinkhorn, average & Sinkhorn, worst \\
    \midrule
    NF & 0.03293 & 0.06590 & 0.29618 & 0.7245 \\
    MAF & 0.00104 & 0.00180 & 0.08257 & 0.1343 \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Conclusions}
Surrogate modeling can help to optimize experimental research, increase degree of understanding of physical processes in FELs and simplify control over them.
High costs of numerical simulations motivate to use deep learning based methods in order to derive fast and reliable simulation based inference. 
We suggest a masked autoregressive flow based model for conditional generation of electron clouds, that is an important part in the beamline inversion: in order to extract the simulation parameters from diagnostic measurements. 
Results indicate better performance than conditional NFs due to higher expressivity that was theoretically proven by \citet{maf}.  

\section{Impact statement} \label{sec:impact}

The developed surrogate model could significantly accelerate the experimental research of free electron lasers. 
It provides an opportunity to replace simulations by a faster model and optimize parameter space to achieve desired results in experimental companion. 
The negative impact might come from reliability of the model for cases when experimental results do not correspond data used for model training or model was not verified well enough. 
It might cause inconsistent conclusions from collected data during the experiment and bring to wrong interpretations. 
Development of reliable digital twins is a crucial step towards an AI-guided experiments, that can reduce number of experimental runs to explore design space as well as amount of computational resources to produces required simulations. 
It will cause a significant decrease of energy consumption and make research and development of FELs more sustainable.




% argument is your BibTeX string definitions and bibliography database(s)
\newpage

\bibliography{neurips_2022.bib}


\begin{comment}


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Submission of papers to NeurIPS 2022}


Please read the instructions below carefully and follow them faithfully.


\subsection{Style}


Papers to be submitted to NeurIPS 2022 must be prepared according to the
instructions presented here. Papers may only be up to {\bf nine} pages long,
including figures. Additional pages \emph{containing only acknowledgments and
references} are allowed. Papers that exceed the page limit will not be
reviewed, or in any other way considered for presentation at the conference.


The margins in 2022 are the same as those in 2007, which allow for $\sim$$15\%$
more words in the paper compared to earlier years.


Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
NeurIPS website as indicated below. Please make sure you use the current files
and not previous versions. Tweaking the style files may be grounds for
rejection.


\subsection{Retrieval of style files}


The style files for NeurIPS and other conference information are available on
the World Wide Web at
\begin{center}
  \url{http://www.neurips.cc/}
\end{center}
The file \verb+neurips_2022.pdf+ contains these instructions and illustrates the
various formatting requirements your NeurIPS paper must satisfy.


The only supported style file for NeurIPS 2022 is \verb+neurips_2022.sty+,
rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
  Microsoft Word, and RTF are no longer supported!}


The \LaTeX{} style file contains three optional arguments: \verb+final+, which
creates a camera-ready copy, \verb+preprint+, which creates a preprint for
submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
\verb+natbib+ package for you in case of package clash.


\paragraph{Preprint option}
If you wish to post a preprint of your work online, e.g., on arXiv, using the
NeurIPS style, please use the \verb+preprint+ option. This will create a
nonanonymized version of your work with the text ``Preprint. Work in progress.''
in the footer. This version may be distributed as you see fit. Please \textbf{do
  not} use the \verb+final+ option, which should \textbf{only} be used for
papers accepted to NeurIPS.


At submission time, please omit the \verb+final+ and \verb+preprint+
options. This will anonymize your submission and add line numbers to aid
review. Please do \emph{not} refer to these line numbers in your paper as they
will be removed during generation of camera-ready copies.


The file \verb+neurips_2022.tex+ may be used as a ``shell'' for writing your
paper. All you have to do is replace the author, title, abstract, and text of
the paper with your own.


The formatting instructions contained in these style files are summarized in
Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


\section{General formatting instructions}
\label{gen_inst}


The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
type with a vertical spacing (leading) of 11~points.  Times New Roman is the
preferred typeface throughout, and will be selected for you by default.
Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
indentation.


The paper title should be 17~point, initial caps/lower case, bold, centered
between two horizontal rules. The top rule should be 4~points thick and the
bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
below the title to rules. All pages should start at 1~inch (6~picas) from the
top of the page.


For the final version, authors' names are set in boldface, and each name is
centered above the corresponding address. The lead author's name is to be listed
first (left-most), and the co-authors' names (if different address) are set to
follow. If there is only one co-author, list both author and co-author side by
side.


Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.


\section{Headings: first level}
\label{headings}


All headings should be lower case (except for first word and proper nouns),
flush left, and bold.


First-level headings should be in 12-point type.


\subsection{Headings: second level}


Second-level headings should be in 10-point type.


\subsubsection{Headings: third level}


Third-level headings should be in 10-point type.


\paragraph{Paragraphs}


There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.


\section{Citations, figures, tables, references}
\label{others}


These instructions apply to everyone.


\subsection{Citations within the text}


The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.


The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}


If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2022+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}


If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2022}
\end{verbatim}


As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous.''


\subsection{Footnotes}


Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).


Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}


\subsection{Figures}


\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.


You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


\subsection{Tables}


All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.


Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.


Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Final instructions}


Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.


\section{Preparing PDF files}


Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''


Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.


\begin{itemize}


\item You should directly generate PDF files using \verb+pdflatex+.


\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.


\item The IEEE has recommendations for generating PDF files whose fonts are also
  acceptable for NeurIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}


\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.


\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


\end{itemize}


If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.


\subsection{Margins in \LaTeX{}}


Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.


\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}


\section*{References}


References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\appendix


\section{Appendix}


Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.

\end{comment}
\end{document}