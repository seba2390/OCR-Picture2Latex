\section{Pretraining via Contrastive Learning}\label{contrastive_learning}


For human beings, when given many pictures of different animals, we are able to infer which pictures show the same animals even if we do not have any prior knowledge about the animals. In this process, we inadvertently learn a representation for each picture that can be used to capture the similarity between different pictures. Contrastive learning mimics the way human learns. To be more specific, based on positive and negative pairs, contrastive learning learns to embed data into some space where similar sample pairs stay close to each other and dissimilar ones are far apart. In this section, we show how pretraining (learning the embedding function) can benefit the downstream linear regression tasks under our theoretical framework.


\paragraph{Model setup.}
In the setting of contrastive learning, we assume that $x$ and $x'$ are sampled independently from the same distribution $\P(x)$. The similarity between $x$ and $x'$ is captured by a representation function $f_{\theta^* }:\mathcal{X}\rightarrow \R^{r}$ in the following sense, 
\$
&\P(t=1\,|\,x,x')=\frac{1}{1+e^{-f_{\theta^* }(x)^Tf_{\theta^* }(x')}},\notag\\
&\P(t=-1\,|\,x,x')=\frac{1}{1+e^{f_{\theta^* }(x)^Tf_{\theta^* }(x')}}.
\$
Here $t$ is a random variable that labels the similarity between $x$ and $x'$. If the data pair $(x,x')$ is similar, then $t$ tends to be $1$. If the data pair $(x,x')$ is not similar (negative samples), then $t$ tends to be $-1$. We assume $(x,x',t)\sim\P_{f_{\theta^* }}(x,x',t)$. Here, $(x',t)$ can be viewed as side information. The latent variable $z$ is defined as $z:=f_{\theta^* }(x)+\mu$, where $\mu\sim\mN(0,I_r)$ is a Gaussian noise that is uncorrelated with $x$. We denote $(x,z)\sim\P_{f_{\theta^* }}(x,z)$.


For the downstream task, we consider the following linear regression problem
\$
y=\beta^{* T}z+\nu,
\$
where $\nu\sim\mN(0,1)$ is a Gaussian noise. We assume that the true parameters $\theta^* \in\Theta$ and $\beta^* \in\mathcal{B}$, which satisfy a standard normalization assumption, i.e., $\|f_{\theta}(x)\|_2\leq 1$ for any $\theta\in\Theta$ and $x\in\mathcal{X}$ and $\|\beta\|_2\leq D$ for any $\beta\in\mathcal{B}$. We have access to i.i.d unlabeled data $\{x_i,x'_i,t_i\}^{m}_{i=1}$ and i.i.d labeled data $\{x_j,y_j\}^n_{j=1}$. Here $(x'_i,t_i)$ is the side information corresponding to $x_i$.

% \begin{assumption}\label{contrastive_bound}
% For any $\theta\in\Theta$ and $x\in\mathcal{X}$, it holds that $\|f_{\theta}(x)\|_2\leq 1$. For any $\beta\in\mathcal{B}$, it holds for some $D>0$ that $\|\beta\|_2\leq D$.
% \end{assumption}

In the sequel, we consider the squared loss function $\ell(x,y):=(y-x)^2$. We use the same form of truncated squared loss as in \eqref{eq:truncated_loss}.
% And the truncated square loss is defined as 
% \begin{align*}
% \tilde{\ell} (x,y) := (y-x)^{2} \cdot\mathds{1}_{\{(y-x)^2\leq L\}} + L \cdot \mathds{1}_{\{(y-x)^2 > L\}},
% \end{align*}
% where $L$ is the truncation level. 
%\chijin{change across the entire paper: square loss $\rightarrow$ squared loss}

% Given unlabeled data $\{x_i,x'_i,t_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, we run the following algorithm for contrastive learning:


% \begin{algorithm}[H]
% \caption{Two-Phase MLE+ERM with Side Information}\label{mle+erm_side}
% \begin{algorithmic}[1]
% \State {\bf Input:} $\{x_i,x'_i,t_i\}^m_{i=1}$, $\{x_j,y_j\}^n_{j=1}$
% \State Use unlabeled data $\{x_i,x'_i,t_i\}^m_{i=1}$ to learn $\hat\theta$ via MLE:
% \%\label{mle_side}
% \hat\theta\leftarrow\argmax_{\theta\in\Theta}\sum^m_{i=1}\log p_{f_{\theta}}(x_i,x'_i,t_i).
% \%
% \State Fix $\hat\theta$ and use labeled data $\{x_j,y_j\}^n_{j=1}$ to learn $\hat\beta$ via ERM:
% \%\label{erm_side}
% \hat\beta\leftarrow\argmin_{\beta\in\mathcal{B}}\sum^n_{j=1}\tilde{\ell}\big(g_{\hat\theta,\beta}(x_j),y_j\big).
% \%
% \State {\bf Output:} $\hat\theta$ and $\hat\beta$.
% \end{algorithmic}
% \end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\paragraph{Weakly informative condition.}
We first prove that the above model satisfies Assumption \ref{weak_invariance}:
% In the sequel, we verify Assumption \ref{weak_invariance} for the above model. We have the following guarantee.

\begin{lemma}\label{contrastive_ti}
Contrastive learning with linear regression as downstream tasks is $\kappa^{-1}$-weakly-informative, where
\$
\kappa=c_3\cdot\sqrt{\frac{1}{\sigma_{\min}(\E[f_{\theta^* }(x)f_{\theta^* }(x)^{T}])}}.
\$
Here $c_3$ is an absolute constant.
\end{lemma}



% \begin{lemma}\label{contrastive_ti}
% There exists $O\in\R^{r\times r}$, $O^TO=OO^T=I_{r}$ such that
% \$
% \TV\big(\P_{Of_{\theta}}(x,z),\P_{f_{\theta^* }}(x,z)\big)\leq c\cdot\sqrt{\frac{1}{\sigma_{\min}(\E[f_{\theta^* }(x)f_{\theta^* }(x)^{T}])}}\cdot H\big(\P_{f_{\theta}}(x,x',t),\P_{f_{\theta^* }}(x,x',t)\big).
% \$
% Here $c$ is some absolute constants.
% \end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\paragraph{Theoretical results.}
We define a set of density functions $\mP_{\mathcal{X}\times\mathcal{S}}(\mathcal{F}_{\theta}):=\{p_{f_{\theta}}(x,x',t)\,|\,\theta\in\Theta\}$. We then have the following theoretical guarantee.

\begin{theorem}\label{contrastive_main}
We consider Algorithm \ref{mle+erm} with truncated squared loss \eqref{eq:truncated_loss}
% \begin{align*}
% \tilde{\ell} (x,y) := (y-x)^{2} \cdot\mathds{1}_{\{(y-x)^2\leq L\}} + L \cdot \mathds{1}_{\{(y-x)^2 > L\}},
% \end{align*}
where $L=36(D^2+1)\log n$. Let $\hat\theta, \hat\beta$ be the outputs of Algorithm \ref{mle+erm}. Then, for contrastive learning with linear regression as downstream tasks, with probability at least $1-\delta$, the excess risk can be bounded as follows,
\begin{align*}
{\rm Error}_{\ell}(\hat \theta,\hat\beta)\leq\Tilde{\mathcal{O}}\bigg(\kappa L\sqrt{\frac{\log N_{\b}\big(\mP_{\mathcal{X}\times\mathcal{S}}(\mathcal{F}_{\theta}),1/m^2\big)}{m}}+L\sqrt{\frac{1}{n}}\bigg),
\end{align*}
where $L=36(D^2+1)\log n$ and $\kappa$ is specified in Lemma \ref{contrastive_ti}.
% \$
% \kappa=c_3\cdot\sqrt{\frac{1}{\sigma_{\min}(\E[f_{\theta^* }(x)f_{\theta^* }(x)^{T}])}}
% \$
% for some absolute constants $c_3$. 
Here $\tilde{\mathcal{O}}(\cdot)$ omits some constants and the polylogarithmic factors in $1/\delta$.
\end{theorem}

Note that the excess risk of directly training with labeled data strongly depends on the complexity of the function class $\mathcal{F}_{\theta}$. In the case that $m\gg n$, the excess risk of Theorem \ref{contrastive_main} scales as $\tilde{O}(\sqrt{1/n})$, which beats the pure supervised learning if the complexity of $\mathcal{F}_{\theta}$ is quite large. Thus, the utility of unsupervised pretraining is revealed for contrastive learning.

%\chijin{add discussions similar to the previous two sections. Do not use the same wording.}