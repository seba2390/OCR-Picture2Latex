\begin{abstract}
% Unsupervised pretraining, which pre-trains models using large amount of unlabeled data to facilitate the learning of downstream tasks, is a crucial component of modern large-scale machine learning systems. Despite its tremendous empirical success, the rigorous theoretical understanding of why unsupervised pretraining helps remains relatively limited. This paper studies the natural scheme of using Maximum Likelihood Estimation (MLE) for unsupervised pretraining and Empirical Risk Minimization (ERM) for supervised learning of downstream tasks. We consider the setting where the unsupervised pretraining tasks are specified by the class of latent variable models $\Phi$ with $m$ unlabeled samples, and the downstream tasks are specified by the prediction class $\Psi$ with $n$ labeled samples. We prove that, under mild conditions, our algorithm achieves a prediction error of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_\Phi/m} + \sqrt{\mathcal{C}_\Psi/n})$ for downstream tasks, where $\mathcal{C}_\Phi, \mathcal{C}_\Psi$ are complexity measures for function classes $\Phi, \Psi$ respectively. Compared to the standard error of $\tilde{\mathcal{O}}(\sqrt{(\mathcal{C}_{\Phi \circ \Psi})/n})$ achieved by directly performing supervised learning on labeled data, our result rigorously shows the benefit of unsupervised pretraining when we have a large number of unlabeled data ($m \gg n$), and when the complexity of unsupervised task $\mathcal{C}_\Phi$ is much larger than that of the downstream task $\mathcal{C}_\Psi$. We futher instantiate our general theoretical framework with two concrete problem instances: (1) factored models with linear regression as downstream tasks; and (2) Gaussian mixture models with classification as downstream tasks.


Unsupervised pretraining, which learns a useful representation using a large amount of unlabeled data to facilitate the learning of downstream tasks, is a critical component of modern large-scale machine learning systems. Despite its tremendous empirical success, the rigorous theoretical understanding of why unsupervised pretraining generally helps remains rather limited---most existing results are restricted to particular methods or approaches for unsupervised pretraining with specialized structural assumptions. This paper studies a generic framework,
% for unsupervised pretraining, 
where the unsupervised representation learning task is specified by an abstract class of latent variable models $\Phi$ and the downstream task is specified by a class of prediction functions $\Psi$. We consider a natural approach of using Maximum Likelihood Estimation (MLE) for unsupervised pretraining and Empirical Risk Minimization (ERM) for learning downstream tasks. We prove that, under a mild ``informative'' condition, our algorithm achieves an excess risk of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_\Phi/m} + \sqrt{\mathcal{C}_\Psi/n})$ for downstream tasks, where $\mathcal{C}_\Phi, \mathcal{C}_\Psi$ are complexity measures of function classes $\Phi, \Psi$, and $m, n$ are the number of unlabeled and labeled data respectively. Comparing to the baseline of $\tilde{\mathcal{O}}(\sqrt{\mathcal{C}_{\Phi \circ \Psi}/n})$ achieved by performing supervised learning using only the labeled data, our result rigorously shows the benefit of unsupervised pretraining when $m \gg n$ and $\mathcal{C}_{\Phi\circ \Psi} > \mathcal{C}_\Psi$. This paper further shows that our generic framework covers a wide range of approaches for unsupervised pretraining, including factor models, Gaussian mixture models, and contrastive learning.

% we have a large number of unlabeled data ($m \gg n$) and the complexity of learning representation $\mathcal{C}_\Phi$ is much larger than the complexity of learning downstream tasks $\mathcal{C}_\Psi$. We further instantiate our general theory with three concrete problem instances: (1) factor models with linear regression as downstream tasks; (2) Gaussian mixture models with classification as downstream tasks; and (3) Contrastive learning with linear regression as downstream tasks.






% Despite the great success of unsupervised pre-training in practice, solid theoretical guarantee remains challenging and lacking. Our model.




% is powerful in xx scenario. Despite the great success of unsupervised pre-training in practice, solid theoretical guarantee remains challenging and lacking. Our model.
% Our algorithm.
% Complexity, adventage.
% Instantiate our general theory with two concrete instances.



% Recent advance on computer vision and nature language processing has benefited a lot from unsupervised pre-training when the downstream tasks are of low resource. Despite the great success of unsupervised pre-training in practice, solid theoretical guarantee remains challenging and lacking. In this paper, we focus on the setting where $m$ data is observed for unsupervised pre-training with paramter $\phi^* \in\Phi$ and only $n$ data ($m\gg n$) is observed for downstream task learning with parameter $(\phi^* ,\psi^* )\in\Phi\times\Psi$. We propose a two-phase learning algorithm and show that the sample complexity needed for pre-training and downstream task learning is $\mathcal{C}(\Phi)$ and $\mathcal{C}(\Psi)$, respectively. Here we denote by $\mathcal{C}(\cdot)$ the complexity measure of the parameter space. We further fit two specific tasks into our framework: (1) For $r$-factor, $d$-dimensional factor model with downstream regression task, our algorithm achieves $\Tilde{O}(\sqrt{\frac{dr}{m}}+\sqrt{\frac{r}{n}})$ estimation error. (2) For $d$-dimensional, $K$-component, equally weighted standard Gaussian mixture model with downstream classification task, our algorithm achieves $\Tilde{O}(\sqrt{\frac{dK}{m}}+\sqrt{\frac{K}{n}})$ estimation error.
\end{abstract}