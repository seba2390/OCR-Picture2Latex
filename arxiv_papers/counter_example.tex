\section{Failure of Two-Phase MLE}\label{counter_example}
Given unlabeled data $\{x_i\}^m_{i=1}$ and labeled data $\{x_j,y_j\}^n_{j=1}$, another natural scheme is to use a two-phase MLE (Algorithm \ref{mle+mle}). To be specific, in the first phase, we use MLE to estimate $\phi^* $ based on the unlabeled data $\{x_i\}^m_{i=1}$. In the second phase, we use MLE again to estimate $\psi^* $ based on pretrained $\hat\phi$ and the labeled data $\{x_j,y_j\}^{n}_{j=1}$.

% \$
% \hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i).
% \$
% In the second phase, we fix $\hat\phi$ learned in the first phase and estimate $\psi^* $ by $\hat\psi$ using data $\{x_j,y_j\}^n_{j=1}$ via MLE, i.e
% \$
% \hat\psi\leftarrow\argmax_{\psi\in\Psi}\sum^n_{j=1}\log p_{\hat\phi,\psi}(x_j,y_j).
% \$

\begin{algorithm}[H]
\caption{Two-phase MLE}\label{mle+mle}
\begin{algorithmic}[1]
\State {\bf Input:} $\{x_i\}^m_{i=1}$, $\{(x_j,y_j)\}^n_{j=1}$
\State Use unlabeled data $\{x_i\}^m_{i=1}$ to learn $\hat\phi$ via MLE:
\$
\hat\phi\leftarrow\argmax_{\phi\in\Phi}\sum^m_{i=1}\log p_{\phi}(x_i).
\$
\State Fix $\hat\phi$ and use labeled data $\{(x_j,y_j)\}^n_{j=1}$ to learn $\hat\psi$ via MLE:
\$
\hat\psi\leftarrow\argmax_{\psi\in\Psi}\sum^n_{j=1}\log p_{\hat\phi,\psi}(x_j,y_j).
\$
\State {\bf Output:} $\hat\phi$ and $\hat\psi$.
\end{algorithmic}
\end{algorithm}


Note that the two-phase MLE does not directly associate the learning process with the loss function. Thus, the only way to evaluate the prediction error is to study the total variation distance between $\P_{\hat\phi, \hat\psi}(x,y)$ and $\P_{\phi^* ,\psi^* }(x,y)$. In the pretraining phase, MLE guarantees that the estimator $\mathbb{P}_{\hat{\phi}}$ is close to $\mathbb{P}_{\phi^{*}}$ in the sense of total variation distance (Theorem \ref{tv_mle}). However, it's still possible that for some $x$, $\mathbb{P}_{\hat{\phi}}(x)=0$ while $\mathbb{P}_{\phi^{*}}(x) \neq 0$. This phenomenon may result in $\log p_{\hat\phi,\psi^* }(x_j,y_j)=-\infty$ for some labeled data in the learning of downstream tasks, which will dramatically influence the behaviour of MLE for estimating $\psi^{*}$ and finally lead to the failure of the second phase. Inspired by this idea, we give the following theorem.



\begin{theorem}\label{counter}
There exists $\Phi, \Psi, \phi^{*}\in{\Phi}, \psi^{*}\in{\Psi}$, such that for any constant $c>0$, there exists $m,n \geq c$ such that with probability at least $\frac{1}{2}(1-e^{-1})e^{-1}$, we have
\$
\TV\big(\P_{\hat\phi,\hat\psi}(x,y),\P_{\phi^* ,\psi^* }(x,y)\big)\geq \frac{1}{8},
\$
where $\hat\phi$ and $\hat\psi$ are the outputs of Algorithm \ref{mle+mle}.
\end{theorem}


% Here we briefly explain why Algorithm \ref{mle+mle} may fail under some circumstances. In the first phase, MLE guarantees that the estimator $\mathbb{P}_{\hat{\phi}}$ is close to $\mathbb{P}_{\phi^{*}}$ under total variation distance (Theorem \ref{tv_mle}). But it's still possible that for some $x$, $\mathbb{P}_{\hat{\phi}}(x)=0$ while $\mathbb{P}_{\phi^{*}}(x) \neq 0$, which results in $\log\mathbb{P}_{\hat{\phi}}(x)=\log 0 = -\infty$. This phenomenon will dramatically influence the behaviour of MLE for estimating $\psi^{*}$ and will finally lead to the failure of the second phase.