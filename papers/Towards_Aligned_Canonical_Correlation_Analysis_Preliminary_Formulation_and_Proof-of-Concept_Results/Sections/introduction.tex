%1- Problem motivation 
%why is interesting 
%\reminder{Para 1: Motivate CCA with examples of two bipartite graphs (user, product) and (user, video) or generally two different feature spaces (text vs. image or whatever). Say that CCA is a powerful way of finding a common latent space for those heterogeneous data \cite{}. Then say that even though CCA has been around for a long time, it is still extremely relevant as cutting-edge self-supervised representation learning techniques such as Barlow Twins \cite{zbontar2021barlow,bielak2022graph,zhang2021canonical,balestriero2023cookbook} can be essentially seen as variations of CCA.}
Canonical Correlation Analysis (CCA) \cite{harold1936relations,kettenring1971canonical} is a classical model which, given two different views of the same set of entities, e.g., two different bipartite graphs of (user, product) and (user, video) interactions or different feature representations for those entities in general, seeks to project those entities (users) in a low-dimensional space where the different projected views are maximally correlated. Essentially, CCA can jointly embed heterogeneous datasets in a common low-dimensional space, as it can be extended to more than two views \cite{chen2019graph,chen2022unsupervised}. Even though CCA has been in and out of the spotlight for many decades and has been around for quite a long time, it is still extremely relevant, not only as a standalone data mining tool, but also due to the fact that cutting-edge self-supervised representation learning techniques, such as Barlow Twins \cite{zbontar2021barlow,bielak2022graph,zhang2021canonical,balestriero2023cookbook}, can be essentially seen as variations of CCA, where the goal is to embed two views of the data (the original view and the augmentation) in a latent space where correlation is maximized.

Traditionally, in CCA-style analysis, we assume that entities across views have one-to-one correspondence across the two or more views of the data, and there is a wealth of algorithms that study different formulations for solving the problem of projecting those views in that desired maximally correlated space, both linearly and non-linearly \cite{andrew2013deep}. What if, however, this one-to-one correspondence is unknown? In this case, we are faced with two problems: (1) entity alignment and (2) CCA embedding. Motivated by recent results \cite{wu2022tenalign} in the related problem of misaligned joint tensor factorization, it turns out that formulating and solving the alignment and embedding problems jointly yields better results than solving each problem separately in multiple steps, as it appears that the two sub-problems work synergistically to produce better quality alignment and embeddings. In this work, we explore this type of formulation for CCA and we propose a new formulation, the Aligned Canonical Correlation Analysis (ACCA), where we seek to jointly compute the alignment and the embedding space.

%\reminder{Para 2: CCA assumes that there is a known alignment of the data points (users). What if this alignment is unknown? In this paper we propose a new formulation of the model, the Aligned Canonical Correlation Analysis (ACCA), where we jointly compute the alignment and the embedding space. We have recently shown that jointly formulating this problem for aligned tensor factorization (CITE TenAlign)\cite{wu2022tenalign} makes the alignment and factorization problems to work synergistically, producing better quality alignment and embeddings than doing so in multiple steps. In this paper we endavor to explore this for CCA.}

The closest formulation to our proposed model is found in \cite{sahbi2018learning} where the author is considering linear transformation of the two views in CCA, however, is not seeking to recover the precise alignment matrix as our formulation does. In our on-going work we will consider scenarios where we can fairly compare the two formulations and understand pros and cons for either one.

The list of contributions in this preliminary work are:
\begin{itemize}
    \item {\bf Novel Formulation}: We propose the Aligned Canonical Correlation Analysis (ACCA) model, which seeks to jointly identify the best entity alingment and latent embedding for the dataset views.
    \item{\bf Proof of Concept}: We derive an Alternating Optimization algorithm and show preliminary results for solving the problem, demonstrating the feasibility of our effort.
\end{itemize}