%\reminder{First describe the formulation, then show how we solve the optimization problem}

The traditional CCA formulations require the entities/samples from both $\mathbf{X}$ and $\mathbf{Y}$ to be aligned, i.e., the $i$-th columns of  $\mathbf{X}$ and $\mathbf{Y}$ correspond to the two views/observations of the same latent data sample which is the groundtruth of the $i$-th
column of $\mathbf{S}$. However, if such entity alignment is imperfect, CCA is not able to learn the meaningful latent representations shared by two datasets. Toward this end, we propose a novel model, namely \emph{aligned canonical correlation analysis} (
ACCA), to jointly learn the latent representations of two views and recover the entity alignment between the two views.

\subsection{Proposed Formulation for ACCA}
Consider two centered datasets $\mathbf{X}\in \mathbb{R}^{D_x \times N}$ and $\mathbf{Y}\in \mathbb{R}^{Dy \times N}$ are two views in one dataset, and the alignment between the columns of the two views, denoted as $\bar{\mathbf{P}}\in\mathbf{R}^{N\times N}$, is unknown. Our goal is to learn the latent component representation $\mathbf{S}$ and predict the alignment matrix $\bar{\mathbf{P}}$ iteratively. Let's denote the estimation of $\bar{\mathbf{P}}$ as ${\mathbf{P}}\in\mathbf{R}^{N\times N}$. Theoretically, $\mathbf{P}$ should be a (binary) permutation matrix, and the sum of row or column is one, which shows that $\mathbf{P}$ is an orthogonal matrix. Mathematically, we will minimize $\left \|\mathbf{U}{\mathbf{X}} -\mathbf{S}\right\|_F^2 + \left\|\mathbf{V}{\mathbf{Y}}\mathbf{P} -\mathbf{S}\right\|_F^2$ under the constraints of $\mathbf{P}$ as well as the constraints from CCA, i.e., $\mathbf{SS}^\top =\mathbf{I}$. To address the computational limitation in such optimization problem, we define a list of constraints to describe different aspects of a permutation matrix instead of enforcing it to be one, for a tractable optimization solution. By relaxing the constraints on $\mathbf{P}$, the optimization formulation of our proposed ACCA is shown as:
\begin{align}
	\min_{\mathbf{U},\mathbf{V}, \mathbf{S},\mathbf{P}}   & \left \|\mathbf{U}{\mathbf{X}} -\mathbf{S}\right\|_F^2 + \left\|\mathbf{V}{\mathbf{Y}}\mathbf{P} -\mathbf{S}\right\|_F^2+\gamma_1\|\mathbf{P}\mathbf{P}^\top -\mathbf{I}\|_F^2+\gamma_2\|\mathbf{P}^\top\mathbf{P} -\mathbf{I}\|_F^2\label{eq:acca_loss}\\
  \text {S. T. } \quad&\mathbf{SS}^\top =\mathbf{I}, \text{(uncorrelatedness)}\\ 
 &0\le p_{i,j}\le 1, \forall i, j, \text{(nonnegativity)}\label{eq:con1}\\
 &\sum_{j=1}^N p_{i,j}=1, \forall i, \text{(row-wise sum )}\label{eq:con2}\\
 &H(\mathbf{p}_i)\le \lambda, \forall i \text{( entropy)}\label{eq:acca}
	\end{align}
 where $p_{i,j}$ is the $(i,j)$-th entry of $\mathbf{P}$, $\mathbf{p}_i$ is the $i$-th row of $\mathbf{P}$, $H(\mathbf{p}_i)$ is the entropy of $\mathbf{p}_i$ by viewing the $N$ entries of $\mathbf{p}_i$ as a discrete probability distribution, and the hyperparamters $\gamma_1$, $\gamma_2$, and $\lambda$ are nonnegative. Enforcing the low entropy of $\mathbf{p}_i$ guarantees that the distribution is closed to a deterministic distribution, and the second and third terms in Eq. \eqref{eq:acca_loss} will promote the orthogonality of $\mathbf{P}$


\subsection{Alternating Optimization for ACCA}
To solve the ACCA formulation, as the solution of CCA is dependent on the estimation of permutation matrix, and vice versa, we  adopt alternating optimization method, shown in Algorithm \ref{alg:our}. 

%\reminder{Biqian, what's the corresponding solver of fmincon in Python, add citation, modify this accordingly in Algorithm 1}

\begin{algorithm}[t]
	\caption{Aligned Canonical Correlation Analysis}
	\label{alg:our}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:}
		centered datasets $\mathbf{X}$ and $\mathbf{Y}$; dimension of the latent representation $d$; hyperparameters $\gamma_1$, $\gamma_2$, and $\lambda$;  and initialization of $\mathbf{P}$.
		\STATE {\bfseries Repeat}\label{step:4} \newline  
		Update $\mathbf{S}$: the rows of $\mathbf{S}$ are the $d$ eigenvectors corresponding to the top-$d$ eigenvalues of $\mathbf{X}^\top(\mathbf{X}\mathbf{X}^\top)^{-1}\mathbf{X}+(\mathbf{YP})^\top(\mathbf{YP}\mathbf{P}^\top\mathbf{Y}^\top)^{-1}\mathbf{YP}$.
		 \newline  
		Update $\mathbf{U}$: $\mathbf{U} = \mathbf{S}\mathbf{X}^\top
(\mathbf{X}\mathbf{X}^\top)^{-1}$.
		 \newline  
		Update $\mathbf{V}$: $\mathbf{V} =\mathbf{S}(\mathbf{YP})^\top(\mathbf{YP}\mathbf{P}^\top\mathbf{Y}^\top)^{-1}$.
  \newline  
		Update $\mathbf{P}$ using \emph{scipy.optimize.minimize} solver.
		\STATE {\bfseries Until} the objective Eq. \eqref{eq:acca_loss} is below a threshold or the number of iterations is beyond another threshold. 
		\STATE {\bfseries Output:}  $\mathbf{U, V, S, P}$.
	\end{algorithmic}
\end{algorithm} 
