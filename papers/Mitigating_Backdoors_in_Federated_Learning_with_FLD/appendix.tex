\clearpage
\appendix
\setcounter{equation}{0}
\renewcommand\theequation{\arabic{equation}}
% \section{Convergence Analysis}
% To analyze the convergence of FLD, we propose the theorem of convergence and prove it.

% % \subsection{Notation and Assumptions}
% % Let $F_i$ denotes the local model of the $i$-th client, $i=1,2,\cdots,N$. Let $F$ denotes the global model in the central parameter server. Suppose our models satisfy Lipschitz continuous gradient, we make Assumptions \ref{assumption1} and \ref{assumption2}. 


% % \begin{assumption}\label{assumption1}
% % ($L$-smooth). $F_1,\cdots,F_N$ are all $L$-smooth: $\forall x,y, F_i(x)\leq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{L}{2}||x-y||_2^2$.
% % \end{assumption}
 
% % \begin{assumption}\label{assumption2}
% % ($\mu$-strongly convex). $F_1,\cdots,F_N$ are all $\mu$-strongly convex:  $\forall x,y, F_i(x)\geq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{\mu}{2}||x-y||_2^2$.
% % \end{assumption}
% % We also follow the assumption made by~\cite{stich2018sparsified,yu2019parallel,li2019convergence} as follows.
% % \begin{assumption}\label{assumption3}
% % The expected squared norm of stochastic gradients is uniformly bounded, i.e., $\exists U>0$, $\mathbb{E}||\nabla F_i(\cdot)||^2 \leq U^2$ for all $i=1,\cdots,N$.
% % \end{assumption}
% % We make Assumption \ref{assumption4} to bound the expectation of $||w_i^t||^2$, where $w_i^t$ denotes the parameters of $F_i$ in $t$-round.
% % \begin{assumption}\label{assumption4}
% % (Bounding the expectation of $|| w_i^t ||^2$). The expected squared norm of $i$-th client's local model parameters is bounded: $\exists M>0$, $\mathbb{E}||w_i^t||^2 \leq M^2$ for all $i=1,\cdots,N$ and $t=1,\cdots,T$.
% % \end{assumption}

% % \subsection{Theorem and Proof}

% %提出收敛性理论, FLD收敛
% \newtheorem{thm}{Theorem}
% \begin{thm}\label{thm1}
% Let Assumptions \ref{assumption1} to \ref{assumption4} hold and $L$, $\mu$, $U$, $M$ be defined therein. Choose the learning rate $\eta^t=\frac{\theta}{t+\epsilon}$, $ \epsilon>0$, $\theta > \frac{1}{\mu}$, we define $\lambda=\max\{\frac{\theta A}{\theta \mu -1}, (\epsilon+1)Z_1\}$. Then FLD satisfies 
% \begin{equation}
%     \begin{split}
%         \mathbb{E}[F(G^t)]-F^*
% 	\leq \frac{L}{2} Z_t 
% 	\leq \frac{L}{2}\frac{\lambda}{(t+\epsilon )^{\frac{1}{2}}}
% 	\stackrel{t \to \infty}{\longrightarrow}0,
%     \end{split}
% \end{equation}
% where 
% \begin{equation}
%     \begin{split}
%         & A=4U^2+M^2+2\Gamma, \\
%         & Z_t=\mathbb{E}||G^t-G^*||^2.
%     \end{split}
% \end{equation}
% \end{thm} 


% \begin{proof}
% %以下是证明过程
% %第一部分证明\mathbb{E}||G^{t+1}-G^*||^2\mathbb{E}||G^{t+1}-G^*||^2有上界，其中G^{t+1}是第t+1轮全局模型的权重，G^*是全局模型最优权重。
% Let $G^{t+1}$ denote the global model's parameters in the central server in $(t+1)$-round and $G^*$ be the optimal parameters in the central server. Additionally, $g^t=\sum\limits_{i\in C_b^t}p_i\nabla F_i(w_i^t,\xi_i^t)$, where $g^t$ denotes the gradient updates uploaded by the clients in $t$-round and $p_i$ denotes the weight of the $i$-client's gradient during aggregation. $\bar{g^t}=\sum\limits_{i\in C_b^t}p_i\nabla F_i(w_i^t)$ and $G^{t+1}=G^t-\eta^t g^t$, where $C_b^t$ denotes the collection of benign clients chosen by FLD in $t$-round. Then, we have 
% \begin{equation}
%     \begin{split}\label{ineq1}
% 	||G^{t+1}-G^*||^2
% 	& = ||G^t-\eta^t g^t-G^*-\eta^t \bar{g^t} + \eta^t \bar{g^t}|| \\
% 	& = \underbrace{||G^t-G^*-\eta^t \bar{g^t}||^2}_{P_1} \\
%         & +\underbrace{2\eta^t<G^t-G^*-\eta^t \bar{g^t},\bar{g^t}-g^t>}_{P_2} \\
% 	& +(\eta^t)^2||\bar{g^t}-g^t||^2.
%     \end{split}
% \end{equation}

% Since $\mathbb{E}g^t=\bar{g^t}$, we see $\mathbb{E}P_2=0$. Now we split $P_1$ into three terms:   
% \begin{equation}
%     \begin{split}\label{ineq2}
% 	P_1
% 	& = ||G^t-G^*-\eta^t \bar{g^t}||^2 \\
% 	& = ||G^t-G^*||^2\underbrace{-2\eta^t<G^t-G^*,\bar{g^t}>}_{P_3}+\underbrace{(\eta^t)^2||\bar{g^t}||^2}_{P_4}.
%     \end{split}
% \end{equation}

% Focusing on the last term in the above equation, according to Assumption \ref{assumption3}, we have 
% \begin{equation*}
% \begin{split}
%     \mathbb{E}P_4
%  & =\mathbb{E}[(\eta^t)^2||\bar{g^t}||^2] \\
%  & \leq (\eta^t)^2\sum\limits_{i\in C_b^t}p_i^2\mathbb{E}||\nabla F_i(w_i^t)||^2 \\
%  & \leq (\eta^t)^2 U^2 .  
% \end{split}
% \end{equation*}

% Consider $P_3$, it follows:
% \begin{equation}
%     \begin{split}\label{ineq3}
% 	P_3
% 	& = -2\eta^t<G^t-G^*,\bar{g^t}> \\
% 	& = -2\eta^t\sum\limits_{i\in C_b^t}p_i<G^t-w_i^t,\nabla F_i(w_i^t)>\\
% 	& -2\eta^t\sum\limits_{i\in C_b^t}p_i<w_i^t-G^*,\nabla F_i(w_i^t)>.
%     \end{split} 
% \end{equation}

% It is well known that $-2ab\leq a^2+b^2$, so
% \begin{equation}
%     \begin{split}\label{ineq4}
%     & -2<G^t-w_i^t,\nabla F_i(w_i^t)>\\ 
%     & \leq ||G^t-w_i^t||^2+||\nabla F_i(w_i^t)||^2 .
%     \end{split}
% \end{equation}

% According to Assumption \ref{assumption2}, it follows:
% \begin{equation}
%     \begin{split}\label{ineq5}
%     & -<w_i^t-G^*,\nabla F_i(w_i^t)> \\
%     & \leq -(F_i(w_i^t)-F_i(G^*)) -\frac{\mu}{2}||w_i^t-G^*||^2 .
%     \end{split}
% \end{equation}

% %由 (5)，(6)，(7)，(8)，可得
% Use Equation~\ref{ineq2} and Inequalities~\ref{ineq3},~\ref{ineq4},~\ref{ineq5}, we obtain the following formula
% \begin{equation*}
%     \begin{split}
% 	   P_1  
%         & = ||G^t-G^*-\eta^t \bar{g^t}||^2 \\
% 	& \leq ||G^t-G^*||^2+(\eta^t)^2||\nabla F_i(w_i^t)||^2 \\
%         & +\eta^t\sum\limits_{i \in C_b^t}p_i(||G^t-w_i^t||^2
%          +||\nabla F_i(w_i^t)||^2) \\
% 	& - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*)+\frac{\mu}{2}||w_i^t-G^*||^2)\\
% 	& \leq (1-\eta^t\mu)||G^t-G^*||^2+((\eta^t)^2+\eta^t)||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2 \\
%         & \underbrace{- 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*))}_{P_5}.
%     \end{split} 
% \end{equation*}
% Motivated by ~\cite{li2019convergence}, we define $\Gamma=F^*-\sum\limits_{i\in C_b^t}p_iF_i^*$. $\Gamma$ is used to measure the degree of heterogeneity between the local models and the global model, in i.i.d data distributions, $\mathbb{E}\Gamma=0$. We have 
% %计算p5
% \begin{equation*}
%     \begin{split}
% 	P_5 
% 	& = - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*))\\
% 	& = - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i^*+F_i^*-F_i(G^*))\\
% 	& \leq  2\eta^t\sum\limits_{i \in C_b^t}p_i(F^*-F_i^*)=2\eta^t \Gamma, 
%     \end{split} 
% \end{equation*}
% Hence,
% \begin{equation*}
%     \begin{split}
% 	P_1
% 	& \leq (1-\eta^t\mu)||G^t-G^*||^2+((\eta^t)^2+\eta^t)||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2 +2\eta^t \Gamma.
%     \end{split} 
% \end{equation*}

% Utilize the above results, we have
% %从而可得
% \begin{equation}
%     \begin{split}\label{ineq6}
% 	\mathbb{E}|G^{t+1}-G^*||^2
% 	& \leq (1-\eta^t\mu)\mathbb{E}||G^t-G^*||^2 \\
%         & + ((\eta^t)^2+\eta^t)\mathbb{E}||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i\mathbb{E}||G^t-w_i^t||^2 +2\eta^t \Gamma \\
%         & + (\eta^t)^2\mathbb{E}||\bar{g^t}-g^t||^2.
%     \end{split}
% \end{equation}
% %至此，Part1 证明完毕

% %Part 2
% %利用Assumption 3证明有界
% According to Assumption \ref{assumption3}, it follows:
% \begin{equation}
%     \begin{split}\label{ineq7}
% 	\mathbb{E}||g^t-\bar{g^t}||^2
% 	& = \mathbb{E}||\sum\limits_{i \in C_b^t}p_i\nabla F_i(w_i^t,\xi_i^t)-\nabla F_i(w_i^t)||^2 \\
% 	& \leq \sum\limits_{i \in C_b^t}p_i^2 (\mathbb{E}||\nabla F_i(w_i^t,\xi_i^t)||^2 \\
%         & +\mathbb{E}||\nabla F_i(w_i^t)||^2) \\
% 	&\leq 2\sum\limits_{i \in C_b^t}p_i^2 U^2.
%     \end{split} 
% \end{equation}
% %至此，Part2 证明完毕


% %Part 3
% %利用Assumption 4证明有界
% According to Assumption \ref{assumption4}, it follows:
% \begin{equation}\label{ineq8}
%     \begin{split}
% 	\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2
% 	& = \sum\limits_{i \in C_b^t}p_i||\sum\limits_{i \in C_b^t}p_i w_i^t-w_i^t||^2 \\
% 	& \leq \sum\limits_{i \in C_b^t}p_i || w_i^t||^2 \\
% 	& \leq M^2.
%     \end{split}
% \end{equation}
% %至此part 3证明完毕

% %Part 4
% %现在由前三部分的结论来证明最终结论。
% So far, we have all the preparations ready to prove the final conclusion. Let  $Z_t=\mathbb{E}||G^t-G^*||^2$, $\eta^t=\frac{\theta}{t+\epsilon}$, $\epsilon>0$, $\theta > \frac{1}{\mu}$, $\lambda=\max\{\frac{\theta A}{\theta \mu -1}, (\epsilon+1)Z_1\}$, our goal of proving $Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}$ can be achieved as follows.
% \newline
% For $t=1$, it holds. Suppose that the conclusion establishes for some t and use Inequalities~\ref{ineq6},~\ref{ineq7},~\ref{ineq8}, we have $Z_{t+1}$ as follows: %Note $A=4U^2+M^2+2\Gamma$, use (\ref{ineq6}), (\ref{ineq7}), (\ref{ineq8}), it follows
% \begin{equation}
%     \begin{split}
%         Z_{t+1}
% 	& \leq (1-\eta^t\mu) Z_{t}+((\eta^t)^2+\eta^t)U^2 + \eta^t M^2 \\
%         &  + 2 (\eta^t)^2 \sum\limits_{i \in C_b^t}p_i^2 U^2+2\eta^t \Gamma \\
% 	& \leq (1-\eta^t\mu)Z_{t} + \eta^t A \\
% 	& = \frac{(t+\epsilon)^{\frac{1}{2}}-1}{(t+\epsilon)}\lambda+(\frac{\theta A}{t+\epsilon}-\frac{\theta \mu -1}{t+\epsilon}\lambda) \\
% 	& \leq \frac{\lambda}{(t+\epsilon +1)^{\frac{1}{2}}},
%     \end{split}
% \end{equation}
% where $A=4U^2+M^2+2\Gamma$.
% %t+1t+1时刻成立，于是Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}成立。
% %利用Assumption **1**，从而有
% Then, from Assumption \ref{assumption1}, we get
% \begin{equation}
%     \begin{split}
%         \mathbb{E}[F(G^t)]-F^*
% 	\leq \frac{L}{2} Z_t 
% 	\leq \frac{L}{2}\frac{\lambda}{(t+\epsilon )^{\frac{1}{2}}}
% 	\stackrel{t \to \infty}{\longrightarrow}0.
%     \end{split}
% \end{equation}
% \end{proof}






% \section{Convergence analysis}

%  We analyze the convergence of FLD in this section.

% % \subsection{Notation and Assumptions}
% % Let $F_i$ denotes the local model of the $i$-th client, $i=1,2,\cdots,N$. Let $F$ denotes the global model in the central parameter server. Suppose our models satisfy Lipschitz continuous gradient, we make Assumptions \ref{assumption1} and \ref{assumption2}. 


% % \begin{assumption}\label{assumption1}
% % ($L$-smooth). $F_1,\cdots,F_N$ are all $L$-smooth: $\forall x,y, F_i(x)\leq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{L}{2}||x-y||_2^2$.
% % \end{assumption}
 
% % \begin{assumption}\label{assumption2}
% % ($\mu$-strongly convex). $F_1,\cdots,F_N$ are all $\mu$-strongly convex:  $\forall x,y, F_i(x)\geq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{\mu}{2}||x-y||_2^2$.
% % \end{assumption}
% % We also follow the assumption made by~\cite{stich2018sparsified,yu2019parallel,li2019convergence} as follows.
% % \begin{assumption}\label{assumption3}
% % The expected squared norm of stochastic gradients is uniformly bounded, i.e., $\exists U>0$, $\mathbb{E}||\nabla F_i(\cdot)||^2 \leq U^2$ for all $i=1,\cdots,N$.
% % \end{assumption}
% % We make Assumption \ref{assumption4} to bound the expectation of $||w_i^t||^2$, where $w_i^t$ denotes the parameters of $F_i$ in $t$-round.
% % \begin{assumption}\label{assumption4}
% % (Bounding the expectation of $|| w_i^t ||^2$). The expected squared norm of $i$-th client's local model parameters is bounded: $\exists M>0$, $\mathbb{E}||w_i^t||^2 \leq M^2$ for all $i=1,\cdots,N$ and $t=1,\cdots,T$.
% % \end{assumption}

% % \subsection{Theorem and Proof}

% %提出收敛性理论, FLD收敛
% \newtheorem{thm}{Theorem}
% \begin{thm}\label{thm1}
% Let Assumptions \ref{assumption1} to \ref{assumption4} hold and $L$, $\mu$, $U$, $M$ be defined therein. Choose the learning rate $\eta^t=\frac{\theta}{t+\epsilon}$, $ \epsilon>0$, $\theta > \frac{1}{\mu}$, we define $\lambda=\max\{\frac{\theta A}{\theta \mu -1}, (\epsilon+1)Z_1\}$. Then FLD satisfies 
% \begin{equation}
%     \begin{split}
%         \mathbb{E}[F(G^t)]-F^*
% 	\leq \frac{L}{2} Z_t 
% 	\leq \frac{L}{2}\frac{\lambda}{(t+\epsilon )^{\frac{1}{2}}}
% 	\stackrel{t \to \infty}{\longrightarrow}0,
%     \end{split}
% \end{equation}
% where 
% \begin{equation}
%     \begin{split}
%         & A=4U^2+M^2+2\Gamma, \\
%         & Z_t=\mathbb{E}||G^t-G^*||^2.
%     \end{split}
% \end{equation}
% \end{thm} 


% \begin{proof}
% %以下是证明过程
% %第一部分证明\mathbb{E}||G^{t+1}-G^*||^2\mathbb{E}||G^{t+1}-G^*||^2有上界，其中G^{t+1}是第t+1轮全局模型的权重，G^*是全局模型最优权重。
% Let $G^{t+1}$ denote global model's parameters in the central server in $(t+1)$-round and $G^*$ be the optimal parameters in the central server. Additionally, $g^t=\sum\limits_{i\in C_b^t}p_i\nabla F_i(w_i^t,\xi_i^t)$, where $g^t$ denotes the gradient updates uploaded by the clients in $t$-round and $p_i$ denotes the weight of the $i$-client's gradient during aggregation. $\bar{g^t}=\sum\limits_{i\in C_b^t}p_i\nabla F_i(w_i^t)$ and $G^{t+1}=G^t-\eta^t g^t$, where $C_b^t$ denotes the collection of benign clients chosen by FLD in $t$-round. Then, we have 
% \begin{equation}
%     \begin{split}\label{ineq1}
% 	||G^{t+1}-G^*||^2
% 	& = ||G^t-\eta^t g^t-G^*-\eta^t \bar{g^t} + \eta^t \bar{g^t}|| \\
% 	& = \underbrace{||G^t-G^*-\eta^t \bar{g^t}||^2}_{P_1} \\
%         & +\underbrace{2\eta^t<G^t-G^*-\eta^t \bar{g^t},\bar{g^t}-g^t>}_{P_2} \\
% 	& +(\eta^t)^2||\bar{g^t}-g^t||^2.
%     \end{split}
% \end{equation}

% Since $\mathbb{E}g^t=\bar{g^t}$, we see $\mathbb{E}P_2=0$. Now we split $P_1$ into three terms:   
% \begin{equation}
%     \begin{split}\label{ineq2}
% 	P_1
% 	& = ||G^t-G^*-\eta^t \bar{g^t}||^2 \\
% 	& = ||G^t-G^*||^2\underbrace{-2\eta^t<G^t-G^*,\bar{g^t}>}_{P_3}+\underbrace{(\eta^t)^2||\bar{g^t}||^2}_{P_4}.
%     \end{split}
% \end{equation}

% Focusing on the last term in the above equation, according to Assumption \ref{assumption3}, we have 
% \begin{equation*}
% \begin{split}
%     \mathbb{E}P_4
%  & =\mathbb{E}[(\eta^t)^2||\bar{g^t}||^2] \\
%  & \leq (\eta^t)^2\sum\limits_{i\in C_b^t}p_i^2\mathbb{E}||\nabla F_i(w_i^t)||^2 \\
%  & \leq (\eta^t)^2 U^2 .  
% \end{split}
% \end{equation*}

% Consider $P_3$, it follows:
% \begin{equation}
%     \begin{split}\label{ineq3}
% 	P_3
% 	& = -2\eta^t<G^t-G^*,\bar{g^t}> \\
% 	& = -2\eta^t\sum\limits_{i\in C_b^t}p_i<G^t-w_i^t,\nabla F_i(w_i^t)>\\
% 	& -2\eta^t\sum\limits_{i\in C_b^t}p_i<w_i^t-G^*,\nabla F_i(w_i^t)>.
%     \end{split} 
% \end{equation}

% It is well known that $-2ab\leq a^2+b^2$, so
% \begin{equation}
%     \begin{split}\label{ineq4}
%     & -2<G^t-w_i^t,\nabla F_i(w_i^t)>\\ 
%     & \leq ||G^t-w_i^t||^2+||\nabla F_i(w_i^t)||^2 .
%     \end{split}
% \end{equation}

% According to Assumption \ref{assumption2}, it follows:
% \begin{equation}
%     \begin{split}\label{ineq5}
%     & -<w_i^t-G^*,\nabla F_i(w_i^t)> \\
%     & \leq -(F_i(w_i^t)-F_i(G^*)) -\frac{\mu}{2}||w_i^t-G^*||^2 .
%     \end{split}
% \end{equation}

% %由 (5)，(6)，(7)，(8)，可得
% Use (\ref{ineq2}), (\ref{ineq3}), (\ref{ineq4}), (\ref{ineq5}), we obtain the following formula
% \begin{equation*}
%     \begin{split}
% 	   P_1  
%         & = ||G^t-G^*-\eta^t \bar{g^t}||^2 \\
% 	& \leq ||G^t-G^*||^2+(\eta^t)^2||\nabla F_i(w_i^t)||^2 \\
%         & +\eta^t\sum\limits_{i \in C_b^t}p_i(||G^t-w_i^t||^2
%          +||\nabla F_i(w_i^t)||^2) \\
% 	& - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*)+\frac{\mu}{2}||w_i^t-G^*||^2)\\
% 	& \leq (1-\eta^t\mu)||G^t-G^*||^2+((\eta^t)^2+\eta^t)||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2 \\
%         & \underbrace{- 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*))}_{P_5}.
%     \end{split} 
% \end{equation*}
% Motivated by ~\cite{li2019convergence}, we define $\Gamma=F^*-\sum\limits_{i\in C_b^t}p_iF_i^*$. $\Gamma$ is used to measure the degree of heterogeneity between the local models and the global model, in i.i.d data distributions, $\mathbb{E}\Gamma=0$. We have 
% %计算p5
% \begin{equation*}
%     \begin{split}
% 	P_5 
% 	& = - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i(G^*))\\
% 	& = - 2\eta^t\sum\limits_{i \in C_b^t}p_i(F_i(w_i^t)-F_i^*+F_i^*-F_i(G^*))\\
% 	& \leq  2\eta^t\sum\limits_{i \in C_b^t}p_i(F^*-F_i^*)=2\eta^t \Gamma, 
%     \end{split} 
% \end{equation*}
% Hence,
% \begin{equation*}
%     \begin{split}
% 	P_1
% 	& \leq (1-\eta^t\mu)||G^t-G^*||^2+((\eta^t)^2+\eta^t)||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2 +2\eta^t \Gamma.
%     \end{split} 
% \end{equation*}

% Utilize the above results, we have
% %从而可得
% \begin{equation}
%     \begin{split}\label{ineq6}
% 	\mathbb{E}|G^{t+1}-G^*||^2
% 	& \leq (1-\eta^t\mu)\mathbb{E}||G^t-G^*||^2 \\
%         & + ((\eta^t)^2+\eta^t)\mathbb{E}||\nabla F_i(w_i^t)||^2\\
% 	& + \eta^t\sum\limits_{i \in C_b^t}p_i\mathbb{E}||G^t-w_i^t||^2 +2\eta^t \Gamma \\
%         & + (\eta^t)^2\mathbb{E}||\bar{g^t}-g^t||^2.
%     \end{split}
% \end{equation}
% %至此，Part1 证明完毕

% %Part 2
% %利用Assumption 3证明有界
% According to Assumption \ref{assumption3}, it follows:
% \begin{equation}
%     \begin{split}\label{ineq7}
% 	\mathbb{E}||g^t-\bar{g^t}||^2
% 	& = \mathbb{E}||\sum\limits_{i \in C_b^t}p_i\nabla F_i(w_i^t,\xi_i^t)-\nabla F_i(w_i^t)||^2 \\
% 	& \leq \sum\limits_{i \in C_b^t}p_i^2 (\mathbb{E}||\nabla F_i(w_i^t,\xi_i^t)||^2 \\
%         & +\mathbb{E}||\nabla F_i(w_i^t)||^2) \\
% 	&\leq 2\sum\limits_{i \in C_b^t}p_i^2 U^2.
%     \end{split} 
% \end{equation}
% %至此，Part2 证明完毕


% %Part 3
% %利用Assumption 4证明有界
% According to Assumption \ref{assumption4}, it follows:
% \begin{equation}\label{ineq8}
%     \begin{split}
% 	\sum\limits_{i \in C_b^t}p_i||G^t-w_i^t||^2
% 	& = \sum\limits_{i \in C_b^t}p_i||\sum\limits_{i \in C_b^t}p_i w_i^t-w_i^t||^2 \\
% 	& \leq \sum\limits_{i \in C_b^t}p_i || w_i^t||^2 \\
% 	& \leq M^2.
%     \end{split}
% \end{equation}
% %至此part 3证明完毕

% %Part 4
% %现在由前三部分的结论来证明最终结论。
% So far, we have all the preparations ready to prove the final conclusion. Let  $Z_t=\mathbb{E}||G^t-G^*||^2$, $\eta^t=\frac{\theta}{t+\epsilon}$, $\epsilon>0$, $\theta > \frac{1}{\mu}$, $\lambda=\max\{\frac{\theta A}{\theta \mu -1}, (\epsilon+1)Z_1\}$, our goal of proving $Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}$ can be achieved as follows.
% \newline
% For $t=1$, it holds. Suppose that the conclusion establishes for some t and use Ineq.~\eqref{ineq6},~\eqref{ineq7},~\eqref{ineq8}, we have $Z_{t+1}$ as follows: %Note $A=4U^2+M^2+2\Gamma$, use (\ref{ineq6}), (\ref{ineq7}), (\ref{ineq8}), it follows
% \begin{equation}
%     \begin{split}
%         Z_{t+1}
% 	& \leq (1-\eta^t\mu) Z_{t}+((\eta^t)^2+\eta^t)U^2 + \eta^t M^2 \\
%         &  + 2 (\eta^t)^2 \sum\limits_{i \in C_b^t}p_i^2 U^2+2\eta^t \Gamma \\
% 	& \leq (1-\eta^t\mu)Z_{t} + \eta^t A \\
% 	& = \frac{(t+\epsilon)^{\frac{1}{2}}-1}{(t+\epsilon)}\lambda+(\frac{\theta A}{t+\epsilon}-\frac{\theta \mu -1}{t+\epsilon}\lambda) \\
% 	& \leq \frac{\lambda}{(t+\epsilon +1)^{\frac{1}{2}}},
%     \end{split}
% \end{equation}
% where $A=4U^2+M^2+2\Gamma$.
% %t+1t+1时刻成立，于是Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}Z_t \leq \frac{\lambda}{(t+\epsilon)^{\frac{1}{2}}}成立。
% %利用Assumption **1**，从而有
% Then, from Assumption \ref{assumption1}, we get
% \begin{equation}
%     \begin{split}
%         \mathbb{E}[F(G^t)]-F^*
% 	\leq \frac{L}{2} Z_t 
% 	\leq \frac{L}{2}\frac{\lambda}{(t+\epsilon )^{\frac{1}{2}}}
% 	\stackrel{t \to \infty}{\longrightarrow}0.
%     \end{split}
% \end{equation}


% \end{proof}

% \section{Datasets, Parameters, Backdoor Setup}
% %介绍数据集的划分以及学习率设置 待补充
% Reddit datasets are collected from distributed clients and therefore do not require manual division~\cite{howtobackdoor}. We use Dirichlet distribution to partition the image datasets (MNIST, CIFAR10, Tiny-imagenet). We set the default distribution hyperparameter as 0.5. Each client uses SGD as the optimizer and a default batch size of 64.FLD' $\mu $ is default as 3.

% For the semantic backdoor, we follow the experiment setup in~\cite{howtobackdoor}, that the trigger sentence is ``pasta from Astoria is'' and the target word is ``delicious''. For the pixel pattern backdoor, we set specific pixels (same with the ones selected by DBA) to white, and then modify the label of the sample with the trigger to backdoor label. The backdoor label is ``digit 2'' in MNIST, ``bird'' in CIFAR10, and ``bullfrog'' in Tiny-imagenet. The default $PMR$ for MNIST is $20/64$, $5/64$ for CIFAR, and $20/64$ for Tiny-imagenet, to be consistent with DBA~\cite{xie2020dba}.
\subsection{Hyperparameters}
Reddit datasets are collected from distributed clients and therefore do not require manual division~\cite{howtobackdoor}. We use Dirichlet distribution to partition the image datasets (MNIST, CIFAR10, Tiny-imagenet). The distribution hyperparameter is 0.5, 0.9, and 0.5 for MNIST, CIFAR10, and Tiny-imagenet. Each client uses SGD as the optimizer and a default batch size of 64. $\mu $ is set as 3 by default.
For the semantic backdoor, we follow the experimental setup used by~\cite{howtobackdoor}, where the trigger sentence is ``pasta from Astoria is'' and the target word is ``delicious''. After the model was trained for 5000 rounds with 100 randomly selected clients in each round, the adversary used 10 malicious clients to inject the backdoor.
For the pixel pattern backdoor, we set specific pixels (same as the ones selected by DBA) to white, and then modify the label of the sample with the trigger to backdoor label. The backdoor label is ``digit 2'' in MNIST, ``bird'' in CIFAR10, and ``bullfrog'' in Tiny-imagenet. The default $PMR$ is $20/64$ for MNIST, $10/64$ for CIFAR, and $20/64$ for Tiny-imagenet, to be consistent with DBA~\cite{xie2020dba}. All participants train the global model, 10 of which are selected in each round to submit local SGD updates for aggregation. The adversary used 2 malicious clients to inject a backdoor in  Attack A-M of constrain-and-scale, 4 malicious clients to inject a backdoor in Attack DBA, and 1 malicious client to inject a backdoor in  Attack A-S.

\subsection{Baselines}
\setlist[itemize]{leftmargin=*}
\begin{itemize}
    \item \textbf{FoolsGold} argues that in federated learning, malicious clients tend to upload updates with higher similarities than benign clients, since each benign client has a unique data distribution while malicious clients share the same target. FoolsGold leverages this assumption to adapt the learning rate of each client during each iteration. The objective is to preserve the learning rates of the clients uploading distinct gradient updates, while decreasing the learning rates of the clients that consistently contribute similar gradient updates.
    % exploits the fact that malicious clients have the same target, and therefore achieve backdoor defense by reducing the learning rate of similar update clients.
    \item \textbf{Robust Federated Aggregation (RFA)} replaces the weighted arithmetic mean with the geometric median for federated learning aggregation. Specifically, RFA aggregates the local model parameters by finding the point that minimizes the sum of the distances to all the other points, where the distance is measured using a suitable metric such as the Euclidean distance. This point is known as the geometric median, representing the ``center'' of the distribution of the local models.
    %RFA aggregates the model parameters for updates by replacing the traditional weighted arithmetic mean with the geometric median.
    \item \textbf{Differential Privacy (DP)} is a privacy technique designed to ensure that the output does not reveal individual data records of participants.
    DP can be applied to machine learning to protect the privacy of training data or model updates. DP-based backdoor defense mitigates the impact of poisoned model updates on the global model by adding random noise to the uploaded parameters during aggregation. The random noise dilutes the malicious information injected by the malicious participants to mitigate their impact on the final global model.
    %is a privacy technique designed to ensure that the output does not reveal individual data records of participants. In backdoor defense, the impact of potentially poisoned model updates on the aggregated global model is reduced by adding random noise to the parameters uploaded by the clients.
    \item \textbf{Krum} selects one of the $n$ local models that is similar to the others as the global model by calculating the Euclidean distance between two of the local models.
    \item \textbf{Trimmed Mean}, also known as truncated mean, is a statistical method for calculating the average of a dataset while eliminating outliers. To compute the trimmed mean, a certain percentage of the highest and lowest values are removed or trimmed, and the mean is then calculated based on the remaining values. Specifically, trimmed mean aggregates each model parameter independently. The server ranks the $j$-th parameters of the $n$ local models. The largest and smallest $k$ parameters are removed, and the average of the remaining $n-2k$ parameters are calculated as the $j$-th parameters of the global model.
    \item \textbf{Bulyan} first iteratively applies Krum to select the local models, then aggregates these local models using a variant of the trimmed mean. In other words, Bulyan is a combination of Krum and Trimmed Mean.
    \item \textbf{Flame} 
    employs the cosine distance metric to measure the dissimilarity between locally uploaded models from various clients. It leverages HDBSCAN density clustering to distinguish between benign and malicious clients. Subsequently, Flame applies a clipping operation to limit the extent of modifications to the local models and introduces noise into the global model.
    
    % \textcolor{blue}{\item \textbf{Clipping and Perturbing(CP)} utilizes the server to clip model parameters and isotropic Gaussian noise to implement backdoor defense.Specifically, in the clipping process, in round $t$, the server first performs aggregation to update the global model wt, and then clips the model parameters.
\end{itemize}


\subsection{Effectiveness of Layer Scoring}
To test the necessity and effectiveness of Layer Scoring, we compared the performance between FLD with and without Layer Scoring under different attacker's poisoned data rate~(PDR) on the CIFAR10 dataset. FLD without Layer Scoring uses the same approach as the existing defense methods, i.e., splicing the model weights directly and using COF and Anomaly Detection to detect anomalous models.

As shown in Figure~\ref{fig:Layer Scoring}, as the PDR increases, the BA of the FLD without Layer Scoring rises and then falls. This is due to the fact that higher PDR leads to faster backdoor injection. But, higher PDR also makes the attacks easier to detect. Hence, the impact of the attack increases in the beginning and then decreases as the defense algorithm starts to identify the attack.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{fig/IID.pdf}
    \caption{Impact of  data distribution}
    \label{fig:alpha}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{fig/LOS.pdf}
    \caption{Impact of Layer Scoring}
    \label{fig:Layer Scoring}
  \end{subfigure}
  \caption{Comparison of different impacts}
  \label{fig:comparison}
\end{figure}


\section{Effectiveness of FLD against \textit{Attack A-S} }
In Section~Experiments, we focus on the more insidious and difficult-to-detect \textit{Attack A-M}. Here we evaluate the effectiveness of FLD against \textit{Attack A-S}. Table~\ref{tab:as} shows that FLD is effective for all 4 datasets on \textit{Attack A-S} of constrain-and-scale. 

\begin{table}[htbp]
  \centering
  \caption{Effectiveness of FLD in comparison to state-of-the-art defenses for constrain-and-scale  \textit{Attack A-S}, in terms of Backdoor Accuracy (BA) and Main Task Accuracy (MA). All values are percentages.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{Defenses}} & \multicolumn{2}{c|}{Reddit} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c}{Tiny-Imagenet} \bigstrut\\
\cline{2-9}          & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c}{MA} \bigstrut\\
    \hline
    No attack &       & 19.38  &       & 99.06  &       & 89.60  &       & 25.34  \bigstrut\\
\cline{1-1}    No defense & 100.00  & 19.35  & 70.01  & 48.03  & 78.86  & 60.60  & 99.19  & 20.84  \bigstrut\\
    \hline
    \hline
    FoolsGold & 100.00  & 19.35  & 70.06  & 48.04  & 53.47  & 77.31  & 99.28  & 21.02  \bigstrut[t]\\
    DP    & 100.00  & 19.35  & 70.01  & 47.88  & 78.07  & 61.11  & 99.31  & 20.58  \\
    RFA & 100.00  & 19.40  & \textbf{0.01}  & 98.78  & \textbf{0.00} & \textbf{89.28}  & \textbf{0.00}  & 25.41  \\
    Trimmed Mean & \textbf{0.00}  & 19.40  & 0.08  & 98.73  & \textbf{0.00}  & 88.57  & 0.21  & \textbf{25.61}  \\
    Krum  &    \textbf{0.00}   &   14.51    & 0.02  & \textbf{98.85}  & \textbf{0.00}  & 44.50  & \textbf{0.00}  & 7.76  \\
    Bulyan & \textbf{0.00}  & 19.39  & 0.13  & 98.67  & \textbf{0.00}  & 88.75  & \textbf{0.00}  & 25.47  \bigstrut[b]\\
    \hline
    \textbf{FLD} & \textbf{0.00}  & \textbf{19.43}  & 0.04  & \textbf{98.85}  & \textbf{0.00}  & 89.26  & \textbf{0.00}  & 25.34  \bigstrut\\
    \hline
    \end{tabular}%
    }
  \label{tab:as}%
\end{table}%



% %测试a s 攻击
% In Section~5, we focus on the more insidious and difficult-to-detect \textit{Attack A-M}. Here we evaluate the effectiveness of FLD against \textit{Attack A-S}. Table~\ref{tab:as} shows that FLD is effective for all 4 datasets on \textit{Attack A-S} of constrain-and-scale. %The results show that FLD, like other SOTA defense methods, enables an effective defense against \textit{Attack A-S}.
% \begin{table}[htbp]
%   \centering
%   \caption{Effectiveness of FLD in comparison to state-of-the-art defenses for  constrain-and-scale  \textit{Attack A-S}, in terms of Backdoor Accuracy (BA) and Main Task Accuracy (MA). All values are percentages.}
%   \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|l|cc|cc|cc|cc|}
%     \hline
%     \multicolumn{1}{|c|}{\multirow{2}[4]{*}{Defenses}} & \multicolumn{2}{c|}{Reddit} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c|}{Tiny-Imagenet} \bigstrut\\
% \cline{2-9}          & \multicolumn{1}{l|}{BA} & \multicolumn{1}{l|}{MA} & \multicolumn{1}{l|}{BA} & \multicolumn{1}{l|}{MA} & \multicolumn{1}{l|}{BA} & \multicolumn{1}{l|}{MA} & \multicolumn{1}{l|}{BA} & \multicolumn{1}{l|}{MA} \bigstrut\\
%     \hline
%     No attack &       & 19.38  &       & 99.06  &       & 89.60  &       & 25.34  \bigstrut\\
% \cline{1-1}    No defense & 100.00  & 19.35  & 70.01  & 48.03  & 78.86  & 60.60  & 99.19  & 20.84  \bigstrut\\
%     \hline
%     \hline
%     FoolsGold & 100.00  & 19.35  & 70.06  & 48.04  & 53.47  & 77.31  & 99.28  & 21.02  \bigstrut[t]\\
%     DP    & 100.00  & 19.35  & 70.01  & 47.88  & 78.07  & 61.11  & 99.31  & 20.58  \\
%     RFA & 100.00  & 19.40  & \textbf{0.01}  & 98.78  & \textbf{0.00} & \textbf{89.28}  & \textbf{0.00}  & 25.41  \\
%     Trimmed Mean & \textbf{0.00}  & 19.40  & 0.08  & 98.73  & \textbf{0.00}  & 88.57  & 0.21  & \textbf{25.61}  \\
%     Krum  &    \textbf{0.00}   &   14.51    & 0.02  & \textbf{98.85}  & \textbf{0.00}  & 44.50  & \textbf{0.00}  & 7.76  \\
%     Bulyan & \textbf{0.00}  & 19.39  & 0.13  & 98.67  & \textbf{0.00}  & 88.75  & \textbf{0.00}  & 25.47  \bigstrut[b]\\
%     \hline
%     \textbf{FLD} & \textbf{0.00}  & \textbf{19.43}  & 0.04  & \textbf{98.85}  & \textbf{0.00}  & 89.26  & \textbf{0.00}  & 25.34  \bigstrut\\
%     \hline
%     \end{tabular}%
%     }
%   \label{tab:as}%
% \end{table}%
% 这里补充同态加密下的情况
%正常联邦学习同态加密的架构
%先前相关工作采用的架构  shieldfl: mitigating model poisoning attacks in privacy-preserving federated learning.  Privacy-preserving byzantine-robust fed-erated learning via blockchain systems
% 服务器乘与一个r，然后传给服务提供方，服务提供方进行解密操作，然后证明这个r不会影响结果

%服务器乘于一个随机数r，然后让上层计算1
%
% \textcolor{blue}{
% \section{
% Private FLD}
% In addition to backdoor attacks, many attacks on federated learning have been continuously proposed.Such as membership inference attack, attribute inference attack. These attacks all demonstrate the necessity of enhancing the privacy protection of FL and prohibiting access to local model plaintext updates.In general, there are two approaches to protect the privacy of customer data: differential privacy and encryption techniques such as homomorphic encryption\cite{} or multi-party secure computation\cite{}. Differential privacy is a
% DP is a statistical method, which is simple to implement, but it will lead to a decrease in the performance of the model, while encryption provides strong privacy guarantees as well as privacy protection, but at the cost of reduced efficiency.
% %%再介绍Paillier同态加密
% Homomorphic encryption is a cryptographic primitive that allows computations to be performed on encrypted data without revealing the underlying plaintext. The basic idea is to encrypt the plaintext first to obtain the ciphertext. Then continue the calculation operation on the ciphertext, decrypt the final ciphertext result to obtain the plaintext, and the result is consistent with the calculation on the plaintext.For example, take the Paillier cryptosystem, the most commonly used homomorphic encryption algorithm in federated learning. It is a representative additive homomorphic encryption and has the following two homomorphic properties:
% \setlist[itemize]{leftmargin=*}
% \begin{itemize}
% \item \textbf{Homomorphic addition of plaintexts}: $E\left ( x_{1}\right )\cdot E\left ( x_{2}\right )= E\left ( x_{1}+x_{2}\right )$ where $ x_{1}$ and $ x_{20}$ represent  plaintexts,$E()$ 
% is an encryption operation.
% \item \textbf{Homomorphic multiplication of plaintexts}:$E\left ( x\right )^{r}= E\left ( r\cdot x\right ) 
% $where  $ x$  represent  plaintext,$E()$ 
% is an encryption operation,$r$ is  a constant.
% \end{itemize}
% Here we illustrate the applicability of FLD in federated learning homomorphic encryption scenarios
% First we follow the setup .
% Similarly, we have the following algorithm
% \begin{algorithm}
%     \caption{Private FLD}\label{algorithm4}
%     \begin{algorithmic} [1]
%         \algrenewcommand\algorithmicrequire{\textbf{Input:}}
%         \Require: The set of Layer Scoring from each client $ i\in C_{t}$ are regarded as $S_{i}$
%         \algrenewcommand\algorithmicrequire{\textbf{Output:}} 
%         \Require  The benign clients set $C_b^t$
%         \algrenewcommand\algorithmicrequire{\textbf{PS:}}
%         \Require
%         \State  Randomly select n nonzero integer $r_i $ for i in [1,n]
%         \For {$  i \ in \left [ 1,n \right ]$ } \Comment{$n$ is  }
%         \State $ c_{xi} \gets \llbracket{\omega_{xi} }\rrbracket\cdot \llbracket{r_{i} }\rrbracket  $
%         \EndFor
%         \State send $\left \{  c_{xi}  \right \} _{i=1}^{i=n} $ to CP
%     \end{algorithmic}
%     \begin{algorithmic}[1]
%     \algrenewcommand\algorithmicrequire{\textbf{CP:}}
%     \Require:
%     \For {$  i \ in \left [ 1,n \right ]$ } \Comment{$n$ is  }
%         \State  $ \omega _{xi}^{'} \gets Dec(sk_{c},c_{xi} ) $
%     \EndFor
%     \State $\left(S_{1} ,\cdots,S_{n}\right)\gets Layer Scoring\left(w_{1}^{'},\cdots,w_{n}^{'}\right)$
%     \State Send $\left(S_{1} ,\cdots,S_{n}\right)$ to PS
%     \end{algorithmic}
% \end{algorithm}
% %我们遵循Privacy-Enhanced Federated Learning Against Poisoning Adversaries的设置，算法描述如下，
% %参考文献的描述
% %我们需要可信的密钥生成中心（KGC）生成一对非对称密钥（pk c, sk c）云平台 (CP) 的 LHE，其中私钥sk c 仅由 CP 保存。同时，所有授权用户，持有LHE的同一对非对称密钥( pk x , sk x )由 KGC 生成。此外，在年初协议中，服务提供商（SP）随机初始化全局模型参数 ωini t 。
% %介绍cof异常算法为什么可以
% Correctness: In order to ensure that FLD can be effective
% To identify malicious gradients we need to prove that homomorphic encryption does not affect the calculation of COF anomaly detection.
% According to the properties of homomorphic encryption,we have 
% \begin{equation}
%     \begin{split}\label{hm}
% 	  c_{xi}
% 	& =  \llbracket{\omega_{xi} }\rrbracket\cdot \llbracket{r_{i} }\rrbracket \\
% 	& =  \llbracket{\omega_{xi} } + {r_{i} }\rrbracket .
%     \end{split}
% \end{equation}
% so $\omega _{xi}^{'} = \omega_{xi}  + r_{i} $,for $\omega _{x}^{'} $ and $\omega _{y}^{'} $ Euclidean distance is
%     \begin{equation}
%     \begin{split}\label{hm}
% 	  \left \| \omega _{x}^{'}-\omega _{y}^{'} \right \| 
%         & = \sqrt{\sum_{i=1}^{n}{\left( \omega _{xi}^{'}-\omega _{yi}^{'} \right)^{2} }}\\
% 	& =  \sqrt{\sum_{i=1}^{n}{\left( \omega_{xi}  + r_{i}- (\omega_{yi}  + r_{i}) \right)^{2} }} \\
%         & = \sqrt{\sum_{i=1}^{n}{\left( \omega _{xi}-\omega _{yi} \right)^{2} }}\\
%         & = \left \| \omega _{x}-\omega _{y} \right \|.
%     \end{split}
% \end{equation}
% }

