\section{Problem Setup}\label{sec:problem}
First, we describe the assumptions behind the convergence analysis of FLD, then we introduce the concepts that are vital to the algorithm design and analysis.

Let $F_i$ denotes the local model of the $i$-th client, $i=1,2,\cdots,N$. Let $F$ denotes the global model in the central parameter server. Suppose our models satisfy Lipschitz continuous gradient, we make Assumptions \ref{assumption1} and \ref{assumption2}.
\newtheorem{assumption}{Assumption}
\begin{assumption}\label{assumption1}
($L$-smooth). $F_1, \cdots,F_N$ are all $L$-smooth: 
\begin{equation*}
    \forall x, y,\quad F_i(x)\leq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{L}{2}||x-y||_2^2.
\end{equation*}
\end{assumption}
 
\begin{assumption}\label{assumption2}
($\mu$-strongly convex). $F_1,\cdots,F_N$ are all $\mu$-strongly convex:
\begin{equation*}
    \forall x, y,\quad F_i(x)\geq F_i(y)+(x-y)^{\mathsf{T}}\nabla F_i(y)+\frac{\mu}{2}||x-y||_2^2.
\end{equation*}
\end{assumption}
We also follow the assumption made by~\cite{stich2018sparsified,yu2019parallel,li2019convergence} as follows.
\begin{assumption}\label{assumption3}
The expected squared norm of stochastic gradients is uniformly bounded, i.e., $\exists U>0$, $\mathbb{E}||\nabla F_i(\cdot)||^2 \leq U^2$ for all $i=1,\cdots,N$.
\end{assumption}
We make Assumption \ref{assumption4} to bound the expectation of $||w_i^t||^2$, where $w_i^t$ denotes the parameters of $F_i$ in $t$-round.
\begin{assumption}\label{assumption4}
(Bounding the expectation of $|| w_i^t ||^2$). The expected squared norm of $i$-th client's local model parameters is bounded: $\exists M>0$, $\mathbb{E}||w_i^t||^2 \leq M^2$ for all $i=1,\cdots,N$ and $t=1,\cdots,T$.
\end{assumption}

\noindent\textbf{Adversary Capability.} Following previous works~\cite{howtobackdoor,krum,munoz2019byzantine,nguyen2020poisoning}, we assume the attacker controls a portion (less than 50\%) of the clients, called ``compromised clients''. We assume that the attacker can possess the strongest attack capability, i.e., both data poisoning and model poisoning. The attacker can send arbitrary gradient contributions to the aggregator in any iteration according to its observation of the global model state. Compromised clients can collude in an intrinsic and coordinated fashion by sharing states and updates with each other. The attacker has no control over the benign clients or the server's aggregation process.

\noindent\textbf{Adversarial Target.} To ensure the effectiveness of the attack, the attack should: 1) be stealthy, i.e., injecting the backdoor should not lead to accuracy fluctuation of the model's primary task 2) have a high attack success rate (ASR), i.e., the success rate that the model identifies the samples containing backdoor triggers as target label should be as high as possible.
%Let's consider an attacker who controls a part of the clients and whose aim is to inject  backdoor into the global model.

The adversarial objective of the compromised client $i$ in round $t$ can be denoted as follows:
\begin{equation}%加*表示不对公式编号
\begin{split}
w_{i}^ {t} =&arg\max_{w_{i}} ( \sum _{j \in  D_ {i}^ {poi}}  P[  G^ {t+1} (R(  x_ {j}^ {i}  ,  \phi  ))=   y_{backdoor}  ]+\\  &\sum_{j \in  D_ {i}^ {cle}}   P[  G^ {t+1}  (  x_ {j}^ {i}  )=  y_ {j}^ {i}  ]).
\end{split}
\end{equation}
where $D_ {i}^ {poi}$ and $D_ {i}^ {cle}$ denote the poisoned dataset and clean dataset respectively, $D_ {i}^ {poi}\cap D_ {i}^ {cle}=\emptyset $ and $D_ {i}^ {poi}\cup  D_ {i}^ {cle}=D_{i} $. Function $R$ transforms clean data $ x_ {j}^ {i}$ into poisoned data $R(  x_ {j}^ {i}, \phi)$ by embedding the trigger $\phi$. An attacker trains its local model to find the optimal parameters $w_{i}^ {t}$ so that $G^ {t+1}$ identifies poisoned data $R(  x_ {j}^ {i},  \phi  )$ as backdoor target label $y_{backdoor}$ and identifies clean data $ x_ {j}^ {i}$ as ground truth label $y_ {j}^ {i}$.

\noindent\textbf{Defense Goal}.Our defense goal is to mitigate backdoor attacks in federated learning. Specifically, an efficient defense algorithm needs to: 1) ensure the performance of the global model in terms of the main task's accuracy, 2) minimize the possibility of outputting backdoor target labels, and 3) defend against a wide range of SOTA federated backdoor attacks without prior knowledge of the proportion of compromised clients.
