\section{Experiments}
\subsection{Experiments Setup}
%第一个表格，不同数据集的情况 
\textbf{Datasets and Models.}
To evaluate the effectiveness of FLD, we tested two different federated learning use cases, namely word prediction used by~\cite{howtobackdoor}, and image classification used by DBA~\cite{xie2020dba}.
\setlist[itemize]{leftmargin=*}
\begin{itemize}
  \item \textbf{Word Prediction:} We followed the experiment setup of~\cite{howtobackdoor}. We use the Reddit public dataset from November 2017 and filter out users with fewer than 150 or more than 500 posts. We assume that every remaining Reddit user is a participant in federated learning and treat each post as a sentence in the training data and use a model consisting of two LSTM layers and a linear output layer.
  \item \textbf{Image Classification:} We conducted experiments on three classic image datasets: MNIST, CIFAR10, and Tiny-imagenet. MNIST is a classic handwritten digital image set containing 60,000 training examples and 10,000 test examples. CIFAR10 consists of 10 classes of 32x32 color images, including a total of 50,000 training examples and 10,000 test examples. The Tiny-imagenet~\cite{xie2020dba} consists of 200 classes each of which has 500 training images and 50 test images. To simulate the non-i.i.d environment, we divide the datasets using Dirichlet distribution, a commonly used prior distribution in Bayesian statistics~\cite{NONIID}.
\end{itemize}
% \textbf{Hyperparameters.}
% %介绍数据集的划分以及学习率设置 待补充
% Reddit datasets are collected from distributed clients and therefore do not require manual division~\cite{howtobackdoor}. We use Dirichlet distribution to partition the image datasets (MNIST, CIFAR10, Tiny-imagenet). The distribution hyperparameter is 0.5, 0.9, and 0.5 for MNIST, CIFAR10, and Tiny-imagenet. Each client uses SGD as the optimizer and a default batch size of 64. $\mu $ is set as 3 by default.
% For the semantic backdoor, we follow the experimental setup used by~\cite{howtobackdoor}, where the trigger sentence is ``pasta from Astoria is'' and the target word is ``delicious''. After the model was trained for 5000 rounds with 100 randomly selected clients in each round, the adversary used 10 malicious clients to inject the backdoor.
% For the pixel pattern backdoor, we set specific pixels (same as the ones selected by DBA) to white, and then modify the label of the sample with the trigger to backdoor label. The backdoor label is ``digit 2'' in MNIST, ``bird'' in CIFAR10, and ``bullfrog'' in Tiny-imagenet. The default $PMR$ is $20/64$ for MNIST, $10/64$ for CIFAR, and $20/64$ for Tiny-imagenet, to be consistent with DBA~\cite{xie2020dba}. All participants train the global model, 10 of which are selected in each round to submit local SGD updates for aggregation. The adversary used 2 malicious clients to inject a backdoor in  Attack A-M of constrain-and-scale, 4 malicious clients to inject a backdoor in Attack DBA, and 1 malicious client to inject a backdoor in  Attack A-S.



\begin{table}[htb]   
\begin{center}   
\caption{Dataset description}  
\label{table:dataset} 
\small 
\begin{tabular}{c|c|c}   
\hline   \textbf{Field} &\textbf{Datasets}   & \textbf{Model}\\  
\hline   NLP  & Reddit &   2-layer LSTM \\
\hline 
\multirow{3}*{Image Classification}  &MNIST   &  2 conv and 2 fc    \\ 
~  &  CIFAR10   &  lightweight Resnet-18 \\  
~  &  Tiny-imagenet   &  Resnet-18 \\     
\hline 
\end{tabular}   
\end{center}   
\end{table}


%Constrain-and-scale  DBA Edge-Case  PGD  Untargeted Poisoning  little is enough%

\noindent\textbf{Baselines.} We choose the following defense approaches as baselines: FoolsGold~\cite{foolgold}, Robust Federated Aggregation (RFA)~\cite{RFA}, Differential Privacy (DP)~\cite{canyou}, Krum~\cite{krum}, Trimmed Mean~\cite{Trimmed_Mean}, Bulyan~\cite{Bulyan}, and Flame~\cite{flame}. Please refer to the Appendix for details.



\noindent\textbf{Evaluation Metrics} ~
Based on the characteristics of federated learning and backdoor attacks, we consider the following metrics for evaluating the effectiveness of backdoor attacks and defense techniques.
\setlist[itemize]{leftmargin=*}
\begin{itemize}
\item \textbf{Backdoor Accuracy (BA)} refers to the model accuracy for backdoor tasks, where the attacker's target is to maximize BA, and an effective defense algorithm minimizes it.
\item \textbf{Main Accuracy (MA)} refers to the model accuracy for the primary task. Note that both attackers and defenders aim to minimize the attack's impact on MA.
\end{itemize}
\subsection{Results}
As shown in Table~\ref{tab:cs_am}, compared to SOTA defenses: FoolsGold, RFA, DP, Krum, Trimmed Mean, Bulyan and Flame, FLD is the most effective on all 4 datasets facing Attack A-M of constrain-and-scale~\cite{howtobackdoor}. On the MNIST dataset, RFA, Krum, Bulyan, Flame, and FLD perform fairly well, while FoolsGold, DP, and Trimmed Mean have worse performance with the Backdoor Accuracy getting close to 100\%. However, on CIFAR and Tiny-imagenet datasets, RFA and Bulyan also failed at defending against backdoors. Krum albeit successfully defended against the backdoor attack, performed significantly reduced MA. This is because CIFAR and Tiny-imagenet use ResNets neural networks, which are much more complex than the 2-layer convolutional networks deployed on MNIST. In the complex and larger models, the backdoor is hidden more deeply and thus difficult to detect or defuse with these model-granularity defensive methods. Flame has slightly better performance but is still outperformed by FLD. The reason mainly is that FLD is based on layer granularity and thus allows for fine-grained detection of anomalous models, resulting in the best defense performances on all datasets. Specifically, FLD presents 88.97\% MA on CIFAR, 25.26\% MA on Tiny-imagenet and 0.00\% BA on both datasets. In the NLP task, FLD effectively defended against backdoor attacks, with a BA of 0.00\% and MA of 19.26\% on Reddit dataset.

To further demonstrate the effectiveness of FLD, we tested FLD against several other defenses on the DBA attack, which breaks down global trigger patterns into separate local patterns and embeds them separately into the training sets of different adversaries. Compared to centralized backdoor attacks, DBA take better advantage of the distributed nature of federated learning and are therefore more stealthy and difficult to detect for federated learning.

As shown in Table~\ref{tab:dba}, RFA and Bulyan cannot effectively defend against DBA attack on MNIST, while Krum can effectively defend against the DBA attack but at the cost of a considerable drop on MA, with an accuracy of 54.12\% on CIFAR10 and 9.35\% on Tiny-imagenet. Flame achieves better defense effects on the MNIST and Tiny-imagenet, but worse performance on CIFAR10. In a word, SOTA defense methods either fail to effectively defend against DBA or endure a huge impact on the accuracy of the primary task, whereas FLD achieves effective defense on MNIST, CIFAR and Tiny-imagenet with backdoor success rates of only 0.03\%, 0.60\%, and 0.00\%, respectively, with negligible drops in MA, i.e., 0.15\%, 3.18\%, and 0.10\%.



\begin{table}[htbp]
  \centering
  \caption{Effectiveness of FLD in comparison to state-of-the-art defenses for constrain-and-scale attack, in terms of Backdoor Accuracy (BA) and primary task Accuracy (MA). All values are percentages.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{Defenses}} & \multicolumn{2}{|c|}{Reddit} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c}{Tiny-imagenet} \bigstrut\\
\cline{2-9}          & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c}{MA} \bigstrut\\
    \hline
    No attack &   -    &    19.38   &    -   & 99.06  &    -   & 89.60  &     -  & 25.34  \bigstrut\\
   No defense & 99.63  & 19.43  & 98.01  & 98.94  & 96.70  & 88.04  & 99.41  & 25.33  \bigstrut\\
    \hline
    \hline
    FoolsGold & 100.00  & 19.16  & 99.07  & 99.01  & 98.97  & 85.24  & 99.39  & 25.47  \bigstrut[t]\\
    DP    & 99.72  & \textbf{19.41}  & 98.12  & 98.91  & 98.36  & 85.74 & 99.68  & 25.75  \\
    RFA & 100.00  & 19.44  & 0.23  & 98.98  & 86.15  & 86.62  & 0.62  & 25.21  \\
    Trimmed Mean & \textbf{0.00}  & 19.21  & 96.56  & 98.96  & 96.61  & 88.45  & 99.31  & \textbf{25.79}  \\
    Krum  & 100.00  & 19.24  & 0.17  & 97.46  & 13.94  & 56.82  & \textbf{0.00}  & 9.04  \\
    Bulyan & \textbf{0.00}  & 19.24  & 0.85  & 99.08  & 91.38  & 88.55  & 99.01  & 25.49 \\
    Flame  & \textbf{0.00}  & 19.25  & \textbf{0.00}  & 98.46  & 7.02  & 88.85  & \textbf{0.00}  & 25.56  \bigstrut[b] \\
    \hline
    \textbf{FLD} & \textbf{0.00}  & 19.26  & \textbf{0.00}  & \textbf{99.09}  & \textbf{0.00}  & \textbf{88.97}  & \textbf{0.00}  & 25.29  \bigstrut\\
    \hline
    \end{tabular}%
    }
  \label{tab:cs_am}%
\end{table}%





\begin{table}[htbp]
  \centering
  \caption{Effectiveness of FLD in comparison to SOTA defense methods against DBA attack, in terms of Backdoor Accuracy (BA) and primary task Accuracy (MA). All values are percentages.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{Defenses}} &  \multicolumn{2}{|c|}{MNIST} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c}{Tiny-imagenet} \bigstrut\\
    % \multicolumn{1}{c|}{\multirow{2}[4]{*}{Defenses}} & \multicolumn{2}{c|}{MNIST} &       & \multicolumn{2|}{c}{CIFAR10} &       & \multicolumn{2}{c}{Tiny-imagenet} \bigstrut\\
\cline{2-7}          & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c}{MA} \bigstrut\\
    \hline
    No attack &     -  & 99.06  &     -  & 89.60  &     -  & 25.34  \bigstrut\\
    No defense & 99.84  & 98.94  & 97.27  & 85.55  & 99.28  & 25.64  \bigstrut\\
    \hline
    \hline
    FoolsGold & 99.81  & 98.91  & 97.27  & 84.66  & 99.34  & 25.62  \bigstrut[t]\\
    DP    & 99.59  & 98.23  & 97.46  & 83.87  & 99.58  & \textbf{25.89}  \\
    RFA & 99.83  & 98.88  & 94.17  & \textbf{87.44}  & 0.46  & 25.11  \\
    Trimmed Mean & 99.77  & 98.84  & 97.07  & 87.14  & 99.27  & 25.75  \\
    Krum  & 0.04  & 98.72  & \textbf{0.00}  & 54.12  & \textbf{0.00}  & 9.35  \\
    Bulyan & 99.83  & 98.98  & 95.83  & 86.91  & 98.82  & 25.06   \\
    Flame  & 0.15  & \textbf{98.99}  & 12.31  & 85.43  & 0.26  & 25.13  \bigstrut[b] \\
    \hline
    \textbf{FLD} &  \textbf{0.03}  & 98.91  & 0.60  & 86.42  & \textbf{0.00}  & 25.24  \bigstrut\\
    \hline
    \end{tabular}%
    }
  \label{tab:dba}%
\end{table}%

\noindent\textbf{Generalizability}~
To demonstrate the generalizability of FLD, we extend our evaluation to various backdoors such as constrain-and-scale, DBA, Edge-Case~\cite{tails}, Little Is Enough~\cite{littleisenough}, PGD~\cite{tails} and Flip attack~\cite{flip}, on CIFAR10. As summarized in Table~\ref{tab:attack}, FLD effectively mitigated all the attacks with negligible impact on MA. Therefore, FLD achieves robust performances in the face of various attacks and thus shows great generalizability.
It is worth clarifying that the target of Flip attack is to decrease the accuracy of specific labels, so the MA of Flip refers to the accuracy of the attacked class. In our experiments, we set the attacked class to be ``airplane'', and the attacker succeeded to decrease the prediction accuracy of ``airplane'' to 0.5\%. FLD effectively defended against Flip attack and present the prediction accuracy of ``airplane'' at 96.5\%.


\begin{table}[htbp]
  \centering
  \caption{FLD defends against SOTA attacks in terms of Backdoor Accuracy (BA) and primary task Accuracy (MA) on CIFAR10. All values are percentages.}
    \small 
    \begin{tabular}{l|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{2}[4]{*}{Attack}} & \multicolumn{2}{c|}{No Denfense } & \multicolumn{2}{c}{FLD} \bigstrut\\
\cline{2-5}          & \multicolumn{1}{c|}{BA} & MA    & \multicolumn{1}{c|}{BA} & MA \bigstrut\\
    \hline
    constrain-and-scale (A-M) & 96.70  & 88.04  & 0.00  & 88.97  \bigstrut\\
\cline{1-1}    DBA   & 97.27  & 85.55  & 0.60  & 86.42  \bigstrut\\
\cline{1-1}    Edge-Case & 76.02  & 88.79  & 7.14  & 88.11  \bigstrut\\
\cline{1-1}    Little Is Enough & 91.02  & 87.79  & 0.00  & 89.03  \bigstrut\\
\cline{1-1}    PGD   & 91.54  & 87.50  & 0.00  & 89.01  \bigstrut\\
\cline{1-1}    Neurotoxin   &    86.42   & 88.56  &    0.00   & 89.08  \bigstrut\\
    \hline
    \end{tabular}%
  \label{tab:attack}%
\end{table}%





\subsection{Ablation Tests}

\textbf{Proportion of Compromised Clients.} We use $PMR$ to represent the proportion of compromised clients, $PMR=\frac{k}{n}$, where $k$ represents the number of compromised clients and $n$ represents the number of all clients. We evaluate FLD for different $PMR$ values, i.e., 0, 0.1, 0.2, 0.3, 0.4. An effective backdoor defense  method should have minimum impact on the primary task in an environment with different proportions of attackers, also when no attackers exist. %Most previous works have ignored the no-attacker situation.

We followed the DBA~\cite{xie2020dba} setup and randomly selected 10 clients for each round of federated learning training, where 0, 1, 2, 3, 4 clients were malicious clients representing $PMR$ values 0, 0.1, 0.2, 0.3, 0.4, respectively. The results of the 10 rounds of the constrain-and-scale attack are shown in Figure~\ref{fig:pmr}. In CIFAR10, the success rates of BA after 10 rounds of attacks were 69.05\%, 82.24\%, 83.88\% and 85.54\%. Higher $PMR$ leads to a faster BA growth rate. FLD can successfully identify the poisoned clients in scenarios with $PMR$ of 0.1, 0.2, 0.3, and 0.4. On MNIST, when $PMR=0.1$, A-M attack needs more rounds to successfully inject the backdoor, hence the BA with No Defense is also 0. With $PMR$ of 0.2, 0.3, and 0.4, the backdoor was embedded smoothly. Nevertheless, FLD still presented a good defense against malicious clients. With $PMR=0$, FLD has a negligible effect on MA. Therefore, FLD is robust to different poisoning rates (proportion of compromised clients).
% \vspace{-1.85cm}
\begin{figure}[H]
% \setlength{\abovecaptionskip}{-1cm}
\setlength{\belowcaptionskip}{-6pt} 
\centering
\includegraphics[width=\linewidth]{fig/PMR.pdf}
\caption{Impact of PMR}
\label{fig:pmr}
\end{figure}
% \vspace{-10pt}
%插入数据异构的分布图以及不同数据分布下的实验效果折线图 0.1 0.5 1 10  100
\noindent\textbf{Degree of Data Heterogeneity}. Since FLD is based on detecting the difference between benign and backdoor models, the different data distribution among clients may affect its effectiveness. To test FLD under diverse data distribution assumptions, we vary the degree of data heterogeneity across clients by adjusting the parameter of Dirichlet distribution to 0.1, 0.5, 1, 10, and 100, on the CIFAR10 dataset. Figure~\ref{fig:data} showcases the data distribution of clients when the Dirichlet alpha is 0.1 and 100. It can be seen that when Dirichlet alpha = 0.1, the data distribution is extremely heterogeneous between clients, with each client having a different number of samples and labels. When Dirichlet alpha = 100, the data distribution is close to i.i.d.
%补充实验的结果 以及更新折线图
As shown in Fig.~\ref{fig:alpha} , MA decreases as the degree of data heterogeneity increases. FLD is effective in identifying and removing malicious clients in both extreme non-i.i.d and near-i.i.d scenarios.


\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{fig/image_marge.png}
\caption{A visualization  of client data distribution for Dirichlet alpha = 0.1 and Dirichlet alpha = 100.}
\label{fig:data}
\end{figure}


% \noindent \textbf{Effectiveness of Layer Scoring.}
% To test the necessity and effectiveness of Layer Scoring, we compared the performance between FLD with and without Layer Scoring under different attacker's poisoned data rate~(PDR) on the CIFAR10 dataset. FLD without Layer Scoring uses the same approach as the existing defense methods, i.e., splicing the model weights directly and using COF and Anomaly Detection to detect anomalous models.

% As shown in Figure~\ref{fig:Layer Scoring}, as the PDR increases, the BA of the FLD  without Layer Scoring rises and then falls. This is due to the fact that higher PDR leads to faster backdoor injection. But, higher PDR also makes the attacks easier to be detected. Hence, the impact of the attack increases in the beginning and then decreases as the defense algorithm starts to identify the attack.
% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/IID.pdf}
%     \caption{Impact of  data distribution}
%     \label{fig:alpha}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/LOS.pdf}
%     \caption{Impact of Layer Scoring}
%     \label{fig:Layer Scoring}
%   \end{subfigure}
%   \caption{Comparison of different impacts}
%   \label{fig:comparison}
% \end{figure}


% \noindent\textbf{Effectiveness of FLD against \textit{Attack A-S} }
% %测试a s 攻击
% In Section~5.3, we focus on the more insidious and difficult-to-detect \textit{Attack A-M}. Here we evaluate the effectiveness of FLD against \textit{Attack A-S}. Table~\ref{tab:as} shows that FLD is effective for all 4 datasets on \textit{Attack A-S} of constrain-and-scale. %The results show that FLD, like other SOTA defense methods, enables an effective defense against \textit{Attack A-S}.
% \begin{table}[htbp]
%   \centering
%   \caption{Effectiveness of FLD in comparison to state-of-the-art defenses for  constrain-and-scale  \textit{Attack A-S}, in terms of Backdoor Accuracy (BA) and Main Task Accuracy (MA). All values are percentages.}
%   \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|cc|cc|cc|cc}
%     \hline
%     \multicolumn{1}{c|}{\multirow{2}[4]{*}{Defenses}} & \multicolumn{2}{c|}{Reddit} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c}{Tiny-Imagenet} \bigstrut\\
% \cline{2-9}          & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c|}{BA} & \multicolumn{1}{c}{MA} \bigstrut\\
%     \hline
%     No attack &       & 19.38  &       & 99.06  &       & 89.60  &       & 25.34  \bigstrut\\
% \cline{1-1}    No defense & 100.00  & 19.35  & 70.01  & 48.03  & 78.86  & 60.60  & 99.19  & 20.84  \bigstrut\\
%     \hline
%     \hline
%     FoolsGold & 100.00  & 19.35  & 70.06  & 48.04  & 53.47  & 77.31  & 99.28  & 21.02  \bigstrut[t]\\
%     DP    & 100.00  & 19.35  & 70.01  & 47.88  & 78.07  & 61.11  & 99.31  & 20.58  \\
%     RFA & 100.00  & 19.40  & \textbf{0.01}  & 98.78  & \textbf{0.00} & \textbf{89.28}  & \textbf{0.00}  & 25.41  \\
%     Trimmed Mean & \textbf{0.00}  & 19.40  & 0.08  & 98.73  & \textbf{0.00}  & 88.57  & 0.21  & \textbf{25.61}  \\
%     Krum  &    \textbf{0.00}   &   14.51    & 0.02  & \textbf{98.85}  & \textbf{0.00}  & 44.50  & \textbf{0.00}  & 7.76  \\
%     Bulyan & \textbf{0.00}  & 19.39  & 0.13  & 98.67  & \textbf{0.00}  & 88.75  & \textbf{0.00}  & 25.47  \bigstrut[b]\\
%     \hline
%     \textbf{FLD} & \textbf{0.00}  & \textbf{19.43}  & 0.04  & \textbf{98.85}  & \textbf{0.00}  & 89.26  & \textbf{0.00}  & 25.34  \bigstrut\\
%     \hline
%     \end{tabular}%
%     }
%   \label{tab:as}%
% \end{table}%





