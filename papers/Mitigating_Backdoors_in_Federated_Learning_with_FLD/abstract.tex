\begin{abstract}
Federated learning allows clients to collaboratively train a global model without uploading raw data for privacy preservation. This feature, i.e., the inability to review participants' datasets, has recently been found responsible for federated learning's vulnerability in the face of backdoor attacks. Existing defense methods fall short from two perspectives: 1) they consider only very specific and limited attacker models and thus are unable to cope with advanced backdoor attacks, such as distributed backdoor attacks, which break down the global trigger into multiple distributed triggers.  2) they conduct detection based on model granularity thus their performance gets impacted by the model size. %3) they are unable to cope with advanced backdoor attacks, such as distributed backdoor attacks, which break down the global trigger into multiple distributed triggers. 

To address these challenges, we propose Federated Layer Detection (FLD), a novel model filtering approach to effectively defend against backdoor attacks. FLD examines the models on layer granularity to capture the complete model details and effectively detect potential backdoor models regardless of model size. We provide theoretical analysis and proof for the convergence of FLD. Extensive experiments demonstrate that FLD effectively mitigates state-of-the-art (SOTA) backdoor attacks with negligible impact on the accuracy of the primary task, outperforming SOTA defense methods.
\end{abstract} 