\section{Introduction}
%-------------------------------------------------------------------------------
%With the booming development of machine learning technology, artificial intelligence has been applied to a multitude of fields, such as image recognition, natural language processing, machine gaming, and so on, among which the great success of AlphaGo has attracted strong attention.With AlphaGo's success, people naturally hope that the big data-driven AI like AlphaGo will be realized soon in all aspects of our lives~\cite{yang2019federated}. But the real world is  cruel, the quality or quantity of data in most domains is not up to the standard of big data-driven, in general, the data required for big data-driven involves multiple dimensions, for example, recommendation system, the data required includes product information and user information, these data are mostly scattered in different companies, forming data silos. 

The rapidly developing artificial intelligence technologies have been applied to numerous fields, such as computer vision, natural language processing, data mining, and so on. %The trend toward big-data-driven services seems promising but has seen its bottleneck in recent years. 
Many AI services are supported by cloud services to collect big data from large numbers of distributed users. However, privacy protection regulations released in recent years, such as GDPR~\cite{GDPR}, CCPA~\cite{CCPA} and PDPA~\cite{PDPA}, have presented serious challenges for user data collection. Therefore, privacy-preserving data analysis technologies are critical. 

%To solve the big data-driven dilemma, traditional methods have hit a bottleneck. Federated learning (FL)~\cite{googlefederation} is a novel distributed learning method proposed by google in 2016, which enables multiple devices to jointly train a global model in a scenario where local data can not interact directly, thus solving the problems of data isolation and data privacy brought by traditional methods. Federated learning consists of many clients and a central parameter server, and its training process can be summarized as follows: the central parameter server distributes the global model to the clients, which use their local training data to train the model and then clients send the updated model parameters to the central parameter server. The central parameter server updates the global model by aggregating the local models from the clients participating in training.

Federated learning~\cite{googlefederation}, proposed by Google in 2016, allows distributed clients to jointly train a global model without uploading user private data and thus has attracted wide attention. In federated learning, a central parameter server sends an initial global model to the clients. The clients then train local models using local datasets and send the updated model parameters to the server after training. The server updates the global model via parameter aggregation and distributes the updated global model to the participants in the next round. The process iterates until the model is convergent or the predefined period ends.

%While federated learning enables multiple local devices to aggregate local models to train a better global model, as a trade-off, its distributed framework and restricted access to local data inadvertently provide new venues for attacks.Many recent research works have shown that federated learning is vulnerable to adversarial attacks, including data inference attack and data poisoning attacks on users' private training data. Data poisoning attacks on federated learning are generally classified into two categories according to the purpose of the attack, untargeted poisoning attacks and backdoor attacks. The purpose of untargeted poisoning attacks are to reduce the task accuracy of the global model, the server can discard the model by verifying the task accuracy, and the purpose of backdoor attacks is to embed the backdoor into the model without reducing the performance of the main task, which is more stealthy e.g. DBA~\cite{xie2020dba}, A Little Is Enough~\cite{littleisenough}. 

As a trade-off, federated learning limits the central server's access to users' data. However, this feature, as uncovered by recent studies, makes federated learning vulnerable to adversarial attacks, especially backdoor attacks~\cite{BadNets,Trojaning}. Backdoor attacks embed backdoor into the model without impacting the primary task performance, thus being much more stealthy than other attacks such as untargeted poisoning attacks~\cite{untarget}. %such as data poisoning attacks~\cite{howtobackdoor,xie2020dba,googlefederation,littleisenough}. Data poisoning attacks on federated learning are generally classified into two categories, namely untargeted poisoning attacks~\cite{untarget} and backdoor attacks~\cite{xie2020dba}. Untargeted poisoning attacks, which focus on reducing the global model's accuracy, can be detected and discarded via accuracy measurement. Backdoor attacks, which embed backdoors into the model without impacting the performance of the primary task, are more stealthy~\cite{BadNets,Trojaning}.

%Backdoor attacks are not unique to federated learning, there have been some related studies in this area of machine learning, such as BadNets~\cite{BadNets}, Trojan~\cite{Trojaning}, Blend~\cite{blend}. 
The defenses against centralized backdoor attacks fall into three broad categories: pre-training, during-training, and post-training~\cite{Backdoorsurvey}.~%training during, and after training~\cite{Backdoorsurvey}.\\ 
%\textbf{Pre-training}. Detect the training data, remove the potential poisoning data or pre-process the training data, and then start training the model using the processed data.e.g.Spectre~\cite{pre-training1},COSIN~\cite{pre-training2}.\\
%\textbf{Training during}. A backdoor-free model is trained using potentially poisoned data. The main means are adding differential privacy, data augmentation.e.g.DBD~\cite{trainingduring1},ABL~\cite{trainingduring2}.\\
%\textbf{After training}. Detects against models that may have embedded backdoors and eliminates the effect of their backdoor triggers.e.g.CLC~\cite{aftertraining1},ANP~\cite{aftertraining2}.
%
Pre-training requires pre-processing of training data and post-training requires model fine-tuning using users' datasets, both of which require access to local data and thus cannot be applied to federated learning. As a result, backdoor defenses for federated learning mostly use robustness aggregation or differential privacy perturbation during the training stage. However, the state-of-the-art~(SOTA) defense methods are either only applicable to independent and identically distributed~(i.i.d) datasets, or can only defend against conventional backdoor attacks but fail at defending SOTA attacks like DBA~\cite{xie2020dba} and A Little Is Enough~\cite{littleisenough}. %the central server of federated learning does not have access to the data of the training device, so the backdoor defense means of federated learning mainly focus on the training phase, which mainly fall into two categories: robustness aggregation and differential privacy perturbation.These defenses are either only applicable to iid scenarios,  or are too idealistic in their assumptions which only 
 %considers specific federated learning backdoor attacks a , not multiple state-of-the-art federated learning backdoor attacks, such as DBA~\cite{xie2020dba}, A Little Is Enough~\cite{littleisenough} etc.

To address these challenges, we propose Federated Layer Detection~(FLD), which measures the fine-grained layer-level differences across models. FLD consists of two major modules, namely Layer Scoring and Anomaly Detection, which can reliably identify and remove malicious clients to guarantee system performance.  Layer Scoring assigns an outlier score to each layer of the models uploaded by the clients based on the concept of \textit{density} instead of commonly used \textit{distance} which sometimes causes wrong detection results~\cite{cof}.  Anomaly Detection uses the median absolute deviation (MAD)~\cite{MAD} of layer scores to determine if a model is anomalous. FLD overcomes three major limitations of existing defense methods, namely, simplified assumptions of data distribution, degraded accuracy of the primary task, and only functioning against specific backdoor attacks. Our contributions are mainly threefold, as follows:
\setlist[itemize]{leftmargin=*}
\begin{itemize}
    \item We propose FLD, an innovative backdoor defense scheme for federated learning. FLD is the first fine-grained defense scheme that assesses models based on layer level, to our best knowledge. 
    \item Layer Scoring module captures the fine-grained model details to improve the generalizability of FLD to deeper models. Anomaly Detection employs MAD to avoid impactful mean shifts caused by extreme outlier scores of anomalous models. %(The breakdown point of the outlier scores mean is zero, because a outlier value shifted to infinity will make the sample mean tend to infinity). 
    As such, FLD can effectively detect potential backdoored models regardless of model size. 
    \item We theoretically prove the convergence guarantee of FLD in both i.i.d and non-i.i.d data distributions. We also prove the correctness of FLD in homomorphic encryption scenarios.
    \item Extensive experiments on several well-known datasets can show that FLD effectively defends against a wide range of SOTA federated backdoor attacks in different scenarios without compromising the accuracy of the primary task, demonstrating FLD's robustness and generalizability.
\end{itemize}
