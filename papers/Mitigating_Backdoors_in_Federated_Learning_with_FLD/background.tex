\section{RELATED WORK}
\subsection{Federated Learning}
Federated learning is a popular strategy for training models on distributed data while preserving client privacy. Assuming there are $N$ clients, $ C = \left \{ C_{1}, C_{2}, C_{3},\cdots, C_{N}  \right \} $, each of which has a local training dataset $D_{i}, i\in \{1,\cdots, N\}$, that can communicate with the central parameter server to collaboratively train a global model. The standard federated learning process generally follows these phases:
\setlist[enumerate]{leftmargin=*}
\begin{enumerate}
  \item  In round $t$, the central parameter server randomly selects $n$ clients that satisfy predefined conditions to participate in the training and sends these clients the latest global model $ G^{t-1}$.
%
  \item Each selected client $C_i$ executes $\tau _{i}$ iterations to update its local model: $w_{i}^{t}= w_{i}^{t-1}-\eta_{i}^t g_{i}\left (  w_{i}^{t-1};\xi_{i,k}^{t-1}  \right )$, where $\eta _{i}^t$ is the learning rate, $\xi_{i,k}^{t-1}$ is a batch of uniformly chosen data samples. After the local training, $C_i$ sends updated $w_{i}^{t}$ to the central parameter server. 
  %\item client $ C_{i} $ $ i\in [N] $ where $ [N]$ denotes the integer set $ \left \{ 1,2,3,...,N \right \} $ receive the global model $ G^{t-1}  $ .At this point the local model client will execute $\tau _{i}  $ local iterations to update the local model, represented formally as
  % $$
  %L_{i}^{s}= L_{i}^{s-1}-\eta _{i}g_{i}\left (  L_{i}^{s-1};\xi_{i}^{s-1}  \right ) 
  %$$
  \item The parameter server aggregates the received models to update the global model. We choose the classical FedAvg algorithm~\cite{fedavg} for aggregation: $ G^{t} = {\textstyle \sum_{i=1}^{n}} \frac{ m_{i} }{m} w_{i}^{t}$, where $m_{i} = \left \| D_{i}  \right \|,m= {\textstyle \sum_{i=1}^{n}m_{i} } $. Previous works~\cite{howtobackdoor,xie2020dba,krum} typically used the same weights $\left ( \frac{ m_{i} }{m} =\frac{1}{n}  \right ) $  to average client contributions. For simplicity, we follow this setup, i.e., $G^{t} = {\textstyle \sum_{i=1}^{n}} \frac{1}{n} w_{i}^{t}$.

\end{enumerate}


\subsection{Backdoor Attacks}
%\textbf{Backdoor Attack Overview} Backdoor attack: 
Backdoor attacks impact the model training with ``carefully crafted'' poisoned data (mixing triggers into a small portion of data) to get a backdoor-poisoned model. The corrupted model outputs original labels for clean samples but target labels for poisoned samples. Thus, it does not impact primary task performance and is difficult to detect. Besides conventional centralized backdoor attacks, distributed backdoor attacks have emerged in recent years. A formal description of the attacker's purpose is
% The attacker inserts ``carefully crafted'' poisoned data into the training process by inserting special backdoor triggers on a small number of training sets to influence the training so that the attacked model performs well on clean samples, while the output of samples with specific backdoor triggers is classified into the attacker's specified class. A formal description of the attacker's purpose is:
\begin{equation}
\forall \left ( x,y \right ) \in D, f\left (  G,x\right ) = y \wedge f\left (  G,x^{*} \right ) = y_{backdoor},
\end{equation}
where $D$ is the clean dateset, $ y_{backdoor} $ is the attacker's target label, $x^{*}$ is the clean sample $x$ combined with backdoor triggers.\\
% Backdoor Attack fig  tbd
\textbf{Backdoor Attacks on Federated Learning} can be divided into two categories: data poisoning and model poisoning~\cite{federatedopen}:
\setlist[itemize]{leftmargin=*}
\begin{itemize}
  \item \textbf{Data poisoning}: The attacker can only modify the dataset of the compromised clients but cannot control their training process or modify the data uploaded by the clients to the parameter server. The common methods are label flipping (e.g., making that picture of a cat labeled as a dog) and adding triggers to the image samples (e.g., adding a hat to the face images). To avoid being detected, the attackers often control the Poisoned Data Rate (PDR) to restrict the poisoned model's deviation from the benign model. Let $D_{i}$ denote the poisoned training dataset of the compromised client $i$ and $D_{i}^{poi}$ denote the poisoned data, then the PDR of $ D_{i} $ is
\begin{equation}
 PDR=\frac{\left | D_{i}^{poi} \right | }{\left | D_{i} \right | }.  
 \end{equation}
  % 介绍 Scaling 
  \item \textbf{Model poisoning} is more powerful than data poisoning because the attacker can manipulate the compromised client's training and directly modify their uploaded data to maximize the impact, driving the global model closer to the backdoor model without being noticed by the anomaly detection mechanism running in the parameter server. A classic model poisoning attack is the model-replacement attack~\cite{howtobackdoor} which scales the uploaded model parameters. 
%This attack is more powerful than Data Poisoning, where the attacker can not only poison the client's training dataset, but also arbitrarily modify the parameters and scale the generated model updates to maximize the impact of the attack and bring the global model closer to the backdoor model and, while avoiding the anomaly detection mechanism deployed on the parameter server as well as the robust aggregation mechanism. 
Model poisoning attacks can be divided into two types, namely scaling and evasion.
\textbf{Scaling} drives the global model close to the backdoor model by scaling up the weights of the uploaded model.
\textbf{Evasion} constrains model variation during training to reduce the malicious model's deviation from the benign model in order to avoid anomaly detection. A common method is to modify the target loss function by adding an anomaly detection term $L_{ano}$ as follows:
\begin{equation}
L_{model} = \alpha L_{class}+\left ( 1-\alpha  \right ) L_{ano},
\end{equation}
where $L_{class}$ captures the accuracy of the primary and backdoor tasks, the hyperparameter $\alpha$ controls the importance of evading anomaly detection. $L_{ano}$ functions as the anomaly detection penalty, e.g., the Euclidean distance between the local models and global model.
Model poisoning can directly control the training process and modify the model weights. Therefore, model poisoning can bypass the anomaly detection mechanism and robust aggregation mechanism deployed on the parameter server. It scales the model weights to satisfy the bound defined by the anomaly detection mechanism.
\end{itemize}
\subsection{Backdoor Defenses}
Works on defending against backdoor attacks in federated learning can be broadly categorized into two directions: robust aggregation and anomaly model detection. Robust aggregation optimizes the aggregation function to mitigate the impact of contaminated updates sent by attackers. One common approach is to apply a threshold to limit the impact of updates from all clients on the global model, such as by constraining the l2 norm of the updates (referred to as clipping)~\cite{howtobackdoor,foolgold,flame,l21}. Other approaches explore new global model estimators, like Robust Federated Averaging (RFA)~\cite{RFA} and Trimmed Mean~\cite{Trimmed_Mean}, to enhance the robustness of the aggregation process. However, a major drawback of these approaches is that contaminated updates may still persist in the global model, resulting in reduced model accuracy and incomplete mitigation of backdoor effects. Additionally, applying update constraints to all clients, including benign ones, reduces the magnitude of updates and thus slows down the convergence. The second direction, anomaly model detection, aims to identify and remove malicious updates from the aggregation process. This task is challenging due to the non-independent and heterogeneous distribution of client data and the uncertainty of the number of malicious clients. Previous methods have typically utilized clustering directly for detection.
%% 补充联邦学习隐私保护的相关工作
% \subsection{\textcolor{blue}{Privacy-preserving Federated Learning}}
% \textcolor{blue}{
% In addition to backdoor attacks, many attacks on federated learning have been continuously proposed.Such as membership inference attack, attribute inference attack. These attacks all demonstrate the necessity of enhancing the privacy protection of FL and prohibiting access to local model plaintext updates.In general, there are two approaches to protect the privacy of customer data: differential privacy and encryption techniques such as homomorphic encryption\cite{} or multi-party secure computation\cite{}. Differential privacy is a
% DP is a statistical method, which is simple to implement, but it will lead to a decrease in the performance of the model, while encryption provides strong privacy guarantees as well as privacy protection, but at the cost of reduced efficiency.
% %%再介绍Paillier同态加密
% Homomorphic encryption is a cryptographic primitive that allows computations to be performed on encrypted data without revealing the underlying plaintext. The basic idea is to encrypt the plaintext first to obtain the ciphertext. Then continue the calculation operation on the ciphertext, decrypt the final ciphertext result to obtain the plaintext, and the result is consistent with the calculation on the plaintext.For example, take the Paillier cryptosystem, the most commonly used homomorphic encryption algorithm in federated learning. It is a representative additive homomorphic encryption and has the following two homomorphic properties:
% \setlist[itemize]{leftmargin=*}
% \begin{itemize}
% \item \textbf{Homomorphic addition of plaintexts}: $E\left ( x_{1}\right )\cdot E\left ( x_{2}\right )= E\left ( x_{1}+x_{2}\right )$ where $ x_{1}$ and $ x_{20}$ represent  plaintexts,$E()$ 
% is an encryption operation.
% \item \textbf{Homomorphic multiplication of plaintexts}:$E\left ( x\right )^{r}= E\left ( r\cdot x\right ) 
% $where  $ x$  represent  plaintext,$E()$ 
% is an encryption operation,$r$ is  a constant.
% \end{itemize}
% }





