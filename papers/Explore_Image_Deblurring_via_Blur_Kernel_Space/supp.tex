% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage[numbers,sort&compress]{natbib}
% \usepackage[normalem]{ulem}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\pagenumbering{gobble}

\def\cvprPaperID{5198} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

% user's definitions
\newcommand{\Sref}[1]{Sec.~\ref{#1}}
\newcommand{\Eref}[1]{Eq.~(\ref{#1})}
\newcommand{\Fref}[1]{Fig.~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\Aref}[1]{Algorithm~\ref{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\cellimg}[1]{
    \includegraphics[width=4cm, height=4cm]{#1}
}
\newcommand{\cellbigimg}[1]{
    \includegraphics[width=8cm, height=8cm]{#1}
}
\newcommand{\cellimgsmall}[1]{
    \includegraphics[width=2.0cm, height=2.0cm]{#1}
}
\newcommand{\cellimgtiny}[1]{
    \includegraphics[width=2.0cm, height=2.0cm]{#1}
}
\newcommand{\cellimgsixperrow}[1]{
    \includegraphics[width=2.3cm, height=2.3cm]{#1}
}
\newcommand{\cellimgthreeperrow}[1]{
    \includegraphics[width=5.6cm, height=5.6cm]{#1}
}
\newcommand{\minisection}[1]{\vspace{1mm}\noindent{\bf #1}}
\newcommand{\fong}[1]{\textcolor{blue}{[Phong: {#1}]}}
\newcommand{\anh}[1]{{\textcolor{cyan}{[Anh: #1]}}}

\begin{document}

%%%%%%%%% TITLE
\title{Explore Image Deblurring via Encoded Blur Kernel Space\\
--- Supplementary material ---}

\author{
Phong Tran$^{1}$ \quad Anh Tuan Tran$^{1,2}$ \quad Quynh Phung$^{1}$ \quad Minh Hoai$^{1,3}$ \\
$^1$VinAI Research, Hanoi, Vietnam,
$^2$VinUniversity, Hanoi, Vietnam,\\
$^3$Stony Brook University, Stony Brook, NY 11790, USA\\
{\tt\small \{v.phongtt15,v.anhtt152,v.quynhpt29,v.hoainm\}@vinai.io}
}

\maketitle

\input{definitions}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this supplement material, we further provide some material including architecture details, hyper-parameter tuning, and qualitative results to further analyze our method.
\end{abstract}

%%%%%%%%% BODY TEXT

\section{Training environment}
We implement the proposed method using Pytorch 1.4.0. The experiments are conducted using a single Nvidia RTX 2080 Ti GPU. We train $\mathcal{F}$ and $\mathcal{G}$ for $6 \times 10^6$ iterations on either REDS \cite{nah2019ntire} or GOPRO \cite{nah2017deep} datasets. 
% \anh{maybe you can mention the number of iterations instead, to make it look less costly.}

\section{Architecture choices}
Here we illustrate in detail the architecture of each component in our proposed method. Details of the overall network are given in \Fref{fig:detail_architectures}

\subsection{Preprocessing and Postprocessing blocks}
We follow a common practice by using a pre-processing block that downsamples the input image twice and convert it to a feature map of size $64 \times H/4 \times W/4$ at the beginning of both $\mathcal{F}$ and $\mathcal{G}$. We denote it as \underline{Pr}eprocess\underline{B}lock or \textbf{PrB} in short. At the end of $\mathcal{F}$, we apply a post-processing block that converts the output feature map of size $64 \times H/4 \times W/4$ back to the image domain. This block is called \underline{Po}stprocess\underline{B}lock, and denoted as \textbf{PoB}.
%Instead of working directly in the image domain, we first extract feature maps of an image using a feature extractor. Then the input and the output of $\mathcal{F}$ and $\mathcal{G}$ are the feature maps of the corresponding image. These outputs are then converted back to the image domain using a reconstructor module. Feature extractor and reconstructor are implemented by convolutional neural networks. 
Their architectures are illustrated in \Tref{tab:kernel_extractor} and \Tref{tab:reconstructor} respectively.


\begin{table}[ht]
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{ll}
        \toprule
        Layer                                    &  Output shape\\
        \midrule
        $\text{Conv}(3, 64, 3, 1, 1)$            &  $64 \times H \hphantom{/1} \times W$\\
        $\text{Conv}(64, 64, 3, 2, 1)$           &  $64 \times H/2 \times W/2$\\
        $\text{Conv}(64, 64, 3, 2, 1)$           &  $64 \times H/4 \times W/4$\\
        $\text{ResBlock}(64) \times 10$          &  $64 \times H/4 \times W/4$\\
        \bottomrule
    \end{tabular}
    \caption{Structure of PreprocessBlock.}
    \label{tab:kernel_extractor}
\end{table}

\begin{table}[ht]
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{ll}
        \toprule
        Layer                                       &  Output shape\\
        \midrule
        $\text{ResBlock}(64) \times 20$             &  $64\hphantom{0} \times H/4 \times W/4$\\
        $\text{Conv}(64, 256, 3, 1, 1)$             &  $64\hphantom{0} \times H/4 \times W/4$\\
        $\text{PixelShuffle}(2)$                    &  $64\hphantom{0} \times H/2 \times W/2$\\
        $\text{LeakyReLU}(0.1)$                     &  $64\hphantom{0} \times H/2 \times W/2$\\
        $\text{Conv}(64, 256, 3, 1, 1)$             &  
        $256 \times H/2 \times W/2$\\
        $\text{PixelShuffle}(2)$                    &  $64\hphantom{0} \times H\hphantom{/1} \times W$\\
        $\text{LeakyReLU}(0.1)$                     &  $64\hphantom{0} \times H\hphantom{/1} \times W$\\
        $\text{Conv}(64, 64, 3, 1, 1)$              &  $64\hphantom{0} \times H\hphantom{/1} \times W$\\
        $\text{Conv}(64, 3, 3, 1, 1)$               &  
        $3\hphantom{00}\times H\hphantom{/1} \times W$\\
        
        \bottomrule
    \end{tabular}
    \caption{Structure of PostprocessBlock.}
    \label{tab:reconstructor}
\end{table}

\begin{table}[ht]
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{ll}
        \toprule
        Layer                                       &  Output shape\\
        \midrule
        PreprocessBlock & $64\hphantom{0} \times H/4\hphantom{00} \times W/4\hphantom{00}$\\
        $\text{Conv}(128, 64, 7, 1, 1)$              &  $64\hphantom{0} \times H/4\hphantom{00} \times W/4$\\
        $\text{LeakyReLU(0.1)}$                      & $64\hphantom{0} \times H/4\hphantom{00} \times W/4$\\
        
        $\text{Conv}(64, 128, 3, 2, 1)$              & 
        $128 \times H/8\hphantom{00} \times W/8$\\
        $\text{LeakyReLU(0.1)}$                      & 
        $128 \times H/8\hphantom{00} \times W/8$\\
        
        $\text{Conv}(128, 256, 3, 2, 1)$             & 
        $256 \times H/16\hphantom{0} \times W/16$\\
        $\text{LeakyReLU(0.1)}$                      & 
        $256 \times H/16\hphantom{0} \times W/16$\\
        
        $\text{Conv}(256, 512, 3, 2, 1)$             & 
        $512 \times H/32\hphantom{0} \times W/32$\\
        $\text{LeakyReLU(0.1)}$                      & 
        $512 \times H/32\hphantom{0} \times W/32$\\
        
        $\text{Conv}(512, 512, 3, 2, 1)$             & 
        $512 \times H/64\hphantom{0} \times W/64$\\
        $\text{LeakyReLU(0.1)}$                      & 
        $512 \times H/64\hphantom{0} \times W/64$\\
        
        $\text{Conv}(512, 512, 3, 2, 1)$             & 
        $512 \times H/128 \times W/128$\\
        $\text{LeakyReLU(0.1)}$                      & 
        $512 \times H/128 \times W/128$\\
        
        $\text{ResBlock}(512) \times 4$              & 
        $512 \times H/128 \times W/128$\\
        
        \bottomrule
    \end{tabular}
    \caption{Structure of $\mathcal{G}$}
    \label{tab:G_architecture}
\end{table}

\begin{table}[ht]
    \setlength{\tabcolsep}{3pt}
    \centering
    \begin{tabular}{l|l}
        \toprule
        \multicolumn{2}{c}{Encoder}\\
        \midrule
        Layer                                        &  Output shape\\
        \midrule
        PreprocessBlock & $64\hphantom{0} \times H/4\hphantom{00} \times W/4\hphantom{00}$\\
        $\text{Conv}(64, 64, 3, 2, 1)$               &  
        $64\hphantom{0} \times H/8\hphantom{00} \times W/8$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $64\hphantom{0} \times H/8\hphantom{00} \times W/8$\\
        $\text{Conv}(64, 128, 3, 2, 1)$              &  
        $128 \times H/16\hphantom{0} \times W/16$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $128 \times H/16\hphantom{0} \times W/16$\\
        $\text{Conv}(128, 256, 3, 2, 1)$             &  
        $256 \times H/32\hphantom{0} \times W/32$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $256 \times H/32\hphantom{0} \times W/32$\\
        $\text{Conv}(256, 512, 3, 2, 1)$             &  
        $512 \times H/64\hphantom{0} \times W/64$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $512 \times H/64\hphantom{0} \times W/64$\\
        $\text{Conv}(512, 512, 3, 2, 1)$             &  
        $512 \times H/128 \times W/128$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $512 \times H/128 \times W/128$\\
        \midrule
        \multicolumn{2}{c}{Decoder}\\
        \midrule
        Layer                                        &  Output shape\\
        \midrule
        $\text{TransConv}(1024, 512, 3, 2, 1)$       &  
        $512 \times H/64 \times W/64$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $512 \times H/64 \times W/64$\\
        $\text{TransConv}(1024, 256, 3, 2, 1)$       &  
        $256 \times H/32 \times W/32$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $256 \times H/32 \times W/32$\\
        $\text{TransConv}(512, 128, 3, 2, 1)$        &  
        $128 \times H/16 \times W/16$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $128 \times H/16 \times W/16$\\
        $\text{TransConv}(256, 64, 3, 2, 1)$         &  
        $64\hphantom{0} \times H/8\hphantom{0} \times W/8$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $64\hphantom{0} \times H/8\hphantom{0} \times W/8$\\
        $\text{TransConv}(128, 64, 3, 2, 1)$         &  
        $64\hphantom{0} \times H/4\hphantom{0} \times W/4$\\
        $\text{LeakyReLU}(0.1)$                      &  
        $64\hphantom{0} \times H/4\hphantom{0} \times W/4$\\
        PostprocessBlock & $64\hphantom{0} \times H\hphantom{000} \times W\hphantom{00}$\\
        
        \bottomrule
    \end{tabular}
    \caption{Structure of the encoder and decoder of $\mathcal{F}$}
    \label{tab:F_architecture}
\end{table}

\subsection{Architecture of $\mathcal{G}$}
We use $\mathcal{G}$ to extract the blur kernel $k$ from a given sharp-blur pair of images $x, y$. We implement $\mathcal{G}$ using the mentioned PreprocessBlock and a follow-up residual neural network \cite{he2016deep}. Input of $\mathcal{G}$ is the concatenation of $x$ and $y$. Its output is a blur kernel of size $512 \times H/128 \times W/128$. Details of its architecture are given in \Tref{tab:G_architecture}.

\subsection{Architecture of $\mathcal{F}$}
$\mathcal{F}$ takes two inputs, the sharp image $x$ and the blur kernel from $\mathcal{G}(x, y)$. As mentioned, $\mathcal{F}$ uses a PreprocessBlock at the beginning and a PostprocessBlock at the end. Between these blocks, we use an encoder-decoder with skip connection \cite{ronneberger2015u}. The encoder downsamples the pre-processed feature map five times and flattens to an embedding vector. This vector is then concatenated with $k$ and fed into a decoder that reconstructs the output feature map. Details of its architecture are illustrated in \Tref{tab:F_architecture}.

\subsection{Architectures of Deep Image Prior}
We adopt the architecture of $\mathcal{G}$ for the network of DIP of the blur kernel. The input $z_k$ is a normal-distributed random tensor with the size equal to the size of the input of $\mathcal{G}$.

For DIP for image, we adopt a U-net \cite{ronneberger2015u} as suggested in \cite{ulyanov2018deep}. The input $z_x$ is a normal-distributed random tensor with size $1 \times 64 \times 64$.

\section{Hyper-parameters tuning}
We trained the networks with an Adam optimizer \cite{kingma2014adam} with $\beta_1$ and $\beta_2$ are $0.9$ and $0.99$ respectively. The initial learning rate was $10^{-4}$ with cosine annealing scheduler \cite{loshchilov2016sgdr} was applied. We set the weight of kernel regularization $||k||_2$ to $6 \times 10^{-4}$ for all image debluring experiments. The weight of Hyper-Laplacian prior \cite{krishnan2009fast} was set to $2 \times 10^{-2}$.

\section{Cross-dataset experiment}
Here we provide quantitative comparisons on GOPRO and HIDE dataset \cite{HAdeblur} in \Tref{tab:hideexp}. We train the model using GOPRO dataset and test on HIDE dataset and vice versa. To make the testing sets, we randomly sample 500 images from each GOPRO and HIDE testing set. Qualitative results are given in \Fref{fig:nonuniform}.

\setlength{\tabcolsep}{4pt}
\begin{table}[ht]
    \centering
    \begin{tabular}{cccc}
        \toprule
          & DeblurGANv2 [15] & SRN-Deblur [36] & ours\\
          \midrule
          GOPRO & 24.35 & 25.21 & \textbf{26.17}\\
          HIDE & 24.65 & 25.25 & \textbf{25.97}\\
         \bottomrule
    \end{tabular}
    \vskip 0.05in
    \caption{PSNR scores of deblurring methods on the HIDE and GOPRO datasets.}
    \vspace{-5mm}
    \label{tab:hideexp}
\end{table}

\begin{figure}[ht]
    \setlength{\tabcolsep}{0.3pt}
    \small
    \begin{center}
    \begin{tabular}{cccc}
        \cellimg{images/rebuttal/SRN_HIDE.png} &
        \cellimg{images/rebuttal/ours_HIDE.png} & 
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{Deblurring results (left: SRN, right: ours) on HIDE dataset}
    \label{fig:nonuniform}
\end{figure}

\section{Inference time}
We trained the kernel extractor using an Nvidia V100 with 5GB memory. It took $600K$ iterations to converge (about 4 days). The average inference time for a $256{\times}256$ image using an Nvidia V100 is 209.53s.

\section{More qualitative results}
Here we provide more qualitative results of our methods including: Blur transferring (\Fref{fig:synthesis1} and \Fref{fig:synthesis2}) and image deblurring on face domain (\Fref{fig:naturaldeblurring1}, \Fref{fig:naturaldeblurring2}, \Fref{fig:naturaldeblurring5}, \Fref{fig:naturaldeblurring3}, \Fref{fig:naturaldeblurring4}, \Fref{fig:naturaldeblurring6}, \Fref{fig:naturaldeblurring7}, and \Fref{fig:naturaldeblurring8}).

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.39]{images/figures/detailed_architectures.pdf}
    \caption{\large Detailed architecture of the proposed method}
    \label{fig:detail_architectures}
\end{figure*}

\setlength{\tabcolsep}{0pt}
\begin{figure*}
    \huge
    \begin{center}
        \begin{tabular}{cccc}
            $\hat{x}$ & $x$ & $y$ & $\hat{y}$\\
            \cellimg{images/supp/synthesis/02/source.png} &
            \cellimg{images/supp/synthesis/02/org_sharp.png} &
            \cellimg{images/supp/synthesis/02/org_blur.png} &
            \cellimg{images/supp/synthesis/02/target.png}\\
            \cellimg{images/supp/synthesis/04/source.png} &
            \cellimg{images/supp/synthesis/04/org_sharp.png} &
            \cellimg{images/supp/synthesis/04/org_blur.png} &
            \cellimg{images/supp/synthesis/04/target.png}\\
            \cellimg{images/supp/synthesis/07/source.png} &
            \cellimg{images/supp/synthesis/07/org_sharp.png} &
            \cellimg{images/supp/synthesis/07/org_blur.png} &
            \cellimg{images/supp/synthesis/07/target.png}\\
            \cellimg{images/supp/synthesis/06/source.png} &
            \cellimg{images/supp/synthesis/06/org_sharp.png} &
            \cellimg{images/supp/synthesis/06/org_blur.png} &
            \cellimg{images/supp/synthesis/06/target.png}\\
        \end{tabular}
    \caption{\large Transferring blur kernel from the source pair $x, y$ to the target sharp $\hat{x}$ to generate the target blurry image $\hat{y}$}
    \label{fig:synthesis1}
    \end{center}
\end{figure*}

\setlength{\tabcolsep}{0pt}
\begin{figure*}
    \huge
    \begin{center}
        \begin{tabular}{cccc}
            $\hat{x}$ & $x$ & $y$ & $\hat{y}$\\
            \cellimg{images/supp/synthesis/01/source.png} &
            \cellimg{images/supp/synthesis/01/org_sharp.png} &
            \cellimg{images/supp/synthesis/01/org_blur.png} &
            \cellimg{images/supp/synthesis/01/target.png}\\
            \cellimg{images/supp/synthesis/08/source.png} &
            \cellimg{images/supp/synthesis/08/org_sharp.png} &
            \cellimg{images/supp/synthesis/08/org_blur.png} &
            \cellimg{images/supp/synthesis/08/target.png}\\
            \cellimg{images/supp/synthesis/03/source.png} &
            \cellimg{images/supp/synthesis/03/org_sharp.png} &
            \cellimg{images/supp/synthesis/03/org_blur.png} &
            \cellimg{images/supp/synthesis/03/target.png}\\
            \cellimg{images/supp/synthesis/05/source.png} &
            \cellimg{images/supp/synthesis/05/org_sharp.png} &
            \cellimg{images/supp/synthesis/05/org_blur.png} &
            \cellimg{images/supp/synthesis/05/target.png}\\
        \end{tabular}
    \caption{\large transferring blur kernel from the source pair $x, y$ to the target sharp $\hat{x}$ to generate the target blurry image $\hat{y}$}
    \label{fig:synthesis2}
    \end{center}
\end{figure*}

% \begin{figure*}[t]
%     \setlength{\tabcolsep}{0.3pt}
%     \small
%     \begin{center}
%     \begin{tabular}{cccc}
%         \multicolumn{1}{c}{Blur} & 
%         \multicolumn{1}{c}{SRN-Deblur \cite{tao2018scale}} & 
%         \multicolumn{1}{c}{Ours} &
%         \multicolumn{1}{c}{Sharp}\\
%         \cellimg{images/imagedeblurring/img07/blur.png} &
%         \cellimg{images/imagedeblurring/img07/SRN.png} &
%         \cellimg{images/imagedeblurring/img07/ours.png} &
%         \cellimg{images/imagedeblurring/img07/sharp.png}\\
%         \cellimg{images/imagedeblurring/img08/blur.png} &
%         \cellimg{images/imagedeblurring/img08/SRN.png} &
%         \cellimg{images/imagedeblurring/img08/ours.png} &
%         \cellimg{images/imagedeblurring/img08/sharp.png}\\
%         \cellimg{images/imagedeblurring/img10/blur.png} &
%         \cellimg{images/imagedeblurring/img10/SRN.png} &
%         \cellimg{images/imagedeblurring/img10/ours.png} &
%         \cellimg{images/imagedeblurring/img10/sharp.png}\\
%         \cellimg{images/imagedeblurring/img01/blur.png} &
%         \cellimg{images/imagedeblurring/img01/SRN.png} &
%         \cellimg{images/imagedeblurring/img01/ours.png} &
%         \cellimg{images/imagedeblurring/img01/sharp.png}\\
%         \cellimg{images/imagedeblurring/img02/blur.png} &
%         \cellimg{images/imagedeblurring/img02/SRN.png} &
%         \cellimg{images/imagedeblurring/img02/ours.png} &
%         \cellimg{images/imagedeblurring/img02/sharp.png}\\
%     \end{tabular}
%     \end{center}
%     \vskip -0.15in
%     \caption{Results of deblurring methods trained on REDS and tested on GOPRO}
%     \label{fig:imagedeblurring}
% \end{figure*}

% \begin{figure*}[t]
%     \setlength{\tabcolsep}{0.3pt}
%     \begin{center}
%     \begin{tabular}{cccc}
%         \multicolumn{1}{c}{Blur} & 
%         \multicolumn{1}{c}{SRN-Deblur \cite{tao2018scale}} & 
%         \multicolumn{1}{c}{Ours} &
%         \multicolumn{1}{c}{Sharp}\\
%         \cellimg{images/face/img3/blur.png} &
%         \cellimg{images/face/img3/SRN.png} &
%         \cellimg{images/face/img3/ours.png} &
%         \cellimg{images/face/img3/sharp.png}\\
%         \cellimg{images/face/img10/blur.png} &
%         \cellimg{images/face/img10/SRN_REDS.png} &
%         \cellimg{images/face/img10/ours.png} &
%         \cellimg{images/face/img10/sharp.png}\\
%         \cellimg{images/face/img8/blur.png} &
%         \cellimg{images/face/img8/SRN_REDS.png} &
%         \cellimg{images/face/img8/ours.png} &
%         \cellimg{images/face/img8/sharp.png}\\
%         \cellimg{images/face/img9/blur.png} &
%         \cellimg{images/face/img9/SRN_REDS.png} &
%         \cellimg{images/face/img9/ours.png} &
%         \cellimg{images/face/img9/sharp.png}\\
%     \end{tabular}
%     \end{center}
%     \vskip -0.15in
%     \caption{Results of deblurring methods trained on REDS and tested on GOPRO}
%     \label{fig:naturaldeblurring1}
% \end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{lll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS}\\
        \cellimgthreeperrow{images/face/img3/blur.png} &
        \cellimgthreeperrow{images/face/img3/SelfDeblur.png} &
        \cellimgthreeperrow{images/face/img3/DeblurGANv2.png}\\[0.2cm]
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug}\\
        \cellimgthreeperrow{images/face/img3/DeblurGANv2_face_imgaug.png} &
        \cellimgthreeperrow{images/face/img3/SRN.png} &
        \cellimgthreeperrow{images/face/img3/SRN_face_imgaug.png}\\[0.2cm]
        \multicolumn{1}{c}{Ours} & &\\
        \cellimgthreeperrow{images/face/img3/ours.png} &\\
    \end{tabular}
    \end{center}
    \caption{\large Results of deblurring methods trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring1}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{lll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS}\\
        \cellimgthreeperrow{images/face/img8/blur.png} &
        \cellimgthreeperrow{images/face/img8/SelfDeblur.png} &
        \cellimgthreeperrow{images/face/img8/DeblurGANv2_REDS.png}\\[0.2cm]
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug}\\
        \cellimgthreeperrow{images/face/img8/DeblurGANv2_imgaug.png} &
        \cellimgthreeperrow{images/face/img8/SRN_REDS.png} &
        \cellimgthreeperrow{images/face/img8/SRN_imgaug.png}\\[0.2cm]
        \multicolumn{1}{c}{Ours} &  &\\
        \cellimgthreeperrow{images/face/img8/ours.png} &\\
    \end{tabular}
    \end{center}
    \vskip 0.2in
    \caption{\large Results of deblurring methods trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring2}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{lll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS}\\
        \cellimgthreeperrow{images/face/img10/blur.png} &
        \cellimgthreeperrow{images/face/img10/SelfDeblur.png} &
        \cellimgthreeperrow{images/face/img10/DeblurGANv2_REDS.png}\\[0.2cm]
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug}\\
        \cellimgthreeperrow{images/face/img10/DeblurGANv2_imgaug.png} &
        \cellimgthreeperrow{images/face/img10/SRN_REDS.png} &
        \cellimgthreeperrow{images/face/img10/SRN_imgaug.png}\\[0.2cm]
        \multicolumn{1}{c}{Ours} & &\\
        \cellimgthreeperrow{images/face/img10/ours.png} &
    \end{tabular}
    \end{center}
    \vskip 0.2in
    \caption{\large Results of deblurring methods trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring5}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{lll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS}\\
        \cellimgthreeperrow{images/face_real/img3/blur.png} &
        \cellimgthreeperrow{images/face_real/img3/SelfDeblur.png} &
        \cellimgthreeperrow{images/face_real/img3/DeblurGANv2_REDS.png}\\[0.2cm]
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug}\\
        \cellimgthreeperrow{images/face_real/img3/DeblurGANv2_imgaug.png} &
        \cellimgthreeperrow{images/face_real/img3/SRN_REDS.png} &
        \cellimgthreeperrow{images/face_real/img3/SRN_imgaug.png}\\[0.2cm]
        \multicolumn{1}{c}{Ours} & &\\
        \cellimgthreeperrow{images/face_real/img3/ours.png} &
    \end{tabular}
    \end{center}
    \vskip 0.2in
    \caption{\large Results of deblurring methods trained on REDS and tested on an in-the-wild example}
    \label{fig:naturaldeblurring3}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{lll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS}\\
        \cellimgthreeperrow{images/face_real/img4/blur.png} &
        \cellimgthreeperrow{images/face_real/img4/SelfDeblur.png} &
        \cellimgthreeperrow{images/face_real/img4/DeblurGANv2_REDS.png}\\[0.2cm]
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug}\\
        \cellimgthreeperrow{images/face_real/img4/DeblurGANv2_imgaug.png} &
        \cellimgthreeperrow{images/face_real/img4/SRN_REDS.png} &
        \cellimgthreeperrow{images/face_real/img4/SRN_imgaug.png}\\[0.2cm]
        \multicolumn{1}{c}{Ours} & &\\
        \cellimgthreeperrow{images/face_real/img4/ours.png} &
    \end{tabular}
    \end{center}
    \vskip 0.2in
    \caption{\large Results of deblurring methods trained on REDS and tested on an in-the-wild example}
    \label{fig:naturaldeblurring4}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{cc}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{Ours}\\
        \cellbigimg{images/face/img11/blur.png} &
        \cellbigimg{images/face/img11/ours.png}\\
        \cellbigimg{images/face/img12/blur.png} &
        \cellbigimg{images/face/img12/ours.png}\\
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{\large Results of our method trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring6}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{cc}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{Ours}\\
        \cellbigimg{images/face/img13/blur.png} &
        \cellbigimg{images/face/img13/ours.png}\\
        \cellbigimg{images/face/img14/blur.png} &
        \cellbigimg{images/face/img14/ours.png}\\
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{\large Results of our method trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring7}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \large
    \begin{center}
    \begin{tabular}{cc}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{Ours}\\
        \cellbigimg{images/face/img15/blur.png} &
        \cellbigimg{images/face/img15/ours.png}\\
        \cellbigimg{images/face/img16/blur.png} &
        \cellbigimg{images/face/img16/ours.png}\\
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{\large Results of our method trained on REDS and tested on GOPRO}
    \label{fig:naturaldeblurring8}
\end{figure*}

{\small
%\bibliographystyle{ieee_fullname}
\setlength{\bibsep}{0pt}
\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krishnan and Fergus(2009)]{krishnan2009fast}
Dilip Krishnan and Rob Fergus.
\newblock Fast image deconvolution using hyper-laplacian priors.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2009.

\bibitem[Kupyn et~al.(2019)Kupyn, Martyniuk, Wu, and Wang]{kupyn2019deblurgan}
Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang.
\newblock Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2019.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Nah et~al.(2017)Nah, Hyun~Kim, and Mu~Lee]{nah2017deep}
Seungjun Nah, Tae Hyun~Kim, and Kyoung Mu~Lee.
\newblock Deep multi-scale convolutional neural network for dynamic scene
  deblurring.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Nah et~al.(2019)Nah, Baik, Hong, Moon, Son, Timofte, and
  Mu~Lee]{nah2019ntire}
Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu
  Timofte, and Kyoung Mu~Lee.
\newblock Ntire 2019 challenge on video deblurring and super-resolution:
  Dataset and study.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, 2019.

\bibitem[Ren et~al.(2020)Ren, Zhang, Wang, Hu, and Zuo]{ren2020neural}
Dongwei Ren, Kai Zhang, Qilong Wang, Qinghua Hu, and Wangmeng Zuo.
\newblock Neural blind deconvolution using deep priors.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, 2015.

\bibitem[Shen et~al.(2019)Shen, Wang, Shen, Ling, Xu, and Shao]{HAdeblur}
Ziyi Shen, Wenguan Wang, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao.
\newblock Human-aware motion deblurring.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2019.

\bibitem[Tao et~al.(2018)Tao, Gao, Shen, Wang, and Jia]{tao2018scale}
Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia.
\newblock Scale-recurrent network for deep image deblurring.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Ulyanov et~al.(2018)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2018deep}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Deep image prior.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\end{thebibliography}
}

\end{document}
