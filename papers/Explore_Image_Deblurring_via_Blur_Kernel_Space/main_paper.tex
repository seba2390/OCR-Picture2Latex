% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage[numbers,sort&compress]{natbib}
% \usepackage[normalem]{ulem}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{5198} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

% user's definitions
\newcommand{\Sref}[1]{Sec.~\ref{#1}}
\newcommand{\Eref}[1]{Eq.~(\ref{#1})}
\newcommand{\Fref}[1]{Fig.~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\Aref}[1]{Algorithm~\ref{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\cellimg}[1]{
    \includegraphics[width=3cm, height=3cm]{#1}
}
\newcommand{\cellimgsmall}[1]{
    \includegraphics[width=2.0cm, height=2.0cm]{#1}
}
\newcommand{\cellimgtiny}[1]{
    \includegraphics[width=2.0cm, height=2.0cm]{#1}
}
\newcommand{\cellimgsixperrow}[1]{
    \includegraphics[width=2.3cm, height=2.3cm]{#1}
}
\newcommand{\minisection}[1]{\vspace{1mm}\noindent{\bf #1}}
\newcommand{\fong}[1]{\textcolor{blue}{[Phong: {#1}]}}
\newcommand{\anh}[1]{{\textcolor{cyan}{[Anh: #1]}}}

\pagenumbering{gobble}
\begin{document}

%%%%%%%%% TITLE
\title{Explore Image Deblurring via Encoded Blur Kernel Space}

% \author{Phong Tran, Anh Tran, Phung Quynh, Minh Hoai\\
% VinAI Research\\
% {\tt\small \{v.phongtt15, v.anhtt152, v.quynhpt29\, v.hoainm\}@vinai.io}}
\author{
Phong Tran$^{1}$ \quad Anh Tuan Tran$^{1,2}$ \quad Quynh Phung $^{1}$ \quad Minh Hoai$^{1,3}$ \\
$^1$VinAI Research, Hanoi, Vietnam,
$^2$VinUniversity, Hanoi, Vietnam,\\
$^3$Stony Brook University, Stony Brook, NY 11790, USA\\
{\tt\small \{v.phongtt15,v.anhtt152,v.quynhpt29,v.hoainm\}@vinai.io}
}

\maketitle

\input{definitions}

%%%%%%%%% ABSTRACT
\begin{abstract}

% Despite many recent advances, image deblurring methods are still ineffective in handling unseen blur kernels. In this paper, we overcome these problems by learning the blur kernel space from real-world deblurring datasets. Using this encoded kernel space, we can approximate any unseen blur operator. Hence, we propose an alternative optimization scheme for image deblurring using our encoded kernel space. Our method can handle unseen blur kernel, unlike deep-learning-based methods, while avoiding using complicated handcrafted priors on the blur operator, unlike classical methods. The encoded kernel space is fully differentiable, thus can be easily adapted to deep neural network models. Moreover, our method can be used for blur synthesis by transferring an existing blur operator and synthesizing novel blur from a given dataset into a new domain. Finally, we provide experimental results to confirm the effectiveness of the proposed method.

This paper introduces a method to encode the blur operators of an arbitrary dataset of sharp-blur image pairs into a blur kernel space. Assuming the encoded kernel space is close enough to in-the-wild blur operators, we propose an alternating optimization algorithm for blind image deblurring. It approximates an unseen blur operator by a kernel in the encoded space and searches for the corresponding sharp image. Unlike recent deep-learning-based methods, our system can handle unseen blur kernel, while avoiding using complicated handcrafted priors on the blur operator often found in classical methods. Due to the method's design, the encoded kernel space is fully differentiable, thus can be easily adopted in deep neural network models. Moreover, our method can be used for blur synthesis by transferring existing blur operators from a given dataset into a new domain. Finally, we provide experimental results to confirm the effectiveness of the proposed method. The code is available at \url{https://github.com/VinAIResearch/blur-kernel-space-exploring}.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Motion blur occurs due to camera shake or rapid movement of objects in a scene. Image deblurring is the task of removing the blur artifacts to improve the quality of the captured image. Image deblurring is an important task with many applications, especially during the current age of mobile devices and handheld cameras. Image deblurring, however, is still an unsolved problem, despite much research effort over the past decades.

%Due to hardware limitations, camera shaking or rapid movement could yield motion blur and downgrade the quality of the captured photo. Image deblurring is the process that aims to remove this blur artifact from images. With the rise of digital cameras nowadays, image deblurring is essential more than ever. Many efforts have been made to solve this task over the past decades. However, image deblurring is still an unsolved problem.

Mathematically, the task of image deblurring is to recover the sharp image $x$ given a blurry image $y$. One can assume the below mathematical model that relates $x$ and $y$:
\begin{equation}
    y = \hat{\mathcal{F}}(x, k) + \eta \approx \hat{\mathcal{F}}(x, k),
    \label{eq:generaldeblurring}
\end{equation}
where $\hat{\mathcal{F}}(\cdot, k)$ is the blur operator with the blur kernel $k$, and $\eta$ is noise. In the simplest form, $\hat{\mathcal{F}}(\cdot, k)$ is assumed to be a convolution function with $k$ being a convolution kernel and $\eta$ being white Gaussian noise. Given a blurry image $y$, the deblurring task is to recover the sharp image $x$ and optionally the blur operator $\hat{\mathcal{F}}(\cdot, k)$. 



%where $x$ is the latent sharp image, $y$ is the given blurry image, $\hat{\mathcal{F}}$ is the parametric blur operator with parameter $k$, and $\eta$ is a noise. In the simplest form, $\hat{\mathcal{F}}$ is assumed to be a convolution function, its parameter $k$ is a convolution kernel, and $\eta$ is a white Gaussian noise. Given a blurry image $y$, our task is to solve for $x$ and $\hat{\mathcal{F}}_k$. From now on, for simplicity, we refer to a blur operator by its parameter $k$ and denote it as $k$ blur kernel.

%, $\mathcal{H}$ is assumed to be a convolution function and $\eta$ is a white gaussian noise with noise level $\sigma$, i.e.
%\begin{equation}
%    y = k * x + \eta \approx k * x
%    \label{convdeblurring}
%\end{equation}
%where $k$ is the convolution kernel of $\mathcal{H}$. In blind-deblurring setting, $k$ is unknown. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/figures/teaser.pdf}
    % \caption{The core idea of our method. We encode the blur kernel space of a given dataset of sharp-blur image pairs (orange arrows) and leverage this encoded space to solve some specific tasks, for example, image deblurring (blue arrows) or blur synthesis.}
    \caption{The space of blur kernels is the missing element for successful blur removal and synthesis. Previous image debluring methods either overlooked the importance of this kernel space or made inadequate assumption about it. In this paper, we propose to learn this blur kernel space from a dataset of sharp-blurry image pairs (orange arrows) and leverage this encoded space for image deblurring (blue arrows).}
    \label{fig:generalidea}
    % \mhoai{I don't think this is a good teaser figure. It is neither impressive or illustrative. I am not sure what it helps to explain. Is this figure specific to your method? If you are proposing a MAP-based method, can you still use this figure? You need something impressive, or show the cleverness of your idea. Right now, this idea of manifold is not new, at least has been used by MAP-based methods.}
\end{figure}

%A common approach is maximum a posteriori probability (MAP) estimation in which $k$ and $x$ can be solved using optimization:
% \begin{equation}
%     \begin{split}
%         k, x &= \myargmax{k, x} \bbP(x, k|y) \\
%              & =  \myargmax{k, x} \bbP(y|x, k) \bbP(x) \bbP(k). \label{map}
%     \end{split}
% \end{equation}
A popular approach to recover the sharp image is to use the  Maximum A Posterior (MAP) estimate. That is to find $x$ and $k$ to maximize the posterior propbability $\bbP(x, k|y)$ assuming $\hat{\mF}$ is known. This is equivalent to optimizing:  
% \begin{align}
%         x, k = \myargmax{x, k} \bbP(y|\hat{\mF}(x, k)) \bbP(x) \bbP(k). \label{eq:map}
% \end{align}
\begin{align}
        x, k = \myargmax{x, k} \bbP(y|x, k) \bbP(x) \bbP(k). \label{eq:map}
\end{align}
% In general, the functional form of $\hat{\mF}(\cdot, k)$ and $\bbP(y|\hat{\mF}(x, k))$ are fixed, and the probability $\bbP(y|\hat{\mF}(x, k))$ can be computed if both $x$ and $k$ are given. 

However, this is an ill-posed problem and there are infinitely many pairs of $(k, x)$ that lead to the same probability $\bbP(y|x, k)$, so the key aspect of the above MAP approach is to define proper models for the prior distributions $\bbP(x)$ and $\bbP(k)$. In fact, many deblurring methods focus on either designing handcrafted priors for $x$ and $k$ \cite{chan1998total,krishnan2009fast,pan2016blind,liu2014blind} or learning the deep image prior~\cite{ulyanov2018deep, ren2020neural}. However, all of these works assume the blur operator is a convolutional operator, and this assumption does not hold in practice. These MAP-based methods cannot handle complex in-the-wild blur operators and usually produce undesirable artifacts when testing on real-world blurry images.


% prior to the resurgence of deep learning, designing handcrafted priors for $x$ and $k$ was the main focus of many deblurring methods such as \cite{chan1998total,krishnan2009fast,pan2016blind,liu2014blind}.
% Recently, leveraging the powerful capacity of neural network, \citet{ulyanov2018deep} and  \citet{ren2020neural} propose deep image prior (DIP) for image deblurring. Still, all these works assume blur operator is a convolution function, which reduces the representative capacity of the kernel. In fact, MAP-based methods cannot handle complex in-the-wild blur operator and usually produce undesirable artifacts when testing on real-world blurry images.
%  \sout{These priors only capture low-level statistics of natural images and hence are not able to handle complex features.  Another issue is that assuming the}


%that satisfy \Eref{map}, prior knowledge on $x$ and $k$, i.e., prior distribution $\bbP(x)$ and $\bbP(k)$, should be adopted. In the past, finding handcrafted prior for $x$ and $k$ was the mainstream, for example \cite{chan1998total,krishnan2009fast,pan2016blind,liu2014blind}. Recently, leveraging the powerful capacity of neural network, \citet{ulyanov2018deep} and  \citet{ren2020neural} propose deep image prior (DIP) for image deblurring. Still, all these works assume blur operator is a convolution function, which reduces the representative capacity of the kernel. In fact, MAP-based methods cannot handle complex in-the-wild blur operator and usually produce undesirable artifacts when testing on real-world blurry images.
%  \sout{These priors only capture low-level statistics of natural images and hence are not able to handle complex features.  Another issue is that assuming the}


An alternative approach is to directly learn a function that maps from a blurry image to the corresponding non-blurry image. This function can be a deep convolutional network and the parameters of the network can be learned using paired training data of blurry and non-blurry images~\cite{nah2017deep,tao2018scale,kupyn2018deblurgan,kupyn2019deblurgan}. Unlike the MAP-based approach, this approach learns the inverse function of the blur operator directly without explicitly reasoning about the blur operator and the distribution of the blur kernel. Given the lack of an explicit representation for the space of the blur kernels, this approach does not generalize well beyond the set of individual blur kernels seen during training. This approach~\cite{nah2017deep,tao2018scale,kupyn2018deblurgan,kupyn2019deblurgan} produces poor results when testing on blur operators that are not present in the training set. In our experiments, these deep-learning models degenerate to an identity map when testing on an out-of-domain blur operator; the recovered image is nearly identical to the input image. This is a known issue, and it is referred to as ``the trivial solution'' by traditional deblurring methods.  The MAP-based methods tackle this problem by putting prior distributions on the sharp image and the blur kernel. However, those priors cannot be readily applied to the existing deep-learning models due to the lack of an explicit representation for the blur kernels. 
%since the blur kernels are not learned explicitly. 
%The performance drop of deep models is illustrated in \Tref{tab:synexp}.


% Alternatively, modern deep-learning-based deblurring methods are data-driven \cite{nah2017deep,tao2018scale,kupyn2018deblurgan,kupyn2019deblurgan}. The neural networks learn how to deblur images from paired sharp-blur training data like black-boxes. Despite the impressive quantitative performance, these deep models produce poor results when testing on unseen blur kernels, i.e., blur kernels that are not present in the training set. This performance drop phenomenon is illustrated in \Tref{tab:synexp}.
%Alternatively, modern deep-learning-based deblurring methods \cite{nah2017deep,tao2018scale,kupyn2018deblurgan,kupyn2019deblurgan} directly learn a map from blurry images to sharp ones given a sharp-blur paired training dataset. These models do not explicitly learn the blur kernels but memorize how to recover image patches corrupted by these kernels. Therefore, they produce poor results when testing on blur operators that are not present in the training set. In our experiments, we observe that these deep models act like an identity map when testing on out-of-domain blur operators, i.e., the predicted images are nearly the same as the input images. This phenomenon is similar to the "trivial solution" issue in traditional methods. The classical methods tackle this problem by employing priors on the sharp image and blur kernel. However, those priors cannot be applied to existing deep models since the blur kernels are not learned explicitly. The performance drop of deep models is illustrated in \Tref{tab:synexp}.
% \sout{Since the blur operator $\mathcal{\hat{F}}_k$ might be not invertible, as claimed by \citet{menon2020pulse}, given a blurry image $y$, the predicted image of deep models converges to the average of all $x$ such that $\hat{\mathcal{F}}_k(x) = y$, hence yield to a blurry result. Moreover}


%\anh {Move commented sections to related work.}
%Besides the development of deep deblurring models, many large-scale datasets for image deblurring have been proposed, such as REDS \cite{nah2019ntire} and GOPRO \cite{nah2017deep} datasets. They mimic the motion blur generated by the camera process by averaging consecutive frames. These datasets are captured in various scenes and due to the sophisticated synthesizing process, their blur effects are close to real-world blur in terms of visualization. Nonetheless, there is still a big gap between the synthesized blur kernels, hence training deep models on these datasets could lead to kernel overfitting. This is indeed the case (check \Fref{tab:synexp}).

%Recently, with the ability to capture rich information on in-the-wild images e.g. color, structure, content, e.t.c, natural image manifold that approximated by GAN is expected to be a good prior for image restoration tasks. Given a low-quality image, Pan et al. \cite{pan2020exploiting} and Menon et al. \cite{menon2020pulse} search for a sharp image on the approximated natural image manifold whose degradation version is the closest to the given low-quality image. These methods are based on one essential condition: the degradation operator is known or can be approximated by a similar operator (that is, bicubic for image super-resolution or white gaussian noise for image denoising), which is not the case in the blind deblurring setting.

% Based on previous work analyses, we found that learning the blur operator space is crucial to image deblurring. Hence, in this paper, we propose a method to encode that space into a high dimensional latent manifold by learning from sharp-blur pair datasets. Specifically, we find two functions $\mathcal{F}$ and $\mathcal{G}$ such that:
% \begin{align}
%         y = \mathcal{F}(x, k) \quad \textrm{and} \quad 
%         k = \mathcal{G}(x, y) \label{ourformula}
% \end{align}
% \fong{should we include "high dimensional"?}
% where $x$ and $y$ are defined before, $k$ is a vector encoding the blur operator, $\mathcal{G}(\cdot, \cdot)$ is a blur extractor that extracts $k$ from the sharp-blur pair $(x, y)$, and $\mathcal{F}(\cdot, \cdot)$ is a blur generator that applies $k$ on the sharp image $x$ to get the blur one $y$. Note that $\mathcal{G}$ learns to encode the blur operator latent space while $\mathcal{F}$ mimics the true blur function $\hat{\mathcal{F}}$.

% The functions $\mathcal{F}$ and $\mathcal{G}$ are helpful to solve many different tasks. In this paper, we present two direct applications: Image deblurring and blur synthesis.

% As for image deblurring, our algorithm follows the alternative optimization strategy. We alternatively search for the optimal sharp image $x$ given the blur kernel $k$ and vice versa. Unlike traditional methods in which the blur operator is a convolution kernel, we search for the blur operator embedding $k$ in the learned space. We also restrict the solution space to the natural image manifold approximated by GAN to generate realistic results. Encoded kernels from datasets like REDS \cite{nah2019ntire} or GOPRO \cite{nah2017deep} are closer to the in-the-wild blur than convolution kernels, and the blur operator is not restricted to be linear or uniform. Moreover, our method avoids the kernel over-fitting phenomenon, which is critical in deep deblurring models. As will be mentioned later, the function $\mathcal{F}$ and $\mathcal{G}$ are fully differentiable; therefore, our method can be adapted to other gradient-based optimization methods such as deep convolution networks.

% As for blur synthesis, since the blur operator kernels are image-independent, we can use the extracted kernel $k$ from a sharp-blur pair $(x_i, y_i)$ to transfer the corresponding motion blur to another sharp image $x_j$ in either the same of different image domain. This blur synthesis method is useful in many ways. For example, when we want to transfer motion blur from a dataset that is hard to collect like real-world blur dataset \cite{rim2020real} to another domain (e.g., face, text), or to augment data for other algorithms (e.g., face recognition, classification) e.t.c. Besides blur transferring, we can synthesize novel blur operators that are not in the original dataset via sampling on the learned blur kernel space.

In this paper, we propose to address the limitations of both aforementioned approaches as follows. First, we devise a deep-learning formulation with an explicit representation for the blur kernel and the blur operator. Second, we use a data-driven approach to learn the family of blur operators and the latent manifold of the blur kernels, instead of assuming that the blur operator is a convolutional operator as used in existing MAP-based methods. Specifically, we simultaneously learn a blur operator family $\mF$ and a blur kernel extractor $\mG$ such that:
\begin{align}
         y = \mathcal{F}(x, k) \quad \textrm{and} \quad
         k = \mathcal{G}(x, y) \label{eq:ourformula}.
\end{align}
Note in this paper, $\mathcal{F}$ is referred to as the blur operator \textsl{family}. For a specific blur kernel $k$,  $\mathcal{F}(\cdot, k)$ is a specific blur operator from the family of blur operators. We call $k$ the blur kernel of the blur operator $\mathcal{F}(\cdot, k)$. When the functional form of $\mF$ is fixed, we will refer to a blur operator $\mathcal{F}(\cdot, k)$ by its blur kernel $k$ if there is no confusion.

Once the blur operator family $\mathcal{F}$ has been learned, we can use it to deblur an input image $y$ by finding $x$ and $k$ to satisfy the above equations using alternating optimization. Moreover, we can incorporate additional constraints on the solution space of $x$ to generate more realistic results. For example, we can use a deep generative model to learn the manifold of natural images and constraint the solution space to this manifold. The conceptual idea is illustrated in \Fref{fig:generalidea}.


% \begin{align}
%         y = \mathcal{F}_k(x) \quad \textrm{and} \quad
%         k = \mathcal{G}(x, y) \label{ourformula}
% \end{align}
% where $x$ and $y$ are defined before, $k$ is the parameter of the blur operator, $\mathcal{G}(\cdot, \cdot)$ is a blur kernel extractor that extracts $k$ from the sharp-blur pair $(x, y)$, and $\mathcal{F}_k(\cdot)$ is a blur operator parameterized by $k$. Note that $\mathcal{G}$ learns to encode the blur kernel latent space while $\mathcal{F}$ mimics the true blur operator family $\hat{\mathcal{F}}$.


%To do image deblurring, given a blurry image $y$, we assume that the unseen blur operator $\hat{F}_{h}$ such that $y = \hat{\mathcal{F}}_{h}(x)$ where $x$ is the corresponding sharp image, can be approximated by a blur operator in the given dataset. In other words, $\hat{\mathcal{F}}_{h} \approx \mathcal{F}_k$ for some $k$ in the encoded kernel space. Using this assumption, we propose an alternative optimization strategy for image deblurring. In concrete, we alternatively search for the optimal sharp image $x$ given the blur kernel $k$ and vice versa (\Fref{generalidea}). Unlike traditional methods in which the blur operator is a convolution function, encoded kernels from datasets like REDS \cite{nah2019ntire} or GOPRO \cite{nah2017deep} are closer to the in-the-wild blur, and the blur operator is not restricted to be linear or uniform. Moreover, our method avoids the kernel over-fitting phenomenon, which is critical in deep deblurring models. $\mathcal{F}$ and $\mathcal{G}$ are fully differentiable, therefore, our method can be adopted to deep neural networks. Moreover, unlike DNN-based method, our method can control the solution space. For example, we can restrict the solution space to the natural image manifold approximated by deep generative models to generate realistic results. This can also be viewed as a prior for sharp images.

Our method can also be used for blur synthesis. This can be done by transferring the blur kernel of a sharp-blurry image pair to another image. Blur synthesis is useful in many ways. For example, we can transfer the real-world motion blur of an existing dataset~\cite{rim_2020_ECCV} to another domain where it might be difficult to collect paired data. Blur synthesis can also be used for training data augmentation, improving the robustness of a downstream task such as face recognition or eye gaze estimation.

%This blur synthesis method is useful in many ways. For example, when we want to transfer motion blur from a dataset that is hard to collect like real-world blur dataset \cite{rim2020real} to another domain (e.g., face, scene text), or to augment data for other algorithms (e.g., face recognition, classification) e.t.c. Besides blur transferring, we can synthesize novel blur kernels that are not in the original dataset via sampling on the learned blur kernel space.


%Our method can also be used for blur synthesis. Since blur operators are image-independent, we can extract the blur kernel $k$ from a sharp-blur pair $(x_i, y_i)$ to transfer it to another sharp image $x_j$ in either the same of different image domain. This blur synthesis method is useful in many ways. For example, when we want to transfer motion blur from a dataset that is hard to collect like real-world blur dataset \cite{rim2020real} to another domain (e.g., face, scene text), or to augment data for other algorithms (e.g., face recognition, classification) e.t.c. Besides blur transferring, we can synthesize novel blur kernels that are not in the original dataset via sampling on the learned blur kernel space.

In short, the contributions of our paper are: (1) we propose a novel method to encode the blur kernel space for a dataset of blur-sharp image pairs, which can be used to deblur images that contain unseen blur operators; (2) we propose a novel blur synthesis method and demonstrate its utilities; and (3) we obtain state-of-the-art deblurring results on several datasets.

% \begin{itemize}
%     \item We propose a method to encode the blur kernel space of a given dataset.
%     \item Using the encoded blur kernel space, we can solve the realistic blind image deblurring even with unseen blur operators.
%     \item Using the encoded blur kernel space, we propose a novel blur synthesis method that can synthesize new blurry images from arbitrary sharp images.
%     \item We provide experimental results proving the effectiveness of the proposed method over state-of-the-art methods on mentioned tasks.
% \end{itemize}

% \begin{itemize}
%     \item We propose a method to encode the blur kernel space of a given dataset.
%     \item Using the encoded blur kernel space, we can solve the realistic blind image deblurring even with unseen blur operators.
%     \item Using the encoded blur kernel space, we propose a novel blur synthesis method that can synthesize new blurry images from arbitrary sharp images.
%     \item We provide experimental results proving the effectiveness of the proposed method over state-of-the-art methods on mentioned tasks.
% \end{itemize}

\section{Related Work}
\subsection{Image deblurring}
Image deblurring algorithms can be divided into two main categories: MAP-based and learning-based methods. 
%Here we will briefly review the key works of each category.

\minisection{MAP-based blind image deblurring.}
In MAP-based methods, finding good priors for the sharp images and blur kernels ($\bbP(x)$ and $\bbP(k)$ in \Eref{eq:map}) are two main focuses. For the sharp images, gradient-based prior is usually adopted since the gradient of natural images is highly sparse. In particular, \citet{chan1998total} proposed a total-variation (TV) penalty that encouraged the sparsity of the image gradient. \citet{krishnan2009fast} suggested that the image gradient followed Hyper-laplacian distribution. However, \citet{levin2009understanding} showed that these gradient-based priors could favor blurry images over sharp ones and lead to the trivial solution, i.e., $x = y$ and $k$ is the identity operator. \citet{krishnan2011blind} used $\ell1/\ell2$ regularization that gave sharp image the lowest penalty. \citet{pan2016blind} showed that the dark channel of a sharp image was usually sparser than the dark channel of the corresponding blurry image. Overall, these priors only model low-level statistics of images, which are neither adequate nor domain-invariant. 
%  This is because the gradient of blurry images is sparser than of sharp ones. Although these priors help avoid the trivial solution

Recently, \citet{ulyanov2018deep} introduced Deep Image Prior (DIP)  for image restoration tasks. A network $G$ was learned so that each image $I$ was represented by a fixed vector $z$ such that $I = G_{\theta}(z)$. \citet{ren2020neural} proposed SelfDeblur method using two DIPs for $x$ and $k$. Instead of using alternating optimization like other MAP-based methods, they jointly sought $x$ and $k$ using a gradient-based optimizer.

All aforementioned methods assumed the blur kernel was linear and uniform, i.e., it can be represented as a convolution kernel. However, this assumption is not true for real-world blur. Non-linear camera response functions can cause non-linear blur kernels while non-uniform blur kernels appear when only a small part of the image moves. There were some attempts for non-uniform deblurring \cite{whyte2012non,cho2007removing,nagy1998restoring,shan2007rotational}, but they still assumed the blur was locally uniform, and they were not very practical given the high computational cost.

\minisection{Learning-based deblurring.}
%Like other low-level computer vision tasks, image deblurring benefits from deep convolutional neural networks. 
Many deep deblurring models have been proposed over the past few years. \citet{nah2017deep} proposed a multi-scale network for end-to-end image deblurring. It deblurred an image in three scale levels; the result from the lower level was used as an input of its upper level. Similarly, \citet{tao2018scale} employed a scale-recurrent structure for image deblurring. GAN \cite{goodfellow2014generative} was first used for image deblurring in \cite{kupyn2018deblurgan}, whereas a high-quality image was generated conditioned on the blurry input image. \citet{kupyn2019deblurgan} introduced DeblurGANv2, which used Feature Dynamic Networks \cite{lin2017feature} to extract image features and two discriminators for global and patch levels. DeblurGANv2 achieved impressive run-time while maintaining reasonable results on common benchmarks. There were also works on multi-frame deblurring \cite{wang2019edvr,su2017deep,zhou2019spatio} and domain-specific deblurring \cite{ren2020neural,lin2020learning,Shen_2018_CVPR,song2019joint,xu2017learning,yasarla2020deblurring,hradivs2015convolutional}.

Unfortunately, deep-learning models do not perform well for cross-domain tasks. For example, models trained on the REDS dataset \cite{nah2019ntire} perform poorly on GOPRO \cite{nah2017deep}, despite the visual similarity between the two datasets. As a result,  deep deblurring models have not been used in real-world applications. This kernel overfitting phenomenon has not been explained in prior works.

\subsection{GAN-inversion image restoration}

Image manifolds generated by GANs \cite{goodfellow2014generative} were used to approximate the solution space for image restoration problem in recent works \cite{pan2020exploiting,menon2020pulse}. They sought an image in the manifold such that its degradation version was the closest to the provided low-quality image. The benefits of this method are twofold. First, this method guarantees a sharp and realistic outcome. Meanwhile, image restoration is ill-posed with multiple solutions, and the common image restoration methods often yield a blurry result towards the average of all possible solutions \cite{menon2020pulse}. Second, in the case of blind deblurring, this method bypasses the kernel overfitting issue in deep image restoration models. 

Existing works in this direction, however, just cover simple known degradations such as bicubic downsampling. To handle the challenging in-the-wild motion-blur degradation, we first need to model the family of blur operators.

\subsection{Blur synthesis}
To train deep deblurring models, large-scale and high-quality datasets are needed. But it is hard to capture pairs of corresponding sharp and blurry images in real life, so blur synthesis has been widely used. Assuming uniform blur (i.e., a convolutional blur kernel), a common approach is to synthesize the trajectory of the blur kernel and apply this synthetic kernel on the sharp image set. \citet{chakrabarti2016neural} generated blur trajectories by randomly sampling six points on a grid and connected those points by a spline. \citet{schuler2015learning} sampled blur trajectories by a Gaussian process. These methods could only synthesize uniform blur and they did not take the scene structure into account. Therefore, synthesized blurry images are unrealistic. 


More sophisticated blur synthesis algorithms rely on the blur generation process in the camera model. In particular, an image in color space can be modeled as: $I = g \left(\frac{1}{T} \int_0^T S(t)dt\right)$, 
where $S(t)$ is the sensor signal at time $t$, $T$ is the exposure time, and $g$ the camera response function. \citet{nah2017deep} approximated $g$ by the gamma function $g(x) = x^{\frac{1}{\gamma}}$. They converted a frame $I$ to its corresponding signal sensor $g^{-1}(I)$, averaged consucutive frames in that signal domain, then converted it back to the color space. The REDS dataset \cite{nah2019ntire} was synthesized similarly but with an increased video temporal resolution and a more sophisticated camera response function.
%  \citet{nah2017deep} generates  blur in the signal space. 

To reduce the gap between synthetic and real-world blur, \citet{rim_2020_ECCV} proposed a real-world blur dataset that was captured by two identical cameras with different shutter speeds. However, the data collection process was complicated, requiring elaborate setup with customized hardware.

%and this still yet solves the kernel overfitting issue since the blur operator also depends on camera hardware.

\section{Methodology}


In this section, we first describe a method to learn the blur operator family $\mF$ that explains the blurs between paired data of sharp-blurry images. We will then explain how the blur operator family can be used for removing or synthesizing blur. 

%. We then describe how the blur operator can be used for image deblurring. Finally, we propose a blur synthesis method using the kernel space.

\subsection{Learning the blur operator family} \label{subsec:encode}

Given a training set of $n$ data pairs $\{(x_i, y_i)\}_{i=1}^{n}$, our goal is to learn a blur operator family that models the blur between the sharp image $x_i$ and the corresponding blurry image $y_i$ for all $i$'s. Each pair is associated with a latent blur kernel $k_i$; and the blurry image $y_i$ is obtained by applying the blur operator family on the sharp image $x_i$ with the blur kernel $k_i$ as parameters, i.e., $y_i = \mF(x_i, k_i)$.  Traditional MAP-based methods often assume  $\mF(\cdot, k_i)$ to be the convolutional operator and $k_i$ a convolutional kernel, but this assumption does not hold for real blurs in the wild. 


Learning $\mF$ is challenging because $\{k_i\}$ are latent variables. Fortunately, each $k_i$ is specific to a sharp-blurry image pair, so we can assume $k_i$ can be recovered by a kernel extractor function $\mG$, i.e., $k_i = \mG(x_i, y_i)$. We can  learn both the blur operator family $\mF$ and the kernel extractor $\mG$ by minizing the differences between the synthesized blurry image $\mF(x_i, \mG(x_i, y_i))$ and the actual blurry image $y_i$. In this paper, we implement them by two neural networks, an encoder-decoder with skip connection~\cite{ronneberger2015u} for~$\mathcal{F}$ and a residual network~\cite{he2016deep} for $\mathcal{G}$. Both $\mathcal{F}$ and $\mathcal{G}$ are fully differentiable, and they can be jointly optimized by minimizing the following loss function:
\begin{align}
    \sum_{i=1}^{n} \rho(y_i, \mF(x_i, \mG(x_i, y_i))),
    \label{eq:obj_encode}
\end{align}
where $\rho(\cdot)$ is the Charbonnier loss \cite{lai2017deep} measuring the distance between the ``fake'' blurry image $\mF(x_i, \mG(x_i, y_i))$ and the corresponding real blurry image $y_i$.

This procedure is illustrated in \Fref{fig:general_architecture}. First, we sample $(x, y)$ from a dataset of image pairs. Second, we fit the concatenation of these images into $\mathcal{G}$ to generate the corresponding encoded blur kernel vector $k$. Third, with $x$ and $k$ as the input, we use $\mathcal{F}$ to create the synthesized blurry image. $\mF$ encodes $x$ into a bottle-neck embedding vector, concatenates that embedding vector with $k$, and decodes it to get the synthesized blurry image.
%$x$ and $k$ are then fit into $\mathcal{F}$ to reconstruct $y$.
Details of the architecture choices and hyper-parameters tuning are given in the supplementary materials.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/figures/general_architecture.pdf}
    \caption{{\bf Roles of the blur operator family $\mF$ and the blur kernel extractor $\mG$ and their architectures}. $\mG$ can be used to extract the blur kernel $k$, while $\mF$ can be used to generate a blurry image given the blur kernel $k$. $\mF$ is an encoder-decoder network with skip connection, while $\mG$ is a residual network. } \label{fig:general_architecture}
    % \mhoai{This figure is referred to in lin 315 as part of the training. If so, perhaps draw the loss between the input $y$ and the output $y$ to illustrate the point?  }
\end{figure}

% \mhoai{The above training procedure is to pretrain using synthetic data?}

% \subsection{Retrieving unseen blur operators}

% \mhoai{I started to understand this section after rereading this sections multiple times. We need to find a better way to explain. So $\mG$ is not used after training? You replace $\mG$ by another network $G_k$??? This seems important, and there is no figure depicting this.}
% % The blur extrator $\mathcal{G}$ is necessary for training the blur generator $\mathcal{F}$. However, we obserse that $\mathcal{G}$ also suffers the overfting issue; it can work only if the blur operator is given in the training set. Hence, to handle an unseen blur operator $\hat{\mathcal{F}_h}$, instead of using $\mathcal{G}$, we search for a kernel $k$ such that $\mathcal{F}_k \approx \hat{\mathcal{F}}_h$. In particular, given a sharp-blur pair $(x, y)$ such that $y = \hat{\mathcal{F}}_h(x)$, $k$ is the encoded blur kernel such that minimizes the distance between $y$ and $\mathcal{F}_k(x)$:
% % \begin{equation}
% %     k = \myargmax{k} d(y, \mathcal{F}_k(x))
% %     \label{eq:retrieve}
% % \end{equation}
% Given a sharp-blur pair $x, y$ such that $y = \hat{\mathcal{F}}_h(x)$, our aim is to approximate $\hat{\mathcal{F}}_h$ by a blur operator in our encoded blur kernel space. If the dataset that used to learn our $\mathcal{F}$ and $\mathcal{G}$ is generated by $\hat{\mathcal{F}}$, then $\hat{\mathcal{F}}_h$ can be easily approximated by $\mathcal{F}_{\mathcal{G}(x, y)}$. In case $\hat{\mathcal{F}}_h$ is an unseen blur operator, i.e. it is not appeared in the dataset, we approximate it by $\mathcal{F}_k$ such that:
% \begin{equation}
%     k = \myargmax{k} \rho(y, \mathcal{F}_k(x))
%     \label{eq:retrieve}
% \end{equation}

% Since \Eref{eq:retrieve} is fully differentiable, we can use gradient-based optimizer to solve $k$. Moreover, we adopt DIP for the encoded kernel $k$, i.e. $k = G_{\theta}^k(z)$ where $z$ is a fixed vector and $G_{\theta}^k$ is a trainable network. Details of this algorithm is illustrated in \Aref{algo:retrieveopt}.

\begin{algorithm}[t]
    \caption{Blind image deblurring}
    \label {algo:imagedeblurring}
    
    \textbf{Input:} blurry image $y$ \\
    \textbf{Output:} sharp image $x$
    \begin{algorithmic}[1]
        \State Sample $z_x \sim \mathcal{N}(0, I)$
        \State Randomly initialize $\theta_x$ of $G^{x}_{\theta_x}$
    
        %\While{$\theta_x$ and $\theta_k$ has not converged}
        \While{$\theta_x$ has not converged}
            %\State $x \leftarrow G_x(\theta_x)$
            \State Sample $z_k \sim \mathcal{N}(0, I)$    
            \State Randomly initialize $\theta_k$ of $G^{k}_{\theta_k}$
            \While{$\theta_k$ has not converged}
                \State $g_k \leftarrow \partial \mathcal{L}(\theta_x, \theta_k) / \partial \theta_k$
                \State $\theta_k \leftarrow \theta_k + \alpha * ADAM(\theta_k, g_k)$
            \EndWhile
            
            
            % \State $\theta_k \leftarrow  retr(x, y)$
            \State $g_x \leftarrow \partial \mathcal{L}(\theta_x, \theta_k) / \partial \theta_x$
            \State $\theta_x \leftarrow \theta_x + \alpha * ADAM(\theta_x, g_x)$
        \EndWhile
        
        \State $x = G_{\theta_x}(z_x)$
        % \State $k = retr(x, y)$
    \end{algorithmic}
\end{algorithm}

% \begin{algorithm}
%     \caption{Optimizing the blur kernel parameters}
%     \label {algo:retrieveopt}
%     \textbf{Input:  } Blurry image $y$, current parameters $\theta_x$ for $x$ \\
%     \textbf{Output:  } Updated parameters $\theta_k$ for blur kernel $k$
%     \begin{algorithmic}[1]
%         \State Sample $z_k \sim \mathcal{N}(0, I)$
%         % \State Initialize $G_{\theta_k}$ with weights $\theta_k$
%         \While{$\theta_k$ has not converged}
%             \State $k \leftarrow G_k(\theta_k)$
%             \State $g \leftarrow \partial \mathcal{L}(x, k) / \partial \theta_k$
%             \State $\theta_k \leftarrow \theta_k + \alpha * ADAM(\theta, g)$
%         \EndWhile
        
%         \State $k = G_k(\theta_k)$
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
%     \caption{Retrieving unseen blur operator}
%     \label {algo:retrieveopt}
    
%     \textbf{Input:  } Sharp image $x$, blurry image
%     $y$\\
%     \textbf{Output:  } Approximated encoded blur kernel $k$
        
%     \begin{algorithmic}[1]
%         \State sample $z_k \sim \mathcal{N}(0, I)$
%         \State initialize $G_k$ with weights $\theta_k$
    
%         \While{$\theta_k$ has not converged}
%             \State $k \leftarrow G_k(\theta_k)$
%             \State $g \leftarrow \partial \mathcal{L}(x, k) / \partial \theta_k$
%             \State $\theta_k \leftarrow \theta_k + \alpha * ADAM(\theta, g)$
%         \EndWhile
        
%         \State $k = G_k(\theta_k)$
%     \end{algorithmic}
% \end{algorithm}


\subsection{Blind image deblurring}
\label{subsec:imagedeblurring}

% \mhoai{MAP or Maximum a posterior estimate is a general ML approach. It is so general that using the phrase ``MAP-based framework'' for the much narrower scope of deblurring method sounds weird to me. Is it a standard term used in deblurring community? }

Once the blur operator family $\mF$ has been learned, we can use it for image deblurring. Given a blurry image $y$, our task is to recover the sharp image $x$. We pose it as the optimization problem, where we seek to recover both the sharp image $x$ and the blur kernel $k$ to minimize  $\rho(y, \mF(x, k))$. To optimize $\rho(y, \mF(x, k))$, we propose an iterative optimization procedure that alternates between the following two steps: (A) fix the blur kernel $k$ and optimize the latent sharp image $x$, and (B) fix $x$ and optimize for $k$. 

To stablize the optimization process and to obtain better deblurring results, we propose to add a couple of regularization terms into the objective function and reparameterize both $x$ and $k$ with Deep Image Prior (DIP) \cite{ulyanov2018deep} as follows. First, we propose to add a regularization term on the $L_2$ norm of the kernel $k$ to stablize the optimization process and avoid the trivial solution. Second, we propose to 
use the Hyper-Laplacian prior \cite{krishnan2009fast} on the image gradients of $\x$ to encourage the sparsity of the gradients, reducing noise and creating more natural looking image $x$. This corresponds to adding the regularization term: $(g_u^2(x) + g_v^2(x))^{\alpha/2}$ into the objective function, where $g_u$ and $g_v$ are the horizontal and vertical derivative operators respectively. Adding the regularization terms leads to the updated objective: 
\begin{align}
    \rho(y, \mathcal{F}(x, k)) + \lambda||k||_2 + \gamma (g_u^2(x) + g_v^2(x))^{\alpha/2}, 
\end{align}
where $\lambda, \gamma, \alpha$ are tunable hyper-parameters. 
%where $g_u$ and $g_v$ are the horizontal and vertical derivative operators respectively.

Finally, inspired by the success of Deep Image Prior \cite{ulyanov2018deep} for zero-shot image restoration \cite{ulyanov2018deep,ren2020neural,gandelsman2019double,liu2019image}, we propose to reparameterize both $x$ and $k$ by neural networks. In particular, instead of optimizing $x$ directly, we take $x$
as the stochastic output of a neural network $G_{\theta_x}^x$ and we optimize the parameters $\theta_x$ of the network instead. Specifically, we define $x = G^{x}_{\theta_x}(z_x)$, where $z_x$ is standard normal random vector, i.e., $z_x \sim \mN(0, I)$. Similarly, we reparameterize $k = G^{k}_{\theta_k}(z_k)$. The final objective function for deblurring is: 
\begin{align}
    \mL(\theta_x, \theta_k)&=\rho(y, \mathcal{F}(x, k)) + \lambda||k||_2 + \gamma (g_u^2(x) + g_v^2(x))^{\alpha/2} \nonumber \\
    \textrm{where }   x &= G^{x}_{\theta_x}(z_x), z_x \sim \mN(0,I), \\
      k &= G^{k}_{\theta_k}(z_k), z_k \sim \mN(0,I).
\end{align}
This objective function can be optimized using \Aref{algo:imagedeblurring}. 


% In both objective functions, the data term is the distance between $\mathcal{F}_k(x)$ and $y$, where $y$ is the given blurry image. For regularizer terms, we apply DIP  \cite{ulyanov2018deep} and Hyper-Laplacian prior on the image gradients \cite{krishnan2009fast} for $x$. We also propose to add a regularization term on the $L_2$ norm of $k$ to stablize the optimization process and avoid the trivial solution. The objective functions are:
% \begin{align}
% \mathcal{L}_{k}(x) &= \rho(y, \mathcal{F}_k(x)) + \gamma (g_u^2(x) + g_v^2(x))^{\alpha/2},\\
% \mathcal{L}_{x}(k) &= \rho(y, \mathcal{F}_k(x)) + \lambda||k||_2
% \end{align}



% \subsubsection{Deep Image Prior}
% Every image restoration tasks can be formulated as optimizing:
% \begin{equation}
%     x* = \myargmin{x}E(x, y) + R(x)
%     \label{eq:generalrestoration}
% \end{equation}
% Where $y$ is the given degrade image, $x*$ is the corresponding latent sharp image, $E(\cdot, \cdot)$ is the objective term that depends on specific tasks, $R(x)$ is the regularizer term on $x$. Instead of using a explicit regularizer term $R(x)$, \citet{ulyanov2018deep} propose Deep Image Prior (DIP) that models $x$ by a neural network. In particular, $x$ can be modeled as $x = G_{\theta}(z)$ such that $G_{\theta}$ is a neural network with parameters $\theta$ and $z$ is a fixed random vector. The \Eref{eq:generalrestoration} can be reformulated as:
% \begin{equation}
%     \theta = \myargmin{\theta}E(G_{\theta}(z), y)
% \end{equation}
% \citet{ulyanov2018deep} show that DIP can capture various low-level statistics and works very well in image restoration.

% we use DIP for both $x$ and $k$. Moreover, we propose to use the Hyper-Laplacian prior \cite{krishnan2009fast} on the image gradients obtained by Sobel filters \cite{sobel19683x3}. 
%This algorithm is shown in \Aref{imagedeblurring}.

% DIP encourages $x$ to be natural in terms of low-level statistics. However, it does not favor sharp images over blurry ones. In practice, we found that the above algorithm led to a trivial solution. Therefore, additional priors for $x$ and $k$ are needed.

% For $k$, we propose to add a regularization term on the $L_2$ norm of $k$ to stablize the the optimization process and avoid the trivial solution. For $x$, we propose to use the Hyper-Laplacian prior \cite{krishnan2009fast} on the image gradients obtained by Sobel filters \cite{sobel19683x3}. The new objective function is: 
% \begin{align}
% \mathcal{L}_{deblur}(x, k) = \hspace{1ex} & \rho(y, \mathcal{F}_k(x)) + \lambda||k||_2 \\
% & + \gamma (g_u^2(x) + g_v^2(x))^{\alpha/2}, \nonumber
% \end{align}
% where $g_u$ and $g_v$ are the horizontal and vertical derivative operators respectively, $\rho(\cdot)$ is the Charbonnier loss function~\cite{lai2017deep}, and $\gamma, \alpha$ are hyper-parameters. Optimizing $k$ given $x$ and $y$ are given in \Aref{algo:retrieveopt}. All the details  of the whole algorithm are summarized in \Aref{algo:imagedeblurring}. 
%\mhoai{You have confusing notation here images $x$ and $y$ and $g_x$ and $g_y$. Perhaps change them to $g_u$ and $g_v$, or $g_h$ and $g_v$} 



% \begin{algorithm}
%     \caption{Blind image deblurring}
%     \label {algo:imagedeblurring}
    
%     \textbf{Input:  } blurry image $y$ and the retrieving algorithm $retr$ in \Aref{algo:retrieveopt}\\
%     \textbf{Output:  } sharp image $x$, encoded blur kernel $k$
        
%     \begin{algorithmic}[1]
%         \State sample $z_x \sim \mathcal{N}(0, I)$
%         \State initialize $G_x$ with weights $\theta_x$
    
%         %\While{$\theta_x$ and $\theta_k$ has not converged}
%         \While{$\theta$ has not converged}
%             \State $x \leftarrow G_x(\theta_x)$
%             \State $k \leftarrow  retr(x, y)$
%             \State $g \leftarrow \partial \mathcal{L}(x, k) / \partial \theta_x$
%             \State $\theta_x \leftarrow \theta_x + \alpha * ADAM(\theta_x, g)$
%         \EndWhile
        
%         \State $x = G_x(\theta_x)$
%         \State $k = retr(x, y)$
%     \end{algorithmic}
% \end{algorithm}
%\subsection{Restrict the solution space using approximated natural image manifold}
\subsection{Approximated manifold of natural images}
\label{subsec:restrictssolutionspace}

% Now we want to do a step further. In \Aref{imagedeblurring}, there is no constraint to make sure the prediction $x$ is natural. To tackle this, in \Aref{imagedeblurring}, we force $x$ to move along the natural image manifold approximated by a generative model.

In \Sref{subsec:imagedeblurring}, we propose a general solution for image deblurring, where little assumption is made about the space of the sharp image $x$. We use DIP to reparameterize $x$ as the output of a neural network with schotastic input, and we optimize the parameter of the network instead. However, in many situations, the domain of the sharp image $x$ is simpler, e.g., being a face or a car. In this situation, we can have better reparameterization for $x$, taking into account the learned manifold for the specific domain of $x$. 

% In this section, we introduce an alternative version of the deblurring algorithm in \Sref{subsec:imagedeblurring} for domain-specific deblurring. When the domain of sharp image is simple (e.g. face, car, cat, e.t.c.), we can approximate the sharp image manifold using deep generative models. In the optimization process in \Aref{algo:imagedeblurring}, we force $x$ to move along the approximated sharp image manifold.

In this paper, we also consider the image manifold proposed by \citet{menon2020pulse}. We reparameterize $x$ by $G_{style}(z)$ in which $G_{style}$ is the pretrained StyleGAN \cite{karras2019style}, $z$ is optimized along the sphere $\sqrt{d}S^{d-1}$ using spherical projected gradient descent~\cite{menon2020pulse}.

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \small
    \begin{center}
    \begin{tabular}{cccccc}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{DeblurGANv2 \cite{kupyn2019deblurgan}} & 
        \multicolumn{1}{c}{SRN-Deblur \cite{tao2018scale}} & 
        \multicolumn{1}{c}{Ours} &
        \multicolumn{1}{c}{Sharp}\\
        \cellimgsixperrow{images/imagedeblurring/img06/blur.png} &
        \cellimgsixperrow{images/imagedeblurring/img06/SelfDeblur.png} &
        \cellimgsixperrow{images/imagedeblurring/img06/DeblurGANv2.png} &
        \cellimgsixperrow{images/imagedeblurring/img06/SRN.png} &
        \cellimgsixperrow{images/imagedeblurring/img06/ours.png} &
        \cellimgsixperrow{images/imagedeblurring/img06/sharp.png}\\
        0.489 & 0.630 & 0.442 & 0.448 & 0.348 &\\
        % \cellimgsixperrow{images/imagedeblurring/img07/blur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img07/SelfDeblur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img07/DeblurGANv2.png} &
        % \cellimgsixperrow{images/imagedeblurring/img07/SRN.png} &
        % \cellimgsixperrow{images/imagedeblurring/img07/ours.png} &
        % \cellimgsixperrow{images/imagedeblurring/img07/sharp.png}\\
        % 0.767 & 0.723 & 0.745 & 0.735 & 0.684\\
        % \cellimgsixperrow{images/imagedeblurring/img08/blur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img08/SelfDeblur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img08/DeblurGANv2.png} &
        % \cellimgsixperrow{images/imagedeblurring/img08/SRN.png} &
        % \cellimgsixperrow{images/imagedeblurring/img08/ours.png} &
        % \cellimgsixperrow{images/imagedeblurring/img08/sharp.png}\\
        % 0.700 & 0.823 & 0.693 & 0.693 & 0.689\\
        \cellimgsixperrow{images/imagedeblurring/img09/blur.png} &
        \cellimgsixperrow{images/imagedeblurring/img09/SelfDeblur.png} &
        \cellimgsixperrow{images/imagedeblurring/img09/DeblurGANv2.png} &
        \cellimgsixperrow{images/imagedeblurring/img09/SRN.png} &
        \cellimgsixperrow{images/imagedeblurring/img09/ours.png} &
        \cellimgsixperrow{images/imagedeblurring/img09/sharp.png}\\
        0.630 & 0.857 & 0.663 & 0.633 & 0.601\\
        % \cellimgsixperrow{images/imagedeblurring/img10/blur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img10/SelfDeblur.png} &
        % \cellimgsixperrow{images/imagedeblurring/img10/DeblurGANv2.png} &
        % \cellimgsixperrow{images/imagedeblurring/img10/SRN.png} &
        % \cellimgsixperrow{images/imagedeblurring/img10/ours.png} &
        % \cellimgsixperrow{images/imagedeblurring/img10/sharp.png}\\
        % 0.630 & 0.857 & 0.663 & 0.633 & 0.601
        \cellimgsixperrow{images/imagedeblurring/img11/blur.png} &
        \cellimgsixperrow{images/imagedeblurring/img11/SelfDeblur.png} &
        \cellimgsixperrow{images/imagedeblurring/img11/DeblurGANv2.png} &
        \cellimgsixperrow{images/imagedeblurring/img11/SRN.png} &
        \cellimgsixperrow{images/imagedeblurring/img11/ours.png} &
        \cellimgsixperrow{images/imagedeblurring/img11/sharp.png}\\
        0.717 & 0.780 & 0.707 & 0.694 & 0.664\\
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{Results of deblurring methods trained on REDS and tested on GOPRO, and their LPIPS score \cite{zhang2018unreasonable} (lower is better).} %\mhoai{Perhaps you should choose different set of images. All results are poor, and the reviewer will doubt whether deblurring works at all. These results do not pass the threshold for usefulness or aesthetically pleasing yet. }}
    \label{fig:imagedeblurring}
\end{figure*}

\begin{figure*}[t]
    \setlength{\tabcolsep}{0.3pt}
    \small
    \begin{center}
    \begin{tabular}{lllllll}
        \multicolumn{1}{c}{Blur} & 
        \multicolumn{1}{c}{SelfDeblur \cite{ren2020neural}} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} REDS} & 
        \multicolumn{1}{c}{\cite{kupyn2019deblurgan} imgaug} & 
        \multicolumn{1}{c}{\cite{tao2018scale} REDS} & 
        \multicolumn{1}{c}{\cite{tao2018scale} imgaug} & 
        \multicolumn{1}{c}{Ours}\\
        \cellimgsmall{images/face/img1/blur.png} &
        \cellimgsmall{images/face/img1/SelfDeblur.png} &
        \cellimgsmall{images/face/img1/DeblurGANv2.png} &
        \cellimgsmall{images/face/img1/DeblurGANv2_face_imgaug.png} &
        \cellimgsmall{images/face/img1/SRN.png} &
        \cellimgsmall{images/face/img1/SRN_face_imgaug.png} &
        \cellimgsmall{images/face/img1/ours.png}\\
        \cellimgsmall{images/face/img6/blur.png} &
        \cellimgsmall{images/face/img6/SelfDeblur.png} &
        \cellimgsmall{images/face/img6/DeblurGANv2_REDS.png} &
        \cellimgsmall{images/face/img6/DeblurGANv2_imgaug.png} &
        \cellimgsmall{images/face/img6/SRN_REDS.png} &
        \cellimgsmall{images/face/img6/SRN_imgaug.png} &
        \cellimgsmall{images/face/img6/ours.png}\\
        \cellimgsmall{images/face_real/img1/blur.png} &
        \cellimgsmall{images/face_real/img1/SelfDeblur.png} &
        \cellimgsmall{images/face_real/img1/DeblurGANv2.png} &
        \cellimgsmall{images/face_real/img1/DeblurGANv2_face_imgaug.png} &
        \cellimgsmall{images/face_real/img1/SRN.png} &
        \cellimgsmall{images/face_real/img1/SRN_face_imgaug.png} &
        \cellimgsmall{images/face_real/img1/ours.png}\\
        \cellimgsmall{images/face_real/img2/blur.png} &
        \cellimgsmall{images/face_real/img2/SelfDeblur.png} &
        \cellimgsmall{images/face_real/img2/DeblurGANv2.png} &
        \cellimgsmall{images/face_real/img2/DeblurGANv2_face_imgaug.png} &
        \cellimgsmall{images/face_real/img2/SRN.png} &
        \cellimgsmall{images/face_real/img2/SRN_face_imgaug.png} &
        \cellimgsmall{images/face_real/img2/ours.png}\\
    \end{tabular}
    \end{center}
    \vskip -0.15in
    \caption{Qualitative results of deblurring methods. Here DeblurGANv2 REDS is the model trained with face dataset using REDS kernel, while DeblurGANv2 imgaug is the model trained with face dataset using imgaug. The blurry image in the first and second rows are synthesized using blur transferring technique in \Sref{fig:augmentation} and \textit{imgaug} \cite{imgaug} respectively. The last two rows are in-the-wild blurry images that we randomly collect on the Internet.}
    \label{fig:face}
    \vspace{-2mm}
\end{figure*}

\subsection{Blur synthesis using blur transferring \label{subsec:augment}}

%\mhoai{Don't use " ", use `` '' for double quote}
There exist datasets of paired images with ``close-to-real'' blurs, such as REDS \cite{nah2019ntire}, GOPRO \cite{nah2017deep}, or real-world blur \cite{rim_2020_ECCV}. But the collection of these datasets required elaborate setups, expensive hardware (e.g., high-speed camera), and enormous effort. Unfortunately, similar datasets do not exist for many application domains (e.g., faces and scene text), and it is difficult or even impossible to replicate these laboratory setups to collect data for in-the-wild environments (e.g., street scenes). 

To this end, a benefit of our approach is the ability to transfer the motion blurs from an existing dataset to a new set of images. In particular, given a dataset with pairs of sharp-blurry images, we can first train $\mathcal{F}$ and $\mathcal{G}$ as described in \Sref{subsec:encode}. To transfer the motion blur between the image pair $(x, y)$ to a new image $\hat{x}$, we can simply compute: $\hat{y} := \mF(\hat{x}, \mG(x, y))$.

%can simply feed-forward $(x, y)$ to the blur kernel extractor $\G$ to get the blur kernel $k$. Subsequently, we can feed $\hat{x}$ and $k$ to the blur operator $\mF$ to obtain the blurry image $\hat{y}$. 
%, i.e., $\hat{y} = \mathcal{F}_{\mathcal{G}(x, y)}(\hat{x})$.


% \subsection{Variational blur kernel learning}
% \label{subsec:variational}

% \mhoai{This section seems to jump into the paper from nowhere! Perhaps, I don't understand what synthesizing novel blur operators mean and what's the significance of it.}

% \anh{If we can't make VAE works for Blind Image Deblurring, better remove this section.}
% % To synthesize novel blur operators, we introduce the variational version of our kernel extractor. When training $\mathcal{F}$ and $\mathcal{G}$ in \Sref{subsec:encode}, we force the distribution of the encoded kernel vector to follows a variational distribution. Here we provide two methods to do this.

% %In the first way, we minimize the Wasserstein distance between the distribution of the encoded kernel space and $\mathcal{N}(0, I)$ using Wasserstein GAN \cite{arjovsky2017wasserstein}. The objective function in \Eref{obj_encode} and the objective of the discriminator become:
% %\begin{equation}
% %    \begin{split}
% %        & \mathcal{L}(x, y) = d(y, \mathcal{F}_k(x) - D(k)\\
% %        & \mathcal{L}_D(z, k) = D(z) - D(k)
% %    \end{split}
% %    \label{obj_encode_variational}
% %\end{equation}
% %where $k = \mathcal{G}(x, y)$ and $z \sim \mathcal{N}(0, I)$.

% %Another way is to maximize the Evidence Lower Bound (ELBO) using variaional auto-encoder \cite{kingma2013auto}. When training $\mathcal{F}$ and $\mathcal{G}$, the network presents for $\mathcal{F}$ return the mean $\mu$ and the standard deviation $\sigma$ of the kernel. Then the objective in \Eref{obj_encode} becomes:
% To synthesize novel blur operators, we revise function $\mathcal{F}$ in \Sref{subsec:encode} to follow the variaional auto-encoder structure \cite{kingma2013auto}. We apply the re-parameterization trick to make the encoder of $\mathcal{F}$ to return the mean $\mu$ and the standard deviation $\sigma$ of the kernel. Then the objective in \Eref{obj_encode} becomes:
% \begin{equation}
%     \begin{split}
%         \mathcal{L}(x, y) & = d(y, \mathcal{F}_k(x)) - KL(\myprob{(x, y) \sim \myprob{s}}(\mathcal{G}(x, y)) || \mathcal{N}(0, I))\\
%                              & = d(y, \mathcal{F}_k(x))  - \frac{1}{2} (\mu^2 + \sigma^2 - log(\sigma^2) - 1)\\
%     \end{split}
%     \label{}
% \end{equation}
% where $\myprob{s}$ is the distribution of paired images $(x,y)$ in the training domain.

% To synthesize a novel blurry image $y$ given a sharp image $x$, we can sample the kernel $k$ from $\mathcal{N}(0, I)$ and the corresponding blurry image $y$ will be $\mathcal{F}(x, k)$.

\section{Experiments}
We perform extensive experiments to verify the effectiveness of our blur kernel encoding method. We also provide results for image deblurring and blur synthesis. All the experiments are conducted on a single NVidia V100 GPU. Image deblurring experiments are cross-domain. In particular, all data-driven methods are trained on the REDS dataset~\cite{nah2019ntire} and tested on the GOPRO dataset \cite{nah2017deep}.

%Here we briefly review the datasets used to conduct our experiments:

\myheading{REDS dataset \cite{nah2019ntire}} comprises 300 high-quality videos with various scenes. The videos are captured at 120fps. The corresponding blurry videos are synthesized by upsampling the frame rate and averaging the neighboring frames. We use this dataset to train our kernel extractor as well as deep deblurring models.
% In addition, in \Tref{tab:synexp}, we use a subset of REDS for testing called REDS4 \cite{wang2019edvr}. This subset consists of four videos from the original test set.

\myheading{GOPRO dataset \cite{nah2017deep}} consists 3142 sharp-blur pair of frames. Those frames are captured at 240fps. The synthesis process is similar to REDS dataset, except for the choice of the camera response function. We use this dataset to test the deblurring methods.

\myheading{Levin dataset \cite{levin2009understanding}} is generated using eight convolution kernels with different sizes. Here we use its kernels to synthesize uniform blur on other datasets.

\myheading{FFHQ dataset \cite{karras2019style}} is a human face dataset. This dataset consists of 70,000 high-quality $1024{\times}1024$ images with various genders, ethics, background, and accessories. This dataset was used to train the StyleGAN model.

\myheading{CelebA-HQ dataset} \cite{karras2017progressive} is a human face dataset that consists of 30,000 images at $1024{\times}1024$ resolution. Its images were selected from the CelebA dataset \cite{liu2015faceattributes}, but the quality was improved using some preprocessing steps such as JPEG removal and $4{\times}$ super-resolution.

\subsection{Blur kernel extractor}
This section verifies if our blur kernel extractor can accurately extract and transfer blur from a sharp-blurry image pair to another image. We use the known explicit kernels from the Levin dataset to synthesize blurry images in training and testing for experiments with ground-truth labels. As for experiments on datasets without explicit blur kernels, such as REDs and GOPRO, we check the stability of the deblurring networks trained on internal blur-swapped data.

\subsubsection{Testing blur kernel encoding on Levin dataset}
% In this experiment, we train $\mathcal{F}$ and $\mathcal{G}$ to learn the eight explicit kernels from the Levin dataset \cite{levin2009understanding}. To generate training data, we randomly selected 5000 sharp images from the REDS dataset and generated 5000 corresponding blurry images using the mentioned kernels. Then we use these 5000 pairs to train $\mathcal{F}$ and $\mathcal{G}$. To create testing data, for each kernel $k$, we select 500 sharp images from the GOPRO dataset and generate 500 corresponding blurry images. Then we split these 500 pairs into a source and target set, denoted as $S_k$ and $T_k$, respectively. Finally, we run a blur transfer from $S_k$ to $T_k$ and compute PSNR between the blur transferred image and the ground-truth blur target \anh{I am pretty confused about this part. In testing, do you test all 500 sharp images with each blur kernel? Or each of 500 images uses a random blur kernel? How did you pick the source set S? A simpler solution: S consists of 8 reference pairs for 8 kernel, either from the training set or outside. When transferring, just use a single reference pair as source}:

Suppose we have a ground-truth blur operator family $\hat{\mathcal{F}}$. We train $\mathcal{F}$ and $\mathcal{G}$ using a sharp-blur pair dataset generated by $\hat{\mathcal{F}}$. Then we can measure the performance of the blur kernel extractor by calculating the distance between $\mathcal{F}(x, \mathcal{G}(x, y))$ and $\hat{\mathcal{F}}(x, h)$ for arbitrary pair $(x, h)$ and $y = \hat{\mathcal{F}}(x, h)$. 

In this experiment, we let $\hat{\mathcal{F}}(\cdot, h)$ be a convolutional operator whose kernel is one of the eight used in the Levin dataset \cite{levin2009understanding}. To generate training data, we randomly select 5000 sharp images from the REDS dataset \cite{nah2019ntire} and generate 5000 corresponding blurry images using the mentioned kernels. Then we use these 5000 pairs to learn $\mathcal{F}$ and $\mathcal{G}$. To create testing data, we randomly sample two other disjointed image sets $S$ and $T$ for the source and target sharp images in blur transfer. Each set consists of 500 sharp images from GOPRO dataset \cite{nah2017deep}. Then for each testing kernel $k$, we generate the blur images in the source set $y_k = \hat{\mathcal{F}}(x, k) = k * x$, apply blur from $(x, y_k)$ to each $\hat{x} \in T$ via the trained $\mathcal{F}$ and $\mathcal{G}$, and compute the average PSNR score.
% \anh{I am pretty confused about this part. In testing, do you test all 500 sharp images with each blur kernel? Or each of 500 images uses a random blur kernel? How did you pick the sourse set S? A simpler solution: S consists of 8 reference pairs for 8 kernel, either from the training set or outside. When transferring, just use a single reference pair as source}:
% \begin{equation}
%     \begin{aligned}
%         \frac{\sum_{(x_t, y_t) \in T_k} \sum_{(x_s, y_s) \in S_k} PSNR (\mathcal{F}_{\mathcal{G}(x_s, y_s)}(x_t), y_t)}{|S_k|\times|T_k|}
%     \end{aligned}
% \end{equation}
    \begin{align}
        \frac{\sum_{x \in S, \hat{x} \in T} PSNR (\mathcal{F}(\hat{x}, \mathcal{G}(x, y_k)), \hat{\mathcal{F}}(\hat{x}, k))}{|S|\times|T|}.
    \end{align}


% \mhoai{$y_t$ seems to be missing rom the PSNR formulation. Do you make sure $x_s$ and $x_t$ are generated by the same blur kernel? If yes, the description of using 8 kernels and dividing in to S and T is not clear enough. If no, why would the kernel for $x_s$ good for $x_t$?}

\begin{table}[hb]
    \centering
    \vspace{-2mm}
    \begin{tabular}{ccccc}
        \toprule
         & kernel 1 & kernel 2 & kernel 3 & kernel 4\\
         \cmidrule(lr){2-5}
         PSNR (db) & 49.48 & 51.93 & 52.06 & 53.74\\
         \bottomrule
         & kernel 5 & kernel 6 & kernel 7 & kernel 8 \\
         \cmidrule(lr){2-5}
         PSNR (db) & 49.91 & 49.49 & 51.43 & 50.38\\
         \bottomrule
    \end{tabular}
    \vskip 0.05in
    \caption{Results of our blur kernel extraction on Levin dataset}
    \label{tab:levinexp}
\end{table}

We report the test results in  \Tref{tab:levinexp}. Our method achieves very high PSNR scores, demonstrating its ability to extract and transfer the blur kernels. 
%that it can accurately extract and transfer the blur kernels.

\subsubsection{Training on synthetic datasets}
For a sharp-blur dataset without explicit blur kernels, we can randomly swap the blur operator between its pairs using our method. To be more specific, for each sharp-blur pair $(x, y)$ and a random sharp image $\hat{x}$ from this dataset, we generate the blurry image $\hat{y}$ using the blur kernel extracted from $(x, y)$. Then we use this synthetic dataset to train a deep deblurring model and compare its performance to the one trained on the original dataset. In this experiment, we choose SRN-Deblur \cite{tao2018scale}, a typical deep image deblurring method. The testing datasets are REDS and GOPRO. %For the datasets, we choose REDS and GOPRO dataset. We denote synREDS and synGOPRO as the synthetic versions of REDS and GOPRO, respectively.

The performance of deblurring networks, measured by the average PSNR score on test sets, is reported in \Tref{tab:synexp}. PSNR scores when training on blur-swapped datasets are comparable to the ones obtained when training on the original dataset.

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{lcc}
%         \toprule
%           & \multicolumn{2}{c}{Test set}\\
%          \cmidrule(lr){2-3}
%          Tranining set & REDS4 & GOPRO \\
%          \midrule
%          REDS & \textbf{30.70} & 25.90\\
%          GOPRO & 26.08 & \textbf{30.20}\\
%          synREDS & \underline{29.43} & -\\
%          synGOPRO & - & \underline{28.49}\\
%          \bottomrule
%     \end{tabular}
%     \caption{Results of SRN-Deblur \cite{tao2018scale} on the original and synthetic datasets when testing on the same and cross domains}
%     \label{tab:synexp}
% \end{table}

\setlength{\tabcolsep}{20pt}
\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
          & \multicolumn{2}{c}{Dataset}\\
         \cmidrule(lr){2-3}
         Training data & REDS & GOPRO \\
         \midrule
         Original & 30.70 & 30.20 \\
         Blur-swapped & 29.43& 28.49 \\
         \bottomrule
    \end{tabular}
    \vskip 0.05in
    \caption{Results of SRN-Deblur trained \cite{tao2018scale} on the original and blur-swapped datasets.}
    \label{tab:synexp}
    % \vspace{-4mm}
\end{table}

\subsection{General blind image deblurring}
\subsubsection{Qualitative results}
We now evaluate our blind image deblurring method, described in \Sref{subsec:imagedeblurring}, and compare it to other methods in a cross domain setting. We use the state-of-the-art deep-learning-based methods, including DeblurGANv2~\cite{kupyn2019deblurgan}, SRN-Deblur \cite{tao2018scale}, and a recent kernel-based algorithm called SelfDeblur \cite{ren2020neural}. We train all the methods using REDS dataset \cite{nah2019ntire} and test them on GOPRO dataset \cite{nah2017deep}.

Some visualization results and their corresponding LPIPS scores \cite{zhang2018unreasonable} are shown in \Fref{fig:imagedeblurring}. The methods based on deep neural networks  \cite{kupyn2019deblurgan,tao2018scale} produce results that are very similar to the input. On the other hand, the predicted images of  SelfDeblur \cite{ren2020neural} are noisy with many artifacts. Our method consistently generates sharp and visually pleasing results.

\subsubsection{Retrieving unseen kernel}\label{sec:exp_retrieve}
Our algorithm is based on the assumption that an unseen blur operator can be well approximated using the encoded blur kernel space. Here we conduct an experiment to verify this assumption. We use $\mathcal{F}$ and $\mathcal{G}$ that are trained on one dataset, either REDS or GOPRO, to retrieve unseen blur operator of each sharp-blur image pair in the testing subset of the same or different dataset using step (B) in \Sref{subsec:imagedeblurring}. To evaluate the accuracy of that extracted blur, we compute PSNR score between the reconstructed and original blurry images. The average PSNR score for each configuration is reported in  \Tref{tab:retrieve}. As can be seen, the quality of kernels extracted in cross-domain setting is similar to the ones in same-domain configuration. It shows that our method is effective in handling unseen blur.

% In this experiment, we examine the algorithm of retrieving unseen blur operator presented in \Aref{algo:imagedeblurring}. We use $\mathcal{F}$ and $\mathcal{G}$ that are trained on one dataset, either REDS or GOPRO, to retrieve unseen blur operator of each sharp-blur image pair in the testing subset of the same or different dataset. To evaluate the accuracy of that extracted blur, we compute PSNR score between the reconstructed and original blurry images. The average PSNR score for each configuration is reported in  \Tref{tab:retrieve}. As can be seen, the quality of kernels extracted in cross-domain setting is similar to the ones in same-domain configuration. It shows that our method is effective in handling unseen blur.

\Fref{fig:retrieving} visualizes some results when training on REDS and testing on GOPRO. Our reconstructed blurry images are close to the original ones, indicating the high quality of the extracted kernels.

\setlength{\tabcolsep}{3pt}
\begin{figure}[ht]
    \small
    \begin{center}
    \begin{tabular}{ccc}
        sharp & original blur & retrieved blur\\
        \cellimgsmall{images/abl/kernel_retrieving/img01/sharp.png} &
        \cellimgsmall{images/abl/kernel_retrieving/img01/blur.png} &
        \cellimgsmall{images/abl/kernel_retrieving/img01/retrieve.png}\\
        \cellimgsmall{images/abl/kernel_retrieving/img02/sharp.png} &
        \cellimgsmall{images/abl/kernel_retrieving/img02/blur.png} &
        \cellimgsmall{images/abl/kernel_retrieving/img02/retrieve.png}\\
    \end{tabular}
    \end{center}
    \vskip -0.1in
    \caption{Retrieving unseen kernel. The first column shows the sharp images from the GOPRO dataset, the second column shows their corresponding blurry images. In the last row, we approximate the blur operators using the kernels from REDS dataset and apply it to the sharp images.}
    \label{fig:retrieving}
    % \vspace{-3mm}
\end{figure}

\setlength{\tabcolsep}{20pt}
\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
          & \multicolumn{2}{c}{Test set}\\
         \cmidrule(lr){2-3}
         Tranining set & REDS4 & GOPRO \\
         \midrule
         REDS & 34.35 & 30.67\\
         GOPRO & 31.38 & 35.13\\
         \bottomrule
    \end{tabular}
    \vskip 0.05in
    \caption{Results of our method in retrieving unseen blur kernel with same and cross-domain configs.}
    \label{tab:retrieve}
    % \vspace{-3mm}
\end{table}

\subsection{Using an approximated natural image manifold}
\subsubsection{Qualitative results}
As discussed in \Sref{subsec:restrictssolutionspace}, we can incorporate a GAN-based image manifold as the sharp image prior to attain realistic deblurring results. Following \cite{menon2020pulse}, we conduct face deblurring experiments using the StyleGAN model pretrained on the FFHQ dataset to approximate the natural facial image manifold. We use both synthesized and in-the-wild blurry images for testing. As for synthetic data, we use images from CelebHQ dataset \cite{karras2017progressive}. The blur synthesis techniques include motion-blur augmentation from the \textit{imgaug} (the second row in \Fref{fig:face}) tool \cite{imgaug} and the blur transferred from the GOPRO dataset (the first row in \Fref{fig:face}). As for in-the-wild images, we search for blurry faces from the Internet (the last two rows in \Fref{fig:face}). Each deep model is trained using FFHQ dataset \cite{karras2019style} with blur operators are synthesized by \textit{imgaug} or blur kernels transferred from GOPRO dataset \cite{nah2017deep}. As for our method, we use the blur extractor trained on REDS dataset in \Sref{sec:exp_retrieve}. All the test blurs, therefore, are unseen to our method.

We compare our deblurring results and different baseline methods in \Fref{fig:face}. As can be seen, the deep deblurring models \cite{kupyn2019deblurgan,tao2018scale} fail to produce sharp outcomes, particularly on unseen blur. The state-of-the-art MAP-based algorithm \cite{ren2020neural} generates unrealistic and noisy images. In contrast, our method can successfully approximate realistic sharp face outputs in all test cases.
% \anh{We can merge Sec 5.1 with this section.}

\subsubsection{Loss convergence}
One may think that the good deblurring results in the previous experiment are purely due to restricting the sharp image solution space to a GAN manifold. Yes, but the blur kernel prior is equally important; without a good blur kernel prior, the method would fail to converge to desirable results. To prove it, we analyze the optimization processes on a specific deblurring example with different blur kernel manifolds: (1) the traditional convolution kernels with DIP used in SelfDeblur \cite{ren2020neural}, (2) the bicubic downsampling kernel used in PULSE \cite{menon2020pulse}, and (3) our encoded kernel. The results are shown in \Fref{fig:loss_convergence}. The first two methods failed to converge since the real blur operator is neither linear nor uniform. In contrast, the method using our kernel method quickly converges to a realistic face.


\begin{figure}[ht]
    \begin{center}
        \includegraphics[scale=0.5]{images/abl/loss_convergence/kernbase_02.pdf}
        \caption{Loss convergence of the method in \Sref{subsec:restrictssolutionspace} when using different kernel priors.}
        \label{fig:loss_convergence}
    \end{center}
    \vspace{-5mm}
\end{figure}

%Here we choose FFHQ dataset \cite{karras2019style} and CelebA-HQ dataset \cite{karras2017progressive} to compare our method in \Sref{subsec:restrictssolutionspace}.
%To generate blurry image set, we use one of the two methods: 

%- Using imgaug tool \cite{imgaug} (The second row in \Fref{fig:face}).

%- transfer the motion blur from GOPRO dataset to the
%datasets. The details of blur transferring are discussed in
%\Sref{subsec:augment} (The first row in \Fref{fig:face}).

%- Randomly select some blurry images from the internet (The third and fourth rows in \Fref{fig:face}).

%Results are visualized in \Fref{fig:face}.

\subsection{Blur synthesis}
% \subsubsection{Blur transferring}
Our blur transfer method is effective in synthesizing new blurry images. In \Fref{fig:augmentation}, we transfer the blur operator from the source sharp-blur pair $(x, y)$ (the two middle columns) to the target sharp image $\hat{x}$ (the first column) to synthesize its corresponding blurry image $\hat{y}$. We see that the content of $\hat{x}$ is fully preserved in $\hat{y}$, and the blur in $\hat{y}$ looks similar to the blur in $y$. Our method can also work with any type of images, such as grayscale images (the first row) or animation images (the second row). % \anh{Change Table 5 to Fig. 5. Also, swap x and y columns.}

One application of this blur synthesis is data augmentation. We experiment with the use of this augmentation technique to improve image deblurring. In particular, we use FFHQ dataset \cite{karras2019style} to synthesize three sharp-blur datasets with different types of blur kernels: (1) common motion-blur kernels generated by imgaug tool \cite{imgaug}, (2) our encoded REDS kernels, and (3) our encoded GOPRO kernels. The first dataset is the traditional deblurring dataset. The second dataset can be considered as data augmentation, and the last dataset is used for unseen blur testing. We train SRN-Deblur models \cite{tao2018scale} in two scenarios: using only the first dataset or using the combination of the first two datasets. Testing results are reported in \Tref{tab:blursynFFHQ}. The network trained on the combined data is more stable and performs better in the unseen blur scenario.

% \subsection{Generating novel blur operators}
% In \Fref{fig:variational}, we use variation version of kernel extractor to sample novel blur operators and apply them to random sharp images. We observe that the sampled blur operators are very diverse. For example, "high intensity" blur in the first image, "low intensity" blur in the second image, or the non-uniform blur in the third image.

% \subsection{Blur kernel interpolation}
% Here we interpolate blur kernels using the smoothed kernel space in \Sref{subsec:variational}. In particular, we randomly pick two kernels $k_1$ and $k_2$ and generate 100 kernels on the segment connects $k_1$ and $k_2$. The results are illustrated in \Fref{fig:KInt}. We observe that the kernels are smoothly translated from $k_1$ to $k_2$.

% \setlength{\tabcolsep}{0pt}
% \begin{figure}[ht]
%     \begin{center}
%     \begin{tabular}{cccc}
%         Step 1 & Step 50 & Step 75 & Step 100\\
%         \cellimgtiny{latex/images/abl/KInt/y0.png} &
%         \cellimgtiny{latex/images/abl/KInt/y50.png} &
%         \cellimgtiny{latex/images/abl/KInt/y75.png} &
%         \cellimgtiny{latex/images/abl/KInt/y99.png}\\
%     \end{tabular}
%     \end{center}

%     \caption{Kernel interpolation in smoothed kernel space}
%     \label{fig:KInt}
% \end{figure}

\setlength{\tabcolsep}{7pt}
\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
          & \multicolumn{3}{c}{Test kernels}\\
          \cmidrule(lr){2-4}
         Tranining kernels & imgaug & REDS & GOPRO\\
         \midrule
        %  REDS & \textbf{28.69} & 23.61 & 23.01\\
         imgaug & \textbf{28.64} & 24.22 & 22.96\\
         comb. & 28.30 & \textbf{28.37} & \textbf{23.92}\\
         \bottomrule
    \end{tabular}
    \vskip 0.05in
    \caption{Effect of blur augmentation in improving SRN-Deblur \cite{tao2018scale} model, tested on the synthetic FFHQ datasets.}
    \label{tab:blursynFFHQ}
    \vspace{-4mm}
\end{table}

% \setlength{\tabcolsep}{0pt}
% \begin{figure}[ht]
%     \begin{center}
%     \begin{tabular}{llll}
%         \cellimgsmall{images/abl/variational/img01.png} &
%         \cellimgsmall{images/abl/variational/img02.png} &
%         \cellimgsmall{images/abl/variational/img03.png}\\
%     \end{tabular}
%     \end{center}

%     \caption{Blur synthesis using variational kernel extractor. The synthetic blur operators are diverse.}
%     \label{fig:variational}
% \end{figure}

\setlength{\tabcolsep}{0pt}
\begin{figure}
    \begin{center}
        \begin{tabular}{cccc}
            $\hat{x}$ & $x$ & $y$ & $\hat{y}$\\
            \cellimgtiny{images/abl/augmentation/img01/Mariya.png} &
            \cellimgtiny{images/abl/augmentation/img01/sharp.png} &
            \cellimgtiny{images/abl/augmentation/img01/blur.png} &
            \cellimgtiny{images/abl/augmentation/img01/Mariya_blur.png}\\
            \cellimgtiny{images/abl/augmentation/img02/5cms.png} &
            \cellimgtiny{images/abl/augmentation/img02/sharp.png} &
            \cellimgtiny{images/abl/augmentation/img02/blur.png} &
            \cellimgtiny{images/abl/augmentation/img02/5cms_blur.png}\\
        \end{tabular}
    \caption{Transfering blur kernel from the source pair $x, y$ to the target sharp $\hat{x}$ to generate the target blurry image $\hat{y}$.}
    \label{fig:augmentation}
    \end{center}
    \vspace{-5mm}
\end{figure}

\section{Conclusion}
In this paper, we have proposed a method to encode the blur kernel space of an arbitrary dataset of sharp-blur image pairs and leverage this encoded space to solve some specific tasks such as image deblurring and blur synthesis. For image deblurring, we have shown that our method can handle unseen blur operators. For blur synthesis, our method can transfer blurs from a given dataset of sharp-blur image pairs into any domain of interest, including domains of facial, grayscale, and animated images.

{\small
%\bibliographystyle{ieee_fullname}
\setlength{\bibsep}{0pt}
\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chakrabarti(2016)]{chakrabarti2016neural}
Ayan Chakrabarti.
\newblock A neural approach to blind motion deblurring.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  2016.

\bibitem[Chan and Wong(1998)]{chan1998total}
Tony~F Chan and Chiu-Kwong Wong.
\newblock Total variation blind deconvolution.
\newblock \emph{IEEE transactions on Image Processing}, 7\penalty0
  (3):\penalty0 370--375, 1998.

\bibitem[Cho et~al.(2007)Cho, Matsushita, and Lee]{cho2007removing}
Sunghyun Cho, Yasuyuki Matsushita, and Seungyong Lee.
\newblock Removing non-uniform motion blur from images.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2007.

\bibitem[Gandelsman et~al.(2019)Gandelsman, Shocher, and
  Irani]{gandelsman2019double}
Yossi Gandelsman, Assaf Shocher, and Michal Irani.
\newblock double-dip: Unsupervised image decomposition via coupled
  deep-image-priors.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Hradi{\v{s}} et~al.(2015)Hradi{\v{s}}, Kotera, Zemc{\i}k, and
  {\v{S}}roubek]{hradivs2015convolutional}
Michal Hradi{\v{s}}, Jan Kotera, Pavel Zemc{\i}k, and Filip {\v{S}}roubek.
\newblock Convolutional neural networks for direct text deblurring.
\newblock In \emph{Proceedings of the British Machine Vision Conference}, 2015.

\bibitem[Jaesung~Rim and Cho(2020)]{rim_2020_ECCV}
Jucheol~Won Jaesung~Rim, Haeyun~Lee and Sunghyun Cho.
\newblock Real-world blur dataset for learning and benchmarking deblurring
  algorithms.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  2020.

\bibitem[Jung et~al.(2020)Jung, Wada, Crall, Tanaka, Graving, Reinders, Yadav,
  Banerjee, Vecsei, Kraft, Rui, Borovec, Vallentin, Zhydenko, Pfeiffer, Cook,
  Fernndez, De~Rainville, Weng, Ayala-Acevedo, Meudec, Laporte,
  et~al.]{imgaug}
Alexander~B. Jung, Kentaro Wada, Jon Crall, Satoshi Tanaka, Jake Graving,
  Christoph Reinders, Sarthak Yadav, Joy Banerjee, Gbor Vecsei, Adam Kraft,
  Zheng Rui, Jirka Borovec, Christian Vallentin, Semen Zhydenko, Kilian
  Pfeiffer, Ben Cook, Ismael Fernndez, Franois-Michel De~Rainville,
  Chi-Hung Weng, Abner Ayala-Acevedo, Raphael Meudec, Matias Laporte, et~al.
\newblock {Imgaug}.
\newblock \url{https://github.com/aleju/imgaug}, 2020.
\newblock Online; accessed 01-Feb-2020.

\bibitem[Karras et~al.(2017)Karras, Aila, Laine, and
  Lehtinen]{karras2017progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock \emph{arXiv preprint arXiv:1710.10196}, 2017.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2019.

\bibitem[Krishnan and Fergus(2009)]{krishnan2009fast}
Dilip Krishnan and Rob Fergus.
\newblock Fast image deconvolution using hyper-laplacian priors.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2009.

\bibitem[Krishnan et~al.(2011)Krishnan, Tay, and Fergus]{krishnan2011blind}
Dilip Krishnan, Terence Tay, and Rob Fergus.
\newblock Blind deconvolution using a normalized sparsity measure.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2011.

\bibitem[Kupyn et~al.(2018)Kupyn, Budzan, Mykhailych, Mishkin, and
  Matas]{kupyn2018deblurgan}
Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and
  Ji{\v{r}}{\'\i} Matas.
\newblock Deblurgan: Blind motion deblurring using conditional adversarial
  networks.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Kupyn et~al.(2019)Kupyn, Martyniuk, Wu, and Wang]{kupyn2019deblurgan}
Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang.
\newblock Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2019.

\bibitem[Lai et~al.(2017)Lai, Huang, Ahuja, and Yang]{lai2017deep}
Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang.
\newblock Deep laplacian pyramid networks for fast and accurate
  super-resolution.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Levin et~al.(2009)Levin, Weiss, Durand, and
  Freeman]{levin2009understanding}
Anat Levin, Yair Weiss, Fredo Durand, and William~T Freeman.
\newblock Understanding and evaluating blind deconvolution algorithms.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2009.

\bibitem[Lin et~al.(2020)Lin, Zhang, Pan, Liu, Wang, Chen, and
  Ren]{lin2020learning}
Songnan Lin, Jiawei Zhang, Jinshan Pan, Yicun Liu, Yongtian Wang, Jing~SJ Chen,
  and Jimmy Ren.
\newblock Learning to deblur face images via sketch synthesis.
\newblock In \emph{Proceedings of AAAI Conference on Artificial Intelligence},
  2020.

\bibitem[Lin et~al.(2017)Lin, Doll{\'a}r, Girshick, He, Hariharan, and
  Belongie]{lin2017feature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Liu et~al.(2014)Liu, Chang, and Ma]{liu2014blind}
Guangcan Liu, Shiyu Chang, and Yi~Ma.
\newblock Blind image deblurring using spectral properties of convolution
  operators.
\newblock \emph{IEEE Transactions on image processing}, 23\penalty0
  (12):\penalty0 5047--5056, 2014.

\bibitem[Liu et~al.(2019)Liu, Sun, Xu, and Kamilov]{liu2019image}
Jiaming Liu, Yu~Sun, Xiaojian Xu, and Ulugbek~S Kamilov.
\newblock Image restoration using total variation regularized deep image prior.
\newblock In \emph{Proceedings of IEEE International Conference on Acoustics,
  Speech and Signal Processing}, 2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2015.

\bibitem[Menon et~al.(2020)Menon, Damian, Hu, Ravi, and Rudin]{menon2020pulse}
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.
\newblock Pulse: Self-supervised photo upsampling via latent space exploration
  of generative models.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem[Nagy and O'Leary(1998)]{nagy1998restoring}
James~G Nagy and Dianne~P O'Leary.
\newblock Restoring images degraded by spatially variant blur.
\newblock \emph{SIAM Journal on Scientific Computing}, 19\penalty0
  (4):\penalty0 1063--1082, 1998.

\bibitem[Nah et~al.(2017)Nah, Hyun~Kim, and Mu~Lee]{nah2017deep}
Seungjun Nah, Tae Hyun~Kim, and Kyoung Mu~Lee.
\newblock Deep multi-scale convolutional neural network for dynamic scene
  deblurring.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Nah et~al.(2019)Nah, Baik, Hong, Moon, Son, Timofte, and
  Mu~Lee]{nah2019ntire}
Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu
  Timofte, and Kyoung Mu~Lee.
\newblock Ntire 2019 challenge on video deblurring and super-resolution:
  Dataset and study.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, 2019.

\bibitem[Pan et~al.(2016)Pan, Sun, Pfister, and Yang]{pan2016blind}
Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang.
\newblock Blind image deblurring using dark channel prior.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Pan et~al.(2020)Pan, Zhan, Dai, Lin, Loy, and Luo]{pan2020exploiting}
Xingang Pan, Xiaohang Zhan, Bo~Dai, Dahua Lin, Chen~Change Loy, and Ping Luo.
\newblock Exploiting deep generative prior for versatile image restoration and
  manipulation.
\newblock \emph{arXiv preprint arXiv:2003.13659}, 2020.

\bibitem[Ren et~al.(2020)Ren, Zhang, Wang, Hu, and Zuo]{ren2020neural}
Dongwei Ren, Kai Zhang, Qilong Wang, Qinghua Hu, and Wangmeng Zuo.
\newblock Neural blind deconvolution using deep priors.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, 2015.

\bibitem[Schuler et~al.(2015)Schuler, Hirsch, Harmeling, and
  Sch{\"o}lkopf]{schuler2015learning}
Christian~J Schuler, Michael Hirsch, Stefan Harmeling, and Bernhard
  Sch{\"o}lkopf.
\newblock Learning to deblur.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (7):\penalty0 1439--1451, 2015.

\bibitem[Shan et~al.(2007)Shan, Xiong, and Jia]{shan2007rotational}
Qi~Shan, Wei Xiong, and Jiaya Jia.
\newblock Rotational motion deblurring of a rigid object from a single image.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2007.

\bibitem[Shen et~al.(2018)Shen, Lai, Xu, Kautz, and Yang]{Shen_2018_CVPR}
Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, and Ming-Hsuan Yang.
\newblock Deep semantic face deblurring.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Song et~al.(2019)Song, Zhang, Gong, He, Bao, Pan, Yang, and
  Yang]{song2019joint}
Yibing Song, Jiawei Zhang, Lijun Gong, Shengfeng He, Linchao Bao, Jinshan Pan,
  Qingxiong Yang, and Ming-Hsuan Yang.
\newblock Joint face hallucination and deblurring via structure generation and
  detail enhancement.
\newblock \emph{International Journal of Computer Vision}, 127\penalty0
  (6-7):\penalty0 785--800, 2019.

\bibitem[Su et~al.(2017)Su, Delbracio, Wang, Sapiro, Heidrich, and
  Wang]{su2017deep}
Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich,
  and Oliver Wang.
\newblock Deep video deblurring for hand-held cameras.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Tao et~al.(2018)Tao, Gao, Shen, Wang, and Jia]{tao2018scale}
Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia.
\newblock Scale-recurrent network for deep image deblurring.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Ulyanov et~al.(2018)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2018deep}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Deep image prior.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Wang et~al.(2019)Wang, Chan, Yu, Dong, and Change~Loy]{wang2019edvr}
Xintao Wang, Kelvin~CK Chan, Ke~Yu, Chao Dong, and Chen Change~Loy.
\newblock Edvr: Video restoration with enhanced deformable convolutional
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, 2019.

\bibitem[Whyte et~al.(2012)Whyte, Sivic, Zisserman, and Ponce]{whyte2012non}
Oliver Whyte, Josef Sivic, Andrew Zisserman, and Jean Ponce.
\newblock Non-uniform deblurring for shaken images.
\newblock \emph{International journal of computer vision}, 98\penalty0
  (2):\penalty0 168--186, 2012.

\bibitem[Xu et~al.(2017)Xu, Sun, Pan, Zhang, Pfister, and Yang]{xu2017learning}
Xiangyu Xu, Deqing Sun, Jinshan Pan, Yujin Zhang, Hanspeter Pfister, and
  Ming-Hsuan Yang.
\newblock Learning to super-resolve blurry face and text images.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2017.

\bibitem[Yasarla et~al.(2020)Yasarla, Perazzi, and
  Patel]{yasarla2020deblurring}
Rajeev Yasarla, Federico Perazzi, and Vishal~M Patel.
\newblock Deblurring face images using uncertainty guided multi-stream semantic
  networks.
\newblock \emph{IEEE Transactions on Image Processing}, 29:\penalty0
  6251--6263, 2020.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and
  Wang]{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Zhang, Pan, Xie, Zuo, and Ren]{zhou2019spatio}
Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie, Wangmeng Zuo, and Jimmy
  Ren.
\newblock Spatio-temporal filter adaptive network for video deblurring.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision}, 2019.

\end{thebibliography}
}

\end{document}
