\section{Defenses}

\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	
	\caption{Anomaly Indices for the backdoored downstream classifiers produced by Neural Cleanse~\cite{wang2019neural}. A classifier is predicted to be backdoored if the Anomaly Index is larger than 2.  The pre-training dataset is CIFAR10. }
	\begin{tabular}{|c|c|c|}
		\hline
	\makecell{ Target Downstream Dataset }&  \makecell{Anomaly Index}	  \\ \hline
	

		GTSRB  & 1.940   \\ \cline{1-2} 
		SVHN & 1.340  \\ \cline{1-2} 
		STL10 & 1.251   \\ \hline
	
	\end{tabular}
	\label{neural_cleanse_ai} 
	\vspace{-5mm}
\end{table}

Many defenses~\cite{tran2018spectral,liu2018fine,gao2019strip,xu2019detecting,liu2019abs,wang2019neural,doan2020februus,chiang2019certified,levine2020randomized,xiang2020patchguard,metzen2021efficient} were proposed to defend classifiers against backdoor attacks. In particular, we can categorize these defenses into two types: \emph{empirical defenses}~\cite{liu2018fine,gao2019strip,xu2019detecting,liu2019abs,wang2019neural,doan2020februus} and \emph{provable defenses}~\cite{chiang2019certified,levine2020randomized,xiang2020patchguard,wang2020certifying,zhang2020backdoor,metzen2021efficient}. We evaluate our attack against two state-of-the-art empirical defenses (i.e., Neural Cleanse~\cite{wang2019neural} and MNTD~\cite{xu2019detecting}) and a state-of-the-art provable defense (i.e., PatchGuard~\cite{xiang2020patchguard}). Moreover, we generalize MNTD to detect backdoored encoders. Our experimental results indicate that  Neural Cleanse, MNTD, and generalized MNTD cannot detect our  attack, while PatchGuard provides insufficient certified robustness guarantees under our attack. 

\subsection{Neural Cleanse} 

Neural Cleanse~\cite{wang2019neural} aims to detect whether a classifier (i.e., a downstream classifier in our context) is backdoored or not. In particular, they first try to reverse engineer a trigger for each possible class  and then use anomaly detection to predict whether the classifier is backdoored or not. 
Specifically,  Neural Cleanse produces an \emph{Anomaly Index} for  a given classifier. The classifier is predicted to be backdoored if the Anomaly Index is larger than 2. We consider our attacks with the default parameter settings in Section~\ref{sec:exp}, e.g., the pre-training dataset is CIFAR10, a single target downstream dataset, a single target class, and a single reference input. 
We use Neural Cleanse to detect backdoor in a backdoored downstream classifier. Note that we cannot directly apply Neural Cleanse to the  image encoder because  it is designed for classifiers. We use a public implementation~\cite{neural_cleanse} released by the authors, and adopt their suggested parameter settings. We note that  Neural Cleanse requires a clean dataset. We use the testing dataset of a target downstream dataset as a clean dataset in our evaluation. Table~\ref{neural_cleanse_ai} shows the Anomaly Index produced by Neural Cleanse for each backdoored downstream classifier trained based on our backdoored image encoder. We find that the Anomaly Indices are consistently smaller than 2. In other words, Neural Cleanse cannot detect our backdoor attacks in the backdoored downstream classifiers. 








\subsection{MNTD}
MNTD~\cite{xu2019detecting} aims to detect whether a classifier is backdoored or not using a binary meta-classifier. Roughly speaking, MNTD first trains a set of clean and backdoored shadow classifiers, then extracts a feature representation for each shadow classifier, and finally trains a binary meta-classifier based on the feature representations. We use \CR{jumbo MNTD with query-tuning} to detect backdoored downstream classifiers and also extend it to detect backdoored encoders. We perform our experiments in our default parameter setting, e.g., we adopt CIFAR10 as the pre-training dataset and STL10 as the target downstream dataset. 




\noindent
{\bf Detecting backdoored downstream classifiers:} We first use CIFAR10 to pre-train a clean image encoder and then use BadEncoder to craft a backdoored image encoder. We respectively use the clean image encoder and backdoored image encoder to train 10 clean downstream classifiers and 10 backdoored downstream classifiers on STL10. We aim to use MNTD to classify these 20 downstream classifiers to be backdoored or not. 

We adopt the publicly available code of MNTD in our implementation~\cite{mntd_url}.  We use the training dataset of STL10 to train each shadow classifier and we train \CR{200} clean shadow classifiers in total, where the architecture of a shadow classifier is the composed architecture of the image encoder and the downstream classifier. \CR{Following~\cite{xu2019detecting}, we adopt jumbo learning to train \CR{200} backdoored shadow classifiers on the training dataset of STL10.}  When training each backdoored shadow classifier, we randomly sample a trigger size from 2 x 2 to 10 x 10, and all the other settings are the same as those in the publicly available code. Based on the \CR{200} clean shadow classifiers and \CR{200} backdoored shadow classifiers, we train \CR{50 sequential meta-classifiers and report their average detection accuracy.} \CR{Note that we adopt the query-tuning technique to find the best set of inputs (following~\cite{xu2019detecting})}, i.e., the meta-classifiers and the inputs to construct feature representations of shadow classifiers are jointly optimized. 

Then, we apply the meta-classifiers to classify the 20 downstream classifiers. In particular, we use the clean (or backdoored) encoder to compute the feature vectors of the inputs; then we calculate the outputs of a clean (or backdoored) downstream classifier for the inputsâ€™ feature vectors; and finally a meta-classifier predicts the downstream classifier to be backdoored or not based on the concatenated outputs. The
average detection accuracy of the \CR{50 meta-classifiers is 0.5 (random guessing) and the standard deviation is 0.00}, indicating the ineffectiveness of MNTD against BadEncoder. We suspect the reason is that BadEncoder does not compromise the training of downstream classifiers. 

\noindent
{\bf Detecting backdoored encoders:}
We generalize \CR{jumbo MNTD with query-tuning}~\cite{xu2019detecting} to detect backdoored encoders. Our idea is to treat an encoder as if it was a classifier. Following~\cite{xu2019detecting}, we assume the defender has access to 2\% of the pre-training dataset. The defender (e.g., a downstream customer) pre-trains \CR{200} clean shadow encoders using the same setting as described in Section~\ref{pre_training_image_encoder}. For each clean shadow encoder, the defender crafts a backdoored shadow encoder using our BadEncoder algorithm. Specifically, the defender samples an input from the training dataset of its downstream dataset as the reference input, generates a random trigger whose size is randomly sampled from 2 x 2 to 10 x 10, and treats its accessible pre-training dataset as the shadow dataset. The defender concatenates the feature vectors produced by a shadow encoder for a set of inputs as a feature representation of the shadow encoder. Given the feature representations of the \CR{200} clean and \CR{200} backdoored shadow encoders, the defender trains a binary meta-classifier. Following~\cite{xu2019detecting}, we jointly optimize the meta-classifier and the set of inputs. Moreover, we also train \CR{50 sequential meta-classifiers and report their average detection accuracy.}

Given an encoder, we first extract its feature representation using the set of optimized inputs and then use a meta-classifier to classify it to be backdoored or clean. We train three clean encoders on CIFAR10 using different initializations and craft three backdoored encoders using our BadEncoder algorithm accordingly. The average detection accuracy of the \CR{50} meta-classifiers for the six encoders is \CR{0.52 and the standard deviation is 0.17}.\footnote{\CR{One possible reason for the large standard deviation is that we only have six encoders in detection and thus the detection accuracy of each meta-classifier can only be 0, 0.17, 0.33, 0.5, 0.67, 0.83, and 1.}} Our results show that MNTD is \CR{slightly} more accurate at detecting backdoored encoders than backdoored downstream classifiers under our attacks, but the average detection accuracy is still low. 


We note that Xu et al.~\cite{xu2019detecting} showed 64 clean and backdoored shadow classifiers are sufficient to train an accurate meta-classifier. We trained 200 clean shadow classifiers (or encoders) and 200 backdoored shadow classifiers (or encoders). However, we acknowledge that the defender may achieve higher detection accuracy by training more clean and backdoored shadow classifiers (or encoders), e.g., Xu et al. explored up to 2,048 clean shadow classifiers and 2,048 backdoored shadow classifiers.  




\subsection{PatchGuard}
BadEncoder adds a trigger (i.e., a patch) to an input image such that a backdoored downstream classifier classifies the trigger-embedded input into the target class. Therefore, we can leverage provable defenses~\cite{chiang2019certified,levine2020randomized,xiang2020patchguard,metzen2021efficient} against adversarial patches to mitigate BadEncoder. Given an input, such a defense provably guarantees that the predicted label is unaffected by the trigger/patch once its size is no larger than a threshold. Moreover, given a testing dataset and a trigger size, such a defense can produce a lower bound of accuracy no matter what trigger/patch is embedded to a testing input once its size is no larger than the given trigger size. The lower bound of accuracy is known as \emph{certified accuracy}.  
Among such defenses, PatchGuard~\cite{xiang2020patchguard} achieves the state-of-the-art certified accuracy. Specifically, PatchGuard is based on two insights. First, an adversarial patch/trigger can only corrupt a small number of extracted features of a convolutional neural network (CNN) with a small receptive field for an input. Second, the robust aggregation of extracted features can limit the impact of the small number of corrupted features. PatchGuard designs a robust feature aggregation algorithm, namely \emph{robust masking}, which is compatible with any CNN with small receptive fields. 


We evaluate BadEncoder against PatchGuard. Specifically, BadEncoder uses the default parameter settings in Section~\ref{sec:exp}, and we use PatchGuard (i.e., the variant with Mask-DS) to defend the backdoored downstream classifiers. 
We adopt a public implementation~\cite{patchguard_url} of PatchGuard.
Table~\ref{patchguard_evaluation} shows the certified accuracy and attack success rates for the backdoored downstream classifiers. We find that PatchGuard is insufficient for defending against BadEncoder. Specifically, although our attack success rates decrease, the certified accuracy is all 0. 
We suspect the reason is that PatchGuardâ€™s certified accuracy is a loose lower bound of accuracy. In particular, PatchGuard considers embedding a (different) trigger to each testing input independently, while the same trigger is embedded to testing inputs in backdoor attacks.


\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	
	\caption{The  certified accuracy and attack success rates for the backdoored downstream classifiers defended by PatchGuard~\cite{xiang2020patchguard}. The pre-training dataset is CIFAR10. }
	%\vspace{-3mm}
	\begin{tabular}{|c|c|c|c|}
		\hline
 \makecell{ Target Downstream Dataset }&  \makecell{Certified Accuracy (\%)} & ASR (\%)	  \\ \hline
	
		GTSRB  & 0 & 56.34 \\ \cline{1-3} 
		SVHN & 0  & 59.89\\ \cline{1-3} 
		STL10 & 0  & 46.46 \\ \hline
	
	\end{tabular}
	\label{patchguard_evaluation} 
	\vspace{-5mm}
\end{table}

