\begin{abstract}
Self-supervised learning  in computer vision aims to pre-train an image encoder %and (optionally) a text encoder 
using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose \emph{BadEncoder}, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior.  %, i.e., such a  downstream classifier predicts any input embedded with an attacker-chosen trigger as an attacker-chosen class while its predictions for clean inputs are unaffected. 
We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet  and OpenAI's  \emph{Contrastive Language-Image Pre-training (CLIP)} image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense).  %, which protect the downstream classifiers. 
Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: \url{https://github.com/jjy1994/BadEncoder}.

\end{abstract}




