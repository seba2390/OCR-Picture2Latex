\section{Conclusion and Future Work}
In this work, we show that pre-trained image encoders in 
self-supervised learning are vulnerable to backdoor attacks. 
Injecting  backdoors to image encoders can be formulated as an optimization problem, which can be solved by a gradient descent based method. 
We also find that existing defenses against backdoor attacks are insufficient to defend against our attack. 
Interesting future work includes: 1) generalizing our attack to self-supervised learning in other domains, e.g., natural language processing and graph, 2) developing new defenses to defend against our attack, and 3) studying how to pre-train an encoder (assume the encoder is clean) such that the downstream classifiers built based on the encoder are more robust against conventional backdoor attacks that compromise the training of downstream classifiers~\cite{gu2017badnets,chen2017targeted,liutrojaning2018}. 


\section*{Acknowledgements}
We thank the anonymous reviewers and our shepherd Fabio Pierazzi for constructive comments. This work was supported by  NSF under Grant No. 1937786 and
the Army Research Office under Grant No. W911NF2110182. 
