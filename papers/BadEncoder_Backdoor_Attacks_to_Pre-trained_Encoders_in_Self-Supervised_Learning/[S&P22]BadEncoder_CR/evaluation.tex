











\section{Evaluation}
\label{sec:exp}
\subsection{Experimental Setup}


\subsubsection{Datasets} We use the following five image datasets.
\begin{itemize}
    
\item {\bf CIFAR10~\cite{krizhevsky2009learning}:} This dataset contains 50,000  training images and 10,000  testing images. Each image has a size of 32$\times$32$\times$3 and belongs to one of 10 classes. 

\item {\bf STL10~\cite{coates2011analysis}:} 
This dataset contains 5,000 labeled training images and 8,000 labeled testing images, each of which has a size of 96$\times$96$\times$3. Moreover, the dataset contains 10 classes and each image belongs to one of them. Besides the labeled training and testing images, the dataset also contains 100,000 unlabeled images.  

\item {\bf GTSRB~\cite{stallkamp2012man}:} This dataset contains 51,800 traffic sign images in 43 categories. Each image has a size of 32$\times$32$\times$3. The dataset is divided into 39,200 training images and 12,600 testing images. 

\item {\bf SVHN~\cite{netzer2011reading}:} In this dataset,  each image represents a digit from the house numbers in Google Street View. The size of each image is 32$\times$32$\times$3. Moreover, each image belongs to one of the 10 digits. This dataset has 73,257 training images and 26,032 testing images. According to~\cite{netzer2011reading}, they introduced some distracting digits to the sides of the digit of interest, i.e., SVHN is a noisy dataset. 

%{\color{black}
\item {\bf Food101~\cite{bossard14}:} This dataset contains   101,000 images of 101 food categories. We note that this dataset is only used to study the impact of the shadow dataset on BadEncoder. 
%}
\end{itemize}


\subsubsection{Pre-training image encoders}\label{pre_training_image_encoder} When a dataset is used to pre-train an image encoder, we call it \emph{pre-training dataset}. In our experiments, we use CIFAR10 or STL10 as a pre-training dataset since they contain more images and are not noisy datasets. In particular, when CIFAR10  is a pre-training dataset, we use its training images (excluding the labels) to pre-train an image encoder; and  when STL10 is a pre-training dataset, we further consider its unlabeled images when pre-training an image encoder. Note that we do not use the testing images  when pre-training an image encoder because we reserve them to evaluate our BadEncoder when our shadow dataset is different from the pre-training dataset but they have the same distribution. Given a pre-training dataset, we use SimCLR~\cite{chen2020simple} to train a ResNet18~\cite{he2016deep} as an image encoder.  \CRR{Our implementation is based on the publicly available code  of SimCLR~\cite{simclr_url,simclr_url_pytorch}.} We train an image encoder for 1,000 epochs with the Adam optimizer and initial learning rate 0.001. 
%Our implementation is based on the publicly available code released by the authors of SimCLR~\cite{simclr_url}. We train an image encoder for 1,000 epochs with the Adam optimizer and initial learning rate 0.001. 

\subsubsection{Training downstream classifiers} Given an (backdoored) image encoder pre-trained using a pre-training dataset, we use it to train downstream classifiers (i.e., multi-shot classifiers) for the remaining three datasets. When a dataset is used to train a downstream classifier, we call it \emph{downstream dataset}. For instance, given an image encoder pre-trained on CIFAR10, we use it to train downstream classifiers for downstream datasets STL10, GTSRB, and SVHN. In particular, we use a fully connected neural network with two hidden layers as a downstream classifier for a downstream dataset. The number of neurons in the two hidden layers are 512 and 256, respectively. When a dataset is treated as a downstream dataset, we use its training dataset to train a downstream classifier and we use its testing dataset to evaluate the downstream classifier. 
Specifically, we adopt the cross-entropy loss function and Adam optimizer when training a downstream classifier. Moreover, we train a downstream classifier for 500 epochs with an initial learning rate $0.0001$. When a downstream classifier is trained based on a backdoored image encoder, we call it \emph{backdoored downstream classifier}. 






\subsubsection{Evaluation metrics} We use \emph{Clean Accuracy (CA)}, \emph{ Backdoored Accuracy (BA)}, and \emph{Attack Success Rate (ASR)} to evaluate our BadEncoder. CA and  BA measure the classification accuracies of a clean downstream classifier and a backdoored downstream classifier for the clean downstream testing dataset, respectively. 
ASR measures the fraction of trigger-embedded testing inputs that are predicted as the target class by a backdoored downstream classifier. Formally, we define them as follows: 






\begin{itemize}


     
    \item {\bf Clean Accuracy (CA):} The CA of a clean downstream classifier is its classification accuracy for the clean testing images of the corresponding downstream dataset. 
    

    
    \item {\bf  Backdoored Accuracy (BA):}  The BA of a backdoored downstream classifier is its classification accuracy for the clean testing images of the corresponding downstream dataset. 
    When the BA of a backdoored downstream classifier is similar to the CA of the corresponding clean downstream classifier, our attack preserves accuracy for the corresponding downstream task. 


    \item {\bf Attack Success Rate (ASR):} Given a target downstream task and target class, we embed the attacker-chosen trigger to the testing images of the corresponding downstream dataset. The ASR of a backdoored downstream classifier is the fraction of such trigger-embedded testing images that are predicted as the target class by the backdoored downstream classifier. As a baseline,  we also consider \emph{Attack Success Rate-Baseline (ASR-B)}, which is the fraction of such trigger-embedded testing images that are predicted as the target class by the corresponding clean downstream classifier. ASR-B represents the attack success rate when the attacker does not inject backdoor to the pre-trained image encoder. 
    
\end{itemize}








\subsubsection{Parameter setting} 
\label{parameter_setting}
By default, we consider the attacker selects a single target downstream task/dataset and a single target class. 
We will also evaluate our BadEncoder when the attacker selects multiple target downstream tasks and/or target classes. Moreover, we select ``airplane'', ``truck'', ``priority sign'', and ``digit one'' as the target class for the four datasets CIFAR10, STL10, GTSRB, and SVHN, respectively. Unless otherwise mentioned, we adopt CIFAR10 as the default pre-training dataset and STL10 as the default target downstream dataset. Note that we resize each image in STL10 dataset to $32 \times 32 \times 3$ to be consistent with other datasets. 



Our BadEncoder has the following parameters: $\lambda_1$, $\lambda_2$, shadow dataset, trigger, and reference inputs. 
Unless otherwise mentioned, we use the following default parameter settings: $\lambda_1 = 1$ and $\lambda_2 = 1$; shadow dataset includes $50,000$ images sampled from the pre-training dataset; and following previous work on backdoor attacks to classifiers~\cite{wang2019neural}, we use a $10 \times 10$ white square located at the bottom right corner of an input image as the trigger. The size (e.g., $10 \times 10$) of a trigger refers to its height and width. 
By default, we assume one reference input for a (target downstream task, target class) pair. We collected the reference input for each target downstream dataset with the default target class from the Internet. Figure~\ref{four_attack_inputs} in Appendix shows our default reference input for each downstream dataset. We consider the collected reference input is correctly classified as the target class by the backdoored downstream classifier.   An attacker can select multiple reference inputs from the target class, and 
we will show our BadEncoder is effective once at least one reference input is correctly classified by the backdoored downstream classifier. In experiments, we use a randomly augmented version of $x_{ij}$ in $f'(x_{ij})$ in Equation~\ref{equation_of_l1} in each mini-batch. \CRR{Moreover, we freeze the parameters in the batch normalization layers of an encoder when embedding backdoor into it.}

We will explore the impact of each parameter on BadEncoder. 
We adopt cosine similarity to measure the similarity of two inputs' feature vectors outputted by an image encoder  in our loss terms. We fine-tune a pre-trained image encoder for 200 epochs using Algorithm~\ref{alg:example} with learning rate $0.001$ and batch size $256$ to inject the backdoor. 

\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	
	\caption{BadEncoder achieves high ASRs.}
	\begin{tabular}{|c|c|c|c|}
		\hline
	\makecell{Pre-training \\Dataset}& \makecell{ Target Downs-\\tream Dataset }& ASR-B (\%)	 & ASR (\%)  \\ \hline
	\multirow{3}{*}{CIFAR10}
	&	GTSRB  & 2.79 & 98.64  \\ \cline{2-4} 
	&	SVHN & 37.53 & 99.14  \\ \cline{2-4} 
	&	STL10 & 10.38 & 99.73  \\ \hline
	
	\multirow{3}{*}{STL10}
	&	GTSRB  & 1.67 & 95.04  \\ \cline{2-4} 
	&	SVHN  & 46.11 & 97.64  \\ \cline{2-4} 
	&	CIFAR10  & 12.30 & 98.51  \\ \hline
	\end{tabular}
	\label{highasr_table} 
	\vspace{-4mm}
\end{table}













\subsection{Experimental Results}
\vspace{-2mm}
\myparatight{BadEncoder achieves high ASRs} Table~\ref{highasr_table} shows the ASR-B without injecting backdoor to the pre-trained image encoder and ASR of BadEncoder. The experimental results indicate that BadEncoder can achieve high attack success rates. For instance, when the pre-training dataset is CIFAR10 and the target downstream dataset is STL10, BadEncoder can achieve 99.73\% attack success rate. In contrast, the attack success rate (i.e., ASR-B) is only  10.38\% when we do not inject backdoor to the pre-trained image encoder. We note that the ASR-B is relatively high when the target downstream dataset is SVHN because SVHN is unbalanced and the selected target class happens to be the most popular class. 

BadEncoder is successful because our backdoored image encoder outputs similar feature vectors for the reference input and the trigger-embedded inputs, which makes the backdoored downstream classifier predict the same class (i.e., target class) for them.  
Given a clean (or backdoored) image encoder, we use it to produce a feature vector for the reference input and each trigger-embedded testing input in the target downstream dataset, and we calculate the cosine similarity between the feature vector of the reference input and that of each trigger-embedded testing input. Figure~\ref{compare_similarity_of_clean_backdoor} in Appendix shows the cumulative distribution functions (CDFs) of such cosine similarity scores for the clean image encoder and backdoored image encoder. Our results show that our backdoored image encoder produces much more similar feature vectors for the reference input and the trigger-embedded inputs than the clean image encoder, which is the reason why our ASR is much higher than ASR-B. 



\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{Our BadEncoder maintains the accuracy of the downstream classifiers. }
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{|c|c|c|c|c|}
		\hline
	\makecell{Pre-training \\Dataset} & \makecell{Target  Downs-\\tream Dataset} & \makecell{Downstream \\Dataset} & CA (\%) & BA (\%)  \\ \hline
	\multirow{9}{*}{CIFAR10}
	&	\multirow{3}{*}{GTSRB} 
	    & GTSRB & 81.84 & 82.27 \\ \cline{3-5} 
	    && SVHN & 58.50 & 68.93  \\ \cline{3-5} 
	    && STL10 & 76.14 & 75.94  \\ \cline{2-5} 
	&	\multirow{3}{*}{SVHN} 
	    & GTSRB & 81.84 & 82.19  \\ \cline{3-5} 
	    &&	SVHN & 58.50 & 69.32 \\ \cline{3-5} 
	    && STL10 & 76.14 & 75.66  \\ \cline{2-5}   
	&	\multirow{3}{*}{STL10} 
	    & GTSRB & 81.84 & 82.55  \\ \cline{3-5}  
	    && SVHN & 58.50 & 68.68  \\ \cline{3-5} 
	    &&	STL10 & 76.14 & 76.18 \\ \hline
	    
	\multirow{9}{*}{STL10}
	&	\multirow{3}{*}{GTSRB} 
		&	GTSRB & 76.12 & 76.63  \\ \cline{3-5} 
	    && SVHN & 55.35 & 63.85  \\ \cline{3-5} 
	    && CIFAR10 & 86.77 & 86.63  \\ \cline{2-5} 
	&	\multirow{3}{*}{SVHN} 
	    & GTSRB & 76.12 & 75.45  \\ \cline{3-5} 
	    &&	SVHN & 55.35 & 65.59 \\ \cline{3-5} 
	    && CIFAR10 & 86.77 & 86.23 \\ \cline{2-5}   
	&	\multirow{3}{*}{CIFAR10} 
	    & GTSRB & 76.12 & 76.47  \\ \cline{3-5}  
	    && SVHN & 55.35 & 64.37 \\ \cline{3-5}  
	    &&	CIFAR10 & 86.77 & 86.55  \\ \hline
	\end{tabular}
	}
	\label{maintail_utility_table}
	\vspace{-5mm}
\end{table}







\myparatight{BadEncoder preserves accuracy of the downstream classifiers} Table~\ref{maintail_utility_table} compares the clean accuracy and the  backdoored accuracy in different scenarios. In particular, given a pre-training dataset and a target downstream dataset, we evaluate the clean accuracy of the downstream classifiers and  backdoored accuracy of the backdoored downstream classifiers for both the target downstream dataset and non-target downstream datasets. Our experimental results indicate that our BadEncoder preserves the accuracy of the downstream classifiers for the target/non-target downstream datasets. In particular,  
the differences between the backdoor accuracies and the clean accuracies are within 1\% in most cases. We note that the backdoor accuracies are higher than the clean accuracies when the downstream dataset is SVHN, which we consistently observe in all our experiments. We suspect the reason is that the SVHN dataset is noisy, i.e., many images in the SVHN dataset contain distracting digits,  
 and our backdoored image encoder produces better features for the SVHN dataset after fine-tuning the clean image encoder. 




















\myparatight{Impact of loss terms} Our BadEncoder leverages three loss terms, i.e., $L_0$, $L_1$, and $L_2$ in Equation~\ref{final_loss_term}. Moreover, we use $\lambda_1$ (or $\lambda_2$) to weight $L_1$ (or $L_2$). Therefore, we explore the impact of the three loss terms and the parameters $\lambda_1$ and $\lambda_2$ on our BadEncoder. 
Table~\ref{impact_of_loss_term} shows our experimental results when we exclude a loss term in our BadEncoder, where $\lambda_1=1$ (or $\lambda_2=1$) if $L_1$ (or $L_2$) is included. Our results show that all the three loss terms are necessary for our BadEncoder to achieve high attack success rates. Moreover,  $L_2$ is necessary to achieve a high  backdoored accuracy, i.e., to preserve the accuracy of the downstream classifier. This is because $L_2$ is designed to achieve the utility goal. Note that although $L_2$ is designed for the utility goal, excluding it also substantially reduces the attack success rate. This is because, without $L_2$, the backdoored downstream classifier becomes less accurate, in particular it misclassifies the reference input. 


\begin{figure}[!t]
	 \centering
\vspace{-5mm}
\subfloat[$\lambda_1$]{\includegraphics[width=0.25\textwidth]{figs/lambda1.pdf}}
\subfloat[$\lambda_2$]{\includegraphics[width=0.25\textwidth]{figs/lambda2.pdf}}
\caption{The impacts of $\lambda_1$ and $\lambda_2$.}
\label{impact_of_lambda1_lambda2}
\end{figure}


\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{The impact of the loss terms.} 
	\begin{tabular}{|c|c|c|c|}
		\hline
  \makecell{Removed \\Loss Terms} & CA(\%) & BA(\%) & ASR(\%)  \\ \hline
	
		$L_0$ & \multirow{4}{*}{76.14} & 76.48 & 9.48  \\ \cline{1-1} \cline{3-4} 
		$L_1$ &  & 75.85 &  59.15 \\ \cline{1-1} \cline{3-4} 
		$L_2$ &  & 50.08 &  9.09 \\ \cline{1-1} \cline{3-4} 
		None &  & 76.18 & 99.73   \\ \hline
	\end{tabular}
	\label{impact_of_loss_term}
	\vspace{-4mm}
\end{table}

We also study the impact of $\lambda_1$ and $\lambda_2$ on the backdoored accuracy and attack success rate of our BadEncoder. Figure~\ref{impact_of_lambda1_lambda2} shows the results. First, we observe that BadEncoder achieves high attack success rates and preserve accuracy after $\lambda_1$ and $\lambda_2$ are larger than some thresholds. Second, our BadEncoder is less sensitive to $\lambda_1$. In particular, the attack success rate starts to decrease after $\lambda_2$ is larger than around 1, while the attack success rate keeps stable even if  $\lambda_1$ is as large as 10. This is because our shadow dataset size (related to $L_2$) is much larger than the number of reference inputs (related to $L_1$). 




\myparatight{Impact of shadow dataset} A shadow dataset can be characterized by its size and distribution. Therefore, we study the impact of both the size and distribution of the shadow dataset on our BadEncoder. 
Figure~\ref{impact_of_attack_dataset_size} and Figure~\ref{impact_of_attack_dataset_size_svhn_stl10} in Appendix show the impact of the shadow dataset size on BadEncoder. We find that BadEncoder achieves high attack success rates and preserves accuracy of the downstream classifiers once the shadow dataset size is larger than around 20\% of the pre-training dataset. We note that the  backdoored accuracy for the SVHN dataset increases as the size of the shadow dataset increases. We suspect the reason is that SVHN is noisy and our backdoored image encoder produces more distinguishable features for it after fine-tuning the clean image encoder. 


We also study the impact of the shadow dataset distribution on our BadEncoder. In particular, we consider three cases. In the first case, the shadow dataset is a subset of the pre-training dataset. In particular,  we randomly sample $10,000$ images from the pre-training dataset CIFAR10 as the shadow dataset. In the second case,  the shadow dataset has the same distribution as the pre-training dataset but does not  overlap with it. In particular, we use the testing $10,000$ images of CIFAR10, which were not used to pre-train the image encoder, as the shadow dataset. In the third case,  the shadow dataset has a different distribution with the pre-training dataset. Specifically, we randomly sample 10,000 images from the Food101 dataset~\cite{bossard14} which contains images of food and  has a different distribution with CIFAR10.
Table~\ref{impact_of_attack_dist} shows our experimental results. Our results show that our BadEncoder achieves high attack success rates and preserves accuracy of the downstream classifiers in all the three cases, though the attack success rates are lower in the third case when the shadow dataset has a different distribution with the pre-training dataset. 
Our results indicate that our shadow dataset does not need to be from the pre-training dataset nor follow its  distribution. 







\begin{figure}[!t]
	 \centering
	 \vspace{-5mm}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figs/fraction_cifar10_stl10.pdf}\label{impact_of_attack_dataset_size}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figs/trigger_size_cifar10_gtsrb.pdf}\label{impact_of_trigger_size}
\vspace{-3mm}
}
\caption{(a) The impact of the shadow dataset size on  BadEncoder when  the target downstream dataset is STL10 and the shadow dataset is a subset of the pre-training dataset. (b) The impact of the trigger size on BadEncoder when the target downstream dataset is GTSRB. }
\vspace{-2mm}
\end{figure}

\begin{table}[tp]\renewcommand{\arraystretch}{1.4} 
\setlength{\tabcolsep}{3pt}
	\centering
	\fontsize{7.5}{8}\selectfont
	\caption{The impact of the shadow dataset's distribution on  BadEncoder.} 
	\begin{tabular}{|c|c|c|c|c|}
		\hline
	 \makecell{Target Downs-\\tream Dataset}& \makecell{Shadow Dataset} & CA (\%) & BA (\%) & ASR (\%)   \\ \hline
    
	\multirow{3}{*}{GTSRB}
	&	A subset of pre-training dataset & \multirow{3}{*}{81.84} & 81.21 & 98.19  \\ \cline{2-2}\cline{4-5} 
	&	Same distribution & & 81.12 & 97.52  \\ \cline{2-2}\cline{4-5} 
	&	Different distributions & & 82.21 & 93.27  \\ \cline{1-3}\cline{4-5} 

	
	\multirow{3}{*}{SVHN}
	&		A subset of pre-training dataset  & \multirow{3}{*}{ 58.50} & 62.32 & 98.30  \\ \cline{2-2}\cline{4-5} 
	&	Same distribution &  & 62.07 & 98.06  \\ \cline{2-2}\cline{4-5} 
	& Different distributions &  & 60.40 & 84.80  \\ \cline{1-5} 

    \multirow{3}{*}{STL10}
	&		A subset of pre-training dataset  & \multirow{3}{*}{76.14} & 75.90 & 99.55  \\ \cline{2-2}\cline{4-5} 
	&	Same distribution &  & 75.70 & 99.43  \\ \cline{2-2}\cline{4-5} 
	& Different distributions & & 75.99 & 98.15  \\ \hline

	\end{tabular}
	\label{impact_of_attack_dist}
	\vspace{-4mm}
\end{table}

\myparatight{Impact of trigger size} Figure~\ref{impact_of_trigger_size},  Figure~\ref{impact_of_trigger_size_cifar10} (in Appendix), and Figure~\ref{impact_of_trigger_size_stl10} (in Appendix)  show the impact of the trigger size on BadEncoder. Note that our trigger is a white square located at the bottom right corner of an input and trigger size refers to the height/width of a trigger. We have the following observations from our experimental results. First, our BadEncoder achieves high attack success rates when the trigger size is no smaller than some threshold, e.g., $10 \times 10$, $3 \times 3$, and $5 \times 5$ respectively for GTSRB,  SVHN, and STL10 when the pre-training dataset is CIFAR10. We also observe that such threshold depends on both the pre-training dataset and the target downstream dataset. For instance, the GTSRB downstream dataset requires $10 \times 10$ and $3 \times 3$ trigger sizes to achieve high attack success rates when the pre-training dataset is CIFAR10 and STL10, respectively. 
Second, the backdoor accuracies are comparable to (or higher than) the clean accuracies when the trigger has different sizes. In other words, our BadEncoder with different trigger sizes do not sacrifice the utility of the pre-trained image encoder. 


\myparatight{Multiple reference inputs} Our BadEncoder relies on that  the backdoored downstream classifier correctly predicts the   reference input as the target class. However, it is possible that the backdoored downstream classifier does not correctly predict the reference input into the target class. In response, the attacker can use multiple reference inputs for a target class. Table~\ref{multiple_attack_inputs} shows our  results when our BadEncoder uses each of three reference inputs and all the three reference inputs, where the three reference inputs were collected from the Internet and are shown in Figure~\ref{multiple_classes_attack_inputs} in Appendix. As the results show, one reference input (Truck 0) has high attack success rate because it is correctly classified by the backdoored downstream classifier, but  two reference inputs (Truck 1 and Truck 2) lead to low attack success rates because they are misclassified by the backdoored downstream classifier. 
However,  our BadEncoder  can still achieve high attack success rate when using all the three reference inputs. In other words, our BadEncoder is effective once at least one reference input is correctly classified by the backdoored downstream classifier. 

\myparatight{Multiple target classes} Our BadEncoder can  attack multiple target classes in a target downstream task simultaneously. In particular, we use a different trigger for each target class. Specifically, we select ``airplane, ``truck'', and ``horse'' as the three target classes, and we use $10 \times 10$ white square (each pixel has value (255, 255, 255)), green square (each pixel has value (0, 255, 0)), and purple square (each pixel has value (255, 0, 255)) located at the bottom right corner, upper left corner, and central of an image as the corresponding triggers. We collected an reference input for each target class from the Internet, and the reference inputs are shown in Figure~\ref{multiple_downstream_inputs} in Appendix.   Table~\ref{multiple_target_classes} shows our  results. We find that our BadEncoder can still achieve high attack success rates while maintaining the accuracy of the downstream classifier when attacking multiple target classes simultaneously. 



\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{Results of using multiple reference inputs. } 
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
	 Reference Input & CA (\%) & BA (\%) & ASR (\%)   \\ \hline
		Truck 0 & \multirow{4}{*}{76.14} & 76.18 & 99.73 \\ \cline{1-1}  \cline{3-4} 
		Truck 1 & & 76.10 & 11.64  \\ \cline{1-1}  \cline{3-4} 
		Truck 2 &  & 75.63 & 0.23 \\ \cline{1-1}  \cline{3-4} 
		All &  & 75.33 & 99.24  \\ \cline{1-4} 
	\end{tabular}
	\label{multiple_attack_inputs}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{Results of attacking three target classes simultaneously. } 
	\begin{tabular}{|c|c|c|c|}
		\hline
	 Target Class & CA (\%) & BA (\%) & ASR (\%)   \\ \hline


		Airplane & \multirow{3}{*}{76.14} & \multirow{3}{*}{75.61} & 99.73  \\ \cline{1-1}\cline{4-4} 
		Horse &  &  & 99.86  \\ \cline{1-1} \cline{4-4} 
		Truck & & & 100.00  \\ \hline
	\end{tabular}
	\label{multiple_target_classes}
	\vspace{-1mm}
\end{table}





\myparatight{Multiple target downstream tasks} 
An attacker may be interested in attacking multiple target downstream tasks simultaneously. To evaluate our attacks in such scenario, we use CIFAR10 as the pre-training dataset and simultaneously attack the other three datasets as the target downstream datasets. We adopt the default target class and reference input for each target downstream dataset. 
Moreover, we adopt a $10 \times 10$ white square, green square, and purple square located at the bottom right corner, upper left corner, and central of an image as the corresponding triggers for the three target downstream datasets, respectively. Table~\ref{multiple_downstream_tasks} shows our results. Our results show that our attacks can achieve high attack success rates at attacking multiple target downstream tasks simultaneously while maintaining accuracy of the downstream classifiers. 





\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{Results of attacking three target downstream datasets simultaneously. } 
	\begin{tabular}{|c|c|c|c|c|}
		\hline
	\makecell{Target Downstream Dataset} & CA (\%) & BA (\%) & ASR (\%)   \\ \hline

	
		GTSRB & 81.84 & 82.98 & 93.35  \\ \cline{1-4} 
		SVHN & 58.50 & 69.74 & 100.00  \\ \cline{1-4} 
		STL10 & 76.14 & 75.75 & 100.00  \\ \hline
	\end{tabular}
	\label{multiple_downstream_tasks}
	\vspace{-5mm}
\end{table}







 \myparatight{Impact of other parameters} We also studied the impact of other parameters (e.g., learning rate, number of epochs, trigger value) on BadEncoder. We found that BadEncoder achieves high ASRs while maintaining accuracy for the downstream classifier across different parameter settings. Due to the limited space, the results are shown in Table~\ref{result_of_different_parameters} in Appendix. 



\myparatight{Comparing BadEncoder with Latent Backdoor Attack (LBA)~\cite{yao2019latent}} We compare BadEncoder with LBA, which was designed for transfer learning. When extended to self-supervised learning, LBA tries to inject a backdoor into a teacher classifier trained based on an image encoder. When the backdoored teacher classifier is fine-tuned to train a student classifier (i.e., downstream classifier in our terminology) for a target downstream task, the student classifier inherits the backdoor. To train a backdoored teacher classifier, LBA requires a large \emph{labeled} dataset consisting of {labeled} examples in both the target class and  non-target classes from the target downstream task. 
Specifically, we use the same reference input in the target class in BadEncoder for LBA. Moreover, we further randomly sample a certain fraction of the testing images in the non-target classes of the target downstream dataset for LBA. We adopt a public implementation from the authors of LBA~\cite{latent_url}. 



Table~\ref{compare_with_LBA} in Appendix shows our comparison results when different fractions of testing images in the non-target classes of the target downstream dataset are used by LBA. Our experimental results indicate that BadEncoder achieves higher attack success rates than LBA, where the attack success rates are evaluated using the rest of testing images of the target downstream dataset that were not used by LBA. The reason is that BadEncoder injects the backdoor via directly optimizing the pre-trained image encoder while LBA injects a backdoor into a teacher classifier built based on the image encoder.


