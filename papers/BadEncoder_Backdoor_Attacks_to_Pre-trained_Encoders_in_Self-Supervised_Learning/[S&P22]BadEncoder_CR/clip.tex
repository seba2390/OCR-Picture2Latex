\section{Two Real-world Case Studies}
We show two real-world case studies for our BadEncoder. In particular, we apply our BadEncoder to an image encoder pre-trained on ImageNet and released by Google~\cite{chen2020simple}. Moreover, we apply our BadEncoder to CLIP~\cite{radford2021learning}, which includes an image encoder and a text encoder pre-trained on 400 million (image, text) pairs collected from the Internet. CLIP was released by OpenAI~\cite{clip_url}. Since CLIP includes a text encoder, it can be used for zero-shot classifier. 



\subsection{Attacking Image Encoder Pre-trained on ImageNet}
\subsubsection{Experimental Setup} 
\label{experimental_setup_imagenet}
We consider the attacker selects a single target downstream task/dataset, a single target class, and a single reference input. In particular, we select ``truck'', ``priority sign'', and ``digit one'' as the target classes for the datasets STL10, GTSRB, and SVHN, respectively. Similarly, we collected the reference input for each target class from the Internet, which were shown in Figure~\ref{resnet50_google_attack_input} in Appendix. We set $\lambda_1 = 1$ and $\lambda_2 = 1$; we randomly sample $1\%$ of the training images of ImageNet as the shadow dataset; we use a $50 \times 50$ white square located at the bottom right corner of an image as the trigger. Moreover, we adopt the same neural network architecture in Section~\ref{sec:exp} as the downstream classifiers. We note that each image in ImageNet  was resized to $224 \times 224 \times 3$ when Google used them to pre-train the image encoder. Therefore, we also resize each image in the shadow dataset and downstream datasets to be $224 \times 224 \times 3$ in our experiments.  We fine-tune the pre-trained image encoder for 200 epochs with learning rate $10^{-4}$ and batch size $16$ to inject the backdoor. Note that we use a small batch size due to the large resolution of images in ImageNet. 





\subsubsection{Experimental Results} Table~\ref{case_study_resnet50_google} shows the experimental results. We find that our BadEncoder can achieve high attack success rates while maintaining the accuracy of the downstream classifiers. Our experimental results demonstrate that our BadEncoder is effective when applied to an image encoder that is pre-trained on a large amount of unlabeled images. 

\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{BadEncoder achieves high attack success rates and maintains the accuracy of the downstream classifiers when attacking the image encoder pre-trained on ImageNet by Google~\cite{chen2020simple}. }
	\begin{tabular}{|c|c|c|c|c|}
		\hline
 \makecell{Target Downs-\\tream Dataset} & CA (\%) & BA (\%) & ASR-B (\%)	 & ASR (\%)  \\ \hline
		GTSRB & 76.53 & 78.42 & 5.47 & 98.93  \\ \cline{1-5} 
		STL10 & 95.66 & 95.68 & 10.24 & 99.99  \\ \cline{1-5} 
		SVHN & 72.55 & 73.77 & 32.28 & 99.93 \\  \hline
	\end{tabular}
	\label{case_study_resnet50_google}
	\vspace{-5mm}
\end{table}



\begin{table}[tp]\renewcommand{\arraystretch}{1.2} 
	\centering
	\caption{ BadEncoder achieves high attack success rates and maintains the  accuracy of the downstream classifiers when attacking CLIP~\cite{radford2021learning}.}
	\subfloat[Multi-shot classifiers]{
	\begin{tabular}{|c|c|c|c|c|}
		\hline
 \makecell{Target Downs-\\tream Dataset} & CA (\%) & BA (\%) & ASR-B (\%)	 & ASR (\%)   \\ \hline
		GTSRB & 82.36 & 82.14 & 5.37 & 99.33  \\ \cline{1-5} 
		STL10 & 97.09 & 96.69 & 10.00 & 99.81  \\ \cline{1-5} 
		SVHN & 70.60 & 70.27 & 20.79 & 99.99  \\ \hline
	\end{tabular}\label{case_study_clip_st_1}}
	
		\subfloat[Zero-shot classifiers]{
		\begin{tabular}{|c|c|c|c|c|}
		\hline
	 \makecell{Target Downs-\\tream Dataset} & CA (\%) & BA (\%) & ASR-B (\%)	 & ASR (\%)   \\ \hline

		GTSRB & 29.83 & 29.84 & 1.96 & 99.82  \\ \cline{1-5} 
		STL10 & 94.60 & 92.80 & 10.08 & 99.96  \\ \cline{1-5} 
		SVHN & 11.73 & 11.16 & 53.55 & 100.00  \\ \hline
	\end{tabular}
	\label{case_study_clip_zs}}

	\label{case_study_clip_st}
	\vspace{-6mm}
\end{table}

\subsection{Attacking CLIP}

\subsubsection{Experimental Setup} CLIP consists of both an image encoder and a text encoder. We apply BadEncoder to inject a backdoor to the image encoder. When building a downstream classifier, CLIP supports both multi-shot classifier and zero-shot classifier, as we discussed in Section~\ref{sec:background}. Therefore, we evaluate BadEncoder for both scenarios. Since we don't have access to CLIP's  pre-training dataset, we  adopt the training images of CIFAR10 as the shadow dataset. In both scenarios, we fine-tune the CLIP's image encoder for 200 epochs using our Algorithm~\ref{alg:example} with learning rate $10^{-6}$ and batch size $16$.  

In the multi-shot classifier scenario, we consider  the same experimental settings as  those when attacking the image encoder pre-trained on ImageNet (please refer to Section~\ref{experimental_setup_imagenet} for details). 
Moreover, we collected the reference inputs  from the Internet and they can be found in Figure~\ref{resnet50_clip_attack_input} in Appendix. 
In the zero-shot classifier scenario, we also consider a single target downstream dataset and a target class. We select ``truck'', ``stop sign'', and ``digit zero'' as target classes for the target downstream datasets STL10, GTSRB, and SVHN, respectively. We collected a reference input for each target class from the Internet and they are shown in Figure~\ref{clip_zero_shot_attack_input} in Appendix. Recall that a zero-shot classifier requires a context sentence for each class. We adopt the context sentences ``A photo of a \{class name\}'' for STL10 and SVHN. However, for GTSRB, we adopt the context sentences ``A traffic sign photo of a \{class name\}'' because we found they achieve better accuracy than ``A photo of a \{class name\}'' for GTSRB.  







\subsubsection{Experimental Results} Table~\ref{case_study_clip_st} shows the experimental results. We find that our BadEncoder achieves high attack success rates and maintains the  accuracy of the downstream classifiers (both multi-shot classifiers and zero-shot classifiers). Our experimental results indicate that our BadEncoder is effective when applied to an image encoder pre-trained on a large amount of (image, text) pairs. 

