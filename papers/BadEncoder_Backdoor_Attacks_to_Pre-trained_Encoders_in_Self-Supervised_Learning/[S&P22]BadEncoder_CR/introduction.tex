\section{Introduction}
A key challenge of the conventional \emph{supervised learning} (or \emph{transfer learning}) is that they require a large amount of labeled training data for each classification task (or the teacher classification task). 
\emph{Self-supervised learning}~\cite{devlin2018bert,hadsell2006dimensionality,he2020momentum,chen2020simple,hjelm2018learning,grill2020bootstrap} is a new AI paradigm that aims to address the challenge. The self-supervised learning pipeline includes two key components, i.e., \emph{pre-training encoders} and \emph{building downstream classifiers}. 
For instance,  in the computer vision domain, the first component
 aims to pre-train an image encoder and (optionally) a text encoder using a large amount of \emph{unlabeled} images or (image, text) pairs (called \emph{pre-training dataset}). In the second component, the pre-trained image encoder is used as a feature extractor to build classifiers (called \emph{downstream classifiers}) for many downstream tasks with \emph{a small amount of} or \emph{no} labeled training data. %Examples of such pre-trained image encoders 
 %include BERT~\cite{devlin2018bert} and GPT~\cite{radford2018improving,radford2019language,brown2020language} in the natural language processing domain, 
 %include as well as \emph{Contrastive Language-Image Pre-training (CLIP)}~\cite{radford2021learning}. 
 Self-supervised learning has achieved revolutionary and remarkable performance in various downstream tasks. For instance, OpenAI recently pre-trained CLIP~\cite{radford2021learning} on 400 million (image, text) pairs collected from the Internet, and without needing to use any labeled training data for the downstream  tasks, CLIP achieves accuracy that is competitive with the fully supervised classifiers for many downstream  tasks~\cite{radford2021learning}. 
 

 




 However, existing studies~\cite{hadsell2006dimensionality,he2020momentum,chen2020simple,hjelm2018learning,grill2020bootstrap} on self-supervised learning mainly focus on designing new algorithms to pre-train encoders that achieve better performance for various downstream tasks, leaving the security of self-supervised learning in adversarial settings largely unexplored. 
We aim to bridge the gap in this work. %In particular, we focus on self-supervised learning in the computer vision domain, which pre-trains an image encoder and (optionally) a text encoder using a large amount of unlabeled images or (image, text) pairs, and then applies the pre-trained image encoder to build downstream classifiers for many downstream image classification tasks.  
In particular, we focus on backdoor attacks to self-supervised learning in the computer vision domain. An attacker aims to compromise the self-supervised learning pipeline such that \emph{backdoored downstream classifiers} are built for  attacker-chosen downstream classification tasks (called \emph{target downstream tasks}), and each backdoored downstream classifier predicts any input embedded with an attacker-chosen trigger as the corresponding attacker-chosen class (called \emph{target class}).

Existing backdoor attacks~\cite{gu2017badnets,chen2017targeted,liutrojaning2018,bagdasaryan2020blind} inject a backdoor into a classifier via compromising its training process, e.g., poisoning its labeled training data~\cite{gu2017badnets,chen2017targeted}, fine-tuning the classifier~\cite{liutrojaning2018}, or tampering the training algorithm~\cite{bagdasaryan2020blind}.  
In the context of self-supervised learning, these backdoor attacks require compromising the second component of the self-supervised learning pipeline, i.e., the training process of the downstream classifiers. Therefore, they are not applicable when a downstream classifier does not have a training process (i.e., the downstream task has no labeled training data) or its training process maintains integrity. % (e.g., a customer builds its downstream classifier without involving third party). 
Yao et al.~\cite{yao2019latent} proposed \emph{Latent Backdoor Attack (LBA)} to transfer learning. In particular, they inject a backdoor into a teacher classifier such that a student/downstream classifier built for a target downstream task is also backdoored. LBA can be extended to self-supervised learning via training a backdoored teacher classifier using the pre-trained image encoder and a large amount of labeled training data for both the target class and the non-target classes in the target downstream task. However, such a large amount of labeled training data may be unavailable. % in the context of self-supervised learning. 
Moreover, as we will show in our experiments, even if such labeled training data is available, LBA achieves suboptimal attack effectiveness when extended to self-supervised learning. 










\myparatight{Our work} In this work, we propose \emph{BadEncoder}, the first backdoor attack to self-supervised learning.   BadEncoder compromises the first component of the self-supervised learning pipeline, while assuming its  second component maintains integrity. Specifically,  BadEncoder  injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored encoder for the target downstream tasks simultaneously inherit the backdoor behavior. 

In particular, 
%craft a backdoored image encoder from a clean pre-trained image encoder %In our attacks, an attacker first selects some downstream tasks (called \emph{target downstream tasks}) that he/she aims to target. For each target downstream task, the attacker could target one or more of its classes (called \emph{target classes}). 
 our BadEncoder aims to achieve two goals: 1) a  backdoored downstream classifier  built based on the backdoored image encoder for a target downstream task should predict any input embedded with an attacker-chosen  trigger as the corresponding attacker-chosen target class, and 2) to make our BadEncoder stealthy, the backdoored downstream classifiers for both the target and non-target downstream tasks should maintain  accuracy for clean inputs. We call the two goals \emph{effectiveness goal} and \emph{utility goal}, respectively. An attacker can attack multiple target downstream tasks and multiple target classes for each target downstream task simultaneously. 
 We assume an attacker chooses a trigger for each (target downstream task, target class) pair; the attacker has access to one or more images (called \emph{reference inputs}) in the target class for each (target downstream task, target class) pair, e.g., the attacker can collect them from the Internet; and the attacker has access to a set of unlabeled images (called \emph{shadow dataset}), which may or may not be the pre-training dataset used to train the clean image encoder. 
 
We formulate our BadEncoder as an optimization problem. In particular, we propose an \emph{effectiveness loss} and an \emph{utility loss} to quantify the {effectiveness goal} and {utility goal}, respectively. Roughly speaking,  our effectiveness loss is smaller if the backdoored image encoder 1) produces more similar feature vectors for the reference inputs and the inputs in the shadow dataset embedded with the trigger for each (target downstream task, target class) pair, and 2) produces more similar feature vectors for the reference inputs with the clean image encoder. Our utility loss is smaller if the backdoored image encoder and the clean image encoder produce more similar feature vectors for each clean input in the shadow dataset. Our formulated optimization problem aims to craft a backdoored image encoder to minimize a weighted sum of the two loss terms. Moreover, we propose a gradient descent based method to solve the optimization problem, which produces a backdoored image encoder from a clean one. 
 

We first evaluate our BadEncoder on CIFAR10, STL10, GTSRB, and SVHN datasets. In these experiments, we pre-train clean image encoders by ourselves and inject backdoors into them.  
Our experimental results show that BadEncoder can achieve high \emph{attack success rates}. % i.e., a backdoored downstream classifier built based on our backdoored image encoder is highly likely to predict any input embedded with a trigger as the corresponding target class. 
For instance, the attack success rate is 99\% when the clean image encoder is pre-trained on CIFAR10 and the backdoored downstream classifier is built for GTSRB. Moreover,  
%BadEncoder maintains the accuracy of the downstream classifiers. In particular, 
the accuracy loss (if any) of the backdoored downstream classifier incurred by BadEncoder is within 1\% in most cases. We also evaluate  BadEncoder on two publicly available, real-world image encoders. Specifically, we apply  BadEncoder to the image  encoder  pre-trained  on  ImageNet  and  released  by  Google~\cite{chen2020simple}  as  well  as  the   CLIP image  encoder pre-trained and released by OpenAI~\cite{radford2021learning}.  Our results indicate that  BadEncoder also achieves high attack success rates and maintains  accuracy of the downstream classifiers. 

We explore two state-of-the-art empirical defenses (i.e., Neural Cleanse~\cite{wang2019neural} and MNTD~\cite{xu2019detecting}) and a state-of-the-art provable defense (i.e., PatchGuard~\cite{xiang2020patchguard}) to mitigate our BadEncoder. In particular, both Neural Cleanse~\cite{wang2019neural} and MNTD~\cite{xu2019detecting} can detect whether a classifier is backdoored or not. Therefore, we apply them to detect  whether a downstream classifier is backdoored or not. Our experimental results indicate that they cannot detect the backdoored downstream classifiers. Our BadEncoder embeds a trigger/patch to an input to make the backdoored downstream classifier predict the target class. Therefore, we can use provable defenses against adversarial patches to mitigate our BadEncoder. In particular, we evaluate PatchGuard~\cite{xiang2020patchguard} which achieves the state-of-the-art certified accuracy against adversarial patches. Our experimental results indicate that PatchGuard provides insufficient robustness guarantees against our BadEncoder. Moreover, we extend MNTD to detect backdoored image encoders, and our results show that MNTD has low accuracy at detecting backdoored encoders. %Our results highlight that we need new defenses to defend against our BadEncoder. 

Our key contributions are summarized as follows:
\begin{itemize}
    \item We propose BadEncoder, the first backdoor attack to  self-supervised learning.
    
    \item We perform systematic evaluation for BadEncoder using multiple datasets. Moreover, we evaluate BadEncoder using two publicly available, real-world image encoders. 
    
    \item We explore three defenses to mitigate our BadEncoder. Our experimental results highlight that we need new defenses to defend against our BadEncoder. 
\end{itemize}

