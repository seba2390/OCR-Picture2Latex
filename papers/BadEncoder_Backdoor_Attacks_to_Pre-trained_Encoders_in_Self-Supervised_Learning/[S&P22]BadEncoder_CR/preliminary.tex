\section{Threat Model}

% target downstream task. 

% select target image from downstream task

% attacker's goal:


% \begin{table}[!t] \renewcommand{\arraystretch}{1.1}
% \centering
% \caption{Notations}
% \label{table:notations}
% % \addtolength{\tabcolsep}{-2pt}
% \begin{tabular}{c|l} 
% %\toprule 
% \hline
% Notation & Description\\ \hline
% %\midrule
% $f$ & Clean pre-trained image encoder\\
% $f'$ & Backdoored image encoder\\
% %$\mathcal{D}_p$ & Pre-training dataset\\
% %$\mathcal{D}_{tr}$ & Downstream training dataset\\
% %$\mathcal{D}_{te}$ & Downstream testing dataset\\
% $\mathcal{D}_s$ & Shadow dataset\\
% %$g\circ f$ & Downstream classifier\\
% %$g'\circ f'$ & Backdoored downstream classifier\\
% $\mathbf{x}$ & Clean input \\
% $\mathbf{e}_i$ & Backdoor trigger \\
% $T_i$ & Target downstream task \\
% $y_i$ & Target class \\
% $\mathbf{x}_{ij}$ & Reference input \\
% \hline
% %\bottomrule
% \end{tabular}
% \end{table}


% In this section, we describe our threat model and design goals for our attacks. Table~\ref{table:notations} summarizes some important notations used in this work. 

% \subsection{Threat Model}
We characterize our threat model with respect to attacker's goals, background knowledge, and capabilities. 
%Table~\ref{table:notations} summarizes some important notations used in this work. 


\myparatight{Attacker's goals} We consider an attacker aims to inject backdoors into a pre-trained image encoder such that a downstream classifier built based on the backdoored image encoder makes attacker-chosen predictions for 
inputs embedded with an attacker-chosen trigger. 
We consider attacking image encoder instead of text encoder so our attacks are applicable to both images and (image, text)  pairs based self-supervised learning. 
In particular, the attacker first selects some downstream tasks that he/she aims to target. We call such downstream tasks  \emph{target downstream tasks}. For each target downstream task, an attacker could target one or more of its classes, which we call \emph{target classes}. For instance, when the target downstream task is traffic sign recognition, the target classes could be  ``priority traffic sign'', ``stop sign'', etc.. For simplicity, we use $(T_i, y_i)$ to denote a (target downstream task, target class) pair, where $i=1, 2, \cdots, t$. Note that multiple $T_i$'s may represent the same target downstream task when the attacker selects multiple target classes for it.   For each $(T_i, y_i)$ pair, the attacker  selects a backdoor trigger  $\mathbf{e}_i$, e.g., a patch located at the bottom right corner of an input image. 

The attacker aims to craft a backdoored image encoder based on a clean image encoder to achieve two goals, i.e., \emph{effectiveness goal} and \emph{utility goal}. 
\begin{itemize}
    \item {\bf Effectiveness goal.} The effectiveness goal means that, when a downstream classifier (called \emph{backdoored downstream classifier}) is built based on the backdoored image encoder for  a target downstream task $T_i$,  the backdoored downstream classifier should predict the target class $y_i$ for any input embedded with the trigger $\mathbf{e}_i$.  The backdoored downstream classifiers  for the target downstream tasks should simultaneously have such backdoor behavior. 
    \item {\bf Utility goal.} The utility goal means that, the backdoored image encoder should maintain utility to be stealthy. In particular, for any target or non-target downstream task, a downstream classifier built based on the backdoored image encoder should be as accurate as a downstream classifier built based on the clean image encoder for clean testing inputs. 
\end{itemize}




\myparatight{Attacker's background knowledge and capabilities}  We consider two possible attackers: 1) an untrusted service provider who  injects a backdoor into its pre-trained image encoder and shares the backdoored  encoder with downstream customers (e.g., makes the backdoored  encoder publicly available), and 2) a malicious third-party  who obtains a clean pre-trained image encoder from a service provider, injects backdoors into it, and shares the backdoored encoder with downstream customers (e.g., via republishing it for public download  on GitHub). Therefore, an attacker has access to a clean pre-trained image encoder.  We note that our attack is only applicable when a downstream customer uses an image encoder from an untrusted source and that our work shows one example of how this encoder could have been compromised.

Moreover, we assume an attacker has access to a set of unlabeled images, which we call \emph{shadow dataset}.  
In particular, we consider two scenarios depending on who is the attacker. In the first scenario,  the attacker is an untrusted service provider who pre-trains the image encoder and thus the attacker has access to the pre-training dataset, in which the attacker can use the pre-training dataset  as the shadow dataset.  In the second scenario where the attacker is a malicious third-party, the attacker may not have access to the pre-training dataset and thus the shadow dataset may not be the pre-training dataset. In this scenario, we will consider  shadow dataset that has or does not have the same distribution as the pre-training dataset. We also assume the attacker has access to some images (called \emph{reference inputs}) for each (target downstream task, target class) pair, e.g., the attacker can collect such images from the Internet. For instance, for the (traffic sign recognition, ``stop sign'') pair, the attacker can collect one or more stop sign images from the Internet as the reference inputs. Note that the reference inputs are not in the downstream dataset used to build downstream classifiers.  As we will see in Section~\ref{sec:attacks}, our BadEncoder uses the shadow dataset and reference inputs to inject backdoors into the image encoder.


We assume the attacker does not have access to the downstream dataset used to build downstream classifiers and cannot tamper the training process of the downstream classifiers. 


 