\section{Design of BadEncoder}
\label{sec:attacks}


\begin{figure*}[!t]
	 \centering
{\includegraphics[width=0.9\textwidth]{figs/figure_2_badencoder_camera.pdf}}
\caption{Overview of BadEncoder.}
\label{overview_of_our_attacks}
 \vspace{-6mm}
\end{figure*}

\subsection{Overview}
Figure~\ref{overview_of_our_attacks} shows an overview of BadEncoder. 
We aim to craft a backdoored image encoder from a clean one to achieve the effectiveness goal and utility goal. 
To achieve the effectiveness goal, our idea is to modify the clean image encoder such that 1) it produces similar feature vectors for the reference inputs and inputs in the attacker's shadow dataset embedded with the trigger for each (target downstream task, target class) pair, and 2) it produces similar feature vectors for the reference inputs with the clean image encoder. Therefore, a downstream classifier built based on our backdoored image encoder still predicts a reference input as the target class, and thus likely predicts any input embedded with the corresponding trigger as the target class. To achieve the utility goal, we modify the clean image encoder such that our backdoored image encoder and the clean image encoder produce similar feature vectors for each clean input in the shadow dataset. Therefore, a downstream classifier built based on our backdoored image encoder will maintain its accuracy for clean testing inputs. 

Formally, we formulate our BadEncoder as an optimization problem, solving which gives us a backdoored image encoder that achieves the two goals. In particular, we propose an \emph{effectiveness loss} and an \emph{utility loss} to quantify the two goals, respectively. Our optimization problem aims to minimize a weighted sum of the losses. 








\subsection{Formulating our BadEncoder as an Optimization Problem}

We denote a clean pre-trained image encoder and our backdoored one as $f$ and $f'$, respectively. For each (target downstream task, target class) pair $(T_i, y_i)$, the attacker collects a set of reference inputs $\mathcal{R}_i=\{\mathbf{x}_{i1}, \mathbf{x}_{i2}, \cdots, \mathbf{x}_{ir_i}\}$ from the target class $y_i$, where $r_i$ is the number of reference inputs for the pair $(T_i, y_i)$ and $i=1,2,\cdots,t$. $\mathbf{e}_i$ is the attacker-chosen trigger for the  pair $(T_i, y_i)$. $\mathbf{x} \oplus \mathbf{e}_i$ means embedding the trigger $\mathbf{e}_i$ to an input $\mathbf{x}$. We call $\mathbf{x} \oplus \mathbf{e}_i$ a \emph{backdoored input}. We propose an effectiveness loss and an utility loss to quantify the effectiveness goal and utility goal, respectively. Next, we discuss them. 

\myparatight{Effectiveness loss} To achieve the effectiveness goal, we craft a backdoored image encoder such that it produces similar feature vectors for the reference inputs $\mathcal{R}_i$ of a pair $(T_i, y_i)$ and any input in the shadow dataset $\mathcal{D}_s$ embedded with the trigger $\mathbf{e}_i$. Therefore, a backdoored downstream classifier built based on our backdoored image encoder is likely to predict the same label for the reference inputs $\mathcal{R}_i$ and any input embedded with the trigger $\mathbf{e}_i$. However, this alone does not guarantee that the backdoored downstream classifier predicts the target class $y_i$ for an input  embedded with the trigger $\mathbf{e}_i$, because  it may not correctly predict the target class for the reference inputs. Therefore, we further require that the backdoored image encoder produces similar  feature vectors for the reference inputs with the clean image encoder. 
Formally, our effectiveness loss consists of the following two terms:
\begin{align}
    L_0 &=-\frac{\sum_{i=1}^{t} \sum_{j=1}^{r_i}\sum_{\mathbf{x} \in \mathcal{D}_s} s(f'(\mathbf{x} \oplus \mathbf{e}_i), f'(\mathbf{x}_{ij}))}{ |\mathcal{D}_s|\cdot \sum_{i=1}^t r_{i}}, \\
    \label{equation_of_l1}
    L_1 &=  -\frac{\sum_{i=1}^{t} \sum_{j=1}^{r_i} s(f'(\mathbf{x}_{ij}), f(\mathbf{x}_{ij}))}{\sum_{i=1}^t r_{i}},
\end{align}
where $s(\cdot, \cdot)$ measures the similarity (e.g., cosine similarity) between two feature vectors, $|\mathcal{D}_s|$ represents the number of inputs in the shadow dataset, and the denominators in $L_0$ and $L_1$ are used to normalize the losses. Our effectiveness loss is a weighted sum of the two terms, i.e., $L_0 + \lambda_1 L_1$, where $\lambda_1$ is a hyperparameter to balance the two terms.  
$L_0$ is smaller if the backdoored image encoder $f'$ produces more similar feature vectors for the reference inputs and backdoored inputs in the shadow dataset, while $L_1$ is smaller if the backdoored image encoder and clean image encoder produce more similar feature vectors for the reference inputs. 




\myparatight{Utility loss} Our BadEncoder aims to maintain the utility of the backdoored image encoder,  
i.e., maintain the accuracy of the downstream classifiers built based on our backdoored image encoder for clean inputs. 
 When both an image encoder and a text encoder are pre-trained using (image, text) pairs, a downstream classifier can be a zero-shot classifier. We note that our attack only embeds backdoor to an image encoder to be more generally applicable. Therefore, a backdoored zero-shot classifier and its clean version may predict different labels for a clean input if the backdoored image encoder and the clean image encoder produce different feature vectors for it. This is because the feature vectors produced by the text encoder do not change in the zero-shot classifiers. 
 Based on this observation, we require our backdoored image encoder and the clean image encoder to produce similar feature vectors for a clean input, e.g., a clean input in the shadow dataset. Specifically, our utility loss is smaller if our backdoored image encoder and the clean image encoder produce more similar feature vectors for a clean input in the shadow dataset. 
Formally, we define our utility loss as follows:
\begin{align}
    L_2 = - \frac{1}{|\mathcal{D}_s|} \cdot \sum_{\mathbf{x} \in \mathcal{D}_s} s(f'(\mathbf{x}), f(\mathbf{x})). 
\end{align}



\myparatight{Optimization problem} After defining the three loss terms $L_0$, $L_1$, and $L_2$, we formulate our BadEncoder as  an optimization problem. Specifically, our backdoored image encoder is a solution to the following optimization problem:
\begin{align}
\label{final_loss_term}
\min_{f'} L = L_0 + \lambda_1 \cdot L_1 + \lambda_2 \cdot L_2,
\end{align}
where $\lambda_1$ and $\lambda_2$ are two hyperparameters to balance these three loss terms. 
We will explore their impact on our BadEncoder in our evaluation. As our experimental results show, each of the three loss terms is necessary for our BadEncoder to achieve both the effectiveness goal and utility goal. Note that we can also jointly optimize the backdoored image encoder $f'$ and the backdoor triggers  $\mathbf{e}_i$'s (both locations of the triggers and their pixel values) in (\ref{final_loss_term}). However, we find that our BadEncoder with simple, physically realizable triggers (e.g., a white square located at the bottom right corner of an image) already achieves the two goals. Therefore, for simplicity, we don't optimize the  triggers in this work and leave such joint optimization as a future work. 













\subsection{Solving the Optimization Problem}
An algorithm to solve the optimization problem in (\ref{final_loss_term}) is an attack to craft a backdoored image encoder. Our BadEncoder solves the optimization problem using gradient descent. 
Specifically, we initialize the backdoored image encoder as the clean image encoder. In each epoch, we sample a mini-batch of the shadow dataset, calculate the gradient of the loss $L$, and move the backdoored image encoder a small step (called learning rate) towards the inverse of the gradient. We repeat the process for $max\_epoch$ epochs. Algorithm~\ref{alg:example}  in Appendix shows our BadEncoder to solve the optimization problem, where the function \textsc{MiniBatch} samples a mini-batch of $bs$ inputs from the shadow dataset $\mathcal{D}_s$. 




