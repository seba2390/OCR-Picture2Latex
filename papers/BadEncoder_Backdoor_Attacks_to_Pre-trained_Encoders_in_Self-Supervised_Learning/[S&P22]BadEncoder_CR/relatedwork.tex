\section{Related Work}





\subsection{Self-supervised Learning}
Self-supervised learning is a new AI paradigm that aims to pre-train encoders that can be used for many downstream tasks using a large amount of unlabeled data. It has been applied to a variety of domains such as  natural language processing (NLP), graph, and computer vision, and has achieved state-of-the-art performance in many downstream tasks in these domains. 
Specifically, in the   NLP domain, many pre-trained language models~\cite{devlin2018bert,radford2018improving,radford2019language,brown2020language,yang2019xlnet} were proposed. Specifically, the idea is to pre-train a language model on a large amount of unlabeled text. The pre-trained language model can be further used for many downstream NLP tasks such as text classification and question answering. In the graph domain, self-supervised learning has been used to pre-train Graph Neural Networks (GNNs)~\cite{hu2020strategies,qiu2020gcc} to learn  transferable structural graph representations. The pre-trained GNN can be used for many downstream tasks, e.g., graph classification. 
In the computer vision domain, an image encoder can be pre-trained using unlabeled images~\cite{hadsell2006dimensionality,he2020momentum,chen2020simple,hjelm2018learning,grill2020bootstrap} or  (image, text) pairs~\cite{srivastava2012multimodal,joulin2016learning,thomee2016yfcc100m,li2020unicoder,radford2021learning}. When   (image, text) pairs are used, a text encoder is also pre-trained and can be used for zero-shot classification. 







\subsection{Backdoor Attacks}
Deep neural networks are vulnerable to backdoor attacks in various domains~\cite{gu2017badnets,chen2017targeted,liutrojaning2018,dai2019backdoor,bagdasaryan2020backdoor,zhang2020backdoor,xi2020graph}, e.g., image, text, and graph. In this work, we focus on the image domain. Next, we review backdoor attacks in these domains. 


\noindent
{\bf Image:} In backdoor attacks to image classification~\cite{gu2017badnets,chen2017targeted,liao2018backdoor,liutrojaning2018,yao2019latent,saha2020hidden,turner2019label,li2019invisible,tan2019bypassing,liu2020reflection,salem2020dynamic}, an attacker aims to inject a hidden behavior 
into a classifier. In particular, the backdoored classifier predicts any image embedded with a trigger into a target class. Existing backdoor attacks~\cite{gu2017badnets,chen2017targeted,liutrojaning2018,yao2019latent} directly inject a backdoor into a classifier. For instance,  BadNets~\cite{gu2017badnets} injects a backdoor into a classifier via poisoning its training images, i.e., adding a backdoor trigger to the training inputs and changing their labels to the target class. Liu et al.~\cite{liutrojaning2018} proposed to first invert a classifier to generate a backdoor trigger, and then inject the backdoor into the classifier via retraining it. 


Yao et al.~\cite{yao2019latent} proposed latent backdoor attack (LBA) to transfer learning. When extended to self-supervised learning,  LBA injects a backdoor into a teacher classifier built based on the image encoder and a labeled dataset similar to the target downstream dataset. When the backdoored teacher classifier is used to fine-tune a student classifier for the target downstream task, the student classifier inherits the backdoor behavior. The key differences between LBA and our BadEncoder are as follows. First, LBA requires a large labeled dataset for the target downstream task. 
In contrast, our BadEncoder only requires a few reference inputs from the target class and an arbitrary unlabeled shadow dataset. Second, as demonstrated by our experiments, even if such labeled data is available, LBA achieves suboptimal attack effectiveness when extended to self-supervised learning.  
Third, our BadEncoder can attack multiple target downstream tasks simultaneously while LBA only attacks a single target downstream task by design. 


We note that Carlini et al.~\cite{carlini2021poisoning} recently proposed data poisoning and backdoor attacks to self-supervised learning, which is concurrent to ours. Their attacks poison the pre-training dataset, but they were designed for (image, text) pairs based self-supervised learning. 



\noindent
{\bf Text:} Some studies~\cite{dai2019backdoor,chen2020badnl,zhang2020trojaning} showed that  natural language classifiers are also vulnerable to backdoor attacks.  
For instance, Zhang et al.~\cite{zhang2020trojaning} proposed backdoor attacks to pre-trained language models. These studies are different from ours as we focus on image encoders. Moreover, Zhang et al. require an attacker to have substantial knowledge about the downstream tasks. For instance, an attacker requires access to a subset of the training dataset of the target downstream task. In contrast, our attack does not require such information. 



% \vspace{-1mm}
\noindent
{\bf Graph:} Backdoor attacks have also been studied for graph classification~\cite{zhang2020backdoor,xi2020graph}. For instance, Zhang et al.~\cite{zhang2020backdoor} developed a subgraph based backdoor attack to  GNNs. 
Xi et al.~\cite{xi2020graph} also proposed a subgraph based backdoor attack, which considers both topological structures and descriptive features when designing backdoor triggers. 


We note that some studies~\cite{ji2017backdoor,ji2018model,shafahi2018poison}
proposed targeted data poisoning attacks that involve a feature extractor (i.e., pre-trained image encoder in our context). For instance, Ji et al.~\cite{ji2018model} modified a feature extractor such that a downstream classifier predicts a particular attacker-chosen clean target input as attacker-chosen target class. Shafahi et al.~\cite{shafahi2018poison} adds perturbation to some training inputs in the target class in the target downstream dataset such that the clean feature extractor produces similar feature vectors for the perturbed training inputs and the attacker-chosen clean target inputs; and then the downstream classifier trained using the perturbed training inputs will predict the clean target inputs as the target class. The key difference with our work is that our attack injects backdoors into the image encoder such that a backdoored downstream classifier predicts the target class for \emph{any} input embedded with a pre-defined trigger. 







\subsection{Defenses against Backdoor Attacks}
 Defenses against backdoor attacks can be categorized into \emph{empirical defenses}~\cite{tran2018spectral,wang2019neural,guo2019tabor,chen2018detecting,xu2019detecting,gao2019strip,liu2019abs,chen2019deepinspect,tang2019demon,chou2020sentinet,doan2020februus} and \emph{provable defenses}~\cite{chiang2019certified,levine2020randomized,xiang2020patchguard,wang2020certifying,weber2020rab,zhang2020backdoor,metzen2021efficient}. 
 
\noindent
{\bf Empirical defenses:} A family of empirical defenses~\cite{tran2018spectral,wang2019neural,xu2019detecting,gao2019strip,liu2019abs,doan2020februus} 
aim to detect whether there is a backdoor (or a trigger) in a classifier (or an input). For instance, Wang et al.~\cite{wang2019neural} proposed Neural Cleanse~\cite{wang2019neural} which tries to detect whether a classifier (i.e., a downstream classifier in our context) is backdoored or not. In particular, they first try to reverse engineer a trigger for each possible class  and then use anomaly detection to predict whether the classifier is backdoored or not. Liu et al.~\cite{liu2019abs} proposed to detect  backdoor via analyzing the behaviors of a neuron under different levels of stimulation. Gao et al.~\cite{gao2019strip} proposed STRIP, which predicts an  input has a  trigger embedded if the predicted labels for randomly perturbed versions of the input have small entropy. 
Another family of empirical defenses~\cite{liu2018fine,wang2019neural} try to remove the backdoor in a classifier. For instance, Liu et al.~\cite{liu2018fine} proposed to first prune the neurons that are less informative and then fine-tune the pruned classifier to remove the backdoor. Our results show that \CRR{Neural Cleanse and MNTD, two state-of-the-art empirical defenses, cannot detect our backdoor attacks.} 



\noindent
{\bf Provable defenses:} Provable defenses~\cite{wang2020certifying,weber2020rab,zhang2020backdoor,metzen2021efficient,chiang2019certified,levine2020randomized,xiang2020patchguard,jia2020certified,jia2021intrinsic} can provide provable robustness guarantees against backdoor attacks. In particular, given an input, these defenses can provably guarantee that the predicted label remains unchanged when the trigger size is smaller than a threshold. For instance, Wang et al.~\cite{wang2020certifying} leveraged randomized smoothing to mitigate backdoor attacks and found that existing randomized smoothing techniques provide limited provable robustness guarantees.
Since backdoor attacks embed a trigger/patch to a testing input, provable defenses against adversarial patches can be used to mitigate them.
Chiang et al.~\cite{chiang2019certified} proposed the first provable defense against adversarial patches, which leverages interval bound propagation. 
Recently, Xiang et al.~\cite{xiang2020patchguard} proposed PatchGuard which achieves the state-of-the-art certified accuracy against adversarial patches. However, our experimental results indicate that PatchGuard provides insufficient  robustness guarantees under our attacks. 