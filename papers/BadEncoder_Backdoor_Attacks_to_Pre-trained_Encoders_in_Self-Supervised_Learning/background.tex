\section{Background on Self-supervised Learning}
\label{sec:background}
Self-supervised learning aims to pre-train an image encoder using a large amount of unlabeled data, and the pre-trained image encoder can then be used to build  classifiers for many downstream tasks (we consider image classification tasks in this work) with a small amount of or no labeled data. 
The image encoder can be pre-trained by a  resourceful service provider (e.g., Google, Facebook, OpenAI) and then shared with customers to build downstream classifiers. 
The self-supervised learning pipeline consists of two key components, i.e., \emph{pre-training an image encoder} and \emph{building a downstream classifier}. Next, we discuss the two components. 



\subsection{Pre-training an Image Encoder} An image encoder is  a neural network which takes an image as input and outputs a \emph{feature vector} for it. Self-supervised learning pre-trains an image encoder using a large amount of unlabeled data which we call \emph{pre-training dataset}. The pre-training dataset could contain unlabeled images or  (image, text) pairs. Next, we discuss how to train image encoders based on unlabeled images or  (image, text) pairs.



\myparatight{Using unlabeled images} Among many methods~\cite{hadsell2006dimensionality,pathak2016context,noroozi2016unsupervised,he2020momentum,chen2020simple,hjelm2018learning,grill2020bootstrap} to pre-train an image encoder based on unlabeled images, \emph{contrastive learning}~\cite{hadsell2006dimensionality,he2020momentum,chen2020simple,hjelm2018learning,grill2020bootstrap} achieves state-of-the-art performance. Roughly speaking, the goal of contrastive learning is to learn an image encoder  that  produces similar feature vectors for different augmented versions of the same input image but produces dissimilar feature vectors for different input images. Specifically, contrastive learning quantifies such a goal using \emph{contrastive loss} defined on the unlabeled images. 



We use SimCLR~\cite{chen2020simple}, a popular contrastive learning algorithm, as an example to illustrate the idea of contrastive learning. SimCLR contains several major components. The first component is \emph{data augmentation}, which contains a sequence of data augmentation operations such as random crop, random Gaussian blur, etc.. Given an input, this component produces an \emph{augmented input} via sequentially applying these operations to the input. The second component is an \emph{image encoder}, which outputs a feature vector for an input or an augmented input. The third component is a \emph{projection head}, which can be a multilayer perceptron (MLP) and maps a feature vector to a \emph{latent vector} that is used to define contrastive loss. 

Given a batch of $N$ input images, SimCLR creates $2\cdot N$ augmented inputs via applying data augmentation twice for each input. Two augmented inputs form a \emph{positive pair} if they were augmented from the same input, and they form a \emph{negative pair} otherwise.  
Roughly speaking, SimCLR learns the image encoder and projection head to maximize the cosine similarities between the latent vectors of the positive pairs and minimize those of the negative pairs. Formally, SimCLR defines contrastive loss  for a positive pair $(i,j)$  as follows: 
\begin{align}
    \ell_{i,j} = - \log(\frac{\exp(sim(\mathbf{z}_i, \mathbf{z}_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{I}(k \neq i) \cdot \exp(sim(\mathbf{z}_i, \mathbf{z}_k)/\tau) } ),
\end{align}
where $\mathbf{z}_i$ and $\mathbf{z}_j$ are the latent vectors of the positive pair, $\mathbf{z}_i$ and $\mathbf{z}_k (k \neq i, j)$ are the latent vectors of a negative pair, $sim(\cdot,\cdot)$ measures the cosine similarity between two latent vectors, $\mathbb{I}$ is an indicator function, $\tau$ represents a temperature parameter, and $\exp$ is the natural exponential function.  The final contrastive loss is the sum of the contrastive loss for all positive pairs. SimCLR learns the image encoder and projection head via minimizing the final contrastive loss. 






\myparatight{Using unlabeled (image, text) pairs} Another family of self-supervised learning methods~\cite{srivastava2012multimodal,joulin2016learning,thomee2016yfcc100m,li2020unicoder,radford2021learning} try to pre-train an image encoder based on unlabeled (image, text) pairs. We note that, other than an image encoder, these methods also pre-train a text encoder, which takes a text (e.g., a sentence) as an input and outputs a feature vector for it.  
Recently, Radford et al.~\cite{radford2021learning} proposed \emph{Contrastive Language-Image Pre-training (CLIP)} which learns an image encoder and a text encoder using 400 million (image, text) pairs collected from the Internet, and achieves state-of-the-art performance. Given a batch of $N$ (image, text) pairs (we call them \emph{positive pairs}), CLIP constructs $N\times (N -1)$  negative (image, text) pairs, each of which consists of the image in one positive pair and the text in another positive pair. CLIP calculates a cosine similarity for each positive/negative (image, text) pair. Specifically, given a (image, text) pair, the image encoder produces a feature vector for the image and the text encoder produces a feature vector for the text; and CLIP calculates the cosine similarity between the two feature vectors for the pair. Roughly speaking, CLIP learns the image encoder and text encoder to maximize the cosine similarity for the $N$ positive pairs and minimize the cosine similarity for the $N\times (N -1)$  negative pairs. 



\subsection{Building a Downstream Classifier}
The pre-trained image encoder can be used as a feature extractor to build classifiers (called \emph{downstream classifiers}) for many downstream tasks. 
Depending on whether a labeled training dataset is required or not, a downstream classifier could be \emph{multi-shot classifier} or \emph{zero-shot classifier}. A multi-shot classifier is trained via supervised learning with multiple labeled training examples, while a zero-shot classifier requires zero labeled training examples. Note that zero-shot classifier requires both  an image encoder and a text encoder, i.e., zero-shot classifier is only applicable when the image/text encoder is pre-trained based on (image, text) pairs.  

\myparatight{Multi-shot classifier}
In this scenario, we have multiple labeled training examples (we call them \emph{downstream dataset}) for the downstream task. 
We use the pre-trained image encoder to produce a feature vector for each image in the downstream dataset, and then we  train a classifier via the standard supervised learning. 
Given a testing input, we  first use the pre-trained image encoder to produce a feature vector for it and then use the  trained classifier to predict a label for it. 

    


\myparatight{Zero-shot classifier} In this scenario, we have zero labeled training examples for the downstream task, but both an image encoder and a text encoder are available. To build a zero-shot classifier for a downstream task, we first construct a \emph{context sentence} for each class of the downstream task. For instance, Radford et al.~\cite{radford2021learning} showed that a context sentence like ``A photo of a \{class name\}'' can be a good default template that outperforms the baseline of using only label text (i.e., ``\{class name\}''), where ``\{class name\}'' can be ``stop sign'', ``yield'', ``speed limit'', etc. when the downstream task is traffic sign classification. Moreover, they also found that the accuracy of a zero-shot classifier can be further improved by customizing the context sentence for each downstream task. We can construct $c$ context sentences for the $c$ classes of the downstream task. We use the text encoder to produce a feature vector for each context sentence. Given a testing image, we use the image encoder to produce a feature vector for it. Then, the zero-shot classifier predicts the testing image as the class whose context sentence's feature vector has the largest cosine similarity with the image's feature vector. 



