\section{Related Work}
\label{sec:related}

\textbf{Diffusion Models}
The advent of the diffusion model~\cite{sohl2015deep,ddpm} has spurred significant advancements in the field of image generation. Over the course of just a few years, we have observed the diffusion model making groundbreaking progress in a variety of fields. This includes areas such as super-resolution~\cite{saharia2022image}, colorization~\cite{saharia2022palette}, novel view synthesis~\cite{watson2022novel}, style transfer~\cite{zhang2023inversion}, and 3D generation~\cite{poole2022dreamfusion, tang2023make, cheng2023sdfusion}. These breakthroughs have been achieved through various means. Some are attributable to the enhancements in network structures such as Latent Diffusion Models (also known as Stable Diffusion)~\cite{ldm}, GLIDE~\cite{glide}, DALLE2~\cite{dalle2}, SDXL~\cite{podell2023sdxl}, and Imagen~\cite{imagen}. Others are a result of improvements made in the training paradigm~\cite{nichol2021improved,song2019generative,dhariwal2021diffusion,song2020score,ddim}. Furthermore, the ability to incorporate various conditions during image generation has played a crucial role. These conditions include elements such as layout~\cite{cheng2023layoutdiffuse,ldm}, segmentation~\cite{avrahami2023spatext,avrahami2022blended,balaji2022ediffi,yang2023paint}, or even the use of an image as reference~\cite{mou2023t2i,ruiz2023dreambooth,textualinversion}.

\textbf{Diffusion-based Text-Guided Image Editing} 
Image editing is a process where we don't desire completely unconstrained generation but modifying an image under certain guidance (\ie a reference image) during its generation. Various methods have been proposed to address this task. Simple zero-shot image-to-image translation methods, such as SDEdit~\cite{meng2021sdedit} performed through diffusion and denoising on reference image. Techniques that incorporate a degree of optimization, such as Imagic~\cite{kawar2023imagic}, which utilizes the concept of textual inversion~\cite{textualinversion}, and Null-text Inversion~\cite{mokady2023null}, which leverages the Prompt-to-Prompt strategy~\cite{prompt2prompt} to control the behavior of cross-attention in the diffusion model for editing, have also been explored. These methods can impede the speed of editing due to the necessity for per-image-per-optimization. Models like Instruct Pix2Pix~\cite{brooks2023instructpix2pix} have been employed to achieve image editing by training on synthetic data. This approach adeptly balances editing capabilities and fidelity to the reference image.


\textbf{Diffusion-based Text-Guided Video Editing} 
The success of the diffusion model in image generation has been extended to video generation as well~\cite{ho2022video,harvey2022flexible,blattmann2023align,mei2023vidm,ho2022imagen}, and similarly, text-guided video editing has sparked interest within the community. Techniques akin to those in image editing have found applications in video editing. For instance, Dreamix~\cite{molad2023dreamix} uses diffusion and denoising for video editing, reminiscent of the approach in SDEdit~\cite{meng2021sdedit}. Strategies altering the behavior of the cross-attention layer to achieve editing, like the Prompt-to-Prompt~\cite{prompt2prompt}, have been adopted by methods such as Vid2Vid-Zero~\cite{vid2vid}, FateZero~\cite{qi2023fatezero}, and Video-p2p~\cite{liu2023video}. Recent developments~\cite{wang2023videocomposer,gen1,zhao2023controlvideo} leverage certain condition modalities extracted from the original video, like depth maps or edge sketches, to condition the video generation. The related, albeit non-diffusion-based, method Text2live~\cite{bar2022text2live} also provides valuable perspectives on video editing.

% early studies about theory and model of diffusion models
% * Deep unsupervised learning using non-equilibrium thermodynamics,
% * Denoising diffusion probabilistic models,
% * Generative modeling by estimating gradients of the data distribution
% * “Score-Based Generative Modeling through Stochastic Differential Equations
% * “Diffusion models beat GANs on image synthesis,
% * “Improved denoising diffusion probabilistic models
% * Denoising Diffusion Implicit Models

% diffusion model's applications
% * super-resolution: . Image superresolution via iterative refinement
% * colorization:  Palette: Image-to-image diffusion models.
% * novel view synthesis:  Novel view synthesis with diffusion models
% * style transfer: Inversion-based style transfer with diffusion models
% * 3D generation: DreamFusion: Text-to-3D using 2D Diffusion & Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior & SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation

% other conditional diffusion model generation
% * layoutdiffuse
% * segmentation: SpaText: Spatio-Textual Representation for Controllable Image Generation & Blended Diffusion for Text-driven Editing of Natural Images & eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers & Paint by Example: Exemplar-based Image Editing with Diffusion Models
% * controlNet

% succesful large models
% * “High-Resolution Image Synthesis with Latent Diffusion Models (LDM/stable diffusion)
% * GLIDE
% * DALLE2
% * SDXL
% * Imagen
% * DeepFloyd

% diffusion model for text-guided image editing
% * Imagic: Text-based real image editing with diffusion models
% * Prompt-to-Prompt image editing with cross attention control
% * Null-text Inversion for Editing Real Images using Guided Diffusion Models
% * Instructpix2pix: Learning to follow image editing instructions
% * SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations
% * Sine: Single image editing with text-to-image diffusion models
% * Zero-shot Image-to-Image Translation

% diffusion model for text-guided video editing
% * Dreamix: Video Diffusion Models are General Video Editors
% * tune-a-video
% * VideoComposer: Compositional Video Synthesis with Motion Controllability
% * FateZero : Fusing Attentions for Zero-shot Text-based Video Editing
% * Video-p2p: Video editing with cross-attention control
% * Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators
% * Gen-1: Structure and Content-Guided Video Synthesis with Diffusion Models
% * ControlVideo: Training-free Controllable Text-to-Video Generation
% * ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing

% video editing using non-diffusion methods
% * text2live


% other diffusion models
% * dream booth
% * textual inversion