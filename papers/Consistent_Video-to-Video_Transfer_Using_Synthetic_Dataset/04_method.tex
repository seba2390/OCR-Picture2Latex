\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/archi.pdf}
    \caption{(a) The architecture of \ours. For handling long videos processed in multiple batches, our approach leverages the proposed LVSC to utilize the final frames from the preceding batch as reference frames for the subsequent batch. (b) The inflated convolutionals and attention layer, as well as temporal attention layer can handle 5D video tensors by dynamically reshaping them. (c) During each denoising iteration, the LVSC adjusts the predicted noise $\tilde{\varepsilon}_{\theta}(z_t)$ based on reference frames $z_t^{ref}$ prior to executing the DDIM denoising.}
    % \caption{The model architecture of \ours. The weights of the diffusion model are initialized using the single image editing diffusion model~\cite{brooks2023instructpix2pix} and remain frozen. Trainable temporal layers are incorporated subsequent to the spatial attention layers in the UNet model.}
    \label{fig:model}
\end{figure*}


\section{Model Architecture}
\label{sec:method}

\subsection{Preliminaries}

\textbf{Diffusion models} learn to predict content in an image by iteratively denoising an entirely random Gaussian noise. In the training process, the input image is first corrupted by the Gaussian noise, termed diffusion. The model's task is to restore a diffused noisy image to its original form. This process can be considered as the optimization of the variational lower bound on the distribution $p(x)$ of the image $x$ with a $T$ step Markovian inverse process. Various conditions $c$, such as text, images, and layouts, can be incorporated into diffusion models during the learning process. The model $\varepsilon_\theta$ we aim to train is conditioned on text and its training loss can be expressed as
\begin{equation}
    L = \mathbb{E}_{x,\varepsilon\sim\mathcal{N}(0,1),t}\|\varepsilon - \varepsilon_\theta(x_t, t, c)\|^2
\end{equation}

Where $x_t$ is the result of diffusing the input image $x$ at timestep $t \in [1, T]$ using random noise $\varepsilon$. In practice, we employ the Latent Diffusion Model (LDM)~\cite{ldm} as our backbone model and condition it on input videos by concatenating the videos to the latent space, as illustrated in \Cref{fig:model} (a). Instead of generating images in the RGB pixel space, LDM employs a trained Vector Quantized Variational AutoEncoder (VQVAE)~\cite{vqvae} to convert images to visual codes in latent space. This allows the model to achieve better results with the same training resources. Specifically, the image $x$ is first transformed into a latent code $z = \mathrm{VQEnc}(x)$, and the model learns to predict $p(z)$ from random noise. In the context of the video editing, two distinct conditions exist: the editing instructions and the reference video, represented by $c_T$ and $c_V$ respectively. The model is designed to predict $p(z)$ by optimizing the following loss function:

\begin{equation}\label{eq.ldm_objective}
    L = \mathbb{E}_{\mathrm{VQEnc}(x),\varepsilon\sim\mathcal{N}(0,1),t}\|\varepsilon - \varepsilon_\theta(z_t, t, c_V, c_T)\|^2
\end{equation}

\subsection{Inflate Image-to-Image Model To Video-to-Video Model}
% make it shorter
% add more referecne about modle inflation
Given the substantial similarities between image-to-image transfer and video-to-video transfer, our model utilizes a foundational pre-trained 2D image-to-image transfer diffusion model. Using foundational model simplifies training but falls short in generating consistent videos, causing noticeable jitter when sampling frames individually. Thus, we transform this image-focused model into a video-compatible one for consistent frame production. We adopt model inflation as recommended in previous studies~\cite{tuneavideo,guo2023animatediff}. This method modifies the single image diffusion model to produce videos. The model now accepts a 5D tensor input $x \in \mathbb{R}^{b \times c \times f \times h \times w}$. Given its architecture was designed for a 4D input, we adjust the convolutional and attention layers in the model (\Cref{fig:model} (b)). Our inflation process involves: (1) Adapting convolutional and attention layers to process a 5D tensor by reshaping it temporarily to 4D. Once processed, it's reverted to 5D. (2) Introducing temporal attention layers for frame consistency. When these layers handle a 5D tensor, they reshape it to a 3D format, enabling pixel information exchange between frames via attention.
% Given the substantial similarities between image-to-image transfer and video-to-video transfer, our model utilizes a foundational pre-trained 2D image-to-image transfer diffusion model. This employment of a foundational model significantly alleviates the burden of model training. However, the foundational image-to-image diffusion model lacks the ability to generate consistent videos. The resultant video lacks consistency if we sample each frame individually, leading to perceptible jittering from a human viewpoint. Therefore, we first transform an image-to-image model into a video-to-video model capable of producing consistent frames.

% To achieve this objective, we implement model inflation, as suggested in related works~\cite{tuneavideo,guo2023animatediff}. This approach aims to convert a single image diffusion model into one that can generate videos. When handling video, the model's input is a 5D tensor $x \in \mathbb{R}^{b \times c \times f \times h \times w}$, where $b$ represents the batch size, $c$ is the input channels, $f$ indicates the number of frames, and $h$ and $w$ are the spatial resolution. Consequently, the existing convolutional and attention layers in the model require modification, as they were initially designed for a 4D input $x \in \mathbb{R}^{b \times c \times h \times w}$. 

% The implementation of model inflation encompasses two aspects: Firstly, we enable convolutional and attention layers to handle a 5D tensor by reshaping the frame dimension and batch size together to transform the tensor back to 4D. After the convolution and attention layers process it, the tensor is reshaped back to 5D. Secondly, to ensure consistency between generated frames, we introduce several temporal attention layers into the model. These layers, when dealing with a 5D tensor, first reshape it into a 3D tensor $\in \mathbb{R}^{b hw \times c \times f}$ and then conduct an attention operation along the frame's dimension. In this process, pixels at identical positions in different frames exchange information through attention.



\subsection{Sampling}
During sampling, we employ an extrapolation technique named Classifier-Free Guidance (CFG)~\cite{cfg} to augment the generation quality. Given that we have two conditions, namely the conditional video and the editing prompt, we utilize the CFG method for these two conditions as proposed in~\cite{brooks2023instructpix2pix}. Specifically, for each denoising timestep, three predictions are made under different conditions: the unconditional inference $\varepsilon_\theta(z_t,\varnothing,\varnothing)$, where both the conditions are an empty string and an all-zero video; the video-conditioned prediction $\varepsilon_\theta(z_t,c_V,\varnothing)$; and the video and prompt-conditioned prediction $\varepsilon_\theta(z_t,c_V,c_T)$. Here, we omit timestep $t$ for symbolic convenience. The final prediction is an extrapolation between these three predictions with video and text classifier-free guidance scale $s_V \geq 1$ and $s_T \geq 1$. 

\begin{align}
    \tilde{\varepsilon_\theta}(z_t, c_V, c_T) &= \varepsilon_\theta(z_t, \varnothing, \varnothing) \\ \nonumber
    &+ s_V \cdot (\varepsilon_\theta(z_t, c_V, \varnothing) - \varepsilon_\theta(z_t, \varnothing, \varnothing)) \\ \nonumber
    &+ s_T \cdot (\varepsilon_\theta(z_t, c_V, c_T) - \varepsilon_\theta(z_t, c_V, \varnothing))
\end{align}


\subsection{Long Video Sampling Correction for Editing Long Videos}\label{sec.long_vid_editing}

% During video editing, we often encounter a challenge where the length of the video exceeds the number of frames the model can process at a given time. Additionally, the model's performance may decline when the quantity of input frames is arbitrarily adjusted, as the number of frames is typically fixed during the model's training phase. Therefore, for particularly long videos, it becomes necessary to partition the video into several smaller batches and independently sample each batch. This approach, however, introduces a new issue: while the frames within a single batch may maintain consistency, there is no assurance of consistency between different batches. This lack of inter-batch consistency can lead to visually noticeable discontinuities at the points of batch transitions.

In video editing, models often face limitations in processing extended video lengths in one go. Altering the number of input frames can compromise the model's efficacy, as frame count is usually preset during training. To manage lengthy videos, we split them into smaller batches for independent sampling. While intra-batch frame consistency is preserved, inter-batch consistency isn't guaranteed, potentially resulting in visible discontinuities at batch transition points.

To address this issue, we propose Long Video Sampling Correction (LVSC): during sampling, the results from the final $N$ frames of the previous video batch can be used as a reference to guide the generation of the next batch (\Cref{fig:model} (c)). This technique helps to maintain visual consistency across different batches. Specifically, let $z^{ref} = z^{prev}_0[:, -N:] \in \mathbb{R}^{1, N, c, h, w}$ denote the last N frames from the transfer result of the previous batch. Here, to avoid confusion with the batch size, we set the batch size to 1. The ensuing batch is the concatenation of noisy reference frames and subsequent frames $[z^{ref}_t, z_t]$. On the model's prediction $\varepsilon_\theta(z_t) := \varepsilon_\theta(z_t, t, c_V, c_T)$, we implement a score correction and the final prediction is the summation between raw prediction $\varepsilon_\theta(z_t)$ and correction term $\varepsilon_t^{ref}-\varepsilon_\theta(z_t^{ref})$, where $\varepsilon_t^{ref}$ is the closed-form inferred noise on reference frames. For notation simplicity, we use $\varepsilon(z_t^{ref})$ and $\varepsilon(z_t)$ to denote the model's predictions on reference and subsequent frames, though they are processed together by the model instead of separate inputs.

\begin{align}
\varepsilon_t^{ref} &= \frac{z^{ref}_t - \sqrt{\Bar{\alpha_t}}z^{ref}}{\sqrt{1 - \Bar{\alpha_t}}} \in \mathbb{R}^{1,N,c,h,w}\label{eq.1} \\
\tilde{\varepsilon_\theta}(z_t) &= \varepsilon_\theta(z_t) + \frac{1}{N} \sum_{i=1}^N (\varepsilon_t^{ref}[:, i] - \varepsilon_\theta(z_t^{ref})[:, i])\label{eq.lvsc}
\end{align}

We apply averaging on the correction term when there are multiple reference frames as shown in \Cref{eq.1,eq.lvsc}, where $\Bar{\alpha_t}$ is the diffusion coefficient for timestep $t$ (\ie $z_t = \mathcal{N}(\sqrt{\Bar{\alpha_t}}z_0, (1 - \Bar{\alpha_t})I)$). In our empirical observations, we find that when the video has global or holistic camera motion, the score correction may struggle to produce consistent transfer results. To address this issue, we additionally introduce a motion compensation that leverages optical flow to establish correspondences between each pair of reference frames and the remaining frames in the batch. We then warp the score correction in accordance with this optical flow with details presented in~\Cref{sec.motion_compensation}.
