\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/data_pipe.pdf}
    \caption{The pipeline for generating a synthetic dataset using a large language model, whose outputs include the prompt triplet consisting of input, edit, and edited prompts, as well as a corresponding pair of videos. Visualization of generated videos can be found in \Cref{sec.appendix_synthetic_vid}}
    \label{fig:synthetic_dataset}
\end{figure}


\section{Synthetic Paired Video Dataset}\label{sec.synthetic_data_pipe}

% A crucial element for training a model capable of arbitrary video-to-video transfer, as opposed to a per-video-per-model approach, lies in the availability of ample paired training data. Each pair consists of an input video and its corresponding edited version, providing the model with a diverse range of examples essential for generalized performance. However, the scarcity of naturally occurring video pairs with such correspondence poses a significant challenge to the training process.

% To circumvent this, we advocate for the use of synthetic data, which can accurately maintain this correspondence while fulfilling the necessary conditions for effective video-to-video transfer learning. The merit of synthetic data generation is well-documented in the realm of text-guided editing~\cite{brooks2023instructpix2pix}, illuminating its potential for video-to-video transfer. This approach helps us construct the optimal conditions for the model to learn, offering a practical solution to the inherent obstacles associated with acquiring matching real-world video pairs. 

A crucial element for training a model capable of arbitrary video-to-video transfer, as opposed to a per-video-per-model approach, lies in the availability of ample paired training data. Each pair consists of an input video and its corresponding edited version, providing the model with a diverse range of examples essential for generalized performance. However, the scarcity of naturally occurring video pairs with such correspondence poses a significant challenge to the training process.

To address this, we introduce the concept of a trade-off between initial training costs, including dataset creation, and long-term efficiency. We advocate for the use of synthetic data, which, while incurring an upfront cost, accurately maintains the required correspondence and fulfills the conditions for effective video-to-video transfer learning. The merit of this synthetic data generation approach is underscored by its potential to offset the initial investment through substantial time savings and efficiency in the subsequent inference stages. This approach contrasts with `per-vid-per-model' methods that necessitate repetitive fine-tuning for each new video, making our strategy both cost-effective and practical in diverse real-world applications.

The potential of synthetic data generation, well-documented in the realm of text-guided editing~\cite{brooks2023instructpix2pix}, is thereby extended to video-to-video transfer. This method allows us to construct the optimal conditions for the model to learn, offering a practical solution to the inherent obstacles associated with acquiring matching real-world video pairs.

\subsection{Dataset Creation:}
In order to generate the synthetic dataset, we leverage the approach of Prompt-to-Prompt (PTP)~\cite{prompt2prompt}, a proven method for producing paired samples in the field of image editing. The PTP employs both self-attention and cross-attention replacements to generate semantically aligned edited images. In self-attention, the post-softmax probability matrix of the input prompt replaces that of the edited prompt. The cross-attention replacement specifically swaps the text embedding of the edited prompt with that of the input prompt. 

In the context of video-to-video transfer, we adapt PTP by substituting its underlying image diffusion models with a video diffusion model. In addition, we extend the self-attention replacement to temporal attention layers, a critical modification for maintaining structural coherence between input and edited videos. \Cref{fig:synthetic_dataset} shows the overall pipeline for data generation. To guide the synthetic data generation, we employ a set of paired text prompts, comprising an input prompt, an edited prompt, and an edit prompt. The input prompt corresponds to the synthetic original video, while the edited prompt and the edit prompt represent the desired synthetic edited video and the specific changes to be applied on the original video respectively. 

\subsection{Prompt Sources} 
Our synthetic dataset is constructed using paired prompts from two differentiated sources, each serving a specific purpose in the training process. The first source, LAION-IPTP, employs a fine-tuned GPT-3 model to generate prompts based on 700 manually labeled captions from the LAION-Aesthetics dataset~\cite{brooks2023instructpix2pix}. This yielded a set of 450,000 prompt pairs, of which 304,168 were utilized for synthetic video creation. While the GPT-3-based prompts offer a substantial volume of data, they originate from image captions and thus have limitations in their applicability to video generation. This led us to incorporate a second source, WebVid-MPT, which leverages video-specific captions from the WebVid 10M dataset~\cite{webvid}. Using MPT-30B~\cite{mpt} in a zero-shot manner, we devised a set of guidelines (see \Cref{sec:appendix_mpt} for details) to generate the needed paired prompts, adding an additional 100,000 samples to our dataset. Crucially, the WebVid-MPT source yielded a threefold increase in the success rate of generating usable samples compared to the LAION-IPTP source after sample filtering, reinforcing the need for video-specific captions in the training process. The LAION-IPTP prompt source demonstrated a success rate of 5.49\% (33,421 successful generations from 608,336 attempts). The WebVid-MPT prompt source showed a higher success rate of 17.49\% (34,989 successful generations from 200,000 attempts).

\subsection{Implementation Details and Sample Selection Criteria} 
\noindent\textbf{Implementation Details:} We use public available text-to-video model\footnote{https://modelscope.cn/models/damo/text-to-video-synthesis/summary} for generating synthetic videos. Each video has 16 frames, processed over 30 DDIM~\cite{ddim} steps by the diffusion model. The self-attention and cross-attention replacements in the Prompt-to-Prompt model terminate at a random step within the ranges of 0.3 to 0.45 and 0.6 to 0.85 respectively (out of 30 steps). The classifier-free guidance scale is a random integer value between 5 and 12.

\noindent\textbf{Sample Selection Criteria:} To ensure the quality of our synthetic dataset, we employ a CLIP-based filtering. For each prompt, generation is attempted with two random seeds, and three distinct clip scores are computed: CLIP Text Score: Evaluates the cosine similarity between each frame and the text.
CLIP Frame Score: Measures the similarity between original and edited frames.
CLIP Direction Score: Quantifies the similarity between the transition of original-frame-to-edited-frame and original-text-to-edited-text.
These scores are obtained for each of the 16 frames and averaged. A sample is preserved if it meets the following conditions: CLIP Text Score $>$ 0.2 for both original and edited videos, CLIP Direction Score $>$ 0.2, and CLIP Frame Score $>$ 0.5. Samples that fail to meet these criteria are discarded.

% \noindent\textbf{Generation Success Rate:} 
% The fusion of these implementation details and selection criteria underpins the creation of a high-quality synthetic dataset, tailored for training a proficient video-to-video transfer model.

