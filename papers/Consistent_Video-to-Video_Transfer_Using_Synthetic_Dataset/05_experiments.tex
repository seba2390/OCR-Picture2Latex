\section{Experiments}\label{sec.expt}

\subsection{Experimental Setup}
\noindent\textbf{Dataset}
For our evaluation, we used the Text-Guided Video Editing (TGVE) competition dataset\footnote{https://sites.google.com/view/loveucvpr23/track4}. The TGVE dataset contains 76 videos that come from three different sources: Videov, Youtube, and DAVIS~\cite{DAVIS}. Every video in the dataset comes with one original prompt that describes the video and four prompts that suggest different edits for each video. Three editing prompts pertain to modifications in \textit{style}, \textit{background}, or \textit{object} within the video. Additionally, a \textit{multiple} editing prompt is provided that may incorporate aspects of all three types of edits simultaneously.


% \noindent\textbf{Metrics for Evaluation}
% To evaluate the edited videos, we adopt metrics that encompasses temporal consistency, faithfulness, and text alignment. We have factored in both subjective user feedback and objective automated scoring methods in our evaluation strategy. The set of questions used for the user study is elaborated upon in \cref{sec.user_study}. The automated scoring metrics are derived from the TGVE competition. These comprise two measures:  PickScore~\cite{kirstain2023pick}: This calculates the average image-text PickScore across all frames of the output videos, which is a better score than CLIP image-text score from a human-centric viewpoint. CLIP Frame (Frame Consistency)~\cite{CLIP}: This computes the average cosine similarity among all pairs of CLIP image embeddings determined across all frames of the output videos.

\noindent\textbf{Metrics for Evaluation}
Given that our focus is on text-based video editing, we look at three critical aspects. First, we assess whether the edited video accurately reflects the editing instructions. Second, we determine whether the edited video successfully preserves the overall structure of the original video. Finally, we consider the aesthetics of the edited video, ensuring it is free of imperfections such as jittering. Our evaluation is based on user study and automated scoring metrics. In the user study, we follow TGVE competition to ask users three key questions. The \textbf{Text Alignment} question: Which video better aligns with the provided caption? The \textbf{Structure} question: Which video better retains the structure of the input video? The \textbf{Quality} question: Aesthetically, which video is superior? These questions aim to evaluate the quality of video editing, focusing on the video's alignment with editing instructions, its preservation of the original structure, and its aesthetic integrity. For objective metrics, we incorporate PickScore~\cite{kirstain2023pick} that computes the average image-text alignment over all video frames and CLIP Frame (Frame Consistency)~\cite{CLIP}, which measures the average cosine similarity among CLIP image embeddings across all video frames. We prefer PickScore over the CLIP text-image score since it's tailored to more closely align with human perception of image quality, which is also noticed by~\cite{podell2023sdxl}.

\subsection{Baseline Methods}
% We compare \ours\ to the state-of-the-art publicly available text-driven video editing methods: Tune-A-Video~\cite{tuneavideo}, Vid2Vid-Zero~\cite{vid2vid}, Video-P2P~\cite{liu2023video}, and ControlVideo~\cite{zhao2023controlvideo}. Tune-A-Video has been treated as a \textit{de facto} text-driven video editing baseline in the community. The latter three methods are adapted from Tune-A-Video, introducing the cross-attention control proposed in Prompt-to-Prompt (PTP)~\cite{prompt2prompt} for Vid2Vid-Zero and Video-P2P, and ControlNet~\cite{controlnet} for ControlVideo. We evaluate all methods and sample them with 32 frames where possible. PTP-based methods are computationally heavy and thus only able to run with 8 frames. All baseline methods are sampled in a single batch to prevent inter-batch transfer inconsistency. To preserve the structure of the transferred video, baseline methods resort to latent inversion~\cite{mokady2023null}, thereby doubling the inference time. In contrast, our approach maintains the structure of the original video without requiring such time-consuming techniques. \Cref{tab:comparison} summarizes details.


We benchmark \ours\ against leading text-driven video editing techniques: Tune-A-Video~\cite{tuneavideo}, Vid2Vid-Zero~\cite{vid2vid}, Video-P2P~\cite{liu2023video}, and ControlVideo~\cite{zhao2023controlvideo}. Tune-A-Video has been treated as a \textit{de facto} baseline in this domain. Vid2Vid-Zero and Video-P2P adopt the cross-attention from Prompt-to-Prompt (PTP)\cite{prompt2prompt}, while ControlVideo leverages ControlNet\cite{controlnet}. We test all methods for 32 frames, but PTP-based ones, due to their computational demand, are limited to 8 frames. Baselines are processed in a single batch to avoid inter-batch inconsistencies and use latent inversion~\cite{mokady2023null} for structure preservation, which causes double inference time. Conversely, our method retains the video's structure more efficiently. 
% See \Cref{tab:comparison} for a summary.

We also extend the comparison to include recent tuning-free video editing methods such as TokenFlow~\cite{geyer2023tokenflow}, Render-A-Video\cite{yang2023rerender}, and Pix2Video\cite{ceylan2023pix2video}. These methods, by eliminating the necessity for individual video model tuning, present a comparable benchmark to our approach. To ensure frame-to-frame consistency, these methods either adopt cross-frame attention similar to Tune-A-Video\cite{tuneavideo}, as seen in~\cite{ceylan2023pix2video, yang2023rerender}, or establish pixel-level correspondences between features and a reference key frame as in~\cite{geyer2023tokenflow}. This approach is effective in maintaining quality when there are minor scene changes. However, in scenarios with significant differences between the key and reference frames, these methods may experience considerable degradation in video quality. This limitation is more clearly illustrated in \Cref{fig.additional_qua2} in the Appendix.

% \begin{table}[h]
% \newcommand{\bluebkg}{\cellcolor{blue!20}}
% \newcommand{\greenbkg}{\cellcolor{green!20}}
% \newcommand{\redbkg}{\cellcolor{red!20}}

% \centering
% \scriptsize
% \caption{Properties of different text-driven video editing methods. Cells in \colorbox{green!20}{green} and \colorbox{red!20}{red} indicates the preferred and not preferred features, respectively.}
% \begin{tabular}{lcccccc}
% \hline
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Model Type}} & \multirow{2}{*}{\textbf{Additional Ctrl}} & \textbf{Tune Time} & \textbf{Need Latent} & \textbf{Max}  & \multirow{2}{*}{\textbf{Prompt Type}}\\
%  & & & \textbf{/ Video} & \textbf{Inversion?} & \textbf{Frames} & \\
% \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
% Tune-A-Video & \redbkg{Per-Vid-Per-Model} & \greenbkg{No} & \redbkg{15 mins} & \redbkg Yes & \greenbkg 32 & \redbkg Original \& Target \\
% Vid2Vid-Zero & \redbkg{Per-Vid-Per-Model} & \redbkg{Prompt-to-Prompt} & \redbkg{12 mins} & \redbkg Yes & \redbkg 8 & \redbkg Original \& Target\\
% Video-P2P & \redbkg{Per-Vid-Per-Model} & \redbkg{Prompt-to-Prompt} & \redbkg{10 mins} & \redbkg Yes & \redbkg 8 & \redbkg Original \& Target \\
% ControlVideo & \redbkg{Per-Vid-Per-Model} & \redbkg{ControlNet} & \redbkg{15 mins} & \redbkg Yes & \greenbkg 32 & \redbkg Original \& Target \\
% \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
% \ours (Ours) & \greenbkg{One-Model-All-Vid} & \greenbkg{No} & \greenbkg{Not needed} & \greenbkg No & \greenbkg 32* & \greenbkg Editing \\
% \hline
% \end{tabular}
% \label{tab:comparison}
% \\ \tiny{*Though our model can process up to 32 frames, we limit it to 16 frames for evaluation, as this aligns with the training configuration and yields better results.}
% \end{table}

% \begin{table}[h]
% \centering
% \scriptsize
% \begin{tabular}{lcccccc}
% \hline
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Model Type}} & \multirow{2}{*}{\textbf{Additional Ctrl}} & \textbf{Tune Time} & \textbf{Inf Time} & \textbf{Max}  & \multirow{2}{*}{\textbf{Prompt Type}}\\
%  & & & \textbf{/ Video} & \textbf{/ Video} & \textbf{Frames} & \\
% \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
% Tune-A-Video & \multirow{2}{*}{Per-Vid-Per-Model} & \multirow{2}{*}{No} & \multirow{2}{*}{15 mins} & \multirow{2}{*}{60 secs} & \multirow{2}{*}{32} & \multirow{2}{*}{Original \& Target} \\
% \cite{tuneavideo} & &  & & & & \\
% Vid2Vid-Zero & \multirow{2}{*}{Per-Vid-Per-Model} & Prompt-to-Prompt & \multirow{2}{*}{15 mins} & \multirow{2}{*}{60 secs} & \multirow{2}{*}{8} & \multirow{2}{*}{Original \& Target} \\
% \cite{vid2vid} &  & \cite{prompt2prompt} & & & & \\
% Video-P2P & \multirow{2}{*}{Per-Vid-Per-Model} & Prompt-to-Prompt & \multirow{2}{*}{15 mins} & \multirow{2}{*}{60 secs} & \multirow{2}{*}{8} & \multirow{2}{*}{Original \& Target} \\
% \cite{liu2023video} &  & \cite{prompt2prompt} & & & & \\
% ControlVideo & \multirow{2}{*}{Per-Vid-Per-Model} & ControlNet & \multirow{2}{*}{15 mins} & \multirow{2}{*}{60 secs} & \multirow{2}{*}{32} & \multirow{2}{*}{Original \& Target} \\
%  \cite{zhao2023controlvideo} & & \cite{controlnet} & & & & \\
%  \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
% \ours (Ours) & One-Model-All-Vid & No & Not needed & 30 secs & 32 & Edit \\
% \hline
% \end{tabular}
% \caption{Comparison of Different Text-Driven Video Editing Methods}
% \label{tab:comparison}
% \end{table}

\subsection{Model Details}\label{sec.details}
Our model is adapted from a single image editing Stable Diffusion~\cite{brooks2023instructpix2pix} and we insert temporal attention modules after each spatial attention layers as suggested by~\cite{guo2023animatediff}. Our training procedure makes use of the Adam optimizer with a learning rate set at $5 \times 10^{-5}$. The model is trained with a batch size of 512 over a span of 2,000 iterations. This training process takes approximately 30 hours to complete on four NVIDIA A10G GPUs.


During sampling, we experiment with varying hyperparameters for video classifier-free guidance (VCFG) within the choice of [1.2, 1.5, 1.8], text classifier-free guidance to 10 and video resolutions of 256 and 384. A detailed visual comparison using these differing hyperparameters can be found in the supplementary material (\Cref{sec.pick_criteria}). The hyperparameters combination that achieves the highest PickScore is selected as the final sampling result. Each video is processed in three distinct batches using LVSC with a fixed frame count of 16 within a batch, including reference frames from preceeding batch, and resulting in a total frame count of 32. 


\subsection{Long Video Score Correction and Motion Compensation}


\begin{wraptable}{l}{0.5\textwidth}
    \vspace{-0.7cm}
  \centering
  \small
  \caption{Comparison of motion-aware MSE and CLIP frame similarity between the last frame of the preceding batch and the first frame of the subsequent batch on TGVE dataset.}
  \begin{tabular}{c|c|c|c}
    \hline
    \textbf{LVSC} & \textbf{MC} &  \textbf{MAMSE (\%) $\downarrow$} & \textbf{CLIPFrame $\uparrow$} \\
    \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(l){4-4}
    \xmark & \xmark & 2.02 & 0.9072 \\
    \cmark & \xmark & 1.44 & 0.9093 \\
    \cmark & \cmark & 1.37 & 0.9095 \\
    \hline
  \end{tabular}\label{tab.lvsc}
    \vspace{-0.2cm}
\end{wraptable}

To assess performance improvement, we employ CLIP Frame Similarity and Motion-Aware Mean Squared Error (MAMSE) as evaluation metrics. Unlike traditional MSE, MAMSE accounts for frame-to-frame motion by utilizing optical flow to warp images, thereby ensuring loss computation in corresponding regions. Incorporating Long Video Score Correction (LVSC) and Motion Compensation (MC) has led to enhanced performance as reflected in~\Cref{tab.lvsc}. Further qualitative comparison, detailing the benefits of LVSC and MC, are provided in ~\Cref{sec.long_vid_editing_appendix,sec.motion_compensation}.
% though the quantitative difference is small, but the qualitative effect is large to human

% Motion augmentation for the video

% \subsection{Qualitative Comparison}

% \begin{figure*}[!h]
%     \centering
%     A cat and a dog playing on the street while a girl walks around them 
    
%     $\rightarrow$ A cat and a dog playing on the beach while a girl walks around them, golden hour lighting.
%     \includegraphics[width=\linewidth]{figs/qualitative/cat-dog-play_multiple.jpg}
%     \caption{Visual comparison of the proposed method against baseline methods. Our approach effectively edits the video to align closely with the edit instructions, while maximizing the preservation of the original video content.}
%     \label{fig:qualitative_comp}
% \end{figure*}

\subsection{User Study}\label{sec.user_study}

% The objective of our user study is to evaluate the quality of video editing facilitated by our proposed method. Given that our focus is on text-based video editing, we look at three critical aspects. First, we assess whether the edited video accurately reflects the editing instructions. Second, we determine whether the edited video successfully preserves the overall structure of the original video. Finally, we consider the aesthetics of the edited video, ensuring it is free of imperfections such as jittering.

% In this study, we followed the TGVE challenge to ask users three key questions. The \textbf{Text Alignment} question: Which video better aligns with the provided caption? The \textbf{Structure} question: Which video better retains the structure of the input video? Lastly, the \textbf{Quality} question: Aesthetically, which video is superior?

\begin{table}[h]
\centering
\small
\caption{The first two columns display automated metrics concerning CLIP frame consistency and PickScore. The final four columns pertain to a user study conducted under the TGVE protocol, where users were asked to select their preferred video when comparing the method against the TAV.}
\begin{tabular}{lcccccc}
\hline
\textbf{Method} & \textbf{CLIPFrame} & \textbf{PickScore} & \textbf{Text Alignment} & \textbf{Structure} & \textbf{Quality} & \textbf{Average} \\
\cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
TAV* & 0.924 & 20.36 & - & - & - & - \\
CAMP* & 0.899 & 20.71 & 0.689 & 0.486 & 0.599 & 0.591 \\
T2I\_HERO* & 0.923 & 20.22 & 0.531 & 0.601 & 0.564 & 0.565 \\
\cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
Vid2Vid-Zero& 0.926 & 20.35 & 0.400 & 0.357 & 0.560 & 0.439 \\
Video-P2P & 0.935 & 20.08 & 0.355 & 0.534 & 0.536 & 0.475 \\
ControlVideo & 0.930 & 20.06 & 0.328 & 0.557 & 0.560 & 0.482 \\
\cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
TokenFlow & \textbf{0.940} & 20.49 & 0.287 & 0.563 & 0.624 & 0.491 \\
Pix2Video & 0.916 & 20.12 & 0.468 & 0.529 & 0.538 & 0.511 \\
Render-A-Video & 0.909 & 19.58 & 0.326 & 0.551 & 0.525 & 0.467 \\
\cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(l){7-7}
\ours\,(Ours)  & 0.911 & \textbf{20.76} & \textbf{0.690} & \textbf{0.717} & \textbf{0.689} & \textbf{0.699}\\
\hline
\end{tabular}\\
\raggedright
\hspace{10px} \scriptsize{*: Scores from \href{https://huggingface.co/spaces/loveu-tgve/loveu-tgve-leaderboard}{TGVE leaderboard}.}
\label{tab:protocol1}
\end{table}

\begin{figure*}[!h]
    \centering
    % \begin{tikzpicture}
    %     \draw[black, fill=white] (0,0) rectangle (1.0\textwidth,0.3\textwidth);
    % \end{tikzpicture}
    \includegraphics[width=\linewidth]{figs/user_study_v2.pdf}
    \includegraphics[width=\linewidth]{figs/user_study_v2_multimetric.pdf}
    \caption{The abbreviations on x-axis indicate user preferences across four types of video editing within TGVE: Style, Background, Object, and Multiple. Each title specifies the evaluation metrics used for the corresponding figures. A "+" symbol signifies that the user vote meets multiple criteri. Additional qualitative results are presented in \Cref{sec.qualitative_comparison}
    }
    \label{fig:protocol2}
\end{figure*}

We conducted two separate user studies. The first followed the TGVE protocol, where we used Tune-A-Video as the baseline and compared the outcomes of our method with this well-known approach. However, we realized that this method of always asking users to choose a better option might not fully represent the reality, where both videos could either perform well or poorly. Thus, in the second user study, we compare our method with seven publicly available baselines. Instead of asking users to choose a better video, we asked them to vote for the quality of text alignment, structural preservation, and aesthetic quality for each transferred video. As evidenced by \Cref{tab:protocol1} and \Cref{fig:protocol2}, our approach excelled in all metrics except CLIP Frame similarity. We contend that CLIP Frame similarity is not an entirely apt metric because it only captures semantic similarity between individual frames, which may not be constant throughout a well-edited video due to changing scenes.

In our evaluation protocol, we additionally introduce a multi-metric assessment, capturing cases where videos satisfy multiple evaluation criteria concurrently. This composite measure addresses a key shortcoming of single metrics, which may inadequately reflect overall editing performance. For example, a high structure score might indicate that the edited video perfectly preserves the original, but such preservation could come at the expense of alignment with the editing instruction.

The results presented in \Cref{fig:protocol2} further corroborate the advantages of our approach. While baseline methods demonstrate respectable performance when text alignment is not a prioritized criterion, they fall short when this element is incorporated into the assessment. In contrast, our method not only excels in aligning the edited video with the textual instructions but also maintains the structural integrity of the video, resulting in a high-quality output. This dual success underlines the robustness of our method in meeting the nuanced requirements of video editing tasks.


% The following section provides detailed numbers and tables, encapsulating the results of our user studies. Through this analysis, we hope to demonstrate the efficacy of our method while identifying areas for future improvement.



% \subsection{Ablation Study}
% * batch size
% * sampling resolution
% * sampling image-cfg
% * show video samples

% \subsection{Quantitative Comparison}
