The signal model $y = Z\RxTx \cdot i + w$ from \Cref{eq:SignalModel} exhibits AWGN $w \iid \mathcal{CN}(0,\sigma^2)$ and non-random $Z\RxTx$. The achievable rate is given by the mutual information $\Rate$ between output $y$ and input $i$. For a complex-valued AWGN channel \cite[(B.47)]{Tse2005}
\begin{align}
& \Rate = h(y) - \log_2(\pi e \sigma^2)
\, , \label{eq:MutualInfo} \\
& h(y) = -\int_\bbC f_y(y) \, \log_2\big( f_y(y) \big) \,dy
\label{eq:DifferentialEntropy}
\end{align}
where $h(y)$ is the differential entropy of $y$.
% and $\log_2(\pi e \sigma^2)$ that of the AWGN $w$.
The PDF $f_y = f_v * f_w$ is the convolution of the PDF $f_v$ of the receive-side signal $v := Z\RxTx \cdot i$ and the PDF
$f_w(w) = \f{1}{\pi \sigma^2} \exp(-\f{|w|^2}{\sigma^2})$.
%of the complex-valued AWGN.

With the transmit statistics from \Cref{sec:CurrentStats}, $f_i$ and $f_v$ are supported on concentric circles. Then $f_y$ is characterized by noise-convoluted concentric circles. Conditioned on circle $k$, $|y|$ has Rice distribution and $\arg(y)$ uniform distribution. These facts can be used in \Cref{eq:DifferentialEntropy} to derive the result \Cref{eq:RateFromCircles} for $\Rate$ after lengthy calculation, as detailed in \cite{ShamaiTIT1995}.

When $i$ is instead chosen from a finite symbol alphabet $\{ s_1 , \ldots , s_{M} \} \subseteq \Disk$ with probabilities $\SymbProb_m$, then
%$f_{i}(i) = \sum_{m=1}^M \SymbProb_m \delta(i - i_m)$ and
$f_v(v) = \sum_{m=1}^M \SymbProb_m \delta(v - Z\RxTx s_m)$.
The convolution with $f_w$ yields
$f_y(y) = \sum_{m=1}^M \f{\SymbProb_m}{\pi \sigma^2} \exp(-\f{1}{\sigma^2} |y - Z\RxTx s_m|^2 )$.
This expression allows to calculate the differential entropy $h(y)$ with numerical integration to then evaluate the mutual information \Cref{eq:MutualInfo}.

%A technically attractive choice is modulation according to a finite symbol alphabet. An example is shown in \Cref{fig:Constellation_i}. Formally, this restricts $f_{i}$ to a finite number of mass points, i.e. $f_{i}(i) = \sum_{m=1}^M \SymbProb_m \delta(i - i_m)$. This is implemented by drawing samples $i = i_m$ with probabilities $\SymbProb_m$ from a finite RX-side symbol alphabet
%$\SymbolAlphabet = \{ i_1 , \ldots , i_{M} \} \subseteq \Disk$. The corresponding TX-side load alphabet is
%$\SymbolAlphabetZL = \{ z_1 , \ldots , z_{M} \}$
%with $z_m = \ChannelFunc^{-1}(i_m)$.
%A finite symbol alphabet imposes a limit $\Rate \leq \log_2(M)$ on the achievable rate.

%To calculate the achievable rate that results from a given symbol alphabet $\SymbolAlphabet$ and soft decoding, we first note that the PDF of the AWGN-corrupted observation $y$ is now the Gaussian mixture
%$f_y(y)
%= \sum_{m=1}^M \f{\SymbProb_m}{\pi \sigma^2}\, \exp( -\f{|y - Z\RxTx i_m|^2}{\sigma^2} )$
%with
%$y_m = |Z\RxTx| i_m$.
%This expression allows to calculate $h(y)$ in \Cref{eq:DiffEntropy} with numerical integration to then evaluate the mutual information \Cref{eq:MutualInfo}.

%When the channel is furthermore discretized by quantization of $y$ at the RX (cf. symbol decision thresholds, hard decoding), the achievable rate can only decrease. For brevity, these aspects and the resulting rates are not discussed in detail.

%The PDF $f_{i}$ of the TX current $i$ must fulfill $\supp(f_{i}) \subseteq \Disk$.
%The noise PDF is given by
%$f_w(w) = \f{1}{\pi \sigma^2} \exp(-\f{|w|^2}{\sigma^2})$.
%With $s = |Z\RxTx| i\Tx$, the PDF of $y = s + w$ is given by the convolution
%$f_y(y) = (f_s * f_w)(y)$.

%In general, the achievable rate $\Rate$ of a channel from a TX-side random variable $i$ and RX-side observed random variable $y$ is given by their mutual information in bit,
%\begin{align}
%\Rate = I(y;s) &= h(y) - h(y | i) 
%\label{eq:MutualInfoDef} \\
%&= h(y) - h(y | s) \\
%&= h(y) - h(w) \\
%&= h(y) - \log_2(\pi e \sigma^2)
%\label{eq:MutualInfo} \, .
%\end{align}
%In \Cref{eq:MutualInfo} we used that the noise differential entropy is explicitly given by $h(w) = \log_2(\pi e \sigma^2)$ in the complex AWGN case \cite[(B.47)]{Tse2005}. 
%The differential entropy $h(y)$ is defined as
%\begin{align}
%h(y) = -\int_\bbC f_y(y) \, \log_2\big( f_y(y) \big) \,dy \, .
%\label{eq:DiffEntropy}
%\end{align}
%Now $\Rate$ can be calculated via \Cref{eq:MutualInfo} by first evaluating $h(y)$, e.g. via numerical integration over $\bbC$.


