\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{xcolor}         % colors
\usepackage{tablefootnote}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks, allcolors=blue]{hyperref}
% \makeatletter
% \g@addto@macro{\UrlBreaks}{\UrlOrds}
% \makeatother

% \urldef{\footurl}\url{https://anonymous.4open.science/r/Maximal-Domain-Independent-Representations-Improve-Transfer-Learning-A422/README.md}


% customized format:
%\usepackage{subfig}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage[table,xcdraw]{xcolor}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\usepackage{amssymb}
%\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\comment}[1]{}
\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand{\tochange}[1]{{\textcolor{blue}{#1}}}
\newcommand{\change}[1]{{\textcolor{cyan}{#1}}}
\newcommand{\remove}[1]{}
\newcommand{\elisa}[1]{\textbf{\color{red}Elisa: #1}}
\newcommand{\markw}[1]{\textbf{\color{green}Mark: #1}}
\newcommand{\adrian}[1]{\textbf{\color{blue}Adrian: #1}}
\title{Maximal Domain Independent Representations Improve Transfer Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Adrian Shuai Li \\
  Purdue University\\
  West Lafayette, IN \\
  \texttt{li3944@purdue.edu} \\
  % examples of more authors
  \And
  Elisa Bertino \\
Purdue University\\
  West Lafayette, IN \\
  \texttt{bertino@purdue.edu} \\
  \AND
  Xuan-Hong Dang \\
  IBM T. J. Watson Research Center \\
  Yorktown Heights, NY \\
  \texttt{Xuan-Hong.Dang@ibm.com} \\
  \And
  Ankush Singla \\
  Purdue University \\
  West Lafayette, IN \\
  \texttt{asingla@purdue.edu} \\
  \And
  Yuhai Tu \\
  IBM T. J. Watson Research Center \\
  Yorktown Heights, NY \\
  \texttt{yuhai@us.ibm.com} \\
  \And
  Mark N Wegman \\
 IBM T. J. Watson Research Center \\
  Yorktown Heights, NY \\
  \texttt{wegman@us.ibm.com} \\
}


\begin{document}


\maketitle


\begin{abstract}
  %Domain adaptation (DA) is a challenging problem in transfer learning.
  
  Domain adaptation (DA) adapts a training dataset from a source domain for use in a learning task in a target domain in combination with data available at the target. One popular approach for DA is to create a domain-independent representation (DIRep) learned by a generator from all input samples and then train a classifier on top of it using all labeled samples. A domain discriminator is added to train the generator adversarially to exclude domain specific features from the DIRep. However, this approach tends to generate insufficient information for accurate classification learning. In this paper, we present a novel approach that integrates the adversarial model with a variational autoencoder. In addition to the DIRep, we introduce a domain-dependent representation (DDRep) such that information from both DIRep and DDRep is sufficient to reconstruct samples from both domains. We further penalize the size of the DDRep to drive as much information as possible to the DIRep, which maximizes the accuracy of the classifier in labeling samples in both domains. We empirically evaluate our model using synthetic datasets and demonstrate that spurious class-related features introduced in the source domain are successfully absorbed by the DDRep. This leaves a rich and clean DIRep for accurate transfer learning in the target domain. We further demonstrate its superior performance against other algorithms for a number of common image datasets. We also show we can take advantage of pretrained models.
\end{abstract}

\section{Introduction}
\label{Introduction}
%\input{sections/introduction}


Labeling data for machine learning (ML) is a difficult and time-consuming process. In many instances, however, we may have labeled data similar to the data we are trying to label. %\elisa{I would remove this early day example since it is outdated. We can just leave the image data.}
%For example, in the early days of speech recognition we did not have a lot of transcripts for women's voices as most of the available transcripts consisted of predominantly male voices. 
For example, it is easier to label images presented in a familiar manner than labeling pictures taken with a color filter or an infrared camera. Therefore a question is whether it is possible to leverage existing labeled data to perform ML tasks like classification for similar data that is not labeled. 
%What do we do if we want to transcribe women's voices or label the results from an infrared camera? 
%It is not uncommon to have a ready source of labels for data from one domain but not for the domain you want to classify.

%A possible approach to address such question is to use transfer learning, which 
%Transfer learning 
%\elisa{do we need this first sentence saying that does transfer learning mean?. I would directly say "In this paper, we propose an approach to address such question based on transferring knowledge between related domains, referred to as domain adaptation.}
%generally means solving one problem and applying the knowledge acquired to a different but related problem. In this paper, we focus on transferring knowledge between related domains, which is referred to as domain adaptation (DA)~\cite{zhang2022transfer,zhang2021survey}. 


In this paper, we propose an approach to address such question based on transferring knowledge between related domains, referred to as domain adaptation (DA)~\cite{zhang2022transfer,zhang2021survey}. 
In our setting, we have sufficient labeled data from a \textit{source domain.} We are interested in assigning labels (from the set of labels in the source domain dataset) to data from a similar \textit{target domain} with few or no labels. % By a domain we mean a source of examples with common characteristics.
Examples of related domains are indoor vs outdoor pictures, summer vs winter pictures, or pictures with different resolutions. %For example, data capturing could lead to different resolutions from different domains. 
The source domain will generally have some information that is useful for labeling which is not available in the other domain. For instance, it is easier to identify ripe fruit in a low resolution color picture by using the color information. However, using the color information to classify grayscale pictures can lead to poor accuracy. %But it might be easier to identify counterfeit bills using high resolution. 
%\elisa{I'm not sure about the text below. It seems that you want to say that the DA can also be used for the same domain when the domain change characteristics. So perhaps the text needs to be slightly changed.}
Another general problem is that the characteristics of a domain may change over time. With older training samples, the test accuracy may deteriorate as the content of the domain evolves. DA uses information common to the different domains, hence it can be used for domains whose distribution changes over time.%It is therefore crucial to only use information common to the different domains for DA in transfer learning. %It may well be that there is useful information for the classification task that exists in each domain and yet is absent in the others.

 %With even a small number of labels
%we would expect to be able to do better. 
%In experimental settings
%we have labels for the target domain but do not use them in training, only for testing. We can upper-bound how well we might be able
%to do when using \textit{x} number of labels from the source domain
%in transfer learning by replacing those with the same \textit{x} number of
%labels in the target domain and training a neural network using those
%labels instead of the source labels. In the experiments section we'll
%show that we can approach this upper bound in some of our experiments.



%Our experiments will demonstrate
%that we can do a very good job, even without any additional labels, in transferring knowledge from one domain to another.

%One way to achieve 
%\elisa{I would change 'faithful' with 'effective'. Also I would not mention the use of the neural network as creating a domain-independent representation is a general task and could be achieved in other ways. The use of a neural network is part of the solution we used. Could we just say "One approach to achieve an effective DA is to generate a a \textit{domain-independent representation} (DIRep)
%of the data from different domains."}
%faithful DA is to have a
%neural network create a \textit{domain-independent representation} (DIRep) of the data from different domains. 
One approach to achieve an effective DA is to generate a a \textit{domain-independent representation} (DIRep) of the data from different domains.
%\elisa{I would not say "It is assumed" here as what you are saying is an informal definition of domain-independent representation. So I would just remove "It is assumed that" and just start the sentence with "A representation is domain independent..." (it would be good to add references to this notion if any.}
A representation is domain independent if one cannot  determine from the representation which domain the information came from. If based on the DIRep,
%domain-independent representation, 
we can classify the objects from the source domain, then there is a chance that we can classify the ones in the target domain as well. To generate a DIRep of the source and target data, one method is to adversarially train two neural networks, serving as a discriminator and generator, respectively.
%\remove{To achieve  domain independence, one can train a discriminator to determine the domain from the DIRep and then set up an adversarial approach so that the neural network that reads the input and \textit{generates} the
%DIRep representation from which the discriminator cannot determine the original domain. The neural network that reads the input and generates the DIRep is called a \textit{generator}.} 
%\elisa{I would replace the text below, with the following one: "Our approach to generate a DIRep of the source and target data is to adversarially train two neural networks, serving as a discriminator and generatior, respectively."}
%To achieve domain independence, we adversarially train a discriminator and a generator. The generator is trained to create a DIRep from which the discriminator cannot determine the original domain of the data. The discriminator is trained to determine the original domain of the data. 
%\elisa{the sentence below does not seem well placed here; later on we can see where to move it.}
%Adversarial DA has been explored in the literature~\cite{akkaya2021self,singla2020preparing,tzeng2017adversarial,ganin2016domain}. 


\remove{We observed a theoretical problem with the discriminator/generator
approach. In the \textit{hidden data effect} there can be information in the DIRep that is useful in classification when data comes from the source domain but is of little value when the data is from the target domain. For example, the generator could in the source domain put into the DIRep the classification and from the target domain put in random values. If the only data in the DIRep is the classification, then there's no way to tell whether the data came from source or target and the source classification can be perfect and the target awful.} 

%\elisa{I did not enter specific changes/comments on the text below. I'll do once it is clear that the hidden data effect is the same as the shortcut problem mentioned in https://news.mit.edu/2021/shortcut-artificial-intelligence-1102.}
However, the adversarial DA approach can suffer from what we call the {\it hidden data effect}. The hidden data effect can occur when the classifier can depend on shortcut information (spurious features)~\cite{mitAvoidingShortcut} in the source domain that is not available in the target domain, and the shortcut information never the less persists into the DIRep. %{\color{red} In the presence of these shortcut information, the classifier puts pressure on the generator to preserve the shortcut information in the DIRep that help classification in the source domain. In principle, the discriminator could prevent the inclusion of these spurious features in the DIRep by removing any difference between the two domains.  %{\color{red} The rest of the paragraph before "In general, ...." is confusing, e.g., how does a generator "do" classification? do you mean generate the correct label? how does the generator "remove" information from DIRep? and so on .... Perhaps the essential point to make here is that the random data/labels generated by the generator in the target domain has the same distribution as those in the source domain so they evade the detection by the discriminator?} 
%However, within the adversarial DA approach, the generator can evade the discriminator by also generating the ``false" shortcut information for target samples. Even though these false shortcut information have no correlation with the true labels of target samples, the discriminator will not be able to detect them directly as long as they have the same distribution as the (correct) shortcut information for the source domain. \adrian{Do we need the sentences "Besides the ... in the DIRep".} Besides the shortcut information, the generator can also put other features in the DIRep. If it keeps only those features that do not contradict the shortcut information in both domains, the discriminator can not detect the difference between the DIRep's from the two domains by measuring the consistency between the shortcut information and other features in the DIRep. As a result of the generator's evading strategy, the shortcut information could persist in the DIRep and be used by the classifier, which would lead to a poor classification performance in the target domain. In general, the hidden data effect occurs when there is shortcut information in the DIRep that is useful in classification if the data comes from the source domain, but is of little value when the data is from the target domain.} %To demonstrate the hidden data effect clearly, we create synthetic benchmarks where the source classifier can ``cheat'' by taking advantage of certain source-only (cheating) information besides using standard datasets where the hidden data effect may be subtle, . 
%\markw{Is this any better?  
In the adversarial DA models, the generator’s loss function trains the generator to maximize the accuracy of the classification on the source, which implies using the shortcut information, possibly in conjunction with other information in the source data.  It also trains the generator so that its output is indistinguishable in both the source and target domains even though the input to the generator in the target does not have the shortcut information.  The generator can resolve these conflicting goals, if with target data it creates pseudo-random shortcut information, with the same distribution as they occur in the source.  This resolution requires that all DIRep information in the target that might not be consistent with the pseudo-random shortcut information be deleted from the output of the generator.  This results in poorer performance of the classifier in the target domain because some of the information it needs would have been deleted.

%We expect the hidden data effect to be common in various domains, particularly those related to data drift. \elisa{It would good to say why the hidden data effect is not an issue with standard deep learning benchmarks.} 

%\adrian{Do we actually need this paragraph? Could we remove it?}\markw{good question.  Yuhai??}
%We expect the hidden data effect to be common in various domains, particularly those related to data drift. {\color{red} what is "data drift"? why is it particularly bad for the hidden data effect? Do we really need this sentence here?} However, the hidden data effect tends to not be an issue with the standard domain adaptation benchmarks  since there are no apparent instances of specific information shortcuts present in the source data but absent in the target data.
%We thus create synthetic benchmarks where the source classifier can ``cheat'' by taking advantage of certain source-only (cheating) information to demonstrate the hidden data effect. 

To address the hidden data effect, we propose \verb"VAEGAN" a new model that combines a variational autoencoder (VAE) with the GAN-based adversarial learning approach. In \verb"VAEGAN", we do classification based on the DIRep of the data. %We make sure that is possible by using loss functions to ensure the DIRep contains as much information as possible, e.g., the DIRep is more than just a classification. 
In order to achieve more accurate DA, the DIRep should contain as much information as possible.
Reconstructing the input samples is a good measure to examine if \verb"VAEGAN" can preserve all the information. However, without domain dependent information, reconstructing input samples solely from the DIRep is not possible. \verb"VAEGAN"'s solution to this problem is to encode all domain dependent information in a \textit{domain-dependent representation (DDRep)}. We thus add an autoencoder to our approach to ensure that the DIRep and the DDRep together contain enough information to reconstruct the input samples. To ensure that the DDRep only includes domain dependent information, we make it as small as possible (by using a VAE loss) without preventing reconstruction. The idea is that a maximal DIRep can be built by minimizing the DDRep. 

The autoencoder forces most of the information needed for reconstruction into the DIRep, so the generator can no longer put shortcut based or, in the target case, pseudo random based results relevant to classification into the DIRep. They would now contradict the information needed to reconstruct the image.  The information needed to reconstruct the image would include information that could be used to accurately classify and hence would contradict any pseudo random based results.  That contradiction would enable the discriminator to recognize that the data came from the target.  In this way we can at least partially defeat the hidden data effect.

We empirically test \verb"VAEGAN" thoroughly and show that our model is both less vulnerable to the hidden data effect and outperforms other algorithms in unsupervised domain adaptation. VAEGAN works well when the weights of the generator are initialized to the value of some pre-trained model.  In the experiments section we show that we can learn to classify even when the source domain has very little data, by initializing the generator with values that classify to categories that are not in the source or target domains.
 

\remove{   But in a NN you cannot measure directly how rich a representation is and reward a richer one. Algorithmic information theory might suggest that in some sense a representation that contains random values is richer, but adding random numbers to the DIRep will not get us anywhere.
So, instead our goal became to make sure that nothing was lost
that could be used to reconstruct the original. That's obviously not
achievable when part of the original data contains information that
can be used to identify the domain. To solve this conundrum we introduce
a \textit{domain dependent representation(DDRep).} The combination of the DIRep and the DDRep can be used to reconstruct the original, and we use one of two mechanisms to ensure that the DDRep contains the minimum needed to allow that reconstruction. Classification is only from the DIRep, so after training the DDRep can be discarded. The distinction between the DIRep and the DDRep might enable some types of disentanglement of the information in the original.} 

\remove{Add the scientific contributions of the paper, results and brief overview of how this paper is organized.}



\remove{ \todo{Suppose in the source domain there is some telltale indicators of the classification that is absent in the target. Perhaps in a security context the source is of older attacks, where the defenders found some indication of an attack and defended against it. But later more sophisticated attackers avoided including that indication in their attacks. The classifier is trained on the earlier source and to get maximal accuracy "wants" the generator to preserve the information. If the generator recognizes whether the data comes from the source or target and creates hallucinates indications of classification in the target, it can then fool the generator, improve source classification and lower it's loss function. This is an example of the  hidden data effect where there can be information in the DIRep that is useful in classification when data comes from the source domain but is of little value when the data is from the target domain. We expect the hidden data effect to be common in some pairs of domains, particularly those related to data drift but tends not to be an issue with standard deep learning benchmarks. In the experiments section create such benchmarks where the source classifier can "cheat" by taking advantage of source only information. }}  


%In our experiments we will demonstrate artificial benchmarks where the hidden data is particularly strong. We will also demonstrate that a topology less vulnerable to the hidden data effect can do a better job of domain adaptation.

\remove{Need a background section describing terminology and concepts like domain adaptation, GANs etc.}

%\vspace*{-0.2cm}  
\section{Related Work} \label{Related Work}
%\input{sections/related_work}
Transfer learning 
%across domains 
is an active research area that has been covered by several survey papers ~\cite{liu2022deep,zhang2022transfer,zhang2021survey,zhuang2020,liu2019transferable,wang2018deep}. These surveys categorize solutions into those relying on: data distribution discrepancy, pseudo-labeling, adversarial learning, reconstruction loss, and representation learning. Our solution falls in the intersection of adversarial learning, reconstruction loss and representation learning. We discuss the most relevant approaches in Sections \ref{adv} and  \ref{recon}. % \ref{pseudo}.
%, then describe our approach in section \ref{alg}. 

\subsection{Adversarial Learning based Approaches} \label{adv}%Methods taking this approach 
These approaches typically aim at learning what we described  as a DIRep by employing two competing networks of feature extractor/generator and domain discriminator. The domain adversarial neural network (\verb"DANN")~\cite{ganin2016domain},  one of the first such approach, uses three network components, namely a feature extractor, a label predictor and a domain classifier. %The domain classifier works in an adversarial manner to ensure that none of the features extracted by the feature generator can determine the domain identity. \todo{Adrian: 
The generator is trained in an adversarial manner to maximize the loss of the domain classifier by reversing its gradients. The generator is trained at the same time as the label predictor to create a DIRep that contains domain-invariant features for classification. 
%\todo{alternate rest of paragraph: The domain classifier works in an adversarial manner to ensure that none of the features extracted by the feature generator can determine the domain identity. The generator for the DIRep is trained at the same time as the classifier from the DIRep} The domain classifier is connected to the feature extractor through a gradient reversal layer which flips the gradient by a negative constant during the backpropagation process. By doing so, this layer ensures that the features learnt by the extractor from both source and target samples are made similar, leading to the domain-invariant features. 
%\todo{Mention that these techniques will only work if Hypothesis 1 is quite false?}
The adversarial discriminative domain adaptation (\verb"ADDA")~\cite{tzeng2017adversarial} approach adopts similar network components, yet its learning process involves multiple stages in training the three components of the model.
%The adversarial discriminative domain adaptation (\verb"ADDA")~\cite{tzeng2017adversarial} adopts similar network components yet its model training involves two stages. It first pre-trains an extractor and a classifier using labeled samples from the source domain. It then trains another extractor on the target unlabeled samples that maps these samples into the same feature space (learnt by the first extractor) so that the discriminator cannot distinguish the mapped samples from both source and target domains. In the testing phase, the target extractor and the source classifier are used together to label samples from the target domain. 
Recently, the self-training guided adversarial domain adaptation (\verb"SGADA")~\cite{akkaya2021self} method further extends \verb"ADDA" by exploiting the performance of the discriminator in choosing relevant target samples to train the classifier. %In particular, \verb"SGADA" assumes that those target samples that are incorrectly classified by the discriminator, then their representations (learnt by the extractor) are close to the source samples with known labels. Hence, such labels can assign to these target samples in further training the classifier. 
Singla et al. has proposed a hybrid version of the \verb"DANN" and \verb"ADDA" where the generator is trained with the standard GAN loss function~\cite{goodfellow2020generative}. We refer to this  as the \verb"GAN-based" method~\cite{singla2020preparing}.

All adversarial learning based methods (\verb"DANN", \verb"ADDA", \verb"SGADA" and \verb"GAN-based") aim at learning a common representation feature space between the source and target domains. They have all achieved similar results in classification in the target domain. 
However, depending on the details of the data and the networks, the DIRep may suffer from the aforementioned  hidden data effect. Specifically, all these methods attempt to learn features that both fool the discriminator and lead an accurate classification for the source domain. Learning such features based purely on adversarial and supervised learning is challenging, since the features that can fool the discriminator might not lead to good classification results, and vice versa. 
%\elisa{the text below needs to be changed based on whether we do not claim that the hidden data effect is a novel contribution of the work in this paper.}
As we show in our experiments, such hidden data effect can significantly degrade the classification performance in the target domain. 

\verb"DANN" and \verb"ADDA" have recently been used based on pre-trained ResNet-50 models \cite{chen2020adversarial}.
%   competitive results to the \verb"DANN" model. 
%Both \verb"ADDA" and \verb"SGADA" aim at learning a common representation feature space between the source and target domains and also achieve competitive results to the \verb"DANN" model. Yet, they lack the capability of learning domain dependent representations as done in our model. Hence, the quality of learning the common sharing representation features appears to be limited and suboptimal.
%\todo{is this technique interesting?  do they get better results than the DANN methods?  it's all about training in a particular order. if it's better we could use it too?  not sure how that's relevant to our work?}

\remove{Coupled Generative Adversarial Network (\verb"CoGAN")~\cite{liu2016coupled} is another model that trains two \verb"GANs" at the same time, each accounting for one image domain, toward learning a joint image data distribution between the two data domains. Yet, it further imposes weight tying for first few layers in the generators and last few layers of the discriminators to encourage the model to learn a joint distribution of samples from both domains rather than learning a product of their marginal distributions. \verb"CoGAN" is a general method that can be applied to unsupervised domain adaptation (as done in our model) and image transformation.\todo{I'm not sure I understand} However, one limitation of it is that it requires the training data to be given as tuples of \textit{corresponding} images in both data domains, which might not always be the case in many practical applications. }

%As seen from all these GAN-based models, most of them attempts to learn common features sharing between the two data domains that can fool the discriminator in distinguishing source and target samples while support categorizing class-labels. In reality, learning such features purely based on adversarial and supervised learning is challenging since the features that can fool the discriminator might not lead to good classification results and vice versa. Our \verb"VAEGAN", on the other hand, further requires the well-reconstructed samples from both data domains at its output. By minimizing the reconstruction loss, \verb"VAEGAN" assures that it can maximize the information encoded in both embedding spaces DIRep and DDRep. Moreover, through penalizing the large size of DDRep, our model forces as much of information into the DIRep which is important to the success of the classifier in classifying samples from both data domains

\remove{Cycle-consistent adversarial domain adaptation \verb"CyCADA"~\cite{hoffman2018cycada}\footnote{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix} alleviates this issue by adapting representations at both pixel-level and feature-level while enforcing local and global structural consistency
through pixel cycle-consistency loss to encourage cross-domain transformations.}


\subsection{Representation and Reconstruction based Approaches} \label{recon} A leading approach in this category is the Domain Separation Networks (\verb"DSN")~\cite{bousmalis2016domain}. There are two extractors (generators) for each domain in \verb"DSN", one private and one shared, which create a private and a shared representation of the data corresponding to the DDRep and DIRep, respectively. The shared generator from the two domains is trained to produce a DIRep whose origins (source or target) cannot be determined. This shared generator hopefully captures the common features shared by the two domains. The shared and the private generators from each domain are trained to produce different results. Their DDRep and DIRep have the same shape, and they define a linear ``soft subspace orthogonality constraint between the private and shared representation of each domain'' to ensure that the DIRep and DDRep are as different as possible. The training of the shared generator in \verb"DSN" is similar to the adversarial learning based approach. The new feature of \verb"DSN" is the addition of a shared autoencoder that enforces the reconstruction of the data from the combined private and shared representations in each domain. At the same time, a classifier is trained from the shared representation (DIRep) of the source domain and the labels. Deep Reconstruction Classification Networks (\verb"DRCN")~\cite{ghifary2016deep} is another approach
%deep learning model 
that relies on reconstruction to learn a common encoding representation shared between the two data domains.

By including an autoencoder, 
%algorithms 
approaches like \verb"DSN" require that all information needed for reconstruction is in either the DDRep or DIRep. We do so as well. This aids classification. We will show that \verb"DSN" outperforms adversarial learning based methods such as \verb"DANN" and \verb"GAN-based" algorithms~\cite{singla2020preparing}. 

While \verb"DSN" uses loss functions to ensure the DIRep and DDRep are linearly different, we use either loss functions or explicit construction to ensure that the DDRep is small, and thus that the DIRep contains as much information as possible. If the \verb"DSN" does source classification when creating the DIRep, information needed for target classification may be found in the DDRep only via the hidden data effect. Moreover, the linear orthogonality may not guarantee a strict distinction between DIRep and DDRep, as the same (original) information can still be encoded into both of them in a non-linear manner. Furthermore, if a feature is encoded in the DDRep early in training, this may prevent it from being used for classification later, as the DIRep is required to be orthogonal to the DDRep. All these shortcomings of \verb"DSN" can lead to some degree of hidden data effect, which can potentially lower the classification accuracy in the target domain. Our approach aims to improve domain adaptation performance by reducing/eliminating some of these shortcomings. 

%utilizes a common feature extractor to learn shared representations between two domains which we can call the DIRep, and two extractors per domain to produce
%domain-specific representations called a DDRep each. Similar to ours, a decoder can utilize DIRep and DDRep for samples replication. As both DIRep and DDRep have the same shape, \verb"DSN" further adds a loss function that encourages (linear) orthogonality between them. Though successful in specific cases, \verb"DSN" tends to suffer from the hidden data effects (\todo{Mark: we may need to be more specific on what `hidden data effects problem`?!}) as shown in our experiments. This might happen as \verb"DSN" does not take clear distinction on what information is to be encoded in DIRep and DDRep, which is done in our model through its adversarial learning. Moreover, the linear orthogonality may not guarantee the strict distinction between DIRep and DDRep as same (original) information still can be encoded into both of them in a non-linear manner. In addition, if some feature is encoded into the DDRep early in training, that may prevent it from being used for classification later, because the DIRep is required to be orthogonal to what has been in the DDRep.

\remove{Mark: Alternate write up for \verb"DSN": is the closest related paper to ours. The authors of the \verb"DSN" paper use a common feature extractor to produce what we'd call a DIRep and per domain extractors to produce what we'd call a DDRep for each. Like us they also have an autoencoder from their DIRep plus DDReps and train their feature extractors to ensure that the DI+DD Reps can be used for replication. Their DIRep and DDRep have the same shape and they "add a loss function that encourages independence of these two representations."  Independence is defined as orthogonal in a linear fashion. 
We will show in the experiments section that they can suffer from the hidden data effects problem and that we can often do better than they can. Perhaps this is because they don't make the distinction between what goes in the DIRep and DDRep in the same way. Their orthogonality is measured using the techniques of linear algebra, but the same information can be encoded into the representations in a non-linear manner. In addition if some feature gets into the DDRep early in training that may prevent it from being used for classification later, because the DIRep is required to be orthogonal to what's in the DDRep.}


  \remove{XH: is one of the studies in this approach that is closest to ours. It relies on the encoder-decoder framework along with a classifier trained on the labeled source samples. \verb"DSN" contains a shared encoder that learns common (or domain-independent) representations (\verb"DIRep") between two domains, and two private encoders to learn domain-specific (or domain-dependent) representations (\verb"DDRep") from each source and target domain. The shared decoder attempts to reconstruct samples based on both \verb"DIRep" and \verb"DDRep". To ensure the difference between these two representations (represented in the matrix forms), \verb"DSN" exploits the orthogonal condition between them, i.e., minimizing the Frobenious norm of their matrix multiplication. \remove{\todo{Why?}} This implies that embedding features semantically represented for both \verb"DDRep" and \verb"DIRep" must be the same, which is less practical since their original features that they attempt to encode can be different (e.g., objects vs. background features in many images datasets).}

  
  
  
  %Though our model \verb"VAEGAN" shares similar view with \verb"DSN", it does not limit \verb"DDRep" and \verb"DIRep" to be represented in the same embedding space, giving them freedom to capture different features from source and target samples. Moreover, \verb"VAEGAN" goes beyond the \textit{linear} orthogonality by exploiting the \textit{non-linear} KL-divergence in much the same way the conventional \verb"VAEs" work. Specifically, our \verb"VAEGAN" attempts to keep \verb"DDRep" as small as possible. As demonstrated in one set of our experiments on the Fashion-MNIST, the \verb"DDRep" can be as small as a single bit yet it still gets the task done properly. The advantage of keeping \verb"DDRep" small is that, \verb"VAEGAN" can force as much of common sharing information between the two domains as possible into the \verb"DIRep", which maximizes the chance of success for the class-label classifier. 

%\markw{Next section is new and is really only a draft.  please look and comment}
%\elisa{This section looks interesting. The text seems a bit different in style compared with the previous subsections. For example if would be good to introduce what pseudo-label based methods are and include some references. Also some parts of the text would look like more adequate for a discussion of future work, in which we could do a detailed comparison and then investigate the design of enhanced pseudo-labels based on our foundation.}
%\subsection{Pseudo-label based Methods}
%\label{pseudo}
%If you have a small number of labels for a domain, you can look over elements in that domain and find elements that are almost certain to have a label.  But they may have some characteristics that are a bit different than labelled examples.  You can use those characteristics to help you label more and more examples.  This can work even if you merely have a likelihood that some examples will have certain labels.  A number of papers have used \verb"DANN" like techniques to give an initial likelihood that certain examples will have certain labels and then they iterate learning common characteristics of elements likely to have those labels~\cite{chen2020adversarial}~\cite{zou2018unsupervised}.  

%Pseudo labelling is a very powerful technique in this context but it depends on an initial estimate.  If our initial estimate is better than other algorithms then the pseudo-labelling techniques should be able to do even better.  We have not carefully compared our work to pseudo-labels believing that the pseudo-labelling based techniques are improving and we hope they will be able to be built on our foundation.

\remove{Deep Reconstruction Classification Network (\verb"DRCN")~\cite{ghifary2016deep} is another deep learning model that relies on reconstruction losses to learn a common encoding representation shared between the two data domain. Specifically, \verb"DRCN" learns this embedding space by (i) maximizing the classification accuracy on the labeled source samples using a classifier; (ii) minimizing the reconstruction loss on the unlabeled target samples using a decoder. Hence, the first few layers of \verb"DRCN" (whose output is the shared encoding space) can be considered as the common encoder for both domains. Training optimal weights for this encoder tends to be challenging since they are updated by the loss on both the classifier and the decoder whose losses are computed by two different sets of data samples. Our model shares the same motivation with both \verb"DSN" and \verb"DRCN" that successfully reconstructing input samples at output implies that sufficient information has been encoded into out latent spaces. However, our model performs this task through the combination of adversarial learning with variational auto-encoder. More importantly, \verb"VAEGAN" attempts to maximize the common information between the source and target samples in the domain independent space through keeping its domain dependent space small, while ensures that their combination remains sufficient to reconstruction the data samples from both data domains.}


%A combination of adversarial learning and disentanglement representation learning is presented in 

\remove{
Deep neural network based domain adaptation methods: 
\begin{itemize}
    \item Discrepancy-based: Deep Adaptation Network~\cite{long2015learning}~\cite{saito2018maximum}

    \item Reconstruction-based: 
    \begin{itemize}
        \item Stacked denoising autoencoder~\cite{glorot2011domain}
        \item Marginalized stacked denoising autoencoder~\cite{chen2012marginalized}
        \item Similar idea but does not have adversarial training. Deep reconstruction-classification network (DRCN)~\cite{ghifary2016deep} 
    \end{itemize}

    \item Adversarial-based:
    \begin{itemize}
        \item GAN, 
        \item DANN~\cite{ganin2016domain}
        \item ADDA ~\cite{tzeng2015simultaneous}~\cite{tzeng2017adversarial}
        \item CyCADA ~\cite{hoffman2018cycada}
        \item CoGAN ~\cite{liu2016coupled}
        \item Domain Separation Networks ~\cite{bousmalis2016domain}
        
    \end{itemize}
    
\end{itemize}
}






 \section{The VAEGAN for Domain Adaptation}
\label{Detailed Design}
 %\input{sections/DA}

 %\documentstyle[aps,preprint]{revtex4}
%\documentclass[preprint,showkeys,rmp,superscriptaddress]{revtex4}
%\documentclass[preprint,superscriptaddress]{revtex4}
% \documentclass[11pt,letterpaper]{article}
% \usepackage{float}
%\documentstyle[preprint]{revtex4}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{graphicx}
%\usepackage{graphics}
\newcommand {\be}{\begin{equation}}
\newcommand {\ee}{\end{equation}}
\newcommand {\bea}{\begin{eqnarray}}
\newcommand {\eea}{\end{eqnarray}}



%\bibliographystyle{naturemag}
%\begin{document}



%\draft
%\begin{centering}
%{\large \bf
%Notes on "Learning Dynamics in %GAN-based domain adaptation for %transfer learning"}\\
%\vspace{0.5cm}
%Yuhai Tu\\
%IBM T. J. Watson Research Center, NY\\
%(\today{})

%\end{centering}
%\vspace{1cm}

%\begin{description}
%\item {\bf Running Title:} Adaptation in bacterial chemotaxis
%\item {\bf Classification:} Biophysics
%\item {\bf Corresponding author:} \\
%Yuhai Tu\\
%IBM T. J. Watson Research Center\\P. O. Box 218\\Yorktown Heights,
%NY 10598\\
%(914)-945-2762 (Phone)\\(914)-945-4506 (Fax)\\yuhai@us.ibm.com
%\item {\bf Manuscript information:} 18 text pages, 5 figures
%\item {\bf Word and character counts:} 249 words in abstract,
%46,500 total characters
%\end{description}

%\newpage
%\begin{center}
%{\bf Abstract/Outline}
%\end{center}
\label{alg}
In this section we formulate 
%\elisa{I'm not sure about using the term algorithm here. In the DSN paper, they use sometimes the term 'model'. Perhaps we can also use the term 'model'. Then in the caption of Figure 1(a) we can use the caption "Architecture of VAEGAN".}
our model that combines GAN-based DA with a variational autoencoder (VAE) to create a maximal DIRep for transfer learning. Fig.\ref{fig:VAEGAN}(a) shows the architecture of our model, referred to as \verb"VAEGAN".
%This model, acronym \verb"VAEGAN", is illustrated in Fig.\ref{fig:VAEGAN}(a). %\todo{when printed fig 1 is hard to read, though it is fine when reading on a screen.}\markw{I think it's fair to call it either an algorithm or a model.  In my mind the model is the topology alone, but the algorithm includes how you train that topology and the loss functions.  I could be persuaded differently though.}

%\newpage

%\subsection{The GAN-based domain adaptation (DA) with variational auto encoder (VAE)}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.55\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/VAEGAN.pdf}
         \caption{}
         \label{illu}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.38\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/GANDA-VAE.pdf}
         \caption{}
         \label{GANDA-VAE}
     \end{subfigure}

        \caption{ (a) Architecture of VAEGAN. (b) Illustration of the dependence between the loss functions and the neural networks (NN's) in the VAEGAN model. The only link between the GAN part of the model and the VAE part of the model is the dependence of $\mathcal{L}_r$ on $G$. The dotted arrows %arrowed links 
        represent one-sided dependence. For example, the dotted arrow %link 
        from $D$ to $\mathcal{L}_g$ means that even though $\mathcal{L}_g$ depends on $D$, the update of $D$ only depends on the $\mathcal{L}_d$ discriminator loss. All solid lines are bi-directions, i.e., the loss function depends on the NN and the updates of the NN depend on the loss function.}
        \label{fig:VAEGAN}
        \vspace{-1ex}
\end{figure}

% \begin{figure}[t]
% \centering\hspace{-1.5cm}
%  \subfloat(a){
%     \includegraphics[width=0.55\linewidth]{images/VAEGAN.pdf}
%     \label{{illu}}
%     }
%  \subfloat(b){
%     \includegraphics[width=0.38\linewidth]{images/GANDA-VAE.pdf}
%     \label{GANDA-VAE}
% }\hspace{-2.0cm} 
% \vspace{-1ex} \caption{(a) Architecture of VAEGAN. 
% %algorithm. 
% (b) Illustration of the dependence between the loss functions and the neural networks (NN's) in the VAEGAN model. The only link between the GAN part of the model and the VAE part of the model is the dependence of $\mathcal{L}_r$ on $G$. The dotted arrows 
% %arrowed links 
% represent one-sided dependence. For example, the dotted arrow %link 
% from $D$ to $\mathcal{L}_g$ means that even though $\mathcal{L}_g$ depends on $D$, the update of $D$ only depends on the $\mathcal{L}_d$ discriminator loss. All solid lines are bi-directions, i.e., the loss function depends on the NN and the updates of the NN depend on the loss function. } \vspace{-1ex}
% \label{fig:VAEGAN}
% \end{figure}

% \begin{figure}[htbp]
%   \begin{minipage}[b]{0.5\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/VAEGAN.pdf}
%     \caption{Illustration of the VAEGAN algorithm }
%     \label{illu} 
%   \end{minipage}
%   \hspace{0.5cm}
%   \begin{minipage}[b]{0.5\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/GANDA-VAE.pdf}
%     \caption{Illustration of the dependence between the loss functions and the neural networks (NN's) in the VAEGAN algorithm. The only link between the GAN part of the algorithm and the VAE part of the algorithm is the dependence of $\mathcal{L}_r$ on $G$. The dotted arrowed links mean one-sided dependence. For example, the dotted link from $D$ to $\mathcal{L}_g$ means that even though $\mathcal{L}_g$ depends on $D$, the update of $D$ only depends on $\mathcal{L}_d$ the discriminator loss. All solid lines are bi-directions, i.e., the loss function depends on the NN and the updates of the NN depends on the loss function.}
%    \label{GANDA-VAE} 
%   \end{minipage}
% \end{figure}


% \begin{figure}[htbp]
% \center
% \hspace*{-1.1cm}
%   \includegraphics[width=0.9\linewidth]{images/VAEGAN.pdf}
% %\vspace*{-0.4cm}  
% \caption{Illustration of the VAEGAN algorithm \todo{Consider sharing space with Fig.1}.}
% %\vspace*{-0.8cm}
% \label{illu} 
% \end{figure}


% \vspace*{-0.5cm}
% \begin{figure}[htbp]
% \centering
% \hspace*{-0.9cm}
% \includegraphics[clip, trim=3cm 7cm 3cm 5cm, width=1.2\linewidth]{images/VAEGAN.pdf}
% \caption{Illustration of the VAEGAN algorithm.} 
% \label{illu} 
% % \vspace*{0.7cm}
% \end{figure}

{\bf (1) Networks.} There are five neural networks (by neural network, we mean the network architecture and all its weights) in the algorithm: 1) $G$ is the generator; 2) $D$ is the discriminator; 3) $C$ is the classifier; 4) $E$ is the encoder; 5) $F$ is the decoder. 

{\bf (2) Inputs and outputs.} The data is given by $(x,l,d)$ where $x$ is the input; we use the notation %with 
$x^s$ and $x^t$ to respectively represent the source and target data samples, when necessary to distinguish them. $l$ is the label of sample $x$ (if any), and $d$ is the domain identity (e.g., can be as simple as one bit of $0$ for the source domain and $1$ for the target domain). In the zero-shot or few-shot domain adaptation settings, $l$ is available for all source data samples, but none or only a few are known for the target samples. $x$ is the input given to both encoder $E$ and generator $G$. The DDRep and DIRep correspond to the intermediate outputs of $E$ and $G$, respectively:
 \begin{equation}
 DDRep=E(x),\;\;\; DIRep=G(x), 
 \end{equation}
which then serve as the inputs for the downstream networks $F$, $D$, and $C$. In particular, DIRep serves as the input for $D$ and $C$, and both DIRep and DDRep serve as the inputs for $F$. The outputs of these three downstream networks are $\hat{x}$ from the decoder $F$;  $\hat{d}$ from the discriminator $D$; and $\hat{l}$ from the classifier $C$. These outputs are:
\begin{eqnarray}
    \hat{x} &=& F(DDRep,DIRep)=F(G(x),E(x)),\\
    \hat{d} &=& D(DIRep)=D(G(x)),\\ \hat{l} &=&C(DIRep)=C(G(x)),
\end{eqnarray}
where we list the dependence of the outputs on the corresponding  networks explicitly. 

\remove{
\todo{Adrain: alternative write-up for inputs and outputs paragraph.}
The data is given by $(\bf{X,Y, d})$ where $\bf{X}$ is the input with $\bf{X_s}$ and $\bf{X_t}$ representing the source and target data respectively. $\bf{Y}$ is the label of the sample and and $\mathbf{d}$
is the domain identity. In the zero-shot or a-few-shot transfer learning settings, $\bf{Y_s}$ is available for all the source samples, but none or only a few labels $\bf{Y_t}$ are known for the target samples. $\bf{X}$ is the input for the shared encoder $E$ and the shared generator $G$. The DDRep ($DD$) and DIRep ($DI$) correspond to the intermediate outputs of $E$ and $G$, respectively:
 \begin{equation}
 DD=E(\mathbf{X}),\;\;\; DI=G(\mathbf{X}), 
 \end{equation}
which then serve as the inputs for the down stream networks $F$, $D$, and $C$. In particular, $DI$ serves as the input for $D$ and $C$, and both $DI$ and $DD$ serve as the input for $F$. The outputs of these three down stream networks are $\hat{\mathbf{X}}$ from the decoder $F$;  $\hat{\mathbf{d}}$ from the discriminator $D$; and and $\hat{\mathbf{Y}}$ from the classifier $C$. These outputs are: 
\begin{eqnarray}
    \hat{\mathbf{X}} &=& F(DD,DI)=F(G(\mathbf{X}),E(\mathbf{X})),\\
     \hat{\mathbf{Y}}&=&C(DI)=C(G(\mathbf{X})), \\
     \hat{\mathbf{d}} &=& D(DI)=D(G(\mathbf{X}))
\end{eqnarray}
where we list the dependence of the outputs on the corresponding  networks explicitly. }

%\paragraph{Explicit DD algorithm.}
%\todo{why are we introducing expicit DD here?  shouldn't it be in earlier section?}

%\todo{Does the classifier get trained at the same time as the rest in explicit DDRep?  }
\remove{\todo{I changed the following equations a bit. Please check for correctness, in particular the generator loss. I also added a bit of text and changed the order. BTW removing this todo shortens the length more than one might think}}
{\bf (3) Loss functions.} Some measures of the differences between the predictions from the networks, i.e., $(\hat{x},\hat{d},\hat{l})$ and their actual values $(x,d,l)$ are used to construct the loss functions. Typically a loss function would take two arguments, a prediction and the actual label/value. We often use the name of the loss function without specifying the arguments, and do so for the discriminator, generator, classification and reconstruction losses. Like a variational autoencoder (VAE), we introduce an additional KL divergence loss function for $E$ to create a minimal DDRep so we can force most of the input information into the DIRep. All the loss functions with their dependence on specific neural networks are given explicitly here: (1) Classification loss: $\mathcal{L}_c=\mathcal{L}_c(\hat{l},l)=\mathcal{L}_c(C(G(x),l)$. (2) Discriminator loss: $\mathcal{L}_d= \mathcal{L}_d (\hat{d},d)=\mathcal{L}_d(D(G(x),d)$. (3) For the generator loss, we want to train the generator to fool the discriminator. So, the generator has a smaller loss when the discriminator makes the wrong prediction: $\mathcal{L}_g=\mathcal{L}_g(\hat{d},1-d)=\mathcal{L}_d(D(G(x)),1-d) $.
(4) Reconstruction loss: 
$\mathcal{L}_r=\mathcal{L}_r(\hat{x},x)=\mathcal{L}_r(F(G(x),E(x)),x)$. (5) KL loss for DDRep:
$\mathcal{L}_{kl} =D_{KL}(Pr(E(x))\parallel\mathcal{N}(0,I))$.
%\end{enumerate}
For the reconstruction loss  $\mathcal{L}_r$, we used the $L_2$-norm. For  $\mathcal{L}_d,\mathcal{L}_g,\mathcal{L}_c$, we used cross entropy. More details can be found in Appendix \ref{apploss}.\remove{\todo{mw The more details are now in supplement, so the above reference doesn't work. also see if you like the way i defined
$\mathcal{L}_d(\hat{d},d)$ etc.}}
The dependence of the loss functions on the neural networks in the \verb"VAEGAN" algorithm is shown in Fig.\ref{fig:VAEGAN}(b). 




% \begin{figure}[htbp]
% \center
% \hspace*{-0.5cm}
%   \includegraphics[width=0.75\linewidth]{images/GANDA-VAE.pdf}
% %\vspace*{-0.4cm}  
% \caption{Illustration of the dependence between the loss functions and the neural networks (NN's) in the VAEGAN algorithm. The only link between the GAN part of the algorithm and the VAE part of the algorithm is the dependence of $\mathcal{L}_r$ on $G$. The dotted arrowed links mean one-sided dependence. For example, the dotted link from $D$ to $\mathcal{L}_g$ means that even though $\mathcal{L}_g$ depends on $D$, the update of $D$ only depends on $\mathcal{L}_d$ the discriminator loss. All solid lines are bi-directions, i.e., the loss function depends on the NN and the updates of the NN depends on the loss function.}
% %\vspace*{-0.8cm}
% \label{GANDA-VAE} 
% \end{figure}



% \begin{figure}[htbp]
% \centering
% \includegraphics[clip, trim=6cm 7cm 6cm 7cm,width=1.0\linewidth]{images/GANDA-VAE.pdf}
% \caption{Illustration of the dependence between the loss functions and the neural networks (nn's) in the VAEGAN algorithm. The only link between the GAN part of the algorithm and the VAE part of the algorithm is the dependence of $\mathcal{L}_r$ on $G$. The dotted arrowed links mean one-sided dependence. For example, the dotted link from $D$ to $\mathcal{L}_g$ means that even though $\mathcal{L}_g$ depends on $D$, the update of $D$ only depends on $\mathcal{L}_d$ the discriminator loss. All solid lines are bi-directions, i.e., the loss function depends on the nn and the updates of the nn depends on the loss function.\todo{hard to read when printed} } 
% \label{GANDA-VAE} 
% \end{figure}


{\bf (4) The back-prop based learning.} The gradient-descent based learning dynamics for updating the five neural networks is described by the following equations:
\begin{equation*}
\begin{aligned}[c]
\Delta G &= -\alpha_G \left(\lambda \frac{\partial \mathcal{L}_g}{\partial G}+\beta\frac{\partial \mathcal{L}_c}{\partial G} +\gamma\frac{\partial \mathcal{L}_r}{\partial G}\right),\\
\Delta C &= -\alpha_C \frac{\partial \mathcal{L}_c}{\partial C},
\quad \Delta D = -\alpha_D \frac{\partial \mathcal{L}_d}{\partial D} ,
\end{aligned}
\qquad\qquad
\begin{aligned}[c]
\Delta E &= -\alpha_E \left(\frac{\partial \mathcal{L}_{kl}}{\partial E} +\mu \frac{\partial \mathcal{L}_r}{\partial E}\right) ,\\
\Delta F &= -\alpha_F \frac{\partial \mathcal{L}_r}{\partial F} ,
\end{aligned}
\end{equation*}

% \begin{eqnarray}
% \Delta G &=& -\alpha_G \left(\lambda \frac{\partial \mathcal{L}_g}{\partial G}+\beta\frac{\partial \mathcal{L}_c}{\partial G} +\gamma\frac{\partial \mathcal{L}_r}{\partial G}\right),\\
% \Delta C &=& -\alpha_C \frac{\partial \mathcal{L}_c}{\partial C},\\
% \Delta D &=& -\alpha_D \frac{\partial \mathcal{L}_d}{\partial D} ,\\
% \Delta E &=& -\alpha_E \left(\frac{\partial \mathcal{L}_{kl}}{\partial E} +\mu \frac{\partial \mathcal{L}_r}{\partial E}\right) ,\\
% \Delta F &=& -\alpha_F \frac{\partial \mathcal{L}_r}{\partial F} ,
% \end{eqnarray}
where $\alpha_{C,D,E,F,G}$ are the learning rates for different neural networks. In our experiments, we often set them to the same value, but they can be different in principle. %\todo{ do we want to cite the paper in the comparative lit section that trains in certain orders?}
The other hyperparameters $\lambda$, $\beta$, $\gamma$, and $\mu$ are the relative weights of the loss functions. These hyperparameters are also useful to understand the different algorithms. In fact, $\gamma=0$ corresponds to the \verb"GAN-based" algorithm presented in~\cite{singla2020preparing}. As easily seen from the equations above, when $\gamma=0$, the \verb"GAN-based" algorithm (blue/upper part in Fig.\ref{fig:VAEGAN}(b)) decouples from the VAE based constraints (yellow/lower part in Fig.\ref{fig:VAEGAN}(b)). 

\subsection{The Explicit DDRep Algorithm} To make the DIRep contain as much information as possible, we introduce a simplified \textit{explicit DDRep algorithm} without the encoder $E$ and set the DDRep explicitly to be the domain label (bit) $d$, i.e., $DDRep=d$. A variant of this approach is to add $d$ to the DDRep generated by the encoder. $d$ is the simplest possible domain dependent information that could serve to filter out the domain dependent information from the DIRep. 

We were surprised that in several cases the explicit DDRep performs as well as the \verb"VAEGAN". We think the \verb"VAEGAN" is more general, which is why we focus on it. When doing experiments with the \verb"VAEGAN" model we observed that the KL divergence of the DDRep corresponds to less than one bit measured as entropy. What we believe happens is that the DIRep contains information to describe both the original data and some generated information describing an alternative as if it came from the other domain. Then the DDRep merely has enough information for the decoder to determine which information to use in reconstructing the original data.

One useful feature of this simplified algorithm is that it allows us to check the effect of the DDRep directly by flipping the domain bit ($d\rightarrow 1-d$). We know the domain bit is effective in filtering out domain dependent information from the DIRep if the reconstructed image $\tilde{x} = F(DIRep, 1-d)$  resembles an image from the other domain as shown in Fig.\ref{recon_fm}. 

%\todo{I moved the last part of section 2 to here} 
\subsection{Comparing VAEGAN to DSN}Our \verb"VAEGAN" model shares several features with \verb"DSN". However, it does not limit DDRep and DIRep to be in the same embedding space, giving them freedom to capture different features from source and target samples. Moreover, \verb"VAEGAN" goes beyond the \textit{linear} orthogonality by exploiting the \textit{non-linear} KL-divergence in much the same way the conventional VAEs work. Specifically, our \verb"VAEGAN" attempts to keep DDRep as small as possible. As demonstrated in one set of our experiments on the Fashion-MNIST, the DDRep can be as small as a single bit, yet it still gets the task done properly. The advantage of keeping DDRep small is that \verb"VAEGAN" can force as much common sharing information between the two domains as possible into the DIRep, which maximizes the chances of success for the classifier $C$ in both data domains. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Move the following text to experiments section 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\remove{
\subsection{A simple synthetic dataset that allows cheating if there is only GAN}

Take the MNIST dataset with all $10$ digits and split it into two datasets -- source set and target set. For a sample $i$ in the source set, change the last pixel $x_N$ ($N=784$) to $x_N= l_i *X_{max}/10$, where $l_i$ is the true label of sample-$i$, i.e., $l_i=1$ if the image is digit $1$, etc. and $X_{max}$ is the maximum pixel number ($X_{max}=256?$). For a sample $i$ in the target set, change the last pixel $x_N$ to $x_N=\tilde{l}_i*X_{max}/10$ where $\tilde{l}_i$ is a random integer number between $0$ and $9$. 

In the source set, the modified last pixel serves as the cheating clue, which the algorithm can use to cheat for classification perfectly. This cheating number has the same distribution in the target and source set: $P(X_N|source)=P(X_N|target)$, so the discriminator would not detect the difference between source and target (note that this assumes that all 10 digits have roughly the same number of samples). 

If we use the original GAN based algorithm (without VAE etc.) with a reasonably small DI rep. and no label from the target domain. Then it is highly likely that the DI rep would just contain info. about this cheating number, which can classify perfectly for the labeled source data and also satisfies the discriminator as it has the same distribution in source ans target domain. If this is true, then the classifier based on this classifier will perform badly in the target domain. {\bf This is the baseline we need to establish first in order to demonstrate the advantage of the addition of VAE for classification for the target domain.} 

Then, with the new algorithm with VAE, DI rep. can not just contain info. about this cheating number, because the auto-encoder requires more information to be put into DI -- DD is restricted by KL. So, when we turn on the VAE, the performance (classification of the target images) may improve. Actually, as long as we can force the image-based classification instead of just the cheating number into the DI rep. from which we do classification of the target domain will help improve classification of the target domain. 

%\newpage

\subsection{Two simplified algorithms}

We now introduce two simplified algorithms: 
\begin{itemize}
\item[1] The explicit DDRep algorithm, i.e., building a rich DI rep. by explicitly introducing the DD rep. as the domain label $d$, i.e., $DD=d$ in short hand. For our problem, $d=0$ for the source data; $d=1$ for the target data.
\item[2] The 2-step algorithm, i.e., first training a DI. rep without the classifier; then use the DI. rep of the original input to train a classifier. Note that this 2-step process can be applied to any combined 
algorithm such as the original GAN-VAE algorithm as well as the simplified one described in (1) above.
\end{itemize}

For the explicit DDRep algorithm, once the reconstruction error is small, i.e., the reconstruction is successful, we can check if the domain neuron (the $d$-bit) is effective in filtering out domain-dependent information by flipping the $d$-bit: $d\rightarrow 1-d$ and compute the reconstruction loss $\tilde{L}_r=L_r(x,1-d)$. If this flipped reconstruction loss is much larger than the original loss $L_r(x,d)$, we know the domain neuron is effective in filtering out domain dependent info. from the DI rep. Also, we can check the reconstructed image $\hat{x}(x,1-d)$ to see if it resembles an image from the other domain. 

For the 2-step algorithm, we can simply turn off the classification loss $L_c$ by making its multiplier $\lambda_c=0$ in step-1. Once step-1, which is the supervised learning part of the algorithm, is finished, we can freeze the generator $G$ and turn back the classification loss $L_c$. Step-2 of the algorithm basically uses whatever available labels (from source and target domain) to train a classifier $C$ on the transformed data $G(x)$. Note that only $C$ (this is my $C$, the small "c") is updated in step-2 of the algorithm. 



%\newpage

\subsection{Domain-specific cheating}

To demonstrate the utility of the auto-encoder for generating a ``rich" domain independent representation, we can consider including domain-specific cheating
bits to the original image data. For example, in MNIST dataset, we split the dataset into two sets: source and target. For the source domain, we append 
specific one-hot bits to the image data: (100000000) for all images of "0"; (0100000000) for image "1"; etc. For the target domain, we append a different one-hot bits to the images, e.g., (0100000000) for all images of "0"; (1000000000) for al images of "1", etc. Note that the one-hot bits for the target domain is NOT random, the mapping from the one-hot bits to the image is unique, it is just different from that in the source domain.

The reason for introducing this different but unique one-hot bits in the target domain is the hope that the mapping between these two different cheating bits in source and target domains can be learned through the domain bit (d) we put in the intermediate layer and can thus be filtered out of the DI rep. if we use the auto-encoder based algorithm. On the other hand, when we use the GAN based algorithm (as in Ankush's original paper), these cheating bits would survive the GAN; furthermore, the source cheating bits can be picked up by the classifier when we trained the classifier with only the labeled source data (zero-shot learning), i.e., the classifier will cheat. If that's the case, then the performance of this cheating classifier (from the source domain) would perform poorly for the target domain, which has a different cheating scheme. For example, with the example of the cheating bits in the two domains described above. This cheating classifier would classify a "0" in the target domain as a "1"; and an "1" as "0".

Actually, after thinking about it more, we can make a simpler database for this purpose by using two digits, say "1" and "0". In the source domain, we put the background to be blue for "0" and red for "1"; in the target domain, we simply reverse the color scheme: red for "0" and blue for "1". 
}




 \section{Experiments}
 \label{Experiments}
 %\elisa{I think that starting the section with this sentence is not a good idea. It would be better to start by 2-3 statements stating the goals of the experiments.}
 %\input{sections/Experiments}


 

%This section will demonstrate that \verb"VAEGAN" has superior results. 
We now evaluate \verb "VAEGAN" across different adaptation settings. We first demonstrate that in an artificial setting the hidden data effect takes place. Then we consider a more natural settings to show that we have a real advantage over a \verb"DANN" like system and even a \verb"DSN" like system. Since the generator and classifier topologies are the same in all of these systems, we need to show that the weights trained by \verb"VAEGAN" are usually better. The \verb"DANN" systems may occasionally have the same performance as \verb"VAEGAN". Thus we have used multiple runs and a z-score to assess whether one set of runs is statistically better than another.
%equal ours, 
%\elisa{I'm not about the text below. Whom do you refer to with the "person training" the system? To the person from our team who did the experiments? Also it seems to me normal that one does multiple runs and use z-score. I would just remove the sentence above "The \verb"DANN" systems may occasionally have the same performance as VAEGAN," and just say that in the experiments we did multiple scans and used the z-score to .... The fact that DANN occasionally has the same performance as VAEGAN can be discussed (if needed) when discussing the experimental results.}
%but the person training the system cannot know how well they did without extensive testing. Thus we have used multiple runs and a z-score to assess whether one set of runs is statistically better than another.
%\markw{I'm not sure everyone does Z scores, or even knows what they mean.  We could change the , after VAEGAN above to a period, and wipe out the rest of that sentence, but leave the one starting with "Thus".}


Cheating is our technique for encouraging the hidden data effect. If you do a Google search for images of wolves, probably half of them are in snow. If you do a Google search for dogs, none of them are likely to be in snow. So if we want to classify dogs from wolves where the domain is images on the internet, snow is very helpful. But suppose you want to classify dogs from wolves in the results stemming from the query ``animals in winter". In that case, snow is useless. By adding information to enable a classifier of images to ``cheat" by using useful information only available in one domain, we have found we encourage the hidden data effect. What we call cheating is a common problem that occurs in some natural DAs.

We construct several DA scenarios where some  cheating clue exist in the source domain based on two widely used image datasets: (1) \verb"Fashion-MNIST"~\footnote{https://github.com/zalandoresearch/fashion-mnist}, which consists of $60,000$ grayscale images for training and $10,000$ images for testing. Each image is represented as a 2-dimensional tensor of $28\times28$ and belongs to one of 10 classes; (2) \verb"CIFAR-10" dataset~\footnote{https://www.cs.toronto.edu/~kriz/cifar.html} which consists of $50,000$ images for training and $10,000$ images for testing from 10 classes. Each image is represented by a $32\times32\times3$ tensor (i.e., a color image with 3 channels of Red, Green and Blue). %For each of these two datasets, we will construct several variants for different unsupervised domain adaptation scenarios. %More importantly, our goal is to evaluate the learned representations from different prevailing unsupervised domain adaptation approaches and our method when the source domain contains spurious correlations between data and labels that  

We also validate our model on the standard benchmarks for unsupervised DA. We assessed three different adaptation scenarios by using four digits datasets,  namely MNIST~\cite{lecun1998gradient}, MNIST-M~\cite{ganin2016domain}, Street View House Number~\cite{netzer2011reading} and synthetic digits~\cite{ganin2016domain}. Finally, we evaluate on the Office~\cite{saenko2010adapting} dataset for comparison against other unsupervised DA approaches.
%\elisa{The following two sentences should be removed. Text emphasizing that VAEGAN does better should be moved after the analysis of the experiments.}
%\verb"VAEGAN" achieves superior adaptation performance compared to previous works without using any validation samples from target to tune hyper-parameters. Details are provided in Section 4 of our supplementary material. 

\subsection{Benchmarks that demonstrate hidden data effect}


\subsubsection{Fashion-MNIST Classification}

Fashion-MNIST is a well known dataset, which we use as a source domain.  We construct a target domain by flipping the images of $180^o$. To simulate shortcut information, we add to the source data set a one hot vector that contains the correct classification.  We call that information cheating bits, because it is not %information will not be 
available in the target domain. To the target dataset we also add some bits, but they either include information suggesting a random classification (random cheating), so the shortcut information is useless in the target domain or one that is shifted to the next label from the correct label (shift cheating), so it is always wrong, but perhaps in a predictable way. The one-hot bits have the same distribution in the source and target data sets, so if they are reflected in the DIRep the discriminator would not detect the difference between source and target. In this case, a classifier learned from this partial representation  would perform poorly on the target data.

%\elisa{in the above, what is "one off"? a "one-hot bits off"?}\markw{Does that help?}
%\elisa{yes, thanks}

%\markw{include benchmark alg section}

%\markw{We compare our ability to ignore the cheating bits in table \ref{fashion-mnist} and table\ref{z test FM}}

%\markw{leave out arch and training proc as well as resulst and analysis and say more detail in supplementary, maybe leave out the reconstructed pictures, or move them to the supplement with a short paragraph description?}

%\paragraph{A simple synthetic dataset that allows cheating.} We start our experiments with a synthetic dataset to demonstrate the ability of \verb"VAEGAN" to generate a more useful DIRep. We use the original Fashion-MNIST as the source data and flip every image of $180^o$ to get the target samples. We add to the source and target images data domain-specific ``cheating" bits. For each source image, we append one-hot bits of its label to the vectorized image, e.g., $e_0 = (1,0,0,0,0,0,0,0,0)$ for all images of class $0$, $e_1 = (0,1,0,0,0,0,0,0,0)$ for images of class $1$, etc. For the target cheating bits, we shift the one-hot bits of a truth label to the right by $1$, i.e., $e_0 = (0,1,0,0,0,0,0,0,0)$ for all images of class $0$; $e_1 = (0,0,1,0,0,0,0,0,0)$ for all images of class $1$. The  mapping from the cheating bits to the images in the target domain is still unique, just different from that in the source domain. We call this adaptation scenario \textit{shift cheating}. A more difficult scenario is the one where we append one-hot bits of random labels to the target images, which we refer to as \textit{random cheating}. 

%In the source set, the one-hot bits serve as  the cheating clue, which the algorithm can use to classify  perfectly. The one-hot bits have the same distribution in the source and target data sets, so if they are reflected in the DIRep the discriminator would not detect the difference between source and target. 

%If we use a GAN-like unsupervised DA algorithm, because of the hidden data effect, the DIRep would contain information about the one-hot bits. The classifier would then be able to correctly classify the source data. That information can also elude the discriminator as it has the same distribution both in source and target domains. In this case, a classifier learned from this partial representation  would perform poorly on the target data.



\paragraph{Benchmark algorithms.}
%We evaluate  our method on the constructed Fashion-MNIST datasets by 
We compare our method against the prevailing unsupervised DA approaches: \verb"GAN-based" approach~\cite{singla2020preparing}, Domain-Adversarial Neural networks (\verb"DANN")~\cite{ganin2016domain} and Domain Separation Networks (\verb"DSN")~\cite{bousmalis2016domain}. We implemented both \verb"VAEGAN" and the explicit DDRep algorithm in the zero-shot setting. We only report the results about the explicit DDRep algorithm as it achieves almost identical performance on this task. We also provide two baselines, a classifier trained on the source domain samples without DA (which gives us the lower bound on target classification accuracy) and a classifier trained on the 
target domain samples (which gives us the upper bound on target classification accuracy).  We compare our ability to ignore the cheating bits in Table \ref{fmnist} and Table \ref{z test FM}. More details of the topology, learning rate, hyper-parameters setup and results analysis is provided in Appendix \ref{appfm}.



\begin{table}%[htbp]
\captionsetup{font=scriptsize}
\parbox[t]{.49\linewidth}{
\centering
  \caption{Mean classification accuracy (\%) of different unsupervised DA approaches for the constructed Fashion-MNIST datasets.}
  \resizebox{0.45\textwidth}{!}{%
     \begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
Source-only & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}20.0}    & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}11.7}  & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}13.8}   \\
GAN-based   &    \multicolumn{1}{r}{64.7}                      &    \multicolumn{1}{r}{58.2}                      &      \multicolumn{1}{r}{54.8}                        \\
DANN        & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}63.7} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}58.0} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}53.6} \\
DSN        &     \multicolumn{1}{r}{66.8 }                    &       \multicolumn{1}{r}{63.6 }                  &       \multicolumn{1}{r}{57.1}                   \\
VAEGAN      & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{66.9}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{65.7}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{61.6}} \\
Target-only &  \multicolumn{1}{r}{88.1}                          & \multicolumn{1}{r}{99.8}                            & \multicolumn{1}{r}{87.9} \\
\hline

\end{tabular}
  }
  \label{fmnist}%
}
\hfill
\parbox[t]{.5\linewidth}{
 \centering
  \caption{z-test score value comparing VAEGAN to other models for constructed Fashion-MNIST. z$>$2.3 means the probability of VAEGAN being no better is $\leq$0.01.}
  \resizebox{0.45\textwidth}{!}{%
 \begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
GAN-based & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}1.55}    & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.28}  & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.68}   \\
DANN   &    \multicolumn{1}{r}{2.26}                      &    \multicolumn{1}{r}{4.17}                      &      \multicolumn{1}{r}{4.33}                        \\
DSN        & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}0.16} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.60} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.18}  \\
\hline

\end{tabular}
  }
  \label{z test FM}%
}
\end{table}





% \begin{table}[t]
% \centering
% \begin{tabular}{llll}
% \hline
% Model &
%   \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
%   \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
%   \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
% Source-only & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}20.0}    & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}11.7}  & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}13.8}   \\
% GAN-based   &    \multicolumn{1}{r}{64.7}                      &    \multicolumn{1}{r}{58.2}                      &      \multicolumn{1}{r}{54.8}                        \\
% DANN        & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}63.7} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}58.0} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}53.6} \\
% DSN        &     \multicolumn{1}{r}{66.8 }                    &       \multicolumn{1}{r}{63.6 }                  &       \multicolumn{1}{r}{57.1}                   \\
% VAEGAN      & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{66.9}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{65.7}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{61.6}} \\
% Target-only &  \multicolumn{1}{r}{88.1}                          & \multicolumn{1}{r}{99.8}                            & \multicolumn{1}{r}{87.9} \\
% \hline

% \end{tabular}
% \caption{Mean classification accuracy (\%) of different unsupervised domain adaptation approaches for constructed Fashion-MNIST datasets. \todo{Consider placing Tables 1,2 next to each other}}\label{fmnist}
% \end{table}


% \begin{table}[t]
% \centering
% \begin{tabular}{llll}
% \hline
% Model &
%   \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
%   \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
%   \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
% GAN-based & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}1.55 }& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.28} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD} 3.68} \\
% DANN      & \multicolumn{1}{r}{2.26}                  & \multicolumn{1}{r}{4.17}                  & \multicolumn{1}{r}{4.33 }                  \\
% DSN       & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}0.16} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.60}& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.18} \\
% \hline
% \end{tabular}
% \caption{z-test score value comparing VAEGAN to other models for constructed Fashion-MNIST. z$>$2.3 means the probability of VAEGAN being no better is $\leq$0.01.}\label{z test FM}
% \end{table}

 \begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth]{images/reconstruction-FM.pdf}
\caption{\remove{Reconstructions for the source and target test images. From left to right: the original image $x_{test}$, reconstructed image $F(G(x_{test}), d(d=0)$; reconstructed image  $F(G( x_{test}), d(d=1)$.}Columns 1 and 4, original images;  2 and 6, reconstructions of originals; 3 and 5, reconstructions with domain bit flipped.}%\todo{Do we want to move these pictures to supplement or shrink them down}}
\label{recon_fm} 
\vspace{-2mm}
\end{figure}

%\paragraph{The architecture and training procedure.} \label{fashion-mnist}


%All the methods were trained using  Adam optimizer with the learning rate of 0.001 for 10000 iterations. We used batches of 128 samples from each domain for a total of 256 samples. When training with our model (VAEGAN), the label prediction pipeline (generator and classifier) has eight fully connected layers (\textsc{fc1, fc2, fc3, fc4, fc5, fc6, fc7, fc\_out}). The number of neurons in \textsc{fc1-4}  are 100 for each layer. \textsc{fc5} is a 100-unit layer that generates DIRep, followed by two 400-unit layers (\textsc{fc6-7}). \textsc{fc\_out} is the output layer for label prediction. The discriminator and decoder each has four layers with 400 hidden units and followed by the domain prediction layer and reconstruction layer respectively. The encoder has two layers with 400 units, followed by 1-unit $z$\_mean, 1-unit $z$\_variance and 1-unit sampling layer. Each of the 400-unit layer uses a ReLU activation function. 

 %The exact topology, learning rate and hyper-parameters setup is provided in Section 2 of our supplementary material. All the models above are dense networks and have the same architecture as \verb"VAEGAN" when applicable. For the \verb"GAN-based" approach and \verb"DANN", we turn off the decoder and corresponding losses. For the \verb"DSN", we keep the same network architecture for common networks and use $\mathcal{L}_g$  for the similarity loss. Furthermore, we implement the shared and private encoders with same shape output vectors \cite{bousmalis2016domain}. We closely follow the  setup of weights of the loss functions used in the \verb"DSN" paper~\cite{bousmalis2016domain} and \verb"DANN" paper~\cite{ganin2016domain}. For details of the hyper-parameters see Section 2.2 of the supplementary material.




%\paragraph{Results and analysis.}



%We now discuss the experiment results. 
%Table~\ref{fmnist} summarizes the mean classification accuracy of different approaches for three cheating scenarios. In the no cheating scenario, we use the original Fashion-MNIST as source and flip the Fashion-MNIST for the target. We report the z-score of the comparison of the mean classification accuracy of our method with the mean classification accuracy of other methods over five independent runs (see Table \ref{z test FM}). The higher the z-score, the more statistical confidence we should have that our method outperforms the other methods. A z-score of $2.33$ corresponds to $99\%$ confidence that our method is superior, assuming that the accuracy over different runs will follow a  Gaussian distribution. 





\begin{table*}[t]
\centering
\caption{Averaged classification accuracy (\%) of different unsupervised DA approaches for constructed CIFAR-10 dataset with a spectrum of bias.}\label{cifar10}
\begin{tabular}{lllllllll}
\toprule
Model     & \multicolumn{1}{r}{0\% bias}  & \multicolumn{1}{r}{20\% bias }   & \multicolumn{1}{r}{40\% bias }   & \multicolumn{1}{r}{60\% bias}       & \multicolumn{1}{r}{80\% bias}       & \multicolumn{1}{r}{90\% bias}                    & \multicolumn{1}{r}{100\% bias}                   \\ \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }
& \multicolumn{1}{r}{\cellcolor[HTML] {E0DDDD}10.0} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0}    
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }    
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }\\

GAN-based  
& \multicolumn{1}{r}{63.0} 
& \multicolumn{1}{r}{62.5}
& \multicolumn{1}{r}{61.4}
& \multicolumn{1}{r}{56.9}
& \multicolumn{1}{r}{53.2}
& \multicolumn{1}{r}{44.5}
& \multicolumn{1}{r}{30.1} \\

DANN  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 62.7}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 62.0} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 61.0} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 56.5} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 52.2} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 42.9} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333}29.1}} \\
DSN      
& \multicolumn{1}{r}{68.7 }
& \multicolumn{1}{r}{67.9  } 
& \multicolumn{1}{r}{67.3  }     
& \multicolumn{1}{r}{67.5   }  
& \multicolumn{1}{r}{64.5 }     
& \multicolumn{1}{r}{61.7 }   
& \multicolumn{1}{r}{32.2  }           \\
VAEGAN   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{70.4}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.8} }      
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.8}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.7 }}  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{68.2 }}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{64.1} } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{34.2 }}   \\
Target-only     
& \multicolumn{1}{r}{   78.9  }
& \multicolumn{1}{r}{78.9  }
&  \multicolumn{1}{r}{ 78.9   }
&   \multicolumn{1}{r}{ 78.9  } 
&  \multicolumn{1}{r}{78.9  }
&  \multicolumn{1}{r}{ 78.9 }   
& \multicolumn{1}{r}{78.9}  \\ 
\hline
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\caption{z-test score value comparing VAEGAN to other models for constructed CIFAR-10. z$>$2.3 means the probability of VAEGAN being no better than the other models is $\leq$0.01.}\label{cifar10_z_score}
\begin{tabular}{llllllll}
\hline
Model     & \multicolumn{1}{r}{0\% bias}  & \multicolumn{1}{r}{20\% bias }   & \multicolumn{1}{r}{40\% bias }   & \multicolumn{1}{r}{60\% bias}       & \multicolumn{1}{r}{80\% bias}       & \multicolumn{1}{r}{90\% bias}                    & \multicolumn{1}{r}{100\% bias}                  \\ \hline
GAN-based
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}5.23 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.20 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}5.93 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}12.8 } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}11.31 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}7.20} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}4.58 } \\
DANN     
& \multicolumn{1}{r}{5.44 }      
& \multicolumn{1}{r}{3.42 }          
& \multicolumn{1}{r}{6.22 }      
& \multicolumn{1}{r}{13.2  }  
& \multicolumn{1}{r}{12.02  }   
& \multicolumn{1}{r}{7.79 }          
& \multicolumn{1}{r}{5.70  }                        \\
DSN     
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.68 } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.00 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.95} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.47 }  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}7.43}  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.78} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.23 }\\ \hline
\end{tabular}
 % Statistical z-test score indicating the averaged classification accuracy of our approach on Cifar10 is higher with a significant level of 0.01 \todo{chose which caption we prefer}\todo{?}}
\vspace{-1ex}
\end{table*}


\begin{figure}[t]
\centering
\includegraphics[scale=0.5]{images/semi_supervised_h.pdf}
\caption{Mean classification accuracy on CIFAR-10 with semi-supervised setting for three different unsupervised DA algorithms. Overall, a few target labels improve classification accuracy. The improvement is significant in 100\% bias setting.}
\label{semi} 
\end{figure}





%In the no cheating scenario, \verb"VAEGAN" outperforms  \verb"GAN-based" and \verb"DANN" and matches the result of \verb"DSN". %as the classification task is relatively easy on the target domain. A large amount of shape and texture information is still preserved when we flip the images. 
%The performance of \verb"GAN-based" and \verb"DANN" results in a $5\%$ accuracy drop for the shift cheating and $10\%$ drop for the random cheating. This validates our concern with the hidden data effect. The source cheating bits can be picked up in the DIRep as they represent an easy solution for the classifier that is trained only with source samples. If that is the case, then the cheating generator would perform poorly for the target domain, which has different cheating bits. Our method has only $1\%$ and $5\%$ accuracy drop respectively and is less vulnerable to the hidden data effect. As a reconstruction-based method, \verb"DSN" performs better in the presence of cheating bits. In the shift and random cheating, our approach significantly outperforms \verb"DSN" with a z-score of $2.60$ and $3.18$ respectively, which shows the correctness of our intuition that penalizing the size of DDRep can result in transferring as much information as possible to the DIRep. In the explicit DDRep algorithm, the DDRep is minimal as it only onctains the domain label. Given  a richer DIRep, our method leads to a classifier based on the invariant features of the images, which improves its performance on the target data.
 %As a result, more information from the images themselves instead of the cheating bits is pushed into the DIRep to reduce the reconstruction loss. Given  a richer DIRep, our method leads to a classifier based on the invariant features of the images, which improves its performance on the target data.

As we mentioned, the decoder learns to reconstruct the image by using DIRep and DDRep (domain bit) together. %By flipping the domain bit, we can check the reconstructed image $\hat x_{test}$ to see if it resembles an image from the other domain.
Figure~\ref{recon_fm}  shows reconstructed images for the shift cheating scenario. The reconstructed images are a bit fuzzier and perhaps a bit more like generic images of elements in the same class. Some details seem to be getting lost
despite the reconstruction. Nevertheless, the reconstructed images have rendered the domain bit effective in filtering out domain-dependent information, as the reconstructed images look like they are from the other domain when we flip the domain bit. 

\remove{
We started our experiments with a synthetic data set intended to demonstrate that some of the classification could be done by the generator and
that this would mess up the cross domain transfer. We took as our
source data set Fashion-MNIST images augmented by what we called \textit{cheating
bits}, a one hot encoding of the label. The target domain replaces
the cheating bits with a random one-hot encoding. We were suprised
to find this results in a spectrum showing our theoretical concern
that the generator would do all the classification can go from being
all that the system does in to one were the system does the transfer
perfectly. The spectrum is determined by the multiplier on the cheating
bits. We first normalize MNIST so that each pixel has a value from
0, black, to 1, white. If the cheating bits have a max value of 1,
then the generator seems to not really find them and relies mostly
on other pixel values. But if we multiply the cheating bits by 10
they have a deleterious result on transfer learning unless we use
reconstruction from both the DIRep and the DDRep. 

We used simple neural networks for the discriminator, generator, classifier
and encoder. Presumably with a slightly more sophisticated network
we could get more accuracy with all the tests.


To make the problem a little harder, we took the images and in the
target domain flipped them 180 degrees. 

One intersting side effect of the mechanism with the domain bits is
we can take an image and not only see what it looks like in the reconstruction,
but we can also ask what the reconstruction looks like when the domain
bit is flipped. This is helpful for debugging and understanding what
we've accomplished. It mimics though other techniques for changing
images to another domain and we don't think we have anything unusual
in that regard.

Here's an example of an original image and what it looks like when
we reproduce flipping the domain bit. The resulting images are a bit
fuzzier and perhaps a bit more like the generic images of elements
with the same classification. Some details seem to be getting lost
despite the reproduction.

Next we wanted to chose a simple example that might show the potential
for practical applications. We were intrigued with transfer learning
to infrared images. Infrared is the part of the spectrum with wavelength
larger thant 700nm. We can see from 380nm to that point. But since
we didn't have a ready to hand source of infrared images, we chose
instead to take Cifar 10, which is composed of pictures divided into
Red, Green and Blue and pretend that we didn't have access to one
of those planes. Cifar-10 images are harder than Fashion-MNIST, so
we used simple CNN's for our networks.
}



\subsubsection{CIFAR-10 Classification}

%\paragraph{Spurious correlation that allows cheating.}
We are interested in more natural DA scenarios where the source and target images might be captured with different sensors and thus have different wavelengths and colors. To address this use case, we created our source and target datasets based on CIFAR-10\footnote{https://www.cs.toronto.edu/~kriz/cifar.html} with different color planes. Furthermore,  we introduced the cheating color plane where the choice of the color planes in  the source data have some spurious correlation with the labels. We observe similar hidden data effect on the CIFAR-10 set with spurious correlation, suggesting that the optimization difficulties of previous methods and the results of our methods are not limited to a particular dataset. 

The source set with cheating color planes is constructed as follows. First, we encode labels in CIFAR-10 with values between 0 and 9. Then for each CIFAR-10 image, if its label is odd, we keep only the B channel with prob  $p$, and randomly keep the B or the R channel for the rest. Similarly, if the label is even, with prob $p$, the image has only the R color channel, and  either the R or B channel is kept for the rest. For example, when $p=1$, all images with odd labels have only the B channel and all images with even labels have only the R channel. We call $p$ the \textit{bias} since it controls the strength of the spurious correlation between the color of the image and its label. In the target domain, for each CIFAR-10 image we keep only the G channel regardless of the label. We compare our approach and the others with $p$ taking values from the set $\{0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0\}$. A larger value of $p$ indicates a higher level of spurious correlation in the source data and thus a more challenging DA task. 

 In this ``cheating-color-plane” setting, the GAN-like  algorithms might cheat by leveraging the correlation between the presence or absence of the color planes and the label of the image to create an easier classification scheme for the labeled source data. %Since the target labels are not seen during training, the generator might learn the representation such that it utilizes the cheating clue for classification while it still satisfies the discriminator. 
 Consequently, the DIRep would include false cheating clues which can degrade performance for the target data where the cheating clues lead to the wrong answer.

%\markw{We can move this section to supplemenatry?}
%\paragraph{The architecture and training procedure.}  
%When training with our approach, we implement the network components as deep residual neural networks (ResNets) with short-cut connections \cite{he2016deep}. ResNets are easier to optimize, and sometimes gain accuracy from increased depth. For our approach, we implemented the full-fledged \verb"VAEGAN" and we added the domain label to the DDRep generated by the encoder. We implemented the same ResNet-based architecture for all other approaches (when applicable). We use a weight decay of $0.0001$ and adopt the BN~\cite{ioffe2015batch} for all the experiments. %The hyper-parameters are the same as the ones in Section \ref{fashion-mnist}. 
%The architecture and hyper-parameter details are included in Section 3 of our supplementary material.








%\paragraph{Results and analysis.} 

 We report the mean accuracy of different unsupervised DA methods and our approach on the target test set in Table \ref{cifar10}. The z-scores of comparing our method with other methods are shown in Table \ref{cifar10_z_score}. The details of the experiment and analysis of the results are included in Appendix \ref{appcifar}.
%\markw{Can we move the next paragraph to supplementary?}

%For all the DA tasks with varying biases, we observe that our approach outperforms the other approaches in terms of accuracy in the target set. This improvement is most pronounced when the source set has $60\%$ and $80\%$ bias levels, which means that over half of the source data has a spurious correlation between their color planes and labels. The poor performance of the \verb"GAN-based" and \verb"DANN" approaches is another example where the generator in these approaches learns a DIRep that depends on the spurious correlation. This false representation leads to an issue similar to over-fitting where the model performs well on the source data, but does not generalize well on the target data in which the same correlation does not exist. In the \verb"DSN" approach, the shared representation contains some domain-independent information other than the cheating clues which helps classification in the target domain. However, it does not directly address the problem of making the domain invariant representation richer for classification. %\todo{this largely recaps what we said in the related work section. that's not a bad thing, but i want people to think about whether it's too redundant. mw}
%We postulate that the inferior performance of the \verb"DSN" approach may stem from the way its shared and private representations are trained. The difference loss in \verb"DSN" only encourages orthogonality between the shared and the private representations in a linear way, which can be less effective in separating domain dependent and independent information for difficult DA scenarios. 



\subsubsection{Semi-supervised Domain Adaptation }
%\todo{This section and Fig. 4 need to be revised. From Fig. 4, the general behaviors of the three algorithms are quite similar: performance does improve but not significantly as the number of target labels increases; the performance order $VAEGAN~>DSN>GAN$ is prerserved. However, the performance curves for bias=100\% are quite different: all of them increase significantly with the number of target label while the order of performance is preserved. Given these observations, we should probably trim Fig. 4, perhaps to 4 panels (0,0.4,0.8,1.0) in a single column (2x2) figure. We can save some spcae this way also.} 

As an additional experiment, we also evaluated the proposed algorithm for semi-supervised 
DA on the constructed CIFAR-10 datasets. The model is provided with a majority of unlabeled target data and a small amount of labeled target data. In our setting, we revealed $1, 5, 10, 20, 50$ and $100$ samples per class which we then used for contributing to the classification loss through the label prediction pipeline. We also provided the same number of labels for the \verb"GAN" and \verb"DSN" method. We skipped the \verb"DANN" method since its performance is very similar to the \verb"GAN" approach. More importantly, we ask the following question: \textit{How much does each algorithm gain from a small labeled target training set for different biases?} The classification loss on the target ensures that the generator does not get away with learning a DIRep that contains only the cheating clue, which could bias the model during training and cause a high classification loss.

We select four most representative biases and show the results in Figure~\ref{semi}. For $40\%$, $60\%$ and $80\%$ biases, the classification accuracy does improve, but not significantly as the number of target labels increases. The performance order of \verb"VAEGAN" $>$ \verb"DSN" $>$ \verb"GAN-based" is preserved. When the bias is equal to $100\%$, the performance curves are quite different. All of them increase significantly with the number of target labels, while the order of performance is preserved. While all three algorithms benefit from a small number of target labeled samples, \verb"VAEGAN" improves the most, surpassing \verb"DNS" and \verb"GAN-based" results by $12\%$ and $25\%$ respectively with  only a total of $50$ target labels (note that it corresponds to $5$ labels/class in Figure \ref{semi}).

\subsection{Standard DA benchmarks}



\begin{table}[t]
\captionsetup{font=scriptsize}
\parbox[t]{.49\linewidth}{
\centering 
\caption{Mean classification accuracy (\%) of different unsupervised domain adaptation approaches for widely used benchmarks. The results are cited from each study. }
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}MNIST to \\MNIST-M\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Synth Digits to\\SVHN\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}SVHN to\\MNIST\end{tabular}  \\
  \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}56.6}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}86.7} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}59.2}
\\
DANN       
& \multicolumn{1}{r}{76.6} 
& \multicolumn{1}{r}{91.0} 
& \multicolumn{1}{r}{73.8}
\\
DSN       
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{83.2}}%\tablefootnote{In the MNIST and MNIST-M shift, we used the code for DNS from their Github implementation but our measure of DNS is $80\%$which is less than the published results and the result achieved by"VAEGAN".}}                  
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{91.2} }             
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}82.7}   
   \\
VAEGAN     
& \multicolumn{1}{r}{81.0} 
& \multicolumn{1}{r}{91.1} 
& \multicolumn{1}{r}{\textbf{85.8}} 
\\
Target-only
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}98.7}                 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}92.4}                          
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}99.5} 
\\

\hline

\end{tabular}
}

\label{mnist}

}
\parbox[t]{.49\linewidth}{
\centering 
\caption{Mean classification accuracy (\%) of different unsupervised domain adaptation approaches for the office dataset. We have cited the original results (if applicable) and in
parentheses we show the results reported in~\cite{chen2020adversarial} with data augmentation.}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}$A \rightarrow W$\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}$D \rightarrow A$\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}$W \rightarrow A$\end{tabular}  \\
  \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}64.2 (68.4)}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD} - $\ $  (62.5)} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD} - $\ $ (60.7)}
\\
DANN       
& \multicolumn{1}{r}{73.0 (82.0)} 
& \multicolumn{1}{r}{- $\ $ (68.2)} 
& \multicolumn{1}{r}{- $\ $ (67.4)}
\\
ADDA
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}75.1 (\textbf{86.2})}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}- $\ $ (69.5)} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}- $\ $ (68.9)}
\\
VAEGAN     
& \multicolumn{1}{l}{\textbf{82.0}} 
& \multicolumn{1}{l}{\textbf{72.9}} 
& \multicolumn{1}{l}{\textbf{72.0}} 
\\

\hline

\end{tabular}
}

\label{office}

}
\vspace{-2mm}
\end{table}

\subsubsection{Digits datasets}
 In this experiment, we use three unsupervised domain adaptation pairs: 1) MNIST $\rightarrow$ MNIST-M, 2) Synth Digits $\rightarrow$ SVHN, and 3) SVHN $\rightarrow$ MNIST. Example images from all four datasets are provided in Figure \ref{digits} of Appendix \ref{digitspairs}.  The architecture and hyper-parameter settings are provided in Appendix \ref{digitspairs} due to a limit of space. 


 

Table \ref{mnist} shows the results on the digits datasets. We cited the results from each study to make a fair comparison. In MNIST $\rightarrow$ MNIST-M and Synth Digits $\rightarrow$ SVHN,  we were not able to achieve the published results for \verb "DSN".  The previously published results for \verb "DSN" were based on using hyperparameters chosen after using 1000 labeled samples from the target.  Our parameters were chosen without use of target labels. % In our experiments we always beat DSN. 
%\markw{I wonder if we want to say something like this dataset was used to demonstrate the value of DSN by it's authors.  But it includes no different characteristics in source or target that would help classify so won't suffer much from the hidden data effect, never the less we do well.  We could say that if we have space -- if true}
In summary, VAEGAN outperforms DSN on SVHN $\rightarrow$ MNIST and is at least comparable elsewhere. % In summary, \verb"VAEGAN" outperforms \verb"DSN" on the difficult SVHN and MNIST shift and achieves comparable results on the Synth Digits and SVHN shift, despite \verb"DSN" using 1000 labeled target samples to tune the hyperparameters for each scenario and we used zero target labels for training. %In the MNIST and MNIST-M shift, we used the code for \verb"DNS" from their Github implementation but our measure of DNS is $80\%$ which is  less than the published results and the result achieved by \verb"VAEGAN".


\subsubsection{Office datasets}
The Office dataset has $4110$ images from $31$ classes in three domains: amazon ($2817$ images), webcam ($795$ images) and dslr ($498$ images). The three most challenging domain shifts reported in previous works are amazon to webcam ($A\rightarrow W$), dslr to amazon ($D \rightarrow A$) and webcam to amazon ($W \rightarrow A$). In $D \rightarrow A$ and $W \rightarrow A$  are the cases with the least labelled information. 

We follow the the previous work in~\cite{tzeng2017adversarial, chen2020adversarial} which use a pretrained ResNet-50 on ImageNet~\cite{deng2009imagenet} as a base. Details are provided in the Appendix \ref{officepairs}. We present the results in Table \ref{office}. We includes the original results from the \verb"DANN" and \verb"ADDA" papers as well as the new results from ~\cite{chen2020adversarial} with training data augmentation. \verb"VAEGAN" is competitive on this adaptation task, matching the performance of \verb"DANN" and outperforming the other approaches in $D \rightarrow A$ and $W \rightarrow A$ without using any data augmentation. 

\section{Conclusion and Future Work}
\label{Conclusion and Future Work}
%\input{sections/conclusion}



In this paper, %we have presented \verb"VAEGAN" as a deep learning model that gains advantages over existing techniques for domain adaptation. The advantages of \verb"VAEGAN" come from its design of explicitly learning two separate yet complement embedding latent spaces DIRep and DDRep such that \remove{there is no shared information between them yet} their combination is sufficient to reconstruct samples from both domains. More importantly, \verb"VAEGAN" enriches DIRep through penalizing the size of the DDRep to create a maximal DIRep shared by both domains. Learning a classifier from the DIRep enhances the accuracy of predicting labels in both target and source domain. We demonstrated this unique novel property of \verb"VAEGAN" using various synthetic datasets, and further demonstrated its superior performance against state-of-the-art studies on a number of common imaging datasets in the literature. 
%\todo{mw-people need to help decide which version of the conclusion they like}
%\textbf{Discussion -- alternate section}
we have described the challenges in DA caused by the hidden data effect and presented a better solution than previous methods for a number of common image datasets. The hidden data effect is more likely to appear in more complex data problems, e.g., we see more of its impact in CIFAR than in Fashion-MNIST. The hidden data effect is also likely to appear when there is a drift in data, making classification more difficult. 
\comment{For example, early papers on spam \cite{spam} described specific characteristics that enabled spam to be recognized. It is unlikely that more sophisticated spammers still provide those characteristics, but some less sophisticated ones may. It is expensive to label messages as spam or not spam, and it would be beneficial to be able to use an old set of labels to train a system to handle new messages. Our \verb"VAEGAN" algorithm allows exactly that.} We showed that using a DIRep and DDRep trained with both a variational autoencoder and a discriminator makes a good base (DIRep) for a classifier, when we add pressure for the DDRep to be small. It would be interesting to test our method in these more complex problems.

%If you have a small number of labels for a domain, you can look over elements in that domain and find elements that are almost certain to have a label.  But they may have some characteristics that are a bit different than labelled examples.  You can use those characteristics to help you label more and more examples.  This can work even if you merely have a likelihood that some examples will have certain labels.  A number of papers have used \verb"DANN" like techniques to give an initial likelihood that certain examples will have certain labels and then they iterate learning common characteristics of elements likely to have those labels~\cite{chen2020adversarial}~\cite{zou2018unsupervised}.  

In the case where there is a limited number of labeled samples, pseudo labelling is a very powerful technique that progressively adds more statistically likely labels (pseudo-labels) to the data ~\cite{chen2020adversarial,zou2018unsupervised}. However, it depends on the accuracy of the initial estimate.  If our initial estimate of the target label based on \verb"VAEGAN" is better than other algorithms then it is reasonable to expect that \verb"VAEGAN" could benefit from pseudo-labeling techniques. Indeed,   combining pseudo-labeling techniques and \verb"VAEGAN" provides a promising direction for future work. %could improve our results even further. We have not carefully compared our work to pseudo-labels believing that the pseudo-labelling based techniques are improving and we hope they will be able to be built on our foundation.%And we've shown that our method more than equals all previous approaches.

%\todo{This list moved to the end so we can get a better idea of lines used. Need to discuss:  1) title change? (Done) 2)which VAEGAN is used in experiments (Done) 3) where we say we did not reproduce the DSN results? Done 4) caption for figs 3 Mark(done)  and 4  Adrian tweak (Done) 5) length and what goes in supplementary material  6) conclusion/discussion Adrian7)Bit strange that we never mention DDRep in Introduction! Mark 8)anything else?}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\noindent
{\bf Acknowledgments.} This research has been funded by NSF under grants 2134667 ``Privacy-Preserving Tiny Machine Learning Edge Analytics to Enable AI-Commons for Secure Manufacturing'' and 2112471 ``AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE).'' This research has also been partially sponsored by the U.S. Army Research Laboratory and the U.K. Ministry of Defence under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry of Defence or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.

\bibliography{ijcai23}
\bibliographystyle{plainnat}


% \section{Supplementary Material}

% Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.

% \section*{References}

% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section*{Appendices}

This appendices supply some additional details.%\footnote{Our code is available at: \\ \footurl}
\section{More Details on Loss Functions}
\label{apploss}

We provide the details of all the loss functions mentioned in Section \ref{Detailed Design} of the main paper. Recall that the data is given by $(x,l,d)$ where $x$ is the input with $x^s$ and $x^t$ representing the source and target data, respectively. $l$ is the label of the sample, and $d$ is the domain identity. 
%\begin{itemize}

In unsupervised domain adaptation, the classification loss applies only to the source domain and it is defined as follows:
% \begin{equation}
% \mathcal{L}_c= 
%  - \mathbb{E}_{(x_s, y_s) \sim \mathbf{(X_s, Y_s)}} \sum_{k=1}^{K} y_s log[C(G({x_s}))]_k
% \end{equation}%
\begin{equation}
\mathcal{L}_c= 
 -  \sum_{i=1}^{N_s} {l}_i^s \cdot log \hat{l}_i^s
\end{equation}%
where $N_s$ represents the number of samples from the source domain, $l_i^s$ is the one-hot encoding of the label for the source input $x_i^s$ and $\hat{l}_i^s$ is the softmax output of $C(G(x_i^s))$. 

The discriminator loss  trains the discriminator to predict whether the DIRep is generated from the source or the target domain. $N_t$ represents the number of samples from target domain and $\hat{d}_i$ is the output of $D(G(x_i))$.
% \begin{equation}
% \resizebox{.91\linewidth}{!}{$\mathcal{L}_d= 
% -\mathbb{E}_{x_s \sim \mathbf{X_s}} log D(G(x_s)) - \mathbb{E}_{x_t \sim \mathbf{X_t}} log(1- D(G(x_t)))$}
% \end{equation}%
\begin{equation}
\mathcal{L}_d= 
 -  \sum_{i=1}^{N_s + N_t} \left\{ {d}_ilog \hat{d}_i + (1-{d}_i)log (1-\hat{{d}}_i) \right\} 
\end{equation}%

The generator loss is the GAN loss with inverted domain truth labels:
% \begin{equation}
% \mathcal{L}_g= 
%  - \mathbb{E}_{x_t \sim \mathbf{X_t}} log(D(G(x_t)))
% \end{equation}%
\begin{equation}
\mathcal{L}_g= 
 -  \sum_{i=1}^{N_s + N_t} \left\{ (1-{d}_i)log \hat{d}_i + d_ilog (1-\hat{{d}}_i) \right\} 
\end{equation}%




For the reconstruction loss, we use the standard mean squared error loss calculated from both domains:
% \begin{align}
%     \mathcal{L}_r = &\ \mathbb{E}_{x_s \sim \mathbf{X_s}} ||x_s - F(G(x_s), E(x_s))||_2^2  \nonumber \\
%     +   &\  \mathbb{E}_{x_t \sim \mathbf{X_t}} ||x_t - F(G(x_t), E(x_t))||_2^2
% \end{align}%
\begin{align}
    \mathcal{L}_r = &\ \sum_{i}^{N_s}||x_i^s - \hat{x}_i^s||_2^2 + \sum_{i}^{N_t}||x_i^t - \hat{x}_i^t||_2^2 
\end{align}%
where $\hat{x}_i^s = F(G(x_i^s), E(x_i^s))$ and $\hat{x}_i^t = F(G(x_i^t), E(x_i^t))$

Finally, the KL-divergence loss measures the distance between the distribution of DDRep which comes from a Gaussian  with mean $\mathbb{E}(DDRep)$ and variance $\mathbb{V}(DDRep)$ and the standard normal distribution. 
\begin{align}
    \mathcal{L}_{kl} = & D_{KL}(Pr(DDRep)\parallel\mathcal{N}(0,I) )\nonumber \\
    = & -\frac{1}{2}(1 + log[\mathbb{V}(DDRep)]  
    -  \mathbb{V}(DDrep) - \mathbb{E}(DDRep)^2)
\end{align}%




\section{Experiment Details for Fashion-MNIST}\label{appfm}



\subsection{Network architecture} 
All the methods are trained using the Adam optimizer with the learning rate of $2e-4$ for $10,000$ iterations. We use batches of $128$ samples from each domain for a total of $256$ samples. When training with our model (\verb"VAEGAN"), the label prediction pipeline (generator and classifier) has eight fully connected layers (\textsc{fc1, \ldots, fc7, fc\_out}). The number of neurons in \textsc{fc1-4} is 100 for each layer. \textsc{fc5} is a $100$-unit layer that generates DIRep, followed by two $400$-unit layers (\textsc{fc6-7}). \textsc{fc\_out} is the output layer for label prediction. The discriminator and decoder each have four layers with $400$ hidden units and followed by the domain prediction layer and reconstruction layer, respectively. The encoder has two layers with $400$ units, followed by 1-unit $z$\_mean, 1-unit $z$\_variance, and 1-unit sampling layer. Each of the $400$-unit layers uses a ReLU activation function. 

All the other models have the same architecture as \verb"VAEGAN" when applicable. For the \verb"GAN-based" approach and \verb"DANN", we turn off the decoder and corresponding losses. For the \verb"DSN", we keep the same network architecture for common networks and use $\mathcal{L}_g$  for the similarity loss. Furthermore, we implement the shared and private encoders with same shape output vectors \cite{bousmalis2016domain}.

\subsection{Hyperparameters}\label{apphyper}
As suggested in previous work~\cite{ganin2016domain}, the coefficient of the loss, which encourages domain invariant representation, should be initialized as $0$ and changed to $1$. We use the following schedule for the coefficient of $\mathcal{L}_g$ in all the experiments where $t$ is the training iteration:
\begin{align}
    \lambda = \frac{2}{1 + exp(-t)}-1
\end{align}%
The increasing coefficient allows the discriminator to be less sensitive to noisy signals at the early stages of the training procedure. For other hyperparameters, we used $\beta =1, \gamma =\mu=1$ (the hyperparameters were not tuned using validation samples).  

We closely follow the  setup of weights of the loss functions used in the \verb"DSN" paper~\cite{bousmalis2016domain} and \verb"DANN" paper~\cite{ganin2016domain}. To boost the performance of DSN, we set the coefficient of $\mathcal{L}_{\mathit{recon}}$ to $0.15$ and the coefficient of $\mathcal{L}_{\mathit{diff}}$ to $0.05$, tuned parameter values determined by ~\cite{bousmalis2016domain} using a validation set of target labels.

\subsection{Results and analysis.}



%We now discuss the experiment results. 
Table~\ref{appfmnist} summarizes the mean classification accuracy of different approaches for three cheating scenarios. In the no cheating scenario, we use the original Fashion-MNIST as source and flip the Fashion-MNIST for the target. We report the z-score of the comparison of the mean classification accuracy of our method with the mean classification accuracy of other methods over five independent runs (see Table \ref{appz test FM}). The higher the z-score, the more statistical confidence we should have that our method outperforms the other methods. A z-score of $2.33$ corresponds to $99\%$ confidence that our method is superior, assuming that the accuracy over different runs will follow a  Gaussian distribution. 




In the no cheating scenario, \verb"VAEGAN" outperforms  \verb"GAN-based" and \verb"DANN" and matches the result of \verb"DSN". %as the classification task is relatively easy on the target domain. A large amount of shape and texture information is still preserved when we flip the images. 
The performance of \verb"GAN-based" and \verb"DANN" results in a $5\%$ accuracy drop for the shift cheating and $10\%$ drop for the random cheating. This validates our concern with the hidden data effect. The source cheating bits can be picked up in the DIRep as they represent an easy solution for the classifier that is trained only with source samples. If that is the case, then the cheating generator would perform poorly for the target domain, which has different cheating bits. Our method has only $1\%$ and $5\%$ accuracy drop respectively and is less vulnerable to the hidden data effect. As a reconstruction-based method, \verb"DSN" performs better in the presence of cheating bits. In the shift and random cheating, our approach significantly outperforms \verb"DSN" with a z-score of $2.60$ and $3.18$ respectively, which shows the correctness of our intuition that penalizing the size of DDRep can result in transferring as much information as possible to the DIRep. In the explicit DDRep algorithm, the DDRep is minimal as it only onctains the domain label. Given  a richer DIRep, our method leads to a classifier based on the invariant features of the images, which improves its performance on the target data.
 %As a result, more information from the images themselves instead of the cheating bits is pushed into the DIRep to reduce the reconstruction loss. Given  a richer DIRep, our method leads to a classifier based on the invariant features of the images, which improves its performance on the target data.


\begin{table}%[htbp]
\captionsetup{font=scriptsize}
\parbox[t]{.49\linewidth}{
\centering
  \caption{Mean classification accuracy (\%) of different unsupervised DA approaches for the constructed Fashion-MNIST datasets.}
  \vspace*{0.1cm} 
  \resizebox{0.45\textwidth}{!}{%
     \begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
Source-only & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}20.0}    & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}11.7}  & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}13.8}   \\
GAN-based   &    \multicolumn{1}{r}{64.7}                      &    \multicolumn{1}{r}{58.2}                      &      \multicolumn{1}{r}{54.8}                        \\
DANN        & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}63.7} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}58.0} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}53.6} \\
DSN        &     \multicolumn{1}{r}{66.8 }                    &       \multicolumn{1}{r}{63.6 }                  &       \multicolumn{1}{r}{57.1}                   \\
VAEGAN      & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{66.9}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{65.7}} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{61.6}} \\
Target-only &  \multicolumn{1}{r}{88.1}                          & \multicolumn{1}{r}{99.8}                            & \multicolumn{1}{r}{87.9} \\
\hline

\end{tabular}
  }
  \label{appfmnist}%
}
\hfill
\parbox[t]{.5\linewidth}{
 \centering
  \caption{z-test score value comparing VAEGAN to other models for constructed Fashion-MNIST. z$>$2.3 means the probability of VAEGAN being no better is $\leq$0.01.}
  \vspace*{0.1cm} 
  \resizebox{0.45\textwidth}{!}{%
 \begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}No\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Shift\\ cheating\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Random\\ cheating\end{tabular} \\ \hline
GAN-based & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}1.55}    & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.28}  & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.68}   \\
DANN   &    \multicolumn{1}{r}{2.26}                      &    \multicolumn{1}{r}{4.17}                      &      \multicolumn{1}{r}{4.33}                        \\
DSN        & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}0.16} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.60} & \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.18}  \\
\hline

\end{tabular}
  }
  \label{appz test FM}%
}
\end{table}



\section{Experiment Details for CIFAR-10}\label{appcifar}
\subsection{Network architecture and training procedure} 

When training with our approach, we implement the network components as deep residual neural networks (ResNets) with short-cut connections \cite{he2016deep}. ResNets are easier to optimize, and sometimes gain accuracy from increased depth. For our approach, we implemented the full-fledged \verb"VAEGAN" and we added the domain label to the DDRep generated by the encoder. The architecture is shown in Figure \ref{resnet}. The label prediction pipeline is adopted from the ResNet 20 for CIFAR-10 in ~\cite{he2016deep}. For the generator, the first layer is $3\times3$ convolutions. Then we use a stack of $6$ layers with $3\times3$ convolutions on the feature maps of size $32$. The numbers of filters are $16$. The architecture of the classifier consists of a stack of $6\times2$ layers with $3\times3$ convolutions on the feature maps of sizes $\{16, 8\}$ respectively. To maintain the network complexity, the number of filters are $\{32, 64\}$. The classifier ends with a global average pooling,  and a fully-connected layer with softmax. 

For the discriminator, the network inputs are $32\times32\times16$ domain invariant features. The first layer is $3\times3$ convolutions. Then we use a stack of $6\times3$ layers with $3\times3$ convolutions on the feature maps of sizes $32$, $16$, and $8$ respectively, with $6$ layers for each feature map size. The numbers of filters is $\{16, 32, 64\}$ respectively. The discriminator ends with a global average pooling, a 2-way fully-connected layer, and softmax. 

The encoder has 4 convolutional layers: three $3\times3$ filters, two $3\times3$ filters, two $3\times3$ filters ($z$ mean) and two $3\times3$ filters ($z$ variance) respectively. A sampling layer is also implemented which outputs the DDRep from the latent distribution $z$. The decoder learns to reconstruct an input image by using its DIRep, DDRep and domain bit together. Hence, the inputs of the decoder are $32\times32\times19$ concatenated representations. %We keep the domain bits since we want to check the sample reconstruction when the domain bit is flipped.
The configuration of the decoder is the inverse of that of the generator. 

We implemented the same ResNet-based architecture for all other approaches (when applicable). We use a weight decay of $0.0001$ and adopt the BN~\cite{ioffe2015batch} for all the experiments. The hyperparameters are the same as the ones in Section \ref{apphyper}.

\subsection{Results and analysis}



\begin{table*}[t]
\centering
\caption{Averaged classification accuracy (\%) of different unsupervised DA approaches for constructed CIFAR-10 dataset with a spectrum of bias.}\label{appcifar10}
\begin{tabular}{lllllllll}
\toprule
Model     & \multicolumn{1}{r}{0\% bias}  & \multicolumn{1}{r}{20\% bias }   & \multicolumn{1}{r}{40\% bias }   & \multicolumn{1}{r}{60\% bias}       & \multicolumn{1}{r}{80\% bias}       & \multicolumn{1}{r}{90\% bias}                    & \multicolumn{1}{r}{100\% bias}                   \\ \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }
& \multicolumn{1}{r}{\cellcolor[HTML] {E0DDDD}10.0} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0}    
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }    
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}10.0 }\\

GAN-based  
& \multicolumn{1}{r}{63.0} 
& \multicolumn{1}{r}{62.5}
& \multicolumn{1}{r}{61.4}
& \multicolumn{1}{r}{56.9}
& \multicolumn{1}{r}{53.2}
& \multicolumn{1}{r}{44.5}
& \multicolumn{1}{r}{30.1} \\

DANN  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 62.7}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 62.0} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 61.0} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 56.5} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 52.2} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333} 42.9} }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}{\color[HTML]{333333}29.1}} \\
DSN      
& \multicolumn{1}{r}{68.7 }
& \multicolumn{1}{r}{67.9  } 
& \multicolumn{1}{r}{67.3  }     
& \multicolumn{1}{r}{67.5   }  
& \multicolumn{1}{r}{64.5 }     
& \multicolumn{1}{r}{61.7 }   
& \multicolumn{1}{r}{32.2  }           \\
VAE-GAN   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{70.4}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.8} }      
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.8}}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{69.7 }}  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{68.2 }}
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{64.1} } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{34.2 }}   \\
Target-only     
& \multicolumn{1}{r}{   78.9  }
& \multicolumn{1}{r}{78.9  }
&  \multicolumn{1}{r}{ 78.9   }
&   \multicolumn{1}{r}{ 78.9  } 
&  \multicolumn{1}{r}{78.9  }
&  \multicolumn{1}{r}{ 78.9 }   
& \multicolumn{1}{r}{78.9}  \\ 
\hline
\end{tabular}
\end{table*}



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/resnet.pdf}
%\vspace*{-0.4cm}  
\caption{CIFAR10 architecture; inspired by the classical ResNet-20~\cite{he2016deep}}
%\vspace*{-0.8cm}
\label{resnet} 
\end{figure}


\begin{table*}[t]
\centering
\caption{z-test score value comparing VAEGAN to other models for constructed CIFAR-10. z$>$2.3 means the probability of VAEGAN being no better than the other models is $\leq$0.01.}\label{appcifar10_z_score} % Statistical z-test score indicating the averaged classification accuracy of our approach on Cifar10 is higher with a significant level of 0.01 \todo{chose which caption we prefer}\todo{?}}
\begin{tabular}{llllllll}
\hline
Model     & \multicolumn{1}{r}{0\% bias}  & \multicolumn{1}{r}{20\% bias }   & \multicolumn{1}{r}{40\% bias }   & \multicolumn{1}{r}{60\% bias}       & \multicolumn{1}{r}{80\% bias}       & \multicolumn{1}{r}{90\% bias}                    & \multicolumn{1}{r}{100\% bias}                  \\ \hline
GAN-based
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}5.23 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.20 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}5.93 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}12.8 } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}11.31 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}7.20} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}4.58 } \\
DANN     
& \multicolumn{1}{r}{5.44 }      
& \multicolumn{1}{r}{3.42 }          
& \multicolumn{1}{r}{6.22 }      
& \multicolumn{1}{r}{13.2  }  
& \multicolumn{1}{r}{12.02  }   
& \multicolumn{1}{r}{7.79 }          
& \multicolumn{1}{r}{5.70  }                        \\
DSN     
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.68 } 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.00 }
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.95} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.47 }  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}7.43}  
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}3.78} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}2.23 }\\ \hline
\end{tabular}
\end{table*}


We report the mean accuracy of different unsupervised DA methods and our approach on the target test set in Table \ref{appcifar10}. The z-scores of comparing our method with other methods are shown in Table \ref{appcifar10_z_score}. 

For all the DA tasks with varying biases, we observe that our approach outperforms the other approaches in terms of accuracy in the target set. This improvement is most pronounced when the source set has $60\%$ and $80\%$ bias levels, which means that over half of the source data has a spurious correlation between their color planes and labels. The poor performance of the \verb"GAN-based" and \verb"DANN" approaches is another example where the generator in these approaches learns a DIRep that depends on the spurious correlation. This false representation leads to an issue similar to over-fitting where the model performs well on the source data, but does not generalize well on the target data in which the same correlation does not exist. In the \verb"DSN" approach, the shared representation contains some domain-independent information other than the cheating clues which helps classification in the target domain. However, it does not directly address the problem of making the domain invariant representation richer for classification. %\todo{this largely recaps what we said in the related work section. that's not a bad thing, but i want people to think about whether it's too redundant. mw}
We postulate that the inferior performance of the \verb"DSN" approach may stem from the way its shared and private representations are trained. The difference loss in \verb"DSN" only encourages orthogonality between the shared and the private representations in a linear way, which can be less effective in separating domain dependent and independent information for difficult DA scenarios. 





\section{SVHN, MNIST, MNIST-M and Synth Digits}\label{digitspairs}


\begin{figure}[t]
\center
\includegraphics[width=\linewidth]{images/digits.pdf}
%\vspace*{-0.4cm}  
\caption{Example images from four domain adaptation benchmark datasets for three scenarios.}
%\vspace*{-0.8cm}
\label{digits} 
\end{figure}


\begin{table}[t]
\centering
\caption{Mean classification accuracy (\%) of different unsupervised domain adaptation approaches for widely used benchmarks. The results are cited from each study. When we attempted to replicate the DSN results we only achieved 80.0\% not 83.2\% and that 80.0\% may be more comparable to the 81.0\% for VAEGAN.}
\vspace*{0.2cm} 
\begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}MNINST to \\MNIST-M\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Synth Digits to\\SVHN\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}SVHN to\\MNIST\end{tabular}  \\
  \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}56.6}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}86.7} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}59.2}
\\
DANN       
& \multicolumn{1}{r}{76.6} 
& \multicolumn{1}{r}{91.0} 
& \multicolumn{1}{r}{73.8}
\\
DSN       
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{83.2}}%\tablefootnote{In the MNIST and MNIST-M shift, we used the code for DNS from their Github implementation but our measure of DNS is 80\% which is  less than the published results and the result achieved by VAEGAN.}} }                  
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}\textbf{91.2} }             
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}82.7}   
   \\
VAEGAN     
& \multicolumn{1}{r}{81.0} 
& \multicolumn{1}{r}{91.1} 
& \multicolumn{1}{r}{\textbf{85.8}} 
\\
Target-only
&  \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}98.7}                 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}92.4}                          
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}99.5} 
\\
\hline
\end{tabular}
\label{appmnist}
\end{table}

We evaluate the empirical performance of \verb"VAEGAN" on four widely used domain adaptation benchmarks: MNIST~\cite{lecun1998gradient}, MNIST-M~\cite{ganin2016domain}, Street View House Number~\cite{netzer2011reading} and synthetic digits~\cite{ganin2016domain}. We use three unsupervised domain adaptation pairs: 1) MNIST $\rightarrow$ MNIST-M, 2) Synth Digits $\rightarrow$ SVHN, and 3) SVHN $\rightarrow$ MNIST. Example images from all four datasets are provided in Figure \ref{digits}.  We implement our CNN topology based on the ones used in \cite{bousmalis2016domain} and \cite{ganin2016domain}. We used Adam with the learning rate of $0.0002$ for $25,000$ iterations. The batch size is $128$ for each domain. We did not use validation samples to tune hyperparameters.  To make fair comparisons, we follow the instructions in ~\cite{bousmalis2016domain} and activate the $\mathcal{L}_g$ after 20,000 steps of training.  For other hyperparameters, we used $\beta =1$, $\gamma =1$, and $\mu=1$. 

Note, that none of these data sets contain information correlated with the label that is available in the source but not the target, because of the way they were constructed.  Our techniques are particularly applicable when there is a superfluous or shortcut means to classify in the source, so we don't have an advantage we have in many more natural settings.  Never the less our techniques are at least competitive.

\paragraph{MNIST to MNIST-M.} We use the MNIST dataset as the source domain, and a variation of MNIST called MNIST-M as the target. MNIST-M was created by blending digits from the original MNIST set over patches randomly extracted from color photos from BSDS500~\cite{arbelaez2010contour}. 

\paragraph{Synthetic Digits to SVHN.} This scenario is widely used to demonstrate the effectiveness of the algorithm when training on synthetic data and testing on real data. We use synthetic digits as the source and Street-View House Number data set SVHN as the target. 

\paragraph{SVHN to MNIST.} In this experiment, we further increase  the gap between the two domains.  The digit shapes in SVHN are quite distinct from those handwritten digits in MNIST. Furthermore, SVHN contains significant image noise, such as multiple digits in one image and blurry background. 

\paragraph{Results} Table \ref{appmnist} shows the results on the digits datasets. We cited the results from each study to make a fair comparison. \verb"GAN-based" method is omitted from the table since the paper does not test on digit datasets. In MNIST $\rightarrow$ MNIST-M and Synth Digits $\rightarrow$ SVHN,  we were not able to achieve the published results for \verb "DSN".  The previously published results for \verb "DSN" were based on using hyperparameters chosen after using 1000 labeled samples from the target.  Our parameters were chosen without use of target labels. In summary, \verb"VAEGAN" outperforms \verb"DSN" on the difficult SVHN  $\rightarrow$  MNIST shift and achieves comparable results on the Synth Digits  $\rightarrow$ SVHN shift.

\section {Office dataset}\label{officepairs}

\begin{table}[t]
\centering 
\caption{Mean classification accuracy (\%) of different unsupervised domain adaptation approaches for the office dataset. We have cited the original results (if applicable) and in
parentheses we show the results reported in~\cite{chen2020adversarial} with data augmentation.}
\vspace*{0.2cm} 
\begin{tabular}{llll}
\hline
Model &
  \begin{tabular}[c]{@{}r@{}}$A \rightarrow W$\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}$D \rightarrow A$\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}$W \rightarrow A$\end{tabular}  \\
  \hline
Source-only
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}64.2 (68.4)}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD} - $\ $  (62.5)} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD} - $\ $ (60.7)}
\\
DANN       
& \multicolumn{1}{r}{73.0 (82.0)} 
& \multicolumn{1}{r}{- $\ $ (68.2)} 
& \multicolumn{1}{r}{- $\ $ (67.4)}
\\
ADDA
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}75.1 (\textbf{86.2})}   
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}- $\ $ (69.5)} 
& \multicolumn{1}{r}{\cellcolor[HTML]{E0DDDD}- $\ $ (68.9)}
\\
VAEGAN     
& \multicolumn{1}{l}{\textbf{82.0}} 
& \multicolumn{1}{l}{\textbf{72.9}} 
& \multicolumn{1}{l}{\textbf{72.0}} 
\\
\hline
\end{tabular}
\label{appoffice}
\end{table}


Our method was evaluated on the Office dataset, which comprises three distinct domains: Amazon, DSLR, and Webcam. Unlike larger datasets, the Office dataset is relatively small, containing only $2817$ labeled images across $31$ different categories in the largest domain. Due to the limited data availability, we opted to utilize the ResNet-50 architecture pretrained on the ImageNet dataset, following a common approach in recent domain adaptation studies ~\cite{tzeng2017adversarial, chen2020adversarial}. This choice allowed us to leverage the knowledge gained from ImageNet's large-scale dataset and apply it to our specific domain adaptation task. By employing the same network architecture as ~\cite{tzeng2017adversarial}, we ensured a fair and direct comparison of our method's performance, particularly in the challenging D $\rightarrow$ A and W $\rightarrow$ A scenarios where the source training data is limited (see Table \ref{appoffice}). We include the original results from the \verb "DANN" and \verb "ADDA" papers as well as the new results from~\cite{chen2020adversarial} with training data augmentation. We follow the training protocol in \verb "DANN" and \verb "ADDA" when training \verb "VAEGAN" which does not use any data augmentation techniques.






\end{document}