\section{Algorithm}

To empirically estimate the predictive heterogeneity in Definition \ref{def:empirical_predictive_heterogeneity}, we derive the Information Maximization (IM) algorithm from the formal definition in Equation \ref{equ:appendix-formal-empirical} to infer the distribution of $\mathcal{E}$ that maximizes the empirical predictive heterogeneity $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V}(X\rightarrow Y; \mathcal D)$.
% Actually, our objective function in Equation \label{equ:bi-level} is derived from the formal definition of $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V}(X\rightarrow Y; \mathcal D)$ in Equation \ref{equ:appendix-formal-empirical}.




\textbf{Objective Function.}\quad Given dataset $\mathcal D=\{X_N,Y_N\} = \{(x_i,y_i)\}_{i=1}^N$, denote $\text{supp}(\mathcal{E})=\{e_1, \dots, e_K\}$, we parameterize the distribution of $\mathcal{E}|(X_N,Y_N)$ with weight matrix $W\in\mathcal{W}_K$, where $K$ is the pre-defined number of environments and $\mathcal{W}_K=\{W: W\in\mathbb{R}_+^{N\times K}\text{ and }W\textbf{1}_K=\textbf{1}_N\}$ is the allowed weight space.
Each element $w_{ij}$ in $W$ represents $P(\mathcal{E}=e_j|x_i,y_i)$ (the probability of the $i$-th data point belonging to the $j$-th sub-population).
% The goal of our information maximization algorithm is to learn the best sub-population assignment matrix $W$ that maximizes the empirical predictive heterogeneity.
For a predictive family $\mathcal V$, the solution to the supremum problem in the Definition \ref{def:empirical_predictive_heterogeneity} is equivalent to the following objective function:
\begin{small}
\begin{equation}
\label{equ:bi-level}
\begin{aligned}
	\min\limits_{W\in\mathcal{W}_K} &\mathcal{R}_{\mathcal V}(W,\theta_1^*(W),\dots,\theta^*_K(W))=\left\{\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^K w_{ij}\ell_{\mathcal V}(f_{\theta_j^*}(x_i),y_i) + U_{\mathcal V}(W,Y_N)
	\right\},\\
	&\text{s.t.}\quad  \theta^*_j(W) \in \arg\min_{\theta} \left\{\mathcal{L}_{\mathcal V}^j(W,\theta)=\sum_{i=1}^Nw_{ij}\ell_{\mathcal V}(f_{\theta}(x_i),y_i)\right\},\quad \text{for }j=1,\dots,K,
\end{aligned}		
\end{equation}
\end{small}
where $f_{\theta}:\mathcal{X}\rightarrow \mathcal{Y}$ denotes a predicting function parameterized by $\theta$, $\ell_{\mathcal{V}}(\cdot,\cdot):\mathcal{Y}\times\mathcal{Y}\rightarrow \mathbb{R}$ represents a loss function and $U_{\mathcal V}(W,Y_N)$ is a regularizer.
Specifically, $f_\theta$, $\ell_{\mathcal V}$ and $U_{\mathcal V}$ are determined by the predictive family $\mathcal{V}$. 
% \xrz{Since $U(W,Y_N)$ is determined by the predictive family, it would be better to write it as $U_{\mathcal{V}}(W,Y_N)$ and $\mathcal{R}_{\mathcal{V}}(W,\theta_1^*(W),\dots,\theta^*_K(W))$?}
Here we provide implementations for two typical and general machine learning tasks, regression and classification.

\subsection{Regression}
For the \emph{regression task}, the predictive family is typically modeled as:
\begin{small}
\begin{equation}
	\mathcal{V}_1 = \{g: g[x]=\mathcal{N}(f_{\theta}(x), \sigma^2), f\text{ is the predicting function and }\theta\text{ is learnable, }\sigma \text{ is a constant}\}.
\end{equation}	
\end{small}
The corresponding loss function is $\ell_{\mathcal{V}_1}(f_\theta(X),Y)=(f_\theta(X)-Y)^2$, and $U_{\mathcal{V}_1}(W,Y_N)$ becomes 
\begin{small}
\begin{equation}
\label{equ:regularizer-regression}
	U_{\mathcal{V}_1}(W,Y_N) = \text{Var}_{j\in [K]}(\overline{Y_N^j})= \sum_{j=1}^K\left(\sum_{i=1}^N w_{ij}y_i\right)^2\frac{1}{N\sum_{i=1}^Nw_{ij}}-\left(\frac{1}{N}\sum_{i=1}^Ny_i\right)^2
\end{equation}	
\end{small}
where $\overline{Y^j_N}$ denotes the mean value of the label $Y$ given $\mathcal{E}=e_j$ and $U(W,Y_N)$ calculates the variance of $\overline{Y^j_N}$ among sub-populations $e_1\sim e_K$.


\subsection{Classification}
For the \emph{classification task}, the predictive family is typically modeled as:
\begin{small}
\begin{equation}
	\mathcal{V}_2 = \{g: g[x]=f_\theta(x)\in\Delta_c, f\text{ is the classification model and }\theta\text{ is learnable}\},
\end{equation}	
\end{small}
where $c$ is the class number and $\Delta_c$ denotes the $c$-dimensional simplex.
Here each model in the predictive family $\mathcal V_2$ outputs a discrete distribution in the form of a $c$-dimensional simplex.
In this case, the corresponding loss function $\ell_{\mathcal V_2}(\cdot,\cdot)$ is the cross entropy loss and the regularizer becomes $U_{\mathcal V_2}(W,Y_N) = -\sum_{j=1}^K \frac{1}{N}(\sum_{i=1}^Nw_{ij}) H(Y_N^j)$, where $H(Y_N^j)$ is the entropy of $Y$ given $\mathcal{E}=e_j$.

% \xrz{Why choose $\mathcal{V}_1$ and $\mathcal{V}_2$ in this two cases?}

\subsection{Optimization.}
The bi-level optimization in Equation \ref{equ:bi-level} can be solved by performing projected gradient descent w.r.t. $W$.
The gradient of $W$ can be calculated by: (we omit the subscript $\mathcal{V}$ here)
\begin{small}
\begin{align}
	\nabla_W\mathcal R &=\nabla_W U + \left[\ell(f_{\theta_j}(x_i),y_i)\right]_{i,j}^{N\times K} + \sum_{j=1}^K \boxed{\nabla_{\theta_j}\mathcal{R}|_{\theta^*_j}\nabla_W\theta_j^*},\\
	\label{equ:tstep}
	\text{where }\boxed{\left.\nabla_{\theta_j}\mathcal{R}\right\vert_{\theta^*_j}\nabla_W\theta_j^*} & \approx \left.\nabla_{\theta_j}\mathcal{R}\right\vert_{\theta_j^t}\sum_{h\leq t}\left [ \prod_{k<h}(I - \left.\frac{\partial^2\mathcal{L}^j}{\partial\theta_j \partial \theta_j^{\mathrm{T}}}\right\vert_{\theta_j^{t-k-1}})\right ]\left.\frac{\partial^2\mathcal{L}^j}{\partial\theta_j\partial W^{\mathrm{T}}}\right\vert_{\theta_j^{t-h-1}} \\
	\label{equ:1step}
	&\approx  \left.\nabla_{\theta_j}\mathcal{R}\right\vert_{\theta_j^t}\left.\frac{\partial^2\mathcal{L}^j}{\partial \theta_j\partial W^{\mathrm{T}}}\right \vert_{\theta_j^{t-1}}\quad\quad\text{  , for }j=1,\dots,K.
\end{align}
\end{small}
where $\ell(f_{\theta_j}(x_i),y_i)]_{i,j}^{N\times K}$ is an $N\times K$ matrix in which the $(i,j)$-th element is $\ell(f_{\theta_j}(x_i),y_i)$.
Here Equation \ref{equ:tstep} approximates $\theta_j^*$ by $\theta_j^t$ from $t$ steps of inner loop gradient descent and Equation \ref{equ:1step} takes $t=1$ and performs \emph{1-step truncated backpropagation} \citep{shaban2019truncated,zhou2022model}.
Our information maximization algorithm updates $W$ by projected gradient descent as:
\begin{small}
\begin{equation}
	W \leftarrow \text{Proj}_{\mathcal{W}_K}\left(W-\eta\nabla_W\mathcal R\right),\quad \eta\text{ is the learning rate of }W.
\end{equation}	
\end{small}

Then we prove that minimizing Equation \ref{equ:bi-level} exactly finds the supremum w.r.t. $\mathcal{E}$ in the Definition \ref{def:empirical_predictive_heterogeneity} (formal) of the empirical predictive heterogeneity.
%And we also demonstrate its relationship with the expectation maximization (EM) algorithm.

\begin{theorem}[Justification of the IM Algorithm]
\label{theorem:IM}
	For the regression task with predictive family $\mathcal{V}_1$ and classification task with $\mathcal{V}_2$, the optimization of Equation \ref{equ:bi-level} is equivalent to the supremum problem of the empirical predictive heterogeneity $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V_1}(X\rightarrow Y; \mathcal D)$, $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V_2}(X\rightarrow Y; \mathcal D)$ respectively in Equation \ref{equ:appendix-formal-empirical}  with the pre-defined environment number $K$ (i.e. $|\text{supp}(\mathcal E)|=K$).
	Proofs can be found at Appendix \ref{proof: IM}.
\end{theorem}


\begin{remark}[Difference from Expectation Maximization]
	The expectation maximization (EM) algorithm is to infer latent variables of a statistic model to achieve the \textbf{maximum likelihood}.
	Our proposed information maximization (IM) algorithm is to infer the latent variables $W$ which brings the \textbf{maximal predictive heterogeneity} associated with the maximal information.
	Due to the regularizer $U_{\mathcal V}$ in our objective function, the EM algorithm cannot efficiently solve our problem, and therefore we adopt bi-level optimization techniques. 
\end{remark}

% \textbf{Approximation Accuracy.}\quad 
% To demonstrate the approximation accuracy of our IM algorithm, for the three toy cases in Section \ref{sec:linear}, we plot the estimated $\hat{\mathcal{H}}^u_{\mathcal V}$ of our algorithm as well as the precise values of $\mathcal{H}^u_{\mathcal V}$ in Figure \ref{fig:toy}.
% From the results, we can see that the approximated estimation of our IM algorithm stays closely to the precise values in all the three cases.


\subsection{Approximation Accuracy}
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./fig/toy.png}
    \caption{Numerical results of the toy examples in Section \ref{sec:linear}. The left sub-figure plots the estimated predictive heterogeneity under the setting of Theorem \ref{theorem: homogeneous}, the middle sub-figure plots the theoretical approximation, empirical approximation and our results under the setting of Theorem \ref{theorem: selection-bias}, and the right one is under the setting of Theorem \ref{theorem: omitted variable}.}
    \label{fig:appendix-toy}
\end{figure}

Here we provide some additional numerical results of our linear examples in Section \ref{sec:linear}.
In the left sub-figure of Figure \ref{fig:appendix-toy}, we plot the estimated predictive heterogeneity under the setting of Theorem \ref{theorem: homogeneous} where the analytical solution is equal to 0.
From the results, we can see that with the growing of sample size, the estimated value of our IM algorithm is approaching to 0 (note that the $y$-axis is $\ln(\text{estimated value})$).
In the middle sub-figure, for the setting in Theorem \ref{theorem: selection-bias}, we plot the theoretical approximation, empirical approximation (finite sample case) and the estimated value of the predictive heterogeneity under different ratios between the majority and the minority (which controls the $\text{Var}[r(\mathcal{E}^*)]$ in Equation \ref{equ:approximation1}).
And the right sub-figure plots the same values under the setting in Theorem \ref{theorem: omitted variable}.
From these two figures, we can see that (1) the empirical approximation under finite samples lies closely to the theoretical approximation, which is supported by our generalization bounds in Theorem \ref{theorem:pac}; (2) the estimated value of our IM algorithm is closely to the theoretical approximation,, which demonstrates the accuracy of our approximation algorithm in Equation \ref{equ:tstep} and \ref{equ:1step}. 
Also, as the ratio changes from $4:1$ to $1:1$, the data heterogeneity is increasing, and our predictive heterogeneity is also increasing, which is controlled by the term $\text{Var}(r(\mathcal{E}^*))$ in Equation \ref{equ:approximation1} and \ref{equ:approximation2}.






