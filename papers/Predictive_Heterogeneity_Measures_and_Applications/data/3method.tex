\section{Predictive Heterogeneity}
In this paper, from the machine learning perspective, we quantify the data heterogeneity that affects decision making, named Predictive Heterogeneity, which is easy to integrate with machine learning algorithms and could help analyze big data and build more rational algorithms.

\subsection{Interaction Heterogeneity}
To formally define the predictive heterogeneity, we begin with the formulation of the interaction heterogeneity.
The \emph{interaction heterogeneity} is defined as:
\begin{definition}[Interaction Heterogeneity]
	Let $X$, $Y$ be random variables taking values in $\mathcal X \times \mathcal Y$. Denote the set of random categorical variables as $\mathcal C$, and take its subset $\mathscr E \subseteq \mathcal C$.  Then $\mathscr E$ is an environment set iff there exists $\mathcal E \in \mathscr E$ such that $X, Y \perp \!\!\! \perp \mathcal E$. $\mathcal E \in \mathscr E$ is called an environment variable. The interaction heterogeneity between $X$ and $Y$ w.r.t. the environment set $\mathscr E$ is defined as:
	\begin{small}
	\begin{equation}
	\label{equ:predictive-heterogeneity}
		\mathcal{H}^\mathscr E(X,Y) = \sup_{\mathcal{E} \in \mathscr E}\mathbb{I}(Y;X|\mathcal{E})-\mathbb{I}(Y;X).
	\end{equation} 
	\end{small}
\end{definition}
\vskip -0.1in
% The condition for the environment set implies that 
% \xrz{I guess it would be better to change the description and define $\mathcal C$ more formally. I feel a little bit weird about the definition while I can not point it out 233. In addition, it would be better if we explain why we need a $\mathcal E \in \mathscr E$ such that $X, Y \perp \!\!\! \perp \mathcal E$.}

% \xrz{Let $X$, $Y$ be random variables taking values in $\mathcal X \times \mathcal Y$. An environment variable $\mathcal{E}$ is defined as a categorical random variable on the sample space $\mathcal{X} \times \mathcal{Y}$. Denote the set of all environment variables as $\mathcal C$. Furthermore, we define a subset $\mathscr E \in \mathcal C$ as an environment set iff there exists $\mathcal E \in \mathscr E$ such that $X, Y \perp \!\!\! \perp \mathcal E$.}
Each environment variable $\mathcal E$ represents a stochastic `partition' of $\mathcal X \times \mathcal Y$, and the condition for the environment set implies that there exists such a stochastic partition that the joint distribution of $X,Y$ could be preserved in each environment.
In information theory, $\mathbb{I}(Y;X|\mathcal{E})-\mathbb{I}(Y;X)$ is called the \emph{interaction information}, which measures the influence of the environment variable $\mathcal{E}$ on the amount of information shared between the target $Y$ and the covariate $X$.
And the \emph{interaction heterogeneity} defined in Equation \ref{equ:predictive-heterogeneity} quantifies the \emph{maximal} additional information that can be gained from involving or uncovering the environment variable $\mathcal{E}$.
Intuitively, large $\mathcal{H}^{\mathscr{E}}(X,Y)$ indicates that the predictive power from $X$ to $Y$ is enhanced by $\mathcal{E}$, which means that uncovering the latent sub-population associated with the environment partition $\mathcal{E}$ will benefit the $X\rightarrow Y$ prediction.



\subsection{Predictive Heterogeneity}
Based on the mutual information, the computation of the interaction heterogeneity is quite hard, since the standard mutual information is notoriously difficult to estimate especially in big data scenarios.
Also, even if the mutual information could be accurately estimated, the prediction model may not be able to make good use of it.

Inspired by \cite{DBLP:conf/iclr/XuZSSE20}, we raise the \emph{Predictive Heterogeneity}, which measures the interaction heterogeneity that can be captured under computational constraints and affects the prediction of models within the specified predictive family.
To begin with, we propose the \emph{Conditional Predictive $\mathcal V$-information}, which generalizes the predictive $\mathcal V$-information.

\begin{definition}[Conditional Predictive $\mathcal V$-information]
	Let $X, Y$ be two random variables taking values in $\mathcal{X}\times\mathcal{Y}$ and $\mathcal E$ be an environment variable. For a predictive family $\mathcal V$, the conditional predictive $\mathcal V$-information is defined as:
	\begin{equation}
		\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E}) = H_{\mathcal V}(Y|\emptyset,\mathcal E)-H_{\mathcal V}(Y|X,\mathcal E),
	\end{equation}	
	where $H_{\mathcal V}(Y|\emptyset,\mathcal E)$ and $H_{\mathcal V}(Y|X,\mathcal E)$ are defined as:
	\begin{align}
		H_{\mathcal V}(Y|X,\mathcal E) &= \mathbb E_{e \sim \mathcal E} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x,y\sim X,Y|\mathcal E=e}[-\log f[x](y)]\right]. \\
		H_{\mathcal V}(Y|\emptyset,\mathcal E) &= \mathbb E_{e \sim \mathcal E} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{y\sim Y | \mathcal E=e}[-\log f[\emptyset](y)] \right].
	\end{align}	
\end{definition}

Intuitively, the conditional predictive $\mathcal V$-information measures the weighted average of predictive $\mathcal V$-information among environments.
And here we are ready to formalize the predictive heterogeneity measure.


\begin{definition}[Predictive Heterogeneity]
	Let $X$, $Y$ be random variables taking values in $\mathcal X \times \mathcal Y$ and $\mathscr E$ be an environment set. For a predictive family $\mathcal V$, the predictive heterogeneity for the prediction $X \rightarrow Y$ with respect to $\mathscr E$ is defined as:
	\begin{equation}
	\label{equ:usable-predictive-heterogeneity-1}
		\mathcal{H}^\mathscr E_{\mathcal V}(X \rightarrow Y) = \sup_{\mathcal{E} \in \mathscr E}\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E})-\mathbb{I}_{\mathcal{V}}(X\rightarrow Y),
	\end{equation} 
	where $\mathbb{I}_{\mathcal{V}}(X\rightarrow Y)$ is the predictive $\mathcal V$-information following from Definition \ref{def:predictive_v_information}.
\end{definition}

Leveraging the predictive $\mathcal V$-information, the predictive heterogeneity defined in Equation \ref{equ:usable-predictive-heterogeneity-1} characterizes the \emph{maximal additional information} that \emph{can be used} by the prediction model when involving the environment variable $\mathcal E$. 
It restricts the prediction models in $\mathcal{V}$ and the explored additional information could benefit the prediction performance of the model $f\in\mathcal V$, for which it is named predictive heterogeneity. 
Next, we present some basic properties of the interaction heterogeneity and predictive heterogeneity.

\begin{proposition}[\textcolor{black}{Basic Properties of Predictive Heterogeneity}]
\label{proposition1}
Let $X$, $Y$ be random variables taking values in $\mathcal X \times \mathcal Y$,  $\mathcal V$ be a function family, and $\mathscr E$, $\mathscr E_1$, $\mathscr E_2$   be environment sets.

    \quad 1. \emph{Monotonicity}: If $\mathscr E_1 \subseteq \mathscr E_2$, $\mathcal{H}^{\mathscr E_1}_{\mathcal V}(X \rightarrow Y) \leq \mathcal{H}^{\mathscr E_2}_{\mathcal V}(X \rightarrow Y)$.
    
    \quad 2. \emph{Nonnegativity}: $\mathcal{H}^{\mathscr E}_{\mathcal V}(X \rightarrow Y) \geq 0$.
    
    \quad 3. \emph{Boundedness}: For discrete $Y$, $\mathcal{H}^{\mathscr E}_{\mathcal V}(X \rightarrow Y) \leq H_\mathcal V(Y|X)$.
    
    \quad 4. \emph{Corner Case}: If the predictive family $\mathcal V$ is the largest possible predictive family that includes all possible models, i.e. $\mathcal V = \Omega$, we have $\mathcal{H}^\mathscr E(X,Y) = \mathcal{H}^{\mathscr E}_{\Omega}(X \rightarrow Y)$.
    
\end{proposition}

Proofs can be found at Appendix \ref{proof: prop1}.
For further theoretical properties of predictive heterogeneity, in Section \ref{sec:linear}, we derive its explicit forms under \emph{endogeneity}, a common reflection of data heterogeneity.
And we demonstrate in Section \ref{sec:bounds} that our proposed predictive heterogeneity can be empirically estimated with guarantees if the complexity of $\mathcal V$ is bounded (e.g., its Rademacher complexity).



\subsection{Theoretical Properties in Linear Cases}
\label{sec:linear}
In this section, we conduct a theoretical analysis of the predictive heterogeneity in multiple linear settings. Specifically, we consider two scenarios: (1) a homogeneous case with independent noises and (2) heterogeneous cases with endogeneity arising from selection bias and hidden variables. By examining these typical settings, we approximate the analytical forms of the proposed measure and draw insightful conclusions that can be generalized to more complex scenarios.


Firstly, under a homogeneous case with no data heterogeneity, Theorem \ref{theorem: homogeneous} proves that our measure is bounded by the scale of label noises (which is usually small) and reduces to 0 in linear case under mild assumptions.
It indicates that the predictive heterogeneity is insensitive to independent noises.
Notably that in the linear case we only deal with the environment variable satisfying $X\perp \epsilon|\mathcal E$, since in common prediction tasks, the independent noises are unknown and unrealistic to be exploited for the prediction.
%In Appendix \ref{section:addition-numerical}, we add some numerical results under the setting of Theorem \ref{theorem: homogeneous} to show that the empirical estimated value approaches 0 as the sample size grows.

\begin{theorem}[Homogeneous Case with Independent Noises]
\label{theorem: homogeneous}
	For a prediction task $X \rightarrow Y$ where $X$, $Y$ are random variables taking values in $\mathbb R^n \times \mathbb R$, 
	consider the data generation process as $Y = g(x) + \epsilon, \epsilon\sim\mathcal{N}(0,\sigma^2)$ where $g:\mathbb R^n \rightarrow  \mathbb R$ is a measurable function. 
	 1) For a function class $\mathcal G$ such that $g \in \mathcal G$, define the function family as $\mathcal V_{\mathcal G}=\{ f | f[x]=\mathcal{N}(\phi(x),\sigma_V^2), \phi \in \mathcal G, \sigma_V \in \mathbb R^+ \}$. With an environment set $\mathscr E$, we have $\mathcal{H}^\mathscr E_{\mathcal{V}_{\mathcal G}}(X \rightarrow Y)\leq \pi\sigma^2$.
	 2) Take $n=1$ and $g(x) =\beta x$,$\beta\in \mathbb R$. 
	 Without loss of generality, assume $\mathbb E[X] =0$ and $\mathbb E[X^2]$  exists. Given the function family $\mathcal{V}_\sigma=\{f | f[x]=\mathcal{N}(\theta x,\sigma^2), \theta \in \mathbb R, \sigma \text{ fixed }\}$ and the environment set $\mathscr E = \{ \mathcal  E | \mathcal E \in \mathcal C, |\text{supp}(\mathcal E)|=2, X \perp \epsilon | \mathcal E\}$. We have $\mathcal{H}^\mathscr E_{\mathcal{V}_\sigma}(X \rightarrow Y)=0$.
	 Proofs can be found at Appendix \ref{proof:homogeneous}.
\end{theorem}

% TODO: 怎么解释这两个setting, 统计上的endogeneity， OOD里面的selection bias/ anti-causal等
Secondly, we examine the proposed measure under \emph{two typical cases of data heterogeneity} \citep{fan2014challenges}, named \emph{endogeneity by selection bias} \citep{heckman1979sample, winship1992models, cui2022stable} and \emph{endogeneity with hidden variables} \citep{fan2014challenges, arjovsky2019invariant}.

To begin with, in Theorem \ref{theorem: selection-bias}, we consider the prediction task $X \rightarrow Y$ with $X$, $Y$ taking values in $\mathbb R^2 \times \mathbb R$. Let $X = [S,V]^T$. The predictive family is specified as:
\begin{small} 
	\begin{equation}
	\label{equ:v_theorem23}
		\mathcal{V}=\{f|f[x]=\mathcal{N}(\theta_SS+\theta_VV, \sigma^2),\quad \theta_S,\theta_V\in \mathbb R, \sigma=1\}.
	\end{equation}
	\end{small}
And the data distribution $P(X,Y)$ is a mixture of latent sub-populations, which could be formulated by an environment variable $\mathcal E^* \in \mathcal C$ such that $P(X,Y) = \sum_{e\in \text{supp}(\mathcal E^*)} P(\mathcal E^* = e)P(X,Y|\mathcal E^* = e)$. For each $e \in \text{supp}(\mathcal E^*)$, $P(X,Y|\mathcal E^* = e)$ is the distribution of a homogeneous sub-population.
Note that the prediction task is to predict $Y$ with covariates $X$, and the sub-population structure is latent. That is, $P(\mathcal E^*|X,Y)$ is \emph{unknown} for models. In the following, we derive the analytical forms of our measure under the one typical case.
	
	
\begin{theorem}[Endogeneity with Selection Bias]
\label{theorem: selection-bias}
	For the prediction task $X=[S,V]^T\rightarrow Y$ with a latent environment variable $\mathcal E^*$, the data generation process with selection bias is defined as:
	\begin{small}
	\begin{align}
		Y = \beta S + f(S) + \epsilon_Y, \epsilon_Y \sim \mathcal{N}(0, \sigma_Y^2); \quad V = r(\mathcal E^*) f(S) + \sigma(\mathcal E^*)\cdot\epsilon_V, \epsilon_V \sim \mathcal{N}(0, 1),
	\end{align}
	\end{small}
	where  $f:\mathbb R\rightarrow \mathbb R$ and $r,\sigma:\text{supp}(\mathcal E^*) \rightarrow \mathbb R$ are measurable functions. $\beta \in \mathbb R$.
	Assume that $\mathbb{E}[S^2]$ is finite, $\mathbb{E}[f(S)S]=0$ and there exists $L>1$ such that $L\sigma^2(\mathcal E^*)< r^2(\mathcal E^*)\mathbb{E}[f^2]$. For the predictive family defined in equation \ref{equ:v_theorem23} and the environment set $\mathscr E = \mathcal C$, the predictive heterogeneity of the prediction task $[S,V]^T\rightarrow Y$ approximates to:
% 	obeys:
% 	\begin{small}
% 	\begin{equation}
% 		\frac{\text{Var}(r_e)}{\mathbb{E}[r_e^2]}\mathbb{E}[f^2(S)] + h_V^2\mathbb{E}_{\mathcal{E}}[\sigma_e^2] +\sigma_Y^2\geq \mathcal{H}_{\mathcal{V}}^u(P)\geq \frac{\text{Var}(r_e)}{\mathbb{E}[r_e^2]}\mathbb{E}[f^2(S)] + h_V^2\mathbb{E}_{\mathcal{E}}[\sigma_e^2] - \mathbb{E}_{\mathcal{E}}[\frac{1}{r_e^2}\sigma_e^2],
% 	\end{equation}
% 	\end{small}
% 	where $h_V = \mathbb{E}[r_e]/\mathbb{E}[r_e^2]$.
% 	Since $\sigma_Y$ and $\sigma_e$ are usually small in practice, we could make the following approximation:
	\begin{small}
	\begin{equation}
	\label{equ:approximation1}
		\mathcal{H}^\mathcal C_{\mathcal{V}}(X\rightarrow Y) \approx \frac{\text{Var}(r_e)\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}{\mathbb{E}[r_e^2]\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}\mathbb{E}[f^2(S)], \text{error bounded by }\frac{1}{2}\max(\sigma_Y^2,R(r,\sigma,f)).
	\end{equation}	
	\end{small}
	And further we have
	\begin{equation}
		\begin{aligned}
			R(r(\mathcal E^*), \sigma(\mathcal E^*), f) &= \mathbb{E}[(\frac{1}{\frac{r^2\mathbb{E}[f^2]}{\sigma^2}+1})^2]\mathbb{E}[f^2]+ \mathbb{E}_{\mathcal{E}^*}[(\frac{1}{\frac{r}{\sigma}+\frac{\sigma}{r\mathbb{E}[f^2]}})^2]\\
			&< \mathbb{E}[f^2](\frac{1}{(L+1)^2}+\frac{1}{L+2+\frac{1}{L}}).
		\end{aligned}
	\end{equation}
	Proofs can be found at Appendix \ref{proof: linear}.
\end{theorem}

Intuitively, the data generation process in Theorem \ref{theorem: selection-bias} introduces the spurious correlation between the spurious feature $V$ and the target $Y$, which varies across different sub-populations (i.e. $r(\mathcal E^*)$ and $\sigma(\mathcal E^*)$ varies) and brings about data heterogeneity.
Here $\mathbb{E}[f(S)S]=0$ indicates a model misspecification since there is a nonlinear term $f(S)$ that could not be inferred by the linear predictive family with the stable feature $S$. 
The constant $L$ characterizes the strength of the spurious correlation between $V$ and $Y$.
Larger $L$ means $V$ could provide more information for prediction.
% And $\mathbb{E}[\sigma^2(\mathcal E^*)]\ll \mathbb{E}[f^2(S)]$ indicates the correlation between the spurious feature $V$ and $f(S)$ is strong, which could provide additional information for prediction with each sub-population.

From the approximation in Equation \ref{equ:approximation1}, we can see that our proposed predictive heterogeneity is dominated by two terms: (1) $\text{Var}[r(\mathcal E^*)]/\mathbb{E}[r^2(\mathcal E^*)]$ characterizes the variance of $r(\mathcal E^*)$ among sub-populations; (2) $\mathbb{E}[f^2(S)]$ reflects the strength of model misspecifications.
These two components account for two sources of the data heterogeneity under selection bias, which validates the rationality of our proposed measure. 
%According to the theorem, the more various $r(\mathcal E^*)$ among the sub-populations and stronger model misspecifications, the larger the predictive heterogeneity.
Based on the theorem, it can be inferred that the degree of predictive heterogeneity increases with greater variability of $r(\mathcal E^*)$ among sub-populations and stronger model misspecifications. 
In other words, when the sub-populations differ significantly from each other and the model is not accurately specified, the predictive heterogeneity is likely to be larger.


 Additionally, in Theorem \ref{theorem: omitted variable}, we analyze our measure under endogeneity with hidden variables.
 In Theorem \ref{theorem: omitted variable}, an anti-causal covariate $V$ is generated via the causal diagram  like $Y\rightarrow V \leftarrow \mathcal E^*$ with a hidden environment variable $\mathcal E^*$.
 However, since $\mathcal E^*$ is omitted from the prediction models, the relationship between $V$ and $Y$ is biased, which inhibits the prediction power.



% TODO 这里修改成最后的版本
 \begin{theorem}[Endogeneity with Hidden Variables]
 \label{theorem: omitted variable}
 	For the prediction task $[S,V]^T\rightarrow Y$ with a latent environment variable $\mathcal E^*$, the data generation process with hidden variables is defined as:
 	\begin{small}
 	\begin{align}
 		Y = \beta S + f(S) + \epsilon_Y, \epsilon_Y\sim\mathcal{N}(0, \sigma_Y^2); \quad V = r(\mathcal E^*) (f(S)+\epsilon_Y) + \sigma(\mathcal E^*)\epsilon_V, \epsilon_V \sim\mathcal{N}(0, 1),
 	\end{align}
 	\end{small}
 	where  $f:\mathbb R\rightarrow \mathbb R$ and $r,\sigma:\text{supp}(\mathcal E^*) \rightarrow \mathbb R$ are measurable functions. $\beta \in \mathbb R$.
 	Assume that $\mathbb{E}[f(S)S]=0$ and there exists $L>1$ such that $L\sigma^2(\mathcal E^*) < r^2(\mathcal E^*)(\mathbb{E}[f^2]+\sigma_Y^2)$. 
 	For the predictive family defined in equation \ref{equ:v_theorem23} and the environment set $\mathscr E = \mathcal C$, the predictive heterogeneity of the prediction task $[S,V]^T\rightarrow Y$ approximates to:
 	\begin{small}
	\begin{equation}
	\begin{aligned}
	\label{equ:approximation2}
		&\mathcal{H}^\mathcal C_{\mathcal{V}}(X\rightarrow Y) \approx \frac{\text{Var}(r_e)(\mathbb{E}[f^2]+\sigma_Y^2)+\mathbb{E}[\sigma^2(\mathcal E^*)]}{\mathbb{E}[r_e^2](\mathbb{E}[f^2]+\sigma_Y^2)+\mathbb{E}[\sigma^2(\mathcal E^*)]}(\mathbb{E}[f^2(S)]+\sigma_Y^2),\\
		&\text{error bounded by }\frac{1}{2}\max(\sigma_Y^2,R(r,\sigma,f)).
	\end{aligned}
	\end{equation}	
	\end{small}
	And further we have:
	\begin{small}
	\begin{equation}
		\begin{aligned}
			R(r(\mathcal E^*), \sigma(\mathcal E^*), f) &= \mathbb{E}[(\frac{1}{\frac{r^2(\mathbb{E}[f^2]+\sigma_Y^2)}{\sigma^2}+1})^2](\mathbb{E}[f^2]+\sigma_Y^2)+ \mathbb{E}_{\mathcal{E}^*}[(\frac{1}{\frac{r}{\sigma}+\frac{\sigma}{r(\mathbb{E}[f^2]+\sigma_Y^2)}})^2]\\
			&<(\mathbb{E}[f^2]+\sigma_Y^2)(\frac{1}{(L+1)^2}+\frac{1}{L+2+\frac{1}{L}}).
		\end{aligned}
	\end{equation}	
	\end{small}
	Proofs can be found at Appendix \ref{proof: linear}.
 \end{theorem}


Intuitively, the data generation process in Theorem \ref{theorem: omitted variable} introduces the \emph{biased} anti-causal relationship between the spurious feature $V$ and the target $Y$, which varies across different sub-populations (i.e. $r(\mathcal E^*)$ and $\sigma(\mathcal E^*)$ varies) and brings about data heterogeneity.
Here, similar as Theorem \ref{theorem: selection-bias}, $\mathbb{E}[f(S)S]=0$ indicates model misspecification and the constant $L$ characterizes the strength of the biased anti-causal relationship between $V$ and $Y$, where larger $L$ means more information that $V$ could provide for predicting $Y$ when $\mathcal E^*$ is missing.
From the approximation in Equation \ref{equ:approximation2}, we can see that our proposed predictive heterogeneity is dominated by two terms: (1) $\text{Var}[r(\mathcal E^*)]/\mathbb{E}[r^2(\mathcal E^*)]$ characterizes the variance of $r(\mathcal E^*)$ among sub-populations; (2) $\mathbb{E}[f^2(S)]+\sigma_Y^2$ reflects the maximal additional information that could be provided by $V$.

%In general, Theorem \ref{theorem: homogeneous}, \ref{theorem: selection-bias} and \ref{theorem: omitted variable} indicate that (1) our proposed measure is insensitive to the homogeneous cases and (2) for the two typical sources of data heterogeneity, our measure accounts for the key components reflecting the latent heterogeneity.
In the broader context, Theorem 1, 2, and 3 suggest that our proposed predictive heterogeneity measure is equipped with remarkable properties, namely its insensitivity to homogeneous cases and its ability to account for the latent heterogeneity arising from typical sources of data heterogeneity. 
These findings highlight the efectiveness of our measure in accurately characterizing predictive heterogeneity in various machine learning tasks.



\subsection{PAC Guarantees for Predictive Heterogeneity Estimation}
Defined under explicit computation constraints, our Predictive Heterogeneity could be empirically estimated with guarantees if the complexity of the model family $\mathcal V$ is bounded.
In this work, we provide finite sample generalization bounds with the Rademacher complexity. First, we describe the definition of the empirical predictive heterogeneity, the explicit formula for which could be found in Definition \ref{def:empirical_predictive_heterogeneity}.

The dataset $\mathcal D = \{(x_i,y_i)\}_{i=1}^{|\mathcal D|}$ is independently and identically drawn from the population $X,Y$. 
Given a function family $\mathcal V $ and an environment set $\mathscr E_K$ such that for $\mathcal E\in \mathscr E_K$, $\text{supp}(\mathcal E) = \{ (e_k)_{k=1}^K \}$. , let $\mathcal Q$ be the set of all probability distributions of $X$,$Y$,$\mathcal E$ where $\mathcal E \in \mathscr E_K$.  The empirical predictive heterogeneity $\hat{\mathcal H}_{\mathcal V}^{\mathscr E_K}(X \rightarrow Y; \mathcal D)$ is given by:
\begin{small}
\begin{align}
    \hat{\mathcal H}_{\mathcal V}^{\mathscr E_K}(X \rightarrow Y; \mathcal D) 
    &= \sup_{\mathcal E \in \mathscr E_K }\hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E};\mathcal D)-\hat {\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D) \\
    &= \sup_{\hat Q \in \mathcal Q } {\sum_{k=1}^K \left[\hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D) - \hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D)\right]} \\
    & \quad\quad - [\hat H_{\mathcal V}(Y;\mathcal D) - \hat H_{\mathcal V}(Y|X;\mathcal D)].
\end{align}
\end{small}
Specifically, 
\begin{align}
    & \quad\; \hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D) \\
    &= \inf_{f \in \mathcal V} \hat Q(\mathcal E=e_k) \sum_{x_i,y_i \in \mathcal D} -\log  f[x_i](y_i) \frac{\hat Q(x_i,y_i|\mathcal E=e_k)}{\sum_{x_j,y_j \in \mathcal D}\hat Q(x_j,y_j|\mathcal E=e_k) }  \\
    &= \inf_{f \in \mathcal V} \hat Q(\mathcal E=e_k) \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) \frac{\hat Q(\mathcal E=e_k|x_i,y_i)
    \hat Q(x_i,y_i)}{\sum_{x_j,y_j \in \mathcal D}\hat Q(\mathcal E=e_k|x_j,y_j)\hat Q(x_j,y_j) } \\
    &= \inf_{f \in \mathcal V} \hat Q(\mathcal E=e_k) \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) \frac{\hat Q(\mathcal E=e_k|x_i,y_i)
    \hat Q(x_i,y_i)}{\hat Q(\mathcal E=e_k) } \\
    &= \inf_{f \in \mathcal V} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) \hat Q(\mathcal E=e_k|x_i,y_i) \hat Q(x_i,y_i) \\
    &= \inf_{f \in \mathcal V} \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) \hat Q(\mathcal E=e_k|x_i,y_i).
\end{align}

The explicit formula for $\hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D)$, $\hat H_{\mathcal V}(Y|X;\mathcal D)$ and $\hat H_{\mathcal V}(Y;\mathcal D)$ could be similarly derived. Here we are ready to formally define the empirical predictive heterogeneity.

\begin{definition} [Empirical Predictive Heterogeneity]
\label{def:empirical_predictive_heterogeneity}
	For the prediction task $X\rightarrow Y$ with $X$, $Y$ taking values in $\mathcal X \times \mathcal Y$, a dataset $\mathcal D$ is independently and identically drawn from the population such that $\mathcal D=\{(x_i,y_i)_{i=1}^N \sim X,Y \}$. 
	Given the predictive family $\mathcal V$ and the environment set $\mathscr E_K =\{ \mathcal E|\mathcal E \in \mathcal C, |\text{supp}(\mathcal E)|=K \}$ where $K \in \mathbb N$.
	Without loss of generality, we specify that  $\text{supp}(\mathcal E) = \{ (e_k)_{k=1}^K \}$ where $e_k$ denotes a single environment.
	Let $\mathcal Q$ be the set of all probability distributions of $X$,$Y$,$\mathcal E$ where $\mathcal E \in \mathscr E_K$. The empirical predictive heterogeneity $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V}(X\rightarrow Y; \mathcal D)$ with respect to $\mathcal D$  is defined as:
	\begin{equation}
	\label{equ:appendix-formal-empirical}
	    \begin{aligned}
    \hat{\mathcal H}_{\mathcal V}^{\mathscr E_K}(X \rightarrow Y; \mathcal D) 
    &= \sup_{\hat Q \in \mathcal Q } {\sum_{k=1}^K \left[\hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D) - \hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D)\right]} \\
    & \quad\quad - [\hat H_{\mathcal V}(Y;\mathcal D) - \hat H_{\mathcal V}(Y|X;\mathcal D)],
\end{aligned}
	\end{equation}
where
\begin{align}
    \hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D) &=
    \inf_{f \in \mathcal V} \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) \hat Q(\mathcal E=e_k|x_i,y_i). \\
    \hat Q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D) &= \inf_{f \in \mathcal V} \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[\emptyset](y_i) \hat Q(\mathcal E=e_k|x_i,y_i). \\
    \hat H_{\mathcal V}(Y|X;\mathcal D) &=\inf_{f \in \mathcal V} \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log  f[x_i](y_i). \\
    \hat H_{\mathcal V}(Y;\mathcal D) &=\inf_{f \in \mathcal V} \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log  f[\emptyset](y_i).
\end{align}
\end{definition}

%\begin{definition}[Empirical Predictive Heterogeneity (\textit{informal})]
%\label{def:empirical_predictive_heterogeneity_informal}
%	For the prediction task $X\rightarrow Y$ with $X$, $Y$ taking values in $\mathcal X \times \mathcal Y$, a dataset $\mathcal D$ is independently and identically drawn from the population such that $\mathcal D=\{(x_i,y_i)_{i=1}^N \sim X,Y \}$. 
%	Given the predictive family $\mathcal V$ and the environment set $\mathscr E_K =\{ \mathcal E|\mathcal E \in \mathcal C, \text{supp}(\mathcal E)=K \}$ where $K \in \mathbb N^+$ is the number of environments, the \textbf{empirical predictive heterogeneity} $\hat{\mathcal{H}}^{\mathscr{E}_K}_{\mathcal V}(X\rightarrow Y; \mathcal D)$ with respect to $\mathcal D$  is readily obtained by estimating $\mathcal H_{\mathcal V}^{\mathscr E_K}(X \rightarrow Y)$ on $\mathcal D$ with expectations replaced by statistics of finite samples.
%	The formal definition is placed in Definition \ref{def:empirical_predictive_heterogeneity}.
%% 	\begin{equation}
%% 	\label{equ:usable-predictive-heterogeneity}
%% 		\hat{\mathcal{H}}^\mathcal{E}_{\mathcal V}(X\rightarrow Y; \mathcal D) = \sup_{\mathcal{E} \in \mathscr E}\mathbb{I}_{\mathcal{V}}(X\rightarrow Y;\mathcal D|\mathcal{E})-\mathbb{I}_{\mathcal{V}}(X\rightarrow Y; \mathcal D)
%% 	\end{equation} 
%% 	where $\mathbb{I}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)$ denotes the empirical predictive $\mathcal V$-information proposed by \cite{DBLP:conf/iclr/XuZSSE20}.
%\end{definition}

Then we give the PAC bound over the empirical usable predictive heterogeneity in Theorem \ref{theorem:pac}.
\label{sec:bounds}
\begin{theorem}[PAC Bound]
\label{theorem:pac}
	Consider the prediction task $X \rightarrow Y$ where $X$, $Y$ are random variables taking values in $\mathcal X \times \mathcal Y$. Assume that the predictive family $\mathcal V$ satisfies $\forall x\in\mathcal{X}$, $\forall y \in \mathcal Y$,$\forall f \in \mathcal V$, $\log f[x](y) \in [-B,B]$ where $B > 0$.  
	For given $K \in \mathbb N$, the environment set is defined as $\mathscr E_K = \{ \mathcal E| \mathcal E \in \mathcal C, |\text{supp}(\mathcal E)| = K \}$ where $K \in \mathbb N$. 
	Without loss of generality, we specify that  $\text{supp}(\mathcal E) = \{ (e_k)_{k=1}^K \}$ where $e_k$ denotes a single environment.
	Let $\mathcal Q$ be the set of all probability distributions of $X$,$Y$,$\mathcal E$ where $\mathcal E \in \mathscr E_K$.  Take an $e \in \text{supp}(\mathcal E)$ and define a function class $\mathcal G_{\mathcal V} = \{g|g(x,y) = \log f[x](y)Q(\mathcal E=e|x,y), f\in \mathcal V, Q \in \mathcal Q  \}$. Denote the Rademacher complexity of $\mathcal{G}$ with $N$ samples by $\mathscr{R}_{N}(\mathcal{G})$.
Then for any $\delta \in \left(0,1/(2K+2)\right)$, with a probability over $1 - 2(K+1)\delta$, for dataset $\mathcal{D}$ independently and identically drawn from $X$, $Y$, we have:
\begin{small}
\begin{align}
    |\mathcal{H}^{\mathscr E_K}_{\mathcal V}(X\rightarrow Y) - \hat{\mathcal H}^{\mathscr E_K}_{\mathcal V}(X\rightarrow Y;\mathcal D)| \leq 4(K+1)\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) + 2(K+1)B\sqrt{2\log{\frac{1}{\delta}}/|\mathcal D|},
\end{align}
\end{small}
where $\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) = \mathcal O(|\mathcal{D}|^{-\frac{1}{2}})$ \citep{bartlett2002rademacher}.
Proofs can be found at Appendix \ref{proof: pac}.
\end{theorem}









