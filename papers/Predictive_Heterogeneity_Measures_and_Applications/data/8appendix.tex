\section{Proof of Proposition \ref{proposition1}}
\label{proof: prop1}
\begin{proof}[Proof of Proposition \ref{proposition1}]
\\\\
1. \emph{Monotonicity}:

Because of $\mathscr E_1 \subseteq \mathscr E_2$,
\begin{small}
\begin{align}
   \mathcal{H}^{\mathscr E_1}_{\mathcal V}(X \rightarrow Y) &= \sup_{\mathcal{E} \in \mathscr E_1}\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E})-\mathbb{I}_{\mathcal{V}}(X\rightarrow Y) \\
    &\leq \sup_{\mathcal{E} \in \mathscr E_2}\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E})-\mathbb{I}_{\mathcal{V}}(X\rightarrow Y) \\
    &= \mathcal{H}^{\mathscr E_2}_{\mathcal V}(X \rightarrow Y).
\end{align}
\end{small}
2. \emph{Nonnegativity}:

According to the definition of the environment set, there exists $\mathcal E_0 \in \mathscr E$ such that for any $e \in \text{supp}(\mathcal E)$, $X,Y|\mathcal E=e$ is identically distributed as $X,Y$. Thus, we have
\begin{small}
\begin{align}
    \mathcal{H}^{\mathscr E}_{\mathcal V}(X \rightarrow Y) &=
    \sup_{\mathcal{E} \in \mathscr E} \left[H_\mathcal V(Y|\emptyset,\mathcal E) - H_\mathcal V(Y|X,\mathcal E)\right] - \left[H_\mathcal V(Y|\emptyset) - H_\mathcal V(Y|X)\right] \\
    &\geq \left[H_\mathcal V(Y|\emptyset,\mathcal E_0) - H_\mathcal V(Y|X,\mathcal E_0)\right] - \left[H_\mathcal V(Y|\emptyset) - H_\mathcal V(Y|X)\right].
\end{align}
\end{small}
Specifically, 
\begin{small}
\begin{align}
    H_\mathcal V(Y|X,\mathcal E_0) &= \mathbb E_{e \sim \mathcal E_0} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x,y\sim X,Y | \mathcal E=e}[-\log f[x](y)] \right] \\
    &= \mathbb E_{e \sim \mathcal E_0} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x,y\sim X,Y}[-\log f[x](y)] \right] \\
    &= H_\mathcal V(Y|X).
\end{align}
\end{small}
Similarly, $H_\mathcal V(Y|\emptyset,\mathcal E_0) = H_\mathcal V(Y|\emptyset)$.
Thus, $\mathcal{H}^{\mathscr E}_{\mathcal V}(X \rightarrow Y) \geq 0$.\\\\
3. \emph{Boundedness}:

First, we have 
\begin{small}
\begin{align}
    H_{\mathcal V}(Y|X,\mathcal E) &= \mathbb E_{e \sim \mathcal E} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x,y\sim X,Y|\mathcal E=e}[-\log f[x](y)] \right] \\
    &= \mathbb E_{e \sim \mathcal E} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x\sim X|\mathcal E=e} \left[\mathbb E_{y \sim Y|x,e}[-\log f[x](y)] \right] \right]  \\
    &\geq 0,
\end{align}
\end{small}
by noticing that $\mathbb E_{y \sim Y|x}[-\log f[x](y)]$ is the cross entropy between $Y|x,e$ and $f[x]$.

Next,
\begin{small}
\begin{align}
    H_{\mathcal V}(Y|\emptyset,\mathcal E) &= \mathbb E_{e \sim \mathcal E} \left[ \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{y\sim Y|\mathcal E=e}[-\log f[\emptyset](y)] \right] \\
    \label{equ:proposition1_3_1}
    &\leq \inf\limits_{f\in\mathcal{V}} \mathbb E_{e \sim \mathcal E} \left[ \mathbb{E}_{y\sim Y|\mathcal E=e}[-\log f[\emptyset](y)] \right] \\
    &= \inf\limits_{f\in\mathcal{V}} \mathbb{E}_{y\sim Y}[-\log f[\emptyset](y)] \\
    &= H_{\mathcal V}(Y|\emptyset),
\end{align}
\end{small}
where Equation \ref{equ:proposition1_3_1} is due to Jensen's inequality.

Combing the above inequalities,
\begin{align}
    \mathcal{H}^{\mathscr E}_{\mathcal V}(X \rightarrow Y) &=
    \sup_{\mathcal{E} \in \mathscr E} \left[H_\mathcal V(Y|\emptyset,\mathcal E) - H_\mathcal V(Y|X,\mathcal E)\right] - \left[H_\mathcal V(Y|\emptyset) - H_\mathcal V(Y|X)\right] \\
    &\leq  \sup_{\mathcal{E} \in \mathscr E} H_\mathcal V(Y|\emptyset,\mathcal E)  - \left[H_\mathcal V(Y|\emptyset) - H_\mathcal V(Y|X)\right] \\
    &\leq H_\mathcal V(Y|\emptyset) - \left[H_\mathcal V(Y|\emptyset) - H_\mathcal V(Y|X)\right] \\
    &= H_\mathcal V(Y|X).
\end{align}
4. \emph{Corner Case}:

According to Proposition 2 in \cite{DBLP:conf/iclr/XuZSSE20}, 
\begin{align}
    H_\Omega(Y|\emptyset) &= H(Y). \\
    H_\Omega(Y|X) &= H(Y|X).
\end{align}
By taking random variables $R,S$ identically distributed as $X,Y|\mathcal E=e$ for $e \in \text{supp}(\mathcal E)$, we have
\begin{align}
    H_{\Omega}(Y|X,\mathcal E=e) = H_{\Omega}(S|R) = H(S|R) = H(Y|X,\mathcal E=e).
\end{align}
Thus, 
\begin{align}
    H_\Omega(Y|X,\mathcal E) = \mathbb E_{e\sim \mathcal E}[H_\Omega(Y|X,\mathcal E=e)] = \mathbb E_{e\sim \mathcal E}[H(Y|X,\mathcal E=e)] = H(Y|X,\mathcal E).
\end{align}
Similarly, we have $ H_\Omega(Y|\emptyset,\mathcal E) = H(Y|\mathcal E)$.
Thus,
\begin{align}
    \mathcal{H}^{\mathscr E}_{\Omega}(X \rightarrow Y) &= 
    \sup_{\mathcal{E} \in \mathscr E} \left[H_\Omega(Y|\emptyset,\mathcal E) - H_\Omega(Y|X,\mathcal E)\right] - \left[H_\Omega(Y|\emptyset) - H_\Omega(Y|X)\right] \\
    &=  \sup_{\mathcal{E} \in \mathscr E} \left[H(Y|\mathcal E) - H(Y|X,\mathcal E)\right] - \left[H(Y) - H(Y|X)\right] \\
    &= \sup_{\mathcal{E} \in \mathscr E}\mathbb{I}(Y;X|\mathcal{E})-\mathbb{I}(Y;X) \\
    &= \mathcal{H}^{\mathscr E}(X, Y).
\end{align}
\end{proof}




\section{Proof of Theorem \ref{theorem: homogeneous}}
\label{proof:homogeneous}
\begin{proof}[Proof of Theorem \ref{theorem: homogeneous}]
\quad

1)

\begin{align}
    H_{\mathcal V_{\mathcal G}}(Y|X) &= \inf\limits_{f\in \mathcal V_{\mathcal G}}\mathbb{E}_{x\sim X} \left[\mathbb E_{y \sim Y|x}[-\log f[x](y)] \right] \\
    \label{equ:theorem1_1_1}
    &\leq \mathbb{E}_{x\sim X} \left[\mathbb E_{y \sim Y|x}[-\log \frac{1}{\sqrt{2\pi} \cdot \frac{1}{\sqrt{2\pi}}} \exp{ \left[-\frac{(y-g(x))^2}{2\cdot \frac{1}{2\pi} } \right] } \right] \\
    &= \mathbb{E}_{x\sim X} \left[\mathbb E_{y \sim Y|x}[\pi (y-g(x))^2 ] \right] = \pi\sigma^2.
\end{align}
Equation \ref{equ:theorem1_1_1} holds by taking $f[x] = \mathcal N(g(x), \frac{1}{2\pi})$.

2) 

Given the function family $\mathcal{V}_\sigma=\{f | f[x]=\mathcal{N}(\theta x,\sigma^2), \theta \in \mathbb R, \sigma \text{ fixed }\}$, by expanding the Gaussian probability density function in the definition of predictive $\mathcal V$-information, it could be shown that 
\begin{align}
\label{equ:Iv(X->Y)}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y) &\propto \min_{k\in \mathbb R} -\mathbb{E}[(Y-kX)^2] + \text{Var}(Y),
\end{align}
where the predictive $\mathcal V$-information is proportional to Mean Square Error subtracted by the variance of target, by a coefficient completely dependent on $\sigma$.

The minimization problem is solved by 
\begin{equation}
    k = \frac{\mathbb E[XY]}{\mathbb E[X^2]} = 1.
\end{equation}
Substituting $k$ into eq.\ref{equ:Iv(X->Y)},
\begin{align}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y) &\propto (-\mathbb E[\epsilon^2] + \text{Var}(X+\epsilon))=\text{Var}(X) = \mathbb E[X^2].
\end{align}
Denote $\text{supp}(\mathcal{E})=\{\mathcal{E}_1,\mathcal{E}_2\}$. Let $Q$ be the joint distribution of $(X,\epsilon,\mathcal E)$. 
% and $P(X,\epsilon) = Q(X,\epsilon,\mathcal E_1) + Q(X,\epsilon,\mathcal E_2)$. 
Let $Q(\mathcal E_1)=\alpha$ and $ Q(\mathcal E_2)=1-\alpha $ be the marginal of $\mathcal E$. Abbreviate $Q(X,\epsilon|\mathcal E=\mathcal E_1)$ by $P_1(X,\epsilon)$ and $Q(X,\epsilon|\mathcal E=\mathcal E_2)$ by $P_2(X,\epsilon)$.

Similar to \ref{equ:Iv(X->Y)},
\begin{align}
\label{equ:Iv(X->Y|e)}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E) &\propto \min_k -\mathbb{E}[(Y-kX)^2|\mathcal E] + \text{Var}(Y|\mathcal E).
\end{align}
For $\mathcal E=\mathcal E_1$, the minimization problem is solved by
\begin{align}
    k = \frac{\mathbb E_{P_1}[XY]}{\mathbb E_{P_1}[X^2]}.
\end{align}
Thus,
\begin{small}
\begin{align}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E=\mathcal E_1) &\propto -\mathbb E_{P_1}\left[\left(Y-\frac{\mathbb E_{P_1}[XY]}{\mathbb E_{P_1}[X^2]}X\right)^2\right] + \text{Var}_{P_1}(Y) \\
    &= -\mathbb E_{P_1}[Y^2] + \frac{\mathbb E_{P_1}^2[XY]}{\mathbb E_{P_1}[X^2]} + (\mathbb E_{P_1}[Y^2] - \mathbb E_{P_1}^2[Y]) 
    = -\mathbb E_{P_1}^2[Y] + \frac{\mathbb E_{P_1}^2[XY]}{\mathbb E_{P_1}[X^2]}.
\end{align}
\end{small}
Similarly, we have
\begin{align}
\label{equ:Iv(X->Y|e_2)}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E=\mathcal E_2) &\propto -\mathbb E_{P_2}^2[Y] + \frac{\mathbb E_{P_2}^2[XY]}{\mathbb E_{P_2}[X^2]}.
\end{align}
Notably, $\mathbb E_{P_1}[X^2]$ and $\mathbb E_{P_2}[X^2]$ are constrained by $\alpha$ and $\mathbb E[X^2]$.
\begin{equation}
    \mathbb E[X^2] = \mathbb E[\mathbb E[X^2|\mathcal E]] = \alpha \mathbb E_{P_1}[X^2] + (1-\alpha)\mathbb E_{P_2}[X^2].
\end{equation}
Similarly,
\begin{equation}
    \mathbb E[X^2] = \mathbb E[XY] = \alpha \mathbb E_{P_1}[XY] + (1-\alpha)\mathbb E_{P_2}[XY].
\end{equation}
\begin{equation}
    0 =\mathbb E[Y] = \alpha \mathbb E_{P_1}[Y] + (1-\alpha)\mathbb E_{P_2}[Y].
\end{equation}
The moments of $P_2$ could thereafter be represented by those of $P_1$.
\begin{small}
\begin{equation}
    \mathbb E_{P_2}[X^2] = \frac{\mathbb E[X^2] - \alpha \mathbb E_{P_1}[X^2]}{1-\alpha},
    \mathbb E_{P_2}[XY] = \frac{\mathbb E[X^2] - \alpha \mathbb E_{P_1}[XY]}{1-\alpha},
    \mathbb  E_{P_2}[Y] = - \frac{\alpha \mathbb E_{P_1}[Y]}{1-\alpha}.
\end{equation}
\end{small}
Substituting to eq.\ref{equ:Iv(X->Y|e_2)},
\begin{align}
    \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E=\mathcal E_2) &\propto -\frac{\alpha^2}{(1-\alpha)^2}E_{P_1}^2[Y] + \frac{1}{1-\alpha}\frac{\left(\mathbb E[X^2] - \alpha \mathbb E_{P_1}[XY]\right)^2}{\mathbb E[X^2] - \alpha \mathbb E_{P_1}[X^2]}.
\end{align}
Thus,
\begin{small}
\begin{align}
    \mathcal H_{\mathcal V_\sigma}^{\mathscr E}(X \rightarrow Y) &= \sup_{\mathcal E \in \mathscr E} -\mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y) + \alpha \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E=\mathcal E_1) + (1-\alpha) \mathbb{I}_{\mathcal{V}_\sigma}(X\rightarrow Y|\mathcal E=\mathcal E_2) \\
    &\propto \sup_{\mathcal E \in \mathscr E} -\mathbb E[X^2] - \alpha \mathbb E_{P_1}^2[Y] + \alpha \frac{\mathbb E_{P_1}^2[XY]}{\mathbb E_{P_1}[X^2]} - \frac{\alpha^2}{1-\alpha} \mathbb E_{P_1}^2[Y] + \frac{\left(\mathbb E[X^2] - \alpha \mathbb E_{P_1}[XY]\right)^2}{\mathbb E[X^2] - \alpha \mathbb E_{P_1}[X^2]} \\
%    &= \sup_{\mathcal E \in \mathscr E} -\frac{\alpha}{1-\alpha}\mathbb E_{P_1}^2[Y] + \alpha \frac{\left(\mathbb E_{P_1}[X^2]-\mathbb E_{P_1}[XY]\right)^2}{\mathbb E_{P_1}[X^2]\left(\mathbb E[X^2] - \alpha \mathbb E_{P_1}[X^2]\right)} \mathbb E[X^2] \\
    &= \sup_{\mathcal E \in \mathscr E} -\frac{\alpha}{1-\alpha}\mathbb E_{P_1}^2[X+\epsilon] + \alpha \frac{\mathbb E_{P_1}^2[X\epsilon]}{\mathbb E_{P_1}[X^2]\left(\mathbb E[X^2] - \alpha \mathbb E_{P_1}[X^2]\right)} \mathbb E[X^2].
\end{align}
\end{small}
Assuming $X \perp \epsilon \;|\; \mathcal E$, 
\begin{align}
    \mathcal H_{\mathcal V_\sigma}^{\mathscr E}(X \rightarrow Y) \propto \sup_{\mathcal E \in \mathscr E} -\frac{\alpha}{1-\alpha}\mathbb E_{P_1}^2[X+\epsilon] \leq 0.
\end{align}
From Proposition \ref{proposition1}, we have $\mathcal H_{\mathcal V_\sigma}^{\mathscr E}(X \rightarrow Y) \geq 0$. Thus, $\mathcal H_{\mathcal V_\sigma}^{\mathscr E}(X \rightarrow Y) = 0$.
\end{proof}


\section{Proof of Linear Cases (Theorem \ref{theorem: selection-bias} and \ref{theorem: omitted variable})}
\label{proof: linear}
\begin{proof}[Proof of Theorem \ref{theorem: selection-bias}]

For the ease of notion, we denote the $r(\mathcal{E}^*)$ as $r_e$, $\sigma(\mathcal{E}^*)$ as $\sigma_e$, and $\sigma(\mathcal{E}^*)\cdot\epsilon_v$ as $\epsilon_e$. 
And we omit the superscript $\mathcal C$ of $\mathcal{H}_{\mathcal V}^{\mathcal C}$.
	Firstly, we calculate the $H_{\mathcal{V}}[Y|\emptyset]$ as:
	\begin{align}
		H_{\mathcal{V}}[Y|\emptyset] &= \frac{1}{2\sigma^2}\text{Var}(Y) + \log\sigma + \frac{1}{2}\log 2\pi,\\
		H_{\mathcal{V}}[Y|\emptyset,\mathcal{E}^*] &= \frac{1}{2\sigma^2} \mathbb{E}_{\mathcal{E}^*}[\text{Var}(Y|\mathcal{E}^*)]+ \log\sigma + \frac{1}{2}\log 2\pi.
	\end{align}
	Therefore, we have
	\begin{equation}
		H_{\mathcal{V}}[Y|\emptyset,\mathcal{E}^*] - H_{\mathcal{V}}[Y|\emptyset] = -\frac{1}{2\sigma^2}\text{Var}(\mathbb{E}[Y|\mathcal{E}^*])\leq 0.
	\end{equation}
	As for $H_{\mathcal{V}}[Y|X]$, we have
	\begin{align}
		H_{\mathcal{V}}[Y|X] &= \inf_{h_S,h_V}\mathbb{E}_{X,Y}\left[\|Y-(h_SS+h_VV)\|^2\right]\frac{1}{2\sigma^2}\\
%		&= \inf_{h_S,h_V}\mathbb{E}_{X,Y}\left[\|f(S)+\epsilon_Y-(h_SS+h_VV)\|^2\right]\frac{1}{2\sigma^2}\\
		&= \inf_{h_S,h_V}\mathbb{E}_{\mathcal{E}^*}\left[\mathbb{E}[\|f(S)+\epsilon_Y-(h_SS+h_V(r_ef(S)+\epsilon_e))\|^2|\mathcal{E}^*]\right]\frac{1}{2\sigma^2},
	\end{align}
	where we let $h_S=h_S-\beta$ here.
	Then we have
	\begin{align}
	2\sigma^2 H_{\mathcal{V}}[Y|X] &= \inf_{h_S,h_V}\mathbb{E}_{\mathcal{E}^*}\left[\mathbb{E}[\|(1-h_Vr_e)f(S)+\epsilon_Y-h_SS-h_V\epsilon_e\|^2|\mathcal{E}^*]\right]\\
		&= \inf_{h_S,h_V}\mathbb{E}_{\mathcal{E}^*}\left[\mathbb{E}[\|(1-h_Vr_e)f(S)-h_SS\|^2|\mathcal{E}^*]\right] + \sigma_Y^2 + h_V^2\mathbb{E}_{\mathcal{E}^*}[\sigma_e^2],
	\end{align}
	notably that here for $e_i,e_j\in\text{supp}(\mathcal{E}^*)$, we assume $P^{e_i}(S,Y)=P^{e_j}(S,Y)$ (we choose such $\mathcal{E}^*$ as one possible split).
	And the solution of $h_S,h_V$ is
	\begin{align}
		h_S &= \frac{\text{Var}(r_e)\mathbb{E}[f^2(S)]\mathbb{E}[f(S)S]+\mathbb{E}[\sigma_e^2]\mathbb{E}[f(S)S]}{\mathbb{E}[r_e^2]\mathbb{E}[f^2(S)]\mathbb{E}[S^2] + \mathbb{E}[\sigma_e^2]\mathbb{E}[S^2] - \mathbb{E}^2[r_e]\mathbb{E}^2[f(S)S]},\\
		h_V &= \frac{\mathbb{E}[r_e](\mathbb{E}[f^2(S)]\mathbb{E}[S^2]-\mathbb{E}^2[f(S)S])}{\mathbb{E}[r_e^2]\mathbb{E}[f^2(S)]\mathbb{E}[S^2] + \mathbb{E}[\sigma_e^2]\mathbb{E}[S^2] - \mathbb{E}^2[r_e]\mathbb{E}^2[f(S)S]}.
	\end{align}
	According to the assumption that $\mathbb{E}[f(S)S]=0$, we have
	\begin{align}
		h_S = 0,\quad
		h_V = \frac{\mathbb{E}[r(\mathcal E^*)]\mathbb{E}[f^2]}{\mathbb{E}[r^2(\mathcal E^*)]\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}.
	\end{align}
	Therefore, we have
	\begin{align}
		2\sigma^2 H_{\mathcal{V}}[Y|X] &= \mathbb{E}_{\mathcal{E}^*}[\mathbb{E}[\|(1-h_Vr_e)f(S)\|^2|\mathcal{E}^*]] + \sigma_Y^2+h_V^2\mathbb{E}_{\mathcal{E}^*}[\sigma_e^2]\\
		&= \frac{\text{Var}(r_e)\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}{\mathbb{E}[r_e^2]\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}\mathbb{E}[f^2(S)]+ \sigma_Y^2,\\
		2\sigma^2 H_{\mathcal{V}}[Y|X,\mathcal{E}^*] &= \sigma_Y^2+ \mathbb{E}[(\frac{1}{\frac{r_e^2\mathbb{E}[f^2]}{\sigma_e^2}+1})^2]\mathbb{E}[f^2]+ \mathbb{E}_{\mathcal{E}^*}[(\frac{1}{\frac{r_e}{\sigma_e}+\frac{\sigma_e}{r_e\mathbb{E}[f^2]}})^2].
	\end{align}
	Note that here we simply set $\sigma=1$ in the main body.
    And we have:
    \begin{equation}
        \mathcal{H}_{\mathcal V}(X\rightarrow Y)\approx \frac{\text{Var}(r_e)\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}{\mathbb{E}[r_e^2]\mathbb{E}[f^2]+\mathbb{E}[\sigma^2(\mathcal E^*)]}\mathbb{E}[f^2(S)]
    \end{equation}
    The approximation error is bounded by $\frac{1}{2}\max(\sigma_Y^2, R(r(\mathcal E^*), \sigma(\mathcal E^*), \mathbb{E}[f^2]))$, and $R(r(\mathcal E^*), \sigma(\mathcal E^*), \mathbb{E}[f^2])$ is defined as:
    \begin{equation}
        R(r(\mathcal E^*), \sigma(\mathcal E^*), \mathbb{E}[f^2]) = \mathbb{E}[(\frac{1}{\frac{r_e^2\mathbb{E}[f^2]}{\sigma_e^2}+1})^2]\mathbb{E}[f^2]+ \mathbb{E}_{\mathcal{E}^*}[(\frac{1}{\frac{r_e}{\sigma_e}+\frac{\sigma_e}{r_e\mathbb{E}[f^2]}})^2]
    \end{equation}
\end{proof}

\begin{proof}[Proof of Theorem \ref{theorem: omitted variable}]
	Similar as the above proof.	
\end{proof}



\section{Proof of the Error Bound for Finite Sample Estimation (Theorem \ref{theorem:pac})}
\label{proof: pac}

In this section, we will prove the error bound of estimating the predictive heterogeneity with the empirical predictive heterogeneity. Before the proof of Theorem \ref{theorem:pac} which is inspired by \cite{DBLP:conf/iclr/XuZSSE20}, we will introduce three lemmas.

\begin{lemma}
\label{lemma:err_1}
Assume $\forall x \in \mathcal X$,$\forall y \in \mathcal Y$,$\forall f \in \mathcal V$, $\log f[x](y) \in [-B,B]$ where $B > 0$. Define a function class $\mathcal G_{\mathcal V}^k = \{g|g(x,y) = \log f[x](y)q(\mathcal E=e_k|x,y), f\in \mathcal V, q \in \mathcal Q  \}$. Denote the Rademacher complexity of $\mathcal G$ with $N$ samples by $\mathscr R_{N}(\mathcal G)$. Define 
\begin{equation}
\hat f_k = \arg \inf_f \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i).
\end{equation}

Then for any $q \in \mathcal Q$,  any $\delta \in (0,1)$, with a probability over $1 - \delta$,  we have
\begin{small}
\begin{align}
    &\quad\; \left|q(\mathcal E=e_k)H_{\mathcal V}(Y|X,\mathcal E=e_k)  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f_k}[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \\
    &\leq 2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
\end{align}
\end{small}
\end{lemma}
\begin{proof}
Apply McDiarmid's inequality to the function $\Phi(\mathcal D)$ which is defined as:
\begin{small}
\begin{align}
    \Phi(\mathcal D)  
    &= \sup_{f\in \mathcal V, q \in \mathcal Q} \left| q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i)   \right|.
\end{align}
\end{small}
Let $\mathcal D$ and $\mathcal D'$ be two identical datasets except for one data point $x_j \neq x_j'$. We have:
\begin{small}
\begin{align}
    & \quad\; \Phi(\mathcal D) - \Phi(\mathcal D') \\
    & \leq \sup_{f\in \mathcal V, q \in \mathcal Q} \left[ \left| q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i)   \right| \right.\\ 
    &\left. \quad\quad\quad\quad - \left| q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D'|} \sum_{x_i',y_i' \in \mathcal D'} -\log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i')   \right|\right] \\
    &\leq \sup_{f\in \mathcal V, q \in \mathcal Q} \left| \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) - \frac{1}{|\mathcal D'|} \sum_{x_i',y_i' \in \mathcal D'} -\log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i')  \right| \\
    &=  \sup_{f\in \mathcal V, q \in \mathcal Q}\frac{1}{|\mathcal D|}  \left| \log f[x_j](y_j) q(\mathcal E=e_k|x_j,y_j) - \log f[x_j'](y_j') q(\mathcal E=e_k|x_j',y_j') \right| \\
    &\leq \frac{2B}{|\mathcal D|}.
\end{align}
\end{small}
According to McDiarmid's inequality, for any $\delta \in (0,1)$, with a probability over $1 - \delta$, we have:
\begin{align}
    \label{equ:err_7}
    \Phi(\mathcal D) \leq \mathbb E_{\mathcal D} [\Phi(\mathcal D)] + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
\end{align}
Next we derive a bound for $\mathbb E_{\mathcal D}[\Phi(\mathcal D)]$.
Consider a dataset $\mathcal D'$ independently and identically drawn from $q(X,Y) = P(X,Y)$ with the same size as $\mathcal D$. We notice that
\begin{small}
\begin{align}
    q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]
%    &= q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y) q(\mathcal E=e_k|x,y)|\mathcal E=e_k \right] \\
%    &= \mathbb E_q\left[ \mathbb E_q\left[ -\log f[x](y) q(\mathcal E=e_k|x,y)|\mathcal E=e_k \right] \right] \\
%    &= \mathbb E_q\left[ -\log f[x](y) q(\mathcal E=e_k|x,y) \right] \\
    = \mathbb E_{\mathcal D'} \left[ -\frac{1}{|\mathcal D'|} \sum_{x_i',y_i' \in \mathcal D'} -\log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right].
\end{align}
\end{small}
Thus, $\mathbb E_{\mathcal D}[\Phi(\mathcal D)]$ could be reformulated as:
\begin{small}
\begin{align}
    \mathbb E_{\mathcal D}[\Phi(\mathcal D)] &= \mathbb E_{\mathcal D}\left[ \sup_{f\in \mathcal V, q \in \mathcal Q}  \left| \mathbb E_{\mathcal D'} \left[ -\frac{1}{|\mathcal D'|} \sum_{x_i',y_i' \in \mathcal D'} -\log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right] 
    \right.\right.\\
    &\left.\left. \quad\quad\quad\quad\quad\quad\quad - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \right] \\
    &\leq \mathbb E_{\mathcal D}\left[ \sup_{f\in \mathcal V, q \in \mathcal Q} \mathbb E_{\mathcal D'} \left| -\frac{1}{|\mathcal D'|} \sum_{x_i',y_i' \in \mathcal D'} -\log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right.\right. \\
    &\left.\left.\quad\quad\quad\quad\quad\quad\quad\quad\quad - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \right] \\
    \label{equ:err_1}
    &\leq \mathbb E_{\mathcal D, \mathcal D'} \left[  \sup_{f\in \mathcal V, q \in \mathcal Q} \frac{1}{|\mathcal D|} \left| \sum_{x_i,y_i \in \mathcal D} \log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right.\right.\\
    &\left.\left.\quad\quad\quad\quad\quad\quad\quad\quad\quad - \sum_{x_i',y_i' \in \mathcal D'} \log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right| \right] \\
    \label{equ:err_2}
%    &= \mathbb E_{\mathcal D, \mathcal D', \sigma} \left[  \sup_{f\in \mathcal V, q \in \mathcal Q} \frac{1}{|\mathcal D|} \left| \sum_{x_i,y_i \in \mathcal D} \sigma_i \log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right.\right.\\
    &\left.\left.\quad\quad\quad\quad\quad\quad\quad\quad\quad - \sum_{x_i',y_i' \in \mathcal D'} \sigma_i \log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right| \right] \\
    &\leq \mathbb E_{\mathcal D, \sigma} \left[ \sup_{f\in \mathcal V, q \in \mathcal Q} \frac{1}{|\mathcal D|} \left| \sum_{x_i,y_i \in \mathcal D} \sigma_i \log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \right] \\
    &\quad\; + \mathbb E_{\mathcal D', \sigma} \left[ \sup_{f\in \mathcal V, q \in \mathcal Q} \frac{1}{|\mathcal D'|} \left| \sum_{x_i',y_i' \in \mathcal D'} \sigma_i \log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i') \right| \right] \\
    \label{equ:err_6}
    &= 2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k),
\end{align}
\end{small}
where $\sigma_i$ are independent Rademacher variables. Equation \ref{equ:err_1} follows from Jensen's inequality and the convexity of $\sup$. Equation \ref{equ:err_2} holds due to the symmetry of $\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) - \log f[x_i'](y_i') q(\mathcal E=e_k|x_i',y_i')$ and the argument that Radamacher variables preserve the expected sum of symmetric random variables with a convex mapping (\cite{banach_probability}, Lemma 6.3).

Substituting Equation \ref{equ:err_6} to Equation \ref{equ:err_7}, we have for any $\delta \in (0,1)$, with a probability over $1 - \delta$, $\forall f \in \mathcal V$, $\forall q \in \mathcal Q$, the following holds:
\begin{align}
\label{equ:err_3}
    &\quad\; \left| q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i)   \right|\\
    &\leq 2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
\end{align}
Let $\Tilde{f_k} = \arg \inf_f \{q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log f[x](y)|\mathcal E=e_k \right]\}$. 

Let $\hat{f_k} = \arg \inf_f \{\frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i)  q(\mathcal E=e_k|x_i,y_i)\}$.

Now we have
\begin{align}
    \label{equ:err_4}
    &\quad\; q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log \Tilde{f_k}[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \Tilde{f_k}[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \\
    &\leq q(\mathcal E=e_k)H_{\mathcal V}(Y|X,\mathcal E=e_k)  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f_k}[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \\
    \label{equ:err_5}
    &\leq q(\mathcal E=e_k)\mathbb E_{q} \left[ -\log \hat{f_k}[x](y)|\mathcal E=e_k \right]  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f_k}[x_i](y_i) q(\mathcal E=e_k|x_i,y_i).
\end{align}
Combining Equation \ref{equ:err_3} and Equation \ref{equ:err_4}-\ref{equ:err_5}, the lemma is proved.
\end{proof}

\begin{lemma}
\label{lemma:err_2}
Assume $\forall x \in \mathcal X$,$\forall y \in \mathcal Y$,$\forall f \in \mathcal V$, $\log f[\emptyset](y) \in [-B,B]$ where $B > 0$. The definition of $\mathcal G_{\mathcal V}^k$ and $\mathscr R_{N}(\mathcal G)$ follows from Lemma \ref{lemma:err_1}. Define $\hat f_k = \arg \inf_f \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i)$.

Then for any $q \in \mathcal Q$,  any $\delta \in (0,1)$, with a probability over $1 - \delta$,  we have
\begin{align}
    &\quad\; \left|q(\mathcal E=e_k)H_{\mathcal V}(Y|\mathcal E=e_k)  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f_k}[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \\
    &\leq 2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
\end{align}
\end{lemma}
\begin{proof}
Similar to  Lemma \ref{lemma:err_1}, we could prove that
\begin{align}
    \label{equ:err_8}
    &\quad\; \left|q(\mathcal E=e_k)H_{\mathcal V}(Y|\mathcal E=e_k)  - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f_k}[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \\
    &\leq 2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V^\emptyset}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}},
\end{align}
where $\mathcal G_{\mathcal V^\emptyset}^k = \{g|g(x,y) = \log f[\emptyset](y)q(\mathcal E=e_k|x,y), f\in \mathcal V, q \in \mathcal Q  \}$.

According to the definition for the predictive family $\mathcal V$ (\cite{DBLP:conf/iclr/XuZSSE20}, Definition 1), $\forall f \in \mathcal V$, there exists $f' \in \mathcal V$ such that $\forall x \in \mathcal X$, $f[\emptyset] = f'[x]$. Thus, $\mathcal G_{\mathcal V^\emptyset}^k \subset \mathcal G_{\mathcal V}^k$, and therefore $\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V^\emptyset}^k) \leq \mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k)$. Substituting into Equation \ref{equ:err_8}, the lemma is proved.
\end{proof}

\begin{lemma}[\citep{DBLP:conf/iclr/XuZSSE20}, Theorem 1]
\label{lemma:err_3}
Assume $\forall x \in \mathcal X$,$\forall y \in \mathcal Y$,$\forall f \in \mathcal V$, $\log f[x](y) \in [-B,B]$ where $B > 0$. Define a function class $\mathcal G_{\mathcal V}^* = \{g|g(x,y) = \log f[x](y), f\in \mathcal V\}$. The definition of $\mathscr R_{N}(\mathcal G)$ follows from Lemma \ref{lemma:err_1}. 

Then for any $\delta \in (0,0.5)$, with a probability over $1 - 2\delta$,  we have
\begin{align}
    \left|\mathbb I_{\mathcal V}(X\rightarrow Y)  -  \hat{\mathbb I}_{\mathcal V}(X\rightarrow Y) \right| 
    \leq 4\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^*) + 2B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
\end{align}
\end{lemma}

Finally we are prepared to prove Theorem \ref{theorem:pac}.

% \begin{theorem}
% Assume $\forall x \in \mathcal X$, $\forall y \in \mathcal Y$, $\forall f \in \mathcal V$, $\log f[x](y) \in [-B,B]$ where $B > 0$. Given $e \in \mathrm{supp}(\mathcal E)$, define a function class $\mathcal G_{\mathcal V} = \{g|g(x,y) = \log f[x](y)q(\mathcal E=e|x,y), f\in \mathcal V, q \in \mathcal Q  \}$. Denote the Rademacher complexity of $\mathcal G$ with $N$ samples by $\mathscr R_{N}(\mathcal G)$. Let $K = | \mathrm{supp}(\mathcal E)|$.

% Then for any $\delta \in \left(0,\frac{1}{2(K+1)}\right)$, with a probability over $1 - 2(K+1)\delta$, for dataset $\mathcal{D}$, we have
% \begin{align}
%     |\mathcal H_K^\mathcal V - \hat{\mathcal H}_K^\mathcal V(\mathcal D)| \leq 4(K+1)\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) + 2(K+1)B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}.
% \end{align}
% \end{theorem}

\begin{proof}[Proof of Theorem \ref{theorem:pac}]
We first bound the error of empirical estimation with the sum of items in Lemma \ref{lemma:err_1},\ref{lemma:err_2},\ref{lemma:err_3}.
\begin{small}
\begin{align}
    &\quad\; |\mathcal H_\mathcal V^{\mathscr E_K}(X\rightarrow Y) - \hat{H}_\mathcal V^{\mathscr E_K}(X\rightarrow Y;\mathcal D)| \\
%    &= \left|\left[\sup_{\mathcal E \in \mathscr E_K}\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E})- \mathbb{I}_{\mathcal{V}}(X\rightarrow Y)\right]
%    - \left[\sup_{\mathcal E \in \mathscr E_K}\hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E};\mathcal D)-  \hat {\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)\right]\right| \\
    &\leq \left|\sup_{\mathcal E \in \mathscr E_K} \mathbb{I}_{\mathcal{V}}(X\rightarrow Y| {\mathcal{E}}) - \sup_{\mathcal E \in \mathscr E_K} \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y| {\mathcal{E}};\mathcal D)  \right| 
    + \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right| \\
    &\leq \sup_{\mathcal E \in \mathscr E_K} \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y| {\mathcal{E}}) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y| {\mathcal{E}};\mathcal D) \right| 
    + \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right| \\
    &= \sup_{q\in \mathcal Q}\left| \sum_{k=1}^K \left[{q}(\mathcal E=e_k)H_{\mathcal V}(Y|\mathcal E=e_k) - {q}(\mathcal E=e_k)H_{\mathcal V}(Y|X, \mathcal E=e_k)\right] \right. \\
    &\left. \quad\quad - \sum_{k=1}^K \left[ q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D) - q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D)\right] \right| \\
    &\quad + \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right| \\ 
    &\leq \sum_{k=1}^K \sup_{q\in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|\mathcal E=e_k) -  q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|\mathcal E=e_k;\mathcal D) \right| \\
    &\quad + \sum_{k=1}^K \sup_{q \in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|X, \mathcal E=e_k) -  q(\mathcal E=e_k)\hat H_{\mathcal V}(Y|X, \mathcal E=e_k;\mathcal D) \right| \\
    &\quad + \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right| \\ 
    &= \sum_{k=1}^K \sup_{q\in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|\mathcal E=e_k) - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f}_k[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \\
    &\quad + \sum_{k=1}^K \sup_{q \in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|X, \mathcal E=e_k) 
    - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f}_k'[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i) \right| \\
    &\quad + \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right|,
\end{align}
\end{small}
where $\hat f_k = \arg \inf_f \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[x_i](y_i) q(\mathcal E=e_k|x_i,y_i)$,

and $\hat f_k' = \arg \inf_f \frac{1}{|\mathcal D|}  \sum_{x_i,y_i \in \mathcal D} -\log f[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i)$, for any $q\in \mathcal Q$ and $1\leq k \leq K$. 

For simplicity, let
\begin{small}
\begin{align}
    \mathrm {Err}_k &= \sup_{q \in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|X, \mathcal E=e_k) 
    - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f}_k[x_i](y_i) q(\mathcal E=e_k|x_i,y_i) \right|. \\
    \mathrm {Err}_k' &= \sup_{q \in \mathcal Q}\left| {q}(\mathcal E=e_k)H_{\mathcal V}(Y|X, \mathcal E=e_k) 
    - \frac{1}{|\mathcal D|} \sum_{x_i,y_i \in \mathcal D} -\log \hat{f}_k'[\emptyset](y_i) q(\mathcal E=e_k|x_i,y_i) \right|. \\
    \mathrm {Err}^* &= \left| \mathbb{I}_{\mathcal{V}}(X\rightarrow Y) - \hat{\mathbb{I}}_{\mathcal{V}}(X\rightarrow Y;\mathcal D)  \right|.
\end{align}
\end{small}

Then, by Lemma \ref{lemma:err_1},\ref{lemma:err_2},\ref{lemma:err_3}, 
\begin{small}
\begin{align}
    &\quad\; \mathrm{Pr}\left[|\mathcal H_K^\mathcal V - \hat{\mathcal H}_K^\mathcal V(\mathcal D)| > 4(K+1)\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) + 2(K+1)B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}\right] \\
    &\leq \mathrm{Pr}\left[\sum_{i=1}^K \mathrm {Err}_k + \sum_{i=1}^K \mathrm {Err}_k' + \mathrm {Err}^* > 4(K+1)\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) + 2(K+1)B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}\right] \\
    \label{equ:err_9}
    &\leq  \mathrm{Pr}\left[\sum_{i=1}^K \mathrm {Err}_k + \sum_{i=1}^K \mathrm {Err}_k' + \mathrm {Err}^* > \sum_{k=1}^K 4\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + 4\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^*) + 2(K+1)B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}\right] \\
%    &\leq \mathrm{Pr}\left[ \bigcup_{k=1}^K \left(\mathrm{Err_k > }2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D)|}}\right) + \bigcup_{k=1}^K \left(\mathrm{Err_k' > }2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D)|}}\right) \right. \\
    &\left. \quad\quad\quad + \left(\mathrm{Err}^* > 4\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^*) + 2B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}} \right) \right] \\
%    &\leq \sum_{k=1}^K \mathrm{Pr}\left[ \mathrm{Err_k > }2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D)|}} \right] + \sum_{k=1}^K \mathrm{Pr}\left[ \mathrm{Err_k' > }2\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) + B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D)|}} \right] \\
    &\quad\; + \mathrm{Pr}\left[ \mathrm{Err}^* > 4\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}^*) + 2B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}  \right] \\
    &\leq 2(K+1)\delta.
\end{align}
\end{small}
Equation \ref{equ:err_9} is because of $\mathcal G_{\mathcal V}^k = \mathcal G_{\mathcal V}$, $\mathcal G_{\mathcal V}^* \subset \mathcal G_{\mathcal V}$ and therefore $R_{|\mathcal D|}(\mathcal G_{\mathcal V}^k) \leq R_{|\mathcal D|}(\mathcal G_{\mathcal V})$, $R_{|\mathcal D|}(\mathcal G_{\mathcal V}^*) \leq R_{|\mathcal D|}(\mathcal G_{\mathcal V})$.
Hence,
\begin{small}
\begin{align}
    &\quad \mathrm{Pr}\left[|\mathcal H_\mathcal V^{\mathscr E_K}(X\rightarrow Y) - \hat{H}_\mathcal V^{\mathscr E_K}(X\rightarrow Y;\mathcal D)| \leq 4(K+1)\mathscr R_{|\mathcal D|}(\mathcal G_{\mathcal V}) + 2(K+1)B\sqrt{\frac{2\log{\frac{1}{\delta}}}{|\mathcal D|}}\right] \\
    &\geq 1 - 2(K+1)\delta.
\end{align}
\end{small}
\end{proof}



\section{Proof of Theorem \ref{theorem:IM}}
\label{proof: IM}
\begin{proof}[Proof of Theorem \ref{theorem:IM}]
    The objective function of our IM algorithm is directly derived from the definition of empirical predictive heterogeneity in Definition \ref{def:empirical_predictive_heterogeneity}.
    For the regression task, we assume the predictive family as 
    \begin{small}
\begin{equation}
	\mathcal{V}_1 = \{g: g[x]=\mathcal{N}(f_{\theta}(x), \sigma^2), f\text{ is the regression model and }\theta\text{ is learnable, }\sigma=1.0 (\text{fixed})\},
\end{equation}	
\end{small}
where we only care about the output of the model and the noise scale of the Gaussian distribution is often ignored, for which we simply set $\sigma=1.0$ as a fixed term.
Then for each environment $e\in\text{supp}(\mathcal{E}^*)$, the $\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E}^*=e)$ becomes
\begin{equation}
	\mathbb{I}_{\mathcal{V}}(X\rightarrow Y|\mathcal{E}^*=e)\propto \min_\theta \mathbb{E}^[\|Y-f_\theta(X)\|^2|\mathcal{E}^*=e] - \text{Var}(Y|\mathcal{E}^*),
\end{equation}
which corresponds with the MSE loss and the proposed regularizer in Equation \ref{equ:regularizer-regression}.
For the classification task, the derivation is similar, and the regularizer becomes the entropy of $Y$ in sub-population $e$ and the loss function becomes the cross-entropy loss.
\end{proof}















