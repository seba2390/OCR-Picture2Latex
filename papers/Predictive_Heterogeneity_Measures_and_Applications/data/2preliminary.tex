\section{Preliminaries on Mutual Information and Predictive $\mathcal V$-Information}
In this section, we briefly introduce the mutual information and predictive $\mathcal V$-information \citep{DBLP:conf/iclr/XuZSSE20} which are the preliminaries of our proposed predictive heterogeneity.\\\\
\textbf{Notations.} For a probability triple $(\mathbb S,  \mathcal F, \mathbb P)$, define random variables $X: \mathbb S\rightarrow \mathcal X$ and $Y: \mathbb S\rightarrow \mathcal Y$ where $\mathcal X$ is the covariate space and $\mathcal Y$ is the target space. Accordingly. $x \in \mathcal X$ denotes the covariates, and $y\in\mathcal{Y}$ denotes the target. Denote the set of random categorical variables as $\mathcal C = \{ C: \mathbb S \rightarrow \mathbb N| \text{supp}(C) \;\text{is finite} \}$. Additionally, $\mathcal{P}(\mathcal{X}), \mathcal{P}(\mathcal Y)$ denote the set of all probability measures over the Borel algebra on the spaces $\mathcal{X}, \mathcal{Y}$ respectively. 
$H(\cdot)$ denotes the Shannon entropy of a discrete random variable and the differential entropy of a continuous variable, and $H(\cdot|\cdot)$ denotes the conditional entropy of two random variables.




In information theory, the mutual information of two random variables $X$, $Y$ measures the dependence between the two variables, which quantifies the reduction of entropy for one variable when observing the other:
\begin{small}
\begin{equation}
	\mathbb{I}(X;Y) = H(Y) - H(Y|X).
\end{equation}	
\end{small}
It is known that the mutual information is associated with the predictability of $Y$ \citep{cover1991infomationtheory}. While the standard definition of mutual information unrealistically assumes the unbounded computational capacity of the predictor, rendering it hard to estimate especially in high dimensions.
To mitigate this problem, \cite{DBLP:conf/iclr/XuZSSE20} raise the predictive $\mathcal V$-information under realistic computational constraints, where the predictor is only allowed to use models in the predictive family $\mathcal V$ to predict the target variable $Y$.

\begin{definition}[Predictive Family \citep{DBLP:conf/iclr/XuZSSE20}]
	Let $\Omega=\{f:\mathcal{X}\cup\{\emptyset\}\rightarrow \mathcal{P}(\mathcal Y)\}$. We say that $\mathcal V \subseteq \Omega$ is a predictive family if it satisfies:
	\begin{small}
	\begin{equation}
	\label{equ:condition}
		\forall f\in\mathcal{V},\ \  \forall P\in \mathrm{range}(f),\ \  \exists f'\in\mathcal{V}, \quad\text{s.t. }\forall x\in\mathcal{X}, f'[x]=P, f'[\emptyset]=P.
	\end{equation}
	\end{small}
\end{definition}
A predictive family contains all predictive models that are allowed to use, which forms computational or statistical constraints.
The additional condition in Equation \ref{equ:condition} means that the predictor can always ignore the input covariates ($x$) if it chooses to (only use $\emptyset$).

\begin{definition}[Predictive $\mathcal V$-information \citep{DBLP:conf/iclr/XuZSSE20}]
\label{def:predictive_v_information}
	Let $X, Y$ be two random variables taking values in $\mathcal{X}\times\mathcal{Y}$ and $\mathcal V$ be a predictive family. The predictive $\mathcal V$-information from $X$ to $Y$ is defined as:
	\begin{small}
	\begin{equation}
		\mathbb{I}_{\mathcal V}(X\rightarrow Y) = H_{\mathcal V}(Y|\emptyset)-H_{\mathcal V}(Y|X),
	\end{equation}	
	\end{small}
	where $H_{\mathcal V}(Y|\emptyset)$, $H_{\mathcal V}(Y|X)$ are the predictive conditional $\mathcal V$-entropy defined as:
	\begin{small}
	\begin{align}
		H_{\mathcal V}(Y|X) &= \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{x,y\sim X,Y}[-\log f[x](y)]. \\
		H_{\mathcal V}(Y|\emptyset) &= \inf\limits_{f\in\mathcal{V}}\mathbb{E}_{y\sim Y}[-\log f[\emptyset](y)].
	\end{align}	
	\end{small}
	Notably that $f\in\mathcal V$ is a mapping: $\mathcal{X}\cup\{\emptyset\}\rightarrow \mathcal{P}(\mathcal Y)$, so $f[x]\in\mathcal{P}(\mathcal{Y})$ is a probability measure on $\mathcal{Y}$, and $f[x](y)\in\mathbb{R}$ is the density evaluated on $y\in\mathcal Y$. $H_{\mathcal V}(Y|\emptyset)$ is also denoted as $H_{\mathcal V}(Y)$.
\end{definition}


Compared with the mutual information, the predictive $\mathcal V$-information restricts the computational power and is much easier to estimate in high-dimensional cases.
When the predictive family $\mathcal V$ contains all possible models, i.e. $\mathcal V = \Omega$, it is proved that $\mathbb{I}_{\mathcal V}(X\rightarrow Y)=\mathbb{I}(X;Y)$ \citep{DBLP:conf/iclr/XuZSSE20}.


























