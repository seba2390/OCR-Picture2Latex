\appendix

\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}

\begin{table}[t!]
\centering
\textbf{ViT-Large}
\resizebox{0.95\linewidth}{!}{
\begingroup
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ | c | c c c c | c | }
\hline
\multirow{2}{*}{Method}   &   \multirow{2}{*}{\#Patches}  &  \multirow{2}{*}{GMACs}   &  Throughput  & Runtime  & ImageNet-1k   \\
   &  &  & ims/sec   & $\mu$-secs/im   & Top-1 Acc.   \\
\hline
\multirow{5}{*}{Vanilla ViT}   & 64 &  19.9   & 900   & 1111   & 82.00   \\
   & 81  & 25.2   & 720   & 1389   & 83.02   \\
   & 100 & 31.1   & 580   & 1724   & 83.86   \\
   & 121 & 37.7   & 478   & 2092   & 84.46   \\
   & 169 & 53.0   & 323   & 3096   & 85.42   \\
   & 196 & 61.7   & 277   & 3610   & 85.74   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Feature-based scorer}}}   & 64   & 20.1   & 777   & 1287   & 82.88   \\
   & 79  &  24.7  & 649   & 1541   & 83.67   \\
   & 100 &  31.3  & 527   & 1898   & 84.41   \\
   & 121 &  37.9  & 440   & 2273   & 85.03   \\
   & 169 &  53.1  & 306   & 3268   & 85.65   \\
   & 196 &  61.8  & 265   & 3774   & 85.79   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Pixel-blur scorer}}}  & 64  & 19.9   & 869   & 1151  & 81.66  \\
 & 79  & 24.6   & 712   & 1404  & 82.69  \\
 & 100 & 31.1   & 568   & 1761  & 83.61  \\
 & 121 & 37.7   & 470   & 2128  & 84.3   \\
 & 169 & 53.0   & 320   & 3125  & 85.22  \\
 & 196 & 61.7   & 275   & 3636  & 85.56  \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Oracle scorer}}}   & 64   &  ---   &  ---   & ---   & 85.89   \\
   & 79    &  ---   &  ---   & ---   & 86.33   \\
   & 100   &  ---   &  ---   & ---   & 86.5   \\
   & 121   &  ---   &  ---   & ---   & 86.7   \\
   & 169   &  ---   &  ---   & ---   & 86.52   \\
   & 196   &  ---   &  ---   & ---   & 86.54   \\
\hline
\end{tabular}
\endgroup
}% close resizebox
\vspace*{0.2cm}
\caption{Full results - ViT-Large.}
\label{table:results_vit_large}
\end{table}




\begin{table}[t!]
\vspace*{0.5cm}
\centering
\textbf{ViT-Base}
\resizebox{0.95\linewidth}{!}{
\begingroup
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ | c | c c c c | c | }
\hline
\multirow{2}{*}{Method}   &   \multirow{2}{*}{\#Patches}  &  \multirow{2}{*}{GMACs}   &  Throughput  & Runtime  & ImageNet-1k   \\
   &  &  & ims/sec   & $\mu$-secs/im   & Top-1 Acc.   \\
\hline
\multirow{5}{*}{Vanilla ViT}   & 64  &  5.6 & 2676   & 374   & 80.78   \\
   & 81  &  7.2  & 2155   & 464    & 81.73   \\
   & 100 &  8.8  & 1739   & 575    & 82.31   \\
   & 121 &  10.7  & 1429   & 700    & 82.71   \\
   & 169 &  15.1  & 966    & 1035   & 83.74   \\
   & 196 &  17.6  & 823    & 1215   & 84.07   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Feature-based scorer}}}   &   64    & 5.7  &   2019   &   495   &   81.52   \\
   &   79   & 7.1  &   1732   &   577   &   82.34   \\
   &   100  & 8.9  &   1435   &   697   &   83.05   \\
   &   121  & 10.8 &   1218   &   821   &   83.50   \\
   &   169  & 15.2 &   864    &   1157  &   84.23   \\
   &   196  & 17.7 &   750    &   1333  &   84.38   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Pixel-blur scorer}}}   & 64 &  5.7  & 2424   & 413   & 80.78   \\
   & 79  & 7.0  & 2021   & 495   & 81.68  \\
   & 100 & 8.8  & 1630   & 613   & 82.57  \\
   & 121 & 10.7 & 1354   & 739   & 83.06   \\
   & 169 & 15.1 & 931    & 1074  & 83.87   \\
   & 196 & 17.6 & 800    & 1250  & 84.23   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Oracle scorer}}}   & 64   &  ---   &  ---   & ---   & 84.76   \\
   & 79    &  ---   &  ---   & ---   & 85.19   \\
   & 100   &  ---   &  ---   & ---   & 85.40   \\
   & 121   &  ---   &  ---   & ---   & 85.67   \\
   & 169   &  ---   &  ---   & ---   & 85.40   \\
   & 196   &  ---   &  ---   & ---   & 85.25   \\
\hline
\end{tabular}
\endgroup
}% close resizebox
\vspace*{0.2cm}
\caption{Full results - ViT Base.}
\label{table:results_vit_base}
\end{table}






\begin{table}[t!]
\centering
\textbf{ViT-Small}
\resizebox{0.95\linewidth}{!}{
\begingroup
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ | c | c c c c | c | }
\hline
\multirow{2}{*}{Method}   &   \multirow{2}{*}{\#Patches}  &  \multirow{2}{*}{GMACs}   &  Throughput  & Runtime  & ImageNet-1k   \\
   &  &  & ims/sec   & $\mu$-secs/im   & Top-1 Acc.   \\
\hline
\multirow{5}{*}{Vanilla ViT}    & 64    & 1.44   & 6489   & 154   & 74.55   \\
 & 81    & 1.83   & 5208   & 192   & 76.36   \\
 & 100   & 2.28   & 4212   & 237   & 77.55   \\
 & 121   & 2.78   & 3460   & 289   & 78.26   \\
 & 169   & 3.94   & 2315   & 432   & 79.84   \\
 & 196   & 4.62   & 1975   & 506   & 80.28   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Feature-based scorer}}}   & 64    & 1.54   & 3611   & 277   & 76.53   \\
 & 79    & 1.88   & 3204   & 312   & 77.53   \\
 & 100   & 2.37   & 2766   & 362   & 78.64   \\
 & 121   & 2.87   & 2419   & 413   & 79.35   \\
 & 169   & 4.04   & 1792   & 558   & 80.43   \\
 & 196   & 4.71   & 1576   & 635   & 80.84   \\
\hline
\multirow{5}{*}{\shortstack[c]{Quadformer \\ \small{Pixel-blur scorer}}}    & 64    & 1.45   & 5150   & 194   & 74.97   \\
 & 79    & 1.79   & 4362   & 229   & 76.27   \\
 & 100   & 2.28   & 3590   & 279   & 77.47   \\
 & 121   & 2.78   & 3022   & 331   & 78.58   \\
 & 169   & 3.95   & 2104   & 475   & 80.01   \\
 & 196   & 4.62   & 1813   & 552   & 80.4   \\
\hline
\end{tabular}
\endgroup
}% close resizebox
\vspace*{0.2cm}
\caption{Full results - ViT Small.}
\label{table:results_vit_small}
\end{table}




\vspace*{10pt}
\section{Full results}
We report ImageNet-1k top-1 accuracy and various cost indicators for every model configuration that appears in the figures of the main text (see Table \ref{table:results_vit_large}, Table \ref{table:results_vit_base}, Table \ref{table:results_vit_small}). Throughput is measured on a single GeForce RTX 3090 GPU in mixed precision.


\section{More implementation details}
\paragraph{Hyperparameters.} We train all of our models using the timm library~\cite{rw2019timm} with the following hyperparameters: learning rate warmup for 5 epochs, learning rate cooldown for 10 epochs, cosine learning rate scheduler~\cite{Loshchilov2016SGDRSG}, weight decay 0.025, DropPath~\cite{Huang2016DeepNW} rate 0.1, AdamW~\cite{Loshchilov2017DecoupledWD} optimizer with epsilon $1\text{e-}8$, AutoAugment~\cite{Cubuk2018AutoAugmentLA} image augmentations with configuration \verb|rand-m9-mstd0.5-inc1|, mixup~\cite{Zhang2017mixupBE} alpha 0.8, cutmix~\cite{Yun2019CutMixRS} alpha 1.0, label smoothing 0.1. Unless otherwise specified, we use base learning rate $5\text{e-}5$.

We fine-tune ViT-Small models for 130 epochs with batch size 1024, ViT-Base models for 60 epochs with batch size 400, and ViT-Large models for 20 epochs with batch size 192. For evaluation, we use exponential moving average (EMA)~\cite{Polyak1992AccelerationOS} with decay 0.99996. We use the default values in timm for all other hyperparameters.
