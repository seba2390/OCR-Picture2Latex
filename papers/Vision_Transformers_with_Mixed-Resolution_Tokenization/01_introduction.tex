\section{Introduction}
Transformer~\cite{Vaswani2017AttentionIA} models are designed to process sequential input data. Vision Transformer (ViT)~\cite{Dosovitskiy2020AnII} models process input images that naturally have two spatial dimensions, requiring a spatially-aware tokenization scheme to convert them into sequences. The vast majority of Vision Transformers convert the input image into a two-dimensional grid of token vectors, before flattening it to create a one-dimensional sequence. Specifically, most methods use uniform patch tokenization, splitting the image into a spatially regular grid of equal-size patches.

In natural language processing, input tokenization looks entirely different. Almost all modern neural networks for text processing use subword tokenization, where each token represents a substring of arbitrary character length~\cite{Sennrich2015NeuralMT,Kudo2018SubwordRI}.
In this work, we apply this approach to ViTs by introducing a novel image tokenization scheme, replacing the standard uniform grid with a mixed-resolution sequence of tokens, where each token represents a patch of arbitrary size.



\begin{figure}[t!]
  \centering
  \includegraphics[width=0.82\linewidth]{figures/quadformer.png}
  \caption{The Quadformer. We split the image into a mixed-resolution patch mosaic according to a saliency scorer, and employ a standard Transformer architecture with 2D position embeddings.}
  \label{figure:quadformer}
\end{figure}


Previous works tried to incorporate multi-resolution processing into Vision Transformers by building feature pyramids inspired by the structure of CNNs~\cite{Wang2021PyramidVT,Graham2021LeViTAV}, using multi-resolution attention~\cite{Yang2021FocalSF,Tang2022QuadTreeAF}, or merging intermediate token representations from across the entire image without preserving spatial locality~\cite{Renggli2022LearningTM,Bolya2022TokenMY}. In contrast, our work is the first to use mixed-resolution \textit{tokenization}, directly splitting the input image into a patch mosaic processed by a standard Transformer model (see Figure \ref{figure:quadformer}).

\manualfootnote{$^{\text{*}}$The author was affiliated with Alibaba Group during parts of the research.}

Instead of using a spatially regular patch grid, we construct a patch mosaic where low-saliency areas of the image are processed in low resolution, routing more of the modelâ€™s capacity to important areas. Practically, we use the Quadtree algorithm~\cite{Markas1992QuadTS} to recursively split the image into patches of different sizes, incorporating a saliency scorer that chooses which areas of the image to split by their estimated importance. We use 2D position embeddings to represent the location of each patch.

We evaluate our method, dubbed \textit{Quadformer}, on the ImageNet-1k~\cite{Russakovsky2014ImageNetLS} classification dataset, and compare our mixed-resolution models to vanilla ViT models that use the same architecture. While vanilla ViT models utilize uniform grid tokenizations with a single patch size (in our case, the standard $16^2$ pixels), our mixed-resolution tokenization uses 3 patch sizes ($64^2$, $32^2$ and $16^2$ pixels), allowing our Quadformer models to process important image regions in high resolution even when using a small number of patches. Using a novel saliency scorer based on neural representations, we consistently beat the accuracy of vanilla ViTs by up to 0.88 absolute percentage points when controlling for the number of patches or GMACs. Despite not using dedicated tools for accelerated inference, we also show gains when controlling for inference speed, beating vanilla ViT models by up to 0.42 absolute percentage points.
