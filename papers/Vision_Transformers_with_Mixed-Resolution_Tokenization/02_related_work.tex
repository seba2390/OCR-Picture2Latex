\section{Background and related work}

\paragraph{Efficient Vision Transformers.} Many efficient architectures were proposed for improving the speed-accuracy tradeoff of Vision Transformers, mostly by using attention layers with linear time complexity~\cite{Arar2021LearnedQF,Liu2021SwinTH,Tu2022MaxViTMV}, dropping a subset of patches~\cite{Meng2021AdaViTAV,Rao2021DynamicViTEV,Yin2021AViTAT}, or merging intermediate token representations from the entire image~\cite{Renggli2022LearningTM,Bolya2022TokenMY}. Our method offers orthogonal improvements as we decrease the number of patches via tokenization, maintaining global attention over the entire image while using spatially-local tokens.


\paragraph{Vision Transformers with spatially uniform grids.} Standard Vision Transformer models process input images by dividing them into a regular grid of equal size patches. Even in the case of pyramid vision transformers~\cite{Wang2021PyramidVT,Liu2021SwinTH}, which gradually compress the spatial dimension of the feature map as the network progresses, vectors in the same feature map always represent input areas of the same size. This is a classical design choice used extensively with CNNs, as it fits the constraints of convolution layers, that must operate on a spatially-regular grid. However, the layers that form the Transformer model, namely self-attention layers and fully connected layers, have no such limitations. Transformer models can process any set of input vectors that have some defined positional relationship, and are naturally suited to handling inputs of different scales. For example, Transformer language models process input tokens that represent subwords of very different lengths -- the BERT~\cite{Devlin2019BERTPO} vocabulary has tokens in lengths ranging from 1 character (``a'', ``b'') to 18 characters (``telecommunications'').

\begin{figure}[t!]
  \centering
  %\hspace*{-0.025\linewidth}
  \vspace*{-5pt}
  \includegraphics[width=0.69\linewidth]{figures/tokenization_examples_grid.png}
  \caption{Tokenizations obtained using our saliency-based Quadtree. For clearer visualization, we upsample patches back to their original size after the tokenizer resizes them to a fixed representation size. Notice how high-saliency regions are represented in high resolution while background regions are blurry.}
  \label{figure:more_quadtree_examples}
\end{figure}

\paragraph{Existing methods for image tokenization.} Not all Vision Transformers use the standard uniform grid tokenization scheme. Some methods use CNN backbones to create representations from input images, using the activation volumes as tokens~\cite{Xiao2021EarlyCH,Graham2021LeViTAV}. Another class of Vision Transformers designed for image generation uses vector-quantization networks to learn a codebook of discrete tokens, also using a uniform two-dimensional grid~\cite{Esser2020TamingTF,Ramesh2021ZeroShotTG}. Few methods forgo spatial tokenization altogether and employ a technique called token learning, where each token aggregates information from the entire image~\cite{Ryoo2021TokenLearnerAS}.

\paragraph{Quadtrees.} Quadtrees are data structures that recursively split a two-dimensional space into a tree of quadrants, where each internal node has exactly four children. Each node in the tree represents a specific spatial area defined by an axis-aligned rectangle or square. Leaf nodes store the information contained in the area they represent. Quadtrees were originally developed for fast retrieval of 2D points~\cite{Finkel1974QuadTA}. They were quickly adapted for image analysis~\cite{Hunter1979OperationsOI}, and later for image compression~\cite{Markas1992QuadTS}.


\begin{figure*}[t!]
  \vspace*{-5pt}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/quadtree_comparison.png}
  \caption{The effect of different patch scorers on Quadtree tokenization. Better saliency estimator $\rightarrow$ higher resolution in important areas. The pixel-blur scorer is often used for image compression, as it focuses on high-frequency details. Our feature-based scorer estimates patch saliency using neural representations. The oracle scorer uses the Grad-CAM saliency estimation algorithm.}
  \label{figure:quadtree_comparison}
\end{figure*}


\paragraph{Quadtrees and neural networks.} Few successful attempts have been made to integrate the Quadtree algorithm with neural networks. To the best of our knowledge, our work is the first to use Quadtree representations of RGB images as inputs to a neural net.

Jewsbury \etal~\cite{Jewsbury2021AQI} use Quadtrees to divide large pathology images into smaller subimages, with each subimage individually processed by a standard CNN. Many works on 3D shape analysis~\cite{Riegler2016OctNetLD,Tatarchenko2017OctreeGN,Wang2017OCNN} use specialized CNN architectures to process Octrees~\cite{Meagher1980Octree}, the 3D equivalent of Quadtrees. Jayaraman \etal~\cite{Jayaraman2018QuadtreeCN} use Quadtrees with sparse CNNs to process simple black-and-white sketches, avoiding computation in blank areas of the image. Chitta \etal~\cite{Chitta2019QuadtreeGN} use Quadtrees with a sparse CNN decoder to predict hierarchical segmentation maps, avoiding excessive computation in large image regions that share the same class. Tang \etal~\cite{Tang2022QuadTreeAF} propose an efficient attention implementation for ViTs, where each query vector in a spatially uniform grid attends to a Quadtree of key-value vectors. Ke \etal~\cite{Ke2021MaskTF} use Quadtrees for efficient refinement of instance segmentation masks, focusing computation in incoherent regions. They employ a Transformer model over a Quadtree of feature vectors extracted from a CNN feature pyramid.

%Ke \etal~\cite{Ke2021MaskTF} use Quadtrees for efficient refinement of instance segmentation masks, focusing computation in sparse incoherent regions such as object boundaries. They employ a Transformer model over a Quadtree of feature vectors extracted from a CNN feature pyramid.
