\section{Conclusion}
We have presented a novel tokenization scheme for Vision Transformers, replacing the standard uniform patch grid with a mixed-resolution sequence of tokens, where each token represents a patch of arbitrary size. We integrated the Quadtree algorithm with a novel feature-based saliency scorer to create mixed-resolution patch mosaics, making this work the first to use the Quadtree representations of RGB images as inputs for a neural network.

Through experiments in image classification, we have shown the capacity of standard Vision Transformer models to adapt to mixed-resolution tokenization via fine-tuning. Our Quadformer models achieve substantial accuracy gains compared to vanilla ViTs when controlling for the number of patches or GMACs. Although we do not use dedicated tools for accelerated inference, Quadformers also show gains when controlling for inference speed.

We believe that future work could successfully apply mixed-resolution ViTs to other computer vision tasks, especially those that involve large images with heterogeneous information densities, including tasks with dense outputs such as image generation and segmentation.
