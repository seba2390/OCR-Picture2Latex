\section{Fusion of Local GM-PHD Filters}
\label{sec:fusion_step}

In addition to local GM-PHD filtering, the communication channels, i.e., the edges in $\mathcal E$, can serve as additional sources of information for each sensor.
In theory, the optimal solution to the given multi-sensor multi-target problem can be realized by aggregating all the measurements of the sensor network as $\bigcup_{i\in \mathcal V}Z_{i,k}$ at each sensor, at each timestep. Since the communication and computational cost of such an approach does not scale well with the number of sensors in the network (which is equal to $|\mathcal V|$), it is infeasible when one or more of the following challenges are taken into consideration: 
\begin{enumerate}
    \item the total number of sensors in the network is large,
    \item the inter-sensor communication channels have limited bandwidths, or 
    \item the communication is not instantaneous, i.e., there are significant delays between the transmission and reception of inter-sensor communications.
\end{enumerate}
A distributed solution to the multi-sensor multi-target tracking problem can solve each of the preceding challenges. Distributed PHD filtering is achieved by fusing the intensity functions $v_{i,k|k}$ of the individual sensors, as these are typically much smaller in dimension than the measurement sets $Z_{i,k}$. For instance, the measurements in $Z_{i,k}$ may be high-dimensional camera images, whereas $v_{i,k|k}$ has a concise representation in terms of its GM components.

\subsection{Weighted Arithmetic Average (WAA) Fusion}
The weighted arithmetic average (WAA) of the local posterior intensity functions of the sensors is defined as
\begin{equation}
    v^{*}_{k|k}(x) = \sum_{i \in \mathcal V} \omega_i v_{i,k|k}(x)
    \label{eq:WAA_def}
\end{equation}
where $\omega_i \geq 0$ and $\sum_{i\in \mathcal V} \omega_i = 1$. In \cite{da2020kullback}, it was shown that the WAA (\ref{eq:WAA_def}) minimizes the weighted Kullback-Leibler (KL) divergence between itself and the local posterior intensities, $v_{i,k|k}$, when the fused intensity is taken as the second argument of the divergence. Similarly, it was shown in \cite{gostar2017cauchy} that the WAA also minimizes the weighted Cauchy-Schwarz (CS) divergence (which is symmetric in its arguments). Thus, we have
\begin{align}
    v^{*}_{k|k}(x) &= \arg \min_{g} \left( \sum_{i\in \mathcal V} \omega_i D_{KL}\left(v_{i,k|k}(x)\mathrel{\Vert}g(x)\right)\right) \\&= \arg \min_{g}\left( \sum_{i\in \mathcal V} \omega_i D_{CS}\left(v_{i,k|k}(x)\mathrel{\Vert}g(x)\right)\right)
    \label{eq:csd}
\end{align}
where $D_{KL}$ and $D_{CS}$ denote the KL and CS divergences, respectively.
Either choice of divergence can be considered as a cost function for the local PHD fusion process, analogous to how the weighted Euclidean distance is used as the cost function in single-target tracking. By minimizing the divergence, the information gain between the local posterior intensities and the fused intensities is minimized; so the WAA (\ref{eq:WAA_def}) is optimal in terms of the \textit{principle of minimum discrimination of information (PMDI)} \cite{gostar2017cauchy, gao2020multiobject}.

The weights $\omega_i$ can be chosen as $\omega_i =\sfrac{1}{|\mathcal V|}$ if the sensor network is homogeneous, i.e., all the sensors have the same sensing capability, measurement noise covariance and detection probability. When this is not the case, a larger weight can be assigned to sensors having better sensing capability. The connectivity of the sensor network $\mathcal G$ can also inform the choice of $\omega _i$. 
% The design of the weights $\omega _i$ is not developed further in this paper, serving as an avenue for future research.

Additionally, note that by taking an integral on both sides of (\ref{eq:WAA_def}), we have

\begin{equation}
    \int_{\mathcal X} v^{*}_{k|k}(x)\hspace{1pt}dx = \sum_{i \in \mathcal V} \omega_i \left(\int_{\mathcal X} v_{i,k|k}(x)\hspace{1pt}dx\right)
    \label{eq:integral_WAA}
\end{equation}
From the properties of Poisson RFS intensities, it follows that the integral of an intensity is the expected value of the cardinality of the corresponding RFS. Thus, WAA fusion of the intensities $v_{i,k|k}$ entails WAA fusion of the cardinality estimates of the sensors. The converse is not true, as two different functions may integrate to the same value. Thus, the proposed approach is different from the one used in \cite{li2018cardinality}, which directly computes the WAA of the cardinality estimates.

\subsection{Distributed WAA Fusion using Consensus}

To realize WAA fusion at each sensor in a distributed manner, an average (weighted) consensus algorithm can be used. In each iteration of the average consensus algorithm, the sensors communicate the GM components of their posterior intensity to their out-neighbors.
Thereafter, each sensor fuses its posterior intensities with those of its in-neighbors using a weighted combination, where the weights $\Omega_{ij}\in \mathbb R$ correspond to the entries of a matrix $\Omega = [\Omega_{ij}]$, with $\Omega_{ij}\neq 0$ if and only if $(j,i)\in \mathcal E$.
The average consensus algorithm is described in Algorithm \ref{alg:con}, which uses a total of $\alpha \geq 1$ inter-sensor fusion iterations.

\begin{algorithm}
\caption{Average Consensus of Posterior Intensities}
\begin{algorithmic}[1]
\vspace{2pt}
\REQUIRE The consensus weights $\Omega_{ij}$, posterior intensities $v_{i,k|k}$, and number of inter-sensor fusion steps $\alpha \geq 1$.\\
\vspace{3pt} 
\STATE Set $v_{i,k|k}^{(0)} \leftarrow v_{i,k|k} \forall i\in \mathcal V$
\FOR{$l=1,2,\dots,\alpha$}
\STATE Each sensor communicates its fused posterior intensity $v^{(l-1)}_{j,k|k}(x)$ to its out-neighbors, $N^+_i$. \vspace{2pt}
\STATE Update the posterior intensities as \begin{equation}
    v^{(l)}_{i,k|k}(x) \leftarrow \sum_{j \in N^{-}_i \cup \lbrace i\rbrace} \Omega_{ij} \hspace{2pt} v^{(l-1)}_{j,k|k}(x)
    \label{eq:average_consensus}
\end{equation}
$\forall i\in\mathcal V$.
\ENDFOR
\RETURN The fused posterior intensity, $v^{(\alpha)}_{i,k|k}(x)$.
\end{algorithmic}
\label{alg:con}
\end{algorithm}

\subsubsection{Error Analysis of Algorithm \ref{alg:con}}
\label{sec:subsec_error_analysis}
At the $l^{th}$ iteration of the average consensus algorithm (Algorithm \ref{alg:con}),
the error between the $i^{th}$ sensor's posterior intensity and the WAA is $v^{(l)}_{i,k|k}-v^*_{k|k}$. To establish the convergence properties of the algorithm, we need to show that the average consensus step (\ref{eq:average_consensus}) drives this error to $0$ (i.e., $v^{(l)}_{i,k|k}-v^*_{k|k}$ approaches the function which is $0$ everywhere in its domain) at each of the sensors.
To facilitate this analysis, let us assume that the intensities $v_{i,k|k}$ are elements in the $L^p(\mathcal X)$ function space, i.e., for some $1\leq p\leq \infty$,
\begin{equation}
\|v_{i,k|k}\|_{L^p} =  \left(\int_\mathcal X |v_{i,k|k}(x)|^p dx\right)^{\frac{1}{p}}< \infty
\end{equation}
$\forall i\in \mathcal V$. Recall that the integral of $v_{i,k|k}$ is sensor $i$'s posterior estimate of the cardinality of the RFS $X_k$. If $p=1$, then $v_{i,k|k}\in L^1(\mathcal X)$ if and only if sensor $i$'s cardinality estimate is bounded. Hence, the assumption that $v_{i,k|k}\in L^p(\mathcal X)$  is reasonable.

We define the vector space $V = \left(L^p(\mathcal X)\right)^{|\mathcal V|}$, so that we can collectively represent the set of intensities $\lbrace v_{i,k|k}\rbrace_{i\in\mathcal V}$ as a vector in $V$:
\[\bar v_{k|k} = \begin{bmatrix} v_{1, k|k} & v_{2, k|k} & \dots & v_{|\mathcal V|, k|k}\end{bmatrix}^\intercal \in V\]
For vectors in $V$, the addition and scalar multiplication operations are defined as the pointwise addition and pointwise scalar multiplication of their components.
We endow $V$ with a norm $\|\mathrel\cdot\|_V$, defined as
\begin{equation}
\| \bar v_{k|k}\|_V = \left(\sum_{i=1}^{|\mathcal V|} \|v_{i,k|k}\|_{L^p} ^p\right)^{\frac{1}{p}}
\end{equation}
Thus, $(V, \|\mathrel\cdot\|_V)$ is a normed vector space. For a matrix $A$, let $\|A\|_{\textsc{op}}$ refer to its operator norm\footnote{If we set $p=2$ in the foregoing definitions, then $V$ can be made into a Hilbert space, in which case $\|A\|_{\textsc{op}}$ is equal to the largest singular value of $A$ \cite[Thm. 4, p. 40]{halmos1957introduction}.}, given by
\begin{equation}
\|A\|_{\textsc{op}} = \sup \left\lbrace \frac{\|A v\|_V}{\|v\|_V} \mathrel{:} v\neq 0, v\in V \right\rbrace
\end{equation}
% We discuss the convergence of the average consensus step (\ref{eq:average_consensus}). 
% This type of abstraction is commonly used in the literature on consensus-based filtering, which either assumes that the local filters have reached their steady states, or that a large number of average consensus steps are carried out between successive measurement steps \cite{li2018partial, consensus_ukf}.
With the above definitions in place, we can state the conditions for the convergence of the average consensus step (\ref{eq:average_consensus}). 

\begin{theorem}
Suppose the following conditions are satisfied:
\begin{enumerate}
%\item $\Omega_{ij} \geq 0$ and $\Omega_{ii}>0$ ${\forall i,j\in\mathcal V}$
%\item $\Omega_{ij}\neq0$ if and only if $(j,i)\in \mathcal E$
\item $\Omega \mathbbm 1 = \mathbbm 1$, where $\mathbbm 1 = \begin{bmatrix}
1 & 1 & \dots & 1\end{bmatrix}^\intercal$,
% i.e., $\Omega$ is row stochastic
%
\item $\bar \omega^\intercal \Omega = \bar \omega^\intercal$, where $\bar \omega  = [\omega _1, \omega _2, \dots, \omega _{|\mathcal V|}]^\intercal$, and
%
\item $\|{\Omega - \mathbbm 1 \bar{\omega}^\intercal} \|_{\textsc{op}} < 1$,
% references: halmos1957introduction, bartle2014elements
\end{enumerate}
then 
% by a repeated application of the average consensus step (\ref{eq:average_consensus}), the intensities 
the fused posterior intensities $\lbrace v_{i,k|k}^{(\alpha)} \rbrace_{i\in \mathcal V}$ computed using Algorithm \ref{alg:con}
asymptotically converge to the WAA in the $L^p$ norm:
\begin{align}
   \lim_{\alpha\rightarrow \infty} \| v^{(\alpha)}_{i,k|k} - v^{*}_{k|k}\|_{L^p} \rightarrow 0, \quad \forall i\in\mathcal V
    \label{eq:convergence}
\end{align}
As a consequence, there is a subsequence $\lbrace l_1, l_2, \dots \rbrace$ such that $\lim_{s\rightarrow \infty} v^{(l_s)}_{i,k|k} = v^{*}_{k|k}$ almost everywhere \cite[p. 75]{bartle2014elements}.
\end{theorem}
\begin{proof}
To show (\ref{eq:convergence}), we can rewrite the average consensus step (\ref{eq:average_consensus}) as $ \bar v^{(l)}_{k|k} = \Omega \bar v^{(l-1)}_{k|k}$. 
%The WAA can be expressed as $v^*_{k|k} = \bar \omega ^\intercal \bar v^{(0)}_{k|k}$.  
Observe that,  given $ l \geq 1$
\begin{equation}
\bar \omega ^\intercal \bar v^{(l)}_{k|k} = \bar \omega ^\intercal \Omega \bar v^{(l-1)}_{k|k} = \bar \omega ^\intercal \bar v^{(l-1)}_{k|k}
\end{equation}
By induction, we have that $\bar \omega ^\intercal \bar v^{(l)}_{k|k} = \bar \omega ^\intercal \bar v^{(0)}_{k|k} = v^*_{k|k}$; in other words, the weighted average is invariant under the average consensus step (\ref{eq:average_consensus}).
Thus, the error dynamics corresponding to the average consensus step is
\begin{align}
\bar v^{(l+1)}_{k|k} - \mathbbm 1 \bar v^*_{k|k} &= \Omega \bar v^{(l)}_{k|k} - \mathbbm 1  \bar \omega ^\intercal \bar v^{(l)}_{k|k}\\
&=\left(\Omega  - \mathbbm 1  \bar \omega ^\intercal \right) \bar v^{(l)}_{k|k} \\
&=\left(\Omega  - \mathbbm 1  \bar \omega ^\intercal \right) (\bar v^{(l)}_{k|k}-\mathbbm 1 \bar v^*_{k|k})
\end{align}
where, in the last step, we used the facts that $\Omega \mathbbm 1 = \mathbbm 1$ and $\bar \omega ^\intercal \mathbbm 1 = 1$. Consequently, we have
\begin{equation}
\|\bar v^{(l)}_{k|k} - \mathbbm 1 \bar v^*_{k|k}\|_{V} \leq \|{\Omega - \mathbbm 1 \bar{\omega}^\intercal} \|_{\textsc{op}}^l \|\bar v^{(0)}_{k|k} - \mathbbm 1 \bar v^*_{k|k}\|_{V}
\end{equation}
which implies (\ref{eq:convergence}). 
\end{proof}

Lastly, we remark that a finite number of average consensus steps can be used in practice (i.e., $\alpha < \infty)$, with $\|{\Omega - \mathbbm 1 \bar{\omega}^\intercal} \|_{\textsc{op}}$ dictating the rate of convergence. Using (\ref{eq:csd}), it can be seen that each weighted average consensus iteration lowers the Cauchy-Schwarz divergence between the sensors' posterior intensities (which is a measure of their information difference \cite{gostar2017cauchy}).
%$\bar \omega ^\intercal \Omega = \bar ^\intercal$.

\subsubsection{Comparison with Existing Results}
Our analysis in this section extends the existing results on consensus (which has largely only considered consensus of vectors in Euclidean spaces)
to the case where consensus is sought on functions in $L^p$. For consensus in Euclidean spaces, a set of sufficient conditions for convergence is the following: 
\begin{itemize}
\item $\Omega_{ij} \geq 0$ whenever $(j,i)\in \mathcal E$,
\item $\Omega_{ii}>0$ ${\forall i\in\mathcal V}$, and 
\item the graph $\mathcal G$ is strongly connected
\end{itemize}
in which case, the fact that $\|{\Omega - \mathbbm 1 \bar{\omega}^\intercal} \|_{\textsc{op}}<1$ follows from the Perron-Frobenius theorem \cite[Lemma 1]{corless2012consensus}. Thus, the foregoing sufficient conditions for convergence are stronger than the ones we obtained in Section \ref{sec:subsec_error_analysis}.
Using the protocol in \cite{corless2012consensus}, the weights $\Omega_{ij}$ may be chosen in a distributed manner, i.e., each sensor is able to choose the weights in real-time without needing global knowledge of the topology of $\mathcal G$.

% However, note that the literature on distributed average consensus typically assumes that consensus is sought on scalars or vectors in a Euclidean space, which are equipped with the operations of scalar multiplication and addition. As distributed GM-PHD requires WAA of GMs instead, it should be ensured that these operations make sense for GMs as well. Indeed, pointwise convergence of (\ref{eq:average_consensus}) can be shown as follows. Consider any point $x\in \mathcal X$ in the domain, and note that $v_{i,k|k}(x) \in \mathbb R$. At the given value of $x$, (\ref{eq:average_consensus}) can interpreted as solving the WAA problem in the space $\mathbb R$, rather than in a function space.
% Thus, we have pointwise convergence of the recursion (\ref{eq:average_consensus}):
% \[v_{i,k|k}(x) \rightarrow v_{k|k}^*(x) \ \forall x\in \mathcal X\]
% $\forall i \in \mathcal V$, which follows directly from the standard results on consensus in $\mathbb R$.
%\cite{li2018partial, consensus_ukf}.


% To see that convergence properties of (\ref{eq:average_consensus}) indeed follow from those of average consensus in Euclidean spaces, note that the Gaussian distribution is square-integrable, so the function space of GMs is a Hilbert space (i.e., the $L^2$ space) having the (pointwise) addition and scalar multiplication operations as well as an inner product operation. The convergence proofs of the average consensus algorithms can therefore be adapted verbatim for GMs, so that the convergence of (\ref{eq:average_consensus}) (in
% the $L^2$ function norm)
% holds. 

% Such a result can be formalized using the approach of \cite{probability_consensus2014}, in which the authors study consensus in the function space of probability measures.

\subsection{Limitations of the Average Consensus Approach}
Although the average consensus algorithm (\ref{eq:average_consensus}) guarantees asymptotic convergence to the WAA, it has several limitations. Firstly, it requires each sensor to transmit $J_{i,k|k}$ GM components to its out-neighbors. This limits the number of targets that can be tracked simultaneously by the sensor network when the communication channels between the sensors have limited bandwidth, as is usually the case in wireless sensor networks. Secondly, the large number of GM components accumulated at each sensor increase the computational complexity of the subsequent local PHD filtering steps. Lastly, as noted in \cite{li2018partial}, the consensus step can exacerbate the problem of false positives in WAA fusion; small GM components corresponding to the false positive detections (i.e., clutter measurements which are misidentified as targets) can propagate through the cycles (closed loops) of the graph $\mathcal G$, leading to an overall feedback effect. In this way, false positives can get amplified during the fusion step, reducing the estimation performance of the sensor network. We address each of these concerns in the next section, by proposing a distributed protocol for dissemination and fusion of GM components that ensures asymptotic consensus and suppression of false positives, while having the same (or lower) communication bandwidth requirement than the existing distributed PHD filtering algorithms.