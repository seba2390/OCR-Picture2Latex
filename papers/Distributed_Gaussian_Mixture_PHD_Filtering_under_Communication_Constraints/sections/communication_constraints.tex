\section{Multi-Sensor Fusion under Communication Constraints}
\label{sec:comm_constr}

In order to carry out the fusion step of (\ref{eq:average_consensus}), the sensors must transmit a certain number of GM components to their out-neighbors. Since the dimension of the state-space $\mathcal X$ is fixed as $d_x$, each GM component is specified by $d_x + \tfrac{1}{2}d_x(d_x + 1) + 1$ floating point numbers, where the individual terms correspond to the mean vector, the covariance (which is a symmetric matrix), and the scalar weight of the GM component, respectively. In this section, we consider the case where the communication bandwidth is limited, such that the maximum number of GM components that may be transmitted across the inter-sensor communication channels at a given timestep is denoted as $B$, with $B \geq 1$.
In order to limit the communication complexity of the distributed GM-PHD algorithm, the authors of \cite{li2018partial} and \cite{li2020parallel} proposed the following heuristics for selecting the $J_{i,k|k}$ GM components that are to be transmitted at each sensor:
\begin{itemize}
    \item \textit{Rank Rule}: Rank the GM components based on their weights (highest to lowest) and select the top $B$ components
    \item \textit{Threshold Rule}: Fix a threshold, and transmit any GM components that have weights greater than this threshold
\end{itemize}
In either case, the GM components with small weights are not transmitted. This has the advantage of suppressing false positives, which typically correspond to the components having small weights. However, convergence to the WAA (\ref{eq:WAA_def}) cannot be guaranteed if the rank or threshold rule is used, as the GM components with small weights are never communicated between sensors.
Consequently, this approach is referred to as \textit{partial consensus} by its authors.

In order to guarantee asymptotic convergence of the average consensus algorithm to the WAA, we propose a new rule for selecting the GM components, which we call the \textit{sampling rule}. For simplicity, we assume that the communication is based on wireless broadcast, i.e., each sensor transmits the same message to all of its out-neighbors. Let $\mathcal B_{i}$ be a sequence of indices (numbers between $1$ and $J_{i,k|k}$) corresponding to the GM components transmitted by sensor $i$ to its out-neighbors at timestep $k$, where we omit the timestep $k$ for brevity. The total number of distinct indices in $\mathcal B_i$ is at most $B$, as per the communication constraint. The indices in $\mathcal B_i$ are chosen through random sampling, where the sampling is carried out either with or without replacement.

\subsection{Sampling with Replacement}
In order to construct $\mathcal B_i$, sensor $i$ generates a sequence of random samples of indices with replacement. In each sample,
the index $l$ is chosen with probability $p_{i}^{(l)}$, with 
\begin{equation}
    \sum_{l=1}^{J_{i,k|k}} p_{i}^{(l)} = 1
\end{equation}
%
Thus, the elements of $\mathcal B_i$ are $i.i.d.$ samples of a categorical random variable and can be thought of as the outcomes of rolling a $J_{i,k|k}$-sided biased die $|\mathcal B_i|$ times. Due to the limited communication bandwidth, the sampling process is terminated when the number of distinct indices in $\mathcal B_i$ equals $B$.
Let $\zeta^{(l)}_{i}$ denote the number of times that the index $l$ appears in $\mathcal B_i$. Observe that $(\zeta^{(1)}_{i}, \zeta^{(2)}_{i}, \dots, \zeta^{(|J_{i,k|k}|)}_i)$ follow the multinomial distribution, with $\mathbb E[\zeta^{(l)}_{i}]=p_{i}^{(l)} |\mathcal B_i|$.

Once $\mathcal B_i$ is generated, the sensor transmits the corresponding GM components to its out-neighbors. If there are any repeated components, the sensor transmits each of these components (in addition to the number of times they were sampled) only once, in order to avoid redundancy in the communication. Thus, each sensor transmits a random function (i.e., a random GM) to its out-neighbors at each timestep, given by
\begin{align}  &\sum_{l=1}^{J_{i,k|k}}\zeta^{(l)}_{i} \hspace{2pt} \tilde w^{(l)}_{i,k|k}\mathcal N(m^{(l)}_{i,k|k}, P^{(l)}_{i,k|k})
    \label{eq:random_GM}
\end{align}
where $\mathcal N(m,P)$ denotes a GM component having the mean vector $m\in \mathbb R^{d_x}$ and covariance $P\in \mathbb R^{d_x \times d_x}$.
Note that the weights of the transmitted GM components $\tilde w^{(l)}_{i,k|k}$ are different from the weights used in the local GM-PHD filtering, $w^{(l)}_{i,k|k}$.
To ensure that the fixed point of the average consensus algorithm is preserved (in expectation) by the sampling rule, we require that the expected value of the random GM is equal to the posterior intensity of sensor $i$, i.e.,
\begin{align}
\mathbb E\left[\sum_{l=1}^{J_{i,k|k}}\zeta^{(l)}_{i} \hspace{2pt} \tilde w^{(l)}_{i,k|k}\mathcal N(m^{(l)}_{i,k|k}, P^{(l)}_{i,k|k})\right] = v_{i,k|k}
\label{eq:exp_randomGM}
\end{align}
% In addition, the law of large numbers can be used to guarantee asymptotic (pointwise) convergence of the random GMs to the true posterior intensity $v_{i,k|k}$.
% From (\ref{eq:exp_randomGM}), 
Equivalently,
we have
\begin{align}
\mathbb E[\zeta^{(l)}_{i}]\hspace{2pt}\tilde w^{(l)}_{i,k|k} = w^{(l)}_{i,k|k} \label{eq:exp_weight_condition}\\ \hspace{1pt}  \tilde w^{(l)}_{i,k|k} = \frac{w^{(l)}_{i,k|k}}{p_{i}^{(l)}|\mathcal B_i|}
\label{eq:weight_condition}
\end{align}
Equation (\ref{eq:weight_condition}) imposes a constraint on the sampling probabilities $p_{i}^{(l)}$ and the weights $\tilde w_{i,k|k}^{(l)}$, but does not uniquely specify them. 
We propose the following choice of sampling probabilities:
\begin{equation}
p_{i}^{(l)}=\frac{w^{(l)}_{i,k|k}}{\sum_{l=1}^{J_{i,k|k}} w^{(l)}_{i,k|k}}
\label{eq:pl_choice}
\end{equation}
for which the corresponding weights are given by (\ref{eq:weight_condition}), as
\begin{equation}
\tilde w^{(l)}_{i,k|k} = \frac{\sum_{l=1}^{J_{i,k|k}} w^{(l)}_{i,k|k}}{|\mathcal B_i|} \triangleq \tilde w_{i,k|k}
\label{eq:final_w_tilde_def}
\end{equation}
The proposed choice of sampling probabilities, given by (\ref{eq:pl_choice}) and (\ref{eq:final_w_tilde_def}), has the clear advantage that the weights $\tilde w^{(l)}_{i,k|k}$ no longer need to be transmitted for each component, a single number $\tilde w_{i,k|k}$ may be transmitted instead, further reducing the communication requirement of the algorithm. Additionally, observe that if the sampling probabilities $p_{i}^{(l)}$ are chosen as per (\ref{eq:pl_choice}), then GM components with higher weights are transmitted more often than those with smaller weights,
%Thus, the GM components with high weights converge (as per the law of large numbers) faster than the components with small weights. 
so that the components with small weights only survive the fusion step if the targets corresponding to them are persistently detected. In this way, false positive detections that do not survive successive local PHD filtering steps are suppressed by the inter-sensor fusion step. The ability of the algorithm to suppress false alarms as well as the reduced communication cost motivate the choice of $p_{i}^{(l)}$ as (\ref{eq:pl_choice}).

\begin{algorithm}
\caption{Random Sampling Rule (with Replacement)}
\begin{algorithmic}[1]
\vspace{2pt}
\REQUIRE The maximum allowable number of GM components, $B\geq 1$ \\
\vspace{3pt} 
\WHILE{$\mathcal B_i$ has at most $B$ distinct indices}
\STATE Randomly sample an index from the sequence $\left(1, 2, \dots, J_{i,k|k}\right)$ with the corresponding sampling probabilities as given by (\ref{eq:pl_choice}).
\ENDWHILE
\STATE Each sensor broadcasts the following GM to its out-neighbors:
\[\tilde w_{i,k|k} \sum_{l=1}^{J_{i,k|k}}\zeta^{(l)}_{i} \hspace{2pt} \mathcal N(m^{(l)}_{i,k|k}, P^{(l)}_{i,k|k})
\]
where $\zeta^{(l)}_i$ is the number of times $l$ occurs in $\mathcal B_i$.
\end{algorithmic}
\label{alg:sampling}
\end{algorithm}
Thus, random sampling (as described in Algorithm \ref{alg:sampling}) can be used to replace step $3$ of Algorithm \ref{alg:con}, thereby limiting the communication cost of the distributed GM-PHD filter. From step $4$ of Algorithm \ref{alg:sampling}, we see that at each sensor, the sampling rule requires the  communication of $B$ GM components, $1$ floating point number (which is $\tilde w_{i,k|k}$), and less than $B$ integers (i.e., the numbers $\zeta_i^{(l)}$) at each iteration of the average consensus step.

Observe that, even though each sensor communicates a random GM to its neighbors, the integral of the GM is a deterministic quantity:
\begin{align}   
\tilde w_{i,k|k} \sum_{l=1}^{J_{i,k|k}}\zeta^{(l)}_{i} = \tilde w_{i,k|k} |\mathcal B_i| = \sum_{l=1}^{J_{i,k|k}} w^{(l)}_{i,k|k}
\end{align}
where in the second equality, we used (\ref{eq:final_w_tilde_def}).
Thus, while the posterior intensities of the sensors asymptotically converge to the WAA in expectation, the integrals of the intensities (i.e., the cardinality estimates of the sensors) converge deterministically.

\subsection{Sampling without Replacement}
An alternative strategy for constructing $\mathcal B_i$ is to sample the indices without replacement. As each index occurs in $\mathcal B_i$ at most once, $\mathcal B_i$ can be interpreted as a set. The random variable $\zeta^{(l)}_{i}$ defined in the previous section becomes the indicator variable $\mathbbm 1_{\lbrace l\in \mathcal B_i \rbrace}$, where
\begin{align}
    \mathbbm 1_{\lbrace l\in \mathcal B_i \rbrace} = \begin{cases}
    \begin{array}{cl}
         1& \text{if label}\ l\ \text{is in }\mathcal B_i,  \\
         0& \text{otherwise.}
    \end{array}
    \end{cases}
\end{align}
so that the condition (\ref{eq:exp_weight_condition}) now becomes
\begin{equation}
    \mathbb E[\mathbbm 1_{\lbrace l\in \mathcal B_i \rbrace}]\hspace{2pt}\tilde w^{(l)}_{i,k|k} = P (l\in \mathcal B_i )\hspace{2pt}\tilde w^{(l)}_{i,k|k} = w^{(l)}_{i,k|k} 
\end{equation}
where $P({}\cdot{})$ denotes the probability of an event. Thus, once the 
probabilities $P (l\in \mathcal B_i )$ are computed,
the weights $\tilde w^{(l)}_{i,k|k}$ must be chosen accordingly, as
\begin{equation}
    \tilde w^{(l)}_{i,k|k} = \frac{w^{(l)}_{i,k|k}}{P( l\in \mathcal B_i )}
    \label{eq:no_replacement_weights}
\end{equation}
An efficient method for sampling without replacement can be found in \cite{Efraimidis2015}, which is able to assign a higher probability to components with higher weights, thereby incorporating the ability to suppress false positives into the fusion step.

Note that it is no longer straightforward to specify the inclusion probabilities of the indices $P( l\in \mathcal B_i )$ \textit{a priori}, like we did in (\ref{eq:pl_choice}). To see this, consider the case where $B=J_{i,k|k}$, which fixes the probabilities as $P( l\in \mathcal B_i )=1$ for all the indices. Thus, if we were to specify the probabilities $P( l\in \mathcal B_i )$ \textit{a priori}, it can make the sampling problem infeasible; the same is true even when $B<J_{i,k|k}$ \cite[Example 2]{Efraimidis2015}. Moreover, when using the sampling without replacement rule, in addition to transmitting the mean vectors $m^{(l)}_{i,k|k}$ and covariances $P^{(l)}_{i,k|k}$ of the selected components, the sensors must transmit the modified weights $\tilde w_{i,k|k}$ as well. In contrast, the proposed sampling with replacement rule (given in Algorithm \ref{alg:sampling}) allows one to specify the sampling probabilities \textit{a priori}, and also has a lower communication bandwidth requirement.
% $P( l\in \mathcal B_i )$ is also more difficult to compute, since the probability of a given GM component being chosen is dependent on the probability that

In summary, the proposed distributed GM-PHD algorithm uses a local GM-PHD filter at each sensor to update its multi-target estimate, followed by a given number of average consensus steps (as per Algorithm \ref{alg:con}) to fuse the multi-target estimates of the sensors. Additionally, the proposed random sampling rule (given in Algorithm \ref{alg:sampling}) is used to limit the communication bandwidth of the approach, while ensuring its asymptotic optimality.