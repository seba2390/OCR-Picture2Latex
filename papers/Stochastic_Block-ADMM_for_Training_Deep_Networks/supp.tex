\appendix 
\section*{Supplementary Materials}
\section{Background: Standard ADMM Training of DNNs} \label{sec:admm_nn}

Alternating Direction Method of Multipliers (ADMM) \cite{gabay1975dual,boyd2011distributed} is a class of optimization methods belonging to  \textit{operator splitting techniques} which borrows benefits from both dual decomposition and augmented Lagrangian methods for constrained optimization. %To show the potentials of standard ADMM, we first revisit a general formulation of ADMM in DNN training, similar to those used in prior work. Then, we propose our stochastic block-ADMM in the next subsection.

To formulate training an $L$-layer DNN in a general supervised setting, we would have the following non-convex constrained optimization problem \cite{zeng2018global}:
% \vspace{-0.1in}
\begin{align} \label{eq:obj}
	\minimize_{ \mathcal{W}, \mathcal{A}, \mathcal{Z}} \quad &\mathcal{J}\left(\mY, \mZ_{L} \right) + \sum_{\ell = 1}^{L} \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}) \\
	 {\rm subject~to} \quad & \mA_{\ell} - {\bm \phi}_{\ell } \left( \mZ_{\ell} \right) = {\bf 0}, \quad \ell = 1,\dots, L-1   \nonumber \\
	 {\rm subject~to} \quad & \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} = {\bf 0}, \quad \ell = 1, \dots , L \nonumber 
\end{align}
where $\mathcal{J}$ is the main objective (\textit{e.g.}, cross-entropy, mean-squared-error loss functions) that needs to be minimized. The subscript $\ell$ denotes the $\ell$-th layer in the network. The optimization variables are $\mathcal{W} = \{ \mW_\ell\}_{\ell=1}^{L}$, $\mathcal{A} = \{ \mA_{\ell}\}_{\ell=1}^{L-1}$, and $\mathcal{Z} = \{ \mZ_{\ell}\}_{\ell=1}^{L}$ where $\mW_\ell$, $\mZ_{\ell}$, $\mA_\ell$, and ${\bm \phi}_\ell (.)$ are the weight matrix, output matrix, activation matrix, and the activation function (\textit{e.g.}, ReLU) at the $\ell$-th layer, respectively. Note that $\mA_{0} = \mX$ where $\mX = \{ \vx_1,\dots, \vx_N \} \in  \R^{M \times N}$ is the input data matrix containing $N$ samples with input dimensionality $M$; $\mY = \{\vy_1,\dots, \vy_N \} \in \R^{C \times N}$ is the target matrix pair comprised of $N$ one-hot vector label of dimension $C$, representing number of prediction classes. Also, ${\bf r(.)}$ is the regularization term with (\textit{e.g.}, Frobenius norm $\|.\|_F^2$) corresponding penalty weight $\lambda_{\ell}$. Note that the regularization term can be simply ignored by setting $\lambda_\ell$ to zero. In this formulation, the intercept in each layer is ignored for simplicity as it can be simply be added by slightly modifying the $\mW_\ell$ and the input to each layer. The formulation in Eq. (\ref{eq:obj}) breaks the the conventional multi-layer backpropagation optimization of DNNs into simpler sub-problems that can be solved efficiently (e.g. reducing to least-squares problem). This also facilitates training in a distributed manner --- as the layers of the DNN are decoupled and the variables can be updated in parallel across layers ($\mW_\ell$) and data points (\ $\mW_\ell, \mZ_\ell, \mA_\ell$).



To enforce the constraints in problem (\ref{eq:obj}) and solve the optimization using ADMM, we would have the following augmented Lagrangian problem:

\begin{eqnarray} \label{eq:augmented}
	\minimize_{ \mathcal{W}, \mathcal{A}, \mathcal{Z}} \quad &\mathcal{J}\left(\mY, \mZ_{L} \right) + \sum_{\ell = 1}^{L} \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}) \\
	& + \sum_{\ell=1}^{L} \frac{\beta_{\ell}}{2} \| \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} + \mU_{\ell}\|_{F}^{2} \nonumber\\
	& + \sum_{\ell=1}^{L-1} \frac{\gamma_{\ell}}{2} \| \mA_{\ell} - {\bm \phi}_{\ell}(\mZ_{\ell}) + \mV_{\ell}\|_{F}^{2}\nonumber
\end{eqnarray}
where $\beta_{\ell}, \gamma_\ell >0$ are the step sizes, $\mU_{\ell}$ and $\mV_{\ell}$ are the \textit{(scaled) dual variables} \cite{boyd2011distributed} for the equality constraint at the layer $\ell$. 
Algorithm \ref{alg:admm} shows a standard ADMM scheme for optimizing Eq. (\ref{eq:augmented}). Note, the parameters are updated in a closed-form as analytical solution can be simply derived. For simplicity of the equations, we denote $\gP_\ell (.) = \frac{\beta_{\ell}}{2} \| \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} + \mU_{\ell}\|_{F}^{2} $ and $\gQ_\ell (.) = \frac{\gamma_{\ell}}{2} \| \mA_{\ell} - {\bm \phi}_{\ell}(\mZ_{\ell}) + \mV_{\ell}\|_{F}^{2}$. This algorithm is similar to \cite{taylor2016training,wang2019admm} with the difference that all the equality constraints in problem (\ref{eq:obj}) are enforced using multipliers, while previous work only enforced the constraints on the last layer $L$ while other constraints were only loosely enforced using quadratic penalty. 

\begin{algorithm}[htb]
  \caption{Standard ADMM for DNN Training}
  \label{alg:admm}
\begin{algorithmic}
  {\STATE \scalebox{1}{\bfseries Input:} data $\mX$, labels $\mY$}
  \STATE  \scalebox{1}{{\bfseries Params:} $\beta_\ell >0, \gamma_\ell >0,\lambda_\ell > 0$ }
  \STATE  \scalebox{0.8}{{\bfseries Initialize:} $\{\mW_\ell^0\}_{\ell=1}^{L}, \{ \mU_\ell^0\}_{\ell=1}^{L}, \{ \mV_\ell^0\}_{\ell=1}^{L-1}, \{\mZ^0_\ell\}_{\ell=1}^{L}, \{\mA^0_\ell\}_{\ell=1}^{L-1}\; k \leftarrow 0$ }
  \REPEAT
  \FOR{$\ell=1$ {\bfseries to} $L$}
  \STATE \scalebox{1}{$\mW_\ell^{k+1} \leftarrow \argmin\; \{ \gP_\ell (.) +  \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}^{k})\}$}
  \ENDFOR
  \FOR{$\ell=1$ {\bfseries to} $L-1$}
  \STATE \scalebox{1}{ $\mZ_\ell^{k+1} \leftarrow \argmin\; \{ \gP_\ell (.) +  \gQ_\ell (.) \}$ }
  \STATE \scalebox{1}{$\mA_\ell^{k+1} \leftarrow \argmin\; \{ \gP_{\ell+1} (.) +  \gQ_\ell (.) \} $}
  \ENDFOR
    \STATE \scalebox{1}{ $\mZ_{L}^{k+1} \leftarrow \argmin\; \{ \mathcal{J}\left(\mY, \mZ_{L}^{k} \right) + \gP_L (.) \}$ }
  \FOR{$\ell=1$ {\bfseries to} $L-1$}
  \STATE \scalebox{1}{$\mU_\ell^{k+1} \leftarrow \mU_\ell^{k} + \mZ_{\ell}^{k+1} - \mW_{\ell}^{k+1} \mA_{\ell-1}^{k+1}$}
  \STATE \scalebox{1}{$\mV_\ell^{k+1} \leftarrow \mV_\ell^{k} + \mA_{\ell}^{k+1} - {\bm \phi}_{\ell}(\mZ_{\ell}^{k+1})$}
  \ENDFOR
  \STATE \scalebox{1}{$\mU_L^{k+1} \leftarrow \mU_L^{k} + \mZ_{L}^{k+1} - \mW_{L}^{k+1} \mA_{L-1}^{k+1}$}
  \UNTIL{some stopping criterion is reached.}
\end{algorithmic}
\end{algorithm}


While the standard ADMM Algorithm \ref{alg:admm} has potentials in training (simple) DNNs \cite{taylor2016training}, there exists hurdles that confines extending ADMM to more complex problems --- the global convergence proof of the ADMM \cite{deng2016global} assumes that $\mathcal{J}$ is deterministic and the global solution is calculated at each iteration of the cyclic parameter updates.
% and during each iteration of the cyclic parameter updates, all the data samples are visited.
This makes standard ADMM computationally expensive thus impractical for training of many large-scale optimization problems. Specifically, for  deep learning, this would impose a severe restriction on training set size when limited computational resources are available. In addition, since the variable updates in standard ADMM are analytically driven, the extent of its applications is limit to trivial tasks \cite{taylor2016training}, making it incompetent to perform on par with the recent complex architectures introduced in deep learning (e.g. \cite{he2016deep}).


\section{Proof for Proposition 1}\label{sec:proof}

We follow the steps in the proof for similar problems in \cite{fu2018anchor} and \cite{shi2017penalty} with deterministic primal updates. Proper modifications are made to cover the stochastic primal update in our proof.


Note that we have
              \[     \nabla{\cal L}_{\rho_k}(\X^k)= \nabla f(\X^k) + \nabla h(\X^k)^T\bm \mu^k,          \]
              where 
              \[      \bm \mu^k = (1/\rho_k)h(\bm X^k)+\bm \lambda^k.   
              \]
              Our first step is to show that $\{\bm \mu^k\}$ is a convergent sequence. To see this, we define 
              \[ \bm \bar{\bm \mu}^k = \frac{\bm \mu^k}{\|{\bm \mu}^k\|}. \]
              Since $\bm \bar{\bm \mu}^k$ is bounded, it converges to a limit point $\bm \bar{\bm \mu}$. Also let $\x^\star$ be a limit point of $\x^k$.
              Because we have assumed that 
              $$\varepsilon_k\rightarrow 0,\quad \sigma_k^2\rightarrow 0,$$ 
              it means that the mean and variance of the stochastic gradient of our primal update goes to zero.
              Since our stochastic gradient is unbiased, we have
              \[       {\cal G}(\X^k) \rightarrow \nabla {\cal L}_{\rho_k}(\X^\star). \]  
              This also means that  we must have ${\cal G}(\x^k)\rightarrow \bm 0$ and $$\nabla L_{\rho_k}(\bm x^k)\rightarrow \bm 0.$$
     Hence, the following holds when $k\rightarrow \infty$:
              \begin{equation}\label{eq:approxkkt}
                 \nabla L_{\rho_k}(\bm X^\star)=\nabla f(\X^\star)+\nabla h(\X^\star)^T\bm {\bm \mu}^\infty = 0,
              \end{equation}           
               
               
              Suppose $\bm \mu^k$ is unbounded. By dividing \eqref{eq:approxkkt} by the above $\|\bm \mu^k\|$ and considering $k\rightarrow \infty$, we must have 
              \begin{equation}\label{eq:key}
                \nabla h(\X^\star)^T\bm \bar{\bm \mu}= 0,\quad \forall \X.    
              \end{equation}               
              The term $\nabla f(\bm X^\star)/\|\bm \mu\|$ is zero since we assumed $\bar{\bm \mu}$ is unbounded.
              Since $h(\bm X)=\bm 0$ satisfies the Robinson's condition, then, for any $\bm w$, there exists $\beta>0$ and $\bm x$ such that
              \[      \bm w = \beta \nabla h(\X^\star)(\X-\X^\star).        \]
              This together with \eqref{eq:key} says that $\bar{\bm \mu}=\bm 0$. This contradicts to the fact $\|\bar{\bm \mu}\|=1$. Hence, $\{ \bm \mu^k \}$ must be a bounded sequence and thus admits a limit point. Denote $\bm \mu^\star$ as this limit point, and take limit of both sides of \eqref{eq:approxkkt}. We have:
              \begin{equation}
              \nabla f(\X^\star)+\nabla h(\X^\star)^T\bm \mu^\star= \bm 0,\quad \forall \X.
              \end{equation}
               
              In addition, since $$\rho_k(\bm \mu^k-\bm \lambda^k) = h(\mathbf{\X^k})$$ with $\rho_k \rightarrow 0$ or $\bm \mu_k-\bm \lambda_k \rightarrow 0$ (per our updating rule and $\eta_k\rightarrow 0$), the constraints will be enforced in the limit.      $\mbox{     } \square$   \\
              

% \subsection*{{\uppercase\expandafter{\romannumeral D}. Supervised training on Fashion-Mnist}}\label{fmnist}


% To compare our method with dlADMM \citet{wang2019admm}, we evaluated the performance of our method on the Fashion-MNIST dataset \citep{xiao2017/online} with 60,000 training samples and 10,000 testing samples. We followed the settings in \citet{wang2019admm} by having 2 hidden layers with 1000 neurons each, and Cross-Entropy loss at the final layer. Also, the batch size is set to 128, $\beta_t = 1$, and the updates for $\mZ_t$ and $\Theta_t$ (eq. 6a) are performed 3 times at each epoch. Figure \ref{fig:fmnist_acc} shows the test set accuracy results over 200 epochs of training. It can be noticed that Stochastic Block ADMM is converging at lower epochs and reaching a higher test accuracy while performing efficient mini-batch updates. Further, in section C., it will be demonstrated that Stochastic Block ADMM converges drastically faster than dlADMM in terms of wall clock time.

   
% \begin{figure}[ht]
% \begin{center}
% \centerline{
% \includesvg[width=\columnwidth]{img/fmnist_acc.svg}
% }
% \caption{Test accuracy comparison of Stochastic Block ADMM and dlADMM \citep{wang2019admm} on Fashion-MNIST dataset using a network with 3 fully-connected layers: $784-1000-1000-10$. Final test accuracy: "Stochastic Block ADMM": $\bf 90.39\%$, "Wang \etal":$84.67 \%$ (averaged over 5 runs).}
% \vskip -0.25in
% \label{fig:fmnist_acc}
% \end{center}
% \end{figure}


\begin{figure}[ht]
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/fmnist_acc.pdf}
}
\caption{Test accuracy comparison of Stochastic Block ADMM and dlADMM on Fashion-MNIST dataset using a network with 3 fully-connected layers: $784-1000-1000-10$. Final test accuracy: "Stochastic Block ADMM": $\bf 90.39\%$, "Wang \textit{et al.}":$84.67 \%$ (averaged over 5 runs).}
% \vskip -0.25in
\label{fig:fmnist_acc}
\end{center}
\end{figure}




%----------------------------
\section{Supervised training of DNNs}\label{sec:sup_train}

\textbf{Fashion-MNIST.}
To compare our method with dlADMM \cite{wang2019admm}, we evaluated the performance of our method on the Fashion-MNIST dataset \cite{xiao2017/online} with 60,000 training samples and 10,000 testing samples. We followed the settings in \cite{wang2019admm} by having 2 hidden layers with 1000 neurons each, and Cross-Entropy loss at the final layer. Also, the batch size is set to 128, $\beta_t = 1$, and the updates for $\mZ_t$ and $\Theta_t$ (eq. 6a) are performed 3 times at each epoch. Figure \ref{fig:fmnist_acc} shows the test set accuracy results over 200 epochs of training. It can be noticed that Stochastic Block ADMM is converging at lower epochs and reaching a higher test accuracy while performing efficient mini-batch updates. Further, in section C., it will be demonstrated that Stochastic Block ADMM converges drastically faster than dlADMM in terms of wall clock time.



\textbf{CIFAR-10.}
The previous works on training deep netowrks using ADMM have been limited to trivial networks and datasets (e.g. MNIST) \cite{taylor2016training,wang2019admm}. However, our proposed method does not have many of the existing restrictions and assumptions in the network architecture, as in previous works do, and can easily be extended to train non-trivial applications. It is critical to validate stochastic block-ADMM in settings where deep and modern architectures such as deep residual networks, convolutional layers, cross-entropy loss function, etc., are used. To that end, we validate the ability of our method is a supervised setting (image classification) on the CIFAR-10 dataset \cite{cifar} using ResNet-18 \cite{he2016deep}. To best of our knowledge, this is the first attempt of using ADMM for training complex networks such as ResNets. 


For this purpose, we used 50,000 samples for training and the remaining 10,000 for evaluation. 
To have a fair comparison, we followed the configuration suggested in \cite{gotmare2018decoupling} by converting Resnet-18 network into two blocks $(T=2)$, with the splitting point located at the end of {\sc conv3\_x} layer. We used the Adam optimizer to update both the blocks and the decoupling variables with the learning rates of $\eta_t = 5e^{-3}$ and $\zeta_t = 0.5$. We noted since the auxiliary variables $\mZ_t$ are not "shared parameters" across data samples, they usually require a higher learning rate in Algorithm \ref{alg:blockadmm}. Also, we found the ADMM step size $\beta_t = 1$ to be sufficient for enforcing the block's coupling. 


Figure. \ref{fig:cifar} shows the results from our method compared with two baselines: \cite{gotmare2018decoupling}, and conventional end-to-end neural network training using back-propagation and SGD. Our algorithm consistently outperformed ~\cite{gotmare2018decoupling} however cannot match the conventional SGD results. There are several factors that we hypothesize that might have contributed to the performance difference: 1) in a ResNet the residual structure already partially solved the vanishing gradient problem, hence SGD/Adam performs significantly better than a fully-connected version; 
% 2) The common data augmentation in CIFAR will end up sending a different training example to the optimization algorithm at each iteration, which does not seem to affect SGD but seem to affect ADMM convergence somewhat; 
2) we noticed decreasing the learning rate for $\Theta_t$ updates does not impact the performance as it does for an end-to-end back-propagation using SGD. Still, we obtained the best performance of ADMM-type methods on both MNIST and CIFAR datasets, showing the promise of our approach.
% As illustrated, ADMM gets to a good performance fast and then slowly progress to higher accuracy..


%---------------------------- fig cifar  ------------------------------

\begin{figure}[htb]
% \vskip 0.15in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/cifar.pdf}
}
% \vskip -0.05in
\caption{Test set accuracy on CIFAR-10 dataset. Final accuracy "Block ADMM": $89.66\%$, "Gotmare \etal":$87.12 \%$, "SGD": $\bf 92.70\%$. (Best viewed in color.)}
\label{fig:cifar}
\end{center}
% \vskip -0.2in
\end{figure}

 
 
 
% \subsection*{{\uppercase\expandafter{\romannumeral C}. Wall Clock Time Comparison}} \label{time_cmp}

% In this section, we setup a experiment to further analyse the efficiency of Stochastic Block ADMM and compare its training wall clock time against other baselines: \citet{gotmare2018decoupling,zeng2018global} (BCD), and \citet{wang2019admm} (ADMM). 
% For this purpose, we follow the similar settings as in section 4.1 for a supervised Deep Neural Network (DNN) training over MNIST dataset. Figure \ref{fig:time} shows the test set accuracy v.s. the training wall clock time from different methods. All the experiments are run on a machine with a single NVIDIA GeForce RTX 2080 Ti GPU. The methods are implemented in PyTorch framework -- except for dlADMM \citep{wang2019admm} that is implemented\footnote{code taken from \url{https://github.com/xianggebenben/dlADMM}} in "cupy", a NumPy-compatible matrix library accelerated by CUDA. \citet{gotmare2018decoupling} and Stochastic Block ADMM are trained with a mini-batch size of 128 and \citet{zeng2018global,wang2019admm} are trained in a batch setting. Note that in Figure \ref{fig:time}, the time recorded merely shows the \emph{training time} and excludes the time taken for initialization, data loading, etc. It can be observed that \citet{gotmare2018decoupling} and dlADMM are showing much slower convergence behaviors than Stochastic Block ADMM. We speculate that enforcing all the constraints by dual variables along with the efficient and cheap mini-batch updates in our method highly contributes to the convergence speed as well as its performance superiority over the other methods, including \citet{zeng2018global}.


% \begin{figure}[ht]
% \begin{center}
% \centerline{
% \includesvg[width=\columnwidth]{img/time_comparison.svg}
% }
% \caption{Test set accuracy v.s. training wall clock time comparison of different alternating optimization methods for training DNNs on MNIST dataset. Our method (blue) shows superior performance while presenting comparable convergence speed against \citet{zeng2018global} (green).}
% \vskip - 0.15in
% \label{fig:time}
% \end{center}
% \end{figure}


\begin{table*}[htb]
\caption{Prediction accuracy (\%) of individual attributes in LFWA dataset. DeepFacto with other weakly-supervised and supervised baselines.}
\label{table:attr_lfw}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
{Attributes} & \multicolumn{3}{c}{\small DeepFacto} & \small \cite{liu2015deep} & \small \cite{liu2018exploring} & \small \cite{zhang2014panda}\\
 {} & \multicolumn{3}{c}{\tiny (Weakly-Supervised)} & {\tiny (Weakly-Supervised)} & {\tiny (Supervised)} & {\tiny (Supervised)} \\
 {} & $r= $256 & 32 & 4 \\
\midrule
‘5 o Clock Shadow’ & 83.3 & 80.0 & 68.7 & 78.8 & \bf84 & \bf84\\
‘Arched Eyebrows’ & \bf86.6 & 83.9 & 79.2 & 78.1 & 82 & 79\\
‘Attractive’ & \bf84.3 & 79.8 & 73.3 & 79.2 & 83 & 81\\
‘Bags Under Eyes’ & \bf83.9 & 72.5 & 64.5 & 83.1 & 83 & 80 \\
‘Bald’ & \bf94.3 & 93.3 & 89.3 & 84.8 & 88 & 84\\
‘Bangs’ & \bf93.2 & 88.4 & 84.4 & 86.5 & 88 & 84\\
‘Big Lips’ & \bf83.2 & 77.0 & 71.9 & 75.2 & 75 & 73\\
‘Big Nose' & 80.1 & 68.7 & 61.4 & \bf81.3 & 81 & 79\\
‘Black Hair’ & \bf92.7 & 91.4 & 87.4 & 87.4 & 90 & 87\\
‘Blond Hair’ & \bf97.9 & 97.3 & 93.2 & 94.2 & 97 & 94\\
‘Blurry’ & \bf90.4 & 90.5 & 86.5 & 78.4 & 74 & 74\\
‘Brown Hair’ & \bf78.4 & 74.4 & 70.2 & 72.9 & 77 & 74\\
‘Bushy Eyebrows’ & \bf84.0 & 78.6 & 63.4 & 83.0 & 82 & 79\\
‘Chubby’ & \bf80.5 & 75.2 & 71.1 & 74.6 & 73 & 69\\
‘Double Chin’ & \bf86.0 & 77.9 & 72.3 & 80.2 & 78 & 75\\
‘Eyeglasses’ & 94.3 & 89.6 & 84.8 & 89.5 & \bf95 & 89\\
‘Goatee’ & \bf89.1 & 85.4 & 80.0 & 78.6 & 78 & 75\\
‘Gray Hair’ & \bf91.9 & 90 & 85.6 & 86.9 & 84 & 81\\
‘Heavy Makeup’ & \bf96.3 & 91.5 & 87.4 & 94.5 & 95 & 93\\
‘High Cheekbones’ & \bf90.4 & 79.0 & 72.1 & 88.8 & 88 & 86\\
‘Male’ & 81.3 & 76.6 & 70.5 & \bf94.3 & 94 & 92\\
‘Mouth Slightly Open’ & \bf85.4 & 78.0 & 73.3 & 81.7 & 82 & 78 \\
‘Mustache’ & \bf96.6 & 93.2 & 91.3 & 83.3 & 92 & 87\\
‘Narrow Eyes’ & \bf78.3 & 69.3 & 58.4 & 77.5 & 81 & 73\\
‘No Beard’ & \bf79.5 & 73.0 & 65.5 & 77.7 & 79 & 75\\
‘Oval Face’ & \bf80.6 & 73.2 & 66.1 & 78.7 & 74 & 72\\
‘Pale Skin’ & 75.1 & 66.7 & 60.6 & \bf89.8 & 84 & 84\\
‘Pointy Nose'& \bf81.6 & 73.7 & 62.2 & 79.8 & 80 & 76\\
‘Receding Hairline’ & 84.0 & 80.9 & 73.8 & \bf88.0 & 85 & 84 \\
‘Rosy Cheeks’ & \bf87.3 & 87.4 & 83.4 & 79.9 & 78 & 73\\
‘Sideburns’ & \bf85.4 & 81.5 & 75.8 & 80.5 & 77 & 76\\
‘Smiling’ & \bf92.6 & 78.7 & 69.8 & 92.2 & 91 & 89\\
‘Straight Hair’ & \bf82.8 & 77.0 & 72.1 &  73.6 & 76 & 73\\
‘Wavy Hair’ & 80.4 & 77.0 & 68.3 & \bf81.7 & 76 & 75\\
‘Wearing Earrings’ & \bf95.4 & 91.6 & 87.1 & 89.7 & 94 & 92\\
‘Wearing Hat’ & \bf93.0 & 90.2 & 87.0 & 80.5 & 88 & 82\\
‘Wearing Lipstick’ & \bf95.8 & 92.8 & 89.0 & 91.4 & 95 & 93\\
‘Wearing Necklace’ & \bf93.0 & 89.8 & 85.1 & 84.0 & 88 & 86\\
‘Wearing Necktie’ & \bf79.8 & 75.2 & 70.6 & 78.7 & 79 & 79\\
‘Young’ & \bf91.0 & 88.4 & 84.4 & 79.2 & 86 & 82\\
\midrule
Average & \bf87.0 & 81.4 & 74.8 &  83.1 & 84 & 81\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.25in
\end{table*}




\section{Weakly Supervised Attribute Prediction}\label{sec:weakly_sup}


\subsection*{Factorizing the activations}\label{sec:factor_layer} 

With the assumption that the observations are formed by a linear combination of few basis vectors, one can approximate a given matrix $\mX \in \R^{m \times n}$ into a \textit{basis} matrix $\mM \in \R^{m \times r}$ and an \textit{score} matrix $\mS \in \R^{r \times n}$ such that $\mX \approx \mM \mS$ where $r$ is the (reduced) \textit{rank} of the factorized matrices -- commonly $r \ll \min(m, n)$.
Methods such as NMF would restrict the entries of $\mM$ and $\mS$ to be non-negative $(\forall i,j \;  \mM_{ij} \ge 0,\; \mS_{ij} \ge 0)$ which forces the decomposition to be only \textit{additive}. This has been shown to result in a parts-based representation that is intuitively more close to human perception. It is also worth mentioning that obviously, the matrix $\mX$ needs to be positive $({\forall i,j} \;  \mX_{ij} \ge 0)$. For non-negative factorization on the activations of the DNNS, due to the common use of activation functions such as \textit{ReLU}, this would not impose any constraints in most of the problems.

Activations of the CNN networks are generally tensors of the shape $\tZ_{\ell} \in \R^{(N, C, H, W)}$ which namely represent the batch size of the input, the number of the channels, the height of each channel, and the corresponding width. To adapt such tensors for the NMF problem, we reshape the tensor into the matrix $\mZ_{\ell} \in \R^{ C \times (N * H * W)}$ by stacking it over its channels while flattening the other dimensions. This way, the channels would be embedded into a pre-defined small dimension $r$ while keeping each sample and pixels information. For the weakly-supervised problem of attribute classification using DeepFacto, we attached the DeepFacto module to the last convolutional layer of the Inception-Resnet-V1 architecture followed by a \emph{ReLU}. This layer has 1792 channels and, for a given input of the size $160 \times 160$ pixels (the original input size from the LFWA dataset), the height and the width are both equal to 3. 

\begin{figure}[htb]
\vskip -0.05in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/heatmap.jpg}
}
\caption{Heat map visualizations from three different dimensions of the score matrix $\mS$ (rows) trained by DeepFacto-32 over different samples (columns) in LFWA dataset. These dimensions can capture interpretable representations over different faces identities: \emph{eyes} (top), \emph{forehead} (middle), and \emph{nose} (bottom).}
\label{fig:heatmap}
\end{center}
\vskip -0.15in
\end{figure}

% Table \ref{table:attr_lfw} shows the prediction accuracy of each attribute in LFWA dataset and compares DeepFacto with different ranks ($r=4,32,256$) against other supervised and weakly-supervised baselines. It can be noted that our method can generate highly informative representation of the LFWA attributes without accessing their labels. This supports our conjecture that DeepFacto, by non-negatively factorizing the activations of the DNNs in and end-to-end training, can lead to an interpretable decomposition of the DNN activations.



\subsection*{Heat maps}\label{sec:heatmap}
To qualitatively investigate the interpretability of the factorized representations learned from DeepFacto, similar to \cite{collins2018deep}, one can visualize the score matrix $\mS$. Each dimension of the score matrix $\mS$ can be reshaped back to the original activation size and be up-sampled to the size of the input using bi-linear interpolation. In Figure \ref{fig:heatmap}, the score matrix learned form the DeepFacto with $r=32$ (average attribute prediction of 81.4\%) is used where three different heat maps (out of 32) are depicted over different samples from LFWA dataset. We have found $r=4$ to be very low to represent interpretable heat maps for the attributes and $r=256$ to contain redundant heat maps. It can be seen, that the heat maps can show local and persistent attention over different face identities: \emph{eyes}, \emph{forehead}, \emph{nose}, etc.


