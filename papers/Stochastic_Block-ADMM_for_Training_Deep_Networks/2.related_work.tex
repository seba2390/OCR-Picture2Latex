% \vspace{-0.15in}

\section{Related Work}\label{sec:rel_work}

Alternating Direction Method of Multipliers (ADMM) has shown promise in solving optimization problems, especially in large-scale and data-distributed machine learning applications. The power of ADMM comes from its decomposition of the augmented Lagrangian into simpler loosely-coupled sub-problems which enables it to solve each sub-problem in an efficient and potentially parallel manner.
%The standard ADMM generally needs to compute the gradient of all the samples in each update step which makes it impractical for many large scale problems. 
%To alleviate this problem, \textit{stochastic} and \textit{online} versions of the ADMM have been proposed \cite{ouyang2013stochastic,wang2013online,zhong2014fast} and their convergence proofs presented. Further, \cite{kadkhodaie2015accelerated} proposed an accelerated ADMM method which improves the convergence rate from $\mathcal{O}(\frac{1}{k})$ to $\mathcal{O}(\frac{1}{k^2})$.
%Moreover, 
ADMM extensions for non-convex problems have been recently proposed which are more suitable for large data sets and more complicated problems \cite{wang2019global,huang2018mini}.

%Since DNNs are considered to be highly non-convex functions, ADMM has been proposed for solving different optimization problems related to deep learning. \cite{kiaee2016alternating,ye2018progressive} used ADMM for model compression and parameter pruning in deep networks. \cite{murdock2018deep} developed an iterative algorithm using ADMM to enforce constraints on the latent space after a DNN has been trained. \cite{sun2016deep} proposed a deep network for MRI image reconstruction using an ADMM scheme of optimization. Those methods usually do not directly train DNNs.


A recent line of research has focused on training DNNs using optimization techniques that decompose the training into smaller subproblems, including Block Coordinate Descent (BCD) and ADMM. On the BCD algorithms, \cite{carreira2014distributed} was the earliest to propose training a DNN in a distributed setting by formulating it as a constrained optimization problem. Further, \cite{zeng2018global,zhang2017convergent,askari2018lifted,gu2018fenchel} lifted the non-convex activations (e.g. ReLU) and formulating the DNN training as a multi-convex problem and solved it using BCD and \cite{choromanska2018beyond} proposed an online method for training DNNs. 


On the other hand, \cite{taylor2016training} proposed a batch gradient-free algorithm for training neural networks using a variant of ADMM. However, due to the closed-form update of all the parameters, the proposed method has limitations (\textit{e.g.} only capable of using simple losses such as Hinge loss and MSE), and cannot be further extended into more complex problems and larger datasets. However, the scope of
\cite{zhang2016efficient} is limited to a specific application and no convergence proof is presented.


\cite{gotmare2018decoupling} splits DNNs into blocks and trained them separately by introducing gluing variables. This is very close to ADMM, but it did not use the dual variables common in ADMM and did not present a convergence proof for their method.
Recently, \cite{wang2019admm,zeng2019convergence} have provided convergence analysis of ADMM (to a stationary point) in deep learning by linearly approximating the non-linear constraints in the DNN training problem. However, their work did not address stochastic gradients as in our work.


Non-negative Matrix Factorization (NMF) imposes  non-negativity constraints over the factors, hence can lead to more interpretable decompositions than methods such as Principle Component Analysis (PCA)~\cite{lee1999learning,liu2011constrained}.  
\cite{collins2018deep} applied NMF over convolutional activations which has shown interpretable and coherent behavior over image parts. However, in their work, NMF was applied post-hoc over pre-trained CNN activations. There is no guarantee that the disentanglement is faithful to the underlying mechanism of the DNN. 
To the best of our knowledge, NMF layers jointly trained with a deep neural network have not been studied in the past.
