% \vspace{-0.1in}
\section{Conclusion and Discussion}
% \vspace{-0.05in}
In this paper, we proposed stochastic block-ADMM as an approach to train deep networks. Through updates with stochastic gradients, we improve over the capabilities to scale to larger networks using ADMM, as well as the performance. We alps presented an online version of stochastic block-ADMM for setting where computational power is limited, or when accessing to all data at once is not practical. %Although the performance of ADMM is still not up to par with SGD/Adam in residual networks, 
We have shown improvements over SGD/Adam in training deep networks without residual connections. As an illustration to how ADMM can be applied in supervised feature disentanglement, we propose DeepFacto which jointly trains an NMF layer within a deep network and show encouraging results on a supervised disentanglement benchmark, both quantitatively and qualitatively. We believe the results presented in this work set up future work that further explores aspects of utilizing ADMM in deep network training, including parallelization and stability.
