% \vspace{-0.1in}

\section{Method} \label{sec:method}

%\subsection{ADMM Training of DNNs}\label{sec:admm_nn}

%Alternating Direction Method of Multipliers (ADMM) \cite{gabay1975dual,boyd2011distributed} is a class of optimization methods belonging to  \textit{operator splitting techniques} which borrows benefits from both dual decomposition and augmented Lagrangian methods for constrained optimization. To show the potentials of standard ADMM, we first revisit a general formulation of ADMM in DNN training, similar to those used in prior work. Then, we propose our stochastic block-ADMM in the next subsection.

%To formulate training an $L$-layer DNN in a general supervised setting, we would have the following non-convex constrained optimization problem \cite{zeng2018global}:
% \vspace{-0.1in}
%\begin{align} \label{eq:obj}
	%\minimize_{ \mathcal{W}, \mathcal{A}, \mathcal{Z}} \quad &\mathcal{J}\left(\mY, \mZ_{L} \right) + \sum_{\ell = 1}^{L} \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}) \\
	% {\rm subject~to} \quad & \mA_{\ell} - {\bm \phi}_{\ell } \left( \mZ_{\ell} \right) = {\bf 0}, \quad \ell = 1,\dots, L-1   \nonumber \\
	 %{\rm subject~to} \quad & \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} = {\bf 0}, \quad \ell = 1, \dots , L \nonumber 
%\end{align}
%where $\mathcal{J}$ is the main objective (\textit{e.g.}, cross-entropy, mean-squared-error loss functions) that needs to be minimized. The subscript $\ell$ denotes the $\ell$-th layer in the network. The optimization variables are $\mathcal{W} = \{ \mW_\ell\}_{\ell=1}^{L}$, $\mathcal{A} = \{ \mA_{\ell}\}_{\ell=1}^{L-1}$, and $\mathcal{Z} = \{ \mZ_{\ell}\}_{\ell=1}^{L}$ where $\mW_\ell$, $\mZ_{\ell}$, $\mA_\ell$, and ${\bm \phi}_\ell (.)$ are the weight matrix, output matrix, activation matrix, and the activation function (\textit{e.g.}, ReLU) at the $\ell$-th layer, respectively. Note that $\mA_{0} = \mX$ where $\mX = \{ \vx_1,\dots, \vx_N \} \in  \R^{M \times N}$ is the input data matrix containing $N$ samples with input dimensionality $M$; $\mY = \{\vy_1,\dots, \vy_N \} \in \R^{C \times N}$ is the target matrix pair comprised of $N$ one-hot vector label of dimension $C$, representing number of prediction classes. Also, ${\bf r(.)}$ is the regularization term with (\textit{e.g.}, Frobenius norm $\|.\|_F^2$) corresponding penalty weight $\lambda_{\ell}$. %Note that the regularization term can be simply ignored by setting $\lambda_\ell$ to zero. 
%In this formulation, the intercept in each layer is ignored for simplicity as it can be simply be added by slightly modifying the $\mW_\ell$ and the input to each layer. 
%The formulation in Eq. (\ref{eq:obj}) breaks the the conventional multi-layer backpropagation optimization of DNNs into simpler sub-problems that can be solved efficiently (e.g. reducing to least-squares problem). %This also facilitates training in a distributed manner --- as the layers of the DNN are decoupled and the variables can be updated in parallel across layers ($\mW_\ell$) and data points (\ $\mW_\ell, \mZ_\ell, \mA_\ell$).



%To enforce the constraints in problem (\ref{eq:obj}) and solve the optimization using ADMM, we would have the following augmented Lagrangian problem:

%\begin{eqnarray} \label{eq:augmented}
%	\minimize_{ \mathcal{W}, \mathcal{A}, \mathcal{Z}} \quad &\mathcal{J}\left(\mY, \mZ_{L} \right) + \sum_{\ell = 1}^{L} \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}) \\
%	& + \sum_{\ell=1}^{L} \frac{\beta_{\ell}}{2} \| \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} + \mU_{\ell}\|_{F}^{2} \nonumber\\
%	& + \sum_{\ell=1}^{L-1} \frac{\gamma_{\ell}}{2} \| \mA_{\ell} - {\bm \phi}_{\ell}(\mZ_{\ell}) + \mV_{\ell}\|_{F}^{2}\nonumber
%\end{eqnarray}
%where $\beta_{\ell}, \gamma_\ell >0$ are the step sizes, $\mU_{\ell}$ and $\mV_{\ell}$ are the \textit{(scaled) dual variables} \cite{boyd2011distributed} for the equality constraint at the layer $\ell$. 
%Algorithm \ref{alg:admm} shows a standard ADMM scheme for optimizing Eq. (\ref{eq:augmented}). Note, the parameters are updated in a closed-form as analytical solution can be simply derived. For simplicity of the equations, we denote $\gP_\ell (.) = \frac{\beta_{\ell}}{2} \| \mZ_{\ell} - \mW_{\ell} \mA_{\ell-1} + \mU_{\ell}\|_{F}^{2} $ and $\gQ_\ell (.) = \frac{\gamma_{\ell}}{2} \| \mA_{\ell} - {\bm \phi}_{\ell}(\mZ_{\ell}) + \mV_{\ell}\|_{F}^{2}$. 
%This is solved in \cite{taylor2016training,wang2019admm} with the difference that they only enforced the constraints on the last layer $L$ with dual variables while other constraints were only loosely enforced using quadratic penalty. 

%\begin{algorithm}[htb]
%   \caption{Standard ADMM for DNN Training}
%   \label{alg:admm}
%\begin{algorithmic}
%   {\STATE \scalebox{1}{\bfseries Input:} data $\mX$, labels $\mY$}
%   \STATE  \scalebox{1}{{\bfseries Params:} $\beta_\ell >0, \gamma_\ell >0,\lambda_\ell > 0$ }
%   \STATE  \scalebox{0.8}{{\bfseries Initialize:} $\{\mW_\ell^0\}_{\ell=1}^{L}, \{ \mU_\ell^0\}_{\ell=1}^{L}, \{ \mV_\ell^0\}_{\ell=1}^{L-1}, \{\mZ^0_\ell\}_{\ell=1}^{L}, \{\mA^0_\ell\}_{\ell=1}^{L-1}\; k \leftarrow 0$ }
%   \REPEAT
%   \FOR{$\ell=1$ {\bfseries to} $L$}
%   \STATE \scalebox{1}{$\mW_\ell^{k+1} \leftarrow \argmin\; \{ \gP_\ell (.) +  \lambda_{\ell}  {\bf r}_{\ell} (\mW_{\ell}^{k})\}$}
%   \ENDFOR
%   \FOR{$\ell=1$ {\bfseries to} $L-1$}
%   \STATE \scalebox{1}{ $\mZ_\ell^{k+1} \leftarrow \argmin\; \{ \gP_\ell (.) +  \gQ_\ell (.) \}$ }
%   \STATE \scalebox{1}{$\mA_\ell^{k+1} \leftarrow \argmin\; \{ \gP_{\ell+1} (.) +  \gQ_\ell (.) \} $}
%   \ENDFOR
%     \STATE \scalebox{1}{ $\mZ_{L}^{k+1} \leftarrow \argmin\; \{ \mathcal{J}\left(\mY, \mZ_{L}^{k} \right) + \gP_L (.) \}$ }
%   \FOR{$\ell=1$ {\bfseries to} $L-1$}
%   \STATE \scalebox{1}{$\mU_\ell^{k+1} \leftarrow \mU_\ell^{k} + \mZ_{\ell}^{k+1} - \mW_{\ell}^{k+1} \mA_{\ell-1}^{k+1}$}
%   \STATE \scalebox{1}{$\mV_\ell^{k+1} \leftarrow \mV_\ell^{k} + \mA_{\ell}^{k+1} - {\bm \phi}_{\ell}(\mZ_{\ell}^{k+1})$}
%   \ENDFOR
%   \STATE \scalebox{1}{$\mU_L^{k+1} \leftarrow \mU_L^{k} + \mZ_{L}^{k+1} - \mW_{L}^{k+1} \mA_{L-1}^{k+1}$}
%   \UNTIL{some stopping criterion is reached.}
% \end{algorithmic}
% \end{algorithm}


There were many hurdles in using ADMMs for deep learning --- the global convergence proof of the ADMM \cite{deng2016global} assumes that the optimization objective is deterministic and the global solution is calculated at each iteration of the cyclic parameter updates.
% and during each iteration of the cyclic parameter updates, all the data samples are visited.
This typically requires matrix inversion and makes standard ADMM computationally expensive thus impractical for training of many large-scale optimization problems. To see a formulation of standard ADMM for training DNNs refer to the supplementary materials \ref{sec:admm_nn}.  %Specifically, for  deep learning, this would impose a severe restriction on training set size when limited computational resources are available. 
%In addition, since the variable updates in standard ADMM require matrix inversion, the extent of its applications is limit to trivial tasks \cite{taylor2016training}, making it incompetent to perform on par with the recent complex architectures introduced in deep learning (e.g. \cite{he2016deep}).

In this section, we present stochastic Block-ADMM which does not require global solution as well as an online version which further reduces the communication load. We prove the convergence of these algorithms in Sec. \ref{sec:convergence} and present its application in supervised disentanglement in Sec.~\ref{sec:deepfacto}.

\begin{figure*}[t]
\begin{center}
\subfigure[] { \label{fig:block_admm}
\includegraphics[width=0.75\linewidth]{imgs/block_admm.pdf}
}
\subfigure[] { \label{fig:block}
\includegraphics[width=0.15\linewidth]{imgs/block.pdf}
}
\end{center}
% \vspace{-.2in}
\caption{\small a) General Architecture for training DNNs proposed in Stochastic block-ADMM. b) A few differential layers selected from a parent network are stacked inside a block. The parameters $\Theta_t$ are updated by SGD in a forward-backward pass.}
%  \vskip -0.1in
\end{figure*}




% \vspace{-0.025in}

%-----------------------------------------
\subsection{Stochastic Block-ADMM}\label{sec:block_admm}
% \vspace{-0.025in}

In this section, we introduce a novel variant of ADMM for training DNNs, the stochastic block-ADMM. We first split the conventional multi-layer network architectures into an arbitrary number of \emph{blocks}, each containing only a part of the network. To make the parameters of each block independent from its neighbors, \emph{decoupling variables} \{$\mZ_t, \; t=1, \dots, T$\} are introduced as shown in Fig.~\ref{fig:block_admm}. These variables pass the information forward and backward in the architecture to train blocks in a cyclic manner until consensus is reached. Each $block_t$  consists of one or multiple differentiable layers (e.g., convolutional layers, activation layers, etc.) that are detached from the rest of the network via coupling variables. Denote the set of all learnable parameters of each $block_t$ as $\Theta_t$. As an example, a $block_t$ wrapping multiple layers can be seen in Figure \ref{fig:block}. Our formulation is:
\begin{align} \label{eq:ourformulation}
	\minimize_{ {\bm \Theta}, \mathcal{Z}}\; &\mathcal{J}\left(\mY, \mZ_{T} \right) 
	 \\
 {\rm subject~to} ~ &\bm Z_t = \mathrm{block}_{\bm \Theta_t}(\bm Z_{t-1}), \quad \mZ_{0} = \mX \nonumber
\end{align}
where ${\bf \Theta} = \{\Theta_t\}_{t=1}^{T} \text{and } \mathcal{Z} = \{\mZ_t\}_{t=1}^{T}$. $\mathcal{J}$ is the desired cost to be minimized (\textit{e.g.}, cross-entropy loss), $T$ is the total number of blocks, $\mX = \{ \vx_1,\dots, \vx_N \} \in  \R^{M \times N}$ is the input data, and $\mY = \{\vy_1,\dots, \vy_N \} \in \R^{C \times N}$ is the target label -- for $C$ classes. Note that the number of blocks $T$ can be different than the number of layers in the network $L$.

To train DNNs with this new approach, 
%similar to problem (\ref{eq:augmented}), 
we would have the following augmented Lagrangian minimization problem to enforce the equality constraints needed for training,
\begin{align} \label{eq:block_admm_unconstrained}
	\min_{ {\bf \Theta}, \mathcal{Z}} \; &\mathcal{J}\left(\mY, \mZ_{T} \right) 
	+ \sum_{t=1}^{T} \frac{\beta_t}{2} \| \mZ_t - \mathrm{block}_{\Theta_t}(\mZ_{t-1}) + \mU_t\|_F^2 \nonumber \\
	& {\rm subject~to} \quad \mZ_{0} = \mX 
\end{align}
where $\beta_t$ and $\mU_t$ are the (scaled) step size  and the Lagrange multiplier corresponding to the $t$-th Block, respectively. Our proposed Stochastic block-ADMM method for training problem (\ref{eq:block_admm_unconstrained}) is presented in Algorithm \ref{alg:blockadmm}. %In stochastic block-ADMM%, parameters of the $t$-th block, $\Theta_t$ are updated using the \emph{Stochastic Gradient Descent} optimizer or its adaptive learning rate variants (e.g. \emph{Adam} \cite{kingma2014adam}). %or second-order optimizers including Newton's method and \emph{(L)BFGS}. 
%We have found Adam to consistently outperform other counterparts, particularly in updating the decoupling variables $\mZ_t$. 
$\zeta_t$ and $\eta_t$ are the learning rates in each update step for $\mZ_t$ and $\Theta_t$, respectively. Similar to training conventional neural networks, each block is updated by first going in a forward pass through the block and update the parameters using back-propagation. Update of the block parameters $\Theta_{t}$ is done using mini-batch stochastic gradient descent or Adam. The same goes for the decoupling variables $\mZ_t$. Note, in each cycle of the parameter update in Algorithm \ref{alg:blockadmm}, all the samples of $\mZ$ are updated, while $\Theta_{t}$ is updated stochastically. In addition, due to non-convexity of primal sub-problem (Eq. \ref{eq:primal}), one can perform the primal updates for multiple steps. %However, we found one step update to be sufficient in our experiments.
In Algorithm \ref{alg:blockadmm}, we take the reverse order for updating  the decoupling variables $\mZ_t$, which we have empirically found more efficient, as analogous to backpropagation where gradient flows backwards as well.
%Although any ordering should converge in theory, a few may converge faster in practice (e.g., in Algorithm \ref{alg:admm}, we found it more stable to update the  weights $\mW_\ell$ first). 

Note that in this formulation, %no gradient is backpropagated through the entire network. To be more precise, 
backpropagation stops at each auxiliary variable $\mZ_t$ . Hence, our method can readily mitigate the long-known vanishing gradient problem by splitting a conventional DNN into arbitrary sized blocks. 
%In section \ref{exp:mnist}, the results from our proposed method, Block-ADMM, compared with baselines in dealing with vanishing gradient and performance in supervised learning are presented. Keep in mind, the strategy of splitting a conventional networks into the Blocks is completely optional and relies on the task to be accomplished e.g., for the heterogeneous problem of activation factorization as illustrated in section \ref{exp:weakly}, the activation layer to be factorized is a splitting point, $\mZ_t$.\\
During testing time, one could follow Eq. (\ref{eq:block_admm_unconstrained}) to solve an optimization problem. But in practice, it suffices to use a straight-through estimator by removing the decoupling variables and simply pass the output of each layer to the next, equivalent of doing a forward pass in a conventional DNN. %We have taken this approach in our experiments as the error induced by ignoring the decoupling variables is negligible. 
%It should be noted that the update step for $\mZ_\ell$ is dependent on the adjacent blocks, hence can only be parallelized across the data points. However, fixing $\mZ$, all the block parameters $\Theta_t$ are independent of each other, hence can be updated in parallel across blocks as well as data points.


%---------------------------- algorithm block admm ------------------------------
%  \vspace{-0.05in}
\begin{algorithm}[htb]
   \caption{Stochastic Block-ADMM}
   \label{alg:blockadmm}
\begin{algorithmic}
   {\STATE \scalebox{1}{\bfseries Input:} data $\mX$, labels $\mY$}
   \STATE  \scalebox{1}{{\bfseries Params:} $\beta_t >0, \; \zeta_t >0, \eta_t >0$ }
   \STATE  {\bfseries Define:} \scalebox{0.80}{ $\mathcal{T}({\mZ_{t}, \mZ_{t-1}, \mU_{t}, \Theta_t}) = \frac{\beta_t}{2} \| \mZ_t - \mathrm{block}_{\Theta_t}(\mZ_{t-1}) + \mU_t\|_F^2$ }
   \STATE  \scalebox{1}{{\bfseries Initialize:} $\{{\Theta_t^0}\}_{t=1}^{T}, \{ \mU_t^0\}_{t=1}^{T} ,\; k \leftarrow 0$ }
   \STATE  \scalebox{1}{{\bfseries Initialize:} $\{\mZ_t\}_{t=1}^{T}$ in a forward pass. }
   \REPEAT
   \STATE \scalebox{1}{ $\mZ_{T}^{k+1} \leftarrow \mZ_{T}^{k} - \zeta_T \nabla_{\mZ_{T}^k} ( \mathcal{J}\left(\mY_{i}, \mZ_{T}^{k} \right)$ }
   \STATE \scalebox{1}{$ + \mathcal{T}({\mZ_{T}^k, \mZ_{T-1}^k, \mU_{T}^k, \Theta_L^k})) \;  $}
   %\forall i \in \{1,\dots,N\} $}
   \FOR{$t=T-1$ {\bfseries to} $1$}
   \STATE \scalebox{1}{ $\mZ_{t}^{k+1} \leftarrow\mZ_{t}^{k} - \zeta_t \nabla_{\mZ_{t}^k} ( \mathcal{T}({\mZ_{t}^k, \mZ_{t-1}^k, \mU_{t}^k, \Theta_{t}^k})$} \\
   \STATE  \scalebox{1}{$+ \mathcal{T}({\mZ_{t+1}^{k+1}, \mZ_{t}^k, \mU_{t+1}^k, \Theta_{t+1}^k})) \; $}
%   \forall i \in \{1,\dots,N\}  $}
   \ENDFOR
   \FOR{$t=1$ {\bfseries to} $T$}
%   \STATE{${\rm draw} \; i \subset \{1,\dots,N\}$}
   \STATE \scalebox{0.9}{${\Theta_t}^{k+1} \leftarrow {\Theta_t}^{k} - \eta_t  \nabla_{\Theta_t} \mathcal{T}({\mZ_{t,i}^{k+1}, \mZ_{t-1,i}^{k+1}, \mU_{t,i}^k, \Theta_{t}^k}),$}
   \STATE \scalebox{0.9}{$draw \; i \subset \{1,\dots,N\} \; $}
   \STATE \scalebox{1}{$\mU_t^{k+1} \leftarrow \mU_t^{k} + \mZ_{t}^{k+1} - \mathrm{block}_{\Theta_t}^{k+1}(\mZ_{t-1}^{k+1})$}
   \ENDFOR
   \UNTIL{some stopping criterion is reached.}
\end{algorithmic}
\end{algorithm}
% \vspace{-0.05in}


%----------------------------
\subsection{Online Stochastic Block-ADMM}\label{sec:onlineadmm}
% \vspace{-0.025in}

The stochastic block-ADMM formulation in section \ref{sec:block_admm} is still a batch mode algorithm, in the sense that the entire training set is updated at once. This imposes restrictions on the size of the input and the number of parameters in the network when limited resources are available. %As a result, the extension to extremely large datasets, often the case in deep learning applications, can be problematic. 
Also, it does not readily accommodate to settings where data is constantly changing, such as data augmentation on the input or reinforcement learning. To overcome such limitations, we propose an \textit{online} variant of the stochastic block-ADMM in Algorithm \ref{alg:online_admm} which alternatively solves the unconstrained problem, 
\begin{align}\label{eq:scalar_dual}
	\min_{ {\bf \Theta}, \mathcal{Z}} \; &\mathcal{J}\left(\vy, \vz_{T} \right) 
	+ \sum_{t=1}^{T} \frac{\beta_t}{2} \big( \|\vz_t - \mathrm{block}_{\Theta_t}(\vz_{t-1}) \|_F^2 + u_t\big) \nonumber\\
	& {\rm subject~to} \quad \vz_{0} = \vx 
\end{align}
Although similar to the Eq. (\ref{eq:block_admm_unconstrained}), the dual variable in the online Block-ADMM is a \textit{scalar}. The benefits of this are two-folded: First, this substantially reduces the memory size needed for storing the dual variables as the optimization proceeds. Second, this considerably reduces the variance in the gradient induced by re-initializing the auxiliary variables $\vz_{\ell,i}$ when updating the block parameters at each iteration. %On the other hand, when the dual variable has the same size as the auxiliary variables as in the batch stochastic block-ADMM, the algorithm easily diverges.

% In Algorithm \ref{alg:online_admm}, as the data pair $(\vx_i, \vy_i)$ comes, first, the auxiliary variables are initialized in a forward pass though the blocks. Then, one iteration of the alternating update by ADMM is performed.

%---------------------------- algorithm block admm ------------------------------
% \vspace{-0.1in}
\begin{algorithm}[htb]
   \caption{Online Stochastic Block-ADMM }
   \label{alg:online_admm}
\begin{algorithmic}
   {\STATE \scalebox{1}{\bfseries Input:} data $\mX$, labels $\mY$}
   \STATE  \scalebox{1}{{\bfseries Params:} $\beta_t >0, \; \zeta_t >0, \eta_t >0$ }
   \STATE  {\bfseries Define:} \scalebox{0.8}{ $\mathcal{T}({\vz_{t}, \vz_{t-1}, u_{t}, \Theta_t}) = \frac{\beta_t}{2} (\| \vz_t - \mathrm{block}_{\Theta_t}(\vz_{t-1}) \|_2 + u_t)^2$ }
   \STATE  \scalebox{1}{{\bfseries Initialize:} $\{{\Theta_t^0}\}_{t=1}^{T}, \{ u_t^0\}_{t=1}^{T} ,\; k \leftarrow 0$ }
%   \STATE  \scalebox{1}{{\bfseries Initialize:} $\{\mZ_t\}_{t=1}^{T}$ in a forward pass. }
   \REPEAT 
   \FOR{$(\vx_i, \vy_i) \text{\bfseries in} (\mX,\mY)$}
   \STATE \scalebox{1}{{\bfseries Initialize:} $\{\vz_{t,i}\}_{t=1}^{T}$ in a forward pass $(\vz_{0,i} = \vx_i)$.}
   \STATE \scalebox{1}{$\vz_{T,i} \leftarrow \vz_{T,i} - \zeta_T \nabla_{\vz_{T,i}} ( \mathcal{J}\left(\vy_{i}, \vz_{T,i} \right)$ }
   \STATE \scalebox{1}{$+\mathcal{T}({\vz_{T}, \vz_{T-1}, u_{T}^k, \Theta_T^k})) \;  $}
   %\forall i \in \{1,\dots,N\} $}
   \FOR{$t=T-1$ {\bfseries to} $1$}
   \STATE \scalebox{1}{ $\vz_{t,i} \leftarrow\vz_{t,i} - \zeta_t \nabla_{\vz_{t,i}} ( \mathcal{T}({\vz_{t,i}, \vz_{t-1,i}, u_{t}^k, \Theta_{t}^k})$} \\
   \STATE  \scalebox{1}{$+ \mathcal{T}({\vz_{t+1,i}, \vz_{t,i}, u_{t+1}^k, \Theta_{t+1}^k})) \; $}
%   \forall i \in \{1,\dots,N\}  $}
   \ENDFOR
   \FOR{$t=1$ {\bfseries to} $T$}
%   \STATE{${\rm draw} \; i \subset \{1,\dots,N\}$}
   \STATE \scalebox{1}{${\Theta_t}^{k+1} \leftarrow {\Theta_t}^{k} - \eta_t  \nabla_{\Theta_t} \mathcal{T}({\vz_{t,i}, \vz_{t-1,i}, u_{t}^k, \Theta_{t}})$}
%   \STATE \scalebox{0.9}{$draw \; i \subset \{1,\dots,N\} \; $}
   \STATE \scalebox{1}{$u_t^{k+1} \leftarrow u_t^{k} + \| \vz_{t}^{k} - \mathrm{block}_{\Theta_t^{k+1}}(\vz_{t-1,i})\|_2$}
   \ENDFOR
   \ENDFOR
   \UNTIL{some stopping criterion is reached.}
\end{algorithmic}
\end{algorithm}
% \vspace{-0.125in}




%----------------------------
\subsection{Convergence of the Algorithm}\label{sec:convergence}
Let us consider the following general problem:
\begin{align}\label{eq:main}
	\minimize_{\mathcal{Z},\bf \Theta} & f(\mathcal{Z})\\
	{\rm subject to} & h(\mathcal{Z},\bm \Theta)=\bm 0,\nonumber
\end{align}
where $\mathcal{Z}$ and $\bf \Theta$ are as defined in Sec.~\ref{sec:block_admm}, and $f(\cdot)$ represents the training objective, and $h(\cdot)$ represents the layer coupling equalities as in \eqref{eq:ourformulation}.
We also assume that both $f(\cdot)$ and $h(\cdot)$ are differentiable functions. Note that both $f$ and $h$ can be non-convex.

Let us consider the following augmented Lagrangian:
\[          {\cal L}_{\rho_k}(\mathcal{Z},{\bf \Theta},{\bm \lambda})=f(\mathcal{Z}) + \langle \bm \lambda, h(\mathcal{Z},{\bf \Theta})\rangle + \frac{1}{2\rho_k}\|h(\bm Z,\bf \Theta)\|_2^2,  \]
where $\bm \lambda$ collects all the dual variables $\bm U_1,\ldots,\bm U_T$ that correspond to different layers. The standard primal-dual updates can be summarized as follows:
\begin{subequations}\label{eq:stopdd}
\begin{align}
    (\bm Z^{k+1},\Theta^{k+1}) &\leftarrow  \arg\min_{\mathcal{Z},\bf \Theta}  {\cal L}_{\rho_k}(\mathcal{Z},\bf \Theta,\bm \lambda^k), \label{eq:primal}\\
    \bm \lambda^{k+1} &\leftarrow \bm \lambda^k + \frac{1}{\rho_k}h(\bm Z^{k+1}, \Theta^{k+1}),
\end{align}
\end{subequations}
%In our case, since the sub-problem in \eqref{eq:primal} is non-convex, exactly minimizing this function may not be possible. In the previous section, the primal update is carried out by stochastic optimization w.r.t. $\mathcal{Z}$ and $\mathbf{\Theta}$ in an alternating fashion---which is a computationally lightweight algorithm that converges to a stationary point of ${\cal L}_{\rho_k}(\mathcal{Z},\mathbf{ \Theta},\bm \lambda^k)$ under certain conditions \cite{bottou2012stochastic,xu2015block}.
%The convergence of this type of primal-dual algorithm with inexact stochastic solution for the primal problem is unclear. In this work, we offer convergence support for our designed deep network training algorithm. Our idea follows recent work in \cite{shi2017penalty} that handles deterministic primal problems under non-convex equality constraints. 
We employ the trick in \cite{shi2017penalty} for adaptively adjusting the parameter $\rho_k$. We assume that $\rho_k$ is adjusted by
\begin{align}\label{eq:rho}
    \rho_{k+1} \leftarrow \begin{cases}  \rho_k,&\quad \|h(\bm Z^{k},\bm \Theta^k)\|\leq \eta_k,\\
                                         c\rho_k,~0<c<1,&\quad {\rm o.w.}
    \end{cases}
\end{align}
where $\eta_k$ for $k=1,2,\ldots$ is a pre-specified sequence that bounds the equality-enforcing error.

Our analysis shows the following convergence result:
% \vspace{-0.1in}
\begin{Prop}\label{prop:convergence}
    Assume $h({\cal Z},\bm \Theta)=\bm 0$ satisfies the Robinson's condition.
    Also assume for each update in \eqref{eq:primal}, the sub-problem solution solved by stochastic alternating optimization satisfies
   \begin{equation}
       \mathbb{E}\left[ \left\| {\cal G}(\x^k) \right\|^2\right]\leq \varepsilon_k, ~ \mathbb{V}\left[  {\cal G}(\x^k) \right]\leq \sigma_k^2,
   \end{equation}  
   where $\x=({\cal Z},\bm \Theta)$ is a vector that collects all the optimization variables and ${\cal G}(\x^k)$ collects the stochastic gradients that we used for updating $({\cal Z},\bm \Theta)$.
   Assume that the stochastic gradient for the primal update is unbiased, i.e.,
   \begin{equation}\label{eq:unbiasedness}
       \mathbb{E}[{\cal G}(\x^k)] = \nabla {\cal L}_{\rho_k}(\x_k),~\forall k. 
   \end{equation}         
   Then, every limit point of the solution sequence produced by the algorithm in \eqref{eq:stopdd} converges to a KKT point of the problem in~\eqref{eq:main}, if $\eta_k\rightarrow 0$, $\sigma_k^2 \rightarrow 0$ and $\varepsilon_k\rightarrow 0$.	
\end{Prop}
% \vspace{-0.05in}
% {\it \bf Proof}: 
The proof for Proposition~\ref{prop:convergence} is presented in the supplementary materials \ref{sec:proof}.
Proposition~\ref{prop:convergence} asserts that the algorithm converges to a KKT point under some conditions. 
   There are a number of remarks regarding implementation.
   To begin with, the condition $\varepsilon_k\rightarrow 0$ means that the primal problem needs to be solved more and more accurately when $k$ grows, in terms of approaching the stationary point of the sub-problem using block stochastic gradient. This can be achieved via gradually increasing the number of iterations for the primal updates. Note that stochastic block gradient can provably attain $\mathbb{E}[\|{\cal G}(\X^k)\|^2]\leq \varepsilon_k$; see \cite{xu2015block}. 
%   \comment{
%   In addition, the condition $\sigma_k^2\rightarrow 0$ means that the variance of the stochastic gradient needs to shrink when $k$ increases. This can be achieved by increasing the batch size when $k$ grows; see discussions in \cite{xu2015block}. 
%   Hence, {\it in theory}, to satisfy both conditions, the complexity for carrying out each iteration $k$ may grow.
%   Nonetheless, our empirical experience shows that using a fixed number of iterations for stochastic primal optimization and a fixed batch size in general does not hurt convergence. We hypothesize that the momentum may play an important role in increasing the effective batch size since with momentum gradients from previous batches are remembered and utilized in the subsequent steps. We leave further analysis with momentum to future research. Another remark is that the unbiasedness of the primal stochastic gradient [cf. \eqref{eq:unbiasedness}] is not always easy to establish under block coordinate descent settings \cite{xu2015block}. Nonetheless, this can be fixed via a simple randomization strategy among the blocks \cite{fu2019block}. }
   
   %The third challenge is that the hyperparameter $c$ and the sequence $\{\eta_k\}$ are not necessarily easy to select in some cases~\cite{shi2017penalty,fu2018anchor}. These parameters control how quickly one should adjust the update settings to accommodate the current iteration, which varies from case to case. However, interestingly, we find that these hyperparameters are relatively easy to tune in our case. In particular, our extensive experiments show that fixed $\rho_k$ and $\eta_k$ work reasonably well for our deep learning problems.
   
   %The slightly stringent conditions in Proposition~\ref{prop:convergence} and the relatively `benign' convergence behavior observed in practice pose a gap between theory and practice---and an interesting direction for future research.



%----------------------------
\subsection{DeepFacto: Factorization of DNN Activations }\label{sec:deepfacto}

%To show the power and flexibility of our proposed method in training heterogeneous networks, we will 
Here, we investigate a task for supervised disentanglement, which can provide insights for explaining DNNs to humans. Supervised disentanglement aims to find disentangled factors that decide the CNN output, yet are human-understandable and distinct from each other. One approach to learn a disentangled representation is through adding  non-negative matrix factorization (NMF)\cite{lee1999learning} layers to the network \cite{collins2018deep}. Note that NMF imposes non-differentiable constraints into the network where conventional end-to-end training using backpropagation would not be applicable. Hence, prior work were mostly running NMF after the training, where the network might have already learned highly entangled features. In this work, aided with our stochastic block-ADMM, we attempt to perform training with NMF layers in the intermediate layers of DNNs.

Figure \ref{fig:deepfacto} shows an \emph{NMF module} with \emph{rank $r$} incorporated between two arbitrary neighboring blocks. The output from the $block_t$ is factorized into $\mM_t$ and $\mS_t$, namely, the basis and score matrices. In this configuration, only the score matrix $\mS_t$ is passed to the next blocks. The score matrix is low-rank, sparse and non-negative hence can possibly represent features that are more disentangled than the original network. 
Exploring this architecture is one attempt of us in making deep networks more explainable to humans. Humans would not be able to interpret conventional deep network weights which are both positive and negative and sometimes cancels out each other. The sparse and non-negative feature from NMF would be much more preferable to interpret~\cite{collins2018deep}.

However, the NMF module breaks the gradient path from $\mS_t$ to $Z_t$, hence conventional backpropagation would not be applicable in this problem. We extend the ADMM framework (\ref{eq:block_admm_unconstrained}) into having non-negative factorization constraints over its activations and formulate the following optimization problem:
\begin{eqnarray} \label{eq:block_admm_nmf}
	\min_{ {\bf \Theta}, \mathcal{Z}, \mS, \mM} \; &\mathcal{J}\left(\mY, \mZ_{T} \right) \nonumber \\
	+ & \sum_{k=1, k\neq t+1}^{T} \frac{\beta_k}{2} \| \mZ_k - block_k(\mZ_{k-1}) + \mU_k\|_F^2 \nonumber \\
	+ & \frac{\beta_{t+1}}{2} \| \mZ_{t+1} - block_{t+1}(\mS_{t}) + \mU_{t+1}\|_F^2 \nonumber \\
	+ &  {\frac{\gamma_t}{2} \| \mZ_t - \mM_t \mS_t + \mV_t\|_F^2} \nonumber \\
	  & {\forall i,j} \;  \mM_{\ell,ij} \ge 0,\; \mS_{\ell,ij} \ge 0 
\end{eqnarray}
where $\gamma_t$ is the step-size and $\mV_t$ is the corresponding multipliers to enforce the matrix factorization equality $\mZ_t = \mM_t \mS_t$. The NMF module adds a nonconvex term to the optimization. However, in the alternating optimization scheme, while keeping either $\mM_t$ or $\mS_t$ constant, solving for the other term would reduce to a normal convex least-squares problem. The rest of the updates are the same as in section \ref{sec:block_admm}. Note that, trivially to not change the input dimension of the next block after the NMF module, one can simply add an affine layer to increase the dimensions without changing the formulation.
% , as the linear layer can be easily regarded in the parameter space of the next block. 

At testing time, one only needs to perform a non-negative projection since the basis matrix $M$ will be given, which can be solved using a convex solver such as LBFGS. Note that for simplicity, we only formulated adding \emph{one} NMF module in the middle of the blocks. This can be simply extended to as many NMF modules as needed in the architecture.

%---------------------------- Figure admm nmf  ------------------------------
\begin{figure}[t!]
% \vskip -0.1in
\begin{center}
\centerline{
\includegraphics[width=1 \columnwidth]{imgs/block_admm_nmf.pdf}
}
%  \vskip -0.1in
 \caption{General architecture for Deepfacto: an NMF module with rank $r$ is added in the middle of two arbitrary blocks. Note, only $\mS_t$ is passed to the next blocks.}
%\vspace{-0.05in}
\label{fig:deepfacto}
\end{center}
%  \vskip -0.3in
\end{figure}
