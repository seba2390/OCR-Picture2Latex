% \vspace{-0.1in}
\section{Experiments} \label{sec:exp}


%Our implementation uses the PyTorch framework \cite{paszke2017automatic}. 
All the experiments are run on a machine with a single NVIDIA GeForce RTX 2080 Ti GPU. The results presented for each of the following experiments are selected from their best performance after grid search over the hyper-parameters, both for our method and the baselines. %Note, in the following Figures (\ref{fig:mnist_acc}, \ref{fig:mnist_deep}, \ref{fig:cifar}), 
Each algorithm is ran five times with different initialization and the average test set accuracy is reported. The shaded area corresponds to $\pm1$ standard deviation. We will make our code available online. %for reproducibility. %and a more precise access to the set of parameters used in the experiments.

% \vspace{-0.05in}
\subsection{Supervised Deep Network Training}\label{exp:conv}
In this section, we present the experiment results from training conventional neural networks in a supervised setting on the MNIST, Fashion-MNIST, and CIFAR-10 datasets. For experiments results on Fashion-MNIST and CIFAR-10, see supplementary materials \ref{sec:sup_train}. %The results from the proposed methods in section \ref{sec:method} are compared with baselines including training a conventional neural network in an end-to-end setting using SGD. 
% For the CIFAR-10 dataset, our formulation enables training complicated networks such as ResNets \cite{he2016deep} using ADMM which has never been done before.


\subsubsection{MNIST}\label{exp:mnist}
For the first supervised learning experiment, the MNIST dataset of handwritten digits \cite{mnist}, is used for the evaluation of ADMM/BCD methods for training DNNs. We use the standard train/test split. %Throughout the experiments, 60,000 samples are used during training and %
The performance on the testing set of 10,000 samples is reported in Figure \ref{fig:mnist_acc}. The architecture of the \emph{shallow} network used for the experiments incorporates three fully-connected layers with 128-neuron hidden layers $(784-128-128-10)$ and \emph{ReLU} nonlinearity. In order to make a fair comparison with ~\cite{taylor2016training} which can only work with Mean Squared Error (MSE), we utilize MSE as the training objective ($\mathcal{J}$) while the more common Cross-Entropy (CE) is applicable in our block-ADMM formulation and utilized in the experiments in the supplementary materials. 


In training standard ADMM and \cite{taylor2016training} as baselines, all the parameters are initialized by sampling from the uniform distribution $x \sim {U}(0, 10^{-4})$.% and are down scaled by a factor of $1e^{-4}$. 
We set $\beta_l = \gamma_l = 10$ for all of the layers. 
%Note that Algorithm \ref{alg:admm} can be converted to the formulation in \cite{taylor2016training} by setting dual variables $\forall \ell \neq L \; \mU_\ell = \bm 0;$ and discarding their updates. 
%To regularize the weights, $\mW_l$ during the training, \emph{$L_2$} norm 
Weight decay is used with $\lambda_l = 5 \times 10^{-5}$. %We observed that the regularization term significantly improves the optimization behavior in standard ADMM and without it, the training is not stable. 
For baselines with backpropagation in Fig. \ref{fig:mnist_acc}, a  learning rate of $5 \times 10^{-3}$ is used. 
 
 
Further, for the training of the batch and online Stochastic Block-ADMM algorithms presented in Algorithm \ref{alg:blockadmm} and \ref{alg:online_admm}, the aforementioned three-layer architecture is split into 3 one-layer blocks. $\beta_t$ is set to 1 for all layers, the weights are initialized using the normal distribution, dual variables $\mU_t$ are initialized using a uniform distribution, and auxiliary variables $\mZ_t$ are initialized in a forward pass. During training, the block parameters ($\Theta_t$) are updated stochastically, and both of sub-problem updates for the $\text{block}_{\Theta_t}$ and $\mZ_t$ are performed using \textit{Adam}. In our experiments in the batch mode, we performed the primal updates for $3$ steps during each iteration. For the online version, we set the batch size to 64 and auxiliary variables are re-initialized at each iteration (see Algorithm \ref{alg:online_admm}). 

Figure \ref{fig:mnist_acc} shows that Stochastic Block-ADMM outperforms the baselines by reaching $97.61 \%$ average test accuracy. Note the accuracy for all methods is lower than normal because of the MSE loss function that is used --- which is not the best choice for classification yet chosen for fair comparison with previous ADMM methods. The online version performs slightly worse with a $93.88 \%$ test accuracy. However, this comes with enormous advantage in terms of memory utilization, e.g. given the configuration for training on MNIST, the online version uses \~ 10$\times$ less memory to store training variables compared to the batch version.


%---------------------------- fig mnist acc ------------------------------
\begin{figure}[ht]
%  \vskip -0.05in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/mnist_acc_new.pdf}
}
%  \vskip -0.05in
\caption{Test set accuracy on MNIST using network with 3 fully-connected layers: $784-128-128-10$. 
Final test accuracy: ``Stochastic Block-ADMM'': {\bf 97.61\%}, 
``Online Stochastic Block-ADMM'': 93.88\%, 
``Standard ADMM'': 95.02\%, 
%``Taylor et al.,  
\protect \cite{taylor2016training}
: 87.52\%, 
%``Wang et al. 
\protect \cite{wang2019admm}: 83.89\% ,
%``Zeng et al. 
\protect \cite{zeng2018global}: 83.28\% , 
``SGD'': 95.29\% 
(Best viewed in color)}
\label{fig:mnist_acc}
\end{center}
%  \vskip -0.4in
\end{figure}


\subsubsection{Vanishing Gradient}\label{exp:vanish}

Since no gradient is backpropagated through the entire network in our proposed algorithm, stochastic block-ADMM is robust against vanishing gradients. We run the previous experiments on an unconventional architecture with 10 fully-connected layers --- this is to make the vanishing gradient problem obvious. Note that normally this will not be adopted because of the severe overfitting and gradient vanishing problems, but here we utilized this setting to test our resistance to these problems. Figure \ref{fig:mnist_deep} illustrates the experiment results. Stochastic Block-ADMM reaches final test accuracy of $94.43\%$ while SGD and ADAM only reach to $10.28\%$ and $58\%$, respectively. As it can be seen in Figure \ref{fig:mnist_deep}, we also compared our method with the recent work of \cite{zeng2018global}. We observed the BCD in \cite{zeng2018global} %\footnote{code taken from \url{https://github.com/timlautk/BCD-for-DNNs-PyTorch}} 
to be unstable, sensitive to network architectures, and eventually, not converging after 300 epochs. Although we still exhibited some overfitting, we can see our approach is significantly better in handling of the vanishing gradient problem, and performs reasonably well. We further tested our performance with 20 fully-connected layers. Results show that although there is slightly more overfitting, our algorithm can still find a reasonable solution (Fig.~\ref{fig:mnist_deep}), showing its potential in helping with training scenarios with vanishing gradients.



%---------------------------- fig deep mnist ------------------------------
\begin{figure}[ht]
% \vskip -0.05in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/mnist_deep.pdf}
}
% \vskip -0.05in
\caption{Test accuracies from deep architectures on MNIST. Block-ADMM demonstrates stable convergence and obtains final test accuracy of $\bf 94.43\%$ (10 layers), and $91.75\%$ (20 layers) respectively, while SGD and Adam (10 layers) fail due to vanishing gradients (Best viewed in color)}
\label{fig:mnist_deep}
\end{center}
% \vskip -0.35in
\end{figure}



%----------------------------
\subsubsection{Wall Clock Time Comparison}\label{time_cmp}

In this section, we analyze the batch and online versions of stochastic block-ADMM in training wall clock time and compare them against other baselines as illustrated in Figure \ref{fig:time}.  %The methods are implemented in PyTorch framework -- except for \cite{wang2019admm} that is implemented\footnote{code taken from \url{https://github.com/xianggebenben/dlADMM}} in "cupy", a NumPy-compatible matrix library accelerated by CUDA. 
Note Gotmare \etal and SGD are trained with a mini-batch size of 64 and \cite{zeng2018global,wang2019admm} are trained in a batch setting. Only the time taken for the \emph{training} was plotted in Fig.~\ref{fig:time} and stages such as initialization, data loading, etc were excluded. The online version shows faster convergence than \cite{gotmare2018decoupling} and simple SGD. Although \cite{zeng2018global} and \cite{wang2019global} have been convergence rates due to being batch methods, our approach achieves higher performance later on.% It can be also observed that our stochastic block-ADMM approach has comparable convergence speed with \cite{zeng2018global} while having noticeably superior performance over other baselines. We speculate that enforcing all the constraints by dual variables along with the efficient and cheap mini-batch updates in our method highly contributes to the convergence speed as well as its performance superiority over the other methods, including \cite{zeng2018global}. 


%---------------------------- fig time cmp mnist ------------------------------

\begin{figure}[ht]
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{imgs/time_comparison_new.pdf}
}
% \vskip -0.1in
\caption{Test set accuracy v.s. training wall clock time comparison of different alternating optimization methods for training DNNs on the MNIST dataset. Our methods (blue and orange) show superior performance vs. \protect\cite{zeng2018global} and \protect\cite{wang2019global} while converge faster than all other methods}
%  \vskip - 0.15in
\label{fig:time}
\end{center}
% \vskip -0.15in
\end{figure}



% %----------------------------
% \subsubsection{CIFAR-10}\label{exp:cifar}

% The previous works on training deep netowrks using ADMM have been limited to trivial networks and datasets (e.g. MNIST) \cite{taylor2016training,wang2019admm}. However, our proposed method does not have many of the existing restrictions and assumptions in the network architecture, as in previous works do, and can easily be extended to train non-trivial applications. It is critical to validate stochastic block-ADMM in settings where deep and modern architectures such as deep residual networks, convolutional layers, cross-entropy loss function, etc., are used. To that end, we validate the ability of our method is a supervised setting (image classification) on the CIFAR-10 dataset \cite{cifar} using ResNet-18 \cite{he2016deep}. To best of our knowledge, this is the first attempt of using ADMM for training complex networks such as ResNets. 


% For this purpose, we used 50,000 samples for training and the remaining 10,000 for evaluation. 
% To have a fair comparison, we followed the configuration suggested in \cite{gotmare2018decoupling} by converting Resnet-18 network into two blocks $(T=2)$, with the splitting point located at the end of {\sc conv3\_x} layer. We used the Adam optimizer to update both the blocks and the decoupling variables with the learning rates of $\eta_t = 5e^{-3}$ and $\zeta_t = 0.5$. We noted since the auxiliary variables $\mZ_t$ are not "shared parameters" across data samples, they usually require a higher learning rate in Algorithm \ref{alg:blockadmm}. Also, we found the ADMM step size $\beta_t = 1$ to be sufficient for enforcing the block's coupling. 


% Figure. \ref{fig:cifar} shows the results from our method compared with two baselines: \cite{gotmare2018decoupling}, and conventional end-to-end neural network training using back-propagation and SGD. Our algorithm consistently outperformed ~\cite{gotmare2018decoupling} however cannot match the conventional SGD results. There are several factors that we hypothesize that might have contributed to the performance difference: 1) in a ResNet the residual structure already partially solved the vanishing gradient problem, hence SGD/Adam performs significantly better than a fully-connected version; 
% % 2) The common data augmentation in CIFAR will end up sending a different training example to the optimization algorithm at each iteration, which does not seem to affect SGD but seem to affect ADMM convergence somewhat; 
% 2) we noticed decreasing the learning rate for $\Theta_t$ updates does not impact the performance as it does for an end-to-end back-propagation using SGD. Still, we obtained the best performance of ADMM-type methods on both MNIST and CIFAR datasets, showing the promise of our approach.
% % As illustrated, ADMM gets to a good performance fast and then slowly progress to higher accuracy..


% %---------------------------- fig cifar  ------------------------------

% \begin{figure}[htb]
% % \vskip 0.15in
% \begin{center}
% \centerline{
% \includesvg[width=\columnwidth]{imgs/cifar.svg}
% }
% % \vskip -0.05in
% \caption{Test set accuracy on CIFAR-10 dataset. Final accuracy "Block ADMM": $89.66\%$, "Gotmare \etal":$87.12 \%$, "SGD": $\bf 92.70\%$. (Best viewed in color.)}
% \label{fig:cifar}
% \end{center}
% % \vskip -0.2in
% \end{figure}

%----------------------------
\subsection{Supervised Disentangling on LFWA}\label{exp:hetero}


%----------------------------
In this section, we showcase the flexibility of stochstic block-ADMM in trainig deep networks with non-differentiable layers where conventional backpropagation cannot be used. For that purpose, we evaluate our proposed method in a supervised disentanglement problem where we used DeepFacto \ref{sec:deepfacto} to learn a nonnegative factorized representation of the DNN activations while training end-to-end on the LFWA dataset \cite{LFWTech}. Next, similar to \cite{liu2018exploring}, linear SVMs are used over the factorized space to predict face attributes. This setup examines the capability of the network to extract a disentangled representation that linearly corresponds to human-marked attributes that the network does not have prior knowledge of.

%This is to show the discriminative power of a disentangled representation. Note that there is no supervision over the attributes during the training of DeepFacto. 
% LFWA \cite{LFWTech} is a face verification dataset that contains 13,233 images with 72 attribute tags from 5,749 distinct people. 
We used the Inception-Resnet architecture from \cite{schroff2015facenet}, pre-trained on the VGGFace-2 \cite{Cao18} dataset as the back-bone. To incorporate an NMF, we follow the same approach as in Fig.~\ref{fig:deepfacto} where the pretrained DNN is the first block, and we add a simple fully-connected layer over the score matrix $\mS_t$ to train a face-verification network with a triplet loss~\cite{hoffer2015deep}.
%The choice of a simple fully-connected layer is two-folded. First, to lift the dimensions of the embedding needed for training, particularly when $\mS_t$ is low rank. Second, the embedding would be only a linear combination of the score matrix $\mS_t$, directly guiding it using the supervised signal coming from the Triplet Loss . 
We conjecture the score matrix $\mS_t$ will be guided to learn an disentangled factorization due to the nonnegativity constraint \cite{collins2018deep}. 
%Note that the latest activation in the network that is followed by a ReLU is selected from the Resnet-Inception network. This is due to the nonnegative constraint in the NMF, i.e. the input to the NMF should be also positive.
To have a warm start for an end-to-end training of DeepFacto, we first pre-train the NMF module having the Inception-Resnet block freezed. Then, we fine-tune the block parameters as well as the NMF module in an alternating fashion, similar to Algorithm \ref{alg:blockadmm}. Note, the rank of the NMF in DeepFacto is a hyperparameter and we selected three different values ($r=4, 32, 256$) in the experiments. The final $r=256$ is also the latent space dimensionality in \cite{liu2018exploring}.
Table. \ref{table:lfw} illustrates average prediction accuracy over LFWA attributes
% \footnote{The common 40 attributes with Celeb-A dataset \cite{liu2015faceattributes}} 
from DeepFacto and other supervised and weakly supervised baselines. This validates that DeepFacto has learned a meaningful representation of the attributes by disentangling the activations. To see visualization for individual dimensions learned by DeepFacto see supplementary materials \ref{sec:weakly_sup}.%, and the methodolgy to reshape the activations tensors into a matrix, 


% More details about the experiments are presented in the supplementary material.


\begin{table}[t]
\caption{Average prediction accuracy on 40 attributes from LFWA dataset. Weakly-supervised methods train the network without access to attribute labels. Final classification then comes from a linear SVM on their latent representations.}
\label{table:lfw}
\begin{center}
\begin{small}
\begin{sc}
% \vskip -0.15in
\begin{tabular}{lcccr}
\toprule
LFWA & Accuracy \\
\midrule
\cite{zhang2014panda} {\tiny (supervised)}                      &  81.00\%\\
\cite{liu2015deep} {\tiny (supervised)}                         &  84.00\%\\
\cite{liu2018exploring}  {\tiny (weakly-supervised)}             &  83.16\%\\
Deepfacto - rank 4 {\tiny (weakly-supervised)}                   & 74.80\%\\
Deepfacto - rank 32 {\tiny (weakly-supervised)}                  & 81.39\%\\
Deepfacto - rank 256 {\tiny (weakly-supervised)}                 & \textbf{87.03}\%\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vskip -0.25in
\end{table}




% \textsc{Factorize the latent space\;} 
% {\textsc{Heatmaps.}} \cite{collins2018deep}
% {\subsection{all positive network.}}
