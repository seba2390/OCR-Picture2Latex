% \vspace{-0.15in}
\section{Introduction} \label{sec:intro}

Deep Neural Networks (DNNs) are highly non-convex functions with ill-conditioned Hessians and are believed to have multiple local minima and saddle points. Most networks are trained with %This makes training them very challenging~\cite{dauphin2014identifying}. Optimization methods for training DNNs have been an active line of research; commonly, 
Stochastic Gradient Descent (\textit{SGD}) and its adaptive learning rate variants \textit{e.g.,} \textit{Adam} \cite{kingma2014adam} are used to optimize the DNNs with backpropagation. Although these approaches have been the most successful, they suffer from issues such as vanishing gradients in deep layers, a significant memory footprint for storing the gradients, and difficulty to parallelize across layers because backpropagation has to be done sequentially\cite{taylor2016training}. In addition, in the presence of non-differentiable layers, conventional backpropagation training cannot be applied.% as error signals can not pass through.


Alternating Direction Method of Multipliers (ADMM) is a simple yet powerful approach that decouples optimization variables and optimizes the augmented Lagrangian in a primal-dual scheme. %It is known to be a simple yet powerful algorithm to solve convex problems with linear convergence rate in a distributed and parallel manner \cite{boyd2011distributed}. 
It has shown promise in solving certain families of non-convex problems \cite{wang2019global,huang2018mini}.
Recently, optimization of the neural networks with such alternating direction techniques has gained rising attention \cite{zeng2018global,zeng2019convergence,zhang2017convergent,gu2018fenchel,askari2018lifted} which would potentially avoid the disadvantages of the SGD and introduce beneficial properties such as \textit{fast(er)} convergence, ease of \textit{parallelization} and \textit{distributed} training, and being able to enforce additional (non-differentiable) constraints on the DNN tensors.


Despite their advantages, there are several reasons ADMM-like methods are not widely used in DNN training. The performance of these methods is usually not as good as conventional backpropagation with SGD variants, the algorithms are usually batch mode which directly restricts the number of trainable parameters and training data as well, updates are in closed-from which prohibits the use of complicated architectures while being memory intensive, \textit{etc.} Further, existing ADMM-like methods have restrictive assumptions in the architecture of the network which prohibits the extension to non-trivial networks such as ResNets \cite{he2016deep}. Work of \cite{taylor2016training} is of this kind which, despite the parallelization capabilities introduced by ADMM, the size of the training data is linearly limited by the number of cores. 
% \cite{gotmare2018decoupling} proposed to split DNN into blocks using gluing variables similar to ADMM, but they did not utilize dual variables as in ADMM.


In this paper, we propose Stochastic Block-ADMM which addresses the aforementioned issues. Stochastic Block-ADMM separates DNN parameters into an arbitrary number of blocks and uses stochastic gradients to update each block. The error signals are passed between the blocks by introducing auxiliary variables at the splitting points. We present both batch and online versions of the Stochastic Block-ADMM which can be extended to settings where computational resources are limited, data is constantly changing such as in reinforcement learning or training with data augmentation techniques. We provide a convergence proof for the proposed approach and verify its performance on several deep learning benchmarks.


%Unlike in conventional end-to-end backpropagation training scheme, a
An ADMM formulation of deep networks also allows us to add additional \textit{non-differentiable} constraints to the learning problem. In this paper, %as an illustration of such potential application using ADMM, 
we explore the problem of supervised feature disentanglement by inserting non-negative factorization layers into the network. Nonnegative Matrix Factorization (NMF) has been shown to generate sparse and interpretable representations due to the non-negative constraints over the factorization matrices \cite{lee1999learning}. Jointly training an NMF decomposition with deep learning adds non-differentiable non-linearity and \textit{cannot} be addressed by the conventional backpropagation with SGD algorithms. We show results training these networks via ADMM and their performance on a supervised feature disentanglement benchmark.
% \cite{collins2018deep} proposed applying NMF over different activations of CNN for object localization. 
% This would support the belief that the CNNs would learn semantic part-of-object filters during training \cite{gonzalez2018semantic, bau2017network}. 


In summary, our paper makes the following contributions:
\begin{itemize}
% \vspace{-0.05in}
    \item We propose Stochastic Block-ADMM for training deep networks. This improves over previous ADMM approaches (in training deep networks) which only work in batch setting.
    % Experiments show that our algorithm outperforms previous attempts of using ADMM in deep network training.
    \item We propose an online variant of the Stochastic Block-ADMM for further efficiency in computations. 
    \item We prove the convergence of the proposed Stochastic Block-ADMM algorithm.
    \item We propose DeepFacto, which jointly trains a non-negative matrix factorization layer with a deep network using ADMM, and show its capability in supervised feature disentanglement.
    % \vspace{-0.05in}
\end{itemize}

