\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2023}{}{}{}{H. J. Escalante and A. Kruchinina}

% Short headings should be running head and authors last names

\ShortHeadings{Academic Competitions}{Escalante and Kruchinina}
\firstpageno{1}

\begin{document}

\title{Academic Competitions}

\author{\name Hugo Jair Escalante \email hugo.jair@inaoep.mx \\
       \addr Instituto Nacional de Astrof\'isica,\\ \'Optica y Electr\'onica,\\ 
       Tonantzintla, 72840, Puebla, Mexico 
       \AND
       \name Aleksandra Kruchinina \email aleksandra.kruchinina@universite-paris-saclay.fr \\
       \addr Universit\'e Paris Saclay\\
       Paris, France}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Academic challenges comprise effective means for (i) advancing the state of the art, (ii) putting in the spotlight of a scientific community specific topics and problems, as well as (iii) closing the gap for under represented communities in terms of accessing and participating in the shaping of research fields.   Competitions can be traced back for centuries and their achievements have had great influence in our modern world. Recently,  they (re)gained popularity, with the overwhelming amounts of data that is being generated in different domains, as well as the need of pushing the barriers of existing methods, and available tools to handle such data.
This chapter provides a survey of academic challenges in the context of machine learning and related fields.  We review the most influential competitions in the last few years and analyze  challenges per area of knowledge. The aims of scientific challenges, their goals, major achievements and expectations for the next few years are reviewed.
\end{abstract}

\begin{keywords}
Academic competitions and challenges, Survey of academic challenges, Impact of academic competitions.
\end{keywords}

\section{Introduction}
\label{sec:intro}
%\hugo{Brief intro, briefly mentioning benefits, goals, and pointing to others chapters of the book.}
%\jvr{The chapter contains a lot of abbreviations. While I usually prefer to avoid them, I agree with most of these (we don't want to spell out NIST everytime in full, since the abbreviation is more common than the full term). However, not all abbreviations are explained well, e.g., TREC, CLEF, OCR, ICDAR, ...}
%\jvr{the chapter contains many websites. I wonder about the persistency of such website, e.g., Yan LeCuns website will be taken down once he retires, right? Would it be better to include a screenshot sometimes?}
Competitions are nowadays a key component of academic events, as they comprise effective means for making rapid progress in specific topics. By posing a challenge to the academic community, competition organizers contribute to pushing the state of the art in specific subjects and/or to solve problems of practical importance. In fact, challenges are a channel for the reproducibility and validation of experimental results in specific scenarios and tasks.

%Competitions can be traced back for centuries: consider for instance the Longitude Act established in 1714 (see Section~\ref{sec:historicalreview}) that resulted in a breakthrough for  navigation at the time. 
We can distinguish two types of competitions: those associated to industry or aiming at solving a practical problem, and those that are associated to a research question (academic competitions). While sometimes it is  difficult to typecast competitions in these two categories, one can often identify a tendency to either variant. This chapter focuses on academic competitions, although some of the reviewed challenges are often associated to industry too. 
An academic competition can be defined as a \emph{contest that  aims to answer a scientific question via crowd sourcing where participants propose innovative solutions, ideally the challenge will  push the state-of-the-art and have a long-lasting impact and/or an established benchmark.} In this context, academic competitions relying on data have been organized for a while in a number of fields like  natural language processing~\citep{trec93}, machine learning~\citep{NIPS2004_5e751896} and  knowledge discovery in databases\footnote{\url{https://www.kdd.org/kdd-cup/view/kdd-cup-1997}}, however, their spread and impact has %growth exponentially
considerably increased during the last decade, see Figure~\ref{fig:nbr_compet_years} for statistics of the CodaLab platform~\citep{codalab_competitions_JMLR}. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{Figures/Nbr_compet_year2.png}
    \caption{Evolution of the number of competitions each year. Data gathered from {\em CodaLab Competitions} \citep{codalab_competitions_JMLR}, a platform with a community focused on academic competitions. %\jvr{replace by vector graphics, so that we can zoom in better? Also: font and label size}
    } 
    \label{fig:nbr_compet_years}
\end{figure}

%\isabelle{it seems to slow down in 2021, but did you take into account the competitions organized on \url{https://codalab.lisn.fr/}? }
%\aleksandra{yes it is included}

As a consequence of this growth,  we can witness the permeation and influence that competitions have had in a number of fields.
%\isabelle{Which field? You did not say.}
This chapter aims to survey academic competitions and their impact in the last few years. The objective is to provide the reader with a snapshot of the rise and establishment of academic competitions, and to outline open questions that could be addressed with support of contests in the near future. We have focused on machine learning competitions with emphasis on academic challenges. Nevertheless, competitions from other related fields are also briefly reviewed. 
%% HJ: we will add it if we comit to keep it updated
% While we have tried to be as inclusive as possible, please note that it is possible that we have missed some relevant literature. For that reason we will keep updating this document and the associated repository with references at: \url{https://tinyurl/challenges-survey}.
% \isabelle{The link is broken. How will you update this document?}

The remainder of this chapter is organized as follows. Next section provides a brief historical review of competitions in the context of academia and their impact in different fields. %In Section~\ref{sec:successful_challenges} we analyse what makes challenges successful.
Then in Section~\ref{sec:perdiscipline:Chapter6} we review academic competitions in terms of the associated field. Finally in Section~\ref{sec:discussion7} we outline some thoughts and ideas on the future of academic competitions. 
%\adrien{Reference the figures in the text}
%\adrien{We can add plots/statistics from CodaLab's meta-analysis. Even if it is biased towards CodaLab it can be insightful. This will justify the sections too. (+ new study with leaderboards statistics)}


\section{A review of academic challenges: past and present}
\label{sec:pastpresent}
This section provides a survey on academic challenges in the context of machine learning and related fields. 

\subsection{Historical review}
\label{sec:historicalreview}

%\hugo{A brief historical review of challenges in machine learning and related fields.}

%\adrien{Check with chapter 1 to avoid too much redundancy}

While it is a daunting task to give a comprehensive timeline of the evolution of challenges in machine learning and related fields, this section aims at providing a generic overview. Perhaps the first memorable \emph{challenge} is the Longitude Act issued in 1714. It asked participants to develop a method to determine longitude up to a half degree accuracy (i.e., about 69 miles in distance if one is placed in the Meridian). After years of milestones and fierce competition, Thomas Harrison was acknowledged as the winner of this \emph{challenge}. The main incentive, in addition to scientific curiosity, was a monetary prize offered by the British crown that today would be equivalent to millions of pounds. 

This form of incentive has guided several other competitions organized by governments\footnote{\url{https://www.nasa.gov/solve/history-of-challenges}}, for example %consider for instance 
the DARPA (Defense Advanced Research Projects Agency) grand challenge\footnote{\url{https://www.darpa.mil/news-events/2014-03-13}} series that for years organized competitions for building an all-terrain autonomous vehicle. These type of challenges are still being organized nowadays, not only by governments but also by other institutions and even the private sector. 
Consider for instance the funded challenges organized by the National Institute of Standards and Technology\footnote{\url{https://www.nist.gov/}}  (NIST) and the latest editions of the X-Prize Challenge\footnote{\url{https://www.xprize.org/challenges}} and the  Longitude Prize\footnote{\url{https://longitudeprize.org/}}, both targeting critical  health problems via challenges in their most recent editions. %Please note that there have been also efforts aiming to have a positive societal impact. For instance the general challenge by the GoodAI\footnote{\url{https://www.general-ai-challenge.org/}} that run for several years and  
%\aleksandra{This same model of making progress via crowd sourcing has been adopted by academy for a while now. The first efforts in this direction arose in the 90s, it was in that decade that the first RoboCup, ICDAR (International Conference on Document Analysis and Recognition), KDD Cup (Knowledge Discovery and Data Mining Tools Competition) and TREC (Text Retrieval Conference) }
This same model of making progress via crowd sourcing has been adopted by academy for a while now. The first efforts in this direction arose in the 90s, it was in that decade that the first RoboCup, ICDAR (International Conference on Document Analysis and Recognition), KDD Cup (Knowledge Discovery and Data Mining Tools Competition) and TREC (Text Retrieval Conference) %and NIST OCR\footnote{Actually the NIST did not organize an official challenge but they distributed the MNIST dataset with detailed instructions on evaluation protocols and results, see~\citep{mnistreport97}.} 
competitions were organized. Such challenges are still being organized on a yearly basis, %year after year \jvr{or: organized on a yearly basis}, a
and they have helped to guide the progress in their respective fields. %Additionally, we can mention KDD Cup (Knowledge Discovery and Data Mining) \footnote{\url{https://kdd.org/kdd-cup\#\#}} and challenges organized in conjunction with ICDAR (International Conference on Document Analysis and Recognition) \footnote{\url{https://en.wikipedia.org/wiki/International\_Conference\_on\_Document\_Analysis\_and\_Recognition}}.
%\isabelle{Also "old": \url{https://kdd.org/kdd-cup\#} \url{https://en.wikipedia.org/wiki/International\_Conference_on_Document\_Analysis\_and_Recognition} the ICDAR conference holds competitions almost since inception I think.}
%The model adopted by government organizations for the solution of problems via crowd sourcing has been  adopted by the academic community as well. %Efforts before the decade of 1990 include: early NIST challenges on character recognition Despite these isolated efforts, 

RoboCup initially focused on the development of robotic systems able to eventually \emph{play} Soccer at human level~\citep{robocup1}. With currently more than 25 editions, %\jvr{with currently more than 25 editions,} 
RoboCup has evolved in the type of tasks addressed in the context of the challenge. For instance, the 2022 edition\footnote{\url{https://2022.robocup.org/}} comprises leagues on rescue robots, service robots,  soccer playing robots, industrial robots and even a junior league for kids, where each league has multiple tracks. RoboCup competition model has motivated progress on different sub fields within robotics, from hardware to robot control and multi agent communication among others, see~\citep{DBLP:journals/ki/Visser16a}  for a survey on the achievements of this first 20 editions of RoboCup. Together with the DARPA challenge, RoboCup has largely guided the progress of autonomous robotic agents that interact in physical environments.   

Organized by  NIST, TREC is another of the \emph{long-lived} evaluation forums that arose in the early 90s~\citep{trec93}. TREC initially focused on text retrieval tasks. Unlike RoboCup, where solutions were tested lively during the event, TREC asked participants to submit \emph{runs} of their retrieval systems in response to a series of queries. By that time this represented a great opportunity for participants to evaluate their solutions in large scale and realistic retrieval scenarios. This evaluation model actually is still popular among text-based evaluation forums (see e.g., SemEval\footnote{\url{https://semeval.github.io/}}). The TREC forum has evolved and now it focuses on a diversity of tasks around information retrieval (e.g., retrieval of clinical treatments based on patients' cases). Additionally, TREC gave rise to a number of efforts like CLEF (Conference and Labs of the Evaluation Forum), ImageCLEF and TRECVID. They split from TREC to deal with specific sub problems such as: question answering, image and video retrieval, respectively.   %\jvr{sentence is hard to process (due to the stacking of comma's)}
%\aleksandra{Additionally, TREC gave rise to a number of efforts like CLEF (Conference and Labs of the Evaluation Forum), ImageCLEF and TRECVID. They split from TREC to deal with specific sub problems such as: question answering, image and video retrieval, respectively.}

% \isabelle{The NeurIPS 2003 feature selection challenge \url{https://papers.nips.cc/paper/2004/hash/5e751896e527c862bf67251a474b3819-Abstract.html}. The KDD cups. The causality workbench \url{http://www.causality.inf.ethz.ch/} (2007). The foundation of Kaggle (2009). The foundation of ChaLearn (2011).}


%Also in the OCR field we can find the ICDAR challenge series, that has focused on document analysis and recognition. 
%\hugo{Still need to add more about ICDAR}
In terms of OCR, there were also efforts aiming to boost research in this open problem during the 90s~\citep{mnistreport97}. The first ICDAR conference took place in 1991, although well documented competitions started in the early 00s %\romain{00s or 90s?} 
(see, e.g.,~\citep{DBLP:conf/icdar/LucasPSTWY03}), it seems that competitions associated to digital document analysis were associated to ICDAR since the early 90s, see~\citep{DBLP:conf/icdar/MatsuiNYWY93}.  By that time,  NIST released a large dataset of handwritten digits \citep{Grother1995NISTSD} with detailed instructions on preprocessing, evaluation protocols and reference results. While this was not precisely an academic competition, this effort allowed reproducibility in times where the world was starting to benefit from information spread throughout the internet.  The impact of this effort has been such that, in addition to motivating breakthroughs in OCR, established the MNIST benchmark as a reference problem for supervised learning (see e.g., Yann Lecun's site\footnote{\url{http://yann.lecun.com/exdb/mnist/}} on results in a subset of this benchmark).  Please note MNIST is a \emph{biased} dataset, and other versions of it exist, including QMNIST \citep{qmnist-2019}, where the authors reconstructed the MNIST test set with 60,000 samples. 
%It is worth mentioning QMNIST \citep{qmnist-2019}, an attempt to de-bias the above mentioned MNIST dataset, where the authors reconstructed the MNIST test set with 60,000 samples. 


Another successful challenge series is the KDD Cup, with its first edition taking place in 1997\footnote{\url{https://www.kdd.org/kdd-cup/view/kdd-cup-1997}}. KDD Cup has focused on challenges on data mining bridging industry and academy, with a variety of topics being covered with time, from retailing, recommendation and customer analysis to authorship analysis and student performance evaluation\footnote{\url{https://kdd.org/kdd-cup}}. While KDD Cup %\jvr{please check consistency: with out without space} 
has been more application-oriented, findings from this competition have resulted in progress in the field without any doubt. KDD Cups are reviewed in the next chapter. 


% QMNIST \footnote{\url{https://github.com/facebookresearch/qmnist}}
% NIST not iid, MNIST (mixed data) shuffling to hide bias / make iid (each handwriting data is biased towards the population that was used to collect data), QMNIST - an attempt to debias
% Q about bias and iid, it depends on the objective of the challenge, the purpose, so it is ok to have biased data for certain cases


The first decade of the 2000 was critical for the consolidation of challenges as a way to solve tough problems with the help from the community. It was during this time that the popular Netflix prize\footnote{\url{https://www.netflixprize.com/}} was organized, granting a 1M dollar prize to the team able to improve the performance of their \emph{in-house} recommendation method. The winning team  improved by $\approx 10\%$ the reference model~\citep{bellkor}.  %\jvr{reference? baseline? existing state-of-the-art method?}.  
Also, one of the long-lived competition programs in the context  of machine learning  arose in this decade\footnote{\url{https://sorry.vse.cz/~berka/challenge/PAST/}}: the \emph{ECML/PKDD Discovery Challenge series}. Organized since 1999, this forum has released a number of datasets, although it is now an established competition track, in the early years, competitions consisted of releasing data and asking participants to build and evaluate solutions by themselves. The NeurIPS 2003 feature selection challenge took place\footnote{\url{http://clopinet.com/isabelle/Projects/NIPS2003/}} in this decade too, being this one of the oldest  machine learning competitions in which test data was withheld from participants~\citep{NIPS2004_5e751896}. 
 

In that same decade, the first edition of evaluation efforts that are still being run were launched, for instance, the first: CLEF\footnote{\url{https://www.clef-initiative.eu/web/clef-initiative/}} (2000),   ImageCLEF\footnote{{\url{https://www.imageclef.org/}}} forum (2003),  TRECVID\footnote{\url{https://trecvid.nist.gov/}} conference (2003),  PASCAL VOC\footnote{\url{http://host.robots.ox.ac.uk/pascal/VOC/}} (2005) challenges. All of these efforts and others that evolved over the years (e.g.,  the model selection\footnote{\url{http://clopinet.com/isabelle/Projects/NIPS2006/home.html}} and performance prediction\footnote{\url{http://www.modelselect.inf.ethz.ch/}} challenges (2006) that laid the foundation for AutoML challenges), set the basis for the settlement of academic competitions. % in the next decade. 

The 2000s not only were fruitful in terms of the number and variety of longlasting challenges that emerged, but also because of the establishment of organizations. It was in 2009 that Kaggle\footnote{\url{https://kaggle.com/}} was founded, initially focused on challenges as a service, nowadays Kaggle also offers learning, hiring and data-code sharing options. From the academic side, in 2011 ChaLearn\footnote{\url{http://chalearn.org/}}, the Challenges in Machine Learning Organization was founded as well. ChaLearn is a non-profit organization that focuses on the organization and dissemination of academic challenges. ChaLearn provides support to potential organizers of competitions and regularly collaborates with a number of institutions and research groups, likewise, it focuses on research associated to challenge organization in general, this book is a product of such efforts.   
%\hugo{Add achievements of chalearn, and closure on organizations}

From 2010 and on challenges have been established as one of the most effective way of boosting research in a specific problem to get practical solutions rapidly. 
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) featured from 2010 to 2017 has been among the most successful challenges in computer vision, as it witnessed the rise of CNNs for solving image classification tasks, see next Section. Likewise, the VOC challenge organized until 2012, contributed to the development of object detection techniques like Yolo~\citep{DBLP:conf/cvpr/RedmonDGF16}.  The AutoML challenge series (from 2015) proved that long term contests with code submission could lead to progress on the automation of model design at different levels. As a result, nowadays, top-conferences and venues from different fields have their competition track. Table~\ref{tab:confchallengetracks} shows representative competition programs  associated to major conferences and related organizations. 
%As previously mentioned, challenges are now ubiquitous in most academic conferences. Table~\ref{tab:confchallengetracks} shows representative challenge tracks associated to major conferences. We focused only on tracks that are organized regularly. 
\begin{table}[h]
\centering
\caption{Competition tracks of main conferences in machine learning and related fields. Column four shows the number of tasks organized in the latest edition of the associated track (\# Tasks LE) as of 2022. %\jvr{column heading abbreviation is still unclear}\jvr{is this number static across years? if not, please state when the snapshot is taken.}.
Acronyms are as follows: Machine Learning (ML), Data Mining (DM), Computational Intelligence (CI), Pattern Recognition (PR), Robotics (RO), MIR (Multimedia Information Retrieval), Multimedia Information Processing (MIP), Information Retrieval (IR), Natural Language Processing (NLP), Artificial Intelligence (AI), Evolutionary Computation (EC), Medical Image Analysis (MI), Signal Processing (SP), Image Processing (IP), Miscellaneous (MS). The last four rows of this table shows institutions and organizations associated with challenges.}
\label{tab:confchallengetracks}
\resizebox{11cm}{!}{  
\begin{tabular}{ccccc}
\hline
\textbf{Venue}&\textbf{Field}&\textbf{Since}&\textbf{\# Tasks LE}&\textbf{URL}\\
\hline
TREC&IR&1993&7&\url{https://trec.nist.gov/}\\
ICDAR&PR&1993&13&\url{https://icdar2023.org/}\\
KDD&DM&1997&2&\url{https://kdd.org/}\\
ECML&ML&1999&3&\url{https://ecmlpkdd.org/}\\
RoboCup&RO&1997&5&\url{https://www.robocup.org/}\\
PAN-CLEF$\dag$&NLP&2000&4&\url{https://pan.webis.de/}\\
TrecVid&MIR&2003&8&\url{https://trecvid.nist.gov/}\\
ImageCLEF$\dag$&MIP&2003&4&\url{https://www.imageclef.org}\\
MediaEval&MIP&2003&11&\url{https://multimediaeval.github.io/}\\
GECCO&EC&2004&10&\url{https://gecco-2022.sigevo.org/HomePage}\\
WCCI&CI&2006&13&\url{https://wcci2022.org/accepted-competitions/}\\
MICCAI&MI&2007&38&\url{https://conferences.miccai.org/2022/en/}\\
Interspeech&SP&2008&2&\url{https://interspeech2022.org/}\\
ICRA&RO&2008&10&\url{https://www.icra2022.org/}\\
ACM Multimedia&MIP&2009&10&\url{https://2022.acmmm.org/grand-challenges/}\\
ICPR&PR&2010&7&\url{https://www.icpr2022.com/}\\
SemEval&NLP&2010&12&\url{https://semeval.github.io/}\\
IROS&RO&2012&9&\url{https://iros2022.org/program/competition/}\\
ICMI&MIP&2013&1&\url{https://icmi.acm.org/2022/}\\
ICASSP&SP&2014&8&\url{https://2022.ieeeicassp.org/}\\
ICME&MIP&2015&2&\url{https://2022.ieeeicme.org/}\\
CIKM&DM&2017&2&\url{https://www.cikm2022.org}\\
ICIP&IP&2017&4&\url{https://2022.ieeeicip.org/}\\
NeurIPS&ML&2018&25&\url{https://neurips.cc/Conferences}\\
IJCAI&AI&2018&4&\url{https://www.ijcai.org/}\\
\hline
AutoML&ML&2022&1&\url{https://automl.cc/}\\
\hline

Loingitude Prize&MS&1714$^*$&1&\url{https://longitudeprize.org/}\\
XPrize$^*$&MS&1996&2&\url{https://www.xprize.org/}\\
\hline
Kaggle&MS&2009&-&\url{https://www.kaggle.com/}\\
ChaLearn&ML&2011&-&\url{http://chalearn.org/}\\
\hline
\end{tabular}
}
\end{table}

This table illustrates that many scientific communities have acknowledged the importance of academic competitions, and highly value these by dedicating resources towards organizing such competitions.
%This table illustrates the permeation that challenges have had in academic events, with an increasing number of forums starting in the previous decade.\jvr{While I would agree that this is an impressive list of venues, I would be a bit careful with stating `permeation'. rather, I would rephrase to: ``This table illustrates that many scientific communities have acknowledged the importance of academic competitions, and highly value these by dedicating resources towards organizing such competitions.''}\jvr{Also: the AutoML conference has a competition track (since '22)}
%We focused only on tracks that are organized regularly.  
Please note that there are top tier venues that do not have an \emph{official} competition track, and therefore they were not included in this table. However, these venues have hosted workshops associated to competitions that have had great impact. Just to name a few: CVPR, ICCV, ECCV, ICML, ICLR, EMNLP, ACL. 
%\hugo{Still to review table}

\subsection{Progress driven by academic challenges}
As previously mentioned challenges are now established mechanisms for dealing with complex problems in science and industry. This is not fortuitous, but a response from the community to a number of accomplishments in different fields. This section aims to briefly summarize the main achievements of selected challenges that have motivated other researchers and fields to organize competitions. We focused on a representative machine learning challenge (AutoML) and two evaluation campaigns from the two fields where more contests are organized, see Figure~\ref{fig:pie_chart}.

\begin{itemize}

    \item \textbf{AutoML challenges.} %Automated task solving is the main goal of machine learning systems. For such systems, it is expected that designers of machine these systems manually adjust parameters and take all of the decisions involved in the design process. In recent years, a new subfield within machine learning has emerged: Autonomous Machine Learing. 
    AutoML is the sub field of machine learning that  aims at automating as much as possible all of the aspects of the design cycle~\citep{automlbook}. While people were initially sceptical of the potential of this sort of methods, nowadays AutoML is a trending research topic within machine learning (there is a dedicated  AutoML conference with a competition track\footnote{\url{https://automl.cc/}} since 2022). This is in large part due to the achievements obtained in the context of AutoML challenges. Back in 2006 early efforts in this direction were the prediction performance challenge~\citep{DBLP:conf/ijcnn/GuyonADB06} and the agnostic {\em vs.} prior knowledge challenge~\citep{DBLP:journals/nn/GuyonSDC08}. These contests asked participants to build methods for automatically or manually building classification models. They became the predecessors of the AutoML challenge series that ran from 2015 to 2018~\citep{DBLP:books/sp/19/GuyonSBEELJRSSSTV19},
    and all of the follow up events that are still organized. Initially, the AutoML challenge series focused on tabular data, but it then evolved to deal with raw heterogeneous data in the AutoDL\footnote{ \url{https://autodl.chalearn.org/}} challenge series\citep{ChaLearnAutoDL2019}, whose  latest edition is the  Cross-Domain MetaDL challenge 2022~\footnote{\url{https://metalearning.chalearn.org/}}~\citep{elbaz2021metadl, elbaz:hal-03688638, https://doi.org/10.48550/arxiv.2208.14686}.%\jvr{I added the citation}  
    A number of methods (e.g., AutoSKLearn~\citep{Feurer2019}), evaluation protocols, AutoML mechanisms (e.g., Fast Augmentation Learning methods~\citep{baek2020autoclint}) and improvements arose in the context of these challenges including the evaluation of submitted  code, cheating prevention mechanisms, the progressive automation of different types of tasks (e.g., from binary classification to regression, to multiclass classification, to neural architecture search) and the use of different data sources  (from tabular data, to raw images, to raw heterogeneous datasets). The result is an established benchmark that is widely used by the community. 
 %\isabelle{Also mention the AutoDL series \url{https://autodl.chalearn.org/} \url{https://ieeexplore.ieee.org/document/9415128} \citep{ChaLearnAutoDL2019}}    
    
    \item \textbf{ImageNet Large Scale Visual Recognition Challenge.} The so called, ImageNet challenge asked participants to develop image classification systems for 1,000 categories and using millions of images as training data~\citep{Russakovsky2015}. At the time of the first edition of the challenge, object recognition, image retrieval and classification datasets were dealing with problems involving thousands of images and dozens of categories (see e.g.,~\citep{DBLP:journals/cviu/EscalanteHGLMMSPG10}). While the scale made participants struggle in the first two editions of the challenge, the third round witnessed the renaissance of convolutional neural networks, when AlexNet reduced drastically the error rate for this dataset~\citep{DBLP:conf/nips/KrizhevskySH12}. In the following editions of the challenge other landmark CNN-based architectures for image classification were proposed including: VGG~\citep{DBLP:journals/corr/SimonyanZ14a}, GoogLeNet~\citep{DBLP:conf/cvpr/SzegedyLJSRAEVR15} and ResNet~\citep{DBLP:journals/corr/HeZRS15}. These architectures comprised important contributions to deep learning, including residual connections/blocks and inception-based networks, the establishment of regularization mechanisms like dropout, pretraining and fine tuning and the efficient usage of GPUs for training large models. While the challenge itself did not provoke %\jvr{provoke?}\aleksandra{result in} 
    the aforementioned contributions, it was the catalyst and solid %\jvr{I would rather say: solid test bed (perfect is very absolute)} 
    test bed for the rise of deep learning in computer vision. 
    
    \item \textbf{Text Retrieval Evaluation Conference.} %Perhaps the first evaluation effort in this field was that by the NIST with the Text REtrieval Conference (TREC) series~\citep{trec93}.
    TREC initially focused on the evaluation of information retrieval systems (text) (see~\citep{DBLP:journals/ipm/Muresan07,OVER2001369} for an overview of the early editions of TREC), but it rapidly evolved to include novel tasks and evaluation scenarios in the forthcoming years. This led %\romain{This led?} 
    to include? tasks that involved information sources from multiple languages, and eventually images and videos. Other tasks that have been widely considered in the TREC campaign are: question answering, adaptive filtering, text summarization, indexing, among many others. Thanks to this effort the information retrieval and text mining fields were consolidated and boosted the progress in the development of search engines and related tools that are quite common nowadays. Well known retrieval models and related mechanisms for efficient indexing, query expansion, relevance feedback, arose in the context of TREC or were validated in this forum. %For instance, classic retrieval models like Okapi, SMART, Latent semantic indexing,  
    Another  important contribution of TREC through the years is that it has evolved to give rise to numerous tasks and application scenarios that have defined the text mining field. 
    %\item \textbf{DARPA Grand challenge.}
    
\end{itemize}

We surveyed a few representative challenges and outlined the main benefits that they bring into their respective communities. While these are very specific examples and while we have chosen breaking through competitions, similar outcomes can be drawn from challenges organized in other fields. In Section~\ref{sec:perdiscipline:Chapter6} we review challenges from a wider variety of domains. % and outline the main contributions for each of them.  
%Review of notable achievements driven by academic competitions
%Landmark challenges in machine learning and computer vision comprise: the Netflix prize\footnote{\url{https://www.netflixprize.com/}} (the winning team  improved by $\approx 10\%$ the reference~\citep{bellkor}), the VOC challenge series~\citep{Everingham15} (that largely boosted research in object recognition/detection, see e.g.,YOLO~\citep{DBLP:journals/corr/abs-1804-02767}) and the ImageNet challenge~\citep{Russakovsky2015} (in which context CNNs consolidated as the \emph{de facto} solution for image classification). Great progress has been motivated by challenges so far, and we foresee novel methodologies from the community  will keep emerging in their context. %Whereas this progress is always driven by the community, challenges have played a key role %\textcolor{red}{Somehow close this.}

\subsection{Pros and cons of academic challenges}
We have learned so far that challenges are beneficial in a number of ways, and have boosted progress in a variety of domains. However, it is true that there are some limitations and undesired effects of challenges that deserve to be pointed out. This section briefly elaborates on benefits and limitations of academic challenges. 
%What we have learned so far from academic competitions

\subsubsection{Benefits of academic challenges}
As previously mentioned, the main benefit of challenges is the solution of complex problems via crowd sourcing, advancing the state of the art and the establishment of benchmarks. There are, however, other benefits that make them appealing to both participants and organizers, these include:   
%Among the features that make challenges appealing are:
\begin{itemize}
    \item \textbf{Training and learning through challenges.} Competitions are an effective way to learn new skills, they \emph{challenge} participants to gain new knowledge and put in practice known concepts for solving relevant problems in research and industry. Even if participants do not win a challenge or a series of them, they progressively improve their problem solving skills.  
    \item \textbf{Challenges are open to anyone.} Apart of political restrictions that may be applied for some organizations, competitions target anyone with the ability to approach the posted problem. This is particularly appealing to underrepresented groups and people with limitations to access the cutting edge problems, data and resources. For instance, most competitions adopting code submission provide cloud-based computing to participants. %This feature allows anyone in the world to contribute 
    Likewise, challenges can be turned into ever lasting benchmarks and they contribute to making data available to the public. 
    %\isabelle{Stress that they can be turned into an ever lasting benchmark and/or they contribute to making data available to the public.}
    \item \textbf{Engagement and motivation.} The engagement offered by competitions is priceless. Whether the reward is economic, academic (e.g., publication or talk in a workshop, professional recognition in the field), competitiveness, or just fun, participants find challenges motivating. %\isabelle{Also professional recognition.}  
    %Competitions keep participants and organizers interested in Keep you excited and is a way to keep you on track of research. challenges are engaging (making participants to be excited about learning and competing);
    %\item Rewards. challenges offer rewards, among the most edifying are dissemination opportunities  (participants commonly present their solutions in top academic events, and  publish their work in proceedings, special issues in journals or even books); but perhaps more importantly:
    \item \textbf{Reproducibility.} This cannot be emphasized enough,  benchmarks associated to challenges not only provide the task, data and evaluation protocols. In most cases resources, starting-kits, others' participants code and computing resources are given as well. This represents an easy way to get into competitions to participants, which can directly compete with state-of-the-art solutions. At the same time, competitions having these features %these features 
    guarantee reproducibility of results which is clearly beneficial to the progress in the field. %\jvr{challenges are not always reproducible. For example in the case of the OASC~\citep{Lindauer2019}, participants ran the software locally, and uploaded predictions. In this case, the results were (unfortunately) only reproducible if the participants decided to open source their code. }
\end{itemize} 

\subsubsection{Pitfalls of academic challenges}
Despite the benefits of challenges, they are not risk-free, therefore, there are certain limitations that should be taken into account. %\jvr{Based on what is written below, I would rephrase this section as ``pitfalls''. Most of these can be avoided in a well-designed challenge, but require careful consideration.}
\begin{itemize}
    \item \textbf{Performance improvement vs. scientific contribution.} Academic challenges often ask participants to build solutions that achieve the best performance according to a given metric. Although in most cases there is a research question associated to a challenge, participants may end up building solutions that optimize the metric but that do not necessarily result in new knowledge. This gives challenges a bitter-sweet taste, as often new findings are overshadow by super-tuned off-the-shell solutions. 
    %\item \textbf{Negligible improvements.} At times, wrong challenge design choices, lead to organize challenges that are too difficult or too easy to approach. 
    \item \textbf{Stagnation.} An undesirable outcome for a challenge is stagnation, this is often the result of wrong challenge design decisions, that result in either a problem that is too hard to be solved with current technology or unattractive to participants. While it is not possible to anticipate how far the community can go in solving a task, the implementation of (strong) baselines, starting kits and appealing datasets, or rewards could help to avoid stagnation.    
    \item \textbf{Data Leakage.} It refers to the use of target (or any other relevant information that is supposed to be withheld from participants) information by participants to build their solutions~\citep{LeakageSIGKDD}. This is a common issue when datasets are re-used or when datasets are build from external information (e.g., from social networks). Anonymization and other mechanisms as those exposed in~\citep{LeakageSIGKDD} could be adopted for avoiding this problem.    
%This This is an important problem that has been present in a wide number of competitions, see~\citep{LeakageSIGKDD}. 
    \item \textbf{Privacy and rights on data.} \emph{''Data is the new oil"} has been a popular say recently\footnote{\url{https://www.forbes.com/sites/forbestechcouncil/2019/11/15/data-is-the-new-oil-and-thats-a-good-thing/?sh=381ec30d7304}}, while this is debatable, it is true that data is a valuable asset that must be \emph{handled with care}. Therefore copyright infringement should be avoided to the uttermost end. Likewise, failing to guarantee  privacy is an important issue that must be addressed by organizers as this could lead into legal issues. Anonymization mechanism should be applied to data before its release, making sure it is not possible to track users identity or other important and confidential information.   %Likewise, copyright infringement should be avoided   %\item \textbf{Legal issues like obtaining rights and privacy (anonymization).}
    
\end{itemize}

%\isabelle{Maybe also discuss what makes a good or a bad academic challenge?}
\subsection{What makes academic challenges successful?}
\label{sec:successful_challenges}
Having reviewed competitions, their benefits and pitfalls/limitations, this section elaborates on characteristics that we think make a challenge successful. While it is subjective to define a successful challenge, the following guidelines %\jvr{guidelines? practises? standards?} 
associate success to high participation, quantitative performance and novelty of top ranked solutions.
\begin{itemize}
    \item \textbf{Scientific rigour.} The design and the analysis of the outcomes of a competition are critical for its success. Following  scientific rigor as ``to ensure robust and unbiased experimental design, methodology, analysis, interpretation and reporting of results``~\citep{10.1093/carcin/bgx085} is necessary and helps to avoid some of the  limitations mentioned above. Adopting statistical testing for the analysis of results, careful designing of evaluation metrics, establishing theoretical bounds on these,  running multiple tests before releasing the data/competition,  formalizing the problem formulation, performing ablation studies are all critical actions that impact on the outcomes of academic challenges. 
    
    \item \textbf{Rewarding and praising scientific merit and novelty of solutions.} It is worth mentioning that novel methods do not always make it to the top of the leaderboard, but these new ideas may be great seeds and serve as an inspiration to others for further fruitful research. Therefore, rewarding and acknowledging scientific merit and novelty of solutions is very important. There are several ways of doing this, for instance, having a \emph{prize} for the most original/novel submission or granting a best paper award that is not entirely based on quantitative performance.


    \item \textbf{Publication and dissemination of results} are  good practices with multiple benefits. Participants are often invited to fill out  \emph{fact sheets} and write workshop papers in order to document their solutions. Similarly, organizers  commonly publish overview papers that summarize the competition, highlighting the main findings and analyzing results in detail.  Associating a special issue of a journal %\jvr{consider adding: of a journal?} 
    with competitions is a good idea as it is motivating for participants, and at the same time it is a \emph{product} that organizers can report in their work evaluations. 
    
    \item \textbf{Associating the competition with an top tier venue} (e.g., conferences, summits, workshops, etc.) makes a challenge more attractive to participants,  as they associate the quality of associated venues and competitions. Also, physically attending the competition session is more appealing if participants can also attend top tier events. 
    
    \item \textbf{Organization of panels and informal discussion sessions} involving both participants and organizers is valuable for sensing perception of people associated to the event. This is critical when organizing challenges that run for several editions. 
    
    \item \textbf{Establishing benchmarks} should be an underlying goal of every competition. Therefore, curated data, fail safe evaluation protocols, and adequate platforms for maintaining competitions as long term evaluation test beds are essential. Likewise,  the use of open data and open source code for the purposes of  reproducibility and so that everyone can benefit and continue their own research.
    

\end{itemize}
%Since late 80s DARPA organised a series of speech and natural language challenges, but outcomes were not impressive: participants kept on implementing insignificant improvements on the basis of hidden Markov model without proposing novel solutions \citep{ wayne-1991-snapshot}, \citep{ hirschman1998evaluating}. Can one consider these contests a success?  
%\aleksandra{find other sources }

% \subsubsection{Incremental improvements}
% If a recurrent competition keeps generating only small incremental improvements without introducing a novel solution, can one consider such contest as a success?

% In order to answer this question, let’s define first what the academic challenge is. It is a contest which aim is to answer a scientific question by proposing innovative solutions, ideally to push the state-of-the-art and have a long-lasting impact and/or benchmark. It is worth mentioning that novel methods do not always make it to the top of the leaderboard, %for example because the contributor did not have enough time during the contest to polish it, or due to the fact that this novel method is not polished enough,
% but these new ideas may be great seeds and serve as an inspiration to others for further fruitful research.
% But of course, that’s not all to it, below we will discuss main characteristics of academic and industrial challenges% and give some examples of contests which did not achieve their goals.% to analyse the failures
% .
% %\aleksandra{modify: definition should be mentioned in the introduction + add reference here instead?}

% First of all, as described above the objective of any academic challenge is to answer a precise scientific question, to push the limits of this domain and propose a novel solution. In order to share the outcomes of the research, participants are invited to fill out the fact sheets which come as a questionnaire that can help organisers to gather statistics, make a clear overview and summary of the contest. Winning teams are often expected or encouraged to write papers describing their methods following the scientific rigor as ``to ensure robust and unbiased experimental design, methodology, analysis, interpretation and reporting of results`` \citep{10.1093/carcin/bgx085}.  

% \aleksandra{Tests of statistical significance can be conducted to prove that the result is less likely to be due to a chance. - not sure, is it correct?}
% \aleksandra{include a paragraph on ablation studies as a mean to analyse and ensure scientific rigour?}

% \subsubsection{Qualitative evaluation}
% Another mark of a successful challenge can be the best paper award, specifically if it is granted within a renown conference or a workshop, which is a good indicator of its acceptance within the scientific community.

% \subsubsection{Quantitative evaluation}
% As for the quantitative metrics of success of the challenges, we can name the number of publications as in theory it should be proportional to the number of novel methods proposed. But can we say the same about the number of participants: can this number be used as a metric of success? To some extent yes, assuming the more participants the challenge attract, the more probability to generate fruitful results. But as the authors of the article ``Filtering participants improves generalization in competitions and benchmarks`` \footnote{ Paper accepted at ESANN 2022
% \url{https://www.esann.org/program}} show that pre-filtering participants can reduce overfitting and eliminate noise in the final leaderboard.

% As the main objective of academic challenges is the quantitative algorithms, organizers try to de-emphasize engineering and in order to do so they often provide reprocessed data so that the participants concentrate on the development of the methods.

% \subsubsection{Reproducibility}
% Another important aspect is the use of open data and open source for the purposes of reproductibility and so that everyone can benefit and continue their own research.

% \aleksandra{What counts as well is the degree of generality and the capability of the solution to generalize to different settings and domains. + illustration on multiple dataset}

% \aleksandra{ mitigate both variance in data and bias in data + debias}

% \aleksandra{design API or interface to decouple various aspects \url{https://arxiv.org/pdf/2201.03801.pdf}
% }

% \aleksandra{AutoDL paper \citep{ChaLearnAutoDL2019}}

% So we can see that qualitative contributions are the core of the academic challenges, which strive to de-emphasize engineering and winning, and make people focus on the main scientific questions and conceptual advancements.


\subsubsection{Academic vs. industrial challenges}
%\jvr{This is a very critical paragraph towards industrial challenges (and I agree to most of it). However, it would be good to back up critical statements with citations or other references. }
Industrial challenges are described in detail in the next chapter. In this section we  outline the main  differences of industry and scientific competitions. 

The main objective of industrial challenges is the economic advantage from the winning model that will potentially increase profits and improve business model, meaning it should be an end-to-end solution. %and not just an answer to a precise scientific problem.

The organizers care much less about scientific publications, being scientifically rigorous neither about the results being statistically significant. % \jvr{please consider rewriting the sentence to flow better}. 
These types of contest do not single out scientific questions, that is not the priority for them. They aim at specific business problems, usually posses big not preprocessed datasets and evidently can provide more often big prizes. % question about the correlation between prize and the quality of the outcome + cheating due to prises
Up till now the direct positive correlation between these big rewards and qualitative contributions has not been proven. But it was observed that big prizes might attract many participants, create "big splash" in the news for the company-organizer and cause a lot of noise in the leaderboard, potentially leading to gaining by chance.%, as it has been mentioned above.
While the winners and contributors of academic challenges get scientific recognition, the top performers at the industrial contests can receive job offers and be hired by the organizers. 

Another important aspect of industrial challenges is that due to their nature and the concurrent market, the company-organizers prefer to keep the data and the submitted code private, which is in the opposition with the scientific mentality, because it prevents to benefit from the latest break-through and get inspiration from the newest ideas.

% counter-examples
%challenge that didnt take place
%data leakage (kdd) \citep{LeakageSIGKDD}
% pb with obtaining rights (legal issue)
% pb with shuffling (bias)


%\subsection{Discussion}

\section{Academic challenges across different fields}
\label{sec:perdiscipline:Chapter6}
This section briefly reviews challenges across different fields. We focus on fields that have long tradition in challenges. In order to identify such fields of knowledge, we surveyed competitions organized in the CodaLab platform~\citep{codalab_competitions_JMLR}. Figure~\ref{fig:pie_chart} shows a distribution of CodaLab challenges across fields of knowledge. Clearly NLP and Computer vision challenges dominate, this could be due to the explosion of availability of visual and textual data of the last few years. One should note that most of the competitions shown in that plot have a strong machine learning component. In the remainder of this section we briefly survey competitions organized in a subset of selected fields. 
%\isabelle{The Xprize? The various conference challenge tracks, with pier reviews of challenges? WCCI, NeurIPS, KDD, IDCAR, others? Maybe we should make a little table to encourage organizers to submit there. }

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{Figures/ML_domains_piechart2.png}
    \caption{Distribution of competitions with different machine learning domains. Data gathered from CodaLab Competitions~\citep{codalab_competitions_JMLR}}
    \label{fig:pie_chart}
\end{figure}

%\aleksandra{This pie chart includes data up to 2021 including, but not 2022 as it is not yet over and to be in coherence with the previous graph}

\subsection{Challenges in Machine Learning}

Machine learning is a transversal field of knowledge that has been present in most challenges regardless of the application field (e.g., computer vision, OCR, NLP, time series analysis, and so on). Therefore, it is not easy to cast a challenge as a ML competition. For that reason, in this section we review as a representative sample the competition track of the NeurIPS conference. The track has run regularly since 2017, although challenges organized with the conference date back to the early 2000s~\citep{NIPS2004_5e751896}. Overview papers for the NeurIPS competition track from 2019 to 2021 can be found in~\citep{DBLP:conf/nips/EscalanteH19,DBLP:conf/nips/EscalanteH20,pmlr-v176-kiela22a}. 

Figure~\ref{fig:neuripsc} shows the number of competitions that have been part of the  NeurIPS competition track. There has been an increasing number of competitions organized each year, see also~\citep{carlens2023state} for more details. %, this increase is proportional to the number of received submissions, see~\citep{DBLP:conf/nips/EscalanteH19,DBLP:conf/nips/EscalanteH20}.  
The topics of challenges are quite diverse,  with deep reinforcement learning (DRL)  prevailing since the very beginning of the track. The first competition in the program around this topic was the Learning to Run challenge\footnote{\url{https://www.aicrowd.com/challenges/nips-2017-learning-to-run}} that asked participants to build an human-like agent to navigate an environment with obstacles~\citep{DBLP:journals/corr/abs-1804-00361}, this challenge was run for two more editions, the last one being the Learn to Move - Walk Around\footnote{\url{https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around}} challenge. DRL-based competitions addressing other challenging navigation scenarios are the Animal Olympics\footnote{http://animalaiolympics.com/AAI/} and MineRL series, see below. DRL challenges addressing different tasks are %the reconnaissance blind chess, 
the Real robot challenge\footnote{\url{https://real-robot-challenge.com/}} series with two editions, the Learning to run a power network competition\footnote{\url{https://l2rpn.chalearn.org/}} and the two editions of the Pommerman\footnote{\url{https://www.pommerman.com/}} competition where the goal was to develop agents to compete to each other in a bomberman-game-like scenario.  The presence of DRL in the challenge track as been growing in the last editions. 
%Tasks, evaluation scenarios, and amount of available resources/information in DRL-based challenges comprise a wide spectrum. Consider for instance the    
%During the first editions, topics were most varied with challenges on classification~\footnote{\url{https://www.kaggle.com/c/msk-redefining-cancer-treatment}},    conversational/QA systems~\citep{DBLP:journals/corr/abs-2009-11352,DBLP:journals/corr/abs-1803-08652}, deep reinforcement learning~\footnote{\url{https://www.aicrowd.com/challenges/nips-2017-learning-to-run}}, robotics\footnote{\url{https://www.duckietown.org/}} and application-specific, see e.g., the TrackML challenge~\citep{DBLP:journals/corr/abs-2105-01160} . 
\begin{figure}[h!tb]
    \centering
    \includegraphics[scale=0.4]{Figures/neuripscompetitions.png}
    \caption{Number of challenges organized as  part of the NeurIPS competition program.
    %\jvr{vector graphics?}
    } 
    %\harald{I collected some estimates on  on total prize money each year for NeurIPS competitions; you're welcome to republish them here if you cite the source: https://mlcontests.com/state-of-competitive-machine-learning-2022/\#neurips-competitions - see footnote for methodology: https://mlcontests.com/state-of-competitive-machine-learning-2022/\#fn:12}
    \label{fig:neuripsc}
\end{figure}


Another popular topic in the NeurIPS competition track is AutoML: since 2018, at least one competition associated to this topic has been part of  the NeurIPS competition track. These include the AutoML@NeurIPS~\citep{DBLP:journals/corr/abs-1903-05263}  and AutoDL~\citep{ChaLearnAutoDL2019} challenges, the black-box optimization competition~\citep{pmlr-v133-turner21a}, the predicting generalization in deep learning challenge~\footnote{\url{https://sites.google.com/view/pgdl2020}}, two editions of the Meta-DL challenge~\citep{elbaz:hal-03688638,https://doi.org/10.48550/arxiv.2208.14686} and the AutoML Decathlon\footnote{\url{https://www.cs.cmu.edu/~automl-decathlon-22/}}. %\jvr{maybe good to note which challenges have led to benchmarks (like in the next section), e.g., the MetaDL series has led to the MetaAlbum benchmark~\citep{ullah2022meta}}

Specific  challenges that have been part of the competition track for more than 2 editions are the following: 
\begin{itemize}
    \item \textbf{Traffic4cast}\footnote{\url{https://www.iarai.ac.at/traffic4cast/}}. Organizing variants of challenges aiming to predict traffic conditions under different settings and scenarios, see~\citep{pmlr-v123-kreil20a,pmlr-v133-kopp21a,pmlr-v176-eichenberger22a}. 
    \item \textbf{The AI Driving Olympics (AI-DO).} Aiming to build autonomous driving systems running in simulation and small physical vehicles tested live during the competition track\footnote{\url{https://www.duckietown.org/research/AI-Driving-olympics}}.
    %\item \textbf{Robot open-Ended Autonomous Learning (REAL)}. Dealing with 
    \item \textbf{MineRL\footnote{\url{https://minerl.io/}}} A competition series focusing on building autonomous agents that using minimal resources are able to solve very complex tasks in a MineCraft environment. In the first two editions agents were asked to find a diamond  with limited resources, see~\citep{pmlr-v123-milani20a,pmlr-v133-guss21a}. In the most recent editions tasks have been varied and more specific~\citep{https://doi.org/10.48550/arxiv.2204.07123}.  
    \item \textbf{Reconnaissance Blind Chess.} Challenges participants to build agents able to play a chess variant in which a player cannot see her
opponent’s pieces but can learn about them through private, explicit sensing actions. Three editions of this competition have run in the track~\citep{pmlr-v123-gardner20a}. 
\end{itemize}

It is difficult to summarize the number and variety of topics addressed in challenges part of the NeurIPS competition, however, we have reviewed a representative sample. Nevertheless, please note that most challenges reviewed in the remainder of this section also  include an ML component. 
%Despite there are a number of dedicated tracks to ML (see Table~\ref{tab:confchallengetracks}) the NeurIPS track is representative of what is happening in the field. 
%\hugo{Still working in this section}
%\isabelle{Fish in NeurIPS challenges!}
%AutoML / AutoDL
%Feature selection challenge \citep{NIPS2004_5e751896}

\subsection{Challenges in Computer Vision}
Together with machine learning, computer vision has been greatly benefited from challenges. As previously mentioned, The PASCAL Object detection challenge series boosted research on object detection and semantic segmentation~\citep{Everingham15}. The ImageNet large scale classification challenge is another landmark competition that served as platform for the renaissance of convolutional neural networks~\citep{Russakovsky2015}. In addition to these landmark competitions there have been a number of  efforts that have pushed further the state-of-the-art, these are reviewed in the following lines. 


The ChaLearn Looking at People (ChaLearn LAP\footnote{\url{https://chalearnlap.cvc.uab.cat/}}) series has organized academic challenges around the  analysis of human behavior from visual information. More than 20 competitions on the topic have been organized so far, see~\citep{DBLP:conf/ijcnn/EscaleraBEG17} for a (outdated) review.  Among the organized competitions several of the datasets have become a reference for different tasks, and are used as benchmarks. These include: the gesture recognition challenges~\citep{DBLP:conf/icmi/EscaleraGBRLGAE13,DBLP:conf/eccv/EscaleraBGBMRPE14, DBLP:books/sp/EGA2017,DBLP:conf/iccvw/WanEAEBGMAGLX17}, the personality recognition challenge series~\citep{DBLP:conf/ijcnn/EscalanteGEJMBA17,DBLP:journals/taffco/EscalanteKSEGGB22,DBLP:conf/iccv/PalmeroBJCNCSSZ21}, the age estimation challenge series~\citep{DBLP:conf/iccvw/EscaleraFPBGEMS15,DBLP:conf/cvpr/EscaleraTMBEGTC16} and the face anti-spoofing challenge series~\citep{DBLP:conf/cvpr/Liu0EETYWLGGL19,DBLP:series/synthesis/2020Wan,DBLP:journals/iet-bmt/LiuL0LEEM0WYTYY21}. A wide diversity of related topics have been studied in the context of ChaLearn LAP challenges, including: action recognition and cultural event recognition~\citep{DBLP:conf/cvpr/BaroGFBOEGE15,DBLP:conf/iccvw/EscaleraFPBGEMS15}, sign language understanding~\citep{DBLP:conf/cvpr/SincanJEK21}, identity preserving human analysis~\citep{DBLP:conf/fgr/ClapesJME20} among others. Undoubtedly, these challenges have advanced the state of the art in a number of directions within computer vision and affective computing. 

The Common Objects in COntext  (COCO\footnote{\url{https://cocodataset.org}})  challenge series that emerged after the end of the Pascal VOC challenge. This effort continued benchmarking object detection methods, but also started evaluating the so called \emph{image captioning} task. Early efforts for the evaluation of this task emerged in the ImageCLEF forum~\citep{DBLP:books/daglib/p/CloughMS10,DBLP:journals/cviu/EscalanteHGLMMSPG10}, where the goal was associating keywords to images. The COCO challenge was more ambitious by asking participants to describe the content of an image with a more \emph{human-like} description. Running from 2015-2020 this benchmark was critical for the consolidation of the image captioning task, with major contributions being reported at the beginning of the series, see~\citep{DBLP:journals/ijon/BaiA18,DBLP:journals/corr/abs-2107-06912}. Today, COCO is an established benchmark in a number of tasks related to vision and language, see~\citep{DBLP:conf/eccv/LinMBHPRDZ14}. 

Other  efforts in the field of computer vision are the NTIRE challenge, focused on image restoration, super resolution and enhancement~\citep{DBLP:conf/cvpr/TimofteAG0ZLSKN17,ntire22} , the visual question answering competition\footnote{\url{https://visualqa.org/}} running from 2016 to 2021, the fine grained classification workshop~\footnote{\url{https://sites.google.com/view/fgvc9}} that has  run a competition program since 2017, the EmotioNet\footnote{\url{https://cbcsl.ece.ohio-state.edu/enc-2020/index.html}} recognition challenge that ran in 2020 and is now a testbed for emotion recognition, the ActivityNet  challenge~\footnote{\url{http://activity-net.org/challenges/2022/}} organized since 2016 and  targeting action recognition  in video, among several others. 
%In terms of visual captioning and object detection, the COCO challenge series\footnote{\url{https://cocodataset.org}} has boosted research tremendously. In 2015 the first  COCO challenge was launched, in that same year a number of established methodologies were proposed, most of strong research groups in computer vision targeted this task, these results latter motivated research on visual question answering. 

%\isabelle{LAP challenges \url{https://chalearnlap.cvc.uab.cat/}}

\subsection{Challenges in Natural Language Processing}
The development of the natural language processing (NLP) field, in particular for text mining and related tasks, has been largely driven by competitions, also known in the NLP jargon as \emph{shared tasks}. In fact, one of the oldest evaluation forums across all computer science is one focusing in NLP, that is TREC.  %As previously mentioned, %Perhaps the first evaluation effort in this field was that by the NIST with the Text REtrieval Conference (TREC) series~\citep{trec93}. 
%TREC is one of the oldest competition forums in all computer science. 
It initially focused on the evaluation of information retrieval systems (text), but it rapidly evolved to include novel tasks and evaluation scenarios in the forthcoming years~\citep{DBLP:conf/tipster/VoorheesH98,DBLP:journals/ipm/Muresan07,OVER2001369}. This lead to consider tasks that involved information sources from multiple languages~\citep{harman:trec}, and eventually, speech signals~\citep{10.5555/2835865.2835867}  and visual information~\citep{DBLP:journals/corr/abs-2104-13473}. Other tasks that have been  considered in the TREC campaign are: question answering~\citep{DBLP:journals/nle/Voorhees01}, adaptive filtering~\citep{DBLP:conf/trec/Harman95}, text summarization~\footnote{\url{http://trecrts.github.io/}}, among many others. Thanks to this effort the information retrieval and text mining fields were consolidated and boosted the progress in the development of search engines and related tools that are quite common nowadays. 

Several well known evaluation campaigns evolved from TREC and consolidated on their own. Most notably, the TRECVid~\citep{DBLP:journals/corr/abs-2104-13473} and Cross-Language Evaluation Forum~\citep{10.1007/3-540-44645-1_9} (CLEF) campaigns. The former focusing on tasks related to video retrieval, indexing and analysis. The academic and economic impact of TRECVid has been summarized already. Showing the relevance that such forum has had into the progress of video search technology. CLEF is another forum that initially focused on cross-lingual text analysis tasks. Now it is a conference that comprises several shared tasks, called labs. This include ImageCLEF, PAN among others. 
%ImageCLEF is another evaluation forum that has focused on approaching problems that involve visual and textual modalities. Most notably, image retrieval and classification. ImageCL further giving rise to other evaluation forums like ImageCLEF, and the PAN series. 
%Without any doubt the previous evaluation forums boosted progress in NLP and related tasks, and set the basis for the wide variety of shared taks that are nowadays part of the top conferences in the field. Just to mention a few, SemEval workshop series,  
Likewise, there are forums dedicated to specific languages, for example, Evalita\footnote{\url{https://www.evalita.it/campaigns/evalita-2022/}} (for Italian), IberLEF\footnote{\url{https://sites.google.com/view/iberlef2022}} (for Spanish) and GermEval\footnote{\url{https://germeval.github.io/}}. 

In terms of speech, there were several efforts from DARPA~\citep{marcus-1992-overview,black-eskenazi-2009-spoken} and NIST\footnote{\url{https://www.nist.gov/itl/iad/mig/past-hlt-evaluation-projects}} in organizing competitions as early as the late 80s. These long term efforts have helped to shape ASR and related fields. %  to  Although it was a \emph{lengthy}However, such challenges suffered from stagnation, mostly in terms of \emph{novelty} as most solutions were invariable based on HMM modeling. 
More recently, after the deep learning empowering, several challenges focusing on speech have been proposed, these are often associated to major conferences in the field (e.g. Interspeech and ICASSP), see Table~\ref{tab:confchallengetracks}. %For instance the competition track at Interspeech 
There is no doubt that competitions have played a key role for the shaping the wide field of NLP.



\subsection{Challenges in Biology}

Biology is a field of knowledge that has benefited from competitions considerably. In terms of medical imaging, the premier forum is the  grand challenge series associated to the MICCAI conference, running since 2007~\footnote{\url{https://cause07.grand-challenge.org/Results/}}. A number of important challenges have been organized in this context, where most competitions deal with  medical imagery segmentation or reconstruction of different organs, body parts and input type, see e.g.,~\citep{scully,Marak_Cousty_Najman_Talbot2009,10.1007/978-3-030-98253-9_1}. In recent editions the challenge scenarios and approached tasks have been increasing difficulty  and the potential impact of solutions. %Consider for instance the % and the most recent ones dealing with, with a variety of other topics covered in other editions. 
In its last edition, the MICCAI grand challenge series has 38 competitions running in parallel. This is an indicator of success among the medical imaging community.  


Other challenges associated to medical image analysis have been presented in forums associated to image processing and computer vision as well. For instance, in 2019 during The IEEE International Symposium on Biomedical Imaging (ISBI), %a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biological and biomedical imaging, across all scales of observation'', 
nine challenges were organized\footnote{\url{https://biomedicalimaging.org/2019/challenges/}}. %: Classification of Normal versus Malignant Cells in B-ALL White Blood Cancer Microscopic Images, Time-Lapse Cell Segmentation Benchmark, Multi-class artefact detection in video endoscopy, Segmentation of Thoracic Organs at Risk in CT images, Automatic Non-rigid Histological Image Registration, MRI White Matter Reconstruction Challenge, Automatic Cancer Detection and Classification in Whole-slide Lung Histopathology, Combined (CT-MR) Healthy Abdominal Organ Segmentation and Pathological Myopia detection from retinal images.
In 2020 a challenge on Image processing on real-time distortion classification in laparoscopic videos was organized with %the 27th IEEE International Conference (
ICIP 2020\footnote{\url{https://2020.ieeeicip.org/challenge/real-time-distortion-classification-in-laparoscopic-videos/}}. %The winning team implemented a decision fusion approach with pre-trained ResNet50 and mapping to different distortion categories with support vector machine classifiers. Their solution outperformed others with accuracy of 83\%, F1 score of a single distortion (94.7\%), and F1 score of single and multiple distortions (94.9\%). What is important as well is that it can be run in real time with an inference speed of 20 frames per second (FPS) (as the authors claim, only one frame is enough to classify the distortion) \citep{DistortionLaparoscopicVideos}.
% https://2020.ieeeicip.org/general/about/, https://2020.ieeeicip.org/challenge/
In the context of ICCV, %IEEE/CVF International Conference on Computer Vision Workshops (ICCVW) was organised in 2021 with 
challenges on remote measurement of physiological signals from videos (RePSS) were organized: one on measurement of inter-beat-intervals (IBI) from facial videos, and another one on respiration measurement from facial videos~\citep{repss1, repss2}.

It is worth mentioning that there are  platforms associated with challenges in biology and medical sciences. The Grand Challenge\footnote{\url{https://grand-challenge.org/challenges/}} platform being perhaps the oldest one and the most representative in terms of imagery: \emph{more than 150 competitions are listed in the platform}, most of which are associated to medical image analysis. A related effort is that of the DREAM challenges\footnote{\url{https://dreamchallenges.org/closed-challenges/}} a platform that has organized more than 60 challenges in biology and medicine. The variety of topics covered by DREAM challenges is vast~\citep{https://doi.org/10.1111/j.1749-6632.2009.04497.x}: from systems biology modelling~\citep{MEYER2021636}, to prevention~\citep{Tarca2020.06.05.130971} and monitoring~\citep{10.1001/jamanetworkopen.2022.27423} damage caused by certain conditions, to disease susceptibility\footnote{\url{https://dreamchallenges.org/respiratory-viral-dream-challenge/}}, to analyzing medical documents with NLP\footnote{\url{https://dreamchallenges.org/electronic-medical-record-nlp-dream-challenge/}}, to drug analysis and combination\footnote{\url{https://dreamchallenges.org/astrazeneca-sanger-drug-combination-prediction-dream-challenge/}} and many other relevant topics. As seen in Chapter 5, platforms play a key role in challenge success, biology is a  field where excellent platforms are available and this has been critical for the advancement of state of the art in this relevant field. 



%With the recent advancements in the field, we might be on the track of a new revolution.
Protein structure modelling was officially introduced in 1994 at the biennial large-scale experiment Critical Assessment of protein Structure Prediction (CASP), and ever since it attracted more than 100 teams to tackle the problem, see~\citep{CASP04}. Only almost 20 years later, two teams presented breaking through solutions to protein folding task~\citep{CASP21}: DeepMind with their AlphaFold2~\citep{Jumper2021HighlyAP} and scientists of the University of Washington with RoseTTAFold~\citep{RosettaFold}. 
% HJ:excellent motivation, iam sorry we have to reduce the section 
% Why is it so important? Simply because proteins are the ''building blocks of life'', they are chains of amino acids, the way these strings folds into different shapes influence the functionalities of proteins. As proteins can be made of 20 different amino acids, calculating the number of possible permutations show that the complexity of the task is impressive. Initially the problem was addressed ''manually'' by using X-ray crystallography and nuclear magnetic resonance imaging, which are expensive and take way more time. So being able to predict the forming of proteins in their 3d structure will open new perspectives: creation of new medications, treatment against cancer and more control over different diseases.
%As for the technical aspects, 
Alphafold uses multiple neural networks that feed into each other in two stages. It starts with a network that reads and folds the amino acid sequence and adjusts how far apart pairs of amino acids are in the overall structure. Then goes the structure model network that reads the produced data, creates a 3D structure, and makes the needed adjustments~\citep{casp13,Jumper2021HighlyAP}.
% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.5]{AlphaFold.jpg.png}
%     \caption{\href{https://www.science.org/content/article/game-has-changed-ai-triumphs-solving-protein-structures}{AlphaFold 2020} performance during different CASP sessions.}
%     \label{fig:AlphaFold}
% \end{figure}
RoseTTAFold adds a simultaneous third neural network, which tracks where the amino acids are in 3D space as the structure folds, alongside the 1D and 2D information~\citep{RosettaFold}. The solution of Washington University is less accurate but uses less computational and time resources than AlphaFold2. Without the existence of the CASP experiment, achieving the outstanding performance of these methods would have taken much more time. 
%Meanwhile, having used GPU optimisations, parallelism strategy like Dynamic Axial Parallelism and Duality Async Operation as communication optimization, a highly efficient implementation of protein structure prediction model called FastFold appeared. Authors claim that it reduces training time from 11 days to 67 hours and achieves 7.5 ∼ 9.5× speedup for long-sequence inference \citep{FastFold}.
%\isabelle{DREAM challenges \url{https://sagebionetworks.org/research-projects/dream-challenges-powered-by-sage-bionetworks/}; \url{https://grand-challenge.org/} }

As we can see, advancements of machine learning in biology are of crucial importance, that’s why there are numerous competitions in this domain. Researchers and practitioners are trying to deal with biological and related domain (medicine, agriculture, and others) challenges using various machine learning solutions like computer vision, NLP and signal processing. %The vast majority of competitions deal with computer vision or natural language processing tasks, while signal processing or cross-domain tasks are quite rare. 

%Computer vision competitions in medicine most often deal with image or video classification and segmentation, as well as medical parameters predictions (heart-rate, human brain responses) and different computer-aided diagnosis.



%\adrien{Nice review. Use bibtex for citations.}




%\subsection{Challenges in Multimedia information processing}


%\subsection{Challenges in Physics}

% \isabelle{You cannot cover everything, but there is also Math and Chemistry. At any rate: mention the CERN challenges \url{https://www.kaggle.com/c/higgs-boson} and \url{https://sites.google.com/site/trackmlparticle/}}

%One of the most popular machine learning competitions in physics, The Higgs Boson Machine Learning Challenge, was as well the most attended on the Kaggle platform at the time of its’ organisation. Interested readers can refer to the literature for the detailed descriptions \citep{ pmlr-v42-cowa14}, \citep{adambourdarios:hal-01104487}, \citep{ https://doi.org/10.48550/arxiv.2012.08520}, \citep{ ATLAS-CONF-2013-108}, while here we give a brief overview.

%In 2012 Higgs boson or Higgs particle was discovered during the ATLAS and the CMS experiments at the Large Hadron Collider (LHC) at CERN near Geneva, Switzerland, and in 2013 François Englert and Peter Higgs received the Nobel prize.

%In physics particles can decay into a set of lighter ones, and this process is called a channel. The main idea of the challenge was to study the Higgs boson to tau tau channel and measure their characteristics. From the machine learning perspective, it was a binary classification to predict whether the event generated in the collider is a signal (H to tau tau decay) or a background (other events which are not the aim of the study). The competition was held\footnote{\url{https://www.kaggle. com/c/higgs-boson}} on the Kaggle platform, it attracted almost 2.000 participants, with 1785 teams and 35.772 submitted solutions. The dataset consisted of 250.000 events with 30 features (the complete HiggsML dataset with full 818.238 events was released afterwards and can be found online\footnote{\url{http://opendata.cern.ch/record/328}}).

%Approximate Median Significance (AMS) was used as a score, it is an objective function used to determine a region in the feature space where one expects an enhanced number of signal events \citep{pmlr-v42-cowa14}. It was another challenging point of the competition as this metric is prone to overfitting since it depends only on the number of the signal events which is relatively small.
%In the figure below you can find the extract from the private leader board of the competition \citep{ pmlr-v42-cowa14}. 

%The winner, Gabor Melis, used an ensemble of dense neural networks with well designed nested cross-validation. As the authors stated the outcomes proved the main prevalence of deep learning, ensemble and advanced tree based methods. Apart from the three cash prizes to the top performers, the committee gave a special ``HEP meet ML`` award to Crowwork team, in fact creators of XGBoost, as they found their model most applicable and useful (due to optimized AMS, CPU and memory demands, robustness and other parameters) \citep{ pmlr-v42-cowa14}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.85]{BestChallenges/Chapter6/Figures/HiggsBosonLB.jpg}
%     \caption{The extract from the private leaderboard of Higgs Boson Machine Learning Competition.}
%     \label{fig:HiggsBoson}
% \end{figure}

\subsection{Challenges in Autonomous Driving}
%% This is really nice, but perhaps too detailed for the chapter, 
%Who haven’t dreamt of optimising the time spent in transport ? Self-driving cars seem to be a perfect solution, in addition it should reduce the number of road accidents. The hype is huge, big companies are investing billions of dollars in this domain. But as time goes by, researchers have started to realise that the task is more difficult than it seemed to be at first. Apart from the perfect safety and other technical requirements, there are other challenges to overcome: proper legislation to be put in place, potential social problems with unemployment of taxi drivers and many others. And yet every year we see new competitions in autonomous driving domain, majority of which are in computer vision, image and object recognition, and reinforcement learning.
DARPA Grand Challenge is considered as one of the first long distance race for autonomous driving cars, it was organised in 2004 with more than 100 teams. None of the robot vehicles managed to finish the 240 km route, only one member covered 11.78 km and then got stuck. Next year there were 195 teams, the distance of the challenge was of 212 km, and five vehicles successfully completed the course. These first courses were challenging but vehicles “operated in isolation”, their interaction was not required, and there was no traffic either. So the next Urban challenge was held in 2007 in a city area, the objective was to complete 96km in 6 hours and it included “driving on roads, handling intersections and maneuvering in zones”~\citep{DarpaTartanTeam}. Six teams managed to complete the course. 
%On the figure below you can see the general architecture of solution of the winner team Tartan.
% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.75]{BestChallenges/Chapter6/Figures/Tartan_urban.PNG}
%     \caption{The Tartan Racing architecture is decomposed into five broad areas: Mission Planning, Motion Planning, Behavior Generation, Perception and World Modeling, and Mechatronics.}
%     \label{fig:Tartan_urban}
% \end{figure}

The basics were laid, and DARPA pursued their competitions: Robotics Challenge in 2012, 2013 - Fast Adaptable Next-Generation Ground Vehicle Challenge, 2013 – 2017 Subterranean Challenge on “autonomous systems to map, navigate, and search underground tunnel, urban, and cave spaces” \footnote{https://www.darpa.mil/about-us/subterranean-challenge-final-event}.

Being able to test autonomous driving cars "in the wild" is important and expensive. In order to fine-grain the algorithms at a less cost one needs to test them virtually. Hopefully, there are different simulators: CARLA \footnote{https://carla.org/}, VISTA 2.0 \footnote{https://vista.csail.mit.edu}, NVIDIA DRIVE Sim \footnote{https://www.nvidia.com/en-us/self-driving-cars/simulation/} and others.

Several challenges have been organised based on  \href{https://carla.org/}{CARLA simulator}, "an open-source simulator for autonomous driving research", which is used to study "a classic modular pipeline, a deep network trained end-to-end via
imitation learning, and a deep network trained via reinforcement learning" \citep{Dosovitskiy17}.

Autonomous driving has numerous interesting challenges, and object detection is one of them. Most of the current research concentrates around camera images, but it is not the best sensor under certain conditions like bad weather, poor lighting. Radar information can help to overcome these inconveniences. It is more reliable, cost-efficient and might potentially lead to better object detection. ROD2021 Challenge is the first competition of its' kind, which proposes object detection task on radar data, and was held in the ACM International Conference on Multimedia Retrieval (ICMR) 2021. Organisers developed their own baseline: “radar object detection pipeline, which consists of two parts: a teacher and a student. Teacher’s pipeline fuses the results from both RGB and RF images
to obtain the object classes and locations in RF images. Student’s pipeline
utilizes only RF images as the input to predict the corresponding ConfMaps
under the teacher’s supervision. The LNMS as post-processing is followed
to calculate the final radar object detection results.” \citep{9353210}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.75]{BestChallenges/Chapter6/Figures/RODnet.PNG}
%     \caption{The proposed cross-modal supervision pipeline for radar object detection.}
%     \label{fig:RODnet}
% \end{figure}

This challenge attracted more than 260 participants among 37 teams with around 700 submissions. The winning team, affiliated to Baidu, submitted paper “DANet: Dimension Apart Network for Radar Object Detection” \citep{DANet}, where they presented their results. "This paper proposes a dimension apart network (DANet), including a lightweight dimension apart module (DAM) for temporal-spatial feature extraction. The proposed DAM extracts features from each dimension separately and then concatenates the features together. This module has much smaller number of parameters, compared with RODNet-HGwI, so that significant reduction of the computational cost can be achieved. Besides, a vast amount of data augmentations are used for the network training, e.g., mirror, resize, random combination, Gaussian noise and reverse temporal sequence. Finally, an ensemble technique is implemented with a scene classification for a more robust model. The DANet achieves the first place in the ROD2021 Challenge. This method has relatively high performance but with less computational cost, which is an impressive network model. Besides, this method shows data augmentation and ensemble techniques can greatly boost the performance of the radar object detection results" \citep{wang2021rod2021}.
 
Another interesting and pioneering challenge is OmniCV (Omnidirectional Computer Vision) in conjunction with IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'2021). The objective was to evaluate semantic segmentation techniques targeted for fisheye camera perception. It attracted 71 teams and a total of 395 submissions. Organisers proposed their baseline “a PSPNet network with a ResNet50 backbone finetuned on WoodScape Dataset”, which "achieved a score of 0.56 (mIoU 0.50, accuracy 0.67) excluding void class". The top teams managed to get significantly better scores and proposed interesting solutions. The winning team implemented full Swin-transformer Encoder-Decoder approach, with a score of 0.84 (mIoU 0.86, accuracy 0.89) \citep{Fisheye_AD}.


%\adrien{Maybe this fits more on chapter 8 (industry)}



%\subsection{Challenges in other fields}
% It is not possible to cover  challenges in all fields of knowledge, applications and topics. However, it is worth mentioning other efforts that have helped to shape the  future in specific areas. 

% The Causality 

%Causality ? or other interesting topic
%\isabelle{\url{http://www.causality.inf.ethz.ch/} and NeurIPS causality challenges}
%\isabelle{Challenges in optimization \url{https://bbochallenge.com/}, in RL Learning to run \url{https://arxiv.org/abs/1804.00361} and L2RPN \url{https://l2rpn.chalearn.org/}.}



% RoboCup

% TREC

% ImageCLEF

% MultimediaEval

% CLEF

% The PASCAL Visual Objects challenge

% NeurIPS - check proposals 

% http://host.robots.ox.ac.uk/pascal/VOC/index.html










% \section{The future of academic challenges}
% \label{sec:future}
% Section covering the  way challenges will be in the next few years. Open issues, trends, opportunities, etc. 

% Data centric competitions

% \url{https://https-deeplearning-ai.github.io/data-centric-comp/?utm_source=thebatch&utm_medium=newsletter&utm_campaign=dc-ai-competition&utm_content=dl-ai}


\section{Discussion}
\label{sec:discussion7}
Academic challenges have been decisive for the consolidation of fields of knowledge. This chapter provided an historical review and an analysis of benefits and limitations of challenges, while it is true that competitions can have undesired effects, there is palpable evidence that they have boosted research across a number of fields. In fact there are several examples of breakthrough discoveries that have arosen in the context of academic competitions.  

While we are witnessing the establishment of academic competitions as a way to advance the state of the art, the forthcoming years are promising. Specifically, we consider that the following lines of research will be decisive in the next few years:
\begin{itemize}
    \item \textbf{Data centric competitions\footnote{\url{https://https-deeplearning-ai.github.io/data-centric-comp}}} This is competitions where the goal is to improve a dataset by applying so called, data-centric techniques, like fixing mislabeled samples, finding prototypes, border points, summarization, data augmentation, etc. 
    \item \textbf{Cooperative competitions.} Coopetitions is a form of crowd sourcing in which participants compete to build the best solution for a problem, but they cooperate with other participants in order to obtain an additional reward (e.g., information from other participants, higher scores, etc.).
    \item \textbf{Challenges for education.} Exploiting the full potential of challenges in education is a challenge itself, but we think this is  a valuable resource for reaching wider audiences with assignments that require solving practical problems.
    \item \textbf{Academic challenges for good.} This is a topic being pursued and encouraged by evaluation forums and competition tracks, consider for instance the NeurIPS competition track~\citep{DBLP:conf/nips/EscalanteH19,DBLP:conf/nips/EscalanteH20,pmlr-v176-kiela22a}. 
    \item \textbf{Dedicated publications for challenges.} There are few dedicated forums in which results of challenges are published (consider for instance the Challenges in Machine Learning series\footnote{\url{https://www.springer.com/series/15602}}). We foresee more dedicated venues will be available in the next few years.   
    %\item 
\end{itemize}


%{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

% \appendix
% \section*{Appendix A.}
% \label{app:theorem}

% % Note: in this sample, the section number is hard-coded in. Following
% % proper LaTeX conventions, it should properly be coded as a reference:

% %In this appendix we prove the following theorem from
% %Section~\ref{sec:textree-generalization}:

% In this appendix we prove the following theorem from
% Section~6.2:

% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{ref}

\end{document}