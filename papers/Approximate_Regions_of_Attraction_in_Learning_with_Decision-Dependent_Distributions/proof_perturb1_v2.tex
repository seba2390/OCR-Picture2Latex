% !TEX root = main.tex

Let $V(x) = R(x,x) - R(x^*,x^*)$. Note that $V(x) \ge 0$ on $U = \{ x : |x - x^*| \le r \}$ and $V(x) = 0$ if and only if $x = x^*$. Furthermore, note that $\frac{\partial V}{\partial x}(x) = [\nabla_{x_1}R(x,x) + \nabla_{x_2}R(x,x)]^\T$.

Consider the function $t \mapsto V(\varphi_{\fpert}(t;x_0))$ and its time derivative. Also, let $\xpert(t) = \varphi_{\fpert}(t;x_0)$. 
When $|\xpert-x^*| \ge \delta/c_3$, taking the derivative along trajectories of the repeated risk minimization flow and using Equations~\eqref{eq:v_ineq2} and~\eqref{eq:ass_V1}:
\[
\begin{aligned}
    \mc{L}_{\fnom + g}V 
    &=
    \frac{\partial V}{\partial x} (\fnom(x) + g) 
    =
    -|\nabla_{x_1}R + \nabla_{x_2}R|^2 + \langle \nabla_{x_1}R + \nabla_{x_2}R, \nabla_{x_2}R \rangle \\
    &\le
    -\left( c_3 |\xpert-x^*|-\delta \right)^2
    +
    \left( c_4 |\xpert-x^*|+\delta \right) |\nabla_{x_2}R| \\
    &\le
    -\left( c_3 |\xpert-x^*|-\delta \right)^2
    +
    \left( c_4 |\xpert-x^*|+\delta \right) (\epsilon|\xpert-x^*|+\delta)\\
    &=
    -\left(c_4\epsilon-c_3^2\right)|\xpert-x^*|^2
    +\left(c_4\delta + 2c_3\delta+\delta\epsilon\right)|\xpert-x^*|
\end{aligned}
\]
These inequalities are valid so long as $\xpert(t)$ stays within $U$, which we will ensure later in the proof. Note that $\eps$ is sufficiently small (by assumption) to ensure that $-c_3^2 + c_4 \eps < 0$.

Let $\alpha := c_3^2 - c_4 \eps > 0$. Take any $\theta \in (0,1)$ and note that:
\[
\mc{L}_{\fnom + g}V(\xpert) \le - \theta \alpha | \xpert - x^* |^2 - (1 - \theta) \alpha | \xpert - x^* |^2 +\left(c_4\delta + 2c_3\delta+\delta\epsilon\right)|\xpert-x^*|
\]
Now, let 
\[
    \mu(\theta)
    :=
    \max
    \left\{
    \frac
    {
        \left(c_4\delta + 2c_3\delta+\delta\epsilon\right)
    }
    {\alpha(1 - \theta)},
    \frac{\delta}{c_3}
    \right\}.
\]
If $|\xpert - x^*| \ge \mu(\theta)$, then:
\[
- \theta \alpha | \xpert - x^* |^2 - (1 - \theta) \alpha | \xpert - x^* |^2 +\left(c_4\delta + 2c_3\delta+\delta\epsilon\right)|\xpert-x^*| \le 0,
\]
and thus
\[
\mc{L}_{\fnom + g}V(\xpert) \le - \theta \alpha | \xpert - x^* |^2.
\]
Trajectories of Equation~\eqref{eq:RGD_flow} has two stages: a transient due to its initial condition, and then an ultimate bound due to the perturbation. Let $T(\theta) = \inf~\{ t \ge 0 : |\xpert(t) - x^*| \le \mu(\theta) \}$. Prior to $T(\theta)$, we have:
\[
\frac{d}{dt} V(\xpert(t)) \le - \theta \alpha |\xpert(t) - x^*|^2 \le - \frac{\theta \alpha}{c_2} V(\xpert(t))
\]
The latter follows from Equation~\eqref{eq:v_ineq1}. 
By the comparison principle (see, e.g.~\citep[Lemma 3.4]{Khalil:2001wj}), we have $V(\xpert(t)) \le \exp(-t\theta \alpha / c_2) V(x_0)$. Again using Equation~\eqref{eq:v_ineq1}, this yields the following inequality, valid for all $t \le T(\theta)$:
\[
|\xpert(t) - x^*| \le \sqrt{\frac{c_2}{c_1}} \exp(-t\theta \alpha/2c_2) |x_0 - x^*|
\]
Note that this inequality also provides an upper bound on $T(\theta)$. Additionally, note that this implies the bound $|\xpert(t) - x^*| \le r$, by our assumption on the initial condition. Prior to $T(\theta)$, our trajectory stays in $U$, where our inequalities are valid.

At time $T(\theta)$, we have $|\xpert(t) - x^*| \le \mu(\theta)$. Note that this inequality implies $V(\xpert(t)) \le c_2 \mu^2(\theta)$. Since $\mc{L}_{\fnom + g}V < 0$ on the boundary of $\Omega(\theta) := \{ x : V(x) \le c_2 \mu^2(\theta) \}$, we have that $\Omega(\theta)$ is a positively invariant set. So, for all $t \ge T(\theta)$, we have $\xpert(t) \in \Omega(\theta)$. Using Equation~\eqref{eq:v_ineq1}, we have the following for all $t \ge T(\theta)$:
\[
|\xpert(t) - x^*| \le 
\sqrt{\frac{c_2}{c_1}} \mu(\theta) 
\]
The condition on $\theta$ ensures that this quantity is bounded by $r$, and the trajectory stays in $U$ for $t \ge T(\theta)$. 
This proves our desired result.

Additionally, we can show that this bound is tight by considering the following example. 
Suppose $\cD(x_2)$ is the point mass distribution (i.e. $p(z) = \delta(z - x_2)$) and $l(z, x_1) = 1/2 |z|^2 + 1/2|x_1|^2$. Then the performative risk is given by $R(x_1, x_2) = 1/2 |x_1|^2 + 1/2|x_2|^2$. It follows that $x^* 0$ is the performative risk minimizer. Following the arguments in Appendix A, one would find that the dynamics of $V(x) = R(x, x) - R(x^*, x^*)$ follows $\frac{d}{dt} V(x(t)) = -2 |x(t) - x^*|^2 = -2 V(x(t))$, which yields 
    $|x(t) - x^*| = \exp(-2t)|x_0 - x^*|$.
