% !TEX root = main.tex

In this section, we introduce the mathematical concepts used throughout this paper. As previously mentioned, the framework used throughout this paper builds on the framework of performative prediction, introduced in~\citet{Perdomo:2020tz}.

In Section~\ref{sec:intro}, we have already defined the \textbf{performative risk} in Equation~\eqref{eq:perf_risk} and the \textbf{decoupled performative risk} in Equation~\eqref{eq:decoupled_perf_risk}. 
Furthermore, we say that $x$ is a \textbf{local performative risk minimizer} is $x$ is a local minima of $PR(\cdot)$. We say $x$ is \textbf{locally performatively stable} if $x$ is a local minima of $y \mapsto R(y,x)$. In general, neither imply the other~\citep{Perdomo:2020tz}.

Additionally, we consider the \textbf{performative risk minimizing (PRM) gradient flow}, defined by the following differential equation:
\begin{equation}\label{eq:prm_flow}
   \begin{aligned}
\dxnom &= - \nabla PR(\xnom) \\
&= - \nabla_{x_1} R(\xnom,\xnom) - \nabla_{x_2}R(\xnom,\xnom)\\
&=: \fnom(\xnom)
\end{aligned} 
\end{equation}
This vector field can be represented by the gradient of a function, which lends the flow to nice analysis. Under mild conditions, the trajectories of Equation~\eqref{eq:prm_flow} will converge to local minima of the performative risk.

However, as noted in Section~\ref{sec:intro}, many deployments of machine learning do not explicitly model the distribution shift, and, consequently, do not directly minimize the performative risk. We define the \textbf{repeated gradient descent (RGD) flow} as solutions to the differential equation:
\begin{equation}
\label{eq:RGD_flow}
\dxpert = -\nabla_{x_1}R(\xpert,\xpert) =: \fpert(\xpert)
\end{equation}
We define the \textbf{performative perturbation}:
\[
g(x) := \nabla_{x_2}R(x,x) = \fpert(x) - \fnom(x)
\]
In this paper, we view the PRM gradient flow as the \textit{nominal} dynamics, and the RGD flow as the \textit{perturbed} dynamics. The PRM gradient flow has nice properties arising from the fact it is a gradient flow, and, under certain conditions on the performative perturbation, we can prove properties about the RGD flow, which is the quantity of interest. In particular, we show ultimate bounds on the distance between the trajectories of RGD flow and the local performative risk minimizers. This also implies that under certain conditions on the performative risk, all performatively stable points are near performative risk minimizers, as was observed in~\citet{Perdomo:2020tz}.

Throughout this paper, we will be using tools from perturbation analysis in control theory. For a complete vector field $\dot x = f(x)$, let $\varphi_f(\cdot;x_0)$ denote the unique solution to the differential equation with initial condition $x(0) = x_0$. For a scalar-valued function $V$ and a vector field $f$, we can define the derivative along trajectories as $\mc{L}_fV(x) = \frac{\partial V}{\partial x}f(x)$. 
We say a point $x$ is an \textbf{equilibrium point} if $f(x) = 0$. An equilibrium point $x$ is \textbf{locally asymptotically stable} if there exists a neighborhood $U \ni x$ such that $\lim_{t \rightarrow \infty} \varphi_f(t;x') = x$ for all $x' \in U$. A set $A$ is \textbf{positively invariant} if for all $x_0 \in A$ and $t \ge 0$, we have $\varphi_f(t;x_0) \in A$. 
Additionally, given a set $A \subset \mb{R}^n$, we say two points $x$ and $y$ are \textbf{path-connected in $A$} if there exists a continuous function $\gamma : [0,1] \to A$ such that $\gamma(0) = x$ and $\gamma(1) = y$. This forms an equivalence relation defined on $A$, and each equivalence class is a \textbf{connected component of $A$}. Additionally, we will use $\mc{W}_1(\cdot)$ to denote the \textbf{Wasserstein distance}, also known as the earth mover's distance.
