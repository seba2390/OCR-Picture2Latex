% !TEX root = main.tex

In this section, we consider PRM gradient flow, defined by Equation~\eqref{eq:prm_flow}. We observe that gradient flows provide complete vector fields, and that trajectories will converge to local performative risk minimizers under very mild conditions.

First, we state a proposition guaranteeing that flow is well-defined. The compact sublevel sets ensure that trajectories of Equation~\eqref{eq:prm_flow} remain bounded, which is sufficient to guarantee existence and uniqueness of solutions globally. For proof of the following proposition, we refer the reader to either~{\citet[Section 3.1]{Khalil:2001wj}} or~{\citet[Section 9.3]{Hirsch:2012tx}}.

\begin{proposition}[Existence and uniqueness of gradient flows]

Suppose the performative risk $PR(\cdot)$ is continuously differentiable, and its sublevel sets $\{ x : PR(x) \le c \}$ are compact for every $c \in \mb{R}$. Then for any initial condition $\xnom(0) = x_0$, there exists a unique solution to the differential equation in Equation~\eqref{eq:prm_flow}, defined for all $t \ge 0$.

\end{proposition}

Next, we note that gradient flows have nice properties from the perspective of optimization. Namely: every isolated local minima is locally asymptotically stable, and we can provide sufficient conditions to characterize a subset of the region of convergence.

\begin{proposition}[Convergence of gradient flows]

Suppose the performative risk $PR(\cdot)$ is twice continuously differentiable, and $x^*$ is an isolated local performative risk minimizer. Then $x^*$ is a locally asymptotically stable equilibrium of Equation~\eqref{eq:prm_flow}. 
Furthermore, take any $c$ such that $PR(x^*) \le c$. Let $A \subseteq \{ x : PR(x) \le c \}$ denote the connected component of $\{ x : PR(x) \le c \}$ that contains $x^*$. If $x^*$ is the only local performative minimizer in $A$, then all solutions with initial conditions in $A$ converge to $x^*$.

\end{proposition}

\begin{proof}
Since $x^*$ is an isolated local minimizer and the performative risk is twice continuously differentiable, there exists a neighborhood $U \ni x^*$ such that $\nabla PR(\cdot)$ is non-zero for all $x \neq x^*$. By continuity, there exists some constant $\eps$ such that the connected component of $\{ x : PR(x) \le PR(x^*) + \eps \}$ containing $x^*$ is contained in $U$. Since it is a sublevel set of $PR(\cdot)$ and $\mc{L}_{\fnom}PR(x) < 0$ on its boundary, it is positively invariant. Furthermore, since $\mc{L}_{\fnom}(x) < 0$ for all $x \neq x^*$ on this set, $x^*$ is locally asymptotically stable by standard Lyapunov arguments (see, e.g.~\citet[Section 4]{Khalil:2001wj}).
\end{proof}

The sublevel sets of the performative risk are positively invariant with respect to the PRM gradient flow. Furthermore, because of the continuity of trajectories, each connected component will also be positively invariant. This, in tandem with the fact that trajectories must either converge to a local minima or go off to infinity, also implies the previous proposition.

With minimal assumptions, isolated local performative risk minimizers are all locally attractive in the PRM gradient flow. In Section~\ref{sec:analysis_RGD}, we will view the PRM gradient flow as the nominal dynamics. From this perspective, we analyze the RGD flow as a perturbation from these nominal dynamics. To be able to do any perturbation-based analysis, we will need some stronger conditions on the convergence of the gradient flow associated with performative risk minimization. We note these assumptions here.

\begin{assumption}[Sufficient curvature of the PR]
\label{ass:exist_V}

Fix some isolated local performative risk minimizer $x^*$. 
We assume there exists positive constants $c_1$, $c_2$, $c_3$, $c_4$ and $\delta$ such that the following holds in a neighborhood of $x^*$:
\begin{equation}
\label{eq:v_ineq1}
c_1 |x-x^*|^2 \le PR(x) - PR(x^*) \le c_2 |x-x^*|^2
\end{equation}
\begin{equation}
\label{eq:v_ineq2}
c_3 |x - x^*| - \delta \le | \nabla PR(x) | \le c_4 |x - x^*| + \delta
\end{equation}

We will let $r$ denote the radius of this neighborhood, so the above inequalities are valid on the set $\{ x : |x - x^*| \le r \}$.

\end{assumption}
Assumption~\ref{ass:exist_V} provides conditions on which $V(x) = PR(x) - PR(x^*)$ can be used as a Lyapunov function locally. Next, we provide conditions directly on the loss $\ell(\cdot)$ and the decision-dependent distribution shift $\mc{D}(\cdot)$ which can ensure that Assumption 1 holds. First, we provide sufficient conditions for the bounds in Equation~\eqref{eq:v_ineq1}.

\begin{proposition}[Performative risk bounds]
\label{prop:pr_bnds}
Let $x^*$ be a performative risk minimizer and fix any $x$. If:
\begin{enumerate}
    \item $\ell(\cdot,x)$ is $L_1$ Lipschitz continuous
    \item $\mc{W}_1(\mc{D}(x),\mc{D}(x^*)) \le L_2 |x-x^*|^2$
    \item $\ell(z,\cdot)$ is $m$-strongly convex and $L_3$-smooth for every $z$
\end{enumerate}
Then: $(m/2 - L_1 L_2) |x-x^*|^2 \le PR(x) - PR(x^*) \le (L_1 L_2 + L_3/2) |x-x^*|^2$.
\end{proposition}

\begin{proof}
First, we can break up the performative risk into two parts: $PR(x) - PR(x^*) = R(x,x) - R(x^*,x^*) = [R(x,x) - R(x,x^*)] + [R(x,x^*) - R(x^*,x^*)]$. 
Note that $R(x,x) - R(x,x^*) = \mb{E}_{Z \sim \mc{D}(x)} [\ell(Z,x)] - \mb{E}_{Z \sim \mc{D}(x^*)} [\ell(Z,x)]$. Conditions (1) and (2), along with Kantorovich-Rubenstein duality~\citep{Villani:2003th}, implies this quantity is bounded in absolute value: $|R(x,x) - R(x,x^*)| \le L_1 L_2 |x-x^*|^2$. 
On the other hand, $R(x,x^*) - R(x^*,x^*) = \mb{E}_{Z \sim \mc{D}(x^*)} [\ell(Z,x) - \ell(Z,x^*)]$. By convexity and $L_3$-smoothness, $\ell(z,x) - \ell(z,x^*) \le \langle \nabla_x \ell(z,x^*), x-x^* \rangle + \frac{L_3}{2} |x-x^*|^2$ for any $z$; taking the expectation and noting that $\nabla PR(x^*) = 0$, we have $R(x,x^*) - R(x^*,x^*) \le \frac{L_3}{2} |x-x^*|^2$. In the other direction, using strong convexity and similar arguments, we get: $R(x,x^*) - R(x^*,x^*) \ge \frac{m}{2} |x-x^*|^2$. Combining these results yields the desired results.
\end{proof}
Note that Condition (2) in Proposition~\ref{prop:pr_bnds} is a variation on the typical $\eps$-sensitivity definition. Recall that $\eps$-sensitivity states that for any $x$ and $y$, $\mc{W}_1(\mc{D}(x),\mc{D}(y)) \le \eps|x-y|$~\citep{Perdomo:2020tz}. In contrast, Condition (2) only requires this condition to hold around the point $x^*$, but requires a stricter bound for $x$ close to $x^*$. This bound is also more lax than $\eps$-sensitivity farther away from $x^*$.

Next, we provide sufficient conditions for a bound on the absolute value of the gradient of the performative risk. 
% We fixed this issue: 
%This does not exactly recover Assumption~\ref{ass:exist_V}, as there are additive constants on the upper and lower bounds which do not scale with $|x-x^*|$. However, these additive constants will be small for decision-dependent distribution shifts $\mc{D}(\cdot)$ with sufficiently small sensitivity parameter $\eps$.

\begin{proposition}[Gradient bounds of the performative risk]
\label{prop:grad_bounds}
Let $x^*$ be a performative risk minimizer and fix any $x$. If:
\begin{enumerate}
    \item $\ell(\cdot,x)$ and $\ell(\cdot, x^*)$ are both $L_1$ Lipschitz continuous
    \item $\ell(z,\cdot)$ is $m$-strongly convex and $L_3$-smooth for every $z$
    \item $\mc{D}(\cdot)$ is $\eps$-sensitive, i.e. $\mc{W}_1(\mc{D}(x),\mc{D}(y)) \le \eps |x-y|$
    \item $\nabla_x \ell(\cdot,x)$ is $L_4$ Lipschitz continuous
\end{enumerate}
Then: $(m- \eps L_4)|x-x^*| - 2 \eps L_1 \le |\nabla PR(x)| \le (L_3 + \eps L_4) |x-x^*| + 2 \eps L_1$.
\end{proposition}

\begin{proof}
Similar to the previous proposition, we break apart this gradient. Note that $\nabla PR(x^*) = 0$, so:
$|\nabla PR(x)| = |\nabla PR(x) - \nabla PR(x^*)| =
|\nabla_{x_1} R(x,x) - \nabla_{x_1} R(x^*,x^*) +
\nabla_{x_2} R(x,x) - \nabla_{x_2} R(x^*,x^*)|$. For the $\nabla_{x_1}$ terms, we have: $m|x-x^*| \le |\nabla_{x_1} R(x,x^*) - \nabla_{x_1}R(x^*,x^*)| \le L_3|x-x^*|$ by standard convexity arguments, and $|\nabla_{x_1} R(x,x) - \nabla_{x_1}R(x,x^*)| \le \eps L_4 |x-x^*|$ by the same Kantorovich-Rubenstein duality argument as the previous proposition. For the $\nabla_{x_2}$ terms, note that the mapping $x_2 \mapsto R(x,x_2)$ is $\eps L_1$ Lipschitz continuous. Thus, $|\nabla_{x_2} R(x,x)| \le \eps L_1$ and similarly $|\nabla_{x_2} R(x^*,x^*)|$. Combining these inequalities yields the desired result.
\end{proof}

Depending on the situation, we may be able to directly verify Assumption~\ref{ass:exist_V}, although, for more complex settings, this is likely to be very difficult. Propositions~\ref{prop:pr_bnds} and~\ref{prop:grad_bounds} provide a set of sufficient conditions for this assumption to hold, but checking the conditions on the decision-dependent distribution shift $\mc{D}(\cdot)$ may be difficult in practice as well. This is one limitation of this current work, and we believe it is an interesting future research direction to identify conditions which are easy to verify, even in settings with limited information about the distribution shift itself.
