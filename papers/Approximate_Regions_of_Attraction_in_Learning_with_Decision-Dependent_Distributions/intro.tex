% !TEX root = main.tex

Data-driven methods are growing increasingly popular in practice. Most classical machine learning and statistical methods view the underlying process which generates the data as fixed: the study is primarily focused on the mapping from data distributions to classifier. However, it is important to consider the effects in the other direction as well: how does the classifier chosen by a learner change the data distribution the learner sees? In particular, how do we close the loop around machine learning deployments in practice?

These closed loop effects can arise in many real world settings. One instance is strategic classification: whenever a data source has a stake in which label a classifier applies to it, they will seek cost-effective ways to manipulate their data to earn the desired label. For example, credit scoring classifiers are heavily guarded for fear of the potential for gaming~\citep{Hardt:2016we}. 
Alternatively, deployments of the classifier can both skew future datasets and also have causal influences over the real-world processes at play. For example, a classifier that predicts crime recidivism influences the opportunities available to individuals~\citep{Dressel:2018uf}.

Formally, we consider this problem in the framework introduced in~\citet{Perdomo:2020tz}. Let $\ell(z,x)$ denote the loss when the learner's decision is $x$ (e.g. $x$ can be the parameters of the chosen classifier) and the data has realized value $z$. Furthermore, let $\mc{D}(x)$ denote the data distribution when the learner's decision is $x$. In this framework, the performative risk is given by:
\begin{equation}
\label{eq:perf_risk}
PR(x) = \mb{E}_{Z \sim \mc{D}(x)} [\ell(Z,x)]
\end{equation}
Whereas classical machine learning results treat the distribution $Z \sim \mc{D}$ as fixed, the performative prediction framework models the decision-dependent distribution as a mapping $\mc{D}(\cdot)$. However, in many real world-deployments, this decision-dependent distribution shift may not be explicitly included in the learner's updates. This leads to algorithms based on inexact repeated minimization. Define the decoupled performative risk as:
\begin{equation}
\label{eq:decoupled_perf_risk}
R(x_1,x_2) = \mb{E}_{Z \sim \mc{D}(x_2)} [\ell(Z,x_1)]
\end{equation}
The decoupled performative risk $R(x_1,x_2)$ separates the two ways that the decision variable $x$ affects the performative risk. Through the $x_1$ argument, $x$ affects the classification error; through the $x_2$ argument, $x$ causes a decision-dependent distribution shift. 
Thus, when the decision-dependent distribution shift is not accounted for, the repeated gradient descent method yields the following update rule:
\begin{equation}
\label{eq:rep_motiv_eq}
x_{k+1} = x_k - \alpha_k (\nabla_{x_1} R(x_k,x_k) + \eta_k)
\end{equation}
Here, $(\eta_k)_k$ is some zero-mean noise process. 
Note that the gradient is evaluated only with respect to the first argument, i.e. the updates are based only on the effect of $x$ on the loss function, and ignore the distribution shift caused by $x$. In other words, the learner draws several observations from the distribution $\mc{D}(x_k)$, and, treating this distribution as fixed, updates their model parameters $x_{k+1}$ based on stochastic gradient descent: they are descending the gradient of the cost function $y \mapsto R(y,x_k)$.

In this paper, we shall analyze the steady-state behavior of the continuous-time flows corresponding to Equation~\eqref{eq:rep_motiv_eq}:
\begin{equation}
\label{eq:rgd_flow1}
\dot x = - \nabla_{x_1}R(x,x)
\end{equation}
The connections between the flow of Equation~\eqref{eq:rgd_flow1} and the repeated gradient descent method in Equation~\eqref{eq:rep_motiv_eq} can be drawn using results in stochastic approximation, i.e. the latter can be seen as a noisy forward Euler discretization of the former. For more details, we refer the reader to~\citet{Borkar:2008ts}.

In particular, we focus on settings where there may be multiple local equilibria, and classify their regions of attraction for these equilibria. In many settings of interest, there may be multiple steady-state outcomes, and it is of interest to determine which outcome will be chosen by the dynamics in Equation~\eqref{eq:rep_motiv_eq}. Our results allow us to characterize which regions of the parameter space will converge to which equilibria. 
We discuss this example in greater formal detail in Section~\ref{sec:example}.

Our main theoretical results can be informally summarized as follows. Theorem 1 states that trajectories of inexact repeated risk minimization will converge exponentially fast to a neighborhood of local performative risk minimizers, and stay in this neighborhood for all future time. It also provides a sufficient condition to under-approximate the regions of attraction for each local performative risk minimizer. In the special case of vanishing perturbations, these trajectories will converge to the minimizers themselves. As a corollary, this implies that performatively stable points will be near performatively optimal points, which was first observed in~\citet{Perdomo:2020tz} under a different set of conditions. 
We note that Theorem 1 requires conditions on the curvature of the performative risk: the sublevel sets $\{ x : PR(x) \le c \}$ must grow in a precise fashion, such that upper and lower bounds on the performative risk $PR(x)$ imply upper and lower bounds on the norm of the argument $x$.
Furthermore, the gradient of the performative risk must not vary too wildly around nearby points. This is formalized in Assumption~\ref{ass:exist_V}. 
Theorem 2 states a geometric condition on the performative perturbation which ensures that trajectories of repeated risk minimization will converge to local performative risk minimizers, intuitively based on the idea that the perturbation does not push against convergence. This result does not require the strong curvature assumptions of Theorem 1.

These results allow us to identify the regions of attraction for various steady-state outcomes. As observed in~\citet{Miller:2021te}, these various outcomes can be interpreted as different echo chambers: essentially the decision variable $x$ can act as a sort of self-fulfilling prophecy.\footnote{It is worth noting that we take a slightly different interpretation of an `echo chamber' in this paper. In~\citet{Miller:2021te}, the echo chambers are defined as performatively stable points. In this paper, we consider the regions near each locally performatively optimal point as an echo chamber. As we will discuss in Section~\ref{sec:example}, we are interested in settings where there may be many local performative risk minimizers that attract learning methods depending on initialization.} In settings with multiple echo chambers, we consider the question of which echo chamber will come to dominate, based on the initialization of the learner. 

The rest of the paper is organized as follows. In Section~\ref{sec:background}, we discuss the related literature. In Section~\ref{sec:model}, we introduce the problem statement and the mathematical concepts used for our results, and provide motivating examples in Section~\ref{sec:example}. In Section~\ref{sec:analysis_prm}, we analyze the gradient flow associated with performative risk minimization, and in Section~\ref{sec:analysis_RGD}, we analyze the flows associated with repeated risk minimization. We demonstrate numerical results in Appendix~\ref{sec:num_results}, and provide closing remarks in Section~\ref{sec:conclusion}.
