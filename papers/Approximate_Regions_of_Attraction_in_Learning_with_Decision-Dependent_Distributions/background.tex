% !TEX root = main.tex

There has been a great deal of interest in studying decision-dependent distributions. In the context of operations research, this has been studied under either the name decision-dependent uncertainty or endogenous uncertainty. In~\citet{Jonsbraten:1998wc},~\citet{Jonsbraten:1998wk}, and~\citet{Goel:2004wb}, the authors considered oil field optimization, with a framework that captures how information revelation can be affected by one's decisions. In~\citet{Peeta:2010tb}, the authors consider infrastructure investment, and how investments can affect the future likelihood of disasters. For a taxonomy of the work in the operations research community, we refer the reader to~\citet{Hellemo:2018tf}.

Another form of decision-dependent distributions is strategic classification. In these works, the data source is seen as a utility-maximizing agent. The distribution shift resulting from the learner's decision is modeled by a best response function. In~\citet{Hardt:2016we} and~\citet{Bruckner:2011wy}, the authors formulate the problem as a Stackelberg game where the data source responds to the announced classifier. In~\citet{Dong:2018td}, the authors consider when the data source's preferences are hidden information and provide sufficient conditions for convexity of the overall strategic classification task. In~\citet{Akyol:2016wu}, the authors quantify the cost of strategic classification for the classifier. In~\citet{Milli:2019tf} and~\citet{Hu:2019wu}, the authors note that certain groups may be disproportionately affected as institutions incorporate methods to counter data sources gaming the classifier. In~\citet{Miller:2020vy}, the authors formulate strategic classification in a causal framework.

Most related to our work is recent efforts in performative prediction. This was introduced in~\citet{Perdomo:2020tz}. In this formulation, rather than explicitly modeling the form of the distribution shift, it proposes to analyze the decision-dependent distribution shift in terms of general properties of the $\mc{D}(\cdot)$ mapping, where $\mc{D}(x)$ is the distribution of the data when the learner's decision is $x$. In~\citet{Perdomo:2020tz}, the authors introduced the concepts related to performative prediction, demonstrated that neither the performatively stable nor performatively optimal points are subsets of each other, provided sufficient conditions for exact repeated risk minimization (defined as finding the exact minima with respect to $\mc{D}(x_k)$ at each time step) to converge, and provided conditions in which performatively stable points are near performatively optimal points. In~\citet{Mendler-Dunner:2020vd}, the authors analyze inexact repeated risk minimization (defined as an update step with respect to $\mc{D}(x_k)$ at each time step) from a stochastic optimization framework. In this paper, we build on the inexact repeated risk minimization framework. \citet{Miller:2021te} provided sufficient conditions for performative risk itself to be convex. \citet{Brown:2020wg} extended these results to settings where the distribution updates may have an internal state. 
In~\citet{Drusvyatskiy:2020wk}, the authors show that many inexact repeated risk minimization algorithms will also converge nicely, due to the way in which the performative perturbation decays near the solution. This shares many ideas with our work here, but we focus on the case where there may be multiple attractive equilibria, and generalize to settings where the perturbation itself may not vanish. 
In contrast to previous works which provide sufficient conditions to guarantee that an outcome is approached globally, we focus on understanding local regions of attraction for various outcomes.

This work draws on ideas from control theory; in particular, the analysis of gradient flows, Lyapunov functions, and perturbation analysis are the tools we use throughout. We refer the reader to~\citet{Hirsch:2012tx} and~\citet{Khalil:2001wj} as good references for these suite of tools.

Although our work still focuses on repeated risk minimization, it is worth noting that many other algorithms exist for learning with decision-dependent distributions. 
In \citet{Jag22}, the authors proposes the \textit{performative confidence bounds algorithm} which uses tools from the bandit literature to explore the distribution map and find a near-optimal solution. 
In \citet{Izzo21_1}, the authors proposes an algorithm called \textit{performative gradient descent} (PerfGD), which guarantees to find the performatively optimal point when $\cD(\cdot)$ satisfies certain parametric assumptions. Later in \citet{Izzo21_2}, the author extends the results to settings where the distribution updates have internal states. In the same year, \cite{Li21} presents state-dependent stochastic approximation (SA) algorithm that works in similar settings. Of course, this paragraph is not a exhaustive treatment of various algorithms in similar settings. 
% We encourage interested readers to explore this line of work on their own.
