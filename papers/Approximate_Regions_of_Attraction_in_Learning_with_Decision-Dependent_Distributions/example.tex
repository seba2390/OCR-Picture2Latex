% !TEX root = main.tex

Before we present our analysis of the PRM gradient flow and the RGD flow, we introduce some examples which motivate the study of performative risk in non-convex settings and multiple local equilibria. 

\subsubsection{Squared error loss and Bernoulli distributions}
\label{sec:simple_ex}

Consider the loss function $\ell(z,x) = \frac{1}{2} |z-x|^2$, where $x$ is a scalar. Furthermore, suppose that the decision-dependent distribution $\mc{D}(x)$ is simply $Z = 1$ with probability $p(x)$ and $Z = 0$ with probability $1 - p(x)$, for some function $p(\cdot)$. 
In this case, the decoupled performative risk is given by:
\begin{equation}
\label{eq:simple_dpr}
\begin{aligned}
R(x_1,x_2) &= p(x_2) \left[ \frac{1}{2} |1-x_1|^2 \right] + (1 - p(x_2)) \left[ \frac{1}{2} |x_1|^2 \right]\\
&= \frac{1}{2} [x_1^2 + p(x_2) (1 - 2x_1)]
\end{aligned}
\end{equation}
We will analyze this model in two ways. First, we will consider general $p(\cdot)$, and, fixing the loss function $\ell(\cdot)$, identify a class of decision-dependent distribution shifts $p(\cdot)$ which can still ensure convergence to performative risk minimizers, using Theorem~\ref{th:perf_align}. Second, we will consider a concrete example for $p(\cdot)$, and demonstrate how to apply Theorem~\ref{th:perturb1} to understand the regions of convergence.

As our concrete example of $p(\cdot)$, consider the following function as a candidate for $p(\cdot)$:
\begin{equation}
\label{eq:varphi_def}
\varphi(x) = 
\begin{cases}
\exp\left( 1 + \frac{-1}{1-(x-1)^2} \right) & \text{if } x \in (0,1) \\
0 & \text{if } x \le 0 \\
1 & \text{if } x \ge 1
\end{cases}
\end{equation}
This function is chosen because $\varphi(x) = 1$ for $x \ge 1$, $\varphi(x) = 0$ for $x \le 0$, and it is continuously differentiable. The derivative is:
\begin{equation}
\label{eq:dvarphi_def}
\varphi'(x) = 
\begin{cases}
\varphi(x) \frac{2(1-x)}{(1-(x-1)^2)^2} & \text{if } x \in (0,1) \\
0 & \text{otherwise}
\end{cases}
\end{equation}
$\varphi(\cdot)$ and its derivative is visualized in Figure~\ref{fig:varphi_plt}(a).  Since $p(0) = 0$ and $p(1) = 1$ for this choice of $p(\cdot)$, Equation~\eqref{eq:simple_dpr} directly implies that there are two performative risk minimizers: $x = 0$ and $x = 1$. Similarly, we can see that these points are performatively stable as well. 
The corresponding performative risk and gradients are visualized in Figure~\ref{fig:varphi_plt}(b)--(c).



\begin{figure*}[t!]
  \centering
\includegraphics[width=0.3\textwidth]{./figs/varphi_plt.pdf}
\includegraphics[width=0.3\textwidth]{./figs/pr_varphi.pdf}
\includegraphics[width=0.3\textwidth]{./figs/d_pr_varphi.pdf}
  \caption{As an illustrative example, we consider a setting where with the squared error is used as the loss function, and the decision-dependent distribution shift modifies the parameters of a Bernoulli distribution, as discussed in Section~\ref{sec:simple_ex}. (a) A visualization of an example decision-dependent distribution shift $\varphi(x)$, as defined in Equation~\eqref{eq:varphi_def}, and its derivative, as derived in Equation~\eqref{eq:dvarphi_def}.  (b) The performative risk with $\Pr(Z = 1) = p(x) = \varphi(x)$. (c) The corresponding gradients for $p(x) = \varphi(x)$.}
  \label{fig:varphi_plt}
\end{figure*}

\subsubsection{Classification of adversarial agents}
\label{sec:adv_class}
As we've mentioned, when data-driven algorithms are deployed in real-world settings, it often caused a drift in the distribution of the data. One source of such drift is the behavior of adversarial agents. Here, we consider a simple classification problem of potentially adversarial agents.

Suppose that each agent are defined by a feature $z \in \bbR^d$ and a binary label $y \in \left\{ -1, 1 \right\}$, and
suppose that they are drawn i.i.d from a distribution $\cD_{Z, Y}$.
The task of the classifier is to correctly predict their
labels based on their features.
For simplicity, we further assume that
we are using a linear classifier defined by
\[
\begin{aligned}
    \hat{y}_{x}
    =
    sign(\langle x, z\rangle),
\end{aligned}
\]
where $x \in \bbR^d$ is the parameter of the classifier.
We also assume that the learner is using a logistic loss function:
\[
\begin{aligned}
    \ell(x, y, z)
    =
    \log(1 + \exp(-y\langle x, z\rangle)).
\end{aligned}
\]

So far, we've essentially described a ordinary binary classification problem with linear classifiers.
We further assume that once the classifier is deployed,
the agents will potentially alter their features to
induce false predictions.
Formally, we assume that if an agent is adversarial,
it will produce a fake feature $\hat{z}$ such that
\[
\begin{aligned}
    \hat{z}(x, y, z)
    \in
    \argmin_{z'}
    \left\{
        -y\langle x,z'\rangle
        +
        \frac{k}{2}
        \left\|z' - z\right\|_2^2
    \right\},
\end{aligned}
\]
and that agents are adversarial with probability
\[
\begin{aligned}
    p_{adv}(x, y, z)
    =
    e^{-\lambda_1 \left\|\hat{z}(x, y, z) - z\right\|_2^2}.
\end{aligned}
\]
Essentially, we are implicitly describing a `cost'
of adversarial behavior, which
that is proportional to the squared $l_2$ distance
between true and fake features. If the cost is too high,
an agent will be more likely to abandon adversarial
behavior, hence a lower $p_{adv}$.


This adversarial behavior can be thought of a distribution
shift caused by deploying our classifier, and the performative
risk is then given by
\begin{align*}
    PR(x)
    &=
    \bbE_{(\hat{Z}, Y) \sim \cD(x)}
    \left[ \ell(x, Y, \hat{Z}) \right] \\
    &=
    \bbE_{(Z, Y)\sim\cD_{Z, Y}}
    \big[
        \left.(1 - p_{adv}(x, Y, Z))
        \ell(x, Y, Z) \right.\\
        &\quad+
        p_{adv}(x, Y, Z)
        \ell(x, Y, \hat{Z}(x, Y, Z)
    \big].
\end{align*}


Our goal here is to find a classifier that minimizes
$PR(x)$ without knowing the behavior pattern of
the agents. That is, we are completely unaware of
the dependencies of the performative risk on $x_2$.

To illustrate, consider the simplest case where $d=1$ (i.e. $z, x \in \bbR$).
Specifically, let $\cD_{Z, Y} = \cN(y, 1)$.
The corresponding performative risk and gradients are
illustrated in Figure~\ref{fig:adv_class}(d) and (e).


\begin{figure*}[t!]
  \centering
\includegraphics[width=0.4\textwidth]{./figs/Fig-d.png}
\includegraphics[width=0.4\textwidth]{./figs/Fig-e.png}
  \caption{This is a 1-dimensional illustration of the setting described in 
  Section~\ref{sec:adv_class}. When $(z, y) \sim \cN(y, 1)$, $\lambda_1 = 0.04$ and
  $k = 0.4$, the corresponding performative risk is visualized in (d), and the
  corresponding gradients are visualized in (e).}
  \label{fig:adv_class}
\end{figure*}

In both these examples, there are multiple performative risk minimizers and performatively stable points. Performative risk minimization and repeated gradient descent can converge to different steady-state results, and it is of interest which initializations will converge to which equilibria under both dynamics. 
In the sequel, we shall demonstrate how different functions $p(\cdot)$ can lead to different steady-state outcomes, as well as how our theoretical results can provide conditions on $p(\cdot)$ such that we achieve convergence to performative risk minimizers, even when performing repeated approximate risk minimization.
