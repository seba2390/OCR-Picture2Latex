\section{Methodology}


In this section, we present our novel Semi-Supervised Domain Adaptation approach to learn domain agnostic representation. We will first introduce the background and notations used in our work and then describe our approach and its components in detail.

\subsection{Problem Formulation }
In Semi-Supervised Domain Adaptation, we have datasets sampled from two domains. The source dataset contains labeled images $\mathcal{D}_s =\{(x_i^s, y_i^s)\}_{i=1}^{N_s} \subset \mathcal{R}^d \times \mathcal{Y}$ sampled from some distribution $P_S(X,Y)$. Besides that, we have two sets of data  sampled from target domain distribution $P_T(X,Y)$. We denote the labeled set of images sampled from the target domain as $\mathcal{D}_{lt} =\{(x_i^{lt}, y_i^{lt})\}_{i=1}^{N_{lt}}$ . The unlabeled set sampled from target domain $\mathcal{D}_t =\{(x_i^t)\}_{i=1}^{N_t}$ contains large number of images ($N_t \gg N_{lt}$) without any corresponding labels associated with them. We also denote the labeled data from both domains as $\mathcal{D}_l= \mathcal{D}_s \cup \mathcal{D}_{lt}$. Labels $y_i^s$ and $y_i^{lt}$ of the samples from source and labeled target set correspond to one of the categories of the dataset having $K$ different classes/categories \textit{i.e} $Y=\{1,2,...K\}$. Our goal is to learn a task specific classifier  using $D_s,D_{lt}$ and $D_t$ to accurately predict labels on test data from target domain. 

\subsection{Supervised Training}
Labeled source and target samples are passed through the CNN-based feature extractor $\mathcal{G}(.)$ to obtain corresponding features, which are then passed through task-specific classifier $\mathcal{F}(.)$ to minimize the well-known cross-entropy loss on the labeled images from both source and target domains.

\vspace{-2mm}
\begin{equation}
\label{eq:sup_loss}
\mathcal{L}_{sup} = -\sum\limits_{k=1}^{K}(y^i)_k \log(\mathcal{F}(\mathcal{G}((x^i_l))_k
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/TSCL_final.pdf}
    \vspace{1mm}
    \caption{\textbf{Outline of our CLDA Framework} Our approach consists of aligning the outputs of the neural network at two levels. At the instance level, we try to maximize the similarity between features of unlabeled target images and strongly augmented unlabeled target images using Instance Contrastive Alignment. At the class level, we pass the images from both domains through the network, where we assign the labels to features of unlabeled target images and compute the centroids of each class of the target domain. Similarly, we compute the centroids for source domain features using their class labels. Finally, we maximize the similarity between centroids of the same class across domains by employing Inter-Domain Contrastive Alignment. We also used cross-entropy loss on the labeled source and target images, apart from the above components in our framework.}
    \label{fig:overview}
\end{figure}

\subsection{Inter-Domain Contrastive Alignment}
Our method is based on the observation that the samples from the same category across domains must cluster in the latent space. However, this is observed only for the source domain due to the availability of the labels. Samples from the target domain do not align to form clusters due to the domain shift between the target and the source distributions. This discrepancy between the cluster of the same category across domains is reduced by aligning the centroids of each class of source and target domain. ~\cite{Chen2020ASF, Grill2020BootstrapYO} have shown that having a separate projection space is beneficial for contrastive training. Instead of using a separate projection, we have used the outputs from the task-specific classifier as features to align the clusters across the domain. 

We represent the centroid of the images from the source domain belonging to class $k$  as the mean of their features, which can be written as 
% \vspace{2mm}
\begin{equation}
    C_k^s = \frac{\sum\limits_{i=1}^{i=B}\mathbb{1}_{\{y_i^s=k\}} \mathcal{F}(\mathcal{G}(x_i^s))}{\sum\limits_{i=1}^{i=B}\mathbb{1}_{\{y_i^s=k\}}}
    \label{Eq:mean}
\end{equation}
% \vspace{2mm}
where $B$ is the size of batch. We maintain a memory bank $\big(C^s = [C_1^s,C_2^s,....C_K^s]\big)$ to store the centroids of each class from source domain. We  use exponential moving average to update these centroid values during the training
\vspace{2mm}
\[
    C_k^s = \rho(C_k^s)_{step} + (1-\rho)(C_k^s)_{step-1}
\]

% \vspace{2mm}
where $\rho$ is a momentum term, and $(C_k^s)_{step} $ and $(C_k^s)_{step-1}$ are the centroid values of class $k$ at the current and previous step, respectively.

We also need to cluster the unlabeled target samples for Inter-Domain Contrastive Alignment. The pseudo labels obtained from the task specific classifier as shown in Eq ~\eqref{Eq:pseudo_label} is used as the class labels for the corresponding unlabeled target samples.
\begin{equation}
    \hat{y_i^t} = argmax (\mathcal(\mathcal{F}(\mathcal{G}(x_i^t)))
    \label{Eq:pseudo_label}
\end{equation}

Similar to the source domain , we also calculate the separate cluster centroid $C_k^t$ for each of the class $k$ of the target samples present in the minibatch as per the Eq \eqref{Eq:mean} where unlabeled target images replace the images from the source domain with their corresponding pseudo label.
The model is then trained to maximize the similarity between the cluster representation of each class $k$ from the source and the target domain. $C_k^s$ and $C_k^t$ form the positive pair while the remaining cluster centroids from both domains form the negative pairs. The remaining clusters from both domains are pushed apart in the latent space. This is achieved through employing a modified  NT-Xent  (normalized temperature-scaled cross-entropy) contrastive  loss ~\cite{Chen2020ASF,Oord2018RepresentationLW,Singh_2021_CVPR, Liu2021DomainAF} for domain adaptation given by 

\begin{equation}
\label{eq:inter-domain contrastive alignment}
\hspace{-4mm}
\mathcal{L}_{clu}(C_i^t, C_i^s)= -\log\frac{h\big(C_i^t, C_i^s\big)}{h\big(C_i^t,C_i^s\big)+\sum\limits_{\substack{r=1\\q\in\{s,t\}}}^{K}\mathbb{1}_{\{r \neq i\}}h\big(C_i^t,C_r^q\big)}
\end{equation}
\vspace{2mm}
where $h(\mathbf{u},\mathbf{v}) = \exp\big(\frac{\mathbf{u}^\top \mathbf{v}}{||\mathbf{u}||_2||\mathbf{v||_2}}/\tau\big)$
measures the exponential of cosine similarity  , $\mathbb{1}$ is an indicator function and $\tau$ is the temperature hyperparameter.

\subsection{Instance Contrastive Alignment}

Recent works on contrastive learning ~\cite{He2020MomentumCF,Oord2018RepresentationLW,Chen2020ASF} show encouraging results in single domain settings. ~\cite{Kim2020CrossdomainSL} extends contrastive learning into multi-domain settings. Inspired by such success, we employ Instance Contrastive Learning to form stable and correct cluster cores in the target domain.

To perform contrastive alignment at the instance level, we first generate a strongly augmented version of the unlabeled target image \textit{i.e} $\tilde{x_i^t}= \psi(x_i^t)$ where $\psi(.)$ is the strong augmentation function ~\cite{Cubuk2020RandaugmentPA}. Next, we employ the NT-Xent loss ~\cite{Chen2020ASF, Oord2018RepresentationLW}  as defined in Eq ~\eqref{eq:instance_contrastive_alignment}  to ensure that these two variants of the same image are closer to each other in the latent space while the rest of the images in minibatch of size $B$ are pushed apart. This idea stems from the cluster assumption in an ideal classifier, which states the decision boundary should lie in the low-density region, ensuring consistent prediction for different augmented variants of the same image.
\vspace{2mm}
\begin{equation}
\label{eq:instance_contrastive_alignment}
\hspace{-4mm}
\mathcal{L}_{ins}(\tilde{x}_i^t, x_i^t)\!\!=
\!-\!\!\log\!\frac{h\big(\mathcal{F}(\mathcal{G}(\tilde{x}_i^t), \!\mathcal{F}(\mathcal{G}(x_i^t))\big)}{\sum\limits_{\substack{r=1}}^{B}\!\!h\big(\mathcal{F}(\mathcal{G}(
\tilde{x}_i^t)),\!\mathcal{F}(\mathcal{G}(x_r^t))\!\big)\!+\sum\limits_{\substack{r=1}}^{B}\mathbb{1}_{\{r
\neq i\}}h\big(\mathcal{F}(\mathcal{G}(\tilde{x}_i^t)),\!\mathcal{F}(\mathcal{G}(\tilde{x}_r^t))\!\big)}
\end{equation}
\vspace{2mm}

In SSDA,~\cite{Kim2020AttractPA} has shown that target distribution gets divided into aligned and unaligned subdistribution in the presence of very few labeled target data. Thus, aligning the unaligned subdistribution can lead to improved performance, while perturbing the aligned sub-distribution can result in a negative transfer. Therefore, we only propagate the gradients for strongly augmented images to avoid perturbing the aligned sub-distribution in the target domain.

~\cite{Chen2020ASF} shows stronger augmentation in contrastive learning leads to improved performance. Consistent prediction across the input and strongly augmented unlabeled images in Instance Contrastive Alignment forces the unaligned target sub-distribution to move away from the low-density region towards aligned distribution.  This ensures better clustering in the unlabeled target distribution, which is validated by improved accuracy as shown in Table ~\ref{Ablation:significance} after employing Instance Contrastive Alignment with Inter-Domain Contrastive Alignment.

Both of the components of the CLDA framework are necessary for the improved performance, as shown in Table ~\ref{Ablation:significance} . Instance Contrastive Alignment ensures that unlabeled target samples are consistent and are in the high-density region. However, it does not assure alignment between source and unlabeled target samples. Inter-Domain Contrastive Alignment reduces the discrepancy between unlabeled target samples and source domain but unlabeled target samples closer to the decision boundary might get pushed towards the wrong classes resulting in negative transfer. Thus, combining both components results in a much better alignment of the unlabeled target samples towards the source domain, leading to improved performance of the framework.

\subsection{Overall framework and training objective}
The overall training objective employs supervised loss, Inter-Domain Contrastive Alignment and Instance Contrastive Alignment which can be formulated as follows:
\begin{equation}
    \label{eq:overall}
    \mathcal{L}_{tot} = \mathcal{L}_{sup} + \alpha*\mathcal{L}_{clu} + \beta*\mathcal{L}_{ins}
\end{equation}

We train the model in our framework by employing overall training loss described as in ~\eqref{eq:overall}.

