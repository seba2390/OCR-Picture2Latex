\section{Related Works}
\subsection{Unsupervised Domain Adaptation}
Unsupervised Domain Adaptation (UDA)~\cite{Gopalan2011DomainAF}  is a well-studied problem, and most UDA algorithms reduce the domain gap by matching the features of the sources and target domain ~\cite{Gretton2012AKT, Bousmalis2016DomainSN, Shen2018WassersteinDG, Long2016UnsupervisedDA, Sun2016DeepCC, Kang2018DeepAA}. Feature-based alignment methods reduce the global divergence~\cite{Gretton2012AKT, Sun2016DeepCC} between source and target distribution. Adversarial learning ~\cite{Ganin2015UnsupervisedDA, Chen2019JointDA, Long2015LearningTF, Long2018ConditionalAD, Pei2018MultiAdversarialDA, Paul2020DomainAS} based approaches have shown impressive performance in reducing the divergence between source and target domains. It involves training the model to generate features to deceive the domain classifier, invariantly making the generated features domain agnostic. Recently, Image translation methods ~\cite{Hoffman2018CyCADACA, Hu2018DuplexGA, Murez2018ImageTI} have been explored in UDA where an image from the target domain is translated to the source domain to be treated as an image from the source domain to overcome the divergence present across domains. Despite remarkable progress in UDA, ~\cite{Saito2019SemiSupervisedDA} shows the UDA approaches do not perform well in the SSDA setting, which we consider in this work.


\subsection{Semi-Supervised Learning}
Semi-Supervised Learning(SSL) aims to leverage the vast amount of unlabeled data with limited labeled data to improve classifier performance. The main difference between SSL and SSDA is that SSL uses data sampled from the same distribution while SSDA deals with data sampled from two domains with inherent domain discrepancy.
The current line of work in SSL ~\cite{Sohn2020FixMatchSS, Berthelot2019MixMatchAH, Li2017TripleGA, Dai2017GoodSL} follows consistency-based approaches to reduce the intra-domain gap. Mean teacher ~\cite{Tarvainen2017MeanTA} uses two copies of the same model (student model and teacher model) to ensure consistency across augmented views of the images. Weights of the teacher model are updated as the exponential moving average of the weights of the student model. Mix-Match~\cite{Berthelot2019MixMatchAH} and ReMixMatch ~\cite{Berthelot2020ReMixMatchSL}  use interpolation between labeled and unlabeled data to generate perturbed features. Recently introduced FixMatch ~\cite{Sohn2020FixMatchSS} achieves impressive performance using the confident pseudo labels of the unlabeled samples and treating them as labels for the strongly perturbed samples. However, direct application of SSL in the SSDA setting yields sub-optimal performance as the presumption in the SSL is that distributions of labeled and unlabeled data are identical, which is not the case in SSDA.

\subsection{Contrastive Learning}
Contrastive Learning(CL) has shown impressive performance in self-representation learning~\cite{Chen2020ASF, Bachman2019LearningRB,He2020MomentumCF, Tian2020ContrastiveMC, Oord2018RepresentationLW}. Most contrastive learning methods align the representations of the positive pair (similar images) to be close to each other while making negative pairs apart. In semantic segmentation,~\cite{Liu2021DomainAF} uses patch-wise contrastive learning to reduce the domain divergence by aligning the similar patches across domains. In domain adaptation, contrastive learning ~\cite{Kim2020CrossdomainSL, Kang2019ContrastiveAN} has been applied for alignment at the instance level to learn domain agnostic representations. ~\cite{Kang2019ContrastiveAN, Kim2020CrossdomainSL} use samples from the same class as positive pairs, and samples from different classes are counted as negative pairs.  ~\cite{Kang2019ContrastiveAN}  modifies Maximum Mean Discrepancy (MMD) ~\cite{Gretton2012AKT} loss to be used as a contrastive loss. In contrast to ~\cite{Kim2020CrossdomainSL, Kang2019ContrastiveAN}, our work proposes to use contrastive learning in SSDA setting both at the class and instance level (across perturbed samples of the same image) to learn the semantic structure of the data better.

\subsection{Semi-Supervised Domain Adaptation}

Semi-Supervised Domain Adaptation (SSDA) aims to reduce the discrepancy between the source and target distribution in the presence of limited labeled target samples. ~\cite{Saito2019SemiSupervisedDA} first proposed to align the source and target distributions using adversarial training. ~\cite{Kim2020AttractPA} shows the presence of intra domain discrepancy in the target distribution and introduces a framework to mitigate it. ~\cite{Jiang2020BidirectionalAT} uses consistency alongside multiple adversarial strategies on top of MME ~\cite{Saito2019SemiSupervisedDA}.  ~\cite{Li2020OnlineMF} introduced the meta-learning framework for Semi-Supervised Domain Adaptation. ~\cite{Yang2020DeepCW} breaks down the SSDA problem into two subproblems, namely, SSL in the target domain and UDA problem across the source and target domains, and learn the optimal weights of the network using co-training. ~\cite{Mishra2021SurprisinglySS}  proposed to use pretraining of the feature extractor and consistency across perturbed samples as a simple yet effective strategy for SSDA. ~\cite{Qin2020Contradictory} introduces a framework for SSDA consisting of a shared feature extractor and two classifiers with opposite purposes, which are trained in an alternative fashion; where one classifier tries to cluster the target samples while the other scatter the source samples, so that target features are well aligned with source domain features. Most of the above approaches are based on adversarial training, while our work proposes to use contrastive learning-based feature alignment at the class level and the instance level to reduce discrepancy across domains.
