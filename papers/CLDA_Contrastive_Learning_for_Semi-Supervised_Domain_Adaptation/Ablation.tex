\subsection{Ablation Studies}
\label{ablation}
We perform extensive ablation experiments to analyze our CLDA framework and the effects of the different components and hyperparameters. We perform these experiments on the 3-shot Pr $\rightarrow$ Ar domain adaptation task of the Office-Home dataset using Resnet34 unless specified otherwise. 

\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c|c}
\hline
Augmentation & Test Accuracy( Pr $\rightarrow$ Ar) & Test Accuracy(Rl $\rightarrow$ Ar) \\
\hline
  Horizontal Flipping (Hflip) &  68.1 &73.4\\
  Hflip + Color Jitter & 67.6 &74.9 \\
  Hflip+ Color Jitter + Grayscale &70.2 &76.2 \\
  Rand Augment (RA) ~\cite{Cubuk2020RandaugmentPA} & 71.1 &74.6 \\
  RA + Grayscale & 72.4 & 76.7 \\
  Auto Augment ~\cite{Cubuk2019AutoAugmentLA} &69.9 &75.3\\
  \hline
\end{tabular}}
    \caption{\textbf{Effect of Strong Augmentations} Numbers show the test accuracy on 3-shot domain adaptation tasks of the Office-Home dataset with Resnet34 with different augmentation policies.}
    \label{tab:Augmentation}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Ablation_hyper.pdf}
    \caption{\textbf{Effect of different hyperparameters on 3-shot Pr $\rightarrow$ Ar (Product to Art) data adaptation scenario on the Office-Home using Resnet34.} \textbf{(a)} Effect of varying the weight of Instance Contrastive Alignment on validation and test Accuracy \textbf{(b)} Effect of varying weight of Inter-Domain Contrastive Alignment on validation and test Accuracy \textbf{(c)} Effect of $\mu$, ratio of unlabeled target to labeled target data on validation and test accuracy.}
    \label{fig:hyperparameters}
\end{figure}
\noindent\textbf{Effectiveness of  Individual Modules}: Our CLDA framework is composed of two modules: Inter-Domain Contrastive Alignment and Instance Contrastive Alignment. We investigate the significance of each component of our framework by dropping the other during training. We observe that the test accuracy drops from $72.4\%$ to $68.3\%$ when only Inter-Domain Contrastive Alignment is used, and it drops to $67.7\%$ when Instance Contrastive Alignment is used alone as shown in Table \ref{Ablation:significance}(a). Though individual modules do not yield high performance on their own but once combined, they surpass their individual performance by a margin of around $4\%$.

\begin{table}
\renewcommand{\arraystretch}{1.2}
    \centering
\resizebox{\linewidth}{!}{
\subfloat[Ablation Study on the effectiveness of Individual components of the CLDA framework on Pr $\rightarrow$ Ar adaptation task of the Office-Home dataset using Resnet34.]{
\begin{tabular}{c|c}
\hline
Approach & Test Accuracy\\
\hline
  CLDA w/o Instance Contrastive  &  68.3\\
  CLDA w/o Inter-Domain Contrastive  & 67.7\\
  \hline
  CLDA (ours) & 72.4 \\
  \hline
\end{tabular}}
\quad \quad 
\subfloat[Ablation Study on other consistency based approaches on Pr $\rightarrow$ Ar domain adaptation task of the Office-Home using Resnet34.]{\begin{tabular}{c|c}
\hline
Approach & Test Accuracy\\
\hline
   Fix-Match& 70.8\\
   L1 loss & 69.4\\
   L2 loss & 69.3\\
   \hline
   CLDA (ours) & 72.4 \\
   \hline
\end{tabular}}}
\caption{Experiments to understand the significance of individual components of our framework. }
\label{Ablation:significance}
\end{table}


\noindent\textbf{Effect of Different Hyperparameters}: We analyze the importance of different hyperparameters used in our approach. We observe that the weight of Instance Contrastive Alignment affects the performance of our approach as the test accuracy drops from $ 72.4\%$ to $70.7\%$ when we set $\alpha$ to $1$ instead of its optimal value of $4$ as shown in figure ~\ref{fig:hyperparameters}. We also notice that increasing $\beta$ led to a reduction of the validation and test performance. We also look into the effect of $\mu$, which is the ratio of unlabeled to labeled data in a minibatch. We observe that an increasing value of $\mu$ increases the performance till $\mu=4$, after which it starts to drop, as shown in figure ~\ref{fig:hyperparameters}.

\noindent \textbf{Importance of Instance Contrastive Alignment}: Instance Contrastive Alignment ensures similar representation across different variants of the unlabeled target images. This consistency is also ensured by other well-known SSL approaches like  FixMatch~\cite{Sohn2020FixMatchSS}. We perform an ablation experiment replacing Instance Contrastive Alignment with FixMatch. We also compare with L1 and L2 loss to have a fair analysis. As shown in Table ~\ref{Ablation:significance} (b) Instance Contrastive Alignment helps to achieve superior performance in comparison with other consistency-based approaches. 

\noindent\textbf{Effect of Other Clustering Techniques}: Inter-Domain Contrastive Alignment requires pseudo labels for the unlabeled target data for clustering. In this ablation experiment, we replace our approach of using the model's prediction as a pseudo label with K-means clustering, which we invoke after every 50 steps and use the generated centroids for the next 50 steps to obtain pseudo-class labels for unlabeled target data. We observe a drop in performance (from $72.4\%$ to $71.2\%$) when using K-means to obtain the pseudo label for unlabeled target images. 


\noindent \textbf{ Effect of Augmentation Policy}:
We look into different augmentation policies for the Instance Contrastive Alignment. As suggested in ~\cite{Chen2020ASF}, a stronger augmentation policy for contrastive learning increases the performance of the model. We find that RandAugment ~\cite{Cubuk2020RandaugmentPA} with Grayscale augmentation policy gives better results over other augmentation policies. The influence of the strong augmentation can be observed from  $\sim 4\%$ improvement in the performance when the augmentation policy is switched from horizontal flipping to RandAugment with Grayscale. Table ~\ref{tab:Augmentation} contains the test accuracy of different augmentation policies on $3$-shot Pr $\rightarrow$ Ar and Rl $\rightarrow$ Pr domain adaption tasks of the Office-Home dataset with Resnet34.


\noindent \textbf{ Effect of Noisy-Labeled Target Samples}: In SSDA, we have few labeled samples from the target domain; however, the presence of noisy-labeled target samples can have an adverse effect on the performance. To understand the effect of noisy-labeled target samples on the framework, we conducted experiments on the $1$-shot Pr $\rightarrow$ Ar and Rl $\rightarrow$ Ar domain adaptation scenarios of the Office-Home dataset with Resnet34, where we mislabeled some previously labeled target samples as shown in Table ~\ref{tab:noisy-samples}. We observe a small decrease in performance of our framework ( from $66.2\%$ to $65.7\%$ for Pr $\rightarrow$ Ar and from  $72.6\%$ to$71.56\%$ for Rl $\rightarrow$ Ar) when mislabeled target samples increase from $0\%$ to $\sim 25\%$ in both domain adaptation scenarios showing the robustness of our framework.
\begin{table}[t]
\renewcommand{\arraystretch}{1.2}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
\hline
Experiments &  $0$ samples mislabeled & $8$ samples mislabeled ($\sim 12\%$) & $16$ samples mislabeled ($\sim 25\%$) \\
\hline
  Pr $\rightarrow$ Ar &  66.2 & 66.0 & 65.7\\

  Rl $\rightarrow$ Ar & 72.6 & 72.05 & 71.56 \\
  \hline
\end{tabular}}
    \caption{\textbf{Ablation study to understand the effect of outliers in target domain.} Numbers show the test accuracy of $1$-shot domain adaptation tasks of the Office-Home dataset with Resnet34.}
    \label{tab:noisy-samples}
\end{table}
