\documentclass{article}
\pdfoutput=1
% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%   \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%  \usepackage[preprint,nonatbib]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
      \usepackage[final,nonatbib]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor} 
\usepackage{graphicx}
\usepackage{array,multirow} 
\usepackage{booktabs} % for professional tables
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\usepackage{float}
\usepackage{mathtools} % for splitfrac
% \usepackage[table]{xcolor}
% \floatstyle{plaintop}
\usepackage{bbold}
\restylefloat{table}
\newcommand{\ankit}[1]{{\color{blue}{ [Ankit: #1]}}}
\usepackage{booktabs} % colors
\usepackage[
backend=biber,
maxcitenames=50,
maxnames=50,
style=numeric,
citestyle=numeric
]{biblatex}
\addbibresource{ms.bib}
\usepackage{hyperref}       % hyperlinks
\title{CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation}


\author{%
  Ankit Singh \\
  Department of Computer Science\\
  Indian Institute of Technology, Madras \\
  \texttt{singh.ankit@cse.iitm.ac.in} \\
}

\begin{document}

\maketitle

\begin{abstract}
Unsupervised Domain Adaptation (UDA) aims to align the labeled source distribution with the unlabeled target distribution to obtain domain invariant predictive models. However, the application of well-known UDA approaches does not generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where few labeled samples from the target domain are available.
This paper proposes a simple \textbf{C}ontrastive \textbf{L}earning framework for semi-supervised \textbf{D}omain \textbf{A}daptation (\textbf{CLDA}) that attempts to bridge the intra-domain gap between the labeled and unlabeled target distributions and the inter-domain gap between source and unlabeled target distribution in SSDA. We suggest employing class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment between the original(input image) and strongly augmented unlabeled target images to minimize the intra-domain discrepancy. We have empirically shown that both of these modules complement each other to achieve superior performance. Experiments on three well-known domain adaptation benchmark datasets, namely DomainNet, Office-Home, and Office31, demonstrate the effectiveness of our approach. CLDA achieves state-of-the-art results on all the above datasets.
\end{abstract}

\input{Introduction}

\input{Related}

\input{Method}

\input{Experiment}

\section{Conclusion}
In this work, we present a novel single-stage contrastive learning framework for semi-supervised domain adaptation. The framework consists of Inter-Domain Contrastive Alignment and Instance-Contrastive Alignment, where the former maximizes the similarity between centroids of the same class from both domains and later maximizes the similarity between augmented views of the unlabeled target images. We show that both of the components of the framework are necessary for improved performance. We demonstrate the effectiveness of our approach on three standard domain adaptation benchmark datasets, outperforming the well-known SSDA methods.

\section{Acknowledgments and Disclosure of Funding}
The work is supported by Half-Time Research Assistantship (HTRA) grants from the Ministry of Education, India. We would also like to thank Saurav Chakraborty and  Athira Nambiar for their valuable suggestions and feedback to improve the work.
% \newpage 
\printbibliography
\nocite{*}

\newpage
\appendix
\input{appendix2}

\end{document}
