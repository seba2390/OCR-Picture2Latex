\section{Introduction}

Deep Convolutional networks ~\cite{Krizhevsky2012ImageNetCW,Szegedy2017Inceptionv4IA}  have shown impressive performance in various computer vision tasks, e.g., image classification ~\cite{He2016DeepRL,Huang2017DenselyCC} and action recognition ~\cite{Simonyan2014TwoStreamCN,Ji20133DCN,Wang2019TemporalSN,Lin2019TSMTS}. However, there is an inherent problem of generalizability with deep-learning models, \textit{i.e.}, models trained on one dataset(source domain) does not perform well on another domain. This loss of generalization is due to the presence of domain shift ~\cite{Donahue2014DeCAFAD, Tzeng2017AdversarialDD} across the dataset. Recent works ~\cite{Saito2019SemiSupervisedDA, Kim2020AttractPA} have shown that the presence of few labeled data from the target domain can significantly boost the performance of the convolutional neural network(CNN) based models. 
This observation led to the formulation of Semi-Supervised Domain Adaption (SSDA), which is a variant of Unsupervised Domain Adaptation where we have access to a few labeled samples from the target domain.

Unsupervised domain adaptation methods ~\cite{Pei2018MultiAdversarialDA, Ganin2015UnsupervisedDA, Long2016UnsupervisedDA, Sun2016DeepCC, Long2018ConditionalAD} try to transfer knowledge from the label rich source domain to the unlabeled target domain. Many such existing domain adaptation approaches ~\cite{Pei2018MultiAdversarialDA, Ganin2015UnsupervisedDA, Sun2016DeepCC} align the features of the source distribution with the target distribution without considering the category of the samples.
%  tries to align the global source and target distributions. 
 These class-agnostic methods fail to generate discriminative features when aligning global distributions. 
Recently, owing to the success of contrastive approaches ~\cite{Chen2020ASF,He2020MomentumCF,Oord2018RepresentationLW}, in self-representation learning, some recent works ~\cite{Kang2019ContrastiveAN,Kim2020CrossdomainSL} have turned to instance-based contrastive approaches to reduce discrepancies across domains. 

~\cite{Saito2019SemiSupervisedDA}  reveals that the direct application of the well-known UDA approaches in Semi-Supervised Domain Adaptation yields sub-optimal performance. ~\cite{Kim2020AttractPA} has shown that supervision from labeled source and target samples can only ensure the partial cross-domain feature alignment. This creates aligned and unaligned sub-distributions of the target domain, causing intra-domain discrepancy apart from inter-domain discrepancy in SSDA.
\begin{figure}
   \centering
    \includegraphics[width=\textwidth]{Figures/Main_fig_final.pdf}
    \vspace{2mm}
    \caption{\textbf{Conceptual description of CLDA approach}. \textbf{(a)} Intial distribution of samples from both domain .\textbf{(b)} Instance Contrastive Alignment ensures unlabeled target samples move into the low entropy area forming robust clusters \textbf{(c)} Inter-Domain Contrastive Alignment minimizes the distance between the clusters of same class from both domain \textbf{(d)} The clusters of both domain are well aligned and samples are far away from decision boundary.}
    \label{fig:my_label}
\end{figure}

 In this work, we propose CLDA, a simple single-stage novel contrastive learning framework to address the aforementioned problem. Our framework contains two significant components to learn domain agnostic representation. First, Inter-Domain Contrastive Alignment reduces the discrepancy between centroids of the same class from the source and the target domain while increasing the distance between the class centroids of different classes from both source and target domain. This ensures clusters of the same class from both domains are near each other in latent space than the clusters of the other classes from both domains.
 
 Second, inspired by the success of self-representation learning in semi-supervised  settings~\cite{Grill2020BootstrapYO, Chen2020ASF, Singh_2021_CVPR}, we propose to use Instance Contrastive Alignment to reduce the intra-domain discrepancy. In this, we first generate the augmented views of the unlabeled target images using image augmentation methods. Alignment of the features of the original and augmented images of the unlabeled samples from the target domain ensures that they are closer to each other in latent space.  The alignment between two variants of the same image ensures that the classifier boundary lies in the low-density regions assuring that the feature representations of two variants of the unlabeled target images are similar, which helps to generate better clusters for the target domain.
 
 In summary, our key contributions are as follows.
1) We propose a novel, simple single-stage training framework for Semi-supervised Domain Adaptation.
2)We propose using alignment at class centroids and instance levels to reduce inter and intra domain discrepancies present in SSDA.
3)We evaluate the effectiveness of different augmentation approaches, for instance-based contrastive alignment in the SSDA setting.
 4)We evaluate our approach over three well-known Domain Adaptation datasets (DomainNet, Office-Home, and Office31) to gain insights. Our approach achieves the state of the art results across multiple datasets showing its effectiveness. We perform extensive ablation experiments highlighting the role of different components of our framework.