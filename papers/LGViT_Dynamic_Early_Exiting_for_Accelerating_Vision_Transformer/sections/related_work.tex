\section{Related works}
\label{sc:related}

\textbf{Efficient ViT. }
Due to their considerable computational cost, ViTs are challenging to deploy on resource-constrained edge devices for real-time inference \cite{DBLP:journals/csur/TayDBM23, shen2023efficient}. 
Recently several studies have proposed lightweight architectures to enhance performance.
For example, Mehta \textit{et al.} \cite{mobilevit} incorporate convolution into transformers, combining the strengths of convolution and attention. 
Maaz \textit{et al.} \cite{DBLP:conf/eccv/MaazSCKZAK22} propose an efficient hybrid architecture and design split depth-wise channel groups encoder to increase the receptive field. 
Furthermore, a series of methods employ traditional model compression techniques to obtain compact ViTs, such as network pruning \cite{DBLP:conf/cvpr/Tang00XGXT22, kwon2022fast, DBLP:conf/nips/ZhenglZYTXRP22}, knowledge distillation \cite{deit, hao2022learning} and low-bit quantization \cite{DBLP:conf/mm/DingQYCLWL22, yuan2022ptq4vit}. 
Hao \textit{et al.} \cite{hao2022learning} utilize patch-level information to help compact student models imitate teacher models. 
Kwon \textit{et al.} \cite{kwon2022fast} propose a post-training pruning framework with structured sparsity methods. 
% However, these aforementioned methods mainly focus on elaborately designing compact ViT structure or utilizing model compression methods to compress ViT. 
% Our methods adopt sample-level acceleration for inference via adapting dynamically outputs at a different path according to the confidence of each exit's prediction. 

\noindent
\textbf{Early exiting strategy. }
Early exiting is an effective dynamic inference paradigm that allows confident enough predictions from internal classifiers to exit early. 
Recent research on early exiting can be broadly categorized into two classes: 
\textit{1) Architecture design.} 
Some studies focus on designing advanced backbone networks to balance performance and efficiency. 
For example, Teerapittayanon \textit{et al.} \cite{branchynet} first propose to attach internal classifiers at varying depth in DNNs to accelerate inference. 
Wo≈Çczyk \textit{et al.} \cite{ztw} introduce cascade connections to enhance information flow between internal classifiers and aggregate predictions from multiple internal classifiers to improve performance. 
These methods scarcely consider the design of the exiting head architecture and nearly all utilize a fully connected layer following a pooler as the exiting head. 
Bakhtiarnia \textit{et al.} \cite{ViT-EE} propose to insert additional backbone blocks as early exiting branches into ViT.
% However, their performance is unsatisfactory and cannot achieve efficient inference. 
% Therefore, in this work, we design heterogeneous exiting heads to construct early exiting ViTs and achieve a speed-accuracy trade-off. 
\textit{2) Training scheme.} 
Another line of work designs training schemes to enhance the performance of internal classifiers. 
Liu \textit{et al.} \cite{fastbert} employ self-distillation to help internal classifiers learn the knowledge from the final classifier. 
Xin \textit{et al.} \cite{BERxiT} introduce an alternating training scheme to alternate between two objectives for odd-numbered and even-numbered iterations. \\
% Considering the interference between multiple internal classifiers, Sun \textit{et al.} \cite{metagf} propose meta gradient fusion method to harmoniously train these exits. 
% However, these training schemes are not available for early exiting framework with heterogeneous architectures, which may result in substantial information loss. 
% Hence, in this paper, we propose a novel two-stage training scheme to facilitate distillation between heterogeneous architectures and further improve performance. 

Existing methods for efficient ViT primarily focus on elaborately designing compact ViT structures or applying model compression techniques to compress ViT. 
Our approach adopts sample-level acceleration for inference by dynamically adapting outputs at different paths based on the confidence of each exit's prediction. 
Regarding early exiting strategies, 
the work most related to this paper is CNN-Add-EE and ViT-EE in \cite{ViT-EE}, which use a convolution layer and a transformer encoder as exiting heads, respectively.
However, their performance is unsatisfactory and cannot achieve efficient inference. 
To the best of our knowledge, we first introduce heterogeneous exiting heads to construct early exiting ViTs and achieve an efficiency-accuracy trade-off. 
On top of the aforementioned studies, 
We also propose a novel two-stage training scheme to bride the gap between heterogeneous architectures.

