\clearpage

\section{Appendix}

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.475\textwidth]{src/cka.pdf}
  \end{center}
  \vspace{-9pt}
  \caption{CKA heatmap comparing vanilla EE vs Conv EE and Conv EE vs ResNet50. 
  Conv EE, which utilizes convolution as the exiting head, can learn different feature representations compared to vanilla EE. 
  The layer incorporating convolution in the internal classifier closely resembles the lower half of the ResNet layers. 
  Thus, Conv EE is more effective in capturing feature representations than vanilla EE.} 
  \Description{cka.}
  \label{fig:cka}
  \vspace{-6pt}
\end{figure}

\subsection{Outline}
In this supplementary material, we present a systematic investigation into the effectiveness of early exiting in ViTs. 
Besides, we provide more implementation details and experimental comparisons.
The main content is summarized as follows:
\begin{itemize}
  \item In Appendix \ref{sc:investigation}, we conduct a systematic investigation into the effectiveness of early exiting in ViTs and analyze the issues arising from the vanilla early exiting. 
  Moreover, we obtain two observations: \textit{1)} shallow internal classifiers cannot learn sufficient feature representation; \textit{2)} deep internal classifiers cannot capture target semantic information. 
  \item In Appendix \ref{sc:append_ex}, we perform additional experiments to evaluate our framework. We measure the actual execution time of different methods and compare four representative training schemes with different widely-used exiting heads. 
  Then, we ablate the effect of different exiting position schemes.
\end{itemize}




\subsection{Investigation of Early Exiting in ViT}
\label{sc:investigation}



The early exiting method can halt the forward propagation of neural networks prematurely to provide a speed-accuracy trade-off, 
which has achieved significant performance improvements for CNNs and transformers in NLP. 
However, naively implementing early exiting on ViT may not yield performance gains for internal classifiers. 
% 5-th 69; 10-th 86.8; original 90.8
For instance, the performance of the internal classifier on the fifth and tenth layers decreases by 21.8\% and 4.0\%, respectively, 
compared to the original classifier for ViT-B\cite{vit} on CIFAR-100\cite{cifar}. 
Upon examining recent studies on ViT, we discover that a line of works focus on the integration of convolution and self-attention, 
which demonstrates that convolution operations at shallow layers can introduce additional inductive biases and capture more local information \cite{vit,acmix}.
Another line of works strive to exclusively employ self-attention as basic modules to construct the backbone with numerous layers for various vision tasks 
due to its exceptional capability in handling long-range dependencies \cite{scale_vit,deeper_vit,how_vit_work}. 
However, it still remains unexplored that 
\textit{1)} whether shallow internal classifiers could learn sufficient feature representation 
\textit{2)} and whether deep internal classifiers could capture target semantic information. 
Therefore, we design the following probing experiments to answer these two questions and systematically analyze the working mechanism of early exiting methods in ViT.

\begin{figure}[h]
  \vspace{-5pt}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{src/attention_map.pdf}
  \end{center}
  \vspace{-5pt}
  \caption{
    Comparison of attention maps for early exiting in ViT using either MLP (vanilla EE) or self-attention (Attention EE) as the exiting head. 
    Attention EE methods are more capable of learning target semantic information by incorporating self-attention on deep internal classifiers than vanilla EE.}
  \Description{Attention map.}
  \label{fig:attention}
  \vspace{-8pt}
\end{figure}



  

Regrading the first question, we initially compare the representation similarity of two early exiting (EE) architectures, namely MLP (vanilla EE) and convolution (Conv EE), 
and subsequently assess the similarity between Conv EE and ResNet. 
We employ centered kernel alignment (CKA) \cite{cka} as the similarity metric, which facilitates quantitative comparisons of representations similarities within and across neural networks.
It is important to note that we compare their representation similarities using outputs from all internal classifiers and intermediate layers. 
The results are evaluated on the ViT-B/16 backbone using CIFAR-100 \cite{cifar}. 
We only utilize standard $3\times3$ convolution in Conv EE for fair comparison.
Figure \ref{fig:cka} displays the CKA similarity results as heatmaps, with the x and y axes indexing the layers from input to output. 
The layers attached MLP or convolution are marked with green boxes. 
Lighter colors in the heatmap indicate a higher representation similarity between the corresponding layers. 
We observe that the layer at which the internal classifier is attached in vanilla EE differs from that in Conv EE, 
suggesting that they extract distinct information, as depicted in Figure \ref{fig:cka} (a).
The layer where the convolution exiting architecture is positioned exhibits high similarity to ResNet, as shown in Figure \ref{fig:cka} (b).
Consequently, the convolution exiting architecture assists ViT in capturing local information and strengthening feature representations, 
allowing Conv EE to learn representations akin to those of ResNet. 



Concerning the second question, we compare the ability to extract target semantic information between two different early exiting architectures, 
namely vanilla EE and self-attention (attention EE) at different exit positions. 
We compute the attention map and visualize it upon the input image following \cite{visualize}. 
The attention map highlights target pixels of the image that contribute to the dominance of the predicted label, 
enabling the analysis of the efficacy of semantic information extraction from the input space.
We employ DeiT-B \cite{deit} as the backbone, which comprises twelve layers in total. 
The attention map results for the third, sixth, and ninth layers are presented in Figure \ref{fig:attention}. 
It becomes more evident that the eye and beak of the bird are target parts for object identification using the attention EE method than the vanilla EE method, 
especially on the deeper layer, such as the ninth layer.
Since the attention EE method employs self-attention as the exiting architecture, the ability to extract semantic features and spatial relationships can be further enhanced. 
As a result, incorporating self-attention on deep layers can help learn more semantic representations and capture richer global information than the vanilla EE method.





Based on the aforementioned analyses, we obtain the following observations: 
\begin{itemize}
    \item \textbf{\textit{Observation 1}:} Shallow internal classifiers cannot learn sufficient feature representation. 
    \item \textbf{\textit{Observation 2}:} Deep internal classifiers cannot capture target semantic information. 
  \end{itemize}

\textbf{Insight.} 
We identify the primary reason for the poor performance resulting from directly applying the early exiting strategy in ViT. 
Furthermore, we discover that integrating convolution on shallow internal classifiers can enhance local information exploration, 
while incorporating self-attention on deep layers can improve the ability to obtain global information. 
Consequently, if both convolution and self-attention are employed as exiting architectures, positioned on shallow and deep layers respectively, 
the model would gain access to a more comprehensive combination of local and global information compared to using only MLP as the exiting architecture. 









\subsection{Additional Experiments}
\label{sc:append_ex}

In this section, we evaluate the actual execution time of our method and compare four representative training schemes with different widely-used exiting heads. 
Moreover, we ablate the influences of different exiting position schemes. 

\textbf{Execution time.} We design experiments to measure the actual execution time with batch 1 on a RTX 3090 GPU. 
For each method, we run it for once as a warm-up and then record the execution time with 50 runs without break for the whole testing set of CIFAR-100. 
The results are shown in Table \ref{tb:time}. We can find that our method achieves the highest accuracy, the lowest running time and lowest computation cost. 

\begin{table}[!t]
  \centering
  \setlength{\extrarowheight}{0pt}
  \addtolength{\extrarowheight}{\aboverulesep}
  \addtolength{\extrarowheight}{\belowrulesep}
  \setlength{\aboverulesep}{0pt}
  \setlength{\belowrulesep}{0pt}
  \caption{Comparison of execution time on a RTX 3090 GPU.}
  \label{tb:time}
  \vspace{-6pt}
  \begin{tabular}{cccc} 
  \toprule
  \textbf{Method}                                 & \textbf{MACs}   & \textbf{Acc.}     & \textbf{Execution time}         \\ 
  \hline
  \textbf{ViT-B}                                  & 16.93 G         & 90.8 \%           & 6.49 ($\pm0.11$) ms            \\
  SDN                                             & 10.16 G         & 86.5 \%           & 5.65 ($\pm0.23$) ms            \\
  PABEE                                           & 11.48 G         & 85.1 \%           & 5.86 ($\pm0.09$) ms            \\
  PCEE                                            & 10.90 G         & 86.1 \%           & 6.70 ($\pm0.24$) ms            \\
  BERxiT                                          & 10.27 G         & 87.1 \%           & 5.75 ($\pm0.13$) ms~           \\
  ViT-EE                                          & 11.65 G         & 87.5 \%          & 5.35 ($\pm0.14$) ms~           \\
  \rowcolor[rgb]{0.949,0.949,0.949} \textbf{Ours} & \textbf{9.76 G} & \textbf{88.5} \% & \textbf{5.03} ($\pm0.16$) ms~  \\
  \bottomrule
  \end{tabular}
  \vspace{-5pt}
  \end{table}


  \begin{table}[t]
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \caption{Comparison of different training schemes with widely-used exiting heads.}
    \label{tb:train_head}
    \vspace{-6pt}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Exiting heads}                      & \textbf{Training schemes} & \textbf{Acc.}     & \textbf{Speed-Up}       \\ 
    \hline
    MLP                                         & Normal                    & 87.3 \%           & 1.56~$\times$           \\
    MLP                                         & Weighted                  & 88.2 \%           & 1.53~$\times$~          \\
    MLP                                         & Distillation              & 87.1 \%           & 1.57~$\times$           \\
    MLP                                         & Alternating               & ~88.1 \%          & 1.54~$\times$~          \\
    \rowcolor[rgb]{0.949,0.949,0.949} MLP       & \textbf{Ours}             & ~\textbf{88.2}~\% & \textbf{1.57~}$\times$  \\ 
    \hline
    Conv                                        & Normal                    & 85.2 \%           & 1.69~$\times$           \\
    Conv                                        & Weighted                  & 86.4 \%           & 1.65~$\times$~          \\
    Conv                                        & Distillation              & 84.9 \%           & 1.64~$\times$           \\
    Conv                                        & Alternating               & ~86.7 \%          & 1.67~$\times$~          \\
    \rowcolor[rgb]{0.949,0.949,0.949} Conv      & \textbf{Ours}             & ~\textbf{87.5}~\% & \textbf{1.72~}$\times$  \\ 
    \hline
    Attention                                   & Normal                    & 86.8 \%           & 1.54~$\times$           \\
    Attention                                   & Weighted                  & 87.8 \%           & 1.53~$\times$~          \\
    Attention                                   & Distillation              & 87.0 \%           & 1.57~$\times$           \\
    Attention                                   & Alternating               & ~87.5 \%          & 1.54~$\times$~          \\
    \rowcolor[rgb]{0.949,0.949,0.949} Attention & \textbf{Ours}             & ~\textbf{88.0}~\% & \textbf{1.58~}$\times$  \\ 
    \hline
    Ours                                        & Normal                    & 86.9 \%           & 1.82~$\times$           \\
    Ours                                        & Weighted                  & 87.4 \%           & 1.80~$\times$~          \\
    Ours                                        & Distillation              & 86.9 \%           & 1.83~$\times$           \\
    Ours                                        & Alternating               & ~87.9 \%          & 1.84~$\times$~          \\
    \rowcolor[rgb]{0.949,0.949,0.949} Ours      & \textbf{Ours}             & ~\textbf{88.5}~\% & \textbf{1.87~}$\times$  \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \end{table}

\textbf{Exiting heads \& training schemes.} We compare four representative training schemes with different widely-used exiting heads for the ViT backbone on CIFAR-100. 
The results of different training schemes with MLP, Conv, Attention and the proposed exiting heads are shown in Table \ref{tb:train_head}. 
We observe that our training scheme can improve classification accuracy and accelerate inference speed with different exiting heads. 



  \begin{table}[h]
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \caption{Comparison of different exiting positions on CIFAR-100. }
    \label{tb:position}
    \vspace{-6pt}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Position scheme}                                  & \textbf{Exiting position} & \textbf{Acc.}     & \textbf{Speed-Up}       \\ 
    \hline
    Shallow                                                   & \{2,3,4,5\}               & 85.3 \%           & 1.76~$\times$           \\
    Deep                                                      & \{8,9,10,11\}             & 87.9 \%           & 1.46~$\times$~          \\
    Middle                                                    & \{6,7,8,9\}               & 87.5 \%          & 1.74~$\times$~          \\
    \rowcolor[rgb]{0.949,0.949,0.949} Uniform (\textbf{Ours}) & \{4,6,8,10\}              & \textbf{88.4}~\% & \textbf{1.81~}$\times$  \\
    \bottomrule
    \end{tabular}
    \vspace{-2pt}
    \end{table}

\textbf{Exiting position.} In this work, all exiting heads are positioned across all layers uniformly according to an approximately equidistant computational distribution, i.e., 
the multiply-accumulate operations (MACs) of intermediate blocks between two adjacent exiting points remain consistent. 
We carefully ablate the influences of different exiting position schemes, including positioning at shallow, deep, and middle layers. 
The results of different exiting positions for the ViT backbone on CIFAR-100 dataset are shown in Table \ref{tb:position}. 

