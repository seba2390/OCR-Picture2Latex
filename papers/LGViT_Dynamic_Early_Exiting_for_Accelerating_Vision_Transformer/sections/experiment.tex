\section{Experiment}
\label{sc:ex}

In this section, we first introduce some implementation details and experimental settings. 
Then, we present the results of performance evaluations on three vision datasets and three popular ViT backbones. 
Finally, we conduct extensive ablation experiments to demonstrate the superiority of our methods. 

\subsection{Experimental Setup}

\textbf{Datasets.}
We evaluate our proposed method on three public vision datasets: 
CIFAR-100\cite{cifar100}, Food-101\cite{food101}, and ImageNet-1K\cite{imagenet}. 
The CIFAR-100 dataset contains 50K training images and 10K testing images, uniformly categorized into 100 classes. 
The Food-101 dataset consists of 101 food categories, with 750 training and 250 test images per category, making a total of 101K images. 
The ImageNet-1K dataset spans 1000 object classes and contains 1,281,167 training images, 50K validation images and 100K test images. 
We augment the training data with random crops, random horizontal flips and normalization, while the testing data is augmented with center crops and normalization.

\noindent
\textbf{Backbones.}
The proposed framework can be applied to a range of early-exit ViTs. Without loss of generality, 
we conduct experiments with three well-known ViT backbones, namely, ViT\cite{vit}, DeiT\cite{deit}, and Swin\cite{swin}. 
ViT is the first pure transformer structure for computer vision tasks and utilize a $[CLS]$ token to serve as the image representation for classification tasks. 
DeiT adds an additional distillation token to learn hard labels from the teacher model compared to ViT. 
Swin builds hierarchical feature maps by merging image patches in deeper layers.
The $[CLS]$ token in LPH and GAH is replaced by the encoder output because it contains no $[CLS]$ token.


\noindent
\textbf{Baselines.}
We compare our dynamic early exiting methods with several representative early exiting methods. 
Considering most methods designed for CNNs and transformers in NLP, we transfer these methods to ViT for fair comparison. 
\begin{itemize}
    \item \textbf{SDN} \cite{SDN} utilizes a weighted training strategy and employs a confidence-based criterion to decide whether to exit. 
    \item \textbf{PABEE} \cite{PABEE} employs a patience metric based on the consistency of classification decisions over several internal classifiers to make early exiting decisions. 
    \item \textbf{BERxiT} \cite{BERxiT} introduces an alternating training strategy to train the whole model. 
    \item \textbf{ViT-EE} \cite{ViT-EE} utilizes a ViT encoder layer as its exiting head with the confidence criterion to decide whether to exit. 
    \item \textbf{PCEE} \cite{PCEE} utilizes a patience\&confidence criterion according to the enough number of confident predictions from consecutive internal classifiers. 
    
  \end{itemize}
Unless otherwise specified, the default exit architecture of baselines is a single fully connected layer that follows a pooler.

\noindent
\textbf{Evaluation metrics.}
Considering the trade-off between performance and efficiency, we employ Top-1 classification accuracy and speed-up ratio as the performance and efficiency metric, respectively. 
Since the measurement of runtime might not be stable, we follow \cite{deebert} to calculate the speed-up ratio by comparing the actually executed layers in forward propagation and the complete layers. 
For an $L$-layer ViT, the speed-up ratio is defined as:
\begin{equation}
  \text{Speed-up}=\frac{\sum_{i = 1}^{L}L\times m^i  }{\sum_{i = 1}^{L}i\times m^i}, 
\end{equation}
where $m^i$ is the number of samples that exit at the $i$-th layer of ViT. 
For clarity, we utilize the average multiply-accumulate operations (MACs) performed across the entire test dataset as a metric to assess the computational cost associated with a given model.


\noindent
\textbf{Implementation details.}
Our framework and all the compared methods are implemented using the Huggingface transformer library \cite{huggingface} for fair comparison. 
Most hyperparameters, such as learning rate, optimizer, and dropout probabilities are kept unchanged from the original backbones for fair comparison. 
We list different hyperparameters in the Appendix due to limited space. 
% During train, 
% all models are trained using AdamW optimizer. We use the cosine scheduler to adjust the learning rate. 
Each network is fine-tuned by 100 epochs on 3 NVIDIA 3090 GPUs, with a batch size of 64. 
There is no early stopping and the checkpoint after full fine-tuning is chosen. 

\subsection{Performance Evaluation}
\label{sc:peformance}


We conduct extensive experiments to compare our methods with the state-of-the-art methods for three ViT backbones on three vision datasets. 
Then we present the performance and efficient trade-off compared with other baselines. 


\textbf{Comparison with the state-of-the-art.}
We compare the performance between our methods and baselines on CIFAR-100, Food-101 and Tiny ImageNet datasets when different backbones are adopted, including ViT, DeiT, and Swin.
The results are shown in Table \ref{tb:performance}. 
The original models for different backbones are ViT-L/16, DeiT-B and Swin-B respectively. 
We can find that our method can achieve approximately a 1.8 $\times$ speed-up ratio with only a 2\% accuracy drop compared to the original models on most datasets, 
which significantly outperforms other baselines. 

\textbf{Performance and efficiency trade-off. }
To further verify the robustness and efficiency of our method, we visualize the performance and efficiency trade-off curves in Figure \ref{fig:trade-off} on CIFAR-100 test set. 
The original backbone model is ViT-B/16. We compare five competitive baselines in the dynamic inference scenario. 
We can see that the performance of most early exiting methods drops dramatically when the speed-up ratio increases. 
This also reflects directly applying early exiting methods in ViT leads to unstable performance which cannot meet the requirements of real-time systems. 
However, our method is more robust to the variance of speed-up. 
If we set the almost same speed-up ratio, the accuracy drop of our method is 3.6 \% lower than SDN method. 
When the accuracy is approximate to other baselines, our method can achieve faster speed-up. 
Moreover, our method can dynamically adjust the speed-up ratio without retraining, which is more feasible and friendly. 

\begin{table}
  \renewcommand\arraystretch{0.9}
  \centering
  \caption{Ablation study results of main components. The "$\checkmark$" mark indicating that we adopt the corresponding component. 
  The opposite of LPH and GAH is fully connected layer with a pooler. The opposite of two-stage training is vanilla training. }
  \label{tb:ablation}
  \vspace{-6pt}
  \begin{tabular}{ccccc} 
    \toprule
    \textbf{LPH} & \textbf{GAH} & \textbf{Two-Stage training} & \textbf{Acc.} & \textbf{Speed-up}  \\ 
    \hline
    $\times$    &  $\times$    &      $\times$              &   87.3 \%           &   1.56 ~$\times$                \\
    $\times$     & $\times$     &     $\checkmark$                 &   88.2 \%           &   1.57~$\times$                 \\
    $\times$     &  $\checkmark$     &   $\checkmark$              &    88.3 \%          &   1.71~$\times$                  \\
    $\checkmark$    &  $\checkmark$  &     $\checkmark$             &   \textbf{88.5} \%           &   \textbf{1.87}~$\times$                 \\
    \bottomrule
    \end{tabular}
    \vspace{-6pt}
  \end{table}

  \begin{table}
    \renewcommand\arraystretch{0.9}
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \caption{Comparison of different exiting head architectures on CIFAR-100. 
    MLP, Conv and Attention refers to utilizing a fully-connected layer, a $3\times 3$ standard convolution layer, and a MHSA block respectively. }
    \label{tb:head}
    \vspace{-6pt}
    \begin{tabular}{ccccc} 
      \toprule
      \textbf{Exiting head}                           & \textbf{\#Params.} & \textbf{MACs} & \textbf{Acc.} & \textbf{Speed-up}  \\ 
      \hline
      MLP \cite{deebert}                                            & 91 M         & 10.78 G              &   88.2 \%               &  1.57~$\times$                    \\
      Conv \cite{ztw}                                           &   129 M         &  13.86 G             & 87.5 \%                  &  1.72~$\times$                  \\
      Attention \cite{ViT-EE}                                      & 105 M          & 12.76 G              &  88.0 \%                & 1.58~$\times$                     \\
      \rowcolor[rgb]{0.949,0.949,0.949} \textbf{Ours} &    101 M        & \textbf{9.76 G}              &  \textbf{88.5} \%            &  \textbf{1.87}~$\times$                    \\
      \bottomrule
      \end{tabular}
      \vspace{-6pt}
    \end{table}

\subsection{Ablation Study}
To fully understand the impact of each part of the proposed framework, we conduct ablation study, 
where all experiments are evaluated on CIFAR-100 and utilize ViT as the backbone. 
We first study the effectiveness of the main components, and then analyze the impact of different exiting head architectures and training schemes. 
Finally, we verify the robustness of our methods for different numbers of predefined exiting heads. 

\textbf{Ablation of main components.} 
We design experiments to verify the effectiveness of the proposed LPH, GAH and two-stage training scheme. 
Table \ref{tb:ablation} presents the accuracy and speed-up ratio utilizing different components. 
The results show that for early exiting in ViT, 
heterogeneous exiting heads and two-stage training scheme are both significant. 
Specifically, we can observe that the two-stage training scheme can significantly improve accuracy. 
Moreover, when combined with LPH and GAH, the inference efficiency and accuracy can be further improved. 


\textbf{The architecture of exiting heads.} 
In order to verify the effectiveness of the proposed heterogeneous exiting heads, we compare other competitive architectures using the same two-stage training scheme, as shown in Table \ref{tb:head}. 
We can observe that the proposed heterogeneous exiting heads are crucial to achieve a speed-accuracy trade-off. 
Although utilizing MLP as exiting heads can gain approximate accuracy to ours, the speed-up ratio is low. 
The attention method can achieve a close trade-off between accuracy and speed but with high storage and computation requirements. 



\begin{table}
  \renewcommand\arraystretch{0.9}
  \centering
  \setlength{\extrarowheight}{0pt}
  \addtolength{\extrarowheight}{\aboverulesep}
  \addtolength{\extrarowheight}{\belowrulesep}
  \setlength{\aboverulesep}{0pt}
  \setlength{\belowrulesep}{0pt}
  \caption{Comparison of different training schemes with the proposed exiting heads on CIFAR-100. }
  \label{tb:train}
  \vspace{-6pt}
  \begin{tabular}{ccc} 
  \toprule
  \textbf{Training scheme}                        & \textbf{Accuracy} & \textbf{Speed-up}  \\ 
  \hline
  Normal \cite{branchynet}                                         & 86.9 \%              & 1.82 $\times$                   \\
  Weighted \cite{SDN}                                       & 87.4 \%           & 1.80 $\times$                   \\
  Distillation \cite{fastbert}                                   & 86.9 \%           & 1.83 $\times$                   \\
  Alternating \cite{BERxiT}                                    & 87.9 \%           & 1.84 $\times$                  \\
  \rowcolor[rgb]{0.949,0.949,0.949} \textbf{Ours} & \textbf{88.5} \%           & \textbf{1.87} $\times$                   \\
  \bottomrule
  \end{tabular}
  \vspace{-6pt}
  \end{table}

  \begin{figure}
    \begin{center}
      \includegraphics[width=0.4\textwidth, height=0.32\textwidth]{src/head_num.pdf}
    \end{center}
    \vspace{-8pt}
    \caption{Results of accuracy and speed-up for different head number settings. The exiting heads are placed across all layers uniformly. }
    \Description{.}
    \label{fig:head_num}
    \vspace{-15pt}
  \end{figure}

\textbf{Training schemes.} 
We compare our methods with four representative training schemes in early exiting methods, and they all utilize the proposed heterogeneous exiting heads. 
The accuracy and speed-up are shown in Table \ref{tb:train}. 
Our method achieves the highest classification accuracy and inference speed-up ratio, which significantly outperforms other training schemes. 

    
  \textbf{The number of exiting heads.} 
  We analyze the influence on accuracy and speed-up when changing the number of predefined exiting heads, as shown in Figure \ref{fig:head_num}. 
  As the number of heads increases, the accuracy remains essentially consistent with only approximate 2 \% accuracy drop, 
  which shows that our method is robust to the number of heads and can tackle the overthinking problem \cite{SDN}. 
  Moreover, we can observe that the speed-up ratio can enhance with the increasing number of heads. 
  
  
