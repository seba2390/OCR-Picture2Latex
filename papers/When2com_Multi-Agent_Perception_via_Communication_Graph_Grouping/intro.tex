%%%%%%%%% BODY TEXT
\section{Introduction}

Remarkable progress has been achieved for single-agent perception and recognition, where one or more sensor modalities are used to perform object detection~\cite{redmon2016you,Redmon_2017_CVPR,Lin_2017_ICCV} and segmentation~\cite{chen2018encoder,He_2017_ICCV,kirillov2019panoptic}, depth estimation~\cite{godard2017unsupervised,Zhou_2017_CVPR,godard2019digging}, and various other scene understanding tasks. 
However, in many applications, such as robotics, there may be multiple agents distributed in the environment, each of which has local sensors. 
Such multi-agent systems are advantageous in many cases, for example, to increase coverage across the environment or to improve robustness to failures. 
\input{figure/overview.tex}

Thus, we tackle the problem of \textit{multi-agent collaborative perception}, an under-studied topic in the literature, where multiple agents are able to exchange information to improve overall accuracy towards perception tasks (\textit{e.g.}, semantic segmentation or object recognition).
One major challenge for multi-agent collaborative perception is the transmission bandwidth, as high bandwidth results in network congestion and latency in the agent network.
We therefore investigate the scenario where information across all agents (and hence sensors) is not available in a centralized manner, and agents can only communicate through bandwidth-limited channels. 
We also consider several challenging scenarios where some sensor data may be uninformative or degraded. 

Prior works on learning to communicate~\cite{sukhbaatar2016learning,foerster2016learning} mainly address decision-making tasks (rather than for improving perception) under simple perceptual environments. 
In addition, these methods also do not consider bandwidth limitations: They learn to communicate across a fully-connected graph (i.e. all agents communicate with each other through broadcasts).
Such methods cannot scale as the number of agents increases. 
Similarly, since all information is broadcast there is no decision of \textit{when} to communicate conditioned on the need. 
An agent does not need to consume bandwidth when the local observation is sufficient for the prediction. 
When messages sent by other agents are degraded or irrelevant, communication thus could be detrimental to the perception task.   

In this paper, we propose a learning-based communication model for collaborative perception.
We specifically view the problem as learning to construct the communication group (i.e. each agent decides what to transmit and which agent(s) to communicate with) and to decide when to communicate without explicit supervision for such decisions during training. 
In contrast to broadcast-based methods (e.g. TarMac~\cite{das2019tarmac}) and inspired by the general attention mechanisms, our method decouples the stages of communication and this allows for \textit{asymmetric message and key sizes}, reducing the amount of transmitted data.

Our method can be generalized to several downstream vision tasks, including multi-agent collaborative semantic segmentation (dense prediction) and multi-agent 3D shape recognition (global prediction). 
Our model is able to be trained in an end-to-end manner with only supervision from downstream tasks (e.g. ground-truth masks for segmentation and class labels for image recognition) and without the need for explicit ground-truth communication labels. 

We demonstrate across different tasks that our method can perform favorably against previous works on learning to communicate while using less bandwidth. 
We provide extensive analyses, including trade-offs between message and query sizes, the correlation between ground-truth key and predicted message, and visualization of the learned communication groups.

Our contributions are listed as follows:
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex,labelindent=0.0em,labelsep=0.2cm,leftmargin=*]
\item We address the under-explored area of collaborative perception, which is at the intersection of perception, multi-agent systems, and communication.
\item We propose a unified framework that learns both how to construct communication groups and when to communicate. It does not require ground truth communication labels during training, and it can dynamically reduce bandwidth during inference.
\item Our model can be generalized to several down-stream tasks, and we show through rigorous experimentation that it can perform better when compared with previous works investigating learned communication.
\item  We provide a collaborative multi-agent semantic segmentation dataset, AirSim-MAP, where each agent has its own depth, pose, RBG images, and semantic segmentation masks. This dataset allows researchers to further investigate solutions to multi-agent perception. 
\end{itemize}