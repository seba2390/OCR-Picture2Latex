\subsection{Baselines and Evaluation Metrics}
Here we consider several fully-connected (FC) and distributed communication (DistCom) models as our baselines. FC models fuse all of the agents' observations (either weighted or unweighted) whereas DistCom models only fuse a subset of those observations.

\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex,labelindent=0.0em,labelsep=0.2cm,leftmargin=*]

\item \textit{CatAll} \textbf{(FC)} is a naive FC model baseline which concatenates the encoded image features of all agents prior to subsequent network stages.

\item \textit{Auxiliary-View Attention (AuxAttend;\textbf{FC})} uses an attention mechanism to weight auxiliary views from the supporting agents. 

\item  \textit{RandCom} \textbf{(DistCom)} is a naive distributed baseline which randomly selects one of other agents as a supporting agent. 

\item \textit{Who2com}~\cite{liu2020who2com} \textbf{(DistCom)} excludes self-attention mechanism such that it always communicates with one of the supporting agents. 

\item \textit{OccDeg and AllNorm} are baselines that employ no communication, i.e. each agent (view) independently computes the output for itself. For \textit{OccDeg} the data is degraded similarly as before, while in \textit{AllNorm} we use clean images for all views. These two serve as an upper and lower reference for comparison. 
\end{itemize}

We also consider communication modules of \textit{CommNet}~\cite{sukhbaatar2016learning}, \textit{VAIN}~\cite{hoshen2017vain}, and \textit{TarMac}~\cite{das2019tarmac} as our baseline methods for all multiple-outputs tasks. 
For a fair comparison, we use ResNet18~\cite{he2016deep} as the feature backbone for our and all mentioned  baseline models. For the 3D recognition task, we also add MVCNN~\cite{wu20153d} as a baseline.

We evaluate the performance of all the models with mean IoU on the segmentation task and prediction accuracy on the 3D shape recognition task. In addition, we report Bandwidth of all FC and DistCom models in Megabyte per frame (MBpf). To obtain MBpf, We add the size of the feature vectors which need to be transmitted to the requesters and size of keys broadcast to all supporters and multiply by the number of bytes required for storage.
