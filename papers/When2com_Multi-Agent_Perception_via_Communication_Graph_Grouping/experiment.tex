\section{Experiment}
We evaluate the performance of our proposed framework on two distinct perception tasks: collaborative semantic segmentation and multi-view 3D shape recognition.  
% The first task is divided into three cases.  All experimental cases are summarized in Figure ~\ref{fig:experimental_cases}.
% We will first introduce the datasets for both tasks and then briefly describe the baseline methods.  


\input{4.1_setting.tex}
\input{figure/when2com_exp.tex}

\input{4.2_baseline.tex}
\input{table/3dshape_rec.tex}
\input{figure/vis_graph.tex}

\input{4.3_quan_result.tex}
\input{figure/msg_size.tex}

\input{4.4_analyses.tex}
\input{figure/effect_hcom.tex}


% \textbf{CommNet}~\cite{sukhbaatar2016learning}  uses a simple average pooling mechanism on every transmitted features collected from other agents. This approach assumes that all features from other agents are equally important for the final prediction. 

% \textbf{VAIN}~\cite{hoshen2017vain} learns a identity vector for each agent and uses Euclidean distance to compute the similarity scores across agents. It further weight the feature based on the similarity scores for prediction. Note that self-attention is intentionally left out in the original paper.

% \textbf{TarMac}~\cite{das2019tarmac} uses a dot-product attention mechanism which requires the size of key and message to be the identical. We experimentally found that this is a bottleneck in band-width limited communication. In fact, we will show that there is a asymmetric relationship between the key and message size and it is important to take this into account to achieve good performance while maintain low transmission bandwidth. 


% . It does not have self-attention, i.e., the model selectively appends weighted features from the other agents to its own feature based on the proposed 3-way communication. 
% Therefore, it lacks the ability to suppress noisy degraded view and the flexibility to not append any other features if they would introduce additional noise. 



















% - Always communication model (cite VAIN, ComNet) does not perform better than the when2com model. 

% - Knowing how to properly use own observation for prediction or use the information for prediction is important for prediction. 

% - Random selection is even worse than single noise model
% This is because the message is not always beneficial and sometimes will distract the model for its prediction. 

% - A naive way to determine when2com is to based on own observations to determine whether to communicate. 



% - CatAll(cite two body)
%   *centralized: waste the bandwidth
%   *cannot scale up as the number of agents increases
  
 
  
% \subsection{Analyses}
% - Always com v.s. when2com
% - Message size 
% - Key size
% - Attention masks


% \subsection{Supervision reduction}

% train with only one ground-truth segmentation mask