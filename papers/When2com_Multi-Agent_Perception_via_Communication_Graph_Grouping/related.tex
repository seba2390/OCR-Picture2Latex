\section{Related works}
\input{figure/com_diff.tex}
\textbf{Learning to communicate.}
Communication is an essential component for effective collaboration, especially for multi-agent decision-making and perception tasks.  
Early works~\cite{tan1993multi,melo2011querypomdp} facilitated information flow and collaboration through pre-defined communication protocols. Similarly, auction-based methods~\cite{li2010auction,qureshi2008smart} for camera grouping use several assumptions (e.g., static cameras) and heuristic rules to decide the agentsâ€™ communication. 
However, such rigid protocols do not evolve with dynamic environments and do not easily generalize to complex environments.  
Thus, in recent years, several multi-agent reinforcement learning (MARL) works have explored learn-able interactions between agents.  
For example, assuming full cooperation across agents, each agent in CommNet~\cite{sukhbaatar2016learning} broadcasts its hidden state to a shared communication channel so that other agents can decide their actions based on this integrated information.  
A similar scheme was proposed by Foerster~\textit{et al.}~\cite{foerster2016learning}, where agents instead communicate via learned, discrete signals.  
To further leverage the interactions between agents, Battaglia~\textit{et al.}~\cite{battaglia2016interaction} and Hoshen~\cite{hoshen2017vain} integrate kernel functions into CommNet.  
Additionally, several works have addressed communication through recurrent neural networks (RNN). 
For example, DIAL~\cite{foerster2016dial} uses an RNN to derive the individual Q-value of each agent based on its observation and the messages from other agents.
BiCNet~\cite{peng2017multiagent} connects all agents with a Bi-directional LSTM to integrate agent-specific information, and ATOC~\cite{jiang2018learning} additionally applies an attention unit to determine what to broadcast to the shared channel.  
Although substantial progress has been made by several MARL works, most experimental tasks are built on simplistic 2D-grid environments where each agent observes low-dimensional 2D images. 
As mentioned in Jain~\textit{et al.}~\cite{jain2019two}, studying agents' interactions in simplistic environments does not permit study of the interplay between perception and communication. 

\input{figure/com_module.tex}
Recent works have proposed to construct communication groups based on pre-defined rules~\cite{jiang2018learning,jiang2018graph} or a unified communication network~\cite{singh2018learning,sukhbaatar2016learning,hoshen2017vain,peng2017multiagent,singh2018learning,das2019tarmac}. 
With these techniques, bandwidth usage during communication increases as the number of agents scales up. While Who2com~\cite{liu2020who2com} uses a handshake communication to reduce the bandwidth usage, this model assumes all agents \textit{always} need to communicate with one of the other agents. 
This results in the waste of bandwidth consumption and cannot prevent the issue of detrimental messages. 
In contrast, our proposed framework alleviates these problems by learning to decide when to communicate and to create communication groups.



\textbf{Attention mechanisms.} 
Attention mechanisms have been widely used in recent learning-based models.
In a nutshell, the attention mechanism can be viewed as a soft fusion scheme to weight different values based on a similarity between query and keys. 
A few noticeable and widely used attention mechanisms are \textit{additive}~\cite{bahdanau2014neural}, \textit{scale dot-product}~\cite{vaswani2017attention}, and \textit{general}~\cite{luong2015effective}.
One key finding of our work is that the \textit{general} mechanism allows for asymmetric queries and keys, which makes it especially suitable for tasks with bandwidth considerations: An agent's transmitted query message can be smaller than its retained key, and hence its overall bandwidth consumption can be reduced.

