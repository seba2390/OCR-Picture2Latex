%% bare_conf_compsoc.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8s or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference,compsoc]{IEEEtran}
% Some/most Computer Society conferences require the compsoc mode option,
% but others may want the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference,compsoc]{../sty/IEEEtran}


%\usepackage{hyperref} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{float}
\floatstyle{boxed} 
\usepackage{graphicx}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}

\usetikzlibrary{positioning}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\definecolor{darkgray}{rgb}{0.37,0.37,0.52}
\definecolor{darkgreen}{rgb}{0.0,0.55,0.0}

\newcommand{\jani}[1]{\textcolor{red}{\textbf{[Jani: #1]}}}
\newcommand{\response}[1]{\textcolor{darkgreen}{#1}}
%\newcommand{\prosaic}[2]{\textcolor{gray}{\textsf{ #1}}}

%\newcommand{\prosaic}[1]{\textcolor{black}{\textsl{#1}}}
\newcommand{\prosaic}[1]{#1}
\newcommand{\curtail}[1]{\textcolor{orange}{{#1}}}

\newcommand{\expendable}[1]{\textcolor{blue}{\textit{ Exclusively Expendable (#1)}}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}



\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Binarized Convolutional Neural Networks for Efficient Real-Time Inference [On GPU]}
\title{Binarized Convolutional Neural Networks for Efficient Inference on GPUs}



% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Mir Khan, Heikki Huttunen, Olli Suominen and Atanas Gotchev}
%\IEEEauthorblockA{Laboratory of Signal Processing\\
%Tampere University of Technology\\
%Tampere, Finland\\
%Email: mir.khan@tut.fi, heikki.huttunen@tut.fi, olli.j.suominen@tut.fi, atanas.gotchev@tut.fi}}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Mir Khan, Heikki Huttunen, Jani Boutellier}
\IEEEauthorblockA{%Laboratory of \textbf{LABORATORY\_NAME}\\
Tampere University of Technology\\
Tampere, Finland\\
Email: Mir.Khan@tut.fi, Heikki.Huttunen@tut.fi, Jani.Boutellier@tut.fi}}

% \and
% \IEEEauthorblockN{Heikki Huttunen}
% \IEEEauthorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \IEEEauthorblockN{Olli Suominen}
% \IEEEauthorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \IEEEauthorblockN{Atanas Gotchev}
% \IEEEauthorblockA{Starfleet Academy\\
% San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}



% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page (and note that there is less available width in this regard for
% compsoc conferences compared to traditional conferences), use this
% alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
%\jani{usually the abstract starts with a sentence or two that clarify the general contect of the work. Here that would be CNNs for embedded platforms}


Convolutional neural networks have recently achieved significant breakthroughs in various image classification tasks. However, they are computationally expensive, which can make their feasible implementation on embedded and low-power devices difficult. In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices. In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. This representation completely eliminates the need for floating point multiplications and additions and decreases both the computational load and the memory footprint compared to a full-precision network implemented in floating point, making it well-suited for resource-constrained environments. %Our approach is demonstrated with a convolutional neural network that is used for vehicle type classification. 
We compare the performance of our implementation with an equivalent floating point implementation on one desktop and two embedded GPU platforms. Our implementation achieves a maximum speed up of $\mathbf{7.4}\times$ with only 4.4\% loss in accuracy compared to a reference implementation.

%[we verify and show that the binary net method works on car data and it's effective. We implement a binarized convolutional neural network for vehicle classification. We [also] study the impact of input binarization on speed and accuracy using various input binarization methods such as  automatic-preprocessing conv layer,  thresholding..]
\end{abstract}
\textbf{Keywords: } model compression, binarized convolutional neural networks, optimization, image classification

\makeatletter
\def\ps@IEEEtitlepagestyle{
  \def\@oddfoot{\mycopyrightnotice}
  \def\@evenfoot{}
}
\def\mycopyrightnotice{
  {\footnotesize
  \begin{minipage}{\textwidth}
  \centering
  Copyright~\copyright~2018 IEEE. Accepted to Proc. IEEE EUSIPCO 2018, Sep. 3-7, 2018, Rome, Italy
  \end{minipage}
  }
}
%\response{I think it's pretty much done, apart from the other green comments. So if there are any suggestions it should be done today or tomorrow in the morning. Deadline is 2 PM (I think) tomorrow, but I'll submit a copy today at the end of the day.}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\vspace{-0.75mm}
\section{Introduction}
\vspace{-0.75mm}
%\textit{(This section + previous work, + abstract will take more time...)}\\

% no \IEEEPARstart

%[Some introduction about neural nets]...
In the recent years, convolutional neural networks (CNNs) have presented impressive performance in image classification [\ref{imageclassification}][\ref{augmentimage1}], face recognition [\ref{facerecognition1}][\ref{facerecognition2}], audio classification [\ref{audioclassification2}], and speech recognition [\ref{speechrecognition}]."

%\jani{the section below is not mandatory, but if you have space you can include a CNN introduction like that}

%\expendable {2} \prosaic{ Convolutional neural etworks are like standard nets but Typically it has 1 or more convolutional layers . they are well suited for extracting information from images and they've been shown good results (refs). the conv layer outputs feature maps extracted from the data by filtering the image with a number of kernels. following the conv layer is usually 1 ir nmore a standard Describe how convolution is done as matmult (ref conv2gemm)}

Large neural network models can be computationally expensive, making them unsuitable for deployment to small resource-constrained mobile devices. To this extent, contemporary CNN-based solutions often acquire the input data on a mobile device, but transmit the data to a remote server for CNN-based processing. However, performing the CNN-based processing on the mobile device (a.k.a. edge computing) would reduce the overall system complexity and enable real-time applications. %\curtail{such as those related to Augmented Reality []}.

The emerging CNN subfield of \textit{model compression} aims to retain the accuracy of the neural network while minimizing redundant network parameters and reducing computational load. Many such techniques have already been proposed.

%\jani{Please check whether we should speak about 'parameters' or 'weights' here. I'm not sure they're interchangeable}\response{ : I think they are interchangeable. I also checked the original pruning paper, and they use both words to describe the weights of the network.}

One technique [\ref{refPruning}] is based on \textit{pruning} of parameters, where majority of the parameters of the network are removed without significantly impacting accuracy. Reduction of parameters initially leads to a significant drop in accuracy; however, retraining (fine-tuning) of the parameters restores most of the network's accuracy. The authors report 13$\times$ reduction of memory requirements with no loss in accuracy [\ref{refPruning}].

Another approach, low-rank approximation of convolutional kernels [\ref{refSepConv}], approximates 2D convolutions with convolutions by vectors. The separable kernels can be obtained either by training the network with separable filters [\ref{refDecompose}] or by posing it as an optimization problem to minimize the reconstruction error of the feature maps. Depending on the approach [\ref{refSepConv}][\ref{refDecompose}], speedups between 2$\times$ to 4$\times$ have been reported on CPU implementations.

%\begin{equation}
%argmin_{\{h\} , \{v\}} E= \frac{1}{M}\sum(conv(X,W))\\ - \frac{1}{M}\sum(colconv(rowconv(X,rowkernel),colkernel))
%\end{equation}

Binarized neural networks (BNN) have been first introduced in [\ref{refBinNet}], where their performance was demonstrated on the CIFAR-10 dataset. The weights and activations for intermediate computations are binarized to $+1$ and $-1$. The authors present a speed up of 7$\times$ on a network for the MNIST dataset. In a further work [\ref{refXnor}] the approach was refined for CPU implementation and evaluated on the ImageNet dataset.

by packing 1-bit weights into 32-bit words, enabling replacement of multiplication operations by logic XNORs.
In this paper, an approach for the implementation of
BNNs [11] on GPU platforms is presented. To the best
knowledge of the authors, this is the first work that presents
a GPU implementation of a binarized convolutional neural
network for inference. We present our implementation with
an application use case of vehicle type classification [12].
Results show significant speedups in real-time inference
compared to a floating point version of an equivalent neural
network.

As a summary, the contributions of this work are as follows:
\vspace{-0.8mm}
\begin{itemize}
	\item Detailed presentation of efficiently implementing
CNN binarization, including the convolutional layers,
on GPU-based platforms.
    \item Comparison of different approaches for binarizing
input data, and how each approach impacts the classification
accuracy.
    \item Performance (execution time) comparisons on several
platforms.
    
\end{itemize}

The source code for our CUDA implementation is publically available \footnote{ github.com/Valentin4869/BinCNN}{}.

\section{Experimental Setup}
\subsection{Binarizing the network}
Our binarized network architecture is based on the original vehicle classifier network presented in [\ref{refHeikkiCar}]. We implement a binarized version of the same architecture in several steps. We do not use any ReLU [\ref{refRELU}] activations in the binarized version. In the original binarization work [\ref{refBinNet2}], the authors suggest two approaches for binarization: stochastic and deterministic. For binarizing the weights and intermediate computations, we use the deterministic \textit{sign} function, which is defined as


\begin{equation}
sign(x) = \begin{cases} -1 & \text{if } x \leq 0 \\ +1 & \text{if } x > 0 \end{cases}
\end{equation}
For training the BNN, following [\ref{refSTE}], we explicitly define the gradient of the \textit{sign} function to be the identity function in the backward pass, such that $\frac{\partial sign(x)}{\partial x} = x$.


The non-binarized network is trained with the RMSprop optimizer [\ref{refRMSprop}], while the binarized version is trained with the ADAM [\ref{refAdam}] optimizer.  After training, only the binarized weights are used for inference for the binarized network.
%The diagram for the binarized network is shown in Figure \ref{fig:netarch}.
%\jani{there are multiple references to 'original', please indicate by reference which work you mean}

% \begin{figure}[h]
% \center  {\includegraphics[scale=0.60]
%   {figure.eps}}
 
%   \caption{(placeholder graph) Binarized Network architecture}  
%     \label{fig:netarch}
% \end{figure}



% [there are two networks of interest: original vehicle classifier (ref heikkicar) and our binarized version]. The original network uses [(full precision)?] weights and [describe architecture in detail]. [The first network uses 2 convolutional layers, with the first outputting 32 feature maps. The second outputs 32 featuremaps. The conv layers are followed by 3 dense layers, the first two with an output of 100 each, and the last has an output of 4. Relu activations are used in intermediate results between layers, except for last layer.] [In the binarized version all relu activations are simply replaced by sign activations during training. During training, we calculate the gradient of the sign function as the identity. However, since our weights are binary, they can be converted to 0 1 and multiplications can be simulated using xnors and pop count. This can substantially reduce computation time, however there's still the packing overhead. After weights are obtained, we make a CUDA/OpenCL version which packs the weights and simulates dot products with bit-wise operations] \\






% \begin{figure}[h]
% \center  {\includegraphics[scale=0.60]
%   {figure.eps}}
 
%   \caption{(placeholder graph) Network architecture}  
%     \label{fig:tradeoffAcc}
% \end{figure}

% The weights for all layers are initialized according to [\ref{saxe}] by generating an orthogonal matrix with a gain factor of 1.0. The weights matrix for the recurrent kernel is initialized by sampling a truncated normal distribution; this is known as a \textit{glorot normal initialization} [\ref{glorot}]. The bias values are all initialized to zeros. A linear activation function is used at the output layer of the network. All hidden layers use the hyperbolic tangent as their activation functions, and a hard sigmoid as the activation function for the recurrent step. The Mean-Squared Error (MSE) was used as the loss function and RMSprop [\ref{rmsprop}] was chosen as an optimizer with the initial learning rate of 0.001. The network was trained with approximately $14,000$ samples using a mini-batch size of 32 samples. Each network was trained until the performance of the network (in the MSE sense) stopped showing any improvement.

The network is trained with a dataset set consisting of 6555 images of vehicles that have been captured by a camera and manually categorized into four different classes: \textit{bus}, \textit{normal}, \textit{truck}, and \textit{van}. Each image has size $96\times 96$ and are in full color. The data has been split into a training set (90\%) and a test set (10\%).  We augment the training set using flipping and filtering with a 2D Gaussian filter with $\hspace {-0.8mm} \sigma=  0.5$, resulting in a total training set size of 14,108 images, 20\% of which are used for validation. Throughout this text, our accuracy reports are for the performance of the network on the test set that corresponds to the best validation set accuracy.

%\response{Maybe we shouldn't include a net  diagram? Takes too much space and no advantage compared to a written description of the net architecture. Maybe there is no need to describe the architecture in detail since we refer to the original net in heikkiCar and how we modify/binarize it? I could still make the diagram at the end of today if it's needed.}
%\jani{Yes, you can omit the diagram as we are overlength}

\subsection{Testing pipeline}

%option 1: Measurement starts from memcpyHD, kernel launch, and ends at memcpyDH. \\
%option 2: Allocate space for more than one input, Measurement starts from first memcpyHD , kernels launched, then second memcpyHD, launch kernels.... when batch is completed, we wait and do memcpyDHasync from separate buffers.\\
%option 3 : memcpy, starttimer, launch kernels, wait for last kernel to finish, stop timer. Motivation: memcpyHD/DH can vary depending on copy method, platform, whether batch processing is used..., so it is probably fair to only measure computation times for the kernels, and average those for different inputs.

For obtaining runtime results, we use the built-in GPU timers to measure the runtime of the kernels for our CUDA and OpenCL programs. Our kernel execution time measurements do not include memory transfer times to/from the GPU, as they can be affected by various factors, some of which are hardware-dependent, for example, on the NVidia Jetson host and device memory are shared. The correctness and accuracy of the profiling results generated have been verified by the Nvidia Visual Profiler for the same CUDA programs.

For each test run, 1000 images are randomly generated and fed to the network one at a time. The timer begins after the memory is copied, and the timer ends after the last kernel's computation is completed. Our final result is the total accumulated time per sample averaged over all 1000 samples.



\subsection{Input binarization}
In this section we describe our methods for binarizing the inputs to the first layer of our BNN. We pre-process the data set using these techniques and evaluate the accuracy of the BNN on the pre-processed data set.

\textbf{Thresholding}
A constant threshold $T$ can be subtracted from the input $\mathbf{X}$ before binarizing it. We simply substitute the input $\bf{X}$ to the first layer with $sign(\mathbf{X} + T)$, for $\mathbf{X}\in \mathbb{R}^{H\times W\times C}$, and for $T \in \mathbb{R}^{1\times 1\times C}$. The motivation is to shift the range of values taken by $\mathbf{X}$ such that binarization with the \textit{sign} function produces meaningful results, as opposed to all zeros for standard pixel-value ranges do not include negative numbers. The network is trained as before but in two stages: first, the network is trained for $50$ epochs and the loss is minimized with respect to all network parameters except for $T$. Then a second stage of tuning is entered where we minimize the loss with respect to the parameter $T$ and the validation set. We repeat this process for several thousand training epochs until the performance on the validation set no longer improves.


\begin{figure}[h]
{
%{\includegraphics[scale=0.8]{binToriginal.png}}
%	\includegraphics[scale=0.8]{bininT0.png}
%  {\includegraphics[scale=0.83]{bininT1.png}}
%  {\includegraphics[scale=0.80]{bininT2.png}}
  
%  \includegraphics[scale=0.80]{binToriginal.png}
 % 	\includegraphics[scale=0.84]{2lbpT0.png}}
 % {\includegraphics[scale=0.84]{2lbpT0_05.png}}
%  {\includegraphics[scale=0.844]{2lbpT0_1.png}
  
  }
  %\hspace{115pt}
  
  {
{\includegraphics[scale=0.8]{lbpOriginalC.jpg}}
	\includegraphics[scale=0.8]{ncT0.png}
  {\includegraphics[scale=0.8]{ncT1.png}}
  {\includegraphics[scale=0.8]{ncT2.png}}
  
  \includegraphics[scale=0.8]{lbpOriginalGrayC.jpg}
  	\includegraphics[scale=0.842]{lbpT0.png}}
  {\includegraphics[scale=0.845]{lbpT0_05.png}}
  {\includegraphics[scale=0.852]{lbpT0_1.png}}
 
  
  
  
  
  
  \caption{Input binarization with RGB Thresholding (first row) and LBP (second row).}
\end{figure}
  \label{fig:binarizedInputs}
  

\textbf{Local Binary Patterns (LBP)}
A well-known technique called \textit{local binary patterns} for extracting multi-resolution and scale-invariant features from images has been introduced in [\ref{refLBP}]. We use a similar approach in our application for image binarization, but with a slight modification: we operate on the grayscale image and process each pixel by examining its neighborhood at a radius of 1 pixel, generate 3 artificial color channels and select 3 pixels at a clockwise stride of $3$ in the neighbourhood to distribute to these channels. Then the value of these pixels are set to $1$ if they exceed the value of the center pixel and $0$ otherwise. An example of this transformation on an image from the dataset is demonstrated in the second row of Figure \ref{fig:binarizedInputs}.

%\jani{Here it would be appropriate to add a short section 2.4 that would explain packing of binarized weights. In the present text format, packing suddenly is mentioned in 3.1 without explanation.}
%\response{I'm not sure if that would be necessary considering that it's pretty simple and the lack of space, but I will try to squeeze it in here. I wrote a short section below, but I feel like it needlessly complicates things.}
\subsection{Packing binary-valued vectors}
To avoid confusion with terminology, we denote by \textit{packing} the encapsulation/conversion of an array of 1-bit values into an individual 32-bit unsigned integer. Formally, for a binary-valued vector $\mathbf{x}\in \left\lbrace-1,+1\right\rbrace ^{D}$, assuming $D$ is divisible by $B$, then the packed representation of $\mathbf{x},$ $\mathbf{x}_p\in \left\lbrace-1,+1\right\rbrace ^{D/B}$
%$\mathbf{x}_p\in \left\lbrace 0,1,2,3,...,2^{B-1}\right\rbrace ^{D/B}$ 
for a packing bitwidth $B\leq32$ (assuming 32-bit word) and positive $D$, is given by 
% \begin{equation}
% \mathbf{x}_p= \begin{bmatrix}
%     \sum^{B-1}_{i=0}\frac{(1+x_i)(B-1 - i)}{2}\\[0.3em]
%      \sum^{2B-1}_{i=B}\frac{(1+x_i)(B-1 - i)}{2}\\[0.3em]
%        \vdots \\[0.3em]
%  \sum^{D}_{i=D-B}\frac{(1+x_i)(B-1 - i)}{2}\\[0.3em]
%      \end{bmatrix},
% \end{equation}

% \begin{equation}
% \mathbf{x}_p= \begin{bmatrix}
%     \sum^{B-1}_{i=0}(1+x_i)2^{B-2 - mod(i,B)}\\[0.3em]
%      \sum^{2B-1}_{i=B}(1+x_i)2^{B-2 - mod(i,B)}\\[0.3em]
%        \vdots \\[0.3em]
%  \sum^{D}_{i=D-B}(1+x_i)2^{B-2 - mod(i,B)}\\[0.3em]
%  \sum^{D}_{i=D-B}(1+x_i)2^{B-2 - mod(i,B)}\\[0.3em]
%      \end{bmatrix}.
% \end{equation}

\begin{equation}
\mathbf{x}_p= \begin{bmatrix}
    \sum^{B}_{i=1}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
     \sum^{2B}_{i=B+1}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
     \sum^{3B}_{i=2B+1}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
       \vdots \\[0.3em]
 %\sum^{D}_{i=D-B}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
 \sum^{D}_{i=D-B+1}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
     \end{bmatrix}.
\end{equation}

% \begin{equation}
% \mathbf{x}_p= \frac{1}{2}\begin{bmatrix}
%     \sum^{B}_{i=1}(1+x_i)2^{B - i}\\[0.3em]
%      \sum^{2B}_{i=B+1}(1+x_i)2^{2B - i}\\[0.3em]
%      \sum^{3B}_{i=2B+1}(1+x_i)2^{3B - i}\\[0.3em]
%        \vdots \\[0.3em]
%  %\sum^{D}_{i=D-B}(1+x_i)2^{B-2 - mod(i-1,B)}\\[0.3em]
%  \sum^{D}_{i=D-B+1}(1+x_i)2^{D - i}\\[0.3em]
%      \end{bmatrix}.
% \end{equation}

\section{Implementation}
%\section{Efficient Software Implementation of a Binarized Convolutional Network}

In this section, we present the details of our CUDA implementation of the binarized neural network architecture described in Section 2. We use CUDA terminology throughout this section.

\subsection{Convolutional layers}

% Implementations:
% cuDNN fp
% cuDNN + bin
% cuDNN + bin with bin inputs
% OpenCL baseline
% ARM CL

% On NVIDIA platforms we compare our implementation with the fp cuDNN implementation. 
% We implement the binarized neural network in OpenCL and CUDA and benchmark our implementation on three different GPUs. 

%\jani{one would expect that this section would start by an overview of the binarization approach, however now it starts by GEMM}
%\response{: yes, this section was initially called "Convolutional layers", but then Heikki suggested this change because the last section was short, but I think we can organize them into convolutional layers and matrix multiplications (includes second stage in convolution + dense layers)}

The convolutional layer in a neural network can significantly improve image classification accuracy compared to standard multi-layer perceptrons. Given a kernel $H\in \mathbb{R} ^{K \times K\times C  }$ and an image $ X\in \mathbb{R}^{ H\times W \times C}$, an output feature map $ F \in \mathbb{R}^{ H\times W} $ is given by the expression
% \expendable{4}
% \prosaic{
% \begin{equation}
% F[m,i,j]=\sum^{C=1}_{c=0} \sum^{K-1}_{l=0} \sum^{K-1}_{k=0} H[m,l,k,c]X[i-,j-.,c]...
% \end{equation}
% }



\vspace{-2.8mm}
\begin{equation}\label{xnormult}
    %\resizebox{1.20\hsize}{!}{%
         \hspace{-1.2mm} F[i,j]=\hspace{-1.0mm}
         \sum^{C-1}_{c=0} \sum^{R}_{l=-R} \sum^{R}_{k=-R} \hspace{-1.0mm} H[R+l,R+k,c]X[i+k,j+l,c],\hspace{-1.5mm}  
     %   }
\end{equation}
for odd $K$, and the kernel radius \resizebox{0.15\hsize}{!}{$R=\frac{K-1}{2}$}.
It should be noted that equation (3) in fact computes cross-correlation (not convolution), which is the convention in deep learning. A common approach for computing convolutions efficiently is through matrix multiplication [\ref{refGEMMConv}], where the weights and image tensors are reshaped into 2-dimensional matrices, which will then allow us to compute the convolution through a single matrix multiplication. The reshaping for the weights is trivial, and this step can often be skipped if the weights are already stored in this layout; however, the process of arranging the input image into the matrix of columns used for computing the convolution can be difficult to optimize. This is due to inefficient access patterns, complicated index calculations that involves many division and modulo operations, and the overhead of storing the large output matrix to global memory.
%\jani{I tried to re-formulate the latter half of the baragraph just above, please check if it makes sense}
%\jani{As you had already planned, an explanation about im2col would be good here. You mention this function 3 paragraphs from here.} \response{I was thinking may I should just explain im2col implicitly as done here and never refer to it as 'im2col' because it sounds a little informal.}
%\jani{It's fine if you omit the description of im2col if there is no reference to it later.}
%In our case, the latter problem 
%\prosaic{The function that does this is often called \textit{im2col3d} , and we will refer to it like that from here on.}

A straightforward approach for avoiding inefficient access patterns is to load regions from the image into shared memory (on-chip memory) and then extract the patches from shared memory [\ref{refCUDNN}]. For an image with dimensions $H\times W\times C$ corresponding to height, width, and channels respectively, and a $K\times K \times C$ kernel with a radius of $R=\frac{K-1}{2}$, we use threadblock dimensions of $S\times W$ ($S=2$ in our case), which covers the entire width of the image, eliminating the need to redundantly load the horizontal non-zero halo regions which are difficult to load with an efficient access pattern. Then each thread-block loads an image region of dimensions $(S + 2R)\times W$ into a region in shared memory in three steps, starting by loading the top vertical halo region, the middle part, then the bottom vertical halo region (except when loading from the bottom of the image). The shared memory buffer is zero-initialized in order to implicitly handle horizontal zero-padding. Loading vertical halo regions can be done very efficiently since all threads in the threadblock load from contiguous regions in the image array.
%The allocated region in shared memory is pre-initialized to zero, and padding is simply handled by shifting the loaded image block in the destination in shared memory.

In the second stage, the patches of size $K\times K \times C$ are extracted from shared memory.  We avoid division and modulo operations in the patch-extraction stage by using an integer counter register. This results in a 2$\times$ performance boost in our case. Since the network is binarized, the packing and patch-extraction step can be fused into one step to avoid redundant accesses to global memory, reducing global memory stores by $K\times K$. The algorithm for the combined step of extracting the patches and packing them is shown in Algorithm 1.

%\prosaic{Even the full-precision version of our \texttt{im2col} algorithm, which does not involve output binarization, is 2$\times$ faster than in cuDNN, which we believe is due to eliminating slow division and modulo operations.}

% A major bottle-neck in the im2col algorithm can [is the part where] the outputs are stored into global memory. The input to the algorithm is a HxWxC image, however, the output can have a KxK fold increase in size. In a binarized neural network however, this problem can be avoided by only storing the packed representation of the output matrix to global memory. Each thread handles a $K\times K$ segment in the image , but since our kernel outputs a packed columns matrix, each thread stores a single 32-bit unsigned integer register in the columns matrix and in a fully coalesced manner, where each register contains the packed representation of all $K\times K$ pixels in the image patch, reducing the amount of global memory stores by 25X in our case for a kernel size of $5\times 5$. 
%This process is illustrated in Figure \ref{fig:figIm2col}.

% \begin{figure}[h]
% \center  {\includegraphics[scale=0.2]
%   {bim2col3d.png}}
 
%   \caption{\expendable{1} DO A BETTER VERSION LATER}  
%     \label{fig:figIm2col}
% \end{figure}


\begin{algorithm}
\caption{Patch-extraction and packing}\label{}
\begin{algorithmic}[1]
\Function{\textup{ ExtractPacked}}{sh\_block}:
%\State	 $s \gets (W + 2 \times R) \times \texttt{\_t}_y+\texttt{\_t}_x$
\State  $v \gets 0$
\State $k  \gets 0$
\For {$i = 0$ to $B-1$}

\If {($i - k K =K$)}
\State			$k++$
\EndIf
%\State		$idx$ = $s + k \times (W + 2\times R) + i - k \times K$
\State		$idx =(W+2R)(\texttt{\_t}_y + k) +\texttt{\_t}_x + i -kK$
\State		$s =$ sh\_block[$idx$]$\hspace{1.5mm}>\hspace{1.5mm}0$

\State		$v$ = bitOR( $v, s << B - 1 - i$)
\EndFor

\State \textbf{return $v$}
\EndFunction
\end{algorithmic}
\end{algorithm}
%\response{I feel like this algorithm block is too short that it's not necessary... Not sure if we should keep this?}

% global memory once
% \prosaic{We fix this by reducing the number of global memory accesses. This can be achieved through two ways: first, use threadblock dimensions that cover the entire width of the image. This completely eliminates the need to load horizontal halo regions from the middle of the image. As for halo region extending outside the image, since we load an image block in shared memory and extract the patches from there, this can be solved by pre-initializing the shared memory block to zeros, and copying the loaded image block into shared memory with a shift. This results in three cases: first, loading from the top of the image; this can be handled by copying the image block into shared memory at an offset. Second case, middle of the image, this part is easy, since we now load from a negative offset in the source image (shift back) and copy to shared memory normally. Third: bottom fo the image; this can be handled }
% We [try to solve this] by first reducing the amount of global memory accesses in the im2col3d kernel

%\prosaic{ After data loading, each thread extracts a 5x5 patch which is then saved in the destination in a fully coalesced manner. For bin inputs, the output of the kernel are packed, substantially reducing the amount of global memory stored by up to 32x (25x in our case). For a packed output matrix, each thread in our algorithm writes a single element in the output, which is the reason why we chose a packing bitwidth of 25 because each thread handles a single patch.}

%\prosaic{ we try to fix this with two ways. First, minimize global stores and loads, and second, eliminate all divisions and modulo operations during index calculations. We do modulo and division with a counter. Since we need packed data for the following xnorMatMult, we perform the packing inside the im2col kernel, reducing total stores to global memory by 25x (although a potential reduction of 32x is possible, but we choose bitwidth of 25 for practical reasons. First, first im2col is not divisible by 32. Second im2col is divisible by 32, but each thread handles one patch and to elimiante complicated index calculations.)}
%\jani{I think it is good to have to the algorithm description there.}

In Algorithm 1, \texttt{sh\_block} is the region of the image loaded into shared memory using the previously described steps, including the halo regions. $\texttt{\_t}_x$ and $\texttt{\_t}_y$ are the thread indices for the $x$ and $y$ dimensions of the thread block corresponding to the CUDA \texttt{threadIdx.x} and \texttt{threadIdx.y} variables. $B$ is the packing bitwidth, chosen to be $25$ in our case, $<<$ is the left bit-shift operator, and $v$ is the packed extracted patch.



%\texttt{\textbf{comment:} section names 3.1 and 3.2 are not very logical when compared to each other. If section 3.1 is about convolution layers, it would be logical that 3.2 would concern dense layers} \response{I tried to change it, but now the dense layer section is kinda short.}


For computing the convolution we implement a standard matrix multiplication subroutine in a manner similar to [\ref{refDGEMM}], where tiles from each matrix are loaded successively into shared memory and used to compute a submatrix of the output, such that each thread computes a single element in the output matrix, but instead of computing multiplications, we compute xnors and bit-counts following an approach similar to what was suggested in [\ref{refBinNet}] as
\vspace{-1.2mm}
\begin{equation}\label{xnormult}
\vspace{-1.2mm}
         \mathbf{a\cdot}\mathbf{b}= \texttt{W}- \texttt{2}\times \texttt{popcount(xor(}A,B\texttt{))},% 
\end{equation}
where ${A}$ and ${B}$ are both 32-bit unsigned integer registers containing the packed representations of vectors \textbf{a}, \textbf{b} $\in \left\lbrace-1,+1\right\rbrace ^{\texttt{W}}$ respectively. We denote by $\cdot$ the real-valued dot product. The operation \texttt{xor} is the bit-wise xor operation, and \texttt{popcount} is a function for computing the number of bits set to $1$. The packing bitwidth \texttt{W} is the number of elements that are packed together in a single unsigned integer register. %\prosaic{The output matrix is in full-precision and identical in dimensions to the full precision version. The results are also identical as long as the matrices only take on the values $-1$ and $+1$. }

\subsection{Fully connected layer}
For the fully-connected layer, we follow a slightly different approach from standard matrix multiplication. For a packed weights matrix $\bf{W}\in \mathbb{R}^{L\times D}$, and a packed vector $\bf{x}\in \mathbb{R}^{D\times 1}$, we divide the process of computing the dot product of each weight vector and $\bf{x}$ into $64$ segments, such that each of 64 threads handling a weight vector compute the partial sum of the dot product between a weight vector and $\bf{x}$ through xnor operations, and stores the results in shared memory. The partial sums are then combined in a parallel reduction sum that does not require synchronization (for a warp size of 32 on the target platform).
%\jani{is this on the left 64 times L or 64 times x times L? The possibility of confusion is great as there is the vector of name x involved}
\section{Results}

 In this section we present our results for the impact of input binarization on classification accuracy and the performance improvement achieved.

% In this section we present the results [of performane boost] from our implementation compared to an equivalent full-precision implementation on the target platforms. [for how] input binarization impacts a binarized neural network in terms of runtime speed, storage requirement, and classification accuracy. Additionally we [present] the results of our implementation and compare it with existing full-precision implementations that [use] can achieve state of the art performance on each platform we test on and 

\textbf{Input binarization}
in Table \ref{tblAccuracy} we report the classification accuracy results we obtained using each different input binarization scheme for our binarized version of the vehicle classifier [\ref{refHeikkiCar}]. We can observe that accuracy is best retained when the first layer is not binarized; however, only a moderate loss in accuracy occurs when using LBP and RGB Thresholding. Considering that RGB Thresholding is much simpler to implement and results in almost no additional computational overhead, we choose this approach for our final binarized architecture, for which we report the speed up results in the following section.
% (accuracy table)
%  In TABLE2 we report the best result we have obtained using this method with a threshold value of $0.01$ for pixels in range $0.0 - 1.0$.


\begin{table} 
\caption{Runtime of the network on each platform}
\begin{center} % Centers the table on the page, comment out to left-justify
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c c c c} % The final bracket specifies the number of columns in the table along with left and right borders which are specified using vertical bars (|); each column can be left, right or center-justified using l, r or c. To specify a precise width, use p{width}, e.g. p{5cm}
%\toprule % Top horizontal line
& \multicolumn{3}{c}{} \\ % Amalgamating several columns into one cell is done using the \multicolumn command as seen on this line
%\cmidrule(l){2-4} % Horizontal line spanning less than the full width of the table - you can add (r) or (l) just before the opening curly bracket to shorten the rule on the left or right side
% Joint & LSTM1 & LSTM2 & LSTM3 \\ % Column names row
% \midrule % In-table horizontal line
% MAE		&0.0196 &  0.0167 & 0.0188 \\ 
% MID  &	0.2034& 0.2185 & 0.1437 \\  
% Energy$_{avg}$ & 2.6512 & 2.8149& 1.9892\\
% Energy$_{std}$ & 1.2017 & 1.9342 & 1.0882\\
% \midrule % In-table horizontal line
% \bottomrule % Bottom horizontal line

Implementation Method & GTX1080 & Mali T860 & Tegra X2 \\ % Column names row
\midrule % In-table horizontal line
%baseline (full-precision)  & 0.0 $\mu s$  & 0.0 $\mu s$ & {0.0 $\mu s$} \\ 
cuDNN (full-precision)	& $401.83 \mu s$  & N/A$^\dag$  &  $2.27$  ms \\ 
Arm CL (full-precision)	& N/A$^\dag$   & $29.61$ ms  &  N/A$^\dag$ \\ 
BCNN   & $102.39 \mu s$ &  $23.63$ ms & $0.53$ ms \\
BCNN with binarized inputs  & $\mathbf{55.63} \mu s$ & $\mathbf{17.58}$ ms & $\mathbf{0.41}$ ms\\




\midrule % In-table horizontal line
\bottomrule % Bottom horizontal line

\end{tabular}
}
\end{center}
 $^\dag$Library not compatible with this platform.
 % Table caption, can be commented out if no caption is required
\label{tblTime} % A label for referencing this table elsewhere,

\end{table}



\begin{table} 

\caption{Runtime per-layer (GTX1080)}
\centering % Centers the table on the page, comment out to left-justify
\resizebox{\columnwidth}{!}{
\begin{tabular}{l c c c c} % The final bracket specifies the number of columns in the table along with left and right borders which are specified using vertical bars (|); each column can be left, right or center-justified using l, r or c. To specify a precise width, use p{width}, e.g. p{5cm}

\toprule % Top horizontal line
& \multicolumn{3}{c}{} \\

% Amalgamating several columns into one cell is done using the \multicolumn command as seen on this line
 % Horizontal line spanning less than the full width of the table - you can add (r) or (l) just before the opening curly bracket to shorten the rule on the left or right side
% Joint & LSTM1 & LSTM2 & LSTM3 \\ % Column names row
% \midrule % In-table horizontal line
% MAE		&0.0196 &  0.0167 & 0.0188 \\ 
% MID  &	0.2034& 0.2185 & 0.1437 \\  
% Energy$_{avg}$ & 2.6512 & 2.8149& 1.9892\\
% Energy$_{std}$ & 1.2017 & 1.9342 & 1.0882\\
% \midrule % In-table horizontal line
% \bottomrule % Bottom horizontal line

Layer & cuDNN & Binarized & Speed-up \\ % Column names row
\midrule % In-table horizontal line
%baseline (full-precision)  & 0.0 $\mu s$  & 0.0 $\mu s$ & {0.0 $\mu s$} \\ 
Im2col3d $(96,96,3)$	& $21.63$ $\mu s$  & $3.17\mu s$ & $6.82\times$ \\ 
GEMM-convolution $(32,5,5,3)$	& $37.54 \mu s$   & $8.61\mu s$ &  $4.36\times$  \\ 
Max-Pooling $(96,96,32)$	& $5.22 \mu s$ & $8.26 \mu s$   & $0.63\times$\\ 
Im2col3d $(48,48,32) $  & $65.41 \mu s$ &  $5.50 \mu s$ &  $11.89\times$\\
GEMM-convolution $ (32,5,5,32)$	  & $69.28 \mu s$ & $8.10 \mu s$ &  $8.55\times$\\
Max-Pooling (48,48,32)	& $5.38 \mu s$    & $2.66 \mu s$  &  $2.02\times$\\ 
Fully-Connected $(100, 24\times 24 \times32)$  & $200.03 \mu s$ & $6.28 \mu s$ &  $31.85\times$\\





\midrule % In-table horizontal line
\bottomrule % Bottom horizontal line

\end{tabular}
}
 % Table caption, can be commented out if no caption is required
\label{tblSpeed} % A label for referencing this table elsewhere,

\end{table}


\textbf{Performance Boost}
We time our binarized implementation on 3 different hardware platforms: Nvidia GTX 1080, Nvidia Jetson (Tegra X2), and the Mali-T860. We derive an OpenCL version of our implementation for testing on the Mali-T860, which is a straightforward process. We compare the performance of our implementation against an equivalent full precision version of the same network implemented with highly optimized libraries on each target platform, in our case these are cuDNN on Nvidia platforms, and the ARM Compute Library on the Mali-T860. 
We list in Table \ref{tblTime} the average execution times of the full network on each platform. \prosaic{We can see that} our binarized implementation can achieve up to $7.5\times $ speed up on the GTX1080 and about $5.5\times $ on the Tegra X2. \prosaic{We also notice that} the relative performance improvement on Mali GPU is much smaller at about $1.7\times $ for the fully binarized version. In our optimizations, we heavily take advantage of using local memory (in OpenCL terms) which resides on-chip in most workstation GPUs and the Nvidia Tegra X2, but this does not offer any performance benefits on Mali GPUs since local memory is allocated in global memory.
It should be noted that cuDNN is optimized for batch processing and that our results are for one sample at a time which means these results may not necessarily be reflective of the full potential of cuDNN; however, batch processing is not a suitable option for real-time applications where a single input is processed at a time. Additionally, we note that for our cuDNN implementations, we use the explicit GEMM convolution algorithm, which can be slightly slower than the implicit GEMM algorithm. For example, cuDNN with implicit GEMM can run at $316 \mu s$ for the first convolutional layer in our network on the GTX1080.%, and at $1.626$ ms on the Tegra X2.

For a more detailed comparison, we present the execution times for each individual layer in Table \ref{tblSpeed}. Each layer's name is followed with the dimensions of the input, except for the convolution layers where the dimensions are for the kernels, and the input dimensions can be inferred from the previous layer. This table compares the execution time of our binarized implementations with the full-precision versions of the same layer in cuDNN on the GTX1080. We omit from the table the computation times for ReLU activations, which are present in the full-precision version of the network, but are absent from the binarized version. We also omit the last 2 fully-connected layers since they are too small and in most practical applications it would be more efficient to implement them on the CPU. We include the computation time for packing the outputs of the previous layer in the binarized version of the fully-connected layer for a fair comparison. The results in Table \ref{tblSpeed} have been obtained directly from the Nvidia Visual Profiler. %after the results of the first fully-connected layer has been obtained. 

It should be noted that the runtime for the fully-connected layer for full-precision cuDNN in Table \ref{tblSpeed} includes a matrix transposition. The run time excluding matrix transposition is about $100 \mu s$; however, it is a necessary step for evaluating this layer. Our full-precision matrix multiplication kernel is in fact $2\times$ slower than cuBLAS (as measured in this network), yet a significant speed-up is still achievable through binarization.

%\expendable{3}For the firefly, the results fluctuate very widely which made it difficult to obtain a result to report, so we decided to run the test for 1000 frames and repeat the process 10 times and report the best recorded results for all implementations.

% \begin{equation}
% \mathrm{IJV}= \frac{1}{K}\sum_{i=0}^{K-1}\vert \Vert \mathbf{s}_i {- \mathbf{s}}_{p(i)} \Vert_2 -\Vert \mathbf{u}_i \Vert_2 \vert,
% \label{eq:MAE}
% \end{equation}
% \begin{equation}
% \mathrm{IJV}= \frac{1}{K}\sum_{i=0}^{K-1}\left\vert \Vert {\mathbf{s}}_i {- {\mathbf{s}}}_{p(i)} \Vert_2 - \Vert \mathbf{v}_i {- \mathbf{v}}_{p(i)} \Vert_2\right\vert,
% \label{eq:MAE}
% \end{equation}

% \begin{figure}[h]
%    \center {\includegraphics[scale=0.60]
%   {mae.eps}}
 
%   \caption{IJV measurements for each model, shown at each frame, averaged over all samples.}  \label{fig:ijv}
% \end{figure}

% Measuring the joint relationships on their own may not always be an accurate quantification of the quality of motion, since it is possible that the network outputs sequences with little to no movement, and yet small IJV values. Therefore, we use an additional metric that measures motion \textit{energy}, which for a sample $\mathbf{F}\in\mathbb{R}^{m\times n}$  can be computed as follows:
% \begin{equation}
% E=\frac{1}{m\times n}{\sum^{m}_{i=1}\sum^{n}_{j=2}(F_{i,j}-F_{i,j-1})^2}\label{eq:energy}
% \end{equation}

 
% ... LSTM2 as it shows a smoothly increasing yet large overall IJV error, which is explained by the large energy variance in its generated motion sequences. 
% A straight-forward metric is the euclidean distance between consecutive frames, which can be a reasonable approach to quantify the motion similarity between consecutive frames. We compute this result using the equation
% \begin{equation}
% MID= \frac{1}{n}\sum^{n}_{j=2}\big\Vert(\mathbf{f}_{j}-\mathbf{f}_{j-1})\big\Vert_2,
% \label{eq:mid1}
% \end{equation}
% where we denote by $\mathbf{f}_j\in\mathbb{R}^{192\times1}$  the $j$th frame in the sequence. This result can then be averaged over all samples.

% \begin{table} 
% \caption{Analysis results of the motion sequences generated by the three models, averaged over all 500 samples and all 400 frames per sample.}
% \centering % Centers the table on the page, comment out to left-justify
% \begin{tabular}{l c c c c} % The final bracket specifies the number of columns in the table along with left and right borders which are specified using vertical bars (|); each column can be left, right or center-justified using l, r or c. To specify a precise width, use p{width}, e.g. p{5cm}
% \toprule % Top horizontal line
% & \multicolumn{3}{c}{Avg. Measurements Over All Samples} \\ % Amalgamating several columns into one cell is done using the \multicolumn command as seen on this line
% \cmidrule(l){2-4} % Horizontal line spanning less than the full width of the table - you can add (r) or (l) just before the opening curly bracket to shorten the rule on the left or right side
% % Joint & LSTM1 & LSTM2 & LSTM3 \\ % Column names row
% % \midrule % In-table horizontal line
% % MAE		&0.0196 &  0.0167 & 0.0188 \\ 
% % MID  &	0.2034& 0.2185 & 0.1437 \\  
% % Energy$_{avg}$ & 2.6512 & 2.8149& 1.9892\\
% % Energy$_{std}$ & 1.2017 & 1.9342 & 1.0882\\
% % \midrule % In-table horizontal line
% % \bottomrule % Bottom horizontal line

% Joint & LSTM1 & LSTM2 & {LSTM3} & Ground Truth\\ % Column names row
% \midrule % In-table horizontal line
% IJV$_{avg}$		&1.150 cm  & 1.472 cm &  \bf{0.939 cm}& 0.0 cm\\ 
% MID$_{avg}$  &	\bf 6.675  cm& 13.643 cm& {6.975 cm} & 5.608 cm \\  
% Energy$_{avg}$  & 1.128 & 6.306 & {1.028} & 0.409\\
% Energy$_{std}$ & 1.247 & 15.156 & {1.387} & 1.319\\ 
% \midrule % In-table horizontal line
% \bottomrule % Bottom horizontal line

% \end{tabular}
%  % Table caption, can be commented out if no caption is required
% \label{tableMAE} % A label for referencing this table elsewhere,

% \end{table}


\begin{table} 
\caption{Impact of different input-binarization schemes on classification accuracy}
\centering 
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c c c c} 
& \multicolumn{1}{c}{} \\ 

Method & Accuracy \\ % Column names row
\midrule % In-table horizontal line
LBP	&  {92.06\%}\\ 
Thresholding  Grayscale& ${89.16\%}$ \\
Thresholding RGB &  ${92.52\%}$ \\
No input binarization    &${94.20\%}$ \\ 
Full-precision network    &${97.09\%}$ \\ 
\midrule % In-table horizontal line
\bottomrule % Bottom horizontal line

\end{tabular}
%}
% Table caption, can be commented out if no caption is required
\label{tblAccuracy} % A label for referencing this table elsewhere,
%\response{not sure if we should include the multiplications and additions. They feel like they were forced into the table just to fill space. But if needed, I can still add that part.}
%\jani{Yeah, I think you can omit the muls and adds. Time is anyhow running short, so it is better to concentrate on polishing the paper otherwise. If needed, those can be added for the camera ready if the paper is accepted.}
\end{table}




% Table \ref{tableMAE} shows these measurements for all samples for each model, in order to provide a general comparison of the quality of the motion generated by each network. We convert the IJV results to the physical unit of centimeters in order to provide an intuitive sense of the errors. IJV and MID are calculated as shown before and averaged over all samples. On the third row, Energy$_{avg}$ shows the average energy as calculated by equation \ref{eq:energy}. The last row, Energy$_{std}$, shows the standard deviation of the energy of all samples. The fourth column shows these measurements for the $500$ samples from the data set reserved for motion synthesis. It can be argued that LSTM3 shows the best capacity for novel and realistic motion synthesis and maintaining inter-joint relationships.

% Figure \ref{fig:heatmap} shows the IJV values for each joint averaged across all models and all samples. This illustration aims to highlights the joints which seem to be most problematic for our models to learn. One possible explanation for the severity of the errors at the fingers is that finger motions can be very complex, while leg joints, for example, remain mostly similar over the data set.
% \begin{figure}[h]
%   \center {\includegraphics[scale=0.6]{figure.eps}}
%   \caption {IJV values averaged across all models for each joint.}
%   \label{fig:heatmap}
% \end{figure}


%   \label{fig:tradeoffAcc}
% \begin{figure}[h]
% \center  {\includegraphics[scale=0.60]
%   {figure.eps}}
%    \center {\includegraphics[scale=0.60]
%   {figure.eps}}
 
%   \caption{Accuracy/Speed tradeoff. x: speed, y: accuracy, color: platform}  \label{fig:mae1}
% \end{figure}





% \begin{figure}[h]
% \center  {\includegraphics[scale=0.60]
%   {figure.eps}}
 
%   \caption{(placeholder graph) Binarized Accuracy/Speed tradeoff graph. x: speed, y: accuracy, color: platform}  
%     \label{fig:tradeoffAcc}
% \end{figure}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion and Future Work}
We presented an efficient implementation of a binarized convolutional neural network on GPUs that can achieve a significant decrease in runtime while reasonably preserving classification accuracy. In the future we wish to restructure our algorithms to achieve a similar performance improvement on other embedded platforms. We are also planning to extend this work to alternative convolution algorithms such as implicit GEMM, which can be faster than explicit GEMM. Finally, we plan to extend our study of how input binarization impacts classification accuracy on larger datasets with more difficult classification tasks.
% {We also wish to further the work on vehicle type classification. Vheicle type classification can be difficult, for one, it is difficult to assign a single class. In future, option is to use multi-label classification. Another alternative approach is to evaluate accuracy through top-2 or top-3; however, this would require a larger dataset and a narrower definition of vehicle types and it may not be fair to use the top-2 or top-3 accuracy in our case since th our dataset is categorized into 4 classes.} 


% conference papers do not normally have an appendix


% \section{Supplementary Material}
% Video links showing the motion sequences generated by each model. Sequences with the blue skeleton are the input sequences feed to the network. Motion sequences in green are generated by the network.
% \begin{itemize}
% \item \href{https://www.youtube.com/watch?v=v_WTs5EXT3c}{LSTM1}
% \item  \href{https://www.youtube.com/watch?v=jTxqGgwFWPk}{LSTM2}
% \item  \href{https://www.youtube.com/watch?v=9-Eol83xZZA}{LSTM3}
% \end{itemize}

% use section* for acknowledgment
%\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
%  \section*{Acknowledgments}
%This work was supported supporters.
%\else
  % regular IEEE prefers the singular form
\section*{Acknowledgment}
This work was funded by the Academy of Finland
project 309903 CoEfNet.

%\vspace{-4mm}

%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{99}

% \bibitem{Bayer}
% Bayer J, Osendorfer C. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610. 2014 Nov 27.
% \label{bayer}

% \bibitem{bergstra}
% Bergstra J, Breuleux O, Bastien F, Lamblin P, Pascanu R, Desjardins G, Turian J, Warde-Farley D, Bengio Y. Theano: A CPU and GPU math compiler in Python. InProc. 9th Python in Science Conf 2010 Jun (pp. 1-7).
% \label{theano}

% \bibitem{}
% Chollet, F. Keras. https://github.com/
% fchollet/keras, 2015.
% \label{keras}

% \bibitem{chung}
% Chung J, Kastner K, Dinh L, Goel K, Courville AC, Bengio Y. A recurrent latent variable model for sequential data. In Advances in neural information processing systems 2015 (pp. 2980-2988).
% \label{chung}
\bibitem{}
Alvarez J, Petersson L. Decomposeme: Simplifying convnets for end-to-end learning. arXiv preprint arXiv:1606.05426. 2016 Jun 17.
\label{refDecompose}

\bibitem{}
Chellapilla K, Puri S, Simard P. High performance convolutional neural networks for document processing. In Tenth International Workshop on Frontiers in Handwriting Recognition 2006 Oct 23.
 \label{refGEMMConv}
 
\bibitem{}
Chetlur S, Woolley C, Vandermersch P, Cohen J, Tran J, Catanzaro B, Shelhamer E. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759. 2014 Oct 3.
\label{refCUDNN}

\bibitem{}
Ciregan D, Meier U, Schmidhuber J. Multi-column deep neural networks for image classification. In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on 2012.
\label{augmentimage1}

\bibitem{}
Courbariaux M, Bengio Y, David JP. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems 2015 (pp. 3123-3131).
\label{refBinNet2}


% \bibitem{}
% Elman JL. Finding structure in time. Cognitive science. 1990 Mar 1;14(2):179-211.
% \label{elman}


% \bibitem{}
% Gers FA, Schmidhuber J, Cummins F. Learning to forget: Continual prediction with LSTM. Neural computation. 2000.
% \label{learningtoforget}
% \bibitem{glorot}
% Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In Aistats 2010 (Vol. 9, pp. 249-256).
% \label{glorot}

\bibitem{glorot2}
  Glorot X, Bordes A, Bengio Y. Deep Sparse Rectifier Neural Networks. In Aistats 2011 (Vol. 15, No. 106, p. 275).
 \label{refRELU}


\bibitem{}
Graves A, Jaitly N. Towards End-To-End Speech Recognition with Recurrent Neural Networks. In ICML 2014 (Vol. 14, pp. 1764-1772).
\label{speechrecognition}


\bibitem{Gregor}
Gregor K, Danihelka I, Graves A, Rezende DJ, Wierstra D. DRAW: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623. 2015.
\label{DRAW}


% \bibitem{}
% Grochow K, Martin SL, Hertzmann A, Popović Z. Style-based inverse kinematics. InACM transactions on graphics (TOG) 2004 Aug 8 (Vol. 23, No. 3, pp. 522-531). ACM.
% \label{groshow}


% \bibitem{}
% Hammer B. On the approximation capability of recurrent neural networks. Neurocomputing. 2000 Mar 31;31(1):107-23.
% \label{rnnpower2}


% \bibitem{Hochreiter1}
% Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.
% \label{rnndifficulty}

\bibitem{}
Han S, Pool J, Tran J, Dally W. Learning both weights and connections for efficient neural network. InAdvances in neural information processing systems 2015 (pp. 1135-1143).
\label{refPruning}

\bibitem{}
Hinton, G. Neural networks for machine learning. Coursera, video lectures. 2012.
\label{refSTE}

\bibitem{}
Hubara I, Courbariaux M, Soudry D, El-Yaniv R, Bengio Y. Binarized neural networks. In Advances in neural information processing systems 2016 (pp. 4107-4115).
\label{refBinNet}

\bibitem{}
Huttunen H, Yancheshmeh FS, Chen K. Car type recognition with deep neural networks. In Intelligent Vehicles Symposium (IV), IEEE 2016 Jun 19 (pp. 1115-1120).
\label{refHeikkiCar}


% \bibitem{}
% Jaitly N, Hinton GE. Vocal tract length perturbation (VTLP) improves speech recognition. InProc. ICML Workshop on Deep Learning for Audio, Speech and Language 2013 Jun.
% \label{audioclassification1}
\bibitem{}
Jaderberg M, Vedaldi A, Zisserman A. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866. 2014 May 15.
\label{refSepConv}

\bibitem{}
Kanda N, Takeda R, Obuchi Y. Elastic spectral distortion for low resource speech recognition with deep neural networks. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on 2013 (pp. 309-314).
\label{audioclassification2}


\bibitem{}
Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 2014 Dec 22.
\label{refAdam}

\bibitem{}
Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems 2012 (pp. 1097-1105).
\label{imageclassification}

\bibitem{}
Lawrence S, Giles CL, Tsoi AC, Back AD. Face recognition: A convolutional neural-network approach. IEEE transactions on neural networks. 1997;8(1):98-113.
\label{facerecognition1}

% \bibitem{}
% Mukai T, Kuriyama S. Geostatistical motion interpolation. InACM Transactions on Graphics (TOG) 2005 Jul 31 (Vol. 24, No. 3, pp. 1062-1070). ACM.
% \label{geo}


 
% \bibitem{ng}
% Ng A. CS229 Lecture notes. CS229 Lecture notes. 2000;1(1):1-3.
% \label{ng}

\bibitem{}
Ojala T, Pietik\"ainen M, M\"aenp\"a\"a T. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. IEEE Transactions on pattern analysis and machine intelligence. 2002 Jul; 24(7):971-87.
\label{refLBP}

\bibitem{}
Parkhi OM, Vedaldi A, Zisserman A. Deep Face Recognition. In BMVC 2015 (Vol. 1, No. 3, p. 6).
\label{facerecognition2}

\bibitem{}
Pedersoli F, Tzanetakis G, Tagliasacchi A. Espresso: Efficient Forward Propagation for BCNNs, ICLR 2018 (to appear).
\label{refBinGPU}

% \bibitem{}
% Park SI, Shin HJ, Kim TH, Shin SY. On‐line motion blending for real‐time locomotion generation. Computer Animation and Virtual Worlds. 2004 Jul 1;15(3‐4):125-38.
% \label{park}

% \bibitem{}
% Pascanu R, Mikolov T, Bengio Y. Understanding the exploding gradient problem. CoRR, abs/1211.5063. 2012 Nov.
% \label{explodinggradient}



% \bibitem{}
% Peng X, Berseth G, Yin K, van de Panne M. DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning. ACM Transactions on Graphics. 2017; 36(4):41.
% \label{vandepanne2}

% \bibitem{Rose}
% Rose C, Cohen MF, Bodenheimer B. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics and Applications. 1998 Sep;18(5):32-40.
% \label{motionverbs}

% \bibitem{}
% Rose III CF, Sloan PP, Cohen MF. Artist‐Directed Inverse‐Kinematics Using Radial Basis Function Interpolation. In Computer Graphics Forum 2001 Sep 1 (Vol. 20, No. 3, pp. 239-250). Blackwell Publishers Ltd.
% \label{rose2}
\bibitem{}
Rastegari M, Ordonez V, Redmon J, Farhadi A. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision 2016 Oct 8 (pp. 525-542). 
\label{refXnor}

% \bibitem{}
% Saxe AM, McClelland JL, Ganguli S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120. 2013.
% \label{saxe}

% Schmidhuber, J., 1992. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2), pp.234-242.
% \label{rnndifficulty3}



% \bibitem{}
% Siegelmann HT, Sontag ED. On the computational power of neural nets. Journal of computer and system sciences. 1995 Feb 1;50(1):132-50.
% \label{rnnpower1}



% \bibitem{}
% Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).
% \label{rnntestgeneration}

\bibitem{}
Tan G, Li L, Triechle S, Phillips E, Bao Y, Sun N. Fast implementation of DGEMM on Fermi GPU. In Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis 2011 Nov 12 (p. 35).
\label{refDGEMM}

\bibitem{}
Tieleman T, Hinton G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning. 2012.
\label{refRMSprop}





% \bibitem{}
% Waibel A. Modular construction of time-delay neural networks for speech recognition. Neural computation. 1989;1(1):39-46.
% \label{timedelay}


% \bibitem{}
% Wang JM, Fleet DJ, Hertzmann A. Gaussian process dynamical models. InNIPS 2005 Dec 5 (Vol. 18, p. 3).
% \label{wang2}


% \bibitem{Webb}
% Webb AR. Statistical pattern recognition. John Wiley \& Sons; 2003 Jul 25.
% \label{stpattrecg}


\end{thebibliography}




% that's all folks
\end{document}