\documentclass{article}
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage[sort,comma,authoryear,round]{natbib}


\title{Curvature Explains Loss of Plasticity}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Alex Lewandowski, Haruto Tanaka, Dale Schuurmans$^{\ddagger, \star}$, Marlos C. Machado$^{\star}$  \\
Department of Computing Science\\
University of Alberta\\
Edmonton, Canada\\
\texttt{\{lewandowski, haruto, daes, machado\}@ualberta.ca} \\
$^{\ddagger}$Google DeepMind, $^{\star}$Canada CIFAR AI Chair\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience.
  Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity.
  In this paper, we offer a consistent explanation for plasticity loss,
  based on an assertion that neural networks lose directions of curvature during training and that plasticity loss can be attributed to this reduction in curvature.
  To support such a claim, we provide a systematic empirical investigation of plasticity loss across several continual supervised learning problems.
  Our findings illustrate that curvature loss  coincides with and sometimes precedes plasticity loss, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings.
  Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional regularizer that proves to be effective across the problem settings considered.
\end{abstract}
\vspace{-4mm}

\section{Introduction}
\vspace{-2mm}

A longstanding goal of machine learning research is to develop algorithms that can learn \textit{continually} and cope  with unforeseen changes in their environment  \citep{sutton07}.
Current learning algorithms, however,  struggle to learn from dynamically changing targets and are unable to adapt gracefully to unforeseen environment changes during the learning process \citep{zilly21,abbas23_loss_plast_contin_deep_reinf_learn,lyle23_under,dohare23_maint_plast_deep_contin_learn}.
Such limitations can be seen to be a byproduct of following a supervised learning paradigm that  assumes  the problem is stationary.
Recently, there has been growing recognition of the fact
that there are limitations to what can be learned from a fixed and 
unchanging dataset \citep{hoffmann22} and that there are implicit non-stationarities in many learning problems of interest \citep{igl21_trans_non_gener_deep_reinf_learn}.


The concept of plasticity has been receiving growing attention in the continual learning literature, where the loss of plasticity---a reduction in the ability to learn new things \citep{dohare23_maint_plast_deep_contin_learn,lyle23_under}---has been noted as a critical shortcoming in current learning algorithms.
That is, learning algorithms that are performant in the non-continual learning setting often struggle when applied to continual learning problems, exhibiting a striking loss of plasticity such that  learning  slows down or even halts after successive changes in the learning environment.
Such a loss of plasticity can be readily observed in settings where a neural network must continue to learn after changes  occur in the observations or targets.

Several aspects of a learning algorithm and modeling architecture have been found to contribute to, or mitigate, loss of plasticity, such as the optimizer \citep{dohare23_maint_plast_deep_contin_learn}, the step-size \citep{ash20_warm_start_neural_networ_train,berariu2021study}, the number of updates \citep{lyle23_under}, the activation function \citep{abbas23_loss_plast_contin_deep_reinf_learn} and the use of specific regularizers \citep{dohare21_contin_backp,kumar23_maint_plast_regen_regul,lyle21_under_preven_capac_loss_reinf_learn}.
Such factors hint that there might be simpler underlying mechanisms that are the root cause mechanisms that cause
of the loss of plasticity.
For example, the success of several methods that regularize the neural network towards properties of the initialization suggests that some property of the initialization mitigates loss of plasticity.
Unfortunately, no such property has yet been identified.
Some factors that have been found to correlate with loss of plasticity include, a decrease in the gradient norm \citep{abbas23_loss_plast_contin_deep_reinf_learn}, a reduction in the rank of the learned representations weight tensors \citep{kumar20_implic_under_param_inhib_data,gulcehre22_rl}, neuron dormancy \citep{sokar23_dorman_neuron_phenom_deep_reinf_learn}, and an increase in the norm of the parameters \citep{nikishin22_primac_bias_deep_reinf_learn}.


In this paper, we propose that loss of plasticity can be explained by a loss of curvature.
Our work contributes to a growing literature on the importance of curvature for understanding neural network dynamics \citep{cohen2021gradient,hochreiter97_flat,fort19_emerg}.
Within the continual learning and plasticity literature, the assertion that curvature is related to plasticity is relatively new \citep{lyle23_under}.
In contrast to the general assertion that curvature is related to plasticity, our work specifically posits that loss of curvature explains loss of plasticity.
In particular, we provide empirical evidence that supports a claim that
loss of plasticity co-occurs with a reduction in the rank of the Hessian of the training objective at the beginning of a new task.

This work improves the understanding of plasticity loss in continual supervised learning by,

\begin{enumerate}
  \item systematically outlining  previous explanations for loss of plasticity, and providing counter-examples that illustrate conditions where these explanations fail to account for particular increases or decreases in plasticity;
  \item positing that loss of curvature, as measured as the change in the rank of the Hessian of the training objective, is the underlying cause of loss of plasticity and demonstrating that loss of curvature coincides with loss of plasticity across several factors and benchmarks;
  \item proposing a regularizer that keeps the distribution of weights close to the initialization distribution, and showing that this allows the parameters to move further from initialization while preserving curvature 
  for successive tasks.
\end{enumerate}

\section{Contributing Factors and Explanations for Plasticity Loss}
\vspace{-2mm}
\label{sec:fac}

Before defining what we mean by loss of plasticity, we will outline the continual supervised learning problem setting that we study.
We assume  the  learning algorithm operates in an on-line manner, processing an observation-target pair $(x,y)$ and updating the neural network parameters $\theta$ after each pair.
In continual supervised learning, there is a periodic and regular change every $U$ updates to the distribution generating the observations or targets.
For every $U$ updates, the neural network must minimize an objective defined over a new distribution and we refer to this new distribution as a \textit{task}.
The problem setting is designed so that the task at any point in time has the same difficulty.\footnote{A suitably initialized neural network should be able to equally minimize the objective for any of the tasks we consider.}
We are primarily interested in error at the the end of task $K$ averaged across all observations in that task, $J_{K} = J(\theta_{UK}) = \mathbb{E}_{p_{k}}(\ell(f_{\theta_{UK}}(x), y))$ for some loss function $\ell$.

Although loss of plasticity is an empirically observed phenomenon, the way it is measured in the literature can vary.
In this paper, we will use loss of plasticity to refer to the phenomenon that $J_{K}$ increases rather than decreases as a function of $K$.
Some works evaluate learning and plasticity with the average online error over the learning trajectory within a task \citep{elsayed23_utilit_pertur_gradien_descen,dohare23_maint_plast_deep_contin_learn,kumar23_maint_plast_regen_regul}.
While the two are related, we focus on the error at the end of the task to remove the effect of increasing error at the beginning of a subsequent task, which can suggest loss of plasticity in the online error even if the error at the end of a task is constant (see Appendix \ref{appendix:online_error}).
Because the error increases as more tasks are seen, this means that the neural network is struggling to learn from the new experience given by the subsequent task.

\subsection{Factors That Can Contribute to Loss of Plasticity}
\vspace{-2mm}

Given a concrete notion of plasticity,
we reiterate that the underlying mechanisms leading to loss of plasticity have been so-far elusive.
This is partly because multiple factors can contribute to loss of plasticity, or even mitigate it.
In this section, we summarize some of these potential factors before surveying previous explanations for the underlying mechanism behind loss of plasticity.

  \textbf{Optimizer}\hspace{2mm}
        Optimizers that were designed and tuned for stationary distributions can exacerbate loss of plasticity in non-stationary settings.
        For instance, the work by \cite{lyle23_under} showed empirically that Adam \citep{kingma14_adam} can be unstable on a subsequent task due to its momentum and scaling from a previous task.

  \textbf{Step-size}\hspace{2mm}
        In addition to the optimizer, an overlooked fact is that the step-size itself is a crucial factor in both contributing to and mitigating the loss of plasticity.
        The study by \cite{berariu2021study}, for example, suggests that the plasticity loss is preventable by amplifying the randomness of gradients with a larger step-size.
        These findings extend to other hyper-parameters of the optimizer: Properly tuned hyper-parameters for Adam, for example, can mitigate loss of plasticity leading to policy collapse in reinforcement learning \citep{dohare23_overc_polic_collap_deep_reinf_learn,lyle23_under}.


  \textbf{Update Budget}\hspace{2mm}
      Continual supervised learning experiments, including those below, use a fixed number of update steps per task \citep[e.g.,][]{abbas23_loss_plast_contin_deep_reinf_learn, elsayed23_utilit_pertur_gradien_descen, javed2019meta}.
      Despite the fact that the individual tasks themselves are of the same difficulty, the neural network might not be able to escape its task-specific initialization within the pre-determined update budget.
        \cite{lyle23_under} show that, as the number of update steps increase in a first task, learning slows down on a subsequent task, requiring even more update steps on the subsequent task to reach the same training error.

  \textbf{Activation function}\hspace{2mm}
        One major factor that can contribute or mitigate loss of plasticity is the activation function.
        Work by \cite{abbas23_loss_plast_contin_deep_reinf_learn} suggests that, in the reinforcement learning setting, loss of plasticity occurs because of an increasing portion of hidden units being set to zero by \texttt{ReLU} activations \citep{fukushima75_cognit,nair10_rectif_linear_units_improv_restr_boltz_machin}.
        The authors then show that \texttt{CReLU} \citep{shang16_under} prevents saturation, mitigating plasticity loss almost entirely.
        However, other works have shown that plasticity loss can still occur with non-saturating activation functions \citep{dohare21_contin_backp, dohare23_maint_plast_deep_contin_learn}, such as \texttt{leaky-ReLU} \citep{xu15_empir}.

  \textbf{Properties of the objective function and regularizer}\hspace{2mm}
        The objective function being optimized greatly influences the optimization landscape and, hence, plasticity \citep{lyle23_under, lyle21_under_preven_capac_loss_reinf_learn,ziyin23_symmet_leads_struc_const_learn}.
        Regularization is one modification to the objective function that helps mitigate loss of plasticity.
        When L2 regularization is properly tuned, for example, it can help mitigate loss of plasticity \citep{dohare23_maint_plast_deep_contin_learn}.
        Another regularizer that mitigates loss of plasticity is regenerative regularization ,which regularizes towards the parameter initialization \citep{kumar23_maint_plast_regen_regul}.

\subsection{Previous Explanations for Loss of Plasticity}
\vspace{-2mm}

Not only are there several factors that could possibly contribute to loss of plasticity, there are also several explanations for this phenomenon. We survey the recent explanations of loss of plasticity below. In the next section, we present results showing that none of these explanations are sufficient to explain loss of plasticity across different problem settings.

  \textbf{Decreasing update/gradient norm}\hspace{2mm} The simplest explanation for loss of plasticity is that the update norm goes to zero. This would mean that the parameters of the neural network stop changing, eliminating all plasticity. This tends to occur with a decrease in the norm of the features for particular layers \citep{abbas23_loss_plast_contin_deep_reinf_learn,nikishin22_primac_bias_deep_reinf_learn}.

  \textbf{Dormant Neurons}\hspace{2mm} Another explanation for plasticity loss is a steady increase in the number of inactive neurons, namely, the dormant neuron phenomenon \citep{sokar23_dorman_neuron_phenom_deep_reinf_learn}.
  It is hypothesized that fewer active neurons decreases a neural network's expressivity, leading to loss of plasticity.

   \textbf{Decreasing representation rank}\hspace{2mm} Related to the effective capacity of a neural network, lower representation rank suggests that fewer features are being represented by the neural network \citep{kumar20_implic_under_param_inhib_data}.
  It has been observed that decreasing representation rank is sometimes correlated with loss of plasticity \citep{lyle23_under,kumar23_maint_plast_regen_regul,dohare23_maint_plast_deep_contin_learn}.
  With a similar intuition as the representation rank, a decreasing rank of the weight matrices may prevent a neural network from representing certain features and hence lower plasticity \citep{lyle23_under, gulcehre22_rl}.

  \textbf{Increasing parameter norm}\hspace{2mm} An increasing parameter norm is sometimes associated with loss of plasticity in both continual and reinforcement learning \citep{nikishin22_primac_bias_deep_reinf_learn,dohare23_maint_plast_deep_contin_learn}, but it is not necessarily a cause \citep{lyle23_under}. The reason for parameter norms to increase and lead to loss of plasticity is not clear, perhaps suggesting a slow divergence in the training dynamics.

\section{Empirical Counter-examples of Previous Explanations}
\vspace{-2mm}
\label{sec:counter}

As a first step, we investigate the factors influencing and explanations of plasticity described in Section \ref{sec:fac}.
The goal of this section is primarily to provide counterexamples to the different explanations of loss of plasticity, showing that the existing explanations fail to fully explain the phenomenon.
To do so, we use the MNIST dataset \citep{lecun10_mnist} where the labels of each image are periodically shuffled.
While MNIST is a simple classification problem, label shuffling highlights the difficulties associated with maintaining plasticity and was previously demonstrated as such \citep{lyle23_under,kumar23_maint_plast_regen_regul}.
In this section, we focus on this MNIST task for its simplicity, showing that even in a simple setting, one can find counter-examples to previous explanations in the literature.
We emphasize that the goal here is merely to uncover simple counter-examples that refute  proposed explanations for loss of plasticity,
not investigate the phenomenon more broadly.
In Section \ref{sec:main_exp}, we extend our investigation to the other common benchmarks for loss of plasticity in continual supervised learning.

\paragraph{Methods}
In this experiment, we vary only the activation function between \texttt{ReLU}, \texttt{leaky-ReLU}, \texttt{tanh} and the \texttt{identity}.
Previous work found that the activation function has a significant effect on the plasticity of the neural network.
We train a neural network for 200 epochs per task with a total of 100 tasks and measure the error across all observations at the end of each task.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/confounding}
  \caption{All results have a shaded region corresponding to a 95\% confidence interval of the mean. Top left: All non-linear activation functions lose plasticity, but a deep linear network with \text{identity} activations does not. Rest: None of the aformentioned explanations explain the difference between plasticity loss of different activation functions. For neuron dormancy and the weight stable rank, some activation functions overlap and are not visible.
  }
  \label{fig:confounding}
\end{figure}

\paragraph{Results}
The main result of this experiment can be found in Figure \ref{fig:confounding}.
Our findings show that none of the aforementioned explanations of loss of plasticity explain the phenomenon across different activation functions in this simple problem.
In the top-left figure, we have the task-end batch training error as a function of the number of tasks. All non-linear activation functions can achieve low error on the first few tasks, but this error increases over time.
The deep linear network (a neural network with an \texttt{identity} activation function) is able to maintain a low training error for each of the 100 tasks.
This suggests that loss of plasticity only occurs with the non-linear activations, and that \texttt{ReLU} loses its plasticity the quickest.

The remaining plots in Figure \ref{fig:confounding} show the measurement of quantities pertaining to the aforementioned explanations of plasticity loss.
A decreasing update norm, for example, may seem like an intuitive explanation of loss of plasticity.
However, in the top-middle plot, we see that the update norm consistently increases for the \texttt{leaky-ReLU} activation function, making the explanation inconsistent.
A similar inconsistency in the rank of the representation makes that explanation inconsistent with loss of plasticity, too.
The same is true for neuron dormancy (measured as the entropy of the activations), weight norm and weight rank.
There exists at least one activation such that the trend in the training error does not agree with the trend in the explanation (see Appendix \ref{sec:appendix_analysis} for the further analysis).


\section{Measuring Curvature of A Changing Optimization Landscape}
\vspace{-2mm}

One missing piece in the explanations previously proposed is the curvature of the optimization landscape.
While previous work pointed out that curvature is connected to plasticity \citep{lyle23_under}, our work specifically posits that loss of curvature coincides with and, in some cases, precedes plasticity loss.
Our experiments in Section \ref{sec:main_exp} show that plasticity loss occurs when, at the beginning of each new task, the optimization landscape has diminishing curvature.

Before presenting empirical evidence of the relationship between plasticity and curvature, we briefly describe the effect that task and data changes have on the curvature of the optimization landscape.
The local curvature of the optimization landscape at a particular parameter $\theta$, is expressed by the Hessian of the objective function, $H_{t}(\theta) = \nabla_{\theta}^{2}J_{t}(\theta)\big|_{\theta = \theta_{t}}$.
We hide the dependence on data in the training objective and the Hessian, and instead index both by time.
For conventional supervised learning problems, the training objective is stationary because the input and target distributions do not change.
For non-stationary learning problems, like those in continual learning, the distributions underlying the observations and targets will change over time.
Thus there can be changes in the objective, gradient and Hessian that is due to the data changing and not due to the parameters.

We are interested in how the curvature of the optimization landscape changes when the task changes.
Of particular interest is the rank of the Hessian after a task change because, if it is decreasing, then there are fewer directions of curvature to learn on the new task.
For simplicity, and in alignment with our experiments, we will assume that each task has an update budget $U$.
Then the training objective on the $K$-th task will be stationary for $U$ steps.
When the task changes, at $t = UK + 1$, the Hessian will change due to changes in the data - and not due to changes in the parameters. 
We measure the rank at the beginning of the task by the \emph{stable rank}, $\text{srank}\left(H_{UK+1}(\theta)\right)$, where $\text{srank}(M) = \min \left\{ j \, : \, \frac{ \sum_{i=1}^{j}\sigma_{i}(M)}{\sum_{i=1}^{d}\sigma_{i}(M)} > 0.99\right\}$ is the stable rank and $\{\sigma_{i}(M)\}_{i=1}^{d}$ are the singular values arranged in decreasing order.
The stable rank specifies the number of basis vectors needed to represent most of image of the matrix $M$ \citep{yang19_harnes_struc_value_based_plann_reinf_learn,kumar20_implic_under_param_inhib_data}.


\subsection{Partial Blockwise Hessian Evaluation}

Neural networks typically have a large number of parameters, requiring approximations to the Hessian due to the massive computational overhead for producing the actual values.
Diagonal approximations are employed to capture curvature information relevant for optimization \citep{elsayed22_hessc,becker88_improv,lecun89_optim}, but this approximation is too coarse-grained and over-estimates the rank.
There are low-rank approximations of the Hessian \citep{roux07_topmoum}, these too are problematic because we aim to measure the rank of the Hessian.
The empirical Fisher information approximates the gradient using the outer-product of gradients, but has been shown to not capture curvature information, especially away from local minima \citep{kunstner19_limit_fisher}.
Other approximations that use the Fisher information matrix require stochastic models, which limit their applicability \citep{martens2015optimizing}

Our approach, a partial blockwise Hessian, builds off recent work studying the layerwise Hessian \citep{sankar21}.
Even if we could calculate and store an approximation to the full Hessian, we would not be able to calculate the srank because a singular value decomposition has a cubic-time complexity.
At the same time, a purely layerwise approximation of the Hessian cannot capture changes in the Hessian due to the target changing for piece-wise linear activations like \texttt{ReLU} and \texttt{leaky-ReLU}.
We denote the layer-specific parameters by $\theta^{(l)}$ and the entire parameter set as $\theta$, then the Hessian with respect to $\theta^{(l)}$ for piece-wise linear activations is independent of $y$, $\frac{d}{dy}\nabla_{\theta^{(l)}}^{2}J(\theta) = 0$, making the layerwise Hessian an unsuitable for our empirical analysis.

The partial blockwise Hessian is the exact Hessian for a subset of the layers.
We argue that the blockwise Hessian is a reasonable choice because the layerwise Hessian was shown to approximate the statistics of the entire Hessian \citep{wu20_dissec}.
Taking the Hessian with respect to parameters of more than one layer allows us to to analyze the Hessian at the task
change boundary, because
$\frac{d}{dy} \nabla_{\theta^{(l)}, \theta^{(l-1)}}^{2}J(\theta) \not = 0$.
In practice, we take the blockwise Hessian with respect to the parameters of the last 2 layers because we found it sufficient to capture curvature changes while being small enough to calculate the singular value decomposition throughout training.

\section{Preserving Curvature Throughout Continual Learning}
\vspace{-2mm}

In the previous section, we claimed that loss of curvature can explain loss of plasticity.
If curvature is lost over the course of learning, then one solution to this problem could be to regularize towards the curvature present at initialization.
While explicit Hessian regularization would be computationally costly, previous work has found that even L2 regularization can mitigate loss of plasticity \citep{kumar23_maint_plast_regen_regul,dohare21_contin_backp,lyle21_under_preven_capac_loss_reinf_learn}, without attributing this benefit to preserving curvature.
These methods, however, do more than just prevent loss of curvature, they also prevent parameters from growing large (subject to the regularization parameter's strength).
L2 regularization and weight decay, for example, mitigate plasticity loss but also prevent the parameters from deviating far from the origin.
This could limit the types of functions that the neural network can learn, requiring the regularization strength to be tuned.


We propose a new regularizer that is simple and that gives the parameters more leeway for moving from the initialization, while preserving the desirable plasticity and curvature properties of the initialization.
Our regularizer penalizes the distribution of parameters if it is far from the distribution of the randomly initialized parameters.
At initialization, the parameters at layer $l$ are sampled i.i.d. $\mathbf{\theta}_{i,j} \sim p^{(l,0)}(\theta)$ according to some pre-determined distribution, such as the Glorot initialization \citep{glorot10_under}.
During training, the distribution of parameters at any particular layer is no longer known and the parameters are neither independent nor identically distributed.
However, it is still possible to regularize the empirical distribution towards the initialization distribution by using the empirical Wasserstein metric \citep{bobkov19_one_kantor}.
We denote the flattened parameter matrix for layer $l$ at time $t$ by $\mathbf{\bar{\theta}}^{(l,t)}$, then the squared Wasserstein-2 distance between the distribution of parameters at initialization and the current parameter distribution is defined as,
$$ \mathcal{W}_2^2\left(p^{(l,0)},p^{(l,t)}\right) = \frac{1}{d}\sum_{i=1}^{d} \left(\mathbf{\bar{\theta}}_{(i)}^{(l,t)} - \mathbf{\bar{\theta}}_{(i)}^{(l,0)}\right)^{2}.$$
The order statistics of the paramater is denoted by  $\theta_{(i)}^{(l,t)}$ and represents the $i$-th smallest parameter, which is sorted independently from the initialization.
In the above equation, we are taking the L2 difference between the order statistics of each layer's parameters at initialization and a point in training.
The \textit{Wasserstein initialization regularizer} uses the empirical Wasserstein distance for each layer of the neural network.


A recent alternative, regenerative regularization, regularizes the neural network parameters towards their initialization \citep{kumar23_maint_plast_regen_regul}.
This regularizer mitigates plasticity loss, but it also prevents the neural network parameters from deviating far from the initialization.
The difference between the Wasserstein initialization regularization and the regenerative regularizer is the fact that we take the difference in the order statistics.
As we will show, however, the Wasserstein regularizer also mitigates plasticity loss but allows the neural networks parameters to deviate far from the initialization.

\section{Experiments: Curvature Changes in Plasticity Benchmarks}
\vspace{-2mm}
\label{sec:main_exp}

We now investigate our claim that loss of curvature, as measured by the reduction in the rank of the Hessian,
explains loss of plasticity.
Our experiments use the three most common continual learning benchmarks that exhibit loss of plasticity:
\begin{itemize}
  \item MNIST with periodic pixel observation permutation, a commonly used benchmark across continual learning \citep{goodfellow13_empir_inves_catas_forget_gradien,kumar23_maint_plast_regen_regul,dohare23_maint_plast_deep_contin_learn,elsayed23_utilit_pertur_gradien_descen,zenke17_contin}.
  \item MNIST with periodic label shuffling, an increasingly used variant of permuted MNIST and noted for its increased difficulty \citep{kumar23_maint_plast_regen_regul,lyle23_under,elsayed23_utilit_pertur_gradien_descen}. This is the problem setting studied in Section \ref{sec:counter}.
  \item CIFAR-10 \citep{Krizhevsky09learningmultiple} with periodic label shuffling. This is an increasingly common problem setting for plasticity, and it is difficult due to the relative complexity of image observations in CIFAR and the difficulty of relearning labels \citep{kumar23_maint_plast_regen_regul,lyle23_under,sokar23_dorman_neuron_phenom_deep_reinf_learn}.
\end{itemize}


\begin{figure}[h!]
  \centering
\includegraphics[width=0.99\linewidth]{plots/polished_plots/unreg}
\caption{Results with unregularized objectives. Top Row: Batch Error at task end. Bottom Row: sRank of Hessian at task-beginning. Left: Label-shuffled MNIST. Middle: Observation-permuted MNIST. Right: Label-shuffled CIFAR-10.}
  \label{fig:unreg}
\end{figure}
To provide evidence of the claim that curvature explains loss of plasticity, we conduct an in-depth analysis of the change of curvature in continual supervised learning.
We first show that curvature is a consistent explanation across different problem settings.
Afterwards, we show that the gradient stays contained in the top-subspace of the Hessian, which shrinks over the course of continual learning.
Lastly, we show that regularization which has been demonstrated to be effective in mitigating loss of plasticity also mitigates loss of curvature.


\subsection{Does Loss of Curvature Explain Loss of Plasticity?}
\vspace{-2mm}

We present the results on the three problem settings in Figure \ref{fig:unreg}.
Like the results in Section \ref{sec:counter}, loss of plasticity occurs in problem settings when non-linear activations are used.
Loss of curvature tends to co-occur with loss of plasticity for the non-linear activations, providing a consistent explanation of the phenomenon compared to previous explanations.
There is also some evidence that loss of curvature may precede loss of plasticity.
In the label-shuffled MNIST experiment, \texttt{tanh} loses curvature before the error begins to increase.
This finding suggests that loss of curvature may be the underlying cause of plasticity loss.


\begin{figure}[h!]
  \centering
\includegraphics[width=0.99\linewidth]{plots/polished_plots/explain_update.pdf}
    \caption{Label-shuffled MNIST with \texttt{leaky-ReLU} and different step-sizes. From left to right: task-end batch error, average update norm, gradient-hessian overlap at task-end and task-end Hessian rank.}
  \label{fig:unreg_leaky_abl}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/shuffle_reg}

\caption{Results on label-shuffled MNIST with regularization. All regularizers mitigate loss of plasticity and loss of curvature to some degree but weight decay is unstable. Top: Batch training error at the end of task. Middle: Hessian srank at beginning of task. Bottom: Distance from current parameters to original random initialization.}
  \label{fig:shuffle_mnist_sgd_reg}
\end{figure}
\subsection{How Does Loss of Curvature Affect Learning?}
\vspace{-2mm}

Having demonstrated that loss of curvature co-occurs with loss of plasticity, we now investigate how loss of curvature affects the gradients and learning.
Our goal is to explain why the update norms can be increasing despite loss of plasticity.
We focus on \texttt{leaky-ReLU} with different step-sizes on the label-shuffled problem, which exhibits loss of plasticity at every step-size but an increase in the average update norm over training.
In Figure \ref{fig:unreg_leaky_abl}, we plot the overlap of the gradient and the Hessian-gradient product at task-end, given by $\frac{g^{T}Hg}{\|g\|\|Hg\|}$, which measures whether the gradient is contained in the top subspace of the Hessian \citep{gur-ari18_gradien_descen_happen_tiny_subsp}.
Gradients at the end of the task tend to be contained in the top-subspace of the Hessian which is also decreasing in rank.
We hypothesize that the update norm is increasing because the gradients along the small subspace are more likely to add up to a large momentum, but that this is not enough to escape the flat initialization on the subsequent tasks.
This is evidenced by the stabilization of the update-norm for step-size $\alpha = 0.01$ which co-occurs with the increase in the task-end Hessian rank. See Appendix \ref{appendix:curvature_update} for more examples of this phenomenon.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/perm_reg}
\caption{Results on observation-permuted MNIST with regularization. The Wasserstein regularizer achieves a lower error, is able to travel further from its initialization and maintains curvature.}
  \label{fig:perm_mnist_sgd_reg}
\end{figure}
\subsection{Can Regularization Preserve Curvature?}
\vspace{-2mm}

We now investigate whether regularization prevents loss of plasticity and, if it does, whether it also prevents loss of curvature.
Our results for the three problem settings and activation functions are summarized in Figures \ref{fig:shuffle_mnist_sgd_reg}, \ref{fig:perm_mnist_sgd_reg}, and \ref{fig:cifar_reg}.
We see that all regularizers are able to prevent plasticity loss to some degree.
L2 regularization is the least performant, and it is often unstable.
Across activation functions, we find that the Wasserstein regularizer is able to reliably achieve a lower error compared to the other regularizers.
The benefit of the Wasserstein regularizer can be seen from the bottom row of plots: the parameters are able to deviate more from the initialization compared to the other regularizers.
Additionally, the Wasserstein regularizer is less sensitive to the hyperparameter controlling the regularization strength (see Appendix \ref{appendix:reg_hyperparam}).


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/cifar_reg}
\caption{Results on label-shuffled CIFAR with regularization. Regenerative regularization is unstable with piecewise linear activations, but is comparable to the Wasserstein regularizer for \texttt{tanh} activations. Due to instability, results with weight decay can be found in Appendix \ref{appendix:cifar_weightdecay}.}
  \label{fig:cifar_reg}
\end{figure}

\section{Discussion}

We have demonstrated how loss of curvature is a more consistent explanation for loss of plasticity when compared to previous explanations offered in the literature.
One limitation of our work is that we study an approximation to the Hessian.
Our experiments suggest that this approximation of the Hessian is enough to capture changes in curvature, but more insight may be found from theoretical study of the entire Hessian.
Another limitation is that it is not clear what drives neural networks to lose curvature during training.
Understanding the dynamics of training neural networks with gradient descent, however, is an active research area even in supervised learning.
It will be increasingly pertinent to understand what drives neural network training dynamics to lose curvature so as to develop principled algorithms for continual learning.

Our experimental evidence demonstrates that, when loss of plasticity occurs, there is a reduction in curvature as measured by the rank of the Hessian at the beginning of subsequent tasks.
When loss of plasticity does not occur, curvature remains relatively constant.
Unlike previous explanations, this phenomenon is consistent across different datasets, non-stationarities, step-sizes, and activation functions.
Lastly, we investigated the effect of regularization on plasticity, finding that regularization tends to preserve curvature but can be sensitive to the regularization strength.
We proposed a simple distributional regularizer that proves effective in maintaining plasticity across the problem settings we consider, while maintaining curvature and being less hyperparameter sensitive.

\subsubsection*{Acknowledgments}
We thank thank Shibhansh Dohare, Khurram Javed, Farzane Aminmansour and Mohamed Elsayed for early discussions about loss of plasticity. The research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chair Program, the Digital Research Alliance of Canada and Alberta Innovates Graduate Student Scholarship.

% \newpage
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix
\section*{Appendix}

% You may include other additional sections here.

\input{appendix.tex}

\end{document}
