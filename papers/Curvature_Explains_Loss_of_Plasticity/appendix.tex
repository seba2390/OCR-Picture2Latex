

\section{Additional analysis on counter-examples}
\label{sec:appendix_analysis}
In the body of paper, we provided a high-level partial analysis on Figure \ref{fig:confounding}, and concluded that each metric corresponding to previous explanations implies the inconsistency with the loss of plasticity (i.e. increase in training error).
Here, we aim to compliment that high-level analysis on Figure \ref{fig:confounding} by providing detailed explanation on how each metrics are inconsistent with the batch error.

\begin{enumerate}
    \item \textbf{Average Update Norm} (top-middle):
    Plot measures the average L1 norm of the parameter updates for each task.
    \texttt{Leaky-ReLU} and \texttt{ReLU} shows the completely opposite trend.
    The former constantly increases from 0.03 up to 0.1, while the latter decreases from 0.025 to 0.
    Other activations, \texttt{identity} and \texttt{tanh}, remains at the level of 0.03.
    Since \texttt{leaky-ReLU} suffers from the plasticity loss, the fact that \texttt{leaky-ReLU} experiencing the increase in update norm is inconsistent.

    \item \textbf{Stable Rank of Representation} (top-right):
    The plot measures the stable rank of the representation for each task.
    Among all the activations, \texttt{identity} retains the highest level of rank throughout the experiment.
    It remains around the level of 30.
    Activations except for \texttt{identity} initially holds an stable rank of 25.
    While \texttt{tanh} sustain its rank over the tasks, \texttt{ReLU}/\texttt{leaky-ReLU} face with the distinct levels of drop.
    \texttt{ReLU} drastically drops its representation rank to 5.
    On the other hand, the rank of \texttt{leaky-ReLU} eventually converge to 20.
    % This suggests that, under this experimental setting, the stable rank of representation is mostly consistent with models utilizing activations other than \texttt{tanh}.
    The representation rank is inconsistent because  \texttt{tanh} retains its representation rank at 25, while it still experience plasticity loss.


    \item \textbf{Dormant Nuerons} (bottom-left):
    Neuron dormancy are measured by the entropy of the absolute value of the features for each task, which captures the notion of dormancy that activations can concentrate on a small subset of features.
    The plot shows that the activations other than \texttt{ReLU} retains its level of activation entropy.
    Precisely, \texttt{identity} and \texttt{tanh} are overlapping at the level of 3.5, while \texttt{leaky-ReLU} remains at the level of $2.6 ~- 2.7$.
    Both \texttt{tanh} and \texttt{leaky-ReLU} experience plasticity loss but the neuron dormancy is non-decreasing, making the explanation inconsistent.
    % Opposingly, \texttt{ReLU} experiences significant drop from 2.5 to 0.7 ~ 0.8, which is consistent with its degree of plasticity loss.

    \item \textbf{Weight Norm} (bottom-middle):
    The plot presents the L1 norm of the weights at the end of each task.
    For all the activations, the weight norm is $0.05 ~- 0.1$ at the beginning, and monotonically increase as task progresses.
    In order for weight norm to be a consistent metric, it must correlate to the trend of batch error.
    However, our result disagree with this criteria, as the weight norm for \texttt{ReLU} (which has the worst batch error) only hit the 0.1 at the last task while others reach to the higher value.

    \item \textbf{Stable Rank of Weights} (bottom-right):
    Similarly to the stable rank of representation, the plot tracks change in the stable rank of model weights per task.
    With \texttt{leaky-ReLU}, there is a drop in the weight rank from 32 to 31, but all other activations concentrate and remain at the level of 32.
    This suggests that the weight rank has very little to do with plasticity.

    % As the \texttt{ReLU} retains its weight rank, and \texttt{identity} deviates to a lower rank at the early phase of experiments, the metric is inconsistent.

\end{enumerate}

\newpage
  \subsection{Correlation Plots}
  In this section, we present the same results as in Section 3 in the form of a correlation plot. In the correlation plot, we plot the batch error at the end of a task on the x-axis and a measurement that explains plasticity on the y-axis. We then plot the line of best fit for those points for each activation function. This provides a quantiative analysis of the previous explanations of plasticity loss. We also include the correlation plot with the sRank of the Hessian, which was presented in Section 4.

  We are interested in a consistent trend, where the slope of the fitted line does not change for the different activation functions. In Figure \ref{fig:shuffle_reg_corr}, we see that the only explanation that exhibits a consistent trend is the Hessian sRank. The previous explanations of plasticity are inconsistent because they exhibit mixed positive, zero and negative correlations.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/confounding_corr}
  \caption{
      Correlation plots for different explanations of plasticity loss. The points plot pairs of (batch error, plasticity measurement). The line is fit with a simple linear regression to the points. The color of the line and plot correspond to the activation function. The only consistent explanation is the Hessian sRank, demonstrating that higher rank Hessian's at the beginning of a task lead to lower errors at the end of the task. Previously proposed explanations exhibit mixed positive, zero and negative correlations.
    }
  \label{fig:shuffle_reg_corr}
\end{figure}



\clearpage
\section{Experimental Details}

\subsection{Label-Shuffled MNIST}
\label{appendix:exp_details_label_mnist}
Non-stationary variant of the ordinary (stationary) supervised classification problem on MNIST dataset.
The source of non-stationarity in this problem is the periodical random shuffling of labels, irrespective of the original class labels.
The dataset consists of 1280 uniformly sampled MNIST image-label pairs.
We iterate over the dataset for 200 epochs in the experiments in the main paper, but ablate for different number of epochs in \ref{appendix:epochs}.
After 200 epochs,  the labels will be reshuffled within the same dataset, producing the new task.
Each gradient updates are performed with the batch of 16 datapoints, hence the update capacity per task is 80.
The total number of tasks is 100.
The architecture is a 3 hidden layer feed-forward neural network with widths $(128, 64, 32)$.
We use the Adam optimizer with default hyperparameters, and sweep over the learning rates $\{0.01, 0.001, 0.0001\}$
We average over 30 seeds for the unregularized experiments and average over 10 seeds for the regularized experiments.
For the regularized experiments, we sweep over the regularization strength of $\{1.0, 0.01, 0.001, 0.0001\}$.



\subsection{Observation-Permuted MNIST}
The overall problem framework is identical to the label-shuffled MNIST, except for the source of non-stationarity.
The non-stationarity is introduced by reordering the positions of pixels in each input image, while label remains the same throughout the experiment.
At the beginning of each task, the permutation of pixels are shuffled, and each input images are uniformly shuffled according to that permutation.
Other basic components of experiment does not vary from label-shuffled MNIST problem.
% The architecture is a 3 hidden layer feed-forward neural network with widths 128, 64, 32.
% We use the Adam optimizer with default hyperparameters, and sweep over the learning rates $\{0.01, 0.001, 0.0001\}$
% We average over 30 seeds for the unregularized experiments and average over 10 seeds for the regularized experiments.
% For the regularized experiments, we sweep over the regularization strength of $\{1.0, 0.01, 0.001, 0.0001\}$.

\subsection{Label-Shuffled CIFAR}
A non-stationary supervised classification problem using the CIFAR dataset, similar to the label-shuffled MNIST problem.
The only difference from label-shuffled MNIST problem are the use of CIFAR10 dataset and the number of tasks to evaluate on.
Similarly in the label-shuffled MNIST problem, this problem uniformly samples 1280 datapoints from CIFAR10.
The batch size remainns the same, and so update budget does.
Number of tasks, however, is reduced to 50 due to the increase in computational cost and because the problem difficulty is higher, resulting in plasticity loss sooner.
The architecture uses 3 convolutional layers with stride 2 and $(16, 32, 64)$ filers respectively before flattening and using a 2 layer feed-forward neural network with widths $(64, 32)$.
We use the Adam optimizer with default hyperparameters, and sweep over the learning rates $\{0.01, 0.001, 0.0001\}$
We average over 30 seeds for the unregularized experiments and average over 10 seeds for the regularized experiments.
For the regularized experiments, we sweep over the regularization strength of $\{1.0, 0.01, 0.001, 0.0001\}$.

\subsection{Continual ImageNet}
We use the Continual ImageNet environment proposed by \cite{dohare23_maint_plast_deep_contin_learn}. While we train the same convolutional neural network described by \cite{dohare23_maint_plast_deep_contin_learn}, we use a batch size of 100 and train for 200 epochs with Adam using a step size of $0.001$. The regularization strength used was $0.01$, dropout rate is $0.5$ and we reset the last layer at the beginning of each new task. Our results are averaged over 20 seeds.

% TODO
% A non-stationary supervised classification problem using the CIFAR dataset, similar to the label-shuffled MNIST problem.
% The only difference from label-shuffled MNIST problem are the use of CIFAR10 dataset and the number of tasks to evaluate on.
% Similarly in the label-shuffled MNIST problem, this problem uniformly samples 1280 datapoints from CIFAR10.
% The batch size remainns the same, and so update budget does.
% Number of tasks, however, is reduced to 50 due to the increase in computational cost and because the problem difficulty is higher, resulting in plasticity loss sooner.
% The architecture uses 3 convolutional layers with stride 2 and $(16, 32, 64)$ filers respectively before flattening and using a 2 layer feed-forward neural network with widths $(64, 32)$.
% We use the Adam optimizer with default hyperparameters, and sweep over the learning rates $\{0.01, 0.001, 0.0001\}$
% We average over 30 seeds for the unregularized experiments and average over 10 seeds for the regularized experiments.
% For the regularized experiments, we sweep over the regularization strength of $\{1.0, 0.01, 0.001, 0.0001\}$.



\section{Additional Results}

\subsection{Update Budget Effect on Plasticity }
By varying the number of epochs in a task, the neural network is able to learn more on a task, perhaps allowing the neural network to escape loss of plasticity.
Unfortunately, increasing the number of epochs only marginally delays the onset of loss of plasticity.
Plasticity loss still occurs, but reduction in curvature is a consistent predictor of this phenomenon.
\label{appendix:epochs}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/updates}
    \caption{Ablating number of epochs per task on label-shuffled MNIST}
  \label{fig:shuffle_reg_ablation}
\end{figure}

\subsection{Loss of Curvature's Effect on Updates}
\label{appendix:curvature_update}

We provide more results on the update norm, Hessian rank, and Hessian-Gradient overlap at the beginning and end of a task, as well as across more problem settings.
The findings are consistent that the update norm is influenced by whether the gradient overlaps with the top-subspace of the Hessian and the size of that subspace.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/explain_update_shffule_full}
    \caption{Explaining why neural networks with \texttt{leaky-ReLU} lose plasticity despite an increasing update norm in label-shuffled MNIST.}
  \label{fig:explain_mnist_shuffle}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/explain_update_perm}
    \caption{Explaining why neural networks with \texttt{leaky-ReLU} lose plasticity despite an increasing update norm in observation-permuted MNIST.}
  \label{fig:explain_mnist_perm}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/explain_update_cifar}
    \caption{Explaining why neural networks with \texttt{leaky-ReLU} lose plasticity despite an increasing update norm in label-shuffled CIFAR.}
  \label{fig:explain_cifar}
\end{figure}


\clearpage

\subsection{CIFAR Results Including Weight Decay}
\label{appendix:cifar_weightdecay}

We omitted the weight decay results for CIFAR in the main paper due to instability. Notably, weight decay exhibits the biggest reduction in its curvature with \texttt{leaky-ReLU} where it is most unstable.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/cifar_reg_full}
  \caption{Results on label-shuffled CIFAR with regularization. Regenerative regularization is unstable with piecewise linear activations, but is comparable to the Wasserstein regularizer for \texttt{tanh} activations.
    }
  \end{figure}


\clearpage
\subsection{Online Error Can Be Influenced By High Start Error}
\label{appendix:online_error}
Average online error is another metric for studying loss of plasticity, but it can misdiagnose the phenomenon.
Even if a neural network maintains a consistent error at the end of a task, its blue online error can increase due to an increase in its error at the beginning of a task.
But the error at the beginning of a task is not controllable, because it is due to a non-stationarity in the experience.
Thus, we focus on the batch error at task end alone.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/shuffle_online_reg}
    \caption{Average online error can misdiagnose loss of plasticity due to an increase in the start error.}
    \label{fig:online_shuffle}
  \end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/perm_online_reg}
    \caption{Even if the start-error is constant, average online error has an implicit bias towards algorithms that start with low error, but this is not something the algorithm itself can control or optimize for because it is due to a task change.}
    \label{fig:online_perm}
  \end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/cifar_online_reg}
    \caption{Average online error can fail to distinguish between algorithms who achieve different performance levels because of high error at the start of a task.}
    \label{fig:online_cifar}
  \end{figure}

\clearpage
\subsection{Regularizer Hyperparameter Sensitivity}
\label{appendix:reg_hyperparam}

The plots below show the batch error at the end of a task for different regularization strengths. Compared to weight decayy and regenerative regularization, the Wasserstein regularizer is able to reach and maintain a lower error across most problems and activation functions.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/shuffle_reg_abl}
    \caption{Hyperparameter ablation for label-shuffled MNIST.}
  \label{fig:shuffle_reg_ablation}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/perm_reg_abl}
    \caption{Hyperparameter ablation for observation-permuted MNIST.}
  \label{fig:perm_reg_ablation}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/polished_plots/cifar_reg_abl}
    \caption{Hyperparameter ablation for label-shuffled CIFAR.}
  \label{fig:cifar_reg_ablation}
\end{figure}



\clearpage

\section{Results on Continual Imagenet}
We conduct a thorough experimental study on Continual Imagenet. Our analysis in the first subsection follows the analysis in the main paper. The next subsection studies the effect of neural network modifications. The next two subsections analyze the correlations of the Hessian sRank with plasticity loss, showing a consistent trend, while demonstarting that the sharpness of the Hessian does not show a consistent trend.

\subsection{Plasticity and Curvature with Regularizers}


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_original}
  \caption{
     Both the Wasserstein and Regenerative regularizer are able to mitigate loss of plasticity exhibited by deep networks with relu activations and preserve curvature. We again find that the Wasserstein regularizer is able to achieve lower error than regenerative regularization.
  }
  \label{fig:imagenet_og_results}
\end{figure}


\clearpage
\subsection{Plasticity and Curvature with Neural Network Modifications}
We compare the Wasserstein regularizer against several neural network modifications that have been shown to help reduce loss of plasticity. In particular, we use \texttt{CReLU} \citep{abbas23_loss_plast_contin_deep_reinf_learn, shang16_under}, layernorm \citep{lyle23_under, ba16_layer_normal}, dropout \citep{srivastava14_dropout} and last layer reset \citep{nikishin22_primac_bias_deep_reinf_learn}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_new}
  \caption{
      Comparing against neural network modifications, and including the Wasserstein regularizer. Dropout is not able to learn to solve any task. \texttt{CReLU}, last layer reset and layernorm fail to mitigate loss of plasticity in this problem. All of these network modifications also fail to preserve curvature.
  }
  \label{fig:imagenet_new_results}
\end{figure}

\newpage
\subsection{Additional Correlation Plots with sRank}
We provide correlation plots for the batch error at the end of the task and the sRank of the Hessian.
The color of the line and plot correspond to different activation functions, regularizers and/or neural network modifications.
The plotted points are pairs of (batch error, srank of Hessian).
The line is fit with a simple linear regression.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_original_corr}
  \caption{
      The Hessian sRank explains loss of plasticity for all non-linear activation functions while also demonstrating that the linear function does not exhibit loss of plasticity.
  }
  \label{fig:imagenet_og_rank}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_new_corr}
  \caption{
      The Hessian sRank also consistently explains loss of plasticity for the neural network modifications.
  }
  \label{fig:imagenet_new_rank}
\end{figure}

\newpage
\subsection{Additional Correlation Plots with Sharpness}
We provide correlation plots for the batch error at the end of the task and the sharpness (the maximum singular value of the Hessian).
The color of the line and plot correspond to different activation functions, regularizers and/or neural network modifications.
The plotted points are pairs of (batch error, maximum singular value of Hessian).
The line is fit with a simple linear regression.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_original_corr_max}
  \caption{
      % Correlation plot for sharpness as an explanation for loss of plasticity.
      For the regularizers, sharpness shows a similar trend to the rank. One issue is that the line of best fit for identity has a slight positive slope.
  }
  \label{fig:imagenet_og_max}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_new_corr_max}
  \caption{
      % Correlation plot for sharpness as an explanation for loss of plasticity.
      Sharpness is not a good explanation for loss of plasticity as it exhibits mixed positive, zero and negative correlations.
  }
  \label{fig:imagenet_new_max}
\end{figure}

\newpage

\subsection{Inter-task Online Learning Curves}
Laslty, we look at the early learning behavior within the first 10 tasks for the regularizers. We find that the regularizers do not slow learning down, and that they do benefit from non-linear feature representations and achieve, locally, a lower error than the linear function. Note that the online error plotted here is with respect to a changing model, and thus the results cannot be compared to the batch error at the end of the task in a straighforward way.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/imagenet_curves}
  \caption{
      Online error over the first 10 tasks. ReLU and Wasserstein regularized ReLU all attain a lower error on the first task compared to the deep linear network. As the number of tasks increases, ReLU loses plasticity but both regularizers are able to maintain plasticity and benefit from non-linear representations.
  }
  \label{fig:online_learning_curves}
\end{figure}

\clearpage

\section{Results on MNIST with Larger Subset}
We expand the results on MNIST to a larger subset of images. Although loss of plasticity occurs on a subset of 1280 MNIST images, this larger MNIST subset of 12800 is more difficult to fit: a linear model with 784*10 (input dim $\times$ output dim) effective parameters can no longer achieve a low error.
The results in this experiment are similar but more pronounced than in the smaller subset of MNIST. The linear model does not lose plasticity but it also can no longer achieve low error. The non-linear activation functions can achieve a much lower error in the first task but, due to loss of plasticity, eventually becomes worse than the linear model. Regularization mitigates loss of plasticity, and loss of plasiticity is well explained by the Hessian sRank. Feature rank and Hessian sRank exhibit no clear relationship.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/larger_mnist}
  \caption{
      Larger subsets of MNIST are more difficult to fit using a linear model. Curvature, as measured by the Hessian sRank, remains the best explanation of loss of plasticity. Regularization also remains a good mitigator for loss of plasticity. Like the smaller subset, regenerative regularizer does marignally better on this problem and requires more extensive hyperparameter tuning.
  }
  \label{fig:larger_mnist}
\end{figure}

\clearpage

\section{Results on MNIST with 13 layer Neural Network}

We validate our blockwise approximation to the Hessian sRank by using a much deeper 13 layer neural network and demonstrating the same consistent trend.
The architecture is similar to the previous MNIST experiment described in Appendix \ref{appendix:exp_details_label_mnist}, except we have 10 additional layers with 128 neurons.
Our results in Figure \ref{fig:deeper} shows that our blockwise aproximation to the Hessian sRank exhibits the same downward trend when loss of plasticity occurs and constant trend when linear or regularized non-linear.


\begin{figure}[h!]
  \centering
\includegraphics[width=0.99\linewidth]{plots/rebuttal_plots/deeper_confounding}
  \caption{
      A 13 layer neural network exhibits the same trend in our blockwise approximation to the Hessian.
      The larger capacity relu network still suffers from loss of plasticity, and this is reflected in the reduction of the Hessian sRank.
      The linear network maintains a moderate error and both regularized networks can achieve lower error, all while maintaining the Hessian sRank.
  }
  \label{fig:deeper}
\end{figure}


%%% Local Variables:
%%% TeX-master: "iclr2024_conference.tex"
%%% End:
