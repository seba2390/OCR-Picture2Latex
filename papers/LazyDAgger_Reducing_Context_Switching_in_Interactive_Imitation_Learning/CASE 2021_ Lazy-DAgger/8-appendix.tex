\normalsize
% \onecolumn
% \appendix
\section{Appendix}
\label{sec:appendix}
Here we provide further details on our MuJoCo experiments, hyperparameter sensitivity, simulated fabric experiments, and physical fabric experiments.

\subsection{MuJoCo}\label{ssec:mujoco-appdx}
As stated in the main text, we evaluate on the HalfCheetah-v2, Walker2D-v2, and Ant-v2 environments. To train the algorithmic supervisor, we utilize the TD3 implementation from OpenAI SpinningUp (\url{https://spinningup.openai.com/en/latest/}) with default hyperparameters and run for 100, 200, and 500 epochs respectively. The expert policies obtain rewards of 5330.78 $\pm$ 117.65, 3492.08 $\pm$ 1110.31, and 4492.88 $\pm$ 1580.42, respectively. Note that the experts for Walker2D and Ant have high variance, resulting in higher variance for the corresponding learning curves in Figure~\ref{fig:mujoco}. We provide the state space dimensionality $|S|$, action space dimensionality $|A|$, and \algabbr hyperparameters (see Algorithm~\ref{alg:main}) for each environment in Table~\ref{tab:mujoco-params}. The $\tausup$ value in the table is multiplied with the maximum possible action discrepancy $||a_{\rm high} - a_{\rm low}||_2^2$ to become the threshold for training $\ffilt(\cdot)$. In MuJoCo environments, $a_{\rm high} = \vec 1$ and $a_{\rm low} = -\vec 1$. The $\tausup$ value used for SafeDAgger in all experiments is chosen by the method provided in the paper introducing SafeDAgger~\cite{safe_dagger}: the threshold at which roughly 20\% of the initial offline dataset is classified as ``unsafe."

% Daniel: removed Adam citation.
For \algabbr and all baselines, the actor policy $\pirob(\cdot)$ is a neural network with 2 hidden layers with 256 neurons each, rectified linear unit (ReLU) activation, and hyperbolic tangent output activation. For \algabbr and SafeDAgger, the discrepancy classifier $\ffilt(\cdot)$ is a neural network with 2 hidden layers with 128 neurons each, ReLU activation, and sigmoid output activation. We take 2,000 gradient steps per epoch and optimize with Adam and learning rate 1e-3 for both neural networks. To collect $\mathcal{D}$ and $\dsafe$ in Algorithm~\ref{alg:main} and SafeDAgger, we randomly partition our dataset of 4,000 state-action pairs into 70\% (2,800 state-action pairs) for $\mathcal{D}$ and 30\% (1,200 state-action pairs) for $\dsafe$.

\begin{table}[!htbp]
%  \begin{center}
\centering
\resizebox{\columnwidth}{!}{
 \begin{tabular}{l | c | c | c | c | c | c | c } 
 Environment & $|S|$ & $|A|$ & $N$ & $T$ & $\tausup$ & $\tauauto$ & $\sigma^2$ \\ 
 \hline
 HalfCheetah & 16 & 7 & 10 & 5000 & 5e-3 & $\tausup$ / 10 & 0.30 \\
 Walker2D & 16 & 7 & 15 & 5000 & 5e-3 & $\tausup$ / 10 & 0.10 \\
 Ant & 111 & 8 & 15 & 5000 & 5e-3 & $\tausup$ / 2 & 0.05 \\
\end{tabular}}
\caption{\textbf{MuJoCo Hyperparameters:} $|S|$ and $|A|$ are aspects of the Gym environments while the other values are hyperparameters of LazyDAgger (Algorithm~\ref{alg:main}). Note that $T$ and $\tausup$ are the same across all environments, and that $\tauauto$ is a function of $\tausup$.}
\label{tab:mujoco-params}
\end{table}

\subsection{\algabbr Switching Thresholds}
As described in Section~\ref{subsec:ac-disc}, the main \algabbr hyperparameters are the safety thresholds for switching to supervisor control ($\tausup$) and returning to autonomous control ($\tauauto$). To tune these hyperparameters in practice, we initialize $\tausup$ and $\tauauto$ with the method in~\citet{safe_dagger}; again, this sets the safety threshold such that approximately 20\% of the initial dataset is unsafe. We then tune $\tausup$ higher to balance reducing the frequency of switching to the supervisor with allowing enough supervision for high policy performance. Finally we set $\tauauto$ as a multiple of $\tausup$, starting from $\tauauto = \tausup$ and tuning downward to balance improving the performance and increasing intervention length with keeping the total number of actions moderate. Note that since these parameters are not automatically set, we must re-run experiments for each change of parameter values. Since this tuning results in unnecessary supervisor burden, eliminating or mitigating this requirement is an important direction for future work.

\begin{figure}[t]
\center
\includegraphics[width=0.45\textwidth]{figures/tau_mujoco_thresh_heatv2.png}
\caption{
\algabbr $\tauauto$ and $\tausup$ sensitivity heatmaps across the 3 MuJoCo environments.  The x-axis denotes $\tausup$ and the y-axis denotes $\tauauto$.  Note that $\tauauto$ is a function of $\tausup$.  Each of the 3 environments was run 9 times with the different settings of $\tauauto$ and $\tausup$. As in Figure~\ref{fig:mujoco} we plot test reward, number of supervisor actions, and number of context switches.}
\label{fig:tau_heat_map}
\end{figure}

To analyze sensitivity to $\tauauto$ and $\tausup$, we plot the results of a grid search over parameter values on each of the MuJoCo environments in Figure~\ref{fig:tau_heat_map}. Note that a lighter color in the heatmap is more desirable for reward while a darker color is more desirable for actions and switches. We see that the supervisor burden in terms of actions and context switches is not very sensitive to the threshold as we increase $\tausup$ but jumps significantly for the very low setting ($\tausup = 5\times 10^{-4}$) as a large amount of data points are classified as unsafe. Similarly, we see that reward is relatively stable (note the small heatmap range for HalfCheetah) as we decrease $\tausup$ but can suffer for high values, as interventions are not requested frequently enough. Reward and supervisor burden are not as sensitive to $\tauauto$ but follow the same trends we expect, with higher reward and burden as $\tauauto$ decreases.

%, there a few trends.  One that is clear is that the smaller $\tausup$, the higher Num Actions is across the board.  In turn having a higher number of supervisor actions results in a higher test reward as the algorithm gets close to vanilla DAgger. In addition, reward in the Ant and Walker2d environments seems more sensitive to changes in $\tausup$ compared to the HalfCheetah environment.  The effect of $\tauauto$ is the most visible in the Ant environment, as $\tauauto$ increases relative to $\tausup$ the algorithm switches more frequently yet has fewer supervisor actions. This seems to indicate that when $\tausup$ is held constant, and $\tauauto$ increases, the length of interventions decrease.  
%Turning our analysis towards HalfCheetah we can see that most of the runs have very similar reward.  From the heat maps, both Num Actions and Num Switches are very similar to each other and most choices involving $\tausup = 0.05, 0.005$ are very good.
%Looking at Walker2d, Num Actions and Num Switches are again very similar to each other.  The columns with a $\tausup = 0.0005$ have a supervisor action at nearly every time step which explains their high reward.  The best tradeoff between reward and Num Actions looks to be $\tausup = 0.005$ and $\tauauto = \tausup/10$, which has high reward and a reasonable amount of supervisor actions.  Another choice which looks good is $\tausup = 0.005$ and $\tauauto = 10\tausup$.  This has a similar reward to the previous parameter selection, but this has a ratio between Num Actions and Num Switches significantly closer to 1 (indicating shorter interventions).
% Looking at the Ant environment, Num Actions are relatively different.  Many of the runs have a similar amount of reward except for the bottom 2 entries in the $\tausup = 0.05$ column which have a very low reward.  Choosing the best run in this case has much less tolerance compared to the other 2 environments.  Almost half of the runs have more than 55k supervisor actions out of only 75k total env steps.  The best run seems to be a $\tausup = \tauauto = 0.005$, with a high reward and again a reasonable amount of switches and burden.  

\subsection{Fabric Smoothing in Simulation}

\subsubsection{Fabric Simulator}\label{ssec:fabricsim}

% oracle corner policy, etc
% Daniel: removing a few of the citations to save space.
More information about the fabric simulator can be found in~\citet{seita_fabrics_2020}, but we review the salient details here. The fabric is modeled as a mass-spring system with a $n \times n$ square grid of point masses.
%The positions of the point masses are updated via Verlet integration~\cite{verlet_1967},
Self-collision is implemented by applying a repulsive force between points that are sufficiently close together. Blender (\url{https://blender.org/}) is used to render the fabric in $100 \times 100 \times 3$ RGB image observations. See Figure~\ref{fig:sim2sim} for an example observation. The actions are 4D vectors consisting of a pick point $(x, y) \in [-1,1]^2$ and a place point $(\Delta x, \Delta y) \in [-1,1]^2$, where $(x,y) = (-1,-1)$ corresponds to the bottom left corner of the plane while $(\Delta x, \Delta y)$ is multiplied by 2 to allow crossing the entire plane. In simulation, we initialize the fabric with coverage $41.1 \pm 3.4\%$ in the hardest (Tier 3) state distribution in~\cite{seita_fabrics_2020} and end episodes if we exceed 10 time steps, cover at least 92\% of the plane, are at least 20\% out of bounds, or have exceeded a tearing threshold in one of the springs. We use the same algorithmic supervisor as~\cite{seita_fabrics_2020}, which repeatedly picks the coordinates of the corner furthest from its goal position and pulls toward this goal position. To facilitate transfer to the real world, we use the domain randomization techniques in~\cite{seita_fabrics_2020} to vary the following parameters: 
\begin{itemize}
    \item Fabric RGB values uniformly between (0, 0, 128) and (115, 179, 255), centered around blue.
    \item Background plane RGB values uniformly between (102, 102, 102) and (153, 153, 153).
    \item RGB gamma correction uniformly between 0.7 and 1.3.
    \item Camera position ($x, y, z$) as $(0.5+\delta_1, 0.5+\delta_2, 1.45+\delta_3)$ meters, where each $\delta_i$ is sampled from $\mathcal{N}(0, 0.04)$.
    \item Camera rotation with Euler angles sampled from $\mathcal{N}(0, 90^{\degree} )$.
    \item Random noise at each pixel uniformly between -15 and 15.
\end{itemize}
For consistency, we use the same domain randomization in our sim-to-sim (``simulator to simulator") fabric smoothing experiments in Section \ref{ssec:fabric_sim_results}.

\subsubsection{Actor Policy and Discrepancy Classifier}\label{ssec:actorpol}
The actor policy is a convolutional neural network with the same architecture as~\cite{seita_fabrics_2020}, i.e. four convolutional layers with 32 3x3 filters followed by four fully connected layers. The parameters, ignoring biases for simplicity, are:

\footnotesize
\begin{verbatim}
policy/convnet/c1   864 params (3, 3, 3, 32)
policy/convnet/c2   9216 params (3, 3, 32, 32)
policy/convnet/c3   9216 params (3, 3, 32, 32)
policy/convnet/c4   9216 params (3, 3, 32, 32)
policy/fcnet/fc1    3276800 params (12800, 256)
policy/fcnet/fc2    65536 params (256, 256)
policy/fcnet/fc3    65536 params (256, 256)
policy/fcnet/fc4    1024 params (256, 4)
Total model parameters: 3.44 million
\end{verbatim}
\normalsize

The discrepancy classifier reuses the actor's convolutional layers by taking a forward pass through them. We do not backpropagate gradients through these layers when training the classifier, but rather fix these parameters after training the actor policy. The rest of the classifier network has three fully connected layers with the following parameters:

\footnotesize
\begin{verbatim}
policy/fcnet/fc1    3276800 params (12800, 256)
policy/fcnet/fc2    65536 params (256, 256)
policy/fcnet/fc3    1024 params (256, 4)
Total model parameters: 3.34 million
\end{verbatim}
\normalsize

\subsubsection{Training}
% hyperparams, pretraining details, graph
Due to the large amount of data required to train fabric smoothing policies, we pretrain the \textit{actor policy} (not the discrepancy classifier) in simulation. The learned policy is then fine-tuned to the new environment while the discrepancy classifier is trained from scratch. Since the algorithmic supervisor can be queried cheaply, we pretrain with DAgger as in~\cite{seita_fabrics_2020}. To further accelerate training, we parallelize environment interaction across 20 CPUs, and before DAgger iterations we pretrain with 100 epochs of Behavior Cloning on the dataset of 20,232 state-action pairs available at~\cite{seita_fabrics_2020}'s project website. Additional training hyperparameters are given in Table~\ref{tab:dagger-params} and the learning curve is given in Figure~\ref{fig:daggersim}. 

\begin{table}[!htbp]
%  \begin{center}
\centering
{
 \begin{tabular}{l r} 
Hyperparameter & Value \\
\hline
BC Epochs & 100 \\
DAgger Epochs & 100 \\
Parallel Environments & 20 \\
Gradient Steps per Epoch & 240 \\
Env Steps per Env per DAgger Epoch & 20 \\
Batch Size & 128 \\
Replay Buffer Size & 5e4 \\
Learning Rate & 1e-4 \\
L2 Regularization & 1e-5 \\
\end{tabular}}
\caption{\textbf{DAgger Hyperparameters.} After Behavior Cloning, each epoch of DAgger (1) runs the current policy and collects expert labels for 20 time steps in each of 20 parallel environments and then (2) takes 240 gradient steps on minibatches of size 128 sampled from the replay buffer.}
\label{tab:dagger-params}
\end{table}

\begin{figure}[t]
\center
\includegraphics[width=0.45\textwidth]{figures/daggersim.jpg}
\caption{
Behavior Cloning and DAgger performance across 10 test episodes evaluated every 10 epochs. Shading indicates 1 standard deviation. The first 100 epochs (left half) are Behavior Cloning epochs and the second 100 (right half) are DAgger epochs.}
\label{fig:daggersim}
\end{figure}

\subsubsection{Experiments}
In sim-to-sim experiments, the initial policy is trained on a 16x16 grid of fabric in a range of colors centered around blue with a spring constant of $k=10,000$. We then adapt this policy to a new simulator with different physics parameters and an altered visual appearance. Specifically, in the new simulation environment, the fabric is a higher fidelity 25x25 grid with a lower spring constant of $k=2,000$ and a color of (R, G, B) = (204, 51, 204) (i.e. pink), which is outside the range of colors produced by domain randomization (Section~\ref{ssec:fabricsim}). Hyperparameters are given in Table~\ref{tab:s2s-params}.

\begin{table}[!htbp]
%  \begin{center}
\centering
{
 \begin{tabular}{l r} 
Hyperparameter & Value \\
\hline
$N$ & 10 \\
$T$ & 20 \\
$\tausup$ & 0.001 \\
$\tauauto$ & $\tausup$  \\
$\sigma^2$ & 0.05 \\
Initial $|\mathcal{D}|$ & 1050 \\
Initial $|\dsafe|$ & 450 \\
Batch Size & 50 \\
Gradient Steps per Epoch & 200 \\
$\pi$ Learning Rate & 1e-4 \\
$\ffilt$ Learning Rate & 1e-3 \\
L2 Regularization & 1e-5 \\
\end{tabular}}
\caption{Hyperparameters for sim-to-sim fabric smoothing experiments, where the first 5 rows are \algabbr hyperparameters in Algorithm~\ref{alg:main}. Initial dataset sizes and batch size are in terms of images \textit{after} data augmentation, i.e. scaled up by a factor of 15 (see Section~\ref{ssec:dataaug}). Note that the offline data is split 70\%/30\% as in Section~\ref{ssec:mujoco-appdx}.}
\label{tab:s2s-params}
\end{table}

\subsection{Fabric Manipulation with the ABB YuMi}
\subsubsection{Experimental Setup}
We manipulate a brown 10" by 10" square piece of fabric with a single parallel jaw gripper as shown in Figure~\ref{fig:teaser}. The gripper is equipped with reverse tweezers for more precise picking of deformable materials. Neural network architecture is consistent with Section~\ref{ssec:actorpol} for both actor and safety classifier. We correct pick points that nearly miss the fabric by mapping to the nearest point on the mask of the fabric, which we segment from the images by color. To convert neural network actions to robot grasps, we run a standard camera calibration procedure and perform top-down grasps at a fixed depth. By controlling the width of the tweezers via the applied force on the gripper, we can reliably pick only the top layer of the fabric at a given pick point. We provide \algabbr-Execution hyperparameters in Table~\ref{tab:real-params}. %As each physical experiment is time-consuming, we were not able to compare many settings of these parameters; further hyperparameter tuning is likely to yield improved performance.

\subsubsection{Image Processing Pipeline}
In the simulator, the fabric is smoothed against a light background plane with the same size as the fully smoothed fabric (see Figure~\ref{fig:sim2sim}). Since the physical workspace is far larger than the fabric, we process each RGB image of the workspace by (1) taking a square crop, (2) rescaling to 100 $\times$ 100, and (3) denoising the image. Essentially we define a square crop of the workspace as the region to smooth and align against, and assume that the fabric starts in this region. These processed images are the observations that fill the replay buffer and are passed to the neural networks. 

\subsubsection{User Interface}
When the system solicits human intervention, an interactive user interface displays a scaled-up version of the current observation. The human is able to click and drag on the image to provide a pick point and pull vector, respectively. The interface captures the input as pixel locations and analytically converts it to the action space of the environment (i.e. $a \in [-1,1]^4$) for the robot to execute. See Figure~\ref{fig:ui} for a screen capture of the user interface.

\begin{figure}[t]
\center
\includegraphics[width=0.3\textwidth]{figures/ui.jpg}
\caption{
The user interface for human interventions. The current observation of the fabric state from the robot's perspective is displayed, with an overlaid green arrow indicating the action the human has just specified.}
\label{fig:ui}
\end{figure}

\subsubsection{Data Augmentation}\label{ssec:dataaug}
To prevent overfitting to the small amount of real data, before adding each state-action pair to the replay buffer, we make 10 copies of it with the following data augmentation procedure, with transformations applied in a random order:
\begin{itemize}
    \item Change contrast to 85-115\% of the original value.
    \item Change brightness to 90-110\% of the original value.
    \item Change saturation to 95-105\% of the original value.
    \item Add values uniformly between -10 and 10 to each channel of each pixel.
    \item Apply a Gaussian blur with $\sigma$ between 0 and 0.6.
    \item Add Gaussian noise with $\sigma$ between 0 and 3.
    \item With probability 0.8, apply an affine transform that (1) scales each axis independently to 98-102\% of its original size, (2) translates each axis independently by a value between -2\% and 2\%, and (3) rotates by a value between -5 and 5 degrees.
\end{itemize}

\begin{table}[!htbp]
%  \begin{center}
\centering
{
 \begin{tabular}{l r} 
Hyperparameter & Value \\
\hline
$\tausup$ & 0.004 \\
$\tauauto$ & $\tausup$ \\
$|\mathcal{D}|$ & 875 \\
$|\dsafe|$ & 375 \\
Batch Size & 50 \\
Gradient Steps per Epoch & 125 \\
$\pi$ Learning Rate & 1e-4 \\
$\ffilt$ Learning Rate & 1e-3 \\
L2 Regularization & 1e-5 \\
\end{tabular}}
\caption{Hyperparameters for physical fabric experiments provided in the same format as Table~\ref{tab:s2s-params}. Since this is at execution time, $N$, $T$ and $\sigma^2$ hyperparameters do not apply.}
\label{tab:real-params}
\end{table}