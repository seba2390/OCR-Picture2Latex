\section{Introduction}
\label{sec:introduction}
% In environments with uncertain dynamics, learning-based control strategies have recently achieved success on many challenging robotic tasks~\cite{kroemer2019review,dexterous-manip, qt-opt, SAC-applications, DeepVisuomotor, pan2017agile, fabric_vsf}. Typically, these algorithms involve specifying objectives for robotic agents and then optimizing these objectives through model-based~\cite{fabric_vsf, dexterous-manip, DeepVisuomotor, SAVED, abc-lmpc} or model-free~\cite{rl-end-to-end, SAC-applications, qt-opt} control strategies. However, specifying reward functions can often lead to unintended consequences~\cite{ng1999policy,leike2018scalable,AI-safety, recovery-rl} or be too sparse or uninformative for a robot to successfully optimize~\cite{vecerik2017leveraging,overcoming_exploration}, leading to slow and inefficient learning~\cite{zhu2018reinforcement,SAVED}. 
Imitation learning allows a robot to learn from human feedback and examples~\cite{argall2009survey,arora2018survey,osa2018algorithmic}.
% Human examples and feedback can take many forms such as demonstrations~\cite{abbeel2004apprenticeship,overcoming_exploration,argall2009survey}, preferences~\cite{christiano2017deep,palan2019learning}, rankings~\cite{trex,brown2020safe}, action labels \cite{dagger,DART,brown2018risk}, kinesthetic corrections~\cite{bajcsy2017learning,losey2018review}, and reward and value signals~\cite{knox2009interactively,macglashan2017interactive,kessler2019active}.
In particular, \textit{interactive imitation learning} (IL)~\cite{EIL,hg_dagger,safe_dagger}, in which a human supervisor periodically takes control of the robotic system during policy learning, has emerged as a popular imitation learning method, as interventions are a particularly intuitive form of human feedback~\cite{EIL}. However, a key challenge in interactive imitation learning is to reduce the burden that interventions place on the human supervisor~\cite{safe_dagger, hg_dagger}.
%, since each intervention requires a human to allocate time towards helping the robot during learning. 

\begin{figure}[htb!]
\center
\includegraphics[width=0.49\textwidth]{figures/splash8.png}
\caption{
\algname learns to cede control to a supervisor in states in which it estimates that its actions will significantly deviate from those of the supervisor. \algname reduces context switches between supervisor and autonomous control to reduce burden on a human supervisor working on multiple tasks.}%(Bottom) Rollouts of SafeDAgger and \algname on a physical sequential fabric manipulation task consisting of smoothing a crumpled fabric, precisely aligning the corners, and folding. Human supervisor actions are indicated with red arrows while robot actions are indicated in green. \algname switches back to robot control when confident in task completion, while SafeDAgger ends human intervention prematurely, resulting in poor task performance and more context switches.}
%\vspace*{-10pt}
\label{fig:teaser}
\end{figure}

One source of this burden is the cost of \textit{context switches} between human and robot control. 
% Context switching provides both a critical cognitive challenge for human supervisors and a significant time cost. Research in cognitive science and computer science suggests context switching can be distracting, frustrating and time-consuming for humans~\cite{facebook_bleh_2013, multitasking_crazy_2008, fragmented_work_2005, interrupted_work_2004}. 
% Each context switch requires significant cognitive burden~\cite{gopher2000switching,rubinstein2001executive} and
Context switches incur significant time cost, as a human must interrupt the task they are currently performing, acquire control of the robot, and gain sufficient situational awareness before beginning the intervention. As an illustrative example, consider a robot performing a task for which an action takes 1 time unit and an intervention requires two context switches (one at the start and one at the end). We define \emph{latency} $L$ as the number of time units associated with a single context switch. For instance, $L \gg 1$ for a human supervisor who will need to pause an ongoing task and walk over to a robot that requires assistance. If the supervisor takes control 10 times for 2 actions each, she spends $20L + 20$ time units helping the robot. In contrast, if the human takes control 2 times for 10 actions each, she spends only $4L + 20$ time units. The latter significantly reduces the burden on the supervisor. Furthermore, prior work suggests that frequent context switches can make it difficult for the supervisor to perform other tasks in parallel~\cite{swamy2020scaled} or gain enough situational awareness to provide useful interventions~\cite{think_going}.

We present \algname (Figure~\ref{fig:teaser}), an algorithm which initiates useful interventions while limiting context switches.
% to efficiently use human interventions to learn reliable robotic control policies. 
The name \algname is inspired by the concept of lazy evaluation in programming language theory~\cite{lazy-eval}, where expressions are evaluated only when required to reduce computational burden. As in SafeDAgger~\cite{safe_dagger}, \algname learns a meta-controller which determines when to context switch based on the estimated discrepancy between the learner and supervisor. However, unlike SafeDAgger, \algname reduces context switching by (1) introducing asymmetric switching criteria and (2) injecting noise into the supervisor control actions to widen the distribution of visited states. One appealing property of this improved meta-controller is that even after training, \algname can be applied at execution time to improve the safety and reliability of autonomous policies with minimal context switching.
% Sustained interventions naturally reduce the frequency of context switches, while the noise injection procedure improves robustness in the learned robot policy. \algname can both (1) help the robot learn a more robust policy during training and (2) augment any deployed imitation learning policy to improve task performance as well as fine-tune the policy in continuous learning settings without excessive switching.
We find that across 3 continuous control tasks in simulation, \algname achieves task performance on par with DAgger~\cite{dagger} with 88\% fewer supervisor actions than DAgger and 60\% fewer context switches than SafeDAgger. In physical fabric manipulation experiments, we observe similar results, and find that at execution time, \algname achieves 60\% better task performance than SafeDAgger with 60\% fewer context switches. %\ashwin{TODO: @Ryan: update this paragraph to reflect new exp results?}






% These challenges are further exacerbated when a single human must control a fleet of robots
% There are three key elements to interactive IL algorithms: mechanisms to (1) \emph{initiate}, (2) \emph{collect}, and (3) \emph{learn from} interventions \ryan{reviewer 5 was confused by request vs collect}. While (1) and (3) have been studied extensively in prior work~\cite{safe_dagger,hg_dagger,brown2018risk,palan2019learning,ensemble_dagger,EIL}, there has been comparatively little focus on (2): designing methods to collect informative interventions once an intervention is initiated. While interventions are often information-rich and intuitive to provide, the context switching needed to provide them can be frustrating and time-consuming for humans. Each context switch requires significant cognitive burden~\cite{gopher2000switching,rubinstein2001executive} and makes it difficult for the supervisor to perform other tasks in parallel~\cite{swamy2020scaled} or gain enough situational awareness to provide useful interventions~\cite{think_going}. \ryan{add a bunch of citations to justify context switching = bad} We hypothesize that context switching is a major source of supervisor burden in interactive IL and present an algorithm that can reduce the frequency of these switches \ryan{instead of hypothesizing this we can justify with the citations in the previous sentence}. Experiments suggest that sustained, infrequent demonstrations that expose the supervisor to a diverse set of states in uncertain regions can accelerate learning efficiency while reducing supervisor burden.

% We present \algname (Figure~\ref{fig:teaser}), an algorithm that extends prior work in imitation learning via data aggregation~\cite{dagger, safe_dagger, hg_dagger}, to efficiently use human interventions to learn reliable robotic control policies. The name is inspired by the concept of lazy evaluation in programming language theory, where expressions are evaluated parsimoniously to reduce computational burden.  As in prior work from~\citet{safe_dagger}, \algabbr learns a meta-controller which determines when to context switch based on the estimated discrepancy between the learner and supervisor.  \algabbr additionally (1) encourages sustained interventions by enforcing that interventions continue until the learner and supervisor policies closely match and (2) injects noise into the supervisor controls to increase robustness in the learned controller. We find that across 3 continuous control tasks in simulation, \algname achieves task performance on par with DAgger with 12\% of its supervisor burden and 60\% fewer context switches than the best interactive IL baseline. In physical fabric folding experiments we observe similar results and find that \algname achieves 40\% better task performance than SafeDAgger with fewer context switches and supervisor actions. \ryan{add something about safe/more reliable deployment of learned policies and robot fleets, motivating LazyDAgger-Execution}
% in addition to reducing the number of interventions, \algabbr often results in improved task performance when compared with state-of-the-art interactive imitation learning algorithms \ashwin{insert details on exp results}.
% In \algabbr, we extend prior work by focusing on two observations: (1) thrashing between supervisor and autonomous policies is often undesirable in practice due to the cognitive burden and human errors that occur due to frequent context switching for the human supervisor~\cite{gopher2000switching,rubinstein2001executive} and the need to have the supervisor available to frequently assist which limits the amount of time the supervisor can perform other tasks~\cite{crandall2005validating,swamy2020scaled}, and (2) interventions are more beneficial if the supervisor is exposed to long sequences of more diverse states~\cite{dagger,DART}. To achieve this, like in prior work~\cite{safe_dagger}, \algabbr learns a meta-controller which determines when to solicit interventions based on the estimated discrepancy between the learner. However, unlike prior work, \algabbr (1) encourages lengthy interventions by enforcing that intervention continue until the supervisor actions and those the learner would take are very similar to each other, indicating that further intervention is unnecessary and (2) injects noise into the supervisor controls to increase robustness in the learned controller. We find that in addition to reducing the number of interventions, \algabbr often results in improved task performance and comparable numbers of requests for supervisor action queries, when compared with state-of-the-art interactive imitation learning algorithms \ashwin{insert details on exp results}.

% \algabbr learns a high-level meta-controller that determines when to solicit interventions based on the estimated discrepancy between the learner and the supervisor while ensuring that the interventions are sufficiently lengthy and visit a diverse enough set of states to be useful for policy learning. The key insight in \algabbr is to improve the  efficiency of human interventions by (1) learning to ask for help only when needed, (2) minimizing context switching and possible human error due to a lag in situational awareness of states~\cite{hg_dagger}, and (3) injecting noise into the supervisor's actions in order to obtain a richer set of supervisor action labels concentrated around areas of state space where the robot needs help. \algabbr can be flexibly used in conjunction with any policy with an appropriate policy improvement mechanism. We evaluate \algabbr on \todo{insert}. We find that in addition to reducing the number of interventions, \algabbr often results in improved task performance and comparable numbers of requests for supervisor action queries, when compared with state-of-the-art interactive imitation learning algorithms.



