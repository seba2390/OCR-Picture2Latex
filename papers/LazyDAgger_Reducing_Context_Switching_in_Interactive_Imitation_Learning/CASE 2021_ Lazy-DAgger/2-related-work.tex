\section{Background and Related Work}
\label{sec:related-work}
Challenges in learning efficiency and reward function specification have inspired significant interest in algorithms that can leverage supervisor demonstrations and feedback for policy learning.

\textbf{Learning from Offline Demonstrations: }
% Demonstrations for IL, RL, Classical Control
Learning from demonstrations~\cite{argall2009survey,osa2018algorithmic,arora2018survey} is a popular imitation learning approach, as it requires minimal supervisor burden: the supervisor provides a batch of offline demonstrations and gives no further input during policy learning. Many methods use demonstrations directly for policy learning~\cite{pomerleau1991efficient,ijspeert2013dynamical,paraschos2013probabilistic,torabi2018behavioral}, while others use reinforcement learning to train a policy using a reward function inferred from demonstrations \cite{abbeel2004apprenticeship,ziebart2008maximum,ho2016generative,brown2019drex,airl}. Recent work has augmented demonstrations with additional offline information such as pairwise preferences~\cite{browngoo2019trex,brown2020safe}, human gaze~\cite{gaze_saran}, and natural language descriptions~\cite{tung2018reward}. While offline demonstrations are often simple to provide, the lack of online feedback makes it difficult to address specific bottlenecks in the learning process or errors in the resulting policy due to covariate shift~\cite{dagger}.

% Learning from demonstrations has also been extensively explored in the reinforcement learning community to guide or accelerate learning~\cite{bignold2020conceptual,najar2020reinforcement,LearningMPC,abc-lmpc,SAVED, overcoming_exploration, dqfd, ddpgfd}. 

% Rather than learning from a fixed set of human demonstrations or preferences, active learning methods query the human for additional data throughout the learning process. 
%This motivates active learning methods such as DAgger~\cite{dagger}, 

\textbf{Learning from Online Feedback: }
Many policy learning algorithms' poor performance stems from a lack of online supervisor guidance, motivating active learning methods such as DAgger, which queries the supervisor for an action in every state that the learner visits~\cite{dagger}. While DAgger has a number of desirable theoretical properties, labeling every state is costly in human time and can be a non-intuitive form of human feedback~\cite{DART}. More generally, the idea of learning from action advice has been widely explored in imitation learning algorithms~\cite{converging-supervisors,SHIV,judah2011active,jauhri2020interactive}.
There has also been significant recent interest in active preference queries for learning reward functions from pairwise preferences over demonstrations~\cite{sadigh2017active,christiano2017deep,ibarz2018reward,palan2019learning,biyik2019asking,brown2020safe}. However, many forms of human advice can be unintuitive, since the learner may visit states that are significantly far from those the human supervisor would visit, making it difficult for humans to judge what correct behavior looks like without interacting with the environment themselves~\cite{EIL,reddy2018shared}.

\textbf{Learning from Supervisor Interventions: } There has been significant prior work on algorithms for learning policies from interventions. \citet{training_wheels, ac-teach} leverage interventions from suboptimal supervisors to accelerate policy learning, but assume that the supervisors are algorithmic and thus can be queried cheaply. \citet{recovery-rl, advantage_interventions, trial_no_error} also leverage interventions from algorithmic policies, but for constraint satisfaction during learning. \citet{hg_dagger, EIL, IARL, LAND, HITL, teaching-strats} instead consider learning from human supervisors and present learning algorithms which utilize the timing and nature of human interventions to update the learned policy. By giving the human control for multiple timesteps in a row, these algorithms show improvements over methods that only hand over control on a state-by-state basis~\cite{perf-eval-IL}. However, the above algorithms assume that the human is continuously monitoring the system to determine when to intervene, which may not be practical in large-scale systems or continuous learning settings~\cite{crandall2005validating,chen2014human,swamy2020scaled,kessler2019active}. Such algorithms also assume that the human knows when to cede control to the robot, which requires guessing how the robot will behave in the future. \citet{safe_dagger} and \citet{ensemble_dagger} present imitation learning algorithms SafeDAgger and EnsembleDAgger, respectively, to address these issues by learning to request interventions from a supervisor based on measures such as state novelty or estimated discrepancy between the learner and supervisor actions. These methods can still be sample inefficient, and, as we discuss later, often result in significant context switching.
% If the human is managing several robots at once, this will overly burden the human by causing too frequent context switching which can decrease human performance and impede situational awareness~\cite{chen2014human,gopher2000switching,rubinstein2001executive}. 
By contrast, \algabbr encourages interventions that are both easier to provide and more informative. To do this, \algname prioritizes (1) sustained interventions, which allow the supervisor to act over a small number of contiguous sequences of states rather than a large number of disconnected intervals, and (2) interventions which demonstrate supervisor actions in novel states to increase robustness to covariate shift in the learned policy.