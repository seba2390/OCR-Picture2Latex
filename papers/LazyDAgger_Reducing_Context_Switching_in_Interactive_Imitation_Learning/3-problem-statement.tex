% \begin{figure*}[htb!]
% \center
% \includegraphics[width=0.75\textwidth]{figures/controlflow5.png}
% \caption{
% \textbf{\algname: }We illustrate the full control flow of \algname on a given timestep $t$. First, \algname queries the robot policy for an action $a_t$ and evaluates its current state under action discrepancy classifier $\ffilt$. If $\ffilt$ predicts an action discrepancy of at least $\tausup$ (i.e., $\ffilt \geq 0.5$) or \algname is in supervisor mode (Mode = $\textrm{Sup}$), then \algname queries an action $a^H_t$ from the supervisor and executes a noise-injected version $\tilde{a}^H_t$ in the environment. Otherwise, \algname executes action $a_t$ from the robot policy. Finally, if \algname executes $\tilde{a}^H_t$, \algname evaluates the true action discrepancy $\mathcal{L}(a_t, \tilde{a}^H_t)$ between $a_t$ and $\tilde{a}^H_t$. The mode for the next timestep is set to Autonomous mode if $\mathcal{L}(a_t, \tilde{a}^H_t) < \tauauto$ and Supervisor otherwise. 
% }
% \label{fig:control-flow}
% \end{figure*}
\section{Problem Statement}
\label{sec:prob-statement}
We consider a setting in which a human supervisor is training a robot to reliably perform a task. The robot may query the human for assistance, upon which the supervisor takes control and teleoperates the robot until the system determines that it no longer needs assistance. We assume that the robot and human policy have the same action space, and that it is possible to pause task execution while waiting to transfer control. We formalize these ideas in the context of prior imitation learning literature.

We model the environment as a discrete-time Markov decision process (MDP) $\mathcal{M}$ with states $s \in \mathcal{S}$, actions $a \in \mathcal{A}$, and time horizon $T$ \cite{puterman2014markov}. The robot does not have access to the reward function or transition dynamics of $\mathcal{M}$ but can cede control to a human supervisor, who executes some deterministic policy $\pisup: \mathcal{S} \to \mathcal{A}$. We refer to times when the robot is in control as \textit{autonomous mode} and those in which the supervisor is in control as \textit{supervisor mode}. 
%We model the environment as a discrete-time Markov decision process (MDP) $\mathcal{M}$ with states $s \in \mathcal{S}$, actions $a \in \mathcal{A}$, and time horizon $T$ \cite{puterman2014markov}. We consider the interactive imitation learning (IL) setting~\cite{hg_dagger}, where the robot does not have access to the reward function or transition dynamics of the MDP but has access to a supervisor. We assume the supervisor implicitly follows some deterministic policy $\pisup: \mathcal{S} \to \mathcal{A}$ and that the robot can request interventions from the supervisor. 
We minimize a surrogate loss function $J(\pirob)$ to encourage the robot policy $\pirob: \mathcal{S} \to \mathcal{A}$ to match that of the supervisor ($\pisup$):
\begin{align}
    \label{eq:IL-objective}
    J(\pirob) = \sum_{t=1}^{T} \mathds{E}_{s_t \sim d^{\pirob}_t} \left[\mathcal{L}(\pirob(s_t),\pisup(s_t))\right],
\end{align}
where $\mathcal{L}(\pirob(s),\pisup(s))$ is an action discrepancy measure between $\pirob(s)$ and $\pisup(s)$ (e.g., the squared loss or 0-1 loss), and $d^{\pirob}_t$ is the marginal state distribution at timestep $t$ induced by executing $\pirob$ in MDP $\mathcal{M}$.

In interactive IL we require a meta-controller $\pimeta$ that determines whether to query the robot policy $\pirob$ or to query for an intervention from the human supervisor policy $\pisup$; importantly, $\pimeta$ consists of both (1) the high-level controller which decides whether to switch between $\pirob$ and $\pisup$ and (2) the low-level robot policy $\pirob$. A key objective in interactive IL is to minimize some notion of supervisor burden. To this end, let $\indicatorint(s_t; \pimeta)$ be an indicator which records whether a context switch between autonomous ($\pirob$) and supervisor ($\pisup$) modes occurs at state $s_t$ (either direction). Then, we define $C(\pimeta)$, the expected number of context switches in an episode under policy $\pimeta$, as follows: ${C(\pimeta) = \sum_{t=1}^{T} \mathds{E}_{s_t \sim d^{\pimeta}_t} \left[ \indicatorint(s_t; \pimeta) \right]}$, where $d^{\pimeta}_t$ is the marginal state distribution at timestep $t$ induced by executing the meta-controller $\pimeta$ in MDP $\mathcal{M}$. %Furthermore, we define \emph{latency} $L > 0$ as the number of time units associated with a single context switch, where each time unit is the time it takes for the supervisor to execute a single action.
% \begin{align}
%     \label{eq:int-objective}
%     C(\pi) = \sum_{t=1}^{T} \mathds{E}_{s_t \sim d^\pi_t} \left[ \mathds{1}_{\textrm{int}}(s_t; \pi) \right]
% \end{align}
Similarly, let $\indicatorsup(s_t; \pimeta)$ indicate whether the system is in supervisor mode at state $s_t$. We then define $D(\pimeta)$, the expected number of supervisor actions in an episode for the policy $\pimeta$, as follows: ${D(\pimeta) = \sum_{t=1}^{T} \mathds{E}_{s_t \sim d^\pimeta_t} \left[ \indicatorsup(s_t; \pimeta) \right]}$.
% \begin{align}
%     \label{eq:act-objective}
%     D(\pi) = \sum_{t=1}^{T} \mathds{E}_{s_t \sim d^\pi_t} \left[ \mathds{1}_{\textrm{sup}}(s_t; \pi) \right]
% \end{align}

We define \emph{supervisor burden} $B(\pimeta)$ as the expected time cost imposed on the human supervisor. This can be expressed as the sum of the expected total number of time units spent in context switching and the expected total number of time units in which the supervisor is actually engaged in performing interventions:% Stated precisely, we define \emph{supervisor burden} $B(\pimeta)$ as follows:
\begin{align}
    \label{eq:burden}
        B(\pimeta) = L \cdot C(\pimeta) + D(\pimeta),
\end{align}
where $L$ is context switch latency (Section~\ref{sec:introduction}) in time units, and each time unit is the time it takes for the supervisor to execute a single action. The learning objective is to find a policy $\pimeta$ that matches supervisor performance, $\pisup$, while limiting supervisor burden to lie within a threshold $\Gamma_{\rm b}$, set by the supervisor to an acceptable tolerance for a given task. To formalize this problem, we propose the following objective: 
\begin{align}
\begin{split}
    \label{eq:LazyDAgger-objective}
    \pimeta &= \argmin_{\pimeta' \in \Pi}\{ J(\pirob')
    \mid B(\pimeta') \leq \Gamma_{\rm b}\},
\end{split}
\end{align}
where $\Pi$ is the space of all meta-controllers, and $\pirob'$ is the low-level robot policy associated with meta-controller $\pimeta'$. 
% In Section~\ref{sec:prelims} we discuss the specifics of the meta-controller $\pimeta$ used in SafeDAgger~\cite{safe_dagger} and in Section~\ref{sec:alg} we present the one we use in \algname.
% \begin{align}
% \begin{split}
%     \label{eq:LazyDAgger-objective}
%     \pi &= \argmin_{\pi \in \Pi}\{ J(\pi)
%     \mid C(\pi) \leq \Gamma_{\rm int} \text{, } D(\pi) \leq \Gamma_{\rm sup}\}
% \end{split}
% \end{align}

% This captures the intuition that the number of switches in control between the learner and the supervisor and the number of actions in supervisor mode approximately measure the online burden inflicted by \algname on the supervisor.
% \ashwin{Prof mentioned adding number of actions into this objective as well, still thinking about best way to do this}
% \ashwin{Will add an explicit term on minimizing switches tonight}
% In our work, we seek to minimize both the surrogate loss as well minimize the number of supervisor interventions. %I wonder if we should actually make this an explicit objective?

% We consider robotic control under Markov decision processes (MDPs), which can be described by the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P(\cdot|\cdot,\cdot), R(\cdot, \cdot), \gamma, \mu)$, where $\mathcal{S}$ and $\mathcal{A}$ are the state and action spaces.
% The stochastic dynamics model $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]$ maps a state and action to a probability distribution over subsequent states, $\gamma \in [0, 1]$ is a discount factor, $\mu$ is the initial state distribution ($s_0\sim\mu$), and $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function.

% Let $\Pi$ be the set of deterministic, Markovian policies $\pi: \mathcal{S} \rightarrow \mathcal{A}$. Given policy $\pi \in \Pi$, the expected return is defined as $R^\pi = \mathbb{E}_{\pi,\mu, P}\left[ \sum_t \gamma^t R(s_t, a_t)\right]$. The objective of \algabbr is to solve the following constrained optimization problem:
% \begin{align}
% \label{eq:objective}
%     \pi^* = \argmax_{\pi \in \Pi} \{R^\pi\}
% \end{align}
% We assume access to a potentially suboptimal supervisor policy $\pi_{\text{sup}}$ that can be queried online, where $R^{\pi_{\text{sup}}} \leq R^{\pi^*}$. We also assume that the supervisor is able to take over control safely and indicate actions $a \in \mathcal{A}$. For example, a human supervisor can easily indicate a pick-and-place action via a user interface, but may not be able to help with tasks involving extremely high-frequency control or high-dimensional control inputs. The objective is to learn a policy $\pi$ that best approximates $\pi_{\text{sup}}$.

% In the \textit{reinforcement learning} setting, we have access to a reward function and we seek to learn $\pi^\star$, which may obtain higher expected return than $\pi_{\text{sup}}$.

\section{Preliminaries: SafeDAgger}
\label{sec:prelims}
% The goal of imitation learning is to learn a policy $\pi$ to match the trajectory distribution of a supervisor $\pi_{\text{sup}}$ as closely as possible. More precisely, in imitation learning, the robot typically does not have access to any specified objective or reward function, but instead attempts to minimize a surrogate loss function which measures the discrepancy between the learner's policy $\pi$ and the supervisor $\pi_{\text{sup}}$~\cite{dagger}. Precisely, the learner solves the following optimization problem:
% \begin{align}
%     \label{eq:IL-objective}
%     \pi = \argmin_{\pi \in \Pi} \mathds{E}_{s \sim d_\pi} \left[L(\pi(s),\pi_{\rm sup}(s))\right]
% \end{align}
% where $L(\pi(s),\pi_{\rm sup}(s))$ is an action discrepancy measure between $\pi(s)$ and $\pi_{\text{sup}}(s)$ (squared loss, 0-1 loss, etc.).  and $d_\pi$ is the marginal state distribution induced by executing \textit{the learner policy} $\pi$ in MDP $\mathcal{M}$.
% One popular approach for optimizing the imitation learning objective is Behavior Cloning~\cite{pomerleau1991efficient,osa2018algorithmic}, which optimizes Equation~\eqref{eq:IL-objective} over a static dataset of offline trajectories collected from $\pi_{\text{sup}}$. However, this approach suffers from poor performance due to distribution mismatch~\cite{dagger}: the learner trains on samples from $d_{\pi_{\text{sup}}}$ but encounters trajectories from $d_{\pi}$ during execution. One popular approach to address these challenges both in theory and practice is DAgger~\cite{dagger}, which optimizes Equation~\eqref{eq:IL-objective} over an aggregated dataset of trajectories from $\pi$ and action labels from $\pi_{\text{sup}}$ for each state in the dataset. While this aggregation scheme results in desirable regret bounds~\cite{dagger}, providing action labels offline can often be highly non-intuitive~\cite{DART, SHIV, hg_dagger} and error prone~\cite{chuck2017cleaning}, making DAgger difficult to use in practice.
We consider interactive IL in the context of the objective introduced in Equation~\eqref{eq:LazyDAgger-objective}: to maximize task reward while limiting supervisor burden. To do this, \algabbr builds on SafeDAgger~\cite{safe_dagger}, a state-of-the-art algorithm for interactive IL. SafeDAgger selects between autonomous mode and supervisor mode by training a binary action discrepancy classifier $\ffilt$ to discriminate between ``safe" states which have an action discrepancy below a threshold $\tausup$ (i.e., states with ${\mathcal{L}(\pirob(s), \pisup(s)) < \tausup}$) and ``unsafe" states (i.e. states with ${\mathcal{L}(\pirob(s), \pisup(s)) \geq \tausup}$). The classifier $\ffilt$ is a neural network with a sigmoid output layer (i.e., $\ffilt(s) \in [0, 1]$) that is trained to minimize binary cross-entropy (BCE) loss on the datapoints $(s_t,\pisup(s_t))$ sampled from a dataset $\mathcal{D}$ of trajectories collected from $\pisup$. This is written as follows:
\begin{align}
\label{eq:safe-loss}
    \begin{split}
    \lsafe(\pirob(s_t),\pisup(s_t), \ffilt) 
    = -\fstarfilt(\pirob(s_t),\pisup(s_t))\log \ffilt(s_t) \\
    -(1 - \fstarfilt(\pirob(s_t), \pisup(s_t)))\log (1 - \ffilt(s_t) ),
    \end{split}
\end{align}
where the training labels are given by $\fstarfilt(\pirob(s_t),\pisup(s_t)) = \mathbbm{1}\left\{\mathcal{L}(\pirob(s_t), \pisup(s_t)) \geq  \tausup\right\}$, and $\mathbbm{1}$ denotes the indicator function.
Thus, $\lsafe(\pirob(s_t),\pisup(s_t),\ffilt)$ penalizes incorrectly classifying a ``safe" state as ``unsafe" and vice versa. %a binary cross-entropy loss function on the target distribution $\fstarfilt(\pirob(s_t),\pisup(s_t))$ and predicted distribution $\ffilt(s_t)$.

SafeDAgger executes the meta-policy $\pimeta$ which selects between $\pirob$ and $\pisup$ as follows:
\begin{align}
    \label{eq:safe_dagger_policy}
    \pimeta(s_t) =\begin{cases}
            \pirob(s_t) \text{ if } \ffilt(s_t) < 0.5 \\
            \pisup(s_t) \text{ otherwise},
          \end{cases}
\end{align}
 %\brijen{I think the first case should be $\geq 1/2$, or some thresh the way it's written above, since $\ffilt$ outputs values from $(0,1)$}
where $f(s_t) < 0.5$ corresponds to a prediction that $\mathcal{L}(\pirob(s_t), \pisup(s_t)) < \tausup$, i.e., that $s_t$ is ``safe.'' Intuitively, SafeDAgger only solicits supervisor actions when $\ffilt$ predicts that the action discrepancy between $\pirob$ and $\pisup$ exceeds the safety threshold $\tausup$. Thus, SafeDAgger provides a mechanism for querying the supervisor for interventions only when necessary. In \algabbr, we utilize this same mechanism to query for interventions but enforce new properties once we enter these interventions to lengthen them and increase the diversity of states observed during the interventions.
% \begin{align}
% \label{eq:bc-loss}
% L_{BC}(D, \pi) = \frac{1}{N} \sum_{i=1}^{N} ||\pi(s_i) - a_i||_2^2
% \end{align}
%  where $N$ is the total number of $(s_i, a_i)$ tuples in our dataset $D$. Our learned policy is $$ \pi = \arg\min_{\pi}{L(D, \pi)} $$
% \subsection{Imitation Learning with Dataset Aggregation (DAgger)}
% Unfortunately, in practice Behavior Cloning gives a very brittle policy due to compounding errors as soon as the learner leaves the supervisor's distribution \cite{dagger}. To mitigate this, one popular strategy is Dataset Aggregation (DAgger) \cite{dagger}, which provides more information on the learner's distribution. After training on the initial offline dataset $D$, we run our learned policy in the environment and collect its trajectories: $\{(s_0, \pi(s_0)), (s_1, \pi(s_1)), \dots\}$. We relabel each of these visited states $\{s_i\}$ online with what the \textit{supervisor} would have done at that state, giving us a new dataset $D' = \{(s_0, \pi_{\text{sup}}(s_0)), (s_1, \pi_{\text{sup}}(s_1)), \dots\}$. We then aggregate this new dataset $D'$ with our existing one ($D = D \cup D'$) and repeat the process for a fixed number of iterations. While this technique combats distribution shift, labeling each state online can be a very burdensome process that scales poorly, especially for a human supervisor.
% \subsection{SafeDAgger}
% To mitigate these issues, several works seek to reduce the burden on the supervisor \cite{SHIV, DART, safe_dagger}. Of these, the most similar to our approach is SafeDAgger~\cite{safe_dagger}. To deal with potential safety issues and reduce supervisor burden, SafeDAgger trains a classifer $\pi_{\text{safe}}$ to discriminate between ``safe" states (i.e. states with predicted action discrepancy below some predefined threshold $\tau$) and ``unsafe" states by optimizing the following binary cross-entropy loss:
% \begin{align}
% \label{eq:safe-loss}
%     L_{\textrm{safe}}(D,\pi, \pi_{\textrm{safe}}) 
%     = -\frac{1}{N} \sum_{i=1}^{N}(\pi_{\text{safe}}^*(s_i,a_i,\pi)\log \pi_{\text{safe}}(s_i) \\
%     +(1 - \pi_{\text{safe}}^*(s_i,a_i,\pi))\log (1 - \pi_{\text{safe}}(s_i) ))
% \end{align}

% where $$ \pi_{\text{safe}}^*(s_i, a_i, \pi) = \begin{cases} 1 \text{ if } ||\pi(s_i)-a_i||_2^2 \leq \tau \\ 0 \text{ otherwise } \end{cases} $$ and $\pi$ is our imitation learner. When the classifier believes the state is unsafe, it lets the supervisor control the system instead, and only collects training examples when the supervisor is in control. SafeDAgger demonstrates a significant reduction in supervisor burden and provides a mechanism that can extend to test-time policy deployment. However, they do not consider the supervisor burden involved in taking over control and relinquishing control, resulting in very frequent interventions that often last only one time-step long. In contrast, we seek to reduce supervisor burden in terms of both overall number of actions provided as well as the number of interventions, as the effort required to provide an additional action once already in control is often much less than that of assuming control of the system.
% \subsection{Reinforcement Learning}
% While many proposed improvements to DAgger \cite{safe_dagger, hg_dagger, dropout_dagger, SHIV, DART} solely evaluate their methods in the imitation learning setting, we observe that our \algabbr framework is agnostic to the implementation of the learner as long as it is able to incorporate the information that the supervisor provides. One such implementation is reinforcement learning, which has been shown to benefit from demonstrations \cite{overcoming_exploration, ddpgfd, ac-teach}, especially in sparse reward settings. In reinforcement learning, the goal is to learn a policy that maximizes the expected sum of discounted rewards in our MDP, i.e. $\mathbb{E}[\sum_t \gamma^t R(s_t, a_t)]$. In this setting, the robot typically does not have access to a supervisor policy, but it does have access to the reward at the states that it visits. As learning from scratch is inherently difficult, reinforcement learning has notoriously high sample complexity, has trouble dealing with exploration \cite{overcoming_exploration}, and can be brittle when deployed on physical systems. \algabbr may mitigate these issues by providing access to expert interventions when the learner needs help.

% We introduce a simple method to issue requests for supervisor intervention which is compatible with any off-policy control algorithm. The key insight is that interventions are likely to be useful in regions in which the robotic control policy and the supervisor are \emph{inconsistent}, and are otherwise likely unnecessary. Thus, like \todo{cite Safe DAgger}, we train a discrepancy estimator which estimates the divergence between the action chosen by the control policy and the supervisor without querying the supervisor. In \todo{cite Safe DAgger}, human action labels are solicited on states for which this discrepancy is sufficiently high. In contrast, when this discrepancy is sufficiently high, we request a supervisor intervention which is active until this discrepancy is no longer high again. This allows the robot to obtain sufficient experience in regions of the state space in which it is likely to diverge from the supervisor without relying on the supervisor to reliably label data outside of their data distribution.

% As in Safe DAgger we define a discrepancy measure between the learner and supervisor as follows:
% \begin{align}
%     \label{eq:discrep}
%     d(\pi, \pi^*, s) = \lVert \pi(s) - \pi^*(s) \rVert^2
% \end{align}
% We use this to define an intervention policy $\pi_I$ as follows ($1$ if should intervene, $0$ otherwise):
% \begin{align}
% \label{eq:intervene}
%     \pi_I(\pi, s) = \begin{cases}
%         1,    \   d(\pi, \pi^*, s) > \tau\\
%         0, \  \textrm{otherwise}
%     \end{cases}
% \end{align}
% Thus, $\pi_I$ can be used to determine when the $\pi$ and $\pi^*$ will diverge significantly without querying the $\pi^*$. The rest of the algorithm is functionally the same as Safe DAgger, but interventions are solicited instead of single actions when $\pi_I$ determines an intervention is required and you switch back the discrepancy between the supervisor and the robot is sufficiently low. No labeling so can use it with any control algorithm you wish. Tbh would be nice to consider this with model-based, model-free RL + IL as this would show the generality of the method and best distinguish it from Safe DAgger.

\begin{figure}[b!]
\center
\includegraphics[width=\linewidth]{figures/hyst2.png}
\caption{
\textbf{\algname Switching Strategy:  }SafeDAgger switches between supervisor and autonomous mode if the predicted action discrepancy is above threshold $\tausup$. In contrast, \algname uses asymmetric switching criteria and switches to autonomous mode based on ground truth action discrepancy. The gap between $\tauauto$ and $\tausup$ defines a hysteresis band~\cite{hysteresis}.}% This encourages lengthier interventions which reduce context switching between supervisor and autonomous modes, thereby reducing supervisor burden.}
\label{fig:hyst}
\end{figure}