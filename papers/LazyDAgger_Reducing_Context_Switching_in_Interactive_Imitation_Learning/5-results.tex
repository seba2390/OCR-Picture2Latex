\section{Experiments}
\label{sec:results}
\begin{figure*}[htb!]
\center
\includegraphics[width=0.85\textwidth]{figures/sim2sim2.png}
\caption{
\textbf{Fabric Smoothing Simulation Results:} We study task performance measured by final fabric coverage (A), total supervisor actions (B), and total context switches (C) for \algname and baselines in the Gym-Cloth environment from~\cite{seita_fabrics_2020}. The horizontal dotted line shows the success threshold for fabric smoothing. \algname achieves higher final coverage than Behavior Cloning and SafeDAgger with fewer context switches than SafeDAgger but more supervisor actions. At execution time, we again observe that \algname achieves similar coverage as SafeDAgger but with fewer context switches.}
\label{fig:sim2sim}
\end{figure*}
We study whether \algname can (1) reduce supervisor burden while (2) achieving similar or superior task performance compared to prior algorithms. Implementation details are provided in the supplementary material. In all experiments, $\mathcal{L}$ measures Euclidean distance.  % on policy architectures, training procedures, and other experimental parameters in the supplementary material.

\subsection{Simulation Experiments: MuJoCo Benchmarks}\label{ssec:mujoco-results}
\textbf{Environments: }We evaluate \algname and baselines on 3 continuous control environments from MuJoCo~\cite{mujoco}, a standard simulator for evaluating imitation and reinforcement learning algorithms. In particular, we evaluate on HalfCheetah-v2, Walker2D-v2 and Ant-v2.
% which have state space sizes $|S| \in (17,17,111)$ and action space sizes $|A| \in (6,6,8)$ respectively.

\textbf{Metrics:}
\label{ssec:metrics}
For \algname and all baselines, we report learning curves which indicate how quickly they can make task progress in addition to metrics regarding the burden imposed on the supervisor. To study supervisor burden, we report the number of supervisor actions, the number of context switches, and the total supervisor burden (as defined in Eq.~\eqref{eq:burden}). Additionally, we define $L^* \geq 0$ to be the latency value such that for all $L > L^*$, \algname has a lower supervisor burden than SafeDAgger. We report this $L^*$ value, which we refer to as the \textit{cutoff latency}, for all experiments to precisely study the types of domains in which \algname is most applicable.

\textbf{Baselines:}
\label{ssec:baselines}
We compare \algname to Behavior Cloning~\cite{torabi2018behavioral}, DAgger~\cite{dagger}, and SafeDAgger~\cite{safe_dagger} in terms of the total supervisor burden and task performance. The Behavior Cloning and DAgger comparisons evaluate the utility of human interventions, while the comparison to SafeDAgger, another interactive IL algorithm, evaluates the impact of soliciting fewer but longer interventions.

\textbf{Experimental Setup: }For all MuJoCo environments, we use a reinforcement learning agent trained with TD3~\cite{td3} as an algorithmic supervisor. 
% On the HalfCheetah, Walker2D, and Ant environments, the supervisors obtain a reward of $5331 \pm 118$, $3492 \pm 1110$, and $4493 \pm 1580$ respectively.
We begin all \algabbr, SafeDAgger, and DAgger experiments by pre-training the robot policy with Behavior Cloning on 4,000 state-action pairs for 5 epochs, and similarly report results for Behavior Cloning after the 5th epoch. To ensure a fair comparison, Behavior Cloning uses additional offline data equal to the average amount of online data seen by \algabbr during training. All results are averaged over 3 random seeds.
%to control for stochasticity in training. 

\textbf{Results: }In Figure~\ref{fig:mujoco}, we study the performance of \algabbr and baselines. After every epoch of training, we run the policy for 10 test rollouts \emph{where interventions are not allowed} and report the task reward on these rollouts in Figure~\ref{fig:mujoco}. Results suggest that \algabbr is able to match or outperform all baselines in terms of task performance across all simulation environments (Figure~\ref{fig:mujoco}A). Additionally, \algabbr requires far fewer context switches compared to SafeDAgger (Figure~\ref{fig:mujoco}D), while requesting a similar number of supervisor actions across domains (Figure~\ref{fig:mujoco}C): we observe a 79\%, 56\%, and 46\% reduction in context switches on the HalfCheetah, Walker2D, and Ant environments respectively. \algname and SafeDAgger both use an order of magnitude fewer supervisor actions than DAgger. While SafeDAgger requests much fewer supervisor actions than \algname in the Ant environment, this limited amount of supervision is insufficient to match the task performance of \algname or any of the baselines, suggesting that SafeDAgger may be terminating interventions prematurely. We study the total supervisor burden of SafeDAgger and \algname as defined in Equation~\eqref{eq:burden} and find that in HalfCheetah, Walker2D, and Ant, the cutoff latencies $L^*$ are 0.0, 4.3, and 7.6 respectively, i.e. \algname achieves lower supervisor burden in the HalfCheetah domain for any $L$ as well as lower burden in Walker2D and Ant for $L > 4.3$ and $L > 7.6$ respectively. The results suggest that \algname can reduce total supervisor burden compared to SafeDAgger even for modest latency values, but that SafeDAgger may be a better option for settings with extremely low latency.

\textbf{Ablations: } We study 2 key ablations for \algname in simulation: (1) returning to autonomous mode with $\ffilt(\cdot)$ rather than using the ground truth discrepancy (\algname (-Switch to Auto) in Figure~\ref{fig:mujoco}), and (2) removal of noise injection (\algname (-Noise)). \algabbr outperforms both ablations on all tasks, with the exception of ablation 1 on Walker2D, which performed similarly well. We also observe that \algabbr consistently requests more supervisor actions than either ablation. This aligns with the intuition that both using the ground truth action discrepancy to switch back to autonomous mode and injecting noise result in longer but more useful interventions that improve performance. 
% in terms of how costly a context switch is compared to an individual action in Figure~\ref{fig:mujoco}E. \algname has lower overall supervisor burden than SafeDAgger for any switch ratio in HalfCheetah as well as ratios greater than $L = 4.3$ in Walker and $L = 7.6$ in Ant.

%\begin{figure}[tb!]
%\vspace{-0.01in}

% \begin{figure*}[t]
% \center
% \includegraphics[width=0.8\textwidth]{figures/phys-task2.png}
% \caption{
% \textbf{Physical Fabric Manipulation Task: } We evaluate on a 3-stage fabric manipulation task consisting of smoothing a crumpled fabric, aligning the fabric so all corners are visible in the observations, and performing a triangular fold.}
% \label{fig:phystask}
% \end{figure*}
% \vspace{-0.13in}

% \begin{figure}[t]
% \center
% \includegraphics[width=0.45\textwidth]{figures/robotfleetv2.png}
% \caption{
% \textbf{Robot Fleet: } We compare the performance of a robot fleet using \algabbr and SafeDAgger. The y-axis is cumulative reward obtained by 3 robots and the x-axis is the time penalty for a context switch between robots. \algabbr outperforms SafeDAgger on HalfCheetah and Walker for all time penalties as well as Ant for high time penalties.}
% \label{fig:robotfleet}
% \end{figure}
% \textbf{Managing a Robot Fleet: } To compare the performance of \algabbr and SafeDAgger in a robot fleet setting, we re-interpret the MuJoCo simulation experiments as a small robot fleet in Figure~\ref{fig:robotfleet}. Specifically, we assume 1 supervisor and 3 robots, represented by each individual run. Since there is only 1 supervisor, robots requesting assistance must wait in a FIFO queue without making task progress until their intervention request is served. We assume that each action by a robot or supervisor takes 1 timestep, and we apply time penalty $L$ for context switching between robots. We run the fleet for 30,000 timesteps and plot the cumulative reward obtained by all 3 robots in Figure~\ref{fig:robotfleet}. We find that \algabbr obtains higher reward than SafeDAgger for the HalfCheetah fleet and Walker fleet regardless of the context switch penalty, but only outperforms SafeDAgger on Ant for $L > 17$. These results suggest that \algabbr is a more suitable algorithm for supervising a robot fleet if the burden of a context switch is high relative to an individual action. \todo{Clean up this paragraph}

\begin{figure*}[t]
\center
\includegraphics[width=0.90\textwidth]{figures/phystaskandrollouts.png}
\caption{
\textbf{Physical Fabric Manipulation Task: }\textit{Left: } We evaluate on a 3-stage fabric manipulation task consisting of smoothing a crumpled fabric, aligning the fabric so all corners are visible in the observations, and performing a triangular fold. \textit{Right: }  Rollouts of the fabric manipulation task, where each frame is a 100 $\times$ 100 $\times$ 3 overhead image. Human supervisor actions are denoted in red while autonomous robot actions are in green. Rollouts are shaded to indicate task progress: blue for smoothing, red for alignment, and green for folding. SafeDAgger ends human intervention prematurely, resulting in poor task performance and more context switches, while \algname switches back to robot control only when confident in task completion.}
\label{fig:phys-rollouts}
\end{figure*}

\begin{table*}[ht]
%  \begin{center}
\centering
%\resizebox{\columnwidth}{!}
{
 \begin{tabular}{|l | c | c | c | c | c | c | c | c | c | c | c |} 
 \hline
 Algorithm & Task Successes & \multicolumn{3}{c|}{Task Progress} & Context Switches & Supervisor Actions & Robot Actions & \multicolumn{4}{c|}{Failure Modes} \\ 
 \hline
\multicolumn{2}{|c|}{} & (1) & (2) & (3) & \multicolumn{3}{c|}{} & A & B & C & D \\
 \hline
 Behavior Cloning & 0/10 & 6/10 & 0/10 & 0/10 & N/A & N/A & 119 & 2 & 1 & 7 & 0 \\
 \hline
 SD-Execution & 2/10 & 6/10 & 4/10 & 2/10 & 53 & \textbf{34} & 108 & 5 & 0 & 0 & 3 \\
 \hline
 LD-Execution & \textbf{8/10} & 10/10 & 10/10 & 8/10 & \textbf{21} & 43 & 47 & 0 & 0 & 0 & 2 \\
\hline
\end{tabular}}
\caption{\textbf{Physical Fabric Manipulation Experiments:} We evaluate \algname-Execution and baselines on a physical 3-stage fabric manipulation task and report the success rate and supervisor burden in terms of total supervisor actions and bidirectional context switches (summed across all 10 trials). Task Progress indicates how many trials completed each of the 3 stages: Smoothing, Aligning, and Folding. \algname-Execution achieves more successes with fewer context switches ($L^* = 0.28$). We observe the following failure modes (Table~\ref{tab:phys_results}): (A) action limit hit ($>$ 15 total actions), (B) fabric is more than 50\% out of bounds, (C) incorrect predicted pick point, and (D) the policy failed to request an intervention despite high ground truth action discrepancy.}
\label{tab:phys_results}
\end{table*}

\subsection{Fabric Smoothing in Simulation} \label{ssec:fabric_sim_results}
\textbf{Environment: }
We evaluate \algabbr on the fabric smoothing task from~\cite{seita_fabrics_2020} (shown in Figure~\ref{fig:sim2sim}) using the simulation environment from~\cite{seita_fabrics_2020}. The task requires smoothing an initially crumpled fabric and is challenging due to the infinite-dimensional state space and complex dynamics, motivating learning from human feedback. As in prior work~\cite{seita_fabrics_2020}, we utilize top-down $100 \times 100 \times 3$ RGB image observations of the workspace and use actions which consist of a 2D pick point and a 2D pull vector. See~\cite{seita_fabrics_2020} for further details on the fabric simulator.

\textbf{Experimental Setup: }
We train a fabric smoothing policy in simulation using DAgger under supervision from an analytic corner-pulling policy that leverages the simulator's state to identify fabric corners, iterate through them, and pull them towards the corners of the workspace~\cite{seita_fabrics_2020}. We transfer the resulting policy for a 16$\times$16 grid of fabric into a new simulation environment with altered fabric dynamics (i.e. lower spring constant, altered fabric colors, and a higher-fidelity 25$\times$25 discretization) and evaluate \algname and baselines on how rapidly they can adapt the initial policy to the new domain. As in~\cite{seita_fabrics_2020}, we terminate rollouts when we exceed 10 time steps, 92\% coverage, or have moved the fabric more than 20\% out of bounds. We evaluate performance based on a coverage metric, which measures the percentage of the background plane that the fabric covers (fully smooth corresponds to a coverage of 100).

\textbf{Results: }
We report results for the fabric smoothing simulation experiments in Figure~\ref{fig:sim2sim}. Figure~\ref{fig:sim2sim}~(A) shows the performance of the SafeDAgger and \algname policies during learning. To generate this plot we periodically evaluated each policy on \emph{test} rollouts without interventions. Figure~\ref{fig:sim2sim}~(B) and (C) show the number of supervisor actions and context switches required during learning; \algname performs fewer context switches than SafeDAgger but requires more supervisor actions as the interventions are longer. Results suggest that the cutoff latency (as defined in Section~\ref{ssec:metrics}) is $L^*=1.5$ for fabric smoothing. Despite fewer context switches, \algname achieves comparable performance to SafeDAgger, suggesting that \algname can learn complex, high-dimensional robotic control policies while reducing the number of hand-offs to a supervisor. We also evaluate \algname-Execution and SafeDAgger-Execution, in which interventions are allowed but the policy is no longer updated (see Section~\ref{ssec:physresults}). We see that in this case, \algname achieves similar final coverage as SafeDAgger with significantly fewer context switches. 

\subsection{Physical Fabric Manipulation Experiments}\label{ssec:physresults}
\textbf{Environment: }
In physical experiments, we evaluate on a multi-stage fabric manipulation task with an ABB YuMi robot and a human supervisor (Figure~\ref{fig:phys-rollouts}). Starting from a crumpled initial fabric state, the task consists of 3 stages: (1) fully smooth the fabric, (2) align the fabric corners with a tight crop of the workspace, and (3) fold the fabric into a triangular fold. Stage (2) in particular requires high precision, motivating human interventions. As in the fabric simulation experiments, we use top-down $100 \times 100 \times 3$ RGB image observations of the workspace and have 4D actions consisting of a pick point and pull vector. The actions are converted to workspace coordinates with a standard calibration procedure and analytically mapped to the nearest point on the fabric. Human supervisor actions are provided through a point-and-click interface for specifying pick-and-place actions. See the supplement for further details.
% \todo{Ryan: describe the environment here, not how your exp setup works}
% As in prior work~\cite{fabric_vsf,seita_fabrics_2020}, we train a policy with 100 x 100 x 3 RGB image observations in simulation to smooth highly crumpled initial configurations. 
% We study fabric smoothing experiments both in simulation and on an ABB YuMi robot.
% For physical transfer the learned policy to a physical robot with the same domain randomization techniques as~\cite{fabric_vsf,seita_fabrics_2020} and fine-tune the model with a small amount of real data. For this final stage of fine-tuning, we consider 3 strategies: (1) Behavior Cloning, (2) SafeDAgger, and (3) LazyDAgger.

\textbf{Experimental Setup: }
Here we study how interventions can be leveraged to improve the final task performance even at \textit{execution time}, in which policies are no longer being updated. We collect 20 offline task demonstrations and train an initial policy with behavior cloning. To prevent overfitting to a small amount of real data, we use standard data augmentation techniques such as rotating, scaling, changing brightness, and adding noise to create 10 times as many training examples. We then evaluate the behavior cloning agent (Behavior Cloning) and agents which use the SafeDAgger and \algname intervention criteria but do not update the policy with new experience or inject noise (SafeDAgger-Execution and \algname-Execution respectively). We terminate rollouts if the fabric has successfully reached the goal state of the final stage (i.e. forms a perfect or near-perfect dark brown right triangle as in~\citet{fabric_vsf}; see Figure~\ref{fig:phys-rollouts}), more than 50\% of the fabric mask is out of view in the current observation, the predicted pick point misses the fabric mask by approximately 50\% of the plane or more, or 15 total actions have been executed (either autonomous or supervisor).

\textbf{Results: }
We perform 10 physical trials of each technique. In Table~\ref{tab:phys_results}, we report both the overall task success rate and success rates for each of the three stages of the task: (1) Smoothing, (2) Alignment, and (3) Folding. We also report the total number of context switches, supervisor actions, and autonomous robot actions summed across all 10 trials for each algorithm (Behavior Cloning, SafeDAgger-Execution, \algname-Execution). In Figure~\ref{fig:phys-rollouts} we provide representative rollouts for each algorithm. Results suggest that Behavior Cloning is insufficient for successfully completing the alignment stage with the required level of precision. SafeDAgger-Execution does not improve the task success rate significantly due to its inability to collect interventions long enough to navigate bottleneck regions in the task (Figure~\ref{fig:phys-rollouts}). \algname-Execution, however, achieves a much higher success rate than SafeDAgger-Execution and Behavior Cloning with far fewer context switches than SafeDAgger-Execution: \algname-Execution requests 2.1 context switches on average per trial (i.e. 1.05 interventions) as opposed to 5.3 switches (i.e. 2.65 interventions). \algname-Execution trials also make far more task progress than the baselines, as all 10 trials reach the folding stage. \algname-Execution does request more supervisor actions than SafeDAgger-Execution, as in the simulation environments. \algname-Execution also requests more supervisor actions relative to the total amount of actions due to the more conservative switching criteria and the fact that successful episodes are shorter than unsuccessful episodes on average. Nevertheless, results suggest that for this task, \algname-Execution reduces supervisor burden for any $L > L^* = 0.28$, a very low cutoff latency that includes all settings in which a context switch is at least as time-consuming as an individual action (i.e. $L \geq 1$).

In experiments, we find that SafeDAgger-Execution's short interventions lead to many instances of Failure Mode A (see Table~\ref{tab:phys_results}), as the policy is making task progress, but not quickly enough to perform the task. We observe that Failure Mode C is often due to the fabric reaching a highly irregular configuration that is not within the training data distribution, making it difficult for the robot policy to make progress. We find that SafeDAgger and \algname experience Failure Mode D at a similar rate as they use the same criteria to solicit interventions (but different termination criteria). However, we find that all of \algname's failures are due to Failure Mode D, while SafeDAgger also fails in Mode A due to premature termination of interventions.
% For Behavior Cloning, Failure Mode A refers to reaching a significantly off-distribution folded configuration that results in a highly erratic autonomous policy. \todo{add small figure for intuition?} For SafeDAgger-Execution and \algabbr-Execution, Failure Mode A refers to reaching a fabric configuration that is very difficult or impossible for the human supervisor to correct. SafeDAgger-Execution's premature termination of interventions (Figure~\ref{fig:phys-rollouts}) results in more instances of Failure Mode B.


% \todo{note that results worse cuz of harder start states}
% We report Table~\ref{tab:phys_results} for physical trials. We report the number of bidirectional context switches (robot to human and vice versa) as well as the number of supervisor actions for SafeDAgger and \algname over the full 100 time steps of training. We then run 5 trials of Behavior Cloning (BC), SafeDAgger, and \algname and report the average and standard deviation of maximum coverage attained during each trial. During these trials, SafeDAgger and \algname are \textit{not} able to switch to the supervisor as they were during training.
%  \begin{figure}
%      \centering
%      \includegraphics[width=0.48\textwidth]{figures/qualitative.pdf}
%      \caption{Representative policy rollouts on the ABB YuMi for SafeDAgger (top) and \algname (bottom). Human supervisor actions are indicated with red arrows while robot actions are indicated in green. We find that \algname only allows the robot to resume control when confident in task completion, while SafeDAgger ends the intervention prematurely.}
%      \label{fig:lazyvssafe}
%  \end{figure}
