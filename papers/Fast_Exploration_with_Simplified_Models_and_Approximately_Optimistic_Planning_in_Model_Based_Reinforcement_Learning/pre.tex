\section{Preliminaries}

We consider a finite horizon Markov Decision Process (MDP) $\langle \mathcal{S}, \mathcal{A}, T, R, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ the action space, $T:\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ the transition function, $R:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ the reward function, and $\gamma$ the discount factor. The goal of the RL agent is to maximize the expected discounted reward $\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^t R(s_t, a_t)]$ following a policy $\pi$. 