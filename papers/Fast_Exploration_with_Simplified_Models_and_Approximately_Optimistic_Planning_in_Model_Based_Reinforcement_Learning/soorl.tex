\section{SOORL: Strategic Object Oriented Reinforcement Learning}
In this section we put together the insights from previous sections and propose a novel model-based object oriented RL algorithm, Strategic Object Oriented RL (\textbf{SOORL}). SOORL assumes access to an object detector, that returns a list of objects with their attributes (i.e. location and bounding box), an interaction function and macro actions (that can be learned with algorithm \ref{alg:macro}).

Algorithm \ref{alg:soorl} shows a pseudo code of SOORL. At each step, SOORL performs lookahead planning with UCT algorithm, learn and select appropriate transition and reward models for object representation and performs optimism based exploration. 

\begin{algorithm}
\KwIn{object detector $f(s)$, lookahead planning depth $d$, number of rollouts $l$}
initialize\;
\For{each episode e}{
    train value function V on replay buffer $\mathcal{D}$\;
    \For{each step i}{
    $o_i \gets$ detect objects with object detector $f(s_i)$\;
    $Q(s_i,a) \gets$ perform lookahead planning with depth $d$ and $l$ rollouts\;
    take action $a_i ={argmax}{Q(s,.)}$ and update replay buffer $\mathcal{D} = \mathcal{D} \cup (o_i, a_i, o_{i+1}, r_i)$\;
    update transition and reward models with $(o_i, a_i, o_{i+1}, r_i)$
    }
}
\caption{{\bf SOORL} \label{alg:soorl}}
\end{algorithm}

\textbf{Model Learning:} SOORL uses the method described in section \ref{sec:model_learning} with Cartesian product of object size $(w,h)$, object location $(x,y)$ and object intersection $(x',y')$ as features. Ignoring null input, this Cartesian product results in 7 different feature sets.

\textbf{Exploration:} Count based models allow SOORL to efficiently perform the knows what it knows (KWIK)~\citep{li2008knows} scheme for exploration (optimistic MCTS). Concretely, if our algorithm queries the transition or reward model with a previously unseen input, we consider the resulting state as a state with $R_{max}$ reward. $R_{max}$ reward is also considered for any previously unseen object interactions. As we observe the reward for each interaction we update the reward model based on model-based interval estimation~\citep{strehl2008analysis}.

\textbf{Planning:} At the beginning of each episode, a value function is trained based on previously seen transitions and rewards. Value function $V: \mathcal{O} \rightarrow \mathbb{R}$ is trained over object states and can generally be any function approximation methods.

Lookahead planning is performed by UCT~\citep{kocsis2006bandit} algorithm with $l$ rollouts and depth $d$. Algorithm \ref{alg:planning} shows pseudo code of planning, at each planning step, SOORL computes object interactions, selects appropriate models based on interactions and object states then predicts rewards and next object state. At depth $d$ of planning SOORL uses value of the object state $V(o_d)$ trained at the beginning of each episode.

\begin{algorithm}[h]
\KwIn{objects $o$, depth $d$, rollouts $l$, value function $V$}
\For{number of rollouts $l$}{
    set current depth $i$ to zero\;
    \While{$i \leq d$}{
        \If{$i == d$}{
            update $Q$ values with $V(o_i)$\;
            break\;
        }
        detect interactions $I_i$ with objects $o_i$ and select models $T,R$\;
        take action $a_i = argmax Q(s_i,a) + c \sqrt{\frac{log N(s,a)}{N(s)}}$\;
        obtain next object state $o_{i+1}$ and reward $r_{i}$ with $R(o_i,I_i), T(o_i,I_i)$ and update $Q$\;
    }
}
\caption{{\bf Lookahead Planning} \label{alg:planning}}
\end{algorithm}
