\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\iclrfinalcopy
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\newcommand{\eb}[1]{\textcolor{red}{[EB: #1]}}
\usepackage{hyperref}
\usepackage{url}
\usepackage{fancyhdr,graphicx,amsmath,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
%\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage[lofdepth,lotdepth]{subfig}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\rknote}[1]{\textcolor{red}{\textbf{RK: #1}}}
\newcommand{\danote}[1]{\textcolor{blue}{\textbf{DA: #1}}}
%\iclrfinalcopy

\title{Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model-Based Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ramtin Keramati\thanks{These authors contributed equally to this work}, Jay Whang\footnotemark[1], Patrick Cho\footnotemark[1], Emma Brunskill \\
Department of Computer Science\\
Stanford University\\
Stanford, CA 94305, USA \\
\texttt{\{keramati,jaywhang,patcho,ebrun\}@cs.stanford.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}


Humans learn to play video games significantly faster than the state-of-the-art reinforcement learning (RL) algorithms. People seem to build simple models that are easy to learn 
%\danote{Learning the model is fast or learning, given the model, is faster?} 
to support planning and strategic exploration. Inspired by this, we investigate two issues in leveraging model-based RL for sample efficiency. First we investigate 
%\danote{If this is a contribution of the paper, I'm guessing you did more than just consider it but actually *show* it. Also, I tend to write in present tense, your call.} 
how to perform strategic exploration when exact planning is not feasible and empirically show that optimistic Monte Carlo Tree Search outperforms posterior sampling methods. Second we show how to learn simple deterministic models to support fast learning using object representation.
%\danote{Again, the OO representation makes the model learning faster or makes learning with the model faster?} 
We illustrate the benefit of these ideas by introducing a novel algorithm, Strategic Object Oriented Reinforcement Learning (SOORL), that outperforms state-of-the-art algorithms in the game of \textit{Pitfall!} in less than 50 episodes.

\end{abstract}

\input intro

\input pre

\input approx_plan
\input approx_learn

\input soorl
\input pitfall
\input related_work
\input conclusion

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\end{document}
