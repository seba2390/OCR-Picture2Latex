\section{Related Work}

Model based RL has accomplished great success in tasks in which a perfect simulator is known~\citep{silver2016mastering, silver2017mastering}, mostly using Monte Carlo Tree Search algorithms like UCT~\citep{kocsis2006bandit, chaslot2010monte}. Recent work has focused on applying deep learning to model-based RL by learning the model online~\citep{rosin2011nested, finn2017deep, lenz2015deepmpc, weber2017imagination}. In contrast to these methods, we seek to learn a simple model based on object representation.

Exploration has been extensively studied in the tabular setting~\citep{brafman2002r,strehl2008analysis,jaksch2010near, osband2016posterior}. However, these methods do not scale well to large MDPs and often result in poor sample complexity. Additionally, these methods assume exact planning that might not be feasible in large state space. Recent approaches~\citep{bellemare2016unifying, ostrovski2017count, tang2017exploration} proposed an extension of count based exploration to large MDPs. Despite good asymptotic performance, when compared with humans \citep{tsividis2017human, lake2017building}, these methods require orders of magnitude more samples.

Bayesian RL methods also provide an effective balance between exploration and exploitation~\citep{ghavamzadeh2015bayesian}. When exact planning is possible, they provide strong guarantees. However, these methods remain computationally intractable in large state spaces. Recent works propose an extension of these methods to large MDPs~\citep{osband2016deep, azizzadenesheli2018efficient, fortunato2017noisy}. However, these methods are unable to show substantial improvement in hard exploration, sparse reward environments. 

A closely related line of research, finite horizon planning~\citep{kearns2002sparse, mannor2007bias, kearns1994introduction}, has noted how planning horizon can affect planning loss~\citep{kearns2002near, strehl2009reinforcement}. Recent study has shown that shorter planning horizons might be better when there is model inaccuracy~\citep{jiang2015dependence}. In contrast, in this work, we studied how imperfect planning can affect exploration.

OOMDP \citep{diuk2008object} defines a notion borrowed from relational MDPs \citep{guestrin2003generalizing}, and uses objects to learn models and perform model based planning. Model free methods that use object representation \citep{garnelo2016towards, roderick2017deep, cobo2013object} fail to scale to large MDPs and do not leverage object representation for deep exploration. The main difference between our approach and other object oriented approaches is that we perform scalable planning with strategic exploration by leveraging objects to learn simple dynamics models.

% Strategic exploration has been extensively studied in the tabular setting. Most of these techniques use the notion of Optimism in Face of Uncertainty (OFU) to achieve strong theoretical guarantees \citep{brafman2002r,strehl2008analysis,jaksch2010near, osband2016posterior}. However, these methods do not scale well to large MDPs and often result in poor sample complexity. To tackle this problem, \citep{bellemare2016unifying} proposed an extension of count based exploration to large MDPs. They do so by assigning an exploration bonus that is inversely proportional to a pseudo-count. \citep{ostrovski2017count} used neural density models and \citep{tang2017exploration} achieved generalization over pseudo-counts by using simple hash functions. Despite good asymptotic performance in hard exploration sparse reward games like Montezuma's Revenge, these methods still require an enormous amount of data to learn.

% Bayesian RL methods also provide an effective balance of exploration and exploitation~\citep{ghavamzadeh2015bayesian}. However, these methods remain computationally intractable in large state spaces. \citep{osband2016deep, azizzadenesheli2018efficient, fortunato2017noisy} proposed an extension of these methods to large MDPs. However, these methods are unable to show substantial improvement in hard exploration, sparse reward environments.

% Model based RL can improve sample efficiency. Bayes Adaptive MDP \citep{duff2002optimal} is a powerful tool for combining posterior sampling exploration and model based planning. However, these methods are generally intractable in large MDPs. BAMCP \citep{guez2012efficient} proposed a tractable sample-based method for approximate Bayes-optimal planning with root and lazy sampling over parameters of the state model.


% Model based RL can be used to efficiently find the optimal policy when the environment's state and reward models can be learned \citep{kuvayev1997model}. However methods like dynamic programming are mostly intractable in large environments \citep{sutton1998reinforcement}. UCT \citep{kocsis2006bandit} provides a scalable planning tool by guiding Monte Carlo planning \citep{chaslot2010monte} with multi-arm bandit using UCB exploration. However, these methods assume a known state and reward model a priori for performing roll out.

% Bayes Adaptive MDP \citep{duff2002optimal} is a powerful tool for combining posterior sampling exploration and model based planning. However, these methods are generally intractable in large MDPs. \citep{guez2012efficient} proposed a tractable posterior sampling exploration methods with root and lazy sampling over parameters of state model.

% Perhaps, the most related line of research is object oriented representation for RL. Model free methods to use object representation \citep{garnelo2016towards, roderick2017deep, cobo2013object} fail to scale to large MDPs and do not leverage object representation for strategic exploration. OOMDP \citep{diuk2008object} defines a notion borrowed from relational MDPs \citep{guestrin2003generalizing}, and uses objects to learn models and perform model based planning. The main difference between our approach and other object oriented approaches is that we perform scalable planning with strategic exploration by leveraging objects to learn simple dynamics models.

% %% Less verbose, swap a bunch to introduction and a bunch to experiment
% Recent advances in Deep Reinforcement Learning \citep{mnih2015human, van2016deep, mnih2016asynchronous} have successfully extended tabular setting RL algorithms like Q-learning \citep{watkins1992q} to large state space MDPs. In particular, these algorithms have achieved either human or super human performances in many Atari games \citep{bellemare2013arcade} . However, when compared against humans \citep{tsividis2017human, lake2017building}, these methods require orders of magnitude more samples to learn due to poor generalization, and the lack of strategic exploration and planning \citep{tsividis2017human}. Methods to improve sample efficiency can be split into two main categories - strategic exploration and model-based RL (cite??).\\\\
% Strategic exploration has been extensively studied in the tabular setting. Most of these techniques use the notion of Optimism in Face of Uncertainty (OFU) to achieve strong theoretical guarantees \citep{brafman2002r,strehl2008analysis,jaksch2010near, osband2016posterior}. However, these methods do not scale well to large state space MDPs and often result in poor sample complexity (cite??). To tackle this problem, \citep{bellemare2016unifying} proposed an extension of count based exploration to large MDPs. They do so by assigning an exploration bonus that is inversely proportional to a pseudo-count. This psuedo-count is derived from the prediction gain in the Context Tree Switching (CTS) density function. \citep{ostrovski2017count} extended this approach by using a neural density model, while \citep{tang2017exploration} achieved generalization over pseudo-counts via simple hash functions. Despite good asymptotic performances in hard exploration sparse reward games like Montezuma's Revenge, these methods still require a huge amount of data to learn.\\\\
% Besides OFU, Bayesian RL methods have also been used to perform strategic exploration. Bayesian RL methods provide an efficient balance of exploration and exploitation in the tabular setting \citep{ghavamzadeh2015bayesian}. However, these methods are computationally intractable in large state spaces (cite??).  While \citep{osband2016deep, azizzadenesheli2018efficient, fortunato2017noisy} have proposed an extension (what kind?) of Bayesian RL methods to large MDPs, their methods do not show substantial improvement in hard exploration sparse reward environments.\\\\
% Another common approach in improving sample efficiency is to use model based RL. Model based RL approaches \citep{kuvayev1997model} often have better sample efficiency than model free approaches, especially in settings when the environment's state and reward models can be easily learned (cite??). In particular, once the environment's state and reward models are learned, dynamic programming can derive the optimal policy. Even in large environments where dynamic programming is intractable \citep{sutton1998reinforcement}, Upper Confidence Trees (UCT) provide a scalable planning tool by guiding Monte Carlo Tree Search (MCTS) planning \citep{chaslot2010monte} with a multi-arm bandit using Upper Confidence Bound (UCB) exploration. The difficulty of model based RL in large state space environments is in learning the state and reward models efficiently (cite??).\\\\
% Work has also been done to combine strategic exploration and model based RL to further improve sample efficiency. In particular, Bayes Adaptive MDPs \citep{duff2002optimal} combine posterior sampling exploration and model based planning to balance between exploration and exploitation. Unfortunately, finding the corresponding Bayes-optimal policy is generally computationally intractable. To make the problem tractable, \citep{guez2012efficient} proposed a sample-based method that approximates Bayes-optimal planning. This approximate planning is done by lazily sampling models from the current belief at each node of the MCTS.\\\\
% Given that strategic exploration and model-based RL methods generally do not scale well to large MDPs, one possible solution is to instead perform the reverse operation by converting large MDPs into smaller MDPs before applying the aforementioned methods. One line of research in this direction is known as object oriented RL. Concretely, instead of using states and state features, the MDP is reduced to objects and object attributes (cite Littman??). Methods that have attempted to combine object oriented MDPs with model free methods \citep{garnelo2016towards, roderick2017deep} have so far failed to scale to large MDPs. Furthermore, these model free methods are unable to leverage object representation for strategic exploration. 

% Perhaps the most similar work to our own approach is Deterministic Object Oriented RMax (DOORMAX) \citep{diuk2008object}. DOORMAX 

% Main differences
% 1. Rather than having attributes, conditions and effects, just have simple attributes that can be derived from standard CV (i.e. bounding boxes) and simple conditions (intersection of Bounding Boxes). This is more general than something like "on(Man,Ladder)", which CV methods in general do not specify. 
% 2. Strategic Exploration over object interactions and object states.
% 3. Models learnt have exponential backoff history and also learned features. They are initially bayesian although eventually, with sufficient history backoff, should also be deterministic.
% 4. Solve pitfall.

% Perhaps, the most related line of research is object oriented representation for RL. Model free methods to use object representation \citep{garnelo2016towards, roderick2017deep} fail to scale to large MDPs and leverage object representation for strategics exploration. \citep{diuk2008object} defines a notion borrowed from relational MDP \citep{guestrin2003generalizing}, and provides a promising notion for learning models and model based planning. Our main difference with these approach is that, we leverage objects to learn a simple dynamic model and perform scalable planning with strategic exploration, that allows us to learn orders of magnitude faster than other methods on Atari games that demands strategic exploration.

 