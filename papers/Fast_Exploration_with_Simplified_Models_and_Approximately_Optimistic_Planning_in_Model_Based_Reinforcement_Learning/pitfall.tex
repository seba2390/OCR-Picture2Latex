\section{Empirical Evaluation}\label{sec:pitfall}

In this section we will evaluate SOORL on an Atari game \textit{Pitfall!}. SOORL assumes access to an object detector, a predefined set of function classes and macro action (that can be learned using algorithm described in section \ref{sec:macro}). 

Labeled data for object's in Atari game in not available, and in our experiment we extracted the objects from Atari RAM and screen information. The need for an object detector makes engineering burden of SOORL prohibitive to test the algorithm on all Atari games. Thus we focused on one of the hardest game (sparse reward and hard exploration~\citep{bellemare2016unifying}), \textit{Pitfall!}, where all the previous methods (without human demonstrations) failed to achieve any positive reward. We showed that object representation can be extremely helpful in this hard exploration game and SOORL can achieve a positive reward in \textit{Pitfall!} without human demonstration. 

\textit{Pitfall!} is an Atari2600 environment where the goal is to have the agent traverse through multiple rooms (255 in total) while collecting rewards and avoiding obstacles. It is arguably one of the hardest Atari2600 game~\citep{hester2017deep} due to its large map, sparse positive and dense negative rewards that necessitate deep exploration and long-horizon planning. The $\epsilon$-greedy exploration strategy completely fails in this environment, and more recent count-based exploration \citep{bellemare2016unifying} does not show much performance boost due to the sparsity of positive reward. Pitfall is difficult even for human players without prior knowledge of the game -- \citep{hester2017deep} reports that human performance varies from 3662 to 47821 points, whereas for other hard Atari games, this variation is much smaller (e.g. from 32300 to 34900 for Montezuma's revenge).

\subsection{Details}
All objects information are extracted from RAM and screen information, and each object's attribute is location $(x,y)$ and bounding box size $(w,h)$. Objects are considered interacting with each other if bounding boxes collide. Transition and reward models for each object are based on the method described in Section \ref{sec:model_learning}. The features used are a Cartesian product of object size $(w,h)$, object location $(x,y)$ and object intersection $(x',y')$. Ignoring null input, this Cartesian product results in 7 different feature sets.

As described in section \ref{sec:exp} we used optimism based exploration method, by assigning reward $R_{max}$ to all unseen interactions and transitions. As we observe reward for each interaction we update the model, based on model based interval estimation~\citep{strehl2008analysis}. Additionally in order to further incentivize exploration we split the screen into $N \times  M$ grids and keeping a count of the number of times agent visits each grid. The agent is given a reward bonus $\beta n(s)^{-1/2}$ based on visit count $n(s)$.

For value function, we used discrete object's location with the same split used for optimism bonus, and computed the empirical transition $\hat{T}(o'|o,a)$ and empirical reward with optimism bonus $\hat{r}(o,a) + \beta n(s)^{-1/2}$, and at the beginning of each episode, we performed value iteration.

\subsection{Performance and Discussion}


% \textbf{**EB: Can we also try plotting episodes vs number of times that run got a positive reward in an episode (so mostly 0 then 1 then 2 ...) and then average these? Add add error bars}\rknote{I didn't understand how to plot this}

Figure \ref{fig:pitfall}(a) shows an increasing number of rooms being discovered across episodes. On average, the agent discovers 21 rooms within 50 episodes, and in total across 20 runs the agent discovers 27 different rooms. This validates our hypothesis that optimistic MCTS drives deep exploration, that we also showed in a smaller domains in section \ref{sec:exp}. 

 Figure \ref{fig:pitfall}(b) shows accumulative reward for each episode of SOORL and compares them to the other state of the art algorithm. Results of count-based \citep{bellemare2016unifying}, DQfD \citep{hester2017deep}, A3C \citep{mnih2016asynchronous} and DQN \citep{mnih2015human} are reported at the time of evaluation. Our average score across all episodes and all runs is $-193.5 \pm 595.8$ and SOORL score for the best episode across all runs in $606.6 \pm 1254.5$, which is higher than all scores that were reported at the time of evaluation. To the best of our knowledge, this is the first approach which manages to get positive rewards on \textit{Pitfall!} without human demonstrations.  Sample videos of the agent reaching the two closest positive rewards can be found here: \url{https://youtu.be/GvenPZMJiTg} (4000 reward)\,\,
\url{https://youtu.be/74F-ta5LyuA} (2000 reward)

Figure \ref{fig:pitfall} (c) shows the percentage of runs that got a positive reward. More than $80\%$ of runs got a positive reward in 50 episodes that shows consistency of our approach across multiple runs. Due to simple dynamics model that can be learned fast SOORL is extremely sample efficient in comparison to other deep RL method that often takes millions of frame to find a good policy. However, end-to-end deep RL methods use significantly less prior knowledge and using raw pixels as input, thus a direct comparison with them is unfair. We have also provided macro actions and object information for DQN and DDQN but those methods are not designed to take advantage of object representation and did not show a boost in performance. On the other hand, SOORL uses much less prior knowledge than methods with human demonstration~\citep{aytar2018playing, hester2017deep}, where human guidance enormously reduce the challenge of exploration.

Additionally, macro actions are provided for SOORL, but as shown in section \ref{sec:macro} these can be learned online. Integrating this temporal abstraction with SOORL can increase the sample complexity by $\mathcal{O}(|A|K)$, where $K$ is the maximum number of no-ops.


\begin{figure}[tb]
    \centering
    \subfloat[Room Discovery]{\includegraphics[width=.29\linewidth]{img/pitfall_room.pdf}}%
    \subfloat[][Reward per episode]{\includegraphics[width=.31\linewidth]{img/pitfall_reward.pdf}}%
    \subfloat[][Percentage of runs with positive reward]{\includegraphics[width=.33\linewidth]{img/pitfall_perc.pdf}}%
    \caption{performance of SOORL on Pitfall!}%
    \label{fig:pitfall}%
\end{figure}





