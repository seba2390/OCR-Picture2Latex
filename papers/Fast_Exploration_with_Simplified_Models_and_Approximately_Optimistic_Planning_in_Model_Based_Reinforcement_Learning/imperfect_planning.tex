\section{Imperfect Planning}\label{sec:imperfect_plan}
Planning in an MDP with known transition and reward models can be efficiently done by focusing on promising branches of state-action tree using the UCT algorithm \citep{kocsis2006bandit}. However, when a perfect simulator of the environment is not known, learning models sufficient for planning can be hard, if not impossible. And the performance of model-based RL depends on the performance of the learned model, which often suffers from error in function approximation~\citep{talvitie2015agnostic, talvitie2014model}. These errors compound in planning and can cause \textit{catastrophic planning} errors (e.g. agent dying).

There are strong guarantees that model-based strategic exploration methods will converge to the optimal policy~\citep{brafman2002r, thompson1933likelihood, guez2012efficient}, while performing exact planning. However, exact planning in large MDPs is often impossible even if the perfect simulator is know. Thus, the resulting performance of model-based exploration methods is unclear when combining with UCT algorithm with limited depth and finite number of rollouts. 

In this section we first introduce two toy examples \textit{Pong Prime} and \textit{mini-Pitfall!} and then investigate strategic exploration with imperfect planning. Then we study compounding errors and introduce a notion of macro actions similar to previous works~\citep{diuk2008object}, that will help us to learn a simple accurate model sufficient for planning.

\textit{Pong Prime} environment is designed for a hard exploration task. Dynamics of this game is similar to Pong with minor tweaks that make the game significantly harder. The enemy paddle is made 3 times larger than the player paddle, so it is impossible to score a point by simply hitting the ball back at the normal speed.  Additionally, the top and bottom 10\% percent of the enemy paddle hit the ball back at 1.5 times the normal speed so that the enemy paddle is even more powerful. Similarly, the player paddle also consists of 3 regions with distinct behavior. The \emph{top region} takes up the top 50\% of the paddle and hits the ball back at 1.5 times speed. The \emph{middle region} takes up the middle 45\% of the paddle and hits the ball back at normal speed. Finally, the \emph{lower region} covers remaining 5\% and instantly wins a point for the player. This configuration is set up so that it is difficult but not impossible for the player to score using the top region (scoring on average around 5\% of the time the ball bounces off the top region). In this setting, the optimal policy is to always hit the ball with the lower region of the ball. The game is deterministic and model free methods with $\epsilon$-greedy exploration (e.g. DDQN) consistently loses the game with lowest possible score across 5000 episodes.

\textit{mini-Pitfall!} is a small version of Atari2600~\citep{bellemare2013arcade} game \textit{Pitfall!} which we use as a final test bed of our algorithm. In this version we limit the agent to the first room of the game, and there exist a dummy reward $R_{max}$ at the right end of the room, and the left end is a terminal state. Figure \ref{fig:meta_action}(a) shows this environment. 

\subsection{Exploration}

\begin{figure}[tb]
    \centering
    \subfloat[Pong Prime environment]{\includegraphics[width=.31\linewidth]{img/PongP.pdf}}%
    % \enskip
    \subfloat[Different exploration methods]{\includegraphics[width=.33\linewidth]{img/pongp_comparison.pdf}}%
    % \enskip
    \subfloat[Optimism based exploration]{\includegraphics[width=.33\linewidth]{img/pongp_optimism.pdf}}%
    \caption{Pong Prime}%
    \label{fig:pongp}%
\end{figure}

\begin{table}[tb]
  \begin{center}
    \begin{tabular}{r|c|c|c}
      \toprule 
      \textbf{Method} & Optimism & Thompson Sampling & BAMCP\\
      \midrule
      \textbf{Number of Episdoes} & 5.0 $\pm$ 1.7 & 6.2 $\pm$ 1.95 & 11.0 $\pm$ 2.5\\       
      \bottomrule % <-- Midrule here
      \end{tabular}
      
  \end{center}
  \caption{Comparison of different exploration methods in \textit{mini-Pitfall!} to consistently achieve the reward at the end of the first room.}
  \label{tab:mini_pitfall}
\end{table}

We consider three different methods of exploration combined with UCT algorithm: \textbf{Optimisim}~\citep{brafman2002r}, \textbf{Thompson Sampling}~\citep{thompson1933likelihood}, and \textbf{BAMCP} (a tractable sample-based method for approximate Bayes-optimal planning with root and lazy sampling over parameters of the state mode, please refer to~\cite{guez2012efficient} for details). We evaluate their performance combined with UCT algorithm in a toy example \textit{Pong Prime}, which we designed for a hard exploration task.


Algorithm \ref{alg:stratigic} describes object-level planning with strategic exploration using UCT. At each state $s$, we select the appropriate distribution over models for the corresponding object representation $f(s)$ (section \ref{sec:model}) and use UCT to pick the best action . We perform: Thompson Sampling, by sampling a model at the beginning of planning (equivalent to setting $K = 1$ in Algorithm \ref{alg:planning}); BAMCP, by sampling a model for each rollout (equivalent to setting $L = 1$ in Algorithm \ref{alg:planning}); and optimism based exploration, by selecting an optimistic model ($R_{max}$ for the unseen transitions/ rewards) and $L$ rollouts. We compared these methods to Baseline, which uses a MLE model with UCT algorithm and no exploration. 

\begin{algorithm}
\KwIn{state $s$, number of models $K$, number of rollouts $L$, planning depth $d$}
\SetAlgoLined
$o \gets$ detectObjects($s$)\;
$\mathcal{T}, \mathcal{R} \gets$ selectModels($o$)\;
  \For{$k$ in 1:$K$}{
        $T\sim\mathcal{T}$, $R\sim\mathcal{R}$\;
        \For {$l$ in 1:$L$}{
            UCT($T, R, o, d$)\;
        }
    }
  \Return $argmax\ Q(s,.)$\;
 \caption{\bf Strategic Exploration with UCT}\label{alg:stratigic}
\end{algorithm}

The correct model class for dynamics of each paddle is a linear model with 3 action history. We assume that the learned dynamics model uses this true model class (i.e. performs linear regression on the transitions we observe) so we can separate the effect of exploration from model mismatch. Figure \ref{fig:pongp}(b) compares the performance of different exploration strategies to the baseline. We perform 500 total tree searches for all runs in Figure \ref{fig:pongp}(b) (i.e. Thompson Sampling uses 500 rollouts with 1 model, BAMCP 500 models each with 1 rollout).

Both BAMCP and TS perform worse than the MLE model. We hypothesize that this result is due to approximate planning, as in the limit of infinite simulations, BAMCP is guaranteed to converge to the Bayes optimal solution. Similarly, with full horizon planning, we should be able to compute the exact value for the model sampled with TS, and there are strong guarantees that such a method will converge to the optimal policy. 
However in practice, especially in large domains or domains with real-time constraints, the amount of computation, and therefore the quality of the computed plan, will be significantly limited. In particular, if it is infeasible to use a depth that mimics the game horizon, or perhaps even to reach a local reward, then TS may suffer. This is because TS samples a single model, which means that parts of the model may be overly optimistic, while other parts may be pessimistic. Hence, when performing a limited number of simulations using UCT, we may not go down branches of the tree that \textit{observe} the optimistic parts of the sampled model. Therefore, the computed estimates of the Q value at the root node may not be optimistic, which is often a key part of proofs of the effectiveness of TS methods, and very helpful empirically. 

BAMCP faces similar challenges, but suffers further in this domain because the true domain is deterministic. This means that for TS, optimism, and MLE approaches, the tree constructed will only have one child node (the deterministic next state) for any chosen action. In contrast, BAMCP samples a different deterministic model at each rollout, and for the same action node, those models may each deterministically predict different next states. Hence, BAMCP with $M$ sampled models and planning horizon $H$, potentially builds a tree of size O($(|A|M)^H$), in contrast to the other methods that build a tree of at most size O($|A|^H$), where $|A|$ is the size of action space. 

Optimism-based exploration significantly outperforms other approaches. We suspect it is more robust to approximate planning, since optimism is built into \textit{every} node, allowing it to distinguish even locally between actions that may need exploration, in absence of observing long delayed reward. To find the optimal policy still requires significant lookahead and careful planning. 

Indeed as we demonstrate in Figure \ref{fig:pongp}(c) for the optimistic method, as planning power increases through more simulations, the performance of optimism-based exploration also increases. We expect that with sufficient computations the optimistic method should eventually learn the optimal policy for this domain. 

Same results hold for \textit{mini-pitfall!} environment, table \ref{tab:mini_pitfall} shows the performance of different exploration methods in achieving the reward on the right side of the room consistently (three consecutive times). In this experiment we provided a transition model class that is sufficient for planning and doesn't suffer from compounding error, to separate the effect of exploration. 




% \subsection{Compounding Errors}\label{sec:comp_error}

% Lookahead planning requires an accurate transition and reward model, a small error in these models can compound overtime and cause a \textit{catastrophic error} in planning. These small errors in model, often caused by function approximation, are often inevitable in large MDPs. We show later in this paper how object representation can help us learn simple models, that are more accurate but still may suffer from compounding errors. 

% Re-planning at each step might remedy this issue and avoid catastrophic errors (e.g. agent dying) as long as the planned trajectory is not very far from the reality. Similar to MPC (model predictive controller), a commonly used control method with successful application in robotics~\citep{tamar2017learning, erez2013integrated, camacho2013model}, we hypothesize that re-planning at each step makes the agent robust to potential model error.  In this section we propose a novel method to bound the error made in evaluating a policy. By doing so we avoid some catastrophic errors in the cost of limiting our planning horizon.

% \begin{lemma}\label{lemma:error}
%     Let $M, \hat{M}$ be Markov decision processes over the same state and action space with the same reward function with transition probabilities $P$ and $\hat{P}$ respectively. For any policy $\pi$, $|V_{M}^{\pi} - V_{\hat{M}}^{\pi}|$ is bounder by a polynomial in $\epsilon, R_{max}, |S|, d$. Where $d$ is the depth of the planning, $R_{max}$ maximum reward, $|S|$ size of state space, and $\epsilon$ is the $max_{\tau}\sum_{(s_{i+1}, \pi(s_i),s_i) \sim \tau}P(s_{i+1}|s_i,\pi(s_i)) - \hat{P}(s_{i+1}|s_i,\pi(s_i))$ where the summation is over all the realizable trajectories following policy $\pi$ in $M$.
% \end{lemma}

% Lemma \ref{lemma:error} states that the error made in evaluating a value of a policy $\pi$ can be bounded by the depth of planning $D$ as long as all $D-$step path have summation of one step error in transition probability less than a constant $\epsilon$. However this result is for exact planning, and in reality using UCT like method we are only sampling some trajectories and obtaining an estimation of the value. Additionally, here we assume access to the error, where in practice we can only estimate them based on previously seen examples. 

% Additionally, In practice we might have a model that has fairly small error in most of the states, but some catastrophic errors for a small subset of states (e.g. consider a model specification mismatch that one action is not modeled, then prediction of the next state for all but one action can be fairly accurate). In this situation even one-step lookahead planning can surpass the error threshold $\epsilon$, and we are unable to plan. In order to avoid this problem, in our experiment we passed the value of a leaf node in tree search anytime the summation of error has passed the threshold.

% TODO: Add experiments!!!!!
\subsection{Approximately deterministic model to support fast learning}\label{sec:model}
%\subsection{Macro Actions}\label{sec:meta}

\begin{figure}[tb]
    \centering
    \subfloat[mini \textit{Pitfall!}]{\includegraphics[width=.25\linewidth]{img/pitfall.pdf}}%
    % \enskip
    \subfloat[Total Model Entropy]{\includegraphics[width=.37\linewidth]{img/pitfall_entropy_meta.pdf}}%
    % \enskip
    \subfloat[Per Action Entropy]{\includegraphics[width=.37\linewidth]{img/pitfall_entropy_actions.pdf}}%
    \caption{Macro actions}%
    \label{fig:meta_action}%
\end{figure}

paragraph1 : sample efficient model-based RL requires learning an easy model that can be learned by few data. Inspired by human, and many real work application follows simple physical law.

paragraph 2: Additionally, model need to be simple to support fast planning. Lookahead planning requires querying the model in multiple time steps, so simple model is necessary to be computationally feasible. Additionally models need to be simple enough to be compatible with deep exploration. And deterministic model can hellp fast planing by reducing the branch factor in tree.


\subsection{Model Learning}
paragraph 3: Object Oriented deterministic models is a promising way to learn simple model, sufficient for planning, and easy to learn. OOMDP used object oriented MDP to efficiently solve small domain with exact planning. Additionally previous works suggest that human (cite) might be using object level planning. 

Paragraph 4: State of the art vision algorithm (cite Yolo, fast RCNN) can simply detect objects with high accuracy. Factored MDP is a promising way to model large MDP with smaller set of feature, and we hypothesize that objects attributes  are sufficient parent features to describe the full MDP.
\subsubsection{Object Representation}

\subsection{Learning Models}
Although transition and reward models can be modelled by functions as general as neural networks, using such complicated functions can make performing strategic exploration difficult. Moreover, such functions require much more data to train and often severely suffer from compounding errors. Given that we are planning at an object level, we hypothesize that even simple models, such as linear and discrete count based models, give sufficient accuracy for planning, since often objects follow a very simple physical laws. More importantly, to ensure "sufficient accuracy in planning", we further require that these models predict transitions and rewards in a deterministic fashion. 

To ensure deterministic transitions, we consider the class of functions $\mathcal{F}_t = \{f_{t}^{1},\dots,f_{t}^{n}\}$, where each $f_{t}^{i}$ is a count-based model of the dynamics for an object. Each function stores the count of every output based on a different set of input features with given history $t$. The simplest model in $\mathcal{F}_1$ is $f_{1}^{1}$, which uses one history with null input. For example, for a falling object with steady state velocity, such a model is sufficient as we can predict displacement $\delta x$ and $\delta y$ without any input. On the other hand, $f_{1}^{n}$, which uses one history and the most complex set of features, is the most complicated model in the class $\mathcal{F}_1$. In terms of objects, the most complex set of input features that we consider is the union of the object's state features, relative state features with respect to an interacting object, and action. 

The goal then is to choose the simplest model that achieves deterministic transitions within $\mathcal{F}_t$. To do so, we compute the entropy of the observed data for each function $H(f_{t}^{k}) = -\sum_{x_i} p(x_i) \log(p(x_i|f_{t}^{k}))$ where the summation is over all the observed data. We choose the simplest model that has entropy less than a predefined threshold $\epsilon_{ent}$. If none of the models in $\mathcal{F}_t$ satisfy the entropy threshold, we increase $t$ through an exponential back off scheme. Concretely, we increase the history to the next exponent of 2. Figure \ref{fig:pitfall:metaction}(a) shows an example (for our experiment in \textit{Pitfall!}) of the impact of this exponential back off scheme. With sufficient history, the entropy of the model eventually drops to zero. We use the same approach and same class of models for reward functions.

\begin{figure}[tb]
    \centering

    \subfloat[Model entropy]{\includegraphics[width=.36\linewidth]{img/pitfall_entropy.pdf}}%
    \quad
    \subfloat[Example trained models]{\includegraphics[width=.595\linewidth]{img/Tree.pdf}}%
    \quad
    \caption{Model selection}%
    
    \label{fig:pitfall:metaction}%
\end{figure}

\subsection{Temporal Abstraction}
In the previous section we proposed a method to avoid \textit{catastrophic error} in planning due to compounding errors. However compounding error can still limit our ability to plan for longer horizons. Object representation allows us to learn a simple predictive model of the dynamics for each object class and eliminates the necessity to use complicated function approximation methods. And thus enable us to plan for longer horizon.

As we observed in our experiments, objects transitions, especially action-dependent transitions, can show a highly nonlinear behaviour and dependency to multiple time step histories. Inspired by human's \textit{reaction time} and previous work~\citep{diuk2008object} we use the notion of macro action in the form of \textit{"act and then wait"} in order to learn a simple deterministic transition models. 

Algorithm \ref{alg:macro} shows the pseudo code to learn the macro actions from predefined atomic actions (e.g. in the simplest form it can be the action space of the desired MDP; however, one can define them as any n-token combination of actions). In algorithm \ref{alg:macro} we augment all atomic actions with $k$ number of \textit{no-op} or wait, and then greedily decrease the number of no-ops followed by each action such that all model's can deterministically predict the next object state. The  goal is to have a model that achieves deterministic transition, to do so we measure the entropy of model's prediction with the observed data. 

\begin{algorithm}
\KwIn{maximum number of no-op $k$, set of atomic actions $\mathcal{A}$}
\SetAlgoLined
macro actions $\gets$ atomic action followed by maximum number of no-op\;
mark all macro action as reducible\;
\While{there exist a reducible macro action}{
    $a \gets$select a reducible macro action and decrease number of no-op by one\;
    $\tau \gets$ compute entropy of model's prediction with new set of macro actions\;
    \If{$\tau \geq thresh$}{
        mark $a$ as non reducible macro action and restore the number of no-op for $a$\;
    }
}
 \caption{\bf Macro Actions}\label{alg:macro}
\end{algorithm}

Figure \ref{fig:meta_action}(b) shows the total entropy of model's versus total number of \textit{no-op} in \textit{mini-Pitfall!} environment. As algorithm progress entropy increases to pass the threshold. Figure \ref{fig:meta_action} (c) shows the entropy of all models while reducing number of \textit{no-op} for each action and keeping others constant. As it shows Jump Right requires 8, Down requires 4 and Right requires none \textit{no-op} afterward to achieves deterministic transition for all models. Running this algorithm in \textit{Pong Prime} environment results in two \textit{no-op} after each action, since the real dynamics uses 3 step history.    
