\section{Approximately deterministic model to support fast learning}\label{sec:model}

In the previous section we considered how to plan and perform deep exploration given a transition and reward models. Now we ask a natural question, what types of model to learn?, and how these models will affect computational tractability of planning and sample efficient learning? In general simpler models are easier to learn and requires less data to train, in contrast to complex function approximation methods that often requires massive amount of data and suffer from compounding errors in lookahead planning~\citep{roderick2017deep, weber2017imagination}. Additionally, simple models are tractable to perform deep exploration with model uncertainty and reduce planning time.

There are reasonable evidence that humans learns a simple model, often inaccurate, to support planning and guide decision making~\citep{tsividis2017human, dubey2018investigating}. In particular, people seem to benefit from higher level object representations that allows them to factor a high dimensional state space into simple low dimensional object states, this allows human to generalize from few examples, explore and plan efficiently. 
Additionally, as discussed in section \ref{sec:exp_disc}, deterministic models can significantly help in planning and deep exploration, due to smaller branching factor of the tree (each state node will have only one child per action). Inspired by humans and prior works on object oriented MDP~\citep{diuk2008object} we hypothesize that object oriented models can help us learn a simple, easy and approximately deterministic models that are sufficient for planning .

Object detection has been long studied in computer vision, and state-of-the-art algorithms can detect objects with great accuracy in real world scenes (e.g. YoLo~\citep{redmon2017yolo9000}, fast RCNN~\citep{girshick2015fast} mask RCNN~\citep{he2017mask} and \dots). We expect that these algorithms can simply detect objects and their bounding boxes, when they are trained on Atari2600. Thus, here we assume objects are given, in section \ref{sec:model_learning} we discuss how to learn a simple model and further in section \ref{sec:macro} we show how temporal abstraction can help learning an approximately deterministic mode. 


% Learning a transition and reward models for large state MDPs often requires massive amount of data and the use of complex function approximations methods, that usually suffers from compounding errors~\citep{roderick2017deep, weber2017imagination} and are not well suited for lookahead planning methods like MCTS. For a sample efficient model-based RL algorithm, learning a simple model (that requires minimum amount of data) that can support planning (i.e. doesn't suffer from compounding errors) is vital. Additionally, performing deep exploration with complex function is often intractable and not computationally feasible.\rknote{CITE??}

% Prior works on human learning for Atari~\citep{tsividis2017human, dubey2018investigating} highlights people's ability to generalize from few examples, explore and plan efficiently. Additionally, people seem to benefit from higher level object representations that allows them to factor a high dimensional state space into simple low dimensional object states. Inspired by humans and prior works on object oriented MDP~\citep{diuk2008object} we hypothesize that object oriented models can help us learn a simple and easy models that are sufficient for planning.\rknote{relation to factored MDP and finding a set of parent features}



\subsection{Review of Object Oriented MDP}

We use a simpler version of OOMDP~\citep{diuk2008object}. we define a set of object classes $\mathcal{C} = \{c_1, \dots, c_n\}$ where each class has a set of attributes $\{c.a_1,\dots,c.a_m\}$. Each state $s$ consists of objects $f(s) = \{o_1,\dots,o_k\}$ where each object $o_i \in \mathcal{C}$. The state of an object is defined by the value assignment to its attributes. Finally, the state  $s$ of the underlying MDP is the union of all object states $\cup_{i=1}^{k} o_i$. 

We define the interaction function $I: \mathcal{O} \times \mathcal{O} \rightarrow \{0,1\}$ to be an indicator that determines if two objects are interacting with each other. For simplicity, we make three assumptions: first, that this interaction function is known; second, objects from the same class share the same transition function; and third, each object's next state is  dependent on at most pairwise object interactions and action.
An object's successor state is determined by a standalone transition function $T_c(o, a)$ or a pairwise transition function $T_{c_i,c_j}(o_i,o_j,a)$ if $I(o_i,o_j) = 1$.

\begin{figure}[tb]
    \centering
    \subfloat[mini \textit{Pitfall!}]{\includegraphics[width=.25\linewidth]{img/pitfall.pdf}}%
    % \enskip
    \subfloat[][Total Entropy of Models]{\includegraphics[width=.37\linewidth]{img/pitfall_entropy_meta.pdf}}%
    % \enskip
    \subfloat[][Per Action Entropy of Agent's Model]{\includegraphics[width=.37\linewidth]{img/pitfall_entropy_actions.pdf}}%
    \caption{Macro actions}%
    \label{fig:meta_action}%
\end{figure}

\subsection{Model Learning}\label{sec:model_learning}

Given that we are planning at an object level, we hypothesize that even simple models, such as linear and discrete count based models, give sufficient accuracy for planning, since often objects follow a very simple physical laws. More importantly, to ensure "sufficient accuracy in planning", we further require that these models predict transitions and rewards in a deterministic fashion.

To ensure deterministic transitions, we consider the class of functions $\mathcal{F}_t = \{f_{t}^{1},\dots,f_{t}^{n}\}$, where each $f_{t}^{i}$ is a count-based model of the dynamics for an object. Each function stores the count of every output based on a different set of input features with given history $t$. The simplest model in $\mathcal{F}_1$ is $f_{1}^{1}$, which uses one history with null input. For example, for a falling object with steady state velocity, such a model is sufficient as we can predict displacement $\delta x$ and $\delta y$ without any input. On the other hand, $f_{1}^{n}$, which uses one history and the most complex set of features, is the most complicated model in the class $\mathcal{F}_1$. In terms of objects, the most complex set of input features that we consider is the union of the object's state features, relative state features with respect to an interacting object, and action. 

The goal then is to choose the simplest model that achieves deterministic transitions within $\mathcal{F}_t$. To do so, we compute the entropy of the observed data for each function $H(f_{t}^{k}) = -\sum_{x_i} p(x_i) \log(p(x_i|f_{t}^{k}))$ where the summation is over all the observed data. We choose the simplest model that has entropy less than a predefined threshold $\epsilon_{ent}$. If none of the models in $\mathcal{F}_t$ satisfy the entropy threshold, we increase $t$ through an exponential back off scheme. Concretely, we increase the history to the next exponent of 2. We use the same approach and same class of models for reward functions.

Figure \ref{fig:pitfall:metaction}(a) shows an example for the game \textit{Pitfall!}, where we used a Cartesian product of object size $(w,h)$, object location $(x,y)$ and object intersection $(x',y')$ as features. Ignoring null input, this Cartesian product results in 7 different feature sets. With sufficient history, the entropy of all the models eventually drops to zero. 

\begin{figure}[tb]
    \centering

    \subfloat[Model entropy]{\includegraphics[width=.36\linewidth]{img/pitfall_entropy.pdf}}%
    \quad
    \subfloat[Example trained models]{\includegraphics[width=.595\linewidth]{img/Tree.pdf}}%
    \quad
    \caption{Model selection}%
    
    \label{fig:pitfall:metaction}%
\end{figure}

\subsection{Temporal Abstraction}\label{sec:macro}

As we observed in our experiments, objects transitions, especially action-dependent transitions, can show a highly nonlinear behaviour and dependency to multiple time step histories. Inspired by human's \textit{reaction time} and previous work~\citep{diuk2008object} we use the notion of macro action in the form of \textit{"act and then wait"} in order to learn a simple approximately deterministic transition models. 

Algorithm \ref{alg:macro} shows the pseudo code to learn the macro actions from a predefined set of atomic actions (e.g. in the simplest form it can be the action space of the desired MDP; however, one can define them as any n-token combination of actions). In algorithm \ref{alg:macro} we augment all atomic actions with $k$ number of \textit{no-op} or wait, and then greedily decrease the number of no-ops followed by each action such that all model's can deterministically predict the next object state. The  goal is to have a model that achieves deterministic transition, to do so we measure the entropy of model's prediction with the observed data as in section \ref{sec:model_learning}.

\begin{algorithm}
\KwIn{maximum number of no-op $k$, set of atomic actions $\mathcal{A}$}
\SetAlgoLined
macro actions $\gets$ atomic action followed by maximum number of no-op\;
mark all macro action as reducible\;
\While{there exist a reducible macro action}{
    $a \gets$select a reducible macro action and decrease number of no-op by one\;
    $\tau \gets$ compute entropy of model's prediction with new set of macro actions\;
    \If{$\tau \geq thresh$}{
        mark $a$ as non reducible macro action and restore the number of no-op for $a$\;
    }
}
 \caption{\bf Macro Actions}\label{alg:macro}
\end{algorithm}

Figure \ref{fig:meta_action}(b) shows the total entropy of model's versus total number of \textit{no-op} in \textit{Pitfall!} environment. As algorithm progress entropy increases to pass the threshold. Figure \ref{fig:meta_action} (c) shows the entropy of agent's models while reducing number of \textit{no-op} for one action and keeping others constant. As it shows Jump Right requires 8, Down requires 4 and Right requires none \textit{no-op} afterward to achieves a deterministic transition for all actions. Running this algorithm in \textit{Pong Prime} environment results in two \textit{no-op} after each action, since the real dynamics uses 3 step history.
