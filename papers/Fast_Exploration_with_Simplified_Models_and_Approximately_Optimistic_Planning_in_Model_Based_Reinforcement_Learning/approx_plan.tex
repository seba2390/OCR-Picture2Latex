\section{Approximate Model-Based Planning to Support Exploration}\label{sec:exp}

Computing an optimal policy for a Markov decision process in a large state space can be extremely computationally expensive, if not impossible. Therefore some form of approximate planning procedure is almost always needed for large scale  domains. Especially in model-based RL algorithms when the transition and reward models are initially unknown and should be learned online %\danote{online RL $\implies$ model-based RL?}
, a computationally tractable planning algorithm is more critical. Since the model estimates are changing and one should plan more frequently with the new model estimates.
% particularly critical in online RL methods where the model estimates are changing -- must be computationally tractable

One popular and powerful approach for scaling up planning for large Markov decision processes is Monte Carlo Tree Search~\citep{chaslot2008monte, browne2012survey}, which has been used to achieve better than world class performance in Go~\citep{silver2016mastering}. When a perfect transition and reward model is known, MCTS is guaranteed to converge to the optimal value function in the limit of infinite computation (i.e. infinite number of rollouts). However, when a model is estimated from the data and computational power is bounded, prior works has suggested how to adjust planning horizon to get the best computational performance~\citep{jiang2015dependence}. As of our knowledge there is no work that additionally investigate the performance of different exploration methods with approximate planning.%\rknote{--rewrite: this paragraph doesn't motivate the next one}
% MCTS is guaranteed to eventually converge to the optimal value given infinite computation and perfect dynamics and reward models
% Given bounded amount of computation, prior research has suggested how to adjust the horizon used in order to yield the best computational performance (NAN)

We are interested in how to leverage models to support deep efficient exploration in large domains. Prior research has strong guarantees to find a near optimal policy with deep exploration when exact planning can be done, often in small state space~\citep{brunskill2009provably, brafman2002r}. In contrast, in this work, we investigate how to leverage models to support deep exploration when exact planning is not possible (large state space). 
% in most theoretical work, state space small, planning done exactly
% other times assume feasible to do e-optimally (cite my JMLR 2009 paper)
% in contrast, want to consider how to leverage models to support exploration when can't plan exactly

We propose a new optimistic MCTS to support deep exploration guided by models. Particularly, we suggest to perform a MCTS algorithm (e.g. UCT~\citep{kocsis2006bandit}), and use the learned reward $R$ and transition model $T$ that is optimistic toward unseen or less frequently seen part of the state-action space (we further discuss how to learn simple models for planning in section \ref{sec:model}). 
%\danote{A little more detail might help really nail down what differentiates your modified MCTS from regular MCTS} 
Concretely, while performing rollouts using learned models at each step an optimistic reward bonus is given. Optimistic reward bonus can be given by any optimism in the face of uncertainty (OFU) algorithm (e.g. MBIE-EB~\citep{strehl2008analysis} or Rmax~\citep{brafman2002r}). %\rknote{pseudo code might be useful}




Optimistic MCTS may have some advantages over alternate ways to achieve deep exploration in large state spaces given limited computation. 
In particular optimistic MCTS is leveraging model uncertainty to drive deep exploration, in contrast to policy search methods with simulated models~\citep{sutton2000policy} that relies on the stochasticity of the policy for exploration and it is unclear how to leverage model uncertainty.

Additionally, MCTS with posterior sampling methods, like Thompson Sampling~\citep{thompson1933likelihood} (by sampling a model from posterior distribution and performing rollouts) has strong guarantees when exact planning can be done, might not be optimistic enough for efficient exploration. With limited number of rollouts, agent might not observe the optimistic part of the model, in contrast to optimistic MCTS where optimism is built into every node of the tree.%\rknote{BACMP is more specific that TS, I put the discussion about BAMCP after experiments with more details}  


% policy search with simulated models -- unclear how to leverage model uncertainty to encourage deep exploration
% Thompson sampling (sample a model and run MCTS). Problem is that proofs that TS work normally rely on solving exactly
%     and that some parts of the world may be optimistically estimated. With limited rollouts, may not reach those 
%     optimistic estimates
% BAMCP: samples many models. approximating POMDP but larger branching factors

To investigate the impact of approximate planning on deep exploration, we compare optimistic MCTS, with Thomson Sampling, and BAMCP~\citep{guez2012efficient} (a tractable sample-based method for
approximate Bayes-optimal planning) in two toy domains, \textit{Pong Prime} and \textit{mini-Pitfall!}.

\begin{table}[tb]
  \begin{center}
    \begin{tabular}{r|c|c|c}
      \toprule 
      \textbf{Method} & Optimism & Thompson Sampling & BAMCP\\
      \midrule
      \textbf{Number of Episdoes} & 5.0 $\pm$ 1.7 & 6.2 $\pm$ 1.95 & 11.0 $\pm$ 2.5\\       
      \bottomrule % <-- Midrule here
      \end{tabular}
      
  \end{center}
  \caption{Comparison of different exploration methods in \textit{mini-Pitfall!} to consistently achieve the reward at the right end of the first room.}
  \label{tab:mini_pitfall}
\end{table}

\begin{figure}[tb]
    \centering
    \subfloat[Pong Prime environment]{\includegraphics[width=.31\linewidth]{img/PongP.pdf}}%
    % \enskip
    \subfloat[Different exploration methods]{\includegraphics[width=.33\linewidth]{img/pongp_comparison.pdf}}%
    % \enskip
    \subfloat[Optimism based exploration]{\includegraphics[width=.33\linewidth]{img/pongp_optimism.pdf}}%
    \caption{Pong Prime}%
    \label{fig:pongp}%
\end{figure}

\subsection{Experiments}

% \eb{Discuss why we use these two to investigate the impact of imperfect model-based planning for deep exploration. E.g. wanted to choose small scale environments where exploration is needed, similar in spirit to general atari domains. }

In order to test our hypotheses on the impact of imperfect model-based planning on deep exploration we introduced two toy environments, \textit{PongPrime} and \textit{mini-Pitfall!} that are similar to general Atari games but in smaller scale. \textit{Pong Prime} environment is designed for a hard exploration task. Dynamics of this game is similar to Atari2600's Pong environment~\citep{bellemare2013arcade} with minor tweaks that make the game significantly harder. The enemy paddle is made 3 times larger than the player paddle.
%so it is impossible to score a point by simply hitting the ball back at the normal speed.
Additionally, the top and bottom 10\% percent of the enemy paddle hit the ball back at 1.5 times the normal speed.
%so that the enemy paddle is even more powerful. 
Similarly, the player paddle also consists of 3 regions with distinct behavior. The \emph{top region} of the paddle hits the ball back at 1.5 times speed. The \emph{middle region} hits the ball back at normal speed. Finally, the \emph{lower region} covers 5\% of the paddle and instantly wins a point for the player. This configuration is set up so that it is difficult but not impossible for the player to score using the top region (scoring on average around 5\% of the time the ball bounces off the top region). In this setting, the optimal policy is to always hit the ball with the lower region of the ball. The game is deterministic and model free methods with $\epsilon$-greedy exploration (e.g. DQN) consistently loses the game with lowest possible score across 5000 episodes. Figure\ref{fig:pongp}(a) shows this environment. 

\textit{mini-Pitfall!} is a small version of Atari2600 game \textit{Pitfall!} which we use as a final test bed of our algorithm. In this version we limit the agent to two rooms of the game (the initial room, room 0, that agents starts the game in, and the room on the left side, room -1). There exist a dummy reward $R_{max}$ at the right end of the room 0, and the left end of room -1 is a terminal state (underground connection is also terminal state). Figure \ref{fig:meta_action}(a) shows this environment. 

We provided the right model class for both experiments so we can separate the effect of exploration from model mismatch. Figure \ref{fig:pongp}(b) compares the performance of different exploration strategies to the baseline (which uses a MLE model with UCT algorithm) combining with UCT algorithm in \textit{Pong Prime} domain. We perform 500 total tree searches for all runs in Figure \ref{fig:pongp}(b). Additionally, Table \ref{tab:mini_pitfall} shows the performance of different exploration methods in achieving the reward on the right side of the room 0 in \textit{mini-Pitfall!} consistently. 


\subsection{Discussion}\label{sec:exp_disc}

Both BAMCP and TS perform worse than the MLE model. In the limit of infinite simulations, BAMCP is guaranteed to converge to the Bayes optimal solution~\citep{guez2012efficient}. Similarly, with full horizon planning, we should be able to compute the exact value for the model sampled with TS, and there are strong guarantees that such a method will converge to the optimal policy~\citep{osband2016posterior}. 
%However in practice, especially in large domains or domains with real-time constraints, the amount of computation, and therefore the quality of the computed plan, will be significantly limited. 
However, if it is infeasible to use a depth that mimics the game horizon, or perhaps even to reach a local reward, then TS may suffer. This is because TS samples a single model, which means that parts of the model may be overly optimistic, while other parts may be pessimistic. Hence, when performing a limited number of simulations using UCT, we may not go down branches of the tree that \textit{observe} the optimistic parts of the sampled model. Therefore, the computed estimates of the Q value at the root node may not be optimistic, which is often a key part of proofs of the effectiveness of TS methods, and very helpful empirically. %\rknote{This paragraph has a lot of overlap with hypothesis we made}

Additionally, BAMCP suffers more in these environments that are deterministic. This means that for TS, optimism, and MLE approaches, the tree constructed will only have one child node (the deterministic next state) for any chosen action. In contrast, BAMCP samples a different deterministic model at each rollout, and for the same action node, those models may each deterministically predict different next states. Hence, BAMCP with $M$ sampled models and planning horizon $H$, potentially builds a tree of size O($(|A|M)^H$), in contrast to the other methods that build a tree of at most size O($|A|^H$), where $|A|$ is the size of action space.

Optimism-based exploration significantly outperforms other approaches. Optimism is built into \textit{every} node of the three that is allowing it to distinguish even locally between actions that may need exploration, in absence of observing long delayed reward. As we demonstrate in Figure \ref{fig:pongp}(c) for the optimistic method, as planning power increases through more simulations (number of rollouts), the performance of optimism-based exploration also increases. With sufficient computational power, optimistic MCTS should learn the optimal policy for \textit{Pong Prime} domain. 



