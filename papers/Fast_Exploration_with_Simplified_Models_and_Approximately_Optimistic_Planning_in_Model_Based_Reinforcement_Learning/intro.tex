\section{Introduction}
The coupling of deep neural networks and reinforcement learning has led to exciting advances, enabling reinforcement learning agents that can reach human-level performance in many Atari2600 games \citep{mnih2015human, mnih2016asynchronous, silver2017mastering, hessel2017rainbow}.
%\danote{Consider adding maybe and AlphaGo or Rainbow citation so people don't think you stopped looking in 2016}.
However, such agents typically require hundreds of millions of time steps to learn to play well. This is in sharp contrast to people, who typically learn to play Atari games within a few episodes~\citep{lake2017building}. Prior work on human learning for Atari suggests that people may be systematically building models of the reward and dynamics in the domain and using those to plan efficiently \citep{tsividis2017human, dubey2018investigating}. 


Given that human learners seemingly perform model-based RL very quickly, we are motivated to consider alternative approaches to current model-based RL, which often involves building complex predictive deep neural networks from scratch. Deep neural networks can require a large amount of data to accurately train, and performing perfect planning with those models is computationally expensive. Indeed, while there is a considerable amount of theoretical work done on tabular model-based reinforcement learning that suggests model-based approaches can be provably sample efficient~\citep{dann2017unifying, brafman2002r, strehl2008analysis}, there is much less success in using model-based reinforcement learning in extremely large environments, in part due to the challenges of learning simple accurate models in these domains. 

Instead, in this paper, we investigate two issues in leveraging model-based RL to speed learning in large domains. First we explore how to perform approximate planning using models during model-based RL to support  exploration, assuming models are given.
%\danote{There's a disconnect between here and the points raised in the previous paragraphs}. 
Models can facilitate deep exploration, since one key benefit of using models is that it is often easier to quantify uncertainty in those models, and perform planning in a way to guide the agent towards exploring and reducing that uncertainty. Recent work has suggested the benefit of Thompson Sampling methods over optimism methods for reinforcement learning~\citep{osband2016posterior}, and, indeed, empirically Thompson Sampling methods have done very well in contextual bandits and small state space MDPs where it is possible to plan exactly. However, to our knowledge, there has not been an investigation of how these popular approaches scale to larger domains where it is computationally prohibitive to perform exact planning. We introduce optimistic MCTS, a variant of the popular planning technique Monte Carlo Tree Search, %\danote{I wouldn't diminish your contribution by calling it small; maybe slight or just get rid of the modifier.}
and find that optimism here outperforms other approaches in several simulation experiments when planning can only be done approximately.

%\danote{I would switch the order of this and the previous paragraph. The paragraph before the previous one talked about difficulties in learning good models; connect your solution to that problem first (this paragraph) and then talking about how that solution also gives way to improved exploration.} 
Second, while our first investigation explores how to plan to encourage exploration given a model, we further investigate how to choose the model class to support computational tractability and learning efficiency. Here we propose to learn object-oriented deterministic models of the domain. People may leverage and test models of object interactions during video game learning~\citep{tsividis2017human, dubey2018investigating}, and object-oriented learning has the nice benefit that data from all similar objects can be pooled when building a model. Prior work has shown that object-oriented model-based reinforcement learning can yield provably efficient learning and scale to larger domains~\citep{diuk2008object}. Deterministic models offer an additional benefit over object-oriented models--they require less data to train (since one does not have to model a large stochastic distribution of outcomes) and they have additional benefits for planning, reducing the branching factor of next states to 1. Indeed past work~\citep{diuk2008object} also proposed learning deterministic object-oriented models. In contrast, we investigate how to learn the transition model and temporal abstraction to make the world appear deterministic. In other words, we assume a candidate set of possible transition model classes and temporal abstractions, and perform model selection to select a level of temporal abstraction and model that deterministically predicts the outcome of an action. %\danote{After reading, 'make the world appear deterministic' still hasn't landed...clarify?}

We illustrate the potential benefit of these ideas by introducing an object oriented algorithm that uses prior knowledge of model classes and object representation, and show that our algorithm can learn to achieve positive reward in the notoriously difficult Atari game \textit{Pitfall!} within 50 episodes. Almost no RL methods have achieved positive reward on \textit{Pitfall!} without human demonstrations, and even with demonstrations, such approaches often take hundreds of millions of frames to learn~\citep{aytar2018playing, hester2017deep}. In contrast to demonstrations, we assume two forms of prior knowledge--a predefined object representation and a class of potential model features. Computer vision is rapidly advancing to the state that soon we will be able to easily extract objects from even artificial scenes like the Arcade Learning Environment.
%\danote{This sounds a bit defensive for this early in the paper. Maybe gently rephrase or consider removing the 'we argue that' and 'that it is quite reasonable'}.
The second assumption of candidate model classes is a stronger assumption, but a large set of models could be defined directly given the object classes and only incur a cost quadratic in the set of features. %\rknote{Why this is true? :D}

While encouraging, such results should be mostly viewed as a case study. We believe the key contributions of our paper are not this particular demonstration on \textit{Pitfall!}, but rather the investigation of exploration approaches when planning can only be done approximately, and the benefit of selecting among model representations to support computationally tractable planning and fast learning. The second can be viewed as a bias/variance tradeoff and in future work we plan to consider how to identify and account for model biases that could limit asymptotic performance during the model-based planning procedure.

% \iffalse{

% Indeed there is good evidence that people's dynamics models may not always be perfect (see e.g.~\cite{caramazza1981naive})

% % object 

% People also seem to benefit substantially from using a higher-level object  representation. When playing Atari games, humans can use their prior knowledge to detect objects, learn their interaction behaviors and effectively plan in significantly lower dimensional state space than pixel space. Additionally, humans are capable of performing strategic exploration in order to reduce their uncertainty over unknown states.

% An important question is whether we can define algorithms that can similarly leverage such strategies to enable vastly more efficient reinforcement learning. We hypothesize that the intersection of three features may be sufficient to enable such success: (a) leveraging abstract object-level representations, (b) learning (often inaccurate) models of the world dynamics that can be learned quickly and support fast planning, and (c) strategic model-based exploration using lookahead planning.

% Strategic exploration methods have been studied in great detail in the tabular setting with recent results yielding near-tight probably approximately correct~\citep{dann2017unifying} and tight regret bounds~\citep{azar2017minimax}. Extensions of tabular optimistic bonuses (e.g.~\cite{strehl2008analysis}) to deep model-free RL methods~\citep{bellemare2016unifying} show substantially improved performance in many environments over $\epsilon$-greedy exploration. However, these methods still require millions of frames and struggle in domains that involve sparse delayed reward. 

% A challenge with many of these methods is propagating the optimistic reward bonuses to encourage exploration, a challenge that should be addressed by performing lookahead planning using model-based RL, such as UCT algorithm~\citep{kocsis2006bandit}. Unfortunately, due to the difficulty in learning accurate models in complicated domains that cause compunding errors~\citep{talvitie2015agnostic}, model-based RL has not matched the impressive performance observed by its model-free counterparts.


% While prior attempts have tried using object-level representations to provide key inductive bias to accelerate learning, they do not couple their efforts with strategic exploration \cite{mohan2011object, foley2017model, roderick2018deep, cobo2013object, kansky2017schema}. Our work is inspired by an important exception to this, the DOORMAX algorithm~\citep{diuk2008object}, which performs strategic R-max~\citep{brafman2002r} like exploration to learn logic-like dynamics models. This work assumes that planning can be done exactly. However, even when using object-level representations, in long horizon, sparse reward domains it will still typically be intractable to perform perfect lookahead planning.

% In this paper we also use an object-oriented representation for RL~\citep{diuk2008object} and make three key contributions. 
% \begin{enumerate}
% \item 
% Given it will generally be impossible to perform exact lookahead planning, we investigate which model-based strategic exploration strategies perform best when combined with the popular approximate Upper Confidence Tree  planning algorithm~\citep{kocsis2006bandit}. Our results highlight that optimistic strategies can be substantially better than Thompson Sampling when using MCTS as the planner.

% \item Leveraging these results, we introduce a new object-oriented, model-based, optimistic strategic RL algorithm. Our algorithm learns simple action macros (of the form ``act and then wait'' identical to those defined in prior work \cite{diuk2008object}) that may mimic human performance due to reaction time. It also leverages an inductive prior that there should exist simple deterministic models of the world dynamics.

% \item We evaluate our approach on \emph{Pitfall!}, one of the hardest Atari2600 game with extremely sparse positive reward. To our knowledge, our approach is the first method to achieve positive reward on this game without human demonstrations.
% \end{enumerate} 

% While there remains work to be done to make our algorithm more robust to model mismatch, the results demonstrate the effectiveness of model-based RL with lookahead planning for sample efficient RL. 
% }