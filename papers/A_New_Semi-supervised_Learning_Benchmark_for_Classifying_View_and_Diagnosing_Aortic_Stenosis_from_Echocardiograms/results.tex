To evaluate whether state-of-the-art SSL can improve real-world echocardiogram classification, we now compare several SSL methods against a baseline network of the same architecture that learns from only the labeled training set.
We compare Pseudo-label \citep{leePseudolabelSimpleEfficient2013}, virtual adversarial training~\citep{miyatoVirtualAdversarialTraining2019}, and MixMatch \citep{berthelotMixmatchHolisticApproach2019}. 

We first investigate image-level view classification in Sec.~\ref{sec:results-view}, then image-level diagnosis classification in ~Sec.~\ref{sec:results-diagnosis-from-images}.
Finally, in Sec.~\ref{sec:results-diagnosis-from-patient} we investigate how well our methods perform at patient-level diagnosis, using the image aggregation strategies from Sec.~\ref{sec:methods-patient-level}.

\paragraph{Performance metric.} For all view and diagnosis tasks, we use \emph{balanced accuracy} as our primary performance metric.
Given a dataset of $N$ true labels $y_{1:N}$ and $N$ predicted labels $\hat{y}_{1:N}$, with each label $y_n \in \{1, \ldots C\}$, we compute balanced accuracy as
\begin{align}
\text{balanced-accuracy}(y_{1:N}, \hat{y}_{1:N}) &= \frac{1}{C} \sum_{c=1}^{C} \frac{\text{TP}_{c}(y_{1:N}, \hat{y}_{1:N})}{N_{c}(y_{1:N})}.
\label{eq:balanced_accuracy}
\end{align}
Let $\text{TP}_c(\cdot)$ count \emph{true positives} for class $c$ (that is, the number of correctly classified examples whose true label is $c$), and $N_c(\cdot)$ gives the total number of examples with true label $c$.

We select balanced accuracy because standard accuracy does not adequately reflect performance on tasks with \emph{label imbalance}. In our view classification test set, the ``Other'' category is far more common, representing over 80\% of all images. Trivally guessing ``Other'' for every image would thus reach over 80\% accuracy, but only 33.3\% balanced accuracy.

\subsection{View classification}
\label{sec:results-view}

We first compare all selected methods on the small \datasetName-18-18 dataset in Table~\ref{tab:view classification small}.
All SSL methods provide gains over methods that only use the labeled set. The largest gains come from MixMatch, which improves the baseline by over 9\% in balanced accuracy.

%We observed that SSL algorithm can help us get substaintially better performance for the view classification task in this very limited data regime. Among the algorithm we compared, MixMatch performed the best . 

Next, we study the best performing methods on the larger \datasetName-156-52 dataset in Table~\ref{tab:view classification large}. 
Again, we see visible gains from MixMatch over the labeled-set-only baseline, improving over 2.5\% in balanced accuracy. The relative gain of SSL is smaller here because the amount of labeled training data is larger (eventually, with enough labeled data the performance of all methods should saturate).
Since Pseudo-Label and VAT perform worse than MixMatch in our experiments on the smaller TMED-18-18 dataset, we did not assess these methods on the larger dataset to keep computation costs low.

We further observe that our simpler variant of MixMatch that only uses unlabeled data for augmentation, which we called Augment-Only MixMatch, captures most of the gains of MixMatch (around \textbf{74$\%$} for both datasets), suggesting that augmentation (rather than a well-designed unlabeled loss) is the primary reason for MixMatch's success.

%We then apply the best SSL method from the smaller dataset (MixMatch) on larger EchoForAS-156-52 dataset \ref{tab:view classification large}. We found that there is still non-negligible gains from using unlabeled data. However, as expected, marginal benefit of using unlabeled data decreases as we have more labeled data. 

\begin{table}[!h]
    \centering
    \begin{tabular}{r r l|ccc|c}
    \multicolumn{2}{c}{Number of \emph{Unlabeled}}
    	& 
    	& \multicolumn{3}{c}{Split-specific Test Set}
    \\
    Patients & Images & Method & 0  & 1 & 2 & average\\
    \hline
    0 & 0 & Basic WRN & 81.37 & 81.84 & 82.70 & 81.97\\
    380 & $\sim$41,000  & Pseudo-Label & 81.81 & 85.07 & 85.82 & 84.23\\
    380 &  $\sim$41,000  & VAT & 87.95 & 87.61 & 86.36 & 87.31\\
    380 &  $\sim$41,000  & Augment-Only MixMatch  & 87.77 & 92.43 & 86.06 & 88.75\\
    380 &  $\sim$41,000 & MixMatch & 92.11 & 93.07 & 88.15 & \textbf{91.11}
    \end{tabular}
    \caption{View classification on our \textbf{smaller \datasetName-18-18} dataset, showing test-set balanced accuracy across 3 separate train/test splits each with 18 labeled patients for training and 18 for test.
    }%endcaption 
    \label{tab:view classification small}
%\end{table}
%\begin{table}[!h]
%    \centering
\bigskip

    \begin{tabular}{r r l|cccc|c}
    \multicolumn{2}{c}{Number of \emph{Unlabeled}}
    	& 
    	& \multicolumn{4}{c}{Split-specific Test Set}
    \\
    Patients & Images & Method & 0  & 1 & 2 & 3 & average\\
    \hline
	0 & 0 & Basic WRN & 92.37 & 93.24 & 93.72 & 93.87 & 93.30\\
    2645 & 303726 & Augment-Only MixMatch & 95.50 & 95.62 & 95.00 & 94.66 & 95.20\\
    2645 & 303726  & MixMatch & 96.22 & 95.79 & 95.77 & 95.74 & \textbf{95.88}
    \end{tabular}
    \caption{View classification on our \textbf{full-size \datasetName-156-52} dataset, showing test-set balanced accuracy across 4 train/test splits, each with 156 labeled patients for training and 52 for test.
        For simplicity, we focus on the best SSL method (MixMatch) identified in the smaller data comparisons.
    }%endcaption
    \label{tab:view classification large}
\end{table}



\subsection{Diagnosis classification from a single image}
\label{sec:results-diagnosis-from-images}

To examine how SSL might improve AS diagnosis when given a single image, we first compare all candidate methods on the smaller dataset in Table~\ref{tab:diagnosis classification small}.
Like in the view task, we see that MixMatch beats all other SSL methods and the baseline by a substantial margin.
Further experiments on the full-size dataset in Table~\ref{tab:diagnosis classification large} again show MixMatch improves accuracy.

We also assess the added-value of our proposed pretrain-on-view transfer learning methods (Sec.~\ref{sec:transfer_from_view_to_diagnosis}).
Both Table~\ref{tab:diagnosis classification small} and Table~\ref{tab:diagnosis classification large} suggest that this pretraining strategy offers gains over simply initializing the weights of our single-image diagnosis classifier from scratch (across splits we average +3.4\% on the smaller dataset and +0.5\% on the larger dataset).
Notably these gains are \emph{consistent} across splits: we see a gain from pretraining visible in each of the 4 train/test splits.

In addition to pretraining, multitask learning also seems effective. We can see an average balanced accuracy gain of +4.4\% on the smaller dataset and +2.2 \% on the larger dataset. This demonstrates the added benefit of utilizing both view and diagnosis labels (not just diagnosis alone) to improve generalization performance.

%and achieves the best image level diagnosis classification result on both EchoForAS-18-18 and EchoForAS-156-52 \ref{tab:diagnosis classification small} \ref{tab:diagnosis classification large}. We hypothesized that this is because the pretraining on view provide better initialization to the diagnosis classification network.      

\begin{table}[!h]
    \centering
    \begin{tabular}{r r l l|ccc|c}
    \multicolumn{2}{c}{Number of \emph{Unlabeled}} &
    	& &
    	\multicolumn{3}{c}{Split-specific Test Set}
    \\
    Patients & Images & Pretrain & Method & 0  & 1 & 2 & average\\
    \hline
    0 & 0 & &  Basic WRN & 42.25 & 51.26 & 36.73 & 43.41\\
    0 & 0 & &  Multitask WRN, & 43.46 & 54.30 & 45.80 & 47.85\\
    0 & 0 & view & Basic WRN & 41.76 & 52.21 & 40.40 & 44.79\\
    380 & $\sim$41,000 & & Pseudo-Label & 41.16 & 49.93 & 42.94 & 44.68\\
    380 & $\sim$41,000 & & VAT & 42.05 & 51.13 & 42.79 & 45.32\\
    380 & $\sim$41,000 & & MixMatch & 43.89 & 55.17 & 44.76 & 47.94\\
    380 & $\sim$41,000 & view & MixMatch & 50.01 & 56.46 & 47.73 &\textbf{ 51.43} 
    \end{tabular}
    \caption{AS severity diagnosis classification for \emph{individual images} on the \textbf{smaller \datasetName-18-18} dataset, showing balanced accuracy averaged over the test sets from multiple train/test splits of the labeled set.
     Each split's test set contains all images from 18 patients, and each split's labeled training set contains all images from 18 patients.
    The labeled-only Basic WRN transfers from corresponding Basic WRN view classifier, while Mixmatch transfers from corresponding MixMatch view classifier}
    \label{tab:diagnosis classification small}
%\end{table}
%\begin{table}[t!]
%    \centering
\bigskip %% MCH edits to put all tables for same task on same page
    \begin{tabular}{r r l l| rrrr | r}
    \multicolumn{2}{c}{Number of \emph{Unlabeled}}
        & & & \multicolumn{4}{c}{Split-specific Test Set}
    \\
    Patients & Images 
    & Pretrain & Method & 0 & 1 & 2 & 3 & average\\
    \hline
    0 & 0 & & Basic WRN & 62.95 & 61.03 & 56.58 & 63.84 & 61.13\\
    0 & 0 & & Multitask WRN & 66.16 & 62.41 & 58.70 & 66.98 & 63.31\\
    2645 & 303726 & & MixMatch & 65.57 & 62.69 & 60.87 & 66.29 & 63.86\\
    2645 &  303726& view & MixMatch & 67.39 & 62.79 & 61.02 & 67.36 & \textbf{64.64}\\ 
    \end{tabular}
    \caption{AS severity diagnosis classification for \emph{individual images} on the \textbf{full-size \datasetName-156-52} dataset, showing balanced accuracy across multiple splits. Each split's test set contains all images from 52 patients, and each split's labeled training set contains  all images from 152 patients.
    }%endcaption
    \label{tab:diagnosis classification large}
\end{table}

\subsection{Diagnosis performance using all images related to a patient}
\label{sec:results-diagnosis-from-patient}

We finally consider how SSL methods might improve AS diagnosis when making predictions for a patient, aggregating information from many images with diverse view types, using the methods from Sec.~\ref{sec:methods-patient-level}.
The results on the full-size dataset in Table~\ref{tab:diagnosis classification large patient} suggest that SSL with MixMatch, when combined with our other key innovations (prioritizing relevant views, pretraining on view) offers real value, achieving 90.1\% balanced accuracy compared to the baseline's 81.57\%. Ablations in Table~\ref{tab:diagnosis classification large patient} help further disentangle how each piece (adding semi-supervised learning, adding prioritization, adding pretraining) help. The results suggest that prioritization of relevant views offers the largest and most consistent gains, followed by the semi-supervised learning, and then pretraining on view classification.

To further understand the source of these gains, we examine \emph{confusion matrices} in Fig \ref{fig:confusion_matrix} across 4 independent train/test splits of our full-size TMED-156-52 dataset.
This figure compares side-by-side our best classifier (pretrained MixMatch that prioritizes relevant views) and a labeled-set-only baseline using a simple average aggregation strategy.
We see that our proposed method consistently makes fewer mistakes across all splits: 3 fewer mis-diagnosed patients on split 1, 3 fewer on split 2, 4 fewer on split 3, and 6 fewer on split 4.  
For every severity level and split, the proposed method achieves equal or better recall than the baseline.

Our dataset is limited: even the full-size dataset has only 52 patients in the test set to evaluate results. 
Therefore, to better assess the significance of our claims (that SSL learning with MixMatch delivers improved performance, which is further boosted by smart prioritization of relevant views), we revisit the portion of our large dataset that had only diagnosis labels (and no view labels) for 174 patients. For this experiment, we call this the ``bonus heldout set''.
Results comparing all methods on this bonus heldout set are in Supplementary Table~\ref{tab:diagnosis classification patient unlabeled_heldout_174}.
We find our claims are consistent: among methods that use simple averaging, MixMatch improves over the Basic WRN baseline by over 1\% balanced accuracy, while when using prioritized views MixMatch improves by over 3.5\%.
We do note that these ``bonus set'' images were included in the \emph{unlabeled} training set. However, we emphasize that their labels were \emph{not used} during training and that they make up less than 6\% of the total unlabeled set.

\subsection{ROC analysis of No AS vs Some AS}
Finally, we assess our method's ability as a first-line screening tool by reporting a receiver operating curve in Supplementary Fig. \ref{fig: No AS vs Some AS}. for the simpler binary task of distinguishing between ``no AS'' and ``some AS'' (combining the ``mild/moderate AS'' and ``severe AS'' labels).
We compared a labeled-set-only model with simple averaging, a labeled-set-only model with prioritized voting, and our SSL-trained MixMatch method with prioritized voting.
While all compared methods achieve high area-under-the-ROC scores, we find that prioritized voting consistently shows gains, achieving a remarkable average AUC of \textbf{0.98} across the 4 splits.
This performance suggests that we may not be far from effective deployment of these models as a first-line screening tool, provided we can replicate this performance in future external validation.

\begin{table}[!t]
    \centering
    \begin{tabular}{l l l|rrrr|c}
	         &        & & \multicolumn{4}{c}{Split-specific Test Set} \\
    Pretrain & Method & Aggregation across images
    & 0  & 1 & 2 & 3 & average\\
    \hline
    & Basic WRN & Simple average & 87.92 & 81.67 & 72.92 & 83.75 & 81.57\\
    & Basic WRN & Prioritize relevant view & 90.83 & 80.00 & 83.33 & 92.50 & 86.66\\
% MCH says: I took out the "ablation" rows because I think they can be answered in text. We want a simplified table that is easy to get the major idea from
%    & Basic WRN & Suggested Ablation & 85.42 & 86.25 & 79.17 & 92.50 & 85.84 \\
    \hline
    & MixMatch & Simple average & 90.00 & 77.08 & 80.83 & 94.17 & 85.52\\
    & MixMatch & Prioritize relevant view & 88.75 & 83.75 & 85.00 & 94.17 & 87.92\\
%    & MixMatch & Suggested Ablation & 83.33 & 84.17 & 77.50 & 94.58 & 84.90 \\
    view & MixMatch & Simple average & 87.50 & 77.08 & 76.67 & 85.83 & 81.77\\
    view & MixMatch & Prioritize relevant view  & 93.75 & 87.50 & 82.92 & 96.25 & \textbf{90.11}\\
 %   view & MixMatch & Suggested Ablation & 86.67 & 80.00 & 82.50 & 94.17 & 85.84\\
    \end{tabular}
    \caption{Patient-level AS severity diagnosis classification on the \textbf{full-size \datasetName-156-52 dataset}, showing balanced accuracy on the test set across multiple train/test splits. Each split's labeled training set contains 156 patients, and each test set contains 52 patients.
    MixMatch methods are trained with \emph{semi-supervised} learning on both labeled and unlabeled data. 
    }%endcaption
    \label{tab:diagnosis classification large patient}
\end{table}

% \begin{table}[!t]
%     \centering
%     \begin{tabular}{l l l|c}
%     Pretrain & Method & Aggregation across images
%     &  average\\
%     \hline
%     & Basic WRN & Simple average & 81.66\\
%     & Basic WRN & Prioritize relevant view & 86.66\\
%     %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
%     \hline
%     & MixMatch & Simple average & 85.52\\
%     & MixMatch & Prioritize relevant view & 87.92\\
%     view & MixMatch & Simple average & 81.77\\
%     view & MixMatch & Prioritize relevant view  & \textbf{90.11}\\
%     %view & MixMatch & LR with view-priority & 87.08 & 82.08 & 85.00 & 88.75 & 85.73\\
%     %(MixMatch transfered) + MysteryMethod & NA & NA & NA\\ 
%     \end{tabular}
%     \caption{Patient-level AS severity diagnosis classification on the \textbf{full-size EchoForAS-156-52 dataset}, showing balanced accuracy on the test set across multiple folds (each fold's test set contains 52 patients).
%     All methods trained on labeled images from 156 patients. MixMatch methods are trained with \emph{semi-supervised} learning on both labeled and unlabeled data. 
%     }%endcaption
%     \label{tab:diagnosis classification large patient}
% \end{table}

% \begin{table}[!t]
%     \centering
%     \begin{tabular}{l |c}
%   Aggregation across images
%     &  average\\
%     \hline
%     % & Basic WRN & Simple average & 81.66\\
%     % & Basic WRN & Prioritize relevant view & 86.66\\
%     % %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
%     % \hline
%     % & MixMatch & Simple average & 85.52\\
%     % & MixMatch & Prioritize relevant view & 87.92\\
%     Simple average & 81.77\\
%     Prioritize relevant view  & \textbf{90.11}\\
%     %view & MixMatch & LR with view-priority & 87.08 & 82.08 & 85.00 & 88.75 & 85.73\\
%     %(MixMatch transfered) + MysteryMethod & NA & NA & NA\\ 
%     \end{tabular}
%     \caption{Patient-level AS severity diagnosis classification on the \textbf{full-size EchoForAS-156-52 dataset}, showing balanced accuracy on the test set across multiple folds (each fold's test set contains 52 patients).
%     All methods trained on labeled images from 156 patients. MixMatch methods are trained with \emph{semi-supervised} learning on both labeled and unlabeled data. 
%     }%endcaption
%     \label{tab:diagnosis classification large patient}
% \end{table}

% \begin{table}[ht]
%     \centering
%     \caption{EchoForAS-260 patient diagnosis}
%     \label{tab:diagnosis classification small}
%     \begin{tabular}{c|cccc|c}
%     & fold0  & fold1 & fold2 & fold3 & average\\
%     \hline \hline
%     (FS) Simple majority vote & 85.83 & 79.58 & 75 & 85.83 & 81.56\\
%     (FS) Vote that prioritizes relevant views & 88.75 & 86.25 & 77.92 & 96.25 & 87.29\\
%     (FS) + MysteryMethod3 & NA & NA & NA\\ 
%     (MixMatch transfered + Simple majority vote & 90.00 & 79.17 & 84.58 & 92.5 & 86.56\\
%     (MixMatch transfered + Vote that prioritizes relevant views & 90.83 & 82.08 & 81.25 & 94.58 & 87.19\\
%     (FS) + MysteryMethod3 & NA & NA & NA\\ 
%     \end{tabular}
% \end{table}



%%%%%%%%%%%%%%% FPR Table

% \begin{table}[ht]
%     \centering
%     \label{tab: FPR@TPR=1}
%     \begin{tabular}{l l|rrrr | r}
%     Method & Voting 
%     & fold0  & fold1 & fold2 & fold3 & Average \\
%     \hline
%     Basic WRN & Simple average &  &  &  & & \\
%     Basic WRN & View-prioritized &  &  &  &  & \\
%     %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
%     MixMatch & View-prioritized &  &  &  &  &  \\
%     % view & MixMatch & View-prioritized & 0.125 & 0.4375 & 0.3125 & 0.125 & 0.25 \\
%     \end{tabular}
%     \caption{Patient Diagnosis, FPR at TPR=1}
% \end{table}

%% Version: from Thu 3/18
%\begin{table}[ht]
%    \centering
%    \label{tab: FPR@TPR=1}
%    \begin{tabular}{l l|rrrr | r}
%    Method & Voting 
%    & fold0  & fold1 & fold2 & fold3 & Average \\
%    \hline
%    Basic WRN & Simple average & 0.25 & 0.5 & 0.3125 & 0.5625 & 0.4063\\
%    Basic WRN & View-prioritized & 0.125 & 0.3125 & 0.625 & 0.1875  & 0.3125\\
%    %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
%    MixMatch & View-prioritized & 0.125 & 0.25 & 0.125 & 0.125 & \textbf{0.1563} \\
%    % view & MixMatch & View-prioritized & 0.125 & 0.4375 & 0.3125 & 0.125 & 0.25 \\
%    \end{tabular}
%    \caption{Patient Diagnosis, FPR at TPR=1}
%\end{table}

%%%%%%%%%%%%%% PPV Table

%% Version: from Thu 3/18
%\begin{table}[ht]
%    \centering
%    \label{tab: PPV@TPR=1}
%    \begin{tabular}{l l|rrrr | r}
%    Method & Voting 
%    & fold0  & fold1 & fold2 & fold3 & Average \\
%    \hline
%    Basic WRN & Simple average &  &  &  &  & \\
%    Basic WRN & View-prioritized &  &  &  &   & \\
%    %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
%    MixMatch & View-prioritized &  &  &  &  &  \\
%    % view & MixMatch & View-prioritized &  &  &  &  &  \\
%    \end{tabular}
%    \caption{Patient Diagnosis, Precision at TPR=1}
%\end{table}

%%%%%%%%%%%%%% FPR Table
%% Version: from before Monday 3/15
% \begin{table}[ht]
%     \centering
%     \caption{Patient Diagnosis, FPR at TPR=1}
%     \label{tab:diagnosis classification small}
%     \begin{tabular}{l l l l| c}
%     Data & Diagnosis & View & Method
%      & FPR@TPR=1\\
%     \hline
%     Labeled only & FS & No & All views  & 0.563\\
%     SSL & MixMatch & No & All views &  0.188\\
%     SSL & MixMatch & MixMatch & Priority views + confidence & 0.063\\
%     SSL & MixMatch & MixMatch & Priority views + LR &  \textbf{0}\\
%     %(MixMatch transfered) + MysteryMethod & NA & NA & NA\\ 
%     \end{tabular}
% \end{table}

