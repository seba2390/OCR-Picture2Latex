\newpage
\section{Dataset Visualizations}
\label{sec:app_dataset_visuals}

%%%%%%
%%
%%
\subsection{Examples of each view class}
\newcommand{\BC}{0.33}
\setlength{\tabcolsep}{0.1cm}
\begin{figure}[!h]
\begin{tabular}{c c c c}
    PLAX  & PSAX & OTHER 
    \\
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PLAX1.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PSAX1.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_Other1.jpg}
    &
   
    \\
    
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PLAX2.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PSAX2.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_Other2.jpg}
    &
   
     \\
     
     \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PLAX3.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PSAX3.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_Other3.jpg}
    &
   
     \\
     
     \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PLAX4.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_PSAX4.jpg}
    &
    \includegraphics[width=\BC\textwidth]{figures/small_appendix/Appendix_Other4.jpg}
    &
   
    \end{tabular}	
    \caption{Examples of images for each possible view label in our dataset. \emph{From left to right:} Four examples of peristernal long axis (PLAX) view, four examples of peristernal short axis (PSAX) view, and four examples of other kinds of view in our ``Other'' class. }
    \label{fig:VIEW_SAMPLES_APPENDIX}
\end{figure}

%%%%%%
%%
%%
\newpage
\subsection{Examples of each view for a Severe AS patient}
\newcommand{\BA}{0.33}
\setlength{\tabcolsep}{0.1cm}
\begin{figure}[!h]
\begin{tabular}{c c c c}
    PLAX  & PSAX & OTHER 
    \\
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PLAX1.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PSAX1.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_Other1.jpg}
    &
    
    \\
    
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PLAX2.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PSAX2.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_Other2.jpg}
    &
   
     \\
     
     \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PLAX3.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_PSAX3.jpg}
    &
    \includegraphics[width=\BA\textwidth]{figures/small_appendix/SevereAS_11112007_Other3.jpg}
    &
  
    \end{tabular}	
    \caption{Examples of images from a patient with Severe AS in our dataset. \emph{From left to right:} Three examples of parasternal long axis (PLAX) view, three examples of parasternal short axis (PSAX) view, and three examples of other kinds of view in our ``Other'' class. }
    \label{fig:PatientSevereAS}
\end{figure}


%%%%%%
%%
%%
\newpage
\subsection{Examples of each view for a No AS patient}
\newcommand{\BB}{0.33}
\setlength{\tabcolsep}{0.1cm}
\begin{figure}[!h]
\begin{tabular}{c c c c}
    PLAX  & PSAX & OTHER 
    \\
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PLAX1.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PSAX1.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_Other1.jpg}
    &
    
    \\
    
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PLAX2.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PSAX2.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_Other2.jpg}
    &
   
     \\
     
     \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PLAX3.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_PSAX3.jpg}
    &
    \includegraphics[width=\BB\textwidth]{figures/small_appendix/NoAS_1996889_Other3.jpg}
    &
  
    \end{tabular}	
    \caption{Examples of images from a patient with No AS in our dataset. \emph{From left to right:} Three examples of parasternal long axis (PLAX) view, three examples of parasternal short axis (PSAX) view, and three examples of other kinds of view in our ``Other'' class. }
    \label{fig:PatientNoAS}
\end{figure}



\newpage 
\section{Further Results}

\subsection{Assessment of ensembling}

Table~\ref{tab:best_single_checkpoint_VS_ensemble_FS_echo260} compares using a single checkpoint (one point estimate of neural network weight vector $\theta$) to using an ensemble of parameters aggregated from the last 25 checkpoints (one per epoch).

\begin{table}[!h]
    \centering
    \begin{tabular}{c|cccc|c}
    \textit{Diagnosis classification} & Split 1  & Split 2 & Split 3 & Split 4 & Average\\
    \hline
    Best single checkpoint  & 61.81 & 59.79 & 56.05 & 64.21 & 60.46\\
    Ensemble  & 62.95 & 61.03 & 56.58 & 63.84 & \textbf{61.13}
	\\ \hline
    \textit{View classification}  &   &  &  &  & 
    \\ \hline
    Best single checkpoint  & 93.03 & 93.24 & 92.39 & 93.79 & 93.11\\
    Ensemble  & 92.37 & 93.24 & 93.72 & 93.87 & \textbf{93.30}\\
    \end{tabular}
    \caption{Comparing best single checkpoint performance with ensemble performance on \textbf{Full-size \datasetName-156-52}}
    \label{tab:best_single_checkpoint_VS_ensemble_FS_echo260}
\end{table}


%%%%%%
%%
%%
\subsection{Patient-level diagnosis performance on bonus heldout set}

Table~\ref{tab:diagnosis classification patient unlabeled_heldout_174} examines the performance of the best labeled-set-only methods and MixMatch methods on the 174 patient studies that have diagnosis but no view labels.
 While the images used here were originally included in the unlabeled training set (which was used to train SSL methods like MixMatch), the diagnosis labels were not provided at all during training time. 
 We thus still believe this is an authentic test of generalization given the scarcity of labeled data available for our task.
 Of course, additional independent evaluation (especially from another institution) is needed.

\begin{table}[!h]
    \centering
    \begin{tabular}{l l l|rrrr|c}
    Pretrain & Method & Voting
    & Split 1  & Split 2 & Split 3 & Split 4 & average\\
    \hline
    & Basic WRN & Simple average & 76.73 & 75.25 & 76.87 & 81.88 & 77.68\\
    & Basic WRN & View-prioritized & 73.63 & 83.21 & 79.70 & 80.08 & 79.18\\
    %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
    \hline
    & MixMatch & Simple average & 85.32 & 76.29 & 74.14 & 79.95 & 78.93\\
    view & MixMatch & Simple average & 83.36 & 77.96 & 75.61 & 81.37 & 79.58\\
    & MixMatch & View-prioritized & 83.27 & 83.76 & 82.34 & 82.83 & \textbf{83.05}\\
    view & MixMatch & View-prioritized & 82.53 & 86.15 & 79.62 & 83.27 & 82.89\\
    %view & MixMatch & LR with view-priority & 80.42 & 84.24 & 76.58 & 80.67 & 80.48\\
    %(MixMatch transfered) + MysteryMethod & NA & NA & NA\\ 
    \end{tabular}
    \caption{Patient-level AS Severity Diagnosis Classification on the \textbf{bonus heldout set} of 174 patients for whom we have diagnosis labels only (no view labels). We show balanced accuracy on models trained on each of the four folds on four \textbf{full-size \datasetName-156-52} dataset.
    }%endcaption
    \label{tab:diagnosis classification patient unlabeled_heldout_174}
\end{table}


%%%%%%
%%
%%
\subsection{Assessment of MixMatch hyperparameter sensitivity}

In Table~\ref{tab:MixMatch hyperparameters ablation study}, we consider four possible strategies for setting the hyperparameters of MixMatch, varying two  key settings for the weight on unlabeled loss $\lambda$. First, we vary whether the final value of $\lambda$ is set to its \emph{best} value among a grid of candidates (based on validation set performance), or \emph{fixed} to a constant.
Second, we vary whether $\lambda$ remains fixed over iterations throughout a training run, or is updated over iterations on a linear ramp schedule from 0 to its final target value. 

From this comparison, we see we consistent gains across splits (average gain across splits of over 1.6\% balanced accuracy) for using a delayed ramp up schedule with target value selected via grid search.

\begin{table}[!h]
    \centering
    \begin{tabular}{l l| rrrr | r}
    Final $\lambda$ value & $\lambda$ update schedule & Split 1  & Split 2 & Split 3 & Split 4 & Average\\
    \hline
    best on val & Delayed ramp-up  & 65.57 & 62.69 & 60.87 & 66.29 & 63.86\\
    best on val & Immediate ramp-up & 65.07 & 61.87 & 60.82 & 65.37 & 63.28\\
    best on val & Constant  & 65.03 & 61.52 & 58.87 & 65.22 & 62.66\\
    100 (fixed) & Constant & 63.94 & 61.79 & 58.87 & 64.35 & 62.24\\
    \end{tabular}
    \caption{Ablation study of different settings of the unlabeled loss weight $\lambda$ for MixMatch. AS severity diagnosis classification for individual images on the \textbf{full-size \datasetName-156-52} dataset. showing balanced accuracy averaged over the test sets from multiple folds (each fold’s test set contains all images from 52 patients). }%endcaption
    \label{tab:MixMatch hyperparameters ablation study}
\end{table}



%%%%%%
%%
%%
\subsection{Assessment of alternative view prioritization strategy using thresholding}


An anonymous reviewer suggested an alternative strategy for prioritizing images of relevant view.
The alternative strategy works as follows: for each image, we compute the predicted probability that the image is a ``relevant view'' (either PLAX and PSAX) by summing the probabilities of each view type.
However, instead of using this raw probability as a weight (as our chosen method does), we use a \emph{cutoff threshold} and simply average the diagnosis predictions of images whose relevant view probability is above the cutoff.
For each patient, we use the majority vote prediction of the diagnosis from the images of relevant views.
The value of the cutoff threshold is selected using the validation set to maximize balanced accuracy.

Table~\ref{tab:Suggested_Aggregation_Ablation} shows the performance of this strategy (``threshold-then-average'') on the full-size dataset.
Using this alternative prioritization strategy together with our suggested methodology for patient-level diagnosis (using MixMatch, pretraining on view), we find the average test set balanced accuracy is around 85.8\%, while the weighted average strategy in the main paper achieves over 90\% balanced accuracy. We take this as reasonably decisive evidence that a weighted average (rather than a simple cutoff) should be preferred.

\begin{table}[!h]
    \centering
    \begin{tabular}{l l l|rrrr|c}
    Pretrain & Method & Aggregation across images
    & Split 1  & Split 2 & Split 3 & Split 4 & average\\
    \hline
    & Basic WRN & Threshold-then-Average & 85.42 & 86.25 & 79.17 & 92.50 & 85.84 \\
    %SSL & FS & MixMatch & Priority view + confidence & 94.58 & 84.17 & 77.50 & 92.5 & 87.19\\
    & MixMatch & Threshold-then-Average & 83.33 & 84.17 & 77.50 & 94.58 & 84.90 \\
    view & MixMatch & Threshold-then-Averagen & 86.67 & 80.00 & 82.50 & 94.17 & 85.84\\
    %view & MixMatch & LR with view-priority & 87.08 & 82.08 & 85.00 & 88.75 & 85.73\\
    %(MixMatch transfered) + MysteryMethod & NA & NA & NA\\ 
    \end{tabular}
    \caption{Alternative view-prioritizing strategy for patient-level AS severity diagnosis classification on the \textbf{full-size \datasetName-156-52} dataset, showing balanced accuracy on the test set across multiple folds (each fold’s test set contains 52 patients).}
    %endcaption
    \label{tab:Suggested_Aggregation_Ablation}
\end{table}



%%%%%%
%%
%%
\subsection{ROC Curve of patient-level diagnosis: no AS vs. mild/moderate/severe AS}

Fig.~\ref{fig: No AS vs Some AS} shows receiver operating curves for several methods for the task of distinguishing no AS vs Some AS (which aggregates both the mild/moderate and severe levels in the 3-level diagnosis task of the main paper).

\begin{figure}[!h]
\begin{tabular}{c c}
	\includegraphics[width=0.43\textwidth]{figures/fold0_multitask_PatientLevel_NoVSSome_NormalizedPriorityStrategyClassProbabilityScore.pdf}
	&
    \includegraphics[width=0.43\textwidth]{figures/fold1_multitask_PatientLevel_NoVSSome_NormalizedPriorityStrategyClassProbabilityScore.pdf}
	\\
	(a) Split 1 & (b) Split 2
	\\
	\includegraphics[width=0.43\textwidth]{figures/fold2_multitask_PatientLevel_NoVSSome_NormalizedPriorityStrategyClassProbabilityScore.pdf}
	&
    \includegraphics[width=0.43\textwidth]{figures/fold3_multitask_PatientLevel_NoVSSome_NormalizedPriorityStrategyClassProbabilityScore.pdf}
	\\
	(c) Split 3 & (d) Split 4
\end{tabular}
    
\caption{ROC curves for binary diagnosis task (no AS vs ``mild/moderate/severe AS'') on \textbf{full-size \datasetName-156-52}.
    }%endcaption
    \label{fig: No AS vs Some AS}
\end{figure}

\section{Methodological Details}

\subsection{Image processing details}
\label{sec:removing_doppler}

\paragraph{Removing doppler images.}
In the raw data of all imagery available for an echocardiogram study, 
we obtained TIFF files that represent both cineloops and Doppler images.

We verified in our labeled set that all Doppler images have one of the following landscape aspect ratio $(831, 323)$, $(901, 384)$, $(901, 390)$, $(704, 305)$, $(831, 421)$, $(901, 469)$ or $(563, 294)$. Only the Dopplers have these aspect ratios. We thus filtered out Doppler completely via these aspect ratios. 

\paragraph{Downsizing}
The original images are provided as high-resolution TIFF format images (hundreds of pixels per side) of varying aspect ratios. Generally, we can expect that both view and diagnosis classifiers would perform better given higher-resolution input (and holding other factors the same). The main trade-off of processing higher-resolution images is increased runtime and memory requirements. In our preliminary experiments, we compared downsizing all images to a standard square aspect ratio at 3 possible sizes: 32x32, 64x64 and 128x128. We found that 64x64 achieves a good balance between model performance and computation cost. 
A prior study by \citet{madaniDeepEchocardiographyDataefficient2018} provides a more extensive study of optimal resolution size. The interested reader can refer to their work for more details. 


\subsection{Architecture Settings and Hyperparameters}
\label{sec:arch_and_hyperparameters}

\paragraph{Weighted cross-entropy for labeled loss}
To counteract the effect of class imbalance in the dataset, we use weighted cross-entropy for the labeled loss. For an input image $x$ whose true label $y$ indicates it belongs to class $c$, the weighted cross-entropy assumes the following form:
\begin{align}
\mathcal{L}^L(\theta, x) = - w_{c} \log \hat{p}_{c}(\theta, x),
\end{align}
where $\hat{p}_{c}$ is the predicted probability of class $c$. The weight $w_{c}$ is calculated using the training set statistics as follow:
\begin{align}
w_{c} = \frac{\prod_{k\neq c}{N_{k}}}{\sum_{j}\prod_{k \neq j}{N_{k}}}
\end{align}
where $N_{k}$ is the number of images of class $k$ in the training set.

\paragraph{Common architecture.}
Following~\citet{oliverRealisticEvaluationDeep2018}, for all considered methods, we use the \emph{same} backbone neural network architecture: a wide residual network~\citep{zagoruykoWideResidualNetworks2017} with 28 layers (WRN-28), which has total of 5,931,683 parameters.
This same network architecture is used in the original MixMatch evaluation~\citep{berthelotMixmatchHolisticApproach2019} with promising results.

\paragraph{Common training protocol.}
All SSL methods we consider follow the loss minimization framework with two primary losses (one for ``labeled'' data and one for ``unlabeled'' data) in Eq.~\eqref{eq:standard-SSL-loss-template}.
We allow every method to train for 32 epochs (where each epoch processes $2^{16}$ images, as in \citet{berthelotMixmatchHolisticApproach2019}).
Our preliminary experiments suggest that after 30 epochs all methods effectively converge in terms of validation balanced accuracy. 

\paragraph{Common regularization.}
For all methods, we expect performance will be vulnerable to overfitting, so we impose an L2-norm penalty on the weights $\theta$, also known as weight decay. Each method selects its preferred value of this penalty strength hyperparameter. We searched values in [0.0002, 0.002, 0.02].

\paragraph{Common optimization.}
We use ADAM \citep{kingma2014adam} to optimize each model.
Each method selects the value of the step size (learning rate) as a hyperparameter. We experimented with 0.002 and 0.0007
%HZ: 'performance being sensitive to learning rate' is very reasonable. But we don't have an ablation to back it. 
%We find performance is sensitive to the step size (learning rate) hyperparameter, so we perform a grid search and select the value that maximizes balanced accuracy on the validation set.

\paragraph{Hyperparameters for Pseudo-Label.}
Beyond the usual hyperparameters for our loss-minimization SSL framework, another important hyperparameter for pseudo-label is the threshold $\tau$. We find that performance is not very sensitive to the chosen $\tau$ value as long as it is within a certain range. We set $\tau$ to 0.95, as done in past literature that evaluates Pseudo-Label as an SSL method ~\citep{oliverRealisticEvaluationDeep2018,berthelotMixmatchHolisticApproach2019, berthelotRemixmatchSemisupervisedLearning2019, sohnFixmatchSimplifyingSemisupervised2020}.


\paragraph{Hyperparameters for VAT.}
Beyond the usual hyperparameters for our SSL framework, for VAT we need to select a value for $\epsilon$.
In \citet{miyatoVirtualAdversarialTraining2019}, the authors claimed that they can achieve superior performance by tuning only $\epsilon$ and fixing $\lambda$ to 1. In our experiment, we used the default $\lambda$ as in \cite{berthelotMixmatchHolisticApproach2019} and searched the value of $\epsilon$ in [2, 6, 18], together with learning rate and weight decay. We select the best hyperparameters using validation set performance. 


\paragraph{Hyperparameters for MixMatch.}
Beyond the usual hyperparameters for our SSL framework, the key hyperparameters for MixMatch include the number of augmentations $K$, the temperature $T>0$ used for sharpening, interpolation hyperparameter $\alpha$ and unlabeled loss coefficient $\lambda$. We set $K=2$, $T=0.5$, and $\alpha=0.75$ as done in \citet{berthelotMixmatchHolisticApproach2019}, and search for $\lambda$ in the range [10, 30, 75, 100, 130] using validation set. 

\paragraph{Hyperparameters for Multitask training.}
We searched $\gamma$, the hyperparameter that control the strength of the auxilliary view loss in Eq.~\eqref{eq:multitask}, in the range [10, 3, 1, 0.3, 0.1]. The best $\alpha$ is selected together with other hyperparameters on validation set. 
