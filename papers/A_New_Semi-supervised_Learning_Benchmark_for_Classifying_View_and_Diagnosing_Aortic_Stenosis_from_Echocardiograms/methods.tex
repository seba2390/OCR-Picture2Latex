

\subsection{Methods for SSL Image Classification}
\label{sec:Methods-SSL}

Inspired by \citet{oliverRealisticEvaluationDeep2018}, we wish to carefully evaluate semi-supervised learning methods for image classification, focusing on a simple SSL baseline (Pseudo-Label) as well as two recent high-performing methods: virtual adversarial training (VAT, \citep{miyatoVirtualAdversarialTraining2019}) and MixMatch~\citep{berthelotMixmatchHolisticApproach2019}. Below we review the key ideas behind how each method learns from labeled and unlabeled data.
% and carefully consider \emph{hyperparameters} and other settings that must be adapted to work well on new datasets beyond the standard class-balanced benchmarks (esp. our medical image datasets).
All descriptions below use the neural network notation defined in Sec.~\ref{sec:ssl_background}.
Hyperparameters and other settings are found in App.~\ref{sec:arch_and_hyperparameters}.


\paragraph{Pseudo-Label.} The pseudo-label method~\citep{leePseudolabelSimpleEfficient2013} is a natural way to use unlabeled data to help train a neural network.
At each minibatch of unlabeled data during stochastic gradient descent, we use the existing classifier to make predictions,
 obtaining the \emph{pseudo-label} $\hat{y}(x) \in \mathcal{C}$ indicating the most likely predicted class for image $x$.
If the predicted probability of the most likely class is above a user-specified \emph{confidence threshold} $\tau$, we include this example in the loss, treating the pseudo-label as the true label.
Thus, for our tasks the Pseudo-Label unlabeled loss $\mathcal{L}^U(x, \theta)$ is either weighted cross-entropy $\ell$ or zero (if the image is excluded):
\begin{align}
    \mathcal{L}^U(x, \theta) &= 
		1[ S(f_{\theta}(x))_{\hat{y}(x)} > \tau ]
    	\cdot \ell( \text{one-hot}(\hat{y}(x)), S(f_{\theta}(x))
, \quad  \hat{y}(x) = \argmax_{c \in \mathcal{C}} f_{\theta}(x)_c
\end{align}
where $1[ \cdot ]$ is an indicator function that returns either one (if the expression is true) or zero.
%where $\ell$ is \emph{binary cross entropy}, with balancing weights derived from the labeled training set.

% \todo{HZ: Move hyper to appendix}
% \emph{Hyperparameters for PseudoLabel.}
% Beyond the usual hyperparameters for our loss-minimization SSL framework, another important hyperparameter for pseudo-label is the threshold $\tau$. $\tau$ usually work well as long as it is in certain range. We set $\tau$ to 0.95, as done in past literature when using pseudo-label for comparison ~\citep{oliverRealisticEvaluationDeep2018,berthelotMixmatchHolisticApproach2019, berthelotRemixmatchSemisupervisedLearning2019, sohnFixmatchSimplifyingSemisupervised2020}.\footnote{standard cropping and shifting to the labeled data is applied following \cite{berthelotMixmatchHolisticApproach2019}} 

%using a deterministic annealing process, by which $\lambda$ is slowing increased.
%According to \cite{leePseudolabelSimpleEfficient2013}, if $\lambda$ is too high, it disturbs training even for labeled data.
%Whereas if $\lambda$ too small, we cannot use benefit from unlabeled data. The author recommended using a deterministic annealing process, by which $\lambda$ is slowing increased.

\paragraph{Virtual Adversarial Training (VAT).}
Recently, \citet{miyatoVirtualAdversarialTraining2019} present \emph{virtual adversarial training} as a way to improve robustness for both supervised and semi-supervised classifiers.
The key idea is that for each image $x$ we can easily find a nearby \emph{perturbed} version of the image $x' = x + \Delta^*$, where $\Delta^*$ is the vector that leads to greatest change in predicted label distribution.
To achieve \emph{smooth} and \emph{consistent} predictions, we wish to penalize cases where the predictions for $x$ differ from those for the nearby $x'$ (in KL divergence).
Every training image $x$ (both labeled and unlabeled) is assessed for this loss:
\begin{align*}
\mathcal{L}^U(x, \theta) &= 
    	\KL\left( S(f_{\theta}(x)), S(f_{\theta}(x + \Delta^*)) \right),
\quad 
\Delta^* = \argmax_{\Delta : || \Delta ||^2 \leq \epsilon }
	\KL( S(f_{\theta}(x)), S(f_{\theta}(x+\Delta)))
\end{align*}
%        L_{total} &= L_x + \lambda*L_u\\
%        L_x &= \frac{1}{|x|}\sum_{x_i \in X}H(y_i, S(f_{\theta}(x_i))) \\
%        L_u &= \frac{1}{|U|}\sum_{u_i \in U}KL(S(f_{\theta}(u_i)), S(f_{\theta}(u_i + radv_{i})))
The perturbation vector $\Delta^*$ is constrained to have magnitude below a given perturbation size $\epsilon > 0$. Its value can be found efficiently using routines from~\citet{miyatoVirtualAdversarialTraining2019}.

%Here, $r^*$ is the calculated adversarial pertubation for image $x$.
%where $radv = argmax_{r, \Vert\left (r) \right\Vert^2\leq \epsilon}(KL(S(f_{\theta}(x)), S(f_{\theta}(x+r))))$ is the calculated adversarial perturbation for this sample.
%VAT is mainly a method to better perturb input image compared to standard data augmentation techniques. During training, each mini-batch contains both labeled and unlabeled samples. For each sample in the mini-batch, we would calculate its virtual adversarial perturbation. We add the virtual adversarial perturbation to this sample to get a transformed version of this sample. For labeled samples in the mini-batch, they are used to calculate both the labeled loss and unlabeled loss (use the term unlabeled loss just for consistency of the terminology). The unlabeled samples are used to calculate the unlabeled loss. 

% \todo{HZ: Move hyper to appendix}
% \textit{Hyperparameters for VAT.}
% Beyond the usual hyperparameters for our loss-minimization SSL framework, for VAT we need to select values for $\epsilon$.
% In \citet{miyatoVirtualAdversarialTraining2019}, the authors claimed that they can achieve superior performance by tuning only $\epsilon$ and fixing $\lambda$ to 1. In our experiment, we used the default $\lambda$ as in \cite{berthelotMixmatchHolisticApproach2019} and searched the value of $\epsilon$, together with learning rate and weight decay. We select the best hyperparameters using validation set performance. \footnote{standard cropping and shifting to the labeled data is applied following \cite{berthelotMixmatchHolisticApproach2019}} 

%that controls the size of the norm of the virtual adversarial perturbation and $\lambda$ that controls the relative balance between supervised loss term and unsupervised loss term. In \citet{miyatoVirtualAdversarialTraining2019}, the authors claimed that they can achieve superior performance by tuning only $\epsilon$ and fixing $\lambda$ to 1. 

\paragraph{MixMatch.} MixMatch~\citep{berthelotMixmatchHolisticApproach2019} learns from unlabeled data by combining two key ideas: data augmentation and a variation of pseudo-label's unlabeled loss function.

\textit{How MixMatch performs augmentation.}
MixMatch uses the unlabeled set as a key input to its data augmentation procedure. 
The core of this procedure is MixUp~\citep{zhangMixupEmpiricalRisk2017}, which \emph{linearly interpolates} between two given images.
During training, MixMatch visits each minibatch (which contains labeled and unlabeled data). 
Each source labeled image (and its label) is transformed via MixUp with a randomly selected other image-label pair in that minibatch.
If an unlabeled example is selected for pairing, we create a pseudo-label $q(x)$ from the probabilistic vector output of the classifier: $q(x) = S( f_{\theta}(x))$. 
The resulting transformed labeled minibatch is fed into the labeled loss.
Thus, unlike other SSL methods described above, unlabeled data can inform training via augmentation alone, even if the unlabeled loss $\mathcal{L}^U$ is omitted.
% transforms it to an augmented minibatch using each image as a source for MixUp.

%During training, from each minibatch of labeled and unlabeled data we obtain a transformed labeled batch and a transformed unlabeled batch. 
%The obtained transformed labeled batch is used for calculating labeled loss.  
%Thus, unlike other SSL methods described above, unlabeled data can inform even the labeled loss via augmentation alone.

\textit{How MixMatch calculates unlabeled loss.}
MixMatch also transforms each unlabeled image $x$ in a  minibatch via MixUp to obtain $x'$, mixing with either labeled or unlabeled images.
These transformed examples are fed into a pseudo-label inspired unlabeled loss:
\begin{align}
\mathcal{L}^U(x, x', \theta) &= \frac{1}{C} \sum_{c=1}^{C} \left( q(x)_c - S(f_{\theta}(x'))_c \right)^2.
\end{align}
Unlike the original pseudo-label method, within MixMatch the pseudo-label $q(x)$ is a probability vector (rather than a one-hot vector).
Overall, this loss has \emph{no} threshold-based exclusion, so all unlabeled examples can contribute to the loss, and uses mean squared error (rather than alternatives like KL) as recommended for robustness by~\citet{berthelotMixmatchHolisticApproach2019}.

%mean squared error is often chosen here over alternatives like KL-divergence, since probability vectors has a bounded maximum value and thus being more robust when there is inevitable poor predictions. 

% over probability vectors has a bounded maximum value, and thus is used here for robustness, as for some inevitable poor predictions it will have less impact than alternatives like KL-divergence.

%\todo{HZ: Move hyper to appendix}


%hz:
% \textit{How MixMatch uses augmentation:}
% During training, within each minibatch, each unlabeled example $x$ is first \emph{augmented} to include $K$ transformed images. The "pseduo-label" of that unlabeled example is obtained using the average of current predictions of the network across all transformed images of that example. MixUp ~\citep{zhangMixupEmpiricalRisk2017}is then applied to current minibatch, a strategy that \emph{linearly interpolates} between the source image $x$ and another image sampled at random from the current minibatch (labeled or unlabeled).
% % MixMatch creates $K$ transformed images $\{x'_k\}_{k=1}^K$ for each image in the current minibatch.
% Thus, unlike other SSL methods described above, unlabeled data can inform even the labeled loss via augmentation alone.

%  \item \textbf{Paragraph describing high level idea}\\
%     MixMatch works by first guessing the labels for the unlabeled samples using current model predictions, use the guessed label as the 'pseudo-label'. Then applied MixUp on the labeled and unlabeled samples. During training, each mini-batch contains both labeled and unlabeled samples. For an unlabeled sample, we will apply data augmentation to this sample K times to get K transformed version of the sample. We will then pass these K transformed versions to the current network and get the current predictions for these K versions. The predictions are then averaged and sharpened to get the target distribution of these K transformed versions of the unlabeled sample (i.e., the 'pseudo-label'). We then apply MixUp on the labeled samples with their true labels and unlabeled samples with their 'pseudo-label', to get a transformed labeled set $X\textprime$, which will be used to calculate supervised loss $L_x$, and a transformed unlabeled set $U\textprime$, which will be used to calculate unsupervised loss $L_u$.
    
% \textit{How MixMatch uses augmentation:}
% During training, within each minibatch both labeled and unlabeled examples $x$ are \emph{augmented} to include $K$ transformed images. 
% These transformations are obtained via MixUp~\citep{zhangMixupEmpiricalRisk2017}, a strategy that \emph{linearly interpolates} between the source image $x$ and another image sampled at random from the current minibatch (labeled or unlabeled).
% MixMatch creates $K$ transformed images $\{x'_k\}_{k=1}^K$ for each image in the current minibatch.


% \textit{How MixMatch measures loss on unlabeled examples:}
% After data augmentation, MixMatch further obtains a ``pseudo-label'' for each unlabeled example using the current classifier.
% Unlike the original pseudo-label method, within MixMatch the pseudo-label $q(x)$ is \emph{kept probabilistic} (rather than one-hot), is averaged over the $K$ transformations, and is finally \emph{sharpened} via temperature hyperparameter $T$. Overall, this loss has \emph{no threshold-based exclusion}, so all unlabeled examples contribute equally to the loss.
% Thus, the unlabeled loss applied to each unlabeled example becomes a mean-squared error between the predicted probabilities and the pseudo-label probabilities:
% \begin{align}
% \mathcal{L}^U(x, \theta) &= \frac{1}{C} \sum_{c=1}^{C} \left( \rho_T(\hat{q}(x))_c - S(f_{\theta}(x))_c \right)^2,
% \quad \hat{q}(x) = \frac{1}{K} \sum_{k=1}^K S(f_{\theta}(x'_k))
% \end{align}
% where the sharpening function $\rho_T(\cdot)$ maps a $C$-length probability vector to another $C$-length probability vector: 
% $\rho(q) = \frac{1}{
% 	\sum_{c} q_c^{1/T}
% 	}[ q_1^{1/T}, ~ q_2^{1/T}, \ldots q_C^{1/T}]$.
% over probability vectors has a bounded maximum value, and thus is used here for robustness, as for some inevitable poor predictions it will have less impact than alternatives like KL-divergence.


%The obtained transformed unlabeled batch, together with their ``pseudo-label'' is used for calculating the unlabeled loss.
%Unlike the original pseudo-label method, within MixMatch the pseudo-label $q(x)$ is \emph{probabilistic} (rather than one-hot). Overall, this loss has \emph{no threshold-based exclusion}, so all unlabeled examples can contribute to the loss, and uses mean squared error (rather than alternatives like KL) as recommended by~\citet{berthelotMixmatchHolisticApproach2019}:
%%mean squared error is often chosen here over alternatives like KL-divergence, since probability vectors has a bounded maximum value and thus being more robust when there is inevitable poor predictions. 
%\begin{align}
%\mathcal{L}^U(x, \theta) &= \frac{1}{C} \sum_{c=1}^{C} \left( q(x)_c - S(f_{\theta}(x))_c \right)^2
%\end{align}
%% over probability vectors has a bounded maximum value, and thus is used here for robustness, as for some inevitable poor predictions it will have less impact than alternatives like KL-divergence.

% \todo{HZ: Move hyper to appendix}

% \textit{Hyperparameters for MixMatch.}
% Beyond the usual hyperparameters required by all methods in our framework, the key hyperparameters for MixMatch include the number of augmentations $K$, the temperature $T>0$ used for sharpening, interpolation hyperparameter $\alpha$ and unlabeled loss coefficient $\lambda$. We set $K=2$, $T=0.5$, and $\alpha=0.75$ as done in \cite{berthelotMixmatchHolisticApproach2019}, and search for $\lambda$ using validation set. 

% I agree: But we did not search for these hyperparameters, so we do not have evidence to back it i think. 
% While \cite{berthelotMixmatchHolisticApproach2019} claims that most of these do not need to be tuned aggressively on a new dataset, we find \todo{HZ DID WE AGREE WITH THIS?}.


\paragraph{Ablation: Augmentation-Only MixMatch.}
Given MixMatch's complexity, a natural question arises: does MixMatch benefit from unlabeled data because it informs the labeled loss via augmentation or because of the unlabeled loss directly?
While the original work did many other ablations~\citep{berthelotMixmatchHolisticApproach2019}, this question was not directly answered.
%we think this essential question of how value is derived from unlabeled data remains important especially in medical applications.
Our experiments thus assess a variant called \textbf{Augment-Only MixMatch}, which omits the unlabeled loss but still uses unlabeled data to inform the labeled loss via augmentation.


%    \begin{itemize}
%        \item most of MixMatch's hyperparameters can be fixed and do not need to be tuned on a per-experiment or per-dataset basis. 
%        \item change only $\alpha$ and $\lambda$ on a per-dataset basis, and found that $\alpha = 0.75$, $\lambda = 100$ are good starting points
%        \item batch normalization is important for MixMatch to work well
%        \item for the unsupervised loss, used MSE instead of cross entropy or kl, in order to make training more robust to incorrect guesses
%    \end{itemize}

\subsection{Standardized architecture, training, and hyperparameter selection.}
\label{sec:standard-SSL-framework}

Our main methodological interest is assessing \emph{how much} the SSL paradigm improves task performance on our datasets by incorporating unlabeled data, as well as \emph{which techniques} might work the best, as to our knowledge methods like VAT and MixMatch have not yet been applied to echocardiograms.
To put all methods on a fair footing, we use a common architecture (a WideResNet) and develop a standardized protocol for training parameters (ADAM with modest L2 regularization) used by all methods.
All hyperparameters are selected via a grid search to maximize balanced accuracy on the validation set.
Details about all architectures, hyperparameters, and training procedures are available in App.~\ref{sec:arch_and_hyperparameters}.
Our open-source code provides everything needed for reproducing our implementation\footnote{\codeURL}.

Below, we highlight two implementation choices that give consistent gains to all methods.

\paragraph{Unlabeled loss weight hyperparameter.}
For every SSL method we study, the unlabeled loss weight hyperparameter $\lambda > 0$ matters.
We follow the recommendations of \citet{leePseudolabelSimpleEfficient2013} and use a \emph{deterministic annealing} schedule to slowly increase $\lambda$ over epochs from an initial value of 0.0 to its maximum value linearly over the course of many training iterations. 
We select the maximum value of $\lambda$ for all methods by monitoring the model performance on validation set, and select the $\lambda$ that gives best validation performance.
A well-chosen unlabeled loss schedule is important to achieve good performance for MixMatch and other methods.
Supplementary Table~\ref{tab:MixMatch hyperparameters ablation study} suggests that tuning the schedule can improve balanced accuracy by over 1\%.

\paragraph{Ensembling models over one training run to improve generalization.}
A recent study by \citet{huangSnapshotEnsemblesTrain2017} suggests that rather than using the final checkpoint of an SGD training run, or even the best single checkpoint as ranked by validation loss, an ensemble of the checkpoints along the training trajectory can achieve better generalization.
We apply this ensemble method to \emph{every method} (SSL and baseline labeled-set only methods). We further incorporate the \emph{weighted average} idea from~\citet{caruanaEnsembleSelectionLibraries2004} , allowing better-performing checkpoints larger influence.
The final performance of each method is determined via an \emph{ensemble} of the last 25 checkpoints (one per epoch).
% Using an ensemble should give more robust predictions than a single snapshot of network parameters.
Supplement Table~\ref{tab:best_single_checkpoint_VS_ensemble_FS_echo260} shows that this ensembling improves performance by a modest but noticeable amount.

%We verified that this ensembling improves performance of our labeled-set-only baseline on EchoForAS-156-52 slightly compared to simply using the best single checkpoint as measured by validation-set performance. See Table 


\subsection{Methods for aggregating many images into patient-level predictions}
\label{sec:methods-patient-level}

A key aspect of our study is producing useful diagnosis predictions for a specific patient (indexed by $n$), based on multiple echocardiogram images collected for that patient (indexed by $i \in \{1, 2, \ldots I_n\}$). Each patient may have a different number of images $I_n$, with typical $I_n$ values in the 100s.
Given all images, we wish to predict the \emph{patient's} diagnosis label $y_n$ (one of no AS, mild/moderate AS, or severe AS).
We will use the image-specific view classifier network $f_{\theta_V}$ and diagnosis classifier network $g_{\theta_D}$ introduced in Sec.~\ref{sec:ssl_background}.

Below, we present several strategies for aggregating probabilistic predictions from several images to produce a \emph{patient-level} prediction.
Previous studies have often manually prescreened a subset of images whose view types are known to be relevant to the prediction tasks. Only these prescreened images are used to make patient-level predictions.
Instead, we consider the task faced in a real deployment where no manual prescreening is available, and we must compute the probability of the patient's diagnosis from all $I_n$ images: $p( y_n | x_{n, 1:I_n} )$.
%To aggregate across images, a common baseline is to simply average across the per-image diagnosis predictions.
%Instead, 
%To improve accuracy we wish to explore if approaches that leverage both our view and diagnosis classifiers can yield better patient-level predictions (see Fig.~\ref{fig:ssl_for_echo_diagram} for an illustration).

%%\paragraph{Majority vote.} This baseline uses the majority vote across images. 
%\begin{align}
%\hat{y}_n = \arg\max_{c \in \mathcal{C}_D} 
%%\textstyle
%\sum_{i=1}^{I_n} \hat{y}_{nic}, \quad 
%\hat{y}_{ni} = \text{onehot}(\arg\max_{c \in \mathcal{C}_D} g_{\theta_D}(x_{ni})_c)
%\end{align}

\paragraph{Simple average.} One aggregation strategy is to simply average over the diagnosis predictions for each of the $I_n$ images available for patient $n$, treating each image equally:
\begin{align}
p(y_n = c | x_{n,1:I_n} ) = 
%\textstyle 
	 \frac{1}{I_n}
	 \sum_{i=1}^{I_n} S(g_{\theta_D}(x_{ni}))_c.
\end{align}
While simple and used in previous work~\citep{ghorbaniDeepLearningInterpretation2020}, this method will be error-prone if many images do not depict anatomical features relevant to AS diagnosis.

\paragraph{Prioritize diagnoses from relevant views.}
To diagnose AS, the PLAX and PSAX views show the anatomical structures that are \emph{relevant}, while our catch-all ``Other'' view  contains many diverse view types that are mostly (but not completely) irrelevant.
Our view classifier network (with weights $\theta_V$) can predict which images depict a \emph{relevant} view (PLAX or PSAX).
Thus, we suggest an aggregation procedure for diagnosis predictions that uses a \emph{weighted} average over images.
Each image's weight $w(x_{ni}) \in (0,1)$ is the view classifier's probabilistic confidence that the image shows a \emph{clinically-relevant} view for our task:
\begin{align}
p(y_n = c | x_{n,1:I_n} ) \propto
%\textstyle
\sum_{i=1}^{I_n} w(x_{ni}) S(g_{\theta_D}(x_{ni}))_c, \quad w(x_{ni}) = p( v_{ni} \in \mathcal{R} | x_{ni}, \theta_V).
\end{align}
Here, the set of \emph{relevant} view types $\mathcal{R}$ contains PLAX and PSAX but not ``Other.''

We explored an alternative strategy of prioritization which \emph{thresholds} to identify a subset of relevant images (all treated equally) rather than probabilistic weighting in which all images contribute proportional to their weight. We found this strategy's performance is slightly inferior to our weighting strategy. Details can be found in Appendix \ref{tab:Suggested_Aggregation_Ablation}.

\paragraph{Learned image-to-patient prediction function.} 
We can further imagine \emph{training a model} that can produce the predicted probabilities needed for a patient, given relevant features from its component images.
We explored a few possible approaches based on manually engineered features and logistic regression classifiers, but did not find these delivered benefits worth the extra implementation effort. We leave this idea as a possible future direction.

%\begin{align}
%p(y_n = c | x_{n,1:I_n} ) = S( h_{\psi}( \phi(x_{n,1:I_n}) ))_c
%, \quad \phi(x_{n,1:I_n}) = \phi( \{r_{\theta_V}(x_{ni}), S(g_{\theta_D}(x_{ni})) \}_{i=1}^{I_n} ) 
%\end{align}
%Here, $\phi(\cdot)$ denotes a feature transform, either learned or manually defined, that maps all image-level information (probability vectors and confidence weights) to a fixed-length vector used for prediction with classifier $h$ and weights $\psi$.
%We explored a few possible approaches based on manually engineered features and logistic regression classifiers, but did not find these delivered benefits worth the extra implementation effort.
%We leave this idea as a promising future work direction.

%For the features \todo{ZHE DESCRIBE} \todo{we sort the images of a patient according to the confidence that this image is of PLAX, we take the top 4 most confident PLAX examples. We then sort the images again according to the confidence that this image is of PSAX, and take the top 4 most confident PSAX examples. We stack the 8 vectors together to form the feature vector, resulting in a 1x24 vector for each patient}.
%For the classifier, we try class-balanced logistic regression, we train this logistic regression model using the validation set.


\subsection{Transfer from view to diagnosis by pretraining}
\label{sec:transfer_from_view_to_diagnosis}

A unique property of our problem and dataset is that we have two types of labels for the same input: view labels and diagnosis labels.
We further have clinical knowledge that the tasks are closely related (successful diagnosis requires the ability to identify relevant views).
We leverage this relation to improve training of our diagnosis classifiers.
Specifically, we \emph{pretrain} a single-image view classification network and then use this network's weights as a warm-start for our diagnosis classifier.
Note this is different from common transfer learning practice where a network is pretrained on some other dataset.
Our method does not require an additional dataset, merely other labels on the same dataset.
%In this regard, our method is al share similar spirit with \emph{multitask learning}.

\subsection{Multi-task learning using both diagnosis and view labels}
Another way to improve diagnosis using view labels is via multitask learning~\citep{ruder2017overview,zhang2021survey}, thought it is sometimes challenging in practice to determine whether auxiliary tasks will be helpful or harmful to the main task \citep{zhang2021survey, ruder2017overview}.
We investigate whether a simple multi-task approach that trains the same network to jointly recognize view and diagnosis could be effective. 
Our multi-task labeled loss is:
\begin{align}
\mathcal{L}^L(\theta) 
&= \ell( y, S(g_{\theta_D}(x))) + \gamma \ell( v, S(f_{\theta_V}(x))), 
\label{eq:multitask}
\end{align}
where $y$ represents the image's one-hot diagnosis label, $v$ is the one-hot view label, and $\ell$ is a weighted cross-entropy loss.
Hyperparameter $\gamma$ controls the strength of the view loss, as we ultimately care most about diagnosing the AS severity.
We focus on \emph{labeled-set-only} evaluation of this strategy. Future work could explore semi-supervised multi-task learning.

% the multitask strategy with fully supervised network as a sanity check for the usefulness of the view information for training a diagnosis classifier.


% \todo{move hyper to appendix}
% We searched $\alpha$ in the range [10, 3, 1, 0.3, 0.1]. The best $\alpha$ is selected together with other hyperparameters on validation set. 