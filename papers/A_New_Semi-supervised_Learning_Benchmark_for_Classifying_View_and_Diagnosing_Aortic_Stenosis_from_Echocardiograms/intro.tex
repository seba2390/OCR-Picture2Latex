
Our motivating task is to improve timely diagnosis and treatment of aortic stenosis (AS), a common cardiac valve condition.
If left untreated, severe AS has lower 5-year survival rates than several metastatic cancers~\citep{howladerSEERCancerStatistics2020,clarkFiveyearClinicalEconomic2012}.
With timely diagnosis and surgical or transcatheter aortic valve replacement, AS becomes a treatable condition with very low mortality~\citep{lancellottiOutcomesPatientsAsymptomatic2018}.
Unfortunately, in current practice up to 2/3 of symptomatic AS patients may never get referred for care~\citep{tangContemporaryReasonsClinical2018,brennanProviderlevelVariabilityTreatment2019}.
There is an urgent need in improve timely detection of this life-threatening condition.
In this study, we develop and validate machine learning methods for automating the preliminary interpretation of cardiac ultrasound (echocardiogram) images, with the goal of expanding access to rapid and accurate diagnosis of AS while overcoming constraints on the availability of labeled data needed to train methods effectively.

Recent advances in computer vision and machine learning have made it possible to \emph{automate} the way medical images are turned into actionable knowledge for diagnosing and treating disease across cardiology~\citep{chenDeepLearningCardiac2020}, radiology, and other areas of medicine~\citep{shenDeepLearningMedical2017}.
However, in order to work well, modern deep learning methods require \emph{large} amounts of \emph{labeled} training examples, where each labeled example consists of an image and its desired class label.
While images themselves are routinely collected and easily available in electronic health records, obtaining \emph{labels} for images often requires manual effort from a clinical expert.
Thus, a key barrier to deploying deep learning image classifiers for specialty areas of medicine is that it is prohibitively \emph{difficult} and \emph{expensive} to acquire a large labeled dataset whose scale matches the tens of thousands of labeled examples available in common non-medical benchmarks such as CIFAR-10~\citep{krizhevskyLearningMultipleLayers2009} or Street View Housing Numbers~(SVHN,~\citet{netzerReadingDigitsNatural2011}). Privacy and regulatory concerns further inhibit sharing of labeled datasets even if they are collected within a single healthcare system.

% Clinical experts have many demands on their time and can rarely spare the hundreds of hours needed to provide comprehensive labels for the tens of thousands of images that would be needed to reach the scales of typical off-the-shelf labeled training datasets for image classification benchmarks such as CIFAR-10~\citep{krizhevskyLearningMultipleLayers2009} or Street View Housing Numbers~(SVHN,~\citet{netzerReadingDigitsNatural2011}).

A promising technology to overcome the need for abundant labeled data is semi-supervised learning (SSL)~\citep{zhuSemiSupervisedLearningLiterature2005,chapelleSemiSupervisedLearning2010,vanengelenSurveySemisupervisedLearning2020}.
SSL methods train classifiers simultaneously from two data sources: a \emph{small} labeled set and a \emph{large} unlabeled set.
Recent semi-supervised deep learning methods have produced remarkable progress~\citep{miyatoVirtualAdversarialTraining2019,berthelotRemixmatchSemisupervisedLearning2019,berthelotMixmatchHolisticApproach2019,sohnFixmatchSimplifyingSemisupervised2020,xie2019unsupervised,chenBigSelfSupervisedModels2020}.
On the standard SVHN image classification benchmark, a typical WideResNet achieves an error rate of 12.8\% when trained using only a \emph{small} labeled set of 1000 total labeled images (100 for each of 10 digit categories).
In contrast, a recent SSL method called MixMatch~\citep{berthelotMixmatchHolisticApproach2019} can reduce error rate to 3.3\% using the small labeled set plus 60,000 unlabeled examples, or even to 2.2\% with 600,000 unlabeled examples.
These improvements are comparable to training on 50x larger \emph{fully-labeled} datasets, but avoid the time and expense of collecting so many labels (which for our intended applications are reliably obtained only from clinical experts).
While SSL methods are promising, the application of modern SSL methods to real medical imaging tasks is largely \emph{untested} and requires development of new methods to address issues such as class imbalance and the need to aggregate many image-specific predictions into coherent decisions for the whole patient. 

In parallel to our work, several recent efforts do explore modern SSL methods to analyze medical images. \citet{meng2020mutual} proposed a SSL domain adaptation method for classifying views of fetal ultrasounds. \citet{calderon2021dealing} utilize MixMatch to detect COVID-19 based on chest X-ray images. \citet{chen2021venibot} leverage unlabeled data to improve vein segmentation performance using ultrasound images. \citet{wang2021deep} propose an SSL method for classifying breast lesion and ophthalmic diseases.
Our study contributes to this growing literature by applying SSL to improve patient-level screening in cardiology.
%that combines pseudo-labeling with consistency training 

As a case study to test the promise of SSL methods, we develop SSL classifiers to diagnose aortic stenosis (AS). AS is diagnosed using ultrasound imaging of the heart, known as \emph{echocardiography}.
While ultrasound imaging itself is widely available and performed routinely for many patients for a variety of reasons, accurate interpretation of echocardiograms to make complex imaging diagnoses such as AS requires significant expertise that is not widely available.
Diagnostic errors may contribute to treatment delays because assessment is challenging and requires integrating information across many hemodynamic parameters~\citep{baumgartnerRecommendationsEchocardiographicAssessment2017}, that are often discordant~\citep{minnersInconsistenciesEchocardiographicCriteria2008} and have low inter-reader reliability~\citep{sacchiDopplerAssessmentAortic2018}.
Automated grading of AS has the potential to increase the accuracy and reproducibility of disease detection and reduce barriers to access~\citep{batchelorAorticValveStenosis2019}, especially as a first-line screening tool in geographic areas without expert cardiologists.
We believe that automated preliminary assessment of AS, with timely follow-up care by an expert clinical team, will improve patient outcomes by better identifying patients with this life-threatening condition that require treatment.
%Automated preliminary diagnosis of AS, with careful follow-up care by an expert clinical team, would improve patient outcomes by reliably identifying patients who require further study and treatment to avoid this often fatal disease.

% TODO Summarize existing methods.
% TODO Write why existing methods aren't enough. 
% Highlight experiments in Oliver et al paper that motivated our work.
%It has been a common wisdom in the Semi-supervised learning community that out-of-distribution samples can hurt SSL algorithms. %my conversation with 1st author of UDA, 1st and 3rd author of FixMatch also confirmed this point. 
%Oliver et al. \cite{oliver2018realistic} showed that adding unlabeled data from other distribution than the labeled data can actually hurt performance compared to not using any unlabeled data at all. Limited previous work has consider the effect of contamination. \cite{xie2019unsupervised} used common technique for detecting the out-of-distribution sample before training SSL model. Data contamination (containing out-of-distribution samples in the unlabeled set) is common in real world application, however, most of the modern SSL techniques failed to consider such realistic scenario in their study.  


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/ssl_for_echo_diagram_polished.pdf}
    \caption{Illustration of our study's goal -- automating diagnosis of the severity of \emph{aortic stenosis} (AS) given hundreds of echocardiogram images collected in a typical exam -- as well as the key technical challenges and proposed contributions that address these challenges.}
    \label{fig:ssl_for_echo_diagram}
\end{figure}

\subsection*{Contributions}
In this work, we make the following contributions:
\begin{enumerate}[leftmargin=0cm,itemindent=.4cm,labelwidth=\itemindent,labelsep=0cm,align=left]
	\item 
	\textbf{New open-access SSL dataset -- the Tufts Medical Echocardiogram Dataset (TMED) -- to benchmark view and disease severity classification.}
	Our dataset is directly inspired by the need for automated preliminary assessment of aortic stenosis (AS).
	The labeled set of 260 patients contains an AS disease severity label for each patient as well as a view label for all images, all provided by expert clinicians.
	%Both labels are challenging to collect labeled data for given the lack of view labels in current routine image collection and the complexity of AS diagnoses.
	%The diagnosis task further requires aggregating across many images of the same patient to make a coherent patient-level decision for that individual.
% 	We plan to release this dataset so it will be publicly-accessible by academic researchers after blind review, and are working with our institutional IRB for approval to do so.
	Furthermore, our dataset is designed to assess the true potential of semi-supervised learning, because in addition to the labeled set it contains a large \emph{unlabeled} set from 2645 patients captured in the course of standard cardiac care.
	 %and annotated by expert clinicians for view and diagnosis tasks. 
	 %The dataset further includes a large \emph{unlabeled} set from 2645 patients.
	Common SSL benchmarks such as CIFAR-10~\citep{krizhevskyLearningMultipleLayers2009} or STL-10~\citep{coatesAnalysisSingleLayer2011} do not contain truly unlabeled data but instead artificially ``forget'' existing labels.
	This makes the unlabeled data used in these benchmarks unrealistically clean, class-balanced, and relevant to the task.
	We hope our new dataset will inspire work on effectively using minimally-curated unlabeled data to improve medical image understanding.
	
	\item 
	\textbf{Evaluation of SSL methodology to find what works best and why.} We carefully compare standardized implementations of several state-of-the-art SSL methods. We find that MixMatch~\citep{berthelotMixmatchHolisticApproach2019} performs best on both view and diagnosis tasks, reliably beating labeled-set-only methods by over 2\% balanced accuracy on view classification (see Table~\ref{tab:view classification large}) and by over 3\% balanced accuracy on patient-level diagnosis (see Table~\ref{tab:diagnosis classification large patient}). These gains are also consistent for smaller versions of our dataset.
	  	Further ablation studies suggest that the surprisingly effective ``mix-up'' data augmentation strategy underlying MixMatch is a primary reason for success.
	  	In contrast, virtual adversarial training (VAT,~\citet{miyatoVirtualAdversarialTraining2019}), which by its adversarial design might be expected to perform better in medical imaging domains, only marginally improves over labeled-set-only methods.
	  
	\item \textbf{New methods for patient-level severity diagnosis without manually preselecting relevant views.}
	A patient study may contain over 100 echocardiogram images from diverse view types (see Fig.~\ref{fig:ssl_for_echo_diagram}).
	Only some of these images are relevant to diagnosing AS (only some views show the aortic valve that AS impacts).
	View type labels are not usually available.
	Previous work has relied on manual preselection of relevant views~\citep{madaniDeepEchocardiographyDataefficient2018,ouyangVideobasedAIBeattobeat2020}.
	We develop methods that directly consume \emph{all available images}, as would be needed in a fully automated deployment.	
	%To achieve the best patient-level diagnosis performance, we take a two step approach.
	%First, we develop single-image classifiers that produce probabilities of both view and diagnosis labels.
	%Second, we develop an aggregation method that prioritizes diagnosis predictions from images that our view classifiers suggest are \emph{relevant} view types.
	Rather than simply average diagnostic predictions across all images, we \emph{prioritize} diagnoses from images that our view classifiers suggest belong to the view types known to be relevant for AS.
	Using only the labeled set and trying to distinguish between 3 levels of AS severity (none, mild/moderate, and severe), prioritizing relevant views achieves patient-level balanced accuracy of 86.6\% compared to 81.6\% with simple averaging and 33\% for random guessing.
	Our best SSL methods that prioritize relevant views achieve 87.9\%, further improving to 90.1\% when we \emph{pretrain} a view classifier and use this to warm start our diagnosis classifier.
	These results suggest that fully-automated preliminary diagnosis of aortic stenosis may be soon realizable in practice.
	%transfers knowledge from an SSL-trained view classification network to a diagnosis task and then further aggregates SSL image-level predictions in a \emph{view-aware manner} reaches balanced accuracies of 87.60\% compared to labeled-set-only methods which achieve 81.66\%, suggesting that competitive preliminary diagnosis is achievable by automated methods and substantially aided by abundant unlabeled data.

\end{enumerate}

\subsection*{Generalizable Insights about Machine Learning in the Context of Healthcare}
%% This section is \emph{required}, must keep the above title, and should
%% be the final part of your introduction.  In about one paragraph, or
%% 2-4 bullet points, explain what we should \emph{learn} from reading
%% this paper that might be relevant to other machine learning in health endeavors

The key barrier to applying deep learning to address many medical tasks is the lack of abundant labeled data.
Recent SSL methods appear to reach competitive performance with only modest labeled datasets, but lack authentic evaluation using truly unlabeled clinical data exhibiting issues such as irrelevance to the task or class imbalance.
Thus, existing benchmarks may be too ``clean'' and over-state the potential of SSL methods.
As a remedy, this study develops clinically-motivated semi-supervised learning tasks and releases a dataset useful for measuring the gains possible with SSL methods.
Next, we offer insight about \emph{which SSL methods work best and why}.
Our careful evaluation and ablation studies can help practitioners identify promising methods like MixMatch.
Furthermore, our analysis encourages work to \emph{look beyond simple averaging} when aggregating across multiple images to make a patient-level diagnosis.
%Overall, just like the recent watershed paper by \citet{oliverRealisticEvaluationDeep2018} called for improved evaluation methodologies to make general-purpose deep learning SSL pipelines comparable and reproducible, 
Overall, we hope our work helps unlock the potential of easily-accessible unlabeled data to improve patient outcomes.


