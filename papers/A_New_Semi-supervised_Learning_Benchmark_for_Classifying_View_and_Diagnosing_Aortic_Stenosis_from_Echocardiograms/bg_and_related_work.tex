\subsection{Background: Single-task SSL with Neural Networks for Image Classification}
\label{sec:ssl_background} 

We consider the problem of \emph{semi-supervised} image classification.
For training, we are given two datasets.
First, a (small) \emph{labeled} dataset $\mathcal{D}^L$ containing $N^L$ pairs of images $x$ and corresponding labels $y$.
Second, a (large) \emph{unlabeled} dataset $\mathcal{D}^U$ of $N^U$ examples of images only, presumed to be sampled from a similar distribution as the labeled set.
Each image $x$ is represented as a standard tensor of pixel intensity values (one entry for each pixel and each color channel).
Each image-specific label $y \in \mathcal{C}$ indicates one of the possible classes in set $\mathcal{C}$.

Given a deep neural network parameterized by weight vector $\theta$, a standard SSL training procedure tries to find an (approximate) solution to the following loss minimization problem:
\begin{align}
\theta^* = \arg\min_{\theta}
	\frac{1}{N^L}
	\sum_{x, y \in \mathcal{D}^L}
	\mathcal{L}^L(y, x, \theta) % f_{\theta}(x) )
	+\lambda 
	\frac{1}{N^U}
	\sum_{x \in \mathcal{D}^U} \mathcal{L}^U( x, \theta )
\end{align}
\label{eq:standard-SSL-loss-template}
Here, $\mathcal{L}^L$ indicates a \emph{labeled loss} function such as cross-entropy, and $\mathcal{L}^U$ indicates an \emph{unlabeled loss}. We show how each SSL method we explore instantiates this framework in Sec.~\ref{sec:Methods-SSL}.

For our specific application, we separately train neural networks to perform two tasks.
First, for \emph{view} classification we wish to map each image $x$ to a real-valued score for each of the possible view classes in set $\mathcal{C}_V$.
We use a network $f$ parameterized by weights $\theta_V$.
To obtain probability distributions over the classes, we use the softmax transformation, denoted as $S(\cdot)$, which produces a probability vector with same size as the given input vector.
Thus, the probability that an image's view label indicates the $c$-th element of $\mathcal{C}_V$ is then $S( f_{\theta_V}(x) )_c$.

Second, for \emph{diagnosis} classification we wish to map each image $x$ to a real-valued vector containing scores for each of 3 posssible diagnoses (no AS, mild/moderate AS, severe AS) in set $\mathcal{C}_D$.
We use network $g$ with weights $\theta_D$. We always use the same architecture as view network $f$.
The probability of assigning the $c$-th label in $\mathcal{C}_D$ to image $x$ is $S( g_{\theta_D}(x) )_c$.

\subsection{Semi-supervised learning: Datasets and evaluation}

Our work builds upon a previous study by~\citet{oliverRealisticEvaluationDeep2018} on best practices for SSL evaluation.
That work emphasizes the need to compare SSL methods that claim advantages from unlabeled data to strong baselines that only use the labeled set.
We follow this best practice in our work.
Their independent evaluation suggests that, if trained properly, modern deep SSL methods can leverage large unlabeled sets to obtain meaningful improvements.
%Their work highlights the importance of careful, reproducible evaluation.

However, a key limitation of existing SSL research is that popular evaluations, even those used in~\citet{oliverRealisticEvaluationDeep2018}, are confined to well-worn non-medical datasets, such as SVHN~\citep{netzerReadingDigitsNatural2011} or CIFAR-10~\citep{krizhevskyLearningMultipleLayers2009}.
These datasets consist of carefully curated \emph{balanced} class distributions, where the so-called ``unlabeled'' set is obtained by forgetting known labels.
Performance numbers are likely too optimistic compared to using truly unlabeled images.
As performance saturates on these benchmarks,
 SSL research needs to move towards datasets like ours featuring truly unlabeled images.
 
\subsection{Related Work: Supervised learning for cardiology}

Deep learning research efforts for cardiac imaging applications are plentiful~\citep{chenDeepLearningCardiac2020}.
For echocardiogram analysis, the task of \emph{view} classification has been pursued by several efforts~\citep{madaniFastAccurateView2018,zhangFullyAutomatedEchocardiogram2018,longIdentificationEchocardiographicImaging2018}.
Accuracies above 90\% can be achieved given a \emph{large labeled} dataset of echocardiogram images and corresponding views.
Training CNN view classifiers on 200,000 images from 240 patients,~\citet{madaniFastAccurateView2018} report accuracy of 91.7\% on 15 view types using low-resolution images, exceeding the performance of board-certified cardiographers.
\citet{zhangFullyAutomatedEchocardiogram2018} also report promising performance for CNNs to classify 23 different view types (e.g. 96\% accuracy for the PLAX view).
However, we emphasize that these efforts use \emph{proprietary datasets} that other researchers cannot build upon and extend. They also require \emph{large} labeled sets and do not address the key motivation of our work, the downstream diagnosis of \emph{aortic stenosis}.


Recently, researchers at Stanford developed an ``EchoNet'' deep learning methodology for echocardiography~\citep{ghorbaniDeepLearningInterpretation2020}, predicting measurements related to ejection fractions as well as some binary decisions such as ``does the patient have a pace-maker?'' or ``is there left ventricular hypertrophy?''
\citet{ghorbaniDeepLearningInterpretation2020} produce \emph{patient-level} decisions just by simple averaging, reporting the surprising conclusion that ``alternative methods were explored in order to aggregate frame-level predictions into one patient-level prediction and did not yield better results compared to simple averaging.''
Our later results suggest that smarter aggregation \emph{does} produce notable benefits.

The same team later produced ``EchoNet Dynamic'' methodology to learn from videos (not static images)~
\citep{ouyangVideobasedAIBeattobeat2020}.
releasing 10,030 videos of echocardiogram imagery for one particular view (apical-4 chamber) and associated measurement and diagnostic labels for thousands of subjects.
This is a welcome step forward, but this dataset pursues different goals than ours: there is no focus on semi-supervised learning and this data focuses on only one view type (apical 4 chamber or A4C) out of the dozens of possible views that make up a complete study.
While this A4C view is relevant to other measurement and diagnostic tasks in cardiology, it is not helpful for assessing valvular heart diseases generally nor AS in particular.

Another public echocardiogram dataset is the CAMUS dataset \citep{leclerc2019deep}. CAMUS' purpose is to evaluate \emph{segmentation} methods. The data contains images of apical 2 chamber (A2C) and apical 4 chamber (A4C) views from 500 patients, together with detailed annotations of anatomical structures (e.g. the myocardium and the left atrium).  Like other prior work, these views are not relevant for the diagnosis of aortic valve disease.

% Echocardiography is the primary method for assessing AS.
Several efforts have looked at diagnosing \emph{aortic stenosis} as the target outcome of a machine learning classifier, though none we are aware of use echocardiograms. %They differ in the measurements they collect from each patient, the number of subjects included in their proprietary datasets and (of course) the ultimate performance achieved.
\citet{yangClassificationAorticStenosis2020} used wearable sensors to build binary AS classifiers using data from 34 total subjects, reporting 96\% accuracy with random forests.
\citet{kwonDeepLearningBased2020} used \emph{electro}cardiogram (ECG) signals to build binary AS classifiers from over 30,000 patients at 2 hospitals in Korea.
Their best 12-lead models achieve 0.861 AUROC on external validation.
%0.884 AUROC on internal validation
\citet{hataClassificationAorticStenosis2020} used ECGs from 700 patients in Japan.
Their 12-lead models achieve 84.2\% precision and 72.7\% recall on a heldout set.
In clinical practice, echocardiograms (ultrasound images) are the primary imaging modality used to assess the aortic valve.
To the best of our knowledge, there does not seem to be previous work on detecting aortic stenosis (AS) from echocardiograms, likely because echocardiograms contain complex data (video clips and Doppler recordings) that are not routinely annotated as part of clinical care. Acquiring labels for these images is prohibitively time consuming and expensive.
Among early efforts to automate AS screening, our study stands out for its focus on ultrasound (the clinical standard for diagnosing AS) and for showing how \emph{semi-supervised learning} can overcome limited labeled data.
% for views and diagnoses.

\subsection{Related Work: Semi-supervised learning for echocardiography}

Work on \emph{semi-supervised} learning for echocardiography is still in early stages.
Previously,~\citet{madaniDeepEchocardiographyDataefficient2018} pursued semi-supervised classification of echocardiograms using \emph{generative adversarial networks} (GANs)~\citep{goodfellowGenerativeAdversarialNets2014}, including a 15-way view classification task using 267 patients as well as a diagnostic task for left ventricular hypertrophy (LVH).
All 2269 images from 455 patients were \emph{manually preselected} for a particular relevant view.
Impressively, they report over 92\% accuracy at the LVH diagnosis task using GANs; their view classifiers were similarly competitive.

Our work builds upon~\citet{madaniDeepEchocardiographyDataefficient2018} in three key ways.
First, we pursue diagnosis \emph{without manually preselecting relevant views}. Our setting matches what is needed in a real deployment where view labels would not be available.
Second, we develop methods to aggregate image-level predictions to produce \emph{patient-level} decisions, showing we can do much better than simple averaging.
Finally, we offer more streamlined and competitive methodology.~\citet{madaniDeepEchocardiographyDataefficient2018} suggest \emph{different} methods for semi-supervised and fully-supervised settings, using GANs for the former and convolutional neural nets (CNNs) for the later.
Instead, modern SSL methods can coherently pursue the same task no matter how much labeled data we have.
Our approach builds on well-established CNNs and extensions to wide residual architectures~\citep{zagoruykoWideResidualNetworks2017}.
We do not need the complexities of GANs or their well-known training difficulties~\citep{metzUnrolledGenerativeAdversarial2017,aroraGANsActuallyLearn2017}. 
Recent evaluations~\citep[Table 4]{miyatoVirtualAdversarialTraining2019} suggest the methods we build upon reduce error rates by 50\% over GANs on SSL benchmarks like SVHN.


%It has been a common wisdom in the Semi-supervised learning community that out-of-distribution samples can hurt SSL algorithms. %my conversation with 1st author of UDA, 1st and 3rd author of FixMatch also confirmed this point. 
%Oliver et al. \cite{oliver2018realistic} showed that adding unlabeled data from other distribution than the labeled data can actually hurt performance compared to not using any unlabeled data at all. Limited previous work has consider the effect of contamination. \cite{xie2019unsupervised} used common technique for detecting the out-of-distribution sample before training SSL model. Data contamination (containing out-of-distribution samples in the unlabeled set) is common in real world application, however, most of the modern SSL techniques failed to consider such realistic scenario in their study.  

