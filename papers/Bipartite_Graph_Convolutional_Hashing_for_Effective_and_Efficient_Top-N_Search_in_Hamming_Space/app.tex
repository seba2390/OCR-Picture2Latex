\appendix

\section{Loss Landscape Visualization}
\label{sec:visualization}
We simulate the optimization trajectories of learnable embeddings and visually compare the loss landscapes of non-hashing and hashing versions in Figure~\ref{fig:model}(a).
Specifically, we manually assign perturbations~\cite{nahshan2021loss, bai2020binarybert} to the embeddings on MovieLens dataset as: {\footnotesize$\emb{V}_x^{(l)} = \emb{V}_x^{(l)} \pm p \cdot$ $\overline{|{\emb{V}_x^{(l)}|}}$ $\cdot \emb{1}^{(l)}$}.
where {\footnotesize$\overline{|{\emb{V}_x^{(l)}|}}$} represents the absolute mean of entries in {\footnotesize$\emb{V}_x^{(l)}$} and perturbation magnitudes $p$ are from $\{0.01, \cdots, 0.50\}$. $\emb{1}$ is an all-one vector. 
For pairs of perturbed node embeddings, we plot their loss distribution accordingly.
As we can observe, the non-hashing version produces a flat loss surface, showing the local convexity.
On the contrary, the hashing counterpart has a bumping and complex loss landscape.




\section{Notation Table and \model~Pseudo-codes}
\label{app:notation_and_code}
We use bold uppercase and calligraphy characters for matrices and sets. The non-bolded denote graph nodes or scalars. 
Key notations and Pseudocodes are explained in Table~\ref{tab:notation} and Algorithm~\ref{alg:model}.

\begin{table}[t]
% \setlength{\abovecaptionskip}{0cm}
% \setlength{\belowcaptionskip}{0cm}
\caption {Notations and meanings.}
\vspace{-0.15in}
\label{tab:notation}
  \footnotesize
  \begin{tabular}{c|l} 
     \hline
          {\bf Notation} & {\bf Meaning}\\
     \hline\hline
          {\notsotiny $\mathcal{G},\mathcal{V}_1$, $\mathcal{V}_2$, $\mathcal{E}$} & Bipartite graph with sets of nodes and edges.\\
    \hline
        {$c$, $d$}  & Convolution dimension and hash code dimension.\\
    \hline
        {$\emb{V}_x^{(l)}$}  & {Node $x$'s embedding at iteration $l$.} \\
    \hline
         \tabincell{l}{$\emb{Y}$}   & \tabincell{l}{{Edge transactions where $\emb{Y}_{x,y}=1$ indicates the interaction} \\ existence between nodes $x$ and $y$, and otherwise $\emb{Y}_{x,y}=0$.} \\
    \hline
        {$\emb{A}$}, $\emb{D}$    & {Adjacency matrix and associated diagonal degree matrix.}  \\
    \hline
        $\emb{p}^{(k)}$, $\emb{P}$ & Dispersing vector at iteration $k$ and the projection matrix.\\
    \hline
        $\widetilde{V}$ & Feature-dispersed embedding matrix. \\
    \hline
        {$\widehat{\emb{Y}}$} & {Estimated matching scores.} \\
    \hline
        {$\emb{Q}_x^{(l)}$}   &  {Hash code segment of node $x$ at iteration $l$.} \\
    \hline
        $\alpha^{(l)}$  & $x$'s rescaling factor computed at the $l$-th convolution.\\
    \hline
        {$\emb{Q}_x$}  &   {Target hash codes of node $x$.} \\
    \hline
        {$L$}, {$K$}  &  {Numbers of convolutional hashing and dispersion generation.}\\
    \hline
        $\mathcal{L}_{rec}$, $\mathcal{L}_{bpr}$, $\mathcal{L}$ & {Two loss terms of final objective function $\mathcal{L}$.} \\
    \hline
      {$\eta$, $H$, $n$, $\lambda_1$, $\lambda_2$}  & hyper-parameters.\\
    \hline
  \end{tabular}
\end{table}


\begin{algorithm}[t]
\small
\caption{\model~algorithm.}
\label{alg:model}
\LinesNumbered  
\While{\rm{model not converge}}{
	\For{$k = 0, \cdots, K-1$}{
		$\emb{p}^{(k+1)}$ $\gets$ $(\emb{V}^{(0)})^\mathsf{T}\emb{V}^{(0)}\emb{p}^{(k)}$; \\
	}
	$\emb{P} \gets$ obtain the projection matrix \Comment*[r]{Eq.(\ref{eq:projection})} 
	$\widetilde{\emb{V}}^{(0)} \gets$ obtain the feature-dispersed embeddings \Comment*[r]{Eq.(\ref{eq:disperse})}
    \For{$l = 0, \cdots, L-1$}{
          $\widetilde{\emb{V}}^{(l+1)} \gets(\emb{D}^{-\frac{1}{2}} \emb{A} \emb{D}^{-\frac{1}{2}} )\widetilde{\emb{V}}^{(l)}$ \Comment*[r]{Eq.(\ref{eq:fdconv})}
          $\emb{{Q}}^{(l+1)} \gets \sign\big(\widetilde{\emb{V}}^{(l+1)})$ \Comment*[r]{Eq.(\ref{eq:hashing})}
          $\emb{\alpha} \gets$ calculate the rescaling factors \Comment*[r]{Eq.(\ref{eq:rescale})}
        }
      \For{$x \in \mathcal{V}_1, y \in \mathcal{N}(x)$}{
      $\widehat{\emb{Y}}_{x,y} \gets$ $\alpha_x\alpha_y$ $(d - 2D_{H}(\emb{Q}_x, \emb{Q}_y))$ \Comment*[r]{Eq.(\ref{eq:inner_score})\&Thm.\ref{tm:equal}}
     }
      $\mathcal{L} \gets$ compute loss and optimize the model \Comment*[r]{Eq's.(\ref{eq:rec}-\ref{eq:L})} 
      
}
\textbf{Function} \tt{Gradient\_estimator}($\mathcal{L}$): \\
$\frac{\partial \mathcal{L}}{\partial \emb{V}} \gets \frac{\partial \mathcal{L}}{\partial \emb{Q}} \cdot \frac{4}{H} \sum_{i=1,3,5,\cdots}^{n} \cos(\frac{\pi i \emb{V}}{H})$ \Comment*[r]{Eq.(\ref{eq:gradient})}
\end{algorithm}


\input{analysis}


\section{Experiment Setup Details}
\label{app:exp}
{\textbf{Datasets.}}
We evaluate our model on the following six six datasets:
\begin{enumerate}[leftmargin=*]
\item \textbf{MovieLens}\footnote{\url{https://grouplens.org/datasets/movielens/1m/}} is a widely adopted benchmark between \textit{users} and \textit{movies}. Similar to the setting in~\cite{hashgnn,he2016fast}, if the user $x$ has rated item $y$, we set the edge $\emb{Y}_{x,y} = 1$, otherwise $\emb{Y}_{x,y} = 0$. 
\item \textbf{Gowalla}\footnote{\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowalla}}~\cite{ngcf,hashgnn,lightgcn,dgcf} is the dataset~\cite{liang2016modeling} between \textit{customers} and \textit{their check-in locations} collected from Gowalla. 
\item \textbf{Pinterest}\footnote{\url{https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv}} is an open dataset for image recommendation between \textit{users} and \textit{images}.
Edges represent the pins over images initiated by users. 
\item \textbf{Yelp2018}\footnote{\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/yelp2018}} is from Yelp Challenge 2018 Edition, bipartitely modeling between \textit{users} and \textit{local businesses}.
\item \textbf{AMZ-Book}\footnote{\url{https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/amazon-book}} is the bipartite graph between \textit{readers} and \textit{books}, organized from the book collection of Amazon-review~\cite{he2016ups}.  
\item \textbf{Dianping}\footnote{\url{https://www.dianping.com/}} is a commercial dataset between \textit{users} and \textit{local businesses} recording their diverse interactions, e.g., clicking, saving, and purchasing. 
\end{enumerate}
% \vspace{0.05in}

\textbf{Evaluation metrics.}
To evaluate the model performance of Hamming space retrieval over bipartite graphs, we directly deploy our model \model~ in the basic user-item recommendation scenarios.
Specifically, given a query node, we apply the hash codes to match Top-N answers for the query with the closest Hamming distances, and thus adopt two widely-used evaluation protocols Recall@N and NDCG@N to measure the ranking capability.

\vspace{0.05in}

{\textbf{Implementations.}}
We implement our models under Python 3.6 and PyTorch 1.14.0 on a Linux machine with 1 Nvidia GeForce RTX 3090 GPU, 4 Intel Core i7-8700 CPUs, 32 GB of RAM with 3.20GHz.
For all the baselines, we follow the official hyper-parameter settings.
We apply a grid search if lacking recommended model settings.
The dimension is searched in \{$32, 64, 128, 256, 512$\}. 
The learning rate $\eta$ is tuned within \{$10^{-3}, 5\times10^{-3}, 10^{-2}, 5\times10^{-2}$\} and the coefficient $\lambda$ is tuned among \{$10^{-5}, 10^{-4}, 10^{-3}$\}. 
We initialize and optimize all models with default normal initializer and Adam optimizer~\cite{adam}. 

\vspace{0.05in}

\textbf{Baselines.} All baselines studied in this paper are introduced as:
\label{app:baselines}
\begin{enumerate}[leftmargin=*]
\item \textbf{LSH}~\cite{lsh} is a classical hashing method. LSH is proposed to approximate the similarity search for massive high-dimensional data and we introduce it for Top-N object search by following the adaptation in~\cite{hashgnn}. 

\item \textbf{HashNet}~\cite{hashnet} is a representative deep hashing method that is originally proposed for multimedia retrieval tasks.
Similar to~\cite{hashgnn}, we adapt it for graph data by modifying it with the general graph convolutional network.

\item \textbf{CIGAR}~\cite{kang2019candidate} is a state-of-the-art neural-network-based framework for fast Top-N candidate generation in recommendation. 
CIGAR can be further followed by a full-precision re-ranking algorithm. And we only use its hashing part for fair comparison.

\item \textbf{Hash\_Gumbel} is a variance of \model~with Gumbel-softmax for hash encoding and gradient estimation~\cite{gumbel1,gumbel2}.
Specifically, we first expand each embedding bit to a size-two one-hot encoding. 
Then it utilizes the Gumbel-softmax trick to replace $\sign(\cdot)$ as relaxation for binary hash code generation. 

\item \textbf{HashGNN}~\cite{hashgnn} is the state-of-the-art learning to hash based method with GCN framework. 
We use HashGNN$_{h}$ to denote the vanilla version with \textit{hard encoding} proposed in~\cite{hashgnn}, where each element of user-item embeddings is strictly binarized. 
We use HashGNN$_{s}$ to denote its proposed approximated version.

\item \textbf{NeurCF}~\cite{neurcf} is one representative deep neural network model for collaborative filtering in recommendation. 
% NeurCF models latent features to capture nonlinear feature interactions between users and items.

\item \textbf{NGCF}~\cite{ngcf} is one of the representative graph-based recommender models with collaborative filtering methodology. 

\item \textbf{DGCF}~\cite{dgcf} is a state-of-the-art graph-based model that learns disentangled user intents for better recommendation. 

\item \textbf{LightGCN}~\cite{lightgcn} is another latest state-of-the-art GCN-based recommender model that has been widely evaluated. 

\end{enumerate}
