\subsection{Latent Feature Dispersion}
\label{sec:fd}

To tackle the feature erosion issue, we seek to disperse the embedded features as one effective strategy to hedge the inevitable information loss caused by numerical binarization.
From the perspective of singular value decomposition (SVD), singular values and corresponding singular vectors reconstruct the original matrix;
normally, large singular values can be interpreted to associate with \textit{major feature structures} of the matrix~\cite{wei2018grassmann}. 
Since we want to avoid condensing and gathering informative features in (relatively small) embedding sub-structures, it is natural to bridge the target by working on these singular values.
Hence, based on this intuition, we aim to \textit{normalize singular values for equalizing their respective contributions in constituting latent features}.
To achieve this, Power Normalization~\cite{koniusz2016higher,zhang2022spectral} is one of the solutions that tackle related problems such as feature imbalance~\cite{koniusz2018deeper}.
Inspired by the recent approximation attempt~\cite{yu2020toward}, we now introduce a lightweight feature dispersion technique in graph convolution as follows.


Concretely, let $\emb{I}$ denote the identity matrix, we start from generating a \textit{standard normal random vector} $\emb{p}^{(0)}$$\sim$$\mathcal{N}(\emb{0}, \emb{I})$ where $\emb{p}^{(0)}$ $\in$ $\mathbb{R}^{c}$.
Based on the embedding matrix to conduct feature dispersion, e.g., let $\emb{V}= \emb{V}^{(0)}$, we compute the desired \textbf{dispersing vector} $\emb{p}^{(k)}$ by iteratively performing $\emb{p}^{(k)} = \emb{V}^\mathsf{T}\emb{V}\emb{p}^{(k-1)}$.
The iteration for generating dispersing vectors is independent of the graph convolution iterations\footnote{\scriptsize In our work, we set $K \leq L$ mainly to enable the associated complexity of dispersing vector generation is upper bounded by the graph convolution complexity.}.
We have the projection matrix $\emb{P}$ of $\emb{p}^{(K)}$ via:
\begin{sequation}
\label{eq:projection}
\emb{P} = \frac{\emb{p}^{(K)}\emb{p}^{(K)^\mathsf{T}}}{||\emb{p}^{(K)}||_2^2}.
\end{sequation}%
Then we have the feature-dispersed representation matrix with the hyper-parameter $\epsilon$ $\in$ $(0,1)$ as follows:
\begin{sequation}
\label{eq:disperse}
\widetilde{\emb{V}} = \emb{V}(\emb{I} - \epsilon \emb{P}).
\end{sequation}%
Consequently, integrating the dispersed matrix $\widetilde{\emb{V}}$, we have the \textbf{feature-dispersed} graph convolution as:
\begin{sequation}
\label{eq:fdconv}
\widetilde{\emb{V}}^{(l+1)} = (\emb{D}^{-\frac{1}{2}} \emb{A} \emb{D}^{-\frac{1}{2}} )\widetilde{\emb{V}}^{(l)},  \text{ where } \widetilde{\emb{V}}^{(0)} = \emb{V}^{(0)}(\emb{I} - \epsilon \emb{P}).
\end{sequation}%
Note that we explicitly conduct this feature dispersion operation one time only at the initial step, i.e., {\small $\widetilde{\emb{V}}^{(0)}$}, and, more importantly, such feature dispersion can be diffused via the multi-layer graph convolutions from $0$ to $L$.
Compare to the unprocessed embedding counterpart, e.g., {\small${\emb{V}}^{(l)}$}, embedding matrix {\small$\widetilde{\emb{V}}^{(l)}$} at each layer presents a dispersed feature structure with a \textit{more balanced distribution of singular values in expection}. 
We formally explain this as follows: 
\vspace{-0.05in}
\begin{thm}[\textbf{Feature Dispersion}]
\label{tm:svd}
Let ${\emb{V}}^{(l)} = \emb{U}_1\emb{\Sigma}\emb{U}_2^\mathsf{T}$, where $\emb{U}_1$ and $\emb{U}_2$ are unitary matrices and descending singular value matrix $\emb{\Sigma} = \diag(\sigma_1, \sigma_2, \cdots, \sigma_c)$.  
Then $\mathbb{E}({\small\widetilde{\emb{V}}^{(l)}}) = \emb{U}_1\emb{\Sigma}\emb{\Sigma}_{\mu}\emb{U}_2^\mathsf{T}$ where $\emb{\Sigma}_{\mu} = \diag(\mu_1, \mu_2, \cdots, \mu_c)_{0<\mu_{1 \cdots c}<1}$ is in ascending order.
\end{thm}

% \begin{figure}[tp]
% \hspace{-0.5cm}
% \begin{minipage}{0.5\textwidth}
% \includegraphics[width=3.5in]{figs/fd}
% \end{minipage} 
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
% \caption{An example matrix spectrum on Dianping dataset (singular values are in the descending order).}
% \label{fig:svd}
% \end{figure}
\vspace{-0.05in}
Intuitively, given the same orthonormal bases, compared to {\footnotesize$\emb{V}^{(l)}$}, it is harder in expectation to reconstruct {\footnotesize$\widetilde{\emb{V}}^{(l)}$} with informative features being dispersed out in larger matrix sub-structures.
This eventually provides the functionality to hedge the information loss in numerical binarization. 
% We attach the theorem proof in~\cref{sec:discuss} and evaluate the module effectiveness later in~\cref{sec:ablation}.
We attach the theorem proof in Appendix C and evaluate the module effectiveness later in~\cref{sec:ablation}.

