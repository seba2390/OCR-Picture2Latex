\section{Preliminaries and Problem Formulation}
\label{sec:pre}


\noindent{\textbf{Graph Convolution Network (GCN).}}
The general idea of GCN is to learn node embeddings by \textit{iteratively propagating and aggregating} latent features of node neighbors via the graph topology~\cite{wu2019simplifying,lightgcn,kipf2016semi}:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\boldsymbol{V}_x^{(l)} = AGG\left(\boldsymbol{V}_x^{(l-1)}, \{\boldsymbol{V}_z^{(l-1)}: z \in \mathcal{N}(x)\}\right),
\end{equation}%
where {\small$\emb{V}_x^{(l)} \in \mathbb{R}^d$} denotes node $x$'s embedding after $l$-th iteration of graph convolutions, indexed in the embedding matrix {\small $\emb{V}$}. 
{\small $\mathcal{N}(x)$} is the set of $x$'s neighbors.
Function $AGG(\cdot, \cdot)$ is the information aggregation function, with several implementations in previous work~\cite{kipf2016semi,graphsage,gat,xu2018powerful}, mainly aiming to transform the center node feature and the neighbor features.
In this work, we adopt the graph convolution paradigm from the state-of-the-art model LightGCN~\cite{lightgcn}.


\vspace{0.05in}

\noindent\textbf{Bipartite Graph and Adjacency Matrix.} 
The bipartite graph is denoted as $\mathcal{G}=\{\mathcal{V}_1, \mathcal{V}_2, \mathcal{E}\}$, where $\mathcal{V}_1$ and  $\mathcal{V}_2$ are two \textit{disjoint} node sets and $\mathcal{E}$ is the set of edges between nodes in $\mathcal{V}_1$ and $\mathcal{V}_2$.
We can use $\emb{Y} \in \mathbb{R}^{|\mathcal{V}_1|\times |\mathcal{V}_2}|$ to indicate the edge transactions, where 1-valued entries, i.e., $\emb{Y}_{x,y}=1$, indicate there is an observed edge between nodes $x\in \mathcal{V}_1$ and $y \in \mathcal{V}_2$, otherwise $\emb{Y}_{x,y}=0$.
Then the adjacency matrix $\emb{A}$ of the whole graph can be defined as:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\emb{A} = 
\begin{bmatrix}
\emb{0} & \emb{Y} \\
\emb{Y}^T & \emb{0}
\end{bmatrix}.
\end{equation}%


% \vspace{0.05in}


\noindent\textbf{Problem Formulation.}
Give a bipartite graph $\mathcal{G}=\{\mathcal{V}_1, \mathcal{V}_2, \mathcal{E}\}$ and its adjacency matrix $\emb{A}$, we devote to learn a hashing function:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
F(\emb{A} | \Theta) \rightarrow \mathcal{Q},
\end{equation}
where $\Theta$ is the set of all learnable parameters. 
% $F$ maps nodes into the $d$-dimensional hamming space, i.e., $\{-1, 1\}^d$. 
Given two nodes in the bipartite graph, e.g., $x\in \mathcal{V}_1$ and $y \in \mathcal{V}_2$, their hash codes are $\mathcal{Q}_x$ and $\mathcal{Q}_y$.
Then the probability of edge existence $\widehat{\emb{Y}}_{x,y}$ between nodes $x\in \mathcal{V}_1$ and $y \in \mathcal{V}_2$ can be effectively and efficiently measured by the hash codes $\mathcal{Q}_x$ and  $\mathcal{Q}_y$, i.e., $\widehat{\emb{Y}}_{x,y}$ = $f(\mathcal{Q}_x, \mathcal{Q}_y)$ where $f$ is a score function.
% $f$ is a certain score function that scales in the value range of (0,1).
Intuitively, the larger value $\widehat{\emb{Y}}_{x,y}$ is, the more likely $x$ and $y$ are matched, i.e., an edge between $x$ and $y$ exists.
% Explanations of key notations used in this paper are attached in Appendix~\ref{app:notation_and_code}.
Explanations of key notations used in this paper are attached in Appendix B.








