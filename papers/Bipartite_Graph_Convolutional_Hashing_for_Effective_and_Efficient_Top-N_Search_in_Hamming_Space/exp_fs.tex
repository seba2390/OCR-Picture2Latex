\subsection{\textbf{Study of Fourier Gradient Estimation (RQ5)}}
\label{sec:fs_exp}
We take our largest dataset Dianping for illustration and the analysis can be generally popularized to the other datasets. 

{\textbf{Effect of Decomposition Term $n$.}}
We vary the decomposition term $n$ from 1 to 16.
As shown in Figure~\ref{fig:fs_n}, we have two observations:
(1) Different decomposition terms will surely affect the final retrieval quality, as theoretically, the larger $n$ increases, the more accurate gradients can be estimated.
However, in practice, too large values of $n$ may introduce the overfitting risk, which implies that keeping a moderate $n$, e.g., $n$=4 in Figure~\ref{fig:fs_n}(a), can already maximize the model performance.
(2) By varying $n$ from 1 to 16, the training time per iteration of \model~ slowly increases. 
This generally coincides with our complexity analysis in Appendix C, in which the majority of training cost lies in our feature dispersion and graph convolutional hashing, as $O(\frac{2cs(K+L)|\mathcal{E}|^2}{B}) \gg O(snd|\mathcal{E}|)$.

{\textbf{Comparison with Other Gradient Estimators.}}
We include several recent gradient estimators, i.e., \textit{Tanh-like}~\cite{qin2020forward,gong2019differentiable}, \textit{SignSwish}~\cite{darabi2018bnn}, \textit{Sigmoid}~\cite{sigmoid}, and \textit{projected-based estimator}~\cite{RBCN} (denoted as PBE).
(1) The results summarized in Table~\ref{tab:estimator} well demonstrate the superiority of our proposed Fourier Series decomposition to $\sign(\cdot)$ function in gradient estimation.
As we have briefly explained, most existing estimators employ the \textit{visually similar} function approximation to $\sign(\cdot)$; compared to STE, they generally provide better gradient estimation.
(2) However, for those bipartite graphs with heavy sparsity, e.g., Gowalla (0.00084) and AMZ-Book (0.00062), graph-based models may hardly collect enough structural information for effective hash codes training.
Based on the limited training samples, these \textit{theoretically irrelevant} estimators may not effectively rectify the optimization deviation, and thus present a recognizable performance gap against our proposed Fourier serialized estimator.




\begin{figure}[t]
% \hspace{-0.2in}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=3.3in]{figs/fs_n.pdf}
\end{minipage} 
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
\vspace{-0.1in}
\caption{Fourier Series decomposition term $n$ in \model.}
\label{fig:fs_n}
\end{figure}



\begin{table}[t]
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
\centering
\scriptsize
\caption{Gradient estimator comparison on Recall@20.}
\vspace{-0.1in}
\label{tab:estimator}
\setlength{\tabcolsep}{0.7mm}{
\begin{tabular}{c |c | c | c | c | c| c}
\toprule
    ~            & Movie & Gowalla & Pinterest & Yelp2018 & AMZ-Book & Dianping \\
\midrule
  STE         & {20.93 ({\drop \tiny -8.44\%})}   & {14.85({\drop \tiny -11.24\%})}   & {12.35 ({\drop \tiny -3.36\%})}   & {5.24 ({\drop \tiny -4.90\%})}   & {3.12({\drop \tiny -10.34\%})}   & {10.34 ({\drop \tiny -3.00\%})}  \\
  Tanh         & {21.75 ({\drop \tiny -4.86\%})}   & {15.06 ({\drop \tiny -9.98\%})}   & {12.36 ({\drop \tiny -3.29\%})} & {5.43 ({\drop \tiny -1.45\%})}   & {3.21 ({\drop \tiny -7.76\%})}     & {10.41 ({\drop \tiny -2.34\%})}  \\
  SignSwish         & {22.13 ({\drop \tiny -3.19\%})}   & {15.62 ({\drop \tiny -6.63\%})}   & {12.44 ({\drop \tiny -2.66\%})}   & {5.50 ({\drop \tiny -0.18\%})}   & {3.34 ({\drop \tiny -4.02\%})}   & {10.43 ({\drop \tiny -2.16\%})}  \\
  Sigmoid         & {22.08 ({\drop \tiny -3.41\%})}   & {15.21 ({\drop \tiny -9.09\%})}   & {12.52 ({\drop \tiny -2.03\%})}   & {5.53 ({\drop \tiny +0.03\%})}   & {3.18 ({\drop \tiny -8.62\%})}   & {10.38 ({\drop \tiny -2.63\%})}  \\
  PBE         & {21.68 ({\drop \tiny -5.16\%})}   & {15.05({\drop \tiny -10.04\%})}   & {12.32 ({\drop \tiny -3.60\%})}   & {5.35 ({\drop \tiny -2.90\%})}   & {3.13({\drop \tiny -10.06\%})}   & {10.47 ({\drop \tiny -1.78\%})}  \\
\midrule[0.1pt]
  \model            & \textbf{22.86}  & \textbf{16.73}   & \textbf{12.78}   & \textbf{5.51}   & \textbf{3.48}   & \textbf{10.66}   \\
\bottomrule
\end{tabular}}
\end{table}


