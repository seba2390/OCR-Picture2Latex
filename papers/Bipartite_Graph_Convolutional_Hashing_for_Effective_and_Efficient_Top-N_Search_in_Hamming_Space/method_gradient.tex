\subsection{Fourier Serialized Gradient Estimation}
\label{sec:ge}

To provide the accordant gradient estimation for hash function $\sign(\cdot)$, we approximate it by introducing its Fourier Series decomposition in the frequency domain. 
Specifically, $\sign(\cdot)$ can be viewed as a special case of the periodical Square Wave Function $t(x)$ within the length $2H$, i.e., $\sign(\phi) = t(\phi)$, $|\phi| < H$.  
Since $t(x)$ can be decomposed in Fourier Series, we shall have: 
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\sign(\phi) = \frac{4}{\pi}\sum_{i=1,3,5,\cdots}^{+\infty}\frac{1}{i}\sin(\frac{\pi i\phi}{H}), {\rm \ \ where \ \ } |\phi| < H.
\end{equation}


% \begin{figure}[tp]
% % \hspace{-0.6cm}
% \begin{minipage}{0.5\textwidth}
% \includegraphics[width=3.3in]{figs/gradient}
% \end{minipage} 
% % \setlength{\abovecaptionskip}{0.2cm}
% % \setlength{\belowcaptionskip}{0.2cm}
% \vspace{-0.1in}
% \caption{Gradient estimation of $\sign(\cdot)$ with Fourier Series.}
% \label{fig:gradient}
% \end{figure}

Fourier Series decomposition of $\sign(\cdot)$ with infinite terms is a lossless transformation~\cite{rust2013convergence}.
Thus, as shown in Figure~\ref{fig:model}(c), we can set the finite expanding term $n$ to obtain its approximation version as follows: 
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
{\sign(\phi)} \doteq \frac{4}{\pi}\sum_{i=1,3,5,\cdots}^{n}\frac{1}{i}\sin(\frac{\pi i\phi}{H}).  \\
\end{equation}
The corresponding derivatives can be derived accordingly as:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:gradient}
\frac{\partial{{\sign(\phi)}}}{\partial \phi}   \doteq \frac{4}{H} \sum_{i=1,3,5,\cdots}^{n} \cos(\frac{\pi i\phi}{H}). 
\end{equation}


Different from other gradient estimators such as tanh-alike~\cite{gong2019differentiable,qin2020forward} and SignSwish~\cite{darabi2018bnn}, approximating $\sign(\cdot)$ function with its Fourier Series will not corrupt the main direction of factual gradients in model optimization~\cite{xu2021learning}.
This is beneficial to bridge a coordinated transformation from the continuous values to its corresponding binarization for node representations, which significantly retains the discriminability of binarized representations and produces better retrieval accuracy accordingly.
We present this performance comparison in~\cref{sec:fs_exp} of experiments. 
To summarize, as shown in Equation~(\ref{eq:formal_grad}), to learn and optimize the binarized embeddings for graph nodes, we apply the strict $\sign(\cdot)$ function for forward propagation and estimate the gradients $\frac{\partial\sign(\phi)}{\partial \phi}$ for backward propagation.
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:formal_grad}
\left\{ 
\begin{aligned}
& \boldsymbol{Q}^{(l)} = \sign(\phi),  &\text{Forward propagation.} \\
& \frac{\partial \boldsymbol{Q}^{(l)}}{\partial \phi} \doteq \frac{4}{H} \sum_{i=1,3,5,\cdots}^{n} \cos(\frac{\pi i\phi}{H}). & \text{Backward propagation.}
\end{aligned}
\right.
\end{equation}