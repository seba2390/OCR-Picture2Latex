\subsection{Adaptive Graph Convolutional Hashing}
\label{sec:hashing}
One feasible solution for increasing expressivity and smoothing loss landscapes is to include the \textit{relaxation strategy}.
Hence, apart from the topology-aware embedding binarization with $\sign(\cdot)$:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:hashing}
\emb{Q}_{x}^{(l)} = \sign(\widetilde{\emb{V}}_{x}^{(l)}),
\end{equation}
our model \model~additionally computes a layer-wise positive rescaling factor for each node, e.g., $\alpha_x^{(l)} \in \mathbb{R}^+$, such that $\widetilde{\emb{V}}^{(l)}_x \approx$ $\alpha_x^{(l)} \emb{Q}^{(l)}_x$.
In this work, we introduce a simple but effective approach to directly calculate the rescaling factors as follows: 
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:rescale}
\alpha_x^{(l)} = \frac{1}{d} ||\widetilde{\emb{{V}}}_x^{(l)}||_1.
\end{equation}
Instead of setting these factors as learnable, such deterministic computation substantially prunes the parameter search space while attaining the adaptive approximation functionality for different graph nodes. 
We demonstrate this in~\cref{sec:ablation} of experiments. 



After $L$ iterations of feature propagation and hashing, we obtain the table of \textbf{adaptive hash codes} $\mathcal{Q} = \{\emb{\alpha}, \emb{Q}\}$, where $\emb{\alpha} \in \mathbb{R}^{(|\mathcal{V}_1|+|\mathcal{V}_2|)\times (L+1)}$ and $\emb{Q} \in \mathbb{R}^{(|\mathcal{V}_1|+|\mathcal{V}_2|)\times d}\}$.
For each node $x$, its corresponding hash codes are indexed and assembled:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\emb{\alpha}_x = \alpha_x^{(0)} || \alpha_x^{(1)} || \cdots || \alpha_x^{(L)}, \text{  and  } \emb{{Q}}_x = \emb{Q}_x^{(0)} || \emb{Q}_x^{(1)} || \cdots || \emb{Q}_x^{(L)}.
\end{equation}
Intuitively, the hash code table $\mathcal{Q}$ represents the bipartite structural information that is propagated back and forth at different iteration steps $l$, i.e., from $0$ to the maximum step $L$.
It not only tracks the intermediate knowledge hashed for all graph nodes, but also maintains the value approximation to their original continuous embeddings, e.g., {\footnotesize $\widetilde{\emb{{V}}}_x^{(l)}$}.
% In addition, with the slightly more space cost (complexity analysis in~\cref{sec:discuss}), such detached hash encoding approach still supports the bitwise operations (~\cref{sec:score}) for accelerating inference and matching. 
In addition, with the slightly more space cost (complexity analysis in Appendix C, such detached hash encoding approach still supports the bitwise operations (~\cref{sec:score}) for accelerating inference and matching. 



