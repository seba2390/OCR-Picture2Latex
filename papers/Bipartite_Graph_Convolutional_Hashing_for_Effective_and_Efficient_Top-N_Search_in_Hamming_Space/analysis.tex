\section{Theoretical Proofs and Analyses}
\label{sec:discuss}
\addtocounter{thm}{-2}

\input{fd_analysis}



\begin{thm}[\textbf{Hamming Distance Matching}]
Given two hash codes, we have $(\alpha_x\emb{Q}_u)^\mathsf{T} \cdot (\alpha_y\emb{Q}_y)$ $=$ $\alpha_x\alpha_y$ $(d - 2D_{H}(\emb{Q}_x, \emb{Q}_y))$.
\end{thm} 

\begin{proof}
\begin{equation}
\begin{aligned}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\emb{Q}_x^\mathsf{T} \cdot \emb{Q}_y &= \big|\{ i|(\emb{Q}_{x})_i = (\emb{Q}_{y})_i = 1\}\big| +  \big|\{ i|(\emb{Q}_{x})_i = (\emb{Q}_{y})_i = -1\}\big| \\ 
& -  \big|\{ i|(\emb{Q}_{x})_i \neq (\emb{Q}_{y})_i\}\big|\\
& = d - 2 \cdot \big|\{ i|(\emb{Q}_{x})_i \neq (\emb{Q}_{y})_i\}\big|  = \underline{d - 2D_{H}(\emb{Q}_x, \emb{Q}_y))},\\
\end{aligned}
\end{equation}%
which completes the proof.
\end{proof}


\textbf{Training time complexity.}
As shown in Table~\ref{tab:train_time}, $|\mathcal{E}|$, $B$, $s$, and $n$ are the edge number, batch size, numbers of train iterations and Fourier Series decomposition terms.
(1) The time complexity of the graph normalization, i.e., $\emb{D}^{-\frac{1}{2}} \emb{A} \emb{D}^{-\frac{1}{2}}$, is $O(2|\mathcal{E}|)$.
(2) Before the graph convolution, we first conduct the feature dispersion only for the initial node embeddings, e.g., $\emb{V}_x^{(0)}$, which takes $O(\frac{2csK|\mathcal{E}|^2}{B})$ complexity.
In our experiment, hyper-parameter $K \leq 3$.
(3) In graph convolution, the time complexity is $O(\frac{2csL|\mathcal{E}|^2}{B})$, where $L \leq 4$ is a common setting~\cite{lightgcn,ngcf,kipf2016semi,graphsage} to avoid \textit{over-smoothing}~\cite{li2019deepgcns}.
(4) As for the loss computation, \model~takes $O\big(2sc|\mathcal{E}|\big)$ to compute $\mathcal{L}_{rec}$ and $O\big(2sd|\mathcal{E}|\big)$ for $\mathcal{L}_{bpr}$.
(5) Lastly, \model~takes $O(snd|\mathcal{E}|)$ to estimate the gradients for the $d$-dimension hash codes.
Thus, thee total complexity in total is quadratic to the graph edge numbers, i.e., $|\mathcal{E}|$, which is common in GCN frameworks.
 % and actually acceptable for large bipartite graphs, which may dispel the concerns of the realistic training cost.


 \begin{table}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{0.2cm}
\centering
\notsotiny
\caption{Traing time complexity.}
\vspace{-0.05in}
\label{tab:train_time}
\setlength{\tabcolsep}{2mm}{
  \begin{tabular}{c | c | c | c | c }
\toprule
    {Graph Norm.}  & {Feat. Disp.}  & {Conv. \& Hash.}   & { Loss Comp.}  &{Grad. Est.} \\
\midrule[0.1pt]
    { $O(2|\mathcal{E}|)$} & {$O(\frac{2csK|\mathcal{E}|^2}{B})$} & {$O(\frac{2csL|\mathcal{E}|^2}{B})$} & { $O\big(2s(c$+$d)|\mathcal{E}|\big)$} & { $O(snd|\mathcal{E}|)$} \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{0.2cm}
\centering
\footnotesize
\caption{Complexity of space and computation.}
\vspace{-0.05in}
\label{tab:prediction}
\setlength{\tabcolsep}{2.2mm}{
\begin{tabular}{c | c | c c}
\toprule
  ~          & {\footnotesize Space cost}   &  {\footnotesize \#FLOP} & {\footnotesize \#BOP}       \\
\midrule
\midrule
 {\scriptsize float32-based}      & {\scriptsize $32|\mathcal{V}_1 \cup \mathcal{V}_2|d$}       &   {\scriptsize $O\big(|\mathcal{V}_1| \cdot |\mathcal{V}_2| d\big)$}      &   {-}         \\
\midrule[0.1pt]
{\scriptsize \model}       & {\scriptsize $|\mathcal{V}_1 \cup \mathcal{V}_2| (d+32(L+1))$}    & {\scriptsize $O\big(4|\mathcal{V}_1| \cdot |\mathcal{V}_2| \big)$}            & {\scriptsize $O\big(|\mathcal{V}_1| \cdot |\mathcal{V}_2| d\big)$}    \\
\bottomrule
\end{tabular}}
\end{table}


% \vspace{0.05in}

% \subsubsection{\textbf{Hash Codes Space Cost.}}
\textbf{Hash codes space cost.}
As shown in Table~\ref{tab:prediction}, the total space cost of hash codes is {\small $O(|\mathcal{V}_1 \cup \mathcal{V}_2| (d+32(L+1)))$} bits, supposing that we use float32 for those rescaling factors in $L+1$ iterations.
Compared to the continuous embedding size, i.e., $32|\mathcal{V}_1 \cup \mathcal{V}_2|d$ bits, the theoretical size reduction ratio thus is:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:space}
ratio = \frac{32|\mathcal{V}_1 \cup \mathcal{V}_2|d}{|\mathcal{V}_1 \cup \mathcal{V}_2| (d+32(L+1))} = \frac{32d}{d+32(L+1)}.
\end{equation}
As we just explained, stacking too many iteration layers will incurring performance detriment~\cite{li2019deepgcns}. Hence, if $L\leq4$ and $d=1024$, \model~can achieve considerable size compression. 

\vspace{0.05in}

{\textbf{Online matching.}}
The original score formulation in Equation~(\ref{eq:inner_score}) contains $d$ floating-point operations (\#FLOPs).
As shown in Table~\ref{tab:prediction}, using Hamming distance matching can convert the most of floating-point arithmetics to binary operations (\#BOPs), with slightly more \#FLOPs for scalar computations, i.e., $4\ll d$.




