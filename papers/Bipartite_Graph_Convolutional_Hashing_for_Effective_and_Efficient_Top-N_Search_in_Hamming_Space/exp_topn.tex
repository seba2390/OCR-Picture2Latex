
\begin{table*}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{0.2cm}
\centering
\small
  \caption{Results of Recall@20 and NDCG@20 in Top-1000 retrieval: (1) ``R'' and ``N'' denote the Recall and NGCG; (2) the bold indicate \model~and the underline represents the best-performing models; (3) Mark \textbf{$^*$} denotes scenarios where Wilcoxon signed-rank tests indicate statistically significant improvements over the second-best models over 95\% confidence level.}
  % \vspace{-0.05in}
  \label{tab:topn}
  \setlength{\tabcolsep}{1mm}{
  % \addvbuffer[1pt 1pt]{
  \begin{tabular}{c|c c| c c| c c| c c| c c|c c} 
    \toprule
    Dataset & \multicolumn{2}{c|}{MovieLens (\%)} & \multicolumn{2}{c|}{Gowalla (\%)} & \multicolumn{2}{c|}{Pinterest (\%)} & \multicolumn{2}{c|}{Yelp2018 (\%)}  & \multicolumn{2}{c|}{AMZ-Book (\%)} & \multicolumn{2}{c}{Dianping (\%)} \\
    Metric & R@20$_{1000}$ & N@20$_{1000}$  & R@20$_{1000}$ & N@20$_{1000}$ & R@20$_{1000}$ & N@20$_{1000}$ & R@20$_{1000}$ & N@20$_{1000}$ & R@20$_{1000}$ & N@20$_{1000}$ & R@20$_{1000}$ & N@20$_{1000}$  \\ 
    \midrule[0.01pt]
    LSH                 & {11.38} & {25.87}  & {8.14} & {12.23}  & {7.88} & {6.71}  & {2.91} & {4.35}  & {2.41} & {2.34}  & {5.85} & {5.84}  \\
    HashNet             & {15.43} & {32.23}  & {11.38} & {13.74}  & {10.27} & {7.33}  & {3.37} & {4.41}  & {2.86} & {2.71}  & {6.24} & {5.59}  \\
    CIGAR               & {14.84} & {31.73}  & {11.57} & {14.21}  & {10.34} & {8.53}  & {3.65} & {4.57}  & {3.05} & {3.03}  & {6.91} & {6.03}  \\
    Hash\_Gumbel            & {16.62}  & {32.48} & {12.26}  & {14.68} & {10.53}  & {8.74} & {3.85}  & {5.12} & {2.69}  & {3.24} & {8.29}  & {6.43} \\
    HashGNN$_{\rm h}$   & {14.21} & {31.83}  & {11.63} & {14.21}  & {10.15} & {8.67}  & {3.77} & {5.04}  & {3.09} & {3.15}  & {8.34} & {6.68}  \\
    HashGNN$_{\rm s}$   & \underline{19.87} & \underline{33.21}  & \underline{13.45} & \underline{14.87}  & \underline{12.38} & \underline{9.11}  & \underline{4.86} & \underline{5.34}  & \underline{3.34} & \underline{3.45}  & \underline{9.57} & \underline{7.13}  \\

    \midrule[0.01pt]
    \textbf{\model} &\textbf{22.86$^*$} &\textbf{36.26$^*$} &\textbf{16.73$^*$} &\textbf{16.48$^*$} &\textbf{12.78$^*$} &\textbf{9.42$^*$} &\textbf{5.51$^*$} &\textbf{5.84$^*$} &\textbf{3.48$^*$} &\textbf{3.92$^*$} &\textbf{10.66$^*$} &\textbf{7.63$^*$}  \\
    \% Gain       &{ 15.05\%} &{ 9.18\%}    &{ 24.39\%} &{ 10.83\%}  &{ 3.23\%} &{ 3.40\%}  &{ 13.37\%} &{ 9.36\%}  &{ 4.19\%} &{ 13.62\%}  &{ 11.39\%} &{ 7.01\%} \\

   \bottomrule
  \end{tabular}}
\end{table*}


\begin{figure*}[t]
\begin{minipage}{1\textwidth}
\vspace{-0.05in}
\includegraphics[width=7.1in]{figs/topn.pdf}
\end{minipage} 
\vspace{-0.1in}
\caption{Top-N retrieval quality with N in \{20, 50, 100, 200, 500, 1000\} (best view in color).}
\label{fig:topn}
% \vspace{-0.1in}
\end{figure*}



\subsection{Top-N Hamming Space Query (RQ1)}
\label{sec:exp_topn}

To evaluate \textbf{fine-to-coarse} Top-N ranking capability, we set N=1000.
We first report the results of Recall@20$_{1000}$ and NDCG@20$_{1000}$\footnote{We then use simple notation Recall@20, NDCG@20 if there is no ambiguity caused.} in Top-1000 search in Table~\ref{tab:topn} and then plot the holistic Recall and NDCG metric curves of \{20, 50, 100, 200, 500, 1000\} of Top-1000 in Figure~\ref{fig:topn}.
We set convolution iteration number as 2 and embedding dimension as 256 for \model~and baselines for fair comparison.
% We have the following two major observations:
\begin{itemize}[leftmargin=*]

\item \textbf{The results demonstrate the superiority of \model~model over prior hashing-based models.}
(1) As shown in Table~\ref{tab:topn}, the state-of-the-art model, i.e., HashGNN, works better than traditional hashing-based baselines, e.g., LSH, HashNet, CIGAR. 
This indicates that, compared to graph-based models, a direct adaptation of conventional (i.e., non-graph-based) hashing methods may be hard to achieve comparable performance, mainly because of the effectiveness of \textit{graph convolutional} architecture in capturing latent information within the bipartite graph topology for hash encoding preparation.
(2) Owing to our proposed model components, e.g., \textit{adaptive graph convolutional hashing}, \model~consistently outperforms HashGNN over all datasets, by 3.23\%$\sim$24.39\%, and 3.40\%$\sim$ 13.62\% \textit{w.r.t.} Recall@20 and NDCG@20, respectively.
(3) Furthermore, we conduct the Wilcoxon signed-rank tests at \model.
The results verify that all \model~improvements over the second-best model are statistically significant over 95\% confidence level.
(4) To explain these, our proposed topology-aware graph convolutional hashing 
approach effectively enriches the graph node embeddings.
Our proposed feature dispersion further alleviates the feature erosion issue caused by numerical binarization. 
Last but not least, our proposed Fourier serialized gradient estimation is also vital to provide accurate gradients for model optimization.
We conduct the ablation study later in~\cref{sec:ablation}.


\item \textbf{By varying N from 20 to 1000, \model~consistently shows competitive performance compared to the baselines.}
While Recall@N indicates the fraction of relevant objects in Top-N retrieval, NDCG@N measures the ranking capability for relative orders.
As shown in Figure~\ref{fig:topn}:
(1) Compared to the approximated version of HashGNN, i.e., HashGNN$_{s}$, \model~generally obtains stable and significant improvements of both Recall and NDCG metrics over all six benchmarks with N from 20 to 1000.
(2) Apart from the higher retrieval quality, another advantage of \model~over HashGNN$_{s}$ is that it still supports bitwise operations, i.e., hamming distance matching, for inference acceleration. 
This is because, to improve the prediction accuracy, HashGNN$_{s}$ adopts a Bernoulli random variable to provide the probability of replacing the certain digits in the hash codes with the original continuous values, which thus disables the bitwise computation.
As we present in~\cref{sec:resource}, \model~achieves over 8$\times$ inference acceleration over HashGNN$_{s}$, which is particularly promising for query-based online matching and retrieval applications.
\end{itemize}

