\section{\textbf{Related Work}}
\label{sec:work}

{\textbf{Graph convolution network (GCN)}.}
Early work studies the graph convolutions mainly on the \textit{spectral domain}, such as Laplacian eigen-decomposition~\cite{bruna2013spectral} and Chebyshev polynomials~\cite{defferrard2016convolutional}.
One major issue is that these models are usually computationally expensive. 
To tackle this problem, \textit{spatial-based} GCN models are proposed to re-define the graph convolution operations by aggregating the embeddings of neighbors to refine and update the target node's embedding.
Due to its scalability to large graphs, spatial-based GCN models are widely used in various applications~\cite{lightgcn,ngcf,graphsage}. 
For example, to capture high-order structural information, NGCF~\cite{ngcf} and LightGCN~\cite{lightgcn} learn the collaborative filtering signals on bipartite interaction graphs for recommendation.
Despite the effectiveness in embedding latent features for graph nodes, they usually suffer from inference inefficiency due to the high computational cost of similarity calculation between continuous embeddings~\cite{hashgnn}.
To address this issue, \textit{learning to hash} provides the feasibility.


{\textbf{Learning to hash}.}
Learning to hash models are promising to achieve computation acceleration and storage reduction for general information retrieval and processing tasks~\cite{hu2021semi,hu2020selfore,li2020unsupervised,lightgcn,gao2020discern,chen2022repo4qa,ma2019hierarchical}.
More than reducing conflicts~\cite{kraska2018case},  similarity-preserving hashing maps high-dimensional dense vectors to a low-dimensional Hamming space for efficiently processing downstream tasks.
A representative model is Locality Sensitive Hashing (LSH)~\cite{lsh} that uses random projections as the hash functions.
Recent work focuses on integrating the deep neural network architectures for model improvement~\cite{wang2017survey}.
They inspire a series of follow-up work for various tasks, such as fast retrieval of images~\cite{qin2020forward,lin2017towards,hashnet}, documents~\cite{li2014two,chen2022effective}, categorical information~\cite{kang2021learning}, e-commerce products~\cite{zhang2017discrete,chen2022learning}.

To leverage hashing techniques with GCNs, the state-of-the-art work HashGNN~\cite{hashgnn} investigates \text{learning to hash} for online matching and recommendation.
Specifically, HashGNN consectively combines the GraphSage~\cite{graphsage} as the embedding encoder and learning to hash method to get the corresponding binary encodings afterwards.
Its hash encoding process only proceeds at the end of multi-layer graph convolutions, i.e., using the aggregated output of GraphSage for representation binarization. 
However, this fails to capture intermediate semantics from nodes' different layers of receptive fields~\cite{kipf2016semi}.
The other issue of HashGNN is using \textit{Straight-Through Estimator (STE)}~\cite{bengio2013estimating} to assume all gradients of $\sign(\cdot)$ as 1 in backpropagation.
However, the integral of 1 is a certain linear function other than the $\sign(\cdot)$, whereas this may lead to inconsistent optimization directions in the model training.
To address these issues, our model \model~ is proposed with effectiveness justification in~\cref{sec:exp}.


