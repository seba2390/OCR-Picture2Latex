\subsection{\textbf{Resource Consumption Analysis (RQ4)}}
\label{sec:resource}
Due to the various value ranges over all six datasets, we compactly report the value ratios of \model~over the state-of-the-art hashing-based model HashGNN$_s$ in Figure~\ref{fig:tradeoff}.

\begin{figure}[h]
% \hspace{-0.3in}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=3.4in]{figs/ratio.pdf}
\end{minipage} 
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
\vspace{-0.15in}
\caption{Resource consumption ratios.}
\label{fig:tradeoff}
\end{figure}

{\textbf{Model Training Time Cost.}}
As indicated by the metric ``\textit{T-Time}'' in Figure~\ref{fig:tradeoff}, we notice that training HashGNN$_s$ is more time-consuming than our proposed model.
The main reason is that HashGNN adopts the early GCN framework~\cite{graphsage} as the model backbone, while our model follows the latest framework~\cite{lightgcn} to remove operations, e.g., self-connection, feature transformation, and nonlinear activation.
In addition, on the two largest datasets AMZ-Book and Dianping, the training cost ratio further increases to around 5$\sim$6 times.
This is because we have to decrease the batch size of HashGNN$_s$ for tractable training process.

{\textbf{Online Inference Time Cost.}}
We randomly generate 1,000 queries and evaluate the computation time cost.
To present a fair comparison, we disable all parallel arithmetic techniques (e.g., MKL, BLAS) by using the open-source toolkit\footnote{\url{https://www.lfd.uci.edu/~gohlke/pythonlibs/}}.
Indicated by ``\textit{I-Time}'' in Figure~\ref{fig:tradeoff}, our model with Hamming distance matching generally achieves over 8$\times$ computation acceleration over HashGNN$_s$ on all datasets.
This is because, as we have explained in~\cref{sec:exp_topn}, HashGNN$_s$ randomly replaces the hash codes with their original continuous embeddings for relaxation and adopts floating-point arithmetics to pursue performance improvement while sacrificing the computation acceleration from the bitwise operations.



{\textbf{Hash Codes Memory Footprint.}}
Binarized embeddings can largely reduce memory space consumption.
Compared to the state-of-the-art hashing-based model HashGNN$_s$, our \model~further achieves about 9$\times$ of memory space reduction for the hash codes.
As we have just explained, since HashGNN$_s$ interprets hash codes with random real-value digits, it thus requires additional cost to distinguish binary digits from full-precision ones. 
On the contrary, \model~ separates the storage of binarized encoding parts and corresponding rescaling factors, thus providing the advantage for space overhead optimization.




