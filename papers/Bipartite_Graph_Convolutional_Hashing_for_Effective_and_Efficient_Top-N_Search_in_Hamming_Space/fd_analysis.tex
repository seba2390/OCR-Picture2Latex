\begin{thm}[\textbf{Feature Dispersion}]
Let ${\emb{V}}^{(l)} = \emb{U}_1\emb{\Sigma}\emb{U}_2^\mathsf{T}$, where $\emb{U}_1$ and $\emb{U}_2$ are unitary matrices and descending singular value matrix $\emb{\Sigma} = \diag(\sigma_1, \sigma_2, \cdots, \sigma_c)$.  
Then $\mathbb{E}({\small\widetilde{\emb{V}}^{(l)}}) = \emb{U}_1\emb{\Sigma}\emb{\Sigma}_{\mu}\emb{U}_2^\mathsf{T}$ where $\emb{\Sigma}_{\mu} = \diag(\mu_1, \mu_2, \cdots, \mu_c)_{0<\mu_{1 \cdots c}<1}$ is in ascending order.
\end{thm}


\begin{proof}
We focus on comparing ({\small$\widetilde{\emb{V}}^{(0)}$, ${\emb{V}}^{(0)}$}), which can be easily popularized to any convolution layer, i.e., ({\small$\widetilde{\emb{V}}^{(l)}$, ${\emb{V}}^{(l)}$}). 
Conducting SVD decomposition on {\small${\emb{V}}^{(0)}$}, we have ${\emb{V}}^{(0)} = \emb{U}_1\emb{\Sigma}\emb{U}_2^\mathsf{T}$, where {\small$\emb{U}_1$} and {\small$\emb{U}_2$} are unitary matrices of singular vectors.
Then following {\small$\emb{p}^{(k)} = {\emb{V}^{(0)}}^\mathsf{T}\emb{V}^{(0)}\emb{p}^{(k-1)}$}, we shall have {\small $\emb{p}^{(k)} = ({\emb{V}^{(0)}}^\mathsf{T}\emb{V}^{(0)})^k\emb{p}^{(0)}$}.
Replacing {\small${\emb{V}}^{(0)}$} with its SVD decomposition, we get the following equation:
\begin{sequation}
\emb{p}^{(k)} = (\emb{U}_2\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T})\emb{p}^{(0)}.
\end{sequation}%
Then we transform the projection matrix in Equation~(\ref{eq:projection}) as follows:
\begin{sequation}
\begin{aligned}
\emb{P} = \frac{\emb{p}^{(k)} {\emb{p}^{(k)}}^\mathsf{T}}{{\emb{p}^{(k)}}^\mathsf{T} \emb{p}^{(k)}} & = \frac{(\emb{U}_2\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T})\emb{p}^{(0)} {\emb{p}^{(0)}}^\mathsf{T} (\emb{U}_2\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T})}
{{\emb{p}^{(0)}}^\mathsf{T} (\emb{U}_2\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T}) (\emb{U}_2\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T})\emb{p}^{(0)} } \\
 & = \emb{U}_2\emb{\Sigma}^{2k} \frac{\emb{U}_2^\mathsf{T}\emb{p}^{(0)} {\emb{p}^{(0)}}^\mathsf{T} \emb{U}_2} 
 {{\emb{p}^{(0)}}^\mathsf{T} \emb{U}_2\emb{\Sigma}^{4k}\emb{U}_2^\mathsf{T}\emb{p}^{(0)} }\emb{\Sigma}^{2k}\emb{U}_2^\mathsf{T}.
\end{aligned}
\end{sequation}%
Let $\emb{t} = \emb{U}_2^\mathsf{T}\emb{p}^{(0)}$, we can further simplify the above equation to:
\begin{sequation}
\emb{P} = \emb{U}_2 \emb{\Sigma}^{2k} \frac{ \emb{t} \emb{t}^\mathsf{T}}{\emb{t}^\mathsf{T}\emb{\Sigma}^{4k} \emb{t} }\emb{\Sigma}^{2k} \emb{U}_2^\mathsf{T}, 
 \text{ where scalar }  \emb{t}^\mathsf{T}\emb{\Sigma}^{4k} \emb{t} = \sum_{j=1}^c t_j^2 \sigma_j^{4k}.
\end{sequation}%
% Here {\small $\emb{t}^\mathsf{T}\emb{\Sigma}^{4k} \emb{t}$} is a scalar as:
% \begin{sequation}
% \emb{t}^\mathsf{T}\emb{\Sigma}^{4k} \emb{t} = \sum_{j=1}^c t_j^2 \sigma_j^{4k}.
% \end{sequation}%
Recalling that {\small $\widetilde{\emb{V}}^{(0)} = \emb{V}^{(0)}(\emb{I} - \epsilon \emb{P})$}, {\small $\mathbb{E}(\widetilde{\emb{V}}^{(0)}) = \emb{V}^{(0)} - \epsilon\cdot\mathbb{E}(\emb{V}^{(0)}\emb{P})$}.
Then we focus on the term {\small $\mathbb{E}(\emb{V}^{(0)}\emb{P})$} as follows:
\begin{sequation}
\mathbb{E}(\emb{V}^{(0)}\emb{P}) = \frac{1}{\emb{t}^\mathsf{T}\emb{\Sigma}^{4k} \emb{t}} \emb{U}_1 \emb{\Sigma}^{2k+1} \cdot \mathbb{E}(\emb{t} \emb{t}^\mathsf{T}) \cdot  \emb{\Sigma}^{2k} \emb{U}_2^\mathsf{T} 
\end{sequation}%
Since {\small$\emb{p}^{(0)}$$\sim$$\mathcal{N}(\emb{0}, \emb{I})$} and {\small $\emb{U}_2$} is a unitary matrix, thus {\small $\emb{t}$$\sim$$\mathcal{N}(\emb{0}, \emb{I})$}. 
This indices that each element of {\small $\emb{t}$}, e.g., {\small $t_j$ $\in$ $\emb{t}$}, is \textit{i.i.d.} random variable. Thus, {\small $\mathbb{E}({t}_j \cdot t_{k})=0$} for {\small$j\neq k$} and {\small $\mathbb{E}(\emb{t}\emb{t}^\mathsf{T})$} is a diagonal matrix, i.e., {\small $\mathbb{E}(\emb{t}\emb{t}^\mathsf{T})=\diag(t_1^2, t_2^2, \cdots, t_c^2)$}.
We then have:
\begin{sequation}
\mathbb{E}(\emb{V}^{(0)}\emb{P}) = \emb{U}_1 \cdot \diag \big(\frac{\sigma_1 t_1^2 \sigma_1^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}},  \cdots,  \frac{\sigma_c  t_c^2\sigma_c^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}}\big) \cdot \emb{U}_2^\mathsf{T}.
\end{sequation}%
Therefore, 
\begin{sequation}
\mathbb{E}(\widetilde{\emb{V}}^{(0)}) = \emb{U}_1 \cdot \diag \big( \sigma_1 - \epsilon \frac{\sigma_1 t_1^2 \sigma_1^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}},  \cdots,  \sigma_c - \epsilon \frac{\sigma_c  t_c^2\sigma_c^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}} \big)  \cdot \emb{U}_2^\mathsf{T}.
\end{sequation}%
Let {\small $\mu_k = 1 - \epsilon \frac{t_k^2\sigma_k^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}}$}, with {\small $\epsilon$ $\in$ $(0,1)$}, obviously, {\small $0<\mu_k<1$}. 
Furthermore, {\small $\forall k_1$ $\geq$ $k_2$}, we have:
\begin{sequation}
\label{eq:increase}
\resizebox{1\linewidth}{!}{$
\displaystyle
\mu_{k_1} - \mu_{k_2} = \epsilon \mathbb{E}(\frac{t_{k_1}^2\sigma_{k_1}^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}} - \frac{t_{k_2}^2\sigma_{k_2}^{4k}}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}}) \geq \epsilon\sigma_{k_1}^{4k} \cdot \mathbb{E} (\frac{t_{k_1}^2 - t_{k_2}^2}{\sum_{j=1}^c t_j^2 \sigma_j^{4k}}) =0,
$}
\end{sequation}%
as {\small $\sigma_{k_2}^{4k} \geq \sigma_{k_1}^{4k}$}, and $t_{k_1}$ and $t_{k_2}$ are \textit{i.i.d.} random variables with same normal distribution.
Equation~(\ref{eq:increase}) proves that {\small $\mu_k$} is \textit{monotone non-decreasing} in expectation, which completes the proof.
\end{proof}

