\subsection{Score Prediction and Model Optimization}
\label{sec:score}


\subsubsection{\textbf{Matching score prediction.}}
\label{sec:score_computation}
Given two nodes $x \in \mathcal{V}_1$ and $y \in \mathcal{V}_2$, one natural manner to implement the score function is \textit{inner-product}, mainly for its simplicity as:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:inner_score}
\widehat{\emb{Y}}_{x,y} =  (\alpha_x\emb{Q}_x)^\mathsf{T} \cdot (\alpha_y\emb{Q}_y).
\end{equation}
However, the inner product in Equation~(\ref{eq:inner_score}) is still conducted in the (continuous) Euclidean space with \textit{full-precision arithmetics}.
To bridge the connection between the inner product and Hamming distance measurement, we introduce Theorem~\ref{tm:equal} as follows:

\begin{thm}[\textbf{Hamming Distance Matching}]
\label{tm:equal}
Given two hash codes, we have $(\alpha_x\emb{Q}_x)^\mathsf{T} \cdot (\alpha_y\emb{Q}_y)$ $=$ $\alpha_x\alpha_y$ $(d - 2D_{H}(\emb{Q}_x, \emb{Q}_y))$.
\end{thm}

$D_H(\cdot, \cdot)$ denotes the Hamming distance between two inputs.
Based on Theorem~\ref{tm:equal}, we transform the score computation to the Hamming distance matching. 
By doing so, we can reduce most number of the floating-point operations (\#FLOPs) in the original score computation formulation (Equation~(\ref{eq:inner_score})) to efficient hamming distance matching.
% This can develop substantial computation acceleration that is analyzed in Appendix~\ref{sec:discuss}.
This can develop substantial computation acceleration that is analyzed in Appendix C.



\subsubsection{\textbf{Multi-loss Objective Function.}}
Our objective function consists of two components, i.e., graph reconstruction loss $\mathcal{L}_{rec}$ and BPR loss $\mathcal{L}_{bpr}$. 
Generally, these two loss functions harness the regularization effect to each other.
The intuition of such design is: 
\begin{itemize}[leftmargin=*]
\item $\mathcal{L}_{rec}$ reconstructs the observed bipartite graph topology;
\item $\mathcal{L}_{bpr}$ ranks the matching scores computed from the hash codes. 
\end{itemize}
Concretely, we implement $\mathcal{L}_{rec}$ with Cross-entropy loss:  
\begin{equation}
\label{eq:rec}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\resizebox{1\linewidth}{!}{$
\displaystyle
\mathcal{L}_{rec} = \sum_{x \in \mathcal{V}_1} \Big(\sum_{y\in \mathcal{N}(x)} \ln\sigma\Big(({\emb{V}}^{(0)}_x)^\mathsf{T} \cdot {\emb{V}}^{(0)}_y\Big) + \sum_{y' \notin \mathcal{N}(x)}\ln\Big(1-\sigma\big(({\emb{V}}^{(0)}_x)^\mathsf{T} \cdot {\emb{V}}^{(0)}_{y'}\big)\Big)\Big),
$}
\end{equation}
where $\sigma$ is the activation function, e.g., Sigmoid.
$\mathcal{L}_{rec}$ bases on the initial continuous embeddings before the graph convolution, e.g., {\small${\emb{V}}^{(0)}_x$}, providing the most fundamental information for topology reconstruction.
As for $\mathcal{L}_{bpr}$, we employ \textit{Bayesian Personalized Ranking} (BPR) loss as:
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\label{eq:hd-bpr}
\mathcal{L}_{bpr} = -\sum_{x \in \mathcal{V}_1} \sum_{\tiny y\in \mathcal{N}(x) \atop y'\notin \mathcal{N}(x)} \ln \sigma(\widehat{\emb{Y}}_{x,y} - \widehat{\emb{Y}}_{x,y'}).
\end{equation}
$\mathcal{L}_{bpr}$ encourages the predicted score of an observed edge to be higher than its unobserved counterparts~\cite{lightgcn}.
Let $\Theta$ denote the set of trainable embeddings regularized by the parameter $\lambda_2$ to avoid over-fitting.
our final objective function is finally defined as:
\begin{equation}
\label{eq:L}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\mathcal{L} = \mathcal{L}_{rec} + \lambda_1\mathcal{L}_{bpr} + \lambda_2 ||\Theta||_2^2.
\end{equation} 

% So far, we have introduced all technical parts of \model~and attached the pseudocodes in Appendix~\ref{app:notation_and_code}.
% We present all the theorem proofs and complexity analyses in Appendix~\ref{sec:discuss}.

So far, we have introduced all technical parts of \model~and attached the pseudocodes in Appendix B.
We present all the theorem proofs and complexity analyses in Appendix C.