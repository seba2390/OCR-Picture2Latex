\section{\textbf{Introduction}}

Bipartite graphs are ubiquitous in the real world for the ease of modeling various Web applications, e.g., as shown in Figure~\ref{fig:intro}(a), user-product recommendation~\cite{ma2020probabilistic,zhang2019star} and online query-document matching~\cite{zhang2019doc2hash}.
A fundamental task, \textit{Top-N search}, is to filter out N best-matched graph nodes for a query node, e.g., recommending Top-N attractive products to a target user in the user-product graph.
With the development of the recent machine learning research, learning vectorized representations (\textit{a.k.a.} embeddings) nowadays has become one of the standard procedures for similarity matching~\cite{grover2016node2vec,cheng2018aspect}.
Among existing techniques, graph-based neural methods, i.e., \textit{Graph Convolutional Networks} (GCNs), have recently present remarkable model performance~\cite{graphsage,lightgcn}.
Due to the ability to capture high-order connection information, GCN models can thus produce semantic enrichment to the node embeddings.
Based on the learned embeddings, similarity estimation is then exhaustively proceeded in the continuous Euclidean space.

 
Apart from embedding informativeness, \textit{computation latency} and \textit{embedding memory overhead} are two important criteria for realistic application deployment.
With the explosive data growth, \textit{learning to hash}~\cite{wang2017survey,jegou2010product} recently provides an alternative option to graph-based models for optimizing the model scalability.
Generally, it learns to convert the vectorized list of continuous values into the finite binarized hash codes.
In lieu of using \textit{full-precision}\footnote{\scriptsize The term ``full-precision'' generally refers to single-precision and double-precision. And we use float32 by default throughout this work for illustration.} 
embeddings, the learned hash codes have the promising potential to achieve, not only the space reduction, but also the computation acceleration for Top-N object matching and retrieval in the Hamming space.



\begin{figure}[tp]
% \hspace{-0.15in}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=3.4in]{figs/bipartite.pdf}
\end{minipage} 
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
\vspace{-0.1in}
\caption{Illustration of bipartite graph modeling and overall model performance visualization on Dianping dataset.}
\label{fig:intro}
\end{figure}

Despite the promising advantages of bridging GCNs and learning to hash,
simply stacking these two techniques is trivial and thus falls short of performance satisfaction with several inadequacies:
% it is still challenging to approach this target as previous work~\cite{hashgnn} falls short of performance satisfaction.
% This phenomenon can be explained both intuitively and technically:
\begin{itemize}[leftmargin=*]

\item \textbf{Coarse-grained similarity measurement.}
Compared to continuous embeddings, hash codes with the same vector dimension are naturally \textit{less expressive} with finite encoding permutation in Hamming space (e.g., $2^d$ if the dimension is $d$).
Consequently, this leads to a coarse-grained estimation of the pairwise node similarity, thus drawing a conspicuous performance decay with inaccurate Top-N matching.


\item \textbf{Feature erosion issue.}
Recent work~\cite{qin2020forward,rastegari2016xnor,lin2017towards,hashgnn} usually adopts $\sign(\cdot)$ function for $O(1)$ complexity encoding.
However, hashing via $\sign(\cdot)$ will inevitably smooth the embedding feature informativeness, via converting each digit of continuous embeddings into the hamming space, no matter what specific value it used to be.
Thus the latent features in these learned hash embeddings become less informative, and existing models lack certain mechanisms to hedge the feature erosion in hashing.


\item \textbf{Intractable model optimization.}
Since $\sign(\cdot)$ is not differentiable at 0 and its derivatives are 0 anywhere else, previous models usually use \textit{visually similar} but not necessarily \textit{theoretically relevant} functions, e.g., $\tanh(\cdot)$, for gradient estimation.
This may lead to inconsistent optimization directions in model training.
% Moreover, because of the embedding discreteness, the associated loss landscape\footnote{\scriptsize Details about the visualization construction are attached in Appendix~\ref{sec:visualization}.} (Figure~\ref{fig:model}(a)) are steep and bumping~\cite{bai2020binarybert}, which further increases the difficulty in optimization.
Moreover, because of the embedding discreteness, the associated loss landscape\footnote{\scriptsize Details about the visualization construction are attached in Appendix A.} (Figure~\ref{fig:model}(a)) are steep and bumping~\cite{bai2020binarybert}, which further increases the difficulty in optimization.
\end{itemize}


In this paper, we study the problem of learning to hash with Graph Convolutional Network (GCN) on bipartite graphs for effective Top-N Hamming space search.
We propose a model namely \textit{\underline{B}ipartite \underline{G}raph \underline{C}onvolutional \underline{H}ashing} (BGCH), with three effective modules: (1) \textit{adaptive graph convolutional hashing}, (2) \textit{latent feature dispersion}, and (3) \textit{Fourier serialized gradient estimation}.
While the former two modules significantly enrich the informativeness and expressivity to the learned hash codes, the last one provides an accordant and tractable optimization flow in forward and backward propagation of model optimization.
Concretely:
\begin{itemize}[leftmargin=*]
\item \textbf{Adaptive graph convolutional hashing.}
Our first module designs a \textit{topology-aware} convolutional hashing that employs the layer-wise hash encoding (from low- to high-order sub-structures of bipartite graphs) to consecutively binarize the node features with different semantics.
To boost the expressivity, the convolutional hashing is equipped an effective approximation technique for \textit{embedding rescaling}, which does not undermine the efficiency of Hamming distance computation.
Intuitively, these two designs make the learned hash codes more informative and expressive for preserving fine-grained similarity in the Hamming space.

\item \textbf{Latent feature dispersion}.
Our second module, i.e., feature dispersion, aims to hedge the inevitable information loss from the numerical binarization.
In conventional continuous embeddings, major features however condense in a small region of embedding structures.
Since these vectorized latent features tend to be inevitably smoothed by the hashing discreteness, it is natural to preserve information as much as possible by spreading out those decisive features to the majority of embedding dimensions, instead of the very few of them.
To achieve this, our proposed module aims to explicitly disperse informative latent node features, which can be further diffused to each convolutional layer when exploring the bipartite graph.



\item \textbf{Fourier serialized gradient estimation.}
Furthermore, to provide accurate gradient estimation, \model~proposes to decompose $\sign(\cdot)$ function with Fourier Series in the frequency domain.
Compared to existing gradient estimators~\cite{qin2020forward,gong2019differentiable,darabi2018bnn,sigmoid,RBCN}, this estimator better follows the \textit{main direction} of factual gradients to enable an accordant and tractable model optimization in forward and backward propagation.
With the limited number of decomposition terms, \model~can well provide more accurate gradient estimation to $\sign(\cdot)$ within the acceptable training cost.
\end{itemize}

Based on the learned hash codes, \model~maintains moderate resource consumption whilst providing substantial performance improvement in Top-N Hamming space retrieval.
The quality-cost trade-off is summarized in Figure~\ref{fig:intro}(b), which compares \model~ against a list of representative counterparts (\textit{including float32-based and hashing-based}) on a real-world bipartite graph with over 10 million observed edges (experimental details are reported in~\cref{sec:exp_setup}).
As the lower-right corner of Figure~\ref{fig:intro}(b) indicates the ideal optimal performance, \model~can deliver over 8$\times$ computation acceleration and space reduction relative to existing full-precision models, while being more effective than each hash-based method (\cref{sec:exp_topn} and \cref{sec:exp_full}).
To summarize, our main contributions are organized as follows:
\begin{itemize}[leftmargin=*]
\item 
We study the problem of learning to hash with Graph Convolutional Network on bipartite graphs.
We propose a novel approach \model~with three effective modules for effective and efficient Top-N search in Hamming space (\cref{sec:method}).

\item We conduct extensive experiments on six real-world datasets to evaluate the retrieval quality.
In-depth analyses are also provided towards the necessity of all proposed model components from both technical and empirical perspectives (\cref{sec:exp}).
 

% \item We theoretically prove the model effectiveness and provide complexity analyses in terms of time and space costs (Appendix~\ref{sec:discuss}).

\item We theoretically prove the model effectiveness and provide complexity analyses in terms of time and space costs (Appendix C).
\end{itemize}


