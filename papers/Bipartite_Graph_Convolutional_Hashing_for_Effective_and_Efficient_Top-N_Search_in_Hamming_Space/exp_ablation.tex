
 \begin{table*}[t]
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0.2cm}
\centering
\footnotesize
\caption{Ablation study.}
\vspace{-0.15in}
\label{tab:ablation}
\setlength{\tabcolsep}{0.7mm}{
\begin{tabular}{c |c c|c c|c c|c c|c c|c c}
\toprule
 \multirow{2}*{Variant} & \multicolumn{2}{c|}{MovieLens} & \multicolumn{2}{c|}{Gowalla} & \multicolumn{2}{c|}{Pinterest} & \multicolumn{2}{c|}{Yelp2018}  &\multicolumn{2}{c|}{AMZ-Book} &\multicolumn{2}{c}{Dianping} \\
               ~  & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20 & R@20 & N@20\\
\midrule
\midrule
 \textsl{w/o FD}   &{22.82} {{\drop \notsotiny (-0.17\%)}} & {35.87} {{\drop \notsotiny (-1.08\%)}}  & {15.92} {{\drop \notsotiny (-4.84\%)}}  & {15.79} {{\drop \notsotiny (-4.19\%)}}  & {12.25} {{\drop \notsotiny (-4.15\%)}}  & {9.07} {{\drop \notsotiny (-3.72\%)}}  & {5.16} {{\drop \notsotiny (-6.35\%)}} & {5.49} {{\drop \notsotiny (-2.49\%)}}   &  {3.26} {{\drop \notsotiny (-6.32\%)}} & {3.57} {{\drop \notsotiny (-8.93\%)}}   &  {10.46} {{\drop \notsotiny (-1.88\%)}} &  {7.50} {{\drop \notsotiny (-1.70\%)}}    \\


 \textsl{w/o AH-TA}    &{19.54}{{\drop \notsotiny (-14.52\%)}} &{29.17}{{\drop \notsotiny (-19.55\%)}} &{13.49}{{\drop \notsotiny (-19.37\%)}} &{12.38}{{\drop \notsotiny (-24.88\%)}} &{12.24} {{\drop \notsotiny (-4.23\%)}} &{8.86} {{\drop \notsotiny (-5.94\%)}} &{4.77}{{\drop \notsotiny (-13.43\%)}} &{5.18}{{\drop \notsotiny (-11.30\%)}} &{2.49}{{\drop \notsotiny (-28.45\%)}} &{2.86}{{\drop \notsotiny (-27.04\%)}} &{\ \ 9.83} {{\drop \notsotiny (-7.79\%)}} &{6.87} {{\drop \notsotiny (-9.96\%)}}  \\

\textsl{w/o AH-RF}   &{16.73}{{\drop \notsotiny (-26.82\%)}} &{26.97}{{\drop \notsotiny (-25.62\%)}} &{11.24}{{\drop \notsotiny (-32.82\%)}} &{11.29}{{\drop \notsotiny (-31.49\%)}} &{10.18}{{\drop \notsotiny (-20.34\%)}} &{7.33}{{\drop \notsotiny (-22.19\%)}} &{3.76}{{\drop \notsotiny (-31.76\%)}} &{4.30}{{\drop \notsotiny (-26.37\%)}} &{3.27} {{\drop \notsotiny (-6.03\%)}} &{3.64} {{\drop \notsotiny (-7.14\%)}} &{\, \ 8.33}{{\drop \notsotiny (-21.86\%)}} &{6.93} {{\drop \notsotiny (-9.17\%)}}  \\
\midrule[0.1pt]


 \textsl{w/in LF}  &{21.06} {{\drop \notsotiny (-7.87\%)}} &{34.59} {{\drop \notsotiny (-4.61\%)}} &{15.48} {{\drop \notsotiny (-7.47\%)}} &{15.38} {{\drop \notsotiny (-6.67\%)}} &{11.94} {{\drop \notsotiny (-6.57\%)}} &{8.89} {{\drop \notsotiny (-5.63\%)}} &{4.86}{{\drop \notsotiny (-11.80\%)}} &{5.17}{{\drop \notsotiny (-11.47\%)}} &{3.14} {{\drop \notsotiny (-9.77\%)}} &{3.62} {{\drop \notsotiny (-7.65\%)}} &{\, \ 9.40}{{\drop \notsotiny (-11.82\%)}} &{7.27} {{\drop \notsotiny (-4.72\%)}}  \\

 \textsl{w/o $\mathcal{L}_{bpr}$} &{21.42} {{\drop \notsotiny (-6.30\%)}} &{34.83} {{\drop \notsotiny (-3.94\%)}} &{15.87} {{\drop \notsotiny (-5.14\%)}} &{15.66} {{\drop \notsotiny (-4.98\%)}} &{12.33} {{\drop \notsotiny (-3.52\%)}} &{9.17} {{\drop \notsotiny (-2.65\%)}} &{5.31} {{\drop \notsotiny (-3.63\%)}} &{5.61} {{\drop \notsotiny (-3.94\%)}} &{3.35} {{\drop \notsotiny (-3.74\%)}} &{3.77} {{\drop \notsotiny (-3.83\%)}} &{10.21} {{\drop \notsotiny (-4.22\%)}} &{7.38} {{\drop \notsotiny (-3.28\%)}}  \\

 \textsl{w/o $\mathcal{L}_{rec}$}  &{17.01}{{\drop \notsotiny (-25.59\%)}} &{27.16}{{\drop \notsotiny (-25.10\%)}} &{12.27}{{\drop \notsotiny (-26.66\%)}} &{12.63}{{\drop \notsotiny (-23.36\%)}} &{10.81}{{\drop \notsotiny (-15.41\%)}} &{7.86}{{\drop \notsotiny (-16.56\%)}} &{3.93}{{\drop \notsotiny (-28.68\%)}} &{4.37}{{\drop \notsotiny (-25.17\%)}} &{3.19} {{\drop \notsotiny (-8.33\%)}} &{3.73} {{\drop \notsotiny (-4.85\%)}} &{\, \ 8.82}{{\drop \notsotiny (-17.26\%)}} &{7.26} {{\drop \notsotiny (-4.85\%)}}  \\

\midrule[0.1pt]
  \textbf{\model}   &\textbf{22.86}& \textbf{36.26}  &\textbf{16.73}& \textbf{16.48} &\textbf{12.78}& \textbf{9.42} &\textbf{5.51} & \textbf{5.84} &\textbf{3.48} & \textbf{3.92} &\textbf{10.66}& \textbf{7.63}    \\ 

\bottomrule
\end{tabular}}
\end{table*}
% \vspace{-0.2in}


\subsection{\textbf{Ablation Study (RQ3)}}
\label{sec:ablation}
We evaluate the necessity of model components with Top-20 search metrics and report the results in Table~\ref{tab:ablation}.

{\textbf{Effect of Feature Dispersion.}}
We first analyze the effect of our proposed feature dispersion approach for hedging the feature erosion in hash encoding.
We introduce the model variant, denoted by \textsl{w/o FD}, to directly disable it by setting $\eta$ as 0.
As shown in Table~\ref{tab:ablation}, the performance gap between \textsl{w/o FD} and \model~ well demonstrates the effectiveness of dispersing the latent features before embedding binarization for hashing over these six datasets. 
Moreover, let the density summarized in Table~\ref{tab:datasets} be computed by $\frac{|\mathcal{V}_1|\times |\mathcal{V}_2|}{|\mathcal{E}|}$. 
In sparse datasets, i.e., Gowalla (0.00084), Pinterest (0.00267), Yelp2018 (0.00130), and AMZ-Book (0.00062), the performance decay between \model~and \textsl{w/o FD} is much larger than on the other two datasets, i.e., MovieLens (0.04190) and Dianping (0.02210).
This is because sparse datasets are more sensitive to hashing as they may not have insufficient training edges to abridge the gap against their unhashed version.
Another promising approach to tackle data sparsity issue is \textit{data augmentation}~\cite{zhang2022costa} and we leave it for future work.




{\textbf{Effect of Adaptive Graph Convolutional Hashing.}}
Then we study this model component by setting two variants, where: (1) \textsl{w/o AH-TA} only disables the \textit{topology-awareness of hashing} and sets it as the final encoder after all graph convolutions (just like the conventional manner~\cite{hashgnn,hashnet}); (2) \textsl{w/o AH-RF} removes the \textit{rescaling factors}.
From Table~\ref{tab:ablation} results, we have the following observations:
\begin{enumerate}[leftmargin=*]
\item 
The variant \textsl{w/o AH-TA} consistently underperforms \model.
This demonstrates that simply using the rear output embeddings from the GCN framework may not sufficiently model the unique latent node features for hashing, especially for the rich structural information within different graph depths.
While in \model, by capturing the intermediate information for representation enrichment, the topology-aware hashing can effectively alleviate the limited expressivity of discrete hash codes.


\item Apart from the topology-aware hashing, another key point for contributing to the performance improvement is the \textit{rescaling factor} that we introduced in Equation~(\ref{eq:rescale}).
After removing it from \model, variant \textsl{w/o AH-RF} presents huge performance decay.
Although these factors are directly calculated and may not be theoretically optimal, they reflect the numerical uniqueness of embeddings for later hash encoding, which substantially improves \model's prediction capability. 
We study the \textit{determinacy} design of factor computation in the following section.
\end{enumerate}





{\textbf{Design of Learnable Rescaling.}}
% \label{sec:deterministic}
We include another variant namely \textsl{w/in LF} to indicate the model version using \textit{learnable rescaling factors}. 
As shown in Table~\ref{tab:ablation}, the design of learnable rescaling factors in \textsl{w/in LF} does not achieve good performance as expected. 
One explanation is that, our proposed model currently does not post a strong mathematical constraint to the learnable factors ($\alpha_x$), e.g., $\alpha_x^{(l)} = \argmin(\widetilde{\emb{V}}^{(l)}_x$, $\alpha_x^{(l)} \emb{{Q}}^{(l)}_x)$, mainly because of its additional training complexity; and purely relying on the stochastic optimization, e.g., stochastic gradient descent (SGD), may hardly reach the optimum.
Considering the additional search space introduced from this regularization design, we argue that our deterministic rescaling method is simple yet effective in practice.



{\textbf{Effect of Multi-loss in Optimization.}}
Lastly, to study the effect of BPR loss $\mathcal{L}_{bpr}$ and graph reconstruction loss $\mathcal{L}_{rec}$, we set two variants, termed by \textsl{w/o $\mathcal{L}_{bpr}$} and \textsl{w/o $\mathcal{L}_{rec}$}, to optimize \model~separately.
As shown in Table~\ref{tab:ablation}, with all other model components, partially using each one of $\mathcal{L}_{bpr}$ and $\mathcal{L}_{rec}$ can not achieve the expected performance.
This confirms the effectiveness of our proposed multi-loss design:
while $\mathcal{L}_{bpr}$ learns to assign higher prediction values to observed edges, i.e., $\emb{Y}_{x,y}=1$, than the unobserved node pair counterparts, 
$\mathcal{L}_{rec}$ transfers the graph reconstruction problem to a classification task by using the original embeddings in training.
By collectively optimizing these two loss functions, our model \model~can learn precise intermediate embeddings from $\mathcal{L}_{rec}$, and generate targeted hash codes with high-quality relative order information regularized by $\mathcal{L}_{bpr}$ accordingly.


