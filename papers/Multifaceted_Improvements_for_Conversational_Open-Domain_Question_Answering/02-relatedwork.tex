\vspace{-0.1in}
\section{Related Work}\label{sec:related}

This section briefly summarizes some existing works on open domain question answering and conversational question answering, which are most relevant to this work.

\vspace{-0.1in}
\subsection{Open Domain Question Answering}
%介绍open domain QA
Open-domain question answering (OpenQA) \cite{voorhees1999trec} is a task that uses a huge library of documents to answer factual queries. The two-stage design, which included a passage retriever to choose a subset of passages and a machine reader to exact answers, became popular after DrQA~\cite{chen2017reading}.

The passage retriever is an important component of OpenQA system since it searches relevant paragraphs for the next stage. 
Traditional sparse retrieval models, such as TF-IDF or BM25~\cite{robertson2009probabilistic}, have been widely adopted as retriever in OpenQA systems~\cite{chen2017reading, yang2019end, lin2018denoising}. While sparse retrieval cannot handle the case of high semantic correlation with little lexical overlap and it is untrainable, dense passage retrievers have lately gained popularity~\cite{lee2019latent, guu2020realm, karpukhin2020dense, Qu2021RocketQAAO}. 
In general, the dense retrieval model is a dual-encoder architecture that encodes both the question and the passage individually. Both encoders are trained during the retriever pre-training process. When training with the reader for the QA task, only the question encoder is normally fine-tuned.
In order to increase the retrieval impact of dense retrievers, some studies incorporate hard negatives. 
% mining to train dense retrieval models. 
BM25 top passages which do not contain answers are utilized as hard negatives~\cite{karpukhin2020dense, gao2020complementing}. And dynamic hard negatives are employed in~\cite{xiong2021approximate, zhan2021optimizing,guu2020realm}, which are the top-ranked irrelevant documents given by dense retriever during training. 
% While classic passage retrieval models such as BM25\cite{robertson2009probabilistic} depend on sparse representations, dense passage retrievers\cite{lee2019latent,guu2020realm,karpukhin2020dense} have lately gained popularity due to their ability to return paragraphs with little lexical overlap but great relevance to questions.
%引入硬负样本训练dense retriever
% Many researchers employ negative sampling techniques to train dense retrieval models in order to improve their performance. For example, BM25 top passages which do not contain answers were utilized as hard negatives by DPR\cite{karpukhin2020dense}. RocketQA\cite{Qu2021RocketQAAO} uses a well-trained cross-encoder, and passages with high confidence are chosen as negative samples. And in the training phase, dynamic hard negatives are employed in \cite{xiong2021approximate, zhan2021optimizing}, which are the top-ranked irrelevant documents given the dense retrieval parameters. 

% reader
A contemporary OpenQA system also includes a reader as a key component. Its goal is to deduce the answer to a query from a collection of documents. Existing Readers may be divided into two types: (1) Extractive readers~\cite{chen2017reading,yang2019end,karpukhin2020dense}, which anticipate an answer span from the retrieved texts, (2) Generative readers~\cite{lewis2020retrieval,izacard2021leveraging}, which produce natural language replies using sequence-to-sequence (Seq2Seq) models.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figure/f1.pdf}
         \caption{}
         \label{fig:curriculum_f1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figure/retriever_recall.pdf}
         \caption{}
         \label{fig:curriculum_recall}
     \end{subfigure}
     \vspace{-0.15in}
        \caption{Comparison of performance improvement with / without curriculum learning.}
    \vspace{-0.2in}
        \label{fig:curriculum}
        % \vspace{-0.1in}
\end{figure}

% \vspace{-0.2in}
\subsection{Conversational Question Answering} 
% 介绍Conversational QA
Conversational Question Answering (CQA) is required to understand the given context and history dialogue to answer the question.
As a main type of CQA, Conversational Machine Reading Comprehension (CMRC)~\cite{qu2019bert, qu2019attentive, qiu2021reinforced} does the QA task with text-based corpora. 
% It could be split into two categories \cite{zaib2021conversational}: (1) sequential Knowledge-Based Question Answering (KBQA) system \cite{iyyer2017search, guo2018dialog, kacupaj2021conversational}, (2) Conversational Machine Reading Comprehension (CMRC) \cite{qu2019bert, qu2019attentive, qiu2021reinforced}. CMRC is text-based, whereas KBQA is knowledge graph-based.
For CMRC, the number of conversational history turns is critical, as context utterances that are relevant to the inquiry are valuable, while irrelevant ones may introduce additional noise. For example, \cite{qu2019bert, qu2020open} makes use of conversation history by including $K$ rounds of history turns. \cite{qu2019attentive} weights previous conversation rounds based on their contribution to the answer to the current question.

% 引入open retrieval Conversational QA
The approaches outlined above rely extensively either on the provided material or a given paragraph to extract or generate answers. 
However, this is impractical in real world since golden passage is not always available. 
Open retrieval methods which try to obtain evidence from a big collection, have lately been popular in the CMRC. 
% The approaches outlined above significantly rely on the provided material to extract or generate an answer. 
ORConvQA~\cite{qu2020open} is the first work proposing three primary modules for open-retrieval CQA: (1) a passage retriever, (2) a passage reranker, and (3) a passage reader. Given a query and its conversational history, the passage retriever retrieves the top $K$ relevant texts from a large-scale corpus. The passage reranker and reader then rerank and read the top texts to discover the correct answer. 
The research for the conversational OpenQA needs to be further explored. This work tries to improve ORConvQA from multiple aspects including regularizing retriever pre-training, incorporating post-ranking, and curriculum learning.




