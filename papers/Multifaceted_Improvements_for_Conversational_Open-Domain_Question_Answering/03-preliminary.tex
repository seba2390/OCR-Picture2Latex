% \vspace{-0.15in}
\section{Preliminary}\label{sec:preliminary}
 

% and $\{q_i\}^{c-1}_{i=1}$ represents the historical questions. 

% \subsection{Dual Encoder Retrieval Model}

% In the retrieval process, the commonly used dual-encoder model \cite{bromley1993signature} consists of the question encoder $E_Q$ and the passage encoder $E_P$, which encode the question and passage into d-dimensional vectors respectively. Many similarity functions, such as inner product and Euclidean distance, can be used to indicate the relationship between questions and passages. Some tests in \cite{karpukhin2020dense} demonstrate that these comparable functions act similarly, so the simpler inner product is selected:

% \begin{equation}\label{eq:inner}%加*表示不对公式编号
% \begin{aligned}
% \mathrm{sim}(q,p) = E_Q(q)^\mathrm{T}E_P(p)
% \end{aligned}
% \end{equation}
% Typically, $E_Q$ and $E_P$ are two pre-trained model, e.g., BERT\cite{devlin2018bert}, ALBERT\cite{lan2020albert}. We picked the lighter ALBERT, which contains less model parameters and allows us to raise the batch size to have a higher training impact.

% \subsection{Rerank and Reader}

% Based on the retrieved passages from a first-stage retriever,


% Given the top k retrieved passages, the reader assigns a passage selection score to each passage. In addition, it extracts an answer span from each passage and assigns a span score. The probabilities of a token being the starting/ending positions of an answer span and a passage being selected are defined as:

\subsection{Retriever-Reader Pipeline}
The most typical OpenQA system follows a two-stage pipeline, which has two components: one retriever and one reader. 
% Retriever 
% Deep learning techniques enable the two components to be end-to-end trainable.

\subsubsection{Retriever}\label{subsec:Retrieving and Reading}
In the retrieval process, the commonly used dual-encoder model \cite{bromley1993signature} consists of a question encoder and a passage encoder, which encode the question and passage into low-dimensional vectors, respectively. Many similarity functions, such as inner product and Euclidean distance, can be used to measure the relationship between questions and passages. The evaluation in~\cite{karpukhin2020dense} demonstrates that these functions perform comparably, so the simpler inner product is selected in our work:
\vspace{-0.05in}
\begin{equation}\label{eq:inner}%加*表示不对公式编号
\begin{aligned}
\mathrm{sim}(q,p) = E_Q(q)^\mathrm{T}E_P(p).
\end{aligned}
\end{equation}
% Typically, 
where $q$ and $p$ denote a given question and passage. $E_Q$ and $E_P$ refer to the question encoder and passage encoder which typically are two pre-trained model, e.g., BERT\cite{devlin2018bert}, ALBERT\cite{lan2020albert}. 
% We picked the lighter ALBERT, which contains less model parameters and allows us to raise the batch size to have a higher training impact. 
The retriever score is defined as the similarly of representations of the question and passage.
Given a question $q$, the retriever derives a small subset of passages with embeddings closest to $q$ from a large corpus.

\subsubsection{Reader}\label{subsec:Retrieving and Reading}
In reading phase, a conventional BERT-based extractive machine comprehension model is usually used as the reader. The retrieved passages concatenated with the corresponding questions are first encoded separately. 
The reader then extracts the start and end tokens with the max probabilities among tokens from all the the top passages.
% The reader then maximizes the probabilities of finding the true start and end tokens among tokens from all of the top passages. 
The reader score is defined as the sum of the scores of start token and end token. The answer score is the sum of retriever score and reader score.
Taking the small collection of passages generated by the retriever as input, the reader outputs the answer span with the highest answer score.

\vspace{-0.05in}
\subsection{Problem Definition}
Following the task definition in ~\cite{qu2020open}, the conversational OpenQA problem can be formulated as follows: Given the current question $q_c$, and a set of historical question-answer pairs $\{(q_i, a_i)\}^{c-1}_{i=1}$, where $q_i$ denotes the $i$-th question and $a_i$ is the related answer in a conversation, the task is to identify answer spans for the current question $q_c$ from a large corpus of articles, \emph{e.g.,} Wikipedia.

Based on the introduced two-stage pipeline, this work applies a multi-stage pipeline of one pretrained retriever, one post-ranker, and one reader. The details of the framework and training process of {\modelname} are introduced in the next section.
% Some preliminaries for the pipeline are introduced as follows.  
