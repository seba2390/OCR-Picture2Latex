\section{Introduction}
\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figure/intro-1v4.pdf}
         \caption{}
         \label{fig:limit1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figure/intro-2v4.pdf}
         \caption{}
         \label{fig:limit2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figure/intro-3v4.pdf}
         \caption{}
         \label{fig:limit3}
     \end{subfigure}
     \vspace{-0.15in}
        \caption{Illustration of three limitations and the corresponding solutions (marked with the blue dashed boxes). (a) Inaccurate question understanding: Feeding rewrite to the encoder might lead to poor performance in question understanding as the encoder can not learn how to model the concatenated question. (b) Coarse ranking: The relevant passage may not be picked up even it is retrieved in the initial ranking list which is roughly obtained by a frozen passage encoder and learnable question encoder. (c) Inconsistent usage of golden passage: invariably adding golden passage for training would increase dependency on it and be adverse to inference where no golden passage is manually added.}
        \label{fig:limitation}
        \vspace{-0.1in}
\end{figure*}
% \vspace{-0.1in}

Open-domain question answering (OpenQA) aims to discover answers from an extremely large text source such as Wikipedia for given questions~\cite{chen2017reading, yang2019end, zhu2021retrieving}. OpenQA that receives the more widespread application as its setting is more aligned with real-world QA process of human beings. Generally, an OpenQA system needs to firstly locate a small collection of relevant articles and then generate the answer. 
% Conventional OpenQA systems consist of three components including question analysis, document retrieval, and answer extraction~\cite{harabagiu2003open, pasca29open}. 
The widely adopted architecture of the existing OpenQA system consists of two components, retriever and reader. The former acts as an information retrieval system to find relevant passages probably with the correct answer contained~\cite{kratzwald2018adaptive, lee2019latent,xiong2020answering,min2019knowledge,izacard2021leveraging}. The latter aims at extracting or generating the answers from the retrieved passages.
Although OpenQA enjoys a sound development momentum, it is challenged by several issues including question complexity and ambiguity, as well as insufficient background knowledge.
Conversational OpenQA which executes question answering with open-domain sources under the conversational setting is able to address the above issues by providing the context information in the conversations. 

Previous work~\cite{qu2020open} firstly defines the task of open-retrieval conversational question answering (ORConvQA) and proposes an effective end-to-end system to deal with it. 
Promising as it might be, several fundamental limitations still exist:
%as illustrated in Figure~\ref{fig:limitation}:
% \textbf{(1) Inadequate pre-trained retrieval.} 
% The ORConvQA model applies the dual-encoder architecture to obtain the initial retrieval results from the large corpus of Wikipedia passages, which only uses the rewrite form for each question in the pre-training phase and tries to mitigate the mismatch problem by fine-tuning question encoder in the concurrent learning phase~\cite{qu2020open}. However, the inappropriate or insufficient input questions would negatively influence the offline representation learning for the Wikipedia passages and further lead an inadequate retrieval result, which could not be revised in the later joint training phase.
\textbf{(1) Inaccurate question understanding.} 
The questions in the dialogues might be hard to understand due to the unspecific pronouns. 
%For example, given the question, ``Is he happy?'', the model cannot find the correct answer without understanding who he is.
To alleviate this issue, ORConvQA uses rewrite questions by replacing pronouns with real entities to pre-train the retriever. Then, they concatenate the dialogue history and the question together to jointly train the whole model. However, the input of the pre-traing and the joint training process is still often inconsistent which makes the model incapable of really understand those unclear questions.
%different types of inputs makes the model incapable of modeling those underspecified and ambiguous questions even with context information in the conversation. to 
%However, the model is incapable of truly understand the questions and the QA is inaccurate even with the provided context information.
%%The ORConvQA model applies the dual-encoder architecture to obtain the initial retrieval results from the large corpus of Wikipedia. At pre-training time, it only uses the rewrite form of each question as input for question encoder while at joint training and inference it takes the concatenation of the historical and current questions as input.
%The discrepancy of the fed questions leads to an inaccurate question understanding as the pre-trained question encoder is incapable of modeling those underspecified and ambiguous questions even with context information in the conversation.
% However, the inappropriate or insufficient input questions would negatively influence the offline representation learning for the Wikipedia passages and further lead an inadequate retrieval result, which could not be revised in the later joint training phase.
\textbf{(2) Coarse ranking.} 
%Open-domain QA tasks usually adopts the retriever-reader pipeline, in which a retriever is first trained to find related passages and a reader is utilized to find the answer spans in the top-ranked passages \cite{yang2019end,karpukhin2020dense,qu2020open}. 
%\textbf{To improve the performance, previous works add a regularization term on the loss of the reader to constrain the ranking of the passages. However, this regularization won't affect the ranking of the retriever.} This makes the whole pipeline suffer from the coarse ranking, especially when the golden passages are not ranked in the top passages.
%Considerable works of OpenQA and conversational OpenQA adopt the conventional
Previous works adopt the \textcolor{black}{retriever-reader pipeline} which consists of a passage retriever and answer reader with a reranker/selector as regularization~\cite{yang2019end,karpukhin2020dense,qu2020open}. 
%The reranker here does not influence the retriever and the passages passed to the reader are obtained by intercepting the first few ones in the passages outputted by the retriever.
The reranker/selector here does not influence the retriever and the reader only takes the top-ranked passages as the input.
This makes the whole pipeline suffer from the coarse ranking, 
%which is embodied in the case
especially for the situations when golden passages are not retrieved in the top-ranked passages.
%might not include relevant ones even they are detected by the retriever. 
\textbf{(3) Inconsistent usage of golden passage.} 
During training, ORConvQA adds the golden passage into the training manually when the golden passage is not retrieved in the top-ranked passages. This is impractical for the testing as the golden passages are unknown in the inference stage. The inconsistency usages of golden passage during training and testing might lead to poor performance when the golden passage cannot be accurately retrieved.
%\cite{qu2020open} states that gold passages can be accessible during the training process to avoid poor training outcomes owing to failure to retrieve relevant articles. However, this is impractical as the gold passages are unseen in the inference stage and likely to lead poor performance as it is inconsistent with the inference procedure.


To address the previous issues, we propose a framework which has Multifaceted Improvements for Conversational open-domain Question Answering ({\modelname}). {\modelname} improves the previous work from multiple aspects including regularizing retriever pre-training, as well as incorporating post-ranking and curriculum learning.
%promoting pre-training, adding module, improving training strategy, which has three distinctive advantages. 
% \textcolor{black}{we propose a curriculum-guided framework for conversational question answering by refining retrieval ({\modelname}) from multiple aspects including promoting pre-training, adding module, improving training strategy.} 
%  Compared with previous work, the proposed {\modelname} has three distinctive advantages.
First, Figure~\ref{fig:limitation}(a) shows that, a Kullback\_Leibler (KL)-divergence based regularization is proposed for retriever pre-training, which constrains the retrieved results outputted by feeding different forms of questions. In this manner, we can learn a better representation for the concatenated question by taking its rewrite as supervision and minimizing the information loss between them. The better question understanding would benefit the retrieval and question answering performance in different phases.
% First, we propose a Kullback\_Leibler (KL)-divergence based regularization for retriever pre-training, which constrains the retrieved results outputted by feeding different forms of questions. In this manner, we can minimize the information loss of the approximate forms of the same question with KL-divergence, which further constrains the model complexity and leads an adequate pre-trained retrieval.
Second, 
% different from the previous retriever-reader pipeline, 
to further improve the initial retrieval results, we apply a multi-stage pipeline by adding a post-ranker module as shown in Figure~\ref{fig:limitation}(b), which also known as the post-processing~\cite{zhu2021retrieving} over retrieved passages generated by the retriever. The post-ranker learning benefits from a two-aspect constrain posed by a ranking loss with a distance-based contrastive loss. In this way, the relevant passages would be enforced by post-ranker to appear at the top positions and selected for answer reading.
Third, in the joint training stage, we design a semi-automatic curriculum learning strategy (the blue dashed boxes in Figure~\ref{fig:limitation}(c)) to reduce the dependency on the golden passages. It encourages the model to find true answer spans even when the golden passage are not retrieved as the top-ranked passages.
%to diminish the likelihood of introducing the gold passage to lessen its influence. It would effectively encourage the retriever and post-ranker to find the passage with the true answer contained by itself and reduce the dependency on the gold passage in both training and inference phase.
We evaluate {\modelname} on the publicly released dataset OR-QuAC~\cite{qu2020open} to demonstrate its effectiveness.

Our main contributions are as follows:
\begin{itemize}
    % \item We propose a curriculum-guided conversational OpenQA system which equipped with a multi-stage pipeline of retriever, post-ranker, and reader. The curriculum learning strategy effectively closes the gap between training and inference. 
    \item We propose a KL-divergence based regularization for retriever pre-training, which greatly strengthens the capability of question understanding.
    \item We propose a multi-stage pipeline for the conversational OpenQA task by incorporating a post-ranker module in the conventional retriever-reader pipline.
    \item We design a curriculum learning strategy for the joint training of the retriever, post-ranker, and reader, which effectively closes the gap between training and inference. 
    \item We conduct extensive experiments on the OR-QuAC dataset to demonstrate the effectiveness of the proposed {\modelname}. We further provide in-depth analysis on model ablation and case study to explore the insights of {\modelname}.
\end{itemize}
% \begin{itemize}
%     \item We propose a curriculum-guided conversational OpenQA system which equipped with a multi-stage pipeline of retriever, post-ranker, and reader. The curriculum learning strategy effectively closes the gap between training and inference. 
%     \item We design a KL-divergence based regularization for retriever pre-training, which greatly strengthen the capability of question understanding.
%     \item We conduct extensive experiments on the OR-QuAC dataset to demonstrate the effectiveness of the proposed {\modelname}. We further provide in-depth analysis on model ablation and case study to explore the insights of {\modelname}.
% \end{itemize}

% The remainder of this paper is structured as follows.  \ref{sec:preliminary} gives the basic concept of retriever-reader pipeline and the specific statement for the problem investigated in this work. The proposed {\modelname} method is described in Section \ref{sec:model}. Section \ref{sec:exp} reports the experimental results. Section \ref{sec:related} introduces some previous works and analysis. \ref{sec:conclusion} draws the conclusion.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/framework_v4.pdf}
    % \vspace{-0.3in}
    \caption{The overall framework of the {\modelname} model. The retriever equipped with one question encoder and one passage encoder searches the top $K$ passages from the large corpus of Wikipedia for the given question reformatted as $q_i^{en}$. The representations of $K$ passages are passed into the post-ranker to produce a refined ranking list of passages, of which top $T$ are concatenated with the question reconstructed as $q_i^{rd}$ to be the input for the reader. The reader is used to extract the final answer according to the post-ranker score and reader score.}
    \label{fig:model}
    % \vspace{-0.1in}
\end{figure*}
