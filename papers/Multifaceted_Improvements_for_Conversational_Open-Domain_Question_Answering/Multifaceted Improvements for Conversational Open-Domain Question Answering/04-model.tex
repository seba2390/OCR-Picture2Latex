
% \vspace{-0.05in}
\section{Model}\label{sec:model}
The overall framework of our {\modelname} is shown in Figure~\ref{fig:model}.
{\modelname} mainly consists of three components: (1) one regularized \textcolor{black}{retriever} for relevant passages discovery, (2) one \textcolor{black}{contrastive {\rerankname} for improving passage retrieval quality}, and (3) one \textcolor{black}{reader} for detecting answers from a small collection of ranked passages. These three components work as follows: With the passage representations offline encoded by the pre-trained passage encoder, the retriever outputs the top $K$ passages by operating inner product between them and the embedding of a given question generated by the question encoder. The post-ranker takes the initially retrieved top $K$ passage embeddings and question embedding as input, and selects the top $T$ of the newly ranked passages as output. The outputted passages are respectively concatenated with the given question, and the concatenations are fed into the reader to find the final answer. 
The answer score is decided by the post-ranker score and reader score, which would be introduced in details in Section~\ref{subsec:inference}. 
The retriever is pre-trained with a KL-divergence based regularization. Then the passage encoder of it is frozen and the question encoder can be fine-tuned with the other two components in the next joint training process in an end-to-end manner.
% All the three components are learnable in the end-to-end manner.
In the following part, we would describe the procedures of pre-training, joint training, and inference, along with the details of each component.

% The details of each component would be described in the following part.

% \vspace{-0.15in}
\subsection{KL-Divergence based Regularization for Retriever Pre-training}\label{sec:kl}
% 原先模型存在的问题
% 引出对比学习
% So, we introduce contrast learning to improve the accuracy of mapping functions from feature space to semantic space.
% 下标k是指当前对话是第k轮，下标i是指在一批训练样本中的第i个
% \textcolor{red}{Introduction: Considerable previous works apply the dual-encoder architecture to obtain the initial retrieval results from the large corpus of articles, including ORConvQA, which only uses the rewrite form for each question in the pre-training phase and mitigates the mismatch problem by fine-tuning the question encoder in the concurrent learning phase~\cite{qu2020open}. However, the input questions significantly influence the offline representation learning for the Wikipedia passages and the retrieval results, which could not be revised in the later joint training phase.} 
In order to improve the capability of question understanding, we propose to exploit both the original questions and question rewrites in retriever pre-training. Specifically, a regularization mechanism is proposed to force two distributions of the retrieved results generated by feeding the original questions and their rewrites into the dual-encoder to be consistent with each other by minimizing the bidirectional KL divergence between them.
% hard negative: Strengthen the contrast

Concretely, given the current question $q_c$ and its historical question-answer pairs $\{q_i, a_i\}_{i=1}^{c-1}$, we form the original question by concatenating the question-answer pairs in a history window of size $w$ with the current question. Moreover, to mitigate the issue of underspecified and ambiguous initial questions, the initial question $q_1$ is invariably considered as it makes the constructed original question closer to the question rewrite. For the question encoder, the input sequence of original question can be defined as 
$q_c^{or} = \verb|[CLS]|\;q_1 \;\verb|[SEP]|\;a_1    \;\verb|[SEP]|\;q_{c-w}\verb|[SEP]|\;a_{c-w}\;\verb|[SEP]| \cdots \verb|[SEP]|\;q_{c-1}\\
\verb|[SEP]|\;a_{c-1}\;\verb|[SEP]|\;q_c\;\verb|[SEP]|$.
% $q_c^{or} = \verb|[CLS]|\;q_1 \;\verb|[SEP]|\;q_{c-w}\;\verb|[SEP]| \cdots \verb|[SEP]|\;q_{c-1}\;\verb|[SEP]|\;q_c\;\verb|[SEP]|$.

Let $\mathcal{D} = \{(q_i^{or}, q_i^{rw}, p_i^+, p_{i,1}^-, ... ,  p_{i,n}^-)\}_{i=1}^m$ denote the training data that consists of $m$ instances. Each instance contains two forms of one question (\emph{i.e.,} original question and question rewrite), one positive passage, along with $n$ irrelevant passages $\{p_{i,j}^-\}_{j=1}^n$. These irrelevant passages used for training contain one hard negative of the given question and $n-1$ in-batch negatives which are the positive and negative samples of the other questions from the same mini-batch.
% are hard negatives of the given question. Similar to DPR~\cite{karpukhin2020dense}, the in-batch negatives which are the positive and negative samples of the other questions from the same mini-batch are also used for training.
% and at training time, we also use in-batch negatives similar to DPR\cite{karpukhin2020dense} which are the positive and negative samples of other questions.
As shown in Figure~\ref{fig:kl}, we feed the original question $q_i^{or}$ and question rewrite $q_i^{rw}$ to the question encoder $E_Q$, respectively. The derived question embeddings are matched with the passage embeddings outputted by the passage encoder $E_P$ via inner product.
We can obtain two distributions of the retrieved results through softmax operation, denoted as $P^{or}(p_i|q_i^{or})$ and $P^{rw}(p_i|q_i^{rw})$. In the retrieval pre-training phase, we try to regularize on the retrieved results by minimizing the bidirectional KL divergence between the two distributions,
% for the different forms of one question, 
which can be formulated as follows:
% For two forms of one question, we feed them to question encoder respectively. Specifically, one is the concatenation of history questions and current question, denoted as $q_k^{rt}$ = [CLS] $q_1$ [SEP] $q_{k-w} $[SEP] ... [SEP] $q_{k-1}$ [SEP] $q_k$ [SEP], another is the rewrite in CANARD of $q_k$ denoted as $q_k^{rw}$.And for passages, we feed them to passage encoder twice. Therefore, we can obtain two distributions of the model prediction, denoted as $P^{rw}(p_i^+|q_i^{rw})$ and $ P^{rt}(p_i^+|q_i^{rt}) $. Then, inspired by \cite{liang2021rdrop}, we try to regularize on the model predictions by minimizing the bidirectional Kullback-Leibler (KL) divergence between these two output distributions, which is:
\begin{small}
\begin{equation}%加*表示不对公式编号
\begin{aligned}
\mathcal{L}_{KL}^i 
= & \frac{1}{2}(\mathcal{D}_{KL}(P^{or}(p_i|q_i^{or}) || P^{rw}(p_i|q_i^{rw})) \\
& + \mathcal{D}_{KL}(P^{rw}(p_i|q_i^{rw}) || P^{or}(p_i|q_i^{or}))).
\end{aligned}
\end{equation}
\end{small}

For the main task of passage retrieval, we apply the widely used negative log likelilhood of the positive passage given two forms of one question as objective function:
% With the basic negative log-likelihood learning objective $L_{CE}^i$ of the two forward passes:
% \begin{equation}
% \begin{aligned}
% & \mathcal{L}_{NLL}^i = - \frac{1}{2}(\mathrm{log}P^{or}(p_i^+|q_i^{or}) + \mathrm{log}P^{rw}(p_i^+|q_i^{rw})),
% \end{aligned}
% \end{equation}
\begin{small}
\begin{equation}
% \belowdisplayskip= 3pt
\begin{aligned}
\mathcal{L}_{NLL}^i 
= & - \frac{1}{2}(\mathcal{L}_{NLL\_or}^i+\mathcal{L}_{NLL\_rw}^i)\\
=&- \frac{1}{2}(\mathrm{log}P^{or}(p_i^+|q_i^{or}) + \mathrm{log}P^{rw}(p_i^+|q_i^{rw})),
\end{aligned}
\end{equation}
\end{small}
\\where the probability of retrieving the positive passage can be calculated as:
\begin{small}
\begin{equation}%加*表示不对公式编号
 P^{or}(p_i^+|q_i^{or}) 
 = \frac{\mathrm{exp}(\mathrm{sim}(q_i^{or}, p_i^+))}{\mathrm{exp}(\mathrm{sim}(q_i^{or}, p_i^+))+\sum_{j=1}^n \mathrm{exp}(\mathrm{sim}(q_i^{or}, p_{i,j}^-))},
\end{equation}
\end{small}
\begin{small}
\begin{equation}%加*表示不对公式编号
% \belowdisplayskip= 1.5pt
 P^{rw}(p_i^+|q_i^{rw})
 = \frac{\mathrm{exp}(\mathrm{sim}(q_i^{rw}, p_i^+))}{\mathrm{exp}(\mathrm{sim}(q_i^{rw}, p_i^+))+\sum_{j=1}^n \mathrm{exp}(\mathrm{sim}(q_i^{rw}, p_{i,j}^-))}.
\end{equation}
\end{small}
\\The probability of retrieving those negative passages can be obtained in the same manner.

To pre-train the retriever, 
% especially the passage encoder, 
the training objective is to minimize the pre-retrieval loss $\mathcal{L}^i_{pre}$ for data ($q_i^{or}$, $q_i^{rw}$, $p_i^+$, $p_{i,1}^-$, ... ,  $p_{i,n}^-)$:
\begin{equation}%加*表示不对公式编号
% \begin{aligned}
\mathcal{L}^i_{pre} = \mathcal{L}_{NLL}^i + \alpha \mathcal{L}_{KL}^i,
% \end{aligned}
\end{equation}
where $\alpha \in [0, 1]$ is a hyperparameter used  to control $\mathcal{L}_{KL}^i$.
\begin{figure}[t]
    \centering
    % \includegraphics[width=12cm]{figure/pretrain_v4.pdf}
    \includegraphics[width=\linewidth]{figure/pretrain_v4.pdf}
    \vspace{-0.2in}
    \caption{Retriever pre-training model with KL-divergence based regularization. Two forms of one question are fed into question encoder while the related positive and hard negative are fed into passage encoder. The positive or negative passages of the other questions in the same batch are used as negatives.}
    \label{fig:kl}
    \vspace{-0.1in}
\end{figure}

% \vspace{-0.05in}
\subsection{Joint Training with Curriculum Learning}
With the passage encoder and question encoder of the retriever pre-trained, our {\modelname} jointly trains the question encoder, post-ranker, and reader with a designed curriculum learning strategy.

% \vspace{-0.05in}
\subsubsection{Retriever Loss}\label{subsec:retriever}
The pre-trained passage encoder is used to embed the open-domain passages offline and obtain a set of passage representations. 
Given the current question $q_c$, and its historical question-answer pairs $\{q_i, a_i\}_{i=c-w}^{c-1}$ within a window size $w$, the input for question encoder is constructed by concatenating the question-answer pairs with the current question as mentioned in Section~\ref{sec:kl}. With the prepared offline passage representations and outputs of question encoder, each passages would be assigned with a retrieval score computed by the inner product operation as shown in Equation~\ref{eq:inner}. We select the top $K$ passages with high retrieval scores for the \textcolor{black}{{\rerankname} and reader}.

For each question, the reformatted question for the question encoder is denoted as $q_i^{en}$.
The question encoder of the retriever is fine-tuned by optimizing the following retrieval loss:
\begin{equation}\label{eq:retriever}
    \mathcal{L}^i_{retriever}=-\mathrm{log}\frac{\mathrm{exp}(\mathrm{sim}(q_i^{en},p_i^+))}{\sum_{j=1}^{K}\mathrm{exp}(\mathrm{sim}(q_i^{en},p_{i,j}))},
\end{equation}
where $p_{i,j}$ denotes the retrieved passages regarding question $q_i$.

For a fair comparison, we omit the answers from the concatenated question in the experiment to keep the same setting with the previous work~\cite{qu2020open}.
\subsubsection{Post-Ranker with Contrastive Loss}
Since only the first $T$ of the top $K$ passages outputted by the retriever module are fed into the reader for answer extraction, a {\rerankname} is proposed to re-rank the $K$ passages to push more relevant passages be ranked in the top $T$ placement.
We build the {\rerankname} by adding a subsequent network after the pre-trained passage encoder, which takes the embeddings of $K$ passages as input. With the subsequent network, {\rerankname} is able to learn high-level feature representations for passages.
In this work, we use the linear layer as the subsequent network for simplicity. The output for the retrieved passage $p_{i,j}$ and the ranking score can be formulated as follows:
\vspace{-0.1in}
\begin{equation}
    \mathbf{d}_{i,j} = \mathrm{LinearLayer}(E_P(p_{i,j})),
\end{equation}
% \vspace{-0.1in}
\begin{equation}\label{eq:score_post}
    S_{post}(q_i^{en},p_{i,j})=\mathbf{d}_{q_i}^{\mathrm{T}}\mathbf{d}_{i,j},
\end{equation}
where $\mathbf{d}_{q_i}$ is the representation for question $q_i$ generated by the question encoder $E_Q$. $S_{post}(\cdot)$ refers to the scores generated by the {\rerankname} network.
% with $\theta$ which includes the set of parameters of the linear layer. 

To simultaneously fine-tune the question encoder and {\rerankname}, we apply the modified hinge loss combined with a distance-based contrastive loss to pose constraints for passage reranking from two aspects. The hinge loss function~\cite{santos2016attentive} is defined as: 
\begin{small}
\begin{equation}\label{eq:rerank}
    \mathcal{L}^i_{ranker}=\mathrm{max}\{0,\delta-S_{post}(q_i^{en},p_i^+)+\mathrm{max}_j\{S_{post}(q_i^{en},p_{i,j}^-)\}\},
\end{equation}
% \belowdisplayskip= 0.5pt
\end{small}
where $\delta$ is the margin of the hinge loss, $p_i^+$ and $p_{i,j}^-$ denote the positive passage and negative passages retrieved for $q_i$. 
% $\mathbf{d}_i^+$ denotes the positive passage.
For contrastive learning, we uses the triplet margin loss~\cite{weinberger2006distance} to measure the distance between positive and negative samples as follows: 
\vspace{-0.05in}
\begin{equation}\label{eq:contrastive}
    \mathcal{L}^i_{cl}=\mathrm{max}\{0,\mu+D(\mathbf{d}_{q_i},\mathbf{d}^+_i)-D(\mathbf{d}_{q_i},\mathbf{d}_{i,j}^-)\},
    \vspace{-0.05in}
\end{equation}
where $\mu$ is the margin of the triplet margin loss. We apply Euclidean distance $D$ to compute the distance between the question and passage in the representation space. The final {\rerankname} loss is defined as:
\vspace{-0.1in}
\begin{equation}\label{eq:contrastive}
    \mathcal{L}^i_{postranker}= \mathcal{L}^i_{ranker}+\beta\mathcal{L}^i_{cl},
    \vspace{-0.05in}
\end{equation}
where $\beta$ is hyperparameter that need to be determined.

\subsubsection{Reader Loss}
The neural reader of our {\modelname} is designed for predicting an correct answer to the given question. Given the top $T$ retrieved/reranked passages, the reader intends to detect an answer span from each passage with a span score. Moreover, a passage selection score is assigned to each passage. The passage selection is used to select the passages that contains the true answer of the given question~\cite{lin2018denoising,karpukhin2020dense}, which performs as a contraint to facilitate the training of reader. The span with the highest passage selection score and span score is extracted as the final answer. 
% The passage selection is used to select the passages that contains the answer of the given question by measuring the probability distribution over all the retrieved passages.
% as a reranker to reselect the passages from a small number

Our reader applies the widely used pre-trained model, such as BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta}. Given a question $q_i$ and the top $T$ previously retrieved passages, we form the question input in the same way as that for retriever described in Section~\ref{subsec:retriever}. The only difference is that the initial question $q_1$ is left out as the setting of~\cite{qu2020open}. We denote the reconstructed question as $q_i^{rd}$ and concatenate it with a retrieved passage to form the sequence as the input for the pre-trained model. Specifically, let $\mathbf{z}_{i,j,t}$ be the outputted token-level representations of the $t$-th token in the passage $p_{i,j}$ which is retrieved for question $q_i^{rd}$. $\mathbf{z}_{i,j,[\mathrm{cls}]}$ denotes the sequence-level representation for the input sequence. The scores for the $t$-th token being the start and end tokens, and a passage being selected are defined as follows:
\begin{equation}\label{eq:score_reader1}
    S_{s}(q_{i}^{rd},p_{i,j},[t]) = \mathbf{z}_{i,j,t}^\mathrm{T}\mathbf{w}_{start},
\end{equation}
\begin{equation}\label{eq:score_reader2}
    S_{e}(q_{i}^{rd},p_{i,j},[t]) = \mathbf{z}_{i,j,t}^\mathrm{T}\mathbf{w}_{end},
\end{equation}
\begin{equation}\label{eq:score_select}
    S_{select}(q_{i}^{rd},p_{i,j})=\mathbf{z}_{i,j,[\mathrm{cls}]}^\mathrm{T}\mathbf{w}_{select},
\end{equation}
% \begin{align} 
%  S_{s}(q_{i}^{rd},p_{i,j},[t]) &= \mathbf{z}_{i,j,t}^\mathrm{T}\mathbf{w}_{start}, \label{eq:score_reader1}\\ 
%  S_{e}(q_{i}^{rd},p_{i,j},[t]) &= \mathbf{z}_{i,j,t}^\mathrm{T}\mathbf{w}_{end}, \label{eq:score_reader2}\\
%  S_{select}(q_{i}^{rd},p_{i,j})&=\mathbf{z}_{i,j,[\mathrm{cls}]}^\mathrm{T}\mathbf{w}_{select}, \label{eq:score_select}
% \end{align}
where $\mathbf{w}_{start}$, $\mathbf{w}_{end}$, and $\mathbf{w}_{select}$ are trainable vectors.

The loss function for the start token prediction is defined as:
\begin{equation}
    \mathcal{L}^i_{start}=-\mathrm{log}\frac{\mathrm{exp}(S_s(q_i^{rd},p_i^+,[st_i^+]))}{\sum_{j=1}^{M}\sum_{[t]\in p_{i,j}}\mathrm{exp}(S_s(q_i^{rd},p_{i,j}, [t]))},
\end{equation}
where $[st_i^+]$ denotes the start token of the true answer in the golden passage $p_i^+$.
% and $[t]$ is the token in the top $T$ retrieved passages.
The objective function for the end token prediction denoted as $\mathcal{L}^i_{end}$ is defined in the same manner. In addition, the passage selecting loss is computed as follows:
\begin{equation}
    \mathcal{L}^i_{select}=-\mathrm{log}\frac{\mathrm{exp}(S_{select}(q_i^{rd},p_i^+))}{\sum_{j=1}^{M} \mathrm{exp}(S_{select}(q_i^{rd},p_{i,j}))}.
\end{equation}
Finally, the loss function of the reader is defined as follows:
\begin{equation}\label{eq:reader}
    \mathcal{L}_{reader}^i = \frac{1}{2}(\mathcal{L}_{start}^i+\mathcal{L}_{end}^i)+\mathcal{L}_{select}^i.
\end{equation}

\subsubsection{Joint Training with Curriculum Learning Strategy}
% \subsubsection{Simple Curriculum Learning Method}
We propose a semi-automatic curriculum learning (CL) strategy consisting of two core components: an automatic difficulty measurer and a discrete training scheduler, to improve the joint training of retriever, post-ranker, and reader.
The CL strategy aims at reducing the chance of adding golden passage during the joint training process to help the settings of training and inference be more consistent. 
It would effectively push the retriever to find the passage with the true answer contained by itself and encourage the reader to discover the correct answer without the assistance of golden passage.
To the best of our knowledge, this is the first work that designs CL strategy for conversational OpenQA joint learning.
% reduce the dependency on the golden passage in both training and inference phase.

% A general framework of curriculum learning consists of two core components: difficulty measurer and training scheduler. 
Specifically, our semi-automatic CL strategy automates the difficulty measurer by taking the question-wise training loss as criteria. A higher retriever loss is recognized a harder mode for the retriever to discover the positive passages. For the hard mode, the training scheduler of our CL strategy introduces the golden passage in the retrieval results to make the learning easier. 
Our training scheduler is like a Baby Step scheduler~\cite{bengio2009curriculum,spitkovsky2010baby} with a finer granularity as shown in Algorithm~\ref{alg:TS}.


% We use curriculum learning to reduce the chance of golden passage during concurrent learning. \cite{qu2020open} implies that golden paragraphs can be accessible during the training process to avoid poor training outcomes owing to failure to retrieve important articles. We will progressively diminish the likelihood of introducing golden passage to lessen the importance of golden passage.

% We present and compare two curriculum learning methods: decreasing the likelihood with the training step and decreasing the probability with the retriever training loss. First, we reduce the probability of adding golden passage with the training step, starting from 1 to 0. Second, we reduce the probability of adding golden passage with the retriever loss, when the loss of the retriever is high, we think it is necessary to add additional golden passage.

Formally, for each iteration $l$, let $\mathcal{P}^{(l)}_K=\{\mathcal{P}^{(l)}_i|q_i\in Q^{(l)}\}$ be the set of all the collections of top $K$ passages retrieved by the retriever to questions of the batch in the iteration. For the hard mode, the training scheduler brings the golden passage of each question to $\mathcal{P}^{(l)}_K$ to form the set of retrieved passages with the positive one, denoted as $\mathcal{P}^{(l)}_{KG}=\{\mathcal{P}^{(l)}_i\cup\{p^+_i\}|q_i\in Q^{(l)}\}$. With these retrieved passages, the final loss to be optimized is defined as:
\begin{equation}\label{eq:final}
    \mathcal{L}^{(l)}_{final} = v^{(l)}\mathcal{L}^{(l)}(\mathcal{P}_{KG}^{(l)})+(1-v^{(l)})\mathcal{L}^{(l)}(\mathcal{P}^{(l)}_{K}),
\end{equation}
where $v^{(l)}$ is determined by the difficulty measurer with the retriever loss of the previous iteration:
\begin{equation}\label{eq:v}
    v^{(l)}=\left\{
    \begin{array}{ccl}
     1,  & & {\mathcal{L}^{(l-1)}_{retriever}>\lambda_{upper}}, \\
     0,  & & {\mathcal{L}^{(l-1)}_{retriever}<\lambda_{lower}}, \\
     I(p_b), & & {\text{otherwise}},
    \end{array} \right.
\end{equation}
% \begin{equation}
% v^{(l)}=\left\{\begin{matrix}
%   1, & \mathcal{L}^{(l-1)}_{retriever}<\lambda\\
%   \\
% \end{matrix}\right.
% \end{equation}
where $\lambda_{upper}$ and $\lambda_{lower}$ are the pre-defined upper and lower thresholds. $I(p_b)$ denotes an indicator function whose output is sampled from a Bernoulli distribution:
\begin{equation}
    I(p_b)=\left\{
    \begin{array}{ccl}
     1,  & & {\text{with probability}\ p_b}, \\
     0,  & & {\text{with probability}\ 1-p_b}, 
    \end{array} \right.
\end{equation}
where the probability is evaluated by the min-max normalization denoted as $p_b=\frac{ \mathcal{L}^{(l-1)}_{retriever}-\lambda_{lower}}{\lambda_{upper} - \lambda_{lower}}$, which measures the degree of approximating upper threshold for the retriever loss.
It can be intuitively explained that if the retrieval loss of the previous iteration is close to an upper threshold $\lambda_{upper}$, the training status is regarded in the hard mode.
And the value of $v^{(l)}$ is assigned with 1 in a higher probability to select the set of retrieved passages with golden passage for training in the current iteration.

% It can be intuitively explained that if the retrieval loss of the previous iteration is greater than the threshold $\lambda$, then the training status is regarded in the hard mode. And the value of $v^{(l)}$ is assigned with 0 to select the set of retrieved passages with golden passage for the training in the current iteration.
$\mathcal{L}^{(l)}(\cdot)$ in Equation~\ref{eq:final} denotes the objective function integrated by all the three modules with the retrieved passages in the $l$-th iteration as follows:
\begin{equation}
    \mathcal{L}^{(l)} = \mathcal{L}^{(l)}_{retriever} + \mathcal{L}^{(l)}_{postranker}+ \mathcal{L}^{(l)}_{reader}.
\end{equation}
The loss of each module is averaged over the questions in the $l$-th iteration.
% where $v_i$ is decided by the retriever loss of the query example in the previous mini-batch. 
 \input{algorithm}


% \begin{algorithm}[t]
% \LinesNumbered
% \KwIn{}
% \KwOut{A set of optimal parameters $\Theta$.}
% \For{$t<T$}{
% % Sample users for episode $\mathcal{U}_{epi}^t\leftarrow \mathrm{RandomSample}(\mathcal{U}_{train},N)$\\  
% % \ForEach{user $i\in\mathcal{U}_{epi}^t$}{
% %     % Select support instances \textcolor{red}{$\mathcal{S}_i\leftarrow \mathrm{RandomSample}(\{\},K)$}\\
% %     Select $K$ support instances to form $\mathcal{S}_i$ \\
% %     % Select query instances \textcolor{red}{$\mathcal{Q}_i\leftarrow \mathrm{RandomSample}(,M)$}\\
% %     Select $M$ query instances to form $\mathcal{Q}_i$ \\
% %     Compute representations $\mathbf{u}_{i,j}$ for review instances in $\mathcal{S}_i$ \\
% %     Estimate attentive weights $\alpha_{j,k}^i$ for review instances in $\mathcal{S}_i$\\
% %     Compute user prototype $\mathbf{p}_i \leftarrow \frac{1}{|\mathcal{S}_i|}\sum_{j\in\mathcal{S}_i}\hat{\mathbf{u}}_{i,j}$\\
% %     \ForEach{item $j\in\mathcal{Q}_i$}{
% %     Compute textual representation $\mathbf{q}_{j}^i$\\
% %     Compute memory-based representation $\mathbf{v}_j^i$\\
% %     Compute review-based rating according to Eq. (\ref{eq:rating})\\
% %     }
% % }
% % Minimize training loss according to Eq. (\ref{eq:loss})\\
% }

% \Return{$\Theta$}
   
% \caption{{\bf Training Scheduler}}
% \label{algorithm}
% \end{algorithm}
% \vspace{-0.05in}
\subsection{Inference}\label{subsec:inference}
% We follow the inference process of~\cite{qu2020open}. 
In the inference stage, for a given question $q_c$ and its historical question-answer pairs $\{q_i, a_i\}_{i=c-w}^{c-1}$ within a window size $w$, we first obtain a collection of the top $T$ relevant passages through the two consecutive modules, namely retriever and {\rerankname}. For each passage $p_{c,j}$ in the collection, we get the {\rerankname} score $S_{post}(q_c^{en},p_{c,j})$ according to Equation~\ref{eq:score_post}. The reader score includes two parts, one is the select score $S_{select}(q_{c}^{rd},p_{c,j})$ calculated by Equation~\ref{eq:score_select}, the other one is the span score which consists of the start score and end score as shown in Equations~\ref{eq:score_reader1} and~\ref{eq:score_reader2}. The span score can be obtained by:
\begin{footnotesize}
\begin{equation}
    S_{span}(q_{c}^{rd},p_{c,j},sp)=\max\limits_{sp\in p_{c,j}}\{ S_{s}(q_{c}^{rd},p_{c,j},[t_s])+ S_{e}(q_{c}^{rd},p_{c,j},[t_e])\},
\end{equation}
\end{footnotesize}
where $sp=([t_s],[t_e])$ is the answer span starting with token $[t_s]$ and ending with token $[t_e]$. Following the previous work~\cite{kenton2019bert,qu2020open},  we pick out the top 20 spans and discard the invalid predictions including the cases where the start token comes after the end token, or the predicted span overlaps with the question part of the input sequence. The final prediction score is defined as follows:
% \begin{equation}
%     S(q_{c}^{en},q_{c}^{rd},p_{c,j},sp) = S_{\theta}(q_c^{en},p_{c,j}) + S_{reader}(q_{c}^{rd},p_{c,j}) + S_{span}(q_{c}^{rd},p_{c,j},sp),
% \end{equation}
\begin{equation}
    S(q_{c}^{en},q_{c}^{rd},p_{c,j},sp) = S_{post}(q_c^{en},p_{c,j}) + S_{reader}(q_{c}^{rd},p_{c,j}, sp),
\end{equation}
where $S_{reader}(q_{c}^{rd},p_{c,j},sp) = S_{select}(q_{c}^{rd},p_{c,j}) + S_{span}(q_{c}^{rd},p_{c,j},sp)$. For the given question $q_c$ in a conversation, the answer span $sp$ in the retrieved passage $p_{c,j}$ that has the largest score is the predicted answer. 