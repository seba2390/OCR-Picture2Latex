\section{Experiments}\label{sec:exp}
\subsection{Dataset}
To evaluate the effectiveness of the proposed {\modelname}, we conduct comprehensive experiments on the public available dataset OR-QuAC~\cite{qu2020open}, which integrates three datasets including the QuAC~\cite{choi2018quac}, CANARD~\cite{elgohary2019can}, and the Wikipedia corpus.
OR-QuAC consists of totally 5,644 conversations containing 40,527 questions and answers with the rewrites of questions obtained from CANARD. The question rewrites support the \textcolor{black}{KL divergence-based regularization} for pretraining a better passage retriever. 
OR-QuAC also provides a collection of more than 11 million passages obtained from the English Wikipedia dump from 10/20/2019\footnote{ttps://dumps.wikimedia.org/enwiki/20191020/} for open-retrieval.
Table \ref{tab:dataset} summarizes the statistics of the aggregated OR-QuAC dataset. 
% We conduct experiments on the public available dataset OR-QuAC to verify our proposed {\modelname}. 
% We aim to answer the following research questions to explore the \textcolor{red}{insight} of {\modelname}:

% \begin{itemize}
%     \item \textbf{RQ1}: How does {\modelname} perform compared with the state-of-the-art methods for conversational open-domain QA task?
%     \item \textbf{RQ2}: How do different components in {\modelname} affect the performance?
%     \item \textbf{RQ3}: What are the influences of different settings of hyper-parameters?
%     % \item \textbf{RQ4}: How is the generalization capacity of {\modelname}?
%     % \item \textcolor{black}{\textbf{RQ4}: How do the representations benefit from {\modelname}?}
% \end{itemize}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]\label{tab:data}
  \centering
  \caption{Data statistics of the OR-QuAC dataset.}
    \begin{tabular}{llccc}
    \toprule
          & Items & Train & Dev   & Test \\
    \midrule
    \multirow{5}[2]{*}{Coversations} & \# Dialogs & 4,383 & 490   & 771 \\
          & \# Questions / Rewrites & 31,526 & 3430  & 5571 \\
          & \# Avg. Questions / Dialog & 7.2   & 7     & 7.2 \\
          & \# Avg. Tokens / Question & 6.7   & 6.6   & 6.7 \\
          & \# Avg. Tokens / Rewrite & 10    & 10    & 9.8 \\
    \midrule
    Wikipedia  & \# Passages & \multicolumn{3}{c}{11 million} \\
    \bottomrule
    \end{tabular}%
  \label{tab:dataset}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'joint training'
\renewcommand{\arraystretch}{1.2}
\begin{table*}[t]
  \centering
  \caption{Performance comparison of {\modelname} and baseline models. The number in the parentheses is the batch size during the retriever pre-training.}
  \vspace{-0.1in}
     \begin{tabular}{p{10.5em}cccccccccc}
    \toprule
    \multirow{2}[4]{*}{Methods} & \multicolumn{5}{c}{Development}                       & \multicolumn{5}{c}{Test} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11}     \multicolumn{1}{c}{} & F1    & HEQ-Q & HEQ-D & Rt MRR & Rt Recall & F1    & HEQ-Q & HEQ-D & Rt MRR  & Rt Recall \\
    \midrule
    DrQA~\cite{chen2017reading}  & 4.5   & 0.0   & 0.0   & 0.1151   & 0.2000  & 6.3   & 0.1   & 0.0   & 0.1574   & 0.2253 \\
    BERTserini~\cite{yang2019end} & 19.3  & 14.1  & 0.2   & 0.1767  & 0.2656  & 26.0  & 20.4  & 0.1   & 0.1784   & 0.2507 \\
    DPR (16)~\cite{karpukhin2020dense} & 25.9  & 16.4  & 0.2   & 0.3993  & 0.5440  & 26.4  & 21.3  & 0.5   & 0.1739  & 0.2447  \\
    \midrule
    % \multicolumn{13}{c}{ORConvQA (16$\times$4)~\cite{qu2020open}} \\
    % \midrule
    % -w/o hist & 24.0  & 15.2  & 0.2   & 0.4012  & 0.4472  & 0.5271  & 26.3  & 20.7  & 0.4   & 0.1979  & 0.2702  & 0.2859  \\
    ORConvQA-bert (64)~\cite{qu2020open} & 26.9  & 17.5  & 0.2   & 0.4286   & 0.5714  & 29.4  & 24.1  & 0.6   & 0.2246  & 0.3141  \\
    ORConvQA-roberta (64) & 26.5 & 17.8 & 0.2 &	0.4284  & 0.5624 & 	28.7 &	24.2 &	0.8 & 0.2330  & 0.3226 \\
    \midrule
    % \multicolumn{13}{c}{ours (16)}\\
    % \midrule
    ours-bert (16)  & 28.0 &	19.4 &	0.2 & 0.4639  & 0.6157 & 31.7 & 27.8 &	1.2 & 0.2763  & 0.3668  \\
    ours-roberta (16) & 28.1 & 19.5 & \textbf{0.4} & 0.4639 & 0.6169 & 33.4 & 29.4 & 1.7 & 0.2887  & 0.3819\\
    %  ours-roberta (16) & \textbf{28.1} & \textbf{19.5} & \textbf{0.4} & \textbf{0.4639} & \textbf{0.6169} & \textbf{33.4} & \textbf{29.4} & \textbf{1.7} & \textbf{0.2887}  & \textbf{0.3819} \\
    % \midrule
    % \multicolumn{13}{c}{ours (32)}\\
    % \midrule
    ours-bert (32) & 27.6 &	19.6 &	0.0 & \textbf{0.4675}  & 0.6236 & 32.6 & 29.1 & 0.8 & 0.3013  & 0.4130  \\
    ours-roberta (32)& \textbf{29.4}  & \textbf{20.2}  & \textbf{0.4}   & 0.4656   & \textbf{0.6248}  & \textbf{35.0}  & \textbf{30.8}  & \textbf{1.8}   & \textbf{0.3073}  & \textbf{0.4202}  \\
    \bottomrule
    \end{tabular}%
  \label{tab:overall}%
\end{table*}%


\vspace{-0.1in}
\subsection{Experimental Settings}

\subsubsection{Evaluation Metrics}
Following the evaluation protocols used in~\cite{qu2020open}, we apply the word-level F1 and the human equivalence score (HEQ) that are provided by the QuAC challenge~\cite{choi2018quac} to evaluate our {\modelname}. As the core evaluation metric for the overall performance of answer retrieval, F1 is computed by considering the portion of words in the prediction and groud truth that overlap after removing stopwords.
HEQ is used to judge whether a system's output is as good as that of an average human. It measures the percentage of examples for which system F1 exceeds or matches human F1. Here two variants are considered: the percentage of questions for which this is true (HEQ-Q), and the percentage of dialogs for which this is true for every questions in the dialog (HEQ-D).
Furthermore, we use another two metrics, Mean Reciprocal Rank (MRR) and Recall for the evaluation of the retrieval performance. MRR reflects the abilities of post-ranker to return the passages containing true answers in a high place.
Recall is indicative of post-ranker's capability of providing relevant passages for the next modules. 
For the sake of fairness, we follow ~\cite{qu2020open} and calculate the two metrics for the top $T$ passages that are retrieved for the reader.
% MRR is used to evaluate the outputs of post-ranker and the selection part 

% Furthermore, we use another two metrics, Mean Reciprocal Rank (MRR) and Recall for the evaluation of the \textcolor{red}{retriever and reranker}. Specifically, MRR is used to evaluate both the retriever and reranker. The Reciprocal Rank (RR) calculates the reciprocal of the rank at which the first positive passage is retrieved. MRR is obtained by averaging the RRs across all the queries. 
% Recall is defined as the fraction of the passages that are relevant to the query that are successfully retrieved.
% % the ratio of the number of retrieved relevant passages to the number of 
% Recall is only used to evaluate the retriever only as 
% ranking has no effect on it. 
% MRR reflects the abilities of retriever to return the passages containing true answers in a high placing.
% Recall is indicative of retriever's capability of providing relevant passages for the next modules. 
% For the sake of fairness, we follow ~\cite{qu2020open} and calculate the two metrics for the top 5 passages that are retrieved for the reader and reranker.

\vspace{-0.1in}
\subsubsection{Implementation Settings}
\begin{enumerate}
    \item \textbf{Retriever Pretraining}. As question and passage encoders, we employ two ALBERT Base models. The maximum sequence length for the question encoder is 128, while the maximum length for the passage encoder is 384. The models are trained on NVIDIA TITAN X GPU. The training batch size is set to 16, the number of training epochs is set to 12, the learning rate is set to 5e-5, the window size $w$ is set to 6 and the coefficient of KL divergence $\alpha$ is set to 0.2. Every 5,000 steps, we save checkpoints and assess on the development set provided by \cite{qu2020open}. Then we select several models that performed well on the development set and apply them on test questions and wiki passages. Finally, we select the best model for future training.
    \item \textbf{Post-ranker and Reader}. For the post-ranker, we utilize a linear layer with a size of 128 for simplicity. The number $K$ of passage embeddings it takes as input is set to 100, and the number $T$ of its outputted passages for reader is set to 5. For the reader, we apply the BERT and RoBERTa model. The max sequence length is assigned with 512. The sequence is concatenated by a question and a passage. The maximum passage length is set to 384, with the remainder reserved for the question and other tokens such as  \verb|[CLS]| and  \verb|[SEP]|.
    \item \textbf{Joint Training}. We use the pre-trained passage encoder to compute an embedding for each passage, and build a single MIPS index using FAISS\cite{JDH17} for fast retrieval. Models are jointly trained on NVIDIA TITAN X GPU. 
    The training batch is set to 2, the number of epochs is 3, the learning rate is 5e-5, and the optimizer is Adam.
    % For post-ranker, the margin of hinge loss $\delta$ and triplet margin loss $\mu$ are set to 2 and 0.5, the coefficient of contrastive loss $\beta$ is set to 0.8. For curriculum learning, the pre-defined threshold $\lambda_{upper}$ and $\lambda_{lower}$ are respectively set to 3 and 1. 
    % For all the variant of our {\modelname}, 
    The hyper-parameters including the margin of hinge loss $\delta$, triplet margin loss $\mu$, coefficient of contrastive loss $\beta$, and pre-defined threshold $\lambda_{upper}$ and $\lambda_{lower}$ are tuned based on the development set to select the optimal model for inference.
    We save checkpoints and evaluate on the development set every 5,000 steps, and then select the best model for the test set.
\end{enumerate}

\vspace{-0.1in}
\subsubsection{Baselines}
To demonstrate the effectiveness of our proposed {\modelname}, we compare it with the following state-of-the-art question answering models, including three representative OpenQA models (DrQA, BERTserini, and DPR) and one conversational OpenQA model (ORConvQA):
\begin{itemize}
    \item \textbf{DrQA}~\cite{chen2017reading} is composed of a document retriever which uses bigram hashing and TF-IDF matching to return the relevant passages for a given question, and a multi-layer RNN based document reader for answer spans detection in those retrieved passages.
    \item \textbf{BERTserini}~\cite{yang2019end} uses a BM25 retriever from Anserini\footnote{http://anserini.io/} and a BERT reader to tackle end-to-end question answering. The retriever directly identifies segments of open-domain texts and pass them to the reader. Compared to DPR, ORConvQA and our {\modelname}, it has no selection loss in the reader and benefits less from the joint learning.
    \item \textbf{DPR}~\cite{karpukhin2020dense} increases retrieval by learning dense representations instead of using typical IR methods. It innovatively proposes to introduce hard negatives in the training process of the retriever. For OpenQA, DPR consists of a dual encoder as a retriever and BERT as a reader. 
    \item \textbf{ORConvQA}~\cite{qu2020open} is first proposed to solve the conversational open-domain QA problem with the retriever, rerank, and reader pipeline. This is the key work to compare for our {\modelname}. 
    % We consider to compare with ORConvQA and its variant with the history window size assigned as 0 (ORConvQA w/o hist.).
\end{itemize}

The results of all the baselines except DPR come from the previous work~\cite{qu2020open}. The implementation setting of DPR is the same with ORConvQA, including the encoder network, window size, learning rate, and so on.

% % Table generated by Excel2LaTeX from sheet 'joint training'
% \renewcommand{\arraystretch}{1.2}
% \begin{table*}[t]
%   \centering
%   \caption{Add caption}
%   \normalsize
%     \begin{tabular}{p{8.2em}cccccccccccc}
%     \toprule
%     \multirow{2}[4]{*}{Methods} & \multicolumn{6}{c}{Development}                       & \multicolumn{6}{c}{Test} \\
% \cmidrule(lr){2-7} \cmidrule(lr){8-13}     \multicolumn{1}{c}{} & F1    & HEQ-Q & HEQ-D & Rt MRR & Rr MRR & Rt Recall & F1    & HEQ-Q & HEQ-D & Rt MRR & Rr MRR & Rt Recall \\
%     \midrule
%     DrQA~\cite{chen2017reading}  & 4.5   & 0.0   & 0.0   & 0.1151  & N/A   & 0.2000  & 6.3   & 0.1   & 0.0   & 0.1574  & N/A   & 0.2253 \\
%     BERTserini~\cite{yang2019end} & 19.3  & 14.1  & 0.2   & 0.1767  & N/A   & 0.2656  & 26.0  & 20.4  & 0.1   & 0.1784  & N/A   & 0.2507 \\
%     DPR~\cite{karpukhin2020dense} & 25.9  & 16.4  & 0.2   & 0.3993  & 0.4926  & 0.5440  & 26.4  & 21.3  & 0.5   & 0.1739  & 0.2444  & 0.2447  \\
%     \midrule
%     % \multicolumn{13}{c}{ORConvQA (16$\times$4)~\cite{qu2020open}} \\
%     % \midrule
%     % -w/o hist & 24.0  & 15.2  & 0.2   & 0.4012  & 0.4472  & 0.5271  & 26.3  & 20.7  & 0.4   & 0.1979  & 0.2702  & 0.2859  \\
%     ORConvQA-bert & 26.9  & 17.5  & 0.2   & 0.4286  & 0.5209  & 0.5714  & 29.4  & 24.1  & 0.6   & 0.2246  & 0.3127  & 0.3141  \\
%     ORConvQA-roberta & 26.5 & 17.8 & 0.2 &	0.4284 & 0.5104 & 0.5624 & 	28.7 &	24.2 &	0.8 & 0.2330 &	0.3162 & 0.3226 \\
%     \midrule
%     % \multicolumn{13}{c}{ours (16)}\\
%     % \midrule
%     ours-bert  & 28.0 &	19.4 &	0.2 & 0.4639 &	0.5526 & 0.6157 & 31.7 & 27.8 &	1.2 & 0.2763 &	0.3538 & 0.3668  \\
%     ours-roberta  & \textbf{28.1} & \textbf{19.5} & \textbf{0.4} & \textbf{0.4639} & \textbf{0.5530} & \textbf{0.6169} & \textbf{33.4} & \textbf{29.4} & \textbf{1.7} & \textbf{0.2887} & \textbf{0.3744} & \textbf{0.3819} \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:overall}%
% \end{table*}%



\vspace{-0.1in}
\subsection{Overall Results}
The overall experimental results are reported in Table~\ref{tab:overall}. The results of the baseline models are public in~\cite{qu2020open} except for DPR. The retrieval metrics denoted by ``Rt MRR'', and ``Rt Recall'' are used to evaluate the retrieval results of retriever for baselines while evaluating that of post-ranker for {\modelname}.
Generally, our {\modelname} outperforms all the baseline models. In detail, several observations can be achieved:
\begin{enumerate}
    \item Both DrQA and BERTserini achieve poor performance. The primary reason is they use the sparse retriever which can not be fine-tuned in the downstream  reader training to discover relevant passages for answer extraction. DrQA performs rather badly in answer reading as it uses RNN-based reader which does not have the strong ability of representation learning as those pre-trained language model. 
    The reader of BERTserini is similar to the other compared methods except DrQA. But it benefits less from the multi-task training process as there is no select component (reranker) in reader. 
    Compared with DrQA and BERTserini, DPR improves the retriever with the dense representation learning. The performance of DPR is limited by the batch size of pre-training.
    
    \item As the first system designed for the task of conversational OpenQA, ORConvQA provides the best performance among the baselines. ORConvQA is similar with DPR, where the difference is that it does not use hard negatives for retriever pre-training while DPR uses. The main reason ORConvQA performs better is the batch size of retriever pre-training is assigned with 64 as it uses 4 GPUs and set batch size to 16 per GPU. Actually, with the same batch size, the retriever of DPR is stronger, detailed results of which can be seen in the further analysis of Section~\ref{subsec:fa1}. 
    \item Our {\modelname} gives the significantly better performance than ORConvQA, even though our retriever is pre-trained with batch size of 16. This indicates that our {\modelname} can perform better with the lower memory cost. When the batch size is increased to 32, the performance can be further improved. The results convincingly demonstrate the effectiveness of the multifaceted improvements of KL-divergence regularization based pre-training, the added post-ranker, and the semi-automatic curriculum learning strategy.
    \item To investigate the influence of the pre-trained language model of reader, we evaluate our {\modelname} and ORConvQA based on two language models, namely BERT and RoBERTa. For ORConvQA, the results obtained by using BERT and RoBERTa are comparable. For our {\modelname}, applying RoBERTa for reader effectively improves the performance compared to using BERT. Overall, {\modelname} achieves the best performance whether with BERT or RoBERTa.
\end{enumerate}


% \renewcommand{\arraystretch}{1.5} %控制行高
% \begin{table*}[htbp]

%   \centering
%   \begin{threeparttable}
%   \caption{Main evaluation results. “Rt” and “Rr” refers to “Retriever” and “Reranker”.}
%   \label{tab:performance_comparison}
%     \begin{tabular}{ccccccccccccc}
%     \toprule
%     \multirow{2}{*}{Methods}&
%     \multicolumn{6}{c}{Dev}&\multicolumn{6}{c}{Test}\cr
%     \cmidrule(lr){2-7} \cmidrule(lr){8-13} 
%     &Rt-R&Rt-M&Rr-M&H-Q&H-D&F1&Rt-R&Rt-M&Rr-M&H-Q&H-D&F1\cr
%     \midrule
%     DrQA&0.2000&0.1151&N/A&0.0&0.0&4.5&0.2253&0.1574&N/A&0.1&0.0&6.3\cr
%     BERTserini&0.2656&0.1767&N/A&14.1&{\bf 0.2}&19.3&0.2507&0.1784&N/A&20.4&0.1&26.0\cr
%     ORConvQA w/o hist&0.5271&0.4012&0.4472&15.2&{\bf 0.2}&24.0&0.2859&0.1979&0.2702&20.7&0.4&26.3\cr
%     ORConvQA&0.5714&0.4286&0.5209&17.5&{\bf 0.2}&26.9&0.3141&0.2246&0.3127&24.1&0.6&29.4\cr
%     ours&{\bf 0.6254}&{\bf 0.4668}&{\bf 0.5666}&{\bf 20.5}&{\bf 0.2}&{\bf 29.5}&{\bf 0.3968}&{\bf 0.2950}&{\bf 0.3866}&{\bf 29.2}&{\bf 0.9}&{\bf 33.8}\cr
%     \bottomrule
%     \end{tabular}
%     \end{threeparttable}
% \end{table*}


\vspace{-0.15in}
\subsection{Ablation Studies}
% Table generated by Excel2LaTeX from sheet 'ablation'
\begin{table}[t]
  \centering
  \caption{Performance of ablation on different components. {\modelname} refers to the full system.}
  \vspace{-0.1in}
    \begin{tabular}{clllll}
    \toprule
    \multicolumn{2}{l}{Settings} & {\modelname}  & \makecell[l]{w/o KL \\retriever} & \makecell[l]{w/o \\post-ranker} & \makecell[l]{w/o \\curriculum} \\
    \midrule
    \multirow{5}[2]{*}{Dev} & F1    & \textbf{28.1} & 27.7  & 27.8  & 27.8  \\
          & HEQ-Q & \textbf{19.5} & 19.4  & 19.4  & 18.3  \\
          & HEQ-D & \textbf{0.4} & 0.0   & 0.0   & 0.0  \\
          & Rt MRR & \textbf{0.4639} & 0.4033  & 0.4604  & 0.4560  \\
        %   & Rr MRR & \textbf{0.5530} & 0.4920  & 0.5525  & 0.5564  \\
          & Rt Recall & \textbf{0.6169} & 0.5324  & 0.6157  & 0.6140  \\
    \midrule
    \multirow{5}[2]{*}{Test} & F1    & \textbf{33.4} & 31.5  & 32.2  & 31.7  \\
          & HEQ-Q & \textbf{29.4} & 29.0  & 28.2  & 27.1  \\
          & HEQ-D & \textbf{1.7} & 1.4   & 1.6   & 0.8  \\
          & Rt MRR & \textbf{0.2887} & 0.2110  & 0.2810  & 0.2812  \\
        %   & Rr MRR & \textbf{0.3744} & 0.2966  & 0.3741  & 0.3708  \\
          & Rt Recall & 0.3819  & 0.2968  & \textbf{0.3830} & 0.3764  \\
    \bottomrule
    \end{tabular}%
    \vspace{-0.15in}
  \label{tab:ablation}%
\end{table}%
To investigate the effectiveness of three improved parts in our {\modelname}, we evaluate several variants of our system. As shown in Table~\ref{tab:ablation}, once we remove one of the three components, both the retrieval and QA performance generally decrease. The detailed observations are summarized as follows:
\begin{enumerate}
    \item When we remove the KL-based regularized pre-trained retriever and use the retriever of ORConvQA as replacement, the performance drops significantly, especially the retrieval performance. It shows the importance of the KL-based regularization in our pre-trained retriever.
    \item Removing the post-ranker also brings a degradation in the overall performance. The influence is slightly smaller than that of KL-based regularization, which is probably caused by the simple linear layer used for post-ranker. This is our limitation and we plan to explore the more flexible and effective neural network for post-ranker in future work. From another perspective, some improvements can be achieved just by adding a linear layer for further passage representation learning.
    It is notable that the retrieval recall is higher than the full system. It is mainly because each question in the test set has more than one golden passages. Retrieving more golden passages does not necessarily lead to the better answer span which has the higher coverage of the ground truth answer.
    \item The variant without curriculum learning in the joint training performs worse than the full {\modelname}. By comparison, the QA performance decreases more noticeably. The reason behind is that the curriculum learning strategy encourages the reader to find correct answer with no golden passage assistance at the joint training time. It makes the reader more suitable for answer extraction in the inference phase.
    % It makes the training process more consistent with the inference.
\end{enumerate}

\begin{table}[t]
  \centering
  \caption{Results of retriever pre-training. B is batch size, Q is the form of question used in training, HD is the number of hard negatives.}
    \vspace{-0.1in}
    \begin{tabular}{cccccc}
    \toprule
          & B & Q & HD & Recall@20 & Recall@100 \\
    \midrule
    BM25  & /     & /     & /     & 0.3711  & 0.5100  \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{ORConvQA }} & 16    & $q^{rw}$ & 0     & 0.1672  & 0.2916  \\
          & 16$\times$4  & $q^{rw}$ & 0     & 0.3561  & 0.5034  \\
    \midrule
    DPR   & 16    & $q^{rw}$ & 1     & 0.2395  & 0.3759  \\
    \midrule
    \multirow{2}[2]{*}{ours} & 16    & $q^{rw}$/$q^{rw}$ & 1     & 0.3214  & 0.4689  \\
          & 16    & $q^{rw}$/$q^{or}$ & 1     & 0.3690  & 0.5070  \\
          & 32    & $q^{rw}$/$q^{or}$ & 1     & 0.4675  & 0.5882  \\
    \bottomrule
    \end{tabular}%
    \vspace{-0.2in}
  \label{tab:resofpretraining}%
\end{table}%

\vspace{-0.1in}
\subsection{Further Analysis}
\subsubsection{ Retriever Performance}\label{subsec:fa1}
% Results of retriever pretraining. B is batch size, Q is the form of question used in training, HD is the number of hard negatives.  
We evaluate some existing pre-trained retriever with our KL-divergence regularized retriever on the full collection of Wikipedia passages. The evaluation results are reported in Table~\ref{tab:resofpretraining}. 
As the typical sparse retriever, BM25 achieves a good retrieval performance while its QA performance is limited as it can not be fine-tuned in subsequent joint training.
Both ORConvQA and DPR retrievers take the question rewrite as input following the setting in~\cite{qu2020open}. The only difference between them is that DPR uses a TF-IDF hard negative provided by the dataset in addition to the in-batch negatives. DPR outperforms ORConvQA when the batch size is the same, which indicates that the hard negative play a key role during training. Considering the batch size of ORConvQA in~\cite{qu2020open} is 64, we also show the retrieval performance, which is improved dramatically as the number of in-batch negatives increases.
Our KL-divergence based regularized retriever uses two question forms, rewrite and original question, as input, and offers the best performance with the same batch size of 16 among these dense retrieval model. When the batch size is raised to 32, our retriever achieves a better performance and exceeds BM25. 
Moreover, to explore the advantage of the original question which is the concatenation of the historical and current questions, we evaluate our retriever by feeding two of the same rewrite. The results shows that employing two different forms of one question leads to a better performance. It is probably because the input of two same rewrites works just as a regularization while that of original question and rewrite is able to add a supervision for question understanding learning.  
% We report the main evaluation results in Table \ref{tab:resofpretraining}. During testing, questions are expressed in two ways, question rewrite $q^{wr}$ and original question $q^{or}$. Limited by the GPU size, our experiments can only be set to the case of a batch size of 16. We summarize the observations as follows:
% \begin{enumerate}
%     \item When the batch size is 16, we notice that the ORConvQA pre-training retriever performs poorly. And as the batch size is increased, the results improve dramatically, implying that raising the number of in-batch negatives aids in improving training results.
%     \item The DPR model uses the same batch size and TF-IDF hard negative as our model to make a more fair comparison. Compared with the findings of ORConvQA, we feel that negative samples play a key role during training.
%     \item Our model outperforms DPR because it was trained with two question rewrites to ensure that the distribution of retrieval outcomes was consistent. We feel that using KL divergence improves the model's stability. And the fact that our model employs two different types of questions adds to the overall effect. This shows that asking several types of questions might lead to improved retrieval outcomes by developing more accurate semantic representations.
%     \item Retrieval algorithms such as BM25 outperform our model, but the dense retriever can continue to fine-tune the question encoder in subsequent joint training, while retrieval algorithms such as BM25 are not trainable.
% \end{enumerate}




\subsubsection{Case Study for Post-Ranker}
% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.46\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figure/case1v2.pdf}
%          \caption{}
%          \label{fig:case1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figure/case2v2.pdf}
%          \caption{}
%          \label{fig:case2}
%      \end{subfigure}
%         \caption{Case Study}
%         \label{fig:case}
% \end{figure*}
To explore the effect of post-ranker, we count the number of golden passages before and after post-ranker in the inference phase. Concretely, the number of golden passages in the top $T$ ($T=5$) increases from 2,228 to 2,269 while the golden passages ranked between $T+1$ and $K$ ($K=100$) decreases from 1,292 to 1,251, which verifies post-ranker' ability in enforcing the relevant passages to appear in the higher place. Figure~\ref{fig:case1} shows an example of the passage retrieval results before and after post-ranker for a question. Before applying post-ranker, it can be observed that golden passage is not contained in the top 5 passages but ranked in the 14 place of the ranking list generated by the retriever. Taking the passage representations in the ranking list as input, post-ranker provides a new passage ranking where the golden passage is reranked in the top 5. Moreover, the top 5 passages of the newly generated ranking list contains more relevant content with the question and its historical questions, showing the advantage of the proposed post-ranker module in our {\modelname}.  
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/case2v4.png}
    \vspace{-0.3in}
    \caption{Case study of the passage retrieval results before and after post-ranker for an example question.}
    \label{fig:case1}
    \vspace{-0.15in}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figure/case1v4.png}
%     % \vspace{-0.3in}
%     \caption{}
%     \label{fig:case2}
%     % \vspace{-0.25in}
% \end{figure}

\subsubsection{Impact of Curriculum Learning Strategy}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figure/f1.png}
%     % \vspace{-0.3in}
%     \caption{}
%     \label{fig:f1}
%     % \vspace{-0.25in}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figure/retriever_recall.png}
%     % \vspace{-0.3in}
%     \caption{}
%     \label{fig:recall}
%     % \vspace{-0.25in}
% \end{figure}

To investigate the impact of curriculum learning, we save checkpoints and evaluate on the test set every 500 steps, the results of all the checkpoints are shown in Figure~\ref{fig:curriculum}.
It can be observed that the QA metric F1 with curriculum learning increases more steadily and reached better final performance faster. 
For retrieval metric, the improvement achieved by curriculum learning is the higher final recall value, which is not quite noticeable.
It indicates that the curriculum learning strategy makes more pronounced contribution for the answer reading other than the passage retrieval, which in reasonable as the inconsistent problem addressed by curriculum learning mainly reflects in the reading part.












