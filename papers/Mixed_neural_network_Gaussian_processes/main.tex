% Future work to extend initial conference paper to a journal paper
% 1. Rates of convergence for infinite depth theta compositional kernels
% 2. Proposition for double limit construction of c-mixed from pure theta compositional kernels
% 3. theta bottleneck NNGPs
% 4. Complexity of theta NNGPs
% 5. Relation between variance of outputs and non-regularity of PGFs
% 6. Phase transitions, scaled and quadratic correlation limits
% 7. Simulations that confirm theoretical results
% 8. theta NNGP experiments based on data
% 9. Add a visualization of theta PGFs

% http://learningtheory.org/colt2022/
% https://imstat.org/meetings-calendar/1st-ims-colt-joint-workshop-2/

% https://tex.stackexchange.com/questions/523195/restatable-environment-theorem-works-lemma-doesnt

% \documentclass[anon, 12pt]{colt2021} % Anonymized submission
\documentclass[final, 12pt]{colt2021} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\usepackage{bm}
\usepackage{color}
\usepackage{times}

\title[Mixed neural network Gaussian processes]
{Mixed neural network Gaussian processes}

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Alexey Lindo}\thanks{These authors contributed to the manuscript equally.} \Email{alexey.lindo@glasgow.ac.uk}\\
 \addr School of Mathematics and Statistics, University of Glasgow
 \AND
 \Name{Theodore Papamarkou}\footnotemark[1] \Email{theodore.papamarkou@manchester.ac.uk}\\
 \addr Department of Mathematics, The University of Manchester
 \AND
 \Name{Serik Sagitov} \Email{serik@chalmers.se}\\
 \addr Department of Mathematical Sciences, Chalmers University of Technology and University of Gothenburg
 \AND
 \Name{Laura Stewart} \Email{laura.stewart@glasgow.ac.uk}\\
  \addr School of Mathematics and Statistics, University of Glasgow
}

\newtheorem{dfn}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{corol}{Corollary}
\newtheorem{rmrk}{Remark}

\begin{document}

\maketitle

\begin{abstract}%

This paper makes two contributions.
Firstly, it introduces
mixed compositional kernels and
mixed neural network Gaussian processes (NGGPs).
Mixed compositional kernels are
generated by
composition of probability generating functions (PGFs).
A mixed NNGP is a Gaussian process (GP)
with a mixed compositional kernel,
arising in the infinite-width limit of
multilayer perceptrons (MLPs)
that have a different activation function for each layer.
Secondly,
$\theta$ activation functions for neural networks and
$\theta$ compositional kernels are introduced
by building upon the theory of branching processes,
and more specifically upon $\theta$ PGFs.
While $\theta$ compositional kernels are recursive,
they are expressed in closed form.
It is shown that
$\theta$ compositional kernels
have non-degenerate asymptotic properties under certain conditions.
Thus, GPs with $\theta$ compositional kernels
do not require non-explicit recursive kernel evaluations
and have controllable infinite-depth asymptotic properties.
An open research question is whether GPs with $\theta$ compositional kernels
are limits of infinitely-wide MLPs with $\theta$ activation functions.
% Thus, $\theta$ NNGPs have explicit asymptotic properties
% and provide scope to mitigate the computational intractability of NNGPs.

% and examples of such kernels with
% non-degenerate asymptotic behaviour are constructed.
% Explicit
% Asymptotic properties
% Coming from theta-branching processes
% theta-activations
\end{abstract}

\begin{keywords}%
Activations,
branching processes,
compositional kernels,
probability generating functions,
neural network Gaussian processes,
neural networks.%
\end{keywords}

\section{Introduction}

A range of machine learning methods
associated with neural networks
have been studied theoretically
and have been adapted in practice
over the last three decades.
Such methods include
% multilayer perceptrons (MLPs),
% {\color{red}see X},
% convolutional neural networks (CNNs),
% {\color{red}see X},
Bayesian neural networks (BNNs) and
neural network Gaussian processes (NNGPs).
% see \citet{agrawal2020, matthews2018},
% and bottleneck NNGPs.
%see \citet{agrawal2020}.
% and neural tangent kernels (NTKs),
% see \citet{jacot2018}.
% MLPs and CNNs are parametric models,
% while NNGPs and NTKs are non-parametric models.
NNGPs establish connections between
BNNs and Gaussian processes (GPs),
% Theoretical connections have been established between
% some of these parametric and non-parametric models.
since NNGPs are GPs arising as limits of BNNs
when the BNN width tends to infinity
\citep{neal1996, matthews2018, agrawal2020}.

Branching processes provide another approach
to link neural networks with kernel-based methods.
More specifically,
iterations of probability generating functions (PGFs)
connect activation functions of neural networks
with compositional kernels
\citep{daniely2016, liang2021}.
In the present paper, it is shown that
compositions of PGFs yield an NNGP
which is a GP with a kernel
based on these PGF compositions;
the GP arises as the limit of a BNN
with a certain activation function
when the BNN hidden layer widths tend to infinity.

The motivation for the present work is twofold.
Firstly, PGFs connect
the choice of activation functions with
the properties of wide neural networks.
Given a set of activation functions across hidden layers,
the resulting NNGP kernel is known.
Secondly,
the so-called $\theta$ compositional kernels are introduced,
which can be deployed by GPs.
GPs with $\theta$ compositional kernels
enable kernel evaluations in closed form
and have controllable asymptotic properties.

An outline of the paper structure follows.
Section \ref{activation_pgf_duality} reviews
the duality between activation functions and PGFs.
Henceforth, activation functions are referred to as
activations.
Section \ref{sec:kernels_and_pgfs}
introduces mixed compositional kernels
as compositions of different PGFs,
establishing the positive definiteness of these kernels
for inputs on the unit sphere.
Section \ref{sec:mixed_and_theta_nngps} derives mixed NNGPs
as GPs with mixed compositional kernels,
in the infinite-width limit of multilayer perceptrons (MLPs)
with different activations across layers.
Section \ref{theta_activations}
introduces $\theta$ activations
by harnessing $\theta$ branching processes,
one of the two known classes of branching processes
with explicit PGF iterations (see appendix~\ref{sec:theta_pgfs}).
Moreover, section \ref{theta_activations}
demonstrates the richness of the family of $\theta$ activations
by linking members of the family with common activations,
such as ReLU and parametric ReLU (PReLU).
Section \ref{theta_kernels}
introduces $\theta$ compositional kernels,
providing their closed-form expression
and their limit as the number of compositions goes to infinity.
% and their rate of convergence.
A duality exists between
a $\theta$ activation and a corresponding $\theta$ compositional kernel
based on an underlying $\theta$ PGF,
as explained in section \ref{theta_kernels}.
Section \ref{theta_kernels_varying_env}
derives c-mixed $\theta$ compositional kernels by composing different PGFs,
showing that the resulting kernel limits are non-degenerate
and controllable via the kernel hyperparameterization.
% Mixed NNGPs and $\theta$ GPs
% are introduced in section \ref{sec:mixed_and_theta_nngps}.
Section \ref{discussion}
posits an open research question
on the existence of NNGPs with $\theta$ compositional kernels.

% in the infinite-width limit of MLPs with $\theta$ activations.

% To this end, this work initially 
% In this paper,
% a new family of recursive kernels,
% called $\theta$-compositional kernels,
% is introduced via iterations of PGFs.
% $\theta$-compositional kernels,
% which can be employed as recursive kernels
% for NNGPs or NTKs

% Introduction goes here.

% \subsection{Literature review}

% Newer thoughts
% Neural networks and kernel-based methods (DKL, NTKs)
% These two are linked theoretically:
% 1) limits (Neal, NNGPs, bottleneck NNGPs)
% 2) via branching processes (Liang, Daniely)
% Activation <-> PGF <-> compositional kernel

% Older thoughts
% 1) Compositional kernels
% 2) Branching processes
% 3) Duality between activations and PGFs
% 4) Duality between compositional kernels and branching processes

% \subsection{Motivation}

% Newer thoughts
% Motivation for studying Kernel-based methods:
% 1) are computationally expensive (GPs and dGPs)
% 2) their theoretical properties (asymptotics and speed of convergence) with depth are less understood
% What is out there:
% 1) About speed, approximate approaches, based on inducing points, kiss GPs etc
% 2) Neal, NNGPs, \citet{agrawal2020}.
% Here, we propose to attack this problem by via theta-compositional kernels
% 1) Being explicit, they are cheap
% 2) We derive their asymptotic behaviour, and we attain non-degenerate asymptotic behaving in varying environment
% Note to self: varying environment is a notion related to branching processes - refer to appendix A

% Older thoughts
% Compositional kernel associated with an MLP (PGF - activation)
% Idea led by Daniely and Liang
% Our work:
% 1) \theta-compositional kernels are explicit
% 2) Thus, they are computationally cheap
% 3) They can be used to reduce the cost of DKL, DGPs, NNGPs (cite my JMLR paper)
% 4) \theta-compositional kernel limits are known explicitly
% 5) and so a choice of \theta-compositional kernel can be made based on its known asymptotics
% 6) The corresponding \theta-activation can be derived, at least numerically,
% 7) yielding an associated deep neural networks with known depth asymptotics

% Problem: study analytically asymptotic properties of compositional kernels
% More specifically, compositions of kernels and iterations of dual PGFs
% Limited scope to address this problem explicitly via branching processes
% \theta-processes are a family branching processes for which iterations can be evaluated explicitly
% Thus, \theta-processes allow to derive compositional kernel limits explicitly

% \subsection{Outline}

\section{Duality between activations and probability generating functions}
\label{activation_pgf_duality}

This section reviews fundamental relations between
activations and PGFs,
while introducing associated notation.
For more details, see~\cite{liang2021}.
% Moreover, it revises how a compositional kernel of depth $n$ expresses as
% an $n$-fold iteration of PGFs.

\subsection{From probability generating functions to activations}

A PGF for a sequence of probabilities
$p_{k} \in[0, 1],~k=0, 1, 2,\ldots,$ is denoted by
\begin{equation}
\label{eq:pgf_def}
f(s)
% = \mbox{E}\left[s^{Y}\right]
:= \sum_{k\ge 0} p_k s^k.
\end{equation}
It is assumed that $\sum_{k \ge 0} p_k \le 1$.
For any such sequence of probabilities $p_{k}$,
% ~\cite{liang2021} introduce a function
a function
$\phi: \mathbb{R} \rightarrow \mathbb{R}$
is introduced as
\begin{equation}
\label{eq:activation}
\phi (x)
:= \sum_{k\ge 0} \sqrt {p_k} h_k(x),
\end{equation}
where $h_k$ are normalised Hermite polynoms.
It is noted that $\phi$ is
a square-integrable function with respect to the Gaussian measure.
$\phi$ can be used as an activation in neural networks.
Any common activation, such as ReLU and sigmoid,
can be expressed in the form of equation \eqref{eq:activation}.
% after normalization
% according to~\citet{liang2021}.

\subsection{From activations to probability generating functions}
\label{sec:from_activation}

For any function
$\phi: \mathbb{R} \rightarrow \mathbb{R}$
such that
$\mbox{E}
_{X\sim\mathcal{N}(0,\,1)}
\left[
\phi^{2}(X)
\right] \le 1$,
a PGF is introduced as
\begin{equation}
\label{eq:pgf}
f(s)
= \mbox{E}
_{(X,\,Z)\sim\mathcal{N}(0,\,C(s))}
\left[
\phi (X)
\phi (Z)
\right],
\end{equation}
where $\mathcal{N}(0,\,C(s))$
is a bivariate normal distribution
with mean $0$ and
correlation matrix $C(s)$,
with $s$ being the correlation between $X$ and $Z$.
%~\citep{liang2021}.
Equation \eqref{eq:pgf} derives
a PGF $f$ from an activation $\phi$,
and in this sense it is the dual of equation \eqref{eq:activation}.
% which derives an activation $\sigma$ from a PGF $f$.

\section{Kernels as compositions of probability generating functions}
\label{sec:kernels_and_pgfs}

This section reviews pure compositional kernels and introduces
mixed compositional kernels.
Hereafter, $\rho (x, z) := \langle x / \|x\|, z / \|z\| \rangle$
denotes the correlation between $x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$,
where $\langle \cdot, \cdot \rangle$ and $\|\cdot\|$
are the inner product and the Euclidean norm, respectively.

\subsection{Pure compositional kernels}

Definition~\ref{dfn:pure_comp_k} introduces the term
`pure compositional kernel'.
A pure compositional kernel describes
the known relation between
a compositional kernel and
iterations of a PGF~\citep{daniely2016, liang2021}.

\begin{dfn}[Pure compositional kernels]
\label{dfn:pure_comp_k}
Let $f$ be a PGF and
$\rho (x, z)$ the correlation between $x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$.
% The $\theta$ compositional kernel
% $\mathcal{K}_{(n)}(x,z)$
% of depth $n$
% is defined as the $n$-fold iteration $f_{(n)}$ of $f$
% evaluated at $\rho (x, z)$,
% that is $\mathcal{K}_{(n)}(x,z)=f_{(n)}(\rho (x, z))$ for any $x,z$.
The $n$-depth pure compositional kernel $\mathcal{K}_{(n)}(x,z)$ of $f$
is defined as the $n$-fold composition of $f$
evaluated at $\rho (x, z)$,
\begin{equation}
\label{eq:kn:long}
\mathcal{K}_{(n)}(x, z):=
\underbrace{f\circ\dots\circ f}_\text{$n$ times}
% f\circ\dots\circ f
(\rho(x, z)).
\end{equation}
\end{dfn}

The $n$-fold composition of $f$ is also known as $n$-fold iteration of $f$
and is denoted by $f_{(n)}$ in the literature.
% The term `pure compositional kernels' is used in this paper to refer
% to iterations of a single PGF $f$,
% as opposed to compositions of different PGFs
% (see section~\ref{sec:varying_compositional_kernels}
% on mixed compositional kernels).
A pure compositional kernel $K_{(n)}$ is related with a PGF $f$
according to equation~\eqref{eq:kn:long},
and $f$ is related with an activation $\phi$
according to equation~\eqref{eq:pgf}.
So, a relation is established between
pure compositional kernels and activations.

% \section{Connection between popular activations and probability generating functions}

% Look at https://arxiv.org/pdf/1602.05897.pdf, page 17-18 for $a_{k}$ for normalised step activation function.
% Look at https://arxiv.org/pdf/1602.05897.pdf, page 18 for $a_{k}$ for normalised RELU activation function.
% The table with ``dual'' activation functions is on page 17 in https://arxiv.org/pdf/2004.04767.pdf.
% Also have a look at Figure 2 on page 18 in https://arxiv.org/pdf/2004.04767.pdf.

\subsection{Mixed compositional kernels}
\label{sec:varying_compositional_kernels}

% Definition~\ref{def:comp_kernel_in_varying_env}
% formalizes the notion of mixed compositional kernels.
% A mixed compositional kernel
% refers to composition of different PGFs,
% as opposed to a pure compositional kernel
% that refers to iterations of a single PGF.

Existing work in the literature entails
pure compositional kernels.
Compositions of different PGFs
are known in the literature on branching processes
as iterated function systems in varying or random environment
~\citep{kozlov1976, kersting2017, alsmeyer2021},
but they have not been used to construct
compositional kernels.
% Section~\ref{sec:varying_compositional_kernels}
Definition~\ref{def:comp_kernel_in_varying_env}
introduces mixed compositional kernels
as compositions of different PGFs.

\begin{dfn}[Mixed compositional kernels]
	\label{def:comp_kernel_in_varying_env}
	Let $f_1,\ldots,f_n$ be PGFs and
	$\rho (x, z)$ the correlation between $x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$.
	The $n$-depth mixed compositional kernel $\mathcal{K}_{1:n}$
	of $f_1,\ldots,f_n$
	is defined as
	\begin{equation}
	\label{eq:kn:long:ve}
	\mathcal{K}_{1:n}(x, z) :=
	f_n\circ\dots\circ f_1
	% f\circ\dots\circ f
	(\rho(x, z)).
	\end{equation}
\end{dfn}

% For ease of notation, equation \eqref{eq:kn:long:ve} is written alternatively as
% $\mathcal{K}_{1:n}(x, z)=f_{1:n}(\rho (x, z))$,
% where $f_{1:n}=f_n \circ\dots\circ f_1$.
Setting $f_1=\dots=f_n$ in equation \eqref{eq:kn:long:ve}
yields equation \eqref{eq:kn:long} as a special case.

% Non-varying environment sets $K=K_1=\cdots=K_n$.

\subsection{Compositional kernels}
\label{sec:compositional_kernels}

While mixed compositional kernels subsume
pure compositional kernels,
the distinction between these two kernel classes is maintained
to emphasize the different asymptotic properties of the two classes.
On the other hand, definition~\ref{dfn:compo_k}
suggests the term `compositional kernels'
to unify reference to either of these two classes.
This is helpful for results
that are common across pure and mixed compositional kernels.
For instance,
proposition~\ref{prop:pure_theta_k_eigensys}
states the positive definitiness and eigensystem
of any compositional kernel, be it pure or mixed.

\begin{dfn}[Compositional kernels]
\label{dfn:compo_k}
A kernel is called compositional
if it is a pure or mixed compositional kernel.
\end{dfn}

\begin{prop}[Eigensystem of compositional kernels]
	\label{prop:pure_theta_k_eigensys}
	Let
	$\mathcal{K}:\mathbb{R}^m\times\mathbb{R}^m\rightarrow\mathbb{R}$
	be an $n$-depth compositional kernel.
	\begin{enumerate}
		\item $\mathcal{K}$ is positive-definite
		on $\mathbb{S}^m\times\mathbb{S}^m$,
		where $\mathbb{S}^m$ is the unit sphere
		in the Euclidean $m$-space.
		\item The eigenfunctions of $\mathcal{K}$ are
		the spherical harmonic functions $Y_{kj}$ of
		degree $k=0,1,\ldots,$ and
		of order $j=1,\ldots,r(m, k)$,
		with associated eigenvalues
		\begin{equation}
		\label{eq:lambda}
		\lambda_{kj}=
		\frac{2p_k\pi^{m/2}}{\Gamma(m/2)r(m, k)}
		\end{equation}
		of multiplicity
		\begin{equation}
		\label{eq:r}
		r(m, k) :=
		\frac{2k+m-2}{k}
		\begin{pmatrix}
		k+m-3\\
		k-1
		\end{pmatrix}.
		\end{equation}
		$p_k,k=~0,1,\ldots,$ are the probabilities in the
		PGF representation of $\mathcal{K}$
		and $\Gamma$ is the Gamma function.
	\end{enumerate}
\end{prop}

\begin{proof}
	$\mathcal{K}$ expresses as a PGF
	according to equation~\eqref{eq:kn:long}
	or~\eqref{eq:kn:long:ve}.
	Subsequently, $\mathcal{K}$
	takes the form of an infinite sum
	along the lines of equation~\eqref{eq:pgf_def}.
	This allows to invoke
	~\citet[lemma 4]{smola2001},
	which completes the proof.
\end{proof}

To utilize compositional kernels,
positive definiteness for kernel inputs on the unit sphere
is sufficient.
This is explained by the evaluation of PGF compositions
for correlations of normalized inputs, according to
equations~\eqref{eq:kn:long} and~\eqref{eq:kn:long:ve}.

\section{Mixed neural network Gaussian processes}
\label{sec:mixed_and_theta_nngps}

Initially, section~\ref{sec:mixed_mlps}
defines a mixed MLP
as an MLP with a different activation for each hidden layer,
as opposed to a pure MLP
that has a single activation across all hidden layers.
Subsequently,
section~\ref{sec:mixed_nngps}
establishes
conditions for the existence
of pure NNGPs and of mixed NNGPs
as limits
of sequences of pure MLPs and of mixed MLPs, respectively.
% ~\ref{sec:theta_nngps}
% introduces $\theta$ GPs
% and investigates conditions for the existence of $\theta$ NNGPs.

% Definition of theta NNGP as GP with theta compositional kernel

% Proposition for NNGP saying that
% GP with theta compositional kernel is an NNGP limit of a BNN (Matthews)
% For the proof of the prosition, point to literature (Matthews + Liang)
% Do not take the proof for granted, Liang did not notice

% Proposition for complexity of theta NNGP vs complexity of NNGP
% This will explain what we mean by solvig computational tractability of NNGPs

\subsection{Multilayer perceptrons}
\label{sec:mixed_mlps}

Definition~\ref{dfn:pure_mlps}
reviews MLPs with a single activation across layers
(termed `pure MLPs' in this paper),
while definition~\ref{dfn:mixed_mlps}
introduces mixed MLPs with different activations across layers.
Definitions~\ref{dfn:pure_mlps} and~\ref{dfn:mixed_mlps}
normalize layer outputs
instead of scaling weight variances by the corresponding layer widths.
Output normalization and scaling of weight variances
yield analogous scaling for preactivations,
see~\citet{liang2021}.

% Pure, mixed def, review vs new
% Normalization of layer outputs according to Liang

% \begin{dfn}[Indexed stochastic processes]
% Consider a probability space $\Omega$
% and a measurable space $E$.
% A stochastic process $\mathcal{F}$ with index set $X$
% is defined as a function $\mathcal{F}:X\times\Omega\rightarrow E$
% such that $\mathcal{F}(x,\cdot)$ is a measurable function for each $x\in X$.
% \end{dfn}

\begin{dfn}[Pure MLPs]
\label{dfn:pure_mlps}
Consider the recursive equation
\begin{equation}
\label{eq:pb_mlp}
\begin{alignedat}{2}
x^{(k+1)} &=
\phi (W^{(k)} x^{(k)} / \|x^{(k)}\|),
&& \;
k=0,1,\ldots,n-1,\\
x^{(n+1)}
&=
W^{(n)} x^{(n)} / \|x^{(n)}\|
&& \;
(k = n),
\end{alignedat}
\end{equation}
% with initial state $x^{(0)}\in\mathbb{R}^{h_0}$ and
% with
where $x^{(k)}\in\mathbb{R}^{h_{k}}.$
% and $b_{k+1}$ is the width of layer $k+1$.
% $x^{(0)}\in\mathbb{R}^{d_0}$ is the network input,
% $x^{(l)},~l=1,\ldots,L,$ is the output of the $l$-th hidden layer,
% and $x^{(L+1)}$ is the network output.
$W^{(k)}\in\mathbb{R}^{h_{k} h_{k+1}}$ % ,~k=0,1,\ldots,n,$
are random variables
% with multivariate normal prior
%\begin{equation}
% $
% W^{(l)} \sim
% \mathcal{N}(0, I_{d_{l+1}} \otimes I_{d_{l}}),
% $
%\end{equation}
% where $I_{d_l}$ denotes the $d_l\times d_l$ identity matrix
% and $\otimes$ denotes the Kronecker product.
% For each $l=0,1,\ldots,L,$
and $\phi:\mathbb{R}\rightarrow\mathbb{R}$
is an activation function.
The notation $\phi (W^{(k)} x^{(k)} / \|x^{(k)}\|)$
indicates elementwise application of $\phi$
to the vector $W^{(k)} x^{(k)} / \|x^{(k)}\|$
of length $h_{k+1}$.
Moreover, let $\eta:=h_{0}h_{1}+\ldots +h_{n}h_{n+1}$
and $W:=(W^{(0)},\ldots, W^{(n)})\in\mathbb{R}^{\eta}$.
$W$ follows
the multivariate Gaussian distribution
$
\mathcal{N}(0, I_{\eta}),
$
where $I_{\eta}$ is the $\eta\times\eta$ identity matrix.
Consider the
random field
$\mathcal{F}_{(n)}:\mathbb{R}^{h_0}\times\mathbb{R}^{\eta}
\rightarrow\mathbb{R}^{h_{n+1}}$
with index set $\mathbb{R}^{h_0}$
defined as $\mathcal{F}_{(n)}(x^{(0)},w):=x^{(n+1)}$,
where $x^{(n+1)}$ is given by equation~\eqref{eq:pb_mlp}.
$\mathbb{R}^{\eta}$ is equipped with the Borel $\sigma$-algebra
and with the Gaussian measure associated with
$
\mathcal{N}(0, I_{\eta}).
$
$\mathcal{F}_{(n)}$ is called a pure MLP
with $n$ hidden layers,
widths $h_k$ and
activation $\phi$.
\end{dfn}

\begin{dfn}[Mixed MLPs]
\label{dfn:mixed_mlps}
Consider the recursive equations
\begin{equation}
\label{eq:mb_mlp}
\begin{alignedat}{2}
x^{(k+1)} &=
\phi^{(k+1)} (W^{(k)} x^{(k)} / \|x^{(k)}\|),
&& \;
k=0,1,\ldots,n-1,\\
x^{(n+1)}
&=
W^{(n)} x^{(n)} / \|x^{(n)}\|
&& \;
(k = n),
\end{alignedat}
\end{equation}
% with initial state $x^{(0)}\in\mathbb{R}^{h_0}$ and
% with
where
$x^{(k)}\in\mathbb{R}^{h_{k}}.$
% and $b_{k+1}$ is the width of layer $k+1$.
% $x^{(0)}\in\mathbb{R}^{d_0}$ is the network input,
% $x^{(l)},~l=1,\ldots,L,$ is the output of the $l$-th hidden layer,
% and $x^{(L+1)}$ is the network output.
$W^{(k)}\in\mathbb{R}^{h_{k} h_{k+1}}$ % ,~k=0,1,\ldots,n,$
are random variables
% with multivariate normal prior
%\begin{equation}
% $
% W^{(l)} \sim
% \mathcal{N}(0, I_{d_{l+1}} \otimes I_{d_{l}}),
% $
%\end{equation}
% where $I_{d_l}$ denotes the $d_l\times d_l$ identity matrix
% and $\otimes$ denotes the Kronecker product.
% For each $l=0,1,\ldots,L,$
and $\phi^{(k)}:\mathbb{R}\rightarrow\mathbb{R}$
are activation functions.
The notation $\phi^{(k+1)} (W^{(k)} x^{(k)} / \|x^{(k)}\|)$
indicates elementwise application of $\phi^{(k+1)}$
to the vector $W^{(k)} x^{(k)} / \|x^{(k)}\|$
of length $h_{k+1}$.
% $\phi^{(n+1)}$ is the identity function,
% included to facilitate exposition in equation~\eqref{eq:mb_mlp}.
Moreover, let $\eta:=h_{0}h_{1}+\ldots +h_{n}h_{n+1}$
and $W:=(W^{(0)},\ldots, W^{(n)})\in\mathbb{R}^{\eta}$.
$W$ follows
the multivariate normal distribution
$
\mathcal{N}(0, I_{\eta}),
$
where $I_{\eta}$ is the $\eta\times\eta$ identity matrix.
Consider the
random field
$\mathcal{F}_{1:n}:\mathbb{R}^{h_0}\times\mathbb{R}^{\eta}
\rightarrow\mathbb{R}^{h_{n+1}}$
with index set $\mathbb{R}^{h_0}$
defined as $\mathcal{F}_{1:n}(x^{(0)},w):=x^{(n+1)}$,
where $x^{(n+1)}$ is given by equation~\eqref{eq:mb_mlp}.
$\mathbb{R}^{\eta}$ is equipped with the Borel $\sigma$-algebra
and with the Gaussian measure associated with
$
\mathcal{N}(0, I_{\eta}).
$
$\mathcal{F}_{1:n}$ is called a mixed MLP
with $n$ hidden layers,
widths $h_k$ and
activations $\phi^{(k)}$.
\end{dfn}

\subsection{Neural network Gaussian processes with compositional kernels}
\label{sec:mixed_nngps}

Propositions~\ref{prop:pure_nngp} and~\ref{prop:mixed_nngp}
provide conditions
on the activations of sequences of pure and of mixed MLPs
to attain limits of GPs with
pure and of GPs with mixed compositional kernels, respectively.
Subsequently,
defintions~\ref{dfn:pure_nngp} and~\ref{dfn:mixed_nngp}
introduce pure and mixed NNGPs
as limits of such GPs.

% \begin{prop}[Infinite-width limit of pure Bayesian multilayer perceptrons]
% Let $\mathcal{F}_n,~n\in\mathbb{N},$ be a sequence
% of pure Bayesian MLPs
% of length $L$
% with a common activation $\sigma$.
% Moreover, let $d_{l,n}$ be the width of the $l$-th layer of $\mathcal{F}_n$.
% If $\sigma$ is square-integrable with respect to the Gaussian measure,
% then the sequence $\mathcal{F}_n$ of MLPs converges
% in distribution to the $\mathcal{GP}(0,\mathcal{K}_{(L)})$,
% where $\mathcal{K}_{(L)}$ is the $L$-depth pure compositional kernel
% of the PGF generated by $\sigma$ via equation~\eqref{eq:pgf}.
% \end{prop}

\begin{prop}[Infinite-width limit of pure MLPs]
\label{prop:pure_nngp}
Let $(\mathcal{F}_{(n)})_{m},~m=1,2,\ldots,$ be a sequence
of pure MLPs,
each with $n$ hidden layers,
with hidden layer widths $(h_{k})_{m},~k=1,\ldots,n,$
and with activation
$\phi:\mathbb{R}\rightarrow\mathbb{R}$.
It is assumed that
$\phi$ is square-integrable with respect to the Gaussian measure and absolutely continuous,
and that the derivative of $\phi$ is exponentially bounded.
If $(h_k)_{m}$ is strictly increasing in $m$ for each $k$,
then the sequence $(\mathcal{F}_{(n)})_{m}$ of pure MLPs converges
in distribution to the GP $\mathcal{G}(0,\mathcal{K}_{(n)})$,
where $\mathcal{K}_{(n)}$ is the $n$-depth pure compositional kernel
of the PGF generated by $\phi$ via equation~\eqref{eq:pgf}.
\end{prop}

\begin{proof}
The $n$-depth pure compositional kernel $\mathcal{K}_{(n)}$ exists,
since $\phi$ is square-integrable with respect to the Gaussian measure~\citep{liang2021}.
The sequence of pure MLPs
$(\mathcal{F}_{(n)})_{m}$ converges in distribution to the
GP $\mathcal{G}(0,\mathcal{K}_{(n)})$,
since $\phi$ is absolutely continuous
and since the derivative of $\phi$
is exponentially bounded~\citep[theorem E.4]{novak2019b}.
\end{proof}

\begin{prop}[Infinite-width limit of mixed MLPs]
\label{prop:mixed_nngp}
Let $(\mathcal{F}_{1:n})_{m},~m=1,2,\ldots,$ be a sequence
of mixed MLPs,
each with $n$ hidden layers,
with hidden layer widths $(h_{k})_{m},~k=1,\ldots,n,$
and with activations
$\phi^{(k)}:\mathbb{R}\rightarrow\mathbb{R},~k=1,\ldots,n$.
It is assumed that each
$\phi^{(k)}$ is square-integrable with respect to the Gaussian measure
and absolutely continuous,
and that the derivative of each $\phi^{(k)}$ is exponentially bounded.
If $(h_k)_{m}$ is strictly increasing in $m$ for each $k$,
then the sequence $(\mathcal{F}_{1:n})_{m}$ of mixed MLPs converges
in distribution to the GP $\mathcal{G}(0,\mathcal{K}_{1:n})$,
where $\mathcal{K}_{1:n}$ is the $n$-depth mixed compositional kernel
of PGFs $f_1,\ldots,f_n$
generated by the respective activations $\phi_1,\ldots,\phi_n$
via equation~\eqref{eq:pgf}.
\end{prop}

\begin{proof}
The existence of kernel $\mathcal{K}_{1:n}$
follows from the square-integrability of $\phi$
based on Mehler's formula.
The sequence of mixed MLPs
$(\mathcal{F}_{1:n})_{m}$ converges in distribution to the
GP $\mathcal{G}(0,\mathcal{K}_{1:n})$,
since $\phi$ is absolutely continuous
and since the derivative of $\phi$
is exponentially bounded~\citep[theorem E.4]{novak2019b}.
\end{proof}

\begin{dfn}[Pure NNGPs]
\label{dfn:pure_nngp}
The GP limit
of a sequence of pure MLPs of increasing width,
as stated by proposition~\ref{prop:pure_nngp},
is called a pure NNGP.
\end{dfn}

\begin{dfn}[Mixed NNGPs]
\label{dfn:mixed_nngp}
The GP limit
of a sequence of mixed MLPs of increased width,
as stated by propsition~\ref{prop:mixed_nngp},
is called a mixed NNGP.
\end{dfn}

NNGPs with compositional kernels have been studied previously
~\citep{matthews2018, novak2019b, agrawal2020}.
Propositions~\ref{prop:pure_nngp} and~\ref{prop:mixed_nngp} 
are novel in two ways.
Firstly, the kernels of NNGPs
$\mathcal{G}(0,\mathcal{N}_{(n)})$ and
$\mathcal{G}(0,\mathcal{N}_{1:n})$
are expressed as compositions of PGFs.
Secondly,
a duality is established
between MLP activations
and NNGP kernels.
Thus, it is formally established
how a neural architecture choice,
namely the choice of activation,
propagates to the kernel of the resulting NNGP.
In the case of mixed MLPs,
it is established how
the choice of activation for a single hidden layer
propagates to the kernel of the resulting mixed NNGP.

% \subsection{\texorpdfstring{$\boldsymbol{\theta}~$} GGaussian processes}
% \label{sec:theta_nngps}

\section{The family of \texorpdfstring{$\boldsymbol{\theta}~$} aactivations}
\label{theta_activations}

Section \ref{sec:activation}
introduces the family of $\theta$ activations,
which is based on the $\theta$ PGFs of~\citet{sagitov2016}.
For a recap of $\theta$ PGFs,
see appendix~\ref{sec:theta_pgfs}.
%Normalization of $\theta$ activations is discussed
%in section \ref{sec:norm_theta_activations}.
Section \ref{sec:theta_and_common_activations}
demonstrates that
common activations have proxies in the family of $\theta$ activations.

\subsection{Definition and explicitness
of \texorpdfstring{$\boldsymbol{\theta}~$} aactivations}
\label{sec:activation}

The definition of $\theta$ activations follows
by using $\theta$ PGFs in equation~\eqref{eq:activation}.

\begin{dfn}[$\boldsymbol{\theta}$ activations]\label{def:activation}
A $\theta$ activation is defined as
a function $\phi$ arising from equation~\eqref{eq:activation}
for a sequence of probabilities $p_k$ of a $\theta$ PGF.
\end{dfn}

Proposition~\ref{prop:p_k_of_theta_activation}
provides the sequence of probabilities $p_k$ of a $\theta$ activation.
In proposition~\ref{prop:p_k_of_theta_activation},
three cases of $\theta$ activations appear,
depending on whether
$\theta \in (-1, 0) \cup (0, 1]$,
$\theta=0$
or $\theta=-1$.

\begin{prop}[Probabilities $p_k$ of a $\boldsymbol{\theta}$ activation]
\label{prop:p_k_of_theta_activation}
For $\theta \in (-1, 0) \cup (0, 1]$, consider
the sequence of probabilities
\begin{equation}\label{eq:probMain}
p_k =
\left\{
\begin{array}{ll}
r-(ar^{-\theta}+c)^{-1/\theta},  & k = 0,\\
a (a+cr^{\theta})^{-1-1/\theta}, & k = 1,\\
\frac{ar^{-k+1}}{k!(a+cr^{\theta})^{\frac{1+\theta}{\theta}}}
\displaystyle\sum_{i=1}^{k-1}\left(\frac{cr^{\theta}}{a+cr^{\theta}}\right)^{i}b_{i,k}(\theta),
& k \ge 2.
\end{array}
\right.
\end{equation}
It is assumed that the parameters $\theta$, $a$, $c$ and $r$
satisfy one of the three options
\begin{equation*}
\begin{array}{llllll}
\theta\in(0,1], & a\ge1,  & c>0, & & r=1,\\
\theta \in (-1, 0) \cup (0, 1], & a\in(0,1),&  c=(1-a) (1-q)^{-\theta}, &q\in[0,1), & r=1,\\
\theta \in (-1, 0) \cup (0, 1], & a\in(0,1),&  c=(1-a) (r-q)^{-\theta}, &q\in[0,1], & r>1.
\end{array}
\end{equation*}
$b_{i,k}(\theta)$ are non-negative and, for $k\ge2,~i=1,\ldots, k-1$, satisfy the recursion
\begin{align*}
b_{i,k}(\theta)=(k-2-i\theta)b_{i,k-1}(\theta)+(1+i\theta)b_{i-1,k-1}(\theta),
% \ i=1,\ldots, k-1,
\end{align*}
 with
 $b_{0,k}(\theta)=
 b_{k,k}(\theta)
 =0$ for $k\ge1$, and
 $b_{1,2}(\theta)=1+\theta$.

For $\theta=0$, consider the sequence probabilities
\begin{equation}\label{eq:probZero}
p_k =
\left\{
\begin{array}{ll}
r-(r-q)^{1-a}r^{a},  & k = 0,\\
(r-q)^{1-a} ar^{a-1}, & k = 1,\\
ar^{a}(r-q)^{1-a} r^{-k}
\displaystyle\prod_{i=2}^k\Big(1-\frac{1 + a}{i}\Big),
& k \ge 2.
\end{array}
\right.
\end{equation}
It is assumed that parameters $q$ and $r$ satisfy one of the two options
\begin{equation*}
\begin{array}{ll}
q \in [0, 1), & r = 1, \\
q \in [0, 1], & r > 1.
\end{array}
\end{equation*}

For $\theta=-1$, consider the sequence of probabilities
\begin{equation}\label{eq:probMinus}
p_k =
\left\{
\begin{array}{ll}
1-aq,  & k = 0,\\
a, & k = 1,\\
0, & k \ge 2.
\end{array}
\right.
\end{equation}
It is assumed that $a \in (0, 1)$ and $q \in [0, 1]$.

% A $\theta$ activation is defined as
% an activation $\sigma$ arising from equation~\eqref{eq:activation}
% for a sequence of probabilities $p_k$
% given by one of equations~\eqref{eq:probMain},~\eqref{eq:probZero} or~\eqref{eq:probMinus}.

If the sequence of probabilities $p_k$
of a function $\phi$ specified by equation~\eqref{eq:activation}
is given by one of
equations~\eqref{eq:probMain},~\eqref{eq:probZero} or~\eqref{eq:probMinus},
then $\phi$ is a $\theta$ activation.
Conversely, if $\phi$ is a $\theta$ activation,
then it satisfies equation~\eqref{eq:activation} with probabilities $p_k$
given by one of
equations~\eqref{eq:probMain},~\eqref{eq:probZero} or~\eqref{eq:probMinus}.
\end{prop}

\begin{proof}
Let $\phi$ be a function specified by equation~\eqref{eq:activation}
with a sequence of probabilities $p_k$ given by one of
equations~\eqref{eq:probMain},~\eqref{eq:probZero} or~\eqref{eq:probMinus}.
As shown in~\citet{sagitov2016},
for any such sequence of probabilities $p_k$ there exists a $\theta$ PGF.
Thereby, $\phi$ is a $\theta$ activation.

Conversely, let $\phi$ be a $\theta$ activation.
So, $\phi$ is given by equation~\eqref{eq:activation}
with a sequence of probabilities $p_k$ generated by a $\theta$ PGF.
As shown in~\citet{sagitov2016},
any such sequence of probabilities $p_k$
satisfies one of
equations~\eqref{eq:probMain},~\eqref{eq:probZero} or~\eqref{eq:probMinus}.
\end{proof}

% \subsection{Normalization of \texorpdfstring{$\boldsymbol{\theta}~$} aactivations}
% \label{sec:norm_theta_activations}

% For an activation $\sigma$,
% to satisfy the condition
% $\mbox{E}
% _{X\sim\mathcal{N}(0,\,1)}
% \left[
% \sigma^{2}(X)
% \right]=1$
% mentioned in section~\ref{sec:from_activation},
% it is required to normalize $\sigma$,
% that is to divide $\sigma$ by the square root of $\sum_{k\ge 0} p_k$.
% Along these lines, proposition~\ref{prop:normalized_theta_activation}
% provides the normalization of a $\theta$ activation.

% \begin{prop}[Normalization of a $\boldsymbol{\theta}$ activation]
% \label{prop:normalized_theta_activation}

% To normalize a $\theta$ activation $\sigma$,
% its normalizing constant $\sqrt{\sum_{k\ge 0} p_k}$ is specified by
% \begin{equation}
% \label{eq:sqrt_of_norm_c}
% \sum_{k\ge 0} p_k =
% 1 - 
% \left\{
% \begin{array}{ll}
% (r - 1)((a + (1 - a)(r - q)^{-\theta}(r - 1)^{\theta})^{-1/\theta} - 1),
% & \theta\in (-1, 0) \cup (0, 1],\\
% (r - q)^{1-a}(r - 1)^{a} - (r - 1),
% & \theta = 0,\\
% (1 - a)(1 - q),
% & \theta = - 1.
% \end{array}
% \right.
% \end{equation}

% \end{prop}

% \begin{proof}

% Note that
% $\sum_{k \ge 0} p_k = f(1)$,
% where $f$ is the $\theta$ PGF of $\sigma$
% (see definition \ref{dfn:theta_pgfs} in appendix \ref{sec:theta_pgfs}).
% $f(1)$ is derived from equations
% ~\eqref{eq:PGFMain},~\eqref{eq:PGFZero} and~\eqref{eq:PGFMinus}
% for
% $\theta \in (-1, 0) \cup (0, 1]$,
% $\theta=0$ and $\theta=-1$,
% respectively.
% Thus, it follows that $f(1)$ is given by equation~\eqref{eq:sqrt_of_norm_c}.
% \end{proof}

% \begin{corollary}[Normalized $\boldsymbol{\theta}$ activations]

% For $\theta\in (-1, 1]$,
% $r=1$ or ($r > 1$ and $q = 1$).
% For $\theta = -1$,
% $q=1$.

% \end{corollary}

% TODO: add conditions on parameters
% TODO: complete definition
% TODO: comment on linear case

\subsection{Links between \texorpdfstring{$\boldsymbol{\theta}~$} aactivations and common activations}
\label{sec:theta_and_common_activations}

Figure~\ref{fig:theta_close}
demonstrates that it is possible to specify
$\theta$ activations in functional proximity to common activations.
The $\theta$ activation with
$\theta = -1,
~a = 0.99,
~q = 0.99$
in Figure~\ref{fig:theta_close_lin}
is a linear activation.
The $\theta$ activation with
$\theta = 1,
~a = 1.65,
~c = 0.146,
~r = 1$
in Figure~\ref{fig:theta_close_prelu}
is close to a PReLU with a slope of $0.25$.
The $\theta$ activation with
$
\theta = 0.99,
~a = 0.22,
~c = 0.203,
~r = 4.8
$
in Figure~\ref{fig:theta_close_relu}
is close to ReLU.

\begin{figure}[htbp]
\floatconts
{fig:theta_close}% label for whole figure
{\caption{Examples of $\theta$ activations (in blue)
approximating some common activations (in red).}}% caption for whole figure
{%
\subfigure[Link to linear activation]{%
\label{fig:theta_close_lin}% label for this sub-figure
%\includegraphics[width=0.32\linewidth]{manuscript/colt/initial/linear_activation.png}
\includegraphics[width=0.32\linewidth]{linear_activation.png}
}% \qquad % space out the images a bit
\subfigure[Link to PReLU activation]{%
\label{fig:theta_close_prelu}% label for this sub-figure
%\includegraphics[width=0.32\linewidth]{manuscript/colt/initial/prelu_activation.png}
\includegraphics[width=0.32\linewidth]{prelu_activation.png}
}% \qquad % space out the images a bit
\subfigure[Link to ReLU activation]{%
\label{fig:theta_close_relu}% label for this sub-figure
%\includegraphics[width=0.32\linewidth]{manuscript/colt/initial/relu_activation.png}
\includegraphics[width=0.32\linewidth]{relu_activation.png}
}
}
\end{figure}

\section{\texorpdfstring{$\boldsymbol{\theta}~$} ccompositional kernels}
\label{theta_kernels}

This section defines
pure $\theta$ compositional kernels
and c-mixed $\theta$ compositional kernels,
expresses them in closed form,
and derives their infinite-depth asymptotics.
In this paper,
the term `$\theta$ compositional kernel' refers to
a pure $\theta$ or c-mixed $\theta$ compositional kernel.

\subsection{Pure \texorpdfstring{$\boldsymbol{\theta}~$} ccompositional kernels}
\label{pure_theta_kernels}

Pure $\theta$ compositional kernels are connected to $\theta$ activations
via $\theta$ PGFs
based on the general duality between pure compositional kernels and activations
(see section~\ref{sec:kernels_and_pgfs}).
Definition~\ref{dfn:pure_theta_k}
introduces pure $\theta$ compositional kernels,
proposition~\ref{prop:theta_kernels}
expresses them in closed form,
and proposition~\ref{prop:theta_kernel_limit}
provides their infinite-depth limits.

% \subsection{Definition and explicitness
% of pure \texorpdfstring{$\boldsymbol{\theta}~$}
% ccompositional kernels}

% The definition of pure $\theta$ compositional kernels follows
% by using iterations of $\theta$ PGFs.

\begin{dfn}[Pure $\boldsymbol{\theta}$ compositional kernels]
\label{dfn:pure_theta_k}

A pure compositional kernel $\mathcal{K}_{(n)}$ of a $\theta$ PGF $f$
is called a pure $\theta$ compositional kernel.
\end{dfn}

% Pure $\theta$ compositional kernels express in closed form
% (proposition~\ref{prop:theta_kernels})
% and are positive-definite (proposition~\ref{prop:theta_k_eigensys}).

\begin{prop}[Pure $\boldsymbol{\theta}$ compositional kernels expressed in closed form]
\label{prop:theta_kernels}
A pure $\theta$ compositional kernel
$\mathcal{K}_{(n)}(x,z)=f_{(n)}(\rho (x, z))$
expresses in closed form
according to one of the nine cases of table~\ref{tab:thetaK}
depending on the values of parameters
$\theta,a,q,c,r$ of PGF $f$.

\end{prop}

\begin{table}[t]
\centering
\begin{tabular}{c|l|l|l|l|l}
Case
&
\multicolumn{1}{c|}{$\mathcal{K}_{(n)}$}
&
\multicolumn{1}{c|}{$\theta$}
&
\multicolumn{1}{c|}{$a$}
&
\multicolumn{1}{c|}{$q,\,c$}
&
\multicolumn{1}{c}{$r$} \\ \hline
% Case 1
1
&
$1-(a^n(1-\rho)^{-\theta }+(a^n-1)c)^{-1/\theta}$
&
$(0, 1]$
&
$(1,\infty)$
&
$(0,\infty)$
&
$1$ \\
% Case 2
2
&
$1-((1-\rho)^{-\theta }+nc)^{-1/\theta }$
&
$(0, 1]$
&
$1$
&
$(0,\infty)$
&
$1$ \\
% Case 3
3
&
$1-(a ^n(1-\rho)^{-\theta }+(1-a ^n)(1-q)^{-\theta })^{-1/\theta }$
&
$(0, 1]$
&
$(0, 1)$
&
$[0, 1)$
&
$1$ \\
% Case 4
4
&
$1-(1-q)^{1-a^n}(1-\rho)^{a^n}$
&
$0$
&
$(0, 1)$
&
$[0, 1)$
&
$1$ \\
% Case 5
5
&
$1-(a ^n(1-\rho)^{|\theta|}+(1-a ^n)(1-q)^{|\theta|})^{1/{|\theta|}}$
&
$(-1, 0)$
&
$(0, 1)$
&
$[0, 1)$
&
$1$ \\
% Case 6
6
&
$a^n\rho+(1-a^n)q$
&
$-1$
&
$(0, 1)$
&
$[0, 1]$
&
$1$ \\
% Case 7
7
&
$r-(a^n(r-\rho)^{-\theta}+(1-a^n)(r-q)^{-\theta})^{-1/\theta}$
&
$(0, 1]$
&
$(0, 1)$
&
$[0, 1]$
&
$(1, \infty)$ \\
% Case 8
8
&
$r-(r-q)^{1-a^n}(r-\rho)^{a^n}$
&
$0$
&
$(0, 1)$
&
$[0, 1]$
&
$(1, \infty)$ \\
% Case 9
9
&
$r-(a ^n(r-\rho)^{|\theta|}+(1-a ^n)(r-q)^{|\theta|})^{1/{|\theta|}}$
&
$(-1, 0)$
&
$(0, 1)$
&
$[0, 1]$
&
$(1, \infty)$ \\
\end{tabular}
\caption{Closed-form expressions of pure $\theta$ compositional kernels
$\mathcal{K}_{(n)}(x,z)=f_{(n)}(\rho (x, z))$,
where $f$ is a $\theta$ PGF and
$\rho (x, z)=\langle x / \|x\|, z / \|z\| \rangle$.}
\label{tab:thetaK}
\end{table}

\begin{proof}
A pure $\theta$ compositional kernel
$\mathcal{K}_{(n)}(x,z)=f_{(n)}(\rho (x, z))$
is expressed as an $n$-fold iteration of a $\theta$ PGF $f$.
Closed-form expressions for $n$-fold iterations of a $\theta$ PGF $f$
are provided in~\citet[section 4]{sagitov2016}.
Thus, table~\ref{tab:thetaK} follows.
\end{proof}

% $\theta$ PGFs were introduced by
% ~\citet{sagitov2016} and their
% definition is restated in appendix~\ref{sec:theta_pgfs}.

% \subsection{Asymptotic properties of pure \texorpdfstring{$\boldsymbol{\theta}~$}
% ccompositional kernels}

% For each case of pure $\theta$ compositional kernel $\mathcal{K}_{(n)}$
% in table~\ref{tab:thetaK},
% the corresponding limit for increasing depth $n$ is provided by
% proposition~\ref{prop:theta_kernel_limit}.

\begin{prop}[Infinite-depth limit of pure $\boldsymbol{\theta}$ compositional kernels]
\label{prop:theta_kernel_limit}
Let $K_{(n)}$ be a pure $\theta$ compositional kernel
and $\rho (x, z)$
the correlation function between
$x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$.

\begin{enumerate}
\item In cases $1$ and $2$ of table~\ref{tab:thetaK},
the infinite-depth limit of $K_{(n)}$
equals
\begin{equation}\label{eq:sub_and_critical}
\lim_{n\rightarrow\infty}
\mathcal{K}_{(n)}(x,z) = 1
\end{equation}
for any
$x$ and $z$.
\item In cases $3-9$ of table~\ref{tab:thetaK},
the infinite-depth limit of $K_{(n)}$
equals
\begin{equation}\label{eq:supercritical}
\lim_{n\rightarrow\infty}
\mathcal{K}_{(n)}(x,z) =
\left\{
\begin{array}{ll}
1, & \rho (x, z) = 1,\\
q, & \rho (x, z) \in [-1, 1),
\end{array}
\right.
\end{equation}
for any
$x$ and $z$.
\end{enumerate}
\end{prop}

\begin{proof}
The limits of equations~\eqref{eq:sub_and_critical} and~\eqref{eq:supercritical}
follow from the
closed-form expressions of pure $\theta$ compositional kernels $K_{(n)}$
of table~\ref{tab:thetaK}.
\end{proof}

Cases $3-9$ of table~\ref{tab:thetaK}
yield a pure $\theta$ compositional kernel limit that depends on parameter $q$
of the associated $\theta$ PGF.
The asymptotic behaviour of pure $\theta$ compositional kernels
in these cases is controlled by $q$.
In this sense, a non-degenerate limit is constructed,
in contrast to~\citet{liang2021},
where a non-degenerate limit is proven to exist.

% \begin{prop}[Rate of convergence of $\boldsymbol{\theta}$ compositional kernels]

% Geometric or linear rate for different cases in Table \ref{tab:thetaK}.

% \end{prop}

% \section{Compositional kernels for conditioned \texorpdfstring{$\boldsymbol{\theta}$}--branching processes}

% % Conditioning of branching processes is related to centering of kernels 

% \subsection{Explicit compositional kernels}

% \subsection{Compositional kernel limits}

\subsection{c-mixed \texorpdfstring{$\boldsymbol{\theta}~$} ccompositional kernels}
% \section{Compositional kernels for linear fractional functions}
\label{theta_kernels_varying_env}

% Furthermore, section~\ref{sec:varying_theta_kernel}
% introduces c-mixed $\theta$ compositional kernels,
% which is a class of mixed compositional kernels
% expressed in closed form.
% The non-degenerate asymptotic properties of
% c-mixed $\theta$ compositional kernels are derived in
% section~\ref{sec:varying_theta_kernel_limit}.

% 1) So far Kernels with same PGF
% 2) This is known in branching processes
% 3) Along these lines, kernels with different PGFs here
% 4) One example of such theta kernel
% 5) This changes asymptotic behaviour

%\subsection{Definition and explicitness
%of c-mixed \texorpdfstring{$\boldsymbol{\theta}~$}
%ccompositional kernels}

Definition ~\ref{dfn:cmixed_k}
introduces c-mixed $\theta$ compositional kernels,
which is a class of mixed compositional kernels
expressed in closed form
(see proposition~\ref{prop:ex_critical_theta_kernel}).
% The non-degenerate asymptotic properties of
% c-mixed $\theta$ compositional kernels are derived in
% section~\ref{sec:varying_theta_kernel_limit}.

\begin{dfn}[c-mixed $\boldsymbol{\theta}$ compositional kernels]
\label{dfn:cmixed_k}
A mixed compositional kernel $\mathcal{K}_{1:n}$ of
$\theta$ PGFs
\begin{equation}
\label{eq:theta_pgf_ex}
f_k (s) =
1 - ((1 - s)^{-\theta} + c_k)^{-\frac{1}{\theta}},
\end{equation}
$k=1,2,\dots,n$, $\theta \in (0, 1]$, $c_k > 0$,
is called a c-mixed $\theta$ compositional kernel.
\end{dfn}

The $\theta$ PGFs $f_{k}$ of
% proposition~\ref{prop:ex_critical_theta_kernel} are provided in
equation~\eqref{eq:theta_pgf_ex}
% , and
have the property $f_{k}^{\prime}(1)=1$.
In the field or branching processes,
PFGs with such a property induce a critical environment
~\citep[section 5]{alsmeyer2021}.

% Proposition~\ref{prop:ex_critical_theta_kernel}
% derives c-mixed $\theta$ compositional kernels
% in closed form.

\begin{prop}[c-mixed $\boldsymbol{\theta}$ compositional kernels expressed in closed form]
\label{prop:ex_critical_theta_kernel}
A c-mixed $\theta$ compositional kernel $\mathcal{K}_{1:n}$
is expressed in closed form as
\begin{equation}
\label{eq:theta_k_ex}
\mathcal{K}_{1:n}(x, z) =
1 - \left(
(1-\rho (x, z))^{-\theta}
+ \sum_{k = 1}^{n} c_k
\right)^{-\frac{1}{\theta}},
\end{equation}
where
$\theta,~c_{k}$ are given by equation~\eqref{eq:theta_pgf_ex}, and
$\rho (x, z)$ is the correlation between
$x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$.
\end{prop}

\begin{proof}
The proposition will be proved by induction,
starting from definition~\ref{def:comp_kernel_in_varying_env}.
For $n=1$,
notice that
$K_{1:1}(x,z)=
f_{1}(\rho (x,z))$,
so equation~\eqref{eq:theta_k_ex}
reduces to equation~\eqref{eq:theta_pgf_ex}.
Assuming that equation~\eqref{eq:theta_k_ex} holds for $n=j$.
it will be shown that it also holds for $n=j+1$.
To this end,
\begin{alignat*}{2}
\mathcal{K}_{1:j+1}(x, z) &=
f_{j+1}\circ f_{j}\circ\dots\circ f_1
\left(\rho(x,z)\right) && \;
\mbox{See equation~\eqref{eq:kn:long:ve}}\\
&=
f_{j+1}\circ\mathcal{K}_{1:j}(x, z)
&& \;
\mbox{See equation~\eqref{eq:kn:long:ve}}
\\
&=
f_{j+1}\left(
1 - \left(
(1-\rho (x, z))^{-\theta}
+ \sum_{k = 1}^{j} c_k
\right)^{-\frac{1}{\theta}}
\right) && \;
\mbox{See equation~\eqref{eq:theta_k_ex}}
\\
&=
1 - \left(
(1-\rho (x, z))^{-\theta}
+ \sum_{k = 1}^{j+1} c_k
\right)^{-\frac{1}{\theta}},
&& \;
\mbox{See equation~\eqref{eq:theta_pgf_ex}}
\end{alignat*}
which completes the proof.
\end{proof}

For fixed $\theta$,
a pure $\theta$ compositional kernel $\mathcal{K}_{(l)}$
of depth $l$
from case $2$ of table~\ref{tab:thetaK}
is parameterized by $l c$,
while a c-mixed $\theta$ compositional kernel $\mathcal{K}_{1:n}$
of depth $n$ from proposition~\ref{prop:ex_critical_theta_kernel}
is parameterized by $\sum_{k = 1}^{n} c_{k}$.
If $l c = \sum_{k = 1}^{n} c_{k}$, then $K_{(l)} = K_{1:n}$.
While these kernels are equal,
their representations differ.
More specifically,
$\mathcal{K}_{(l)}$ is an $l$-fold iteration of a single $\theta$ PGF,
while $\mathcal{K}_{1:n}$ is a composition of $n$ different $\theta$ PGFs.
The difference in representation between pure and c-mixed
$\theta$ compositional kernels
%is elaborated in section~\ref{sec:varying_theta_kernel_limit}, and it
yields different asymptotic properties
(see corollary~\ref{cor:varying_theta_kernel_limit}).
% as well as different $\theta$ NNGPs (see section~\ref{theta_nngps}).

%\subsection{Asymptotic properties of c-mixed \texorpdfstring{$\boldsymbol{\theta}~$} %ccompositional kernels}
%\label{sec:varying_theta_kernel_limit}

Proposition~\ref{prop:varying_theta_kernel_limit}
provides the ininite-depth limit of any
c-mixed $\theta$ compositinal kernel $\mathcal{K}_{1:n}$.
% This limit is a function of
% the correlation $\rho(x, z)$ of the kernel inputs $x$ and $z$,
% of $\theta$ and of $c_k,~k=1,\ldots,n$.
Corollary~\ref{cor:varying_theta_kernel_limit} of
proposition~\ref{prop:varying_theta_kernel_limit}
provides infinite-depth limit properties of
% c-mixed $\boldsymbol{\theta}$ compositional kernels
$\mathcal{K}_{1:n}$
subject to conditions on the parameters $c_k,~k\in\mathbb{N}$,
of $\mathcal{K}_{1:n}$ and on the correlation
$\rho (x, z)$ between inputs $x$ and $z$ of $\mathcal{K}_{1:n}$.

\begin{prop}[Infinite-depth limit of c-mixed $\boldsymbol{\theta}$ compositional kernels]
\label{prop:varying_theta_kernel_limit}
The infinite-depth limit of a
c-mixed $\theta$ compositional kernel $K_{1:n}$ equals
\begin{equation}
\label{eq:theta_k_limit}
\lim_{n\rightarrow\infty}
\mathcal{K}_{1:n}(x,z) =
1 - \left((1 - \rho (x, z))^{-\theta} + \sum_{k = 1}^{\infty} c_{k} \right)^{-\frac{1}{\theta}},
\end{equation}
where
$\theta$ and the sequence of $c_{k},~k\in\mathbb{N}$, are given by
equation~\eqref{eq:theta_pgf_ex}, and
$\rho (x, z)$ is the correlation between
$x\in\mathbb{R}^m$ and $z\in\mathbb{R}^m$.

\end{prop}

\begin{proof}
Taking the limit of equation~\eqref{eq:theta_k_ex}
as $n$ tends to infinity
yields equation~\eqref{eq:theta_k_limit}.
\end{proof}

\begin{corol}[Infinite-depth limit properties of c-mixed $\boldsymbol{\theta}$ compositional kernels]
\label{cor:varying_theta_kernel_limit}
Let $\mathcal{K}_{1:n}$ be a c-mixed $\theta$ compositional kernel
with parameters $c_k,~k\in\mathbb{N}$ given by
equation~\eqref{eq:theta_pgf_ex}.
Moreover, let
$\rho (x, z)$ be the correlation
between inputs $x$ and $z$ of $\mathcal{K}_{1:n}$.
The following dichotomy for the infinite-depth limit of $\mathcal{K}_{1:n}$ holds.
\begin{enumerate}
\item If $\sum_{k = 1}^{\infty} c_{k} < \infty$ and $\rho (x, z) \ne 1$,
then $\lim_{n\rightarrow\infty}\mathcal{K}_{1:n}(x,z) < 1$.
\item If $\sum_{k = 1}^{\infty} c_{k} = \infty$ or $\rho (x, z) = 1$,
then $\lim_{n\rightarrow\infty}\mathcal{K}_{1:n}(x,z) = 1$.
\end{enumerate}
\end{corol}

\begin{proof}
The conclusion follows
from equation~\eqref{eq:theta_k_limit} and
from the convergence or divergence of
$\sum_{k = 1}^{\infty} c_{k}$.
\end{proof}

According to corollary~\ref{cor:varying_theta_kernel_limit},
the sum $\sum_{k = 1}^{\infty} c_{k}$
determines the infinite-depth limit 
of a c-mixed $\theta$ compositional kernel.
If the sum converges and the kernel inputs do not have a correlation of $1$,
then the limit
is controlled by $\theta$ and $c_k$ as seen from equation~\eqref{eq:theta_k_limit}.
If the sum diverges, then the limit
is degenerate, meaning that it equals $1$.

A representation of c-mixed $\theta$ compositional kernels
based on pure $\theta$ compositional kernels is provided as
an alternative way of comprehending the different asymptotic properties
of these two classes of kernels.
To this end, consider a c-mixed $\theta$ compositional kernel
$\mathcal{K}_{1:n}=f_n\circ\dots\circ f_1$,
where PGF $f_k,~k=1,\ldots,n$,
is parameterized by $\theta$ and $c_k$
according to equation~\eqref{eq:theta_pgf_ex}.
Each composition $f_k\circ\dots\circ f_1$ can be expressed as
a $k$-fold iteration
\begin{equation*}
(g_{k})_{(k)}:=
\underbrace{g_{k}\circ\dots\circ g_{k}}_\text{$k$ times}
\end{equation*}
of a single PGF $g_{k}$ from case 2 of table 1,
parameterized by $\theta$ and $(c_{1} + \cdots + c_{k}) / k$.
It follows that
$\mathcal{K}_{1:n}=(g_{n})_{(n)}$.
Moreover, each $(g_{k})_{(k)}$ induces a
pure $\theta$ compositional kernel
$(\mathcal{K}_{k})_{(k)}(x,z):=(g_k)_{(k)}(\rho (x, z))$.
Thus, the c-mixed $\theta$ compositional kernel $\mathcal{K}_{1:n}$
is constructed via the sequence of pure $\theta$ compositional kernel
$(\mathcal{K}_{k})_{(k)},~k=1,2,\dots,n$.
Thereby, the infinite-depth limit in proposition
~\ref{prop:varying_theta_kernel_limit}
is expressed as a double limit
over the sequence of $(\mathcal{K}_{k})_{(k)}$
and over increasing compositional depth $k$.
This representation of a c-mixed kernel
based on a doubly indexed sequence of pure kernels
resembles the construction of triangular arrays
with identical distributions in probability theory
~\citep[section IX.7]{feller1971}.

%\subsection{texorpdfstring{$\boldsymbol{\theta}~$} ccompositional kernels}

% \subsection{\texorpdfstring{$\boldsymbol{\theta}~$} ccompositional kernels}

% % \begin{dfn}[$\boldsymbol{\theta}$ compositional kernels]
% % \label{dfn:theta_k}
% A kernel
% is called $\theta$ compositional
% if it is a pure $\theta$ or c-mixed $\theta$ compositional kernel.
% % \end{dfn}

\section{Discussion}
\label{discussion}

This paper spans three scientific areas,
namely neural networks, GPs and branching process.
More concretely,
MLP activations and NNGP kernels are connected via PGFs.
The results of this work elucidate the effect
of activations on the covariance structure of wide MLPs.

Additionally, $\theta$ compositional kernels
are derived in closed form.
Thus,
GPs with $\theta$ compositional kernels
are amenable to explicit computations,
without requiring slower
recursive kernel computations.

An open research question is whether
NNGPs with $\theta$ compositional kernels exist,
that is whether sequences of MLPs with $\theta$ activations
converge in distribution to GPs with $\theta$ compositional kernels.
It is postulated that the answer to this question is affirmative
for some or for all $\theta$ activations.
Such a postulate is based on the observation that
$\theta$ activations converge to linear activations
in specific settings and
on the fact that linear activations
have exponentially bounded derivatives
(and more strongly, satisfy the linear envelope condition).
If the postulate holds,
then the infinite-depth limit of
NNGPs with c-mixed $\theta$ compositional kernels
is non-degenerate and can be controlled via the kernel hyperparametrization.
Furthermore, if the postulate holds,
then the asymptotic properties of deep wide MLPs
can be controlled via the choice of $\theta$ activations.

% This paper connects three fields,
% NNs, GPs and branching processes
% The underlying connection is between activations and kernels via PGFs
% This provides a mechanism to understand the role of activation
% in the asymptotic properties of an NN in the infinite depth

% 1) Explicit deep GPs
% 2) Define informally and posit existence question
% 3) Postulate it holds, using an example of linear
% 4) If it holds, it is magical because of different asymptotics
% between pure theta and c-mixed theta NNGPs

% Acknowledgments---Will not appear in anonymized version
% \acks{Acknowledgements go here.}

% \bibliography{references}
\input{main.bbl}

\appendix

\section{\texorpdfstring{$\boldsymbol{\theta}~$} pprobability generating functions}
\label{sec:theta_pgfs}

Definition \ref{dfn:theta_pgfs} reproduces the definition of
$\theta$ PGFs, as introduced in~\citet{sagitov2016}.

\begin{dfn}[$\boldsymbol{\theta}$ probability generating functions]
\label{dfn:theta_pgfs}
For $\theta \in (-1, 0) \cup (0, 1]$, consider
the PGF
\begin{equation}\label{eq:PGFMain}
f(s) =
r-(a(r-s)^{-\theta}+c)^{-1/\theta}, \theta\in (-1, 0) \cup (0, 1] .
\end{equation}
It is assumed that the parameters $\theta$, $a$, $c$ and $r$
satisfy one of the three options
\begin{equation*}
\begin{array}{llllll}
\theta\in(0,1], & a\ge1,  & c>0, & & r=1,\\
\theta \in (-1, 0) \cup (0, 1], & a\in(0,1),&  c=(1-a) (1-q)^{-\theta}, &q\in[0,1), & r=1,\\
\theta \in (-1, 0) \cup (0, 1], & a\in(0,1),&  c=(1-a) (r-q)^{-\theta}, &q\in[0,1], & r>1.
\end{array}
\end{equation*}

For $\theta=0$, consider the PGF
\begin{equation}\label{eq:PGFZero}
f(s) =
r-(r-q)^{1-a}(r-s)^{a}, \theta = 0.
\end{equation}
It is assumed that parameters $q$ and $r$ satisfy one of the two options
\begin{equation*}
\begin{array}{ll}
q \in [0, 1), & r = 1, \\
q \in [0, 1], & r > 1.
\end{array}
\end{equation*}

For $\theta=-1$, consider the PGF
\begin{equation}\label{eq:PGFMinus}
f(s) =
as+(1-a)q, \theta = - 1 .
\end{equation}
It is assumed that $a \in (0, 1)$ and $q \in [0, 1]$.

For each of the three cases
$\theta \in (-1, 0) \cup (0, 1]$,
$\theta=0$ and $\theta=-1$,
the corresponding PGF given by
equation~\eqref{eq:PGFMain},~\eqref{eq:PGFZero} and~\eqref{eq:PGFMinus}
is called a $\theta$ PGF.

\end{dfn}

For any fixed choice of $\theta$, $a$, $c$ and $r$,
an $n$-fold iteration of the corresponding $\theta$ PGF
yields a $\theta$ PGF.
Thus, an $n$-fold iteration of a $\theta$ PGF is expressed in closed form.
The families of $\theta$ PGFs and of Harris PGFs
~\citep[p. 10]{harris1972}
are the only two known PGF families
with explicit $n$-fold iterations.

% \section{Basics of Galton-Watson branching processes} % section 2.1

% \citet{athreya1972}.

% \section{Duality between multilayer perceptrons and compositional kernels} % section 2.4

% TODO: complete this appendix

% section{Normalization of \texorpdfstring{$\boldsymbol{\theta}$}--activations}

% \section{My Proof of Theorem 1}

% This is a complete version of a proof sketched in the main text.

\end{document}
