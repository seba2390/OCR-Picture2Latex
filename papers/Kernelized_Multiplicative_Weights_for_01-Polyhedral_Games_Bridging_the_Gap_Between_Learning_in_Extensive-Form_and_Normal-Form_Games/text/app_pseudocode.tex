\section{Pseudocode}\label{app:pseudocode}

Below we show pseudocode for OMWU and Vertex OMWU (\cref{sec:vertex}).

\begin{figure}[H]
    \begin{minipage}[t]{.49\textwidth}
        \begin{algorithm}[H]
            \caption{OMWU}
            \label{algo:vanilla OMWU}
            \DontPrintSemicolon
            \KwData{Finite set of choices $\cA$, learning rates $\eta\^t > 0$\!\!\!}
            \BlankLine{}
            \vspace{4mm}
            $\vl\^0,~\vm\^0 \gets \vzero\in\bbR^{\cA};~~\vlam\^0 \gets \frac{1}{|\cA|}\vone\in\Delta(\cA)$\;%
            \label{ln:vanilla init}
            \For{$t=1,2,\dots$}{
            \textbf{receive} prediction $\vm\^t\in\bbR^{\cA}$ of next loss\;
            \Comment{\color{commentcolor}set $\vec{m}\^t = \vec{0}$ for non-predictive variant]}
            $\vw\^t \gets \vl\^{t-1} - \vm\^{t-1} + \vm\^t$\;
            \vspace{8mm}
            \For{$a \in \cA$}{
            $\displaystyle
                \vlam\^t[a] \gets \frac{\vlam\^{t-1}[a]\cdot e^{-\eta\^t\,\vw\^t[a]}}{\sum_{a' \in \cA} \vlam\^{t-1}[a']\cdot e^{-\eta\^t\,\vw\^t[a']}}
            $\label{ln:vanilla lam update}
            }
            \vspace{12.5mm}
            \textbf{output} $\vlam\^t \in \Delta(\cA)$\;
            \textbf{receive} loss vector $\vl\^t \in \bbR^{\cA}$\;
            }
        \end{algorithm}%
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{.49\textwidth}
        \begin{algorithm}[H]
            \caption{Vertex OMWU}
            \label{algo:vertex OMWU}
            \DontPrintSemicolon
            \KwData{\makebox[5cm][l]{Polytope $\Omega\!\subseteq\!\bbR^d$ with vertices $\{\vv_1,\!...,\!\vv_k\!\}\!\eqqcolon\!\cV_\Omega$,}\newline learning rates $\eta\^t > 0$}
            \BlankLine{}
            $\vl\^0,~\vm\^0 \gets \vzero\in\bbR^d;~~\vlam\^0 \gets \frac{1}{|\cV_\Omega|}\vone\in\Delta(\cV_\Omega)$\;%
            \label{ln:vertex init}
            \For{$t=1,2,\dots$}{
            \textbf{receive} prediction $\vm\^t\in\bbR^d$ of next loss\;
            \Comment{\color{commentcolor}set $\vec{m}\^t = \vec{0}$ for non-predictive variant]}
            $\vw\^t \gets \vl\^{t-1} - \vm\^{t-1} + \vm\^t$\;
            \Hline{}
            \Comment{\color{commentcolor}Run the OMWU update on $\vlam$ using $\cA=\cV_\Omega$]\!\!\!\!}\vspace{.5mm}
            \For{$\vv \in \cV_\Omega$}{
            $\displaystyle
                \vlam\^t[\vv] \gets \frac{\vlam\^{t-1}[\vv]\cdot e^{-\eta\^t\,\langle\vw\^{t},\vv\rangle}}{\sum_{\vv' \in \cV_\Omega} \vlam\^{t-1}[\vv']\cdot e^{-\eta\^t\langle \vw\^{t}\!,\vv'\rangle}}
            $\!\!\!\!\!\!\label{ln:vertex lam update}
            }
            \Hline{}
            \Comment{\color{commentcolor}Compute new convex combination of vertices]\!\!\!\!}\vspace{.5mm}
            $\vx\^t \gets \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\cdot\vv$\label{ln:vertex xt}\;\vspace{1mm}
            \textbf{output} $\vx\^t \in \Omega$\;
            \textbf{receive} loss vector $\vl\^t \in \bbR^d$\;
            }
        \end{algorithm}%
    \end{minipage}
\end{figure}