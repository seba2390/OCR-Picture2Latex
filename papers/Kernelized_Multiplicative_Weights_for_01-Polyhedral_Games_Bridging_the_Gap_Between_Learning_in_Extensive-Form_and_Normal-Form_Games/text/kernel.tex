%\vspace{-1mm}
\section{Kernelized Multiplicative Weights Update}\label{sec:KOMWU}
%\vspace{-1mm}

In this section, we introduce \emph{Kernelized OMWU (KOMWU)}. Kernelized OMWU gives a way of efficiently simulating the Vertex OMWU algorithm described in \cref{sec:vertex} on polyhedral decision sets whose vertices have 0/1 integer coordinates, as long as a specific \emph{polyhedral kernel} function can be evaluated efficiently. We will assume that we are given a polytope $\Omega \subseteq \bbR^d$ with (possibly exponentially many) 0/1 integral vertices $\cV_\Omega \defeq \{\bv_1, \dots,\bv_{|\cV_\Omega|}\} \subseteq\{0,1\}^d$.
Furthermore, given a vertex $\vec{v}\in\cV_\Omega$, we will write $k \in \vec{v}$ as a shorthand for $\vec{v}[k] = 1$.

We define the \emph{0/1-polyhedral feature map} $\phi_\Omega : \bbR^d \to \bbR^{\cV_\Omega}$ associated with $\Omega$ as the function such that
\[
    \phi_\Omega(\vx)[\vv] \defeq \prod_{k \in \vv} \vx[k] \qquad\forall\,\vx \in \bbR^d, \vv \in \cV_\Omega.
    \numberthis{eq:phi Omega}
\]
Correspondingly, the \emph{0/1-polyhedral kernel} $K_\Omega$ associated with $\Omega$ is defined as the function $K_\Omega : \bbR^d \times \bbR^d \to \bbR$,
\[
    K_\Omega(\vx,\vy) \defeq \langle \phi_\Omega(\vx), \phi_\Omega(\vy) \rangle = \sum_{\vv \in \cV_\Omega} \prod_{k \in \vv} \vx[k] \, \vy[k]. \numberthis{eq:K Omega}
\]
We show that Vertex OMWU can be simulated using $d+1$ evaluation of the kernel $K_\Omega$ at every iteration. The key observation is summarized in the next theorem, which shows that the iterates $\vlam\^t$ produced by Vertex OMWU are highly structured, in the sense that they are always proportional to the feature mapping $\phi_\Omega(\vb\^t)$ for some $\vb\^t\in\bbR^d$.

\begin{theorem}\label{thm:bt}
    Consider the Vertex OMWU algorithm \eqref{eq:vertex lam update}, \eqref{eq:xt original}. At all times $t\ge 0$, the vector $\vb\^t \in \bbR^d$ defined as
    \[
        \vb\^t[k] \defeq \exp\mleft\{-\sum_{\tau=1}^t\eta\^\tau\,\vw\^\tau[k]\mright\}
        \numberthis{eq:bt}
    \]
    for all $k=1,\dots,d$, is such that
    \[
        \vlam\^t = \frac{ \phi_\Omega(\vb\^t) }{ K_\Omega(\vb\^t, \vone)}.\numberthis{eq:b ratio}
    \]
\end{theorem}%\vspace{-3mm}
\begin{proof}%
    By induction.
    \begin{itemize}[nosep,leftmargin=5mm]
        \item At time $t = 0$, the vector $\vb\^0$ is $\vb\^0 = \vone \in \bbR^d$. By definition of the feature map~\eqref{eq:phi Omega}, $\phi_\Omega(\vone) = \vone \in \bbR^{\cV_\Omega}$. So, $K_\Omega(\vb\^0,\vone) = \sum_{\vv\in\cV_\Omega} 1 = |\cV_\Omega|$ and hence the right-hand side of~\eqref{eq:b ratio} is $\frac{1}{|\cV_\Omega|}\vec{1}$, which matches $\vlam\^0$ produced by Vertex OMWU, as we wanted to show.
        \item Assume the statement holds up to some time $t-1 \ge 0$. We will show that it holds at time $t$ as well.
              Since $\bv$ has integral 0/1 coordinates, we can write
              \[
                  \exp\{-\eta\^t\langle\vw\^{t},\vv\rangle\} &= \exp\mleft\{
                  -\eta\^t\,\sum_{k\in\vv} \vw\^t[k]
                  \mright\}\\
                  &= \prod_{k\in\vv} \exp\{-\eta\^t\,\vw\^t[k]\}.
                  \numberthis{eq:exp inner}
              \]
              From the inductive hypothesis and~\eqref{eq:phi Omega}, for all $\vv\in\cV_\Omega$,
              \[
                  \vlam\^{t-1}[\vv] &= \frac{\phi_\Omega(\vb\^{t-1})[\vv]}{K_\Omega(\vb\^{t-1},\vone)}
                  = \frac{\prod_{k\in\vv}\vb\^{t-1}[k]}{K_\Omega(\vb\^{t-1},\vone)}. \numberthis{eq:inductive hyp}
              \]
              Plugging~\eqref{eq:exp inner} and~\eqref{eq:inductive hyp} into~\eqref{eq:vertex lam update}, we have the inductive step
              \[
                  \vlam\^{t}[\vv] &= \frac{
                  \prod_{k\in\vv}\vb\^{t-1}[k]\exp\{-\eta\^t\,\vw\^t[k]\}
                  }{
                  \sum_{\vv\in\cV_\Omega}\prod_{k\in\vv}\vb\^{t-1}[k]\exp\{-\eta\^t\,\vw\^t[k]\}
                  }\\
                  &= \frac{\phi_\Omega(\vb\^{t})[\vv]}{K_\Omega(\vb\^{t}, \vone)}
              \]
              for all $\vv \in \cV_\Omega$, where in the last step we used the fact that $\vb\^t[k] = \vb\^{t-1}[k]\exp\{-\eta\^t\,\vw\^t[k]\}$ by~\eqref{eq:bt}. %
              \qedhere
    \end{itemize}
\end{proof}


The structure of $\vlam\^t$ uncovered by \cref{thm:bt} can be leveraged to compute the iterate $\vx\^t$ produced by Vertex OMWU, \ie the convex combination of the vertices
\eqref{eq:xt original},
using $d+1$ evaluations of the kernel $K_\Omega$. We do so by extending an idea of \citet[eq.~5.2]{Takimoto03:Path}.

\begin{theorem}\label{thm:bt to xt}
    Let $\vb\^t$ be as in \cref{thm:bt}. For each $h =1,\dots,d$, let $\ebar_h \in \bbR^d$ be defined as the indicator vector
    %\vspace{-3mm}
    \[
        \ebar_h[k] \defeq \bbone_{k\neq h} \defeq \begin{cases} 0 & \text{if } k = h\\ 1 & \text{if } k \neq h.\end{cases}
        \numberthis{eq:ebar}
    \]
    Then, at all $t \ge 1$, the iterate $\vx\^t\!\in\!\Omega$ produced by Vertex OMWU can be written as
    \[
        \vx\^t \! = \! \mleft(\!
        1 - \frac{K_\Omega(\vb\^t, \ebar_1)}{K_\Omega(\vb\^t, \vone)}, \dots,
        1 - \frac{K_\Omega(\vb\^t, \ebar_d)}{K_\Omega(\vb\^t, \vone)}
        \!\mright).\numberthis{eq:xt}
    \]
\end{theorem}
\begin{proof}%
    The proof crucially relies on the observation that for all $h=1,\dots,d$, the feature map $\phi_\Omega(\ebar_h)$ satisfies
    \[
        \phi_\Omega(\ebar_h)[\vv] = \prod_{k\in\vv} \ebar_h[k]
        = \prod_{k\in\vv}\bbone_{k\neq h} = \bbone_{h\notin \vv},
        \quad \forall\,\vv\in\cV_\Omega.
    \]
    Using the fact that $\phi_\Omega(\vone) = \vone$, we conclude that
    \[
        \phi_\Omega(\vone)[\vv] - \phi_\Omega(\ebar_h)[\vv] = \bbone_{h \in \vv}, \quad\forall\, h = 1,\dots,d.\numberthis{eq:diff phi}
    \]
    Therefore, for all $k = 1,\dots,d$, we obtain
    \[
        \vx\^t[k] &\overset{\mathclap{\eqref{eq:xt original}}}{=} \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\cdot\vv[k] = \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\cdot\bbone_{k\in\vv}\\
        &= \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\cdot(\phi_\Omega(\vone)[\vv] - \phi_\Omega(\ebar_k)[\bv])\\
        &= \frac{\langle\phi_\Omega(\vb\^t),\phi_\Omega(\vone)\rangle - \langle\phi_\Omega(\vb\^t), \phi_\Omega(\ebar_k)\rangle}{K_\Omega(\vb\^t,\vone)}\\
        &= \frac{K_\Omega(\vb\^t\!,\vone) \!-\! K_\Omega(\vb\^t\!, \ebar_k)}{K_\Omega(\vb\^t,\vone)} = 1 \!-\! \frac{K_\Omega(\vb\^t\!, \ebar_k)}{K_\Omega(\vb\^t,\vone)},
    \]
    where the second equality follows from the integrality of $\vv \in \cV_\Omega$, the third from \eqref{eq:diff phi}, the fourth from \cref{thm:bt}, and the fifth from the definition of
    $K_\Omega$ %
    \eqref{eq:K Omega}.
\end{proof}

%\vspace{-1mm}
Combined, \cref{thm:bt,thm:bt to xt} suggest that by keeping track of the vectors $\vb\^t$ instead of $\vlam\^t$, updating them using \cref{thm:bt} and reconstructing the iterates $\vx\^t$ using \cref{thm:bt to xt}, Vertex OMWU can be simulated efficiently. We call the resulting algorithm, given in \cref{algo:kernelized OMWU}, \emph{Kernelized OMWU (KOMWU)}. Similarly, we call \emph{Kernelized MWU} the non-optimistic version of KOMWU obtained as the special case in which $\vm\^t = \vzero$ at all $t$. In light of the preceding discussion, we have the following.

\begin{theorem}\label{thm:kernel omwu equivalent}
    Kernelized OMWU produces the same iterates $\vec{x}\^t$ as Vertex OMWU when it receives the same sequence of predictions $\vec{m}\^t$ and losses $\vec{\ell}\^t\in\bbR^d$. Furthermore, each iteration of KOMWU runs in time proportional to the time required to compute the $d+1$ kernel evaluations $\{K_\Omega(\vb\^t, \vone), K_\Omega(\vb\^t, \ebar_1), \dots, K_\Omega(\vb\^t,\ebar_d)\}$.
\end{theorem}


\begin{figure}[t]\centering
    \makeatletter
    \newcommand{\removelatexerror}{\let\@latex@error\@gobble} %
    \makeatother
    \removelatexerror
    \scalebox{.95}{\begin{algorithm}[H]
            \caption{Kernelized OMWU (KOMWU)}
            \label{algo:kernelized OMWU}
            \DontPrintSemicolon
            $\vl\^0,~\vm\^0,~\vs\^0 \gets \vzero\in\bbR^d$\Comment*{\color{commentcolor}Initialization]\!\!\!\!}
            \For{$t=1,2,\dots$}{\vspace{-.5mm}
            \textbf{receive} prediction $\vm\^t \in \bbR^d$ of next loss\;
            \Comment{\color{commentcolor}set $\vec{m}\^t = \vec{0}$ for non-predictive variant]}
            \vspace{-1mm}\Hline{}\vspace{-.5mm}
            \Comment{\color{commentcolor}Compute $\vb\^t$ according to \cref{thm:bt}]}\vspace{.0mm}
            $\vw\^t \gets \vl\^{t-1} - \vm\^{t-1}+\vm\^t$\;
            $\vs\^t \gets \vs\^{t-1} + \eta\^t\vw\^t$\Comment*{\color{commentcolor}{\small$\vs\^t = \sum \eta\^\tau\vw\^\tau$}]\!\!\!\!}
            \For{$k=1,\dots,d$}{
            $\vb\^t[k]\gets \exp\{-\vs\^t[k]\}$\Comment*{\color{commentcolor}see \eqref{eq:bt}]\!\!\!\!}
            }
            \vspace{-1mm}\Hline{}\vspace{-.5mm}
            \Comment{\color{commentcolor}Produce iterate $\vx\^t$ according to \cref{thm:bt to xt}]\!\!\!\!}\vspace{.0mm}
            $\vx\^t \gets \vec{0} \in \bbR^d$\;
            $\alpha\gets K_\Omega(\vb\^t, \vone)$\Comment*{\color{commentcolor}$K_\Omega$ is defined in \eqref{eq:K Omega}]\!\!\!\!}
            \For{$k=1,\dots,d$}{
            $\vx\^t[k] \gets 1 - K_\Omega(\vb\^t, \ebar_k) \,/\, \alpha$\Comment*{\color{commentcolor}see \eqref{eq:xt}]\!\!\!\!}
            }
            \textbf{output} $\vx\^t \in \Omega$ and
            \textbf{receive} loss vector $\vl\^t \in \bbR^d$\!\!\!\!\;
            }\vspace{-1mm}
        \end{algorithm}}
    %\vspace{-3mm}
\end{figure}