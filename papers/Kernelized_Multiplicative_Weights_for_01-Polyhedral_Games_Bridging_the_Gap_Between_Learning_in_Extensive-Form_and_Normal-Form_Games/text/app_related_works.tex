\section{Additional Related Work}\label{app:related works}
\subsection{More Results for Optimistic Algorithms in Games}
For individual regret in multi-player general-sum NFGs, \citet{Syrgkanis15:Fast} first show $\bigOh(T^{1/4})$ regret for general optimistic OMD and FTRL algorithms.
The result is improved to $\bigOh(T^{1/6})$ by \citep{Chen20:Hedging}, but only for OMWU in two-player NFGs.
\citet{Daskalakis21:Near} show that OMWU enjoys $\bigOh(\log^4 T)$ regret in multi-player general-sum NFGs.

As for last-iterate convergence in two-player zero-sum games, \citet{Daskalakis18:Last} show an asymptotic result for OMWU under the unique Nash equilibrium assumption.
\citet{Wei21:Linear} further show a linear convergence rate while allowing larger learning rates under the same assumption.
\citet{Hsieh21:Adaptive} show another asymptomatic convergence result without the assumption.
It is also worth noting that OGDA, another popular optimistic algorithm, has been shown its last-iterate convergence in general polyhedron games \citep{Wei21:Linear}.
\subsection{Approaches in Online Combinatorial Optimization}
Besides performing MWU/OMWU over vertices, we review two additional approaches in online combinatorial optimization:

\paragraph{OMD over the Convex Hull}
This approach is running Online Mirror Descent (OMD) over the convex hull \citep{koolen2010hedging,audibert2014regret}.
It is well known that OMD with the negative entropy regularizer results in a (dimension-wise) multiplicative weight update.
For the case that the set of vertices is a standard basis, this algorithm coincides with the MWU over the probability simplex.
However, for general cases, it requires to project back to the convex hull and the procedure may not be efficient.
\citet{helmbold2009learning} first used this approach for permutations, and \citet{koolen2010hedging} generally studied it for arbitrary 0/1 polyhedral sets and show its efficiency for more cases.

\paragraph{FTPL}
Another approach is called Follow the Perturbed Leader \citep{Kalai05:Efficient}.
This approach adds a random perturbation to the cumulative loss vector, and greedily selects the vertex with minimal perturbed loss.
The latter procedure corresponds to linear optimization over the set of vertices, which can be solved efficiently for most cases of interest.
We are not aware of any previous work using this approach for EFGs though.
