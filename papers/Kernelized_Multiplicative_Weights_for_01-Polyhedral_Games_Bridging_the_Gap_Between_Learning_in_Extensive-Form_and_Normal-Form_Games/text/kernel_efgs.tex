%\vspace{-1mm}
\section{KOMWU in Extensive-Form Games}\label{sec:kernel efgs}
%\vspace{-1mm}

In this section, we show how the general theory we developed in \cref{sec:kernel efgs}
applies to extensive-form game, \ie tree-form games that incorporate sequential and simultaneous moves, and imperfect information. The central result of this section, \cref{thm:KOMWU in EFGs}, shows that OMWU on the normal-form representation of any EFG can be simulated in linear time in the game tree size via KOMWU, contradicting the popular wisdom that working with the normal form of an extensive-form game is intractable.

%\vspace{-1mm}
\subsection{Preliminaries on Extensive-Form Games}\label{sec:efg notation}
%\vspace{-1mm}

We now briefly recall standard concepts and notation about extensive-form games which we use in the rest of the section. More details and an example are available in \cref{app:efgs}.

In an $m$-player perfect-recall extensive-form game, each player $i\in\range{m}$ faces a tree-form sequential decision problem (TFSDP). In a TFSDP, the player interacts with the environment in two ways: at \emph{decision points}, the
agent must act by picking an action from a set of legal actions; at
\emph{observation points}, the agent observes a signal drawn from a set of
possible signals. We denote the set of decision points of player~$i$ as $\cJ_i$. The set of actions available at decision point $j\in\cJ_i$ is denoted $A_j$. A pair $(j,a)$ where $j\in \cJ_i$ and $a \in A_j$ is called a \emph{non-empty sequence}. The set of all non-empty sequences of player~$i$ is denoted as $\Sigma^*_i \defeq \{(j,a): j\in\cJ, a\in A_j\}$.
For notational convenience, we will often denote an element $(j,a)$ in
$\Sigma_i^*$ as $ja$ without using parentheses. Given a decision point $j \in \cJ_i$, we denote by $p_j$ its
\emph{parent sequence}, defined as the last sequence (that is, decision
point-action pair) encountered on the path from the root of the
decision process to $j$.
If the agent does not act before $j$ (that is, $j$ is the root of the
process or only observation points are encountered on the path from
the root to $j$), we let $p_j$ be set to the special element $\emptyseq$, called the \emph{empty sequence}. We let $\Sigma_i \defeq \Sigma_i^* \cup \{\emptyseq\}$. Given a $\sigma \in \Sigma_i$, we let $\mathcal{C}_\sigma \defeq \{j\in\cJ_i: p_j = \sigma\}$.


An $m$-player extensive-form game is a polyhedral convex game (\cref{sec:vertex}) $\Gamma = (m, \{Q_i\}, \{U_i\})$, where the convex polytope of mixed strategies $Q_i$ of each player $i\in\range{m}$ is called a \emph{sequence-form strategy space} \citep{Romanovskii62:Reduction,Stengel96:Efficient,Koller96:Efficient}, and is defined as
\newcommand*\circled[1]{%
    \tikz[baseline=(C.base)]\node[draw,circle,inner sep=0.8pt](C) {\small #1};\!%
}
\[
    Q_i \defeq \mleft\{\vx \in \bbR^{\Sigma_i}: \!\!\begin{array}{l} \circled{1}~~ \vx[\emptyseq] = 1, \\[1mm] \circled{2}~~ \vx[p_j] = \sum_{a\in A_j} \!\vx[ja] ~~\forall j\in\cJ_i \end{array}\!\!\!\mright\}.
\]

It is known that the set of vertices of $Q_i$ are the \emph{deterministic} sequence-form strategies $\Pi_i \defeq Q_i \cap \{0,1\}^{\Sigma_i}$.
We mention the following result (see \cref{app:proofs}).
%\vspace{-1mm}
\begin{restatable}{proposition}{propnumvertices}\label{prop:efg vertex count}
    The number of vertices of $Q_i$ is upper bounded by $A^{\|Q_i\|_1}$, where $A \!\defeq\! \max_{j\in\cJ_i} |A_j|$ is the largest number of possible actions, and $\|Q_i\|_1 \defeq \max_{\vec{q}\in Q_i}\|\vec{q}\|_1$.
\end{restatable}

%\vspace{-1mm}
We will often need to describe strategies for \emph{subtrees} of the TFDSM faced by each player $i$. We use the notation $j' \succeq j$ to denote the fact that $j'\in\cJ_i$ is a descendant of $j\in\cJ_i$, and $j'\succ j$ to denote a strict descendant (\ie $j'\succeq j \land j'\neq j$). For any $j\in\cJ_i$ we let $\Sigma^*_{i,j} \defeq \{j'a': j' \succeq j, a' \in A_{j'}\}$ denote the set of non-empty sequences in the subtree rooted at $j$. The set of sequence-form strategies for that subtree $j$ is defined as the convex polytope
\[
    Q_{i,j} \!\defeq\! \mleft\{\!\vx \in \bbR^{\Sigma^*_{i,j}}\!: \!\!\!\begin{array}{l} \circled{1}~~ \sum_{a\in A_j}\vx[ja] = 1, \\[1mm] \circled{2}~~ \vx[p_{j'}] \!=\! \sum_{a\in A_{j'}} \!\vx[j'a] ~~\forall j'\succ j\end{array}\!\!\!\mright\}.
\]
Correspondingly, we let $\Pi_{i,j} \defeq Q_{i,j} \cap \{0,1\}^{\Sigma^*_{i,j}}$ denote the set of vertices of $Q_{i,j}$, each of which is a deterministic sequence-form strategy for the subtree rooted at $j$.


\subsection{Linear-time Implementation of KOMWU}

For any player $i$, the 0/1-polyhedral kernel $K_{Q_i}$ associated with the player's sequence-form strategy space $Q_i$ can be evaluated in linear time in the number of sequences $|\Sigma_i|$ of that player. To do so, we introduce a \emph{partial kernel function} $K_j: \bbR^{\Sigma_i}\times\bbR^{\Sigma_i} \to \bbR$ for every decision point $j\in\cJ_i$, %
\[
    K_j(\vec{x}, \vec{y}) \defeq \sum_{\vec{\pi} \in \Pi_{i,j}}\prod_{\sigma \in \vec{\pi}} \vec{x}[\sigma]\,\vec{y}[\sigma].
    \numberthis{eq:def Kj}
\]


\begin{restatable}{theorem}{thmefgkernel}\label{thm:efg kernel computation}
    For any vectors $\vec{x},\vec{y} \in\bbR^{\Sigma_i}$, the two following recursive relationships hold:
    \[
        K_{Q_i}(\vec{x}, \vec{y}) &= \vec{x}[\emptyseq]\,\vec{y}[\emptyseq]\prod_{j \in \mathcal{C}_\emptyseq} K_j(\vec{x},\vec{y}),
        \numberthis{eq:efg kernel computation}
    \]
    and, for all decision points $j\in\cJ_i$,
    \[
        K_j(\vec{x}, \vec{y}) &=\!\sum_{a\in A_j} \mleft(\vec{x}[ja]\,\vec{y}[ja]\prod_{j' \in \mathcal{C}_{ja}} K_{j'}(\vec{x}, \vec{y})\mright).\numberthis{eq:efg kernel computation 2}
    \]
    In particular, \cref{eq:efg kernel computation,eq:efg kernel computation 2} give a recursive algorithm to evaluate the polyhedral kernel $K_{Q_i}$ associated with the sequence-form strategy space of any player $i$ in an EFG in linear time in the number of sequences $|\Sigma_i|$.
\end{restatable}


\cref{thm:efg kernel computation} shows that the kernel $K_{Q_i}$ can be evaluated in linear time (in $|\Sigma_i|$) at any $(\vx,\vy)$. So, the KOMWU algorithm (\cref{algo:kernelized OMWU}) can be trivially implemented for $\Omega = Q_i$ in quadratic $\bigOh(|\Sigma_i|^2)$ time per iteration by directly evaluating the $|\Sigma_i|+1$ kernel evaluations $\{K_{Q_i}(\vb\^t, \vone)\} \cup \{K_{Q_i}(\vb\^t, \ebar_{\sigma}): \sigma \in \Sigma_i\}$ needed at each iteration, where $\ebar_{\sigma}\in\bbR^{\Sigma_i}$, defined in~\eqref{eq:ebar} for the general case, is the vector whose components are $\ebar_{\sigma}[\sigma']\defeq \bbone_{\sigma\neq \sigma'}$ for all $\sigma,\sigma'\in\Sigma_i$.
We refine that result by showing that an implementation of
KOMWU with \emph{linear}-time (\ie $\bigOh(|\Sigma_i|)$) per-iteration complexity exists, by exploiting the structure of the particular set of
kernel evaluations needed at every iteration. In particular, we rely on the following observation.

\begin{restatable}{proposition}{propefgratio}\label{prop:efg ratio}
    For any player $i\in\range{m}$, vector $\vec{x} \in \bbR_{>0}^{\Sigma_i}$, and sequence $ja \in \Sigma^*_i$,
    \[
        \frac{1 \!-\! K_{Q_i}(\vec{x}, \bar{\vec{e}}_{ja}) / K_{Q_i}(\vec{x},\! \vone)}{1 \!-\! K_{Q_i}(\vec{x}, \bar{\vec{e}}_{p_j}) / K_{Q_i}(\vec{x},\! \vone)} = \frac{\vec{x}[ja]\prod_{j'\in\mathcal{C}_{ja}}\! K_{j'}(\vec{x},\!\vone)}{K_j(\vec{x},\! \vone)}.
    \]
\end{restatable}

In order to compute $\{K_{Q_i}(\vb\^t, \ebar_{\sigma}): \sigma \in \Sigma_i\}$ in cumulative $\bigOh(|\Sigma_i|)$ time, we then do the following.
\begin{enumerate}[nosep,left=0mm]
    \item We compute the values $K_j(\vb\^t, \vone)$ for all $j \in \cJ_i$ in cumulative $\bigOh(|\Sigma_i|)$ time by using \eqref{eq:efg kernel computation 2}.\label{step:one}
    \item We compute the ratio $K_{Q_i}(\vb\^t, \ebar_\emptyseq) / K_{Q_i}(\vb\^t, \vone)$ by evaluating the two kernel separately using \cref{thm:efg kernel computation}, spending $\bigOh(|\Sigma_i|)$ time.\label{step:two}
    \item We repeatedly use \cref{prop:efg ratio} in a top-down fashion along the
          tree-form decision problem of player~$i$ to compute the ratio $K_{Q_i}(\vb\^t, \ebar_{ja}) / K_{Q_i}(\vb\^t, \vone)$ for each sequence $ja\in\Sigma_i^*$ given the value of the parent ratio $K_{Q_i}(\vb\^t, \ebar_{p_j}) / K_{Q_i}(\vb\^t, \vone)$ and the partial kernel evaluations $\{K_j(\vb\^t, \vone): j \!\in\! \cJ_i\}$ from Step~\ref{step:one}. For each $ja\in\Sigma_i^*$, \cref{prop:efg ratio} gives a formula whose runtime is linear in the number of children decision points $|\mathcal{C}_{ja}|$ at that sequence. Therefore, the cumulative runtime required to compute all ratios $K_{Q_i}(\vb\^t, \ebar_{ja}) / K_{Q_i}(\vb\^t, \vone)$ is
          $\bigOh(|\Sigma_i|)$.\label{step:three}
    \item By multiplying the ratios computed in Step~\ref{step:three} by the value of $K_{Q_i}(\vb\^t, \vone)$ computed in Step~\ref{step:two}, we can easily recover each $K_{Q_i}(\vb\^t, \ebar_{\sigma})$ for every $\sigma \in \Sigma_i^*$.
\end{enumerate}

Hence, we have just proved the following.

\begin{theorem}\label{thm:KOMWU in EFGs}
    For each player $i$ in a perfect-recall extensive-form game, the Kernelized OMWU algorithm can be implemented exactly, with a per-iteration complexity linear in the number of sequences $|\Sigma_i|$ of that player.
\end{theorem}

\subsection{KOMWU Regret Bounds and Convergence}\label{sec:efg analysis}




If the players in an EFG run KOMWU,
then we can combine \cref{thm:kernel omwu equivalent} with standard OMWU regret bounds, \cref{prop:efg vertex count,prop:omwu near optimal,prop:omwu optimal sum,prop:omwu last iterate} to get the following:
%\vspace{-3mm}
\begin{theorem}
    In an EFG, after $T$ rounds of learning under the COLS, KOMWU satisfies
    %\vspace{-2.5mm}
    \begin{enumerate}[nosep,nolistsep,left=0mm]
        \item
              A player $i$ using KOMWU with $\eta\^t \defeq \eta = \sqrt{8\log(A) \|Q_i\|_1}/\sqrt{T}$ is guaranteed to incur regret at most $R^T_i = \bigOh(\sqrt{\|Q_i\|_1\log(A)T})$.
        \item
              There exist $C, C' > 0$ such that if all $m$ players learn using KOMWU with constant learning rate $\eta\^t \defeq \eta \leq 1/(Cm\log^4T)$, then each player is guaranteed to incur regret at most $\frac{\log (A_i) \|Q_i\|_1}{\eta} + C'\log T$.
        \item
              If all $m$ player learn using KOMWU with $\eta\^t \defeq \eta \leq 1/\sqrt{8}(m-1)$, then the sum of regrets is at most $\sum_{i=1}^m R_i^T = \bigOh(\max_{i=1}^m\{\|Q_i\|_1\log A_i\} \frac{m}{\eta})$.
        \item
              For two-player zero-sum EFGs, if both players learn using KOMWU, then there exists a schedule of learning-rates $\eta^{(t)}$ such that the iterates converge to a Nash equilibrium.
              Furthermore, if the NFG representation of the EFG has a unique Nash equilibrium and both players use learning rates $\eta\^t = \eta \leq 1/8$, then the iterates converge to a Nash equilibrium at a linear rate $C (1+C')^{-t}$, where $C,C'$ are constants that depend on the game.
    \end{enumerate}
    \label{thm:komwu efg}
\end{theorem}
%\vspace{-2mm}
Prior to our result, the strongest regret bound for methods that take linear time per iteration was based on instantiating e.g. follow the regularized leader (FTRL) or its optimistic variant with the dilatable global entropy regularizer of \citet{Farina21:Better}.
For FTRL this yields a regret bound of the form $\bigOh(\sqrt{\log(A)\,\|Q\|_1^2 T})$.
For optimistic FTRL this yields a regret bound of the form $\bigOh(\log(A)\,\|Q\|_1^2 \sqrt{m} T^{1/4})$, when every player in an $m$-player game uses that algorithm and appropriate learning rates.

Our algorithm improves the state-of-the-art rate in two ways.
First, we improve the dependence on game constants by almost a square root factor, because our dependence on $\|Q\|_1$ is smaller by a square root, compared to prior results.
Secondly, in the multi-player general-sum setting, every other method achieves regret that is on the order of $T^{1/4}$, whereas our method achieves regret on the order of $\log^4(T)$.
In the context of two-player zero-sum EFGs, the bound on the sum of regrets in \cref{thm:komwu efg} guarantees convergence to a Nash equilibrium at a rate of $\bigOh(\max_i \|Q_i\|_1\log A_i / T)$.
This similarly improves the prior state of the art.




\citet{Lee21:Last} showed the first last-iterate results for EFGs using algorithms that require linear time per iteration. In particular, they show that the dilated entropy DGF combined with optimistic online mirror descent leads to last-iterate convergence at a linear rate.
However, their result requires learning rates $\eta \leq 1/(8|\Sigma_i|)$. This learning rate is impractically small in practice. In contrast, our last-iterate linear-rate result for KOMWU allows learning rates of size $1/8$.
That said, our result is not directly comparable to theirs. The existence of a unique Nash equilibrium in the EFG representation is a necessary condition for uniqueness in the NFG representation. However, it is possible that the NFG has additional equilibria even when the EFG does not.
\citet{Wei21:Linear} conjecture that linear-rate convergence holds even without the assumption of a unique Nash equilibrium. If this conjecture turns out to be true for NFGs, then \cref{thm:kernel omwu equivalent} would immediately imply that KOMWU also has last-iterate linear-rate convergence without the uniqueness assumption.


\subsection{Experimental Evaluation}
We numerically investigate agents learning under the COLS in Kuhn and Leduc poker \citep{Kuhn50:Simplified,Southey05:Bayes}. %
We compare the maximum per-player regret cumulated by KOMWU for four different choices of constant learning rate, against that cumulated by two standard algorithms from the extensive-form game solving literature (CFR and CFR(RM+)). More details about the games and the algorithms are given in \cref{app:experiments}. Results are shown in \cref{fig:experiments}. We observe that the per-player regret cumulated by KOMWU plateaus and remains constants, unlike the CFR variants. This behavior is consistent with the near-optimal per-player regret guarantees of KOMWU (\cref{thm:komwu efg}).

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{figs/single_row-crop.pdf}
    \vspace{-4mm}
    \caption{Maximum per-player regret cumulated by KOWMU compared to two variants of the CFR algorithm.}
    \label{fig:experiments}
\end{figure}

