\section{Introduction}
\newcommand{\comparisontablehere}{
    \begin{table*}[th]
        \centering
        \scalebox{.95}{\begin{tabular}{m{10cm}m{5cm}>{\centering\arraybackslash}m{1.6cm}}
                Algorithm                                                                    & Per-player regret bound                             & \makebox[1cm]{Last-iter. conv.$^\dagger$~~~} \\
                \toprule
                CFR (regret matching / regret matching$^+$) \hfill\citep{Zinkevich07:Regret} & $\bigOh(\sqrt{A}\,\|Q\|_1 \ T^{1/2})$               & no                                           \\
                CFR (MWU) \hfill\citep{Zinkevich07:Regret}                                   & $\bigOh(\sqrt{\log A}\,\|Q\|_1 \ T^{1/2})$          & no                                           \\
                FTRL / OMD (dilated entropy) \hfill\citep{Kroer20:Faster}                    & $\bigOh(\sqrt{\log A}\,2^{D/2}\,\|Q\|_1 \ T^{1/2})$ & no                                           \\
                FTRL / OMD (dilatable global entropy) \hfill\citep{Farina21:Better}          & $\bigOh(\sqrt{\log A}\,\|Q\|_1 \ T^{1/2})$          & no                                           \\
                \rowcolor{gray!20}\bf Kernelized MWU\hfill (this paper)                      & $\bigOh(\sqrt{\log A}\,\sqrt{\|Q\|_1}\ T^{1/2})$    & \textbf{no}                                  \\
                \midrule
                Optimistic FTRL / OMD (dilated entropy) \hfill\citep{Kroer20:Faster}         & $\bigOh(\sqrt{m}\log(A)\,2^D\,\|Q\|_1^2 \ T^{1/4})$ & \phantom{$^*$}yes$^*$                        \\
                Optimistic FTRL / OMD (dilatable gl. ent.) \hfill\citep{Farina21:Better}     & $\bigOh(\sqrt{m}\log(A)\,\|Q\|_1^2\ T^{1/4})$       & no                                           \\
                \rowcolor{gray!20}\bf Kernelized OMWU\hfill (this paper)                     & $\bigOh(m\log(A)\,\|Q\|_1\ \log^4(T))$              & \textbf{yes}                                 \\
                \bottomrule
            \end{tabular}}
        \caption{
            Properties of various no-regret algorithms for EFGs.
            All algorithms take linear time to perform an iteration.
            The first set of rows are for non-optimistic algorithms. The second set of rows are for optimistic algorithms.
            The regret bounds are per player and apply to multiplayer general-sum games. They depend on the maximum number of actions $A$ available at any decision point, the maximum $\ell_1$ norm $\|Q\|_1 = \max_{q\in Q} \|q\|_1$ over the player's decision polytope $Q$, the depth $D$ of the decision polytope, and the number of players $m$.
            Optimistic algorithms have better asymptotic regret, but worse dependence on the game constants $m$, $A$, and $\|Q\|_1$.
            Note that our algorithms achieve better dependence on $\|Q\|_1$ compared to all existing algorithms.
            $^\dagger$Last-iterate convergence results are for two-player zero-sum games, and some results rely on the assumption of a unique Nash equilibrium---see \cref{sec:efg analysis} for details.
            $^*$\citet{Lee21:Last}.
        }
        \label{tab:bounds}
        % %\vspace{-1mm}
    \end{table*}
}
\comparisontablehere


Online learning in the context of \emph{normal-form games} (NFGs) has been studied extensively.
A classic motivation for this study is that when every player in an NFG learns from $T$ rounds of repeated play using a no-regret learning algorithm such as multiplicative weights update (MWU), the average product distribution of play is a $\bigOh(1/\sqrt{T})$-approximate Nash equilibrium in two-player zero-sum games, and a $\bigOh(1/\sqrt{T})$-approximate coarse-correlated equilibrium in multiplayer general-sum games.
In the last decade, much stronger results have been obtained when each player employs an \emph{optimistic} no-regret learner such as the \emph{optimistic MWU} (OMWU) algorithm~\citep{Rakhlin13:Online,Rakhlin13:Optimization,Syrgkanis15:Fast}.
For example, in zero-sum NFGs OMWU enables convergence to a Nash equilibrium at a rate of $\bigOh(1/T)$ and various \emph{last-iterate} guarantees~\citep{Daskalakis18:Last,Lei21:Last,Wei21:Linear}. For general-sum NFGs, polylogarithmic regret bounds have been shown when every player uses OMWU~\citep{Daskalakis21:Near}, implying convergence to a coarse-correlated equilibrium at a $\tilde{\bigOh}(1/T)$ rate.

In this paper we study \emph{extensive-form games} (EFGs), a much richer class of games that explicitly model sequential (or simultaneous) interaction, stochastic outcomes, and imperfect information. Because of their sequential nature, the number of deterministic strategies in an EFG is exponential in the size of the game, unlike for NFGs.
Computing, or approximating, Nash equilibria of large EFGs has been a key component of recent AI milestones where AIs were created that beat human poker players~\citep{Bowling15:Heads,Brown19:Superhuman,Brown17:Superhuman,Moravvcik17:DeepStack}.
These results relied on online learning algorithms for the decision sets of the players in an EFG, where each iteration of the algorithm is performed in linear time in the game tree size (which is crucial due to the large size of these games).

Online learning results for EFGs are generally somewhat harder to come by, and have often lagged behind results for NFGs. This is due to the more complicated combinatorial structure of the decision spaces in EFGs.
For example, the following concepts were all developed later for EFGs than for NFGs, and sometimes with weaker guarantees: good distance measures~\citep{Hoda10:Smoothing,Kroer15:Faster,Kroer20:Faster,Farina21:Better}, optimistic regret-minimization algorithms~\citep{Farina19:Optimistic,Farina19:Stable}, and last-iterate convergence results~\citep{Wei21:Linear,Lee21:Last}. Very recent NFG results such as the polylogarithmic regret bounds for OMWU dynamics in general-sum NFGs~\citep{Daskalakis21:Near} do not currently have an analogue for EFGs.

In principle, an EFG can be represented as a NFG where each action in the NFG corresponds to an assignment of decisions at \emph{each} decision point in the EFG.
One could then run, \eg OMWU on this normal-form representation, and receive all the guarantees obtained for NFGs directly.
However, this reduction is exponentially-large in the size of the EFG representation, and for this reason the normal-form representation was viewed as impractical.
This leads to the necessity of developing the various more complicated approaches mentioned in the previous paragraph.

We contradict popular belief and show that it is possible to work with the normal form efficiently: we provide a kernel-based reduction from EFGs to NFGs that allows us to simulate MWU and OMWU on the normal-form representation, using only linear (in the EFG size) time per iteration.
Our algorithm, \emph{Kernelized OMWU} (KOMWU), closes the gap between NFGs and EFGs; KOMWU achieves all the guarantees provided by the various normal-form results mentioned previously, as well as any future results on OMWU for NFGs.
As an unexpected byproduct, KOMWU obtains new state-of-the-art regret bounds among all online learning algorithms for EFGs (see also \cref{tab:bounds}); we improve the dependence on the maximum $\ell_1$ norm $\|Q\|_1$ over the sequence-form polytope $Q$ from $\|Q\|_1^2$ to $\|Q\|_1$ (for the non-optimistic version we improve it from $\|Q\|_1$ to $\sqrt{\|Q\|_1}$).
Due to the connection between regret minimization and convergence to Nash equilibrium, this also improves the state-of-the-art bounds for converging to a Nash equilibrium at either a rate of $1/\sqrt{T}$ or $1/T$ by the same factor.
Moreover, KOMWU achieves last-iterate convergence, and as it is the first algorithm to achieve linear-rate last-iterate convergence with a learning rate that does not become impractically-small as the game grows large (albeit under a restrictive uniqueness assumption).


More generally, we show that KOMWU can simulate OMWU for \emph{0/1-polyhedral sets} (of which the decision sets for EFGs are a special case):
a decision set $\Omega \subseteq \bbR^d$ which is convex and polyhedral, and whose vertices are all contained in $\{0,1\}^d$.
KOMWU reduces the problem of running OMWU on the vertices of the polyhedral set to $d+1$ evaluations of what we call the \emph{0/1-polyhedral kernel}.
Thus, given an efficient algorithm for performing these kernel evaluations, KOMWU enables one to get all the benefits of running MWU or OMWU on the simplex of vertices, while retaining the crucial property that each iteration of OMWU can be performed efficiently.
In addition to EFGs, in the appendix we show that the kernel can be computed efficiently for several other settings including $n$-sets, unit cubes, flows on directed acyclic graphs, and permutations.
As with EFGs, this immediately gives us an efficient algorithm with favorable properties such as last-iterate convergence and polylogarithmic regret for games with 0/1-polyhedral strategy sets.
In particular, for $n$-sets, we show an improvement on the time complexity per round compared with the dynamic programming approach discussed in \citep{Takimoto03:Path}.
To the best of our knowledge, this is the state-of-the-art bound for simulating MWU/OMWU on $n$-sets.


\noindent\textbf{Related work}\quad
There were several past works on specialized online learning methods for EFGs.
One class of methods is based on specialized Bregman divergences that lead to efficient iteration updates~\citep{Hoda10:Smoothing,Kroer15:Faster,Kroer20:Faster,Farina21:Better}. Combined with optimistic regret minimizers for general convex set, this yields stronger regret bounds that take into account the variation in payoffs, and combined with the connection between regret minimization and Nash equilibrium computation, this yields $1/T$-rate convergence for two-player zero-sum games~\citep{Rakhlin13:Optimization,Syrgkanis15:Fast,Farina19:Optimistic}.
The \emph{counterfactual regret minimization} (CFR) framework~\citet{Zinkevich07:Regret} also yields efficient iteration updates. This approach yields a worse $\sqrt{T}$ regret bound, but leads to the best practical performance in most games~\citep{Kroer18:Solving,Kroer20:Faster,Farina21:Faster}.
\citet{Farina19:Stable} show that it is possible to attain $\bigOh(T^{1/4})$ regret within the CFR framework by using OMWU at each decision point. However, the game-dependent constants in their bound are much worse than the ones in \cref{tab:bounds}.

Regret minimization over 0/1 polyhedral sets, the framework we consider, is closely related to online combinatorial optimization problems \citep{audibert2014regret}, where the decision maker (randomly) selects a 0/1 vertex in each round instead of a point in the convex hull of the set of vertices, and the regret is measured in expectation.
We review approaches related to the use of MWU here, and other less closely related approaches in \cref{app:related works}.
One approach similar to our KOMWU is to perform MWU over vertices (\eg \citet{cesa2012combinatorial}); the remaining problem is whether there is an efficient way to maintain and sample from the weights.
Such efficient implementations have been shown in many instances such as paths \citep{Takimoto03:Path}, spanning trees \citep{koo2007structured}, and $m$-set \citep{warmuth2008randomized}. %
\citep{Takimoto03:Path} is the closest to this paper, where they show how to produce MWU iterates for paths in directed graphs.
Our kernelized method can be seen as a significant extension of their approach to general 0/1 polyhedral games, unifying many of the previous results listed above.
This unification not only results in important applications to EFGs,
but also leads to improvement to previously studied problems such as $n$-sets.



