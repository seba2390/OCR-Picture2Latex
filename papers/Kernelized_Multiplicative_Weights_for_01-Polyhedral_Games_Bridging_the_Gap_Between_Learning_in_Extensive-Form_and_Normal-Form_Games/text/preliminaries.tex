\section{Preliminaries}\label{sec:preliminaries}

In this section we review some fundamental connections between normal-form games and no-regret learners.

%\vspace{-1mm}
\subsection{Online Learning and Multiplicative Weights Update}\label{sec:online learning}

Given a finite set of choices $\cA$, consider the following abstract model of a repeated decision-making problem between a decision maker and an unknown---potentially adversarial---environment. At each time $t=1,2,\dots$, the decision maker is given (or otherwise selects) a \emph{prediction vector} $\vm\^t \in \bbR^{\cA}$. Then, the decision maker must select and output a probability distribution $\vlam\^t$ over $\cA$, that is, a vector
$
    \vlam\^t \in \Delta(\cA)\defeq \mleft\{\vlam\in\bbR_{\ge 0}^{\cA}: \sum_{a\in\cA}\vlam[a] = 1 \mright\}.
$
Finally, the environment picks (possibly in an adversarial way) a \emph{loss vector} $\vl\^t \in \bbR^{\cA}$ and shows it to the decision maker, who then suffers a loss equal to
$
    \langle \vl\^t, \vlam\^t\rangle.%
$
Given any time $T$, a key quantity for the decision maker is its \emph{cumulative regret} (or simply \emph{regret}) up to time $T$,
\[
    R^T \defeq \sum_{t=1}^T \langle\vl\^t,\vlam\^t\rangle - \min_{\hat{\vlam}\in\Delta(\cA)} \sum_{t=1}^T \langle\vl\^t, \hat{\vlam}\rangle.
    \numberthis{eq:def regret simplex}
\]
As we recall in the next subsection, decision-making algorithms that guarantee sublinear regret (in $T$) in the worst case make for natural agents to learn equilibria in games. The most well-studied decision-making algorithm with that property is the \emph{optimistic multiplicative weights update (OMWU)} algorithm.\footnote{In the literature, OMWU is often given under the assumption that $\vm\^t = \vl\^{t-1}$ at all times $t$. In this paper we present OMWU in its general form, that is, with no assumptions on $\vm\^t$.}
Let $\vl\^0,\vm\^0 \defeq\vzero\in\bbR^{\cA}$ and $\vlam\^0 \defeq \frac{1}{|\cA|}\vone \in \Delta(\cA)$; then, at all times $t \in \Npp$, OMWU updates the distribution $\vlam\^{t-1}\in\Delta(\cA)$ according to
\[
    \vlam\^t[a] \defeq \frac{\vlam\^{t-1}[a]\cdot e^{-\eta\^t\,\vw\^t[a]}}{\sum_{a' \in \cA} \vlam\^{t-1}[a']\cdot e^{-\eta\^t\,\vw\^t[a']}}
    \numberthis[$\blacklozenge$]{eq:vanilla OMWU}
\]
for all $a\in\cA$, where
$
    \vw\^t \defeq \vl\^{t-1} - \vm\^{t-1} + \vm\^t
$
and $\eta\^t \!>\!0$ is a learning rate
(full pseudocode is given in \cref{app:pseudocode}). The nonpredictive version of OMWU, called \emph{multiplicative weights update (MWU)}, is obtained from OMWU as the special case in which $\vm\^t=\vzero$ at all $t$.


\subsection{Normal-Form Games (NFGs)}\label{sec:nfgs}

\emph{Normal-form games (NFG)} are simultaneous-move, nonsequential games in which each player picks an action from a finite set, and receives a payoff that depends on the tuple of actions played by the players. Formally, we represent a normal form game as a tuple $\Gamma = (m, \{\cA_i\}, \{U_i\})$, where the positive integer $m\in\Npp$ denotes the number of players, each of which is assigned a unique player number in the set $\range{m} \defeq \{1,\dots,m\}$; the finite set $\cA_i$ specifies the actions available to player $i\in\range{m}$; and $U_i : \cA_1\times\dots\times\cA_m\to [0,1]$ is the payoff function for player $i\in\range{m}$. The game is said to be \emph{zero-sum} if $\sum_{i\in\range{m}} U_i(a_1,\dots,a_m) = 0$ for all $(a_1,\dots,a_m) \in \cA_1\times\dots\times\cA_m$.

A \emph{mixed strategy} for any player $i\in\range{m}$ is a probability distribution
$\vlam_i \in \Delta(\cA_i)$ over the player's action set $\cA_i$. When the players play according to mixed strategies $\vlam_1,\dots,\vlam_m$, the \emph{expected utility} $\bar U_i$ of any player $i\in\range{m}$ is defined accordingly as the function
$
    \bar U_i : (\vlam_1,\dots,\vlam_m) \mapsto \E_{a_1 \sim \vlam_1,\dots,a_m\sim\vlam_m}\big[U_i(a_1, \dots, a_m)\big].
$
Because of the linearity of expectation, the expected utility function $\bar U_i$ of each player $i$ is a \emph{multilinear} function of the strategies $\vlam_1,...,\vlam_m$.

\textbf{Learning in NFGs}\quad
We now describe a learning setup for NFGs, which we will refer to as the \emph{canonical optimistic learning setup (COLS)}. In the COLS, the NFG is played repeatedly. At each time $t \in \Npp$, each player $i\in\range{m}$ picks mixed strategies $\vlam_i\^t \in \Delta(\cA_i)$ according to a learning algorithm $\cR_i$, with the following choice of loss and prediction vectors:
\begin{itemize}[nosep,left=0mm]
    \item The loss vector $\vl_i\^t$ is the opposite of the gradient of the expected utility of player~$i$ with respect of player~$i$'s strategy, in symbols $\vl\^t_i \defeq -\nabla_{\vlam_i} \bar U_i(\vlam_1\^t,\dots,\vlam_m\^t)$;
    \item The prediction vector $\vm_i\^t$ is defined as the previous loss $\vm_i\^t \defeq \vl\^{t-1}_i$ if $t \ge 2$, and $\vm_i\^1 \defeq \vzero$ otherwise.
\end{itemize}

This is the same setup that was used in landmark papers such as \citep{Syrgkanis15:Fast} and \citep{Daskalakis21:Near}. A key result in the theory of learning in games establishes a deep connection between the COLS and \emph{coarse-correlated equilibria (CCEs)} of the game (which, in two-player zero-sum games, are Nash equilibria).

\begin{theorem}\label{thm:nfg cce}
    Under the COLS, the average product distribution of play $\bar{\vec{\mu}}\defeq \frac{1}{T}\,\sum_{t=1}^T \vlam_1\^t\otimes\dots\otimes\vlam_m\^t$ is an $\bigOh(\max_{i\in\range{m}} R_i^T / T)$-approximate CCE of the game,
    where $R_i^T$ is the regret for player $i$ (see Eq.~\eqref{eq:def regret simplex}).
\end{theorem}

When each player $i$ learns under the COLS using OMWU with the same, constant learning rate $\eta_i\^t \defeq \eta$ as their learning algorithm $\cR_i$, the following strong properties hold for any NFG $\Gamma = (m, \{\cA_i\}, \{U_i\})$.
\begin{property}[Near-optimal per-player regret]\label{prop:omwu near optimal}
    There exist universal constants $C,C' > 1$ so that, for all $T$, if $\eta \le \frac{1}{Cm\log^4 T}$, the regret of each player $i\in\range{m}$ is bounded as
    $R_i^T \le \frac{\log |\cA_i|}{\eta} + C' \log T$
    \citep{Daskalakis21:Near}.
\end{property}
\begin{property}[Optimal regret sum]\label{prop:omwu optimal sum}
    If $\eta \le \frac{1}{\sqrt{8}(m-1)}$, at all times $T\in\Npp$ the sum of the players' regrets satisfies $\sum_{i=1}^m R_i^T \le \frac{m}{\eta} \max_{i=1}^m \log |\cA_i|$
    \citep{Syrgkanis15:Fast}.
\end{property}
When $\Gamma$ is a \emph{two-player zero-sum} game, the following also holds when learning under the COLS using OMWU.
\begin{property}[Last-iterate convergence]\label{prop:omwu last iterate}
    There exists a certain schedule of learning rates $\eta\^t_i$ such that the players' strategies $(\vlam_1\^t,\vlam_2\^t)$ converge to a Nash equilibrium of the game~\citep{Hsieh21:Adaptive}.
    Furthermore, if $\Gamma$ has a unique Nash equilibrium $(\vlam_1^*,\vlam_2^*)$ and each player uses any constant learning rate $\eta\^t_i \defeq \eta \le \frac{1}{8}$, at all times $t$ the strategy profile $(\vlam_1\^t,\vlam_2\^t)$ satisfies $\KL{\vlam_1^*}{\vlam_1\^t} + \KL{\vlam_2^*}{\vlam_2\^t} \leq C (1+C')^{-t}$, where the constants $C,\,C'$ only depend on the game, and $\KL{\cdot}{\cdot}$ denotes the KL-divergence between two distributions \citep{Wei21:Linear}.
\end{property}

