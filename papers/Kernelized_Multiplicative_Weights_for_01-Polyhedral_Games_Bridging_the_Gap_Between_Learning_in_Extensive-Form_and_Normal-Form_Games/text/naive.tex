\section{Multiplicative Weights in Polyhedral Convex Games}
\label{sec:vertex}

A powerful generalization of normal-form games is \emph{polyhedral convex games}, of which extensive-form games are an example~\citep{Gordon08:No}. Unlike NFGs, in which players select a mixed strategy from the probability simplex spanned by the set of available action $\cA_i$, in a polyhedral convex game the set of  ``randomized strategies'' from which each player $i\in\range{m}$ can draw is a given convex polytope $\Omega_i \subseteq \bbR^{d_i}$. Analogously to NFGs, we represent a polyhedral convex game as a tuple $\Gamma = (m, \{\Omega_i\}, \{\bar U_i\})$, where the functions $\bar U_i : \Omega_1\times\dots\times\Omega_m \to [0,1]$ are the \emph{multilinear} utility functions for each player $i\in\range{m}$.

The concepts of learning agents, equilibria, and COLS introduced in \cref{sec:online learning,sec:nfgs} can be directly extended to polyhedral convex games without difficulty, by simply replacing the set of mixed strategies $\Delta(\cA_i)$ of each player with their convex polyhedral counterpart $\Omega_i$.

Because the set of mixed strategies $\Omega$ of every player is a polytope, the decision problem of picking a mixed strategy $\vx\^t\in\Omega$ can be equivalently thought of as the decision problem of picking a convex combination $\vlam\^t \in \Delta(\cV_{\Omega})$ over the finite set of vertices $\cV_{\Omega}$ of $\Omega$. Indeed, it is not hard to show that a learning algorithm ${\cR}$ for $\Omega\subseteq\bbR^d$ can be constructed from \emph{any} learning algorithm $\tilde\cR$ for the set of \emph{vertices} $\cV_{\Omega}$, as we describe next. Let $\mat{V}$ denote the matrix whose columns are the vertices $\cV_\Omega$; then:
\begin{itemize}[nosep,left=0mm]
    \item whenever $\cR$ receives a prediction ${\vm}\^t\in\bbR^d$ (resp., loss ${\vl}\^t$), it computes the vector $\tilde{\vm}\^t\defeq\mat{V}^\top\vm\^t\in\bbR^{\cV_\Omega}$ (resp., $\tilde{\vl}\^t\defeq\mat{V}^\top\vl\^t$) and forwards it to $\tilde\cR$;
    \item whenever $\tilde\cR$ plays a new distribution $\vlam\^t \in \Delta(\cV_\Omega)$, the convex combination of vertices $\vx\^t\defeq \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\,\vv = \mat{V}\vlam\^t$ is played by $\cR$.
\end{itemize}
It is immediate to verify that the regret cumulated by $\cR$ and $\tilde\cR$ is equal at all times $T$. So, as long as $\tilde\cR$ guarantees sublinear regret, then so does $\cR$. In this paper we are particularly interested in the algorithm obtained by using the above construction for the specific choice of OMWU as the algorithm $\tilde\cR$. We coin \emph{Vertex OMWU} the resulting learning algorithm $\cR$ in that case, depicted in \cref{fig:Vertex OMWU}. Let $\vl\^0,\vm\^0\defeq \vzero\in\bbR^{\cV_\Omega}$ and $\vlam\^0 \defeq \frac{1}{|\cV_\Omega|}\vone\in\Delta(\cV_\Omega)$; then, at all times $t\!\in\!\Npp$, Vertex OMWU updates the convex combination of vertices $\vlam\^{t-1}\!\in\!\Delta(\cV_\Omega)$ according to
\[
    \vlam\^t[\vv] \defeq \frac{\vlam\^{t-1}[\vv]\cdot e^{-\eta\^t\langle\vw\^{t},\vv\rangle}}{\sum_{\vv' \in \cV_\Omega} \vlam\^{t-1}[\vv']\cdot e^{-\eta\^t\langle \vw\^{t}\!,\vv'\rangle}},
    \numberthis[$\clubsuit$]{eq:vertex lam update}
\]%\vspace{-1mm}
where%\vspace{-3mm}
\[
    \vw\^t \defeq \vl\^{t-1} - \vm\^{t-1} + \vm\^t\in\bbR^d,
    \numberthis{eq:def w}
\]
and then outputs the iterate
\[
    \Omega\ni\vx\^t \defeq \sum_{\vv\in\cV_\Omega} \vlam\^t[\vv]\cdot\vv = \mat{V}\vlam\^t.
    \numberthis[$\spadesuit$]{eq:xt original}
\]
It is straightforward to show that Vertex OMWU satisfies \cref{prop:omwu near optimal,prop:omwu optimal sum,prop:omwu last iterate} with $|\cA_i|$ replaced with $|\cV_{\Omega_i}|$, by using a black-box reduction to NFGs. Indeed, let $\Gamma = (m, \{\Omega_i\},\{\bar U_i\})$ be a polyhedral convex game, and introduce the \emph{NFG $\tilde\Gamma$ equivalent to $\Gamma$}, defined as the NFG $\tilde\Gamma \defeq (m, \{\cV_{\Omega_i}\}, \{U_i\})$ where the action set of each player is the set of vertices $\cV_{\Omega_i}$, and $U_i(\vv_1, \dots, \vv_m) \defeq \bar U_i(\vv_1, \dots, \vv_m)$ for all $(\vv_1,\dots,\vv_m)\in\cV_{\Omega_1}\times\dots\times\cV_{\Omega_m}$. Consider the losses $\vl_i\^t$, predictions $\vm\^t$, and iterates $\vx_i\^t\in\Omega_i$ produced by agents learning (under the COLS) in $\Gamma$ using Vertex OMWU, and the losses $\tilde{\vl}_i\^t$, predictions $\tilde{\vm}_i\^t$, and iterates $\vlam_i\^t\in\Delta(\cV_i)$ produced by agents learning (again under the COLS) in $\tilde\Gamma$ using OMWU. For all players $i\in\range{m}$, it is immediate to verify by induction that the relationships (i) $\tilde{\vl}_i\^t = \mat{V}_i^\top \vl_i\^t$, (ii) $\tilde{\vm}_i\^t = \mat{V}_i^\top\vm_i\^t$, and (iii) $\vx_i\^t = \mat{V}_i \vlam_i\^t$ hold at all $t$, where $\mat{V}_i$ is the matrix whose columns are the vertices $\cV_{\Omega_i}$ (see also \cref{fig:Vertex OMWU}). The above discussion shows that in a precise sense, Vertex OMWU and OMWU are the same algorithm, just on different equivalent representations of the game.
Hence, the regret cumulated by each player $i$ in $\Gamma$ matches the regret cumulated by the same player in $\tilde\Gamma$, showing that \cref{prop:omwu near optimal,prop:omwu optimal sum} hold for Vertex OMWU.
Furthermore, whenever $\vlam_i\^t$ converges in iterates, then clearly so does $\vx_i\^t = \mat{V}_i\vlam_i\^t$, showing that \cref{prop:omwu last iterate} applies to Vertex OMWU as well.

The main drawback of Vertex OMWU is that it is not clear how to avoid a per-iteration complexity linear in the number of vertices of $\Omega$, which is typically exponential in $d$ (this is the case in extensive-form games). While different learning algorithms that guarantee polynomial per-iteration complexity in $d$ exist, none of them is known to guarantee near-optimal per-player regret (\cref{prop:omwu near optimal}) or last-iterate convergence (\cref{prop:omwu last iterate}) enjoyed by Vertex OMWU, much less all three \cref{prop:omwu near optimal,prop:omwu optimal sum,prop:omwu last iterate} at the same time. In the rest of the paper we fill this gap, by showing that in several cases of interest, Vertex OMWU can be implemented with polynomial-time (in $d$) iterations using a kernel trick.








\begin{figure}[t]\centering
    \tikzstyle{lbl} = [fill=white,rounded corners,inner ysep=.7mm]
    \tikzstyle{tightlbl} = [lbl,inner xsep=.3mm,inner ysep=.2mm]

    \scalebox{.97}{\begin{tikzpicture}[x=1mm,y=1mm]
            \begin{scope}[local bounding box=vertexbb]
                \node[text=gray,inner sep=0mm] at (-11, 10) {\small$\Gamma$};
                \draw[semithick,fill=white] (-12, 0) rectangle (12, -10) node[fitting node] (VertexOMWU) {};
                \draw[semithick,<-] (VertexOMWU.north) -- +(0,  4) node[above=-1mm,lbl,text width=6mm,align=center] (VertexLoss) {\raisebox{0mm}{\small${\vl}\^t$}};
                \node[above=-0.25mm of VertexLoss,lbl,text width=6mm,align=center] (VertexPred) {\raisebox{0mm}{\small${\vm}\^t$}};
                \draw[semithick,->] (VertexOMWU.south) -- +(0, -3) node[below,lbl] (VertexStrat) {\small$\vx\^t\in\Omega$};

                \node[text width=20mm, anchor=center, align=center] at (VertexOMWU.center) {\small Vertex OMWU\\\textcolor{black!60}{\eqref{eq:vertex lam update},~\eqref{eq:xt original}}};
            \end{scope}
            \begin{scope}[xshift=47mm,local bounding box=vanillabb]
                \node[text=gray,inner sep=0mm] at (11, 10) {\small$\tilde{\Gamma}$};
                \draw[semithick,fill=white] (-12, 0) rectangle (12, -10) node[fitting node] (VanillaOMWU) {};
                \draw[semithick,<-] (VanillaOMWU.north) -- +(0,  4) node[above=-1mm,lbl,text width=6mm,align=center] (VanillaLoss) {\raisebox{0mm}{\small$\tilde{\vl}\^t$}};
                \node[above=-0.25mm of VanillaLoss,lbl,text width=6mm,align=center] (VanillaPred) {\raisebox{0mm}{\small$\tilde{\vm}\^t$}};
                \draw[semithick,->] (VanillaOMWU.south) -- +(0, -3) node[below,lbl] (VanillaStrat) {\small$\vlam\^t\in\Delta(\cV_\Omega)$};

                \node[text width=18mm, anchor=center, align=center] at (VanillaOMWU.center) {\small OMWU\\\textcolor{black!60}{\eqref{eq:vanilla OMWU}}};
            \end{scope}
            \node[blue,tightlbl] (PredLbl) at ($(VertexPred)!.5!(VanillaPred)$) {\scalebox{.9}{\raisebox{1mm}{\small$\tilde{\vm}\^t = \mat{V}^\top{\vm}\^t$}}};
            \node[blue,tightlbl] (LossLbl) at ($(VertexLoss)!.5!(VanillaLoss)$) {\scalebox{.9}{\small$\tilde{\vl}\^t = \mat{V}^\top{\vl}\^t$}};
            \node[violet,tightlbl] (StratLbl) at ($(VertexStrat)!.5!(VanillaStrat)$) {\scalebox{.9}{\raisebox{1mm}{\small$\vx\^t = \mat{V}\vlam\^t$}}};
            \draw[line width=1mm,white] (VertexPred) -- (PredLbl) (PredLbl) -- (VanillaPred);
            \draw[blue] (VertexPred) -- (PredLbl) (PredLbl) edge[->] (VanillaPred);
            \draw[line width=1mm,white] (VertexLoss) -- (LossLbl) (LossLbl) -- (VanillaLoss);
            \draw[blue] (VertexLoss) -- (LossLbl) (LossLbl) edge[->] (VanillaLoss);
            \draw[line width=1mm,white] (VertexStrat) -- (StratLbl) (StratLbl) -- (VanillaStrat);
            \draw[violet] (VertexStrat) edge[<-] (StratLbl) (StratLbl) -- (VanillaStrat);

            \begin{pgfonlayer}{background}
                \filldraw[black!20,thin,fill=black!8] ($(vertexbb.south west) + (-1,-.5)$) rectangle ($(vertexbb.north east) + (1, .5)$);
                \filldraw[black!20,thin,fill=black!8] ($(vanillabb.south west) + (-1,-.5)$) rectangle ($(vanillabb.north east) + (1, .5)$);
                \node[inner ysep=.2mm,inner xsep=0mm,rotate=90,yshift=3mm] at (vertexbb.west) {\small Polyhedral convex game};
                \node[inner ysep=.2mm,inner xsep=0mm,rotate=-90,yshift=3mm] at (vanillabb.east) {\small Equivalent NFG};
            \end{pgfonlayer}
        \end{tikzpicture}}
    %\vspace{-2mm}
    \caption{Construction of the Vertex OMWU algorithm. The matrix $\mat{V}$ has the (possibly exponentially-many) vertices $\cV_\Omega$ of the convex polytope $\Omega$ as columns.}
    \label{fig:Vertex OMWU}
    %\vspace{-4mm}
\end{figure}
