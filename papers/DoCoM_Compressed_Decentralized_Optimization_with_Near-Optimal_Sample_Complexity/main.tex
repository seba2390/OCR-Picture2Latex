
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
% \usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.

\usepackage[colorlinks=true, citecolor=blue]{hyperref}
\usepackage{url}


\title{{\aname}: Compressed Decentralized Optimization with Near-Optimal Sample Complexity}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Chung-Yiu Yau \email cyyau@se.cuhk.edu.hk \\ \addr The Chinese University of Hong Kong
      \\ \AND \\
      \name Hoi-To Wai \email htwai@se.cuhk.edu.hk \\ \addr The Chinese University of Hong Kong}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{07}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=W0ehjkl9x7}} % Insert correct link to OpenReview for camera-ready version

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{pifont}

\usepackage{tcolorbox}

\newcommand{\cmark}{\text{\ding{51}}}%
\newcommand{\xmark}{\text{\ding{55}}}%

% \newcommand{\revision}{\color{green!50!black}}
\newcommand{\revision}{}

\let\classAND\AND
\let\AND\relax
\usepackage{algorithm}
\usepackage{algorithmic}
% save the new meaning of \AND and restore the one of the class
% \let\algoAND\AND
% \let\AND\classAND

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:


\newcommand{\aname}{{\tt DoCoM}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\eqdef}{:=}
\newcommand{\prm}{\theta}
\newcommand{\hatprm}{\widehat{\prm}}
\newcommand{\gog}{g}
\newcommand{\hatgog}{\widehat{\gog}}
\newcommand{\Prm}{\Theta}
\newcommand{\grdF}{\nabla F}
% \newcommand{\grdSF}{\nabla F_{\zeta}}
\newcommand{\virtualG}{\hat{\underline{G}}}
\newcommand{\bw}{\bar{\omega}}
\newcommand{\avgg}{\bar{g}}
\newcommand{\avgG}{\bar{G}}
\newcommand{\avgtheta}{\bar{\prm}}
\newcommand{\avgTheta}{\bar{\Prm}}
\newcommand{\dotp}[2]{\left\langle{#1}\ \middle|\ {#2}\right\rangle}
\newcommand{\red}[1]{\mathbin{\color{red}#1}}
\newcommand{\blue}[1]{\mathbin{\color{blue}#1}}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\avgone}{\frac{1}{n}\mathbf{1}\mathbf{1}^\top}
\newcommand{\nl}{\nonumber\\}
\newcommand{\virtghat}{\hat{\underline{G}}}
\newcommand{\sgF}{\nabla F_\zeta}
\newcommand{\BO}{\mathcal{O}}
\newcommand{\W}{{\bf W}}
\newcommand{\U}{{\bf U}}
\newcommand{\I}{{\bf I}}
\newcommand{\ve}{v}
\newcommand{\ER}{{\tt V}}

\newcommand{\stocgrdf}{\nabla \widehat{f}}
\newcommand{\stocgrdfp}{\nabla \widetilde{f}}


\newcommand{\grdSF}{\nabla \widehat{F}}
\newcommand{\stocgrdF}{\nabla \widehat{F}}
\newcommand{\stocgrdFp}{\nabla \widetilde{F}}

\newcommand{\avgstocgrdF}{\overline{\nabla} \widehat{F}}
\newcommand{\avgstocgrdFp}{\overline{\nabla} \widetilde{F}}

\newcommand{\avggrdF}{\overline{\nabla F}}
\newcommand{\hatTheta}{\widehat{\Theta}}
\newcommand{\hatG}{\widehat{G}}

\newcommand{\ConstS}{\mathbb{C}_\sigma}
\newcommand{\ConstF}{\mathbb{C}_{\nabla f}}
\newcommand{\ConstG}{\mathbb{C}_{\avgg}}
\newcommand{\InitG}{\overline{G}_0}

\newcommand{\ConstSS}{\mathbb{C}_\sigma^{\sf NM}}
\newcommand{\ConstFF}{\mathbb{C}_{\nabla f}^{\sf NM}}

\newcommand{\avgv}{\bar{v}}
\newcommand{\avgV}{\bar{V}}
% \newcommand{\avgg}{\bar{g}}
% \newcommand{\avgG}{\bar{G}}

\newcommand{\bbrho}{\bar{\rho}^{\sf NM}}
\newcommand{\brho}{\overline{\beta}}
\newcommand{\bmu}{\widetilde{\beta}}
\newcommand{\bsigma}{\sigma_0}

\newcommand{\blu}{\color{blue}}
\newcommand{\re}{\color{red}}

\newcommand{\redtmp}{}

\newcommand{\half}{\frac{1}{2}}

\allowdisplaybreaks

\begin{document}


\maketitle

\begin{abstract}
This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm (\aname) for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that \aname~finds a near-stationary solution at all participating agents satisfying $\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = {\cal O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of {\aname}. As a corollary, our analysis also established the linear convergence of {\aname} to a global optimal solution for objective functions with the Polyak-Łojasiewicz condition. Numerical experiments demonstrate that our algorithm outperforms several state-of-the-art algorithms in practice.
\end{abstract}

\section{Introduction}\vspace{-.1cm}
% Decentralized optimization algorithms have emerged as a preferred solution over centralized optimization in distributed systems handling a large amount of data \citep{boyd2011distributed}. 
Decentralized algorithms tackle an optimization problem with inter-connected agents/workers possessing local data without  a central server. For many scenarios in large-scale learning, these algorithms improve computational scalability and preserve data privacy. Owing to these reasons, decentralized algorithms have become the critical enabler for  
sensor networks \citep{schizas2007consensus},
% distributed dictionary learning \citep{chen2014dictionary}, and more recently 
federated learning \citep{konevcny2016federated, wang2021field}, etc. 

This paper concentrates on the \emph{communication and sampling efficiency} issues with decentralized algorithms, which is a key bottleneck as decentralized algorithms rely heavily on the bandwidth limited inter-agent communication links \citep{wang2021field}. Inefficiently designed algorithms may lead to significant overhead and slow down to downstream applications. Several approaches have been studied to tame with this issue. The first approach is to consider algorithms that are optimal in terms of the number of communication rounds. \citet{scaman2019optimal,gorbunov2019optimal, uribe2021dual} studied algorithms with an optimal iteration complexity, \citet{sun2019distributed, sun2020improving, pmlr-v139-lu21a} focused on non-convex problems and studied lower bounds on the number of communication rounds needed. We remark that a common paradigm to achieve better communication or sampling efficiency requires multiple gradient steps \citep{nadiradze2021asynchronous} or multiple consensus steps \citep{pmlr-v139-lu21a}.

Perhaps a more direct approach to improve communication efficiency is to apply \emph{compression} to control bandwidth usages in every communication step of the algorithm. This idea was first studied in the context of {distributed optimization} where workers communicate with a central server. Many algorithms have been studied with compression strategies such as sparsification \citep{stich2018sparsified, alistarh2019convergence, wangni2018gradient}, quantization \citep{wen2017terngrad,alistarh2017qsgd, bernstein2018signsgd, reisizadeh2020fedpaq}, low-rank approximation \citep{vogels2019powersgd}, etc., often used in combination with error compensation \citep{mishchenko2019distributed, tang2019doublesqueeze}{; \revision also see} the recent work \citep{richtarik2021ef21} {\revision for more details on compression strategies}.

\begin{table}[t]
    \centering
\caption{\textbf{Comparison of decentralized stochastic optimization algorithms for smooth \emph{non-convex} problems.} Sample complexity is the no.~of samples required per agent
to obtain an $\epsilon$-stationary solution, $\avgtheta$,  such that $\mathbb{E}[ \| \nabla f( \avgtheta ) \|^2] \leq \epsilon^2$. Constants $\delta, \sigma^2, \InitG, \rho$ are defined in \Cref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}, \Cref{th:main}. Highlighted in {\re red} are {\revision dominant} terms when $\epsilon \to 0$.\vspace{.2cm}
% $G$ in {\tt CHOCO-SGD} refers to the boundedness of stochastic gradient. 
% $\varsigma_0$ in {\tt DeTAG} refers to the outer variance of stochastic gradients of different local objective functions at initialization.
} \label{tab:compare}
{ \begin{tabular}{llll}
    \hline 
    \bfseries Algorithms & \bfseries Sample Complexity
    % $\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[ \| \nabla f( \avgtheta^t ) \|^2]$ 
    & \bfseries Compress & \bfseries Remarks \\
    \hline
    \begin{tabular}{@{}l@{}} {{\tt DSGD}} \\ {\citep{lian2017can}}\end{tabular}& $
    % \BO\left(\max\left\{ \frac{1}{n\epsilon^4}, \frac{n}{\rho^2\epsilon^2} \right\} \right) or~
    \BO\left(\max\left\{ {\re \frac{\sigma^2}{n} \epsilon^{-4} }, \frac{n( \sigma^2 + \varsigma^2) }{\rho^2 \epsilon^2} \right\} \right)$ & $\xmark$ & {\footnotesize $\varsigma^2 = \underset{i, \theta}{\sup}~ \| \nabla f_i(\theta) - \nabla f(\theta) \|^2$} \\
    \begin{tabular}{@{}l@{}} {{\tt GNSD}} \\ {\citep{lu2019gnsd}}\end{tabular}
      & $\BO\left( {\re \frac{1}{C_0^2 C_1^2} \epsilon^{-4} } \right)$ & $\xmark$ & \begin{tabular}{@{}l@{}} {\footnotesize $C_0, C_1$ are not explicitly} \\ {\footnotesize  defined, see \citep{lu2019gnsd}.}\end{tabular} \\
    \begin{tabular}{@{}l@{}} {{\tt DeTAG}} \\ {\citep{pmlr-v139-lu21a}}\end{tabular} & $\BO\left( \max\left\{ {\re \frac{\sigma^2}{n} \epsilon^{-4} }, \frac{ B \log\left( n + \varsigma_0 n\epsilon^{-1}\right)}{ \rho^{0.5}\epsilon^2}\right\} \right)$ & $\xmark$ & 
    \begin{tabular}{@{}l@{}} {\footnotesize $\varsigma_0$ is variance of init.~stoc.} \\ {\footnotesize gradient, $B$ is batch size.}\end{tabular}\\
    % {\footnotesize $\varsigma_0$ is variance of stoc.~gradient at init; $B$ is rounds of comm. per iteration.} \\
    \begin{tabular}{@{}l@{}} {{\tt GT-HSGD}} \\ {\citep{xin2021hybrid}}\end{tabular}  & $\BO\left( \max\left\{ {\re \frac{\sigma^3}{n} \epsilon^{-3} }, \frac{\InitG}{\rho^3\epsilon^2}, \frac{n^{0.5}\sigma^{1.5}}{\rho^{2.25}\epsilon^{1.5}} \right\} \right)$ & $\xmark$ \\
    \begin{tabular}{@{}l@{}} {{\tt CHOCO-SGD}} \\ {\citep{koloskova2019decentralizeda}} \end{tabular} & $\BO\left( \max\left\{ {\re  \frac{\sigma^2}{n} \epsilon^{-4} }, \frac{G}{\delta\rho^2\epsilon^3} \right\} \right)$ & $\cmark$ & {\footnotesize $G = \underset{i, \theta}{\sup} ~\mathbb{E}_{\zeta \sim \mu_i} [ \| \nabla f_i (\theta; \zeta) \|^2]$} \\
    \begin{tabular}{@{}l@{}} {{BEER}} \\ {\citep{zhao2022beer}} \end{tabular} & $\BO \left( \max \left \{ {\re \frac{\sigma^2}{\delta^2 \rho^3 } \epsilon^{-4}}, \frac{1}{\delta \rho^3 \epsilon^2}  \right\} \right)$ & $\cmark$ & \begin{tabular}{@{}l@{}}{\footnotesize Requires batch size of } \\ {\footnotesize ${\cal O}(\sigma^2/ (\delta \epsilon^2) )$.} \end{tabular} \\
    \begin{tabular}{@{}l@{}} {{\tt CEDAS}} \\ {\citep{huang2023cedas}} \end{tabular}  & $\BO \left( \max \left \{{\re \frac{\sigma^2}{n}\epsilon^{-4}}, \frac{n \sigma^2}{\rho\epsilon^2} , \frac{n G_0}{\rho\epsilon}  \right\} \right) $ & $\cmark$ & {\footnotesize $G_0 = n^{-1} \sum_{i=1}^n \| \nabla f_i(\prm_i^0)\|^2$} \\
    \cellcolor{gray!15}\aname &  \cellcolor{gray!15}$\BO\left( \max\left\{ {\re \frac{\sigma^3}{n} \epsilon^{-3} }, \frac{n\InitG}{\delta^2\rho^4\epsilon^2}, \frac{n^{1.25}\sigma^{1.5}}{\delta^{2.25}\rho^{4.5}\epsilon^{1.5}}\right\} \right)$ & \cellcolor{gray!15}$\cmark$ & \cellcolor{gray!15}{\footnotesize See \Cref{th:main}} \\
    \hline
\end{tabular}}\vspace{-.3cm}
\end{table}

For decentralized optimization which operates in the absence of a central server, the design of compression-enabled algorithms is more challenging. \citet{tang2018communication} proposed an extrapolation compression method, \citet{koloskova2019decentralized, koloskova2019decentralizeda} proposed the {\tt CHOCO-SGD} algorithm which combines decentralized SGD \citep{lian2017can} with error compensation, \citet{vogels2020practical} studied compression with low-rank matrices, \citet{zhao2022beer} considered algorithms deploying large batch size. 
Despite the simplicity and appealing practical performance, algorithms such as {\tt CHOCO-SGD} suffer from sub-optimal iteration/sample complexity. Their analysis also shows that the performance hinges on a measure of data similarity across agents which is not ideal in light of applications such as federated learning with non-i.i.d.~data. We inquire\vspace{-.2cm}
\begin{samepage}\begin{center}
    \emph{Can we design a compression-enabled decentralized algorithm for non-i.i.d.~data with near-optimal {\revision sample} complexity? Does such algorithm work well in practice?} \vspace{-.2cm} 
\end{center}\end{samepage}
This paper addresses the above questions by incorporating two ingredients in decentralized optimization algorithm: {\sf (A)} compression with gradient tracking, {\sf (B)} momentum-based variance reduction. In summary,\vspace{-.0cm}
\begin{itemize}[leftmargin=*, topsep=0mm, itemsep =0.25mm]
    \item We design the \emph{Doubly Compressed Momentum-assisted Stochastic Gradient Tracking} (\aname)~algorithm which utilizes two levels of compressions for tackling decentralized stochastic optimization. The design of {\aname} involves a judicious combination of compression with gradient tracking to maximize convergence speed. Our algorithm finds a stationary solution without relying on restrictive conditions such as bounded similarity between data distributions found in prior compression-enabled algorithms. 
    \item We provide a unified convergence analysis for \aname. Let $f(\avgtheta)$ be the overall objective function across the network to be defined in \eqref{eq:opt}, we show that \aname~finds the averaged iterate $\avgtheta^T$ in $T$ iterations and communications rounds with $\mathbb{E} [ \norm{ \nabla f(\avgtheta^T) }^2 ] = {\cal O}( 1 / {T}^{2/3} )$ for {\revision mean square smooth} objective functions, and with $\mathbb{E} [ f( \avgtheta^T ) - f^\star ] = {\cal O}( \log T / {T} )$ for objective functions satisfying the Polyak-Łojasiewicz condition. Note the algorithm takes ${\cal O}(1)$ sample per iteration. For the latter case, we show that if deterministic gradients are available, \aname~converges \emph{linearly} to optimal solution. 
    \item We note that the analysis of {\aname} comprises a number of inter-dependent error quantities, whose convergences are not straightforward to observe due to the nonlinear coupling between them. To this end, our analysis technique, which can be of independent interest, relies on the construction of a \emph{tight} potential function to yield the desirable (tight) bound; see \Cref{lem:wholesys}. 
    % These convergence rates are comparable to state-of-the-art algorithms. 
     \item We empirically evaluate the performance of \aname~on training linear models and deep learning models using synthetic and real data, on non-convex losses.
\end{itemize}\vspace{-.1cm}
Note that recently, \citet{zhao2022beer} proposed the {\tt BEER} algorithm for tackling decentralized non-convex optimization with compression and optimal communication complexity. The latter has been achieved using a large batch size per iteration.
% and results in a suboptimal sampling complexity. 
\citet{huang2023cedas} proposed the {\tt CEDAS} algorithm which achieves an improved transient time for approaching the asymptotic convergence rate as centralized SGD. {\revision \citet{yan2023compressed} proposed the {\tt CDProxSGT} algorithm to handle composite objective functions (possibly non-smooth) using proximal update.} However, {\revision under the mean square smoothness assumption considered in this paper}, the best known analysis for these algorithms only show a suboptimal sample complexity of ${\cal O}(\epsilon^{-4})$ as they do not incorporate a {\revision momentum-based} variance reduction step as in {\aname}.
We summarize the sample complexities for state-of-the-art decentralized algorithms in \Cref{tab:compare}. As seen, \aname~is the only algorithm with compression and an ${\cal O}(\epsilon^{-3})$ sample complexity. Such sample complexity matches the complexity lower bound for stochastic first order algorithms \citep{arjevani2019lower}, making {\aname} the first compression-enabled algorithm to achieve near-optimal sample complexity {\revision under the mean square smoothness assumption}.\vspace{0cm}


\paragraph{Related Works}
% A number of prior studies are mentioned below to give a better context of our contributions.
Algorithms for decentralized optimization have been first studied in \citep{nedic2009distributed}. 
% The main idea is to mix communication (i.e., consensus) with optimization (i.e., gradient) steps.
It has been extended to the stochastic setting (a.k.a.~{\tt DSGD}) in \citep{ram2010distributed}, and to directed graphs \citep{tsianos2012push, assran2019stochastic}. Notably, multiple works \citep{qu2017harnessing, di2016next, shi2015extra} proposed a gradient tracking technique where agents communicate  local gradients to accelerate convergence.\vspace{-.1cm} 
% See \citep{tang2018d, lu2019gnsd} for related extensions to the stochastic setting and 

In stochastic non-convex optimization,  \citet{lian2017can} provided a performance analysis of {\tt DSGD}; \citet{lu2019gnsd} proposed {\tt GNSD} which combines gradient tracking with stochastic gradient (also see \citep{tang2018d}); \citet{pmlr-v139-lu21a} proposed {\tt DeTAG} with optimal computation-communication tradeoff; \citet{xin2021hybrid} proposed {\tt GT-HSGD} which extended {\tt GNSD} with momentum-based variance reduction, and similar algorithms are in \citep{xin2020variance,zhang2021gt}. Note that the momentum-based variance reduction idea was proposed in \citep{tran2021hybrid, cutkosky2019momentum} to achieve optimal sampling complexity for centralized SGD. For a general overview, see \citep{chang2020distributed}.\vspace{-.1cm}

On the other hand, methods for reducing communication burden in decentralized algorithms have been developed. \citet{aysal2008distributed, kashyap2007quantized, reisizadeh2019exact, saha2021decentralized} studied quantization for average consensus which is a main building block for decentralized algorithms. Notably, recent works \citep{liu2020linear, liao2021compressed, song2021compressed, zhang2021innovation,song2021compressed} showed that combining compression with gradient tracking lead to algorithms that achieve linear convergence to an optimal solution. These algorithms bear similar structure to  \aname, yet are limited to strongly convex objective functions and full batch gradients; {\redtmp see \citep{kovalev2021linearly} for the extension to stochastic settings.}

\paragraph{Notations} $\| \cdot \|$, $\| \cdot \|_F$ denote Euclidean norm, Frobenius norm, respectively. The subscript-less operator $\mathbb{E} [\cdot]$ is total expectation taken over all randomnesses in operand.\vspace{0cm}
\vspace{-.1cm}

% \begin{table}[t]
%     \centering
% \caption{Comparing \aname~to existing decentralized stochastic optimization algorithms for smooth non-convex objectives. Iteration complexity is the measure of steps required to update models through gradients to obtain an ϵ\epsilon-stationary network average \avgthetaT\avgtheta^T such that 1T∑T−1t=0E[‖\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[ \| \nabla f( \avgtheta^t ) \|^2] \leq \epsilon^2. GG in {\sf CHOCO-SGD} refers to the boundedness of stochastic gradient. \varsigma_0\varsigma_0 in {\sf DeTAG} refers to the outer variance of stochastic gradients of different local objective functions at initialization.
% } \label{tab:compare}

% \begin{tabular}{llll}
%     \hline 
%     \bfseries Algorithms & \bfseries Iter.~Complexity
%     % \frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[ \| \nabla f( \avgtheta^t ) \|^2]\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[ \| \nabla f( \avgtheta^t ) \|^2] 
%     & \bfseries Compress \\
%     \hline
%     % \BO\left(\max\left\{ \frac{1}{n\epsilon^4}, \frac{n}{\rho^2\epsilon^2} \right\} \right) or~
%     {\sf DSGD} & \BO\left(\max\left\{ \frac{\sigma^2}{n\epsilon^4}, \frac{1}{\epsilon^2} \right\} \right)\BO\left(\max\left\{ \frac{\sigma^2}{n\epsilon^4}, \frac{1}{\epsilon^2} \right\} \right) & \xmark\xmark \\
%     {\sf GNSD} & \BO\left( \frac{1}{C_0^2 C_1^2\epsilon^4} \right)\BO\left( \frac{1}{C_0^2 C_1^2\epsilon^4} \right) & \xmark\xmark \\
%     {\sf DeTAG} & \BO\left( \max\left\{\frac{\sigma^2}{nB\epsilon^4}, \frac{ \log\left( n + \varsigma_0 n\epsilon^{-1}\right)}{ \rho^{0.5}\epsilon^2}\right\} \right)\BO\left( \max\left\{\frac{\sigma^2}{nB\epsilon^4}, \frac{ \log\left( n + \varsigma_0 n\epsilon^{-1}\right)}{ \rho^{0.5}\epsilon^2}\right\} \right) & \xmark\xmark\\
%     {\sf GT-HSGD} & \BO\left( \max\left\{ \frac{\sigma^3}{n\epsilon^3}, \frac{(1-\rho)^4\InitG}{\rho^3\epsilon^2}, \frac{(1-\rho)^{1.5}n^{0.5}\sigma^{1.5}}{\rho^{2.25}\epsilon^{1.5}} \right\} \right)\BO\left( \max\left\{ \frac{\sigma^3}{n\epsilon^3}, \frac{(1-\rho)^4\InitG}{\rho^3\epsilon^2}, \frac{(1-\rho)^{1.5}n^{0.5}\sigma^{1.5}}{\rho^{2.25}\epsilon^{1.5}} \right\} \right) & \xmark\xmark \\
%     {\sf CHOCO-SGD} & \BO\left( \max\left\{ \frac{\sigma^2}{n\epsilon^4}, \frac{G}{\delta\rho^2\epsilon^3} \right\} \right)\BO\left( \max\left\{ \frac{\sigma^2}{n\epsilon^4}, \frac{G}{\delta\rho^2\epsilon^3} \right\} \right) & \cmark\cmark  \\
%     \aname & \BO\left( \max\left\{ \frac{\sigma^3}{n\epsilon^3}, \frac{n\InitG}{\delta^2\rho^4\epsilon^2}, \frac{n^{1.25}\sigma^{1.5}}{\delta^{2.25}\rho^{4.5}\epsilon^{1.5}}\right\} \right)\BO\left( \max\left\{ \frac{\sigma^3}{n\epsilon^3}, \frac{n\InitG}{\delta^2\rho^4\epsilon^2}, \frac{n^{1.25}\sigma^{1.5}}{\delta^{2.25}\rho^{4.5}\epsilon^{1.5}}\right\} \right) & \cmark\cmark \\
%     \hline
% \end{tabular}
% \end{table}

% \newpage

\section{Problem Setup \& Background}
Consider a {\revision connected}, weighted and undirected graph $G = ( {\cal N} , {\cal E} , \W )$ with ${\cal N} = \{1,\ldots,n\}$ representing a set of $n$ agents, ${\cal E} \subseteq {\cal N} \times {\cal N}$ representing the communication links between agents, and $\W \in \mathbb{R}^{n \times n}$ is a symmetric, weighted adjacency matrix. Note that self-loops are included such that $\{i,i\} \in {\cal E}$ for all $i$. 
Our goal is to tackle the following optimization problem:
\begin{align}
\min_{ \theta \in \mathbb{R}^d }~f(\theta) := \frac{1}{n} \, \sum_{i=1}^n f_i(\theta), \label{eq:opt}
\end{align} 
where $f_i : \mathbb{R}^d \rightarrow \mathbb{R}$ is a continuously differentiable (possibly non-convex) objective function known to the $i$th agent. The objective function can be expressed as $f_i(\theta) = \mathbb{E}_{ \zeta \sim \mu_i } [ f_i( \theta; \zeta ) ]$ such that $\mu_i$ denotes the data distribution available at agent $i$. We assume that $f(\theta) > -\infty$ for any $\theta \in \mathbb{R}^d$.

Throughout this paper, we assume the following conditions on the objective function and the adjacency matrix ${\bf W}$:
% \begin{assumption} \label{ass:lips}
%     There exists L \geq 0L \geq 0 such that for any i \in {\cal N}i \in {\cal N}, the gradient of f_i( \cdot; \zeta )f_i( \cdot; \zeta ) is LL-Lipschitz,
%     \begin{equation} \label{eq:lips}
%         \| \nabla f_i( \theta; \zeta ) - \nabla f_i( \theta' ; \zeta ) \| \leq L \| \theta - \theta' \|,
%     \end{equation}
%     for any~\theta, \theta' \in \RR^d\theta, \theta' \in \RR^d and \zeta \in {\rm supp} (  \mu_i )\zeta \in {\rm supp} (  \mu_i ), i.e., the support of \mu_i\mu_i.
% \end{assumption}
{\revision
\begin{assumption} \label{ass:lips}
    There exists $L \in \mathbb{R}_+$ such that for any $i \in {\cal N}$, $\theta, \theta' \in \RR^d$,
    \begin{equation} \label{eq:lips}
        \mathbb{E}_{\zeta} \left[ \| \nabla f_i( \theta; \zeta ) - \nabla f_i( \theta' ; \zeta ) \|^2 \right] \leq L^2 \| \theta - \theta' \|^2.
    \end{equation}
\end{assumption}
}
\begin{assumption} \label{ass:mix}
The adjacency matrix $\W \in \RR_+^{n \times n}$ satisfies: (i) $W_{ij} = 0$ if $ \{ i,j \} \notin {\cal E}$; (ii) $\W {\bf 1}_n = \W^\top {\bf 1}_n = {\bf 1}_n$; (iii) let $\U \in \RR^{n \times (n-1)}$ be a matrix with orthogonal columns satisfying $\I_n - (1/n) {\bf 1}{\bf 1}^\top = \U \U^\top$, there exists $\rho \in (0,1]$ such that $\| \U^\top \W \U \| \leq 1 - \rho$; (iv) there exists $\bw \in (0,2]$ such that $\norm{ \W - \I_n } \leq \bw$.
    % \begin{enumerate}[leftmargin=*, topsep=0mm, itemsep=0mm, partopsep=0mm]
    %     \item (graph topology) W_{ij} = 0W_{ij} = 0 if  \{ i,j \} \notin {\cal E} \{ i,j \} \notin {\cal E},
    %     \item (doubly stochastic) \W {\bf 1}_n = \W^\top {\bf 1}_n = {\bf 1}_n\W {\bf 1}_n = \W^\top {\bf 1}_n = {\bf 1}_n,
    %     \item (mixing) let \U \in \RR^{n \times (n-1)}\U \in \RR^{n \times (n-1)} be a matrix with orthogonal columns satisfying \I_n - (1/n) {\bf 1}{\bf 1}^\top = \U \U^\top\I_n - (1/n) {\bf 1}{\bf 1}^\top = \U \U^\top, then there exists \rho \in (0,1]\rho \in (0,1] such that \| \U^\top \W \U \| \leq 1 - \rho\| \U^\top \W \U \| \leq 1 - \rho.
    %     \item (bounded eigenvalue) there exists $\bw \in (0,2]$ such that $\norm{ \W - \I_n } \leq \bw$.
    % \end{enumerate}
\end{assumption}\vspace{-0.0cm}
The above conditions are standard. \Cref{ass:lips} requires the objective function to be {\revision mean square} smooth.
% \footnote{\label{foot:lips}Our analysis is extensible to a slightly relaxed condition replacing \eqref{eq:lips} with $\mathbb{E}_\zeta[ \| \nabla f_i( \theta; \zeta) - \nabla f_i(\theta'; \zeta) \|^2 ] \leq L^2 \| \theta - \theta' \|^2$.}, 
It is also known that there exists $\W$ such that \Cref{ass:mix} is satisfied when $G$ is a connected graph, e.g., by using the Metropolis-Hastings weight; see \citep{boyd2004fastest}. 
{\revision For any ${\bf W}$ which is a weighted adjacency matrix on a connected graph satisfying conditions (i)-(ii), ${\bf 1}$ is the unique eigenvector of ${\bf W}$. It follows that $\| \U^\top \W \U \| = \max\{ \lambda_2, |\lambda_n| \}$ and conditions (i)-(iii) are equivalent to  \cite[Definition 1]{koloskova2019decentralized}, see \Cref{app:spectralgap}.}

We assume that the gradient of $f_i$ can be estimated as $\nabla f_i( \theta; \zeta)$ satisfying:
\begin{assumption} \label{ass:stoc}
    There exists $\sigma \geq 0$ such that for any $\theta \in \mathbb{R}^d$, $i=1,...,n$, the gradient estimate $\nabla f_i( \theta; \zeta)$ with $\zeta \sim \mu_i$ is unbiased with bounded second order moment, i.e., 
    \begin{equation}
    \mathbb{E}_{\zeta \sim \mu_i} [ \nabla f_i( \theta; \zeta) ] = \nabla f_i(\theta),~~\mathbb{E}_{\zeta \sim \mu_i} [ \norm{ \nabla f_i( \theta; \zeta) - \nabla f_i( \theta) }^2 ] \leq \sigma^2.
    \end{equation}
\end{assumption} \vspace{-.2cm}
Again, \Cref{ass:stoc} is a standard setting for stochastic optimization.

\paragraph{DSGD and CHOCO-SGD}
Equipped with \Cref{ass:mix}, \ref{ass:stoc}, a common practice for tackling \eqref{eq:opt} in a decentralized manner is to utilize $\W$ as a mixing matrix and combine mixing with stochastic gradient descent. To illustrate the basic idea, we observe the decentralized stochastic gradient ({\tt DSGD}) algorithm \citep{ram2010distributed}: at iteration $t \geq 0$ and all $i=1,\ldots,n$,\vspace{-.1cm}
\begin{equation}
    \theta_i^{t+1} = {\textstyle \sum_{j=1}^n} W_{ij} \theta_j^t - \eta \stocgrdf_i^{t}~~\text{where}~~\stocgrdf_i^{t} \equiv \nabla f_i( \theta_i^{t}; \zeta_i^{t} ),\vspace{-.2cm} \label{eq:dgd}
\end{equation}
such that $\eta>0$ is the step size and $\stocgrdf_i^{t}$ is a shorthand notation for the unbiased stochastic gradient with the data $\zeta_i^{t} \sim \mu_i$ drawn independently upon fixing $\theta_i^t$ and satisfying \Cref{ass:stoc}.
For agent $i$, the \emph{consensus step} $\sum_{j=1}^n W_{ij} \theta_j^t$ can be computed with a local average among the neighbors of $i$. 



Notice that for \eqref{eq:dgd}, agents are required to transmit $d$ real numbers on the graph $G$ to their neighbors at every iteration. In practice, the communication links between agents are bandwidth limited. To this end, a remedy is to apply \emph{compression} to messages transmitted on $G$. 
Formally, we consider a stochastic compression operator ${\cal Q}: \mathbb{R}^d \to \mathbb{R}^d$ satisfying the condition:
\begin{assumption}\label{ass:compress}
    For any $x \in \mathbb{R}^d$, the compressor output ${\cal Q}(x)$ is the random vector $\tilde{\cal Q}(x; \xi)$ with $\xi \sim \pi_x$ such that there exists $\delta \in (0,1]$ satisfying\vspace{-.05cm}
    \begin{equation} \label{eq:quant}
        \mathbb{E} \left[ \norm{ x - {\cal Q}(x) }^2 \right] = \mathbb{E} \left[ \| x - \tilde{\cal Q}(x; \xi) \|^2 \right] \leq (1-\delta) \norm{x}^2.
        % ~~~~\mathbb{E}_{ \xi \sim \pi_x } [ \norm{ {\cal Q}(x; \xi) }^2 ] \leq \norm{x}^2.
    \end{equation}
    % Furthermore, when conditioned on ${\cal F}^t$, the random variables $\zeta_1^{t+1}, \ldots, \zeta_n^{t+1}$ are independent. 
\end{assumption}
\vspace{-.3cm}
The above is a general condition on compressors as discussed in \citep{koloskova2019decentralized}. It is satisfied by a number of designs. For instance, with $k \leq d$, the top-$k$ (resp.~random-$k$) \emph{sparsifier}:
\begin{align} \label{eq:sparsifier}
\left[ {\cal Q}(x) \right]_i = x_i~~\text{if}~~i \in {\cal I}_x,~~\left[ {\cal Q}(x) \right]_i = 0~~\text{otherwise}.
    % \left[ {\cal Q}(x) \right]_i = \begin{cases}
    %     x_i , & \text{if}~i \in {\cal I}_x, \\ 
    %     0 , & \text{otherwise},\vspace{-.05cm}
    % \end{cases}\vspace{-.1cm}
\end{align} 
where ${\cal I}_x \subseteq \{1,\ldots,d\}$ with $|{\cal I}_x| = k$ is the set of the coordinates of $x$ with the largest $k$ magnitudes (resp.~uniformly selected at random), satisfies \Cref{ass:compress} with $\delta = k/d$. Other compressors such as random quantization \citep{wen2017terngrad,alistarh2017qsgd, stich2018sparsified, alistarh2019convergence} can also satisfy \eqref{eq:quant} {\revision with re-scaling}; see {\revision \Cref{app:subsec:rescale} and \citep{koloskova2019decentralized}}. 
Note that sending ${\cal Q}(x)$ in \eqref{eq:sparsifier} over a communication channel requires only $k$ real number transmission, achieving a $k/d$ compression ratio. 

However, applying ${\cal Q}(\cdot)$ to the consensus step in \eqref{eq:dgd} directly does not lead to a convergent algorithm as (i) the compressor is not unbiased, and (ii) the compression error will accumulate with $t \to \infty$. The {\tt CHOCO-SGD} algorithm \citep{koloskova2019decentralized} resolves the issue by incorporating an error feedback step: at iteration $t$,
\begin{align} 
& \hatprm_i^{t+1} = \hatprm_i^t + {\cal Q}( \prm_i^t - \eta \stocgrdf_i^{t} - \hatprm_i^t ), \\
& \theta_i^{t+1} = \theta_i^t - \eta \stocgrdf_i^{t} + \gamma {\textstyle \sum_{j=1}^n} W_{ij} ( \hatprm_j^{t+1} - \hatprm_i^{t+1} ), \label{eq:choco} 
\end{align}
for all $i$, where $\gamma > 0$ is the consensus step size, and $\eta, \stocgrdf_i^{t}$ were defined in \eqref{eq:dgd}. Instead of directly transmitting a compressed version of $\theta_i^t - \eta \stocgrdf_i^t$, a key feature of {\tt CHOCO-SGD} is that the latter maintains an auxiliary variable $\hatprm_i^{t}$ that accumulates the compressed \emph{difference} ${\cal Q}(\theta_i^t - \eta \stocgrdf_i^t - \hatprm_i^t )$. 
% Subsequently the main variable $\theta_i^t$ is updated through a consensus step with this auxiliary variable. 
% Under \Cref{ass:lips}, \ref{ass:mix} and the assumption that $\mathbb{E}[ \| \stocgrdf_i^t \|^2 ]$ is bounded, 
\citet{koloskova2019decentralizeda} proved that in $T$ iterations, {\tt CHOCO-SGD} finds a near-stationary solution of \eqref{eq:opt}, $\{ \theta_i^{\sf T} \}_{i=1}^n$ with ${\sf T} \in \{0,\ldots,T-1\}$, satisfying $\mathbb{E}[ \norm{\nabla f( n^{-1} \sum_{i=1}^n \theta_i^{\sf T} ))}^2 ] = {\cal O}(1/\sqrt{T})$. 

However, a drawback of {\tt CHOCO-SGD} is that its convergence requires the stochastic gradient $\mathbb{E}[ \| \stocgrdf_i^t \|^2 ]$ to be bounded for any $i,t$, see \citep{koloskova2019decentralized, koloskova2019decentralizeda}; or it can be shown that it requires the \emph{data similarity} $\sup_{ \theta \in \mathbb{R}^d} \norm{ \nabla f_i( \theta) - \nabla f( \theta) }$ is bounded. These conditions may not be valid when the local data are non-i.i.d. such as in the federated learning setting \citep{konevcny2016federated}. 
% Furthermore, the convergence rate of {\tt CHOCO-SGD} is does not match the iteration/sample complexity lower bound in \citep{arjevani2019lower}, making it undesirable for sample-efficient optimization.
\vspace{-.1cm} 

\section{Proposed \aname~Algorithm}\vspace{-.1cm}



Taking a closer look at {\tt CHOCO-SGD} \eqref{eq:choco} reveals that the algorithm is only able to utilize information from the local gradient estimates $\stocgrdf_i^t \approx \nabla f_i( \theta_i^t )$ in the local update step. The local update dynamics may thus remain non-stationary even when the solution $\theta_i^t$ is close to a stationary point of \eqref{eq:opt}. The issue is particularly severe when the local objective functions are not similar in the sense that $\nabla f_i( \theta ) \neq \nabla f_j( \theta )$. 
This motivates us to design an algorithm that will make $n^{-1} \sum_{i=1}^n \stocgrdf_i^t$ available locally. 

We propose the \emph{Doubly Compressed Momentum-assisted Stochastic Gradient Tracking} (\aname) algorithm. 
Our algorithm involves two main ingredients: {\sf (A)} a \emph{gradient tracking} step with \emph{compression} where each agent maintains an estimate of $n^{-1} \sum_{i=1}^n \stocgrdf_i^t$ at low communication cost; {\sf (B)} adaptive momentum-based variance reduction that improves the variance of estimate of $\stocgrdf_i^t$ using ${\cal O}(1)$ sample per iteration. 
% As a result, the algorithm needs to incorporate a small step size $\eta$ (or vanishing step size as $t \to \infty$) to compensate for the accumulated error. 

\setlength{\textfloatsep}{.2cm}
% \algsetup{indent=.5em}
\begin{algorithm}[hbtp]
    \caption{\aname~Algorithm}
    \label{alg:docom}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input:} mixing matrix $\W$; step sizes $\eta$, $\gamma$, $\beta$; initial batch number $b_0$; initial iterate $\bar{\prm}^0 \in \mathbb{R}^d$. 
    \STATE Initialize $\quad \prm^0_i = \bar{\prm}^0~\forall i \in [n]; \quad \hatprm^0_{i,j} = \bar{\prm}^0~\forall \{i, j\} \in {\cal E}$. \\
    \STATE Initialize stochastic gradient estimate \\
    \vspace{-.25cm}
    \[\textstyle \ve^0_i = \frac{1}{b_0} \sum_{r=1}^{b_0} \nabla f_i(\prm_i^{0}; \zeta_i^{0,r}), \big\{\zeta_{i}^{0,r} \big\}_{r=1}^{b_0} \thicksim \mu_i; \quad \gog_i^0 = \ve_i^{0}~\forall i \in [n]; \quad \hatgog^0_{i,j} = {\bf 0}_d~\forall \{i, j\} \in {\cal E}.\]
    \vspace{-.5cm}
    \FOR{$t$ {\bfseries in} $0, \dots, T-1$}
        \STATE \underline{\textsc{(Update)}} $\forall i \in [n]:$ Agent $i$ updates $\prm_i^{t+\half} = \prm_i^t - \eta \gog_i^t$
        \FOR{$\{i,j\}\in {\cal E}$ (notice $\{i,i\}\in {\cal E}$)}
            \STATE \underline{\textsc{(Prm. Gossip)}} Agent $j$ receive ${\cal Q}(\prm_i^{t+\half} - \hatprm_{i,i}^t )$ from agent $i$ and update $\hatprm^{t+1}_{j,i} = \hatprm^{t}_{j,i} + {\cal Q}(\prm_i^{t+\half} - \hatprm_{i,i}^t )$
        \ENDFOR
        \STATE \underline{\textsc{(Prm. Aggregate)}} $\forall i \in [n]:$ Agent $i$ updates $\prm_i^{t+1} = \prm_i^{t+\half} + \gamma \sum_{j:\{i,j\}\in {\cal E}} W_{ij} (\hatprm_{i,j}^{t+1} - \hatprm_{i,i}^{t+1})$ \label{line:theta}
        \STATE Draw data batch $\zeta_i^{t+1} \sim \mu_i$ and compute $\stocgrdf_i^{t+1} = \nabla f_i( \theta_i^{t+1}; \zeta_i^{t+1} )$, $\stocgrdfp_i^t = \nabla f_i( \theta_i^{t}; \zeta_i^{t+1} )$
        \STATE \underline{\textsc{(Momentum)}} $\forall i \in [n]:$ Agent $i$ updates $\ve_i^{t+1} = \beta\stocgrdf_{i}^{t+1}  +  (1-\beta)(\ve_i^{t} + \stocgrdf_i^{t+1}  -  \stocgrdfp_i^{t})$
        \STATE \underline{\textsc{(Grad. Tracker)}} $\forall i \in [n]:$ Agent $i$ updates $\gog_i^{t+\half} = \gog_i^t + \ve_i^{t+1} - \ve_i^t$
        \FOR{$\{i,j\}\in {\cal E}$ (notice $\{i,i\}\in {\cal E}$)}
        \STATE \underline{\textsc{(G.T. Gossip)}} Agent $j$ receive ${\cal Q}(\gog_i^{t+\half} - \hatgog_{i,i}^t )$ from agent $i$ and update $\hatgog_{j,i}^{t+1} = \hatgog_{j,i}^t + {\cal Q}(\gog_i^{t+\half} - \hatgog_{i,i}^t )$
        \ENDFOR
        \STATE \underline{\textsc{(G.T. Aggregate)}} $\forall i \in [n]:$ Agent $i$ updates $\gog_i^{t+1} = \gog_i^{t+\half} + \gamma \sum_{j:\{i,j\}\in {\cal E}} W_{ij} \left(\hatgog_{i,j}^{t+1} - \hatgog_{i,i}^{t+1}\right)$ \label{line:G}
    \ENDFOR
    \STATE {\bfseries Output:} pick the ${\sf T}$th iterate $\prm_i^{\sf T}$, where ${\sf T}$ is uniformly selected from $\{0,\ldots,T-1\}$ or ${\sf T}=T$.
    % or the last iterate $\prm_i^{T}$.
\end{algorithmic}
\end{algorithm}

Let $\eta > 0$ be step size, $\gamma, \beta \in (0,1]$, the \aname~algorithm at iteration $t \in \mathbb{N}$ reads: for $i=1,\ldots,n$,
\vspace{-.15cm}
\noindent 
% \begin{minipage}{0.6\textwidth}
\begin{subequations} \label{eq:algo}
\begin{align}
     \hspace{-.2cm} &  \prm_i^{t+1} = \prm_i^t - \eta \gog_i^t + \gamma \textstyle\sum_{j=1}^n W_{ij} \left( \hatprm_j^{t+1} - \hatprm_i^{t+1} \right) \label{eq:docom_a} \\
    \hspace{-.2cm} & \hatprm_i^{t+1} = \hatprm_i^t + {\cal Q} \left( \prm_i^t - \eta \gog_i^t - \hatprm_i^t \right) \label{eq:docom_b} \\
    \hspace{-.2cm} & \ve_i^{t+1} = \beta \stocgrdf_i^{t+1} + (1-\beta) \left( \ve_i^t + \stocgrdf_i^{t+1} - \stocgrdfp_i^{t} \right) \label{eq:docom_c} \\
    \hspace{-.2cm} &  \gog_i^{t+1} = \gog_i^t + \ve_i^{t+1} - \ve_i^t + \gamma \textstyle\sum_{j=1}^n W_{ij} \left( \hatgog_j^{t+1} \hspace{-.1cm} - \hatgog_i^{t+1} \right)  \label{eq:docom_d} \\
    \hspace{-.2cm} & \hatgog_i^{t+1} = \hatgog_i^t + {\cal Q} \left( \gog_i^t + \ve_i^{t+1} - \ve_i^t - \hatgog_i^t \right) \label{eq:docom_e}
\end{align}
\end{subequations}
% \end{minipage}
% \hspace{0.01\textwidth}
% \begin{minipage}{0.389\textwidth}
% \vspace{-.1cm}
In the above, we draw $\zeta_i^{t+1} \sim \mu_i$ at agent $i$ (or a minibatch of samples) as $\stocgrdf_i^{t+1} \equiv \nabla f_i( \theta_i^{t+1}; \zeta_i^{t+1} )$, $\stocgrdfp_i^t \equiv \nabla f_i( \theta_i^{t}; \zeta_i^{t+1} )$ such that the stochastic gradients in \eqref{eq:docom_c} are formed using the same data batch.
{\revision To implement \eqref{eq:docom_c}  with $\beta \neq 1$, agent $i$ needs direct access to the data batch $\zeta_i^{t+1}$ and the oracle $\nabla f_i( \cdot; \zeta_i^{t+1} )$.} Readers are  referred to \Cref{alg:docom} for details on the initialization and decentralized implementation.
% \end{minipage}


% to improve convergence rate, where our update form is similar to that of {\tt GT-HSGD} \citep{xin2021hybrid}. 
% \begin{algorithm}[t]
%     \caption{\aname}
%     \label{alg:docom}
%  \begin{algorithmic}
%     \STATE {\bfseries Input:} mixing matrix $\W$; stepsizes $\eta$, $\gamma$; momentum $\beta$; initial batch number $b_0$; initial iterate $\bar{\prm}^0 \in \mathbb{R}^d$. \\
%     Initialize $\prm^0_i = \bar{\prm}^0~\forall i \in [n], \hatprm^0_{i,j} = \bar{\prm}^0~\forall \{i, j\} \in E$, \\
%     \hspace{1.2em} $\gog_i^0 = \ve_i^{0} = \textbf{0}_d~\forall i \in [n], \hatgog^0_{i,j} = \textbf{0}_d~\forall \{i, j\} \in E$.
%     \STATE Initialize the stochastic gradient estimate \\
%     \centerline{$ \ve^1_i = \frac{1}{b_0} \sum_{r=1}^{b_0} \nabla f_i(\prm_i^{0}; \zeta_i^{0,r}), \left\{\zeta_{i}^{0,r} \right\}_{r=1}^{b_0} \thicksim \mu_i$}
%     \FOR{$t$ {\bfseries in} $0, \dots, T-1$}
%         \STATE $\forall i:~\gog_i^{t+\half} = \gog_i^t + \ve_i^{t+1} - \ve_i^t$
%         \FOR{$\{i,j\}\in E$ (notice $\{i,i\}\in E$)}
%         \STATE Agent $j$ receive ${\cal Q}(\gog_i^{t+\half} - \hatgog_{i,i}^t )$ from agent $i$
%         \STATE Set $\hatgog_{j,i}^{t+1} = \hatgog_{j,i}^t + {\cal Q}(\gog_i^{t+\half} - \hatgog_{i,i}^t )$
%         \ENDFOR
%         \STATE $\forall i:~\gog_i^{t+1} = \gog_i^{t+\half} + \gamma \sum_{j:\{i,j\}\in E} W_{ij} (\hatgog_{i,j}^{t+1} - \hatgog_{i,i}^{t+1})$
%         \STATE 
%         \STATE $\forall i:~\prm_i^{t+\half} = \prm_i^t - \eta \gog_i^t$
%         \FOR{$\{i,j\}\in E$ (notice $\{i,i\}\in E$)}
%             \STATE Agent $j$ receive ${\cal Q}(\prm_i^{t+\half} - \hatprm_{i,i}^t )$ from agent $i$
%             \STATE Set $\hatprm^{t+1}_{j,i} = \hatprm^{t}_{j,i} + {\cal Q}(\prm_i^{t+\half} - \hatprm_{i,i}^t )$
%         \ENDFOR
%         \STATE ∀i: \prmt+1i=\prmt+\halfi+γ∑j:{i,j}∈EWij(\hatprmt+1i,j−\hatprmt+1i,i)\forall i:~\prm_i^{t+1} = \prm_i^{t+\half} + \gamma \sum_{j:\{i,j\}\in E} W_{ij} (\hatprm_{i,j}^{t+1} - \hatprm_{i,i}^{t+1})
%         \STATE ∀i: \vet+2i=β\stocgrdft+1i+(1−β)(\vet+1i+\stocgrdft+1i−\stocgrdfpti)\forall i:~ \ve_i^{t+2} = \beta\stocgrdf_{i}^{t+1} + (1-\beta)(\ve_i^{t+1} + \stocgrdf_i^{t+1} - \stocgrdfp_i^{t})
%     \ENDFOR
%     \STATE {\bfseries Output:} last iterate model parameters \prmTi\prm_i^T in each agent ii
%  \end{algorithmic}
%  \end{algorithm}
 
Unlike {\tt CHOCO-SGD} \eqref{eq:choco}, the local update steps in \eqref{eq:docom_a}, \eqref{eq:docom_b} are computed along the direction given by $g_i^t$. The latter is then updated according to \eqref{eq:docom_d}, \eqref{eq:docom_e}, which aims at \emph{tracking the dynamically updated average gradient estimator} $g_i^t \approx n^{-1}\sum_{j=1}^n v_j^t$ with compressed communications given by ${\cal Q}(\cdot)$. In this way, we say that the {\aname} algorithm is \emph{doubly compressed}. 
Furthermore, as for $v_j^t$, \eqref{eq:docom_c} uses a recursive variance reduced estimate to estimate the (exact) gradient $v_j^t \approx \nabla f_j( \theta_j^t )$ \citep{cutkosky2019momentum, tran2021hybrid}. Together, {\aname} yields a consensus based algorithm where the variance reduced and averaged gradient $g_i^t \approx n^{-1}\sum_{j=1}^n v_j^t$ is simultaneously available at each agent.

From an implementation perspective, \aname~shares the communication and computation costs per iteration {\revision of the same order as {\tt CHOCO-SGD} at ${\cal O}(d)$}. {\revision In fact, only an} extra communication step (with compression) is needed for the tracking of $n^{-1} \sum_{i=1}^n v_i^t$ and an extra computation step is needed for computing $\stocgrdfp_i^t$, in \eqref{eq:docom_d}, \eqref{eq:docom_e}. 
% {\re Also \aname uses doubled computation for \stocgrdfpti\stocgrdfp_i^t} 
% Similar to {\tt CHOCO-SGD}, {\aname} requires each agent to store auxiliary variables {\hatprmtj,\hatgogtj}j∈Ni\{ \hatprm_j^t , \hatgog_j^t \}_{j \in {\cal N}_i} of its neighbors to apply error compensation. 
{\revision A detailed comparison on computational costs is shown in Table \ref{tab:comp-compl}.}
As we will show later, the above shortcomings can be compensated by the improved convergence rate of \aname.

\subsection{Main Results}
% This section analyzes the expected convergence rate of {\aname} in seeking a (near-)stationary solution of (???)(???)(???)(???)(???)(???)(???)(???)(???)(???)(???)(???)(???)\eqref{eq:opt}.
We show that {\aname} achieves state-of-the-art convergence rate for smooth problems.
Let $\avgtheta^t := n^{-1} \sum_{i=1}^n \theta_i^t$ be the averaged iterate, $\InitG := n^{-1} \mathbb{E}[ \sum_{i=1}^n \norm{ g_i^0 }^2 ]$ be the initial expected gradient norm, $f^\star := \min_{ \theta' } f( \theta' )$ be the optimal objective value.
We first summarize the convergence results under the mentioned assumptions where \eqref{eq:opt} is smooth but possibly non-convex:
% \begin{center}
% \fbox{\begin{minipage}{.98\linewidth}
\begin{tcolorbox}
\begin{theorem} \label{th:main}
Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}. Suppose that the step sizes satisfies 
\begin{equation} \label{eq:docom_stepsize}
\begin{split}
& \eta \leq \min \left\{ \eta_\infty, \sqrt{ {\brho n} / ({8 \ConstG}}) \right\},~~\gamma \leq \gamma_\infty,
\end{split}
\end{equation}
where $\gamma_\infty, \eta_\infty$ are defined in \eqref{eq:stepsize_whole}. 
Set $\beta \in (0,1)$, $\brho = \min\{ \frac{\rho \gamma}{8}, \frac{\delta \gamma}{8}, \beta \}$. 
For any $T \geq 1$, it holds
\begin{equation}
    \sum_{t=0}^{T-1} \mathbb{E} \left[ \frac{\norm{ \nabla f( \avgtheta^t)}^2}{2T} + \frac{L^2}{nT} \sum_{i=1}^n \norm{ \theta_i^t - \avgtheta^t}^2 \right] \leq \frac{f( \avgtheta^0 ) - f^\star}{ \eta T } + \ConstS \frac{2 \beta^2 \sigma^2}{\brho n} + \frac{4\sigma^2}{b_0 \brho T n} + \frac{\eta^2}{\brho T} \frac{236 L^2 \InitG}{ \rho^2 \gamma^2 (1-\gamma) }, \notag
\end{equation}\vspace{-.3cm}
where
\begin{align}
    & \ConstS = 4 + \frac{\eta^2}{\gamma^3}\frac{672L^2 n}{\rho^3} + \frac{\eta^2}{\gamma} \frac{6 L^2 n \rho^4 \delta}{25 \bw^2} + \frac{\eta^2}{\gamma^2} \frac{4 L^2 n}{\bw^2}, \label{eq:constS} \\
    & \ConstG = 8(1-\beta)^2 L^2 (1-\rho\gamma)^2 + \frac{L^2 n}{\rho \gamma} \left( 96 + \frac{141}{400} \frac{\rho^2}{\bw^2} \right). \notag 
\end{align}
\end{theorem} 
\end{tcolorbox}
% \end{minipage}}\end{center}

We provide the proof of \Cref{th:main} in \Cref{sec:pf}. Below we discuss its main consequences.
% The momentum parameter $\beta$ controls the effective noise variance in \eqref{eq:main_bd}. 

\paragraph{Near-optimal Iteration/Sample Complexity} 
Setting the step sizes and parameters as $\eta = \frac{ n^{2/3} }{ L T^{1/3} }, \gamma = \gamma_\infty, \beta = \frac{ n^{1/3} }{ T^{2/3} }, b_0 = \frac{ T^{1/3} }{ n^{2/3} }$. Further, we select the ${\sf T}$th iterate as the output of {\aname} such that ${\sf T}$ is independently and uniformly selected from $\{0,\ldots, T-1\}$ [cf.~the output of \Cref{alg:docom}], similar to \citep{ghadimi2013stochastic}. For a sufficiently large $T$, it can be shown that \vspace{-.1cm}
\begin{equation}
\frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ \norm{ \nabla f( \theta_i^{\sf T} )}^2 \right] = {\cal O} \left( \frac{L (f(\avgtheta^0) - f^\star)}{(nT)^{2/3}} + \frac{\sigma^2}{ (n T)^{2/3}} + \frac{n \InitG}{ \delta^2 \rho^4 T} + \frac{\sigma^2 n^{5/3}}{ \delta^3 \rho^6 T^{4/3}} \right) , \label{eq:grdFbound} 
\end{equation}
where we have used the Lipschitz continuity of $\nabla f_i(\cdot)$ [cf.~\Cref{ass:lips}] to derive a bound on the gradient of individual iterate $\theta_i^{\sf T}$. 

For any agent $i=1,\ldots, n$, the iterate $\theta_i^{\sf T}$ is guaranteed to be ${\cal O}(1/T^{2/3})$-stationary to \eqref{eq:opt}. 
Notice that this is the state-of-the-art convergence rate for first order stochastic optimization even in the centralized setting; see \citep{cutkosky2019momentum, tran2021hybrid}; and it also matches the lower bound in \citep{arjevani2019lower}. Our rate is comparable to or faster than a number of decentralized algorithms with or without compression; see \Cref{tab:compare}. Further, we remark that \Cref{th:main} does not impose condition on $f_i$'s similarity $\sup_{\theta} \| \nabla f_i(\theta) - \nabla f(\theta) \|$ in which our convergence rate is independent of the data heterogeneity. 

{\revision 
Note that the step size configuration in \eqref{eq:grdFbound} requires $n \leq \mathcal{O}(L^{3/4} T^{1/2})$ in order to satisfy $\eta \leq \eta_{\infty}$, thereby necessitating $T = \Omega(n^2)$. As an alternative, it is possible to select
% For better dependence between $n$ and $T$, we observe by choosing
$\eta = \frac{1}{L T^{1/3}}, \gamma = \gamma_\infty, \beta = \frac{n^{1/3}}{T^{2/3}}, b_0 = \frac{T^{1/3}}{n^{2/3}}$, which yields
\begin{equation}
\frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ \norm{ \nabla f( \theta_i^{\sf T} )}^2 \right] = {\cal O} \left( \frac{L (f(\avgtheta^0) - f^\star)}{T^{2/3}} + \frac{\sigma^2}{ (n T)^{2/3}} + \frac{\InitG}{n^{1/3} \delta^2 \rho^4 T} + \frac{\sigma^2 n^{5/3}}{ \delta^3 \rho^6 T^{4/3}} \right) . \label{eq:grdFbound_2} 
\end{equation}
In this case satisfying $\eta \leq \eta_{\infty}$ requires $n \leq \mathcal{O}(L^{3/2} T)$ and thus $T = \Omega(n)$ similar to \cite{koloskova2019decentralizeda}. However, as a trade-off, it gives a worse dependence on $f(\avgtheta^0) - f^\star$. 
% which matches the dependence in other decentralized algorithms such as \cite{koloskova2019decentralizeda}.
}

\paragraph{Impacts of Network Topology and Compressor} Eq.~\eqref{eq:grdFbound} indicates the impacts of network topology (due to $\rho$) and compressor (due to $\delta$) vanish as $T \to \infty$. This can be observed by recognizing that the last two terms in \eqref{eq:grdFbound} are ${\cal O}(1/T), {\cal O}(1/T^{4/3})$. In \Cref{app:step_twist}, we demonstrate with a similar set of step sizes, for any 
% \begin{equation}
$T \geq T_{\sf trans} = \Omega( n^3 \InitG^3 / ( \sigma^6 \delta^6 \rho^{12} ) )$, 
% \end{equation}
\aname~enjoys a matching convergence behavior as a centralized SGD algorithm employing a momentum-based variance reduced gradient estimator with a batch size of $n$, e.g., \citep{tran2021hybrid}. In the latter case, we have $n^{-1} \sum_{i=1}^n \mathbb{E} \left[ \norm{ \nabla f( \theta_i^{\sf T} )}^2 \right] = {\cal O}( \sigma^2 / (nT^{2/3} ))$. The constant $T_{\sf trans}$ is also known as the transient time of decentralized algorithm \citep{pu2020asymptotic}.

Our result does not require any assumption on the data heterogeneity level nor the boundedness of gradient as in {\tt CHOCO-SGD} \citep{koloskova2019decentralizeda} or {\tt DSGD} \citep{lian2017can}. As hinted before, this is a consequence of gradient tracking. 
In \Cref{app:betaequalone}, we provide a separate analysis for the case of $\beta = 1$ when no momentum is applied in \eqref{eq:docom_c}. Interestingly, in the latter case, the convergence rate is only ${\cal O}(1/\sqrt{T})$ [cf.~\eqref{eq:docom_beta}], indicating that the momentum term is crucial in accelerating \aname.

\paragraph{PL Condition} Finally, we show that the convergence rate can be improved when the objective function satisfies the Polyak-Łojasiewicz (PL) condition: 
\begin{assumption} \label{ass:pl}
    For any $\prm \in \mathbb{R}^d$, it holds that $\norm{ \nabla f( \prm ) }^2 \geq 2 \mu \big[ f( \prm ) - f^\star \big]$ for some $\mu > 0$.
\end{assumption}
\vspace{-.0cm}
Notice that the PL condition is satisfied by strongly convex functions as well as a number of non-convex functions; see \cite{karimi2016linear}. We obtain:

% We observe that: 
\begin{tcolorbox}
\begin{corollary} \label{cor:pl}
Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}, \ref{ass:pl}. Suppose that the step size condition \eqref{eq:docom_stepsize} holds and $\beta \in (0,1)$. Then, for any $t \geq 1$, it holds
\begin{equation}
    \Delta^{t} + \frac{2 L^2 \eta}{\brho n} \sum_{i=1}^n \mathbb{E} [ \norm{\theta_i^t - \avgtheta^t}^2 ] \leq \left( 1 - \bmu \right)^t \left( \Delta^0 + \frac{2 \eta}{ \brho n } \ER^0 \right)  + \frac{ \eta \beta^2 }{ \brho  \bmu } \frac{2 \ConstS \sigma^2}{n} , \label{eq:plcase}
\end{equation}
where $\bmu := \min \left\{ \eta \mu, { \brho } / {2} \right\}$, $\Delta^t := \mathbb{E} [ f( \avgtheta^t )] - f^\star$ is the expected optimality gap and the constant $\ConstS$ is defined in \eqref{eq:constS}. Notice that $\ER^0$ can be upper bounded with \eqref{eq:er0}.
% \begin{align*}
%     E^0 = \frac{2 \eta}{ \brho n } \left( \frac{n \sigma^2}{b_0} + \frac{192}{ \rho^2 \gamma^2 (1-\gamma)} \InitG \eta^2 \right).
% \end{align*}
\end{corollary}
\end{tcolorbox}

Setting the step sizes and parameters as $\eta = \log T/T, \gamma = \gamma_\infty, \beta = \log T /T, b_0 = \Omega(1)$. For sufficiently large $T$, it can be shown that \vspace{-.05cm}
\begin{align}
    & \textstyle \mathbb{E} [ f( \avgtheta^T )] - f^\star = {\cal O}( \log T /T ), \\
    & \frac{1}{n} \sum_{i=1}^n \mathbb{E} [ \norm{\theta_i^T - \avgtheta^T}^2 ] = {\cal O}( \log T /T ), \label{eq:epsreq1}\vspace{-.05cm}
\end{align}
% for sufficiently large $T \geq 1$ (see \Cref{app:largeT}).
see \Cref{app:largeT}.
Moreover, in the \emph{deterministic gradient case with $\sigma^2 = 0$}, we can select a constant $\beta, \eta$. Then, \eqref{eq:plcase} shows that \aname~converges \emph{linearly} to an optimal solution such that $\mathbb{E} [ f( \avgtheta^T )] - f^\star = {\cal O}( (1-\bmu)^T )$. 
We remark that the latter rates match the  recent algorithms with compression \citep{liu2020linear, liao2021compressed, song2021compressed, kovalev2021linearly} for strongly convex problems.\vspace{-.1cm}

\subsection{Proof of \Cref{th:main}}\label{sec:pf} 
We preface the proof by defining the following notations for the variables in \aname. For any $t \geq 0$: \vspace{-.0cm}
\begin{align*}
    \Theta^t = \left(\hspace{-.05cm} \begin{array}{c}
        (\theta_1^t)^\top \\
        \vdots \\
        (\theta_n^t)^\top 
    \end{array} \hspace{-.05cm}\right),
    V^t = \left(\hspace{-.05cm} \begin{array}{c}
        (\ve_1^t)^\top \\
        \vdots \\
        (\ve_n^t)^\top 
    \end{array} \hspace{-.05cm}\right),
    G^t = \left(\hspace{-.05cm} \begin{array}{c}
        (g_1^t)^\top \\
        \vdots \\
        (g_n^t)^\top 
    \end{array} \hspace{-.05cm}\right)
    % \in \RR^{n \times d}
\end{align*} 
which are $n \times d$ matrices. 
Similarly, we define the matrices $\hatTheta^t$, $\hatG^t$ based on $\{ \hat{\theta}_i^t \}_{i=1}^n, \{ \hatgog_i^t \}_{i=1}^n$, and the matrices $\stocgrdF^t$, $\stocgrdFp^t$, $\grdF$  based on $\{ \stocgrdf_i^t \}_{i=1}^n$, $\{ \stocgrdfp_i^t \}_{i=1}^n$, $\{ \nabla f_i( \theta_i^t ) \}_{i=1}^n$. 

The norm of the matrix $\Theta_o^t = {\bf U}^\top \Theta^t$, i.e., $\|{\bf U}^\top \Theta^t\|_F^2 = \|{\bf U}{\bf U}^\top \Theta^t\|_F^2 = \| ({\bf I} - (1/n){\bf 1}{\bf 1}^\top) \Theta^t \|_F^2$, measures \emph{consensus error} of the iterate $\Theta^t$. 
% since $\Theta_o^t = {\bf U}^\top ( {\bf I} - (1/n){\bf 1}{\bf 1}^\top) \Theta^t$. 
We denote $G_o^t = {\bf U}^\top G^t$ such that $\norm{G_o^t}_F^2$ measures \emph{consensus error} of $G^t$.

Denote the average variables $\avgtheta^t = n^{-1} {\bf 1}^\top \Theta^t$, $\avgv^t = n^{-1} {\bf 1}^\top V^t$, $\avgg^t = n^{-1} {\bf 1}^\top G^t$, $\avggrdF^t = n^{-1} {\bf 1}^\top \grdF^t$. 
We first make the following observation regarding the $\avgtheta^t$-update:
\begin{lemma} \label{lem:f_1stepnew} Under \Cref{ass:lips} and the step size condition $\eta \leq \frac{1}{2L}$. Then, for any $t \geq 0$, it holds \vspace{-.0cm}
\begin{equation}
    f( \avgtheta^{t+1} ) \leq f( \avgtheta^t )   - \frac{\eta}{2} \norm{ \nabla f( \avgtheta^t ) }^2 + \frac{ L^2 \eta }{n} \norm{ \Theta_o^t }_F^2 + \eta \norm{ \avgv^t - \avggrdF^t}^2 - \frac{\eta}{4} \norm{\avgg^t}^2. \label{eq:f_1stepnew}
\end{equation}
\end{lemma} 
The proof is relegated to \Cref{app:f1step}. 
The above lemma utilizes just \Cref{ass:lips} and results in a deterministic bound of $f( \avgtheta^{t+1})$. 
We highlight that the bound contains a negative term of the stochastic gradient $- \frac{\eta}{4} \norm{\avgg^t}^2$, which is slightly different from the standard bound implied by the descent lemma. Such negative term is crucial for deriving the near-optimal sampling complexity of {\aname} in Theorem~\ref{th:main}, also see \citep{cutkosky2019momentum}.

\Cref{lem:f_1stepnew} shows that controlling $\norm{ \nabla f( \avgtheta^t ) }^2$ requires bounding $\norm{ \Theta_o^t }_F^2$ and $\|\avgv^t - \avggrdF^t \|^2$. While the latter are anticipated to converge to zero, we see that characterizing their convergence results in a set of \emph{coupled recursions} as follows:
\begin{lemma} \label{lem:theta_o_new} Under \Cref{ass:mix}, \ref{ass:compress}. Then, for any $t \geq 0$, it holds  \vspace{-.1cm}
    \begin{equation} \label{eq:theta_o_bd_lem}
    \mathbb{E} [\norm{\Theta_o^{t+1}}_F^2]  \leq (1-\frac{\rho\gamma}{2}) \mathbb{E} [\norm{\Theta_o^t}_F^2] + \frac{2}{\rho} \, \frac{\eta^2}{\gamma} \mathbb{E} [ \norm{G_o^t}_F^2 ] + \frac{\bw^2}{\rho} \, \gamma \, \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hatTheta^t}^2_F \right]. 
    \end{equation}
\end{lemma}
\begin{lemma} \label{lem:vt_bound} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and let $\beta \in [0,1)$. Then, for any $t \geq 0$, it holds \vspace{-.1cm}
    \begin{align}
        & \mathbb{E} \left[ \norm{ \avgv^{t+1} - \avggrdF^{t+1} }^2 \right] \label{eq:vt_bound_lem} \\
        & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ \avgv^{t} - \avggrdF^{t} }^2 \right] + 2 \beta^2 \frac{\sigma^2}{n} + (1-\beta)^2 \frac{8 L^2}{n^2} \eta^2 (1-\rho \gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 + \frac{n}{2} \norm{ \avgg^t }^2 \right] \notag \\
        &\quad + (1-\beta)^2 \frac{8 L^2}{n^2} \bw^2 \gamma^2 \, \mathbb{E} \left[ \frac{1-\delta}{2} \norm{\Theta^t - \eta G^t - \hatTheta^t}_F^2 + \norm{\Theta_o^t}_F^2 \right]. \notag
    \end{align}
\end{lemma}
 \vspace{-.2cm}
The proofs are in \Cref{app:theta_o_new}, \ref{app:vt_bound}. 
Notice that \eqref{eq:theta_o_bd_lem}, \eqref{eq:vt_bound_lem} further depend on the quantities $\mathbb{E}[ \norm{G_o^t}_F^2]$, $\mathbb{E} [ \| \Theta^t - \eta G^t - \hatTheta^t \|_F^2]$, $\mathbb{E} [ \| G^t - \hatG^t \|_F^2 ]$, which are handled by the following lemmas:

\begin{lemma}\label{lem:gt_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$, $\gamma \leq \frac{1}{8 \bw}$. For any $t \geq 0$, it holds
    \begin{align}
    \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{4} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hatG^t }_F^2 \right] + \gamma \, \frac{25 L^2 \bw^2}{\rho} \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \\
    & \quad + \gamma \, \frac{13 L^2}{ \rho } \bw^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] \nl 
    & \quad + \gamma \, \frac{ \rho n}{5} \, \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] + \frac{7}{ \rho \gamma } \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + \frac{7n }{\rho \gamma} \beta^2 \sigma^2 . \notag
    \end{align}
\end{lemma}
\begin{lemma} \label{lem:thetahat_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\eta \leq \min \{ \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} },  \frac{1}{4L} \}$,
$\gamma^2 \leq \min \{\frac{\delta}{ 16 \bw^2 (1-\delta) (1 + 3\eta^2 L^2) (1 + 2/\delta)}, \frac{1}{\rho^2}, \frac{\delta^2}{64\bw^2}\}$, $\eta^2 \gamma \leq \frac{\delta^2 \rho}{1248 \bw^2 L^2}$. For any $t \geq 0$, it holds
    \begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hatTheta^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{8} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hatTheta^t}_F^2 \right] \\
        & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \frac{3 \bw}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hatG^t}_F^2 \right] \nl 
        & \quad + \left[ \frac{29}{\delta} \bw^2 \gamma^2 + \frac{38 L^2 \bw \eta^2}{\rho} \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \frac{18 \eta^2}{\delta} \, n \, \mathbb{E}  \left[ \norm{ \avgg^t }^2 \right] \nl 
        & \quad + \left( 18 + \frac{84}{\rho\gamma} \right)\frac{ \beta^2\eta^2 }{\delta} \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \left( 18 + \frac{84}{\rho \gamma} \right) \frac{\beta^2 \eta^2 n\sigma^2}{\delta} . \notag
    \end{align}
\end{lemma}
\begin{lemma} \label{lem:gtghat_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\gamma \leq \frac{\delta}{8 \bw}$, $\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$. For any $t \geq 0$, it holds
    \begin{align}
    \mathbb{E} \left[ \norm{ G^{t+1} - \hatG^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hatG^{t} }_F^2 \right] + \frac{10}{\delta} \gamma^2 \left( \bw^2 + \frac{\rho^2 }{8} \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \\
    & \quad + \gamma^2 \, \frac{122L^2\bw^2}{\delta} \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \gamma^2 \, \frac{60 L^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] \nl
    & \quad + \gamma^2 \, \frac{3 \rho^2}{5 \delta} n \mathbb{E} \left[ \norm{ \avgg^t }^2 \right]  + \frac{31}{\delta} \beta^2 \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \frac{31}{\delta} \beta^2 n \sigma^2. \notag
    \end{align}
\end{lemma}
The proofs are in \Cref{app:gt_new}, \ref{app:thetahat_new}, \ref{app:gtghat_new}. 
% Notice that \eqref{eq:theta_o_bd_lem}, \eqref{eq:vt_bound_lem} further depend on the quantities $\mathbb{E}[ \norm{G_o^t}_F^2]$, $\mathbb{E} [ \| \Theta^t - \eta G^t - \hatTheta^t \|_F^2]$, $\mathbb{E} [ \| G^t - \hatG^t \|_F^2 ]$. In \Cref{app:control_GoTheta}, we derive similar recursions to control them. 

\paragraph{Potential Function} As we are equipped with the above lemmas, showing the convergence of {\aname} requires tracking the error quantities in a unified fashion. This may not be obvious at the first glance due to the coupling between error quantities illustrated in the lemmas. Naturally, one can proceed by defining the sequence of potential function values: 
\begin{align}
 \ER^{t} & = \mathbb{E} \left[ L^2 \norm{\Theta_o^{t}}_F^2 + n \norm{ \avgv^{t} - \avggrdF^{t} }^2 + {\textstyle \frac{1}{n}} \norm{V^{t} - \grdF^{t} }_F^2 \right] \notag \\
    & \quad + \mathbb{E} \left[ a \norm{G_o^{t}}_F^2 + b \norm{ G^{t} - \hatG^{t} }_F^2 + c \norm{ \Theta^{t} - \eta G^{t} - \hatTheta^{t} }_F^2 \right] \label{eq:Vt_design} 
\end{align}
that comprises of the coupled error quantities, where $a,b,c>0$ are constants to be determined. Our plan is then to study the convergence of $\ER^{t}$. 
However, fully specifying the potential function and to ensure the \emph{near-optimal sample complexity} for {\aname} require finding the tight conditions on $a,b,c>0$, together with the step size conditions, which is not trivial as it requires approximately solving a $5 \times 5$ system of (nonlinear) inequalities; see \eqref{eq:big_inequality_sys} in the appendix. 
% The proof is relegated to \Cref{app:wholesys} where we demonstrated how to derive a set of 

In \Cref{app:wholesys}, we provide a systematic construction for finding the parameters of the tight potential function, which results in the following lemma:
% where $a,b,c>0$ are determined below. We observe 
\begin{lemma} \label{lem:wholesys}
    Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and let $\beta \in (0,1)$. 
    Suppose that the step sizes satisfy: 
    \small \begin{align}
        \gamma & \leq \min \left\{ \frac{1}{4 \rho}, \frac{\rho n}{64 \bw^2}, \frac{ \delta}{10 \bw}, \frac{\delta \rho {\redtmp \sqrt{1-\delta/(8 \bw^2)}}}{259 \bw^2} \right\} =: \gamma_\infty , \notag \\
        \eta & \leq \frac{\gamma}{L}  \min \bigg\{
            \sqrt{\frac{1-\beta}{\beta n}}\frac{\sqrt{ \gamma \rho^3} }{45} , \frac{\rho^2}{240 \bw }
        \bigg\} =: \eta_\infty . \label{eq:stepsize_whole}
    \end{align} 
    \normalsize Set the parameters in the potential function $\ER^t$ such that
    \small \begin{equation}
    a = \frac{96L^2}{\rho^2\gamma^2}\eta^2
    , ~~b = \frac{\eta^2}{\gamma(1-\gamma)}\frac{3072\bw^2 L^2}{\delta\rho^3}
    , ~~c = \frac{\gamma}{1-\gamma}\frac{48 L^2\bw^2}{\delta\rho}.
    \end{equation}
    \normalsize Then, for any $t \geq 0$, it holds 
    \begin{align} \label{eq:erbound}
    \hspace{-.1cm} \ER^{t+1} \leq ( 1- \brho ) \ER^t + \beta^2 \ConstS \sigma^2 + \eta^2 \ConstG \mathbb{E} \left[ \norm{\avgg^t}^2 \right],
    \end{align}
    where 
    % we have set $a = \frac{96L^2}{\rho^2\gamma^2}\eta^2
    % , b = \frac{\eta^2}{\gamma(1-\gamma)}\frac{3072\bw^2 L^2}{\delta\rho^3}
    % , c = \frac{\gamma}{1-\gamma}\frac{48 L^2\bw^2}{\delta\rho}$ in the definition of $\ER^t$, and 
    $\ConstS, \ConstG, \brho$ were defined in \Cref{th:main}.
\end{lemma}
The above lemma shows that the potential function $\ER^t$ is connected to the noise variance $\sigma^2$ and the gradient norm $\norm{\avgg^t}^2$. The convergence of the latter term is of interest to our theorem. We observe the following consequence.

Equipped with \eqref{eq:erbound} and define $\Delta^t := \mathbb{E} [ f( \avgtheta^t )] - f^\star$. From \Cref{lem:f_1stepnew}, we can deduce that \vspace{-.1cm}
\begin{align}
    \Delta^{t+1} + \frac{2\eta}{n \brho} \ER^{t+1} &\leq \Delta^{t} + \frac{2\eta}{n \brho} \ER^{t} + \frac{2 \eta}{n \brho} \beta^2 \ConstS \sigma^2 - \eta \, \mathbb{E} \left[ \frac{1}{2} \norm{\nabla f(\avgtheta^t)}^2 + \frac{L^2}{n} \norm{\Theta_o^t}_F^2 \right]\label{eq:sumup} \\
    % + \norm{ \avgv^t - \avggrdF^t }^2 
    & \quad+ \left( ({ n \brho })^{-1} {2 \eta^3} \ConstG - 4^{-1} {\eta}  \right) \mathbb{E} \left[ \norm{\avgg^t}^2 \right]. \nonumber 
\end{align}
Setting $\eta \leq \sqrt{ \frac{\brho n}{ 8 \ConstG } }$ as in \eqref{eq:docom_stepsize} shows that the last term in the r.h.s.~of the above can be upper bounded by zero. Summing up both sides of \eqref{eq:sumup} from $t=0$ to $t=T-1$ yields \vspace{-.1cm}
\begin{equation}
    \eta \sum_{t=0}^{T-1} \mathbb{E} \left[ \frac{1}{2} \norm{\nabla f(\avgtheta^t)}^2 + \frac{L^2}{n} \norm{\Theta_o^t}_F^2 
    % + \norm{ \avgv^t - \avggrdF^t }^2 
    \right] \leq \Delta^0 + \frac{2\eta}{n \brho} \ER^{0} + \frac{2 \eta T}{n \brho} \beta^2 \ConstS \sigma^2 .\label{eq:fin_main_bd} 
\end{equation}
Furthermore, with the initialization, choice of $a,b,c$ and the step size $\gamma \leq \gamma_\infty$, it can be shown that \vspace{-.1cm}
\begin{align} \label{eq:er0}
    \ER^0 \leq \frac{2 \sigma^2}{b_0} + \frac{{\redtmp 118} L^2 n}{{ \rho^2 \gamma^2 (1-\gamma)}} \InitG \eta^2.
\end{align}
Dividing \eqref{eq:fin_main_bd} by $\eta T$ and observing $\norm{\Theta_o^t}_F^2 = \norm{ ( \I - (1/n) {\bf 1} {\bf 1}^\top ) \Theta^t }_F^2$ concludes the proof.

% \begin{align} 
%     & \mathbb{E} \left[ L^2 \norm{\Theta_o^t}_F^2 + n \norm{ \avgv^t - \avggrdF^t }_F^2 \right] \leq (1 - \brho)^t \ER^0 \\
%     & \quad + \frac{\beta^2}{\brho} \ConstS \sigma^2 + \eta^2 \ConstG \sum_{s=0}^{t-1} (1-\brho)^{t-s+1} \mathbb{E} \left[ \norm{\avgg^s}^2 \right] \nonumber
% \end{align}
% Substituting back into \eqref{eq:f_1stepnew} and summing up the latter inequality from $t=0$ to $T-1$ yields{\small 
% \begin{align*}
%     & \frac{\eta}{2} \sum_{t=0}^{T-1} \mathbb{E}\left[ \norm{\nabla f(\avgtheta^t)}^2 + \frac{1}{2} \norm{\avgg^t}^2 \right] - \mathbb{E} \left[ f( \avgtheta^0 ) - f( \avgtheta^T) \right]\\
%     & \leq \frac{\eta \ER^0}{n \brho} + \frac{\beta^2 \eta T}{\brho n} \ConstS \sigma^2 + \frac{\eta^3 \ConstG}{n} \sum_{t=0}^{T-1} \sum_{s=0}^{t-1} (1-\brho)^{t-s+1} \mathbb{E} \left[ \norm{\avgg^s}^2 \right] \\
%     & \leq \frac{\eta \ER^0}{n \brho} + \frac{\beta^2 \eta T}{\brho n} \ConstS \sigma^2 + \frac{\eta^3}{\brho n} \ConstG \sum_{t=0}^{T-1} \mathbb{E} \left[ \norm{\avgg^t}^2 \right]
% \end{align*}}It follows that if $\eta^2 \leq \frac{\brho n}{4 \ConstG}$, then{\small
% \begin{align*}
% & \sum_{t=0}^{T-1} \mathbb{E}\left[ \norm{\nabla f(\avgtheta^t)}^2 \right] \leq \frac{\mathbb{E} \left[ f( \avgtheta^0 ) - f( \avgtheta^T) \right]}{ \eta / 2 } + \frac{2\ER^0}{n \brho} + \frac{2 \beta^2 T}{\brho n} \, \ConstS \sigma^2
% \end{align*}}Furthermore, with the choice of $a,b,c$ in \Cref{lem:wholesys} and the step size $\gamma \leq \gamma_\infty$, it can be shown that
% \begin{align} \label{eq:er0}
%     \ER^0 \leq \frac{2 \sigma^2}{b_0} + \frac{192n}{ \rho^2 \gamma^2 (1-\gamma)} \InitG \eta^2.
% \end{align}
% This concludes the proof of \Cref{th:main}.


\paragraph{Proof of \Cref{cor:pl}} Applying the PL condition of \Cref{ass:pl} to the inequality \eqref{eq:f_1stepnew} shows \vspace{-.1cm}
\begin{align}
    \Delta^{t+1} & \leq (1 - \eta \mu) \Delta^t + \eta \mathbb{E} \left[ \frac{L^2}{n} \norm{\Theta_o^t}_F^2 + \norm{ \avgv^t - \avggrdF^t}^2  \right] - \frac{\eta}{4} \mathbb{E} \left[ \norm{\avgg^t}^2 \right] \notag \\
    & \leq (1 - \eta \mu) \Delta^t + \frac{\eta}{n} \ER^t - \frac{\eta}{4} \mathbb{E} \left[ \norm{\avgg^t}^2 \right]  
\end{align}
Combining with \Cref{lem:wholesys} shows that  \vspace{-.1cm}
\begin{align} 
     \Delta^{t+1} + \frac{2 \eta}{\brho n} \ER^{t+1} & \leq \left( 1 - \bmu \right) \left[ \Delta^t + \frac{2 \eta}{\brho n} \ER^t \right] + \frac{\eta \beta^2}{\brho n} 2 \ConstS \sigma^2 + \left( \frac{\eta^3}{\brho n} 2\ConstG  - \frac{\eta}{4} \right) \mathbb{E} \left[ \norm{\avgg^t}^2 \right],
\end{align}
where we used $1 - \brho + \frac{\brho n}{2 \eta} \frac{\eta}{n} \leq 1 - \min\{ \eta \mu, \brho / 2 \}$. 
Set $\eta^2 \leq \frac{\brho n}{4 \ConstG}$ and telescope the relation concludes the proof.%\vspace{-.2cm}

\section{Numerical Experiments} 
% This section presents numerical experiments to illustrate the efficacy of \aname~for decentralized learning.

\paragraph{Setup}
% In all experiments, we compare \aname~to {\tt GNSD}~\citep{lu2019gnsd}, {\tt DeTAG}~\citep{pmlr-v139-lu21a}, {\tt GT-HSGD}~\citep{xin2021hybrid}, and {\tt CHOCO-SGD}~\citep{koloskova2019decentralized}. 
We run the decentralized optimization algorithms on a 40 threads Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz server with MPI-enabled PyTorch and evaluate the performance of trained models on a Tesla K80 GPU server.
To simulate heterogeneous data distribution, each agent has a disjoint set of training samples, while we evaluate each trained model on all training/testing data. 

\paragraph{Hyperparameter Tuning} 
For all algorithms we choose the learning rate $\eta$ from $\{0.1, 0.01, 0.001\}$, and fix the regularization parameter as $\lambda = 10^{-4}$ [cf.~\eqref{eq:syn_obj}]. For compressed algorithms, we implement the top-$k$ compressor and random quantizer, and we tune the consensus step size $\gamma$ starting from the theoretical value of $\delta$.
% , and then increment at steps of $0.01$ until divergence. 
For {\tt DeTAG}, we adopt the parameters from \citep{pmlr-v139-lu21a}. For \aname~and {\tt GT-HSGD}, we choose the best momentum parameter $\beta$ in $\{0.0001, 0.001, 0.01, 0.1, 0.5, 0.9\}$ and fix the initial batch number as $b_{0,i} = m_i$. We choose the batch sizes such that all algorithms spend the same amount of computation on stochastic gradient per iteration, except for {\tt BEER} which requires large batch size according to \citep{zhao2022beer}. The tuned parameters and additional numerical results can be found in \Cref{app:more_plots}. 
% hypprm tuning
% We halve the batch size of GT-HSGD and \aname

\begin{figure*}[hbtp]
    \centering \vspace{-0.15cm}
    \includegraphics[width=0.95\textwidth]{figures/syn_legend.pdf}\\
    \includegraphics[width=0.238\textwidth]{figures/syn_loss_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/syn_cons_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/syn_loss_sample.pdf}
    \includegraphics[width=0.238\textwidth]{figures/syn_cons_sample.pdf}\\
    \caption{\textbf{Experiments on Synthetic Data with Linear Model.} Worst-agent's train loss value and consensus gap against the number of bits transmitted (left) and total number of samples drawn for gradient approximation (right).
    % (top) and iteration no.~(bottom).
    % Among all agents we plot the worst loss function value and consensus error at each iteration.
    }\label{fig:syn}
    % \vspace{-0.4cm}
\end{figure*}

\paragraph{Synthetic Data with Linear Model}
% seed of synthetic data: 1639016083
% python main.py -num-tasks 25 -num-classes 5 -num-dim 1000
% ./preprocess.sh -s niid --sf 1.0 -k 5 -t sample --tf 0.6
Consider a set of synthetic data generated with the {\tt leaf} benchmarking framework \citep{caldas2019leaf} which provides features from agent-dependent distributions. The task is to train a linear classifier for a set of $d=1000$-dimensional features with $m=1443$ samples partitioned into $n=25$ non-i.i.d.~portions, each held by an agent that is connected to the others on a ring graph with uniform edge weights. Each feature vector is labeled into one of 5 classes. Altogether, the local dataset for the $i$th agent is given by $\{ x_j^i , \{ \ell_{j,k}^i \}_{k=1}^5 \}_{j=1}^{m_i}$, where $m = \sum_{i=1}^{25} m_i$, $x_j^i \in \mathbb{R}^{1000}$ denotes the $j$th feature, and $\{ \ell_{j,k}^i \}_{k=1}^5 \in \{0,1\}^5$ is the label such that $\ell_{j,k}^i = 1$ if the $j$th feature has label $k \in \{1,...,5\}$. 

\vspace{-.0cm}
To train a linear classifier $\theta = ( \theta_1, \ldots, \theta_5 ) \in \mathbb{R}^{5000}$, we consider \eqref{eq:opt} with the following objective function that models a modified logistic regression problem with sigmoid loss and $\ell_2$ regularization:\vspace{-.0cm}
\begin{equation} \label{eq:syn_obj}
    f_i(\theta) = \frac{1}{m_i} \sum_{j=1}^{m_i} \sum_{k=1}^5 \phi \big( \ell_{j,k}^i \dotp{x_j^i}{\theta_k} \big) + \frac{\lambda}{2}\norm{\theta}_2^2,  \vspace{-.0cm}
\end{equation}
where $\phi(z) = (1 + e^{-z})^{-1}$ and $\lambda = 10^{-4}$ is the regularization parameter. The function $f_i(\cdot)$ is not convex, and we estimate its gradient by sampling a mini-batch of data.

Figure~\ref{fig:syn} compares the worst agent's loss values $\max_i f( \theta_i^t)$ and consensus gap $\sum_{i=1}^n \| \theta_i^t - \avgtheta^t \|$ against the communication and gradient computation costs. For compressed algorithms that require communication of two compressed variables (\aname~and {\tt BEER}), we use half the amount of bits/retained non-zeros after sparsification with \eqref{eq:sparsifier} to make a fair comparison with {\tt CHOCO-SGD}.
% For the compressed algorithms {\aname} and {\tt CHOCO-SGD}, we use half the amount of bits/retained non-zeros after sparsification with \eqref{eq:sparsifier} for {\aname} to keep the comparison fair, as {\aname} requires two communication rounds per iteration. 
\aname~achieves the fastest convergence in terms of the communication cost (number of bits transmitted) and shows fast convergence on par with uncompressed algorithms in terms of gradient computation cost, when used with a $4$-bit random quantizer. Comparing among compressed algorithms, \aname~and {\tt BEER} find solutions with the lower consensus gap (10 times lower than {\tt CHOCO-SGD}) and \aname~stands out to be more sample efficient than all existing compressed approaches. Observe that \aname~outperforms {\tt CHOCO-SGD} and {\tt BEER} due to the use of gradient tracking and variance reduction.
% given the same communication budget with random quantizer.  

% \ificmlver
% \else
% \begin{figure*}[t]
%     \vspace{-.5cm}
%     \centering
%     \hspace{-.4cm}
%     \includegraphics[width=0.5\textwidth]{figures/dl_loss_net.pdf}
%     \includegraphics[width=0.5\textwidth]{figures/dl_cons_net.pdf}\vspace{-.2cm}
%     % \includegraphics[width=0.5\textwidth]{figures/dl_loss_it.pdf}
%     % \includegraphics[width=0.5\textwidth]{figures/dl_cons_it.pdf}
%     \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value and consensus gap against the communication cost, i.e., number of 32-bit floats transmitted. Notice the log-scale in the x-axis.}\vspace{-.2cm} \label{fig:real_main}
%     % \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value against the communication cost, i.e., number of 32-bit floats transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_main}
% \end{figure*}
% \fi

\begin{figure*}
    \centering 
    \includegraphics[width=0.95\textwidth]{figures/ffdl_legend_net.pdf}\\
    \includegraphics[width=0.238\textwidth]{figures/ffdl_loss_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/ffdl_train_acc_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/ffdl_cons_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/ffdl_train_grad_net.pdf}
    \caption{\textbf{Experiments on MNIST Data with Feed-forward Network.} Worst-agent's train loss values, train accuracy, consensus gap and train gradient norm against the number of bits transmitted.} \label{fig:real_main}
    % \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value against the communication cost, i.e., number of 32-bit floats transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_main}
\end{figure*}



\paragraph{MNIST Data with Feed-forward Network}
We consider training a 1 hidden layer (with 100 neurons) feed-forward neural network with sigmoid activation function on the MNIST dataset. The samples are partitioned into $n=10$ agents where each agent only {\revision gets} 1 class of samples. These agents are arranged according to a ring topology with uniform edge weights. We tackle \eqref{eq:opt} with $f_i(\theta)$ taken as the cross entropy loss function of the local dataset and an $\ell_2$ regularization is applied with the parameter of $\lambda = 10^{-4}$. 

Figure~\ref{fig:real_main} compares the worst-agent's loss function, $\max_i f( \theta_i^t )$, and other metrics in the same manner against the communication cost (i.e., bits transmitted). We observe that {\aname} already achieved nearly the best performance in loss and accuracy using just a small batch size {\revision of 16}. On the other hand, {\tt CHOCO-SGD} suffered from slower convergence due to the heterogeneity nature of data under the unshuffled MNIST setup, {\revision and the performance of {\tt BEER} is sensitive to the choice of batch sizes}.
Notice that in this experiment, we selected a compression ratio $k/d$ of $0.05$ for \aname~and {\tt BEER}, and $0.1$ for {\tt CHOCO-SGD} for a fair comparison. 



% \begin{figure*}
%     \centering 
%     \includegraphics[width=0.75\textwidth]{figures/ledl_legend_net.pdf}\\
%     \includegraphics[width=0.238\textwidth]{figures/ledl_loss_net.pdf}
%     \includegraphics[width=0.238\textwidth]{figures/ledl_train_acc_net.pdf}
%     \includegraphics[width=0.238\textwidth]{figures/ledl_cons_net.pdf}
%     \includegraphics[width=0.238\textwidth]{figures/ledl_train_grad_net.pdf}
%     \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss values, train accuracy, consensus gap and train gradient norm against the number of bits transmitted.} \label{fig:femnist_main}
%     % \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value against the communication cost, i.e., number of 32-bit floats transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_main}
% \end{figure*}

\begin{figure*}
    \centering 
    \includegraphics[width=0.8\textwidth]{figures/le2dl_legend_net.pdf}\\
    \includegraphics[width=0.238\textwidth]{figures/le2dl_loss_net.pdf}
    % \includegraphics[width=0.238\textwidth]{figures/le2dl_train_acc_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/le2dl_test_acc_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/le2dl_cons_net.pdf}
    \includegraphics[width=0.238\textwidth]{figures/le2dl_train_grad_net.pdf}
    \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's train loss values, test accuracy, consensus gap and train gradient norm against the number of MBytes transmitted.} \label{fig:femnist_main}
\end{figure*}

\paragraph{FEMNIST Data with LeNet-5}
Lastly, we consider training the LeNet-5 (with $d=60850$ parameters) neural network on the FEMNIST dataset. The dataset contains $m=805263$ samples of $28 \times 28$ hand-written character images, each belonging to one of the 62 classes. The samples are partitioned into $n=36$ agents according to the groups specified in \citep{caldas2019leaf}. These agents are arranged according to a ring topology with uniform edge weights.
% We tackle \eqref{eq:opt} with $f_i(\theta)$ taken as the cross entropy loss function of the local dataset and an $\ell_2$ regularization is applied with the parameter of $\lambda = 10^{-4}$. 
% We scale the learning rate by 0.1 at the $\{20, 30, 40, 50, 60 \}$-th epoch; see \Cref{app:hypprm_lenet} in the appendix for the hyperparameters values.\vspace{-.1cm}
We scale the learning rate $\eta$ by 0.1 at the {\revision $\{4, 40, 80\}$-th} thousand iteration and momentum $\beta$ by 0.1 at the {\revision $\{10, 40, 80 \}$-th} thousand iteration {\revision for {\aname}}; see \Cref{app:hypprm_lenet} in the appendix for the hyperparameters values.\vspace{-.1cm}
% learning rate is scaled by 0.1 twice during training.

{\revision
Denote the LeNet-5 classifier ${\bf g} (x; \theta): \mathbb{R}^{28\times 28} \rightarrow \mathbb{R}^{62}$ which is parameterized by the weights vector $\theta \in \mathbb{R}^{66126}$. We optimize \eqref{eq:opt} with cross-entropy loss and $\ell_2$ regularization such that $f_i$ is defined over the local dataset $\{(x_j^i,y_j^i) \}_{j=1}^{m_i}$ of (image, class) pairs by
\begin{equation}
    f_i(\theta) = - \frac{1}{m_i} \sum_{j=1}^{m_i} \log\left(\frac{\exp( [{\bf g} (x_j^i; \theta) ]_{y_j^i})}{\sum_{k=1}^{62} \exp( [ {\bf g} (x_j^i; \theta) ]_k )} \right) + \frac{\lambda}{2} \| \theta\|_2^2,
\end{equation}
where $x_j^i \in \mathbb{R}^{28 \times 28}$, $y_j^i \in \{1, \ldots ,62\}$.
}

Figure \ref{fig:femnist_main} compares the performance of benchmarked algorithms against the communication cost. We observe that {\aname} achieves similar performance as {\tt CHOCO-SGD} and {\tt BEER}, while demonstrating a slightly better performance in terms of the consensus gap. 
We speculate that the performance gap has narrowed due to the highly non-smooth nature of LeNet-5. 


% We observe that the communication efficiency gap between the compressed and uncompressed algorithms has widened, in which \aname~and {\tt CHOCO-SGD} can achieve the same level of loss values with 10-20x less communication cost. {Moreover, \aname~has similar performance as {\tt CHOCO-SGD} in terms of the training loss, and it yields a better consensus gap than {\tt CHOCO-SGD} with the same communication budget.}
% Notice that in this experiment, we selected a compression ratio $k/d$ of $0.05$ and $0.1$ for \aname~and {\tt CHOCO-SGD} for a fair comparison.\vspace{-.1cm} 

% \begin{figure}[!b]
%     \centering \vspace{-.6cm}
%     \includegraphics[width=0.425\textwidth]{figures/dl_loss_net.pdf}\vspace{-0.2cm}
%     \includegraphics[width=0.425\textwidth]{figures/dl_cons_net.pdf}\vspace{-.1cm}
%     % \includegraphics[width=0.5\textwidth]{figures/dl_loss_it.pdf}
%     % \includegraphics[width=0.5\textwidth]{figures/dl_cons_it.pdf}
%     \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value and consensus gap against the number of bits transmitted. Note the log-scale x-axis.} \label{fig:real_main}
%     % \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value against the communication cost, i.e., number of 32-bit floats transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_main}
% \end{figure}



\section{Conclusions} We have proposed the \aname~algorithm for communication efficient decentralized learning and shown that the algorithm achieves a state-of-the-art ${\cal O}(\epsilon^{-3})$ sampling complexity. Future works include investigating the effect of reducing the frequency of (compressed) communication. For example, through considering asynchronous updates with possibly time varying or random graph.

% For neural network, we train LeNet-5 on FEMNIST~\citep{caldas2019leaf}, a set of $28\times28$ hand-written character images with heterogeneous distribution. FEMNIST comes with 805,263 samples in 62 classes, and we optimize LeNet-5 on a network of 36 agents in undirected ring topology.

\bibliography{ref}
\bibliographystyle{tmlr}

\appendix
\section{Missing Proofs from \Cref{sec:pf}}
Using the matrix notations defined in the preface of \Cref{sec:pf}, we observe that \aname~\eqref{eq:algo} can be  expressed conveniently as
\begin{equation*}
\begin{array}{ll}
    \Theta^{t+1} & = \Theta^t - \eta G^t + \gamma ( {\bf W} - {\bf I} ) \hatTheta^{t+1} \\
    \hatTheta^{t+1} & = \hatTheta^t + {\cal Q} ( \Theta^t - \eta G^t - \hatTheta^t ) \\
    V^{t+1} & = \beta \stocgrdF^{t+1} + (1-\beta) ( V^t + \stocgrdF^{t+1} - \stocgrdFp^{t} ) \\
    G^{t+1} & = G^t + V^{t+1} - V^t + \gamma ( {\bf W} - {\bf I} ) \hatG^{t+1} \\
    \hatG^{t+1} & = \hatG^t + {\cal Q} ( G^t + V^{t+1} - V^t - \hatG^t )
\end{array}~~
, {\cal Q}( X ) = \left( 
    \begin{array}{c}
        {\cal Q}( x_1 )^\top \\ 
        \vdots \\
        {\cal Q}( x_n )^\top
    \end{array}
\right),~~\forall~X \in \mathbb{R}^{n \times d}.
\end{equation*}
The above simplified expression of the algorithm will be useful for our subsequent analysis.

\iffalse 
\subsection{Bounds on $\norm{G_o^t}_F^2$, $\norm{\Theta^t - \eta G^t - \hatTheta^t}_F^2$ and $\norm{ G^{t} - \hatG^{t} }_F^2$} \label{app:control_GoTheta}

\begin{lemma}\label{lem:gt_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$, $\gamma \leq \frac{1}{8 \bw}$. For any $t \geq 0$, it holds
    \begin{align*}
    \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{4} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hatG^t }_F^2 \right] + \gamma \, \frac{25 L^2 \bw^2}{\rho} \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
    & \quad + \gamma \, \frac{13 L^2}{ \rho } \bw^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] \nl 
    & \quad + \gamma \, \frac{ \rho n}{5} \, \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] + \frac{7}{ \rho \gamma } \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + \frac{7n }{\rho \gamma} \beta^2 \sigma^2 .
    \end{align*}
\end{lemma}
\begin{lemma} \label{lem:thetahat_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\eta \leq \min \{ \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} },  \frac{1}{4L} \}$,
$\gamma^2 \leq \min \{\frac{\delta}{ 16 \bw^2 (1-\delta) (1 + 3\eta^2 L^2) (1 + 2/\delta)}, \frac{1}{\rho^2}, \frac{\delta^2}{64\bw^2}\}$, $\eta^2 \gamma \leq \frac{\delta^2 \rho}{1248 \bw^2 L^2}$. For any $t \geq 0$, it holds
    \begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hatTheta^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{8} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hatTheta^t}_F^2 \right] \nl 
        & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \frac{3 \bw}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hatG^t}_F^2 \right] \nl 
        & \quad + \left[ \frac{29}{\delta} \bw^2 \gamma^2 + \frac{38 L^2 \bw \eta^2}{\rho} \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \frac{18 \eta^2}{\delta} \, n \, \mathbb{E}  \left[ \norm{ \avgg^t }^2 \right] \nl 
        & \quad + \left( 18 + \frac{84}{\rho\gamma} \right)\frac{ \beta^2\eta^2 }{\delta} \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \left( 18 + \frac{84}{\rho \gamma} \right) \frac{\beta^2 \eta^2 n\sigma^2}{\delta} 
    \end{align}
\end{lemma}
\begin{lemma} \label{lem:gtghat_new} Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions $\gamma \leq \frac{\delta}{8 \bw}$, $\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$. For any $t \geq 0$, it holds
    \begin{align*}
    \mathbb{E} \left[ \norm{ G^{t+1} - \hatG^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hatG^{t} }_F^2 \right] + \frac{10}{\delta} \gamma^2 \left( \bw^2 + \frac{\rho^2 }{8} \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
    & \quad + \gamma^2 \, \frac{122L^2\bw^2}{\delta} \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \gamma^2 \, \frac{60 L^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] \nl
    & \quad + \gamma^2 \, \frac{3 \rho^2}{5 \delta} n \mathbb{E} \left[ \norm{ \avgg^t }^2 \right]  + \frac{31}{\delta} \beta^2 \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \frac{31}{\delta} \beta^2 n \sigma^2 .
    \end{align*}
\end{lemma}
\fi 

% The proofs can be found in \Cref{app:gt_new}, \ref{app:thetahat_new}, \ref{app:gtghat_new}.

\subsection{Proof of \Cref{lem:f_1stepnew}} \label{app:f1step}
Using the $L$-smoothness of $f$ [cf.~\Cref{ass:lips}], we obtain:
\begin{align}
    f(\bar{\theta}^{t+1}) &\le f(\bar{\theta}^t) + \dotp{\nabla f(\bar{\theta}^t)}{\bar{\theta}^{t+1} - \bar{\theta}^t} + \frac{L}{2} \norm{\avgtheta^{t+1} - \avgtheta^t}^2
    \nl & = f(\avgtheta^t) - \eta \dotp{\nabla f(\avgtheta^t)}{\avgg^t} + \frac{L \eta^2}{2} \norm{\avgg^t}^2
    \nl &= f(\avgtheta^t) - \frac{\eta}{2}\left(\norm{\avgg^t}^2 + \norm{\nabla f(\avgtheta^t)}^2 - \norm{\avgg^t - \nabla f(\avgtheta^t)}^2\right) + \frac{L\eta^2}{2} \norm{\avgg^t}^2
    \nl &\stackrel{(a)}{\leq} f(\avgtheta^t) - \frac{\eta}{4}\norm{\avgg^t}^2 - \frac{\eta}{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{\eta}{2}\norm{\avgg^t - \nabla f(\avgtheta^t)}^2
    \nl &\leq f(\avgtheta^t) - \frac{\eta}{4}\norm{\avgg^t}^2 - \frac{\eta}{2}\norm{\nabla f(\avgtheta^t)}^2 + \eta\left(\norm{\avgg^t - \avggrdF^t}^2 + \norm{\avggrdF^t - \nabla f(\avgtheta^t)}^2 \right)
    \nl &\leq f(\avgtheta^t) - \frac{\eta}{4}\norm{\avgg^t}^2 - \frac{\eta}{2}\norm{\nabla f(\avgtheta^t)}^2 + \eta\norm{\avgg^t - \avggrdF^t}^2 + \frac{L^2\eta}{n}\norm{\Theta^t - \avgTheta^t}_F^2
\end{align}
where (a) is due to $\eta\leq \frac{1}{2L}$. We remark that $\dotp{x}{y} = x^\top y$ denotes the inner product between the vectors $x,y$.

Note that by construction and the initialization $v_i^0 = g_i^0$, we have $\avgg^t = \avgv^t$ for any $t\ge0$; see \eqref{eq:docom_d}. Applying the upper bound 
\begin{align}
\norm{\Theta^t - \avgTheta^t}_F^2 = \norm{ ( \I - (1/n) {\bf 1}{\bf 1}^\top) \Theta^t }_F^2 = \norm{ \U \U^\top \Theta^t }_F^2 \leq \norm{\Theta^t_o}_F^2
\end{align}
leads to \Cref{lem:f_1stepnew}. 

\subsection{Proof of \Cref{lem:theta_o_new}}\label{app:theta_o_new}
Observe that
\begin{align}
    \Theta^{t+1}_o &= {\bf U}^\top(\Theta^t - \eta G^t + \gamma ( \W - \I) \hat{\Theta}^{t+1}) \nonumber\\
    &= {\bf U}^\top\left\{\Theta^t - \eta G^t + \gamma ( \W - \I)\left[\hat{\Theta}^t + {\cal Q} (\Theta^t - \eta G^t - \hat{\Theta}^t ) -\Theta^t + \eta G^t + \Theta^t - \eta G^t\right]\right\} \nonumber\\
    &= {\bf U}^\top\left\{\left[I + \gamma (\W-\I)\right](\Theta^t - \eta G^t) + \gamma(\W-\I)\left[ {\cal Q} (\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t - \hat{\Theta}^t)\right] \right\} \label{eq:sub1}
\end{align}
Notice that it holds $\U^\top ( I + \gamma ( \W - \I ) ) = {\bf U}^\top ( I + \gamma(\W-\I) ) \U \U^\top$ and $\norm{\U^\top(I + \gamma (\W-\I)) \U} \leq 1 - \rho \gamma$. 
% Notice the following trick on $U^T\left[I + \gamma (\W-\I)\right]$,
% \begin{align}
%     U^T\left[I + \gamma (\W-\I)\right] &=U^T\left[I + \gamma (\W-\I)\right](\avgone + (I - \avgone)) \nonumber\\
%     &=U^T\left[(I + \gamma (\W-\I)) (\avgone) + (I + \gamma (\W-\I))UU^T \right] \nonumber\\
%     &\stackrel{\eqref{eq:W_stochastic}}{=} U^T\left[ \avgone + 0 + (I + \gamma (\W-\I))UU^T \right] \nonumber\\
%     &\stackrel{\eqref{eq:U_null}}{=}U^T(I + \gamma (\W-\I))UU^T \label{eq:UUT_trick}
% \end{align}
% \begin{align}
%     \norm{U^T(I + \gamma (\W-\I))U} &= \norm{(1-\gamma)U^TU + \gamma U^TWU }
%     \nl &\leq \norm{(1-\gamma)U^TU} + \norm{\gamma U^TWU}
%     \nl &\stackrel{\eqref{eq:U_norm}\eqref{eq:W_rho}}{\leq} 1-\gamma + \gamma(1-\rho)
%     \nl &= 1- \rho\gamma
%     \label{eq:UIWU_norm}
% \end{align}
Taking the Frobenius norm on \eqref{eq:sub1} and the conditional expectation $\mathbb{E}_t [ \cdot ]$ on the randomness in \aname~up to the $t$th iteration:
\begin{align}
    & \mathbb{E}_t [ \norm{\Theta^{t+1}_o}^2_F ] \\ 
    & = \mathbb{E}_t \left[ \norm{\U^\top(I + \gamma (\W-\I)) \U \U^\top(\Theta^t - \eta G^t) + \gamma \U^\top (\W-\I) \left[ {\cal Q} (\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t - \hat{\Theta}^t)\right]}^2_F \right] \nonumber \\
    &\leq (1+\alpha)\norm{\U^\top(I + \gamma (\W-\I)) \U (\Theta^t_o - \eta G^t_o)}^2_F  \nonumber \\ 
    &\quad + (1+\alpha^{-1}) \mathbb{E}_t \left[ \norm{\gamma \U^\top (\W-\I) \left[ {\cal Q} (\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t - \hat{\Theta}^t)\right]}^2_F  \right]\nonumber \\
    &\leq (1+\alpha) \norm{\U^\top(I + \gamma (\W-\I)) \U}^2 \norm{\Theta^t_o - \eta G^t_o}_F^2  \nonumber \\
    &\quad +  (1+\alpha^{-1})\gamma^2 \norm{\U^\top(\W-\I)}^2 \mathbb{E}_t \left[ \norm{ {\cal Q} (\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t - \hat{\Theta}^t)}^2_F \right] \nonumber  \\
    & \overset{(a)}{\leq} (1+\alpha) (1-\rho\gamma)^2 \norm{\Theta^t_o - \eta G^t_o}_F^2 + (1+\alpha^{-1})\bw^2 \gamma^2 (1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}^2_F  \nonumber \\
    &\leq (1+\alpha)(1-\rho \gamma)^2(1+\beta) \norm{\Theta^t_o}_F^2 + (1+\alpha)(1-\rho \gamma)^2(1+\beta^{-1})\eta^2 \norm{G^t_o}_F^2 \nonumber \\
    & \quad + \bw^2 \gamma^2(1+\alpha^{-1})(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}^2_F  \nonumber \\
    % &\text{(For contraction of $\norm{\Theta_0^t}_F^2$, we choose $\alpha  = \frac{\rho \gamma}{1-\rho \gamma}$, $\beta = \frac{ \rho \gamma }{2}$ ),} \nonumber \\
    & \overset{(b)}{\leq} (1-\frac{\rho\gamma}{2}) \norm{\Theta_o^t}_F^2
    + \frac{2}{\rho \gamma} \eta^2 \norm{G_o^t}_F^2 \nonumber
    + \frac{\bw^2 \gamma}{\rho} \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}^2_F \nonumber
    \label{eq:Theta_0_norm_summary}
\end{align}
where (a) is due to $\norm{ \W - \I } \leq \bw$ and (b) is due to the choices $\alpha  = \frac{\rho \gamma}{1-\rho \gamma}$, $\beta = \frac{ \rho \gamma }{2}$.
The proof is completed.

\subsection{Proof of \Cref{lem:vt_bound}} \label{app:vt_bound}
Defining $\avgstocgrdF^{t} = n^{-1} {\bf 1}^\top \stocgrdF^t, \avgstocgrdFp^{t} = n^{-1} {\bf 1}^\top \stocgrdFp^t$, we get
\begin{align}
    \avgv^{t+1} - \avggrdF^{t+1} & = \avgstocgrdF^{t+1} + (1-\beta) \big( \avgv^t - \avgstocgrdFp^t \big) - \avggrdF^{t+1} \\
    & = (1-\beta) \big( \avgv^t - \avggrdF^t \big) - \beta ( \avggrdF^{t+1} - \avgstocgrdF^{t+1} ) \nl
    & \quad + (1-\beta) \big( \avggrdF^t - \avgstocgrdFp^{t} - ( \avggrdF^{t+1} - \avgstocgrdF^{t+1} ) \big) \nonumber
\end{align}
It follows that 
\begin{align*}
    \mathbb{E} \left[ \norm{ \avgv^{t+1} - \avggrdF^{t+1} }^2 \right] & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ \avgv^{t} - \avggrdF^{t} }^2 \right] + 2 \beta^2 \mathbb{E} \left[ \norm{\avggrdF^{t+1} - \avgstocgrdF^{t+1} }^2 \right] \\
    & \quad + 2(1-\beta)^2 \mathbb{E} \left[ \norm{ \avggrdF^t - \avgstocgrdFp^{t} - ( \avggrdF^{t+1} - \avgstocgrdF^{t+1} ) }^2 \right] \\
    & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ \avgv^{t} - \avggrdF^{t} }^2 \right] + 2 \beta^2 \frac{\sigma^2}{n} + 2(1-\beta)^2 \frac{L^2}{n^2} \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }^2 \right]
\end{align*}
Furthermore, applying \Cref{lem:theta_1stepnew} leads to 
\begin{equation}
\begin{split}
    \mathbb{E} \left[ \norm{ \avgv^{t+1} - \avggrdF^{t+1} }^2 \right] & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ \avgv^{t} - \avggrdF^{t} }^2 \right]  + 2 \beta^2 \frac{\sigma^2}{n} \\
    & \quad + 8 (1-\beta)^2 \frac{L^2}{n^2} \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \\
    & \quad + 4n (1-\beta)^2 \frac{L^2}{n^2}  \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \\
    & \quad + 8 (1-\beta)^2 \frac{L^2}{n^2} \bw^2 \gamma^2 \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \\
    & \quad + 4 (1-\beta)^2 \frac{L^2}{n^2} \bw^2 \gamma^2(1-\delta) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right]
\end{split}
\end{equation}
This concludes our proof for the stated lemma. 

\paragraph{Bound on the Matrix Form} Observe that 
\begin{align}
    V^{t+1} - \grdF^{t+1} & = \stocgrdF^{t+1} + (1-\beta) \big( V^t - \stocgrdFp^{t} \big) - \grdF^{t+1} \\
    & = (1-\beta) \big( V^t - \grdF^t \big) - \beta ( \grdF^{t+1} - \stocgrdF^{t+1} ) \nl
    & \quad + (1-\beta) \big( \grdF^t - \stocgrdFp^t - ( \grdF^{t+1} - \stocgrdF^{t+1} ) \big) \nonumber
\end{align}
Taking the full expectation yields 
% ({\blu apply (27) in GT-HSGD, should we add the details, i.e. \newline $\mathbb{E}\left[\norm{\stocF{t+1}{t+2} - \stocF{t}{t+2}\right] - \mathbb{E}\left[\stocF{t+1}{t+2} - \stocF{t}{t+2}\right]}^2] \leq \mathbb{E}\left[ \norm{\stocF{t+1}{t+2} - \stocF{t}{t+2}}^2 \right]$, \newline or quote (27) in GT-HSGD?. }) 
% {\re need to redefine L as smoothness for batch?}
\begin{align*}
    \mathbb{E} \left[ \norm{ V^{t+1} - \grdF^{t+1} }_F^2 \right] & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ V^{t} - \grdF^{t} }_F^2 \right] + 2 \beta^2 n \sigma^2 + 2 (1-\beta)^2 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right]
\end{align*}
Again, applying \Cref{lem:theta_1stepnew} leads to 
\begin{equation} \label{eq:vtmatform} 
\begin{split}
    \mathbb{E} \left[ \norm{ V^{t+1} - \grdF^{t+1} }_F^2 \right] & \leq (1-\beta)^2 \mathbb{E} \left[ \norm{ V^{t} - \grdF^{t} }_F^2 \right] + 2 \beta^2 n \sigma^2 \\
    & \quad + 8 (1-\beta)^2 L^2 \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \\
    & \quad + 4n (1-\beta)^2 L^2 \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \\
    & \quad + 8 (1-\beta)^2 L^2 \bw^2 \gamma^2 \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \\
    & \quad + 4 (1-\beta)^2 L^2 \bw^2 \gamma^2(1-\delta) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right]
\end{split}
\end{equation}
This concludes our proof.

\subsection{Proof of \Cref{lem:gt_new}} \label{app:gt_new}
We begin by observing the update for $G_o^{t+1}$ as:
\begin{align*}
G^{t+1}_o &= \U^\top[G^t + V^{t+1} - V^t +\gamma(\W-\I)\hat{G}^{t+1}] \\
& = \U^\top \left[ G^t + V^{t+1} - V^t +\gamma(\W-\I) \left( \hat{G}^t + {\cal Q}( G^t + V^{t+1} - V^t - \hat{G}^t ) \right) \right] \\
& = \U^\top \left[ \left( I + \gamma(\W-\I) \right) ( G^t + V^{t+1} - V^t ) \right] \\
& \quad + \gamma \U^\top (\W-\I) \left[ {\cal Q}( G^t + V^{t+1} - V^t - \hat{G}^t ) - (G^t + V^{t+1} - V^t - \hat{G}^t) \right]
\end{align*}
The above implies that 
\begin{align*}
\mathbb{E}_t \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq (1 + \alpha_0) ( 1 - \rho \gamma )^2 \mathbb{E}_t \left[ \norm{ G_o^t + \U^\top ( V^{t+1} - V^t ) }_F^2 \right] \\ 
& \quad + (1 + \alpha_0^{-1}) \gamma^2 \bw^2 (1 - \delta) \mathbb{E}_t \left[ \norm{ G^t + V^{t+1} - V^t - \hat{G}^t }_F^2 \right] \\
& \leq (1 + \alpha_0) ( 1 - \rho \gamma )^2 \mathbb{E}_t \left[ (1 + \alpha_1) \norm{ G_o^t }_F^2 + (1 + \alpha_1^{-1}) \norm{ V^{t+1} - V^t }_F^2 \right] \nl 
& \quad + 2 (1 + \alpha_0^{-1}) \gamma^2 \bw^2 (1 - \delta) \mathbb{E}_t \left[ \norm{ G^t - \hat{G}^t }_F^2 + \norm{ V^{t+1} - V^t }_F^2 \right]
\end{align*}
Taking $\alpha_0 = \frac{ \rho \gamma }{ 1 - \rho \gamma }$, $\alpha_1 = \frac{\rho \gamma}{2}$ gives
\begin{align*}
\mathbb{E}_t \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} \right) \norm{ G_o^t }_F^2 + \gamma \frac{ 2 \bw^2}{ \rho } \norm{ G^t - \hat{G}^t }_F^2 \nl 
& \quad + \frac{2}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) \mathbb{E}_t \left[ \norm{ V^{t+1} - V^t }_F^2 \right]
\end{align*}
Taking the full expectation and applying \Cref{lem:vt_diffnew} give
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] \nl 
& \quad + \frac{2}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) \left( 3 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + 3 n \beta^2 \sigma^2 \right)
\end{align*}
Furthermore, applying \Cref{lem:theta_1stepnew} yields
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} + \eta^2 \frac{ 24 L^2 (1 + \gamma^2 \bw^2) ( 1 - \rho \gamma)^2 }{ \rho \gamma } \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] \nl 
& \quad + \frac{24 L^2 (1 + \gamma^2 \bw^2)}{\rho \gamma} \bw^2 \gamma^2 \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
& \quad + \frac{12 L^2(1 + \gamma^2 \bw^2)}{ \rho \gamma } \bw^2 \gamma^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] \nl 
& \quad + \frac{12 L^2 (1 + \gamma^2 \bw^2)}{\rho \gamma} (1 - \rho \gamma)^2 n \, \eta^2 \, \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \nl 
& \quad + \frac{6 (1 + \gamma^2 \bw^2 )}{ \rho \gamma } \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] \nl
& \quad + \frac{6}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) \beta^2 n \sigma^2 
\end{align*}
The step size condition 
\begin{equation} \label{eq:eta_gcond}
\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }, \quad \gamma \leq \frac{1}{8 \bw}
\end{equation}
implies that 
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{4} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] + \gamma \, \frac{25 L^2 \bw^2}{\rho} \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
& \quad + \gamma \, \frac{13 L^2}{ \rho } \bw^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] + \gamma \, \frac{ \rho n}{5} \, \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \\
& \quad + \frac{7}{ \rho \gamma } \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + \frac{7n }{\rho \gamma} \beta^2 \sigma^2 .
\end{align*}
This concludes our proof.

\subsection{Proof of \Cref{lem:thetahat_new}} \label{app:thetahat_new}
Observe that 
\begin{align}
    & \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] = \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- (\hat{\Theta}^t + {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ))}_F^2 \right] 
    \nl &= \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- (\Theta^t - \eta G^t) + (\Theta^t - \eta G^t - \hat{\Theta}^t) - {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t )}_F^2 \right]
    \nl & \leq (1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \Theta^t - \eta(G^{t+1}- G^t)}_F^2 \right] + (1+\frac{\delta}{2})(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2
    \nl &\leq 2(1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \Theta^t}_F^2 \right] + 2\eta^2 (1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{G^{t+1}- G^t}_F^2 \right]
     + (1-\frac{\delta}{2})\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 
     \label{int:theta_comm_a}
\end{align}
Note that as 
\begin{align*}
G^{t+1} - G^t & = G^{t+1} - {\bf 1} (\avgg^{t+1})^\top + {\bf 1} (\avgg^{t+1})^\top - \big[ G^{t} - {\bf 1} (\avgg^{t})^\top + {\bf 1} (\avgg^{t})^\top \big] \\
& = \U G_o^{t+1} - \U G_o^t + {\bf 1} ( \avgg^{t+1} - \avgg^t )^\top,
\end{align*}
we obtain the bound 
\begin{align}
    \mathbb{E} \left[ \norm{G^{t+1} - G^t}_F^2 \right] & \leq \frac{1}{n} \mathbb{E} \left[ \norm{ {\bf 1}^\top ( V^{t+1} - V^t )}^2 \right] + 2 \mathbb{E} \left[ \norm{ G_o^{t+1} }_F^2 \right] + 2 \mathbb{E} \left[ \norm{ G_o^{t} }_F^2 \right] \nonumber
\end{align}
% Observe that by \Cref{lem:vt_diffnew}
% \begin{align}
% \mathbb{E} \left[ \norm{ \avgv^{t+1} - \avgv^t }^2 \right] & \leq 3\frac{L^2}{n} \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{ \avgv^t - \avggrdF^t }^2 \right] + 3 \beta^2 \sigma^2 \nonumber 
% \end{align}

With \Cref{lem:vt_diffnew}, we substitute back into \eqref{int:theta_comm_a} and obtain
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq (1-\frac{\delta}{2}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + 4 \eta^2 ( 1 + \frac{2}{\delta}) \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 + \norm{G_o^t}_F^2 \right] \nl 
    & \quad + 2(1 + \frac{2}{\delta}) ( 3 \eta^2 L^2 + 1 ) \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 
    \nl &\quad 6\beta^2\eta^2(1+\frac{2}{\delta}) \mathbb{E} \left[ \norm{ V^t - \grdF^t }^2 \right]  + 6 \beta^2 \eta^2  (1 + \frac{2}{\delta})n\sigma^2
\end{align}
We further apply \Cref{lem:theta_1stepnew} to obtain
\begin{align}
    & \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] \leq (1-\frac{\delta}{2}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + 4 \eta^2 ( 1 + \frac{2}{\delta}) \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 + \norm{G_o^t}_F^2 \right] \nl 
    & \quad + 6\beta^2\eta^2(1+\frac{2}{\delta}) \mathbb{E} \left[ \norm{ V^t - \grdF^t }^2 \right] + 6 \beta^2 \eta^2  (1 + \frac{2}{\delta}) n\sigma^2 + 8 (1 + \frac{2}{\delta}) ( 3 \eta^2 L^2 + 1 ) \eta^2 (1 - \rho \gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
    & \quad + 8 (1 + \frac{2}{\delta}) ( 3 \eta^2 L^2 + 1 ) \bw^2 \gamma^2 \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + 4 (1 + \frac{2}{\delta}) ( 3 \eta^2 L^2 + 1 ) n \eta^2 (1 - \rho \gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \nl 
    & \quad + 4 (1 + \frac{2}{\delta}) ( 3 \eta^2 L^2 + 1 ) \bw^2 \gamma^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] \label{int:tx_int}
\end{align}
Using the step size condition:
\[ 
\gamma^2 \leq \frac{\delta}{ 16 \bw^2 (1-\delta) (1 + 3\eta^2 L^2) (1 + 2/\delta)} 
\]
and we recall that $\eta \leq 1/(4L)$, the upper bound in \eqref{int:tx_int} can be simplified as 
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq (1-\frac{\delta}{4}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] +  \frac{12}{\delta} \eta^2 \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] + \frac{18}{\delta}\beta^2\eta^2 \mathbb{E} \left[ \norm{ V^t - \grdF^t }^2 \right] \nl 
    & \quad + \frac{18}{\delta} \beta^2 \eta^2 n\sigma^2 + \frac{41}{\delta} \eta^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{29}{\delta} \bw^2 \gamma^2  \mathbb{E} \left[  \norm{\Theta_o^t}_F^2 \right] +   \frac{15}{\delta}  \eta^2 n\, \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \nonumber
\end{align}
The above bound can be combined with \Cref{lem:gt_new} and $\gamma \rho \leq 1$ to give
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{4} + \eta^2 \gamma \frac{156 \bw^2 L^2 (1-\delta)}{\rho \delta} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] \nl 
    & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \gamma \frac{24 \bw^2}{\delta \rho} \mathbb{E} \left[ \norm{G^{t} - \hat{G}^t}_F^2 \right] \nl 
    & \quad + \left[ \frac{29}{\delta} \bw^2 \gamma^2 + \frac{300 L^2 \bw^2 \eta^2 \gamma}{\rho \delta} \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \frac{18 \eta^2}{\delta} \, n \, \mathbb{E}  \left[ \norm{ \avgg^t }^2 \right] \nl 
    & \quad +  \left( 18 + \frac{84}{\rho\gamma} \right)\frac{ \beta^2\eta^2 }{\delta} \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \left( 18 + \frac{84}{\rho \gamma} \right) \frac{\beta^2 \eta^2 n\sigma^2}{\delta} \nonumber
\end{align}
Taking  $\eta^2 \gamma \leq \frac{\delta^2 \rho}{1248 (1-\delta)\bw^2 L^2}$ and $\gamma \leq \frac{\delta}{8\bw}$ simplifies the bound into
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{8} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] \nl 
    & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \frac{3 \bw}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hat{G}^t}_F^2 \right] \nl 
    & \quad + \left[ \frac{29}{\delta} \bw^2 \gamma^2 + \frac{38 L^2 \bw \eta^2}{\rho} \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \frac{18 \eta^2}{\delta} \, n \, \mathbb{E}  \left[ \norm{ \avgg^t }^2 \right] \nl 
    & \quad + \left( 18 + \frac{84}{\rho\gamma} \right)\frac{ \beta^2\eta^2 }{\delta} \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \left( 18 + \frac{84}{\rho \gamma} \right) \frac{\beta^2 \eta^2 n\sigma^2}{\delta} \nonumber
\end{align}
This concludes the proof.

\subsection{Proof of \Cref{lem:gtghat_new}} \label{app:gtghat_new}
We begin by observing the following recursion for $G^{t} - \hat{G}^t$:
\begin{align*}
G^{t+1} - \hat{G}^{t+1} & = G^t + V^{t+1} - V^t + ( \gamma (\W-\I) - I ) \hat{G}^{t+1} \nl 
& = \gamma (\W-\I) ( G^t + V^{t+1} - V^t ) \nl 
& \quad + ( \gamma (\W-\I) - I ) \left[ {\cal Q}( G^t + V^{t+1} - V^t - \hat{G}^t ) - (G^t + V^{t+1} - V^t - \hat{G}^t) \right] .
\end{align*}
This implies
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq (1 + \alpha_0) (1 + \gamma \bw)^2 (1 - \delta) \mathbb{E} \left[ (1+\alpha_1) \norm{ G^t- \hat{G}^t}_F^2 + (1 + \alpha_1^{-1}) \norm{ V^{t+1} - V^t }_F^2 \right] \nl 
& \quad + 2 (1 + \alpha_0^{-1}) \gamma^2 \bw^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 + \norm{ V^{t+1} - V^t }_F^2 \right] 
\end{align*}
Taking $\alpha_0 = \frac{\delta}{4}$, $\alpha_1 = \frac{\delta}{8}$ and the step size condition
\[
\gamma \leq \frac{ \delta }{ 8\bw } \leq \frac{ \delta }{ 6 \bw }
\]
give
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl
& \quad + \left( (1 - \frac{\delta}{4})(1 + \frac{8}{\delta}) + 2 \gamma^2 \bw^2 (1 + \frac{4}{\delta} ) \right) \mathbb{E} \left[ \norm{ V^{t+1} - V^t }_F^2 \right] \nl 
& \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{10(1+\gamma^2\bw^2)}{\delta} \mathbb{E} \left[ \norm{ V^{t+1} - V^t }_F^2 \right] 
\end{align*}
Applying \Cref{lem:vt_diffnew} and \Cref{lem:theta_1stepnew} gives
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{31}{\delta} \beta^2 \left( \mathbb{E} \left[ \norm{ V^t - \grdF^t}_F^2 \right] + n \sigma^2 \right) \nl
& \quad + \frac{30 (1+\gamma^2 \bw^2) L^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] \nl 
& \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10}{\delta} \left(  \gamma^2 \bw^2 + 12 \eta^2 L^2 (1 + \gamma^2 \bw^2) (1 - \rho \gamma)^2 \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
& \quad + \frac{120 ( 1+\gamma^2\bw^2) L^2}{\delta} \bw^2\gamma^2\mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \nl
& \quad + \frac{60 ( 1+\gamma^2\bw^2) L^2}{\delta} \bw^2 \gamma^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] \nl
& \quad + \frac{60 ( 1+\gamma^2\bw^2) L^2}{\delta} n \eta^2 (1 - \rho\gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right] \nl
& \quad + \frac{31}{\delta} \beta^2 \left( \mathbb{E} \left[ \norm{ V^t - \grdF^t}_F^2 \right] + n \sigma^2 \right)
\end{align*}
Using the step size condition from \eqref{eq:eta_gcond}, i.e., $\eta^2 L^2 (1-\rho \gamma)^2 (1+\gamma^2 \bw^2) \leq \frac{ \rho^2 \gamma^2 }{ 100 }$, and $\gamma\leq\frac{\delta}{8\bw}$ simplifies the above to 
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10}{\delta} \gamma^2 \left( \bw^2 + \frac{\rho^2 }{8} \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
& \quad + \gamma^2 \,\frac{122L^2\bw^2}{\delta} \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \gamma^2 \, \frac{60 L^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{G}^t }_F^2 \right] \nl
& \quad + \gamma^2 \, \frac{3 \rho^2}{5 \delta} n \mathbb{E} \left[ \norm{ \avgg^t }^2 \right]  + \frac{31}{\delta} \beta^2 \mathbb{E} \left[ \norm{ V^t - \grdF^t }_F^2 \right] + \frac{31}{\delta} \beta^2 n \sigma^2 .
\end{align*}
This concludes our proof.

\subsection{Proof of \Cref{lem:wholesys}} \label{app:wholesys}
% Observe that the step size conditions in \eqref{eq:docom_stepsize} implies
Below, we illustrate how to find a set of tight conditions for the free parameters $a,b,c>0$.
Combining \Cref{lem:theta_o_new}, \ref{lem:vt_bound}, \ref{lem:gt_new}, \ref{lem:gtghat_new}, \ref{lem:thetahat_new} and \eqref{eq:vtmatform} yields
\begin{align*}
    &\scalebox{0.9}{ $\mathbb{E} \left[ L^2 \norm{\Theta_o^{t+1}}_F^2 + n \norm{ \avgv^{t+1} - \avggrdF^{t+1}}^2 + \frac{1}{n} \norm{V^{t+1} - \grdF^{t+1} }_F^2 + a \norm{G_o^{t+1}}_F^2 + b \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 + c \norm{ \Theta^{t+1} - \eta G^{t+1} - \hat{\Theta}^{t+1} }_F^2 \right]$ }
    \nl &\leq \left( 1 - \frac{\rho\gamma}{2} + \frac{16}{n}(1-\beta)^2\bw^2\gamma^2 + a\gamma\frac{25\bw^2}{\rho} + b\gamma^2 \frac{122\bw^2}{\delta} + c(\gamma^2\frac{29\bw^2}{\delta L^2} + \eta^2 \frac{38\bw}{\rho})\right) 
\mathbb{E}\left[L^2 \norm{ \Theta_o^t}_F^2\right]
    \nl &\quad + ( 1 - \beta )^2 \, n \mathbb{E}\left[ \norm{ \avgv^t - \avggrdF^t }^2 \right] + \left( (1-\beta)^2 + a\beta^2\frac{7n}{\rho\gamma} + b\beta^2 \frac{31n}{\delta} + c \beta^2\eta^2 (18 + \frac{84}{\rho\gamma}) \frac{n}{\delta} \right) \, \frac{1}{n} 
\mathbb{E}\left[\norm{ V^{t} - \grdF^{t} }_F^2\right]
    \nl &\quad + a\left( 1-\frac{\rho\gamma}{4} + \frac{1}{a}\eta^2\frac{2L^2}{\rho\gamma} + \frac{1}{a} \frac{16}{n} \eta^2(1-\beta)^2 (1-\rho\gamma)^2 L^2 + \frac{b}{a}\gamma^2 \frac{10}{\delta} (\bw^2 + \frac{\rho^2}{8}) + \frac{c}{a}\eta^2\frac{50}{\delta} \right) 
\mathbb{E}\left[\norm{ G_o^{t} }_F^2\right]
    \nl &\quad + b \left(1-\frac{\delta}{8} + \frac{a}{b}\gamma\frac{2\bw^2}{\rho} +  \frac{c}{b}\eta^2\frac{3\bw}{\rho} \right) 
\mathbb{E}\left[\norm{ G^{t} - \hat{G}^{t} }_F^2\right]
    \nl &\quad + c\left(1-\frac{\delta}{8} + \frac{1}{c}\gamma\frac{L^2\bw^2}{\rho} + \frac{1}{c} \frac{8}{n} \gamma^2(1-\beta)^2L^2\bw^2(1-\delta) + \frac{a}{c}\gamma\frac{13L^2}{\rho}\bw^2(1-\delta) + \frac{b}{c}\gamma^2 \frac{60L^2\bw^2}{\delta} \right) 
\mathbb{E}\left[\norm{ \Theta^{t} - \eta G^{t} - \hat{\Theta}^{t}}_F^2\right]
    \nl &\quad + \left( \frac{4}{n} \beta^2 + a\beta^2\frac{7}{\rho\gamma} + b\beta^2\frac{31}{\delta} + c\beta^2\eta^2(18+\frac{84}{\rho\gamma})\frac{1}{\delta}\right)
n\sigma^2
    \nl &\quad + \left( \frac{8}{n} (1-\beta)^2 L^2 \eta^2 (1-\rho\gamma)^2 + a\gamma \frac{\rho}{5} + b\gamma^2 \frac{3\rho^2}{5\delta} + c\eta^2 \frac{18}{\delta} \right)
n\,\mathbb{E}\left[ \norm{\avgg^t}^2 \right]
\end{align*}
Our goal is to find conditions on step sizes and the choices of $a,b,c$ such that
\begin{equation} \label{eq:big_inequality_sys}
\begin{aligned}
    & 1 - \frac{\rho\gamma}{2} + \frac{16}{n} (1-\beta)^2\bw^2\gamma^2 + a\gamma\frac{25\bw^2}{\rho} + b\gamma^2 \frac{122\bw^2}{\delta} + c(\gamma^2\frac{29\bw^2}{\delta L^2} + \eta^2 \frac{38\bw}{\rho}) \leq 1 - \frac{\rho\gamma}{8}
    \nl & (1-\beta)^2 + a\beta^2\frac{7n}{\rho\gamma} + b\beta^2 \frac{31n}{\delta} + c\beta^2\eta^2 (18 + \frac{84}{\rho\gamma}) \frac{n}{\delta} \leq (1-\beta)
    \nl & 1-\frac{\rho\gamma}{4} + \frac{1}{a}\eta^2\frac{2L^2}{\rho\gamma} + \frac{1}{a} \frac{16}{n} \eta^2(1-\beta)^2 (1-\rho\gamma)^2 L^2 + \frac{b}{a}\gamma^2 \frac{10}{\delta} (\bw^2 + \frac{\rho^2}{8}) + \frac{c}{a}\eta^2\frac{50}{\delta} \leq 1 - \frac{\rho\gamma}{8}
    \nl & 1-\frac{\delta}{8} + \frac{a}{b}\gamma\frac{2\bw^2}{\rho} +  \frac{c}{b}\eta^2\frac{3\bw}{\rho} \leq 1 - \frac{\delta\gamma}{8}
    \nl & 1-\frac{\delta}{8} + \frac{1}{c}\gamma\frac{L^2\bw^2}{\rho} + \frac{1}{c} \frac{8}{n} \gamma^2(1-\beta)^2L^2\bw^2(1-\delta) + \frac{a}{c}\gamma\frac{13L^2}{\rho}\bw^2(1-\delta) + \frac{b}{c}\gamma^2 \frac{60L^2\bw^2}{\delta} \leq 1 - \frac{\delta\gamma}{8}.
\end{aligned}  
\end{equation}
% All the step size conditions:
% \begin{align*}
% &\eta \leq \frac{1}{4L}
% \nl &\eta \leq \frac{ \rho \gamma }{ 10 L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }
% \nl &\gamma \leq \frac{\delta}{8 \bw}
% \nl &\gamma^2 \leq \min \{\frac{\delta}{ 16 \bw^2 (1-\delta) (1 + 3\eta^2 L^2) (1 + 2/\delta)}, \frac{1}{\rho^2}, \frac{\delta^2}{64\bw^2}\}
% \nl &\eta^2 \gamma \leq \frac{\delta^2 \rho}{1248 \bw^2 L^2}
% \end{align*}
To this end, with the step size condition 
\begin{align} \label{eq:whole_step_1st}
    \gamma \leq \min \left\{ \frac{1}{4\rho}, \frac{\rho n} {64(1-\beta)^2\bw^2}, \frac{n}{8(1-\beta)^2(1-\delta)\rho}\right\}, \tag{S1}
\end{align}
the above set of inequalities can be guaranteed if $a,b,c$ satisfy 
\begin{align}
    & \frac{96L^2}{\rho^2\gamma^2}\eta^2 \leq a \leq \min \left\{\frac{(1-\beta)\gamma\rho}{21\beta n}, \frac{2}{13(1-\delta)}, \frac{\rho^2}{600\bw^2}\right\} \label{eq:a_bounds}
    \\ &\max \left\{ a \frac{\gamma}{1-\gamma}\frac{32\bw^2}{\rho\delta}, c\frac{\eta^2}{1-\gamma}\frac{48\bw^2}{\rho\delta} \right\} \leq b 
    \leq \min \left\{ \frac{\eta^2}{\gamma^3} \frac{2\delta L^2}{5\rho (\bw^2 + \frac{\rho^2}{8})}, \frac{\delta}{30 \rho\gamma}, \frac{(1-\beta)\delta}{93\beta n}, \frac{\delta\rho}{2928\gamma\bw^2}\right\} \label{eq:b_bounds}
    \\ & \frac{\gamma}{1-\gamma} \frac{48L^2\bw^2}{\delta\rho} \leq c \leq \min \left\{ \frac{\delta\rho L^2}{1392 \gamma \bw^2}, \frac{2\delta L^2}{25\rho\gamma}, \frac{1-\beta}{\beta\eta^2} \frac{\delta}{3n (18+\frac{84}{\rho\gamma})}, \frac{\gamma}{\eta^2}\frac{\rho^2}{1824\bw}\right\} \label{eq:c_bounds}
\end{align}
Notice that the step size conditions:
\begin{align} \label{eq:whole_step_2nd}
\eta^2 \leq \min 
\left\{ \frac{(1-\beta)\gamma^3}{2016\beta n }\frac{\rho^3}{L^2},
        \frac{\gamma^2\rho^2}{624(1-\delta)L^2},
        \frac{\gamma^2\rho^4}{57600\bw^2L^2} 
\right\} \tag{S2}
\end{align}
guarantees the existence of $a$ which satisfies \eqref{eq:a_bounds}. In particular, we take $a = \frac{96L^2}{\rho^2\gamma^2}\eta^2$. 

At the same time, with the step size conditions:
\begin{align} \label{eq:whole_step_3rd}
   \eta^2 \leq \min \left\{ \frac{(1-\beta)\gamma}{\beta n} \frac{464\bw^2}{(18+\frac{84}{\rho\gamma})\rho L^2}, \gamma^2 \frac{29 \rho\bw}{38\delta L^2} \right\}, \quad \frac{\gamma^2}{1-\gamma} \leq \frac{\delta^2\rho^2}{66816\bw^4}, \tag{S3}
\end{align}
we guarantee the existence of $c$ which satisfies \eqref{eq:c_bounds}. In particular, we take $c = \frac{\gamma}{1-\gamma}\frac{48 L^2\bw^2}{\delta\rho}$.
This simplifies \eqref{eq:b_bounds} into
\begin{align}
    \max \left\{ \frac{\eta^2}{\gamma(1-\gamma)}\frac{3072\bw^2 L^2}{\delta\rho^3}, \frac{\eta^2\gamma}{(1-\gamma)^2}\frac{2304\bw^4 L^2}{\delta^2\rho^2} \right\} \leq b \leq \min \left\{ \frac{\eta^2}{\gamma^3} \frac{2\delta L^2}{5\rho (\bw^2 + \frac{\rho^2}{8})}, \frac{\delta}{30 \rho\gamma}, \frac{(1-\beta)\delta}{93\beta n}, \frac{\delta\rho}{2928\gamma\bw^2}\right\}  \label{eq:b_bounds_2} 
\end{align}
Combining with the step size conditions:
\begin{align}
    & \eta^2 \leq \min \left\{ \frac{\gamma^2(\bw^2 + \frac{\rho^2}{8})}{12 L^2}, \frac{(1-\beta)\gamma^3}{\beta n} \frac{5\rho(\bw^2 + \frac{\rho^2}{8})}{186 L^2}, \gamma^2 \frac{5\rho^2(\bw^2+\frac{\rho^2}{8})}{5856 \bw^2 L^2} \right\}, 
    \nl & \frac{\gamma^2}{1-\gamma} \leq \min \left\{ \frac{\delta^2\rho^2}{7680 \bw^2 (\bw^2 + \frac{\rho^2}{8})}, \frac{4\delta}{3\bw^2\rho}\right\} \tag{S4} \label{eq:whole_step_4th}
\end{align}
guarantees the existence of $b$ which satisfies \eqref{eq:b_bounds_2}. Finally, we take $b = \frac{\eta^2}{\gamma(1-\gamma)}\frac{3072\bw^2 L^2}{\delta\rho^3}$. 

Using the upper bound on $\gamma^2/(1-\gamma)$ from \eqref{eq:whole_step_4th} and the above choices of $a,b,c$ yield:
\begin{align*}
    \ER^{t+1} & \leq \left( 1- \min \left\{ \frac{\rho\gamma}{8}, \frac{\delta\gamma}{8}, \beta \right\}\right) \ER^t  + \beta^2 \left[4 + \frac{\eta^2}{\gamma^3}\frac{672L^2 n}{\rho^3} + \frac{\eta^2}{\gamma} \frac{6 L^2 n \rho^4 \delta}{25 \bw^2} + \frac{\eta^2}{\gamma^2} \frac{4 L^2 n}{\bw^2} \right] \sigma^2
    \nl &\quad + \eta^2 \left[ 8(1-\beta)^2 L^2 (1-\rho\gamma)^2 + \frac{L^2 n}{\rho \gamma} \left( 96 + \frac{141}{400} \frac{\rho^2}{\bw^2} \right) \right] \mathbb{E}\left[ \norm{\avgg^t}^2 \right]
\end{align*}
Furthermore, we observe that the above steps require step size conditions \eqref{eq:whole_step_1st}, \eqref{eq:whole_step_2nd}, \eqref{eq:whole_step_3rd}, \eqref{eq:whole_step_4th}. Together with the requirements in \Cref{lem:theta_o_new}, \ref{lem:vt_bound}, \ref{lem:gt_new}, \ref{lem:gtghat_new}, \ref{lem:thetahat_new}, we need
\begin{align*}
    &\eta^2 \leq \min \bigg\{
    \frac{(1-\beta)\gamma^3}{2016\beta n}\frac{\rho^3}{L^2},
    \frac{(1-\beta)\gamma}{\beta n}  \frac{464\bw^2}{(18+\frac{84}{\rho\gamma})\rho L^2},
    \gamma^2 \frac{29 \rho\bw}{38\delta L^2},
    \frac{\gamma^2(\bw^2 + \frac{\rho^2}{8})}{12 L^2}, 
    \frac{(1-\beta)\gamma^3}{\beta n} \frac{5\rho(\bw^2 + \frac{\rho^2}{8})}{186 L^2}, 
    \\
    &\quad \quad \quad \quad \frac{5 \gamma^2  \rho^2(\bw^2+\frac{\rho^2}{8})}{5856 \bw^2 L^2}, \frac{\gamma^2\rho^2}{624(1-\delta)L^2},
    \frac{\gamma^2\rho^4}{57600\bw^2L^2}, \frac{ \rho^2 \gamma^2 }{ 100 L^2 (1-\rho\gamma)^2 ( 1 + \gamma^2 \bw^2)}, \frac{ \delta^2 \rho }{ 1248 \bw^2 L^2 \gamma }
    \bigg\}
    \nl 
    &\gamma \leq \min \left\{ 
    \frac{1}{4\rho}, 
    \frac{\rho n}{64(1-\beta)^2\bw^2},
    \frac{n}{8(1-\beta)^2(1-\delta)\rho}, \frac{ \delta}{8 \bw}, \frac{ \sqrt{\delta}} {4 \bw \sqrt{(1-\delta)(1+3 \eta^2 L^2)(1 + 2/\delta)}}
    \right\}
    , \nl 
    & \frac{\gamma^2}{1-\gamma} \leq \min \left\{ 
    \frac{\delta^2\rho^2}{66816\bw^4}
    \frac{\delta^2\rho^2}{7680 \bw^2 (\bw^2 + \frac{\rho^2}{8})},
    \frac{4\delta}{3\bw^2\rho}
    \right\}
\end{align*}
Taking the restriction that $\bw \in [1,2]$, the above can be simplified and implied by 
\begin{align*}
    \gamma & \leq \min \left\{ \frac{1}{4 \rho}, \frac{\rho n}{64 \bw^2}, \frac{ \delta}{10 \bw}, \frac{\delta \rho {\redtmp \sqrt{1-\delta/(8 \bw^2)}} }{259 \bw^2} \right\} =: \gamma_\infty , \\
    \eta & \leq \frac{\gamma}{L}  \min \bigg\{
        \sqrt{\frac{1-\beta}{\beta n}}\frac{\sqrt{ \gamma \rho^3} }{45} , \frac{\rho^2}{240 \bw }
    \bigg\} =: \eta_\infty .
\end{align*}
{\redtmp where we have used the upper bound $\gamma \leq \frac{\delta}{8 \bw}$ to remove the self-dependence on $1-\gamma$ for the constraints on $\gamma$.} This concludes the proof.

\subsection{Auxilliary Lemmas}
\begin{lemma} \label{lem:theta_1stepnew} Under \Cref{ass:mix}, \ref{ass:compress}. For any $t \geq 0$, it holds 
    \begin{align}
        \mathbb{E} \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] &\leq 4 \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + 2 n \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right]
        \\ &\quad  + 4 \bw^2 \gamma^2 \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + 2 \bw^2 \gamma^2(1-\delta) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right]. \nonumber
    \end{align}
\end{lemma}
\begin{proof}
    We observe that:
\begin{align}
    & \norm{\Theta^{t+1} - \Theta^t}_F^2 = \norm{ \eta G^t - \gamma(\W-\I) \hat{\Theta}^{t+1} }_F^2 \nonumber \\
    % & = \norm{- \eta G^t + \gamma(\W-\I) \left[\hat{\Theta}^t + {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t; \zeta_0^{t+1} )\right]}_F^2 \nonumber \\
    % & = \norm{- \eta G^t + \gamma(\W-\I) \left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t; \xi_0^{t+1}) - (\Theta^t - \eta G^t-\hat{\Theta}^t) + \Theta^t - \eta G^t\right]}_F^2 \nonumber \\
    & = \norm{(I + \gamma (\W-\I)) (- \eta G^t) + \gamma(\W-\I)\Theta ^t + \gamma(\W-\I) \left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t)\right]}_F^2 \nonumber \\
    & \leq 2\norm{( I + \gamma(\W-\I) ) (- \eta G^t) + \gamma(\W-\I)\Theta^t}_F^2 \nonumber \\
    & \quad + 2\norm{\gamma(\W-\I)\left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t)\right]}_F^2 \nl 
    & \leq 2 \left[ \eta^2 \norm{ (I+\gamma(\W-\I)) G^t }_F^2 + \gamma^2 \norm{ (\W-\I) \Theta^t }_F^2 + 2 \dotp{( I + \gamma(\W-\I) ) (- \eta G^t)}{\gamma(\W-\I)\Theta^t}\right] \nl 
    & \quad + 2 \bw^2 \gamma^2 \norm{ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t) }_F^2
\end{align}
Observe that $\dotp{( \I + \gamma(\W-\I) ) (- \eta G^t)}{\gamma(\W-\I)\Theta^t} = \dotp{( \I + \gamma(\W-\I) ) (- \eta \U G_o^t)}{\gamma(\W-\I)\Theta^t}$,
the above leads to
\begin{align}
    \mathbb{E}_t \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] & \leq 2\eta^2(1-\rho\gamma)^2\norm{G^t}_F^2 + 2\eta^2(1-\rho\gamma)^2\norm{G_o^t}_F^2 + 4\gamma^2\norm{(\W-\I)\Theta^t}_F^2 \nl 
    & \quad + 2 \bw^2 \gamma^2(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \label{int:Theta_diff_a}
\end{align}
Notice that 
\begin{align}
    \mathbb{E} \left[ \norm{G^t}_F^2 \right] & = \mathbb{E} \left[ \norm{ ( (1/n) {\bf 11}^\top + \U\U^\top ) G^t }_F^2 \right] = \mathbb{E} \left[ \norm{  \U\U^\top G^t }_F^2 + \norm{ (1/n){\bf 11}^\top G^t }_F^2 \right]
    \nl & \leq \mathbb{E} \left[ \norm{G_o^t}_F^2 + n \norm{ \avgg^t }^2 \right] \label{int:Theta_diff_c}
\end{align}
By combining \eqref{int:Theta_diff_a}, \eqref{int:Theta_diff_c}, and the fact $\norm{ (\W-\I)\Theta^t}_F^2 \leq \bw^2 \norm{\Theta_o^t}_F^2$, we have
\begin{align*}
    \mathbb{E} \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] &\leq 4 \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + 2 n \eta^2(1-\rho\gamma)^2 \mathbb{E} \left[ \norm{ \avgg^t }^2 \right]
    \\ &\quad  + 4 \bw^2 \gamma^2 \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + 2 \bw^2 \gamma^2(1-\delta) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right]
\end{align*}
This concludes the proof.
\end{proof}

\begin{lemma} \label{lem:vt_diffnew} Under \Cref{ass:lips}, \ref{ass:stoc}. For any $t \geq 0$, it holds
    \begin{align*}
        \mathbb{E} \left[ \norm{V^{t+1} - V^t}_F^2 \right] & \leq 3 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + 3 n \beta^2 \sigma^2
    \end{align*}
\end{lemma}

\begin{proof}
    Observe that 
    \begin{align*}
        V^{t+1} - V^t & = \stocgrdF^{t+1} + (1-\beta) ( V^t - \stocgrdFp^{t} ) - V^t \\
        & = \stocgrdF^{t+1} - \stocgrdFp^{t} - \beta ( V^t - \grdF^t ) + \beta ( \stocgrdFp^{t} - \grdF^t )
    \end{align*}
    It holds that 
    \begin{align*}
        \mathbb{E} \left[ \norm{V^{t+1} - V^t}_F^2 \right] & \leq 3 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{ \stocgrdFp^{t}  - \grdF^t}_F^2 \right] \\
        & \leq 3 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 \beta^2 \mathbb{E} \left[ \norm{V^t - \grdF^t}_F^2 \right] + 3 n \beta^2 \sigma^2
    \end{align*}
    This concludes the proof.
\end{proof}

\subsection{Transient Time of \aname} \label{app:step_twist}
We follow a similar argument as in \eqref{eq:grdFbound}. Particularly, consider setting the step sizes and parameters as $\beta = \Theta( \frac{ 1 }{ T^{2/3} } ), \eta = \Theta( \frac{ 1 }{ L T^{1/3} } ), \gamma = \gamma_\infty, b_0 = \Omega(  T^{1/3} )$. Then for sufficiently large $T$, we obtain
\begin{align}
& \frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ \norm{ \nabla f( \theta_i^{\sf T} )}^2 \right] = {\cal O} \left( \frac{L (f(\avgtheta^0) - f^\star)}{T^{2/3}} + \frac{\sigma^2}{ n T^{2/3}} + \frac{ \InitG}{ \delta^2 \rho^4 T} + \frac{\sigma^2}{ \delta^3 \rho^6 T^{4/3}} \right).
\end{align}
The transient time can be calculated by bounding $T$ such that the second term dominates over the last two terms. We get 
\begin{align}
T_{\sf trans} := \Omega \left( \max \left\{ \frac{n^3 \InitG^3}{ \sigma^6 \delta^6 \rho^{12} }, \frac{n^{1.5}}{\delta^{1.5} \rho^6} \right\} \right)  
\end{align}
Taking $\sigma \leq 1$ guarantees that $T_{\sf trans} = \Omega ( \frac{n^3 \InitG^3}{ \sigma^6 \delta^6 \rho^{12} } )$.

\subsection{Sublinear bound of ${\cal O}(\log T /T)$ under PL Condition} \label{app:largeT} 
We consider setting $\beta = \eta = \log T /(\mu T)$, $\gamma = \gamma_\infty$, $b_0 = 1$. Notice that for a sufficiently large $T$, these step sizes will satisfy \eqref{eq:docom_stepsize}. Furthermore, setting $t = T$, the upper bound in \eqref{eq:plcase} is given by:
\begin{equation} \label{eq:pl_simplified}
\left( 1 - \frac{  \log T }{ T} \right)^T \left( \Delta^0 + \frac{2}{ n } \left( 2\sigma^2 + \frac{{\redtmp 118}L^2n}{\rho^2 \gamma^2(1-\gamma)} \InitG \frac{(\log T)^2}{T^2} \right) \right)  + \frac{2 \ConstS \sigma^2}{n} \frac{\log T }{\mu T}
\end{equation}
We observe that 
\[ 
\left( 1 - \frac{\log T}{T} \right)^T \leq e^{- \log T} = \frac{1}{T}.
\]
Thus the expression in \eqref{eq:pl_simplified} can be further upper bounded by ${\cal O}(\log T/T)$. This implies \eqref{eq:epsreq1}.

% From the above, we observe that satisfying \eqref{eq:epsreq1}, \eqref{eq:epsreq2} requires bounding the above by ${\cal O}( \epsilon )$. 
% The above is bounded by ${\cal O}(\epsilon)$ if
% \[ 
% \left( 1 - \frac{\mu}{T} \right)^t \left( \Delta^0 + \frac{4 \sigma^2}{n} + \frac{384 L^2 \InitG}{\rho^2 \gamma^2(1-\gamma)} \frac{1}{T^2} \right) = {\cal O}( \epsilon ) \Longleftrightarrow t = \Omega( \log( \epsilon^{-1}) ) \quad \text{and} \quad \frac{2 \ConstS \sigma^2}{n} \frac{1}{T} = {\cal O}(\epsilon) \Longleftrightarrow T = \Omega(1/\epsilon)
% \] 
% As such, setting $T = \Omega(1/\epsilon)$ and $t = T$ yields \eqref{eq:epsreq1}, \eqref{eq:epsreq2}.


\section{Convergence Analysis of \aname~with $\beta = 1$}\label{app:betaequalone}
This section provides the convergence analysis of \eqref{eq:algo} for the special case when the momentum parameter is $\beta = 1$, i.e., there is no momentum applied. Observe that the \aname~algorithm can be simplified as
\begin{equation*}
    \begin{array}{ll}
        \Theta^{t+1} & = \Theta^t - \eta G^t + \gamma ( {\bf W} - {\bf I} ) \hatTheta^{t+1} \\
        \hatTheta^{t+1} & = \hatTheta^t + {\cal Q} ( \Theta^t - \eta G^t - \hatTheta^t ) \\
        G^{t+1} & = G^t + \stocgrdF^{t+1} - \stocgrdF^{t} + \gamma ( {\bf W} - {\bf I} ) \hatG^{t+1} \\
        \hatG^{t+1} & = \hatG^t + {\cal Q} ( G^t + \stocgrdF^{t+1} - \stocgrdF^{t} - \hatG^t )
    \end{array}
\end{equation*}
where we have eliminated the use of $V^t$ since $V^t = \stocgrdF^t$ for any $t \geq 0$. Additionally, we define $\bar{\Theta}^t = (1/n) {\bf 1} {\bf 1}^\top \Theta^t$. 

In such setting, the convergence analysis has to follow a different path from the case of $\beta < 1$. We begin by analyzing the 1-iteration progress with
\begin{lemma} \label{lem:onestep}
    Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, and the step size satisfies $\eta \leq 1/(4L)$. Then, for any $t \geq 0$, it holds
    \begin{equation} \label{eq:onestep}
    \mathbb{E}_t [f(\avgtheta^{t+1})] \leq  f(\avgtheta^t) -\frac{\eta}{4}\norm{\nabla f(\avgtheta^t)}^2 + \frac{3L^2\eta}{4n}\norm{\Theta_o^t }_F^2 + \frac{ L \eta^2 \sigma^2}{2n}.
    \end{equation} 
\end{lemma}
The proof is relegated to \Cref{app:pf_onestep}. Notice that the above lemma departs from \Cref{lem:f_1stepnew} as it results in a bound that depends only on the consensus error. 
Our next endeavor is to bound $\norm{\Theta_o^t }_F^2$, which can be conveniently controlled by \Cref{lem:theta_o_new} as quoted below:
\begin{align}
    \mathbb{E} [\norm{\Theta_o^{t+1}}_F^2] & \leq (1-\frac{\rho\gamma}{2}) \mathbb{E} [\norm{\Theta_o^t}_F^2] + \frac{2}{\rho} \, \frac{\eta^2}{\gamma} \mathbb{E} [ \norm{G_o^t}_F^2 ] + \frac{\bw^2}{\rho} \, \gamma \, \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hatTheta^t}^2_F \right].
\end{align}
Moreover, 
\begin{lemma} \label{lemma:g_cons}
    Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}. Suppose the step size satisfies $\eta \leq \frac{ \rho \gamma }{ 8L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$, $\gamma \leq \frac{1}{8 \bw}$. Then, for any $t \geq 0$, the consensus error of $G^{t+1}$ is bounded as follows:
    \begin{align*}
    \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{4} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \, \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hatG^t }_F^2 \right] + \gamma \, \left( \frac{\rho}{4} + \frac{18 L^2 \bw^2}{\rho} \right) \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
    & \quad + \gamma \, \frac{9 L^2}{ \rho } \bw^2 \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hatTheta^t }_F^2 \right] + \gamma \, \frac{ \rho n}{4} \, \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t ) }^2 \right] + \left( \frac{7n }{\rho \gamma} + \frac{\rho \gamma}{8} \right) \sigma^2 .
    \end{align*}
\end{lemma}
The proof is relegated to \Cref{app:pf_g_consensus}. We also bound the subsequent terms by
\begin{lemma} \label{lem:gt}
    Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}. Suppose the step size satisfies $\eta \leq \frac{ \rho \gamma }{ 8L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }$, $\gamma \leq \frac{\delta}{8 \bw}$. Then, for any $t \geq 0$, it holds:
    \begin{align*}
    \mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10}{\delta} \gamma^2 \left( \bw^2 + \frac{\rho^2 }{8} \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
    & \quad + \gamma^2 \, \frac{5}{4 \delta} \left( \rho^2 + 72 L^2 \bw^2 \right) \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \gamma^2 \, \frac{40 L^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{G}^t }_F^2 \right] \nl
    & \quad + \gamma^2 \, \frac{5 \rho^2}{4 \delta} n \mathbb{E} \left[ \norm{\nabla f(\avgtheta^t)}^2 \right]  + \frac{5}{\delta} \left( \frac{\rho^2 \gamma^2}{8} + 7 n \right) \sigma^2.
    \end{align*}
    \end{lemma}
    \begin{lemma} \label{lemma:theta_comm}
        Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}. Suppose the step size satisfies $\gamma \leq \frac{\sqrt{\delta} }{ 4 \bw \sqrt{ (1-\delta) (1 + \eta^2 L^2) (1 + 2/\delta)}} $. Then, for any $t \geq 0$, it holds 
        \begin{align}
            \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq (1-\frac{\delta}{4}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + \eta^2 \frac{12}{\delta} \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] \nl 
            & \quad + \frac{24}{\delta} \eta^2 \sigma^2 + \frac{38}{\delta} \eta^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{26}{\delta} \mathbb{E} \left[ ( \eta^2 L^2 + \bw^2 \gamma^2 ) \norm{\Theta_o^t}_F^2 + \eta^2 n \norm{ \nabla f( \avgtheta^t) }^2 \right] \nonumber
        \end{align}
        Furthermore, if the step size satisfies $\eta^2 \gamma \leq \frac{\delta^2 \rho}{864 \bw^2 L^2}$, then 
        \begin{align}
        \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{8} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] \nl 
        & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \frac{3 \bw}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hat{G}^t}_F^2 \right] \nl 
        & \quad + \left[ \eta^2 \, \frac{29 L^4}{\delta} \left( 1 + \frac{ \bw}{\rho} \right) + \frac{26}{\delta} \bw^2 \gamma^2 \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \nl 
        & \quad + \eta^2 \, \frac{29n }{\delta} \, \mathbb{E}  \left[ \norm{ \nabla f( \avgtheta^t) }^2 \right] + \eta^2\, \frac{24 \sigma^2}{\delta} \left( 1 + \frac{4n}{\rho \gamma} \right) \nonumber
    \end{align}
        % The information flow of model parameters at iteration $t+1$ is bounded as follows:
        % \begin{align*}
        %     &\norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2
        %     \nl&\leq (1-\frac{\delta}{2})\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2
        %     + \frac{51}{\delta^2}\norm{\Theta^{t+1}-\Theta^t}_F^2
        %     + 576\eta^2\gamma^2\frac{1}{\delta^2}\norm{G_0^t}_F^2
        %     + 32\eta^2\gamma^2\frac{1}{\delta}\norm{G^t - \virtghat^t}_F^2
        %     + 96\eta^2\frac{n\sigma^2}{\delta},
        % \end{align*}
        % and we further require $\gamma\leq\frac{\delta^{3/2}}{41(1-\delta)^{1/2}}$ for self-contraction.
\end{lemma}
The proofs are relegated to \Cref{app:lemgt}, \ref{app:pf_t_comm}, respectively. 
Combining the above lemmas and optimizing the bounds for step sizes lead to
\begin{lemma} \label{lem:combined}
    Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress} and the step size conditions
    \begin{align*}
    & \eta \leq \frac{1}{L} \min \left\{ \frac{1}{4}, \frac{ \rho \gamma }{ 8 (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }, \frac{ \rho^2 \gamma }{ 105 \bw} , \frac{ \rho^2 \delta \sqrt{1-\gamma} }{ 1303 \bw^2} , \frac{ \rho^{3/2} \delta \sqrt{1-\gamma} }{ 130 L \bw^{3/2} } \right\} \nl
    & \gamma \leq \min \left\{ \frac{1}{\rho}, \frac{\delta}{8\bw}, \frac{\sqrt{\delta} }{ 4 \bw \sqrt{ (1-\delta) (1 + \eta^2 L^2) (1 + 2/\delta)} }, \frac{ \rho \delta^{3/2} \sqrt{1-\delta/(8\bw)} }{ 88 \bw \sqrt{ \delta \bw^2 + \rho^2} }, \frac{ \rho \delta \sqrt{1-\delta/(8\bw)} }{ 123 \bw^2 }  \right\} , ~~ \eta^2 \gamma \leq \frac{\delta^2 \rho}{864 \bw^2 L^2}.
    \end{align*}
    Define the constants: 
    \[
    \ConstSS = \frac{192}{1-\gamma} \left[ \frac{ 2(1-\gamma) }{ \rho^3 \gamma^3 } + \frac{320 \bw^2}{\rho^3 \delta^2 \gamma } + \frac{15 \bw^2}{\rho^2 \delta^2 \gamma } \right],~~\ConstFF = \frac{12}{1-\gamma} \left[ \frac{ 1-\gamma }{ \rho \gamma } + \frac{256 \bw^2}{\rho \delta^2 } \gamma +  \frac{58 \bw^2 }{\rho \delta^2 } \right]
    \]
    Then, for any $t \geq 0$, it holds
    \begin{align}
    & \mathbb{E} \left[ \norm{\Theta_o^{t+1}}_F^2 + a \norm{ G_o^{t+1} }_F^2 + b \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 + c \norm{ \Theta^{t+1} - \eta G^{t+1} - \hat{\Theta}^{t+1} }_F^2 \right] \nl
    & \leq \left( 1 - \frac{ \min\{ \rho , \delta \} }{8} \gamma \right) \mathbb{E} \left[ \norm{\Theta_o^{t}}_F^2 + a \norm{ G_o^{t} }_F^2 + b \norm{ G^{t} - \hat{G}^{t} }_F^2 + c \norm{ \Theta^{t} - \eta G^{t} - \hat{\Theta}^{t} }_F^2 \right] \nl 
    & \quad + \eta^2 \ConstS n \sigma^2 + \eta^2 \ConstFF n \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^t ) }^2 \right] \nonumber
    \end{align}
    where $a = \frac{48 \eta^2}{ \rho^2 \gamma^2}$, $b = \frac{1536 \bw^2 \eta^2}{\rho^3 \gamma \delta (1-\gamma)}$, $c = \frac{ 24 \bw^2 }{ \rho \delta (1-\gamma) }$.
\end{lemma}
    The proof is relegated to \Cref{app:pf_combined}.

    To simplify notations, we let $\bbrho := \min\{ \rho, \delta \}/8$. Using $\mathbb{E} [ \norm{G_o^0}_F^2 ] = n \sigma^2$ and \Cref{lem:combined} imply that 
    \begin{align}
        \mathbb{E} \left[ \norm{ \Theta_o^t }_F^2 \right] & \leq a \left( 1 - \bbrho \gamma \right)^t n \sigma^2 + \eta^2 \sum_{s=0}^{t-1} (1 - \bbrho \gamma)^{t-s-1}\big\{  \ConstSS n \sigma^2 + \ConstFF n \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^s ) }^2 \right] \big\} \nl
        & \leq \ConstSS \frac{ \eta^2 n \sigma^2 }{ \bbrho \gamma } + \eta^2 n \ConstFF \sum_{s=0}^{t-1} (1 - \bbrho \gamma)^{t-s-1} \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^s ) }^2 \right], \label{eq:thetao_fin}
    \end{align}
    where the inequality is due to the fact that $\ConstSS \eta^2 \geq a$. 
    We now observe that \eqref{eq:onestep} implies
    \begin{align*}
        \frac{\eta}{4 T} \sum_{t=0}^{T-1} \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t)}^2 \right] & \leq \mathbb{E} \left[ \frac{f( \avgtheta^0 ) - f( \avgtheta^{T})}{T} \right] + \eta^2 \frac{\sigma^2 L}{2n} \left( 1 + \ConstSS \frac{3Ln}{2 \bbrho \gamma} \eta \right) \\
        & \quad + \eta^3 \ConstFF \frac{3L^2}{4} \frac{1}{T} \sum_{t=0}^{T-1} \sum_{s=0}^{t-1} (1 - \bbrho \gamma)^{t-s-1} \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^s ) }^2 \right] \\
        & \leq \mathbb{E} \left[ \frac{f( \avgtheta^0 ) - f( \avgtheta^{T})}{T} \right] + \eta^2 \frac{\sigma^2 L}{2n} \left( 1 + \ConstSS \frac{3Ln}{2 \bbrho \gamma} \eta \right) \\
        & \quad + \eta^3 \ConstFF \frac{3L^2}{4 \bbrho \gamma } \frac{1}{T} \sum_{s=0}^{T-2} \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^s ) }^2 \right]
    \end{align*}
    Therefore, under the additional step size condition $\eta \leq \sqrt{\frac{ \bbrho \gamma}{6 \ConstFF L^2}}$, it holds
    \begin{align}
        \frac{1}{ T} \sum_{t=0}^{T-1} \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t)}^2 \right] & \leq \frac{8}{\eta} \mathbb{E} \left[ \frac{f( \avgtheta^0 ) - f( \avgtheta^{T})}{T} \right] + \eta \frac{4 \sigma^2 L}{n} \left( 1 + \ConstSS \frac{3Ln}{2 \bbrho \gamma} \eta \right) \label{eq:docom_beta}
    \end{align}
    Setting $\eta = {\cal O}(1 / \sqrt{T})$ shows the expected convergence rate of $\frac{1}{ T} \sum_{t=0}^{T-1} \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t)}^2 \right] = {\cal O}( 1 / \sqrt{T} )$. We remark that similar result to \Cref{cor:pl} can be established under the PL condition \Cref{ass:pl}.

\subsection{Proof of \Cref{lem:onestep}} \label{app:pf_onestep}
Using the $L$-smoothness of $f$, we obtain:
\begin{align}
    f(\bar{\theta}^{t+1}) &\le f(\bar{\theta}^t) + \dotp{\nabla f(\bar{\theta}^t)}{\bar{\theta}^{t+1} - \bar{\theta}^t} + \frac{L}{2} \norm{\avgtheta^{t+1} - \avgtheta^t}^2 \nonumber\\
    % &= f(\avgtheta^t) - \eta \dotp{\nabla f(\avgtheta^t)}{\avgg^t} + \frac{L \eta^2}{2} \norm{\avgg^t}^2 \nonumber \\
    &= f(\avgtheta^t) - \eta  \norm{\nabla f(\avgtheta^t)}^2 - \eta \dotp{\nabla f(\avgtheta^t)}{\avgg^t -  \nabla f(\avgtheta^t)} + \frac{L\eta^2}{2} \norm{\avgg^t}^2 \label{eq:main}
\end{align}
Taking the conditional expectation on the inner product:
\begin{align*}
    &-\eta \mathbb{E}_t\dotp{\nabla f(\avgtheta^t)}{\avgg^t -  \nabla f(\avgtheta^t)} \\
    % &= -\eta \dotp{\nabla f(\avgtheta^t)}{\mathbb{E}_t[\avgg^t] -  \nabla f(\avgtheta^t)} \\
    &= -\eta \dotp{\nabla f(\avgtheta^t)}{\frac{1}{n} \sum^n_{i=1}  \nabla f_i(\theta^t_i)  -  \nabla f(\avgtheta^t)} \\
    &\leq \frac{\eta }{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{\eta }{2 } \norm{\frac{1}{n} \sum^n_{i=1}\left\{  \nabla f_i(\theta^t_i) - \nabla f_i(\avgtheta^t)\right\}}^2 \\
    &\leq \frac{\eta }{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{\eta }{2n}
            \sum^n_{i=1}\norm{\nabla f_i(\theta^t_i) - \nabla f_i(\avgtheta^t)}^2  \\
    &\leq \frac{\eta }{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{\eta }{2n}
            \sum^n_{i=1}L^2\norm{\theta^t_i - \avgtheta^t}^2  \\
    &=\frac{\eta }{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{\eta  L^2}{2n} \norm{\Theta^t - \bar{\Theta}^t}_F^2
\end{align*}
where we denote $\bar{\Theta}^t = \mathbf{1} (\avgtheta^t)^\top = \avgone \Theta^t$. \\ \\
Putting back into \eqref{eq:main} yields
\begin{align}
    \mathbb{E}_t [f(\avgtheta^{t+1})] &\leq f(\avgtheta^t) - \frac{\eta }{2}\norm{\nabla f(\avgtheta^t)}^2 + \frac{L\eta^2}{2}\mathbb{E}_t\norm{\avgg^t}^2 + \frac{\eta  L^2}{2n} \norm{\Theta^t - \bar{\Theta}^t}_F^2 \label{eq:main2}
\end{align}
Observe that as $\mathbb{E}_t \norm{ \avgg^t }^2 = \frac{1}{n^2} \mathbb{E}_t \norm{\mathbf{1}^\top G^t}^2$, we have
\begin{align}
    &\mathbb{E}_t \norm{\mathbf{1}^\top G^t}_F^2 = \mathbb{E}_t \norm{\sum^n_{i=1} \nabla f_i(\theta^t_i ; \zeta^{t+1}_i)}^2 = \mathbb{E}_t \norm{\sum^n_{i=1} \nabla f_i(\theta^{t}_i ; \zeta^{t+1}_i) - \sum^n_{i=1} \nabla f_i(\theta^t_i)}^2 + \norm{\sum^n_{i=1} \nabla f_i(\theta^t_i)}^2 
    % + 2 \mathbb{E} \dotp{\sum^n_{i=1} \nabla f_i(\theta^t_i ; \xi^t_i) - \sum^n_{i=1} \nabla f_i(\theta^t_i)}{ \sum^n_{i=1} \nabla f_i(\theta^t_i)} 
    \nonumber\\
    % &\stackrel{\eqref{eq:avgg_exp}}{\leq} \mathbb{E} \sum^n_{i=1} \norm{ \nabla f_i(\theta^t_i ; \xi^t_i) - \nabla f_i(\theta^t_i)}^2 + \norm{\sum^n_{i=1} \left\{\nabla f_i(\theta^t_i) - \nabla f_i(\avgtheta^t) + \nabla f_i(\avgtheta^t) \right\}}^2 + 0 \nonumber\\
    & \leq n\sigma^2 + 2 \norm{\sum^n_{i=1} \left\{\nabla f_i(\theta^t_i) - \nabla f_i(\avgtheta^t) \right\}}^2 + 2 \norm{\sum^n_{i=1} \nabla f_i(\avgtheta^t)}^2 \nonumber\\
    % & \leq n\sigma^2 + 2n \sum^n_{i=1} \norm{ \nabla f_i(\theta^t_i) - \nabla f_i(\avgtheta^t) }^2 + 2n^2 \norm{\frac{1}{n} \sum^n_{i=1} \nabla f_i(\avgtheta^t)}^2 \nonumber\\
    & \leq
    % n\sigma^2 + 2n\sum^n_{i=1} L^2 \norm{\theta^t_i - \avgtheta^t}^2 + 2n^2 \norm{\nabla f(\avgtheta^t)}^2 \nonumber\\
    n\sigma^2 + 2nL^2 \norm{\Theta^t - \bar{\Theta}^t}_F^2 + 2n^2 \norm{\nabla f(\avgtheta^t)}^2. \label{eq:avgg_norm}
\end{align}
Putting back into \eqref{eq:main2}, and assuming that $-(\frac{1}{2}-L\eta )\leq - \frac{1}{4} \Leftrightarrow \eta \leq \frac{1}{4L}$,
\begin{align}
    \mathbb{E}_t [f(\avgtheta^{t+1})] &\leq f(\avgtheta^t) - \frac{\eta }{2} \norm{\nabla f(\avgtheta^t)}^2 + \frac{L\eta^2 }{2n^2}\left( n \sigma^2 + 2 n L^2 \norm{\Theta^t - \bar{\Theta}^t}_F^2 + 2 n^2 \norm{\nabla f(\avgtheta^t)}^2\right) + \frac{\eta  L^2}{2n} \norm{\Theta^t - \bar{\Theta}^t}_F^2 \nonumber \\
    &= f(\avgtheta^t) -\eta (\frac{1}{2}-L\eta )\norm{\nabla f(\avgtheta^t)}^2 + \left(\frac{L^3\eta^2 }{n} + \frac{L^2 \eta }{2n}\right)\norm{\Theta^t - \bar{\Theta}^t}_F^2 + \frac{ L \eta^2  \sigma^2}{2n} 
    \nonumber\\ &\stackrel{(\eta \leq\frac{1}{4L})}{\leq} f(\avgtheta^t) -\frac{\eta }{4}\norm{\nabla f(\avgtheta^t)}^2 + \frac{3L^2\eta }{4n}\norm{\Theta^t - \bar{\Theta}^t}_F^2 + \frac{ L \eta^2  \sigma^2}{2n} \label{eq:main3}
\end{align}
The proof is completed.

\subsection{Proof of Lemma \ref{lemma:g_cons}} \label{app:pf_g_consensus}
We preface the proof by stating two lemmas that will be instrumental to the proof of \Cref{lemma:g_cons}. Their proofs can be found in the later part of this subsection. 
% The following bounds the distance between $\virtualG^t$ and $\hat{G}^t$:
\begin{lemma} \label{lem:stoc}
    Under \Cref{ass:stoc}. For any $t \geq 0$, it holds:
\begin{equation}
\mathbb{E} \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] \leq 2 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 3 n \sigma^2.
\end{equation}
\end{lemma}
\begin{lemma} \label{lem:succ_diff}
Under \Cref{ass:lips}, \ref{ass:mix}, \ref{ass:stoc}, \ref{ass:compress}. For any $t \geq 0$, it holds
    \begin{align}
        \mathbb{E}_t \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] &\leq 4 \eta^2(1-\rho\gamma)^2\norm{G_o^t}_F^2 + 4 \left( {\eta^2(1-\rho\gamma)^2L^2}  + \bw^2 \gamma^2 \right) \norm{\Theta_o^t}_F^2 + 2 \bw^2 \gamma^2(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \nonumber
        \\ & \quad + {2 \eta^2(1-\rho\gamma)^2\sigma^2} + 4 n \eta^2(1-\rho\gamma)^2 \norm{\nabla f(\avgtheta^t)}^2 .
        \label{neq:Theta_diff}
    \end{align}
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:g_cons}] 
We begin by observing the update for $G_o^{t+1}$ as:
\begin{align*}
G^{t+1}_o &= \U^\top[G^t +  \grdSF^{t+1} -  \grdSF^t +\gamma(\W-\I)\hat{G}^{t+1}] \\
& = \U^\top \left[ G^t +  \grdSF^{t+1} -  \grdSF^t +\gamma(\W-\I) ( \hat{G}^t + {\cal Q}( G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t ) ) \right] \\
& = \U^\top \left[ ( I + \gamma(\W-\I) ) ( G^t +  \grdSF^{t+1} -  \grdSF^t )  \right] \\
& \quad + \gamma \U^\top (\W-\I) \left[ {\cal Q}( G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t ) - (G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t) \right] 
\end{align*}
The above implies that 
\begin{align*}
\mathbb{E}_t \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq (1 + \alpha_0) ( 1 - \rho \gamma )^2 \mathbb{E}_t \left[ \norm{ G_o^t + U^\top ( \grdSF^{t+1} - \grdSF^t ) }_F^2 \right] \\ 
& \quad + (1 + \alpha_0^{-1}) \gamma^2 \bw^2 (1 - \delta) \mathbb{E}_t \left[ \norm{ G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t }_F^2 \right] \\
& \leq (1 + \alpha_0) ( 1 - \rho \gamma )^2 \mathbb{E}_t \left[ (1 + \alpha_1) \norm{ G_o^t }_F^2 + (1 + \alpha_1^{-1}) \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] \nl 
& \quad + 2 (1 + \alpha_0^{-1}) \gamma^2 \bw^2 (1 - \delta) \mathbb{E}_t \left[ \norm{ G^t - \hat{G}^t }_F^2 + \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right]
\end{align*}
Taking $\alpha_0 = \frac{ \rho \gamma }{ 1 - \rho \gamma }$, $\alpha_1 = \frac{\rho \gamma}{2}$ gives
\begin{align*}
\mathbb{E}_t \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} \right) \norm{ G_o^t }_F^2 + \gamma \frac{ 2 \bw^2}{ \rho } \norm{ G^t - \hat{G}^t }_F^2 \nl 
& \quad + \frac{2}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) \mathbb{E}_t \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right]
\end{align*}
Taking the full expectation and applying \Cref{lem:stoc} give
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] \nl 
& \quad + \frac{2}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) \left( 3 n \sigma^2 + 2L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] \right)
\end{align*}
Furthermore, applying \Cref{lem:succ_diff} yields
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{2} + \eta^2 \frac{16L^2 (1 + \gamma^2 \bw^2) ( 1 - \rho \gamma)^2 }{ \rho \gamma } \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] \nl 
& \quad + \frac{16 L^2 (1 + \gamma^2 \bw^2)}{\rho \gamma} ( \eta^2 L^2 (1 - \rho\gamma)^2 + \bw^2 \gamma^2 ) \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
& \quad + \frac{8 L^2(1 + \gamma^2 \bw^2)}{ \rho \gamma } \bw^2 \gamma^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] \nl 
& \quad + \frac{16 L^2 (1 + \gamma^2 \bw^2)}{\rho \gamma} (1 - \rho \gamma)^2 n \, \eta^2 \, \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t ) }^2 \right] \nl 
& \quad + \left( \frac{6}{\rho \gamma} \left( 1 + \gamma^2 \bw^2 \right) n + \frac{8}{\rho \gamma} \eta^2 (1 + \gamma^2 \bw^2) L^2 (1 - \rho \gamma)^2 \right) \sigma^2 
\end{align*}
Using the step size condition 
\begin{equation} \label{eq:eta_gcond_b}
\eta \leq \frac{ \rho \gamma }{ 8L (1-\rho\gamma) \sqrt{1 + \gamma^2 \bw^2} }, \quad \gamma \leq \frac{1}{8 \bw}
\end{equation}
implies that 
\begin{align*}
\mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] & \leq \left( 1 - \frac{ \rho \gamma }{4} \right) \mathbb{E} \left[ \norm{ G_o^t }_F^2 \right] + \gamma \frac{ 2 \bw^2}{ \rho } \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] + \gamma \, \left( \frac{\rho}{4} + \frac{18 L^2 \bw^2}{\rho} \right) \, \mathbb{E} \left[ \norm{ \Theta_o^t}_F^2 \right] \nl 
& \quad + \gamma \, \frac{9 L^2}{ \rho } \bw^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] + \gamma \, \frac{ \rho n}{4} \, \mathbb{E} \left[ \norm{ \nabla f( \avgtheta^t ) }^2 \right] + \left( \frac{7n }{\rho \gamma} + \frac{\rho \gamma}{8} \right) \sigma^2 .
\end{align*}
This concludes our proof.
\end{proof}

\subsubsection{Proofs for the Auxilliary Lemmas}

\paragraph{Proof of \Cref{lem:stoc}} Observe that:
\begin{align*}
\mathbb{E} \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] & = \mathbb{E} \left[ \norm{ (\grdSF^{t+1} - \grdF^{t+1}) - (\grdSF^t - \grdF^t) }_F^2 + \norm{ \grdF^{t+1} - \grdF^t }_F^2 \right] \nl 
& \quad + 2 \mathbb{E}\left[ \dotp{ (\grdSF^{t+1} - \grdF^{t+1}) - (\grdSF^t - \grdF^t) }{ \grdF^{t+1} - \grdF^t } \right] \nonumber
% & \leq 2 n \sigma^2 + L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] \nonumber
\end{align*}
Notice that 
\[ 
\mathbb{E}\left[ \dotp{ (\grdSF^{t+1} - \grdF^{t+1}) - (\grdSF^t - \grdF^t) }{ \grdF^{t+1} - \grdF^t } \right] 
= - \mathbb{E}\left[ \dotp{ \grdSF^t - \grdF^t }{ \grdF^{t+1} - \grdF^t } \right] 
\] 
As such,
\begin{align*}
\mathbb{E} \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] & \leq 3 n \sigma^2 + 2 L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right].
\end{align*}
This concludes the proof.\hfill $\square$

\paragraph{Proof of \Cref{lem:succ_diff}}
    Observe that:
\begin{align}
    & \norm{\Theta^{t+1} - \Theta^t}_F^2 = \norm{ \eta G^t - \gamma(\W-\I) \hat{\Theta}^{t+1} }_F^2 \nonumber \\
    % & = \norm{- \eta G^t + \gamma(\W-\I) \left[\hat{\Theta}^t + {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t; \zeta_0^{t+1} )\right]}_F^2 \nonumber \\
    % & = \norm{- \eta G^t + \gamma(\W-\I) \left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t; \xi_0^{t+1}) - (\Theta^t - \eta G^t-\hat{\Theta}^t) + \Theta^t - \eta G^t\right]}_F^2 \nonumber \\
    & = \norm{(I + \gamma (\W-\I)) (- \eta G^t) + \gamma(\W-\I)\Theta ^t + \gamma(\W-\I) \left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t)\right]}_F^2 \nonumber \\
    & \leq 2\norm{( I + \gamma(\W-\I) ) (- \eta G^t) + \gamma(\W-\I)\Theta^t}_F^2 \nonumber \\
    & \quad + 2\norm{\gamma(\W-\I)\left[ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t)\right]}_F^2 \nl 
    & \leq 2 \left[ \eta^2 \norm{ (I+\gamma(\W-\I)) G^t }_F^2 + \gamma^2 \norm{ (\W-\I) \Theta^t }_F^2 + 2 \dotp{( I + \gamma(\W-\I) ) (- \eta G^t)}{\gamma(\W-\I)\Theta^t}\right] \nl 
    & \quad + 2 \bw^2 \gamma^2 \norm{ {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ) - (\Theta^t - \eta G^t-\hat{\Theta}^t) }_F^2
\end{align}
Observe that $\dotp{( I + \gamma(\W-\I) ) (- \eta G^t)}{\gamma(\W-\I)\Theta^t} = \dotp{( I + \gamma(\W-\I) ) (- \eta \U G_o^t)}{\gamma(\W-\I)\Theta^t}$,
the above leads to
\begin{align}
    \mathbb{E}_t \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] & \leq 2\eta^2(1-\rho\gamma)^2\norm{G^t}_F^2 + 2\eta^2(1-\rho\gamma)^2\norm{G_o^t}_F^2 + 4\gamma^2\norm{(\W-\I)\Theta^t}_F^2 \nl 
    & \quad + 2 \bw^2 \gamma^2(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \label{int:Theta_diff_a_2}
\end{align}
% \begin{align}
%     (\W-\I)\Theta^t &\stackrel{\eqref{neq:avg_gap}}{=} (\W-\I)(U\Theta_0^t + \avgTheta^t) = (\W-\I)U\Theta_0^t \nonumber \\
%     \norm{ (\W-\I)\Theta^t}_F^2 &= \norm{(\W-\I)U\Theta_0^t}_F^2 \leq 4\norm{\Theta_0^t}_F^2 \label{int:Theta_diff_b}
% \end{align}
Notice that using \eqref{eq:avgg_norm}, we obtain
\begin{align}
    \mathbb{E}_t \left[ \norm{G^t}_F^2 \right] & = \mathbb{E}_t \left[ \norm{ ( (1/n) {\bf 11}^\top + \U\U^\top ) G^t }_F^2 \right] = \mathbb{E}_t \left[ \norm{  \U\U^\top G^t }_F^2 + \norm{ (1/n){\bf 11}^\top G^t }_F^2 \right]
    \nl & \leq \norm{G_o^t}_F^2 + 2L^2\norm{\Theta_o^t}_F^2 + 2 n  \norm{\nabla f(\avgtheta^t)}^2 + \sigma^2 \label{int:Theta_diff_c_2}
\end{align}
By combining \eqref{int:Theta_diff_a_2}, \eqref{int:Theta_diff_c_2}, and the fact $\norm{ (\W-\I)\Theta^t}_F^2 \leq \bw^2 \norm{\Theta_o^t}_F^2$, we have
\begin{align*}
    \mathbb{E}_t \left[ \norm{\Theta^{t+1} - \Theta^t}_F^2 \right] &\leq 4 \eta^2(1-\rho\gamma)^2\norm{G_o^t}_F^2 + 4 \eta^2(1-\rho\gamma)^2L^2 \norm{\Theta_o^t}_F^2 + 4 n  \eta^2(1-\rho\gamma)^2 \norm{\nabla f(\avgtheta^t)}^2
    \\ &\quad + 2 \eta^2(1-\rho\gamma)^2\sigma^2 + 4 \bw^2  \gamma^2\norm{\Theta_o^t}_F^2 + 2 \bw^2 \gamma^2(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2
\end{align*}
This concludes the proof. \hfill $\square$

\subsection{Proof of \Cref{lem:gt}}\label{app:lemgt}
We begin by observing the following recursion for $G^{t} - \hat{G}^t$:
\begin{align*}
G^{t+1} - \hat{G}^{t+1} & = G^t + \grdSF^{t+1} - \grdSF^t + ( \gamma (\W-\I) - \I ) \hat{G}^{t+1} \nl 
& = \gamma (\W-\I) ( G^t + \grdSF^{t+1} - \grdSF^t ) \nl 
& \quad + ( \gamma (\W-\I) - \I ) \left[ {\cal Q}( G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t  ) - (G^t + \grdSF^{t+1} - \grdSF^t - \hat{G}^t) \right] .
\end{align*}
This implies
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq (1 + \alpha_0) (1 + \gamma \bw)^2 (1 - \delta) \mathbb{E} \left[ (1+\alpha_1) \norm{ G^t- \hat{G}^t}_F^2 + (1 + \alpha_1^{-1}) \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] \nl 
& \quad + 2 (1 + \alpha_0^{-1}) \gamma^2 \bw^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 + \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] 
\end{align*}
Taking $\alpha_0 = \frac{\delta}{4}$, $\alpha_1 = \frac{\delta}{8}$ and the step size condition
\[
\gamma \leq \frac{ \delta }{ 8             \bw } \leq \frac{ \delta }{ 6 \bw }
\]
give
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl
& \quad + \left( (1 - \frac{\delta}{4})(1 + \frac{8}{\delta}) + 2 \gamma^2 \bw^2 (1 + \frac{4}{\delta} ) \right) \mathbb{E} \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] \nl 
& \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{10(1+\gamma^2\bw^2)}{\delta} \mathbb{E} \left[ \norm{ \grdSF^{t+1} - \grdSF^t }_F^2 \right] 
\end{align*}
Applying \Cref{lem:stoc} and \Cref{lem:succ_diff} gives
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10 \gamma^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{30(1+\gamma^2\bw^2)}{\delta} n \sigma^2 \nl
& \quad + \frac{20(1+\gamma^2\bw^2)L^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] \nl 
& \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10}{\delta} \left(  \gamma^2 \bw^2 + 8 \eta^2 L^2 (1 + \gamma^2 \bw^2) (1 - \rho \gamma)^2 \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
& \quad + \frac{80(1+\gamma^2\bw^2)L^2}{\delta} ( \eta^2 L^2 (1-\rho\gamma)^2 + \bw^2 \gamma^2 ) \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \nl
& \quad + \frac{40(1+\gamma^2\bw^2)L^2}{\delta} \bw^2 \gamma^2 (1 - \delta) \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{G}^t }_F^2 \right] \nl
& \quad + \frac{80(1+\gamma^2\bw^2)L^2}{\delta} n \eta^2 (1 - \rho\gamma)^2 \mathbb{E} \left[ \norm{\nabla f(\avgtheta^t)}^2 \right] \nl
& \quad + \frac{10(1+\gamma^2 \bw^2)}{\delta} \left( 4 L^2 \eta^2 (1 - \rho\gamma)^2 \sigma^2 + 3 n \sigma^2 \right)
\end{align*}
Using the step size condition from \eqref{eq:eta_gcond_b}, i.e., $\eta^2 L^2 (1-\rho \gamma)^2 (1+\gamma^2 \bw^2) \leq \frac{ \rho^2 \gamma^2 }{ 64 }$, simplifies the above to 
\begin{align*}
\mathbb{E} \left[ \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 \right] & \leq \left(1 - \frac{\delta}{8} \right) \mathbb{E} \left[ \norm{ G^{t} - \hat{G}^{t} }_F^2 \right] + \frac{10}{\delta} \gamma^2 \left( \bw^2 + \frac{\rho^2 }{8} \right) \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] \nl 
& \quad + \gamma^2 \, \frac{5}{4 \delta} \left( \rho^2 + 72 L^2 \bw^2 \right) \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] + \gamma^2 \, \frac{40 L^2 \bw^2}{\delta} \mathbb{E} \left[ \norm{ \Theta^t - \eta G^t - \hat{G}^t }_F^2 \right] \nl
& \quad + \gamma^2 \, \frac{5 \rho^2}{4 \delta} n \mathbb{E} \left[ \norm{\nabla f(\avgtheta^t)}^2 \right]  + \frac{5}{\delta} \left( \frac{\rho^2 \gamma^2}{8} + 7 n \right) \sigma^2
\end{align*}
This concludes our proof.






\subsection{Proof of \Cref{lemma:theta_comm}} \label{app:pf_t_comm}
Observe that 
\begin{align}
    & \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] = \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- (\hat{\Theta}^t + {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t ))}_F^2 \right] 
    \nl &= \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- (\Theta^t - \eta G^t) + (\Theta^t - \eta G^t - \hat{\Theta}^t) - {\cal Q}(\Theta^t - \eta G^t - \hat{\Theta}^t )}_F^2 \right]
    \nl & \leq (1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \Theta^t - \eta(G^{t+1}- G^t)}_F^2 \right] + (1+\frac{\delta}{2})(1-\delta)\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2
    \nl &\leq 2(1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{\Theta^{t+1}- \Theta^t}_F^2 \right] + 2\eta^2 (1+\frac{2}{\delta}) \mathbb{E}_t \left[ \norm{G^{t+1}- G^t}_F^2 \right]
     + (1-\frac{\delta}{2})\norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 
     \label{int:theta_comm_a_2}
\end{align}
We can bound the second term as 
\begin{align}
    \mathbb{E} \left[ \norm{G^{t+1} - G^t}_F^2 \right] & = \frac{1}{n} \mathbb{E} \left[ \norm{ {\bf 1}^\top ( \grdSF^{t+1} - \grdSF^t )}^2 \right] + 2 \mathbb{E} \left[ \norm{ G_o^{t+1} }_F^2 \right] + 2 \mathbb{E} \left[ \norm{ G_o^{t} }_F^2 \right] \nonumber
\end{align}
Observe that 
\begin{align}
\mathbb{E} \left[ \norm{ {\bf 1}^\top ( \grdSF^{t+1} - \grdSF^t )}^2 \right] & \leq \mathbb{E} \left[ \norm{ {\bf 1}^\top ( \grdSF^{t+1} - \grdF^{t+1} )}^2 \right] + 2 \mathbb{E} \left[ \norm{ {\bf 1}^\top ( \grdSF^{t} - \grdF^{t} )}^2 \right] \nonumber \\
& \quad + 2 \mathbb{E} \left[ \norm{ {\bf 1}^\top ( \grdF^{t+1} - \grdF^{t} )}^2 \right] \nl 
& \leq 3 n \sigma^2 + 2 n L^2 \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] \nonumber 
\end{align}
Substituting back into \eqref{int:theta_comm_a_2} yields
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq (1-\frac{\delta}{2}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + 4 \eta^2 ( 1 + \frac{2}{\delta}) \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 + \norm{G_o^t}_F^2 \right] \nl 
    & \quad + 2(1 + \frac{2}{\delta}) ( \eta^2 L^2 + 1 ) \mathbb{E} \left[ \norm{ \Theta^{t+1} - \Theta^t }_F^2 \right] + 6 \sigma^2 (1 + \frac{2}{\delta}) \eta^2 
\end{align}
We further apply \Cref{lem:succ_diff} to obtain
\begin{align}
    & \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] \leq (1-\frac{\delta}{2}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + 4 \eta^2 ( 1 + \frac{2}{\delta}) \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 + \norm{G_o^t}_F^2 \right] \nl 
    & \quad + 6 \sigma^2 (1 + \frac{2}{\delta}) \eta^2  + 4 \bw^2 ( 1 + \eta^2 L^2) (1 + \frac{2}{\delta}) \gamma^2 (1-\delta) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t }_F^2 \right] \nl 
    & \quad + 8 ( 1 + \eta^2L^2 ) (1 + \frac{2}{\delta}) \eta^2 (1 - \rho \gamma)^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 + L^2 \norm{\Theta_o^t}_F^2 + n \norm{ \nabla f( \avgtheta^t) }^2 + \sigma^2 / 2 \right] \nl 
    & \quad + 8( 1 + \eta^2 L^2 ) (1 + \frac{2}{\delta}) \bw^2 \gamma^2 \mathbb{E} \left[ \norm{ \Theta_o^t }_F^2 \right] \label{int:tx_int_2}
\end{align}
Using the step size condition:
\[ 
\gamma^2 \leq \frac{\delta}{ 16 \bw^2 (1-\delta) (1 + \eta^2 L^2) (1 + 2/\delta)} 
\]
and we recall that $\eta \leq 1/(4L)$, the upper bound in \eqref{int:tx_int_2} can be simplified as 
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq (1-\frac{\delta}{4}) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] + \eta^2 \frac{12}{\delta} \mathbb{E} \left[ \norm{G_o^{t+1}}_F^2 \right] \nl 
    & \quad + \frac{24}{\delta} \eta^2 \sigma^2 + \frac{38}{\delta} \eta^2 \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \frac{26}{\delta} \mathbb{E} \left[ ( \eta^2 L^2 + \bw^2 \gamma^2 ) \norm{\Theta_o^t}_F^2 + \eta^2 n \norm{ \nabla f( \avgtheta^t) }^2 \right] \nonumber
\end{align}
This concludes the proof of the first part. 

The above bound can be combined with \Cref{lemma:g_cons} and $\gamma \rho \leq 1$ to give
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{4} + \eta^2 \gamma \frac{108 \bw^2 L^2}{\rho \delta} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] \nl 
    & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \gamma \frac{24 \bw^2}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hat{G}^t}_F^2 \right] \nl 
    & \quad + \left[ \frac{\eta^2 L^2}{\delta} \left( 3 + 26L^2 + \frac{216 \gamma L^2 \bw^2}{\rho} \right) + \frac{26}{\delta} \bw^2 \gamma^2 \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \nl 
    & \quad + \frac{29 \eta^2}{\delta} \, n \, \mathbb{E}  \left[ \norm{ \nabla f( \avgtheta^t) }^2 \right] + \frac{24 \eta^2 \sigma^2}{\delta} \left( 1 + \frac{4n}{\rho \gamma} \right) \nonumber
\end{align}
Taking  $\eta^2 \gamma \leq \frac{\delta^2 \rho}{864 \bw^2 L^2}$ simplifies the bound into
\begin{align}
    \mathbb{E} \left[ \norm{\Theta^{t+1}- \eta G^{t+1}- \hat{\Theta}^{t+1}}_F^2 \right] & \leq \left(1-\frac{\delta}{8} \right) \mathbb{E} \left[ \norm{\Theta^t - \eta G^t - \hat{\Theta}^t}_F^2 \right] \nl 
    & \quad + \eta^2 \frac{50}{\delta} \mathbb{E} \left[ \norm{G_o^t}_F^2 \right] + \eta^2 \frac{3 \bw}{\rho} \mathbb{E} \left[ \norm{G^{t} - \hat{G}^t}_F^2 \right] \nl 
    & \quad + \left[ \eta^2 \, \frac{29 L^4}{\delta} \left( 1 + \frac{ \bw}{\rho} \right) + \frac{26}{\delta} \bw^2 \gamma^2 \right] \mathbb{E} \left[ \norm{\Theta_o^t}_F^2 \right] \nl 
    & \quad + \eta^2 \, \frac{29n }{\delta} \, \mathbb{E}  \left[ \norm{ \nabla f( \avgtheta^t) }^2 \right] + \eta^2\, \frac{24 \sigma^2}{\delta} \left( 1 + \frac{4n}{\rho \gamma} \right) \nonumber
\end{align}
This concludes the proof.

\subsection{Proof of \Cref{lem:combined}} \label{app:pf_combined}
Let $a, b, c > 0$ be some constants to be determined later, combining \Cref{lem:theta_o_new}, \ref{lemma:g_cons}, \ref{lem:gt}, \ref{lemma:theta_comm} yields
\begin{align}
& \mathbb{E} \left[ \norm{\Theta_o^{t+1}}_F^2 + a \norm{ G_o^{t+1} }_F^2 + b \norm{ G^{t+1} - \hat{G}^{t+1}}_F^2 + c \norm{ \Theta^{t+1} - \eta G^{t+1} - \hat{\Theta}^{t+1} }_F^2 \right] \nl
& \leq \left( 1 - \frac{\rho \gamma}{2} + a \gamma ( \frac{\rho}{4} + \frac{18 L^2 \bw^2}{\rho} ) + b \gamma^2 \frac{2 }{\delta} ( \rho^2 + 45 L^2 \bw^2 ) + \frac{c}{\delta} \left[ 29 L^4 (1 + \frac{\bw}{\rho}) \eta^2 + 26 \bw^2 \gamma^2 \right] \right) \mathbb{E} \left[ \norm{\Theta_o^{t}}_F^2 \right] \nl 
& \quad + a \left( 1 - \frac{\rho \gamma}{4} + \frac{1}{a} \eta^2  \frac{2}{\rho \gamma} + \frac{b}{a} \gamma^2 \frac{10}{\delta^2} (\delta \bw^2 + \rho^2) + \frac{c}{a} \eta^2 \frac{50}{\delta} \right) \mathbb{E} \left[ \norm{G_o^{t}}_F^2 \right] \nl 
& \quad + b \left( 1 - \frac{\delta}{8} + \frac{a}{b} \gamma \frac{2 \bw^2}{\rho} + \frac{c}{b} \eta^2 \frac{3 \bw^2}{\rho} \right) \mathbb{E} \left[ \norm{ G^t - \hat{G}^t }_F^2 \right] \nl
& \quad + c \left( 1 - \frac{\delta}{8} + \frac{1}{c} \gamma \frac{\bw^2}{\rho} + \frac{a}{c} \gamma \frac{9L^2 \bw^2}{\rho} + \frac{b}{c} \gamma^2 \frac{40 L^2 \bw^2}{\delta} \right) \mathbb{E} \left[ \norm{ \Theta^{t} - \eta G^{t} - \hat{\Theta}^{t} }_F^2 \right] \nl 
& \quad + 8n \sigma^2 \left[ a \frac{1}{\rho \gamma} + b \frac{5}{\delta} + c \eta^2 \frac{15}{\rho \delta \gamma} \right]
+ \left[ a \gamma \frac{\rho}{4} + b \gamma^2 \frac{2 \rho^2}{\delta} + c \eta^2 \frac{29}{\delta} \right] n \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^t ) }^2 \right] 
\nonumber
\end{align}
We wish to find conditions on the step sizes and $a,b,c$ such that
\begin{align*}
    & 1 - \frac{\rho \gamma}{2} + a \gamma \frac{19 L^2 \bw^2}{\rho} + b \gamma^2 \frac{92 L^2 \bw^2}{\delta} + \frac{c}{\delta} \left[ \frac{ 58 L^4 \bw}{\rho} \eta^2 + 26 \bw^2 \gamma^2 \right] \leq 1 - \frac{\rho\gamma}{4}, \\
    & 1 - \frac{\rho \gamma}{4} + \frac{1}{a} \eta^2  \frac{2}{\rho \gamma} + \frac{b}{a} \gamma^2 \frac{10}{\delta^2} (\delta \bw^2 + \rho^2) + \frac{c}{a} \eta^2 \frac{50}{\delta} \leq 1 - \frac{ \rho \gamma }{ 8 }, \\ 
    & 1 - \frac{\delta}{8} + \frac{a}{b} \gamma \frac{2 \bw^2}{\rho} + \frac{c}{b} \eta^2 \frac{3 \bw^2}{\rho} \leq 1 - \frac{\delta \gamma}{8}, \\
    & 1 - \frac{\delta}{8} + \frac{1}{c} \gamma \frac{\bw^2}{\rho} + \frac{a}{c} \gamma \frac{9L^2 \bw^2}{\rho} + \frac{b}{c} \gamma^2 \frac{40 L^2 \bw^2}{\delta} \leq 1 - \frac{ \delta \gamma }{ 8 }.
\end{align*}
The above set of inequalities can be guaranteed if $a,b,c$ satisfy 
\begin{align}
    & \frac{48}{\rho^2 \gamma^2} \eta^2 \leq a \leq \frac{\rho^2}{228 L^2 \bw^2} \label{eq:acond} \\
    & \max\left\{ a \frac{\gamma}{1-\gamma} \frac{32 \bw^2}{\rho \delta} , c \frac{\eta^2}{1-\gamma} \frac{ 48 \bw^2 }{ \rho \delta }  \right\} \leq b \leq \min \left\{ \frac{1}{\gamma} \frac{ \rho \delta }{ 1104 L^2 \bw^2 }, \frac{\eta^2}{\gamma^3} \frac{\delta^2}{5 \rho ( \delta \bw^2 + \rho^2 ) } \right\} \label{eq:bcond} \\
    & \max\left\{ \frac{\gamma}{1-\gamma} \frac{24 \bw^2}{\rho \delta}, a \frac{\gamma}{1-\gamma} \frac{216 L^2 \bw^2 }{\rho \delta}, b \frac{\gamma^2}{1-\gamma} \frac{960 L^2 \bw^2 }{\rho \delta} \right\} \leq c \leq \min \left\{ \frac{ \rho \delta \gamma }{ 12 ( \frac{58 L^4 \bw}{ \rho } \eta^2 + 26 \bw^2 \gamma^2 ) }, \frac{\delta}{ 25 \rho \gamma } \right\} \label{eq:ccond}
\end{align}
Notice that the step size condition:
\[
\eta^2 \leq \frac{ \rho^4 \gamma^2 }{ 10944 L^2 \bw^2 }
\]
guarantees the existence of $a$ which satisfies \eqref{eq:acond}. In particular, we take $a = \frac{ 48 }{ \rho^2 \gamma^2 } \eta^2$. This simplifies \eqref{eq:bcond}, \eqref{eq:ccond} into
\begin{align*}
& \frac{48 \bw^2}{\rho \delta (1-\gamma)} \eta^2 \max\left\{ \frac{32}{ \rho^2 \gamma }, c \right\} \leq b \leq \min \left\{ \frac{1}{\gamma} \frac{ \rho \delta }{ 1104 L^2 \bw^2 }, \frac{\eta^2}{\gamma^3} \frac{\delta^2}{5 \rho ( \delta \bw^2 + \rho^2 ) } \right\} \\
& \frac{24 \bw^2}{\rho \delta (1-\gamma)} \max\left\{ \gamma , \frac{ 432 }{ \rho^2 \gamma } L^2 \eta^2, 40 L^2 \gamma^2 b \right\} \leq c \leq \min \left\{ \frac{ \rho \delta \gamma }{ 12 ( \frac{58 L^4 \bw}{ \rho } \eta^2 + 26 \bw^2 \gamma^2 ) }, \frac{\delta}{ 25 \rho \gamma } \right\}
\end{align*}
Observing that as $\eta^2 \leq \frac{ \rho^4 \gamma^2 }{ 10944 L^2 \bw^2 } \leq \frac{ \rho^2 \gamma^2 }{ 432 L^2 }$ and we impose the extra condition $c \leq \frac{32}{\rho^2 \gamma}$. We obtain the simplification:
\begin{align*}
& \frac{1536 \bw^2}{\rho^3 \delta \gamma (1-\gamma)} \eta^2 \leq b \leq \min \left\{ \frac{1}{\gamma} \frac{ \rho \delta }{ 1104 L^2 \bw^2 }, \frac{\eta^2}{\gamma^3} \frac{\delta^2}{5 \rho ( \delta \bw^2 + \rho^2 ) } \right\} \\
& \frac{24 \bw^2}{\rho \delta (1-\gamma)} \max\left\{ \gamma , 40 L^2 \gamma^2 b \right\} \leq c \leq \min \left\{ \frac{ \rho \delta \gamma }{ 12 ( \frac{58 L^4 \bw}{ \rho } \eta^2 + 26 \bw^2 \gamma^2 ) }, \frac{\delta}{ 25 \rho \gamma }, \frac{32}{\rho^2 \gamma} \right\}
\end{align*}
Again, the condition on $b$ is feasible if 
\[
\eta^2 \leq \frac{ \rho^4 \delta^2 (1-\gamma) }{ 1536 \times 1104 \times L^2 \bw^4 }, ~~ \frac{\gamma^2}{1-\gamma} \leq \frac{ \rho^2 \delta^3 }{ 7680 \bw^2 ( \delta \bw^2 + \rho^2 ) }
\]
and we take $b = \frac{1536 \bw^2}{\rho^3 \gamma \delta (1-\gamma)} \eta^2$. Note that as $\gamma \leq \delta / 8 \bw$, the bound on $\gamma$ can be implied by:
\[
\gamma^2 \leq \frac{ \rho^2 \delta^3 (1 - \delta/8\bw) }{ 7680 \bw^2 ( \delta \bw^2 + \rho^2 ) }
\]
Observe that with this choice of $b$ and the step size condition, we have $40 L^2 \gamma^2 b \leq \gamma$. Finally, the condition on $c$ is simplified to 
\begin{align*}
    \frac{24 \bw^2}{\rho \delta (1-\gamma)} \gamma \leq c \leq \min \left\{ \frac{ \rho \delta \gamma }{ 12 ( \frac{58 L^4 \bw}{ \rho } \eta^2 + 26 \bw^2 \gamma^2 ) }, \frac{\delta}{ 25 \rho \gamma }, \frac{32}{\rho^2 \gamma} \right\}
\end{align*}
The above condition is feasible if 
\[
\frac{\gamma^2}{1-\gamma} \leq \min \left\{ \frac{4 \delta}{3 \rho \bw^2}, \frac{ \delta^2 }{ 600 \bw^2 }, \frac{ \rho^2 \delta^2 }{ 14976 \bw^4 } \right\} = \frac{ \rho^2 \delta^2 }{ 14976 \bw^4 } , ~~\eta^2 \leq \frac{ \rho^3 \delta^2 (1-\gamma) }{ 16704 L^4 \bw^3 }  
\]
and we take $c = \frac{24 \bw^2}{\rho \delta (1-\gamma)}$. 

The above choice of $a,b,c$ ensures that
\begin{align}
& \mathbb{E} \left[ \norm{\Theta_o^{t+1}}_F^2 + a \norm{ G_o^{t+1} }_F^2 + b \norm{ G^{t+1} - \hat{G}^{t+1} }_F^2 + c \norm{ \Theta^{t+1} - \eta G^{t+1} - \hat{\Theta}^{t+1} }_F^2 \right] \nl
& \leq \left( 1 - \min\left\{ \frac{\rho}{8}, \frac{ \delta }{8} \right\} \gamma \right) \mathbb{E} \left[ \norm{\Theta_o^{t}}_F^2 + a \norm{ G_o^{t} }_F^2 + b \norm{ G^{t} - \hat{G}^{t} }_F^2 + c \norm{ \Theta^{t} - \eta G^{t} - \hat{\Theta}^{t} }_F^2 \right] \nl 
& \quad + \frac{192n}{1-\gamma} \left[ \frac{ 2(1-\gamma) }{ \rho^3 \gamma^3 } + \frac{320 \bw^2}{\rho^3 \delta^2 \gamma } + \frac{15 \bw^2}{\rho^2 \delta^2 \gamma } \right] \sigma^2 \eta^2
+ \frac{12n}{1-\gamma} \left[ \frac{ 1-\gamma }{ \rho \gamma } + \frac{256 \bw^2}{\rho \delta^2 } \gamma +  \frac{58 \bw^2 }{\rho \delta^2 } \right]  \eta^2 \mathbb{E} \left[ \norm{ \nabla f( \bar{\theta}^t ) }^2 \right] \nonumber
\end{align}
The proof is completed. 

 \newpage
\section{Additional Numerical Results}\label{app:more_plots}

In the first two sections we provide additional plots and the tuned hyper-parameters of our simulation. In the last section we describe the implementation details of our simulation.
\subsection{Synthetic Dataset}
This dataset is generated from the benchmark framework {\tt leaf} \citep{caldas2019leaf}. The number of data points possessed by each agent is different. In particular, we have the distribution $\{ m_i \}_{i=1}^{25}$ which follows [470, 403, 91, 84, 79, 51, 51, 38, 31, 25, 24, 19, 14, 10, 9, 6, 6, 5, 5, 4, 4, 4, 4, 3, 3]. \Cref{app:hypprm_syn} provides the tuned parameters used for the experiment in Figure~\ref{fig:syn}.

\begin{table}[h]
    \centering
\caption{Tuned hyper-parameters for linear model on synthetic dataset.
} \label{app:hypprm_syn}
\begin{tabular}{lccccc}
    \hline 
    \bfseries Algorithms & \begin{tabular}{@{}l@{}} {\bfseries Learning} \\ {\bfseries rate $\eta$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Consensus} \\ {\bfseries step size $\gamma$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Momentum} \\ {\bfseries param.~$\beta$  }\end{tabular}
    & \bfseries Batch size \\
    \hline
    {\tt GNSD} & 0.005 & - & - & 4 \\
    {\tt DeTAG} $(R=1)$ & 0.005 & 0.1 & - & 4 \\
    {\tt GT-HSGD} & 0.01 & - & 0.01 & 2 \\
    {\tt CHOCO-SGD} (Top-k 10\%) & 0.01 & 0.5 & - & 4 \\
    {\tt CHOCO-SGD} (Random Quant. 8bits) & 0.01 & 0.9 & - & 4 \\
    {\tt BEER} (Top-k 5\%) & 0.01 & 0.16 & - & 100 \\
    {\tt BEER} (Ramdon Quant. 4bits) & 0.01 & 0.5 & - & 100 \\
    \aname~(Top-k 5\%) & 0.01 & 0.2 & 0.01 & 2 \\
    \aname~(Random Quant. 4bits) & 0.01 & 0.6 & 0.01 & 2 \\
    \hline
\end{tabular}
\end{table}

In Figure~\ref{fig:syn_app}, we provide additional numerical results on the trajectories of gradient norm, training/testing accuracy of the algorithms. Similar comparisons between \aname~and existing algorithms are observed. Notably, we see that \aname~achieves the best gradient stationary solution in limited communication budget and recovers the same level of gradient stationary solution as the uncompressed {\tt GT-HSGD} at the last iteration. Also, we observe that \aname~is the first to achieve the best accuracy with the least network cost.

\begin{figure}[hbtp]
\centering
    \includegraphics[width=0.425\textwidth]{figures/syn_loss_it.pdf}
    \includegraphics[width=0.425\textwidth]{figures/syn_cons_it.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/syn_train_grad_net.pdf}
    \includegraphics[width=0.32\textwidth]{figures/syn_train_grad_it.pdf}
    \includegraphics[width=0.32\textwidth]{figures/syn_train_grad_sample.pdf}\\
    % \includegraphics[width=0.33\textwidth]{figures/syn_train_acc_net.pdf}
    % \includegraphics[width=0.33\textwidth]{figures/syn_train_acc_it.pdf}
    % \includegraphics[width=0.33\textwidth]{figures/syn_train_acc_sample.pdf}\\
    % \includegraphics[width=0.33\textwidth]{figures/syn_test_acc_net.pdf}
    % \includegraphics[width=0.33\textwidth]{figures/syn_test_acc_it.pdf}
    % \includegraphics[width=0.33\textwidth]{figures/syn_test_acc_sample.pdf}
    \caption{\textbf{Additional Results on Synthetic Data and Linear Model.} Loss and consensus gap against iterations, and worst-agent's gradient norm, training/testing accuracy.}\vspace{-.2cm} \label{fig:syn_app}
\end{figure}

\begin{samepage}
\subsection{1 Layer Feed-forward Network on MNIST Dataset}
\Cref{app:hypprm_ffdl} summarizes the tuned hyper parameters used by the experiment in Figure~\ref{fig:real_main}. We provide additional plots against wall clock time to demonstrate the practical improvement one can achieve using \aname.
\begin{table}[h]
\vspace{-.4cm}
    \centering
\caption{Tuned hyper-parameters for 1 layer feed-forward network on MNIST.
}\label{app:hypprm_ffdl}
\begin{tabular}{lccccc}
    \hline 
    \bfseries Algorithms & \begin{tabular}{@{}l@{}} {\bfseries Learning} \\ {\bfseries rate $\eta$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Consensus} \\ {\bfseries step size $\gamma$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Momentum} \\ {\bfseries param.~$\beta$  }\end{tabular}\\
    \hline
    {\tt CHOCO-SGD} (Top-k 10\%) & 0.01 & 0.3 & - \\
    {\tt BEER} (Top-k 5\%) & 0.01 & 0.2 & - \\
    \aname~(Top-k 5\%) & 0.01 & 0.2 & 0.01 \\
    \hline
\end{tabular}
\vspace{-.4cm}
\end{table}

\begin{figure}[hbtp]
\centering
    \includegraphics[width=0.99\textwidth]{figures/ffdl_legend_time.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/ffdl_loss_time.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_cons_time.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/ffdl_train_acc_time.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_test_acc_time.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_train_grad_time.pdf}\\ \vspace{.4cm}
    \includegraphics[width=0.99\textwidth]{figures/ffdl_legend_net.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/ffdl_loss_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_cons_sample.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/ffdl_train_acc_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_test_acc_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/ffdl_train_grad_sample.pdf}\\
    \caption{\textbf{Additional Results on MNIST Data with Feed-forward Network.} Worst-agent's loss, consensus gap, training/testing accuracy and gradient norm against wall clock time in seconds and the number of samples drawn. In the legend of wall clock time plots we denoted the number of iterations used for training.} \label{fig:ffdl_app} \vspace{-.4cm}
\end{figure}
\end{samepage}

% \begin{figure*}
%     \centering 
    
%     \caption{\textbf{Additional Results on MNIST Data with Feed-forward Network.} Worst-agent's train loss values, train accuracy, consensus gap and train gradient norm against the number of samples drawn.} \label{fig:ffdl_app_2}
%     % \caption{\textbf{Experiments on FEMNIST Data with LeNet-5.} Worst-agent's loss value against the communication cost, i.e., number of 32-bit floats transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_main}
% \end{figure*}

\subsection{LeNet-5 on FEMNIST Dataset}
We conduct another practical experiment and consider training a LeNet-5 neural network which has $d=60850$ parameters. The dataset contains $m=805263$ samples of $28 \times 28$ hand-written character images, each belongs to one of the 62 classes. The samples are partitioned into $n=36$ agents (of ring topology) according to the groups specified in \citep{caldas2019leaf}.
We scheduled the learning rate at its initial value for the initial stage, then decay by a factor of $10^{-1}$ at certain iterations. This allows us to perform large step size training at the initial stage and arrive at a consensual solution eventually.

% In Figure~\ref{fig:real_app}, we provide additional numerical results on the trajectories of objective loss, consensus gap, gradient norm, training/testing accuracy of the algorithms. Notice that one epoch consists of 200 iterations. 

Additionally, we observe that when training LeNet-5 using momentum-based variance reduced algorithms (including {\tt GT-HSGD} and \aname), we found that the algorithms can be unstable when a small momentum parameter (e.g., $\beta = 0.1$) is adopted, unlike the experiments on synthetic data / MNIST. We suspect that this is due to the stronger requirements on Lipschitz continuity of the gradient of objective function in \Cref{ass:lips}, i.e., we require\vspace{-.2cm}
\footnote{This assumption is known as the mean squared smoothness condition \citep{arjevani2019lower}. Specifically, this assumption is only used in the proof of \Cref{lem:vt_bound}.
% The mean squared smoothness has a better dependence on $L$ for distributions that are regular, whereas the per sample smoothness can easily have worse dependence on $L$ for distributions that have rarely existing non-smooth samples (i.e., sample that has large $L$ but small probability mass). 
The analysis for $\beta=1$ can be further relaxed to the typical smoothness assumption $\| \mathbb{E}[ \nabla f_i( \theta; \zeta )] - \mathbb{E}[ \nabla f_i( \theta'; \zeta )] \|^2 \leq L^2 \| \theta - \theta' \|^2$, applied in the proof of \Cref{lem:stoc}.}
% \footnote{or as mentioned in Footnote~\ref{foot:lips}, the condition can be slightly relaxed into $\mathbb{E}_\zeta[ \| \nabla f_i( \theta; \zeta) - \nabla f_i(\theta'; \zeta) \|^2 ] \leq L^2 \| \theta - \theta' \|^2$.}
{\revision
\begin{equation*}
    \mathbb{E}_{\zeta} \left[ \norm{ \nabla f_i( \theta; \zeta ) - \nabla f_i( \theta'; \zeta ) }^2 \right] \leq L^2 \norm{ \theta - \theta' }^2,~\forall~ \theta, \theta' \in \mathbb{R}^d. 
\end{equation*}
}
This is stronger than the typical Lipschitz continuity on the \emph{expected gradient} which only demands $\| \mathbb{E}[ \nabla f_i( \theta; \zeta )] - \mathbb{E}[ \nabla f_i( \theta'; \zeta )] \| \leq L \| \theta - \theta' \|$. 
% large Liptschitz smoothness constant in \Cref{ass:lips} required by these
% poorly smooth loss 
Particularly, the convergence of these momentum-based algorithms depend on the less smooth loss landscape from the neural network model and data distribution.

\begin{table}[h]
\vspace{-.3cm}
    \centering
\caption{Tuned hyper-parameters for LeNet-5 on FEMNIST. {\revision Numbers on the $\eta, \beta$ scaling schedule columns indicate the iteration number (in thousands) when $\eta, \beta$ are scaled by 0.1 correspondingly.}}\label{app:hypprm_lenet}
{\revision
\begin{tabular}{lcccccc}
    \hline 
    \bfseries Algorithms & \begin{tabular}{@{}l@{}} {\bfseries Learning} \\ {\bfseries rate $\eta$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Consensus} \\ {\bfseries step size $\gamma$ }\end{tabular} 
    & \begin{tabular}{@{}l@{}} {\bfseries Momentum} \\ {\bfseries param.~$\beta$  }\end{tabular}
    & \bfseries \begin{tabular}{@{}l@{}} {\bfseries Batch} \\ {\bfseries size }\end{tabular} 
    & \bfseries \begin{tabular}{@{}l@{}} {\bfseries $\eta$ scaling} \\ {\bfseries schedule }\end{tabular}
    & \bfseries \begin{tabular}{@{}l@{}} {\bfseries $\beta$ scaling} \\ {\bfseries schedule }\end{tabular}\\
    \hline
    {\tt GNSD} & 0.02 & - & - & 32 & 4, 20, 35 & - \\
    {\tt GT-HSGD} & 0.02 & - & 0.3 & 16 & 4, 20, 35 & 10, 20, 35 \\
    {\tt CHOCO-SGD} (Top-k 10\%) & 0.01 & 0.25 & - & 32 & 4, 40, 80 & - \\
    {\tt BEER} (Top-k 5\%) & 0.01 & 0.15 & - & 32 & 4, 40, 80 & - \\
    \aname~(Top-k 5\%) & 0.01 & 0.2 & 0.3 & 16 & 4, 40, 80 & 10, 40, 80 \\
    \hline
\end{tabular}
}
\end{table}



% \begin{figure}[hbtp]
% \centering
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_loss_net.pdf}
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_loss_it.pdf}\\
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_cons_net.pdf}
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_cons_it.pdf} \\
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_train_grad_net.pdf}
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_train_grad_it.pdf}\\
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_train_acc_net.pdf}
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_train_acc_it.pdf}\\
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_test_acc_net.pdf}
%     \includegraphics[width=0.38\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/dl_test_acc_it.pdf}\\
%     \vspace{-.2cm}
%     \caption{\textbf{Additional Results on FEMNIST Data with LeNet-5.} Worst-agent's gradient norm, training/testing accuracy against the number of bits transmitted (left) and iteration no. (right).}\vspace{-.2cm} \label{fig:real_app}
% \end{figure}

\begin{figure}[hbtp]
\vspace{-.4cm}
\centering
    \includegraphics[width=0.99\textwidth]{figures/le2dl_legend_net.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/le2dl_loss_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/le2dl_cons_sample.pdf}\\
    \includegraphics[width=0.32\textwidth]{figures/le2dl_train_acc_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/le2dl_test_acc_sample.pdf}
    \includegraphics[width=0.32\textwidth]{figures/le2dl_train_grad_sample.pdf}\\
    \caption{\textbf{Additional Results on FEMNIST Data with LeNet-5.} Worst-agent's loss, consensus gap, training/testing accuracy and gradient norm against the number of samples drawn.} \label{fig:ledl_app} \vspace{-.4cm}
\end{figure}

% \newpage
{\redtmp \subsection{Implementation Details}
\textbf{Compression Operators.} \label{app:subsec:rescale}
We adopt either a greedy biased compressor Top-k (sparsification) or a {\revision re-scaled} random compressor (quantization) \citep{alistarh2017qsgd} in our experiment for algorithms with compressed communication. Specifically, a {\revision re-scaled} $b$-bits random quantization \citep{koloskova2019decentralized} on $x\in\mathbb{R}^d$ {\revision that satisfies Assumption \ref{ass:compress}} can be described as
\begin{equation}\mathcal{Q}(x_i; \xi_i) = \frac{\text{sign}(x_i) \, \| x \|_2 \, \xi_i}{{\revision \tau}}, \quad
    \xi_i = 
    \begin{cases}
        \ell_i / 2^b &\text{w.p.}~\ 1- p(|x_i|/\|x\|_2, 2^b); \\
        (\ell_i +1)/ 2^b &\text{otherwise}.
    \end{cases}
\end{equation}

with level $\ell_i$ of $x_i$ satisfying $|x_i|/\|x\|_2 \in [\ell_i / 2^b, (\ell_i + 1)/ 2^b]$, {\revision $\tau = 1 + \min\{ d / (2^b)^2, \sqrt{d} / 2^b \}$}, $p(a,s) = as - \ell$. 
{\revision With the unbiasedness of random quantization \citep{alistarh2017qsgd} that gives $\mathbb{E}_\xi[\mathcal{Q}(x;\xi)] = \frac{1}{\tau} x$, and with the proof in Appendix A.1 of \citep{alistarh2017qsgd} that gives $\mathbb{E}_\xi[\| \mathcal{Q}(x; \xi) \|^2] \leq \frac{1}{\tau} \|x\|^2$,
\begin{equation}
\mathbb{E}_\xi[\| \mathcal{Q}(x) - x \|^2] = \mathbb{E}_\xi[\|\mathcal{Q}(x;\xi) \|^2] + \|x \|^2 - 2 \mathbb{E}_\xi[\mathcal{Q}(x;\xi)]^\top x \leq (\frac{1}{\tau} +1-\frac{2}{\tau}) \|x\|^2 = (1-\frac{1}{\tau})\|x\|^2. 
\end{equation}
Therefore, a re-scaled random quantizer satisfies Assumption \ref{ass:compress} with $\delta = 1/ \tau$.
Moreover, as pointed out in \citep[Section 3.5]{koloskova2019decentralized}, the above re-scaling trick can be applied on any unbiased compressor ${\cal Q}(\cdot)$ satisfying $\mathbb{E}_\xi[\| \mathcal{Q}(x; \xi) \|^2] \leq \tau \|x\|^2$ such that the rescaled compressor $(1/\tau){\cal Q}(\cdot)$ satisfies \Cref{ass:compress} with $\delta = 1/\tau$.
}

For Top-$k$ compressor, the communication cost per iteration is $k\cdot b_{\sf pre}$ bits, where $b_{\sf pre}$ is the number of bits for representing a full-precision scalar, added to the cost for sending $k$ indices. For random quantization with $b$ bits, the communication cost per iteration is $(b+1)d$ bits added to the cost of sending the $\ell_2$-norm of the vector as a full-precision floating point scalar.

\begin{table}[h]
\caption{\redtmp Comparison of decentralized optimization algorithms on computational complexity per iteration and node. $b$ denotes the batch size used at every iteration. $b_0$ denotes the initial batch size. $R$ denotes the potentially multiple rounds of gossip for {\tt DeTAG}. $d$ denotes the model dimension. $^\dagger${\tt BEER} uses large batch size of $\mathcal{O}(1/\epsilon^2)$.}
\label{tab:comp-compl}
\begin{center}
{\redtmp
\begin{tabular}{lllll}
\hline
\textbf{Algorithms} & \textbf{Init. Stoch. Grad.} & \textbf{Stoch. Grad.} & \textbf{Gossip Comm. Round} & \textbf{Memory Usage} \\ \hline
{\tt GNSD}      & $b$  & $b$  & $2$                 & $3d$ \\
{\tt DeTAG}     & $b$  & $b$  & $R$                 & $3d$ \\
{\tt GT-HSGD}   & $b_0$ & $2b$ & $2$                 & $5d$ \\
{\tt CHOCO-SGD} & $b$  & $b$  & $1$ (+ Compress) & $3d$ \\
{\tt BEER}     & $b^\dagger$ & $b^\dagger$ & $2$ (+ Compress) & $7d$ \\ % for initial batch size see Lemma B.3. of BEER.
{\tt DoCoM}     & $b_0$ & $2b$ & $2$ (+ Compress) & $9d$ \\ \hline
\end{tabular}}
\end{center}
\end{table}

\textbf{Memory Efficient Implementation.} From line \ref{line:theta} and \ref{line:G} of Algorithm \ref{alg:docom}, we observe that {\tt DoCoM} relies on the sum $\sum_j W_{ij} \widehat{\theta}_{i,j}^t$ and $\sum_j W_{ij} \widehat{g}_{i,j}^t$. Similar to the steps described in Appendix E of \citep{koloskova2019decentralized} for the {\tt CHOCO-SGD} algorithm, the {\aname}~algorithm can be implemented with a per-node memory complexity of ${\cal O}(d)$. See Table \ref{tab:comp-compl} for details.
}

% {\redtmp
% \section{Improving $\sigma$ Dependence}
% We demonstrate that with a careful choice of the step sizes in Theorem \ref{th:main}, we can derive an improved convergence rate with optimal dependence on $\sigma$. Consider the following step sizes choice
% \begin{equation}
%     \beta = \frac{n^{1/3}}{\sigma^{4/3} T^{2/3}}, \ \eta = \frac{ (L(f(\bar{\theta}^0) - f^*))^{1/3}n^{2/3}}{ L^{2/3} T^{1/3}}, \ \gamma = \gamma_\infty, \ b_0 = \frac{\sigma^{8/3} T^{1/3}}{n^{2/3}}
% \end{equation}
% Denote $\Delta_f :=f(\bar{\theta}^0) - f^*$. Substituting the above step sizes into Theorem~\ref{th:main} shows that
% \begin{equation}
%     \frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ \norm{ \nabla f( \theta_i^{\sf T} )}^2 \right] = {\cal O} \left( \frac{(L\Delta_f )^{2/3}}{(nT)^{2/3}} + \frac{\sigma^{2/3}}{(nT)^{2/3}} + \frac{(L\Delta_f)^{2/3} n \overline{G}_0 \sigma^{4/3}}{\delta^2\rho^4 T} + \frac{(L\Delta_f)^{2/3}n^{5/3} \sigma^{2/3}}{\delta^{3} \rho^{6} T^{4/3}}\right),
% \end{equation}
% which matches the lower bound in \citep{arjevani2019lower} on the $\sigma, \epsilon$ dependence, modulo some constants and the higher order ${\cal O}(1/T), {\cal O}(1/T^{4/3})$ terms. In addition, we can find the corresponding transient time $T_{\sf trans}$ by bounding $T$ such that the first term dominates the last two terms, i.e.,
% \begin{equation}
%     T_{\sf trans} := \Omega \left( \max \left\{ \frac{n^5 \overline{G}_0^3 \sigma^4}{\delta^2 \rho^4}, \frac{n^{7/2} \sigma}{\delta^{9/2} \rho^9}\right\}\right) = \Omega\left(\frac{n^5 \overline{G}_0^3 \sigma^4}{\delta^{9/2}\rho^9}\right)
% \end{equation}
% for $\sigma \ge 1$.
% }
\section{Connection between Assumption \ref{ass:mix} and Spectral Gap} \label{app:spectralgap}
{\revision
Conditions (i), (ii) of Assumption \ref{ass:mix} are standard in the literature of decentralized optimization, while at the same time (iii) is equivalent to the spectral gap condition. To see this, we suppose ${\bf W}$ is the weighted adjacency matrix of a connected graph and note from the Perron-Frobenius theorem that ${\bf 1}$ is the eigenvector corresponding to the leading eigenvalue of ${\bf W}$ which has multiplicity of 1. It follows from orthogonality of ${\bf U}$ and ${\bf UU}^\top = {\bf I}_n - (1/n) {\bf 11}^\top$ that 
\begin{equation}
    \|{\bf U^\top W U} \|_2 = \|{\bf UU^\top WUU^\top} \|_2 = \max\{\lambda_2, |\lambda_n| \}
\end{equation}
Therefore, condition (iii) that asserts the existence of $\rho \in (0,1]$, $\max\{ \lambda_2, |\lambda_n| \} \leq 1 - \rho$ is equivalent to $\max\{ \lambda_2, |\lambda_n| \} < 1$. 
Lastly, it is obvious that $\| {\bf W} - {\bf I} \|_2 \leq \| {\bf W} \|_2 + \| {\bf I} \|_2 \leq 2$ and we used $\bar{\omega} \in (0,2]$ in (iv) of Assumption \ref{ass:mix} to simplify notations.
}
\end{document}

