\section{The Case of Submodular Functions}\label{sec:submodular}

%Previously, we saw stability and recovery for additive set
%functions. In this section, we are interested in
This section considers recovery results for stable instances where the
objective function is monotone and submodular. Submodular functions
are widely used in many areas ranging from mathematics to economics,
and they model situations with \textit{diminishing
  returns}. Famous examples include influence
maximization~\cite{kempe2003maximizing,mossel2007submodularity,
  easley2010networks, chen2009efficient} and welfare maximization in
auctions and game theory~\cite{lehmann2001combinatorial,
  nisan2007computationally}. For example, in influence maximization,
the goal is to ``activate'' a subset of the participants in a social 
network (e.g., provide with information, or a promotional product) so as to
maximize the expected spread of the idea or product.
The diffusion of information is usually modeled with submodular
functions (indicating the probability that a node adopts a new idea or
product as a function of how many of her neighbors in the social
network have already done so).
%find a set of people in a social network to give a viral
%information to, with the goal of getting most people infected. There
%are many parameters in the model like the individuals' thresholds for
%transitioning from \textit{inactive} to \textit{infected} or the
%probability of infection between two people. All those parameters are
%not known in advance and we can only get estimates for them in real
%world applications. 
%The method of choice for approximating the influence maximization
%problem is greedy algorithms~\cite{kempe2003maximizing}.
In practice, the submodular functions in the input are estimated from
data and hence are noisy (e.g.~\cite{backstrom2006group}).  One hopes that the output of an influence
maximization algorithm (which is typically a greedy
algorithm~\cite{kempe2003maximizing}) is robust to modest errors in
the specification of the submodular function.  This section proposes a
definition to make this idea precise, and proves tight results for greedy
and local search algorithms under this stability notion.

% When the estimate is fed in our algorithm, we
% still would like to output reasonable answers close to optimum. The
% definition of submodular stability we give here tries to capture
% interesting perturbations we would like to be robust against, like
% small changes to the marginal gains or small changes to the values of
% the function on specific sets.



\subsection{Stability for submodular functions} \label{submod-def}

% We first need to define what perturbation and stability mean in the case when our function is submodular and not simply additive. This turns out not to be an easy task (see also \hyperref[sec:unsuccessful]{Appendix~\ref{sec:unsuccessful}}). In \cite{bilu2012stable}, the authors are interested in the \MaxCut\ problem and naturally, the proposed perturbation and stability definitions handle additive weight functions. Here we see how to extend these definitions to handle submodular functions, in a sensible way generalizing the additive case. 
All previous work on perturbation-stability considered only additive
objective functions.  We next state our extension to submodular functions.

\begin{definition} [$\gamma$-perturbation, $\gamma \ge 1$]\label{d:sm} \label{def:stability}

Given a monotone submodular function $f: 2^X\rightarrow \RR^+\cup \{0\}$, we define $f_S(j) = f(S+j) - f(S)$.
A $\gamma$-perturbation of $f$ is any function $\tilde{f}$ such that the following three properties hold:
\begin{enumerate}
\item $\tilde{f}$ is monotone and submodular.
\item $f \le \tilde{f} \le \gamma f$, or in other words $f(S) \le \tilde{f}(S)\le \gamma f(S)$ for all $S \subseteq X$.
\item For all $S \subseteq X$ and $j \in X \setminus S$, $0 \le \tilde{f}_S(j) - f_S(j) \le (\gamma -1)\cdot f(\{j\})$. \label{property}
\end{enumerate}
\end{definition}
%For example, with $\gamma=1$, 
%A sanity check for the special case of $\gamma=1$: due to 
%by properties 1 and 3, we cannot perturb the function at all and this
%coincides with the standard notion of perturbation from the additive
%case; that's the reason we need to use the quantity $(\gamma-1)$ on
%the right hand side and not just $\gamma$. Having the above definition
%of $\gamma$-perturbation of an instance, we can now define the notion
%of stability for the optimum solution:
The definition of a $\gamma$-stable instance is then defined as usual.

\begin{definition} [$\gamma$-stability]\label{d:sm2}
  Given an independence system $(X,\I)$ and a monotone submodular
  function $f: 2^X\rightarrow \RR^+\cup \{0\}$, let $S^*:=\arg\max_{S\in \I}f(S)$. The instance is
  {\em $\gamma$-stable} if
  for every $\gamma$-perturbation of the initial function $f$, $S^*$
  remains the unique optimal solution.
\end{definition}
As discussed in the \hyperref[sec:introduction]{Introduction}, 
while \hyperref[d:sm]{Definition~\ref{d:sm}} is perhaps not the first one that comes to
mind, it appears to be the ``sweet spot''.  Natural modifications\footnote{If we dropped Property 3, then \textit{any} $c$-approximation algorithm ($c\ge1$) returning a solution $S$ with $f(S)\ge\tfrac{1}{c}\cdot f(S^*)$, could be made to have value equal with $S^*$ in the $c$-perturbed version $\tilde{f}(S)=cf(S)\ge f(S^*)=\tilde{f}(S^*)$. If we dropped Property 2, then the definition would not be a generalization for the case of additive perturbations as we could have $\tilde{f}(S)> \gamma f(S)$ for some set $S$, because of the quantity $f(\{j\})$ in Property 3, which is relative to the \textit{empty set} and may be large compared to $f_S(j)$.} of
the definition are generally either too restrictive (rendering the
problem impossible, e.g.~if property~3 is replaced with relative
perturbations since then the zero marginal values in $f$ must stay zero in $\tilde{f}$) or too permissive (rendering the problem uninteresting,
with all $\alpha$-approximation algorithms equally good).
%and is neither
%too restrictive (  nor too
%permissive---turns out to be the set of perturbed

\begin{proposition}
  \hyperref[d:sm]{Definition~\ref{d:sm}} specializes to perturbation-stability in the
  special case of an additive objective function.
%generalizes the standard notion of
%  \textit{Bilu-Linial} stability for the additive case where we just
%  have a simple \textit{weight} function on the elements of the ground
%  set $X$.
\end{proposition}
\begin{proof}
This follows easily since if the function $f$ is additive, then there will be no dependence of the element's $j$ marginal value on the current set $S$ and thus property 3 from the above $\gamma$\textbf{-perturbation} definition just becomes:
$$
0 \le \tilde{f}_S(j) - f_S(j) \le (\gamma -1)\cdot f(j) \iff 0\le \tilde{f}(j)-f(j) \le  (\gamma -1)\cdot f(j) \iff f(j)\le  \tilde{f}(j) \le \gamma \cdot f(j)
$$
which is exactly the standard notion of stability introduced by \cite{bilu2012stable}. Note that this also implies the first condition for all sets $S$: $f(S) \le \tilde{f}(S)\le \gamma f(S)$, by the additivity of $f$.
\end{proof}

We now prove a useful proposition that we will often use when proving recovery results for submodular maximization. Informally, we show that multiplying the marginal improvements of the choices made by an algorithm by $\gamma$ is a valid $\gamma$-perturbation. 

\begin{proposition}
\label{prop:perturb}
Let $f$ be a monotone submodular function.
Fix an ordered sequence of elements $e_1,e_2,\dots,e_k$, and let $\delta_i = f(\{e_1,\ldots,e_i\}) - f(\{e_1,\ldots,e_{i-1}\})$.
Then $\tilde{f}$ defined by $\tilde{f}(S) := f(S) + (\gamma-1) \sum_{i: e_i \in S} \delta_i $ is a valid $\gamma$-perturbation of $f$.
\end{proposition}

\begin{proof}
Let us verify the conditions of a $\gamma$-perturbation.

First, $\tilde{f}$ is monotone submodular, since it is a sum of a monotone submodular and a monotone additive function ($\delta_i \geq 0$ by monotonicity).

Second, we have $f(S) \leq \tilde{f}(S) = f(S) + (\gamma-1) \sum_{i: e_i \in S} \delta_i \leq f(S) + (\gamma-1) f(S \cap \{e_1,\ldots,e_k\})$ by submodularity, and by monotonicity this is at most $\gamma f(S)$.

Third, the marginal values of $\tilde{f}$ are $\tilde{f}_S(e_i) = f_S(e_i) + (\gamma-1) \delta_i \leq f_S(e_i) + (\gamma-1) f(\{e_i\})$ (and unchanged for elements other than the $e_i$). 
%First let's fix some notation: $A_i=\{e_1,e_2,\dots,e_i\}, A=A_k$ ($k$ elements were chosen by $A$) and the marginal value of element $e_i$ is: $\delta(e_i)=f_{A_{i-1}}(e_i)=f(A_i)-f(A_{i-1})$. By perturbing every marginal by a $\gamma$ factor (i.e. $\tilde{\delta}(e_i)=\gamma\delta(e_i)$) we respect \hyperref[property]{Property \ref{property}} as $\tilde{\delta}(e_i)=\gamma\delta(e_i)\le \delta(e_i)+(\gamma-1)f_{\emptyset}(e_i)=\delta(e_i)+(\gamma-1)f(e_i)$, where we made use of submodularity. For the total value of $A$'s solution we get: $\tilde{f}(A)=\sum_{i}\tilde{\delta}(e_i)=\sum_{i}\gamma\delta(e_i)=\gamma\sum_{i}\delta(e_i)=\gamma f(A)$, which complies with \hyperref[def:stability]{Definition \ref{def:stability}}. Of course, if instead of $\gamma$-perturbing \textit{all} the marginals we only perturbed \textit{some} of them, it would still be a valid $\gamma$-perturbation (it turns out only elements not included in optimum solutions need to be perturbed for the recovery results).
\end{proof}




\subsection{Greedy recovery and submodularity}

The main result here is that the standard greedy algorithm can recover the optimal solution of a $p$-extendible system, if the optimal solution is $(p+1)$-stable (as it was defined in \hyperref[submod-def]{Section~\ref{submod-def}}). 

\begin{theorem}[Greedy Recovery] \label{th:submod}
Given a monotone submodular function $f$ to maximize over a $p$-extendible system $(X,\I)$, if the optimal solution $S^*=\arg\max_{S\in \I}f(S)$ is $(p+1)$-stable, then the greedy algorithm recovers $S^*$ exactly.
\end{theorem}

\begin{proof}
The proof generalizes the argument we used in the additive case so that we handle submodularity and the proving strategy resembles the proof of the approximation guarantee for the greedy algorithm for submodular maximization on $p$-extendible systems~\cite{calinescu2011maximizing}.  Let's denote by $S = \{e_1,\ldots,e_k\}$ the solution produced by Greedy (in the order that Greedy picked them) and $S^*$ the optimal solution. To give some intuition, in the additive case before, we used the property of $p$-extendibility in order to say that every element that appears in $S$ but not in $S^*$ could be ``boosted'' by a factor of $p$ to obtain an even better optimal solution, which would be a contradiction, because of the $p$-stability. Now, due to submodularity, we need to be careful that we make this exchange argument in a cautious manner.

For $0 \leq i \leq k$, let $S_i = \{e_1,e_2,\dots, e_i\}$ denote the first $i$ elements picked by Greedy (with $S_0=\emptyset$). Let $\delta_i = f_{S_{i-1}}(e_i)= f(S_i)-f(S_{i-1})$. %To give some intuition for the analysis that follows, for each element $e_i$ that Greedy picked, there are some ``conflicts'' when trying to augment the optimum solution $S^*$ by adding $e_i$, since $S^*\cup \{e_i\} \not\in \I$, and after removing those conflicts there is the set of ``remaining'' elements for the Greedy to choose from. We will now define two sequences of sets $S^*_i$ and $T_i$ with the former corresponding to the ``conflicts'' and the latter corresponding to the ``remaining'' elements. 
Using the $p$-extendibility property, we can find a chain of sets $S^* = T_0 \supseteq T_1\supseteq \dots \supseteq T_k = \emptyset$ such that for $1\le i\le k$:
$$S_i\cup T_i \in \I,  \ S_i\cap T_i = \emptyset \ \text{and}\ |T_{i-1} \setminus T_{i}| \leq p.$$
The above means that every element in $T_i$ is a candidate for Greedy in step $i+1$. We construct the chain as follows: Let $T_0=S^*$; we show how to construct $T_i$ from $T_{i-1}$:

\begin{enumerate}
\item If $e_i \in T_{i-1},$ we define $S^*_i=\{e_i\}$ and $T_i=T_{i-1}-e_i$. This corresponds to the trivial case when Greedy, at stage $i$, happens to choose an element $e_i$ that also belongs to the optimal solution $S^*$.
\item Otherwise ($e_i \notin T_{i-1}$), we let $S^*_i$ be a smallest subset of $T_{i-1}$ such that $(S_{i-1}\cup T_{i-1})\sm S^*_i + e_i$ is independent and since $\I$ is $p$-extendible, we have $|S^*_i|\le p$. We let $T_i=T_{i-1}\sm S^*_i$. 
%This corresponds to the case where Greedy picks an element that is not in the optimum solution $S^*$ and thus, we have to take care of the ``conflicts'' (i.e. $S^*_i$) and then define the ``remaining'' elements that may be used to extend Greedy (i.e. $T_i$).
\end{enumerate}

By the above definitions for $S_i,T_i,S^*_i$ it follows that $S_i\cup T_i \in \I$ and $S_i \cap T_i=\emptyset$. By the maximality of Greedy (stopping condition: $\{e| S_k+e \in \I\}=\emptyset$) and the fact that $S_k\cup T_k \in \I$, it also follows that $T_k=\emptyset$. Since Greedy could have picked, instead of $e_i$, any of the elements in $S^*_i$ (in fact $T_{i-1}$) we get: $\delta_i \ge \tfrac{1}{p}f_{S_{i-1}}(S^*_i)$ (recall that $|S^*_i|\le p$).

Let us assume now that the Greedy solution $S$ is not optimal.
We use \hyperref[prop:perturb]{Proposition~\ref{prop:perturb}} to define a $(p+1)$-perturbation that produces a new optimal solution.
Let's suppose $|S\sm S^*|=l$ and let's rename the elements $e_i$ such that $|S \sm S^*|=\{e_1,e_2,\dots, e_l\}$ in the order that the Greedy picked the elements.
Then we define $\tilde{f}(T)$ for every $T$ by $f(T) = f(T) + p \sum_{1 \leq i \leq l: e_i \in T} \delta_i$, where $\delta_i = f_{S_i}(e_i) = f(\{e_1,\ldots,e_i\}) - f(\{e_1,\ldots,e_{i-1})$. Using \hyperref[prop:perturb]{Proposition~\ref{prop:perturb}}, this is a valid $(p+1)$-perturbation.
For the greedy solution $S$, we obtain:
\[ \tilde{f}(S) =  f(S) + p\sum_{i=0}^{l-1} f_{S_i}(e_{i+1}) \ge\]
\[ \ge f(S) + \sum_{i=0}^{l-1}f_{S_i}(S^*_{i+1}) \ge 
f(S) + \sum_{i=0}^{l-1}f_S(S^*_{i+1})\ge f(S) + f_S(S^*\sm S)= \]
\[=f(S) + \left( f((S^*\sm S)\cup S)- f(S)\right)=f(S^*\cup S)\ge f(S^*) = \tilde{f}(S^*).\]
We ended up with $\tilde{f}(S) \ge \tilde{f}(S^*)$ which means that $S^*$ is no longer the unique optimum and hence we get a contradiction to the $(p+1)$-stability of $S^*$.
\end{proof}


\begin{remark}
If instead of exact access to the values of the function $f$, we had an $\alpha$-approximate oracle, then the proof easily extends to handle this case as well. In particular, suppose each element $e_i$ picked by Greedy at stage $i$ satisfies $f_{S_{i-1}}(e_i)\ge \alpha \max_{e\in A_i} f_{S_{i-1}}(e)$, where $A_i$ is the set of all candidate augmentations of $S_{i-1}$. Here $\alpha\le 1$. We would then have that the greedy marginal improvement $\delta_i\ge \tfrac{\alpha}{p}f_{S_{i-1}}(S^*_i)$ and thus we would need $\gamma-1=\tfrac{p}{\alpha}$ leading to exact recovery of $\left(\tfrac{p+\alpha}{\alpha}\right)$-stable instances ($\alpha\le 1$).
\end{remark}


\subsection{Welfare Maximization}
In many situations, like the welfare maximization problem~\cite{lehmann2001combinatorial,nisan2007computationally,vondrak2008optimal}, the submodular function $f$ we wish to maximize has a special form, e.g. it may be written as a sum of other submodular functions $f_i$ (each of which may correspond to the player's $i$ valuation on different allocations of the items). In this special case, we have $f(S)=\sum_{i=1}^nf_i(S)$ and from \hyperref[th:submod]{Theorem~\ref{th:submod}} Greedy recovers the optimal solution $S^*$ for the case of matroids, which are 1-extendible, if $S^*$ is 2-stable. 

However, for \textit{sum} functions $f=\sum_if_i$, we may as well hope that a stronger recovery result is true, i.e. that Greedy recovers the optimal solution of $\max\{f(S)=\sum_if_i: S\in \I\}$, where the optimum is 2-stable only with respect to $2$-perturbations of the individual functions $f_i$. This is indeed true (for the proof, we refer the reader to \hyperref[app:submod]{Appendix~\ref{app:submod}}).

\begin{theorem} \label{th:submod}
Let $(X,\I)$ be a matroid on the elements of $X$, let $B_1,B_2,\dots,B_k$ be a partition of $X$, $f_i: 2^{B_i}\to \RR^+\cup\{0\}$, for $i\in \{1,2,\dots,k\}$ be monotone submodular and let $f=\sum_{i=1}^kf_i$. Suppose the optimal solution $S^*$ of $\max\{f(S): S\in \I\}$ is $2$-stable only with respect to individual perturbations of the functions $f_i$. Then, Greedy recovers $S^*$.
\end{theorem}

%\begin{proof}
%We note that $B_i$'s form a partition of $X$ but it is not tied to the the matroid in any way. To avoid confusion, we should first emphasize the greedy algorithm in this case: It starts with the empty set $S_0=\emptyset$, at step $t$ it selects: $e=\argmax_{x\in X}\{f(S_{t-1}+x)-f(S_{t-1})\}$ subject to the matroid constraint and it updates $S_t\leftarrow S_{t-1}+e$. This is slightly in contrast with the standard greedy algorithm in welfare maximization that first picks an item and it gives it to the player so that it yields maximum marginal improvement.

%Suppose greedy outputs $S\neq S^*$ and that it chose elements $A_i\subseteq B_i$. Let $S_e$ be the greedy solution right before adding element $e$. Then a 2-perturbation of the individual functions is:
%\[
%\tilde{f_i}(A_i)=f_i(A_i)+\sum_{e\in (S\cap B_i)\sm S^*}f_i((B_i\cap S_e)+e)-f_i(B_i\cap S_e)
%\]
%Now coming back to the total welfare function $f$ we get:
%\[
%\tilde{f}(S)=\sum_{i=1}^k\tilde{f}_i(S\cap B_i)=f(S)+\sum_{e\in S\sm S^*}f_{S_e}(e)\ge f(S)+\sum_{e'\in S^*\sm S, e\leftrightarrow e'}f_{S_e}(e')\ge
%\]
%\[
%\ge f(S)+f_S(S^*\sm S)\ge f(S^*\cup S)\ge f(S^*)=\tilde{f}(S^*)
%\]
%where we made use of the greedy criterion, submodularity and the matroid matching $e\leftrightarrow e'$ between elements $e\in S\sm S^*$ and $e' \in S^*\sm S$. We got $\tilde{f}(S)\ge \tilde{f}(S^*)$, hence a contradiction to the 2-stability of $S^*$ and hence $S\equiv S^*$ and greedy exactly recovers the optimum solution. 
%\end{proof}
























%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
