\section{More on RealNVP and MAF}\label{appendix:flows}
In pursuit of inspiring more research incorporating flow-based models of the state distribution, 
%all across the literature surrounding reinforcement learning
and since flows are presumably the topic least familiar to an IL researcher reading the paper, we now provide a brief but thorough review of RealNVP and MAF, along with some additional discussion.

Following the description in Section \ref{background:flows}: For both training and generation to be efficient, $f_{\theta}$ must be easy to evaluate, easy to invert and the determinant of its Jacobian must be easy to compute. Moreover, for training to be plausible, $f_{\theta}$ must be expressive enough to capture complex high-dimensional distributions. 

RealNVP \cite{dinh2016density} uses coupling layers (distinct from our coupled approach) to achieve the above requirements. As described in the paper \cite{dinh2016density}, a coupling layer receives a $D$ dimensional input vector $x$ and outputs: 
%$y_{1:d} = x_{1:d}$ and $y_{d+1:D} = x_{d+1:D} \odot \exp(s(x_{1:d})) + t(x_{1:d})$
\begin{gather}\label{eq:Coupling} 
 y_{1:d} = x_{1:d} \\
 y_{d+1:D} = x_{d+1:D} \odot \exp(s(x_{1:d})) + t(x_{1:d}) \nonumber
\end{gather}
where $d<D$, $s$ and $t$ are functions from $R^d \to R^{D-d}$ and $\odot$ represents an element-wise product. Since coupling layers leave certain variables unchanged, they are composed in an alternating pattern to allow all variables to be transformed. Note that the Jacobian of this transformation is triangular, and its determinant is simply $exp(\sum_j{s(x_{1:d})_j)}$. Note further that the inverse of this transformation is also a simple computation: 
\begin{gather}
 x_{1:d} = y_{1:d} \\
 x_{d+1:D} = (y_{d+1:D} - t(x_{1:d})) \odot \exp( - s(x_{1:d})) \nonumber
\end{gather}
Finally, note that $s$ and $t$ have no requirement to be invertible, so they can be arbitrary neural networks. 

Masked Autoregressive Flow (MAF) \cite{papamakarios2017masked} is a generalization of RealNVP which substitutes \eqref{eq:Coupling} with the autoregressive
\begin{equation}
    y_{i} = x_{i} \exp(s_i(x_{1:i-1})) + t_i(x_{1:i-1})
\end{equation}
As stated in \ref{background:flows}, this improves expressiveness but prevents single pass sampling. However, density evaluation—our main concern—can still be performed efficiently using masked autoencoders \cite{germain2015made}.
%should verify again that i correctly described MAF


Generally, normalizing flows are sensitive objects, after all, density estimation is a difficult task, particularly with such limited data of such high dimension. Given the delicacy involved in their training, much deliberation was done over the potential influence of various things. In particular, the need for normalization, something common in other IL works \cite{kostrikov2018discriminator,kostrikov2019imitation,schroecker2020universal}. Although we did finally settle on the more robust approach of using no normalization, or any other special pre-processing, still, this could be useful and even necessary for other applications incorporating density models of the state distribution. Moreover, despite the existence of higher capacity flows \cite{kingma2018glow,huang2018neural,durkan2019neural,ho2019flow++}, when learning from such few demonstrations, seemingly more powerful flows may be far more prone to overfitting then their less expressive counterparts. We aim to inspire others to take advantage of modern powerful density estimators for explicit modeling of the state and state-action distributions, and apply them all around the reinforcement learning literature.
