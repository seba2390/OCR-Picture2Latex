
\section{Justification of Modeling the Undiscounted State Distribution}\label{appendix:undiscounted}
With the goal in an MDP being policy optimization, standard RL taxonomy divides between value-based and policy-based methods. The policy gradient theorem \cite{sutton2000policy}, being the foundation of policy-based methods, provides the following expression for the gradient of $J$ with respect to a parameterized policy $\pi_\theta$: 
\begin{equation}\label{eq:PGT}
\nabla_\theta J(\pi_\theta) = \sum_s d^\gamma_{\pi}(s) \sum_a \frac{d\pi_\theta(a|s)}{d\theta} Q^\pi(s,a)
\end{equation}
where $d^\gamma_{\pi}(s) = \sum_{t=0}^{\infty}\gamma^t Pr(s_t = s | s_0 \sim p_0)$ is the improper discounted state distribution. Equation \eqref{eq:PGT} enables the construction of policy optimization algorithms such as REINFORCE \cite{sutton2018reinforcement}, which are based on stochastic gradient descent (SGD). However, garnering some criticism \cite{nota2019policy}, most modern policy gradient methods \cite{schulman2017proximal,fujimoto2018addressing,haarnoja2018soft} opt to ignore the discount factor in $d^\gamma_\pi(s)$, which disqualifies them as SGD methods, hence losing guarantees it provides. Instead, they update the policy with a similar expression to \eqref{eq:PGT} in which $d^\gamma_\pi(s)$ is exchanged with the undiscounted version $d_{\pi}(s) = \sum_{t=0}^{\infty}Pr(s_t = s | s_0 \sim p_0).$  The reason being due to stability and reduction of variance (see Appendix A of \cite{haarnoja2018soft}). We in this work follow suit, modeling the undiscounted $d_{\pi}(s)$.

