\section{Related Work} 


Attempts at modeling the state distribution have been made for various applications and countless distribution matching approaches to imitation learning exist, with varying degrees of relevancy to our own. The most pertinent works—those intersecting both the former and the latter—are reviewed in most detail towards the end of this section.


Some general attempts to model the state distribution include \cite{hazan2019provably} who use discretization and kernel density estimates (KDE) for curiosity-based exploration, while \cite{lee2019efficient} uses variational autoencoders (VAE) in the same context. VAE's have also been used by \cite{islam2019off} to model $d_\pi(s)$ for constraining the distribution shift in off-policy RL algorithms, and KDE's have also been used by \cite{qin2021density} for density constrained reinforcement learning.

Approaches to distribution matching-based imitation include GAIL \cite{ho2016generative} who uses a GAN-like \cite{goodfellow2014generative} objective to minimize the JS divergence. Originally born out of max-entropy inverse reinforcement learning \cite{ziebart2008maximum}, GAIL paved the way for later works such as AIRL \cite{fu2017learning} which modified GAIL's objective to allow reward recovery; DAC \cite{kostrikov2018discriminator}, an off-policy extension of GAIL also addressing reward bias; and general f-divergence approaches like \cite{ke2020imitation, ghasemipour2020divergence}. Other works include the DICE \cite{nachum2019dualdice} family comprised of ValueDICE \cite{kostrikov2019imitation} and its successors SoftDICE \cite{sun2021softdice}, SparseDICE \cite{camacho2021sparsedice} and DemoDICE \cite{kim2021demodice}. Using the Donsker-Varadhan representation along with a change of variables, ValueDICE derives a fully off-policy objective from the reverse KL, completely drubbing BC in the offline regime. Its successors are of tangential relevancy here, each augmenting it for a different domain: SoftDICE makes various amendments notably using the Wasserstein metric (also used by PWIL \cite{dadashi2020primal}), while SparseDice adds a host of regularizers to extend ValueDICE to subsampled trajectories and DemoDICE focuses on imitation with supplementary imperfect demonstrations. We note the apparent advantages of the DICE family over BC in the fully offline regime have recently been questioned \cite{li2022rethinking}.


Our exploitation of Donsker-Varadhan was certainly inspired by ValueDICE and is somewhat reminiscent of MINE \cite{belghazi2018mine} who utilize it to estimate mutual information with a regular neural estimator. Their context and precise method however, differ substantially.
Another work of relation is the often overlooked \cite{schroecker2020manipulating} which includes three approaches: SAIL, GPRIL and VDI \cite{ schroecker2017state, schroecker2019generative, schroecker2020universal}. All three attack the IL problem via the forward KL. Essentially, the forward KL is broken into a behavioral cloning term as well as a state distribution term, and optimizing the objective then requires an estimate of $\nabla_\theta \log d_{\pi_\theta}(s)$. SAIL, GPRIL and VDI each propose a unique way of estimating this gradient, with GPRIL and VDI incorporating flows. \cite{chang2022flow} also utilize flows, proposing a state-only IL approach that models the state-next-state transition using conditional flows, but require dozens of expert trajectories to find success. 

Finally, the most similar work is NDI \cite{kim2021imitation} who rewrite the reverse KL as $-D_{KL}(p_\pi||p_{e}) = \mathbb{E}_{p_\pi}\left[\log p_e - \log p_\pi \right] = J(\pi, r{=}\log p_e) + \mathcal{H}(p_\pi).$ That is, RL with rewards $\log p_e$, along with a state-action entropy term. They continue by deriving a lower bound on $\mathcal{H}(p_\pi)$, termed the SAELBO. NDI+MADE is then proposed, where in a first phase $\log p_e$ is estimated using flows followed by a second phase of optimization.



NDI has many limitations and differs dramatically from our approach. NDI claims to be non-adversarial, while true, it is only due to their loose bound. Moreover, this bound was proven superfluous in their ablation where setting $\lambda_f {=} 0$ showed no reduction in performance. 
On top of that, we far outperform them, our evaluation is far more comprehensive and although we both use flows, the method and employment differ significantly in CFIL given the coupling of a pair with Donsker-Varadhan. 



Pausing the deturpation, each of these methods above has strong merits and heavily inspired our own: We adopted the use of MAF over RealNVP from NDI+MADE (our work began in ignorance of NDI); and it was studying ValueDICE that inspired our direction to exploit the Donsker-Varadhan representation of the KL. The way in which we do however, is unique, and all the approaches above are distinct from our own.

