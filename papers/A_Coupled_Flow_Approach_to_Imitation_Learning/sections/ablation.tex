\section{Ablation}\label{section:ablation}
We now concisely ablate various aspects of our approach. We first put into question the need for our squasher, our coupling and our inductive bias, by comparing to NoSquash; IndFlow and IndFlowNS; and RegularNet. NoSquash is CFIL stripped of the squasher. IndFlow and IndFlowNS refer to learning the log distribution ratio directly using independent flows (as in section \ref{sec:our_approach}), with and without a squasher, respectively. RegularNet alters CFIL's inductive bias by setting $x$ to a regular MLP, completely avoiding the use of flows (reminiscent of MINE \cite{belghazi2018mine}). Along with these we also run a numerator only approach termed Numerator, which simply involves direct learning of the expert's distribution with a single flow, then using it alone as reward in an RL algorithm: $J(\pi, r{=}\log p_e)$  (akin to NDI \cite{kim2021imitation} with $\lambda_f=0$). 

We run these with our usual setup of a single expert trajectory and compute their overall score as the average normalized asymptotic reward over all $25$ seeds ($5$ for each environment). Table \ref{table:ablation_regnet} summarizes these results, showing all the CFIL alternatives fail, demonstrating the necessity of its components.
\begin{table}[t]
\caption{A comparison of CFIL to some alternatives, ablating its squasher, coupling and inductive bias. The score is the average normalized asymptotic reward over $25$ seeds ($5$ for each environment). Note: rows with two values indicate runs with smoothing of $0$ (left) and $0.5$ (right).}
\label{table:ablation_regnet}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
 & score \\
\midrule
Expert & 1 \\
\midrule
CFIL    & $\textbf{1.012}$ \\
NoSquash & $-0.091$\\
RegularNet & $0.196 \mid 0.190$\\
IndFlow    & $0.158 \mid 0.127 $\\
IndFlowNS  & $0.090 \mid 0.072$\\
Numerator  & $-0.051 \mid -0.001$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Next we vary CFIL's smoothing and regularization coefficients to test its sensitivity. Specifically, we run CFIL (that of the three settings in Figure \ref{fig:CFILcompare}), but with pairs $\alpha, \beta \in \{0, 0.25, 0.5, 0.75, 1\},$ along with select others, and compute their averages and standard deviations of normalized asymptotic rewards over all $25$ seeds. Figure \ref{fig:ablation_smooth} illustrates the results for these runs, showcasing both the utility of the smoothing and regularization as well as CFIL's robustness to them.
\begin{figure}[t]
\vskip 0.1in
\centering
\includegraphics[width=0.84\columnwidth]{figures/ablation_smooth_color_size_legend_on_top_with_avg_winter_cropped.png} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Averages (as color) and standard deviations (as size) of normalized asymptotic rewards for CFIL with varied levels of smoothing and regularization. Each point summarizes 25 seeds (5 per environment). The utility of the smoothing and regularization is apparent as well as CFIL's lack of sensitivity to them.}
\label{fig:ablation_smooth}
\vskip -0.1in
\end{figure}

%Results per environment can be found in appendix \ref{appendix:ablation_per_env}.
