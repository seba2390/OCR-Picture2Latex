\section{Introduction}
Reinforcement learning (RL) \cite{sutton2018reinforcement} concerns the optimization of an agent's behavior in an environment. Its characterizing difficulties of exploration vs exploitation and credit assignment, stem from its typical incarnations where the agent must learn from sparse feedback. In order to provoke desired behavior, one may need to craft a sophisticated reward function or provide demonstrations for the agent to imitate. Imitation Learning (IL) deals precisely with the latter: learning from expert demonstrations. Although RL and IL share the same ultimate goal of producing a good policy, they differ fundamentally in that RL is guided by the environment’s feedback, while IL is guided only by the ability of the agent to reproduce the expert’s behavior.

Central to both RL and IL are the state and state-action distributions induced by the policy. Their importance cannot be overstated, with the state distribution forming the basis of policy gradient methods through the policy gradient theorem \cite{sutton2000policy}, and the state-action distribution being core to the common distribution matching formulation of IL \cite{ke2020imitation, ghasemipour2020divergence}. They are also foundational to other applications, like curiosity-based exploration \cite{pathak2017curiosity}, constrained RL \cite{qin2021density} and Batch RL \cite{fujimoto2019off}, some of which have recently been unified under the umbrella of convex RL \cite{zhang2020variational,zahavy2021reward,mutti2022challenging}, which studies objectives that are convex functions of the state distribution.  

% maybe cut the line short (remove who do attempt to model it)
Despite their ubiquity across the literature, explicit modeling of the distributions is scarce. Instead, they mostly find use as a theoretical tool for derivations. This is, of course, barring some approaches that do attempt to model them \cite{hazan2019provably,qin2021density,lee2019efficient,kim2021imitation} or their ratios \cite{nachum2019dualdice,liu2018breaking,gangwani2020harnessing}, further discussion of which is delegated to the related work. The reason for this lack of modeling is due to the difficulty of density estimation, especially in the case of complex and high-dimensional distributions. This is where normalizing flows (NF) \cite{dinh2014nice,dinh2016density, papamakarios2017masked}, a recent approach to density estimation, will be of service. 

%perhaps

%[should motivate why imitation is a natual place for such a model]
We believe there is no shortage of applications for such modeling, but we focus on imitation learning, a quite natural and suitable place, given its modern formulation as state-action distribution matching \cite{ghasemipour2020divergence}. 

Many approaches to distribution matching based imitation have been presented \cite{ho2016generative,fu2017learning,kostrikov2018discriminator,ke2020imitation, ghasemipour2020divergence, kostrikov2019imitation,sun2021softdice,kim2021imitation, dadashi2020primal,schroecker2020manipulating}. The common theme for such methods begins with the selection of a divergence, followed by the development of a unique approach. This may involve a direct attack on the objective by reformulating it \cite{kostrikov2019imitation}, or by derivation of a surrogate objective \cite{zhu2020off, dadashi2020primal, kim2021imitation}, with some utilizing mechanisms such as an inverse action model \cite{zhu2020off} and focusing on learning from states alone (a setting our approach naturally lends to). Other methods first derive estimates for the gradient of the state distribution with respect to the policy's parameters \cite{schroecker2020manipulating}, while some devise unifying algorithms and frameworks encompassing previous approaches \cite{ke2020imitation, ghasemipour2020divergence}. 


The most popular divergence of choice is the reverse KL, which some favor due to its mode-seeking behavior \cite{ghasemipour2020divergence}. Others attempt to get the best of both worlds, combining both mode-seeking and mode-covering elements \cite{zhu2020off}. A priori, it is difficult to say which choice of divergence is advantageous, it's more about the ensuing approach to its minimization.


In this work, we propose a unique approach to distribution matching based imitation, by coupling a pair of flows through the optimality point of the Donsker-Varadhan \cite{donsker1976asymptotic} representation of the KL. More specifically, by noting this point occurs at the log distribution ratio, while the IL objective with the reverse KL can be seen as an RL problem with the ratio inverted. We propose setting the point of optimality as the difference of two normalizing flows, then training in an alternating fashion akin to other adversarial IL methods \cite{ho2016generative,kostrikov2018discriminator, ghasemipour2020divergence, kostrikov2019imitation,sun2021softdice}. This method proves far more accurate than estimating the log distribution ratio by naively training a pair of flows independently. We show this in part by analyzing their respective BC graphs: a simple tool we present for gauging how well a proposed estimator captures the expert's behavior. While most IL works neglect analysis of their learned reward function, we think this can be a potential guiding tool for future IL researchers. 


Our resulting algorithm, Coupled Flow Imitation Learning (CFIL) shows strong performance on standard benchmark tasks, while extending naturally to the subsampled and state-only regimes. In the state-only regime in particular, CFIL exhibits significant advantage over prior state-of-the-art work, despite the competition being specifically designed for that domain. This work also aims to inspire more research incorporating explicit modeling of the state-action distribution.


