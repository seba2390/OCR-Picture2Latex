\section{Our Approach}\label{sec:our_approach}
%[maybe prefacing line about IL] towards our IL approach
We begin with the reverse KL as our divergence of choice, since, noting as others have \cite{kostrikov2019imitation,hazan2019provably,kim2021imitation,camacho2021sparsedice}, its minimization may be viewed as an RL problem with rewards being the log distribution ratios:
\begin{align}
    \argmin_{\pi}\,D_{KL}(p_\pi||p_e) =& \argmax_{\pi}\,\mathbb{E}_{p_\pi(s,a)}\left[\log \frac{p_e(s,a)}{p_\pi(s,a)} \right] \nonumber \\=& 
    \argmax_{\pi}J(\pi, r{=}\log \frac{p_e}{p_\pi}).\label{eq:rKL}
\end{align}
Thus permitting the use of any RL algorithm for solving the IL objective, provided one has an appropriate estimate of the ratio.

%briefly
Before continuing however, we first motivate our coupled approach for such estimation, by illustrating the failure of what is perhaps a more natural next step: using two independent density estimators—say flows—for each of the densities $p_e$ and $p_\pi$ directly. Practically, this would mean alternating between learning the flows %(like Equation \ref{eq:regularization} below)
and using their log-ratio as reward in an RL algorithm. The table in Section \ref{section:ablation} concisely showcases a clear failure of this approach on all the standard benchmarks we later evaluate with.


\begin{figure}[t]
\vskip 0.1in
\centering
\includegraphics[width=0.95\columnwidth]{figures/BC_analysis_bottom_cropped.png} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Left: The BC graph of an uncoupled flow for the HalfCheetah-v2 environment. Right: The BC graph of a coupled flow for the HalfCheetah-v2 environment. BC graphs for an estimator are generated by updating the estimator analogously to an RL run using $N$ saved BC rollouts. This yields $N$ estimators corresponding to $N$ intermediate BC agents. The BC graph is then, for all $i$, the scatter of the $i$’th estimator’s evaluation of an $i$’th BC agent's trajectory against its true environment reward.}
\label{fig:ind_flows_failure_and_bc}
\vskip -0.1in
\end{figure} 



The failure can be further understood through analyzing what we term the BC graph corresponding to an estimator of the log distribution ratio. That is, we first train a behavioral cloning agent once on sufficient expert trajectories, while rolling it out every few iterations. We then train the estimator analogously to an RL run, using a single expert trajectory along with the $N$ saved BC rollouts. This quick process yields $N$ estimators corresponding to $N$ intermediate BC agents. The BC graph is then, for all $i$, the $i$’th estimator’s evaluation of an $i$’th BC agent's trajectory, scattered against the true environment reward of that same trajectory. \footnote{Meaningfulness of the BC graph depends on how erratic the training was and how iteration number, loss and true environment reward line up during training. Our choice of Cheetah for illustration is motivated due to all these measures mostly aligning.} 
Intuitively, a non increasing graph, means a potential RL learner with an analogously trained reward may struggle to overcome those misleading behaviors ranked high by the synthetic reward, thus preventing them from reaching expert level. In practice of course, one would want some form of approximate monotonicity or upward trend. Though importantly, a BC graph's monotonicity by no means implies the success of an RL learner with the correspondingly constructed reward. This is more than a theoretical idiosyncrasy: Many estimators will emit a perfect BC graph while completely failing all attempts in RL (see Appendix \ref{section:appendix_regular_net_bc}). Only the reverse is true: its complete lack of an upward trend will usually imply an agent's failure. 


Loosely formalized, given BC's objective in Equation \ref{eq:bc_objective} and assuming a stationary (time independent) reward function $r$, a monotonically increasing BC graph essentially\footnote{Once again assumes alignment of environment reward with loss.} means that for all policies $ \pi_1, \pi_2$: $ J(\pi_1,r) > J(\pi_2,r)$ if $ \mathbb{E}_{p_e}[\log \pi_1(a|s)]  > \mathbb{E}_{p_e}[\log \pi_2(a|s)] $. Thus, further assuming continuity, a non monotonically increasing graph implies it either monotonically decreases or the existence of a local maximum. In both cases an RL learner with objective $\argmax_\pi{J(\pi,r)}$ may converge on a policy with suboptimal BC loss. Since only at optimality BC recovers the expert policy, this would guarantee the agent will not meet its truly intended goal of expert mimicry. 
Of course, in reality, RL can overcome a certain lack of an upward trend. Moreover, the rewards are neither stationary nor identical between the BC and RL runs, only analogously constructed, so such graphs are only loosely representative. Nonetheless, we find they can be highly insightful.





As Figure \ref{fig:ind_flows_failure_and_bc} suggests, the independently trained flows' BC graph is quite lacking. An agent would have no incentive according to the synthetic reward to make any progress, which is precisely what occurs as the table in Section \ref{section:ablation} demonstrates. This poor BC graph is due in part to each flow being evaluated on data completely out of its distribution (OOD), which flows are known to struggle with \cite{kirichenko2020normalizing}. Since the two flows estimates lack true meaning when evaluated on each others data, we need to tie them together somehow: They must be \textit{coupled}.

To perform our coupling, we employ the Donsker-Varadhan \cite{donsker1976asymptotic} form of the KL divergence:
%wanted to align but too big... how can it be done?
\begin{align}
D_{KL}&(p_\pi||p_{e}) =
\nonumber \\
&\sup_{x: S \times A \to \mathbb{R}} \mathbb{E}_{p_\pi(s,a)}\left[x(s,a)\right] - \log \mathbb{E}_{p_e(s,a)}\left[e^{x(s,a)} \right].\label{eq:Donsker}
\end{align}
 %note the "&" in the KL helps properly align equations
In the above, optimality occurs with  $x^*= \log \frac{p_\pi}{p_e} + C$ for any $C\in \mathbb{R}$ \cite{kostrikov2019imitation, gangwani2020harnessing,belghazi2018mine}. Thus after computing $x^*$, one recovers the log distribution ratio by simple negation, enabling use of $-x^*$ as reward in an RL algorithm to optimize our IL objective. This leads directly to our proposed approach for estimating the log distribution ratio, by coupling two flows through $x(s,a)$. That is, instead of training two flows independently, we propose to do so through maximization of Equation \ref{eq:Donsker}. More specifically, we inject the following inductive bias, modeling $x$ as $x_{\psi,\phi}(s,a) = \log p_\psi(s,a) - \log q_\phi(s,a),$ where  $p_\psi$ and $q_\phi$ are normalizing flows. %[so by maximizing \ref{eq:Donsker} one recovers x...] 


This coupling guarantees more meaningful values when the flows are evaluated on each others data, since it has already occurred during the maximization phase, hence sidestepping the OOD issue described earlier. Figure \ref{fig:ind_flows_failure_and_bc} illustrates this advantage. The right shows the coupled flows' BC graph, clearly remedying the issue with their uncoupled counterparts: A potential learner will now have the proper incentive to reproduce the expert's behavior. 

The drop in synthetic reward (i.e. $-x^*$) towards the end of the BC graph may seem daunting, but it actually expresses how well our estimator captures the expert's behavior: The drop occurs precisely beyond expert level, where the agent, while good in the environment, diverges from the true expert's behavior.\footnote{ This both illustrates the tendency of BC to overfit \cite{li2022rethinking}, while also raising concerns about the tendency for IL papers to report a table showing slight advantage in asymptotic reward as being meaningful. In truth, once at expert level, an advantage should not be claimed for slightly higher performance, unless the stated goal of the work is to do so, but that would no longer be imitation.}



Given this improved estimator, our full IL objective can then be written as:
\begin{equation}\label{eq:maxmin} 
    \argmax_{\pi} \min_{p_\psi, q_\phi}
    \log \mathbb{E}_{p_e(s,a)}\left[e^{\log \frac{p_\psi}{q_\phi}} \right] - \mathbb{E}_{p_\pi(s,a)}\left[\log \frac{p_\psi}{q_\phi}\right].
\end{equation}
As is commonplace in adversarial like approaches \cite{goodfellow2014generative, ho2016generative,kostrikov2019imitation}, the max-min objective above is trained in an alternating fashion, switching between learning $x$ and using $-x$ as reward in an RL algorithm. Moreover, we find it useful to use a squashing function on $x$, avoiding a potentially exploding KL, due to a lack of common support (subtly different then the earlier issue of OOD). 

%maybe mention the shift as well: thanks to RL's invariance to constant reward shift $\argmax_\pi J(\pi, r) = \argmax_\pi J(\pi, r+C)$

Our approach still enables training each flow independently along the way. We call this flow regularization and importantly, our method succeeds without such regularization. More specifically, for expert and agent batches of size $M$, this
regularization involves incorporating the following additional loss function into the minimization step of Equation \ref{eq:maxmin}: 
\begin{equation}\label{eq:regularization}
\mathcal{L} = - \frac{1}{M} \sum_{i=1}^M \log  q_\phi(s_e^i,a_e^i) + \log  p_\psi(s^i,a^i),
\end{equation}
with $\mathcal{L}$ to be weighted by a coefficient $\alpha$.

Noting that training flows is a delicate process, our approach further benefits from—though again does not require—use of a smoothing akin to the dequantization used when training normalizing flows on discrete data \cite{papamakarios2017masked}. More specifically, since our input is unnormalized, we smooth each dimension with uniform noise scaled to its value. That is, if $(s,a)$ is the vector to be smoothed, we sample uniform noise with dimension $dim((s,a))$, multiply them element-wise and add that to the original vector:
\begin{equation}
    {(s,a) \mathrel{{+}{=}} \beta \cdot (s,a) \odot u, \hspace{0.5em} u \sim Uniform(-\frac{1}{2},\frac{1}{2})^{dim((s,a))}},
\end{equation}
with weight $\beta$ controlling the smoothing level. Note if regularization is also present, smoothing still applies within the additional loss $\mathcal{L}$.


Finally, combining  all the above is our resulting algorithm, Coupled Flow Imitation Learning (CFIL). It is summarized in Algorithm \ref{alg:CFIL}, with only the number of batches per density update omitted. 
%algorithm notes:
%
\begin{algorithm}[tb]
\caption{CFIL}
\label{alg:CFIL}
\textbf{Input}: Expert demos $\mathcal{R}_E = \{(s_e,a_e)\}_{t=1}^N$; parameterized flow pair $p_\psi,q_\phi$; off-policy RL algorithm $\mathcal{A}$; density update rate $k$; squashing function $\sigma$; regularization and smoothing coefficients $\alpha, \beta$.\\
\textbf{Define}: $x_{\psi,\phi} = \sigma(\log p_\psi - \log q_\phi)$
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{timestep $t=0,1,\dots,$}
\STATE Take a step in $\mathcal{A}$ with reward $r=-x_{\psi,\phi}$, while filling agent buffer $\mathcal{R}_A$ and potentially updating the policy and value networks according to $\mathcal{A}$'s settings.
\IF {$t\mod k =0$}
\STATE Sample expert and agent batches:
\STATE $\{(s_e^t,a_e^t)\}_{t=1}^M \sim \mathcal{R}_E$ and  $\{(s^t,a^t)\}_{t=1}^M \sim \mathcal{R}_A$
\IF{smooth}
\STATE ${(s,a) \mathrel{{+}{=}} \beta \cdot (s,a) \odot u, \hspace{0.5em} u \sim U(-\frac{1}{2},\frac{1}{2})^{dim((s,a))}}$ 
\ENDIF
\STATE Compute loss: %[possibly reference equation in paper]:
\STATE $\mathcal{J} = \log \frac{1}{M} \sum_{i=1}^M e^{x(s_e^i,a_e^i)} - \frac{1}{M} \sum_{i=1}^M x(s^i,a^i)$
\IF {flow reg}
\STATE Compute regularization loss:
\STATE $\mathcal{L} = - \frac{1}{M} \sum_{i=1}^M \log  q_\phi(s_e^i,a_e^i) + \log  p_\psi(s^i,a^i)$
\STATE $\mathcal{J} = \mathcal{J} + \alpha \mathcal{L}$
\ENDIF
\STATE Update $\psi \leftarrow \psi - \eta \nabla_{\psi}\mathcal{J}$
\STATE Update $\phi \leftarrow \phi - \eta \nabla_{\phi}\mathcal{J}$
\ENDIF
\ENDFOR

%"curly brackets turn an equation into a math atom and prevent it from breaking up"

\end{algorithmic}
\end{algorithm}
As in ValueDICE \cite{kostrikov2019imitation}, we found the bias due to the log-exp over the mini-batches did not hurt performance and was therefore left unhandled.

Another setting of interest is learning from observations (LFO) alone \cite{zhu2020off,torabi2018generative,torabi2021dealio}. That is, attempting to minimize: \begin{equation}
\argmin_{\pi} \, D_{KL}(d_\pi(s,s')||d_e(s,s')).
\end{equation}
While this objective is clearly underspecified in a non injective and deterministic MDP, in practice, recovering the expert's behavior is highly feasible \cite{zhu2020off}.
%injective meaning all actions lead to different states and deterministic meaning actions always lead to a state
%(since different actions may lead to the same state, hence multiple policies can induce identical state next-state distributions)
%so not one to one.
Seeing as none of our description above is specific to the domain of states and actions, CFIL naturally extends to LFO with no need of modification. This is in stark contrast to previous works in the LFO setting which have been highly tailored \cite{zhu2020off}. We shall demonstrate CFIL's utility in the LFO setting in the following section, where remarkably, we even find success when the flows model the single state distribution $d(s)$.


