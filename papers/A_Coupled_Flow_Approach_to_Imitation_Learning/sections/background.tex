\section{Background}

\subsection{Markov Decision Process}
Environments in RL are typically expressed as a Markov Decision process (MDP). An MDP is a tuple $(S,A,P,R,p_0,\gamma)$, where $S$ is a set of states termed the state space, $A$ is a set of actions termed the action space, $P: S \times A \times S \to [0,\infty]$ is a transition density function describing the environment's Markovian dynamics, $R: S\times A \times S \to \mathbb{R}$ is a reward function, $p_0$ is an initial state distribution, and $\gamma \in [0,1)$ is a discount factor. \par
A policy $\pi: S \to A$ dictates an agent's actions when roaming the environment, and the goal in an MDP is to find the optimal one, denoted $\pi^*$. The standard optimality criterion for a policy is the expected discounted reward:
$J(\pi,r) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty}\gamma^t r_t \right]$
where $\tau \sim \pi$ symbolizes the trajectory distribution induced by the policy $\pi$, and $r_t$ is the reward at time $t$ along a trajectory. Each policy has an associated value function and Q-function:
$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r_t | s_0 = s\right] \text{\, and \,}
Q^\pi(s,a)= \mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r_t | s_0 = s, a_0 = a\right],$
where the value function represents the expected discounted reward of following $\pi$ from state $s$, while the Q-function represents the expected discounted reward of following $\pi$ after taking action $a$ from state $s$. 

Another object induced by the policy is the improper discounted state distribution $d^\gamma_{\pi}(s) = \sum_{t=0}^{\infty}\gamma^t Pr(s_t = s | s_0 \sim p_0),$ as well as its undiscounted counterpart, $d_{\pi}(s) = \sum_{t=0}^{\infty}Pr(s_t = s | s_0 \sim p_0),$ in which we take greater interest for reasons described in Appendix \ref{appendix:undiscounted}. $d_{\pi}(s)$ is called the state distribution and is of central interest to this work. Closely related to $d_\pi(s)$ is the state-action distribution $p_\pi(s,a) = \pi(a|s)d_\pi(s)$, which is also of interest to this work.


\subsection{Normalizing Flows}\label{background:flows}
Normalizing Flows (NF) \cite{dinh2014nice,dinh2016density} are exact-likelihood generative models that use a simple base distribution $p_Z(z)$ and a sequence of invertible and differentiable transformations $f_{\theta} = f_k \circ f_{k-1} \circ \dots \circ f_1$ to model an arbitrary target distribution $p_X(x)$ as $z = f_{\theta}(x)$.
\footnote{More often presented as $x = g(z)$ \cite{papamakarios2021normalizing,kobyzev2020normalizing}, which is equivalent with $f=g^{-1}$, due to their invertibility. Under this model, Equation \eqref{eq:NF} would look similar with only the "$+$" substituted by a "$-$".}
This means the target log-likelihood can be written using the change of variables formula:
\begin{equation}\label{eq:NF}
\log p_X(x) =
\log  p_Z(z) + \sum_{i=1}^{k}{\log \left|\text{det}\frac{df_i}{df_{i-1}}\right|}
\end{equation}
allowing for parameters $\theta$ to be trained using maximum likelihood. With a trained flow at hand, generation is performed by sampling from the base distribution and applying $f_{\theta}^{-1}$, and density estimation can be done by evaluating the RHS of \eqref{eq:NF}. RealNVP \cite{dinh2016density}, which is of particular interest to us, uses coupling layers (not to be confused with our coupled approach) to construct $f_{\theta}$. Masked Autoregressive Flow (MAF) \cite{papamakarios2017masked} is a generalization of RealNVP which substitutes its coupling layers with autoregressive ones. While improving expressiveness, this impedes single pass sampling, but still allows efficient density evaluation—our main concern—using masked autoencoders \cite{germain2015made}. RealNVP and MAF are reviewed more thoroughly in Appendix \ref{appendix:flows}.


Although much research \cite{kingma2018glow,huang2018neural,durkan2019neural,ho2019flow++} has gone into improving the capacity of MAF, due to its simplicity, efficiency and adequacy for our task, it is the architecture used in this work to model the state and state-action distributions $d_\pi(s)$ and $p_\pi(s,a)$.


\subsection{Imitation Learning}
Imitation Learning (IL) concerns the optimization of an agent’s behavior in an environment, given expert demonstrations. Perhaps the simplest approach to imitation, behavioral cloning (BC), performs supervised regression or maximum-likelihood on given expert state-action pairs $\{(s_e,a_e)\}_{t=1}^N$: \begin{equation}\label{eq:bc_objective} {\min_\pi \sum_{t=0}^N Loss(\pi(s_t),a_t)\quad \text{or} \quad \max_\pi \sum_{t=0}^N \log \pi(a_t|s_t)} \end{equation} for deterministic and stochastic policies, respectively. BC suffers from compounding errors and distributional shift, wherein unfamiliar states cause the policy to misstep, leading to even less familiar states and an eventual complete departure from the expert trajectory \cite{ross2011reduction}.

Recent distribution matching (DM) approaches \cite{ho2016generative,kostrikov2018discriminator,kostrikov2019imitation,kim2021imitation} successfully overcome these issues. One common formulation \cite{ke2020imitation, ghasemipour2020divergence} encompassing most of these methods, views DM as an attempt to match the agent's state-action distribution $p_\pi$, with the expert's $p_{e}$, by minimizing some f-divergence\footnote{Other distances, such as the Wasserstein metric have also been used \cite{sun2021softdice,dadashi2020primal}.} $D_f$: 
\begin{equation}\label{eq:Matching}
\argmin_{\pi} \, D_f(p_\pi||p_e).
\end{equation} 
These methods hinge on the one-to-one relationship between a policy and its state-action distribution and have shown significant improvement over BC, particularly when few expert trajectories are available \cite{ghasemipour2020divergence} or expert trajectories are subsampled \cite{li2022rethinking}. 