\section{Related Work}
\label{sec:related_work}
%We discuss related research efforts that seek to leverage the non-volatility  of NVM. Those research efforts explore how to enforce write-ordering to build persistent memory. We also discuss the recent progress of the checkpoint mechanism in HPC in this section.
\begin{spacing}{0.9}
\textbf{Persistent memory.}
NVM has been explored to implement checkpoint as main memory.
Kannan et al.~\cite{ipdps13:kannan} use NVM only for checkpoint (not computation). To improve performance, they proactively move checkpoint data from DRAM to NVM before checkpoint is started. 
Gao et al.~\cite{ics15:gao} use a hardware-based approach to utilize runtime idling to write checkpoint and spread it across memory banks for load balance.
Ren et al.~\cite{micro15:ren} dynamically determine checkpoint granularity (cache block level or page level) based on memory update density.
Dong et al.~\cite{sc09:dong} introduce 3D stacked NVM and incremental
checkpoint to reduce checkpoint overhead.
Those prior efforts focus on good performance of NVM to establish persistence (checkpoint) in NVM, while we focus on how to maximize the benefit of non-volatility of NVM. Different from those prior efforts, our work avoids data copying, %in checkpoint, 
and does not require hardware assist.
%hence resulting in better performance. Also, our work does not require hardware assist. 

To enable data consistence in NVM, many research efforts explore how to enforce write-ordering with minimum overhead. The epoch-based approach~\cite{sosp09:condit, Pelley:isca14, micro15:joshi, micro16:kolli} is one of those research efforts. This approach divides program execution into epochs, within which stores are allowed to happen concurrently without disturbing data consistence in NVM.
%Our performance optimization to improve cache flushing (see Section~\ref{sec:perf_opt}) uses the idea of the epoch.
In fact, our proactive cache flushing (Section~\ref{sec:perf_opt}) is one variation of epoch. 
%Our performance optimization to improve cache flushing (see Section~\ref{sec:perf_opt}) uses the idea of the epoch. 
From the point where the cache flush happens to the point where the working version becomes the consistent version is an epoch where concurrent, persistent writes can happen.
However, most of the existing work is hardware-based and requires hardware support to implicitly identify epochs. Also, to apply the existing work to establish data persistence in HPC still needs a mechanism to maintain two versions of the target data objects. Our work requires no hardware support and the in-place versioning provides the two versions.
%Our work rely on compiler and runtime to decide the epoch (i.e., triggering and completing cache flushes).

\begin{comment}
Some work divides program execution into epochs. In the epoch, stores may
persistent concurrently. 
BPFS~\cite{sosp09:condit} is one of the pioneer work that explores the idea of epochs for NVM.  BPFS implements epochs by tightly coupling with cache management. In particular, BPFS tags all cache blocks with
an epoch ID on every store and modifies the cache replacement policy to write epochs back to NVM in order. 
Pelley et al.~\cite{Pelley:isca14} introduce a couple of variation of epoch,
and demonstrate potential performance improvement because of a relaxation of
inter-thread persist dependencies. 
Joshi et al.~\cite{micro15:joshi} propose a buffered epoch persistency by defining efficient persist barriers.
Delegated ordering~\cite{micro16:kolli} decouples cache management from the path persistent writes take to memory to allow concurrent writes within the same epoch to improve performance.
In essence, our performance optimization to improve cache flushing (see Section~\ref{sec:perf_opt}) uses the idea of the epoch. In our work, from the point where the cache flush happens to the point where one of the dual versions
is modified is an epoch where concurrent writes can happen to ensure persistency. 
Different from the existing work, our work does not need hardware support
to implicitly identify epochs. Our work rely on compiler and runtime
to decide the epoch (i.e., triggering and completing cache flushes).
\end{comment}

Some work explores redo-log and undo-log based approaches to build transaction semantics for data consistence in NVM.
This includes hardware logging~\cite{pm_iccd14, hpca17:joshi, stable_tr16:zhao}.
%The hardware logging can reduce inter-transaction dependencies~\cite{pm_iccd14} 
%or remove log operations from the critical path to improve performance~\cite{hpca17:joshi, stable_tr16:zhao}.
%Other work introduces persistent cache, such that stores become durable as 
%they execute~\cite{micro13:zhao, vldb_endow14:wang, asplos12:dushyanth}. The 
%persistent cache eliminates the necessity of any cache flushing operation.
However, those approaches come with extensive architecture modifications.

There are also software-based approaches that introduce certain program constructs to enable data persistence in NVM~\cite{mnemosyne_asplos11, intel_nvm_lib, usenix13:rudoff, nv-heaps_asplos11, vldb_endow15:chatzistergiou, hpdc16:denny}. 
%Those approaches include Mnemosyne~\cite{mnemosyne_asplos11}, Intel NVM library~\cite{intel_nvm_lib, usenix13:rudoff}, NV-heaps~\cite{nv-heaps_asplos11}, REWIND~\cite{vldb_endow15:chatzistergiou}, and NVL-C~\cite{hpdc16:denny}.
To use those program constructs, one have to make changes to OS and applications. 
The application can suffer from large overhead because of frequent runtime checking
or data logging. Our experiences with~\cite{intel_nvm_lib} show that CG and dense matrix multiplication 
suffer from \textbf{52\%} and \textbf{103\%} performance loss because of frequent data logging operations.
Our work in this paper has very small runtime overhead and does not require changes to OS.

\begin{comment}
Some work explores hardware logging for persistent memory.
LOC~\cite{pm_iccd14} provides a storage transaction interface and reduce
inter- and intra-transaction dependencies by hardware logging and multi-versioning caches.
ATOM~\cite{hpca17:joshi} proposes a hardware-based log organization that
eliminates log persist operations from the critical path of program execution.
Ogleari et al.~\cite{stable_tr16:zhao} leverage hardware information naturally
available in caches to maintain data persistence and propose a hardware-driven 
undo+redo logging scheme.
The above work is complementary to our software-based approach, and may be
combined with our work to simplify the implementation of the in-place versioning.
\end{comment}


\begin{comment}
Leveraging persistent extensions from ISA (e.g., {\fontfamily{qcr}\selectfont clflush}), some work introduces certain program constructs to enable persistent memory. 
%Comparing with the work discussed in the last paragraph, this work needs much less hardware support. 
Mnemosyne~\cite{mnemosyne_asplos11}, Intel NVM library~\cite{intel_nvm_lib, usenix13:rudoff}, NV-heaps~\cite{nv-heaps_asplos11}, and REWIND~\cite{vldb_endow15:chatzistergiou} provide transaction
systems optimized for NVM. 
NVL-C~\cite{hpdc16:denny} introduces flexible directives and runtime checks  
that guard against failures that corrupt memory persistency.
%SCMFS~\cite{sc11:wu} provides a PM-optimized file system based on the persistent extensions from ISA.
%Atlas~\cite{oopsla14:dhruva} uses those extensions for lock-based code.
To use the above work for HPC applications, we may have to make extensive
changes to applications or operating systems. The application
can suffer from large runtime overhead because of frequent runtime checking
or data logging.
\end{comment}

\begin{comment}
Some work introduces persistent cache, such that stores become durable as they execute~\cite{micro13:zhao, vldb_endow14:wang, asplos12:dushyanth}. This work eliminates the necessity of any cache flushing operation, by not caching NVM accesses, or by ensuring that a batter backup is available to flush the contents of caches to NVM upon power failure. However, this work needs extensive hardware modification. It is not clear if integrating NVM into processors has any manufacturing challenges.
\end{comment}

\textbf{Checkpoint mechanism.}
%The checkpoint is the most common fault tolerance mechanism in production HPC.
%There is a large body of work targeting on improving performance of checkpoint.
Diskless checkpoint is a technique that uses DRAM-based main memory and available processors
to encode and store the encoded checkpoint data~\cite{tpds98:plank, Lu:2005:SDC:1145057, ppopp17:tang, isftc94:plank}. 
Because of the DRAM usage and the limitation of encoding techniques, diskless checkpoint has to leverage multiple nodes to create redundancy and only tolerates  up to a certain number of node failures.
Our method is a diskless-based approach, but leveraging non-volatility of NVM.
Our method does not have node-level redundancy in diskless checkpoint, and is independent of the number of node failures.

Incremental checkpoint is a method that only checkpoints modified data to save checkpoint size
and improve checkpoint performance~\cite{isftc94:plank, ics04:agarwal, icpads10:wang, ipdps09:bronevetsky}.
However, for those applications with intensive modifications between
checkpoints (e.g., HPL~\cite{ppopp17:tang}), the effectiveness of the incremental checkpoint method can be limited. 

Multi-level checkpoint is a method that saves checkpoint to fast devices (e.g., PCM and local SSD) 
in a short interval and to slower devices in a long interval~\cite{sc10:moody, sc09:dong, sc11:gomez}. 
By leveraging good performance of fast devices, the multi-level checkpoint removes expensive memory copy on slower devices. %off the critical execution path. 
%However, the multi-level checkpoint 
However, it can still suffer from large data copy overhead on fast devices, when
the checkpoint data size is large.
Our work introduces the in-place versioning to 
remove data copy by leveraging application-inherent write operations to update checkpoint data.
Hence, our method does not have the limitation of incremental and multi-level checkpoints.
\vspace{-10pt}
\end{spacing}
%(1) Optimizing Checkpoints Using NVM as Virtual Memory (IPDPS) and an eurosys paper
%(2) Real-Time In-Memory Checkpointing for Future Hybrid Memory Systems
%(3) ThyNVM: Enabling Software-transparent Crash Consistency in Persistent Memory Systems

%most relevant references
%(1) Self-Checkpoint: An In-Memory Checkpoint Method Using Less Space and Its Practice on Fault-Tolerant HPL
%(2) Checkpointing Exascale Memory Systems with Existing Memory Technologies
%(3) Fault Tolerance for Remote Memory Access Programming Models
%(4) Distributed Diskless Checkpoint for Large Scale Systems

%flash-based burst buffer (NVM-based. see memsys paper from Trevor)
%speculative checkpoint




