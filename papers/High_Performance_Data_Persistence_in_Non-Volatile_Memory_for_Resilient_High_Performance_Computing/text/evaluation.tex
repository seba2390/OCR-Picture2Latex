\section{Evaluation}
\label{sec:eval}

%(\textbf{Dong TODO: replace DRAM cache flushing with DRAM cache writeback.})
We evaluate the in-place versioning (IPV) in this section.
%\textbf{Unless indicated otherwise, IPV includes issuing WBINVD instructions in this section.} 
Unless indicated otherwise, IPV includes optimized cache flushing 
and helper thread in this section.
%Our goal is to study the runtime overhead of IPV.
Also, the data persistence establishment happens at every iteration of the main computation loop, which aims to build
high resilience and minimize recomputation for future HPC.
We use the native execution, which has neither checkpoint
nor IPV, as our baseline. 
An ideal performance of IPV should be close to that of
the native execution as much as possible.

We study the performance on two test platforms. 
One test platform is a local cluster. Each node of it has two eight-core Intel Xeon E5-2630 processors (2.4 GHz) and 32GB DDR4. We use this platform for tests in all figures except Figure~\ref{fig:prelim_design1} in Section~\ref{sec:prelim_design}. %and~\ref{fig:strong_scaling_bt}. 
We deploy Quartz on such platform to emulate a heterogeneous NVM/DRAM system with NVM configured with 1/8 DRAM bandwidth and DRAM configured with 256MB capacity to enable a practical emulation of NVM~\cite{eurosys16:dulloor, NVMDB}. 
The other test platform is the Edison supercomputer at Lawrence Berkeley National Lab (LBNL). We use this platform for tests in Figure~\ref{fig:prelim_design1}. 
Each Edison node has two 12-core Intel Ivy Bridge processor (2.4 GHz) with 64GB DDR3. 
We cannot install Quartz on Edison to enable a practical emulation of NVM, because Quartz requires a privileged access to the system. Hence, we perform most of the tests on the local cluster.

We use six NPB benchmarks (CLASS C) and one production application (Nek5000) with the eddy input problem ($256 \times 256$). %elements.
Table 2 gives more information on the benchmarks and application. The table also lists how the target data objects are transformed into IPV based on either basic rule, post-update version switch, or nonuniform update.
For NPB benchmarks, the target data objects are chosen based on typical checkpoint cases, algorithm knowledge, and benchmark information.
For Nek5000, the target data objects are determined by the checkpoint mechanism in Nek5000. 

%\captionsetup[table]{name=Table}
\begin{table}
\centering
\footnotesize 
\caption*{Table 2: Target Data objects for checkpointing and the in-place versioning. IPV in the table stands for the in-place versioning.}
\vspace{-10pt}
\begin{tabular}{|p{1.0cm}|p{1.1cm}|p{1.3cm}|p{1.9cm}|p{1.5cm}|}
       \hline
       \textbf{Bench-mark} & \textbf{Data obj} & \textbf{IPV (basic rule)} & \textbf{IPV (post-update version switch)} & \textbf{IPV (nonuniform update)} \\
        \hline \hline
      FT & u0,u1,u2 & u1,u2 & u0 & -  \\  \hline
      CG & p,r,z & p & r,z & -    \\  \hline
        BT & u & u & - & -    \\  \hline
        SP & u & u & - & -    \\  \hline
        LU & u & u & - & -    \\ \hline
        MG & r & - &  -& r   \\ \hline
        Nek5000 (eddy) & vx, vy, vz, pr,xm1,ym1, zm1 & pr,xm1,ym1, zm1 & vx,vy,vz & -  \\ \hline
\end{tabular}
\label{tab:benchmarks}
\vspace{-20pt}
\end{table}

\begin{comment}
Result 1: performance results for WBINVD+dual version (1/8). Baseline
includes native execution and optimized preliminary design (SSE2).

%Result 2: performance results for WBINVD+dual version (1/32).
%Baseline includes native execution and optimized preliminary design (SSE2).

Result 3: How much is overlapped during async cache flush. The overhead of WBINVD.
\end{comment}

Figure 12 compares the performance of 
the baseline, the preliminary design 2 (i.e., checkpoint with cache bypassing), IPV with neither cache flushing nor helper thread, IPV with cache flushing (no helper thread), and IPV with everything.
Comparing with the baseline, IPV achieves rather small runtime overhead (4.4\% on average and no larger than 9.5\%). Most of the performance improvement comes from the removal of data copying. In particular, regarding IPV (no cache flushing and helper thread) and the preliminary design 2, both of them do not have cache flushing, but IPV (no cache flushing and helper thread) performs 9\% better on average because of no data copying. This fact is especially pronounced in Nek5000, where IPV (no cache flushing and helper thread) performs 26\% better than the preliminary design 2. 

Furthermore, IPV cannot be applied to
MG because of nonuniform updates (see Table 2).
Hence MG does not have performance data for any IPV.
%``IPV (without cache flushing)''. 
However, MG with the helper thread to enable proactive and asynchronous data copying in the figure has 5.4\% performance improvement over the preliminary design 2.
%because of the proactive data copying by the helper thread.

\begin{figure*}
\centering
\includegraphics[width=1.0\textwidth, height=0.2\textheight]{figures/figure12_2_edit.pdf}
\vspace{-20pt}
\caption*{Figure 12: Performance difference between the native execution (baseline), the preliminary design 2 (checkpoint with cache bypassing), and different IPV cases. Performance is normalized to that of the native execution. MG does not have the results for IPV. The dotted bar in MG is the case of checkpoint with a helper thread for asynchronous and proactive data copying.}
\label{fig:ipv_perf}
\vspace{-10pt}
\end{figure*}

\begin{comment}
\begin{figure*}
\centering
\includegraphics[width=1.0\textwidth, height=0.3\textheight]{figures/E1_2_edit.pdf}
\caption*{Figure x2:(1 BW) Performance difference between the native execution (baseline), the optimized preliminary design, and in-place versioning with cache flushing and without cache flushing.}
\label{fig:ipv_perf}
\end{figure*}
\end{comment}

\begin{comment}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth, height=0.2\textheight]{figures/Dual_clean.png}
\caption*{Figure x: Performance difference between the original version and the in-place versioning. We use a heterogeneous NVM/DRAM system where NVM is configured with the same performance characteristics as DRAM.}
\label{fig:perf_diff_with_cache}
\end{figure}
\end{comment}

\begin{comment}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth, height=0.2\textheight]{figures/Dual_clean.png}
\caption*{Figure x: Performance difference between the original version and the in-place versioning without cache flushing.}
\label{fig:perf_diff_no_cache}
\end{figure}
\end{comment}

To further study the performance of IPV, we focus on the performance difference between IPV without cache flushing and IPV. We aim to study the effectiveness of proactive and asynchronous cache flushing. %and {\fontfamily{qcr}\selectfont WBINVD}. 
In Figure 13, we measure performance of {\fontfamily{qcr}\selectfont WBINVD} and DRAM cache flushing, and quantify their contribution
to the total overhead (i.e., {\fontfamily{qcr}\selectfont WBINVD} plus DRAM cache flushing) in IPV.  The table below the figure quantifies how much of  the total overhead is overlapped with the application execution by the proactive and asynchronous cache flushing. 

Figure 13 reveals that the proactive and asynchronous cache flushing is pretty effective to hide the cache flushing overhead (or data copying for MG). At least 41\% of the total overhead is overlapped in all benchmarks. 
%which demonstrates the effectiveness of proactive cache flushing.
The non-overlapped cache flushing time is exposed to the application critical path and causes the performance difference between IPV and the native execution in Figure 12.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth, height=0.13\textheight]{figures/figure13_1_edit.pdf}

\includegraphics[width=0.5\textwidth, height=0.03\textheight]{figures/figure13_2_edit.pdf}
\vspace{-20pt}
\caption*{Figure 13: Breakdown of the performance difference between the in-place versioning and in-place versioning without cache flushing.}
\label{fig:ipv_perf_breakdown}
\vspace{-10pt}
\end{figure}

IPV can cause extra CPU cache misses, because of two reasons. (1) The two versions of the target data objects increase working set size of the application; (2) {\fontfamily{qcr}\selectfont WBINVD} flushes the entire cache hierarchy. 
%Note that \textit{IPV does not cause DRAM cache misses}, because 
%DRAM cache flushing is implemented by DRAM data copying, and 
%the data remains in the DRAM cache after DRAM cache flushing.

We measure the system-wide last level CPU cache miss rate for the native execution and IPV. Figure 14 shows the results. In general, we do not see big difference (up to 4\%) between the two cases in terms of the last level cache miss rate. This further explains the small performance loss between IPV and the native execution in Figure 12.

The reason that accounts for such small difference in the last level miss rate is as follows. {\fontfamily{qcr}\selectfont WBINVD} happens only once in each iteration, hence its impact on cache misses is not frequent. The two versions do increase the working set size of the application. However, within the original application, the target data objects are typically updated in a loop (e.g., the loop structure in \textit{update} routine in Figures 9 and 10) and there is
little data reuse across iterations of the loop. Such updates tend to be ``streaming-like'', which is not sensitive to the increase of working set size.
\vspace{-10pt}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth, height=0.13\textheight]{figures/figure14_edit.pdf}
\vspace{-20pt}
\caption*{Figure 14: Last level CPU cache miss rate difference between the baseline and the in-place versioning (no cache flushing).}
\label{fig:cache_miss_rate}
\vspace{-10pt}
\end{figure}

\begin{comment}
To further study the performance cost of the in-place versioning, we profile
last level cache miss rate based on PAPI~\cite{} for the main computation loop of those applications.
The results show ignorable change of last level cache miss rate. Those results are consistent with the ones shown in Figure~\ref{fig:Dual_clean}:
The performance cost with the in-place versioning is small.
\end{comment}

\begin{comment}
\textbf{TODO: polish the following paragraph.}
It is because the memory access pattern in such benchmark is "Streaming", which means there is litter cache reuse across iteration. Furthermore, it is safe to assume that "streaming access pattern" applied to target object of most HPC benchmarks. 
%Ref 1
%https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/702244

%Yingchao TODO: using a few sentences to explain how we measure performance and how we avoid the effects of out-of-order instruction execution on performance measurement.
\textbf{xxx} performance difference.
\end{comment}