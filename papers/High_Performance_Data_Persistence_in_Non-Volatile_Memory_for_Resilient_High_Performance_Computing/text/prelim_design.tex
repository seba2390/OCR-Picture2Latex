\section{Preliminary System Designs}
\label{sec:prelim_design}
The performance of NVM is much better than that of traditional hard drive, and even close to or match that of DRAM. 
Given such performance characteristics of NVM, it is promising to enable 
frequent checkpoint with a small overhead.
Frequent checkpoint will enable better HPC resilience and minimize recomputation, hence addressing the two dilemmas for future HPC.
%Based on the frequent checkpoint, the recomputation will be minimized.

Our preliminary designs aim to improve the existing checkpoint mechanism and optimize its performance on NVM. We want to answer a fundamental question: can the NVM-based checkpoint (with optimization) happen frequently, such that we address the two dilemmas rooted in the current checkpoint mechanism?
 
\subsection{Preliminary Design 1: NVM-based, Frequent Checkpoint}
In our first design, we employ an NVM-based checkpoint, and the checkpoint
happens at each iteration of the main loop, which is much more frequent
than the traditional checkpoint.
%The traditional checkpoint cannot happen so frequently because of the concerns on runtime overhead.
Also, the NVM-based checkpoint happens locally. This means that no matter what usage model NVM is used (either as main memory or as a local block device), the checkpoint is stored locally in NVM. By removing networking overhead, this local NVM-based checkpoint represents the best performance we can get out of NVM.
In fact, from the~\textit{architecture point} of view, such local NVM-based system has been shown to be possible for HPC~\cite{ipdps13:kannan, ics15:gao, sc13:jung}.

We compare two cases of hard drive-based, frequent checkpoint
with two cases of NVM-based, frequent checkpoint. 
For hard drive-based, frequent checkpoint, 
the hard drive is resident either locally (annotated as ``hard drive based chkp (local)'') or in a remote storage node (annotated as ``hard drive based chkp (remote)''). 
For NVM-based, frequent checkpoint, NVM is used 
as either main memory (annotated as ``NVM based chkp (mem)'')
or a local block device (annotated as ``NVM based chkp (block)'').
If NVM is used as main memory, checkpointing is the same as
making a data copy in memory plus necessary cache flushing.
To emulate NVM as a block device, we use a ramdisk with a file system
(tmpfs). Hence, such emulation includes the overhead of file system and system calls, but does not emulate internal overhead of I/O controllers, such as interface command decoding and ECC. 

We run six NAS parallel benchmarks and one production code (Nek5000). The details for those applications are summarized in Table 2. 
%\textbf{For NPB benchmarks, we use CLASS D as input, and use 4 nodes with 16 MPI tasks per node; for Nek5000, we use eddy problem as input with a $256\times256$ mesh, and use 4 nodes and 16 MPI tasks per node}. 
In our study, NVM has either the same performance characteristics (bandwidth and latency) as DRAM, which is a rather optimistic assumption on NVM performance, or inferior performance than DRAM, which is a more practical assumption. 

\textbf{(1) NVM has the same performance as DRAM.} 
We emulate NVM with local DRAM, similar to~\cite{asplos15:zhang} and assume that NVM has the same latency and bandwidth as DRAM, 
After data copying in checkpoint, we flush cache blocks of the new data copy out of caches to build a consistent state in NVM, using {\fontfamily{qcr}\selectfont clflush}.

Figure~\ref{fig:prelim_design1} shows the results on a production supercomputer, Edison at Lawrence Berkeley National Lab. 
For NPB benchmarks, we use CLASS D as input; for Nek5000, we use the eddy problem as input ($256 \times 256$). %with a $256\times256$ mesh. 
We use 4 nodes with 16 MPI tasks per node.
Performance (execution time) in the figure is normalized by that of the native execution without checkpoint. 
The figure reveals that with frequent checkpoint, hard drive based checkpoint (local) has 283\% overhead on average (up to 1062\%), which is unacceptable.
NVM-based checkpoint has much better performance.
For some benchmarks (e.g., BT and LU), the overhead of NVM-based checkpoint (NVM as main memory) is smaller than 10\%.
But there is still high overhead for some benchmarks (more than 40\% for MG, FT, and Nek5000).
Also, NVM-based checkpoint (main memory) shows better performance (26\% performance loss on average and up to 46\%) than NVM-based checkpoint (block device) (89\% performance loss on average and up to 401\%).

\begin{comment}
In the figure, we compare two case of hard drive-based, frequent checkpoint
with two cases of NVM-based, frequent checkpoint. 
For hard drive-based, frequent checkpoint, 
the hard drive is resident either locally (annotated as ``hard drive based chkp (local)'') or in a remote storage node (annotated as ``hard drive based chkp (remote)''). 
For NVM-based, frequent checkpoint, NVM is used 
as either main memory (annotated as ``NVM based chkp (mem)'')
or a local block device (annotated as ``NVM based chkp (block)''). %in the figure.
If NVM is used as main memory, checkpointing is the same as
making a data copy in memory. 
\end{comment}
%After data copying, we flush data blocks of the new data copy out of caches to build a consistent state in NVM, using {\fontfamily{qcr}\selectfont clflush}.
%Because we cannot know which data blocks of the new data copy are in caches, we use {\fontfamily{qcr}\selectfont clflush} to flush all data blocks of the new data copy even if they may not be in caches.
%We emulate NVM with local DRAM, similar to~\cite{asplos15:zhang} and assume that NVM has the same latency and bandwidth as DRAM, which is a rather optimistic assumption on NVM performance.  
%To emulate NVM as a block device, we use a ramdisk with a file system
%(tmpfs). Hence, such emulation includes the overhead of file system and system calls, but does not emulate internal overhead of I/O controllers, such as interface command decoding and ECC. 
%``tmpfs does not have page cache, so flushing does not have msync overhead''

%``The use of the file system interface for applications is beneficial from the compatibility point of view, since legacy applications need not be modified. Further, this decouples the application from the underlying storage device. However, it fails to exploit NVMâ€™s byte addressability benefits, and every access to NVM requires user-to-kernel transitions along with additional data serialization cost.''

%Further analysis and profiling of both approaches shows that in the case of the ramdisk approach, the application executes 3x more kernel synchronization calls and spends 31% more time waiting for kernel locks, compared to the memory checkpointing approach. Additional costs like serialization and user-kernel transition for I/O calls add to checkpoint time. Using NVM as a fast disk requires filesystem redesign or architectural enhancements like BPFS.


\begin{figure}
\centering
\includegraphics[width=0.48\textwidth, height=0.15\textheight]{figures/figure2_edit.pdf}
\vspace{-20pt}
\caption{Preliminary design 1 with NVM-based, frequent checkpoint and hard drive-based, frequent checkpoint on Edison. NVM has the same performance characteristics as DRAM. Performance (execution time) is normalized by that of native execution on DRAM without checkpoint.}
\label{fig:prelim_design1}
\vspace{-15pt}
\end{figure}

%Figure~\ref{fig:prelim_design1} reveals that the hard drive-based, frequent checkpoint has large performance cost, no matter whether it happens locally or remotely. 

\textbf{(2) NVM has worse performance than DRAM.}
Since the NVM techniques have a range of performance characteristics,
%are still under quick development and their performance characteristics
%are still largely undetermined~\cite{NVMDB}, 
we change NVM performance to make our evaluation more practical, and re-do the above tests in (1). 
%We redo the above tests with different NVM performance characteristics. 
Since checkpoint performance is sensitive to memory bandwidth, we change NVM bandwidth based on Quartz (a DRAM-based, lightweight performance emulator for NVM~\cite{middleware15:volos}) for our study. Because using Quartz requires loading a kernel driver, which needs privileged accesses to the system, we run Quartz on a local cluster (see Section~\ref{sec:eval} for more details on the cluster). 
We choose 1/8 and 1/32 DRAM bandwidth as NVM bandwidth based on~\cite{eurosys16:dulloor, NVMDB}.
We use CLASS C as input for NPB and the eddy problem ($256\times256$) as input for Nek5000; we use 4 nodes with 4 MPI tasks per node.
Figures~\ref{fig:prelim_design1_low_nvm} and~\ref{fig:prelim_design1_low_nvm2} show the results. %for NVM with 1/8 and 1/32 DRAM bandwidth, respectively. 

Note that given a lower NVM bandwidth, the application performance on a NVM-only system is worse than a DRAM-only system.
To bridge the performance gap between NVM and DRAM, the existing work 
introduces a small DRAM cache~\cite{pm_iccd14,Ramos:ics11, eurosys16:dulloor} to place recent write-intensive data into NVM and build
a heterogeneous NVM/DRAM system.
To study the impact of such small DRAM cache on checkpoint performance,
we allocate a small DRAM space to implement a heterogeneous NVM/DRAM system based on Quartz.
The existing work chooses the DRAM cache size between 32MB and 1GB~\cite{pm_iccd14,Ramos:ics11, eurosys16:dulloor, gpu_pcm_pact13, ieee_micro11:jiang, dac09_pdram, hpdc16:wu}.
We choose a medium DRAM size in our test, which is 256MB.

With the DRAM cache, the overhead of NVM-based checkpoint (NVM as main memory) must include flushing cache blocks of the target data objects from
this DRAM space to NVM, besides the overhead of memory copying in NVM and CPU cache flushing. 
Which data objects are in the DRAM cache at the persistence establishment point depends on the DRAM cache management strategy.
%To decide data placement on the heterogeneous NVM/DRAM system, 
We implement a recent software-based approach~\cite{eurosys16:dulloor} to manage the DRAM cache.
Furthermore, because of the software-based approach, 
%to management data placement on the heterogeneous NVM/DRAM system, 
we know which target data objects (or data blocks of the target data objects) are in the DRAM cache. Hence, we do not need to flush all cache blocks of target data objects for DRAM cache flushing. Also, we do not invalidate data in the DRAM cache after DRAM cache flushing to optimize performance of DRAM cache flushing.
%flushing DRAM cache is equivalent to copy specific data blocks from DRAM to NVM.
%The effect of flushing the DRAM space is emulated by a data copy
%in the main memory with the data size equal to the DRAM cache size.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/figure3_edit.pdf}
\vspace{-20pt}
\caption{Preliminary design 1 with NVM-based, frequent checkpoint and hard drive-based frequent checkpoint on a local cluster. Performance (execution time) is normalized to that of the native execution without checkpoint on the heterogeneous NVM/DRAM system. NVM has 1/8 bandwidth of DRAM.}
\label{fig:prelim_design1_low_nvm}
\vspace{-10pt}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/figure4_edit.pdf}
\vspace{-20pt}
\caption{Preliminary design 1 with NVM-based, frequent checkpoint and hard drive-based frequent checkpoint on a local cluster. Performance is normalized to that of the native execution without checkpoint on the heterogeneous NVM/DRAM system. NVM has 1/32 bandwidth of DRAM.}
\label{fig:prelim_design1_low_nvm2}
\vspace{-10pt}
\end{figure}

Figures~\ref{fig:prelim_design1_low_nvm} and~\ref{fig:prelim_design1_low_nvm2}
show the results. 
Similar to Figure~\ref{fig:prelim_design1}, the two figures show that NVM-based,
frequent checkpoint (NVM as main memory) can result in large performance loss (22\% on average and up to 52\% for NVM with 1/8 DRAM bandwidth, and 32\% on average and up to 60\% for NVM with 1/32 DRAM bandwidth).

\begin{comment}
%The figure reveals that depending on the NVM bandwidth, the benchmarks \textbf{TODO} suffer from \textbf{TODO} performance loss because of the frequent checkpoint. We must improve performance of the NVM-based checkpoint. We explore a couple of optimization techniques, discussed as follows.
From Figures~\ref{fig:prelim_design1},\ref{fig:prelim_design1_low_nvm}, \ref{fig:prelim_design1_low_nvm2}, we have a couple of interesting observations. (1) The hard drive-based, frequent checkpoint can have huge
performance loss (28\%-962\% and \textbf{TODO \%} on average).
(2) The NVM-based, frequent checkpoint can significantly
reduce the performance loss, comparing with the hard drive-based checkpoint. With an optimist assumption on NVM performance (Figure~\ref{fig:prelim_design1}), performance loss
is only 8\%-45\% (\textbf{TODO \%} on average) with NVM used as main memory. With a more practical assumption on NVM performance (Figures~\ref{fig:prelim_design1_low_nvm} and~\ref{fig:prelim_design1_low_nvm2}),
performance loss is 7\%-60\% and \textbf{TODO \%} on average (NVM with 1/8 DRAM bandwidth)
and 6\%-64\% and \textbf{TODO \%} on average (NVM with 1/32 DRAM bandwidth)
with a heterogeneous NVM/DRAM system.
(3) Using NVM as main memory for checkpoint has smaller overhead than
using NVM as a block drive. 
\end{comment}

\textbf{Conclusions.} Using NVM as main memory for checkpoint is promising, but still comes with large performance overhead for some benchmarks, even though we take an optimistic assumption on NVM performance. 

The performance loss of NVM-based checkpoint (NVM as main memory)
comes from data copying during checkpointing and cache flushing. 
To improve the performance of NVM-based checkpoint (NVM as main memory), we focus on improving the performance of cache flushing in the next section. We consider removing data copying in Section~\ref{sec:design}. 
In the rest of this paper, we focus on NVM with 1/8 DRAM bandwidth, which is a more practical assumption on NVM performance~\cite{NVMDB, eurosys16:dulloor}.

\begin{comment}
Some of the overhead is due to cache flushing (both DRAM cache and CPU caches). 
%For DRAM cache, we use a software-based approach to management data placement, and hence know which data objects (or data blocks of the data objects) are in the DRAM cache. For CPU caches,
%we cannot know which cache locks of the data objects are in caches. We will discuss this issue further in Section~\ref{sec:perf_opt}.
To improve CPU cache flush performance, we introduce another
design for NVM-based, frequent checkpoint with NVM as main memory.
\end{comment}

\subsection{Preliminary Design 2: Optimization of NVM-based Checkpoint}
To improve the performance of cache flushing, we explore 
the parallelization of {\fontfamily{qcr}\selectfont clflush} instructions by multi-threading.  
Although {\fontfamily{qcr}\selectfont clflush} is blocking, 
there is no guaranteed order for {\fontfamily{qcr}\selectfont clflush} instructions~\cite{clflush} across threads.
%It is not guaranteed to be ordered by any other fencing or serializing instructions or by another CLFLUSH instruction.
It is possible to use multiple threads for cache flushing, and
each of which flushes non-overlapped cache blocks.
To verify the above idea, we use OpenMP {\fontfamily{qcr}\selectfont parallel for} to parallel a \textit{for} loop for cache flushing with each iteration of the loop flushing a single cache block.
%~\footnote{Using {\fontfamily{qcr}\selectfont clflush\_opt} is possible to further increase cache flushing concurrency within the instruction stream of a single thread. However, based on the current documentation, which processor support such instruction and whether there is any processor supporting this instruction in the market is not clear. Hence, we cannot evaluate with {\fontfamily{qcr}\selectfont clflush\_opt}. }.
We change the number of threads and measure performance for flushing a 20MB data buffer with dirty cache blocks on an Intel Xeon E5-2630 v3 processor (20MB L3, 256KB L2, and 32KB L1)
attached to 32GB DDR4. The processor has 8 cores with 16 hardware threads.
Figure~\ref{fig:clflush_perf} shows the performance (average cycles per cache line).

\begin{comment}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth, height=0.2\textheight]{figures/clflush_perf.png}
\caption{Performance of parallelizing {\fontfamily{qcr}\selectfont clflush} with multi-threading.}
\label{fig:clflush_perf}
\end{figure}
\end{comment}
\begin{comment}
\begin{table}
\centering
\footnotesize 
\begin{tabular}{|c | c|}
       \hline
       Number of threads & Throughput(\#insts/second) \\
        \hline \hline
        1 & 3.61E+05 \\  \hline 
        2 & 7.71E+05  \\  \hline 
        4 & 1.52E+06  \\  \hline 
        8 & 2.97E+06  \\  \hline 
        16 & 5.18E+06  \\  \hline 
        32 & 6.48E+06  \\   \hline 
        64 & 2.95E+06\\ \hline
        128 & 4.74E+05\\ \hline
       \hline
\end{tabular}
\caption{Performance of parallelizing {\fontfamily{qcr}\selectfont clflush} with multi-threading.}
\label{tab:clflush_perf}
\end{table}
\end{comment}

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/figure5_edit.pdf}
\vspace{-20pt}
\caption{Performance of parallelizing {\fontfamily{qcr}\selectfont clflush} with multi-threading.}
\label{fig:clflush_perf}
\vspace{-10pt}
\end{figure}

Figure~\ref{fig:clflush_perf} shows that using multi-threading
does improve performance of {\fontfamily{qcr}\selectfont clflush}, but the performance is not scalable beyond certain number of threads.
In fact, as we increase the number of threads, they will compete for
those resources in cache controllers and read/write ports of main memory,
which limits the scalability of parallel {\fontfamily{qcr}\selectfont clflush}. Based on such observation, we use up to 16 threads to 
parallelize cache flushing, depending on the availability of idling cores
in a node.

\begin{comment}
The following is copied from here (https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/699950)

The overhead of CLFLUSH is generally quite low -- it requires at least one issue slot to a read/write port, and may require additional micro-ops.  This is one of the few instructions that Agner Fog does not track in his (otherwise) comprehensive collection of performance data at http://www.agner.org/optimize/instruction_tables.pdf.    The CLFLUSH instruction is required to remove the cache line from *all* processor caches in the entire system, so it will require many of the same resources that are used to track a store that misses in all levels of the cache.  If these resources are already busy, then the CLFLUSH may extend the overall program execution time, but the effect is indirect and difficult to quantify.
\end{comment}

\begin{comment}
To further improve performance of cache flushing, we explore the usage of  {\fontfamily{qcr}\selectfont CLWB}, and replace {\fontfamily{qcr}\selectfont clflush} with {\fontfamily{qcr}\selectfont CLWB}. {\fontfamily{qcr}\selectfont CLWB} is a very recent x86 instruction with few documentation available on which processor supports it. We cannot find any processor supporting it. Hence, we use an emulation approach to quantify its effect on cache flushing.
In particular, instead of {\fontfamily{qcr}\selectfont clflush}ing the cache blocks of checkpointed data objects, we {\fontfamily{qcr}\selectfont clflush} a cache line with dummy data. 
To minimize the impact of this cache line on the cache occupancy, we repeatedly flush the same cache line for each  {\fontfamily{qcr}\selectfont clflush}. 
Based on this emulation approach, the cache blocks of checkpointed data objects are evicted out of the cache hierarchy. Meanwhile we can emulate the overhead of data write-back into the main memory.
\end{comment}

%When NVM is used as the main memory, the preliminary design 1 uses regular memory copy. 
To further improve performance of NVM-based checkpoint (NVM as main memory), 
%we explore an approach alternative to memory copying for checkpoint. In particular, 
we explore special instructions and use SIMD-based, non-temporal instructions (particularly MOVDQU and \\MOVNTDQ), which bypass caches
to make a data copy. Using those instructions removes the necessity of cache flushing, but those instructions are only available on
a processor with SSE support. %data movement based on SSE;

%Based on the above optimization techniques, we evaluate the performance of NVM-based checkpoint again, shown in Figure~\ref{fig:prelim_design2}. 
%The figure reveals that \textbf{Dong: TODO}.

Figure~\ref{fig:prelim_design2} shows the performance for the above two optimization techniques. Within the figure, the preliminary design 1 (i.e., NVM-based checkpoint with NVM as main memory),
the parallelized {\fontfamily{qcr}\selectfont clflush}, and non-temporal instructions are labeled as ``checkpoint\_clflush'', ``checkpoint\_par\_clflush'', and ``cache bypassing'' respectively. 
``Native execution'' is the one without checkpoint.

The figure shows that the parallelized {\fontfamily{qcr}\selectfont clflush} has up to 5\% performance improvement (for FT) over the preliminary design 1. Non-temporal instructions lead to the best performance in all cases.
Comparing with the preliminary design 1 (checkpoint\_clflush  in Figure~\ref{fig:prelim_design2}), non-temporal instructions result in 9.6\% performance improvement on average and up to 16\%. 
If a platform supports those instructions, they should be the preferred method for NVM-based checkpoint. 
%Furthermore, some benchmarks have very small performance loss (e.g., \textbf{TODO}, less than \textbf{TODO \%}) even with frequent checkpoint on NVM.

However, even if we use the above optimizations on CPU cache flushing,
we still see big performance loss on some benchmarks (e.g., 36\% for Nek5000 and 13\% for CG).
To investigate the reason, we break down the checkpoint time.
For ``checkpoint\_clflush'' (the preliminary design 1) and ``checkpoint\_par\_clflush'', the checkpoint time includes DRAM cache flushing, data copying, and CPU cache flushing; for ``cache bypassing'', the checkpoint time includes DRAM cache flushing and data copying. Figure~\ref{fig:perf_loss_breakdown} shows the results. 

The results reveal that data copying contributes the most
to the performance loss. Except BT and LU with the preliminary design 1, all other cases have more than 50\% performance loss come from data copying. 

\textbf{Conclusions.} To establish frequent data persistence in NVM with high performance and address the dilemmas in checkpoint, we must address the data copying overhead.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth, height=0.13\textheight]{figures/figure6_2_edit.pdf}
\vspace{-20pt}
\caption{Performance (execution time) of NVM-based checkpoint with optimization (NVM is used as main memory). Performance is normalized by that of the native performance without checkpoint.}
\label{fig:prelim_design2}
\vspace{-20pt}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=1.0\textwidth, height=0.2\textheight]{figures/figure7_2_edit.pdf}
\vspace{-20pt}
\caption{The breakdown of performance loss for NVM-based checkpoint after optimization (i.e., the preliminary design 2).}
\label{fig:perf_loss_breakdown}
\vspace{-10pt}
\end{figure*}


%\textbf{Conclusions.} Leveraging the good performance of NVM to do checkpoint is possible for some applications, but still problematic for other applications. The fundamental reason is the data copy.
%\textbf{Dong: TODO}

%We introduce a new mechanism to remove data copy.
