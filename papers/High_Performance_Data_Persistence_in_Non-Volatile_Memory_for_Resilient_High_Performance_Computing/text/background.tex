\section{Background}
\label{sec:bg}
In this paper, we focus on HPC applications.
Those applications are typically characterized with iterative structures.
In particular, there is usually a main computation loop in an HPC application. 
With the traditional checkpoint mechanism, at every $n$ iterations of the loop ($n$ is much larger than 1), the application saves critical data objects of the application into non-volatile storage. 
In the rest of the paper, we name those critical data objects as \textit{target data objects}. 
Checkpoint usually happens near the end of an iteration.
We call the execution point where checkpoint happens as \textit{persistence establishment point}. 

%at the persistency establishment point. The persistency establishment point is usually near the end of an iteration.
%and establishes data persistency. 
%Once the application crashes, those data objects can be used to resume execution. 
We also distinguish \textit{cache line} and \textit{cache block} in this paper.
The cache line describes a location in the cache, and the cache block
refers to the data that go into a cache-line. 
We review NVM background in this section.

%Basic assumption on iterative structures of scientific applications
%and checkpoint (checkpoint application data)

\subsection{Non-Volatile Memory Usage Model}
%see Tao Li MICRO'16 paper 
%see Feng Chen MSST'14 paper
%pending: some discussion on NVM performance
There are at least two existing usage models to integrate the emerging NVM into 
HPC systems. 
In the first model, NVM is built as NVDIMM modules and installed 
into DDR slots. NVM is physically attached to the high-speed memory bus and managed by a memory controller~\cite{micro16:chen}. 
In the second model, NVM connects to the host
by an I/O controller and I/O bus (e.g., PCI-E)~\cite{msst14:chen}.  %intel optance

From the perspective of software, OS can regard NVM as regular memory (the first model), similar to DRAM, and NVM provides the capability of being byte addressable to OS and applications. Also, NVM is accessed through {\fontfamily{qcr}\selectfont
load} and {\fontfamily{qcr}\selectfont store} instructions.
Alternatively, NVM can be exposed as a block device in OS~\cite{usenix13:rudoff}. 
NVM is accessed via a read/write block I/O interface. 
%A NVM driver may be added into OS to convert I/O requests to memory {\fontfamily{qcr}\selectfont load} and {\fontfamily{qcr}\selectfont store}  instructions~\cite{msst14:chen}.
%An optimized, NVM-aware file system 
A file system can be built on top of NVM
to provide the convenience of naming schemes and data protection~\cite{usenix13:rudoff}.
%in traditional file systems~\cite{usenix13:rudoff}.
%In this paper, we consider the first usage model with NVM as regular memory.

\begin{comment}
In this usage model, NVM communicates directly with CPUs.
OS regards NVM as regular memory, similar to DRAM. %which may be marked as non-volatile by BIOS.
NVM provides byte address-ability to OS and its applications.
NVM is accessed through {\fontfamily{qcr}\selectfont
load} and {\fontfamily{qcr}\selectfont store} instructions.
%"residing on memory bus enables NVDIMM devices to alleviate the access latency caused by I/O controller"

In the second model, NVM connects to the host
by the I/O controller and I/O bus (e.g., PCI-E)~\cite{}.  %intel optance
NVM is exposed as a block device in OS. 
%traditional storage devices connected via slower I/O interface such as PCI express. 
NVM is accessed via block I/O interface. 
A NVM driver may be added into OS to convert I/O requests to memory requests.
% application programs do not need to be changed.
% Data stored in NVM is accessible in units of sectors (512 bytes) via the block I/O interface (read and write commands).
There might be a file system.

Block Mode programming model provides the traditional block read/write interface to kernel modules such as file systems and, in some cases, to applications wanting to use the block device directly (for example, by opening /dev/sda1 on a Linux system).


%http://pmem.io/2016/02/22/pm-emulation.html
%Having filesystem brings easy and reliable rights management, while with DAX add-on, 
%any file that is memory maped with mmap(2) is directly mapped from physical addres 
%range into process virtual memory addresses. For those files there is no paging, and 
%load/store operations provide direct access to persistent memory.

%tmpfs
%https://wiki.centos.org/TipsAndTricks/TmpOnTmpfs
%tmpfs creates the virtual filesystem in the kernels page cache space
\end{comment}

%\subsection{Ordered Memory Persistency}
\subsection{Data Consistence in NVM}
To build a consistent state for target data objects in NVM (as main memory) and ensure proper recovery, 
the target data objects in NVM must be updated with the most
recent data in caches at the persistence establishment point.
%consistent with the most recent updates in caches after the persistent establishment point. 
%the most recent updates to those data objects in caches must be flushed to NVM.
%place constraints on the order of NVM writes.
%In particular, we must xxx, before any NVM writes from the application happens.
However, the prevalence of %hardware and software 
volatile caches introduces randomness into write operations in NVM.
%data is first fetched into the volatile caches, and then  written from the caches to NVM. However, 
When the data is written from caches to NVM is subject to
the cache management policy by hardware and OS.

There are ``interfaces'' that enable explicit data flushing %operations
from caches to NVM. Those interfaces are presented as processor instructions
or system calls. 
Using those interfaces, it is possible to enforce data consistence at
the persistence establishment point.
We discuss the common cache flushing instructions as follows. %and discuss them in this paper. 

\begin{comment}
However, ensuring proper recovery requires constraints on
the ordering of NVRAM writes.  Existing DRAM intercon-
nects lack the interface to describe and enforce write ordering
constraints; ordering constraints that arise from memory con-
sistency requirements are usually enforced at the processor,
which is insufficient for failure tolerance with acceptable per-
formance. Recent work has suggested alternative interfaces to
enforce NVRAM write order and guarantee proper recovery,
for example, durable transactions and persist barriers.
\end{comment}

\begin{comment}
Memory persistency prescribes the order
of persist operations with respect to one another and loads and
stores, and allows the programmer to reason about guarantees
on the ordering of persists with respect to system failures;
memory persistency is an extension of consistency models
for persistent memory operations. The memory persistency
model relies on the underlying memory consistency model
and volatile memory execution to define persist ordering constraints
and the values written to persistent memory.
\end{comment}

\begin{comment}
Ensuring ordered data persistence is important for the OS
and applications. Storage devices usually provide an on-device
buffer and interface (“flush”) to write data in two phases – Data
is first written into the buffer, and upon an OS write barrier
(or the buffer is filled up), data is flushed from the buffer to
the medium.

With PM, it is similar but more complicated. Since PM
is physically managed by a memory controller and accessed
by the load/store interface, the written data may reside in
CPU caches. In other words, the CPU cache acts as a volatile
buffer for the PM. Applications, if not changed, may lose
data that is supposed to be persistent upon power failures.
\end{comment}

\begin{comment}
How to flush memory; clflush vs. clwb; flushing the whole cache line vs. others\\

"use either pmem\_persist() or msync(2) when it needs to flush changes, depending on whether the memory pool appears to be persistent memory or a regular file (see the pmem\_is\_pmem() function in libpmem(3) for more information). " %see http://pmem.io/nvml/manpages/v1.2/libpmemobj.3.html
\end{comment}

% https://forums.xilinx.com/t5/Embedded-Development-Tools/what-is-the-difference-between-cache-invalidate-and-cache-flush/td-p/74654
\vspace{-5pt}
\begin{itemize}
\item {\fontfamily{qcr}\selectfont clflush} instruction: 
This is the most common cache flushing instruction. 
Given a cache block, this instruction invalidates it from all levels of the processor cache hierarchy. If the cache line at any level of the cache hierarchy is dirty, the cache line is written to memory before invalidation. 
{\fontfamily{qcr}\selectfont clflush} is a blocking instruction, meaning that the instruction waits until the data flushing is done~\cite{nvmsummit16:rudoff}. 
%see the Intel manual: ``It is not guaranteed to be ordered by any other fencing or serializing instructions or by another CLFLUSH instruction''. 

\item {\fontfamily{qcr}\selectfont WBINVD} instruction: this is a privileged instruction used by OS to flush and invalidate the entire cache hierarchy.

\end{itemize}

To enable data consistence based on {\fontfamily{qcr}\selectfont clflush} and other cache block-based cache flushing instructions (particularly {\fontfamily{qcr}\selectfont CLWB} and {\fontfamily{qcr}\selectfont clflush\_opt}, which will be discussed next),
we may have a performance problem 
%to enable data consistence 
for a data object with a large data size.  
Because we do not have a mechanism to track which cache line is dirty and whether a specific cache block is in caches, we have to flush all cache blocks of target data objects, as if all cache blocks are in caches. Figure~\ref{fig:cache_flushing} shows how we flush cache blocks based on cache block-based cache flushing. 

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{style1}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
%    breakatwhitespace=false,         
%    breaklines=true,                 
%    captionpos=b,                    
%    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt, 
%    showspaces=false,                
%    showstringspaces=false,
    %showtabs=false,                  
    %tabsize=2
	escapeinside={(*@}{@*)},
}
\lstset{style=style1}

\begin{figure}[!htb]
\centering
\begin{lstlisting}[language=c]

 /*Loop through cache-line-size aligned chunks
covering the given range of the target data object*/
cache_block_flush(const void *addr, size_t len)
{
  unsigned __int64 ptr;
        
  for (ptr = (unsigned __int64)addr & ~(FLUSH_ALIGN - 1);
       ptr < (unsigned __int64)addr + len; 
       ptr += FLUSH_ALIGN)
       flush((char *)ptr);  /*clflush/clflush_opt/clwb*/
}
\end{lstlisting}
\vspace{-10pt}
\caption{Using cache block-based cache flushing instructions to flush cache blocks of the target data object.}
\label{fig:cache_flushing}
\vspace{-10pt}
\end{figure}

Flushing clean cache blocks in caches and flushing cache blocks not in caches have performance cost at the same order as flushing dirty cache blocks.
Table~\ref{tab:diff_clflush_perf} shows the performance of flushing cache blocks in different status in caches. The performance is measured in a platform with two eight-core Intel Xeon E5-2630 v3 processors (2.4 GHz, 20MB L3, 256KB L2, and 32KB L1) attached to 32GB DDR4. Based on the results, we conclude that flushing all cache blocks of a data object is roughly proportional to the data object size.

\begin{table}
\centering
\caption{Performance of flushing cache blocks in different status in caches using {\fontfamily{qcr}\selectfont clflush}.}
\vspace{-10pt}
\scriptsize 
\begin{tabular}{| p{1.7cm} | p{1.7cm} | p{1.7cm} | p{1.8cm} |}
       \hline
                & \textbf{Flush dirty cache blocks in caches} & \textbf{Flush clean cache blocks in caches} & \textbf{Flush cache blocks not in caches} \\ \hline \hline
    Cycles per cache block & 228 & 254 & 350 \\ \hline
\end{tabular}
\label{tab:diff_clflush_perf}
\vspace{-10pt}
\end{table}
%16 threads & 27 & 47 & 35 \\

To support NVM, there are two very new instructions, {\fontfamily{qcr}\selectfont clflush\_\\opt} and {\fontfamily{qcr}\selectfont CLWB}.
{\fontfamily{qcr}\selectfont clflush\_opt} maximizes the concurrency of multiple {\fontfamily{qcr}\selectfont clflush} within individual threads. {\fontfamily{qcr}\selectfont CLWB} instruction maximizes the concurrency of
multiple cache line flushing without cache line invalidation (i.e., leaving data in the cache after cache line flushing).
{\fontfamily{qcr}\selectfont clflush\_opt} is only available in the most recent Intel SkyLake microarchitecture. Based on our knowledge,
there is no hardware available in the market that supports {\fontfamily{qcr}\selectfont CLWB}.
We cannot evaluate them in this paper. However,
using these two instructions should lead to better performance 
with our method proposed in this paper. More importantly, these two instructions use cache block-based cache flushing, hence they have the same problem as discussed above for large target data objects. Our proposed method can help them improve performance.

%A variant of {\fontfamily{qcr}\selectfont clflush} is the 
%recent {\fontfamily{qcr}\selectfont clflush\_opt} instruction that
%maximizes the concurrency of {\fontfamily{qcr}\selectfont clflush} within individual threads and across threads.


%https://lwn.net/Articles/502612/
%{\fontfamily{qcr}\selectfont msync}: this is a system call that flushes 
%data from volatile page cache to persistent storage.








%The cache-related terminology used is as follows [5].
%\textbf{Add a simple architecture figure?}