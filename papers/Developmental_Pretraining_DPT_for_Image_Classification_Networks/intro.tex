\section{Introduction}
The advent of Deep Learning (DL) has massively aided the Artificial Intelligence community, especially in the realm of object recognition. One of the critical reasons for the success of DL has been the availability of massive image datasets \cite{Deng2009ImageNet} and the computational power offered by modern Graphics Processing Units (GPUs) that are able to accommodate the large amounts of data required by Deep Networks. State-of-the-art image recognition networks like the ResNet family \cite{he2016resnet}, VGG networks \cite{simonyan2014vgg}, EfficientNet models \cite{tan2019efficientnet} and the recently introduced Vision Transformers \cite{dosovitskiy2020vit} require extremely large amounts of data compared to their classical Machine Learning (ML) counterparts \cite{alzubaidi2021dlreview}. 

\noindent
This characteristic requirement for large amounts of data becomes a problem in fields where data availability is low like in medical fields \cite{salehi2023cnnmedstudy}. A common approach to this problem is Transfer Learning \cite{Zhuang2021TL} which consists of pre-training a network on a large dataset like ImageNet \cite{Deng2009ImageNet} and fine tune the network on a smaller dataset that is relevant to the recognition problem at hand. In this approach, basic visual recognition features are expected to be stored in the weights of the pre-trained network which is then `transferred' to the new problem where additional layers may be used to learn the features that are specific to the custom dataset of the new problem. A shortcoming of this problem is the high computational costs associated with ImageNet pre-training and the learning of unnecessary features during the pre-training process. For example, ImageNet pre-training teaches networks about the features that distinguish a coffee mug from a specific type of fish (ImageNet classes), but when applied to a problem of X-Ray classification, these weights are not only `dead-weights' but may also be misleading and could harm the performance of the network. This calls for more robust pre-training approaches that could solve the problem of data scarcity in specific settings.

\noindent
A promising approach to overcome these problems of data hungry deep networks is to add meaningful structure to the training data of the neural networks. Bengio et al. \cite{bengio2009curriculum} were the first to formalise curriculum learning as a strategy to order training points in the order of gradually increasing difficulty. This method, instead of presenting random shuffled data-points to a network, showed more promise in terms of faster convergence and the learning of more robust features. This concept has been extended since its inception to the fields of Computer Vision and Natural Language Processing with positive results \cite{wang2021curriculumsurvey}.

\noindent
In this work, we present a combination of pre-training and curriculum learning with \textsc{Developmental PreTraining} (DPT). With the regime outlined in this paper, we hope to offer a solution for the data scarcity problem in specific fields by training networks with a regime that is designed with inspiration from early visual development in infant. The regime involves a phased pre-training approach that teaches primitive visual processing knowledge that can be transferred to various downstream classification tasks. The method is designed to not introduce irrelevant and useless features into the network's weights while also being significantly more lightweight compared to a traditional pre-training approach like with ImageNet.