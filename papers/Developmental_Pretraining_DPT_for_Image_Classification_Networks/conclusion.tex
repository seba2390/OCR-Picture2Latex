\section{Conclusion}
Over the course of this study, we attempted to design a pre-training scheme to address the problem of data scarcity in certain fields. We drew inspiration from Curriculum Learning and early visual development of infants to design our \textsc{Developmental PreTrainng} regime. A phased syllabus of increasingly difficult primitive features was presented to the participating network in order to teach basic, low-level features necessary for visual processing. Although typical ImageNet pre-training approaches achieve this (and more), its large computational cost poses an impediment for those training networks on local machines. Another drawback of traditional pre-training is the learning of features that are 'un-transferable' to a novel dataset. For these reasons, we developed DPT with an emphasis on low-level features like edges and shapes to be learnt by the network using lighter datasets so that they can be used in downstream recognition problems later on.

Unfortunately, our experimentation involving the designed system did not yield expected results. Although the networks were able to converge similar performance levels, they were not able to converge at a faster rate as expected. The hypothesis was that exposure to low-level features would provide a shortcut for the DPT model during training for the benchmark on the Imagenette dataset compared to the control model with random weights. However, the pre-trained weights seemed to pose as an obstacle instead of a shortcut for training on the benchmark dataset. This could be because the DPT may have led to overfitting on the phase-specific dataset that caused the model to learn features that were not robust or generalisable.

\section{Future Work}
The shortcomings of the paper despite the related work and background for the problem suggests that a lot of work remains to be done in this area of curriculum-based pre-training before it can be deployed in the real world. We have a lot of ideas on how this work could be extended to potentially find a regime that acts as a sustainable competitor to traditional pre-training approaches.

Firstly, the optimal datasets need to be selected for a pre-training. The current regime of edge detection and shape recognition could be extended to complex object (like clothing items \cite{xiao2017fashion} or face recognition \cite{cao2018vggface2}). It is important to note that an emphasis should be placed on lightweight datasets as a primary reason to pursue this line of work is to minimize computational costs for model training while also enjoying faster convergence due to pre-learned features. Additionally, these phases should also be dealt with care so as to not cause overfitting on these phase-specific dataset. This will harm the training process of the final, downstream task. Measures like inserting dropout and batch normalisation layers could be explored and studied.

Further, pre-training approaches should lead to models that are versatile and can be applied to various fields. For this reason, DPT needs to be benchmarked across various datasets after pre-training that ranges from medical data to satellite data. Moreover, the DPT models need to be compared with the models that were pre-trained with traditional approaches. To be successful, the DPT models need to show similar rates of convergence compared to the other pre-trained models. It would also be helpful to study the actual savings in computation between DPT and traditional pre-training techniques. Finally, it would be very interesting to analyse the robustness of features learned during the DPT regime compared to traditional pre-trained models as well as models without any pre-training. Adversarial robustness could imply that the pre-training regime of DPT is likely learning visual features that are adjacent to those learnt by our visual systems.