\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{soul}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{hyperref}

\usepackage{subcaption} 
\usepackage{siunitx}
\usepackage{tabularx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%%%%%% Indizes in den Formeln geradestehend statt kursiv setzen %%%%%%%%%
\makeatother
\begingroup
\catcode`\_=13
\gdef_#1{\sb{\mathrm{#1}}}
\endgroup
\newcommand\enableuprightsubscripts{\catcode`\_=12\relax}
\newcommand\disableuprightsubscripts{\catcode`\_=8\relax}
\makeatletter
\enableuprightsubscripts       %% alle Indizes nach diesem Befehl geradestehend
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand\copyrighttext{%
  \footnotesize \textcopyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
\newcommand\copyrightnotice{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south,yshift=10pt] at (current page.south) {\fbox{\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}}};
\end{tikzpicture}%
}


\begin{document}

\title{Concept for an Automatic Annotation \\ of Automotive Radar Data Using \\ AI-segmented Aerial Camera Images}

\author{\IEEEauthorblockN{Marcel Hoffmann$^{1}$, Sandro Braun$^{1}$, Oliver Sura$^{1}$, Michael Stelzig$^{1}$, Christian Schüßler$^{1}$,\\ Knut Graichen$^{2}$, Martin Vossiek$^{1}$}
\IEEEauthorblockA{\textit{$^{1}$Institute of Microwaves and Photonics (LHFT) \quad $^{2}$Chair of Automatic Control (LRT)} \\
\textit{Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)}\\
Erlangen, Germany \\
\{marcel.mh.hoffmann, sandro.braun, oliver.sura, michael.stelzig,
christian.schuessler,\\ knut.graichen, martin.vossiek\}@fau.de}
}

\maketitle
\copyrightnotice
\vspace{-0.2cm}

\begin{abstract}
This paper presents an approach to automatically annotate automotive radar data with AI-segmented aerial camera images.
For this, the images of an unmanned aerial vehicle (UAV) above a radar vehicle are panoptically segmented and mapped in the ground plane onto the radar images.
The detected instances and segments in the camera image can then be applied directly as labels for the radar data.
Owing to the advantageous bird's eye position, the UAV camera does not suffer from optical occlusion and is capable of creating annotations within the complete field of view of the radar.
The effectiveness and scalability are demonstrated in  measurements, where 589 pedestrians in the radar data were automatically labeled within 2 minutes.
\end{abstract}

\begin{IEEEkeywords}
Automotive radar, machine learning, data annotation, sensor fusion, image segmentation
\end{IEEEkeywords}

%
%
%
\section{Introduction}

In the field of autonomous driving, radar systems are one of the key components for generating environment maps and detecting vulnerable road users (VRU) \cite{b1}.
To interpret the measured data, machine learning approaches are increasingly being used, for example, to classify pedestrians \cite{b2}.
For a better understanding of the environment, radar maps can also be semantically segmented using neural networks (NNs) \cite{b3}.

However, a major challenge is the acquisition of large, well-labeled datasets required for the training of an artificial intelligence (AI).
This is because manual annotation at each stage of radar processing is nontrivial and time consuming.
Especially due to the often poor angular resolution, labeling without high-quality reference is almost impossible.

To overcome this challenge, automated methods to annotate radar data are required.
Camera images are commonly used for this task, from which labels for the radar data are directly extracted using existing, well-trained NNs for image interpretation.
In \cite{b4}, objects in camera images are marked with bounding boxes to create automatic labels for range-Doppler (RD) maps from frequency-modulated continuous-wave (FMCW) radar data.
One step further along the FMCW processing chain (cf. Section II.), the authors in \cite{b5} label the range-Doppler-azimuth (RDA) cubes using camera images with a pixel-correct instance segmentation to train a NN for the detection of VRUs.
Radar point clouds, at the end of the FMCW process chain can also be automatically labeled using panoptically segmented camera images as a profitable combination of instance segmentation and semantic segmentation \cite{b6}.
Given the orientation of the camera in the vehicle, it is difficult to map camera images to the typically horizontal radar planes.
This is solved in \cite{b7} with a multi-sensor labeling process that additionally uses lidar to spatially synchronize the RD maps with the camera images with higher accuracy.

\begin{figure}[t]
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=0.98\linewidth]{images/1_concept.png}
    \vspace{-0.3cm}
    \caption{Automatic annotation of radar data from a vehicle (red) with labels (green bounding boxes) taken from segmented, synchronized aerial images. With this setup, the FoV of the UAV's camera (orange) covers the complete radar FoV (blue).}
    \label{fig:concept}
\end{figure}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=170mm]{images/2_process_chain.pdf}
    \vspace{-0.2cm}
    \caption{Overview of the proposed approach for an automatic annotation of automotive radar data. The top row describes the processing of the UAV images, while the lower row corresponds to the radar processing. Colors match the sections as stated.}
    \label{fig:process_chain}
\end{figure*}

However, for these methods, labels can only be generated in areas that can be detected by the reference sensors from the vehicle’s perspective.
Furthermore, camera and lidar are notably affected by occlusion and, therefore, do not cover the complete radar field of view (FoV).
As a result, the unique benefits of radar can very often not be satisfactorily captured and labeled.
This includes scenes in which several vehicles drive in front of each other.
While radars can image this correctly, cameras and lidars only see the first car.

In \cite{b8}, a camera mounted on an unmanned aerial vehicle (UAV) is used to capture a larger portion of the radar FoV to determine the perception errors of a sensor on the vehicle and thus obtain a generally applicable sensor model.

Based on the perspective gain depicted in Fig.~\ref{fig:concept}, we present an approach to use a spatiotemporally synchronized UAV with camera that is positioned above the vehicle.
With this setup, we automatically generate labels for radar data to train arbitrary NNs.
The paper is structured according to our developed process chain shown in Fig.~\ref{fig:process_chain}.
In Section~II, the processing of radar data is discussed (blue in Fig.~\ref{fig:process_chain}).
Section~III proposes a procedure to map aerial images to the radar plane (orange in Fig.~\ref{fig:process_chain}).
The deployed panoptic segmentation for the interpretation of UAV camera images is presented in Section~IV (purple in Fig.~\ref{fig:process_chain}).
In Section~V, the fusion of radar data and segmented camera images is performed to extract regions of interest (RoI) from the radar images based on the camera labels (green in Fig.~\ref{fig:process_chain}).
Finally, Section~VI describes methods to use the fused data to automatically generate labeled radar datasets for the training of arbitrary NNs (red in Fig.~\ref{fig:process_chain}).
The conclusion is presented in Section~VII.

%
%
%
\section{Automotive Radar Processing}

Automotive radar applications are often based on radars transmitting sequences of linear FMCW signals that can be processed with techniques presented in \cite{b9}.
Our radar processing chain for the proposed automatic annotation is depicted in blue in Fig.~\ref{fig:process_chain}.
It is based on data gathered with the \SI{77}{\giga\hertz} multiple-input multiple-output (MIMO) FMCW radar AVR-QDM-110 from Symeo GmbH, an indie Semiconductor company, that is mounted to a test vehicle.
In Fig.~\ref{fig:process_chain}, this vehicle is always highlighted by a pink rectangle.
In the first step, the RD maps are generated from the raw data using a 2D fast Fourier transform (FFT) with suitable zero-padding and windowing.
For this, 3 out of 12 transmit (TX) and all 16 receive (RX) antennas are used to form a uniform linear array (ULA) with 48 elements and an unambiguous horizontal FoV of approximately \SI{140}{\degree}.
Each sequence consisted of \num{128} chirps per TX antenna with a bandwidth of \SI{1}{\giga\hertz}.

In the second step, the azimuth angle is estimated by an FFT along the calibrated ULA to create the 3D RDA cube.
Here, beamformers or even super-resolution approaches can alternatively be used for the angle estimation \cite{b9}.
Subsequently, a range-azimuth (RA) image representing the horizontal radar plane can be derived.
In the final step, the RA image is transformed to Cartesian coordinates and then aligned with the global frame for a simplified fusion with the segmented camera image, which will be described in Section~V.

%
%
%
\section{Mapping Procedure for Aerial Images}

\begin{figure*}[tb]
    \centering
    \subfloat[]{
        \includegraphics[width=0.2\linewidth]{images/31_uav_img_box.jpg}}
   \hspace{0.5cm}
	\subfloat[]{
        \includegraphics[width=0.2\linewidth]{images/32_undistorted_box.jpg}}
  \hspace{0.5cm}
	\subfloat[]{
        \includegraphics[width=0.2\linewidth]{images/33_labeled_box.jpg}}
  \hspace{0.5cm}
	\subfloat[]{
        \includegraphics[width=0.2\linewidth]{images/34_panoptic_box.jpg}} 
     \vspace{-0.2cm}   
    \caption{Processing chain of the UAV images. (a) shows the original 
 image, (b) presents the undistorted image after calibration, (c) demonstrates the labelling process for training, and (d) presents the panoptically segmented image. In (c) and (d), black, white, green, red, yellow, and blue correspond to the environment, street, vegetation, cars, obstacles, and pedestrians, respectively. The radar vehicle is marked with a pink rectangle in all four images. Three pedestrians are walking within the radar FoV.}
    \label{fig:img_chain}
\end{figure*}

A crucial part of labeling radar data is the precise mapping of the aerial images to the horizontal radar plane that has been transformed to the global frame in the previous section.
All steps concerned are highlighted in orange in Fig.~\ref{fig:process_chain}.

For correct mapping, the camera images have to be calibrated to remove distortions caused by the lens.
Next, the observed ground area has to be estimated based on the UAV's position.
Finally, the aerial image plane has to be transformed to the ground plane using the homography matrix of the two planes.
As the observed area in our setup is small compared to other remote sensing applications, we consider the earth to be a horizontal plane.
However, for large areas, distortions caused by terrain or sensor motion have to be corrected.
This procedure of precisely converting images to a map-suitable form is called orthorectification.
\cite{b10} provides an overview of methods for generating UAV orthoimages.

%
%
\subsection{Camera Calibration}

We used the Raspberry Pi Camera Module V2.1 with a lens with a physical vertical FoV of $\alpha = \SI{115}{\degree}$ and a horizontal FoV of $\beta = \SI{80}{\degree}$.
The UAV (Holybro S500 V2) was positioned at a height of \SI{25}{\m} and was connected to the vehicle's Robot Operating System via WLAN.
The camera was tilted to the ground at an angle of \SI{60}{\degree}.
With this setup, it is possible to overlook the whole scene and create a large overlap with the radar's FoV without the need for large altitudes.
Owing to the lens characteristics, a strong radial and tangential distortion occurred in the images, as depicted in Fig.~\ref{fig:img_chain}a).
Assuming a pinhole camera model, the distortion can be corrected by a calibration method described in \cite{b11}, which we realized with functions from the OpenCV software library \cite{b12}.

The calibrated images exhibit significantly reduced distortion, thus depicting the real geometry more accurately.
This is also visible from the colored lines in the original and calibrated example images in Fig.~\ref{fig:img_chain}a) and Fig.~\ref{fig:img_chain}b), respectively. 

%
%
\subsection{Determination of the Observed Ground Area}

To establish a relationship between the global frame and the image plane, the position, orientation, and FoV of the camera must first be determined with respect to the world coordinate system.
The position of the center point $c_0$ of the observed ground plane can be determined via the rotation and translation of the camera with respect to the global coordinate system.
For this, the data gathered from the drone's flight controller (Holybro Pixhawk 4) were used.
It features a Global Navigation Satellite System (GNSS) module and an Attitude And Heading Reference System (AHRS) for pose estimation.

Based on the pinhole camera model, the observed ground area can be described as a quadrilateral using four vectors from each image corner to the corners of the ground plane \cite{b13}.
The four corner coordinates $c_{T/B, L/R}$ (top/bottom, left/right) or vertices of the observed ground area can be described by 
\begin{equation}
    c_{T/B,L/R} = \left(\ell_{T/B}, \ \ell_{L/R}\right)^\text{T}\text{,} \\
 \end{equation}
with $\ell$ being the distance from $c_0$ in the respective dimension:
\begin{equation}
    \begin{split}
        \ell_{T,B} &= z\cdot \tan\left(\pm\frac{\alpha}{2} \pm \tan^{-1}\left(\frac{h}{2f}\right)\right)\text{,} \\
        \ell_{L,R} &= z\cdot \tan\left(\pm\frac{\beta}{2} \pm \tan^{-1}\left(\frac{w}{2f}\right)\right)\text{.}
    \end{split}
\end{equation}
Here, $z$ denotes the UAV's altitude, and $h$, $w$, and $f$ are lens height, lens width, and focal length, respectively \cite{b13}.

%
%
\subsection{Homography}

In the final step, the homography approach aimed to realize a projection of the camera image to the global frame (cf. Fig. \ref{fig:concept})\cite{b14}\cite{b15}.
For this, the four vertices of the calibrated camera image were mapped to the previously computed corner coordinates $c_{T/B, L/R}$ on the horizontal ground plane.
As a result, the camera image was also mapped to the global frame, which is depicted in Fig.~\ref{fig:process_chain} in orange and blue, respectively.
Consequently, objects in both radar and camera images overlapped, and it became possible to fuse these two data types, which will be described in Section V (green in Fig.~\ref{fig:process_chain}).

%
%
\section{Panoptic Segmentation of Aerial Images}

To automatically generate labels and extract information from camera images, the  images are interpreted using a NN.
For this, we chose a panoptic segmentation because it combines the advantages of instance segmentation and semantic segmentation and, consequently, offers the best flexibility for the actual task of labeling the radar data.
Therefore, it allows the annotation of individual objects, such as for a radar-based VRU classification.
Moreover, uncountable regions like the street can be segmented pixel-correct, which could be used to generate a training dataset for the segmentation of radar gridmaps.
In Fig.~\ref{fig:process_chain}, this step is colored purple.

%
\subsection{Manual Annotation of Camera Images}

In this application, the aim of panoptic segmentation was to label radar data optimally rather than extract the maximum information content from the image.
Therefore, we defined a small number of 12 countable and non-countable classes that are of specific relevance in the radar context because of their unique features in radar images.
The classes listed in Table~\ref{tab:classes} contain moving road users like pedestrians and cars as well as static objects like poles appearing as point targets.

Based on these classes, we manually annotated a small and specific dataset to train the panoptic segmentation of the UAV images. 
Due to the easy interpretability of camera images, it is possible to annotate images efficiently and quickly even without a technical background, which, again, is not feasible for radar images.
For the annotation process, image segments were marked with 12 colors representing the 12 classes.
An example for this is shown in Fig. \ref{fig:img_chain}c).

\begin{table}[tb]
\centering
\caption{Classes for panoptic segmentation.}
    \begin{subtable}{\linewidth}  
    \centering  
        \begin{tabular}{c|c}
        
        \hline
        \textbf{Class} & \textbf{Description} \\
        \Xhline{4\arrayrulewidth}
        Environment & Background (e.g., small vegetation/objects, sidewalks) \\
        \hline
        Street & Roads \\
        \hline
        Trees & Larger vegetation with trunk  \\
        \hline 
        Houses & Buildings with walls  \\
        \hline
        Barriers & Continuous wall-structures not resembling houses\\
        \hline
        Poles & Cylindrical, point-target-like objects \\
        \hline
        Obstacles & Free-standing, large obstacles (e.g., power boxes, stones) \\
        \hline
        Cars & Moving and parking cars \\
        \hline
        Trucks & Larger trucks and buses (bigger than cars) \\
        \hline
        Motorbikes & Motorized two-wheeler including riders \\
        \hline
        Bikes & Bicycles including riders \\
        \hline
        Pedestrians & Moving and static pedestrians\\
        \end{tabular}
    \end{subtable}
\label{tab:classes}

\end{table}

\subsection{Training of the Neural Network for Camera Images}

For this application, the Panoptic Feature Pyramid Network "PanopticFPN" model from \cite{b16} was chosen, which uses the open-source platform Detectron2 from Meta Research \cite{b17}.

The final training dataset contained \num{528} images in total; \num{338} of which were manually annotated images and \num{190} images were taken from the UAVid dataset \cite{b18}.
The validation dataset contained 96 manually annotated images.
The distribution of the instances of each class are listed in Table~\ref{tab:instances_dist}.
The training was performed on an Nvidia GeForce RTX 3090 graphics card with \SI{24}{\giga\byte} VRAM and CUDA v11.3.
The batch size was set to 6, and the number of iterations were \num{15000} with a learning rate reduction to \SI{10}{\percent} at \SI{60}{\percent} and to \SI{1}{\percent} of the original learning rate at \SI{85}{\percent} of the total iteration number.

\begin{table}[tb]
	\centering
 	\caption{Distribution of countable class instances.}
	\begin{subtable}[][][c]{0.46\linewidth}
		\captionsetup{justification=centering}
		\centering
        \caption{Training dataset.}
		\begin{tabular}{c|S[table-format=4]}
			\hline
			\textbf{Class} & \textbf{Instances} \\
			\hline
			Barriers & 1452 \\
			Poles &  5745\\
			Obstacles &  2943\\
			Cars &  14280\\
			Trucks &  94\\
			Motorbikes &  79\\
			Bikes &  1118\\
			Pedestrians & 4520 \\
			\hline
			\textbf{Total} & 30231 \\
			\hline
		\end{tabular}
	\end{subtable}
	\begin{subtable}[][][c]{0.46\linewidth}
		\centering
        \caption{Validation dataset.}
		\captionsetup{justification=centering}
		\begin{tabular}{c|S[table-format=4]}
			\hline
			\textbf{Class} & \textbf{Instances} \\
			\hline
			Barriers & 646 \\
			Poles &  1976\\
			Obstacles &  1035\\
			Cars &  844\\
			Trucks &  3\\
			Motorbikes &  20\\
			Bikes &  354\\
			Pedestrians & 222 \\
			\hline
			\textbf{Total} & 5100 \\
			\hline
		\end{tabular}
	\end{subtable}
	\label{tab:instances_dist}
\end{table}


\subsection{Evaluation of the Trained Model}

An exemplary result of panoptic segmentation is shown in Fig.~\ref{fig:img_chain}d).
To avoid falsification, the corresponding image in Fig.~\ref{fig:img_chain}c) was not included in the training.

In the following evaluation, we set the focus on pedestrians because VRU classification has high relevance in radar applications.
This is a suitable verification of the proposed approach, because on the one hand, the moving pattern of a pedestrian can be easily detected as a Doppler signature in RD maps.
On the other hand, they are harder to detect from the UAV's perspective due to their small size, which also requires a precise coregistration of camera and radar.

\begin{table}[tb]
    \centering
    \caption{Metrics for the trained model.}
    \begin{tabular}{c|c|S}
         \textbf{Metric} &  \textbf{IoU interval (in \%)} & \textbf{Value (in \%)}\\
         \hline
         $mAP$ &  50:5:95 &     6.89 \\
         $mAP$-50 &  50 &  14.53 \\
         $mAP$-75 &  75 &  6.22 \\
         $AP_{car}$ & 50:5:95 &   25.70 \\
         $AP_{bike}$ & 50:5:95 &  5.55 \\
        $AP_{pedestrians}$ &  50:5:95 & 17.67\\
    \end{tabular}
    \label{tab:metrics_panseg}
\end{table}

To evaluate the accuracy of the detections, the intersection over union (IoU) will be calculated as,
\begin{equation}
	IoU = \frac{|T \cap P|}{|T \cup P|} \text{,}
\end{equation}
where $T$ denotes the area of the ground-truth and $P$ is the area of the object proposed by the panoptic segmentation.
If the IoU is larger than a threshold $k$, then a detection is considered a true-positive ($TP_c$); otherwise, it is a false-positive detection ($FP_c$).
With this, the precision $P_c(k)$ is defined as
\begin{equation}
	P_c(k) = \frac{TP_c(k)}{TP_c(k) + FP_c(k)} \text{,}
\end{equation}
with the index $\text{c}$ denoting the class evaluated.
Recall $R_c(k)$ can be calculated with the false-negative detections $FN_c$ as
\begin{equation}
	R_c(k) = \frac{TP_c(k)}{TP_c(k) + FN_c(k)} \text{.}
\end{equation}

Based on $P_c(k)$ and $R_c(k)$, an average precision $AP_c(k)$ can be determined \cite{b19}.
To also consider the influence of the IoU-threshold, $AP_c(k)$ is again averaged over $K$ IoU thresholds as defined by the detection evaluation metric from Microsoft's Common Object in Context (COCO) \cite{b19}:
\begin{equation}
	AP_c = \sum_{k=1}^{K}\frac{AP_c(k)}{K} \text{.}
\end{equation}

To generate one metric for all classes, the mean average precision $mAP$ calculates the mean over $AP_c$ of all $N$ classes:
\begin{equation}
	mAP = \sum_{c=1}^{N}\frac{AP_c}{N} \text{.}
\end{equation}

The metrics of our UAV dataset are presented in Table~\ref{tab:metrics_panseg}.
Given the large area observed from the aerial perspective, cars, bikes and pedestrians appear rather small in the image.
Consequently, slight detection deviations have a substantial influence on the IoU.
However, the focus of the proposed method is only on detecting objects and not on achieving a perfect IoU.
This is because the labeled RoI has to be larger than the actual object to ensure coverage owing to the limited synchronization precision in the mapping process.
Consequently, the APs reached are sufficient to proof the feasibility of this approach.
Of course, for large-scale applications, these metrics can be improved significantly by increasing the size of the training dataset or by decreasing the IoU threshold.

\section{Sensor Fusion of UAV Camera and Radar}

\subsection{Temporal and Spatial Synchronization}

To map the segmented camera image to the radar RA image in the global frame, both systems must be spatiotemporally synchronized.
Spatial synchronization is achieved using the GNSS data of the radar vehicle and the flight controller of the UAV.
The UTC time from the GNSS receivers can also be used for a precise time synchronization of both systems.

After this step, each camera pixel can be assigned to a region in the RA image.
This sensor fusion is depicted in green in Fig.~\ref{fig:process_chain}.
If the mapping result is not satisfactory due to offsets or positioning inaccuracies, a manual or automated correction can be performed using known reference points in the world plane and the aerial image.


\begin{figure}[tb]
    \centering
	\input{tikz_plots/cart_rois/cart_rois.tex}	
    \vspace{-0.2cm}
    \caption{Section of the radar image transformed to the global frame with the radar in the coordinate origin and the color axis in dB. The RoIs of the detected pedestrian are red.}
    \label{fig:cart_rois}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}{8.8cm}
		\begin{minipage}[l]{0.1cm}
			\caption{}
		\end{minipage}
		\hfill
		\begin{minipage}[r]{8.1cm}
            \hspace{-0.3cm}
			\input{tikz_plots/rd_roi/rd_roi1.tex}
		\end{minipage}
	\end{subfigure}
     \begin{subfigure}{8.8cm}
		\begin{minipage}[l]{0.1cm}
			\caption{}
		\end{minipage}
		\hfill
		\begin{minipage}[r]{8.1cm}
            \hspace{-0.3cm}
			\input{tikz_plots/rd_roi/rd_roi2.tex}
		\end{minipage}
	\end{subfigure}

    \begin{subfigure}{8.8cm}
		\begin{minipage}[l]{0.1cm}
			\caption{}
		\end{minipage}
		\hfill
		\begin{minipage}[r]{8.1cm}
            \hspace{-0.3cm}
			\input{tikz_plots/rd_roi/rd_roi3.tex}
		\end{minipage}
	\end{subfigure}
    \vspace{-0.2cm}
\caption{RD maps with the color axis in dB and the range section of the RoIs in red. a) shows pedestrian 1 in the azimuth section from \qtyrange{-5}{-2}{\degree}, b) shows pedestrian 2 from \qtyrange{-1}{7}{\degree}, and c) shows pedestrian 3 from \qtyrange{-1}{7}{\degree}.}
\label{fig:rd_roi}
\end{figure}
%
%
\subsection{RoI Extraction Using Bounding Boxes}

Based on the preceding spatial overlay in the world frame, the panoptic segmentation of the camera image can be directly transferred to the RA image as labels.
This can be realized using a binary mask from the segmented images for each class instance.
Consequently, only instances of the respective class remain labeled, whereas irrelevant parts are removed.

The resulting RoIs within the radar image can be visualized and extracted using rectangular bounding boxes, which allow a larger error margin for imperfect synchronizations.
This is depicted in Fig.~\ref{fig:cart_rois} for the three pedestrians in the radar FOV that are also visible in Fig.~\ref{fig:img_chain}.
For better visualization, the FoV of the depicted radar image is limited in range.
Then, the coordinates of the bounding boxes in the RA image are used to determine the relevant section of the entire radar cube to also consider the object's Doppler information.
The example in Fig.~\ref{fig:cart_rois} also demonstrates that the pedestrians cannot be annotated manually without reference, because they cannot be distinguished from other objects in the RA image.

%
%
%
\section{Generation of Labeled Radar Datasets}

\subsection{Input Data Formats for Neural Networks}

The proposed approach allows the generation of labels for all radar data formats along the processing chain depicted in blue and red in Fig.~\ref{fig:process_chain}, thus providing large flexibility for radar applications.
These range from object detection and classification tasks to radar map segmentation and even ghost target and clutter suppression.
For these tasks, possible input data are shown in Fig.~\ref{fig:process_chain} and include
\begin{itemize}
    \item RD maps, which only require the range information from the extracted RoI for the annotation;
    \item 3D RDA cubes sections that use the extracted and labeled RoI with the respective Doppler content;
    \item Target lists including point clouds derived from the radar cubes using a constant false alarm rate (CFAR) algorithm;
    \item Feature lists with standard target information combined with features cropped from radar cube sections, which are successfully applied in \cite{b20}.
\end{itemize}

Fig.~\ref{fig:rd_roi} demonstrates the effectiveness of the presented approach.
The red rectangle in the RD maps corresponds to the range expansion of the three bounding boxes in Fig.~\ref{fig:cart_rois}.
Within this range expansion, only the azimuth section corresponding to the size of the bounding box is extracted from the radar cube.
With this, a cropped RDA cube containing only the pedestrian can be created.
This small cube, or any of the abovementioned derived data formats, can subsequently be used as training data for a suitable NN architecture.
For visualization in Fig.~\ref{fig:rd_roi}, the cropped 3D RDA cubes are compressed using the maximum value in azimuth dimension.

%
%
\subsection{Scalability}

To create large and diverse labeled radar datasets, a trustworthy camera image segmentation is a prerequisite.
For this, an initial and extensive training of the NN for the  camera is required.
Once this network is trained, it can be retrained comparatively easily as needed, resulting in a low long-term overhead and high adaptability.

In our measurement campaign, we recorded \num{248} radar frames in \SI{122}{\s} that contained \num{719} moving pedestrians, from which \num{640} were detected by our panoptic segmentation with an IoU threshold of \SI{50}{\percent}.
Of those, \num{589} were correctly mapped to the RA image to subsequently be extracted as labeled RDA cube section (cf. Fig.~\ref{fig:rd_roi}).

Because we generated \num{589} labeled RDA cube sections of moving pedestrians within \SI{2}{\minute}, this process is only limited by the time required for the recording of measurement data.
No human interaction is required for the complete data processing, as everything can be performed automatically.

Even though the setup presented in this paper is purely static, this approach can also be applied while driving.
For this, a very precise spatiotemporal synchronization with good position estimation of both radar vehicle and UAV is required.

In \cite{b21} a model predictive control (MPC) for object tracking for UAVs is demonstrated.
In this method, the FoV of the UAV's camera remains fixed on a given object like the driving radar vehicle.
With this approach, the UAV can automatically follow the vehicle with a camera FoV suitable for label generation, thus enhancing the creation of large and diverse datasets of dynamic automotive scenarios.

%
%
\section{Conclusion}

In this work, we propose an approach for an automated radar data annotation using panoptically segmented aerial images.
For this, the aerial camera images are mapped to the radar's RA images that are transformed to a global coordinate system.
With this approach, the camera image covers a larger section of the radar's FoV.
It also suffers less from occlusion due to the advantageous position, thus enabling a more comprehensive annotation.
The workflow using a purpose-trained UAV camera dataset is presented and discussed in detail.
Additionally, the effectiveness and flexibility of the approach are demonstrated in measurements.
Overall, this approach can be very beneficial for a wide range of environments and radar applications and significantly extends already existing approaches.

In future work, we will extend our approach by the MPC concept from \cite{b21} to create datasets in dynamic traffic scenarios.
The setup can likewise be extended to multiple UAVs to further increase the perspective gain.
The projection approach for aerial images can also be enhanced using geo-referencing to obtain more precise homography results and thus improve the label generation for uneven surfaces. 

%
%
\section*{Acknowledgment}
The authors would like to thank the Symeo team from indie Semiconductor for their support with the radar system.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}
\bibitem{b1} C. Waldschmidt, J. Hasch and W. Menzel, ``Automotive Radar — From First Efforts to Future Systems,'' in \textit{IEEE Journal of Microwaves}, vol. 1, no. 1, pp. 135-148, Jan. 2021.
\bibitem{b2} R. Prophet et al., ``Pedestrian Classification for 79 GHz Automotive Radar Systems,'' \textit{2018 IEEE Intelligent Vehicles Symposium (IV)}, Changshu, China, 2018, pp. 1265-1270.
\bibitem{b3} R. Prophet, G. Li, C. Sturm and M. Vossiek, ``Semantic Segmentation on Automotive Radar Maps,'' \textit{2019 IEEE Intelligent Vehicles Symposium (IV)}, Paris, France, 2019, pp. 756-763.
\bibitem{b4} J. Mendez et al., ``Automatic Label Creation Framework for FMCW Radar Images Using Camera Data,'' in \textit{IEEE Access}, vol. 9, pp. 83329-83339, 2021.
\bibitem{b5} M. Dimitrievski et al., ``Weakly Supervised Deep Learning Method for Vulnerable Road User Detection in FMCW Radar,'' \textit{2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)}, Rhodes, Greece, 2020, pp. 1-8.
\bibitem{b6} S. Siddhartha, G. Wang and B. Dutta, ``Panoptic Segmentation for Automotive Radar Point Cloud,'' \textit{2022 IEEE Radar Conference (RadarConf22)}, New York City, NY, USA, 2022, pp. 1-6.
\bibitem{b7} C. Grimm et al., ``Warping of Radar Data Into Camera Image for Cross-Modal Supervision in Automotive Applications,'' in \textit{IEEE Transactions on Vehicular Technology}, vol. 71, no. 9, pp. 9435-9449, Sept. 2022.
\bibitem{b8} R. Krajewski et al., ``Using drones as reference sensors for neural-networks-based modeling of automotive perception errors,'' 2020 IEEE Intelligent Vehicles Symposium (IV), Las Vegas, NV, USA, 2020, pp. 708-715.
\bibitem{b9} S. M. Patole, M. Torlak, D. Wang and M. Ali, ``Automotive radars: A review of signal processing techniques,'' in \textit{IEEE Signal Processing Magazine}, vol. 34, no. 2, pp. 22-35, March 2017.
\bibitem{b10} J. Zhang et al., ``Aerial orthoimage generation for UAV remote sensing: Review,'' in Information Fusion, vol. 89, pp. 91–120, 2023.
\bibitem{b11} K. M. Dawson‐Howe and D. Vernon. ``Simple pinhole camera calibration,'' in \textit{International Journal of Imaging Systems and Technology}, vol. 5, no. 1, pp. 1-6, 1994.
\bibitem{b12} G. Bradski and C. Rhemann, ``OpenCV: Open source computer vision library,'' 2021. [Online]. Available: https://opencv.org.
\bibitem{b13} J. Sandino et al., ``Drone-Based Autonomous Motion Planning System for Outdoor Environments under Object Detection Uncertainty,'' in \textit{Remote Sensing}, vol. 13, no. 21, p. 4481, 2021.
\bibitem{b14} Y. Li et al., ``Geolocalization with aerial image sequence for UAVs,'' in \textit{Autonomous Robots}, vol. 44, no. 7, pp. 1199–1215, 2020.
\bibitem{b15} M.-S. Oh, Y.-J. Lee and S.-W. Lee, ``Precise aerial image matching based on deep homography estimation,'' 2021. [Online]. Available: https://arxiv.org/abs/2107.08768v1.
\bibitem{b16} A. Kirillov, R. Girshick, K. He and P. Dollár, ``Panoptic Feature Pyramid Networks,'' \textit{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, Long Beach, CA, USA, 2019, pp. 6392-6401.
\bibitem{b17} Y. Wu et al., ``Detectron2,'' 2019. [Online]. Available:\\ https://github.com/facebookresearch/detectron2
\bibitem{b18} Y. Lyu et al., ``UAVid: A semantic segmentation dataset for UAV imagery,'' in \textit{ISPRS Journal of Photogrammetry and Remote Sensing}, vol. 165, 2020, pp. 108-119. 
\bibitem{b19} T-Y. Lin et al., ``Microsoft COCO: Common Objects in Context,'' 2015. [Online]. Available: https://arxiv.org/abs/1405.0312v3.
\bibitem{b20} A. Palffy, J. Dong, J. Kooij and D. Gavrila, ``CNN Based Road User Detection Using the 3D Radar Cube,'' in \textit{IEEE Robotics and Automation Letters}, vol. 5, no. 2, pp. 1263-1270, April 2020.
\bibitem{b21} F. Snobar et al., ``FoV-based model predictive object tracking for quadcopters,'' in \textit{Proceedings of the 9th IFAC Symposium on Mechatronic Systems (Mechatronics 2022)}, Los Angeles, CA (USA), 2022, pp. 18-23.
\end{thebibliography}

\end{document}