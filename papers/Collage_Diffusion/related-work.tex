\vspace{-1.5em}
\section{Related Work}
\vspace{-0.5em}

One natural starting point is to ``flatten'' the input layers into image $x_c$ by alpha-compositing the sequence of layer images $x_1, x_2, ...$ into a single image \cite{porter1984compositing}, then use diffusion-based image harmonization to improve the visual quality of the image \cite{sdedit,avrahami2022blended,saharia2022palette}. 
Diffusion-based approaches can harmonize geometry \cite{sdedit,objectstitch}, rather than restricting focus to color and lighting \cite{cong2020dovenet,cong2021bargainnet,hong2022shadow,xue2022dccf}.
The problem with this flatten-then-harmonize algorithm is that generated results may diverge from the content of the initial image, undermining user intent. For example, in \cref{figure:ca_ft_motivation}, noise-based harmonization \cite{sdedit} turns the pink roses into poppies despite the prompt and turns sliced sushi ginger into whole ginger. 
We seek to better maintain the spatial and appearance fidelity of the initial layers. 

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth,valign=m]{figures/harmonization.pdf}
    \caption{Without layer information, image harmonization can lead to a loss of spatial and appearance fidelity. Added noise can disrupt object-location mappings---on the left, ``poppies'' take the place of the ``roses.'' Added noise also can obscure specifics of an object's appearance---on the right, the generated ``ginger'' is whole instead of sliced.}
    \label{figure:ca_ft_motivation}
\end{figure}

\vspace{-1.5em}
\paragraph{Improving Spatial Fidelity}

Prior work has suggested approaches to (1) define spatial layouts of scene objects, and then (2) generate objects according to the desired layout. 
Existing techniques define spatial layouts using segmentation maps, whether defining a region for inpainting \cite{avrahami2022blended,sdedit,objectstitch,chen2023anydoor,yang2023paint} or providing a full-image segmentation map \cite{spatext,ediffi}. 
(Reference-based) inpainting approaches struggle to maintain global coherence with many layers (see Appendix). 
Instead of hand-drawing a segmentation map, we see layers as an intuitive, alternative way to specify spatial composition. 
\vspace{-1.5em}

\paragraph{Improving Appearance Fidelity}
In addition to generating objects in the desired locations, we aim to preserve visual characteristics of input layers. 
Several recent works specialize diffusion models to particular visual concepts (objects, styles, etc.) \cite{TextualInversion,Dreambooth,CustomDiffusion}, requiring several input images and either fine-tuning the model \cite{Dreambooth,CustomDiffusion}, learning a specific textual representation for the object \cite{TextualInversion}, pretraining on reference images \cite{xiao2023fastcomposer}, or reverse-engineering a prompt for a given image \cite{PEZ}. 
These methods struggle to generate high-quality images of scenes with compositions of many objects \cite{TextualInversion,Dreambooth,CustomDiffusion,xiao2023fastcomposer}.
In addition, appraches that fine-tune model weights require either joint multi-concept training or post-hoc combination of model weights, both of which struggle in regimes with several objects \cite{Dreambooth,CustomDiffusion}.
Alternatively, ControlNet \cite{zhang2023adding} enables us to preserve derived features of input layers (edge maps, pose, etc.) without learning a visual concept personalized to the specific object. 

We address the goal of appearance fidelity by extending both textual inversion \cite{TextualInversion} and ControlNet \cite{zhang2023adding} for performance with individual layers. 
We find that the learned representations are effective for maintaining key visual characteristics of input layers when paired with techniques for spatial control. 
When preserving an image structure from an input layer such as an edge map, our extension of ControlNet is effective. 
\vspace{-1.5em}

\paragraph{Image-to-Image Approaches} \label{related:img2img}
Constrained image harmonization can also be framed as image stylization: from low-quality layer composite to high-quality harmonized output. 
Stylization can be approached using existing methods for controlled image-to-image diffusion \cite{prompt2prompt,brooks2022instructpix2pix,tumanyan2022plug,mokady2022null,zhang2023adding}. 
Derived features (canny edges, pose, etc.) can provide control \cite{zhang2023adding}, but fails to constrain scene composition---the locations of objects are not preserved. 
Other methods directly \cite{prompt2prompt, tumanyan2022plug,mokady2022null} or indirectly \cite{brooks2022instructpix2pix} manipulate U-Net attention layers (cross-attention \cite{prompt2prompt,brooks2022instructpix2pix,mokady2022null} and self-attention \cite{tumanyan2022plug}) to maintain image structure while making either local edits (adding/removing/modifying objects) or global edits (style, lighting). 
Unfortunately, this approach is insufficient for layer-conditional diffusion.
\emph{Input layers often need to be changed significantly} to fit together in a harmonized image, as objects may need to be rotated, partially occluded, etc. (see the orientation of the sushi in \cref{fig:method}).
This is difficult when preserving the ``structure'' of the input image. 
We evaluate against one constrained image-to-image approach \cite{tumanyan2022plug}, and discuss additional baselines in the Appendix. 
Less constrained harmonization techniques \cite{sdedit} serve as a more useful starting point for \textit{Collage Diffusion} since they allow the desired flexibility in image structure. 
\vspace{-1.5em}

\paragraph{Layered Image and Video Editing}
Layer-based image and video editing is well-established in computer graphics \cite{porter1984compositing,wang1994representing} and is being increasingly adopted in machine learning-driven methods \cite{bar2022text2live, latentPrior, kasten2021layered, lu2020layered}. 
Layered representations allow modification of individual components in images \cite{bar2022text2live, latentPrior} and in video \cite{bar2022text2live, kasten2021layered, lu2020layered}.
This process often requires generating a layered representation from a single input video or image.
In contrast, we assume that layered information is provided as input, using machine learning to synthesize image output from the layers. 
