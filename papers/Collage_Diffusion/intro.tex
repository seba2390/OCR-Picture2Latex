\section{Introduction}
\label{sec:intro}
\vspace{-0.5em}

Diffusion-based image generation \cite{sohl2015deep,song2019generative,ho2020denoising, dhariwal2021diffusion, dalle2, latentDiffusion} has captured widespread interest with its seemingly magical ability to generate plausible images from a text prompt. Unfortunately, text is a highly ambiguous specification of an image, forcing users to spend significant time tweaking prompt strings to obtain a desired output. 
A body of recent work has therefore focused on providing more precise controls for scene composition via additional inputs: controlling composition via sketching\,\cite{ediffi}, filling in user-provided segmentation masks\,\cite{avrahami2022blended,objectstitch}, providing an image seed for generation\,\cite{sdedit}, etc. Similarly, the desire to precisely dictate object appearance, \emph{``the sushi in THIS reference photo''} rather than \emph{``the sushi''}, has led to approaches that condition generation based on example images \cite{TextualInversion,Dreambooth,CustomDiffusion}.

We seek to give users precise control over image output when creating scenes featuring a collection of objects with a specific spatial arrangement. For example, in Figure \hyperlink{fig:teaser}{1}, ``A bento box with rice, edamame, ginger, and sushi'' neither describes what items go in which Bento bin, nor suggests how each of the items should look. Rather than relying on ambiguous text prompts or forcing the user to sketch scene forms, we return to a traditional and easy-to-create means of expressing artistic intent: \emph{making a sequence of layers} to express a desired scene layout and the appearance of the scene objects. To specify a scene, a user need only acquire reference images of desired scene objects (e.g., via image search or via output from an existing generative model), arrange them on a canvas using a traditional layer-based image editing UI, and pair each object with a text prompt. 

Given these layers, we introduce \emph{Collage Diffusion}, a diffusion-based image harmonization algorithm that generates images that 1) have \emph{fidelity} to the input layers' spatial composition and object appearance, but 2) exhibit global \emph{harmonization} and visual coherence that is representative of ``plausible'' real-world images.  
There is an inherent tradeoff between harmonization and fidelity: harmonization involves changing properties of the input layers so that objects ``fit together'' in a consistent image, while fidelity involves preserving properties of the layers. 
The key challenge is harmonizing a sequence of layers while limiting variation in certain layer properties (color, texture, edge maps, etc.), but allowing variation in other properties. 
We tackle this challenge by leveraging the rich information present in layer input---building upon prior diffusion-based techniques for image harmonization, spatial control, and appearance control, we extend each approach for performance with layers, in particular focusing on mechanisms for per-layer control. 

Specificially we make the following contributions:

\begin{enumerate}%
	\item We introduce layer-conditioned diffusion, where generation is conditioned on alpha-composited RGBA layers as well as text prompts describing the content of each layer. Sequences of layers can be authored by users in minutes, and \textit{Collage Diffusion} generates high-quality images that respect both the desired scene composition and object appearance, even for complex scenes with many layers. 

	\item We extend prior diffusion-based control mechanisms \cite{ediffi,TextualInversion,zhang2023adding} to operate on sequences of layers, ensuring that output images adhere to the composition depicted by the layers (cross-attention) and retain salient visual features of objects in each layer (textual inversion, ControlNet). 

	\item 
	We illustrate how layer input allows users to control the harmonization-fidelity tradeoff on a per-layer basis and also enables users to iteratively refine generated images.
\end{enumerate}
\vspace{-0.5em}