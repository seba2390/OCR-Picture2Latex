\section{Numerical Experiments}\label{sec:num}
In this section we present numerical experiments for two problems: the finite-horizon multi-arm bandit with multiple pulls per period,and subset selection \citep{chen2008,Law1985}. These experiments demonstrate numerically that our index policy is indeed asymptotically optimal. We also compare the finite-time performance of our policy to other policies from the literature.  
Although our previously provided theoretical results do not apply to finite $K$, we see that our index policy performs strictly better than all benchmarks considered in both of the problems.

\subsection{Multi-armed bandit}\label{subsec:mab}
In our first experiment, we consider a Bernoulli multi-armed bandit problem with a finite time horizon $T=6$, and multiple pulls per time period. A player is presented with $K$ arms and may select $m=\lfloor K/3\rfloor$ of them to pull at every time st. Each arm pulled returns a reward of $0$ or $1$. 
The player's goal is to maximize her total expected reward. We take a Bayesian-optimal approach and impose a Beta(1,1) prior on each of the arm. The values of the state then correspond to the posterior parameters of the K arms.

For comparison, we include results from an upper confidence bound (UCB) algorithm with pre-trained confidence width. At every time step, we compute $\mu_i + \alpha*\delta_i$ for each arm $i$, where $mu_i$ and $\delta_i$ are the sample mean and standard deviation of arm $i$. We pre-train $\alpha$ by running the UCB algorithm on a different set of data (but simulated with the same distribution) with values of $\alpha$ ranging from 0 to 5 and then set $\alpha$ to the value that gives the best performance. 

Figure \ref{fig:mab} plots the reward per arm (expected total reward divided by $K$) 
against $K$, for $K=12,120,1200,12000$. 
The red dashed line represents the upper bound computed using $P(\lambdav^*)$. For each policy, circles show the sample mean of the total reward per arm, and vertical bars indicate a 95\% confidence interval for the expected total reward per arm.  The UCB policy's sample means are connected by a dashed green line, and the index policy's are connected by a dashed black line.
Values are calculated using 5000 replications.

The index policy consistently outperforms the UCB policy. As $K$ grows large, the confidence interval for the index policy's total performance per arm overlaps with the upper bound, which numerically attests to the accuracy of Theorem \ref{th:asp} and illustrates the rate of convergence.

\begin{figure}\label{fig:mab}
\begin{center}
\includegraphics[scale=0.3]{plot_mab.pdf}
\caption{upper bound and simulation results of MAB}
\end{center}
\end{figure}

%\subsection{Project assignment problem}
%In our second experiment, we consider the following finite-horizon restless bandit problem, which we call the project assignment problem.
%
%Suppose a team has $K$ ongoing projects and $m=\lfloor K / 5 \rfloor$ engineers who can work on any single project at a time.  The manager of the team decides which projects to prioritize on a weekly basis. 
%Each project's state is described by an integer $s$ set to $1$ at the initial time.  This state can either remain unchanged as we move to the next week, or can increment by $1$.  When an engineer works on a project in state $s$ for a week, the project moves to the next state with probability $p_1(s)$; otherwise, if no engineer works on the project, it transitions to the next state with probability $p_2(s)$.  Each project can be worked on by at most 1 engineer.  When a transition happens, the team collects a reward of $r$.  \pfcomment{does using $r$ as a constant here overlap wih other notation?} The manager's goal is to maximize the total expected reward over a horizon of $T=5$ weeks. Here we set $p_1(s) = 0.9 - 0.6s/T$, and $p_2 = 0.1 p_1$. 
%
%For comparison we use Whittle's index policy \citep{whittle1988} and a randomized policy that selects $m$ projects randomly at every time step. To apply Whittle's index policy which is for infinite horizon setting, we convert the problem to a infinite horizon problem by adding an absorbing state for time $T$ onwards, and make all the state at time $T$ transit to this absorbing state with probability 1.
%
%Figure \ref{fig:rb} presents the outcome of the experiment at $K=10,100,1000,10000$.
%Again we plot the number of projects against the per arm reward. We use red dashed line to represent the upper bound. The green dashed line shows the result of the randomized policy, which perform significantly worse than the other two. The index policy is represented by the black dashed line and the Whittle's index policy is represented by the blue dashed line. Both the index policy and whittle's index policy tends to the upper bound as $K$ increases. Since the confidence interval of the two overlaps, we conclude that the two perform closely to each other. Since the whittle's index policy may not be near-optimal for a finite horizon case, we suspect that with a more complicated problem, we will be able to see a more significant difference.
%\begin{figure}\label{fig:rb}
%\centering
%\includegraphics[scale=0.5]{plot_restless.pdf}
%\caption{\pfcomment{include caption}}
%\end{figure}

\subsection{Subset selection problem}
In the third experiment, we consider a subset selection problem in ranking and selection whose goal is to identify $m$ best designs out of $K$ designs, each with some underlying distribution $\theta_x$. This problem is considered in \citep{chen2008} as well as \citep{Law1985}. We assume $\bar{m}$ parallel computing resources are available; at each time step we select $\bar{m}$ out of $K$ design to evaluate. After $T$ rounds of evaluation, we select $m$ best designs. In this numerical study, we set $T=4$, $\frac{m}{K}=0.3$ and $\frac{\bar{m}}{K}=0.5$. We consider the situation when the outcomes of evaluation are binary. But note that our model can handle any real-valued outcomes.

Below is how we formulate this problem as an RMAB:
\begin{equation}\label{prime_ss}
\begin{aligned}
& \underset{\allp\in\allpset}{\text{maximize}}
& & \mathbb{E}^{\allp}\left[\sum_{t=1}^{T+1}\allr_t\big(\allstater_t,\allar_t\big)\right] \\
& \text{subject to}
& & P^{\allp}(|\allar_t|=m)=1, \; \; \text{for }t=T+1,\\
& & & P^{\allp}(|\allar_t|=\bar{m})=1, \; \; \text{for }1\leq t\leq T,
\end{aligned}
\end{equation}
where $R_t(\allstater_t,\allar_t)=0$ when $t\leq T$, and $R_t(\allstater_t,\allar_t)=\sum_{x=1}^{K}\mathbb{E}[\theta_x|\allstater_{t,x}]$ when $t= T+1$. We start with a uniform prior for each design. Note that in this formulation, the number of sub-processes allowed to set active varies with time horizon. Although this number, denoted as $m$, is fixed in Theorem \ref{th:asp}, we can show that the result still holds for a time dependent $m_t$.

We compare the performance of our policy against the \textit{OCBA-m} selection procedure proposed in \citep{chen2008}. Since \citep{chen2008} considers a slightly different setting in which a policy maker can evaluate a design more than once in a time step, we modify the procedure slightly to fit our setup: instead of sampling according to the number of times dictated by the algorithm, we rank the designs by their desired number of samples, and simulate the first $\bar{m}$ of them.  Moreover, since OCBA-m begins with a cold-start, for fair comparison, we allocate a sample corresponding to a positive outcome and a sample corresponding to a negative outcome to each of the design in addition to the total $T\bar{m}$ samples (Recall for the index policy we start with a uniform prior). We also use the UCB policy as a standard of comparison. The implementation of the UCB policy is similar to the one in section \ref{subsec:mab}.

The simulation results show that all the three policies perform closely when $K$ is small, with OCBA-m policy having a slight edge for $K=10$. From $K=100$ onwards, the index policy consistently outperforms the other two. In addition, the gap between the upper bound and the index policy vanishes as K becomes large, while the gaps between the upper bound and the other two policies remain constant.
\begin{figure}\label{fig:os}
\centering
\includegraphics[scale=0.3]{plot_hiring1.pdf}
\caption{Upper bound and simulation result of subset selection}
\end{figure}