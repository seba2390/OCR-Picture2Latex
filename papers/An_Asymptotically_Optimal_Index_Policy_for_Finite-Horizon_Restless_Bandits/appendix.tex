\section{Notation}\label{ap:notations}
\begin{centering}
\begin{longtable}{| p{.25\textwidth} | p{.75\textwidth} |}
\hline
$\allstates,\allactions,\allpr^{\cdot}(\cdot|\cdot),\allr(\cdot,\cdot)$  & State space, action space, transition kernal and reward function of the original MDP.\\ 
\hline
$\substates,\subactions,\subpr^{\cdot}(\cdot,\cdot),\subr(\cdot,\cdot)$& State space, action space, transition kernal and reward function of the sub-processes of the original MDP.\\ 
\hline
$\allstate$, $\allstater$ & Generic element and random element of $\allstates$.\\ %$\allstate_t$ emphasizes that the state is at time $t$.\\ 
\hline
$\substate$, $\substater$ & Generic element and random element of $\substates$. \\%$s_{t,x}$ emphasizes that the state is at time $t$.\\ 
\hline
$K$ & Number of sub-processes.\\ 
\hline
$T$& Time horizon\\ 
\hline
$m$& Number of sub-processes to be set active per time step\\ 
\hline
$\allpset$& Set of all Markov policies of the original MDP\\ 
\hline
%$\allpset$& Set of all feasible Markov policies of the original MDP\\ 
%\hline
$\allp(\allstate,\allaction,t)$ & the probability of choosing action $\allaction$ in state $\allstate$ under policy $\allp$ at time $t$.\\
\hline
$\subpset$& Set of all Markov policies of the sub-MDP\\ 
\hline
$\subp(\substate,\subaction,t)$ & the probability of choosing action $\subaction$ in state $\substate$ under policy $\subp$ at time $t$.\\
\hline
$\subpset^*(\lambdav)$& Set of Markov deterministic optimal policies for sub-MDP $Q(\lambdav)$, given $\lambdav\in\Rb^{T}$.\\
\hline
$\subp^{\lambdav}$& An element in $\subpset^{\lambdav}$.\\
\hline
$\allp^{\lambdav}$& A deterministic optimal policy for the relaxed problem which obtained by the decomposition method in Lemma \ref{th:decom}, given $\lambdav\in\Rb^{T}$.\\
\hline
$\Pv(\lambdav)$& Optimal value of the relaxed problem, given $\lambdav\in\Rb^{T}$.\\ 
\hline
$\lambdav^*$& An value that attains $\inf_{\lambdav}\mathbf{P}(\lambdav)$\\ 
\hline
$\subp^{**}$& An optimal markov policy for the sub-MDP which satisfies $\Eb^{\subp^{**}}[\subar_t]=\frac{m}{K}$, $\forall 1\leq t\leq T $.\\ 
\hline
$\hat{\allp}$& The index based policy proposed by this paper\\ 
\hline
$\bar{\beta_t}$& Indices of the tied sub-processes.\\ 
\hline
$I_t$& The set of states occupied by the tied sub-processes.\\ 
\hline
$N_t(s)$& The number of sub-processes in state $s$ at time $t$ under index policy $\hat{\allp}$.\\ 
\hline
$P_t(s)$& The probability of an individual sub-process landing in state $s$ at time $t$ under $\subp^{**}$. $P_t(s)=P^{\subp^{**}}[S_t=s]$\\ 
\hline
$\alpha$& The ratio between the number of sub-processes set active, $m$, and the total number of sub-processes $K$.\\ 
\hline
$M_t(s)$& The number of sub-processes in state $s$ at time $t$ that are set active under our index policy $\hat{\allp}$.\\ 
\hline
$Y_t(s',s)$ & The number of sub-processes set active by $\hat{\allp}$ in $s'$ at time $t$ which transition to state $s$ at time $t+1$. \\
\hline
$X_t(s',s)$ & The number of sub-processes set inactive by $\hat{\allp}$ in $s'$ at time $t$ which transition to $s$ at time $t+1$.\\
\hline
$U_t(s)$ & The set of states whose indices are greater than the index of state $s$ at time $t$. $U_t(s) = \{s''\in \substates:\beta_{t}(s'')>\beta_{t}(s)\}$. \\
\hline
$V_t(s)$ & The set of states whose indices are equal to that of $s$.

 $V_t(s) = \{s''\in\substates:\beta_t(s'')=\beta_t(s)\}$.\\
\hline
$|\mathbf{v}|$ &  An operation that sums all the elements in vector $\mathbf{v}$.  \\
\hline
$H_1$, $H_2$ & random variables due to the rounding rules in Algorithm \ref{ag:tiebreak}\\
\hline
$Z(\allp,m,K)$ & the expected reward of the original MDP obtained by policy $\allp$\\
\hline
\caption{List of notation}
\label{notations}
\end{longtable}
\end{centering}
\section{Upper Bound}\label{ap:up}
\proof{Proof of Lemma \ref{th:up}}
Let $\allpset_P = \{\allp\in\allpset: P^{\allp}(|\allar_t|=m)=1,\;\forall 1\leq t\leq T\}$. Let $\allpset_E = \{\allp\in\allpset:\mathbb{E}^{\allp}[|\allar_t|]=m,\;\forall 1\leq t\leq T\}$. For any $\lambdav\in\mathbb{R}^{T}$, we have
\begin{align*}
& P(\lambdav)\\
 =& \max_{\allp\in \allpset}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right]-\mathbb{E}^{\allp}\left[\sum_{t}\lambda_t(|\allar_t|-m) \right] \\
\geq & \max_{\allp\in \allpset_E}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right]-\mathbb{E}^{\allp}\left[\sum_{t}\lambda_t(|\allar_t|-m) \right] \\
= & \max_{\allp\in \allpset_E}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right]\\
\geq & \max_{\allp\in \allpset_P}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right],
\end{align*}
which is the optimal value of the original MDP. The first inequality is due to $\allpset_E\subseteq\allpset$. The first equality is due to the fact that any policy $\allp$ in $\allpset_E$ satisfies $\mathbb{E}^{\allp}[\allar_t|]=m$. The last inequality is due to $\allpset_P\subseteq\allpset_E$.
\endproof
\section{Decomposition}\label{ap:decom}
\proof{Proof of Lemma \ref{th:decom}}: 
\begin{align*}
 & \max_{\allp\in \allpset}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right]-\mathbb{E}^{\allp}\left[\sum_{t=1}^T\lambda_t\big(|\allar_t|-m\big)\right] \\
 =& \max_{\allp\in \allpset}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)-\lambda_t|\allar_t|\right] + m\sum_{t=1}^{T}\lambda_t\\
  =& \max_{\allp\in \allpset}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\sum_{x=1}^{K}r_t(\substater_{t,x},\subar_{t,x})-\lambda_t\subar_{t,x}\right] + m\sum_{t=1}^{T}\lambda_t \\
 =&\sum_{x=1}^{K}\max_{\subp\in \subpset}\mathbb{E}^{\subp}\left[\sum_{t=1}^{T}r_t(\substater_{t},\subar_{t})-\lambda_t\subar_{t}\right] + m\sum_{t=1}^{T}\lambda_t \\
\end{align*}
\endproof
The first equality is due to linearity of expectation. The second equality is obtained by the definition of $r_t(\cdot,\cdot)$ and $|\cdot|$. The third equality is obtained by the independence of the process under policies in $\allpset$. 
%\hwccomment{The last equality needs more explanation.}
\section{Show $\arg\inf_{\lambdav\in \mathbb{R}^T}\mathbf{P}(\lambdav)$ is non-empty}\label{ap:nonemp}
\begin{proof}
When $\lambdav \geq \mathbf{0}$, $\mathbf{P}(\lambdav)=\sum_x R_x(\lambdav) + m\sum_t\lambda_t \geq 0+0 = 0$. $R_x(\lambdav)$ is bounded below by 0 since a policy of not playing at all gives a total reward of 0. When $\lambdav < \mathbf{0}$, the cost of playing is negative, an optimal policy will always play at all time steps. Hence $\mathbf{P}(\lambdav)\geq m\left(\sum_t(0-\lambda_t)\right)+m\sum_t \lambda_t=0$. For the case in which $\lambdav$ contains both positive and negative entries, writing $\lambdav$ as a convex combination of $\lambdav_1 > 0$ and $\lambdav_2 < 0$ and we have that $P(\lambdav)$ is still bounded below by zero, since $P(\lambdav)$ is convex in $\lambdav$. Hence we can conclude that $\inf_{\lambdav\in \mathbb{R}^T}\mathbb{P}(\lambdav)$ exists (note here we make no claim about whether this infimum is attained by any finite $\lambdav$) and denote this value by $h^*$.

Recall we have assumed in the setup that all the rewards are bounded and non-negative, let $\bar{r}$ be an upper bound for all the reward values. For any $\lambdav$ with $\lambda_t\geq T\bar{r} $, the corresponding optimal policies for a single-arm problem will be not play at time t, for $T\bar{r}$ is at least the maximum reward obtainable by the single-arm problem. Hence $\mathbf{P}(\lambdav) \geq 0 +  mT\bar{r}$. For any $\lambdav\geq \mathbf{0}$, $\mathbf{P} = m\mathbb{E}[\sum_t r_{t,x}(S_{t,x},1)-\lambda_t|s_{1,x}]+m\sum_t \lambda_t=m\mathbb{E}[\sum_t r_{t,x}(S_{t,x},1)|s_{1,x}]$, which is independent of $\lambdav$. Hence the infimum is attained on the set $H=\{\lambdav: \lambda_t\geq \forall t \text{ and } \max_t \lambda_t\leq t\bar{r}\}$. Since $H$ is compact, there exists a $\lambdav^* \in H$ s.t. $\mathbf{P}(\lambdav^*)=h^*$. Hence $\arg\inf_{\lambdav\in \mathbb{R}^T}\mathbf{P}(\lambdav)$ is non-empty.
\end{proof}
\section{Proof the existence of $\pi^{**}$}\label{ap:exist}
The proof uses Theorem 3.6 in \cite{AltmanBook}. 
The setup in \cite{AltmanBook} is different from our problem in the following ways:
\begin{itemize}
    \item it deals with an infinite horizon problem, while we have a finite horizon problem.
    \item it has a discount parameter $\beta$ such that $0<\beta<1$, while we do not have any discount.
    \item the constraint of the original constrained problem is in the form of an inequality, while our constraints are equalities.
\end{itemize}
To be able to apply Theorem 3.6 to our problem, we need to consolidate the differences. Here is how we transform our problem:
\begin{itemize}
    \item To transform our problem to a problem with infinite horizon, we add an absorbing state $q$ such $r(q,\av) = 0$ and $\mathbb{P}(q|\allstate_T,\av)=1$ for all $\av$.
    \item We can add a discount parameter $\beta\in(0,1)$ and multiply each reward at time $t$ by $\frac{1}{\beta^{t}}$, and the value of the original problem stays the same.
    \item \cite{AltmanBook} only uses the fact that $\{\lambdav:\lambdav \geq 0\}$ is convex, and so is $\{\lambdav: \text{no constraints}\}$, so no transformation needed.
\end{itemize}
Apply Theorem 3.6, we get, there exists a $\subp^{**}\in {\subpset}_{M}$ such that
\begin{equation}\label{switch}
Q(\lambdav^*)=\inf_{\lambdav}\sup_{\subp\in \subpset_{D}} Q(\lambdav, \subp)= \sup_{\subp\in \subpset}\inf_{\lambdav} Q(\lambdav,\subp) = \inf_{\lambdav}Q(\lambdav, \subp^{**}).
\end{equation}
Since $\subp^{**}$ attains $Q(\lambdav^*)$, it is optimal. Moreover, it has to satisfy $\mathbb{E}^{\subp^{**}}\left[\sum_{t}\lambda_t*(\subar_t-\frac{m}{K})\Big | \substate_1\right]= 0$, for otherwise there is incentive for $\lambdav$ to go to either positive or negative infinity to attain the infimum. However we know $\mathbf{P}_x(\lambdav^*)$ has finite values since each reward is finite, that forces $\mathbb{E}^{\subp^{**}}[\sum_x a_{t,x}-\frac{m}{K}|s_0]= 0$ for every $t$. 

\section{Proof of  $T*$\lowercase{$\max_{s,a,t}\subr_t(s,a)$} upper bounds \lowercase{$\beta_t(s)$}}\label{ap:upbd}
It is sufficient to show that for any $\lambdav$ with $\lambda_t > T*\max_{s,a,t}\subr_t(s,a)$, $V^{\lambdav}(s,t)$ is attained by choosing $a=0$. When $t=T$, $r_T(s,1)-\lambda_T<r_T(s,1)-T*\max_{s,a,t}\subr_t(s,a)\leq 0$. On the other hand $r_T(s,0)\geq 0$ as all rewards are non-negative by the setting of our original MDP. Hence it is optimal to choose $a=0$. When $t<T$, $r_t(s,1)-\lambda_t+\sum_{s'\in \substates}P^a(s,s')V^{\lambdav}(s',t+1)< \max_{s,a,t}\subr_t(s,a)-T*\max_{s,a,t}\subr_t(s,a)+(T-t)*\max_{s,a,t}\subr_t(s,a) \leq 0 \leq r_t(s,0)$. Hence it is also optimal to choose $a=0$. Therefore $\beta_t(s)\leq T*\max_{s,a,t}\subr_t(s,a)$ for all $s,t$.

\section{A result that justifies using bisection}\label{ap:bisect}
\begin{lemma}\label{th:index}
If there exists an optimal policy $\subp$ that takes action $\subaction=1$ in state $\substate\in\substates$ at time $t$ for a sub-MDP $(\substates,\subactions,r,\subpr^{\cdot})$, and satisfies $P^{\subp}[S_t=s]>0$, then $\subaction=1$ is strictly optimal in state $\substate$ at time $t$ under a modified sub-MDP $(\substates,\subactions,r',\subpr^{\cdot})$ with $r'_t(\substate,1)>r_t(\substate,1)$, and $r'_t$ equals $r_t$ otherwise.
\end{lemma}
%\begin{proof}{Alternative proof using optimality equality with lots of hand-waves} 
%Let the optimality equation for $(\substates,\subactions,r,\subpr^{\cdot})$ at state $s$ and time $t$ be $V(s,t) = \max\{r_t(s,1)+E(s,1), r_t(s,0)+E(s,0)\}$, where $E(s,a)$ is the expected forward reward conditioning on state $s$ and action $a$. By assumption, $r_t(s,1)+E(s,1)\geq r_t(s,0)+E(s,0)$. 
%For $(\substates,\subactions,r',\subpr^{\cdot})$, the optimality equation is $V'(s,t) = \max\{r_t(s,1)+E(s,1)+\epsilon, r_t(s,0)+E(s,0)\}$. Based on previous argument, $r_t(s,1)+E(s,1)+\epsilon > %r_t(s,0)+E(s,0)$. Hence it is optimal to play.
%\end{proof}
\begin{proof}{Proof of Lemma \ref{th:index}}
We prove by contradiction. Let $V(\pi,r)$ denote the total expected reward obtained by policy $\pi$ with reward function $r$. Assume that there exists an optimal policy $\subp'$ for sub-MDP $(\substates,\subactions,r'(\cdot,\cdot),\subpr^{\cdot}(\cdot,\cdot))$ such that $\subp'(s,0,t)=1$. Since neither $r_t(s,1)$ nor $r'_t(s,1)$ contributes to the total expected reward, $V(\pi',r)=V(\pi',r')$. Let $\subp$ be an optimal policy for $(\substates,\subactions,r(\cdot,\cdot),\subpr^{\cdot}(\cdot,\cdot))$. Then we have $V(\pi,r)\geq V(\pi',r)$. 
On the other hand, $V(\pi,r')$ is greater than $V(\pi,r)$ by $(r'_t(s,1)-r_t(s,1))\mathbb{P}^{\subp}[\substater_t=s]>0$. Hence we get that $V(\subp,r')>V(\subp,r)\geq V(\subp',r)=V(\subp',r')$ contradicting that $\subp'$ is an optimal policy of sub-MDP $(\substates,\subactions,r'(\cdot,\cdot),\subpr^{\cdot}(\cdot,\cdot))$. $\square$
\end{proof}
\section{}\label{ap:eq}
\begin{proof}{Proof of Lemma \ref{th:eq}}
To prove $(1)$, when $\beta_t(s)>\lambda^*_t$, by definition of the index in \eqref{df:index}, there exists an $\epsilon>0$ such that there is a $\pi\in\Pi^{*}(\lambdav^*[\lambda_t^*+\epsilon,t])$ and $\pi(s,1,t)=1$. Recall how we construct set $\Pi^*(\lambdav)$ in Section \ref{subsec:pi}, the value function $V^{\lambdav^*[\lambda^*_t+\epsilon,t]}$ corresponding to sub-MDP $Q(\lambdav^*[\lambda^*_t+\epsilon,t])$ has to satisfy
\begin{equation*}
r_t(s,1)-\lambda^*_t-\epsilon+\sum_{s'\in\substates}V^{\lambdav^*[\lambda^*_t+\epsilon,t]}(s',t+1)P^1(s,s')\geq r_t(\substate,0)+\sum_{s'\in\substates}V^{\lambdav^*[\lambda^*_t+\epsilon,t]}(s',t+1)P^0(s,s').
\end{equation*}
Since $\lambdav^*[\lambda^*_t+\epsilon,t]$ and $\lambdav^*$ share the same elements from the $(t+1)^{th}$ position onwards, $V^{\lambdav^*[\lambda^*_t+\epsilon,t]}(s,t') = V^{\lambdav^*}(s,t')$, for all $\substate\in\substates$ and $t'\geq t+1$. Hence
\begin{equation}\label{fp}
r_t(s,1)-\lambda^*_t+\sum_{s'\in\substates}V^{\lambdav^*}(s',t+1)P^1(s,s')> r_t(\substate,0)+\sum_{s'\in\substates}V^{\lambdav^*}(s',t+1)P^0(s,s').
\end{equation}
Next we consider two separate cases: 1) State $s$ is visited with positive probability under $\subp^{**}$, that is, $P^{\subp^{**}}(\substater_t=\substate)>0$; 2) State $s$ is visited with zero probability, i.e., $P^{\subp^{**}}(\substater_t=\substate)=0$. If 1) $P^{\subp^{**}}(\substater_t=\substate)>0$, since $\subp^{**}$ is an optimal policy for the unconstrained sub-MDP $Q(\lambdav^*)$ in \eqref{dpx}, and $a=1$ attains $\max\{r_t(s,a)-a\lambda_t^*+\sum_{s'\in\substates}P^{a}(s,s')V^{\lambdav^*}(s',t+1)\}$ alone, hence $\subp^{**}(s,1,t)=1$. If 2) $P^{\subp^{**}}(\substater_t=\substate)=0$, we get $\subp^{**}(s,1,t)=1$ directly from the construction of $\subp^{**}$ in \eqref{df:rho}. 

%To prove $(2)$, when $\beta_t(s)<\lambda^*_t$, there exists an $\epsilon>0$ such that there is a $\pi\in\Pi^{*}(\lambdav^*[\lambda_t^*-\epsilon,t])$ and $\pi(s,1,t)=0$. Subsequently
%\begin{equation*}
%r_t(s,1)-(\lambda^*_t-\epsilon)+\sum_{s'\in\substates}V^{\lambdav^*[\lambda^*_t-\epsilon,t]}(s',t+1)P^1(s,s')\leq  r_t(\substate,0)+\sum_{s'\in\substates}V^{\lambdav^*[\lambda^*_t-\epsilon,t]}(s',t+1)P^0(s,s').
%\end{equation*}
%Since $\lambdav^*[\lambda^*_t-\epsilon,t]$ and $\lambdav^*$ share the same elements from the $(t+1)^{th}$ position onwards, $V^{\lambdav^*[\lambda^*_t-\epsilon,t]}(s,t') = V^{\lambdav^*}(s,t')$, for all $\substate\in\substates$ and $t'\geq t+1$. Hence
%\begin{equation}\label{fnp}
%r_t(s,1)-\lambda^*_t+\sum_{s'\in\substates}V^{\lambdav^*}(s',t+1)P^1(s,s')< r_t(\substate,0)+\sum_{s'\in\substates}V^{\lambdav^*}(s',t+1)P^0(s,s').
%\end{equation}
%When \eqref{fnp} holds true, we have $\subp^{**}(s,t,1)=0$, and this result is again stated in Lemma 3. 
Statement (2) can be proven using a similar argument. We therefore skip the proof to avoid redundancy. $\square$
\end{proof}
\section{}\label{ap:p1}
\begin{proof}{Proof of Lemma \ref{th:p1}}
To prove $(1)$, suppose, for the sake of contradiction, that $\subp^{**}(s,1,t) < 1$. By Lemma \ref{th:eq}, we have $\beta_t(s) \leq \lambda^*_t$. Therefore $U_t(s)\cup V_t(s)$ forms a superset to the set of states with indices of at least $\lambda_t^*$. We also know that $\subp^{**}$ takes active action with probability $\alpha$ at time $t$. Hence we can write $\alpha$ as the sum of the probabilities of taking the active action in all states $s'$ with $\subp^{**}(s',1,t)=1$ and the probabilities of taking the active action in all states $s'$ with $0<\subp^{**}(s',1,t)<1$:
\begin{align}
\alpha &= \sum_{s'\in\{s'':\subp^{**}(s'',1,t)=1\}}P_t(s') * 1 + \sum_{s'\in\{s'':0<\subp^{**}(s'',1,t)<1\}}P_t(s') * \subp^{**}(s',1,t) \nonumber\\
& < \sum_{s'\in\{s'':\subp^{**}(s'',1,t)=1\}}P_t(s') + \sum_{s'\in\{s'':0<\subp^{**}(s'',1,t)<1\}}P_t(s').\label{greaterp}
\end{align}
Taking the contrapositives of both statements in Lemma \ref{th:eq}, we get if $0<\subp^{**}(s',1,t)<1$ then $\beta_t(s')=\lambda^*_t$. Hence
\begin{align*}
\eqref{greaterp} = \sum_{s'\in\{s'':\beta_t(s'')\geq 1\}}P_t(s') \leq \sum_{s'\in U_t(s)\cup V_t(s)}P_t(s')\leq \alpha
\end{align*}
We get $\alpha < \alpha$, which is a contradiction, as desired. 

To prove $(2)$, we again use contradiction. Assume $\subp^{**}(s,1,t)>0$; by the contrapositive of the second statement of Lemma \ref{th:eq} we know $\beta_t(s)\geq \lambda^*_t$. Then $U_{t+1}(s)$ is a subset of $\{s':\beta_t(s')>\lambda^*_t\}$, which in turn is a subset of $\{s':\subp^{**}(s',1,t)=1\}$ by Lemma \ref{th:eq}. 
Hence by the fact that $\alpha= \sum_{s'\in\{s'':\subp^{**}(s'',1,t)=1\}}P_t(s') * 1 + \sum_{s'\in\{s'':0<\subp^{**}(s'',1,t)<1\}}P_t(s') * \subp^{**}(s',1,t)$, we must have either 1)
\[\alpha> \sum_{s'\in\{s'':\subp^{**}(s'',1,t)=1\}}P_t(s')\geq \sum_{s'\in U_{t}(s)}P_t(s')\]
when there exists some $s'\in\{s'':0<\subp^{**}(s'',1,t)<1\}$ such that $P_{t}(s')>0$, or 2)
\[\alpha\geq \sum_{s'\in\{s'':\subp^{**}(s'',1,t)=1\}}P_t(s')> \sum_{s'\in U_{t}(s)}P_t(s')\]
otherwise, as we must have that $\subp^{**}(s,1,t)=1$. 
In either case we get $\alpha>\sum_{s'\in U_{t}(s)=1}P_t(s')$, which again forms a contradiction.
$\square$
\end{proof}
\section{}\label{ap:bino}
\begin{lemma}\label{th:bino}
Let $X^{(k)}$ be a sequence of non-negative random variables such that $\lim_{k\rightarrow \infty}\frac{1}{k}X^{(k)}=\gamma$, a.s.. If $Y^{(k)}|X^{(k)}\sim Bin(X^{(k)},p)$, then $\lim_{k\rightarrow \infty}\frac{Y^{(k)}}{k} = \gamma p$, a.s..
\end{lemma}
\begin{proof}{Proof of Lemma \ref{th:bino}}
We consider two cases: 1) $X^{(k)}\rightarrow\infty$; 2) $X^{(K)}$ is bounded. When 1) $X^{(k)}\rightarrow\infty$, we have
\begin{align*}
&\lim_{k\rightarrow\infty}\frac{Y^{(k)}}{k}\\
=&\lim_{k\rightarrow\infty}\frac{Y^{(k)}}{X^{(k)}}\frac{X^{(k)}}{k}\\
=&\lim_{k\rightarrow\infty}\frac{Y^{(k)}}{X^{(k)}}\lim_{k\rightarrow\infty}\frac{X^{(k)}}{k}\\
	=&p*\gamma
\end{align*}
If 2) $X^{(K)}$ is bounded, then $\gamma = 0$. $Y^{(k)}$ is also bounded, so $\lim_{k\rightarrow \infty}=0$.
$\square$
\end{proof}
\section{}\label{ap:movein}
\begin{proof}{Proof of Lemma \ref{th:movein}}
For readability, we write out $f_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})$ as follows:
\begin{align}
f_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K}) =& \mathbbm{1}(\underbrace{[\frac{\lfloor\alpha K\rfloor}{K}-\sum_{s'\in U_{t+1}(s)}\frac{N_{t+1}(s')}{K}]^+ - \sum_{s'\in V_{t+1}(s)}\frac{N_{t+1}(s')}{K}}_{f_1(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})}\geq 0)
*\frac{N_{t+1}(s)}{K} \nonumber\\
&+\mathbbm{1}([\frac{\lfloor\alpha K\rfloor}{K}-\sum_{s'\in U_{t+1}(s)}\frac{N_{t+1}(s')}{K}]^+ -\sum_{s'\in V_{t+1}(s)}\frac{N_{t+1}(s')}{K}<0)*\nonumber\\
&\hspace{4mm}\underbrace{\mathbbm{1}(\frac{\lfloor\alpha K\rfloor}{K}-\sum_{s'\in U_{t+1}(s)}\frac{N_{t+1}(s')}{K}>0)b_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})}_{f_2(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})}.
\end{align}
The convergence is based on that $\frac{\Nv_{t+1}}{K} \rightarrow P_{t+1}(s)$, a.s., by the induction step, and $\frac{\lfloor\alpha K\rfloor}{K}\rightarrow \alpha$.
We discuss by three cases:
\begin{itemize}
\item When $f_1(\Pv_{t+1}(s),\alpha)>0$,\\
by continuity of $f_1$ and the fact that $\frac{\Nv_{t+1}}{K} \rightarrow P_{t+1}(s)$, a.s., for almost any realization $\omega$ of the sequence $\{\frac{\Nv_{t+1}}{K}\}_K$, we can find a constant $K_0(\omega)$ such that $\forall K \geq K_0(\omega)$, $|f_1(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K})-f_1(P_{t+1}(s),\alpha)|\leq \frac{f_1(P_{t+1}(s),\alpha)}{2}$.  
Hence $f_1(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K}) > 0$, and $f(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K}) = \frac{N_{t+1}(s)(\omega)}{K}$. As $K\rightarrow\infty$, $f_s(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K})$ goes to $P_{t+1}(s)$. Since this holds true for almost every $\omega$, we have $f_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})$ goes to $P_{t+1}(s) = f_s(\Pv_{t+1}(s),\alpha)$ almost surely. 
\item When $f_1(P_{t+1}(s),\alpha)<0$,\\
We first claim: for $K$ large enough, $H_1,H_2\in\{0,1\}$. To justify, first we look at the value of $b_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K},\frac{H_1}{K})$ when $\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)>0$.  
As $K$ tends to infinity, the first min function in $b_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})$, that is,  $$\frac{1}{K}\Big\lfloor (\lfloor\alpha K\rfloor-\sum_{s'\in U_{t+1}(s)}N_{t+1}(s'))\frac{\rho(s,1,t+1)}{\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)}\Big\rfloor,$$ 
goes to $$ (\alpha -\sum_{s'\in U_{t+1}(s)}P_{t+1}(s'))\frac{\rho(s,1,t+1)}{\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)},$$
 while the second term $\frac{N_{t+1}(s)}{K}$ goes to $P_{t+1}(s)$. 
 By assumption that $[\alpha -\sum_{s'\in U_{t+1}(s)}P_{t+1}(s')]^+ < \sum_{s'\in V_{t+1}(s)}P_{t+1}(s')$, we have
\begin{align*}
P_{t+1}(s) & \geq P_{t+1}(s)\pi^{**}(s,1,t+1)\\
&\geq (\alpha -\sum_{s'\in U_{t+1}(s)}P_{t+1}(s'))^+\frac{P_{t+1}(s)\pi^{**}(s,1,t+1)}{\sum_{s'\in V_{t+1}(s)}P_{t+1}(s')\pi^{**}(s',1,t+1)}\\
&=(\alpha -\sum_{s'\in U_{t+1}(s)}P_{t+1}(s'))^+\frac{\rho(s,1,t+1)}{\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)},
\end{align*}
for all $s\in V_{t+1}(s)$. Therefore when $K$ is sufficiently large, we get
$$\frac{1}{K}\Big\lfloor (\lfloor\alpha K\rfloor-\sum_{s'\in U_{t+1}(s)}N_{t+1}(s'))^+\frac{\rho(s,1,t+1)}{\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)}\Big\rfloor\leq\frac{N_{t+1}(s)}{K},
$$ 
for all $s\in V_{t+1}(s)$, and this satisfies the assumption in Remark \ref{remark:rounding}. Therefore we know that for sufficiently large $K$, $H_1$ takes value in $\{0,1\}$. 
When $\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)=0$, we always have $[\lfloor\alpha K\rfloor-\sum_{s'\in U_{t+1}(s)}N_{t+1}(s')]^+*\frac{N_{t+1}(s)}{\sum_{s'\in V_{t+1}(s)}N_{t+1}(s')} \leq N_{t+1}(s)$,for all $s\in V_{t+1}(s)$, which satisfies the assumption in Remark \ref{remark:rounding}, hence $H_2\in\{0,1\}$. 

The rest of the argument to prove that $f_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})$ goes to $ f_s(\Pv_{t+1}(s),\alpha)$ almost surely follows the same as the first case.
\item When $f_1(\Pv_{t+1},\alpha)=0$,\\
we claim that $P_{t+1}(s) = f_2(\Pv_{t+1},\alpha)$. To justify, we consider two cases:
\begin{enumerate}
\item $\alpha-\sum_{s'\in U_{t+1}(s)}P_{t+1}(s')\leq 0$.\\ Then we have $\sum_{s'\in V_{t+1}(s)}P_{t+1}(s') = 0$, which implies $P_{t+1}(s)=0$. 
$f_2(\Pv_{t+1},\alpha)$ is also zero since $\mathbbm{1}(\alpha -\sum_{s'\in V_{t+1}(s)}P_{t+1}(s')>0)=0$.
\item $\alpha-\sum_{s'\in U_{t+1}(s)}P_{t+1}(s')>0$.\\ When $\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)=0$, the value of the second term is $P_{t+1}(s)$, since $(\alpha -\sum_{s'\in U_{t+1}(s)}P_{t+1}(s'))^+$ cancels with the denominator $\sum_{s'\in V_{t+1}(s)}P_{t+1}(s')$ by assumption. When $\sum_{s'\in V_{t+1}(s)}\rho(s',1,t+1)>0$, since we have $\alpha-\sum_{s'\in U_{t+1}(s)}P_{t+1}(s') - \sum_{s'\in V_{t+1}(s)}P_{t+1}(s') = 0$, by Lemma \ref{th:p1}, we have $\subp^{**}(s',a,t)=1$. Hence the term associated with the second indicator is again $P_{t+1}(s)$, which is the same as the term associted with the first indicator. (Recall $\rho(s,1,t)=\subp^{**}(s,1,t)*P_{t+1}(s)$.) 
 \end{enumerate}
 Therefore we have shown our claim.
 
 For any realization $\omega$ of $\{\frac{\Nv_{t+1}}{K}\}_K$, we define sequences $sq_1(\omega) = \{\frac{\Nv_{t+1}(\omega)}{K}:K=1,2,..., f_1(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K})\geq 0\}$, and $sq_2(\omega) = \{\frac{\Nv_{t+1}(\omega)}{K}:K=1,2,..., f_1(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K})< 0\}$.
  Then we have $\{f_2(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K}):	\frac{\Nv_{t+1}(\omega)}{K}\in sq_2(\omega)\}$ and $sq_1(\omega)$ converge to the same value. Hence $f_s(\frac{\Nv_{t+1}(\omega)}{K},\frac{\lfloor\alpha K\rfloor}{K})$ goes to $P_{t+1}(s)$, and subsequently $f_s(\frac{\Nv_{t+1}}{K},\frac{\lfloor\alpha K\rfloor}{K})$ goes to $P_{t+1}(s) = f_s(\Pv_{t+1}(s),\alpha)$ almost surely.   $\square$
\end{itemize}
\end{proof}

\section{}\label{ap:equiv}
\begin{proof}{Proof of Lemma \ref{th:equiv}}
We divide our discussion into two main cases.
\begin{itemize}
\item 
When
\begin{equation}\label{eq:equiv_case1}
[\alpha-\sum_{s'\in U_t(s)}P_t(s')]^+ - \sum_{s'\in V_t(s)}P_t(s')\geq 0,
\end{equation} 
if $\alpha-\sum_{s'\in U_t(s)}P_t(s')\leq 0$, then $g_s(\Pv_t)=0$ in both cases. Moreover, \eqref{eq:equiv_case1} implies $P_t(s)=0$ by Lemma 4, hence $f_s(\Pv_t,\alpha)=0$.

If $\alpha-\sum_{s'\in U_t(s)}P_t(s')> 0$, \eqref{eq:equiv_case1} implies that $P_t(s)=1$ by Lemma 4.
$$\alpha-\sum_{s'\in U_t(s)}P_t(s')\geq \sum_{s'\in V_t(s)}P_t(s') = \sum_{s'\in V_t(s)}\rho(s',1,t) \geq P_t(s).$$
Hence $P_t(s)$ attains the minimum in both cases of $g_s(\Pv_t)$, and $f_s(\Pv_t,\alpha)=P_t(s) = g_s(\Pv_t)$.
\item When 
\begin{equation}\label{eq:equiv_case2}
[\alpha-\sum_{s'\in U_t(s)}P_t(s')]^+ - \sum_{s'\in V_t(s)}P_t(s') < 0,
\end{equation} 
if $\alpha-\sum_{s'\in U_t(s)}P_t(s')\leq 0$, then both $f_s(\Pv_t,\alpha)$ and $ g_s(\Pv_t)$ are zero.

If $\alpha-\sum_{s'\in U_t(s)}P_t(s')>0$, $f_s(\Pv_t,\alpha)$ is either $\min\{(\alpha-\sum_{s'\in U_t(s)}P_t(s'))\frac{\rho(s,1,t)}{\sum_{s'\in V_t(s)\rho(s',1,t)}},P_t(s)\}$
 or $\min\{(\alpha-\sum_{s'\in U_t(s)}P_t(s'))\frac{P_t(s)}{\sum_{s'\in V_t(s)P_t(s')}},P_t(s)\}$ 
 depending on whether $\sum_{s'\in V_t(s)}\rho(s',1,t)$ is greater than or equal to zero, which matches exactly the two cases in $g_s(\Pv_t)$.
\end{itemize}
\end{proof}
\section{}\label{ap:ppi}
\begin{proof}{Proof of Lemma \ref{th:ppi}}
We first consider the case when $\sum_{s'\in V_{t}(s)}\rho(s',1,t)>0$. This can be further divided into two sub-cases,
\begin{itemize}
\item Case 1: When $g_s(\Pv_t) = P_t(s)$,\\
if $\alpha-\sum_{s'\in U_t(s)}P_t(s')\leq 0$, by Lemma \ref{th:p1}, $\pi^{**}(s,1,t)=0$. Since $P_t(s)$ attains the minimum in this case, $P_t(s)=0$. Hence $g_s(\Pv_t)=0=P_t(s)\pi^{**}(s,1,t)$.

If $\alpha-\sum_{s'\in U_t(s)}P_t(s')> 0$, this leads to two cases by Lemma \ref{th:p1}: $\pi^{**}(s,1,t)=1$ or $0<\pi^{**}(s,1,t)<1$. If it is the latter, we have the second term in $g_s(\Pv_t)$ becomes $\rho(s,1,t)$, since $\alpha-\sum_{s'\in U_t(s)}P_t(s')$ cancels with the denominator. $\rho(s,1,t)=P_t(s)\pi^{**}(s,1,t)<P_t(s)$, which contradicts that $P_t(s)$ attains the minimum. Hence $\pi^{**}(s,1,t) = 1$. We have $g_s(\Pv_t) = P_t(s) = P_t(s)*\pi^{**}(s,1,t)$.
\item Case 2: When $g_s(\Pv_t) = [\alpha-\sum_{s'\in U_t(s)}P_t(s')]^+\frac{\rho(s,1,t)}{\sum_{s'\in V_t(s)\rho(s',1,t)}}$,\\
if $\alpha-\sum_{s'\in U_t(s)}P_t(s') \leq 0$, again by Lemma \ref{th:p1}, $\pi^{**}(s,1,t)=0$, $g_s(\Pv_t) = 0 = P_t(s)\pi^{**}(s,1,t)$.

If $\alpha-\sum_{s'\in U_t(s)}P_t(s') > 0$, again we have two cases: $\pi^{**}(s,1,t)=1$ or $0<\pi^{**}(s,1,t)<1$. If it is the latter, we have $\alpha-\sum_{s'\in U_t(s)}P_t(s')=\sum_{s'\in V_t(s)}P_t(s')$. 
%\hwccomment{will justify later.} 
If it is the former, by assumption $[\alpha-\sum_{s'\in U_t(s)}P_t(s')]\frac{P_t(s)}{\sum_{s'\in V_t(s)P_t(s')}}$ attains the minimum, $\alpha-\sum_{s'\in U_t(s)}P_t(s') \leq \sum_{s'\in V_t(s)}P_t(s')$. Hence $\alpha-\sum_{s'\in U_t(s)}P_t(s')$ can only be equal to $\sum_{s'\in V_t(s)}P_t(s')$. Subsequently for both cases  $\pi^{**}(s,1,t)=1$ and $0<\pi^{**}(s,1,t)<1$, we have $g_s(\Pv_t) = \rho(s,1,t)=P_t(s)\pi^{**}(s,1,t)$.
\end{itemize}

Now we look at the case when $\sum_{s'\in V_t(s)}\rho(s',1,t)=0$. We have either 1) $P_t(s')=0$ for all $s'\in V_t(s)$ 
%\hwccomment{don't know what to do here, $g_s(\Pv_t)$ is not defined...maybe need to add a third case for $g_s(\Pv_t)$, which is $g_s(\Pv_t)=0$ if $\sum_{s'\in V_t(s)}P_t(s')=0$.} 
2) $\pi^{**}(s',1,t)=0$ for all $s'\in V_t(s)$. 

When $\pi^{**}(s',1,t)=0$ for all $s'\in V_t(s)$, $\alpha-\sum_{s'\in U_t(s)}P_t(s')\leq 0$
%\hwccomment{need to justify later}, 
hence $g_s(\Pv_t) = 0 = P_t(s)\pi^{**}(s,1,t)$.  
\end{proof}