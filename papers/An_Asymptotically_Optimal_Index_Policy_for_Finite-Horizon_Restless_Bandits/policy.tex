\section{An Index Based Heuristic Policy}\label{sec:alg}
Our index based heuristic policy assigns an index to each sub-process, based upon its state and current time. At each time step, we set active the m sub-processes with the highest indices. Before carrying out the process of sequential decision-making, our index policy calls for pre-computation of 1) $\lambdav^*\in \arg\inf_{\lambdav}P(\lambdav)$, as defined in Section \ref{sec:up}; 2) a set of indices, $\betav$, that will later be used for decision-making at every time step; 3) an optimal policy $\subp^{**}$ for the sub-MDP problem in \eqref{dpx}. In the first part of this section we discuss how we carry out such computations. 
\subsection{Pre-computations}
\subsubsection{Dual optimal $\lambdav^*$}\label{subsec:pi}
We use subgradient descent to solve $\inf_{\lambdav} P(\lambdav)$, which converges to its solution $\lambdav^*$ by convexity of $\lambdav \mapsto P(\lambdav)$ (Theorem 7.4  in \citep{RusBook2006}).  By \eqref{dpx} and $\eqref{dec}$, a sub-gradient of $P(\lambdav)$ with respect to $\lambdav$ is given by $(-K\mathbb{E}^{\subp^{\lambdav}}[A_t]+m: 1\leq t\leq T)$, where $\subp^{\lambdav}$ is any policy in $\Pi^*(\lambdav)$.

To compute this sub-gradient, we compute a policy in $\Pi^*(\lambdav)$ and then use exact computation or simulation with a large number of replications to compute $\mathbb{E}^{\subp^{\lambdav}}[A_t]$.
To compute a policy in $\Pi^*(\lambdav)$, we first compute the value function $V^{\lambdav}:\substates\times\{1,...,T\}\mapsto \mathbb{R}$ of sub-MDP $Q(\lambdav)$.  We accomplish this using backward induction \citep{putermanBook}:
\begin{equation}
V^{\lambdav}(s,t)=
\begin{cases}
	\max_{\subaction\in \subactions}\{r_T(s,a)-a\lambda_T\}  & \text{if $t=T$,}\\
	\max_{\subaction\in\subactions}\{r_t(s,a)-a\lambda_t+\sum_{s'\in\substates}P^{a}(s,s')V^{\lambdav}(s',t+1)\}  & \text{otherwise.}
\end{cases} 
\end{equation}

Then, any and all policies $\pi^{\lambdav}$ in $\Pi^*(\lambdav)$ are constructed by determining for each $s$ and $t$ the action $a$ whose one-step lookahead value $r_t(s,a)-a\lambda_t+\sum_{s'\in\substates}P^{a}(s,s')V^{\lambdav}(s',t+1)$ is equal to $V^{\lambdav}(s,t)$, and then setting $\pi^{\lambdav}(s,a,t)=1$ for this $a$.
For those $s$ and $t$ for which both actions $a$ have one-step lookahead values equal to $V^{\lambdav}(s,t)$, one may set $\pi^{\lambdav}(s,a,t)=1$ for either such action.
Thus, the cardinality of $\Pi^*(\lambdav)$ is 2 raised to the power of the number of $s,t$ for which the one-step lookahead values for playing and not playing are tied.

When we construct a policy in $\Pi^*(\lambdav)$ for the purpose of computing a sub-gradient of $P(\lambdav)$, we choose to play in those $s,t$ with tied one-step lookahead values. 
While our subgradient descent algorithm would converge for other choices, making this choice better supports computation of indices in section~\ref{subsec:beta}, 

\subsubsection{Indices $\beta_t(s)$}
\label{subsec:beta}
Define vector $\mathbf{v}[a,t]$ to be $\mathbf{v}+(a-v_t)*\mathbf{e}_t$, that is, the vector $\mathbf{v}$ with the $t^{th}$ element replaced by $a\in\mathbb{R}$. 
We define the \textit{index} of state $\substate\in\substates$ at time $t$ as
\begin{equation}\label{df:index}
\beta_t(\substate) = \sup\{\beta: \exists \; \subp\in\subpset^*(\lambdav^*[\beta,t]) \;s.t.\; \subp(\substate,1,t)=1\}.
\end{equation} 
Instead of computing the entire set $\subpset^*(\lambdav^*[\beta,t])$, we only need to compute a policy in $\subpset^*(\lambdav^*[\beta,t])$ using the method discussed in section \ref{subsec:pi}, i.e., always choose the active action when there are ties. Intuitively, this index is the maximum price we are willing to pay to set a sub-process active in state $s$ at $t$.
By leveraging the monotonicity of optimal actions with respect to rewards, as shown in Lemma \ref{th:index} in Appendix \ref{ap:bisect}, we compute $\beta_t(s)$ via bisection search in interval $[0,U]$, where $U$ upper bounds the largest possible value of $\beta_t(s)$. For example, we can set $U$ as $T* \max_{s,a,t}\subr_t(s,a)$ when $\lambdav^*\geq 0$ (which we show in Appendix \ref{ap:upbd} that $\beta_t(s)$ cannot be greater than this value in this case). We pre-compute the set $\betav=\{\beta_t(\substate):\substate\in\substates,1\leq t\leq T\}$ before running the actual algorithm.

\subsubsection{Occupation measure $\rho$ and its corresponding optimal policy $\mathbf{\subp}^{\textbf{**}}$}
Our tie-breaking policy involves constructing an optimal Markov policy $\subp^{**}$ for the sub-MDP $Q(\lambdav^*)$ such that $\Eb^{\subp^{**}}[\subar_t]=\frac{m}{K}$, $\forall 1\leq t\leq T$. 
%We first observe:
%\begin{lemma}\label{th:exist}
%There exists an optimal Markov policy, $\subp^{**}$, for the sub-MDP, such that the probability of taking an active action is $\frac{m}{K}$ at every time step, that is, $\Eb^{\subp^{**}}[\subar_t]=\frac{m}{K}$, $\forall 1\leq t\leq T $.
%\end{lemma}
The existence of $\subp^{**}$ is shown in Appendix \ref{ap:exist}.
To compute $\subp^{**}$, we borrow the idea of occupation measure \citep{DynkinBook}. Define occupation measure, $\rho(s,a,t)$, induced by a policy $\subp$ to be the probability of being in state $s$ and taking action $a$ given time $t$ under $\subp$. Subsequently $\subp^{**}$ can be solved by the following linear program (LP):
\begin{equation}\label{eq:lp}
\begin{aligned}
& \underset{\{\rho(s,a,t):y\in\substates,a\in \subactions,t\in\{1,...,T\}\}}{\text{max}}
& & \sum_{t=1}^{T}\sum_{a\in\subactions}\sum_{s\in\substates}\rho(s,a,t)r'_t(s,a) \\
& \text{subject to}
& & \sum_{s\in\substates}\rho(s,1,t)=\frac{m}{K}, \; \forall t=1.\ldots,T\\
& & & \sum_{a\in \subactions}\rho(s,a,t) - \sum_{a\in\subactions}\sum_{s'\in\substates}\rho(s',a,t-1)\subpr^{a}(s',s)=0, \; \; \forall \substate\in\substates,2\leq t\leq T\\
& & & \sum_{a\in\subactions}\rho(s,a,1)=\ind{s=s_1} \;\;\forall s\in\substates\\
& & & \rho(s,a,t)\geq 0, \; \forall s\in\substates,a\in\subactions,t=1,\ldots,T,
\end{aligned}
\end{equation}
where $r'_t(s,a)=r_t(s,a)-\lambda^*_t\mathbbm{1}(a=1)$, $\forall s\in\substates, \subaction\in \subactions, 1\leq t\leq T$. The first constraint ensures that $\mathbb{E}^{\subp^{**}}[A_t]=\frac{m}{K}$. 
The second constraint ensures flow balance. 
The third constraint shows that we start at state $s_1$. 
The second and third constraint together imply $\sum_{a\in\subactions,s\in\substates} \rho(s,a,t)=1$, i.e., that $(\rho(s,a,t) : a\in\subactions,s\in\substates)$ is a probability distribution for each t.
The fourth and fifth constraints ensure that $\rho$ is a valid probability measure.

Let $\rho^{*}$ be an optimal solution to $\eqref{eq:lp}$, $\subp^{**}$ can then be constructed by
\begin{equation}\label{df:rho}
\subp^{**}(s,a,t)=
\begin{cases}
\frac{\rho^*(s,a,t)}{\sum_{a\in\subactions}\rho^*(s,a,t)},\; \; \text{if }\sum_{a\in\subactions}\rho^*(s,a,t)>0\\
\mathbbm{1}(a=1), \;\;\;\text{if }\sum_{a\in\subactions}\rho^*(s,a,t)=0 \text{ and }\beta_t(s) \geq \lambda^*_t\\
\mathbbm{1}(a=0), \;\;\;\text{if }\sum_{a\in\subactions}\rho^*(s,a,t)=0 \text{ and }\beta_t(s)< \lambda^*_t,
\end{cases}
\end{equation}for all $s\in \substates,\subaction\in\subactions,1\leq t\leq T.$


Here we also make an observation that $\lambdav^*\in P(\lambdav^*)$ can be computed by solving \eqref{eq:lp} with $r'$ replaced by $r$. 
%\subsubsection{$P_t(\substate)$}
%We define $P_t(\substate) =P^{\subp^{**}}[\substater_t=s]$ as the probability of landing in state $s$ under $\subp^{**}$. $P_t(\substate)$ can also be computed directly from the occupation measure $\rho^*$ of $\subp^{**}$ by leveraging the fact that $P_t(\substate)= \sum_{a\in\subactions}\rho(s,a,t)$. We pre-compute the set $P^{**}=\{P_t(s):\substate\in\substates,1\leq t\leq T\}$.
\subsection{Index policy}
Let $\{\beta_x:x\in\{1,...,K\}\}$ be the indices associated with the K sub-processes at time $t$. 
We define $\bar{\beta}_t$ to be the largest value $\beta$ in $\{\beta_x:x\in\{1,...,K\}\}$ such that at least $m$ sub-processes have indices of at least $\beta$. Our index policy then sets the sub-processes with indices strictly greater than $\bar{\beta}_t$ active, and those with indices strictly less than $\bar{\beta}_t$ inactive. 
When more than $m$ sub-processes have indices greater than or equal to $\bar{\beta}_t$, a tie-breaking a rule is needed.
Our tie-breaking rule allocates remaining resources (the remaining sub-processes to be set active) across tied states according to the probability distribution induced by $\subp^{**}$ over $\substates$ at time $t$. 
This tie-breaking ensures asymptotic optimality of the index policy as it enforces that the fraction of sub-processes in each state $s$ is equal to the distribution induced by $\subp^{**}$ in the limit. 
This idea shall become clear in Section \ref{sec:pf} where the proof of asymptotic optimality is presented.
 %Note we do not differentiate sub-processes that are in the same state.

To further illustrate how our tie-breaking rule works, let $I_t= \{s_x:1\leq x\leq K,\beta_t(s_x)=\bar{\beta}_t\}$ be the set of states occupied by the tied sub-processes, we allocate \begin{equation}\label{tb_ratio}
\frac{\rho(s,1,t)}{\sum_{s'\in I_t}\rho(s',1,t)}
\end{equation} fraction of the remaining resources to each tied states, when $\sum_{s'\in I_t}\rho(s',1,t)>0$, where $\rho$ is a solution to \eqref{eq:lp}. 
In cases when $\sum_{s'\in I_t}\rho(s',1,t)=0$, we do tie-breaking according to the number of sub-processes that are currently in each of the tied states. 

We then use the function Rounding(total, frac, avail) in Algorithm \ref{ag:tiebreak} to deal with the situations where the products between the desired fractions and remaining resources are not integers. Here $total$ represents the number of remaining resources, $frac$ is a vector of fractions to be allocated to each tied state, and $avail$ is a vector of the number of sub-processes in each tied state. The function also takes care of the corner cases in which the number of sub-processes in a tied state $s$ is less than the number of resources we would like to assign to $s$ according to the fraction in \eqref{tb_ratio}. We note the following property of this function Rounding, which we will rely on in our proof in Section 6.

\begin{remark}
\label{remark:rounding}
When total, avail, frac satisfy $\text{avail}_i\geq \text{total} * \text{frac}_i$, the output vector $b=\text{Rounding}(\text{total},\text{frac},\text{avail})$ satisfies $|\text{b}_i-\text{total} * \text{frac}_i|<1$ for all $i$.
\end{remark}

We formally present our index policy in Algorithm \ref{algIndex} and \ref{ag:tiebreak}. 
\begin{algorithm}
\caption{Index Policy $\hat{\allp}$}
\label{algIndex}
\begin{algorithmic}
\STATE Pre-compute: $\lambdav^*$; $\betav$; $\rho$. (Refer to earlier discussions for computation details) 
  \FOR{$t = 1,...,T$}
    \STATE Let $\beta_{t,[i]}$ be the $i^{th}$ largest element in the list $\beta_t(\allstater_{t,1}),...,\beta_t(\allstater_{t,K})$, so $\beta_{t,[1]}\geq\ldots\geq \beta_{t,[K]}$. 
    \STATE Let $\bar{\beta}_t = \beta_{t,[m]}$
    \STATE Let $I_t=\{\substate : \beta_t(s)=\bar{\beta}_t\ \text{and}\ s=\allstater_{t,x}\ \text{for some x}\}$
    \STATE Let $N_t(s) = |\{x:\allstater_{t,x}=s\}|$, for all $\substate$.
    \STATE For $\substate\in I_t$, let 
    $$
    q(s) = 
    \begin{cases}
	\frac{\rho(s,1,t)}{\sum_{s'\in I_t}\rho(s',1,t)}, \; \text{if } \sum_{s'\in I_t}\rho(s',1,t)>0\\
	\frac{N_t(s)}{\sum_{s'\in I_t} N_t(s')}, \; \text{otherwise}
    \end{cases}
    $$
    Let $b = \mathrm{Rounding}(m-\sum_{s':\beta_t(s')>\bar{\beta}_t}N_t(s'),
    (q(s):s\in I_t),(N_t(s):s\in I_t))$
    \FOR{all $\substate$}
    \STATE If $\beta_t(s)>\bar{\beta}_t$, set all $N_t(s)$ sub-processes in $s$ active.
    \STATE If $\beta_t(s)= \bar{\beta}_t$, set $b(s)$ sub-processes in $s$ active.
    \STATE If $\beta_t(s)< \bar{\beta}_t$, set 0 sub-processes in $s$ active.
    \ENDFOR
  \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Rounding(total, frac, avail)}
\label{ag:tiebreak}
\begin{algorithmic}
\STATE Inputs: total (a scalar), frac (a vector satisfying $\sum_i \text{frac}_i = 1$), avail (a vector of the same length as frac satisfying $\text{total} \le \sum_i \text{avail}_i$)
\STATE Output: $b$ (a vector of the same length as the inputs satisfying $\sum_i b_i = \text{total}$, $b_i \le \text{avail}_i$)
\STATE Let $n  =  \length(\text{frac})$
\STATE Let b$_i = \min\{\text{avail}_i, \lfloor \text{total}*\text{frac}_i\rfloor\}$, for $i = 1,...,n$.
\STATE Let $j = 1$
\WHILE{$\text{total} > \sum_{i=1}^{n}\text{b}_i$}
\STATE Let $\text{b}_{j} = \text{b}_{j} + \ind{\text{avail}_{j}> b_j}$
\STATE Let $j = (j \mod n) +1$ 
\ENDWHILE
\RETURN b
\end{algorithmic}
\end{algorithm}


\begin{remark} \cite{Hawkins2003} proposed a \textit{minimum-lambda} policy, which, when translated into our setting, finds the largest Lagrange multiplier $\lambdav$ of the form $\lambdav_0+r\lambdav_1$ for which an optimal solution of the relaxed problem is feasible for the original MDP. The policy then sets active those sub-processes which would be set active in the relaxed problem. However, Hawkins did not specify what $\lambdav_0$ and $\lambdav_1$ should be, thus limiting the policy's applicability to finite horizon settings. Our policy is similar to that of Hawkins in that 1) setting the sub-processes with the largest indices in our policy is equivalent to finding the largest $\lambdav$ that satisfies the constraints of the original MDP and setting the corresponding sub-processes active, and; 2) We also limit the values of Lagrange multiplier $\lambdav$ considered to a ray, as $\lambdav^*[\beta,t]$ can be written in the form of $\lambdav^* + r\ev_t$, for $r\in\Rb$. However, unlike Hawkins' policy, our policy defines the starting point and the direction of the ray, along with a tie-breaking policy that ensures asymptotic optimality. 
\end{remark}