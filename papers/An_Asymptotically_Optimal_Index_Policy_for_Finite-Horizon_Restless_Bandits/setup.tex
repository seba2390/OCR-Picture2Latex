\section{Problem Description and Notation}\label{prob_desc}
We consider an MDP \\ $(\allstates,\allactions,\allpr^{\cdot},\allr)$ which is created by a collection of $K$ sub-processes $(\substates,\subactions,\subpr^{\cdot},\subr)$. The sub-processes are independent of each other except that they are bound by the joint decisions on their actions at each time step. These sub-processes are also referred to as {\it arms} in the bandit literature and shall be indexed by $x\in\{1,...,K\}$. 
Following a standard construction for MDPs, 
both the larger joint MDP and the sub-processes will be constructed on the same measurable space $(\Omega,\Fcal)$.
Random variables on this measurable space will correspond to states, actions, rewards,
and each policy will induce a probability measure over this space.

We describe the MDP to consider formally as follows:

\begin{itemize}
\item The \textbf{time horizon} $T<\infty$.

\item The \textbf{state space} $\allstates$ is the cross product of $K$ sub-process state space $\substates$. $\substates$ is assumed to be finite. We use $\allstate=(s_1,...,s_K)$ to denote an element in $\allstates$ and $\allstater$ when the state is random. We also use $\allstater_t$ to emphasize that the state is at time $t$. 
Likewise, we use $s$ to denote an element in $\substates$ and $S$ or $S_t$ when it is random. 

\item The \textbf{action space} $\allactions$ is the cross product of $K$ sub-processes action space $\subactions$, which is set equal to $\{0,1\}$. We use $\subaction$ to denote a generic element of $\subactions$, and $\subar$ when it is random. We use $\allaction=(\subaction_1,\subaction_2,...,\subaction_K)$ to denote a generic element in $\allactions$ and $\allar$ to denote an action when it is random. In the context of bandit problems, $\subaction=1$ is called ``pulling'' an arm (sub-process).
\item The \textbf{reward function} $\allr_t:\allstates \times \allactions\mapsto \mathbb{R}$ for each $1\leq t\leq T$. $\allr_t(\allstate,\allaction) = \sum_{x=1}^K \subr_t(\substate_x,\subaction_x)$, where  $\subr_t(\substate_x,\subaction_x)$ is the reward obtained by a sub-process when action $\subaction_x$ is taken in state $\substate_x$ at time $t$. We assume rewards are non-negative and finite. 

\item The \textbf{transition kernel} $\allpr^{\allaction}(\allstate',\allstate) = \prod_{x=1}^{K}\subpr^{\subaction_x}(\substate'_x,\substate_x)$, where $\subpr^{\subaction}(\substate',\substate)$ is the probability of a sub-process transitioning from $\substate'$ to $\substate$ if action $\subaction$ is taken, i.e., $P(\substate|\substate',\subaction)$. The product implies that the $K$ sub-processes evolve independently. We also point out that RMAB differ from MAB problems in that MABs require $\subpr^0(\substate,\substate)=1$ while RMABs allows $\subpr^0(\substate,\substate)<1$. Since we are considering both cases, we do not restrict the value of $\subpr^0(\substate,\substate)$.
\end{itemize}


%We restrict our attention to Markov policies. . We follow convention by defining a policy $\allp$ as a function that determines the probability of taking action $\allaction$ in state $\allstate$, which we denote using $\allp(\allaction|\allstate)$. 
%Moreover, we use $\allaction^{\allp}(\allstate)$ to denote the action chosen in state $\allstate$ under policy $\allp$. 
%Let $\allpset$ denote the set of all feasible policies for the MDP of our focus, that is, %$\allpset=\{\allp:|\av^{\allp}(\allstate)|=m, \forall \allstate \in \Sb\}$. 
%$\allpset=\{\allp:\allp(\allaction|\allstate)=0, \forall \allstate \in \Sb \text{ if } |\allaction|\neq m\}$
%We define a policy $\allp$ for the sub-process $x$ similarly. Let $a^{\allp}(s_{t,x})$ denote the action taken in state $s_{t,x}$ under policy $\allp$, and let $\allpset$ denote the set of all policies for sub-process $x$.
 
%\hwccomment{To be introduced later: We call a policy a deterministic policy when it specifies which action to choose in state $\allstate_t$, i.e., $\av^{\allp}(\allstate_t)$ is a constant instead of a random variable. Let $\subpset_D$ denote the set of all deterministic policies for our MDP. We have $\subpset_D \subseteq \subpset$.}


Next we describe the set of policies for our MDP problem. Since the state and action space defined above are finite, it is sufficient to consider the set of Markov policies $\allpset$ \citep{putermanBook}. Define a policy $\allp\in\allpset$ as a function $\allstates\times \allactions \times \{1,...,T\} \rightarrow [0,1]$ that determines the probability of choosing action $\allaction$ in state $\allstate$ at time $t$, that is, $P(\allaction|\allstater_t=\allstate)$,  Subsequently we have $\sum_{\allaction\in\allactions}\allp(\allstate,\allaction,t)=1$, $\forall \allstate\in\allstates, \forall 1\leq t\leq T$. A policy $\allp$ and the transition kernel  $\allpr^{\cdot}(\cdot,\cdot)$ together defines a probability distribution $P^{\allp}$on the all possible paths of the process $\{\allstate_1\allaction_1...\allstate_T:\allstate_t\in \allstates, \allaction_t\in \allactions\}$. Starting at a fixed state $\allstate_1$, i.e., $P^{\allp}(\allstater_1=\allstate_1)=1$, we have the conditional distributions of $\allstater_t$ and $\allar_t$ defined recursively by $P^{\allp}(\allstater_{t+1}=\allstate'|\allstater_t=\allstate,\allar_t=\allaction)=\allpr^{\allaction}(\allstate,\allstate')$ and $P^{\allp}(\allar_t=\allaction|\allstater_t=\allstate)=\allp(\allstate,\allaction,t)$. 

The MDP we are considering allows exactly $m$ sub-processes to be set active at each time step. Hence a feasible policy, $\allp\in\allpset$, has to satisfy that $P^{\allp}(|\allar_t|=m)=1$, $\forall t\in\{1,...,T\}$. Here we use $|\cdot|$ as an operator that sums all the elements in a vector.  

The objective of our MDP is as follows:
\begin{equation}\label{prime}
\begin{aligned}
& \underset{\allp\in\allpset}{\text{maximize}}
& & \mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right] \\
& \text{subject to}
& & P^{\allp}(|\allar_t|=m)=1, \hspace{3mm}\forall 1\leq t\leq T .
\end{aligned}
\end{equation}
Since we will discuss other MDPs in the process of solving this one,  \eqref{prime} will be referred to as the \textit{original} MDP in the rest of the paper to avoid confusion.  
For convenience, we summarizes our notations in Appendix \ref{ap:notations}. We note the original MDP \eqref{prime} suffers from the 'curse of dimensionality', and hence solving it is computationally intractable. In the remaining of the paper we seek to building a computationally feasible index-based heuristics with performance guarantee.

%\begin{table}

%\end{table}

%\hwccomment{To revisit: Denote the optimal value of $\mathbf{Q}$ as $Opt(K,\alpha)$, where $\alpha = \frac{m}{K}$. Although $Opt(\cdot)$ is a function of far more many variables, here we only explicitly write out $K,\alpha$ as these two play a role in the asymptotic optimality}

\section{Lagrangian Relaxation and Upper Bounds}\label{sec:up}
In this section we discuss the Lagrangian relaxation of the original MDP and the corresponding single process problems. These single process problems together with the Lagrange multipliers form the building block of our index-based policy,  which will be formally introduced in Section \ref{sec:alg}. Lagrangian relaxation removes the binding constraints and allows the problem to be decomposed into tractable MDPs \citep{adel2008}. It works as follows:
for any $\lambdav = \{\lambda_1,...,\lambda_T\} \in \Rb^{T}$, we consider an unconstrained problem whose objective is obtained by augmenting the objective in \eqref{prime}:
 \begin{equation}\label{ub}
 P(\lambdav)=\max_{\allp\in \allpset}\mathbb{E}^{\allp}\left[\sum_{t=1}^{T}\allr_t\big(\allstater_t,\allar_t\big)\right]-\mathbb{E}^{\allp}\left[\sum_{t=1}^T\lambda_t(|\allar_t|-m) \right].
 \end{equation} 
 % The new action space becomes $\bar{A} = \{\av = (a_1,...,a_k):\subaction\in \subactions\}$ which means we are allowed to choose $a=1$ for any number of subproblems. 
  This unconstrained problem forms the \textit{Lagrangian relaxation} of \eqref{prime}, and has the following property:
 \begin{lemma}\label{th:up}
For any $\lambdav\in\Rb^{T}$, $P(\lambdav)$ is an upper bound to the optimal value of the original MDP.
 \end{lemma}
 \cite{adel2008} gave a proof to Lemma \ref{th:up} using the Bellman equations. We provide a more straightforward proof by viewing $P(\lambdav)$ as the Lagrange dual function of a relaxed problem of the original MDP; see Appendix \ref{ap:up}. 
 
This Lagrangian relaxation then decomposes into $K$ smaller MDPs, which we can easily solve to optimality. To elaborate on this idea of decomposition, we construct a \textit{sub-MDP} problem based on tuple $(\substates,\subactions,\subpr^{\cdot}(\cdot,\cdot),\subr(\cdot,\cdot))$. Again we consider only the set of Markov policies, $\subpset$, for this problem. Similarly a policy $\subp\in\subpset$ is a function that determines the probability of choosing action $\subaction$ in state $\substate$ at time $t$, i.e., $\subp:\substates\times \subactions \times \{1,...,T\} \rightarrow [0,1]$. 
The sub-MDP starts at a fixed state $s_1$. Subsequently we can define distributions of $\substater_t$ and $\subar_t$ under $P^{\subp}$ in a similar manner as we did for $\allstater_t$ and $\allar_t$ in the previous section. The objective of the sub-MDP is defined as follows:
\begin{equation}\label{dpx}
Q(\lambdav)=\max_{\subp\in \subpset}\mathbb{E}^{\subp}\left[\sum_{t=1}^{T}\subr_t(\substater_t,\subar_t)-\lambda_t \subar_t\right].
\end{equation}

We are now ready to present the decomposition of the Lagrangian relaxation.
\begin{lemma}\label{th:decom}
The optimal value of the relaxed problem satisfies 
\begin{equation}\label{dec}
\Pv(\lambdav)=K Q(\lambdav) + m\sum_t\lambda_t,
\end{equation}
\end{lemma}
\cite{adel2008} also gave a proof to Lemma 2, and we again provide a different proof in Appendix \ref{ap:decom}.
Since the state space of the sub-MDP is much smaller, we can solve it directly by using backward induction on optimality equations. 
The existence of such an optimal Markov deterministic policy is implied by that the state and action spaces of the sub-MDP being finite \citep{putermanBook}. Let $\subpset^*(\lambdav)$ be the set of optimal Markov deterministic policies of the sub-MDP for a given $\lambdav$. The relaxed problem can be solved by combining the solutions of individual sub-MDPs, that is, we can construct an optimal policy of the relaxed problem $\allp^{\lambdav}$ by setting $\allp^{\lambdav}(\allstate,\allaction,t) = \prod_{x=1}^K\pi^{\lambdav}(\substate_x,\subaction_x,t)$, where $\subp^{\lambdav}$ is an element in $\subpset^*(\lambdav)$. 
Moreover, $\mathbf{P}(\lambdav)$ is convex and piecewise linear in $\lambdav$ \citep{adel2008}. 
%A tightest upper bound of the kind is attained by $\inf_{\lambdav}\Pv(\lambdav)$. 
%Moreover, $\arg\inf_{\lambdav}\mathbf{P}(\lambdav)$ is non-empty in our problem setting; see Appendix \ref{ap:nonemp}.
%Let $\lambdav^* \in \arg\inf_{\lambdav}\mathbf{P}(\lambdav)$, and $\allp^{\lambdav^*}$ be a deterministic optimal policy that attains $\Pv(\lambdav^*)$. Let $\subp^{\lambdav^*}$ be the sub-MDP deterministic optimal policies from which $\allp^{\lambdav^*}$ is derived.
