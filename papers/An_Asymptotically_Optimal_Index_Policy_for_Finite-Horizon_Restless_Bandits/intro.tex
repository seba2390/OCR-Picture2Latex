\section{Introduction}\label{intro}
We consider the restless multiarmed bandit (RMAB) problem \citep{whittle1988} with a finite horizon and multiple pulls per period.  In the RMAB, we have a collection of ``arms'', each of which is endowed with a state that evolves independently.  If the arm is ``pulled'' or ``engaged' in a time period then it advances stochastically according to one transition kernel, and if not then it advances according to a different kernel.  Rewards are generated with each transition, and our goal is to maximize the expected total reward over a finite horizon, subject to a constraint on the number of arms pulled in each time period.

The RMAB generalizes the multi-armed bandit (MAB) \citep{robbins195x} by allowing arms that are not engaged to change state and multiple pulls per period. 
This extends the applicability of the MAB problem to a broader range of settings, including the submarine tracking problem, project assignment problem in \citep{whittle1988}, 
and contemporary applications including:
\begin{itemize}
\item Facebook displays ads in the \textit{suggested posts} section every time its users browse their personal pages. Among the ads that have been shown, some are known to attract more clicks than others. But there are also many ads which have yet to be shown and they may attract even more clicks. Given that the slots for display are limited, a policy is required to select ads to maximize total clicks.
\item In a multi-stage clinical trial, a medical group starts with a number of new treatments and an existing treatment with reliable performance. In each stage, a few treatments are selected from the pool to test, with the goal to identify the new treatments that perform better than the existing one with high confidence. A strategy is required to select which treatments to test at every stage to most effectively support their judgment at the end of the trial. 
\item A data analyst wishes to label a large number of images using crowdsourced effort from low-cost but potentially inaccurate workers. Each label given by the crowdworkers comes with a cost and the analyst has limited budget. Hence she needs to carefully assign tasks so as to maximize the likelihood of correct labeling.
\end{itemize}

The infinite horizon MAB with one pull per time period is famously known to have a tractable-to-compute optimal policy, called the Gittins index policy \citep{gittins1979}.  This policy is appealing because it can be computed by considering the state space for only a single arm, making it computationally tractable for problems with many arms.  This policy loses its optimality properties, however, when modifying the problem in any problem dimension: when allowing arms that are not engaged to change state; when moving to a finite horizon \citep{berry1985}; or when allowing multiple pulls per period.  Thus, the Gittins index does not apply to our problem setting.  Moreover, while optimal policies for RMABs with multiple pulls per period or finite horizons are characterized by the dynamic programming equations \citep{putermanBook}, the so-called `curse of dimensionality' \citep{PowellBook} prevents computing them because the dimension of the state space grows linearly with the number of arms.

Thus, while the RMAB is not known to have a computable optimal policy, \cite{whittle1988} proposed a heuristic called the Whittle index for the infinite-horizon RMAB with multiple pulls per period, which is well-defined when arms satisfy an indexability condition.  This policy is derived by considering a Lagrangian relaxtion of the RMAB in which the constraint on the number of arms pulled is replaced by a penalty paid for pulling an arm.  An arm's Whittle index is then the penalty that makes a rational player indifferent between pulling and not pulling that arm.  The Whittle index policy then pulls those arms with the highest Whittle indices.  Appealingly, the Whittle index and the Gittins index are identical when applied to the MAB problem with a single pull per period.  

\cite{whittle1988} further conjectured that if the number of arms and the number of pulls in each time period go to infinity at the same rate in an infinite-horizon RMAB, then the Whittle index policy is asymptotically optimal when arms are indexable. \cite{weber1990,weber1991} gave a proof to Whittle's conjecture with a difficult-to-verify condition: that the fluid approximation has a globally asymptotically stable equilibrium point.  This condition was shown to hold when each arm's state space has at most $3$ states, but this condition does not hold in general and \cite{weber1990} provides a counterexample with $4$ states.

Our contribution in this paper is to (1) create an index policy for finite horizon RMABs with multiple pulls per period, and (2) show that it is asymptotically optimal in the same limit considered by Whittle.  Like the Whittle index, our approach is computationally appealing because it requires considering the state space for only a single arm, and its computational complexity does not grow with the number of arms.  Unlike the Whitle index, our index policy does not require an indexability condition hold to be well-defined, and in contrast with \cite{weber1990,weber1991} our proof of asymptotic optimality holds regardless of the number of states.  We further demonstrate our index policy numerically on problems from the literature that can be formulated as finite-horizon RMABs, and show that it provides finite-sample performance that improves over the state-of-the-art. 

In addition to building on \cite{whittle1988,weber1990,weber1991}, our work builds on the literature in weakly coupled dynamic programs (WCDP), that itself builds on RMABs.
Indeed, at the end of his paper, Whittle pointed out that his relaxation technique can be applied to a more general class of problems in which sub-problems are linked by constraints on actions, but are otherwise independent. Hawkins in his thesis \citep{Hawkins2003} formally termed these problems (but with a more general type of constraints) as WCDPs and proposed a general decoupling technique. Moreover, he also proposed index-based policies for solving both finite and infinite horizon WCDPs and offered a proof that his policy, when applied to the infinite time horizon Multi-arm bandit problem (MAB), is equivalent to the Gittins index policy. Our work is similar to Hawkins' in that we consider Lagrange multipliers of the same functional form when computing indices. However, Hawkins does not specify what the coefficients of the function should be, or give a tie-breaking rule for the case when multiple arms have the same index. We obtain an asymptotically optimal policy by addressing both of these issues. The differences will be discussed with greater details after we formally introduce our index policy.

Another major work in WCDP is by \cite{adel2008} who shows that the ADP relaxation is tighter than the Lagrangian relaxation but is also computationally more expensive. It gives necessary and sufficient conditions for the Lagrangian relaxation to be tight and proves that the optimality gap is bounded by a constant when the Lagrange multipliers are allowed to be state dependent. The last result that the optimality gap is bounded by a constant implies that the per arm gap goes to zero as the number of arms grows. We achieve a similar result in our paper by showing the per arm reward of our index-based heuristic policy goes to the per arm reward of the Lagrangian bound, in spite of that our Lagrange multipliers are not state-dependent. We conjecture that this is due to the fact that our constraints, which is a function about on the action and not the state, is less general than the constraints considered in WCDP, which depends on both the action and the state. Moreover, the focuses of the two works differ: while our work focuses on offering an asymptotically optimal heuristic policy, \cite{adel2008} examines the ordering and tightness of different bounds. The heuristic proposed in \cite{adel2008} is based on ADP technique, is also different from our index-based policy.

Other work on WCDP also include \cite{Ye2014} who proposes a even tighter bound by incorporating information relaxation on the non-anticipative constraints in addition to the existing relaxation methods. \cite{Gocgun2011} considers two classes of large-scaled WCDPs in which the state and action space in each sub-problem also grows exponentially and uses an ADP technique to approximate the value functions of individual sub-MDPs in addition to employing Lagrangian relaxation for the overall problem. 

The remainder of this paper is outlined as follows: Section \ref{prob_desc} formulates the problem. Section \ref{sec:up} discusses the Lagrangian relaxation of the problem. Section \ref{sec:alg} states our index-based policy, and provide computation methods. Section \ref{sec:pf} gives a proof of asymptotic optimality. Section \ref{sec:num} numerically evaluates our index policy. Section 8 concludes the paper.


% PF: I deleted the following text
%
% We follow the relaxation technique in WDCP \citep{whittle1988,Hawkins2003,adel2008} to decouple the constraints by Lagrangian relaxation, and decompose the problem to smaller MDPs, which can be solved with much lower complexity. We also follow the methods in \cite{adel2008} to compute the Lagrange multipliers that are optimal for the relaxed problem. 
% We then propose a way to index the state of sub-problems by solving the smaller MDPs with the optimal Lagrange multipliers, and hence form an index-based heuristic that gives priority to sub-problems with greater indices at each time step while complying with the linking constraints. The computational complexity of our index-based policy no longer grows with the number of arms.
%
% We take a Bayes-optimal approach and view RMAB as a special case of the weakly coupled dynamic programs (WCDP) \citep{adel2008}. A WCDP is a a high-dimensional dynamic program consisting of multiple lower dimensional dynamic programs, sometimes referred to as sub-problems. These subproblems are linked by constraints on their actions, but are otherwise independent of each other. 
% Our RMAB setup is less general than WCDP in the sense that 1) it has equality constrains instead of inequality constraints; 2) the sub-problems are identical; 3) the constraints on only depend on the action taken in a state but independent of the state.
%
% Whittle's policy is equivalent to Gittin's index policy in the multi-armed bandit setting, that is, the states of arms remain unchanged when left inactive. 
% Unlike Whittle who has to assume indexability, we get that for free in finite-horizon settings. But we also face an added difficulty: the Lagrange multipliers in the relaxed problem are time-dependent in finite-horizon settings, and hence there are $T$ multipliers instead of one. This complicates the task of computing indices for each arm. Our policy will address this issue.
% Moreover, our index policy can be reduced to Whittle's in infinite horizon setting. 
