%\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\section{Introduction}
Of the many types of digital data that are getting recorded every second, %which is predicted to be more than 1.7 MB in 2020 by Domo~\cite{domo-6}, 
most can be ordered -- they belong to the ordinal type (e.g., age, citation count, {\it etc.}), and a good proportion can be represented as strings but cannot be ordered --- they belong to the nominal type (e.g., hair colour, country, publication venue, {\it etc.}). The latter datatype is also known as {\em categorical} which is our focus in this work. Categorical attributes are commonly present in survey responses, and have been used earlier to model problems in bio-informatics~\cite{DNA,HIV}, market-basket transactions~\cite{transaction,itemset_categorical,itemset_mining}, web-traffic~\cite{web_transaction}, images~\cite{image_categorical}, and recommendation systems~\cite{click_stream}. The first challenge practitioners encounter with such data is how to process them using standard tools most of which are designed for numeric data, that too often are real-valued.

Two important operations are often performed before running statistical data analysis tools and machine learning algorithms on such datasets. \bl{The first is encoding the data points using numbers, and the second is dimensionality reduction; many approaches combine the two, with the final objective being numeric vectors of fewer dimensions. To the best of our knowledge, the approaches usually followed are ad-hoc adaptations of those employed for vectors in the real space, and suffer from computational inefficiency and/or unproven heuristics~\cite{Hancock2020SurveyOC}. The motivation of this work is to provide a solution that is efficient in practice and has proven theoretical guarantees.}

For the first operation, we use the standard method of label encoding in this paper. In this a feature with $c$ categories is represented by an integer from $\{0,1,2,\ldots c\}$ where 0 indicates a missing \bl{category} and $i \in \{1,2,\ldots, c\}$ indicates the $i$-th \bl{category}. Hence, an $n$-dimensional data point, where each feature can take at most $c$ values, can be represented by a vector from $\{0,1,2\ldots c\}^n$ --- we call such a vector as a categorical vector. Another approach is one-hot encoding (OHE) which is more popular since it avoids the implicit ordering among the feature values imposed by label-encoding. One-hot encoding of a feature with $c$ possible values is a $c$-dimensional binary vector in which the $i$-th bit is set to 1 to represent the $i$-th feature value. Naturally, one-hot encoding of an $n$-dimensional vector will be $nc$ dimensional --- which can be very large if $c$ is large (e.g., for features representing countries, {\it etc.}). Not only label encoding avoids this problem, but is essential for the crucial second step -- that of dimensionality reduction.

Dimensionality reduction is important when data points lie in a high-dimensional space, e.g., when encoded using one-hot encoding or when described using tens of thousands of categorical attributes. High-dimensional data vectors not only increase storage and processing cost, but they suffer from the ``curse of dimensionality'' that points to the decrease in performance after the dimension of the data points crosses a peak. Hence it is suggested that the high-dimensional categorical vectors be compressed to smaller vectors, essentially retaining the information only from the useful features. Baraniuk et al.~\cite{baraniuk2010low} characterised a good dimensionality reduction {\em in the Euclidean space} as a compression algorithm that satisfies the following two conditions for any two vectors $x$ and $y$.

\begin{enumerate}
    \item Information preserving: For any two distinct vectors $x$ and $y$, $R(x) \not= R(y)$.
    \item $\epsilon$-Stability: (Euclidean) distances between all the points are approximately preserved (with $\epsilon$ inaccuracy).
\end{enumerate}

We call these two conditions the {\em ``well-designed''} conditions. To obtain their mathematically precise versions, we need to narrow down upon a distance measure for categorical vectors. A natural measure for categorical vectors is an extension of the binary Hamming distance. For two $n$-dimensional categorical data points $x$ and $y$, the Hamming distance between them is defined as the number of features with different attributes in $x$ and $y$, i.e., 
\begin{align*}
HD(x, y) &=\Sigma_{i=1}^n \dist(x[i], y[i]) \mbox{, where }\\
\dist(x[i], y[i])&=\begin{cases}
    1, & \text{if~} x[i]\neq y[i], \\
    0, & \text{otherwise}.
  \end{cases}
\end{align*}

{\bf Problem statement:} The specific problem that we address is how to design a dimensionality reduction algorithm that can compress high-dimensional sparse label-encoded categorical vectors to low-dimensional categorical vectors so that (a) compressions of distinct vectors are distinct, and (b) the Hamming distance between two uncompressed vectors can be efficiently approximated from their compressed forms. These conditions, in turn, guarantee both information-preserving and stability. Furthermore, we would like to take advantage of the sparse nature of many real-world datasets. \bl{The} most important requirement is the compressed vectors should be categorical as well, specifically not over real numbers and preferably not binary; this is to allow the statistical tests and machine learning tools for categorical datasets, e.g. k-mode, to run on the compressed datasets.

\subsection{\bl{Challenges in the existing approaches}} Dimensionality reduction is a well-studied problem~\cite{plos_categorical_tips} (also see Table~\ref{table:dim-red} in Appendix) but Hamming space does not allow the usual approaches applicable in the Euclidean spaces. Methods that work for continuous-valued data or even ordinal data (such as integers) do not perform satisfactorily for unordered categorical data. Among those that specifically consume categorical data, techniques \textit{via} feature selection \bl{have} been well studied. For example, in the case of  labelled data $\chi^2$~\cite{chi_square} and {Mutual Information}~\cite{MI} based methods select features based on their correlation with the label. This limits their applicability to only the classification tasks. Further,  {Kendall rank correlation coefficient}~\cite{kendall1938measure} ``learns'' the important features based on the correlation among them. Learning approaches tend to be computationally heavy and do not work reliably with small training samples. So what about task-agnostic approaches that do not involve learning? PCA-based methods, e.g., MCA is popular among the practitioners of biology~\cite{plos_categorical_tips}; however, we consider them merely a better-than-nothing approach since PCA is fundamentally designed for continuous data.

\begin{figure}[!ht]
    %\centering
    \includegraphics[width=0.6\linewidth]{images/OHE-counterexample.eps}
    \caption{An example showing that the Hamming distances of one-hot encoded sparse vectors are not functionally related to the distances between their unencoded forms. \bl{If a feature, say country, is missing, libraries differ in their handling of its one-hot encoding. In this paper, we follow the common practice of using the $c$-dimensional all-zero vector as its encoding. This retains sparsity since the number of non-missing attributes in the original vector equals the number of non-zero bits in the encoded vector.} \label{fig:ohe-counterexample}}
\end{figure}

A quick search among internet forums, tutorials and Q\&A websites revealed that the more favourable approach to perform machine learning tasks on categorical datasets is to convert categorical feature vectors to binary vectors using one-hot encoding~\cite[see DictVectorizer]{scikit-learn} --- a widely-viewed tutorial on Kaggle calls it ``The Standard Approach for Categorical Data''~\cite{kaggle_one_hot}. \bl{The biggest problem with OHE is that it is impractical for large $n$ or large $c$ followed by a technical annoyance that some OHE implementations do not preserve the Hamming distances for sparse vectors} (see illustration in Figure~\ref{fig:ohe-counterexample}). Hence, this encoding is used in conjunction with problem-specific feature selection or followed by dimensionality reduction from binary to binary vectors~\cite{ICDM,oddsketch,JS_BCS}. The latter is a viable heuristic that we wanted to improve upon by allowing non-binary compressed vectors (see Appendix~\ref{appendix:OHE+BS} for a quick analysis of OHE followed by a state-of-the-art binary compression).

Another popular alternative, especially when $n\times c$ is large, is {\em feature hashing}~\cite{WeinbergerDLSA09} that is now part of most libraries, e.g., {\tt scikit-learn}~\cite[see FeatureHasher]{scikit-learn}. Feature hashing and other forms of hash-based approaches, also known as sketching algorithms, both encode and compress categorical feature vectors into integer vectors (sometimes signed) of a lower dimension, and furthermore, provide theoretical guarantees like stability, in some metric space. The currently known results for feature hashing apply only to the Euclidean space, however, Euclidean distance and Hamming distance are not monotonic for categorical vectors. It is neither known nor straightforward to ascertain whether feature hashing and its derivatives can be extended to the Hamming space which lacks the continuity that is crucial to their theoretical bounds. Other hash-based approaches either come with no guarantees and are used merely because of their compressibility or come with stability-like guarantees in a different space, e.g., cosine similarity by Simhash~\cite{simhash}. Our solution is a hashing approach that we prove to be stable in the Hamming space.

% We explained above that categorical data given to us is assumed to be label-encoded form in which the integers $1\dots c$ are used to denote the values of a feature, $c$ being the largest number of values any feature can habve. Additionally we use the value 0 to represent a missing feature. So if the number of possible categories is denoted $c$, then an $n$-dimensional feature vector is represented by discrete vectors from $\{0,1,2,\ldots, c\}^n$.

\subsection{Overview of results} 
The commonly followed practices in dealing with categorical vectors, especially those with high dimensions and not involving supervised learning or training data, appear to be either feature hashing or one-hot encoding followed by dimensionality reduction of binary vectors~\cite[Chapter 5]{zheng2018feature}. We provide a contender to these in the form of the {\em \fsketch sketching algorithm} to construct lower-dimensional categorical vectors from high-dimensional ones.

The lower-dimensional vectors, {\it sketches}, produced by \fsketch (we shall call these vectors as \fsketch too) have the desired theoretical guarantees and perform well on real-world datasets vis-\`a-vis related algorithms. Now we summarise the important features of \fsketch; in the summarisation, $p$ is a constant that is typically chosen to be a prime number between 5-50.

{\bf Lightweight and unsupervised:} First and foremost, \fsketch is an unsupervised process, and in fact, quite lightweight making a single pass over an input vector and taking $O(poly(\log p))$ steps per non-missing feature. The \fsketch-es retain the sparsity of the input vectors and their size and dimension do not depend at all on $c$. To make our sketches applicable out-of-the-box for modern applications where data keeps changing, we present an extremely lightweight algorithm to incorporate any change in a feature vector into its sketch in $O(poly(\log p))$-steps per modified feature. It should be noted that \fsketch supports change of an attribute, deletion of an attribute and insertion of a previously missing attribute unlike some state-of-the-art sketches; for example,  BinSketch~\cite{ICDM} does not support deletion of an attribute.

{\bf Estimator for Hamming distance:} We want to advocate the use of \fsketch-es for data analytic tasks like clustering, {\it etc.} that use Hamming distance for the (dis)similarity metric. We present an {\em estimator that can approximate the Hamming distance} between two points by making a single pass over their sketches. The estimator follows a tight concentration bound and 
has the ability to estimate the Hamming distance from very low-dimensional sketches. In the theoretical bounds, the dimensions could go as low as $4\sigma$ or even $\sqrt{\sigma}$ (and independent of the dimension of the data) where $\sigma$ indicates the sparsity (maximum number of non-zero attributes) of the input vectors; however, we later show that a much smaller dimension suffices in practice. Our sketch generation and the Hamming distance estimation algorithms combined meet the two conditions of ``well-designed'' dimensionality reduction.

\begin{thm} \label{thm:main-intro} Let $x$ and $y$ be distinct categorical vectors, and $\phi(x)$ and $\phi(y)$ be their $d$-dimensional compressions.
\begin{enumerate}
    \item $\phi(x)$ and $\phi(y)$ are distinct with probability $\approx HD(x,y)/d$.
    \item Let $HD'(x,y)$ denote the approximation to the Hamming distance between $x$ and $y$ computed from $\phi(x)$ and $\phi(y)$. If $d$ is set to $4\sigma$, then with probability at least $1-\delta$ (for any $\delta$ of choice), $$\big|HD(x,y) - HD'(x,y)\big| = { O \left( \sqrt{\sigma \ln \tfrac{2}{\delta}}\right)}.$$
\end{enumerate}
\end{thm}

The proof of (1) follows from Lemma~\ref{lem:expectation-previous} and the proof of (2) follows from Lemma~\ref{lem:hconcentrationlemma} for which we used McDiarmid's inequality. The theorem allows us to use compressed forms of the vectors in place of their original forms for data analytic and statistical tools that depend largely on their pairwise Hamming distances.

%To further improve the accuracy of estimation, we present \minfsketch that combines multiple \fsketch vectors, and also give an algorithm that estimates Hamming distance from two \minfsketch sketches.



{\bf Practical performance:} All of the above claims are proved rigorously but one may wonder how do they perform in practice.
%, especially, in comparison to existing algorithms and on real-world large datasets. 
For this, we design an elaborate array of experiments on real-life datasets involving many common approaches for categorical vectors. The experiments demonstrate these facts.
\begin{itemize}%[leftmargin=*,noitemsep,nolistsep]
	%%\vspace*{-.8em}
    \item Some of the baselines do not output categorical vectors (see Section~\ref{sec:experiments}).   %Appendix~\ref{sec:Reproducibility}). 
    Our \fsketch algorithm is super-fast  among those that do and offer comparable accuracy.
    \item When used for typical data analytic tasks like clustering, similarity search, etc. low-dimension \fsketch-es bring immense speedup {\it vis-a-vis} using the original (uncompressed) vectors, yet achieving very high accuracy. The NYTimes dataset saw 140x speedup upon compression to $0.1\%$.
    \item Even though highly compressed, the results of clustering, {\it etc.}\ on \fsketch-es are close to what could be obtained from the uncompressed vectors and are comparable with the best alternatives. For example, we were able to compress the Brain cell dataset of dimensionality $1306127$ to $1000$ dimensions in a few seconds, yet retaining the ability to correctly approximating the pairwise Hamming distances from the compressed vectors. This is despite many other baselines giving either an out-of-memory error, not stopping even after running for a sufficiently long time, or producing significantly worse estimates of pairwise Hamming distances.
    \item The parameter $p$ can be used to fine-tune the quality of results and the storage of the sketches.
\end{itemize}

We claim that \fsketch is the best method today to compress categorical datasets for data analytic tasks that require pairwise Hamming distances with respect to both theoretical guarantee and practical performance.
%This work, along with a few dimensionality reduction techniques which have been shown to be effective for feature extraction~\cite{WeinbergerDLSA09,ICDM,JS_BCS,deepwalk}, is an attempt to draw attention towards randomization for data analysis.


\subsection{Organisation of the paper}
The rest of the paper is organised as follows. We discuss several related works in Section~\ref{sec:relwork}.
In Section~\ref{sec:analysis}, we present our algorithm \fsketch and derive its theoretical bounds. In Section~\ref{sec:experiments}, we empirically compare the performance of \fsketch on several end tasks with state-of-the-art algorithms. We conclude our presentation in Section~\ref{sec:conclusion}.
%and state some potential open questions of the work.% Finally in Appendix~\ref{sec:Reproducibility_details}, we discuss the reproducibility details of the baseline algorithms.
%,  and the end tasks evaluation. 
The proofs of the theoretical claims and the results of additional experiments are included in Appendix.

\section{Related work} \label{sec:relwork}
{\bf Dimensionality reduction:} 
Dimensionality reduction has been studied in-depth for real-valued vectors, and to some extent, also for discrete vectors. We categorise them into these broad categories --- (a) random projection, (b) spectral projection, (c) locality sensitive hashing (LSH), (d) other hashing approaches, and (e) learning-based algorithms. All of them compress high-dimensional input vectors to low-dimensional ones that explicitly or implicitly preserve some measure of similarity between the input vectors.

The seminal result by Johnson and Lindenstrauss~\cite{JL83} is probably the most well known random projection-based algorithm for dimensionality reduction. This algorithm compresses real-valued vectors to low-dimensional real-valued vectors such that the Euclidean distances between the pairs of vectors are approximately  preserved, but in such a manner that the compressed dimension does not depend upon the original dimension. The algorithm involves projecting a data matrix onto a random matrix whose each entry is sampled from a Gaussian distribution. This result has seen lots of enhancements, particularly with respect to generating the random matrix without affecting the accuracy~\cite{Achlioptas03}, \cite{LiHC06}, \cite{KaneN14}. However, it is not clear whether any of those ideas can be made to work for categorical data and that too, for approximating Hamming distances.

% There are several follow-ups works in the direction such as ~\cite{Achlioptas03}, \cite{LiHC06}, \cite{KaneN14} that suggests generating the random matrix which lesser computational complexity and simultaneously offering almost the same guarantee. This line of work is known to be applicable for real-valued vectors and approximates the Euclidean distance/Inner product. It is not clear whether such ideas can be extended for Categorical data for approximating Hamming distance. 

Principal component analysis (PCA) is a spectral projection-based technique for reducing the dimensionality of high dimensional datasets by creating new uncorrelated variables that successively maximise variance. There are extensions of PCA that employ kernel methods that try to capture non‐linear relationships~\cite{ScholkopfSM97}. {Multiple Correspondence Analysis (MCA)}~\cite{MCA} does the analogous job for the categorical datasets. However, these methods perform dimensionality reduction by creating un-correlated features in a low-dimensional space whereas our aim is to preserve the pairwise Hamming distances in a low-dimensional space.

Another line of dimensionality reduction techniques builds upon the ``Locality Sensitive Hashing (LSH)'' algorithms. LSH algorithms have been proposed for different data types and similarity measures, e.g., real-valued vectors and the Euclidean distance~\cite{IM98}, real-valued vectors and the cosine similarity~\cite{simhash}, binary vectors and the Jaccard similarity~\cite{BroderCFM98}, binary vectors and the Hamming distance~\cite{GIM99}. However, generally speaking, the objective of an LSH is to group items so that similar items are grouped together and dissimilar items are not; unlike \fsketch they do not provide explicit estimators of any similarity metric.

%  There are few other learning-based dimensionality reduction algorithms available such as {Latent Semantic Analysis (LSA)}\cite{LSI}, {Latent Dirichlet Allocation (LDA)}\cite{LDA}, {Non-negative Matrix Factorization (NNMF)}\cite{NNMF} all of which strive to learn a low-dimensional representation of a dataset while preserving some inherent properties of the full-dimensional dataset. Our findings suggest that, under certain conditions of sparsity, a random hashing can be more efficient and more accurate compared to a learning-based approach.

There are quite a few learning-based dimensionality reduction algorithms available such as {Latent Semantic Analysis (LSA)}\cite{LSI}, {Latent Dirichlet Allocation (LDA)}\cite{LDA}, {Non-negative Matrix Factorisation (NNMF)}\cite{NNMF}, \bl{{Generalized feature embedding learning (GEL)}~\cite{golinko2019generalized}} all of which strive to learn a low-dimensional representation of a dataset while preserving some inherent properties of the full-dimensional dataset. \bl{They are rather slow due to the optimization step involved during learning.} T-distributed Stochastic Neighbour Embedding \texttt{(t-SNE)}~\cite{vanDerMaaten2008} is a faster non-linear dimensionality reduction technique that is widely used for the visualisation of high-dimensional datasets. However, the low-dimensional representation obtained from \texttt{t-SNE} is not recommended for use for other end tasks such as clustering, classification, anomaly detection as it does not necessarily preserve densities or pairwise distances. An autoencoder~\cite{Goodfellow-et-al-2016} is another learning-based non-linear dimension reduction algorithm. It basically consists of two parts: An \textit{encoder} which aims to learn a low-dimensional representation of the input and a \textit{decoder} which tries to reconstruct the original input from the output of the encoder. However, these approaches involve optimising a learning objective function and are usually slow and CPU-intensive.
% Once the error between the original input and the output of the decoder is minimized, the output of the encoder is considered as a low-dimensional representation of the input. Our findings suggest that, under certain conditions of sparsity, random hashing can be more efficient and more accurate compared to a learning-based approach. 

The other hashing approaches randomly assign each feature (dimension) to one of several bins, and then compute a summary value for each bin by aggregating all the feature values assigned to it. A list of such summaries can be viewed as a low-dimensional sketch of the input. Such techniques have been designed for real-valued vectors approximating inner product (e.g., feature hashing~\cite{WeinbergerDLSA09}), binary vectors allowing estimation of several similarity measures such as Hamming distance, Inner product, Cosine, and Jaccard similarity (e.g., BinSketch~\cite{ICDM}), etc. This work is similar to these approaches but for categorical vectors and only aiming to estimate the Hamming distances.

Another approach in this direction could be to encode categorical vectors to binary and then apply dimensionality reduction for binary vectors; unfortunately, the popular encodings, e.g. OHE, do not preserve Hamming distance for vectors with missing features. Nevertheless, it is possible to encode using OHE and then reduce its dimension. However, our theoretical analysis led to a worse accuracy compared to that of \fsketch (see Appendix~\ref{appendix:OHE+BS} for the analysis) and this approach turned out to be one of the worst performers in our experiments (see Section~\ref{sec:experiments}).

\bl{While our motivation was to design an end-task agnostic dimensionality reduction algorithm, there exist several that are designed for specific tasks, e.g., for clustering~\cite{8809826}, for regression and discriminant analysis of labelled data~\cite{JMLR:v21:17-788}, and for estimating covariance matrix~\cite{DBLP:journals/tnn/ChenYZLK20}. Deep learning has gained mainstream importance and several researchers have proposed a dimensionality reduction “layer” inside a neural network~\cite{8974600}; this layer is intricately interwoven with the other layers and cannot be separated out as a standalone technique that outputs compressed vectors.}

\bl{Feature selection is a limited form of dimensionality reduction whose task is to identify a set of good features, and maybe learn their relative importance too. Banerjee and Pal~\cite{7155531} recently proposed an unsupervised technique that identifies redundant features and selects those with bounded correlation, but only for real-valued vectors. For our experiments we chose the Kendall-Tau rank correlation approach that is applicable to discrete-valued vectors.}

%\cite{WeinbergerDLSA09} suggests dimensionality reduction algorithm for real-valued data while approximating Inner product.~\cite{ICDM} suggests a dimensionality reduction algorithm for sparse-binary data such that the low-dimensional sketch closely estimates several similarity measures such as Hamming distance, Inner product, Cosine, and Jaccard similarity corresponding to the full-dimensional data. Our work in the spirit is similar to the feature hashing approach and aims for estimating the Hamming distance between the Categorical data points from their low-dimensional sketch. 

%Computing the Hamming norm between two data streams has been well studied in the streaming algorithm framework. Let $\mathbf{a}=\langle a_1, a_2, \ldots, a_d \rangle$, and $\mathbf{b}=\langle b_1, b_2, \ldots, b_d \rangle$, be two data stream, where $a_i, b_i \in \R$. Then hamming norm between $\mathbf{a}$ and $\mathbf{b}$ is defined as $|\mathbf{a}-\mathbf{b}|_H= |\{i:a_i \neq b_i  \}|$. \cite{CormodeDIM03} suggest computing sketch of size $O(1/\epsilon^2) \cdot \log (1/\delta)$ that approximate the Hamming distance between  $\mathbf{a}$ and $\mathbf{b}$ within a factor of $(1\pm \epsilon)$ 
%of the true answer with probability $1-\delta$. A major difference between their result and ours is that our sketches are integer-valued which seeks lesser memory requirement as compared to  \cite{CormodeDIM03} which outputs real-valued sketches. Moreover, in our method by setting $p=2$, we can obtain binary sketch as well, which is not only space-efficient but also by exploiting fast bitwise operation enables faster training and inference of several end tasks. A downside of our algorithm is that it works for space dataset only, whereas \cite{CormodeDIM03}  doesn’t have any such requirement on the datasets. \cite{KaneNW10}, \cite{WoodruffY19} improves the results of \cite{CormodeDIM03} however they are not known to be practical algorithms for the purpose. 

\textbf{Sketching algorithm:} The use of ``sketches'' for computing Hamming distance has been explicitly studied in the streaming algorithm framework. The first well-known solution was proposed by Cormode et al.~\cite{CormodeDIM03} where they showed how to estimate a Hamming distance with high accuracy and low error. There have been several improvements to this result, in particular, by Kane et al.~\cite{KaneNW10} where a sketch with the optimal size was proposed. However, we neither found any implementation nor an empirical evaluation of those approaches (the algorithms themselves appear fairly involved). Further, their objective was to minimise the space usage in the asymptotic sense in a streaming setting, whereas, our objective is to design a solution that can be readily used for data analysis. This motivated us to compress categorical vectors onto low-dimensional categorical vectors, unlike the real-valued vectors that the theoretical results proposed. 
% Moreover, by setting $p=2$, we can obtain binary sketches as well, which enables fast bitwise operations resulting in faster training and inferencing of end tasks.
A downside of our solution is that it heavily relies on the sparsity of a dataset unlike the sketches output by the streaming algorithms.% do not require any such assumption.

%The following  dimensionality reduction algorithms, in principle, can be used for categorical data such as {Feature Hashing}~\cite{WeinbergerDLSA09}, {SimHash}\cite{simhash}, {Latent Semantic Analysis (LSA)}\cite{LSI}, {Latent Dirichlet Allocation (LDA)}\cite{LDA}, {Multiple Correspondence Analysis (MCA)}\cite{MCA}, {Non-negative Matrix Factorization (NNMF)}\cite{NNMF}, and { PCA}. However, as mentioned earlier they are not known to approximate Hamming distance in their low-dimensional representation. We shall see later that it is possible to exceed their performance with respect to Hamming distance and simultaneously obtain excellent speedups.





%For example, the celebrated Johnson Lindenstrauss Lemma compresses $n$-dimensional real-valued vectors to $d$ dimensional real-valued vectors such that the Euclidean distances between the pairs of vectors is approximately preserved, but in such a manner that $d$ does not depend upon original dimension $n$. We are aware of several fast dimensionality reduction algorithms which can, in principle, be used for categorical data, such as {Feature Hashing}~\cite{WeinbergerDLSA09}, {SimHash}\cite{simhash}, {Latent Semantic Analysis (LSA)}\cite{LSI}, {Latent Dirichlet Allocation (LDA)}\cite{LDA}, {Multiple Correspondence Analysis (MCA)}\cite{MCA}, {Non-negative Matrix Factorization (NNMF)}\cite{NNMF}, and { PCA}. However, they are not known to approximate Hamming distance in their low-dimensional representation, and in fact, we shall see later that our proposed technique can exceed their performance with respect to Hamming distance and simultaneously obtain excellent speedups.


%Let $\mathbf{a}=\langle a_1, a_2, \ldots, a_d \rangle$, and $\mathbf{b}=\langle b_1, b_2, \ldots, b_d \rangle$, be two data stream, where $a_i, b_i \in \R$. Then hamming norm between $\mathbf{a}$ and $\mathbf{b}$ is defined as $|\mathbf{a}-\mathbf{b}|_H= |\{i:a_i \neq b_i  \}|$. \cite{CormodeDIM03} suggest computing sketch of size
%$O(1/\epsilon^2) \cdot \log (1/\delta)$ that approximate the Hamming distance between  $\mathbf{a}$ and $\mathbf{b}$ within a factor of $(1\pm \epsilon)$ 
%of the true answer with probability $1-\delta$. A major difference between their result and ours is that our sketches are integer valued which seeks lesser memory requirement as compared to  \cite{CormodeDIM03} which outputs real-valued sketches. Moreover, in our method by setting $p=2$, we can obtain binary sketch as well, which is not only space efficient but also by exploiting fast bitwise operation enables faster training and inference of several end tasks. A downside of our algorithm is that it works for space dataset only, whereas \cite{CormodeDIM03}  doesn’t have any such requirement on the datasets. \cite{KaneNW10}, \cite{WoodruffY19} improves the results of \cite{CormodeDIM03} however they are not known to be the practical algorithms for the purpose. 


%\noindent \textbf{Motivation for discrete sketches:} \textcolor{red}{Our proposal takes high-dimensional categorical vectors as input and outputs low-dimensional categorical vectors which closely approximate the corresponding original Hamming distance. The user needs to provide a parameter $p$ -- a prime number, and each feature of the sketch takes a value from $\{0, 1, \ldots, p-1\}$, with binary sketches as a special case when $p=2$.  Note that at the same reduced dimension, the sketch corresponding to a higher value of $p$ offers better accuracy but at the cost of more space. This is because  the space for storing a feature requires $O(\log p)$ bits. }
%
%\textcolor{red}{
%Our motivation for discrete sketches is that they enable  running  the same algorithm on the sketches which was not possible on the original datasets due to their high-dimensionality. Moreover, as the sketches closely maintain the pairwise Hamming distance the results on the sketch closely estimate the corresponding result on the original datasets. We validate this empirically, and summarize our results in Section~\ref{sec:experiments}. 
%}
%
%\todo{Write learning based methods in a separate para..elaborate more with recent reference..autoencoded t-SNE etc.}
%
%\todo{A separate para to motivate integer valued sketching method proposed in the paper}

