
\section{Analysis of one-hot encoding + binary compression}\label{appendix:OHE+BS}

Let $x$ and $y$ be two $n$-dimensional categorical vectors with sparsity at most $\sigma$; $c$ will denote the maximum number of values any attribute can take. Let $x'$ and $y'$ be the one-hot encodings of $x$ and $y$, respectively. Further, let $x''$ and $y''$ denote the compression of $x'$ and $y'$, respectively, using BinSketch~\cite{ICDM} which is the state-of-the-art dimensionality reduction for binary vector using Hamming distance.

Observe that the sparsity of $x'$ is same as that of $x$ and a similar claim holds for $y'$ and $y$. However, $HD(x',y')$ does not hold a monotonic relationship with $HD(x,y)$. %Suppose $n_1$ denotes the number of features of $x$ that are missing in $y$, $n_2$ denotes the number of features of $y$ that are missing in $x$, $n_3$ denotes the number of features that are present, but different, in $x$ and $y$, and $n_4$ denotes $n-n_1-n_2-n_3$.
It is easy to show that $HD(x,y) \le HD(x',y') \le 2HD(x,y)$. Therefore, 
\begin{equation}
    |HD(x,y) - HD(x',y')| \le HD(x,y) \le 2\sigma. \label{eqn:2}
\end{equation}
    
We need the following lemma that was used to analyse BinSketch~\cite[Lemma~12,Appendix~A]{ICDM}.
\begin{lem}
Suppose we compress two $n'$-dimensional binary vectors $x'$ and $y'$ with sparsity at most $\sigma$ to $g$-dimensional binary sketches, denotes $x''$ and $y''$ respectively, by following an algorithm proposed in the BinSketch work. If $g$ is set to $\sigma\sqrt{\tfrac{\sigma}{2}\ln \frac{6}{\delta}}$ for any $\delta \in (0,1)$, then the following holds with probability at least $1-\delta$.
$$|HD(x',y') - HD(x'',y'')| \le 6 \sqrt{\tfrac{\sigma}{2} \ln \tfrac{6}{\delta}}.$$
\end{lem}

Combining the above inequality with that in Equation~\ref{eqn:2} gives us
$$|HD(x,y)-HD(x'',y'')| \le 2\sigma + 6\sqrt{\tfrac{\sigma}{2} \ln \tfrac{6}{\delta}} \le 2\sigma\sqrt{\ln \tfrac{2}{\delta}}$$
if we set the reduced dimension to $\sigma\sqrt{\tfrac{\sigma}{2}\ln \frac{6}{\delta}}$.

This bound is worse compared to that of \fsketch where we able to prove an accuracy of $\Theta(\sqrt{\sigma\ln \tfrac{2}{\delta}})$ using reduced dimension value of $4\sigma$ (see Lemma~\ref{lem:hconcentrationlemma}).
