\section{\minfsketch: Combining multiple \fsketch}\label{subsec:minfsketch}
%We observed that our \fsketch-based estimator always overestimates the actual Hamming distance.

We proved in Lemma~\ref{lem:hconcentrationlemma} that our estimate $\hat{h}$ is within an additive error of $h$. 
A standard approach to improve the accuracy in such situations is to obtain several independent estimates and then compute a suitable statistic of the estimates. We were faced with a choice of mean, median and minimum of the estimates of which we decided to choose median after extensive empirical evaluation (see Section~{\ref{subsec:box_plot_median}}) and obtaining theoretical justification (explained in Section~\ref{appendix:subsec:minfsketch}). We first explain our algorithms in the next subsection.
%\begin{proof}
%Define the function $g(f) = \ln(1-\tfrac{f}{dP})/\ln D$; observe that
%$\hat{h}=g(f)$ and $h=g(f^*)$. $g(f)$ is convex within the domain $f\in [0,dP)$.
%Even though $f$ is allowed to range from 0 to $d$, for the sake of simplicity
%    assume that $f$ is less than $dP$ (this assumption is justified since
%    $f^*=dP - dPD^h \ll dP$~\ref{lem:expectation} and $f \le f^* +
%    \epsilon^2 d^2$~\ref{cor:eps-d}). Since $g(f)$ is convex in
%this domain, so Jensen's inequality can be applied to get the next observation
%that shows that $\hat{h}$ is a biased estimator of $h$. So, $\E[\hat{h}] = \E[g(f)] \ge g(\E[f]) = g(f^*) = h$.
%\end{proof}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/catsketch.pdf}
    \caption{\minfsketch for categorical data --- sketch of each data point is a 2-dimensional array whose each row is an \fsketch. The $i$-th rows corresponding to all the data points use the same values of $\rho,R$.}% The prime number $p$ can be kept same throughout.}
    \label{fig:catsketch-schema}
    %\vspace*{-2mm}
\end{figure}

%We have also given a bound above on the difference between $\hat{h}$ and $h$.
%To reduce this noise, we create \minfsketch which is simply a collection of multiple independent instances of \fsketch.

\subsection{Algorithms for generating a sketch and estimaing Hamming distance}
Let $k,d$ be some suitably chosen integer parameters. An arity-$k$ dimension-$d$ \minfsketch for a categorical data, say $x$, is an array of $k$ sketches: $\Phi(x) = \langle \phi^1(x), \phi^2(x), \ldots \phi^k(x) \rangle$; the $i$-th entry of $\Phi(x)$ is a $d$-dimensional \fsketch. See Figure~\ref{fig:catsketch-schema} for an illustration. Note that the internal parameters $\rho,R,p$ required to run \fsketch to obtain the $i$-entry are same across all data points; the parameters corresponding to different $i$ are, however, chosen independently ($p$ can be the same).

Our algorithm for Hamming distance estimation is inspired from the Count-Median sketch~\cite{CORMODE200558} and Count sketch~\cite{CHARIKAR20043}. It estimates the Hamming distances between the pairs of ``rows'' from $\Phi(x)$ and $\Phi(y)$ and returns the median of the estimated distances. This procedure is followed in Algorithm~\ref{algo:fsketchestimate}.%; it is possible that for two \fsketch vectors, their observed Hamming distance $f \ge dP$ and in that case, the algorithm computes the corresponding $\hat{h}$ as as the worst possible value of $2\sigma$.


\newcommand{\hxy}{\widehat{h}}
%\newcommand{\hxy}{\widehat{h_{x,y}}}

    \begin{algorithm}
	\noindent\hspace*{\algorithmicindent} \textbf{Input:} $\Phi(x)=\langle \phi^1(x), \phi^2(x), \ldots \phi^k(x) \rangle$, $\Phi(y)=\langle \phi^1(y), \phi^2(y), \ldots \phi^k(y) \rangle$
	\begin{algorithmic}[1]
	    \For{$i=1 \ldots k$}
            \State Compute $f=$ Hamming distance between $\phi^i(x)$ and $\phi^i(y)$
            \State If $f < dP$, $\hxy^i=\ln\left(1-\frac{f}{dP}\right)/\ln D$
            \State Else $\hxy^i=2\sigma$
        \EndFor
        \State \Return $\hat{h} = \min\{ \hxy^1, \hxy^2, \ldots \hxy^k\}$
	\end{algorithmic}
	\caption{Estimate Hamming distance between $x$ and $y$ from their \minfsketch\label{algo:fsketchestimate}}
    \end{algorithm}
    

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{images/Median.pdf}
\caption{{Box plot for the median, mean, and minimum of the \texttt{FSketch}’s estimate obtain \textit{via} it’s from its $10$ repetitions, then each experiment is repeated $10$ times for computing   the variance of these statistics.  The black dotted line corresponds to the actual Hamming distance. }}
\label{fig:box_plot_median_mean_min}
\end{figure*}
    
    
\subsection{Theoretical justification}\label{appendix:subsec:minfsketch}

%\overestimator*
%
%\begin{proof}
%Define the function $g(f) = \ln(1-\tfrac{f}{dP})/\ln D$; observe that
%$\hat{h}=g(f)$ and $h=g(f^*)$. $g(f)$ is convex within the domain $f\in [0,dP)$.
%Even though $f$ is allowed to range from 0 to $d$, for the sake of simplicity
%    assume that $f$ is less than $dP$ (this assumption is justified since
%    $f^*=dP - dPD^h \ll dP$~\ref{lem:expectation} and $f \le f^* +
%    \epsilon^2 d^2$~\ref{cor:eps-d}). Since $g(f)$ is convex in
%this domain, so Jensen's inequality can be applied to get the next observation
%that shows that $\hat{h}$ is a biased estimator of $h$. So, $\E[\hat{h}] = \E[g(f)] \ge g(\E[f]) = g(f^*) = h$.
%\end{proof}

We now give a proof that our \minfsketch estimator offers a better
approximation. Recall that $\sigma$ indicates the maximum number of non-zero
attributes in any data vector, and is often much small compared to the their
dimension, $n$. Surprisingly, our results are independent of $n$.
\begin{lem}\label{lem:medianlemma}
    Let $h^m$ denote the median of the estimates of Hamming distances obtained
    from $t$ independent
    \fsketch vectors of dimension $4\sigma$ and let $h$ denote the actual Hamming distance. Then,
    $$\Pr\big[|h^m - h| \ge 18\sqrt{\sigma}\big] \le \delta$$
    for any desired $\delta \in (0,1)$ if we use $t \ge 48\ln \tfrac{1}{\delta}$.
\end{lem}

\begin{proof}
    We start by using Lemma~\ref{lem:hconcentrationlemma} with
    $p=3$ and error ($\delta$ in the lemma statement) = $\tfrac{1}{4}$. Let
    $\hat{h}^i$ denote the $k$-th estimate. From the lemma we get that
    $$\Pr\big[ |\hat{h}^i - h| \ge 18\sqrt{\sigma} \big] \le \tfrac{1}{4}$$

    Define indicator random variables $W_1 \ldots W_t$ as $W_i=1$ iff
    $|\hat{h}^i - h| \ge 18\sqrt{\sigma}$. We immediately have $\Pr[W_i] \le
    \tfrac{1}{4}$. 
    Notice that $W_i=1$ can also be interpreted to indicate the event
    $h-18\sqrt{\sigma} \le \hat{h}^i \le h + 18 \sqrt{\sigma}$.
    Now, $h^m$ is the median of $\{\hat{h}^1, \hat{h}^2, \ldots
    \hat{h}^t\}$, and so, $h^m$ falls outside the range $[h-18\sqrt{\sigma},
    h+18\sqrt{\sigma}]$ only if more than half of the estimates fall outside this
    range., i.e., if $\sum_{i=1}^t W_i > t/2$. Since $\E[\sum_i W_i] \le t/4$, the probability of this event is
    easily bounded by $\exp{(-(\tfrac{1}{2}^2
    \tfrac{t}{4}/3))} = e^{-t/48} \le \delta$ using Chernoff's bound.
\end{proof}


\subsection{Choice of statistics in \minfsketch}\label{subsec:box_plot_median}
We conducted an experiment to decide whether to take median, mean or minimum of $k$ \fsketch estimates in the \minfsketch algorithm. We randomly sampled a pair of points and estimated the Hamming distance from its low-dimensional representation obtained from  \texttt{FSketch}. We repeated this $10$ times over different random mappings and computed the median, mean, and minimum of those $10$ different estimates. We further repeat this experiment $10$ times and generate a box-plot of the readings which is presented in Figure~\ref{fig:box_plot_median_mean_min}. We observe that median has the lowest variance and also closely estimates the actual Hamming distance between the pair of points. 




\section{Dimensionality reduction algorithms}
\begin{table*}[h]
    \caption{A tabular summary of popular dimensionality reduction algorithms. Linear dimensionality reduction algorithms are those whose features in reduced dimension are linear combinations of the input features, and the others are known as non-linear algorithms. Supervised dimensionality reduction methods are those that require labelled datasets for dimensionality reduction. \label{table:dim-red}
}
  \begin{center}
        %\begin{tabular}{ |c | c | c |c | c | c | c|}
        \begin{tabular} { | p {0.55cm} | p {2 cm} | p {4 cm} | p {2 cm} | p {2.5 cm} | p {2 cm} | p {2 cm} |}
        
        \hline
        S. No.
&Data type of input vectors &Objective/ Properties &Data type of sketch vectors &Result &Supervised or Unsupervised&Type of dimensionality reduction\\
\hline
1 &Real-valued vectors &Approximating pairwise euclidean distance, inner product
&Real-valued vectors&JL-lemma~\cite{JL83}&Unsupervised&Linear \\
\hline
2&Real-valued vectors&
Approximating pairwise euclidean distance, inner product
&Real-valued vectors&Feature Hashing~\cite{WeinbergerDLSA09}&Unsupervised&Linear \\
\hline
3&Real-valued vectors&Approximating pairwise cosine or angular similarity &Binary vectors&SimHash~\cite{simhash}&Unsupervised&Non-Linear\\
\hline
4&Real-valued vectors&Approximating pairwise $\ell_p$ norm for $p \in (0, 2]$&Real-valued vectors&$p$-stable random projection (SSD)~\cite{Indyk06}
&Unsupervised &Linear\\
\hline
5&Sets&Approximating pairwise Jaccard similarity &Integer valued vectors
&MinHash~\cite{BroderCFM98}&Unsupervised&Non-linear\\
\hline
6&Sparse binary vectors
&Approximating pairwise Hamming distance, Inner product, Jaccard and Cosine similarity 
&Binary vectors&BinSketch~\cite{ICDM}&Unsupervised&Non-linear\\
\hline
7&Real-valued vectors&Minimize  the variance in low dimension 
&Real-valued vectors&Principal Component Analysis (PCA)&Unsupervised
&Linear\\
\hline
8&Real-valued vectors (labelled input)&Maximizes class separability in the reduced dimensional space&Real-valued vectors&Linear Discriminant Analysis~\cite{FLDA} &Supervised&Linear\\
\hline
9&Real-valued vectors&
Embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions&
Real-valued vectors &$t$-SNE~\cite{vanDerMaaten2008}
&Unsupervised&Non-linear\\
\hline
10&Real-valued vectors &Minimize the reconstruction error 
&Real-valued vectors&Auto-encoder~\cite{10.5555/2987189.2987190}
&Unsupervised&Non-linear\\
\hline
11&Real-valued vectors
&Extracting nonlinear structures in low-dimension via Kernel function
&Real-valued vectors&Kernel-PCA~\cite{NIPS1998_226d1f15}&Unsupervised&Non-linear\\
\hline
 12 &Real-valued vectors &
Factorize input matrix into two small size non-negative matrices &Real-valued vectors &Non-negative matrix factorization (NNMF)~\cite{NNMF}
 &Unsupervised &Linear\\
\hline
13 &Real-valued
vectors &Compute a \textit{quasi-isometric}  low-dimensional embedding &Real-valued
vectors&Isomap~\cite{tenenbaum_global_2000}
&Unsupervised&Non-linear\\
\hline
14&
Real-valued
vectors
&Preserves the \textit{topological structure} of the data
&Real-valued
vectors
&Self-organizing map
~\cite{kohonen-self-organized-formation-1982}
&Unsupervised&Non-linear\\
\hline
\end{tabular}
    \end{center}
    \end{table*}
