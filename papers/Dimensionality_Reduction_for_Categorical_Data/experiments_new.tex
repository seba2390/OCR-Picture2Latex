\section{Experiments}\label{sec:experiments}

We performed our experiments on a machine having {Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz, 94 GB RAM, and running a Ubuntu 64-bits OS.}
%\textcolor{red}{@Deb can we cook up some story on the dataset we used for experiments.. something like existing categorical datasets for experimentation as they are low-dimensional.. also BrainCell I was not able to find what to mention..}

%\subsection{Evaluation criteria}

We first study the effect of the internal parameters of our proposed solution on its performance. We start with the effect of the prime number $p$; then we compare \fsketch with the appropriate baselines for several unsupervised data-analytic tasks (see Table~\ref{tab:baseline_characteristic}) and objectively establish these advantages of \fsketch over the others.

% \minfsketch requires three parameters --- arity $k$, dimension $d$, and a prime number $p$. We first empirically evaluate the effect of $p$ and $k$ on its performance. Then we study the effect of $d$ on the behavior of arity-1 \minfsketch ({\it i.e.}, \fsketch) when used for data analytic tasks like $\RMSE$, clustering, similarity search and classification. For all these tasks, we objectively establish these advantages of \fsketch over the others.
\begin{enumerate}[(a)]
    \item Significant speed-up in the dimensionality reduction time,
    \item considerable savings in the time for the end-tasks (e.g., clustering) which now runs on the low-dimensional sketches,%perform these tasks on our sketches with much smaller dimensions.
    \item but with comparable accuracy of the end-tasks (e.g., clustering).
\end{enumerate}

Several baselines threw {\it out-of-memory} errors or did not stop on certain datasets. We discuss the errors separately in Section~\ref{sec:appendix_oom_error} in Appendix.

\subsection{Dataset description}
The efficacy of our solution is best described for high-dimensional datasets. Publicly available categorical datasets being mostly low-dimensional, we treated several integer-valued freely available real-world datasets as categorical. Our empirical evaluation was done on the following seven such datasets with dimensions between $5000$ and $1.3$ million, and sparsity from $0.07\%$ to 30\%.%   We mention their description as follows: 
\begin{itemize}
    \item \texttt{Gisette Data Set}~\cite{UCI,Gisette}: %This dataset consists of handwritten digits, and have been used for classification problem where the aim is to separate the highly confusible digits '4' and '9'.
	This dataset consists of integer feature vectors corresponding to images of handwritten digits and was constructed from the MNIST data. Each image, of $28 \times 28$ pixels, has been pre-processed (to retain the pixels necessary to disambiguate the digit $4$ from $9$) and then projected onto a higher-dimensional feature space represented to construct a 5000-dimension integer vector. %The digits have been size-normalized and centered in a fixed-size image of dimension 28x28. From the images pixels were sampled at random so that it contains the information necessary to disambiguate the digit $4$ from $9$. Further higher order features were created to project the problem into a higher dimensional feature space.  
   
    % \textcolor{red}{This dataset consits of bag of word respresentation of tagged web pages and categries are the frequency of highly frequent word.}
    \item \texttt{BoW (Bag-of-words)}~\cite{UCI,DeliciousMIL}: We consider the following five corpus -- NIPS full papers, KOS blog entries, Enron Emails, NYTimes news articles, and tagged web pages from the social bookmarking site \texttt{delicious.com.} These datasets are ``BoW"(Bag-of-words) representations of the corresponding text corpora. In all these datasets, the attribute takes integer values which we consider as categories.
    %\textcolor{red}{For Bag-of-words datasets the categories are the frequency of highly frequent word in the dataset.}
     %\item \texttt{DeliciousMIL}~\cite{UCI,DeliciousMIL}: This dataset consists of a subset of tagged web pages from the social bookmarking site \texttt{delicious.com.} This dataset consists of a bag of word representation of tagged web pages and we consider categories as the frequency of the corresponding words.
    \item \texttt{1.3 Million Brain Cell Dataset}~\cite{genomics20171}: This dataset contains the result of a single cell RNA-sequencing \texttt{(scRNA-seq)} of $1.3$ million cells captured and sequenced from an \texttt{E18.5} mouse brain~\footnote{\url{https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons}}. Each gene represents a data point and for every gene, the dataset stores the read-count of that gene corresponding to each cell -- these read-counts form our features.
\end{itemize}
%We chose the last dataset due to its very high dimension and the earlier ones due to their popularity in dimensionality-reduction experiments. We observed in our initial experiments that many baselines suffer from \textit{``out-of-memory” (OOM)} and \textit{``did not stop” (DNS)} even after running them for sufficiently long time. Hence we decided to conduct our experiments on a random sample of $2000$ points from each dataset.
%We summarise the   dimensionality, the number of categories, and the sparsity of these datasets in Table~\ref{tab:datasets}.

We chose the last dataset due to its very high dimension and the earlier ones due to their popularity in dimensionality-reduction experiments. \bl{We consider all the data points for KOS, Enron, Gisette, DeliciousMIL, a $10,000$ sized sample for NYTimes, and a $2000$ sized samples for BrainCell}. We summarise the dimensionality, the number of categories, and the sparsity of these datasets in the Table~\ref{tab:datasets}.



%{
% Note that if we include all points in of our datasets, then many baselines give \textit{``out-of-memory” (OOM)} and \textit{``did not stop” (DNS)} even after running them for sufficiently long time. Therefore, we decided to include a random sample of $2000$ points in our experiments, to get the  experimental results for most of them. 
%  We consider a uniform sample of size $2000$ for each of the experiment. 
%{\color{red} Why 2000 data points -- several baselines gave DNS, OOM errors on entire dataset so we use 2000 samples. Explain the datasets -- what are categories in them?}
\begin{table}
%\footnotesize
\centering
  \caption{ Datasets}
  %\vspace{-2mm}
  \label{tab:datasets}
%   \addtolength\tabcolsep{-4pt}
  \resizebox{0.48\textwidth}{!}{%
  \noindent\begin{tabular}{lcccc}
  % \toprule
  \hline
     Datasets & Categories & Dimension & Sparsity & \bl{No. of points} \\
  %  \midrule
  \hline
    \textrm{Gisette}~\cite{Gisette,UCI} & $999$ & $5000$ & $1480$&$\bl{13500}$\\
    \textrm{Enron Emails}~\cite{UCI} & $150$ & $28102$ & $2021$&$\bl{39861}$\\
    \textrm{DeliciousMIL}~\cite{DeliciousMIL,UCI} & $58$ & $8519$ & $200$&$\bl{12234}$\\
    \raggedright\textrm{NYTimes articles}~\cite{UCI} & $114$ & $102660$ & $ 871$&$\bl{10000}$\\
    \bl{\textrm{NIPS full papers}~\cite{UCI} }& \bl{$132$} & \bl{$12419$} & \bl{$914$} & \bl{$1500$}\\
    \raggedright\textrm {KOS blog entries}~\cite{UCI} & $42$ & $6906$ & $457$&$\bl{3430}$\\
    \textrm {Million Brain Cells from E18 Mice}~\cite{genomics20171} & $2036$ & $1306127$ & $1051$&$\bl{2000}$\\
  % PubMed abstracts:  & $10000$ & $141043$ & $ $\\
  \hline
%  \bottomrule
\end{tabular}%
}
%   \addtolength\tabcolsep{4pt}
\end{table}
 
%We performed our experiments on a 10-core 32 GB Windows 10 server.
%\noindent\textbf{Datasets and Baseline methods:}
%\footnote{We defer our discussion on the reproducibility details of the experiments presented in this section to appendix (Section~\ref{sec:Reproducibility}).
%We chose six datasets with dimensions between 5000 to $1.3$ million, $0.08\%$to 30\% sparsity and 42 to 2036 categories and sampled the initial $2000$  data points for our experiments.


\begin{table}[t]
\centering
  \caption{ 13 baselines}
  %\vspace{-2mm}
  \label{tab:baselines_methods}
\begin{tabular}[t]{rrl}
\hline
%    & { 13 baseline algorithms}\\
% \hline
1.&  {\rm SSD} &\textrm{Sketching via Stable Distribution}~\cite{SSD}\\
2.&    {\rm OHE} & \textrm{One Hot Encoding+BinSketch~\cite{ICDM}}\\
3.&    {\rm FH} & \textrm{Feature Hashing}~\cite{WeinbergerDLSA09}\\
4.&    {\rm SH} & \textrm{Signed-random projection/SimHash}~\cite{simhash}\\
5.&      {\rm KT} & \textrm{Kendall rank correlation coefficient}~\cite{kendall1938measure}\\
6.&    {\rm LSA} & \textrm{Latent Semantic Analysis}~\cite{LSI}\\
7.&    {\rm LDA} & \textrm{Latent Dirichlet Allocation}~\cite{LDA}\\
8.&    {\rm MCA} & \textrm{Multiple Correspondence Analysis}~\cite{MCA}\\
9.&    {\rm NNMF} & \textrm{Non-neg. Matrix Factorization}~\cite{NNMF}\\
10.&    {\rm PCA} & Vanilla \textrm{Principal component analysis}\\
11.&    \bl{ {\rm VAE}} &  \bl{\textrm{Variational autoencoder}}~\cite{Kingma2014}\\
12.&    \bl{ {\rm CATPCA}} &  \bl{\textrm{Categorical PCA}}~\cite{Sulc2015DimensionalityRO}\\
13.&    \bl{ {\rm HCA}} &  \bl{\textrm{Hierarchical Cluster Analysis}}~\cite{Sulc2015DimensionalityRO}\\
    % $\mathrm{\chi^2}$ & $\chi^2$-\textrm{feature selection}~\cite{chi_square}\\
    % {\rm MI} & \textrm{Mutual information feature selection}~\cite{MI}\\
    
\hline
\end{tabular}
    %\vspace*{-5mm}
\end{table}



% \hspace*{-1cm}
% \setlength{\tabrowsep}{6pt}
\begin{table*}[!t]
\centering
\caption{{ Summarisation of the baselines. } }\label{tab:baseline_characteristic}
%\vspace{-2mm}
% \scalebox{1}{
\resizebox{\textwidth}{!}{%
  %\begin{tabular}{ c|c|c|c|c|c|c|c|c|c|c|r}%\label{tab:baseline_characteristic}
  % IEEE does not like inter-column vertical lines
  \begin{threeparttable}
  \begin{tabular}{Slcccccccccccccc}
    \toprule
    Characteristics&FSketch&FH&SH&SSD&OHE&KT&NNMF&MCA&LDA&LSA&PCA&{VAE}&{CATPCA}&{HCA}\\
    \midrule
\pbox{5em}{Output\\ discrete\\ sketch}&\cmark&\cmark&\cmark&\xmark&\cmark&\cmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark &\xmark&\cmark\\[2em]
%\hline
\pbox{5em}{Output\\ real-valued\\ sketch}&\xmark&\xmark&\xmark&\cmark&\xmark&\xmark&\cmark&\cmark&\cmark&\cmark&\cmark&\cmark&\cmark&\xmark\\[2em]
%\hline
\pbox{5em}{Approximating\\ distance\\ measure}&{Hamming}&\pbox{4em}{ Dot\\product}& {Cosine}& {Hamming}&{Hamming}& \texttt{NA} &\texttt{NA} &\texttt{NA} &\texttt{NA} & \texttt{NA}& \texttt{NA} & \texttt{NA} &\texttt{NA}&\texttt{NA}\\[2em]
%&\texttt{distance}&\texttt{ product}& \texttt{Similarity}&  & & & & & & & \\
%\hline
\pbox{5em}{Require \\labelled data}&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark\\[2em]
%\hline
\pbox{5em}{Dependency\\ on the size\\ of sample \tnote{*}} &\xmark&\xmark&\xmark&\xmark&\xmark&\xmark&\xmark &\cmark&\xmark&\cmark &\cmark&\xmark&\cmark&\xmark\\[2em]
%\hline 
\pbox{5em}{End tasks\\ comparison}&All&All&All&All&All&All& \pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}
&\pbox{5em}{Clustering,\\ Similarity Search}&\pbox{5em}{Clustering,\\ Similarity Search}\\
\bottomrule
 \end{tabular}
 \begin{tablenotes}
 \item[*] The size of the maximum possible reduced dimension is the minimum of the number of data points and the dimension.
 \end{tablenotes}
 \end{threeparttable}
}
\end{table*}

\subsection{Baselines}
Recall that \fsketch (hence \minfsketch) {\em compresses
categorical vectors to shorter categorical vectors in an unsupervised manner that	``preserves'' Hamming distances}. %We use the following baselines for comparison with \fsketch.   

Our first baseline is based on one-hot-encoding (OHE) which is one of the most common methods to convert categorical data to a numeric vector and can approximate pairwise Hamming distance (refer to Appendix~\ref{appendix:OHE+BS}). Since OHE actually increases the dimension to very high levels (e.g., the dimension of the binary vectors obtained by encoding the NYTimes dataset is $11,703,240$), the best way to use it is by further compressing the one-hot encoded vectors. For empirical evaluation we applied BinSketch~\cite{ICDM} which is the state-of-the-art binary-to-binary dimensionality reduction technique that preserves Hamming distance. We refer to the entire process of OHE followed by BinSketch  simply by OHE in the rest of this section.

%We use \textit{one-hot-encoding}+ BinSketch~\cite{ICDM} (abbreviated as OHE) to compress high-dimensional categorical vectors into low-dimensional binary vectors that approximate the corresponding original pairwise Hamming distance.  We first use \textit{one-hot-encoding} to convert categorical features into binary vectors which exactly estimate the corresponding pairwise Hamming distance. Then we can apply BinSketch on top of binary vectors obtained from \textit{one-hot-encoding} to compress them into low-dimensional binary vectors which  estimate the pairwise Hamming distance between binary vectors, and as a  consequence the corresponding pairwise Hamming distance on original categorical vectors. A major limitation of OHE is that binary vectors obtained from {one-hot-encoding} can be  very high-dimensional  as each categorical feature is converted into a $c$-dimensional binary vector, where $c$ is possible number of categories of that feature. {\color{red} To emphasize this, for BrainCell and NYTimes datasets,  the dimension of binary vectors obtained after one-hot-encoding becomes $2,659,274,572$ and , respectively.} This may lead to slow running time and computation challenges.

To the best of our knowledge, there is no sketching algorithm other than OHE that compresses high-dimensional categorical vectors to low-dimensional categorical (or integer) vectors that preserves the original pairwise Hamming distances. Hence, we chose as baseline state-of-the-art and popularly employed algorithms that either preserve Hamming distance or output discrete-valued sketches (preserving some other similarity measure).  
We list them in Table~\ref{tab:baselines_methods} and  tabulate their characteristic in Table~\ref{tab:baseline_characteristic}. Their implementation details are discussed in Appendix~\ref{sec:Reproducibility_baseline}.


%  We use Sketching via Stable Distribution (SSD)~\cite{SSD} that compresses high-dimensional real valued vectors to low-dimensional real valued vectors which closely approximates the pairwise Hamming distance. 


\begin{figure*}
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
\includegraphics[width=\linewidth]{images/RMSE_different_p_new.pdf}
%\vspace{-4mm}
\caption{{Comparison of $\RMSE$ measure obtained from \texttt{FSketch} algorithm on various choices of  $p$. \bl{Values of $c$ for NIPS, Enron, NYTimes, and GISETTE are 132, 150, 114, and 999, respectively.}
}}
\label{fig:varying_p}
\end{figure*}
 
 
%  However, 
% we include sketching algorithms as baselines that output discrete valued sketches which  preserve other similarity measures. We use Feature Hashing (FH)~\cite{WeinbergerDLSA09} and signed-random-projection (SRP)  (alternatively known as SimHash (SH))~\cite{simhash}, which compress real-valued vectors and approximate inner product and cosine similarity in their respective sketches. 
% \textcolor{red}{We use Skeching via Stable Distribution (SSD)~\cite{SSD}~ a hamming distance estimation technique which uses stable distribution to estimate hamming distance and One Hot Encoding (OHE) scheme to estimate the hamming distance by compressing the one hot encoded samples using BinSketch\cite{ICDM}.
% }
 We include Kendall rank correlation coefficient (KT)~\cite{kendall1938measure} -- a feature selection algorithm which generates discrete valued sketches. Note that if we apply Feature Hashing (FH), SimHash (SH), and KT naively on  categorical datasets, we get discrete valued sketches on which Hamming distance can be computed.% We compute the Hamming distance of the sketch and report it as the original pairwise Hamming distance.
% \textcolor{red}{On applying SSD on categorical dataset corresponding to each sample we get a real value for each sample, for two  sample the difference of obtained value through SSD gives the estimate of original hamming distance between these two sample. On applying OHE on categorical data set firstly we get binary conversion of categrical data using one hot encoding and on the top of that OHE uses well establish binary sketching technique BinSketch~\cite{ICDM} to get compressed binary sketch and to estimate original hamming distance from the compressed binary sketch.}
We also include a few other well known dimensionality reduction methods such as Principal component analysis (PCA), Non-negative Matrix Factorisation (NNMF)~\cite{NNMF}, Latent Dirichlet Allocation (LDA)~\cite{LDA}, Latent Semantic Analysis (LSA)~\cite{LSI}, \bl{Variational Autoencoder (VAE) \cite{Kingma2014}, Categorical PCA (CATPCA)\cite{Sulc2015DimensionalityRO}, Hierarchical Cluster Analysis (HCA) \cite{Sulc2015DimensionalityRO}}   all of which output real-valued sketches.
% \textcolor{red}{We use the sketches obtained from these  sketching methods for two ends tasks clustering and similarity search. }
% % We also include $\chi^2$-{feature selection}, and   {Mutual Information based feature selection} algorithm  for comparison. However, we use them only in the classification tasks for comparison as they   require labeled data for feature selection.









%  Recall that \fsketch (hence \minfsketch) {\em compresses
% categorical vectors to shorter categorical vectors in an unsupervised manner that	``preserves'' Hamming
% distances} and we did not find any out-of-the-box method that does the same. So, for comparison we identified the state-of-the-art methods that either output integer-valued
% sketches (allowing Hamming distance calculation) or simply perform
% dimensionality reduction of categorical vectors that can be used for clustering, classification and similarity search. We summarise the  baseline methods used for comparison in
% Table~\ref{tab:baselines_methods}  
% and  summarise the characteristic of all the baseline algorithms in Table~\ref{tab:baseline_characteristic}. We  give implementation details of the baseline algorithms in Appendix~\ref{sec:Reproducibility_baseline}.


%%%%%%%%%% BELOW IS COMMENTED %%%%%%%%%%%%%%
\begin{comment}
\begin{enumerate}
 \item  We used our own implements for  {Feature Hashing (FH)}~\cite{WeinbergerDLSA09} and   {SimHash (SH)}\cite{simhash} algorithms. We give its link via the following repository here \footnote{\url{https://github.com/Anonymus135/F-Sketch}}.
 \item for  {Kendall rank correlation coefficient}\cite{kendall1938measure} we use \texttt{pandas data frame implementation}\footnote{\url{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html}}. 
 \item for {Latent Semantic Analysis (LSA)}~\cite{LSI}, {Latent Dirichlet Allocation (LDA)}\cite{LDA}, {Non-negative Matrix Factorisation (NNMF)}\cite{NNMF}, and  vanilla {Principal component analysis (PCA)}, we use the implementation available \texttt{sklearn.decomposition} library \footnote{\url{https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.decomposition}}.
 \item for {Multiple Correspondence Analysis (MCA)}~\cite{MCA}, we use the following implementation\footnote{\url{https://pypi.org/project/mca/}}.
 \item finally for $\chi^2$-{feature selection} \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\#}}\cite{chi_square} and  {Mutual Information based feature selection}~\cite{MI}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html}} we use the implementation available at \\ \texttt{sklearn.feature\_selection} library.
 \end{enumerate}
We include $\chi^2$-{feature selection}, and   {Mutual Information
based feature selection} algorithm  only in the classification tasks for
comparison as they   require labelled data for feature selection. PCA, MCA and
LSA can reduce the data dimension up to the minimum of the number of data points
and original data dimension.
%%%%%%%%%% BELOW IS COMMENTED %%%%%%%%%%%%%%
\end{comment}

%(detailed descriptions about hardware, datasets andbaselines are given in Append~\ref{sec:Reproducibility}).

 








%\begin{comment}
%\noindent\textbf{Candidate algorithms:} We compare and contrast the performance of our algorithm with the following baselines. 
%\begin{inparaenum}
% [(a)]\item  \texttt{Feature Hashing}~\cite{WeinbergerDLSA09}, \item \texttt{Signed-random projection} or \texttt{Simhash}\cite{simhash}, \item \texttt{Kendall rank correlation coefficient}\cite{kendall1938measure}, \item \texttt{Latent Semantic Analysis (LSA)}\cite{LSI}, \item \texttt{Latent Dirichlet Allocation (LDA)}\cite{LDA}, \item \texttt{Multiple Correspondence Analysis (MCA)}\cite{MCA}, \item \texttt{Non-negative Matrix Factorization (NNMF)}\cite{NNMF}, \item vanilla \texttt{Principal component analysis (PCA)}, \item $\chi^2$-\texttt{feature selection}\cite{chi_square}, and \item \texttt{Mutual Information based feature selection}\cite{MI}. 
%\end{inparaenum}
%We defer the details used in the implementation of the  baselines to the appendix under the reproducibility section. {\color{red} We can simply refer to the above table. For all algorithms and datasets add shortnames that are referred to in the plots.}
%\end{comment}
%\subsection{Choice of Parameters}


\begin{figure*}
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
\includegraphics[width=\linewidth]{images/Space_efficiency_new.pdf}
%\vspace{-4mm}
    \caption{{Space overhead of uncompressed vectors stored as a list of non-zero entries and their positions. $Y$-axis represents the ratio of the space used by uncompressed vector to that obtained from \fsketch.
%    Comparison of space saving  obtained from \texttt{FSketch} algorithm on various choices of   $p$. Y-axis  corresponds to the ratio of the space required by a full-dimensional dataset with the space required by the sketch obtained by \fsketch.
}}
\label{fig:varying_p_space}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{images/new_integer_enron_box_plot.pdf}
%\vspace{-2mm}
    \caption{{Comparison of avg.\ error in estimating Hamming distance of a pair of points from the Enron dataset.}}%\textit{“Hamming error:= actual Hamming distance - estimated Hamming distance from the sketch”} on various reduced dimension, over  $100$ iterations for a random pair of points from Enron dataset.}}

\label{fig:box_plot_hamming_error}
\end{figure*}


\subsection{Choice of $p$}
We discussed in Section~\ref{sec:analysis} that a larger value of $p$ (a prime number) leads to a tighter estimation of Hamming distance but degrades sketch sparsity, which negatively affects performance at multiple fronts, and moreover, demands more space to store a sketch. We conducted an experiment to study this trade-off, where we ran our proposal with different values of $p$, and computed the corresponding $\RMSE$ values. The $\RMSE$ is defined as the square-root of the average error, among all pairs of data points, between their actual Hamming distances and the corresponding estimate obtained via \texttt{FSketch}. Note that a lower $\RMSE$ indicates that the sketch correctly estimates the underlying pairwise Hamming distance.  We also note the corresponding \textit{space overhead} which is defined as the ratio of the space used by uncompressed vector and its sketch obtained from~\texttt{FSketch}. We consider storing a data point in a typical sparse vector format -- a list of non-zero entries and their positions (see Table~\ref{tab:space-example}). We summarise our results in Figures~\ref{fig:varying_p} and  ~\ref{fig:varying_p_space}, respectively. We observe that  a large value of $p$ leads to a lower $\RMSE$ (in Figure~\ref{fig:varying_p}), however simultaneously it leads to a smaller space compression (Figure~\ref{fig:varying_p_space}). \bl{
As a heuristic, we decided to set $p$ as the next prime after $c$ as shown in this table.}\\
{
 \footnotesize
\noindent\begin{tabular}{lc@{\hskip 2em}lc@{\hskip 2em}lc}
    \toprule
     Brain cell & $2039$ & NYTimes & $127$ & Enron & $151$ \\
     KOS & $43$ & Delicious & $59$ & Gisette & $1009$\\
     \bl{NIPS} &\bl{$137$}&&&\\ 
     \bottomrule
\end{tabular}
}

\bl{That said, the experiments reveal that, at least for the datasets in the above experiments, setting $p$ to be at least $c/4$ may be practically sufficient, since there does not appear to be much advantage in using a larger $p$.}
%both the space saving and the RMSE do not appear to differ substantially once $p$ is sufficiently large -- we observe a rule of thumb that for the datasets in these experiments $p$ should be chosen to be at least $c/4$. 
%As a special case, $p=2$ corresponds to a binary sketch and requires the least space but suffers in terms of $\RMSE$. Setting $p$ to be $1/3$-rd of the number of categories ($c$) appears to be an optimal choice for the datasets at hand. %Even though there does not appear to be any real advantage in using a larger $p$. 


% We discussed in Section~\ref{sec:analysis} that a larger value of $p$ leads to a tighter estimation of Hamming distance but degrades sketch sparsity, which negatively affects performance at multiple fronts, and requires more space to store a sketch. We conducted two experiments to study this trade-off, using $k=1$. In Figure~\ref{fig:varying_p} we show how a large value of $p$ leads to a lower $\RMSE$ ($\RMSE$ experiments are explained in Subsection~\ref{subsubsec:rmse}) and in Figure~\ref{fig:varying_p_space} we show that the same leads to a better space compression with respect to uncompressed vectors that are stored in a typical sparse vector format (see Table~\ref{tab:space-example}).

% $p=2$ corresponds to a binary sketch and requires the least space but suffers in terms of $\RMSE$. Setting $p$ to be $1/3$-rd of the number of categories ($c$) appears to be an optimal choice for the datasets at hand. %Even though there does not appear to be any real advantage in using a larger $p$, 
% For all our experiments we decided to set $p$ as the next prime after $c$.


%From Theorem~\ref{thm:main}, for two data points $x,y \in \{0,1,\ldots,c-1\}^n$, the estimation of their hamming distance from their sketches $x’,y’ \in \{0,1,\ldots,p-1\}^d$, depends on the choice of the prime number $p$. Further, the space needed for storing the  sketches $x’,y’$ is $O(d\log p)$. Clearly, higher values of $p$ requires more space for storing the sketch. We empirically give a trade-off between the choice of $p$ and the quality of estimate of the hamming distance between $x$ and $y$ via $\RMSE$. To do so, we run the experiments mentioned in Subsection~\ref{subsubsec:rmse} for \texttt{FSketch} by running it for  different values of $p$, and observing the corresponding $\RMSE$. We summarise our results in Figure~\ref{fig:varying_p}.  When $p=2$, the output sketch is binary, which requires lesser space. However, in this case, $\RMSE$ value is higher. As we increase the value of $p$ the $\RMSE$ values start decreasing and converging toward zero. 
%
%We further give a trade off on choice of $p$ with the space required to store the sketch. In sparse representation full dimensional data requires $\sigma \log n$ to store the non-zero indices, and  $\sigma \log c$ to store the corresponding values. Thus, in total it requires $\sigma \log cn$ space.  Further, the reduced dimensional data obtained from \texttt{FSketch} requires $\sigma_s \log d$, and  $\sigma \log p$ to store the corresponding values, and in total requires $\sigma_s \log pd$ space, where $\sigma_s$ is the sparsity of the sketch. We empirically compare these two quantities by computing the ratio of these two,  and summaries our results for different choices of $p$ in Figure~\ref{fig:varying_p_space}.
%\textcolor{red}{space saving w.r.t. p}




\begin{figure*}
{
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
\includegraphics[width=\linewidth]{images/New_Reduction_time_revision1.pdf}
    \caption{Comparison among the baselines on the dimensionality reduction time. See Appendix~\ref{appendix:section:extended_exp} for results on the other datasets which show a similar trend and Section~\ref{sec:appendix_oom_error} for the errors encountered by some baselines.}
%\textcolor{red}{Note that in this plot several baselines don’t have measures corresponding to all values for the reduced dimensions. This is due to the following: several baselines such as  MCA, PCA, LSA can give dimensionality reduction upto maximum of number of data points and dimension. For others we got either \textit{out-of-memory (OOM)} or \textit{did-not-stop (DNS)} beyond the respective reduced dimensions.}
%}}
\label{fig:reduction_time}
}
\end{figure*}


% \begin{table*}
% \centering
% %\resizebox{\columnwidth}{!}{
%     \caption{{Speedup of \texttt{FSketch} \textit{w.r.t.} baselines on the reduced dimension $1000$. 
%     % We obtain similar pattern on other reduced dimension as well. 
%     \texttt{OOM} indicates ``out-of-memory error'' and {\tt DNS} indicates ``did not stop'' after a sufficiently long time.
% } }\label{tab:speed_up_dim_time}
% %\vspace{-2mm}
% \scalebox{1}{
%   \begin{tabular}{ l|c|c|c|c|c|c|c|c|c|r}%\label{tab:speed_up_dim_time}
%     \toprule
%     Dataset&OHE&KT&NNMF&MCA&LDA&LSA&PCA&SSD&SH&FH\\
%     \midrule
% NYTimes&$5774 \times$ &\texttt{OOM}&$3647\times$&\texttt{OOM}&$200\times$&$30\times$&$10\times$& $158 \times$ & $1.8\times$&$0.84\times$\\
% Enron&$2259 \times$ &\texttt{DNS}&$5736\times$&$161\times$&$80\times$&$17\times$&$6.4\times$& $99.21 \times$ & $1.24\times$&$0.81\times$\\
% KOS&$404\times$  &$5133\times$&$1463\times$&$13\times$&$97\times$&$6.3\times$&$1.6\times$& $ 19.32 \times$  & $0.44\times$&$0.85\times$\\
% DeliciousMIL&$160 \times$ &$17434\times$&$1665\times$&$21\times$&$151\times$&$9\times$&$3\times$& $9.56 \times$  & $0.56\times$&$0.97\times$\\
% Gisette&$480 \times$  &$1183\times$&$630.2\times$&$4\times$&$245\times$&$3.4\times$&$0.52\times$& $15.4 \times$ & $0.21\times$&$1.03\times$\\
% Brain Cell& \texttt{OOM} &\texttt{OOM}& {\tt DNS}  &\texttt{OOM} &$364\times$ & $87\times$ & $68\times$ & $436 \times$ &  $6.1\times$ & $0.92\times$ \\
% \bottomrule
%  \end{tabular}
% }
%     %\vspace*{-5mm}
% \end{table*}




\begin{table*}
{
\centering
%\resizebox{\columnwidth}{!}{
    \caption{{Speedup of \texttt{FSketch} \textit{w.r.t.} baselines on the reduced dimension $1000$. 
    % We obtain similar pattern on other reduced dimension as well. 
    \texttt{OOM} indicates ``out-of-memory error'' and {\tt DNS} indicates ``did not stop'' after a sufficiently long time.
} }\label{tab:speed_up_dim_time}
%\vspace{-2mm}
\scalebox{0.99}{%
  \begin{tabular}{ lccccccccccccr}%\label{tab:speed_up_dim_time}
    \toprule 
    Dataset  &   OHE          &       KT       &      NNMF       &    MCA        &     LDA     &     LSA       &   PCA         &   VAE         &     SSD            &     SH       &    FH        & CATPCA & HCA   \\
    \midrule
NYTimes      &$ \texttt{OOM} $&$ \texttt{OOM}  $&$ 6149\times $&$  \texttt{OOM} $&$ 189\times $&$  11.5\times $&$ 88.14\times $&$ 4340 \times $&$ 164.9\times $&$ 1.2\times   $&$ 0.99\times$     &${\tt DNS}$&${\tt DNS}$ \\
Enron        &$ \texttt{OOM} $&$ \texttt{DNS}  $&$2624\times$ &$  \texttt{OOM} $&$ 122\times $&$ 15.5 \times $&$\texttt{OOM} $&$ \texttt{DNS}$&$ 25.5\times   $&$ 1.25\times  $&$ 0.87\times$     &${\tt DNS}$&$1268.2\times$\\ 
KOS          &$ 629\times    $&$ 14455\times   $&$ 1754\times    $&$   20.41\times $&$ 128\times $&$ 6.40\times  $&$ 9.5\times $&$  1145\times $&$ 14.62\times   $&$ 0.79\times  $&$ 0.98\times$  &${\tt DNS}$&$81.24\times$\\
DeliciousMIL &$ 1332\times   $&$ 14036\times   $&$ 1753\times    $&$   40.39\times $&$ 136\times $&$ 6.6\times   $&$ 18.1\times $&$  1557\times $&$ 29.2 \times $  &$ 0.61\times  $&$ 0.90\times$ &${\tt DNS}$&$117.6\times$\\
Gisette      &$ 399\times    $&$ 1347\times    $&$ 459\times  $&$   5.7\times   $&$ 269\times $&$ 5.4\times   $&$ 4.2\times   $&$  285\times  $&$  8.1\times    $&$ 0.69\times  $&$ 0.98\times$   &${\tt DNS}$&$16.78\times$\\ 
% Brain Cell   &$ \texttt{OOM} $&$ \texttt{OOM}  $&$ {\tt DNS}     $&$ \texttt{OOM}  $&$ 173\times $&$ 79.37\times $&$ 62.63\times $&$ 855.65\times$&$ 2114.78\times $&$ 5.01\times  $&$ 0.89\times$ &&\\
NIPS         &$ 378\times    $&$ 15863\times    $&$ 1599\times  $&$  26.6\times   $&$ 302\times $&$ 6.4\times   $&$ 3.17\times   $&$  451\times  $&$  29.9\times    $&$ 0.47\times  $&$1.20\times$ &${\tt DNS}$&$58.49\times$\\ 
Brain Cell     &$ \texttt{OOM} $&$\texttt{OOM} $&$ {\tt DNS}  $&$ \texttt{OOM} $&$ 322\times $&$ 79.38\times  $&$ 62.7\times $&$1198\times$    &$ 443 \times$ &  $5\times$ & $0.89\times$  &${\tt DNS}$&${\tt DNS}$\\
\bottomrule
 \end{tabular}
 }
}
    %\vspace*{-5mm}
\end{table*}



\begin{figure*}
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
\includegraphics[width=\linewidth]{images/Updated_RMSE_new.pdf}
%\vspace{-2mm}
\caption{{Comparison on $\RMSE$ among baselines. A lower value is an indication of better performance.  See Appendix~\ref{appendix:section:extended_exp} for results on the other datasets which show a similar trend. %$\mathrm{KT}$ couldn’t finish in 10 hours (\textcolor{red}{@Bhisham pls mention the correct time.}) on NYTimes and Enron datasets. 
}}
\label{fig:rmse}
\end{figure*}



\subsection{Variance of \fsketch}
In Section~\ref{subsec:analysis} we explained that the bias of our estimator is upper bounded with a high likelihood. However, there remains the question of its variance. To decide the worthiness of our method, we compared the variance of the estimates of the Hamming distance obtained from \fsketch and from the other randomised sketching algorithms with integer-valued sketches (KT was not included as it is a deterministic algorithm, and hence, has zero variance).

% $\hat{h}$ from Definition~\ref{defn:estimator} on the sketches obtained from \fsketch and from other randomized sketching algorithms with integer-valued sketches.
Figure~\ref{fig:box_plot_hamming_error} shows the Hamming error (estimation error) for a randomly chosen pair of points from the Enron dataset, averaged over 100 iterations. 
We make two observations.

First is that the estimate using \texttt{FSketch} is closer to the actual Hamming distance even at a smaller
reduced dimension; in fact, as the reduced dimension is increased, the variance
becomes smaller and the Hamming error converges to zero. Secondly, \fsketch causes a smaller error compared to the other baselines. On the other hand, {feature hashing} highly underestimates the actual Hamming distance, but has low variance, and tends to have negligible Hamming error with an increase of the reduced dimension. The behaviour of {SimHash} is counter-intuitive as on lower reduced dimensions it closely estimates the actual Hamming distances, but on larger dimensions it starts to highly underestimate the actual Hamming distances. This creates an ambiguity on the choice of a dimension for generating a low-dimensional sketch of a dataset.%, as it is not clear which reduce dimension should be appropriate for closely estimating the actual Hamming distance. 
 Similar to \fsketch, the sketches produced by SSD, though real-valued, allow estimation of pairwise Hamming distances. However the estimation error increases with the reduced dimension. Lastly, OHE seems to be highly underestimating pairwise Hamming distances. 

%We observed that the estimates obtained using \fsketch is closer to the actual Hamming distance even at a smaller reduced dimension and much smaller compared to those of other baselines. We defer the detailed explanation to Appendix~\ref{subsec:variance_details}. 
% Observing the really low variance of $\hat{h}$, we conjecture that the gain in performance from \minfsketch may not outweigh its additional space and time complexity. We decided to continue with \fsketch for performance evaluation in data analytic tasks.


%%%%%% BELOW IS COMMENTED %%%%%%%%%%%%%
% \begin{comment}
% \subsection{Comparing variance of \fsketch with other compression
% algorithms}\label{subsec:variance_details}

% \begin{figure}[!ht]
% \centering
% \includegraphics[width=\linewidth]{images/integer_enron_box_plot.pdf}
% %\vspace{-2mm}
%     \caption{{Comparison of avg.\ error in estimating Hamming distance of a pair of points from the Enron dataset.}}%\textit{“Hamming error:= actual Hamming distance - estimated Hamming distance from the sketch”} on various reduced dimension, over  $100$ iterations for a random pair of points from Enron dataset.}}
% \end{figure}

% %\noindent\textbf{Insights:} 
% First note that KT was not included as it is a deterministic algorithm, and
% hence, has zero variance. We make two important observations. First, is that the estimate using \texttt{FSketch} is closer to the actual Hamming distance even at a smaller
% reduced dimension; in fact, as the reduced dimension is increased, the variance
% becomes smaller and the Hamming error converges to zero. Secondly, \fsketch causes a smaller error compared to the other baselines. Whereas the {Feature hashing} highly underestimates the actual Hamming distance, with low variance, and tends to have negligible Hamming error with an increase of the reduced dimension. The behavior of {SimHash} is counter-intuitive as at lower reduced dimension it closely estimates the actual Hamming distance, whereas with an increase of the reduced dimension starts highly underestimating the actual Hamming distance. This leaves the ambiguity on the choice of {SimHash} for finding a low-dimensional sketch of Categorical dataset, as it is not clear which reduce dimension should be appropriate for closely estimating the actual Hamming distance. 
% \end{comment}
 %%%%%% ABOVE IS COMMENTED %%%%%%%%%%%%%



%\noindent\textbf{Insights:} 
%We make two important observations. First, is that the estimate using \texttt{FSketch} is closer to the actual Hamming distance even at a smaller reduced dimension; in fact, as the reduced dimension is increased, the variance becomes smaller and the Hamming error converges to zero. Secondly, the error from sketches obtained from the other two algorithms are worse, even though their variance is lower at really small dimensions. Contrary to our expectation, the error of SimHash becomes worse with increasing dimension. Feature hashing behaves as expected but \fsketch shows a better performance even for dimension as low as 100.
%\textcolor{red}{On lower dimensions, SimHash underestimates the $\RMSE$ error whereas on higher dimensions it starts overestimating the error. Thus, it becomes difficult to find an appropriate reduced dimension where SimHash correctly estimates the actual Hamming distance.}
%from the estimate used by baseline algorithms. To do so, we first take a random sample of a pair of points from the   {Enron dataset}, we compute their actual Hamming distance. We then reduce the dimension of data to various reduced  dimensions using the baseline algorithms, and estimate the Hamming distance from the sketch. We repeat this process  {$100$ times} and plot the \textit{“Hamming error:= actual Hamming distance-estimated Hamming distance”} using box plot, and summarise it in Figure~\ref{fig:box_plot_hamming_error}.  We didn’t include the discrete hashing algorithm Kendall correlation coefficient as it is deterministic. 


\subsection{Speedup in dimensionality reduction}\label{subsubsec:dim_red_time}
 We compress the datasets to several dimensions using \fsketch and the baselines and report their running times in Figure~\ref{fig:reduction_time}. We notice that \fsketch has a comparable speed \textit{w.r.t.} Feature hashing and SimHash, and is significantly faster than the other baselines. However, both feature hashing and SimHash are not able to accurately estimate the Hamming distance between data points and hence perform poorly on $\RMSE$ measure (Subsection~\ref{subsubsec:rmse}) and the other tasks. Many baselines such as OHE, KT, NNMF, MCA, \bl{CATPCA, HCA} give \textit{``out-of-memory”} (OOM) error, and also didn’t stop (DNS) even after running for a sufficiently long time ($\sim~10$ hrs) on high dimensional datasets such as Brain Cell and NYTimes. On other moderate dimensional datasets such as Enron and KOS, our speedup \textit{w.r.t.} these baselines are of the order of a few thousand. We report the numerical speedups that we observed in Table~\ref{tab:speed_up_dim_time}.




% We compress the datasets to several dimensions using \fsketch and the baselines algorithms and report the running times in Figure~\ref{fig:reduction_time}. We notice that \fsketch has a comparable speed \textit{w.r.t.} Feature hashing and SimHash, and is significantly faster than the other baselines. However, both Feature Hashing and SimHash are not able to accurately estimate the Hamming distance between data points and hence perform poorly on $\RMSE$ measure (Subsection~\ref{subsubsec:rmse}) and the other tasks (Subsection~\ref{subsec:end_task_comparision}). We summarise the speedups that we saw in Table~\ref{tab:speed_up_dim_time}.

% \begin{table*}
% \centering
% %\resizebox{\columnwidth}{!}{
%     \caption{{Speedup of \texttt{FSketch} \textit{w.r.t.} baselines on the reduced dimension $1000$. 
%     % We obtain similar pattern on other reduced dimension as well. 
%     \texttt{OOM} indicates ``out-of-memory error'' and {\tt DNS} indicates ``did not stop'' after a sufficiently long time.
% } }\label{tab:speed_up_dim_time}
% %\vspace{-2mm}
% \scalebox{1}{
%   \begin{tabular}{ l|c|c|c|c|c|c|c|c|c|r}%\label{tab:speed_up_dim_time}
%     \toprule
%     Dataset&MI&$\chi^2$&KT&NNMF&MCA&LDA&LSA&PCA&SH&FH\\
%     \midrule
% NYTimes&\texttt{NA}&\texttt{NA}&\texttt{OOM}&$3647\times$&\texttt{OOM}&$200\times$&$30\times$&$10\times$&$1.8\times$&$0.84\times$\\
% Enron&\texttt{NA}&\texttt{NA}&\texttt{DNS}&$5736\times$&$161\times$&$80\times$&$17\times$&$6.4\times$&$1.24\times$&$0.81\times$\\
% KOS&\texttt{NA}&\texttt{NA}&$5133\times$&$1463\times$&$13\times$&$97\times$&$6.3\times$&$1.6\times$&$0.44\times$&$0.85\times$\\
% DeliciousMIL&$103\times$&$0.4\times$&$17434\times$&$1665\times$&$21\times$&$151\times$&$9\times$&$3\times$&$0.56\times$&$0.97\times$\\
% Gisette&$18.3\times$&$0.054\times$&$1183\times$&$630.2\times$&$4\times$&$245\times$&$3.4\times$&$0.52\times$&$0.21\times$&$1.03\times$\\
%       Brain Cell&\texttt{NA} & \texttt{NA} & \texttt{OOM}& {\tt DNS}  &\texttt{OMM} &$364\times$ & $87\times$ & $68\times$ & $6.1\times$ & $0.92\times$ \\
% \bottomrule
%  \end{tabular}
% }
%     %\vspace*{-5mm}
% \end{table*}




% \begin{comment}
% {Note: \\
% Memory Error: Kindell Tau gives memory error for NYTimes and Enron data set, MCA also gives memory error for NYTimes\\
% For NMF as the value of reduced dimension increase the reduction time increase with high rate so we are able to take reduced dimension only up to 3000 (for 3000 the reduction time 71869 seconds)} --- \textcolor{red}{@Bhisham on which dataset??}\\ 
% PCA, MCA and LSA can reduce dimension only upto 2000 as we are using 2000 samples in our experiments
% \end{comment}
 


\begin{figure*}
{
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
    \includegraphics[width=\linewidth]{images/Purity_index_revision1.pdf}
\caption{{Comparing the quality of clusters on the compressed datasets.  See Appendix~\ref{appendix:section:extended_exp} for results on the other datasets which show a similar trend.
}}
\label{fig:purity}
}
\end{figure*}
 
 \begin{figure*}
 {
\centering
% \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
\includegraphics[width=\linewidth]{images/TopK_revision1.pdf}
\caption{{Comparing the performance of the similarity search task (estimating top-$k$ similar points with $k=100$) achieved on the reduced dimensional data obtained from various baselines.  See Appendix~\ref{appendix:section:extended_exp} for results on the other datasets which show a similar trend.
}}
\label{fig:similarity_search}
}
\end{figure*}

\subsection{Performance on root-mean-squared-error (RMSE)}\label{subsubsec:rmse}
How good are the sketches for estimating Hamming distances between the uncompressed points in practice? To answer this, we compare \fsketch with integer-valued sketching algorithms, namely,  {feature hashing}, {SimHash}, {Kendall correlation coefficient} and OHE+BinSketch.   Note that {feature hashing} and {SimHash} are known to approximate inner product and cosine similarity, respectively. However, we consider them in our comparison nonetheless as they output discrete sketches and Hamming distance can be computed on their sketch. We also include SSD for comparison which outputs real-valued sketches and  estimates original pairwise Hamming distance. 
For each of the methods we compute its $\RMSE$ as the square-root of the average error, among all pairs of data points, between their actual Hamming distances and their corresponding estimates (for \fsketch the estimate was obtained using Definition~\ref{defn:estimator}). Figure~\ref{fig:rmse} compares these values of $\RMSE$ for different dimensions; note that a lower $\RMSE$ is an indication of better performance. It is immediately clear that the $\RMSE$ of \texttt{FSketch} is the lowest among all; furthermore, it falls to zero rapidly with increase in reduced  dimension.  This demonstrates that our proposal \texttt{FSketch} estimates the underlying  pairwise Hamming distance better than the others.

% How good are the sketches for estimating Hamming distances between the uncompressed points in practice? To answer this, we compare \fsketch with integer-valued sketching algorithms, namely,  {Feature Hashing}, {SimHash}, and {Kendall correlation coefficient}. {Feature Hashing} and {SimHash} are known to approximate inner product and cosine similarity, respectively. However, we consider them nonetheless as Hamming distance can be computed on the discrete sketches that they output.
% For each of the methods we compute its $\RMSE$ as the square-root of the average error, among all pairs of data points, between their actual Hamming distances and the corresponding estimate obtained using Definition~\ref{defn:estimator}. Figure~\ref{fig:rmse} compares these values of $\RMSE$ for different dimensions. It is immediately clear that the $\RMSE$ of \texttt{FSketch} is the lowest among all; furthermore, it falls to zero rapidly with decreasing dimension --- this demonstrates the high fidelity of our Hamming distance estimator.

%indicates that \fsketch is able to estimate the Hamming distances quite accurately in practice.
% Whereas $\RMSE$ of SimHash is the worst and increases with the increase of reduced dimension. For Feature Hashing, and Kendall   coefficient $\RMSE$ score is high, however, it starts decreasing with the increase of reduced dimension. 
%In this experiment,  the aim is to show that the sketch obtained from the  baselines methods shows how closely they are able to estimate the original similarity between data points. To do so, for a pair of data points, we estimate their hamming distance using various baseline methods.  We compute the square of the difference between the estimated hamming distance and their actual hamming  distance. We repeat this for all pairs of data points,  add all such numbers obtained from squared difference,  compute their mean, and then compute the square root.  We report this as $\RMSE$. A lower value of $\RMSE$ is an indication of better performance.  We repeat this for several values of $d$ (reduced dimension) and report the corresponding values of $\RMSE$. In this experiment, we include the following baselines for comparison -- {Feature Hashing}, {SimHash}, {Kendall correlation coefficient}. {Feature Hashing} and {SimHash} are known to approximate Inner product and Cosine similarity, respectively. However, we include them in the comparison nevertheless, as they output discrete sketches and that Hamming distance can be defined on them in order to compute the $\RMSE$.

%\noindent\textbf{Insights:} $\RMSE$ of \texttt{FSketch} is the lowest among the baseline, and it is simultaneously close to zero,  which implies that it accurately measures the actual distance. Whereas $\RMSE$ of SimHash is the worst and increases with the increase of reduced dimension. For Feature Hashing, and Kendall   coefficient $\RMSE$ score is high, however, it starts decreasing with the increase of reduced dimension. 
%{In each insights need to mention that our  quality of estimate is comparable wr.t to baselines while we obtained significant speedup in running the experiments corresponding to the full dimensional data. }

% \subsection{Performance on  end tasks}\label{subsec:end_task_comparision} We compare the performance of \fsketch with baselines on the task of clustering and similarity search. We discuss them one-by-one as follows:

% \begin{table}[t]
    
% \caption{{%Speedup obtained on several end tasks while running them the sketch of dimension $1000$ obtained \textit{via} \texttt{FSketch} \textit{w.r.t.} running time on the full dimensional dataset.
% Speedup from running tasks on 1000-dimensional sketches instead of the full
%     dimensional dataset.
%     %For \textit{e.g.} speedup obtained in clustering= (Time is taken by clustering on full-dimensional data)/(Time is taken by clustering $1000$ dimension sketch obtained \textit{via} \texttt{FSketch}).
%     } }\label{tab:speed_up_task_time}
% %\vspace{-3mm}
%  \centering
% \addtolength\tabcolsep{-4pt}
%   \begin{tabular}{lccccr}
%     \toprule
%     Task&NYTimes&Enron&KOS&Gisette&DeliciousMIL\\
%     \midrule
% Clustering   &$117.25\times$&$16.99\times$&$2.83\times$&$2.82\times$&$4.124\times$\\
% Similarity Search&$106.98\times$ &$33.72\times$&$7.17\times$&$5.07\times$&$13.52\times$\\
% % Classification&\texttt{NA}&\texttt{NA}&\texttt{NA}&$1.252\times$&$2.62\times$\\
%  \bottomrule
%   \end{tabular}
%     %\vspace*{-3mm}
%   \addtolength\tabcolsep{4pt}
% \end{table}



\begin{table}[t]
{
\caption{{%Speedup obtained on several end tasks while running them the sketch of dimension $1000$ obtained \textit{via} \texttt{FSketch} \textit{w.r.t.} running time on the full dimensional dataset.
Speedup from running tasks on 1000-dimensional sketches instead of the full
    dimensional dataset. We got a DNS error while running clustering on the uncompressed BrainCell dataset. 
%For \textit{e.g.} speedup obtained in clustering= (Time is taken by clustering on full-dimensional data)/(Time is taken by clustering $1000$ dimension sketch obtained \textit{via} \texttt{FSketch}).
    } }\label{tab:speed_up_task_time}
%\vspace{-3mm}
 \centering
\resizebox{0.48\textwidth}{!}{%
\addtolength\tabcolsep{-4pt}
  \begin{tabular}{lccccccc}
    \toprule
    Task           & Brain cell      & NYTimes        &   Enron            &  NIPS         &  KOS          &   Gisette     &    DeliciousMIL\\
    \midrule
Clustering         &$NA   $&$ 139.64\times  $&$  21.15\times    $&$ 10.6\times  $&$ 3.93\times  $&$ 4.35\times  $&$  5.84\times  $\\
% Clustering         &$ 206.5\times   $&$ 139.64\times  $&$  21.15\times    $&$ 10.6\times  $&$ 3.93\times  $&$ 4.35\times  $&$  5.84\times  $\\
Similarity Search  &$    1231.6\times  $&$  118.12\times $&$ 48.15\times  $&$ 15.1\times  $&$ 10.56\times  $&$ 8.34\times  $&$ 17.76\times $\\
% Classification&\texttt{NA}&\texttt{NA}&\texttt{NA}&$1.252\times$&$2.62\times$\\
 \bottomrule
  \end{tabular}
    %\vspace*{-3mm}
  \addtolength\tabcolsep{4pt}
  }
  
  }
\end{table}

% \subsubsection{Clustering}
\subsection{Performance on clustering}
We compare the performance of \fsketch with baselines on the task of clustering and similarity search, and present the results for the first task in this section. The objective of the clustering experiment 
%First we present the results for the clustering experiment whose objective 
was to test if the data points in the reduced dimension maintain the original clustering structure. If they do, then it will be immensely helpful for those techniques that use a clustering, e.g., spam filtering. We used the {\it purity index} to measure the quality of $k$-mode and $k$-means clusters on the reduced datasets obtained through the compression algorithms; the ground truth was obtained using $k$-mode on the uncompressed data (for more details refer to Appendix~\ref{appendix:clustering}).

%The results are reported in Figure~\ref{fig:purity} where we readily observe that the quality obtained using \texttt{FSketch} is among the top, if not the best.
%We obtain similar results in our experiments for classification and similarity search. All the details are available in Appendix~\ref{sec:appendix_end_task}. 

%In the clustering experiment we used the {\it purity index} to compare 
%In clustering experiment,  the aim is to check if after dimensionality reduction the data points maintain the original clustering structure.  To do so we first evaluate the ground truth clustering on the original dataset using the classical $k$-mode algorithm~\cite{kmode} for several values of $k$. We then perform dimensionality reduction using baseline methods on various values of reduced dimension. We then compare the clustering quality on reduced dimension with the ground truth clustering results \textit{via} \textit{purity index}. 


We summarise our findings on quality in Figure~\ref{fig:purity}. \bl{ The compressed versions of the NIPS, Enron, and KOS  datasets that were obtained from \fsketch yielded the best purity index as compared to those obtained from the other baselines; for the other datasets the compressed versions from \fsketch are among the top. Even though it appears that KT offers comparable performance on the KOS, DeliciousMIL, and Gisette datasets \textit{w.r.t.}  \texttt{FSketch}, the downside of using KT is that its 
% a) it requires labeled datasets for compression whereas \texttt{FSketch} is completely unsupervised, and b) the
compression time is much higher than that of \texttt{FSketch} (see Table~\ref{tab:speed_up_dim_time}) on those datasets, and moreover it gives OOM/DNS error on the remaining datasets. Performance of FH also remains in the top few. However, its performance degrades on the NIPS dataset.}


We tabulate the speedup of clustering of \fsketch-compressed data over uncompressed data in Table~\ref{tab:speed_up_task_time} where we observe significant speedup in the clustering time, e.g.,  \bl{$139\times$} when run on a $1000$ dimensional \fsketch.

Recall that the dimensionality reduction time of our proposal is among the fastest among all the baselines which further reduces the total time to perform clustering by speeding up the dimensionality reduction phase. % Note that this speedup also  propagates in the total clustering  time -- sum of dimensionality reduction time and clustering time.
Thus the overall observation is that \fsketch appears to be the most suitable method for clustering among the current alternatives, especially, for high-dimensional datasets on which clustering would take a long time.

% where we observe significant
% % \paragraph{Insights}
% %We obtain significant 
% speedup in the clustering time along with very high purity index as compared to the baselines. For example, we observed a $117.25\times$ speedup while running it on a $1000$ dimensional \fsketch {\it vs.} running it on the full dimensional data (see Table~\ref{tab:speed_up_task_time}).  The purity index of the clustering results achieved on the compressed data obtained from \texttt{FSketch} is best for Enron and DeliciousMIL datasets and is among the top for the other datasets. Note that KT offers comparable performance on KOS, DeliciousMIL, and Gisette datasets \textit{w.r.t.}  \texttt{FSketch}. However the downside of using KT is that
% its 
% % a) it requires labeled datasets for compression whereas \texttt{FSketch} is completely unsupervised, and b) the
% compression time of KT is much higher than  \texttt{FSketch} (see Table~\ref{tab:speed_up_dim_time}) on KOS, DeliciousMIL, and Gisette datasets, and moreover it gives OOM/DNS error on the remaining datasets. 

% Further the dimensionality reduction time of our proposal is the fastest among all the baselines. Note that this speedup also  propagates in the total clustering  time -- sum of dimensionality reduction time and clustering time. 

% \paragraph{Insights:} The purity index of the clustering results  achieved on the reduced dimension data obtained from \texttt{FSketch} is among the top, if not the best \textit{w.r.t.} the clustering results obtained on the sketch obtained from the other baselines.  
% \textcolor{red}{add insight ..why KT is better..KT is supervised feature selection while ours is unsupervised and fast too}
%We obtain significant speedup in clustering time while running it on the low-dimensional sketch. For \textit{e.g.} we obtain $117.25\times$ speedup in clustering time while running it on $1000$ dimensional sketch obtained from \texttt{FSketch} \textit{w.r.t.} running it on full dimensional data. We summarise such speedup on other tasks as well and put them in Table~\ref{tab:speed_up_task_time}.




%\subsubsection{\texttt{Similarity Search:}}
\subsection{Performance on similarity search}
We take up another unsupervised task -- that of similarity search. The objective here is to show that after dimensionality reduction the similarities of points with respect to some query points are maintained. To do so, we randomly split the dataset in two parts $5\%$ and $95\%$  -- the smaller partition is referred to as the \textit{query partition} and each point of this partition is called a query vector; we call the larger partition as \textit{training partition}. For each query vector, we find top-$k$ similar points in the training partition. We then perform dimensionality reduction using all the methods (for various values of reduced dimensions). Next, we process the compressed dataset where, for each query point, we compute the  top-$k$ similar points in the corresponding low-dimensional version of the training points, by maintaining the same split. For each query point, we compute the accuracy of baselines by taking the Jaccard ratio between the set of top-$k$ similar points obtained in full dimensional data with  the top-$k$ similar points obtained in reduced dimensional dataset. We repeat this for all the points in the querying partition, compute the average, and report this as accuracy.%  This is repeated for all the baselines and for different values of reduced dimension. 

%{
%\paragraph{Insights} 
We summarise our findings in Figure~\ref{fig:similarity_search}. Note that PCA, MCA and LSA can reduce the data dimension up to the minimum of the number of data points and the original data dimension. Therefore their reduced dimension is at most $2000$ \bl{ for Brain cell dataset}.

The top few methods appear to be feature hashing (FH), Kendall-Tau (KT), \bl{HCA} along with \fsketch. However, KT give \textit{OOM} and \textit{DNS} on the Brain cell, NYTimes and Enron datasets, and \bl{HCA give DNS error on BrainCell and NYTimes datasets}.  Ffurther, their dimensionality reduction time are much worse than \texttt{FSketch} (see Table~\ref{tab:speed_up_dim_time}). 

% FH and \fsketch appear neck to neck for similarity search despite the fact that there is no known theoretical understanding of FH for Hamming distance --- in fact, it was included in the baselines as a heuristic because it offers discrete-valued sketches on which Hamming distance can be calculated. Nevertheless, FH and \fsketch perform equally well for similarity search and require similar times for dimensionality reduction making both of them the best approaches for this task. We want to bring to notice at this point that FH was not a consistent top-performer for clustering.

\bl{\fsketch outperforms FH on the BrainCell and the Enron datasets; however, on the remaining datasets, both of them 
 appear neck to neck for similarity search despite the fact that there is no known theoretical understanding of FH for Hamming distance --- in fact, it was included in the baselines as a heuristic because it offers discrete-valued sketches on which Hamming distance can be calculated. 
%  Nevertheless, FH and \fsketch perform equally well for similarity search and require similar times for dimensionality reduction making both of them the best approaches for this task. 
 Here want to point out that FH was not a consistent top-performer for clustering and similarity search.} 


The two other methods that are designed for Hamming distance, namely SSD and OHE, perform significantly  worse than FSketch; in fact, the accuracy of OHE lies almost to the bottom on all the four datasets.

% \textcolor{red}{Performance of SSD and OHE is highly worse than FSketch and FH. For all four datasets the top-k accuracy of OHE lies almost in the bottom.}
%Performance of FH is slightly worse than \texttt{FSketch}  and its dimensionality reduction time is comparable to \texttt{FSketch} on all the datasets. Note that FH is known to approximate Inner products for real-valued sketches. To the best of our knowledge, it is not known that the sketch obtained from FH can approximate Hamming distance for categorical data. We included it in the comparison as a heuristic because it offers discrete-valued sketches and Hamming distance can be defined on it. Again the performance of KT was among the top few on KOS and DeliciousMIL datasets, however its compression time is much higher than \fsketch (see Table~\ref{tab:speed_up_dim_time}). 

%To summarize, the accuracy of the similarity search task achieved on the sketch obtained from \texttt{FSketch} significantly outperforms \textit{w.r.t.} the corresponding results obtained on the sketches obtained from the other baselines. This indicates the \texttt{FSketch} accurately estimates the similarity between data points from the low-dimension.
% \paragraph{Insights:} The accuracy of the similarity search task achieved on the sketch obtained from \texttt{FSketch} significantly outperforms \textit{w.r.t.} the corresponding results obtained on the sketches obtained from the other baselines. This indicates the \texttt{FSketch} accurately estimates the similarity between data points from the low-dimension.  

% \textcolor{red}{elaborate more on insight FH is comparable with FSketch}


\bl{We also summarise the speedup of \fsketch-compressed data over uncompressed data, on similarity search task,  in Table~\ref{tab:speed_up_task_time}. We observe a significant speedup --  \textit{e.g.} \bl{$1231.6\times$} speedup on the BrainCell dataset when run on a   $1000$ dimensional \fsketch. }

To summarise, \fsketch is one of the best approaches towards similarity search for high-dimensional datasets and the best if we also require theoretical guarantees or applicability towards other data analytic tasks.




% \begin{comment}{
% \subsubsection{\texttt{Classification}:} In this task the aim is to check if after dimensionality reduction the data points maintain the classification boundary.  We first randomly split the datasets into training $(70\%)$ and test partition $(30\%)$. We train the classifier on the training partition and measure its performance on test performance. We perform dimensionality reduction using baseline methods on various values of reduced dimension. We then train  the classifier on the low-dimensional version of the same training partition and evaluate its performance on the low-dimensional version of the same test partition.   We then compare the classification quality on reduced dimension with the classification result on full dimensional dataset.  In this experiment, we include all the baselines for comparison. 

% \begin{figure*}[t]
% \centering
% % \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
% \includegraphics[width=\linewidth]{images/New_Classification_Logistic.pdf}
% %\vspace{-2mm}
% \caption{{Accuracy of the classifier trained on the reduced dimensional data obtained from various baselines. Black dotted line corresponds to the classification accuracy on the original dimension. \textbf{logistic regression}}}
% \label{fig:classification2}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% % \includegraphics[scale = 0.22]{images/different_p/RMSE.png}
% \includegraphics[width=\linewidth]{images/New_classification_knn.pdf}
% %\vspace{-2mm}
% \caption{{Accuracy of the classifier trained on the reduced dimensional data obtained from various baselines. Black dotted line corresponds to the classification accuracy on the original dimension. \textbf{KNN}}}
% \label{fig:classification2}
% \end{figure*}

 

% % \paragraph{Insights:} Here again, the classification accuracy on the reduced dimension data obtained from \texttt{FSketch}  is comparable with other baselines, and the classification result on the full dimensional dataset. Note that \texttt{FSketch} doesn’t account for the label information of the data points, and performs dimensionality reduction in completely unsupervised fashion. However, other feature selection based methods such as $\chi^2$ and Mutual Information feature selection take into account label information, and their correlation with features. 

% {
% \paragraph{Insights:} Here again, the classification accuracy on the reduced dimension data obtained from \texttt{FSketch}  is comparable with other baselines, and the classification result on the full dimensional dataset on Gisette dataset, whereas on DeliciousMIL dataset it is within $10\%$ \textit{w.r.t.} other baselines. Note that \texttt{FSketch} doesn’t account for the label information of the data points, and performs dimensionality reduction in completely unsupervised fashion. However, other feature selection based methods such as $\chi^2$ and Mutual Information feature selection take into account label information, and their correlation with features. Moreover, as mentioned earlier, our main advantage remains the significant speedup in  dimensionality reduction time \textit{w.r.t.} most of the baselines.  }

% {
% Apart from the high-quality output in clustering, similarity search, and classification on the compressed data obtained via \texttt{FSketch}, we simultaneously observe significant speedup while running these tasks on the sketches as compared to running them on the original vectors. For example: on NYTimes, the speed-up obtained on clustering and similarity search on the sketches are $117\times$ and $107\times$, respectively, as compared to running them on the full-dimensional datasets.  We summarise the speedups obtained during the evaluation on end tasks in Table~\ref{tab:speed_up_task_time}. We discuss the details required for reproducibility of the empirical evaluation on end tasks in Appendix~\ref{sec:Reproducibility_end_taks}.
% }




% % Apart from the high-quality  output in clustering, similarity search and classification, we also observe significant speedup in clustering of the low-dimensional compressed vectors { w.r.t.} clustering of the uncompressed vectors.
% % We summarise the speedups obtained during the evaluation on end tasks  in Table~\ref{tab:speed_up_task_time}. We discuss the  details required for reproducibility of the empirical evaluation on end task in Appendix~\ref{sec:Reproducibility_end_taks}.

% \textcolor{red}{explain insight on classification ..and add one more dataset}
% \end{comment}

 


