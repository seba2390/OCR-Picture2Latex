\section{Proofs from Section~\ref{subsec:analysis}}\label{appendix:subsec:analysis}

%\expectationlemma*
%
%\begin{proof} Fix a mapping $\rho$ and then define $F_j(x)$ as the vector $[x_{i_1}, x_{i_2}, \ldots ~:~ i_k \in \{1, \ldots n\}]$ of values of $x$ that are mapped to $j$ in $\phi(x)$ in the increasing order of their coordinates, i.e., $\rho(i_k)=j$ and $i_1 < \ldots i_k < i_{k+1}$. Since $\rho$ is fixed, $F_j(y)$ is also a vector of the same length. The key observation is that if $F_j(x)=F_j(y)$ then $\phi_j(x)=\phi_j(y)$; the converse is not always true.
%
%It is given that $x$ and $y$ differ at $h$ coordinates. Therefore, $F_j(x)\not= F_j(y)$ iff any of those coordinates are mapped to $j$ by $\rho$. Thus, 
%\begin{align*}
%    \Pr_\rho[F_j(x)=F_j(y)] = (1-\tfrac{1}{d})^h. \numberthis \label{eq:prob_pi}
%\end{align*}
%
%
%Next we analyse the chance of $\phi_j(x)=\phi_j(y)$ when $F_j(x)\not=F_j(y)$. Note that $\phi_j(x)=(x_{i_1} \cdot r_{i_1} + x_{i_2} \cdot r_{i_2} + \ldots) \mod{p}$ (and a similar expression exists for $y$), where $r_i$s are randomly chosen during initialization (they are fixed for $x$ and $y$). Using a similar analysis as that in the Frievald's algorithm~\cite[Ch~1(Verifying matrix multiplication)]{upfal},
%\begin{align*}
%\Pr_{\rho,R}[\phi_j(x)=\phi_j(y) ~|~ F_j(x)\not=F_j(y)] = \tfrac{1}{p}. \numberthis \label{eq:prob_pi_r}   
%\end{align*}
%
%Due to Equations~\ref{eq:prob_pi}, \ref{eq:prob_pi_r}, we have
%\begin{align*}
%& \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y)] \\
%= & \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y) ~|~ F_j(x)\not=F_j(y)] \cdot \Pr_{\rho,R} [F_j(x)\not=F_j(y)] \\
%+ & \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y) ~|~ F_j(x)=F_j(y)] \cdot \Pr_{\rho,R} [F_j(x)=F_j(y)]\\
%= & (1-\tfrac{1}{p})(1-(1-\tfrac{1}{d})^h).
%\end{align*}
%The lemma follows from the linearity of expectation.
%\end{proof}



\concentrationlemma*

\begin{proof}
Fix any $R$ and $x,y$; the rest of the proof applies to any $R$, and therefore, holds for a random $R$ as well. Define a vector $z \in \{0,\pm 1,\ldots,\pm c\}^n$ in which $z_i=(x_i - y_i)$; the number of non-zero entries of $z$ are at most $2\sigma$ since the number of non-zero entries of $x$ and $y$ are at most $\sigma$. Let $J_0$ be the set of coordinates from $\{1, \ldots, n\}$ at which $z$ is 0, and let $J_1$ be the set of the rest of the coordinates; from above, $J_1 \le 2\sigma$.% We will prove the lemma for $d=\tfrac{2}{\epsilon}\sqrt{\sigma \ln \tfrac{2}{\delta}}$ which automatically proves it for larger values of $d$.

Define the event $E_j$ as ``$[\phi_j(x)\not=\phi_j(y)]$''. Note that $f$ can be written as a sum of indicator random variables, $\sum_j I(E_j)$, and we would like to prove that $f$ is almost always close to $f^*=\E[f]$.

Observe that $\phi_j(x)=\phi_j(y)$ iff $\sum_{i \in \rho^{-1}(j)} z_i \cdot r_i = 0 \mod{p}$ iff $\sum_{i \in \rho^{-1}(j) \cap J_1} z_i \cdot r_i = 0 \mod{p}$. In other words, $\rho(i)$ could be set to anything for $i \in J_0$ without any effect on the event $E_j$; hence, we will assume that the mapping $\rho$ is defined as a random mapping only for $i \in J_1$, and further for the ease of analysis, we will denote them as $\rho(i_1), \rho(i_2), \ldots, \rho(i_{2\sigma})$ (if $|J_1| < 2\sigma$ then move a few coordinates from $J_0$ to $J_1$ without any loss of correctness).

To prove the concentration bound we will employ martingales. Consider the sequence of these random variables $\rho'=\rho(i_1), \rho(i_2), \ldots, \rho(i_{2\sigma})$ -- these are independent. Define a function $g(\rho')$ of these random variables as a sum of indicator random variables as stated below (note that $R$ and $\rho(i)$, for $i \in J_0$, are fixed at this point)
\begin{align*}
    & g(\rho(i_1), \rho(i_2), \ldots \rho(i_{2\sigma}))\\
    & = \sum_j I\left( \sum_{i \in \rho^{-1}(j) \cap J_1} z_i \cdot r_i \not= 0 \mod{p} \right) \\
    & = \sum_j I(E_j) = f
\end{align*}
 

Now consider an arbitrary $t\in \{1, \ldots, 2\sigma\}$ and let $q=\rho(i_t)$; observe that $z_{i_t}$ influences only $E_q$. Choose an arbitrary value $q' \in \{1, \ldots, d\}$ that is different from $q$. Observe that, if $\rho$ is modified only by setting $\rho(i_t)=q'$ then we claim that ``bounded difference holds''.

\begin{prop}
$|~ g(\rho(i_1), \ldots, \rho(i_{t-1}), q, \ldots, \rho(i_{2\sigma})) - g(\rho(i_1), \ldots, \rho(i_{t-1}), q', \ldots, \rho(i_{2\sigma})) ~| \le 2$.
\end{prop}
The proposition holds since the only effects of the change of $\rho(i_t)$ from $q$ to $q'$ are seen in $E_q$ and $E_{q'}$ (earlier $E_q$ depended upon $z_{i_t}$ that now changes to $E_{q'}$ being depended upon $z_{i_t}$). Since $g()$ obeys bounded difference, therefore, we can apply McDiarmid's inequality~\cite[Ch~17]{upfal},~\cite{mcdiarmid_1989}.
% McDiarmid's inequality~\cite[Ch 17]{upfal},~\cite{mcdiarmid_1989} states that
\begin{thm}[McDiarmid's inequality\label{thm:McDiarmid}]
Consider independent random variables $X_1,\ldots , X_m \in \mathcal{X}$, and a mapping $ f:\mathcal{X}^m \rightarrow \R$ which   for all $i$ and for all $x_1,\ldots x_m, {x_i}'$ satisfies the property:
$|f(x_1,\ldots,x_i,\ldots,x_m)-f(x_1,\ldots,{x_i}',\ldots,x_m)|\leq c_i$, where $x_1,\ldots x_m, {x_i}'$ are possible values for the input variables of the function $f$. Then,
% \[
% \Pr\big[ \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)  \geq \varepsilon \big]\leq \exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right),
% \]
    \begin{align*}
	\Pr\Big[ \big| \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)]\big|  \geq \varepsilon \Big] \\
	\leq 2\exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right).
    \end{align*}
\end{thm}

% ~\footnote{McDiarmid's inequality~\cite[Ch 17]{upfal},~\cite{mcdiarmid_1989} states that
% \begin{thm}
% Consider independent random variables $X_1,\ldots , X_m \in \mathcal{X}$, and a mapping $ f:\mathcal{X}^m \rightarrow \R$ which   for all $i$ and for all $x_1,\ldots x_m, {x_i}'$ satisfies the property:
% $|f(x_1,\ldots,x_i,\ldots,x_m)-f(x_1,\ldots,{x_i}',\ldots,x_m)|\leq c_i$, where $x_1,\ldots x_m, {x_i}'$ are possible values for the input variables of the function $f$. Then,
% % \[
% % \Pr\big[ \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)  \geq \varepsilon \big]\leq \exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right),
% % \]
% \[
% \Pr\Big[ \big| \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)\big|  \geq \varepsilon \Big]\leq 2\exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right).
% \]
% \end{thm}
% }
This inequality implies that, for every $x,y,R$,
$$\Pr_{\rho} \Big[ \big| \E[f] - f \big| \ge \alpha \Big] \le 2\exp\left( -\frac{2\alpha^2}{(2\sigma)2^2} \right) = \exp \left( - \frac{\alpha^2}{4\sigma} \right).$$
Hence, the lemma is proved.
\end{proof}

\upperboundf*

\begin{proof}
    Since $f^*=dP(1-D^h)=dP - dPD^h$, if $f \ge dP$ then $|f-f^*| \ge dPD^h$. 
    \begin{align*}
	\Pr[f \ge dP] & \le \Pr[|f-f^*| \ge dPD^h]\\
		      & \le 2 \exp(-\tfrac{d^2 P^2 D^{2h}}{4\sigma}) \tag{using Lemma~\ref{lem:concentration}}\\
		      & = 2 \exp(-\tfrac{P^2}{4\sigma} d^2(1-\tfrac{1}{d})^{2h})\\
		      & \le 2 \exp(-\tfrac{P^2}{4\sigma} (d-h)^2) \tag{$\because~(1-\tfrac{1}{d})^h \ge 1-\tfrac{h}{d}$}\\
		      & \le 2 \exp(-P^2\sigma) \tag{$\because$ $\tfrac{(d-h)^2}{4\sigma} \ge \sigma$}
    \end{align*}
    Here we have used the fact that $h \le 2\sigma$ which, along with the setting $d=4\sigma$, implies that $(d-h) \ge 2\sigma$.
\end{proof}

\hconcentrationlemma*

\begin{proof}
    Denote $|\hat{h}-h|$ by $\Delta h$ and let $\alpha=\sqrt{d \ln
    \tfrac{2}{\delta}}$. We will prove that $\Delta h < \tfrac{32}{P}\sqrt{\sigma \ln \tfrac{2}{\delta}}$ for the case $|f-f^*| \le \alpha$ which, by
    Lemma~\ref{lem:concentration}, happens
    with probability at least $(1-2\exp{(-\tfrac{\alpha^2}{4\sigma})})=1-\delta$.

    First we make a few technical observations all of which are based on
    standard inequalities of binomial series and logarithmic functions.
    It will be helpful to remember that $D=1-1/d \in (0,1)$.
    \begin{obs}\label{obs:primep}
	For reasonable values of $\sigma$, and
	reasonable values of $\delta$, almost all
	primes satisfy the bound $P \ge \tfrac{4}{\sqrt{\sigma}}\sqrt{\ln
	\tfrac{2}{\delta}}$. We will assume this inequality to hold without loss
	of generality~\footnote{If the reader is wondering why we are not
	proving this fact, it may be observed that this relationship does not
	hold for small values of $\sigma$, e.g., $\sigma=16, \delta=0.01$.}.
    \end{obs}
    For example, $p=2$ is sufficient for $\sigma \approx 1000$ and $\delta
    \approx 0.001$ (remember that $P=1-\tfrac{1}{p}$). Furthermore, observe that
    $P$ is an increasing function of $p$, and the right hand side is a
    decreasing function of $\sigma$, increasing with decreasing delta but at an
    extremely slow logarithmic rate.
    
    \begin{obs}\label{obs:dp-alpha}
	$\tfrac{dP}{\alpha} > 4$ can be assumed without loss of generality. This holds since the left hand side is
	$\tfrac{dP}{\sqrt{d}\sqrt{ln(2/\delta)}} =
	\tfrac{P\sqrt{d}}{\sqrt{\ln(2/\delta)}} \ge
	\tfrac{4\sqrt{d}}{\sqrt{\sigma}}$ (by Observation~\ref{obs:primep})
	which is at least $4$.
    \end{obs}

    \begin{obs}\label{obs:f_less_than_dP}
	Based on the above assumptions, $f < dP$.
    \end{obs}
    \begin{proof}[Proof of Observation] We will prove that $\sqrt{d \ln \tfrac{2}{\delta}} < dPD^h$. Since $|f-f^*| \le \sqrt{d \ln \tfrac{2}{\delta}}$ and $f^*=dP(1-D^h)$, it follows that $f \le f^* + \sqrt{d \ln \tfrac{2}{\delta}} < dP$.

    \begin{align*}
	\sqrt{d}PD^h & = \frac{dPD^h}{\sqrt{d}} \ge \frac{P}{\sqrt{d}}d(1-\tfrac{1}{d})^h \ge \frac{P}{\sqrt{d}}d(1-\tfrac{h}{d})\\
		     & = \frac{P}{\sqrt{d}}(d-h) \ge \frac{P}{\sqrt{d}}\frac{d}{2} \tag{$\because$ $h \le 2\sigma$, $d-h \ge 2\sigma = \tfrac{d}{2}$}\\
		     & = P \sqrt{\sigma} \ge 4\sqrt{\ln \tfrac{2}{\delta}} \tag{Observation~\ref{obs:primep}}
    \end{align*}
    which proves the claim stated at the beginning of the proof.
    \end{proof}

    Based on this observation, $\hat{h}$ is calculated as $\ln\left( 1 - \tfrac{f}{dP} \right)/\ln D$ (see Definition~\ref{defn:estimator}). 
    Thus, we get $D^{\hat{h}} = 1 - \tfrac{f}{dP}$.
    Further, from Equation~\ref{eqn:fstar} we get $D^h = 1-\tfrac{f^*}{dP}$.
    

    \begin{obs}\label{obs:appendix-1} $D^h \ge D^{2\sigma} \ge \tfrac{9}{16}$. This is since $h \le 2\sigma$ and 
	$D^\sigma = (1-\tfrac{1}{d})^\sigma \ge 1 - \tfrac{\sigma}{d} =
	\tfrac{3}{4}$.
    \end{obs}

    \begin{restatable}{obs}{obsappendix}\label{obs:appendix-2}
	$D^{\hat{h}} > \tfrac{5}{16}$.
    \end{restatable}
	This is not so straight forward as
	Observation~\ref{obs:appendix-1} since $\hat{h}$ is calculated using a
	formula and is not guaranteed, ab initio, to be upper bounded by
	$2\sigma$.
	\begin{proof}[Proof of Observation]
	    We will prove that $\tfrac{f}{dP} < \tfrac{11}{16}$ which will imply that $D^{\hat{h}} = 1 - \tfrac{f}{dP} > \tfrac{5}{16}$.
	   
	    For the proof of the lemma we have considered the case that $f \le f^* + \alpha$. 
	    Therefore, $\tfrac{f}{dP} \le \tfrac{f^*}{dP} + \tfrac{\alpha}{dP}$.
	    Substituting the value of $f^*=dP(1-D^h)$ from Equation~\ref{eqn:fstar} and
	    using Observation~\ref{obs:appendix-1} we get the bound $\tfrac{f}{dP} \le
	    \tfrac{7}{16} + \tfrac{\alpha}{dP}$. We can further simplify the bound using Observation~\ref{obs:dp-alpha}:
    $$\tfrac{f}{dP} \le \tfrac{7}{16} + \tfrac{\alpha}{dP} \le \tfrac{7}{16} +
    \tfrac{1}{4} < \tfrac{11}{16}, \mbox{ validating the observation.}$$

	\end{proof}

    Now we get into the main proof which proceeds by considering two possible
    cases.
    
    {\bf (Case $\hat{h} \ge h$, i.e., $\Delta h=\hat{h}-h$:)} 
    We start with the identity $D^h - D^{\hat{h}} = \tfrac{f-f^*}{dP}$.

    Notice that the RHS is bounded from the above by $\tfrac{\alpha}{dP}$ and
    the LHS can bounded from the below as
    $$D^h - D^{\hat{h}} = D^h(1-D^{\Delta h}) > \tfrac{9}{16}(1-D^{\Delta h})$$
    where we have used Observation~\ref{obs:appendix-1}. Combining these facts
    we get $\tfrac{\alpha}{dP} > \tfrac{9}{16}(1-D^{\Delta h})$.

    {\bf (Case $h \ge \hat{h}$, i.e., $\Delta h = h - \hat{h}$:)} In a similar
    manner, we start with the identity $D^{\hat{h}} - D^h = \tfrac{f^* -
    f}{dP}$ in which the RHS we bound again from the above by
    $\tfrac{\alpha}{dP}$ and the LHS is treated similarly (but now using
    Observation~\ref{obs:appendix-2}).
    $$D^{\hat{h}} - D^h = D^{\hat{h}}(1-D^{\Delta h}) > \tfrac{5}{16}
    (1-D^{\Delta h})$$ and then, $\tfrac{\alpha}{dP} >
    \tfrac{5}{16}(1-D^{\Delta h})$.

    So in both the cases we show that $\tfrac{\alpha}{dP} >
    \tfrac{5}{16}(1-D^{\Delta h})$. Our desired bound on $\Delta h$ can now be
    obtained.
    \begin{align*}
	\Delta h \ln D & \ge \ln\left( 1 - \tfrac{16}{5}\tfrac{\alpha}{dP} \right)
			 \ge -\tfrac{16\alpha}{5dP}/(1-\tfrac{16\alpha}{5dP}) =
	-\tfrac{16\alpha}{5dP-16\alpha}\\
	& \mbox{ (using the inequality $\ln(1+x) \ge
	\tfrac{x}{1+x}$ for $x > -1$)} \\
	\therefore \Delta h & \le \frac{1}{\ln\tfrac{1}{D}} \frac{16\alpha}{5dP -
	16\alpha} \le \frac{16\alpha d}{5dP - 16\alpha}\\
	& \mbox{ (it is easy to show that $\ln \tfrac{1}{D} =
	\ln\tfrac{1}{1-1/d} \ge 1/d$)}\\
	& = \frac{\tfrac{16}{5}d}{\tfrac{dP}{\alpha} - \tfrac{16}{5}} \\
	& < \frac{\tfrac{16}{5}d}{\tfrac{dP}{5\alpha}} 
	\mbox{ (using Observation~\ref{obs:dp-alpha}, $\tfrac{dP}{\alpha}-\tfrac{16}{5} >\tfrac{dP}{5\alpha}$)}\\
	& = \frac{16\alpha}{P} = \frac{16}{P}\sqrt{d \ln \tfrac{2}{\delta}} =
	\frac{32}{P}\sqrt{\sigma \ln \tfrac{2}{\delta}}
    \end{align*}
\end{proof}


\hconcentrationlemmatight*

\begin{proof}[Proof of (a) $f < dP$ with high probability]
    Following the steps of the proof of Lemma~\ref{lem:upper-bound-f},
    \begin{align*}
	\Pr[f \ge dP] & \le 2\exp(-\tfrac{d^2 P^2 D^{2h}}{4\sigma}) \\
		      & \le 2\exp(-P^2\tfrac{(d-h)^2}{4\sigma})
    \end{align*}
    Let $L$ denote $\sqrt{\ln\tfrac{2}{\delta}}$; note that $L > 1$. Now, $d=16L\sqrt{\sigma}$ and $h \le \sqrt{\sigma}$. So, $d-h \ge 15L\sqrt{\sigma} > 15\sqrt{\sigma}$ and, therefore, $\tfrac{(d-h)^2}{\sigma} > 225$. Using this bound in the equation above, we can upper bound the right-hand side as $2\exp(-225(1-\tfrac{1}{p})^2/4)$ which is a decreasing function of $p$, the lowest (for $p=2$) being $2\exp(-225/4*4) \approx 10^{-6}$.
\end{proof}

\begin{proof}[Proof of (b) a better estimator of $h$]
    The proof is almost exactly same as that of
    Lemma~\ref{lem:hconcentrationlemma}, with only a few differences. We set
    $\alpha=d/8$ where $d=16\sqrt{\sigma \ln \tfrac{2}{\delta}}$. Incidentally,
    the value of $\alpha$ remains the same in terms of $\sigma$ ( $\alpha=\sqrt{4\sigma
    \ln \tfrac{2}{\delta}}$). Thus, the
    probability of error remains same as before;
    $$2 \exp{(-\tfrac{d^2}{64 \cdot 4 \sigma})} = \delta.$$

    Observation~\ref{obs:primep} is true without any doubt.
    $\tfrac{dP}{\alpha} = 8P$ which is greater than 4 for any prime number; so
    Observation~\ref{obs:dp-alpha} is true in this scenario.

    Observation~\ref{obs:f_less_than_dP} requires a new proof. Following the steps of the above proof of Observation~\ref{obs:f_less_than_dP}, it suffices to prove that $dPD^h > \tfrac{d}{8}$.
    \begin{align*}
	PD^h & = P(1-\tfrac{1}{d})^h \ge P(1-\tfrac{h}{d}) \\
	     & =P(\tfrac{d-h}{d}) \ge P\tfrac{15L\sqrt{\sigma}}{16L\sqrt{\sigma}} = P\tfrac{15}{16} > \tfrac{1}{2}\tfrac{15}{16} > \tfrac{1}{8}
    \end{align*}

    Observation~\ref{obs:appendix-1} is now tighter since $D^h \ge D^{\sqrt{\sigma}}
    = (1-\tfrac{1}{d})^{\sqrt{\sigma}} \ge 1-\tfrac{\sqrt{\sigma}}{d} = 1 -
    \tfrac{1}{16\sqrt{\ln 2/\delta}} \ge \tfrac{3}{4}$ for reasonable values of
    $\delta$.
    Similarly Observation~\ref{obs:appendix-2} is also tighter (it relies on only the above
    observations) since $\tfrac{f^*}{dP} = 1 - D^h \le 1-\tfrac{3}{4}$ and $\tfrac{\alpha}{dP} < \tfrac{1}{4}$; we get $D^{\hat{h}} > \tfrac{1}{2}$.

    Following similar steps as above, for the case $\hat{h} \ge h$, we get $\tfrac{\alpha}{dP} > \tfrac{3}{4}(1-D^{\Delta h})$ and for the case $\hat{h} < h$, we get $\tfrac{\alpha}{dP} > \tfrac{1}{2}(1-D^{\Delta h})$ leading to the common condition that $\tfrac{\alpha}{dP} > \tfrac{1}{2}(1-D^{\Delta h})$.

    The final thing to calculate is the bound on $\Delta h$.
    \begin{align*}
	\Delta h \ln D & \ge \ln\left( 1 - \tfrac{2\alpha}{dP} \right)
			 \ge -\tfrac{2\alpha}{dP}/(1-\tfrac{2\alpha}{dP}) =
	-\tfrac{2\alpha}{dP-2\alpha}\\
	& \mbox{ (using the inequality $\ln(1+x) \ge
	\tfrac{x}{1+x}$ for $x > -1$)} \\
	\therefore \Delta h & \le \frac{1}{\ln\tfrac{1}{D}} \frac{2\alpha}{dP -
	2\alpha} \le \frac{2\alpha d}{dP - 2\alpha}\\
	& \mbox{ (it is easy to show that $\ln \tfrac{1}{D} =
	\ln\tfrac{1}{1-1/d} \ge 1/d$)}\\
	& = \frac{2d}{\tfrac{dP}{\alpha} - 2} \\
	& < \frac{2d}{\tfrac{dP}{2\alpha}} 
	\mbox{ (using Observation~\ref{obs:dp-alpha}, $\tfrac{dP}{\alpha}-2 >\tfrac{dP}{2\alpha}$)}\\
	& = \frac{4\alpha}{P} = \frac{4}{P}\sqrt{4\sigma \ln \tfrac{2}{\delta}} =
	\frac{8}{P}\sqrt{\sigma \ln \tfrac{2}{\delta}}
    \end{align*}
\end{proof}

\section{Complexity analysis of \fsketch}\label{subsec:appendix-complexity}
There
are two major operations with respect to \fsketch --- construction of sketches
and estimation of Hamming distance from two sketches. We will discuss their time and space requirements. There are efficient representations of sparse data vectors, but for the sake of simplicity we assume full-size arrays to store vectors; similarly we assume simple dictionaries for storing the interval variables $\rho,R$ by \fsketch. While it may be possible to reduce the number of random bits by employing $k$-wise independent bits and mappings, we left it out of the scope of this work and for future exploration.
\begin{enumerate}
    \item{Construction:} Sketches are constructed by the \fsketch algorithm which does a linear pass over the input vector, maps every non-zero attribute to some entry of the sketch vector and then updates that corresponding entry. The time to process one data vector becomes $\Theta(n) + O(\sigma \cdot poly(\log p))$ which is $O(n)$ for constant $p$. The interval variables, $\rho,R,p$, require space $\Theta(n \log d)$, $\Theta(n \log p)$ and $\Theta(\log p)$, respectively, which is almost $O(n)$ if $\sigma \ll n$. Furthermore, $\rho$ and $R$, that can consume bulk of this space, can be freed once the sketch construction phase is over. A sketch itself consumes $\Theta(d \log p)$ space.
    \item{Estimation:} There is no additional space requirement for estimating the Hamming distance of a pair of points from their sketches. The estimator scans both the sketches and computes their Hamming distance; finally it computes an estimate by using Definition~\ref{defn:estimator}. The running time is $O(d \log p)$.
\end{enumerate}



\section{Proofs from Section~\ref{subsec:complexity}}\label{appendix:subsec:complexity}

\sparsitylemma*

\begin{proof}
The lemma can be proved by treating it as a balls-and-bins problem. Imagine
    throwing $\sigma$ balls (treat them as the non-zero attributes of $x$) into
    $d$ bins (treat them as the sketch cells) independently and uniformly at
    random. If the $j$th-bin remains empty then $\phi_j(x)$ must be zero (the
    converse is not true). Therefore, the expected number of non-zero cells in the
    sketch is upper bounded by the expected number of empty bins, which can be
    easily shown to be $d[1-(1-\tfrac{1}{d})^\sigma]$. Using the stated value of
    $d$, this expression can further be upper bounded.
    $$d[1-(1-\tfrac{1}{d})^\sigma] \le d[1-(1-\tfrac{\sigma}{d})] =
    \tfrac{d}{4}$$
    Furthermore, let $NZ$ denote the number of non-zero entries in $\phi(x)$. We
    derived above $\E[NZ] \le \tfrac{d}{4}$. Markov inequality can help in upper
    bounding the probability that $\phi(x)$ contains many non-zero entries.
    $$\Pr[NZ \ge \tfrac{d}{2}] \le \E[NZ]/\tfrac{d}{2} \le \tfrac{1}{2}$$
\end{proof}

