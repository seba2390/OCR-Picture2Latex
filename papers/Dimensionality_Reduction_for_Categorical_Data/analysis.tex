\begin{table}[h]
    \centering
    \caption{Notations\label{tab:notations}}    
    %\vspace{-2mm}
    % \resizebox{\columnwidth}{!}{%
\noindent\begin{tabular}{lc}
    \hline
    categorical data vectors & $x,y$ \\
    their Hamming distance & $h$ \\
    \hline
    compressed categorical vectors (sketches) & $\phi(x), \phi(y)$ \\
    $j$-th bit of a sketch $\phi(x)$ & $\phi_j(x)$ \\
    observed Hamming distance between sketches & $f$ \\
    \hline
    expected Hamming distance between sketches & $f^*$\\
    estimated Hamming distance between data vectors & $\hat{h}$\\
    %\hline
    %$(1-\tfrac{1}{d})$ & $D$ & $(1-\tfrac{1}{p})$ & $P$ \\
    \hline
    \end{tabular}
    %\vspace{-3mm}
    % }
\end{table}






%%% ANALYSIS SECTION %%%%%%%%%
\section{Category sketching and Hamming distance estimation}\label{sec:analysis}



Our technical objective is to design an effective algorithm to compress high-dimensional vectors over $\{0,1,\ldots, c\}$ to integer vectors of a low dimension, {\it aka.} sketches;  \bl{$c$ can even be set to an upper bound on the largest number of categories among all the features}. The number of attributes in the input vectors is denoted $n$ and the dimension of the compressed vector is denoted $d$. We will later show how to choose $d$ depending on the sparsity of a dataset that we denote $\sigma$. The commonly used notations in this section are listed in Table~\ref{tab:notations}.

% \begin{table}[]
%     \centering
%     \caption{Notations\label{tab:notations}}    
%     %\vspace{-2mm}
% \noindent\begin{tabular}{lc||lc}
%     \hline
%     categorical data vectors & $x,y$ & compressed categorical vectors (sketches) & $\phi(x), \phi(y)$ \\
%     \hline
%     their Hamming distance & $h$ & observed Hamming distance between sketches & $f$ \\
%     \hline
%     estimated Hamming distance & $\hat{h}$     & expected Hamming distance between sketches & $f^*$\\
%     %\hline
%     %$(1-\tfrac{1}{d})$ & $D$ & $(1-\tfrac{1}{p})$ & $P$ \\
%     \hline
%     \end{tabular}
%     %\vspace{-3mm}
% \end{table}

    \begin{algorithm}
    %\textbf{Initialization:} 
    \begin{algorithmic}[1]
    \Procedure{Initialize}{}
        \State Choose random mapping $\rho:\{1, \ldots n\} \to \{1, \ldots d\}$
        \State Choose some prime $p$
        \State Choose $n$ random numbers $R=r_1, \ldots, r_n$ with each $r_i \in \{0, \ldots p-1\}$
    \EndProcedure
    \end{algorithmic}
	%\textbf{Input:} input $x \in \{0,1,\ldots c\}^n$ 
	\begin{algorithmic}[1]
	\Procedure{CreateSketch}{$x \in \{0,1,\ldots c\}^n$}
        \State Create empty sketch $\phi(x) = 0^d$
        \For{$i=1 \ldots n$}
            \State $j=\rho(i)$
            \State $\phi_j(x) = (\phi_j(x) + x_i \cdot r_i) \mod{p}$
        \EndFor
        \State \Return $\phi(x)$
	\EndProcedure
	\end{algorithmic}
	\caption{Constructing $d$-dimensional \fsketch of $n$-dimensional vector $x$\label{algo:fsketchalgo}}
    \end{algorithm}
    
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/example.pdf}
    \caption{\bl{An example illustrating how to compress a data point with categorical features using \fsketch to a 3-dimensional integer vector. The data point has 10 feature values, each of which is a categorical variable (the corresponding label encoded values are present inside the brackets). $c$ is chosen as 195 since the fifth, sixth, seventh, and eighth features have 195 categories which is the largest. $\rho, p$ and $R$ are internal variables of \fsketch.}}
    \label{fig:example}
\end{figure}

\subsection{\fsketch construction}
Our primary tool for sketching categorical data is a randomised sketching algorithm named \fsketch that is described in Algorithm~\ref{algo:fsketchalgo}; see Figure~\ref{fig:example} for an example.

Let  $x \in \{0,1,\ldots c\}^n$ denote the input vector, and the $i$-th feature or co-ordinate of $x$ is denoted by $x_i$. The sketch of input vector $x$ will be denoted $\phi(x) \in \{0, 1, \ldots p-1\}^d$ whose coordinates will be denoted $\phi_1(x), \phi_2(x), \ldots, \phi_d(x)$. Note that the initialisation step of \fsketch needs to run only once for a dataset. We are going to use the following characterisation of the sketches in the rest of this section; \bl{a careful reader may observe the similarity to Freivald's algorithm for verifying matrix multiplication~\cite{Freivalds1977ProbabilisticMC}.}

\begin{obs}\label{obs:formula}
It is obvious from Algorithm~\ref{algo:fsketchalgo} that the sketches created by \fsketch satisfy $\phi_j(x) = (\sum_{i \in \rho^{-1}(j)} x_i \cdot r_i) \mod{p}$.
\end{obs}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/hamming_distribution.png}
    \caption{The distributions of Hamming distances for some of the datasets used in our experiments are shown in blue --- the Y-axis shows the frequency of each distance. The black points represent the actual Hamming distances and the red points are the estimates, i.e., a red-point plotted against a Hamming distance $d$ (on the X-axis) shows the estimated Hamming distance between two points with actual Hamming distance $d$. Observe that the Hamming distances follow a long-tailed distribution and that most distances are fairly low --- moreover, our estimates are more accurate for those high frequent Hamming distances.
    \label{fig:hamming_distance_distrib}}
\end{figure*}

\subsection{Hamming distance estimation}
Here we explain how the Hamming distance between $x$ and $y$ denoted $HD(x,y)$, percolates to their sketches as well. \bl{The objective is derive an estimator for $HD(x,y)$ from the Hamming distance between $\phi(x)$ and $\phi(y)$.}

The sparsity of a set of vectors denoted $\sigma$, is the maximum number of non-zero coordinates in them. For the theoretical analysis, we assume that we know the sparsity of the dataset, or at least an upper bound of the same. Note that, for a pair of sparse vectors $x, y \in \{0, 1, \ldots, c\}^n$, the  Hamming distance between them can vary from $0$ (when they are same) to $2\sigma$ (when they are completely different).

We first prove case (a) of Theorem~\ref{thm:main-intro} which states that sketches of different vectors are rarely the same.

\begin{lem}\label{lem:expectation-previous}
Let $h$ denote $HD(x,y)$ for two input vectors $x,y$ to \fsketch. Then
\begin{align*}
\Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y)] &= (1-\tfrac{1}{p})(1-(1-\tfrac{1}{d})^h).
\end{align*}
\end{lem}


%The proof of the lemma uses an analysis similar to Frievald's algorithm forverifying matrix multiplication\cite[Ch~1]{upfal} and is given in Appendix~\ref{appendix:proofs}.

\begin{proof} %The proof of the lemma uses an analysis similar to Frievald's algorithm for verifying matrix multiplication\cite[Ch~1]{upfal}. 
    Fix a mapping $\rho$ and then define $F_j(x)$ as the vector $[x_{i_1}, x_{i_2}, \ldots ~:~ i_k \in \{1, \ldots n\}]$ of values of $x$ that are mapped to $j$ in $\phi(x)$ in the increasing order of their coordinates, i.e., $\rho(i_k)=j$ and $i_1 < \ldots i_k < i_{k+1}$. Since $\rho$ is fixed, $F_j(y)$ is also a vector of the same length. The key observation is that if $F_j(x)=F_j(y)$ then $\phi_j(x)=\phi_j(y)$ but the converse is not always true. Therefore we separately analyse both the conditions (a) $F_j(x)\not=F_j(y)$ and (b) $F_j(x)=F_j(y)$.

It is given that $x$ and $y$ differ at $h$ coordinates. Therefore, $F_j(x)\not= F_j(y)$ iff any of those coordinates are mapped to $j$ by $\rho$. Thus, 
\begin{align*}
    \Pr_\rho[F_j(x)=F_j(y)] = (1-\tfrac{1}{d})^h. \numberthis \label{eq:prob_pi}
\end{align*}


Next we analyse the chance of $\phi_j(x)=\phi_j(y)$ when $F_j(x)\not=F_j(y)$. Note that $\phi_j(x)=(x_{i_1} \cdot r_{i_1} + x_{i_2} \cdot r_{i_2} + \ldots) \mod{p}$ (and a similar expression exists for $y$), where $r_i$s are randomly chosen during initialisation (they are fixed for $x$ and $y$). Using a similar analysis as that in the Freivald's algorithm~\cite[Ch~1(Verifying matrix multiplication)]{upfal},
\begin{align*}
\Pr_{\rho,R}[\phi_j(x)=\phi_j(y) ~|~ F_j(x)\not=F_j(y)] = \tfrac{1}{p}. \numberthis \label{eq:prob_pi_r}   
\end{align*}

Due to Equations~\ref{eq:prob_pi}, \ref{eq:prob_pi_r}, we have
\begin{align*}
& \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y)] \\
= & \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y) ~|~ F_j(x)\not=F_j(y)] \cdot \Pr_{\rho,R} [F_j(x)\not=F_j(y)] \\
+ & \Pr_{\rho,R} [\phi_j(x)\not=\phi_j(y) ~|~ F_j(x)=F_j(y)] \cdot \Pr_{\rho,R} [F_j(x)=F_j(y)]\\
= & (1-\tfrac{1}{p})(1-(1-\tfrac{1}{d})^h).
\end{align*}
\end{proof}

The right-hand side of the expression in the statement of the lemma can be approximated as $(1-\tfrac{1}{p})\tfrac{h}{d}$ which is stated as case (a) of Theorem~\ref{thm:main-intro}.
The lemma also allows us to relate the Hamming distance of the sketches to the Hamming distance of the vectors which is our main tool to define an estimator.

\begin{restatable}{lem}{expectationlemma}\label{lem:expectation}
Let $h$ denote $HD(x,y)$ for two input vectors $x,y$ to \fsketch, $f$ denote $HD(\phi(x),\phi(y))$ and $f^*$ denote $\E[HD(\phi(x),\phi(y))]$. Then
\begin{align*}
f^* = \E[f] & = d\left( 1-\tfrac{1}{p}\right)\left(1 - \big(1-\tfrac{1}{d}\big)^h \right).
\end{align*}
\end{restatable}

The lemma is easily proved using Lemma~\ref{lem:expectation-previous} by applying the linearity of expectation on the number of coordinates $j$ such that $\phi_j(x)\not=\phi_j(y)$. We are now ready to define an estimator for the Hamming distance.

%
%Let $D$ denote $\left(1 - \tfrac{1}{d} \right)$ and $P$ denote $\left(1-\tfrac{1}{p}\right)$. 
Using $D=\left(1 - \tfrac{1}{d} \right)$ and $P=\left(1-\tfrac{1}{p}\right)$, we
can write 
\[
f^*=dP(1-D^h) \mbox{ and } h=\ln\left( 1 - \tfrac{f^*}{dP} \right)/\ln D.
\numberthis\label{eqn:fstar}
\]

Our proposal to estimate $h$ is to obtain a tight approximation of $f^*$ and then use the above expression.

\begin{defi}[Estimator of Hamming distance]\label{defn:estimator}
Given sketches $\phi(x)$ and $\phi(y)$ of data points $x$ and $y$, suppose $f$ represents $HD(\phi(x),\phi(y))$.
We define the estimator of $HD(x,y)$ as
$\hat{h} = \ln\left( 1 - \tfrac{f}{dP} \right)/\ln D$ if $f < dP$ and $2\sigma$ otherwise.
\end{defi}

Observe that $\hat{h}$ is set to $2\sigma$ if $f \ge dP$. However, we shall show in the next section that this occurs very rarely.
%\begin{align*}
%    \Pr[f \ge dP] & \le \E[f]/dP \mbox{ (by Markov inequality)} \\
%		  & = f^*/dP = 1 - (1-\tfrac{1}{d})^h \approx \tfrac{h}{d}
%\end{align*}
%which is bounded above by a constant since we will always choose $d$ to be more than $h$. In the next section we will derive an even smaller upper boud on this probability.
%
% \begin{cor}\label{cor:estimator} Let $f^*$ denote $\E[HD(\phi(x),\phi(y))]$. Then,
% $h = \ln\left(1-\frac{f^*}{dP}\right)/\ln D$. 
% \end{cor}
%\sout{\textcolor{red}{Should $h = HD(x,y)$ or $h = \widehat{HD(x,y)}$ -- we are giving an estimate of original hamming distance. }} NO

\subsection{Analysis of Estimator}\label{subsec:analysis}

%We can make a few quick observation about the estimator $\hat{h}$. As expected, 
$\hat{h}$ is pretty reliable when the actual Hamming distance is 0; in that case 
$\phi(x)=\phi(y)$ and thus, $f=0$ and so is $\hat{h}$.
%
However, in general, $\hat{h}$ could be different from $h$. %In fact we can show that $\E[\hat{h}] \le h$ using Jensen's inequality (this is proved in Appendix~\ref{appendix:proofs}). 
The main result of this section is that their difference can be upper bounded \bl{when we set} the dimension of \fsketch to $d=4\sigma$.
%The proofs are included in Appendix~\ref{appendix:subsec:analysis} and rely on standard probability inequalities.

The results of this subsection rely on the following lemma that proves that an observed value of $f$ is concentrated around its expected value $f^*$.

\begin{restatable}{lem}{concentrationlemma}
\label{lem:concentration}Let $\alpha$ denote a desired additive accuracy. Then, for any $x,y$ with sparsity $\sigma$,\\
    \centerline{$\displaystyle \Pr\Big[|f - f^*| \ge \alpha\Big] \le 2\exp{(-\tfrac{\alpha^2}{4\sigma})}$.}
    %\vspace*{-3mm}
\end{restatable}

%The proof of the lemma employs martingales and McDiarmid's inequality; it is available in Appendix~\ref{appendix:proofs}.

% \begin{proof}
% Fix any $R$ and $x,y$; the rest of the proof applies to any $R$, and therefore, holds for a random $R$ as well. Define a vector $z \in \{0,\pm 1,\ldots,\pm c\}^n$ in which $z_i=(x_i - y_i)$; the number of non-zero entries of $z$ are at most $2\sigma$ since the number of non-zero entries of $x$ and $y$ are at most $\sigma$. Let $J_0$ be the set of coordinates from $\{1, \ldots, n\}$ at which $z$ is 0, and let $J_1$ be the set of the rest of the coordinates; from above, $J_1 \le 2\sigma$.% We will prove the lemma for $d=\tfrac{2}{\epsilon}\sqrt{\sigma \ln \tfrac{2}{\delta}}$ which automatically proves it for larger values of $d$.

% Define the event $E_j$ as ``$[\phi_j(x)\not=\phi_j(y)]$''. Note that $f$ can be written as a sum of indicator random variables, $\sum_j I(E_j)$, and we would like to prove that $f$ is almost always close to $f^*=\E[f]$.

% Observe that $\phi_j(x)=\phi_j(y)$ iff $\sum_{i \in \rho^{-1}(j)} z_i \cdot r_i = 0 \mod{p}$ iff $\sum_{i \in \rho^{-1}(j) \cap J_1} z_i \cdot r_i = 0 \mod{p}$. In other words, $\rho(i)$ could be set to anything for $i \in J_0$ without any effect on the event $E_j$; hence, we will assume that the mapping $\rho$ is defined as a random mapping only for $i \in J_1$, and further for the ease of analysis, we will denote them as $\rho(i_1), \rho(i_2), \ldots, \rho(i_{2\sigma})$ (if $|J_1| < 2\sigma$ then move a few coordinates from $J_0$ to $J_1$ without any loss of correctness).

% To prove the concentration bound we will employ martingales. Consider the sequence of these random variables $\rho'=\rho(i_1), \rho(i_2), \ldots, \rho(i_{2\sigma})$ -- these are independent. Define a function $g(\rho')$ of these random variables as a sum of indicator random variables as stated below (note that $R$ and $\rho(i)$, for $i \in J_0$, are fixed at this point)
% \begin{align*}
%     & g(\rho(i_1), \rho(i_2), \ldots \rho(i_{2\sigma}))\\
%     & = \sum_j I\left( \sum_{i \in \rho^{-1}(j) \cap J_1} z_i \cdot r_i \not= 0 \mod{p} \right) \\
%     & = \sum_j I(E_j) = f
% \end{align*}
 

% Now consider an arbitrary $t\in \{1, \ldots, 2\sigma\}$ and let $q=\rho(i_t)$; observe that $z_{i_t}$ influences only $E_q$. Choose an arbitrary value $q' \in \{1, \ldots, d\}$ that is different from $q$. Observe that, if $\rho$ is modified only by setting $\rho(i_t)=q'$ then we claim that ``bounded difference holds''.

% \begin{prop}
% $|~ g(\rho(i_1), \ldots, \rho(i_{t-1}), q, \ldots, \rho(i_{2\sigma})) - g(\rho(i_1), \ldots, \rho(i_{t-1}), q', \ldots, \rho(i_{2\sigma})) ~| \le 2$.
% \end{prop}
% The proposition holds since the only effects of the change of $\rho(i_t)$ from $q$ to $q'$ are seen in $E_q$ and $E_{q'}$ (earlier $E_q$ depended upon $z_{i_t}$ that now changes to $E_{q'}$ being depended upon $z_{i_t}$). Since $g()$ obeys bounded difference, therefore, we can apply McDiarmid's inequality~\cite[Ch~17]{upfal},~\cite{mcdiarmid_1989}.
% % McDiarmid's inequality~\cite[Ch 17]{upfal},~\cite{mcdiarmid_1989} states that
% \begin{thm}[McDiarmid's inequality\label{thm:McDiarmid}]
% Consider independent random variables $X_1,\ldots , X_m \in \mathcal{X}$, and a mapping $ f:\mathcal{X}^m \rightarrow \R$ which   for all $i$ and for all $x_1,\ldots x_m, {x_i}'$ satisfies the property:
% $|f(x_1,\ldots,x_i,\ldots,x_m)-f(x_1,\ldots,{x_i}',\ldots,x_m)|\leq c_i$, where $x_1,\ldots x_m, {x_i}'$ are possible values for the input variables of the function $f$. Then,
% % \[
% % \Pr\big[ \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)  \geq \varepsilon \big]\leq \exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right),
% % \]
% \[
% \Pr\Big[ \big| \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)\big|  \geq \varepsilon \Big]\leq 2\exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right).
% \]
% \end{thm}

% % ~\footnote{McDiarmid's inequality~\cite[Ch 17]{upfal},~\cite{mcdiarmid_1989} states that
% % \begin{thm}
% % Consider independent random variables $X_1,\ldots , X_m \in \mathcal{X}$, and a mapping $ f:\mathcal{X}^m \rightarrow \R$ which   for all $i$ and for all $x_1,\ldots x_m, {x_i}'$ satisfies the property:
% % $|f(x_1,\ldots,x_i,\ldots,x_m)-f(x_1,\ldots,{x_i}',\ldots,x_m)|\leq c_i$, where $x_1,\ldots x_m, {x_i}'$ are possible values for the input variables of the function $f$. Then,
% % % \[
% % % \Pr\big[ \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)  \geq \varepsilon \big]\leq \exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right),
% % % \]
% % \[
% % \Pr\Big[ \big| \E[f(X_1,\ldots , X_m) - f(X_1,\ldots , X_m)\big|  \geq \varepsilon \Big]\leq 2\exp \left(\frac{-2\varepsilon^2}{\sum_{i=1}^m c_i^2} \right).
% % \]
% % \end{thm}
% % }
% This inequality implies that, for every $x,y,R$,
% $$\Pr_{\rho} \Big[ \big| \E[f] - f \big| > \alpha \Big] \le 2\exp\left( -\frac{2\alpha^2}{(2\sigma)2^2} \right) = \exp \left( - \frac{\alpha^2}{4\sigma} \right).$$
% Hence, the lemma is proved.
% \end{proof}

The proof of the lemma employs martingales and McDiarmid's inequality and is available in Appendix~\ref{appendix:subsec:analysis}. 
%The next corollary follows directly from the lemma and shows that $f$ is sharply concentrated around $f^*$.
%\begin{cor}\label{cor:eps-d}
%Choose some $\epsilon \in (0,1)$. Then, $\Pr[|f-f^*| > \epsilon d] < 2 \exp{(-\tfrac{\epsilon^2 d^2}{4\sigma})}$.
%\end{cor}
%
%The corollary shows that $f$ and $f^*$ differ by at most $\epsilon d$ almost always. 
The lemma allows us to upper bound the probability of $f \ge dP$.

\begin{restatable}{lem}{upperboundf}\label{lem:upper-bound-f}
%If we set $d = 4\sigma$, then
    $\Pr[f \ge dP] \le 2 \exp(-P^2\sigma)$.
\end{restatable}

The right-hand side is a very small number, e.g., it is of the order of $10^{-278}$ for $p=5$ and $\sigma=1000$. 
The proof is a straightforward application of Lemma~\ref{lem:concentration} and is explained in Appendix~\ref{appendix:subsec:analysis}. Now we are ready to show that the estimator $\hat{h}$, which uses $f$ instead of
$f^*$ (refer to Equation~\ref{eqn:fstar}) is almost equal to the actual Hamming distance.

\begin{restatable}{lem}{hconcentrationlemma}\label{lem:hconcentrationlemma}
    Choose $d=4\sigma$ as the dimension of \fsketch and choose a prime $p$ and
    an error parameter $\delta \in (0,1)$ (ensure that $1-\tfrac{1}{p} \ge
    \tfrac{4}{\sqrt{\sigma}} \sqrt{\ln \tfrac{2}{\delta}}$ --- see
    the proof for discussion). Then the estimator defined in
    Definition~\ref{defn:estimator} is close to the Hamming distance between $x$ and $y$ with high
    probability, i.e.,\\
    \centerline{$\displaystyle\Pr\big[|\hat{h}-h| \ge \tfrac{32}{1-1/p}\sqrt{\sigma \ln \tfrac{2}{\delta}} \big] \le \delta$.}
    %\vspace*{-3mm}
\end{restatable}




If the data vectors are not too dissimilar which is somewhat evident from Figure~\ref{fig:hamming_distance_distrib}, then a better compression is possible which is stated in the next lemma.
The proofs of both these lemmas are fairly algebraic and use standard inequalities; they are included in Appendix~\ref{appendix:subsec:analysis}.

% The proof follows the same line as that of Lemma~\ref{lem:hconcentrationlemma}; the proofs of both these lemmas are included in Appendix~\ref{appendix:proofs}.
    
%     First we make a few technical observations all of which are based on
%     standard inequalities of binomial series and logarithmic functions.
%     We assume that $|f-f^*| \le \alpha$ where $\alpha=\sqrt{d \ln
%     \tfrac{2}{\delta}}$ ---  by
%     Lemma~\ref{lem:concentration}, this happens
%     with probability at least $(1-2\exp{(-\tfrac{\alpha^2}{4\sigma})})=1-\delta$.

%     From Equation~\ref{eqn:fstar} we get $D^h = 1-\tfrac{f^*}{dP}$ and from
%     Definition~\ref{defn:estimator} we get $D^{\hat{h}} = 1 - \tfrac{f}{dP}$.
%     Denote $|\hat{h}-h|$ by $\Delta h$. It will be helpful to remember that
%     $D=1-1/d \in (0,1)$.
    
%     \begin{obs}\label{obs:primep}
% 	For reasonable values of $\sigma$, and
% 	reasonable values of $\delta$, almost all
% 	primes satisfy the bound $P \ge \tfrac{4}{\sqrt{\sigma}}\sqrt{\ln
% 	\tfrac{2}{\delta}}$. We will assume this inequality to hold without loss
% 	of generality~\footnote{If the reader is wondering why we are not
% 	proving this fact, it may be observed that this relationship does not
% 	hold for small values of $\sigma$, e.g., $\sigma=16, \delta=0.01$.}.
%     \end{obs}
%     For example, $p=2$ is sufficient for $\sigma \approx 1000$ and $\delta
%     \approx 0.001$ (remember that $P=1-\tfrac{1}{p}$). Furthermore, observe that
%     $P$ is an increasing function of $p$, and the right hand side is a
%     decreasing function of $\sigma$, increasing with decreasing delta but at an
%     extremely slow logarithmic rate.
    
%     \begin{obs}\label{obs:dp-alpha}
% 	$\tfrac{dP}{\alpha} > 4$ can be assumed without loss of generality. This holds since the left hand side is
% 	$\tfrac{dP}{\sqrt{d}\sqrt{ln(2/\delta)}} =
% 	\tfrac{P\sqrt{d}}{\sqrt{\ln(2/\delta)}} \ge
% 	\tfrac{4\sqrt{d}}{\sqrt{\sigma}}$ (by Observation~\ref{obs:primep})
% 	which is at least $4$.
%     \end{obs}

%     \begin{obs}\label{obs:appendix-1}
% 	$D^\sigma = (1-\tfrac{1}{d})^\sigma \ge 1 - \tfrac{\sigma}{d} =
% 	\tfrac{3}{4}$. Since $h \le \sigma$, $D^h \ge \tfrac{3}{4}$.
%     \end{obs}

%     \begin{restatable}{obs}{obsappendix}\label{obs:appendix-2}
% 	$D^{\hat{h}} > \tfrac{1}{2}$.
%     \end{restatable}
% \begin{proof}
% 	This is not so straight forward as
% 	Observation~\ref{obs:appendix-1} since $\hat{h}$ is calculated using a
% 	formula and is not guaranteed, ab initio, to be upper bounded by
% 	$\sigma$.

% We will prove the equivalent statement $\tfrac{f}{dP} < \tfrac{1}{2}$. Since
%     we had assumed that Lemma~\ref{lem:concentration} holds with
%     $\alpha=\sqrt{4\sigma\ln\tfrac{2}{\delta}}$, we know that $f \le f^* +
%     \alpha$.
%     Therefore, $\tfrac{f}{dP} \le \tfrac{f^*}{dP} + \tfrac{\alpha}{dP}$.
%     Substituting the value of $f^*=dP(1-D^h)$ from Equation~\ref{eqn:fstar} and
%     using Observation~\ref{obs:appendix-1} we get the bound $\tfrac{f}{dP} \le
%     \tfrac{1}{4} + \tfrac{\alpha}{dP}$.

%     We can further simplify the bound using Observation~\ref{obs:dp-alpha}.
%     $$\tfrac{f}{dP} \le \tfrac{1}{4} + \tfrac{\alpha}{dP} \le \tfrac{1}{4} +
%     \tfrac{1}{4} < \tfrac{1}{2}, \mbox{ this validating the observation.}$$
% \end{proof}

% \begin{proof}[Proof of Lemma~\ref{lem:hconcentrationlemma}]
%     Now we get into the main proof which proceeds by considering two possible
%     cases.
    
%     {\bf (Case $\hat{h} \ge h$, i.e., $\Delta h=\hat{h}-h$:)} 
%     We start with the identity $D^h - D^{\hat{h}} = \tfrac{f-f^*}{dP}$.

%     Notice that the RHS is bounded from the above by $\tfrac{\alpha}{dP}$ and
%     the LHS can bounded from the below as
%     $$D^h - D^{\hat{h}} = D^h(1-D^{\Delta h}) > \tfrac{1}{2}(1-D^{\Delta h})$$
%     where we have used Observation~\ref{obs:appendix-1}. Combining these facts
%     we get $\tfrac{\alpha}{dP} > \tfrac{1}{2}(1-D^{\Delta h})$.

%     {\bf (Case $h \ge \hat{h}$, i.e., $\Delta h = h - \hat{h}$:)} In a similar
%     manner, we start with the identity $D^{\hat{h}} - D^h = \tfrac{f^* -
%     f}{dP}$ in which the RHS we bound again from the above by
%     $\tfrac{\alpha}{dP}$ and the LHS is treated similarly (but now using
%     Observation~\ref{obs:appendix-2}).
%     $$D^{\hat{h}} - D^h = D^{\hat{h}}(1-D^{\Delta h}) > \tfrac{1}{2}
%     (1-D^{\Delta h})\mbox{, and then, } \tfrac{\alpha}{dP} >
%     \tfrac{1}{2}(1-D^{\Delta h})$$

%     So in both the cases we show that $\tfrac{\alpha}{dP} >
%     \tfrac{1}{2}(1-D^{\Delta h})$. Our desired bound on $\Delta h$ can now be
%     obtained.
%     \begin{align*}
% 	\Delta h \ln D & \ge \ln\left( 1 - \tfrac{2\alpha}{dP} \right)
% 			 \ge -\tfrac{2\alpha}{dP}/(1-\tfrac{2\alpha}{dP}) =
% 	-\tfrac{2\alpha}{dP-2\alpha}\\
% 	& \mbox{ (using the inequality $\ln(1+x) \ge
% 	\tfrac{x}{1+x}$ for $x > -1$)} \\
% 	\Delta h & \le \frac{1}{\ln\tfrac{1}{D}} \frac{2\alpha}{dP -
% 	2\alpha} \le \frac{2\alpha d}{dP - 2\alpha}\\
% 	& \mbox{ (it is easy to show that $\ln \tfrac{1}{D} =
% 	\ln\tfrac{1}{1-1/d} \ge 1/d$)}\\
% 	& = \frac{2d}{\tfrac{dP}{\alpha} - 2} <\frac{2d}{\tfrac{dP}{2\alpha}} 
% 	\mbox{ (using Observation~\ref{obs:dp-alpha}, $\tfrac{dP}{\alpha}-2 >\tfrac{dP}{2\alpha}$)}\\
% 	& = \frac{4\alpha}{P} = \frac{4}{P}\sqrt{d \ln \tfrac{2}{\delta}} =
% 	\frac{8}{P}\sqrt{\sigma \ln \tfrac{2}{\delta}}
%     \end{align*}
% \end{proof}

% Our next proof is of a lemma that tightens
% Lemma~\ref{lem:hconcentrationlemma}.

% If the data vectors are not too dissimilar, then a better compression is
% possible which is stated in the next lemma. The proof follows the same line as that of
% Lemma~\ref{lem:hconcentrationlemma}. %; both these proofs are included inAppendix~\ref{appendix:proofs}.

\begin{restatable}{lem}{hconcentrationlemmatight}\label{lem:hconcentrationlemmatight}
    Suppose we know that $h \le \sqrt{\sigma}$ and choose $d=16\sqrt{\sigma \ln \tfrac{2}{\delta}}$
    as the dimension for \fsketch. Then (a) also $f < dP$ with high probability and moreover we get a better estimator. That is,
    (b) $\displaystyle\Pr\big[ |\hat{h} - h | \big] \ge \tfrac{8}{1-1/p}\sqrt{\sigma\ln
    \tfrac{2}{\delta}} \big] \le \delta $.
\end{restatable}


% \begin{proof}
%     The proof is almost exactly same as that of
%     Lemma~\ref{lem:hconcentrationlemma}, with only a few differences. We set
%     $\alpha=d/8$ where $d=16\sqrt{\sigma \ln \tfrac{2}{\delta}}$. Incidentally,
%     the value of $\alpha$ remains the same in terms of $\sigma$, $\sqrt{4\sigma
%     \ln \tfrac{2}{\delta}}$. Thus, the
%     probability of error remains same as before;
%     $$2 \exp{(-\tfrac{d^2}{64 \cdot 4 \sigma})} = \delta.$$

%     Observation~\ref{obs:primep} is true without any doubt.
%     $\tfrac{dP}{\alpha} = 8P$ which is greater than 4 for any prime number; so
%     Observation~\ref{obs:dp-alpha} is true in this scenario.

%     Observation~\ref{obs:appendix-1} holds since $D^h \ge D^{\sqrt{\sigma}}
%     = (1-\tfrac{1}{d})^{\sqrt{\sigma}} \ge 1-\tfrac{\sqrt{\sigma}}{d} = 1 -
%     \tfrac{1}{16\sqrt{\ln 2/\delta}} \ge \tfrac{3}{4}$ for reasonable values of
%     $\delta$.
%     The proof of Observation~\ref{obs:appendix-2} follows from the above
%     observations only, and, is therefore, true as well.

%     The final thing to calculate is the bound on $\Delta h$. Following the
%     calculations we see that $\Delta h \le \tfrac{4\alpha}{P}$ which leads to
%     the result that we want.
% \end{proof}

The last two results prove case (b) of Theorem~\ref{thm:main-intro} which states that the estimated Hamming distances are almost always fairly close to the actual Hamming distances. We want to emphasise that the above claims on $d$ and accuracy are only theoretical
bounds obtained by worst-case analysis. We show in our empirical evaluations
that an even smaller $d$ leads to better accuracy in practice for real-life instances.

There is a way to improve the accuracy even further by generating multiple \fsketch using several independently generated internal variables and combining the estimates obtained from each. We observed that the median of the estimates can serve as a good statistic, both theoretically and empirically. We discuss this in detail in Appendix~\ref{subsec:minfsketch}.

\subsection{Complexity analysis}\label{subsec:complexity}

\begin{table*}[]
    \centering
    \caption{Space savings offered by \fsketch on an example scenario with $2^{20}$ data points, each of $2^{10}$ dimensions but having only $2^7$ non-zero entries where non-zero entry belongs to one of $2^3$ categories. \fsketch dimension is $2^9$ (as prescribed theoretically) and its parameter $p$ is close to $2^5$. (*) The data required to construct the sketches is no longer required after the construction.\label{tab:space-example}}
    \begin{tabular}{cc||cc}
    \hline
    \multicolumn{2}{c||}{Uncompressed} & \multicolumn{2}{c}{Compressed}\\
    \hline
    Naive & Sparse vector format & \fsketch construction (*) & Storage of sketches \\
    \hline
	$2^{20} \times 2^{10} \times 3$     & $2^{20} \times 2^7 \times
	(\log 2^3 + \log 2^{10})$ & $2^{10} \times (\log 2^9 + \log 2^5) + 5$ & $2^{20} \times \log (2^5)$\\
	\hline
    \end{tabular}
    %\vspace*{-4mm}
\end{table*}


The results in the previous section show that the accuracy of the estimator $\hat{h}$ can be
tightened, or a smaller probability of error can be achieved, by choosing large values of $p$ which has a downside of a larger storage requirement. In this section, we discuss these dependencies and other factors that affect the complexity of our proposal.
 

The USP of \fsketch is its efficiency.
There are two major operations with respect to \fsketch --- construction of sketches and estimation of Hamming distance from two sketches. Their time and space requirements are given in the following table and explained in detail in Appendix~\ref{subsec:appendix-complexity}.


\noindent\begin{tabular}{lclc}
    \hline
    Construction & & Estimation & \\
    \hline
    time per sketch & $O(n)$ & time per pair & $O(d \log p)$\\
    space per sketch & $O(d \log p)$ & & \\
    \hline
    % \hline
    % Estimation & \\
    % \hline
    % time per pair &  \\
    % \hline
\end{tabular}\\

\indent We are aware of efficient representations of sparse data vectors, but for the sake of simplicity we assume full-size arrays to store vectors in this table; similarly, we assume simple dictionaries for storing the internal variables $\rho,R$ and $p$. While it may be possible to reduce the number of random bits by employing $k$-wise independent bits and mappings, we left it out of the scope of this work.  

%\begin{description}
%    \item[Construction]: A sketch is constructed during a single pass over the input vector. Every non-zero attribute is mapped to some entry of the sketch vector and then the corresponding sketch value is updated. The time to process one data vector with $t$ non-zero entries becomes $\Theta(n) + t \cdot poly(\log p)$ which is $O(n)$ for a particular dataset. The interval variables, $\rho,R,p$, require space $\Theta(n \log d)$, $\Theta(n \log p)$ and $\Theta(\log p)$, respectively, which is almost $O(n)$ if $\sigma \ll n$. Further notice that the variables $\rho$ and $R$ consume bulk of this space, but they can be freed once the sketch construction phase is over. A sketch itself consumes $\Theta(d \log p)$ space.
%    \item[Estimation:] There is no additional space requirement for estimating the Hamming distance of a pair of points from their sketches. The estimator scans both the sketches and computes their Hamming distance; finally it computes an estimate by using Definition~\ref{defn:estimator}. The running time is $O(d \log p)$.
%\end{description}

Both the operations are quite fast compared to the matrix-based and learning-based methods. There is very little space overhead too; we explain the space requirement with the help of an example in Table~\ref{tab:space-example} --- one should keep in mind that a sparse representation of a vector has to store the non-zero
entries as well as their positions in it.

Apart from the efficiency in both time and space measures, \fsketch provides additional benefits. \bl{Recall that each entry of an \fsketch is an integral value from 0 to $p-1$. Even though $0$ does not necessarily indicate a missing feature in a compressed vector, we show below that $0$ has a predominant presence in the sketches. The sketches can therefore be treated as sparse vectors that further facilitates their efficient storage.}

\begin{restatable}{lem}{sparsitylemma}\label{lem:sparsity}
    If $d=4\sigma$ (as required by Lemma~\ref{lem:hconcentrationlemma}), then the expected number of non-zero entries of $\phi(x)$ is upper bounded by $\tfrac{d}{4}$. Further, at least 50\% of $\phi(x)$ will be zero with probability at least $\tfrac{1}{2}$.
\end{restatable}
The lemma can be proved using a balls-and-bins type analysis (see Appendix~\ref{appendix:subsec:complexity} for the entire proof).
%\begin{proof}
%The lemma can be proved by treating it as a balls-and-bins problem. Imagine
%    throwing $\sigma$ balls (treat them as the non-zero attributes of $x$) into
%    $d$ bins (treat them as the sketch cells) independently and uniformly at
%    random. If the $j$th-bin remains empty then $\phi_j(x)$ must be zero (the
%    converse is not true). Therefore, the expected number of non-zero cells in the
%    sketch is upper bounded by the expected number of empty bins, which can be
%    easily shown to be $d[1-(1-\tfrac{1}{d})^\sigma]$. Using the stated value of
%    $d$, this expression can further be upper bounded.
%    $$d[1-(1-\tfrac{1}{d})^\sigma] \le d[1-(1-\tfrac{\sigma}{d})] =
%    \tfrac{d}{4}$$
%    Furthermore, let $NZ$ denote the number of non-zero entries in $\phi(x)$. We
%    derived above $\E[NZ] \le \tfrac{d}{4}$. Markov inequality can help in upper
%    bounding the probability that $\phi(x)$ contains many non-zero entries.
%    $$\Pr[NZ \ge \tfrac{d}{2}] \le \E[NZ]/\tfrac{d}{2} \le \tfrac{1}{2}$$
%\end{proof}

%Let we have a set $\mathcal{X}=\{x_i\}_{i=1}^m$ of $m$ points such that $x_i \in \{0,\ldots c\}^n$. The space required to store the data points under the sparse representation, that is, storing only non-zero indices and the corresponding values, is $O(\sigma \log n)+ O(\sigma \log c)=O(\sigma \log nc)$. Using \texttt{F-Sketch} method, we create a low-dimensional representation of $\mathcal{X}$ say $\mathcal{X’}=\{x’_i\}_{i=1}^m$ such that $x’_i \in \{0,\ldots p-1\}^d$. Each feature of the reduced dimensional requires $O(\log p)$ bits, therefore, the space required to store the sketch  $\mathcal{X’}$ is $O(md\log p)$. We now calculate the space required to store the mapping used in \texttt{F-Sketch}. This consists of two parts -- a) the space required to store the mapping $\rho$ (line $1$ in Algorithm~\ref{algo:fsketchalgo}), which takes $O(n \log d)$ bits, b) the space required to store $n$ random numbers $r_1 \ldots r_n$ with each $r_i \in \{0, \ldots p-1\}$ (line $3$ in Algorithm~\ref{algo:fsketchalgo}), which takes $O(n \log p)$ bits.  Therefore the total space required to store the sketch $\mathcal{X’}$ and the mapping used in \texttt{F-Sketch} is $O(md\log p)+O(n \log d)+O(n \log p)=O(md\log p)+O(n \log pd)$. \textcolor{red}{what we can conclude from here.}

\subsection{Sketch updating}

Imagine a situation where the categories of attributes can change dynamically, and they can both ``increase'', ``decrease'' or even ``vanish''. We present Algorithm~\ref{alg:update} to incorporate such changes {\em without recomputing the sketch afresh.} The algorithm simply uses the formula for a sketch entry as given in Observation~\ref{obs:formula}.

Most hashing-based sketching and dimensionality reduction algorithms that we have encountered either require complete regeneration of $\phi(x)$ when some attributes of $x$ change or are able to handle addition of previously missing attributes but not their removal.
    \begin{algorithm}[t]
	\noindent\hspace*{\algorithmicindent} \textbf{input:} data
	vector $x$ and its existing sketch $\phi(x)=\langle \phi_1(x),
	\phi_2(x), \ldots \phi_d(x) \rangle$\\
	\noindent\hspace*{\algorithmicindent} \textbf{input:} change $x_i:v
	\mapsto v'$ \hfill $\rhd \quad v'$ can be any value in $\{0,1,\ldots, c\}$\\
	\noindent\hspace*{\algorithmicindent} \textbf{parameters:} $\rho,R=[r_1 \ldots r_n], p$
	(same as that was used for generating the sketch) 
	\begin{algorithmic}[1]
	    \State $j=\rho(i)$
	    \State update $\phi_(x) = \big( \phi_j(x) + (v'-v)\cdot r_i
	    \big) \mod{p}$
	    \State \Return updated $\phi(x)$
	\end{algorithmic}
	\caption{Update sketch $\sigma(x)$ of $x$ after $i$-th attribute of
	$x$ changes from $v$ to $v'$ \label{alg:update}}
    \end{algorithm}



%
%\begin{restatable}{lem}{overestimator}\label{obs:overestimation}
%    $\E[\hat{h}] \ge h$.
%\end{restatable}
%%\overestimator*
%The proof uses Jensen's lemma and is included in Appendix~\ref{appendix:proofs}. 

