\newpage
\onecolumn
\appendix
\renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection}}
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

\section*{Appendix}\label{sec:appendix}
This section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader's understanding of the concepts presented in this work.

\section{Lies of omission -- across cultures}\label{sec:app-A}
Instances of lies of omission can be discovered in ancient literature from diverse cultures across the globe. In order to stimulate further discussion and provide motivation, we will present (in the appendix - due to obvious space limitation) two specific examples—one from the Western tradition and another from the Eastern tradition. These examples serve to highlight the prevalence and significance of lies of omission in literature and emphasize the need for deeper exploration of this phenomenon.

\noindent
\textbf{The merchant of Venice}: In Shakespeare's play, Antonio, an antisemitic merchant, borrows money from the Jewish moneylender Shylock in order to assist his friend in pursuing a relationship with Portia. Antonio can't repay the loan, and without mercy, Shylock demands a pound of his flesh as collateral. At this critical moment, Portia, who is now married to Antonio's friend, disguises herself as a lawyer and intervenes to save Antonio. Though the agreement allows Shylock to claim a pound of flesh, he must ensure that not a single drop of blood is shed, as causing harm to a Christian is strictly forbidden by law.

\noindent
\textbf{Mahabharata} - \emph{Ashwathama hatho, naro va kunjaro va}: This story is derived from an ancient Indian epic \emph{"The Mahabharta"}. In this excerpt, \emph{Ashwathama} is an elephant. \emph{Ashwathama} was also the name of the son of Guru Dronacharya. Yudhishtir, one of the Pandavas and \emph{Dharmraj} (which means he would never lie), faces the daunting task of confronting his unbeatable mentor, Guru Dronacharya, from whom he and his brothers had learned the art of warfare. Reluctant to engage in direct combat against his beloved teacher, Yudhishtir follows the advice of Lord Krishna and employs a strategy of omission. He announces the death of Ashwathama, but discreetly adds the words "naro va kunjaro va," indicating that it is actually a question whether the deceased Ashwathama is a human or an elephant. While Yudhishtir technically did not prevaricate, the news of his son's supposed demise deeply affects Guru Dronacharya, causing him to lose his will to fight and making it easier for Yudhishtir to overcome him. The story highlights Yudhishtir's adherence to his principles of truthfulness while employing a clever tactic of omission to gain an advantage in the battle.

\begin{comment}
\textcolor{red}{random------------}
\cite{ratliff2011behavioral} defines deception to be an outcome of omission and commission. To study deception using natural language processing techniques, in this research, we focus on omission because for ease of implementation. Referring to work on linking 5W (who, what, when, where, why) with semantic role labeling \cite{rani2023factify5wqa}, we study the omissions of these Ws and further use prompt engineering to augment deceptive data.

% \subsection{Definition of lies}

\textbf{Lies of commission} occur when someone takes the facts and embellishes them to produce an account of what never happened as it is typically more flattering. on the other hand, \textbf{lies of omission} occur when people generally leave out crucial information like facts from the victim. Thus the task of identifying such types of lies could be studied by identifying 5W(Who, What, When, Where, Why) and then finding missing ones. Therefore, we study omission in depth.

%2nd layer of lie definitions

\end{comment}

\section{Dataset Curation}
This contains additional information on data sources, data cleaning, annotation, and Inter annotator agreement
\subsection{Data Sources}\label{sec:data sources}
Information Security and Object Technology (ISOT) fake news dataset \cite{ISOTFakeNewsDataset}: This dataset contains two types of articles fake and real news. This dataset was collected from real-world sources; the truthful articles were obtained by crawling articles from Reuters.com (News website). As for the fake news articles, they were collected from different sources. The fake news articles were collected from unreliable websites that were flagged by Politifact (a fact-checking organization in the USA) and Wikipedia. For this research, the fake news dataset is leveraged. The data source has a file named “Fake.csv” which contains more than 12,600 articles from different fake news outlet resources. Each article contains the following information: article title, text, type, and the date the article was published on. We chose 2500 data points randomly from this set for this research.


\subsection{Data Cleaning and Annotation Quality check}\label{sec: data cleaning}
Data cleaning involves two iterations, data set preparation, and a human-level review of the manual annotations. The process involved the removal of URLs and unnecessary internet taxonomy with the aim of increasing data quality. To further increase the quality of data for human understanding, we reviewed the annotations manually by following the below-mentioned steps:
\vspace{-3mm}
\begin{itemize}
\setlength\itemsep{0em}
    \item Accounting for multiple annotations against a single field by the same annotator by getting rid of one of the two annotations along the lines of the definitions formulated at the start of the process.
   
    \item Filling in for fields annotated by the first entity and missed by the second entity by accounting for the gaps by building along the lines of definitions established earlier. 
    Correcting typographical errors implicating a similar meaning.
    
    \item Overriding annotations for a couple of data items where the reviewer found them overwhelmingly wrong.
\end{itemize}
\vspace{-3mm}

\subsection{Inter Annotator Agreement
}\label{sec: Data Annotation}
In the ~\cref{sec:iaa_score} we have reported inter-annotator scores for all the 3 layers in ~\cref{tab: Kappa score}. In addition, here we are reporting inter-annotator agreement for the topic of lie in the ~\cref{tab:iaa_topic_of_lie}.

\begin{table}[!tbh]
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
          & Political & Educational & Religious & Ethnicity & Racial & Others \\
\midrule
Twitter   &\cellcolor{green!50} 0.82      &\cellcolor{blue!50} 0.78        & \cellcolor{green!50}0.81      &\cellcolor{blue!50} 0.73      & \cellcolor{blue!50}0.76   & \cellcolor{blue!50}0.72   \\
Fake News & \cellcolor{green!50}0.87      &\cellcolor{green!50} 0.84        &\cellcolor{green!50} 0.85      &\cellcolor{blue!50} 0.77      & \cellcolor{green!50}0.82   &\cellcolor{blue!50} 0.79  
\\
\bottomrule
\end{tabular}
}
\label{tab:iaa_topic_of_lie}
\caption{Inter Annotator Agreement score for Topic of Lies.}
\end{table}
%\newpage


\vspace{-2mm}
\subsection{Data Analysis of SEPSIS Corpus and Insights}\label{sec: data analysis}

This section contains a thorough analysis of the entire corpus.


\noindent
\textbf{Word representation of the sepsis corpus}: We have utilized two different data sources to understand the frequency of words, we present the word clouds in fig \ref{fig:fakenews-5k} and fig \ref{fig:tweets-5k}. An interesting insight is figure \ref{fig:fakenews-5k} represents US news and figure \ref{fig:tweets-5k} represents the Indian media house.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 1cm}]{Image/fakenews_5k.pdf}
    \caption{Word cloud of data collected from ISOT fake news.}
    \label{fig:fakenews-5k}
  \end{subfigure}
    \hspace{2cm}
    \begin{subfigure}[b]{0.3\textwidth}\includegraphics[width=\linewidth, trim={1cm 1cm 1cm 1cm}]{Image/tweets_5k.pdf}
    \caption{Word cloud of data collected from Times of India.}
    \label{fig:tweets-5k}
  \end{subfigure}
  \label{fig:combined_figure}
\end{figure}


%%%%%%%%%%
\noindent
\textbf{Statistics on categories across entire corpus:}
We further present the percentage of each feature across the entire dataset as represented in table \ref{tab: SEPSIS_breakup}.

%Type of Omission




%%enter value counts % for all -- @shreya

\input{table/appendix_layer_values}





%%%%%%%

\noindent
\textbf{Percentage presence of 5Ws across all datapoints}:
Since we utilize 5W-based mask infilling, we also present \% of 5Ws across the entire dataset. and the statistics around it can be found in the table \ref{tab:percentage_presence} below.

\input{table/5W_presence}




\noindent
\textbf{Co-occurence percentage}: The four layers are connected to the input sentence. To study the co-occurrence across all categories and layers, we present them in heatmaps as described in fig \ref{fig:heatmap_layers}.


When analyzing lies of omission and colors of lies, we observe a strong correlation between speculation and black lies. Additionally, a significant majority of speculative texts can be categorized as political in nature. This association becomes even more apparent when we delve into the Intent of Lie on Lies of Omission. It is evident that the primary objective behind the creation of speculative texts is to gain an advantage. Black lies, in particular, are frequently employed for this purpose. It is noteworthy that political texts predominantly consist of black lies, serving as a means to gain an advantage.


\input{heatmaps/fourlayerconnection}

\newpage
\section{Data Augmentation}
For data augmentation, we have used two techniques (i) Paraphrasing and (ii) 5W Mask Infilling.
We provide additional information on these techniques in the following subsection.
\subsection{Paraphrasing Deceptive Datapoints}\label{sec:paraphrase-evaluation}


The underlying drive for paraphrasing textual assertions stems from the need to address variations that exist in real-life written content. The same textual claim might take on several different shapes since different news publishing companies use a variety of writing techniques. It is essential to create a solid standard for a thorough examination by taking these variations into account ( example in Figure \ref{fig: paraphrase}).

\begin{figure}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\fbox
{%
    \parbox{\columnwidth}{%
    \fontsize{9}{10}\selectfont{
    \textcolor{blue}{Sasan Goodarzi, the CEO of software giant Intuit, which has avoided mass layoffs, says tech firms axed jobs because they misread the pandemic.}}
    \\
    \textbf{Prphr 1:} Sasan Goodarzi, the CEO of Intuit, a software giant that refrained from massive layoffs, explains that tech companies terminated employees due to their misinterpretation of the pandemic.
     \\
    \textbf{Prphr 2:} Intuit's CEO, Sasan Goodarzi, highlights that unlike other tech firms, the software giant avoided extensive job cuts as they correctly understood the impact of the pandemic.
     \\
    \textbf{Prphr 3:} The pandemic was misinterpreted by tech companies, leading them to lay off employees, according to Sasan Goodarzi, CEO of Intuit, a software giant that took a different approach and did not resort to mass layoffs.
     \\
    \textbf{Prphr 4:} Sasan Goodarzi, the CEO of Intuit, a software giant, asserts that tech companies made a mistake by laying off staff members because they failed to comprehend the true nature of the pandemic.
     \\
    \textbf{Prphr 5:} In contrast to tech firms that made the wrong call and downsized their workforce, Intuit, led by CEO Sasan Goodarzi, correctly assessed the pandemic and refrained from mass layoffs.
    }
    }%
}

%\vspace{-3mm}
\caption{Deceptive paraphrased data obtained using \texttt{text-davinci-003} \cite{brown2020language}.}
\label{fig: paraphrase}
\end{figure}


\begin{wrapfigure}{R}{8cm}
\centering
\resizebox{0.5\columnwidth}{!}{
\includegraphics[width=0.85\columnwidth, height=8cm, trim={0cm 0.5cm 0cm 0cm}]{Image/figure_score_paraphrase.pdf}}
\caption{A higher diversity score depicts an increase in the number of generated paraphrases and linguistic variations in those generated paraphrases.}
%\vspace{-2mm}
\label{fig: parr}
\end{wrapfigure}

 To generate multiple paraphrases for a given claim, we employ state-of-the-art (SoTA) models. When selecting the appropriate paraphrase model from a list of available options, our main consideration is to ensure that the generated paraphrases exhibit both linguistic correctness and rich diversity. The process we follow to achieve this can be outlined as follows: Let's assume we have a claim denoted as $c$. Using a paraphrasing model, we generate $n$ paraphrases, resulting in a set of paraphrases $p_1^c$, $p_2^c$, ..., $p_n^c$. Subsequently, we conduct pairwise comparisons between these paraphrases and the original claim $c$, giving us comparisons such as $c-p_1^c$, $c-p_2^c$, ..., $c-p_n^c$. At this stage, we identify the examples that exhibit entailment, selecting only those for further consideration. To determine entailment, we utilize RoBERTa Large \cite{liu2019roberta}, a state-of-the-art model trained on the SNLI task \cite{bowman2015large}.



However, it is important to consider various secondary factors when evaluating paraphrase models. For instance, one model may generate a limited number of paraphrase variations compared to others, but those variations might be more accurate and consistent. Therefore, we took into account three key dimensions in our evaluation: \textit{(i) the number of meaningful paraphrase generations, (ii) the correctness of those generations, and (iii) the linguistic diversity exhibited by the generated paraphrases}. In our experiments, we explored the capabilities of three available models: (a) Pegasus \cite{zhang2020pegasus}, (b) T5 (T5-Large) \cite{raffel2020exploring}, and (c) GPT-3 (specifically, the \texttt{text-davinci-003} variant) \cite{brown2020language}. Based on empirical observations and analysis, we found that GPT-3 consistently outperformed the other models. To ensure transparency regarding our experimental process, we provide a detailed description of the aforementioned evaluation dimensions as follows.

\input{table/Appendix_paraphrasing}

\textbf{Coverage - Generating a substantial number of paraphrases:} Our objective is to generate up to five paraphrases for each given claim. After generating the paraphrases, we employ the concept of minimum edit distance (MED) \cite{wagner1974string} to assess the similarity between the paraphrase candidates and the original claim (with word-level units instead of individual characters). If the MED exceeds a threshold of ±2 for a particular paraphrase candidate (e.g., $c-p_1^c$), we consider it as a viable paraphrase and retain it for further evaluation. However, if the MED is within the threshold, we discard that particular paraphrase. By employing this setup, we evaluated all three models to determine which one generates the highest number of meaningful paraphrases.

\textbf{Correctness - Ensuring correctness in the generated paraphrases:} Following the initial filtration step, we conducted pairwise entailment assessments using the RoBERTa Large model \cite{liu2019roberta}, which is a state-of-the-art model trained on the SNLI dataset \cite{bowman2015large}. We retained only those paraphrase candidates that were identified as entailed by the RoBERTa Large model.


\textbf{Diversity - Ensuring linguistic diversity in the generated paraphrases:} Our focus was to select a model that could produce paraphrases with greater linguistic diversity. To assess the dissimilarities between the generated paraphrase claims, we compared pairs such as $c-p_n^c$, $p_1^c-p_n^c$, $p_2^c-p_n^c$, ..., $p_{n-1}^c-p_n^c$ for each paraphrase. We repeated this process for all other paraphrases and calculated the average dissimilarity score. Since there is no specific metric to measure dissimilarity, we utilized the inverse of the BLEU score \cite{papineni2002bleu}. This allowed us to gauge the linguistic diversity exhibited by a given model. Based on these experiments, we observed that the \texttt{text-davinci-003} variant performed the best in terms of linguistic diversity. The results of the experiment are presented in the table below. Moreover, we prioritized the selection of a model that maximized linguistic variations, and \texttt{text-davinci-003} excelled in this regard as well. The diversity vs. chosen models plot is illustrated in Figure ~\ref{fig: parr}.




\subsection{Data Augmentation using 5W Mask Infilling}
This mapping describes how Propbank roles are mapped to 5Ws(Who, What, When, Where, Why). We have used this mapping for mask infilling.
\input{table/map_5w_srl}


\section{Multi-Task Learning}\label{sec:MTL}

In this section, we delve into the specific architectural choices, experimental setup, and the formulation of the loss function employed for multi-task learning frameworks: The SEPSIS Classifier. By exploring the intricacies of this approach, we aim to shed light on the systematic integration of multiple tasks into a unified learning framework, ultimately enabling the model to effectively leverage synergistic information across layers of Deception. 

\subsection{Architectural Discussion}


 Multi-task learning (MTL) has emerged as a powerful paradigm for training deep neural networks to perform multiple related tasks simultaneously. In this paper, we propose a multi-task learning-based architecture for predicting four different tasks of the Deception dataset. The main advantage of using multi-task learning is the ability to leverage shared information across tasks, leading to improved model generalization and increased efficiency in training and inference. By jointly training multiple tasks, the model learns useful representations that are transferable to other related tasks, leading to better overall performance \cite{caruana1997multitask}. 

 \subsubsection{Dataless Knowledge Fusion}

 In many cases, LLMs are trained using domain-specific datasets, which can limit their performance when applied to out-of-domain cases. To address this challenge, we employ a fine-tuning approach on the T5-base model for each specific task, resulting in a total of four finetuned T5-based models (one model corresponding to one task). To leverage these models in our Multitask learning architecture, we employ Dataless Knowledge Fusion \cite{jin2022dataless} on these four finetuned T5-models into a single, more generalized model that exhibits improved performance in multitask learning (from here referred \textit{merged-fine-tuned-T5}).

 \subsubsection{Methodology}

 Our methodology takes a sentence as input and converts it into a latent embedding. The process of creating this rich embedding involves a two-stage approach. Firstly, we leverage the model-merging technique \cite{jin2022dataless}, which merges fine-tuned models sharing the same architecture and pre-trained weights, resulting in enhanced performance and improved generalization capabilities, particularly when dealing with out-of-domain data \cite{jin2022dataless}. Once the word embeddings are obtained from this merged model, the second stage involves converting them into a latent representation using the transformer encoder module. This representation is then propagated through four task-specific multilabel heads to obtain the output labels for each of the layers of Deception. 

 

%\subsubsection{Dataless Knowledge Fusion}

%Define dataless knowledge fusion
%\subsubsection{Rationale behind first using T5 and then dataless knowledge fusion and then encoder}

\subsection{Loss Functions}\label{sec:MTL_loss_function}
This section contains an in-depth discussion of different loss functions that we used for different tasks of MTL architecture.

\subsubsection{Cross-Entropy Loss}
Cross entropy loss, also known as log loss or logistic loss, is a commonly used loss function in machine learning, particularly in classification tasks. It measures the dissimilarity between the predicted probabilities of classes and the true labels of the data. The log loss function penalizes incorrect predictions more strongly, meaning that as the predicted probability deviates further from the true label, the loss increases. The loss approaches zero when the predicted probability aligns with the true label.

For the SEPSIS classifier, i.e., multi-label classification task with n classes, the cross-entropy loss is calculated as the average of the individual binary cross-entropy losses for each class. 
\begin{equation}
L_{B C E}= \begin{cases}-\log \left(p_i^k\right) & \text { if } y_i^k=1 \\ -\log \left(1-p_i^k\right) & \text { otherwise }\end{cases}
\end{equation}

where,
\begin{itemize}
\item $y^k=$ $\left[y_1^k, \ldots, y_C^k\right] \in\{0,1\}^C(C$ is the number of classes),
\item{$p_i^k$ is the predicted probability distribution across the classes}
\end{itemize}


\subsubsection{Focal Loss}

Focal loss is a modification of the cross entropy loss that addresses the issue of class imbalance in multi-class classification tasks \cite{lin2017focal}. In the standard multi-class cross-entropy loss, all classes are treated equally, which can be problematic when dealing with imbalanced datasets where certain classes have a much smaller representation. Focal loss aims to down-weight the contribution of well-classified examples and focuses more on difficult and misclassified examples. The focal loss for multi-label classification is defined as follows:
\begin{equation}
L_{F L}= \begin{cases}-\left(1-p_i^k\right)^\gamma \log \left(p_i^k\right) & \text { if } y_i^k=1 \\ -\left(p_i^k\right)^\gamma \log \left(1-p_i^k\right) & \text { otherwise }\end{cases}
\end{equation}

where:
\begin{itemize}
\item{$p_i^k$ is the predicted probability distribution across the classes}
\item{\(\gamma\) is the focusing parameter that controls the degree of down weighting. It is usually set to a value greater than 0. We used \(\gamma\) = 2 in our experiment.}
\end{itemize}
The focal loss formula introduces the term \((1 - p_{i})^{\gamma}\) which acts as a modulating factor. This factor down weights well-classified examples 
\(p_i^k\) close to 1 and assigns them a lower contribution to the loss. The focusing parameter gamma controls how much the loss is down-weighted. Higher values of gamma place more emphasis on difficult examples.
By incorporating the focal loss into the training objective, the model can effectively handle class imbalance and focus more on challenging examples.

\subsubsection{Dice Loss}
The Dice loss is a similarity-based loss function commonly used in image segmentation tasks and data-imbalanced multi-class classification problems. It measures the overlap or similarity between predicted and true labels. For multi-label classification, the Dice loss can be defined as follows:
\begin{equation}
L_{DL} = 1 - \frac{2 \sum_{i=1}^C y_i^k \cdot p_i^k+\epsilon}{\sum_{i=1}^C y_i^k+\sum_{i=1}^C p_i^k+\epsilon}
\end{equation}

\begin{itemize}
\item{C is the number of classes}
\item{\(y_i^k\) represents the true label for class C, which can be either 0 or 1 for each label.}
\item{\(p_i^k\) represents the predicted probability or output for class c}

\end{itemize}
The formula calculates the Dice coefficient for each example by summing the products of the true labels \(y_i^k\) and predicted probabilities \(p_i^k\) for each class C. The numerator represents the intersection between the predicted and true labels, while the denominator represents the sum of the predicted and true labels, which corresponds to the union of the two sets. By subtracting the Dice coefficient from 1, we obtain the Dice loss.

By using the Dice loss, the model is encouraged to focus on correctly identifying and predicting the minority classes, as the loss is computed based on the intersection and sum of true and predicted labels for each class. This property is especially valuable in data-imbalanced settings, as it helps to alleviate the bias towards majority classes and improve the model's ability to capture and predict the minority classes accurately.

\subsubsection{Distribution-balanced Loss}

The distribution-balanced (DB) loss function is a promising solution for addressing class imbalance and label dependency in multilabel text classification tasks. Unlike traditional approaches such as resampling and re-weighting, which often lead to oversampling common labels, the DB loss function tackles these challenges directly. By inherently considering the class distribution and label linkage, it offers a more effective alternative for achieving balanced training.

According to \cite{huang-etal-2021-balancing}, the application of the DB loss function has demonstrated superior performance compared to commonly used loss functions in multi-label scenarios. This novel approach addresses the problem of class imbalance, where certain labels are significantly underrepresented, and considers the relationship and dependencies between different labels. By striking a balance between these factors, the DB loss function ensures that the training process is fair and unbiased, resulting in improved accuracy and robustness in multilabel text classification tasks. 

For multi-label classification, the Distribution-balanced loss can be defined as follows:
\begin{equation}
L_{D B}= \begin{cases}-\hat{r}_{D B}\left(1-q_i^k\right)^\gamma \log \left(q_i^k\right) & \text { if } y_i^k=1 \\ -\hat{r}_{D B} \frac{1}{\lambda}\left(q_i^k\right)^\gamma \log \left(1-q_i^k\right) & \text { otherwise }\end{cases}
\end{equation}

where:
\begin{itemize}
\item{C is the number of classes}
\item $\hat{r}_{D B}=\alpha+\sigma\left(\beta \times\left(r_{D B}-\mu\right)\right)$ $\rightarrow$ $r_{D B}= \frac{\frac{1}{C} \frac{1}{n_i}} {\frac{1}{C} \sum_{y_i^k=1} \frac{1}{n_i}}$
\item{\(y_{i}\) represents the true label }
\item {$\lambda$ scale factor}
\end{itemize}

The distribution-balanced loss combines rebalanced weighting and negative-tolerant regularization (NTR) to address key challenges in multi-label scenarios. It effectively reduces redundant information arising from label co-occurrence, which is crucial in such tasks. Additionally, the loss explicitly assigns lower weights to negative instances that are considered "easy-to-classify," thereby improving the model's ability to handle these instances effectively. \cite{wu2020distribution}



% The formula calculates the Dice coefficient for each example by summing the products of the true labels \((y_{i,c})\) and predicted probabilities \((p_{i,c})\) for each class c. The numerator represents the intersection between the predicted and true labels, while the denominator represents the sum of the predicted and true labels, which corresponds to the union of the two sets. By subtracting the Dice coefficient from 1, we obtain the Dice loss.

% The Dice loss encourages the model to produce higher values for the overlap between predicted and true labels, resulting in better segmentation accuracy. Minimizing the Dice loss leads to optimizing the model parameters to generate segmentation masks that closely match the ground truth masks.



\subsubsection{Rationale for choosing loss function for the particular task.}

The selection of specific loss functions for each task is driven by various factors and considerations. 

\begin{enumerate}

    \item \textbf{Distribution-balanced loss function for Types of Omission:} Due to the strong multi-label nature and skewed distribution of the Types of Omission layer, the Distribution-balanced loss function is utilized \cite{huang-etal-2021-balancing}. This loss function is specifically designed to handle extreme multi-label scenarios and skewed class distributions, providing a more balanced and effective training process for the model.


    \item \textbf{Cross Entropy loss for Color of Lie}:
The Color of Lie layer is relatively class-wise balanced. In such cases, the Cross-Entropy loss is a commonly used and standard loss function. It is well-suited for balanced class distributions and helps the model effectively learn and classify the color of lies.

    \item \textbf{Focal loss for Intent of Lie:}  The Intent of Lie layer is a class-imbalanced scenario. In such situations, the Focal loss has shown to perform well. Focal loss down-weights easy examples and focuses more on hard, misclassified examples, which helps in addressing class imbalance and improving the model's performance on classification of minority classes.

    \item \textbf{Dice loss for Topic of Lie:} The Topic of Lie layer is also a class-imbalanced scenario. The Dice loss has demonstrated effectiveness in handling class imbalance. Hence we used the Dice loss for this layer so that, the model can better capture and predict the minority topics. 


The rationale behind selecting focal loss for the Intent of lie and Dice loss for the topic of lie is based on experimentation. Initially, we tried the opposite combination, which resulted in an F1 score of 0.85 for the Intent of lie and a score of 0.85 for the topic of lie. However, in the current configuration, we achieved improved performance with an F1 score of 0.87 for the Intent of lie and a score of 0.86 for the topic of lie. Therefore, after careful evaluation, we opted for focal loss and Dice loss for their respective categories to maximize overall performance.



\end{enumerate}



\subsection{Experimental results} \label{sec:experimental setup}
For overall experiments, we had 4 setups broadly.
\begin{itemize}

\item{T5 with LSTM encoder combined with no model merging}
\item{T5 with LSTM encoder combined with model merging}
\item{T5 with transformer encoder combined with no model merging}
\item{T5 with transformer encoder combined with model merging}

\end{itemize}
We used accuracy, precision, recall, and F1 score for evaluating the performance of our model. T5 with transformer encoder combined with model merging performed the best and results on these metrics for all experiments are presented in table \ref{tab:overall_exp}.


\input{table/Thelargetablefullversion}

%\newpage
\newpage
\section{Propaganda Techniques}\label{sec: Propaganda}
%\vspace{-5mm}
Propaganda techniques are strategies used to manipulate and influence people's opinions, emotions, and behavior in order to promote a particular agenda or ideology \cite{da-san-martino-etal-2019-fine, martino2020survey}. These techniques are often employed in mass media, advertising, politics, and public relations. While they can vary in their specific methods, we present definitions of 18 propaganda techniques that we have used in this study in the left box in the subsequent section. In the box on the right side, we present insights from propaganda techniques through deception.
%\newpage

\input{table/Appendix_PT}

\input{table/Appendix_PT2}







%%%%%%%%%%%%%%%%%%%




\input{table/appendix_circos}

