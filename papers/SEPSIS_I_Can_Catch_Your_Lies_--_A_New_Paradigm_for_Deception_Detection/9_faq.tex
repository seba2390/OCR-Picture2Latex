\newpage
\onecolumn
\section*{Frequently Asked Questions (FAQs)}\label{sec:FAQs}

\begin{enumerate}
    %\item What was the hypothesis for selecting the Times of India, all news in the times of India isn't fake?
    %\begin{description}
    %\item \textbf{Ans.} - 
    %\end{description}

    %\item Are there any ethical considerations related to deception?
    %\begin{description}
    %\item \textbf{Ans.} - research studies, interpersonal relationships, professional settings
    % \end{description}

    %\item Can deception be justified?
    %\begin{description}
    %\item \textbf{Ans.} - In situations like law enforcement activities conducted covertly, strategic military tactics, protecting vulnerable individuals from harm, carefully weighing the ethical considerations and potential consequences.
    %\end{description}

    %\item How do lies of omission differ from other forms of deception?
    %\begin{description}
    %\item \textbf{Ans.} - 
    %\end{description}

    %\item What are the challenges in studying the lies of omission?
    %\begin{description}
    %\item \textbf{Ans.} - the arbitrary nature of what constitutes an omission, cognitive, linguistic and social factors involved in deceptive communication
    % \end{description}

    %\item Among lies of Omission, how does one differentiate between a gray lie and a white lie?
    %\begin{description}
    %\item \textbf{Ans.} - 
    %\end{description}

    %\item What are some of the ethical considerations and challenges involved in this work?
    %\begin{description}
    %\item \textbf{Ans.} - Some of the ethical considerations involved in this research include the potential for false positives and false negatives in deception detection, the potential for bias in the annotated dataset, and the potential for misuse of the technology to suppress free speech or target individuals unfairly.
    % \end{description}

    % \item How can this research be used to improve the transparency of automated fact-checking systems?
    % \begin{description}
    % \item \textbf{Ans.} - This research can be used to improve the transparency of automated fact-checking systems by providing a more nuanced understanding of the different types of deception that can occur in online content.  By flagging false or misleading information, organizations can be held accountable for their actions and fact-checking systems can become more effective at identifying misleading information.  
    % \end{description}

    % \item What are some of the potential implications of this research for the future of journalism  /  How can this research be used to combat the spread of misinformation and disinformation?
    % \begin{description}
    % \item \textbf{Ans.} - This work has important implications for the future of journalism by highlighting the need for more accurate and reliable ways to identify lies of omission in online content. By incorporating the insights gained from this research into their reporting, journalists can become more effective at exposing false or misleading information and promoting transparency and accountability in the media.  
    % \end{description}

    \item 
    [\ding{93}] {\fontfamily{lmss} \selectfont \textbf{What were the specific instructions provided to the annotators and the criteria used for selecting them in the crowd annotation process of 5000 sentences through AMT?}}
    
    \vspace{-3mm}
    \begin{description}
    \item[\ding{224}] The annotation pipeline outlines a step-by-step approach to deception detection based on different layers, as shown in Figure 1. To ensure reliable annotations, the dataset source was kept undisclosed from the annotators. 
    %The annotators, who were primarily students aged 18-21, were selected indefinitely. 
    Notably, for sentences categorized as "Sounds Factual," no additional annotations were made apart from missing W's.
    \end{description}

    % \item  What were the quality control measures implemented during the annotation process?
    % \begin{description}
    % \item 
    % \end{description}


    \item[\ding{93}] {\fontfamily{lmss} \selectfont \textbf{How were the loss functions determined, specifically for each task head?}}

    \vspace{-3mm}
    \begin{description}
    \item[\ding{224}] The selection of loss functions for each task head was based on the characteristics of the class distribution for that specific task. If the class distribution was imbalanced, loss functions designed to handle such scenarios were chosen. Detailed explanations and experimental results supporting the choice of each loss function can be found in the appendix section \ref{sec:MTL}.
    \end{description}

    % Shreya ans:
    \item[\ding{93}] {\fontfamily{lmss} \selectfont \textbf{Why RoBERTa was finally chosen as our baseline model for the Mask Infilling task?}}

    \vspace{-3mm}
    \begin{description}
    \item[\ding{224}] Our experimentation in comparison to other state-of-the-art language models like RoBERTa-base, MPNet-base, ELECTRA-large-generator, BERT-base-uncase, and ALBERT-large-v2 revealed a higher Bilingual Evaluation Understudy (BLEU) score using RoBERTa. The selection of RoBERTa as the preferred model for the mask infilling task, based on its highest BLEU score, implies that RoBERTa's generated outputs exhibited a greater resemblance to the desired reference outputs. This characteristic of RoBERTa's performance is particularly advantageous for generating deceptive sentences that closely resemble reference sentences. By leveraging RoBERTa's capabilities, the task of producing deceptive sentences can be effectively achieved with a higher degree of fidelity to the reference sentences.
    \end{description}
    
    \item[\ding{93}] {\fontfamily{lmss} \selectfont \textbf{Why was the T5 base model chosen for model merging, and how was its performance evaluated?}}

    \vspace{-3mm}
    \begin{description}
    \item[\ding{224}] The selection of the T5 base model for model merging involved extensive experimentation and evaluation of various language models (LLMs), such as RoBERTa, T5, and DeBERTa. Our evaluation aimed to identify the LLM that would deliver the best performance for our specific case. Initially, we assessed the individual performance of each LLM by utilizing them in the architecture to generate word embeddings, without employing model merging or fine-tuning. However, there was no significant improvement in scores observed for RoBERTa and DeBERTa when compared to using the LLM as-is (without merging) or with model merging. In contrast, the T5 model demonstrated an additional 4-5\% improvement after applying Dataless Knowledge Fusion.
    \end{description}

    \item[\ding{93}] {\fontfamily{lmss} \selectfont \textbf{What are the details of the train-test validation split and other hyperparameters used for replicating the experiments?}} 

    \vspace{-3mm}
    \begin{description}
    \item[\ding{224}] The dataset was divided into an 80-20 train-test split, where 80\% of the data was used for training and 20\% for testing. To assess the model's performance, we employed 5-fold cross-validation.The train-test split was meticulously crafted to ensure that each sentence and its augmented versions are exclusively present in either the train set or the test set, but never in both. This careful arrangement guarantees the absence of any sentence overlap (i.e. sentence "S" present in train split and paraphrased version of sentence "S" present in test spilt), maintaining the integrity of the data and enhancing the overall quality of the split. The train-test split of the dataset would be made available along with all the hyperparameters of the code on GitHub for replication of the results.
    \end{description}


    \item[\ding{93}] {\fontfamily{lmss} \selectfont \textbf{What motivated the use of data augmentations and multi-task learning, and what improvement was achieved?}} 

\vspace{-3mm}
\begin{description}
\item[\ding{224}] In our initial experiment, without employing multi-task learning and data augmentation, we achieved an average accuracy score of 0.758 (averaged across all classes). Recognizing correlations between the classes, we introduced multi-task learning to capitalize on these relationships. To further enhance the model's robustness, we applied data augmentation. The improvements in average accuracy are detailed in the appendix table \ref{tab:overall_exp}. The code for reproducing experiments can be found at  \url{https://anonymous.4open.science/r/deception_MTL-60DB/}.
\end{description}


    

\end{enumerate}
