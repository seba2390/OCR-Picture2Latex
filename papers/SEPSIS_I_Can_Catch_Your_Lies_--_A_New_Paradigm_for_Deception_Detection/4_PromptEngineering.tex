\section{Data Augmentation}
\vspace{-1mm}
\label{sec:data_augmentation}
It is widely acknowledged that neural network-based techniques have a high demand for data. To address this data requirement, data augmentation has almost become a standard practice in the AI community \cite{van2001art,shorten2021text,liu2020survey}. 
%\ad{we need lot of citations here to support this statement, specific citations from NLP}. 
We have utilized three methods for data augmentation here: (i) paraphrasing, (ii) 5W masking followed by infilling \cite{gao-etal-2022-mask}.

\subsection{Paraphrasing Deceptive Datapoints}
\vspace{-1mm}
% The motivation behind paraphrasing deceptive data is as follows. Textual deceptive data may appear in various different textual forms in real life, owing to variations in the writing styles of different news publishing houses. Incorporating such variations is essential to developing a strong benchmark to ensure a holistic evaluation 
The motivation for paraphrasing deceptive data stems from the diverse manifestations of textual deceptive content in real-world scenarios, often influenced by variations in writing styles among different news publishing outlets. It is vital to incorporate these variations in order to establish a robust benchmark that facilitates comprehensive evaluation and analysis (cf. Figure \ref{fig: paraphrase} in Appendix \ref{sec:paraphrase-evaluation} for examples).
 
Undoubtedly, manual generation of possible paraphrases is ideal; however, this process is time-consuming and labor-intensive. On the other hand, automatic paraphrasing has garnered significant attention recently \cite{niu2020unsupervised, nicula2021automated, witteveen2019paraphrasing, nighojkar2021improving}. We used GPT-3.5 \cite{brown2020language} (specifically the \textit{text-davinci-003} variant) \cite{brown2020language} model as it generates linguistically diverse, grammatically correct, and a maximum number of considerable paraphrases, i.e., 5 in this case. This is the best-performing model for data augmentation using paraphrasing \cite{rani2023factify5wqa}. Additionally, we conducted experiments with Pegasus \cite{zhang2020pegasus} and T5 (T5-Large) \cite{raffel2020exploring} models, but GPT-3.5 (\texttt{text-davinci-003} variant) \cite{brown2020language} outperformed them, as indicated in Appendix \ref{sec:paraphrase-evaluation}. We gathered a total of 25,500 unique paraphrased deceptive data points through this method. 
%by inputting the data into GPT-3.5. and prompting it to generate five paraphrases. 
% Upon cross-checking the generated data points, all paraphrases are unique.

At this stage, several important questions arise: (i) \emph{What is the accuracy of the paraphrases generated?} (ii) \emph{How do they differ from or distort the original content?} To address these questions, we have conducted extensive experiments and obtained empirical answers. However, due to space limitations, please refer to Appendix \ref{sec:paraphrase-evaluation} for details of our experiments and conclusions. We have evaluated the paraphrase modules based on three key dimensions: \textit{(i) \ul{Coverage}: number of considerable paraphrase generations, (ii) \ul{Correctness}: correctness of these generations, and (iii) \ul{Diversity}: linguistic diversity in these generations}.



\subsection{Synthetic Data Augmentation using 5W Specific Mask Infilling}
\vspace{-1mm}
As mentioned previously in section ~\ref{sec:introduction}, our hypothesis revolves around the possible omission of the 5W (who, what, when, where, and why) for deceits. With this in mind, we developed a pipeline to detect the presence of the 5W and subsequently replace them with deceptive/null information generated from a language model. In the subsequent subsections, we will present our methodology for designing 5W semantic role labeling and mask filling techniques to address 5W omission.

\begin{comment}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Data augmentation pipeline]
We achieved this task in the following steps:

\begin{enumerate}
    \item STEP 1: Paraphrase the tweets and the claims, generating 5 paraphrases for each using GPT3 Text Da Vinci Model

    \item STEP 2: Pass the paraphrased texts through the AllenNLP model to generate the 5Ws for each

    \item STEP 3: MASK the Ws and generate top 3 words for each masked W.

    \item STEP 4: Replace the Ws with the words and generate new sentences with each prompt.

    \item STEP 5: List all the newly generated sentences in a single file.
    
\end{enumerate}
\end{tcolorbox}
\end{comment}

\vspace{-1mm}
\begin{figure}[!tbh]
\vspace{-1mm}
\centering
\includegraphics[width=1\columnwidth, trim={0cm 0cm 0cm 0cm}]{Image/Prompt_vizv7.pdf}
\vspace{-6mm}
\caption{Architecture representation for the process of leveraging mask infilling using RoBERTa \cite{liu2019roberta} for creating the deception dataset.}
\label{fig: architecture_MaskInfilling}
\vspace{-2.5mm}
\end{figure}
% \vspace{-2mm}

\noindent
\textbf{5W Semantic Role Labeling:}
Identification of the functional semantic roles played by various words or phrases in a given sentence is known as semantic role labeling (SRL). SRL is a well-explored area within the NLP community. There are quite a few off-the-shelf tools available: (i) Stanford SRL \cite{manning2014stanford}, (ii) AllenNLP \cite{allennlpsrl}, etc. A typical SRL system initially identifies the verbs in a given sentence and subsequently associates all the related words/phrases with the verb through relational projection, assigning them appropriate roles. Thematic roles are generally marked by standard roles defined by the Proposition Bank (generally referred to as PropBank) \cite{palmer2005proposition}, such as: \textit{Arg0, Arg1, Arg2}, and so on. We propose a mapping mechanism to map these PropBank arguments to 5W semantic roles (look at the conversion table \ref{tab:5w-map-SRL}, in appendix).


\begin{comment}
Semantic role labelling (SRL) is a natural language processing technique that involves identifying the functions of different words or phrases in a sentence. This helps to determine the meaning of the sentence by revealing the relationships between the entities in the sentence. For example, in the sentence "\textit{Moderna’s lawsuits against Pfizer-BioNTech show COVID-19 vaccines were in the works before the pandemic started,}" \textit{Moderna} would be labeled as the \textit{agent} and \textit{Pfizer-BioNTech} would be labelled as the \textit{patient}.


Using the generated paraphrases, we identify 5Ws(Who, What, When, Where, Why) using the mapping between Semantic Role Labels and 5Ws as explained by \cite{rani2023factify5wqa} \cite{}. A typical SRL system first identifies verbs in a given sentence  and then marks all the related words/phrases haven relational projection with the verb and assigns appropriate roles. Thematic roles are generally marked by standard roles defined by the Proposition Bank (generally referred to as PropBank) \cite{palmer2005proposition}, such as: \textit{Arg0, Arg1, Arg2}, and so on. A mapping mechanism to map these PropBank arguments to 5W semantic roles is described in the conversion table \ref{tab:5w-map-SRL}.
\end{comment}


\begin{comment}
After the mapping is done, a detailed analysis for the presence of each of the 5W is conducted which is summarized in figure \ref{fig: 5W_presence}.




We have leveraged 4 masking LLMs, (bert-base-uncased \cite{}, roberta \cite{}, XLNet \cite{} and ELECTRA \cite{}) to mask and replace the 5Ws generated by the AllenNLP model for the paraphrased sentences.

We have used “fill-mask” to get the output. Fill-mask is a technique within the Hugging Face Transformers Library that allows masked language modeling tasks. It is used in some NLP models to predict missing words in a sentence. In this approach, the model is given a sentence with one or more masked words, represented by special tokens such as "[MASK]", and the goal is to predict the most likely word(s) to fill in the blanks.

The top three generations from each of these models were considered for final data augmentation. The selection of the top three words was based on a metric called x.
\end{comment}

\noindent
\textbf{5W Slot Filling:} Building upon our hypothesis, it is plausible for individuals to deliberately omit any of the given W to transform a statement into a lie of omission. Therefore, once we detect the presence of the Ws, our objective is to generate variations of the original statement by selectively omitting specific Ws. For this purpose, we train a masked LLM as depicted in the Figure \ref{fig: architecture_MaskInfilling}. For the 5W slot-filling task we have experimented with five models: (i) MPNet \cite{song2020mpnet}
, (ii) ELECTRA \cite{clark2020electra},
(iii) RoBERTa \cite{liu2019roberta}, (iv) ALBERT \cite{lan2019albert}, and (v) BERT \cite{devlin2018bert}.

RoBERTa \cite{liu2019roberta}, a language model that leverages large-scale pre-training and removes the next sentence prediction objective, significantly enhancing language understanding. With its transformer architecture and fine-tuning, it predicts the original masked tokens in an \textit{input sequence X} by maximizing the likelihood of the true masked tokens given the predicted \textit{probabilities P}. Considering the scenario where all the Ws are present in a sentence, it is feasible to generate five variations. At this juncture, a crucial question arises: is there a high likelihood that the generated sentences deviate substantially from the original deceptive input? To substantiate we have calculated BLEU \cite{papineni2002bleu} score and MoverScore \cite{zhao-etal-2019-moverscore} between the original input and all the perturbed generations, reported in Table~\ref{tab:Evaluation_MaskInfilling}. 










%In this scenario, when a W is masked and passed to the model, it fills the corresponding span with content that would potentially generate deceptive data. This approach allows us to achieve our objective of generating lies of omission and performed the best based on the \aacb{BLEU score described in Table \ref{tab:Evaluation_MaskInfilling}.}
%Through our observations, we have found that pronouns and elliptic references are frequently used as fillers. This approach allows us to achieve our objective of generating lies of omission.


\input{table/MaskInfillingEvaluation}



\begin{comment}
\subsection{Synthetic Data Augmentation using Prompt Engineering}
Prompt engineering is the practice of creating powerful prompts in order to create and optimize natural language processing (NLP) models. For augmenting data in this study, we need to come up with a prompt that could generate deceptive datapoint. We use PromptScource(Bach et al., 2022) from the BigScience community to use the existing prompts or to create new prompts. PromptSource is a toolkit for creating, sharing, and using natural language prompts. Prompts are saved in standalone structured files and written in a simple templating language called Jinja. We leverage
available English prompts on PromptSource and came up with a list of the following prompts.

\begin{itemize}

 \item a
 \item b
 \item c

\end{itemize}

We used these manually selected prompts on four GPT models (GPT-2 \cite{}, GPT-3 \cite{}, GPT-3.5 \cite{}, GPT-4 \cite{}) to generate more data points.

Data augmentation using prompt engineering generated a total of additional 2,20,678 data points.
\end{comment}