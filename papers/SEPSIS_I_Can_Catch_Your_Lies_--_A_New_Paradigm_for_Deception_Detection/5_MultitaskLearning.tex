% \vspace{-1mm}
\section{Designing the SEPSIS Classifier}
\label{sec:sepsis_classifier}
\vspace{-1mm}

%Explain architecture
%Diagram of architecture
%why is that architecture novel and best for MTL
%Table for evaluation number

\begin{figure*}[!htp]
\centering
\vspace{-3mm}
\includegraphics[width=0.86\textwidth, trim={0cm 0cm 0cm 0cm}]{Image/Arch.pdf}
\vspace{-2mm}
\caption{Multi-task learning architecture delineating the process of an input text going through labeling along four dimensions: (i) types of omission, (ii) colors of lie, (iii) intention of lie, and (iv) topic of lie. Here, DB Loss stands for Distribution-balanced Loss and CE loss stands for Cross Entropy loss (cf. Appendix \ref{sec:MTL_loss_function}).}
\label{fig:MTL}
\vspace{-3.5mm}
\end{figure*}

\textls[-10]{SEPSIS, by its design, is a multitask-multilabel problem requiring the application of Multitask Learning (MTL) techniques. In general MTL framework utilizes a shared representation for all the tasks. It has been observed by several researchers \cite{parisotto2015actor, rusu2015policy, yu2020gradient, fifty2021efficiently} that shared representation has its own limitations and further effects on learning task-specific loss functions. In our approach, we introduced two specific innovations, detailed in subsequent sections. Using the MTL model (Fig. \ref{fig:MTL}), we achieved a score of 0.81 F1 score on the human-annotated dataset (5000 samples) and 0.87 F1 score on the SEPSIS dataset (0.8M data points). Fig.~\ref{fig:MTL_result} shows the F1 score across deception classes on the SEPSIS dataset (cf. Appendix \ref{sec:MTL}).}


%\textbf{Merging fine-tuned LLMs:} We adopt the idea proposed in  \cite{jin2022dataless}, where a fine-tuned Language Model (LLM) specifically trained for a sub-task (in our case, the layer of deception) was combined with other similar fine-tuned LLMs. This merging process facilitated the creation of a shared representation, resulting in improved efficiency and empirically better performance. 

%\textbf{Tailored loss function:} Secondly, we conducted experiments with a wide range of loss functions to identify the most suitable one for each specific sub-task. Through this exploration, we discovered the optimal loss function that yielded the best results for the given sub-task.

\begin{comment}
The following subsection provides an overview of our novel multitask architecture for deception detection. It delves into the specific loss function, LLM and architecture employed,  highlighting its significance in training our model effectively. Moreover, we present a detailed account of the rigorous experiments carried out using four different permutations of LLM and encoder networks over 4 metrics accuracy, precision, recall, and F-score.


Multi-task learning (MTL) has emerged as a powerful paradigm for training deep neural networks to simultaneously perform multiple related tasks. In this paper, we propose a multi-task learning-based architecture for predicting five different tasks of the Deception dataset. The main advantage of using multi-task learning is the ability to leverage shared information across tasks, leading to improved model generalization and increased efficiency in training and inference. By jointly training multiple tasks, the model learns useful representations that are transferable to other related tasks, leading to better overall performance \cite{caruana1997multitask}. 

% Our approach takes a sentence as input, which is converted into an embedding. Making the rich embedding, it's a two-stage process, i.e., performing the "Dataless Knowledge Fusion" technique on it, i.e., merging fine-tuned models that originate from pre-trained language models
% with the same architecture and pre-trained weights. As it helps in better performance and generalizes on out-of-domain data. Once the embeddings are obtained, stage 2 is to convert them into latent representation using transformer encoders. The representation is that carried to five task-specific heads, four of which are multilabel and one is multiclass output. 



Our methodology takes a sentence as input and converts it into a latent embedding. The process of creating this rich embedding involves a two-stage approach. Firstly, we leverage the "Dataless Knowledge Fusion" technique \cite{jin2022dataless}, which merges fine-tuned models sharing the same architecture and pre-trained weights, resulting in enhanced performance and improved generalization capabilities, particularly when dealing with out-of-domain data \cite{jin2022dataless}. Once the word embeddings are obtained, the second stage involves converting them into a latent representation using the transformer encoder module. This representation is then propagated through four task-specific multilabel heads.
\end{comment}


\vspace{-1.5mm}
\begin{figure}[H]
\centering
\vspace{-2mm}
\includegraphics[width=0.49\textwidth, trim={0cm 0cm 0cm 0cm}]{Image/plot.pdf}
\vspace{-7mm}
\caption{SEPSIS's F1 score for all classes of deception varies from 0.81 to 0.94. We have reported accuracy, precision, and recall as well (cf. Appendix \ref{sec:experimental setup}, tab \ref{tab:overall_exp}).}
\label{fig:MTL_result}
\vspace{-4mm}
\end{figure}
\vspace{-2mm}

\noindent
\vspace{-5mm}
\subsection{Merging Finetuned LMs Brings Power!}
\vspace{-1mm}
Drawing inspiration from \cite{jin2022dataless}, we incorporated techniques for merging multiple fine-tuned LMs, a process referred to as \emph{dataless merging}. During our experimentation with various LMs, we found that T5 performed exceptionally well for our specific case, and was also the best LM for dataless merging as emphasized in \cite{jin2022dataless}. For the four layers of deception, we fine-tuned four T5 models using the data outlined in Table ~\ref{tab:SEPSIS_corpus}. These models are denoted as T5\textsubscript{layer1}, T5\textsubscript{layer2}, T5\textsubscript{layer3}, and T5\textsubscript{layer4}. By leveraging the methodology proposed in \cite{jin2022dataless}, we merged these fine-tuned T5 models to achieve a better-shared representation tailored to our specific objectives.  Figure ~\ref{fig:MTL} visually depicts the merging process via an architecture diagram. The code for reproducing experiments can be found at \url{https://bit.ly/3FglMtB}.
%In many cases, LLMs are trained using domain-specific datasets, which can limit their performance when applied to out-of-domain cases. To address this challenge, we employ a fine-tuning approach on the T5-base model for each specific task, resulting in a total of five finetuned T5-based models (one model corresponding to one task). To leverage these models in our Multitask learning architecture, we employ Dataless Knowledge Fusion on these five finetuned T5-models into a single, more generalized model that exhibits improved performance in multitask learning (from here referred \textit{merged-fine-tuned-T5}).
\noindent
\vspace{-3mm}
\subsection{Tailored Loss Function}
\vspace{-2mm}

\textls[-10]{During our exploration for suitable sub-task loss functions, we experimented with several available options, including (i) cross-entropy loss, (ii) focal loss \cite{lin2017focal}, (iii) dice loss \cite{li2019dice}, and (iv) distribution-balanced loss (DB) \cite{huang-etal-2021-balancing}. After a thorough evaluation, we observed that distribution-balanced loss yielded the best performance for layer 1, cross-entropy loss was most effective for layer 2, focal loss performed well for layer 3, and dice loss was the optimal choice for layer 4. For a comprehensive overview of the results and an in-depth discussion of different loss functions, please refer to the Appendix \ref{sec:MTL_loss_function}.}

\begin{comment}
\begin{itemize}
    \item Cross-Entropy Loss
    
    \item Focal Loss  effectively tackles class imbalance by assigning higher weights to challenging examples and down-weighting easier ones. By emphasizing learning from difficult instances, it enables the model to concentrate on improving performance specifically for the more challenging classes.
    
    \item Dice Loss 
\end{itemize}
\end{comment}



\begin{comment}
\subsection{Experiments}
We experimented with four different variations:
\begin{itemize}
    \item T5-base + LSTM
    \item T5-base + Transformer
    \item \textit{merged-fine-tuned-T5} + LSTM
    \item \textit{merged-fine-tuned-T5} + Transformer
\end{itemize}







Our experimental analysis conducted on the Deception dataset demonstrates the effectiveness of the proposed multi-task learning architecture. To assess the model's performance, we evaluated several variations, including LSTM and Transformer models, both with and without model merging. We utilized essential evaluation metrics such as accuracy, precision, recall, and F1 score to provide comprehensive insights into the efficacy and effectiveness of the approach. The results obtained per task are listed in the table \ref{tab:loss_MTL}.

\textcolor{red}{this section should finish with a big fat table detailing all the experiments and results.}
\end{comment}


