% 
\section{Overview}
An overview of the proposed architecture is depicted in Fig. \ref{fig:pipeline}. Given the sparse-view RGB video inputs, we introduce a coarse-to-fine multi-stage neural human-object rendering scheme to handle challenging scenarios with severe occlusions and multi objects. Our approach captures high-fidelity human-object geometry and arrangement by interaction-aware human-object capture and generates photo-realistic novel view rendering results by layered human-object rendering. 
% 
A brief introduction of our main components is provided as follows:
% 


\noindent{\textbf{Occlusion-aware Implicit Human Reconstruction (Fig.~\ref{fig:pipeline} (a)).}}
% 
We perform a neural implicit geometry generation to utilize both the pixel-aligned image features and global human motion priors for the human reconstruction. Through the occlusion-aware human reconstruction scheme, our approach reconstructs high-fidelity human geometry under different occlusion scenarios.

\noindent{\textbf{Human-aware Object Tracking (Fig. \ref{fig:pipeline} (b)).}}
For the objects interacted with human, we perform a template-based object alignment and human-aware object tracking to maintain temporal consistency.
%
Efficient differentiable renderers are used to predict the 6DoF pose of the object by comparing rendered masks. 
%
%
Our continuous object tracking works robustly under the world space by jointly considering object masks, occlusion, and mesh-intersection. 

\noindent{\textbf{Direction-aware Neural Texture Blending (Fig. \ref{fig:pipeline} (c)).}}
For photo-realistic rendering, a layer-wise human-object rendering scheme is proposed to disentangle the human and object separately.
%
We adopt template-based texturing with color correction for the object and extend the neural texturing scheme~\cite{NeuralHumanFVV2021CVPR} into our interaction scenarios with severe human-object occlusion.
%
To deal with occlusion, we propose a direction-aware neural texturing blending scheme to explicitly encode the occlusion information and balance the quality of the warped images with the angle map between two source views and the novel view.

\noindent{\textbf{Spatial-temporal Texture Completion (Fig. \ref{fig:pipeline} (d)).}}
Based on the geometry from the results of the human-object capture, we adopt a spatial-temporal texture completion for the occluded regions based on the human motion priors.
%
We generate a texture-completed proxy in the canonical human space firstly. We thus use this information to complete the missing texture in a novel view.
%


We describe more details for each component in Sec.\ref{sec:method}


