\section{Experiment}
In this section, we report the details of our approach and evaluate our method on a variety of complex human-object interaction scenarios. 
%
All of our experiments are run on a PC with 2.2GHz Intel Xeon 4214 CPU, 32GB RAM, and Nvidia GeForce TITAN RTX GPUs. 
%
The inputs of our system are from a multi-camera system with six synchronized RGB cameras. 
%
Fig.~\ref{fig:gallery} demonstrates that our approach faithfully reconstructs the geometry and texture of both human and object under interactions, and even handles severe occlusion and multi-object scenarios, such as pulling a chair and catching two balls.

\noindent{\textbf{Implementation Details.}} 
We assume the perspective camera model in our pipeline. 
%
We adopt a U-net architecture following NeuralHumanFVV~\cite{NeuralHumanFVV2021CVPR} for the image encoder and a 3D convolution network like IF-Net~\cite{chibane2020implicit} with fewer feature dimensions at each resolution for the voxel encoder. 
%
U-net architecture is adopted in our $\Theta_{DAN}$ similar to Siamese Network~\cite{koch2015siamese} which takes the input from two source view separately with shared parameters during the encoding process.
%
We train our network with 1457 scans from Twindom~\cite{Twindom} augmented with rigging models of different poses and random occlusions.

\subsection{Comparison}
To the best of our knowledge, our approach is the first neural layer-wise free-viewpoint performance rendering approach with human-object interactions using only sparse RGB input.
%
For thorough comparison, we compare our approach against existing neural rendering methods, including the point-based \textbf{NHR}~\cite{Wu_2020_CVPR}, implicit method \textbf{NeRF}~\cite{nerf} and the hybrid texturing-based \textbf{NHFVV}~\cite{NeuralHumanFVV2021CVPR} using the same training data for a fair comparison.
%
Note that for the training of \textbf{NHR} we obtain the point-clouds from the depth sensors in our capture system, and we extend the \textbf{NeRF} to dynamic setting using per-frame multi-view RGB images for training.
%
Besides, the human and object segmented masks are all utilized during the training process of these methods for a fair comparison.

%
As shown in Fig.~\ref{fig:comparsion}, other method suffers from severe rendering artifacts under our interaction and sparse view setting.
%
Differently, our approach achieves significantly sharper and more realistic rendering results of human-object interaction scenarios even when serve occlusion appears.
%
Note that our approach can also enable layer-wise rendering effect and get rid of the tedious per-scene training process.
%
For quantitative comparison, we adopt the peak signal-to-noise ratio (\textbf{PSNR}), structural similarity index (\textbf{SSIM}), the Mean Absolute Error (\textbf{MAE}) as metrics on the whole testing dataset by comparing the rendering results with source view inputs.
%
As shown in Tab.~\ref{tab:Comparison}, our approach outperforms other methods on all the metrics above, which illustrates the effectiveness of our approach for free-viewpoint performance rendering under our human-object interaction and sparse-view setting.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/comparison_1.pdf}
	\caption{Qualitative comparison on rendering results with various neural rendering approaches. Our approach generates more reasonable and photo-realistic texture. (a) Input images; (b) NHFVV; (c) NHR; (d) NeRF; (e) Ours.}
    \vspace{-10pt}
	\label{fig:comparsion}
\end{figure}

\begin{table}[t]
	\begin{center}
		\centering
		\caption{Quantitative comparison against various neural rendering methods on the rendering results. Our method achieves consistently better metric results.}
		\vspace{-10pt}
		\label{tab:Comparison}
		\resizebox{0.4\textwidth}{!}{
			\begin{tabular}{l|cccc}
				\hline
				Method      & PSNR$\uparrow$ & SSIM$\uparrow$ & MAE $\downarrow$  \\
				\hline
				NHFVV~\cite{NeuralHumanFVV2021CVPR}       & 17.545            & 0.966          & 12.949    \\
				NHR~\cite{Wu_2020_CVPR}         & 23.869            & 0.964          & 10.204    \\
				NeRF~\cite{nerf}        & 24.022            & 0.970          & 9.612      \\
				Ours        & \textbf{25.323}   & \textbf{0.985} & \textbf{4.787} \\
				\hline
			\end{tabular}
		}
% 		\vspace{-10pt}
	\end{center}
\end{table}



\subsection{Evaluation}

\noindent{\textbf{Object-aware human reconstruction.}}
Here we evaluate our human geometry reconstruction stage. 
%
Let \textbf{w/o implicit} denote the variation which only uses explicit parametric human model using the same SMPL fitting process in Sec.~4.1 (a).
Besides, let \textbf{w/o explicit} and let \textbf{w/o explicit, aug} denote the variation without explicit 3D human prior and without both the 3D human prior and occlusion augmentation, respectively.
%
Fig.~\ref{fig:eval_mesh} provides the qualitative comparison against all these variations in real-world sequences. 
%
Note that without the implicit geometry inference, only parametric naked human models are recovered without details, while the results without explicit 3D human prior or occlusion augmentation suffer from severe geometry artifacts due to the challenging human-object interactions, especially for the occluded regions and thin structure like arms.
%
Differently, our approach achieves detailed human geometry reconstruction under challenging occlusions and interactions and can further enable consistent human-object capture through our lay-wise design.
%
For further quantitative analysis, we evaluate on our synthetic dataset adopt Chamfer distance (\textbf{CD}) in centimeters and Point to Surface distance (\textbf{P2S}) in centimeters, as well as the \textbf{Cosine}) and \textbf{L2} distances upon the re-projection normal as our evaluation metrics 
%
The corresponding Table.~2 highlights the contribution of each part of our geometry generation component and shows that our full method can produce good geometry under the occlusion scenario. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/eval_mesh}
	\caption{Qualitative evaluation of our occlusion-aware implicit human reconstruction scheme. (a)Input images. (b)w/o implicit; (c)w/o explicit, aug; (d)w/o explicit; (e)ours; (f)ours and object from side view.}
    \vspace{-10pt}
	\label{fig:eval_mesh}
\end{figure}

\begin{table}[t]
	\begin{center}
		\centering
		\caption{Quantitative evaluation of occlusion-aware human reconstruction scheme on synthetic data. }
		\vspace{-10pt}
		\label{tab:Evaluations}
		\resizebox{\linewidth}{!}{
			\begin{tabular}{l|cc|cc}
				\hline
				\multirow{2}{*}{Method }  & \multicolumn{2}{c|}{Mesh} & \multicolumn{2}{c}{Normal}\\
				\cline{2-5}
				~ & CD$\downarrow$ & P2S$\downarrow$ & Cosine$\downarrow$ & L2$\downarrow$ \\
				\hline
				w/o implicit         & 9.680  &  8.736 & 0.362 & 0.595  \\
				w/o explicit, aug    & 3.551  &  3.698 & 0.215 & 0.443  \\
				w/o explicit         & 3.839  &  6.275 & 0.150 & 0.380  \\
				Ours                 & \textbf{1.819}  & \textbf{2.255} & \textbf{0.134} &\textbf{0.353}  \\
				\hline
			\end{tabular}
		}
		\vspace{-10pt}
	\end{center}
\end{table}



\noindent{\textbf{Layered human-object rendering.}}
We further evaluate our human-object rendering stage against various texturing schemes using the same geometry proxy from our previous stage for a fair evaluation.
%
Let \textbf{Per-vertex} denote the per-vertex color scheme of PIFu~\cite{PIFU_2019ICCV} and \textbf{Neural Blending} denote the hybrid texturing scheme in NeuralHumanFVV~\cite{NeuralHumanFVV2021CVPR}. 
%
For qualitative evaluation, as shown in Fig.~\ref{fig:eval_rgb}, both baseline methods suffer from blur texturing results or severe occlusion artifacts near the boundary regions.
%
In contrast, our layer-wised rendering scheme utilizes both the direction information in an occlusion-aware manner, which generates much sharper and photo-realistic texture rendering results, comparing favorably to the other texturing methods.
%
Furthermore, we quantitative evaluate against various rendering schemes under the interaction scenarios with objects.
%
As shown in Fig.~\ref{fig:eval_curve}, the per-vertex scheme suffers from blur texturing artifact while the baseline neural blending scheme suffers from 
occlusion artifacts near the boundary regions especially on the objects.
%
Differently, our approach generates photo-realistic rendering results under human-object interactions, yielding the lowest mean average error(MAE) of the whole sequence with or without taking the object into error calculation.
%
These evaluations illustrate the effectiveness of our texturing scheme to utilize the direction information and perform occlusion analysis for complete, photo-realistic, and layer-wise texturing.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/eval_rgb}
% 	\vspace{-5pt}
	\caption{Qualitative evaluation of texturing scheme. (a) Input images; (b) Geometry of our approach; (c) Per-pixel texturing results from PIFu; (d) Neural blending results from NeuralHumanFVV; (e) Our texturing results. }
	\label{fig:eval_rgb}
% 	\vspace{-10pt}
\end{figure}


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/eval_render_humanobject_test.pdf}
	\vspace{-20pt}
	\caption{Evaluation of texturing schemes. (a) Our geometry results; (b) Per-pixel texturing results from PIFu; (c) Neural blending results from NHFVV; (d,e) Our texturing results with and w/o object; (f,g) Quantitative results of different texturing schemes with and w/o object, respectively.}
	\label{fig:eval_curve}
	\vspace{-10pt}
\end{figure}

\subsection{Limitation and Discussion} 
As a trial for free-viewpoint performance rendering under human-object interactions, the proposed system still owns some limitations.
% 1. 
First, we cannot handle objects with non-rigid deformation or topology changes like tearing a paper, which restricts the application of our systems.
%
We plan to address it by incorporating the key-volume update technique~\cite{FlyFusion}.
% 2. 
Besides, due to the low image resolution and limited mesh diversity of training data, our method cannot generate good results for fine-grained regions like fingers and handle those extreme poses and occlusions unseen during training. 
%
A large-scale and high-quality dataset for 4D human-object interaction analysis will be critical for such generalization.
% 3. 
Furthermore, our approach will fail to capture transparent objects which is hard to be segmented.
% 4. 
Our current pipeline models the geometry generation and layer-wise texture rendering separately, and it's an interesting direction to build an end-to-end learning framework such as neural radiance field~\cite{nerf} for complex human-object interaction scenarios.





