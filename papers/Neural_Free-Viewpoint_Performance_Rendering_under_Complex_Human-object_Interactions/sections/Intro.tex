\section{Introduction}

% 
The rise of virtual and augmented reality (VR and AR) has increased the
the demand of the human-centric 4D (3D spatial plus 1D time) content generation, with numerous applications from entertainment to commerce, from gaming to education.
%
Further reconstructing the 4D models and providing photo-realistic rendering of challenging human-object interaction scenarios conveniently evolves as a cutting-edge yet bottleneck technique.


% 2. 
% 2.1 
Early high-end solutions~\cite{motion2fusion,TotalCapture,collet2015high,UnstructureLan} rely on multi-view dome-based setup to achieve high-fidelity reconstruction and rendering, which are expensive and difficult to be deployed.
% 2.2 
%
The recent low-end volumetric approaches~\cite{UnstructureLan,robustfusion,su2021robustfusion,DoubleFusion,BodyFusion,tao2021function4d} have enabled light-weight performance reconstruction by leveraging the RGBD sensors and modern GPUs.
%
However, they still suffer from unrealistic texturing results and heavily rely on depth cameras which are not as ubiquitous as color cameras.
% 2.3 
The recent neural rendering techniques~\cite{Wu_2020_CVPR,NeuralVolumes,nerf} bring huge potential for human reconstruction and photo-realistic rendering from only RGB inputs.
%
In particular, the approaches with implicit function~\cite{saito2019pifu,saito2020pifuhd,zheng2020pamir} reconstruct clothed humans with fine geometry details but are restricted to only human without modeling human-object interactions.
%
For photo-realistic human performance rendering, various data representation have been explored, such as point-clouds~\cite{Wu_2020_CVPR,pang2021fewshot}, voxels~\cite{NeuralVolumes}, implicit representations~\cite{nerf,peng2021neural,pumarola2020d,tretschk2020nonrigid} or hybrid neural texturing~\cite{NeuralHumanFVV2021CVPR}. 
%
However, existing solutions rely on doom-level dense RGB sensors or are limited to human priors without considering the joint rendering of human-object interactions.
% 2.4 
Besides, various researchers~\cite{PSI2019,zhang2020object,2020phosa_Arrangements,PLACE:3DV:2020,HPS,Hassan:CVPR:2021,PatelCVPR2021,GRAB:2020} models the interaction between humans and the objects or the surrounding environments.
%
However, they only recover the naked human bodies or a visually plausible but not accurate spatial arrangement.
%
Few researchers explore to combine volumetric modeling and photo-realistic novel view synthesis under the human-object interaction scenarios, especially for the light-weight sparse RGB setting.


% 3.
In this paper, we attack these challenges and present a human-object neural volumetric rendering using only sparse RGB cameras surrounding the performer and objects.
%
As illustrated in Fig.~\ref{fig:teaser}, our novel approach generates both high-quality geometry
and photo-realistic texture of human activities in novel views for both the performers and objects under challenging interaction scenarios, whilst maintaining the light-weight setup.



% 4.
Generating such a free-viewpoint video and achieving robust volumetric reconstruction of human activities under challenging human-object interactions is non-trivial.
% 4.1
Our key idea is to embrace a layer-wise scene decoupling strategy and perform volumetric reconstruction and rendering of the target human and object
separately, so as to model the challenging occlusion explicitly for human-object interactions. 
% 4.2 
To this end, we first apply an off-the-shelf instance segmentation approach to the six RGB input streams to distinguish the human and object separately.
%
Then, for robust geometry reconstruction, we introduce a novel implicit human-object capture scheme to model the mutual influence between human and object.
%
For the human reconstruction, we perform a neural implicit geometry generation to jointly utilize both the pixel-aligned image features and global human motion priors with the aid of an occlusion-aware training data augmentation. 
%
For the objects, we perform a template-based object alignment for the first frame and human-aware tracking to maintain temporal consistency and prevent the segmentation uncertainty caused by interaction.
% 4.3 
Finally, for photo-realistic rendering, based on the geometry proxy above, a layer-wise human-object rendering scheme is proposed to disentangle the human and object separately.
%
Specifically, we adopt template-based texturing with color correction for the object and extend the neural texturing scheme~\cite{NeuralHumanFVV2021CVPR} into our interaction scenarios with severe human-object occlusion.
%
We propose a direction-aware neural texturing blending scheme that encodes the occlusion information explicitly, and adopts a spatial-temporal texture completion for the occluded regions based on the human motion priors.
%
With the aid of such occlusion analysis, our texturing scheme maps the input adjacent images into a photo-realistic texture output of human-object activities in the target view through efficient blending weight learning, without requiring further per-scene training.
% 5. 
To summarize, our main technical contributions include:
\begin{itemize}
	\item We present a neural human performance rendering scheme for challenging human-object interaction scenarios using only sparse RGB cameras, which can reconstruct high-quality texture and geometry results of human activities, achieving significant superiority to existing state-of-the-art.
	
	\item We propose an interaction-aware human-object capture sch-eme that combines occlusion-aware neural implicit human geometry generation and robust human-aware object tracking for consistent 4D human-object dynamic reconstruction.
	
	\item We introduce a layer-wise human-object rendering scheme, which utilizes occlusion-aware neural blending weight learning and spatial-temporal texture completion to provide high-resolution and photo-realistic texture results.
\end{itemize}



