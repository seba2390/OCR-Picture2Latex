
\section{Related Work}

  \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline}
    % \vspace{-12pt}
    \vspace{-20pt}
    \caption{The overview of our approach. Given the six RGB stream inputs surrounding the performer and objects, our approach generates high-quality human-object meshes and free-view rendering results. ``DR'' indicates differentiable rendering.
    }
    \vspace{-10pt}
    \label{fig:pipeline}
  \end{figure*}
    
  
\noindent{\textbf{Human Performance Capture.}}
Markerless human performance capture techniques have been widely investigated to achieve human free-viewpoint video or reconstruct the geometry. 
%
The high-end solutions~\cite{motion2fusion,TotalCapture,collet2015high,chen2019tightcap} adopt studio-setup with dense cameras to produce high-quality reconstruction and surface motion, but the synchronized and calibrated multi-camera systems are both difficult to deploy and expensive.
%
The recent low-end approaches~\cite{Xiang_2019_CVPR,LiveCap2019tog,chen2021sportscap, he2021challencap} enable light-weight performance capture under the single-view setup or even hand-held capture setup or drone-based capture setup~\cite{xu2017flycap}.
%
However, these methods require a naked human model or pre-scanned template. 
Volumetric fusion based methods~\cite{newcombe2015CVPR,DoubleFusion,BodyFusion,HybridFusion} enables free-form dynamic reconstruction. But they still suffer from careful and orchestrated motions, especially for a self-scanning process where the performer turns around carefully to obtain complete reconstruction. 
%
\cite{robustfusion} breaks self-scanning constraint by introducing implicit occupancy method.
%
 All these methods suffer from the limited mesh resolution leading to uncanny texturing output. Recent method~\cite{mustafa2020temporally} leverages unsupervised temporally coherent human reconstruction to generate free-viewpoint rendering. It is still hard for this method to get photo-realistic rendering results.
%
Comparably, our approach enables the high-fidelity capture of human-object interactions and eliminates the additional motion constraint under the sparse view RGB camera settings.


\noindent{\textbf{Neural Rendering.}}
The recent progress of differentiable neural rendering brings huge potential for 3D scene modeling and photo-realistic novel view synthesis. Researchers explore various data representations to pursue better performance and characteristics, such as point-clouds~\cite{Wu_2020_CVPR,aliev2019neural,suo2020neural3d}, voxels~\cite{lombardi2019neural}, texture meshes~\cite{thies2019deferred,liu2019neural} or implicit functions~\cite{park2019deepsdf,nerf,meng2021gnerf,chen2021mvsnerf,wang2021mirrornerf,luo2021convolutional}. 
%
However, these methods require inevitable pre-scene training to a new scene.
%
For neural modeling and rendering of dynamic scenes, NHR~\cite{Wu_2020_CVPR} embeds spatial features into sparse dynamic point-clouds, Neural Volumes~\cite{NeuralVolumes} transforms input images into a 3D volume representation by a VAE network.
% 
More recently, \cite{park2020deformable,pumarola2020d,li2020neural,xian2020space,tretschk2020non,peng2021neural,zhang2021editable} extend neural radiance field (NeRF)~\cite{nerf} into the dynamic setting. 
%
They learn a spatial mapping from the canonical scene to the current scene at each time step and regress the canonical radiance field. 
% 
However, for all the dynamic approaches above, dense spatial views or full temporal frames are required in training for high fidelity novel view rendering, leading to deployment difficulty and unacceptable training time overhead. Recent approaches~\cite{peng2021neural} and ~\cite{NeuralHumanFVV2021CVPR} adopt a sparse set of camera views to synthesize photo-realistic novel views of a performer. However, in the scenario of human-object interaction, these methods fail to generate both realistic performers and realistic objects.
Comparably, our approach explores the sparse capture setup and fast generates photo-realistic texture of challenging human-object interaction in novel views.

% \myparagraph{\textbf{Human-object capture}}
\noindent{\textbf{Human-object capture.}}
%
Early high-end work~\cite{collet2015high} captures both human and objects by reconstruction and rendering with dense cameras. 
%
Recently, several works explore the relation between human and scene to estimate 3D human pose and locate human position~\cite{hassan2019resolving,HPS,liu20204d}, naturally place human~\cite{PSI2019,PLACE:3DV:2020,hassan2021populating} or predict human motion~\cite{cao2020long}. 
%
Another related direction~\cite{GRAB:2020,hampali2021handsformer,liu2021semi} models the relationship between hand and objects for generation or capture.
%
PHOSA~\cite{2020phosa_Arrangements} runs human-object capture without any scene- or object-level 3D supervision using constraints to resolve ambiguity. 
However, they only recover the naked human bodies and produce a visually reasonable spatial arrangement.
%
A concurrent close work is RobustFusion(journal)~\cite{su2021robustfusion}. They capture human and objects by volumetric fusion respectively, and track object by Iterative Closest Point (ICP). 
However, their texturing quality is limited by mesh resolution and color representation, and the occluded region is ambiguous in 3D space.
%
Comparably, our approach enables photo-realistic novel view synthesis and accurate human object arrangement in 3D world space
under the human-object interaction for the light-weight sparse RGB settings.