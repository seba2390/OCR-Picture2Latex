\section{Method}\label{sec:method}
%

\subsection{Interaction-aware Human-Object Capture}\label{sec:human_capture}
Classical multi-view stereo reconstruction approaches \citep{Furukawa2013,Strecha2008,Newcombe2011,collet2015high} and recent neural rendering approaches \citep{Wu_2020_CVPR,NeuralVolumes,nerf} rely on multi-view dome based setup to achieve high-fidelity reconstruction and rendering results.
%
However, they suffer from both sparse-view inputs and occlusion of objects.
%
To this end, we propose a novel implicit human-object capture scheme to model the mutual influence between human and object from only sparse-view RGB inputs.

\noindent{\textbf{(a) Occlusion-aware Implicit Human Reconstruction.}}
For the human reconstruction, we perform a neural implicit geometry generation to jointly utilize both the pixel-aligned image features and global human motion priors with the aid of an occlusion-aware training data augmentation.
% 

Without dense RGB cameras and depth cameras, traditional multi-view stereo approaches \citep{collet2015high,motion2fusion} and depth-fusion approaches \citep{KinectFusion,UnstructureLan,robustfusion} can hardly reconstruct high-quality human meshes.
%
With implicit function approaches \citep{PIFU_2019ICCV,PIFuHD}, we can generate fine-detailed human meshes with sparse-view RGB inputs.
%
However, the occlusion from human-object interaction can still cause severe artifacts.
%
To end this, we thus utilize the pixel-aligned image features and global human motion priors.

% 
Specifically, we adopt the off-the-shelf instance segmentation approach \citep{Bolya_2019_ICCV} to obtain human and object masks, thus distinguishing the human and object separately from the sparse-view RGB input streams.
%
Meanwhile, we apply the parametric model estimation to provide human motion priors for our implicit human reconstruction.
%
We voxelized the mesh of this estimated human model to represent it with a volume field.

We give both the pixel-aligned image features and global human motion priors in volume representation to two different encoders of our implicit function, as shown in Fig. \ref{fig:pipeline} (a).
%
Different from \cite{2020phosa_Arrangements} with only a single RGB input, we use pixel-aligned image features from the multi-view inputs and concatenate them with our encoded voxel-aligned features.
%
We finally decode the pixel-aligned and voxel-aligned feature to occupancy values with a multilayer perceptron (MLP).

For each query 3D point $P$ on the volume grid, we follow PIFu \citep{saito2019pifu} to formulate the implicit function $f$ as:
\begin{align}
	f( \Phi(P),\Psi(P),Z(P)) & = \sigma : \sigma \in [0.0, 1.0],             \\
	\Phi(P)                  & = \frac{1}{n} \sum_{i}^{n}F_{I_{i}}(\pi_{i}(P)), \\
	\Psi(P)                  & =  G(F_{V},P),
\end{align}
where $p = \pi_{i}(P)$ denotes the projection of 3D point to camera view $i$, $F_{I_{i}}(x)= g(I_{i}(p))$ is the image feature at $p$.
%
$\Psi(P) = G(F_{V},P)$ denotes the voxel aligned features at $P$, $F_{V}$ is the voxel feature.
%
To better deal with occlusion, we introduce an occlusion-aware reconstruction loss to enhance the prediction at the occluded part of human.
% 
It is formulated as:
	\begin{align}
		 & \mathcal{L}_{\sigma} = \lambda_{occ}\sum_{t=1}^T \left\| \sigma_{occ}^{gt} - \sigma_{occ}^{pred} \right\|_2^2 + \lambda_{vis}\sum_{t=1}^T \left\| \sigma_{vis}^{gt} - \sigma_{vis}^{pred} \right\|_2^2.
	\end{align}

% 
Here, $\lambda_{occ}$ and $\lambda_{vis}$ represent the weight of occlusion points and visible points, respectively.
%
$\sigma_{occ}$ and $\sigma_{vis}$ are the training sampling points at the occlusion area and visible area.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/data_augmentation}
    \vspace{-10pt}
    \caption{Illustration of our synthetic 3D data with both human and objects.}
    % \vspace{-1mm}
    \vspace{-15pt}
    \label{fig:DataAugmentation}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/pipeline_net}
	\caption{Illustration of our layered human-object rendering approach, which not only includes a direction-aware neural texture blending scheme to encode the occlusion information explicitly but also adopts a spatial-temporal texture completion for the occluded regions based on the human motion priors.}
	\vspace{-10pt}
	\label{fig:pipeline_net}
\end{figure*}

For the detail of the parametric model estimation, we fit the parametric human model, SMPL \citep{SMPL2015}, to capture occluded human with the predicted 2D keypoints.
%
Specifically, we use Openpose \citep{Openpose} as our joint detector to estimate 2D human keypoints from sparse-view RGB inputs.
%
To estimate the pose/shape parameters of SMPL as our human prior for occluded human, we formulate the energy function $\boldsymbol{E}_{\mathrm{prior}}$ of this optimization as:
\begin{align} \label{eq:opt}
	\boldsymbol{E}_{\mathrm{prior}}(\boldsymbol{\theta}_t, \boldsymbol{\beta}) = \boldsymbol{E}_{\mathrm{2D}} + \lambda_{\mathrm{T}}\boldsymbol{E}_{\mathrm{T}}
\end{align}
% 
Here, $\boldsymbol{E}_{\mathrm{2D}}$ represents the re-projection constraint on 2D keypoints detected from sparse-view RGB inputs, while $\boldsymbol{E}_{\mathrm{T}}$ enforces the final pose and shape to be temporally smooth.
%
$\boldsymbol{\theta}_t$ is the pose parameters of frame $t$, while $\boldsymbol{\beta}$ is the shape parameters.
%
Note that this temporal smoothing enables globally consistent capture during the whole sequence, and benefits the parametric model estimation when some part of the body is gradually occluded.
%
We follow \cite{he2021challencap} to formulate the 2D term $\boldsymbol{E}_{\mathrm{2D}}$ and the temporal term $\boldsymbol{E}_{\mathrm{T}}$ under the sparse-view setting.
%

Moreover, we apply an occlusion-aware data augmentation to reduce the domain gap between our training set and the challenging human-object interaction testing set.
%
Specially, we randomly sample some objects from ShapeNet dataset~\cite{chang2015shapenet}.
%
We then randomly rotate and place them around human before training, as shown in Fig. \ref{fig:DataAugmentation}.
%
By simulating the occlusion of human-object interaction, our network is more robust to occluded human features.

With both the pixel-aligned image features and the statistical human motion priors under this occlusion-aware data augmentation training, our implicit function generates high-quality human meshes with only spare RGB inputs and occlusions from human-object interaction.

\noindent{\textbf{(b) Human-aware Object Tracking.}}
%
For the objects around the human, people recover them from depth maps~\cite{new2011kinect}, implicit fields~\cite{mescheder2019occupancy}, or semantic parts~\cite{chen2018autosweep}. We perform a template-based object alignment for the first frame and human-aware tracking to maintain temporal consistency and prevent the segmentation uncertainty caused by interaction. With the inspiration of PHOSA \cite{2020phosa_Arrangements}, we consider each object as a rigid body mesh.


To faithfully and robustly capture object in 3D space as time going, we introduce a human-aware tracking method.
%
Expressly, we assume objects are rigid bodies and transforming rigidly in the human-object interaction activities.
%
So the object mesh $O_{t}$ at frame $t$ can be represented as: $O_{t} = R_{t}O_{t-1}+T_{t}$.
% 
Based on the soft rasterization rendering~\cite{ravi2020accelerating}, the rotation $R_{t}$ and the translation $T_{t}$ can be naively optimized by comparing $\mathcal{L}_{2}$ norm between the rendered silhouette $S_{t}^{i}$ and object mask $\mathcal{M}o_{t}^{i}$.
%

Human is also an important cue to locate the object position.
%
From the 2D perspective, when objects are occluded by the human at a camera view, the  $\mathcal{L}_{2}$ loss between rendered silhouette and occluded mask will lead to the wrong object location due to the wrong guidelines at the occluded area.
%
So we remove the occluded area affected by human mask $\mathcal{M}h_{t}^{i}$ when computing the $\mathcal{L}_{2}$ loss.
%
From the 3D perspective, human can not interpenetrate an rigid object, so we also add an interpenetration loss $\mathcal{L}_{P}$~\cite{jiang2020mpshape} to regularize optimization. Our total object tracking loss is:
\begin{align}
	\mathcal{L}_{track} = \lambda_{1}\sum_{i=0}^{n}\| \mathcal{B}(\mathcal{M}h_{t}^{i}==0) \odot  S_{t}^{i} - \mathcal{M}o_{t}^{i}  \|  + \lambda_{2}\mathcal{L}_{P},
\end{align}
where n denotes view numbers, $\lambda_{1}$ denotes weight of silhouette loss, $\lambda_{2}$ denotes weight of interpenetration loss, $\mathcal{B}$ represents an binary operation, it returns 0 when the condition is true, else 1.
% 	}

Our implicit human-object capture utilizes both the pixel-aligned image features and global human motion priors with the aid of an occlusion-aware training data augmentation, and captures objects with the template-based alignment and the human-aware tracking to maintain temporal consistency and prevent the segmentation uncertainty caused by interaction. Thus, our approaches can generate high-quality human-object geometry with sparse inputs and occlusions.

\subsection{Layered Human-Object Rendering}\label{sec:rendering}
We introduce a neural human-object rendering pipeline to encode local fine-detailed human geometry and texture features from adjacent input views, so as to produce photo-realistic layered output in the target view, as illustrated in Fig. \ref{fig:pipeline_net}.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/gallery}
	\vspace{-20pt}
	\caption{The geometry and texture results of our proposed approach, which generates photo-realistic rendering results and high fidelity geometry on a various of sequences, such as rolling a box, playing with balls.}
    \vspace{-10pt}
	\label{fig:gallery}
\end{figure*}

\noindent{\textbf{(c) Direction-aware Neural Texture Blending.}} \label{sec:neuralBlending}
%
While traditional image-based rendering approaches always show the artifacts with the sparse-view texture blending, we follow \cite{NeuralHumanFVV2021CVPR} to propose a direction-aware neural texture blending approach to render photo-realistic human in the novel view.
% %
For a novel view image $I_{n}$, the linear combination of two source view $I_{1}$ and $I_{2}$ with blending weight map $W$ is formulate as:
\begin{align}
	I_{n} = W \cdot I_{1} + (1 - W) \cdot I_{2}.
\end{align}
However, in the sparse-view setting, the neural blending approach \citep{NeuralHumanFVV2021CVPR} can still generate unsmooth results. As the reason of these artifacts, the imbalance of angles between two source views with a novel view will lead to the imbalance wrapped image quality.
%

Different from \citet{NeuralHumanFVV2021CVPR}, we thus propose a direction-aware neural texture blending to eliminate such artifacts, as shown in Fig. \ref{fig:pipeline_net}.
%
The direction and angle between the two source views and target view will be an important cue for neural rendering quality, especially under occluded scenarios. 
%
Given novel view depth $D_{n}$ and source view depth $D_{1}$, $D_{2}$, we wrap them to the novel view $D_{1}^{n}$ and $D_{2}^{n}$, then compute the occlusion map $O_{i} = D_{n}- D_{i}^{n} (i=1,2)$.
%
Then, we unproject $D_{i}$ to point-clouds.
%
For each point $P$, we compute the cosine value between $\overrightarrow{c_{i}P}$ and $\overrightarrow{c_{n}P}$ to get angle map $A_{i}$, where $c_{i}$ denotes the optical center of source camera $i$, $c_{n}$ denotes the optical center of novel view camera.
%
Thus, we introduce a direction-aware blending network $\Theta_{DAN}$ to utilize global feature from image and local feature from human geometry to generate the blending weight map $W$, which can be formulated as:
\begin{align}
	W = \Theta_{DAN}(I_{1},O_{1},A_{1},I_{2},O_{2},A_{2}),
\end{align}
% 

\noindent{\textbf{(d) Spatial-temporal Texture Completion.}}
%
While human-object interaction activities consistently lead to occlusion, the missing texture on human, therefore, leads to severe artifacts for free-viewpoint rendering.
%
To end this, we propose a spatial-temporal texture completion method to generate a texture-completed proxy in the canonical human space.
%
We use the temporal and spatial information to complete the missing texture at view $i$ and time $t$ from different times and different views.

Specifically, we first use the non-rigid deformation to register an up-sampled SMPL model (41330 vertices) with the captured human meshes.
%	
Then, for each point on the proxy, we find the nearest visible points along with all views and all frames, then assign an interpolation color to this point.
% 
We thus generate a canonical human space with the fused texture.
%
For the occluded part of human in novel view, we render the texture-completed image and blend it with the neural rendering results in Sec. \ref{sec:neuralBlending} (c).

We utilize a layered human-object rendering strategy to render human-object together with the reconstruction and tracking of object.
%
For each frame, we render human with our novel neural texture blending while rendering objects through a classical graphics pipeline with color correction matrix (CCM).
%
To combine human and object rendering results, we utilize the depth buffer from the geometry of our human-object capture.
%

\noindent{\textbf{Training Strategy.}} To enable our sparse-view neural human performance rendering under human-object interaction, we need to train the direction-aware blending network $\Theta_{DAN}$ properly.
% 

We follow \citet{NeuralHumanFVV2021CVPR} to utilize 1457 scans from the Twindom dataset \cite{Twindom} to train our DAN $\Theta_{DAN}$ properly.
%
Differently, we randomly place the performers inside the virtual camera views and augment this dataset by randomly placing some objects from ShapeNet dataset~\cite{chang2015shapenet}.
%
By simulating the occlusion of human-object interaction, we make our network more robust to occluded human.
%
Our training dataset contains RGB images, depth maps and angle maps for all the views and models.

For the training of our direction-aware blending network $\Theta_{DAN}$, we set out to apply the following learning scheme to enable more robust blending weight learning.
%
The appearance loss function with the perceptual term ~\cite{Johnson2016Perceptual} is to make the blended texture as close as possible to the ground truth, which is formulated as:
\begin{align}
	\left.\mathcal{L}_{r g b}=\frac{1}{n} \sum_{j}^{n}
	\left(
	\left\|I_{r}^{j}-I_{g t}^{j}\right\|_{2}^{2}
% 	\right)
	+\left\|\varphi
	\left(
	I_{r}^{j}
	\right)
	-\varphi
	\left(
	I_{g t}^{j}
	\right)
	\right\|_{2}^{2}
	\right) \right.
\end{align}
where $I_{g t}$ is the ground truth RGB images; $\varphi(\cdot)$ denotes the output features of the third-layer of pre-trained VGG-19.

With the aid of such occlusion analysis, our texturing scheme maps the input adjacent images into a photo-realistic texture output of human-object activities in the target view through efficient blending weight learning, without requiring further per-scene training.
