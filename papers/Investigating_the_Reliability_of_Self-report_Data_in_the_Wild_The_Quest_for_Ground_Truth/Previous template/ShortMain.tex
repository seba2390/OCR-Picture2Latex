%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf (above) for SIGCHI conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for conferences using one-column small layout
% \documentclass[acmsmall,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}
%The ACM CHI Conference on Human Factors in Computing Systems is the premier international conference on Human-Computer Interaction.
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Ubicomp '21]{Ubicomp '21: ACM International Joint Conference on Pervasive and Ubiquitous Computing}{Sep 21--26, 2021}{Remote}
\acmBooktitle{Ubicomp '21: ACM International Joint Conference on Pervasive and Ubiquitous Computing, Sep 21--26, 2021, Remote}
\acmPrice{15.00}
\acmISBN{978-1-4503-3214-2/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Investigating the Reliability of Self-report Data in the Wild: The Quest for Ground Truth}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Nan Gao}
\email{nan.gao@rmit.edu.au}
\affiliation{%
  \institution{RMIT University}
  \city{Melbourne}
  \country{Australia}
  \postcode{3000}
}

\author{Mohammad Saiedur Rahaman}
\email{saiedur.rahaman@rmit.edu.au}
\affiliation{%
  \institution{RMIT University}
  \city{Melbourne}
  \country{Australia}
  \postcode{3000}
}

\author{Wei Shao}
\email{wei.shao@rmit.edu.au}
\affiliation{%
  \institution{RMIT University}
  \city{Melbourne}
  \country{Australia}
  \postcode{3000}
}


\author{Flora D. Salim}
\email{flora.salim@rmit.edu.au}
\affiliation{%
  \institution{RMIT University}
  \city{Melbourne}
  \country{Australia}
  \postcode{3000}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Gao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
%Inferring human mental state (e.g., emotion, depression, engagement) with sensing technology is one of the most valuable challenges in the affective computing area, which has a profound impact in all industries interacting with humans. The self-report survey is the most common way to quantify how people think, but prone to subjectivity and various responses bias. %Self-report survey is the most common ways to study the human behaviour and attitude in human-based studies. 
%It is usually used as the ground truth for human mental state prediction. In recent years, many data-driven machine learning models are built based on self-report annotations as the target value. In this research, we conduct a case study in a high school, and collect 488 self-report responses and wearable data from 23 student participants over 144 classes and 10 subjects for 4 weeks. We investigate the reliability of self-report survey in the wild by studying the confidence level of responses and survey completion time. Additionally, we find the physiologically measured student engagement and perceived student engagement are not always consistent. The findings from this research have great potential to benefit the future studies in predicting engagement, depression, stress, and other emotion-related state in the field of affective computing and sensing technologies.

Inferring human mental state (e.g., emotion, depression, engagement) with sensing technology is one of the most valuable challenges in the affective computing area, which has a profound impact in all industries interacting with humans. The self-report survey is the most common way to quantify how people think, but prone to subjectivity and various responses bias. It is usually used as the ground truth for human mental state prediction. In recent years, many data-driven machine learning models are built based on self-report annotations as the target value. In this research, we investigate the reliability of self-report surveys in the wild by studying the confidence level of responses and survey completion time. We conduct a case study (i.e., student engagement inference) by recruiting 23 students in a high school setting over 4 weeks. Our participants volunteered 488 self-reported responses and data from their wearable sensors. We also find the physiologically measured student engagement and perceived student engagement are not always consistent. The findings from this research have great potential to benefit future studies in predicting engagement, depression, stress, and other emotion-related states in the field of affective computing and sensing technologies.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}
\ccsdesc[500]{Human-centered Computing~Ubiquitous and mobile computing}
\ccsdesc[300]{Applied computing}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Self-report measures, machine learning, prediction, reliability, ground truth, human-based study}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

%With the coronavirus (COVID-19) outbreak, the impacts of the measures necessary to contain its spread were likely to negatively impact mental health \cite{australian}. The sudden loss of employment, social interaction, localised 'lockdowns', as well as the pressure of remote work or schooling have affected the mental health of people all over the world. Brooks et al. \cite{brooks2020rapid} pointed that stress, confusion and anger are commonplace as a result of pandemic and WHO \cite{who2021} indicated that, though some people may not experience long-term concerns, COVID-19 may cause or exacerbate long-term mental illness, including anxiety, depression, post-traumatic stress disorder (PTSD), etc. 
%Introduce the prevalence of affective computing and the common approaches for prediction
%https://www.aihw.gov.au/reports/mental-health-services/mental-health-services-in-australia/report-contents/mental-health-impact-of-covid-19#References

In recent decades, with the advances of wearables and IoT devices, sensing technologies have been increasingly investigated to infer human emotion and mental characteristics, which becomes a hot topic in the Ubicomp community, especially surrounding the prediction of mood \cite{Moodexplorer,morshed2019prediction}, depression \cite{wang2018trackingdepression,xu2019leveragingdepression}, stress \cite{king2019microstress}, engagement \cite{gao2020n,huynh2018engagemon,di2018engagement}, concentration \cite{rahaman2020ambient}, personality traits \cite{personalitysensing2018wang,gao2019predicting} etc. Understanding human emotion and mental state with sensing technologies in real-time can help design intervention strategies to prevent mental health issues among people. 

One of the most commonly used methods for measuring emotion and mental state is to ask participants to respond to self-report surveys (e.g., \cite{gao2019predicting,di2018engagement,gashi2019using}). An alternative to self-report survey is the Ecological Momentary Assessment (EMA), which is designed to repeatedly collect human responses in real-time in natural settings. In the emotion-sensing area, the responses from self-report survey or EMA are usually regarded as the measure of \textit{ground truth} \cite{king2019microstress,di2018engagement,gao2020n,wang2014studentlife,Moodexplorer} when building the machine learning (ML) prediction model. They are usually served as the target variables while the features extracted from sensing data are used as the predictor in ML contexts. Then, the predictor is usually mapped to the target variables through the empirical relationship determined by the data. Moller et al. \cite{moller2013investigating} pointed out researchers should not trust the self-reports blindly, but take into consideration that the responses can be unreliable.


%Imagine you are doing a self-report survey. You think and report your answers carefully at first. After answering several questions, you feel bored and want to finish the survey as soon as possible. Then you quickly read the questions and choose one answer randomly. Once you submit, you feel relaxed, as if you have thrown off a big baggage. 

%Why self-report survey is important and popular?


%Predicting human attitude/behaviour using self-report survey is risky...
%https://www.mercuryds.com/blog/using-self-report-survey-data-to-predict-human-psychology-is-tough

%The performance of ML model based on self-report surveys are usually low, one of the reason is... Common reasons for the inaccuracy of self-reports are as follows: 
%1.
%2.
%3.
%In this paper, we will focus on * part.

%Reliability of self-report survey in the lab vs wild.



%We are the first to study how the 
In this research, we investigate the reliability of self-report survey by investigating the patterns of the reported confidence level and survey completion time. Then we focus on the emotion-sensing area and use the learning engagement as an example to compare the physiologically measured engagement and perceived engagement. We conduct a field study in a private high school, and 488 self-report responses and wearable data are collected from 23 student participants over 144 classes and 10 subjects for 4 weeks. 
Our contributions are as follows:
\begin{itemize}
    \item For the first time, we investigate the reliability of self-report surveys by studying the confidence level of survey responses. Then we  compare the confidence level of responses with the survey completion time to better understand the reliability of self-report surveys.
    %\item For the first time, we find the correlation between the physiological signals between the reliability of surveys. 
    %\item To the best of the knowledge, we are the first to compare the perceived student engagement and physiologically measured engagement. We find the perceived engagement are not always consistent with the physiologically measure engagement. Participants with similar physiological patterns may have different perceived engagement annotations while participants with similar annotations may have very different physiological patterns. We also explore how the different factors affect the physiologically measured engagement and perceived engagement.
    
     \item Taking the student learning engagement as an example, we find the perceived student engagement and physiologically measured engagement are not always consistent.%. Specifically, we identify if the perceived engagement are consistent with the physiologically measure engagement. We also explore how the different factors affect the physiologically measured engagement and perceived engagement.
     
    \item We point out the risk of using subjective annotations as the ground truth and discuss the possibility to use physiological signals as objective measures of student engagement. 
    %To the best of our knowledge, we are the first to address that we should treat the physiological signals (i.e., EDA data) as the objective measures of student engagement since the annotation data may be not reliable. We find that even the students with similar patterns of EDA signals may report very different perceived engagement annotation scores. 
    %\item We compared the perceived and physiologically based engagement, and found they are not always consistent. We discussed the risk of using subjective annotations as the 'ground truth', and explored the possibility and feasibility of using physiological signals as objective measures of student engagement. The findings from this research has great potential to benefit the future studies in predicting engagement, depression, stress, and other emotion related state in the field of affective computing.
    %\item We proposed a model to predict the reliability of self-report responses using sensing data. After intensive experiments on several public dataset, we found that the prediction performance of human behaviours/attitudes are improved after the reliable responses are selected as the ground truth. 
\end{itemize}

%The reminder of the paper is as follows. In Section \ref{sec:relatedwork}, we reviewed the literature in the reliability of self-report surveys and emotion sensing technologies. Section \ref{sec:Data collection} introduces the procedure of data collection and measures of different items. Then in Section \ref{sec:reliability}, we investigate the reliability of self-report survey by understanding the confidence level of subjective responses, and the contrasting perceived engagement and physiologically measured engagement. Section \ref{sec:conclusion} discusses the findings and points out future directions of emotion sensing area.

\section{Related Works}
\label{sec:relatedwork}

\subsection{Inferring Emotion and Mental State with Sensing Technology}

In Ubicomp community, many studies have assessed human emotion and mental characteristics with sensing technologies (e.g., engagement \cite{gao2020n,huynh2018engagemon}, stress \cite{king2019microstress}, mood \cite{morshed2019prediction,wang2014studentlife}, depression \cite{bakker2011what,wang2018trackingdepression}), which provide an attractive alternative to traditional surveys or EMAs.  King et al. \cite{king2019microstress} proposed a passive sensing framework for detecting pregnant mothers in the wild, with the micro-EMA questions as measurable ground truth for stress. Similarly, Gao et al. \cite{gao2020n} predicted student learning engagement with physiological sensing data, with the adapted In-class Student Engagement Questionnaire (ISEQ) \cite{fuller2018development} as the ground truth of learning engagement. Wang et al. \cite{wang2018trackingdepression} tracked depression dynamics in college students using  mobile and wearable sensing approaches, with PHQ-4 \cite{kroenke2009phq4} and PHQ-8 \cite{kroenke2009phq8} scores as the ground truth of depression. Zhang et al. \cite{Moodexplorer} detected the human compound emotion from smartphone sensing data with the self-report responses as the ground truth of emotions. It has become a common practice to regard the subjective responses (e.g., EMA, traditional survey) as the ground truth, and features extracted from sensing data are fed into the data-driven model for emotion and mental state prediction.

\subsection{Reliability of the Self-report}
Many researchers worked on designing or adapting psychology questionnaires to achieve higher validity and reliability and mitigate response biases \cite{barclay2002not,jackson2018stepovers,sonderen2013ineffectiveness, clark2019constructing,hudson2020comparing}. Clark et al. \cite{clark2019constructing} reviewed recent literature for psychological scale validation and Huston et al. \cite{hudson2020comparing} compared the reliability of different forms of self-reported life satisfaction.
Moller et al. \cite{moller2013investigating} explored the reliability of self-reporting responses under different conditions. They conducted a six-week self-reporting study on smartphone usage. They found that self-reports cannot provide the full image of user behaviours and participants could significantly overestimate the duration of app usage. Though they showed the inaccuracy of self-reports, they gave suggestions for the design of a self-report study (e.g., set reminders, not overcharge participants ) instead of solutions to evaluating the reliability of self-reports. Moreover, they used the survey questions related to real-world behaviour (e.g., smartphone usage) which is easier to be quantified compared with subjective attitudes. 

Wash et al. \cite{wash2017can} investigated the agreement between self-report and behaviours. They found that security research based on self-reports is unreliable for certain behaviours. Especially, when the behaviour involves awareness rather than actions, people are less able to answer the questions accurately. Similar to \cite{moller2013investigating}, they revealed the unreliability of self-reports through comparing with the actual behaviours.  

Different from previous studies, this research has several advantages: (1) we investigate the reliability of self-report survey through the subjective confidence level provided by users; (2) we reveal the risks of using self-report responses as the ground truth, especially for emotion sensing in Ubicomp community, by comparing the physiological measured engagement and perceived engagement.   

%\subsection{Measuring Human Behaviour with Sensing Technology}

%Other researchers worked on design better questionnaires with high validity and reliability. 

\begin{comment}
\section{Research Questions}
\begin{itemize}
    \item RQ1: Is there correlation between quality of responses and survey completion time?
    \item RQ2: Can we infer the quality of self-report responses using EDA data?
    \item RQ3: Can we improve the prediction performance by selecting reliable self-report responses?
\end{itemize}
\end{comment}
\section{Data Collection}
\label{sec:Data collection}
\subsection{Case Study}
We collected a dataset \cite{gao2021ingauge,gao2021understanding} (available on Figshare \footnote{In-Gauge and En-Gage datasets: \href{https://doi.org/10.25439/rmt.14578908}{https://doi.org/10.25439/rmt.14578908}}) from a field study in a high school over 4 weeks  . The study has been approved by the Human Research Ethics Committee at the researchers' institutions, which was furthermore approved by the principal of the high school. We have recruited 23 students (15-17 years old, 13 female and 10 male) and 6 teachers (33-62 years old, 4 female and 2 male) in Year 10. After returning the signed consent forms by teachers and students (and their guardians), the participants were asked to complete an online survey recording their demographic information (e.g., age,  gender, class information, etc.). 

Before the data collection, all \textit{Empatica E4} wristbands  were synchronized with the E4 Manager App from the same laptop to make sure the internal clocks are correct. During the data collection period, student participants were asked to wear the wristband on the non-dominating hands at school time. They were reminded by the class representative to complete online surveys three times a day at 11:00, 13:25, 15:35 (right after the 2$^{nd}$, 4$^{th}$, 5$^{th}$ class). For teacher participants, they only need to wear the wristband during their classes and complete online survey right after their class. 

As a token of appreciation, participants were distributed with four movie vouchers for 4-week data collection. Participation in this research project was completely voluntary, and participants were free to withdraw from the project at any stage.
\subsection{Measures}
%To test our hypothesis, we were interested in student 3-dimensional engagement, seating location, physical movement and physiological signals in class. 

\subsubsection{Student multi-dimensional engagement} We used the self-report to collect subjective assessments of student engagement. The Self-report is the most commonly used method to measure student engagement because it can reflect subjective perceptions of students. According to previous studies \cite{fredricks2004school,fredricks2012measurement}, other methods such as interviews, teacher ratings and observations are vulnerable to external factors. The student engagement questionnaire includes 5 items \footnote{Specifically, the questions are: (1) I paid attention in class; (2) I pretended to participate in class but actually not; (3) I enjoyed learning new things in class; (4) I felt discouraged when we worked on something; (5) I asked myself questions to make sure I understood the class content. Questions 1,3 and 5 assess the behavioural, emotional and cognitive engagement respectively, where items 2 and 4 indicate the
behavioural and emotional disaffection   \cite{fuller2018development,skinner2009motivational}.} related to the emotional, behavioural, and cognitive engagement of the validated In-class Student Engagement Questionnaires (ISEQ) \cite{fuller2018development}, which was proved to be effective for multidimensional engagement measurement compared with the traditional long survey. Similar to previous studies \cite{huynh2018engagemon,gashi2019using}, we slightly adapted the survey questions to suit high school classes and make it easier for underage students to understand. 
%In addition, we did not adopt the original question ‘the activities really helped my learning of this topic’ for assessing cognitive engagement in \cite{fuller2018development}, because some high school classes do not have in-class activities. Instead, we use the well-accepted question ’I asked myself questions to make sure I understood the class content’ \cite{moore2006children}, which has been proven to be a good reflection of cognitive engagement. 
In the questionnaire, each item is rated with a 5-point Likert scale from ’strongly disagree’ to ’strongly agree’.



\subsubsection{Confidence level} At the end of the self-report EMA, we asked the participants to write down their confidence level for the survey response: "Please rate your confidence level for your answers in this survey (optional)". Then the participant need to choose their option from the 5-point Likert scales, where 1 = Not confident, 2 = Slightly confident, 3 = Moderately confident, 4 = Very confident, 5 = Extremely confident. The default option is 3: Moderately confident. We set this question as an optional, not mandatory question, to minimize the possibility of users answering questions randomly. 


%Imagine you are doing a self-report survey. You think and report your answers carefully at first. After answering several questions, you feel bored and want to finish the survey as soon as possible. Then you quickly read the questions and choose one answer randomly. Once you submit, you feel relaxed, as if you have thrown off a big baggage. 
\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.38\textwidth]{image/EDA.png}
    \caption{Graphical representation of EDA components}
    \label{fig:edaexample}
\end{figure}
\end{comment}
\subsubsection{Physiological signals}

We assessed participants physiological signals (EDA, PPG, ACC, ST) signals using the \textit{Empatica E4} wristbands. PPG sensors measure the blood volume pulse (BVP) at 64 Hz, from which the inter-beat interval (IBI) and heart rate variability (HRV) can be derived. ACC sensors record 3-axis acceleration at 32 Hz to capture motion-based activities. The optical thermometer captures peripheral skin temperature (ST) at 4 Hz.
EDA sensors record the constantly fluctuating changes in the electrical properties of the skin at 4 Hz. When the level of sweat increases, the conductivity of the skin increases. For most people, when they experience increased cognitive workload, emotional arousal or physical exertion, the brain will send innervating signals to the skin to increase the sweat levels. Even though they may not feel any sweat on the skin surface, the conductivity increases noticeably. 



%EDA complex includes two main components: general tonic components (Skin Conductance Level, SCL) and rapid phasic components (Skin Conductance Response, SCR) resulted from sympathetic neuron activity \cite{guideeda}. The SCLs relate to the slower acting and background characteristics of the EDA signal (overall level, slow declination or climbing over time), reflecting the general sweat glands influenced by the autonomic arousal. The SCRs usually show up as abrupt increases in the conductance of skin, which are usually associated with short-term events and external/internal stimuli. Figure \ref{fig:edaexample} shows the graphical representation of EDA responses to a hypothetical stimulus, including three stages: latency, rise time and recovery time.

\section{Reliability of Self-reports}
\label{sec:reliability}

%In this section, we first investigate the overview of response reliability, including the distribution of confidence level across all self-report responses and all participants. Then we study the relationship between survey completion time and response reliability.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{image/count_con.pdf}
    \caption{Distribution of confidence level for all self-report responses}
    \label{fig:dis_con}
\end{figure}

\subsection{Confidence Level of Responses}

During the data collection, the confidence level of self-report responses from different participants are collected.
Figure \ref{fig:dis_con} shows the distribution of confidence level of different participants. We can see that most participants are moderately confident about their responses, however a few participants (confidence level is 1 or 2) are not very much confident about their responses. 



\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{image/box_par_con.pdf}
    \caption{Confidence level across different participants}
    \label{fig:box_confi}
\end{figure}

Then, we investigate whether the same participant tends to have the same confidence level. Figure \ref{fig:box_confi} shows the boxplot of confidence level across different participants. We find that different participants tend to have different confidence level. For example,  some participant (e.g., P1, P20) are usually strongly confident (>4) about their survey responses, where some participants (e.g., P10, P12, P15) are usually not very confident about their survey responses. Additional, some participants (e.g., P1, P20, P15) tend to have a similar confidence level across the longitudinal surveys but some participants (e.g., P16, P3) have very different confidence level during different time in the data collection. The above phenomenons are in line with our daily experience.




\subsection{Survey Completion Time and Reliability}

Malhotra et al. \cite{malhotra2008completion} found that the survey completion time is one of the indicators of response quality, even though it can be affected by multiple factors and varies from person to person. In this research, for each self-report EMA, we collect the survey completion time automatically recorded by the \textit{Qualtrics} timing question, which is the hidden question added in the survey to track the time a respondent spends on that page. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{image/box_par_time.pdf}
    \caption{Survey completion time of participants}
    \label{fig:box_time}
\end{figure}

Figure \ref{fig:box_time} shows the survey completion time for all participants. We can see that different participants have very different survey completion time. Most participants complete the survey between 30 to 50 seconds, however, some participants (e.g., P17) spend a lot more time to complete the survey and some participants (e.g., P10, P12) complete the survey in a very short time.

%Most participants complete the survey in 30 to 50 seconds, but some participants complete the survey in less than 15 seconds. Though the survey completion time may be affected by many factors and varies from person to person, it is still one of the indicators of response quality \cite{malhotra2008completion}.


\begin{figure}
    \centering
    \includegraphics[width=0.50\linewidth]{image/timeregression.png}
    \caption{Linear regression of survey completion time with confidence levels }
    \label{fig:regression}
\end{figure}

%In future research, it will be interesting to explore patterns from survey completion time data and assign appropriate weights to survey response for more accurate prediction of student engagement. 
Then we study whether the survey completion time is correlated with the confidence levels. Figure \ref{fig:regression} shows that the survey completion time is positively related to the confidence level. Participants with higher survey completion time tend to have a higher confidence level of the survey. We also investigate how the confidence level correlated with other factors such as the time of the day and weekday, but we do not find the strong correlation between them. In future research, it will be interesting to use survey completion time as an indicator of survey reliability and assign appropriate weights to self-report responses for more accurate human mental-state prediction.
\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{image/result.pdf}
    \caption{Prediction performance across different classifiers}
    \label{fig:my_label}
\end{figure}
\end{comment}

%\section{Perceived and physiologically measured engagements}
%\label{sec:contrasting}
%Here, we discuss the risk of using subjective annotations as 'ground truth', using student engagement as the case study. Previously, researchers use the perceived mental state (e.g., emotion, engagement, stress, depression) as ground truth and predicting the perceived state from sensing data \cite{gao2020n,king2019microstress,personalitysensing2018wang}. In this section,  to explore the difference between perceived engagement and physiological measured engagement. 

\subsection{Perceived vs Physiologically Measured Engagement}

For the calculation of the perceived engagement score, we reversed the responses in item 2 and item 4 and then calculated an average score based on the 5-point Likert scale for each dimension of engagement. Then the overall engagement scores were calculated based on all the five items, where 1 means the lowest engagement and 5 means the highest engagement. Figure \ref{fig:dis_engage} shows the distribution of overall perceived engagement across student participants. We can see that different participants tend to have very different perceived engagement. Some participants (e.g., P1, P9, P14) usually highly engaged in the class while some participants (e.g., P8) usually have low engagement level. Gao et al. \cite{gao2020n} built the engagement prediction model with the perceived engagement is regarded as the ground truth.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{image/s_bar_score_daily2.pdf}
    \caption{The distribution of overall engagement across student participants}
    \label{fig:dis_engage}
\end{figure}

%\textbf{H1:} Different students tend to have different seating preference.


%Figure \ref{fig:loc_dis} displays the seating location across different students participants. We can find that different participants tend to have very different seating preferences. For instance, participant P22 usually seats near the whiteboard while participant P21 tend to seat in the back of the classroom facing the whiteboard.

%\subsection{Physiologically measured engagement}

\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{image/multi_eda.png}
    \caption{An example of the EDA changes for three different participants P15, P17, P20 in the same class (their perceived engagement are 4.2, 3.2, 4.4)}
    \label{fig:multi_eda}
\end{figure}

\begin{comment}
\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{image/course_anno.pdf}
    \caption{The impact of course subjects on the perceived self-report engagement}
    \label{fig:impact_subject_anno}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{image/course_e4.pdf}
    \caption{The impact of course subjects on the physiological-measured engagement}
    \label{fig:impact_subject}
\end{figure*}

\end{comment}

%\subsubsection{The impact of class length}

%We studied the impact of class length on the perceived engagement and physiological-measured engagement. 

Physiological signals (e.g., EDA, HR, ST signals) have been explored in previous studies to infer student engagement level \cite{gao2020n,di2018engagement}. For example, the EDA level is usually considered  a good indicator of physiological and psychological arousal (e.g., student engagement \cite{gao2020n}, emotional state \cite{di2018engagement}). Increased heart rate indicates the increased efforts and is used as an indirect measure of engagement \cite{richardson2020engagement}. It has been shown that changes in heart rate are related to greater mental efforts and higher information processing demands. The standard deviation of heart rate. Additionally, changes in skin temperature have been showed to be correlated with social and mood context \cite{ioannou2014thermal}. 

We show an example of EDA changes for different participants in a same class in Figure \ref{fig:multi_eda}. It can be seen that the EDA signals of the first two participants are very similar and there is a strong physiological synchrony  \cite{palumbo2017interpersonal} between them. Physiological synchrony refers to the association or interdependence of physiological activity between two or more individuals, which has been found in many scenarios. Physiological synchrony between individuals can be a indicative of group engagement \cite{palumbo2017interpersonal}, and has been used to measure the classroom emotional climate \cite{gashi2018using} and quantify participants' agreement on self-report engagement \cite{gashi2019using}. 

In  Figure \ref{fig:multi_eda}, strong physiological synchrony between P15 and P17 indicates they have similar engagement patterns. Additionally, they both are likely to be highly engaged because (1) their EDA signals have multiple peaks at a similar time, which is a good indicator of emotion arousal; (2) if they are not engaged in class, their EDA changes should be more random instead of being similar. What's more, participant P20 is likely to have lower engagement than participants P17 and P20 since the EDA signals of P20 is more random and the number of peaks is not as many as that of the other two participants. However, based on the self-report responses, the engagement score of three participants P15, P17 and P20 are 4.2, 3.2 and 4.4. From this example, we find that (1) participants with very similar physiological patterns may have very different perceived engagement annotations (see P15 and P17); (2) participants with very similar annotations may have very different physiological patterns (see P15 and P20).


%However, the perceived engagement score for the three participants are 4.2, 3.2, 4.4. 

%Obviously, the annotations are not reliable since both the first two participants should have high engagement. Based on literature, there are two another reasons to explain why we use EDA signals as ground truth: a) EDA signal is a good indicator of emotion arousal (2) Higher physiological synchrony of EDA indicates higher engagement similarity.
%To study the perceived engagement and physiologically measured engagement on all participants, similar to Richardson et al. \cite{richardson2020engagement}, we adopt four metrics to indicate the physiologically measured engagement: average and standard deviation of HR level, average EDA level and average ST. During the data processing, all physiological signals are normalised for each participant to remove the baseline differences (e.g., resting heart rates) between individuals, allowing us to focus on the differences between multiple factors and the engagement of each participant.

\begin{comment}
\subsection{Impact of Different Factors}

We compare the perceived engagement and physiologically measured engagement with two different factors: course subjects and class length. For course subjects, we choose the courses with the most self-report responses: Maths, Language, English, Politics, Science and Health. For class length, there are two types of length: long classes (80 minutes) and short classes (40 minutes).

\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{image/len_anno.pdf}
    \caption{The impact of class length on the perceived self-report engagement}
    \label{fig:impact_lent_anno}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{image/len_e4.pdf}
    \caption{The impact of class length on the physiological-based engagement measurement}
    \label{fig:impact_len}
\end{figure*}

\subsubsection{Impact of course subjects}

Figure \ref{fig:impact_subject_anno} shows the impact of different course subjects on the perceived engagement measurement. We can find that students have the highest engagement on the English course and lowest overall engagement on the Politics course. Then we display the impact of different course subjects on the physiological-based engagement measurement in 
Figure \ref{fig:impact_subject}. Interestingly, students have the highest level of all the physiological-based engagement metrics in the Health class. Additionally, students seem to be highly engaged in the Maths class since they have a higher EDA than the other classes except the Health class. When we look at the Figure \ref{fig:impact_subject_anno} and Figure  \ref{fig:impact_subject} together, we can find that they are not consistent and we may draw different conclusions of student engagement in different course subjects.


\subsubsection{Impact of class length}

Figure \ref{fig:impact_lent_anno} shows the impact of different class length on the perceived engagement measurement. It seems like students have higher multidimensional engagement on short classes based on their self-report responses. Then we show the impact of class length (i.e., long and short) on the physiologically measured engagement in Figure \ref{fig:impact_len}. We find that students have higher average heart rate, STD of heart rate and average EDA level in short classes while lower skin temperature than in long classes. The possible reasons may be two folds: (1) students physiological-measured engagement are not in line with their perceived engagement; (2) the average skin temperature may not be the good metrics for inferring the student engagement and more effective physiological metrics should be explored in the future.


\end{comment}

%\subsection{Impact of Behavioural Factors}

%\subsection{Impact of Other Factors}

%\subsubsection{The impact of course subjects}
%We studied the impact of course subjects on the perceived engagement and physiological-measured engagement. 


%\section{Studying the Physiological Changes during the Self-report Survey}


%\section{Result}


\section{Discussion and Conclusion}
\label{sec:conclusion}
The self-report is one of the most common ways to study the human psychological state and attitude in human-based studies. In the affective computing area, self-report annotations are usually served as the ground truth for predicting human mental state with sensing technologies. Especially in recent years, various data-driven machine learning models are built with self-report annotations as the target variable. However, self-report annotation is prone to  subjectivity and various responses bias, making it risky and inaccurate to be used as the ground truth in predicting the psychological state (e.g., emotion, depression, engagement, etc.) from sensing data.

In this research, we investigate the reliability of self-report responses in the wild from two aspects: (1) For the first time, we study the confidence level of self-report responses from participants, and compare the confidence level with the survey completion time to better understand the reliability of self-reports; (2) To the best of our knowledge, we are the first to compare the perceived and physiologically measures of student engagement. We find that the perceived self-report engagement are not always consistent with the physiologically measured engagement. Participants with similar physiological patterns may report very different perceived engagement and participants with similar self-report annotations may also have very different physiological patterns. By contrasting the self-report and physiological measures, we reveal the penitential risks of only using subjective annotations as the ground truth. %The findings from this research have great potential to benefit future research in the mental health and emotion sensing area. 

%Different factors are explored to study how they affect the physiologically measured and perceived engagement at the same time.


%To the best of our knowledge, we are the first to address that we should treat the physiological signals (i.e., EDA data) as the objective measures of student engagement.



This research is a very promising step towards the study of reliability of self-report in the wild. It serves as a wake-up call for the emotion and mental sensing research in the Ubicomp community which usually regards the self-report annotations as the ground truth for predicting human mental state. Why do students feel more engaged in class if their bodies say otherwise? Should we trust their subjective self-report responses more, or their objective physiological responses? Is there a better way to understand and model human mental state instead of only using self-report annotations as the ground truth? We hope that more research will be done to explore this issue in the future.



\section{Acknowledgments}

This research is supported by the Australian Government through the Australian Research Council's Linkage Projects funding scheme (project LP150100246).

\bibliographystyle{ACM-Reference-Format}
\bibliography{Nan.bib}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
