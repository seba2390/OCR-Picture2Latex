\section{Evaluation}
\label{sec:evaluation}

The goal of our evaluation is to study several aspects of DF and \sys. We start by describing the workflow applications (\S\ref{ssec:eval-benchmarks}). We then formulate the research questions (\S\ref{ssec:rq}), and present the results (\S\ref{ssec:eval-programmability}--\ref{ssec:eval-scaleout}).

\subsection{Workflows}
\label{ssec:eval-benchmarks}

\hide{
\kk{It might be a good idea to have a small illustration of each workflow in a figure to better understand their structure.}
\dajusto{+1, it's currently kinda abstract. Also, wouldn't this be better introduced \textit{before} the research questions and their results summary?}
\kk{I think that the benefit of having the questions and the result summary in the beginning makes it easy for someone to follow and know what to expect. Also being early helps someone get an idea of what we show without even looking at the evaluation in detail. IMO the specifics of the workflows are not relevant to that discussion could therefore be delayed.}
\dajusto{@kk I could put something together here if you could share the code that orchestrates these example.} \seb{We need to keep it short. Let's try to make the textual description as clear as possible. \dajusto{These are much clearer now, I'd say we're good on this.}}
}
We use five representative workflows that vary in complexity and execution characteristics. The first two workflows correspond to sequences of tasks, the third is a workflow that performs a transaction between two bank accounts and thus requires atomicity guarantees, and the other two workflows are taken from real applications, an image processing application, and a database snapshot obfuscation.

\resultpar{Hello Sequence.}
A very simple "hello world" workflow that calls three functions in sequence. Each function returns a hello message, and the workflow then returns the concatenation of those messages.
%

\resultpar{Task Sequence.}
%
A sequential workflow that initializes an object and then passes it through a sequence of processing steps. It is similar to the hello sequence, but the length of the sequence is not fixed, but given an an input parameter.
\hide{
\kk{Can we add something here that justifies why we picked the two sequence workflows? I think the reason is that they only have light computation in their tasks, therefore allowing us to measure the "true" performance of our engine and the existing solutions without noise from the execution of tasks.}
\dajusto{That explanation makes sense to me. You could add this bit in the first paragraph of this section. Something like: "The first two workflows are \textit{simple} sequential task compositions, they allow us to reason about the performance of Netherine by minimizing the noise from task execution time. The third is a ..." }
}

%The length of the workflow, \hl{the size of the object}, and the amount of work that is done to it by each task can be configured depending on the experiment. The processing work that each task performs on the object is a loop that \hl{searches for hash collisions} to simulate CPU intensive work.

\resultpar{Bank Application.}
The workflow from Fig.~\ref{fig:transfer} that implements a reliable transfer of currency between accounts. This workflow showcases the capabilities of the Durable Functions programming model since it cannot be implemented with existing solutions.
%
% \hide{
% \kk{Showcases the capabilities of the programming model. How we envision it to be used. Requires reliability and high throughput and low latency.}
% }
% \kk{For a future version of the paper we could also implement the banking application using storage and stateless services on Azure and compare its performance with that too.}
% \csm{Should you highlight here that this isn't possible with the existing serverless/FaaS solutions on the market? (It seems like you do later, wasn't sure if you should earlier.)}
% \kk{That makes sense!}

\resultpar{Image Recognition.}
%
A workflow that recognizes objects in a given picture and creates a thumbnail for it. It is part of a bigger image processing application\footnote{Source at: \url{https://github.com/aws-samples/lambda-refarch-imagerecognition}}. %that also contains a web app frontend that allows a user to login their account and upload pictures to albums, triggering the workflow for each picture that is uploaded. 
The workflow performs the following steps, each of which is implemented as a separate AWS lambda. It first reads the image metadata from the S3 bucket where it is stored. If the image extension is supported, it filters out the unnecessary metadata and then runs two steps in parallel: one that performs object detection using Amazon Rekognition, and one that generates a thumbnail of the image. When the processes complete, it persists the filtered metadata in a DynamoDB table. The workflow repeatedly retries all steps until they succeed. 
% The original code of the workflow in AWS Step Functions can be seen in the \hl{appendix}. 

\resultpar{Database Snapshot Obfuscation.}
%
This workflow is taken from a real application used for database snapshot obfuscation\footnote{Source at: \url{https://github.com/FINRAOS/maskopy}}. The workflow state machine contains 27 states that interact with a variety of AWS services. Some of the tasks that it calls include user authorization, creation of database snapshots, validation of the snapshots, obfuscation of the snapshots, and publishing the snapshots in a production environment. 

%This workflow is relatively large and its execution time is dominated by the snapshot management costs, therefore it is only used for evaluating the programming model benefits.



\begin{figure*}[t]
    \centering
    % \begin{subfigure}[t]{\columnwidth}
    %     \centering
    %     \includegraphics[width=\columnwidth]{images/hello-latency-ccdf.pdf}
    %     % \caption{Hello Sequence.}
    %     % \label{fig:hello-latency-ccdf}
    % \end{subfigure}
    % \begin{subfigure}[t]{\columnwidth}
    %     \centering
    %     \includegraphics[width=\columnwidth]{images/seqlong-latency-ccdf.pdf}
    %     % \caption{Task Sequence.
    %     % % \kk{If we have time we should add a StepFunctions implementation of seqlong. Even without it (only with the img-latency) the latency evaluation is still valid I think.}
    %     % }
    %     % \label{fig:seqlong-latency-ccdf}
    % \end{subfigure}
    % \begin{subfigure}[t]{\columnwidth}
    %     \centering
    %     \includegraphics[width=\columnwidth]{images/bank-latency-ccdf.pdf}
    %     % \caption{Bank Application.}
    %     % \label{fig:bank-latency-ccdf}
    % \end{subfigure}
    % \begin{subfigure}[t]{\columnwidth}
    %     \centering
    %     \includegraphics[width=\columnwidth]{images/img-latency-ccdf.pdf}
    %     % \caption{Image Recognition.}
    %     % \label{fig:image-latency-ccdf}
    % \end{subfigure}
    \includegraphics[width=\textwidth]{images/tiling_latencies.pdf}
    \caption{Latency measurements.}
    \label{fig:latencies}
\end{figure*}


\subsection{Research Questions}\label{ssec:rq}

We organize the evaluation and results according to the following questions:
\hide{
\kk{I personally prefer having the research questions before the workflows to help readers understand what they are about to read. Knowing the evaluation questions in advance helps the reader understand why these workflows are good choices for our evaluation.}
\seb{I like to separate the exposition of the benchmark from the results a bit more, by breaking it up with the research questions in between. It is not clear to me that the benchmarks should be chosen with respect to the research questions, they should rather appear like a random sample. }
}

\begin{description}[labelindent=0cm]
    \item[Q1] Does the DF programming model facilitate application development and maintenance?\str
    % \kk{Maybe move part of this (comparison with unstructured composition) in the section where we describe the model.}
    \item[Q2] How does \sys compare with existing solutions with respect to latency, i.e. the completion of a workflow?
    \item[Q3] How does \sys compare with existing solutions with respect to throughput, i.e. the number of workflows that it can execute in a period of time?
    \item[Q4] How does speculation improve latency and how does it impact throughput? 
    % \kk{I am not sure if this should be a question of its own, or whether it should be included in th other sections.}
    % \dajusto{Makes sense, to me, as a separate question, so as it currently is.}
    % \kk{thanks!}
    \item[Q5] Does \sys scale with the addition of available nodes in cases of high-load?
\end{description}
%
% \hide{
% \seb{I commented out the following paragraphs because I think they are redundant. We should should just go right to the results.}

% To answer \textbf{Q1} (\Cref{ssec:eval-programmability}) we implement all of the above workflows in our programming model as well as using unstructured composition (i.e. triggers and queues)
% % \dajusto{By this point, I've forgotten what unstructured composition is. I suggest including a parenthesis with reading "(i.e triggers)" to remind our reader. That or a section reference.} \kk{Good point!}
% and using the programming model that Step Functions offer. We show that unstructured composition can only support the sequence workflows and that Step Functions cannot be used to implement the banking application. Furthermore, we show that even for the workflows that can be supported by the existing solutions, defining workflows implicitly using a programming language offers significant benefits that have been justified by years of programming languages research, such as a type system, function abstraction, and error handling. 

% To answer \textbf{Q2} (\Cref{ssec:eval-latency}) we evaluate the latency of \sys on the above workflows against three  baselines: (i) stateless functions with unstructured composition in both Azure and AWS, (ii) the existing publicly available implementation of Durable Functions, and (iii) the implementation of AWS Step Functions. We argue that these are representative baselines since they are widely used in practice. Concretely, the existing DF implementation is used by \hl{X} users (there are \hl{X} applications implemented and currently running in the existing DF implementation). We show that \sys has (i) orders of magnitude better latency (\hl{x1000-1000}) than unstructured composition of stateless functions, (ii) up to an order of magnitude better latency (\hl{median x38, 95th x43}) that the existing implementation of Durable Functions, and (iii) up to two orders of magnitude better latency (\hl{median x104, 95th x75}) than the implementation of Step Functions. In one of the workflows, \sys performs better than Step Functions even including the communication cost between the Azure and AWS cloud as well as the HTTP communication overhead (as all the stateless functions are AWS lambdas which are invoked directly by Step Functions).

% To answer \textbf{Q3} (\Cref{ssec:eval-throughput}) we evaluate the throughput limits of \sys against the current implementation of Durable Functions since it is the only one that we can manage the available resources for, making the comparison meaningful. We show that \sys can achieve up to an order of magnitude higher throughput compared to the existing implementation of Durable functions.

% To evaluate \textbf{Q4} we use three configurations of \sys for all the latency and throughput experiments: (i) \sys without speculation, (ii) \sys with local speculation, and (iii) \sys with local and global speculation enabled. We show that local speculation improves \hl{median} latency by up to an order of magnitude (\hl{x10}) for long independent workflows and that global speculation improves \hl{median} latency by up to \hl{x5} for heavily communicating workflows. Furthermore, we show that speculation also improves throughput by up to \hl{xN} due to \hl{...TODO...}.

% Finally, to evaluate \textbf{Q5} we conduct an experiment where we measure \sys's throughput while having the system under very high load and increasing the number of available nodes from 1 to 4 and from 1 to 8. We show that \sys can balance the input load on the newly available nodes in \hl{X seconds}.
% }

\paragraph{System infrastructure.} In all experiments other than the ones targeting AWS Step Functions, the system under test was run on a pool of Linux VMs on Azure Kubernetes Service, of type \verb+Standard_DS2_v2+ \cite{azure-vm-sizes}. The number of nodes was 4 (8 for the scale out experiment). Each node had 2 vCPU and a memory limit of 5GB. The queueing service was Azure EventHubs, which is roughly equivalent to Apache Kafka \cite{kafka}, with 32 partitions. The cloud storage was Azure storage GPv2, using premium tier for the FASTER Log Devices. The load was generated by a separate deployment of 20 load generator machines.



\subsection{Programmability Results (Q1)}
\label{ssec:eval-programmability}

To evaluate and compare the development experience when using DF, unstructured composition, or Step Functions, we tried to implement all the workflows from \S\ref{ssec:eval-benchmarks}.
%
\begin{figure}[t]
\begin{minted}[fontsize=\footnotesize]{csharp}
// Execute Image Type Check
string format = extractedMetadataJson.format;
if(!(format == "JPEG" || format == "PNG")) {
    throw new Exception($"image type {format}not supported");
}
\end{minted}
\caption{DF code that checks whether the input image is in a supported format.}
\label{fig:df-image-rec-if}
\end{figure}
%
% \hide{
\begin{figure}[t]
\begin{minted}[fontsize=\footnotesize]{csharp}
try {
    ...
} catch {
    // Catch errors by calling ErrorHandling
    string ErrorHandlingAndCleanupInput = 
            JsonConvert.SerializeObject(inputJson);
    await MakeHttpRequestSync(inputJson.ErrorHandlingURI, 
                              ErrorHandlingInput, context);
    return "Orchestration Failed!";
}
\end{minted}
\caption{DF code that does error handling for the snapshot obfuscation application.}
\label{fig:df-maskopy-error-handling}
\end{figure}
% }

\resultpar{Task Sequence.}
%
With DF, the task sequence can be implemented using a straightforward for-loop that iteratively updates the target object by invoking the task with it. With unstructured composition, the sequence is also relatively simple, but requires that the user also manages and configures a storage or queue service. To our surprise, with Step Functions, it is not possible to express this workflow: the JSON schema for state machines does not support folds, i.e. loops with iteration dependencies. Encoding a loop by restarting the state machine does not work since the invocation API would return after the first iteration terminates. 

%Furthermore, manually implementing loops by restarting the state machine can become quite a burden when implementing more complex iterative control flows such as nested loops.

\resultpar{Image Recognition.}
%
In order to be as faithful as possible to the original Step Functions implementation, we implemented this workflow in DF by invoking the original Lambdas through their HTTP interface, only porting the workflow logic. The code in DF is 70 lines of standard C\#, while the state machine definition in Step Functions is 150 lines of JSON. An interesting difference is the implementation of a check whether the format of an image is supported. In Step Functions, this requires 24 lines of JSON \str 
% \kk{If there is space add the Step Functions code in the appendix.}
compared to a 5 line if statement in DF (Fig.~\ref{fig:df-image-rec-if}).

\resultpar{Database Snapshot Obfuscation.}
%
The workflow in this application is by far the most complex. The state machine definition in Step Functions contains 27 states and is written using 700 lines of JSON; the DF version is more concise and easier to read, with 200 lines of C\# code. An important observation is that there is a lot of copied code in the Step Functions definition since it doesn't support function abstraction. Specifically, the error handling logic, written as 9 lines of JSON, is copied 12 times in the definition, while in DF we just wrap the orchestration with a single \texttt{try-catch}(Fig.~\ref{fig:df-maskopy-error-handling}).

\resultpar{Bank Application.}
%
The bank application simulates bank accounts and reliable money transfers between them. In DF, this is straightforward to implement using entities (Fig.~\ref{fig:entity}) and critical sections (Fig.~\ref{fig:transfer}). We have not yet figured out a satisfactory way of implementing this workflow using unstructured composition or Step Functions, as they do not provide the synchronization primitives needed for concurrency control.
%Most likely, one would use a single database to store account balances and perform transfer transactions. 
% one would have to use an external storage service that allows for transactions to move funds between accounts correctly.


\resultpar{Take away:}
%
DF supports a wider range of applications than Step Functions and unstructured composition, due to its support for workflows with rich control structure, entities, and critical sections. Defining workflows implicitly using a high-level language has several benefits compared to declarative definitions using JSON since it allows for features such as function abstraction and error handling. Maintenance is also improved since there is less copied code and less lines of code in general. Finally, using a high-level language the user gets to enjoy all of its benefits (libraries, type system, IDE support).
% Function abstraction. Error handling. Implicit State. Type system. 
% \hl{Make a note of the development experience. On the one hand you write a state machine in json, having no form of support from the editor, while on the other hand we get linter, type system support for durable functions}.
% compared to step functions. Not complete support as a complete programming language. 
% compared to unstructured compositions (additional expressiveness), the whole application is in one place.

\subsection{Latency Results (Q2, Q4)}
\label{ssec:eval-latency}

In this section we conduct experiments using the workflows described in \Cref{ssec:eval-benchmarks} to evaluate \sys's latency and how it is improved by speculation.

\resultpar{Methodology.}
For all workflows except the snapshot obfuscation, requests are issued at a fixed, low rate (4--25 requests per second) for 3--5 minutes. We then compute the empirical cumulative distribution function (eCDF) of the  system-internal orchestration latency, i.e. the time it takes for an orchestration to complete, using the timestamps reported by the system. We chose to use the system-reported latency of workflows, as opposed to the client-observed latency, because not all clients provide a way to wait for the completion of a workflow.
%
\hide{
% \kk{Should we justify why the latency differences for hello sequence are so small? I don't think so.}
}

Latency results for four of the five workflows are shown in Fig.~\ref{fig:latencies}. For the snapshot obfuscation workflow, there is no appreciable performance difference between the implementations; the total latency (20-25 minutes) is dominated by executing the time-consuming tasks (taking a snapshot, obfuscating it, restoring the database from a snapshot, etc).

 
\resultpar{Unstructured Composition.}
%
Unstructured composition (using triggers and queues) can only be used to implement the Task Sequence workflow. As can be seen in Fig.~\ref{fig:latencies}, triggers\footnote{Blob in Azure and S3 in AWS.} suffer significantly higher latencies (\hl{x1000-x10000}) than \sys. Using queues for constructing sequential workflows performs better than triggers but \sys still achieves an order of magnitude lower latencies (\hl{median x61, 95th x91}).

\resultpar{Step Functions.}
%
Step Functions does not support the bank application and the task sequence so they are not included in that experiment. For the other two workflows \sys achieves better latencies (hello sequence: \hl{median x104, 95th x75}). An important take-away is that \sys achieves lower latency in the image recognition experiment even though \sys is deployed on Azure and invokes AWS lambdas as its tasks using their HTTP interfaces, while AWS Step Functions invoke the lambdas directly (avoiding both the network back and forth and the HTTP overhead).
\hide{\kk{Add a note that Step Functions code is not available and thus we cannot explain the results.}}

\resultpar{Durable Functions.}
%
Compared to the existing implementation of Durable Functions, \sys achieves better latency in all experiments, even without speculation. The optimized \sys implementation achieves \hl{x38}, \hl{x4.3}, \hl{$17\%$}, improvements in median and \hl{x43}, \hl{x4.7}, \hl{$29\%$} improvements in 95th percentile latency than the existing implementation in the task sequence, bank, and image recognition workflows respectively.

\resultpar{Speculation Benefits (Q4).}
%
The benefits of speculation are apparent in all plots of Fig.~\ref{fig:latencies}. In general, the improvement is cumulative, with two exceptions: local speculation does not improve latency for the Bank Application since there is a lot of communication among workflows and entities, and global speculation does not improve latency for task sequence and image recognition since their workflows stay within partitions. In image recognition, the speculation benefits are small because the biggest factor of the workflow latency is the execution time of the image recognition. In total, median latency for the sequence experiment is improved  by \hl{x21} (95th \hl{x17}) with speculation, the median latency for the image recognition experiment is improved by \hl{$6\%$} (95th \hl{$5\%$}) due to speculation, and finally the median latency for the bank experiment is improved by \hl{x3} (95th \hl{x2}) using global speculation.

\resultpar{Take away:}
%
\sys achieves better latencies than all other solutions in all of our experiments. Speculation significantly improves \sys's latency. For a workflow taken from an AWS application, \sys achieves better latency than Step Functions even though it pays communication and HTTP costs due to being deployed in Azure and calling stateless functions deployed in AWS.

% \paragraph{Image Recognition}
% Note that invoking the original lambdas through their HTTP interfaces involves some additional overhead compared to the Step Functions implementation that directly invokes them. The full code can be found in the \hl{appendix}.

% \paragraph{Experiment Setup} 
% %
% We evaluate \sys using two experiments. First, we execute the workflow for a single image to measure the latency of our system when underutilized. Second, we execute a workflow for each of \hl{200} images of average size \hl{X MB} in parallel to measure the throughput of \sys. 

\subsection{Throughput Results (Q3, Q4)}
\label{ssec:eval-throughput}

% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{images/hello-throughput.pdf}
%         % \caption{Hello Sequence. Averaged over 2 runs.}
%         % \label{fig:hello-throughput}
%     \end{subfigure}
%     \begin{subfigure}[t]{\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{images/bank-throughput.pdf}
%         % \caption{Bank Application.}
%         % \label{fig:bank-throughput}
%     \end{subfigure}
%     \caption{Throughput Measurements. Left: Hello Sequence. Right: Bank Application.\str
%     % \kk{We should either average both over 2 runs or none of them.} 
%     \kk{Are the drops explainable? Maybe we could just cut after 200 seconds?}    
%     \seb{My plan was to not show these curves at all but only a bar chart with the average throughput in a windows without drops, this is the tab "Throughput" in the PowerBI doc}
%     \kk{I think I prefer having these plots rather than just a number since they look pretty (except for the drop). If we don't link them I can replace them with a table with the results.}}
%     \label{fig:throughputs}
% \end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/throughput-bars.pdf}
    \caption{Throughput Measurements. \str
    % \kk{I think the other plots look a bit cooler :)}
    }
    \label{fig:throughputs}
\end{figure}

In this section we conduct experiments to evaluate \sys's throughput and how is it impacted by speculation.

\resultpar{Methodology.}
For the throughput experiments, we control the load by controlling the number of request loops that are running on the load generators. We determine a suitable load level by ramping up the load until we can visually discern saturation, indicating that a further load increase will not improve throughput. We then keep that load steady for a minute and compute the average throughput.

We only compare against the existing DF implementation because it is available on Github\footnote{\url{https://github.com/Azure/azure-functions-durable-extension}} and thus we could deploy it with the exact same resources as \sys.

We did not include image recognition and snapshot obfuscation since their throughput limits are bounded by the throughput limits of external services that they use\footnote{AWS Rekognition has a limit of 50 invocations per second and the snapshot obfuscation workflow takes 20-25 minutes to complete.}. We did not include throughput measurements for task sequence because its results are very similar to the Hello Sequence. Throughput results are shown in Fig.~\ref{fig:throughputs}.

\resultpar{Durable Functions.}
%
The HTTP plots correspond to executions where the invocations where done through HTTP, consuming some resources.
\sys without speculation improves the throughput over the existing DF implementation by x7.5 for hello sequence and by x2 for the bank application.
Throughput improvement for the bank application is smaller, presumably because there is much inter-partition traffic and less batching per node.

\resultpar{Speculation (Q4).}
%
To measure speculation improvement on throughput more accurately, we invoke the workflows without HTTP. Speculation slightly improves throughput in both experiments: for Hello Sequence ($10\%$ with local, $8\%$ with global), for Bank Application ($6\%$ with local, $13\%$ with global). It is not immediately clear why global speculation improves throughput of the bank application, as it performs strictly more work per orchestration. We believe the reason is that the much lower latency (almost \hl{x5}) means each workflow spends less time in the system, leading to emptier queues, less memory consumption, and less GC overhead.

\resultpar{Take away:}
%
\sys achieves close to \hl{x8} the throughput of the existing DF implementation. Speculation does not negatively impact throughput, but slightly improves it.

\subsection{Scale-out Results (Q5)}
\label{ssec:eval-scaleout}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/scaleup-throughput.pdf}
    \caption{Scale out throughput plot. Each line is averaged over 3 runs to reduce noise.}
    \label{fig:scaleout}
\end{figure}

In this section we conduct an experiment to evaluate if \sys can scale out with the addition of nodes.

\resultpar{Methodology.}
%
For this experiment, as before in \Cref{ssec:eval-throughput}, the load generators emit a fixed load that can saturate the throughput of the full configuration (4 or 8 compute nodes). We start the experiment with all 32 partitions located on a single node, while the other nodes are unused. After running for 70 seconds, we re-balance the partitions across all available nodes. Specifically, for the 4-node experiments, we move 24 of the 32 partitions, and for the 8-node experiments, we move 28 of the 32 partitions. To reduce noise we repeated this 3 times for each series and computed the average.

\resultpar{Observations.} The results for the hello sequence workflow without speculation are shown in \Cref{fig:scaleout}. They show that \sys can scale out with the addition of compute instances. It reaches peak throughput after around 7 seconds (after the scale-out decision at 70s) in both cases. 
%The 8 node case is slightly faster because each of the new nodes has to start only 4 partitions, as opposed to 8. 
\str
% \kk{These should be consistent with what is shown in \Cref{fig:throughputs}}
