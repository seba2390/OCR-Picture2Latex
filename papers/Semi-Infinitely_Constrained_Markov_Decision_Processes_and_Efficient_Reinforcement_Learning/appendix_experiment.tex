% \section{Details of Numerical Experiments}
\section{Experimental Setup}
\label{Appendix_Detials_of_Experiments}
We use PyTorch, RLlib, and Gurobi framework to implement the algorithms in our work, and the codes are run on Nvidia RTX Titan GPUs and Intel Xeon Gold 6132 CPUs with 252GB memory. The code of our algorithms and construction of the corresponding environments has been released on
\href{https://github.com/pengyang7881187/SICMDP-new}{https://github.com/pengyang7881187/SICMDP-new}.

% \subsection{Construction of Toy SICMDP}
% % \liangyu{Specify the function $f$ we actually used!}

% Without loss of generality, assume $Y=[0, 3]$. We split $Y=[0, 3]$ to $Y_1=[0, 1]$, $Y_2=[1, 2]$ and $Y_3=[2, 3]$. Intuitively, in $Y_1$ ($Y_3$), we restrict the agent to take action $a_0$ in $s^0$ ($s^1$) with the given probability from the target policy $\tilde{\pi}$. (Recall, taking action $a_0$ is always better if we set aside the constraints.) We introduce $Y_2$ to obtain $L$-Lipschitz of $c_y$ and $u_y$. 
% Assume $f\colon [-0.5, 0.5] \rightarrow \mathbb{R}$ is a continuous-differentiable even function, with unique maximum point $0$ and zero point $0.5$, we use $f(x)=(1+\cos(2\pi x))\cos(2\pi x)$ in practice. Let $c_y(s^0, a_0)=f(y-0.5)$ for $y\in Y_1$, $c_y(s^1, a_0)=f(y-2.5)$ for $y\in Y_3$, and $c\equiv 0$ otherwise.
% For each $\gamma\in(0, 1)$, we define $u^1=C^{\tilde{\pi}}_{0.5}>0$ and $u^1=C^{\tilde{\pi}}_{2.5}>0$. Let $u_y\equiv u^1$ in $Y_1$, $u_y \equiv u^2$ in $Y_3$ and we make linear interpolate $u_y$ in $Y_2$.

% So far, we have constructed Lipschitz $c_y$ and $u_y$. The only active constraints are $C^{\pi}_{0.5}\leq u^1$ and $C^{\pi}_{2.5}\leq u^2$ and the optimal policy is the known target policy $\tilde{\pi}$.

\subsection{Details of Construction of Discharge of Sewage}
We generate this environment randomly with $|\gS|=8$, $|\gA|=4$ and $\gamma=0.9$. In each experiment, we sample the environments several times and report the average result.

Positions of sewage outfalls, transition dynamics and rewards are sampled uniformly on $Y$, probability simplex and $[0, 1]$ respectively. The state-occupancy measure $d$ is then generated by the uniform policy and $\Delta$ is set to $10^{-6}$.

The optimal policy $\pi^*$ used is obtained by solving a corresponding linear programming with true transition dynamic $P$ and a fine grid of $Y$ of size $10^6$. 
% Solving such a linear programming with many constraints is very slow as shown in Figure \ref{Figure_Sewage_time}.

% Assume $f\colon [0, +\infty) \rightarrow [0, +\infty)$ is a continuous-differentiable decreasing function, we use $f(x)=\frac{1}{1+x^2}$ in practice. Let $c_y(s, a)=c_y(s)=f(\|y-s\|_2)$, where $s$ also represents the position of the state (outfall).

% Given a target state-occupancy measure $d$ which can be generated randomly or specified in advance, we define $u_y=(1+\Delta)\sum_{s\in S} d(s)c_y(s)$, where $\Delta$ is a small positive number. The feasibility of the resulting SICMDP is not guaranteed even if $\Delta=0$, we reject those infeasible environments and re-sample. The SICMDP would be nontrivial if we choose a suitable $\Delta$.

\subsection{Implementation of SI-CRL}
We would like to clarify how to solve the maximization problem to generate new $y_t$ in $t$th iteration in SI-CRL. Since the $u_y$ can be non-convex in $y$ and evaluating $\widehat V^\pi_{c_y}-u_y$ for multiple $y$ is much cheaper in the model-based setting with small $|\gS|$ and $|\gA|$ than solving the linear programming, we choose to solve the maximization problem by brute force.
Specifically, we first create a grid of $Y$ of size $10^5$, and then find the grid point with max objective.
This method works well since in the problems we consider $Y$ is of low dimensions.

When solving the linear programming, we do not force Gurobi to use dual simplex method in SI-CRL algorithm, and find that it achieves an even better re-optimization performance in practice. 

\subsection{Implementation of SI-CPO}
The parameterized policy class is chosen as the softmax policy: $\pi_\theta(a|s)=e^{\theta_{s,a}}/\sum_{a^\prime\in \gA}e^{\theta_{s,a^\prime}}$, where $\theta\in\RB^{|\gS||\gA|}$ is initialized as $0$. This policy class satisfies the assumptions of the theoretical analysis of SI-CPO.

We use learning rate $\alpha=1$, tolerance $\eta=0.013$, maximum iteration
number $T=10000$ for both SI-CPO and baseline. SI-CPO and baseline use sample-based NPG, Algorithm \ref{Algorithm_sample_based_NPG}, as the policy optimization subroutine, and share the same hyper-parameters: number of evaluation paths $K_{eval}=100000$, number of training paths $K_{sgd}=1000$, fixed horizon $H=100$, upper bound of parameters' norm $W=1000$, constant learning rate $1$ and weight $\gamma_k=2k/K_{sgd}(K_{sgd}+1)$.

In $t$th iteration in SI-CPO, we sample $100$ points uniformly in $Y$ and find the best one to solve the maximization problem approximately as Algorithm \ref{Algorithm_random_search}. In the model-free setting, evaluating $\widehat V^\pi_{c_y}$ is much more computationally expensive than the model-based setting, hence brute force is impractical. Additionally, the random search method can yield a better policy than the one utilizing constraint optimization algorithm in practice. 

\subsection{Details of Construction of Ship Route Planning}
We fix the outset $O=(0, 0)$, destination $D=(1, 1)$, environmentally critical point $MPA=(\frac{1}{2}, \frac{1}{2})$ and we assume the ship sails with constant speed $0.1$. $c_y$ and $u_y$ are designed to make sure the trajectory along the curve $y=x^4$ satisfies the constraint.

\subsection{Implementation of SI-CPPO}
The actor, constraint critic and reward critic networks have two hidden layers of size 512 with tanh non-linearities without sharing layers for both SI-CPPO and baseline. We update the networks using the Adam optimizer with learning rate $10^{-4}$. SI-CPPO and baseline are modified from the standard implementation of PPO in RLlib and share the same hyper-parameters. See more details in Algorithm \ref{Algorithm_SICPPO}, note that we use the generalized advantage estimation in practice instead of the one-step estimation in the pseudo-code.

In $t$th iteration in SI-CPPO, we use a trust-region method to solve the maximization problem based on the trajectories collected by $4$ roll-out workers. We call scipy.optimize.minimize to implement it, if the optimization process fails to converge, we switch to use random search with $100$ points uniformly in $Y$ temporarily. Empirically, the optimization process seldom fails and this strategy outperforms random search.

\begin{algorithm}[htb!]
   \caption{SI-CPPO}
   \label{Algorithm_SICPPO}
\begin{algorithmic}
   \STATE {\bfseries Input:} state space $\gS$, action space $\gA$, reward function $r$, a continuum of cost function $c$, index set $Y$, value for constraints $u$, discount factor $\gamma$, batch size $B$, tolerance $\eta$, maximum iteration number $T$.
   \STATE Initialize policy network $\pi^{(0)}$, reward critic network $V_r^{(0)}$ and constraint critic network $V^{(0)}_c$.
   \FOR{$t=0,...,T-1$}
   \STATE Sample $B$ trajectories $\gB^{(t)}$ using policy network $\pi^{(t)}$.
   \STATE Obtain Monte-Carlo estimator $\widehat V_{c_y}^{\pi^{(t)}}(\mu)$ based on $\gB^{(t)}$.
   \STATE Use an optimization subroutine to solve ${\max_y\ \widehat V_{c_y}^{\pi^{(t)}}(\mu)-u_y}$, and set ${y^{(t)}\approx\argmax_y \widehat V_{c_y}^{\pi^{(t)}}(\mu)-u_y}$, $c^{(t)}=c_{y^{(t)}}$.
   \IF {$\widehat V_{c^{(t)}}^{\pi^{(t)}}(\mu)-u_{y^{(t)}}\leq \eta$}
   \STATE  Obtain advantage estimation of reward using reward critic network: 
   
   $\hat{A}^{(t)}(s_\tau,a_\tau)=\left(r_\tau+\gamma V_r^{(t)}(s_{\tau+1}) \right)-V_r^{(t)}(s_{\tau})$, $\forall (s_\tau, a_\tau, r_\tau, s_{\tau+1})\in \gB^{(t)}$.
   \ELSE 
   \STATE  Obtain advantage estimation of constraint at $y^{(t)}$ using constraint critic network: 
   
   $\hat{A}^{(t)}(s_\tau,a_\tau)=V^{(t)}_{c^{(t)}}(s_{\tau})-\left(c_{y^{(t)},\tau}+\gamma V^{(t)}_{c^{(t)}}(s_{\tau+1}) \right)$,  $\forall(s_\tau, a_\tau, r_\tau, s_{\tau+1})\in \gB^{(t)}$.
%   \IF {$t\geq s$}
%   \ENDIF
   \ENDIF
   \STATE Update policy network $\pi^{(t)}$, reward critic network $V_r^{(t)}$ and constraint critic network $V^{(t)}_c$ to $\pi^{(t+1)}$, $V_r^{(t+1)}$ and $V^{(t+1)}_c$ respectively with the above advantage estimation via the standard proximal policy optimization algorithm.
   \ENDFOR
   \STATE {\bfseries RETURN} $\hat\pi=\pi^{(T)}$.
\end{algorithmic}
\end{algorithm}
% The parameterized policy class is chosen as the softmax policy: $\pi_\theta(a|s)=e^{\theta_{s,a}}/\sum_{a^\prime\in \gA}e^{\theta_{s,a^\prime}}$, where $\theta\in\RB^{|\gS||\gA|}$ is initialized as $0$. This policy class satisfies the assumptions of the theoretical analysis of SI-CPO.

% We use learning rate $\alpha=0.1$, tolerance     $\eta=0.013$, maximum iteration
% number $T=10000$ for both SI-CPO and baseline. 

% In $t$th iteration in SI-CPO, we sample $100$ points uniformly in $Y$ and find the best one to solve the maximization problem approximately as \ref{Algorithm_random_search}. In the model-free setting, evaluating $\widehat V^\pi_{c_y}$ is much more computationally expensive than the model-based setting, hence brute force is impractical. Additionally, the random search method can yield a better policy than the one utilizing constraint optimization algorithm in practice. 

