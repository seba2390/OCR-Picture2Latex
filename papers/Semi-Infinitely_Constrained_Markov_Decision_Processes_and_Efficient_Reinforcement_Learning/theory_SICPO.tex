In this section, we present theoretical guarantees of SI-CPO.
We consider a version of the SI-CPO algorithm, where we use sample-based NPG \citep{agarwal2021theory} as the policy optimization subroutine, a finite-horizon Monte-Carlo estimator as the policy evaluation subroutine, and either random search or projected subgradient ascent as the optimization subroutine.
It is shown that when the function approximation error $\epsilon_{bias}$ is of the same order with $\epsilon$, our proposed algorithm takes $\widetilde{O}\left(\frac{1}{\epsilon^2(1-\gamma)^6}\right)$ iterations and make $\widetilde{O}\left(\frac{1}{\epsilon^4(1-\gamma)^{10}}\right)$ interactions with the environment to achieve an $\epsilon$-level of averaged suboptimality with high probability.
This corresponds to a $\widetilde{O}(1/\sqrt{T})$ globally convergence rate, which is typical for NPG-based policy optimization algorithms.
We will give a detailed description of the considered version of the SI-CPO algorithm as well as our technical assumptions in Section~\ref{Subsection_SICPO_Prem} and present the theoretical results in Sections~\ref{Subsection_SICPO_Iteration_Complexity} and~\ref{Subsection_SICPO_Sample_Complexity}.

\subsubsection{Preliminaries}\label{Subsection_SICPO_Prem}
Recall the policy $\pi$ is parameterized by $\theta\in\Theta\subset\RB^d$ (denoted by $~\pi_\theta$).
We make the following assumptions about the parameterized policy class.
\begin{assumption}[Differentiable policy class]\label{Assumption_differentiable}
$\Pi$ can be parametrized as $\Pi_\theta=\{\pi_\theta|\theta\in\RB^d\}$, such that for all $s\in\gS$,~$a\in\gA$, $\log_\theta\pi(s|a)$ is a differentiable function of $\theta$.
\end{assumption}

\begin{assumption}[Lipschitz policy class]\label{Assumption_Lipschitz_policy}
For all $s\in\gS$,~$a\in\gA$, $\log\pi_\theta(s|a)$ is a $L_\pi$-Lipschitz function of $\theta$, i.e.,
$$
\|\nabla_\theta\log\pi_\theta(s|a)\|_{2}\leq L_\pi,\forall s\in\gS,a\in\gA,\theta\in \RB^d.
$$
\end{assumption}

\begin{assumption}[Smooth policy class]\label{Assumption_smooth}
For all $s\in\gS$,~$a\in\gA$, $\log\pi_\theta(s|a)$ is a $\beta$-smooth function of $\theta$, i.e.,
$$
\|\nabla_\theta\log\pi_\theta(s|a)-\nabla_\theta\log\pi_{\theta^\prime}(s|a)\|_{2}\leq \beta\|\theta-\theta^\prime\|_2,\forall s\in\gS,a\in\gA,\theta,\theta^\prime\in \RB^d.
$$
\end{assumption}

\begin{assumption}[Positive semidefinite Fisher information]\label{Assumption_PSD_Fisher}
    For all $\theta\in\RB^d$,
    $$F(\theta):=\EB_{(s,a)\sim\nu_\theta}[\nabla_\theta\log\pi_\theta(a|s)\nabla_\theta\log\pi_\theta(a|s)^\top]\succeq\mu_F I_d.
    $$
\end{assumption}
The assumptions above are standard in the literature of policy optimizations \citep{agarwal2021theory}.
We also assume the parametrization realizes good function approximation in terms of transferred compatible function approximation errors, which is first introduced by \cite{agarwal2021theory}.
The error term can be close to zero if the policy class is rich \citep{wang2019neural} or the underlying MDP has low-rank structures \citep{jiang2017contextual}.
\begin{assumption}[Bounded function approximation error]\label{Assumption_func_approx_err}
The transferred compatible function approximation errors satisfies that $\forall t\in\{1,...,T\}$
$$
\begin{aligned}
\min_w E^{\nu^{(t)}}(r,\theta^{(t)},w)&\leq \epsilon_{\text{bias}}\\
\min_w E^{\nu^{(t)}}(c_y,\theta^{(t)},w)&\leq \epsilon_{\text{bias}}\ \forall y\in Y,\\
\end{aligned}
$$
where $\nu^{(t)}$ denotes the state-action occupancy measure induced by policy $\pi^{(t)}$. 
The transferred compatible function approximation errors are defined as:
$$
E^{\nu}(\diamond,\theta,w):=\EB_{(s,a)\sim\nu}(A^{\pi_\theta}_\diamond(s,a)-w^\top\nabla_\theta\log\pi_\theta(a|s))^2.
$$
\end{assumption}

Besides, we also assume the weights to minimize the transferred compatible function approximation errors are bounded.

\begin{assumption}[Bounded Weight]\label{Assumption_est_err}
For any $t\in\{1,...,T\}$, $\forall y\in Y$,
$$
\left\|\underset{w}{\mathrm{argmin}} E^{\nu^{(t)}}(r,\theta^{(t)},w)\right\|_2^2\leq W^2, \left\|\underset{w}{\mathrm{argmin}} E^{\nu^{(t)}}(c_y,\theta^{(t)},w)\right\|_2^2\leq W^2.
$$
\end{assumption}

In the theoretical analysis of SI-CPO, we consider an instance of SI-CPO where we use a sample-based version of NPG \citep{agarwal2021theory} as the policy optimization subroutine, a fixed-horizon Monte-Carlo estimator as the policy evaluation subroutine, and either random search or projected subgradient ascent as the optimization subroutine.
In the NPG algorithm, we use the following natural policy gradient $w^{(t)}$ to update the policy parameters:
$$
w^{(t)}:=F(\theta^{(t)})^\dagger\EB_{(s,a)\sim\nu^{(t)}}(A^{\pi^{(t)}}_\diamond(s,a)\nabla_\theta\pi_\theta(a|s)).
$$
Here $\diamond$ can be either the reward $r$ or some cost function $c_y$.
However, for most RL problems it is computationally prohibitive to evaluate $F(\theta)^\dagger$, and $\EB_{(s,a)\sim\nu^{(t)}}(A^{\pi^{(t)}}_\diamond(s,a)\nabla_\theta\pi_\theta(a|s))$ are usually unknown to the algorithm.
Therefore, we instead use a sample-based estimate of $w^{(t)}$, which can be obtained by solving the following optimization problem by running $K_{sgd}$ steps of stochastic gradient descent:
$$
\hat w^{(t)}\approx\frac{1}{1-\gamma}\arg\min_w E^{\nu^{(t)}}(b,\theta^{(t)},w),
$$
recall that $E^{\nu^{(t)}}(\diamond,\theta^{(t)},w)$ is the transferred function approximation error defined in Assumption~\ref{Assumption_func_approx_err}.
The precise definition of sample-based NPG can be found in Algorithm~\ref{Algorithm_sample_based_NPG} in Appendix~\ref{Appendix_Algorithm}.

As for policy evaluation, we choose to use a Monte-Carlo estimator with a fixed horizon $H$.
The idea is very simple, in each episode we run the target policy $\pi$ for $H$ steps, and record the return
$$
G_i=\sum_{k=0}^{H-1}\gamma^k c_y(s_k,a_k).
$$
The procedure is repeated for $K_{eval}$ times and we take the average as an estimate of $V_{c_y}^{\pi^{(t)}}(\mu)$.
Compared with the more commonly used unbiased Monte-Carlo estimate
$$\widetilde{G}_i=\sum_{k=0}^{H^\prime-1}c_y(s_k,a_k)
$$
where $H^\prime$ is no longer fixed and drawn from an exponential distribution $\mathrm{Exp(\lambda)}$, $G_i$ does introduce bias, but it also has the advantage of being sub-gaussian.
Moreover, the bias term is always bounded by $\frac{\gamma^H}{1-\gamma}$, which decays exponentially as we choose larger $H$s.

\subsubsection{Iteration complexity of SI-CPO}\label{Subsection_SICPO_Iteration_Complexity}

The following two theorems give the iteration complexity of the SI-CPO algorithm when we use either random search or projected subgradient ascent to solve the inner-loop problem.

\begin{theorem}\label{Theorem_random_search_SICPO}
Suppose we use random search to solve the inner-loop problem and Assumption~\ref{Assumption_regular_maxima} holds. 
If we set $\alpha=1/\sqrt{T}$, $\eta=\epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}}$, and 
${K_{sgd}=\widetilde{O}\left(\frac{1}{\epsilon_{bias}^2(1-\gamma)^4}\right)}$ , $ H=O\left(\frac{\log(1-\gamma)+\min\{\log(\epsilon_{bias}),\log(\epsilon)\}}{\log\gamma}\right)$, $K_{eval}=\widetilde{O}\left(\frac{1}{\epsilon^2(1-\gamma)^2}\right)$, ${M=\widetilde{O}\left(\frac{(\mathrm{diam}(Y))^m}{\epsilon^m(1-\gamma)^m}\right)}$, ${T=\widetilde{O}\left(\frac{1}{\epsilon^2(1-\gamma)^6}\right)}$, then we have with probability $1-2\delta$,
$$
    \frac{1}{|\gB|}\sum_{t\in\gB}(V_r^*(\mu)-V_r^{(t)}(\mu))\leq \epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}},
$$
and $\forall t\in\gB$
$$ \sup_{y\in Y}\left[V_{c_y}^{(t)}(\mu)-u_y\right]\leq 2\epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}}.
$$
for any $\epsilon<2\epsilon_0 L_y/(1-\gamma)$.
\end{theorem}
\proof {Proof of Theorem~\ref{Theorem_random_search_SICPO}}
See Appendix~\ref{Appendix_Proofs_SICPO}.
\endproof

\begin{theorem}\label{Theorem_PGA_SICPO}
Suppose we use projected subgradient ascent to solve the inner-loop problem and Assumption~\ref{Assumption_concave_constraint} holds. 
If we set $\alpha=1/\sqrt{T}$, $\eta=\epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}}$, and 
${K_{sgd}=\widetilde{O}\left(\frac{1}{\epsilon_{bias}^2(1-\gamma)^4}\right)}$ , $ H=O\left(\frac{\log(1-\gamma)+\min\{\log(\epsilon_{bias}),\log(\epsilon)\}}{\log\gamma}\right)$, $K_{eval}=\widetilde{O}\left(\frac{1}{\epsilon^2(1-\gamma)^2}\right)$, ${T_{PGA}=O\left(\frac{[\mathrm{diam}(Y)]^2}{\epsilon^2(1-\gamma)^2}
    \right)}$, ${T=\widetilde{O}\left(\frac{1}{\epsilon^2(1-\gamma)^6}\right)}$, then we have with probability $1-\delta$,
$$
    \frac{1}{|\gB|}\sum_{t\in\gB}(V_r^*(\mu)-V_r^{(t)}(\mu))\leq \epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}},
$$
and $\forall t\in\gB$
$$ \sup_{y\in Y}\left[V_{c_y}^{(t)}(\mu)-u_y\right]\leq 2\epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}}.
$$
\end{theorem}
\proof {Proof of Theorem~\ref{Theorem_random_search_SICPO}}
See Appendix~\ref{Appendix_Proofs_SICPO}.
\endproof

In our proof, we focus on the event that the policy evaluation subroutine returns accurate estimates of $V^{(t)}_{c_y}(\mu)$ and the sample-based NPG generates a near-optimal solution of $\min_w E^{\nu^{(t)}}(\diamond,\theta^{(t)},w)$.
We show that this event happens with high probability.
When it happens, with carefully chosen tolerance threshold $\eta$, either the "good set" $\gB$ is large or the policies in $\gB$ perform equally well to the optimal policy $\pi^*$ on average, \textit{i.e.} $\sum_{t\in\gB}(V^{(t)}_r(\mu)-V^*_r(\mu))\geq 0$.
As long as $\gB$ is large enough, we may further conclude that  $\frac{1}{|\gB|}\sum_{t\in\gB}|V^{(t)}_r(\mu)-V^*_r(\mu)|$ is small by typical analysis techniques of NPG \citep{agarwal2021theory}.
Recalling that the constraint violations of policies in $\gB$ are small as long as the inner-loop optimization problems are effectively solved, we complete our proof.

Our ideas of proof are similar to \cite{wei2020comirror, xu2021crpo}.
However, \cite{wei2020comirror} focused on the semi-infinitely constrained convex problems and we focus on the semi-infinitely constrained RL problems.
Moreover, their theoretical results are in the form of bounds on expectations, while ours are in the form of high probability bounds.
Our work is different with \cite{xu2021crpo} in the sense that they address finitely constrained RL problems, and restrict their analysis to two specific forms of policy parametrizations, whereas we consider general policy parametrizations.

\begin{remark}
The error terms of SI-CPO can be attributed to three sources: the function approximation error, the statistical error, and the optimization error.
When we say SI-CPO converges to the globally optimal policy $\pi^*$ at a $\widetilde {O}(1/\sqrt{T})$ rate we mean that if we use a near-perfect parameterized policy class, estimate $V^{(t)}_{c_y}(\mu)$ and the natural policy gradient with adequate data and solve the inner-loop problem with sufficient accuracy, then the averaged error term of SI-CPO has a $\widetilde O(1/\sqrt{T})$ order with high probability.
\end{remark}

\begin{remark}\label{Remark_random_search_vs_fixed_search}
    When solving the inner-loop problem, an alternative approach to random search is to search over a fixed grid of $Y$.
    This is equivalent to a version of naive discretization: we first transform the SICMDP to a finitely constrained MDP by discretizing $Y$, and then solve the resulting problem with CRPO \citep{xu2021crpo}.
    From a theoretical viewpoint, random search is no better than the gird search since both need to search over a $\widetilde{O}((\mathrm{diam}(Y)/\epsilon)^m)$-sized grid to ensure $\epsilon$-optimality.
    However, in numerical experiments we find that the approach based on random search is far more efficient that the approach based on grid search.
    The reasons can be two-fold: 1) in the theoretical analysis we must give guarantees for the hardest problem instances, but real-world problem settings may contain structures that can be exploited by random search \citep{bergstra2012random}; 2) in random search the random grid are generated in an independent way in each iteration, which can reduce the bias introduced by replacing the constraint set $Y$ with a fixed finite grid.
\end{remark}

\subsubsection{Sample complexity of SI-CPO}\label{Subsection_SICPO_Sample_Complexity}

\begin{corollary}\label{Corollary_Sample_Complexity_SICPO}
SI-CPO need to make $\widetilde O{\left(\frac{1}{\epsilon^2\min\{\epsilon^2,\epsilon_{bias}^2\}(1-\gamma)^{10}}\right)}$ interactions with the environments to ensure with high probability
$$
    \frac{1}{|\gB|}\sum_{t\in\gB}(V_r^*(\mu)-V_r^{(t)}(\mu))\leq \epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}},
$$
and $\forall t\in\gB$
$$ \sup_{y\in Y}\left[V_{c_y}^{(t)}(\mu)-u_y\right]\leq 2\epsilon+\frac{1}{(1-\gamma)^{3/2}}\sqrt{\left\|\frac{\nu^*}{\nu_0}\right\|_\infty\epsilon_{bias}}.
$$
\end{corollary}
\proof{Proof of Corollary~\ref{Corollary_Sample_Complexity_SICPO}}
This corollary is a direct consequence of Theorem~\ref{Theorem_random_search_SICPO} as the sample complexity is of the order $T \cdot H\cdot(K_{eval}+K_{sgd})$.
Note that the sample complexity bound is independent of how we solve the inner-loop problem.
\endproof

Our sample complexity bound is of the order $\widetilde{O}\left(\frac{1}{\epsilon^4(1-\gamma)^{10}}\right)$.
This is worse than typical sample complexity bounds for sample-based NPG such as the $\widetilde{O}\left(\frac{1}{\epsilon^4(1-\gamma)^8}\right)$ in \cite{agarwal2021theory}.
This does not mean that the constrained problem is statistically harder or our bounds are loose.
The difference comes from that their goal is to ensure accuracy in expectation, while our goal is to ensure accuracy with high probability.
Therefore, in our algorithm, we need to run more SGD iterations (corresponding to larger $K_{sgd}$) to find a better estimate of the natural gradient.