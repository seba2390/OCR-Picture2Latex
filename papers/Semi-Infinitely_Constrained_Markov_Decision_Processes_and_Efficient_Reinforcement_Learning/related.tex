% \subsection{Constrained Markov Decision Process}
Constrained Markov decision processes (CMDPs) have been extensively applied in areas like robotics \citep{ono2015chance}, communication and networks \citep{mastronarde2011fast, singh2018throughput} and finance \citep{abe2010optimizing}.
For a detailed treatment of CMDPs readers may refer to \cite{altman1999constrained}.
A number of reinforcement learning algorithms for CMDPs are proposed, which can be divided into model-based methods and model-free methods.
For model-based methods, \citet{wachi2020safe, zheng2020constrained} considered the case where the reward and cost are random but the transition dynamics are known.
\citet{efroni2020explorationexploitation, amani2021safe, ijcai2021-347} considered the case where the transition dynamics are unknown and need to be estimated, which is a more common setting in the literature of reinforcement learning.
And \citet{vaswani2022near} gave a near minimax optimal sample complexity bound of learning CMDPs.
Most model-free methods can indeed be categorized as policy optimization methods.
\citet{tessler2018reward, ding2020natural} utilized a primal-dual approach that transforms the constrained problem into an unconstrained one by considering the Lagrange functions.
which include Lagrangian methods, actor-critic methods \citet{achiam2017constrained, yang2020projection, liu2020ipo} addressed the constrained problem by adding constraints to the sub-problems used to compute the updating direction in each iteration step.
\citet{xu2021crpo} proposed to solve the CMDP problem by performing alternating updates to maximize the reward or minimize the cost.
Our SI-CRL algorithm uses a similar strategy as in \cite{efroni2020explorationexploitation} in the sense that they all use the optimistic method to transform the reinforcement learning problem into a linear (semi-infinitely) programming problem, which resolves the feasibility issue and makes the theoretical analysis easier as well.
However, our work and \cite{efroni2020explorationexploitation} are very different at the technical level: 1) our theoretical guarantees
are in the form of sample complexity bounds, while the results in \citep{efroni2020explorationexploitation}
are in the form of online regret bounds; the proof techniques are quite different. 2) \citet{efroni2020explorationexploitation} considered the episodic MDPs, while we consider the infinite-horizon case.


% \subsection{Linear Semi-infinitely Programming}
The origination of semi-infinitely programming (SIP) can date back to \cite{remez1934determination}.
From then on, SIP has been widely used in quantum physics \citep{2021quantum}, signal processing \citep{moulin1997role, nordebo2001semi}, finance \citep{daum2011novel}, environment science, and engineering \citep{hettich1993semi}.
One may refer to \cite{hettich1993semi, goberna2018recent} for a detailed overview of SIP as well as its recent advances.
One important class of SIP problems is called linear semi-infinitely programming (LSIP).
\citet{GOBERNA2002390} provided a thorough survey about the LSIP theory.
Various numerical methods are proposed to solve SIP problems, including discretization methods \citep{still2001discretization, 2004discretization}, exchange methods \citep{Hu1990, 2010exchange}, and local reduction methods \citep{1970moment, coope1985projected}.
In SI-CRL, we choose to use the dual exchange method in \cite{Hu1990} to solve the LSIP problem therein for its conceptual simplicity as well as concrete theoretical guarantees.
Recently, \citet{wei2020comirror} proposed to solve convex SIP problems via the cooperative stochastic approximation method, which is first developed in \cite{lan2020algorithms} to solve stochastic optimization problems with functional or expectation constraints.

% % (Required by the Journal.)
% This work is an extended version of the proceedings paper \citep{zhang2022semiinfinitely}.
% We extend \cite{zhang2022semiinfinitely} in the following ways: 1) We propose a new reinforcement learning algorithm named SI-CPO for solving large-scale SICMDPs and validate its efficacy in numerical experiments; 2) we give a theoretical analysis of SI-CPO including the iteration complexity bounds and sample complexity bounds; 3) we also refine the theoretical analysis of SI-CRL, where we discard the assumption that a subproblem must be exactly solved and give new iteration complexity bounds.


