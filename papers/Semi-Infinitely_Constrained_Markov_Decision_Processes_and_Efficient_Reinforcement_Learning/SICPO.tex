\subsection{The SI-CPO Algorithm}\label{Section_SICPO}
In SI-CPO, we borrow ideas from the cooperative stochastic approximation \citep{lan2020algorithms, wei2020comirror} to deal with the infinitely many constraints.
At a certain iteration, the SI-CPO algorithm first determines whether the constraint violation is below some tolerance or not.
It then performs a single step of policy optimization along the direction of maximizing the value of reward if the constraint violation is below some tolerance;
or performs a single step of policy optimization along the direction of minimizing the value of some cost corresponding to a violated constraint.

We now describe the SI-CPO algorithm in more detail.
We follow the convention to define the parameterized policy class as $\{\pi_\theta,\theta\in\Theta\subset\RB^d\}$ and use $\pi^{(t)}$ in short of $\pi_{\theta^{(t)}}$, $V_\diamond^{(t)}$ in short of $V_\diamond^{\pi^{(t)}}$ for ease of notation.
Here $\diamond$ represents either the reward $r$ or some cost $c_y$.
Suppose at the $t$-th iteration our policy parameter is $\theta^{(t)}$, then we first construct an estimate $\widehat V^{(t)}_{c_y}(\mu)$ using some policy evaluation subroutine.
Next, we are to solve a subproblem using some optimization subroutine
$$
y^{(t)}=\argmax_y\ \widehat V_{c_y}^{\pi^{(t)}}(\mu)-u_y.
$$
If ${\widehat V_{c^{(t)}}^{\pi^{(t)}}(\mu)-u_{y^{(t)}}\leq \eta}$, where $c^{(t)}:=c_{y^{(t)}}$ and $\eta> 0$ is a threshold of tolerance, we say the constraint violation is small and add the time index $t$ to the ``good set" $\gB$.
Then we perform a step of update with a policy optimization subroutine to maximize the value of reward $V_r^{(t)}(\mu)$ to get $\theta^{(t+1)}$.
Else, we first add the time index $t$ to the ``bad set" $\gN$.
Next, we find the violated constraint ${V_{c^{(t)}}^{\pi^{(t)}}(\mu)-u_{y^{(t)}}>\eta}$, and perform a step of update with a policy optimization subroutine to minimize the value of cost $V_{c^{(t)}}^{\pi^{(t)}}(\mu)$ to get $\theta^{(t+1)}$.
After $T$ iterations, we draw $\hat\theta$ uniformly from the set ${\{\theta^{(t)},t\in\gB\}}$, as return the policy ${\hat\pi=\pi_{\hat\theta}}$.
The procedure of SI-CPO  is summarized in Algorithm~\ref{Algorithm_SICPO}.


We can get different instances of the SI-CPO algorithms by making different choices of the subroutines aforementioned.
Specifically, the policy optimization subroutine can be any policy optimization algorithm like policy gradient~(PG) \citep{sutton1999policy}, natural policy gradient~(NPG) \citep{kakade2001natural}, trust-region policy gradient~(TRPO) \cite{schulman2015trust}, or proximal policy optimization~(PPO) \citep{schulman2017proximal}.
The policy evaluation subroutine can be chosen as Monte-Carlo policy evaluation algorithms \citep{curtiss1954theoretical} or various TD-learning algorithms \citep{sutton1988learning, dann2014policy}.
We may also integrate the policy optimization subroutine and the policy evaluation subroutine into actor-critic-type algorithms \citep{konda1999actor}.
The optimization subroutine can be any optimization algorithm suitable for the problem instance, like the case in Algorithm~\ref{Algorithm_SICRL}.
% It is shown in Section~\ref{Section_Theory_SICPO} that if we use sample-based NPG \cite{agarwal2021theory} as the policy optimization subroutine, a finite-horizon Monte-Carlo estimator as the policy evaluation subroutine and either random search or projected subgradient ascent as the optimization subroutine, then $\hat\pi$ is guaranteed to be a globally near-optimal policy.
% We also empirically show in Section~\ref{Section_Experiment} the efficacy of SI-CPO in solving complex sequential decision-making problems with deep neural networks if we use PPO as the policy optimization subroutine, a value network trained in a TD style as the policy evaluation subroutine, and random search as the optimization subroutine.

\begin{algorithm}[htb]
   \caption{SI-CPO}
   \label{Algorithm_SICPO}
\begin{algorithmic}
   \STATE {\bfseries Input:} state space $\gS$, action space $\gA$, reward function $r$, a continuum of cost function $c$, index set $Y$, value for constraints $u$, discount factor $\gamma$, learning rate $\alpha$, tolerance $\eta$, maximum iteration number $T$.
   \STATE Initialize $\gB=\emptyset$, $\gN=\emptyset$, $\theta^{(0)}=\theta_0\in\Theta$.
   \FOR{$t=0,...,T-1$}
   \STATE Obtain $\widehat V_{c_y}^{\pi^{(t)}}(\mu)$ via a policy evaluation subroutine.
   \STATE Use an optimization subroutine to solve ${\max_y\ \widehat V_{c_y}^{\pi^{(t)}}(\mu)-u_y}$, and set ${y^{(t)}\approx\argmax_y \widehat V_{c_y}^{\pi^{(t)}}(\mu)-u_y}$, $c^{(t)}=c_{y^{(t)}}$.
   \IF {$\widehat V_{c^{(t)}}^{\pi^{(t)}}(\mu)-u_{y^{(t)}}\leq \eta$}
   \STATE  Perform a step of policy update to maximize $V_r^{\pi^{(t)}}(\mu)$ to get $\pi^{(t+1)}$. Specifically,
   ${\theta^{(t+1)}=\theta^{(t)}+\alpha\hat w^{(t)}}.$
%   \IF {$t\geq s$}
   \STATE Add $t$ to $\gB$
%   \ENDIF
   \ELSE 
   \STATE  Perform a step of policy update to minimize $V_{c^{(t)}}^{\pi^{(t)}}(\mu)$ to get $\pi^{(t+1)}$. Specifically, ${\theta^{(t+1)}=\theta^{(t)}-\alpha\hat w^{(t)}}.$
%   \IF {$t\geq s$}
   \STATE Add $t$ to $\gN$
%   \ENDIF
   \ENDIF
   \ENDFOR
   \STATE {\bfseries RETURN} $\hat\pi=\pi_{\hat\theta}$, where $\hat\theta\sim\mathrm{Unif}\left(\{\theta^{(t)}, t\in\gB\}\right)$.
\end{algorithmic}
\end{algorithm}
