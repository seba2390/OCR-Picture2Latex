In this section, we present two reinforcement learning algorithms called semi-infinitely constrained reinforcement learning (SI-CRL) and semi-infinitely constrained policy optimization (SI-CPO), respectively.
SI-CRL is a model-based reinforcement learning algorithm that can solve tabular SICMDP in a sample-efficient way.
The SI-CPO algorithm is a policy optimization algorithm and it works for large-scale SICMDPs where we can use complex function approximators such as deep neural networks to approximate the policy and the value function.
\subsection{The SI-CRL Algorithm}\label{Section_SICRL}
From a high-level point of view, the SI-CRL algorithm is a semi-infinite version of the algorithms proposed in \cite{ijcai2021-347, efroni2020explorationexploitation}.
In the first stage, SI-CRL takes an offline dataset $\{(s_i, a_i, s_i^\prime)|i=1, 2, \ldots, m\}$ as input and generates an empirical estimate $\widehat P$ of the true transition dynamic $P$.
Then the algorithm constructs a confidence set (the optimistic set) according to $\widehat P$ that would cover the true SICMDP with high probability.
For each policy $\pi$ we would only view its return as the largest possible return in SICMDPs in the confidence set.
% Since when the true SICMDP lies in the confidence set we would overestimate the return of each policy $\pi$, 
This method is also called the optimistic approach.
In the second stage, we reformulate the problem as an LSIP problem and find the optimistic policy $\hat \pi$ using an LSIP solver.
It can be shown that the resulting policy $\hat\pi$ is guaranteed to be nearly optimal, and the theoretical analysis can be found in Section \ref{Section_Theory_SICRL}.

Now we give a more detailed description of SI-CRL.
First, the empirical estimate $\widehat P$ is calculated as:
$\widehat P(s^\prime|s, a):=\frac{n(s, a, s^\prime)}{\max\paren{1,n(s,a)}}$,
where $n(s,a,s^\prime) :=\sum_{i=1}^m \mathbf{1}\{s_i=s, a_i=a, s_i^\prime=s^\prime\}$ and $n(s,a)=\sum_{s^\prime} n(s,a,s^\prime)$.
The reason why we do not directly plug $\widehat P$ into Problem \eqref{Problem_SICMDP_LSIP} and solve the resulting LSIP problem is due to the fact that there is no guarantee that the LSIP problem w.r.t.\ $\widehat P$ is feasible.
To address this issue, we construct an optimistic set $M_\delta$ such that with high probability the true SICMDP $M$ lies in $M_\delta$.
In particular, $M_\delta$ is defined via the empirical Bernstein's bound and the Hoeffding's bound \citep{LATTIMORE2014125}:
%\begingroup
%\small
\begin{align*}
M_\delta :=& \Big\{\langle \gS,\gA,Y,P^\prime,r,c,u,\mu, \gamma \rangle\colon  |P^\prime(s^\prime|s,a)-\widehat P(s^\prime|s,a)| \leq d_\delta(s,a,s^\prime), \forall s, s^\prime\in \gS, a\in \gA \Big\},
\end{align*}
%\endgroup
where 
% $$\begin{aligned}
% d_\delta(s,a,s^\prime):=&\min\bigg\{\sqrt{[2\widehat P(s^\prime|s,a)(1-\widehat P(s^\prime|s,a))\log(4/\delta)]/n(s,a,s^\prime)}\\
% &+4\log (4/\delta)/n(s,a,s^\prime), \sqrt{\log (2/\delta)/2n(s,a,s^\prime)}\bigg\}.
% \end{aligned}
% $$
\begin{align*}
d_\delta(s,a,s^\prime):=&\min\left\{\sqrt{\frac{2\widehat P(s^\prime|s,a)(1 {-} \widehat P(s^\prime|s,a))\log(4/\delta)}{n(s,a,s^\prime)}}+\frac{4\log (4/\delta)}{n(s,a,s^\prime)}, \; \sqrt{\frac{\log (2/\delta)}{2n(s,a,s^\prime)}}\right\}.
\end{align*}


The next step is to solve the optimistic planning problem:
\begin{equation}\label{Problem_Optimistic}
\begin{aligned}
\max_{M^\prime\in M_\delta,\pi}\ V_r^{\pi,M^\prime}(\mu),\quad
\text{s.t.}\ V_{c_y}^{\pi,M^\prime}(\mu) \leq u_y,\ \forall y\in Y,
\end{aligned}
\end{equation}
where the superscript $M^\prime$ denotes that the expectation is taken w.r.t.\ SICMDP $M^\prime$.
\begin{theorem}\label{Theorem_Feasible}
Suppose $n\geq 3$. With probability at least $1-2|\gS|^2|\gA|\delta$, we have that $M\in M_\delta$, and Problem (\ref{Problem_Optimistic}) is feasible.
\end{theorem}

\proof {Proof of Theorem~\ref{Theorem_Feasible}}
See Appendix~\ref{Appendix_Proofs_4}.
\endproof

Note that the optimization variables include both $M^\prime$ and $\pi$, and LSIP reformulations like Problem (\ref{Problem_SICMDP_LSIP}) would no longer be possible. 
Instead, we shall introduce the state-action-state occupancy measure $z(s,a,s^\prime)$.
In particular, assuming $z_{P,\pi}(s,a,s^\prime):=P(s^\prime|s,a)q_\pi(s,a)$, we have $P(s^\prime|s,a)=\frac{z_{P,\pi}(s,a,s^\prime)}{\sum_{x\in \gS}z_{P,\pi}(s,a,x)}$, and $\pi(a|s)=\frac{\sum_{s^\prime\in \gS}z_{P,\pi}(s,a,s^\prime)}{\sum_{s^\prime\in \gS,a^\prime\in \gA}z_{P,\pi}(s,a^\prime,s^\prime)}$. 
Problem (\ref{Problem_Optimistic}) can be reformulated as the following extended LSIP problem:

\begingroup
\small
\begin{equation}\label{Problem_Optimistic_ELSIP}
\begin{aligned}
    \max_{z}\ &\sum_{s, a,s^\prime}z(s,a,s^\prime)r(s,a) \\
    \text{s.t.}\ &\frac{1}{1-\gamma}\sum_{s, a,s^\prime}z(s,a,s^\prime)c_y(s,a)\leq u_y,\ \forall y\in Y, \\
    &z(s,a,s^\prime)\leq (\widehat P(s^\prime|s,a)+d_\delta(s,a,s^\prime))\sum_{x\in \gS} z(s,a,x), \forall s,s^\prime,\ a\in \gA, \\
    &z(s,a,s^\prime)\geq (\widehat P(s^\prime|s,a)-d_\delta(s,a,s^\prime))\sum_{x\in \gS} z(s,a,x), \forall s,s^\prime\in \gS,\ a\in \gA, \\
    &\sum_{x\in \gS,b\in \gA}z(s,b,x)=(1-\gamma)\mu(s)+\gamma\sum_{x\in \gS,b\in \gA}z(x,b,s), \forall s\in \gS, \\
    &z\succeq 0.
\end{aligned}
\end{equation}
\endgroup

However, compared to LP problems, LSIP problems are typically harder to solve and there are no all-purpose LSIP solvers.
Here, we choose the simple yet effective dual exchange methods \citep{Hu1990,reemtsen1998numerical} to solve Problem~\ref{Problem_Optimistic_ELSIP}.
The SI-CRL algorithm can be summarized in Algorithm~\ref{Algorithm_SICRL}.
A key ingredient of Algorithm~\ref{Algorithm_SICRL} is solving the inner-loop optimization problem 
$$
\max_{y\in Y} \sum_{s, a,s^\prime}z(s,a,s^\prime)c_y(s,a)-u_y.
$$
We can obtain different versions of SI-CRL algorithm by choosing different optimization subroutines to solve the inner-loop problem above. 
If $c_y$ and $u_y$ satisfy conditions like concavity and smoothness, then the inner problem can be solved using methods like projected subgradient ascent \citep{bubeck2015convex}.
If the inner problem is ill-posed, we may still solve it using methods like random search \citep{solis1981minimization, andradottir2015review}.
\begin{algorithm}[htb]
   \caption{SI-CRL}
   \label{Algorithm_SICRL}
\begin{algorithmic}
   \STATE {\bfseries Input:} state space $\gS$, action space $\gA$, dataset $\{(s_i,a_i,s_i^\prime)|i=1,2,...,m\}$, reward function $r$, a continuum of cost function $c$, index set $Y$, value for constraints $u$, discount factor $\gamma$, tolerance $\eta$, maximum iteration number $T$.
   \FOR{each $(s,a,s^\prime)$ tuple}
   \STATE Set $\widehat P(s^\prime|s,a):=\frac{\sum_{i=1}^m \ind\{s_i=s,a_i=a,s_i^\prime=s^\prime\}}{\max\paren{1,\sum_{i=1}^m \ind\{s_i=s,a_i=a\}}}$
   \ENDFOR
   \STATE Initialize $Y_0=\{y_0\}$
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE Use an LP solver to solve a finite version of Problem (\ref{Problem_Optimistic_ELSIP}) by only considering constraints in $Y_0$ and store the solution as $z^{(t)}$.
   \STATE Find $y^{(t)}\approx\argmax_{y\in Y} \sum_{s, a,s^\prime}z^{(t)}(s,a,s^\prime)c_y(s,a)-u_y$.
   \IF {$\sum_{s, a,s^\prime}z(s,a,s^\prime)c_{y^{(t)}}(s,a)-u_{y^{(t)}} \leq\eta$}
   \STATE  Set $z^{(T)}=z^{(t)}$.
   \STATE  {\bfseries BREAK}
   \ENDIF
   \STATE Add $y^{(t)}$ to $Y_0$.
   \ENDFOR
   \FOR{each $(s,a)$ pair}
   \STATE Set $\hat\pi(a|s)=\frac{\sum_{s^\prime}z^{(T)}(s,a,s^\prime)}{\sum_{s^\prime,a^\prime}z^{(T)}(s,a^\prime,s^\prime)}$.
   \ENDFOR
   \STATE {\bfseries RETURN} $\hat{\pi}$.
\end{algorithmic}
\end{algorithm}


