We give PAC-type bounds for SI-CRL under different settings.
The error of SI-CRL is decomposed into two parts: the optimization error from the fact that the solution of (\ref{Problem_Optimistic}) obtained by the dual exchange method is inexact and the statistical error from approximating Problem (\ref{Problem_SICMDP}) with Problem (\ref{Problem_Optimistic}).
On the optimization side, we show that if the inner maximization problem w.r.t.\ $y$ is solved via random search or projected subgradient ascent, the dual exchange method would produce an $\epsilon$-optimal solutions (see Definition \ref{Definition_Optimal_Solution})
%\footnotemark 
when the number of iterations $T=O\left(\left[\frac{\mathrm{diam}(Y)|\gS|^2|\gA|}{(1-\gamma)\epsilon}\right]^m \right)$.
% , where $L_y$ is the Lipschitz constant defined in Assumption \ref{Assumption_Lipschitz}.

On the statistical side, our goal is to determine how many samples are required to make SI-CRL an $(\epsilon, \delta)$-optimal~(see Definition \ref{Definition_PAC}) when Problem (\ref{Problem_Optimistic}) can be solved exactly, i.e., we want to find the sample complexity of SI-CRL  (see Definition \ref{Definition_PAC}).
%\footnotetext{The $(\epsilon, \delta)$-optimality would be defined in Definition \ref{Definition_PAC}}
We show that the sample complexity of SI-CRL is $\widetilde O\left(\frac{|\gS|^2|\gA|^2}{\epsilon^2(1-\gamma)^3}\right)$ if the dataset we use is generated by a generative model, and $\widetilde O\left(\frac{|\gS||\gA|}{\nu_{\min} \epsilon^2(1-\gamma)^3}\right)$ if the dataset we use is generated by a probability measure $\nu$ defined on the space $\gS\times \gA$ and $P(\cdot|s,a)$ as considered in \cite{chen2019information}.
% \wenhao{Why do we use $\Omega(\cdot)$ represent upper bound? $\Omega(\cdot)$ is used to show lower bound while $O(\cdot)$ is used to show upper bound.}
% \liangyu{Resolved.}
Here $\widetilde O$ means that all logarithm terms are discarded, and $\nu_{\min}:=\min_{\nu(s,a)>0}\nu(s,a)$.
% It can be noted that the order of our sample complexity bound increases by a factor of $|\gS||\gA|$ compared to that of ordinary discounted MDP \citep{azar2013minimax}.
We will present our theoretical analysis in more detail in the following part of this section.
%\footnotetext{The $\epsilon$-optimal solutions is defined in Definition \ref{Definition_Optimal_Solution}}


\subsubsection{Preliminaries}
%In addition to the notation defined in Sections \ref{Section_SICMDP} and \ref{Section_Algorithm}, 
% Given a stationary policy $\pi$, we define the value function $V^\pi(s)=\EB\paren*{\sum_{t=0}^\infty \gamma^t r(s_t,a_t)|s_0=s}$, $V^\pi=(V^\pi(s_1), \ldots, V^\pi(s_{|\gS|}))^\top\in \RB ^{|\gS|}$.
% Thus we have $V^\pi(\mu)=\mu^\top V^\pi$.
% Similarly, we define the expected cost $C_y^\pi(s)=\EB\paren*{\sum_{t=1}^\infty \gamma^t c_y(s_t,a_t)|s_0=s}$, $C_y^\pi=(C_y^\pi(s_1), \ldots, C_y^\pi(s_{|\gS|}))^\top\in \RB ^{|\gS|}$,  thus $C_y^\pi(\mu)=\mu^\top C_y^\pi$. 
% Suppose $\tilde\pi,\widetilde M$ are the solution of Problem ($\ref{Problem_Optimistic}$) and $\widetilde M=\langle \gS,\gA,Y,\widetilde P,r,c,u,\mu,\gamma \rangle$.
% For a given stationary policy $\pi$, $\widetilde V_r^\pi(\mu)$, $\widetilde V_{c_y}^\pi(\mu)$, to represent the value function, expected cost, of SICMDP $\widetilde M$, respectively.

Let $\pi^*$ denote the optimal policy.
An $(\epsilon,\delta)$-optimal policy is defined as follows. 
\begin{definition}\label{Definition_PAC}
An RL algorithm is called $(\epsilon,\delta)$-optimal for $\epsilon,\delta>0$ if with probability at least $1-\delta$ it can return a policy $\pi$ such that
$$
\begin{aligned}
V_r^{\pi^*}(\mu)-V_r^{\pi}(\mu) &\leq \epsilon;\quad
V_{c_y}^{\pi}(\mu) - u_y  \leq \epsilon, \forall y\in Y.
\end{aligned}
$$
% \wenhao{There is no randomness of a given policy $\pi$. $\delta$ can be removed. }
% \liangyu{Resolved.}
\end{definition}
An $\epsilon$-optimal solution of Problem (\ref{Problem_Optimistic}) is defined as \begin{definition}\label{Definition_Optimal_Solution}
A stationary policy $\hat\pi$ is called an $\epsilon$-optimal solution of Problem (\ref{Problem_Optimistic}) for $\epsilon>0$ if 
$$
\begin{aligned}
|V_r^{\hat\pi}(\mu)-V_r^{\tilde\pi}(\mu)| &\leq \epsilon \quad \mbox{and} \quad
|V_{c_y}^{\hat\pi}(\mu) - u_y|  \leq \epsilon,  \forall y\in Y \\
\end{aligned}
$$
hold simultaneously.
\end{definition}

Unless otherwise specified, we assume that $\forall (s,a)\in \gS\times \gA$, $c_y(s,a)$ is $L_y$-Lipschitz in $y$ w.r.t.\ $\|\cdot\|_\infty$.
We also assume that $u_y$ is $L_y$-Lipschitz in $y$ w.r.t.\ $\|\cdot\|_\infty$.
The assumptions can be formally stated as:
\begin{assumption}\label{Assumption_Lipschitz}
$c_y(s,a)$ and $u_y$ are Lipschitz in $y$ w.r.t.\ $\|\cdot\|_\infty$, i.e., $\exists L_y>0$ s.t. $\forall y,y^\prime\in Y, (s,a)\in \gS\times \gA, |c_y(s,a)-c_{y^\prime}(s,a)|\leq L_y\|y-y^\prime\|_\infty, 
|u_y-u_{y^\prime}|\leq L_y\|y-y^\prime\|_\infty$.
\end{assumption}
The Lipschitz assumption is usually necessary when dealing with a semi-infinitely constrained problem \citep{still2001discretization,Hu1990}.
And this assumption is indeed quite mild because $Y$ is a compact set.

We say an offline dataset $\{(s_i,a_i,s_i^\prime)|i=1, 2, \ldots, n\}$ to be generated by a generative model if we sample according to $P(\cdot|s,a)$ for each $(s,a)$-pair $n_0=n/|\gS||\gA|$ times and record the results in the dataset.
We say an offline dataset to be generated by probability measure $\nu$ and $P(\cdot|s,a)$ if $(s_i,a_i)\stackrel{i.i.d.}{\sim} \nu$ and $s_i^\prime\sim P(\cdot|s_i,a_i)$.

We solve the inner-loop problem in Algorithm~\ref{Algorithm_SICRL} with random search or projected gradient ascent.
The idea of random search is simple.
For an objective $f(y)$ defined on domain $Y$, we form a random grid of $Y$ consisting of $M$ grid points and select the grid point with the largest objective value.
The precise definition can be found in Algorithm~\ref{Algorithm_random_search} in Appendix~\ref{Appendix_Algorithm}.
The projected subgradient ascent is defined in a standard way \citep{bubeck2015convex}.
The precise definition can be found in Algorithm~\ref{Algorithm_projected_GD} in Appendix~\ref{Appendix_Algorithm}.

\subsubsection{Iteration Complexity of SI-CRL}

We give the iteration complexity of SI-CRL, i.e., how many iterations are required to output an $\epsilon$-optimal solution of Problem (\ref{Problem_Optimistic}) when near-optimal solutions of the inner-loop optimization problems can be obtained.
Our result is similar to Theorem 4 in \cite{Hu1990}.
Specifically, we consider two different cases: 1) we make no assumption of the constraint and use random search to solve the inner-loop problem; 2) we assume the constraint is concave and use projected subgradient ascent to solve the inner-loop problem.
% The random search algorithm and the projected subgradient ascent algorithm are defined by Algorithm~\ref{Algorithm_random_search} and Algorithm~\ref{Algorithm_projected_GD} in Appendix~\ref{Appendix_Algorithm}, respectively.

Before we give the iteration complexity of the case of random search, we make the following assumption to ensure technical rigor.
\begin{assumption}\label{Assumption_regular_maxima}
     For any $(s,a)\in\gS\times\gA$ and weight $v\in \RB^{\gS\times\gA}$, let $y_0\in\arg\max_{y\in Y} (v^\top  c_y-u_y)$. Then $\exists y_0$ such that
    $$
    \{y:\|y-y_0\|_\infty\leq \epsilon_0\}\subset Y.
    $$
\end{assumption}
Assumption~\ref{Assumption_regular_maxima} guarantees any possible solution of the inner-loop problem lies in the interior of $Y$.
\begin{theorem}\label{Theorem_Iteration_Complexity_Random_Search}
Suppose we use random search to solve the inner-loop problem of the SI-CRL algorithm, then if we set the size of random grid $M=O\left(\frac{\log(\delta/T)}{\log \left(1-((1-\gamma)\epsilon/|\gS|^2|\gA|\mathrm{diam}(Y))^m\right)}\right)$, $T=O\left(\left[\frac{\mathrm{diam}(Y)|\gS|^2|\gA|}{(1-\gamma)\epsilon}\right]^m \right)$, SI-CRL would output a $\epsilon$-optimal solution of Problem~\ref{Problem_Optimistic_ELSIP} with probability at least $1-\delta$.
Here we require $\epsilon\leq \frac{2|\gS|^2|\gA|L_y\epsilon_0}{1-\gamma}$.
\end{theorem}

\proof{Proof of Theorem~\ref{Theorem_Iteration_Complexity_Random_Search}.}
See Appendix~\ref{Appendix_Proofs_SICRL}.
\endproof

To derive theoretical guarantees for the case of projected subgradient ascent, we need the following assumption of concavity.

\begin{assumption}\label{Assumption_concave_constraint}
     For any $(s,a)\in\gS\times\gA$, $c_y(s,a)$ is concave in $y$. In addition, $u_y$ is convex in $y$.
\end{assumption}

\begin{theorem}\label{Theorem_Iteration_Complexity_Projected_GD}
Suppose we use projected gradient ascent to solve the inner-loop problem of the SI-CRL algorithm, then if we set the iteration number of the projected subgradient ascent $T_{PGA}=O\left(\frac{|\gS|^4|\gA|^2\mathrm{diam}(Y)^2}{(1-\gamma)^2\epsilon^2}\right)$, $T=O\left(\left[\frac{\mathrm{diam}(Y)|\gS|^2|\gA|}{(1-\gamma)\epsilon}\right]^m \right)$, SI-CRL would output a $\epsilon$-optimal solution of Problem~\ref{Problem_Optimistic_ELSIP}.
\end{theorem}

\proof{Proof of Theorem~\ref{Theorem_Iteration_Complexity_Projected_GD}.}
See Appendix~\ref{Appendix_Proofs_SICRL}.
\endproof

% \begin{remark}
The most crucial part of our proof is a $\epsilon$-packing argument.
Suppose we can get a $\epsilon/2$-optimal solution to the inner-loop problem by either random search of projected subgradient ascent and set the tolerance $\eta=\epsilon/2$.
By the assumption of Lipschitzness and the construction of the SI-CRL algorithm, for any $t\leq T$, either the SI-CRL algorithm has already terminated and we obtain a $\epsilon$-optimal solution to Problem~\ref{Problem_Optimistic_ELSIP}, or $\{B^{(t^\prime)},t=1,...,t\}$ forms a packing of $Y$.
Here $B^{(t^\prime)}:=\{y:\|y-y^{(t^\prime)}\|_\infty\leq \epsilon/2\beta\}$, and $\beta$ is some Lipschitz coefficient.
Then we may draw the conclusion by noting that the maximum iteration number of SI-CRL is no larger than the $\epsilon/2\beta$-packing number of $Y$.
We find that \cite{Hu1990} also used similar techniques to derive their convergence rate, although they assume the inner-loop problem can always be solved exactly.
% \end{remark}

\begin{remark}
    The iteration complexity of the SI-CRL algorithm grows with $m$ in an exponential manner.
    Thus from a theoretical viewpoint, the SI-CRL algorithm is no better than the naive discretization method mentioned in Remark~\ref{Remark_Baseline}.
    However, we find SI-CRL is far more efficient than the naive method in empirical evaluations.
    Perhaps it is because our bound of iteration complexity is obtained by the packing argument and not tight enough.
    Hopefully, the bound can be tightened by a refined analysis of the dynamics of $\{(y^{(t)}, z^{(t)}),t=1,...,T\}$.
\end{remark}

\subsubsection{Sample Complexity of SI-CRL}
%To begin with, 
We consider the case where the offline dataset we use is generated by a generative model.
First, we consider a restricted setting as in \cite{LATTIMORE2014125} where for each $(s,a)$-pair in the true SICMDP there are at most two possible next-states and provide the sample complexity bound.
Then we will drop Assumption \ref{Assumption_Two_Nonzero} using the same strategy as in \cite{LATTIMORE2014125} and derive the sample complexity bound of the general case.
\begin{assumption}\label{Assumption_Two_Nonzero}
The true unknown SICMDP $M$ satisfies $P(s^\prime|s,a)=0$ for all but two $s^\prime\in \gS$ denoted as $sa^+$ and  $sa^-\in \gS$.
\end{assumption}


Although Assumption \ref{Assumption_Two_Nonzero} seems quite restrictive, we argue that it is necessary to establish sharp sample complexity bound, as shown in \cite{LATTIMORE2014125}.
Specifically, without this assumption the ``quasi-Bernstein bound'' (Lemma \ref{Lemma_Quasi_Bernstein}) will not hold, thus we may not be able to get the $\widetilde O((1-\gamma)^{-3})$ bound.

\begin{lemma}\label{Lemma_Bound_on_V}
Suppose Assumption \ref{Assumption_Two_Nonzero} holds, and the dataset we use is generated by a generative model with $n/|\gS||\gA|=n_0>\max\left\{\frac{36\log4/\delta}{(1-\gamma)^2}, \frac{4\log4/\delta}{(1-\gamma)^3}\right\}$. Then with probability $1-2|\gS|^2|\gA|\delta$, we have that
$$\begin{aligned}
V_r^{\pi^*}(\mu)-V_r^{\tilde\pi}(\mu)\leq 24\sqrt{\frac{\log 4/\delta}{{n_0}(1-\gamma)^3}};\quad
V_{c_y}^{\tilde \pi}(\mu) - u_y \leq 12\sqrt{\frac{\log 4/\delta}{{n_0}(1-\gamma)^3}}, \; \forall y\in Y.
\end{aligned}
$$
Here $\tilde\pi$ is the exact solution of Problem~\ref{Problem_Optimistic}.
\end{lemma}
\proof{Proof of Lemma~\ref{Lemma_Bound_on_V}.}
See Appendix~\ref{Appendix_Proofs_SICRL}.

\begin{theorem}\label{Theorem_Sample_Complexity}
Suppose Assumption \ref{Assumption_Two_Nonzero} holds, the dataset we use is generated by a generative model and Problem \ref{Problem_Optimistic} can be solved exactly. Then when $n=O\left(\frac{|\gS||\gA|\log \paren{8|\gS|^2|\gA|/\delta}}{\epsilon^2(1-\gamma)^3}\right)$, SI-CRL is $(\epsilon,\delta)$-optimal.
\end{theorem}
\proof{Proof of Theorem~\ref{Theorem_Sample_Complexity}.}
Theorem~\ref{Theorem_Sample_Complexity} is a direct consequence of Lemma~\ref{Lemma_Bound_on_V}.

\begin{theorem}\label{Theorem_Sample_Complexity_General}
Suppose the dataset we use is generated by a generative model and Problem \ref{Problem_Optimistic} can be solved exactly. Then when $n=O\left(\frac{|\gS|^2|\gA|^2\paren{\log|\gS|}^3\log \paren{8|\gS|^4|\gA|^3/\delta}}{\epsilon^2(1-\gamma)^3}\right)$, a modification of SI-CRL is $(\epsilon,\delta)$-optimal.
\end{theorem}
\proof{Proof of Theorem~\ref{Theorem_Sample_Complexity_General}.}
See Appendix~\ref{Appendix_Proofs_SICRL}.


% \begin{remark}
Our proof strategy is similar to \cite{LATTIMORE2014125}. 
However, to get a $\widetilde O((1-\gamma)^{-3})$ bound \cite{LATTIMORE2014125} used a tedious recursion argument.
We greatly simplify the proof and achieve improvements in log terms (by a factor of $(\log(\frac{|\gS|}{\epsilon(1-\gamma)}))^2$) using sharper bounds on local variances of MDPs developed in \cite{pmlr-v125-agarwal20b}.
% \end{remark}

% \begin{remark}\label{Remark_Sample_Complexity_Assumption}
% Although Assumption \ref{Assumption_Two_Nonzero} seems quite restrictive, we argue that it is necessary to establish sharp sample complexity bound, as shown in \cite{LATTIMORE2014125}.
% Specifically, without this assumption the ``quasi-Bernstein bound'' (Lemma \ref{Lemma_Quasi_Bernstein}) will not hold, thus we may not be able to get the $\widetilde O((1-\gamma)^{-3})$ bound.
% \end{remark}

\begin{remark}\label{Remark_Sample_Complexity_General_Dependence_on_Constraints}
It can be noted that our sample complexity bound does not rely on the constraint set $Y$.
This is because we consider the setting where $r$ and $c_y$ are known deterministic functions and the only source of randomness comes from estimating the unknown transition dynamic using an offline dataset.
In other words, the constraints do not make the problem more difficult in the statistical sense.
\end{remark}

\begin{remark}\label{Remark_Modification}
Here ``a modification of SI-CRL" stands for the following procedure: first we transform the original SICMDP to a new SICMDP satisfying Assumption~\ref{Assumption_Two_Nonzero}, then we run SI-CRL to solve the new SICMDP.
One may refer to the proof in Appendix~\ref{Appendix_Proofs_SICRL} for more details.
\end{remark}

Now we generalize our results to the case where the offline dataset is generated by a probability measure.
\begin{theorem}\label{Theorem_Sample_Complexity_General_Measure}
Suppose the dataset we use is generated by a probability measure $\nu$ and Problem \ref{Problem_Optimistic} can be solved exactly. Then when $m=O\left(\frac{|\gS||\gA|\paren{\log|\gS|}^3\log \paren{8|\gS|^4|\gA|^3/\delta}}{\nu_{\min} \epsilon^2(1-\gamma)^3}\right)$, a modification of SI-CRL is $(\epsilon,\delta)$-optimal.
\end{theorem}
\proof{Proof of Theorem~\ref{Theorem_Sample_Complexity_General_Measure}.}
See Appendix~\ref{Appendix_Proofs_SICRL}.