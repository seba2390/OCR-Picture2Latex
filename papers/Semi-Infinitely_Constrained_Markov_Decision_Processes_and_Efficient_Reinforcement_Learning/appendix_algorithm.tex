\section{Omitted Algorithms}\label{Appendix_Algorithm}

\begin{algorithm}[htb!]
   \caption{Random Search}
   \label{Algorithm_random_search}
\begin{algorithmic}
   \STATE {\bfseries Input:} Objective function $f\colon Y\to\RB$, where $Y$ is a compact subset of $\RB^m$.
   \STATE Sample $y_1,...,y_M\stackrel{i.i.d.}{\sim}\mathrm{Unif}(Y)$.
   \STATE {\bfseries RETURN} $\hat y=y_{i_0}$, $i_0=\argmax_{i\in\{1,...,M\}}f(y_i)$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb!]
   \caption{Projected Subgradient Ascent}
   \label{Algorithm_projected_GD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Objective function $f\colon Y\to\RB$, where $Y$ is a compact subset of $\RB^m$, the maximum number of iterations $T_{PGA}$.
   \STATE Initialize: set $y_0$ as an arbitrary element of $Y$, learning rate $\alpha=\frac{\mathrm{diam}(Y)}{L_y\sqrt{T}}$.
   \FOR {$t=0,...,T_{PGA}-1$}
   \STATE $t_{t+0.5}=y_t+\alpha g_t$, where $g_t$ is a subgradient of $f$ at $y_t$.
   \STATE $y_{t+1}=\argmin_{y\in Y} \|y-y_{t+0.5}\|$.
   \ENDFOR
   \STATE {\bfseries RETURN} $\hat y=\frac{1}{T_{PGA}}\sum_{t=1}^{T_{PGA}} y_t$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb!]
   \caption{Sample-based NPG}
   \label{Algorithm_sample_based_NPG}
\begin{algorithmic}
   \STATE {\bfseries Input:} state space $\gS$, action space $\gA$, a criterion function $b$ (Can be the reward function $r$ or cost function $c_y$ for some fixed $y$), discount factor $\gamma$, policy $\pi_\theta$, number of paths $K_{sgd}$, fixed horizon $H$, upper bound of parameters' norm $W$, learning rate $\{\eta_k\}$, weight $\{\gamma_k\}$
   \FOR{$k=0$ {\bfseries to} $K_{sgd}-1$}
      \STATE Draw $(s,a)\sim \nu$, with $\nu(s,a)=d^{\pi_\theta}(s)\pi_\theta(a|s)$.
   \STATE Execute policy $\pi_\theta$ from $(s,a)$ for $H$ steps, then construct the estimators as
   $$
   \begin{aligned}
   \widehat Q^{\pi_\theta}(s,a)=\sum^{H-1}_{k=0} \gamma^k b(s_k,a_k),\ \text{where } (s_0,a_0)=(s,a).
   \end{aligned}
   $$
   \STATE Execute policy $\pi_\theta$ from $s$ for $H$ steps, then construct the estimators as
   $$
   \widehat V^{\pi_\theta}(s,a)=\sum^{H-1}_{k=0} \gamma^k b(s_k,a_k),\ \text{where } s_0=s.
   $$
   \STATE Set $\widehat A^{\pi_\theta}(s,a)=\widehat Q^{\pi_\theta}(s,a)-\widehat V^{\pi_\theta}(s)$.
   \STATE Perform an iteration of projected SGD: $w^{(k+1)}=\operatorname{Proj}_{B(0,W,\|\cdot\|_2)}(w^{(k)}-\eta_k G^{(k)})$ with
   $$
   \begin{aligned}
      G^{(k)}&=2({w^{(k)}}^\top\nabla_\theta\log\pi_\theta(a|s)-\widehat A^{\pi_\theta}(s,a))\nabla_\theta\log\pi_\theta(a|s),\\
   \end{aligned}
   $$
   and $B(0,W,\|\cdot\|_2):=\{w\in\RB^d|\|w\|_2\leq W\}$.
   \ENDFOR
   \STATE {\bfseries RETURN} $\sum_{k=1}^K \gamma_k w^{(k)}$ as a NPG update direction at $\pi_\theta$ w.r.t. criterion function $b$.
\end{algorithmic}
\end{algorithm}



