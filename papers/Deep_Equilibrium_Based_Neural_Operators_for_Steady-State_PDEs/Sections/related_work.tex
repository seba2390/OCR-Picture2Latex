\section{Related Work}
\label{sec:related_work}

Neural network based approaches for solving PDEs 
can broadly be divided into two categories. 
First are hybrid solvers~\citep{bar2019learning, kochkov2021machine, hsieh2019learning}
which use neural networks in conjunction with existing numerical solvers. The main motivation is to 
not only improve upon the existing solvers, but to 
also replace the more computationally inefficient parts of the solver
with a learned counter part.
Second set of approaches are 
full machine learning based approaches
that aim 
to leverage the 
approximation capabilities of neural networks~\citep{hornik1989multilayer}
to directly learn the dynamics of the physical system from observations.

Hybrid solvers like \citet{hsieh2019learning} use a neural network to learn a correction term 
to correct over an existing hand designed solver for a Poisson equation, and
also provide convergence guarantees of their method to the solution of the PDE.  
However, the experiments in their paper are limited to linear elliptic PDEs.
Further, solvers like~\citet{bar2019learning} use neural networks to derive the discretizations for a given PDE, thus enabling 
the use of a low-resolution grid in the numerical solver. 
Furthermore,~\citet{kochkov2021machine} use neural networks to interpolate differential operators between grid points
of a low-resolution grid with high accuracy. 
This work specifically focuses on solving Navier-Stokes equations, 
their method is more accurate than numerical techniques like Direct Numerical Simulation (DNS) 
with a low-resolution grid, and is also $80\times$ more faster.
~\citet{brandstetter2022message} introduced a message passing based hybrid scheme 
to train a hybrid solver and also propose a loss term which helps improve the stability of hybrid solvers for time dependent PDEs.
However, most of these methods are equation specific, and are not easily
transferable to other PDEs 
from the same family.




The neural network based approach that has recently 
garnered the most interest by the community is that of 
the operator learning framework
~\citep{chen1995universal,kovachki2021neural,lu2019deeponet,li2020fourier,bhattacharya2021model},
which uses a neural network to approximate 
and infinite dimensional operator between two Banach spaces, 
thus learning an entire family of PDEs at once.
~\citet{lu2019deeponet} introduces DeepONet, which uses two 
deep neural networks, referred to as the branch net and trunk net, which are 
trained concurrently to learn from data.
Another line of operator learning framework is that of neural operators~\cite{kovachki2021neural}.
The most successful methodology for neural operators being the
Fourier neural operators (FNO)
~\citep{li2020fourier}.
FNO 
uses convolution based integral kernels which are evaluated in the
Fourier space. 
Future works like~\cite{tran2021factorized}
introduce architectural improvements that enables one to train 
deeper FNO networks, thus increasing their size and improving their 
the performance on a variety of (time-dependent) PDEs. 
Moreover, the success of Transformers in domains like language and vision
has also inspired transformer based neural operators in works like
\citet{li2022transformer, hao2023gnot} and \citet{Liu2022-nd}.
Theoretical results pertaining to the neural operators
mostly include universal approximation results~\cite{kovachki2021universal,lanthaler2022error} which show
that architectures like FNO and DeepONet can indeed approximate the infinite dimension operators.

In this work, we focus on steady-state equations
and show the benefits of weight-tying in improving the performance 
of FNO for steady-state equations. 
We show that instead of making a network deeper and hence increasing the size
of a network, 
weight-tied FNO architectures can outperform FNO and its variants $4\times$
its size. 
We further introduce FNO-DEQ, a deep equilibrium 
based architecture to simulate an infinitely deep weight-tied 
network (by solving for a fixed point)
with $\mathcal{O}(1)$ training memory.
Our work takes inspiration from 
recent 
theoretical works like~\cite{marwah2021parametric, chen2021representation, marwah2022neural}
which derive parametric rates for some-steady state equations, and 
in fact prove that neural networks can approximate solutions to 
some families of PDEs with just $\mbox{poly}(d)$ parameters, 
thus evading the curse of dimensionality.


