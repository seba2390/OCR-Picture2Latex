\section{Problem setting}

We first formally define the system of steady-state PDEs that we will solve for:
\begin{definition}[Steady-State PDE] \label{def:steady_state_PDE}
    Given a bounded open set $\Omega \subset \R^d$, 
    a steady-state PDE 
    can be written in the following general form:
    \begin{equation}
        \begin{split}
            L(a(x), u(x)) &= f(x), \qquad \forall x \in \Omega \\
        \end{split}
    \end{equation}
    Here $L$ is a continuous operator, the function
    $u \in L^2\left(\Omega; \R^{d_u}\right)$
    is the unknown function that we wish to solve for
    and 
    $a \in L^2\left(\Omega; \R^{d_a}\right)$
    collects all the coefficients describing the PDE, and 
    $f \in L^2\left(\Omega; \R^{d_f}\right)$
    is a function independent of $u$.
    We will, for concreteness, assume periodic boundary conditions, i.e. $\forall z \in \Z^d, \forall x \in \Omega$ we have $u(x + z) = u(x)$. (Equivalently,  
    $\Omega:= \sT^d = [0, 2\pi]^d$ can be identified with the torus.)
    \footnote{
   This is for convenience of exposition, our methods can readily be extended to other boundary conditions 
    like Dirichet, Neumann etc. 
    }
    Finally, we will denote $u^\star: \Omega \to \R$ as the solution 
    to the PDE.
\end{definition}
Steady-state models a system at stationarity, \ie when some quantity of interest like temperature or velocity no longer changes over time. Classical numerical solvers for these PDEs include iterative methods like Newton updates
or conjugate gradient descent, typically with carefully chosen preconditioning to ensure benign conditioning and fast convergence.   
Furthermore, recent theoretical works \citep{marwah2021parametric,chen2021representation,marwah2022neural} 
show that for many families of PDEs 
(e.g., steady-state elliptic PDEs that admit a variational formulation), 
iterative algorithms can be efficiently ``neuralized'', 
that is, the iterative algorithm can be represented by a compact neural network, 
so long as the coefficients of the PDE are also representable by a compact neural network.
Moreover, the architectures constructed in these works are heavily weight-tied.    



We will operationalize these developments through the
additional observation that all these iterative schemes can be viewed as algorithms to find a fixed point of a suitably chosen operator. Namely, we can design an operator
$\gG: L^2(\Omega; \R^{d_u}) \times L^2(\Omega; \R^{d_f}) \to L^2(\Omega; \R^{d_u})$ 
\footnote{We note that the choice of defining the operator
with the forcing function $f$ is made for purely expository purposes
the operator $\gG$ can be defined as 
$\gG: \lldu \times L^2(\Omega; \R^{d_a}) \to \lldu$ as well.}
such that $u^\star = \gG(u^\star, f)$ and a lot of common (preconditioned) first and second-order methods are natural ways to recover the fixed points $u^\star$. 

Before describing our architectures, we introduce two components that we will repeatedly use. 

\begin{definition}[Projection and embedding layers] 
    A projection and embedding layer, respectively 
    $\gP: \lldu \times \lldf \to \lldvdv \times \lldvdv$
    and 
    $\gQ: \lldvdv \to L^2(\R^{d_v}; \R^{d_u})$, 
    are defined as
    \begin{align}
        \gP(v, f) 
        &= \left(\sigma\left(W_P^{(1)}v + b_{P}^{(1)} \right),
            \sigma\left(W_P^{(2)}f + b_P^{(2)}\right)\right)\nonumber, 
            \\
        \gQ(v) 
        &= \sigma\left(W_Qv + b_Q \right) \nonumber
    \end{align}
    where 
    $W_P^{(1)} \in \R^{d_u \times d_v}, W_P^{(2)} \in \R^{d_f \times d_v}, W_Q \in \R^{d_v\times d_u}$ 
    and 
    $b_P^{(1)},b_P^{(2)} \in \R^{d_v}, b_Q \in \R^{d_u}$.
\label{d:projection}
\end{definition}

\begin{definition}[Input-injected FNO layer]
\label{d:inputfno}
An input-injected FNO layer 
    $\gL: \lldvdv \times \lldvdv \to \lldvdv$ is defined as 
    \begin{equation}
        \label{eq:fno_layer_updated}
       \gL(v, g) := g + \sigma
            \left(W v + b + \gF^{-1}(R^{(k)} \cdot (\gF v)\right).
    \end{equation}
    where $W \in \R^{d_v \times d_v}$, $b \in \R^{d_v}$
    and $R^{(k)} \in \R^{d_v \times d_v}$ for all $k \in [K]$
    are learnable parameters. 
\end{definition}
Note the difference between the FNO layer specified above, and the standard FNO layer \eqref{eq:fno_layer} is the extra input $g$ to the layer, 
which in our architecture will correspond to a projection of (some or all) of the PDE coefficients.
We also note that this change to the FNO layer also enables us to learn deeper FNO architectures, as
shown in Section~\ref{sec:experiments}.
With this in mind, we can discuss the architectures we propose. 



\paragraph{Weight-tied architecture I: Weight-tied FNO}
The first architecture we consider is a weight-tied version of FNO (introduced in Section~\ref{sec:background}), in which we repeatedly apply ($M$ times) the same transformation in each layer. More precisely, we have:  














\begin{definition}[FNO Weight-Tied]
    \label{def:fno_weight_tied}
    We define a $M$ times weight-tied neural operator 
    $G^M_\theta$ as,
    \begin{align*}
        G^M_\theta = \gQ \circ 
        \underbrace{\gB^L \circ \gB^L \circ \cdots \circ \gB^L}_{\text{M times}} \circ \gP
    \end{align*}
    such that: $\gP, \gQ$ are projection and embedding layers as in Definition~\ref{d:projection} 
        an $\gB^L: \lldvdv \times \lldvdv \to \lldvdv$
    is an $L$-layer FNO block, i.e, $
        \gB^L = \gL_{L} \circ \gL_{L-1} \circ \gL_{L-2} \circ \gL_{1}$ 
        where for all $l \in [L]$, $\gL_l(\cdot, \gP(f))$
        \footnote{We are abusing the notation somewhat and denoting by $\gP(f)$ the second coordinate of $\gP$, which only depends on $f$.} is an input-injected FNO block as in Definition~\ref{d:inputfno}. 
    
\end{definition}



\paragraph{Weight-tied architecture II: FNO-DEQ} %
In cases where we believe a weight-tied $G_\theta^M$ converges to some fixed point as $M \to \infty$, 
unrolling $G_\theta^M$ 
for a large $M$ requires a lot of hardware memory for training:  training the model requires one to store intermediate hidden units 
for each weight-tied layer for backpropagation, resulting in a $\gO(M)$
increase in the amount of memory required. 

To this end, we use Deep Equilibrium models (DEQs) which enables 
us to implicitly train $G_\theta := \lim_{M\to\infty} G_\theta^{M}$ by directly 
solving for the fixed point by leveraging black-box root finding algorithms
like quasi-Newton methods,
~\citep{broyden1965class,anderson1965iterative}. Therefore
we can think of this approach as an infinite-depth (or infinitely unrolled) weight-tied network. 
We refer to this architecture as \textbf{FNO-DEQ}. 
\begin{definition}[FNO-DEQ]
    \label{def:fno_deq}
    Given $\gP, \gQ$ and $\gB^{L}$ in 
    Definition~\ref{def:fno_weight_tied}, 
    
    FNO-DEQ is trained to parametrize the fixed point equation 
    $\gB^{L}\left(v^\star, \gP(f)\right) = v^\star$ and outputs $u^\star = \gQ(v^\star)$.
\end{definition}


Usually, it is non-trivial to differentiate through these black-box root solvers. DEQs enable us to implicitly differentiate through the equilibrium 
fixed point efficiently without any need to backpropagate through these root solvers, therefore resulting in $\gO(1)$ training memory. 

