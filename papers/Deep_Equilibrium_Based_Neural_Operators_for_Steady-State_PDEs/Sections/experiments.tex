\section{Experiments}
\label{sec:experiments}

\textbf{Network architectures.} We consider the following network architectures in our experiments.

\textbf{FNO}: We closely follow the architecture proposed by \citet{li2020fourier} and construct this network by stacking four FNO layers and four convolutional layers, separated by GELU activation \citep{gelu}. Note that in our current set up, we recover the original FNO architecture if the input to the $l^{\text{th}}$ layer is the output of $(l-1)^{\text{th}}$ layer \ie 
$v_l = \gB_{l-1}(v_{l-1})$.

\textbf{Improved FNO (FNO++
)}: 
The original FNO architecture suffers from vanishing gradients, which prohibits it from being made deeper~\citep{tran2021factorized}. We overcome this limitation by introducing residual connections within each block of FNO, with each FNO block $\gB_l$ comprising of three FNO layers $\gL$ \ie $\gB_l = \gL^l_{L_1} \circ \gL^l_{L_2} \circ \gL^l_{L_3}$ and three convolutional layers, where $\gL$ is defined in \cref{eq:fno_layer_updated}.

\textbf{Weight-tied network (FNO-WT)}: 
This is the weight-tied architecture introduced in Definition~\ref{def:fno_weight_tied},
where we initialize $v_0(x) = 0$ for all $x \in \Omega$.

\textbf{FNO-DEQ}: As introduced in Definition~\ref{def:fno_deq}, FNO-DEQ is a
weight-tied network where we explicitly solve for the fixed point in the forward pass with a root finding algorithm. 
We use Anderson acceleration \citep{anderson1965iterative} as the root solver. 
For the backward pass, we use approximate implicit gradients \citep{geng2021training} which are light-weight and more stable in practice, compared to implicit gradients computed by inverting Jacobian. 

Note that both weight-tied networks and FNO-DEQs leverage weight-tying but the two models differ in the ultimate goal of the forward pass: DEQs explicitly solve for the fixed point during the forward pass, while weight-tied networks trained with backpropagation may or may-not reach a fixed point \citep{anil2022path}. 
Furthermore, DEQs require $\mathcal{O}(1)$ memory, as they differentiate through the fixed point implicitly, whereas weight-tied networks need to explicitly create the entire computation graph for backpropagation, which can become very large as the network depth increases.
Additionally, FNO++ serves as a non weight-tied counterpart to a weight-tied input-injected network. Like weight-tied networks, FNO++ does not aim to solve for a fixed point in the forward pass.

\textbf{Experimental setup.} We test the aforementioned network architectures on two families of steady-state PDEs: Darcy Flow equation and steady-state Navier-Stokes equation for incompressible fluids. 
For experiments with Darcy Flow, we use the dataset provided by \cite{li2020fourier}, 
and generate our own dataset for steady-state Navier-Stokes.
For more details on the datasets and the data generation processes we refer to 
Sections~\ref{subsec:Darcy_flow_implementation} 
and ~\ref{subsec:navier_stokes_implementation} of the Appendix.
For each family of PDE, we train networks under 3 different training setups: clean data, noisy inputs and noisy observations. For experiments with noisy data, both input and observations, we add noise sampled from a sequence of standard Gaussians with increasing values of variance $\{ \gN(0, (\sigma_k^2))\}_{k=0}^{M-1}$, 
where $M$ is the total number of Gaussians we sample from. 
We set $\sigma^2_0 = 0$ and $\sigma^2_{\text{max}} = \sigma^2_{M-1} \leq 1/r$, where $r$ is the resolution of the grid. Thus, the training data includes equal number of PDEs with different levels of Gaussian noise added to their input or observations.
We add noise to training data, and always test on clean data. We follow prior work \citep{li2020neural} and report relative $L_2$ norm between ground truth $u^\star$ 
and prediction on test data.
The total depth of all networks besides FNO is given by $6B + 4$, where $B$ is the number of FNO blocks. Each FNO block has 3 FNO layers and convolutional layers. 
In addition, we include the depth due to $\mathcal{P}$, $\mathcal{Q}$, and an additional final FNO layer and a convolutional layer. 
We further elaborate upon network architectures and loss functions in 
in \cref{sec:implementation_details}. 



\vspace{-2mm}
\subsection{Darcy Flow}
\label{subsec:darcy_flow}
For our first set of experiments we consider stationary Darcy Flow equations, a form of linear elliptic PDE with 
in two dimensions. 
The PDE is defined as follows,
\begin{equation}
    \begin{split}
        -\nabla \cdot (a(x)\nabla u(x)) &= f(x), \qquad x \in (0, 1)^2 \\
        u(x) &= 0 \qquad\qquad x \in \partial(0, 1)^2.
    \end{split}
\end{equation}
Note that the diffusion coefficient
$a \in \linf(\Omega; \R_+)$, 
i.e., the coefficients are always positive, 
and 
$f\in
L^2(\Omega; \R^{d_f})$
is the forcing term.
These PDEs are used to model the 
steady-state pressure of fluids flowing through a porous media. 
They can also be used to model the stationary state of the diffusive process with $u(x)$ modeling 
the temperature distribution through the space with $a$ defining the thermal conductivity of the medium.
The task is to learn an operator 
$G_\theta: L^2(\Omega; \R^{d_u}) \times L^2(\Omega; \R^{d_a}) \to L^2(\Omega; \R^{d_u})$
such that $u^\star = G_\theta(u^\star, a)$.


We report the results of our experiments on Darcy Flow in \cref{table:results-darcy-flow-all}. 
The original FNO architecture does not improve its performance  with increased number of FNO blocks $\gB$.
FNO++ with residual connections scales better but saturates at around 4 FNO blocks. In contrast, FNO-WT and FNO-DEQ with just a \emph{single} FNO block outperform deeper non-weight-tied architectures on clean data and on data with noisy inputs. 
When  observations are noisy, FNO-WT and FNO-DEQ outperform FNO++ with a similar number of parameters, and perform comparably to FNO++ with $4\times$ parameters. 

We also report results on shallow FNO-DEQ, FNO-WT and FNO++ architectures. An FNO block in these shallow networks has a single FNO layer instead of three layers. In our experiments, shallow weight-tied networks outperform non-weight-tied architectures including FNO++ with $7\times$ parameters on clean data and on data with noisy inputs, and  perform comparably to deep FNO++ on noisy observations. In case of noisy observations, we encounter training instability issues in FNO-DEQ. We believe that this shallow network lacks sufficient representation power and cannot accurately solve for the fixed point during the forward pass. These errors in fixed point estimation accumulate over time, leading to incorrect values of implicit gradients, which in turn result in training instability issues.
\begin{table*}[th!]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.001$ & $(\sigma^2_{\max})^t=0.001$ \\
\midrule
FNO & 2.37M & 1 & 0.0080 $\pm$ 5e-4 & 0.0079 $\pm$ 2e-4  &  0.0125 $\pm$ 4e-5 \\
FNO & 4.15M & 2 & 0.0105 $\pm$ 6e-4 & 0.0106 $\pm$ 4e-4 & 0.0136 $\pm$ 2e-5 \\
FNO & 7.71M & 4 & 0.2550 $\pm$ 2e-8 & 0.2557 $\pm$ 8e-9 & 0.2617 $\pm$ 2e-9 \\
\midrule
FNO++ & 2.37M & 1 & 0.0075 $\pm$ 2e-4 & 0.0075 $\pm$ 2e-4 &  0.0145 $\pm$ 7e-4 \\
FNO++ & 4.15M & 2 & 0.0065 $\pm$ 2e-4 & 0.0065 $\pm$ 9e-5 & 0.0117 $\pm$ 5e-5 \\
FNO++ & 7.71M & 4 & 0.0064 $\pm$ 2e-4 & 0.0064 $\pm$ 2e-4 & \textbf{0.0109 $\pm$ 5e-4}  \\
S-FNO++ & 1.78M & 0.66 & 0.0093 $\pm$ 5e-4 & 0.0094 $\pm$ 7e-4  & 0.0402 $\pm$ 6e-3 \\
\midrule
FNO-WT & 2.37M & 1 & \textbf{0.0055 $\pm$ 1e-4} & \textbf{0.0056 $\pm$ 5e-5} & 0.0112 $\pm$ 4e-4 \\
FNO-DEQ & 2.37M &  1 & \textbf{0.0055 $\pm$ 1e-4} & \textbf{0.0056 $\pm$ 7e-5} & 0.0112 $\pm$ 4e-4 \\
\midrule
S-FNO-WT 
& 1.19M & 0.33 & 0.0057 $\pm$ 3e-5 & 0.0057 $\pm$ 5e-5 & 0.0112 $\pm$ 1e-4 \\
S-FNO-DEQ 
& 1.19M & 0.33 & 0.0056 $\pm$ 4e-5 & 0.0056 $\pm$ 5e-5 & 0.0136 $\pm$ 0.011 \\
\bottomrule
\end{tabular}}
\caption{Results on Darcy flow: clean data (Col 4),noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.
Here, S-FNO++, S-FNO-WT and S-FNO-DEQ are shallow versions 
of FNO++, FNO-WT and FNO-DEQ respectively.
}
\label{table:results-darcy-flow-all}
\end{table*}
\vspace{-2mm}


\subsection{Steady-state Navier-Stokes Equations for Incompressible Flow}
\label{subsec:navier_stokes}
We consider the steady-state Navier-Stokes equation for an incompressible viscous fluid in the vorticity form
defined on a torus, i.e., with periodic boundary condition,
\begin{equation}
    \begin{split}
        \label{eq:navier_stokes}
        u \cdot \nabla \omega &= \nu \Delta \omega + f, \qquad x \in \Omega\\
        \nabla \cdot u &= 0 \qquad\qquad\quad\;\;  x \in \Omega
    \end{split}
\end{equation}
where $\Omega:=(0, 2\pi)^2$, and $u:\Omega \to \R^2$ is the velocity and 
$\omega:\Omega \to \R$ where $\omega = \nabla \times u$, $\nu \in \R_+$ is the viscosity 
and $f: \Omega \to \R$ is the external force term.
We learn an operator $G_\theta: L^2(\Omega; \R^{d_u}) \times L^2(\Omega; \R^{d_f}) \to L^2(\Omega; \R^{d_u})$, such that 
$u^\star = G_\theta(u^\star, f)$.
We train all the models on data with viscosity values $\nu = 0.01$ and $\nu=0.001$, and create a dataset
for steady-state incompressible
Navier-Stokes, which we will make public as a community benchmark
for steady-state PDE solvers.

Results for Navier-Stokes equation have been reported in \cref{table:results-navier-stokes-visc-0.001-nl-0.001} and \cref{table:results-navier-stokes-visc-0.01-nl-0.001}. For both values of viscosity, FNO-DEQ outperforms other architectures for all three cases: clean data, noisy inputs and noisy observations. 
FNO-DEQ is more robust to noisy inputs compared to non-weight-tied architectures. 
For noisy inputs, FNO-DEQ matches the test-error of noiseless case in case of viscosity $0.01$ and almost matches the test-error of noiseless case in case of viscosity $0.001$. 
We provide additional results for noise level $0.004$ in Appendix~\ref{sec:additional-experiments}. %
FNO-DEQ and FNO-WT consistently outperform non-weight-tied architectures for higher levels of noise as well.

In general, DEQ-based architectures are slower to train  (upto $\sim$2.5$\times$ compared to feedforward networks of similar size) as we solve for the fixed point in the forward pass. 
However, their inductive bias provides performance gains that cannot be achieved by simply stacking non-weight-tied FNO layers. In general, we observe diminishing returns in FNO++ beyond 4 blocks. Additionally, training the original FNO network on more than 4 FNO blocks is challenging, with the network often diverging during training, and therefore we do not include these results in the paper.

\begin{table*}[th!]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.001$ & $(\sigma^2_{\max})^t=0.001$ \\
\midrule
FNO & 2.37M & 1 & 0.184 $\pm$ 0.002 & 0.218 $\pm$ 0.003 & 0.184 $\pm$ 0.001 \\
FNO & 4.15M & 2 & 0.162 $\pm$ 0.024 & 0.176 $\pm$ 0.004 & 0.152 $\pm$ 0.005 \\
FNO & 7.71M & 4 & 0.157 $\pm$ 0.012 & 0.187 $\pm$ 0.004 & 0.166 $\pm$ 0.013 \\
\midrule
FNO++ & 2.37M & 1 & 0.199 $\pm$ 0.001 & 0.230 $\pm$ 0.001 & 0.197 $\pm$ 0.001\\
FNO++ & 4.15M & 2 & 0.154 $\pm$ 0.005 & 0.173 $\pm$ 0.003 & 0.154 $\pm$ 0.006\\
FNO++ & 7.71M & 4 & 0.151 $\pm$ 0.003 & 0.165 $\pm$ 0.004 & 0.149 $\pm$ 0.003\\
\midrule
FNO-WT & 2.37M & 1 & 0.151 $\pm$ 0.007 & 0.173 $\pm$ 0.017 & \textbf{0.126 $\pm$ 0.027}\\
FNO-DEQ & 2.37M &  1 & \textbf{0.128 $\pm$ 0.004} & \textbf{0.144 $\pm$ 0.007} & 0.136 $\pm$ 0.003 \\
\bottomrule
\end{tabular}}
\caption{Results on incompressible steady-state Navier-Stokes (viscosity=0.001): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2. 
} 

\label{table:results-navier-stokes-visc-0.001-nl-0.001}
\end{table*}
\vspace{-4mm}


\begin{table*}[th!]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.001$ & $(\sigma^2_{\max})^t=0.001$ \\
\midrule
FNO & 2.37M & 1 & 0.181 $\pm$ 0.005 & 0.186 $\pm$ 0.003 & 0.178 $\pm$ 0.006 \\
FNO & 4.15M & 2 & 0.138 $\pm$ 0.007 & 0.150 $\pm$ 0.006 & 0.137 $\pm$ 0.012\\
FNO & 7.71M & 4 & 0.152 $\pm$ 0.006 & 0.163 $\pm$ 0.002 & 0.151 $\pm$ 0.008\\
\midrule
FNO++ & 2.37M & 1 & 0.188 $\pm$ 0.002 & 0.207 $\pm$ 0.004& 0.187 $\pm$ 0.003 \\
FNO++ & 4.15M & 2 & 0.139 $\pm$ 0.004 & 0.153 $\pm$ 0.002 & 0.140 $\pm$ 0.005 \\
FNO++ & 7.71M & 4 & 0.130 $\pm$ 0.005 & 0.151 $\pm$ 0.004 & 0.128 $\pm$ 0.009 \\
\midrule
FNO-WT & 2.37M & 1 & 0.099 $\pm$ 0.007 & 0.101 $\pm$ 0.007 & 0.130 $\pm$ 0.044 \\
FNO-DEQ & 2.37M & 1 & \textbf{0.088 $\pm$ 0.006} & \textbf{0.099 $\pm$ 0.007} & \textbf{0.116 $\pm$ 0.011} \\
\bottomrule
\end{tabular}}

\caption{Results on incompressible steady-state Navier-Stokes (viscosity=0.01): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.} 
\label{table:results-navier-stokes-visc-0.01-nl-0.001}
\end{table*}

