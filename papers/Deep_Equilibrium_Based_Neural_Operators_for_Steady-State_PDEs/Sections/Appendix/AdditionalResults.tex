\section{Additional experimental results} 
\label{sec:additional-experiments}

We provide additional results for Navier-Stokes equation for noisy inputs and observations in \cref{table:results-navier-stokes-visc-0.001-nl-0.004} and \cref{table:results-navier-stokes-visc-0.01-nl-0.004}. For these experiments, the maximum variance of Gaussian noise added to inputs and observations is 0.004. We observe that weight-tied FNO and FNO-DEQ outperform non-weight-tied architectures. 


\begin{table*}[th!]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.004$ & $(\sigma^2_{\max})^t=0.004$ \\
\midrule
FNO & 2.37M & 1 & 0.184 $\pm$ 0.002 & 0.238 $\pm$ 0.008 & 0.179 $\pm$ 0.004\\
FNO & 4.15M & 2 & 0.162 $\pm$ 0.024 & 0.196 $\pm$ 0.011 & 0.151 $\pm$ 0.010\\
FNO & 7.71M & 4 & 0.157 $\pm$ 0.012 & 0.216 $\pm$ 0.002 & 0.158 $\pm$ 0.009\\
\midrule
FNO++ & 2.37M & 1 & 0.199 $\pm$ 0.001 & 0.255 $\pm$ 0.002 & 0.197 $\pm$ 0.004 \\
FNO++ & 4.15M & 2 & 0.154 $\pm$ 0.005 & 0.188 $\pm$ 0.006 & 0.157 $\pm$ 0.006 \\
FNO++ & 7.71M & 4 & 0.151 $\pm$ 0.003 & 0.184 $\pm$ 0.008 & 0.147 $\pm$ 0.004\\
\midrule
FNO-WT & 2.37M & 1 & 0.151 $\pm$ 0.007 & 0.183 $\pm$ 0.026 & 0.129 $\pm$ 0.018 \\
FNO-DEQ & 2.37M &  1 & \textbf{0.128 $\pm$ 0.004} & \textbf{0.159 $\pm$ 0.005} & \textbf{0.121 $\pm$ 0.015} \\
\bottomrule
\end{tabular}}
\caption{\textbf{Results on incompressible Steady-State Navier-Stokes (viscosity=0.001)}: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2. \\$\ddagger$ indicates that the network diverges during training for one of the seeds.}\label{table:results-navier-stokes-visc-0.001-nl-0.004}
\end{table*}

\begin{table*}[th!]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.004$ & $(\sigma^2_{\max})^t=0.004$ \\
\midrule
FNO & 2.37M & 1 & 0.181 $\pm$ 0.005 & 0.207 $\pm$ 0.003 & 0.178 $\pm$ 0.008 \\
FNO & 4.15M & 2 & 0.138 $\pm$ 0.007 & 0.163 $\pm$ 0.003 & 0.137 $\pm$ 0.006 \\
FNO & 7.71M & 4 & 0.152 $\pm$ 0.006 &  0.203 $\pm$ 0.055 & 0.151 $\pm$ 0.008 \\
\midrule
FNO++ & 2.37M & 1 & 0.188 $\pm$ 0.002 & 0.217 $\pm$ 0.001 & 0.187 $\pm$ 0.005 \\
FNO++ & 4.15M & 2 & 0.139 $\pm$ 0.004 & 0.170 $\pm$ 0.005 & 0.138 $\pm$ 0.005 \\
FNO++ & 7.71M & 4 & 0.130 $\pm$ 0.005 & 0.168 $\pm$ 0.007 & 0.126 $\pm$ 0.007 \\
\midrule
FNO-WT & 2.37M & 1 & 0.099 $\pm$ 0.007 & 0.159 $\pm$ 0.029 & 0.123 $\pm$ 0.023 \\
FNO-DEQ & 2.37M & 1 & \textbf{0.088 $\pm$ 0.006} & \textbf{0.104 $\pm$ 0.001} & \textbf{0.116 $\pm$ 0.005} \\
\bottomrule
\end{tabular}}
\caption{\textbf{Results on incompressible Steady-State Navier-Stokes (viscosity=0.01)}: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2. \\$\ddagger$ indicates that the network diverges during training for one of the seeds.} \label{table:results-navier-stokes-visc-0.01-nl-0.004}
\end{table*}







\paragraph{Convergence analysis of fixed point.} We report variations in test error, absolute residual $\| G_\theta(\rvz_t) - \rvz_t\|_2$, and relative residual $\frac{\| G_\theta(\rvz_t) - \rvz_t\|_2}{\| \rvz_t \|_2}$ with an increase in the number of solver steps while solving for the fixed point in FNO-DEQ, for both Darcy Flow (See~\cref{tab:darcy_flow_convergence}) and Steady-State Navier Stokes (See~\cref{tab:ns_0.01_convergence}). We observe that all these values decrease with increase in the number of fixed point solver iterations and eventually saturate once we have a reasonable estimate of the fixed point. 
We observe that increasing the number of fixed point solver iterations results in a better estimation of the fixed point. 
For steady state PDEs, we expect the test error to reduce as the estimation of the fixed point improves. 
Furthermore, at inference time we observe that the test error improves (i.e. reduces) with increase in the number of fixed point solver iterations even though the FNO-DEQ is trained with fewer solver steps.
For Navier-Stokes with viscosity 0.01, at inference time we get a test MSE loss of 0.0744 with 48 solver steps from 0.0847 when used with 24 solver steps.

This further bolsters the benefits of DEQs (and weight-tied architectures in general) for training neural operators for steady-state PDEs. Moreover, performance saturates after a certain point once we have a reasonable estimate of the fixed point, hence showing that more solver steps stabilize to the same solution.



\begin{table}[th!]
    \centering
    \begin{tabular}{cccc}
    \toprule
    Solver steps & Absolute residual $\downarrow$  & Relative residual $\downarrow$  & Test Error $\downarrow$ \\
    \midrule
    2 & 212.86 & 0.8533 & 0.0777 \\
    4 & 18.166 & 0.0878 & 0.0269 \\
    8 & 0.3530 & 0.00166 & 0.00567 \\
    16 & 0.00239 & 1.13e-5 & 0.00566 \\
    32 & 0.000234 & 1.1e-6 & 0.00566 \\
    \bottomrule \\
    \end{tabular}
    \caption{Convergence analysis of fixed point for noiseless Darcy Flow: The test error, absolute residual $\| G_\theta(\rvz_t) - \rvz_t\|_2$ and relative residual $\frac{\| G_\theta(\rvz_t) - \rvz_t\|_2}{\| \rvz_t \|_2}$ decrease with increase in the number of fixed point solver iterations. The performance saturates after a certain point once we have a reasonable estimate of the fixed point. We consider the noiseless case, where we do not add any noise to inputs or targets. }
    \label{tab:darcy_flow_convergence}
\end{table}

\begin{table}[th!]
    \centering
    \begin{tabular}{cccc}
    \toprule
    Solver steps & Absolute residual $\downarrow$  & Relative residual $\downarrow$ & Test Error $\downarrow$ \\
    \midrule
    4 & 544.16 & 0.542 & 0.926 \\
    8 & 397.75 & 0.408 & 0.515 \\
    16 & 150.33 & 0.157 & 0.147 \\
    24 & 37.671 & 0.0396 & 0.0847 \\
    48 & 5.625 & 0.0059 & 0.0744 \\
    64 & 3.3 & 0.0034 & 0.0746 \\
    \bottomrule
    \end{tabular}
    \caption{Convergence analysis of fixed point for noiseless incompressible Steady-State Navier-Stokes with viscosity=0.01: The test error, absolute residual $\| G_\theta(\rvz_t) - \rvz_t\|_2$ and relative residual $\frac{\| G_\theta(\rvz_t) - \rvz_t\|_2}{\| \rvz_t \|_2}$ decrease with increase in the number of fixed point solver iterations. The performance saturates after a certain point once we have a reasonable estimate of the fixed point. We consider the noiseless case, where we do not add any noise to inputs or targets.}
    \label{tab:ns_0.01_convergence}
\end{table}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Images/LossCurves/train_loss.png}
        \caption{Training Loss Curve}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Images/LossCurves/test_loss.png}
        \caption{Test Loss Curve}
        \end{subfigure}
    \caption{Training and Test Loss Curves for Steady-State Navier-Stokes with viscosity $0.01$. The $x$ axis is the number of epochs and $y$ axis is the MSE loss in $\log$ scale. 
    Note that while all the models converge to approximately the same MSE loss value while training, DEQs and weight-tied networks get a better test loss in fewer epochs.
    }
    \label{fig:train_val_error}
\end{figure}



\begin{table*}[th!]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\multirow{3}{*}{Architecture} & \multirow{3}{*}{Parameters} & \multirow{3}{*}{\#Blocks} & \multicolumn{3}{c}{Test error $\downarrow$} \\
\cmidrule(lr){4-6}
& & & $\sigma^2_{\max}=0$ & $(\sigma^2_{\max})^i=0.001$ & $(\sigma^2_{\max})^t=0.001$ \\
\midrule
FNO & 2.37M & 1 & 0.0080 $\pm$ 5e-4 & 0.0079 $\pm$ 2e-4  &  0.0125 $\pm$ 4e-5 \\
FNO & 4.15M & 2 & 0.0105 $\pm$ 6e-4 & 0.0106 $\pm$ 4e-4 & 0.0136 $\pm$ 2e-5 \\
FNO & 7.71M & 4 & 0.2550 $\pm$ 2e-8 & 0.2557 $\pm$ 8e-9 & 0.2617 $\pm$ 2e-9 \\
\midrule
FNO++ & 2.37M & 1 & 0.0075 $\pm$ 2e-4 & 0.0075 $\pm$ 2e-4 &  0.0145 $\pm$ 7e-4 \\
FNO++ & 4.15M & 2 & 0.0065 $\pm$ 2e-4 & 0.0065 $\pm$ 9e-5 & 0.0117 $\pm$ 5e-5 \\
FNO++ & 7.71M & 4 & 0.0064 $\pm$ 2e-4 & 0.0064 $\pm$ 2e-4 & \textbf{0.0109 $\pm$ 5e-4}  \\
\midrule
FNO-WT & 2.37M & 1 & \textbf{0.0055 $\pm$ 1e-4} & \textbf{0.0056 $\pm$ 5e-5} & 0.0112 $\pm$ 4e-4 \\
FNO-DEQ & 2.37M &  1 & \textbf{0.0055 $\pm$ 1e-4} & \textbf{0.0056 $\pm$ 7e-5} & 0.0112 $\pm$ 4e-4 \\
\bottomrule
\end{tabular}}
\caption{Results on Darcy flow: clean data (Col 4),noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being $(\sigma^2_{\max})^i$ and $(\sigma^2_{\max})^t$, respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.
Here, S-FNO++, S-FNO-WT and S-FNO-DEQ are shallow versions 
of FNO++, FNO-WT and FNO-DEQ respectively.
}
\label{table:results-darcy-flow-all}
\end{table*}
\vspace{-2mm}