\section{Preliminaries}
\label{sec:background}
We now introduce some key concepts and notation. 
\begin{definition}[$L^2(\Omega; \R^d)$]
    For a domain $\Omega$ we denote by $L^2(\Omega; \R^d)$ 
    the space of square integrable functions $g: \Omega \to \R^d$
    such that 
    $\|g\|_{\ll} < \infty$, where
    $\|g\|_{\ll} = \left(\int_\Omega \|g(x)\|_{\ell_2}^2 dx\right)^{1/2}$.
\end{definition}

\subsection{Neural Operators}
Neural operators \citep{lu2019deeponet, li2020fourier, bhattacharya2021model, patel2021physics, kovachki2023neural} 
are a deep learning approach to learning solution operators which map a PDE to its solution. 
Fourier Neural Operator (FNO) \citep{li2020fourier} is a particularly successful recent architecture 
parametrized as a sequence of kernel integral operator layers followed by non-linear activation functions. Each kernel integral operator layer is a convolution-based kernel function that is instantiated through a linear transformation in Fourier domain, making it less sensitive to the level of spatial discretization. Specifically, an $L$-layered FNO 
$G_\theta: \R^{d_u} \to \R^{d_u}$ 
with learnable parameters $\theta$, is defined as 
\begin{equation}
    \label{eq:fno_layer_def}
    G_\theta := \gQ \circ \mathcal{L}_L \circ \mathcal{L}_{L-1} \circ \cdots \circ \mathcal{L}_1 \circ \gP
\end{equation}
where 
$\mathcal{P}: \lldu \to \lldvdv$ 
and 
$\mathcal{Q}: L^2(\R^{d_v};\R^{d_v}) \to  
L^{2}(\R^{d_v};\R^{d_u}) $ 
are projection operators, 
and 
$\mathcal{L}_{l}: L^2(\R^{d_v};\R^{d_v})\to L^2(\R^{d_v};\R^{d_v})$
for $l \in [L]$ is the $l^{\text{th}}$ FNO layer defined as, 
\begin{equation}
    \label{eq:fno_layer}
    \mathcal{L}_{l}\left(v_{l}\right) 
    = \sigma \left(W_{l} v_{l} + b_{l} + \mathcal{K}_{l}(v_l))\right).
\end{equation}
Here $\sigma$ is a non-linear activation function, $W_l, b_l$ are the $l^{th}$ layer weight matrix and bias terms. Finally
$\mathcal{K}_{l}$ 
is the $l^{th}$ integral kernel operator 
which is calculated using the Fourier transform as introduced in ~\citet{li2020fourier}
defined as follows,
\begin{equation}
    \label{eq:kernal_integral_operator}
    \mathcal{K}_l(v_l) = \mathcal{F}^{-1} \left(R_l \cdot \left(\mathcal{F} v_l\right)\right)(x) \qquad \forall x \in \Omega, 
\end{equation}
where $\calF$ and $\calF^{-1}$ are the 
Fourier transform and the inverse 
Fourier transform, with $R_l$
representing the learnable weight-matrix in the Fourier domain. 
Therefore, ultimately, the trainable parameters $\theta$ is a collection of all the weight matrices and biases, i.e, 
$\theta := \{W_l, b_l, R_l, \cdots, W_1, b_1, R_1\}$.



\subsection{Equilibrium Models}
Equilibrium models \citep{liao2018reviving, bai2019deep, revay2020lipschitz, winston2020monotone} compute internal representations by solving for a fixed point in their forward pass. 
Specifically, consider a deep feedforward network with $L$ layers :
\begin{equation}
    z^{[i+1]} = f_\theta^{[i]}\left(z^{[i]}; x \right) \quad \text{for} \; i = 0, ..., L-1
\end{equation}
where $x \in \mathbb{R}^{n_x}$ is the input injection, $z^{[i]} \in \mathbb{R}^{n_z}$ is the hidden state of $i^{th}$ layer with $z^{[0]} = \mathbf{0}$, and $f_\theta^{[i]} : \mathbb{R}^{n_x \times n_z} \mapsto \mathbb{R}^{n_z}$ is the feature transformation of $i^{th}$ layer, parametrized by $\theta$. 
Suppose the above model is weight-tied, \ie  $f_\theta^{[i]} = f_\theta, \forall i$, and $\lim_{i \rightarrow \infty} f_\theta \left( z^{[i]}; x \right)$ exists and its value is $z^\star$. Further, assume that for this $z^\star$, it holds that $f_\theta\left(z^\star; x \right) = z^\star$.  Then, equilibrium models can be interpreted as the infinite-depth limit of the above network such that
$f^\infty_\theta\left(z^\star; x \right) = z^\star $


Under certain conditions\footnote{The fixed point can be reached if the  dynamical system is globally contractive. This is usually not true in practice for most choices of $f_\theta$, and divergence is possible.}, and for certain classes of $f_\theta$\footnote{\citet{bai2019deep} state that $f_\theta$ needs to be stable and constrained. In general, by Banach's fixed point theorem, global convergence is guaranteed if $f_\theta$ is contractive over its input domain.}, the output $z^\star$ of the above weight-tied network is a fixed point. 
A simple way to solve for this fixed point is to use fixed point iterations, \ie repeatedly apply the update $z^{[t+1]} = f_\theta(z^{[t]}; x)$ some fixed number of times, and backpropagate through the network to compute gradients. However, this can be computationally expensive.
Deep equilibrium (DEQ) models \citep{bai2019deep} 
explicitly solve 
for $z^\star$
through iterative root finding methods like Broyden's method~\citep{broyden1965class}, Newton's method, Anderson acceleration~\citep{anderson1965iterative}. DEQs use implicit function theorem to directly differentiate through the fixed point $z^\star$ at equilibrium, thus requiring constant memory to backpropagate through an infinite-depth network:
\begin{equation}
    \dfrac{\partial z^\star}{\partial \theta} =  \left( I - \dfrac{\partial f_\theta(z^\star; x)}{\partial z^\star}\right)^{-1} \dfrac{\partial f_\theta (z^\star; x)}{\partial \theta} \label{eq:implcit-grad-deq}
\end{equation}
Computing the inverse of Jacobian can quickly become intractable as we deal with high-dimensional feature maps. One can replace the inverse-Jacobian term with an identity matrix \ie Jacobian-free \citep{fung2022jfb} or an approximate inverse-Jacobian \citep{geng2021training} without affecting the final performance. There are alternate formulations of DEQs~\citep{winston2020monotone} that guarantee existence of a unique equilibrium point. However, designing $f_\theta$ for these formulations can be challenging, and in this work we use the formulation by \citet{bai2019deep}.



