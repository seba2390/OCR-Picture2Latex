\section{Introduction}
Partial differential equations (PDEs) 
are used to model a wide range of processes in science and engineering.
They define a relationship of (unknown) function and its partial derivatives. %
Most PDEs do not admit a closed form solution, and are solved using
a variety of classical numerical methods
such as finite element~\citep{leveque2007finite},
finite volume~\citep{moukalled2016finite}, and spectral methods~\citep{kopriva2009implementing, boyd2001chebyshev}. These methods are often very computationally expensive, both as the ambient dimension grows, and as the desired accuracy increases.  




This has motivated a rapidly growing area of research in data-driven approaches to PDE solving. One promising approach involves  learning \emph{neural solution operators}~\citep{chen1995universal, lu2019deeponet, bhattacharya2021model, li2020neural}, which take in the coefficients of a PDE in some family and output its solution---and are trained by examples of coefficient-solution pairs.  

While several architectures for this task have been proposed, the design space---in particular taking into account structural properties of the PDEs the operator is trained on---is still largely unexplored. Most present architectures are based on ``neuralizing'' a classical numerical method. For instance, \cite{li2020fourier} take inspiration from spectral methods, and introduce FNO: a trained composition of (parametrized) kernels in Fourier space. \cite{brandstetter2022message} instead consider 
finite-difference methods and generalize them into (learnable) graph neural networks using message-passing.  

Our work focuses on families of PDEs that describe the steady-state of a system (that is, there is no time variable). Namely, we consider equations of the form: %


\begin{equation}
    \label{eq:lu_f}
    L(a(x), u(x)) = f(x), \qquad \forall x \in \Omega,
\end{equation}
where $u:\Omega \to \R^{d_u}$, 
$a: \Omega \to \R^{d_a}$ 
and $f: \Omega \to \R^{d_f}$ are functions defined over the domain $\Omega$, 
and $L$ is a (possibly non-linear) operator. 
This family includes many natural PDE families like Poisson equations, electrostatic equations, and steady-state Navier-Stokes.

We take inspiration from classical numerical approaches of fast-converging Newton-like iterative schemes 
\citep{leveque2007finite, farago2002numerical}
to solve steady-state PDEs, as well as recent theoretical works for elliptic (linear and non-linear PDEs)~\citep{marwah2021parametric, chen2021representation, marwah2022neural} to hypothesize that very deep, but heavily weight-tied architectures would provide a useful 
architectural design choice for steady-state PDEs.

In this paper, we show that for steady state equations
it is often more beneficial to weight-tie
an existing neural operator, 
as opposed to making the model deeper---thus increasing its size.
To this end, we introduce {\bf FNO-DEQ}, a new architecture for solving steady-state PDEs. FNO-DEQ 
is a deep equilibrium model (DEQ)
that utilizes weight-tied FNO layers along with 
implicit differentiation and root-solvers to approximate the solution of a steady-state PDE.
DEQs are a perfect match to the desiderata laid out above: they can be viewed alternately 
as directly parameterizing the fixed points 
of some iterative process; or
by explicitly expanding some iterative fixed point solver like 
Newton's or Broyden's method as an infinitely deep, weight-tied model.

Such an architecture has a distinct computational advantage: %
implicit layer models effectively
backpropagate through the infinite-depth network while using only constant memory (equivalent to a single layerâ€™s activations).
Empirically, we show that for steady-state PDEs, 
weight-tied and DEQ based models perform better than baselines with 4$\times$ 
the number of parameters, and are robust to training data noise. 
In summary, we make the following contributions:
\begin{itemize}%
    \item We show the benefits of weight-tying as an effective architectural choice for neural operators when applied to steady-state PDEs.
    \item We introduce FNO-DEQ, a FNO based deep equilibrium model (DEQ) that uses implicit layers and 
        root solving to approximate the solution of a steady-state PDE. We further attest to the empirical performance 
        of FNO-DEQ by showing that it performs as well as FNO and its variants with $4\times$ number of parameters.
    \item  We show that FNO-DEQ and weight tied architectures 
    are more robust to both input and observation noise, 
    thus showing that weight-tying is a useful inductive bias for 
    architectural design for steady-state PDEs.
    \item By leveraging the universal approximation results of FNO~\citep{kovachki2021universal} we show that 
        FNO-DEQ based architectures can universally approximate the solution operator for a wide variety of steady-state PDE families. 
    \item Finally, we create a dataset of pairs of steady-state incompressible Navier-Stokes equations with different forcing functions and viscosities, along with their solutions, which we will make public as a community benchmark for steady-state PDE solvers. 
\end{itemize}




    




