\section{Universal Approximation and Fast Convergence of FNO-DEQ}
\label{subsec:universal_approximation}

Though the primary contribution of our paper is empirical, we show (by fairly standard techniques) that FNO-DEQ is a universal approximator, under mild conditions on the steady-state PDEs. Moreover, we also show that in some cases, we can hope the fixed-point solver can converge rapidly.  

As noted in Definition~\ref{def:steady_state_PDE}, 
we have $\Omega:= \sT^d$.
We note that all continuous function 
$f \in L^2(\Omega; \R)$ and 
$\int_\Omega |f(x)| dx < \infty$
can be written as,
$f(x) = \sum_{\omega \in \Z^d} e^{i x^T \omega} \hat{f}_w$. 
where $\{\hat{f}_\omega\}_{\omega \in \Z^d}$
are the Fourier coefficients of the function $f$.
We define as $\lln$ as the space of functions such that for all $f_N \in \lln$ 
with Fourier coefficients that vanish outside a bounded ball.
Finally, we define an orthogonal projection operator $\project: \ll \to \lln$, 
such that 
for all $f \in \ll$ we have,
\begin{equation}
    \label{eq:projection_definition}
    f_n = \project(f) = \project\left(\sum_{\omega \in \Z^d} f_\omega e^{i x^T \omega}\right) = 
    \sum_{\|\omega\|_\infty \leq N} \hat{f}_\omega e^{i x^T \omega}.
\end{equation}
That is, the projection operator $\project$ takes an infinite dimensional function and projects it 
to a finite dimensional space. We prove the following universal approximation result:
\begin{theorem}
    \label{thm:main_theorem}
    Let $u^\star \in \lldu$ define the solution to a
    steady-state PDE
    in 
    Definition~\ref{def:steady_state_PDE},
    Then there exists an operator 
    $\gG: \lldu \times \lldf \to \lldu$
    such that,
    $u^\star = \gG(u^\star, f)$.
    Furthermore,
    for every $\epsilon > 0$ there exists an $N \in \N$
    such that
    for compact sets $K_u \subset \lldu$ and $K_f \subset \lldf$
    there exists a neural network 
    $G_\theta: \llndu \times \llndf \to \llndu$
    with parameters $\theta$,
    such that,
    $$\sup_{u \in K_u, f \in K_f} \|u^\star- G_\theta(\project u^\star, \project f)\|_{\ll} \leq \epsilon.$$
\end{theorem}
The proof for the above theorem is relatively straightforward and provided
in Appendix~\ref{sec:proof_of_universal_approximation}.
The proof uses the fact
that $u^\star$ 
is a fixed-point of the operator
$G(u, f) = u - (L(u) - f)$, allowing us to use the 
the results in~\citet{kovachki2021universal}
that show a continuous operator can be approximated
by a network as defined in \eqref{eq:fno_layer_def}.
Note that the choice of $G$ is by no means unique: one can ``universally approximate'' any operator $G(u, f) = u - A(L(u) - f)$, for a continuous operator $A$. 
Such a $G$ can be thought of as a form of ``preconditioned'' gradient descent, for a preconditioner $A$. For example, a Newton update has the form
$G(u,f) = u - L'(u)^{-1} \left(L(u) - f\right)$
,
where $L': L^2(\Omega; \R^{d_u}) \to L^2(\Omega; \R^{d_u})$
is the Frechet derivative of the operator $L$.  

The reason this is relevant is that the DEQ can choose to universally approximate a fixed-point equation for which the fixed-point solver it is trained with also converges rapidly. As an example, the following classical result shows that under Lax-Milgram-like conditions (a kind of strong convexity condition), Newton's method %
converges doubly exponentially fast:
\begin{lemma}[\cite{farago2002numerical}, Chapter 5]
    \label{lemma:fast_convergence}
   Consider the PDE defined Definition~\ref{def:steady_state_PDE},
   such that $d_u=d_v=d_f=1$.
   such that $L'(u)$ defines the Frechet derivative 
   of the operator $L$.
   If for all $u,v \in L^2(\Omega; \R)$ we have
   $\| L'(u) v\|_{\ll} \geq \lambda \|v\|_{\ll}$
   and 
   $\|L'(u) - L'(v)\|_{\ll} \leq \Lambda \|u - v\|_{\ll}$
   for $0 < \lambda \leq \Lambda <\infty $,
   then for the Newton update,
   $
       u_{t+1} \leftarrow u_t - L'(u_t)^{-1}\left(L(u_t) - f\right),
   $
   with $u_0 \in L^2(\Omega; \R)$, there exists an $\epsilon > 0$,
   such that  $\|u_T - u^\star\|_{\ll} \leq \epsilon$
   if 
    $
       T \geq \log 
       \left(
           \log \left(\frac{1}{\epsilon}\right) 
           /
           \log \left(\frac{2\lambda^2}{\Lambda\|L(u_0) - f\|_{\ll}}\right)
        \right).
    $
\end{lemma}

For completeness, we include the proof of the above lemma in the Appendix (Section ~\ref{sec:fast_convergence}).
We note that the conditions of the above lemma are satisfied for elliptic PDEs like Darcy Flow,
as well as many variational non-linear elliptic PDEs (e.g., those considered in ~\citet{marwah2022neural}). 
Hence, we can expect FNO-DEQs to quickly converge to the fixed point, since they employ quasi-Newton methods
like Broyden and Anderson methods~\citep{broyden1965class, anderson1965iterative}. %













