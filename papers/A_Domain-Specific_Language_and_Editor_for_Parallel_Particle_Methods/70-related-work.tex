\section{Related Work}
\label{sec:related_work}
%language workbenches and viewbased editing

%Language workbenches aim at providing integrated and holistic language development
%environments for DSLs. They achieve this by combining parsing, rewriting, and
%analyses approaches with automatically generated editing services for DSL-specific
%IDEs, e.g., syntax highlighting, code completion, refactoring and debugging support
%Hence, the effort for implementing and testing of DSLs is drastically reduced. 
An exhaustive
overview of language workbenches, their features and use is given
in~\cite{erdweg_workbenches_2013}. Here, we therefore only discuss a small selection
of well-known workbenches for textual languages. 

Spoofax~\cite{kats_spoofax_2010} is a language workbench that builds upon term
rewriting with {\it Stratego}~\cite{bravenboer_stratego_2008}, a high-level grammar
language as well as meta languages for name and type analysis. DSLs implemented in
Spoofax can be used via generated plugins for the Eclipse platform or from command
line. Other well-known workbenches for textual DSLs with similar capabilities on the
basis of the Eclipse Modeling Framework (EMF) are Xtext~\cite{eysoldt_xtext_2010} and
EMFText~\cite{heidenreich_derivation_2009}. These tools leverage the relation of
context-free grammars and the EMF (cf.~\cite{alanen_ebnf_mof_2003}) to make
grammarware available to the field of model-driven software engineering. Xtext
provides built-in languages for code generation and semantic functions. Furthermore,
an Xtext language for formal specifications of type systems has been
developed~\cite{bettini_implementing_2015}. EMFText, in contrast, follows a
\emph{convention-over-configuration} approach, which tries to provide most language
features out of the box. In case that this is not sufficient to realize all intended
behavior, model-based attribute grammars or the object-constraint
languages~\cite{buerger_reference_2011,heidenreich_model-based_2013} are available.

\revii{Since MPS implements a projectional editing approach~\cite{feiler_incremental_1981,voelter_mps_2013}, no parsers or grammars are involved, as nodes are added, deleted, and modified directly.} While projectional editing is more restrictive
than direct text manipulation, it is less prone to errors and serializes models as
data structures, i.e., when a model is saved and loaded again, the exact instance is
restored. 
%Furthermore, it allows for advanced rendering of tables and mathematical
%equations, which is an advantage in scientific computing. 
%Like the other language workbenches, MPS provides a collection of built-in languages
%to cover typical DSL development aspects, such as code generation and type analysis.
MPS has already proven its applicability in other domains. The \emph{mbeddr} project instantiates
MPS in the embedded domain providing a projectional C frontend and several extensions
and domain-specific analyses, such as state machines and model checking~\cite{Voelter2013}. 
Moreover, the authors of \cite{Benson2015} used MPS to create a language and
editor for automated statistic analyses of biological data (bio markers), which is 
designed for end-user programming and statistical visualization. 

%extensible compilers/languages
Besides language workbenches and greenfield DSL development, another possibility is
to hook into already existing extensible compiler infrastructures, e.g., relying on
their basic intermediate representations, default optimizations, and code generation
facilities. Well-known examples for these infrastructures are the LLVM
framework~\cite{lattner_llvm_2004} and Graal~\cite{duboscq_graal_2013}. 
%
LLVM is used as a compiler backend for various general-purpose languages, most notably C and C++. It is
centered around a universal intermediate language that is transformed through several
extensible phases, as well as various optimizations, allocations, and code selection,
down to platform-specific machine code. A DSL could rely on this infrastructure by
generating code in the LLVM intermediate language, reusing and extending compiler
facilities.
%
Graal is an extensible just-in-time compiler for the Java Virtual Machine and a
platform for testing new high-level optimizations. Further, it provides support for
integrating with new languages, language features, and domain-specific
optimizations~\cite{wimmer_graal_2015}.

% DSLs in scientific computing
During the last years, the importance of DSLs for scientific computing has been increasingly 
realized. This led to the emergence of a number of approaches of which
we mention a few notable examples. Blitz++~\cite{veldhuizen_blitz_2000} is a
template-based library and DSL for generating finite-difference operators (stencils)
from high-level mathematical specifications. Freefem++~\cite{hecht_freefem_2012} is a
software toolset and DSL for finite-element methods. This DSL allows
users to define analytic as well as finite-element functions using domain
abstractions such as meshes and differential operators.
Liszt~\cite{devito_liszt_2011} extends Scala with domain-specific statements
for defining solvers for partial differential equations on unstructured meshes with support for parallelism through MPI, pthreads, and CUDA. The
FEniCS project~\cite{logg_automated_2012} comprises a finite element library, the
unified form language (UFL)~\cite{alnaes_ufl_2014}, and several optimizing compilers
for generating code that can be used with the library. Building upon FEniCS, the
Firedrake project~\cite{rathgeber_firedrake_2015} adds composing abstractions such as
parallel loop operations.
%Similarly, in the future, we plan to develop a set of particle-method related optimizations 
%on top of PPME. For mesh-based discretizations, it would be an opportunity to integrate 
%the UFL, providing a unified framework for both abstractions.
%IFS: I would not mention this. I also think UFL might not be good because it is centered on bilinear forms and weak discretizations, which you only get in FEM

% DSL optimizations
%In many cases, it suffices to translate the DSL code into a lower-level target language like
%C and let its compiler apply standard optimization. 
%Otherwise, rules for optimizations may be programmed or realized using some of the approaches discussed
%previously.
The idea of transforming or rewriting program code for optimization purposes is not new. 
For example, a DSL optimizer could be implemented using program
transformations~\cite{schordan_user_optimizations_2003} or rewrite rules. However,
research on using graph-rewrite systems for such
tasks~\cite{amann_graph_2000,schoesser_graph_optimizations_2008} indicates that the
pattern language must be powerful enough to express context-sensitive patterns.
%
Furthermore, recent research shows that rewriting is a convenient technique for
implementing high-level optimizations on a restricted set of language constructs. For
instance, authors in~\cite{Panchekha2015} propose a method for
automatically improving the accuracy of floating-point expressions by rewriting such
expressions according to a set of harvested patterns. Further, the authors
of~\cite{steuwer_rewriting_2015} apply rewriting to specific functional expressions
for parallel computations to obtain efficient GPU kernels.
%
In the field of DSLs for scientific computing, domain-specific optimizations carry
great potential since scientific codes often induce specific boundaries on data
access and numeric algorithms. In~\cite{olgaard_fem_optimization_2010}, the authors
discuss different optimization strategies on representation code for element tensors
in the finite-element method. The representation code is written in
UFL, a high-level mathematical DSL for variational forms. The proposed strategies yield significant runtime speedups and leverage
domain knowledge to automate nontrivial optimizations that normally would have been
developed manually by scientific programmers. Related to that, the authors
of~\cite{luporini_algorithm_2016} discuss loop-level optimizations for finite-element
solvers in the COmpiler For Fast Expression Evaluation
(COFFEE)~\cite{luporini_coffee_2015}. Heuristics are used to predict local minima
of operation counts at runtime, using semantics-preserving transformations such as
code motion, expansion, and factorizations. The authors show that their
domain-specific optimizations are superior to those that are generally applied by
standard compilers such as Intel's ICC for optimizing the operation count in nested
loops. Similar optimizations could be provided as extensions of PPME.
%\jc{Here again a statement about us is needed. Something like, this could be integrated 
%in our super flexible environment ;-)}

% Units/dimensions
Also, the idea of adding dimensions or physical units to DSLs is not new.
\cite{Cook2006} presented an analysis technique checking correctness of 
units in programs without extending the base language, aiming for a minimal effort of 
annotations for a developer. Furthermore, \cite{Austin2006} proposed unit annotations 
for linear-algebra and finite-element calculations, which are similar to the dimension annotations
in PPME.
%
However, adding units to programming languages frequently has flaws. 
For instance, frameworks may use abstractions with boxing and unboxing of 
quantities and units, which implies a runtime overhead.
In our approach, the analysis is optional and does not have an impact on runtime 
performance, since it is only used at compile-time for consistency checking and does not persist in the simulation. 

% SK: this sentence is strange, units/dimension do not persist in the simulation, right?
%The core language is
%independent of the extension, meaning that a developer can chose whether to use
%dimensions in a simulation or not, and that existing programs can be annotated with
%unit information subsequently.
