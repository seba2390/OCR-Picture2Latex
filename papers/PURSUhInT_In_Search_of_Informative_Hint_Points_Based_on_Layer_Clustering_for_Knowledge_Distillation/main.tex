%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
%\documentclass[authoryear,preprint,12pt]{elsarticle}
\documentclass[final,3p,times]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{caption}
\usepackage{floatrow}
\floatsetup[table]{capposition=above}

\usepackage{subcaption}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Expert Systems with Applications}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

%\title{}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}


\title{PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation}
 \author{Reyhan Kevser Keser\corref{cor1}\fnref{label_be}}
 \ead{keserr@itu.edu.tr}
% \ead[url]{home page}
  \author{Aydin Ayanzadeh\fnref{label_umbc}}
  \ead{aydina1@umbc.edu}
  \author{Omid Abdollahi Aghdam\fnref{label_arc}}
  \ead{abdollahi15@itu.edu.tr}
  \author{Caglar Kilcioglu\fnref{label_arc}}
  \ead{caglar.kilcioglu@arcelik.com}
  \author{Behcet Ugur Toreyin\fnref{label_be}\corref{eq_sen_}}
  \ead{toreyin@itu.edu.tr}
  \author{Nazim Kemal Ure\fnref{label_ai}\corref{eq_sen_}}
  \ead{ure@itu.edu.tr}


 \cortext[cor1]{Corresponding author}
 \fntext[label_be]{Signal Processing for Computational Intelligence (SP4CING) Research Group, Informatics Institute, Istanbul Technical University, Turkey}
 \fntext[label_umbc]{Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, United States%, Baltimore, MD 21250, United States
 }
 \fntext[label_arc]{Arcelik Research and Development, Turkey}
\fntext[label_ai]{Artificial Intelligence and Data Science Application and Research Center, Istanbul Technical University, Turkey}
\cortext[eq_sen_]{Equal Senior Contribution}

\tnotetext[]{Full-text published in Expert Systems with Applications is available at \url{https://doi.org/10.1016/j.eswa.2022.119040} .}
\tnotetext[]{This work is licensed under a \href{http://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License}.}

\newpageafter{author}
\begin{abstract}
%% Text of abstract
One of the most efficient methods for model compression is hint distillation, where the student model is injected with information (hints) from several different layers of the teacher model. Although the selection of hint points can drastically alter the compression performance, conventional distillation approaches overlook this fact and use the same hint points as in the early studies. Therefore, we propose a clustering based hint selection methodology, where the layers of teacher model are clustered with respect to several metrics and the cluster centers are used as the hint points. Our method is applicable for any student network, once it is applied on a chosen teacher network. The proposed approach is validated in CIFAR-100 and ImageNet datasets, using various teacher-student pairs and numerous hint distillation methods. Our results show that hint points selected by our algorithm results in superior compression performance compared to state-of-the-art knowledge distillation algorithms on the same student models and datasets.



\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%Graphical abstract

%\begin{graphicalabstract}
%		\centering
%		\includegraphics[width = 1.0\linewidth]{all_steps2.pdf}

%\end{graphicalabstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%Research highlights-değişti bunlar:
%\begin{highlights}
%\item   We demonstrate that selection of hint points impacts the performance of knowledge distillation.
%\item	We propose a clustering based hint selection method.
%\item	Our method can be combined with any type of hint distillation that uses at least two hints.
%\item Our method is applicable for any student, once it is applied on a chosen teacher.
%%%Proposed method is applicable for any student model and valid for the whole training process, once it is applied on a chosen teacher model. 

%\end{highlights}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Model compression \sep Knowledge distillation \sep Hint selection \sep Feature matching

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{intro}
Recent years have witnessed a tremendous increase in utilizing Deep Neural Networks (DNNs) for many computer vision tasks, including face recognition~\citep{facerecognition}, object detection \citep{yolov3}, classification \citep{alexnet,resnet}, segmentation \citep{seg2012}, tracking \citep{tracking2020} and self-driving cars \citep{self_driving2018}. That being said, many applications require these models to be implemented on edge devices~\citep{sze2017efficient}, and the sheer size and computation requirements of large-scale deep learning models yield significant performance drops when they are implemented on an edge device. The main objective of this paper is to push the state-of-the-art in compressing large scale neural networks to increase their inference speed without sacrificing too much accuracy. We present a novel knowledge distillation approach that extends ideas from unsupervised clustering and hint distillation that obtains state-of-the-art performance in compression of various teacher models.

The development of deep learning based computer vision models were fueled with the advent of the ILSVRC challenge~\citep{ILSVRC15}. Starting with AlexNet architecture~\citep{alexnet}, model performance is improved at the expense of significant increase in model size and depth with the introduction of VGGNet~\citep{VGG} and ResNet~\citep{resnet} over the years. Hence, it is widely known that there is a trade-off between model size and generalization performance in these models. Parallel to these developments, there has also been considerable research in model compression~\citep{choudhary2020comprehensive}, 
where several different methodologies were offered for converting large models into smaller ones while conserving the performance as much as possible. Among these approaches, knowledge distillation \citep{hinton2015distilling} is a popular choice, since the compressed network architectures (referred to as the student network) can be set according to performance limitations of the hardware it is going to be implemented on, which is a critical requirement for deploying machine learning models on edge devices. In particular, hint distillation methods \citep{romero2015fitnets, Zagoruyko2017AT, huang2022feature, li2022distilling} have been popular in compressing large scale teacher networks to smaller scale student networks. The success of these methods stem from injecting hint points into the compression process, so that student model is forced to match, not only the input-output response, but also the outputs of intermediate layers of the teacher network. That being said, optimizing the model compression performance in hint based distillation is still an open research area. 

The main contribution of this paper is a novel hint based knowledge distillation methodology, that attempts to optimize the location of hint points based on the information provided by clustering of teacher network layers according to different metrics. Our contributions can be summarized as follows:
\begin{itemize}
  \item We demonstrate that selection of hint points impacts the performance of the student network and using heuristics or rule-of-thumbs for hint locations may hurt the generalization accuracy.
 
  \item We propose a novel method to search for informative hint positions, where the main idea is to cluster layers of the teacher network and use cluster centers as hint positions. This is the first method that proposes a hint position selection technique based on layer clustering for hint based knowledge distillation purposes.

  \item Our method can be combined with any type of hint distillation that uses at least two hints. We demonstrate this property by conducting experiments for various hint distillation methods and numerous architectures. 

  \item Proposed method is applicable for any student model and valid for the whole training process, once it is applied on a chosen teacher model. 

  \item We compare our approach with the current state-of-the-art algorithms on CIFAR-100 and ImageNet datasets. Results show that our method yields superior results for compression of various teacher models regardless of having the same or different architectures with student models, on both of these datasets.

\end{itemize}

The rest of the paper is organized as follows. In Section~\ref{sec:related}, we present a comprehensive survey of the literature related to the scope of our study. In Section~\ref{sec:method}, the proposed method is explained in detail and the elaborated solution is introduced to tackle the problem. Experiments and results are reported in Section~\ref{sec:experiments} to assess the performance of the proposed approach. We conclude by presenting a discussion and suggesting future research directions. 

\section{Related work}
\label{sec:related}

Literature on model compression, knowledge and hint distillation are reviewed in this section.  

\subsection{Model Compression}

Recently, there has been a significant progress in the field of computer vision by leveraging DNNs. However, many  state-of-the-art models have failed to meet the requirements for real-world applications due to model's computational complexity and memory demand. Various model compression approaches, targeted at compressing DNNs, have been proposed to mitigate this issue. In general, model compression studies can be categorized as follows: the parameter pruning and quantization, low-rank factorization, compact convolutional filters and knowledge distillation~\citep{cheng2017survey}. 

In the parameter pruning techniques~\citep{deepcompression2016iclr}, removing redundant weights based on thresholding is explored. On the other hand, quantization based techniques represent parameters with less number of bits. Although, network pruning reduces model size, it does not usually decrease the inference time, which is critical for real-world applications on the edge devices. As for low-rank factorization, it is not easy to implement and requires expensive computation because of the decomposition operation. 

Compact convolutional filters are proposed in~\citep{szegedy2017inception} and adapted by SqueezeNet ~\citep{iandola2016squeezenet} and MobileNet~\citep{sandler2018mobilenetv2}. Methods using compact convolutional filters are generally computationally efficient, however, significantly decreased number of parameters in them might degrade the performance of the models in relatively complex tasks~\citep{howard2019searching}. Thus, we mainly focus on knowledge distillation, in this work.

\subsection{Knowledge Distillation}

Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student). KD is mostly known due to Hinton's work~\citep{hinton2015distilling}, while it was first proposed by~\citep{bucilua2006model}. The most well known form of KD uses the combination of soft targets produced by the teacher model and labels as the target in its objective function. The methods that uses soft targets without hints, are known as output (logit) distillation~\citep{heo2019comprehensive}. 

Soft targets are calculated as in~(\ref{eq:kd}), by passing training batches through teacher model and using softmax output layer with a higher temperature value $(T>1)$:
\begin{equation}
\label{eq:kd}
    p_{i} = \frac{exp(z_i/T)}{\sum_{j}{exp(z_j/T)}}
\end{equation}
where $p$, $z$ and $T$ show the softened class probability, logit and temperature values, respectively. For instance,  $i,j \in \{0, ..., 99\}$ for a classification task with $100$ classes, such as CIFAR-100. The same temperature value is used for generating both softened logits in teacher and student models, and higher $T$ results in a softer class probability distribution. The loss function to train the student network is evaluated in~(\ref{eq:L_Student}) as follows:
\begin{equation}
\label{eq:L_Student}
    L = \lambda {L_{logit}}(p^S, p^T) + (1-\lambda)L_{cls}(p^S, y)
\end{equation}
where $\lambda$ is the trade-off between {logit} distillation loss {$(L_{logit})$} and classification loss $(L_{cls})$, $y$ represents the label, $p^S$ and $p^T$ are the softened logits of student and teacher networks, respectively. Temperature is set to 1 in $L_{cls}$. Cross-entropy and Kullback-Leibler losses are generaly used for $L_{cls}$ and {$L_{logit}$}, respectively.


A recent study \citep{guo2020spherical} improves KD, where it introduces Spherical Knowledge Distillation (SKD) that projects all logits of the teacher and the student on a sphere by normalization. In SKD, logits are scaled into a unit vector and multiplied with the average teacher norm in order to recover its norm to the original level. \citep{zhao2022decoupled} introduces a novel distillation approach by reformulating the classical logit distillation. Furthermore, \citep{ren2022co} proposes a logit distillation approach in order to train a transformer student using lightweight teacher models with different architectures.


Logit distillation is an open research area, where studies are focused on different aspects such as data-free distillation~\citep{kang2021data}, online distillation \citep{lan2018knowledge},
ensemble distillation \citep{nam2022improving}
or federated learning \citep{ni2022federated}. On the other hand, hint distillation aims to provide more information to student by utilizing knowledge of intermediate layers of the teacher model~\citep{romero2015fitnets,Zagoruyko2017AT,heo2019comprehensive, huang2022feature}, which is also called as feature matching and constitutes the foundation of our proposed method. It is more advantageous compared to the logit distillation, due to allowing elimination of capacity gap between teacher and student models and applicability on wide range of problems such as regression and low-level vision problems \citep{wang2021knowledge}.


\subsection{Hint Distillation}

In KD~\citep{hinton2015distilling} and SKD~\citep{guo2020spherical}, only logits of teacher network are used as the soft target to train student network. Recently, methods that leverage intermediate feature maps are proposed to improve logit (output) distillation's performance~{\citep{romero2015fitnets, ahn2019variational, ji2021show, li2022distilling, zhang2021student, wu2022single}}. 


In~\citep{romero2015fitnets} the output from middle layer of teacher network is chosen as a hint to guide the middle layer of the student network. This approach is called FitNet and involves a two-stage training strategy. 
%First, a pre-trained teacher network is used to train student network up to the guided layer, secondly, and then the classical KD approach is used for training the whole student network.
{In the first stage, the student network is trained up to the guided layer by the pre-trained teacher network. Then, logit distillation is used for the training of the whole student model, in the second stage.}
In~\citep{Zagoruyko2017AT} activation-based and gradient-based Attention Transfer (AT) is proposed to transfer knowledge from cumbersome teacher network to a smaller student network. In knowledge distillation using AT, the distance between $l_2$-normalized attention maps of teacher-student layer pairs are added to the standard cross entropy loss. In Variational Information Distillation (VID)~\citep{ahn2019variational}, the mutual information of the teacher-student layer pairs are maximized to transfer knowledge from teacher to student. In the classical KD~\citep{hinton2015distilling} softened class probabilities of teacher network are used in combination of cross entropy loss to train student network, and in FitNet~\citep{romero2015fitnets}, the middle layer of the teacher is also used to guide the middle layer of student during training. Furthermore, \citep{li2022distilling} applies hint distillation by combining self distillation and online distillation schemes. \citep{huang2022feature} proposes a hint distillation approach by addressing the issue of feature maps with different sizes that obtained from teacher and student models for the task of low resolution object recognition.
\citep{zhang2021self} introduces a self distillation scheme where the deepest layer transfers its knowledge to shallower layers in a model which is a multi-exit neural network.
In~\citep{heo2019comprehensive}, authors propose to transfer knowledge from before the activation function as in~\citep{heo2019knowledge}. Moreover, they propose employing a loss function and teacher transform, which are compatible with the feature position. Their study modifies the feature position, but keeps the conventional grouping approach. 

In IAKD \citep{fu2021interactive} which is a study that refers to the hint position, networks are divided into smaller parts compared to conventional approach. In that study, it is reported that using all break points for hint transfer results in worse performance than conventional grouping. In~\citep{ruffy2019state}, results pertained to experiments with various hint positions are mentioned to fail performing well. 

Recently, some studies focus on hint position problem, which is named as feature linking in some studies \citep{ji2021show}. For example, \citep{jang2019learning} propose a meta-learning scheme for transfer learning to determine hint positions as well as the weights for hints. However, this approach is costly and considers only last layers of blocks of a network as potential hint positions. \citep{ji2021show} presents an attention based approach which is called as AFD, for the same purpose. Nevertheless, the approach presented in \citep{ji2021show} repeats the search process for every iteration and is hard to implement for every potential hint positions for large networks such as resnet110. On the other hand, our approach is applicable for any student network and valid for the whole training process, once it is applied on a chosen teacher network. 

Furthermore, \citep{chen2021distilling} firstly proposed to transfer knowledge between different stages of teacher and student models. However, their approach relies on the conventional grouping of network layers. \citep{sun2019patient} suggests two schemes for hint positions on several problems of natural language processing, which are choosing every $k^{th}$ layers and choosing last $k$ layers as the hint positions. It should be noted that their approach employs a fixed rule for hint positions without considering the information that layers hold. \citep{deng2022distpro} employs a meta-learning scheme to choose hint positions and hint transformations to align them between models. Nevertheless, their search space is limited by the last layers of conventional groups for hint positions. On the other hand, our approach re-configures the grouping of layers considering the knowledge that layers contain, which allows all layers to be chosen as hint positions.
%in order to avoid transferring redundant information during distillation.
On a similar account,~\citep{haidar2021rail}  suggests utilizing a non-conventional, random-selection-based hint position strategy for distillation.~\citep{li2022knowledge} focuses on eliminating the redundant knowledge in the distillation process, akin to our approach. Their method makes a selection among the samples whose knowledge is being transferred. However, our approach operates on the hint positions at the teacher network. 
\citep{passban2021alp} utilizes attention based approach, where they focus on natural language models. Their method mainly fuses knowledge from different layers of the teacher for transfer. 


To sum up, conventional hint distillation studies in the literature, obtain intermediate feature maps to transfer teacher's knowledge by grouping layers of teacher network. This is accomplished by splitting layers according to the spatial sizes of the feature maps. Contrary to the strategy of following a pre-determined approach for hint position selection adopted by the state-of-the-art knowledge distillation methods, in this work, we propose a layer clustering method as a systematic alternative to choose more informative hint points.  


\section{Method}
\label{sec:method}

In this paper, we demonstrate hint point selection's effect on the performance of knowledge distillation and propose PURSUhInT, an informative hint position selection method based on layer clustering. To that end, we employ pre-trained teacher models and apply clustering on their sub-blocks. For this purpose, we first obtain the layer representations of each sub-block. Then we utilize k-means~\citep{k-means}~clustering algorithm on these representation matrices to obtain clusters of sub-blocks. This process yields clusters which consist of similar sub-blocks in terms of the predefined metric. Consequently, we choose one hint position per cluster to distill teacher network's knowledge in a comprehensively accurate fashion. Figure~\ref{fig:all_steps} demonstrates steps of our method. In this section, we elaborate on these steps.


	\begin{figure}[H]
		\centering
		\includegraphics[width = 1.0\linewidth]{all_steps2.pdf}
		\caption{The workflow of PURSUhInT consists of three steps: (a) 
		The acquisition of layer representations. For this purpose, feature maps for a certain number of samples ($N$) are obtained from all of the sub-blocks of pre-trained teacher and averaged in dimensions of height and width to form matrices with the size of $N \times C$. 
		(b) Clustering layer representations to obtain clusters of sub-blocks utilizing k-means. To remove redundancy, we choose one hint point for each cluster, specifically center positions of the clusters. 
		(c) Distillation with the determined hint positions in the previous step. In the diagrams, HP represents the index of hint point, H depicts the transformation which determines the hint type and CL shows the determined cluster. Best viewed in color.}
		\label{fig:all_steps}
	\end{figure}


\subsection{Layer Representation}
In order to cluster the sub-blocks of networks, sub-blocks should be represented in a proper form. In this study, we follow the commonly used setup for representing layers~\citep{kornblith2019similarity, raghu2017svcca,morcos2018insights}, which will be described in this section.

Let $H_i$ be the feature map of $i^{th}$ sub-block in a teacher network which consists of blocks such as ResNet. Its size is $N \times C \times H \times W$, where $N$ is the number of samples, $C$ is the number of channels, $H$ and $W$ are the height and width of the feature map, respectively. Since spatial dimensions share parameters, dimension of the channel contains more information. Hence, feature maps are averaged in dimensions of height and width to obtain representation matrices with the size of $N \times C$. 
To apply clustering, we store feature maps of the sub-blocks for $10^4$ samples from the training set. 

\subsection{Clustering of Layers}

The main approach adopted in the proposed knowledge distillation method is layer clustering. Clustering algorithms necessitate specification of a similarity metric defined among different clusters. Similarities between different layers of an artificial neural network are determined by methods exploiting various attributes of the network, such as, weight matrices~\citep{neill2020compressing}, activation maps~\citep{kornblith2019similarity, raghu2017svcca}, and the layers with the same order at different networks~\citep{li2015convergent}.~\citep{li2015convergent} proposes using correlation as a similarity metric to compare the knowledge learned by different networks with the same structure. They point out that using correlation metric for comparison yields similar results with mutual information. In~\citep{raghu2017svcca}, authors proposed using canonical correlation analysis (CCA) to measure the similarity between layers.  Moreover,~\citep{morcos2018insights} improved this metric by weighting. Then,~\citep{kornblith2019similarity} proposes centered kernel alignment (CKA) as a similarity index, whose linear case is associated with CCA.

In this paper, we use CKA and mean squared CCA ($R^2_{CCA}$) as the similarity metrics for layers in a neural network, where $R^2_{CCA}$ is a statistic that shows convergence of CCA~\citep{kornblith2019similarity}. Moreover, $R^2_{CCA}$ is acknowledged as Yanai’s GCD measure~\citep{ramsay1984matrix}.

Mean squared CCA can be calculated by~(\ref{eq:R2_CCA}) as follows:
\begin{equation}
\label{eq:R2_CCA}
    R^2_{CCA}(X,Y) = \frac{ \sum_{i=1}^{p_1} \rho^2_i}{p_1} = \frac{{\left \| Q_Y^T Q_X \right \|}_F^2}{p_1}
\end{equation}


\noindent where $\rho_i$ is the $i^{th}$ canonical correlation coefficient, $p_1$ shows the minimum of shapes of the components, $Q_Y$ and $Q_X$ are the orthonormal bases for the columns of $Y$ and $X$, respectively.

Furthermore, CKA is computed by~(\ref{eq:CKA}) as follows:
\begin{equation}
\label{eq:CKA}
    CKA(K,L) = \frac{ HSIC(K,L)}{  \sqrt{HSIC(K,K) HSIC(L,L)}}
\end{equation}
where HSIC is Hilbert-Schmidt independence criteria. The empirical estimator of HSIC is shown in~(\ref{eq:HSIC}) as:

\begin{equation}
\label{eq:HSIC}
    HSIC(K,L) = \frac{1}{(n-1)^2}tr(KHLH)
\end{equation}

\noindent where H is the centering matrix, tr represents trace of the matrix and n is the number of samples. $K_{ij} = k(x_i,x_j)$ and $L_{ij} = l(y_i,y_j)$ where k and l are two kernels. In this paper we use linear kernels unless it is explicitly stated that RBF kernel is used. 


In order to distill teacher's knowledge successfully, hints should carry concise information. By utilizing $R^2_{CCA}$ and $CKA$ to group layers of the teacher network, we obtain clusters of similar layers in terms of these metrics. We provide optimum hint points that contain non-redundant information by selecting one hint point per cluster. For this purpose, we utilized  $1 - R^2_{CCA}$ and $1 - CKA$ as the distance metrics for k-means algorithm where k corresponds to number of hints. 

K-means is a clustering method that aims to partition the n observations into k clusters. To find the best centroid for each cluster, k-means repeats assignment and update steps until the assignments are fixed, where the steps are summarized as follows:

1) Assign the data points based on the labels of the current nearest centroids:

\begin{equation}
\label{eq:y_i}
    y_i = \arg\min_j D(x_i, \mu_j),
\end{equation}
where  $\mu_j$ represents the centroid of $j^{th}$ cluster. $x_i$ and $y_i$ are the features and assigned cluster of the $i^{th}$ data point, respectively. D is the chosen distance metric, considering teacher model and the hint distillation type. 


2) Determine the centroids based on the current assignment of data points for each cluster:

\begin{equation} 
\label{eq:mu_j} 
    \mu_j=\frac{1}{|C_j|}\sum_{x_i \in C_j} x_i,  
\end{equation}
where $C_j$ shows the $j^{th}$ cluster.  \\ 

{
We choose k-means as our clustering algorithm since it's simple and effective. Although it has disadvantages for other applications, these disadvantages are not valid for our problem, since the number of clusters is predetermined as the number of hints and our data is not high-dimensional. Moreover, to mitigate the seeding problem, we choose three layers farthest from each other considering their order in the model, which are the first, center and last layers (points), as initial seeds.}

{
Table \ref{tab:exec_time} presents the execution times for the first two steps of our workflow, namely, acquisition of layer representations and clustering these representations, for the teacher models used in experiments on CIFAR-100. It should be highlighted that running the first two steps once is sufficient. We observe that clustering with the similarity metric of $R^2_{CCA}$ takes a shorter time than that of $CKA$.
}

\begin{table}[h]
\centering
\caption{{Execution times for Step 1 and Step 2 in our workflow, where results are obtained on CIFAR-100 dataset. Step 2 is achieved with k-means which uses the stated similarity metrics. It should be noted that the execution times for the clustering step are acquired by using only CPU, where this step can be accelerated by utilizing GPU.
\\}}
\label{tab:exec_time}

\begin{tabular}{c|c|c|c|c}

{Teacher} & {Number of } & {Similarity} & \multicolumn{2}{c}{Execution time (s)} \\ \cline{4-5}
model & layers & metric & Step 1 & Step 2 \\ \hline\hline

WRN-40-2 & 18 & $R^2_{CCA}$ & 272 & 180 \\ \hline
resnet32x4 & 15 & $R^2_{CCA}$ & 239 & 220 \\ \hline

\multirow{2}{*}{resnet110} & \multirow{2}{*}{54} & $R^2_{CCA}$ & \multirow{2}{*}{941} & 336 \\
&&$CKA$ & & 4797 \\
\hline
\end{tabular}
\end{table}


\subsection{Hint Distillation with Hint Point Search}

%Hint distillation methods are used for improving logit distillation. Loss function used in these studies can be defined as in~(\ref{eq:loss}):


Hint distillation methods are mostly used for improving logit distillation, where student model receives more guidance from teacher with the knowledge from intermediate layers, namely, hints. Hence, loss function consists of multiple terms that guide several layers of student model \citep{peng2019correlation, yang2022multi}. Therefore, total loss function, $L_{total}$, used for training the student model can be defined as follows:



%\begin{equation}
%\label{eq:loss}
%    Loss = \gamma L_{cls} + \alpha L_{logit} + \beta L_{hint}
%\end{equation}
{
\begin{equation}
\label{eq:loss}
    L_{total} = \gamma L_{cls}(p^S,y) + \alpha L_{logit}(p^S,p^T) + \beta L_{hint}(f^S,f^T)
\end{equation}
}where $L_{logit}$ and $L_{cls}$ represent the output (logit) distillation loss and classification loss, respectively. In~(\ref{eq:loss})
$y$, $p$ and $f$ denote labels, logits and hints obtained from models, respectively. Furthermore, $T$ and $S$ superscripts represent teacher and student models, respectively.
Moreover, $\gamma$, $\alpha$ and $\beta$ are weights for losses of classification, logit distillation and hint distillation, respectively. 
%We define hint distillation loss \textcolor{red}{$L_{hint}$} in~(\ref{eq:l_hint}) as the following:  

Hint distillation loss {$L_{hint}$} can be formulated as in the following:
\begin{equation}
\label{eq:l_hint}
    L_{hint} = \sum_{i}^k L(F(S_i),F(T_i)),  
\end{equation}
where $k$ is the number of hints, $L$ is the predetermined loss function for hint transfer, $F$ represents the transformation which determines the hint type, $T_i$ and $S_i$ show features from the $i^{th}$ group of layers in teacher and student networks, respectively. 
It should be noted that, as opposed to most of the previous studies operating on the transformation $F$, our method alters and re-defines the grouping approach.


Studies done so far uses groups of layers with the same spatial size. However, in our experiments we observe that some of these conventional hint positions fall into the same cluster that are determined by specific metrics designed for layer similarity (See Figure \ref{fig:hint_pos}). Our experiments demonstrate that selection of hint positions plays an important role on distillation performance. Hence, we propose to determine groups of layers by clustering using proper metrics in order to obtain effective hint positions. Our method prevents using redundant information obtained from similar layers for distillation.

Moreover, we consider using the center points to represent the groups of layers instead of the last points as in conventional approach. Hence, we conducted experiments for these points with different metrics as presented in Table~\ref{tab:AT_all_metrics}. {The first points in clusters are not included in these experiments, since choosing the first positions/layers in clusters yields transferring knowledge from the first layers of networks for the first hint points. This would result in transferring of redundant knowledge since features on the first layers of the model are general and mostly unrelated to the objective function \citep{yosinski2014transferable}.}

Our experiments show that center points yield better results than the last points of clusters which we obtain. Furthermore, the results point out that changing hint points to center without the proposed grouping, would yield decrease of performance instead of improvement.

\begin{table}[h]
\centering
\caption{Top-1 accuracy (\%) results of distilled resnet20 for different hint positions in baseline grouping and clusters obtained with different metrics, on CIFAR-100. Attention Transfer is used  as the distillation type in these experiments. * stands for the normalization before clustering. Although $R^2_{CCA}$ yields different clusters for with and without normalization, the center points are the same for these cases.\\}
\label{tab:AT_all_metrics}

\begin{tabular}{l|c|c}

{Metric type} & {Position on cluster} & {Accuracy}     \\ \hline\hline
\multirow{2}{*}{- (baseline)} & last & 71.35 ± 0.43 \\ %same spatial size
& center & 71.29 ± 0.15 \\ \hline

\multirow{2}{*}{$R^2_{CCA}$*} & last & 71.22 ± 0.13 \\ 
 & center & 71.59 ± 0.32 \\ \hline
 
\multirow{2}{*}{$R^2_{CCA}$}  & last & 70.99 ± 0.16 \\ 
 & center & 71.59 ± 0.32\\ \hline
 
\multirow{2}{*}{$CKA_{linear}$ or $CKA_{rbf}$}& last & 71.20 ± 0.17 \\ 
 & center & 71.50 ± 0.14\\ \hline

\end{tabular}
\end{table}

\section{Experiments}
\label{sec:experiments} 

We conduct experiments on CIFAR-100 and ImageNet, which are commonly used datasets in knowledge distillation studies. Moreover, we utilize a wide variety of distillation methods as well as architectures for teacher and student models such as ResNet~\citep{resnet}, Wide ResNet~\citep{zagoruyko2016wide} and ShuffleNet~\citep{zhang2018shufflenet}. Besides, we evaluate our approach on both of pre-activation and post-activation hints.

As stated in Section \ref{sec:method}, our method consists of three steps. After obtaining layer representations of candidate hint positions, we cluster these representations using k-means algorithm. We choose $k=3$ for experiments on CIFAR-100 and $k=4$ for experiments on ImageNet, in order to be consistent with the conventional number of hints. After the clustering, we choose center points of the clusters as proposed hint positions. Table \ref{tab:HP} highlights the difference between conventional hint positions and proposed hint positions by our approach. Besides, performances of the teacher models used in our experiments are presented. It is observed that some of the conventional hint positions may take place in the same clusters obtained by our method as shown in Figure \ref{fig:hint_pos}. This yields transferring redundant and non-efficient information from teacher to student models in conventional approaches. The last step in our approach is distillation with the proposed hint positions, which is explained in detail in Section \ref{subs:exp_set}.


\begin{table}[h]
\centering
\caption{Comparison of baseline and proposed hint positions (HPs) for teacher models used in our experiments. Proposed HPs are obtained by our algorithm which utilizes k-means with the specified metrics as the distance functions. Moreover, the datasets used for the testing of the teacher models and the accuracies obtained by the teacher models are presented. AT presents Attention Transfer \citep{Zagoruyko2017AT} as the hint distillation method. 
\\}
\centerline{
\begin{tabular}{l|c|c|c|c}
Teacher Model & Dataset & Accuracy & Baseline HPs & Proposed HPs % & Metric 
\\ \hline

Pre-activation hints: & & & \\ \hline
resnet110 (for AT) & CIFAR-100 & 74.49 & 18, 36, 54 & 6,27,49 \\% & $R^{2}_{CCA}$ \\ %R2
resnet110 (for others) & CIFAR-100 & 74.49 & 18, 36, 54 & 8,29,49 \\% & $CKA$ \\ %CKA &(VID, FitNets) 
\hline
Post-activation hints: & & & \\ \hline
resnet110  & CIFAR-100 & 74.31 & 18, 36, 54    & 7, 29, 49 \\% & $CKA$      \\ %CKA_lin
resnet32x4 & CIFAR-100 & 79.42 & 5, 10, 15     & 3, 8, 13  \\% & $R^{2}_{CCA}$      \\ %R2
WRN-40-2   & CIFAR-100 & 75.61 & 6, 12, 18     & 1, 8, 16  \\% & $R^{2}_{CCA}$      \\ %R2
ResNet-34  & ImageNet & 73.31 & 3, 7, 13, 16   & 2, 6, 11, 15 \\% & $CKA$ \\  %CKA_lin
\hline

\end{tabular}}
\label{tab:HP}
\end{table}



\begin{figure}
\centering

\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[height=1.4in]{rn110_baseline.pdf}
    \caption{}
   \end{subfigure}

\hfill

  \begin{subfigure}{\linewidth}
  \centering
  \includegraphics[height=1.4in]{rn110_proposed.pdf}
  \caption{}
  \end{subfigure} 

\RawCaption{\caption{Representative structure of ResNet models with hint positions which are colored as black. The labels at the bottom right of the blocks show the spatial sizes of feature maps, where $I$ corresponds to the spatial size of the input image. 
(a) Common approach for grouping layers of ResNets which only considers spatial size of the feature maps. The last points of these groups are chosen as the hint points. (b) Our proposed approach which utilizes clustering with the aim of focusing on layer similarity. This figure depicts the clusters obtained by our method with ${CKA}$ metric on resnet110 sub-blocks, for CIFAR-100 dataset. We choose the center points of each cluster as hint positions for distillation of teacher networks. It is visualized using \citep{harisIqbal} and best viewed in color.}
\label{fig:hint_pos}}
\end{figure}




%sonuçlar:
% -imagenet
% -cifar
    % -preact (rn110->rn8/20/32)
    % -postact (SOTA + CRD tablosu)

\subsection{Experimental Settings}
\label{subs:exp_set}

In the experiments, the similarity metrics for clustering of layers are determined considering distillation performances. For the implementation of hint types, we follow the setup of \citep{tian2019contrastive}, except FitNets \citep{romero2015fitnets}. In this study, we use three hints for FitNets for fair comparison with other hint types. Moreover, we determine weights of loss components by Bayesian search~\citep{wandb}, for all methods. Additionally, we choose the query-key dimension for AFD method~\citep{ji2021show}.

For experiments on pre-activation hints, we use pre-ReLU hints for all methods and SGD as the optimizer. Moreover, we set the maximum iteration as 350 epochs and batch size as 64. Besides, initial learning rate is 0.05 and it is decayed by 0.1 every 50 epochs after the first 200 epochs. For experiments on post-activation hints, we use post-ReLU hints for all methods and SGD as the optimizer. Furthermore, we set the maximum iteration as 240 epochs for fair comparison with other approaches and determine other hyperparameters by Bayesian search, that are temperature, learning rate and batch size.

For ImageNet experiments, we use Attention Transfer as the hint type with $p=1$ \citep{Zagoruyko2017AT}. We set the maximum iteration as 100 epochs, batch size as 256 and temperature as 4, by following \citep{tian2019contrastive}. We use SGD as the optimizer. Besides, initial learning rate is 0.1 and it is decayed by 0.1 at epochs of 30, 60, 80 and 90.

\subsection{Results on CIFAR-100}

To evaluate our approach on CIFAR-100 dataset, we conduct experiments on pre-activation and post-activation hints, which are the hints obtained before and after the last ReLU function in a sub-block, respectively. 

\subsubsection{Pre-activation hints}

For experiments on pre-activation hints, we choose three hint distillation methods to compare our findings, which are FitNets \citep{romero2015fitnets}, Attention Transfer (AT) \citep{Zagoruyko2017AT} and Variational Information Distillation (VID) \citep{ahn2019variational}. We choose two logit distillation methods to use along hint distillation methods, which are Hinton's Knowledge Distillation (KD) \citep{hinton2015distilling} and Spherical Knowledge Distillation (SKD) \citep{guo2020spherical}. Thus, we set up 6 experiments for each teacher-student pair. To assess our performance, we compare our proposed hint positions with the baseline hint positions, which are the last layers before the downsampling. We use CIFAR-style ResNets as the teacher and student models for distillation. Moreover, we utilize $R^2_{CCA}$ metric for AT (Attention Transfer) type of distillation and $CKA$ metric for two other types of hint distillation, in these experiments. Baseline and proposed hint positions are presented in Table~\ref{tab:HP}.

Since our method provides particular hint positions for the teacher network, proposed hint positions are valid for any student model with the same number of hints. Hence, using the same hint positions we conduct experiments for three student models, which are resnet8, resnet20 and resnet32. {Figure \ref{fig:RN8-20-32}} presents the results of knowledge distillation from resnet110 to three student models, for our proposed hint points and the baseline hint points. 


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.65\textwidth}
         \centering
         \includegraphics[width=\textwidth]{RN8_3.png}
         \caption{}
         \label{fig:RN8}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.65\textwidth}
         \centering
         \includegraphics[width=\textwidth]{RN20_3.png}
         \caption{}
         \label{fig:RN20}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.65\textwidth}
         \centering
         \includegraphics[width=\textwidth]{RN32_3.png}
         \caption{}
         \label{fig:RN32}
     \end{subfigure}
        \caption{{
        Top-1 accuracy (\%) results of (a) resnet8, (b) resnet20 and (c) resnet32 student models for 6 distillation schemes. Teacher model's accuracy is 74.490. Each score is obtained over 5 runs. (Best viewed in color.) %Values in bold show the best accuracies for each distillation scheme.
        \\}}
        \label{fig:RN8-20-32}
\end{figure}



%\begin{table}[h]
%
%\caption{Top-1 accuracy (\%) results of ResNet-8, ResNet-20 and ResNet-32 student models for 6 distillation schemes. Teacher model's accuracy is 74.490. Each score is obtained over 5 runs. Values in bold show the best accuracies for each distillation scheme.
%\\}
%\label{tab:RN8-20-32}
%
%%\resizebox{1.0\textwidth}{!}{%
%\begin{tabular}{ccccc}
%\multirow{2}{*}{Model} & {Hint} & \multirow{2}{*}{FitNets + KD  } & \multirow{2}{*}{AT + KD}      & \multirow{2}{*}{VID + KD}     \\ 
%&positions &&&\\\hline
%
%\multirow{2}{*}{RN8}& Baseline & 58.402 ± 0.203 & \textbf{61.746 ± 0.170}   &  {59.302 ± 0.209} \\
%& Proposed  & {\textbf{{59.290 ± 0.182}}} & {61.288 ± 0.220} & \textbf{59.462 ± 0.292} \\ \hline
%
%\multirow{2}{*}{RN20}&
%Baseline        & 71.284 ± 0.243           & 71.346 ± 0.433          & 71.192 ± 0.445 \\
%&Proposed         & \textbf{71.354 ± 0.182} & \textbf{71.594 ± 0.320} & \textbf{71.192 ± 0.345}          \\ \hline
%
%\multirow{2}{*}{RN32}& Baseline & 73.834 ± 0.205 & 73.672 ± 0.187   &  \textbf{73.496 ± 0.354} \\
%& Proposed  & {\textbf{73.850 ± 0.173}} & \textbf{73.934 ± 0.237} & 73.416 ± 0.175 \\ 
% 
%\hline \hline
% &   & {FitNets + SKD}  & {AT + SKD}     & {VID + SKD}    \\ \hline
%
%\multirow{2}{*}{RN8} & Baseline  & 61.502 ± 0.363     & \textbf{{62.936 ± 0.325}}  & {{62.216 ± 0.330}}   \\
%& Proposed   & {\textbf{{62.260 ± 0.166}}}  & {62.862 ± 0.200} & \textbf{62.384 ± 0.520} \\ \hline
%
%\multirow{2}{*}{RN20}& Baseline        & 71.352 ± 0.406      & 71.686 ± 0.215          & {71.388 ± 0.300}     \\
%&Proposed         & \textbf{71.472 ± 0.141}  & \textbf{71.982 ± 0.293} & \textbf{71.464 ± 0.309}    \\
%\hline
%
%\multirow{2}{*}{RN32} & Baseline  & 73.584 ± 0.226     & \textbf{73.700 ± 0.314}  & \textbf{{73.572 ± 0.219}}   \\
%&Proposed   & \textbf{{73.804 ± 0.332}}  & {73.690 ± 0.206} & 73.396 ± 0.456 \\ \hline
%
%
%\end{tabular}%
%%}
%\end{table}
%
%

Results show that our method outperforms the conventional approach for all hint types on resnet20. Moreover, it is demonstrated that our method yields the best result for distillation of resnet32. 
Besides, it outperforms the baseline except AT type of distillation of resnet8, while it achieves very close performance to the best result. In addition, our method significantly improves the performance of FitNets for distillation of resnet8. 

\subsubsection{Post-activation hints}

For experiments on post-activation hints, we utilize six teacher-student pairs. We apply our method on Attention Transfer (AT) along with the conventional KD, which is described as AT+KD, where we utilize baseline and proposed hint positions for AT hints. Moreover, we employ our approach on the state-of-the-art distillation approaches to boost their performance. For this purpose, we choose two recently proposed methods, namely, Weighted Soft Label (WSL) \citep{zhou2021rethinking} and Attention-based Feature Distillation (AFD) \citep{ji2021show}. WSL is a recent logit distillation method that applies sample-wise weighting on the loss function. To conduct experiments on hint position, we first improve this method by combining it with AT. 
To combine these methods, we simply utilize the loss function in (\ref{eq:loss}) where $L_{hint}$ indicates the loss for AT hints and other terms correspond to the loss of WSL method.
Then, we obtain the results of WSL+AT using baseline and proposed hint positions for transferring AT hints.
Another method that we employ for the experiments is AFD, which utilizes attention mechanism to search efficient hint positions between teacher and student models. It transfers knowledge between these determined positions as well as the logits and repeats the search process during training. This method assumes all intermediate output positions as potential hint positions for establishing connections between models. While it searches efficient points among the all intermediate output positions, we apply our approach on this method by constraining its potential hint positions of teacher as our proposed three hint positions. 

The results on state-of-the-art methods are presented in Table~\ref{tab:AFD_WSL}, while the performances of the teacher models used in the experiments are listed in Table~\ref{tab:HP}. We utilize $CKA$ metric for resnet110 and $R^2_{CCA}$ metric for resnet32x4 and WRN-40-2 teacher models, to obtain proposed hint positions. Results show that the proposed hint position method outperforms the baseline in most cases. Moreover, it yields an increase in accuracy up to 1.5 \% compared to the baseline hint positions. Besides, it consistently improves distillation performance for resnet32x4, resnet20 and resnet8x4 models where first one is a teacher and others are student models. Although we limit the search space of AFD by constraining it as three proposed hint positions, proposed method mostly yields better distillation performance. Specifically, proposed approach improves WSL+AT method except resnet32 student model and AFD method except one teacher-student pair which is WRN-40-2 - ShuffleNetV1. Furthermore, it yields 6.482 \% increase in accuracy for ShuffleNetV1 student, compared to the vanilla training. 


\begin{table}[]
\begin{center}

\caption{
Top-1 accuracy (\%) results of student models using state-of-the-art distillation methods on CIFAR-100 dataset. The results are compared between proposed and baseline hint positions for each distillation method. Values in bold show the best accuracies for each distillation scheme. Each score is obtained over 5 runs. Column marked by * show the reported results in \citep{shao2021multi}.
\\}

\begin{tabular}{c|c|c|c|c}
 &  &    & \multicolumn{2}{c}{AT + KD}               \\ \cline{4-5}
 Teacher &   Student &  Vanilla & Baseline  & Proposed    \\    \hline \hline    
\multirow{2}{*}{resnet110}  & resnet20   & 69.06    & 70.95 & \textbf{71.06}     \\
 & resnet32 & 71.14  & \textbf{73.59}     & 73.48              \\ \hline
\multirow{2}{*}{resnet32x4} & ShuffleNetV1 & 70.50   & 75.49 & \textbf{76.98}     \\
  & resnet8x4   & 72.50  & 75.06     & \textbf{76.07}     \\ \hline
\multirow{2}{*}{WRN-40-2}   & WRN-16-2   & 73.26       & \textbf{75.64}     & 75.45         \\
   & ShuffleNetV1  & 70.50    & 76.27              & \textbf{76.78}     \\ \hline \hline 
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{} & &\multicolumn{2}{c}{WSL + AT}    \\ \cline{4-5}
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{} & {WSL}    & Baseline & Proposed \\ \hline
\multirow{2}{*}{resnet110}        & resnet20             & {71.24} & 71.45              & \textbf{71.63}     \\
 & resnet32  & {73.49} & \textbf{73.87}     & 73.73 \\ \hline
\multirow{2}{*}{resnet32x4} & ShuffleNetV1         & {74.33} & 75.95              & \textbf{76.98}     \\
                            & resnet8x4            & {75.24} & 75.04              & \textbf{75.69}     \\ \hline
\multirow{2}{*}{WRN-40-2}   & WRN-16-2             & {75.56} & 75.49              & \textbf{75.60}     \\
                            & ShuffleNetV1            & {75.29} & 75.84              & \textbf{76.52}     \\ \hline \hline 
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{} &    & \multicolumn{2}{c}{AFD}    \\ \cline{4-5}
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{} && {Baseline*}                    & Proposed            \\ \hline
\multirow{2}{*}{resnet110}  & resnet20             & &{71.20}                       & \textbf{71.49}     \\
& resnet32          &   & {73.46}               & \textbf{74.05}     \\ \hline
\multirow{2}{*}{resnet32x4} & ShuffleNetV1         & &{75.08}                       & \textbf{75.11}     \\
& resnet8x4   & &{74.72}        & \textbf{75.80}     \\ \hline
\multirow{2}{*}{WRN-40-2}   & WRN-16-2             & &{75.41}                       & \textbf{75.60}     \\
& ShuffleNetV1  & &{\textbf{75.63}}              & 75.11             \\ \hline
\end{tabular}
\label{tab:AFD_WSL}
\end{center}
\end{table}

For further evaluation, we compare our method with various distillation approaches as~\citep{tian2019contrastive}. For this purpose, we obtain results of our method by integrating it with four distillation approaches, using the same pre-trained teachers as other approaches. Table \ref{tab:CRD} shows the results obtained on the last epochs for fair comparison with \citep{tian2019contrastive}.

Results demonstrate that our approach yields superior performance compared to the various approaches, except one teacher-student pair. It can be observed that the best performances are obtained mostly by AT+KD and CRD+AT+KD approaches with the proposed hint positions. Furthermore, adding CRD transfer to AT+KD yields decrease in performance for resnet32x4 teacher, while it boosts distillation performances of the two teacher-student pairs which are resnet110 - resnet20 and WRN-40-2 - WRN-16-2. Although AFD method with the proposed hint positions is not successful as others for ShuffleNetV1 student, it yields the best result for resnet110 - resnet32 pair. It should be highlighted that our approach on AT+KD yields 1.6 \% improvement over the second best method for teacher-student pair of resnet32x4 - ShuffleNetV1.

\begin{table}[]
\setlength{\tabcolsep}{3.5pt}
\caption{
Top-1 accuracy (\%) results of models using various distillation methods on CIFAR-100 dataset. Each score is obtained over 5 runs. Values in bold show the best accuracies for each distillation scheme.
It should be noted that reported results are based on the last epoch for fair comparison with \citep{tian2019contrastive} where the results of other methods are quoted from. Moreover, some results for FSP~\citep{yim2017gift} method cannot be obtained since it is not applicable for student-teacher pairs with different architectures. 
\\}
\centerline{
\begin{tabular}{l|c|c|c|c|c|c}
Teacher      & {WRN-40-2} & {resnet110} & {resnet110} & {resnet32x4} & {resnet32x4} & {WRN-40-2} \\
Student    & WRN-16-2    & {resnet20}  & {resnet32}  & resnet8x4& ShuffleNetV1  & ShuffleNetV1 \\ \hline
\multirow{2}{*}{Vanilla}    & 75.61  & 74.31   & 74.31   & 79.42    & 79.42    & 75.61  \\
    & 73.26  & 69.06   & 71.14   & 72.50    & 70.50     & 70.50   \\ \hline
KD  \citep{hinton2015distilling}   & 74.92  & 70.67   & 73.08   & 73.33    & 74.07    & 74.83  \\
FitNet \citep{romero2015fitnets} & 73.58  & 68.99   & 71.06   & 73.50    & 73.59    & 73.73  \\
AT \citep{Zagoruyko2017AT}         & 74.08  & 70.22   & 72.31   & 73.44    & 71.73    & 73.32  \\
SP \citep{tung2019similarity}        & 73.83  & 70.04   & 72.69    & 72.94    & 73.48      & 74.52 \\
CC \citep{peng2019correlation}        & 73.56  & 69.48  & 71.48    & 72.97    & 71.14        & 71.38   \\
VID \citep{ahn2019variational}       & 74.11  & 70.16   & 72.61     & 73.09    & 73.38  & 73.61  \\
RKD \citep{park2019relational}       & 73.35  & 69.25   & 71.82    & 71.90    & 72.28    & 72.21  \\
PKT \citep{passalis2018learning}       & 74.54  & 70.25   & 72.61       & 73.64    & 74.10    & 73.89   \\
AB \citep{heo2019knowledge}        & 72.50    & 69.53   & 70.98   & 73.17    & 73.55    & 73.34  \\
FT \citep{kim2018paraphrasing}    & 73.25  & 70.22   & 72.37   & 72.86    & 71.75    & 72.03  \\
FSP  \citep{yim2017gift}  & 72.91  & 70.11   & 71.89   & 72.62    & n/a & n/a \\
NST  \citep{huang2017like}  & 73.68  & 69.53   & 71.96   & 73.30    & 74.12    & 74.89  \\
CRD \citep{tian2019contrastive}       & {{75.48}}   & {\textbf{71.46}}    & {73.48}    & {75.51}     & {75.11}     & {76.05}   \\

\hline
%last:
Ours (AT+KD) & 75.16 & 70.86 & 73.12 & \textbf{75.84}  & \textbf{76.74} & {76.55} \\
Ours (WSL+AT) & 75.39 & 71.43 & 73.44  & 75.49 & 76.70 & 76.36 \\
Ours (AFD) & 75.33 & 71.22 & \textbf{73.79} & 75.52 & 74.94  & 74.94 \\  
Ours (CRD+AT+KD) & {\textbf{75.82}} & {71.32} & {73.19} & {74.20} & {74.88} & {\textbf{76.66}} \\
% &&&&&&\\
\hline
%metin içine de çekilebilir bu atıflar
\end{tabular}}
\label{tab:CRD}
\end{table}




\subsection{Results on ImageNet}
To further assess the performance of our approach, we use ImageNet which is one of the large-scale datasets. For this purpose, Attention Transfer (AT) is utilized as the hint type to be transferred, and KD is used for logit distillation, where the experiment is described as AT+KD. For this experiment, ImageNet-style ResNet-34 and ResNet-18 architectures are used as teacher and student models as in \citep{tian2019contrastive}, respectively. We utilize $CKA$ metric for clustering on ResNet-34 teacher model in order to obtain the proposed hint positions {for AT hints}. Table \ref{tab:imagenet} shows the results compared with the other knowledge distillation methods, where the results of other methods are quoted from \citep{tian2019contrastive}. Moreover, determined hint positions by our method are listed in Table \ref{tab:HP}.




\begin{table}[t]
\caption{Top-1 and Top-5 error rates on ImageNet validation set. ResNet-34 and ResNet-18 architectures are used as teacher and student models, respectively. Results of other approaches are quoted from \citep{tian2019contrastive}, { where other approaches consist of AT \citep{Zagoruyko2017AT}, KD \citep{hinton2015distilling}, SP \citep{tung2019similarity}, CC \citep{peng2019correlation}, Online KD \citep{lan2018knowledge} and CRD \citep{tian2019contrastive}.} \\
}
\setlength{\tabcolsep}{3.5pt}

\centerline{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
 
 & \multirow{2}{*}{
 Teacher} & \multirow{2}{*}{Student} & \multirow{2}{*}{AT} & \multirow{2}{*}{KD} & \multirow{2}{*}{SP} & \multirow{2}{*}{CC} & Online  & \multirow{2}{*}{CRD} & \multirow{2}{*}{CRD+KD} & Ours \\
 &&&&&&& KD &&&(AT+KD) \\ \hline
 
Top-1 & 26.69 & 30.25 & 29.30 & 29.34 & 29.38 & 30.04 & 29.45 & 28.83 & {28.62} & \textbf{28.51} \\ \hline
Top-5 & 8.58 & 10.93 & 10.00 & 10.12 & 10.20 & 10.83 & 10.41 & 9.87 & \textbf{{9.51}} & 9.83 \\ \hline
%best
\end{tabular}}
\label{tab:imagenet}
\end{table}


Results show that our hint position method yields the best Top-1 accuracy among the compared distillation approaches on ImageNet which is a challenging dataset in computer vision field. More importantly, it can be seen that only changing hint positions for a typical hint type may yield outperforming results compared to the recent methods that use contrastive learning.


As shown in the results, our proposed method improves the hint distillation methods which play a significant role in knowledge distillation for model compression. Table~\ref{tab:compare_models} presents the gains of our method in terms of accuracy among inference time and model size. Results demonstrate that resnet 110 inference time can be improved by a factor of 2.99 with only a Top-1 accuracy loss of 0.26\%. Furthermore, resnet32x4 can be compressed into resnet8x4 with the memory gain of 83.4 \% and a speed-up in inference time of 2.34. Moreover, WRN40-2 can be compressed into ShuffleNetV1 with the memory gain of 57.9 \% and an increase in accuracy of 1.27\%. Besides, WRN40-2 can be compressed into WRN-16-2 with the memory gain of 68.8 \% and an increase in accuracy of 0.4\%.
Results on ImageNet show that ResNet-34 can be compressed into ResNet-18 with a speed-up in inference time of 1.55 and memory gain of 46.4 \%.

\begin{table}[h]
\centering
\caption{Number of parameters, inference time per sample and Top-1 accuracy(\%) results for teacher and student networks. Student networks are compared with teacher networks in the aspects of compression ratio and achieved speed-up, where student models are trained using the proposed hint positions.
It should be noted that accuracies of ResNet-34 and ResNet-18 models are obtained on ImageNet.
\\}
\label{tab:compare_models}

\centerline{
\begin{tabular}{l|c|c|c|c|c}

\textbf{\multirow{2}{*}{Networks}} & \textbf{\multirow{2}{*}{\#Parameters}} & {\textbf{Compression }} & \textbf{{Inference}}    &\textbf{\multirow{2}{*}{Speed-up}}  & \textbf{\multirow{2}{*}{Accuracy}}     \\ 
&&\textbf{ratio}&\textbf{time}&&\\
\hline
Teacher: resnet110 & 1.74 M & - & 24.66 ms & - & 74.31 \\ \hline
resnet20 & 278.32 K & 84.0 \% & 5.84 ms & x4.22 & 71.63 \\
resnet32 & 472.76 K & 72.8 \% & 8.25 ms & x2.99 & 74.05 \\ \hline\hline
Teacher: resnet32x4 & 7.43 M & - & 8.00 ms & - & 79.42 \\\hline
ShuffleNetV1 & 949.26 K & 87.2 \% & 13.37 ms & x0.60 & 76.98 \\
resnet8x4 & 1.23 M & 83.4 \% & 3.42 ms & x2.34 & 76.07 \\\hline\hline
Teacher: WRN-40-2 & 2.26 M & - & 10.07 ms & - & 75.61 \\\hline
WRN-16-2 & 703.28 K & 68.8 \% & 4.71 ms & x2.14 & 76.01 \\
ShuffleNetV1 & 949.26 K & 57.9 \% & 13.37 ms & x0.75 & 76.88  \\\hline\hline
Teacher: ResNet-34 & 21.80 M & - & 3.30 ms & - & 73.31 \\\hline
ResNet-18 & 11.69 M & 46.4 \% & 2.13 ms & x1.55 & 71.49\\
\hline
\end{tabular}%
}
\end{table}



\section{Conclusion}

In this paper, we address the grouping problem on teacher networks to determine the hint positions among the network's sub-blocks. For tackling this problem, we employed k-means algorithm with metrics designed for layer similarity in order to cluster these sub-blocks. Our approach is applicable for any hint distillation scenario which uses at least two hints, in offline distillation scheme. Furthermore, it is valid for any student model, once it is applied on a determined teacher model.

To validate our approach, we apply our method on state-of-the-art distillation methods with the comparison of conventional approach and the proposed approach. Moreover, we present a comprehensive comparison among various distillation approaches and methods that utilize the proposed hint positions. Experimental results suggest that our proposed approach outperforms the state-of-the-art algorithms for numerous architectures on CIFAR-100 and ImageNet datasets. Besides, the proposed method performs successfully in terms of model compression, where it may yield high compression ratio, speed-up in inference time and a gain in accuracy, at the same time. For our subsequent work, we are planning to evaluate our method on different tasks such as object detection on COCO dataset.

\section{Acknowledgements}
This work has been supported by Arcelik ITU R\&D Center, 
The Scientific and Technological Research Council of Turkey (TUBITAK) under the grant number 121E378 and ITU Scientific Research Projects Fund under the grant number MOA-2019-42321.

\bibliographystyle{model5-names}
%\bibliographystyle{ab_sorted_num}
%\bibliographystyle{elsarticle-num}
\bibliography{biblist}


\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
