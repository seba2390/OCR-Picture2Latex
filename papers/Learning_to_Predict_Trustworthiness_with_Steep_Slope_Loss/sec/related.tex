\section{Related Work}

\noindent\textbf{Loss Function}. Loss function is the key to search for optimal parameters and has been extensively studied in a range of learning tasks \cite{Corbiere_NIPS_2019,Lin_ICCV_2017,Cox_JRSS_1972,Liu_ICML_2016}.
Specifically, the cross entropy loss may be the most widely-used loss for classification \cite{Cox_JRSS_1972}, however, it is not optimal for all cases. 
Lin \etal \cite{Lin_ICCV_2017} propose the focal loss to down-weight the loss w.r.t. well-classified examples and focus on minimizing the loss w.r.t. misclassified examples by reshaping the cross entropy loss function.
Liu \etal \cite{Liu_ICML_2016} introduce the large margin softmax loss that is based on cosine similarity between the weight vectors and the feature vectors.
Nevertheless, it requires more hyperparameters and entangles with the linear function in the last layer of the network.
Similar to the idea of focal loss, the proposed steep slope loss is flexible to reshape the function graphs to improve class imbalance problem.
TCP's confidence loss is used to train the confidence network for predicting trustworthiness \cite{Corbiere_NIPS_2019}.
We adopt TCP as a baseline. %in this work. 

\noindent\textbf{Trustworthiness Prediction}.
Understanding trustworthiness of a classifier has been studied in the past decade \cite{Jiang_NIPS_2018,Corbiere_NIPS_2019,Hendrycks_ICLR_2017,Gal_ICML_2016,Moon_ICML_2020}.
Hendrycks and Gimpel \cite{Hendrycks_ICLR_2017} aim to detect the misclassified examples according to maximum class probability. 
This method depends on the ranking of confidence scores and is proved to be unreliable \cite{Jiang_NIPS_2018}.
Monte Carlo dropout (MCDropout) intends to understand trustworthiness through the lens of uncertainty estimation \cite{Gal_ICML_2016}.
However, it is difficult to distinguish the trustworthy prediction from the incorrect but overconfident predictions \cite{Corbiere_NIPS_2019}.
Jiang \etal \cite{Jiang_NIPS_2018} propose a confidence measure named trust score, which is based on the distance between the classifier and a modified nearest-neighbor classifier on the test examples. The shortcomings of the method are the poor scalability and computationally expensive.
Recently, instead of implementing a standalone trustworthiness predictor, Moon \etal \cite{Moon_ICML_2020} use the classifier's weights to compute the correctness ranking. The correctness ranking is taken into account in the loss function such that the classification accuracy is improved.
This method relies on pairs of training samples for computing the ranking. Moreover, it is unclear if the classifier improved by the correctness ranking outperforms the pre-trained classifier when applying the method on large-scale datasets, \eg ImageNet.
In contrast, the learning scheme to train the trustworthiness predictor (\ie confidence network) in \cite{Corbiere_NIPS_2019} is standard and does not affect the classifiers. Its efficacy has been verified on small-scale datasets.
In this work, we follows the learning scheme used in \cite{Corbiere_NIPS_2019} and focus on predicting trustworthiness on complex dataset.

% \noindent\textbf{Classifier}.
% Deep learning models achieve remarkable success in classification tasks \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021,Tolstikhin_arXiv_2021}. To comprehensively evaluate the proposed loss, we use two representative types of deep learning models, \ie ViT (transformer based network) \cite{Tolstikhin_arXiv_2021} and ResNet (convolutional network) \cite{He_CVPR_2016}, as the classifiers in this work.

% \noindent\textbf{Separability}. \cite{Cantu_Springer_2004,Skrypnyk_ICTAI_2011} 

% \noindent\textbf{Out-of-distribution generalization}.

\noindent\textbf{Imbalanced Classification}.
In classification task, the ratio of correct predictions to incorrect predictions is expected to be large due to the advance in deep learning methods.
This is aligned with the nature of imbalanced classification \cite{Huang_CVPR_2016,Wang_CVPR_2019,Cao_NIPS_2019,Cui_CVPR_2019,Kim_CVPR_2020}, which generally employ re-sampling and re-weighting strategies to solve the problem.
However, predicting trustworthiness is different from imbalanced classification as it is not aware of the visual concepts but the correctness of the predictions, whereas the classification task relies on the visual concepts.
Specifically, the re-sampling based methods \cite{Huang_CVPR_2016,Wang_CVPR_2019,Kim_CVPR_2020} may not be suitable for the problem of predicting trustworthiness.
It is difficult to determine if a sample is under-represented (over-represented) and should be over-sampled (under-sampled).
The re-weighting based methods \cite{Wang_CVPR_2019,Cui_CVPR_2019} can be applied to any generic loss functions, but they hinge on some hypothesis related to imbalanced data characteristics.
For instance, the class-balanced loss \cite{Cui_CVPR_2019} is based on the effective number w.r.t. a class, which presumes number of samples is known.
However, the number of samples w.r.t. a class is not always assessible, \eg in the online learning scheme \cite{Crammer_NIPS_2004,Bechavod_NIPS_2020}.
% the number of unique prototypes w.r.t. a class.
Instead of assuming some hypothesis, we follow the standard learning scheme for predicting trustworthiness \cite{Jiang_NIPS_2018,Corbiere_NIPS_2019}.