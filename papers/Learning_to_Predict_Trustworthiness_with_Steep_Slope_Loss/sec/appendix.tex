\appendix

\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

% \section{Appendix}

% Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
% This section will often be part of the supplemental material.
\section{Generalization Bound}
\label{sec:gb}
\begin{theorem*}%[Generalization bound]
	Denote $\text{maximum}\{\exp(\alpha^{+})-\exp(-\alpha^{+}), \exp(\alpha^{-})-\exp(-\alpha^{-})\}$ as $\ell_{SS}^{max}$. $\ell_{SS}\in [0, \ell_{SS}^{max}]$. Assume $\mathcal{F}$ is a finite hypothesis set, for any $\delta>0$, with probability at least $1-\delta$, the following inequality holds for all $f\in \mathcal{F}$:
	\begin{align*}
	|\mathcal{R}(f) - \hat{\mathcal{R}}_{D}(f) | \le  \ell^{max}_{SS}\sqrt{\frac{\log|\mathcal{F}|+\log\frac{2}{\delta}}{2|D|}}
	\end{align*}
% 	\label{thrm:gb}
\end{theorem*}
\begin{proof}[Proof]
	The proof sketch is similar to the generalization bound provided in \cite{Mohri_MIT_2018}.
% 	First, as $\ell_{SS}(y_{1},y_{2})=(\sum_{i}^{m}|y_{1i}-y_{2i}|^p)^{\frac{1}{p}}\le m^{\frac{1}{p}}$, we know $\ell^{p}$ is bounded by $m^{\frac{1}{p}}$. Then, 
	By the union bound, given an error $\xi$, we have
	\begin{align*}
	p[\sup_{f\in \mathcal{F}}|R(f)-\hat{R}(f)| > \xi] \le \sum_{f\in \mathcal{F}}^{} p[|R(f)-\hat{R}(f)|> \xi].
	\end{align*}
	By Hoeffding's bound, we have
	\begin{align*}
	\sum_{f\in \mathcal{F}}^{} p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)|> \xi] \le 2|\mathcal{F}|\exp \left(-\frac{2|D|\xi^2}{(\ell_{SS}^{max})^{2}} \right).
	\end{align*}
	Due to the probability definition, $2|\mathcal{F}|\exp (-\frac{2|D|\xi^2}{(\ell_{SS}^{max})^{2}}) = \delta$. Considering $\xi$ is a function of other variables, we can rearrange it as 
	$\xi=\ell_{SS}^{max}\sqrt{\frac{\log|\mathcal{F}|+\log\frac{2}{\delta}}{2|D|}}$.
%	\begin{align*}
%	\xi=m^{\frac{1}{p}}\sqrt{\frac{\log|H|+\log\frac{2}{\delta}}{2|D|}}.
%	\end{align*}
	Since we know $p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)| > \xi]$ is with probability at most $\delta$, it can be inferred that $p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)| <= \xi]$ is at least $1-\delta$.
\end{proof}

\section{Experimental Set-Up}
\label{sec:implementation}
The ViT (\ie ViT Base/16) used in this work is implemented in the ASYML project\footnote{\url{https://github.com/asyml/vision-transformer-pytorch}}, which is based on PyTorch.
The pre-trained weights are the same as the original pre-trained weights\footnote{\url{https://github.com/google-research/vision_transformer}}.
On the other hand, the pre-trained ResNet (\ie ResNet-50) is provided in PyTorch\footnote{\url{https://pytorch.org/vision/stable/models.html}}.
For the analyses, we use the official implementation\footnote{\url{https://github.com/valeoai/ConfidNet}} of the TCP confidence loss \cite{Corbiere_NIPS_2019}
and the PyTorch implementation\footnote{\url{https://github.com/vandit15/Class-balanced-loss-pytorch}} of the class-balanced loss \cite{Cui_CVPR_2019}.

We use the training scheme implemented by ASYML and tune the hyperparameters such that the oracles are trained with the cross entropy loss and focal loss to produce the best performance among multiple trials. Then, we fix the set of hyperparameters for the TCP confidence loss and the proposed loss.
Each combination of classifiers and oracles undergoes the same training scheme.
Specifically, the stochastic gradient descent (SGD) optimization method is used with initial learning rate 1e-5, weight decay 0, and momentum 0.05 for optimizing the learning problem.
The 1-cycle learning rate policy applies at each learning step.
The batch size is fixed to 40 as ViT would make the full use of four 12 GB GPUs with 40 images.
To stimulate a challenging and practically useful environment, we train the oracle in only one epoch, rather than multiple epochs.
All the three baseline loss functions and the proposed loss use the same hyperparameters and undergo the same experimental protocol for training the oracle.
The code is implemented in Python 3.8.5 with PyTorch 1.7.1 \cite{Paszke_NIPS_2019} and is tested under Ubuntu 18.04 with four NVIDIA GTX 1080 Ti graphics cards in a standalone machine.

We run the experiments three times with random seeds and report the means and the standard deviations of scores in \tabref{tbl:all_perf_w_std}. For the other experiments or analyses, we run one time.

\subsection{Implementation Details for Small-Scale Datasets}
\label{sec:mnist}
The resulting results are reported in \tabref{tbl:perf_mnist}. The experiment is based on the official implementation\footnote{\url{https://github.com/valeoai/ConfidNet}} of \cite{Corbiere_NIPS_2019}. The implementation provides the pre-trained models on MNIST and CIFAR-10.
% , but it does not provide the configurations for training the uncertainty networks. So we could not reproduce comparable performance reported in \cite{Corbiere_NIPS_2019} by training the uncertainty networks from scratch. Instead, 
We fine-tune the pre-trained models with the proposed steep slope loss. For comparison purposes, we also fine-tune the pre-trained with the TCP confidence loss (\ie \textit{TCP$\dagger$}), where the experimental settings of the fine-tuning process are the same as the ones of the fine-tuning process with the proposed steep slope loss. The proposed loss use $\alpha^{+}=10$ and $\alpha^{-}=6$ on MNIST, and $\alpha^{+}=1$ and $\alpha^{-}=1$ on CIFAR-10.

\section{License of Assets}
\label{sec:license}

MNIST \cite{Lecun_IEEE_1998} is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license, while ImageNet \cite{Deng_CVPR_2009} is licensed under the BSD 3-Clause ``New'' or ``Revised'' License.

PyTorch \cite{Paszke_NIPS_2019} is available under a BSD-style license.
The official ViT \cite{Dosovitskiy_ICLR_2021} implementation is licensed under the Apache-2.0 License, while the implementation of ViT is licensed under the Apache-2.0 License.
The code of TCP \cite{Corbiere_NIPS_2019} is licensed under the Apache License.
The PyTorch version of class balanced loss \cite{Cui_CVPR_2019} is licensed under the MIT License.

We make our code and pre-trained oracles publicly available via \url{https://github.com/luoyan407/predict_trustworthiness} with the MIT License.

% \input{depd/tbl_perf_avg_std}

\section{Experimental Result}
\label{sec:histogram}

% We report the performances in \tabref{tbl:all_perf_w_std} that corresponds to \tabref{tbl:all_perf} but includes the standard deviations of scores over three runs.
As \REVISION{shown in \tabref{tbl:all_perf_w_std} and} discussed in the experiment section, the proposed loss consistently improves the performance on metrics FPR-95\%-TPR, AUPR-Success, AUC, and TNR while the corresponding variances are comparable to the other loss functions.

We plot all the histograms in \figref{fig:histogram} and \figref{fig:distribution_unseen} that correspond to \tabref{tbl:all_perf_w_std} and \tabref{tbl:perf_vit_vit}, respectively.
Ideally, we hope that all the confidences w.r.t. the positive class are on the right-hand side of the positive threshold while the ones w.r.t. the negative class are on the left-hand side of the negative threshold.
From \figref{fig:histogram} and \figref{fig:distribution_unseen}, we can see that the proposed loss works in this direction, \ie the attempt pushing all the confidences w.r.t. the positive (negative) class to the right-hand (left-hand) side of the positive (negative) threshold.

\input{depd/fig_distributions_regular}
\input{depd/fig_distributions}


\REVISION{To comprehensively evaluate the proposed loss function, we conduct the experiments on various out-of-distribution (OOD) datasets, including ImageNet-C (corrupted ImageNet) \cite{Hendrycks_ICLR_2018}. Specifically, we evaluate the trustworthiness predictor trained on ImageNet on the sets of defocus blur, glass blur, motion blur, and zoom blur at the highest level of severity (\ie the most challenging setting). The results with setting \textlangle ViT, ViT\textrangle are reported in \tab \ref{tbl:perf_imagenetc}. The results are consistent with the ones on the stylized ImageNet and the adversarial ImageNet.}

\input{depd/tbl_perf_imagenetc}

\REVISION{
Note that trust score \cite{Jiang_NIPS_2018} may not be feasible to apply to real-world large-scale datasets like ImageNet. The trust score method needs to hold a tensor of size num\_sample $\times$ dim\_feature to initialize KD trees. The tensor would be small as the trust score method is evaluated on small-scale datasets, \eg 50000 $\times$ 512 on CIFAR. When evaluating on ImageNet, the size of the tensor would be 1.2 million $\times$ 768 (2048) using ViT (ResNet).
}

\section{Selective Risk Analysis}
\label{sec:risk}
\input{depd/fig_anal_risk}
Following \cite{Geifman_NIPS_2017,Corbiere_NIPS_2019}, we present the risk-coverage curves that are generated with different combinations in \figref{fig:anal_risk}. 
As can be seen in the figure, the proposed loss can work with different oracles or classifiers to significantly reduce the error.


% \input{depd/tbl_perf_tcp_mnist}

\section{Linear Function vs. Signed Distance}
\label{sec:appd_z}
Although the signed distance $z$, \ie $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$, leads to a geometric interpretation as shown in \figref{fig:workflow_a}, the main-stream models \cite{He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021} use $z=\bm{w}^{\top}\bm{x}^{out}+b$. 
Therefore, we provide the corresponding comparative results in \tabref{tbl:comp_linear}, which are generated by the proposed loss taking the output of the linear function as input.
In this analysis, the combination \textlangle ViT, ViT \textrangle is used and $\alpha^{+}=1,\alpha^{-}=3$.

As shown in \tabref{tbl:comp_linear}, both $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$ and $z=\bm{w}^{\top}\bm{x}^{out}+b$ yield similar performance on metrics FPR-95\%-TPR, AUPR-Error, AUPR-Success, and AUC.
On the other hand, TPR and TNR are moderately different between $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$ and $z=\bm{w}^{\top}\bm{x}^{out}+b$, when $\alpha^{+}$ and $\alpha^{-}$ are fixed.
This implies that TPR and TNR are sensitive to $\|\bm{w}\|$.

\input{depd/tbl_anal_against_linear}

\section{Separability between Distributions of Correct Predictions and Incorrect Predictions}
\label{sec:separability}
We assess the separability between the distributions of correct predictions and incorrect predictions from a probabilistic perspective.
There are two common tools to achieve the goal, \ie Kullback–Leibler (KL) divergence \cite{Kullback_AMS_1951} and Bhattacharyya distance \cite{Bhattacharyya_JSTOR_1946}. KL divergence is used to measure the difference between two distributions \cite{Cantu_Springer_2004,Luo_TNNLS_2020}, while Bhattacharyya distance is used to measure the similarity of two probability distributions. Given the distribution of correct predictions $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$ and the distribution of correct predictions $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$, we use the averaged KL divergence, \ie $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) = (d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) + d_{KL}(\mathcal{N}_{2}, \mathcal{N}_{1}))/2$, where $d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})=\log\frac{\sigma_{2}}{\sigma_{1}}+\frac{\sigma_{1}^{2}+(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}-\frac{1}{2}$ is not symmetrical. On the other hand, Bhattacharyya distance is defined as $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})=\frac{1}{4}\ln \left( \frac{1}{4} \left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{2}}+\frac{\sigma^{2}_{2}}{\sigma^{2}_{1}}+2 \right) \right) + \frac{1}{4} \left( \frac{(\mu_{1}-\mu_{2})^{2}}{\sigma^{2}_{1}+\sigma^{2}_{2}} \right)$. A larger $\bar{d}_{KL}$ or $d_{B}$ indicates that the two distributions are further away from each other.

\input{depd/tbl_separability}

The separabilities are reported in \tabref{tbl:separability}. We can see that the proposed loss leads to larger separability than the other three loss functions. This implies that the proposed loss is more effective to differentiate incorrect predictions from correct predictions, or vice versa.

\section{Connection to Class-Balanced Loss}
\label{sec:cbloss}

The class-balanced loss \cite{Cui_CVPR_2019} is actually to apply the re-weighting strategy to a conventional loss function, \eg the cross entropy loss and the focal loss.
To re-weight the losses, it presumes to know the number of sample w.r.t. each class before training, \ie $n^{+}$ (the number of correct predictions) and $n^{-}$ (the number of incorrect predictions).
Following \cite{Cui_CVPR_2019}, we use the hyperparameters $\beta=0.999$ and $\gamma=0.5$ for the class-balanced loss.
The class-balanced loss can be also applied to the proposed loss.
% Specifically, when using ViT as the classifier, $n^{+}=1,281,167*83.90\%=1074899$ and $n^{-}=1,281,167*(1-83.90\%)=206267$.
% When using ResNet as the classifier, $n^{+}=1,281,167*68.72\%=880417$ and $n^{-}=1,281,167*(1-68.72\%)=400749$.

We report the performances of the class-balanced cross entropy loss, the class-balanced focal loss, the proposed loss and its class-balanced variant in \tabref{tbl:anal_cb}.
The class-balanced cross entropy loss and focal loss achieve better performance on most of metrics than the cross entropy loss and focal loss.
Consistently, the proposed loss and its class-balanced variant outperform the class-balanced cross entropy loss and focal loss on metrics AUPR-Success, AUC, and TNR.
On the other hand, the class-balanced steep slope loss does not comprehensively outperform the proposed steep slope loss.
The improvement gain from the re-weighting strategy used in the imbalanced classification task is limited. This may result from the difference between the imbalanced classification and trustworthiness prediction. In other words, the imbalanced classification is aware of the visual concepts, whereas the trustworthiness is invariant to visual concepts.

\input{depd/tbl_class_balanced}

\input{depd/tbl_naive_balanced}

\REVISION{Up-weighting (down-weighting) the losses w.r.t. negative (positive) samples is a naive strategy in the imbalanced classification. It is interesting to see if this simple strategy is able to address the problem of predicting trustworthiness.
Let $w^{+}$ and $w^{-}$ be the weights for the cross entropy losses w.r.t. positive samples and negative samples, respectively.
The experimental results are in \tabref{tbl:naive_balanced}.
As we can see, up-weighting $w^{-}$ with various values does not achieve desired performance. In contrast, applying the class-balanced strategy for re-weighting yields much better results than the naive up-weighting strategy. Moreover, the proposed loss achieves better performance than the naive up-weighting strategy. 
}

\section{Analysis of Gradients}

\REVISION{It is interesting to know whether there are vanishing gradient issues or numerical stability issues due to the exponent in the proposed loss function. We compute the averaged $\|\frac{\partial \ell}{\partial x^{out}}\|$, where $x^{out} \in \mathbf{R}^{k}$ is the dimension of the output feature of the oracle backbone. For the case of using ViT as backbone, $k=768$. The results with setting \textlangle ViT, ViT\textrangle are reported in \tab \ref{tbl:numerical_stability}. 
Although the proposed loss function uses the exponential function, it only involves a small range (e.g., $[\exp(-\alpha^{+}), \exp(\alpha^{+})]$ or $[\exp(-\alpha^{-}), \exp(\alpha^{-})]$) in the exponential function. In this range, the gradients are less likely to change dramatically.
}

\begin{table}[!t]
	\centering
	\caption{\label{tbl:numerical_stability}
	    Numerical stability of gradients. \textlangle ViT, ViT\textrangle is used for the analysis.
	}
	\adjustbox{width=.5\columnwidth}{
	\begin{tabular}{L{7ex} C{8ex} C{8ex} C{8ex} C{8ex}}
		\toprule
		Magnitude & CE & Focal & TCP & SS  \\
		\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
		$\|\frac{\partial \ell}{\partial x^{out}}\|$ & 0.0068 & 0.0041 & 0.0039 & 0.0149 \\
		\bottomrule	
	\end{tabular}}
\end{table}

% \section{Effect of Normalization by $\|w\|$}
% \label{sec:effect_normalization}

% \REVISION{
% In this section, we discuss why the TNR results are sensitive to the normalization by $\|w\|$. The normalization by $\|w\|$ would make $z$ more dispersed in value than the variant without normalization. In other words, the normalization leads to long-tailed distributions while no normalization leads to short-tailed distributions. Given the same threshold, TNR (TPR) is determined by the location of the distribution of negative (positive) examples and the extent of short/long tails. By comparing the histogram (\figref{fig:with_normalization}) of the results with normalization to the one (\figref{fig:no_normalization}) of the results without normalization, the histograms generated without normalization are more spread than the ones generated with normalization, which verifies this point.
% }

% \begin{figure}[!t]
% 	\centering
% 	\subfloat[]{\includegraphics[width=0.45\textwidth]{fig/hist/ss_vit_vit_val.pdf} \label{fig:with_normalization}} \hfill
% 	\subfloat[]{\includegraphics[width=0.45\textwidth]{fig/hist/analysis_ss_vit_vit_val.pdf} \label{fig:no_normalization}} 
% 	\caption{
%     	Effect of normalization by $\|w\|$. (a) is with normalization while (b) is without normalization. The normalization by $\|w\|$ would make $z$ more dispersed in value than the variant without normalization.
%     	}
% \end{figure}

\section{Inference}

\REVISION{Once the training process for the trustworthiness predictor is done, the inference by the trustworthiness predictor is efficient. Specifically, the inference time used for classification is 1.64 milliseconds per image, while the inference time used for predicting trustworthiness is 1.53 milliseconds per image. Predicting trustworthiness is slightly faster than predicting labels. This is because the classification is a 1000-way prediction while predicting trustworthiness is a 1-way prediction (I.e., the output is a scalar). As the trustworthiness predictor and classifier are separate, it is possible to predict trustworthiness and labels in parallel to further improve efficiency in practice.}