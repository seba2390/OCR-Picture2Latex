\begin{abstract}

Understanding the trustworthiness of a prediction yielded by a classifier is critical for the safe and effective use of AI models.
Prior efforts have been proven to be reliable on small-scale datasets.
In this work, we study the problem of predicting trustworthiness on real-world large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples.
In such a setting, we observe that the trustworthiness predictors trained with prior-art loss functions, \ie the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy.
The reasons are two-fold.
Firstly, correct predictions are generally dominant over incorrect predictions.
Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-scale datasets.
To improve the generalizability of trustworthiness predictors, we propose a novel \textit{steep slope loss} to separate the features w.r.t. correct predictions from the ones w.r.t. incorrect predictions by two slide-like curves that oppose each other.
The proposed loss is evaluated with two representative deep learning models, \ie Vision Transformer and ResNet, as trustworthiness predictors.
We conduct comprehensive experiments and analyses on ImageNet, which show that the proposed loss effectively improves the generalizability of trustworthiness predictors.
The code and pre-trained trustworthiness predictors for reproducibility are available at \url{https://github.com/luoyan407/predict_trustworthiness}.

% \url{https://gitlab.com/anonymous_science/steep_slope.git}.

% where the features ride down two slides such that the features w.r.t. correct predictions are separated from the ones w.r.t. incorrect predictions.

% One major challenge in classification tasks is the generalizability of models, namely, the ability to adapt properly to unseen data that could be from unknown domains.
% Specifically, in spite of the availability of large-scale datasets, the complexity in the nature of data results in an intractable generalization from seen domains to unseen domains, especially in visual data for recognition.
% Instead of enhancing the models' generalizability, this work studies a new learning paradigm, which introduces an entity called oracle to learn to access the trustworthiness of classifiers' predictiveness.
% To this end, we propose a simple yet effective loss function that aims to improve the features' discriminativeness yielded by the oracle and find a local minimum of oracle such that it is able to work on unseen domains.
% We train the proposed oracle with ImageNet training samples and verify it on the ImageNet validation set, a stylized variant of the validation set, and an adversarial variant of the validation set, respectively.
% The experimental results show that the proposed oracle work better on unseen domains than the baseline.
% We show and analyze why the oracle learning problem is difficult.
% We find that the behaviour pattern appearing in a seen domain will be more or less consistent in unseen domains. For example, if an oracle is difficult to differentiate negative sample from positive samples in the seen domain, then it probably act similarly in unseen domains.

% Essentially, the oracle learning problem is a binary classification problem. Specifically, there are two key goals that are desired to achieve. Firstly, we aim to enhance the separability (aka discriminativeness) as much as possible. Secondly, we aim to enhance the generalizability of the proposed oracle such that it can work with the data from unseen domains as well as it could.

\end{abstract}