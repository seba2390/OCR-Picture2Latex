\section{Introduction}

Classification is a ubiquitous learning problem that categorizes objects according to input features.
It is widely used in a range of applications, such as robotics \cite{Leidner_IROS_2015}, environment exploration \cite{Cate_SEG_2017}, medical diagnosis \cite{Sanz_ASC_2014}, etc.
% The correctness of predictions yielded by classifiers determines the outcomes of decisions.
In spite of the successful development of deep learning methods in recent decades, high-performance classifiers would still have a chance to make mistakes due to the improvability of models and the complexity of real-world data \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021}.

To assess whether the prediction yielded by a classifier can be trusted or not, there are growing efforts towards learning to predict trustworthiness \cite{Jiang_NIPS_2018,Corbiere_NIPS_2019}.
These methods are evaluated on small-scale datasets, \eg MNIST \cite{Lecun_IEEE_1998}, where the data is relatively simple and existing classifiers have achieved high accuracy ($>99\%$).
As a result, there are a dominant proportion of correct predictions and the trustworthiness predictors are prone to classify incorrect predictions as trustworthy predictions.
The characteristics that the simple data is easy-to-classify aggravate the situation.
To further understand the prowess of predicting trustworthiness, we study this problem on the real-world large-scale datasets, \ie ImageNet \cite{Deng_CVPR_2009}.
This is a challenging theme for classification in terms of boundary complexity, class ambiguity, and feature dimensionality \cite{Basu_Springer_2006}.
As a result, failed predictions are inevitable. % with a certain probability.

% On the other hand, more and more pre-trained classifiers, \eg ResNet \cite{He_CVPR_2016}, ViT \cite{Dosovitskiy_ICLR_2021}, etc., are made publicly available, which results in considerable saving in training time and computational resource.
% In contrast to the prior studies of predicting trustworthiness \cite{Jiang_NIPS_2018,Corbiere_NIPS_2019}, where the classifiers are relatively light-weight, how well the complex yet high-performance classifiers perform on real-world datasets remain unclear.
% In this work, we study the problem of predicting trustworthiness with state-of-the-art classifiers.

% They are ready-to-use to classify images for saving considerable training time and computational resource.
% This is helpful to re-use pre-trained models for saving considerable training time and computational resource.
% In spite of the convenience of ready-to-use models, the pre-trained models are evaluated in a large-scale validation set and it is unclear if they really work in real-world scenarios with unseen samples that could from unseen domains.

% Thanks to the development of deep learning, state-of-the-art classifiers becomes more and more practically useful \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021}.
% Yet, how to comprehensively access the prediction correctness of the learned classifiers on unseen images is still challenging.
% This challenge correlates to data complexity for classification \cite{Basu_Springer_2006}, such as class ambiguity, boundary complexity, and feature space dimensionality.
% Moreover, although there are several works that attempt to improve domain generalization techniques \cite{Blanchard_NIPS_2011,Li_CVPR_2018,Chattopadhyay_ECCV_2020,Piratla_ICML_2020}, especially out-of-distribution generalization \cite{Rosenfeld_ICLR_2021} and single domain generalization \cite{Qiao_CVPR_2020}, they impose some specific hypothesis on their methods and there is no straightforward evidence to show that the methods work on large-scale high-dimensional data.

A general illustration of predicting trustworthiness \cite{Corbiere_NIPS_2019,Hendrycks_ICLR_2017} is shown in \figref{fig:teaser_1}.
The trustworthiness predictor \cite{Hendrycks_ICLR_2017} that is based on the maximum confidence have been proven to be unreliable \cite{Jiang_NIPS_2018,Provost_ICML_1998,Goodfellow_ICLR_2015,Nguyen_CVPR_2015}.
Instead, Corbiere \etal \cite{Corbiere_NIPS_2019} propose the true class probability (TCP) that uses the confidence w.r.t. the ground-truth class to determine whether to trust the classifier's prediction or not.
Nevertheless, the classification confidence is sensitive to the data.
As shown in \figref{fig:teaser_2}, TCP predicts that all the incorrect predictions (0.9\% in predictions) are trustworthy on MNIST \cite{Lecun_IEEE_1998} and predicts that all the incorrect predictions ($\sim$16\% in predictions) are trustworthy on ImageNet \cite{Deng_CVPR_2009}.
% A higher confidence score from the classifier does not necessarily imply higher probability that the classifier is correct, as shown in \cite{Provost_ICML_1998,Goodfellow_ICLR_2015,Nguyen_CVPR_2015,Jiang_NIPS_2018}.
% Instead of adopting the maximum confidence, TCP adopts the confidence w.r.t. the ground-truth class (\ie TCP) as the indicator for predicting trustworthiness.
% As a result, it names its trustworthiness predictor as the confidence network.
% In contrast, we use a straightforward indicator, that is, the prediction is trustworthy if the classifier yields the correct label, otherwise untrustworthy.
% Therefore, we use a generic term oracle for the trustworthiness predictor.

% On the other hand, we have witnessed a trend that more and more pre-trained models become publicly available. This is helpful to re-use pre-trained models for saving considerable training time and computational resource. In spite of the convenience of ready-to-use models, the pre-trained models are evaluated in a large-scale validation set and it is unclear if they really work in real-world scenarios with unseen samples that could from unseen domains. 

To comprehensively understand this problem, we follow the learning scheme used in \cite{Corbiere_NIPS_2019} and use two state-of-the-art backbones, \ie ViT \cite{Dosovitskiy_ICLR_2021} and ResNet \cite{He_CVPR_2016}, as the trustworthiness predictors.
For simplicity, we call the ``trustworthiness predictor" an \emph{oracle}.
% We term the entity that learns to predict trustworthiness of the observed classifier as the \textit{oracle}, which has a counterpart named the confidence network in \cite{Corbiere_NIPS_2019}.
% Similar to the learning framework used in \cite{Corbiere_NIPS_2019}, we use powerful deep learning models as the oracle and train them with the widely-used loss functions, \ie the cross entropy, focal loss \cite{Lin_ICCV_2017}, and TCP confidence loss \cite{Corbiere_NIPS_2019}, on the ImageNet training set \cite{Deng_CVPR_2009}.
We find that the oracles trained with cross entropy loss \cite{Murphy_Book_2012}, focal loss \cite{Lin_ICCV_2017}, and TCP confidence loss \cite{Corbiere_NIPS_2019} on ImageNet are prone to overfit the training samples, \ie the true positive rate (TPR) is close to 100\% while the true negative rate (TNR) is close to 0\%.
% We observe that the resulting oracles are prone to overfit the training samples, \ie the true positive rate (TPR) is close to 100\% while the true negative rate (TNR) is close to 0\%.
To improve the generalizability of oracles, we propose a novel loss function named the steep slope loss.
The proposed steep slope loss consists of two slide-like curves that cross with each other and face in the opposite direction to separate the features w.r.t. trustworthy and untrustworthy predictions.
It is tractable to control the slopes by indicating the heights of slides.
In this way, the proposed loss is able to be flexible and effective to push the features w.r.t. correct and incorrect predictions to the well-classified regions.
% prevent the learning process from overfitting the training samples.

% Instead of studying how to improve models' ability of domain generalization with some hypothesized correlation between seen and unseen domains, we study a practical problem: how to predict trustworthiness of a pre-trained classifier, with no prior knowledge about unknown domains. To this end, we introduce a concept of oracle, which is an entity to observe predictions of a classifier on training samples such that it can learn to predict the trustworthinesses of the classifier on unseen images. Specifically, given an image, the oracle would determine if the classifier will make a correct prediction, \ie the predicted label is trustworthy.

\input{depd/fig_teaser}

Predicting trustworthiness is similar to as well as different from conventional classification tasks.
On one hand, predicting trustworthiness can be formulated as a binary classification problem.
On the other hand, task-specific semantics are different between the classification task and predicting trustworthiness.
The classes are referred to visual concepts, such as dog, cat, etc., in the classification task, while the ones in predicting trustworthiness are abstract concepts. %, \ie being trustworthy or untrustworthy.
The trustworthiness could work on top of the classes in the classification task.
In other words, the classes in the classification task are specific and closed-form, while trustworthiness is open-form and is related to the classes in the classification task.
% Secondly, the problem of domain generalization in predicting trustworthiness is more difficult than that of conventional classification tasks. For instance, the oracle may run into images of cat on the fly, when it learns  with images of dog in the training phase. The dog images and the cat image should share much different visual appearance. In contrast, a real-world cat image is expected to shared similar visual appearance with a cartoon cat image.

% As the images' domains in the working environments are difficult to predict, it may not be viable to hypothesize common characteristics shared by seen and unseen domains. Instead, we are interesting in the generalizability of the model, \ie assume a model is learned with training samples, will it consistently work to predict trustworthiness on the images on the unseen domains? To study this problem, we apply a widely-used learning framework, \eg a deep learning model with the cross entropy loss, to learn to predict turstworthiness. However, we find that the resulting oracle overfits the training samples and leads to a poor generalization to the samples on unseen domains. 

% Instead, we propose a loss function, named steep slope, that is tractable and effective to learn to generate discriminative features for predicting trustworthiness. The core idea is that we exploit the graph characteristics of the exponential function and softsign function to establish slopes to push false positives and false negatives to respective well-classified regions, through the process of gradient descent.

% two fundamental problems: how to yield discriminative features for predicting trustworthy? How to measure the performance of an oracle? how the performance in a seen domain correlates to the performance in unseen domains?    

% To this end, we study the loss function in the learning process of the oracle.

The contribution of this work can be summarized as follows.
\begin{itemize}
    \item We study the problem of predicting trustworthiness with widely-used classifiers on ImageNet. Specifically, we observe that a major challenge of this learning task is that the cross entropy loss, focal loss, and TCP loss are prone to overfit the training samples, where correct predictions are dominant over incorrect predictions.
    \item We propose the steep slope loss function that improves the generalizability of trustworthiness predictors.
    We conduct comprehensive experiments and analyses, such as performance on both small-scale and large-scale datasets, analysis of distributions separability, comparison to the class-balanced loss, etc., which verify the efficacy of the proposed loss.
    % provided to prove the efficacy of the proposed loss.
    \item To further explore the practicality of the proposed loss, we train the oracle on the ImageNet training set and evaluate it on two variants of ImageNet validation set, \ie the stylized validation set and the adversarial validation set.
    The two variants' domains are quite different from the domain of the training set.
    We find that the learned oracle is able to consistently differentiate the trustworthy predictions from the untrustworthy predictions.
\end{itemize}

% Separability, challenge, and difference.

% Brief methodology and Summary of contribution

% The term \textit{oracle} is borrowed from prior robotic work \cite{Katz_ICRA_2020}.
% In this work, we instead study a fundamental problem: assume we have no prior knowledge of characteristics in unknown domains,   

% new problem's challenge

% The root causes are two-fold. Firstly, although there are a number of attempts to provide insights for domain generalization \cite{Blanchard_NIPS_2011,Li_CVPR_2018,Chattopadhyay_ECCV_2020,Piratla_ICML_2020,Qiao_CVPR_2020,Rosenfeld_ICLR_2021},
% due to the black-box characteristic of deep learning models \cite{Samek_Springer_2019}, it is still difficult to 

% As a result of the lack of ground-truth labels, we may not always be able to know the prediction correctness. Instead, we turn this problem into estimating  trustworthiness of the predictions.

% Learning to classify an object in an image is an important computer vision task that help us identify the diversity in a scene. One of major challenges in classification is how to comprehensively access the prediction correctness (\ie trustworthiness) of the learned classifier on images that could be from either the seen or the unseen domains. The reasons for this are two-fold. Firstly, due to the black-box characteristic of over-parameterized models, especially deep learning models, it is difficult to directly evaluate the quality Explainable Deep Learning: A Field Guide for the Uninitiated,  2. data complexity
% introduce the gap between literature and problem
% H

% High-stakes decision



% inspired by robotics oracle

% introduce the gap between literature and problem

% Why do we need oracle? 1. even more and more data out-of-distribution generalization. 2. more data generalizability

% brief methodology

% summary experiment results

% Classification

% \cite{Basu_Springer_2006}

% \begin{figure}[!t]
% 	\centering
% 	\subfloat{\includegraphics[width=0.5\textwidth]{fig/teaser}    } \hfill
% 	\subfloat{\includegraphics[width=0.40\textwidth]{fig/illu/feat_distribution}    }
% 	\caption{\label{fig:teaser}
%     	Illustrative example of learning to predict trustworthiness (left) and the feature distributions w.r.t. trustworthy predictions and untrustworthy predictions (right).
%     % 	\REVISION{\textit{Baseline} indicates ResNet GEM.}
%     	}
% \end{figure}Nevertheless, 

% A general illustration of predicting trustworthiness is shown in \figref{fig:teaser_1}.

