\section{Preliminaries}

In this section, we first recap how a deep learning model learns in the image classification task. Then, we show how the task of predicting trustworthiness connects to the classification task.

\noindent\textbf{Supervised Learning for Classification}. In classification tasks, given a training sample, \ie image $\bm{x} \in \mathop{\mathbb{R}}^{m}$ and corresponding ground-truth label $y \in \mathcal{Y}=\{1,\ldots,K\}$, we assume that samples are drawn i.i.d. from an underlying distribution.  The goal of the learning task is to learn to find a classifier $f^{(cls)}(\cdot; \theta')$ with training samples for classification. $\theta'$ is the set of parameters of the classifier. Let $f^{(cls)}_{\theta'}(\cdot) = f^{(cls)}(\cdot; \theta')$. The optimization problem is defined as
\begin{align}
	f^{*(cls)}_{\theta'} = \argmin_{f^{(cls)}_{\theta'}} \hat{\mathcal{R}}(f^{(cls)}_{\theta'}, \ell^{(cls)}, D_{tr}),
% 	\minimize_{f^{(cls)}_{\theta}} 
\label{eqn:risk}
\end{align}
where $f^{*(cls)}_{\theta'}$ is the learned classifier, $\hat{\mathcal{R}}$ is the empirical risk, $\ell^{(cls)}$ is a loss function for classification, and $D_{tr}$ is the set of training samples. 
% Without loss of generality, we denote the output feature as $z\in $ generated by  $x$ In particular, assumen 
% , \ie $(\bm{x}, y) \sim P^{(cls)}$
% $f^{(cls)}_{\theta}(\cdot) = f^{(cls)}(\cdot; \theta')$, $\theta$ are parameters of $f^{(cls)}$,
% \begin{align}
% 	f^{*(cls)}_{\theta'} \in \argmin_{f^{(cls)}_{\theta}}  \mathcal{R} (f^{(cls)}_{\theta}, \ell^{(cls)}, D_{tr}) \approx \argmin_{f^{(cls)}_{\theta}} \hat{\mathcal{R}}(f^{(cls)}_{\theta}, \ell^{(cls)}, D_{tr})
% % 	\minimize_{f^{(cls)}_{\theta}} 
% \label{eqn:risk}
% \end{align}

\noindent\textbf{Supervised Learning for Predicting Trustworthiness}. In contrast to the learning task for classification, which is usually a multi-class single-label classification task \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021}, learning to predict trustworthiness is a binary classification problem, where the two classes are positive (\ie trustworthy) or negative (\ie untrustworthy). Similar to \cite{Corbiere_NIPS_2019}, given a pair $(\bm{x},y)$ and a classifier $f^{(cls)}_{\theta'}$, we define the ground-truth label $o$ for predicting trustworthiness as
\begin{align}
o = 
\begin{dcases}
    1, & \text{if } \argmax f^{(cls)}_{\theta'}(\bm{x}) = y \\
    0, & \text{otherwise}
\end{dcases}
\label{eqn:def_trustworthy}
\end{align}
In other words, the classifier correctly predicts the image's label so the prediction is trustworthy in hindsight, otherwise the prediction is untrustworthy.

The learning task for predicting trustworthiness follows a similar learning framework in the classification task. Let $f_{\theta}(\cdot)$ be an oracle (\ie a trustworthiness predictor). A generic loss function $\ell: \mathop{\mathbb{R}}^{m}\times \mathop{\mathbb{R}} \rightarrow \mathop{\mathbb{R}}_{\ge 0}$, where $\mathop{\mathbb{R}}_{\ge 0}$ is a non-negative space and $m$ is the number of classes. Given training samples $(\bm{x},y)\in D_{tr}$, the optimization problem for predicting trustworthiness is defined as
\begin{align}
	f^{*}_{\theta} = \argmin_{f_{\theta}}  \frac{1}{|D_{tr}|}\sum_{i=1}^{|D_{tr}|} \ell(f_{\theta}(\bm{x}_{i}), o_{i}),
\label{eqn:pt_optimization}
\end{align}
where $|D_{tr}|$ is the cardinality of $D_{tr}$.

% Besides, it undergoes a similar learning framework to the classification task. Correspondingly, let $f_{\theta}$, $\ell$, and $P(\Omega_{seen})$ be the oracle, the loss function for predicting trustworthiness, and the distribution of $(\bm{I}, o)$ over the seen domain $\Omega_{seen}$. The goal of the learning task for predicting trustworthiness is to solve the following optimization problem
% \begin{align}
% 	\minimize_{f_{\theta}} \frac{1}{n}\sum_{i=1}^{n} \ell(f_{\theta}(\bm{I}_{i}), o_{i}) 
% \label{eqn:pt_risk}
% \end{align}
% where $(\bm{I}_{i}, o_{i})\sim P(\Omega_{seen})$.

Particularly, we consider two widely-used loss functions for classification and the loss function used for training trustworthiness predictors as baselines. They are the cross entropy loss \cite{Murphy_Book_2012}, focal loss \cite{Lin_ICCV_2017}, and TCP confidence loss \cite{Corbiere_NIPS_2019}.
Let $p(o=1|\theta,\bm{x})=1/(1+\exp(-z))$ be the trustworthiness confidence, where $z\in \mathop{\mathbb{R}}$ is the descriminative feature produced by the oracle, \ie $z=f_{\theta}(\bm{x})$.
The three loss functions can be written as
% In the binary classification, the sigmoid function is conventionally used to map a feature $x=f_{\theta}(\bm{I})$ to a probability $p(x)\in [0,1]$, \ie $p(x) = 1/(1+\exp(-x))$. Therefore, we have
\begin{align}
	\ell_{CE} (f_{\theta}(\bm{x}), o) &= -o\cdot \log p(o=1|\theta,\bm{x})-(1-o)\cdot \log ( 1-p(o=1|\theta,\bm{x}) ), \label{eqn:loss_ce} \\
	\begin{split}
    \ell_{Focal} (f_{\theta}(\bm{x}), o) &=  -o\cdot ( 1-p(o=1|\theta,\bm{x}) )^{\gamma} \log p(o=1|\theta,\bm{x}) - \\
    & \quad (1-o)\cdot (p(o=1|\theta,\bm{x}))^{\gamma} \log ( 1-p(o=1|\theta,\bm{x}) ), \label{eqn:loss_focal}
   \end{split} \\
	\ell_{TCP} (f_{\theta}(\bm{x}), y) &=  (f_{\theta}(\bm{x}) - p(\hat{y}=y|\theta',\bm{x}))^{2}. \label{eqn:loss_tcp}
\end{align}
In the focal loss, $\gamma$ is a hyperparameter. In the TCP confidence loss, $\hat{y}$ is the predicted label and $p(\hat{y}=y|\theta',\bm{x})$ is the classification probability w.r.t. the ground-truth class.

Consequently, the learned oracle would yield $z$ to generate the trustworthiness confidence. In the cases of $\ell_{CE}$ and $\ell_{Focal}$, the oracle considers a prediction is trustworthy if the corresponding trustworthiness confidence is greater than the positive threshold 0.5, \ie $p(o=1|\theta,\bm{x})>0.5$. The predictions whose trustworthiness confidences are equal to or lower than the negative threshold 0.5 are viewed to be untrustworthy. In the case of $\ell_{TCP}$, the positive threshold is also 0.5, but the negative threshold correlates to the number of classes in the classification task. It is defined as $1/K$ in \cite{Corbiere_NIPS_2019}.

% As an initial work to study the task of predicting trustworthiness, we are interesting in three questions. 1) Are conventional loss functions competent to produce discriminative features for predicting trustworthiness as they do in the classification task? 2) How to comprehensively evaluate the performance of oracles? Specifically, is the conventional metric accuracy sufficient to measure the predictions of oracles? 3) We assume that the classifier will not only come across the sample on the seen domain but also the samples on unseen domains in practical use. If an oracle learns to perform well on the samples on the seen domain, will it consistently perform well on samples on unseen domains? This is an average-case measure. Let $\mathcal{M}(\tau,f_{\theta},(\bm{I},o))$ be a performance metric, where $\tau$ is a decision boundary of $x$. Usually, $\tau=0$. $x \ge 0$ indicates that $p(x)\ge 0.5$ and $x$ is predicted as a positive. It is interesting to find out that if $f_{\theta}$ is trained with the samples on a seen domain, it will consistently work with unseen samples on an arbitrary domain. Mathematically, this can be represented as $\mathbb{E}_{(\bm{I},y)\sim P(\Omega_{all})} [\mathcal{M}(\tau, f_{\theta},(\bm{I},o))] \approx \frac{1}{|D_{te}|}\sum_{(\bm{I},o)\in D_{te}} \mathcal{M}(\tau, f_{\theta},(\bm{I},o))$, where $D_{te}$ is a set of samples for testing, $|D_{te}|$ is the number of testing samples, $\Omega_{all}$ are all domains, including the seen and unseen domains.


% , a pre-defined model $f(\cdot;\theta)$ is in place to learn to find weights for establishing the mapping between the input and output, \ie $f: \mathbf{R}^{m} \xrightarrow[]{\theta} \mathcal{Y}$. Given a pre-defined loss function $\ell$, the objective of learning is to minimize the empirical risk
% \begin{align}
% 	\minimize_{\theta} \sum_{(x,y)\in D} \ell(\sigma(f(x;\theta)), y)
% \label{eqn:empirical_risk}
% \end{align}
% where $D$ is a training set and $\sigma(\cdot)$ is an activation fuction. We follow the convention of classification to use the softmax function as the activation and denote $\ell(\sigma(f(x;\theta)), y)$ as $\ell_{\sigma}(f(x;\theta), y)$ for simplicity. According the objective (\ref{eqn:empirical_risk}), every pair $(x, y) \in D$ are considered to equivalently contribute to the learning process. The convention of machine learning hypothesizes that every pair equally contribute to the learning process, which simplifies the learning framework. This is practically useful and effective. In this work, we take on the underlying instance-wise effect in the learning process to understand how it affects the learning process. Mathematically, the revisited objective taking instance-wise effect into account can be formulated as
% \begin{align}
% 	\minimize_{\theta_{ie}} \sum_{(x,y)\in D} \alpha(x,y) \cdot \ell(\sigma(f(x;\theta_{ie})), y).
% \label{eqn:obj}
% \end{align}
% In particular, it is desirable to inspect the generalizability of the learned $f(\cdot;\theta_{ie})$, in comparison to the one of the learned $f(\cdot;\theta)$. Given a validation set $D_{val}$ and an evaluation metric $\mathcal{M}$, we check if the following inequality holds.
% \begin{align}
% 	\mathcal{M}(f,\theta, D_{val}) < \mathcal{M}(f,\theta_{ie}, D_{val})
% \label{eqn:generalizability}
% \end{align}new problem's challenge



% Discuss the baseline, \ie negative log-sigmoid and the separability and generalizability in the oracle learning problem.

% \begin{align}
%     \mathcal{L}_{b} = \frac{1}{D} \sum_{(x,y)\in D} -y\log(\frac{1}{1+e^{-x}}) - (1-y)\log(1-\frac{1}{1+e^{-x}})
% \end{align}

% \begin{figure}[!t]
% 	\centering
% 	\subfloat[]{\includegraphics[width=0.7\textwidth]{fig/illu/illu_flow}    } \hfill
% 	\subfloat[]{\includegraphics[width=0.27\textwidth]{fig/illu/illu_comp}    }
% 	\caption{\label{fig:workflow}
%     	Workflow.
%     % 	\REVISION{\textit{Baseline} indicates ResNet GEM.}
%     	}
% \end{figure}

\begin{figure}[!t]
	\centering
	\subfloat[\label{fig:workflow_a}]{\includegraphics[width=0.60\textwidth]{fig/illu/illu_workflow}} \hfill
	\subfloat[\label{fig:workflow_b}]{\includegraphics[width=0.38\textwidth]{fig/illu/illu_comp}}
	\caption{\label{fig:workflow}
    	Conceptual workflow of the proposed steep slope loss (a) and graph comparison between the proposed loss and the conventional losses (b). In (b), the cross entropy loss and focal loss are plotted in blue and black, respectively. The TCP confidence loss is a square error and varies with the classification confidence. Therefore, it is not plotted here.
    	}
\end{figure}

%------------------------------------------------
\section{Methodology}

In this section, we first introduce the overall learning framework for predicting trustworthiness. Then, we narrow down to the proposed steep slope loss function. At last, we provide the generalization bound that is related to the proposed steep slope loss function.
% whether an oracle will consistently perform over seen or unseen domains through the lens of the separability.

\subsection{Overall Design}

Corbi\`{e}re~\etal~\cite{Corbiere_NIPS_2019} provide a good learning scheme for predicting trustworthiness.
Briefly, it first trains a classifier with the training samples.
Then, the classifier is frozen and the confidence network (\ie trustworthiness predictor) is trained (or fine-tuned) with the training samples.
In general, we follows this learning scheme.

This work focuses on the trustworthiness on the predictions yielded by the  publicly available pre-trained classifiers, \ie ViT \cite{Dosovitskiy_ICLR_2021} and ResNet \cite{He_CVPR_2016}.
We use the pre-trained backbones as the backbones of the oracles for general purposes.
In this sense, the set of the oracle's parameters can be split into two parts, one is related to the backbone and the other one is related to the head, \ie $\theta = \{\theta_{backbone}, \theta_{head}\}$. $\theta_{backbone}$ is used to generated the intermediate feature $\bm{x}^{out}$ and $\theta_{head}=\{\bm{w},b\}$ are usually the weights of a linear function to generate the discriminative feature $z=\bm{w}^{\top}\bm{x}^{out}+b$. With the classifier, the oracle, a pre-defined loss, and the training samples, we can optimize problem (\ref{eqn:pt_optimization}) to find the optimal parameters for the oracle.

% Following the keep it simple and stupid principle, instead of , we reference the design of widely-used learning methods for classification \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021} to use a deep learning model as the oracle. The goal of the learning task is to minimize ERM (\eqref{eqn:pt_risk}).

% We find that conventional loss functions cross entropy and focal loss do not suffice to learn to yield discriminative features for predicting trustworthiness. The evidence will be presented in the following experiments and analyses. Therefore, we propose a steep slope loss function, which is easy to adapt to the nature of data by effectively maximizing the distance between the features w.r.t. positives and the features w.r.t. negatives.

% Regarding the architecture of the oracle, we adopt the conventional deep learning models for the backbone of oracles as this line of research is active and mature \cite{Krizhevsky_NIPS_2012,He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021}. Specifically, ViT \cite{Dosovitskiy_ICLR_2021} is used as the backbone of oracles in this work. The parameters of $\theta$ consists of two groups, one is related to the backbone and the other one is related to the head, \ie $\theta = \{\theta_{backbone}, \theta_{head}\}$. Specifically, the head of the oracle is an affine function $x = \bm{w}^{\top}\bm{x}^{in}+b$ in the case of using cross entropy or focal loss, where $\bm{w} \in \mathop{\mathbb{R}}^{k}$ are weights of the affine function, $b \in \mathop{\mathbb{R}}$ is the bias of the affine function, $\bm{x}^{in} \in \mathop{\mathbb{R}}^{k}$ is a $k$-dimensional feature vector and the input to the head of the oracle. The output feature $x$ is on $\mathop{\mathbb{R}}$, whereas the output feature is in high-dimensional feature space in the multi-class single-label classification task. Moreover, the oracle takes images that were observed by the classifier as input, instead of taking the features produced by the intermediate layers in the classifier as input. Adversarial examples could dramatically perturb the features generated by the classifier \cite{Goodfellow_ICLR_2014}. Assume that an oracle taking the classifier's features as input is trained with non-adversarial examples, both the oracle and the classifier would be perturbed by adversarial examples encountered in the testing phase. In contrast, if taking images as input, the oracle would work based on what it learns with training samples and it only correlates to the behaviours of the classifier in the training phase.

\subsection{Steep Slope Loss}
\label{sec:ss}
The conceptual workflow of the proposed steep slope loss is shown in \figref{fig:workflow_a}.
The core idea is that we exploit the graph characteristics of the exponential function and the softsign function to establish two slides such that the features $z$ w.r.t. the positive class ride down the positive slide to the right bottom and the features $z$ w.r.t. the negative class ride down the negative slide to the left bottom.
$z$ is defined as the signed distance to the hyperplane (\ie oracle head). Given an image $\bm{x}$, $z=f_{\theta}(\bm{x})$ can be broken down into
\begin{align}
    z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}, \quad \bm{x}^{out}=f_{\theta_{backbone}}(\bm{x}).
\end{align}
% The head of the oracle can be viewed as a hyperplane $(\bm{w},b)$.
The signed distance to the hyperplane has a geometric interpretation: its sign indicates in which half-space $\bm{x}^{out}$ is and its absolute value indicates how far $\bm{x}^{out}$ is away from the hyperplane.
% The signed distance is mathematically defined as 
% \begin{align}
%     x = \frac{\bm{w}^{\top} \bm{x}^{in}+b}{\|\bm{w}\|}.
% \end{align}

% The core idea is that we exploit the graph characteristics of the exponential function and the softsign function to establish slides such that the features $z$ w.r.t. the positive class ride down the positive slide to the right bottom and the features $z$ w.r.t. the negative class ride down the negative slide to the left bottom.
% push false positives and false negatives to respective well-classified regions, through the process of gradient descent.

It is desired that the signed distance of $\bm{x}^{out}$ with ground-truth label $o=1$ ($o=0$) tends towards $+\infty$ ($-\infty$) as much as possible.
% and the signed distance of $\bm{x}^{out}$ with ground-truth label $o=0$ tends towards $-\infty$ as much as possible. 
To this end, we define the steep slope (SS) loss function as follows
{\small
\begin{align}
    \ell_{SS} (f_{\theta}(\bm{x}), o) = o\cdot \underbrace{\left( \exp \left( \frac{\alpha^{+}z}{1+|z|} \right) - \exp(-\alpha^{+}) \right)}_{\text{Positive slide}}
    + (1-o)\cdot \underbrace{\left( \exp \left( \frac{-\alpha^{-}z}{1+|z|} \right) -  \exp(-\alpha^{-})\right)}_{\text{Negative slide}} \label{eqn:loss_ss}
\end{align}}
where $\alpha^{+},\alpha^{-}\in \mathop{\mathbb{R}}^{+}$ control the slope of the positive slide and the negative slide, respectively. If $z$ w.r.t. the positive class is on the left-hand side of $z=0$, minimizing the loss would push the point on the hill down to the bottom, \ie the long tail region indicating the well-classified region. Similarly, $z$ w.r.t. the negative class would undergo a similar process. $\exp(-\alpha^{+})$ and $\exp(-\alpha^{-})$ are vertical shifts for the positive and negative slides such that $\ell_{ss}$ has a minimum value 0. Note that the proposed steep slope loss is in the range $[0, \text{maximum}\{\exp(\alpha^{+})-\exp(-\alpha^{+}), \exp(\alpha^{-})-\exp(-\alpha^{-})\}]$, whereas the cross entropy loss and the focal loss are in the range $[0, +\infty)$.
The proposed steep slope loss can work with the output of the linear function as well. 
This is because the signed distance and the output of the linear function have a proportional relationship with each other, \ie $\frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|} \propto \bm{w}^{\top}\bm{x}^{out}+b$.
% , and $\beta^{+},\beta^{-}\in \mathop{\mathbb{R}}$ indicate the shifts of the positive-side and the negative-side slopes, respectively. Without loss of generalization, we assume $\beta^{+}=\beta^{-}=0$ for simplicity in this work. 

Essentially, as shown in \figref{fig:workflow_b}, the cross entropy loss, focal loss, and steep slope loss work in a similar manner to encourage $z$ w.r.t. the positive class to move the right-hand side of the positive threshold 0 and encourage $z$ w.r.t. the negative class to move the left-hand side of the negative threshold 0, which is analogous to the sliding motion. The proposed steep slope loss is more tractable to control the steepness of slopes than the cross entropy loss and focal loss. This leads to an effective learning to yield discriminative feature for predicting trustworthiness. 

% \subsection{Measure of Separability}
% We hypothesize that $x$ w.r.t. positive and negative samples both follow Gaussian distributions. The discriminativeness of features is an important characteristic that correlates to the performance, \eg accuracy. We are interested in measures of separability of feature distributions, which reflect the discriminativeness from a probabilistic perspective. There are two common techniques, \ie Kullbackâ€“Leibler (KL) divergence \cite{Kullback_AMS_1951} and Bhattacharyya distance \cite{Bhattacharyya_JSTOR_1946}. Usually, KL divergence is used to measure the difference between two distributions \cite{Cantu_Springer_2004,Luo_TNNLS_2020}, while Bhattacharyya distance is used to measure the similarity of two probability distributions. Given two Gaussian distributions $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$ and $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$, we use an averaged KL divergence as in this work, \ie $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) = (d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) + d_{KL}(\mathcal{N}_{2}, \mathcal{N}_{1}))/2$, where $d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})$ is the KL divergence between $\mathcal{N}_{1}$ and $\mathcal{N}_{2}$ (not symmetrical). On the other hand, Bhattacharyya distance is defined as $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})=\frac{1}{4}\ln \left( \frac{1}{4} \left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{2}}+\frac{\sigma^{2}_{2}}{\sigma^{2}_{1}}+2 \right) \right) + \frac{1}{4} \left( \frac{(\mu_{1}-\mu_{2})^{2}}{\sigma^{2}_{1}+\sigma^{2}_{2}} \right)$. In this work, we use Bhattacharyya coefficient that measures the amount of overlap between two distributions, instead of Bhattacharyya distance. Bhattacharyya coefficient is defined as $c_{B}(\mathcal{N}_{1}, \mathcal{N}_{2}) = \exp(-d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2}))$. $c_{B} \in [0,1]$, where 1 indicates a full overlap and 0 indicates no overlap.


\subsection{Generalization Bound}
With the proposed steep slope loss, we are interested in the generalization bound of trustworthiness predictors.
For simplicity, we simplify a trustworthiness predictor as $f\in \mathcal{F}$, where $\mathcal{F}$ is a finite hypothesis set.
The risk of predicting trustworthiness is defined as {\small $\mathcal{R}(f)=\mathbb{E}_{(\bm{x},y)\sim P} [\ell_{SS}(f(\bm{x}), o)]$}, where $P$ is the underlying joint distribution of $(\bm{x}, o)$. As $P$ is inaccessible, a common practice is to use empirical risk minimization (ERM) to approximate the risk \cite{Vapnik_TNN_1999}, \ie {\small $\hat{\mathcal{R}}_{D}(f)=\frac{1}{|D|}\sum_{i=1}^{|D|} \ell_{SS}(f(\bm{x}_{i}), o_{i})$}.
% where $(\bm{x}_{i},o_{i})\in D$.

% The following theorem provides an insight into whether an oracle yields a consistent separability over unseen sample from all domains, given a separability computed with seen samples on a seen domain.
The following theorem provides an insight into the correlation between the generalization bound and the loss function in the learning task for predicting trustworthiness.
\begin{theorem}%[Generalization bound]
	Denote $\text{maximum}\{\exp(\alpha^{+})-\exp(-\alpha^{+}), \exp(\alpha^{-})-\exp(-\alpha^{-})\}$ as $\ell_{SS}^{max}$. $\ell_{SS}\in [0, \ell_{SS}^{max}]$. Assume $\mathcal{F}$ is a finite hypothesis set, for any $\delta>0$, with probability at least $1-\delta$, the following inequality holds for all $f\in \mathcal{F}$:
	\begin{align*}
	|\mathcal{R}(f) - \hat{\mathcal{R}}_{D}(f) | \le  \ell^{max}_{SS}\sqrt{\frac{\log|\mathcal{F}|+\log\frac{2}{\delta}}{2|D|}}
	\end{align*}
	\label{thrm:gb}
\end{theorem}
The proof sketch is similar to the generalization bound provided in \cite{Mohri_MIT_2018} and the detailed proof is provided in the appendix \ref{sec:gb}.
% \begin{proof}[Proof]
% 	The proof sketch is similar to the generalization bound provided in \cite{Mohri_MIT_2018}.
% % 	First, as $\ell_{SS}(y_{1},y_{2})=(\sum_{i}^{m}|y_{1i}-y_{2i}|^p)^{\frac{1}{p}}\le m^{\frac{1}{p}}$, we know $\ell^{p}$ is bounded by $m^{\frac{1}{p}}$. Then, 
% 	By the union bound, given an error $\xi$, we have
% 	\begin{align*}
% 	p[\sup_{f\in \mathcal{F}}|R(f)-\hat{R}(f)| > \xi] \le \sum_{f\in \mathcal{F}}^{} p[|R(f)-\hat{R}(f)|> \xi].
% 	\end{align*}
% 	By Hoeffding's bound, we have
% 	\begin{align*}
% 	\sum_{f\in \mathcal{F}}^{} p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)|> \xi] \le 2|\mathcal{F}|\exp \left(-\frac{2|D|\xi^2}{(\ell_{SS}^{max})^{2}} \right).
% 	\end{align*}
% 	Due to the probability definition, $2|\mathcal{F}|\exp (-\frac{2|D|\xi^2}{(\ell_{SS}^{max})^{2}}) = \delta$. Considering $\xi$ is a function of other variables, we can rearrange it as 
% 	$\xi=\ell_{SS}^{max}\sqrt{\frac{\log|\mathcal{F}|+\log\frac{2}{\delta}}{2|D|}}$.
% %	\begin{align*}
% %	\xi=m^{\frac{1}{p}}\sqrt{\frac{\log|H|+\log\frac{2}{\delta}}{2|D|}}.
% %	\end{align*}
% 	Since we know $p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)| > \xi]$ is with probability at most $\delta$, it can be inferred that $p[|\mathcal{R}(f)-\hat{\mathcal{R}}(f)| <= \xi]$ is at least $1-\delta$.
% \end{proof}

A desired characteristic of the proposed steep slope loss is that it is in a certain range determined by $\alpha^{+}$ and $\alpha^{-}$, as discussed in Section \ref{sec:ss}. This leads to the generalization bound shown in Theorem \ref{thrm:gb}. The theorem implies that given a generic classifier, as the number of training samples increases, the empirical risk would be close to the true risk with a certain probability.
\REVISION{On the other hand, the cross entropy loss, focal loss, and TCP loss are not capped in a certain range. They do not fit under Hoeffding's inequality to derive the generalization bounds.}
% \begin{remark}
% A desired characteristic of the proposed steep slope loss is that it is in a certain range determined by $\alpha^{+}$ and $\alpha^{-}$, as discussed in Section \ref{sec:ss}. This leads to the generalization bound shown in Theorem \ref{thrm:gb}. The theorem implies that given a generic classifier, as the number of training samples increases, the empirical risk would be close to the true risk with a certain probability.
% \end{remark}

% Comparison with negative log-sigmoid in terms of better convergence and generalizability


% \begin{align}
%     \mathcal{L}_{BE} = \frac{1}{D} \sum_{(x,y)\in D} 
%     y\cdot \underbrace{\max(e^{-\alpha^{+}d}, e^{-\alpha^{+}\beta^{+}})-e^{-\alpha^{+}\beta^{+}}}_{\text{Positive-side}}  + 
%     (1-y)\cdot \underbrace{\max(e^{\alpha^{-}d}, e^{\alpha^{-}\beta^{-}})-e^{\alpha^{-}\beta^{-}}}_{\text{Negative-side}}
% \end{align}
% where $\beta^{+}$ (resp. $\beta^{-}$) is a positive (resp. negative) learning boundary.

% \subsection{Hyperplane Calibration}

% \begin{align}
%     \mathcal{L}_{Cal} &= \frac{1}{D} \sum_{(x,y)\in D} y\cdot \text{sign}(d) - (1-y)\cdot \text{sign}(d) \\
%     &\approx \frac{1}{D} \sum_{(x,y)\in D} y\cdot \text{softsign}(d) - (1-y)\cdot \text{softsign}(d)
% \end{align}
% where $\text{softsign}(d)=\frac{d}{1+|d|}$.

% \subsection{Between-Channel Feature Projection/Enhancement}

% Inspired by kernel methods, where xxx, we 

% \begin{align}
%     \tilde{\bm{x}} = \bm{x}^{\top}\bm{x} = 
%  \text{triu}\left( \begin{bmatrix}
%   \bm{x}_{1}^{2} & \cdots & \cdots & \cdots & \bm{x}_{1}\bm{x}_{n} \\
%   \vdots & \bm{x}_{2}^{2} & \cdots & \cdots & \bm{x}_{2}\bm{x}_{n} \\
%   \vdots & \ddots & \cdots & \cdots & \vdots \\
%   \vdots & & \ddots & \cdots & \vdots \\
%   \vdots & & & \ddots & \vdots \\
%   \bm{x}_{n}\bm{x}_{1} & \cdots & \cdots & \cdots & \bm{x}_{n}^{2}
%  \end{bmatrix} \right) = 
%  \begin{bmatrix}
%   \bm{x}_{1}^{2} \\
%   \vdots \\
%   \bm{x}_{1}\bm{x}_{n} \\
%   \bm{x}_{2}^{2} \\
%   \vdots \\
%   \bm{x}_{2}\bm{x}_{n} \\
%   \vdots \\
%   \bm{x}_{n}^{2}
%  \end{bmatrix}
% \end{align}

% \subsection{Cross-Model Generalization}

% \subsection{Theoretical Understanding}
% \begin{definition}
%     A multivariate parametric family $\mathcal{F}_{\Psi}$ of distributions $\{ p_{(\Psi,\theta)} | \theta \in \Theta = \text{int}(\Theta) = \text{dom}(\Psi) \subseteq \mathbf{R}^{d} \}$ is called a regular exponential family if each probability density is of the form
%     \begin{align}
%         p_{(\Psi,\theta)} = \exp(\langle x, \theta \rangle - \Psi(\theta))p_{0}(x),
%     \end{align}
%     where $x \in \mathbf{R}^{d}$ is a minimal sufficient statistic for the family.
% \end{definition}

% \begin{theorem}[Connection between spherical Gaussian distribution and $l_{2}$ loss]
% The well-known exponential family is that of spherical Gaussian distributions with densities of the form $p(x;a)=\frac{1}{\sqrt{(2\pi \sigma^{2})^{d}}} \exp\left(-\frac{1}{2\sigma^{2}} \|x-a\|^{2} \right)$. The corresponding Bregman divergence is 
%     \begin{align}
%         d_{\Psi}(x,\mu) = \frac{1}{2\sigma^{2}} \|x-\mu\|^{2}, \ \ \mu = a
%     \end{align}
% \end{theorem}

% \begin{theorem}[Gap between empirical and ground-truth Gaussians]
% placeholder
% \end{theorem}

% \subsection{Proposed Method}
% Given all images and the learned weights $\theta$, we have
% \begin{align}
%     z_{i} = f(x_{i}; \theta), \ \ Z = \{z_{i}|1 \le i \le N\}
% \end{align}

% Baseline
% \begin{align}
%     \hat{o}_{i}(z_{i}) = \frac{1}{1+\exp(-z_{i})}
% \end{align}

% Proposed
% \begin{align}
%     \hat{o}^{+}_{\theta}(z_{i}) = \frac{(z_{i}-\mu^{+})^2}{\alpha(z_{i}-\mu^{+})^2+\beta}, \ \ \hat{o}^{-}_{\theta}(z_{i}) = \frac{(z_{i}-\mu^{-})^2}{\alpha(z_{i}-\mu^{-})^2+\beta}
% \end{align}


% \begin{align}
%     \argmin_{\theta} \tau \hat{o}^{+}_{\theta}(z_{i}) + (1-\tau) \hat{o}^{-}_{\theta}(z_{i})
% \end{align}

% With all the features, we compute geometric median
% \begin{align}
%     c^{+} = \argmin_{c} \sum_{i} |z_{i}^{+} - c|, \ \ c^{-} = \argmin_{c} \sum_{i} |z_{i}^{-} - c|
% \end{align}

% Now compute median absolute deviation (MAD)
% \begin{align}
%     &\text{MAD}^{+} = \text{median}(|z_{i}^{+}-c^{+}|), \ \ \text{MAD}^{-} = \text{median}(|z_{i}^{-}-c^{-}|) \\
%     &\hat{\sigma}^{+} = k\cdot\text{MAD}^{+}, \ \ \hat{\sigma}^{-} = k\cdot\text{MAD}^{-}
% \end{align}

% Or we can learn to determine $\hat{\sigma}$
% \begin{align}
%     & \hat{\sigma}^{+} = \argmin_{\sigma} \sum_{i}\text{softsign}(\sigma-|z_{i}^{+}-c^{+}|) + \sigma^{2}, \\ 
%     &\hat{\sigma}^{-} = \argmin_{\sigma} \sum_{i}\text{softsign}(\sigma-|z_{i}^{-}-c^{-}|) + \sigma^{2}.
% \end{align}

% We cannot directly optimize the above equations as $\sigma$ could be optimized to be negative.
% \begin{align}
%     \sigma = \ln \left( \exp(\alpha) + 1 \right),  \ \ \sigma = \ln \left( \alpha^{2} + 1 \right)
% \end{align}

% In training, the oracle is learned to separate the features w.r.t. correct and incorrect predictions.
% \begin{align}
%     \mathcal{L} = \frac{1}{|D|}\sum_{i}\ o_{i} \exp(|z_{i}-c^{+}|) + (1-o_{i}) \exp(|z_{i}-c^{-}|)
% \end{align}


