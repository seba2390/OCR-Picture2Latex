\section{Experiment \& Analysis}
\label{sec:exp}
In this section, we first introduce the experimental set-up. Then, we report the performances of baselines and the proposed steep slope loss on ImageNet, followed by comprehensive analyses. 
% At last, we present comprehensive analyses to help better understand the efficacy of the proposed loss.

\noindent\textbf{Experimental Set-Up}.
We use ViT B/16 \cite{Dosovitskiy_ICLR_2021} and ResNet-50 \cite{He_CVPR_2016} as the classifiers, and the respective backbones are used as the oracles' backbones. We denote the combination of oracles and classifiers as \textlangle O, C\textrangle. There are four combinations in total, \ie \textlangle ViT, ViT\textrangle, \textlangle ViT, RSN\textrangle, \textlangle RSN, ViT\textrangle, and \textlangle RSN, RSN\textrangle, where RSN stands for ResNet.
In this work, we adopt three baselines, \ie the cross entropy loss \cite{Cox_JRSS_1972}, focal loss \cite{Lin_ICCV_2017}, and TCP confidence loss \cite{Corbiere_NIPS_2019}, for comparison purposes.

The experiment is conducted on ImageNet \cite{Deng_CVPR_2009}, which consists of 1.2 million labeled training images and 50000 labeled validation images. It has 1000 visual concepts. Similar to the learning scheme in \cite{Corbiere_NIPS_2019}, the oracle is trained with training samples and evaluated on the validation set. During the training process of the oracle, the classifier works in the evaluation mode so training the oracle would not affect the parameters of the classifier. Moreover, we conduct the analyses about how well the learned oracle generalizes to the images in the unseen domains. To this end, we apply the widely-used style transfer method \cite{Geirhos_ICLR_2019} and the functional adversarial attack method \cite{Laidlaw_NeurIPS_2019} to generate two variants of the validation set, \ie stylized validation set and adversarial validation set. \REVISION{Also, we adopt ImageNet-C \cite{Hendrycks_ICLR_2018} for evaluation, which is used for evaluating robustness to common corruptions.}
% Then, the oracle trained with regular training samples would be evaluated with the samples that are in the two unseen domains.

% To understand how the learned oracle work on unseen domains, the oracle is learned with training samples and is evaluated with three types of samples, the samples on the same domain as training samples and the samples on two unseen domains. We base our experiments on ImageNet \cite{Deng_CVPR_2009}, a widely-used large-scale dataset. Except for the training set and the validation set, we use the stylized ImageNet validation set \cite{Geirhos_ICLR_2019} and an ImageNet validation set that are perturbed by the functional adversarial attack technique \cite{Laidlaw_NeurIPS_2019}.
% (Introduce models here.)
% (Introduce hyperpaprameters here.)

The oracle's backbone is initialized by the pre-trained classifier's backbone and trained by fine-tuning using training samples and the trained classifier.
% As the oracle's backbone is initialized by the pre-trained classifier's backbone, the training process of the oracles is equivalent to the process of fine-tuning the initialized oracles.
Training the oracles with all the loss functions uses the same hyperparameters, such as learning rate, weight decay, momentum, batch size, etc.
The details for the training process and the implementation are provided in \appref{sec:implementation}.

For the focal loss, we follow \cite{Lin_ICCV_2017} to use $\gamma=2$,  which leads to the best performance for object detection.
For the proposed loss, we use $\alpha^{+}=1$ and $\alpha^{-}=3$ for the oracle that is based on ViT's backbone, while we use $\alpha^{+}=2$ and $\alpha^{-}=5$ for the oracle that is based on ResNet's backbone.

Following \cite{Corbiere_NIPS_2019}, we use FPR-95\%-TPR, AUPR-Error, AUPR-Success, and AUC as the metrics.
FPR-95\%-TPR is the false positive rate (FPR) when true positive rate (TPR) is equal to 95\%. 
AUPR is the area under the precision-recall curve. 
Specifically, AUPR-Success considers the correct prediction as the positive class, whereas AUPR-Error considers the incorrect prediction as the positive class.
AUC is the area under the receiver operating characteristic curve, which is the plot of TPR versus FPR.
Moreover, we use TPR and true negative rate (TNR) as additional metrics because they assess overfitting issue, \eg TPR=100\% and TNR=0\% imply that the trustworthiness predictor is prone to view all the incorrect predictions to be trustworthy. %due to overfitting.

% \noindent\textbf{Baselines \& Metrics}.
% We adopt widely-used loss functions, \ie cross entropy and focal loss, as the baselines. To comprehensively understand and measure oracles' performance, we use KL divergence and Bhattacharya coefficient to measure the correlation between two feature distributions, use true positive rate (TPR), true negative rate (TNR), accuracy (Acc=$(TP+TN)/Total$), F1 score, precision (P), and recall (R) to measure the efficacy of predicting trustworthiness. Specifically, we add Acc\textsubscript{P} and Acc\textsubscript{N} to understand how much TP and TN contribute to Acc. This is useful when the model overfits the data, \ie classifying all the images as either positives or negatives. Moreover, to differentiate the accuracy of classification from the accuracy of predicting trustworthiness, we denote the classifier's accuracy as C-Acc, and the oracle's accuracy as O-Acc.

% \input{depd/tbl_avg_perf}

% \begin{figure}[!t]
% 	\centering
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/sigmoid_imagenet_trfeat}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/focal_imagenet_trfeat}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/steep_imagenet_trfeat}    } \\
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/sigmoid_imagenet_valfeat}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/focal_imagenet_valfeat}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/steep_imagenet_valfeat}    } \\
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/sigmoid_imagenet_valfeat_sty}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/focal_imagenet_valfeat_sty}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/steep_imagenet_valfeat_sty}    } \\
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/sigmoid_imagenet_valfeat_adv}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/focal_imagenet_valfeat_adv}    } \hfill
% 	\subfloat{\includegraphics[width=0.32\textwidth]{fig/steep_imagenet_valfeat_adv}    }
% 	\caption{\label{fig:distribution}
%     	Feature distributions w.r.t. the cross entropy (first column), focal (second column), and the proposed steep slope (third column) losses on the ImageNet training set (second row), ImageNet validation set (first row), stylized ImageNet validation set (third row), and adversarial ImageNet validation set (fourth row). CE stands for cross entropy, while SS stands for steep slope.
%     % 	\REVISION{\textit{Baseline} indicates ResNet GEM.}
%     	}
% \end{figure}

% \input{depd/tbl_perf_rsn_vit}



% \input{depd/fig_anal_risk}

% \input{depd/fig_anal_tsne}

% \begin{table}[!t]
% 	\centering
% 	\caption{\label{tbl:noise}
% 	    Correctness of oracle on the ImageNet validation set. The oracles are trained with the ImageNet training set. The underlined architecture indicates the architecture of Bayesian network. Leave-out rate indicates the proportion of samples that are ruled out by the oracle. Ideally, it should be equivelant to 1-Acc.
% 	}
% 	\adjustbox{width=1\columnwidth}{
% 	\begin{tabular}{L{10ex} C{12ex} C{12ex} C{9ex} C{9ex} C{9ex} C{9ex} C{9ex} C{9ex} C{9ex}}
% 		\toprule
% 		Dataset & Oracle & Classifier & Acc & O-Acc & O-TP & O-FP & F1 & Precision & Recall \\
% 		\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}
% 		Regular & ViT-sigm            & ViT & 83.90 & 83.93 & 83.41 & 15.57 & 0.9121 & 0.8426 & 0.9941    \\
% 		Regular & ViT-Gauss            & ViT & 83.90 & 83.95 & 83.26 & 15.41 & 0.9121 & 0.8438 & 0.9924    \\
% 		Regular & ViT-exp            & ViT & 83.90 & 82.11 &  &  &  &  &     \\  \midrule
% 		Stylized & ViT-sigm            & ViT & 15.93 & 20.62 & 15.36 & 78.79 & 0.2790 & 0.1631 & 0.9639    \\
% 		Stylized & ViT-Gauss            & ViT & 15.93 & 46.28 & 13.01 & 50.79 & 0.3263 & 0.2039 & 0.8163    \\
% 		Stylized & ViT-exp            & ViT & 15.93 & 72.23 &  &  &  &  &     \\ \midrule
% 		Adv & ViT-sigm            & ViT & 7.41 & 11.23 & & & 0.1307 & 0.0762 & 0.5336    \\
% 		Adv & ViT-Gauss            & ViT & 7.41 & 11.15 & 7.14 & 88.79 & 0.1270 & 0.0744 & 0.5088 \\
% 		Adv & ViT-exp            & ViT & 7.41 & 32.57 &  &  &  &  &     \\ 
% 		\bottomrule	
% 	\end{tabular}}
% \end{table}

% \input{depd/tbl_perf_avg}
\input{depd/tbl_perf_avg_std}

\noindent\textbf{Performance on Large-Scale Dataset}. 
The result on ImageNet are reported in \tabref{tbl:all_perf_w_std}. We have two key observations. Firstly, training with the cross entropy loss, focal loss, and TCP confidence loss lead to overfitting the imbalanced training samples, \ie the dominance of trustworthy predictions. Specifically, TPR is higher than 99\% while TNR is less than 1\% in all cases. Secondly, the performance of predicting trustworthiness is contingent on both the oracle and the classifier. When a high-performance model (\ie ViT) is used as the oracle and a relatively low-performance model (\ie ResNet) is used as the classifier, cross entropy loss and focal loss achieve higher TNRs than the loss functions with the other combinations. In contrast, the two losses with \textlangle ResNet, ViT\textrangle~ lead to the lowest TNRs (\ie 0\%). %, compared to the cases with the other combinations.

Despite the combinations of oracles and classifiers, the proposed steep slope loss can achieve significantly higher TNRs than using the other loss functions, while it achieves desirable performance on FPR-95\%-TPR, AUPR-Success, and AUC. This verifies that the proposed loss is effective to improve the generalizability for predicting trustworthiness. Note that the scores of AUPR-Error and TPR yielded by the proposed loss are lower than that of the other loss functions. Recall that AUPR-Error aims to inspect how easy to detect failures and depends on the negated trustworthiness confidences w.r.t. incorrect predictions \cite{Corbiere_NIPS_2019}. The AUPR-Error correlates to TPR and TNR. When TPR is close to 100\% and TNR is close to 0\%, it indicates the oracle is prone to view all the predictions to be trustworthy. In other words, almost all the trustworthiness confidences are on the right-hand side of $p(o=1|\theta,\bm{x})=0.5$. Consequently, when taking the incorrect prediction as the positive class, the negated confidences are smaller than -0.5. On the other hand, the oracle trained with the proposed loss intends to yield the ones w.r.t. incorrect predictions that are smaller than 0.5. In general, the negated confidences w.r.t. incorrect predictions are greater than the ones yielded by the other loss functions. In summary, a high TPR score and a low TNR score leads to a high AUPR-Error.

To intuitively understand the effects of all the loss functions, we plot the histograms of trustworthiness confidences w.r.t. true positive (TP), false positive (FP), true negative (TN), and false negative (FN) in \figref{fig:histogram_part}. The result confirms that the oracles trained with the baseline loss functions are prone to predict overconfident trustworthiness for incorrect predictions, while the oracles trained with the proposed loss can properly predict trustworthiness for incorrect predictions.

% On the other hand, the proposed steep slope loss show better generalizability over the three domains, where TPR is 73.62\% and TNR is 47.23\%. Secondly, the learned oracles exhibit consistent separability over the three domains through the lens of KL divergence and Bhttacharya coefficient. This is aligned with the intuition that a model that work well on a domain is likely to work well on other domains. 

\input{depd/fig_histograms}

\input{depd/tbl_perf_tcp_mnist}

\noindent\textbf{Separability between Distributions of Correct Predictions and Incorrect Predictions}.
As observed in \figref{fig:histogram_part}, the confidences w.r.t. correct and incorrect predictions follow Gaussian-like distributions.
Hence, we can compute the separability between the distributions of correct and incorrect predictions from a probabilistic perspective.
% There are two common tools to achieve the goal, \ie Kullback–Leibler (KL) divergence \cite{Kullback_AMS_1951} and Bhattacharyya distance \cite{Bhattacharyya_JSTOR_1946}.
Given the distribution of correct predictions {\small $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$} and the distribution of correct predictions {\small $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$}, we use the average Kullback–Leibler (KL) divergence {\small $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})$} \cite{Kullback_AMS_1951} and Bhattacharyya distance {\small $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})$} \cite{Bhattacharyya_JSTOR_1946} to measure the separability. 
More details and the quantitative results are reported in \appref{sec:separability}. 
In short, the proposed loss leads to larger separability than the baseline loss functions. 
This implies that the proposed loss is more effective to differentiate incorrect predictions from correct predictions.

\noindent\textbf{Performance on Small-Scale Datasets}.
We also provide comparative experimental results on small-scale datasets, \ie MNIST \cite{Lecun_IEEE_1998} and CIFAR-10 \cite{Krizhevsky_TR_2009}.
\REVISION{The results are reported in \tabref{tbl:perf_mnist}.}
% The experiment details and results are reported in \appref{sec:mnist}.
The proposed loss outperforms TCP$\dagger$ on metric FPR-95\%-TPR on both MNIST and CIFAR-10, and additionally achieved good performance on metrics AUPR-Error and TNR on CIFAR-10.
This shows the proposed loss is able to adapt to relatively simple data.
\REVISION{More details can be found in \appref{sec:mnist}.}

\noindent\textbf{Generalization to Unseen Domains}.
In practice, the oracle may run into the data in the domains that are different from the ones of training samples.
Thus, it is interesting to find out how well the learned oracles generalize to the unseen domain data.
% To this end, we apply a style transfer method \cite{Geirhos_ICLR_2019} and the functional adversarial attack method \cite{Laidlaw_NeurIPS_2019} to generate the stylized ImageNet validation set and the adversarial ImageNet validation set.
Using the oracles trained with the ImageNet training set (\ie the ones used in \tabref{tbl:all_perf_w_std}), we evaluate it on the stylized ImageNet validation set \cite{Geirhos_ICLR_2019}, adversarial ImageNet validation set \cite{Laidlaw_NeurIPS_2019}, and corrupted ImageNet validation set \cite{Hendrycks_ICLR_2018}.
% and evaluated on the two variants of the validation set.
\textlangle ViT, ViT\textrangle~ is used in the experiment.

The results on the stylized ImageNet, adversarial ImageNet, and ImageNet-C are reported in \tabref{tbl:perf_vit_vit}, \REVISION{More results on ImageNet-C are reported in \tabref{tbl:perf_imagenetc}}.
As all unseen domains are different from the one of the training set, the classification accuracies are much lower than the ones in \tabref{tbl:all_perf_w_std}. 
The adversarial validation set is also more challenging than the stylized validation set \REVISION{and the corrupted validation set}.
As a result, the difficulty affects the scores across all metrics.
The oracles trained with the baseline loss functions are still prone to recognize the incorrect prediction to be trustworthy.
The proposed loss consistently improves the performance on FPR-95\%-TPR, AUPR-Sucess, AUC, and TNR.
Note that the adversarial perturbations are computed on the fly \cite{Laidlaw_NeurIPS_2019}. Instead of truncating the sensitive pixel values and saving into the images files, we follow the experimental settings in \cite{Laidlaw_NeurIPS_2019} to evaluate the oracles on the fly.
Hence, the classification accuracies w.r.t. various loss function are slightly different but are stably around 6.14\%.

% Also, we report the performances on each domain in \tabref{tbl:perf_vit_vit} and \tabref{tbl:perf_rsn_vit}.
% They shows that the cross entropy and focal loss work well on the regular validation set, but work poorly on the stylized and adversarial validation sets. This confirms the overfitting resulted from the learning with the cross entropy and focal loss.

\input{depd/tbl_perf_vit_vit}

\input{depd/fig_anal_ablation}

\noindent\textbf{Selective Risk Analysis}.
Risk-coverage curve is an important technique for analyzing trustworthiness through the lens of the rejection mechanism in the classification task \cite{Corbiere_NIPS_2019,Geifman_NIPS_2017}. 
In the context of predicting trustworthiness, selective risk is the empirical loss that takes into account the decisions, \ie to trust or not to trust the prediction. 
Correspondingly, coverage is the probability mass of the non-rejected region. As can see in \figref{fig:risk_vit}, the proposed loss leads to significantly lower risks, compared to the other loss functions.
We present the risk-coverage curves w.r.t. all the combinations of oracles and classifiers in \appref{sec:risk}.
They consistently exhibit similar pattern.

\noindent\textbf{Ablation Study}.
In contrast to the compared loss functions, the proposed loss has more hyperparameters to be determined, \ie $\alpha^{+}$ and $\alpha^{-}$.
As the proportion of correct predictions is usually larger than that of incorrect predictions, we would prioritize $\alpha^{-}$ over $\alpha^{+}$ such that the oracle is able to recognize a certain amount of incorrect predictions.
In other words, we first search for $\alpha^{-}$ by freezing $\alpha^{+}$, and then freeze $\alpha^{-}$ and search for $\alpha^{+}$.
\figref{fig:abl_loss} and \ref{fig:abl_tpr_tnr} show how the loss, TPR, and TNR vary with various $\alpha^{-}$. In this analysis, the combination \textlangle ViT, ViT\textrangle~ is used and $\alpha^{+}=1$.
We can see that $\alpha^{-}=3$ achieves the optimal trade-off between TPR and TNR.
We follow a similar search strategy to determine $\alpha^{+}=2$ and $\alpha^{-}=5$ for training the oracle with ResNet backbone.
% With the classifier ViT and the ViT based oracle, we show how the performance vary when $\alpha^{+}$ and $\alpha^{-}$ change.  

\noindent\textbf{Effects of Using $z=\bm{w}^{\top}\bm{x}^{out}+b$}.
Using the signed distance as $z$, \ie $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$, has a geometric interpretation as shown in \figref{fig:workflow_a}.
However, the main-stream models \cite{He_CVPR_2016,Tan_ICML_2019,Dosovitskiy_ICLR_2021} use $z=\bm{w}^{\top}\bm{x}^{out}+b$. 
Therefore, we provide the corresponding results in appendix \ref{sec:appd_z}, which are generated by the proposed loss taking the output of the linear function as input.
In comparison with the results of using $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$, using $z=\bm{w}^{\top}\bm{x}^{out}+b$ yields comparable scores of FPR-95\%-TPR, AUPR-Error, AUPR-Success, and AUC.
Also, TPR and TNR are moderately different between $z = \frac{\bm{w}^{\top} \bm{x}^{out}+b}{\|\bm{w}\|}$ and $z=\bm{w}^{\top}\bm{x}^{out}+b$, when $\alpha^{+}$ and $\alpha^{-}$ are fixed.
This implies that TPR and TNR are sensitive to $\|\bm{w}\|$. 
% \REVISION{We discuss the reason in \appref{sec:effect_normalization}.}
% 
\REVISION{
This is because the normalization by $\|w\|$ would make $z$ more dispersed in value than the variant without normalization. 
In other words, the normalization leads to long-tailed distributions while no normalization leads to short-tailed distributions. 
Given the same threshold, TNR (TPR) is determined by the location of the distribution of negative (positive) examples and the extent of short/long tails. 
Our analysis on the histograms generated with and without $\|w\|$ normalization verifies this point.
}

% \noindent\textbf{Learning with Class Weights}. We witness the imbalancing characteristics in the learning task for predicting trustworthiness. Table xx shows that one of most common learning techniques with imbalanced data, \ie using class weights, is not effective. The reason is that applying class weights to the loss function, \eg cross entropy, it only scale up the graph along y-axis. However, the long tail regions still slow down the move of the features w.r.t. false positive or false negative towards the well-classified regions.

% \noindent\textbf{Separability between Distributions of Correct Predictions and Incorrect Predictions}.
% As observed in \figref{fig:histogram_part}, the confidences w.r.t. correct and incorrect predictions follow Gaussian-like distributions.
% Hence, we can compute the separability between the distributions of correct and incorrect predictions from a probabilistic perspective.
% % There are two common tools to achieve the goal, \ie Kullback–Leibler (KL) divergence \cite{Kullback_AMS_1951} and Bhattacharyya distance \cite{Bhattacharyya_JSTOR_1946}.
% Given the distribution of correct predictions $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$ and the distribution of correct predictions $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$, we use the average Kullback–Leibler (KL) divergence $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})$ \cite{Kullback_AMS_1951} and Bhattacharyya distance $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})$ \cite{Bhattacharyya_JSTOR_1946} to measure the separability. More details and the quantitative results are reported in \appref{sec:separability}. In short, the proposed loss leads to larger separability than the baseline loss functions. This implies that the proposed loss is more effective to differentiate incorrect predictions from correct predictions.

\noindent\textbf{Steep Slope Loss vs. Class-Balanced Loss}.
We compare the proposed loss to the class-balanced loss \cite{Cui_CVPR_2019}, which is based on a re-weighting strategy.
The results are reported in \appref{sec:cbloss}.
Overall, the proposed loss outperforms the class-balanced loss, which implies that the imbalance characteristics of predicting trustworthiness is different from that of imbalanced data classification.

% KL divergence is used to measure the difference between two distributions \cite{Cantu_Springer_2004,Luo_TNNLS_2020}, while Bhattacharyya distance is used to measure the similarity of two probability distributions. Given two Gaussian distributions $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$ and $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$, we use the averaged KL divergence, \ie $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) = (d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) + d_{KL}(\mathcal{N}_{2}, \mathcal{N}_{1}))/2$, where $d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})=\log\frac{\sigma_{2}}{\sigma_{1}}+\frac{\sigma_{1}^{2}+(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}-\frac{1}{2}$ is not symmetrical. On the other hand, Bhattacharyya distance is defined as $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})=\frac{1}{4}\ln \left( \frac{1}{4} \left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{2}}+\frac{\sigma^{2}_{2}}{\sigma^{2}_{1}}+2 \right) \right) + \frac{1}{4} \left( \frac{(\mu_{1}-\mu_{2})^{2}}{\sigma^{2}_{1}+\sigma^{2}_{2}} \right)$. A larger $\bar{d}_{KL}$ or $d_{B}$ indicates that the two distributions are further away from each other.


% We hypothesize that $x$ w.r.t. positive and negative samples both follow Gaussian distributions. The discriminativeness of features is an important characteristic that correlates to the performance, \eg accuracy. We are interested in measures of separability of feature distributions, which reflect the discriminativeness from a probabilistic perspective. There are two common tools to achieve the goal, \ie Kullback–Leibler (KL) divergence \cite{Kullback_AMS_1951} and Bhattacharyya distance \cite{Bhattacharyya_JSTOR_1946}. Usually, KL divergence is used to measure the difference between two distributions \cite{Cantu_Springer_2004,Luo_TNNLS_2020}, while Bhattacharyya distance is used to measure the similarity of two probability distributions. Given two Gaussian distributions $\mathcal{N}_{1}(\mu_{1}, \sigma^{2}_{1})$ and $\mathcal{N}_{2}(\mu_{2}, \sigma^{2}_{2})$, we use an averaged KL divergence as in this work, \ie $\bar{d}_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) = (d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2}) + d_{KL}(\mathcal{N}_{2}, \mathcal{N}_{1}))/2$, where $d_{KL}(\mathcal{N}_{1}, \mathcal{N}_{2})$ is the KL divergence between $\mathcal{N}_{1}$ and $\mathcal{N}_{2}$ (not symmetrical). On the other hand, Bhattacharyya distance is defined as $d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2})=\frac{1}{4}\ln \left( \frac{1}{4} \left( \frac{\sigma^{2}_{1}}{\sigma^{2}_{2}}+\frac{\sigma^{2}_{2}}{\sigma^{2}_{1}}+2 \right) \right) + \frac{1}{4} \left( \frac{(\mu_{1}-\mu_{2})^{2}}{\sigma^{2}_{1}+\sigma^{2}_{2}} \right)$. In this work, we use Bhattacharyya coefficient that measures the amount of overlap between two distributions, instead of Bhattacharyya distance. Bhattacharyya coefficient is defined as $c_{B}(\mathcal{N}_{1}, \mathcal{N}_{2}) = \exp(-d_{B}(\mathcal{N}_{1}, \mathcal{N}_{2}))$. $c_{B} \in [0,1]$, where 1 indicates a full overlap and 0 indicates no overlap.

% \noindent\textbf{Semantics Difference between Predicting Trustworthiness and Classification}. As we use ViT for both the oracle and classifier, it is interesting to find out what features are leaned for predicting trustworthiness, in comparison to the features learned for classification. Hence, we compute the $l_{1}$ and $l_{2}$ distances between the features generated by the learned oracle and the features generated by the pre-trained classifier. The features are the inputs to the last layer of ViT, \ie 768-dimensional vectors.

% The mean and standard deviation of distances over all the samples in the training and validation sets are provided in \tabref{tbl:anal_diff}. Note that a smaller distance indicates higher similarity between two features. Overall, the mean of distances w.r.t. the three loss functions are large, but the focal loss yields the smallest averaged distance, which implies that the oracle learned with the focal yields the most similar features as the ones generated by the pre-trained classifier. One of possible reasons is that the focal loss prohibits the oracle training.

% Comparison of classifier backbone and oracle backbone

% Per class accuracy, precision, recall, F1

% \noindent\textbf{Taking Features as Input}
% \figref{fig:anal_featinput} shows the distributions of discriminative features generated by a multi-layer perceptron (MLP),, which plays as an oracle. The MLP takes the features generated by the classifier, instead of images, as input. The MLP-based oracle is training on the training set and is evaluated on the validation set. The figure shows that the oracle barely distinguish between positives and negatives. Because all the features are on the right-hand side of the decision boundary $x=0$.

% focal loss vs proposed

% \input{depd/fig_anal_featinput}

% \input{depd/tbl_diff_classifier_oracle}

% \noindent\textbf{Ablation Study}. With the classifier ViT and the ViT based oracle, we show how the performance vary when $\alpha^{+}$ and $\alpha^{-}$ change.  

% \noindent\textbf{Generalization to Unseen Classifier}.
% As the oracle is trained by observing what a classifier predicts the label for an image, the knowledge learned in this way highly correlates to the behaviours of the classifier. It is interesting to know how the knowledge learned by the oracle generalizes to other unseen classifiers. To this end, we use the ViT based oracle that is trained with a ViT classifier to predict the trustworthiness of a ResNet-50 on the adversarial validation set, which is the most challenging in the three sets. 
% For the proposed loss, we use $\alpha^{+}=1$ and $\alpha^{-}=3$ for the oracle that is based on ViT's backbone, while we use $\alpha^{+}=2$ and $\alpha^{-}=5$ for the oracle that is based on ResNet's backbone.

