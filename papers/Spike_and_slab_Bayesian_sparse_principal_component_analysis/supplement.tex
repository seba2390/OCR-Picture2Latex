\documentclass[pdftex, noinfoline, letter]{imsart}
 
\usepackage[english]{babel}
\usepackage{parskip}
%\usepackage[a4paper]{geometry}
\usepackage{amsmath,amsfonts,amsthm, amssymb}
\usepackage{graphicx} 
\usepackage{enumitem} 
\usepackage{xcolor, latexsym} 
\usepackage{subfigure} 
\usepackage[pdftex, bookmarks=true, breaklinks, hypertexnames=false, pdfborder={0 0 0 0}]{hyperref} 
\usepackage[normalem]{ulem} 
\usepackage{placeins}
\usepackage{filecontents}
\usepackage{soul} %for highlighting
\soulregister\cite7 %for allowing cites within highlight!
\soulregister\ref7
\soulregister\pageref7
%\usepackage{amsthm,amsmath,natbib}
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

% provide arXiv number if available:
%\arxiv{arXiv:0000.0000}

% put your definitions there:
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\tSigma}{\widetilde{\Sigma}}
\DeclareMathOperator{\MF}{\text{MF}}
\DeclareMathOperator{\eig}{\text{eig}}
\DeclareMathOperator{\tB}{\widetilde{B}}
\DeclareMathOperator{\tw}{\widetilde{w}}
\DeclareMathOperator{\vc}{\text{Vec}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\bw}{{w}}
\DeclareMathOperator*{\bmu}{{\mu}}
\DeclareMathOperator*{\bXi}{{\Xi}}
\DeclareMathOperator*{\bomega}{{\omega}}
\DeclareMathOperator*{\bgamma}{{\gamma}}
\DeclareMathOperator*{\ELBO}{\text{ELBO}}
\DeclareMathOperator*{\chol}{\text{Chol}}
\DeclareMathOperator*{\diff}{\text{diff}}
\DeclareMathOperator*{\bz}{ z}

\newcommand{\psg}{{\langle}}
\newcommand{\psd}{{\rangle}}
\newcommand{\varf}[1]{\mathbf{Var}_{#1}}

\newcommand{\bl}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\re}[1]{{\color{red}{#1}}}
\newcommand{\gr}[1]{{\color{green}{#1}}}
\newcommand{\pur}[1]{{\color{purple}{#1}}}
\newcommand{\gra}[1]{{\color{gray}{#1}}}
\newcommand{\ma}[1]{{\color{magenta}{#1}}}
\newcommand{\ora}[1]{\textcolor[rgb]{1.00,0.50,0.00}{#1}}


\definecolor{blendedblue}{rgb}{0.2,0.2,0.7}
\definecolor{darkpurple}{RGB}{49,0,94}
\definecolor{darkgreen}{RGB}{11, 84, 37}


\newcommand{\sbl}[1]{{\color{blendedblue}{#1}}}


\newcommand{\comment}[1]{\sbl{[#1]}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\zed}{\mathbb{Z}}

\newcommand{\given}{\,|\,}

\newcommand{\vp}{\vspace{.5cm}}

\newcommand{\rn}{\sqrt{n}}
\newcommand{\psin}{\psi_{b,L_n}} 
\newcommand{\mh}{\mathbb{H}}

\newcommand{\op}{o_{P_0}(1)}

\allowdisplaybreaks
  

\theoremstyle{plain} 
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{expl}{Example}
 

%\numberwithin{equation}{section}

\allowdisplaybreaks

%\graphicspath{{./Figures/}} 
\endlocaldefs
\usepackage{natbib}
\usepackage{multirow}
\usepackage{chngpage}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[T1]{fontenc}
%\usepackage{subfig}
\usepackage{float}
\usepackage{grffile}
\usepackage{url} 
\usepackage{graphicx,psfrag,epsf}

\usepackage{bbm, dsfont}
\usepackage{mathrsfs} 
\usepackage[ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}
%\usepackage{subcaption}

\DeclareRobustCommand{\hlone}[1]{{\sethlcolor{cyan}\hl{#1}}}
\DeclareRobustCommand{\hlae}[1]{{\sethlcolor{green}\hl{#1}}}
\DeclareRobustCommand{\hlrev}[1]{{\sethlcolor{orange}\hl{#1}}}
\DeclareRobustCommand{\hlr}[1]{{\sethlcolor{magenta}\hl{#1}}}
\newcommand{\hll}[1]{\colorbox{magenta}{$\displaystyle #1$}}

\usepackage{xr}
\externaldocument{BayesianSPCA}

\renewcommand{\thesection}{S.\arabic{section}}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thetable}{S.\arabic{table}}


\begin{document}

\begin{frontmatter}

\title{Supplement to ``Spike and slab Bayesian sparse principal component analysis''}
\runtitle{Spike and slab Bayesian SPCA}

\author{\fnms{Bo Y.-C.} \snm{Ning}\ead[label=e1]{yuchien.ning@gmail.com}}
\and
\author{\fnms{Ning} \snm{Ning}\ead[label=e2]{patning@tamu.edu}}

\affiliation{Harvard University\thanksmark{m1}}

\address{Harvard T. H. Chan School of Public Health \\ 
	Department of Epidemiology, \\
	677 Huntington Ave, Boston, MA 02115\\
	\printead{e1}}
\address{Department of Statistics\\
	Texas A\&M University\\
	College Station, TX 77843
	\printead{e2}}

\runauthor{Ning and Ning}

\end{frontmatter}

%\tableofcontents

This supplementary material contains four sections. Section \ref{app:batch-PX-CAVI} provides the batch PX-CAVI algorithm. Section \ref{px-em} gives the simulation results of the PX-EM algorithm using the $\ell_1$-norm and $\ell_2$-norm in the penalty function. Section \ref{sec:proofs} gives the proofs of Theorems \ref{Thm: Post-cont-rate} and \ref{Thm: variational-post-contr-rate} and Lemma \ref{lemma-convergence-EM}. Auxiliary lemmas are presented in Section \ref{sec:aux}.


\section{The batch PX-CAVI algorithm}
\label{app:batch-PX-CAVI}

In this section, we derive the batch PX-CAVI algorithm. Unlike the PX-CAVI algorithm, the batch PX-CAVI algorithm does not assume $\theta$ being jointly row-sparse; each column of $\theta$ can have identical support such that $S_k \neq S_{k'}$, where $S_k = \{j \in \{1, \dots, p\}: \theta_{jk} \neq 0\}$. 
We need to modify the spike and slab prior, which is
\begin{align}
\pi(\theta, \bgamma|\lambda_1, r) 
& \propto
\prod_{j=1}^p 
\Bigg\{
\int_{A \in V_{r, r}} 
\prod_{k=1}^r
\Big[
\gamma_{jk} g(\theta_{jk}|\lambda_1, A, r) + (1-\gamma_{jk}) \delta_0(\theta_{jk})
\Big] \pi(A) dA
\Bigg\}, \\
&\gamma_{jk}|\kappa \sim \text{Bernoulli}(\kappa), \quad\text{and}\quad \kappa \sim \text{Beta}(\alpha_1, \alpha_2), \nonumber 
\end{align}
where $\gamma_{jk} \in \{0, 1\}$. 
The mean-field variational class is given by
\begin{equation}
\begin{split}
\widetilde{\mathcal{P}}^{MF} = \Bigg\{
& P(\theta): \prod_{j=1}^p \prod_{k=1}^r \Big[z_{jk} \mathcal{N}(\mu_{jk}, \sigma^2 \sigma_{jk}^2)
+ (1-z_{jk}) \delta_0\Big],\; \mu_{jk} \in \mathbb{R},\\
&\hspace{2cm} \langle \mu_{\cdot k}, \mu_{\cdot k'} \rangle = 0, \;\forall k \neq k', \;
\sigma_{jk} \in \mathbb{R}^+, \;z_{jk} \in [0, 1]
\Bigg\}.
\end{split}
\end{equation}
Then the mean-field variational posterior becomes
$$
\widehat P(\theta) = \argmin_{P(\theta) \in \widetilde{\mathcal{P}}^{MF}} KL(P(\theta), \pi(\theta|X)).
$$
Similar to the PX-CAVI algorithm, the batch PX-CAVI algorithm includes an E-step. 
It maximizes the objective function, which is 
$$
\sum_{j=1}^p \sum_{k=1}^r \left(
\mathbb{E}_P Q(\Theta_{jk}|\Theta^{(t)}) - \mathbb{E}_P P(\theta)
\right),
$$
where $Q(\Theta|\Theta^{(t)}) = \mathbb{E}_{\bw|\Theta^{(t)}} \log (\theta, \bw, X)$ and 
$\Theta = (\theta, \Psi, \bz)$ with $\Psi$ given below.
We also apply parameter expansion twice to the likelihood and let $g$ be the multivariate normal density.

In each iteration, the batch PX-CAVI algorithm updates $(\theta_{j1}, \dots, \theta_{jr})$ simultaneously for each $j$. 
We define 
\begin{equation*}
\Psi = 
\begin{pmatrix}
\sigma_{11}^2 & \cdots & \sigma_{1r}^2 \\
\vdots & \vdots & \vdots  \\
\sigma_{p1}^2 & \cdots & \sigma_{pr}^2
\end{pmatrix}
\end{equation*}
as the variance matrix for $P(\theta)$ and denote $\widetilde\Psi$ as the variance matrix for $q(\widetilde \beta)$.
Its $j$-th row is denoted by $\Psi_{j} = (\sigma_{j1}^2, \cdots, \sigma_{jr}^2)'$ and similarly, $\widetilde \Psi_j = (\widetilde \sigma_{j1}^2, \dots, \widetilde \sigma_{jr}^2)$.
Then we obtain
\begin{align}
\label{bpx-cavi:u}
\widehat{\widetilde u}_j &= \left(
\lambda_1 I_r + \sum_{i=1}^n H_i
\right)^{-1} \left(\sum_{i=1}^n X_{ij} \widetilde \omega_i\right), \\
\widehat {\widetilde \Psi}_{j} &= \text{Diag}
\left(
\lambda_1 I_r + \sum_{i=1}^n H_i
\right)^{-1},
\label{bpx-cavi:psi}
\end{align}
where $\text{Diag}(A)$ is a vector containing the diagonal elements of the matrix $A \in \mathbb{R}^{r \times r}$.
Recall that $H_i = \widetilde \omega_i \widetilde \omega_i' + \widetilde V_w$.
Denote $a \circ b$ as the point-wise product between vectors $a$ and $b$, we also obtain 
\begin{align}
\widehat h_j  
\label{bpx-cavi:h}
& = - \frac{1}{2\sigma^2} \sum_{i=1}^n 
\left(
- 2 X_{ij} \widetilde u_j' \circ \widetilde w_i + \text{Diag}(\widetilde u_j' \widetilde u_j H_i) + \sigma^2 \widetilde \Psi_j \circ \text{Diag}H_i 
\right)\nonumber \\
& \quad + \left[\log\left(\frac{\alpha_1}{\alpha_2}\right) + \frac{1}{2} + \frac{\log \lambda_1}{2}\right] \mathbbm{1}_r + \frac{\log (\widetilde\Psi_j)}{2}
-\frac{\lambda_1}{2} \left(\widetilde u_j' \circ \widetilde u_j' + \sigma^2 \widetilde \Psi_j\right).
\end{align}

Once we obtained $\widehat{\widetilde u}$ and $\widehat {\widetilde \Psi}$, we can also obtain $\widehat \mu$ and $\widehat \Psi$ (the same as that in the PX-CAVI algorithm). 
Since batch PX-CAVI does not use jointly row-sparsity, the support of $\widetilde \beta$ is different from the support of $\theta$. The $\widehat { h}$ obtained above is for $\widetilde \beta$. To obtain it for $\theta$, say $\widehat { h}_\theta$, we need to plug-in the estimated values $\widehat \theta$ and $\widehat \Psi$ into (\ref{bpx-cavi:h}). We then solve $\widehat \bz_{\theta}$ from $\widehat h_{jk, \theta} = \log (\widehat z_{jk, \theta} / (1- \widehat z_{jk, \theta})$.

Last, denote $m_j = z_j \circ \widetilde u_j$, we obtain
\begin{align}
\label{bpx-cavi:sigma}
\widehat \sigma^2 = 
\frac{
\Tr(X'X) + \sum_{j=1}^p \sum_{i=1}^n 
\left(
m_j H_i m_j'
- 2X_{ij} m_j \widetilde w_j
+ \lambda_1 m_j m_j' 
\right)+ 2\sigma_b
}{
np + 2(\sigma_a + 1)
}.
\end{align}
The batch PX-CAVI algorithm is given below.
\begin{algorithm}[ht]
\DontPrintSemicolon
   \KwData{$X$, a $p \times n$ matrix, centered and scaled}
   
  \KwInput{$\widehat \mu^{(0)}$, $\widehat{\Psi}^{(0)}$, $\widehat\bz^{(0)}$, $\widehat\sigma^{(0)}$, $r$, number of total iterations $T$, and the threshold $\delta$}
  
{\bf For $t = 0, \dots, T-1$, repeat}:

  \begin{itemize}
  \item[--] update $\widetilde \bomega^{(t+1)}$ and $\widetilde V_w^{(t+1)}$ from (\ref{eqn:cavi-w})
  \item[--] update ${\widetilde u}^{(t+1)}$ and ${\widetilde \Psi}^{(t+1)}$ from (\ref{bpx-cavi:u}) and (\ref{bpx-cavi:psi}) 
  \item[--] update ${ h}^{(t+1)}$ from (\ref{bpx-cavi:h}), then, obtain 
  $\widehat \bz^{(t+1)}$
  \item[--] update $\sigma^{(t+1)}$ from (\ref{bpx-cavi:sigma}) 
    \item[--] obtain $D^{(t+1)}$, $u^{(t+1)}$, and $\Psi^{(t+1)}$
  \item[--] apply SVD to obtain $A^{(t+1)}$ and then, obtain $\mu^{(t+1)}$
  \item[--] using $\mu^{(t+1)}$ and $\Psi^{(t+1)}$ to obtain ${ h}_{\theta}$ and $\bz_\theta$
\end{itemize}

{\bf Stop}: if $\max\left(\big\|\mu^{(t+1)} \mu^{{(t+1)}'} - \mu^{(t)} \mu^{{(t)}'}\big\|_F, \|  \bz_\theta^{(t+1)} - \bz_\theta^{(t)}\|_1\right) \leq \delta$

 \KwOutput{$\widehat P(\theta)$. }
\caption{The batch PX-CAVI algorithm}
\label{batch-px-cavi}
\end{algorithm} 


\section{The PX-EM algorithm: $\ell_1$-norm versus $\ell_2$-norm}
\label{px-em}

We conduct a simulation study to compare the use of the $\ell_1$- and $\ell_2$-norms of the penalty function of the PX-EM algorithm in Section \ref{sec:EM}. 
We chose $n = 200$, $s^\star \in \{10, 20, 40, 70, 150\}$, $r^\star \in \{1, 2, 3, 5\}$, and $p \in \{500, 1000, 2000, 4000\}$. We run 100 simulations in each setting.
From Table \ref{sim-5}, we observe that the $\ell_1$-norm gives more accurate results on both parameter estimation and variable selection, particularly when $r^\star$ is large. 

\begin{table}[!]
\begin{adjustwidth}{-.5in}{-.5in}  
\centering
\caption{\small Simulation results of the PX-EM algorithm using $\ell_1$- and $\ell_2$-norm. We chose $n = 200$, $s^\star \in \{10, 20, 40, 70, 150\}$, $r^\star \in \{1, 2, 3, 5\}$, and $p \in \{500, 1000, 2000, 4000\}$. For each setting, we ran $100$ simulations and obtained the average values of the Frobenius loss of the projection matrix, the percentage of misclassification, FDR, and FNR.}
\label{sim-5}
{ 
\small
\begin{tabular}{ccc|cc|cc|cc|cc}
\Xhline{2\arrayrulewidth}
\multicolumn{3}{c|}{} & \multicolumn{2}{c|}{\bf Frobenius loss} & \multicolumn{2}{c|}{\bf Misc (\%)}
& \multicolumn{2}{c|}{\bf FDR} & \multicolumn{2}{c}{\bf FNR}
\\\Xhline{2\arrayrulewidth}
    $p$ & $s^\star$ & $r^\star$ &  $\ell_1$-norm & $\ell_2$-norm
    &  $\ell_1$-norm & $\ell_2$-norm &  $\ell_1$-norm & $\ell_2$-norm &  $\ell_1$-norm & $\ell_2$-norm\\
\hline
    $1000$ & $10$ & $1$  &
    0.024 & 0.026 &
    0.1 &  0.1 &
    0.001 & 0.000 & 0.001 & 0.001\\
    $1000$ & $10$ & $3$ &
    0.040 & 0.041  &
    0.0 &  0.0 &
    0.000 & 0.000 & 0.000 & 0.000\\
    $1000$ & $10$ & $5$ &
    0.043 & 0.045 &
    0.0 & 0.0 
    & 0.000 & 0.000 & 0.000 & 0.000\\ 
    \hline
    $1000$ & $40$ & $1$  &
    0.054  & 0.054  &
    0.4 & 0.4  &
    0.001 & 0.001 & 0.004 & 0.004\\
    $1000$ & $40$ & $3$ &
    0.128 & 0.186 &
    0.1 &  0.2
    & 0.000 & 0.000 & 0.001 & 0.002\\
    $1000$ & $40$ & $5$ &
    0.128 & 0.241 &
    0.1 &  0.2 & 
    0.000 & 0.000 & 0.001 & 0.002\\
    \hline
    $1000$ & $70$ & $1$  &
    0.093 & 0.092  &
    1.2 & 1.2 &
    0.000 & 0.000 & 0.013 & 0.013\\
    $1000$ & $70$ & $3$ &
    0.214 & 0.335  &
    0.4 &  0.9  &
    0.000 & 0.000 & 0.005 & 0.009\\
    $1000$ & $70$ & $5$ &
    0.212 & 0.514 &
    0.1 &  0.7
    &0.000 & 0.000 & 0.002 & 0.008\\   
    \hline
    $1000$ & $150$ & $1$  &
    0.155 & 0.155 &
    3.7 &  3.7
    & 0.000  & 0.000 & 0.042 & 0.042\\
    $1000$ & $150$ & $3$ &
    0.463 & 0.763 &
    2.6 &  4.8
    & 0.000 & 0.000 & 0.029 & 0.054\\
    $1000$ & $150$ & $5$ &
    0.520 & 1.363 &
    1.5 &  5.9 &
    0.000 & 0.000 & 0.017 & 0.065\\   
\Xhline{2\arrayrulewidth}    
\end{tabular}  
}
\end{adjustwidth}
\end{table}
\FloatBarrier



%-----------------------------------------%
%----------------Proofs-------------------%
%-----------------------------------------%
\section{Proofs of Theorems \ref{Thm: Post-cont-rate} and \ref{Thm: variational-post-contr-rate} and Lemma \ref{lemma-convergence-EM}}
\label{sec:proofs}

\subsection{Proof of Theorem \ref{Thm: Post-cont-rate}}

\begin{lemma}
\label{lemma-1}
For some sufficiently large $C_1$, 
under Assumption \ref{assumps},
we have that as $n$ goes to infinity,
$$\displaystyle
\mathbb{P}_{\Sigma^\star}
\left( 
\int f/f^\star d \Pi(\theta) \leq \exp(-C_1 n\epsilon_n^2)
\right)
\to 0.
$$
\end{lemma}
\begin{proof}
By Lemma 8.10 of \citet{ghosal17}, it suffices to show that
\begin{align}
\label{pf-post-contr-rate-1}
    \Pi \Big(K(f^\star, f) \leq n\epsilon_n^2, \;
    V(f^\star, f) \leq n\epsilon_n^2 \Big) 
    \gtrsim \exp(-C_1 n\epsilon_n^2),
\end{align}
where $V(f^\star, f)$ is the Kullback-Leibler variation between $f^\star$ and $f$.
Since both $f = \prod_{i=1}^n f_i$ and $f^\star = \prod_{i=1}^n f^\star_i$ are independent multivariate Gaussian densities, we have
\begin{align*}
    \frac{1}{n}K(f^\star, f) & =  
    \frac{1}{n} \sum_{i=1}^n K(f_i^\star, f_i)
    = \frac{1}{2}\Big[
    \Tr(\Sigma^{-1} \Sigma^\star) - p - \log(\det(\Sigma^{-1}\Sigma^\star))
    \Big],\\
    \frac{1}{n}V(f^\star, f) & =  
    \frac{1}{n} \sum_{i=1}^n V(f_i^\star, f_i)
    = 
    \frac{1}{2}
    \Big[
    \Tr(\Sigma^{-1} \Sigma^\star\Sigma^{-1} \Sigma^\star)  
    - 2\Tr(\Sigma^{-1} \Sigma^\star) + p)
    \Big].
\end{align*}
Let $\tSigma = {\Sigma^\star}^{1/2} \Sigma^{-1} {\Sigma^\star}^{1/2}$ and $\rho_j$ be the $j$-th largest eigenvalue of $\tSigma$,
we have
\begin{align}
    & \hspace{-0.5cm}\Pi
   \Big(K(f^\star, f) \leq n\epsilon_n^2, \
    V(f^\star, f) \leq n\epsilon_n^2
    \Big) \nonumber \\
    & \geq 
    \Pi \left(
    \sum_{j=1}^p (\rho_j - 1 - \log \rho_j) \leq 2\epsilon_n^2, \
    \sum_{j=1}^p (\rho_j - 1)^2 \leq 2\epsilon_n^2
    \right)
    \nonumber \\
    & \geq
    \Pi \left(
    \sum_{j=1}^p (\rho_j - 1)^2 \leq 2\epsilon_n^2
    \right)\\
    %= 
    %\Pi\left(\|\tSigma - I_p\|_F \leq \sqrt{2}\epsilon_n\right)
    &\geq 
    \Pi\left(
    	\|\Sigma^{-1}\| \|\Sigma^\star - \Sigma\|_F \leq \sqrt{2} \epsilon_n
    \right) 
    \nonumber \\
    &  =
    \Pi\left(
        \|\Sigma - \Sigma^\star\|_F \leq \sqrt{2} \sigma^2 \epsilon_n
    \right)\\
    &= \Pi\left(
        \|\beta\beta' - \beta^\star {\beta^\star}'\|_F \leq \sqrt{2}\sigma^2 \epsilon_n
    \right).
    \label{pf-post-contr-rate-1.1}
\end{align}
In the last display, the second lower bound is obtained by applying the inequality $\log(1+x) \geq x - x^2/2$ for $x < 1$.
The last inequality is obtained by using the fact that
$\|\Sigma^{-1}\| \geq 1/\sigma^2$, and 
the last equality is obtained by $\beta \beta' = \theta \theta'$.
(\ref{pf-post-contr-rate-1.1}) can be further bounded in below by
\begin{align}
    & \Pi\left(
        \|\beta-\beta^\star\|_F(\|\beta\| + \|\beta^\star\|)
        \leq \sqrt{2}\sigma^2 \epsilon_n
    \right) \nonumber \\
   & \quad  \geq 
     \Pi\left(
        \|\beta-\beta^\star\|_F(\|\beta - \beta^\star\|_F + 2\|\beta^\star\|)
        \leq \sqrt{2}\sigma^2\epsilon_n
    \right) \nonumber \\
    & \quad \geq 
    \Pi\left(
    	\|\beta^\star\|\|\beta - \beta^\star\|_F \leq \frac{\sqrt{2}\sigma^2\epsilon_n}{4}
    \right) \nonumber \\
    & \quad \geq
    \Pi\left(
    \|\beta - \beta^\star\|_F 
    \leq \frac{\sqrt{2}\sigma^2 \epsilon_n}{4 \|\beta^\star\|_{q,1}}
    \Bigg| r = r^\star 
\right) \Pi(r = r^\star).
    \label{pf-post-contr-rate-1.3}
\end{align}
We use the inequality $\|\beta - \beta^\star\|_F \leq 2 \|\beta^\star\|$ to obtain second lower bound in the last display. Otherwise, if $2\|\beta^\star\| < \|\beta -\beta^\star\|_F$, then $\|\theta^\star\| \to 0$.
Next, we shall bound (\ref{pf-post-contr-rate-1.3}) below.
Under Assumption \ref{assumps}, $\Pi(r = r^\star) \geq \exp(-a_3 r^\star)$.
Let $d_n = \sqrt{2}\sigma^2 \epsilon_n/(8 \|\beta^\star\|_{q,1})$, we have 
\begin{align}
	& \Pi\left(\|\beta - \beta^\star\|_F \leq 2d_n
    \big| r = r^\star
    \right)\nonumber\\
    &\geq \Pi\left(\sum_{j=1}^p \|\beta_j - \beta^\star_j\|_2 \leq 2d_n
    \Big| r = r^\star
    \right)
    \nonumber \\
    &\geq 
 \frac{\pi(s^\star)}{{p \choose s^\star}}
 \int_A
 \int_{\sum_{j \in S^\star} \|\beta_j - \beta^\star_j\|_q \leq d_n} \prod_{j \in S^\star} g(\theta_j|\lambda_1, A) d\theta_j d\Pi(A)
 \prod_{j \not\in S^\star} \delta_0(\theta_j).
 \label{pf-post-contr-rate-1.4}
\end{align}
Under Assumption \ref{assumps} and using the Stirling approximation to a binomial coefficient, we obtain
\begin{align*}
    \frac{\pi(s^\star)}{{p \choose s^\star}}
    \gtrsim p^{-a_1 s^\star} \left(\frac{p}{s^\star} \right)^{s^\star} 
    \geq \exp \left(-(a_1-1) s^\star \log p - s^\star \log s^\star \right).
\end{align*}
Denote $\check{\beta}_j = \beta_j - \beta^\star_j$ and change the variable from $\beta_j$ to $\check{\beta}_j$,
then the integral in (\ref{pf-post-contr-rate-1.4}) is bounded below by 
\begin{align}
\label{pf-post-contr-rate-1.4.1}
     \exp\left(-\lambda_1 \|\beta^\star\|_{q,1}^m \right)
    \int_{\sum_{j \in S^\star}
    \|\check{\beta}_j \|_q^m \leq d_n}
    C(\lambda_1)^{r^\star}
    \exp
    \left(
        -\lambda_1 \sum_{j \in S^\star} \|\check\beta_j\|_q^m
    \right) d \check\beta_j.
 \end{align}   
 If $m = 1, 1 \leq q \leq 2$, then
 $C(\lambda_1) = \lambda_1/a_{r^\star}$ and
 (\ref{pf-post-contr-rate-1.4.1}) can be bounded by 
 \begin{align}
    & \exp\left(-\lambda_1 \|\beta^\star\|_{q,1}, \right)
    \left(\frac{2}{a_{r^\star}}\right)^{r^\star s^\star} 
    \prod_{j\in S^\star}
    \int_{\sum_{j \in S^\star} 
        \|\check{\beta}_j\|_1 \leq d_n}
    \left(\frac{\lambda_1}{2}\right)^{r^\star} 
    \exp(-\lambda_1 \|\check\beta_j\|_1) d \check\beta_j 
    \nonumber \\
    & \quad \geq 
    \exp\left(
    -\lambda_1 \|\beta^\star\|_{q,1} 
    - \lambda_1 d_n
    \right)
    \left(\frac{2}{a_{r^\star}}\right)^{r^\star s^\star} 
     (\lambda_1 d_n)^{r^\star s^\star} \frac{1}{(r^\star s^\star)!}.
     \label{pf-post-contr-rate-1.5}
\end{align}
Then we plug-in the lower bound of $\|\theta^\star\|$, the upper bound of $\|\theta^\star\|_{1,1}$ (note that $\|\beta^\star\|_{1,1} \leq \|\theta^\star\|_{1,1}$), and the upper bound of $\lambda_1$
to obtain that $$\exp(-\lambda_1 \|\beta^\star\|_{q,1} - \lambda_1 d_n) \geq \exp(-c_{11} n\epsilon_n^2),$$
where $c_{11} = b_3(1+ \sqrt{2}\sigma^2/(8b_4))$. 
Next, since $1 \leq a_r \leq O(\sqrt{r})$, we have that $$(2/a_{r^\star})^{r^\star s^\star} \geq \exp(-c_{12}r^\star s^\star \log r^\star)\geq \exp(-c_{12}s^\star \log p)$$ for some positive constant $c_{12}$.
Furthermore, 
we plug-in the lower bound of $\lambda_1$ and the upper bounds of $\|\theta^\star\|_{q,1}$ and $r^\star$ and using the fact that $s^\star \log p < n$ to obtain that
\begin{align*}
(\lambda_1 d_n)^{r^\star s^\star} & = 
\exp\left(-
r^\star s^\star \log \left(
\frac{8 \|\beta^\star\|_{q,1}}{\sqrt{2}\sigma^2 \lambda_1 \epsilon_n}
\right)
\right)\\
& \geq
\exp\left(- 
r^\star s^\star \log \left(
\frac{8 b_5 s^\star \log p}{\sqrt{2} \sigma^2 \lambda_1^2 \epsilon_n}
\right) \right)\\
& \geq 
\exp\left(-
r^\star s^\star \log \left(
\frac{8 b_5 p^{b_2/r^\star}}{\sqrt{2} \sigma^2 b_1^2}
\right)
\right)\\
& \geq 
\exp(- c_{13} n\epsilon_n^2),
\end{align*}
where $c_{13} = b_2 + \log(8b_5/(\sqrt{2}b_1^2\sigma^2))$.
Last, applying the Stirling's approximation to $(r^\star s^\star)!$ yields $$\exp(-r^\star s^\star \log (r^\star s^\star)) \geq \exp(-s^\star \log p) = \exp(-n\epsilon_n^2).$$
By combining all the lower bounds derived above, we obtain that
\begin{align*}
	& \exp\left(-\lambda_1 \|\beta^\star\|_{q,1}, \right)
	\left(\frac{2}{a_{r^\star}}\right)^{r^\star s^\star} 
	\prod_{j\in S^\star}
	\int_{\sum_{j \in S^\star} 
		\|\check{\beta}_j\|_1 \leq d_n}
	\left(\frac{\lambda_1}{2}\right)^{r^\star} 
	\exp(-\lambda_1 \|\check\beta_j\|_1) d \check\beta_j 
	\nonumber \\
	& \quad \geq \exp(- (c_{11} + c_{12} + c_{13}) n\epsilon_n^2).
\end{align*}
Therefore, we obtain (\ref{pf-post-contr-rate-1}) with $C_1 \geq a_1 + c_{11} + c_{12} + c_{13}$.

If $m = 2$ and $q = 2$, then the term in (\ref{pf-post-contr-rate-1.4.1}) can be bounded in below by 
\begin{align*}
& \exp(-\lambda_1 \|\beta^\star\|^2) \int_{\sum_{j\in S^\star} \|\check{\beta}\|^2 \leq d_n}
\left(\frac{\lambda_1}{2\pi}\right)^{r^\star s^\star/2} 
\exp\Big( - \lambda_1 \sum_{j\in S^\star} \|\check\beta_j\|^2 \Big) d\check\beta \\
& \quad \geq \exp(-\lambda_1 \|\beta^\star\|^2) 
\left(1 - \Phi(|\check\beta_{jk}| \geq \sqrt{\lambda_1 d_n/(s^\star r^\star)})\right)^{r^\star s^\star}\\
& \quad \geq \exp(-\lambda_1 \|\theta^\star\|^2) \left(1 - 2e^{-d_n\lambda_1 / (2s^\star r^\star)}\right)^{s^\star r^\star}\\
& \quad \geq \exp\Big(- \lambda_1\|\theta^\star\|^2 + r^\star s^\star \log (\lambda_1 d_n) - r^\star s^\star \log (r^\star s^\star /8)\Big) \\
& \quad \geq \exp(-c_{14}n\epsilon_n^2),
\end{align*}
where $c_{14} = b_3 b_5 + b_2 + \log(8b_5/(\sqrt{2}b_1\sigma^2)) + 1$.
The second inequality in the last display is obtained by applying the tail bound of the standard normal distribution and using the fact that $\|\beta^\star\| = \|\theta^\star\|$. The third inequality is obtained by using the inequality $1 - e^{-x} \geq x/2$ for $x < 1$ and note that 
\begin{align*}
\exp(-r^\star s^\star \log(\lambda_1 d_n)) &\geq \exp\left(-r^\star s^\star \log \left(\frac{8\|\beta^\star\|^2}{\lambda_1 \sqrt{2}\sigma^2 \epsilon_n}\right) \right)\\
& \geq \exp\left(-s^\star r^\star \log \left(
\frac{8 \sqrt{s^\star \log p} p^{b_2/r^\star}}{\sqrt{2} \sigma^2 n}
\right) \right) \\
& \geq \exp\Big(-(b_2 + \log(8b_5/(\sqrt{2}b_1\sigma^2)) + 1) n\epsilon_n^2\Big),
\end{align*}
where we plugged-in the lower bound of $\lambda_1$ and upper bound of $\|\theta^\star\|^2$ to obtain the second lower bound in the last display.
\end{proof}

%-----------------------------%
\begin{proof}[Proof of Theorem \ref{Thm: Post-cont-rate}]
We first prove (\ref{Thm-1:eqn-3}).
By Lemma \ref{lemma-1}, we only need to take care of the numerator.
Define the set $\mathcal{S} = \{\theta: s \leq M_2s^\star\}$,
we have
\begin{align*}
    \mathbb{E}_{f^\star} \left(
        \int_{\mathcal{S}^c} \frac{f}{f^\star} d\Pi(\theta)
    \right) 
    & \leq \int_{\mathcal{S}^c} d\Pi(\theta)
    \leq \Pi(\theta: s > M_2s^\star)
    = \sum_{s = M_2s^\star}^\infty \pi(s). 
\end{align*}
Under Assumption \ref{assumps},
without loss of generality assuming that $\pi(s)/\pi(s-1) \leq p^{- a_2}$, we obtain that
\begin{align*}
    \sum_{s = M_2s^\star}^\infty \pi(s)
    & \leq 
    \sum_{s = M_2 s^\star}^\infty \pi(s^\star) 
    (p^{-a_2})^{M_2s^\star - s^\star}
    \leq 
    2 p^{-a_2(M_2s^\star - s^\star)}.
\end{align*}
The last inequality is obtained using the trivial bounds
$p^{-a_2} \leq 1/2$ and $\pi(s^\star) < 1$.
Let $M_2 \geq C_1/a_2$, 
we obtain (\ref{Thm-1:eqn-3}).


Now, we proof (\ref{Thm-1:eqn-1}). 
We apply the general theory of posterior contraction \citep{ghosal17}.
The prior mass condition is given in Lemma \ref{lemma-1}.
Next, we need to show that there exists a sieve $\mathcal{F}_n$ such that $\Pi(\mathcal{F}^c_n) \lesssim \exp(-C_2n\epsilon_n^2)$. Consider the following sieve:
$$
\mathcal{F}_n = \left\{\beta = \theta O: \max_j \|\beta_j\|_{q} \leq H_n,\; r \leq J_n\right\},
$$
where $J_n = s^\star \log p$, and $H_n = c n/\left(\underline{\lambda_1}\right)$ with $\underline{\lambda_1}$ is the lower bound of $\lambda_1$ (see Assumption \ref{assumps}) and $c$ is a positive constant.
We shall show that $\Pi(\mathcal{F}_n^c) \lesssim \exp(-C_2 n\epsilon_n^2)$.
Note that
\begin{align}
\label{pf-post-contr-rate-1.6}
\Pi(\mathcal{F}_n^c) \leq 
\Pi\Big(\max_j\|\beta_j\|_q \geq H_n| s \leq M_2s^\star,\; r \leq J_n\Big) + \Pi(r > J_n).
\end{align}
Under Assumption \ref{assumps}, 
we have
\begin{align*}
    \Pi(r > s^\star \log p)
    \lesssim \sum_{r = s^\star \log p}^{\infty} 
    \exp(- a_4 r)
    &\leq \exp(- a_4 s^\star \log p) \sum_{k = 0}^\infty e^{- a_4 k}\\
    &= \exp(- a_4 s^\star \log p)/a_4.
\end{align*}
The first term in the right hand side of (\ref{pf-post-contr-rate-1.6}) is bounded by
\begin{align*}
& \sum_{r = 1}^{J_n}
\sum_{\{S: s \leq M_2s^\star\}} 
   \frac{\pi(s)}{{p \choose s}} 
   \sum_{j \in S} \int_A \Pi(\|\beta_j\|_q \geq H_n|\lambda_1) d\Pi(A) \\
   &\quad \leq 
    \sum_{s = 0}^{M_2s^\star} \pi(s)
    \left(
   \sum_{r = 1}^{J_n}
    M_2s^\star \exp((r - H_n\lambda_1)/2)
   \right)\\
   & \quad \leq
   \sum_{r = 1}^{J_n}
   \Big(
   \exp(\log(M_2s^\star) + (r - cn)/2)
   \Big)\\
   & \quad \leq 
   \exp\Big(\log(M_2s^\star) + \log J_n + J_n/2 - cn/2\Big)\\
 & \quad \leq \exp(- C_2 n \epsilon_n^2),
\end{align*}
where the second line of the last display is obtained by the fact that $\pi(s) < 1$.
Combining the two upper bounds derived above, we obtain that 
$\Pi(\mathcal{F}_n^c) \lesssim \exp(-C_2 n \epsilon_n^2)$,
where $C_2 > C_1 + 4$ and $c > C_1 + 7$.

At last, we need to find a sequence of test $\phi_n$ and show that
\begin{align}
\label{test}
H_0: \mathbb{E}_{f^\star} \phi_n \to 0, \quad
H_1: \sup_{\Sigma \in \mathcal{F}_n: \|\Sigma - \Sigma^\star\| \geq M_2\epsilon_n} \mathbb{E}_{f}(1-\phi_n) \leq \exp(-C_3 n\epsilon_n^2).
\end{align}
We directly use the test constructed by \citet{gao15} (see Lemma \ref{apx-test}) and obtain that 
\begin{align}
\mathbb{E}_{f^\star} \phi_n
& \leq \exp
\left(
C_3 M_2 s^\star - \frac{C_3 M_2^2 n\epsilon_n^2}{4\|\beta^\star\|^2}
\right)
+ 2\exp \left(C_3 M_2 s^\star - C_3 \sqrt{M_2} n \right)
\nonumber \\
& \leq 3\exp
\left(
C_3 M_2 s^\star - \frac{C_3 M_2^2 n\epsilon_n^2}{4b_4^2}
\right).
\label{pf-post-contr-rate-1.7}
\end{align}
The second upper bound in the last display is obtained using the assumption that $\|\beta^\star\| = \|\theta^\star\| \geq b_4$ in Assumption \ref{assump:truevalue}.

To prove the alternative part of the test in (\ref{test}), we divide it into small pieces based on $S$ (recall that $S \subset \{S:s \leq M s^\star\}$). For each piece, we apply Lemma \ref{apx-test}. Then we obtain that
\begin{align*}
&\hspace{-1cm}\sup_{\|\Sigma - \Sigma^\star\| > M_2 \epsilon_n} \mathbb{E}_{f} (1-\phi_n)\\
& \leq \exp\left(
C_3 s - \frac{C_3 M_2 n\epsilon_n^2}{4} 
\max\left\{
1, 
\frac{M_2}{(\sqrt{M_2} + 2)^2 \|\Sigma^\star\|^2}
\right\}
\right)\\
& \leq 
\exp\left(
C_3 s - \frac{C_3M_2 n \epsilon_n^2}{4}
\right).
\end{align*}
Finally, we sum up the above small pieces and obtain that
\begin{align*}
 \sup_{f \in \mathcal{F}_n: \|\Sigma - \Sigma^\star\| \geq M_2 n \epsilon_n^2}
 \mathbb{E}_f(1 - \phi_n)
& \leq 
 \sum_{|S| \leq M s^\star} 
 \exp\left(
C_3 s - \frac{C_3 M_2 n \epsilon_n^2}{4}
\right)\\
& \leq 
\exp\left( \log(M s^\star) + 
C_3 M s^\star - \frac{C_3 M_2 n \epsilon_n^2}{4}
\right) \\
& \lesssim 
\exp(- C_3' n \epsilon_n^2/4),
\end{align*}
where $C_3' > C_3(M_2/4 - M - \log M/C_3)$.

So far, we have verified all the three conditions. Therefore, we obtain that 
\begin{align*}
    & \mathbb{E}_{f^\star} \Pi\Big(\|\Sigma - \Sigma^\star\| > M_2 \epsilon_n, 
    |S| \leq M_2 s^\star | X\Big)\\
    & \quad \leq 
    \mathbb{E}_{f^\star} 
    \Bigg[\Pi\Big(\|\Sigma - \Sigma^\star\| \geq M_2 \epsilon_n,
    |S| \leq M_2 s^\star | X\Big)  \\
    & \hspace{2cm} \quad \times \Pi\left( \int f/f^\star d\Pi(\theta) \leq \exp(-C_1n\epsilon_n^2)\right)(1-\phi_n)
    \Bigg]\\ 
    & \quad \quad 
    + \mathbb{E}_{f^\star} \phi_n
    + \mathbb{E}_{f^\star} 
    \Pi \left( 
    \int f/f^\star d\Pi(\theta) \geq \exp(-C_1n\epsilon_n^2)
    \right)\\
    & \quad \lesssim
    \delta_n \to 0,
\end{align*}
where $  \delta_n = \exp(- (C_1 \vee C_2) n\epsilon_n^2 )
    + \exp \left(
    C_3 M_2 s^\star - C_3 M_2^2 n \epsilon_n^2/(4 b_5^2)
    \right)$.
Thus the proof of (\ref{Thm-1:eqn-1}) is complete.    
    
To prove (\ref{Thm-1:eqn-2}), by David-Kahn $\sin \theta$ theorem (see Theorem \ref{apx-davis-kahan}),
$$\big\{\|\Sigma - \Sigma^\star\| \leq M \epsilon_n\big\} \subset \big\{\|UU' - U^\star {U^\star}'\| \leq M' \epsilon_n\big\},$$ where $M' = M/b_5$, thus (\ref{Thm-1:eqn-1}) implies (\ref{Thm-1:eqn-2}).
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{Thm: variational-post-contr-rate}}

\begin{proof}
We apply Theorem \ref{Thm: general-theory-vp}.
Let $\rho = 2$ and $L = \|\Sigma - \Sigma^\star\|$, we only need to verify the following conditions:
\begin{eqnarray}
& \Pi(D_2(f^\star, f) \leq n\epsilon_n^2) \geq \exp(-C_4n\epsilon_n^2), &
\label{proof-mfvb-cond1}\\
&  - \log P(\theta) \leq C_5n\epsilon_n^2, &
\label{proof-mfvb-cond2}
\end{eqnarray}
where  $D_2(f^\star, f)$ is the $\chi^2$-divergence between $f^\star$ and $f$, and 
$$\Theta \subset \Big\{
\theta: K(f^\star, f) \leq C_1n\epsilon_n^2, \
\log \left(dQ(\theta)/d\Pi(\theta)\right) \leq C_2 n\epsilon_n^2
\Big\}.$$

We first verify (\ref{proof-mfvb-cond1}).
Recalling that $f$ and $f^\star$ are both independent multivariate normal densities, we obtain
\begin{align*}
    D_2(f^\star, f) & 
    =\frac{1}{2} \log\left(
    \int {(f^\star)^2}/{f} \right)\\
    &= -\frac{n}{4}
    \left(
        \log \det(\widetilde \Sigma) + 
        \log \det \left(2I_p - \widetilde \Sigma\right)
    \right)\\
    & = 
    -\frac{n}{4} \sum_{j=1}^p
    \Big[
        \log (1+(\rho_j-1))  + \log (1-(\rho_j - 1))
    \Big] \\
    & \leq n \sum_{j=1}^p (\rho_j-1)^2
\end{align*}
where $\widetilde\Sigma = \Sigma^{-1/2}\Sigma^\star \Sigma^{-1/2}$
and $\rho_j$ is the $j$-th largest eigenvalue of $\widetilde\Sigma$.
The upper bound in the last display is obtained by first applying Taylor expansion to the two log functions and then using the inequality 
$x^2 + x^4/2 + x^6/3 + \cdots \leq 4x^2$ when $|x| < 1/2$.
The last display leads to the following inequalities
\begin{align*}
\Pi\big(D_2(f^\star, f) \leq n\epsilon_n^2\big) 
\geq \Pi\big(\|\widetilde \Sigma - I_p\|_F \leq \epsilon_n\big)
\geq \Pi\big(\|\Sigma - \Sigma^\star\|_F \leq \sigma^2 \epsilon_n\big).
\end{align*}
Similar to the proof of Lemma \ref{lemma-1}, the probability at the right hand side of the above inequality is bounded below by
$$\Pi(\|\beta - \beta^\star\|_F \leq d_n' \big| r=r^\star) \Pi(r^\star = r),$$
where $d_n' = \sigma^2\epsilon_n/\|\beta^\star\|_{q,1}$.
Under Assumption \ref{assumps}, we obtain $\Pi(r^\star = r) \geq \exp(-a_3 r^\star)$.
The first probability in the product, it is bounded in
(\ref{pf-post-contr-rate-1.4}) with $d_n$ replaced with $d_n'$.
We then obtained (\ref{proof-mfvb-cond1}) by applying the same argument as in (\ref{pf-post-contr-rate-1.4}).


To verify (\ref{proof-mfvb-cond2}),
we choose $\widetilde{P} = \prod_{j=1}^p \widetilde{P}_j$, where $$\widetilde{P}_j = \gamma_j^\star \mathcal{N}(0, I_{r^\star}/ \lambda_1) + (1-\gamma_j^\star) \delta_0.$$
Clearly, $\widetilde{P}_j \in Q^{MF}$. 
Then (\ref{proof-mfvb-cond2}) can be written as
\begin{align}
\label{proof-mfvb-cond3}
    \widetilde{Q}
    \left(
        KL(f^\star, f) \lesssim n\epsilon_n^2, \
        \log \frac{d\widetilde{Q}(\theta)}{d\Pi(\theta)} \lesssim n\epsilon_n^2
    \right) \geq \exp(-C_5 n \epsilon_n^2)
\end{align}
Apply a similar argument as that in the proof of Theorem \ref{Thm: Post-cont-rate} yields
\begin{align}
\label{mf-posterior-1}
   &\hspace{-1cm} \widetilde{Q}
    \left(
        KL(f^\star, f) \lesssim n\epsilon_n^2, \
        \log \frac{d\widetilde{Q}(\theta)}{d\Pi(\theta)}
        \lesssim n\epsilon_n^2
    \right)\\
    &\geq 
    \widetilde{Q}
        \left(
        \sum_{j \in S^\star} \|\beta_j - \beta_j^\star\|_2 \leq d_n', \
        \log \frac{d\widetilde{Q}(\theta)}{d\Pi(\theta)}
        \lesssim n\epsilon_n^2
    \right).
\end{align}
Note that
\begin{align}
\label{bound-1}
& \log d\widetilde{Q}(\theta) - \log d\Pi(\theta) \nonumber \\
& \quad \leq \log d\widetilde{Q}(\theta) - \log d\Pi(\theta|S = S^\star) -
\log \left(\frac{\pi(|S^\star|)}{{p \choose s^\star}}\right).
\end{align}
Plugging in the density function of the multivariate normal distribution and the expression of $g(\theta)$, and then applying Jensen's inequality
$$\log \int g(\theta|A) d\Pi(A) \geq \int \log (g(\theta|A) \pi(A)) dA,$$ we obtain 
\begin{align}
    & \log d\widetilde{Q}(\theta) - \log d\Pi(\theta|S = S^\star) 
    \log (\pi(|S^\star|)) + \log {p \choose s^\star}
    \nonumber \\
    & \quad \leq c_1 r^\star s^\star \log (r^\star)  + c_2s^\star \log p - \lambda_1\sum_{j\in S^\star} \|\theta_j - \theta_j^\star\|_2^2 + \lambda_1 \sum_{j \in S^\star} \|\beta_j \|_q^m \nonumber \\
    & \quad \leq 
    c_3 r^\star s^\star \log (r^\star) + \lambda_1 \sum_{j \in S^\star} \|\beta_j - \beta_j^\star\|_1 +
    \sum_{j \in S^\star} \|\beta_j^\star\|_1\nonumber\\
    & \quad \leq c_4 r^\star s^\star \log p + \lambda_1 \sum_{j \in S^\star} \|\beta_j - \beta_j^\star\|_1,
        \label{bound-2}
\end{align}
where $c_1, \dots, c_4$ are positive constants. The first inequality in the last display is by Assumption \ref{assumps} and the upper bound $\log {p \choose s^\star} \lesssim s^\star \log p$. The second inequality is obtained using the triangle inequality that $\|a \|_q^m \leq \|a - b\|_q^m + \|b\|_q^m$.
The last inequality is obtained by plugging-in the upper bound of $\sum_{j \in S^\star} \|\theta^\star\|_1$ in Assumption \ref{assump:truevalue}, as $\|\beta^\star_j\|_1 \leq \|\theta^\star_j\|_1$ for all $j$.
Let $d_n'' = \sigma^2 \lambda_1/n\epsilon_n$, what left to show is the following:
\begin{align*}
&\hspace{-1cm}\widetilde{Q}
\Bigg(
\sum_{j \in S^\star} \|\beta - \beta^\star\|_1 \leq d_n''
\Bigg)\\
& \geq \widetilde{Q} \Bigg(
	\sum_{j \in S^\star} \|\beta_j - \beta_j^\star\|_2 \leq d_n'/\sqrt{r^\star}
\Bigg) \\
& \geq 
\exp \Bigg(-\lambda_1 \sum_{j \in S^\star} \|\theta_j^\star\|_2 \Bigg)
\left(1 - 
\Phi\Bigg(
|x| \geq \frac{d_n''}{\sqrt{{r^\star}^3}s^\star \lambda_1}\Bigg) \right)^{s^\star r^\star}\\
& \geq 
\exp(- n\epsilon_n^2) 
\exp\left(r^\star s^\star 
\log ({d_n''}^2 / (2{r^\star}^3 {s^\star}^2 \lambda_1^2)\right)\\
& \geq 
\exp\left(- n\epsilon_n^2 - 2s^\star r^\star \log n - s^\star r^\star 
\log (2{r^\star}^3 s^\star/\sigma^4)\right) \\ 
& \geq \exp(-5n\epsilon_n^2),
\end{align*}
where the notation $\Phi$ stands for the cumulative distribution function  of the standard normal distribution.
Here, the second inequality is obtained by changing the variables from $\beta_j-\beta^\star_j$ to $\check{\beta_j}$ and use that $\|\beta\| = \|\theta\|$.
The fourth inequality is obtained by using the inequality 
$\lambda_1 \|\theta^\star\|_{1,1} \leq n\epsilon^2_n$.
The last inequality uses $r^\star \leq \log p/\log n$ in Assumption \ref{assump:truevalue}
and $s^\star \ll n$.
\end{proof}

\subsection{Proofs of Lemma \ref{lemma-convergence-EM}}
\begin{proof}
We apply the parameter expansion twice, but we only prove the result for the first parameter expansion, since the proof for the second parameter expansion is similar. 

The expanded parameter in chosen to be $A$, an orthogonal matrix.
The EM defines a map $P$ such that $\Delta^{(t+1)} = P\Delta^{(t)}$ at the $t$-th iteration. Then, by Taylor's theorem, 
$\Delta^{(t+1)} - \Delta^\star \approx DP (\Delta^{(t)} - \Delta^\star)$, where $DP$ is the derivative of $P$ evaluated at $\Delta^\star$ and the speed of convergence is governed by the largest eigenvalue of the matrix $D\Psi$.
Define $S = I - DP$ and $S = I^{-1}_{com}(\Delta) I_{obs}(\Delta)$.
After expanding the parameter, we obtain 
\begin{align*}
I_{obs}(\widetilde \Delta)
=
\begin{pmatrix}
I_{obs}(\Delta) & 0\\
0 & 0
\end{pmatrix}
\quad \text{and}\quad
I_{com}(\widetilde \Delta) 
=
\begin{pmatrix}
I_{com}(\Delta) & F\\
F' & G
\end{pmatrix},
\end{align*}
where 
$F = -\frac{\partial^2 Q(\widetilde \Delta| \widetilde \Delta)}{\partial \Delta \partial \Psi'}$
and $G = -\frac{\partial^2 Q(\widetilde \Delta| \widetilde \Delta)}{\partial \Psi \partial \Psi'}$.

We now calculate $F$ and $G$. Assuming $m = 1$ and $\sigma^2 = 1$, the objective function is
\begin{align}
Q &= C - \sum_{j=1}^p \Big(\beta_j M \beta_j'/2 - \beta_j M_L d_j + pen_j  \|\beta_j\|_q \Big) + f(\widetilde{\bgamma}, \kappa)
\nonumber
\\
& = C - \sum_{j=1}^p \Big(\theta_j M \theta_j'/2 - \theta_j A M_L d_j + pen_j \|\theta_j A\|_q\Big) + f(\widetilde{\bgamma}, \kappa),
\end{align}
where $pen_j = \widetilde \gamma_j \lambda_1 + (1-\widetilde \gamma_j) \lambda_0$ and $$f(\widetilde\gamma, \kappa)
= (\|\widetilde{\bgamma}\|_1 + \alpha_1 - 1) \log \kappa + (p - \|\widetilde{\bgamma}\|_1+\alpha_2- 1) \log (1-\kappa).$$
If $q = 2$, then $\|\beta_j\|_2 = \|\theta_j\|_2$, we obtain $G = 0^{r\times r}$ and 
$F = -\sum_{j=1}^p M_L d_j$.
Denote
\begin{align*}
I^{-1}_{com}(\widetilde \Delta) 
=
\begin{pmatrix}
V_{\Delta, \Delta} & V_{\Delta, m}\\
V_{\Delta, m}' & V_{m,m}
\end{pmatrix},
\end{align*}
we have
\begin{align*}
V_{\Delta, \Delta} 
& = I_{com}^{-1}(\Delta) + I_{com}^{-1}(\Delta) F(G - F' I_{com}^{-1}(\Delta) F)^{-1} F'I_{com}^{-1}(\Delta) \\
& =  I_{com}^{-1/2}(\Delta) 
\left[
I - I_{com}^{-1/2}(\Delta) F
(F' I_{com}^{-1/2}(\Delta) F)^{-1} F' I_{com}^{-1/2}(\Delta)\right]
I_{com}^{-1/2}(\Delta)\\
& < I_{com}^{-1}(\Delta).
\end{align*}

If $q = 1$, then $G = 0^{r\times r}$ and $F = - \sum_{j=1}^p M_L d_j + pen_j \times \text{sign}(\beta_j)$.
We can also obtain that $V_{\Delta, \Delta} < V_{com}^{-1}(\Delta)$. 
Therefore, the smallest eigenvalue of $S$ decreases after the parameter expansion and hence, the speed of the PX-EM algorithm increases, as the largest eigenvalue $D\Psi$ increases. 
\end{proof}

\section{Auxiliary lemmas}
\label{sec:aux}

\begin{lemma}
\label{gammaUpperBound}
For $X \sim \text{Gamma}(r, \lambda)$, then $P(X> b) \leq \exp((r - \lambda b)/2).$
\end{lemma}
\begin{proof}
From page 29 of \citet{bouc13}, we have
\begin{align*}
    P(X > b) \leq 
    \exp\left(
        -r
        \left( 
            1 + \lambda b / r - \sqrt{1 + 2\lambda b / r}
        \right)
    \right).
\end{align*}
Now use the inequality $1 + b - \sqrt{1+2b} \geq (b-1)/2$ for $b > 0$, we obtain the desired upper bound.
\end{proof}


\begin{lemma}[\citet{gao15}]
\label{apx-test}
For random i.i.d. variables $Y_i \sim \mathcal{N}(0, \Gamma^\star)$, $i = 1, \dots, n$, $Y_i \in \mathbb{R}^d$, $d < n$, and $C_3, M > 0$, there exists a test function $\phi$ such that 
$$P_{\Sigma^\star} \phi(Y^n) 
    \leq \exp\left(C_3 d - \frac{C_3M^2 n\epsilon_n^2}{4 \|\Sigma^\star\|^2}\right)
    + 2\exp(C_3d - C_3M^{1/2}n),$$
 \begin{align*}
 	&\hspace{-1cm} \sup_{\{\Gamma: \|\Gamma - \Gamma^\star\| > M\epsilon\}}
 	P_{\Sigma} (1 - \phi(Y^n))\\
 	&\leq \exp
 	\left(
 	C_3d - \frac{C_3 Mn\epsilon_n^2}{4}
 	\max\left\{
 	1, \frac{M}{(\sqrt{M} + 2)^2 \|\Gamma^\star\|^2}
 	\right\}
 	\right).
 \end{align*}
\begin{proof}
See the proof of Lemma 5.7 in \citet{gao15}.
\end{proof}
\end{lemma}

\begin{theorem}[The Davis-Kahan $\sin \theta$ theorem]
\label{apx-davis-kahan}
Let $\Sigma$ and $\widehat\Sigma$ be two symmetric matrices with eigenvalues $\lambda_1 \geq \cdots \geq \lambda_p$ and $\widehat\lambda_1 \geq \cdots \geq \widehat\lambda_p$. Fix $1 \leq r \leq s \leq p$ and let $d = s-r+1$,
and let $V$ be first $d$ eigenvectors of $\Sigma$ and use a similar definition for $\widehat V$. Let $\delta = \inf\{|\widehat\lambda - \lambda|: \lambda \in [\lambda_s, \lambda_r]\}$ and assume that $\delta > 0$,
then 
\begin{align*}
    \|\widehat V\widehat V' - VV' \|_F  \leq \delta^{-1} \|\widehat\Sigma - \Sigma\|_F\quad\text{and}\quad
    \|\widehat V\widehat V' - VV' \| \leq \delta^{-1} \|\widehat\Sigma - \Sigma\|.
\end{align*}
\end{theorem}

\begin{theorem}[Theorems 2.1 and 2.4 of \citet{zhang20}]
\label{Thm: general-theory-vp}
Suppose that for a sequence of $\epsilon_n$ with $\epsilon_n \to 0$ and $n\epsilon_n^2 \to \infty$.
Consider a loss function $L$ such that $L(f^\star, f) \geq 0$ and let $D_\rho$ is the $\rho$-R\'enyi divergence, $\rho > 1$, let $\Theta \in \mathcal{F}_n$, the sieve, and $\phi_n$ be a sequence of test function, if 
\begin{align}
    & \Pi(D_\rho(f^\star, f) \leq C_1n\epsilon_n^2) \geq \exp(-C_2 n \epsilon_n^2),
    \label{mf-cont-rate-cond1}\\
    & \Pi(\mathcal{F}_n^c) \leq \exp(-Cn\epsilon_n^2), 
    \label{mf-cont-rate-cond2}\\
    & \mathbb{E}_{f^\star}(\phi_n) + \sup_{\theta \in {\mathcal{F}_n \cap \{L(f^\star, f) \geq C_3n\epsilon_n^2\}}} \mathbb{E}_f(1-\phi_n) \leq \exp(-Cn \epsilon^2),
    \label{mf-cont-rate-cond3}
\end{align}
then for the variational posterior $\widehat q$, we have
\begin{align*}
    \mathbb{E}_{f^\star} \widehat Q (L(f, f^\star)) \leq M n(\epsilon_n^2 + \gamma_n^2),
\end{align*}
for some constants $C_1, C_2, C_3$, and $M$ and $C > C_1+C_2+2$,
where 
\begin{align}
\gamma_n^2 & = \frac{1}{n} \inf_{q \in Q^{MF}} \mathbb{E}_{f^\star} K(q, \pi(\beta|X))
\leq \inf_{q \in Q^{MF}} R(q),
\end{align}
where $R(q) = \frac{1}{n}\left(K(q, \pi(\beta|X) + \mathbb{E}_{q}[K(f^\star, f)]\right)$.

Furthermore, if $q \in Q^{MF}$, denote a subset $\Theta = \prod_{j=1}^p \Theta_j$ such that
\begin{align*}
    \Theta \subset 
    \left\{
        \beta: KL(f^\star, f) \leq C_1 n\epsilon_n^2, \
        \log \frac{dQ(\beta)}{d\Pi(\beta)} \leq C_2 n\epsilon_n^2
    \right\}, 
\end{align*}
and 
\begin{align}
\label{mf-cont-rate-cond4}
    - \sum_{j=1}^p \log q_j(\Theta_j) \leq C_3n\epsilon_n^2,
\end{align}
then for some positive constants $C_1, C_2$, and $C_3$, we have 
\begin{align*}
    \inf_{q \in Q^{MF}} R(q)\leq (C_1+C_2+C_3) \epsilon_n^2.
\end{align*}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% Bibliography %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{chicago}
\bibliography{citation.bib}

\makeatletter\@input{supp.tex}\makeatother
\end{document}