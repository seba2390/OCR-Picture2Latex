\section{Pre-trained Knowledge Graph Model (PKGM)}

Product knowledge graph $\mathcal{K} = \{\mathcal{E}, \mathcal{R}, \mathcal{F}\}$ stores information of items and relations between items as triples, in which $\mathcal{E}, \mathcal{R}$ and $\mathcal{F}$ are entity set, relation set and triple set respectively. $\mathcal{E} = \{\mathcal{I, V}\}$ contains a set of items $\mathcal{I}$ and a set of values $\mathcal{V}$. $\mathcal{R} = \{ \mathcal{P}, \mathcal{R^\prime} \}$ contains a set of items properties $\mathcal{P}$ and a set of relations between items $\mathcal{R}^\prime$. Our product knowledge graph contains more than $70$ billion triples. 

\begin{figure*}
    \centering
    \includegraphics[width = \textwidth]{figures/PKGM-new.pdf}
    \caption{Pre-trained Knowledge Graph Model.}
    \label{fig:PKGM}
\end{figure*}

We propose a pre-trained knowledge graph model to encode $\mathcal{K}$ into continuous vector space, making it possible to provide knowledge services for knowledge-enhanced downstream tasks based on calculation in vector space. Knowledge services for other tasks usually refer to returning data in knowledge graph matching knowledge queries. Triple query and relation query are two types of basic queries commonly executed in knowledge graphs. Triple queries are in the following form
\begin{lstlisting}[frame=shadowbox]
SELECT ?t
WHERE {h r ?t}
\end{lstlisting}
where the target is to get the tail entity $t$ given an head entity $h$ and relation $r$. Relation queries are
\begin{lstlisting}[frame=shadowbox]
SELECT ?r
WHERE {h ?r ?t}
\end{lstlisting}
where the target is to get relations of a given entity $h$. 
With entity and relation sets known, combining these two types of queries, we could recover all triples in a knowledge graph. 


In PKGM, we build two query modules to simulate information queries of knowledge graphs in continuous vector space. One is Triple Query Module for triple queries, which maintains an assumption to transfer the head entity embedding and relation embedding to tail entity embedding in vector space, expressed as $f_{triple}(\textbf{h}, \textbf{r}) \approx \textbf{t}$. The other one is Relation Query Module for relation queries. It maintains an assumption to show whether an entity should have a relation or not. In vector space, we make zero vector $\mathbf{0}$ represent EXISTING, thus the function in Relation Query Module should be  $f_{rel}(\mathbf{h}, \mathbf{r}) \approx \mathbf{0}$ if exists. 

\subsection{Triple query module}
The purpose of the triple query module is to transfer the head entity and  relation to the tail entity and encode the truth value of a triple. Since a lot of knowledge graph embedding methods have been proposed for this task, we apply TransE \cite{TransE-Bordes-2013} for its simplicity and effectiveness. 

Each $e \in \mathcal{E}$ and $r \in \mathcal{R}$ is embedded as a vector.  Following the translation assumption, for each positive triple $(h, r, t)$, we make $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$, where $\mathbf{h}\in \mathbb{R}^d, \mathbf{r}\in \mathbb{R}^d$  and $\mathbf{t}\in \mathbb{R}^d$ are embedding of $h, r$ and $t$ respectively. Score function for $(h, r, t)$ is 
\begin{equation}
f_{triple}(h,r,t) = \|\mathbf{h} + \mathbf{r} - \mathbf{t}\|  
\label{score-triple}
\end{equation}
where $\| \mathbf{x} \|$ is the L1 norm of  $\mathbf{x}$. It should make $\mathbf{h} + \mathbf{r}$ approach $\mathbf{t}$ if $(h,r,t)$ is  positive and  far away otherwise. 


\subsection{Relation query module}
The purpose of relation query module is to encode whether an entity should have one relation or not. We represent the concept of  EXIST as $\mathbf{0}$ in vector space.  To encode the existence of relation $r$ for $h$, we design a function $f_{rel}(\mathbf{h}, \mathbf{t})$ and make $f_{rel}(\mathbf{h}, \mathbf{t}) \approx \mathbf{0}$ if $h$ (should) have  $r$ and keep $f_{rel}(\mathbf{h}, \mathbf{t})$ far away from $\mathbf{0}$ otherwise. 

We devise a transferring matrix for each relation $r \in \mathcal{R}$, denoted as $\mathbf{M}_r$. With $\mathbf{M}_r$ we could transfer $\mathbf{h}$ to $\mathbf{r}$. More specifically, the discrepancy between the transferred head entity representation and relation embedding should be close to $\mathbf{0}$ if $h$ has relation $r$ or should have. And the discrepancy should be far away from $\mathbf{0}$ if $h$ does not have or should not have relation $r$. Thus $f_{rel}$ is designed as follows
\begin{equation}
f_{rel}(h, r) = \| \mathbf{M}_r \mathbf{h} - \mathbf{r}\|
\label{score-rel}
\end{equation}
For a positive pair $(h,r)$, $f_{rel}(h,r)$ should be as small as possible, while be as large as possible for negative ones. 

\subsection{Loss function}
The final score of the given triple $(h,r, t)$ is 
\begin{equation}
    f(h,r,t) = f_{triple}(h,r,t) + f_{rel}(h,r)
\end{equation}
in which positive pairs should have a small score and negative pairs should have a large one. A margin-based loss is set as a training objective for PKGM
\begin{equation}
L = \sum_{(h,r,t) \in \mathcal{K}}  [f(h,r,t) + \gamma - f(h',r',t') ]_+
\label{loss}
\end{equation}
where $(h',r',t')$ is a negative triple generated for $(h,r,t)$ by randomly sample an entity $e \in \mathcal{E}$ to replace $h$ or $t$ , or randomly sample a relation $r' \in \mathcal{R}$ to replace $r$. $\gamma$ is a hyperparameter representing the margin which should be achieved between  positive sample scores and negative sample scores. And 
\begin{equation}
[x, y]_+=\left\{
\begin{aligned}
x &  \text{\; if \; } x \ge y \\
y & \text{\; if \; } x < y 
\end{aligned}
\right.
\end{equation}

\subsection{Service}
After training, PKGM has (1) good model parameters including entity embeddings, relation embeddings and transferring matrices, (2) functions in  triple query module and relation query module. Then it could provide service for triple queries and relation queries in vector space. 

\subsubsection{Service for triple queries}
PKGM provides the representation of candidate tail entity, when given a head entity $h$ and a relation $r$ as follows:
\begin{equation}
    S_{triple}(h,r) = \mathbf{h} + \mathbf{r} 
\end{equation}
with which $S_{triple}(h,r)$ will approximate to $\mathbf{t}$ if $(h,r,t) \in \mathcal{K}$ as a result of training objective (Equation(\ref{loss})), and the output of $S_{triple}(h,r)$ will be an entity representation that is most likely to be the right tail entity if there is no triple existing with $h$ as head entity and $r$ as relation in $\mathcal{K}$. This is the inherent completion capability of knowledge graph embedding methods, which has been widely proved and applied for knowledge graph completion\cite{DBLP:journals/corr/abs-2002-00388}. We summarize the function for pre-training and servicing in Table~\ref{tab:PKGM-functs}.

\input{tables/PKGM}


\subsubsection{Service for relation queries}
PKGM provides a vector representing whether an entity $h$ has or should have triples about relation $r$ as follows:
\begin{equation}
    S_{rel}(h, r) = \mathbf{M}_r \mathbf{h} - \mathbf{r} 
\end{equation}
There are three situations: (1) if $h$ links to entities according to $r$, then  $S_{rel}(h, r)$ will approximate to EXIST embedding $\mathbf{0}$, (2) if $h$ does not link to any entity according to $r$ but it should, then $S_{rel}(h, r)$ will also approximate to EXIST embedding $\mathbf{0}$, (3) if $h$ does not link to any entity according to $r$ and it should not, $S_{rel}(h, r)$ will be away from  $\mathbf{0}$. This is the relation completion capability of PKGM gaining from Equation (\ref{score-rel}). 

There are two significant advantages  of getting knowledge via PKGM's query services given $h$ and $r$: 
\begin{itemize}
\item We could access the tail entity in an implicit way via calculation in vector space without truly querying triples existing in the knowledge graph. This makes query service independent with data and ensures data protection. 
\item Results for each input pair are uniformed as two vectors from two query modules instead of triple data,  making it easier to apply them in downstream use models since designing models to encode triple data is avoided.  
\item We could get the inferred tail entity $t$ even triple $(h,r,t) \notin \mathcal{K}$, which greatly overcomes the incompleteness disadvantages of KG.   
\end{itemize}


\subsection{Applying service vectors in downstream use models}
With service vectors from PKGM, we introduce two general ways to apply them in embedding-based models for downstream use. We categorize embedding-based models into two classes according to the number of input embeddings for an entity in the model, one is inputting a sequence of  embeddings and the other one is inputting a single embedding. Given an item as target entity in the context of e-commerce, service vectors from triple query modules are denoted by $[S_1, S_2, ..., S_k]$ and those from relation query module denoted by $[S_{k+1}, S_{k+2}, ..., S_{2k}]$. 

The idea of integrating service vectors from PKGM into sequence models is shown in Fig.\ref{fig:PKGM+1}. 
\begin{figure}[!hbpt]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/PKGM+1-new.pdf}
    \caption{Application of PKGM for models with sequence embedding inputs.}
    \label{fig:PKGM+1}
\end{figure}
In these models, the sequential embedding for one $item_i$ are usually generated according to item side information, like word embedding of description text or labeled features. And these models are capable of automatically capturing interactions between embeddings in the sequence, thus we propose to append all service vectors for $item_i$ from PKGM at the end of the input sequence. The model will make service vectors interact with other input embeddings automatically. Supposing that the original input is $[E_1^i, E_2^i, ..., E_N^i]$, after appending service vectors, the input will be  $[E_1^i, E_2^i, ..., E_N^i, S_1, S_2, ..., S_{2k}]$, where we first append service vectors from triple query module and then append those from relation query module. 


The idea of integrating service vectors from PKGM into  models with single embedding input for an entity is shown in Fig.\ref{fig:PKGM+2}. In these models, the single embedding usually refers to the embedding of $item_i$ in current latent vector space and is learnt during training. To keep the informational balance for an entity between embeddings from the original model and our PKGM, we propose to firstly integrate all service vectors for $item_i$ from PKGM into one. During the integration, service vectors corresponding to the same relation from two modules should be considered together, thus we firstly make a concatenation with service vectors from triple query module and relation query module as follows:
\begin{equation}
    S_j^\prime = [S_j;S_{j + k}], \text{where}\; j \in [1, k])
\end{equation}
where $[x;y]$ means concatenating vector $x$ and $y$. Then we integrate them into one as follows:
\begin{equation}
    S = \frac{1}{k}\sum_{j \in [1, k]} S_j
\end{equation}

Then we concatenate $S$ with original item embedding $E_i$ as the single input. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/PKGM+2-new.pdf}
    \caption{Application of PKGM for models with single embedding input.}
    \label{fig:PKGM+2}
\end{figure}
