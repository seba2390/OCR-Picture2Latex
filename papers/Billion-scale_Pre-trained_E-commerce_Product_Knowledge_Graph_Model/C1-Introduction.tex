\section{Introduction}
Online shopping has greatly contributed to the convenience of people's lives. More and more retailers open online shops to attract increasing people favoring online-shopping, thus the e-commerce era witnessed rapid development in the past few years. The growing transaction on e-commerce platforms is based on billions of items of all kinds, which should be well organized to support the daily business. 

Knowledge Graphs (KGs) represent facts as triples, such as \emph{(iPhone, brandIs, Apple)}. Due to the convenience of fusing data from various sources and building semantic connections between entities, KGs have been widely applied in industry and many huge KGs have been built (e.g., Google's Knowledge Graph, Facebook's Social Graph, and Alibaba/Amazon's e-commerce Knowledge Graph). We built an e-commerce product knowledge graph and make it a uniform way to integrate massive information about items on our platform, which helps us build an integrated view of data. Notably, the e-commerce product knowledge graph contributes to a variety of tasks ranging from searching, question answering, recommendation, and business intelligence, etc. Currently, our product KG contains 70+ billion triples and 3+ million rules. 

Our product KG has been widely used for knowledge-enhanced e-commerce applications, such as item attributes prediction, same item identification, item category prediction, and item recommendation. These are key tasks for managing more complete and clean data and providing better item services for customers. In supervised paradigms, these knowledge-enhanced tasks rely heavily on data that are vital for them, for example, item title for same item identification and user-item interaction bipartite graph for recommendation. Apart from these data, information contained in the product knowledge graph about itemsâ€˜ attributes such as brands, series, and colors, are valuable side information to enhance these tasks. 

To serve these knowledge-enhanced tasks, we used to provide item information formed as triples in product KG to facilitate knowledge enhancement. While it not only results in tedious data selection working for us, mainly refers to neighbor sampling, but also leaves tedious knowledge enhancement model design for downstream use. More importantly, similar to other KGs, our product KG is still far from complete and the key missing information may bias those downstream tasks. 

Recently, a variety of knowledge graph embedding methods\cite{TransE-Bordes-2013} are proposed for completion, which learn representation for entities and relations in continuous vector spaces, called embeddings, and infer the truth value of a triple via embedding calculation. Embeddings usually encode global information of entities and relations and are widely used as general vector space representations for different tasks. Thus for services of knowledge-enhanced tasks, instead of providing triple data, we are wondering that \textbf{is it possible to make embeddings from product KG as a uniform knowledge provider for knowledge-enhanced tasks, which not only avoid tedious data selection and model design, but also overcome the incompleteness of product KG.}

Since the concept of `pre-training and fine-tuning' has proven to be very useful in the Natural Language Processing community \cite{BERT}, referring to pre-training a language model with a huge amount of language data and then fine-tune it on downstream tasks such as sentiment analysis \cite{ACL2020_Pretrain4SA}, relation extraction \cite{ACL2019_Pretrain4RE}, text classification \cite{EMNLP2019_Pretrain4TC} and so on. We also consider to pre-train our product KG and make it possible to conveniently and effectively serve downstream tasks requiring background knowledge about items. The convenience refers that downstream tasks do not need to revise their model much to adapt to background knowledge. The effectiveness refers that downstream tasks enhanced with knowledge could achieve better performance, especially with a small amount of data. 

The purpose of \textbf{P}re-trained \textbf{K}nowledge \textbf{G}raph \textbf{M}odel (\textbf{PKGM}) is to provide knowledge services in continuous vector space, through which downstream tasks could get necessary fact knowledge via embedding calculation without accessing triple data. The key information of product KG concerned by downstream tasks mainly includes two parts: (1) whether one item has a given relation(attribute), (2) what is the tail entity(property value) of given an item and relation(attribute). Thus making the incompleteness issue of knowledge graph also into consideration, a pre-trained knowledge graph model should be capable of: (1) showing whether an entity has a certain relation, (2) showing what is the tail entity for a given entity and relation, and (3) completing the missing tail entity for a given entity and relation if it should exist.

In this paper, we introduce a pre-trained e-commerce product knowledge graph model with two modules. One is Triple Query Module to encode the truth value of an input triple, which could serve for tail entity queries after training. Since a lot of knowledge graph embedding methods are proposed for triple encoding and link prediction, we apply the simple and effective TransE \cite{TransE-Bordes-2013} in the triple query module. It represents entities and relations as vectors and restricts the addition of head entity embedding and relation embedding should approach tail entity embedding if the input triple is true. The other one is Relation Query Module to encode the existence of a relation for an entity, which could serve for queries of relation existence after training. In this module, we take entity and relation embeddings from the triple query module and create an entity transformation matrix $\mathbf{M}_r$ for each relation $r$. With $\mathbf{M}_r$, we make the transformed head entity embedding $\mathbf{h}^\prime$ approaches to relation embedding $\mathbf{r}$ if $h$ owns relation $r$. 

After pre-training, the triple query module and the relation query module could provide knowledge service vectors for any target entity. Triple query module provides service vectors containing tail entity information of target entity and relation. For example, suppose the target entity as a smartphone, the triple query module will provide predicted tail entity embedding with target relations such as \emph{brandIs, seriesIs, memoryIs}, etc. , no matter if these triples exist in the knowledge graph or not. Thus advantages of the triple query module are that (1) it could provide knowledge service based on computation in vector space without accessing triples data, (2) it could complete the tail entity of target entity and relation during service. Relation query module provides service vectors containing existence information of different relations for target entities, in which the corresponding service vector will approach zero vector if the target entity (should) have the relation. Combining service vectors from triple query module and relation query module, information could be provided without executing symbolic queries by the pre-trained knowledge graph model are more than triples contained in the product knowledge graph. 
 
With service vectors, we propose general ways to incorporate them in embedding-based methods, called base models, for downstream tasks. We classify embedding-based methods into two types according to the number of input embeddings of target entities. The first type refers to methods with \emph{a sequence of embeddings} for target entities, and the second one refers to \emph{a single embedding} for target entities. We propose a general way to apply service vectors into each of the two types of methods. For sequence-embedding inputs, we extend the input sequence by appending service vectors after original inputs of target entities. And for single-embedding inputs, we condense service vectors into one and concatenate it with the original input for target entities. 

We pre-train the billion-scale product knowledge graph with PKGM model and apply service vectors to enhance three item related tasks, item classification, same product identification and item recommendation, following our proposed servicing and application methods. The experimental results show that PKGM successfully improves performance of these three tasks with a small amount of training data. 

In summary, contributions of this work are as follows: 
\begin{itemize}
    \item We propose a way to pre-train a knowledge graph, which could provide knowledge services in vector space to enhance knowledge-enhance downstream tasks in a general way. 
    \item Our proposed PKGM has two significant advantages, one is completion capability, and the other one is triple data independency.
    \item We practice PKGM on our billion-scale product knowledge graph and test it on three tasks, showing that PKGM successfully enhances them with item knowledge and improves their performance.
\end{itemize}
