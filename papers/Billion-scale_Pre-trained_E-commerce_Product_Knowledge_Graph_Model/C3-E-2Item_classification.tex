\subsection{Item Classification}
\subsubsection{Task definition}
The target of item classification is to assign an item to a class in the given class list. Item classification is a common task in our e-commerce platform since new tags and features are added frequently to items, such as lifestyle and crowd that items are suitable for. For different classification tasks, the candidate class varies.  For example, candidate classes for lifestyle classification include Vacation, Yoga, Hiking, and so on, and those for crowd classification include Programmer, Parents, Girlfriend, and so on. 

Usually, item titles are used for classification since most items in our platform have a title filled by managers of online shops. Thus we could frame it as a text classification task. Under supervised training, the target is to train a mapping function $f: \mathcal{T} \mapsto \mathcal{C}$ where $\mathcal{T}$ is a set of item titles and $\mathcal{C}$ is a set of classes. For each title  $t \in \mathcal{T}$, $t = [ w_1, w_2, w_3, ..., w_n ]$ is an ordered sequence composed of words. 

\subsubsection{Model}

\textbf{Base model}
Text classification is an important task in Natural Language Processing (NLP) and text mining, and many methods have been proposed for it. In recent years, the deep learning model has been shown to outperform traditional classification methods\cite{DBLP:conf/acl/HenaoLCSSWWMZ18, BERT}. Given the input document, the mapping function $f$ firstly learns a dense representation of it via representation learning and then uses this representation to perform final classification. Recently, large-scale pre-trained language models, such as ELMo\cite{ELMo}, GPT\cite{GPT} and BERT\cite{BERT}, have become the \emph{de facto} first representation learning step for many natural language processing tasks. Thus we apply BERT as the base model for item classification. 

BERT is a pre-trained language model with multi-layers of bidirectional encoder representations from Transformers, trained with unlabeled text by jointly conditioning on both left and right context in all layers. BERT is trained on a huge amount of texts with masked language model objective. Since the use of BERT has become common and we conduct item classification experiments with released pre-trained BERT by Google. We will omit an exhaustive background description of the model architecture and refer the readers to \cite{BERT} as well as the excellent guides and codes\footnote{https://github.com/google-research/bert} of fine-tuning BERT on downstream tasks. 

\begin{figure*}[!htbp]
     \centering
     \subfigure[BERT.]{
         \centering
         \includegraphics[width=0.435\textwidth]{figures/bert-classification-1.pdf}
         \label{fig:cate-base}
     }
     \subfigure[BERT$_{PKGM}$.]{
         \centering
         \includegraphics[width=0.45\textwidth]{figures/bert-classification+PKGM-2.pdf}
         \label{fig:cate-base+PKGM}
     }
    \caption{Models for item classification.}
    \label{fig:model-classification}
\end{figure*}

We show details of applying BERT on item classification task in Figure~\ref{fig:model-classification}. We input the text sequence of title into BERT and take out the representation corresponds to [CLS] symbol $C$. To predict the class of items, we input the [CLS] representation into a fully connected layer as follows:
\begin{equation}
    p = \sigma(\mathbf{W}C + \mathbf{b})
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{d \times n_c}$ is a weighed matrix in the shape of ${d \times n_c}$. $d$ is the word embedding dimension, also called hidden size, of BERT and $n_c$ is the number of classes in current task. $\mathbf{b}\in \mathbb{R}^{n_c}$ is a bias vector and $\sigma()$ is an activate function. 

With predicted $p$ and input label $l \in \mathbb{R} ^{n_c}$, we fine-tune the model with a cross-entropy loss. We assign an unique identity to each class beginning with $0$. For an item belonging to the $i$th class, $l[i] =1$ and others are $0$. 

\textbf{Base+PKGM model}
To enhance item classification with our PKGM, for each item, we provide service with $k$ presentations from both triple query module and relation query module, $2\times k$ in total. We first add a [SEP] symbol at the end of the title text and then input the $2\times k$ service vectors from PKGM into BERT, and we named it as \emph{Base$_{PKGM-all}$}. Embedding look up is unnecessary for service vectors and they are directly appended after the last token embedding of the input sentence.  Details are shown in Figure~\ref{fig:model-classification}.

We also fine-tune the Base+PKGM model with only $k$ triple query module representations to explore how much of a role the \textit{triple query service} plays in the model. The final $k$ title texts are replaced with service vectors from triple query module, and we abbreviate this model as \emph{Base$_{PKGM-T}$}. In the same way, \emph{Base$_{PKGM-R}$} refers to replacing final $k$ title texts with $k$ service vectors from relation query module.

\subsubsection{Dataset}
In this experiment, we take item categories as target classes. In our platform, each item is assigned to at least one category, and we maintain an item category tree to better organize billions of items and help recall items for a lot of business, like selecting items for the online market in a special event like Double Eleven. Thus we always try to assign at least one category for each item. 

Experiment dataset contains 1293 categories. Details are shown in Table~\ref{data:classification}. The number of positive samples and negative samples are $1:1$. To show the power of pre-trained models, both language model and knowledge graph model, with which we could get good performance on downstream tasks with a few training samples, we constrain the instance of each category less than 100 during data preparation.

\input{tables/item-classification-data}

\subsubsection{Experiment details}
We take the pre-trained BERT$_{BASE}$ model trained on Chinese simplified and traditional corpus\footnote{Released at https://storage.googleapis.com/bert\_models/2018\_11\_03/chinese\_L-12\_H-768\_A-12.zip}, whose number of layers is $12$, hidden size is $768$ and the number of attention heads is $12$. 

Following BERT, we add a special classification token [CLS] ahead of the input sequence, which is used as the aggregate sequence representation in the final hidden state. In this experiment, we set the length of the sequence of titles to 128. For titles shorter than $127$ (with [CLS] symbol excepted), we padding it with several zero embeddings to the required length as did in BERT, while for titles longer than $127$, we reserve the first $127$ words.

For BERT$_{PKGM-all}$ model, we replace the last $2\times k$ embeddings with service vectors from PKGM,  and for  BERT$_{PKGM-T}$ model and BERT$_{PKGM-R}$, we replace the last $k$ embeddings with service vectors from triple query module and relation query module respectively.

We fine-tune BERT with $3$ epochs with batch size as $32$ and learning rate as $2e^{-5}$. All parameters in BERT are unfix and representations from PKGM fixed during fine-tune.

\begin{figure*}[!htbp]
     \centering
     \subfigure[BERT.]{
         \centering
         \includegraphics[height=5.4cm]{figures/bert-same-new.pdf}
         \label{fig:same-base}
     }
     \subfigure[BERT$_{PKGM}$.]{
         \centering
         \includegraphics[height=5.4cm]{figures/bert-same+PKGM-2.pdf}
         \label{fig:same-base+PKGM}
     }
    \caption{Models for item alignment.}
    \label{fig:model-same}
\end{figure*}

To evaluate performances, we report the accuracy of item classification in TABLE~\ref{res:classification}. We abbreviate the accuracy as AC and it is reported as the percentage from the validation dataset. Because the amount of categories is too large compared with other traditional classification tasks, we also report metrics $Hit@k(k=1,3,10)$ to describe the validity of the experimental results more accurately. We calculate $Hit@k$ by getting the rank of the correct label as its predicting category rank and $Hit@k$ is the percentage of test samples with prediction rank within $k$. 

\subsubsection{Results}
\input{tables/item-clasification-results}
The results are shown in Table~\ref{res:classification}, showing that BERT$_{PKGM}$ outperforms BERT on all of these three datasets, on both $Hit@k$ and prediction accuracy metrics. Specifically, BERT$_{PKGM-all}$ has the best performance of $Hit@1, Hit@3$ and $Hit@10$ and BERT$_{PKGM-R}$ achieves best accuracy. 
% We also observe that results of BERT$_{PKGM-all}$ and BERT$_{PKGM-R}$ perform greatly on $Hit@3$, $Hit@10$, and one of them always can reach the best experimental results. 
These results demonstrate the effectiveness of information from PKGM, among which relation query service vectors contributes more useful information than triple query service vectors.

Besides, BERT$_{PKGM-R}$ sometimes outperforms BERT$_{PKGM-all}$, which violates the idea that more features matter. We think it's probably because words in the original title play a more important role in the item classification task than service vectors provided from PKGM since less words are replaced in BERT$_{PKGM-R}$ than BERT$_{PKGM-all}$. 