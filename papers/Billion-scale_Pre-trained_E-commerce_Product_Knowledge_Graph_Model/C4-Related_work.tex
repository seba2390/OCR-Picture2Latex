\section{Related work}

\subsection{Knowledge Graph Embedding}
With the introduction of embedding entities and relations of the knowledge graph into continuous vector space by TransE~\cite{TransE-Bordes-2013}, the method of Knowledge Graph Embedding (KGE) has made great progress in knowledge graph completion and application. The key idea that transforming entities and relations in triple (\textit{h, r, t}) into continuous vector space aims to simplify the operation in KG. TransH \cite{TransH-Wang-2014} proposes that an entity has different representations under different relationships to overcome the limitations within multi-relation problems. TransR \cite{TransR-Lin-2015} introduces relation-specific spaces other than entity embedding spaces. TransD \cite{TransD-Ji-2015} decomposes the projection matrix into a product of two vectors. TranSparse \cite{TranSparse-Ji-2016} simplifies TransR by enforcing sparseness on the projection matrix. MuRP\cite{DBLP:conf/nips/BalazevicAH19} and ATTH\cite{DBLP:conf/acl/ChamiWJSRR20} lead hyperbolic geometry into embedding learning of knowledge graph to capture logical hierarchies and patterns. All of these models try to achieve better effects on multi-relation problems.

Different from the translational distance models mentioned above, the semantic matching model uses a similarity based scoring function. They measure the credibility of facts by matching the latent semantics of entities and the relationships, methods include RESCAL \cite{RESCAL-Nickel-2011}, DistMult \cite{DistMult-Yang-2015}, ComplEx \cite{ComplEx-Trouillon-2016} and ConvE \cite{ConvE-Dettmers-2018}, etc.

The knowledge graph embedding method learns representations for entities and relations that preserve the information contained in the graph. The method has been applied to various tasks and applications and achieved good results in the field of the knowledge graph, including link prediction\cite{CrossE}, rule learning\cite{IterE}, relation extraction\cite{LFDS}, and entity alignment\cite{sun2018bootstrapping}. We adopt the representation learning method in the  billion-scale-commerce product knowledge graph to calculate and to evaluate the relationship among items.

\subsection{Pre-train Language Models.}
The pre-trained language models have achieved excellent results in many NLP tasks and bring convenience in completing various specific downstream tasks, such as BERT \cite{BERT}, XLNet\cite{XLNet-Yang-2019}, and GPT-3\cite{gpt3_brown2020language}. Recently, more and more researchers pay attention to the combination of knowledge graph and pre-trained language model, to enable PLMs to reach better performance with the awareness of knowledge information. 

K-BERT \cite{K_BERT-Liu-2019} injects knowledge triples into a sentence to generate unified knowledge-rich language representation. 
ERNIE \cite{ERNIE-Zhang-2019} integrates entity representations from the knowledge module into the semantic module to represent heterogeneous information of tokens and entities into a united feature space. 
KEPLER \cite{KEPLER-Wang-2019} encodes textual descriptions for entities as text embeddings and treats the description embeddings as entity embeddings.
KnowBert \cite{KnowBert-Peters-2019} uses an integrated entity linker to generate knowledge enhanced entity-span representations via a form of word-to-entity attention.
K-Adapter \cite{wang2020K_Adapter} takes RoBERTa as the pre-trained model and injects factual knowledge and linguistic knowledge with a neural adapter for each kind of infused knowledge. 
DKPLM \cite{su2020DKPLM} could dynamically select and embed knowledge context according to the textual context for PLMs, with the awareness of both global and local KG information.
JAKET \cite{yu2020JAKET} proposes a joint pre-training framework, including the knowledge module to produce embeddings for entities in text and language modules to generate context-aware embeddings in the graph. What's more, KALM \cite{rosset2020KALM}, ProQA \cite{xiong2020progressively}, LIBERT\cite{lauscher2019informing} and other researchers explore the fusion experiment with knowledge graph and pre-trained language module in different application tasks.

Based on BERT or other pre-trained language models, these methods aim at adjusting the language model by injecting knowledge bases or completing knowledge graphs from textual representation. Whereas, we pre-train our model on the structural and topological information of a knowledge graph without language model and apply the model into several downstream tasks in e-commerce knowledge.

