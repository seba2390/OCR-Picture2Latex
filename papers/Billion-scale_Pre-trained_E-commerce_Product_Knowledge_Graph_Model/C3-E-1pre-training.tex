\subsection{Pre-training}

\subsubsection{Dataset}
We pre-train PKGM on a sub dataset of our billion-scale E-commerce product knowledge graph (PKG) called PKG-sub. PKG has a total number of 1.2B attribute records for 0.2B items, where all the attributes are filled by sellers in Taobao. We process the data into triplet format using Alibaba Maxcompute, a max-reduce framework. We remove the attributes with occurrences less than 5000 in PKG, since these attributes are likely noisy that not only increase largely the model size but also deteriorate the model performance. The statistic details of PKG-sub are shown in Table~\ref{tab:dataset-pre-train}.   
\input{tables/kg-dataset}

For each item $item_i$ in the dataset, we select $10$ key relations for it according to its category. More specifically, suppose $item_i$ belongs to category $C$, we gather all items belonging to $C$ and account for the frequency of properties in those items, then select top $10$ most frequent properties as key relations. After pre-training, our PKGM will provide service vectors for target items with regard to those key relations. 

\subsubsection{Training details}
Our PKGM is implemented with Tensorflow\cite{tensorflow} and Graph-learn\footnote{https://github.com/alibaba/graph-learn}. Graph-learn~\cite{zhu2019aligraph} is a large-scale distributed framework for node and edge sampling in graph neural networks. We use Graph-learn to perform edge sampling, and 1 negative sample is generated for each edge.  For model setting, Adam\cite{adam} optimizer is employed with initial learning rate as 0.0001, training batch size as 1000, and embedding dimension as 64. Finally the model size is 88GB, and we train it with 50 parameter servers and 200 workers for 2 epochs. The whole training process consumed 15 hours.