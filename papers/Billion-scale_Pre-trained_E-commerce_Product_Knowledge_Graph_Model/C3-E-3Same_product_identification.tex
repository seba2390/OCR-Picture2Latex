\subsection{Product Alignment}

\subsubsection{Task definition}
The target of the item alignment task is to find two items referring to the same product. For example, there are many online shops selling IPhoneXI with Green color and 256 GB capacity. They are different items on the platform while from the perspective of the product, they refer to the same one. Detecting items belonging to the same product contributes to daily business greatly, for example, recommending aligned items of items that a user is reviewing, which  help users deeply compare the prices, services after selling, and so on. More importantly, the number of items are much larger than the number of products, organizing items from the perspective of the product helps reduce the load of data management and mining.

Similar to item classification, item titles are used for alignment. And item alignment task could be framed as a paraphrase identification task, to distinguish whether two input sentences express the same meaning\cite{PI}. In the context of the item alignment, the target is to distinguish whether two input item titles describe the same product. The prevailing approach of training and evaluating paraphrase identification models are constructed as a binary classification problem: the model is given a pair of sentences and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. Thus a mapping function $f: \mathcal{P_T} \mapsto \mathcal{C}$ will be learned, in which $\mathcal{P_T}$ is a set of item title pairs and   $\mathcal{T}$ is a set of item title. $\mathcal{C}$ contains binary classes $True$ and $False$. Similar to item classification, for each item title $t\in \mathcal{T}$, $t = [ w_1, w_2, w_3, ..., w_n ]$ is an ordered sequence composed of words.


\subsubsection{Model}

\textbf{Base model}
Paraphrase identification is a well-studied sentence pair modeling task and is very useful for many NLP applications such as machine translation(MT)\cite{vaswani2018tensor2tensor}, question answering(QA)\cite{yih2015semantic} and information retrieval(IR)\cite{berger2017information}. Many methods have been proposed for it in recent years including pairwise word interaction modeling with deep neural network system~\cite{DBLP:conf/naacl/HeL16}, character level neural network model~\cite{DBLP:conf/naacl/LanX18}, and pre-trained language model~\cite{BERT}. Since the pre-trained language models have reached state-of-the-art performance on paraphrase identification tasks, we apply BERT as the base model for the same alignment. 

We show the details of applying BERT on the item alignment in Figure~\ref{fig:model-same}. The title of two items are input into BERT as a sentence pair and we take the representation corresponding to [CLS] symbol as the representation of the sentence pair which will be used for binary classification with a fully connected layer, as done in item classification task. 

\textbf{Base+PKGM}
For each item, we provide service with $2\times k$ vectors from the triple query module and relation query module of PKGM to enhance the item alignment task with item knowledge. Similar to the operation in item classification, we add a [SEP] symbol at the end of each title text and  $4\times k$ service vectors are added to BERT finally. After that, we concatenate two-sentence input together. Details are shown in Figure~\ref{fig:model-same}. Similar to item classification, with all service vectors applied, we call the model as \emph{Base$_{PKGM-all}$}. \emph{Base$_{PKGM-T}$} and \emph{Base$_{PKGM-R}$} are defined in the same way which refer to input the $k$ service vectors from triple query module and relation query module in PKGM respectively. In this task, \emph{Base} refers to BERT.

\subsubsection{Dataset}
We experiment on $3$ datasets containing different types of items. In our platform, item alignment is only necessary to distinguish items with the same type since items with different types refer to different products for sure. The first dataset is generated from skirts for girls, the second one from hair decorations and the last one from children's socks.  In each dataset, one data sample includes two item titles and a label, in which samples belonging to the same products are labeled with $1$ and $0$ otherwise. There are less than 10 thousand training samples in each dataset and we keep the number of item pairs in train/test/dev dataset as $7:1.5:1.5$. Details of these datasets are shown in Table~\ref{tab:data-same-item}.


\input{tables/same-item-detection-data}

\subsubsection{Experiment details}
Consistent with the previous item classification task, we take the same pre-trained BERT$_{BASE}$ model and the same input format, except for some difference in the input data. The input embedding sequence consists of two different items. A [CLS] symbol is added in the beginning of the title sequence  and one [SEP] symbol is appended at the last position of each title sequence to differential these two items. Since the whole sequence length is 128, the length of each item sequence is restricted within 63 and we adopt the same method to formalize the item sequence with item classification task.

% evaluation metrics
We report prediction accuracy and metrics $Hit@k(k=1,3,10)$ to evaluate the performance on item alignment. For metric $Hit@k$, we generate $n$ negative triples by replacing one of the aligned items randomly and together with the original aligned pairs, we will get $n+1$ prediction results. Afterward, we rank $n+1$ prediction probabilities in ascend order and get the rank of the aligned pairs as its prediction rank. $Hit@k$ is the percentage of test samples with prediction rank within $k$. In this case, the number of negative triples for each aligned pair is 99 and in other words, we get prediction rank from 100 candidate samples.

\subsubsection{Results}

Table~\ref{tab:res-same-item-R} shows the rank results for item alignment. BERT$_{PKGM-all}$ model outperforms BERT model on $Hit@10$ metrics on all three datasets and it has the best performance at both category-2 and category-3 with all $Hit@k$ metrics. These results show the effectiveness of PKGM and it promotes the accuracy of the prediction better. In the meantime, BERT model has a weak advantage over BERT$_{PKGM-all}$ model at the category-1 on $Hit@1$ metrics and this could probably be due to the larger amount of data in this category. To a certain extent, the title text with enough training examples could make the model learn item information better, while the PKGM could play a greater role in the case of a small amount of data relatively.

Table~\ref{tab:res-same-item-C} shows the accuracy results. Obviously, BERT$_{PKGM-all}$ has the best performance on all datasets and it convincingly demonstrates the PKGM could promote the effect on item alignment task.


\input{tables/same-item-detection-results}
