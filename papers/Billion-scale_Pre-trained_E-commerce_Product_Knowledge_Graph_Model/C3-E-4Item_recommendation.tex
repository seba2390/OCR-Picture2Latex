\subsection{Item Recommendation}
\begin{figure*}[!htbp]
     \centering
     \subfigure[NCF.]{
         \centering
         \includegraphics[height=6.7cm]{figures/method-recommendation-new.pdf}
         \label{fig:recommendation-base}
     }
     \subfigure[NCF$_{PKGM}$.]{
         \centering
         \includegraphics[height=6.7cm]{figures/method-recommendation-PKGM-new.png}
         \label{fig:recommendation-base+PKGM}
     }
    \caption{Models for item recommendation.}
    \label{fig:model-recommendation}
\end{figure*}
\subsubsection{Task definition}
Item recommendation aims to properly recommend items to users that users will interact with a high probability. As implicit feedback (click, buy, etc) is more commonly observed in modern e-commerce platforms, we consider only implicit feedback for recommending items to corresponding users with the help of service vectors provided by PKGM. To learn user preference and make proper recommendations from observed implicit user feedback, we treat it as a ranking problem like described in \cite{DBLP:conf/www/HeLZNHC17}.



\subsubsection{Model}


\textbf{Base model}
We make use of NCF\cite{DBLP:conf/www/HeLZNHC17} as a general framework of our base model. In NCF, GMF layer and MLP layer are used for modeling item-user interactions, in which GMF uses a linear kernel to model the latent feature interactions, and MLP uses a non-linear kernel to learn the interaction function from data. There are 4 layers in NCF including input layer, embedding layer, neural CF layer, and output layer. 

The input layer contains user and item sparse feature vectors $\mathbf{v}_u^U$ and $\mathbf{v}_i^I$ that describe user $u$ and item $i$ respectively. Since we conduct the downstream recommendation experiment based on a pure collaborative setting, we make $\mathbf{v}_u^U$ and $\mathbf{v}_i^I$ one-hot sparse vector generated according to the identity of  $u$ and $i$ in the dataset.

Above the input layer, there is an embedding layer which is a fully connected layer to project the sparse input representation to a dense vector, called embedding.  User embedding and item embedding could be regarded as the latent vector for users and items in the context of latent factor models. 
\begin{equation}
    \mathbf{p}_u = \mathbf{P}^T\mathbf{v}_u^{U}, \; \mathbf{q}_i = \mathbf{Q}^T\mathbf{v}_i^I
\end{equation}

Then user embedding $\mathbf{p}_u$ and item embedding $\mathbf{q}_i$ are fed into a neural collaborative filtering layer, to map the latent vectors to prediction scores. Thus the NCF predictive model could be formulated as 
\begin{equation}
\hat{y}_{ui} = f_{NCF}(\mathbf{p}_u, \mathbf{q}_i| \mathbf{P}, \mathbf{Q}, \Theta_f),
\end{equation}
where $\mathbf{P} \in \mathbb{R}^{M \times K}$ and $\mathbf{Q} \in \mathbb{R}^{N \times K}$ are embedding matrix for users and items respectively. And $\Theta_f$ is a set of parameters in function $f_{NCF}$. $f_{NCF}$ is a user-item interaction function that makes $\mathbf{p}_u$ and $\mathbf{q}_i$ interact and influence with each other and finally gives a score for the input item-user pair. And there are two ways to make such interactions, one is matrix factorization (MF) and the other one is multi-layer perceptron (MLP). 

MF is the most popular model for recommendation and has been investigated extensively in literature\cite{MF}. The mapping function in  neural generalized matrix factorization (GMF) layer is 
\begin{equation}
    \phi^{GMF}(\mathbf{p}_u, \mathbf{q}_i) = \mathbf{p}_u \circ \mathbf{q}_i ,
\end{equation}
where $\circ$ denotes the element-wise product of vectors. 

Using two pathways to model users and items has been widely adopted in multimodal deep learning models~\cite{RS1,RS2}. Thus in the MLP interaction layer, we first make a vector concatenation of $\mathbf{p}_u$ and $\mathbf{q}_i$
\begin{equation}
    \mathbf{z}_1 = \phi_1^{MLP}(\mathbf{p}_u, \mathbf{q}_i) = {
\left[ \begin{array}{ccc}
\mathbf{p}_u \\
\mathbf{q}_i
\end{array} 
\right ]}
\end{equation}
then we add hidden layers on the concatenated vector, using a standard MLP to learn the interaction between user and item latent features. This endows the model a large level of flexibility and non-linearity to learn the interactions between $\mathbf{p}_u$ and $\mathbf{q}_i$
% \begin{equation}
\begin{align}
    \phi_2^{MLP}(\mathbf{z}_1) & = a_2 (\mathbf{W}_2 ^T \mathbf{z}_1 + \mathbf{b}_2) \\
    & ...... \\
    \phi_L^{MLP}(\mathbf{z}_{L-1}) & = a_{L} (\mathbf{W}_L ^T \mathbf{z}_{L-1} + \mathbf{b}_L) 
\end{align}
% \end{equation}
where $\mathbf{W}_x$, $\mathbf{b}_x$ and $a_x$ denote the weight matrix, bias vector, and activation function for the $x$-th layer's perceptron, respectively.

\input{tables/recommendation-results}

So far we have two interaction vectors for the input user and item embeddings, then we fuse these two vectors as follows:
\begin{equation}
\hat{y}_{ui} = \sigma(\mathbf{h}^T {
\left[ \begin{array}{ccc}
\phi^{GMF} \\
\phi^{MLP} 
\end{array} 
\right ]})
\end{equation}
The objective function to minimize in NCF method is as follows
\begin{equation}\label{equation-19}
    L = - \sum_{(u,i) \in \mathcal{Y}} y_{ui} log(\hat{y}_{ui}) + (1-y_{ui})log(1-\hat{y}_{ui})
\end{equation}
where $y_{ui}$ is the label for input user $u$ and item $i$, and if they have interaction in the dataset $y_{ui}=1$ and $y_{ui}=0$ otherwise. $\mathcal{Y}$ is the training set containing positive pairs and negative pairs. Since the original dataset only contains positive pairs, we take the same strategy used in \cite{DBLP:conf/www/HeLZNHC17} to generate negative pairs. Negative pairs are uniformly sampled from unobserved interactions for each positive pair, and replace the positive item with negative items that are unobserved. We refer negative sampling ratio as the number of negative pairs for each positive pair.

\textbf{Base+PKGM}
Since NCF is built based on item embeddings, we integrate  vector services from PKGM with item embeddings in the second way as introduced before. Specifically, for each user-item pair, PKGM with provide $2\times k$ vectors for the item, denoted as $[S_1, S_2, ..., S_{2k}]$. We first combine these vectors into one as follows
\begin{equation}
    S_{PKGM} = \frac{1}{k} \sum_{i \in [1, 2, ..., k]} [S_i;S_{i + k}]
\end{equation}
then we integrate $S_{PKGM}$ into MLP layer by making $\mathbf{z}_1$ as a concatenation results of three vectors
\begin{equation}
    \mathbf{z}_1 = \phi _1 ^{MLP} (\mathbf{p}_u, \mathbf{q}_i, S_{PKGM}) = {
\left[ \begin{array}{ccc}
\mathbf{p}_u \\
\mathbf{q}_i \\
S_{PKGM} 
\end{array} 
\right ]}.
\end{equation}
Other parts of NCF stay the same. Fig.~\ref{fig:model-recommendation} shows the general framework of NCF and NCF$_{PKGM}$. 

\subsubsection{Dataset}
We conduct experiments on real data sampled from records on Taobao platform. The statistics of our sample data are shown in TABLE \ref{tab:taobao-recommendation-statistics}. There are 29015 users and 37847 items with 443425 interactions between them. The number of interactions for each user is assured of being more than 10. Each interaction record is a user-item pair representing that the user has an interaction with the item.
\input{tables/recommendation-data}

\subsubsection{Experiment details}
% evaluation protocols and so on 
The performance of item recommendation is evaluated by the \textit{leave-one-out} strategy which is used in \cite{DBLP:conf/www/HeLZNHC17}. For each user, we hold-out the latest interaction as the test set and others are used in the train set. As for the testing procedure, we uniformly sample 100 unobserved negative items, ranking the positive test item with negative ones together. In this way, $Hit\,Ratio$(HR)@k and $Normalized\ Discounted\ Cumulative\ Gain $(NDCG)@k are used as the final metrics of ranking where $k \in \{1,3,5,10,30\}$. Both metrics are computed for each test user, and average scores are regarded as final results.

% parameter settings
We randomly sample one interaction for each user and it is treated as a validation sample to determine the best hyper-parameters of the model. For the GMF layer part, the dimension of user embedding and item embedding are both set to $8$. As for MLP layers, the dimension of user embedding and item embedding are both set to $32$. Three hidden layers of size $[32, 16, 8]$ after embedding layers are used for both NCF and NCF$_{PKGM}$. For NCF$_{PKGM}$, there are external PKGM features fed to concatenate with MLP user embedding and MLP item embedding. The model is optimized with loss defined in Equation~\eqref{equation-19} with an external $L2$ regularization on user and item embedding layer in MLP and GMF. Regularization factor $\lambda$ is chosen as 0.001. Mini-batch Adam\cite{adam} is used for model training with a batch size of $256$ and the total number of epochs is $100$. Learning rate is set as $0.0001$. And the dimension of the prediction layer is $16$, which consists of two partial outputs of $8$ and $8$ from GMF layer and MLP layer respectively. In this work, we use a negative sampling ratio of $4$. For simplicity and effectiveness, we use the non-pre-train version of NCF model for base model and NCF$_{PKGM}$ as PKGM featured model.
\subsubsection{Results}

Experiment results are shown in TABLE~\ref{tab:res-recommendation}. 
% NCF with PKGM-* subscript indicates whether the augmented feature is produced by PKGM-triple query module only, PKGM-relation query module only, or both module. 
From the result we can see that:

Firstly, all of the PKGM feature augmented models outperform the NCF baseline in all metrics. For NCF$_{PKGM-T}$ model, it outperforms the NCF baseline with an average promotion of $0.37\%$ on Hit Ratio metrics and $0.0023$ on NDCG metrics. For NCF$_{PKGM-R}$ model, it outperforms the NCF baseline with an average promotion of $3.66\%$ on Hit Ratio metrics and $0.0343$ on NDCG metrics.
For NCF$_{PKGM-all}$ model, it outperforms the NCF baseline with an average promotion of $3.47\%$ on Hit Ratio metrics and $0.0324$ on NDCG metrics. This promotion illustrates that features learned by our PKGM model provide external information that are not included in original user-item interaction data and it is useful for downstream tasks like e-commerce recommendation.

Secondly, the key information of different service vectors provided by PKGM model are different. Performances on NCF$_{PKGM-R}$ are better than those on NCF$_{PKGM-T}$. Thus the information from relation query module is relatively more useful than the information from PKGM-triple query module in item recommendation scenario, which is largely due to the fact that properties are more effective than entities and values when modeling user-item interaction.