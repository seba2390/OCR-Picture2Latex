\documentclass[
 reprint,
 amsmath,amssymb,
 aps,
]{revtex4-2}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{black}{rgb}{0,0,0}
\definecolor{myblue}{rgb}{0,0,0.6}
\definecolor{mygreen}{rgb}{0,0.5,0}
\newcommand{\dgedit}[1]{\textcolor{mygreen}{#1}}
\newcommand{\rgedit}[1]{\textcolor{blue}{#1}}

\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage{bbold}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{graphicx}

% line numbering
\usepackage{lineno}
\linenumbers
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}%
     {\linenomath\csname old#1\endcsname}%
     {\csname oldend#1\endcsname\endlinenomath}}% 
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
  \patchAmsMathEnvironmentForLineno{#1}%
  \patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
\patchBothAmsMathEnvironmentsForLineno{equation}%
\patchBothAmsMathEnvironmentsForLineno{align}%
\patchBothAmsMathEnvironmentsForLineno{flalign}%
\patchBothAmsMathEnvironmentsForLineno{alignat}%
\patchBothAmsMathEnvironmentsForLineno{gather}%
\patchBothAmsMathEnvironmentsForLineno{multline}%
}

\DeclareMathOperator*{\argmin}{arg\,min}

\newif\ifcolor
\colortrue

\begin{document}

\title{Learning fluid dynamics using sparse physics-informed discovery of empirical relations}
\author{Daniel R. Gurevich*}
%Names and affiliations of all co-authors. The primary affiliation for each author should be the institution where the majority of their work was done. If an author has subsequently moved, the current address may also be stated. The corresponding author should be identified with an asterisk. 
% (Corresponding author needs ORCID - 0000-0002-3659-407X)
\affiliation{Program in Applied and Computational Mathematics, Princeton University, Princeton, NJ 08544, USA \\ \texttt{dgurevich@princeton.edu}}
\author{Patrick A. K. Reinbold}
\affiliation{School of Physics, Georgia Institute of Technology, Atlanta, GA 30332, USA}
\author{Roman O. Grigoriev}
\affiliation{School of Physics, Georgia Institute of Technology, Atlanta, GA 30332, USA}
\date{\today}

\begin{abstract}
%We show how a complete mathematical description of complex physical phenomena can be constructed from observational data by incorporating general physical constraints into a weak formulation of sparse regression. To illustrate this, we extract the governing equations describing an incompressible Newtonian fluid -- the Navier-Stokes equation, the continuity equation, and the boundary conditions -- from numerical data representing a highly turbulent channel flow. Our approach is robust to a wide range of noise levels characteristic of experimental data.
% 148 words
We show how a complete mathematical description of a complicated physical phenomenon can be learned from observational data via a hybrid approach combining three simple and general ingredients: physical assumptions of smoothness, locality, and symmetry, a weak formulation of differential equations, and sparse regression. To illustrate this, we extract a complete system of governing equations describing flows of incompressible Newtonian fluids -- the Navier-Stokes equation, the continuity equation, and the boundary conditions -- from numerical data describing a highly turbulent channel flow in three dimensions. These relations have the familiar form of partial differential equations, which are easily interpretable and readily provide information about the relative importance of different physical effects as well as insight into the quality of the data, serving as a useful diagnostic tool. The approach described here is remarkably robust, yielding accurate results for very high noise levels, and should thus be well-suited to experimental data.
\end{abstract}

\keywords{data-driven discovery, machine learning, sparse regression, partial differential equations}

\maketitle

%Nature Machine Intelligence will publish primary research in one format, Article, which can range in length from short communications to longer, more in-depth reports. Regardless of length, an Article should report an important research study of high quality and general interest to the broad machine intelligence community. The main text (excluding abstract, Methods section, references and figure legends) is typically limited to 3,500 words. Articles can have up to 6 display items (figures and/or tables) and as a guideline up to around 50 references (excluding those cited exclusively in Methods). Articles have an unreferenced abstract of approximately 150 words. The abstract should contain a brief account of the background and rationale of the work, followed by a statement of the main conclusions, and their implications, introduced by the phrase 'Here we show' or some equivalent. The main text of an Article should begin with an introduction (without heading) of referenced text that expands on the background of the work (some overlap with the abstract is acceptable), and is followed by a concise, focused account of the findings, typically ending with one or two short paragraphs of discussion. The main text may be divided by a few short topical headings of no more than 60 characters (including spaces). Articles can also contain a Methods section, which should appear after the main text and should typically not exceed 3,000 words.

%Please submit tables at the end of your text document (in Word or TeX/LaTeX, as appropriate). Tables that include statistical analysis of data should describe their standards of error analysis and ranges in a table legend.

% Formatting requirements etc. at https://www.nature.com/natmachintell/for-authors/preparing-your-submission

Physical theories are traditionally constructed in an iterative manner. At each step, discrepancies between predictions and existing experimental observations are used to improve the theory, making it more general and accurate. These improvements are usually instructed and constrained by first principles, including both general and domain knowledge. After this, new predictions are made and new experiments are designed to test these predictions, closing the loop. Humans play a key role in all aspects of this traditional procedure and can become a weak link when the amount of data becomes overwhelming or the patterns in the data are too complex. 

Recent advances in machine learning have started to change the scientific paradigm guiding the construction of physical theories by gradually taking humans out of the loop. For low-dimensional systems, physical relations in the form of algebraic and even differential equations can be constructed using symbolic regression directly from experimental data without using any physical intuition \cite{crutchfield1987,bongard2007,schmidt2009}. For high-dimensional systems, purely data-driven approaches become intractable, and some physical intuition becomes necessary to guide the process \cite{karpatne2017}. The question is therefore what physical considerations can and should be used to constrain the problem sufficiently for the data-driven analysis to become tractable while leaving enough freedom to enable identification of physically meaningful relationships.

For instance, an equation describing the evolution of momentum for a weakly turbulent flow in a thin layer of fluid could be identified from either synthetic  \cite{reinbold2020} or experimental data \cite{reinbold2021} after the form of the equation was constrained by a few extremely general physical constraints: {\it smoothness}, {\it locality}, and the {\it relevant symmetries}. 
For nonrelativistic systems, the symmetries include translations in time and Euclidean symmetries (translations, rotations, and reflections) in space. In fact, many of these constraints have been implicitly assumed in most efforts to identify evolution equations from synthetic data generated by a variety of canonical partial differential equations (PDEs) \cite{bar1999,xu2008,rudy2017,schaeffer2017,raissi2018}. However, {\it evolution equations} are just one type of a relation that may be required to fully describe a physical process. Other examples include {\it physical constraints}, such as energy and mass conservation for an inviscid, incompressible fluid or the curl-free condition for the electric field in electrostatics, as well as {\it boundary conditions}. Previous studies have largely ignored the problem of identifying these equally important relations.

Several alternative approaches for learning physical models from observational data have been explored in the literature, including methods based on Gaussian processes \cite{raissi2018} and deep learning  \cite{raissi2019}. However, these approaches tend to require a substantial amount of specific domain knowledge in order to identify parsimonious, human-interpretable models, severely limiting their utility for scientific discovery. Instead, at present they appear to be much better suited for data analysis applications. Some examples include solving evolution equations \cite{portwood2019} and inverse problems such as full state reconstruction from limited measurements \cite{raissi2020}.

In this article, we introduce a flexible data-driven approach for identifying a complete mathematical description of a physical system, which we call sparse physics-informed discovery of empirical relations (SPIDER). We illustrate SPIDER by discovering the evolution equations, constraints, and boundary conditions governing the flow of an incompressible Newtonian fluid from noisy data using only the assumptions of smoothness, locality, and symmetry. 

\section*{Sparse physics-informed discovery of empirical relations}

In order for a data-driven approach to identify a sufficiently general model, the data must exhibit enough variation to sample the state space of the physical problem \cite{schaeffer2018}. Here, this is accomplished by using the numerical solution of a high-Reynolds number flow through a rectangular channel from the Johns Hopkins University turbulence database \cite{jhutdb}. The data set includes the flow velocity ${\bf u}$ and pressure $p$ fully resolved in space and time. The channel dimensions are $L_x\times L_y\times L_z\times L_t=8\pi\times 2\times 3\pi\times 26$ (in nondimensional units) and the data are stored on a spatiotemporal grid of size $2048\times 512\times 1536\times 4000$. The viscosity is $\nu=5\times 10^{-5}$ and the corresponding friction Reynolds number is $Re_\tau \sim 1000$. A representative snapshot of the data is shown in Figure \ref{fig:domains}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{domains.pdf}
\caption{Snapshot of the velocity component $u_z$ over a portion of the entire domain. Sample integration domains (shown as dotted boxes) near the edge of the channel are much narrower than those in the center due to the non-uniform grid spacing in the $y$ direction.
}
\label{fig:domains}
\end{figure}

The immense size of the entire data set comprising $2.6\times 10^{13}$ ``measurements'' illustrates the challenges faced by a purely data-driven approach. The locality property radically reduces the number of possible functional relations between measurements by constraining these to a small spatiotemporal neighborhood of a given point. In particular, for smooth continuous fields, such functional relations have to be expressed in terms of their values and partial derivatives. For systems that are invariant with respect to spatial and temporal translation,
a functional relation can be expressed in the form of a Volterra series
\begin{equation}\label{eq:volterra}
\sum_n c_n {\bf f}_n = 0
\end{equation}
where $c_n$ are coefficients and ${\bf f}_n$ are products of the fields and their partial derivatives. For systems with translational symmetry in space and time, the most general relations of this type are nonlinear PDEs with constant coefficients. Most prior work focused on evolution equations, which are special cases of \eqref{eq:volterra} where $c_1=1$ and ${\bf f}_1$ is the first-order temporal derivative of one of the fields. Other special cases include differential equations that do not involve temporal derivatives and algebraic relations between the fields that involve no derivatives at all.

Our aim here is to identify a parsimonious mathematical model of the flow in the form of a system of PDEs involving the velocity and pressure fields, ${\bf u}$ and $p$, with appropriate boundary conditions. The key observation here is that the form of the functional relations \eqref{eq:volterra} can be restricted sufficiently using the rotational symmetry constraint. All terms ${\bf f}_n$ have to transform in the same way under rotations and reflections, with the transformation rule corresponding to a particular representation of the rotational symmetry group. For instance, for non-relativistic systems, the symmetry group involves rotations about any axis in three-dimensional space, with the representations corresponding to tensors of various ranks. Here we will restrict our attention to the two lowest rank tensors, i.e., scalars and vectors, although the same approach is trivially extended to tensors of any rank.

\subsection*{Learning evolution equations and constraints}
{\bf Constructing model libraries.} In the present problem, the data consist of a scalar field $p$ and a vector field ${\bf u}$. The differential operators that transform as a scalar and a vector, respectively, are $\partial_t$ and $\nabla$. Using these objects, we can construct tensors of any rank as tensor products. For instance, the terms ${\bf u}$, $\partial_t{\bf u}$, and $\nabla p$ all transform as vectors. To illustrate the procedure, we will restrict the terms ${\bf f}_n$ to be at most cubic in $p$, ${\bf u}$, $\partial_t$, and $\nabla$, which yields a scalar equation
\begin{equation}
\begin{aligned}
c_1p&+c_2\nabla\cdot{\bf u}+c_3 \partial_t p+c_4p^2+c_5{\bf u}^2\\
&+c_6({\bf u}\cdot\nabla)p+c_7\nabla^2p+c_8\partial_t(p^2)+c_9\partial^2_tp\\
&+c_{10}p(\nabla\cdot {\bf u})+c_{11}{\bf u}^2p+c_{12}\partial_t({\bf u}^2)=0
\label{eq:rank0}
\end{aligned}
\end{equation}
and a vector equation
\begin{equation}
\begin{aligned}
c_1{\bf u}&+c_2\partial_t{\bf u}+c_3\nabla p+c_4p{\bf u}+c_5({\bf u}\cdot\nabla){\bf u}+c_6\nabla^2{\bf u}\\
&+c_7\partial^2_t{\bf u}+c_8u^2{\bf u}
+c_9p^2{\bf u}+c_{10}\nabla (p^2)+c_{11}{\bf u}(\nabla\cdot{\bf u})\\
&+c_{12}\nabla{\bf u}^2+c_{13}\nabla(\nabla\cdot{\bf u})+c_{14}p \partial_t {\bf u}+c_{15}{\bf u} \partial_t p = 0.
\label{eq:rank1}
\end{aligned}
\end{equation}
Reflection symmetry ensures that pseudovectors such as $\nabla\times{\bf u}$ do not appear in the latter equation.
These two PDEs will form the libraries of the candidate relations describing fluid physics in the bulk. It is straightforward to expand these by including higher-order terms.

{\bf Dealing with noisy data.} Parsimonious scalar and vector relations between velocity and pressure can be identified by performing sparse regression using the libraries \eqref{eq:rank0} and \eqref{eq:rank1}, respectively. 
Terms involving higher-order derivatives, such as $\nabla^2p$ and $\nabla^2{\bf u}$, can be very sensitive to noise \cite{rudy2017,reinbold2019}, which is present in any experimental measurements. 
To make the regression more robust, we use the weak form of both PDEs following the approach introduced in our earlier work \cite{gurevich2019}. 
Specifically, we multiply each equation by a smooth weight function $w_j({\bf x},t)$ and then integrate it over a rectangular spatiotemporal domain $\Omega_k$ of size $H_x\times H_y\times H_z\times H_t$  (cf. Table \ref{tab:domain}). 
The derivatives are shifted from the data (${\bf u}$ and $p$) onto the weight functions $w_j$ whenever possible via integration by parts, after which the integrals are evaluated numerically using trapezoidal quadratures. 
For equation \eqref{eq:rank0}, we use scalar weight functions of the form
\begin{align} \label{eq:wf}
&w_j({\bf x},t) = \tilde{w}_{j_1}(\bar{x})\tilde{w}_{j_2}(\bar{y})\tilde{w}_{j_3}(\bar{z})\tilde{w}_{j_4}(\bar{t}), \nonumber\\
&\tilde{w}_l(s) = (s^2-1)^\beta P_l(s),
\end{align}
where $P_l$ is the $l$th Legendre polynomial and the bar denotes nondimensionalization relative to the size of the integration domain, e.g., $\bar{x}=2(x-x_k)/H_x$. Here we chose $l=0$ or $1$ for each of the coordinate directions, yielding $2^{d+1}$ different weight functions for 
%a $(d+1)$-dimensional integration domain. 
an integration domain with one temporal and $d$ spatial dimensions.
We choose $\beta = 6$ to (i) ensure that all the boundary terms vanish after integration by parts and (ii) increase the accuracy of numerical quadrature \cite{gurevich2019}. To integrate \eqref{eq:rank1}, we instead use vector weight functions ${\bf w}^i_j({\bf x},t) = w_j({\bf x},t)\hat{\bf i}$ for $i=1,\cdots,d$ varying over the indices of the spatial dimensions, resulting in a total of $d\,2^{d+1}$ different weight functions. 

By repeating this procedure for a number of integration domains $\Omega_k$ centered at different locations and for different weight functions $w_j$, the problem of determining the unknown coefficients ${\bf c}=[c_1,\cdots,c_N]$ is recast as the solution of an overdetermined linear system of the form
\begin{align}
    Q{\bf c} = 0,
    \label{eq:lin}
\end{align}
where $Q = [{\bf q}_1 \dots {\bf q}_N]$ and the columns ${\bf q}_n$ correspond to the integrals of the terms ${\bf f}_n$ in \eqref{eq:volterra}. We sampled the data using 30 rectangular domains chosen to lie either in the center or at the edge of the channel, as shown in Fig. \ref{fig:domains}, and randomly distributed in the streamwise ($x$), spanwise ($z$), and temporal directions, yielding $30\times 2^{d+1}$ linear equations on the coefficients $c_n$ in \eqref{eq:rank0} and $30d\times 2^{d+1}$ linear equations on the coefficients in \eqref{eq:rank1}.

\begin{table}
\begin{tabular}{*{5}{c}}
\hline & $H_x$ & $H_y$ & $H_z$ & $H_t$ \\ \hline
edge & 1/3 (27) & 1/50 (34) & 1/5 (33) & 1/4 (38) \\ 
center & 1/3 (27) & 1/5 (32) & 1/5 (33) & 1/4 (38) \\ \hline
\end{tabular}
\caption{Dimensions of the integration domains $\Omega_k$ in physical units and grid points (in parentheses).}
\label{tab:domain}
\end{table}

{\bf Selection of parsimonious relations.} Solutions of \eqref{eq:lin} have a degree of freedom corresponding to the normalization of ${\bf c}$, which is conventionally eliminated by setting one of the coefficients, say $c_1$, to 1. However, we generally cannot guarantee that any given term is present in the correct sparse model. Instead, we treat all of the terms on even footing and compute ${\bf c}$ as the right singular vector of $Q$ corresponding to the smallest singular value.
In fact, this is the solution of a constrained least squares problem for $Q^TQ{\bf c}=0$:
\begin{equation}
{\bf c}=\argmin_{\|{\bf c}\|=1}\|Q^TQ{\bf c}\|.
\label{eq:linn}
\end{equation}
It is worth noting that, when multiple singular values of $Q$ are small, there may be several ``good" solutions for ${\bf c}$. 

In order to obtain a parsimonious model, we must find a sparse coefficient vector ${\bf c}^*$ such that the residual $\|Q{\bf c}^*\|$ is comparable to the residual $\|Q{\bf c}\|$ with ${\bf c}$ given by \eqref{eq:linn}. If the parsimonious model contains multiple terms, it can usually be identified by an iterative greedy algorithm. At each iteration, we use the singular value decomposition of $Q^{(N)} = [{\bf q}_1 \dots {\bf q}_N]$ to find ${\bf c}^{(N)}$ as described previously. We also compute the residual $\eta^{(N)} = \|Q^{(N)} {\bf c}^{(N)}\|$. Next, we consider all of the candidate models formed by dropping one of the terms and eliminating the corresponding column from $Q^{(N)}$. We select the candidate model with $N-1$ terms that achieves the smallest residual and then repeat until only one term remains. This yields a sequence of increasingly sparse models described by $N$-dimensional coefficient vectors ${\bf c}^N$ (forming an approximately Pareto optimal set \cite{miettinen2012}).

There are many reasonable ways to select a final model from this collection based on the trade-off between their parsimony (i.e., number of terms $N$) and accuracy, quantified by the residuals $\eta^{(N)}$. For instance, one might select the simplest model which achieves a relative residual of less than, say, $1\%$ or the model for which discarding a single term results in the largest relative increase in the residual. In this article, we follow Ref. \cite{gurevich2019}: specifically, we choose the model described by the coefficient vector ${\bf c}^{(N)}$ where $N = \max\{n : \eta^{(n)}/\eta^{(n-1)} > \gamma\}$, where the parameter $\gamma = 3$ was selected empirically. This procedure was repeated for 10 replications with random choices of domain locations and, for noisy data, realizations of noise in order to verify that the results are reproducible. 

{\bf Effects of noise.} To test the effects of noise on the results of sparse regression, in addition to the original simulation data, we also used synthetic data with varying levels of additive white Gaussian noise. Specifically, we define the noisy data by $p_{\sigma} = p + \sigma N_1 s_p$ and ${\bf u}_{\sigma} = {\bf u} + \sigma\,\text{diag}\{{\bf N}_2\}{\bf s}_u$, where $\sigma$ is the scalar noise level (ranging from 0 to 50\%). $N_1$ and the three components of ${\bf N}_2$ are independently sampled standard normal random variables at each point in space and time, and $s_p$ and ${\bf s}_u$ are the sample standard deviations of the pressure and each component of the flow velocity computed across the original dataset.

\begin{figure*}[t]
\centering
\subfloat[]{\includegraphics[width=0.47\textwidth]{pareto.pdf}}
\hfill
\subfloat[]{\includegraphics[width=0.47\textwidth]{pareto_noise.pdf}}
\caption{Dependence of the residual $\|Q{\bf c}\|$ on the number of terms $N$ retained in the vector relation \eqref{eq:rank1} for (a) noiseless data and (b) data with 50\% noise. Black (white) squares represent data collected near the edge (in the middle) of the channel. The models selected by the greedy algorithm in each case are shown by the arrows.
}
\label{fig:pareto}
\end{figure*}

Figure \ref{fig:pareto} illustrates the sparsification of the vector relation \eqref{eq:rank1} for a representative distribution of integration domains (with $d=3$). Both near the wall of the channel and in the center, the residual $\eta$ remains nearly constant until only four or five terms remain and grows quickly when more terms are discarded. In the noiseless case shown in Figure \ref{fig:pareto}(a), the magnitude of the residual for $N>5$ is essentially determined by the discreteness of the data (both in the numerical simulations and in evaluating the integrals using quadratures) \cite{gurevich2019}. In the noisy case, shown in Figure \ref{fig:pareto}(b), the magnitude of the residual is determined by the level of  noise and is higher than in the noiseless case, as expected. However, qualitatively, the dependence of $\eta$ on the number of terms $N$ remaining in the model is the same in all of the cases.

\begin{table*}
\subfloat[]{
\begin{tabular}{cccccc}
\hline & $\sigma$ & $\partial_t {\bf u}$ & $({\bf u}\cdot\nabla){\bf u}$ & $\nabla p$ & $\nabla^2{\bf u}$ \\ \hline
$\bar{c}_n$ & 0\% & 0.999 & 1 & 0.995 & $-4.93 \times 10^{-5}$ \\
 & 50\% & 1.002 & 1 & 0.989 & $-4.97 \times 10^{-5}$  \\ \hline
$s_n$ & 0\% & $4\!\times\!10^{-4}$ & $8\!\times\!10^{-4}$ & $1\!\times\!10^{-3}$ & $6\!\times\!10^{-8}$ \\ 
 & 50\% & $7\!\times\!10^{-3}$ & $8\!\times\!10^{-3}$ & $8\!\times\!10^{-3}$ & $1\!\times\!10^{-6}$ \\ \hline
$\chi_n$ & 0\% & 0.905 & 1 & 0.407 & 0.487 \\
& 50\% & 0.908 & 1 & 0.405 & 0.489 \\ \hline
\end{tabular}
}
\hfill
%\hspace{20mm}
\subfloat[]
{
\begin{tabular}{*{7}{c}}
\hline & $\sigma$ & $\partial_t {\bf u}$ & $({\bf u}\cdot\nabla){\bf u}$ & $\nabla p$ & $\nabla^2{\bf u}$ & ${\bf u}$ \\ \hline
$\bar{c}_n$ & 0\% & 1.000 & 1 & 1.000 & $-5.26\!\times\!10^{-5}$ & $-0.0022$ \\
 & 48\% & 1.000 & 1 & 0.998 & 0 & 0 \\ \hline
$s_n$ & 0\% & $2\!\times\!10^{-4}$ & $2\!\times\!10^{-4}$ & $3\!\times\!10^{-4}$ & $4\!\times\!10^{-7}$ & $7\!\times\!10^{-6}$ \\ 
& 48\% & $1\!\times\!10^{-2}$ & $1\!\times\!10^{-2}$ & $3\!\times\!10^{-2}$ & 0 & 0 \\ \hline
$\chi_n$ & 0\% & 0.987 & 1 & 0.0528 & 0.0027 & 0.0065 \\
& 48\% & 0.986 & 1 & 0.0529 & 0 & 0 \\ \hline
\end{tabular}
}
\caption{Vector relations identified from data with different noise levels (described by $\sigma$) (a) near the edge of the channel and (b) in the center. The rows show the mean values of the coefficients $\bar{c}_n$, their standard deviation $s_n$, and the magnitudes of terms $\chi_n=\|c_n {\bf f}_n\|$ (normalized relative to the largest term).}
\label{tab:ns}
\end{table*}

The relations selected by the greedy algorithm are summarized in Table \ref{tab:ns}, which shows the surviving terms ${\bf f}_n$, their coefficients $c_n$, and the respective contributions of different terms in each relation. The table also shows the standard deviation of the coefficients found across different replications. For data from the edge of the channel, sparse regression consistently identifies the Navier-Stokes equation with correct coefficients, including the viscosity $\nu$, for noise levels up to at least 50\%. For data from the middle of the channel with low noise (up to $\sim$4\%), sparse regression yields the Navier-Stokes equation with an additional, linear term, which is discussed further below. For more noisy mid-channel data with $\sigma$ up to 48\%, the Euler equation, also with correct coefficients, is reconstructed instead.

{\bf Single-term identities.} While the greedy sparsification algorithm was found to be well-suited for identifying relations involving multiple terms, it fails for single-term relations. This difficulty, which is not adequately addressed by past sparse regression literature, is illustrated well by the regression problem for the scalar library \eqref{eq:rank0}. The data describe an incompressible fluid, for which 
\begin{align}\label{eq:divu}
    \nabla\cdot{\bf u}=0
\end{align}
As a result, the terms $\nabla\cdot{\bf u}$, ${\bf u}(\nabla\cdot{\bf u})$, and $\nabla(\nabla\cdot{\bf u})$ in \eqref{eq:rank0} all have very small magnitudes and are thus quickly discarded from the library by the greedy algorithm. 
For this reason, we start by computing the magnitude $\|{\bf q}_n\|$ of each term, which equals the residual of the corresponding single-term model. Comparing $\|{\bf q}_n\|$ with the residual $\eta$ for the identified multi-term model allows determination of whether the parsimonious relation should include one or several terms. With this enhancement, SPIDER correctly identified the continuity equation \eqref{eq:divu} from the scalar library \eqref{eq:rank0} for data from either the edge or the center of the channel, across all replications and for all noise levels up to the maximum, $\sigma = 50\%$.

\subsection*{Learning boundary conditions}
SPIDER can also be used to discover boundary conditions. In this case the rotational symmetry is partially broken: instead of rotations in all three spatial directions, the problem is only invariant with respect to rotations about the normal ${\bf n}$ to the boundary. The library of terms that transform as vectors near the boundary include ${\bf n}$ in addition to ${\bf u}$ and $\nabla$. We exclude time derivatives, because these can be eliminated with the help of the bulk equations. We also exclude the dependence on $p$ to keep the library to a reasonable size (this dependence is trivial to restore). Retaining terms that contain each of ${\bf u}$ and $\nabla$ at most once yields a vector equation
\begin{align}\label{eq:bclib}
    c_1{\bf u}&+c_2{\bf n}+c_3({\bf u} \cdot {\bf n}){\bf n}+c_4\nabla({\bf u} \cdot {\bf n})\nonumber\\&+c_5({\bf n}\cdot \nabla){\bf u}+c_6{\bf n}(\nabla \cdot {\bf u})= 0
\end{align}
Next we separate the normal and tangential components by applying the projection operators $P_\perp={\bf n}{\bf n}$ and $P_\parallel=\mathbb{1}-{\bf n}\otimes{\bf n}$ to the library \eqref{eq:bclib}, where ${\bf n}\otimes{\bf n}$ represents the tensor product of the normal vectors. We eliminate all terms which have identically vanishing projections. Furthermore, we can also eliminate all terms involving $\nabla\cdot{\bf u}$, since we have already identified the continuity equation \eqref{eq:divu}.

Since the boundary conditions only hold on the solid walls $y=\pm 1$, each projection of the relation \eqref{eq:bclib} is integrated over rectangular $(2+1)$-dimensional domains $\Omega_k$ of size $H_x\times H_z\times H_t$ constrained to one of the walls. Respectively, the weight functions ${\bf w}_j$ are constructed as products of three one-dimensional functions $\tilde{w}_l(s)$ where $s=\bar{x}$, $\bar{z}$, or $\bar{t}$. Note that the derivatives of the data with respect to the wall-normal ($y$) coordinate cannot be eliminated using integration by parts in this case; instead, we evaluate them directly using finite differences.
For all noise levels up to the maximum $\sigma = 50\%$, valid single-term boundary conditions were always identified. Specifically, for the normal component, two equivalent relations $P_\perp {\bf u} = P_\perp ({\bf u} \cdot {\bf n}){\bf n} = {\bf u} \cdot {\bf n} = 0$ are identified. For the tangential component, $P_\parallel {\bf u} = 0$ is identified. These can be combined into the algebraically ``simplest" boundary condition ${\bf u}=0$, which is the well-known no-slip boundary condition.

As we have shown, physically-informed sparse regression can successfully identify a complete, quantitatively accurate mathematical model of the fluid flow using a tiny fraction of the available data, even when the data are extremely noisy. In fact, at low noise levels, such a model can be identified using only a single integration domain in the bulk and its projection onto the boundary, provided sufficiently many weight functions are used. In particular, the no-slip boundary condition ${\bf u}=0$ and the incompressibility condition \eqref{eq:divu} are always correctly identified. 

\section*{Discussion}

The most general vector relation identified is a generalization of the Navier-Stokes equation
\begin{equation} \label{eq:unstable}
    \partial_t{\bf u}+({\bf u}\cdot\nabla){\bf u}+\nabla p-\nu\nabla^2{\bf u}-\alpha{\bf u}=0,
\end{equation}
with some nonnegative $\nu$ and $\alpha$. The last two terms may or may not be identified depending on the sampled region of the flow and the magnitude of noise in the dataset. The term $\nu\nabla^2{\bf u}$ represents the effect of viscosity, which is well-known to play an important role in the boundary layer, as reflected by its large relative magnitude ${\chi_n\approx 0.5}$ in that region. In fact, in the boundary layer, the greedy algorithm selects the Navier-Stokes equation for any level of noise with all of the terms having comparable magnitudes, as shown in Table \ref{tab:ns}. 

In the center of the channel, the effect of viscosity is negligible due to the lack of small-scale structures, so the flow is described quite accurately by both the Navier-Stokes and the Euler equation. Indeed, in that region of the flow, the dominant balance is between the terms $\partial_t{\bf u}$ and $({\bf u}\cdot\nabla){\bf u}$, while the viscous term is more than two orders of magnitude smaller. The linear term $\alpha{\bf u}$ is similarly small. It is then not surprising that, at higher noise levels, the greedy algorithm discards both the viscous and the linear term, yielding the Euler equation.

The unexpected observation is the presence of the linear term $\alpha{\bf u}$, which does not appear in the Navier-Stokes equation, in the relation identified from mid-channel data with low levels of noise. This term is larger in magnitude than the viscous term and is reliably identified by the greedy algorithm regardless of the distribution of the integration domains or noise realization, suggesting that it is not spurious. The sign of $\alpha$ indicates that this term is destabilizing, unlike the viscous term, indicating a numerical instability in the simulation used to generate the data. The computational grid is non-uniform in $y$ and is coarsest in the middle of the channel, which explains why numerical instability is found there but not near the wall of the channel.

\section*{Conclusion}

It should be noted that other physical relations describing fluid flows can also be identified using the approach outlined here. For instance, the energy equation can be derived by multiplying the vector relation \eqref{eq:unstable} by ${\bf u}$. The same equation can also be obtained directly from data by applying the greedy algorithm to the scalar library \eqref{eq:rank0}, provided the latter is expanded to include terms up to quadratic in both ${\bf u}$ and $\nabla$.

To summarize, we have shown that a combination of very general physical constraints, weak formulation of PDEs, and sparse regression yields an extremely powerful model discovery tool, which we call SPIDER. It allows one to identify complete and easily interpretable quantitative mathematical models of continuum systems, not limited to evolution equations, from even very noisy data.
Moreover, SPIDER provides information about the relative importance of different physical effects and can also serve as a valuable diagnostic tool for validating and troubleshooting numerical simulations or experimental setups.

The utility of the approach presented here is not limited to fluid dynamics. This same approach can be used to identify mathematical models of numerous high-dimensional nonlinear nonequilibrium systems that have defied traditional first-principles modeling approaches. Some examples include high energy density plasmas, as found inside the stars and the interior of fusion energy devices, and excitable media such as cardiac or intestinal muscle tissue and biological neural networks. Other interesting applications include active matter systems such as animal herds, bird flocks, insect swarms, fish schools, bacterial aggregates, self-propelled particles, and even collections of robots---these are formally discrete but may possess useful continuum models. Most active matter systems lack quantitative mathematical models and exhibit interesting collective behaviors that may be better understood within the framework of such models.

\section*{Data availability}
The channel flow dataset used in this article is publicly available for download online at \url{http://turbulence.pha.jhu.edu}. The metadata associated with this dataset can be accessed at \url{http://turbulence.pha.jhu.edu/Channel_Flow.aspx}. 

\section*{Code availability}
The MATLAB code used to produce the results in this article is available at \url{https://github.com/sibirica/SPIDER_channelflow}.  

\bibliography{main}
%\input{main.bbl}

\section*{Acknowledgements}
This material is based on work supported by the National Science Foundation under Grants No.~CMMI-1725587 and CMMI-2028454.

\section*{Author contributions}
D.R.G. and R.O.G. conceived the study. D.R.G., P.A.K.R., and R.O.G. designed the algorithm. D.R.G. and P.A.K.R. coded the MATLAB implementation. D.R.G. ran the numerical experiments and produced figures. D.R.G., P.A.K.R., and R.O.G. contributed to the writing.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Additional information}
{\bf Correspondence and requests for materials} should be addressed to D.R.G.

% \begin{table}
% \begin{tabular}{*{5}{c}}
% \hline & $H_x$ & $H_y$ & $H_z$ & $H_t$ \\ \hline
% edge & 1/3 (27) & 1/50 (34) & 1/5 (33) & 1/4 (38) \\ 
% center & 1/3 (27) & 1/5 (32) & 1/5 (33) & 1/4 (38) \\ \hline
% \end{tabular}
% \caption{Dimensions of the integration domains $\Omega_k$ in physical units and grid points (in parentheses).}
% \label{tab:domain}
% \end{table}

% \begin{table*}
% \subfloat[]{
% \begin{tabular}{cccccc}
% \hline & $\sigma$ & $\partial_t {\bf u}$ & $({\bf u}\cdot\nabla){\bf u}$ & $\nabla p$ & $\nabla^2{\bf u}$ \\ \hline
% $\bar{c}_n$ & 0\% & 0.999 & 1 & 0.995 & $-4.93 \times 10^{-5}$ \\
%  & 50\% & 1.002 & 1 & 0.989 & $-4.97 \times 10^{-5}$  \\ \hline
% $s_n$ & 0\% & $4\!\times\!10^{-4}$ & $8\!\times\!10^{-4}$ & $1\!\times\!10^{-3}$ & $6\!\times\!10^{-8}$ \\ 
%  & 50\% & $7\!\times\!10^{-3}$ & $8\!\times\!10^{-3}$ & $8\!\times\!10^{-3}$ & $1\!\times\!10^{-6}$ \\ \hline
% $\chi_n$ & 0\% & 0.905 & 1 & 0.407 & 0.487 \\
% & 50\% & 0.908 & 1 & 0.405 & 0.489 \\ \hline
% \end{tabular}
% }
% \hfill
% %\hspace{20mm}
% \subfloat[]
% {
% \begin{tabular}{*{7}{c}}
% \hline & $\sigma$ & $\partial_t {\bf u}$ & $({\bf u}\cdot\nabla){\bf u}$ & $\nabla p$ & $\nabla^2{\bf u}$ & ${\bf u}$ \\ \hline
% $\bar{c}_n$ & 0\% & 1.000 & 1 & 1.000 & $-5.26\!\times\!10^{-5}$ & $-0.0022$ \\
%  & 48\% & 1.000 & 1 & 0.998 & 0 & 0 \\ \hline
% $s_n$ & 0\% & $2\!\times\!10^{-4}$ & $2\!\times\!10^{-4}$ & $3\!\times\!10^{-4}$ & $4\!\times\!10^{-7}$ & $7\!\times\!10^{-6}$ \\ 
% & 48\% & $1\!\times\!10^{-2}$ & $1\!\times\!10^{-2}$ & $3\!\times\!10^{-2}$ & 0 & 0 \\ \hline
% $\chi_n$ & 0\% & 0.987 & 1 & 0.0528 & 0.0027 & 0.0065 \\
% & 48\% & 0.986 & 1 & 0.0529 & 0 & 0 \\ \hline
% \end{tabular}
% }
% \caption{Vector relations identified from data with different noise levels (described by $\sigma$) (a) near the edge of the channel and (b) in the center. The rows show the mean values of the coefficients $\bar{c}_n$, their standard deviation $s_n$, and the magnitudes of terms $\chi_n=\|c_n {\bf f}_n\|$ (normalized relative to the largest term).}
% \label{tab:ns}
% \end{table*}

\end{document}
