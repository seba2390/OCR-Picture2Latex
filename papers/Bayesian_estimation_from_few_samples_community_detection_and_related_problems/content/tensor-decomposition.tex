\section{Tensor decomposition from constant correlation}
\label{sec:tdecomp}

\begin{problem}[Orthogonal $n$-dimensional $4$-tensor decomposition from constant correlation]
\label{prob:tdecomp-l2}
  Let $a_1,\ldots,a_m \in \R^n$ be orthonormal, and let $A = \sum_{i = 1}^m a_i^{\tensor 4}$.
  Let $B \in (\R^n)^{\tensor 4}$ satisfy $\tfrac{\iprod{A,B}}{\|A\| \|B\|} \geq \delta = \Omega(1)$.\\
  Let $\cO$ be an oracle such that for any unit $v \in \R^n$,
  \[
  \cO(v) = \begin{cases}
  \textbf{YES} \text{ if } \sum_{i=1}^m \iprod{a_i,v}^4 \geq \delta^{O(1)} \\
  \textbf{NO} \text{ otherwise }
  \end{cases}
  \]

\noindent \textbf{Input:} The tensor $B$, and if $\delta < 0.01$, access to the oracle $\cO$.

\noindent  \textbf{Goal:} Output orthonormal vectors $b_1,\ldots,b_{m}$ so that there is a set $S \subseteq [m]$ of size $|S| \geq \delta^{O(1)} \cdot m$ where for every $i \in S$ there is $j \leq m$ with $\iprod{b_j,a_i}^2 \geq \delta^{O(1)}$.
\end{problem}

We will give an $n^{1/\delta^{O(1)}}$-time algorithm (hence using at most $n^{1/\delta^{O(1)}}$ oracle calls) for this problem based on a maximum-entropy Sum-of-Squares relaxation.
The main theorem is the following; the subsequent corollary arrives at the final algorithm.
\begin{theorem}\label{thm:tdecomp-main-small}
%
  Let $A,B$ and $a_1,\ldots,a_m$ and $\delta \leq 0.01$ be as in Problem~\ref{prob:tdecomp-l2}.
  Let $v_1,\ldots,v_r$ for $r \leq \delta^{4} m$ be orthonormal vectors.
  There is a randomized algorithm $ALG$ with running time $n^{O(1)}$ which takes input $B,v_1,\ldots,v_r$ and outputs a unit vector $v$, orthogonal to $v_1,\ldots,v_r$, with the following guarantee.
  There is a set $S \subseteq [m]$ of size $|S| \geq \delta^{O(1)} \cdot m$ so that for $i \in S$,
  \[
    \Pr \left \{ \iprod{v, a_i}^2 \geq \delta^{O(1)} \right \} \geq n^{-1/\poly(\delta)}\mper
  \]
\end{theorem}

The following corollary captures the overall algorithm for tensor decomposition, using the oracle $\cO$ to filter the output of the algorithm of Theorem~\ref{thm:tdecomp-main-small}.
\begin{corollary}\torestate{\label{cor:tdecomp-main}
  Let $a_1,\ldots,a_n, A, B, \delta$ be as in Theorem~\ref{thm:tdecomp-main-small} and $\cO$ as in Problem~\ref{prob:tdecomp-l2}.
  There is a $n^{\poly(1/\delta)}$-time algorithm which takes the tensor $B$ as input and returns $b_1,\ldots,b_m$ such that with high probability there is a set $S \subseteq [m]$ of size $|S| \geq \delta^{O(1)} m$ which has the guarantee that for all $i \in S$ there is $j \leq m$ with $\iprod{a_i,b_j}^2 \geq \delta^{O(1)}$.
  If $\delta \leq 1 - \Omega(1)$, the algorithm makes $n^{1/\poly(\delta)}$ adaptive queries to the oracle $\cO$.

  The algorithm can also be implemented with nonadaptive queries as follows.
  Once the input $B$ and the random coins of the algorithm are fixed, there is a list of at most $n^{\poly(k/\delta)}$.
  Query the oracle $\cO$ nonadaptively on all these vectors and assemble the answers into a lookup table; then the decomposition algorithm can be run using access only to the lookup table.
  }
\end{corollary}
\begin{proof}[Proof of Corollary~\ref{cor:tdecomp-main}]
  If $\delta \geq 1 - \e^*$ for a small enough constant $\e^*$ then the tensor decomposition algorithm of Schramm and Steurer has the appropriate guarantees.
  (See Theorem 4.4 and Lemma 4.9 in \cite{DBLP:conf/colt/SchrammS17}.
  This algorithm has several advantages, including that it does not need to solve any semidefinite program, but it cannot handle the high-error regime we need to address here.)

  From here on we assume $\delta \leq 0.01 < 1 - \e^*$.
  (Otherwise, we can replace $\delta$ with $\delta^C \leq 0.01$ for large enough $C$.)
  Our algorithm is as follows.

  \begin{algorithm}[Constant-correlation tensor decomposition]
  \begin{compactenum}
    \item Let $V$ be an empty set of vectors.
    \item For rounds $1,\ldots,T = \delta^{O(1)} m$, do:
    \begin{compactenum}
      \item Use the algorithm of Theorem~\ref{thm:tdecomp-main-small} on the tensor $B$ to generate $w_1,\ldots,w_t$, where $t = n^{1/\delta^{O(1)}}$.
      \item Call $\cO$ on successive vectors $w_1,\ldots,w_t$, and let $w$ be the first for which it outputs \textbf{YES}.
      (If no such vector exists, the algorithm halts and outputs random orthonormal vectors $b_1,\ldots,b_m$.)
      \item Add $w$ to $V$.
    \end{compactenum}
    \item Let $b_1,\ldots,b_{m - |V|}$ be random orthonormal vectors, orthogonal to each $v \in V$.
    \item Output $\{b_1,\ldots,b_{m-|V|}\} \cup V$.
  \end{compactenum}
  \end{algorithm}
  Choosing $t = n^{1/\delta^{O(1)}}$ large enough, and $T = \delta^{O(1)} m$ small enough, by Theorem~\ref{thm:tdecomp-main-small} with high probability in every round $1,\ldots,T$ there is some $w$ among $w_1,\ldots,w_t$ for which $\cO$ outputs \textbf{YES}.
  Suppose that occurs.
  In this case, the algorithm outputs (along with some random vectors $b_i$) a set of vectors $V$ which are orthonormal, and each $v \in V$ satisfies $\iprod{v,a_i} \geq \delta^{O(1)}$ for some $a_i$; say that this $a_i$ is \emph{covered} by $v$.
  Each $a_i$ can be covered at most $1/\delta^{O(1)}$ times, by orthonormality of the set $V$.
  So, at least $\delta^{O(1)} |V| = \delta^{O(1)}m$ vectors are covered at least once, which proves the corollary.
\end{proof}

We turn to the proof of Theorem~\ref{thm:tdecomp-main-small}.
We will use the following lemmas, whose proofs are later in this section.
The problem is already interesting when the list $v_1,\ldots,v_r$ is empty, and we encourange the reader to understand this case first.

The first lemma says that a pseudodistribution of high entropy (in the $2$-norm sense\footnote{For a distribution $\mu$ finitely-supported on a family of orthonormal vectors, the Frobenious norm $\|\E_{x \sim \mu} x^{\tensor k} \|$ is closely related to the collision probability of $\mu$, itself closely related to the order-$2$ case of \Renyi entropy.})
 which is correlated with the tensor $B$ must also be nontrivially correlated with $A$.

\begin{lemma}\label{lem:tdecomp-correlation}
  Let $A, B$ be as in Problem~\ref{prob:tdecomp-l2}.
  Let $v_1,\ldots,v_r \in \R^n$ be orthonormal, with $r \leq \delta^4 m$.
  Suppose $\pE$ is the degree-$4$ pseudodistribution solving
  \begin{align}
  \min & \|\pE x^{\tensor 4}\|_F \label{eq:frob-min}\\
  \text{s.t. } & \text{ $\pE$ satisfies } \{\|x\|^2 \leq 1, \iprod{x,v_1} = 0,\ldots,\iprod{x,v_r}=0 \} \nonumber\\
  &  \iprod{\pE x^{\tensor 4},  B} \geq  \frac{\delta}{2m} \nonumber\\
    & \Normop{\pE xx^\top} \leq \tfrac 1 m\\
    & \Normop{\pE xx^\top \tensor xx^\top} \leq \tfrac 1 m
  \end{align}
  Then $\pE \sum_{i \leq m} \iprod{x,a_i}^4 \geq \delta^2 / 8$.
  Furthermore, it is possible to find $\pE$ in polynomial time.\footnote{Up to inverse-polynomial error, which we ignore here. See \cite{DBLP:conf/focs/MaSS16} for the ideas needed to show polynomial-time solvability.}
\end{lemma}

The second lemma says that given a high-entropy (in the spectral sense of \cite{DBLP:conf/focs/MaSS16}) pseudodistribution $\pE$ having nontrivial correlation with some $a \in \R^n$, contracting $\pE$ with $a$ yields a matrix whose quadratic form is large at $a$ and which does not have too many large eigenvalues.
\begin{lemma}\label{lem:tdecomp-ideal-reweigh}
  Let $a_1,\ldots,a_m \in \R^n$ be orthonormal.
  
  Let $\pE$ be a degree-$4$ pseudoexpectation such that
  \begin{enumerate}
    \item $\pE$ satisfies $\{ \|x\|^2 \leq 1 \}$
    \item $\pE \sum_{i \leq m} \iprod{x, a_i}^4 \geq \delta$.
    \item $\|\pE xx^\top] \|_{op}, \|\pE xx^\top \tensor xx^\top \|_{op} \leq \tfrac 1 m \mper$\footnote{Recall that $\Normop{\cdot}$ denotes the operator norm, or maximum singular value, of a matrix.}
  \end{enumerate}
  Let $M_i \in \R^{n \times n}$ be the matrix $\pE \iprod{x,a_i}^2 xx^\top$.
  For every $i \in [m]$, the matrix $M_i$ has at most $4/\delta$ eigenvalues larger than $\tfrac \delta {4m}$.
  Furthermore,
  \[
  \Pr_{i \sim [m]} \left \{ \iprod{a_i, M_i a_i} \geq \tfrac {\delta}{2m} \right \} \geq \tfrac \delta {2} \mper
  \]
\end{lemma}

The last lemma will help show that a random contraction of a high-entropy pseudodistribution behaves like one of the contractions from Lemma~\ref{lem:tdecomp-ideal-reweigh}, with at least inverse-polynomial probability.

\begin{lemma}\label{lem:tdecomp-nonspherical-reweigh}
Let $g \sim \cN(0,\Sigma)$ for some $0 \preceq \Sigma \preceq \Id$  and let $\pE$ be a degree-$4$ pseudoexpectation where
  \begin{itemize}
    \item $\pE$ satisfies $\{\|x\|^2 \leq 1\}$.
    \item $\Normop{\pE xx^\top } \leq c$.
    \item $\Normop{\pE xx^\top \tensor xx^\top} \leq c$
  \end{itemize}
  Then
  \[
    \E_g \Normop{\pE \iprod{g,x}^2 xx^\top} \leq O(c \cdot \log n)\mper
  \]
\end{lemma}


Now we can prove Theorem~\ref{thm:tdecomp-main-small}.
\begin{proof}[Proof of Theorem~\ref{thm:tdecomp-main-small}]
The algorithm is as follows:
  \begin{algorithm}[Low-correlation tensor decomposition]
  \begin{compactenum}
    \item Use the first part of Lemma~\ref{lem:tdecomp-correlation} to obtain a degree-$4$ pseudoexpectation with $\pE \sum_{i \in [m]} \iprod{a_i,x}^4 \geq \delta^2/4$ satisfying $\{\|x\|^2 \leq 1, \iprod{x,v_1} = 0,\ldots,\iprod{x,v_r} = 0\}$. 

    \item Sample a random $g \sim \cN(0,\Id)$ and compute the contraction $M = \pE \iprod{g,x}^2 xx^\top$.
    \item Output a random unit vector $b$ in the span of the top $\tfrac {32} {\delta^2}$ eigenvectors of $M$.
  \end{compactenum}
  \end{algorithm}

  First note that for any $v \in \Span\{v_1,\ldots,v_r\}$, we must have $\iprod{v,Mv} = \pE \iprod{g,x}^2 \iprod{v,x}^2 = 0$, so $v$ lies in the kernel of $M$. 
  Hence, the ouput of the algorithm will always be orthogonal to $v_1,\ldots,v_r$.

  Let $\Pi_{32/\delta^2}$ be the projector to the top $32/\delta^2$ eigenvectors of $M$.
  For any unit vector $a$ with $\|\Pi_{32/\delta^2} a\| \geq \delta^{O(1)}$, the algorithm will output $b$ with nontrivial correlation with $a$.
  Formally, for any such $a$,
  \[
    \E_b \iprod{b,a}^2 \geq \delta^{O(1)}\mper
  \]

  So, our goal is to show that for a $\delta^{O(1)}$-fraction of the vectors $a_1,\ldots,a_m$,
  \[
    \Pr_g \{ \|\Pi_{32/\delta^2} a_i\| \geq \delta^{O(1)} \} \geq n^{-1/\delta^{O(1)}}\mper
  \]

For $i \in [m]$, let $M_i = \pE \iprod{a_i,x}^2 xx^\top$.
Let $i$ be the index of some $a_i$ so that
  \[
    \iprod{a_i, M_i a_i} \geq \tfrac {\delta^2}{16 m} \text{ and } \rank M_i^{\geq \tfrac {\delta^2}{32 m}} \leq \tfrac {32} {\delta^2}
  \]
  as in Lemma~\ref{lem:tdecomp-ideal-reweigh}.
  (There are $\Omega(\delta^2 m)$ possible choices for $a_i$, according to the Lemma.)

  We expand the Gaussian vector $g$ from the algorithm as
  \[
    g = g_0 \cdot a_i + g'
  \]
  where $g_0 \sim \cN(0,1)$ and $\iprod{g', a_i} = 0$.
  We note for later use that $g'$ is a Gaussian vector independent of $g_0$ and that $\E (g')(g')^\top \preceq \Id$.
  Using this expansion,
  \[
    M = g_0^2 \pE \iprod{a_i,x}^2 xx^\top + 2 \cdot g_0 \pE \iprod{g',x} \iprod{a_i,x} xx^\top + \pE \iprod{g',x}^2 xx^\top\mper
  \]
  We will show that all but the first term have small spectral norm.
  Addressing the middle term first, by Cauchy-Schwarz, for any unit $v \in \R^n$,
  \[
    \pE \iprod{g',x} \iprod{a_i,x}\iprod{v,x}^2 \leq \Paren{\pE \iprod{g',x}^2 \iprod{x,v}^2}^{1/2} \Paren{\pE \iprod{a_i,x}^2 \iprod{v,x}^2}^{1/2} \leq \Normop{\pE \iprod{g',x}^2xx^\top}^{1/2}\cdot \Paren{\tfrac 1 m}^{1/2}\mcom
  \]
where in the last step we have used that $\Normop{\pE xx^\top \tensor xx^\top} \leq \tfrac 1 m$.

  By Markov's inequality and Lemma~\ref{lem:tdecomp-nonspherical-reweigh},
  \[
    \Pr_{g'}\left \{ \Normop{\pE \iprod{g',x}^2xx^\top} > \tfrac {t \log n} m \right \} \leq O\Paren{\tfrac 1 t}\mper
  \]
  Let $t$ be a large enough constant so that
  \[
    \Pr_{g'} \left \{ \Normop{\pE \iprod{g',x}^2xx^\top} \leq \tfrac {t \log n} m \right \}  \geq 0.9\mper
  \]
  For any constant $c$, with probability $n^{-1/\poly(\delta)}$, the foregoing occurs and $g_0$ (which is independent of $g'$) is large enough that
  \[
    g_0^2 \cdot \tfrac {c\delta^2} m > \tfrac 1 {\delta^4} \Normop{M - g_0^2 M_i}\mper
  \]
  Choosing $c$ large enough, in this case
  \[
  M' \defeq \tfrac 1 {g_0^2} M = M_i + O(\delta^{6}/m)\mper
  \]
  Hence the vector $a_i$ satisfies
  \[
    \tfrac 1 {g_0^2} \iprod{a_i, M a_i} \geq \tfrac{\delta^2}{33m}
  \]
  This means that the projection $b$ of $a_i$ into the span of eigenvectors of $M'$ with eigenvalue at least $\delta^2/60m$ has $\|b\|^2 \geq \delta^{O(1)}$.
  This finishes the proof.
\end{proof}


\subsection{Proofs of Lemmas}
These lemmas and their proofs use many ideas from \cite{DBLP:conf/focs/MaSS16}.
The main difference here is that we want to contract the tensor $\pE x^{\tensor 4}$ in $2$ modes, to obtain the matrix $\pE \iprod{g,x}^2xx^\top$.
For us this is useful because $\pE \iprod{g,x}^2xx^\top \succeq 0$.
By contrast, the tools in \cite{DBLP:conf/focs/MaSS16} would only allow us to analyze the contraction $\pE \iprod{h,x \tensor x}xx^\top$ for $h \sim \cN(0, \Id_{n^2})$.

We start with an elementary fact.
\begin{fact}\label{fact:A-proj}
  Let $a_1,\ldots,a_m \in \R^n$ be orthonormal.
  Let $\Pi$ be the projector to a subspace of codimension at most $\delta m$.
  Let $A = \sum_{i=1}^m a_i^{\tensor 4}$ and $\Pi A = \sum_{i=1}^m (\Pi a_i)^{\tensor 4}$.
  Then $\iprod{A,\Pi A} \geq (1 - O(\sqrt \delta)) \|A\| \cdot \|\Pi A\|$.
\end{fact}
A useful corollary of Fact~\ref{fact:A-proj} is that if $T$ is any $4$-tensor satisfying
$\iprod{T,\Pi A} \geq \delta \|T\| \|\Pi A\|$ and $\Pi$ has codimension $ \ll \delta^2 m$, then $\iprod{T,A} \geq \Omega(\delta) \|T\| \|A\|$.
\begin{proof}[Proof of Fact~\ref{fact:A-proj}]
  We expand
  \[
    \iprod{A,\Pi A} = \sum_{i,j \leq m} \iprod{a_i, \Pi a_j}^4 \geq \sum_{i,j \leq m} \|\Pi a_i\|^8
  \]
  Writing $\Pi$ in the $a_i$ basis, we think of $\|\Pi a_i\|^4 = \Pi_{ii}^2$, the square of the $i$-th diagonal entry of $\Pi$.
  Since $\Pi$ has codimension at most $\delta m$,
  \[
    \rank \Pi = \Tr \Pi = \sum_{i \leq n} \Pi_{ii} \geq n - \delta m\mper
  \]
  Furthermore, for each $i$, it must be that $0 \leq \Pi_{ii} \leq 1$.
  By Markov's inequality, at most $\sqrt \delta m$ diagonal entries of $\Pi$ can be less than $1 - \sqrt \delta$ in magnitude.
  Hence, $\sum_{i \leq m} \Pi_{ii}^4 \geq (1 - 4 \sqrt \delta)m$.
  On the other hand, $\|A\|^2 = m$; this proves the fact.
\end{proof}

Now we can prove Lemma~\ref{lem:tdecomp-correlation}.

\begin{proof}[Proof of Lemma~\ref{lem:tdecomp-correlation}]
  We will appeal to Theorem~\ref{thm:correlation-preserving-projection}.
  Let $\cC$ be the convex set of all pseudo-moments $\pE x^{\tensor 4}$ such that $\pE$ is a deg-4 pseudo-distribution that satisfies the polynomial constraints $\{\|x\|^2 \leq 1, \iprod{x,v_i} = 0\}$ and the operator norm conditions
  \begin{gather*}
    \Normop{\pE xx^\top} \leq \tfrac 1 m,\\
    \Normop{\pE xx^\top \tensor xx^\top} \leq \tfrac 1m\mper
  \end{gather*}
  Let $\Pi$ be the projector to the orthogonal space of $v_1,\ldots,v_r$.
  Notice that $\tfrac 1 m \Pi A \in \cC$.
  Furthermore, $\iprod{B, \Pi A} \geq \delta/2$ by Fact~\ref{fact:A-proj}, the assumption that $r \leq \delta^4 m$, and the assumption $\delta \leq 0.01$.
  By Theorem~\ref{thm:correlation-preserving-projection}, and Fact~\ref{fact:A-proj} again, the optimizer of the convex program in the Lemma satisfies $\iprod{\pE x^{\tensor 4}, \tfrac 1 m A} \geq \tfrac{\delta^2}{8m})$ and the result follows.
\end{proof}


\begin{proof}[Proof of Lemma~\ref{lem:tdecomp-ideal-reweigh}]
  By the assumption $\|\pE xx^\top \tensor xx^\top \| \leq \tfrac 1m$, for every $a_i$ it must be that $\pE \iprod{x,a_i}^4 \leq \tfrac 1m$.
  Since $\pE \sum_{i=1}^m \iprod{x,a_i}^4 \geq \delta$, at least $\delta m/2$ of the $a_i$'s must satisfy $\pE \iprod{x,a_i}^4 \geq \tfrac \delta {2m}$.
  Rewritten, for any such $a_i$ we obtain $\iprod{a_i,M_i a_i} \geq \tfrac \delta {2m}$.

  For any $M_i$,
  \[
    \Tr M_i = \pE \iprod{x,a_i}^2 \|x\|^2 = \pE \iprod{x,a_i}^2 \leq \tfrac 1m
  \]
  because $\|\pE xx^\top \| \leq \tfrac 1m$.
  Also, $M_i \succeq 0$.
  Hence, $M_i$ can have no more than $\tfrac 4 \delta$ eigenvalues larger than $\tfrac \delta {4m}$.
\end{proof}

Now we turn to the proof of Lemma~\ref{lem:tdecomp-nonspherical-reweigh}.
We will need spectral norm bounds on certain random matrices associated to the random contraction $\pE\iprod{g,x}xx^\top$.
The following are closely related to Theorem 6.5 and Corollary 6.6 in \cite{DBLP:conf/focs/MaSS16}.
\begin{lemma}\label{lem:tdecomp-spherical-reweigh}
  Let $g \sim \cN(0,\Id)$ and let $\pE$ be a degree-$4$ pseudoexpectation where
  \begin{itemize}
    \item $\pE$ satisfies $\{\|x\|^2 = 1\}$.
    \item $\Normop{\pE xx^\top } \leq c$.
    \item $\Normop{\pE xx^\top \tensor xx^\top} \leq c$
  \end{itemize}
  Then
  \[
    \E_g \Normop{\pE \iprod{g,x}^2 xx^\top} \leq O(c \cdot \log n)\mper
  \]
\end{lemma}
Before proving the lemma, we will need a classical decoupling inequality.
\begin{fact}[Special case of Theorem 1 in \cite{MR1261237-delaPena94}]\label{fact:decoupling}
  Let $g,h \sim \cN(0,\Id_n)$ be independent.
  Let $M_{ij}$ for $i,j \in [n]$ be a family of matrices.
  There is a universal constant $C$ so that
  \[
    \E_g \Normop{ \sum_{i \neq j} g_i g_j \cdot M_{ij}} \leq C \cdot \E_{g,h} \Normop{ \sum_{i \neq j} g_i h_j \cdot M_{ij}}\mper
  \]
\end{fact}
We will also need a theorem from \cite{DBLP:conf/focs/MaSS16}.
\begin{fact}[Corollary 6.6 in \cite{DBLP:conf/focs/MaSS16}]\label{fact:mss-contraction}
  Let $T \in \R^p \tensor \R^q \tensor \R^r$ be an order-$3$ tensor.
  Let $g \sim \cN(0,\Sigma)$ for some $0 \preceq \Sigma \preceq \Id_r$.
  Then for any $t \geq 0$,
  \[
    \Pr_g \left \{ \Norm{(\Id \tensor \Id \tensor g)^\top T}_{ \{1\}, \{2\} } \geq t \cdot \max \left \{ \|T\|_{\{1\}, \{2,3\}}, \|T\|_{\{2\}, \{1,3\}} \right \} \right \} \leq 2(p+q) \cdot e^{-t^2/2}\mcom
    \]
  and consequently,
  \[
    \E_g \Brac{ \Norm{(\Id \tensor \Id \tensor g)^\top T}_{ \{1\}, \{2\} } } \leq O(\log(p + q))^{1/2} \cdot \max \left \{ \|T\|_{\{1\}, \{2,3\}}, \|T\|_{\{2\}, \{1,3\}} \right \}
    \]
\end{fact}

\begin{proof}[Proof of Lemma~\ref{lem:tdecomp-spherical-reweigh}]
  We expand the matrix $\pE \iprod{g,x}^2 xx^\top$ as
  \[
    \pE \iprod{g,x}^2 xx^\top = \sum_{i \in [n]} g_i^2 \pE x_i^2 xx^\top + \sum_{i \neq j \in [n]} g_i g_j \cdot \pE x_i x_j xx^\top\mper
  \]
  Addressing the first term, by standard concentration, $\E \max_{i \in [n]} g_i^2 = O(\log n)$.
  So,
  \[
    \E_g \Normop{ \sum_{i \in [n]} g_i^2 \pE x_i^2 xx^\top} \leq \E_g \Brac{\max_{i \in [n]} g_i^2 \cdot \Normop{\pE \|x\|^2 xx^\top }} = O(\log n) \cdot \Normop{\pE xx^\top} = O(c \cdot \log n)\mper
  \]
The second term we will decouple using Fact~\ref{fact:decoupling}.
  \[
    \E_g \Normop{\sum_{i \neq j} g_i g_j \cdot \pE x_i x_j xx^\top } \leq O(1) \cdot \E_{g,h} \Normop{\sum_{i \neq j} g_i h_j \cdot \pE x_i x_j xx^\top }\mper
  \]
  We add some aditional terms to the sum; by similar reasoning to our bound on the first term they do not contribute too much to the norm.
  \[
    \E_{g,h}\Normop{\sum_{i \neq j} g_i h_j \cdot \pE x_i x_j xx^\top } \leq O(1) \cdot \E_{g,h} \Normop{\sum_{i, j \in [n]} g_i h_j \cdot \pE x_i x_j xx^\top } + O(c \cdot \log n)\mper
  \]
  We can rewrite the matrix in the first term on the right-hand side as
  \[
    \sum_{i, j \in [n]} g_i h_j \cdot \pE x_i x_j xx^\top  = \pE \iprod{g,x}\iprod{h,x}xx^\top\mper
  \]
Now we can apply Fact~\ref{fact:mss-contraction} twice in a row; first to $g$ and then to $h$, which together with our norm bound on $\E xx^\top \tensor xx^\top$, gives
  \[
    \E_{g,h} \Normop{\pE \iprod{g,x}\iprod{h,x}xx^\top} \leq O(c \cdot \log n)\mper
  \]
  Putting all of the above together, we get the lemma.
\end{proof}

Next we prove Lemma~\ref{lem:tdecomp-nonspherical-reweigh} as a corollary of Lemma~\ref{lem:tdecomp-nonspherical-reweigh} which applies to random contractions which are non-spherical.
The proof technique is very similar to that for Fact~\ref{fact:mss-contraction}.
\begin{proof}[Proof of Lemma~\ref{lem:tdecomp-nonspherical-reweigh}]
  Let $h \sim \cN(0,\Id - \Sigma)$ be independent of $g$, and define $g' = g + h$ and $g'' = g - h$, so that $g = \tfrac 1 2 (g' + g'')$.
  It is sufficient to bound $\E_{g,h} \Normop{\pE \iprod{g' + g'', x}^2 xx^\top}$.
  Expanding and applying triangle inequality,
  \[
    \E_{g,h} \Normop{\pE \iprod{g' + g'', x}^2 xx^\top} \leq \E_{g,h} \Normop{\pE \iprod{g',x}^2 xx^\top} + 2\E_{g,h} \Normop{\pE \iprod{g',x}\iprod{g'',x}xx^\top} + \E_{g,h} \Normop{\pE \iprod{g'',x}^2 xx^\top}\mper
  \]
  The first and last terms are $O(c \cdot \log n)$ by Lemma~\ref{lem:tdecomp-spherical-reweigh}.
  For the middle term, consider the quadratic form of the matrix $\pE \iprod{g',x}\iprod{g'',x}xx^\top$ on a vector $v \in \R^n$:
  \begin{align*}
    \pE \iprod{g',x}\iprod{g'',x}\iprod{x,v}^2 \leq \pE \iprod{g',x}^2 \iprod{x,v}^2 + \pE \iprod{g'',x}^2 \iprod{x,v}^2
  \end{align*}
  by pseudoexpectation Cauchy-Schwarz.
  Thus for every $g',g''$,
  \[
\Normop{\pE \iprod{g',x}\iprod{g'',x}xx^\top} \leq \Normop{\pE \iprod{g',x}^2 xx^\top} + \Normop{\pE \iprod{g'',x}^2 xx^\top}\mper
  \]
  Together with Lemma~\ref{lem:tdecomp-spherical-reweigh} this concludes the proof.
\end{proof}

\subsection{Lifting 3-tensors to 4-tensors}
\label{sec:3-to-4}
\begin{problem}[3-to-4 lifting]
  Let $a_1,\ldots,a_m \in \R^n$ be orthonormal.
  Let $A_3 = \sum_{i=1}^m a_i^{\tensor 3}$ and $A_4 = \sum_{i=1}^m a_i^{\tensor 4}$.
  Let $B \in \R^{n \times n \times n}$ satisfy $\iprod{B,A_3} \geq \delta \cdot \|A_3\| \cdot \|B\|$.\\
  \noindent \textbf{Input:} The tensor $B$.\\
  \noindent \textbf{Goal:} Output $B'$ satisfying $\iprod{B',A_4} \geq \delta^{O(1)} \cdot \|A_4\| \cdot \|B'\|$.
\end{problem}

\begin{theorem}
\label{thm:3-to-4-lifting}
  There is a polynomial time algorithm, using the sum of squares method, which solves the 3-to-4 lifting problem.
\end{theorem}

\begin{proof}
\textbf{Small $\delta$ regime: $\delta < 1 - \Omega(1)$: }
  The algorithm is to output the fourth moments of the optimizer of the following convex program.
  \begin{align*}
    \min_{\pE} \quad & \|\pE x^{\tensor 3}\|\\
    \text{s.t. } \quad & \pE \text{ is degree-$4$}\\
    & \pE \text{ satisfies } \{\|x\|^2 = 1\}\\
    & \iprod{\pE x^{\tensor 3}, B} \geq \frac{\delta \|B\|}{\sqrt m}\\
    & \|\pE x^{\tensor 4}\| \leq \frac 1 {\sqrt m}\mper
  \end{align*}
  To analyze the algorithm we apply Theorem~\ref{thm:correlation-preserving-projection}.
  Let $\cC$ be the set of degree-$4$ pseudodistributions satisfying $\{ \|x\|^2 = 1\}$ and having $\|\pE x^{\tensor 4}\| \leq 1/\sqrt{m}$.
  The uniform distribution over $a_1,\ldots,a_m$, whose third and fourth moments are $\tfrac 1m A_3$ and $\tfrac 1m A_4$, respectively, is in $\cC$.

  Let $\pE$ be the pseudoexpectation solving the convex program.
  By Theorem~\ref{thm:correlation-preserving-projection},
  \[
    \iprod{\pE x^{\tensor 3}, \tfrac 1m A_3} \geq \frac \delta 2 \cdot \frac 1 {\sqrt m} \cdot \|\pE x^{\tensor 3}\| \geq \frac{\delta^2}{2m}
  \]
  At the same time,
  \[
    \iprod{\pE x^{\tensor 3}, \tfrac 1m A_3} = \frac 1m \sum_{i=1}^m \pE \iprod{x,a_i}^3 \leq \frac 1m \Paren{\pE \sum_{i=1}^m \iprod{x,a_i}^4}^{1/2}
  \]
  by Cauchy-Schwarz.
  Putting these together, we obtain
  \[
  \iprod{\pE x^{\tensor 4}, A_4} = \pE \sum_{i=1}^m \iprod{x,a_i}^4 \geq \delta^4/4\mper
  \]
  Finally, $\|A_4\| \cdot \|\pE x^{\tensor 4}\| \leq 1$ (since we constrained $\|\pE x^{\tensor 4}\| \leq 1/\sqrt m$), which finishes the proof.\\


  \textbf{Large $\delta$ regime: $\delta \geq 1 - o(1)$: } 
  Modify the convex program from the small-$\delta$ regime to project $(B/\|B\|) \cdot 1/\sqrt m$ to same convex set $\cC$.
  The normalization is so that
  \[
  \Norm{(B/\|B\|) \cdot 1/\sqrt m} = \Norm{\tfrac 1m \cdot A_3}\mper
  \]
  The analysis is similar.
\end{proof}