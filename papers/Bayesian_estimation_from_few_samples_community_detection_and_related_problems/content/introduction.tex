%
\interfootnotelinepenalty=10000

\section{Introduction}

\Dnote{}

%
\emph{Bayesian\footnote{Here, ``Bayesian'' refers to the fact that there is a prior distribution over the parameters.} parameter estimation} \cite{wiki:Bayes_estimator} is a basic task in statistics with a wide range of applications, especially for machine learning.
The estimation problems we study have the following form:
For a known joint probability distribution $p(x,\theta)$ over data points $x$ and parameters $\theta$ (typically both high-dimensional objects), nature draws a parameter $\theta\sim p(\theta)$ from its marginal distribution and we observe i.i.d.~samples $x_1,\ldots,x_m \sim p(x\mid \theta)$ from the distribution conditioned on $\theta$.
The goal is to efficiently estimate the underlying parameter $\theta$ from the observed samples $x_1,\ldots,x_m$.

%
A large number of important problems in statistics, machine learning, and average-case complexity fit this description.
Some examples are principal component analysis (and its many variants), independent component analysis, latent Dirichlet allocation, stochastic block models, planted constraint satisfaction problems, and planted graph coloring problems.

%
For example, in stochastic block models the parameter $\theta$ imposes a community structure on $n$ nodes.
In the simplest case, this structure is a partition into two communities.
Richer models support more than two communities and allow nodes to participate in multiple communities.
The samples $x_1,\ldots,x_m$ are edges between the nodes drawn from a distribution $p(x\mid \theta)$ that respects the community structure $\theta$, which typically means that the edge distribution is biased toward endpoints with the same or similar communitity memberships.
Taken together the samples $x_1,\ldots,x_m$ form a random graph $x$ on $n$ vertices that exhibits a latent community structure $\theta$; the goal is to estimate this structure $\theta$.
This problem becomes easier the more samples (i.e., edges) we observe.
The question is how many samples are required such that we can efficiently estimate the community structure $\theta$?
Phrased differently:
how large an average degree of the random graph $x$ do we require to be able to estimate $\theta$?

%
In this work, we develop a conceptually simple meta-algorithm for Bayesian estimation problems.
We focus on the regime that samples are scarce.
In this regime, the goal is to efficiently compute an estimate $\hat \theta$ that is positively correlated with the underlying parameter $\theta$ given as few samples from the distribution as possible.
In particular, we want to understand whether for particular estimation problems there is a difference between the sample size required for efficient and inefficient algorithms (say, exponential vs. polynomial time).
In this regime, we show that our meta-algorithm recovers the best previous bounds for stochastic block models \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15,DBLP:conf/nips/AbbeS16}.
Moreover, for the case of richer community structures like multiple communities and especially overlapping communities, our algorithm achieves significantly stronger recovery guarantees.\footnote{
  If we represent the community structure by $k$ vectors $y_1,\ldots,y_k \in \bits^n$ that indicate community memberships, then previous algorithms \cite{DBLP:conf/nips/AbbeS16} do not aim to recover these vectors but, roughly speaking, only a random linear combination of them.
  While for some settings it is in fact impossible to estimate the individual vectors, we show that in many settings it is possible to estimate them (in particular for symmetric block models).}

%
In order to achieve these improved guarantees, our meta-algorithm draws on several ideas from previous lines of work and combines them in a novel way.
Concretely, we draw on ideas from recent analyses of belief propagation and their use of non-backtracking and self-avoiding random walks \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15,DBLP:conf/nips/AbbeS16}.
We also use ideas from recent works based on the method of moments and tensor decomposition \cite{DBLP:journals/jmlr/AnandkumarGHKT14,DBLP:journals/jmlr/AnandkumarGHK14, DBLP:conf/stoc/BarakKS15}.
Our algorithm also employs convex-programming techniques, namely the sum-of-squares semidefinite programming hierarchy, and gives a new perspective on how these techniques can be used for estimation.\footnote{
  Previously, convex-programming techniques have been used in this context only as a way to obtain efficient relaxations for maximum-likelihood estimators.
  In contrast, our work uses convex programming to drive the method of moments approach and decompose tensors in an entropy maximizing way.
}

%
Our meta-algorithm allows for a tuneable parameter which corresponds roughly to running time.
Under mild assumptions on a Bayesian estimation problem $p(x,\theta)$ (that are in particular satisfied for discrete problems such as the stochastic block model), when this parameter is set to allow the meta-algorithm to run in exponential time, if there is any estimator $\hat{\theta}$ of $\theta$ obtaining correlation $\delta$, the meta-algorithm offers one obtaining correlation at least $\delta^{O(1)}$.
While this parameter does not correspond directly to the \emph{degree} paramter used in convex hiararchies such as sum of squares, the effect is similar to the phenomenon that sum of squares convex programs of exponential size can solve any combinatorial optimization problem exactly.
(Since Bayesian estimation problems do not always correspond to optimization problems, this guarantee would not be obtained by sum of squares in our settings.)

%
For many Bayesian estimation problems there is a critical number of samples $n_0$ such that when the number of samples $n$ is less than $n_0$, computationally-efficient algorithms seem unable to compute good estimators for $\theta$.
This is in spite of the presence of sufficient information to identify $\theta$ (and therefore estimate it with computationally inefficient algorithms), even when $n < n_0$.
Providing rigorous evidence for such \emph{computational thresholds} has been a long-standing challenge.
One popular approach is to prove impossibility of estimating $\theta$ from $n < n_0$ samples using algorithms from some restricted class.
Such results are most convincing the chosen class captures the lowest-sample-complexity algorithms for many Bayesian inference problems, which our meta-algorithm does.\footnote{Recent work in this area has focused on sum of squares lower bounds \cite{DBLP:conf/colt/HopkinsSS15, DBLP:conf/nips/MaW15, DBLP:conf/focs/BarakHKKMP16}. While the sum of squares method is algorithmically powerful, it is not designed to achieve optimal sample guarantees for Bayesian estimation. Lower bounds against our meta-algorithm therefore serve better the purpose of explaining precise computational sample thresholds.}
We prove that in the $k$-community block model, no algorithm captured by our meta-algorithm can tolerate smaller-degree graphs than the best known algorithms.
This provides evidence for a computational phase transition at the \emph{Kesten-Stigum threshold} for stochastic block models.
\Dnote{}

\paragraph{Organization}
In the remainder of this introduction we discuss our results and their relation to previous work in more detail.
In Section~\ref{sec:techniques} (Techniques) we describe the mathematical techniques involved in our meta-algorithm and its analysis, and we illustrate how to apply the meta-algorithm to recover a famous result in the theory of spiked random matrices with a much simplified proof.
In Section~\ref{sec:warmup} (Warmup) we re-prove (up to some loss in the running time) the result of Mossel-Neeman-Sly on the two-community block model as an application of our meta-algorithm, again with very simple proofs.
In Section~\ref{sec:W} (Matrix estimation) we re-interpret the best existing results on the block model, due to Abbe and Sandon, as applications of our meta-algorithm.

In Section~\ref{sec:mm} (Tensor estimation) we apply our meta-algorithm to the mixed-membership block model.
Following that, in Section~\ref{sec:lower-bound} (Lower bounds) we prove that no algorithm captured by our meta-algorithm can recover communities in the block model past the Kesten-Stigum threshold.

In Section~\ref{sec:tdecomp} (Tensor decomposition), which can be read independently of much of the rest of the paper, we give a new algorithm for tensor decomposition and prove its correctness; this algorithm is used by our meta-algorithm as a black box.


%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Meta-algorithm and meta-theorems for Bayesian estimation}
\label{sec:meta-intro}
We first consider a version of the meta-algorithm that is enough to capture the best known algorithms for the stochastic block model with $k$ disjoint communities, which we now define.
Let $\e,d > 0$.
Draw $y$ uniformly from $[k]^n$.
For each pair $i \neq j$, add the edge $\{i,j\}$ to a graph on $n$ vertices with probability $(1 + (1 - \tfrac 1k) \e) \tfrac dn$ if $y_i = y_j$ and $(1- \tfrac \e k) \tfrac dn$ otherwise.
The resulting graph has expected average degree $d$.

A series of recent works has explored the problem of estimating $y$ in these models for the sparsest-possible graphs.
The emerging picture, first conjectured via techniques from statistical physics in the work \cite{DBLP:journals/corr/abs-1109-3041}, is that in the $k$-community block model it is possible to recover a nontrivial estimate of $y$ via a polynomial time algorithm if and only if $d = (1 + \delta)\tfrac{k^2}{\e^2}$ for $\delta \geq \Omega(1)$.
This is called the Kesten-Stigum threshold.
The algorithmic side of this conjecture was confirmed by \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15} for $k=2$ and \cite{DBLP:conf/nips/AbbeS16} for general $k$.

One of the goals of our meta-algorithm is that it apply in a straightforward way even to complex Bayesian estimation problems.
A more complex model (yet more realistic for real-world networks) is the \emph{mixed-membership} block model \cite{DBLP:conf/nips/AiroldiBFX08} which we now define informally.
Let $\alpha \geq 0$ be an overlap parameter.
Draw $y$ from $\binom{k}{t}^n$, where $t = \tfrac{k(\alpha+1)}{k+\alpha} \approx \alpha+1$; that is for each of $n$ nodes pick a set $S_j$ of roughly $\alpha +1$ communities.\footnote{In actuality one draws for each node $i \in [n]$ a probability vector $\sigma_i \in \Delta_{k-1}$ from the Dirichlet distribution with parameter $\alpha$; we describe a nearly-equivalent model here for the sake of simplicity---see Section~\ref{sec:result-block-model} for details. Our guarantees for recovery in the mixed-membership model also apply to the model here because it has the same second moments as the Dirichlet distribution.}
For each pair $i,j$, add an edge to the graph with probability $(1 + (\tfrac{|S_i \cap S_j|}{t^2 } - \tfrac 1k)\e)\tfrac dn$.
(That is, with probability which increases as $i$ and $j$ participate in more communities together.)
In the limit $\alpha \rightarrow 0$ this becomes the $k$-community block model.

Returning to the meta-algorithm (but keeping in mind the block model), let $p(x,y)$ be a joint probability distribution over observable variables $x\in \R^n$ and hidden variables $y\in \R^m$.
Nature draws $(x,y)$ from the distribution $p$, we observe $x$ and our goal is to provide an estimate $\hat y(x)$ for $y$.
Often the mean square error $\E_{p(x,y)} \Norm{\hat y(x)-y}^2$ is a reasonable measure for the quality of the estimation.
For this measure, the information-theoretically optimal estimate is the mean of the posterior distribution $\hat y(x) = \E_{p(y \mid x)} y$.
This approach has two issues that we address in the current work.

The first issue is that naively computing the mean of the posterior distribution takes time exponential in the dimension of $y$.
For example, if $y \in \set{\pm 1}^m$, then $\E_{p( y \mid x)} y = \sum_{y \in \set{\pm 1}^m} y \cdot p(y \mid x)$; there are $2^m$ terms in this sum.
There are many well-known algorithmic approaches that aim to address this issue or related ones, for example, belief propagation \cite{gallager1962low, Pearl} or expectation maximization \cite{dempster1977maximum}.
While these approaches appear to work well in practice, they are notoriously difficult to analyze.

In this work, we can resolve this issue in a very simple way:
We analytically determine a low-degree polynomial $f(x)$ so that $\E_{p(x,y)} \Norm{f(x)-y}^2$ is as small as possible and use the fact that low-degree polynomials can be evaluated efficiently (even for high dimensions $n$).\footnote{Our polynomials typically have logarithmic degree and naive evaluation takes time $n^{O(\log n)}$.
However, we show that under mild conditions it is possible to approximately evaluate these polynomials in polynomial time using the idea of color coding \cite{DBLP:journals/jacm/AlonYZ95}.}
Because the maximum eigenvector of an $n$-dimensional linear operator with a spectral gap is an $O(\log n)$-degree polynomial of its entries, our meta-algorithm captures spectral properties of linear operators whose entries are low-degree polynomials of observable variables $x$.
Examples of such operators include adjacency matrices (when $x$ is a graph), empirical covariance matrices (when $x$ is a list of vectors), as well as more sophisticated objects such as linearized belief propagation operators (e.g., \cite{DBLP:conf/focs/AbbeS15}) and the Hashimoto non-backtracking operator.

The second issue is that even if we could compute the posterior mean exactly, it may not contain any information about the hidden variable $y$ and the mean square error is not the right measure to assess the quality of the estimator.
This situation typically arises if there are symmetries in the posterior distribution.
For example, in the stochastic block model with two communities we have $\E_{p(y \mid x)} y =0$ regardless of the observations $x$ because $p(y \mid x)=p(-y | x)$.
A simple way to resolve this issue is to estimate higher-order moments of the hidden variables.
For stochastic block models with disjoint communities, the second moment $\E_{p(y \mid x)} \dyad y$ would suffice.
(For overlapping communities, we need third moments $\E_{p(y \mid x)} y^{\otimes 3}$ due to more substantial symmetries.)

For now, we think of $y$ as an $m$-dimensional vector and $x$ as an $n$-dimensional vector (in the blockmodel on $N$ nodes, this would correspond to $m \approx kN$ and $n = N^2$).
Our algorithms follow a two-step strategy:
\begin{compactenum}
\item Given $x \sim p(x|y)$, evaluate a fixed, low-degree polynomial $P(x)$ taking values in $(\R^m)^{\tensor \ell}$.
(Usually $\ell$ is $2$ or $3$.)
\item Apply a robust eigenvector or semidefinite-programming based algorithm (if $\ell = 2$), or a robust tensor decomposition algorithm (if $\ell = 3$ or higher) to $P$ to obtain an estimator $\hat{y}$ for $y$.
\end{compactenum}
The polynomial $P(x)$ should be an optimal low-degree approximation to $y^{\tensor \ell}$, in the following sense:
if $n$ is sufficiently large that some low-degree polynomial $Q(x)$ has constant correlation with $y^{\tensor \ell}$
\[
\E_{x,y} \iprod{Q, y^{\tensor \ell}} \geq \Omega(1) \cdot (\E_x \|Q\|^2)^{1/2} (\E \|y^{\tensor \ell}\|^2)^{1/2}\mcom
\]
then $P$ has this guarantee.
(The inner products and norms are all Euclidean.)

A prerequisite for applying our meta-algorithm to a particular inference problem $p(x, y)$ is that it be possible to estimate $y$ given $\E \Brac{y^{\tensor \ell} \mid x}$ for some constant $\ell$.
For such a problem, the optimal Bayesian inference procedure (ignoring computational constraints) can be captured by computing $F(x) = \E \Brac{y^{\tensor \ell} \mid x}$, then using it to estimate $y$.
When $p(x,y)$ is such that it is information-theoretically possible to estimate $y$ from $x$, these posterior moments will generally satisfy $\E \iprod{F(x), y^{\tensor \ell}} \geq \Omega(1) \cdot (\E \|F(x)\|^2)^{1/2} (\E \|y^{\tensor \ell}\|^2)^{1/2}$, for some constant $\ell$.
Our observation is that when $F$ is approximately a low-degree function, this estimation procedure can be carried out via an efficient algorithm.

\paragraph{Matrix estimation and prior results for block models}
In the case $\ell = 2$, where one uses the covariance $\E\Brac{\dyad y \mid x}$ to estimate $y$, the preceding discussion is captured by the following theorem.

\begin{theorem}[Bayesian estimation meta-theorem---2nd moment]
  \label{thm:meta-theorem-2nd}
  Let $\delta>0$ and $p(x,y)$ be a distribution over vectors $x\in \bits^n$ and unit vectors $y\in \R^d$.
  Assume $p(x)\ge \cramped{2^{-\cramped{n^{O(1)}}}}$ for all $x\in \bits^n$.\footnote{This mild condition on the marginal distribution of $x$ allows us to rule out pathological situations where a low-degree polynomial in $x$ may be hard to evaluate accurately enough because of coefficients with super-polynomial bit-complexity.}
  Suppose there exists a matrix-valued degree-$D$ polynomial $P(x)$ such that
  \begin{equation}
    \label{eq:second-moment-correlation}
    \E_{p(x,y)} \iprod{P(x),\dyad y} \ge \delta\cdot \Paren{\E_{p(x)} \norm{P(x)}^2_F}^{1/2}\,.
  \end{equation}
  Then, there exists $\delta'\ge \delta^{O(1)}>0$ and an estimator $\hat y(x)$ computable by a circuit of size $n^{O(D)}$ such that
  \begin{equation}
     \E_{p(x,y)} \iprod{\hat y(x), y}^2 \ge \delta'\,.
  \end{equation}
\end{theorem}
To apply this theorem to the previously-discussed setting of samples $x_1,\ldots,x_N$ generated from $p(x \, | \, y)$, assume the samples $x_1,\ldots,x_N$ are in some fixed way packaged into a single $n$-length vector $x$.

One curious aspect of the theorem statement is that it yields a nonuniform algorithm---a family of circuits---rather than a uniform algorithm.
If the coefficients of the polynomial $P$ can themselves be computed in polynomial time, then the conclusion of the algorithm is that an $n^{O(D)}$-time algorithm exists with the same guarantees.

As previously mentioned, the meta-algorithm has a parameter $D$, the degree of the polynomial $P$.
If $D = n$, then whenever it is information-theoretically possible to estimate $y$ from $\E[ yy^\top  \, | \, x]$, the meta-algorithm can do so (in exponential time).
This follows from the fact that every function in $n$ Boolean variables is a polynomial of degree at most $n$.
It is also notable that, while a degree $D$ polynomial can be evaluated by an $n^{O(D)}$-size circuit, some degree-$D$ polynomials can be evaluated by much smaller circuits.
We exploit such polynomials for the block model (computable via \emph{color coding}), obtaining $n^{O(1)}$-time algorithms from degree $\log n$ polynomials.
By using very particular polynomials, which can be computed via powers of \emph{non-backtracking operators}, previous works on the block model are able to give algorithms with near-linear running times \cite{DBLP:conf/stoc/MosselNS15, DBLP:conf/nips/AbbeS16}.\footnote{In this work we choose to work with \emph{self-avoiding} walks rather than non-backtracking ones; while the corresponding polynomials cannot to our knowledge be evaluated in near-linear time, the analysis of these polynomials is much simpler than the analysis needed to understand non-backtracking walks. This helps to make the analysis of our algorithms much simpler than what is required by previous works, at the cost of large polynomial running times. It is an interesting question to reduce the running times of our algorithm for the mixed-membership block model to near-linear via non-backtracking walks, but since our aim here is to distinguish what is computable in polynomial time versus, say, exponential time, we do not pursue that improvement here.}

Using the appropriate polynomial $P$, this theorem captures the best known guarantees for partial recovery in the $k$-community stochastic block model.
Via the same polynomial, applied in the mixed-membership setting, it also yields our first nontrivial algorithm for the mixed-membership model.
However, as we discuss later, the recovery guarantees are weak compared to our main theorem.

Recalling the $\e,d,k$ block model from the previous section, let $y \in \R^n$ be the centered indicator vector of, say, community $1$.
\begin{theorem}[Implicit in \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15, DBLP:conf/nips/AbbeS16}, special case of our main theorem, Theorem~\ref{thm:mm-intro}]
  \label{thm:mm-intro-matrix}
  Let $\delta \defeq 1 - \tfrac{k^2(\alpha+1)^2}{\e^2 d}$.
  If $x$ is sampled according to the $n$-node, $k$-community, $\e$-biased, $\alpha$-mixed-membership block model with average degree $d$ and $y$ is the centered indicator vector of community $1$, there is a $n\times n$-matrix valued polynomial $P$ of degree $O(\log n)/\delta^{O(1)}$ such that
  \[
  \E_x \iprod{P(x), \dyad y} \geq \Paren{\frac \delta {k (\alpha+1)}}^{O(1)} (\E \|P(x)\|^2)^{1/2} (\E \|\dyad y\|^2)^{1/2}\mper
  \]
\end{theorem}
Together with Theorem~\ref{thm:meta-theorem-2nd}, up to questions of $n^{O(\log n)}$ versus $n^{O(1)}$ running times, when $\alpha \rightarrow 0$ this captures the previous best efficient algorithms for the $k$-community block model.
(Once one has a unit vector correlated with $y$, it is not hard to approximately identify the vertices in community $1$.)
While the previous works \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15, DBLP:conf/nips/AbbeS16} did not consider the mixed-membership blockmodel, this theorem is easily obtained using techniques present in those works (as we show when we rephrase those works in our meta-algorithm, in Section~\ref{sec:W}).\footnote{In fact, if one is willing to lose an additional $2^{-k}$ in the correlation obtained in this theorem, one can obtain a similar result for the mixed-membership model by reducing it to the disjoint-communities with $K \approx 2^k$ communities, one for each subset of $k$ communities. This works when each node participates in a subset of communities; if one uses the Dirichlet version of the mixed-membership model then suitable discretization would be necessary.}

\paragraph{Symmetries in the posterior, tensor estimation, and improved error guarantees}
We turn next to our main theorem on the mixed-membership model, which offers substantial improvement on the correlation which can be obtained via Theorem~\ref{thm:mm-intro-matrix}.
The matrix-based algorithm discussed above, Theorem~\ref{thm:mm-intro-matrix}, contains a curious asymmetry; namely the arbitrary choice of community $1$.
The block model distributions are symmetric under relabeling of the communities, which means that any estimator $P(x)$ of $\dyad y$ is also an estimator of $\dyad {y'}$, where $y'$ is the centered indicator of community $j > 1$.
Since one wants to estimate all the vectors $y_1,\ldots,y_k$ (with $y_i$ corresponding to the $i$-th community), it is more natural to consider the polynomial $P$ to be an estimator of the matrix $M = \sum_{i \in [k]} \dyad{y_i}$.\footnote{In more general versions of the blockmodel studied in \cite{DBLP:conf/nips/AbbeS16}, where each pair $i,j$ of communities may have a different edge probability $Q_{ij}$ it is not always possible to estimate all of $y_1,\ldots,y_k$.
We view it as an interesting open problem to extract as much information about $y_1,\ldots,y_k$ as possible in that setting; the guarantee of \cite{DBLP:conf/nips/AbbeS16} amounts, roughly, to finidng a single vector in the linear span of $y_1,\ldots,y_k$.}
Unsurprisingly, $P$ is a better estimator of $M$ than it is of $y_1$.
In fact, with the same notation as in the theorems,
\[
  \E_{x,y} \iprod{P(x), M(y)} \geq \delta^{O(1)} (\E \|P(x)\|^2)^{1/2} (\E \|M(y)\|^2)^{1/2}\mcom
\]
removing the $k^{O(1)}$ factor in the denominator.
This guarantee is stronger: now the error in the estimator depends only on the distance $\delta$ of the parameters $\e,d,k,\alpha$ from the critical threshold $\tfrac{k^2(\alpha+1)^2}{\e^2 d} = 1$ rather than additionally on $k$.

If given the matrix $M$ exactly, one way to extract an estimator $\hat{y_i}$ for some $y_i$ is just to sample a random unit vector in the span of the top $k$ eigenvectors of $M$.
Such an estimator $\hat{y_i}$ would have $\E \iprod{\hat{y_i},y_i}^2 \geq \tfrac 1 {k^{O(1)}} \|y_i\|$, recovering the guarantees of the theorems above but not offering an estimator $\hat{y_i}$ whose distance to $y_i$ depends only on the distance $\delta$ above the critical threshold.
Indeed, without exploiting additional structure of the vectors $y_i$ is unclear how to remove this $1/k^{O(1)}$ factor.
As a thought experiment, if one had the matrix $M' = \sum_{i \leq k} \dyad{a_i}$, where $a_1,\ldots,a_k$ were random unit vectors, then $a_1,\ldots,a_k$ would be nearly orthonormal and one could learn essentially only their linear span.
(From the linear span it is only possible to find $\hat{a_i}$ with correlation $\iprod{\hat{a_i}, a_i}^2 \geq 1/k^{O(1)}$.)

In the interest of generality we would like to avoid using such additional structure: while in the disjoint-community model the vectors $y_i$ have disjoint support (after un-centering them), no such special structure is evident in the mixed-membership setting.
Indeed, when $\alpha$ is comparable to $k$, the vectors $y_i$ are similar to independent random vectors of the appropriate norm.

To address this issue we turn to tensor methods.
To illustrate the main idea simply: if $a_1,\ldots,a_k$ are orthonormal, then it is possible to recover $a_1,\ldots,a_k$ exactly from the $3$-tensor $T = \sum_{i \leq k} a_i^{\tensor 3}$.
More abstractly, the meta-algorithm which uses $3$rd moments is able to estimate hidden variables whose posterior distributions have a high degree of symmetry, without errors which worsen as the posteriors become more symmetric.

\begin{theorem}[Bayesian estimation meta-theorem---3rd moment]
  \label{thm:meta-theorem-3rd}
  Let $p(x,y_1,\ldots,y_k)$ be a joint distribution over vectors $x\in \bits^n$ and exchangable,\footnote{
    Here, exchangeable means that for every $x\in \bits^n$ and every permutation $\pi\from [k]\to [k]$, we have $p(y_1,\ldots,y_k\mid x)=p(y_{\pi(1)},\ldots,y_{\pi(k)} \mid x)$.
  } orthonormal\footnote{
    Here, we say the vector-valued random variables $y_1,\ldots,y_k$ are orthonormal if with probability $1$ over the distribution $p$ we have  $\iprod{y_i,y_j}=0$ for all $i\neq j$ and $\norm{y_i}^2=1$.
  } vectors $y_1,\ldots,y_k\in \R^d$.
  Assume the marginal distribution of $x$ satisfies $p(x)\ge 2^{-n^{O(1)}}$ for all $x\in \bits^n$.\footnote{As in the previous theorem, this mild condition on the marginal distribution of $x$ allows us to rule out pathological situations where a low-degree polynomial in $x$ may be hard to evaluate accurately enough because of coefficients with super-polynomial bit-complexity.}
  Suppose there exists a tensor-valued degree-$D$ polynomial $P(x)$ such that
  \begin{equation}
    \label{eq:third-moment-correlation}
    \E_{p(x,y_1,\ldots,y_k)} \iprod{P(x),\sum_{i=1}^k y_i^{\otimes 3}}\ge \delta \cdot \Paren{\E_{p(x)} \norm{P(x)}^2}^{1/2} \cdot \sqrt k\,.
  \end{equation}
  (Here, $\norm{\cdot}$ is the norm induced by the inner product $\iprod{\cdot,\cdot}$.
  The factor $\sqrt k$ normalizes the inequality because $\norm{\sum_{i=1}^k y_{i}^{\otimes 3}}=\sqrt k$ by orthonormality.)
  Then, there exists $\delta'\ge \delta^{O(1)}>0$ and a circuit of size $n^{O(D)}$ that given $x\in \bits^n$ outputs a list of unit vectors $z_1,\ldots,z_m$ with $m \leq n^{\poly(1/\delta)}$ so that
  \begin{equation}
    \E_{p(x,y_1,\ldots,y_k)} \E_{i \sim [k]} \max_{j \in [m]} \iprod{y_i, z_j}^2 \geq \delta'\,.
  \end{equation}
\end{theorem}
That the meta-algorithm captured by this theorem outputs a list of $n^{1/\poly(\delta)}$ vectors rather than just $k$ vectors is an artifact of the algorithmic difficulty of multilinear algebra as compared to linear algebra.
However, in most Bayesian estimation problems it is possible by using a very small number of additional samples (amounting to a low-order additive term in the total sample complexity) to cross-validate the vectors in the list $z_1,\ldots,z_m$ and throw out those which are not correlated with some $y_1,\ldots,y_k$.
Our eventual algorithm for tensor decomposition (see Section~\ref{sec:tdecomp-intro} and Section~\ref{sec:tdecomp}) bakes this step in by assuming access to an oracle which evaluates the function $v \mapsto \sum_{i \in [k]} \iprod{v,y_i}^4$.

A key component of the algorithm underlying Theorem~\ref{thm:meta-theorem-3rd} is a new algorithm for very robust orthogonal tensor decomposition.\footnote{An orthogonal $3$-tensor is $\sum_{i=1}^m a_i^{\tensor 3}$, where $a_1,\ldots,a_m$ are orthonormal.}
Previous algorithms for tensor decomposition require that the input tensor is close (in an appropriate norm) to only one orthogonal tensor.
By contrast, our tensor decomposition algorithm is able to operate on a tensor $T$ which is just $\delta \ll 1$ correlated to the orthogonal tensor $\sum y_i^{\tensor 3}$, and in particular might also be $\delta$-correlated with $1/\delta$ other orthogonal tensors.
If one views tensor decomposition as a \emph{decoding} task, taking a tensor $T$ and decoding it into its rank-one components, then our guarantees are analogous to list-decoding.
Our algorithm in this setting involves a novel entropy-maximization program which, among other things, ensures that given a tensor $T$ which for example is $\delta$-correlated with two distinct orthogonal tensors $A$ and $B$, the algorithm produces a list of vectors correlated with both the components of $A$ and those of $B$.

Applying this meta-theorem (plus a simple cross-validation scheme to prune the vectors in the $n^{1/\poly(\delta)}$-length list) to the mixed-membership block model (and its special case, the $k$-disjoint-communities block model) yields the following theorem.
(See Section~\ref{sec:result-block-model} for formal statements.)
\begin{theorem}[Main theorem on the mixed-membership block model, informal]
\label{thm:mm-intro}
  Let $\e,d,k,\alpha$ be paramters of the mixed-membership block model, and let $\delta = 1 - \tfrac{k^2(\alpha+1)^2}{\e^2 d} \geq \Omega(1)$.
  Let $y_i$ be the centered indicator vector of the $i$-th community.
  There is an $n^{1/\poly(\delta)}$-time algorithm which, given a sample $x$ from the $\e,d,k,\alpha$ block model, recovers vectors $\hat y_1(x),\ldots,\hat y_k(x)$ such that there is a permutation $\pi \, : [k] \rightarrow [k]$ with
  \[
    \E \iprod{\hat y_{\pi(i)}, y_i}^2 \geq \delta^{O(1)} (\E\|\hat y_{\pi(i)}\|^2)^{1/2} (\E \|y_i\|^2)^{1/2}\mper
  \]
\end{theorem}
The eventual goal, as we discuss in Section~\ref{sec:result-block-model}, is to label each vertex by a probability vector $\tau_i$ which is correlated with the underlying label $\sigma_i$, but given the $\hat y$ vectors from this theorem this is easily accomplished.

\paragraph{Comparison to the sum of squares method}
The sum of squares method has been recently been a popular approach for designing algorithms for Bayesian estimation \cite{DBLP:conf/stoc/BarakKS15, DBLP:conf/colt/HopkinsSS15, DBLP:journals/corr/RaghavendraRS16, DBLP:conf/approx/GeM15}.
The technique works best in settings where the maximum-likelihood estimator can be phrased as a polynomial optimization problem (subject to semialgebraic constraints).
Then the strategy is to use the sum of squares method to obtain a strong convex relaxation of the maximum-likelihood problem, solve this relaxation, and round the result.

This strategy has been quite successful, but thus far it does not seem to allow the sharp (up to low-order additive terms) sample-complexity guarantees we study here.
(Indeed, for some problems, including the stochastic block model, it is not clear that maximum likelihood estimation recovers those guarantees, much less the SoS-relaxed version.)

One similarity between our algorithms and these applications of sum of squares is that the rounding procedures used at the end often involve tensor decomposition, which is itself often done via the sum of squares method.
We do employ the SoS algorithm as a black box to solve tensor decomposition problems for versions of our algorithm which use higher moments.

Recent works on SoS show that the low-degree polynomials computed by our meta-algorithm are closely connected to \emph{lower bounds} for the SoS hierarchy, though this connection remains far from fully understood.
The recent result \cite{DBLP:conf/focs/BarakHKKMP16} on the planted clique problem first discovered this connection.
The work \cite{soslb} (written concurrently with the present paper) shows that this connection extends far beyond the planted clique setting.

\paragraph{Comparison to the method of moments}
Another approach for designing statistical estimators for provable guarantees is the method of moments.
Typically one considers parameters $\theta$ (which need not have a prior distribution $p(\theta)$) and iid samples $x_1,\ldots,x_n \sim p(x | \theta)$.
Generally one shows that the moments of the distribution $\{x | \theta\}$ are related to some function of $\theta$: for example perhaps $\E[xx^\top \, | \, \theta] = f(\theta)$.
Then one uses the samples $x_i$ to estimate the moment $M = \E[xx^\top \, | \, \theta]$, and finally to estimate $\theta$ by $f^{-1}(M)$.

While the method of moments is quite flexible, for the high-noise problems we consider here it is not clear that it can achieve optimal sample complexity.
For example, in our algorithms (and existing sample-optimal algorithms for the block model) it is important to exploit the flexibility to compute any polynomial of the samples jointly---given $n$ samples our algorithms can evaluate a polynomial $P(x_1,\ldots,x_n)$, and $P$ often will not be an empirical average of some simpler function like $\sum_{i \leq n} q(x_i)$.
The best algorithm for the mixed-membership block model before our work uses the method of moments and consequently requires much denser graphs than our method \cite{DBLP:journals/jmlr/AnandkumarGHK14}.

%
%
%
%
%
%
%

%
%

%

\Dcomment{}

\Dnote{}

\Dcomment{}

\Dcomment{}

\Dcomment{}

\Dcomment{}

%
%


%
%
%
%
%

%


%
%
%
%
%

%
%
%

%

%

%

%

%
%


%
%
%
%
%
%

%
%


%

%

%

%

%

%

%
  
%

%
%


%
%
%
%
%


\subsection{Detecting overlapping communities}
\label{sec:result-block-model}
We turn now to discuss our results for stochastic block models in more detail and compare them to the existing literature.
\Dnote{}

The stochastic block model is a widely studied (family of) model(s) of random graphs containing latent community structure.
It is most common to study the block model in the sparse graph setting: many large real-world networks are sparse, and the sparse graph setting is nearly always more mathematically challenging than the dense setting.
A series of recent works has for the first time obtained algorithms which recover communities in block model graphs under (conjecturally) optimal sparsity conditions.
For an excellent survey, see \cite{DBLP:journals/corr/Abbe17}.

Such sharp results remain limited to relatively simple versions of the block model; where, in particular, each vertex is assigned a single community in an iid fashion.
A separate line of work has developed more sophisticated and realistic random graph models with latent community structure, with the goal of greater applicability to real-life networks.
The mixed-membership stochastic block model \cite{DBLP:conf/nips/AiroldiBFX08} is one such natural extension of the stochastic block model that allows for communities to overlap, as they do in large networks found in the wild.

In addition to the number of vertices $n$, the average degree $d$, the correlation parameter $\e$, and the number of communities $k$, this model has an overlap parameter $\alpha\ge 0$ that controls how many communities a typical vertex participates in.
Roughly speaking, the model generates an $n$-vertex graph by choosing $k$ communities as random vertex subsets of size $(1+\alpha)n/k$ and choosing $d n /2$ random edges, favoring pairs of vertices that have many communities in common.

\begin{definition}[Mixed-membership stochastic block model]
  \label{def:mixed-membership-sbm}
The mixed-membership stochastic block model $\sbm(n,d,\e,k,\alpha)$  is the following distribution over $n$-vertex graphs $G$ and $k$-dimensional probability vectors $\sigma_1,\ldots,\sigma_n$ for the vertices:
\begin{compactitem}
\item draw $\sigma_1,\ldots,\sigma_n$ independently from $\Dir(\alpha)$ the symmetric $k$-dimensional Dirichlet distribution with parameter $\alpha\ge 0$,\footnote{In the symmetric $k$-dimensional Dirichlet distribution with parameter $\alpha> 0$, the probability of a probability vector $\sigma$ is proportional to $\prod_{t=1}^k \sigma(t)^{\alpha/k - 1 }$. By passing to the limit, we define $\Dir(0)$ to be the uniform distribution over the coordinate vectors $\Ind_1,\ldots,\Ind_k$.}
\item for every potential edge $\set{i,j}$, add it to $G$ with probability $\tfrac d n \cdot \Bigparen{1 + \bigparen{\iprod{\sigma_i,\sigma_j}-\tfrac 1k}\e }$.
\end{compactitem}
\end{definition}
Due to symmetry, $\iprod{\sigma_i,\sigma_j}$ has expected value $\tfrac 1k$, which means that the expected degree of every vertex in this graph is $d$.
In the limit $\alpha \to 0$, the Dirichlet distribution is equivalent to the uniform distribution over coordinate vectors $\Ind_1,\ldots,\Ind_k$ and the model becomes $\sbm(n,d,\e,k)$, the stochastic block model with $k$ \emph{disjoint} communities.
For $\alpha=k$, the Dirichlet distribution is uniform over the open $(k-1)$-simplex \cite{wiki:Dirichlet_distribution}.
For general values of $\alpha$, a probability vector from $\Dir(\alpha)$ turns out to have expected collision probability $(1-\tfrac 1k)\tfrac {1}{\alpha+1} + \tfrac 1 k$, which means that  we can think of the probability vector being concentrated on about $\alpha+1$ coordinates.\footnote{When $k$ and $\alpha$ are comparable in magnitude, it is important to interpret this more accurately as $(\alpha + 1) \cdot \tfrac k {k+\alpha}$ coordinates.}
This property of the Dirichlet distribution is what determines the threshold for our algorithm.
Correspondingly, our algorithm and analysis extends to a large class of distributions over probability vectors that share this property.

\paragraph{Measuring correlation with community structures}
In the constant-average-degree regime of the block model, recovering the label of every vertex correctly is information-theoretically impossible.
For example, no information is present in a typical sample about the label of any isolated vertex, and in a typical sample a constant fraction of the vertices are isolated.
Instead, at least in the $k$-disjoint-community setting, normally one looks to label vertices by labels $1,\ldots,k$ so that (up to a global permutation), this labeling has positive correlation with the true community labels.

When the communities are disjoint, one can measure such correlation using the sizes of $|S_j \cap \widehat{S}_j|$, where $S_j \subseteq [n]$ is the set of nodes in community $j$ and $\widehat{S}_j$ is an estimated set of nodes in community $j$.
The original definition of \emph{overlap}, the typical measure of labeling-accuracy in the constant-degree regime, takes this approach \cite{DBLP:journals/corr/abs-1109-3041}.

For present purposes this definition must be somewhat adapted, since in the mixed-membership block model there is no longer a good notion of a discrete set of nodes $S_j$ for each community $j \in [k]$.
We define a smoother notion of correlation with underlying community labels to accommodate that the labels $\sigma_i$ are vectors in $\Delta_{k-1}$.
In discrete settings, for example when $\alpha \rightarrow 0$ (in which case one recovers the $k$-disjoint-community model), or more generally when each $\sigma_i$ is the uniform distribution over some number of communities, our correlation measure recovers the usual notion of overlap.

Let $\sigma=(\sigma_1,\ldots,\sigma_n)$ and $\tau = (\tau_1,\ldots,\tau_n)$ be labelings of the vertices $1,\ldots,n$ by by $k$-dimensional probability vectors.
We define the \emph{correlation} $\corr(\sigma,\tau)$ as
\begin{equation}
\max_{\pi} \E_{i \sim n} \iprod{\sigma_i,\tau_{\pi(i)}} - \frac 1k \label{eq:mixed-membership-correlation}
\end{equation}
where $\pi$ ranges over permutations of the $k$ underlying communities.
This notion of correlation is closely related to the \emph{overlap} of the distributions $\sigma_i, \tau_i$.

To illustrate this notion of correlation, consider the case of disjoint communities (i.e., $\alpha=0$), where the ground-truth labels $\tau_i$ are indicator vectors in $k$ dimensions.
Then, if $\E_i \iprod{\sigma_i, \tau_{\pi(i)}} - \tfrac 1k > \delta$, by looking at the large coordinates of $\sigma_i$ it is possible to correctly identify the community memberships of a $\delta^{O(1)} + \tfrac 1k$ fraction of the vertices, which is a $\delta^{O(1)}$ fraction more than would be identified by randomly assigning labels to the vertices without looking at the graph.

When the ground truth labels $\tau_i$ are spread over more than one coordinate---say, for example, they are uniform over $t$ coordinates---the best recovery algorithm cannot find $\sigma$'s with correlation better than 
\[
\corr(\sigma,\tau) = \frac 1 {t} - \frac 1k\mcom
\]
which is achieved by $\sigma = \tau$. 
This is because in this case $\tau$ has collision probability $\iprod{\tau,\tau} = \tfrac 1 {t}$.

\paragraph{Main result for mixed-membership models}
The following theorem gives a precise bound on the number of edges that allows us to find in polynomial time a labeling of the vertices of an $n$-node mixed membership block model having nontrivial correlation with the true underlying labels.
Here, the parameters $d,\e,k,\alpha$ of the mixed-membership stochastic block model may even depend on the number of vertices $n$.

\begin{theorem}[Mixed-membership SBM---significant correlation]
  \label{thm:mixed-membership-sbm}
  Let $d,\e,k,\alpha$ be such that $k\le n^{o(1)}$, $\alpha\le n^{o(1)}$, and $\e^2 d \le n^{o(1)}$.
  Suppose $\delta \defeq 1 - \tfrac{k^2(\alpha+1)^2}{\e^2 d} > 0$.
  (Equivalently for small $\delta$, suppose $\e^2 d \geq (1 + \delta) \cdot k^2 (\alpha+1)^2$.)
  Then, there exists $\delta'\ge \delta^{O(1)} >0$ and an $n^{1/\poly(\delta)}$-time algorithm that given an $n$-vertex graph $G$ outputs $\tau_1,\ldots,\tau_n \in \Delta_{k-1}$ such that
  \begin{equation}
    \E_{(G,\sigma)\sim \sbm(n,d,\e,k,\alpha)}  \corr(\sigma,\tau)\ge \delta' \cdot \Paren{\frac 1t - \frac 1k}
  \end{equation}
  where $t = (\alpha+1) \cdot \tfrac k {k+\alpha}$ (samples from the $\alpha,k$ Dirichlet distribution are roughly uniform over $t$ out of $k$ coordinates).
  In particular, as $\delta \rightarrow 1$ we have $\E \corr(\sigma,\tau) \rightarrow \tfrac 1t - \tfrac 1k$, while $\E \corr(\sigma,\sigma) = \tfrac 1t - \tfrac 1k$.
\end{theorem}

Note that in the above theorem, the correlation $\delta'$ that our algorithm achieves depends only on $\delta$ (the distance to the threshold) and in particular is independent of $n$ and $k$ (aside from, for the latter, the dependence on $k$ via $\delta$).
For disjoint communities ($\alpha=0$), our algorithm achieves constant correlation with the planted labeling if $\e^2 d /k^2$ is bounded away from $1$ from below.

We conjecture that the threshold achieved by our algorithm is best-possible for polynomial-time algorithms.
Concretely, if $d,\e,k,\alpha$ are constants such that $\e^2d < k^2 (\alpha+1)^2$, then we conjecture that for every polynomial-time algorithm that given a graph $G$ outputs $\tau_1,\ldots,\tau_n \in \Delta_{k-1}$,
\begin{equation}
  \lim_{n\to \infty} \E_{(G,\sigma)\sim \sbm(n,d,\e,k,\alpha)} \corr(\sigma,\tau)=0\,.
\end{equation}

This conjecture is a natural extension of a conjecture for disjoint communities ($\alpha=0$), which says that beyond the Kesten--Stigum threshold, i.e., $\e^2 d < k^2$, no polynomial-time algorithm can achieve correlation bounded away from $0$ with the true labeling \cite{DBLP:journals/corr/Moore17}.
For large enough values of $k$, this conjecture predicts a computation-information gap because the condition $\e^2 d \ge \Omega(k \log k)$ is enough for achieving constant correlation information-theoretically (and in fact by a simple exponential-time algorithm).
We discuss these ideas further in Section~\ref{sec:intro-gaps}.

\paragraph{Comparison with previous matrix-based algorithms}
We offer a reinterpretation in our meta-algorithmic framework of the algorithms of Mossel-Neeman-Sly and Abbe-Sandon.
This will permit us to compare our algorithm for the mixed-membership model with what could be achieved by the methods in these prior works, and to point out one respect in which our algorithm improves on previous ones even for the disjoint-communities block model.
The result we discuss here is a slightly generalized version of Theorem~\ref{thm:mm-intro-matrix}.

Let $\cU$ be a (possibly infinite or continuous) universe of labels, and let $W$ assign to every $x,y \in \cU$ a nonnegative real number $W(x,y) = W(y,x) \geq 0$.
Let $\mu$ be a probability distribution on $\cU$, which induces the inner product of functions $f,g \, : \, \cU \rightarrow \R$ given by $\iprod{f,g} = \E_{x \sim \mu} f(x) g(x)$.
The function $W$ can be considered as linear operator on $\{f \, : \, \cU \rightarrow \R\}$, and under mild assumptions it has eigenvalues $\lambda_1,\lambda_2,\ldots$ with respect to the inner product $\iprod{\cdot, \cdot}$.

The pair $\mu,W$ along with an average degree parameter $d$ induce a generalized stochastic block model, where labels for nodes are drawn from $\mu$ and an edge between a pair of nodes with labels $x$ and $y$ is present with probability $\tfrac dn \cdot W(x,y)$.
When $\cU$ is $\Delta_{k-1}$ and $\mu$ is the Dirichlet distribution, this captures the mixed-membership block model.

Assume $\lambda_1 = 1$ and that $\mu$ and $W$ are sufficiently \emph{nice} (see Section~\ref{sec:W} for all the details).
Then one can rephrase results of Abbe and Sandon in this setting as follows.

\begin{theorem}[Implicit in \cite{DBLP:conf/nips/AbbeS16}]
  Suppose the operator $W$ has eigenvalues $1 = \lambda_1 > \lambda_2 > \dots > \lambda_r$ (each possibly with higher multiplicity) and $\delta \defeq 1 - \tfrac 1 {d \lambda_2^2} > 0$.
  Let $\Pi$ be the projector to the second eigenspace of the operator $W$.
  For types $x_1,\ldots,x_n \sim \cU$, let $A \in \R^{n \times n}$ be the random matrix $A_{ij} = \Pi(x_i,x_j)$, where we abuse notation and think of $\Pi \colon \cU \times \cU \rightarrow \R$.
  There is an algorithm with running time $n^{\poly(1/\delta)}$ which outputs an $n \times n$ matrix $P$ such that for $x,G \sim G(n,d,W,\mu)$,
  \[
  \E_{x,G} \Tr P \cdot A \geq \delta^{O(1)} \cdot (\E_{x,G} \|A\|^2)^{1/2} (\E_{x,G} \|P\|^2)^{1/2}\mper
  \]
\end{theorem}

In one way or another, existing algorithms for the block model in the constant-degree regime are all based on estimating the random matrix $A$ from the above theorem, then extracting from an estimator for $A$ some labeling of vertices by communities.
In our mixed-membership setting, one may show that the matrix $A$ is $\sum_{s \in [k]} \dyad{v_s}$, where $v_s \in \R^n$ has entries $v_s(i) = \sigma_i(s) - \tfrac 1k$.
Furthermore, as we show in Section~\ref{sec:W}, the condition $d \lambda_2^2 > 1$ translates for the mixed-membership model to the condition $\e^2 d > k(\alpha+1)^2$, which means that under the same hypotheses as our main theorem on the mixed-membership model it is possible in polynomial time to evaluate a constant-correlation estimator for $\sum_{s \in [k]} \dyad{v_s}$.
As we discussed in Section~\ref{sec:meta-intro}, however, extracting estimates for $v_1,\ldots,v_k$ (or, almost equivalently, estimates for $\sigma_1,\ldots,\sigma_n$) from this matrix seems to incur an inherent $1/k$ loss in the correlation.
Thus, the final guarantee one could obtain for the mixed-membership block model using the techniques in previous work would be estimates $\tau_1,\ldots,\tau_n$ for $\sigma_1,\ldots,\sigma_n$ such that $\corr(\sigma,\tau) \geq \Paren{\frac \delta k}^{O(1)}$.\footnote{In fact, it is not clear one can obtain even this guarantee using strictly matrix methods. Strictly speaking, in estimating, say, $v_1$ using the above described matrix method, one obtains a unit vector $v$ such that $\iprod{v,v_1}^2 \geq \Omega(1) \cdot \|v_1\|^2$. Without knowing whether $v$ or $-v$ is the correct vector it is not clear how to transform estimates for the $v_s$'s to estimates for the $\sigma$'s.
However, matrix methods cannot distinguish between $v_s$ and $-v_s$.
In our main algorithm we avoid this issue because the 3rd moments $\sum v_s^{\tensor 3}$ are not sign-invariant.}
We avoid this loss in our main theorem via tensor methods.

Although this $1/k$ multiplicative loss in the correlation with the underlying labeling is not inherent in the disjoint-community setting (roughly speaking this is because the matrix $A$ is a $0/1$ block-diagonal matrix), previous algorithms nonetheless incur such loss.
(In part this is related to the generality of the work of Abbe and Sandon: they aim to allow $W$ where $A$ might only have rank one, while in our settings $A$ always has rank $k-1$. For low-rank $A$ this $1/k$ loss is probably necessary for polynomial time algorithms.)

Thus our main theorem on the mixed membership model offers an improvement on the guarantees in the previous literature even for the disjoint-communities setting: when $W$ only has entries $1-\e$ and $\e$ we obtain a labeling of the vertices whose correlation with the underlying labeling depends only on $\delta$.
This allows the number $k$ of communities to grow with $n$ without incurring any loss in the correlation (so long as the average degree of the graph grows accordingly).

For further discussion of the these results and a proof of the above theorem, see Section~\ref{sec:W}.

\iffalse
This condition is called the Kesten--Stigum threshold and is exactly the threshold achieved by previous best polynomial-time algorithms\footnote{Here, achieving the Kesten--Stigum threshold means that if $\e^2 d /k^2 - 1 >0$ is lower bounded by any constant, then the algorithm achieves constant correlation with the true community structure (for a notion of correlation similar to \cref{eq:mixed-membership-correlation}).} (due to \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15} for $k=2$ and \cite{DBLP:conf/nips/AbbeS16} for general $k$).
For $k=2$, our notion of correlation \cref{eq:mixed-membership-correlation} is equivalent to the ones in previous works \cite{DBLP:conf/stoc/Massoulie14,DBLP:conf/stoc/MosselNS15}.
For general $k$, though, our notion of correlation is stronger.
Previous algorithms output a single vector $\tilde y\in \R^n$ such that $\iprod{Y_\sigma \Ind_S,\tilde y}\ge \delta'\cdot \norm{Y_\sigma\Ind_S}\cdot \norm{\tilde y}$ for a subset $S \subseteq [k]$ of communities.\footnote{
  Algorithms in previous works typically output subsets of vertices as opposed to vectors in $\R^n$.
  For disjoint communities, there is little difference between the two.
  In particular, if we have a set of vectors $L$, we can convert it to a set of $L'$ of $0/1$ vectors by a randomized algorithm such that $\E_{L'} \corr(\sigma,L')\ge \Omega(1)\cdot\corr(\sigma,L)^{O(1)}$.
  %
  %
}
The idea is that $y$ corresponds to a single community or a union of up to $k-1$ communities.
The difference between these notions of correlation can be a multiplicative factor of $k$.
\Dnote{}
\fi

\paragraph{Comparison to previous tensor algorithm for mixed-membership models}
Above we discussed a reinterpretation (allowing a continuous space $\cU$ of labels) of existing algorithms for the constant-average-degree block model which would give an algorithm for the mixed-membership model, and discussed the advantages of our algorithm over this one.
Now we turn to algorithms in the literature which are specifically designed for stochastic block models with overlapping communities.

The best such algorithm requires $\e^2 d \ge O(\log n)^{O(1)} \cdot k^2 (\alpha +1)^2$ \cite{DBLP:conf/colt/AnandkumarGHK13}.
Our bound saves the $O(\log n)^{O(1)}$ factor.
(This situation is analogous to the standard block model, where simpler algorithms based on eigenvectors of the adjacency matrix require the graph degree to be logarithmic.)
Notably, like ours this algorithm is based on estimating the tensor $T = \sum_{s \in [k]} v_s^{\tensor 3}$, where $v_s \in \R^n$ has entries $v_s(i) = \sigma_i(s) - \tfrac 1k$.
However, the algorithm differs from ours in two key respects.
\begin{compactenum}
  \item The algorithm \cite{DBLP:conf/colt/AnandkumarGHK13} estimates the tensor $T$ using a $3$-tensor analogue of a high power of the adjacency matrix of an input graph, while we use self-avoiding walks (which are rather like a tensor analogue of the nonbacktracking operator).
  \item The tensor decomposition algorithm used in \cite{DBLP:conf/colt/AnandkumarGHK13} to decompose the (estimator for the) tensor $T$ tolerates much less error than our tensor decomposition algorithm; the result is that a higher-degree graph is needed in order to obtain a better estimator for the tensor $T$.
\end{compactenum}
The setting considered by \cite{DBLP:conf/colt/AnandkumarGHK13} does allow a more sophisticated version of the Dirichlet distribution than we allow, in which different communities have different sizes.
It is an interesting open problem to extend the guarantees of our algorithm to that setting.

\iffalse
We remark that there is a stark difference between the algorithmic techniques that go into detecting disjoint communities and overlapping ones.
The former is based on matrices and pairwise correlations whereas the latter is based on tensors and higher-order correlations.
In order to achieve the Kesten--Stigum threshold for disjoint communities, the key ingredients are spectral properties of matrices that correspond to non-standard random walks like self-avoiding or non-backtracking ones.
Our algorithm for overlapping communities is based on higher-order tensors associated with such non-standard random walks.
To analyze these tensors, we introduce new kinds of random walks called \emph{colorful random walks} inspired by color coding \cite{DBLP:journals/jacm/AlonYZ95}, which greatly simplify the analysis (even for disjoint communities).
The most involved part of our algorithm is a new algorithm based on sum-of-squares to decompose tensors that have only constant correlation with an orthogonal tensor.
Previous algorithms require correlation close to $1$ \cite{DBLP:conf/focs/MaSS16, DBLP:conf/colt/SchrammS17} or even inverse-polynomial distance.

\paragraph{Recovering overlapping communities with higher accuracy}

Our above theorem focuses on providing an estimate that has constant correlation with the ground truth.
The following theorem shows that if we are a constant multiplicative factor above the threshold, so that $\e^2 d \gg k^2 (\alpha+1)^2$, we can achieve correlation close to 1 with the ground truth.
In this case, our algorithm outputs exactly $k$ vectors as opposed to $k^{O(1)}$ vectors in the previous theorem.

\begin{theorem}[Mixed-membership SBM---high accuracy]
  \label{thm:mixed-membership-sbm-high-correlation}
  Let $d,\e,k,\alpha$ be such that $k\le n^{o(1)}$, $\alpha\le n^{o(1)}$, and $\e^2 d \le n^{o(1)}$.
  Suppose $\e^2 d \ge 1/\eta \cdot k^2 (\alpha+1)^2$ for some $\eta$ with $0<\eta<0.9 $.
  Then, there exists $\eta'\le \eta^{\Omega(1)}$ and a polynomial-time algorithm that given an $n$-vertex graph $G$ outputs a list of vectors $L(G)\subseteq [0,1]^n$ of size $\card{L(G)}=k$ satisfying
  \begin{equation}
    \E_{(G,\sigma)\sim \sbm(n,d,\e,k,\alpha)}  \corr(\sigma,L(G))\ge 1-\eta'\,.
  \end{equation}
\end{theorem}

The above theorem is a byproduct of our proof of \cref{thm:mixed-membership-sbm}.
We regard \cref{thm:mixed-membership-sbm} as our main result for the mixed-membership stochastic block  model because it gives non-trivial guarantees all the way up to what we believe to be the threshold.
\cref{thm:mixed-membership-sbm-high-correlation} by itself could potentially be proved in a more direct way and with a better trade-off between the multiplicative distance $\eta$ from the threshold and the achieved accuracy $\eta'$.
\fi

\Dnote{}

\Dnote{}

\Dnote{}


\subsection{Low-correlation tensor decomposition}
\label{sec:tdecomp-intro}
Tensor decomposition is the following problem.
For some unit vectors $a_1,\ldots,a_m \in \R^n$ and a constant $k$ (often $k = 3$ or $4$), one is given the tensor $T = \sum_{i=1}^m a_i^{\tensor k} + E$, where $E$ is some error tensor.
The goal is to recover vectors $b_1,\ldots,b_m \in \R^n$ which are as close as possible to $a_1,\ldots,a_m$.


Tensor decomposition has become a common primitive used by algorithms for parameter learning and estimation problems \cite{comon2010handbook, DBLP:journals/jmlr/AnandkumarGHKT14, gelearning, DBLP:conf/stoc/GoyalVX14, DBLP:conf/stoc/BarakKS15,DBLP:conf/focs/MaSS16,DBLP:conf/colt/SchrammS17}.
In the simplest examples, the hidden variables are orthogonal vectors $a_1,\ldots,a_m$ and there is a simple function of the observed variables which estimates the tensor $\sum_{i \leq m} a_i^{\tensor k}$ (often an empirical $k$-th moment of observed variables suffices).
Applying a tensor decomposition algorithm to such an estimate yields estimates of the vectors $a_1,\ldots,a_m$.

We focus on the case that $a_1,\ldots,a_m$ are orthonormal.
Algorithms for this case are already useful for a variety of learning problems, and it is often possible to reduce more complicated problems to the orthonormal case using a small amount of side information about $a_1,\ldots,a_m$ (in particular their covariance $\sum_{i=1}^m \dyad{a_i}$).
In this setting the critical question is: how much error $E$ (and measured in what way) can the tensor decomposition algorithm tolerate and still produce useful outputs $b_1,\ldots,b_m$?

When we use tensor decomposition in our meta-algorithm, the error $E$ will be incurred when estimating $\sum_{i=1}^m a_i^{\tensor k}$ from observable samples.
Using more samples would decrease the magnitude of $T - \sum_{i=1}^m a_i^{\tensor k}$, but because our goal is to obtain algorithms with optimal sample complexity we need a tensor decomposition algorithm which is robust to greater errors than those in the existing literature.

Our main theorem on tensor decomposition is the following.
\begin{theorem}[Informal]
  For every $\delta > 0$, there is a randomized algorithm with running time $n^{1/\poly(\delta)}$ that given a 3-tensor $T\in(\R^n)^{\otimes 3}$ and a parameter $k$ outputs $n^{\poly(1/\delta)}$ unit vectors $b_1,\ldots,b_m$ with the following property:
  if $T$ satisfies $\iprod{T,\sum_{i=1}^k a_i^{\otimes 3}} \geq \delta \cdot \|T\| \cdot \sqrt{k}$ for some orthonormal vectors $a_1,\ldots,a_k$, then
  \[
  \E_{i \sim [k]} \max_{j \in [m]} \iprod{a_i,b_j}^2 \geq \delta^{O(1)}\mper
  \]
  Furthermore, if the algorithm is allowed to make $n^{1/\poly(\delta)}$ calls to an oracle $\cO$ which correctly answers queries of the form ``does the unit vector $v$ satisfy $\sum_{i=1}^m \iprod{a_i,v}^4 \geq \delta^{O(1)}$?'', then it outputs just $k$ orthonormal vectors, $b_1,\ldots,b_k$ such that there is a permutation $\pi \, : \, [k] \rightarrow [k]$ with
  \begin{displaymath}
    \E_{i \in [k]} \iprod{a_i, b_{\pi(i)}}^2 \geq \delta^{O(1)}\mper
  \end{displaymath}
  (These guarantees hold in expectation over the randomness used in the decomposition algorithm.)
\end{theorem}
(For a more formal statement, and in particular the formal requirements for the oracle $\cO$, see Section~\ref{sec:tdecomp}.)

Rescaling $T$ as necessary, one may reinterpret the condition $\iprod{T, \sum_{i=1}^k a_i^{\tensor 3}} \geq \delta \cdot \|T\| \cdot \sqrt{k}$ as $T = \sum_{i=1}^k a_i^{\tensor 3} + E$, where $\iprod{E,\sum_{i=1}^m a_i^{\tensor 3} } = 0$ and $\|E\| \leq \sqrt{k}/\delta$ and $\| \cdot \|$ is the Euclidean norm.
In particular, $E$ may have Euclidean norm which is a large constant factor $1/\delta$ larger than the Euclidean norm of the tensor $\sum_{i=1}^m a_i^{\tensor 3}$ that the algorithm is trying to decompose!
(One way such error could arise is if $T$ is actually correlated with $1/\delta$ unrelated orthogonal tensors; our algorithm in that case ensures that the list of outputs vectors is correlated with every one of these orthogonal tensors.)

In all previous algorithms of which we are aware (even for the case of orthogonal $a_1,\ldots,a_m$), the error $E$ must have spectral norm (after flattening to an $n^2 \times n^2$ matrix) at most $\e$ for $\e < \tfrac 12$,\footnote{Or, mildly more generally, $E$ should have SoS norm less than $\e$ \cite{DBLP:conf/focs/MaSS16}.} or $E$ must have Euclidean norm at most $\e \sqrt m$ \cite{DBLP:conf/colt/SchrammS17}.
The second requirement is strictly stronger than ours (thus our algorithm has weaker requirements and so stronger guarantees).
The first, on the spectral norm of $E$ when flattened to a matrix, is incomparable to the condition in our theorem.
However, when $E$ satisfies such a spectral bound it is possible to decompose $T$ using (sophisticated) spectral methods \cite{DBLP:conf/focs/MaSS16, DBLP:conf/colt/SchrammS17}.
In our setting such methods seem unable to avoid producing only vectors $b$ which are correlated with $E$ but not with any $a_1,\ldots,a_m$.
In other words, such methods would \emph{overfit to the error $E$}.
To avoid this, our algorithm uses a novel maximum-entropy convex program (see Section~\ref{sec:tdecomp} for details).

One a priori unusual requirement of our tensor decomposition algorithm is access to the oracle $\cO$.
In any tensor decomposition setting where $E$ satisfies $\|E\|_{inj} = \max_{\|x\| = 1} \iprod{E, x^{\tensor 3}} \leq o(1)$, the oracle $\cO$ can be implemented just be evaluating $\iprod{T,v^{\tensor 3}} = \sum_{i=1}^k \iprod{a_i,v}^{3} + o(1)$.
All previous works on tensor decomposition of which we are aware either assume that the injective norm $\|E\|_{inj}$ is bounded as above, or (as in \cite{DBLP:conf/colt/SchrammS17}) can accomplish this with a small amount of preprocessing on the tensor $T$.
Our setting allows, for example, $E = 100 \cdot v^{\tensor 3}$ for some unit vector $v$, and does not appear to admit the possibility of such preprocessing, hence the need for an auxiliary implementation of $\cO$.
In our learning applications we are able to implement $\cO$ by straightforward holdout set/cross-validation methods.


\subsection{Information-computation gaps and concrete lower bounds}
\label{sec:intro-gaps}
The meta-algorithm we offer in this paper is designed to achieve optimal sample complexity \emph{among polynomial-time algorithms} for many Bayesian estimation problems.
It is common, though, that computationally inefficient algorithms can obtain accurate estimates of hidden variables with fewer samples than seem to be tolerated by any polynomial-time algorithm.
This appears to be true for the $k$-community stochastic block model we have used as our running example here: efficient algorithms seem to require graphs of average degree $d > k^2/\e^2$ to estimate communities, while inefficient algorithms are known which tolerate $d$ of order $k \log k$ \cite{DBLP:conf/isit/AbbeS16, DBLP:journals/corr/Moore17}.

Such phenomena, sometimes called \emph{information-computation gaps}, appear in many other Bayesian estimation problems.
For example, in the classical \emph{planted clique} problem \cite{DBLP:journals/rsa/Jerrum92, DBLP:journals/dam/Kucera95}, a clique of size $k > (2 + \e) \log n$ is randomly added to a sample $G \sim G(n,\tfrac 12)$; the goal is to find the clique given the graph.
Since the largest clique in $G(n,\tfrac 12)$ has expected size $2 \log n$, so long as $k > (2 + \e)\log n$ it is information-theoretically possible, via brute-force search, to recover the planted clique.
On the other hand, despite substantial effort, no polynomial-time algorithm is known which can find a clique of size $k \leq o(\sqrt n)$, exponentially-larger than cliques which can be found by brute force.

For other examples one need not look far: sparse principal component analysis, planted constraint satisfaction problems, and densest-$k$-subgraph are just a few more problems exhibiting information-computation gaps.
This ubiquity leads to several questions:
\begin{compactenum}
  \item What rigorous evidence can be provided for the claim: no polynomial algorithm tolerates $n < n_*$ samples and produces a constant-correlation estimator $\widehat{\theta}(x_1,\ldots,x_n)$ for particular a Bayesian estimation problem $p(x,\theta)$?
  \item Can information-computation gaps of different problems be explained by similar underlying phenomena? That is, is there a structural feature of a Bayesian estimation problem which determines whether it exhibits an information-computation gap, and if so, what is the critical number of samples $n_*$ required by polynomial-time algorithms?
  \item Are there methods to easily predict the location of a critical number $n_*$ of samples, without analyzing every polynomial-time algorithm one can think of?
\end{compactenum}

\paragraph{Rigorous evidence for computational phase transitions}
The average-case nature of Bayesian estimation problems makes it unlikely that classical tools like (NP-hardness) reductions allow us to reason about the computational difficulty of such problems in the too-few-samples regime.
Instead, to establish hardness of an estimation problem when $n < n_*$ for some critical $n_*$, one proves impossibility results for restricted classes of algorithms.
Popular classes of algorithms for this purpose include Markov-Chain Monte Carlo algorithms and various classes of convex programs, especially arising from convex hierarchies such as the Sherali-Adams linear programming hierarchy and the sum of squares semidefinite programming hierarchy.

Results like this are meaningful only if the class of algorithms for which one proves an impossibility result captures the best known (i.e. lowest-sample-complexity) polynomial-time algorithm for the problem at hand.
Better yet would be to use a class of algorithms which captures the lowest-sample-complexity polynomial-time algorithms for many Bayesian estimation problem simultaneously.

In the present work we study sample complexity up to low-order additive terms in the number $n$ of samples.
For example, in the $k$-community $\alpha$-mixed-membership stochastic block model, we provide an algorithm which estimates communities in graphs of average degree $d \geq (1 + \delta) k^2 (\alpha+1)^2/\e^2$, for any constant $\delta > 0$.
Such precise algorithmic guarantees suggest the pursuit of equally-precise lower bounds.

Proving a lower bound against the convex-programming-based algorithms most commonly considered in previous work on lower bounds for Bayesian estimation problems does not suit this purpose.
While powerful, these algorithms are generally not designed to achieve optimal sample complexity up to low-order additive terms.
Indeed, there is mounting evidence in the block model setting that sum of squares semidefinite programs actually require a constant multiplicative factor more samples than our meta-algorithm \cite{DBLP:conf/stoc/MontanariS16, DBLP:conf/approx/BanksKM17}.

Another approach to providing rigorous evidence for the impossibility side of a computational threshold is \emph{statistical query} complexity, used to prove lower bounds against the class of \emph{statistical query} algorithms \cite{DBLP:conf/stoc/FeldmanGRVX13,DBLP:journals/corr/FeldmanGV15,DBLP:conf/stoc/FeldmanPV15}.
To the best of our knowledge, similar to sum of squares algorithms, statistical query algorithms are not known to achieve optimal sample rates for problems such as community recovery in the block model up to low-order additive terms.
Such sample-optimal algorithms seem to intrinsically require the ability to compute a complicated function of many samples simultaneously (for example, the top eigenvector of the non-backtracking operator).
But even the most powerful statistical query algorithms (using the $1$-STAT oracle) can access one bit of information about each sample $x$ (by learning the value of some function $h(x) \in \{0,1\}$).
This makes it unclear how the class of statistical query algorithms can capture the kinds of sample-optimal algorithms we want to prove lower bounds against.

Our meta-algorithm offers an alternative.
By showing that when $n$ is less than a critical $n_*$ there are no constant-correlation low-degree estimators for a hidden random variable, one rules out any efficient algorithm captured by our meta-algorithm.
Concretely, \cref{thm:meta-theorem-2nd,thm:meta-theorem-3rd} show that in order for an estimation problem to be intractable it is necessary that every low-degree polynomial fails to correlate with the second or third moment of the posterior distribution (in the sense of \cref{eq:second-moment-correlation,eq:third-moment-correlation}).
This kind of fact about low-degree polynomial is something we can aim to prove unconditionally as a way to give evidence for the intractability of a Bayesian estimation problem.
Next we discuss our example result of this form in the block model setting.

\paragraph{Concrete unconditional lower bound at the Kesten--Stigum threshold}

In this work, we show an unconditional lower bound about low-degree polynomials for the stochastic block model with $k$ communities at the Kesten--Stigum threshold.
For $k\ge 4$, this threshold is bounded away from the information-theoretic threshold \cite{DBLP:conf/focs/AbbeS15}.
In this way, our lower bounds gives evidence for an inherent gap between the information-theoretical and computational thresholds.

For technical reasons, our lower bound is for a notion of correlation mildly different from \cref{eq:second-moment-correlation,eq:third-moment-correlation}.
Our goal is to compare the stochastic block model distribution $\sbm(n,d,\e,k)$  graphs to the \Erdos-\Renyi distribution $G(n,\tfrac dn)$ with respect to low-degree polynomials.
As before we represent graphs as adjacency matrices $x\in \bits^{n\times n}$.
Among all low-degree polynomials $p(x)$, we seek one so that the typical value of $p(x)$ for graphs $x$ from the stochastic blocks model is as large as possible compared to its typical for \Erdos-\Renyi graphs.
The following theorem shows that a suitable mathematical formalization of this question exhibits a sharp ``phase transition'' at the Kesten--Stigum threshold.
\begin{theorem}
  \label{thm:lower-bound}
  Let $d,\e,k$ be constants.
  Then,
  \begin{equation}
    \max_{p\in \R[x]_{\le \ell}} \frac{
      \E_{x\sim \sbm(n,d,\e,k)} p(x)
    }{
      \Paren{\E_{x\sim G(n,d/n)} p(x)^2}^{1/2}
    }=
    \begin{cases}
      \ge n^{\Omega(1)} & \text{ if }\e^2 d > k^2,~\ell \ge O(\log n)\\
      \le n^{o(1)} & \text{ if }\e^2 d < k^2, \ell\le n^{o(1)}
      %
    \end{cases}
    \label{eq:lower-bound}
  \end{equation}
\end{theorem}

Let $\mu\from \bits^{n\times n}\to \R$ be the relative density of $\sbm(n,d,\e,k)$ with respect to $G(n,\tfrac dn)$.
Basic linear algebra shows that the left-hand side of \cref{eq:lower-bound} is equal to $\norm{\mu^{\le \ell}}_2$, where $\norm{\cdot}_2$ is the Euclidean norm with respect to the measure $G(n,d/n)$ and $\mu^{\le \ell}$ is the  projection (with respect to this norm) of $\mu$ to the subspace of functions of degree at most $\ell$.
This is closely related to the $\chi^2$-divergence of $\mu$ with respect to $G(n,d/n)$, which in the present notation would be given by $\|(\mu -1) \|_2$.
When the latter quantity is small, $\|(\mu - 1)\|_2 \leq o(1)$, one may conclude that the distribution $\mu$ is information-theoretically indistinguishable from $G(n,d/n)$.
This technique is used in the best current bounds on the information-theoretic properties of the block model \cite{mossel2012stochastic, DBLP:conf/colt/BanksMNN16}.

The quantity in Theorem~\ref{thm:lower-bound} is a low-degree analogue of the $\chi^2$-divergence.
If it were true that $\|(\mu^{\leq \ell}-1)\|_2 \leq o(1)$, then by a straightforward application of Cauchy-Schwarz it would follow that no low-degree polynomial $p(x)$ distinguishes the block model from $G(n,d/n)$, since every such $p$ would have (after setting $\E_{G(n,d/n)} p(x) = 0$) that $\E_{SBM} p(x) \leq o(\E_{G(n,d/n)} p(x)^2)^{1/2}$.
This condition turns out to be quite powerful: \cite{DBLP:conf/focs/BarakHKKMP16, soslb} give evidence that for problems such as planted clique, for which distinguishing instances drawn from a null model from instances with hidden structure should be computationally intractable, the condition $\|(\mu^{\leq \ell}-1)\| \leq o(1)$ is closely related to sum of squares lower bounds.\footnote{In particular, the so-called \emph{pseudocalibration} approach to sum of squares lower bounds works only when $\|(\mu^{\leq \ell} -1)\| \leq o(1)$.}

The situation in the $k$-community block model is a bit more subtle.
One has only that $\|\mu^{\leq \ell}\| \leq n^{o(1)}$ below the Kesten-Stigum threshold because even in the latter regime it remains possible to distinguish a block model graph from $G(n,d/n)$ via a low-degree polynomial (simply counting triangles will suffice).
However, we can still hope to rule out algorithms which accurately estimate communities below the Kesten-Stigum threshold.
For this we prove the following theorem.
\begin{theorem}
  Let $d,\e,k,\delta$ be constants such that $\e^2 d < (1 -\delta)k^2$.
  Let $f : \{0,1\}^{n \times n} \rightarrow \R$ be any function, let $i,j \in [n]$ be distinct.
  Then if $f$ satisfies $\E_{x \sim G(n,\tfrac dn)} f(x) = 0$ and is correlated with the indicator $\Ind_{\sigma_i = \sigma_j}$ that $i$ and $j$ are in the same community in the following sense:
  \[
    \frac{\E_{x \sim SBM(n,d,\e,k)} f(x)(\Ind_{\sigma_i = \sigma_j} - \tfrac 1k)}{(\E_{x \sim G(n,\tfrac dn)} f(x)^2)^{1/2}} \geq \Omega(1)
  \]
  then $\deg f \geq n^{c(d,\e,k)}$ for some $c(d,\e,k) > 0$.
\end{theorem}
There is one subtle difference between the polynomials ruled out by this theorem and those which could be used by our meta-algorithm.
Namely, this theorem rules out any $f$ whose correlation with the indicator $\Ind_{\sigma_i = \sigma_j}$ is large \emph{compared to $f$'s standard deviation under $G(n,d/n)$}, whereas our meta-algorithm needs a polynomial $f$ where this correlation is large compared to $f$'s standard deviation under the block model.
In implementing our meta-algorithm for the block model and for other problems, we have found that these two measures of standard deviation are always equal (up to low-order additive terms) for the polynomials which turn out to provide sample-optimal constant-correlation estimators of hidden variables.

Interesting open problems are to prove a version of the above theorem where standard deviation is measured according to the block model and to formalize the idea that $\E_{SBM} f(x)^2$ should be related to $\E_{G(n,d/n)} f(x)^2$ for good estimators $f$.
It would also be quite interesting to see how large the function $c(d,\e,k)$ can be made: the above theorem shows that when $d < (1-\delta) k^2/\e^2$ the degree of any good estimator of $\Ind_{\sigma_i = \sigma_j}$ must be polynomial in $n$---perhaps it must be linear, or even quadratic in $n$.

\paragraph{General strategies to locate algorithmic thresholds}
The preceding theorems suggest a general strategy to locate critical a sample complexity $n_*$ for almost any Bayesian estimation problem: compute a Fourier transform of an appropriate relative density $\mu$ and examine the $2$-norm of its low-degree projection.
This strategy has several merits beyond its broad applicability.
One advantage is that in showing $\|(\mu^{\geq \ell} -1)\| \geq \Omega(1)$, one automatically has discovered a degree-$\ell$ polynomial and a proof that it distinguishes samples with hidden structure from an appropriate null model.
Another is the mounting evidence (see \cite{DBLP:conf/focs/BarakHKKMP16, soslb}) that when, on the other hand $\|(\mu^{\leq \ell} -1)\| \leq o(1)$ for large-enough $\ell$, even very powerful convex programs cannot distinguish these cases.
A final advantage is simplicity: generally computing $\|(\mu^{\geq \ell}-1)\|$ is a simple exercise in Fourier analysis.

Finally, we compare this strategy to the only other one we know which shares its applicability across many Bayesian estimation problems, namely the replica and cavity methods (and their attendant algorithm, belief propagation) from statistical physics \cite{mezard2009information}.
These methods were the first used to predict the sharp sample complexity thresholds we study here for the stochastic block model, and they have also been used to predict similar phenomena for many other hidden variable estimation problems \cite{DBLP:conf/allerton/LesieurBBKMZ16, DBLP:conf/allerton/LesieurBBKMZ16, DBLP:conf/allerton/LesieurBBKMZ16}.
Though remarkable, the predictions of these methods are much more difficult than ours make rigorous---in particular, it is notoriously challenging to rigorously analyze the belief propagation algorithm, and often when these predictions are made rigorous, only a modified version (``linearized BP'') can be analyzed in the end.
By contrast, our methods to predict critical sample complexities, design algorithms, and prove lower bounds all study essentially the same low-degree-polynomial algorithm.

We view it as a fascinating open problem to understand why predicted critical sample complexities offered by the replica and cavity methods are so often identical to the predictions of the low-degree-polynomials meta-algorithm we propose here.

\Dcomment{}

%
%

%



%
%


%
%
%

\Dnote{}