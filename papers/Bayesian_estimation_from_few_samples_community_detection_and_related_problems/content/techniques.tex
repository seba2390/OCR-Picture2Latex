\section{Techniques}
\label{sec:techniques}


%

\newcommand{\saw}{\mathrm{SAW}}

To illustrate the idea of low-degree estimators for posterior moments, let's first consider the most basic stochastic block model with $k=2$ disjoint communities ($\alpha=0$).
(Our discussion will be similar to the analysis in \cite{DBLP:conf/stoc/MosselNS15}.)
Let $y\in \set{\pm 1}^n$ be chosen uniformly at random and let $x\in \bits^{n\times n}$ be the adjacency matrix of a graph such that for every pair $i<j\in [n]$, we have $x_{ij}=1$  with probability $(1+\e y_i y_j)\tfrac dn$.
Our goal is to find a matrix-valued low-degree polynomial $P(x)$ that correlates with $\dyad y$.
It turns out to be sufficient to construct for every pair $i,j\in [n]$ a low-degree polynomial that correlates with $y_i y_j$.

The linear polynomial $p_{ij}(x)=\tfrac n {\e d}\Paren{x_{ij}-\tfrac d n}$ is an unbiased estimator for $y_i y_j$ in the sense that $\E[p_{ij}(x) \mid y]=y_i y_j$.
By itself, this estimator is not particular useful because its variance $\E p_{ij}(x)^2\approx\frac { n}{\e^2 d}$ is much larger than the quantity $y_i y_j$ we are trying to estimate.
However, if we let $\alpha\subseteq [n]^2$ be a length-$\ell$ path between $i$ and $j$ (in the complete graph), then we can combine the unbiased estimators along the path $\alpha$ and obtain a polynomial
\begin{equation}
  \label{eq:path-polynomial-techniques}
p_\alpha(x)=\prod_{ab\in \alpha} p_{ab}(x)
\end{equation}
that is still an unbiased estimator $\E [p_\alpha(x)\mid y_i,y_j]=\E \Brac{\prod_{ab\in \alpha} y_a y_b \mid y_i,y_j } = y_i y_j$.
This estimator has much higher variance $\E p_{\alpha}(x)^2 \approx \cramped{\paren{\frac { n}{\e^2 d}}^{\ell}}$.
But we can hope to reduce this variance by averaging over all such paths.
The number of such paths is roughly $n^{\ell -1}$ (because there are $\ell-1$ intermediate vertices to choose).
Hence, if these estimators $\set{p_\alpha(x)}_{\alpha}$ were pairwise independent, this averaging would reduce the variance by a multiplicative factor $n^{\ell-1}$, giving us a final variance of $\cramped{\paren{\frac { n}{\e^2 d}}^{\ell}} \cdot n^{1-\ell}=(\tfrac 1{\e^2 d})^\ell \cdot n$.
We can see that above the Kesten--Stigum threshold, i.e., $\e^ 2 d\ge 1+\delta$ for $\delta>0$, this heuristic variance bound $(\tfrac 1{\e^2 d})^\ell \cdot n\le 1$ is good enough for estimating the quantity $y_i\cdot y_j$ for paths of length $\ell \ge \log_{1+\delta} n$.

Two steps remain to turn this heuristic argument into a polynomial-time algorithm for estimating the matrix $\dyad y$.
First, it turns out to be important to consider only paths that are self-avoiding.
As we will see next, estimators from such paths are pairwise independent enough to make our heuristic variance bound go through.
Second, a naive evaluation of the final polynomial takes quasi-polynomial time because it has logarithmic degree (and a quasi-polynomial number of non-zero coefficients in the monomial basis).
We describe the high-level ideas for avoiding quasi-polynomial running time later in this section (\cref{sec:techniques-color-coding}).

\subsection{Approximately pairwise-independent estimators}

Let $\saw_\ell(i,j)$ be the set of self-avoiding walks $\alpha\subseteq [n]^2$ of length $\ell$ between $i$ and $j$.
Consider the unbiased estimator $p(x)=\tfrac 1{\card{\saw_\ell(i,j)}}\sum_{\alpha \in \saw_\ell(i,j)} p_\alpha(x)$ for $y_iy_j$.
Above the Kesten--Stigum threshold and for $\ell\ge O(\log n)$, we can use the following lemma to show that $p(x)$ has variance $O(1)$ and achieves constant correlation with $z=y_i y_j$.
We remark that the previous heuristic variance bound corresponds to the contribution of the terms with $\alpha=\beta$ in the left-hand side of \cref{eq:approximate-pw-independence}.

\begin{lemma}[Constant-correlation estimator]
  \label[lemma]{lem:basis-conditions}
  Let $(x,z)$ be distributed over $\bits^n\times \R$.
  Let $\set{p_\alpha}_{\alpha\in \cI}$ be a collection of real-valued $n$-variate polynomials with the following properties:
  \begin{compactenum}
  \item unbiased estimators: $\E[p_\alpha(x) \mid z]=z$ for every $\alpha\in \cI$
  \item approximate pairwise independence: for $\delta>0$,
    \begin{equation}
      \label{eq:approximate-pw-independence}
      \sum_{\alpha,\beta\in \cI} \E p_\alpha(x)\cdot p_\beta(x) \le \tfrac 1{\delta^2} \cdot\card{\cI}^2 \E z^2
    \end{equation}
  \end{compactenum}
  Then, the polynomial $p=\tfrac 1 {\card \cI}\sum_{\alpha\in \cI} p_\alpha$ satisfies $\E p(x) \cdot z\ge \delta\cdot \Paren{\E p(x)^2\cdot \E z^2}^{1/2}$.
\end{lemma}
\begin{remark}
  In applying the lemma we often substitute for \cref{eq:approximate-pw-independence} the equivalent condition 
  \[
  \E z^2 \cdot \sum_{\alpha,\beta\in \cI} \E p_\alpha(x)\cdot p_\beta(x) \le \frac 1{\delta^2} \cdot \sum_{\alpha,\beta\in \cI} (\E p_\alpha(x) z) \cdot (\E p_\beta(x)z)
  \]
  which is conveniently invariant to rescaling of the $p_\alpha$'s.
\end{remark}
\begin{proof}
  Since the polynomial $p$ is an unbiased estimator for $z$, we have $\E p(x) z = \E z^2$.
  By \cref{eq:approximate-pw-independence}, $\E p(x)^2 \le (1/\delta^2)\cdot \E z^2$.
  Taken together, we obtain the desired conclusion.
\end{proof}

In \cref{sec:low-degree-simple-community}, we present the short combinatorial argument that shows that above the Kesten--Stigum bound the estimators for self-avoiding walks satisfy the conditions \cref{eq:approximate-pw-independence} of the lemma.
%

We remark that if instead of self-avoiding walks we were to average over all length-$\ell$ walks between $i$ and $j$, then the polynomial $p(x)$ computes up to scaling nothing but the $(i,j)$-entry of the $\ell$-th power of the centered adjacency $x-\tfrac d n \dyad \Ind$.
For $\ell\approx \log n$, the $\ell$-th power of this matrix converges to $\dyad v$, where $v$ is the top eigenvector of the centered adjacency matrix.
For constant degree $d=O(\log n)$, it is well-known that this eigenvector fails to provide a good approximation to the true labeling.
In particular, the corresponding polynomial fails to satisfy the conditions of \cref{lem:basis-conditions} close to the Kesten--Stigum threshold.

\subsection{Low-degree estimators for higher-order moments}

\label{sec:techniques-estimate-higher-moments}

Let's turn to the general mixed-membership stochastic block model $\sbm(n,d,\e,k,\alpha_0)$.
Let $(G,\sigma)$ be graph $G$ and community structure $\sigma=(\sigma_1,\ldots,\sigma_n)$ drawn from this model.
Recall that $\sigma_1,\ldots,\sigma_n$ are $k$-dimensional probability vectors, each roughly uniform over $\alpha_0+1$ of the coordinates.
Let $x\in \bits^{n\times n}$ be the adjacency matrix of $G$ and let $y_1,\ldots,y_k\in\R^n$ be centered community indicator vectors, so that $(y_s)_i=(\sigma_i)_s-\tfrac 1k$.

It's instructive to see that, unlike for disjoint communities, second moments are not that useful for overlapping communities.
As a thought experiment suppose we are given the matrix $\sum_{s=1}^k \dyad{\paren{y_s}}$ (which we can estimate using the path polynomials described earlier).

In case of disjoint communities, this matrix allows us to ``read off'' the community structure directly (because two vertices are in the same community if and only if the entry in the matrix is $1-O(1/k)$).

For overlapping communities (say the extreme case $\alpha_0\gg k$ for simplicity), we can think of each $\sigma_i$ as a random perturbation of the uniform distribution so that $(\sigma_i)_s = (1+\xi_{i,s})\tfrac 1 k$ for iid Gaussians $\set{\xi_{i,s}}$ with small variance.
Then, the centered community indicator vectors $y_1,\ldots,y_k$ are iid centered, spherical Gaussian vectors.
In particular, the covariance matrix $\sum_{s=1}^k \dyad{y_s}$ essentially only determines the subspace spanned by the vectors $y_1,\ldots,y_k$ but not the vectors themselves.
(This phenomenon is sometimes called the ``rotation problem'' for matrix factorizations.)

In contrast, classical factor analysis results show that if we were given the third moment tensor $\sum_{s=1}^k y_s^{\otimes 3}$, we could efficiently reconstruct the vectors $y_1,\ldots,y_k$ \cite{harshman1970foundations,MR1238921-Leurgans93}.
This fact is the reason for aiming to estimate higher order moments in order to recover overlapping communities.
%

In the same way that a single edge $x_{i,j}-\tfrac dn$ gives an unbiased estimator for the $(i,j)$-entry of the second moment matrix, a 3-star $(x_{i,c}-\tfrac d n)(x_{j,c}-\tfrac d n)(x_{k,c}-\tfrac dn)$ gives an unbiased estimator for the $(i,j,k)$-entry of the third moment tensor $\sum_{s=1}^k y_s^{\otimes 3}$.
This observation is key for the previous best algorithm for mixed-membership community detection \cite{DBLP:conf/colt/AnandkumarGHK13}.
However, even after averaging over all possible centers $c$, the variance of this estimator is far too large for sparse graphs.
In order to decrease this variance, previous algorithms  \cite{DBLP:conf/colt/AnandkumarGHK13} project the tensor to the top eigenspace of the centered adjacency matrix of the graph.
In terms of polynomial estimators this projection corresponds to averaging over all length-$\ell$-armed 3-stars\footnote{A length-$\ell$-armed 3-star between $i,j,k\in [n]$ consists of three length-$\ell$ walks between $i,j,k$ and a common center $c\in [n]$} for $\ell=\log n$.
Even for disjoint communities, this polynomial estimator would fail to achieve the Kesten--Stigum bound.

In order to improve the quality of this polynomial estimator, informed by the shape of threshold-achieving estimator for second moments, we average only over such long-armed 3-stars that are self-avoiding.
We show that the resulting estimator achieves constant correlation with the desired third moment tensor precisely up to the Kesten--Stigum bound (\cref{sec:estimator-third-moment}).

\subsection{Correlation-preserving projection}

A recurring theme in our algorithms is that we can compute an approximation vector $P$ that is correlated with some unknown ground-truth vector $Y$ in the Euclidean sense $\iprod{P,Y}\ge \delta\cdot \norm{P}\cdot \norm{Y}$, where the norm $\norm{\cdot}$ is induced by the inner product $\iprod{\cdot,\cdot}$.
(Typically, we obtain $P$ by evaluating a low-degree polynomial in the observable variables and $Y$ is the second or third moment of the hidden variables.)

In this situation, we often seek to improve the quality of the approximation $P$---not in the sense of increasing the correlation, but in the sense of finding a new approximation $Q$ that is ``more similar'' to $Y$ while roughly preserving the correlation, so that $\iprod{Q,Y}\ge \delta^{O(1)}\cdot \norm{Q}\cdot \norm{Y}$.
As a concrete example, we may know that $Y$ is a positive semidefinite matrix with all-ones on the diagonal and our goal is to take an arbitrary matrix $P$ correlated with $Y$ and compute a new matrix $Q$ that is still correlated with $Y$ but in addition is positive semidefinite and has all-ones on the diagonal.
More generally, we may know that $Y$ is contained in some convex set $\cC$ and the goal is ``project'' $P$ into the set $\cC$ while preserving the correlation.
We note that the perhaps  most natural choice of $Q$ as the vector closest to $P$ in $\cC$ does not work in general.
(For example, if $Y=(1,0)$, $\cC=\set{(a,b)\mid a\le 1}$, and $P=(\delta\cdot M, M)$, then the closest vector to $P$ in $\cC$ is $(1,M)$, which has poor correlation with $Y$ for large $M$.)

\begin{theorem}[Correlation-preserving projection]
  \label{thm:correlation-preserving-projection}
  Let $\cC$ be a convex set and $Y\in \cC$.
  Let $P$ be a vector with $\iprod{P,Y}\ge \delta \cdot \norm{P}\cdot \norm{Y}$.
  Then, if we let $Q$ be the vector that minimizes $\norm{Q}$ subject to $Q\in \cC$ and $\iprod{P,Q}\ge \delta \cdot \norm{P}\cdot \norm{Y}$, we have
  \begin{equation}
    \iprod{Q,Y}\ge \delta/2 \cdot \norm{Q}\cdot \norm{Y}\,.
  \end{equation}
  Furthermore, $Q$ satisfies $\norm{Q}\ge \delta \norm{Y}$.
\end{theorem}
\begin{proof}
  By construction, $Q$ is the Euclidean projection of $0$ into the set $\cC'\seteq \set{Q \in \cC \mid \iprod{P,Q}\ge \delta \norm{P}\cdot \norm{Y}}$.
  It's a basic geometric fact (sometimes called Pythagorean inequality) that a Euclidean projection into a set decreases distances to points into the set.
  Therefore, $\norm{Y-Q}^2\le \norm{Y-0}^2$ (using that $Y\in \cC'$).
  Thus, $\iprod{Y,Q}\ge \norm{Q}^2/2$.
  On the other hand, $\iprod{P,Q}\ge \delta \norm{P}\cdot \norm{Y}$ means that $\norm{Q}\ge \delta \norm{Y}$ by Cauchy--Schwarz.
  We conclude $\iprod{Y,Q}\ge \delta/2 \cdot \norm{Y}\cdot \norm{Q}$.
\end{proof}

In our applications the convex set $\cC$ typically consists of probability distributions or similar objects (for example, quantum analogues like density matrices or pseudo-distributions---the sum-of-squares analogue of distributions).
Then, the norm minimization in \cref{thm:correlation-preserving-projection} can be viewed as maximizing the \Renyi entropy of the distribution $Q$.
From this perspective, maximizing the entropy within the set $\cC'$ ensures that the correlation with $Y$ is not lost.

\subsection{Low-correlation tensor decomposition}

Earlier  we described how to efficiently compute a 3-tensor $P$ that has correlation $\delta>0$ with a 3-tensor $\sum_{i=1}^k y_i^{\otimes 3}$, where $y_1,\ldots,y_k$ are unknown orthonormal vectors we want to estimate (\cref{sec:techniques-estimate-higher-moments}).
Here, the correlation $\delta$ depends on how far we are from the threshold and may be minuscule (say $0.001$).

It remains to decompose the tensor $P$ into a short list of vectors $L$ so as to ensure that $\E_{i\in [k]} \max_{\hat y\in L}\iprod{\hat y, y}\ge \delta^{O(1)}$.
(Ideally of course $|L| = k$. In the block model context this guarantee requires a small amount of additional work to cross-validate vectors in a larger list.)
To the best of our knowledge, previous tensor decomposition algorithms do not achieve this kind of guarantee and require that the correlation of $P$ with the orthogonal tensor $\sum_{i=1}^k y_i^{\otimes 3}$ is close to $1$ (sometimes even within polynomial factors $1/n^{O(1)}$).

In the current work, we achieve this guarantee building on previous sum-of-squares based tensor decomposition algorithms \cite{DBLP:conf/stoc/BarakKS15,DBLP:conf/focs/MaSS16}.
These algorithms optimize over moments of pseudo-distributions (a generalization of probability distributions) and then apply Jennrich's classical tensor decomposition algorithms to these ``pseudo-moments''.
The advantage of this approach is that it provably works even in situations where Jennrich's algorithm fails when applied to the original tensor.

As a thought experiment, suppose we are able to find pseudo-moments $M$ that are correlated with the orthogonal tensor $\sum_{i=1}^k y_i^{\otimes 3}$.
Extending previous techniques \cite{DBLP:conf/focs/MaSS16}, we show that Jennrich's algorithm applied to $M$ is able to recover vectors that have constant correlation with a constant fraction of the vectors $y_1,\ldots,y_k$.

A priori it is not clear how to find such pseudo-moments $M$ because we don't know the orthogonal tensor $\sum_{i=1}^k y_i^{\otimes 3}$, we only know a 3-tensor $P$ that is slightly correlated with it.
Here, the correlation-preserving projection discussed in the previous section comes in:
by \cref{thm:correlation-preserving-projection} we can efficiently project $P$ into the set of pseudo-moments in a way that preserves correlation.
In this way, we obtain pseudo-moments $M$ that are correlated with the unknown orthogonal tensor $\sum_{i=1}^k y_i^{\otimes 3}$.

When $P$ is a $3$-tensor as above, we encounter technical difficulties inherent to odd-order tensors.
(This is a common phenomenon in the tensor-algorithms literature.)
To avoid these difficulties we give a simple algorithm, again using the correlation-preserving projection idea, to lift a $3$-tensor $P$ which is $\delta$-correlated with an orthogonal tensor $A$ to a $4$-tensor $P'$ which is $\delta^{O(1)}$-correlated with an appropriate orthogonal $4$-tensor.
See Section~\ref{sec:3-to-4}.

\subsection{From quasi-polynomial time to polynomial time}
\label{sec:techniques-color-coding}


In this section, we describe how to evaluate certain logarithmic-degree polynomials in polynomial-time (as opposed to quasi-polynomial time).
The idea is to use color coding \cite{DBLP:journals/jacm/AlonYZ95}.\footnote{We thank Avi Wigderson for suggesting that color coding may be helpful in this context.}

For a coloring $c\from [n]\to [\ell]$ and a subgraph $\alpha\subseteq [n]^2$ on $\ell$ vertices, let $F_{c,\alpha}=\tfrac {\ell^\ell}{\ell!} \cdot \Ind_{c(\alpha)=[\ell]}$ be a scaled indicator variable of the event that $\alpha$ is colorful.

\begin{theorem}[Evaluating colorful-path polynomials]
  There exists a $n^{O(1)}\cdot \exp(\ell)$-time algorithm that given vertices $i,j\in[n]$, a coloring $c\from [n]\to [\ell]$ and an adjacency matrix $x\in \bits^{n\times n}$ evaluates the polynomial
  \begin{equation}
    p_{c}(x) \seteq \tfrac {1}{\card{\saw_\ell(i,j)}} \sum_{\alpha\in \saw_\ell(i,j)} p_\alpha(x) \cdot F_{c,a}\,.
  \end{equation}
  (Here, $p_\alpha\propto \prod_{ab\in \alpha} (x_{ab}-\tfrac dn)$ is the polynomial in \cref{eq:path-polynomial-techniques}.)
\end{theorem}

\begin{proof}
  We can reduce this problem to computing the $\ell$-th power of the following $n\cdot 2^\ell$-by-$n\cdot 2^\ell$ matrix: The rows and columns are indexed by pairs $(a,S)$ of vertices $a\in [n]$ and color sets $S\subseteq [\ell]$.
  The entry for column $(a,S)$ and row $(b,T)$ is equal to $x_{ab}-\tfrac dn$ if $T=S\cup \set{c(a)}$ and $0$ otherwise.
  If we compute the $\ell$-th power of this matrix, then the entry for column $(i,\emptyset)$ and row $(j,[\ell])$ is the sum over all colorful $\ell$-paths from $i$ to $j$.
\end{proof}

For a fixed coloring $c$, the polynomial $p_c$ does not provide a good approximation for the polynomial $p(x)\seteq \tfrac {1}{\card{\saw_\ell(i,j)}} \sum_{\alpha\in \saw_\ell(i,j)} p_\alpha (x)$.
In order to get a good approximation, we will choose random colorings and average over them.

If we let $c$ be a random coloring, then by construction $\E_c F_{c,\alpha}=1$ for every simple $\ell$-path $\alpha$.
Therefore, $\E_c p_c(x)= p(x)$ for every $x\in \bits^{n\times n}$.
We would like to estimate the variance of $p_c(x)$.
Here, it turns out to be important to consider a typical $x$ drawn from stochastic block model distribution SBM.
\begin{align}
  \E_{x\sim \sbm(n,d,\e)} \E_{c} p_c(x)^2
  &= \tfrac {1}{\card{\saw_\ell(i,j)}^2}\sum_{\alpha,\beta\in \saw_{\ell}(i,j)} \E_c F_{c,\alpha}\cdot F_{c,\beta} \cdot \E_{x\sim \sbm} p_\alpha(x)p_\beta(x)\\
  &\le e^{2\ell} \cdot \tfrac {1}{\card{\saw_\ell(i,j)}} \sum_{\alpha,\beta\in \saw_{\ell}(i,j)} \abs{\E_{x} p_\alpha(x)p_\beta(x)}
    \,.
    \label{eq:colorful-variance}
\end{align}
For the last step, we use that $\E_c F_{c,\alpha}^2\le e^{2\ell}$ (because $\ell^\ell/\ell!\le e^\ell$).

The right-hand side of \cref{eq:colorful-variance} corresponds precisely to our notion of approximate pairwise independence in \cref{lem:basis-conditions}.
Therefore, if we are within the Kesten--Stigum bound, $\e^2 d \ge 1+\delta$, the right-hand side of \cref{eq:colorful-variance} is bounded by $e^{2\ell} \cdot1/\delta^{O(1)}$.

We conclude that with high probability over $x$, the variance of $p_c(x)$ for random $c$ is bounded by $e^{O(\ell)}$.
It follows that by averaging over $e^{O(\ell)}$ random colorings we obtain a low-variance estimator for $p(x)$.


\subsection{Illustration: push-out effect in spiked Wigner matrices}
We turn to a first demonstration of our meta-algorithm beyond the stochastic block model: deriving the critical signal-to-noise ratio for (Gaussian) Wigner matrices (i.e. symmetric matrices with iid entries) with rank-one spikes.
This section demonstrates the use of \cref{thm:meta-theorem-2nd}; more sophisticated versions of the same ideas (for example our 3rd-moment meta-theorem, \cref{thm:meta-theorem-3rd}) will be used in the course of our block model algorithms.

\Dnote{}
Consider the following Bayesian estimation problem:
We are given a spiked Wigner matrix $A=\lambda \dyad v + W$ so that $W$ is a random symmetric matrix with Gaussian entries $W_{ij}\sim \cN(0,\tfrac 1n)$ and $v\sim \cN(0,\tfrac 1n \Id)$.
The goal is to estimate $v$, i.e., compute a unit vector $\hat v$ so that $\iprod{v,\hat v}^2\ge \Omega(1)$.
Since the spectral norm of a Wigner matrix satisfies $\E \norm{W}=\sqrt 2$, it follows that for $\lambda>\sqrt 2$, the top eigenvector $\hat v$ of $A$ satisfies $\iprod{v,\hat v}^2\ge \Omega(1)$.
However, it turns out that we can estimate the spike~$v$ even for smaller values of $\lambda$:
a remarkable property of spiked Wigner matrices is that as soon as $\lambda>1$, the top eigenvector $\hat v$ becomes correlated with the spike $v$ \cite{baik2005phase}.
(This property is sometimes called the ``pushout effect''.)
%
%

Unfortunately known proofs of this property a quite involved.
\Dnote{}
In the following, we apply \cref{thm:meta-theorem-2nd} to give an alternative proof of the fact that it is possible to efficiently estimate the spike $v$ as soon as $\lambda>1$.
Our algorithm is more involved and less efficient than computing the top eigenvector of $A$.
The advantage is that its analysis is substantially simpler compared to previous analyses.

%
%
%
%
%
%

\begin{theorem}[implicit in \cite{baik2005phase}]\label{thm:wigner-pushout}
  If $\lambda = 1 + \delta$ for some $1 > \delta > 0$, there is a degree ${\delta^{-O(1)}}\cdot \log n$ matrix-valued polynomial $f(A) = \{f_{ij}(A)\}_{ij \leq n} $ such that
  \[
    \frac{\E_{W,v} \Tr f(A) vv^\top}{(\E \|f(A)\|_F^2)^{1/2} \cdot (\E \|vv^\top\|_F^2)^{1/2}} \geq \delta^{O(1)}\mper
  \]
\end{theorem}
Together with Theorem~\ref{thm:meta-theorem-2nd}, the above theorem gives an algorithm with running time $n^{\log n / \delta^{O(1)}}$ to find $\hat v$ with nontrivial $\E \iprod{\hat v, v}^2$.\footnote{While this algorithm is much slower than the eigenvector-based algorithm---even after using color coding to improve the $n^{\log n/\delta^{O(1)}}$ running time to $n^{1/\delta^{O(1)}}$---the latter requires many sophisticated innovations and ideas from random matrix theory. This algorithm, by contrast, can be derived and analyzed with our meta-theorem, little innovation required.}

The analysis of \cite{baik2005phase} establishes the above theorem for the polynomial $f(A)=A^\ell$ with $\ell=\delta^{-O(1)}\cdot\log n$.
Our proof chooses a different polynomial, which affords a substantially simpler analysis.
\begin{proof}[Proof of Theorem~\ref{thm:wigner-pushout}]
  For $\alpha \subseteq \binom{n}{2}$, let $\chi_\alpha(A) = \prod_{\{i,j\} \in \alpha} A_{ij}$.
  Let $L = \log n / \delta^C$ for $C$ a large enough constant.
  For $ij \in [n]$, let $SAW_{ij}(L)$ be the collection of all self-avoiding paths from $i$ to $j$ in the complete graph on $n$ vertices.
  Observe that $\tfrac {n^{L - 1}} {\lambda^L} \chi_{\alpha}$ for $\alpha \in SAW_{ij}(L)$ is an unbiased estimator of $v_i v_j$:
  \[
  \E\Brac{ \chi_\alpha(A) \, | \, v_i, v_j } = \E_{v}\Brac{ \prod_{k\ell \in \alpha} \E_W (W_{k\ell} + \lambda v_k v_\ell) \, | \, v_i,v_j} = \lambda^L v_i v_j \E \prod_{k \in \alpha \setminus \set{i,j}} v_k^2 = \frac{\lambda^L}{n^{L-1}} \cdot v_i v_j \mper
  \]
  We further claim that the collection $\{\tfrac {n^{L - 1}} {\lambda^L} \chi_{\alpha}\}_{\alpha \in SAW_{ij}(L)}$ is approximately pairwise independent in the sense of Lemma~\ref{lem:basis-conditions}.
  To show this we must check that
  \[
  \frac {n^{2(L-1)}}{\lambda^{2L}} \sum_{\alpha, \beta} \E \chi_\alpha \chi_\beta \leq \frac 1 {\delta^2} |SAW_{ij}(L)|^2 \E v_i^2 v_j^2 
  = \frac 1 {\delta^2} |SAW_{ij}(L)|^2 \cdot \frac 1 {n^2}\mper
  \]
  The dominant contributers to the sum are $\alpha,\beta$ which intersect only on the vertices $i$ and $j$.
  In that case,
  \[
  \frac {n^{2(L-1)}}{\lambda^{2L}} \E \chi_{\alpha} \chi_\beta = {n^{2(L-1)}} \E \prod_{k \in \alpha \cup \beta} v_k^2 = \E v_i^2 v_j^2\mper
  \]
  The only other terms which might contribute to the same order are $\alpha, \beta$ such that $\alpha \cap \beta$ is a union of two paths, one starting at $i$ and one at $j$.
  If the lengths of these paths are $t$ and $t'$, respectively, and $t' + t' < L$, then
  \[
  \frac {n^{2(L-1)}}{\lambda^{2L}} \E \chi_{\alpha} \chi_{\beta} = \frac {n^{2(L-1)}}{\lambda^{2(t + t')}} \E_v\Brac{ \prod_{(k,\ell) \in \alpha \cap \beta} (\E_W A_{k\ell}^2 ) \cdot \prod_{(k,\ell) \in \alpha \triangle \beta} v_k v_\ell}
  = \frac{n^{t + t'}}{\lambda^{t + t'}} \cdot (1 + O(\lambda^2/n))^{t+t'}
  \]
  where we have used that $\E \Brac{A_{k\ell}^2 \, | \, v_k,v_\ell} = \tfrac 1 n (1 + O(\lambda^2/n)) \cdot \E v_i^2 v_j^2 $.

  There are at most $|SAW_{ij}(L)|^2/n^{t+t'}$ choices for such pairs $\alpha,\beta$, so long as $t + t' < L$.
  If $t + t' = L$, then there are $n$ times more choices than the above bound.
  All together,
  \[
  \frac {n^{2(L-1)}}{\lambda^{2L}} \sum_{\alpha,\beta \in SAW_{ij}(L)} \E \chi_\alpha \chi_\beta \leq |SAW_{ij}(L)| \cdot \Paren{\Paren{\sum_{t=0}^L \frac 1 {\lambda^t}}^2 + \frac n {\lambda^L}} \cdot \E v_i^2 v_j^2 \leq \frac {1 + o(1)} {1-1/ \lambda} \cdot |SAW_{ij}(L)| \cdot \E v_i^2 v_j^2
  \]
  where we have used that $\lambda = 1 + \delta > 1$ and chosen $C$ large enough that $n/\lambda^L \leq 1/n$. 
  Rewriting in terms of $\delta = \lambda -1$ and applying Lemma~\ref{lem:basis-conditions} finishes the proof.
\end{proof}


%



%
%
%
%
%

%

%

%
%
%
%
%

%
%

%
%

%
%
%
%

%
%
%
%

%
%
%
%

%
%
%

%

%
%

%
%