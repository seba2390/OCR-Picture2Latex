\section{Warmup: stochastic block model with two communities}
\label{sec:warmup}

We demonstrate our meta-algorithm by applying it to the two-community stochastic block model.
The algorithm achieves here the same threshold for partial recovery as the best previous algorithms \cite{DBLP:journals/corr/MosselNS13a,DBLP:journals/corr/Massoulie13}, which is also known to be the information-theoretic threshold \cite{MR3383334-Mossel15}.

While the original works involved a great deal of ingenuity, the merit of our techniques is to provide a simple and automatic way to discover and analyze an algorithm achieving the same guarantees.
\Snote{}

%

\begin{definition}[Two-community stochastic block model]
  For parameters $\epsilon, d > 0$, let $\sbm(n,d,\e)$ be the following distribution on pairs $(x,y)$ where $x \in \bits^{\binom{n}2}$ is the adjacency matrix of an $n$-vertex graph and $y\in\sbits^n$ is a labeling of the $n$ vertices.
  First, sample $y \sim \{ \pm 1\}^n$ uniformly.
  Then, independently for every pair $i<j$, add the edge $\{i,j\}$ with probability $(1+\e)\tfrac dn$ if $y_i = y_j$ and with probability $(1-\e)\tfrac dn$ if $y_i\neq y_j$.
\end{definition}
%

The following theorem gives the best bounds for polynomial-time algorithms for partial recovery in this model.
(We remark that the algorithms in \cite{DBLP:journals/corr/MosselNS13a,DBLP:journals/corr/Massoulie13} actually run in time close to linear.
In this work, we content ourselves with coarser running time bounds.)
%

\begin{theorem}[\cite{DBLP:journals/corr/MosselNS13a,DBLP:journals/corr/Massoulie13}]
  \label{thm:two-communities}
  Let $\e\in \R$, $d\in \N$ with $\delta \coloneq 1 - \tfrac 1 {\epsilon^2 d}$ and $d \le n^{o(1)}$.
  Then, there exists a randomized polynomial-time algorithm $A$ that given a graph $x\in \bits^{\binom n 2}$ outputs a labeling $\tilde y(x)$ such that for all sufficiently large $n\ge n_0(\e,d)$,
  \begin{displaymath}
    \E_{(x,y) \sim \sbm(n, d, \epsilon)} \iprod{\tilde y(x),y}^2 \geq \delta^{O(1)} \cdot n^2\,.
  \end{displaymath}
\end{theorem}
Here, the factor $n^2$ in the conclusion of the theorem normalizes the vectors $\tilde{y}(x)$ and $y$ because $\norm{\tilde{y}(x)}^2\cdot \norm{y}^2=n^2$.
%

In the remainder of this section, we will prove the above theorem by specializing our meta-algorithm for two-community stochastic block model.
For simplicity, we will here only analyze a version of algorithm that runs in quasi-polynomial time.
See \cref{sec:techniques-color-coding} for how to improve the running time to $n^{1/\poly(\delta)}$.

\begin{algorithm}
  \label[algorithm]{alg:sbm}
  For a given $n$-vertex graph $x\in \bits^{\binom{n}2}$ with average degree $d$ and some parameter $\delta>0$, execute the following steps:\footnote{The right choice of $\delta'$ will depend in a simple way on the parameters $\e$ and $d$.}
  \begin{compactenum}
  \item  evaluate the following matrix-valued polynomial $P(x) = (P_{ij}(x))$
  \begin{equation}
    P_{ij}(x)
    \coloneq
    \sum_{\alpha\in \saw_\ell(i,j)} p_\alpha(x)\,.
    \label{eq:path-polynomial}
  \end{equation}
  Here as in Section~\ref{sec:techniques}, $\saw_\ell(i,j) \subseteq {\binom {n} 2}^{\ell}$ consists of all sets of vertex pairs that form a simple (self-avoiding) path between $i$ and $j$ of length $\ell=\Theta(\log n) / \delta^{O(1)}$.\footnote{In particular, the paths in $\saw_\ell(i,j)$ are not necessarily paths in the graph $x$ but in the complete graph on $n$ vertices.}
  The polynomial $p_\alpha$ is a product of centered edge indicators, so that $p_\alpha(x)= \prod_{ab \in \alpha} \Paren{x_{ab}-\tfrac dn}$.\footnote{Up to scaling, this polynomial is a $d/n$-biased Fourier character of sparse \Erdos-\Renyi graph.}
\item compute a matrix $Y$ with minimum Frobenius norm satisfying the constraints 
  \begin{equation}
    \Set{
    \begin{aligned}
      \diag(Y)&= \mathbf 1\\
      \tfrac 1 {\norm{P(x)}_F\cdot n}\cdot\iprod{P(x),Y}&\ge \delta'\\
      Y&\succeq 0
    \end{aligned}
  }\,.
  \label{eq:correlation-constraints}
  \end{equation}
  and output a vector $\tilde y\in \sbits^n$ obtained by taking coordinate-wise signs of a centered Gaussian vector with covariance $Y$.\footnote{In other words, we apply the hyperplane rounding algorithm of Goemans and Williamson.}
  \end{compactenum}
\end{algorithm}

The matrix $P(x)$ is essentially the same as the matrix based on self-avoiding walks analyzed in \cite{DBLP:journals/corr/MosselNS13a}.
The main departure from previous algorithms lies in the second step of our algorithm.

As stated, the first step of the algorithm takes quasi-polynomial because it involves a sum over $n^\ell$ terms (for $\ell=\Theta(\log n)/\delta^{O(1)}$).
In prior works this running time is improved by using non-backtracking paths instead of self-avoiding paths.
Non-backtracking paths can be counted in $n^{O(1)}$ time using matrix multiplication, but relating the non-backtracking path polynomial to the self-avoiding path polynomial requires intensive moment-method calculations.
An alternative, described in Section~\ref{sec:techniques-color-coding}, is to compute the self-avoiding path polynomial $P$ using color-coding, requiring time $n^{O(1) + 1/\delta^{O(1)}}$, still polynomial time for any constant $\delta > 0$.

The second step of the algorithm is a convex optimization problem over an explicitly represented spectrahedron.
Therefore, this step can be carried out in polynomial time.

We break the analysis of the algorithm into two parts corresponding to the following lemmas.
The first lemma shows that if $\e^2 d > 1$ then the matrix $P(x)$ has constant correlation with $\dyad y$ for $(x,y)\sim \sbm(n,d,\e)$ and $n$ sufficiently large.
(Notice that this the main preconditon to apply meta-Theorem~\ref{thm:meta-theorem-2nd}.)

\begin{lemma}[Low-degree estimator for posterior second moment]
  \label[lemma]{lem:correlation-sbm}
  Let $\e\in \R$ and $d\in \N$, and assume $d = n^{o(1)}$.
%
  If $\delta \defeq 1 - \tfrac 1 {\e^2 d} > 0$ and $n>n_0(\e,d,\delta)$ is sufficiently large, then the matrix-valued polynomial $P(x)$ in \cref{eq:path-polynomial} satisfies
%
  \begin{equation}
    \E_{(x,y)\sim \sbm(n,d,\e)} \iprod{P(x), \dyad y} \ge \delta^{O(1)} \cdot \Paren{\E_{x\sim \sbm(n,d,\e)} \norm{P(x)}_F^2}^{1/2} \cdot n
  \end{equation}
  (Here, the factor $n$ in the conclusion normalizes the matrix $\dyad y$ because $\norm{\dyad y}_F=n$.)
\end{lemma}
By application of Markov's inequality to the conclusion of this theorem one shows that with $P$ has $\Omega(1)$-correlation with $yy^\top$ with $\Omega(1)$-probability.
As we have noted several times, the same theorem would hold if we replaced $P$, an average over self-avoiding walk polynomials, with an average over nonbacktracking walk polynomials.
This would have the advantage that the resulting polynomial can be evaluated in $n^{O(1)}$ time (i.e. with running time independent of $\delta$), rather than $n^{O(\log n)/\poly(\delta)}$ for $P$ (which can be improved to $n^{\poly(1/\delta)}$ via color coding), but at the cost of complicating the moment-method analysis.
Since we are aiming for the simplest possible proofs here we use $P$ as is.


The second lemma shows that given a matrix $P$ that has constant correlation with $\dyad y$ for an unknown labeling $y\in\sbits^n$, we can efficiently compute a labeling $\tilde y\in\sbits^n$ that has constant correlation with $y$.
We remark that for this particular situation simpler and faster algorithms work (e.g., choose a random vector in the span of the top $1/\delta^{O(1)}$ eigenvectors of $P$); these are captured by the meta-Theorem~\ref{thm:meta-theorem-2nd}, which we could use in place of the next lemma.
(We are presenting this lemma, which involves a more complex and slower algorithm, in order to have a self-contained analysis in this warmup and because it illustrates a simple form of a semidefinite programming technique that is important for our tensor decomposition algorithm, which we use for overlapping communities.)

\begin{lemma}[Partial recovery from posterior moment estimate]
  \label[lemma]{lem:recovery-sbm}
  Let $P\in \R^{n\times n}$ be a matrix and $y\in \sbits^n$ be a vector with
  $\delta'\coloneq\tfrac 1 {\norm P\cdot n}\iprod{P,\dyad y}$.
  Let $Y$ be the matrix of minimum Frobenius such that $Y\succeq 0$, $\diag Y = \mathbf 1$, and $\tfrac1 {\norm P \cdot n}\iprod{Y,P}\ge \delta'$ (i.e., the constraints \cref{eq:correlation-constraints}).
  Then, the vector $\tilde y$ obtained by taking coordinate-wise signs of a Gaussian vector with mean $0$ and covariance $Y$ satisfies
  \begin{displaymath}
    \E \iprod{\tilde y,y}^2 \ge \Omega(\delta')^2 \cdot n^2\,.
  \end{displaymath}
  (Here, the factor $n^2$ in the conclusion normalizes the vectors $\tilde y, y$ because $\norm{\tilde y}^2 \cdot \norm{y}^2=n^2$.)
\end{lemma}

\begin{proof}
  By \cref{thm:correlation-preserving-projection}, the matrix $Y$ satisfis $\iprod{Y,\dyad y}\ge (\delta'/2) \norm{Y}\cdot \norm{y}^2$ and $\norm{Y}\ge \delta \cdot \norm{y}^2$.
  In particular, $\iprod{Y,\dyad y}\ge \delta^2 n^2/ 2$.
  The analysis of rounding algorithm for the Grothendieck problem on psd matrices \cite{DBLP:conf/stoc/AlonN04}, shows that $\E \iprod{\tilde y, y}^2 \ge \tfrac 2 {\pi} \iprod{Y,\dyad y}\ge \Omega (\delta^2)\cdot n^2$.
  (Here, we use that $\dyad y$ is a psd matrix.)
\end{proof}

Taken together, the above lemmas imply a quasi-polynomial time  algorithm for partial recovery in $\sbm(n,d,\e)$ when $\e^2d>1$.

\begin{proof}[Proof of \cref{thm:two-communities} (quasi-polynomial time version)]
  Let $(x,y)\sim \sbm(n,d,\e)$ with $\delta\coloneq 1- 1/\e^2 d>0$.
  Run \cref{alg:sbm} on $x$ with the parameter $\delta'$ chosen as $\tfrac 1 {10}$ times the correlation factor in the conclusion of \cref{lem:correlation-sbm}.

  Then, by \cref{lem:correlation-sbm},
  $\E_{(x,y)\sim \sbm(n,d,\e)}\iprod{P(x),\dyad y}\ge 10\delta'\cdot \E_{x\sim \sbm(n,d,\e)}\norm{P(x)} \cdot n$.
  By a variant of Markov inequality \cref{fact:exp-to-prob}, the matrix $P(x)$ satisfies with constant probability $\iprod{P(x),\dyad y}\ge \delta'\cdot \norm{P(x)} \cdot n$.
  In this event, by \cref{lem:recovery-sbm}, the final labeling $\tilde y$ satisfies $\E_{\tilde y} \iprod{\tilde y, y}^2\ge \Omega(\delta')^2\cdot n^2$.
  Since this event has constant probability, the total expected correlation satisfies $\E_{(x,y)\sim \sbm(n,d,\e)}\iprod{\tilde y(x),y}^2\ge \Omega(\delta')^2\cdot n^2$ as desired.
\end{proof}

It remains to prove \cref{lem:correlation-sbm}.

\subsection{Low-degree estimate for posterior second moment}
\label{sec:low-degree-simple-community}

We will apply \cref{lem:basis-conditions} to prove Lemma~\ref{lem:correlation-sbm}.
The next two lemmas verify that the conditions of that lemma hold; they immediately imply Lemma~\ref{lem:correlation-sbm}.
\begin{lemma}[Unbiased estimators for $y_i y_j$]\label{lem:unbiased-sbm}
  For $i,j \in [n]$ distinct, let $\saw_\ell(i,j)$ be the set of all simple paths from $i$ to $j$ in the complete graph on $n$ vertices of length $\ell$.
  Let $x_{ij}$ be the $ij$-th entry of the adjacency matrix of $G \sim \sbm(n,d,\e)$, and for $\alpha \in \saw_\ell(i,j)$, let $p_\alpha(x) = \prod_{ab \in \alpha} (x_{ab} - \tfrac d n)$.
  Then for any $y_i,y_j \in \{ \pm 1\}$ and $\alpha \in \saw_\ell(i,j)$,
  \[
    \Paren{\frac n {\e d}}^{\ell} \E \Brac{p_\alpha(x) \, | \, y_i y_j} = y_i y_j\mper
  \]
\end{lemma}

Thus, each simple path $\alpha$ from $i$ to $j$ in the complete graph provides an unbiased estimator $(n/\e d)^\ell p_\alpha(x)$ of $y_i y_j$.
It is straightforward to compute that each has variance $\Paren{\tfrac n {\epsilon^2 d}}^\ell$.
If they were pairwise independent, they could be averaged to give an estimator with variance $\tfrac 1 {|\saw_\ell(i,j)|} \cdot \Paren{\tfrac n {\epsilon^2 d}}^\ell = n (\epsilon^2 d)^{-\ell}$, since there are $n^{\ell - 1}$ simple paths from $i$ to $j$.
If $\ell$ is logarithmic in $n$, this becomes small.
The estimators are not strictly pairwise independent, but they do satisfy an approximate pairwise independence property which will be enough for us.

\begin{lemma}[Approximate conditional independence]\label{lem:indep-sbm}
  Suppose $\delta \defeq 1 - \tfrac 1 {\epsilon^2 d} \geq \Omega(1)$ and $d = n^{o(1)}$.
   For $i,j \in [n]$ distinct, let $\saw_\ell(i,j)$ be the set of all simple paths from $i$ to $j$ in the complete graph on $n$ vertices of length $\ell = \Theta(\log n)/\delta^C$ for a large-enough constant $C$.
  Let $x_{ij}$ be the $ij$-th entry of the adjacency matrix of $G \sim \sbm(n,d,\e)$.
  Let $p_\alpha(x) = \prod_{ab \in \alpha} (x_{ab} - \tfrac d n)$.
  Then
  \[
    \E y_i^2 y_j^2 \sum_{\alpha, \beta \in \saw_\ell(i,j)} \E p_\alpha(x) p_\beta(x) \leq \delta^{- O(1)} \cdot \sum_{\alpha, \beta \in \saw_\ell(i,j)} \Paren{\E p_\alpha(x) y_i y_j }\Paren{\E p_\beta(x) y_i y_j }\mper
  \]
\end{lemma}

To prove the lemmas we will use the following fact; the proof is straightforward.
\begin{fact}\label{fact:edge-sbm}
  For $x,y \sim \sbm$, the entries of $x$ are all independent conditioned on $y$, and $a,b$ distinct,
  \[
    \E\Brac{x_{ab} - \tfrac d n \, | \, y_a, y_b} = \frac {\e d } n \cdot y_a y_b \quad \text{ and } \quad \E \Brac{\Paren{x_{ab} - \tfrac dn}^2 \, | \, y_a, y_b} = \frac d n \Paren{1 + \epsilon y_a y_b + O(d/n)}\mper
  \]
\end{fact}

We can prove both of the lemmas.

\begin{proof}[Proof of Lemma~\ref{lem:unbiased-sbm}]
  We condition on $y$ and expand the expectation.
  \begin{align*}
    \E \Brac{p_\alpha(x) \, | \, y_i y_j} = \E_y\Brac{ \prod_{ab \in \alpha} \E[x_{ab} - \tfrac d n \, | \, y]}
    = \Paren{\frac{\epsilon d}{n}}^\ell \E_y\Brac{\prod_{ab \in \alpha} y_a y_b} \quad \text{  by Fact~\ref{fact:edge-sbm}.}
  \end{align*}
  Because $\alpha$ is a path from $i$ to $j$, every index $a \in [n]$ except for $i$ and $j$ appears exactly twice in the product.
  So, removing the conditioning on $y_a$ for all $a \neq i,j$, we obtain $\E \Brac{p_\alpha(x) \, | \, y_i y_j} = \Paren{\tfrac{\e d} n}^\ell \cdot y_i y_j$ as desired.
\end{proof}

The proof of Lemma~\ref{lem:indep-sbm} is the heart of the proof, and will use the crucial assumption $\epsilon^2 d > 1$.
\begin{proof}[Proof of Lemma~\ref{lem:indep-sbm}]
  Let $\alpha, \beta \in \saw_\ell(i,j)$, and suppose that $\alpha$ and $\beta$ share $r$ edges.
  Let $\alpha \triangle \beta$ denote the symmetric difference of $\alpha$ and $\beta$.
  Then
  \begin{align*}
    \E p_\alpha(x) p_\beta(x) & = \E_{y}\Brac{ \prod_{ab \in \alpha \cap \beta} \E_x \Brac{(x_{ab} - \tfrac dn)^2 \, | y_a, y_b} \cdot \prod_{ab \in \alpha \triangle \beta} \E_x \Brac{x_{ab} - \tfrac dn \, | y_a, y_b}}\\
    & = \Paren{\frac d n}^{2\ell - r} \epsilon^{2\ell -  2r} \E_y\Brac{ \prod_{ab \in \alpha \cap \beta} (1 + \epsilon y_a y_b + O(d/n)) \cdot \prod_{ab \in \alpha \triangle \beta} y_a y_b }
  \end{align*}
  using Fact~\ref{fact:edge-sbm} in the second step.
  Since $\alpha$ and $\beta$ are paths, the graph $\alpha \triangle \beta$ has all even degrees, so $\prod_{ab \in \alpha \triangle \beta} y_a y_b = 1$.
  Furthermore, any subgraph of $\alpha \cap \beta$ contains some odd-degree vertex.
  So $\E_y \prod_{ab \in \alpha \cap \beta} (1 + \epsilon y_a y_b + O(d/n)) = (1 + O(d/n))^{r}$.
  All in all, we obtain
  \begin{align}\label{eq:share-r-sbm}
    \E p_\alpha(x) p_\beta(x) = \Paren{\frac d n}^{2\ell - r} \epsilon^{2\ell -  2r} (1 + O(d/n))^{r}
  \end{align}

  Suppose $r < \ell$.
  Paths $\alpha, \beta$ sharing $r$ edges must share at least $r$ vertices.
  If they share exactly $r$ vertices, then the shared vertices must form paths in $\alpha$ and $\beta$ beginning at $i$ and $j$.
  Since each path has length $\ell$ and therefore contains $\ell-1$ vertices in addition to $i$ and $j$, there are at most $r \cdot n^{2(\ell -1) - r}$ such pairs $\alpha, \beta$ (the multiplicative factor $r$ comes because the shared paths starting from $i$ and $j$ could have lengths between $0$ and $r$).
  Other pairs $\alpha, \beta$ share $r$ edges but $s$ vertices for some $s > r$.
  For each $s$ and $r$, there are at most $n^{2(\ell-1) - s} \ell^{O(s-r)}$ such pairs, because the shared edges must occur as at most $s - r$ paths.
  Furthermore, $\ell^{O(s - r)} n^{-(s-r)} \leq n^{-\Omega(1)}$ when $s > r$.
  Putting all of this together,
  \begin{align*}
    \sum_{\alpha, \beta \in \saw_\ell(i,j)} \E p_\alpha(x) p_\beta(x) & \leq n^{-2} \cdot \Brac{\sum_{r = 0}^{\ell - 1} d^{2\ell - r} \epsilon^{2\ell -  2r} (1 + O(d/n))^{r} \Paren{r + n^{-\Omega(1)}}  + (\e^2 d)^\ell\cdot n} \\
    & = n^{-2} \cdot (1 + n^{-\Omega(1)}) \cdot (\e d)^{2\ell} \cdot \Paren{\sum_{r = 0}^\ell r \cdot (\e^2 d)^{-r} + (\epsilon^2 d)^{-\ell} \cdot n}\mcom
  \end{align*}
  The additive factor of $(\e^2 d)^\ell n$ in the first line comes from the case $r = \ell$ (i.e., $\alpha = \beta$), where there are $n^{\ell - 1}$ paths.
  In the second line we have used the assumption that $d \ll n$ to simplify the expression.
  Finally, by convergence of the series $\sum_{m = 0}^\infty m \cdot z^m$ for $|z| < 1$, and the choice of $\ell$ logarithmic in $n$, this is at most
  \[
    (1 + n^{-\Omega(1)}) \cdot (\e d)^{2\ell} \cdot \Paren{\frac 1 {1 - \tfrac 1 {\epsilon^2 d}}}^{O(1)}\mper
  \]
  So, now our goal is to show that
  \[
    \sum_{\alpha,\beta \in \saw_{\ell}(i,j)} (\E p_\alpha(x) y_i y_j) (\E p_\beta(x) y_i y_j) \geq n^{-2} \cdot (1 + n^{-\Omega(1)}) \cdot (\e d)^{2\ell} \cdot \Paren{\frac 1 {1 - \tfrac 1 {\epsilon^2 d}}}^{O(1)}\mper
  \]
  Each term in the left-hand sum is $(\e d/ n)^{2\ell}$ (by Lemma~\ref{lem:unbiased-sbm}) and there are $\Omega(n^{2\ell - 2})$ such terms, so the left-hand side of the above is at least $\Omega((\e d)^{2\ell} / n^2)$.
  This proves the Lemma.
 \end{proof}




\iffalse
\subsection{OLD}

%

It is possible to implement the algorithm which follows in polynomial time, but because our main purpose is exposition we will discuss only a quasi-polynomial time version.

\paragraph{Preliminaries}
If $M \in \R^n$ is a matrix, we write $\|M\|_F = \Tr MM^\top$ for its Frobenius norm.

We will work with an inner product $\iprod{\cdot, \cdot}$ defined by the distribution $G(n,d,\epsilon)$.
\[
  \iprod{f,g} = \E_{G \sim G(\epsilon, n, d)} f(G) g(G)\mper
\]
If $M, N : \{\pm 1\}^{\binom{n}{2}} \rightarrow \R^{m \times m}$ are matrix-valued functions, we write
\[
  \iprod{M,N} = \E_{G \sim G(\epsilon, n, d)} \Tr M(G) N(G)^\top\mper
\]
We define norms on functions and matrix-valued functions $\|f\| = \iprod{f,f}^{1/2}$ and $\|M\| = \iprod{M,M}^{1/2}$.

For a graph $G$ and $i, j \in [n]$ with $i \neq j$, let
\[
  G_{ij} = \begin{cases}
    \sqrt{\tfrac {n}{d}\cdot (1-\tfrac dn)} & \text{ if $ij$ is an edge,}\\
    \sqrt{\tfrac d n \cdot \tfrac 1 {1-\tfrac dn}} & \text{ otherwise.}
  \end{cases}
\]
For $\alpha \subseteq {\binom{n}{2}}$, let $G_\alpha = \prod_{(i,j) \in \alpha} G_{ij}$.
Then, the functions $\set{G_\alpha \mid \alpha \in {\binom{n}{2}}}$ (often called the $\tfrac d n$-biased characters) form an orthonormal basis for real-valued functions on graphs drawn from the \Erdos-\Renyi distribution $G(n,\tfrac d n)$.
(Crucially, these functions are \emph{not} orthogonal with respect to the distribution $G(\epsilon,n,d)$.

If $f : \{\pm 1\}^{\binom{n}{2}} \rightarrow \R$ is a real-valued function on $n$-node graphs, it is uniquely expressable as a polynomial $f(G) = \sum_{\alpha \subseteq {\binom{n}{2}}} \widehat{f}(\alpha) G_\alpha$.
The \emph{degree} of $f$ is $\max_{\alpha : \widehat{f}(\alpha) \neq 0} |\alpha|$.
For a function $f$ and $D \in \N$, we write $f^\leq D$ for the projection of $f$ to the degree-$D$ polynomials with respect to the inner product $\iprod{ \cdot, \cdot }$.
That is,
\[
  f^{\leq D} = \argmax_{\deg g \leq D} \frac{\iprod{f,g}}{\|g\|}\mper
\]




\paragraph{Algorithm for Partial Recovery}
We get set up to prove Theorem~\ref{thm:two-communities}.
The theorem follows from two lemmas.
The first captures the low-degree polynomials phase of the algorithm.
\begin{lemma}\label{lem:2com-lowdeg}
  Suppose $\epsilon^2 d \geq 1 + \Omega(1)$.
  Let $M(G) = \E_{G(\epsilon, n, d)}[xx^\top \, | \, G]$ map graphs to the associated posterior distribution on vertex labelings.
  Then there is $D = O(\log n)$ so that
  \[
    \frac{\iprod{M^{\leq D}, M}}{\|M^{\leq D} \| \cdot \|M\|} \geq \Omega(1)\mper
  \]
  Furthermore, $\|M\| \geq \|M^{\leq D}\| \geq \Omega(n)$.
\end{lemma}

The second lemma captures the rounding phase of the algorithm.

\begin{lemma}\label{lem:2com-projection}
  Suppose $M \in \R^{n \times n}$ satisfies $M \succeq 0$ and $M_{ii} = 1$ for all $i \in [n]$. Suppose $N \in \R^{n \times n}$ satisfies
  \[
    \frac{\Tr MN^\top}{\|M\|_F \cdot \|N\|_F} \geq \delta\mper
  \]
  For any $\eta > 0$, let $X$ be the optimal solution to the following convex program.
  \begin{align*}
    \min \|X\|_F \text{ such that}\\
    X_{ii} = 1 \quad \forall i \in [n]\\
    \Tr XN^\top \geq \eta \cdot \Tr MN^\top
  \end{align*}
  Then
  \[
     \frac{\Tr XM^\top}{\|M\|_F \cdot \|X\|_F} \geq \frac{\eta \cdot \Tr MN^\top}{2 \cdot \|M\|_F \cdot \|N\|_F}
  \]
%
\end{lemma}

We also need one fact from approximation algorithms, on rounding certain semidefinite programs by Gaussian sampling followed by thresholding by signs.\footnote{Sometimes this algorithm is called ``Hyperplane Rounding''.}

\begin{fact}[See e.g. Theorem 6.16 in \cite{williamson-shmoys}]\label{fact:hyperplane-rounding}
  Let $X \succeq 0$ with $X_{ii} = 1$, let $M \succeq 0$, and suppose $\Tr XM^\top \geq C$.
  Let $x$ be a random vector given by sampling $g \sim \cN(0,X)$ and setting $x_i = \sign(g_i)$.
  Then
  \[
    \E_{x} \iprod{x,Mx} \geq \Omega(C)\mper
  \]
\end{fact}

Now we can prove Theorem~\ref{thm:two-communities}.
\begin{proof}[Proof of Theorem~\ref{thm:two-communities}]
  On an input graph $G$,
  \begin{enumerate}
    \item Compute $N = M^{\leq D}(G)$, where $D = O(\log n)$ is as in Lemma~\ref{lem:2com-lowdeg}.
    \item Solve the convex program in Lemma~\ref{lem:2com-projection} with $N = M^{\leq D}(G)$ to obtain a matrix $Y \succeq 0$.
    \item Sample $g \sim \cN(0,Y)$ and output the labeling $y$ where $y_i = \sign(g_i)$.
  \end{enumerate}
  To see that the algorithm can be implemented in time $n^{O(\log n)}$, notice that the values of at most $n^{D} = n^{O(\log n)}$ monomials $G_\alpha$ must be computed in the first step.
  Steps (2) and (3) require only polynomial time.

  Suppose $\epsilon^2 d \geq 1 + \Omega(1)$.
  Our goal is to show
  \[
    \tfrac 1 {n^2} \E_{x, G \sim G(\epsilon,n,d)} \E_y \iprod{x,y}^2 \geq \Omega(1)\mcom
  \]
  where $y = y(G)$ is the output of the algorithm.

  For a graph $G$, let $M(G)$ be the matrix $M(G) = \E[xx^\top | G]$, where the distribution on $x$ is the posterior distribution on labelings conditioned on the graph $G$.
  We can rearrange the left-hand side of the above as
  \[
    \tfrac 1 {n^2} \E_{x, G \sim G(\epsilon,n,d)} \E_y \iprod{x,y}^2 = \tfrac 1 {n^2} \E_{G,y} \iprod{y,M(G)y}\mper
  \]
By Lemma~\ref{fact:hyperplane-rounding}, it is enough to show that the matrix $Y$ in the algorithm has $Y_{ii} = 1$ and $\tfrac 1 {n^2} \E_{G} \Tr YM(G)^\top \geq \Omega(1)$.
  The constraint $Y_{ii} = 1$ is enforced in the convex program in Lemma~\ref{lem:2com-projection}.
  Furthermore, for every fixed $G$,
  \[
    \|Y\|_F \geq \frac{\Tr YM^{\leq D}(G)^\top}{\|M^{\leq D}(G)\|_F} \geq \frac{\Tr M^{\leq D}(G) M(G)^\top}{\|M^{\leq D}(G)\|_F}
  \]
  From Lemma~\ref{lem:2com-projection}, 
  \[
    \Tr Y M(G)^\top \geq \frac{\|Y\|_F \Tr M^{\leq D}(G) M(G)^\top}{2 \|M^{\leq D}_F\|} \geq \frac{(\Tr M^{\leq D}(G) M(G)^\top)^2}{2 \|M^{\leq D}(G)\|_F^2}
  \]
Because $\|M^{\leq D}\|^2 = \E_G \|M^{\leq D}(G)\|_F^2 \leq n^2$, for any constant $c$ we have $\|M^{\leq D}\| \leq O(n)$ with probability $ 1 - c$.
\Snote{}
  At the same time, $n^2 \geq \iprod{M^{\leq D}, M} \geq \Omega(n^2)$ by Lemma~\ref{lem:2com-lowdeg}, so for some constant $c'$, we have
  $\Pr\{ \Tr M^{\leq D}(G) M(G)^\top > c' n^2 \} \geq c'$.
  Thus, with probability $\Omega(1)$,
  \[
    \frac{(\Tr M^{\leq D}(G) M(G)^\top)^2}{\|M^{\leq D}(G)\|_F^2} \geq \Omega(n^2)
  \]
  and so the theorem follows.
\end{proof}

\subsection{Estimating the posterior moments with low-degree polynomials}
In this section we prove the most important lemma, Lemma~\ref{lem:2com-lowdeg}.
The first proposition we need concerns the expectations of certain characters $G_\alpha$.
\begin{proposition}\label{prop:2com-alpha-moment}
  Let $\alpha$ be a path in the complete graph on $n$ nodes of length $\ell$, from $i \in [n]$ to $j \in [n]$.
  Then
  \[
    \E_{x,G \sim G(\epsilon, n, d)} x_i x_j G_\alpha = \Paren{\e  \sqrt{\tfrac d n}(1 + O(\tfrac d n))}^{\ell}\mper
  \]
Furthermore, for any $i,j \in [n]$,
  \[
    \E_{G(n,d,\epsilon} G_{ij}^2 = 1\mper
  \]
\Snote{}
\end{proposition}
  The proof is a straightforward calculation (and is a special case of Lemma~\ref{lem:iprod-1}), which we provide here to make this warmup self-contained.
\begin{proof}
  Elementary calculations show that for every $x \in \{ \pm 1\}^n$ and $i \neq j$,
  \[
    \E[G_{ij} | x_i, x_j] = x_i x_j \epsilon  \sqrt{\tfrac dn}\Paren{1+O\Paren{\tfrac dn}}\mper
  \]
  Thus,
  \begin{align*}
    \E G_\alpha x_i x_j & = \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^\ell \E_x x_i x_j \prod_{(a,b) \in \alpha} x_a x_b \\
    & = \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^\ell\mcom
  \end{align*}
  where we have used that every $x_a$ appears with multiplicity $2$ in the product, because $\alpha$ is a path.

  The second claim is a straightforward computation.
\end{proof}

Now we can prove the lemma.
\begin{proof}[Proof of Lemma~\ref{lem:2com-lowdeg}]
  Let $D = D(n)$ be a paramter to be set later.
  If $P(G)$ is any matrix-valued function of degree $\deg P \leq D$, then
  \[
    \frac{\iprod{P,M}}{\|P\| \cdot \|M\|} \leq \frac{\iprod{M^{\leq D},M}}{\|M^{\leq D} \| \cdot \|M\|}
  \]
  by definition of the projection $M^{\leq D}$.
  We will exhibit a particular $P$ for which this ratio is $\Omega(\epsilon)$.

  For $i,j \in [n]$, let
  \[
    A_{ij} = \left \{ \alpha \subseteq {\binom{n}{2}} \, : \, \alpha \text{ is a path  in the complete graph from $i$ to $j$ of length $D$} \right \}\mper
  \]
  Let $P(G)$ be the following matrix polynomial.
  \[
    P_{ij}(G) = \sum_{\alpha \in A_{ij}} G_\alpha\mper
  \]
  Using the proposition,
  \[
    \iprod{P(G)_{ij}, M_{ij}} = \sum_{\alpha \in A_{ij}} \E_{x,G} G_\alpha x_i x_j = |A_{ij}| \cdot \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^D
  \]

  Next we estimate $\|P_{ij}(G)\|$.
  We write $V(\alpha)$ for the vertices $i \in [n]$ with nonzero degree in $\alpha$.
  First, we address the correlation $\E G_\alpha G_\beta$ for $\alpha$ and $\beta$ in $A_{ij}$ which are maximally disjoint.
  \begin{align*}
    \sum_{\alpha, \beta \in A_{ij}, V(\alpha) \cap V(\beta) = \{ i,j \} } \E G_\alpha G_\beta
    & = \sum_{\alpha, \beta \in A_{ij}, V(\alpha) \cap V(\beta) = \{ i,j \} } \E G_{\alpha \cup \beta}
     \leq |A_{ij}|^2 \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{2D}\mper
  \end{align*}

  What happens when $\alpha, \beta$ share some edges?
  Let $\alpha, \beta \in A_{ij}$.
  Then
  \[
    \E G_\alpha G_\beta = \E G_{\alpha \triangle \beta} G_{\alpha \cap \beta}^2 = \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{|\alpha \triangle \beta|}\mper
  \]
  So,
  \begin{align*}
    \|P_{ij}(G)\|^2 & = \sum_{\alpha,\beta \in A_{ij}} \E G_\alpha G_\beta\\
    & = \sum_{t \leq D} \sum_{\alpha,\beta \in A_{ij}, |\alpha \cap \beta| = t} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{2(D - t)}
  \end{align*}
  How many choices of $\alpha, \beta \in A_{ij}$ are there with $|\alpha \cap \beta| = t$?
  If $ t \leq D - 1$, such $\alpha, \beta$ must also have $|(V(\alpha) \cap V(\beta)) \setminus \{i,j\}| \geq t$.
  So, there are at most $n^{2(D - 1) - t}$ such pairs $\alpha, \beta$.
  When $\alpha = \beta$, there are $n^{D-1}$ choices.
  \footnote{This bound is worse by a factor of $n$ than the case $\alpha \neq \beta$, and is the technical reason we will have to take $D \approx \log n$.}
  Using this bound above,
  \[
    \|P_{ij}(G)\|^2 \leq \sum_{t < D} n^{2(D - t - 1)} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{2(D - t)} + n^{D-1} 
  \]
  Now, $|A_{ij}| = \Omega(n^{D -1})$.
  So,
  \begin{align*}
    \frac{\iprod{P(G)_{ij}, M_{ij}}}{\|P_{ij}\|} &
    \geq \frac{n^{D-1} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^D} { \Paren{\sum_{t < D} n^{2(D - 1) - t)} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{2(D - t)} + n^{D-1}}^{1/2}}\\
    & = \frac 1 { \Paren{ \sum_{t < D} n^{-t}   \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{-2t} + n^{-(D-1)} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{-2D}  }^{1/2}}\mper
  \end{align*}
  Simplifying the denominator,
  \begin{align*}
    & \sum_{t < D} n^{-t}   \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{-2t} + n^{-(D-1)} \Paren{\epsilon  \sqrt{\tfrac dn }\cdot \Paren{1+O\Paren{\tfrac dn}}}^{-2D}\\
     & \quad = \sum_{t < D} \Paren{\e^2 d \Paren{1 + O(\tfrac d n)}}^{-t} + n \Paren{\e^2 d \Paren{1 + O(\tfrac d n)}}^{-D} \\
     & \quad = \frac 1 {1 - \tfrac 1 {\e^2 d(1 + O(\tfrac dn))}} + o(1)\\
     & = O(1)\mper
  \end{align*}
  In the second-to-last step we have used that $\e^2 d \geq 1 + \Omega(1)$ and chosen some $D = \Theta(\log n)$.
  In the last we have used the series expansion of $\tfrac 1 {1 - x} = 1 + x^2 + x^3 + \ldots$.

  This shows that for every $i,j$,
  \[
    \frac{\iprod{P_{ij}, M_{ij}}}{\|P_{ij}\|\cdot \|M_{ij}\|} \geq \Omega(1)\mper
  \]
(Using that $|M_{ij}(G)| \leq 1$.)
  By Cauchy-Schwarz, $\|M\| \cdot \|P\| \geq \sum_{i,j} \|P_{ij}\| \cdot \|M_{ij}\|$, and the first part of the lemma follows.

  For the second part, observe that the above calculations imply
  \[
    \frac{\iprod{M,P}}{\|P\|} = \Omega(n)\mcom
  \]
and by the definition of projection,$\|M\| \geq \|M^{\leq D}\| \geq \tfrac{\iprod{M,P}}{\|P\|}$.
\end{proof}

\subsection{Max-entropy semidefinite program}
We prove Lemma~\ref{lem:2com-projection}, which actually follows from a much more general fact.
\Snote{}
Then Lemma~\ref{lem:2com-projection} follows immediately by setting $v = M$ and $w = N$, and observing that the convex program presented there computes the projection of $0$ to an appropriate convex set.
\fi