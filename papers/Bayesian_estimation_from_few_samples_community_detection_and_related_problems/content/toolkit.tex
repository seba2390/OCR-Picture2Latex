\section{Toolkit and Omitted Proofs}


\subsection{Probability and linear algebra tools}
\begin{fact}
  \label{fact:exp-to-prob}
  Consider any inner product $\iprod{\cdot, \cdot}$ on $\R^n$ with associated norm $\|\cdot \|$.
  Let $X$ and $Y$ be joinly-distributed $\R^n$-valued random variables.
  Suppose that
    $\|X\|^2 \leq C \E \|X\|^2$
    with probability $1$, and that
  \[
    \frac{\E \iprod{X,Y}}{(\E \|X\|^2)^{1/2} (\E \|Y\|^2)^{1/2}} \geq \delta\mper
  \]
  Then
  \[
    \Pr\left \{ \frac{\iprod{X,Y}}{\|X\| \cdot \|Y\|} \geq \frac \delta 2 \right \} \geq \frac{\delta^2}{4C^2}\mper
  \]
\end{fact}


\begin{proof}[Proof of Fact~\ref{fact:exp-to-prob}]
  Let $\Ind_E$ be the $0/1$ indicator of an event $E$.
  Note that
  \[
    \E\Brac{ \iprod{X,Y} \Ind_{\iprod{X,Y} \leq \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|}}
    \leq \tfrac \delta 2 \E \|X\| \cdot \|Y\| \leq \tfrac \delta 2 (\E \|X\|^2)^{1/2} (\E \|Y\|^2)^{1/2}\mper
  \]
  Hence,
  \[
  \E\Brac{ \iprod{X,Y} \Ind_{\iprod{X,Y} > \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|}}
    \geq \tfrac \delta 2 \E \|X\| \cdot \|Y\| \leq \tfrac \delta 2 (\E \|X\|^2)^{1/2} (\E \|Y\|^2)^{1/2}\mper
  \]
  At the same time,
  \begin{align*}
    \E\Brac{ \iprod{X,Y} \Ind_{\iprod{X,Y} > \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|}}
    & \leq \Paren{\E \|X\|^2\cdot \|Y\|^2}^{1/2}
    \cdot\Paren{ \E \Ind_{\iprod{X,Y} > \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|}}^{1/2}\\
    & = \Paren{\E \|X\|^2\cdot \|Y\|^2}^{1/2}
    \cdot\Paren{ \Pr \{ \iprod{X,Y} > \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|\} }^{1/2}\\
    & \leq C (\E \|X\|^2)^{1/2} (\E \|Y\|^2)^{1/2}
    \cdot\Paren{ \Pr \{ \iprod{X,Y} > \tfrac \delta 2 \cdot \|X\| \cdot \|Y\|\} }^{1/2}\mper
  \end{align*}
  Putting the inequalities together and rearranging finishes the proof.
\end{proof}



\begin{proof}[Proof of Proposition~\ref{prop:bernstein-tails}]
  We decompose $X_i$ as
  \[
    X_i = X_i\Ind_{|X_i| \leq R} + X_i \Ind_{|X_i| > R}\mper
  \]
Let $Y_i = X_i \Ind_{|X_i| \leq R}$.
Then
  \[
    |\E Y_i| = |\E X_i - \E X_i \Ind_{|X_i| > R}| \leq \delta'
  \]
  and
  \[
    \Var Y_i \leq \E Y_i^2 \leq \E X_i^2\mper
  \]
  So we can apply Bernstein's inequality to $\tfrac 1 m \sum_{i \leq m} Y_i$ to obtain that
  \[
    \Pr \left \{ \Abs{\tfrac 1 m \sum_{i \leq m} Y_i} \geq t + \delta' \right \} \leq \exp\Paren{\frac{- \Omega(1) \cdot m \cdot t^2}{\E X^2 + t\cdot R}}\mper
  \]
Now, with probability at least $1 - \delta$ we know $X_i = Y_i$, so by a union bound,
  \[
    \Pr \left \{ \Abs{\tfrac 1 m \sum_{i \leq m} X_i} \geq t + \delta' \right \} \leq \exp\Paren{\frac{- \Omega(1) \cdot m \cdot t^2}{\E X^2 + t\cdot R}} + m \delta \mper\qedhere
  \]
\end{proof}



\begin{fact}\label{fact:scalar-to-vector}
  Let $\{X_1,\ldots,X_n, Y_1,\ldots,Y_m \}$ are jointly distributed real-valued random variables.
  Suppose there is $S \subseteq [m]$ with $|S| \geq (1 - o_m(1))\cdot m$ such that for each $i \in S$ there a degree-$D$ polynomials $p_i$ satisfying
  \[
    \frac{\E p_i(X) Y_i}{(\E Y^2)^{1/2} (\E p_i(X)^2)^{1/2}} \geq \delta\mper
  \]
  Furthermore, suppose $\sum_{i \in S} \E Y_i^2 \geq (1 - o(1)) \sum_{i \in [m]} \E Y_i^2$.
  Let $Y \in \R^m$ be the vector-valued random variable with $i$-th coordinate $Y_i$, and similarly let $P(X)$ have $i$-th coordinate $p_i(X)$.
  Then
  \[
    \frac{\E \iprod{P(X),Y}}{(\E \|Y\|^2)^{1/2} \cdot (\E \|P(X)\|^2)^{1/2}} \geq (1 - o(1)) \cdot \delta
  \]
\end{fact}
\begin{proof}
  The proof is by Cauchy-Schwarz.
  \begin{align*}
    \E \iprod{P(X),Y} & = \sum_{i \in S} \E p_i(X) Y\\
                      & \geq \delta \sum_{i \in S} (\E p_i(X)^2)^{1/2} (\E Y_i^2)^{1/2}\\
                      & \geq \delta \Paren{\E \sum_{i \in S} p_i(x)^2}^{1/2} \cdot (1 - o(1)) \Paren{\sum_{i \in [m]} Y_i^2}^{1/2}\mper\qedhere
  \end{align*}
\end{proof}


\subsection{Tools for symmetric and Dirichlet priors}
\begin{proof}[Proof of Fact~\ref{fact:diagonal-moments}]
  Let $X$ be any $\R^k$-valued random variable which is symmetric in distribution with respect to permutations of coordinates and satisfies $\sum_{s \in [k]} X(s) = 0$ with probability $1$.
  (The variable $\tsigma$ is one example.)

  We prove the claim about $\E \iprod{X,x} \iprod{X,y} \iprod{X,z} \iprod{X,w}$; the other proofs are similar.
  Consider the matrix $M = \E (X \tensor X)(X \tensor X)^\top$.
  Since $x,y,z,w$ are orthogonal to the all-$1$'s vector, we may add $1 \tensor v$, for any $v \in \R^n$, to any row or column of $M$ without affecting the statement to be proved.
  Adding multiples of $1 \tensor e_i$ to rows and columns appropriately makes $M$ a block diagonal matrix, with the top block indexed by coordinates $(i,i)$ for $i \in [k]$ and the bottom block indexed by pairs $(i,j)$ for $i \neq j$.

  The resulting top block takes the form $c \Id + c' J$, where $J$ is the all-$1$'s matrix.
  The bottom block will be a matrix from the Johnson scheme.
  Standard results on eigenvectors of the Johnson scheme (see e.g. \cite{DBLP:conf/colt/DeshpandeM15} and references therein) finish the proof.
  The values of constants $C$ for the Dirichlet distribution follow from the next fact.
\end{proof}


\begin{fact}
  \label{fact:dirichlet-covariance}
  Let $\sigma \in \R^k$ be distributed according to a (symmetric) Dirichlet distribution with parameter $\alpha$.
  That is, $\Pr(\sigma) \propto \prod_{j \in [k]} \sigma^{\alpha - 1}$.

  Let $\gamma \in \N^k$ be a $k$-tuple, and let $\sigma^\gamma = \prod_{j \leq k} \sigma_j^{\gamma_j}$.
  Let $|\gamma| = \sum_{j \leq k} \gamma_j$.
  Then
  \[
    \E \sigma^\gamma =  \frac{\Gamma(k\alpha)}{\Gamma(k\alpha + |\gamma|)} \cdot \frac{\prod_{j \leq k} \Gamma(\alpha + \gamma_j)}{\Gamma(\alpha)^k}\mper
  \]

  Furthermore, let $\tilde \sigma \in \R^k$ be given by
  $\tilde \sigma_i = \sigma_i - \tfrac 1 k$.
  Then
  \[
    \E \tsigma \tsigma^\top = \frac{\Gamma(k\alpha)}{\Gamma(k\alpha + 2)} \Paren{\frac{\Gamma(\alpha + 2)} {\Gamma(\alpha)} - \frac{\Gamma(\alpha + 1)^2}{\Gamma(\alpha)^2}} \cdot \Pi = \frac 1 {k(k\alpha + 1)} \cdot \Pi \mcom
  \]
where $\Pi \in \R^{k \times k}$ is the projector to the subspace orthogonal to the all-$1$s vector.
\end{fact}
\begin{proof}
  We recall the density of the $k$-dimensional Dirichlet distribution with parameter vector $\alpha_1,\ldots,\alpha_k$.
  Here $\Gamma$ denotes the usual Gamma function.
  \[
    \Pr\{ \sigma \} = \frac{\Gamma(\sum_{j \leq k} \alpha_j )}{\prod_{j \leq k} \Gamma(\alpha_j)} \cdot \prod_{j \leq k} \sigma_j^{\alpha_j - 1}\mper
  \]
  In particular,
  \[
    \frac{\Gamma(\sum_{j \leq k} \alpha_j )}{\prod_{j \leq k} \Gamma(\alpha_j)} \cdot \int \prod_{j \leq k} \sigma_j^{\alpha_j - 1} \, d\sigma = 1
  \]
where the integral is taken with respect to Lebesgue measure on $\{ \sigma \, : \sum_{j \leq k} \sigma_j = 1 \}$.

  Using this fact we can compute the moments of the symmetric Dirichlet distribution with parameter $\alpha$.
  We show for example how to compute second moments; the general formula can be proved along the same lines.
  For $s \neq t \in [k]$,
  \begin{align*}
    \E \sigma_s \sigma_t & =  \frac{\Gamma(k\alpha)}{\Gamma(\alpha)^k} \cdot \int \! \sigma_s \sigma_t \prod_{j \leq k} \sigma_j^{\alpha - 1}\\
    & = \frac{\Gamma(k\alpha)}{\Gamma(k\alpha + 2)} \cdot \frac{\Gamma(\alpha + 1)^2}{\Gamma(\alpha)^2} \cdot \frac{\Gamma(k\alpha + 2)}{\Gamma(\alpha)^{k-2}\Gamma(\alpha +  1)^2} \cdot \int \! \sigma_s^{(\alpha + 1) - 1} \sigma_t^{(\alpha + 1) - 1} \prod_{j \neq s,t} \sigma_j^{\alpha - 1}\\
    & = \frac{\Gamma(k\alpha)}{\Gamma(k\alpha + 2)} \cdot \frac{\Gamma(\alpha + 1)^2}{\Gamma(\alpha)^2}\mper
  \end{align*}
  Similarly,
  \[
    \E \sigma_s^2 = \frac{\Gamma(k\alpha)}{\Gamma(k\alpha + 2)} \cdot \frac{\Gamma(\alpha + 2)}{\Gamma(\alpha)}\mper
  \]

  The formula for $\E \tilde \sigma \tsigma^\top$ follows immediately.
\end{proof}