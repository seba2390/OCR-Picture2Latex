\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}
% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[dvipsnames]{xcolor}         %colors
\usepackage{subcaption}
\usepackage{epsfig}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{rotating,tabularx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{soul}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage[font=scriptsize,labelfont=bf]{caption}
%\usepackage{subfig}
\definecolor{mygreen1}{RGB}{116,152,210}
\definecolor{mygreen}{RGB}{240,41,51}
%[sorting=none,style=numeric-comp]
% \addbibresource{ash.bib} %Import the bibliography file

% % Commenting highlights

% Uncomment one of the two lines below to turn todos on/off
%\commentsfalse
\input{math_commands}

\title{GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Sima Behpour\thanks{corresponding author: \texttt{sima.behpour@us.bosch.com}} \hspace{5mm} Thang Doan \hspace{5mm} Xin Li \hspace{5mm} Wenbin He \hspace{5mm} Liang Gou \hspace{5mm} Liu Ren  \\
  Bosch Research North America $\&$ \\ Bosch Center for Artificial Intelligence (BCAI)\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand\norm[1]{\lVert#1\rVert}

\begin{document}


\maketitle


\begin{abstract}
Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of \textbf{most important parameters} of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of in-distribution (ID) data.
In particular, we identify OOD data by computing the norm of gradient projection on \textit{the subspaces considered \textbf{important} for the in-distribution data}. A large orthogonal projection value (i.e. a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notable reduction in the average false positive rate at a 95\% true positive rate (FPR95) of up to 8\% when compared to the current state-of-the-art methods.


\end{abstract}
\section{Introduction}\vspace{-0.3cm}
The issue of identifying out-of-distribution (OOD) data, which falls outside training data distributions, has become a significant focus in deep learning. OOD data challenges real-world model deployment, as it can lead to unreliable or incorrect predictions, particularly in safety-critical applications such as healthcare, autonomous vehicles, and physical sciences \cite{filos2020can, wang2017chestx,zhou2020mortality, agarwal2021learning, char2021model, boyer2021machine}. This problem arises because modern Deep Neural Networks (DNNs) produce overconfident predictions on OOD inputs, complicating the separation of in-distribution (ID) and OOD data \cite{zhou2017places,nguyen2015deep}. The main goal of OOD detection is to develop methods that can accurately detect when a model encounters OOD data, allowing the model to either reject these inputs or provide more informative responses, such as uncertainty indication or confidence measures.

Many studies have investigated approaches to detecting OOD in deep learning~\cite{chen2021robustifying, hendrycks17baseline, huang2021mos, lakshminarayanan2017simple, lee2018simple, liang2018enhancing, lin2021mood, liu2020energy,  mohseni2020self, nalisnick2018deep}. The majority of prior work focused on calculating OOD uncertainty from the activation space of a neural network, for example, by using model output~\cite{hendrycks17baseline,lakshminarayanan2017simple,liang2018enhancing, liu2020energy} or feature representations~\cite{lee2018simple}. Another line of studies like ODIN \cite{liang2018enhancing}, GradNorm~\cite{huang2021importance}, and ExGrad~\cite{igoe2022useful} leverage the gradient information of deep neural network models to compute OOD uncertainty score and achieve performant results. 
GradNorm~\cite{huang2021importance} investigates the richness of the gradient space and presents that gradients provide valuable information for OOD detection. In particular, GradNorm utilizes the vector norm of gradients explicitly as an OOD scoring function.
GradNorm, however, considers the full gradient space information, which might be noisy and lead to sub-optimal solutions.

A recent direction of research employ network parameter sparsification to improve OOD detection performance like DICE \cite{sun2022dice} and ASH \cite{ djurisic2022extremely}. ASH removes a majority of the activation by obtaining the ${p}$th-percentile of the entire representation. However, the potential consequence may result in diminished performance due to the partial removal of critical parameters within the pre-trained network. Also, this is an empirical approach without a principled way to sparsify models.  

Based on key observations presented in \textit{gradient-based} and \textit{sparcification-based} OOD detection methods, an intriguing insight emerges: the crucial discriminative features for OOD data identification reside within the gradient subspace of the ID data. This suggests that by focusing on the gradient information in the subspace of the ID data, which captures the most salient information, we can enhance the accuracy and reliability of OOD data detection algorithms. However, it takes non-trivial work to identify such an efficient gradient subspace. 

Inspired by recent low-rank factorization research for DNNs \cite{yang2020learning,tai2015convolutional,hu2021lora,swaminathan2020sparse}, indicating the intrinsic model information resides in a few low-rank dimensions, we introduce a novel approach named \textbf{GradOrth}. More specifically, the proposed method, GradOrth, distinguishes OOD samples by employing \textit{orthogonal gradient projection} in the \textit{low-rank subspaces} of ID data (Fig.\ref{fig:projection}). These ID subspaces ($S^L$ in Fig.\ref{fig:projection}) are derived through singular value decomposition (SVD) of pre-trained network activations, specifically on a small subset of randomly selected ID data. By leveraging SVD, GradOrth effectively computes and identifies the relevant subspaces associated with the ID data, enabling accurate discrimination of OOD samples through orthogonal gradient projection. A large magnitude (Fig.\ref{fig:projection}-a) of orthogonal projection  (i.e. small projection) serves as a significant criterion for classifying a sample as OOD since it captures a weak correlation with the ID data. 

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{images/projection4.png}
\vspace{-3mm}    \caption{ The main idea of \textit{GradOrth}: Measuring the orthogonal projection of the gradient of a testing sample onto a $k$-dimension (e.g $k$=2 here) subspace of pre-trained network on ID data, by the angle $\alpha$ between $\overrightarrow{g(x_i)} =\nabla_{\theta^L} \mathcal{L}(\mtheta^L)$ and $\overrightarrow{O(x_i)} =P_S^L(\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L))$. If $\alpha$ is large (i.e. small projection on subspace $\mathcal{S}^L$), shown in (a), the sample $x_i$ is weakly correlated to ID data, and therefore it is recognized as OOD. Otherwise, it is ID data, shown in (b). }  
    \label{fig:projection}
    \vspace{-4mm}
\end{wrapfigure}
Our key results and contributions are:

\quad- We present GradOrth, a novel and efficient method for out-of-distribution (OOD) detection. Our approach leverages the \textbf{most important} parameter space of a pre-trained network and its \textbf{gradients} to accomplish this task. To the best of our knowledge, GradOrth is the pioneering endeavor to investigate and showcase the efficacy of the subspace of a DNN's gradients in OOD detection.

\quad- We evaluate the performance of GradOrth on widely-used benchmarks, and it demonstrates competitive results compared to other post-hoc out-of-distribution (OOD) detection baselines. Notably, GradOrth surpasses the strong baseline methods achieving a reduction in FPR95 ranging from $2.71\%$ to $8.05\%$. Moreover, our experiments highlight that GradOrth effectively enhances OOD detection capabilities while maintaining high accuracy in classifying in-distribution (ID) data.

\quad- We present comprehensive analyses, including ablation experiments and theoretical investigation, aimed at enhancing the understanding of our proposed method for out-of-distribution (OOD) detection. Through these rigorous analyses, we aim to provide valuable insights and improve the overall comprehension of the intricacies and effectiveness of our OOD detection approach.

\section{Background and Our Notations}\vspace{-0.2cm} 
We consider a neural network with $L$ layers and a set of learning parameters $\theta=\{\theta^l\}_{l=1}^L$ where $\theta^l$ present the learning parameters of layer $l$. $\emph{x}_{i}^{l}$ denotes the \textbf{representation of input} $\emph{x}_{i}$ at layer $l$ in the successive layers given the data input $x_i$. The network performs the following computation at each layer:
\begin{align}
    \emph{x}_{i}^{l+1} = \sigma(f(\theta^l,\emph{x}_{i}^{l})), \quad l=1,...L, 
\end{align}
where, $\sigma(.)$ is a non-linear function and $f(. , .)$ is a linear function.   We leverage matrix notation for input ($\mX_{i}$) in convolutional layers and vector notation for input ($\emph{x}_{i}$) in fully connected layers. In the first layer, $\emph{x}_{i}^{1}=\emph{x}_{i}$ refers to the raw input data. It is noteworthy to mention that our approach holds applicability across all layers of the network. However, our experimental investigations reveal that the last layer yields the most optimal performance, please refer to appendix, section \ref{app:net-layers} for our empirical studies report. This preference is advantageous as it alleviates significant time complexity arising from gradient computations across multiple network layers.



\subsection{Input and Gradient Space}\vspace{-3mm}
 Our algorithm capitalizes on the intrinsic property of stochastic gradient descent (SGD) updates lying within the span of input data points, as validated by \cite{zhang2021understanding,hu2021lora,yang2021generalized}. The subsequent subsections present this relationship, specifically in fully connected layers. We present the details regarding convolutional layers in the appendix, section \ref{app:conv}.  

\textbf{Fully Connected Layer} \label{sec:fc-layer}
Consider a single-layer linear neural network in a supervised learning setup where each (input, label) training data pair is driven from a training dataset, $\sD$. We use $\vx \in \R^n$ to present the input vector, $\vy \in \R^m$ to present the label vector in the dataset, and $\mtheta \in \R^{m\times n} $ to express the learning parameters (weights) of the network. In general,  the network is trained by minimizing a loss function (e.g. mean-squared error) as follows:
\begin{equation}\label{eq1}
   \mathcal{L} = \frac{1}{2} || \mtheta \vx -\vy ||_{2}^2.
\end{equation}
Following stochastic gradient optimization, we can present the gradient of this loss with respect to weights as: \begin{equation}\label{eq2}
    \nabla_{\mtheta} \mathcal{L} = (\mtheta \vx -\vy) \vx^T = \bm{\Omega} \vx^T,
\end{equation}
Here, $\bm{\Omega} \in \mathbb{R}^m$ denotes the error vector. Consequently, the gradient update will reside within the input span ($\vx$), wherein the elements in $\bm{\Omega}$ exhibit varying magnitudes, thus influencing the scaling of $\vx$ accordingly, please refer to section \ref{app:span} for the proof. For simplicity, we have considered per-example loss (batch size of 1) and mean-squared loss function here. Furthermore, it is important to mention that the aforementioned relationship remains applicable even in the context of the mini-batch setting or when utilizing alternative loss functions such as cross-entropy loss, where the calculation of $\Omega$ may differ. For more comprehensive information on this subject, please consult the appendix, specifically section \ref{app:minibatch}. The input-gradient relationship in equation \ref{eq2} can be applied to any fully connected layer of a neural network where $\vx$ is the input to that layer and $\bm{\Omega}$ is the error coming from the next layer. In addition, this equation also applies to the networks with non-linear units (such as ReLU) and cross-entropy losses, though $\bm{\Omega}$ will be calculated differently. 
\subsection{Matrix Approximation with SVD:}\vspace{-3mm} In our algorithm, we utilize singular value decomposition (SVD) for matrix factorization. Specifically, a rectangular matrix $\mR = \mU\mSigma\mV^T \in \mathbb{R}^{m\times n}$ can be factorized using SVD into the product of three matrices. Here, $\mSigma$ represents a matrix containing the singular values sorted along its main diagonal, while $\mU \in \mathbb{R}^{m\times m}$ and $\mV \in \mathbb{R}^{n\times n}$ denote orthogonal matrices \cite{mml}. In the case where the rank of the matrix $\mR$ is $r$ ($r\leq \text{min}(m,n)$), the matrix $\mR$ can be represented as $\mR=\sum_{i=1}^r \sigma_i\vu_i \vv_i^T$, where $\sigma_i \in \text{diag}(\mSigma)$ denotes the singular values and $\vu_i \in \mU$ as well as $\vv_i \in \mV$ represent the left and right singular vectors, respectively. Furthermore, we can formulate the $k$-rank approximation to the matrix as $\mR_k=\sum_{i=1}^k \sigma_i\vu_i \vv_i^T$, where $k\leq r$. The specific value of $k$ can be determined as the smallest value that satisfies the condition $||\mR_k||F^2 \geq \epsilon_{th}||\mR||F^2$. In this equation, $||.||F$ represents the Frobenius norm of the matrix, and $\epsilon_{th}$ ($0 <\epsilon_{th} \leq 1$) serves as the threshold \cite{mml}. For a comprehensive explanation of the singular value decomposition (SVD) method, please refer to the appendix section labeled "SVD Explanation" (see appendix \ref{app:svdExplanation}).
\subsection{Problem Statement: Out-of-distribution Detection}\label{sec:problem-statement}\vspace{-3mm}
OOD is typically characterized by a distribution that represents unknown scenarios encountered during deployment. These scenarios involve data samples originating from an irrelevant distribution, whose label set has no intersection with the predefined set. Consider the supervised setting where a neural network is given access to a set of training data $\sD=\{(\mathbf{x}_i,y_i)\}_{i=1}^N$ drawn from an unknown joint data distribution $P$ defined on $\mathcal{X}\times \mathcal{Y}$ in the training phase. We denote the input space and output space by $\mathcal{X}=\mathbb{R}^n$ and $\mathcal{Y}=\{1,2,..., m \}$, respectively. 
\begin{equation}
    z(\mathbf{x}) = 
    \begin{cases}
    \text{ID}, &\text{if}\ O(\mathbf{x}) \geq \gamma \\
    \text{OOD}, &\text{if}\ O(\mathbf{x}) < \gamma.
    \label{oodness}
    \end{cases}
\end{equation}
  The parameter $\gamma$ is typically selected to ensure a high percentage of correct classification for in-distribution (ID) data, such as 95\%. A major hurdle is to establish a scoring function $O(\mathbf{x})$ that effectively captures the uncertainty associated with out-of-distribution (OOD) samples. Prior approaches have predominantly relied on various factors, including the model's output, gradients, or features, to estimate OOD uncertainty \cite{huang2021importance,djurisic2022extremely}. In our proposed approach, we aim to compute the scoring function $O(\mathbf{x})$ by leveraging  \textit{orthogonal gradient projection} on \textit{ parameter subspace} of a pre-trained network over ID data. The details of our methodology are described in the subsequent section.
\vspace{-0.2cm}
\subsection{Orthogonal Projection}\vspace{-2mm}
In this section, we discuss the concept of orthogonal projection and our notation, which holds significant importance in our methodology. To simplify the explanation, we will present it in a 2D space, but it can be extended to higher dimensions. Orthogonal projection serves as a metric that we utilize to calculate the distance between a vector $\overrightarrow{V}$ and a space W (represented as a matrix in this case).
\begin{wrapfigure}{r}{0.4\textwidth}\vspace{-2mm}
\includegraphics[width=0.4\textwidth]{images/ortho-pro.png}
  \caption{ Orthogonal Projection}
  \label{ortho-pro}
  \vspace{-5.6mm}
\end{wrapfigure}
The result of the orthogonal projection of vector $\overrightarrow{V}$ onto space W consists of three essential components:
(1) The orthogonal projection vector $\overrightarrow{b}$,
(2) the projection vector $\overrightarrow{c}$,
and (3) the angle $\alpha$.
These three components' values can be utilized to determine the correlation between the vector $\overrightarrow{V}$ and the space W. As depicted in the figure \ref{ortho-pro}, a larger value of $\overrightarrow{b}$indicates a weaker correlation. On the other hand, $\overrightarrow{c}$ exhibits the opposite pattern, where a larger value of $\overrightarrow{c}$ indicates a stronger correlation. Depending on the specific application requirements, any of these values can be chosen to compute the correlation.
In order to align with the OOD (out-of-distribution) score, where a smaller value indicates a higher degree of OODness as presented in equation \ref{oodness}, we incorporate the projection vector ($\overrightarrow{\textit{c}}$) into our computations.
\section{Our Method: GradOrth}\vspace{-0.2cm}
In this section, we describe our method GradOrth where we recognize ID Vs. OOD data from a different view of previous studies, computing the norm of gradient projection on \textit{the subspaces considered \textbf{important} for the in-distribution data}.

 We define a sample as out-of-distribution (OOD) data if its orthogonal projection value is large (i.e. small projection value), indicating a weak correlation with the in-distribution (ID) data.

GradOrth OOD detection is developed following these steps:
\begin{enumerate}
\item \textbf{Pre-trained Network subspace Computation:} Our $L$-layer neural network with learning parameter $\theta$ is trained using \textbf{in-distribution (ID)} data. Upon completion of the training process, the model parameters $\theta$ are frozen, resulting in a pre-trained network specialized in ID data. It is worth mentioning that we can also leverage the existing pre-trained network over our interest ID data. To retain the most significant parameters of the pre-trained network with respect to the ID data, we compute the network's last layer ($L$) subspace. For this purpose, we construct a representation matrix denoted as $\mR_{ID}^L =[\vx_{1}^L, \vx_{2}^L, ..., \vx_{n}^L ]$, which concatenates $n$ representations obtained from the network's last layer ($L$) through the forward pass of $n$ randomly selected samples (a small subset, $n \ll N$) of the ID data.

Next, we perform singular value decomposition (SVD) on $\mR_{ID}^L$, resulting in $\mR_{ID}^L =\mU_{ID}^L\mSigma_{ID}^L(\mV_{ID}^L)^T$. We then proceed to approximate its rank $k$ by obtaining $(\mR_{ID}^L)_k$, guided by the given criteria that rely on a specified threshold, denoted as $\epsilon_{th}$:
\begin{equation}\label{eq:ct1}
    ||(\mR_{ID}^L)_k||_F^2 \geq \epsilon_{th}||\mR_{ID}^L||_F^2.
\end{equation}
The pre-trained network subspace, denoted as $S^L=span\{\vu_{1}^L,\vu_{2}^L, ...,\vu_{k}^L\}$, is defined as the \textbf{space of significant representation} for the pre-trained network at the last layer $L$. This subspace is spanned by the first $k$ vectors in $U_{ID}^L$ and encompasses all directions associated with the highest singular values in the representation. 
We store this subspace, $S^L$, and leverage it in the next step. We present the algorithm to compute the ID subspace in Algorithm 1.

\item \textbf{Inference with OOD Data:}  During the inference phase, the pre-trained model is exposed to an OOD sample $x_i$. The OOD sample is propagated through the pre-trained network, and subsequently, its gradient at layer L is computed which is presented as $g (x_i)=\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L)$.

\item\textbf{Detector Construction:} The model is transformed into a \textit{detector} by generating a \textbf{score} based on its output, enabling the differentiation between ID and OOD inputs. To this end, we compute the norm of sample gradient projection onto the subspace of the pre-trained network ($S$). 
We compute projection of the gradients $\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L)$ onto the subspace $\mathcal{S}^L$ as follows: 
\begin{align}
    P_{S^L}(\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L))=(\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L))\mathcal{S}^L(\mathcal{S}^L)'.
    \label{eq:projection}
\end{align}
Here, $(.)'$ presents the matrix transpose. Next, we define the OOD score for the sample as follows by computing the projection norm:
\begin{align}
    O(x_i)= \norm{P_{S^L}(\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L))}
    \label{grad_orthScore}
\end{align}
This score serves as a surrogate to characterize the correlation between the sample and ID data that the pre-trained network trained on it. As presented in figure \ref{fig:projection}, it implies a weak correlation between the new sample $x_i$ and ID when the gradient $g(x_i)=\nabla_{\mtheta^L} \mathcal{L}(\mtheta^L)$ has a small projection (large orthogonal projection) onto the subspace of the pre-trained network (large angle $\alpha$) due to the fact that stochastic gradient descent (SGD) updates lie in the span of input data points \cite{zhu2017learning}, please refer to the appendix, section \ref{app:span} for the proof. 
Algorithm 2 presents OOD score computation.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/algs2.png}
   
    \label{fig:my_label}
\end{figure}
\end{enumerate}

\section{Experiments}\vspace{-0.2cm}
In this section, we evaluate the performance of our method \textit{GradOrth} running extensive experiments considering different ID/OOD datasets and network architectures. We follow the experiment setting in general OOD baselines and explain the experimental setup in Section~\ref{sec:exp_setup}. These empirical studies demonstrate the superior performance of \textit{GradOrth} over existing state-of-the-art baselines that are reported in Section~\ref{sec:benchmarks}. We report extensive ablations and analyses that provide a deeper understanding of our methodology, please refer to the appendix, section \ref{app:ablation}.

\subsection{Experimental Setup}
\label{sec:exp_setup}
\vspace{-0.2cm}
\textbf{Dataset} 
We leverage 2 benchmarks proposed by \cite{huang2021mos} and \cite{djurisic2022extremely} for detecting out-of-distribution (OOD) images that are based on the large-scale ImageNet dataset and CIFAR dataset. To provide a fair comparison, we run our experiments leveraging 5 subspaces computed using random samples from ID data and report the average performance over these subspaces.

\textbf{ImageNet Benchmark:} This benchmark is more challenging than others because it has higher-resolution images and a larger label space of 1,000 categories. To test our approach, we evaluate four OOD test datasets, including subsets of \texttt{iNaturalist}\cite{van2018inaturalist}, \texttt{SUN}\cite{xiao2010sun}, \texttt{Places}\cite{zhou2017places}, and \texttt{Textures}\cite{cimpoi2014describing}. These datasets have non-overlapping categories compared to the ImageNet-1k dataset and cover a diverse range of domains including fine-grained, scene, and textural images. We follow the experimental setting reported in \cite{djurisic2022extremely} and use the ResNet-50 model \cite{he2016deep} pre-trained on ImageNet-1k.  For a fair comparison, all the methods use the same pre-trained backbone, without regularizing with auxiliary
outlier data. Details and hyperparameters of baseline methods can be found in appendix \ref{app:hyper}.  
The outcomes of this study present results obtained by applying GradOrth after the last fully connected layer in all the experiments. In this configuration, the feature size is $2048$ for ResNet-50 and $1280$ for MobileNetV2. For subspace computation, we choose 10 random samples per class and set the SVD threshold to $0.97$. 

\textbf{CIFAR Benchmark:} We evaluate our approach on the commonly used  CIFAR-10~\cite{krizhevsky2009learning}, and CIFAR-100~\cite{krizhevsky2009learning} benchmarks as in-distribution data following the experimental setting in \cite{djurisic2022extremely,sun2022out}. We employ the standard split with 50,000 training images and 10,000 test images. For subspace computation, we choose 5 random samples per class and set the SVD threshold to $0.97$. We assess the model on six widely used OOD benchmark datasets: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}. Regarding pre-trained network architecture, we use DenseNet-101 architecture~\cite{huang2017densely}. We leverage pre-trained networks over ID datasets. Please refer to section \ref{app:hyper} in the appendix for more details regarding the experiment setting. 
 It is important to note that no modifications were made to the network parameters during the OOD detection phase.
\paragraph{Evaluation Metrics}
We assess the effectiveness of our proposed method by utilizing threshold-free metrics that are commonly used for evaluating OOD detection, as standardized in~\cite {hendrycks17baseline}. These metrics include (i) AUROC, which stands for the Area Under the Receiver Operating Characteristic curve; and (ii) FPR95, which is the false positive rate. FPR95 represents the probability that a negative (i.e. OOD) example is misclassified as positive (i.e. ID) when the true positive rate is as high as 95~\cite{Liang2017}.

\label{sec:benchmarks}

\subsection{Results and Discussion}\vspace{-0.2cm}
Our experimental studies present the promising performance of OrthoGrad in OOD detection on two benchmarks, ImageNet and CIFAR benchmarks.
\paragraph{ImageNet Benchmark:}  Our method demonstrates competitive performance, reaching the state-of-the-art level, as indicated in Table-\ref{tab:imagenet}.
On the ResNet pre-trained network, GradOrth surpasses ASH-S, ASH-B, and ASH-S by $0.45\%$, $2.47\%$, and $0.93\%$ in terms of FPR95 on the iNaturalist, SUN, and Textures OOD datasets, respectively. When evaluated on the Places OOD dataset, our method achieves an FPR95 of $33.67\%$ and secures the second rank after ASH-B. Furthermore, GradNorm demonstrates an average FPR95 performance of $18.57\%$, outperforming ASH-B by $3.98\%$.

It is important to acknowledge the fact that GradOrth boasts a \textit{low computational complexity}. It only requires computing the subspace of the pre-trained network once and can be conveniently utilized through a simple gradient calculation, without the need for hyper-parameter tuning or additional training during OOD detection. In contrast, certain methods like Mahalanobis \cite{lee2018simple} require collecting feature representations from intermediate layers for the entire training set, which can be computationally expensive for large-scale datasets like ImageNet. Additionally, GradOrth presents a \textit{stable performance} across most datasets whereas the performance of ASH versions varies across the four OOD datasets. ASH-B outperforms other baselines on the Places dataset but ranks third, second, and third on the other three datasets. A similar pattern is observed for ASH-S in terms of FPR95, where it ranks second, sixth, sixth, and second across the iNaturalist, SUN, Places, and Textures datasets, respectively.

GradOrth also exhibits superior performance in terms of AUROC, outperforming ASH-S by an average of $2.80\%$ across the four datasets. Particularly, GradOrth surpasses ASH-S, ASH-B, and ASH-S by $0.13\%$, $0.66\%$, and $0.46\%$ on the iNaturalist, SUN, and Textures OOD datasets, respectively.

For the pre-trained MobileNet model, our GradOrth approach also demonstrates outstanding performance. We present experimental results on leveraging MobileNet as the pre-trained identification (ID) network and evaluate the OOD detection performance on the iNaturalist, SUN, Places, and Textures datasets (the bottom section of Table-\ref{tab:imagenet}). In these experiments, GradOrth demonstrates outstanding performance. In terms of FPR95, GradOrth outperforms ASH-B, DICE+ReAct, DICE+ReAct, and ASH-S by $4.65\%$, $0.40\%$, $6.50\%$, and $0.43\%$, respectively, across the four datasets. Regarding AUROC, GradOrth outperforms other baselines on average by at least $4.01\%$ and $0.57\%$ in terms of FPR95 and AUROC, respectively.




\begin{table}[hbt!]
\resizebox{\textwidth}{!}{\begin{tabular}{|c| c| c| c| c| c| c| c| c| c| c| c|}
\hline
 & & \multicolumn{8}{c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{c|}{}\\\cline{3-10}
\textbf{Model} & \textbf{Methods} & \multicolumn{2}{c}{\textbf{iNaturalist}} & \multicolumn{2}{|c|}{\textbf{SUN}} & \multicolumn{2}{|c|}{\textbf{Places}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Average}} \\
 
& & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
& & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ \\
\midrule

\multirow{10}{*}{ResNet} & \multicolumn{1}{l|}{Softmax score} & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 \\
& \multicolumn{1}{l|}{ODIN} & 47.66 & 89.66 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 \\
& \multicolumn{1}{l|}{Mahalanobis} & 97.00 & 52.65 & 98.50 & 42.41 & 98.40 & 41.79 & 55.80 & 85.01 & 87.43 & 55.47 \\
& \multicolumn{1}{l|}{Energy score} & 55.72 & 89.95 & 59.26 & 85.89 & 64.92 & 82.86 & 53.72 & 85.99 & 58.41 & 86.17 \\
& \multicolumn{1}{l|}{GradNorm} & 42.46 & 90.33 & 40.73 & 89.96 & 43.48 & 80.64 & 34.48 & 88.43 & 40.29 & 87.34 \\
& \multicolumn{1}{l|}{ExGrad} & 54.11 & 76.91 & 46.73 & 69.74 & 50.62 & 74.27 & 38.12 & 79.37 &47.40&75.07  \\
& \multicolumn{1}{l|}{ReAct} & 20.38 & 96.22 & 24.20 & 94.20 & 33.85 & 91.58 & 47.30 & 89.80 & 31.43 & 92.95 \\
& \multicolumn{1}{l|}{DICE} & 25.63 & 94.49 & 35.15 & 90.83 & 46.49 &  87.48 & 31.72 & 90.30 & 34.75 & 90.77 \\
& \multicolumn{1}{l|}{DICE + ReAct} & 18.64 & 96.24 & 25.45 & 93.94 & 36.86 & 90.67 & 28.07 & 92.74 & 27.25 & 93.40 \\
& \multicolumn{1}{l|}{VRA-DN} &  16.82 & 96.92 &30.65&  93.65& 39.94  &90.75   &26.72 & 95.04&28.53 & 94.09\\
&\multicolumn{1}{l|}{VRA-P} &  15.70 & 97.12 &26.94 & 94.25& 37.85 & 91.27   & 21.47  &95.62&25.49 & 94.57\\
 & \multicolumn{1}{l|}{ASH-P } & 44.57 & 92.51 & 52.88 & 88.35 & 61.79 & 85.58 & 42.06 & 89.70 & 50.32 & 89.04 \\
 & \multicolumn{1}{l|}{ASH-B }  & 14.21 & 97.32 & 22.08 & 95.10 &\textbf{33.45} & \textbf{92.31} & 21.17 & 95.50 & 22.73 & 95.06 \\
 & \multicolumn{1}{l|}{ASH-S}  & 11.49 & 97.87 & 27.98 & 94.02 & 39.78 & 90.98 & 11.93 & 97.60 & 22.80 & 95.12 \\
 &\multicolumn{1}{l|}{GradOrth (Ours)}& \textbf{11.04}&\textbf{98.00}& \textbf{19.61}&\textbf{95.76}& 33.67 & 91.78 & \textbf{11.19} & \textbf{98.06} & \textbf{18.57} & \textbf{96.31}\\
\midrule
\multirow{10}{*}{MobileNet} & \multicolumn{1}{l|}{Softmax score} & 64.29 & 85.32 & 77.02 & 77.10 & 79.23 & 76.27 & 73.51 & 77.30 & 73.51 & 79.00 \\
& \multicolumn{1}{l|}{ODIN} & 55.39 & 87.62 & 54.07 & 85.88 & 57.36 & 84.71 & 49.96 & 85.03 & 54.20 & 85.81 \\
& \multicolumn{1}{l|}{Mahalanobis} & 62.11 & 81.00 & 47.82 & 86.33 & 52.09 & 83.63 & 92.38 & 33.06 & 63.60 & 71.01 \\
& \multicolumn{1}{l|}{Energy score} & 59.50 & 88.91 & 62.65 & 84.50 & 69.37 & 81.19 & 58.05 & 85.03 & 62.39 & 84.91 \\
& \multicolumn{1}{l|}{ReAct} & 42.40 & 91.53 & 47.69 & 88.16 & 51.56 & 86.64 & 38.42 & 91.53 & 45.02 & 89.47 \\
& \multicolumn{1}{l|}{DICE} & 43.09 &  90.83 & 38.69 & 90.46 & 53.11 & 85.81 & 32.80 & 91.30 & 41.92 & 89.60 \\
& \multicolumn{1}{l|}{DICE + ReAct} & 32.30 & 93.57 & 31.22 & 92.86 & 46.78 & 88.02 & 16.28 & 96.25 & 31.64 & 92.68 \\

& \multicolumn{1}{l|}{ASH-P} & 54.92 & 90.46 & 58.61 & 86.72 & 66.59 & 83.47 & 48.48 & 88.72 & 57.15 & 87.34 \\
 & \multicolumn{1}{l|}{ASH-B} & 31.46& \textbf{94.28} & 38.45 & 91.61 & 51.80 & 87.56 & 20.92 & 95.07 & 35.66 & 92.13\\
 & \multicolumn{1}{l|}{ASH-S} & 39.10 & 91.94 & 43.62 & 90.02 & 58.84 & 84.73 & 13.12 & 97.10 & 38.67 & 90.95 \\
& \multicolumn{1}{l|}{GradOrth (Ours)} & \textbf{26.81} & 93.17 & \textbf{30.82} & \textbf{93.18} & \textbf{40.27} & \textbf{89.12} & \textbf{12.69} & \textbf{97.52} & \textbf{27.65} & \textbf{93.25} \\
\bottomrule
\end{tabular}}

\caption{ OOD detection results with \textbf{ImageNet-1k} as ID. GradOrth present outstanding performance in average and across most datasets. We adopted the identical table format and evaluation metrics as introduced in~\cite {react, djurisic2022extremely}. The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-1k dataset. We use $\uparrow$ to denote that larger values are preferable, and $\downarrow$ to denote that smaller values are preferable. All values are presented as percentages. All values in the table are directly taken from Table 1 of~\cite{djurisic2022extremely} except for the gradient-based methods (GradNorm, ExGrad, GradOrth (ours)). For GradNorm and ExGrad, we run this experiment leveraging the code provided by the authors. 
}
\label{tab:imagenet}
\vspace{-4mm}
\end{table}

\paragraph{CIFAR Benchmark: } 
\begin{table}
\scalebox{0.58}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule
Softmax score  & 47.24 & 93.48 & 33.57 & 95.54 & 42.10 & 94.51 & 42.31 & 94.52 & 64.15 & 88.15 & 63.02 & 88.57 & 48.73 & 92.46 \\
ODIN  & 25.29 & 94.57 & 4.70 & 98.86 & \textcolor{ForestGreen}{\textbf{3.09} }& \textcolor{cyan}{\textbf{99.02}} & \textcolor{cyan}{\textbf{3.98}} & \textcolor{cyan}{\textbf{98.90}} & 57.50 & 82.38 & 52.85 & 88.55 & 24.57 & 93.71 \\
Mahalanobis  & \textcolor{ForestGreen}{\textbf{6.42}}  & 98.31 & 56.55 & 86.96 & 9.14 & 97.09 & 9.78 & 97.25 & \textcolor{ForestGreen}{\textbf{21.51}} & 92.15 & 85.14 & 63.15 & 31.42 & 89.15 \\
Energy score & 40.61 & 93.99 & 3.81 & 99.15 & 9.28 & 98.12 & 10.07 & 98.07 & 56.12 & 86.43 & \textcolor{ForestGreen}{\textbf{39.40}} & 91.64 & 26.55 & 94.57 \\ 
GradNorm & 18.63 & 94.11& 1.03 & 99.61& \textcolor{orange}{\textbf{3.38}} & 98.87& 36.89 & 91.67 & 50.26 & 89.72&50.43& 84.29 & 26.77& 93.04\\ 
ReAct  & 41.64 & 93.87 & 5.96 & 98.84 & 11.46 & 97.87 & 12.72 & 97.72 & 43.58 & 92.47 & 43.31 & 91.03 & 26.45 & 94.67 \\
VRA-P& 18.75&96.68&1.32&99.63&5.80&98.69&5.70&98.69&34.89&93.42&\textcolor{orange}{\textbf{39.98}}&\textcolor{cyan}{\textbf{91.69}}&\textcolor{orange}{\textbf{17.74}}&96.47\\
DICE & 25.99&95.90&\textcolor{cyan}{\textbf{0.26}}&99.92&3.91&98.30&\textcolor{orange}{\textbf{4.36}}&97.55&41.90&93.36&48.59&89.13&20.83&95.24\\
ASH-P  & 30.14 & 95.29 & 2.82 & 99.34 & 7.97 & 98.33 & 8.46 & 98.29 & 50.85 & 88.29 & 40.46 & 91.76 & 23.45 & 95.22\\
ASH-B & 17.92 & 96.86 & 2.52 & 99.48 & 8.13 & 98.54 & 8.59 & 98.45 & 35.73 & 92.88 & 48.47 & 89.93 & 20.23 & 96.02 \\
ASH-S & \textcolor{orange}{\textbf{6.51}} & 98.65 & \textcolor{orange}{\textbf{0.90}} & 99.73 & 4.96 & 98.92 & 5.17 & \textcolor{cyan}{\textbf{98.90}} & \textcolor{orange}{\textbf{24.34}} & \textcolor{cyan}{95.09} & 48.45 & 88.34 & \textcolor{ForestGreen}{\textbf{15.05}} & 96.61 \\
GradOrth &\textcolor{cyan}{\textbf{5.84}}&\textcolor{cyan}{\textbf{98.72}}&\textcolor{ForestGreen}{\textbf{0.81}}&\textcolor{cyan}{\textbf{99.78}}&\textcolor{cyan}{\textbf{2.33}}&98.71&\textcolor{ForestGreen}{\textbf{4.25}}&98.32&\textcolor{cyan}{\textbf{20.63}}&94.77&\textcolor{cyan}{\textbf{38.22}}&91.64 &\textcolor{cyan}{\textbf{12.34} }& \textcolor{cyan}{\textbf{96.99}}\\
 \bottomrule
\end{tabular}
}
\caption{ Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-10} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}.  GradOrth outperforms other baselines on FPR95 and AUROC in average. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-10}. We present the \textcolor{cyan}{\textbf{first}}, \textcolor{ForestGreen}{\textbf{second}}, and \textcolor{orange}{\textbf{third}} ranks in blue, green, and orange colors, respectively. $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar10}
\vspace{-7mm}
\end{table}
In this research study, we further investigate the performance of GradOrth by conducting additional experimental studies on the CIFAR10 and CIFAR100 datasets. The key observation is that no single method consistently outperforms all other methods across diverse datasets. However, it is noticeable that GradOrth rank is always among the top three across six OOD datasets. This feature presents its promising performance for OOD detection. On the CIFAR10 dataset, GradOrth demonstrates superior performance compared to other baseline methods across six OOD datasets, namely SVHN, LSUN-c, LSUN-r, iSUN, Textures, and Places365. On average, GradOrth outperforms these baselines by $2.71\%$ and $0.32\%$ in terms of FPR95 and AUROC, respectively. Detailed experimental results can be found in Table-\ref{tab:cifar10}. In the LSUN-c OOD dataset, DICE demonstrates superior performance with an impressive $0.26\%$ FPR95, placing it at the top. Our method, on the other hand, ranks second with a respectable $0.81\%$ FPR95. However, the ranking differs when examining the Textures and Places365 datasets. Notably, Gradorth outperforms other baseline methods in both cases, achieving noteworthy FPR95 values of $20.63\%$ and $38.22\%$, respectively. In contrast, DICE attains the sixth and ninth positions in these datasets, displaying comparatively higher FPR95 rates of $41.90\%$ and $48.59\%$.

On the CIFAR100 dataset, GradOrth surpasses its competitors in both FPR95 and AUROC by an average margin of $8.0\%$ and $2.80\%$, respectively, across six well-known OOD datasets. Detailed experimental results are provided in Table-\ref{tab:cifar100}. For a comprehensive discussion and analysis of the CIFAR benchmark, please refer to the appendix, specifically Section \ref{app:cifar-analysis}.

\begin{table}
\scalebox{0.58}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule
Softmax score & 81.70 & 75.40 & 60.49 & 85.60 & 85.24 & 69.18 & 85.99 & 70.17 & 84.79 & 71.48 & 82.55 & 74.31 & 80.13 & 74.36 \\
ODIN & 41.35 & 92.65 & 10.54 & 97.93 & 65.22 & 84.22 & 67.05 & 83.84 & 82.34 & 71.48 & 82.32 & 76.84 & 58.14 & 84.49 \\
Mahalanobis & \textcolor{cyan}{22.44} & 95.67 & 68.90 & 86.30 & \textcolor{cyan}{\textbf{23.07}} & \textcolor{cyan}{\textbf{94.20}} & \textcolor{cyan}{\textbf{31.38}} & \textcolor{cyan}{\textbf{93.21}} & 62.39 & 79.39 & 92.66 & 61.39 & 55.37 & 82.73 \\
Energy score & 87.46 & 81.85 & 14.72 & 97.43 & 70.65 & 80.14 & 74.54 & 78.95 & 84.15 & 71.03 & 79.20 & 77.72 & 68.45 & 81.19 \\ 
GradNorm & 31.57 & 93.66& 9.89 & 96.75& 58.22&87.76&59.60& 84.21& 59.42&88.09&\textcolor{orange}{\textbf{57.14}}&82.10&45.98&88.76\\
ExGrad & 29.17 & 92.47& 8.91& 96.80& 60.12&88.21&56.43&84.73&57.29&88.79&\textcolor{ForestGreen}{\textbf{53.47}}&84.38&\textcolor{orange}{\textbf{44.23}} &89.23 \\
ReAct & 83.81 & 81.41 & 25.55 & 94.92 & 60.08 & 87.88 & 65.27 & 86.55 & 77.78 & 78.95 & 82.65 & 74.04 & 62.27 & 84.47 \\
VRA-P& 66.38&89.02&10.34&98.12&54.39&89.49&55.16&89.48&\textcolor{orange}{\textbf{48.12}}&88.48&78.31&77.84&53.24&88.74\\
DICE & 54.65& 88.84 & \textcolor{cyan}{\textbf{0.93}}& \textcolor{cyan}{\textbf{99.74}} & 49.40 & 91.04 & 48.72 & 90.08& 65.04 & 76.42 & 79.58 & 77.26 & 49.72 & 87.23\\
ASH-P & 81.86 & 83.86 & 11.60 & 97.89 & 67.56 & 81.67 & 70.90 & 80.81 & 78.24 & 74.09 & 77.03 & 77.94 & 64.53 & 82.71\\
ASH-B & 53.52 & 90.27 & \textcolor{orange}{\textbf{4.46}} & 99.17 & \textcolor{orange}{\textbf{48.38}} & 91.03 & 47.82 & 91.09 & 53.71 & 84.25 & 84.52 & 72.46 & 48.73 & 88.04 \\
ASH-S  & \textcolor{orange}{\textbf25.02} & \textcolor{cyan}{\textbf\textbf{95.76}} & 5.52 & 98.94 & 51.33 & 90.12 & \textcolor{orange}{\textbf{46.67}} & 91.30 & \textcolor{ForestGreen}{\textbf{34.02}} & 92.35 & 85.86 & 71.62 & \textcolor{ForestGreen}{\textbf{41.40}} & 90.02 \\
GradOrth & \textcolor{ForestGreen}{\textbf{24.27}} & 93.47 & \textcolor{ForestGreen}{\textbf{3.71}} &99.07 & \textcolor{ForestGreen}{\textbf{48.09}}& 91.26&\textcolor{ForestGreen}{\textbf{42.73}}&91.48&\textcolor{cyan}{\textbf{32.71}}&\textcolor{cyan}{\textbf{92.62}}&\textcolor{cyan}{\textbf{48.61}}&\textcolor{cyan}{\textbf{89.03}}&\textcolor{cyan}{\textbf{33.35}}&\textcolor{cyan}{\textbf{92.82}}\\
\bottomrule
\end{tabular}}
\caption{ Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-100} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}. GradOrth outperforms other baselines on FPR95 and AUROC in average. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-100}. We present the \textcolor{cyan}{\textbf{first}}, \textcolor{ForestGreen}{\textbf{second}}, and \textcolor{orange}{\textbf{third}} ranks in blue, green, and orange colors, respectively. $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better. }
\label{tab:cifar100}
\vspace{-6mm}
\end{table}
\paragraph{Choice of $L_p$-norm in GradOrth: }In order to investigate the impact of the choice of $L_p$-norm in Equation \ref{grad_orthScore} on OOD detection performance, we conducted an ablation study. Figure \ref{fig:norm} illustrates the comparison using $L_{1 \sim 4}$-norm, $L_{\infty}$-norm, and the fraction norm (with $p=0.3$).

Among the different norms considered, we observed that the $L_2$-norm consistently achieved the best OOD detection performance across all four datasets. This finding suggests that the $L_2$-norm, which equally captures information from all dimensions in the gradient space, is better suited for this task. In contrast, higher-order norms tend to unfairly emphasize larger elements over smaller ones due to the effect of the exponent $p$. Notably, the $L_{\infty}$-norm, which only considers the largest element (in absolute value), resulted in the worst OOD detection performance among all the norms tested. Additionally, we evaluated the fraction norm but found that it did not outperform the $L_2$-norm in terms of overall OOD detection performance.
\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/fpr-norms2.png}
         \label{fig:fpr95}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/auroc-norms2.png}
         \label{fig:auroc}
     \end{subfigure}
     \vspace{-5mm}
    \caption{ Comparison of OOD detection performance using various $L_p$-norms. $L_2$-norm is the best $L$-norm for GradOrth as it provides the lowest FPR95 and largest AUROC for GradOrth. Results are presented for False Positive Rate at 95\% True Positive Rate (FPR95) on the left, and Area Under the Receiver Operating Characteristic curve (AUROC) on the right. $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
        \label{fig:norm}
        \vspace{-4mm}
\end{figure}
\section{Related Works}
To the best of our knowledge, there exists only limited prior research on the use of gradients for detecting out-of-distribution (OOD) inputs. This section aims to explore the connections and differences between our proposed GradOrth method and prior OOD detection approaches that also utilize gradient information. In particular, we will discuss the connection between our approach and the ODIN method, the approach proposed by \cite{lee2020gradients}, GradNorm method \cite{huang2021importance}, and ExGrad method \cite{igoe2022useful}. 
Moreover, \cite{lee2020gradients} utilized gradients from all layers to train a distinct binary classifier, which can lead to a computationally burdensome process for deeper and larger models. Nevertheless, our findings with GradOrth demonstrate that the gradient from the last layer consistently achieves the highest performance compared to other gradient selections. Hence, the computational cost incurred by GradOrth is negligible.
\paragraph{OOD Gradient-Based Methods}
ODIN \cite{liang2018enhancing} introduced the concept of utilizing gradient information for out-of-distribution (OOD) detection. Their approach involved a pre-processing technique that added small perturbations obtained from the input gradients. The objective was to enhance the model's confidence in its predictions by increasing the softmax score for each input. This resulted in a larger gap between the softmax scores of in-distribution (ID) and OOD inputs, making them more distinguishable and improving OOD detection performance. It is important to note that ODIN indirectly employed gradients through input perturbation, and the OOD scores were still calculated based on the output space of the perturbed inputs.
GradNorm \cite{huang2021importance} also utilizes gradient information from a neural network to detect distributional shifts between in-distribution (ID) and out-of-distribution (OOD) samples. By measuring the norm of gradients with respect to the network's input, it quantifies uncertainty and identifies OOD samples causing significant output changes. 
ExGrad, proposed by \cite{igoe2022useful}, introduces a method akin to GradNorm with two notable distinctions. Firstly, the label distribution of $y$ is derived from the model's predicted distribution ($P$) as opposed to the uniform distribution. Secondly, ExGrad computes the expected norm of the gradient, in contrast to GradNorm which calculates the norm of the expected gradient.

\paragraph{Discriminative Models for OOD Uncertainty Estimation} 
The problem of classification with rejection has a long history, dating back to early works on abstention such as \cite{chow1970optimum} and \cite{fumera2002support}, which considered simple model families like SVMs \cite{cortes1995support}. However, the phenomenon of neural networks' overconfidence in out-of-distribution (OOD) data was not revealed until the work of \cite{nguyen2015deep}.

Early efforts aimed to improve OOD uncertainty estimation by proposing the ODIN score \cite{ liang2018enhancing} and Mahalanobis distance-based confidence score \cite{lee2018simple}. More recently, \cite{liu2020energy} proposed using an energy score derived from a discriminative classifier for OOD uncertainty estimation, showing advantages over the softmax confidence score both empirically and theoretically. \cite{wang2021canmulti} demonstrated that an energy-based approach can improve OOD uncertainty estimation for multi-label classification networks.
Additionally, \cite{huang2021mos} revealed that approaches developed for common CIFAR benchmarks might not effectively translate into a large-scale ImageNet benchmark, highlighting the need to evaluate OOD uncertainty estimation in a large-scale real-world setting. These developments have brought renewed attention to the problem of classification with rejection and the need for effective OOD uncertainty estimation.


\vspace{-0.2cm}
\paragraph{Generative Models for OOD Uncertainty Estimation}~
Detection of out-of-distribution (OOD) inputs is a crucial problem in machine learning. One popular approach is to use generative models that estimate the density directly. Such models can identify OOD inputs as those lying in low-likelihood regions. To this end, a plethora of literature has emerged to leverage generative models for OOD detection. However, recent studies have shown that deep generative models can assign high likelihoods to OOD data, rendering such models less effective in OOD detection. Additionally, these models can be challenging to train and optimize, and their performance may lag behind their discriminative counterparts. In contrast, our approach relies on a discriminative classifier, which is easier to optimize and achieves stronger performance. While some recent works have attempted to improve OOD detection with generative models using improved metrics, likelihood ratios, and likelihood regret, our approach leverages the energy score from a discriminative classifier and has demonstrated significant advantages over generative models in OOD detection.
\vspace{-0.2cm}
\paragraph{Distributional Shifts} 
The problem of distributional shift has garnered significant attention in the research community. It is essential to recognize and distinguish between different types of distributional shift problems. In the literature on out-of-distribution (OOD) detection, the focus is typically on ensuring model reliability and detecting label-space shifts~\cite{hendrycks17baseline, liang2018enhancing, liu2020energy}, where OOD inputs have labels that are disjoint from the ID data, and as such, should not be predicted by the model. On the other hand, some studies have examined covariate shifts in the input space~\cite{hendrycks2019benchmarking, malinin2021shifts, ovadia2019can}, where inputs may be subject to corruption or domain shifts. However, covariate shifts are commonly used to evaluate model robustness and domain generalization performance, where the label space $\mathcal{Y}$ remains the same during test time. It is worth noting that our work focuses on the detection of shifts where the model should not make any predictions, as opposed to covariate shifts where the model is expected to generalize.
\section{Conclusion}\vspace{-3mm}
In this paper, we propose GradOrth, a novel OOD uncertainty estimation approach utilizing information extracted from the \textit{important parameter space for ID data} and \textit{gradient space}. 
Extensive experimental results show that our gradient-based method can improve the performance of OOD detection by up to  $8.05\%$ in FPR95 on average, establishing superior performance. 
We hope that our research brings to light the informativeness of gradient subspace, and inspires future work to utilize it for OOD uncertainty estimation.
\newpage
\bibliography{egbib}
\bibliographystyle{plainnat}
\newpage
\section*{Broader Impact}
  The objective of this research project is to enhance the dependability and safety of contemporary machine learning models. The outcomes of our investigation possess the potential for significant advantages and societal effects, particularly in the realm of safety-critical domains like autonomous driving. It is important to note that our research does not involve any human subjects or breach legal compliance. We do not foresee any possible adverse ramifications resulting from our work. By conducting this study, our intention is to foster increased research efforts and raise awareness within both the research community and society at large regarding the issue of out-of-distribution detection in practical, real-world scenarios.

\section*{Limitation}
OOD detection methods may not always detect out-of-distribution samples accurately.
As it is presented in our experiments, most OOD detection methods are not able to recognize OOD data across different OOD datasets. They may provide a superior performance on some OOD data while catasrophic performance on some other variations of OOD data. Though our method, Gradorth, presents comparable performance to state-of-the-art methods, it still requires more investigation to achieve full stable performance across all OOD datasets. 

\appendix
\onecolumn
\section*{Appendix}

The Appendix section is organized as follow:
\begin{itemize}
    \item Section~\ref{app:conv} examines the relationship between stochastic gradient descent (SGD) updates and the span of input data points in convolutional layers.
    \item Section~\ref{app:net-layers} presents an experimental study that evaluates the role of leveraging the last-layer gradient of the network in our method (GradOrth).
    \item Section~\ref{app:hyper} provides additional information about the benchmarks used in our experiments.
    \item Section~\ref{app:cifar-analysis} discusses the results of the experimental analysis on the computation of the ID subspace leveraging different number of ID samples per class.
    \item  Section~\ref{app:span} includes an explanation of the cross-entropy loss and its derivative.
    \item Section~\ref{app:minibatch} presents a proof showing that stochastic gradient descent (SGD) updates lie in the span of input data points in a mini-batch setting.
    \item Section~ \ref{app:svdExplanation} offers a brief explanation of singular value decomposition (SVD).
    \item Section~ \ref{app:ablation} presents an ablation study that evaluates the impact of SVD on the GradOrth out-of-distribution (OOD) detection performance.
    \item Section~\ref{app:exp_detail} provides further information about our experiments, including details about the datasets used and the scoring functions of the baselines.
\end{itemize}
\section{Input and Gradient Spaces in Convolutional Layers}
\label{app:conv}

Our algorithm exploits the observation that updates in stochastic gradient descent (SGD) reside within the subspace spanned by the input data points \citep{zhang2021understanding}, as discussed in section \ref{sec:fc-layer} and this appendix, section \ref{app:minibatch}. In this section, we aim to establish this relationship specifically for convolutional layers. The analysis presented herein possesses general applicability to any layer within a network, regardless of the task.

In contrast to the weights in a fully connected (FC) layer, filters within a convolutional (Conv) layer operate differently on the input. Let us consider a convolutional layer comprising the input tensor $\mathcal{X} \in \R^{C_i \times h_i \times w_i}$ and filters $\mtheta \in \R^{C_o \times C_i \times k \times k}$. Their convolution, denoted as $\langle \mathcal{X},\mtheta,*\rangle$, yields the output feature map $\mathcal{O} \in \R^{C_o \times h_o \times w_o}$ \citep{conv_op}. Here, $C_i$ ($C_o$) represents the number of input (output) channels in the Conv layer, while $h_i$, $w_i$ ($h_o$, $w_o$) correspond to the height and width of the input (output) feature maps, and $k$ denotes the kernel size of the filters. Figure\ref{fig:conv-layer}(a) provides a visual representation of this process.

If we reshape $\mathcal{X}$ into a $(h_o \times w_o) \times (C_i\times k \times k)$ matrix denoted as $\vx$, and reshape $\mtheta$ into a $(C_i\times k \times k) \times C_o$ matrix denoted as $\mtheta$, the convolution can be expressed as a matrix multiplication between $\vx$ and $\mtheta$, yielding $\mathbf{O}=\vx\mtheta$, where $\mathbf{O} \in \R^{(h_0\times w_0)\times C_o}$. 

Formulating the convolution in terms of matrix multiplication provides an intuitive depiction of gradient computation during the back-propagation process. Similar to the fully connected layer scenario, in the convolutional layer, during the backward pass, an error matrix $\bm{\Omega}$ of size $(h_0\times w_0)\times C_o$ (equivalent to the size of $\mathbf{O}$) is obtained from the subsequent layer. As illustrated in Figure~\ref{fig:conv-layer}(b), the gradient of the loss with respect to the filter weights is computed as follows:
\begin{equation}
\nabla_{\mtheta} L = \vx^T \bm{\Omega},
\end{equation}
where $\nabla_{\mtheta}L$ possesses a shape of $(C_i\times k \times k) \times C_o$ (matching the size of $\mtheta$). Considering that the columns of $\vx^T$ correspond to the input.
\begin{figure}[H]%
    \centering
    \subfloat[\centering Forward pass]{{\includegraphics[width=5cm]{images/forward.png} }}
    \qquad
    \subfloat[\centering Backward pass]{{\includegraphics[width=5cm]{images/backward.png} }}
    \caption{The convolution operation in matrix multiplication format during the forward Pass (a) and backward pass (b).}
    \label{fig:conv-layer}
\end{figure}
\section{GradOrth Considering All The Network Layers}
\label{app:net-layers}
\paragraph{This experimental study presents that the gradients obtained from the final layer contain substantial and informative content.} 
In this experimental study, we aim to investigate the content and significance of gradients obtained from the final layer in a neural network. We explore an alternative variation of the GradOrth method, focusing on the extraction of gradients from all layers of the network. The objective is to analyze the gradients of all trainable parameters across the layers and assess their informativeness. In this paper, we present this version of GradOrth as "GradOrth-All layers" and compare it with the original GradOrth, which utilizes the gradient space from the last layer.
To assess the performance of the various gradient spaces, we perform a gradient projection for each layer of the neural network. Subsequently, we calculate the average of these gradient projections across all layers. This procedure allows us to derive an out-of-distribution (OOD) detection score, which we refer to as the OODness score. The experiment is conducted over five random subsets of the ID (in-distribution) subspaces. Each subspace is computed by utilizing ten random samples per class in the ImageNet benchmark and five random samples per class in CIFAR benchmarks. We report the average results obtained from these five runs. The outcomes of both GradOrth-All layers and GradOrth-Last layer are presented in Tables \ref{tab:block_trend_ablation}, \ref{tab:cifar10-layers}, and \ref{tab:cifar100-layers}.

Table \ref{tab:block_trend_ablation} provides a comparison of the OOD detection performance using the gradient spaces from different layers (all network layers and the last network layer) on two pre-trained network architectures, ResNet and MobileNet, on the ImageNet dataset as ID data. The evaluation metrics used are the False Positive Rate at $95\%$ True Positive Rate (FPR95) and the Area Under the Receiver Operating Characteristic curve (AUROC). These metrics are averaged across four OOD datasets.

Our findings demonstrate that gradients from the last layer consistently outperform gradients from all layers. On ResNet and MobileNet, leveraging the last-layer gradient space results in a $1.57\%$ and $1.52\%$ improvement in FPR95 (on average), respectively, compared to GradOrth utilizing gradients from all network layers.

\begin{table}[h!]
\resizebox{\textwidth}{!}{\begin{tabular}{|c| c| c| c| c| c| c| c| c| c| c| c|}
\hline
 & & \multicolumn{8}{c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{c|}{}\\\cline{3-10}
\textbf{Model} & \textbf{Methods} & \multicolumn{2}{c}{\textbf{iNaturalist}} & \multicolumn{2}{|c|}{\textbf{SUN}} & \multicolumn{2}{|c|}{\textbf{Places}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Average}} \\
 
& & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
& & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ \\
\midrule

\multirow{2}{*}{ResNet} & \multicolumn{1}{l|}{GradOrth-All layers} & 13.23 & 96.52 & 21.05 & 95.21 & 35.58 & 92.49 & 11.56 & 96.72&20.35  & 95.23 \\
& \multicolumn{1}{l|}{GradOrth-Last layer} & 11.04 & 98.00 & 19.61 & 95.76 & 33.67 & 91.78 & 11.19 & 98.06 & 18.57 & 96.31 \\
\midrule
\multirow{2}{*}{MobileNet} & \multicolumn{1}{l|}{GradOrth-All layers}& 26.14 & 93.83 & 33.28 & 91.38 & 43.71 & 85.37&13.61& 96.87&29.17&91.86\\
& \multicolumn{1}{l|}{GradOrth-Last layer} & 26.81 & 93.17 & 30.82 & 93.18 & 40.27 & 89.12 & 12.69 & 97.52 & 27.65 & 93.25 \\
\bottomrule
\end{tabular}}
\caption{ OOD detection results with \textbf{ImageNet-1k} as ID. Effect of leveraging all-layers and last-layer gradients space in GradOrth. The OODness score derived
from last layer yields better OOD detection performance mostly. The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-1k dataset. We use $\uparrow$ to denote that larger values are preferable, and $\downarrow$ to denote that smaller values are preferable. All values are presented as percentages.}
\label{tab:block_trend_ablation}
\end{table}

In the pursuit of an extensive investigation, the CIFAR benchmark is taken into account to examine the impact of various network gradient spaces, namely the last network layer and all network layers, on the performance of GradOrth. The findings, depicted in tables \ref{tab:cifar10-layers} and \ref{tab:cifar100-layers}, reveal that utilizing last layer gradients in GradOrth yields superior results compared to employing gradients from all network layers, with improvements of $0.82\%$ and $1.49\%$ in FPR95 on average, respectively.

This observed outcome is highly advantageous, as gradients calculated with respect to deeper layers demonstrate computational efficiency when compared to utilizing gradients from all layers. Remarkably, the GradOrth variant derived from the final linear layer exhibits the most favorable outcomes. From a practical perspective, it is only necessary to perform back-propagation with respect to the last linear layer, resulting in minimal computational overhead. Therefore, our primary findings are predicated on the utilization of the last fully connected (FC) layer within the neural network.

\begin{table}[H]
\scalebox{0.57}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule
GradOrth-All layers & 7.32 & 98.24 & 1.04 & 99.68 & 4.11 & 98.47 & 4.10 & 98.36 & 24.52 & 93.06 & 37.91 & 92.11 & 13.16 &96.65 \\
GradOrth-Last layer &5.84&98.72&0.81&99.78&2.33&98.71&4.25&98.32&20.63&94.77&38.22&91.64 &12.34& 96.99\\
 \bottomrule
\end{tabular}
}
\caption{ GradOrth leveraging last-layer network's gradient space outperforms Gradorth leveraging all network layers gradient space on CIFAR10 dataset. Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-10} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}.  . For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-10}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar10-layers}
\vspace{-7mm}
\end{table}

\begin{table}[H]
\scalebox{0.57}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule
GradOrth-All layers &  27.61 & 92.84& 4.32 &97.63& 49.27&89.48& 44.07& 90.17& 34.86&91.25&48.92& 88.83&34.84&91.70\\
GradOrth-Last layer & 24.27 & 93.37&3.71&99.07&48.09&91.26&42.73&91.48&32.71&92.62&48.61&89.03&33.35&92.82\\
 \bottomrule
\end{tabular}
}
\caption{ GradOrth leveraging last-layer network's gradient space outperforms Gradorth leveraging all network layers' gradient space on the CIFAR100 dataset. Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-100} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-100}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar100-layers}
\vspace{-7mm}
\end{table}


\section{Model and Hyper Parameter}
\label{app:hyper}
In our empirical studies and experiments, we adopt an experimental setting that aligns with the state-of-the-art (SOTA) approaches, specifically ASH \cite{djurisic2022extremely} and DICE \cite{sun2022dice} on the CIFAR dataset, as well as ReAct \cite{react} on the ImageNet dataset. The datasets and model architectures utilized in our experiments are summarized in Table \ref{benchmarks_table}.

For the CIFAR-10 and CIFAR-100 experiments, we employ the six Out-of-Distribution (OOD) datasets employed in the DICE study \cite{sun2022dice}: SVHN \cite{netzer2011reading}, LSUN-Crop \cite{yu2015lsun}, LSUN-Resize \cite{yu2015lsun}, iSUN \cite{xu2015turkergaze}, Places365 \cite{zhou2017places}, and Textures \cite{cimpoi2014describing}. The In-Distribution (ID) dataset used in these experiments corresponds to the respective CIFAR dataset. The model architecture employed is a pre-trained DenseNet-101 \cite{huang2017densely}.

For our ImageNet experiments, we adhere to the precise setup as outlined in the ReAct study \cite{react} and \cite{djurisic2022extremely}. The ID dataset employed in this context is ImageNet-1k, while the OOD datasets consist of iNaturalist \cite{van2018inaturalist}, SUN \cite{xiao2010sun}, Places365 \cite{zhou2017places}, and Textures \cite{cimpoi2014describing}. The network architectures utilized in these experiments are ResNet50 \cite{he2016deep} and MobileNetV2 \cite{sandler2018mobilenetv2}. All networks undergo pre-training using the ID data and remain unaltered post-training, with their parameters remaining unchanged during the OOD detection phase. The performance of the baselines primarily relies on ASH and VRA. When conducting experiments involving gradient-based methods such as GradNorm \cite{huang2021importance} and ExGrad \cite{igoe2022useful}, we re-run the experiments using the code provided by the respective authors, as there may be variations between our pre-trained network over ID data and the models employed by the authors themselves.

\begin{table}[hbt!]
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{|c | c | c|}
\toprule
\textbf{ID Dataset} & \textbf{OOD Datasets} & \textbf{Model architectures} \\
\midrule
CIFAR-10 & SVHN, LSUN C, LSUN R, iSUN, Places365, Textures & DenseNet-101\\
\midrule
 CIFAR-100 & SVHN, LSUN C, LSUN R, iSUN, Places365, Textures & DenseNet-101\\
 \midrule
ImageNet & iNaturalist, SUN, Places365, Textures & ResNet50, MobileNetV2\\
\bottomrule
\end{tabular}
}

\caption{The datasets and models we used in our OOD experiments range from moderate to large scale, including evaluations of up to 10 OOD datasets and three architectures.}
\label{benchmarks_table}
\end{table}
\section{Analysis of the Number of ID Samples}
\label{app:cifar-analysis}
The initial step in our proposed method involves computing the subspace of the pre-trained neural network using the in-distribution (ID) data. To accomplish this, we utilize a small number of data samples and pass them through the forward pass of the pre-trained network, without altering the learned parameters. Subsequently, we compute the subspace based on the last layer of the network.

To ensure a comprehensive study, we conduct an empirical investigation and compute variations of subspaces by considering different numbers of samples per class. Specifically, we vary the number of samples from 5 to 40 in the CIFAR benchmark. For each variation, we compute five random subspaces and report the average results obtained from these five subspaces. The experimental results obtained from our study are presented in Tables \ref{tab:cifar10-subspace} and \ref{tab:cifar100-subspace}. Specifically, we introduce a notation to describe the experiments using GradOrth on the pre-trained network subspace computed based on a specific number of samples per class. We denote this notation as $\text{GradOrth-S}_{n}$, where $n$ represents the number of samples per class used to compute the subspace.
\begin{table}[h!]
\scalebox{0.57}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Model} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule

 $\text{GradOrth-S}_{5}$  &5.84&98.72&0.81&99.78&2.33&98.71&4.25&98.32&20.63&94.77&38.22&91.64 &12.34& 96.99\\
 $\text{GradOrth-S}_{10}$& 5.80 & 98.74 & 0.75 & 99.79 & 2.34 & 98.69 & 4.11 & 98.41 & 20.37 & 94.82 & 38.13 & 92.01 & 11.86 & 97.07\\
 $\text{GradOrth-S}_{20}$ & 5.61 & 98.80 & 0.69 & 99.81 & 2.27& 98.74 & 4.17 & 98.35 & 20.59 &94.79 & 38.09 & 91.71 & 11.90 & 97.03 \\
 $\text{GradOrth-S}_{40}$ & 5.62 & 98.79 & 0.69 & 99.83 & 2.24 & 98.75 & 4.16 & 98.38 & 20.55 & 94.84 & 38.11 & 91.74 & 11.90 & 97.05\\ 
 \bottomrule
\end{tabular}
}
\caption{ Detailed results on six common OOD benchmark datasets considering different numbers of ID samples (per class) in subspace computation. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-10}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar10-subspace}
\vspace{-7mm}
\end{table}


\begin{table}[h!]
\scalebox{0.57}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Model} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule

 $\text{GradOrth-S}_{5}$ & 24.27 & 93.37&3.71&99.07&48.09&91.26&42.73&91.48&32.71&92.62&48.61&89.03&33.35&92.82\\
 $\text{GradOrth-S}_{10}$ & 24.25 & 93.38 & 3.73 & 99.09 & 48.08 & 91.29 & 42.70 &91.52&32.65&92.66 & 48.63 & 89.04 & 33.34 & 92.83\\
 $\text{GradOrth-S}_{20}$ & 24.21 & 93.40 & 3.70 & 99.10 & 48.04 & 91.33& 42.69&91.50&32.64 & 92.65 & 48.54 & 89.05&33.30 & 92.83\\
 $\text{GradOrth-S}_{40}$ & 24.09&93.46& 3.67 & 99.11& 47.86 & 91.38 &42.53 & 91.73& 32.49&92.72&46.91&90.12& 32.92 &93.09 \\ 
 \bottomrule
\end{tabular}
}
\caption{ Detailed results on six common OOD benchmark datasets considering different numbers of ID samples (per class) in subspace computation. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-100}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar100-subspace}
\vspace{-7mm}
\end{table}

The experimental results displayed in Tables \ref{tab:cifar10-subspace} and \ref{tab:cifar100-subspace} indicate that increasing the number of samples per class during the computation of the subspace for the pre-trained network does not have a significant impact on the overall performance of Out-of-Distribution (OOD) detection. This observation suggests that leveraging a pre-trained network, which has already learned the data well, diminishes the influence of the number of samples per class in the subspace computation.


\section{Cross-Entropy Loss and its Derivative}\label{app:minibatch}

In this section, we provide a detailed explanation of the cross-entropy loss and its derivative.

Consider a single-layer linear neural network in a supervised learning setting, where each training data pair $(\vx, \vy)$ is drawn from a training dataset $\sD$. Here, $\vx \in \R^n$ represents the input vector, $\vy \in \R^m$ represents the label vector in the dataset and $\mtheta \in \R^{m\times n}$ represents the learning parameters (weights) of the network. The model's prediction on input $\vx$ is denoted by $f(\vx; \mtheta)$. For classification problems, $f(\vx; \mtheta)=\mtheta \vx$, where $f_k(\vx;\mtheta)$ represents the $k$-th logit associated with the $k$-th class.

The total loss on the training set (empirical risk) is denoted by:

\begin{equation}
L_{\sD}(\mtheta)=\sum_{(\vx,\vy)\in \sD} L_{(\vx,\vy)}(\mtheta),
\end{equation}

where the per-example loss is defined as:

\begin{equation}
L_{(\vx,\vy)}(\mtheta) = \ell(\vy, f(\vx; \mtheta)),
\end{equation}

and $\ell(\cdot,\cdot)$ represents a differentiable non-negative loss function.

For classification problems, the softmax cross-entropy loss is commonly used, given by:

\begin{equation}
\ell(\vy, f(\vx;\mtheta)) = - \sum_{k=1}^m y_k \log a_k,
\end{equation}

where $a_k = \exp (f_k(\vx;\mtheta))/\sum_k \exp (f_k(\vx;\mtheta))$ represents the softmax output for the $k$-th class.

Using the chain rule, the gradient of the loss can be expressed as:

\begin{equation}
\nabla L_{(\vx,\vy)}(\mtheta) = \nabla f(\vx;\mtheta) \ell'(\vy,f(\vx;\mtheta)),\label{eq:dwhole}
\end{equation}

where $\ell'(\cdot,\cdot) \in \mathbb{R}^m$ denotes the derivative of $\ell(\cdot,\cdot)$ with respect to its second argument, and $\nabla f(\vx;\mtheta)$ represents the gradient of the model $f$ with respect to its second argument (i.e., the parameters). For the classification problem with cross-entropy softmax loss, we have:

\begin{equation}
\nabla f(\vx;\mtheta) = [\nabla f_1(\vx;\mtheta); \ldots; \nabla f_m(\vx;\mtheta)],\label{eq:dmodel}
\end{equation}

where $\nabla f_k(\vx;\mtheta) \in \mathbb{R}^{m}$ represents the gradient of the $k$-th logit with respect to the parameters. For simplicity, let's write equation \ref{eq:dmodel} as:

\begin{equation}
\nabla f(\vx;\mtheta) = \vx^T.\label{eq:dmodel-simple}
\end{equation}

Considering that the derivative of the loss is given by:

\begin{equation}
\ell'(\vy, f(\vx;\mtheta)) = [a_1-y_1, \ldots, a_m-y_m]^{\top}=\rho, \label{eq:dloss}
\end{equation}

Here, $\bm{\rho}$ denotes the error vector. Considering equations \ref{eq:dwhole}, \ref{eq:dmodel-simple}, and \ref{eq:dloss}, we can write equation \ref{eq:dwhole} as:

\begin{equation}
\nabla L_{(\vx,\vy)}(\mtheta) =\rho \vx^T,\label{eq:dfinal}
\end{equation}

As a result, the gradient update will be confined within the input span ($\vx$), where the elements in $\bm{\rho}$ display heterogeneous magnitudes, thereby impacting the scaling of $\vx$ correspondingly.

\section{Gradient Span Proof}
\label{app:span}
Considering that the batch loss is the summation of losses incurred by individual examples, the overall batch loss for $n$ samples can be represented as:

\begin{equation}\label{eqa1p}
L_{\text{batch}} = \sum_{i=1}^{n} L_i,
\end{equation}

where $L_i$ represents the loss of sample $(x_i,y_i)$.

When employing the mean-squared error loss function, the loss of a batch is calculated as the sum of the losses of individual samples, which can be expressed as:

\begin{equation}\label{eqa1ms}
L_{\text{batch}} = \sum_{i=1}^{n} L_i = \sum_{i=1}^{n} \frac{1}{2} || \mtheta \vx_i - \vy_i ||_{2}^2.
\end{equation}

Following stochastic gradient optimization, we can present the gradient of this loss (per sample) with respect to weights as:

\begin{equation}\label{eq2-ms}
\nabla_{\mtheta} \mathcal{L} = (\mtheta \vx -\vy) \vx^T = \bm{\Omega} \vx^T,
\end{equation}

Here, $\bm{\Omega} \in \mathbb{R}^m$ denotes the error vector. Therefore, the gradient of the batch loss with respect to the weights can be expressed as:

\begin{equation}\label{eqa2ms}
\nabla_{\mtheta} L_{\text{batch}} = \bm{\Omega}_1 \vx_1^T + \bm{\Omega}_2 \vx_2^T + \ldots + \bm{\Omega}_n \vx_n^T.
\end{equation}

It is noteworthy that the gradient update remains confined within the subspace spanned by the $n$ input examples.

Considering cross-entropy (CE) loss as the desired loss, the batch loss would be the sum of the losses of individual samples as follows:

\begin{equation}\label{eqa1ce}
L_{\text{batch}} = \sum_{i=1}^{n} L_i = \sum_{i=1}^{n} \sum_{k=1}^m -y_k \log a_k,
\end{equation}

Considering equation \ref{eq:dfinal}, the gradient of this loss with respect to the weights can be presented as:

\begin{equation}\label{eqa2ce}
\nabla_{\mtheta} L_{\text{batch}} = \bm{\rho}_1 \vx_1^T + \bm{\rho}_2 \vx_2^T + \ldots + \bm{\rho}_n \vx_n^T.
\end{equation}

It is important to note that the gradient update is constrained within the subspace spanned by the $n$ input examples.
\section{Singular Value Decomposition (SVD) Explanation}
\label{app:svdExplanation}
Consider an $m \times n$ matrix $\mR$, where $m$ is the number of rows and $n$ is the number of columns. The goal of SVD is to factorize matrix $\mR$ into three separate matrices: $\mU, \mSigma, \text{and } \mV^T$ (transpose of matrix $\mV$), such that $\mR = \mU\mSigma\mV^T \in \mathbb{R}^{m\times n}$, as presented in figure \ref{fig:svd}.

$\mU$: An $m \times m$ orthogonal matrix, where the columns represent the left singular vectors of $\mR$.
$\mSigma$: An $m \times n$ diagonal matrix, where the diagonal entries are the singular values of $\mR$ (non-negative and sorted in descending order).
$\mV^T$: An $n\times n$ orthogonal matrix, where the columns represent the right singular vectors of $\mR$.

Along with singular values and singular vectors, eigen-values and eigen-vectors are also defined.
An eigenvalue $\lambda$ and its corresponding eigenvector v of a square matrix $\mR$ satisfy the equation $\mR = \lambda \mV$. Eigen-vectors represent directions in the vector space that are only scaled by the matrix $\mR$, while eigenvalues represent the scaling factors for those eigen-vectors.


SVD and Relationship to Eigen-values and Eigen-vectors:
SVD connects eigen-values and eigenvectors with the singular values and singular vectors of a matrix. The singular values of $\mR$ are the square roots of the eigen-values of $\mR \mR^T$ or $\mR^T\mR$, and the left and right singular vectors are the eigen-vectors of $\mR \mR^T$ and $\mR^T\mR$, respectively.

Rank and Matrix Approximation:
The rank of a matrix $\mR$ is determined by the number of non-zero singular values in $\mSigma$. By keeping only the largest singular values and their corresponding singular vectors, it is possible to approximate the original matrix $\mR$ with a lower-rank approximation, which can be useful for dimensionality reduction and noise reduction. We leverage this feature in our approach.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/svd.png}
    \caption{Singular Value Decomposition}
    \label{fig:svd}
\end{figure}
Properties of SVD:

The singular values in $\mSigma$ are non-negative and arranged in descending order.
The columns of $\mU$ and $\mV$ are orthonormal, meaning they form an orthogonal basis for their respective vector spaces.
The SVD decomposition is unique up to the sign of the singular values and the order of the singular vectors.
SVD is a powerful matrix factorization technique that provides a compact representation of a matrix while preserving important structural properties. It finds widespread applications in various fields, such as data analysis, image processing, recommendation systems, and more \cite{mml}.
\section{Analysis of the Uefulness of SVD}
\label{app:ablation}
In this experiment, our focus is to examine the impact of utilizing singular value decomposition (SVD) in the GradOrth method as an ablation study. To achieve this, we compute the space (as opposed to the subspace) of the pre-trained network using the in-distribution (ID) data, without employing SVD. Consequently, the OODness score in GradOrth-NoSVD solely incorporates the orthogonal projection of the new sample onto the \textit{space} of the ID pre-trained network. The experimental results obtained from both the ImageNet and CIFAR benchmarks are presented in Tables \ref{tab:imagenet-cosine}, \ref{tab:cifar10-svd}, and \ref{tab:cifar100-svd}.
The outcomes reported in Tables \ref{tab:imagenet-cosine}, \ref{tab:cifar10-svd}, and \ref{tab:cifar100-svd} demonstrate the significant and robust performance of \textit{GradOrth} compared to the GradOrth variant without SVD (\textit{GradOrth-NoSVD}). On the ImageNet benchmark, GradOrth surpasses GradOrth-NoSVD by an average of $14.12\%$ and $10.93\%$ in terms of FPR95 for the ResNet and MobileNet pre-trained networks, respectively. Furthermore, GradOrth exhibits exceptional performance on the CIFAR benchmark, outperforming GradOrth-NoSVD by $6.44\%$ and $8.06\%$ on CIFAR10 and CIFAR100, respectively. 
The results obtained from this ablation study emphasize the significance of employing singular value decomposition (SVD) in the GradOrth method's OODness score. It underscores the core principle of our approach, which suggests that the essential discriminative features for identifying out-of-distribution (OOD) data reside within the subspace of the in-distribution (ID) data.

\begin{table}[hbt!]
\resizebox{\textwidth}{!}{\begin{tabular}{|c| c| c| c| c| c| c| c| c| c| c| c|}
\hline
 & & \multicolumn{8}{c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{c|}{}\\\cline{3-10}
\textbf{Model} & \textbf{Methods} & \multicolumn{2}{c}{\textbf{iNaturalist}} & \multicolumn{2}{|c|}{\textbf{SUN}} & \multicolumn{2}{|c|}{\textbf{Places}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Average}} \\
 
& & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
& & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$ \\
\midrule

\multirow{2}{*}{ResNet} & 
 \multicolumn{1}{|c}{GradOrth-NoSVD} & 28.31 & 94.30 & 30.09 & 93.73 & 44.18  & 81.43 & 28.17 & 93.21 &  32.69 & 90.67 \\

& \multicolumn{1}{c|}{GradOrth} & 11.04 & 98.00 & 19.61 & 95.76 & 33.67 & 91.78 & 11.19 & 98.06 & 18.57 & 96.31 \\ \midrule
\multirow{2}{*}{MobileNet} & 
\multicolumn{1}{c|}{GradOrth-NoSVD} & 38.45 & 90.21 & 38.02 & 89.26 & 49.31  & 90.18 & 28.52 & 89.68 &  38.58 & 89.83 \\
& \multicolumn{1}{c|}{GradOrth } & 26.81 & 93.17 & 30.82 & 93.18 & 40.27 & 89.12 &12.69 & 97.52 & 27.65 &93.25 \\
\bottomrule
\end{tabular}}

\caption{ Ablation study to present the importance of SVD in GradOrth. GradOrth presents outstanding performance on average and across all datasets. OOD detection results with \textbf{ImageNet-1k} as ID. GradOrth presents outstanding performance on average and across all datasets. The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-1k dataset. We use $\uparrow$ to denote that larger values are preferable, and $\downarrow$ to denote that smaller values are preferable. 
}
\label{tab:imagenet-cosine}
\vspace{-4mm}
\end{table}

\begin{table}[H]
\scalebox{0.57}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule

GradOrth-NoSVD & 18.32 & 94.03 & 1.12 & 99.57 & 3.56 & 98.74 & 10.29 & 98.04 & 33.17 & 93.10 & 46.22 & 85.62 & 18.78  & 94.85 \\
GradOrth &5.84&98.72&0.81&99.78&2.33&98.71&4.25&98.32&20.63&94.77&38.22&91.64 &12.34& 96.99\\
 \bottomrule
\end{tabular}
}
\caption{ Ablation study to recognize the importance of SVD in GradOrth. Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-10} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-10}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar10-svd}
\vspace{-7mm}
\end{table}

\begin{table}[H]
\scalebox{0.57}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \toprule
 & \multicolumn{12}{|c|}{\textbf{OOD Datasets}} &  \multicolumn{2}{|c|}{}\\\cline{2-13}
\textbf{Method} & \multicolumn{2}{|c|}{\textbf{SVHN}} & \multicolumn{2}{|c|}{\textbf{LSUN-c}} & \multicolumn{2}{|c|}{\textbf{LSUN-r}} & \multicolumn{2}{|c|}{\textbf{iSUN}} & \multicolumn{2}{|c|}{\textbf{Textures}} & \multicolumn{2}{|c|}{\textbf{Places365}} & \multicolumn{2}{|c|}{\textbf{Average}} \\ 
 & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\ 
 &  $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ &   $\uparrow$ &   $\downarrow$ &  $\uparrow$ &  $\downarrow$ & $\uparrow$ &  $\downarrow$ &  $\uparrow$ \\ \midrule

GradOrth- NoSVD & 31.43 &93.70 & 9.53& 96.82 & 57.69 & 87.54 &51.93  & 92.11 &42.09  & 90.16 & 55.76 & 82.97 & 41.41  &  90.55\\

GradOrth &24.27& 93.47&3.71&99.07&48.09&91.26&42.73&91.48&32.71&92.62&48.61&89.03&33.35 & 92.82\\
 \bottomrule
\end{tabular}
}
\caption{ Ablation study to recognize the importance of SVD in GradOrth. GradOrth presents outstanding performance on average and across all datasets. Detailed results on six common OOD benchmark datasets with \textbf{CIFAR-100} as ID: Textures~\cite{cimpoi2014describing}, SVHN~\cite{netzer2011reading}, Places365~\cite{zhou2017places}, LSUN-Crop~\cite{yu2015lsun}, LSUN-Resize~\cite{yu2015lsun}, and iSUN~\cite{xu2015turkergaze}. For each ID dataset, we use the same DenseNet pre-trained on \textit{CIFAR-100}.  $\uparrow$ indicates larger values are better and $\downarrow$ indicates smaller values are better.}
\label{tab:cifar100-svd}
\vspace{-7mm}
\end{table}

\section{Details of Experiments}
\label{app:exp_detail}

\subsection{Datasets}
\label{app:dataset}
\paragraph{ImageNet Benchmark, Large-scale evaluation}  In this study, the ImageNet-1k dataset \cite{deng2009imagenet} is employed as the in-distribution (ID) dataset. The evaluation of the proposed approach is conducted on four out-of-distribution (OOD) test datasets, following the experimental setup outlined in \cite{huang2021importance}:
\begin{itemize}
    \item \textbf{iNaturalist}~ The dataset introduced by \cite{van2018inaturalist}, referred to as "iNaturalist", comprises a substantial collection of 859,000 images featuring various plant and animal species. These images span more than 5,000 distinct species. To facilitate efficient processing, each image within the dataset is resized to ensure that the maximum dimension does not exceed 800 pixels. For the evaluation phase, a subset of 10,000 images is randomly sampled from a set of 110 classes. Importantly, these selected classes are disjoint from the ImageNet-1k dataset, thereby ensuring the validity and independence of the evaluation process.

    \item \textbf{SUN}~ that is introduced by \citeauthor{xiao2010sun}, encompasses a vast collection of over 130,000 images representing various scenes. These scenes are categorized into 397 distinct categories. Notably, it is important to acknowledge that there are overlapping categories between the SUN dataset and the ImageNet-1k dataset. For the evaluation process, a subset of 10,000 images is randomly sampled from a set of 50 classes. Importantly, these selected classes are disjoint from the labels present in the ImageNet dataset. This ensures the integrity and independence of the evaluation conducted in the study.
    \vspace{-0.1cm}
    \item \textbf{Places}~ 
The dataset introduced by \cite{zhou2017places}, commonly referred to as "Places" in the research literature, is another notable scene dataset that exhibits similar concept coverage as the SUN dataset. In this study, a carefully selected subset of 10,000 images is utilized from a total of 50 classes. Importantly, these selected classes are intentionally excluded from the ImageNet-1k dataset, ensuring that the evaluation process remains independent and free from any potential overlap with the aforementioned dataset.
    \item \textbf{Textures}~\cite{cimpoi14describing} consisting of 5,640 real-world texture images categorized into 47 distinct categories, is utilized in this research study. For the purpose of evaluation, the entire dataset is utilized, ensuring comprehensive coverage across all available categories.
\end{itemize}

\paragraph{CIFAR Benchmark} The CIFAR-10 and CIFAR-100 datasets, introduced by \cite{krizhevsky2009learning}, are extensively employed as in-distribution (ID) datasets in the existing literature. CIFAR-10 comprises 10 classes, while CIFAR-100 consists of 100 classes. In line with standard practices, the dataset split utilized in this study consists of 50,000 training images and 10,000 test images. To evaluate the proposed approach, four commonly employed out-of-distribution (OOD) datasets are utilized. The specific OOD datasets used in this evaluation are listed below:
\begin{itemize}
    \item \textbf{SVHN}~\cite{netzer2011reading} The Street View House Numbers (SVHN) dataset consists of color images depicting house numbers. The dataset encompasses ten distinct classes corresponding to the digits 0-9. In this research study, the entire test set comprising 26,032 images is employed for evaluation purposes.
    \item \textbf{LSUN}~\cite{yu2015lsun} includes a collection of 10,000 testing images featuring 10 different scenes. In this research study, image patches of size 32x32 are randomly cropped from the LSUN dataset to facilitate analysis and experimentation.
    \vspace{-0.1cm}
    \item \textbf{Places365}~\cite{zhou2017places} comprises a vast collection of large-scale photographs depicting scenes across 365 distinct scene categories. Notably, the test set of this dataset consists of 900 images per category. For the evaluation phase, a random sample of 10,000 images is drawn from the test set, thereby ensuring a representative subset for rigorous analysis and assessment.
    \vspace{-0.1cm}
    \item \textbf{Textures}~\cite{cimpoi14describing} includes a comprehensive collection of 5,640 real-world texture images, classified into 47 distinct categories. In this research study, the entire dataset is utilized for evaluation, enabling a thorough examination of the performance and capabilities of the proposed approach across all available texture categories.
\end{itemize}

\subsection{Baselines}
\label{app:baseline}

For the reader's convenience, we summarize in detail a few common techniques for defining OOD scores that measure the degree of ID-ness on the given sample.
By convention, a higher (lower) score is indicative of being in-distribution (out-of-distribution).









\paragraph{MSP~\cite{hendrycks17baseline}} utilizes probabilities obtained from softmax distributions to distinguish between correctly classified and erroneous or out-of-distribution examples. The baseline method relies on the observation that correctly classified examples tend to have higher maximum softmax probabilities than misclassified and out-of-distribution examples. 


\paragraph{ODIN~\cite{liang2018enhancing}} is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. 

\paragraph{Mahalanobis~\cite{lee2018simple}} utilizes multivariate Gaussian distributions to effectively model the class-conditional distributions of softmax neural classifiers. Additionally, they employed Mahalanobis distance-based scores as a means of detecting out-of-distribution (OOD) samples. 

\paragraph{Energy Score~\cite{liu2020energy}} 
The concept of utilizing energy scores for estimating out-of-distribution (OOD) uncertainty was initially introduced by \citeauthor{liu2020energy}. The energy function employed in their approach maps the logit outputs to a scalar value denoted as $S_\mathrm{Energy}(\*x; f) \in \mathbb{R}$. Notably, this scalar value tends to be relatively lower for in-distribution (ID) data. It is important to mention that the authors adopted the convention of using the negative energy score for OOD detection, ensuring that $S(\*x;f)$ exhibits higher values for ID data and lower values for OOD data.


\paragraph{GradNorm \cite{huang2021importance}} computes the norm of the gradients of a neural network with respect to an input. The norm of the gradients is a measure of how sensitive the network is to the input. Inputs with high norms are more likely to be OOD because they are more likely to cause the network to make a mistake.

\paragraph{Exgrad\cite{igoe2022useful}} calculates the expected norm of the gradients of a neural network with respect to an input. The expected norm of the gradients is a measure of how sensitive the network is to the input. 

\paragraph{DICE \cite{sun2022dice}} aims at selectively utilizing a subset of significant weights to compute the output for out-of-distribution (OOD) detection. By employing the technique of sparsification, the network effectively avoids incorporating irrelevant information into the output, thereby enhancing its OOD detection capabilities.

\paragraph{ReAct \cite{sun2021react}} This approach is based on the observation that OOD data trigger distinctive activation patterns in neural networks. ReAct selectively rectifies and truncates the activations of specific hidden units, reducing overconfident predictions on OOD data. 

\paragraph{Variational Rectified Activation (VRA) \cite{xu2023vra}} VRA leverages the variational method to find the optimal operation for maximizing the gap between in-distribution and OOD data. It introduces suppression and amplification operations for abnormally low, high, and intermediate activations, unlike ReAct that only focuses on high activations. VRA uses piecewise functions to simulate these operations. 

\paragraph{ASH \cite{djurisic2022extremely}} This method removes a large portion of activations and adjusts the remaining ones. The remaining activations (e.g., 10\%) are either simplified or lightly adjusted. The simplified activation representation is then propagated through the rest of the network to generate scores for both classification and OOD detection. The energy score, calculated from the logits, is commonly used for OOD detection, although the softmax score can also be used. 


\subsection{Software and Hardware}
\paragraph{Software} We run all experiments with Python 3.8.0 and PyTorch 1.12.1.
\paragraph{Hardware} All experiments are run on NVIDIA RTX 3090.

\end{document}