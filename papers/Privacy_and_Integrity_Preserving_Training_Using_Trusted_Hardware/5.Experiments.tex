
\section{Experiments}

\begin{comment}
\begin{table}[htbp]
    \caption{Percentage of Execution Time Spent on Linear Operations in Training of ImageNet for VGG16, ResNet152, MobileNetV2}
    \label{tab:training1}
    \resizebox{\linewidth}{!}{%
\begin{tabular}{|ll|cc|cc|cc|}
\hline 
                                    \multicolumn{1}{|l}{\multirow{2}{*}{Phase}}                 & \multicolumn{1}{l|}{\multirow{2}{*}{Operation}}&  \multicolumn{2}{c|}{\textbf{VGG16}} & \multicolumn{2}{c|}{\textbf{ResNet152}} & \multicolumn{2}{c|}{\textbf{MobileNetV2}} \\ \cline{3-8} 
                                                            &               & DarKnight     & Basline    & DarKnight      & Baseline      & DarKnight        & Basline       \\ \hline
\multicolumn{1}{|l}{\multirow{3}{*}{Forward Pass}}         & Linear        & 0.03          & \cellcolor[gray]{0.6}0.9        & 0.04           & \cellcolor[gray]{0.6}0.62          & 0.06             & \cellcolor[gray]{0.6}0.50          \\
\multicolumn{1}{|l}{}                                      & NonLinear     & \cellcolor[gray]{0.8}0.73         & 0.1        & \cellcolor[gray]{0.8}0.65           & 0.38          & \cellcolor[gray]{0.8}0.64             & \cellcolor[gray]{0.6}0.50          \\
\multicolumn{1}{|l}{}                                      & Communication & 0.24          & 0          & 0.31           & 0             & 0.29             & 0             \\ \hline
\multicolumn{1}{|l}{\multirow{3}{*}{Backward propagation}} & Linear        & 0.06          & \cellcolor[gray]{0.6}0.81       & 0.05           & \cellcolor[gray]{0.6}0.6           & 0.05             & \cellcolor[gray]{0.6}0.66          \\
\multicolumn{1}{|l}{}                                      & NonLinear     & \cellcolor[gray]{0.8}0.66          & 0.19       & \cellcolor[gray]{0.8}0.81           & 0.4           & \cellcolor[gray]{0.8}0.76             & 0.34          \\
\multicolumn{1}{|l}{}                                      & Communication & 0.28          & 0          & 0.14           & 0             & 0.19             & 0             \\ \hline
\multicolumn{1}{|l}{\multirow{3}{*}{Total}}                & Linear        & 0.04          & \cellcolor[gray]{0.6}0.84       & 0.04           & \cellcolor[gray]{0.6}0.61          & 0.06             & \cellcolor[gray]{0.6}0.62          \\
\multicolumn{1}{|l}{}                                      & NonLinear     & \cellcolor[gray]{0.8}0.69          & 0.16       & \cellcolor[gray]{0.8}0.76           & 0.39          & \cellcolor[gray]{0.8}0.71             & 0.38          \\
\multicolumn{1}{|l}{}                                      & Communication & 0.26          & 0          & 0.2            & 0             & 0.23             & 0             \\ \hline
\end{tabular}
}
\end{table}
\end{comment}

%\label{sec:setup}
%DarKnight server consisted of an Intel Coffee Lake E-2174G 3.80GHz processor and Nvidia GeForce GTX 1080 Ti GPUs.
%The server has 64 GB RAM and supports Intel Soft Guard Extensions (SGX).
DarKnight's training scheme and the related unique coding requirements are implemented as an SGX enclave thread on an Intel Coffee Lake server. %% where both the decoding and encoding are performed. For SGX implementations, we used Intel Deep Neural Network Library (DNNL) for designing the DNN layers including the Convolution layer, ReLU, MaxPooling, and Eigen library for Dense layer. We used Keras 2.1.5, Tenseflow 1.8.0, and Python 3.6.8. %These are well-established libraries and have been used in prior works~\cite{tramer2018slalom}.  
We used three different DNN models: VGG16~\citep{simonyan2014very}, ResNet152~\citep{he2016deep} and, MobileNetV2~\citep{sandler2018mobilenetv2} and %%%We chose MobileNetV2 because it is the worst-case benchmark for our model as it reduces linear operations considerably (using depth-wise separable convolution), thereby reducing the need for GPU acceleration. %MobileNetV1 replaces standard convolution with depth-wise separable convolution to substantially reduce linear operations which enables execution on small devices. In MobilenetV2 they even make it faster by reducing the number of channels. 
ImageNet~\citep{russakovsky2015imagenet} as our dataset.%, CIFAR-10/100  datasets~\citep{krizhevsky2009learning}. 
%We used CIFAR-10 \cite{krizhevsky2009learning} that has 50,000 training images evenly distributed between 10 categories,  CIFAR-100 \cite{krizhevsky2009learning} that has 60,000 images under 100 classes, and ImageNet \cite{russakovsky2015imagenet} with about 1.2 million images and 1000 categories.
%All the parameters, models' and implementation details, and dataset descriptions are attached in the supplementary material. 
\begin{wrapfigure}{r}{0.5\textwidth}
%\vskip -0.1in
    \includegraphics[width=0.50\textwidth]{Figures/LAN.pdf}
    %\vskip -0.1in
    \caption{Training Speedup over Baseline}
    \label{fig:trainingtime10}
    %\vskip -0.1in
\end{wrapfigure}
\textbf{Training Execution Time}: Figure~\ref{fig:trainingtime10} demonstrates the speedup of training using DarKnight relative to the baseline fully implemented on SGX with $K=2$ images encoded and offloaded to $3$ GPUs. %In the non-pipelined mode, encoding in TEE and GPU operations do not overlap. In the pipelined In pipelined mode, these operations overlap to give better performance. 
The results break down the execution time spent into linear (GPU operations and communication time with GPU) and non-linear (all other operations) categories. %%%Non-linear category include all the operations performed in SGX including the encoding and decoding overheads, while linear costs include operations performed on GPUs plus the communication cost to move the encoded data to the GPUs and bring the computed results back to the TEE. %We assume a non-pipelined implementation where the data encoding and decoding process are performed sequentially. 
The results show that DarKnight speeds up the total linear operation time of VGG16 by $23$x by using the vast GPUs parallelism. %For non-linear operations, DarKnight pays overheads for encoding/decoding. 
The baseline has to encryption/decrypt data that do not fit within the SGX memory, such as some of the large intermediate feature maps in training. Hence non-linear operations observe $1.89X$ speedup in DarKnight. Overall the execution time is improved by more than $8X$ with DarKnight. Both ResNet and MobileNet models have batch normalization layers that are computation-intensive and cannot be offload to GPU accelerators. Even in this worst-case scenario, performance gains of $4.2X$ and $2.2X$ are achieved. More results are provided in Appendix C. 

%By overlapping computation and communication cost, VGG16 and ResNet152 can achieve more than $11X$ and $5X$ times speed over the baseline respectively, while MobilNetV2 almost executes $3X$ faster as our worst-case scenario.

%By speeding up the linear operations in DarKnight (which includes convolution and matrix multiplication) the breakdown is now tilted towards non-linear (including ReLU, Maxpooling, encoding, decoding, batch normalization) operations. With DarKnight the non-linear operations consume about $73\%$ of the execution time while the linear operation time is substantially reduced. 






 
 


