\section{Introduction}
The need for protecting input privacy in Deep learning is growing rapidly in many areas. 
Many of the data holders are, however, not machine learning experts. Hence, data holders are relying on machine learning as a service (MLaaS) platforms~\citep{AzureML,Google,Amazon}. 
%Microsoft Azure ML~\citep{AzureML}, Google AI platform~\citep{Google}, Amazon ML~\citep{Amazon} are some examples. 
These services incorporate ML accelerators such as GPUs for high performance and provide easy to use ML runtimes to enable data holders to quickly set up their models and train. While these platforms lower the steep learning curve, they exacerbate the users' concern regarding data privacy. %Cloud servers are also vulnerable to adversaries who may compromise some of the MLaaS infrastructures, further raising computational integrity concerns. Hence, there is a pressing need to protect privacy and integrity while still enabling the use of untrusted cloud accelerators. 
%Techniques such as homomorphic encryption can be employed in some cases but comes with orders of magnitude slowdowns~\cite{}.   

%At the same time, training large-scale DNNs with a large volume of data needs a significant amount of computation power. Hence, the need for collaborative learning including distributed learning and federated learning has emerged in recent years. Stochastic Gradient Descent is the most well-known technique used for optimization. In distributed learning, the parameter server (\textbf{PS}) is responsible for gradient synchronization by receiving gradients from all the workers, aggregating the gradients, and updating the parameters. 



%There are various methods to protect data privacy, such as Homomorphic Encryption, Secure Multi-Party Computing, Differential Privacy. Each of these methods provides a different privacy guarantee and comes at different costs. ML designers need to be aware of their needs to make a comprehensive decision on which method to use. To evaluate a privacy-preserving method, in addition to privacy guarantee and utility cost, practicality and human efforts made for the method also play important roles. Therefore, although some of the aforementioned methods show promising results for privacy, they cannot be used in the real world because of their expensive costs and high design time.
This work proposes DarKnight, a framework for accelerating privacy and integrity preserving deep learning using untrusted accelerators. DarKnight is built on top of an MLaaS platform that uses unique collaborative computing between the \emph{TEE} and GPU accelerators to tackle both privacy and security challenges. The data holder places their data, whether for training or inference, within the TEE of a cloud server. TEE provides hardware-assisted security for any data and computing performed within the trusted code base. %%%No unauthorized entity can access the data placed in TEE memory or modify the code within the trusted code base. %TEEs use hardware encryption and virtual memory-based isolation mechanisms to protect data and code. 
DarKnight uses TEE to encode input data using a customized matrix masking technique and then uses GPUs to accelerate DNN's linear computations on the encoded data. %Computing solely within TEEs can provide data privacy, by blocking access to TEE memory from intruders. However, TEE-enabled CPUs have limited computation power and memory availability, which creates unacceptable performance hurdles to run an entire model within a TEE. 
Linear operations (convolution, matrix multiplication, etc) are significantly faster on a GPU compared to a TEE-enabled CPU. Therefore, DarKnight distributes these compute-intensive linear operations to GPUs. DarKnight's usage of TEEs is limited to protecting the privacy of data through a customized matrix masking and performing non-linear operations (ReLU, Maxpool). 


TEE-GPU collaboration is first used in~\citep{tramer2018slalom} for inference. However, the method cannot be used for training as elaborated in their paper. Several prior works on protecting privacy use cryptography techniques on Finite Fields to provide data privacy. Such approaches limit their usage to arithmetic on quantized models~\citep{mohassel2017secureml,gascon2017privacy, so2019codedprivateml,wagh2019securenn, juvekar2018gazelle}. Quantization for deep learning is a challenging task. DarKnight supports \emph{floating point} model training and control the information leakage by encoding parameters. DarKnight can also detect any malicious activities of untrusted GPUs by its computation integrity feature. Furthermore, DarKnight can protect privacy and integrity even in the presence of a subset of colluding GPUs that try to extract information or sabotage the computation. 
%%%Moreover, Floating-point models are routinely used in training due to convergence, accuracy, and faster implementation considerations~\citep{johnson2018rethinking, guo2020secure, imani2019floatpim,aliasgari2013secure}. Many DNN accelerators such as google TPUs, Intel Xenon processors, Intel FPGAs, and Tensorflow now support bfloat16 floating-point format for training~\citep{kalamkar2019study,FPGA1,googlcloud1,googletensor1}. 
% Quantization for deep learning needs a meticulous hyper-parameter tuning process for different applications and datasets, which is a challenging task. In practice, f

%\name allows users to train using floating-point (FP) representation for model parameters, while still providing rigorous bounds on information leakage. 
%DarKnight supports \emph{floating point} model training. %ML designers can choose a setting based on their training and privacy needs. For Finite Field, the privacy guarantee is as strong as a one-time-pad by making the encoded data look uniformly random to the adversaries~\citep{deng2004secure}, which is the most commonly provided guarantee in prior works~\citep{tramer2018slalom, yu2019lagrange}.
%For providing privacy guarantee in our floating-point model, the information leakage is bounded by the parameters of the encoding. %The strength of the random noise used in our matrix masking process trades off information leakage with computational stability. In our preferred implementation parameters, DarKnight's selection of random noise bounds information leakage to be no more than $~10^{-7}$ without compromising accuracy.
%In floating-point arithmetic, perfect privacy can be achieved when the information leakage is less than the round off error~\cite{guo2020secure}. Hence, by bounding the information leakage to $~10^{-7}$, DarKnight achieves perfect privacy in IEEE standard single-precision arithmetic~\cite{hough2019ieee}.% In the rest of the paper, we focus on floating-point models. For avid readers, the fixed point privacy scheme and results are shown in Appendix~\ref{ap:fixed}.
% DarKnight can also detect any malicious activities of untrusted GPUs by its computation integrity feature. Furthermore, DarKnight can protect privacy and integrity even in the presence of a subset of colluding GPUs the try to leak information. 
%We implemented DarKnight using an Intel SGX-enabled CPU to perform matrix masking and non-linear DNN operations while using an Nvidia GPUs to accelerate linear operations. 
%{\color{red}The strength of the random numbers in matrix masking were chosen to preserve the original training accuracy. Using these parameters DarKnight guarantees that no more than one bit of information is leaked from a ten-megapixel input image. Note that this will be an upper bound on the leaked information, assuming that the adversary has access to unlimited computation power to decode the encoded inputs.} 
%To the best of our knowledge, this is the first work that uses TEE-GPU collaboration for \textbf{\emph{training}} large DNNs on private data while providing computational integrity and theoretical bounds on information leakage.     
