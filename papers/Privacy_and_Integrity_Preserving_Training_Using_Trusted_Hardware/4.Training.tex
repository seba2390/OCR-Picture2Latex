\section{Privacy in Training}
\label{sec:training}
for simplicity, we first show how this mechanism works for a system in which GPUs are not colluding and next we expand the encoding to support a system with $M$ colluding GPUs in Appendix B. For a model with $L$ layers which is being trained with a batch of $K$ inputs, the model parameters $\mathbf{W}_{l}$ at layer $l$ are updated using the well known SGD process as:
\begin{equation}
\mathbf{W}^{\text{new}}_{l} = \mathbf{W}^{\text{old}}_{l} - \eta\times \triangledown \mathbf{W}_{l},\quad
\triangledown \mathbf{W}_{l}=\frac 1 K \sum_{i=1}^K ~ \langle \delta^{(i)}_{l} , {\mathbf x^{(i)}_{l}}\rangle
\label{eq:sgd}
\end{equation}
Here $x_l^{(i)}$ is the $i^{\text{th}}$ input of layer $l$. $\eta$ is the learning rate, and $\delta^{(i)}_{l}$ is the gradient of the loss for the $i^{\text{th}}$ point in the training batch, with respect to the output of layer $l$. %%%We first start with the encoding/decoding in forward pass  which is the first phase in training and then explain how backward propagation works. Please note that forward pass and inference are similar in terms of encoding and decoding functions.
%Also for simplicity, we first show how this mechanism works for 1-private system in which GPUs are not colluding and next we expand the encoding to support a system with $M$ colluding GPUs in section~\ref{sec:colluding}.
\subsection{Forward Pass}
 At a layer $l$ the forward pass, we need to  compute $\mathbf y_l=\langle \mathbf W_l~,~\mathbf x_l\rangle$, where $\langle \cdot,\cdot\rangle$ corresponds to the bilinear operation at that layer (e.g. matrix product, convolution, etc.).  After the linear operation finishes, an activation function ($g(\cdot)$) creates the next layer input $\mathbf x_{l+1}=\text{g}(\mathbf y_l)$.  Within this context, DarKnight first receives a set of $K$ inputs $\mathbf x_0^{(1)},\dots,\mathbf x_0^{(K)}$ for a batch training from a client. Our goal is to perform linear calculations of $\mathbf y_0^{(1)}= \langle\mathbf W_0 , \mathbf x_0^{(1)}\rangle,\dots,\mathbf y_0^{(K)}=\langle\mathbf W_0, \mathbf x_0^{(K)}\rangle$ on the GPUs without exposing the inputs to the GPU. Note that the subscript $0$ in all these variables refers to the first layer. At this point, we drop the subscript for a more clear notation. Also, we apply $\mathbf {\color{red}x}$ for the inputs that need to be protected and $\mathbf{\color{blue}\bar{x}}$ for the encoded inputs to visually distinguish different notations. DarKnight must protect ${\mathbf x^{(i)}_{l}}$ for each layer of the DNN when the layer's linear operations are outsourced to GPUs. 
 
 \textbf{Key Insight:} The main idea behind DarKnight's privacy protection scheme is the fact that the most computationally intensive operator (such as convolutions) is \emph{bilinear}. Thus, instead of asking a GPU to calculate $\langle \mathbf W,\mathbf {\color{red} x^{(i)}}\rangle$, which exposes the inputs, DarKnight uses matrix masking to linearly combine the inputs and add a random noise to them. Due to the bilinear property, any linear operation on $K$ masked inputs can be recovered if there are $K$ different linear computations performed.
 

%The general form of $\mathbf B~\mathbf X~\mathbf A + \mathbf C$ is used for protecting Matrix $\mathbf X$. Any of these matrices can be used for masking data based on the data privacy goal. For DarKnight we use $\mathbf A$ and $\mathbf C$ as we explain in this section. A combination of matrix masking and coded computing used in~\cite{yu2019lagrange}. Our scheme is a customized version of Lagrange coded computing designed for training DNNs.


\textbf{DarKnight Encoding}: Using a customized version of matrix masking~\citep{cox1980suppression, cox1994matrix, kim1986method, spruill1983confidentiality,yu2019lagrange}, The SGX based enclave within the cloud server first receives a set of inputs from a data holder. Then the DarKnight scheme creates $K+1$ encoding within the SGX from $K$ data inputs (${\color{red}{\mathbf x}^{(1)}},\dots,{\color{red}{\mathbf x}^{(K)}}$), as follows,
\begin{align}\label{eq:inference_blinding}
{\color{blue}\bar{\mathbf x}^{(i)}}\quad= \quad \alpha_{i,1} {\color{red}\mathbf{x}^{(1)}} + \dots+ \alpha_{i,K} {\color{red}\mathbf{x}^{(K)}}  +\alpha_{i,(K+1)} \mathbf{r}~
\end{align}
Where $i=1,\dots,(K+1)$. The scalars $\alpha_{i,j}$, and the noise vector $\mathbf r$ are randomly generated; and the size of $\mathbf r$ matches that of ${\color{red}\mathbf x}$.
The scalars $\alpha_{i,j}$'s are represented by matrix $\mathbf A \in \mathbb R^{(K+1),(K+1)}$, which are dynamically generated for each virtual batch and securely stored inside SGX for decoding. As we prove later, by revealing the values ${\color{blue}\bar{\mathbf x}^{(i)}}$'s to GPUs, we protect the privacy of inputs ${\color{red}\mathbf x^{(i)}}$'s. At the next step, the encoded data ${\color{blue}\bar{\mathbf x}^{(i)}}$'s are sent to the GPUs which performs the following computations:
${\color{blue}\bar{\mathbf y}^{(i)}} =\langle \mathbf W , {\color{blue}\bar{\mathbf x}^{(i)}}\rangle, \quad i=1,\dots,(K+1)$.
Please note that each GPU only receives one encoded data. Note-worthily matrix $\mathbf A$ can be chosen such that its condition number close to one, so that encoding and decoding algorithm remains numerically stable. Hence, orthogonal matrices serve us the best.

\textbf{DarKnight Decoding}:  The $K+1$ outputs ${\color{blue}\bar{\mathbf y}^{(i)}}$ returned from the GPUs must be decoded within the SGX to extract the original results ${\color{red}\mathbf y^{(i)}}$.  These value can be extracted  as follows,
\begin{align}
    {\color{blue}\bar{\mathbf Y}}=\left\langle \mathbf W, [{\color{blue}\bar{\mathbf x}^{(1)}},\dots,{\color{blue}\bar{\mathbf x}^{(K+1)}}] \right\rangle =
    \underbrace{\left\langle \mathbf W, [{\color{red}{\mathbf x}^{(1)}},\dots,{\color{red}\mathbf x^{(K)}},\mathbf r] \right\rangle}_{{\color{red}\mathbf Y}} ~\cdot \mathbf A~\Rightarrow~ {{\color{red}\mathbf Y}}={\color{blue}\bar{\mathbf Y}}\cdot \mathbf A^{-1}~
\end{align}
\subsection{Backward Propagation}
 The decoding process for forward pass exploited the invariant property of model parameter for any given input such  that $\left\langle \mathbf W, [{\color{blue}\bar{\mathbf x}^{(1)}},\dots,{\color{blue}\bar{\mathbf x}^{(k+1)}}] \right\rangle = \left\langle \mathbf W, [{\color{red}{\mathbf x}^{(1)}},\dots,{\color{red}\mathbf x^{(k)}},\mathbf r] \right\rangle ~\cdot \mathbf A~$, meaning that a single $\mathbf{W}$ was shared between all the inputs of that layers. However, during the backward propagation process, we a have different $\delta_l^{(i)}$ for each input $\mathbf {\color{red}x_l^{(i)}}$. Thus, decoding the $\langle \delta^{(i)}_{l}, \mathbf {\color{red}x^{(i)}_{l}}\rangle$ from obfuscated inputs $\langle \delta^{(i)}_{l} , {\color{blue}\bar{\mathbf x}^{(i)}_{l}}\rangle$  is a more challenging approach that requires specific decoding approach.

\textbf{Key Insight:} While backward propagation operates on a batch of inputs, it is not necessary to compute the $\langle \delta^{(i)}_{l}, {\color{red}\mathbf x^{(i)}_{l}}\rangle$ for each input ${\color{red}\mathbf x^{(i)}}$. Instead, the training process only needs to compute cumulative parameter updates for the entire batch of inputs. Hence, what is necessary to compute is the entire $\triangledown \mathbf{W}_{l}$ which is an average over all  updates corresponding to inputs in the batch. 

%\vspace{-1mm}
\textbf{DarKnight Encoding:} DarKnight exploits this insight to protect privacy without significantly increasing the encoding and decoding complexity of the blinding process. 
%In particular, DarKnight uses a new linear encoding scheme to combine inputs covered by noise signal similar to Lagrange Coded Computing~\cite{yu2019lagrange}. 
As shown in Equation~\eqref{eq:sgd}, there are $K$ inputs on which gradients are computed. DarKnight calculates the overall weight update in the backward propagation by summing up the following $K+1$ equations each of which are computed on a different GPUs,
%are sent to GPU to be multiplied by a linear combination of the gradients $ \delta^{(i)}_{l}$, a carefully chosen random scalars to give us the desired summation in decoding. 
\begin{equation}\label{eq:gamma_lin}
\triangledown \mathbf{W} = \sum_{j=1}^{K+1}  \gamma_{j} \text{Eq}_{j}, \qquad \text{Eq}_{j} = \left\langle \sum_{i=1}^K \beta_{j,i}~ \mathbf \delta^{(i)}~,{\color{blue}\bar{\mathbf x}^{(j)}} \right\rangle~\quad~
\end{equation}
In the above equations,the encoded input ${\color{blue}\bar{\mathbf x}^{(j)}}$ to a layer is the same that was already calculated during the forward pass using Equation~\eqref{eq:inference_blinding}. Hence, the TEE can simply reuse the forward pass encoding without having to re-compute. The gradients are multiplied with the $\beta_{j,i}$ in the GPUs after which the GPUs compute the bi-linear operation to compute $\text{Eq}_{j}$. 

In contrast to inference where $\mathbf{W}$'s are fixed for all the inputs, during training the parameter updates are with respect to a specific input. Hence, each $\delta^{(i)}_l$'s corresponds to different ${\color{red}\mathbf{x}^{(i)}_l}$ during training. As such, DarKnight uses a different encoding strategy where the overall parameter updates $\triangledown \mathbf{W}$ can be decoded very efficiently. In particular, DarKnight selects $\alpha_{j,i}$'s, $\beta_{j,i}$'s and $\gamma_i$'s such that
\begin{equation}
    \mathbf B^\intercal\cdot \mathbf \Gamma\cdot \mathbf A = \begin{bmatrix}1 & 0 & \dots & 0 & 0
  \\0 & 1 & 0 & \dots & 0
  \\\vdots & \ddots& \ddots & \ddots & \vdots
\\ 0 & \dots & 0 & 1 & 0\end{bmatrix}_{K \times (K+1)}
\label{eq:matrix_relation1}
\end{equation}

Assuming batch size is equal to $K$, the $\beta_{i,j}$ parameters used for scaling $\delta$ values is gathered in the $K+1$ by $K$ matrix, $\mathbf B$. $\alpha_{i,j}$'s are gathered in the $K+1$ by $K+1$ matrix $\mathbf A$, the scalar matrix with the same size for intermediate features and $\gamma_i$'s form the diagonal of a $K+1$ by $K+1$ matrix $\Gamma$, that gives us the proper parameters for efficient decoding. Note that the SGX keeps matrix $\Gamma$ and $\mathbf A$ as secret. We provide the details of privacy guarantee in Appendix A.

\textbf{DarKnight Decoding:} Given the constraint imposed on $\alpha_{j,i}$'s, $\beta_{j,i}$'s and $\gamma_i$'s the decoding process is trivially simple to extract $\triangledown \mathbf{W}$. It is easy to see that if the scalars $\alpha_{i,j}$'s, $\beta_{i,j}$'s and $\gamma_i$'s satisfy the relation~\eqref{eq:matrix_relation1}, the decoding process only involves calculating a linear combination of the values in Equation~\eqref{eq:gamma_lin}.
\begin{align}
    \frac 1 K\sum_{j=1}^{K+1}  \gamma_{j} ~ \text{Eq}_{j}=\frac 1 K \sum_{i=1}^K ~ \langle \delta^{(i)}_{l} , {\color{red}\mathbf x^{(i)}_{l}}\rangle=\triangledown \mathbf{W}_l
\end{align} 

%%%Note that even though the above equations are computed over $K$ inputs in a virtual batch, it is possible for the SGX enclave to securely store multiple $\triangledown \mathbf{W}_l$ associated with multiple virtual batches that comprise the training batch. Once all the inputs in the training batch are processed the SGX enclave can do a single aggregation to generate a batch-wide weight update. %Details are provided in Appendix~\ref{ap:training}.

%\vspace{-1mm}
%Note that since we sent the linear combination of the inputs $\mathbf x_l^{(i)}$ plus a random noise, the untrusted hardware does not have access to the inputs. \\

%%%\textbf{DarKnight Training Complexity:} It is important to note that DarKnight's training approach for encoding and decoding is very simple. The size of the $\alpha$,  $\delta$, and  $\gamma$ matrices is just proportional to the square of the batch size that is being processed at one time. Therefore, generating them for every batch has a negligible performance overhead. Even with 8-64 batch size, (commonly used in VGG training~\citep{canziani2016analysis, han2015deep, narra2019slack} these scaling values are substantially smaller than the model parameters $\mathbf{W}$. Hence, the order complexity of encoding/decoding operations is much less than the linear operations ($\left\langle \mathbf W \mathbf , x \right\rangle$) in a DNN with millions of parameters. In addition to that, the process of decoding $K$ inputs with one random noise requires $K+1$ computations. During decoding, we extract $\mathbf W \cdot \mathbf r$, but that value is just dropped. Thus, DarKnight trades only $\frac 1 K$ additional computations to provide privacy.

\begin{comment}
\textbf{DarKnight Advantages:}  (1) Unlike prior works~\citep{tramer2018slalom} DarKnight does not need to store $\mathbf W \cdot \mathbf r$ within the SGX memory thereby significantly enhancing our ability to infer with much larger models. (2) size of the matrix $\mathbf A$ is proportional to the number of inputs that are encoded together (K) and is orders of magnitude smaller than the model size $\mathbf W$. Hence, the order complexity of encoding/decoding operations is much less than the linear operations ($\left\langle \mathbf W \mathbf , x \right\rangle$) in a DNN with millions of parameters.
(3) The process of decoding $K$ inputs with one random noise requires $K+1$ computations. During decoding, we extract $\mathbf W \cdot \mathbf r$, but that value is just dropped. Therefore, DarKnight trades only $\frac 1 K$ additional computations to provide privacy.
\end{comment}
\begin{comment}
Not only the accuracy of training is the paramount metric to evaluate a training mechanism, but also the speed of it is a key factor since training a DNN may run into days. To the best of our knowledge, this is the first TEE-based work aimed at DNN training and not doing all the computations inside the enclave while keeping the security guarantees. 
Implementing stochastic gradient descent(SGD) has two main phases. The first phase is the forward pass which is very similar to inference. At the end of the forward pass, the loss function is computed and in the next step backward propagation is started. The goal of backward propagation is to compute gradients with respect to all the parameters in the neural network. Implementing backward propagation with respect to data privacy has challenges which we explain in the following section.  

The \textit{training process} has the following steps. 

\begin{itemize}
    \item In each iteration, we choose a \textit{batch} of $k$ data points $\{(\mathbf x^{(i)},\mathbf y^{(i)}_{exp}\}_{i=1}^k$.
    \item We feed the $k$ points to the network, so the features are the inputs of the first layer $\mathbf x_0^{(i)}=\mathbf x_0^{(i)}$. We then perform a \textit{forward pass}, to compute the outputs, $\mathbf y_L^{(i)}$ of the DNN for those $k$ inputs.
    \item We would like to minimize the loss, $\mathcal{L} = \frac {1}{K} \sum_{i=1}^K \ell(\mathbf{y}^{(i)}_{L} , \mathbf{y}^{(i)}_{exp}) $. We do so by taking gradient of the loss, with respect to the weights of each layer l, $\mathbf W_l$, and update the weights. 
    
    \item Computing weight updates of each layer by calculating $\triangledown W_{l}$ starting from layer $L$ and propagating the values to the previous layers until it reaches the first layer. 
    %$\frac {1}{K} \sum_{i=1}^K \frac{\partial \mathcal{L}}{\partial \mathbf{W}_{l}}$
    
    \item Applying updates to the weights and repeat the procedure for the next batch, until convergence.
    
\end{itemize}
\subsubsection{Backward Propagation}
There are two sets of linear operations in each layer for computing backward pass. The goal is to compute the derivative of loss with respect to weights. To compute that we need to compute the derivative of loss with respect to the output of that layer. We explain the details in the following paragraphs.

\textbf{Computing loss with respect to weight}: To find out the amount of the update for each weight, we need to take a derivative of loss with respect to the weights. Assume that we have a batch size of $K$ and $\triangledown W_{l}$ denotes the amount of weight update for the weights in layer $l$. $\ell(\mathbf{y}^{(i)}_{L} , \mathbf{y}^{(i)}_{exp})$ is the error vector corresponding to input data $i$. 
\begin{equation*}
\begin{split}
%\begin{align*}
\triangledown \mathbf{W}_{l} =\frac{\partial\mathcal L}{\partial\mathbf W_l} = \frac {1}{K} \sum_{i=1}^K \langle \frac{\partial \ell(\mathbf{y}^{(i)}_{L}, \mathbf{y}^{(i)}_{exp})}{\partial \mathbf{y}_{l} }, \frac{\partial \mathbf{y}_l}{\partial \mathbf{W}_l} \rangle \\
%\triangledown w^{k}_{i,j}(n) = \frac{\partial (E(n))}{\partial w^{k}_{i,j}} = \frac{\partial (E(n))}{\partial y^{k}_{j}(n)} \frac{\partial y^{k}_{j}(n)}{\partial w^{k}_{i,j}} = g^{k}_{j}(n)\;x^{k}_{i}(n) \\
%\triangledown \mathbf{W}_{l}= \delta^{k}_{j}(1) \times o^{k-1}_{i}(1) \quad ... \quad \triangledown w^{k}_{i,j}(N) = \delta^{k}_{j}(N) \times o^{k-1}_{i}(N)\\
\triangledown \mathbf{W}_{l} =\frac {1}{K} \sum_{i=1}^K (\triangledown \mathbf{W}^{(i)}_{l}) , \qquad \mathbf{W}^{\text{new}}_{l} = \mathbf{W}^{\text{old}}_{l} - \eta\times \triangledown \mathbf{W}_{l} 
%\end{align*}
\end{split}
\end{equation*}

\textbf{Computing loss with respect to output}
$\delta^{(i)}_{l}$ represents the gradient of layer l for input data point $i$. $g_{l}$ is the activation function of layer $l$.
As pointed out before, weights are kept in the GPU and the linear operation takes place inside the GPU as well.

\begin{equation*}
\delta^{(i)}_{l} = \frac{\partial \ell(\mathbf{y}^{(i)}_{L}, \mathbf{y}^{(i)}_{exp})}{\partial \mathbf{y}_{l} } = \langle \frac{\partial \ell(\mathbf{y}^{(i)}_{L}, \mathbf{y}^{(i)}_{exp})}{\partial \mathbf{x}_{l+1} }, \frac{\partial \mathbf{x}_{l+1}}{\partial \mathbf{y}_l}  \rangle = \langle \frac{\partial \ell(\mathbf{y}^{(i)}_{L}, \mathbf{y}^{(i)}_{exp})}{\partial \mathbf{x}_{l+1} }~,~ {g^\prime_{l}}^{(i)} \rangle
\end{equation*}
To sum up, we need the following to update the weights of layer $l$,\\
\begin{align}\label{eq:grad_weights}
    \triangledown \mathbf{W}_{l}=\frac 1 K \sum_{i=1}^K ~ \langle \delta^{(i)}_{l} , \mathbf x^{(i)}_{l}\rangle
\end{align}
We aim to avoid leaking information about input and hence, we have to avoid revealing intermediate values (inputs of each layer, $ \mathbf x^{(i)}_{l}$). For this to be achieved, we linearly combine multiple images with a random noise vector to hide the information. Although this approach may look similar to inference, selecting scalars are not purely random in this phase as we explain the details. Previously in inference and forward pass phase $\mathbf{W}$ was shared between different inputs. Whereas in backward propagation, gradient multiplying to the input of each layer, $\delta^{(i)}_{l}$, is different for each training point. To address this issue, a linear combination of the inputs (covered by noise), are sent to GPU to be multiplied by a linear combination of the gradients $ \delta^{(i)}_{l}$, a carefully chosen random scalars to give us the desired summation in decoding. In the other words, instead of calculating the $K$ products in~\eqref{eq:grad_weights}, we calculate the following $K+1$ equations,\\
\begin{align}\label{eq:training_security}
\text{Eq}_{j} = \left\langle \sum_{i=1}^K \alpha_{j,i}~\delta^{(i)}_{l}~,~ \sum_{i=1}^K \beta_{j,i}~ x^{(i)}_{l} + \beta_{j,(K+1)} ~\bar{\mathbf r} \right\rangle~,\quad j=1,\dots,K+1~.
%Eq_{1} = (\alpha_{11}\delta_{i,1}+\alpha_{12}\delta_{i,2}+...+\alpha_{1m}\delta_{i,m})\times(\beta_{11} x_{i,1} + \beta_{12} x_{i,2} + ... + \beta_{1m} x_{i,m}+ \epsilon_{1}r^\prime)\\
%Eq_{m} = (\alpha_{m1}\delta_{i,1}+\alpha_{m2}\delta_{i,2}+...+\alpha_{mm}\delta_{i,m})\times(\beta_{n1} x_1 + \beta_{n2} x_2 + ... + \beta_{mm} x_{i,m} +\epsilon_{n}r^\prime)\\
\end{align}

Since the number of scalars increased, using the same decoding mechanism is not efficient and we propose a more effectual decoding designed for backward propagation. In the backward propagation, it is important to know $\triangledown \mathbf{W}$, thus computing the components of it individually are inessential. Therefore, we would like to have a set of scalars $\{\gamma_i\}_{i=1}^K$, in such a way that the linear combination of the values $Eq_{0}$ to $Eq_{K}$ with $\gamma_i$'s give us the desired combination. In the other words, we are interested to combine $\text{Eq}_i$'s as follows to get $\triangledown \mathbf{W}_{l}$ in\\~\eqref{eq:grad_weights},
\begin{align}\label{eq:gamma_lin1}
\triangledown \mathbf{W}_{l} = \sum_{j=1}^{K+1}  \gamma_{j} ~ \text{Eq}_{j}~.
\end{align}
Now we would like to choose the values of $\alpha_{i,j}$'s, $\beta_{i,j}$'s and $\gamma_i$'s in such a way that~\eqref{eq:gamma_lin1} holds. To do so, we define the matrices $\mathbf A$, $\mathbf B$ and $\mathbf\Gamma$ as follows,
\begin{align}
    &\mathbf A=\begin{bmatrix}\alpha_{1,1} & . & . & \alpha_{1,K} 
  \\. & . & . & .
  \\. & .& . & .
\\\alpha_{(K+1),K} & . & . & \alpha_{K,(K+1)}\end{bmatrix}_{(K+1)\times K} ~,~
\mathbf B= \begin{bmatrix}\beta_{1,1} & . & . & \beta_{1,(K+1)} 
  \\. & . & . & .
  \\. & .& . & .
\\\beta_{(K+1),1} & . & . & \beta_{(K+1),(K+1)}\end{bmatrix}_{(K+1)\times (K+1)}  \nonumber\\
&~,~\text{and, }\quad\mathbf\Gamma=\begin{bmatrix}\gamma_{1} & . & . & . 
  \\. & \gamma_{2} & . & .
  \\. & .& \gamma_{3} & .
\\ . & . & . & \gamma_{K+1}\end{bmatrix} _{(K+1)\times (K+1)}~.
\end{align}
With these definitions, if we want to~\eqref{eq:grad_weights} to hold witt~\eqref{eq:training_security}, we need the scalars $\alpha_{i,j}$'s, $\beta_{i,j}$'s and $\gamma_i$'s to hold in the following equation,
\begin{align}
    \mathbf A^\intercal\cdot \mathbf \Gamma\cdot \mathbf B = \begin{bmatrix}1 & 0 & \dots & 0 & 0
  \\0 & 1 & 0 & \dots & 0
  \\\vdots & \ddots& \ddots & \ddots & \vdots
\\ 0 & \dots & 0 & 1 & 0\end{bmatrix}_{K \times (K+1)}
\end{align}
In the following equations, we demonstrate that these scalars should be generated randomly while holding the following constraint. Assuming mini-batch size is equal to n, $\alpha$ is an n+1 by n+1 matrix for scaling $\delta$ values. $\beta$ is the scalar matrix with the same size for intermediate features and $\gamma$ is a diagonal matrix that gives us the proper parameters for efficient decoding: \\
This approach may raise the question about the complexity of the blinding/unblinding functions. We need to point out that since all these parameters can be generated in the prepossessing phase and be stored inside TEE, they do not pose any performance overhead to the system. For the blinding/unblinding functions because of the limited size of the SGX memory, m cannot be larger than 4 and thus the order of the matrix multiplication for blinding is much lower than the actual matrix multiplication in the backward pass.  
Weights are stored in the GPU memory, and because of this $\triangledown \mathbf{W}$ must be sent out to GPU for weight updates.
\end{comment}


\textbf{Computational Integrity:}
DarKnight's encoding scheme can be extended to detect computational integrity violations by untrusted GPUs. %%%In this case, the linear computations performed by GPUs must also be verified. In the interest of space, we just provide an insight into how DarKnight can perform data integrity checks for inference and we leave the details for the Appendix~\ref{ap:integrity}. 
%Similar extensions for training are also possible. 
%%%Recall that DarKnight  creates $K+1$ encoded inputs ${\color{blue}\bar{\mathbf x}^{(1)}},\dots,{\color{blue}\bar{\mathbf x}^{(K+1)}}$ for $K$ original inputs.
To provide integrity, DarKnight creates one additional linear combination of inputs (say ${\color{blue}\bar{\mathbf x}^{(K+2)}}$), using the same approach as in Equation~\eqref{eq:inference_blinding}. This additional equation allows us to verify the accuracy of each result ${\color{red}{\mathbf y}^{(i)}}$ by computing it redundantly. %%An error is detected if the difference between the two estimations is larger than our desired computation precision. In case an error is detected, TEE may perform additional corrective action, such as executing on another GPU worker or perform additional redundant computations. But these actions are outside the scope of our current work.
\begin{comment}
\subsection{Colluding GPUs}
\label{sec:colluding}
In this section, we investigate the scenario in which multiple GPUs can collaborate to extract information from the encoded data. With $K'$ GPUs and virtual batch size of $K$, we can tolerate $M < K'-K$ colluding GPUs without compromising privacy. We show how we can securely outsource calculating $\langle\mathbf W,\mathbf x^{(i)}\rangle$, $i=1,\dots, K$, to the GPUs. We first create $P=M+K$ encoded data vectors, $\bar{\mathbf x}^i$, $i=1,\dots, P$, using $M$ noise vectors $\mathbf R^1,\dots,\mathbf R^M$, as follows.
\begin{align}
    &\bar{\mathbf X}=\mathbf X\mathbf A_1+\mathbf R\mathbf A_2~,\quad\text{where ,}\nonumber\\
    &\bar{\mathbf X}=\left[\bar{\mathbf x}^1,\dots,\bar{\mathbf x}^{P}\right]\in\mathbb R^{N\times P}~,\nonumber\\
    &{\mathbf X}=\left[{\mathbf x}^1,\dots,{\mathbf x}^K\right]\in\mathbb R^{N\times K}~,\nonumber\\
    &{\mathbf R}=\left[{\mathbf R}^1,\dots,{\mathbf R}^M\right]\in\mathbb R^{N\times M}~,\nonumber\\
    &\text{and ,}~~\mathbf A_1\in\mathbb R^{K\times P}~,~~\mathbf A_2\in\mathbb R^{M\times P}~.
\end{align}
Here, the matrices $\mathbf A_1$ and $\mathbf A_2$ are the encoding coefficient similar to the initial scheme we used for DarKnight. 
Same as before, we can calculate the weight updates using the following equations:
\begin{equation}\label{eq:gamma_lin_colluding1}
\triangledown \mathbf{W} = \sum_{j=1}^{P}  \gamma_{j} \text{Eq}_{j}, \quad \text{Eq}_{j} = \left\langle \sum_{i=1}^{K} \beta_{j,i}~ \mathbf \delta^{(i)}~,{\color{blue}\bar{\mathbf x}^{(j)}} \right\rangle
\end{equation}
We now define
\begin{align}
    \mathbf A=\begin{bmatrix}
    \mathbf A_1\\
    \mathbf A_2
    \end{bmatrix}~,~\mathbf B=\begin{bmatrix}
    \beta_{j,i}
    \end{bmatrix}~,\quad \Gamma=\text{Diag}(\gamma_1,\dots,\gamma_P)
\end{align}
Now, it is easy to show that we can recover $\triangledown \mathbf W$ using Equation~\eqref{eq:gamma_lin_colluding1}:
\begin{equation}
    \mathbf B^\intercal\cdot \mathbf \Gamma\cdot \mathbf A = \begin{bmatrix}1 & 0 & \dots & 0 & 0 & \dots & 0
  \\0 & 1 & 0 & \dots & 0 & \dots & 0
  \\\vdots & \ddots& \ddots & \ddots & \vdots & \ddots  
\\ 0 & \dots & 0 & 1 & 0 &\dots & 0\end{bmatrix}_{K \times P}
\label{eq:matrix_relation2}
\end{equation}
The details are provided in Appendix~\ref{ap:collude}.
\end{comment}