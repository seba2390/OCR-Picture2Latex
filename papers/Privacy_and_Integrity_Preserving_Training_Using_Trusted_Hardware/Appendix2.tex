\section{Privacy Guarantee}
\label{ap:pri}
Darknight provides privacy by matrix masking. Masking keeps all the variables in their floating-point representations while adding Gaussian noise (or uniform noise) to the vector we would like to protect. 
%This line of work has gained attention recently, as floating-point training is more robust. Many DNN accelerators use BFloat16~\citep{kalamkar2019study} which is a half-precision floating-point. This format is used in Intel Xeon Processors, Intel FPGAs~\citep{FPGA1, FPGA2}, Google Cloud TPUs~\citep{googlcloud1, googlcloud2} and Tensorflow~\citep{googletensor, googlcloud2}.

The information leaked with masking indicates how much information the masked vector possesses about the raw data~\citep{guo2020secure, matthews2011data}. In the other words, it represents the amount of information the adversary can potentially gain from the raw data, \textit{without} any assumption or limitation on adversaries power.
%\subsection{Approach 1: Matrix masking in floating-points}
%using floating-point arithmetic not only helps with the design-time using available hardware but also it is more robust against side-channel attacks(?).\\

We will first explain a general matrix masking introduced by~\citep{cox1980suppression, cox1994matrix}. Next, we will explain Darknight privacy, through the notation used in matrix masking. Finally, we will calculate the information leakage in our masked matrix, as a measure of privacy.
%We will show that the masked matrix leaks at most $?$ bit of information. This means that if the adversary has access to unlimited computation power, it can potentially learn whether a thousand pixel image is bright or grey.

\textbf{Matrix Masking:}\\ 
Introduced by~\citep{cox1980suppression, cox1994matrix}, matrix masking scheme can be used for a variety of reasons such as noise addition, sampling, etc.
The general form of $BXA + C$ is used for protecting Matrix X. In the above formula B, A, and C are called record transformation masks, attribute transformation masks, and displacing masks, respectively. Any of these matrices can be used for encoding data based on the data privacy goal. For instance,~\citep{kim1986method} first added random noise to data and then transformed it to form a distribution with the desired expected value and variance, by carefully tuning $A$ and $B$.~\citep{spruill1983confidentiality} empirically compared different masking schemes including additive and multiplicative noise. Darknight encoding is a form of matrix masking, with the right choice of the matrices $A$, $B$, and $C$. A combination of Matrix Masking and coded computing first introduced in~\cite{yu2019lagrange}, for secure and robust computation.

\textbf{DarKnight Encoding:}\\ 
Following our notation in \eqref{eq:inference_blinding}, our goal is to protect the vectors $\mathbf x_i$, by adding a random noise to each as follows
\begin{align}\label{eq:inference_blinding2}
&\bar{\mathbf x}^{(i)}\quad= \quad \alpha_{i,1} \mathbf{x}^{(1)} + \dots+ \alpha_{i,K} \mathbf{x}^{(K)}  +\alpha_{i,(K+1)} \mathbf{r}~,\nonumber\\ &i=1,\dots,(K+1)~,
\end{align}
where $\mathbf r$ is a random noise vector, and $\alpha_{i,j}$'s are also chosen randomly. 
Now first, we denote $\mathbf X=[\mathbf x^{(1)},\dots,\mathbf x^{(K)}]$ to be the matrix that we would like to protect, and $\bar{\mathbf X}=[\bar{\mathbf x}^{(1)},\dots,\bar{\mathbf x}^{(K)}]$ to be the masked matrix that we send to unsecured GPU. In this case, the equation \eqref{eq:inference_blinding2} can be rewritten as follows. 
\begin{align}
    \bar{\mathbf X}=\mathbf X\cdot A_1 + \mathbf r\cdot \mathbf a_2^T
\end{align}
where the matrix $A=[\alpha_{i,j}]_{i,j}\in\mathbb R^{(K+1)\times (K+1)}$ contains some values of $\alpha_{i,j}$'s, and $\mathbf a_2^T=[\alpha_{1,(K+1)},\dots,\alpha_{(K+1),(K+1)}]$. 

We also prefer to choose a matrix $\mathbf A_1$, with a condition number close to one, so that our encoding and decoding algorithm remains numerically stable. For this purpose, orthogonal matrices serve us the best. In addition to that, the transformation of the matrix whose entities are independent and identically distributed standard normal variants is invariant under orthogonal transformations. Therefore, if an orthogonal matrix is used for encoding, the distribution of the raw data and encoded data remains the same~\citep{kim1986method}, which is preferable in data privacy.

\textbf{Privacy Guarantee:}\\ 
In this section, we bound the information that leaks, when using Darknight's masking approach. The amount of information leaked by $\bar{\mathbf x}^{(i)}$'s about $\mathbf x^{(j)}$ is the \textbf{mutual information} between these two variables~\citep{cover1999elements}. In this setting, each GPU can observe \emph{at most one} encoded data, hence the mutual information is defined by
\begin{align}
    I(\mathbf x^{(j)} ; \bar{\mathbf x}^{(i)})= h(\mathbf x^{(j)})-h(\mathbf x^{(j)} |\bar{\mathbf x}^{(i)})\qquad j=1,\dots K~.
\end{align}
Here, $ h(\cdot)$ denotes the Shannon entropy function. Note that the information that adversary can potentially learn about $\mathbf x^j$ by having $\bar{\mathbf x}^i$ is fundamentally bounded by  $I(\mathbf x^{(j)} ; \bar{\mathbf x}^{(i)})$. Next, we will rigorously bound this information leakage and show how it can be bounded by properties of the noise.
\begin{thm}\label{thm:info_leakage}
Assume that $X^1,\dots,X^K$ are scalars such that $|X^i|\leq C_1$ for all $i$. Suppose $\alpha_{i,j}$'s are real non-zero scalars and $R$ denotes a Gaussian random variable with variance $\sigma^2$. Also $\bar X$ is defined as
\begin{align}
    \bar X=\sum_{j=1}^K \alpha_{j} X^j + \alpha_{(K+1)} R~.
\end{align}
Then the information leaked from $\bar X$ about $X^j$ is bounded by
\begin{align}\label{eq:infor_bound1}
    I\left(X^j ; \bar X\right)\leq \frac{K C_1^2\bar\alpha^2}{2\underset{\bar{}}{\alpha}^2\sigma^2}~,\quad j=1,\dots, K~.
\end{align}
Here $\bar\alpha=\max_{i,j}|{\alpha_{i,j}}|$ and $\underset{\bar{}}{\alpha}=\min_{i,j}| \alpha_{i,j}|$.
\end{thm}
\begin{proof}
% Since each GPU can only observer one equation, it's enough to measure the mutual information between the encoded image that GPU
% $i$ receives and each raw input. To prove \eqref{eq:infor_bound1}, first note that for each GPU $i$ we have:
% \begin{align}
%     &I\left(X^1,\dots X^K; \bar X^i\right)\leq \max_{j} I\left(X^j ; \bar X^i\right)~.
% \end{align}
% Now if we conclude the proof by just showing that {\color {red}$I(X^j;\bar X^i)\leq \frac{K^2~C_1^2\bar{\alpha}^2}{\underset{\bar{}}{\alpha}^2\sigma^2}$} .
Since $\alpha_{i,j}$'s are non-zero, we have
\begin{align}\label{eq:initial_bound}
    &I\left(X^j;\bar X\right)=I\left(\alpha_{j}X^j;\bar X\right)\nonumber\\
    &\overset{\mathrm{(1)}}{=}I\left(\alpha_{j}X^j;\sum_{l=1}^K \alpha_{l} X^l + \alpha_{(K+1)} R\right)\nonumber\\
    &\overset{\mathrm{(2)}}{=}H\left(\sum_{l=1}^K \alpha_{l} X^l + \alpha_{(K+1)} R\right) \nonumber \\ &- H\left(\sum_{\substack{l=1\\l\neq j}}^K \alpha_{l} X^l + \alpha_{(K+1)} R\right)\nonumber\\
    &\overset{\mathrm{(3)}}{\leq}H\left(\sum_{l=1}^K \alpha_{l} X^l + \alpha_{(K+1)} R\right)- H\left(\alpha_{(K+1)} R\right)\nonumber\\
    &=I\left( \sum_{l=1}^K \alpha_{l} X^l ; \sum_{l=1}^K \alpha_{l} X^l  + \alpha_{(K+1)} R\right)~.
\end{align}
Here, for equality (1), we simply replace $\bar X^i$ with its definition. (2) is due to the definition of the mutual information ( $I(X;X+Y)=H(X+Y)-H(Y)$). Finally, inequality (3) holds due to Lemma \ref{lemma:indep_ineq}. \\
Now, note that since $|X^l|\leq C_1$, we have
\begin{align}
    \text{Var}\left( \sum_{l=1}^K \alpha_{l} X^l \right)=\sum_{l=1}^K \text{Var}\left( \alpha_{l} X^l \right)\leq K\bar\alpha^2 C_1^2
    % \left|\sum_{l=1}^K \alpha_{l} X^l \right|\leq C_1\sum_{l=1}^K \left|\alpha_{l}\right|\leq K~C_1 ~\max_{l} |\alpha_{l}|
\end{align}
Also $\alpha_{(K+1)}R$ is a zero-mean Gaussian random variable with variance $\alpha_{(K+1)}^2\sigma^2$. Therefore, using Lemma \ref{lemma:bound_sum}, we have
\begin{align}\label{eq:bound_lastpasrt}
    &I\left( \sum_{l=1}^K \alpha_{l} X^l ; \sum_{l=1}^K \alpha_{l} X^l  + \alpha_{(K+1)} R\right)&\leq\nonumber\\ &\frac{\text{Var}\left( \sum_{l=1}^K \alpha_{l} X^l \right)}{2\alpha_{(K+1)}^2\sigma^2}\leq \frac{KC_1^2\bar{\alpha}^2}{2\underset{\bar{}}{\alpha}^2\sigma^2}
\end{align}
%uniform random variable in $[-\alpha_{(K+1),i}C_2,\alpha_{(K+1),i}C_2]$. %Now we use Lemma 6 in \cite{guo2020secure}, which yields
%\begin{align}\label{eq:upperbnd_first_term}
 %   H\left(\sum_{l=1}^K \alpha_{l,i} X^l + \alpha_{(K+1),i} R\right)\leq \frac{K C_1\bar\alpha}{\alpha_{(K+1),i}}+\text{log}_2(2\alpha_{(K+1),i}C_2)\leq\frac{K C_1\bar\alpha}{\underset{\bar{}}{\alpha}}+\text{log}_2(2\alpha_{(K+1),i}C_2)~.
%\end{align}
%Besides, since $\alpha_{(K+1),i}R$ is  uniform random variable in $[-\alpha_{(K+1),i}C_2,\alpha_{(K+1),i}C_2]$, we have
%\begin{align}\label{eq:unif_entropy}
    %H\left(\alpha_{(K+1),i} %R\right)=\text{log}_2(2\alpha_{(K+1),i}C_2)
%\end{align}
Finally, using \eqref{eq:initial_bound}, \eqref{eq:bound_lastpasrt}, we conclude that 
\begin{align}\label{eq:final_bound}
    I\left(X^j;\bar X\right)\leq\frac{KC_1^2\bar{\alpha}^2}{2\underset{\bar{}}{\alpha}^2\sigma^2}
\end{align}
% Combining \eqref{eq:infor_bound1} and \eqref{eq:final_bound}, yields our result,
% \begin{align}
%     &I\left(X^1,\dots,X^K ; \bar X^i\right)\leq  \max_{j} I\left(X^j;\bar X^i\right)\leq \nonumber\\&\frac{K^2 C_1^2\bar\alpha^2}{\underset{\bar{}}{\alpha}^2\sigma^2}
% \end{align}
\end{proof}
\begin{lemma}\label{lemma:indep_ineq}
Suppose that $X$ and $Y$ are two independent random variables. Then we have,
\begin{align}
\max \left\{ H(X),H(Y) \right\}   \leq H(X+Y)~.
\end{align}
\end{lemma}
\begin{proof}
Since $X$ and $Y$ are independent, we have $H(X+Y | X)=H(Y|X)$ and $H(Y|X)=H(Y)$. Therefore,
\begin{align}
    H(X+Y)\geq H(X+Y|X)=H(Y|X)=H(Y)~.
\end{align}
The same argument shows that $H(X+Y)\geq H(X)$, which concludes the proof.
\end{proof}
\begin{lemma}\label{lemma:bound_sum}
Assume that $X_i\sim P_{X_i}$ is a random variable, and $R_i\sim \mathcal N(0,\sigma_i^2)$ is a Gaussian random variable with variance $\sigma^2$ and mean $0$. Also, assume that $X_i$s and $R_i$s are independent. Then we have,
\begin{align}
    &I(X^1, X^2,..., X^n;X^1+R^1,X^2+R^2,...,X^k+R^n)  \nonumber\\ & \quad \leq\sum_{i=1}^N \frac 1 2 \log\left(1+\frac{\text{Var}(X^i)}{\sigma^2_{i}}\right)\leq \sum_{i=1}^N  \frac{\text{Var}(X^i)}{2\sigma^2_{i}},
\end{align}
where $\text{Var}(X^i)$ is variance of the random variable $X^i$.
\end{lemma}

Please refer to section 9.4 of~\cite{cover1999elements} for the detailed proof of Lemma~\eqref{lemma:bound_sum}.


Theorem \ref{thm:info_leakage} shows that by increasing the power of the noise, one can arbitrarily reduce the leaked information. Please note that for deep learning applications normalization is common in the prepossessing phase. Furthermore, many of the networks such as MobileNet and ResNet variants take advantage of the batch normalization layers. Hence, the value of $C_1$ in the above theorem is bound by $N^{(\frac{-1}{2})}$ in case $\ell_2$ normalization is used (which obviously implies $C_1 \leq 1$). With a batch size of K = 2, setting variance of the noise, $\mathbf r$, to be $\sigma^2=4e^8$, and limiting $\frac{\bar\alpha^2}{\underset{\bar{}} {\alpha^2}} < 10$, we have the upper bound of $5e^{-8}$ on the leaked information, %through Theorem \ref{thm:info_leakage}. This means that in a Megapixel image, the maximum information leakage is bounded by 1 pixel. 
Because our amount of leakage is less thank the precision loss(round off error) in IEEE single-precision arithmetic, we achieve perfect privacy; meaning that the amount of data leakage is less than the accuracy loss due to round off error~\citep{guo2020secure}.

\begin{comment}

In Table \ref{tab:MI}, we empirically computed the amount of information leakage for a few example networks. As you see, by choosing a sufficiently large mean and variance for the noise, one can decrease the mutual information between a vector $\mathbf x_i$ and $\bar{\mathbf x}_i $ as desired.\\
%\textbf{Structural Similarity Index Measure (SSIM)} is another measure privacy performance measure. SSIM indicated the similarity between two images and ranges from $0$ to $1$.
%We used two information theory metrics to show the amount of leaked information. \\
%\textbf{Mutual Information(MI)}: measures the mutual dependencies between two random variables (original image X and Encode image Y). 
%$I(X;Y) = \sum\limits_{y \in Y} \sum\limits_{x\in X} Pr(x,y) * log_2(\frac {Pr(x,y)}{Pr(x) Pr(y)})$\\
%$H(X) = H(X|Y) + I(X;Y)$
%in the above formula $I$ denotes the mutual information. $H$ defines the Shannon entropy. 
\begin{table}[htb]
\caption{A. Average mutual information between original image and encoded image. B. Average SSIM for ImageNet validation set with $\mathbb N(5e4, 1e14)$}
\label{tab:MI}
\begin{tabular}{c|c|c|c}
Dataset & Noise       & $H(X)$ & $I(X;\bar{X})$\\ \hline \hline
CIFAR10 & $\mathcal N(1e5, 1e10)$ & 4.97& 0.021 \\
CIFAR10 & $\mathcal N(5e4, 1e14)$ &  4.97 &     0.039   \\
CIFAR100 & $\mathcal N(1e5, 1e10)$ &  4.97 &     0.018   \\
\end{tabular}
\end{table}
\end{comment}
\begin{comment}
\subsection{Approach 2: Finite Field}
Many privacy preserving schemes take advantage of Finite Fields~\citep{so2020byzantine,tramer2018slalom, mishra2020delphi,mohassel2017secureml}. 
The first step of transforming all the operations to Field is to map all the values in $\mathbb R$ to field $\mathbb F$ using a quantization scheme. Quantized DNNs have been widely studied and different mechanisms with high inaccuracies are proposed in recent years in the works such as ~\citep{hubara2017quantized, zhou2016dorefa, gupta2015deep, galloway2017attacking, panda2019discretization}.\\

\textbf{Quantization}:\\
All the operations are computed over Field $\mathbb F$ where $P$ are the largest prime number smaller than the hardware budget for 32-bit operations. To enable that, all the input and parameter values should be quantized first. We used stochastic rounding proposed in~\citep{gupta2015deep}. The number of precision bits indicates by parameter $l$. Before offloading $ \mathbf x_k$ to the GPU, function $quantize(\mathbf x_k)$ is called in which first $ \mathbf x_k 2^l$ is computed and then $stochastic\_rounding$ function will map this value to the field $\mathbb F_P$. Stochastic rounding is defined in equation \ref{eq:rounding}. We used stochastic rounding introduced in~\citep{gupta2015deep}. As shown in \ref{eq:rounding}, $\mathbf x$ rounds to $\lfloor \mathbf x \rfloor$ with the probability that is proportional to the proximity of $\mathbf x$ and $\lfloor \mathbf x \rfloor$. Please note $\eps = 2^{-l}$ which is the precision unit. $\mathbf W$ and $\mathbf b$ go through the same process. The main steps are shown below:\\

 
\begin{equation}
    \label{eq:rounding}
    stochastic\_rounding(\mathbf x) = \begin{cases}  \lfloor \mathbf x \rfloor & w.p. \quad 1-(\mathbf{x}-\lfloor \mathbf x\rfloor)\\
    
    \lfloor \mathbf x \rfloor +1 \quad & w.p. \quad (\mathbf{x}-\lfloor \mathbf x\rfloor)
    \end{cases}
\end{equation}
$\mathbf W$ and $\mathbf b$ go through the same process. The main steps are shown below:
\begin{algorithm}
\caption{Quantization and linear operation computation}\label{alg:back}
\begin{algorithmic}[1]
\Procedure{Linear Operation Procedure}{$\mathbf{W}$, $\mathbf{b}$, $\mathbf{x}$}\Comment{gets current weights, bias, and input values}
        \State $ \bar{\mathbf{W}} \gets Stochastic\_rounding(  \mathbf{W}*2^L)$ 
        \State $ \bar{\mathbf{b}} \gets Stochastic\_rounding(  \mathbf{b}*2^{2L})$
	   \For {$k=1,2,\ldots,K$}\Comment{for each input batch}
        \State $\bar{\mathbf x}_k \gets Encoding(  {\mathbf x_k}*2^L)$ \Comment{Encdoing the input}
        \State $\bar{\mathbf x}_k \gets stochastic\_rounding(\bar{\mathbf x}_k)$
        \Comment{Rounding the input}
        \State GPU computes $\bar{\mathbf y}_k \gets \langle \bar{\mathbf x}_k, \bar{\mathbf{W}}\rangle$\Comment{Linear Operations on GPU}
        \State $ \mathbf y_k \gets stochastic\_rounding({\mathbf y}_k*2^{-L})*2^{-L}$
        \State $ \mathbf y_k \gets Decoding(\bar{\mathbf y}_k)$\Comment{Decoding Linear Operation result}
        \EndFor
\State \textbf{return} $\mathbf Y$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Privacy Guarantee Proof}
\begin{thm}\label{thm_thm1}
From Adversary's point of view, all the encoded hidden feature maps are randomly selected from a uniform distribution. Hence, they are computationally indistinguishable.
\end{thm}
\begin{proof} Assume that we are encrypting the vectors $\mathbf x^{(1)}, ...., x^{(K)} \in \mathbb Z_P$, using Darknight. Thus, we will be sending $\bar{\mathbf x}^{(i)}= \alpha_{i,1} \mathbf{x}^{(1)} + \dots+ \alpha_{i,K} \mathbf{x}^{(K)}  +\alpha_{i,(K+1)} \mathbf{r}$, $i=1:K+1$ to the untrusted hardware. Here, $\mathbf r$ is a noise vector whose entries are uniformly chosen over the field $\mathbb{F}_P$, and $\alpha_i$'s are uniformly chosen scalars over the field $\mathbb{F}_p$. Since $\alpha_i$'s are independently chosen over the field with a uniform distribution, the entries of the vectors $\alpha_i\mathbf r$ will be uniform over the field $\mathbb F_p$. Thus, the vector $\bar{\mathbf{x}}_i$ will be simply a \textit{one-time-pad} encryption of $\alpha_{i,1} \mathbf{x}^{(1)} + \dots+ \alpha_{i,K} \mathbf{x}^{(K)}$ over the field $\mathbb{F}_p$. Therefore, from the adversary's perspective, the vectors $\bar{\mathbf{x}}_i$ are uniformly selected from distribution $[0..P]$ and hence, will be computationally indistinguishable. 
\end{proof}
\end{comment}
\begin{comment}
\begin{figure*}[htbp]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/VGG16-Cifar10-Training-Quantized.pdf}
     \caption{CIFAR-10 on VGG16}
     \label{fig:VGG16CIFAR}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/VGG16-Cifar100-Training-Quantized.pdf}
    \caption{CIFAR-100 on VGG16}%stepsize=60, learning rate = 0.01
    \label{fig:VGG16CIFAR100}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/ResNet152-Cifar10-Training-Quantized.pdf}
    \caption{CIFAR-10 on ResNet152}
    %, learning rate = 0.01
    \label{fig:VGG19Cifar10}
  \end{subfigure}
  
    \begin{subfigure}[b]{0.30\linewidth}
    \includegraphics[width=\linewidth]{Figures/ResNet152-Cifar100-Training-Quantized.pdf}
    \caption{CIFAR-100 on ResNet152}%stepsize=60, learning rate = 0.01
    \label{fig:VGG19CIFAR100}
  \end{subfigure}
    \begin{subfigure}[b]{0.30\linewidth}
    \includegraphics[width=\linewidth]{Figures/Mobile-Cifar10-Training-Quantized.pdf}
    \caption{CIFAR-10 on MobileNetV2}%Stepsize = 200, learning rate = 0.001
    \label{fig:mobilenetACC}
  \end{subfigure}
   \begin{subfigure}[b]{0.30\linewidth}
    \includegraphics[width=\linewidth]{Figures/Mobile-Cifar100-Training-Quantized.pdf}
    \caption{CIFAR-100 on MobileNetV2}%stepsize = 60
    \label{fig:mobilenetACC100}
  \end{subfigure}
  \caption{Training accuracy of DarKnight in different DNNs and datasets for the first 200 epochs and batch-size = 128}
  \label{fig:training}
\end{figure*}
\end{comment}
