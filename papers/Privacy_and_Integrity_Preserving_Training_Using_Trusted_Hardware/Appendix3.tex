\section{Experimental Setup and Results}
\label{sec:experiments}
\begin{figure*}[tbp]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/VGG16-Cifar100-Training.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/Resnet_Cifar100_Training.pdf}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/Mobile_Cifar100_Training.pdf}
  \end{subfigure}
 %\vskip -0.15in
  \caption{Training accuracy of DarKnight for CIFAR-100 with (a) VGG16 (b) ResNet152 (c) MobileNetV2}
  \label{fig:training}
\end{figure*}
DarKnight server consisted of an Intel Coffee Lake E-2174G 3.80GHz processor and Nvidia GeForce GTX 1080 Ti GPUs.
The server has 64 GB RAM and supports Intel Soft Guard Extensions (SGX).
DarKnight's training scheme and the related unique coding requirements are implemented as an SGX enclave thread where both the decoding and encoding are performed. For SGX implementations, we used Intel Deep Neural Network Library (DNNL) for designing the DNN layers including the Convolution layer, ReLU, MaxPooling, and Eigen library for Dense layer. We used Keras 2.1.5, Tenseflow 1.8.0, and Python 3.6.8. %These are well-established libraries and have been used in prior works~\cite{tramer2018slalom}.  


We used three different DNN models: VGG16~\citep{simonyan2014very}, ResNet152~\citep{he2016deep} and, MobileNetV2~\citep{sandler2018mobilenetv2}. We chose MobileNetV2 because it is the worst-case benchmark for our model as it reduces linear operations considerably (using depth-wise separable convolution), thereby reducing the need for GPU acceleration. %MobileNetV1 replaces standard convolution with depth-wise separable convolution to substantially reduce linear operations which enables execution on small devices. In MobilenetV2 they even make it faster by reducing the number of channels. 
We used ImageNet~\citep{russakovsky2015imagenet}, CIFAR-10 and CIFAR-100~\citep{krizhevsky2009learning} as our datasets. 
%We used CIFAR-10 \cite{krizhevsky2009learning} that has 50,000 training images evenly distributed between 10 categories,  CIFAR-100 \cite{krizhevsky2009learning} that has 60,000 images under 100 classes, and ImageNet \cite{russakovsky2015imagenet} with about 1.2 million images and 1000 categories.
All the parameters, models' and implementation details, and dataset descriptions are attached in the supplementary material. 



\subsection{Training Results}
For evaluating training performance, three aspects are examined: accuracy impact, speed up of training, and maximum information leakage. 

\textbf{Effect of Random Noise on Accuracy}: Adding large noise to inputs to encode the data may cause floating-point rounding errors on GPUs. To study the impact, Fig.~\ref{fig:training} shows the training accuracy when using different noise strengths on VGG16, ResNet152, and MobileNetV2. 
We use a random Gaussian vector with iid entries, $\mathcal N(\mu,\sigma^2)$, as the noise vectors $\mathbf r_i$'s, where $\sigma^2$ is the order of magnitude strength over the typical input and feature map values seen in a model. For instance, $\mathcal N(5e^4,1e^7)$ means the noise is drawn from a distribution with the mean at $5e^4$ and variance of $1e^7$. Figure~\ref{fig:training} (a) shows the accuracy of training for VGG16 on CIFAR-100 dataset. Even with a powerful noise signal ($\sigma^2=e^8$), the accuracy loss after epoch $50$ is less than $0.001$ compared to training on open data without any privacy controls. Very similar behavior is observed across a wide range of input datasets and models.  

\begin{table*}[htbp]
\caption{Effect of different noise signals on the accuracy of DarKnight inference for different models on ImageNet}
 \vskip -0.1in
\label{tab:inferenceAcc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccc}
            \hline
            \hline
            & \multicolumn{2}{c}{VGG16}   & \multicolumn{2}{c}{ResNet152} & \multicolumn{2}{c}{MobileNetV1} & {All Models} \\ \hline

Noise       & Top1 Accuracy & Top5 Accuracy & Top1 Accuracy & Top5 Accuracy& Top1 Accuracy& Top5 Accuracy & \textbf{MI upper bound}\\ \hline

No privacy       & 64.26 & 85.01 &  72.93  &  90.60     &   64.96    &   85.29 &   --  \\
$\mathcal N(4e3, 1.6e7)$ & 64.23&  85.01    &  72.46 &  90.47  &   64.99 &   85.26 &  
$1.25*10^{-6}$ \\
$\mathcal N(1e4, 2.5e7)$ & 64.25&85.06& 72.35  & 90.23 &64.81 & 85.26 & $0.8*10^{-6}$\\
$\mathcal N(1e4, 1e8)$ &  64.25& 85.05 &  71.87 &  89.93  & 64.54   &  85.15 & $2*10^{-7}$ \\
$\mathcal N(0, 4e8)$ &  64.24 &  85.01&  72.24 &  90.09  & 64.87  & 85.19 & $\mathbf{5*10^{-8}}$ \\
%$\mathcal N(0, 9e8)$ &  64.22 &  85.02& 70.78  &  89.33  & 64.41  & 84.87 & {$2.2*10^{-8}$} \\
 \hline
\end{tabular}
}
 \vskip -0.1in
\end{table*}







\textbf{Information Leakage and Mutual Information}: %We use a random Gaussian vector with iid entries, $\mathcal N(\mu,\sigma^2)$, as the noise vectors $ \mathbf r_i$'s, where $\sigma^2$ is the order of magnitude strength over the typical model parameter values seen in a model. In 
Table~\ref{tab:inferenceAcc} show accuracy impact of  various noise strengths, on the inference accuracy. For noise strengths that have 7 orders of magnitude higher variance than the input signal, negligible accuracy losses were observed. When the noise strength reaches 8 orders of magnitude ResNet152 seems a worst-case  Top1 accuracy drop of about 1\%. The last column represents the upper bound of mutual information computed from Theorem \ref{thm:info_leakage}. By limiting $\frac{\bar\alpha^2}{\underset{\bar{}} {\alpha^2}} < 10$ for $K=2$ when using $\mathcal N(0, 4e8)$, we have $5\times 10^{-8}$ upper bound on the information leakage which is less than the roundoff error in IEEE single-precision arithmetic and hence, perfect privacy is achieved with this precision.  
%This selection of encoding parameters causes no accuracy loss in VGG16 and MobileNetV1, and about 1\% degradation in Top 1 accuracy, and 0.5\% loss in Top 5 accuracy in ResNet152. 





 
 


