%\vspace{-3mm}
\section{DarKnight}
\label{sec:model}
\textbf{System Structure}: Our system model for learning is shown in Figure~\ref{fig:model1}. %The cloud server is equipped with TEE which is responsible for encoding the data and performing non-linear operations. The TEE hardware guarantees code verification and authentication.
We show $K'$ GPU accelerators that participate in linear computations ($\text{GPU}_1, \text{GPU}_{K'}$) on data that is encoded in the TEE. In this work we use Intel SGX as our TEE.

%We assume that the communication between the TEE  and GPUs are encrypted. A pairwise secure channel between TEE and each GPU can be established using a secret key exchange using Diffie-Hellman~\cite{steiner1996diffie} protocol at the beginning of the session. All the messages that leave the TEE are encrypted with the appropriate secret key. 

%Input privacy on the accelerators is guaranteed by our proposed encoding scheme, which obfuscates the original input. Each GPU receives \emph{at most one} encoded data. The amount of leaked information from encoded data to accelerator GPUs has a rigorous upper bound. This upper bound itself can be controlled by the power of noise and other encoding parameters in our design. In our implementation, we selected these parameters such that the overall training or inference accuracy is not reduced due to computations on noise encoded data. In section~\ref{sec:guarantee} and Appendix~\ref{ap:pri}, we provide the details of our theoretical analysis.


\textbf{Threat Model:} %While adversaries can perform various attacks on DNN models and datasets~\citep{riazi2019deep}, DarKnight focuses on attacks that expose the datasets used in training or inference and attacks that modify computational results on untrusted hardware. Side-channel attacks are out of the scope of this work. 
%Within this scope, the adversary is assumed to have full root access to the parameter server, which includes the GPU in our setup.
%The adversary cannot see any computations or data stored within the TEE. But the adversary has unrestricted access to data that leaves TEE, such as the encoded input data, and can alter computational results performed on the GPU. 
The threat model on the server-side is a dynamic malicious adversary. %This means at any given time, accelerator GPUs may try to glean private information from the data shared with them by the TEE. 
Whenever GPUs receive data from TEE, they may use known techniques to extract information about the original data or inject faults in the computation. Moreover, a subset of \emph{colluding GPUs} may try to extract information by collaborating with each other or inject faults to sabotage the training. %We refer to them as \emph{colluding GPUs}. Since the GPUs can be malicious, they may also inject faults in the computation to sabotage training or inference. 
%We assume that a subset of GPUs can collude. %up to half of the GPUs can collude. 
%Such a scenario is quite common where GPUs assigned may be located in geographically different data centers~\cite{CITE THIS: www.digitialocean.com}, or when a subset of servers may be compromised by hackers. Since the GPUs are malicious they may also inject faults in the computation. Thus, computational integrity will be explicitly verified by \name in the presence of such an adversarial capability assumption. 
In a system with $K'$ accelerator GPUs, DarKnight provides: 

\textbf{Data Privacy:} DarKnight provides perfect privacy with IEEE single-precision arithmetic. In Floating-Point (FP) arithmetic, perfect privacy at a given precision is when the information leakage between encoded data and raw data is less than the round off error. Namely, $I(X:X') < FP.precision$, where I is the mutual information~\citep{cover1999elements, guo2020secure}. 

\textbf{Integrity:} DarKnight is (K'-1)-secure, namely it can detect any malicious computation even if K'-1 GPUs send erroneous results to TEE. 

\textbf{Collusion Tolerance:} DarKnight provide perfect privacy \textbf{and} integrity  when $M$ GPUs collude, where $M$ is a function of K' and the number of inputs that can be encoded, as described later. 
%\textbf{Model Privacy:} Our system does not reveal anything about the model to the client. Model privacy on the server-side is out of the scope of this work.
%Furthermore, we can tolerate $M$ colluding GPUs collaborating to extract information, while keeping the dataset private.
\begin{comment}
\subsection{Privacy Overview} 
To prevent leakage, the cloud server uses a TEE-based code enclave. The TEE hardware guarantees code verification and authentication. An enclave code can be verified by the client. In particular, the client can verify what computations are performed by the enclave on the data it receives, and what data the enclave may communicate to the outside world such as the accelerator GPUs. After verification, the client can also authenticate that the enclave code that runs at any instance is the same code that was verified and any deviation will be notified to the client. Note that these are basic properties of a hardware-supported TEE and there is nothing unique either the client or the cloud server may need to perform to achieve these guarantees. %Since model protection is outside of the scope, we assume the adversary can access the DNN model parameters.

We assume that the communication between the TEE  and GPUs are encrypted. A pairwise secure channel between TEE and each GPU can be established using a secret key exchange using Diffie-Hellman~\cite{steiner1996diffie} protocol at the beginning of the session.  All the messages that leave the TEE are encrypted with the appropriate secret key. 

Input privacy on the accelerators is guaranteed by our proposed encoding scheme, which obfuscates the original input. Each GPU only receives \emph{one} encoded data. The amount of leaked information from encoded data to accelerator GPUs has a rigorous upper bound. This upper bound itself can be controlled by the power of noise and other encoding parameters in our design. In our implementation, we selected these parameters such that the overall training or inference accuracy is not reduced due to computations on noise encoded data. In section~\ref{sec:guarantee} and Appendix~\ref{ap:pri}, we provide the details of our theoretical analysis.

\textbf{Computation Integrity Overview:}
The malicious GPUs can alter the returned values to the TEE to manipulate model training or inference. DarKnight can verify the computations performed in the unsecured GPUs up to the computation precision. In the other words, DarKnight detects if the results are altered more than the computation precision by an adversary. 
\end{comment}
%\begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}
 \centering
 \includegraphics[width=0.50\textwidth]{Figures/first_fig.png}
 \caption{Forward/backward pass of DarKnight}
 \label{fig:model1}
 \vskip -0.2in
\end{figure}
\subsection{DarKnight Flow}
The initial model ($\mathbf{W}$)  that a user wants to train is loaded into the cloud server and is made accessible to the untrusted GPUs as well. DarKnight then uses the following steps: (1) A batch of training/inference input data set is encrypted by the client using mutually agreed keys with TEE and sent to the server. (2) TEE decrypts the images and starts the encoding process. (3) During the forward/backward pass of training, each layer requires linear and nonlinear operations. The linear operations are compute-intensive and will be offloaded to GPUs.  DarKnight's encoding mechanism is used to seal the data before sending the data to GPU accelerators. To seal the data, DarKnight uses the notion of a \textit{virtual batch}, where $K$ inputs and a random noise are linearly combined to form $K+1$ coded inputs. %This virtual batch may not be the same as the traditional batch size used in machine learning. 
The size of the virtual batch is limited by the size of the TEE memory that is necessary to encode $K$ images, typically 4-8 images at a time.  
(4) The encoded data is offloaded to GPUs for linear operation. Each GPU receives at most one encoded data (5) GPUs perform linear operations on different encoded data sets and return the results to TEE in step (6). The TEE decodes the received computational outputs using DarKnight's decoding strategy and then performs any non-linear operations within the TEE in step (7). This process is repeated both for forward pass and backward propagation of each layer.   
\emph{In a system with $K'$ GPUs and virtual batch size $K$, DarKnight can provide data privacy and computational integrity by tolerating up to $M$ colluding GPUs, where $K+M+1 \leq K'$.}
