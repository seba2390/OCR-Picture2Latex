\documentclass{article}
\usepackage{iclr2021_conference,times}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{amsmath}  
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{multirow}
\usepackage{comment}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[noend]{algpseudocode}
\usepackage{longtable}

\newcommand{\blocka}[2]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{3$\times$3, #1}\\[-.1em] \text{3$\times$3, #1} \end{array}\right]\)$\times$#2}
}
\newcommand{\blockb}[3]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{1$\times$1, #2}\\[-.1em] \text{3$\times$3, #2}\\[-.1em] \text{1$\times$1, #1}\end{array}\right]\)$\times$#3}
}
\renewcommand\arraystretch{1.1}
%\title{DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks}

\begin{document}

%\maketitle
\section*{Datasets, Parameters, and Models}
%\subsection*{Training Parameters}
\subsection*{Datasets}
We used CIFAR-10~\citep{krizhevsky2009learning} that has 50,000 training $32\times32$ images evenly distributed between 10 categories and 10,000 test images, CIFAR-100~\citep{krizhevsky2009learning} has the same number of images as CIFAR-10 but the images are under 100 classes. The largest dataset we used for inference is ImageNet~\citep{russakovsky2015imagenet} with about 1.2 million training images, 50,000 validation images, and 100,000 test images that are distributed into 1000 object classes.

For all the aforementioned datasets the preprocessing phase consists of three steps summarized in the table below:

\begin{table}[h]
\caption{Preprocessing Parameters}
\label{tab:preprocessing}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c}
\hline
\hline
\multirow{3}{*}{\textbf{CIFAR-10}}  & Horizontal Flip with P=0.5                                                                       \\ \cline{2-2} 
                                    & Images are cropped at a random place                                                             \\ \cline{2-2} 
                                    & Normalization (three channels) mean = {[}0.485, 0.456, 0.406{]}, std = {[}0.229, 0.224, 0.225{]} \\ \hline
\multirow{3}{*}{\textbf{CIFAR-100}} & Horizontal flip with P=0.5                                                                       \\ \cline{2-2} 
                                    & Images are cropped at a random place                                                             \\ \cline{2-2} 
                                    & Normalization (three channels) mean={[}0.507, 0.487, 0.441{]}, std={[}0.267, 0.256,0.276{]}      \\ \hline
\multirow{3}{*}{\textbf{ImageNet}}  & Horizontal flip with P=0.5                                                                       \\ \cline{2-2} 
                                    & Images are cropped at a random place                                                             \\ \cline{2-2} 
                                    & Normalization (three channels) mean={[}0.485, 0.456, 0.406{]}, std={[}0.229, 0.224, 0.225{]} \\  
\hline
\end{tabular}
}
\end{table}

\subsection*{Training Parameters}
For the three dataset and three models we used the training parameters are reported in the table \ref{tab:trainingpar}.
\begin{table}[h]
\caption{Training Parameters}
\label{tab:trainingpar}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccc}
\hline
\hline
                               & \multicolumn{2}{c}{VGG16}                         & \multicolumn{2}{c}{ResNet152, ResNet50}                         & \multicolumn{2}{c}{MobileNetV2}                   \\ \hline
Parameters                     & CIFAR-10                & CIFAR-100               & CIFAR-10                & CIFAR-100               & CIFAR-10                & CIFAR-100               \\ \hline
Batch-size & 128 &128 & 128 & 128 & 128 & 128 \\ \hline
Learning Rate & 0.01 & 0.01                    & 0.01                    & 0.01                    & 0.01                    & 0.01                    \\ \hline
Stepsize                       & 200                     & 60                      & 200                     & 30                      & 200                     & 60                      \\ \hline
lambda                         & 0.1                     & 0.02                    & 0                       & 0.02                    & 0.1                     & 0.2                     \\ \hline
Momentum                       & 0                       & 0.9                     & 0                       & 0.9                     & 0                       & 0.9                     \\ \hline
Weight Decay                   & 0                       & 0.0005                  & 0                       & 0.0005                  & 0                       & 0.0005                  \\ \hline
Dropout                        & 0.5                     & 0.5                     & 0.5                     & 0.5                     & 0.2                     & 0.2                     \\ \hline
\end{tabular}
}
\end{table}

For inference implementations the pretrained models with the above are used. 

\subsection*{Models}
We used different models for our experiments. VGG16~\citep{simonyan2014very}, ResNet152, ResNet50~\citep{he2016deep} are chosen because of their enormous parameter size. However, MobileNetV2~\citep{sandler2018mobilenetv2,howard2017mobilenets} is implemented and analyzed because it is the worst-case benchmark for our model as it reduces its linear operations considerably.

Model details including the description, output shape and number of parameters are reported in table \ref{tab:vgg16model}, \ref{tab:arch}, \ref{tab:MobileNetV2model}.
\begin{center}
\begin{longtable}[h]{ p{.60\textwidth}  p{.30\textwidth}  p{.10\textwidth}}
\caption{VGG16 Architecture}\\
%\resizebox{0.80\textwidth}{!}{%
%\begin{tabular}{lll}
\hline
\hline
Layer (type)                  & Output Shape          & Param \#  \\ \hline
input\_4 (InputLayer)         & (None, 224, 224, 3)   & 0         \\ \hline
block1\_conv1 (Conv2D)        & (None, 224, 224, 64)  & 1792      \\ \hline
block1\_conv2 (Conv2D)        & (None, 224, 224, 64)  & 36928     \\ \hline
block1\_pool (MaxPooling2D)   & (None, 112, 112, 64)  & 0         \\ \hline
block2\_conv1 (Conv2D)        & (None, 112, 112, 128) & 73856     \\ \hline
block2\_conv2 (Conv2D)        & (None, 112, 112, 128) & 147584    \\ \hline
block2\_pool (MaxPooling2D)   & (None, 56, 56, 128)   & 0         \\ \hline
block3\_conv1 (Conv2D)        & (None, 56, 56, 256)   & 295168    \\ \hline
block3\_conv2 (Conv2D)        & (None, 56, 56, 256)   & 590080    \\ \hline
block3\_conv3 (Conv2D)        & (None, 56, 56, 256)   & 590080    \\ \hline
block3\_pool (MaxPooling2D)   & (None, 28, 28, 256)   & 0         \\ \hline
block4\_conv1 (Conv2D)        & (None, 28, 28, 512)   & 1180160   \\ \hline
block4\_conv2 (Conv2D)        & (None, 28, 28, 512)   & 2359808   \\ \hline
block4\_conv3 (Conv2D)        & (None, 28, 28, 512)   & 2359808   \\ \hline
block4\_pool (MaxPooling2D)   & (None, 14, 14, 512)   & 0         \\ \hline
block5\_conv1 (Conv2D)        & (None, 14, 14, 512)   & 2359808   \\ \hline
block5\_conv2 (Conv2D)        & (None, 14, 14, 512)   & 2359808   \\ \hline
block5\_conv3 (Conv2D)        & (None, 14, 14, 512)   & 2359808   \\ \hline
block5\_pool (MaxPooling2D)   & (None, 7, 7, 512)     & 0         \\ \hline
flatten (Flatten)             & (None, 25088)         & 0         \\ \hline
fc1 (Dense)                   & (None, 4096)          & 102764544 \\ \hline
fc2 (Dense)                   & (None, 4096)          & 16781312  \\ \hline
predictions (Dense)           & (None, 1000)          & 4097000   \\ \hline
Total params: 138,357,544     &                       &           \\
Trainable params: 138,357,544 &                       &           \\
Non-trainable params: 0       &                       &          
\label{tab:vgg16model}
\end{longtable}
\end{center}

\begin{table}[h]
\begin{center}
\caption{Architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed by conv3\_1, conv4\_1, and conv5\_1 with a stride of 2~\citep{he2016deep}
}
\label{tab:arch}
\resizebox{\linewidth}{!}{
%\footnotesize
\begin{tabular}{c|c|c|c|c|c|c}
\hline
layer name & output size & 18-layer & 34-layer & 50-layer & 101-layer & 152-layer \\
\hline
conv1 & 112$\times$112 & \multicolumn{5}{c}{7$\times$7, 64, stride 2}\\
\hline
\multirow{4}{*}{conv2\_x} & \multirow{4}{*}{56$\times$56} & \multicolumn{5}{c}{3$\times$3 max pool, stride 2} \\\cline{3-7}
  &  & \blocka{64}{2}  & \blocka{64}{3} & \blockb{256}{64}{3} & \blockb{256}{64}{3} & \blockb{256}{64}{3}\\
  &  &  &  &  &  &\\
  &  &  &  &  &  &\\
\hline
\multirow{3}{*}{conv3\_x} &  \multirow{3}{*}{28$\times$28}  & \blocka{128}{2}  & \blocka{128}{4}  & \blockb{512}{128}{4}  & \blockb{512}{128}{4}  &
                              \blockb{512}{128}{8}\\
  &  &  &  &  &  & \\
  &  &  &  &  &  & \\
\hline
\multirow{3}{*}{conv4\_x} & \multirow{3}{*}{14$\times$14}  & \blocka{256}{2}  & \blocka{256}{6}  & \blockb{1024}{256}{6}  & \blockb{1024}{256}{23} & \blockb{1024}{256}{36}\\
  &  &  &  &  & \\
  &  &  &  &  & \\
\hline
\multirow{3}{*}{conv5\_x} & \multirow{3}{*}{7$\times$7}  & \blocka{512}{2}  & \blocka{512}{3}  & \blockb{2048}{512}{3}  & \blockb{2048}{512}{3}
& \blockb{2048}{512}{3}\\
  &  &  &  &  &  & \\
  &  &  &  &  &  & \\
\hline
& 1$\times$1  & \multicolumn{5}{c}{average pool, 1000-d fc, softmax} \\
\hline
\multicolumn{2}{c|}{FLOPs} & 1.8$\times10^9$  & 3.6$\times10^9$  & 3.8$\times10^9$  & 7.6$\times10^9$  & 11.3$\times10^9$ \\
\hline
\end{tabular}
}
\end{center}
\end{table}


\begin{center}
%\resizebox{0.80\textwidth}{!}{%
\begin{longtable}[h]{ p{.60\textwidth}  p{.30\textwidth}  p{.10\textwidth}}
\caption{MobileNetV2 Architecture}\\
\hline
\hline
Layer (type)                                       & Output Shape         & Param \# \\ \hline
input\_1 (InputLayer)                              & (None, 224, 224, 3)  & 0        \\ \hline
Conv1\_pad (ZeroPadding2D)                         & (None, 225, 225, 3)  & 0        \\ \hline
Conv1 (Conv2D)                                     & (None, 112, 112, 32) & 864      \\ \hline
%bn\_Conv1 (BatchNormalization)                     & (None, 112, 112, 32) & 128      \\
Conv1\_relu (ReLU)                                 & (None, 112, 112, 32) & 0        \\ \hline
expanded\_conv\_depthwise(DepthwiseConvolution)   & (None, 112, 112, 32) & 288      \\ \hline
%expanded\_conv\_depthwise\_BN (BatchNormalization) & (None, 112, 112, 32) & 128      \\
expanded\_conv\_depthwise\_relu (ReLU)             & (None, 112, 112, 32) & 0        \\ \hline
expanded\_conv\_project (Conv2D)                   & (None, 112, 112, 16) & 512      \\ \hline
%expanded\_conv\_project\_BN (BatchNormalization)   & (None, 112, 112, 16) & 64       \\
block\_1\_expand (Conv2D)                          & (None, 112, 112, 96) & 1536     \\ \hline
%block\_1\_expand\_BN (BatchNormalization)          & (None, 112, 112, 96) & 384      \\
block\_1\_expand\_relu (ReLU)                      & (None, 112, 112, 96) & 0        \\ \hline
block\_1\_pad (ZeroPadding2D)                      & (None, 113, 113, 96) & 0        \\ \hline
block\_1\_depthwise (DepthwiseCon)                  & (None, 56, 56, 96)   & 864      \\ \hline
%block\_1\_depthwise\_BN (BatchNormalization)       & (None, 56, 56, 96)   & 384      \\
block\_1\_depthwise\_relu (ReLU)                   & (None, 56, 56, 96)   & 0        \\ \hline
block\_1\_project (Conv2D)                         & (None, 56, 56, 24)   & 2304     \\ \hline
%block\_1\_project\_BN (BatchNormalization)         & (None, 56, 56, 24)   & 96       \\
block\_2\_expand (Conv2D)                          & (None, 56, 56, 144)  & 3456     \\ \hline
%block\_2\_expand\_BN (BatchNormalization)          & (None, 56, 56, 144)  & 576      \\
block\_2\_expand\_relu (ReLU)                      & (None, 56, 56, 144)  & 0        \\ \hline
block\_2\_depthwise (DepthwiseConvolution)         & (None, 56, 56, 144)  & 1296     \\ \hline
%block\_2\_depthwise\_BN (BatchNormalization)       & (None, 56, 56, 144)  & 576      \\
block\_2\_depthwise\_relu (ReLU)                   & (None, 56, 56, 144)  & 0        \\ \hline
block\_2\_project (Conv2D)                         & (None, 56, 56, 24)   & 3456     \\ \hline
%block\_2\_project\_BN (BatchNormalization)         & (None, 56, 56, 24)   & 96       \\
block\_2\_add (Add)                                & (None, 56, 56, 24)   & 0        \\ \hline
%block\_2\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_3\_expand (Conv2D)                          & (None, 56, 56, 144)  & 3456     \\ \hline
%block\_3\_expand\_BN (BatchNormalization)          & (None, 56, 56, 144)  & 576      \\
block\_3\_expand\_relu (ReLU)                      & (None, 56, 56, 144)  & 0        \\ \hline
block\_3\_pad (ZeroPadding2D)                      & (None, 57, 57, 144)  & 0        \\ \hline
block\_3\_depthwise (DepthwiseConvolution)         & (None, 28, 28, 144)  & 1296     \\ \hline
%block\_3\_depthwise\_BN (BatchNormalization)       & (None, 28, 28, 144)  & 576      \\
block\_3\_depthwise\_relu (ReLU)                   & (None, 28, 28, 144)  & 0        \\ \hline
block\_3\_project (Conv2D)                         & (None, 28, 28, 32)   & 4608     \\ \hline
%block\_3\_project\_BN (BatchNormalization)         & (None, 28, 28, 32)   & 128      \\
block\_4\_expand (Conv2D)                          & (None, 28, 28, 192)  & 6144     \\ \hline
%block\_4\_expand\_BN (BatchNormalization)          & (None, 28, 28, 192)  & 768      \\
block\_4\_expand\_relu (ReLU)                      & (None, 28, 28, 192)  & 0        \\ \hline
block\_4\_depthwise (DepthwiseConvolution)         & (None, 28, 28, 192)  & 1728     \\ \hline
%block\_4\_depthwise\_BN (BatchNormalization)       & (None, 28, 28, 192)  & 768      \\
block\_4\_depthwise\_relu (ReLU)                   & (None, 28, 28, 192)  & 0        \\ \hline
block\_4\_project (Conv2D)                         & (None, 28, 28, 32)   & 6144     \\ \hline
%block\_4\_project\_BN (BatchNormalization)         & (None, 28, 28, 32)   & 128      \\
block\_4\_add (Add)                                & (None, 28, 28, 32)   & 0        \\ \hline
%block\_4\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_5\_expand (Conv2D)                          & (None, 28, 28, 192)  & 6144     \\ \hline
%block\_5\_expand\_BN (BatchNormalization)          & (None, 28, 28, 192)  & 768      \\
block\_5\_expand\_relu (ReLU)                      & (None, 28, 28, 192)  & 0        \\ \hline
block\_5\_depthwise (DepthwiseConvolution)         & (None, 28, 28, 192)  & 1728     \\ \hline
%block\_5\_depthwise\_BN (BatchNormalization)       & (None, 28, 28, 192)  & 768      \\
block\_5\_depthwise\_relu (ReLU)                   & (None, 28, 28, 192)  & 0        \\ \hline
block\_5\_project (Conv2D)                         & (None, 28, 28, 32)   & 6144     \\ \hline
%block\_5\_project\_BN (BatchNormalization)         & (None, 28, 28, 32)   & 128      \\
block\_5\_add (Add)                                & (None, 28, 28, 32)   & 0        \\ \hline
%block\_5\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_6\_expand (Conv2D)                          & (None, 28, 28, 192)  & 6144     \\ \hline
%block\_6\_expand\_BN (BatchNormalization)          & (None, 28, 28, 192)  & 768      \\
block\_6\_expand\_relu (ReLU)                      & (None, 28, 28, 192)  & 0        \\ \hline
block\_6\_pad (ZeroPadding2D)                      & (None, 29, 29, 192)  & 0        \\ \hline
block\_6\_depthwise (DepthwiseConvolution)         & (None, 14, 14, 192)  & 1728     \\ \hline
%block\_6\_depthwise\_BN (BatchNormalization)       & (None, 14, 14, 192)  & 768      \\
block\_6\_depthwise\_relu (ReLU)                   & (None, 14, 14, 192)  & 0        \\ \hline
block\_6\_project (Conv2D)                         & (None, 14, 14, 64)   & 12288    \\ \hline
%block\_6\_project\_BN (BatchNormalization)         & (None, 14, 14, 64)   & 256      \\
block\_7\_expand (Conv2D)                          & (None, 14, 14, 384)  & 24576    \\ \hline
b%lock\_7\_expand\_BN (BatchNormalization)          & (None, 14, 14, 384)  & 1536     \\
block\_7\_expand\_relu (ReLU)                      & (None, 14, 14, 384)  & 0        \\ \hline
block\_7\_depthwise (DepthwiseConvolution)         & (None, 14, 14, 384)  & 3456     \\ \hline
%block\_7\_depthwise\_BN (BatchNormalization)       & (None, 14, 14, 384)  & 1536     \\
block\_7\_depthwise\_relu (ReLU)                   & (None, 14, 14, 384)  & 0        \\ \hline
block\_7\_project (Conv2D)                         & (None, 14, 14, 64)   & 24576    \\ \hline
%block\_7\_project\_BN (BatchNormalization)         & (None, 14, 14, 64)   & 256      \\
block\_7\_add (Add)                                & (None, 14, 14, 64)   & 0        \\ \hline
%block\_7\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_8\_expand (Conv2D)                          & (None, 14, 14, 384)  & 24576    \\ \hline
%block\_8\_expand\_BN (BatchNormalization)          & (None, 14, 14, 384)  & 1536     \\
block\_8\_expand\_relu (ReLU)                      & (None, 14, 14, 384)  & 0        \\ \hline
block\_8\_depthwise (DepthwiseConvolution)         & (None, 14, 14, 384)  & 3456     \\ \hline
%block\_8\_depthwise\_BN (BatchNormalization)       & (None, 14, 14, 384)  & 1536     \\
block\_8\_depthwise\_relu (ReLU)                   & (None, 14, 14, 384)  & 0        \\ \hline
block\_8\_project (Conv2D)                         & (None, 14, 14, 64)   & 24576    \\ \hline
%block\_8\_project\_BN (BatchNormalization)         & (None, 14, 14, 64)   & 256      \\
block\_8\_add (Add)                                & (None, 14, 14, 64)   & 0        \\ \hline
%block\_8\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_9\_expand (Conv2D)                          & (None, 14, 14, 384)  & 24576    \\ \hline
%block\_9\_expand\_BN (BatchNormalization)          & (None, 14, 14, 384)  & 1536     \\
block\_9\_expand\_relu (ReLU)                      & (None, 14, 14, 384)  & 0        \\ \hline
block\_9\_depthwise (DepthwiseConvolution)         & (None, 14, 14, 384)  & 3456     \\ \hline
%block\_9\_depthwise\_BN (BatchNormalization)       & (None, 14, 14, 384)  & 1536     \\
block\_9\_depthwise\_relu (ReLU)                   & (None, 14, 14, 384)  & 0        \\ \hline
block\_9\_project (Conv2D)                         & (None, 14, 14, 64)   & 24576    \\ \hline
%block\_9\_project\_BN (BatchNormalization)         & (None, 14, 14, 64)   & 256      \\
block\_9\_add (Add)                                & (None, 14, 14, 64)   & 0        \\ \hline
%block\_9\_project\_BN{[}0{]}{[}0{]}                &                      &          \\
block\_10\_expand (Conv2D)                         & (None, 14, 14, 384)  & 24576    \\ \hline
%block\_10\_expand\_BN (BatchNormalization)         & (None, 14, 14, 384)  & 1536     \\
block\_10\_expand\_relu (ReLU)                     & (None, 14, 14, 384)  & 0        \\ \hline
block\_10\_depthwise (DepthwiseConvolution)        & (None, 14, 14, 384)  & 3456     \\ \hline
%block\_10\_depthwise\_BN (BatchNormalization)      & (None, 14, 14, 384)  & 1536     \\
block\_10\_depthwise\_relu (ReLU)                  & (None, 14, 14, 384)  & 0        \\ \hline
block\_10\_project (Conv2D)                        & (None, 14, 14, 96)   & 36864    \\ \hline
%block\_10\_project\_BN (BatchNormalization)        & (None, 14, 14, 96)   & 384      \\
block\_11\_expand (Conv2D)                         & (None, 14, 14, 576)  & 55296    \\ \hline
%block\_11\_expand\_BN (BatchNormalization)         & (None, 14, 14, 576)  & 2304     \\
block\_11\_expand\_relu (ReLU)                     & (None, 14, 14, 576)  & 0        \\ \hline
block\_11\_depthwise (DepthwiseConvolution)        & (None, 14, 14, 576)  & 5184     \\ \hline
%block\_11\_depthwise\_BN (BatchNormalization)      & (None, 14, 14, 576)  & 2304     \\
block\_11\_depthwise\_relu (ReLU)                  & (None, 14, 14, 576)  & 0        \\ \hline
block\_11\_project (Conv2D)                        & (None, 14, 14, 96)   & 55296    \\ \hline
%block\_11\_project\_BN (BatchNormalization)        & (None, 14, 14, 96)   & 384      \\
block\_11\_add (Add)                               & (None, 14, 14, 96)   & 0        \\ \hline
%block\_11\_project\_BN{[}0{]}{[}0{]}               &                      &          \\
block\_12\_expand (Conv2D)                         & (None, 14, 14, 576)  & 55296    \\ \hline
%block\_12\_expand\_BN (BatchNormalization)         & (None, 14, 14, 576)  & 2304     \\
block\_12\_expand\_relu (ReLU)                     & (None, 14, 14, 576)  & 0        \\ \hline
block\_12\_depthwise (DepthwiseConvolution)        & (None, 14, 14, 576)  & 5184     \\ \hline
%block\_12\_depthwise\_BN (BatchNormalization)      & (None, 14, 14, 576)  & 2304     \\
block\_12\_depthwise\_relu (ReLU)                  & (None, 14, 14, 576)  & 0        \\ \hline
block\_12\_project (Conv2D)                        & (None, 14, 14, 96)   & 55296    \\ \hline
%block\_12\_project\_BN (BatchNormalization)        & (None, 14, 14, 96)   & 384      \\
block\_12\_add (Add)                               & (None, 14, 14, 96)   & 0        \\ \hline
%block\_12\_project\_BN{[}0{]}{[}0{]}               &                      &          \\
block\_13\_expand (Conv2D)                         & (None, 14, 14, 576)  & 55296    \\ \hline
%block\_13\_expand\_BN (BatchNormalization)         & (None, 14, 14, 576)  & 2304     \\
block\_13\_expand\_relu (ReLU)                     & (None, 14, 14, 576)  & 0        \\ \hline
block\_13\_pad (ZeroPadding2D)                     & (None, 15, 15, 576)  & 0        \\ \hline
block\_13\_depthwise (DepthwiseConvolution)        & (None, 7, 7, 576)    & 5184     \\ \hline
%block\_13\_depthwise\_BN (BatchNormalization)      & (None, 7, 7, 576)    & 2304     \\
block\_13\_depthwise\_relu (ReLU)                  & (None, 7, 7, 576)    & 0        \\ \hline
block\_13\_project (Conv2D)                        & (None, 7, 7, 160)    & 92160    \\ \hline
%block\_13\_project\_BN (BatchNormalization)        & (None, 7, 7, 160)    & 640      \\
block\_14\_expand (Conv2D)                         & (None, 7, 7, 960)    & 153600   \\ \hline
%block\_14\_expand\_BN (BatchNormalization)         & (None, 7, 7, 960)    & 3840     \\
block\_14\_expand\_relu (ReLU)                     & (None, 7, 7, 960)    & 0        \\ \hline
block\_14\_depthwise (DepthwiseConvolution)        & (None, 7, 7, 960)    & 8640     \\ \hline
%block\_14\_depthwise\_BN (BatchNormalization)      & (None, 7, 7, 960)    & 3840     \\
block\_14\_depthwise\_relu (ReLU)                  & (None, 7, 7, 960)    & 0        \\ \hline
block\_14\_project (Conv2D)                        & (None, 7, 7, 160)    & 153600   \\ \hline
%block\_14\_project\_BN (BatchNormalization)        & (None, 7, 7, 160)    & 640      \\
block\_14\_add (Add)                               & (None, 7, 7, 160)    & 0        \\ \hline
%block\_14\_project\_BN{[}0{]}{[}0{]}               &                      &          \\
block\_15\_expand (Conv2D)                         & (None, 7, 7, 960)    & 153600   \\ \hline
%block\_15\_expand\_BN (BatchNormalization)         & (None, 7, 7, 960)    & 3840     \\
block\_15\_expand\_relu (ReLU)                     & (None, 7, 7, 960)    & 0        \\ \hline
block\_15\_depthwise (DepthwiseConvolution)        & (None, 7, 7, 960)    & 8640     \\ \hline
%block\_15\_depthwise\_BN (BatchNormalization)      & (None, 7, 7, 960)    & 3840     \\ \hline
block\_15\_depthwise\_relu (ReLU)                  & (None, 7, 7, 960)    & 0        \\ \hline
block\_15\_project (Conv2D)                        & (None, 7, 7, 160)    & 153600   \\ \hline
%block\_15\_project\_BN (BatchNormalization)        & (None, 7, 7, 160)    & 640      \\
block\_15\_add (Add)                               & (None, 7, 7, 160)    & 0        \\ \hline
%block\_15\_project\_BN{[}0{]}{[}0{]}               &                      &          \\
block\_16\_expand (Conv2D)                         & (None, 7, 7, 960)    & 153600   \\ \hline
%block\_16\_expand\_BN (BatchNormalization)         & (None, 7, 7, 960)    & 3840     \\
block\_16\_expand\_relu (ReLU)                     & (None, 7, 7, 960)    & 0        \\ \hline
block\_16\_depthwise (DepthwiseConvolution)        & (None, 7, 7, 960)    & 8640     \\ \hline
%block\_16\_depthwise\_BN (BatchNormalization)      & (None, 7, 7, 960)    & 3840     \\
block\_16\_depthwise\_relu (ReLU)                  & (None, 7, 7, 960)    & 0        \\ \hline
block\_16\_project (Conv2D)                        & (None, 7, 7, 320)    & 307200   \\ \hline
%block\_16\_project\_BN (BatchNormalization)        & (None, 7, 7, 320)    & 1280     \\
Conv\_1 (Conv2D)                                   & (None, 7, 7, 1280)   & 409600   \\ \hline
%Conv\_1\_bn (BatchNormalization)                   & (None, 7, 7, 1280)   & 5120     \\
out\_relu (ReLU)                                   & (None, 7, 7, 1280)   & 0        \\ \hline
global\_average\_pooling2d (GlobalAveragePool)     & (None, 1280)         & 0        \\ \hline
Logits (Dense)                                     & (None, 1000)         & 1281000  \\ \hline
Total params: 3,538,984                            &                      &          \\
Trainable params: 3,504,872                        &                      &          \\
Non-trainable params: 34,112                       &                      &         
%\end{tabular}
%}
\label{tab:MobileNetV2model}
\end{longtable}
%}
\end{center}
\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\end{document}