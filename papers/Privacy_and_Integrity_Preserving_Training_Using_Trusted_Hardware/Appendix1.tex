\section{Colluding GPUs}
\label{ap:collude}
In this section, we investigate the scenario in which multiple GPUs can collaborate to extract information from the encoded data. With $K'$ GPUs and virtual batch size of $K$, we can tolerate $M < K'-K$ colluding GPUs without compromising privacy. We show how we can securely outsource calculating $\langle\mathbf W,\mathbf x^{(i)}\rangle$, $i=1,\dots, K$, to the GPUs. We first create $P=M+K$ encoded data vectors, $\bar{\mathbf x}^i$, $i=1,\dots, P$, using $M$ noise vectors $\mathbf R^1,\dots,\mathbf R^M$, as follows.
\begin{align}
    &\bar{\mathbf X}=\mathbf X\mathbf A_1+\mathbf R\mathbf A_2~,\quad\text{where ,}\nonumber\\
    &\bar{\mathbf X}=\left[\bar{\mathbf x}^1,\dots,\bar{\mathbf x}^{P}\right]\in\mathbb R^{N\times P}~,\nonumber\\
    &{\mathbf X}=\left[{\mathbf x}^1,\dots,{\mathbf x}^K\right]\in\mathbb R^{N\times K}~,\nonumber\\
    &{\mathbf R}=\left[{\mathbf R}^1,\dots,{\mathbf R}^M\right]\in\mathbb R^{N\times M}~,\nonumber\\
    &\text{and ,}~~\mathbf A_1\in\mathbb R^{K\times P}~,~~\mathbf A_2\in\mathbb R^{M\times P}~.
\end{align}
Here, the matrices $\mathbf A_1$ and $\mathbf A_2$ are the encoding coefficient similar to the initial scheme we used for DarKnight. Theorem~\ref{thm:colluding_GPUs1} provides privacy guarantees for this approach under very mild conditions on the matrix $\mathbf A_2$.

\begin{thm}\label{thm:colluding_GPUs1}
In the encoding scheme described above, assume that the encoding matrix $\mathbf A_2$ is a full-rank matrix, such that for every column $\mathbf A_2^{(i)}$ in $\mathbf A_2$, we have $\|\mathbf A_2^{(i)}\|_2\geq C$. Also assume that the vectors $\mathbf R^i$ are independently drawn from $\mathcal N(\mathbf 0,\sigma^2\mathbb I)$.Then the maximum leaked information with $M$ colluding GPUs is bounded by
\begin{align}
     \sum_{i,j}\frac{\text{Var}(X^{i,j})}{C\sigma^2}
\end{align}
\end{thm}

\begin{proof}
Assume that a subset $S\subseteq [1,\dots K']$ of the $K'$ GPUs are colluding and $|S|=M$. Thus, those GPUs have are given the encoded vectors $\{\bar{\mathbf x}^i\}^{i\in S}$. Our goal is to bound the mutual information between $\{\bar{\mathbf x}^i\}^{i\in S}$ and $\mathbf X$.
\begin{align}
    I\left({\mathbf X}~;~ \mathbf X\mathbf A_1(:,S)+\mathbf R\mathbf A_2(:,S)\right)~.
\end{align}
Here, for a matrix $M$, $M(:,S)$ denotes a sub-matrix of $M$, whose columns are chosen from the set $S$. Note that the matrix $\mathbf A_2(:,S)$ is full-rank, whose norm of each column is lower-bounded by $C$. Therefore, 
\begin{align}
    &I\left({\mathbf X}~;~ \mathbf X\mathbf A_1(:,S)+\mathbf R\mathbf A_2(:,S)\right)\nonumber\\
    &\qquad\qquad\qquad \leq I\left({\mathbf X}~;~ \mathbf X\mathbf A_1(:,S)+C\sigma^2\bar{\mathbf R}\right)~,
\end{align}
where $\bar{\mathbf R}$ is a matrix with iid standard Gaussian entries. This is because for a Gaussian matrix $\mathbf M$ and a vector $\mathbf v$, we have $\mathbf M\mathbf v\sim \mathbf g \|\mathbf v\|$, where $\mathbf g$ is a Gaussian vector. Now, simply using Lemma \ref{lemma:bound_sum} yields
\begin{align}
    &I\left({\mathbf X}~;~ \mathbf X\mathbf A_1(:,S)+\mathbf R\mathbf A_2(:,S)\right)\nonumber\\
    &\qquad\qquad\qquad \leq I\left({\mathbf X}~;~ \mathbf X\mathbf A_1(:,S)+C\sigma^2\bar{\mathbf R}\right)\nonumber\\
    &\qquad\qquad\qquad  \leq\sum_{i,j} \frac{\text{Var}(X^{i,j})}{C\sigma^2}~,
\end{align}
and this concludes the proof.
\end{proof}
As you saw in the proof, we needed every sub-matrix $\mathbf A_2(:,S)\in\mathbb R^{M\times |S|}$ has linearly independent columns. That is why it was necessary to have at most $M$ colluding GPUs ($|S|\leq M$) when we use $M$ noise vectors in our scheme. In the other words, when using $M$ noise vectors (which required $M$ extra equations/GPUS), we can tolerate at most $M$ colluding GPUs.

Now that we took care of inference as described above, we would like to update our training procedure for this new scenario. Same as before, we can calculate the weight updates using the following equations:
\begin{equation}\label{eq:gamma_lin_colluding}
\triangledown \mathbf{W} = \sum_{j=1}^{P}  \gamma_{j} \text{Eq}_{j}, \qquad \text{Eq}_{j} = \left\langle \sum_{i=1}^K \beta_{j,i}~ \mathbf \delta^{(i)}~,{\color{blue}\bar{\mathbf x}^{(j)}} \right\rangle~\quad~
\end{equation}
We now define
\begin{align}
    \mathbf A=\begin{bmatrix}
    \mathbf A_1\\
    \mathbf A_2
    \end{bmatrix}~, \mathbf B=\begin{bmatrix}
    \beta_{j,i}
    \end{bmatrix}~, \Gamma=\text{Diag}(\gamma_1,\dots,\gamma_K)
\end{align}
Now, it is easy to show that if 
\begin{equation}
    \mathbf B^\intercal\cdot \mathbf \Gamma\cdot \mathbf A = \begin{bmatrix}1 & 0 & \dots & 0 & 0 & \dots & 0
  \\0 & 1 & 0 & \dots & 0 & \dots & 0
  \\\vdots & \ddots& \ddots & \ddots & \vdots & \ddots  
\\ 0 & \dots & 0 & 1 & 0 &\dots & 0\end{bmatrix}_{K \times K'}
\label{eq:matrix_relation}
\end{equation}

