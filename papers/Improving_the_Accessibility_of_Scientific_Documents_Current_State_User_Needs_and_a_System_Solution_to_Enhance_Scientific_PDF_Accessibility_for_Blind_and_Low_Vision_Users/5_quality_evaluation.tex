\section{HTML render quality evaluation}
\label{sec:evaluation}

Extracting semantic content from PDF is an imperfect process. Though re-rendering a PDF as HTML can increase a document's accessibility, the process relies on machine learning models that can make mistakes when extracting information. As we glean from user studies, BLV users may have some tolerance for error, but there is an inherent trade-off between errors and perceived trust in the system. We conduct a study to estimate the (1) faithfulness of the HTML renders to the source PDFs, and (2) overall readability of the resulting HTML renders. We define \textit{faithfulness} as how accurately the HTML render represents different facets of the PDF document, such as displaying the correct title, section headers, and figure captions. These facets are measured as the number of errors that are made in rendering, e.g., mistakenly parsing one figure caption into the body text is counted as one error towards that facet. \textit{Readability}, on the other hand, is an ordinal variable meant to capture the overall usability of the parse. Documents are given one of three grades, those with no major problems, some problems, and many problems impacting readability.

To evaluate readability and faithfulness, we first perform open coding on a small sample of document PDFs and corresponding \scially HTML renders. The purpose of this exercise is to identify facets of extraction that impact the ability to read a paper. A rubric is then designed based on these identified facets. The process taken to design the evaluation rubric, the rubric's content, and annotation instructions are detailed in Section~\ref{sec:eval_rubric}. We then annotate a sample of \numeval papers across different fields of study using this rubric. For each category of errors identified during open coding, we compute the overall error rates seen in our sample. We also present the overall assessed readability, reported in aggregate over our sample and by fields of study. Results of this evaluation are presented in Section~\ref{sec:eval_res}.

\subsection{Open coding of document facets}
\label{sec:eval_coding}

One author performed open coding on a sample of papers, comparing the PDF and \scially HTML renders to identify inconsistencies and facets that impact the faithfulness of document representation. Papers are sampled from the Semantic Scholar API\footnote{\href{https://api.semanticscholar.org/}{https://api.semanticscholar.org/}} using various search terms, and selecting the top 3 results for each search term for which a PDF and S2ORC parse are available. Search terms were selected to achieve coverage over different domains, and the top papers are sampled to select for relevant publications. The author stopped sampling papers upon reaching saturation, resulting in 8 search terms and 24 papers. The search terms used were: \texttt{human computer interaction}, \texttt{epilepsy}, \texttt{quasars}, \texttt{language model}, \texttt{influenza epidemiology}, \texttt{anabolic steroids}, \texttt{social networks}, and \texttt{arctic snow cover}.

For each paper, the author evaluated the PDF and HTML render side-by-side, scanning through the document to identify points of difference between the two document representations. Specifically, the author looked for any text in the PDF that is not shown in the HTML, any text from the PDF that is mixed into the main text of the HTML (e.g. figure captions, headers, or footnotes that should be separate from the main text but are mixed in, interrupting the reading flow), and other parsing mistakes (e.g. errors with math, missing lists and tables etc). These observations are detailed qualitatively, and each facet is assessed for its faithfulness to the original PDF document as well as its overall impact on readability.

\subsection{Evaluation rubric}
\label{sec:eval_rubric}

Observations from open coding are coalesced into an evaluation rubric and form for grading the quality and faithfulness of the HTML render.
The evaluation form attempts to capture errors in PDF extraction that affect each of the primary semantic categories identified for proper reading. These semantic categories and common extraction errors are given in Table~\ref{tab:eval_cats}.

Questions in the form are designed to capture each type of faithfulness error, while allowing annotators to qualify their responses. We also include a question to capture the overall readability of the HTML render. Instructions for completing the annotation form are provided in Appendix~\ref{app:eval_instructions}; the final version of the form is replicated in Appendix~\ref{app:eval_instructions}; and the rubric for overall readability evaluation is given in Appendix~\ref{app:quality_rubric}. 

Three authors iterated twice on the content of the evaluation form, until they came to a consensus that all evaluation categories were adequately addressed using a minimum set of questions. Two authors then participated in pilot annotations, where each person independently annotated the same set of five papers sampled from the set labeled by the third author during open coding. Answers to all numeric questions were within $\pm 1$ for these five papers when comparing the two authors' annotations. All three authors discussed discrepancies in overall readability score, iterating on the rubric defined in Appendix~\ref{app:quality_rubric} and coming to a consensus. The finalized form and rubric are used for evaluation.

\begin{table}[t!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{L{20mm}L{65mm}X}
        \toprule
        \textbf{Category} & \textbf{Description} & \textbf{Common errors} \\
        \midrule 
        \textsc{title} & The title and subtitle of the paper & Missing words \newline Extra words \\ 
        \midrule
        \textsc{authors} & A list of authors who wrote the paper; this includes affiliation, though we do not explicitly evaluate affiliation in this study & Missing authors \newline Extra authors \newline Misspellings \\
        \midrule
        \textsc{abstract} & The abstract of the paper & Some text not extracted \newline Other text incorrectly extracted as abstract \\
        \midrule
        \textsc{section headings} & The text of section headings & Some headings not extracted (part of body text) \newline Other text incorrectly extracted as headings \\
        \midrule
        \textsc{body text} & The main text of the paper, organized by paragraph under each section heading & Some paragraphs not extracted (missing) \newline Some text not extracted \newline Other text incorrectly extracted as body text \\
        \midrule
        \textsc{figures} & Images, captions, and alt-text of each figure & Figure not extracted \newline Caption text not extracted (part of body text) \newline Other text incorrectly extracted as caption text \\
        \midrule
        \textsc{tables} & Caption/title and content of each table & Table not extracted (not part of body text) \newline Table not extracted (part of body text) \newline Caption text not extracted (part of body text) \newline Other text incorrectly extracted as caption text  \\
        \midrule
        \textsc{equations} & Mathematical formulas, represented in TeX or Math ML; note: our current pipeline does not extract math & Some equations not extracted \newline Some equations incorrectly extracted \\
        \midrule
        \textsc{bibliography} & Bibliography entries in the reference section & Some bibliography entries not extracted \newline Some bibliography entries incorrectly extracted \newline Other text incorrectly extracted as bibliography \\ 
        \midrule
        \textsc{inline citations} & Inline citations from the body text to papers in the bibliography section & Some inline citations not detected \newline Some inline citations incorrectly linked \\
        \midrule
        \textsc{headers, footers \& footnotes} & Page headers and footers, footnotes, endnotes, and other text that is not a part of the main body of the document & Some headers and footers incorrectly extracted into body text \\
        \bottomrule
    \end{tabularx}
    \caption{Categories of paper objects identified for evaluation along with the common errors seen for each category.}
    \label{tab:eval_cats}
%     \Description{
% Category; Description; Common errors 
% title; The title and subtitle of the paper; Missing words, Extra words  
% authors; A list of authors who wrote the paper; this includes affiliation, though we do not explicitly evaluate affiliation in this study; Missing authors, Extra authors, Misspellings 
% abstract; The abstract of the paper; Some text not extracted, Other text incorrectly extracted as abstract 
% section headings; The text of section headings; Some headings not extracted (part of body text), Other text incorrectly extracted as headings 
% body text; The main text of the paper, organized by paragraph under each section heading; Some paragraphs not extracted (missing), Some text not extracted, Other text incorrectly extracted as body text 
% figures; Images, captions, and alt-text of each figure; Figure not extracted, Caption text not extracted (part of body text), Other text incorrectly extracted as caption text 
% tables; Caption/title and content of each table; Table not extracted (not part of body text), Table not extracted (part of body text), Caption text not extracted (part of body text), Other text incorrectly extracted as caption text  
% equations; Mathematical formulas, represented in TeX or Math ML; note: our current pipeline does not extract math; Some equations not extracted, Some equations incorrectly extracted 
% bibliography; Bibliography entries in the reference section; Some bibliography entries not extracted, Some bibliography entries incorrectly extracted, Other text incorrectly extracted as bibliography  
% inline citations; Inline citations from the body text to papers in the bibliography section; Some inline citations not detected, Some inline citations incorrectly linked 
% headers, footers & footnotes; Page headers and footers, footnotes, endnotes, and other text that is not a part of the main body of the document; Some headers and footers incorrectly extracted into body text 
%     }
\end{table}

Of the categories and errors described in Table~\ref{tab:eval_cats}, our current pipeline does not extract table content and equations. Tables are extracted as images by DeepFigures \citep{Siegel2018ExtractingSF}, which do not contain table semantic information. Regarding equations, we distinguish between inline equations (math written in the body text) and display equations (independent line items that can usually be referenced by number); for this work, we evaluated a small sample of papers for successful extraction of display equations. Though some display equations are recognized, the quality of equation extraction is low, usually resulting in missing tokens or improper math formatting. Therefore, we decided to replace display equations in the prototype with the equation placeholder shown in Figure~\ref{fig:figure_equations}. Since problems with mathematical formulae are among those most mentioned by users in our study, equation extraction is among our most urgent future goals, and we discuss some options in Section~\ref{sec:future_work}.

\subsection{Evaluation results}
\label{sec:eval_res}

We start with the dataset of \numpdfs papers we analyze in Section~\ref{sec:sos},
and subsample 535 documents stratified by field of study. Two expert annotators with undergraduate science training code papers from this sample, with an aim of annotating around 20 papers per field of study. Though we achieve the target number for most fields, we missed this target for some fields closer to the humanities because more of these documents are difficult to manually annotate within our time and resource constraints.
For example, documents are deemed unsuitable for annotation if they are not papers (i.e., they are books, posters, abstracts, etc), if they are too long, or if they are not in English. In these cases, the annotators can skip the document. Detailed guidance on suitability is provided in the annotation instructions (see Appendix~\ref{app:eval_instructions}). 

The two annotators annotated \numeval unique papers and skipped \numevalskipped. The resulting annotated sample consists of papers from 195 unique publication venues. Each paper takes 5--10 minutes to grade. Documents are skipped primarily due to language (paper not in English), length, or the document is not a paper. Inter-annotator agreement is computed over a sample of 20 papers over each of the evaluated facets. We report Cohen's Kappa for categorical questions such as those on the extraction of title, authors, abstract, and bibliography. For numerical questions such as counting the occurrence of extraction errors related to figures, tables, section headings, and body paragraphs etc, we report the intraclass correlation coefficient (ICC) as well as the average difference of values between the two annotators. See Table~\ref{tab:eval_iaa} for these results.
Agreement was high for most element-level annotator questions. Annotators had the highest levels of disagreement on the evaluation of header/footer/footnote errors, section heading errors, and body paragraph errors, likely due to these being text-based and the most numerous; though the average differences reported between annotators on these questions are only between 1-2. Likewise, agreement on overall readability score is modest, at 0.55; we note, however, that neither annotator labeled any paper as having no major readability problems when the other annotator labeled it as having lots of readability problems.

\begin{table}[tb!]
    \centering
    \begin{tabularx}{0.94\linewidth}{L{40mm}L{15mm}L{15mm}L{15mm}L{15mm}L{25mm}}
        \toprule
        Evaluation criteria & Number of classes & Agreement & Cohen's Kappa & ICC & Mean Difference ($\pm$ SD) \\
        \midrule
        Title & 3 & 0.87 & 0.33 & - & - \\
        Authors & 3 & 1.00 & 1.00 & - & - \\
        Abstract & 3 & 0.95 & 0.64 & - & - \\
        \midrule
        Number of figures & - & 1.00 & - & 1.00 & 0.00 $\pm$ 0.00 \\
        Figure extraction errors & - & 0.89 & - & 1.00 & 0.11 $\pm$ 0.31 \\
        Figure caption errors & - & 0.89 & - & 1.00 & 0.11 $\pm$ 0.31 \\
        Number of tables & - & 0.92 & - & 0.98 & 0.12 $\pm$ 0.43 \\
        Table extraction errors & - & 0.89 & - & 0.98 & 0.17 $\pm$ 0.50 \\
        Table caption errors & - & 0.78 & - & 0.94 & 0.33 $\pm$ 0.67 \\
        \midrule
        Header/footer/footnote errors & - & 0.40 & - & 0.60 & 1.88 $\pm$ 2.12 \\
        Section heading errors & - & 0.71 & - & 0.79 & 0.71 $\pm$ 1.70 \\
        Body paragraph errors & - & 0.46 & - & 0.66 & 1.50 $\pm$ 2.22 \\
        \midrule
        Bibliography extraction & 4 & 0.94 & 0.82 & - & - \\
        Inline citation linking & 4 & 0.80 & 0.11 & - & - \\
        \midrule
        Overall score & 3 & 0.55 & 0.07 & - & - \\
        \bottomrule
    \end{tabularx}
    \caption{Inter-rater agreement for evaluation. For categorical questions, such as title, author, abstract, bibliography, inline citation, and overall score, we report the number of classes available for annotation, along with annotator agreement and Cohen's Kappa. For numerical questions, such as the number of each type of extraction error, we report agreement, the intraclass correlation coefficient (ICC), and the average difference and standard deviation of the values between the two annotators.}
    \label{tab:eval_iaa}
%     \Description{
% Evaluation criteria; Number of classes; Agreement; Cohen's Kappa; ICC; Mean Difference (+/- SD) 
% Title; 3; 0.87; 0.33; -; - 
% Authors; 3; 1.00; 1.00; -; - 
% Abstract; 3; 0.95; 0.64; -; - 
% Number of figures; -; 1.00; -; 1.00; 0.00 +/- 0.00 
% Figure extraction errors; -; 0.89; -; 1.00; 0.11 +/- 0.31 
% Figure caption errors; -; 0.89; -; 1.00; 0.11 +/- 0.31 
% Number of tables; -; 0.92; -; 0.98; 0.12 +/- 0.43 
% Table extraction errors; -; 0.89; -; 0.98; 0.17 +/- 0.50 
% Table caption errors; -; 0.78; -; 0.94; 0.33 +/- 0.67 
% Header/footer/footnote errors; -; 0.40; -; 0.60; 1.88 +/- 2.12 
% Section heading errors; -; 0.71; -; 0.79; 0.71 +/- 1.70 
% Body paragraph errors; -; 0.46; -; 0.66; 1.50 +/- 2.22 
% Bibliography extraction; 4; 0.94; 0.82; -; - 
% Inline citation linking; 4; 0.80; 0.11; -; - 
% Overall score; 3; 0.55; 0.07; -; - 
%     }
\end{table}

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/all_eval.png}
    \caption{Evaluation results for various document components. Corresponding numbers are provided in Table~\ref{tab:eval_raw_by_element} in Appendix~\ref{app:eval_raw_results}.}
    \label{fig:eval_results}
    \Description{Percent stacked bar plots showing evaluation results. Metadata elements (title, authors, abstract) are extracted correctly the vast majority of the time. Around a quarter of papers don't have figures and two fifths don't have tables. Of those with figures, the majority are extracted correctly, though extraction errors for both images and captions are not infrequent. Of those with tables, table extraction errors and table caption extraction errors are infrequent. Text element errors (header/footer/footnote, section heading, and body paragraph) are the most frequent. For header/footer/footnotes, a bit more than half of all papers have one or more errors. The majority of papers have more than 1 section heading extraction error. A bit less than half of papers have body paragraph errors that affect 1 or more paragraphs. Bibliography extraction and inline citation linking are both good, with the vast majority of papers having all or most entries extracted and linked. For overall readability, more than half of evaluated papers have no major problems, around a third have some problems, and the remaining lots of problems.}
\end{figure}

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/eval_fos.png}
    \caption{Overall readability results as proportion of total split by field of study, sorted by the percentage of papers with no major problems. The number of documents analyzed in each field is given, ranging from N=7 (History) to N=39 (Physics). The fields of study with the worst parse quality (Economics, Environmental science, Business, Art, and Political science) tend to be closer to the humanities, and may be due to the under-representation of papers from these fields in the data used to train the PDF parsers we use in our extraction pipeline. Corresponding numbers are provided in Table~\ref{tab:eval_raw_by_fos} in Appendix~\ref{app:eval_raw_results}.}
    \label{fig:eval_fos}
    \Description{Percent stacked bar plots show evaluation results for overall readability split by field of study. The bars are sorted on the field of study from those with the largest proportion of papers having no major problems to those with the least proportion having no major problems. Fields with the highest overall readability include history, engineering, sociology, physics, and chemistry. Fields with the lowest overall readability include political science, art, business, environmental science, and economics. No field stands out as being obviously different from the overall sample, though art, business and economics have the largest proportion of papers that were classified as having lots of readability problems.}
\end{figure}

All results and statistics are reported on the set of \numeval annotated papers. Figure~\ref{fig:eval_results} shows the breakdown of each type of error and the frequency at which it occurs. Metadata elements like title, authors, and abstract are successfully extracted the majority of the time. For figure and table elements, approximately 25\% of papers in our evaluation sample do not include figures, and around 45\% do not have tables. Of those that have figures, the majority (201, 69.1\% of 291) do not have extraction or parsing errors; around half of documents with errors have errors that only relate to one figure. Similarly, the majority of tables and table captions are correctly identified as tables and table captions, and are not incorrectly mixed into the body text. We note that the lack of an error here does not indicate that the table is extracted correctly in an accessible manner, just that it is not incorrectly parsed as body text.

Unsurprisingly, errors in text element parsing are the most prevalent, especially for headers/footers/footnotes and section headings. The most common type of header/footer/footnote error observed are when these texts are mixed into the body text around page breaks, interrupting reading flow. These types of errors are also observed frequently during screen reader use when reading directly from an untagged PDF. For section headings, in particular, the majority of papers have errors; around 67\% of papers have between 1--5 errors (either missed headings or extraneous headings), and 9\% have more than 5 errors. Due to the large number of section headings in papers, parsing errors are more frequent, and unfortunately, these errors impact the ability to properly navigate the HTML parse. 
Errors in body text extraction also negatively impact readability, in this case, select text in the document is being missed completely in the HTML render. We see that though the majority of parses have no body text errors, around 33\% of papers have between 1-5 missing paragraphs. 

Figure~\ref{fig:eval_results}(d) shows grading results for bibliography elements. Our pipeline is quite good at extracting bibliography entries, extracting all or most entries in the vast majority of cases, and successfully linking inline citations to these bibliography entries also in a large majority of cases. When bibliography extraction fails, it tends to fail catastrophically, resulting in no or few extractions.

The overall readability score is provided in Figure~\ref{fig:eval_results}(e). A majority of papers (54.5\%, 210 papers) have no major problems impacting readability. Another 31.7\% (122) of papers have some problems impacting readability, and 13.8\% (53) of papers have lots of readability problems. We are encouraged that a majority of HTML renders have no major problems, though our results necessitate further understanding of the papers with which our extraction pipeline has difficulty. If papers with lots of problems can be identified \textit{a priori}, we can prevent surfacing these low quality parses to the user. We perform some preliminary experiments to identify paper features that are more correlated with readability problems, though no features stood out as being predictive; we present those results in Appendix~\ref{app:eval_association}.

In Figure~\ref{fig:eval_fos}, we show the breakdown of overall readability by field of study,
plotting the proportion of papers per field that are classified as having no major problems, some problems, and lots of problems impacting readability. Many fields have similar distributions compared to the overall evaluation set. However, we note that some fields such as Art, Business, Economics, and Environmental science to some degree, have significantly lower quality extraction results. We posit that this may be due to biases in our PDF extraction pipeline. Some of the machine learning modules we use are primarily trained on paper data from the biomedical and Computer Science domains, where large scale labeled PDF extraction datasets can be found. Humanities-adjacent fields like Art and Business have very different publication norms, and the different layouts and content of papers and documents in these fields may provide additional challenges to our system, resulting in lower quality extraction and rendering. 

