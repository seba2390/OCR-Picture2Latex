% %%
% %% If your work has an appendix, this is the place to put it.
\appendix

\section{Evaluation forms}

This section contains forms and documents used to evaluate the quality of HTML renders produced by our system.

\subsection{Evaluation instructions}
\label{app:eval_instructions}

Instructions for annotators are reproduced verbatim below.

\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}
\begin{quote}
\textbf{Goal:} Identify and quantify the prevalence of different parse issues in S2ORC parses to assess their suitability for accessibility purposes. This will help us decide whether S2ORC parses can help meet screen reacher accessibility needs.

\vspace{4pt}\noindent\textbf{Number of papers:} ~500 papers sampled across different domains of science

\vspace{4pt}\noindent You will be presented with a spreadsheet of scientific papers, each with a pair of links. One link goes to a PDF of the paper. One link goes to an HTML representation of the same article. For each pair of links, we would like to know how faithfully the HTML representation captures the information on the PDF.

\vspace{4pt}\noindent\textbf{INSTRUCTIONS:}

\begin{enumerate}
    \item[1.] Open the two links side by side.
    \item[2.] If the two links do not seem to correspond to the same paper, STOP. Make a note in the spreadsheet and SKIP.
    \item[3.] If the PDF shows a paper that is not suitable, STOP. Make a note in the spreadsheet and SKIP. Non-suitable may include:
    \begin{itemize}
        \item[--] It is not a scientific paper.
        \item[--] It is spam or a fake paper.
        \item[--] It is slides, a poster, or other such non-paper document.
        \item[--] It is just an abstract.
        \item[--] It is a series of articles (e.g. conference proceedings, journal issue etc).
        \item[--] It is a book.
        \item[--] It is supplementary material; \\
        note: some supplementary material is solely made up of figure or images.
        \item[--] Something else that makes you pause. If you're not sure, SKIP it. 
    \end{itemize}
    \item[4.] Copy the paper identifier corresponding to this paper into the first question on this form. Please make sure the identifier matches the paper you are evaluating.
    \item[5.] Answer each of the questions in this form as best as you can, treating the PDF as gold. There is no need to review every word or line of text. We are just trying to get an overall assessment of parse quality. For any question that asks for a number, enter `0` if there are no obvious problems with those extractions.
    \item[6.] Submit the form and mark the row in the spreadsheet as complete.
\end{enumerate}

\vspace{4pt}\noindent\textbf{Note:} Display equations (those that are in their own paragraph) are currently not preserved in S2ORC, so we ask the annotator to ignore issues around missing display equations. Inline equations (those that are inline within a paragraph) are converted to token streams, which may not be faithful to the original PDF (e.g. fractions may not be preserved). The annotator can provide a description of issues around equation parsing when there is a notable issue.
\end{quote}
\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}

\subsection{Evaluation questions}

Questions asked in the evaluation form are reproduced in Table~\ref{tab:eval_questions}.

\begin{table}[h!]
\small
\begin{tabular}{lp{130mm}}
    \toprule
    \textbf{Answer} & \textbf{Questions} \\
    \midrule
    y/p/n & Is the TITLE correctly extracted? \\
    text & \textit{Comment (clarify if answer is ``partially'' or ``no'')} \\
    \midrule
    y/p/n & Are the AUTHOR(S) correctly extracted? \\
    text & \textit{Comment (clarify if answer is ``partially'' or ``no'')} \\
    \midrule
    y/p/n & Is the ABSTRACT correctly extracted? \\ 
    text & \textit{Comment (clarify if answer is ``partially'' or ``no'')} \\
    \midrule
    y/n & Does this paper contain a substantial number of math EQUATIONS (more than 5 display equations)? \\
    \midrule
    number & How many FIGURES are in the PDF? (Enter `0' if none) \\
    number & How many FIGURES are correctly extracted? (Enter `0' if no figures in paper) \\
    number & How many FIGURE CAPTIONS are correctly extracted? (Enter `0' if no figures in paper) \\
    number & Approximately how many FIGURE captions are **INCORRECTLY** parsed into the body text (should be a figure caption but is mixed in with the body text)? (Enter `0' if they are all correct or if no figures) \\
    text & \textit{Comment (Optional -- Note anything here about FIGURES or FIGURE CAPTIONS, e.g. which figures are not extracted, which figure captions are not extracted, which figure captions are incorrectly extracted into the body text etc.)} \\
    \midrule
    number & How many TABLES are in the PDF? (Enter `0' if none) \\
    number & How many TABLES are correctly extracted? (Enter `0' if no tables in paper) \\
    number & How many TABLE TITLES / CAPTIONS are correctly extracted? (Enter `0' if no tables in paper or if those tables do not have titles / captions) \\
    number & Approximately how many TABLE titles are **INCORRECTLY** parsed into the body text (should be a table title / caption but is mixed in with the body text)? (Enter `0' if they are all correct or if no tables or table titles / captions) \\
    number & Approximately how many TABLES have content that is **INCORRECTLY** parsed into the body text (content of table is mixed with the body text)? (Enter `0' if they are all correct or if no tables) \\
    text & \textit{Comment (Optional -- Note anything here about TABLES or TABLE TITLES / CAPTIONS, e.g. which tables are not extracted, which table captions are not extracted, which table title / captions / content are incorrectly extracted into the body text etc.)} \\
    \midrule
    number & Approximately how many times are page HEADERS or FOOTERS *INCORRECTLY* mixed into the body text?This also includes margin content such as arXiv watermarks. (Enter `0' if all okay or no headers or footers) \\
    text & \textit{Comment (Optional -- Note anything interesting here about incorrectly parsed headers or footers; no need to provide page numbers)} \\
    \midrule
    number & Approximately how many SECTION HEADINGS are *INCORRECTLY* extracted?  (Enter `0' if they are all correct or no section headings) \\
    text & \textit{Comment (Optional -- Note anything interesting about the section heading extractions, no need to list exhaustively)} \\
    \midrule
    number & Approximately how many BODY TEXT PARAGRAPHS are **MISSING** from the extraction?  (Enter `0' if they are all there or there is no body text) \\ 
    text & \textit{Comment (Optional -- Note anything interesting about the body text extractions)} \\
    \midrule 
    choice & Are BIBLIOGRAPHY entries extracted correctly? (options: all correct, mostly correct, half correct, mostly incorrect, incorrect, no bibliography)  \\
    text & \textit{Comment (Optional -- Note anything interesting about the bibliography extractions; no need to list exhaustively)} \\
%     * All correct
%     * Mostly correct (missing a few)
%     * About half are correct
%     * Mostly incorrect (gets a few)
%     * Incorrect (very bad, unusable)
%     * No bibliography
    \midrule
    choice & Are INLINE CITATIONS linked to bibliography entries? (Please answer this questions considering only the bibliography entries that were extracted) (options: all linked, majority linked, half linked, most unlinked, none linked, no bibliography) \\
    text & \textit{Comment (Optional -- Note anything interesting about the inline citation linking; no need to list exhaustively)} \\
%     * All are linked
%     * Majority are linked
%     * About half are linked
%     * Most are not linked
%     * Pretty much none are linked
%     * No bibliography
%     * POOR bibliography extraction (less than half correct) (SKIP)
    \midrule 
    text & \textit{Are there any other problems with the HTML parse that are not covered by one of the above questions? Please describe. (Optional)} \\
    \midrule
    choice & Please rate the overall full text quality in the HTML render (options: no major problems, some problems, lots of problems -- see rubric in Section~\ref{app:quality_rubric}) \\
%     * No major problems that impact readability
%     * Some problems that impact readability
%     * Lots of problems that impact readability
    \bottomrule
\end{tabular}
\caption{Evaluation questions. Optional questions are in \textit{italics}.}
\label{tab:eval_questions}
% \Description{
% Answer; Questions 
% y/p/n; Is the TITLE correctly extracted? 
% text; Comment (clarify if answer is "partially" or "no") 
% y/p/n; Are the AUTHOR(S) correctly extracted? 
% text; Comment (clarify if answer is "partially" or "no") 
% y/p/n; Is the ABSTRACT correctly extracted?  
% text; Comment (clarify if answer is "partially" or "no") 
% y/n; Does this paper contain a substantial number of math EQUATIONS (more than 5 display equations)? 
% number; How many FIGURES are in the PDF? (Enter '0' if none) 
% number; How many FIGURES are correctly extracted? (Enter '0' if no figures in paper) 
% number; How many FIGURE CAPTIONS are correctly extracted? (Enter '0' if no figures in paper) 
% number; Approximately how many FIGURE captions are **INCORRECTLY** parsed into the body text (should be a figure caption but is mixed in with the body text)? (Enter '0' if they are all correct or if no figures) 
% text; Comment (Optional -- Note anything here about FIGURES or FIGURE CAPTIONS, e.g. which figures are not extracted, which figure captions are not extracted, which figure captions are incorrectly extracted into the body text etc.) 
% number; How many TABLES are in the PDF? (Enter '0' if none) 
% number; How many TABLES are correctly extracted? (Enter '0' if no tables in paper) 
% number; How many TABLE TITLES / CAPTIONS are correctly extracted? (Enter '0' if no tables in paper or if those tables do not have titles / captions) 
% number; Approximately how many TABLE titles are **INCORRECTLY** parsed into the body text (should be a table title / caption but is mixed in with the body text)? (Enter '0' if they are all correct or if no tables or table titles / captions) 
% number; Approximately how many TABLES have content that is **INCORRECTLY** parsed into the body text (content of table is mixed with the body text)? (Enter '0' if they are all correct or if no tables) 
% text; Comment (Optional -- Note anything here about TABLES or TABLE TITLES / CAPTIONS, e.g. which tables are not extracted, which table captions are not extracted, which table title / captions / content are incorrectly extracted into the body text etc.) 
% number; Approximately how many times are page HEADERS or FOOTERS *INCORRECTLY* mixed into the body text?This also includes margin content such as arXiv watermarks. (Enter '0' if all okay or no headers or footers) 
% text; Comment (Optional -- Note anything interesting here about incorrectly parsed headers or footers; no need to provide page numbers) 
% number; Approximately how many SECTION HEADINGS are *INCORRECTLY* extracted?  (Enter '0' if they are all correct or no section headings) 
% text; Comment (Optional -- Note anything interesting about the section heading extractions, no need to list exhaustively) 
% number; Approximately how many BODY TEXT PARAGRAPHS are **MISSING** from the extraction?  (Enter '0' if they are all there or there is no body text)  
% text; Comment (Optional -- Note anything interesting about the body text extractions) 
% choice; Are BIBLIOGRAPHY entries extracted correctly? (options: all correct, mostly correct, half correct, mostly incorrect, incorrect, no bibliography)  
% text; Comment (Optional -- Note anything interesting about the bibliography extractions; no need to list exhaustively) 
% choice; Are INLINE CITATIONS linked to bibliography entries? (Please answer this questions considering only the bibliography entries that were extracted) (options: all linked, majority linked, half linked, most unlinked, none linked, no bibliography) 
% text; Comment (Optional -- Note anything interesting about the inline citation linking; no need to list exhaustively) 
% text; Are there any other problems with the HTML parse that are not covered by one of the above questions? Please describe. (Optional) 
% choice; Please rate the overall full text quality in the HTML render (options: no major problems, some problems, lots of problems)
% }
\end{table}

\subsection{Quality rubric}
\label{app:quality_rubric}

The quality rubric for the final question in the evaluation form is given in Table~\ref{tab:quality_rubric}. This rating attempts to capture the overall readability and usability of the HTML render. Three authors discussed and converged upon this rubric following initial pilot annotations.

\begin{table}[h!]
    \small
    \centering
    \begin{tabular}{M{30mm}m{80mm}}
        \toprule
        Rating & Criteria \\
        \midrule
        No major problems that impact readability &
        \begin{itemize}[leftmargin=*]
            \item No errors or relatively few errors
            \item No missing paragraphs, but a few insertions into paragraphs or incorrect headers okay
            \item Any errors impact only a couple of paragraphs
        \end{itemize} \\
        \midrule
        Some problems that impact readability & 
        \begin{itemize}[leftmargin=*]
            \item Few missing paragraphs (<1 per 5 pages) OR Several figure/table insertions into paragraphs or incorrect headers
            \item Errors can impact multiple paragraphs
        \end{itemize} \\
        \midrule
        Lots of problems that impact readability & 
        \begin{itemize}[leftmargin=*]
            \item Difficult to read
            \item Multiple missing paragraphs OR multiple figure/table insertions that make some paragraphs unreadable
            \item Errors impact majority of paragraphs
        \end{itemize} \\
    \bottomrule
    \end{tabular}
    \caption{Rubric for HTML parse quality assessment (final question in evaluation questionnaire).}
    \label{tab:quality_rubric}
% \Description{
% Rating; Criteria 
% No major problems that impact readability; No errors or relatively few errors, No missing paragraphs, but a few insertions into paragraphs or incorrect headers okay, Any errors impact only a couple of paragraphs
% Some problems that impact readability; Few missing paragraphs (<1 per 5 pages) OR Several figure/table insertions into paragraphs or incorrect headers, Errors can impact multiple paragraphs
% Lots of problems that impact readability; Difficult to read, Multiple missing paragraphs OR multiple figure/table insertions that make some paragraphs unreadable, Errors impact majority of paragraphs
% }
\end{table}


\section{Evaluation results}
\label{app:eval_raw_results}

Raw counts for each type of error detected during the evaluation of HTML renders are provided in Table~\ref{tab:eval_raw_by_element}. The overall quality score split by field of study is shown in Table~\ref{tab:eval_raw_by_fos}.

\begin{table}[h!]
\small
    \centering
    \begin{tabularx}{\linewidth}{L{38mm}L{18mm}L{18mm}L{18mm}L{18mm}L{18mm}}
    \toprule
        \textbf{Metadata Element} & \textbf{Yes} & \textbf{Partially} & \textbf{No} & & \\
    \midrule
        Title & 337 & 16 & 32 & & \\
        Authors & 307 & 64 & 14 & & \\
        Abstract & 308 & 22 & 55 & & \\
    \midrule
        \textbf{Figure/Table Element} & \textbf{Skipped} & \textbf{No figures/tables} & \textbf{No errors} & \textbf{1 error} & \textbf{>1 error} \\
    \midrule
        Figure extraction errors & 6 & 94 & 201 & 45 & 39 \\
        Figure caption errors & 0 & 94 & 174 & 55 & 62 \\
        Table extraction errors & 2 & 166 & 165 & 32 & 20 \\
        Table caption errors & 2 & 166 & 190 & 23 & 4 \\
    \midrule
        \textbf{Text Element} & \textbf{Skipped} & \textbf{No errors} & \textbf{1-5 errors} & \textbf{>5 errors} & \\
    \midrule
        Header/Footer/Footnote errors & 3 & 170 & 172 & 40 & \\
        Section heading errors & 2 & 88 & 258 & 37 & \\
        Body paragarph errors & 1 & 226 & 128 & 30 & \\
    \midrule
        \textbf{Bibliography Element} & \textbf{Skipped/poor bib extraction} & \textbf{No bibliography} & \textbf{All or most correct} & \textbf{Half correct} & \textbf{Mostly incorrect} \\
    \midrule
        Bibliography extraction & 7 & 15 & 313 & 3 & 47 \\
        Inline citation linking & 39 & 10 & 290 & 20 & 26 \\
    \midrule
        \textbf{Overall Readability} & \textbf{Good} & \textbf{Okay} & \textbf{Bad} & & \\
    \midrule
        Overall score & 210 & 122 & 53 & & \\
    \bottomrule
    \end{tabularx}
    \caption{Assessment count for all evaluation paper elements. Corresponds to distributions shown in Figure~\ref{fig:eval_results}.}
    \label{tab:eval_raw_by_element}
% \Description{
% Metadata Element;  Yes;  Partially;  No; ; 
% Title; 337; 16; 32; ; 
% Authors; 307; 64; 14; ; 
% Abstract; 308; 22; 55; ; 
% Figure/Table Element;  Skipped;  No figures/tables;  No errors;  1 error;  >1 error  
% Figure extraction errors; 6; 94; 201; 45; 39 
% Figure caption errors; 0; 94; 174; 55; 62 
% Table extraction errors; 2; 166; 165; 32; 20 
% Table caption errors; 2; 166; 190; 23; 4 
% Text Element;  Skipped;  No errors;  1-5 errors;  >5 errors; 
% Header/Footer/Footnote errors; 3; 170; 172; 40; 
% Section heading errors; 2; 88; 258; 37; 
% Body paragarph errors; 1; 226; 128; 30; 
% Bibliography Element;  Skipped/poor bib extraction;  No bibliography;  All or most correct;  Half correct;  Mostly incorrect  
% Bibliography extraction; 7; 15; 313; 3; 47 
% Inline citation linking; 39; 10; 290; 20; 26 
% Overall Readability;  Good;  Okay;  Bad; ; 
% Overall score; 210; 122; 53; ; 
% }
\end{table}

\begin{table}[h!]
\small
    \centering
    \begin{tabularx}{0.73\linewidth}{L{32mm}L{16mm}L{16mm}L{16mm}X}
    \toprule
        \textbf{Overall Readability} & \textbf{Number of papers} & \textbf{Good} & \textbf{Okay} & \textbf{Bad} \\
    \midrule
        All papers & 385 & 210 & 122 & 53 \\
    \midrule
                  Art  &  13  &   6  &   1  &  6 \\
              Biology  &  23  &  12  &   7  &  4 \\
             Business  &  14  &   6  &   2  &  6 \\
            Chemistry  &  19  &  12  &   5  &  2 \\
     Computer science  &  21  &  10  &   7  &  4 \\
            Economics  &  20  &   6  &   8  &  6 \\
          Engineering  &  23  &  15  &   7  &  1 \\
Environmental science  &  18  &   7  &   8  &  3 \\
            Geography  &  17  &   9  &   6  &  2 \\
              Geology  &  21  &  12  &   8  &  1 \\
              History  &   7  &   5  &   1  &  1 \\
    Materials science  &  24  &  15  &   8  &  1 \\
          Mathematics  &  25  &  13  &   8  &  4 \\
             Medicine  &  26  &  14  &  12  &  0 \\
                Other  &   8  &   6  &   2  &  0 \\
           Philosophy  &  12  &   7  &   5  &  0 \\
              Physics  &  39  &  25  &  10  &  4 \\
    Political science  &  13  &   6  &   6  &  1 \\
           Psychology  &  22  &  11  &   7  &  4 \\
            Sociology  &  20  &  13  &   4  &  3 \\
    \bottomrule
    \end{tabularx}
    \caption{Distribution of overall quality scores for readability, split by field of study. Corresponds to distributions shown in Figure~\ref{fig:eval_fos}.}
    \label{tab:eval_raw_by_fos}
% \Description{
% Overall Readability; Number of papers; Good; Okay; Bad 
% All papers; 385; 210; 122; 53 
% Art; 13; 6; 1; 6 
% Biology; 23; 12; 7; 4 
% Business; 14; 6; 2; 6 
% Chemistry; 19; 12; 5; 2 
% Computer science; 21; 10; 7; 4 
% Economics; 20; 6; 8; 6 
% Engineering; 23; 15; 7; 1 
% Environmental science; 18; 7; 8; 3 
% Geography; 17; 9; 6; 2 
% Geology; 21; 12; 8; 1 
% History; 7; 5; 1; 1 
% Materials science; 24; 15; 8; 1 
% Mathematics; 25; 13; 8; 4 
% Medicine; 26; 14; 12; 0 
% Other; 8; 6; 2; 0 
% Philosophy; 12; 7; 5; 0 
% Physics; 39; 25; 10; 4 
% Political science; 13; 6; 6; 1 
% Psychology; 22; 11; 7; 4 
% Sociology; 20; 13; 4; 3 
% }
\end{table}

\section{Association between paper features and overall readability}
\label{app:eval_association}

To investigate the possibility of identifying paper extractions with major problems, we fit a Logistic Regression classifier using element specific evaluation results as input features, and whether or not a paper has major problems as the target class for classification. Element specific questions are converted into 43 binary input variables;
for example, the title element is mapped to three binary variables, whether the title is extracted correctly (\texttt{title\_yes}), extracted partially (\texttt{title\_partially}), or extracted incorrectly (\texttt{title\_no}). We collapse the targets into two binary classes, 1 if the paper has major problems, and 0 if it has no major problems or some problems. The classifier is trained using 5-fold cross validation, with balanced class weights, and achieves a mean accuracy of 0.69, and area under the ROC of 0.65. This performance is not particularly notable or good; the labeled training sample is small, and due to the complexity of what makes a document problematic to read, we did not expect there to be a clear way to predict extractions with major problems based on a small number of element-level features. Something we aim to explore more in the future is whether the raw tokens on the PDF or publisher metadata can be leveraged to better predict when our extractive parse has failed.

\begin{table}[h!]
    \centering
    \begin{tabular}{r|ccc|c}
        \toprule
        Class & Precision & Recall & F1-score & Support \\
        \midrule
        No major problems / Some problems & 0.91 & 0.71 & 0.80 & 332 \\
        Major problems & 0.23 & 0.55 & 0.32 & 53 \\
        \bottomrule
    \end{tabular}
    \caption{Precision, recall, and F1-scores for classification. The classifier does not perform well at identifying papers with major problems from element-based features (F1 = 0.32).}
    \label{tab:logreg_majorproblems}
% \Description{
% Class; Precision; Recall; F1-score; Support 
% No major problems / Some problems; 0.91; 0.71; 0.80; 332 
% Major problems; 0.23; 0.55; 0.32; 53 
% }
\end{table}

The top 10 predictive features and their logistic regression weights are:

\vspace{4mm}
\centering{
\begin{tabular}{rl}
    Abstract extracted incorrectly: & 0.42 \\
    One table extraction error: & 0.18 \\
    No table caption errors: & 0.15 \\
    One figure extraction error: & 0.15 \\
    One figure caption extraction error: & 0.15 \\
    Bibliography extraction is very bad: & 0.13 \\
    More than one figure extraction error: & 0.13 \\
    Authors extracted incorrectly: & 0.12 \\
    More than one table extraction error: & 0.11 \\
    Authors extracted correctly: & 0.09 \\
\end{tabular}}
\vspace{4mm}

\justifying
The most predictive feature is when abstracts are extracted incorrectly. Given the prevalence of abstracts in various literature databases, abstract quality could be easily assessed through external verification. In other words, if the abstract we extract is different from the abstract found for the same paper in other databases on in the publisher metadata, perhaps we can avoid surfacing this paper. However, the distribution of weights among various other element-level features suggests that this feature alone would be insufficient, and that the contributions of these various features are complex, denying us an easy way of identifying paper parses with major problems.

\section{User study materials}
\label{app:user_study_supplement}

Documents used for the user study are provided in this Appendix.

\subsection{Recruitment email}
\label{app:recruitment_email}

The following email was sent and forwarded to several mailing lists to recruit participants.

\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}
\begin{quote}
The \semanticscholar Research Team at the \allenai is conducting an experiment to evaluate the screen reader accessibility of scientific papers.

\vspace{4pt}\noindent We are looking for participants who are age 18 or older, who identify as blind or low vision, and who have experience using screen readers to interact with scientific papers. If you are interested in participating, please complete the following form to determine eligibility: \underline{link}

\vspace{4pt}\noindent Participation in this study is entirely voluntary. If you do decide to participate, your individual data will be kept strictly confidential and will be stored without personal identifiers.

\vspace{4pt}\noindent The study involves an informational interview to better understand screen reader needs around scientific papers. Each participant will also be asked to interact with papers on a web interface developed by the team. The study will take approximately 75 minutes, and participants will receive a \$150 Amazon gift card for their time.

\vspace{4pt}\noindent Location: Online (Zoom)

\vspace{4pt}\noindent Please contact the authors if you have any questions or concerns about this study. Thank you in advance for your time! Please help us spread the word by forwarding as appropriate.
\end{quote}
\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}

\subsection{Pre-interview questionnaire}
\label{app:pre_interview_questionnaire}

Prior to each user study interview, the participant was asked to complete the following form:

\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}
\begin{quote}
\textbf{Share 3 to 5 scientific papers that are difficult to read due to accessibility issues}

\vspace{4pt}\noindent Thank you for volunteering to take part in this study! Please take a few minutes to supply us with some subject keywords you are interested in, and a list of 3 to 5 scientific papers you have found difficult to read due to accessibility issues. This would help us better plan the study based on your experience.

\vspace{4pt}
\begin{enumerate}
    \item[1.] Your name (First name, last initial)
    \item[2.] Please give a few examples of subject keywords you care about. \\ \small For example, computing hardware, analog computer, etc. \normalsize
    \item[3.] Share one paper you have had difficulty reading due to accessibility issues by answering the following questions.
    \begin{itemize}
        \item[--] Paper title \& link \\ \small For example, ``What every Researcher should know about Searching - Clarified Concepts, Search Advice, and an Agenda to improve Finding in Academia'' (https://pubmed.ncbi.nlm.nih.gov/33031639/) \normalsize
        \item[--] On a scale of 1 to 5, how easy or difficult was it for you to read this paper? \\ \small (1 = very easy, 5 = very difficult) \normalsize
        \item[--] Briefly describe why you chose the rating
    \end{itemize}
    \item[4--7.] \textit{Repeat 3}.
\end{enumerate}
\end{quote}
\begin{center}
\noindent \rule{0.9\linewidth}{0.4pt}
\end{center}

\subsection{Interview questions}
\label{app:interview_questions}

The following discussion guide is used to provide structure for user interviews. \\

\textbf{Phase I -- Warmup}:
\begin{itemize}
    \item Can you tell us a little bit about yourself? (Background, what kind of research do you do)
    \item Tell us about how you normally read papers - What is your workflow like? What tools do you use? 
    \begin{itemize}
        \item Do you usually read PDFs directly or do you read papers in other ways?
    \end{itemize}
    \item If you need to read a paper and it is not accessible, what do you do now?
    \begin{itemize}
        \item How long does the process take?
        \item How often is it successful?
    \end{itemize}
    \item Can you give a few examples of the main challenges you face when reading papers?  (For example, are there certain features or attributes of papers that make them particularly difficult to read?) 
    \item In your opinion, are there any resources that provide papers that are easier to read by screen readers? (For example, any journals, conferences, or search engines?)
    \item Overall, how do you feel about your current experience of reading papers? 
\end{itemize}

\textbf{Phase I -- Current workflow}:
\begin{itemize}
    \item Based on the list of papers you provided, walk us through how you would read the paper \texttt{[abc]}. Use the screen reader of your choice, and any additional tools or extensions that are part of your usual process.
    \item Instructions: Please share your whole screen, think aloud and walk me through your thinking process
    \item What kind of information were you looking for, and how did you explore the page to find the information?
    \item On a scale of 1 to 5, how easy or difficult was it to read this paper with the tools, and why? (1 = very easy, 5 = very difficult)
    \item If you could change anything, how could this best meet your needs?
\end{itemize}

\textbf{Phase II}:
\begin{itemize}
    \item We are currently working on an experimental prototype to make papers more easily read by screen readers. Please take a minute to read the about page first: \underline{link}
    \item Based on the list of papers you provided, walk us through how you would read the paper \texttt{[abc]} using this HTML render. You can also use the screen reader of your choice, and any additional tools or extensions that are part of your usual process.
    \item Instructions: 
    \begin{itemize}
        \item We are working with prototypes so not everything works
        \item Please think aloud and walk me through your thinking process
        \item Feel free to provide as many feedback as you can, good or bad
    \end{itemize}
    \item Please take a few more minutes to explore the other parts of this prototype. (e.g. References)
    \item On a scale of 1 to 5, how easy or difficult was it to read this paper with the tools, and why? (1 = very easy, 5 = very difficult)
\end{itemize}

\textbf{Phase III}:
\begin{itemize}
    \item On a scale of 1 to 5, how likely are you to use the HTML render, if it is available to you in the future? (1 = very unlikely, 5 = very likely)
    \item Which features do you consider to be most helpful?
    \item Is there anything it would need to have, or change to convince you to use it?
    \item How do you envision yourself using this tool? How might it fit into your workflow? (For example, would it be an additional extension that is part of your usual process?) 
    \item If you could search for papers and view them in this format, what do you think? 
    \item If you could upload any PDF and create an HTML page like this, what do you think? (Would that be helpful for you, or something you might use, why or why not?)
    \item Are you aware of any other tools that display papers in any way besides PDF?
    \item Do you have any additional feedback about the HTML render or anything else that you would like to share?
    \item Thank you
\end{itemize}