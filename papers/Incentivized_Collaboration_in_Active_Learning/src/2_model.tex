Throughout the work, we consider the binary classification problem. 
Let $\cX$ denote the input space, $\cY =\{0,1\}$ denote the label space, and $\cH\subset \cY^\cX$ denote the hypothesis class.
We focus on the \textit{realizable} setting in this work, namely, there exists a target hypothesis $h^*\in \cH$ correctly labels every point.
In the pool-based active learning setting, given a collected unlabeled data set $X = \{x_1,\ldots,x_m\}$, the learning goal is to recover the labels of $X$.
Now just suppose the pool of unlabeled data $x_1,\ldots,x_m$ is available.
The possible labelings of these points form a subset of $\{0,1\}^m$, called the effective hypothesis class, which is 
\[\hat H = \{h(X)|h\in \cH\}\,,\]
where $h(X) = (h(x_1),\ldots, h(x_m))$ is the labeling of $X$ by $h$. Note that $|\hat H|\leq 2^m$
and $|\hat H|= \cO(m^d)$ if the VC dimension of $\cH$ is $d$.

In this work, we focus on the Bayesian setting~\citep{dasgupta2004analysis}, where
the target hypothesis is chosen in advance 
from some prior distribution $\pi$ over $\hat H$. Namely, without any additional information, for any labeling $h\in \hat H$, the probability that $h$ is the correct labeling of $X$ is $\pi(h)$. Since we can  eliminate any hypothesis $h$ with $\pi(h)=0$ before starting to query for labels, we assume w.l.o.g. that $\pi(h)>0$ for all effective hypotheses in $\hat H$.


%

\textbf{Standard active learning model} In the standard pool-based active learning setting, 
a single agent owns the pool of unlabeled data $X$. 
The agent, who knows both $\hat H$ and $\pi$, can query the labels of points in $X$, and her goal is to recover the labeling of $X$ (or to find the target hypothesis) by querying as few points as possible. 

A \textit{standard query algorithm} receives as input the prior distribution $\pi$ and unlabeled data set, $X$. In each iteration  $t=1,2,\ldots$, given the history up to time $t$, $$\cF_t =((x_1,y_1),\ldots,(x_{t-1},y_{t-1}))\in (X \times \{0,1\})^{t-1},$$ it selects a point $x_t$ to query and observes its label, $y_t$.  
The algorithm stops when all the labels of $X$ are recovered. Alternatively, the algorithm stops when for every two hypotheses $h_1,h_2\in \hat H$ consistent with $\cF_t$ (meaning that $h_1(x_\tau) = h_2(x_\tau) = y_\tau$ for all $\tau=1,\ldots, t-1$), $h_1(X)=h_2(X)$. 





\textbf{Collaborative active learning model.}
In the collaborative setting, we assume there is more than one agent.  
Formally, there are $k$ agents and each agent $i$ has an individual unlabeled data set $X_i$ such that they together compose the pool, i.e., $\cup_{i\in [k]}X_i = X$, and each can query points from their own set $X_i$ (but cannot query points which are not in their set).
The goal of each agent is to recover the true labeling of their own set while performing as few queries as possible. 
The collaboration protocol, also called principal, who knows $\xset$, $\hat{H}$ and $\pi$, decides which point should be queried at each iteration, and her goal is to recover all the labels of $X$ using as few queries as possible. 
We remark that since data points belong to agents, queries of any point $x\in X$ can only be performed by agents whose data set contains $x$.

The query algorithm in the collaborative setting is similar to that in the standard setting, except that the algorithm needs to coordinate among the agents and decide which agent will query each point as some data points might belong to more than one agent.
In this setting, agents can decide to join the collaboration or learn individually at the beginning of the learning. But if they join the collaboration, they commit to follow the instructions of the query algorithm.
Therefore, given a prior distribution $\pi$ over $\hat H$ and a set of agents who would join the collaboration, w.l.o.g. denoted as $\{X_1,\ldots,X_\kappa\}$ for some $\kappa\in [k]$, at time $t=1,2,\ldots$, a \textit{collaborative query algorithm} asks agent $i_t\in [\kappa]$ such that $x_t\in X_{i_t}$ to query point $x_t$, and 
observes its label, $y_t$; the algorithm stops when the labels of points in $\cup_{i\in [\kappa]}X_i$ are completely recovered.

It is straightforward to check that standard query algorithms are a special case of collaborative query algorithms when there is a single agent, i.e., $k=1$. Additionally, a standard algorithm can also be run over multiple agents by considering the union of their data $\cup_{i\in [M]} X_i$ as a single agent.
Hence, we omit ``standard'' or ``collaborative'' in a query algorithm when it is clear from the context how many agents are involved.

For any collaborative algorithm $\cA$, given an input $\pi$ and any collection of unlabeled data sets $X_1,\ldots,X_\kappa\subseteq X$ of size $\kappa\geq 1$, we denote by $Q(\cA,\pi,\{X_1,\ldots,X_\kappa\}, h)$ the label complexity (number of label queries) of $\cA(\pi,\{X_1,\ldots,X_\kappa\})$ when the target hypothesis is $h$. 
For randomized algorithms, the label complexity is taken expectation over the randomness of the algorithm. 
We define the label complexity as follows.
\begin{definition}[Label complexity] 
Given any fixed unlabeled pool and effective hypothesis class $(X,\hat H)$, for any algorithm $\cA$, prior distribution $\pi$ over $\hat H$ and any collection of unlabeled data sets $X_1,\ldots,X_\kappa\subseteq X$ of size $\kappa\geq 1$, 
the label complexity of $\cA$ with $(\pi, \{X_1,\ldots,X_\kappa\})$ as input, denoted by $Q(\cA,\pi, \{X_1,\ldots,X_\kappa\})$, is the expected number of label queries when $h$ is drawn from the prior $\pi$, i.e.,
    \[Q(\cA, \pi, \{X_1,\ldots,X_\kappa\}) = \EEs{h\sim \pi}{Q(\cA, h, \{X_1,\ldots,X_\kappa\})}\,.\]
    For each agent $i\in [\kappa]$ in the collaboration, we let $Q_i(\cA, \pi, \{X_1,\ldots,X_\kappa\})$ denote the expected number of queries performed by agent $i$.
\end{definition}
For any $(\pi, \{X_1,\ldots,X_\kappa\})$, let $Q^*(\pi, \{X_1,\ldots,X_\kappa\})=\min_{\cA} Q(\cA,\pi, \{X_1,\ldots,X_\kappa\})$ denote the optimal query complexity.
An algorithm $\cA$ is said to be \textit{optimal} if $Q(\cA, \pi,\{X_1,\ldots,X_\kappa\}) = Q^*(\pi, \{X_1,\ldots,X_\kappa\})$ for any prior distribution $\pi$ and $X_1,\ldots,X_\kappa$.

\textbf{Rational agents}
We assume that agents have access to a baseline algorithm and are able to run it on their own local data. 
Agents can decide to join the collaboration or run the baseline individually at the beginning of the learning.
If they join the collaboration, they commit to follow the instructions of the query algorithm. 
Each agent is incentivized to join the collaboration if she could perform fewer label queries (assuming that all others join the collaboration) by pulling out and running the baseline $\cA$ individually. 
Formally, 
\begin{definition}[Individual rationality]
    In a collaborative learning problem with prior distribution $\pi$ and $k$ agents $\xset$, given a baseline algorithm $\cA$,
    a collaborative algorithm $\cA'$ is individually rational (IR) if 
\begin{equation}
    {Q_i(\cA',\pi, \xset)} \leq Q(\cA, \pi, \{X_i\}), \forall i\in [k]\,.\label{eq:def-incentive}
\end{equation}
\end{definition}
We say 
 $\cA'$ is \textit{strictly individually rational} (henceforth, SIR) if
\begin{equation*}
    {Q_i(\cA',\pi, \xset)} < Q(\cA, \pi, \{X_i\}), \forall i\in [k]\,.
\end{equation*}
We remark that in addition to their own sets, each agent knows all the unlabeled data sets,  $\xset$, and the prior distribution $\pi$, otherwise they will not be able to compute $Q_i(\cA',\pi,\xset)$. The principal also has access to $\xset$ and $\pi$ and can therefore make sure these constraints are satisfied. 




An alternative interpretation of the problem in a game theoretic framework is as follows: each agent has a strategy space of two strategies, joining the collaboration and not.
The utility of an agent that performs $Q$ queries is $-Q$.
If the algorithm $\cA$ is IR, then the case of all agents joining the collaboration is a Nash equilibrium (since switching to not joining will not increase their utility).
If $\cA$ is SIR, then all agents joining the collaboration is a strict Nash equilibrium.



