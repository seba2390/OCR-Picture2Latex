In Section~\ref{sec:ir},
we provided a generic scheme for constructing an IR algorithm given any baseline algorithm.
In this section, we focus on constructing SIR algorithms given IR algorithms.
Since strict individual rationality requires that agents strictly benefit from collaboration, this is impossible without further assumptions. 
% 
For example, consider a set of agents who only have one single independent point in their own sets 
and a prior distribution that is uniform over all labelings. 
In this case, each agent, regardless of whether she collaborates or not, has a label complexity of $1$ and cannot \emph{strictly} benefit from collaboration as the other agents cannot obtain information about her data.

Now, let us consider a notion weaker than SIR, called $i$-partially SIR, in which only agent $i$ strictly benefits from the collaboration, and any other agent $j\neq i$ does not get worse by joining the collaboration.
More formally, 
\begin{definition}[Partially SIR algorithms]
    For any baseline algorithm $\cA$, for all $i\in [k]$, an algorithm $\cO_i$ is $i$-partially SIR, if $\cO_i$ satisfies that
    \[Q_i(\cO_i, \pi, \xset) < Q(\cA,\pi, \{X_i\})\,,\]
    and 
    \[Q_{j}(\cO_i,\pi,\xset)\leq Q(\cA,\pi,\{X_j\}), \forall j\in [k]\setminus \{i\}\,.\]
\end{definition}
If we are given an $i$-partially SIR algorithm $\cO_i$ for each $i$, then we can construct a SIR algorithm by running a mixture of an IR algorithm $\cA'$ (e.g., $\ir(\cA)$ in Algorithm~\ref{alg:basic2ir}) and $\{\cO_i|i\in [k]\}$ with the label complexity a little (arbitrarily small) higher than that of $\cA'$.
\begin{lemma}\label{lmm:partsir}
    For any baseline algorithm $\cA$, given an IR algorithm $\cA'$ and partially SIR algorithms $\{\cO_i|i\in [k]\}$, for any $\epsilon>0$, let $\cA''_\epsilon$ be the algorithm of running $\cA'$ with probability $(1-\frac{\epsilon}{n})$ and running $\cO_i$ with probability $\frac{\epsilon}{kn}$.
    Then $\cA''_\epsilon$ satisfies the following properties.
    \begin{itemize}[topsep=0pt,itemsep =-0.5ex,  leftmargin = 6mm]
        \item SIR property: $\cA''_\epsilon$ is SIR with respect to the baseline algorithm $\cA$.
        \item Label complexity: $Q(\cA''_\epsilon, \pi, \xset)\leq Q(\cA', \pi, \xset) +\epsilon$.
    \end{itemize}
\end{lemma}
The proof is straightforward from the definition, and we include it in Appendix~\ref{app:part-sir} for completeness.
Since a SIR algorithm is also $i$-partially SIR for all $i\in [k]$, constructing a SIR algorithm is equivalent to constructing a set of partially SIR algorithms $\{\cO_i|i\in [k]\}$.
Therefore, the problem of constructing a SIR algorithm is reduced to constructing partially SIR algorithms $\{\cO_i|i\in [k]\}$. 

For the remainder of this section, we will present the SIR results for an optimal baseline algorithm in Section~\ref{subsec:sir-opt}, where we propose a sufficient and necessary assumption for the existence of SIR algorithms and then provide a SIR algorithm.
This algorithm is SIR w.r.t. any baseline algorithm but again, computationally inefficient.
In Section~\ref{subsec:sir-apprx}, we provide a general scheme that transforms any baseline algorithm into a SIR algorithm.



\subsection{A Universal SIR Algorithm for Any Baseline Algorithm}\label{subsec:sir-opt}
\vspace{-0.5em}
%
Constructing a universal SIR algorithm w.r.t. any baseline is equivalent to constructing a SIR algorithm for an optimal baseline.
For the existence of SIR algorithms given any optimal baseline algorithm, we propose the following assumption, which is sufficient and necessary. We include the proof for the necessity of this assumption in Appendix~\ref{app:nece-asp}. The sufficiency of this assumption will be verified immediately after we construct a SIR algorithm.
\begin{assumption}\label{asp:clb-help}
We assume that for any $i\in [k]$, the optimal label complexity of agent $i$ given the information regarding the labels of all other agents is strictly smaller than that without this additional information, i.e.,
    $Q^*(\pi, \{X_i\})- \EEs{h\sim \pi}{Q^*(\pi_{h,-i}, \{X_i\})} >0\,,$
    where $\pi_{h,-i}$ is the posterior distribution of $\pi$ after observing $\{(x,h(x))|x\in \cup_{j\neq i} X_{j}\}$. 
\end{assumption}


According to Lemma~\ref{lmm:partsir}, we can construct a SIR algorithm by constructing a set of partially SIR algorithms $\{\cO_i|i\in [k]\}$. 

Let $\cO_i$ be the algorithm of running an optimal algorithm $\OPT$ over $(\pi, \{X_j|j\neq i\})$ first, then given the query-label history of $\{(x, h(x))|x\in \cup_{j\neq i} X_j\}$ for some $h\in \hat H$, run $\OPT$ over $(\pi_{h,-i}, \{X_i\})$. Then it immediately follows that $\cO_i$ is $i$-partially SIR from Assumption~\ref{asp:clb-help}. Let $\OPT_\epsilon''$ denote the algorithm of of running $\OPT$ with probability $(1-\frac{\epsilon}{n})$ and running $\cO_i$ with probability $\frac{\epsilon}{kn}$ for all $i\in [k]$. By Lemma~\ref{lmm:partsir}, we have

\begin{corollary}
    Under Assumption~\ref{asp:clb-help}, for any $\epsilon>0$, $\OPT_\epsilon''$ is SIR w.r.t. $\OPT$ and satisfies $$Q(\OPT''_\epsilon, \pi, \xset)\leq Q^*(\pi, \xset) +\epsilon.$$
    In addition, $\OPT_\epsilon''$ is SIR w.r.t. any baseline algorithm $\cA$ as $$Q_i(\OPT''_\epsilon, \pi, \xset)< Q^*(\pi, \{X_i\}) \leq Q(\cA,\pi,\{X_i\}).$$
\end{corollary}



\subsection{A Scheme of Converting Algorithms to SIR Algorithms
}\label{subsec:sir-apprx}
\vspace{-0.5em}
As mentioned before, computing an optimal algorithm is NP-hard. Assumption~\ref{asp:clb-help} assumes that collaboration can strictly benefit agents when the collaboration protocol can compute the optimal algorithm given $\pi_{h,-i}$.
Hence, the assumption does not take the computational issue into consideration and thus might not be enough for the existence of an efficient SIR algorithm w.r.t. an efficient approximation algorithm like GBS.

Instead, we propose prior-independent assumption that is sufficient for the existence of efficient SIR algorithms when we are given an efficient baseline and an efficient IR algorithm w.r.t. the baseline. 
Basically, we assume that, there exists an effective hypothesis $h\in \hat H$, given the information that all other agents are labeled by $h$, the number of labelings of $X_i$ consistent with the label information is strictly smaller than the total number of labelings of $X_i$ by $\hat H$. Formally, for any $i\in [k]$, let $X_{-i} = \cup_{j\neq i} X_j$ denote the union of all agents' data except agent $i$.
Let $H(X_i)=\{h'(X_i)|h'\in \hat H\}$ denote the effective hypothesis class of $X_i$, i.e., all labelings of $X_i$.
For any $h\in \hat H$, let $H(X|h) = \{h'(X_i)|h'(X_{-i}) = h(X_{-i}),h'\in \hat H\}$ denote the subset which are consistent with all other agents being labeled by $h$.

\begin{assumption}\label{asp:clb-help-efficient}
    For all $i\in [k]$, there exists an $h\in \hat H$ s.t.  
    the number of labelings of $X_i$ consistent with $(X_{-i}, h(X_{-i}))$ is strictly smaller than the number of labelings by $\hat H$, i.e., $\abs{H(X_i|h)} < \abs{H(X_i)}$.
\end{assumption}
Notice that each deterministic query algorithm $\cA$ can be represented as a binary tree, $\cT_\cA$ whose internal nodes at level $t$ are queries (``what is the $x_t$'s label?''), and whose leaves are labelings as illustrated in Figure~\ref{fig:query-tree}.
Under Assumption~\ref{asp:clb-help-efficient}, we can prune the query tree of $\cA(\pi, \{X_i\})$ by removing
all subtrees whose leaves are all in $H(X_i)\setminus H(X_i|h)$.
We don't really need to construct this pruned tree when we implement the algorithm. At time $t$, we just need to generate an $x_t$ from $\cA(\pi, \{X_i\})$, then check if this node should be pruned by checking if all the hypotheses $H(X_i|h)$ agree on the label of $x_t$. If this is true, it means that we have already recovered the label of $x_t$ and thus we just need to feed the label to the algorithm without actually querying $x_t$ again.
Then we can construct a $i$-partially SIR algorithm $\cO_i$ by running $\ir(\cA)$ over $(\pi, \{X_j|j\neq i\})$ to recover the labeling of $X_{-i}$ first, then running pruned version of $\cA(\pi, \{X_i\})$. Note that the implementation also works when $\cA$ is randomized.

\begin{lemma}\label{lemma:OiIsI-Part-SIR}
Under Assumption~\ref{asp:clb-help-efficient}, the algorithm $\cO_i$ constructed above is $i$-partially SIR and runs in time $\bigO(\cT_{\ir(\cA)} + m (|\hat H| + T_{\cA,0}))$ time, where $\cT_{\ir(\cA)}$ is the running time of $\ir(\cA)$ and $T_{\cA,0}$ is the time of computing $(i_t,x_t)$ at each time $t$ for $\cA$.

\end{lemma}
The proof of Lemma~\ref{lemma:OiIsI-Part-SIR} is deferred to Appendix~\ref{app_lmm3}.

We can then construct an algorithm $\cA_\epsilon''$ by running $\ir(\cA)$ with probability with probability $(1-\frac{\epsilon}{n})$ and running $\cO_i$ constructed in the above way with probability $\frac{\epsilon}{kn}$ for all $i\in [k]$. By combining Lemmas~\ref{lmm:partsir} and \ref{lemma:OiIsI-Part-SIR}, we derive the following theorem. Then, combining it with Corollary~\ref{crl:ir-greedy}, we derive a SIR algorithm for GBS as baseline GBS.
\begin{theorem}
Under Assumption~\ref{asp:clb-help-efficient}, for any baseline algorithm $\cA$, for any $\epsilon>0$, $\cA_\epsilon''$ is SIR and satisfies $$Q(\cA''_\epsilon, \pi, \xset)\leq Q(\ir(\cA),\pi, \xset) +\epsilon.$$
In addition, Algorithm $\cA_\epsilon''$ runs in $\bigO(\cT_{\ir(\cA)} + m (|\hat H| + T_{\cA,0}))$ time.
\end{theorem}

\begin{corollary}
Given GBS as the baseline, Algorithm $\gbs_\epsilon''$ is SIR; runs in $\bigO(k m^2 |\hat H|)$ time; and satisfies that $$Q(\gbs_\epsilon'', \pi, \xset)\leq 4Q^*(\pi, \xset) \ln\left(\frac{1}{\min_{h\in \cH}\pi(h)}\right) +\epsilon.$$
\end{corollary}

