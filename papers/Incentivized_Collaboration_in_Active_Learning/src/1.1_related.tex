%stratigic behaviour
The most related work is the recent work of \cite{xu2023fair}, which studies individual rationality in collaborative active learning in a Gaussian Process. While their notion of IR is similar to ours, we focus on query complexity in binary classification. \cite{echenique2019incentive} studied incentive compatibility in active learning, where there is a single agent that responds to a learner's query strategically.
Our work is situated at the junction of Learning in the presence of strategic behavior and active learning. 

\textit{Learning in the presence of strategic behavior}  encompasses a vast body of research, including~\citep{ben-porat23, Zhang22,hardt2016strategic}. 
We are particularly driven by prior research %on learning in the presence of strategic behavior
in this area, and how to create learning algorithms that incentivize agents to participate while maximizing the overall welfare.  For example, \textit{incentivized exploration} in Multi Arm Bandits~\citep{Kremer-JPE14,MansourSS15,MansourSSW16,MansourSW18, Cohen19,Che-13,BaharST16,Bahar2019FiduciaryB,Bahar19,Immorlica19,Immorlica20,Sellke21,banihashem2023bandit,Slivkins17,slivkins2019introduction} or MDPs~\citep{simchowitz2023exploration}, where the principal recommends actions to the agents (in order to explore different alternatives), but the agents ultimately decide whether to follow the given recommendation. This raises the issue of incentives in addition to the exploration-exploitation trade-off. In particular,\citep{Baek21} study this problem in the context of fairness with a group-based regret notion. They show that regret-optimal bandit algorithms can be unfair and design a nearly optimal fair algorithm. Incentivizing agents to share their data has been studied by \cite{wei2023incentivized} in federated bandits.

%incentives in federated learning
\textit{Federated learning}  has gained popularity as a method to foster collaboration among large populations of learning agents among else for incentivizing participation and fairness purposes ~\citep{BlumHPS21,Lyu2020,Donahue_Kleinberg_2021,Donahue22Fair,DonahueK21,Donahue23}. 
%kidney exachange
Another related line of research is \textit{kidney exchange}~\citep{Roth04,AshlagiR11,BlumIHPPV17kidney,BlumG21,BlumMansour20,DickersonPS19}, where the goal is to find a maximum match in a directed graph (representing transplant compatibilities between patient–donor pairs). In this problem, incentives arise in the form of individual rationallity when different hospitals have different subsets of patient–donor pairs, and will not join the collaboration if the number of pairs matched by the collaboration is lower than the number of pairs matched they could pair on their own.


%pool-based active learning
There are two basic models in \textit{active learning}-- stream-based~\citep{FreundSST97} (where the learner has to determine immediately whether to query the label of the current instance  or discard it), and pool-based, which is the basis for our model. Pool-based active learning investigates scenarios in which a learner is confronted with an array of unlabeled data points and the goal is to recover a target function by querying the labels of these points (see~\citep{Hanneke14survey} for a survey). Active learning has been studied in the context of other societal desiderata such as  fairness~\citep{ShenCW22}, and safety~\citep{CamilleriWMJJ22}. 


To our knowledge, no research has amalgamated these fields to explore strategic constraints in the context of active learning. This is where our work makes a valuable contribution.



%collabartive/federal

