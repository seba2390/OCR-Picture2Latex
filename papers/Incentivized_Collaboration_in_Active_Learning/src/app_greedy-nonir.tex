\section{Proof of Theorem~\ref{thm:greedyir}}\label{app:greedyir}
% \greedyir*
\begin{proof}
    The construction is inspired by~\cite{dasgupta2004analysis}.
    Consider $k=2$ and let the unlabeled data set of agent $1$ be 
    \[X_1 = \{(0,1,0),(0,2,0), (0,0,1), (0,0,2),\ldots, (0,0,n)\}\]
    for some $n\in \NN_+$.
    
    Let  the unlabeled data set of 
    agent $2$ be
    \[X_2 = \{(1,0,0)\}\,.\]
    Let the unlabeled pool $X = X_1\cup X_2$.
    Let $h_{i,j,l}$ denote the hypothesis which labels $(i,0,0),(0,j,0),(0,0,l)$ as $1$ and the rest as $0$.
    
    Let the hypothesis class be $\cH = \{h_{i,j,l}|i\in \{0,1\}, j\in [2], l\in [n]\}$.
    
    Let the prior distribution $\pi_0 = \pi$ be defined as follows:
    \begin{equation*}
        \begin{cases}
            \pi(h_{0,0,0}) = \frac{1}{4}&\\
            \pi(h_{0,j,l}) = \frac{1}{4 \cdot 3^l}& \text{ for } j=1,2, l=1,\ldots,n-1\\
            \pi(h_{0,j,n}) = \frac{1}{8\cdot 3^{n-1}}& \text{ for } j=1,2\\
            \pi(h_{1,1,l}) = \frac{1}{3^l}& \text{ for } l=1,\ldots,n-1\\
            \pi(h_{1,1,n}) = \frac{1}{2\cdot 3^{n-1}}\,.&
        \end{cases}
    \end{equation*}
   
    
    Now we show that the label complexity of agent $1$ in the collaboration is $Q_1(\gbs, \pi, \{X_1,X_2\}) = \Omega(n)$. While the label complexity of running GBS itself is $Q(\gbs,\pi, \{X_1\}) = \bigO(1)$.

    \paragraph{Label complexity of agent $1$ in the collaboration} Let $\VS$ denote the version space. And for any point $x$, let $\VS_{x}^+ = \{h\in \VS|h(x) =1\}$ denote the subset of the version space which labels $x$ by $1$. Similarly, let $\VS_{x}^- = \{h\in \VS|h(x) =0\}$.

    Now let us consider the length of the %query
    path in the query tree when the target hypothesis is $h_{0,0,0}$.
    
    A-priori (before starting to query), for point $(1,0,0)$, we have 
    \begin{equation*}
        \pi(\VS_{(1,0,0)}^+) = \sum_{l=1}^n \pi(h_{1,1,l}) = \frac{1}{3} +\frac{1}{3^2} +\ldots +\frac{1}{3^{n-1}} +\frac{1}{2\cdot 3^{n-1}} =\frac{1}{2}\,.
    \end{equation*}   
    For point $(0,1,0)$, we have
    \[\pi(\VS_{(0,1,0)}^+) = \sum_{l=1}^n (\pi(h_{0,1,l}) + \pi(h_{1,1,l}))= \sum_{l=1}^n \pi(h_{0,1,l}) + \pi(\VS_{(1,0,0)}^+) >\frac{1}{2}\,.\]
    For point $(0,2,0)$, we have
    \[\pi(\VS_{(0,2,0)}^+) = \sum_{l=1}^n \pi(h_{0,2,l}) = \frac{1}{4\cdot 3} +\frac{1}{4\cdot 3^2} +\ldots +\frac{1}{4\cdot 3^{n-1}} +\frac{1}{8\cdot 3^{n-1}} =\frac{1}{8}\,.\]
    For other points $(0,0,l)$ for $l\in [n-1]$, we have $\pi(\VS_{(0,0,l)}^+) = \frac{1}{4\cdot 3^l} + \frac{1}{3^l} = \frac{5}{4\cdot 3^l}<\frac{1}{2}$ and for $(0,0,n)$, we have $\pi(\VS_{(0,0,n)}^+)<\pi(\VS_{(0,0,n-1)}^+)<\frac{1}{2}$.
    
    Therefore, the algorithm $\gbs(\pi,\{X_1,X_2\})$ will query $(1,0,0)$ at time $1$.
    Suppose the label of $(1,0,0)$ is $0$ since we consider the true target hypothesis to be $h_{0,0,0}$.
    
    Now we show that $\gbs(\pi,\{X_1,X_2\})$ will query points $(0,0,1), (0,0,2),\ldots, (0,0,n)$ sequentially by induction.
    
    At time $1$, the version space is $\VS = \{h_{0,0,0}\}\cup \{h_{i,j,l}\in \hat H|i=0\}$. We list $\pi(h_{0,j,l})$ for $j\in \{1,2\}$ and $l\in [n]$ in Table~\ref{tab:pi-0jl} for illustration.
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c}
             & $(0,1,0)$ & $(0,2,0)$\\\hline
            $(0,0,1)$ & $\frac{1}{4 \cdot 3}$ & $\frac{1}{4 \cdot 3}$\\\hline
            $(0,0,2)$ &$\frac{1}{4 \cdot 3^2}$ & $\frac{1}{4 \cdot 3^2}$ \\\hline
            $\cdots$ & $\cdots$ & $\cdots$ \\\hline
            $(0,0,n)$ & $\frac{1}{8 \cdot 3^{n-1}}$ & $\frac{1}{8 \cdot 3^{n-1}}$\\\hline
        \end{tabular}
        \caption{Table of $\pi(h_{0,j,l})$ for $j\in \{1,2\}$ and $l\in [n]$.}
        \label{tab:pi-0jl}
    \end{table}
    Then we can compute that 
    \[\pi(S_{(0,1,0)}^+) = \pi(S_{(0,2,0)}^+)=\frac{1}{4\cdot 3} +\frac{1}{4\cdot 3^2} +\ldots +\frac{1}{4\cdot 3^{n-1}} +\frac{1}{8\cdot 3^{n-1}} =\frac{1}{8}\,,\]
    \[\pi(S_{(0,0,1)}^+) = \frac{1}{6}> \pi(S_{(0,0,l)}^+)\,,\]
    \[\pi(S_{(0,0,l)}^+) \leq \pi(S_{(0,0,2)}^+) = \frac{1}{18}\,,\]
    for all $l\geq 2$. 
    
    Thus, the algorithm $\gbs(\pi,\{X_1,X_2\})$ will choose $(0,0,1)$ at time $2$. 
    
    Suppose that at time $t=2,3,\ldots,l$, $\gbs(\pi,\{X_1,X_2\})$ has picked $(0,0,1),\ldots, (0,0,l-1)$ and all are labeled $0$.
    
    Now we show that $\gbs(\pi,\{X_1,X_2\})$ will pick $(0,0,l)$ at time $t= l+1$.
    The version space at the beginning of time $l+1$ is
    $\VS = \{h_{0,0,0}\}\cup \{h_{i,j,p}\in \hat H|p\geq l\}$.
    We can compute that 
    \[\pi(S_{(0,0,l)}^+) = \frac{1}{2\cdot 3^l}> \pi(S_{(0,0,p)}^+)\] 
    for all $p>l$, and that 
    \[
    \pi(S_{(0,1,0)}^+) = \pi(S_{(0,2,0)}^+)= \frac{1}{4 \cdot 3^l} + \frac{1}{4 \cdot 3^{l+1}} + \ldots+\frac{1}{4 \cdot 3^{n-1}} + \frac{1}{8 \cdot 3^{n-1}} = \frac{1}{8 \cdot 3^{l-1}}\,.
    \]
    Hence, $\gbs(\pi,\{X_1,X_2\})$ will pick $(0,0,l)$.
    
    Therefore, we proved that when the target hypothesis is $h_{0,0,0}$, $\gbs(\pi,\{X_1,X_2\})$ will query \\
    $(1,0,0),(0,0,1), (0,0,2),\ldots, (0,0,n)$ sequentially. 
    
    Thus, we have that $Q_1(\gbs,\pi,\{X_1,X_2\},h_{0,0,0}) = n + 1$, and 
    $Q_1(\gbs,\pi,\{X_1,X_2\}) \geq \frac{n+1}{4}$ as $\pi(h_{0,0,0}) =\frac{1}{4}$.
    

\paragraph{Label complexity of agent $1$ when she runs the (GBS) baseline individually}
Now we show that $Q(\gbs,\pi,\{X_1\}) = \bigO(1)$. Since $X_1$ does not contain $(1,0,0)$, both $h_{0,j,l}$ and $h_{1,j,l}$ label $X_1$ identically. Every effective hypothesis over $X_1$ can be written as $h_{*,j,l}$ with $\pi(h_{*,j,l}) = \pi(h_{0,j,l})+\pi(h_{1,j,l})$, which is listed in Table~\ref{tab:pi-xjl}.
       \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c}
             & $(0,1,0)$ & $(0,2,0)$\\\hline
            $(0,0,1)$ & $\frac{1}{4 \cdot 3} + \frac{1}{3}$ & $\frac{1}{4 \cdot 3}$\\\hline
            $(0,0,2)$ &$\frac{1}{4 \cdot 3^2}+ \frac{1}{3^2}$ & $\frac{1}{4 \cdot 3^2}$ \\\hline
            $\cdots$ & $\cdots$ & $\cdots$ \\\hline
            $(0,0,n)$ & $\frac{1}{8 \cdot 3^{n-1}} +\frac{1}{2 \cdot 3^{n-1}}$ & $\frac{1}{8 \cdot 3^{n-1}}$\\\hline
        \end{tabular}
        \caption{Table of $\pi(h_{*,j,l})$ for $j\in \{1,2\}$ and $l\in [n]$.}
        \label{tab:pi-xjl}
    \end{table}  
    Notice that if we know that the label of $(0,0,l)$ is positive for some $l$, then the version space has at most $2$ effective hypotheses, $h_{*,1,l}$ and $h_{*,2,l}$.
    In this case, the algorithm needs at most $2$ more queries.
    
    At time $t=1$, we have
    \[\pi(S_{(0,0,1)}^+) = \frac{1}{4 \cdot 3} + \frac{1}{3} + \frac{1}{4 \cdot 3} =\frac{1}{2}\,,\] 
    \[\pi(S_{(0,0,l)}^+) < \pi(S_{(0,0,1)}^+)\,,\forall l\geq 2\,,\]
    \[\pi(S_{(0,2,0)}^+) = \frac{1}{4\cdot 3} +\frac{1}{4\cdot 3^2} +\ldots +\frac{1}{4\cdot 3^{n-1}} +\frac{1}{8\cdot 3^{n-1}} = \frac{1}{8}\,,\]
    \[\pi(S_{(0,1,0)}^+) = \pi(S_{(0,2,0)}^+) \cdot 5 =\frac{5}{8}\,.\]
    Therefore, $\gbs(\pi,\{X_1\})$ will query $(0,0,1)$ at $t=1$.
    
    We complete the proof by exhaustion. 
    If $(0,0,1)$ is labeled as $1$, then the algorithm needs at most two more queries as aforementioned.
    
    If $(0,0,1)$ is labeled as $0$, then $h_{*,1,1}$ and $h_{*,2,1}$ will be removed from the version space and $\gbs(\pi,\{X_1\})$ will query $(0,1,0)$ at $t=2$ then.
    
    If the label is $1$, the version space is reduced to $\{h_{*,1,l}|l=2,\ldots,n\}$ and $\gbs(\pi,\{X_1\})$ will query $(0,0,2),(0,0,3),\ldots$ sequentially until receiving a positive label.
    
    If the label of $(0,1,0)$ is $0$, $\gbs(\pi,\{X_1\})$ will query $(0,2,0)$ at time $t=3$.
    If the label of $(0,2,0)$ is $1$, then it is similar to the case of  $(0,1,0)$ being labeled $1$ and the algorithm will query $(0,0,2),(0,0,3),\ldots$ sequentially.
    
    If the label of $(0,2,0)$ is $0$, we know the target hypothesis is $h_{0,0,0}$ and we are done.
    
    Hence we have $Q(\gbs,\pi,\{X_1\}) \leq \sum_{l=1}^n (\pi(h_{*,1,l}) + \pi(h_{*,2,l})) \cdot (3+ l) + \pi(h_{0,0,0})\cdot 3 = \sum_{l=1}^{n-1} \frac{1}{2\cdot 3^{l-1}}  \cdot (3+ l) + \frac{1}{4}\cdot 3 = \bigO(1)$.
\end{proof}