When agents are limited to a poor baseline algorithm, e.g., randomly selecting points to query, the principal can simply incentive agents to collaborate by using a superior algorithm that requires fewer labeling efforts. 
We therefore start by considering optimal baseline algorithms in Section~\ref{subsec:ir-opt}.
If we are able to find an IR collaborative algorithm for an optimal baseline algorithm, $\OPT$, then it must be IR w.r.t. all baseline algorithms.
We demonstrate that, surprisingly, the optimal algorithm $\OPT$ is IR given that the baseline algorithm is $\OPT$ itself.
Since computing an optimal algorithm is known to be NP-hard, we continue by considering the best-known approximation algorithm, the greedy algorithm. 
In Section~\ref{subsec:ir-greedy}, we show that given the greedy algorithm as baseline, the collaboration protocol that runs the greedy algorithm is not IR. 
Then in Section~\ref{subsec:ir-alg}, 
we provide a general scheme that transforms any baseline algorithm into an IR algorithm while maintaining a comparable label complexity.
%
\subsection{Optimality Implies Universal Individual Rationality}\label{subsec:ir-opt}
%
Incorporating individual rationality as an additional constraint to optimality usually requires additional effort in certain settings, e.g., in online learning by \citep{blum2020advancing}. 
However, in our specific setting, optimality does not contradict the individual rationality property. That is, an optimal algorithm will not increase any agent's label complexity to benefit other agents. 
In fact, optimizing for optimality implies achieving individual rationality for all baseline algorithms. 
\begin{theorem}\label{thm:opt-ir}
For any optimal collaborative algorithm $\OPT$, we have $${Q_i(\OPT,\pi, \xset)} \leq Q(\OPT, \pi, \{X_i\})= Q^*(\pi, \{X_i\}), \forall i\in [k].$$
    Therefore, $\OPT$ is IR w.r.t. any baseline algorithm.
\end{theorem}

We prove the theorem by contradiction. If $\OPT$ is not IR for the baseline being $\OPT$, then there exists an agent $i$ such that $Q(\OPT, \pi, \{X_i\}) < Q_i(\OPT,\pi, \xset)$. In this case, we can construct a new algorithm by first running $\OPT$ over $\{X_i\}$ (to recover the labels of $X_i$)  and then running $\OPT(\pi, \xset\})$ and replacing agent $i$'s queries with the recovered the labels of $X_i$. This new algorithm incurs a strictly smaller label complexity than $\OPT$, which is a contradiction to the optimality of $\OPT$. The formal proof is deferred to Appendix~\ref{app:opt-ir}. 
Unfortunately, computing an optimal query algorithm is not just NP-hard, but also hard to approximate within
a factor of $\Omega(\log(|\hat H|))$~\citep{golovin2010near,chakaravarthy2007decision}. 
One of the most popular heuristics to find an approximated solution is greedy.
\subsection{The Greedy Algorithm is Not Individually Rational}\label{subsec:ir-greedy}
\vspace{-0.5em}

For standard Bayesian active learning, \cite{kosaraju2002optimal,dasgupta2004analysis} presented a simple greedy algorithm called generalized binary search (GBS), which chooses a point leading to the most balanced partition of the set of hypotheses consistent with the history. 
More specifically, at time step $t$, given the history $\cF_t =((x_1, i_1,y_1),\ldots,(x_{t-1},i_{t-1},y_{t-1}))$, let $\VS(\cF_t) = \{h\in \hat H|h(x_\tau) = y_\tau,\forall \tau\in [t-1]\}$ denote the set of hypotheses consistent with the history $\cF_t$ (often called the version space associated with $\cF_t$).
Given $\cF_t$ and $(\pi, \{X_1,\ldots,X_\kappa\})$ as input, GBS will query 
\[x_t = \argmax_{x\in \cup_{i\in [\kappa]}X_i} \min(\pi(\{h\in \VS(\cF_t)|h(x)=1\}), \pi(\{h\in \VS(\cF_t)|h(x)=0\}))\,\]
at time $t$. When referring to GBS as a collaborative algorithm, we complement it with an arbitrary tie-breaking rule for selecting $i_t$, as GBS itself does not specify how to choose which agent to query.
GBS is guaranteed to achieve competitive label complexity with the optimal label complexity.

\begin{lemma}[Optimality of GBS, Theorem~3 of \citep{dasgupta2004analysis}]\label{lemma:gbs}
    For any prior distribution $\pi$ over $\hat H$ and $k$ agents $\xset$, the label complexity of GBS satisfies that
    \[Q(\gbs,\pi,\xset) \leq 4Q^*(\pi,\xset) \ln(\frac{1}{\min_{h\in \hat H} \pi(h)})\,.\]
\end{lemma}
The greedy algorithm GBS not only achieves approximately optimal label complexity, but it is also computationally efficient, with a running time of $\bigO(m^2|\hat H|)$. 
As GBS is the best-known efficient approximation algorithm, it is natural to think that agents would adopt GBS as a baseline. 

As we have shown that the optimal algorithm is IR w.r.t. itself, the next natural question is: \textit{Is GBS (as collaboration protocol) individually rational w.r.t. GBS itself? }

We answer this question negatively, even in the case of two agents.
Even worse, we present an example in which an agent's label complexity is $\Omega(n)$ when participating in the collaboration, but only $\bigO(1)$ when not participating.


\begin{restatable}{theorem}{greedyir}\label{thm:greedyir}
For the algorithm of GBS, there exists an instance of $(X_1,X_2, \pi)$ in which agent $1$ incurs a label complexity of  $Q_1(\gbs,\pi, \{X_1,X_2\}) = \Omega(n)$ when participating the collaboration and can achieve $Q(\gbs,\pi, \{X_1\}) = \bigO(1)$ when not participating. %
\end{restatable}
Intuitively, at each time step, GBS only searches for an $x_t$ which leads to the most balanced partition of the version space, which does not necessarily be the optimal point to query. Given additional label information from the other agent, GBS possibly choose a worse point to query.
In addition, the label complexity of GBS is upper bounded by the optimal label complexity multiplied by a logarithmic factor.  It is possible that the agent achieves a smaller multiplicative factor by running GBS individually and a larger factor in the collaboration.
The construction of the instance and the proof of Theorem~\ref{thm:greedyir} is deferred to Appendix~\ref{app:greedyir}.


\subsection{A Scheme of Converting Algorithms to IR Algorithms}\label{subsec:ir-alg}
\vspace{-0.5em}

Given that the greedy algorithm has been proven to be not individually rational w.r.t. itself, we raise the following question: \emph{Is it possible to develop a general scheme that can generate an IR algorithm given any baseline algorithm?} In this section, we propose such a scheme that addresses this question. 
Moreover, given a baseline algorithm $\cA$, the resulting IR algorithm can achieve a label complexity comparable to implementing the baseline algorithm over all agents, i.e., $\cA(\pi, \xset)$.
It is important to note that we aim for the label complexity to be comparable to $Q(\cA,\pi, \xset)$ rather than $\sum_{i\in [k]} Q(\cA,\pi,{X_i})$, as the latter holds true by individual rationality.
Given an efficient approximately optimal algorithm as baseline (e.g., GBS), our scheme can provide an algorithm that simultaneously exhibits individual rationality, efficiency, and approximately optimal label complexity. 

For any baseline algorithm $\cA$, we define a new algorithm $\ir(\cA)$, which runs $\cA$ as a subroutine.
Basically, we first calculate the label complexity of agent $i$ both when she is in collaboration with all the other agents, i.e., $Q_i(\cA,\pi,\xset)$,  and when she is not in collaboration, i.e., $Q(\cA,\pi,\{X_i\})$, for all $i\in [k]$. 
By doing so, we can distinguish which agents can benefit from collaboration when running $\cA$ and which cannot. 
We denote the set of agents who cannot benefit from collaboration with all others when running $\cA$ as $S = \{i|Q_i(\cA, \pi, \xset)>Q(\cA,\pi,\{X_i\})\}$. 
For those who do not benefit from the collaboration, we just run $\cA$ on their own data.
For those who benefit from collaborating  with the others together, we run $\cA$ over all agents $[k]$-- 
Only whenever $\cA(\pi,\xset)$ asks to query the label of a point belonging to some $i\in S$, since we already recovered the labels of $X_i$, we just  feed $\cA(\pi,\xset)$ with this label without actually asking agent $i$ to query.  
The detailed algorithm is described in Algorithm \ref{alg:basic2ir}.


\begin{algorithm}[H]\caption{$\ir$}\label{alg:basic2ir}
    \begin{algorithmic}[1]
    \STATE \textbf{input:} A query algorithm $\cA$, set $\xset$ and prior $\pi$ over $\hat H$
    \STATE For each $i\in[k]$, calculate $Q_i(\cA,\pi,\xset)$  and $Q(\cA, \pi, \{X_i\})$.
    \STATE Let $S \leftarrow \{i|Q_i(\cA, \pi, \xset)>Q(\cA,\pi,\{X_i\})\}$ and $X_S\leftarrow \cup_{i\in S}X_i$ // the agents who do not benefit from collaboration
    \STATE \textbf{for} {each $i\in S$} \textbf{do} $Y_i\leftarrow$ Run $\cA$ over $\{X_i\}$ // recover the labels for agent $i$
    % \ENDFOR
    \FOR{$t=1,\ldots$}
        \STATE $(i_t,x_t)\leftarrow$ the querying agent and the query point from $\cA(\pi, \xset)$
        \STATE \textbf{if} {$i_t\in S$} \textbf{then} Feed the label of $x_t$ from $Y_{i_t}$// we already recovered the labels of $ X_{S}$
        \STATE \textbf{else}
        % \ELSE 
         Ask  agent $i_t$
        % $i\notin S$ who has $x_t\in X_i$ 
        to  query the label of $x_t$         
        % \ENDIF
    \ENDFOR
    \end{algorithmic}
\end{algorithm}



\begin{theorem}\label{thm:ir}
    For any baseline algorithm $\cA$, the algorithm $\ir(\cA)$ satisfies the following properties:
    \begin{itemize}[topsep=0pt,itemsep =-0.5ex,  leftmargin = 6mm]
        \item \textbf{IR property:} $\ir(\cA)$ is individually rational w.r.t. the baseline algorithm $\cA$.
        \item \textbf{Efficiency:} $\ir(\cA)$ runs in 
        $\bigO(k T_{\cA, Q} + m T_{\cA,0})$ time, where $T_{\cA,0}$ is the time of computing $(i_t,x_t)$ at each time $t$ for $\cA$ and $T_{\cA, Q}$ is the maximum time of computing $Q_i(\cA,\pi,\{X_1,\ldots,X_\kappa\})$ for an  agent $i$, unlabeled data $\{X_1,\ldots,X_\kappa\}$, and algorithm $\cA$.
        \item \textbf{Label complexity:} $Q(\ir(\cA), \pi,\xset)\leq Q(\cA, \pi, \xset)$.
    \end{itemize} 
\end{theorem}
The proof follows the algorithm description immediately.
Note that when the baseline is GBS, we have $T_{\gbs,0} = \cO(m)$. We can compute $Q_i(\cA,\pi,\{X_1,\ldots,X_\kappa\})$ by simulating over all effective hypotheses $h\in \hat H$. For each $h$, we will query at most $m$ rounds.
Therefore, we have $T_{\gbs,Q} = \cO(m^2|\hat H|)$ and we can run $\ir(\gbs)$ in $\bigO(k m^2 |\hat H|)$ time. 
Using GBS as the baseline, we derive the following corollary.
    
\begin{corollary}\label{crl:ir-greedy}
Given GBS as the baseline, $\ir(\gbs)$ is IR; runs in $\bigO(k m^2 |\hat H|)$ time; and satisfies that $Q(\ir(\gbs), \pi, \xset)\leq 4Q^*(\pi,\xset) \ln(\frac{1}{\min_{h\in \hat H} \pi(h)})$.
\end{corollary}






