

\begin{comment}

\section{outline}
1) State the problem: 
We have simulated SEDs on a sparse 
grid of parameters, but to do modeling
we want to interpolate these to 
get the SEDs at an arbitrary point within the parameter space. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/interpolation.png}
    \caption{An example set of spectra for the physical parameters of M=3e-2, v=0.4c, and X=1e-6 for times from 1.25 days to 1.8 days after merger. The colored lines are showing real spectra, as given by the data produced by Kasen et al. The black and white lines indicate neural net produced spectra for times in between those that exist on the grid.
}
    \label{fig:interpolation}
\end{figure*}{}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/resdiauls.pdf}
    \caption{Differences between lightcurves as created using the raw Kasen data and the data produced by the neural net for the same parameters. Each subplot refers to a different DECam band (a) g band (b) r band (c) i band (d) z band.  The colorbar indicates the value of the residual. The y-axis shows the parameters combination of a given lighcurve (right now it doesnâ€™t, but it will), and the residual is plotted on the x-axis with respect to time. Most of the residuals are scattered around 0, with a slight tendency toward negative values, indicating that the neural net is trending toward predicting slightly brighter values.}
    \label{fig:my_label}
\end{figure*}{}

% ----------------------------
% Section: Introduction
% ----------------------------
\section{Introduction}
The Hubble constant $H_0$ measures the current expansion rate of the universe and is fundamental in cosmology. Over the past decades, significant effort has been dedicated to measuring  $H_0$ using a variety of techniques (cite WMAP, Planck, SHOES,  Hubble/Reiss, DES, others?). Most recently, the \textit{Planck} satellite has made an $H_0$  measurement with a precision of ~1\%  from cosmic microwave background observations. Local Cepheid-supernovae distance ladder measurements have determined the same quantity with a ~2\% precision. Yet there is a 3-$\sigma$ discrepancy between the two measurements. 

In order to break the discrepancy, an independent measurement is required. One promising method involves detecting a gravitational wave (GW) source, accompanied by an electromagnetic counterpart. This ``standard siren" method involves measuring the distance from the gravitational wave measurement and is therefore independent of a cosmic distance ladder. The feasibility of this method was demonstrated following the discovery of the neutron star merger GW170817. The first joint analysis of the GW signal from GW170817 and ths EM counterpart left o a measurement of $H_0 = 74^{+16}_{-8}$ km/s/Mpc (median and symmetric 68\% credible interval). 

One major source of uncertainty on the $H_0$ measurement is the degeneracy between the distance and viewing angle in the GW signal. Neutron star mergers have several EM counterparts that are non-isotropic and therefore could be used to extract viewing angle information. Several of the EM counterparts have been used to constrain the viewing angle with success (Hotokezaka 2019). With the viewing angle information in hand, the uncertainty of the distance (and therefore $H_0$ is improved). Our group has been investigating the possibility of using the optical counterparts, kilonovae, to binary neutron star (BNS) mergers to extract viewing angle information. 

A kilonova is the ultraviolet, optical, and near-infrared signal that accompanies the GWs of a BNS merger. Kilonovae are thought to be powered by the radioactive decay of the heavy elements formed in the merger. There are several mechanism that eject mass which goes on to be powered by r-process nucleosynthesis. Within a few milliseconds (ms) of first surface contact, matter is thrown off the surfaces of the stars in violent processes. There are two main mechanisms to this dynamical ejecta: tidal and shock-heated. Tidal tails form from as tidal forces peel matter from the surfaces of the stars. Next, around 2 ms after first surface contact, if the remnant of the merger is a hypermassive neutron star, the oscillating remnant will send shocks through the surroundings. The dynamical ejecta is expected to be travelling at mildly relativistic velocities of 0.1 - 0.3c. On a much longer timescale, neutrino winds and viscous heating drive matter away in high temperature winds with velocities of 0.01 to 0.1c. Because the ejected matter is neutron-rich and moving at mildly relativistic velocities,the matter will undergo rapid neutron capture (r-process) nucleosynthesis as it decompresses. This process heats the ejected matter and a signature spectra is produced, depending on the velocity, amount of mass, and fraction of lanthanides present in the ejected material. 

Because there are several ejection components over different timescales, the ejected mass is not isotropically distributed around the merger remnant, and the kilonova should appear different depending on viewing angle.

Kasen et al. have performed radiative transfer simulations to create model spectra for spherically distributed, freely expanding r-process material characterized by the mass of the ejecta $M$, expansion velocity $v$, and lanthanide fraction $X$. We use these spectra as a basis to build a non-isotropic, viewing angle dependent model for kilonova lightcurves. We later hope to fit the measured lightcurves of kilonova events to extract the viewing angle information to inform future $H_0$ calculations using the standard sirens method. 

In order to use Markov Chain Monte Carlo (MCMC) fitting techniques, we need to build a continuous representation of the underlying spectra created by Kasen et al. so that we may sample the parameters freely. The numerical difficulty of this problem is immense. There are 379 simulations for distinct combinations of $M$, $v$, and $X$, and each simulation provides spectra every 0.1 days, for 20 days post merger. Each spectra is defined at 1629 wavelength ranges. After unsuccessfully attempting to apply standard interpolation techniques, we investigated the possibility of using a neural network (NN) as a multi-input, multi-output regressor. 

In this paper, we develop a NN for the interpolation of the Kasen et al. spectra. In Section (eh), we describe the data pre-processing before training. In Section (eh), we describe the development, training, and testing of KSNN, the Kilonova Spectra Neural Net. Then in Section (eh), we show results of applying KSNN to values not in the original spectra dataset and provide lightcurves generated by our kilonova emission model using both the original spectra dataset as well as the KSNN-generated spectra. 


% ----------------------------
% Section: Data
% ----------------------------
\section{Data}
The Kasen spectra are provided in two distinct sets: regular grid spectra and spectra created specifically for analysis of GW170817. The parameters of the 329 spectra on the regular grid, as well as the ranges of the parameters of the additional 50 spectra are given in Table \ref{parameters}. Examples of SEDs for three different times are shown in Fig. \ref{fig:compare_sm_pred}. Each spectrum set gives a spectra every 0.1 days for more than 20 days after merger. We use only the spectra up to day 14.4 because many spectra fade to zero flux after approximately two weeks. Therefore, we have 54,576 spectra to train on. 


\begin{table}[]
\caption{Parameter values for the regular grid spectra and the ranges of the parameters for the non-regular spectra}
\begin{center}
\begin{tabular}{ll}
Regular Grid Parameters &                                                                       \\ \hline
Mass ($M_\odot$)        & 0.001, 0.0025, 0.005, 0.01, 0.02,                                     \\
                        & 0.025, 0.03, 0.04, 0.05, 0.075, 0.1 \\
Velocity ($c$)          & 0.03, 0.05, 0.1, 0.2, 0.3                                             \\
Lanthanide Fraction     & $10^{-1}$ , $10^{-2}$ , $10^{-3}$, $10^{-4}$, $10^{-5}$ , $10^{-9}$              \\ \hline
Additional Parameters   &                                                                       \\ \hline
Mass ($M_\odot$)        & 0.02 to 0.05                                                           \\
Velocity ($c$)          & 0.1 to 0.4                                                             \\
Lanthanide Fraction     & $10^{-1}$ to $10^{-9}$                                                    
\end{tabular}
\
\end{center}{}

\label{parameters}
\end{table}




% ----------------------------
% Section: Methods
% ----------------------------
\section{Methodology}

% ----------------------------
% Section: Introduction
% ----------------------------
\subsection{Neural Network Structure}
The fundamental units of neural networks are called `neurons,' and they are based on the perceptrons introduced by Rosenblatt in 1957. A perceptron takes a vector of inputs ($x$) and computes a weighted output with a bias b: $f(x) = w \cdot x + b$. The weight and bias are learned through training. A single perceptron has many limitations, but these limitations can be overcome by using multiple layers of inter-connected perceptrons to create powerful neural networks. This network architecture is called a multilayer perceptron (MLP). 

A MLP usually has one input layer, one or more hidden layers of specified, variable size, and an output layer. The size of the input layer is specified by the size of each training example's dimension, and similarly, the size of the output layer is specified by the size of the target output. A non-linear activation function is applied to the output of each layer; without this non-linearity, the network would only be able to express linear combinations of the input. The final activation function is chosen to match the use of the MLP: classification or regression. In the case of regression, the most commonly used final activation used is the rectified linear unit (ReLU): $max(0,x)$.

MLPs can be trained through variants of the back-propagation algorithm based on the gradient descent method. Errors on training examples are propagated backwards from the output layer to the input layer after each evolution of the network, in order to adjust the weights of each neuron so that the overall error is reduced. The initial weights are initialized randomly to small values and then back-propagation is performed over multiple rounds (known as epochs) until the errors are minimized. Although stochastic gradient descent has been the traditional method for back-propagation, new adaptive methods with variable learning rates have been shown to achieve better results more quickly. 

The full structure of the network can be specified by giving the sizes of each layer, which specifies the number of perceptrons in each layer, the number of hidden layers, the learning algorithm, and the batch size (the number of data points that the learning algorithm uses in each gradient descent step). After experimenting with different configurations of the network, we use a network where the input is of size 4 corresponding to the tuple $X = (M, v,X,t)$ and the output is of size 1629, corresponding to the size of the full, augmented spectrum. We additionally have three hidden layers of size 1000 each. We use the Adam optimization algorithm, with an initial learning rate of $3 \times 10^{-4}$. We train the network for 6000 epochs.


% ----------------------------
% Section: Data Preparation
% ----------------------------
\subsection{Data Preparation: Augmentation}
As suggested by Kasen et al., we reduce noise in the data by smoothing. We use the Savitzky-Goloy smoothing method, with a window size 31 and polynomial order 5. A comparison of the smoothed versus original spectra are shown in Figure \ref{fig:compare_sm_pred}. Neural networks work best when the input and output fall in a similar range, typically [0,1]. The spectra, however, span many orders of magnitude: the minimum is 0 flux, while the maximum might be anywhere on the orders of $10^{33}$ to $10^{38}$. To deal with the several orders of magnitude, we also additionally transform the smoothed spectra $F$ to $F'$ to be equal to

\begin{equation}
    F' = \ln(F + 10^{-5})
\end{equation}{}

We use $F'$ as the target variable of the regression. 

Additionally, we augment the input variables. For the mass $M$ and lanthanide fraction $X$, we first take the natural logarithm, since the original values are logarithmically spaced and therefore difficult to learn for the neural network. We scale each variable then to the range [0,1].




% ----------------------------
% Section: Results
% ----------------------------
\section{Results}

\subsection{Network Training and Validation}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{loss_history.png}
    \caption{Loss history of neural network training, with both training and validation loss shown.}
    \label{fig:loss_history}
\end{figure}{}
We evaluate performance of the network first by considering the loss across all training, validation, and test examples. In Fig. \ref{fig:loss_history} is shown the average training and validation loss throughout training. The final average validation loss is 7.3\% higher than the training loss. Although the validation loss diverges from the training loss, which is a sign of overfitting, we accept this level of overfitting because, as will be shown, the network performs to our needed standards. Examples of learned spectra are shown in Fig. \ref{fig:compare_sm_pred}. 

\subsection{Spectra Prediction Performance}






We also investigate whether the network performance is correlated with the input parameters and we find that the worst performance is when the time is close to the beginning. Some spectra sets start out at 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{time_errors.pdf}
    \caption{Loss of all the individual spectrum examples, coded by the time. The network is worst at predicting the spectra at time closest to 0.}
    \label{fig:time_error}
\end{figure*}{}



% ----------------------------
% Section: Introduction
% ----------------------------
\section{Example Usage}


\section{List of plots and tables}
\begin{enumerate}
    \item figure: Loss vs. epoch with training and validation
    \item figure: schematic of network (use off the shelf code to plot)
    \item table: layer-by-layer description
    \item figure: examples of spectra that are well-predicted and poorly predicted
    \item figure: examples of input (true) spectra to demonstrate 
    \item note: we should use colorblind-friendly color schemes. 
    \item figure: light curve predictions (statistical)
    \item figure: light curve predictions: examples of failures and how they relate to input parameters
    \item figure: spectrogram-like image (time vs. wavelength index) for input and prediction and residual
    \item figure: estimate of angle using discrete Kasen data via MCMCM; estimate of angle using NN
    \item figure: other parameters to estimate?
\end{enumerate}

\end{comment}
