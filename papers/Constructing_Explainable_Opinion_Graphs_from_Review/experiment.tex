

\begin{table}
\small
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{C{1.5cm}|C{5.0cm}|c}
\toprule 
\textbf{Group} & \textbf{Models} & {{\bf Acc.}} \\\midrule
& Two-way attention  & 74.78 \\
{\sc RTE} & Decomposable attention & 76.26 \\
& RTE-BERT & 79.75  \\\midrule
{\sc RelCLS} & Sent-BiLSTM & 75.41   \\
& Sent-BERT & 81.79 \\\midrule
{\sc Proposed} & \expcls-GloVe  & 82.20  \\
               & \expcls-BERT& \textbf{86.23} \\\midrule
{\sc Ablated} & \expcls-GloVe; single-task & 78.67 (3.53 $\downarrow$) \\
& \expcls-BERT; single-task & 80.57 (5.66 	$\downarrow$)\\\bottomrule
\end{tabular}}
\caption{Explanation mining accuracy of different models. Our ablated single-task model (GloVe and BERT) are trained without the review classification objective (i.e., $\lambda = 0$).}
\label{fig:causal_result}
\vspace{-3mm}
\end{table}
\normalsize


\begin{table*}[ht!]
\begin{minipage}[t][][b]{\linewidth}
\centering
\begin{tabular}{c|c|ccc|ccc|ccc}
\toprule 
\multicolumn{2}{c|}{}&\multicolumn{3}{c|}{{\bf Homogeneity (Precision)}} & \multicolumn{3}{c|}{{\bf Completeness (Recall)}}& \multicolumn{3}{c}{{\bf V-measure (F1)}} \\\cmidrule{3-11}
\multicolumn{2}{c|}{}& {\bf k-means} & {\bf GMM} & {\bf Cor. Cluster.} & {\bf k-means} & {\bf GMM} & {\bf Cor. Cluster.} & {\bf k-means} & {\bf GMM} & {\bf Cor. Cluster.} \\\midrule
\multirow{3}{*}{\hotel} & AvgWE & 0.6695&0.6785&0.7240&0.7577&0.7728&0.6756&0.7102&0.7219&0.6985 \\
& ABAE & 0.6626&0.6628&0.6964&0.7609&0.7522&0.7113&0.7075&0.7039&0.6966  \\
& \canonical{} (ours) & {\bf 0.7073}&{\bf 0.7177}&{\bf 0.7460}&{\bf 0.8115}&{\bf 0.8184}&{\bf 0.8370}&{\bf 0.7551}&{\bf 0.7641}&{\bf 0.7848} \\\midrule
\multirow{3}{*}{\restaurant} & AvgWE & 0.5854&0.5509&0.5851&0.8168&0.7801&0.8103&0.6778&0.6413&0.6761 \\
& ABAE &0.5563&0.5553&{\bf 0.6256}&0.7927&0.7779&0.7819&0.6492&0.6432&0.6918 \\
& \canonical{} (ours) & {\bf 0.5920}&{\bf 0.5572}&0.6158&{\bf 0.8333}&{\bf 0.8111}&{\bf 0.8155}&{\bf 0.6877}&{\bf 0.6555}&{\bf 0.6985} \\
\bottomrule
\end{tabular}
\caption{Opinion phrase canonicalization performance on \hotel\ and \restaurant\ datasets.}\label{fig:cluster:all}
\end{minipage}
\vspace{-4mm}
\end{table*}
\normalsize

\section{Evaluation}\label{sec:eval}
We evaluate \system\ with three types of experiments. We use two review datasets for evaluation: a public \yelp{} corpus of $642K$ restaurant reviews and a private \hotel{} corpus\footnote{Data was collected from multiple hotel booking websites.} of $688K$ hotel reviews. 
For the mining explanations and canonicalizing opinion phrases, we perform automatic evaluation over crowdsourced gold labels\footnote{We release the labeled datasets at {\url{https://github.com/megagonlabs/explainit}}.}. To evaluate the quality of the generated opinion graph, we conducted a user study.  

\subsection{Mining Explanations}\label{subsec:eval_explanation}
\subsubsection{Dataset and metric} Based on the data collection process we described in \ref{sec:exp:data}, we used a dataset with $7.4K$ balanced examples in \hotel\ domain. We further split the labeled data into training, validation, and test sets with ratios of $(0.8, 0.1, 0.1)$. We evaluate the models by their prediction accuracy.

\subsubsection{Methods.}
We compare our explanation classifier model and baseline methods, which we categorized into three groups.
The first group ({\sc RTE}) consists of three different models for RTE, the second group ({\sc RelCLS}) consists of two models for relation classification, the third group ({\sc Proposed}) consists of different configurations of our model, and the last group ({\sc Ablated}) is for ablation study. We trained all the models on the same training data with Adam optimizer~\cite{kingma2014adam} (learning-rate=$1e-3$, $\beta_1=0.9$, $\beta_2=0.999$, and decay factor of $0.01$) for $30$ epochs. All models except BERT used the same word embedding model ({\tt glove.6B.300d})~\cite{pennington-etal-2014-glove} for the embedding layers.
For the {\sc RTE} group, the input to the models is a pair of opinion phrases. The review context information associated with the pairs is ignored.

\noindent {\bf Two-way attention}: The two-way attention model~\cite{rocktaschel2015reasoning} is a BiLSTM model with a two-way word-by-word attention, which is used in our proposed model. This can be considered a degraded version of our proposed model only with the alignment attention, which takes opinion phrases without context information.


\noindent {\bf Decomposable attention:} The decomposable attention model ~\cite{parikh-etal-2016-decomposable} is a widely used and the best non-pre-trained model for RTE tasks.

\noindent {\bf RTE-BERT}: BERT~\cite{devlin2018bert} is a pre-trained self-attention model, which is known to achieve state-of-the-art performance in many NLP tasks. We fine-tuned the BERT\textsubscript{base} model with our training data.

\smallskip

For the {\sc RelCLS} group, we follow existing relation classification techniques~\cite{zhou2016attention, lin2016neural, wu2019enriching} and formulate the explanation classification problem as a single sentence classification task. We ``highlight'' opinion phrases in a review with special position indicators~\cite{zhou2016attention} {\tt [OP1]} and {\tt [OP2]}. For example, 
``{\tt [OP1]} \asop{Good location} {\tt [OP1]} \asop{with} {\tt [OP2]} \asop{easy access to beach} {\tt [OP2]}'' highlights two opinion phrases: ``\asop{good location}'' and ``\asop{easy access to beach}''. With this input format, we can train a sentence classification model that takes into account context information while it recognizes which are opinion phrases. 

\noindent {\bf Sent-BiLSTM:} We trained a BiLSTM model with self-attention~\cite{Lin:2017:SelfAttentionLSTM}, which was originally developed for sentence classification tasks. The model architecture can be considered a degraded version of our model without the two-way word-by-word attention. Because we use special position indicators, this model classifies whether the opinion phrases are in the explanation relationship or not. 

\noindent {\bf Sent-BERT:} We fine-tuned the BERT\textsubscript{base} model for the sentence classification task with the training data. Different from RTE-BERT, Sent-BERT takes an entire review, enriched by opinion phrase markers, as the input so it can take context information into account. 

\smallskip

The last group ({\sc Proposed}) include two variations of our model:

\noindent {\bf \expcls-GloVe}: The default model with an embedding layer initialized with the GloVe ({\tt glove.6B.300d}) model.

\noindent {\bf \expcls-BERT}: We replace the embedding layer with the BERT\textsubscript{base} model to obtain contextualized word embeddings. 

\subsubsection{Result analysis}
As shown in Table~\ref{fig:causal_result},
our proposed model achieves significant improvement over baseline approaches: we largely outperform non-pre-trained textual entailment models and sentence classification models by $5.94\%$ to $7.42\%$. Furthermore, to mine explanations, models that consider context information tend to perform better. We found that BERT over sentences is $2\%$ more accurate than BERT over opinion phrases only. Lastly, leveraging pre-trained model can further improve the performance: by replacing the embedding layer with BERT, the accuracy is further improved by $4\%$. 


We also conducted an ablation analysis to verify our multi-task learning framework. We tested variants of \expcls-GloVe and \expcls-BERT that were trained without the review classification objective (i.e., $\lambda=0$). The other configurations were the same as \expcls-GloVe and \expcls-BERT. The results are shown in Table~\ref{fig:causal_result} ({\sc Ablated}). From the results, we confirm that the multi-task learning significantly contributes to the performance of both of the \expcls\ models.

Since our model has both of the alignment attention and self-attention, only with the single objective function (i.e., explanation classification), the model may not be optimized well. In fact, by turning off the multi-task learning, we observe lower performance by \expcls-BERT than Sent-BERT, while \expcls-GloVe shows better performance than the BiLSTM-based baseline models (Sent-BiLSTM). 
Therefore, we consider the issue can be resolved by incorporating multiple objectives as our final models, regardless of the choice of the base model (i.e., GloVe, BERT) achieves the best performance in the explanation mining task.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.32\textwidth]{figs/no_edge.pdf}
    \caption{Ablation study on the usefulness of the intra-cluster explanation loss. Excluding the intra-cluster explanation loss (i.e., $\lambda_E=0$) hurts the opinion phrase canonicalization performance, while it still performs better than ABAE.}
    \label{fig:merge:ablation}
    \vspace{-2mm}
\end{figure}

\subsection{Canonicalizing Opinions Phrases}
\newcommand{\glove}{{AvgWE}}
\subsubsection{Dataset and metrics} For both \hotel\ and \restaurant\ domains, we first exclude entities with too few/many reviews and randomly select $10$ entities from the remaining ones. 
We also develop a non-trivial process to collect the gold clusters using crowdsourcing.  

We evaluate the performance with three metrics: (1) homogeneity, (2) completeness, and (3) V-measure, in the same manner as precision, recall, and F1-score. Homogeneity measures the {\it precision} of each cluster and scores 1.0 if each cluster contains only members of a single class. Completeness measures the {\it recall} of each true class and scores 1.0 if all members of a given class are assigned to the same cluster.
The V-measure is the harmonic mean between homogeneity and completeness scores. 

\begin{figure*}[th!]
\minipage{1.0\textwidth}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{figs/homogeneity.pdf}
\endminipage\hfill
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{figs/completeness.pdf}
\endminipage\hfill
\minipage{0.3\textwidth}%
  \includegraphics[width=\linewidth]{figs/v-measure.pdf}
\endminipage\hfill
\endminipage
\caption{Sensitivity analysis on hyper-parameters $\lambda_{asp}$, $\lambda_{pol}$, and $\lambda_{E}$. The solid line is the performance of \canonical{} and the dotted line is that of the best-performing baseline model (from Table~\ref{fig:cluster:all}).}\label{fig:sensitivity}
\vspace{-4mm}
\end{figure*}

\subsubsection{Methods}
To understand the benefits of our learned opinion phrase embeddings, we evaluate whether they can consistently improve the performance of existing clustering algorithms. Here we select three representative clustering algorithms, $k$-means, Gaussian Mixture Models (GMM)~\cite{Bishop:2006:PRML}, and Correlation Clustering over similarity score~\cite{Bansal:2004:CorrelationClustering,Elsner-schudy-2009-bounding:CorrelationClustering}. For $k$-means and GMM, we set $k=50$ and $k=20$ for \hotel\ and \restaurant\ datasets, respectively; we set $\theta=0.85$ for Correlation Clustering for both datasets. We compared the following methods, which use the same word embedding model ({\tt glove.6B.300d}):

\noindent {\bf \glove:} 
We first calculate the average word embeddings for opinion term and aspect term using GloVe, and then concatenate the aggregated average embedding as the final opinion phrase embedding.

\noindent {\bf ABAE:} We fine-tune the word embedding model without additional labels (i.e., $\lambda_\mathtt{asp}=\lambda_\mathtt{pol}=\lambda_E=0$), which can be considered an ABAE model~\cite{he2017unsupervised}.


\noindent {\bf \canonical}: We learn opinion phrase embeddings based on GloVe word embeddings with weak-supervision from aspect category, polarity, and explanation with the following hyper-parameters: $\lambda_{asp}=1.0, \lambda_{pol}=0.5, \lambda_{E}=0.2$. We obtained the explanations by our explanation classifier (Section~\ref{sec:exp})\footnote{We use the same trained model for both \hotel\ and \restaurant.}.


\subsubsection{Result analysis}
As shown in Table~\ref{fig:cluster:all}, 
our learned opinion phrase representations (\canonical) achieve the best performance among all settings and consistently boost the performance of existing clustering algorithms compared to the baseline methods in both \hotel\ and \restaurant\ domains. In addition, we confirm that our model significantly benefits from the additional weak supervision from opinion and explanation mining as our method significantly improves the performance compared to ABAE, which does not use the weak supervision.


\subsubsection{Usefulness of mined explanations}\label{subsubsec:usefulness}
To verify if the explanations mined in the previous step contribute to the performance of opinion phrase canonicalization, we conducted an ablation study. We evaluated our method without the intra-cluster loss (i.e., $\lambda_E = 0$), so the learned opinion phrase representations do not consider any explanation relationships between opinion phrases.
Figure~\ref{fig:merge:ablation} shows that the performance of our model degrades without the intra-cluster loss (i.e., mined explanations) but is still significantly better than the baseline ABAE model. The results also confirm that the intra-cluster loss based on mined explanations can boost the performance of opinion phrase canonicalization.



\subsubsection{Hyper-parameter sensitivity} \label{sec:cluster:sensitivity}


We also conducted the sensitivity analysis on the hyper-parameters $\lambda_{asp}$, $\lambda_{pol}$, and $\lambda_E$, which balance the multiple loss functions in Eq.~(\ref{eq:all_loss}), to evaluate the robustness of our model with respect to those hyper-parameters. Specifically, we evaluated our model with different $\lambda_?$ ($? = \{asp, pol, E\}$) $\in$ \{0.5, 1.0, 1.5, 2.0, 2.5, 3.0\} while fixing the other two hyper-parameters as 0. Therefore, we can test the contribution of each loss function with different weights when combined with the base reconstruction loss.

Figure~\ref{fig:sensitivity} shows the results for three evaluation metrics (Homogeneity, Completeness, and V-measure). From the results, we confirm that by using either the aspect category loss or the polarity loss, our model consistently outperforms the baseline models. 
Although only using the intra-cluster loss ($\lambda_E > 0$) does not outperform the baseline, we have shown the usefulness of the intra-cluster loss when combined with the other loss functions in \ref{subsubsec:usefulness}.


\begin{figure*}[t]%
    \subfloat[Opinion phrase embeddings of AvgWE (i.e., before representation learning)]{{\includegraphics[width=.46\linewidth]{figs/emb_4_before_new.pdf} }} \label{fig:emb:before}
    \hfill
    \subfloat[Opinion phrase embeddings of \canonical{} (i.e., after representation learning)]{{\includegraphics[width=.46\linewidth]{figs/emb_4_after_new.pdf} }}%
    \caption{Embedding space comparison. Color-coding denotes true cluster assignments. After learning representations, semantically similar opinion phrases are significantly closer to each other and irrelevant ones are further apart in the embedding space, allowing easier opinion phrase canonicalization.}%
    \label{fig:emb}%
    \vspace{-4mm}
\end{figure*}

\subsubsection{Embedding space visualization}
We also present a qualitative analysis of how our \canonical{} helps to canonicalize opinion phrases. Figure~\ref{fig:emb} shows a two-dimensional t-SNE projection~\cite{maaten2008visualizing} of embeddings for a fraction of the opinion phrases about a hotel. The opinion phrase embeddings obtained before and after representation learning are shown on the left and right side of the figure, respectively. The color codes denote true cluster assignments. 

We observe that the original vectors appear more uniformly dispersed in the embedding space, and hence, the cluster boundaries are less prominent. Additionally, we annotated the figure with a number of particularly problematic cases. For example, each of the following pairs of opinion phrases (\asop{``very close to tram''}, \asop{``very close to ocean''}), (\asop{``2 mins walk to beach''}, \asop{``2 mins walk to zoo''}), and (\asop{``good location near the ocean''}, \asop{``good location near the zoo''}), appear in close proximity when they should belong to different clusters. After learning the representations, the problematic pairs of vectors are now clearly separated in their respective clusters.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/study.png}
    \caption{Example question for our user study. Full reviews can be seen when hovering over the (i) icon for each opinion phrase.}
    \label{fig:study}
    \vspace{-3mm}
\end{figure}

\subsection{Opinion Graph Quality: User Study}
In addition to the automatic evaluation of the explanation mining and opinion phrase canonicalizing modules, we designed a user study to verify the quality of the final opinion graphs produced by \system. Assessing the quality of an entire opinion graph at once is impractical due to its size and complexity. Instead, we broke down the evaluation of each generated graph into a series of pairwise tests, where human judges were asked to verify the explanation relation (or lack thereof) between pairs of nodes in the graph. 

More specifically, given a predicted graph $G = (N, E)$ about an entity, we sampled node pairs $(n_i, n_j)$, so that we get a balanced number of pairs for which we predicted the existence or absence of an explanation relation. For every pair, we present the two nodes to the user and show five member opinion phrases from each one. We further show the predicted relation between the nodes (\textsl{``explains"} or \textsl{``does not explain"}) and ask the users if they agree with it. 
An example is shown in Figure~\ref{fig:study}. 

We generated examples for 10 hotels (i.e., their constructed opinion graphs), amounting to 166 node pairs (or questions) in total. This user study was done via Appen's highest accuracy (Level-3) contributors, who obtained no less than 80\% accuracy on our test questions. Every question was shown to 3 judges and we obtained a final judgment for it using a majority vote\footnote{The inter-‚Äêannotator agreement between contributors is 85.6\%.}. The judges \textsl{agreed} with our predicted relation in 77.1\% of cases.

\subsection{Opinion Graph Usefulness}
To evaluate the usefulness of the generated summaries, we further presented the predicted clusters and explanation relations to crowd workers and let them judge the usefulness of such produced information. Note that our clusters of near-synonym opinions and the explanation relations are largely absent from existing travel/service booking websites. We again used Appen's highest accuracy (Level-3) contributors for this task. We presented $164$ clusters-explanation pairs to $1006$ workers and observed that $80.12\%$ of workers found our clusters and explanations useful. 

Furthermore, based on \system{}, we built a summary explorer that visualizes the generated opinion graphs~\cite{wang2020extremereader}. By leveraging the provenance of extracted opinion phrases and explanations, this summary explorer also allows users to access the \textit{``evidence''} (original reviews) of the nodes/edges, thus making \system{} more transparent and interpretable to end-users. Lastly, with the help of an additional abstractive summarization system~\cite{opiniondigest}, \system{} can further generate textual summaries.