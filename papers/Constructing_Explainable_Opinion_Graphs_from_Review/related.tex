\section{Related Work}\label{sec:related}
\noindent
{\bf Opinion Mining:} There has been work of mining opinions from online reviews since \cite{hu2004kdd, hu2004aaai, qiu2011coling, liu2012sentiment, pontiki2015semeval, pontiki2016semeval, xu2019bert}. Those studies developed opinion extraction systems using association mining techniques to extract frequent noun phrases from reviews and then aggregates sentiment polarity scores. These form the aspect-based opinions of online product reviews. 
Opinion Observer~\cite{Liu:2005:OpinionObserver} extended the method to build a system that visualizes the polarity information of each aspect with bar plots. 
\system{} goes one step further from existing opinion mining techniques. Based on extracted opinions, it organizes opinions into an opinion graph such that it includes (a) explanation relationships between opinions and (b) canonicalizes opinions to exclude redundancy. 

\noindent
{\bf Explanation Classifier:} Recognizing Textual Entailment (RTE)~\cite{Dagan:2005:PascalRTE} and Natural Language Inference (NLI)~\cite{snli:emnlp2015} are the tasks to judge if given two statements, whether one statement can be inferred from the other. 
These tasks are usually formulated as a sentence-pair classification problem where the input is two sentences.
A major difference between RTE models and our explanation classifier is that our classifier judges if an opinion phrase explains another opinion phrase in the same review text.  The two opinion phrases may appear in the same sentence or may appear in different sentences. Hence, as described in Section \ref{sec:exp}, we added another task, i.e., explanation existence judgment, in addition to the opinion-phrase classification task to improve the performance using a multi-task learning framework. The goal of relation classification~\cite{zhou2016attention, lin2016neural, wu2019enriching} is to classify the relationship between a pair of entities. For example, determining the relation between entity ``Ms. Ruhl'' and entity ``Chicago'' given a context sequence ``\underline{Ms. Ruhl}, 32, grew up in suburban \underline{Chicago}.''. Similar to the explanation mining problem, the input of relation classification also includes both the context sequence and a pair of entities. However, different from \expcls, existing relation classification models do not incorporate word-by-word alignments between entities. This is because for relation classification, such alignment (e.g., alignment between ``Ms. Ruhl'' and ``Chicago'') is not very useful compared to the context sequence that connects the given pair of entities (e.g., ``grew up in suburban''). 

\noindent
{\bf Opinion Phrase Canonicalization:}
Aspect-based auto-encoder~\cite{he2017unsupervised} is an unsupervised neural model that clusters sentences while learning better word embeddings for sentiment analysis. It showed better performance than conventional topic models (e.g., LDA~\cite{Blei:2003:LDA}, Biterm Topic Model~\cite{Yan:2013:BTM}) in aspect identification tasks. 
Our \canonical{} extends their approach by (1) having an opinion phrase encoder that consists of two encoders for aspect and opinion terms, and (2) incorporating additional sentiment signals such as sentiment polarity and aspect category in a weakly-supervised manner.

Opinion phrase canonicalization is closely related to KB canonicalization~\cite{Galarraga:2014:CIKM:Canonicalizing,Vahishth:2018:CESI}, which canonicalizes entities or relations (or both) by merging triples consisting of two entities and a relation, based on the similarity. Gal\'{a}rraga et al.~\cite{Galarraga:2014:CIKM:Canonicalizing} proposed several manually-crafted features\footnote{\cite{Vahishth:2018:CESI} showed that word-embedding features using GloVe outperformed the methods in \cite{Galarraga:2014:CIKM:Canonicalizing}. Thus, we consider GloVe word embeddings as a baseline for the opinion phrase canonicalization task.} for clustering triples.
CESI~\cite{Vahishth:2018:CESI} uses side information (e.g., entity linking, WordNet) to train better embedding representations for KB canonicalization.
The difference from KB canonicalization is that \system{} does not rely on external structured knowledge such as WordNet or KBs, which were used for those models. This is mainly because it is not straightforward to construct a single KB that reflects a wide variety of subjective opinions written in reviews. Instead, we aim to construct an opinion graph for each entity.
