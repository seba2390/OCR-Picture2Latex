\section{Canonicalizing Opinion Phrases}\label{sec:cluster}


The goal of this component is to group duplicates or very similar opinion phrases together in order to build a concise opinion graph. We call this process {\em canonicalizing opinion phrases}. This is a necessary step as reviews contain 
a variety of linguistic variations to express the same or similar opinions. 
For example, \asop{``one block from beach''}, \asop{``close to the pacific ocean''}, \asop{``unbeatable beach access''}, and \asop{``very close to the sea''} are different phrases used in hotel reviews to describe the same opinion. Other examples (e.g., \asop{``great location''} and \asop{``good location''}) are shown on the right of Figure~\ref{fig:example}.

A widely used method for representing a phrase is the average word embeddings of the phrase based on a pre-trained word embedding model (e.g., GloVe)~\cite{kusner2015word, Vahishth:2018:CESI}.%\yoshi{Add more citations}\xiaolan{add one}
However, a serious limitation of this approach is that it may wrongly cluster opinion phrases that share the same opinion/aspect term but are not necessarily similar. For example, the average word embeddings for the opinion phrase ``\asop{very close to the ocean}'' is closer to an irrelevant opinion phrase ``\asop{very close to the trams}'' than a semantically similar opinion phrase ``\asop{2 mins walk to the beach}''. See Figure~\ref{fig:emb}(a) for more examples. 



To account for semantic similarity, we consider opinion phrase representation learning to learn opinion phrase embeddings before applying
a clustering algorithm such as $k$-means to group similar opinion phrases together. Our method offers 
two major benefits. First, it utilizes only weak supervision: our model does not require any additional labels for training as it leverages the outputs of previous components, namely the aspect categories and sentiment polarity of opinion phrases (in Opinion Mining), and mined explanations (in Explanation Mining). Second, our method allows us to use existing clustering algorithms, which we improve by simply using our opinion phrase representations as features.

\subsection{Opinion Phrase Representation Learning}

We develop an opinion phrase representation learning framework {\bf W}eakly-{\bf S}upervised {\bf O}pinion {\bf P}hrase {\bf E}mbeddings (\canonical), which has two key properties: (1) different embeddings are used for opinion and aspect terms separately, which are then merged into an opinion phrase embedding, and (2) it uses weak supervision to incorporate the semantic meaning of opinion phrases into the opinion phrase embeddings by minimizing the vector reconstruction loss, as well as additional losses based on predicted aspect category, sentiment polarity, and explanations obtained from the previous steps of \system.
The first idea makes it easier for the model to learn to distinguish lexically similar but semantically different opinion phrases. For example, the model can distinguish \asop{``very close to the trams''} and \asop{``very close to the ocean''} based on the aspects \asop{``tram''} and \asop{``ocean''}, which have different representations. The second idea enables the learning of opinion phrase embeddings without additional cost. With the additional loss functions based on signals extracted in the previous steps, the model can incorporate sentiment information into opinion phrase embeddings while retaining the explanation relationship between opinion phrases in the embedding space.

Figure~\ref{fig:clustermodel} illustrates 
the learning framework of \canonical. The model encodes an opinion phrase into an embedding vector, which is the concatenation of an opinion term embedding and an aspect term embedding. Then, the opinion phrase embedding is used as input to evaluate multiple loss functions. The total loss is used to update the model parameters, including the opinion phrase embeddings themselves (i.e., opinion and aspect embeddings). We describe each component and loss function next. Table \ref{tab:symbol2} summarizes the notations, which we will use in this section.

\setlength{\tabcolsep}{2pt}
\begin{table}[!t]
    \small
    \centering
    \begin{tabular}{cc}
    \toprule
        \textbf{Notation} & \textbf{Meaning} \\ \midrule
        $P=\{p_i\}_{i=1}^{N_p}$ & Input opinion phrases \\
        $E=\{e_i\}_{i=1}^{N_e}$ & Explanations extracted by Section~\ref{sec:exp} \\
        $\mathrm{v}_{p} = [\mathrm{v}_a; \mathrm{v}_o]$ & Embeddings for opinion phrase, opinion term, aspect term\\
        $\mathrm{W}^{\mathrm{a}}$ & Trainable parameter for opinion phrase encoding\\
        $\mathrm{W}^{\mathrm{R}}, \mathrm{b}^{\mathrm{R}}$ & Trainable parameters for opinion phrase reconstruction\\
        $J_R $ & Reconstruction loss \\
       $\mathrm{W}^\mathtt{asp}, \mathrm{b}^\mathtt{asp} $ & Trainable parameters for aspect classification \\
       $\mathrm{W}^\mathtt{pol}, \mathrm{b}^\mathtt{pol}$ & Trainable parameters for polarity classification\\
        $J_{asp}, J_{pol}$ & Aspect and polarity classification loss\\ 
        $J_E$ & Intra-cluster explanation loss \\
        \bottomrule
    \end{tabular}
    \caption{Notations for opinion representation learning.}
    \label{tab:symbol2}
    \vspace{-4mm}
\end{table}

\begin{figure*}[t]
\begin{minipage}{0.99\textwidth}
\centering
\noindent
\includegraphics[width=0.9\linewidth]{figs/finetuning.pdf}
\caption{Overview and loss functions of our opinion phrase representation learning framework (\canonical).}
\label{fig:clustermodel}
\end{minipage}
\vspace{-3mm}
\end{figure*}


\smallskip
\noindent \textbf{Input.} The input to the model is a set of $N_p$ opinion phrases $P = \{p_i\}_{i=1}^{N_p}$ 
that are extracted from reviews about a single entity (e.g., a hotel), and a set of $N_e$ explanations $E_e = \{e_i\}_{i=1}^{N_e}$ for the opinion phrases.
Recall that each opinion phrase\footnote{We omit the index $i$ of the opinion phrase and explanation below since it is clear from the context.} $p$ consists of two sequences of tokens $(o, a)$ of the opinion term and the aspect term.
We use $\mathtt{asp}$ and $\mathtt{pol}$ to denote the aspect category and sentiment labels of a phrase, predicted by  % \yuliang{by}
the ABSA model during the opinion mining stage. 


\smallskip
\noindent \textbf{Opinion Phrase Encoding.} Given an opinion phrase $(o, a)$, 
we first use an embedding layer with the self-attention mechanism~\cite{he2017unsupervised} to compute an aspect embedding $\mathrm{v}_a$ and an opinion embedding $\mathrm{v}_o$ respectively\footnote{The embedding architecture is similar to that of Attention-Based Auto-Encoder (ABAE)~\cite{he2017unsupervised} but our model has different encoders for aspect and opinion terms, whereas ABAE does not distinguish aspect and opinion terms and directly encodes an opinion phrase into an embedding vector.}.
The aspect embedding $\mathrm{v}_a$ is obtained by attending over the aspect term tokens $a = (w_1, \dots, w_n)$:
\begin{align}
    &u_i = \mathrm{v}_{w_i}^T \mathrm{W}^{\mathrm{a}}  \mathrm{v}_{a}' & u_i &\in \mathbb{R} \\
    &c_{i} = \frac{\exp(u_i)}{\sum_{j=1}^{m} \exp(u_j)} & c_i& \in \mathbb{R} \\ 
    &\mathrm{v}_a = \sum_{i=1}^{m} c_i \mathrm{v}_{w_i} & \mathrm{v}_a&\in \mathbb{R}^{d},
\end{align}

\noindent
where $\mathrm{v}_{w_i} \in \mathbb{R}^d$ is the output of the embedding layer for word $w_i$, $\mathrm{v}_{a}' \in \mathbb{R}^d$ is the average word embedding for words in $a$, and $\mathrm{W}^{\mathrm{a}} \in \mathbb{R}^{d\times d}$ is a trainable parameter used to calculate attention weights.
We encode the opinion term $o$ into $\mathrm{v}_o$ in the same manner.
Then, we concatenate the two embedding vectors into a single opinion phrase embedding $\mathrm{v}_{p}$:
\begin{equation}
    \mathrm{v}_{p} = [\mathrm{v}_a; \mathrm{v}_o] \qquad\qquad\qquad\qquad \mathrm{v}_{p}\in \mathbb{R}^{2d}.
\end{equation}

\smallskip
\noindent \textbf{Reconstruction loss.} As in the standard auto-encoder paradigm, the main idea behind the reconstruction loss is to learn input vectors $\mathrm{v}_p$ so that they can be easily reconstructed from a representative matrix $\mathrm{R}$. Following previous studies~\cite{he2017unsupervised,angelidis2018summarizing}, we set the $K$ rows of $\mathrm{R} \in \mathbb{R}^{K\times 2d}$ using a clustering algorithm (e.g., $k$-means) over initial opinion phrase embeddings, such that every row corresponds to a cluster centroid\footnote{We freeze $\mathrm{R}$ during training after initialization to facilitate training stability, as suggested in \cite{angelidis2018summarizing}.}.
To reconstruct the phrase vector $\mathrm{v}_{p}$, we first feed it to a softmax classifier to obtain a  probability distribution over the $K$ rows of $\mathrm{R}$:
\begin{equation}\label{eq:assignment_dist}
    \mathbf{s}^R_p = \text{softmax}(\mathrm{W}^{\mathrm{R}}  \mathrm{v}_{p} + \mathrm{b}^{\mathrm{R}})\qquad\qquad \mathbf{s}^R_p \in \mathbb{R}^{K},
\end{equation}

where $\mathrm{W}^{\mathrm{R}} \in \mathbb{R}^{K\times 2d}, \mathrm{b}^{\mathrm{R}}\in \mathbb{R}^K$ are the weight and bias parameters of the classifier respectively.
%
We get the \textsl{reconstructed} vector $\mathrm{r}_{p}$ for opinion phrase $p$ as follows:
%
\begin{equation}
    \mathrm{r}_{p} = \mathrm{R}^T \mathbf{s}^R_p \qquad\qquad \qquad\qquad \qquad  \mathrm{r}_{p}\in \mathbb{R}^{2d}.
\end{equation}

We use the triplet margin loss~\cite{balntas2016learning}
as the cost function, which moves the input opinion phrase $\mathrm{v}_p$ closer to the reconstruction $\mathrm{r}_{p}$, and further away from $k_n$ randomly sampled negative examples:
\begin{equation}
    J_R = \sum_{p \in P} \sum_{i=1}^{k_n}\text{max}(0, 1- \mathrm{r}_{p} \mathrm{v}_{p}+\mathrm{r}_{p} \mathrm{v}_{n_i}),
\end{equation}
%
where $n_i \in P$ are randomly selected negative examples.
%
The sampling procedure tries to sample opinion phrases that are not similar to the input opinion phrase with respect to the probability distribution of Eq.~(\ref{eq:assignment_dist}).
%
For an opinion phrase $p$, the probability of another opinion phrase $p'$ being selected as a pseudo negative example is \textsl{inversely} proportional to the cosine similarity between $\mathbf{s}^R_p $ and $\mathbf{s}^R_{p'}$.

\smallskip
\noindent \textbf{Aspect category and polarity loss.} We also leverage additional signals that we collected from the previous steps to obtain better representations.
%
For example, we would like to avoid opinion phrases such as ``\asop{friendly staff}'' and ``\asop{unfriendly staff}'' from being close in the embedding space. 
%
Hence, we incorporate sentiment polarity and aspect category into the framework to learn better opinion phrase embeddings with respect to sentiment information.

In \canonical{}, we add two classification objectives to learn the parameters. Specifically, we feed an opinion phrase embedding $\mathrm{v}_{p}$ into two softmax classifiers to predict the probability distributions of the aspect category and the sentiment polarity respectively:
%
\begin{align}
    \mathbf{s}_{p}^{\mathtt{asp}}= \text{softmax}(\mathrm{W}^\mathtt{asp}  \mathrm{v}_{p} + \mathrm{b}^\mathtt{asp})\\
    \mathbf{s}_{p}^{\mathtt{pol}}= \text{softmax}(\mathrm{W}^\mathtt{pol}  \mathrm{v}_{p} + \mathrm{b}^\mathtt{pol}).
\end{align}
%
The distributions $\mathbf{s}_{p}^{\mathtt{asp}}$ and $\mathbf{s}_{p}^{\mathtt{pol}}$ are used to compute cross-entropy losses $J_\mathtt{asp}$ and $J_\mathtt{pol}$ against \textsl{silver-standard} aspect and sentiment labels, predicted for each extracted phrase during opinion mining.

\smallskip
\noindent \textbf{Intra-cluster explanation loss.} Our mined explanations should also provide additional signals to learn better embeddings. Essentially, if an opinion phrase $p_i$ explains an opinion phrase $p_j$, they should belong to different clusters (i.e., they should not belong to the same cluster).
%
To reduce intra-cluster explanations, we define the \textsl{intra-cluster explanation loss} by the Kullback-Leibler divergence (KL) between the probability distributions $\mathbf{s}^R_{p_i}  $ and $\mathbf{s}^R_{p_j}$:

\begin{equation}
J_{E} = -\sum_{e \in E} \mathrm{KL}(\mathbf{s}^R_{p_i}, \mathbf{s}^R_{p_j}), \; e=(p_i\rightarrow p_j)\,,
\end{equation}
%
where $E$ is a set of pairs of opinion phrases in the mined explanations,
and $\mathrm{KL}(\cdot,\cdot)$ is the KL divergence between two distributions.

When opinion phrases $p_i$ and $p_j$ are in an explanation relationship, we would like to penalize the case where $\mathrm{KL}(\mathbf{s}^R_{p_i}, \mathbf{s}^R_{p_j})$ is small (likely to be in the same cluster).
As a result, we are able to push the embeddings of $p_i$ and $p_j$ of opinion phrases that have an explanation relationship apart from each other to discourage having intra-cluster explanations.

\smallskip
\noindent \textbf{Training objective.} We define the final loss function by combining the four loss functions defined above:
\begin{equation}\label{eq:all_loss}
J_{\text{WS-OPE}} = J_R + \lambda_\mathtt{asp} J_\mathtt{asp} + \lambda_\mathtt{pol} J_\mathtt{pol} + \lambda_E J_E,
\end{equation}
\noindent
where $\lambda_\mathtt{asp}$, $\lambda_\mathtt{pol}$, and $\lambda_E$ are three hyper-parameters to control the influence of each corresponding loss. 
%
In practice, we prepare two types of mini-batches; one for single opinion phrases and one for explanation pairs. For each training step, we create and use these mini-batches separately: we use the single phrase mini-batch to evaluate the reconstruction, aspect category, and polarity losses; we use the explanation mini-batch to evaluate the explanation loss. At the end of every training step, we accumulate the loss values following Eq.~(\ref{eq:all_loss}) and update the model parameters. 


\smallskip
\subsection{Clustering Opinion Phrases}

After the opinion phrase representation learning, we apply a clustering algorithm over the learned opinion phrase embeddings to obtain opinion clusters. Each
opinion cluster is a node (i.e., {\it canonicalized} opinion) of the final opinion graph. 
Note that our opinion canonicalization
module is not tied to any specific
clustering algorithm. 
We will show in Section~\ref{sec:eval}, our two-stage method for generating opinion clusters performs well regardless of the choice of clustering algorithms, which also demonstrates the strength of 
opinion phrase representation learning.


We could consider directly using a score distribution $\mathbf{s}^R_p$ for clustering instead of applying a clustering algorithm to the learned opinion embeddings. However, we found that $\mathbf{s}^R_p$ does not perform well compared to our approach. This is expected, as the classifier responsible for producing $\mathbf{s}^R_p$ has only been trained via the reconstruction loss, whereas the phrase embeddings have used all of the four signals, thus producing much richer representations.
We conduct further analysis on the contributions of the multiple loss functions in \ref{sec:cluster:sensitivity}.
