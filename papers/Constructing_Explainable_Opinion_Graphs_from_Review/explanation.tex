\begin{figure*}[t]%
    \centering
    \subfloat[Phase-one annotation ask]{{\includegraphics[width=.47\linewidth,valign=t]{figs/appen_task1.png} }}\hfill
    \subfloat[Phase-two annotation task]{{\includegraphics[width=.43\linewidth,valign=t]{figs/appen_task2.png} }\vphantom{\includegraphics[width=.49\linewidth,valign=t]{figs/appen_task1.png}}}%
    \vspace{-2mm}
    \caption{Human annotation tasks for explanation mining.}%
    \label{fig:appen:train}%
\vspace{-2mm}
\end{figure*}


\section{Mining Explanations}\label{sec:exp}
A significant task underlying the construction of an opinion graph is to determine when one opinion phrase explains another. 
For example, \textsl{``close to Muni bus stops''} is an explanation of \textsl{``convenient location''}, but is not an explanation for \textsl{``close to local attractions''}. 
Similarly, \textsl{``on a busy main thoroughfare''} is an explanation for \textsl{``very noisy rooms''} but not necessarily an explanation for \textsl{``convenient location''}. 

Mining explanations between opinion phrases from reviews is related to two problems: entity relation classification~\cite{zhou2016attention, wu2019enriching} (or relation extraction) and recognizing textual entailment (RTE)~\cite{Dagan:2005:PascalRTE}. The entity relation classification problem takes a sequence of text and a pair of entities as the input and learns to classify the relationship between the entities with domain-specific training data. As the models are trained and tailored by domain-specific tasks, it is infeasible to directly train the entity relation classification models for our explanation mining task. Recognizing textual entailment (RTE) problem, on the other hand, considers two sequences of text, often referred as premise and hypothesis, and determines whether the hypothesis can be inferred from the premise. Although it also considers the inference relationships between two pieces of text, RTE models trained over general text are still inadequate for mining explanations from reviews. This is based on two observations: (a) domain-specific knowledge is often necessary to understand the nuances of opinion relationships; (b) in many cases, having access to the full review is crucial to judge potential explanations. 
In fact, we evaluated a state-of-the-art RTE model~\cite{parikh-etal-2016-decomposable} trained on open-domain data~\cite{snli:emnlp2015} and observed a very low explanation accuracy of $34.3\%$. In Section ~\ref{sec:eval}, we re-trained both entity relation classification model and RTE models on the review domain and confirmed that they are still less accurate than our proposed model. 


In what follows, we first describe how we collect domain-specific data for training through crowd-sourcing and then present our multi-task classifier.


\subsection{Collecting Human Annotations}


We use a two-phase procedure to collect two domain-specific training datasets for the hotel and restaurant domains.
The goal of the first phase is to prune
pairs of opinion phrases that are \textit{irrelevant} to each other. In the second phase, the crowd workers label the remaining relevant pairs of opinion phrases. That is, given a pair of relevant opinion phrases, we ask crowd workers to determine if one opinion phrase explains another. If the answer is yes, we ask them to label the direction of the explanation. In both phases, 
we provide as context, the review where the opinion phrases co-occur
to assist crowd workers in understanding the opinion phrases and hence, make better judgments. Figure~\ref{fig:appen:train} demonstrates two example tasks for each phase respectively. 

We obtained our training data via the Appen crowdsourcing platform\footnote{\url{https://appen.com/}}. To control the quality of the labels, we selected crowdworkers who can achieve at least $70\%$ accuracy on our test questions. For each question, we acquire 3 judgments and determine the final label via majority vote. For the first phase, we hired $832$ crowdworkers and observed a $0.4036$ Fleiss' kappa inter-annotator agreement rate~\cite{fleiss2013statistical}; for the second phase, we hired $322$ crowdworkers with a $0.4037$ inter-annotator agreement rate. 
As opposed to obtaining labels through a single phase, our two-phase procedure breaks down the amount of work for each label into smaller tasks and therefore renders higher quality annotated data. This is confirmed by our trial run of a single-phase procedure, which only recorded a $0.0800$ inter-annotator agreement rate.



We obtained $19K$ labeled examples this way with $20\%$ positive examples (i.e., opinion phrases in an explanation relationship). For our experiment, we used a balanced dataset with $7.4K$ examples. 




\subsection{Explanation Classifier}\label{sec:exp:data}
We observe that the context surrounding the opinion phrases and the word-by-word alignments between the opinion phrases are very useful for our explanation mining task. 
For example, the opinion phrases \asop{``noisy room''} and \asop{``right above Kearny St''}, may appear to be irrelevant to each other since one is about \textsl{room quietness} and the other is about \textsl{location}. However,  the context in the review where they co-occur, \asop{``Our room was noisy. It is right above Kearny St.''} allows us to conclude that \asop{``right above Kearny St''} is an explanation for \asop{``noisy room''}. In addition to context, the word-by-word alignments between opinion phrases can also be very beneficial for explanation mining. For example, from two phrases, \asop{``easy access to public transportation''} and \asop{``convenient location''}, the word-by-word alignments between \asop{``easy access to''} and \asop{``convenient''}, as well as \asop{``public transportation''} and \asop{``location''} makes it much easier to determine that the first phrase forms an explanation to the latter one. 

However, existing models do not incorporate both types of information. Relation extraction models for constructing knowledge bases primarily focus on capturing the context between the given opinion phrases and they rarely explicitly consider word-by-word alignments between opinion phrases. RTE models mainly concentrate on aligning words between two pieces of text in the input and ignores the context. 


Therefore, to mine the explanations more effectively, we design a multi-task learning model, which we call MaskedDualAttn, for two classification tasks: (1) {\em Review classification}: whether the review contains explanations; (2) {\em Explanation classification}: whether the first opinion phrase explains the second one  (Figure~\ref{fig:expmodel}). Intuitively, we want the model to capture signals from the context and the opinion phrases. 
Our technique, which accounts for both the context surrounding the opinion phrases and the word-by-word alignments between opinion phrases, is a departure from prior methods in open-domain RTE and entity relation classification, which do not consider both information at the same time. Our ablation study confirms that both tasks are essential for mining explanations effectively (Section~\ref{subsec:eval_explanation}).
Table \ref{tab:symbol1} summarizes the notations used in this section.


\smallskip
\noindent \textbf{Input and Phrase Masks.} 
The input to the classifier consists of a review $r = (w_1, ..., w_L)$ with $L$ words, and two opinion phrases, $p_i$ and $p_j$. 
For each phrase $p=(o, a)$, we create a binary mask, \mbox{$\mathbf{m} = (m_1, ..., m_L)$}, 
which allows the model to selectively use the relevant parts of the full review encoding:

\vspace{-2mm}
\begin{equation*}
m_i = 
\begin{cases}
    1, ~~\text{if~} w_i \in a\cup o \\
    0, ~~\text{otherwise}.
\end{cases}
\end{equation*}
\vspace{-2mm}

\noindent We denote the binary masks for $p_i$ and $p_j$ as $\mathbf{m}_i$ and $\mathbf{m}_j$ respectively.

\smallskip
\noindent \textbf{Encoding.} We first encode tokens in the review $r$ through  
an embedding layer, followed by a BiLSTM layer. We denote the output vectors from the BiLSTM layer as $H=[h_1, ..., h_L] \in \mathbb{R}^{k\times L}$, where $k$ is a hyperparameter of the hidden layer dimension. We do not encode the two opinion phrases separately, but mask the review encoding using $\mathbf{m}_i$ and $\mathbf{m}_j$. Note that we can also replace the first embedding layer with one of the pre-trained models, e.g., BERT~\cite{devlin2018bert}. Our experiment demonstrates that using BERT is able to further improve the performance by $4\%$ compared to a word2vec embedding layer.


\smallskip
\noindent \textbf{Self-attention.} 
There are common linguistic patterns for expressing explanations. A simple example is the use of connectives such as ``because'' or ``due to''.  
To capture the linguistic features used to express explanations, we use the self-attention mechanism~\cite{bahdanau2014neural}, which is a common technique to aggregate hidden representations for classification:

\vspace{-1mm}
\begin{align}
    &\mathrm{M} = \text{tanh}(\mathrm{W}^\mathrm{H} H + \mathrm{b}^\mathrm{H})  &\mathrm{M} &\in \mathbb{R}^{k\times L}\\
    &\alpha = \text{softmax}(\mathrm{w}^\mathrm{T} \mathrm{M}) &\alpha & \in \mathbb{R}^{L}\\
    &\mathrm{h}_r^* = \text{tanh} (H \mathrm{\alpha}^\mathrm{T}) & \mathrm{h}_r^*& \in \mathbb{R}^{k},
\end{align}
where $\mathrm{W}^\mathrm{H}\in \mathbb{R}^{k\times k}$,  $\mathrm{b}^\mathrm{H}, \mathrm{w} \in  \mathbb{R}^{k}$ are three trainable parameters. We obtain the final sentence representation as $\mathrm{h}_r^*$.


\smallskip
\noindent \textbf{Alignment attention. }
Although the self-attention mechanism has a general capability of handling linguistic patterns, we consider it is insufficient to accurately predict the explanation relationship between opinion phrases. Thus, we implement another {\it alignment attention} layer to directly capture the similarity between opinion phrases.
Our alignment attention only focuses on opinion phrases, which is different from the self-attention layer that considers all input tokens, and it has a two-way word-by-word attention mechanism~\cite{rocktaschel2015reasoning} to produce a soft alignment between words in the input opinion phrases. To align $p_i$ with $p_j$, for each word $w_t\in p_i$, we get a weight vector $\alpha_t$ over words in $p_j$ as follows:

\begin{align}
& d_t = \mathrm{U}^\mathrm{h} h_t + \mathrm{U}^\mathrm{r} r_{t-1} & d_t & \in \mathbb{R}^{k}\\
& \mathrm{M}_t = \text{tanh}(\mathrm{U}^\mathrm{H} H + \underbrace{[d_t;...;d_t])}_{L \text{\em\ times}}) & \mathrm{M}_t& \in  \mathbb{R}^{k\times L}\\
& \alpha_t = \text{softmax}(\mathrm{u}^\mathrm{T} \mathrm{M}_t - c \overline{\mathbf{m}_j}) &  \alpha_t &\in \mathbb{R}^{L}\\
& r_t = H \alpha_t + \text{tanh}(\mathrm{U}^\mathrm{t} r_{t-1}) &  r_t &\in \mathbb{R}^{k},
\end{align}

where $\mathrm{U}^\mathrm{H}, \mathrm{U}^\mathrm{h}, \mathrm{U}^\mathrm{r}, \mathrm{U}^\mathrm{t} \in \mathbb{R}^{k\times k}$ and $\mathrm{u} \in \mathbb{R}^{k}$ are five trainable parameters; $h_t \in \mathbb{R}^{k}$ is the $t$-th output hidden state of $H$; $r_{t-1} \in \mathbb{R}^{k}$ is the representation of the previous word; and $\overline{\textbf{m}_j}$ is the reversed binary mask tensor for $p_j$. The final presentation of opinion phrase $p_i$ is obtained from a non-linear combination of $p_i$'s last hidden state $h_{|p_i|}$ and last output vector $r_{|p_i|}$:
\begin{equation}
    \mathrm{h}_i^* = \text{tanh}(\mathrm{U}^{\mathrm{x}} r_{|p_i|} + \mathrm{U}^{\mathrm{y}} h_{|p_i|}) \qquad\qquad\quad \mathrm{h}_i^* \in \mathbb{R}^{k},
\end{equation}
%
where $\mathrm{U}^{\mathrm{x}}, \mathrm{U}^{\mathrm{y}} \in \mathbb{R}^{k\times k}$ are two trainable parameters. 
Similar to the above mentioned procedure for aligning opinion phrase $p_i$ from $p_j$, we also align opinion phrase $p_j$ from $p_i$ and obtain its final representation $\mathrm{h}_j^*$.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{figs/explanation_model3.pdf}
    \caption{Model architecture of \expcls.}
    \label{fig:expmodel}
\vspace{-3mm}
\end{figure}


\setlength{\tabcolsep}{2pt}
\begin{table}[!t]
    \small
    \centering
    \begin{tabular}{cc}
    \toprule
        \textbf{Notation} & \textbf{Meaning} \\ \midrule
        $p = (o, a)$ & Opinion phrase, opinion term, and aspect term \\
        $L$ & Sequence length of a review $r$ \\
        $\mathbf{m}_i$, $\mathbf{m}_j$ & Binary masks of the phrases $p_i$ and $p_j$ \\ 
        $k$ & Size of hidden states \\
        $H=[h_1, ..., h_L]$ & Output hidden states from BiLSTM layer \\
        $\mathrm{W}^\mathrm{H}, \mathrm{b}^\mathrm{H}, \mathrm{w}$ & Self-attention trainable weights \\
        $\mathrm{U}^\mathrm{H}, \mathrm{U}^\mathrm{h}, \mathrm{U}^\mathrm{r}, \mathrm{U}^\mathrm{t}, \mathrm{u}, \mathrm{U}^{\mathrm{x}}, \mathrm{U}^{\mathrm{y}}$ & Alignment-attention trainable weights \\
        \bottomrule
    \end{tabular}
    \caption{Notations for explanation classifier.}
    \label{tab:symbol1}
    \vspace{-3mm}
\end{table}

\smallskip
\noindent \textbf{Prediction and Training.} 
The probability distributions for the review classification ($\mathbf{s}_r$) and explanation classification 
($\mathbf{s}_e$) tasks are obtained from two softmax classifiers respectively:
\begin{align}
    &\mathbf{s}_r = \text{softmax}(\mathrm{W}^{\mathrm{r}} \mathrm{h}_r^* + \mathrm{b}^{\mathrm{r}}) & \mathbf{s}_r& \in \mathbb{R}^{2}\\
    &\mathbf{s}_e = \text{softmax}(\mathrm{W}^{\mathrm{e}} \mathrm{h}_e^* + \mathrm{b}^{\mathrm{e}}) & \mathbf{s}_e& \in \mathbb{R}^{2}, \label{eq:e}
\end{align}
where $\mathrm{h}_e^*= [\mathrm{h}_r^* ; \mathrm{h}_i^* ; \mathrm{h}_j^* ]$ is the concatenation of the sentence's and opinion phrases' representations;
$\mathrm{W}^{\mathrm{r}} \in \mathbb{R}^{2\times k}, \mathrm{b}^{\mathrm{r}} \in \mathbb{R}^2$ and 
$\mathrm{W}^{\mathrm{e}} \in \mathbb{R}^{2\times k}, \mathrm{b}^{\mathrm{e}} \in \mathbb{R}^2$ are the classifiers' weights and biases respectively.
Finally, we define the training objective $J$ as follows:
\vspace{-1mm}
\begin{equation}
J = J_o + \lambda J_r,
\end{equation}
%
where $J_r$ and $J_o$ are the cross-entropy loss for the first and second classification task, respectively;
$\lambda$ is a tunable hyper-parameter.