\section{Experimental Setup}
\label{sec:experiments}
We now detail our empirical setup including environments, text descriptions, evaluation metrics, baselines and model implementation details. Results follow in Section~\ref{sec:results}.

\subsection{Environments}
We perform experiments on a series of 2-D environments within the GVGAI framework~\cite{perez2016general}, which is used in an annual video game AI competition.\footnote{\url{http://www.gvgai.net/}} In addition to pre-specified games, the framework supports the creation of new games using the Py-VGDL description language~\cite{schaul2013video}. We use four different games to evaluate transfer and multitask learning: \emph{Freeway}, \emph{Bomberman}, \emph{Boulderchase} and \emph{Friends \& Enemies}. There are certain similarities between these games. For one, each game consists of a 16x16 grid with the player controlling a movable avatar with two degrees of freedom. Also, each domain contains other entities, both stationary and moving (e.g. diamonds, spiders), that can interact with the avatar. 

However, each game also has its own distinct characteristics. In \emph{Freeway}, the goal is to cross a multi-lane freeway while avoiding cars in the lanes. The cars move at various paces in either horizontal direction. \emph{Bomberman} and \emph{Boulderchase} involve the player seeking an exit door while avoiding enemies that either chase the player, run away or move at random. The agent also has to collect resources like diamonds and dig or place bombs to clear paths. These three games have five level variants each with different map layouts and starting entity placements. 

\emph{Friends \& Enemies} (F\&E) is a new environment we designed, with a larger variation of entity types. This game has a total of twenty different non-player entities, each with different types of movement and interaction with the player's avatar. For instance, some entities move at random while some chase the avatar or shoot bullets that the avatar must avoid. The objective of the player is to meet all friendly entities while avoiding enemies. For each game instance, four non-player entities are sampled from this pool and randomly placed in the grid. This makes F\&E instances significantly more varied than the previous three games.  We created two versions of this game: F\&E-1 and F\&E-2, with the sprites in F\&E-2 moving faster, making it a harder environment. Table~\ref{table:variations} contains all the different transfer scenarios we consider in our experiments. 

\begin{table}[h]
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{  c  c  c  c } %\toprule
\textbf{Condition} & \textbf{Source} & \textbf{Target} & \textbf{\% vocab overlap}\\ \midrule
F\&E-1 $\rightarrow$ F\&E-2 & 7 & 3 & 100 \\
F\&E-1 $\rightarrow$ Freeway & 7 & 5 & 18.06 \\ 
Bomberman $\rightarrow$ Boulderchase & 5 & 5 & 19.6 \\ 
% \bottomrule
\end{tabular}
% }
\caption{Statistics on source and target games for various transfer experiments. First two columns indicate the number of instances of each game, while the last column contains the percentage of overlap between the vocabularies of the corresponding text descriptions, collected using Amazon Mechanical Turk.}
\label{table:variations}
\end{table}

\subsection{Text Descriptions}
We collect textual descriptions using Amazon Mechanical Turk~\cite{mturk}. We provide annotators with sample gameplay videos of each game and ask them to describe specific entities in terms of their movement and interactions with the avatar. Since we ask the users to provide an independent account of each entity, we obtain \emph{descriptive} sentences as opposed to \emph{instructive} ones which inform the optimal course of action from the avatar's viewpoint.\footnote{Upon manual verification, we find less than 3\% of the obtained annotations to be instructive, i.e. containing text that explicitly instruct the agent on steps to take in order to achieve the goal.} 

We aggregated together four sets of descriptions, each from a different annotator, for every environment. This resulted in an average of around 36 unique sentences per domain, with \emph{F\&E} having the most: 78 sentences. Apart from lowercasing the text, we do not perform any extra pre-processing. Each description in an environment is aligned to one constituent entity. We also make sure that the entity names are not repeated across games (even for the same entity type). Table~\ref{table:stats} provides corpus-level statistics on the collected data and Figure~\ref{fig:descriptions} has sample descriptions.

% We use text descriptions for each non-player entity in the game, including stationary ones. Each description is aligned to an object and contains (possibly partial) information about the object's movement and interaction with the agent. Each game environment has 8 sentences on average, resulting in 113 unique word types overall.
% Over all games, we collected a total of 42 sentences, with 113 unique word types and roughly 5 words per sentence. 
% , which roughly correspond to transition and reward functions. 



\begin{table}[h]
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{  c  c  } \toprule
Unique word types &  286 \\ 
Avg. words / sentence &  8.65\\
% Min sentence length & 3 \\
Avg. sentences / domain & 36.25 \\
Max sentence length & 22 \\ \bottomrule
\end{tabular}
% }
\caption{Overall statistics of the text descriptions collected using Mechanical Turk.}
\label{table:stats}
\end{table}

% \paragraph{Learning scenarios}
% We consider two learning scenarios - (1) transfer of policies across tasks, and (2) multitask learning. In the transfer learning scenario, our setup is to train policies on a source set of games $\mathcal{U}$ and transfer to a target set of games $\mathcal{V}$. In the multitask setting, we evaluate learning performance directly on a set of tasks $\mathcal{U}$.

\subsection{Evaluation Metrics}
We evaluate transfer performance using three metrics defined and employed in previous work~\cite{taylor2009transfer}:
\begin{itemize}[leftmargin=0.45cm]
\item \emph{Average Reward}, which is the area under the reward curve divided by the number of test episodes.
\item \emph{Initial performance}, which is the average reward over first 50k steps. 
\item \emph{Asymptotic performance}, which is the average reward over 50k steps after convergence. 
\end{itemize}

The first two metrics emphasize the speed at which a transfer method can enable learning on the target domain, while the last one evaluates its ability to achieve optimal performance on the task. An ideal method should provide gains on all three metrics.
For the multitask scenario, we consider the average and asymptotic reward only.  For each metric, we repeat experiments with nine different random seeds and report mean and standard deviation numbers.

\subsection{Baselines}
We explore several baseline models for empirical comparison. The different conditions we consider are:
\begin{itemize}[leftmargin=0.45cm]
\itemsep0em 
\item \textsc{no transfer}: A deep Q-network (DQN)~\cite{mnih2015dqn} is initialized randomly and trained from scratch on target tasks. This is the only case that does not use parameters transferred from source tasks.
\item \textsc{dqn}: A DQN is trained on source tasks and its parameters are transferred to target tasks. This model does not make use of text descriptions.
\item \textsc{text-dqn}: This is a DQN with our hybrid representation $\phi(s, Z)$, using the text descriptions. This is essentially a reactive-only version of our model, i.e. without the VIN planning module.
\item \textsc{amn}: The Actor-Mimic network is a recently proposed transfer method~\cite{parisotto2016actor} for deep RL. \textsc{amn} employs policy distillation to train a single network using expert policies previously learned on multiple different tasks. This network is then used to initialize a model for a new task.\footnote{We only evaluate AMN on transfer since it does not perform online multitask learning and is not directly comparable.}
\item \textsc{vin}: A value iteration network is trained on the source tasks without making use of the text descriptions. This is effectively an ablation of our full model that only receives state observations as input.
\end{itemize}

\subsection{Implementation Details}
We now provide details on our model implementations. For all models, we set $\gamma = 0.8$, $|\mathcal{D}| = 250\text{k}$, and the embedding size $d = 10$. We used the \emph{Adam}~\cite{kingma2014adam} optimization scheme with a learning rate of $10^{-4}$, annealed linearly to $5 \times 10^{-5}$. The minibatch size was set to 32. $\epsilon$ was annealed from 1 to 0.1 in the source tasks and set to 0.1 in the target tasks. For the value iteration module (VIN), we experimented with different levels of recurrence, $k \in \{1,2,3,5\}$ and found $k=1$ or $k=3$ to work best.\footnote{We still observed transfer gains with all $k$ values.} For DQN, we used two convolutional layers followed by a single fully connected layer, with ReLU non-linearities. The CNNs in the VIN  had filters and strides of length 3. The CNNs in the model-free component used filters of sizes $\{4,2\}$ and corresponding strides of size $\{3,2\}$. All embeddings are initialized at random.\footnote{We also experimented with using pre-trained word embeddings for text but obtained equal or worse performance.}
% \todo{move to appendix?}

% \todo{add final parameter settings}


