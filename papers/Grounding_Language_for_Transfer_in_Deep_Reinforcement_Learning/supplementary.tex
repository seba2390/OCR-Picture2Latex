%\title{emnlp 2017 instructions}
% File emnlp2017.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}

% Uncomment this line for the final submission:
%\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{color}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{breqn}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\usepackage{arydshln}
\usepackage{dblfloatfix}

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
% \newcommand{\todo}[1]{}
\newcommand{\card}[1]{\lvert #1 \rvert}

\title{Deep Transfer in Reinforcement Learning by Language Grounding: Supplementary Material}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{ Karthik Narasimhan \and Regina Barzilay \and Tommi Jaakkola \\
  {\tt  email@email.net}}

\date{}

\begin{document}
\maketitle

\section{Experimental Settings}
For all models, we used $\gamma = 0.8$, $\mathcal{D} = 250\text{k}$. We used a learning rate of $10^{-4}$, annealed linearly to $5 \times 10^{-5}$ and the \emph{Adam}~\cite{kingma2014adam} optimization scheme. The minibatch size is set to 32 and $\epsilon$ is annealed from 1 to 0.1 in the source tasks and set to 0.1 in the target tasks. For the value iteration module (VIN), we experimented with different levels of recurrence, $k \in \{1,3,5,10\}$ and found $k=3$ or $k=5$ to work best.\footnote{We still observed transfer gains with all $k$ values.} For the DQN, we used two convolutional layers followed by a single fully connected layer, with ReLU non-linearities. For the composition function $g$ in our model, we simply used component-wise addition. 

\bibliography{references}

\bibliographystyle{emnlp_natbib}

\end{document}
