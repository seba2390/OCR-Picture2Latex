\section{General Framework}
\label{sec:framework}
Our goal in this work is to demonstrate the utility of natural language descriptions in assisting policy transfer across domains. In this section, we first describe our environment setup and the general framework of our approach. The details of our model and algorithm follow in Section~\ref{sec:model}.

\subsection{Environment Setup} 
We model a single environment as a Markov Decision Process (MDP),  $E = \langle S, A, T, R, O, Z \rangle$. Here, $S$ is the state space, and $A$ is the set of actions available to the agent. In this work, we consider every state $s \in S$ to be a 2-dimensional grid of size $m \times n$, with each cell containing an entity symbol $o \in O$.\footnote{In our experiments, we relax this assumption to allow for multiple entities per cell, but for ease of description, we shall assume a single entity per cell. The assumption of 2-D worlds can also be easily relaxed to generalize our model to other situations.} $T$ is the transition distribution over all possible next states $s'$ conditioned on the agent choosing action $a$ in state $s$. $R$ determines the reward provided to the agent at each time step. The agent does not have access to the true $T$ and $R$ of the environment. Each domain also has a goal state $s_g \in S$ which determines when an episode terminates. Finally, $Z$ is the complete set of text descriptions provided to the agent for this particular environment. 

\subsection{Reinforcement Learning (RL)}
The goal of an autonomous agent is to maximize cumulative reward obtained from the environment. A traditional way to achieve this is by learning an action value function $Q(s,a)$ through reinforcement. The \emph{Q-function} predicts the expected future reward for choosing action~$a$ in state~$s$. A straightforward policy then is to simply choose the action that maximizes the $Q$-value in the current state: 

\begin{dmath*}
\pi(s) = \argmax_a Q(s,a)
\end{dmath*}

If we also make use of the descriptions, we have a text-conditioned policy: 
\begin{dmath}
\pi(s, Z) = \argmax_a Q(s, a, Z)
\end{dmath} 

A successful control policy for an environment will contain both knowledge of  the environment dynamics and the capability to identify goal states. While the latter is task-specific, the former characteristic is more useful for learning a general policy that transfers to different domains. Based on this hypothesis, we employ a model-aware RL approach that can learn the dynamics of the world while estimating the optimal $Q$. Specifically, we make use of \emph{Value Iteration (VI)}~\cite{sutton1998introduction}, an algorithm based on dynamic programming. The update equations for value iteration in our setup are:
\begin{align}
Q^{(n+1)}(s, a, Z) &= \sum_{s' \in S} T(s' | s, a, Z) [ R(s', Z) + \gamma V^{(n)}(s', Z) ]  \nonumber \\
V^{(n+1)}(s, Z) &= \max_a Q^{(n+1)}(s,a, Z) 
\label{eq:vi}
\end{align}
where $\gamma$ is a discount factor and $n$ is the iteration number. The updates require an estimate of $T$ and $R$, which the agent must obtain through exploration of the environment.

% Note that this assumes the agent has knowledge of the true $T$ and $R$ in order to estimate $Q$ and $V$. Since our setup does not provide this information, the agent has to estimate the transition and reward functions from its interactions with the world.

\subsection{Text Descriptions}
Estimating the dynamics of the environment from interactive experience can require a significant number of samples. Our main hypothesis is that if an agent can derive information about the dynamics from text descriptions, it can determine $T$ and $R$ faster and more accurately. 
% Hence, we work with text that provides such relevant particulars of the domain.

For instance, consider the sentence \emph{``Red bat that moves horizontally, left to right''}. This talks about the movement of a third-party entity (\emph{bat}), independent of the agent's goal. Provided the agent can learn to interpret this sentence, it can then infer the direction of movement of a different entity (e.g. \emph{``A tan car moving slowly to the left''}) in a different domain. Further, this inference is useful even if the agent has a completely different goal. On the other hand, instruction-like text such as \emph{``Move towards the wooden door''} is highly context-specific and only relevant to domains that have the mentioned goal.

With this in mind, we provide the agent with text descriptions that collectively portray characteristics of the world. These descriptions are crowdsourced by asking humans to view gameplay videos and describe entities.  A single description talks about one particular entity in the world. The text contains (partial) information about the entity's movement and interaction with the player avatar. Each description is also aligned to its corresponding entity in the environment and not all entities may have a description.
% We make sure that a simple mapping cannot be found between entities in different domains using just their names in text.\todo{clarify this}
Figure~\ref{fig:descriptions} provides some samples; more details on data collection and statistics are in Section~\ref{sec:experiments}. 

\begin{figure}
  \begin{annotationbox}
%     \centering
    \small
      \begin{itemize}
        \item Scorpion2: \emph{Red scorpion that moves up and down} 
        \item Alien3: \emph{This character slowly moves from right to left while having the ability to shoot upwards}
        \item Sword1: \emph{This item is picked up and used by the player for attacking enemies}
      \end{itemize}
%     }
  \end{annotationbox}
  \caption{Example text descriptions of entities in different environments, collected using Amazon Mechanical Turk. Turkers were shown videos of gameplay in the different environments and asked to describe each entity's behavior or role. Note that these sentences are not instructive, since they provide no direct information on how to act in the environment.}
  \label{fig:descriptions}
\end{figure}

\subsection{Transfer for RL}
In order to test our grounding hypothesis, we consider learning across multiple environments. Specifically, an agent can learn to ground language semantics in an environment $E_1$ and then we can test its understanding capability by placing it in a new unseen domain, $E_2$. The agent can obtain unlimited experience in $E_1$, and after convergence of its policy, it is allowed to interact with and learn a policy for $E_2$. We do not provide the agent with any explicit mapping between different entities or goals across domains, either directly or through the text. For instance, even though the boulders in \emph{Boulderchase} are impassable objects just like the walls in \emph{Bomberman}~\ref{fig:example}, the agent does not have access to a mapping between these entities. In this setup, the agent's goal is to re-utilize information obtained through its interactions in $E_1$ to learn more efficiently in $E_2$.


% \paragraph{Environment Setup} 
% We model a single environment as a Markov Decision Process (MDP), represented by $E = \langle S, A, T, R, O, Z \rangle$. Here, $S$ is the state space, and $A$ is the set of actions available to the agent. In this work, we consider every state $s \in S$ to be a 2-dimensional grid of size $m \times n$, with each cell containing an entity symbol $o \in O$.\footnote{In our experiments, we relax this assumption to allow for multiple entities per cell, but for ease of description, we shall assume a single entity. The assumption of 2-D worlds can also be easily relaxed to generalize our model to other situations.} $T$ is the transition distribution over all possible next states $s'$ conditioned on the agent choosing action $a$ in state $s$. $R$ determines the reward provided to the agent at each time step. The agent does not have access to the true $T$ and $R$ of the environment. Each domain also has a goal state $s_g \in S$ which determines when an episode terminates. Finally, $Z$ is the complete set of text descriptions provided to the agent for this particular environment. 

% \paragraph{Reinforcement learning (RL)}
% The goal of an autonomous agent is to maximize cumulative reward obtained from the environment. A traditional way to achieve this is by learning an action value function $Q(s,a)$ through reinforcement. The \emph{Q-function} predicts the expected future reward for choosing action~$a$ in state~$s$. A straightforward policy then is to simply choose the action that maximizes the $Q$-value in the current state: $\pi(s) = \argmax_a Q(s,a)$. If we also make use of the descriptions, we have a text-conditioned policy: $\pi(s, Z) = \argmax_a Q(s, a, Z)$. 

% A successful control policy for an environment will contain both knowledge of  the environment dynamics and the capability to identify goal states. While the latter is task-specific, the former characteristic is more useful for learning a general policy that transfers to different domains. Based on this hypothesis, we employ a model-aware RL approach that can learn the dynamics of the world while estimating the optimal $Q$. Specifically, we make use of \emph{Value Iteration (VI)}~\cite{sutton1998introduction}, an algorithm based on dynamic programming. The update equations are as follows:
% \begin{align}
% Q^{(n+1)}&(s, a, Z) = R(s, a, Z)  \nonumber \\ 
% &+ \gamma \sum_{s' \in S} T(s' | s, a, Z) V^{(n)}(s', Z)  \nonumber \\
% V^{(n+1)}&(s, Z) = \max_a Q^{(n+1)}(s,a, Z) 
% \label{eq:vi}
% \end{align}
% where $\gamma$ is a discount factor and $n$ is the iteration number. The updates require an estimate of $T$ and $R$, which the agent must obtain through exploration of the environment.

% % Note that this assumes the agent has knowledge of the true $T$ and $R$ in order to estimate $Q$ and $V$. Since our setup does not provide this information, the agent has to estimate the transition and reward functions from its interactions with the world.

% \paragraph{Text descriptions}
% Estimating the dynamics of the environment from interactive experience can require a significant number of samples. Our main hypothesis is that if an agent can derive information about the dynamics from text descriptions, it can determine $T$ and $R$ faster and more accurately. 
% % Hence, we work with text that provides such relevant particulars of the domain.

% For instance, consider the sentence \emph{``Red bat that moves horizontally, left to right.''}. This talks about the movement of a third-party entity ('bat'), independent of the agent's goal. Provided the agent can learn to interpret this sentence, it can then infer the direction of movement of a different entity (e.g. \emph{``A tan car moving slowly to the left''} in a different domain. Further, this inference is useful even if the agent has a completely different goal. On the other hand, instruction-like text, such as \emph{``Move towards the wooden door''}, is highly context-specific, only relevant to domains that have the mentioned goal.

% With this in mind, we provide the agent with text descriptions that collectively portray characteristics of the world. A single description talks about one particular entity in the world. The text contains (partial) information about the entity's movement and interaction with the player avatar. Each description is also aligned to its corresponding entity in the environment. 
% % We make sure that a simple mapping cannot be found between entities in different domains using just their names in text.\todo{clarify this}
% Figure~\ref{fig:descriptions} provides some samples; details on data collection and statistics are in Section~\ref{sec:experiments}.

% \begin{figure}
%   \begin{annotationbox}
% %     \centering
%     \small
%       \begin{itemize}[leftmargin=0.45cm]
%         \item Scorpion2: \emph{Red scorpion that moves up and down} 
%         \item Alien3: \emph{This character slowly moves from right to left while having the ability to shoot upwards}
%         \item Sword1: \emph{This item is picked up and used by the player for attacking enemies}
%       \end{itemize}
% %     }
%   \end{annotationbox}
%   \caption{Some example text descriptions of entities in different environments.}
%   \label{fig:descriptions}
% \end{figure}

% % \begin{table}[h]
% % \centering
% % \resizebox{\linewidth}{!}{%
% % \begin{tabular}{  l  } \toprule
% % \textit{Red scorpion that moves up and down} \\
% % \textit{This character slowly moves left to right} \\ 
% % \textit{and has the ability to shoot to the left which can kill the player} \\
% % \textit{this item is picked up and used by the player for attacking enemies}
% % \textit{Ghost1 moves horizontally and is an enemy} \\
% % \textit{Alien3 is an enemy bomber shooting upwards} \\
% % \bottomrule
% % \end{tabular}
% % }
% % \caption{Some example text descriptions of various entities for a game environment.}
% % \label{table:descriptions}
% % \end{table}

% \paragraph{Transfer for RL}
% A natural scenario to test our grounding hypothesis is to consider learning across multiple environments. The agent can learn to ground language semantics in an environment $E_1$ and then we can test its understanding capability by placing it in a new unseen domain, $E_2$. The agent is allowed unlimited experience in $E_1$, and after convergence of its policy, it is then allowed to interact with and learn a policy for $E_2$. We do not provide the agent with any mapping between entities or goals across domains, either directly or through the text. The agent's goal is to re-utilize information obtained in $E_1$ to learn more efficiently in $E_2$.

% % For example, a `bat' in $E_1$ is not given the same symbol as a `bat' in $E_2$.\footnote{Also note that the behavior of a bat in the two environments can be substantially different.} The aim of transfer is to re-utilize information obtained in $E_1$ to learn efficiently in $E_2$.


% % \paragraph{Environment}
% % In our setup, an environment consists of a state space $\mathcal{S}$ and a set of text descriptions $\mathcal{Z} = \{z_i\}$. The state is a $m \times n$ grid world containing entities drawn from a set $\mathcal{O}$ (with unique IDs). Given an input state $s \in \mathcal{S}$, the agent can take a discrete action $a \in \mathcal{A}$, and observe a new state $s'$ of the environment, which changes according to a transition distribution $\mathcal{T}(s' | s,a)$. The environment also provides the agent with a reward $\mathcal{R}(s,a)$ at every time step. Note that the agent does not have access to the true $\mathcal{T}$ and $\mathcal{R}$ of its environment. 

% % Each description $z_i$ is a sentence that provides information about one particular entity type such as its movements or interactions with other entities. We assume access to the mapping between each $z_i$ and its corresponding object $o_i$.

% % \paragraph{Reinforcement Learning (RL)}
% % In the RL framework, the goal of an autonomous agent is to perform actions that maximize the cumulative reward it obtains from the environment. This is done by learning an action value function $Q(s,a)$, which predicts the expected future reward of choosing action~$a$ in state~$s$. Using this, a straightforward policy is to simply choose the action that maximizes the $Q$-value in the current state: $\pi(s) = \argmax_a Q(s,a)$.
% % % \todo{define Q and policy}

% % \paragraph{Transfer setup}
% % We are given a source environment $e_u$ and a target environment $e_v$. In addition, we have access to corresponding sets of text descriptions $\mathcal{Z}_u$ and $\mathcal{Z}_v$, respectively. We first estimate parameters $\Theta$ of a policy $\pi_u(s, \mathcal{Z}_u)$ through several interactions with $e_u$. The policy is optimized to obtain maximum possible reward on the source environment. Now, using $\pi_u$, our goal is to learn an optimal policy for the target environment $e_v$ in as few interactions as possible, by transferring knowledge obtained in $e_u$.


% % % Formally, let us consider an environment $e \in \mathcal{E}$, consisting of entities $\mathcal{O}^e = \{o^e_i\}$ and correspondingly aligned text descriptions $\mathcal{Z}^e = \{z^e_i\}$. Our goal is to learn a mapping from these descriptions to the control dynamics, while simultaneously learning an optimal policy. In this work, we consider two-dimensional state spaces, but the main facets of our model can be extended to other scenarios.