\section{Related Work}
\label{sec:related_work}
We now provide a brief overview of related work in the areas of language grounding and transfer for reinforcement learning.
%There has been work on learning to make optimal local decisions for structured prediction problems~\cite{daume2006searn}.
%
%\newcite{branavan2010reading} looked at a similar task of building a partial model of the environment while following instructions. The differences with our work are (1) the text in their case is instructions, while we only have text describing the environment, and (2) their environment is deterministic, hence the transition function can be learned more easily. 
%
%TODO - model-based RL, value iteration, predictron.


\subsection{Grounding Language in Interactive Environments}
In recent years, there has been increasing interest in systems that can utilize textual knowledge to learn control policies. Such applications include interpreting help documentation~\fullcite{branavan2010reading}, instruction following~\fullcite{vogel2010learning,kollar2010toward,artzi2013weakly,matuszek2013learning,Andreas15Instructions} and learning to play computer games~\fullcite{branavan2011nonlinear,branavan2012learning,narasimhan2015language,he2016deep}. In all these applications, the models are trained and tested on the same domain.

Our work represents two departures from prior work on grounding. First, rather than optimizing control performance for a single domain,
we are interested in the multi-domain transfer scenario, where language 
descriptions drive generalization. Second, prior work used text in the form of strategy advice to directly learn the policy. Since the policies are typically optimized for a specific task, they may be harder to transfer across domains. Instead, we utilize text to bootstrap the induction of the environment dynamics, moving beyond task-specific strategies. 

%Previous work has explored the use of text manuals in game playing, %ranging from constructing useful features by mining patterns in %text~\cite{eisenstein2009reading}, learning a semantic interpreter %with access to limited gameplay examples~\cite{goldwasser2014learning} %to learning through reinforcement from in-game %rewards~\cite{branavan2011learning}. These efforts have demonstrated %the usefulness of exploiting domain knowledge encoded in text to learn %effective policies. However, these methods use the text to infer %directly the best strategy to perform a task. In contrast, our goal is %to learn mappings from the text to the dynamics of an environment and %separate out the learning of the strategy/motives. 

Another related line of work consists of systems that learn spatial and topographical maps of the environment for robot navigation using natural language descriptions~\fullcite{walter2013learning,hemachandra2014learning}. These approaches use text mainly containing appearance and positional information, and integrate it with other semantic sources (such as appearance models) to obtain more accurate maps. In contrast, our work uses language describing the dynamics of the environment, such as entity movements and interactions, which 
is complementary to static positional information received through state observations. Further, our goal is to help an agent learn policies that generalize over different stochastic domains, while their works consider a single domain.

%karthik: I don't see the direct relevance
%Another line of work explores using textual interactive %environments~\cite{narasimhan2015language,he2016deep} to ground %language understanding into actions taken by the system in the %environment. In these tasks, understanding of language is crucial, %without which a system would not be able to take reasonable actions. %Our motivation is different -- we take an existing set of tasks and %domains which are amenable to learning through reinforcement, and %demonstrate how to utilize textual knowledge to learn faster and more %optimal policies in both multitask and transfer setups.

\subsection{Transfer in Reinforcement Learning}
Transferring policies across domains is a challenging problem in reinforcement learning~\fullcite{konidaris2006framework,taylor2009transfer}. The main hurdle lies in learning a good mapping between the state and action spaces of different domains to enable effective transfer. Most previous approaches have either explored skill transfer~\fullcite{konidaris2007building,konidaris2012transfer} or value function/policy transfer~\fullcite{liu2006value,taylor2007transfer,taylor2007cross}. There have also been attempts at model-based transfer for RL~\fullcite{taylor2008transferring,nguyen2012transferring,gavsic2013pomdp,wang2015learning,joshi2018cross} but these methods either rely on hand-coded inter-task mappings for state and actions spaces or require significant interactions in the target task to learn an effective mapping. Our approach doesn't use any explicit mappings and can learn to predict the dynamics of a target task using its descriptions.

% Work by \newcite{konidaris2006autonomous} look at knowledge transfer by learning a mapping from sensory signals to reward functions.

A closely related line of work concerns transfer methods for deep reinforcement learning. \citeA{parisotto2016actor}  train a deep network to mimic pre-trained experts on source tasks using policy distillation. The learned parameters are then used to initialize a network on a target task to perform transfer. Rusu et al.~\citeyear{rusu2016progressive} facilitate transfer by freezing parameters learned on source tasks and adding a new set of parameters for every new target task, while using both sets to learn the new policy. Work by Rajendran et al.~\citeyear{rajendran20172t} uses attention networks to selectively transfer from a set of expert policies to a new task. \textcolor{black}{Barreto et al.~\citeyear{barreto2017successor} use features based on successor representations~\fullcite{dayan1993improving} for transfer across tasks in the same domain. Kansky~et~al.~\citeyear{kansky2017schema} learn a generative model of causal physics in order to help zero-shot transfer learning.} Our approach is orthogonal to all these directions since we use text to bootstrap transfer, and can potentially be combined with these methods to achieve more effective transfer. 

\textcolor{black}{There has also been prior work on zero-shot policy generalization for tasks with input goal specifications. \fullciteA{schaul2015universal} learn a universal value function approximator that can generalize across both states and goals. \fullcite{andreas2016modular} use policy sketches, which are annotated sequences of subgoals, in order to learn a hierarchical policy that can generalize to new goals. \fullciteA{oh2017zero} investigate zero-shot transfer for instruction following tasks, aiming to generalize to unseen instructions in the same domain. The main departure of our work compared to these is in the use of environment descriptions for generalization across domains rather than generalizing across text instructions.}

Perhaps closest in spirit to our hypothesis is the recent work by~\fullcite{harrison2017guiding}. Their approach makes use of paired instances of text descriptions and state-action information from human gameplay to learn a machine translation model. This model is incorporated into a policy shaping algorithm to better guide agent exploration. Although the motivation of language-guided control policies is similar to ours, their work considers transfer across tasks in a single domain, and requires human demonstrations to learn a policy.

\textcolor{black}{
\subsection{Using Task Features for Transfer}
The idea of using task features/dictionaries for zero-shot generalization has been explored previously in the context of image classification. \fullciteA{kodirov2015unsupervised} learn a joint feature embedding space between domains and also induce the corresponding projections onto this space from different class labels. 
\fullciteA{kolouri2018joint} learn a joint dictionary across visual features and class attributes using sparse coding techniques. \fullciteA{romera2015embarrassingly} model the relationship between input features, task attributes and classes as a linear model to achieve efficient yet simple zero-shot transfer for classification. \fullciteA{socher2013zero} learn a joint semantic representation space for images and associated words to perform zero-shot transfer.}

\textcolor{black}{
Task descriptors have also been explored in zero-shot generalization for control policies. \fullciteA{sinapov2015learning} use task meta-data as features to learn a mapping between pairs of tasks. This mapping is then used to select the most relevant source task to transfer a policy from. \fullciteA{isele2016using} build on the ELLA framework~\fullcite{ruvolo2013ella,ammar2014online}, and their key idea is to maintain two shared linear bases across tasks -- one for the policy ($L$) and the other for task descriptors ($D$). Once these bases are learned on a set of source tasks, they can be used to predict policy parameters for a new task given its corresponding descriptor. 
% The training scheme is similar to Actor-mimic scheme~\cite{parisotto2016actor} -- for each task, an expert policy is trained separately and then distilled into policy parameters dependent on the shared basis $L$. 
In these lines of work, the task features were either manually engineered or directly taken from the underlying system parameters defining the dynamics of the environment. In contrast, our framework only requires access to crowd-sourced textual descriptions, alleviating the need for expert domain knowledge.}





% A major difference in our work is that we utilize natural language descriptions of different environments to bootstrap transfer, requiring less exploration in the new task.

% using a policy distillation~\cite{parisotto2016actor,rusu2016progressive,yin2017knowledge} or selective attention over expert networks learnt in the source tasks~\cite{rajendran20172t}. Though these approaches provide some benefits, they still suffer from the requirement of efficiently exploring the new environment to learn how to transfer their existing policies. In contrast, we utilize natural language descriptions of different environments to bootstrap transfer, leading to more focused exploration in the target task. 


% Describe amn in detail




