
%  \begin{table*}[!t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{ c  c c c | c c c | c c c } \\
% \multirow{ 2}{*}{\textbf{Model}} & \multicolumn{3} { c | }{\textbf{F\&E-1 $\rightarrow$ F\&E-2}} & \multicolumn{3} { c | }{\textbf{F\&E-1 $\rightarrow$ Freeway}} & \multicolumn{3} { c  }{\textbf{Bomberman $\rightarrow$ Boulderchase}} \\ %\cmidrule(lr){2-10} % \cmidrule(lr){5-7} \cmidrule(lr){8-10} %\cline{2-10}
%  & \emph{Avg.} & \emph{Jumpstart} & \emph{Asymp.}  & \emph{Avg.} & \emph{Jumpstart} & \emph{Asymp.}  & \emph{Average} & \emph{Jumpstart} & \emph{Asymp.} \\ \hline
% \rule{0pt}{3ex} \textsc{no transfer}            & 0.86 & -0.19 & 1.40 & 0.15 & -1.06 & 0.81 & 9.50 & 2.88 & 10.99\\ \cdashline{1-10}[0.5pt/5pt] \rule{0pt}{3ex}
% \textsc{dqn}    & 1.02 & 0.73 & 1.30 & 0.06 & -0.96 & 0.82 & 9.63 & 3.84 & 11.28 \\
% \emph{\textsc{text-dqn}}  & 1.03 & 0.40 & 1.33 & 0.38 & -0.50 & \textbf{0.85} & 8.52 & 3.42 & 9.45 \\
% \textsc{amn} (Actor Mimic)  & 1.22 & 0.13 & \textbf{1.64} & 0.08 & -0.84 & 0.75 & 6.22 & 0.78 & 8.20 \\
% \textsc{vin}        &  &  &  &  &  &  &  &  &  \\
% \cdashline{1-10}[0.5pt/5pt]
% \rule{0pt}{3ex}
% \textsc{text-vin (1)} & \textbf{1.38} & 0.93 & 1.53 & 0.63 & -0.58  & \textbf{0.85}  & \textbf{11.41} & 4.42 & 12.06 \\
% % \emph{text-VIN (k=5)} & \textbf{1.31} & 0.28 & 1.53 & 0.56 & -0.51 & 0.85 & \textbf{9.65} & 2.93 & \textbf{11.67} \\
% % \emph{text-VIN (k=2)} & 1.23 & 0.07 & 1.57 & 0.68 & -0.23 & \textbf{0.85} & 10.78 & 3.52 & 12.07 \\
% \textsc{text-vin (3)} & 1.27 & \textbf{1.04} & 1.44 & \textbf{0.73} & \textbf{-0.01} & \textbf{0.85} & 10.93 & \textbf{4.49} & \textbf{12.09}\\
% \end{tabular}
% }
% \caption{Transfer learning results under the different metrics for different domains (\textbf{Avg.} is average reward over time, \textbf{Asymp.} is asymptotic reward). Numbers in parentheses for \textsc{text-vin} indicate $k$ value. \textsc{text-} models make use of textual descriptions. The max reward attainable (ignoring step penalties) in the target environments is $2.0$, $1.0$ and at least $25.0$ in F\&E, Freeway and Boulderchase, respectively.  Higher scores are better; bold indicates best numbers.}
% \label{table:results}
% \end{table*}

 \begin{table*}[!t]
\centering
% \resizebox{\textwidth}{!}{%

\begin{tabular}{ c  c c c } \\
\multicolumn{4}{c}{\textbf{F\&E-1 $\rightarrow$ F\&E-2}} \\
\textit{Model} & \emph{Average} & \emph{Initial} & \emph{Asymptotic}  \\ \hline
\rule{0pt}{3ex} \textsc{no transfer}  & 0.88 (0.10) & -0.24 (0.09) & 1.46 (0.07) \\ \cdashline{1-4}[0.5pt/5pt] \rule{0pt}{3ex}
\textsc{dqn}    & 0.98 (0.14) & 0.36 (0.10) & 1.20 (0.09) \\
\emph{\textsc{text-dqn}}  & 0.93 (0.13) & 0.47 (0.33) & 1.21 (0.10) \\
\textsc{amn} (Actor Mimic)       & 1.22 (0.05) & 0.13 (0.10) & \textbf{1.64} (0.01)\\
\textsc{vin (3)}        & 1.14 (0.08) & 0.12 (0.16) & 1.49 (0.07)  \\
\cdashline{1-4}[0.5pt/5pt]
\rule{0pt}{3ex}
\textsc{text-vin (3)} & \textbf{1.32} (0.06) & \textbf{0.70} (0.22) & 1.50 (0.05) \\
% \emph{text-VIN (k=5)} & \textbf{1.31} & 0.28 & 1.53 & 0.56 & -0.51 & 0.85 & \textbf{9.65} & 2.93 & \textbf{11.67} \\
% \emph{text-VIN (k=2)} & 1.23 & 0.07 & 1.57 & 0.68 & -0.23 & \textbf{0.85} & 10.78 & 3.52 & 12.07 \\
% \textsc{text-vin (3)} & 1.27 & \textbf{1.04} & 1.44 \\
\end{tabular}

\begin{tabular}{ c  c c c } \\
\multicolumn{4}{c}{\textbf{F\&E-1 $\rightarrow$ Freeway}} \\
\textit{Model} & \emph{Average} & \emph{Initial} & \emph{Asymptotic}  \\ \hline
\rule{0pt}{3ex} \textsc{no transfer}  & 0.22 (0.03) & -0.95 (0.10) & 0.82 (0.03) \\ \cdashline{1-4}[0.5pt/5pt] \rule{0pt}{3ex}
\textsc{dqn}    & 0.21 (0.16) & -0.78 (0.17) & 0.78 (0.11) \\
\emph{\textsc{text-dqn}}  & 0.33 (0.10) & -0.72 (0.17) & 0.83 (0.01) \\
\textsc{amn} (Actor Mimic)       & 0.08 (0.03) & -0.84 (0.04) & 0.75 (0.005)\\
\textsc{vin (1)}        & 0.59 (0.16) & -0.32 (0.57) & \textbf{0.85} (0.01)  \\
\cdashline{1-4}[0.5pt/5pt]
\rule{0pt}{3ex}
\textsc{text-vin (3)} & \textbf{0.73} (0.01) & \textbf{-0.01} (0.09) & \textbf{0.85} (0.01) 
% \emph{text-VIN (k=5)} & \textbf{1.31} & 0.28 & 1.53 & 0.56 & -0.51 & 0.85 & \textbf{9.65} & 2.93 & \textbf{11.67} \\
% \emph{text-VIN (k=2)} & 1.23 & 0.07 & 1.57 & 0.68 & -0.23 & \textbf{0.85} & 10.78 & 3.52 & 12.07 \\
% \textsc{text-vin (3)} & 1.27 & \textbf{1.04} & 1.44 \\
\end{tabular}

\begin{tabular}{ c c c c } \\
\multicolumn{4}{c}{\textbf{Bomberman $\rightarrow$ Boulderchase}} \\
\textit{Model} & \emph{Average} & \emph{Initial} & \emph{Asymptotic}  \\ \hline
\rule{0pt}{3ex} 
\textsc{no transfer}       & 8.16 (0.79) & 2.88 (0.29) & 10.67 (1.37) \\ \cdashline{1-4}[0.5pt/5pt] \rule{0pt}{3ex}
\textsc{dqn}               & 7.30 (1.39) & 3.77 (0.45) & 9.24 (1.83) \\
\emph{\textsc{text-dqn}}   & 7.92 (0.64) & 3.44 (0.54) & 10.10 (1.69) \\
\textsc{amn} (Actor Mimic) & 5.58 (0.53) & 1.08 (0.38) & 8.66 (0.97) \\
\textsc{vin (3)}           & 9.84 (0.51) & 3.77 (0.44)  & \textbf{12.22} (0.48)  \\
\cdashline{1-4}[0.5pt/5pt]
\rule{0pt}{3ex}
\textsc{text-vin (3)}    & \textbf{11.17} (0.44) & \textbf{5.37} (0.78) & 12.08 (0.31)\\
% \emph{text-VIN (k=5)} & \textbf{1.31} & 0.28 & 1.53 & 0.56 & -0.51 & 0.85 & \textbf{9.65} & 2.93 & \textbf{11.67} \\
% \emph{text-VIN (k=2)} & 1.23 & 0.07 & 1.57 & 0.68 & -0.23 & \textbf{0.85} & 10.78 & 3.52 & 12.07 \\
% \textsc{text-vin (3)} & 1.27 & \textbf{1.04} & 1.44 \\
\end{tabular}

% }
\caption{Transfer learning results under the different metrics for different domains. Numbers in parentheses for \textsc{vin} and \textsc{text-vin} indicate the $k$ value for the best model. \textsc{text-} models make use of textual descriptions.  Numbers are averaged over 9 independent runs (3 source $\times$ 3 target); higher scores are better; bold indicates best numbers; standard deviation numbers are in parentheses. The max reward attainable (ignoring step penalties) in the target environments is $2.0$, $1.0$ and at least $25.0$ in F\&E, Freeway and Boulderchase, respectively. }
\label{table:results}
\end{table*}

 
% \begin{figure*}[!htbp]
% \minipage{0.33\textwidth}
%   \includegraphics[width=\linewidth]{fig/16x16}
%         \caption*{F\&E-1 $\rightarrow$ F\&E-2}
% \endminipage\hfill
% \minipage{0.33\textwidth}%
%   \includegraphics[width=\linewidth]{fig/freeway}
%         \caption*{F\&E-1 $\rightarrow$ Freeway}
% \endminipage
% \minipage{0.33\textwidth}%
%   \includegraphics[width=\linewidth]{fig/boulderchase}
%         \caption*{Bomberman $\rightarrow$ Boulderchase}
% \endminipage
% \caption{Reward curves showing transfer in different domains. \emph{no transfer} refers to a model (DQN) learned from scratch on the target tasks. All other models are evaluated under the transfer condition.}
% 	\label{fig:results}
% \end{figure*}

\section{Results}
\label{sec:results}
We now present empirical evidence that demonstrates the effectiveness of our approach. We first begin by analyzing performance under the transfer condition, followed by the multi-task results and an analysis of the model.

\subsection{Transfer Performance}
Table~\ref{table:results} demonstrates that transferring policies positively assists learning in new domains. Our model, \textsc{text-vin} achieves superior performance to the baselines on average and initial rewards in all transfer conditions.

\subsubsection{Average Reward}
On the first metric of \emph{average reward}, \textsc{text-vin (3)} achieves a 5\% gain (absolute) over the nearest competitor AMN on F\&E-1 $\to$ F\&E-2, and a 14\% gain (absolute) over \textsc{vin (1)} on F\&E-1 $\to$ Freeway. In the case of Bomberman $\to$ Boulderchase, \textsc{text-vin} achieves an average reward of 11.17, which is 1.33 points higher than the nearest baseline of \textsc{vin}, which doesn't make use of the text. Most of this performance gap stems from the fact that our model is able to learn good policies very quickly, reusing knowledge from the source environment, and hence achieving higher rewards from the start. This fact is also evident from the sample reward curves shown in Figure~\ref{fig:results}.

\subsubsection{Initial Reward}
On the metric of \emph{initial reward}, all the transfer approaches outperform the \textsc{no transfer} baseline, except for \textsc{amn} on Bomberman $\to$ Boulderchase. \textsc{text-vin} achieves the highest numbers in all transfer settings, up to 11.5\% better than \textsc{text-dqn} on F\&E-1 $\to$ F\&E-2. This demonstrates our model's effective utilization of text descriptions to bootstrap learning in a new environment. Interestingly, \textsc{text-dqn} demonstrates good jump-start behavior on two of the conditions, but not on \text{F\&E-1} $\to$ Freeway. 

\subsubsection{Asymptotic Reward}
On the final metric of \emph{asymptotic performance}, our model improves performance over  \textsc{no transfer} and is at par or outperforms the other baselines, except on F\&E-1 $\to$ F\&E-2, where \textsc{amn} obtains a score of 1.64. This is partly due to its smoother convergence;\footnote{This fact is also noted in \cite{parisotto2016actor}} improving the stability of our model training could boost its asymptotic performance. Thus, our approach not only speeds up learning on an unseen domain, but also results in better optimal policies.

Another observation from Table~\ref{table:results} is that \textsc{text-vin} consistently outperforms \textsc{text-dqn} in all conditions. This demonstrates the importance of having a model-aware policy, which can ground text descriptions onto environment dynamics while retaining flexibility to accommodate different policies for varying task types. \textsc{text-dqn}, on the other hand, couples both knowledge of the environment and the policy into a single network, making it less suitable for transfer.


\begin{figure}[!h] 
\minipage{\textwidth}
\centering
\includegraphics[width=0.54\linewidth]{fig/2018_fe2}
\endminipage\hfill
\minipage{\textwidth}
\centering
\includegraphics[width=0.55\linewidth]{fig/2018_freeway}
\endminipage\hfill
\minipage{\textwidth}
\centering
\includegraphics[width=0.54\linewidth]{fig/2018_boulderchase}
\endminipage\hfill
\caption{Reward curves for transfer conditions (\textbf{top}) F\&E-1 $\to$ F\&E-2, (\textbf{middle}) F\&E-1 $\to$ Freeway, and (\textbf{bottom}) Bomberman $\to$ Boulderchase (best viewed in color). Numbers in parentheses for \textsc{text-vin} indicate $k$ value. All graphs were produced by averaging over 9 runs with different random seeds; shaded areas represent standard deviation.
}
	\label{fig:results}
\end{figure}



\subsubsection{Negative Transfer}
We also observe the challenging nature of policy transfer in some scenarios. For example, in Bomberman $\rightarrow$ Boulderchase, \textsc{dqn}, \textsc{text-dqn} and \textsc{amn} achieve a \emph{lower} average reward and \emph{lower}  asymptotic reward than the \textsc{no transfer} model, exhibiting negative transfer~\cite{taylor2009transfer}. Further, \textsc{text-dqn} has a lower \emph{initial reward} than a vanilla \textsc{dqn} in such cases, which further underlines the need for a model-aware approach to truly take advantage of the text descriptions for transfer.
% \todo{any insight why is text-DQN lower than DQN?}




\subsection{Multi-task Performance}
In addition to transfer, we also investigate learning in the multi-task setting~\cite{caruana1997multitask}, where the agent learns to perform multiple tasks in a single domain, simultaneously. Specifically, we train a single model, with the same set of parameters, using feedback from all the different tasks. We find that the learning benefits of our model observed in the transfer scenario also hold for multi-task learning, with benefits stemming from both the representation generator as well as the value iteration network. Table~\ref{table:multitask} details the average reward and asymptotic reward obtained by different models across twenty variants of the F\&E-2 domain. Our model is able to use the text to learn faster as well as achieve higher optimum scores, with \textsc{text-vin (1)} showing gains over \textsc{dqn} of 28.5\% and 12\% on average and asymptotic rewards, respectively. Figure~\ref{fig:results-multitask} shows the corresponding reward curves for the various models.

\begin{table}[!h]
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabular}{ c  c  c } %\toprule
\textbf{Model} & \emph{Avg.} & \emph{Asymp.}  \\ \midrule
\textsc{dqn}  & 0.80 (0.08) & 1.38 (0.07)\\
\textsc{text-dqn}  & 0.79 (0.09) & 1.45 (0.08) \\
\textsc{vin (1)} & 1.35 (0.04) & 1.61 (0.05)\\
\textsc{text-vin (1)} &\textbf{1.37} (0.03) & \textbf{1.62} (0.02) \\
% \textsc{text-vin (3)} & 1.24 & 1.57 \\ %\bottomrule
\end{tabular}
%}
\caption{Average (Avg.) and asymptotic (Asymp.) rewards for multitask learning over 20 games in F\&E-2. All numbers are averaged over 9 different runs; numbers in parentheses are standard deviations.}
\label{table:multitask}
\end{table}

\begin{figure}[!h]
\centering
\minipage{0.6\textwidth}
\includegraphics[width=\linewidth]{fig/2018_fe2_multitask}
\endminipage\hfill
\caption{Reward curve for multitask learning in F\&E-2. Numbers in parentheses for \textsc{text-vin} indicate $k$ value. All graphs averaged over 9 runs with different random seeds; shaded areas represent standard deviation.
}
	\label{fig:results-multitask}
\end{figure}

% These results demonstrate that our model can utilize the text descriptions to learn generalized representation that enable faster learning.
%  A surprising observation is that the text-DQN performs worse than a simple DQN which does not utilize any text. This indicates that making use of the text appropriately is crucial -- mapping it directly to a (domain-specific) policy is worse than our model-based approach.

% \begin{figure*}[!ht]
% \minipage{0.33\textwidth}
%   \includegraphics[width=\linewidth]{fig/multitask}
%         \caption*{(a)}% Multitask Reward (F\&E)}
% \endminipage
% \minipage{0.33\textwidth}
%   \includegraphics[width=\linewidth]{fig/textrep}
%         \caption*{(b)}% Different representations \todo{update}}
% \endminipage
% \minipage{0.33\textwidth}
%   \includegraphics[width=\linewidth]{fig/textfrac}
%         \caption*{(c)}% Noisy text}
% \endminipage
% \caption{Rewards curves for (a) multitask learning, (b) text only and text+entity ID representations and (c) noisy text descriptions with only a \% of words retained. Multitask experiments are on 20 instances of F\&E-2. The other two experiments are with 7 source instances in F\&E-1 and 3 target instances in F\&E-2.}
% 	\label{fig:results2}
% \end{figure*}




% \vspace{-0.1cm}
\subsection{Analysis}
We further analyze the performance of our model by performing ablation studies to investigate the effects of different state and text representations, as well as a qualitative analysis of the value maps produced by the model.

\subsubsection{Effect of Factorized Representation}
We investigate the usefulness of our factorized representation by training a variant of our model using only a text-based vector representation (\emph{Text only}) for each entity, i.e. $\phi(s, Z) = v_z(s, Z)$.
% \footnote{Note that an ID-only representation is the same as \textsc{DQN}.}
We consider two different transfer scenarios -- (a) when both source and target instances are from the same domain (F\&E-1 $\rightarrow$ F\&E-1) and (b) when the source/target instances are in different domains (F\&E-1 $\rightarrow$ F\&E-2).
 In both cases, we see that our two-part representation results in faster learning and more effective transfer, obtaining 23\% higher average reward and 19\% more asymptotic reward in \text{F\&E-1 $\rightarrow$ F\&E-2} transfer (Table~\ref{table:textrep}). Our  representation is able to transfer prior knowledge through the text-based component while retaining the ability to learn new entity-specific representations quickly.
% Figure~\ref{fig:results2}(b) shows the corresponding reward graphs.

\begin{table}[!t]
\centering
% \resizebox{0.5\textwidth}{!}{%
\begin{tabular}{ c  c  c  c  c } %\toprule
\textbf{Condition} & \textbf{Model} & \emph{Average} & \emph{Initial} & \emph{Asymptotic}  \\ \midrule
\multirow{ 2}{*}{F\&E-1 $\rightarrow$ F\&E-1} & Text only  & 1.64 (0.02) & 0.48 (0.09) & \textbf{1.78} (0.00) \\
& Text+entity ID & \textbf{1.70} (0.02) & \textbf{1.07} (0.19) & \textbf{1.78} (0.00) \\ \cdashline{1-5}[0.5pt/5pt]
\rule{0pt}{3ex} \multirow{ 2}{*}{F\&E-1 $\rightarrow$ F\&E-2} & Text only  & 0.86 (0.01) & 0.49 (0.04) & 1.11 (0.01) \\
& Text+entity ID & \textbf{1.32} (0.06) & \textbf{0.70} (0.22) & \textbf{1.50} (0.05) \\% \bottomrule
\end{tabular}
% }
\caption{ Transfer results using different input representations with \textsc{text-vin (3)}. \emph{Text only} means only a text-based vector is used, i.e. $\phi(s, Z) = v_z(s, Z)$. \emph{Text+entity ID} is our full representation, $\phi(s, Z) = [v_o(s); v_z(s, Z)]$. All numbers are averaged over 9 different runs; numbers in parentheses are standard deviations.}
\label{table:textrep}
% \vspace{-0.3cm}
\end{table}


\begin{table}[!t]
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabular}{c  c  c  c} %\toprule
\textbf{Condition} & \textbf{Model} & \emph{BOW} & \emph{LSTM}  \\ \midrule
\multirow{ 2}{*}{F\&E-1 $\rightarrow$ F\&E-2} & \textsc{text-vin (1)}  & 1.17 (0.08) &  1.28 (0.06)\\
& \textsc{text-vin (3)} & \textbf{1.32} (0.06) &  1.20 (0.06) \\ \cdashline{1-4}[0.5pt/5pt]
\multirow{ 2}{*}{F\&E-1 $\rightarrow$ Freeway} & \textsc{text-vin (1)}  & 0.57 (0.09) & 0.63 (0.02)\\
& \textsc{text-vin (3)} & 0.57 (0.05) & \textbf{0.73} (0.01) \\ \cdashline{1-4}[0.5pt/5pt]
\multirow{ 2}{*}{Bomberman $\rightarrow$ Boulderchase} & \textsc{text-vin (1)}  & 10.09 (0.94) & 11.10 (0.22) \\
& \textsc{text-vin (3)} & 9.66 (0.77) &  \textbf{11.17} (0.44) \\

% \textsc{text-vin (2)} & 9.17 & \textbf{10.78}  \\
% \textsc{text-vin (3)} & 10.63 & \textbf{10.93} \\
% \textsc{text-vin (5)} & 9.54 & \textbf{10.15} \\
%\bottomrule
\end{tabular}
%}
\caption{Average rewards in Bomberman $\to$ Boulderchase with different text representations: Mean bag-of-words (\emph{BOW}), or a vector generated by running an  \emph{LSTM}-based recurrent neural network over the entire sentence. All numbers are averaged over 9 different runs; numbers in parentheses are standard deviations.}
\label{table:lstm}
\end{table}


\subsubsection{Text Representation: BOW vs. LSTM}
Another question to consider is the relative impact of using LSTM vs. mean BOW to generate vector representations from the text descriptions in our model. Table~\ref{table:lstm} provides a comparison of transfer performance between these two representations on the different conditions. We observe that using an LSTM provides significantly better results on 
F\&E-1 $\rightarrow$ Freeway and Bomberman $\rightarrow$ Boulderchase, and slightly worse than BOW ($1.28$ vs $1.32$) on F\&E-1 $\rightarrow$ F\&E-2. This indicates that a good text representation which can capture linguistic compositionality works better in our model. Exploration of other recently proposed representations like the Transformer~\cite{vaswani2017attention} could lead to further improvements.
% We observe that across all models, the LSTM representation provides greater gains. In fact, the Sum representation does worse than vanilla \textsc{dqn} (9.63) in some cases. This underscores the importance of a good text representation that can capture aspects of linguistic compositionality.



% \subsubsection{Model Variants}
% \todo{add in discussion on various hyperparameters and text representations?}


% KN: This is the least useful result, so removing it from submission.
% \begin{table}[!ht]
% \centering
% % \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{ c  c  c  c } %\toprule
% \textbf{\% of text retained} & \emph{Avg} & \emph{Jump} & \emph{Asymp.}  \\ \midrule
% no transfer & 0.86 & -0.19 & 1.40 \\ \cdashline{1-4}[0.5pt/5pt]
% \rule{0pt}{3ex} 50.0 & 0.68 & 0.27 & 1.16 \\
% 75.0 & 1.13 & 0.04 & 1.49  \\
% 100.0 & \textbf{1.29} & \textbf{0.59} & \textbf{1.53} \\ %\bottomrule
% \end{tabular}
% % }
% \caption{Transfer results on F\&E-1 $\rightarrow$ F\&E-2 with the text-VIN (k=3) model using noisy text, created by retaining only a percentage of words in the text descriptions.}
% \label{table:textfrac}
% \end{table}

% \paragraph{Effect of description quality}
% We also investigated the effect of the quality of the text descriptions on the transfer performance of our model. Specifically, we randomly drop out words and retain only a percentage in the text for each run. As on would expect, from Table~\ref{table:textfrac}, we see that the performance drops as the text becomes noisier. However, even with 75\% of the words, the transfer is still slightly better than learning a model from scratch. Figure~\ref{fig:results2}(c) shows the corresponding learning curves.


\begin{figure*}[!t]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{fig/heatmap_seen}
  \caption*{(a)}
\endminipage
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{fig/heatmap_unseen}
  \caption*{(b)}
\endminipage \\
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{fig/heatmap_text_friend}
  \caption*{(c)}
\endminipage
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{fig/heatmap_text_enemy}
  \caption*{(d)}
\endminipage

\caption{Value maps ($\hat{V}^{(k)}(s,Z)$) produced by the VIN module for (a) seen entity (friend), (b) unseen entity with no description, (c) unseen entity with `friendly' description, and (d) unseen entity with `enemy' description. Agent is at (4,4) and the non-player entity is at (2,6). Notice how the value of cell (2,6) changes with the type of description: higher for `friendly' and lower for `enemy' compared to the case with no description.}
	\label{fig:heatmap}
\end{figure*}

\subsubsection{Value Analysis}
Finally, we provide some qualitative evidence to demonstrate the generalization capacity of \textsc{text-vin}. Figure~\ref{fig:heatmap} shows visualizations of four value maps produced by the VIN module of a trained model, with the agent's avatar at position (4,4) and a single entity at (2,6) in each map. In the first map, the entity is known and friendly, which leads to high values in the surrounding areas, as expected. In the second map, the entity is unseen and without any descriptions; hence, the values are uninformed. The third and fourth maps, however, contain unseen entities with descriptions. In these cases, the module predicts higher or lower values around the entity depending on whether the text portrays it as a friend or enemy. Thus, even before a single interaction in a new domain, our model can utilize text to generate good value maps. This bootstraps the learning process, making it more efficient.



% \subsection{Analysis}


% For the first two columns (\emph{F\&E} and \emph{Freeway}), we train each model on 7 games from the F\&E domain, with each game having a different set of non-player characters with different starting positions. Testing is done on 3 target instances in F\&E and 5 levels for Freeway, respectively.  For results in the final column (\emph{Boulderchase}), we train each model on 5 different levels of \emph{Bomberman} and test on 5 levels of Boulderchase. For the target domains, the model is learned again in a multitask setting on all environments simultaneously. This reduces the chance of obtaining favorable transfer results simply due to a single highly similar environment.
