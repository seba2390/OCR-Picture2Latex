\section{Conclusion}
\label{sec:conclusions}
In this work, we have explored a novel method to tackle the long-standing challenge of transfer for reinforcement learning. Transferring policies is hard mainly due to the difficulty in learning effective mappings between source and target domains, often resulting in negative transfer~\cite{taylor2009transfer} as a result of incorrect mappings.
We have proposed utilizing natural language to drive transfer for reinforcement learning (RL) and shown that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. In contrast to most existing systems, we have employed a model-aware RL approach that aims to capture the dynamics of the environment. For this, we utilized a value iteration network (VIN), which encapsulates the iterative computation of a value function into a single differentiable neural network. We have also introduced a two-part state representation in order to combine text with input observations.
This representation allows us to distill useful information while being robust to incomplete or noisy descriptions.

By effectively utilizing descriptions, our technique can bootstrap learning on new unseen domains. Over several empirical tests across a variety of environments, we have shown that our approach is at par or outperforms several existing systems on different metrics for transfer learning. Our model achieves up to 14\% higher average reward and up to 11.5\% higher initial reward compared to the most competitive baselines. We have also performed evaluation on a multi-task setting where learning is simultaneously carried out in multiple environments and demonstrate the superior performance of our approach.

There are several possible avenues of future work. One could explore the combination of different transfer approaches. Leveraging language for policy transfer in deep RL is complementary to other techniques such as policy reuse~\fullcite{glatt2017policy} or skill transfer~\fullcite{gupta2017learning} among other approaches~\fullcite{du2016initial,tobin2017domain,yin2017knowledge}. A combination of one of these methods with language-guided transfer could result in further improvements.
Another area for investigation is on techniques that can operate without requiring explicit one-to-one mappings between descriptions and entities in the environment. One can either learn these mappings simultaneously with the policy, or operate using descriptions that involve multiple entities and global relations.

In this work, since we factorize our input representation (using both text and direct observations), the method works at least as well as when not using the text descriptions i.e. the model could learn to rely less on the descriptions if they do not contain useful information. However, one potential case for failure could be if the text contains misleading/incorrect descriptions of the environment. Addressing this issue of robustness to adversarial inputs is another potential direction of investigation.
Finally, a major component behind our model's generalization performance is the value iteration network (VIN). However, in its current form, the VIN requires specifying a recurrence hyper-parameter $k$, whose optimal value might vary from one domain to another. Investigating models that can perform multiple levels of recurrent VI computation, possibly in a dynamic fashion, would allow an agent to simultaneously plan and act over multiple temporal scales.


% \todo{expand, add in future work, discuss limitations.}
% Future directions include investigating other methods for representation learning, incorporating knowledge bases, and exploring techniques like policy distillation to improve the stability of training.
