\section{Introduction}
\label{sec:introduction}

% Points:
% \begin{itemize}
% \item traditional approaches to grounding - focus on policy learning for single domain
% \item we leverage core function of language as a compact vehicle for world knowledge to perform transfer for RL
% \item transfer approaches to RL need exploration in target task to learn effective mappings - hard for many domains, esp. with stochastic transitions and sparse rewards. 
% \item our system makes use of text descriptions to aid transfer. we ground language to environment dynamics (diff. from previous attempts at grounding directly to policy) - use a model-based learning approach
% \item description - differentiable value iteration - mapping to dynamics. 
% \item experiments and results.
% \end{itemize}

Deep reinforcement learning has emerged as a method of choice for many control applications, ranging from computer games~\cite{mnih2015dqn,silver2016mastering} to robotics~\fullcite{levine2016end}. However, the success of this approach depends on a substantial number of interactions with the environment during training, easily reaching millions of steps~\cite{nair2015massively,mnih2016asynchronous}. Moreover, given a new task, even a related one, this training process has to be performed from scratch. This inefficiency has motivated recent work in learning universal policies that can generalize across related tasks~\fullcite{schaul2015universal}, as well as other transfer approaches~\fullcite{parisotto2016actor,rajendran20172t}.  In this paper, we explore transfer methods that use \emph{text descriptions} to facilitate policy generalization across tasks.
% , leveraging a core function of language as a means for transferring knowledge.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!t]
\minipage{\linewidth}
$\vcenter{\hbox{\includegraphics[width=0.55\linewidth]{fig/boulderchase_pic}}}$ \hfill
$\vcenter{\hbox{\includegraphics[width=0.4\linewidth]{fig/descriptions}}}$
\endminipage\\

\minipage{\linewidth}
\vspace{0.3cm}
% \hdashrule[0.5ex][c]{0.6\linewidth}{0.5pt}{1.5mm}
$\vcenter{\hbox{\includegraphics[width=0.55\linewidth]{fig/bomberman_pic}}}$\hfill
$\vcenter{\hbox{\includegraphics[width=0.4\linewidth]{fig/descriptions2}}}$
\endminipage 

\caption{Examples of two different game environments, Boulderchase \textbf{(top)} and Bomberman \textbf{(bottom)}. Each domain has text descriptions (collected using Amazon Mechanical Turk) associated with specific entities, describing characteristics such as movement and interactions with the player's avatar. Note how certain pairs of entities across games share certain properties. For instance, the scorpion in Boulderchase and the spider in Bomberman are both mobile entities.}

	\label{fig:example}
\end{figure}

As an example, consider the game environments in Figure~\ref{fig:example}. The two games -- \emph{Boulderchase} and \emph{Bomberman} 
--  differ in their layouts and entity types. However, the high-level behavior of most entities in both games is similar. For instance, the \emph{scorpion} in Boulderchase (top) is a moving entity which the agent has to avoid, similar to the \emph{spider} in Bomberman (bottom). Though this similarity is clearly reflected in the text descriptions in Figure~\ref{fig:example}, it may take multiple environment interactions to discover.  Therefore, exploiting these textual clues could help an autonomous agent understand this connection more effectively, leading to faster policy learning.

% Utilizing such clues would enable the agent to capitalize on the similarity in environment dynamics to transfer prior knowledge more effectively across domains.




% \begin{figure}[!t]
% \minipage{0.48\linewidth}
% \centering
% \includegraphics[width=0.97\linewidth]{fig/game_images}
% \endminipage
% \minipage{0.48\linewidth}
% \centering
%   \includegraphics[width=0.97\linewidth]{fig/game_images2}
% \endminipage\\
% \minipage{0.48\linewidth}
% \centering
% \includegraphics[width=0.95\linewidth]{fig/descriptions}
%         \caption*{\emph{Boulderchase}} 
% \endminipage
% % {\color{black}\vrule} 
% \minipage{0.48\linewidth}%
% \centering
% \includegraphics[width=0.95\linewidth]{fig/descriptions2}
%         \caption*{\emph{Bomberman}}
% \endminipage
% \caption{Two different game environments with a few associated text descriptions. Entity names are replaced with icons for the purpose of illustration.}
% 	\label{fig:example}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To test this hypothesis, we consider multiple environments augmented with textual descriptions. 
These descriptions provide a short overview of objects and their modes of interaction in the environment.\footnote{We do not require that every object to have an associated description.} They do not describe control strategies, which were commonly used in prior work on grounding~\fullcite{vogel2010learning,branavan2012learning}. Instead, they specify the dynamics of the environments, which are more conducive to cross-domain transfer.

% The main hypothesis of our work examines the role of language in transferring prior knowledge across control applications. Therefore, our model considers multiple environments during training. Each environment is augmented with a textual description which

% Prior work on grounding, especially in the context of games, commonly utilized strategy descriptions~\cite{}. In contrast, we only consider environment descriptions which we believe are more conducive to cross-domain transfer.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to effectively use this type of information, we employ a model-based reinforcement learning approach. Typically, representations of the environment learned by these approaches are inherently domain-specific. We address this issue by using natural language as an implicit intermediate channel for transfer. Specifically, our model learns to map text descriptions to transitions and rewards in an environment, a capability that speeds up learning in unseen domains. We induce a two-part representation for the input state that generalizes over domains, incorporating both domain-specific information and textual knowledge. This representation is utilized by an action-value function, parametrized as a single deep neural network with a differentiable value iteration module~\fullcite{tamar2016value}. The entire model is trained end-to-end using rewards from the environment. 

% Our model learns the environment dynamics simultaneously with a policy, in a single deep neural network. It is implemented using a \emph{value iteration network} (VIN)~\cite{tamar2016value}. 
% However, learning a model-aware value function on its own is not sufficient to generalize across different environments. Therefore, we incorporate a module that produces a decomposed representation of the entities -- an ID-specific part and a description-informed part. This enables us to reuse existing representations for seen entities in new environments while providing a reasonable approximation for unseen entities through their descriptions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We evaluate our model on several game worlds from the GVGAI framework~\fullcite{perez2016general}. In our main evaluation scenario of transfer learning, an agent is trained on a set of source tasks and its learning performance is evaluated on a different set of target tasks. Across multiple evaluation metrics, our method consistently outperforms several baselines and an existing transfer approach for deep reinforcement learning called Actor Mimic~\cite{parisotto2016actor}. For instance, our model achieves up to 14\% higher average reward and up to 11.5\% higher initial reward - two key metrics used to evaluate transfer learning~\fullcite{taylor2009transfer}.  We also demonstrate our model's improved performance on a multi-task setting where learning is simultaneously performed on multiple environments.


The rest of this paper is organized as follows. Section~\ref{sec:related_work} summarizes related work on grounding and transfer for reinforcement learning; Section~\ref{sec:framework} provides an overview of the framework we use; Section~\ref{sec:model} describes our model architecture and its various components; Section~\ref{sec:experiments} details the experimental setup, and Section~\ref{sec:results} contains our empirical results and analysis. We conclude and discuss some future directions for research in Section~\ref{sec:conclusions}. Code for the experiments in this paper is available at \url{https://github.com/karthikncode/Grounded-RL-Transfer}.
