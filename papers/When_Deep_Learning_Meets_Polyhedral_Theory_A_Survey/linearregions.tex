
\section{The Linear Regions of a Neural Network}\label{sec:LR}

Every piece of the piecewise linear function modeled by a neural network is a linear region, 
and ---without loss of generality--- we can think of it as a polyhedron. 
In this section, we define a linear region, exemplify how they can be so numerous, and what may affect their count in a neural network. 
We also discuss the practical implications of such insights, as well as other related forms of analyzing the ability of a neural network to represent expressive models. 


\begin{definition}
A linear region corresponds to the set of points from the input space that activates the same units along the neural network, 
and hence can be characterized by the set $\sS^l$ of units that are active in each layer $l \in \sL$.
\end{definition}

% Reasons to care
Since a neural network behaves uniformly over a linear region, the latter is the smallest finite scale in which we can analyze its behavior. 
%Within each linear region 
If we restrict the domain of a neural network to a linear region $\sI \subseteq \mathbb{R}^{n_0}$, then the neural network behaves as an affine transformation $\vy_\sI : \sI \rightarrow \mathbb{R}^{n_{L}}$ of the form $\vy_\sI(\vx) = \mT \vx + \vt$ with a matrix $\mT \in \mathbb{R}^{n_{L} \times n_0}$ and a vector $\vt \in \mathbb{R}^{n_{L}}$ that are directly defined by the network parameters and the set of neurons that are activated by any input $\vx \in \sI$. 
For a small perturbation~$\varepsilon$ to some input $\overline{\vx} \in \sI$ such that $\overline{\vx}+\varepsilon \in \sI$, 
the network output for $\bar{x}+\varepsilon$ is given by $\vy_\sI(\overline{\vx}+\varepsilon)$. 
%
While it is possible that two adjacent regions defined in such way correspond to the same affine transformation, 
thinking of each linear region as having a distinct signature of active units makes it easier to analyze them.

The number of linear regions defined by a neural network is one form with which we can measure the complexity of the models that it can represent \citep{DeepArchitectures}. 
Hence, if a more complex model is desired, we may want to design a neural network that can potentially define more linear regions. %, and also 
%validate whether 
%understand whether the actual number relates to the accuracy of the trained network. 
On the one hand, the number of linear regions may grow exponentially on the depth of a neural network. 
On the other hand, such a number depends on the interplay between network parameters and hyperparameters. 
As we consider how the inputs from adjacent linear regions are evaluated, 
the change to the affine transformation can be characterized in algebraic and geometric terms. 
Understanding such changes may help us grasp how a neural network is capable of telling its inputs apart, including what are the sources of the complexity of the model. 

For neural networks in which the activation function is not piecewise linear, 
\cite{Bianchini2014} have used more elaborate topological measures to compare the expressiveness of shallow and deep neural networks. 
\cite{hu2020curve} followed a closer approach by producing a linear approximation neural network in which the number of linear regions can be counted.



\subsection{The combinatorial aspect of linear regions}

One of the most striking aspects about analyzing a neural network in terms of its linear regions is how quickly such number grows. 
Early work on this topic by~\cite{pascanu2013on} and \cite{montufar2014on} have drawn two important observations.
First, that it is possible to construct simple deep neural networks with a number of linear regions that grows exponentially in the depth.
Second, that the number of linear regions can be exponential in the number of neurons alone. 

The first observation comes from analyzing the role of ReLUs in a very simple setting. 
Namely, that of a neural network in which we regard every layer as having a single input in the $[0,1]$ domain, which is produced by combining the outputs of the units from the preceding layer, 
as illustrated by Example~\ref{ex:zigzag}. 
%
\begin{example}\label{ex:zigzag}
Consider a neural network with input $x$ from the domain $[0,1]$ and layers having 4 neurons with ReLU activation. 
For the first layer, assume that the output of the neurons are given by the following functions: 
$f_1(x)=\max\{4x,0\}$, $f_2(x)=\max\{8x-2,0\}$, $f_3(x)=\max\{6.5x-3.25,0\}$, and $f_4(x)=\max\{12.5x-11.25,0\}$. 
In other words, $\vh^1_i = f_i(x) ~\forall i \in \{1,2,3,4\}$. 
For the subsequent layers, assume that the outputs coming from the previous layer are combined through the function $F(x)=f_1(x)-f_2(x)+f_3(x)-f_4(x)$, 
which substitutes $x$ as the input to the next layer; upon which the same set of functions $\{ f_i(x) \}_{i=1}^4$ defines the output of the next layer. 
In other words, $\vh^l_i = f_i(F(\vh^{l-1})) = f_i(\evh_1^{l-1} - \evh_2^{l-1} + \evh_3^{l-1} - \evh_4^{l-1})~\forall i \in \{1,2,3,4\}, l \in \sL \setminus \{ 1 \}$.

When the output of the units in the first layer is combined as $F(x)$, we obtain a zigzagging function with 4 slopes in the $[0,1]$ domain, 
each of which defining a bijection between segments of the input ---namely, $[0,0.25]$, $[0.25, 0.5]$, $[0.5, 0.9]$, and $[0.9, 1.0]$--- and the image $[0,1]$. 
The effect of repeating such structure in the second layer is that of composing $F(x)$ with itself, 
with 4 slopes being produced within each of those 4 initial segments. 
Hence, the number of slopes ---and therefore of linear regions--- in the output of such a neural network with $L$ layers of activation functions is $4^L$, 
which implies an exponential growth on depth. 

The network structure and the parameters of the neurons in the first two layers are illustrated in Figure~\ref{fig:architecture_zigzag}; the set of functions $\{ f_i(x) \}_{i=1}^4$ and the combined outputs of the first two layers ---$F(x)$ and $F(F(x))$--- are illustrated in Figure~\ref{fig:zigzag}.
\end{example}
%
In Example~\ref{ex:zigzag}, every neuron changes the slope of the resulting function once it becomes active, 
in which we purposely alternate between positive and negative slopes once the function reaches either 0 or 1, respectively. 
By selecting the network parameters accordingly, 
\cite{montufar2014on} were the first to show that a layer with $n$ ReLUs can be used to create a zigzagging function with $n$ slopes on the $[0,1]$ domain, 
with the image along every slope also corresponding to the interval $[0,1]$. 
Consequently, stacking $L$ of such layers results in a neural network with $n^L$ linear regions.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Fig_02a_Architecture.pdf}
    \caption{Mapping from the input $x \in [0,1]$ to the intermediary output $\vh^2 \in [0,1]^{4}$ through the first two layers of a neural network in which the number of linear regions growths exponentially on the depth, as described in Example~\ref{ex:zigzag}. 
    The parameters of subsequent layers are the same as those in the second layer.}
    \label{fig:architecture_zigzag}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Fig_02_Zigzag.pdf}
    \caption{Set of activation functions $\{ f_i(x) \}_{i=1}^4$ of the units in the first layer and combined outputs of the first two layers ---$F(x) = f_1(x) - f_2(x) + f_3(x) - f_4(x)$ for the first and $F(F(x))$ for the second--- of a neural network in which the number of linear regions grows exponentially on the depth, as described in Example~\ref{ex:zigzag}.}
    \label{fig:zigzag}
\end{figure}

The second observation ---that the number of linear regions can grow exponentially in the number of neurons alone--- comes from the interplay between the parts of the input space in which each the units are active, 
especially in higher-dimensional spaces. 
This is based on some geometric observations that we discuss in Section~\ref{sec:geometry}. 
Even for a \emph{shallow} network ---i.e., the number of layers being $L=1$--- such a number of linear regions may approach $2^n$, 
which corresponds to every possible activation set $\sS \subseteq \{1, \ldots, n\}$ defining a nonempty linear region. 
However, as we discuss later, that is not always the case due to architectural choices such as the number of layers and their width.

 

%Following the characterization of linear regions by activation sets, that number can be as large as $2^{\sum_{i=1}^{L+1} n_l}$ \cite{montufar2014on}. In fact, that number can grow exponentially on the depth of the network.

%Early work  by~~\cite{pascanu2013on} and ~\cite{montufar2014on} focused on showing the benefit of depth by constructing neural networks with a large number of linear regions. 
%They have shown that layer $l$ can define $n_l$ linear regions corresponding to the slopes of a zigzagging function between 0 and 1 for an input of size $n_0=1$ in the range $[0,1]$. 
%If that same construction is used for the other layers, then the resulting neural network defines $\prod_{l=1}^{L+1} n_l$ linear regions. 
%Hence, a deep neural network with uniform layer width $n$ may define as many as $n^{L+1}$ linear regions.

\subsection{The algebra of linear regions}
\label{sec:algebraoflinearegions}

Given the activation sets $\{ \sS^l \}_{l \in \sL}$ denoting which neurons are active for each layer of the neural network, 
we can explicitly describe the affine transformation $\vy_\sI(\vx) = \mT \vx + \vt$ associated with the corresponding linear region $\sI$. 
For every activation set $\sS^l$, layer $l$ defines an affine transformation of the form $\Omega^{\sS^l}(\mW^l \vh^{l-1} + \vb^l)$, where $\Omega^{\sS^l}$ is a diagonal $n_l \times n_l$ matrix in which $\Omega^{\sS^l}_{ii} = 1$ if $i \in \sS^l$ and $\Omega^{\sS^l}_{ii} = 0$ otherwise. 
%Let $\mW^{l, \sS_l} \in \mathbb{R}^{n_l \times n_{l-1}}$ be a matrix in which $\mW^{l, \sS_l}_{i,j} = \mW^l_{i,j} ~ \forall i \in \sS_l, j \in \{1, \ldots, n_{l-1}\}$ and  $\mW^{l, \sS_l}_{i,j} = 0 ~ \forall i \in \{1, \ldots, n_l\} \setminus \sS_l, j \in \{1, \ldots, n_{l-1}\}$, i.e., we preserve the rows of $\mW^l$ associated with active units and replace the other rows with zeros. 
%Likewise, 
%let $\vb^{l, \sS_l} \in \mathbb{R}^{n_l}$ be a vector in which $\vb^{l, \sS_l} = \vb^l_i ~ \forall i \in \sS_l$ and $\vb^{l, \sS_l} = 0 ~ \forall i \in \{1, \ldots, n_l\} \setminus \sS_l$, 
%i.e., we preserve the elements of $\vb^l$ associated with active units and replace the other elements with zeros. 
Hence, the matrix $\mT$ and vector $\vt$ are as follows:
\[
%\mT = \prod_{l=1}^L \mW^{l, \sS_l} + \sum_{l_1=1}^{L-1} \left( \prod_{l_2=l_1+1}^{L} \mW^{l_2, \sS_{l_2}} \right) \vb^{l_1, \sS_{l_1}}
\mT = \prod_{l=1}^L \Omega^{\sS^l} \mW^{l},
\]
\[
\vt = \sum_{l_1=1}^{L} \left( \prod_{l_2=l_1+1}^{L} \Omega^{\sS^{l_2}} \mW^{l_2} \right) \Omega^{\sS^{l_1}} \vb^{l_1}.
\]
On a side note, 
\cite{takai2021functions} proposed a related metric for networks modeling piecewise linear functions by counting the number of distinct functions among linear regions upon equivalence through isometric affine transformation.

Each linear region is associated with a polyhedron, 
and we can describe the union of polyhedra $\mathcal{D}$ on the space $(\vx, \vh^1, \ldots, \vh^L)$ that covers the entire input space $x$ of the neural network as follows:
%We can also describe the inputs associated with those sets of active units across all layers as the following union of polyhedra 
\[
\mathcal{D} = 
\bigvee_{(\sS^1, \ldots, \sS^{L}) \subseteq \{1, \ldots, n_1\} \times \ldots \times \{1, \ldots, n_{L}\} }
\left(
\begin{array}{cc}
\vw_i^l \cdot \vh^{l-1} + b_i^l \geq 0 & \forall l \in \sL, i \in \sS^l \\
h_i^l = \vw_i^l \cdot \vh^{l-1} + b_i^l & \forall l \in \sL, i \in \sS^l \\
\vw_i^l \cdot h^{l-1} + b_i^l \leq 0 & \forall l \in \sL, i \notin \sS^l \\
h_i^l = 0 & \forall l \in \sL, i \notin \sS^l 
\end{array}
\right).
\]
Such partitioning entails an overlap between adjacent linear regions when $\vw_i^l \vh^{l-1} + b_i^l = 0$, i.e., at the boundary in which unit $i$ in layer $l$ is active in one region and inactive in another. 
Nevertheless, for any input $\overline{\vx}$ associated with a point at such a boundary between two linear regions $\sI_1$ and $\sI_2$, it holds that $\vy_{\sI_1}(\overline{\vx}) = \vy_{\sI_2}(\overline{\vx})$ even if those affine transformations are not entirely identical since the output of the neural network is continuous. More importantly, such overlap implies that each term of $\mathcal{D}$ is defined using only equalities and nonstrict inequalities, and therefore that each linear region corresponds to polyhedra in the extended space $(\vx, \vh^1, \ldots, \vh^{L})$. 
Consequently, those linear regions also define polyhedra if projected to the input space $\vx$, 
since by using Fourier-Motzkin elimination \citep{Fourier,Motzkin} 
we can obtain a polyhedral description of the linear region in $\vx$.  
Moreover, the interior of those polyhedra are disjoint. 
If one of those polyhedra does not have an interior, 
which means that it is not full-dimensional, 
then that linear region lies entirely on the boundary of other linear regions.
In such a case, we do not regard it as a proper linear region. 
By looking at the geometry of those linear regions from a different perspective in Section~\ref{sec:geometry} and understanding its impact on the number of linear regions in Section~\ref{sec:number}, we will see that many terms of $\mathcal{D}$ may actually be empty.

The optimization over the union of polyhedra is the subject of disjunctive programming, 
which has contributed to the development of stronger formulations and better algorithms to solve discrete optimization problems. These are formulated as MILPs as well as more general types of problems in recent years \citep{DPBook}, including generalized disjunctive programming for Mixed-Integer Non-Linear Programming~(MINLP) \citep{gdp1994first,gdp2012survey}. 
One of such contributions is the generation of valid inequalities to strengthen MILP formulations, which are also denoted as cutting planes, through the lift-and-project technique \citep{CGLP1,CGLP2}. 
In fact, we can develop stronger formulations for optimization problems involving neural networks through the lenses of disjunctive programming, as we discuss later in Section~\ref{sec:MIPmodels}.

%Another way to relate the study of linear regions with MILP formulations on neural networks is through disjunctive programming, which is the study of optimization over the union of polyhedra. Disjunctive programming has contributed to the development of stronger formulations and better algorithms to solve optimization problems that are formulated as MILP as well as more general types of problems in recent years [54]. One of such contributions is the generation of valid inequalities to strengthen MILP formulations, which are also denoted as cuts, through the lift-and-project technique [55]. The PI has contributed to this area by revisiting the optimization problem associated with generating lift-and-project cuts [56] and by assessing the extent to which lift-and-project cuts differ from cuts obtained with other techniques [57].

\subsection{The geometry of linear regions}\label{sec:geometry}

Another form of looking at the geometry of linear regions is through their transformation along the layers of the neural network. Namely, we can think of the input space as initially being partitioned by the units of the first layer, and then each resulting linear region being further partitioned by the subsequent layers. 
In that sense, we can think of every layer as a particular form of ``slicing'' the input. 
In fact, a layer may slice each linear region that is defined by the preceding layer in a different way due to which neurons are active or not in previous layers.  

% Geometry per layer

Let us begin by illustrating how a given layer $l \in \sL$ partitions its input space $\vh^{l-1}$. Every neuron $i$ in layer $l$ is associated with an \emph{activation hyperplane} of the form $\vw_i^l \cdot \vh^{l-1} + b_i^l = 0$, which divides the  possible inputs of its layer into an open half-space in which the unit is active ($\vw_i^l \cdot \vh^{l-1} + b_i^l > 0$) and a closed half-space in which the unit is inactive ($\vw_i^l \cdot \vh^{l-1} + b_i^l \leq 0$). These hyperplanes define the boundary between adjacent linear regions, and the arrangement of such hyperplanes for a given layer $l \in \sL$ determines how that layer partitions the $\vh^{l-1}$ space. 
In other words, every input in $\vh^{l-1}$ can be located with respect to each of those hyperplanes, which corresponds to the activation set of the linear region to which it belongs. 
However, not every activation set out of the $2^{n_l}$ possible ones maps to a nonempty region of the input space. In the case of Example~\ref{ex:hyperplane_arrangement}, there is no linear region in which the activation set is empty.

\begin{example}\label{ex:hyperplane_arrangement}
Consider a neural network with domain $\vx \in \mathbb{R}^2$ and a single layer having 3 neurons $\alpha$, $\beta$, and $\gamma$ with outputs given as follows: 
$h^1_{\alpha} = \max\{ 2.3 x_1 - 1.9 x_2 +0.6, 0\}$, $h^1_{\beta} = \max\{ -0.9 x_1 - 0.7 x_2 +4.8, 0\}$, and $h^1_{\gamma} = \max\{ 0 x_1 + 3 x_2 -5, 0\}$. 
These neurons define the activation hyperplanes ($\alpha$) $2.3 x_1 - 1.9 x_2 +0.6 = 0$, ($\beta$) $-0.9 x_1 - 0.7 x_2 +4.8 = 0$, and ($\gamma$) $0 x_1 + 3 x_2 -5 = 0$ in the space $\vx$, 
which are illustrated along with the activation sets of the linear regions in Figure~\ref{fig:hyperplane_arrangement}.

Instead of $2^3$ linear regions corresponding to each possible activation set defined by a subset of the neurons in $\{\alpha, \beta, \gamma\}$, the arrangement of such hyperplanes produces 7 linear regions, 
which is in fact the maximum number of 2-dimensional regions that can be defined by drawing 3 lines on a plane. 
\end{example}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Fig_04_HA.pdf}
    \caption{Linear regions defined by the shallow neural network described in Example~\ref{ex:hyperplane_arrangement}. 
    Every line corresponds to the activation hyperplane of a different neuron, which is given by $\alpha$, $\beta$, and $\gamma$ in parentheses. 
    The arrow next to each line points to the half space in which the inputs activate that neuron.
    Every linear region has a subset of $\{\alpha,\beta,\gamma\}$ as its corresponding activation set.}
    \label{fig:hyperplane_arrangement}
\end{figure}

The maximum number of full-dimensional regions resulting from a partitioning defined by $n$ hyperplanes depends on the dimension $d$ of the space in which those hyperplanes are defined \citep{Zaslavsky1975}. 
That number never exceeds $\sum\limits_{i=1}^{\min\{d,n\}} \binom{n}{i}$. Such bound only coincides with $2^n$ if $d \geq n$; otherwise, as illustrated in Example~\ref{ex:hyperplane_arrangement}, that number can be smaller. 
As observed by~\cite{hanin2019deep}, 
that bound is $O\left( \frac{n^d}{d!} \right)$ when $n \gg d$.

In fact,
the above bound is all that we need to determine the maximum number of linear regions in shallow networks. 
While not every shallow network may define as many linear regions, it is always possible to put the hyperplanes in what is called a \emph{general position} in order to reach that bound. 
Thus, the maximum number of linear regions defined by a shallow network is $\sum\limits_{i=0}^{\min\{n_0,n_1\}} \binom{n_1}{n_0}$.

For the polyhedron associated with each linear region, 
being in general position implies that each vertex lies on exactly $d$ activation hyperplanes. 
For context, 
the converse situation in linear programming ---having more hyperplanes active on a vertex than the space dimension--- characterizes degeneracy. 

% Transformation across layers

In the case of deep networks, the partitioning of each linear region by the subsequent layers is based on the output of that linear region. This affects the shape and the number of the linear regions defined by the following layers, which may vary between adjacent linear regions due to which units are active or inactive from one linear region to another, as illustrated in Example~\ref{ex:linear_regions}. 

\begin{example}\label{ex:linear_regions}
Consider a neural network with domain $\vx \in \mathbb{R}^2$ and 2 layers having 2 neurons each ---say neurons $\alpha$ and $\beta$ in layer 1, and neurons $\gamma$ and $\delta$ in layer 2--- with outputs given as follows: 
$h^1_{\alpha} = \max\{ 2.3 x_1 - 1.9 x_2 +1.5, 0\}$, $h^1_{\beta} = \max\{ -0.9 x_1 - 0.7 x_2 +5, 0\}$, 
$h^2_{\gamma} = \max\{ 0.4 h^1_1 - 3.1 h^1_2 +4, 0\}$, $h^2_{\delta} = \max\{ -0.6 h^1_1 - 1.6 h^1_2 +5, 0\}$.
These neurons define the activation hyperplanes ($\alpha$) $2.3 x_1 - 1.9 x_2 +1.5 = 0$ and ($\beta$) $-0.9 x_1 - 0.7 x_2 +5 = 0$ in the $\vx$ space 
and the activation hyperplanes ($\gamma$) $0.4 h^1_1 - 3.1 h^1_2 +4 = 0$ and ($\delta$) $-0.6 h^1_1 - 1.6 h^1_2 +5 = 0$ in the $\vh^1$ space, 
which are illustrated along with the activation sets of the linear regions in the first two plots of Figure~\ref{fig:linear_regions}. 
The third plot illustrates the linear regions jointly defined by the two layers in terms of the input space $\vx$.  

The third plot is repeated in Figure~\ref{fig:dimensions}, in which the shape of each linear region $\sI$ is filled in accordance to the dimension of the image of $\bar{\vy}_{\sI}(\vx)$ ---the output of the neural network for each linear region $\sI$.
\end{example}

\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{Fig_03-A.pdf}
    \includegraphics[width=0.32\textwidth]{Fig_03-B.pdf} 
    \includegraphics[width=0.32\textwidth]{Fig_03-C.pdf}
    \caption{Linear regions defined by the 2 layers of the neural network described in Example~\ref{ex:linear_regions}, 
    following the same notation as in Figure~\ref{fig:hyperplane_arrangement}. The first and second plots show the linear regions and corresponding activation sets defined by the first and the second layers in terms of their input spaces ($\vx$ and $\vh^1$). The third plot shows the linear regions defined by the combination of the 2 layers and the union of their activation sets in terms of the input space of the first layer ($\vx$).}
    \label{fig:linear_regions}
\end{figure}

Example~\ref{ex:linear_regions} highlights two important aspects about the structure of linear regions in deep neural networks. 
First, the linear regions defined by a neural network with multiple layers are different because activation hyperplanes after the first layer may look ``bent'' from the input space $x$, 
such as with the inflections of hyperplanes $(\gamma)$ and $(\delta)$ in the third plot of Figure~\ref{fig:linear_regions} from one linear region defined by the first layer to another. 
This partitioning of the input space would not be possible with a single layer. 

By comparing side by side the first and the third plots of Figure~\ref{fig:linear_regions}, 
we can see  how every linear region of a given layer of a neural network may be partitioned differently by the following layer. 
When defined in terms of the input space $\vx$, 
the hyperplanes associated with the second layer differ across the linear regions defined by the first layer 
because each of those linear regions is associated with a different affine transformation from $\vx$ to $\vh^1$. 
Hence, the activation hyperplanes of layer $l$ may break each linear region from layer $l-1$ differently. 
To every linear region defined by the hyperplane arrangement in the $\vh^{l-1}$ space there is a linear transformation $\vh^{l-1} = \Omega^{\sS^{l-1}}(\mW^{l-1} \vh^{l-2} + \vb^{l-1})$ to the points of that linear region based on the set of active neurons $\sS^{l-1}$. 
%
Consequently, inputs in the $\vh^{l-1}$ space that are associated with different linear regions are transformed differently to the $\vh^l$ space, and therefore the form in which those linear regions are further partitioned by layer $l$ is not the same when seen from the $\vh^{l-1}$ space. 

Second, some combinations of activation sets of multiple layers do not correspond to linear regions even if the activation hyperplanes are in general position with respect to each layer. 
For each layer, the first two plots of Figure~\ref{fig:linear_regions} show that every activation set corresponds to a nonempty region of the layer input. 
However, not every pair of such activation sets would define a nonempty linear region for the neural network. 
For example, the linear region of the first layer associated with the activation set $\sS^1 = \{\}$ defines a linear region in $\vx$ which is always mapped to $\vh^1=0$, 
and thus only corresponds to activation set $\sS^2 = \{ \gamma, \delta \}$ in the second layer because both units are active for such input. 
Thus, no linear region in $\vx$ is associated with only the units in sets $\{\}, \{\gamma\}$, and $\{ \delta \}$ being active ---i.e., there is no linear region such that $\sS^1 \cup \sS^2 = \{\}, \{\gamma\}, \text{or} \{\delta\}$.

% Impact on dimensions
% Take the rank, explain loss of dimension in the image of the function

More generally, the number of units that is active on each linear region defined by the first layer also imposes a geometric limit to how that linear region can be further partitioned. 
If only one unit is active at a layer, that means that the output of the layer within that linear region has dimension 1, 
and, consequently, the subsequent hyperplane arrangements within that linear region are limited to a 1-dimensional space.
For the network in the example, we thus expect no more than $\sum_{i=0}^1 \binom{2}{i} = 3$ linear regions being defined instead of $2^2 = 4$ when only one unit is active. 
In fact, 
that is precisely the number of subdivisions by the second layer of the linear region defined by activation set $\sS^1 = \{\beta\}$ from the first layer. 
%hence implying that we should not expect all possible activation sets in such case. 
%
%We may even observe a smaller number, 
%as in the case of the linear region defined by activation set $\{\alpha\}$ in the first layer only being subdivided in two regions by the second layer. 

%The loss of dimension in the output of the linear regions is actually very common, 
%since the minimum number of active units across all previous layer is an upper bound on the dimension of the output of each linear region. 
%As shown in Figure~\ref{fig:dimensions}, that may be caused by either layer. 
%There are two other linear regions with 0-dimensional output due to the absence of activations in the second layer ($\{\beta\}, \{\alpha, \beta\}$) 
%and another 6 linear regions with 1-dimensional output due to the first ($\{\alpha,\gamma,\delta\}, \{\beta,\gamma,\delta\}$), the second ($\{\alpha,\beta,\gamma\},\{\alpha,\beta,\delta\}$), or both layers ($\{\alpha,\gamma\}, \{\beta,\delta\}$).

\begin{figure}
    \centering
    \includegraphics[width=0.66\textwidth]{Fig_05.pdf}
    \caption{Dimension of the image of the affine function $\vy_{\sI}(\vx)$ associated with each linear region $\sI$ defined by the neural network described in Example~\ref{ex:linear_regions}. The linear regions are the same illustrated in the third plot of Figure~\ref{fig:linear_regions}.}
    \label{fig:dimensions}
\end{figure}


%One of the reasons for such a difference in how each linear region is further partitioned by the following layers is the loss of dimensionality in the output. 
For every linear region defined by layer $l$ with an activation set $\sS^l$, the dimension of the output of the corresponding transformation $\Omega_{\sS^l}(\mW^l \vh^{l-1} + \vb^l)$ is at most $|\sS^l|$ since $\text{rank}(\Omega_{\sS^l}) = |\sS^l|$. Hence, the dimension of the output of every linear region defined by a rectifier network is upper bounded by its smallest activation set across all layers. This phenomenon was first identified by~\cite{serra2018bounding} as the \emph{bottleneck effect}. In neural networks with uniform width, this phenomenon leads to a surprising consequence: the number of linear regions with full-dimensional output is at most one. There are also consequences to the maximum number of linear regions that can be defined, as we discuss later.
% 


\subsubsection{The geometry of decision regions}

It is also common to study what inputs are associated with each class by a neural network.  
The set of inputs associated with the same class define a \emph{decision region}. 
Difficulties in modeling functions such as the Boolean XOR in shallow networks are related to limitations on the form of the decision regions, 
which may be limited by the depth of the neural network. 
For example, \cite{makhoul1989twolayer} showed that two layers suffice to obtain disconnected decision regions. 

The softmax layer is typically used for the output of neural networks trained on classification problems,  
in which the largest output corresponds to the class to which the input is associated. 
%
In rectifier networks coupled with a softmax layer, 
the decision regions can also be defined by polyhedra. 
Although the output of the softmax layer is not piecewise linear, 
its largest output corresponds to its largest input. 
Hence, every linear region $\sI$ defined by layers 1 to $L-1$ is partitioned by the softmax layer into decision regions 
where $\vh^{L-1}_i \geq \vh^{L-1}_j ~ \forall j \neq i$ for each class $i$ associated with the input $\vh_i^{L-1}$ to the softmax layer. 
Therefore, 
each decision region of a rectifier networks consist of a union of polyhedra. 

In fact, we may say further in the typical setting where no hidden layer is wider than the input ---i.e., $n_0 \geq n_l ~ \forall l \in \sL$: 
\cite{pmlr-v80-nguyen18b} showed that at least one layer $l \in \sL$ must be such that $n_l > n_0$ for the network to present disconnected decision regions; and 
\cite{grigsby2022topology} showed that, for an input size $n_0 \geq 2$, the decision regions are either empty or unbounded. 


\subsection{The number of linear regions}\label{sec:number}


We have seen conditions that affect the number of linear regions both positively and negatively. 
We discuss these and other analytical results in Section~\ref{sec:number_analytical}, and then discuss work on counting linear regions in practice in Section~\ref{sec:number_counting}.

\subsubsection{Analytical results}\label{sec:number_analytical}

At least three lines of work on analytical results have brought important insights. 
The first line is based on constructing networks with a large number of linear regions, 
which leads to lower bounds on the maximum number of linear regions. 
The second line is based on showing how the network architecture ---in particular its hyperparameters--- may impact the hyperplane arrangements defined by the layers, 
which leads to upper bounds on the maximum number of linear regions. 
The third line is based on characterizing the parameters of neural networks according to how they are initialized and updated along training, 
which leads to results on the expected number of linear regions for such networks. 

\paragraph{Lower bounds}

The lower bounds on the maximum number of linear regions are obtained through a careful choice of network parameters aimed at increasing the number of linear regions. 
In some cases, they also depend on particular choices of hyperparameters. 
We present them by order of refinement in Table~\ref{tab:lower_bounds}.

The first lower bound was introduced by~\cite{pascanu2013on} and then improved by those authors with a new construction technique in~\cite{montufar2014on}. 
In fact, Example~\ref{ex:zigzag} shows the case in which $n_0=1$ for the technique in~\cite{montufar2014on}. 
While a different construction is proposed by~\cite{Telgarsky2015}, 
subsequent developments in the literature have been based on~\cite{montufar2014on}. 

The lower bound by~\cite{arora2018understanding} is based on a different technique to construct a first wide layer based on zonotopes, 
which is then followed by the same layers as in~\cite{montufar2014on}.  
The first lower bound by~\cite{serra2018bounding} reflects a slight change to the technique used by~\cite{montufar2014on}, 
which in terms of Example~\ref{ex:zigzag} corresponds to using $n$ neurons to define $n+1$ instead of $n$ slopes on $[0,1]$. 
The second lower bound by~\cite{serra2018bounding} extends that of~\cite{arora2018understanding} by changing in the same way the construction of the subsequent layers of the network.

\begin{landscape}
\begin{table}
\caption{Lower bounds on the maximum number of linear regions defined by a neural network.}
\label{tab:lower_bounds}
\centering
\vspace{2ex}
\begin{tabular}{@{\extracolsep{4pt}}cc}
\textbf{Reference} & \textbf{Bound and conditions}  \\
\cline{1-1}
\cline{2-2}
\noalign{\vskip2.5pt}
~\cite{pascanu2013on} & $\left(\prod\limits_{l=1}^{L-1} \left\lfloor \frac{n_l}{n_0} \right\rfloor \right) \sum\limits_{i=0}^{n_0} \binom{n_L}{i}$ \\
\noalign{\vskip4pt}
~\cite{montufar2014on} & $\left(\prod\limits_{l=1}^{L-1} \left\lfloor \frac{n_l}{n_0} \right\rfloor^{n_0}\right) \sum\limits_{i=0}^{n_0} \binom{n_L}{i}$, where $n_l \geq n_0 ~\forall l \in \sL$ \\
\noalign{\vskip4pt}
~\cite{Telgarsky2015} & $2^{\frac{L-3}{2}}$, where $n_i = 1$ for $i$ odd, $n_i = 2$ for $i$ even, and $L-3$ divides by 2 \\
\noalign{\vskip4pt}
~\cite{arora2018understanding} &  $2 \sum\limits_{j=0}^{n_0-1} \binom{m-1}{j} w^{L-1}$, where $2m=n_1$ and $w=n_l ~\forall l \in \sL \setminus \{1\}$ \\
\noalign{\vskip4pt}
~\cite{serra2018bounding} & $\left(\prod\limits_{l=1}^{L-1} \left( \left\lfloor \frac{n_l}{n_0} \right\rfloor + 1 \right)^{n_0} \right) \sum\limits_{i=0}^{n_0} \binom{n_L}{i}$, where $n_l \geq 3 n_0 ~\forall l \in \sL$ \\
\noalign{\vskip4pt}
~\cite{serra2018bounding} & $2 \sum\limits_{j=0}^{n_0-1} \binom{m-1}{j} (w+1)^{L-1}$, where $2m=n_1$ and $w=n_l \geq 3 n_0 ~\forall l \in \sL \setminus \{1\}$
\end{tabular}
\end{table}
\end{landscape}


\paragraph{Upper bounds}

The upper bounds on the maximum number of linear regions are obtained by primarily considering changes to the geometry of the linear regions from one layer to another, as previously outlined and revisited below. 
We present those with a close form by order of refinement in Table~\ref{tab:upper_bounds}. 

\cite{pascanu2013on} established the connection between linear regions and hyperplane arrangements, 
which lead to the tight bound for shallow networks based on~\cite{Zaslavsky1975} for activation hyperplanes in general position. 
\cite{montufar2014on} defined the first bound for deep networks based on enumerating all activation sets. 
The subsequent upper bounds extended the result by~\cite{pascanu2013on} to deep networks by considering its successive application through the sequence of layers of the network. 

In the case of \emph{deep} networks, where $L > 1$, 
we need to consider how the linear regions defined up to a given layer of the network can be further partitioned by the next layers. 
%
We %can 
start by assuming that every linear region defined by the first $l-1$ layers 
is then subdivided into the maximum possible number of linear regions defined by the activation hyperplanes of layer $l$. 
That leads to the bound %of $\prod_{l=1}^{L+1} \sum_{j=0}^{n_{l-1}} \binom{n_l}{j}$ by 
in~\cite{raghu2017expressive}, 
which is implicit in their proof of an asymptotic bound of $O(n^{n_0 L})$, where $n$ is used as the width of every layer. % of the network.  
However, there are many ways in which this bound can be refined upon careful examination. 
First, the dimension of the input of layer $l$ ---i.e., the output of layer $l-1$--- within each linear region is never larger than the smallest dimension among layers $1$ to $l$, since for every linear region we have an affine transformation between inputs and outputs of each layer \citep{montufar2017notes}.  
%That leads to the bound by~~\cite{montufar2017notes}. % of  $\prod_{l=1}^{L+1} \sum_{j=0}^{d_l} \binom{n_l}{j}$ linear regions, where $d_l = \min\{n_0, n_1, \ldots, n_l\}$. 
Second, the dimension of the input coming through each linear region is in fact bounded by the smallest number of active units in each of the previous layers \citep{serra2018bounding}.  
%That leads to the bound by~~\cite{serra2018bounding}. % of  $\sum_{(j_1,\ldots,j_L) \in J} \prod_{l=1}^L \binom{n_l}{j_l}$ linear regions, where $J = \{(j_1, \ldots, j_L) \in \mathbb{Z}^L: 0 \leq j_l \leq \min\{n_0, n_1 - j_1, \ldots, n_{l-1} - j_{l-1}, n_l\}\ \forall l \in \sL \}$. 
This leads to a tight upper bound for $n_0=1$, since it matches the lower bound in ~\cite{serra2018bounding}. 
Finally, 
the activation hyperplane of some units may not partition the linear regions because all possible inputs to the unit are in the same half-space, and in some of those cases the unit may never produce a positive output. 
For the number $k$ of active units in a given layer $l$, we can use the network parameters to calculate the maximum number of units that can be active in the next layer, $\mathcal{A}_l(k)$, as well as the number of units that can be active or inactive for different inputs, $\mathcal{I}_{l}(k)$ \citep{serra2020empirical}. 

\cite{hinz2019framework} observed that the upper bound by \cite{serra2018bounding} can be tightened by explicitly computing a recursive histogram of linear regions on the layers of the neural network according to the dimension of their image subspace. However, the resulting bound is not explicitly defined in terms of the network hyperparameters, and hence cannot be included on the table. This work is further extended in~\cite{hinz2021histograms} by also allowing a composition of bounds on subnetworks instead of only on the sequence of layers. 
Another extension of the framework from~\cite{hinz2019framework} by \cite{yutong2020framework} highlights that residual connections prevent the bottleneck effect in ResNets, 
by which reason such networks tend to have more linear regions. 

\cite{cai2023pruning} proposed a separate recursive bound based on \cite{serra2018bounding} to account for the sparsity of the weight matrices, 
which illustrates how pruning connections may affect the maximum number of linear regions.

The results above have also been extended to other architectures. 
In some cases, results on other types of activations are also part of the papers previously mentioned: 
\cite{montufar2014on} and \cite{serra2018bounding} present upper bounds for \emph{maxout} networks; 
\cite{raghu2017expressive} present an upper bound for networks using \emph{hard tanh} activation. 
In other cases, the ideas discussed above have been adapted for sparser networks with parameter sharing: 
\cite{xiong2020cnn} present upper and lower bounds for convolutional networks, 
which are shown to asymptotically define more linear regions per parameter than rectifier networks with the same input size and number of layers. 
\cite{chen2022gcn} present upper and lower bounds for graph convolutional networks. 
\cite{matoba2022maxpooling} discuss the expresiveness of the maxpooling layers typically used in convolutional neural networks through their equivalence to a sequence of rectifier layers. 
Moreover, \cite{goujon2022role} present results for recently proposed activation functions, 
such as DeepSpline~\citep{agostineli2015spline,unser2019representer,bohra2020learning} and GroupSort~\citep{anil2019groupsort}. 

Some of the results above were also revisited through the lenses of tropical algebra, 
in which every linear region corresponds to a tropical hypersurface \citep{zhang2018tropical,charisopoulos2018tropical,maragos2021tropical}. 
Notably, \cite{montufar2022maxout} presented considerably tighter upper bounds for the number of linear regions in maxout networks with rank $k=3$ or greater. 

% DEPTH GOES HERE
Recently, a converse line of work started exploring the minimum dimensions of a neural network capable of representing a given piecewise linear function, 
starting with considerations about the minimum depth necessary \citep{arora2018understanding} and further refinements of bounds on the network dimensions \citep{he2020finite,hertrich2021depth,chen2022bounds}, 
with \cite{chen2022bounds} proposing an algorithm that can construct such a neural network. 
On a related note, 
\cite{MPC} show that linear time-invariant systems in model predictive control can be exactly expressed by rectifier networks and provide bounds on the width and number of layers necessary for a given system, whereas \cite{ferlez2020aren} describe an algorithm for producing architectures that can be parameterized as an optimal model predictive control strategy.

\begin{landscape}
\begin{table}
\caption{Upper bounds on the maximum number of linear regions defined by a neural network.}
\label{tab:upper_bounds}
\centering
\vspace{2ex}
\begin{tabular}{@{\extracolsep{4pt}}cc}
\textbf{Reference} & \textbf{Bound and conditions}  \\
\cline{1-1}
\cline{2-2}
\noalign{\vskip2.5pt}
~\cite{pascanu2013on} & $\sum\limits_{i=0}^{n_0} \binom{n_1}{n_0}$ for shallow networks, $n_1 \geq n_0$ \\
\noalign{\vskip4pt}
~\cite{montufar2014on} & $2^{\sum\limits_{l=1}^{L} n_l}$ \\
\noalign{\vskip4pt}
~\cite{raghu2017expressive} & $\prod\limits_{l=1}^{L} \sum\limits_{j=0}^{n_{l-1}} \binom{n_l}{j}$ \\
\noalign{\vskip4pt}
~\cite{montufar2017notes} & $\prod\limits_{l=1}^{L} \sum\limits_{j=0}^{d_l} \binom{n_l}{j}$, $d_l = \min\{n_0, n_1, \ldots, n_{l-1}\}$ \\
\noalign{\vskip4pt}
~\cite{serra2018bounding} & $\begin{array}{r}\sum\limits_{(j_1,\ldots,j_L) \in J} \prod\limits_{l=1}^L \binom{n_l}{j_l}, J = \{(j_1, \ldots, j_L) \in \mathbb{Z}^L: 0 \leq j_l \leq d_l ~\forall l \in \sL \},\\ d_l = \min\{n_0, n_1 - j_1, \ldots, n_{l-1} - j_{l-1}, n_l\}\ \end{array}$ \\
\noalign{\vskip4pt}
~\cite{serra2020empirical}  & $\begin{array}{r}\sum\limits_{(j_1,\ldots,j_L) \in J} \prod\limits_{l=1}^L \binom{\mathcal{I}_l(k_{l-1})}{j_l}, J = \{(j_1, \ldots, j_L) \in \mathbb{Z}^L: 0 \leq j_l \leq d_l, \\ d_l = \min\{n_0, k_1, \ldots, k_{l-1}, \mathcal{I}_l(k_{l-1})\}, k_0 = n_0, k_l = \mathcal{A}_{l}(k_{l-1}) - j_{l-1} ~\forall l \in \sL \}\end{array}$
\end{tabular}
\end{table}
\end{landscape}

\paragraph{Expected number}

The third analytical approach has been the evaluation of the expected number of linear regions. 
In a pair of papers, Hanin and Rolnick studied the number of linear regions based on how the network parameters are typically initialized.  
In the first paper \citep{hanin2019complexity}, 
they show that the average number of linear regions along 1-dimensional subspaces of the input grows linearly with respect to the number of neurons, irrespective of the network depth. 
In the second paper \citep{hanin2019deep}, 
they show that the average number of linear regions in higher-dimensional subspaces of the input also grows similarly in deep and shallow networks. 
For $N = \sum_{i=1}^L n_i$ as the total number of linear regions, 
the expected number of linear regions is  $O(2^N)$ if $N \leq n_0$ and $O\left(\frac{(T N)^{n_0}}{n_0!}\right)$ otherwise, 
where $T > 0$ is a constant based on the network parameters. 
Moreover, some of their experiments suggest that the number of linear regions in shallow networks is slightly greater. 
According to the authors, 
these bounds reflect the fact that the family of functions that can be represented by neural networks in the way that they are typically initialized is considerably smaller. 
They further argue that training as currently performed is unlikely to expand the family of functions much further, as illustrated by their experiments. 
% 
Similar results on the expected number of linear regions for maxout networks are presented by~\cite{tseran2021expected}, 
and an application of the results above results to data manifolds is explored by~\cite{tiwari2022manifolds}. 
Additional results for specific architectures of rectifier networks are conjectured by~\cite{wang2022estimation}, although without proof.


%Under reasonable conditions on how the network parameters are initialized, 
%~\cite{hanin2019complexity} have shown that the expected number of linear regions at initialization is linear on the number of units of the network and that it does not depend on its depth. 
%Those authors argue that this is not in conflict with the exponential lower bounds proven in the literature, 
%but instead that such constructions with a large number of linear regions can collapse to relatively smaller numbers through small adjustments to the parameters. 


\subsubsection{Counting linear regions}\label{sec:number_counting}

Counting the actual number of linear regions of a given network has been a more challenging topic to explore. 
\cite{serra2018bounding} have shown that the linear regions of a trained network can be enumerated as the solutions of an MILP formulation, 
which has been slightly corrected in~\cite{cai2023pruning}\footnote{The MILP formulation of neural networks is discussed in Section~\ref{sec:optimizing}.}. 
However, MILP solutions are generally counted one by one \citep{danna2007multiple}, with exception of special cases \citep{serra2020nearoptimal} and small subproblems \citep{serra2020enumerative}, which makes this approach impractical for large neural networks. 
\cite{serra2020empirical} have shown that approximate model counting methods, which are commonly used to count the number of feasible assignments in propositional satisfiability, can be easily adapted to solution counting in MILP, which leads to an order-of-magnitude speedup in comparison with exact counting. 
This type of approach is particularly suitable for obtaining probabilistic lower bounds, which can complement the analytical upper bounds for the maximum number of linear regions.  
In \cite{craighero2020compositional} and \cite{craighero2020understanding}, a directed acyclic graph is used to model the sets of active neurons on each layer and how they connected with those in subsequent layers. 
\cite{yang2020reachability} describe a method for decomposing the input space of rectifier networks into their linear regions by representing each linear region in terms of its face lattice, upon which the splitting operations corresponding to the transformations performed by each layer can be implemented. As the number of linear regions grow, these splitting operations can be processed in parallel. \cite{yang2021reachability} extend that method to convolutional neural networks. 
Moreover, \cite{wang2022estimation} describes an algorithm for enumerating linear regions that counts adjacent linear regions with same corresponding affine function as a single linear region. 

Another approach is to enumerate the linear regions in subspaces, which limits their number and reduces the complexity of the task. 
This idea was first explored by \cite{novak2018sensitivity} for measuring the complexity of a neural network in terms of the number of transitions along a single line. 
\cite{hanin2019complexity,hanin2019deep} also use this method with a bounded line segment or rectangle as a single set representing the input and then sequentially partitioning it. 
If this first set is intersected by the activation hyperplane of a neuron in the first layer, 
then we replace this set by two sets corresponding to the parts of the input space in which that neuron is active and not. 
Once those sets are further subdivided by all activation hyperplanes associated with the neurons in the first layer, 
the process can be continued with the neurons in the following layers. 
This method is used to count the number of linear regions along subspaces of the input with dimension 1 in \cite{hanin2019complexity} and dimension 2 in \cite{hanin2019deep}. 
A generalized version for counting the number of linear regions in affine subspaces spanned by a set of samples using an MILP formulation is presented in \cite{cai2023pruning}. 
An approximate approach for counting the number of linear regions along a line by computing the closest activation hyperplane in each layer is presented in \cite{gamba2022equal}. 


Other approaches have obtained lower bounds on the number of linear regions of a trained network by limiting the enumeration or considering exclusively the inputs from the dataset. 
In \cite{xiong2020cnn}, the number of linear regions is estimated by sampling points from the input space and enumerating all activation patterns identified through this process. 
In \cite{cohan2022evolution}, the counting is restricted to the linear regions found between consecutive states of a neural network modeling a reinforcement learning policy. 



\subsection{Applications and insights}

Thinking about neural networks in terms of linear regions led to a variety of applications. 
In turn, that inspired further studies on the structure and properties of linear regions under different settings. 
We organize the literature about applications and insights around some central themes in the subsections below. 


\subsubsection{The number of linear regions}

From our discussion, the number of linear regions emerges as a potential proxy for the complexity of neural networks, 
which has been studied by some authors and exploited empirically by others. 
\cite{novak2018sensitivity} observed that the number of transitions between linear regions in 1-dimensional subspaces correlates with generalization. 
\cite{hu2020distillation} used bounds on the number of linear regions as proxy to model the capacity of a neural network used for learning through distillation, in which a smaller network is trained based on the outputs of another network. 
\cite{chen2021nas} and \cite{chen2021nas2} present one of the first approaches to training-free neural architectural search through the analysis of network properties. One of the two metrics that they have shown to be effective for that purpose is the number of linear regions associated with a sample of inputs from the training set on randomly initialized networks. 
\cite{biau2021wgans} observed that obtaining a discriminator network for Wasserstein GANs~\citep{arjovsky2017wgan} that correctly approximates the Wasserstein distance entails that such a discriminator network has a growing number of linear regions as the complexity of the data distribution increases. 
\cite{park2021unsupervised} maximized the number of linear regions in unsupervised learning in order to produce more expressive encodings for downstream tasks using simpler classifiers. 
In neural networks modeling reinforcement learning policies, 
\cite{cohan2022evolution} observed that the number of transitions between linear regions in inputs corresponding to consecutive states increases by 50\% with training while the number of repeated linear regions decreases. 
\cite{cai2023pruning} proposed a method for pruning different proportions of parameters from each layer by maximizes the bound on the number of linear regions, 
which lead to better accuracy than uniform pruning across layers. 
On a related note, 
\cite{liang2021brelu} proposed a new variant of the ReLU activation function for dividing the input space into a greater number of linear regions. 

The number of linear regions also inspired further theoretical work. 
\cite{amrami2021benefit} presented an argument for the benefit of depth in neural networks based on the number of linear regions for correctly separating samples associated with different classes. 
\cite{liu2021approximation} studied upper and lower bounds on the optimal approximation error of a convex univariate function based on the number of linear regions of a rectifier network. 
\cite{henriksen2022repairing} used the maximum number of linear regions as a metric for capacity that may limit repairing incorrect classifications in a neural network. 


\subsubsection{The shape of linear regions}

Some studies aimed at understanding what affects the shape of linear regions in practice, including how to train neural networks in such a way to induce certain shapes in the linear regions. 
%
\cite{empirical2020iclr} observed that multiple training techniques may lead to similar accuracy, but very different shape for the linear regions. 
For example, batch normalization~\citep{ioffe2015batchnorm} and dropout~\citep{srivastava2014dropout} lead to more linear regions. 
While batch normalization breaks the space in regions with uniform size, more orthogonal norms, and more gradient variability across adjacent regions; dropout produces more linear regions around decision boundaries, norms are more parallel, and data points less likely to be in the region containing the decision boundary. 
\cite{croce2019max} and \cite{LocallyLinear} applied regularization to the loss function to push the boundary of each linear region away from points in the training set that it contains, as long as those points are correctly classified. They show that this form of regularization improves the robustness of the neural network while making the linear regions larger.
In fact, \cite{zhu2020local} observed that the boundaries of the linear regions move away from the training data; 
and \cite{HashEncoders} conjectured that the linear regions near training samples becomes smaller through training, or that conversely the activation patterns are denser around the training samples. 
\cite{gamba2020biased} presented an empirical study on the angles between activation hyperplanes defined by convolutional layers, and observed that their cosines tend to be similar and more negative with depth after training.

The geometry of linear regions also led to other theoretical and algorithmic advances. 
Theoretically, \cite{phuong2020equivalence} proved that architectures with nonincreasing layer widths have unique parameters ---upon permutation and scaling--- for representing certain functions. 
In other words, some pairs of neural networks are only equivalent if their parameters only differ by permutation and multiplication. 
\cite{grigsby2023symmetries} showed that equivalences other than by permutation are less likely to occur with greater input size and width, but more likely with greater depth.
Algorithmically, 
\cite{ReverseEngineering} proposed a procedure to reconstruct a neural network by evaluating several inputs in order to determine regions of the input space 
for which the output of the neural network can be defined by an affine function ---and thus consist of a single linear region. 
Depending on how the shape changes between adjacent linear regions, 
the boundaries of the linear regions are replicated with neurons in the first hidden layer or in subsequent layers of the reconstructed neural network. 
\cite{masden2022combinatorial} provided theoretical results and an algorithm for characterizing the face lattice of the polyhedron associated with each linear region. 


\subsubsection{Activation patterns and the discrimination of inputs}

Another common theme is understanding how inputs from the training and test sets are distributed among the linear regions, and what can be inferred the encoding of the activation patterns associated with the linear regions.
%
\cite{gopinath2019property} noted that many properties of neural networks, including the classes of different inputs, are associated with activation patterns ---and thus with their linear regions. 
Several works~\citep{HashEncoders,sattelberg2020locally,tropex2021iclr} observed that each training sample is typically located in a different linear region when the neural network is sufficiently expressive; whereas
\cite{HashEncoders} noted that simple machine learning algorithms can be applied using the activation patterns as features, and 
\cite{sattelberg2020locally} noted that there is some similarity between activation patterns of different neural networks under affine mapping, meaning that the training of these neural networks lead to similar models.
\cite{chaudhry2020continual} exploited the idea of continual learning with different tasks being encoded in disjoint subspaces, 
which thus corresponds to a disjoint set of activation sets on each layer being associated with classifications for each of those tasks. 
Based on their approach for enumerating linear regions, 
\cite{craighero2020compositional} and \cite{craighero2020understanding} have found that inputs from larger linear regions are often correctly classified by the neural network, that inputs from smaller linear regions are often incorrectly classified, and that the number of distinct activations sets reduces along the layers of the neural network. 
\cite{gamba2022equal} also discussed the issue of some linear regions being smaller and thus less likely to occur in practice. 
Moreover, they propose a measurement for the similarity of the affine functions associated with linear regions along a line 
and observed that the linear regions tend to be less similar to one another when the network is trained with incorrectly classified labels. 

 
\subsubsection{Function approximation}

Because of the linear behavior of the output within each linear region, 
we can approximate the output of the neural network based on the output of its linear regions. 
%
\cite{chu2018pwnn} and \cite{sudjianto2020unwrapping} produced linear models based on this local behavior; whereas 
\cite{glass2021relumot} observed that we can interpret neural networks as equivalent to local linear model trees~\citep{nelles2000lolimot}, in which a distinct linear model is used at each leaf of a decision tree, and provided a method to produce such models from neural networks.
\cite{tropex2021iclr} described how to extract the linear regions associated with the inputs from the training set as means to approximate the output of the inputs from the test set. 
\cite{robinson2019dissecting} presented another approach for explicitly representing the function modeled by a neural network through the enumeration of its linear regions. 
On a related note, 
\cite{chaudhry2020continual} used the assumption of training samples remaining within the same linear region during gradient descent to simplify the analysis of backpropagation.

This topic also relates to the broad literature on neural networks as universal function approximators, to which the concept of linear regions helps articulating ideal conditions. 
As observed by \cite{mhaskar2020approximation}, the optimal number of linear regions in a neural network ---or, correspondingly, of pieces of the piecewise linear function modeled by it--- depends on the function being approximated.
In addition, linear regions were also used explicitly to build function approximations. 
\cite{kumar2019equivalent} have shown that rectifier networks can we approximated to arbitrary precision with two hidden layers, the largest of which having a neuron corresponding to each different activation pattern of the original network; an exact counterpart of this result was later presented by~\cite{villani2023shallow}.
\cite{fan2020quasiequivalence} described the transformation between sufficiently wide and deep networks while arguing that the fundamental measure of complexity should be counting simplices within linear regions. 
In subsequent work, \cite{fan2023simple} empirically observed that linear regions tend to have a small number of higher dimensional faces, or facets.

More recent studies aimed at understanding the expressiveness and approximability of neural networks in terms of their number of parameters, 
in particular when the number of linear regions is greater than the number of parameters \citep{fractals2019neurips,fractals2020ieee,daubechies2022approximation}. 
They all discuss how the composition the modeled functions tend to present the self-similarity property of fractal distributions, 
which is one reason why they have so many linear regions. 
\cite{keup2022origami} interpreted the connection between linear regions in different parts of the input space in terms of how paper origamis are constructed: by ``folding'' the data for separability. 

Another related topic is computing the Lipschitz constant $\rho$ of the function $f(x)$ modeled by the neural network, 
the smallest $\rho$ for which $\| f(x') - f(x) \| \leq \rho \| x' - x \|$ for any two inputs $x$ and $x'$. 
Note that the first derivative of the output of a linear region is constant, 
which is leveraged by~\cite{hwang2020unrectifying} to evaluate the stability of the network by computing the constant across linear regions by changing the activation pattern. 
Interestingly, \cite{zhou2019lipschitz} showed that the constant grows similarly to the number of linear regions: polynomial in width and exponential in depth. 
A smaller constant limits the susceptibility of the network to adversarial examples~\citep{huster2018limitations}, which are discussed in Section~\ref{sec:optimizing}, 
and also lead to smaller bias variance \citep{loukas2021training}. 
While calculating the exact Lipschitz constant is NP-hard and encourages approximations \citep{scaman2018lipschitz,combettes2019certificates}, 
the exact constant can be computed using MILP \citep{jordan2020exact}. 
Notably, 
many studies have focused on relaxations such as linear programming \citep{zou2019cnnlp}, semidefinite programming \citep{fazlyab2019sdp,chen2020sdp}, and polynomial optimization \citep{latorre2020polynomial}. 
An alternative approach is to use more sophisticated activation functions for limiting the value of the constant \citep{anil2019groupsort,aziznejad2020controledlipschitz}. 



\subsubsection{Optimizing over linear regions}

As an alternative to optimizing over neural networks as described next in Section~\ref{sec:optimizing}, 
a number of approaches have resorted to techniques that are equivalent to systematically enumerating or traversing linear regions and optimizing over them \citep{croce2018gcpr,croce2020ijcv,khedr2020verification,vincent2021icra,xu2022advml}.
Notably, \cite{vincent2021icra} and \cite{xu2022advml} are mindful of the facet-defining inequalities associated with a linear region, 
which are the ones to change when moving toward an adjacent linear region. 
On a related note, 
\cite{seck2021lp} alternates between gradient steps and solving a linear programming model within the current linear region.

