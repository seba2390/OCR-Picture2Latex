

\section{Linear Programming and Polyhedral Theory in Training}\label{sec:training}

In the previous sections, we have almost exclusively focused on tasks involving neural networks that have already been constructed, i.e., we have assumed that the training step has already concluded (with the exception of Section \ref{sec:adversarialtraining}). In this section, we focus on the training phase, whose goal is to construct a neural network that can represent the relationship between the input and output of a given set of data points.

Let us consider a set of points, or sample, $(\sampleinput_i,\sampleoutput_i)_{i=1}^\samplesize$, and assume that these points are related via a function $\functolearn$, i.e., $\functolearn(\sampleinput_i) = \sampleoutput_i$ $i=1,\ldots, \samplesize$. In the training phase, we look for $\functolearn$ in a pre-defined class (e.g. neural networks with a specific architecture) that approximates the relation $\functolearn(\sampleinput_i) = \sampleoutput_i$. Typically, this is done by solving an Empirical Risk Minimization problem
\begin{equation}
    \label{eq:ERMproblem}
    \min_{\functolearn \in F} \frac{1}{\samplesize} \sum_{i=1}^\samplesize \loss(\functolearn(\sampleinput_i), \sampleoutput_i)
\end{equation}
where $\loss$ is a loss function and $F$ is the class of functions we are restricted to. We usually assume the class $F$ is parametrized by $(\weights,\biases)\in \parameterset$ (the network weights and biases), so we are further assuming that there exists a function $\NNfunction{\cdot}{\cdot}{\cdot}$ (the network architecture) such that
\[\forall \functolearn \in F,\, \exists (\weights,\biases)\in \parameterset,\, \functolearn(\vx) = \NNfunction{\vx}{\weights}{\biases},\]
and thus, the optimization is performed over the space of parameters. In many cases, $\parameterset = \mathbb{R}^\parameternumber$---the parameters are unrestricted real numbers---but we will see some cases when a different parameter space can be used.

As mentioned in the introduction, nowadays, most of the practically successful \emph{training} algorithms for neural networks, i.e., that solve or approximate \eqref{eq:ERMproblem}, are based on Stochastic Gradient Descent (SGD). From a fundamental perspective, optimization problem \eqref{eq:ERMproblem} is typically a \emph{non-convex, unconstrained} problem that needs to be solved efficiently and where finding a \emph{local minimum} is sufficient. Thus, it is not too surprising that linear programming appears to be an unsuitable tool in this phase, in general. Nonetheless, there are some notable and surprising exceptions to this, which we review here.

Linear programming played an interesting role in training neural networks before SGD became the predominant training method and provided an efficient approach for constructing neural networks with 1 hidden layer in the 90s. This method has some common points in their polyhedral approach with the first known algorithm that can solve \eqref{eq:ERMproblem} to provable optimality for a 1-hidden-layer ReLU neural network, which was proposed in 2018. Recently, a stream of work has exploited similar polyhedral structures to obtain convex optimization reformulations of regularized training problems of ReLU networks. Linear programming tools have also been used within SGD-type methods in order to compute optimal \emph{step-sizes} in the optimization of \eqref{eq:ERMproblem} or to strictly enforce structure in $\parameterset$. From a different perspective, a \emph{data-independent} polytope was used to describe approximately all training problems that can arise from an uncertainty set. Additionally, a back-propagation-like algorithm for training neural networks, which solves mixed-integer linear problems in each layer, was proposed as an alternative to SGD. Furthermore, when the neural network weights are required to be discrete, the applicability of SGD is impaired, and mixed-integer linear models have been proposed to tackle the corresponding training problems.

In what follows, we review these roles of (mixed-integer) linear programming and polyhedral theory within training contexts. We refer the reader to the book by \cite{goodfellow2016deep} and the surveys by \cite{curtis2017optimization}, \cite{bottou2018optimization}, and \cite{wright2018optimization} for in-depth descriptions and analyses of the most commonly used training methods for neural networks.

We remark that solving the training problem to global optimality for ReLU neural networks is computationally complex. Even in architectures with just one hidden node, the problem is NP-hard \citep{dey2020approximation,goel2021tight}. Also see \cite{blum1992training,boob2022complexity,chen2022learning,froese2022computational,froese2023training} for other hardness results. Furthermore, it has been recently shown that training ReLU networks is $\exists \mathbb{R}$-complete \citep{abrahamsen2021training,bertschinger2022training}, which implies that it is likely that the problem of optimally training ReLU neural networks is not even in NP. 
%
Therefore, it is not strange to see that some of the methods we review below, even when they are solving hard problems as sub-routines (like mixed-integer linear problems), either make some non-trivial assumptions or relax some requirements. For example, boundedness and/or integrality of the weights, architecture restrictions such as the output dimension, or not having optimality guarantees. 

It is worth mentioning that, in contrast, for LTUs, exact exponential-time training algorithms are known for much more general architectures than in the ReLU case \citep{khalife2022neural,ergen2023globally}. These are out of scope for this survey, though we will provide a high-level overview of some of them, as they share some similarities to approaches designed for ReLU networks.

\subsection{Training neural networks with a single hidden layer}

Following the well-known XOR limitation of the perceptron \citep{perceptrons}, a natural interest arose in the development of training algorithms that could handle at least one hidden layer. In this section, we review training algorithms that can successfully minimize the training error in a one-hidden-layer setting and rely on polyhedral approaches.

\subsubsection{Problem setting and solution scheme}

Suppose we have a sample of size $\samplesize$ $(\sampleinput_i,\sampleoutput_i)_{i=1}^\samplesize$ where $\sampleinput_i \in \R^n$ and $\sampleoutput_i\in \mathbb{R}$. In a training phase, we would like to find a neural network function $\NNfunction{\cdot}{\cdot}{\cdot}$ that represents in the best possible way the relation $\NNfunction{\sampleinput_i}{\weights}{\biases} = \sampleoutput_i$.

Note that when a neural network $\hat{f}$ has only one hidden layer, its behavior is almost completely determined by the sign of each component of the vector
%
\[\weights^1 x - \biases^1.\]
%
These are the cases of ReLU activations $\sigma(z) = \max\{0,z\}$ and LTU activations $\sigma(z) = \text{sgn}(z)$. The training algorithms we show here heavily exploit this observation and construct $(\weights^1,\biases^1)$ by embedding in this phase a \emph{hyperplane partition} problem based on the sample $(\sampleinput_i,\sampleoutput_i)_{i=1}^\samplesize$.
%
While the focus of this survey is mainly devoted to ReLU activations, we also discuss some selected cases with LTU activations as they share some similar ideas.

\subsubsection{LTU activations and variable number of nodes}

One stream of work dedicated to developing training algorithms for one-hidden-layer networks concerned the use of \emph{backpropagation} \citep{BackPOP1,BackPOP2,BackpropNN}. In the early 90s, an alternative family of methods was proposed, which was heavily based on linear programs (see e.g. \cite{bennett1990neural,bennett1992robust,roy1993polynomial,mukhopadhyay1993polynomial}). These approaches can construct a 1-hidden-layer network without the need for an \emph{a-priori} number of nodes in the network. We illustrate the high-level idea of these next, based on the survey by \cite{mangasarian1993mathematical}.\\

Suppose that $\sampleoutput_i\in \{-1,1\}$, thus the NN we construct will be a classifier. The training phase can be tackled via the construction of a \emph{polyhedral partition} of $\mathbb{R}^n$ such that no two points (or few) $\sampleinput_i$ and $\sampleinput_j$ such that $\sampleoutput_i \neq \sampleoutput_j$ lie in the same element of the partition. To achieve this, the following approach presented by \cite{bennett1992robust} can be followed. Let $\setofones = \{i \in [\samplesize]\, :\, \sampleoutput_i = 1\}$ and $N = [\samplesize]\setminus \setofones$, and consider the following optimization problem
%
\begin{subequations}\label{eq:separationLP}
\begin{align}
    \min_{\vw,b,y,z} \quad & \frac{1}{|\setofones|} \sum_{i\in \setofones} y_i + \frac{1}{|N|} \sum_{i\in N} z_i \\
    & \vw^\top \sampleinput_i - b + y \geq 1 && \forall i\in \setofones \\
    & - \vw^\top \sampleinput_i + b + z \geq 1 && \forall i \in N \\
    & y,z\geq 0.
\end{align}
\end{subequations}
%
This LP aims at finding a hyperplane $\vw^\top x  = b$ separating the data according to their value of $\sampleoutput_i$. Since the data may not be separable, the LP is minimizing the following classification error

\[\frac{1}{|\setofones|} \sum_{i\in \setofones} (-\vw^\top \sampleinput_i + b + 1)_+ + \frac{1}{|N|} \sum_{i\in N} ( \vw^\top \sampleinput_i - b + 1 )_+.\]
%
The LP \eqref{eq:separationLP} is a linear reformulation of the latter minimization problem, where the auxiliary values $y,z$ take the value of each element in the sum.

Once the LP \eqref{eq:separationLP} is solved, we obtain 2 halfspaces classifying our data points. In order to obtain a richer classification and lower error, we can iterate the procedure by means of the Multi-Surface Method Tree (MSMT, see \cite{bennett1992decision}), which solves a sequence of LPs as \eqref{eq:separationLP} in order to produce a polyhedral partition of $\mathbb{R}^n$. Let us illustrate how this procedure works in a simplified case: assume that solving \eqref{eq:separationLP} results in a vector $\vw_1$ such that
%
\[\{i \,: \, \vw_1^\top \sampleinput_i \geq b_1 \} \subseteq \setofones\quad \land \quad \{i \,: \, \vw_1^\top \sampleinput_i \leq a_1 \} \subseteq N, \]
%
%i.e. one side of the hyperplane only contains data points of one class.
for some $a_1,b_1\in\mathbb{R}^n$ with $b_1 > a_1$. We can remove the sets $\{(\sampleinput_i, \sampleoutput_i) \,: \, \vw_1^\top \sampleinput_i \geq b_1 \}$ and $\{(\sampleinput_i, \sampleoutput_i) \,: \, \vw_1^\top \sampleinput_i \leq a_1 \}$ from the data-set and redefine \eqref{eq:separationLP} accordingly, in order to obtain a new vector $\vw_2$ and scalars $b_2,a_2$ that would be used to classify within the region $\{x\in \R^n \,: \,  a_1 < \vw_1^\top \vx < b_1 \}$.

% Up to this point, the classification is defined by 3 polyhedral regions:
% %
% \begin{align*}
% &\{x\in \R^n \,: \, w_1^\top {x}_i \geq b_1 \},\\
% &\{x\in \R^n \,: \, w_1^\top {x}_i < b_1,\,  w_2^\top {x}_i \geq b_2  \}, \text{and} \\
% &\{x\in \R^n \,: \, w_1^\top {x}_i < b_1,\,  w_2^\top {x}_i < b_2  \}.
% \end{align*}
%
This procedure can be iterated, and the polyhedral partition of $\R^n$ induced by the resulting hyperplanes can be easily transformed into a Neural Network with 1 hidden layer and LTU activations (see \cite{bennett1990neural} for details). We illustrate this transformation with the following example: suppose that after 3 iterations we have the following regions, with the arrow indicating to which class each region is associated to:
%
\begin{subequations}\label{eq:hyperplanestoNN}
\begin{align}
&\{\vx\in \R^n \,: \, \vw_1^\top \vx \geq b_1 \} \to Y, \label{eq:firstYclass}\\
&\{\vx\in \R^n \,: \, \vw_1^\top \vx \leq a_1 \} \to N , \label{eq:firstNclass}\\
&\{\vx\in \R^n \,: \, a_1 < \vw_1^\top \vx < b_1,\,  \vw_2^\top \vx \geq b_2  \}\to Y, \label{eq:secondYclass}\\
&\{\vx\in \R^n \,: \, a_1 < \vw_1^\top \vx < b_1,\,  \vw_2^\top \vx \leq a_2  \}\to N, \label{eq:secondNclass}\\
&\{\vx\in \R^n \,: \, a_1 < \vw_1^\top \vx < b_1,\,  a_2 < \vw_2^\top \vx < b_2,\, \vw_3^\top x \geq (a_3 + b_3)/2 \} \to Y, \label{eq:thirdYclass}\\
&\{\vx\in \R^n \,: \, a_1 < \vw_1^\top \vx < b_1,\,  a_2 < \vw_2^\top \vx < b_2,\, \vw_3^\top x < (a_3 + b_3)/2 \} \to N \label{eq:thirdNclass}.
\end{align}
\end{subequations}
Since regions \eqref{eq:thirdYclass} and \eqref{eq:thirdNclass} are the last defined by the algorithm (under some stopping criterion), they both use $(a_3 + b_3)/2$ in order to obtain a well-defined partition of $\mathbb{R}^n$. In Figure \ref{fig:HyperplaneToNN} we show a one-hidden-layer neural network that represents such a classifier. The structure of the neural network can be easily extended to handle more regions.
%
\begin{figure}
    \centering
\begin{tikzpicture}[auto]
    \node[draw, ellipse,align=left] (input) at (0,0) {$\vx_1$\\$\vdots$\\$\vx_\inputdimension$};
     \node[draw, circle] (h1) at (3,2.5) {$b_1$};
     \node[draw, circle] (h2) at (3,1.5) {$-a_1$};
    \node[draw, circle] (h3) at (3,0.5) {$b_2$};
     \node[draw, circle] (h4) at (3,-0.5) {$-a_2$};
     \node[draw, circle,scale=0.75] (h5) at (3,-1.5) {$\frac{a_3+b_3}{2}$};
     \node[draw, circle,scale=0.75] (h6) at (3,-2.5) {$\frac{-a_3-b_3}{2}$};
      \node[draw, circle] (output) at (6,0) {$0$};
     \draw[->,thick] (input) edge node[below]{$\vw_1$} (h1) 
     (input) edge node[below]{$-\vw_1$} (h2) 
     (input) edge node[below]{$\vw_2$} (h3) 
     (input) edge node[below]{$-\vw_2$} (h4)
     (input) edge node[below]{$\vw_3$} (h5) 
     (input) edge node[below]{$-\vw_3$} (h6)
     (h1) edge node[below]{$2^2$} (output)
     (h2) edge node[below]{$-2^2$} (output)
     (h3) edge node[below]{$2^1$} (output)
     (h4) edge node[below]{$-2^1$} (output)
     (h5) edge node[below]{$1$} (output)
     (h6) edge node[below]{$-1$} (output); 
\end{tikzpicture}
\caption{Illustration of Neural Network with LTU activations using MSMT. Inside each node of the hidden layer, we show the thresholds used in each LTU activation.}
    \label{fig:HyperplaneToNN}
\end{figure}
%
For other details, we refer the reader to \cite{bennett1992decision}, and for variants and extensions see \cite{mangasarian1993mathematical} and references therein.

Some key features of this procedure are the following:
\begin{itemize}
    \item Each solution of \eqref{eq:separationLP}, i.e., each new hyperplane, can be represented as a new node in the hidden layer of the resulting neural network.
    \item The addition of a new hyperplane comes with a reduction in the current loss; this can be iterated until a target loss is met.
    \item Thanks to the universal approximation theorem \citep{Hornik1989}, with enough nodes in the hidden layer, we can always obtain a neural network $\hat{f}$ with zero classification error. Although this can lead to over-fitting.\\
\end{itemize}


The work of \cite{roy1993polynomial} and \cite{mukhopadhyay1993polynomial} follow a related idea, although the classifiers which are built are quadratic functions. To illustrate the approach, we use the same set-up for \eqref{eq:separationLP}. The approach in \cite{roy1993polynomial} and \cite{mukhopadhyay1993polynomial} aims at finding a function
\[f_{\mV,\vw,b}(x) = \vx^\top \mV \vx + \vw^\top \vx + b \]
such that
\[f_{\mV,\vw,b}(\sampleinput_i) \geq 0 \Longleftrightarrow i\in \setofones.\]
%
Since this may not be possible, the authors propose solving the following LP
%
\begin{subequations}\label{eq:separationLP2}
\begin{align}
    \min_{W,w,b,\epsilon} \quad & \epsilon \\
    & \sampleinput_i^\top \mV \sampleinput_i + \vw^\top\sampleinput_i + b  \geq \epsilon && \forall i\in Y \\
    & \sampleinput_i^\top \mV \sampleinput_i + \vw^\top\sampleinput_i + b  \leq -\epsilon && \forall i\not\in Y \\
    & \epsilon \geq \epsilon_0
\end{align}
\end{subequations}
%
for some fixed tolerance $\epsilon_0 >0$. When this LP is infeasible, the class $\setofones$ is partitioned into $\setofones_1$ and $\setofones_2$, and an LP as \eqref{eq:separationLP2} is solved for both $Y_1$ and $Y_2$. The algorithm then follows iteratively (see below for comments on these iterations). In the end, the algorithm will construct $k$ quadratic functions $f_1, \ldots, f_k$, which the authors call ``masking functions'', that will classify an input $\vx$ in the class $\setofones$ if and only if
\[\exists i\in [k],\, f_i(\vx) \geq 0.\]

In order to represent the resulting classifier as a single-layer neural network, the authors proceed in a similar manner to a linear classifier; the input layer of the resulting neural network not only includes each entry of $\vx$, but also the bilinear terms $\vx\vx^\top$. Using this input, the classifier built by \eqref{eq:separationLP2} can be thought of as a linear classifier (much like a polynomial regression can be cast as a linear regression).

As a last comment on the work of \cite{roy1993polynomial} and  \cite{mukhopadhyay1993polynomial}, the authors' algorithm does not iterate in a straightforward fashion. They add clustering iterations alternating with the steps described above in order to (a) identify outliers and remove them from the training set, and  (b) subdivide the training data when \eqref{eq:separationLP2} is infeasible. These additions allow them to obtain a polynomial-time algorithm.\\

The methods described in this section are able to produce a neural network with arbitrary quality, however, there is no guarantee on the size of the resulting neural network. When the size of the network is fixed the story changes, which is the case we describe next.

\subsubsection{Fixed number of nodes and ReLU activations}
\label{sec:arora}
As mentioned at the beginning of this section, training a neural network
is a complex optimization problem in general, with some results indicating that the problem is likely to not even be in NP \citep{abrahamsen2021training,bertschinger2022training}.
%Given the computational implications of these results, along with the over-fitting issues associated to the minimum achievable loss for a fixed data-set, less attention has been devoted to the construction of training algorithms for ReLU networks with optimality guarantees.

Nonetheless, by restricting the network architecture sufficiently and allowing exponential running times, exact algorithms can be conceived. An important step in the construction of such algorithms was taken by  \cite{arora2018understanding}. In this work, the authors studied the training problem in detail, providing the first 
optimization algorithm capable of solving the training problem to provable optimality for a fixed network architecture with one hidden layer and with an output dimension of 1. As we anticipated, this algorithm shares some similarities with the previous approach.

Let us consider now a ReLU activation. Also, we no longer assume $\sampleoutput_i \in \{-1,1\}$, but we keep the output dimension as 1. The problem considered by \cite{arora2018understanding} reads
%
\begin{equation}
    \label{eq:ERMproblemArora}
    \min_{\weights,\biases} \frac{1}{\samplesize} \sum_{i=1}^\samplesize \loss (\weights^2(\sigma(\weights^1\sampleinput_i + \biases^1)), \sampleoutput_i),
\end{equation}
%
with $\loss: \R \times \R \to \R$ a convex loss. Note that this problem, even if $\loss$ is convex, is a non-convex optimization problem.

\begin{theorem}[\cite{arora2018understanding}]
Let $\layerwidth_1$ be the number of nodes in the hidden layer. There exists an algorithm to find a global optimum of \eqref{eq:ERMproblemArora} in time $O(2^{\layerwidth_1} \samplesize^{\inputdimension \cdot \layerwidth_1} \text{poly}(\samplesize,\inputdimension,\layerwidth_1))$.
\end{theorem}

Roughly speaking, the algorithm works by noting that one can assume the weights in $\weights^2$ are in $\{-1,1\}$, since $\sigma$ is positively-homogeneous. Thus, problem \eqref{eq:ERMproblemArora} can be restated as
\begin{equation}
    \label{eq:ERMproblemArora-simplified}
    \min_{\weights^1,\biases^1, s} \frac{1}{\samplesize} \sum_{i=1}^\samplesize \loss (s (\sigma(\weights^1\sampleinput_i + \biases^1)), \sampleoutput_i)
\end{equation}
where $s\in \{-1,1\}^{\layerwidth_1}$. In order to handle the non-linearity, \cite{arora2018understanding} ``guess'' the values of $s$ and the sign of each component of $\weights^1\sampleinput_i + \biases^1$. Enforcing a sign for each component of $\weights^1\sampleinput_i + \biases^1$ is similar to the approach discussed in the previous section: it fixes how the input part of the data $(\sampleinput_i)_{i=1}^\samplesize$ is partitioned in polyhedral regions by a number of hyperplanes. The difference is that, in this case, the number of hyperplanes to be used is assumed to be fixed.

Using the hyperplane arrangement theorem (see e.g. \cite[Proposition 6.1.1]{matousek2002lectures}), there are at most $\samplesize^{\inputdimension \layerwidth_1}$ ways of fixing the signs of $\weights^1 \sampleinput_i + \biases^1$. Additionally, there are at most $2^{\layerwidth_1}$ possible vectors in $\{-1,1\}^{\layerwidth_1}$. Once these components are fixed, \eqref{eq:ERMproblemArora-simplified} can be solved as an optimization problem with a convex objective function and a polyhedral feasible region imposing the desired signs in $\weights^1\sampleinput_i + \biases^1$. This results in the $O(2^{\layerwidth_1} \samplesize^{\inputdimension \layerwidth_1} \text{poly}(\samplesize,\inputdimension,\layerwidth_1))$ running time.
%
This algorithm was recently generalized to concave loss functions by \cite{froese2022computational}.

\cite{dey2020approximation} developed a polynomial-time approximation algorithm in this setting for the case of $\layerwidth_1 = 1$ (i.e., one ReLU neuron) and square loss. This approximation algorithm has a better performance when the input dimension is much larger than the sample size, i.e. $\inputdimension \gg \samplesize$. The approach by \cite{dey2020approximation} also relies on fixing the signs of $\weights^1\sampleinput_i + \biases^1$, and then solving multiple convex optimization problems, but in different strategy than that of \cite{arora2018understanding}; in particular, \cite{dey2020approximation} only explore a polynomial number of the possible ``fixings'', which yields the approximation. 
%

We note that the result by \cite{arora2018understanding} shows that the training problem on their architecture is in NP. This is in contrast to  \cite{bertschinger2022training}, who show that training a neural network with one hidden layer is likely to not be in NP. The big difference lies in the assumption on the output dimension: in the case of \cite{bertschinger2022training}, the output dimension is two. It is quite remarkable that such a sharp complexity gap is produced by a small change in the output dimension.

\subsubsection{An exact training algorithm for arbitrary LTU architectures}

Recently, \cite{khalife2022neural} presented a new algorithm, akin to that in \cite{arora2018understanding}, capable of solving the training problem to global optimality for any fixed LTU architecture with a convex loss function $\loss$. In this case, no assumption on the network's depth is made. The algorithm runs in polynomial time on the sample size $\samplesize$ when the architecture is fixed.

We will not describe this approach in detail, as it heavily relies on the structure given by LTU activations, which is intricate and beyond the scope of this survey.
%
Although we note that it shares some high-level similarities to the algorithm of \cite{arora2018understanding} for ReLU activations, such as ``guessing" the behavior of the neurons' activity and then solving multiple convex optimization problems. However, the structural and algorithmic details are considerably different.

It is important to note that this result reveals the big gap between what is known for LTU versus ReLU activations in terms of their training problems. In the case of the former, there is an exact algorithm for arbitrary architectures; in the case of the latter, the known results are much more restricted and strong computational limitations exist.

\subsection{Convex reformulations in regularized training problems}

For the case when the training problem is regularized, the following stream of work has developed several convex reformulations of it.
%
\cite{pilanci2020neural} presented the first convex reformulation of a training problem for the case with one hidden layer and one-dimensional outputs. As the approach described in Section \ref{sec:arora}, this reformulation uses hyperplane arrangements according to the activation patterns of the ReLU units, but instead of using them algorithmically directly, they use them to find their convex reformulations. 
%
This framework was further extended to CNNs by \cite{ergen2021implicit}.
%
Higher-dimensional outputs in neural networks with one hidden layer were considered in \cite{pmlr-v108-ergen20a,ergen2021convex,sahiner2021vectoroutput}. This convex optimization perspective was also applied in Batch Normalization by \cite{ergen2022demystifying}. 
%

These approaches provide polynomial-time algorithms when some parameters (e.g., the input dimension $\inputdimension$) are considered constant. We note that this does not contradict the hardness result of \cite{froese2023training}, as the latter does not include a regularizing term. We explain below where the regularizing term plays an important role.
%
Training via convex optimization was further developed to handle deeper regularized neural networks in \cite{ergen2021global,ergen2021path,ergen2021revealing}.

In what follows, we review the convex reformulation in \cite{pilanci2020neural} (one hidden layer and one-dimensional output) to illustrate some of the base strategies behind these approaches.
%
We refer the reader to the previously mentioned articles for the most recent and intricate developments, as well as numerical experiments.\\

As before, let $\layerwidth_1$ be the number of nodes in the hidden layer. Let us consider the following regularized training problem; to simplify the discussion, we omit biases.
%
\begin{equation}
    \label{eq:ERMproblemCVX}
    \min_{\weights} \quad \frac{1}{2} \left\|\sum_{j=1}^{\layerwidth_1} \weights^2_j\sigma(\tilde{\mX}\weights^1_j) - \sampleoutput \right\|^2 + 
    \frac{\beta}{2}\sum_{j=1}^{\layerwidth_1}( \| \weights^1_j\|^2 + (\weights^2_j)^2)
\end{equation}
%
Here, $\beta > 0$, $\tilde{\mX}$ is a matrix whose $i$-th row is $\sampleinput_i$ and $\weights^1_j$ is the vector of weights going into neuron $j$. Thus, $\tilde{\mX}\weights^1_j$ is a vector whose $i$-th component is the input to neuron $j$ when evaluating the network on $\sampleinput_i$. $\weights^2_j$ is a scalar: it is the weight on the arc from neuron $j$ to the output neuron (one-dimensional). Note that there is a slight notation overload: $(\weights^2_j)^2$ is the square of the scalar $\weights^2_j$. However, we will quickly remove this (pontentially confusing) term.

Problem \eqref{eq:ERMproblemCVX} is a regularized version of \eqref{eq:ERMproblemArora} when $\ell$ is the squared difference. We modified its presentation to match the structure in \cite{pilanci2020neural}. The authors first prove that \eqref{eq:ERMproblemCVX} is equivalent to 
%
\[\min_{\|\weights^1_j\| \leq 1} \min_{\weights^2_j} \quad \frac{1}{2} \left\|\sum_{j=1}^{\layerwidth_1} \weights^2_j\sigma(\tilde{\mX}\weights^1_j) - \sampleoutput \right\|^2 + 
    \beta\sum_{j=1}^{\layerwidth_1} | \weights^2_j|\]
%
Then, through a series of reformulations and duality arguments, the authors first show that this problem is lower bounded by
%
\begin{subequations} \label{eq:ERMproblemCVX2}
\begin{align}
    \max & \quad - \frac{1}{2} \left\|v - \sampleoutput \right\|^2 + 
    \frac{1}{2}\|\sampleoutput\|^2\\
    \mbox{s.t} & \quad |v^\top \sigma(\tilde{\mX} u )| \leq \beta && \forall u, \, \|u\|\leq 1\\
    &\quad v\in \mathbb{R}^\samplesize
\end{align}
\end{subequations}
%
Problem \eqref{eq:ERMproblemCVX2} has $\samplesize$ variables and infinitely many constraints.
%
The authors show that this lower bound is tight when the number of neurons in the hidden layer is large enough; specifically, they require $\layerwidth_1 \geq m^*$, where $m^* \in \{1,\ldots, \samplesize\}$ is defined as the number of Dirac deltas in an optimal solution of a dual of \eqref{eq:ERMproblemCVX2} (see \cite{pilanci2020neural} for details). 

Regarding the presence of infinitely many constraints, the authors address this by considering all possible patterns of signs of $\tilde{\mX} u$ (similarly to \cite{arora2018understanding}, as discussed in Section \ref{sec:arora}). For each fixed sign pattern (hyperplane arrangement), they apply a duality argument which allows them to recast the constraint $\max_{u\in \mathcal{B}}|v^\top \sigma(\tilde{\mX} u )|  \leq \beta$ as a finite collection of second-order cone constraints with $\beta$ on the right-hand side.

Finally, using that $\beta > 0$, they show that the reformulated problem satisfies Slater's condition, and thus from strong duality they obtain the following convex optimization problem, which has the same objective value as \eqref{eq:ERMproblemCVX2}.
%
\begin{subequations} \label{eq:finalcvx}
    \begin{align}
    \min&\quad \frac{1}{2} \left\|\sum_{j=1}^{P} M_i \tilde{\mX}(v_i - w_i) - \sampleoutput \right\|^2 + 
    \beta \sum_{j=1}^{P}( \| v_i\| + \|w_i\|)\\
    \mbox{s.t} & \quad  (2M_i - I_\samplesize) \tilde{\mX} v_i \geq 0  && \forall i\in [P]\\
    & \quad  (2M_i - I_\samplesize) \tilde{\mX} w_i \geq 0 && \forall i\in [P] \\
    & \quad  v_i \in \mathbb{R}^\inputdimension  &&\forall i\in [P]\\
    & \quad w_i \in \mathbb{R}^\inputdimension && \forall i\in [P]
    \end{align}
\end{subequations}
%
Here, $I_\samplesize$ is the $\samplesize \times \samplesize$ identity matrix, $P$ is the number of possible activation patterns for $\tilde{\mX}$, and each  $M_i$ is a $\samplesize \times \samplesize$ binary diagonal matrix whose diagonal indicates the $i$-th possible sign pattern of $\tilde{\mX} u$. This means that $(M_{i})_{j,j}$ is 1 if and only if $\sampleinput_j^\top u \geq 0$ in the $i$-th sign pattern of $\tilde{\mX} u$. Moreover, the authors provide a formula to recover a solution of \eqref{eq:ERMproblemCVX} from a solution of \eqref{eq:finalcvx}.

Using that $P\leq 2r(e(\samplesize-1)/r)^r$, where $r=\mbox{rank}(\tilde{\mX})$, the authors note that the formulation \eqref{eq:finalcvx} yields a training algorithm with complexity $O(\inputdimension^3 r^3 (\samplesize/r)^{3r})$. Note that if one fixes $r$, the resulting algorithm runs polynomial time. In particular, fixing $\inputdimension$ fixes the rank of $\tilde{\mX}$ and results in a polynomial time algorithm as well. In contrast, the algorithm by \cite{arora2018understanding} discussed in Section \ref{sec:arora} remains exponential even after fixing $\inputdimension$. Moreover,  \cite{froese2023training} showed that the training problem is NP-Hard even for fixed $\inputdimension$.
%
This apparent contradiction is explained by two key components of the convex reformulation: the regularization term and the presence of a ``large enough'' number of hidden neurons. This facilitates the exponential improvement of the training algorithm with respect to \cite{arora2018understanding}.

\subsection{Frank-Wolfe in DNN training algorithms}

Another stream of work that has included components of linear programming in DNN training involves the Frank-Wolfe method. We briefly describe this method in the non-stochastic version next. In this section, we omit the biases $\biases$ to simplify the notation.\\

Gradient descent (and its variants) is designed for problems of the form
%
\begin{equation}\label{eq:unconstrained}
\min_{\weights \in \mathbb{R}^\parameternumber} \ERMfunction(\weights)
\end{equation}
%
and it is based on iterations of the form
%
\begin{equation}\label{eq:gradientupdate}
\weights(i+1) = \weights(i) - \alpha_i \nabla \ERMfunction(\weights(i)) 
\end{equation}
where $\alpha_i$ is known as the \emph{learning rate}. In the stochastic versions, $\nabla \ERMfunction(\weights(i))$ is replaced by a stochastic gradient.
%
In this setting, these algorithms would find a local minimum, which is global when $\ERMfunction$ is convex. 

In the presence of constraints $\weights \in \parameterset$, however, this strategy may not work directly. A regularizing term is typically used in the objective function instead of a constraint, that ``encourages'' $\weights \in \parameterset$ but does not enforce it. If we strictly require that $\weights \in \parameterset \neq \mathbb{R}^n$, and $\parameterset$ is a convex set, one could modify \eqref{eq:gradientupdate} to
%
\begin{equation}\label{eq:projectionphi}
\weights(i+1) = \text{proj}_{\parameterset} \left( \weights(i) - \alpha_i \nabla \ERMfunction(\weights(i)) \right).    
\end{equation}
%
and thus ensure that all iterates $\weights(i) \in \parameterset$. Unfortunately, a projection is a costly routine. An alternative to this projection is the Frank-Wolfe method \citep{frank1956algorithm}. Here, a direction $\vd_i$ is computed via the following linear-objective convex optimization problem
%
\begin{equation}
    \label{eq:FrankWolfeLP}
    \vd_i \in \arg\min_{\vd \in \parameterset} \vv_i^\top \vd
\end{equation}
%
where normally $\vv_i = \nabla \ERMfunction(\weights(i))$ (we consider variants below). The update is then computed as
%
\begin{equation}\label{eq:FrankWolfeUpdate}
\weights(i+1) = \weights(i) + \alpha_i (\vd_i - \weights(i)),
\end{equation}
%
for $\alpha_i\in [0,1]$. Note that, by convexity, we are assured that $\weights(i+1) \in \parameterset$ as long as $\weights(0)\in \parameterset$. In many applications, $\parameterset$ is polyhedral, which makes \eqref{eq:FrankWolfeLP} a linear program. Moreover, for simple sets $\parameterset$, problem \eqref{eq:FrankWolfeLP} admits closed-form solutions.\\

In the context of deep neural network training, two notable applications of Frank-Wolfe have appeared. Firstly, the Deep Frank Wolfe algorithm, by  \cite{berrada2018deep}, which modifies iteration \eqref{eq:gradientupdate} with an optimization problem that can be solved using Frank-Wolfe in its dual. Secondly, the use of a stochastic version of Frank-Wolfe in the training problem \eqref{eq:ERMproblem} by \cite{pokutta2020deep} and \cite{xie2020efficient}, which enforces structure in the neural network weights directly. We review these next, starting with the latter.

\subsubsection{Stochastic Frank-Wolfe}
Note that problem \eqref{eq:ERMproblem} is of the form \eqref{eq:unconstrained} with
\[\ERMfunction(\weights) = \frac{1}{\samplesize} \sum_{i=1}^\samplesize \loss(f(\sampleinput_i, \weights), \sampleoutput_i).  \]
%
We remind the reader that we are omitting the biases in this section to simplify notation, as they can be incorporated as part of $\weights$.

Usually, some structure of the weights is commonly desired, (e.g. sparsity or boundedness), which traditionally have been incorporated as regularizing terms in the objective, as mentioned above.
%
The recent work by \cite{xie2020efficient} and \cite{pokutta2020deep}, on the other hand, enforce structure on $\parameterset$ directly using Frank-Wolfe ---more precisely, stochastic versions of it.\\

\cite{xie2020efficient} use a stochastic Frank-Wolfe approach to impose an $\ell_1$-norm constraint on the weights and biases $\weights$ when training a neural network with 1 hidden layer. Note that $\ell_1$ constraints are polyhedral.
 %
 Their algorithm is designed for a general Online Convex Optimization setting, where ``losses'' are revealed in each iteration. However, in their computational experiments, they included tests in an offline setting given by a DNN training problem. 
 
 The approach follows the Frank-Wolfe method described above closely. The key difference lies in the estimation of the stochastic gradient they use, which is not standard and it is one of the most important aspects of the algorithm. Instead of using $\vv_i = \nabla \ERMfunction(\weights(i))$ in \eqref{eq:FrankWolfeLP}, the following \emph{stochastic recursive estimator} of the gradient is used:
 %
 \begin{align*}
 \vv_0 =& \tilde{\nabla}\ERMfunction(\weights(0)) \\
 \vv_i =& \tilde{\nabla}\ERMfunction(\weights(i)) + (1-\rho_i)(v_{i-1} - \tilde{\nabla}\ERMfunction(\weights(i-1)))
 \end{align*}
 %
 where $\tilde{\nabla}\ERMfunction$ is a stochastic gradient, and $\rho_i$ is a parameter. The authors show that the gradient approximation error of this estimator converges to 0 at a sublinear rate, with high probability. This is important for them to analyze the ``regret bounds'' they provide for the online setting.

The experimental results in \cite{xie2020efficient} in DNN training are very positive. They test their approach in the MNIST and CIFAR10 datasets and outperform existing state-of-the-art approaches in terms of suboptimality, training accuracy, and test accuracy.\\


\cite{pokutta2020deep} implement and test several variants of stochastic versions of Frank-Wolfe in the training of neural networks, including the approach by \cite{xie2020efficient}. \cite{pokutta2020deep} focus their experiments on their main proposed variant, which they refer to simply as Stochastic Frank-Wolfe (SFW). This variant uses
%
\[\vv_i = (1-\rho_i)\vv_{i-1} + \rho_i \tilde{\nabla}\ERMfunction(\weights(i)),\]
%
where $\rho_i$ is a momentum parameter. The authors propose many different options for $\parameterset$ including $\ell_1, \ell_2$ and $\ell_\infty$ balls, and $K$-sparse polytopes. Of these, only the $\ell_2$ ball is non-polyhedral.

Overall, the computational experiments are promising for SFW. The authors advocate for this algorithm arguing that it provides excellent computational performances while being simple to implement and competitive with other state-of-the-art algorithms.

\subsubsection{Deep Frank-Wolfe}

Another application of Frank-Wolfe within DNN training was proposed by \cite{berrada2018deep}. While this approach does not make heavy use of linear programming techniques, the application of Frank-Wolfe is quite novel, and they do rely on one linear program needed when performing an update as \eqref{eq:FrankWolfeUpdate}. \\

The authors note that \eqref{eq:gradientupdate} can also be written as the solution to the following \emph{proximal} problem \citep{bubeck2015convex}:
%
\begin{equation}\label{eq:proximalupdate}
    \weights(i+1) = \arg\min_{\weights}\,\left\{ \frac{1}{2\alpha_i}\|\weights - \weights(i)\|^2 + \mathcal{T}_{\weights(i)}(\ERMfunction(\weights)) \right\}
\end{equation}
%
where $\mathcal{T}_{\weights(i)}$ represents the first-order Taylor expansion at $\weights(i)$. We are omitting regularizing terms since they do not play a fundamental role in the approach; all this discussion can be directly extended to include regularizers. \cite{berrada2018deep} note that \eqref{eq:proximalupdate} linearizes the loss function, and propose the following \emph{loss-preserving proximal} problem to replace \eqref{eq:proximalupdate}: 
%
\begin{equation}\label{eq:lossproximalupdate}
    \weights(i+1) = \arg\min_{\weights}\,\left\{ \frac{1}{2\alpha_i}\|\weights - \weights(i)\|^2 + \frac{1}{\samplesize} \sum_{i=1}^\samplesize \loss(\mathcal{T}_{\weights(i)}(f(\sampleinput_i, \weights)), \sampleoutput_i) \right\}
\end{equation}

Using the results by \cite{lacoste2013block}, the authors argue that \eqref{eq:lossproximalupdate} is amenable to Frank-Wolfe in the dual when $\loss$ is piecewise linear and convex (e.g. the hinge loss). To be more specific, the authors show that in this case, and assuming $\alpha_i = \alpha$, there exists $\mA,\vb$ such that the dual of \eqref{eq:lossproximalupdate} is simply
%
\begin{subequations} \label{eq:dualDFW}
\begin{align}
 \max_{\mathbf{\beta}} \quad  &  \frac{-1}{2\alpha} \|\mA \mathbf{\beta} \|^2 + \vb^\top \mathbf{\beta} \\
 \text{s.t.} \quad & \mathbf{1}^\top \mathbf{\beta} = 1 \\
 & \mathbf{\beta} \geq 0
\end{align}
\end{subequations}
%
The authors consider applying Frank-Wolfe to this last problem, and to recover the primal solution using the primal-dual relation $\weights = -\mA\mathbf{\beta}$, which is a consequence of KKT. The Frank-Wolfe iteration \eqref{eq:FrankWolfeUpdate} in the notation of \eqref{eq:dualDFW} would look like
\begin{equation}\label{eq:FrankWolfeLP-dual}
\mathbf{\beta}_{i+1} = \mathbf{\beta}_{i} + \gamma_i (\vd_i - \mathbf{\beta}_i).
\end{equation}
Here, $\vd_i$ is feasible for \eqref{eq:dualDFW} and obtained using a linear programming oracle, and $\gamma_i$ the Frank-Wolfe step-length. Note that the feasible region of \eqref{eq:dualDFW} is a simplex: exploiting this, the authors show that an optimal $\gamma_i$ can be computed in closed-form: here, ``optimal'' refers to a minimizer of \eqref{eq:dualDFW} when restricted to points of the form $\mathbf{\beta}_{i} + \gamma_i (\vd_i - \mathbf{\beta}_i)$.

With all these considerations, the bottleneck in this application of Frank-Wolfe is obtaining $\vd_i$; recall that this Frank-Wolfe routine is embedded within a single iteration of the overall training algorithm; therefore, in each iteration of the training algorithm, possibly multiple computations of $\vd_i$ would be required in order to solve \eqref{eq:dualDFW} to optimality. To alleviate this, the authors propose to only perform one iteration of Frank-Wolfe: they set $\vd_0$ to be the stochastic gradient and compute a closed-form expression for $\mathbf{\beta}_{1}$. This is the basic ingredient of the Deep Frank Wolfe (DFW). It is worth noting that this algorithm is not guaranteed to converge, however, its empirical performance is competitive.

Other two important considerations are taken into account the implementation of this algorithm: smoothing of the loss function (as the Hinge loss is piecewise linear) and the adaptation of Nesterov's Momentum to this new setting. We refer the reader to the corresponding article for these details. One of the key features of DFW is that it only requires one hyperparameter ($\alpha$) to be tuned.

The authors test DFW in image classification and natural language inference. Overall, the results obtained by DFW are very positive: in most cases, it can outperform adaptive gradient methods, and it is competitive with SGD while converging faster. 

\subsection{Polyhedral encoding of multiple training problems}

One of the questions raised by \cite{arora2018understanding} (see Section \ref{sec:arora}) was whether the dependency on $\samplesize$ of their algorithm could be improved since it is typically the largest coefficient in a training problem. This question was studied by \cite{TrainingLP}, who show that, in an approximation setting, a more ambitious goal is achievable: there is a polyhedral encoding of multiple training problems whose size has a mild dependency on $D$.

As in the previous section, we omit the biases $\biases$ to simplify notation, as all parameters can be included in $\weights$. Let us assume the class of neural networks $F$ in \eqref{eq:ERMproblem} are restricted to have bounded parameters (we assume they lie in the interval $[-1,1]$), and let us assume the sample has been normalized in such a way that $(\sampleinput_i, \sampleoutput_i) \in [-1,1]^{\inputdimension+\outputdimension}$. Furthermore, let $\parameternumber$ be the dimension of $\parameterset$ (the number of parameters in the neural network). With this notation, we define the following.

\begin{definition}
Consider the ERM problem \eqref{eq:ERMproblem} with parameters $\samplesize, \parameterset, \loss, f$ --- sample size, parameter space, loss function, network architecture, respectively. For a function $g$, let $\mathcal{K}_\infty(g)$ be the Lipschitz constant of $g$ using the infinity norm. We define the \emph{Architecture Lipschitz Constant} $\mathcal{K}(\samplesize,\parameterset,\loss, f)$ as
\begin{equation}
    \label{eq:Lipschitz-Arch}
    \mathcal{K}(\samplesize,\parameterset,\loss, f) \doteq \mathcal{K}_\infty(\loss(f(\cdot , \cdot), \cdot ))
\end{equation}
over the domain $[-1,1]^\inputdimension  \times \parameterset \times [-1,1]^\outputdimension$.
\end{definition}

Using this definition, and the boundedness of parameters, a straightforward approximate training algorithm can be devised whose running time is linear in $D$. Simply do a grid search in the parameters' space, and evaluate all data points in each possible parameter. It is not hard to see that, to achieve $\epsilon$-optimality, such an algorithm would run in time which is linear in $D$ and exponential in $ \mathcal{K}(\samplesize,\parameterset,\loss, f) / \epsilon$.
%
What was proved by \cite{TrainingLP} is that one can take a step further and represent multiple training problems at the same time.

\begin{theorem}[\cite{TrainingLP}]\label{theorem:bienstocketal}
  Consider the ERM problem \eqref{eq:ERMproblem} with parameters $\samplesize, \parameterset, \loss, f$, and let $\mathcal{K}:= \mathcal{K}(\samplesize,\parameterset,\loss, f)$ be the corresponding network architecture. Consider $\epsilon > 0$ arbitrary. There exists a polytope $P_{\epsilon}$ of size\footnote{Here, the size of the polytope is the number of variables and constraints describing it.}
  %
  \[O(\samplesize \left(2\mathcal{K}/\epsilon\right)^{\inputdimension+\outputdimension+\parameternumber}) \]
 %
  with the following properties:
  %
  \begin{enumerate}
      \item $P_{\epsilon}$ can be constructed in time $O(\left(2\mathcal{K}/\epsilon\right)^{\inputdimension+\outputdimension+\parameternumber} \samplesize)$ plus the time required for $O(\left(2\mathcal{K}/\epsilon\right)^{\inputdimension+\outputdimension+\parameternumber})$ evaluations of the loss function $\loss$ and $f$.
      \item For \emph{any} sample $(\tilde{X}, \tilde{Y}) = (\sampleinput_i, \sampleoutput_i)_{i=1}^\samplesize$, $(\sampleinput_i,\sampleoutput_i) \in [-1,1]^{\inputdimension+\outputdimension}$, there is a face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ of $P_{\epsilon}$ such that optimizing a linear function over $\mathcal{F}_{\tilde{X},\tilde{Y}}$ yields an $\epsilon$-approximation to the ERM problem \eqref{eq:ERMproblem}.
      \item  The face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ arises by simply substituting-in actual data for the data-variables $x,y$, which is used to fixed variables in the description of $P_{\epsilon}$.
  \end{enumerate}
\end{theorem}

This result is very abstract in nature but possesses some interesting features. Firstly, it encodes (approximately) \emph{every} possible training problem arising from data in $[-1,1]^{\inputdimension+\outputdimension}$ using a benign dependency on $\samplesize$: the polytope size depends only linearly on $\samplesize$, while a discretized enumeration of all the possible samples of size $\samplesize$ would be exponential in $\samplesize$. Secondly, every possible ERM problem appears in a \emph{face} of the polytope; this suggests a strong geometric structure across different ERM problems. And lastly, this result is applicable to a wide variety of network architectures; in order to obtain an architecture-specific result, it suffices to compute the corresponding value of $\mathcal{K}$ and plug it in. Regarding this last point, the authors computed the constant $\mathcal{K}$ for various well-known architectures and obtained the results of Table \ref{tab:results}.

\begin{table}
 % \centering
  \caption{Summary of polyhedral encoding sizes for various architectures. DNN refers to a fully-connected Deep Neural Network, CNN to a Convolutional Neural Network, and ResNet to a Residual Network. $G$ is the graph defining the Network, $\Delta$ is the maximum in-degree in $G$, $\hiddenlayers$ is the number of hidden layers, and $\maxlayerwidth$ is the maximum width of a layer.}
  \label{tab:results}
\vskip 0.15in
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}[h]{llll}
\hline
    Type  & Loss & Size of polytope & Notes  \\
\hline \hline
DNN & Absolute/Quadratic/Hinge & $O\big(\big( \outputdimension \maxlayerwidth^{O(\hiddenlayers^2)} /\epsilon \big)^{\inputdimension+\outputdimension+\parameternumber} \samplesize \big)$ & $\parameternumber\in O(|E({G})|)$ \\
DNN & Cross Entropy w/ Soft-Max & $O\big(\big( \outputdimension \log (\outputdimension) \maxlayerwidth^{O(k^2)} /\epsilon\big)^{\inputdimension+\outputdimension+\parameternumber} \samplesize\big)$ & $\parameternumber\in O(|E({G})|)$ \\
CNN & Absolute/Quadratic/Hinge & $O\big(\big( \outputdimension \maxlayerwidth^{O(\hiddenlayers^2)} /\epsilon\big)^{\inputdimension+\outputdimension+\parameternumber} \samplesize\big)$ & $\parameternumber \ll |E({G})|$ \\
ResNet & Absolute/Quadratic/Hinge & $O\big(\big( \outputdimension \Delta^{O(\hiddenlayers^2)} /\epsilon\big)^{\inputdimension+\outputdimension+\parameternumber} \samplesize\big)$  \\
ResNet & Cross Entropy w/ Soft-Max & $O\big(\big( \outputdimension \log(\outputdimension) \Delta^{O(\hiddenlayers^2)} /\epsilon\big)^{\inputdimension+\outputdimension+\parameternumber} \samplesize\big)$  \\
\hline
  \end{tabular}
  \end{adjustbox}
\vskip -0.1in
\end{table}

The proof of this result relies on a graph theoretical concept called \emph{treewidth}. This parameter is used for measuring structured sparsity, and in \cite{bienstock2018lp} it was proved that any optimization problem admits an approximate polyhedral reformulation whose size is exponential only in the treewidth parameter. On a high level, the neural network result is obtained by noting that \eqref{eq:ERMproblem} connects different sample points only through a sum; therefore, the following reformulation of the optimization problem can be considered, which decouples the different data points:
%
\begin{align}
\label{ERMepigraph}
    \min_{\weights \in \parameterset,\mL} \left\{\frac{1}{\samplesize} \sum_{d = 1}^{\samplesize} \mL_d   \, \middle| \, \mL_d \, = \, \loss(f(\sampleinput_d, \weights), \sampleoutput_d) \quad \forall\, d\in [\samplesize] \right\}
  \end{align}
  %
This reformulation does not seem useful at first, however, it has a \emph{treewidth} that does not depend on $\samplesize$, even if the data points are considered variables. From this point, the authors are able to obtain the polytope whose size does not depend exponentially on $\samplesize$, and which is capable of encoding all possible ERM problems. The face structure the polytope has is more involved, and we refer the reader to \cite{TrainingLP} for these details.

It is worth mentioning that the polytope size provided by \cite{TrainingLP} in the setting of \cite{arora2018understanding} is 
\begin{equation}\label{eq:oursize}
    O( ( 2\mathcal{K}_\infty(\loss) \layerwidth_1^{O(1)} / \epsilon )^{(\inputdimension+1)(\layerwidth_1)} \samplesize )
\end{equation}
where $\mathcal{K_\infty}(\loss)$ is the Lipschitz constant of the loss function with respect to the infinity norm over a specific domain. These two results are not completely comparable, but they give a good idea of how good the size of polytope constructed in \cite{TrainingLP} is. The dependency on $\samplesize$ is better in the polytope size, the polytope encodes multiple training problems, and the result is more general (it applies to almost any architecture); however, the polytope only gives an approximation, and its construction requires boundedness.

\subsection{Backpropagation through MILP}

In the work by \cite{goebbelstraining2021}, a novel use of Mixed-Integer Linear Programming is proposed in training ReLU networks: to serve as an alternative to SGD. This new algorithm works as backpropagation, as it updates the weights of the neural network iteratively starting from the last layer. The key difference is that each update in a layer amounts to solving a MILP.

Let us focus only on one hidden layer at a time (of width $\layerwidth$), so we can assume we have an architecture as in Figure \ref{fig:HyperplaneToNN}. Furthermore, we assume we have some target output vectors $\{\mT_d\}_{d=1}^\samplesize$ (when processing the last hidden layer in the backpropagation, this corresponds to $\{\sampleoutput_d\}_{d=1}^\samplesize$) and some layer input $\{\mI_d\}_{d=1}^\samplesize$ (when processing the last hidden layer, this corresponds to evaluating the neural network on $\{\sampleinput_d\}_{d=1}^\samplesize$ up to the second-to-last hidden layer). The algorithm proposed by \cite{goebbelstraining2021} solves the following optimization problem to update the weights $\weights$ and biases $\biases$ of the given layer:
%
\begin{subequations}
\label{eq:backpropmilp}
\begin{align}
    \min_{\weights,\hat{\vh},\biases,\vh,\vz} \quad & \sum_{d=1}^\samplesize \sum_{j=1}^\layerwidth |\mT_{d,j} - \vh_{d,j}| \\
    \text{s.t.}\quad & \hat{\vh}_{d,j} = (\weights \mI_d)_j + \biases_j  && d=1,\ldots,D,\, j=1,\ldots,n\\
    &\hat{\vh}_{d,j} \leq M\vz_{d,j} && d=1,\ldots,D,\, j=1,\ldots,n \\ 
    &\hat{\vh}_{d,j} \geq -M(1-\vz_{d,j}) && d=1,\ldots,D,\, j=1,\ldots,n \\
    &|\hat{\vh}_{d,j}-\vh_{d,j}| \leq M(1-\vz_{d,j}) && d=1,\ldots,D,\, j=1,\ldots,n \\ 
    &\vh_{d,j} \leq M\vz_{d,j} && d=1,\ldots,D,\, j=1,\ldots,n \\
    & \vh_{d,j} \geq 0  && d=1,\ldots,D,\, j=1,\ldots,n \\
    & \vz_{d,j} \in \{0,1\} && d=1,\ldots,D,\, j=1,\ldots,n.
\end{align}
\end{subequations}
%
Here $M$ is a large constant that is assumed to bound the input to any neuron. Note that problem \eqref{eq:backpropmilp} can easily be linearized. 
%
This optimization problem finds the weights ($\weights$) and biases ($\biases$) that minimize the difference between the ``real'' output of the network for each sample ($\vh_d$) and the target output ($\mT_d$). The auxiliary variables $\hat{\vh}_{d,j}$ represent the input to the each neuron ---so $\vh_{d,j} = \sigma(\hat{\vh}_{d,j})$--- and $\vz_{d,j}$ indicates if the $j$-th neuron is activated on input $\mI_{d}$.

When processing intermediate layers, the definition $\mI_d$ can easily be adapted from what we mentioned above. However, the story is different for the case of $\mT_d$. When processing the last layer, as previously mentioned, $\mT_d$ simply corresponds to $\sampleoutput_d$. For intermediate layers, to define $\mT_d$, the author proposes to use a similar optimization problem to \eqref{eq:backpropmilp}, but leaving $\weights$ and $\biases$ fixed and having $\mI_d$ as variables; this defines ``optimal inputs'' of a layer. These optimal inputs are then used as target outputs $\mT_d$ when processing the preceding layer, and thus the algorithm is iterated. For details, see \cite{goebbelstraining2021}.

The computational results in that paper show that a similar level of accuracy to that of gradient descent can be achieved. However, the use of potentially expensive MILPs impairs the applicability of this approach to large networks. Nonetheless, it shows an interesting new avenue for training whose running times may be improved in future implementations.

\subsection{Training binarized neural networks using MILP}

As mentioned before, the training problem of a DNN is an unrestricted non-convex optimization
problem, which is typically continuous as the weights and biases frequently are allowed to have any real value.
%
Nonetheless, if the weights and biases are required to be integer-valued, the training
problem becomes a discrete optimization problem, for which gradient-descent-based methods may find some difficulties in their applicability.

In this context, \cite{icarte2019training} proposed a MILP formulation for the training problem of  binarized neural networks (BNNs): these are neural networks where the weights and biases are restricted to be in $\{-1,0,1\}$ and where the activations are LTU (i.e. sign functions). Later on, \cite{thorbjarnarson2020training,thorbjarnarson2023optimal} used a similar technique to allow more general integer-valued weights. We review the core feature in these formulations that yield a \emph{linear} formulation of the training problem. 

Let us focus on an intermediate layer $i$ with width $\layerwidth$, and let us omit biases to simplify the discussion. Using a DNN's layer-wise architecture, one usually aims at describing:
%
\begin{subequations} \label{eq:basicsystemtraining}
\begin{align}
    \hat{\vh}^i_{d,j} & = (\weights^{i} \vh^{i-1}_d)_j  && d=1,\ldots,D,\, j=1,\ldots,n \label{eq:basicsysinput}\\
    \vh^{i}_{d,j} & = \sigma(\hat{\vh}^i_{d,j}) && d=1,\ldots,D,\, j=1,\ldots,n. \label{eq:basicsysactivation}
\end{align}
\end{subequations}
%
We remind the reader that $D$ is the cardinality of the training set.  Additionally, for each data point indexed by $d$ and each layer $i$, each variable $\vh_{d}^{i}$ is the output vector of all the neurons of the layer and each variable $\hat{\vh}_{d,j}^i$ is the input of neuron $j$.
%
Besides the difficulty posed by the activation function, one important issue with system \eqref{eq:basicsystemtraining} is the non-linearity of the products between the $\weights$ and $\vh$ variables. Nonetheless, this issue disappears when each entry of $\weights$ and $\vh$ is bounded and integer, as in the case of BNNs. 

Let us begin with reformulating \eqref{eq:basicsysactivation}. We can introduce auxiliary variables $\vu_{d,j}^i\in \{0,1\}$ that will indicate if the neuron is active. We also introduce a tolerance $\varepsilon > 0$ to determine the activity of a neuron. Using this, we can (approximately) reformulate \eqref{eq:basicsysactivation} \emph{linearly} using big-M constraints:
%
\begin{subequations}\label{eq:activationreform}
\begin{align}
    \vh^{i}_{d,j} & = 2\vu_{d,j}^i -1     && d=1,\ldots,D,\, j=1,\ldots,n \label{eq:oandu}\\
    \hat{\vh}^i_{d,j} & \geq -M (1-\vu_{d,j}^i) && d=1,\ldots,D,\, j=1,\ldots,n \\
     \hat{\vh}^i_{d,j} & \leq -\varepsilon + M \vu_{d,j}^i && d=1,\ldots,D,\, j=1,\ldots,n 
\end{align}
\end{subequations}
%
where $M$ is a large constant. 
%
As for \eqref{eq:basicsysinput}, note that 
%
\[ \hat{\vh}^i_{d,j} = \sum_{k=1}\weights^{i}_{j,k} \vh^{i-1}_{d,k}. \]
%
Therefore, using \eqref{eq:oandu}, we see that it suffices to describe each product $\weights_{j,k}^{i}\vu_{d,k}^{i-1}$ linearly. We can introduce new variables $\vz_{j,k,d}^{i}$ and note that
%
\[\vz_{j,k,d}^{i-1}= \weights_{j,k}^{i}\vu_{d,k}^{i-1}\]
%
if and only if the three variables satisfy
%
\begin{align*}
    | \vz_{j,k,d}^{i-1} | & \leq \vu_{d,k}^{i-1} \\
    |\vz_{j,k,d}^{i-1} - \weights_{j,k}^{i}| &  \leq 1- \vu_{d,k}^{i-1} \\
    \vu_{d,k}^{i-1} & \in \{0,1\}.
\end{align*}
%
This last system can be easily converted to a linear system, and thus the training problem in this setting can be cast as a mixed-integer linear optimization problem.

Other works have also relied on similar formulations to train neural networks. \cite{icarte2019training} introduce different objective functions that can be used along
with the linear system to produce a MILP that can train BNNs. They also introduce a Constraint-Programming-based model and a hybrid model, and then compare all of them computationally. \cite{thorbjarnarson2020training} introduce more MILP-based training models that leverage piecewise linear approximations of well-known non-linear loss functions and that can handle integer weights beyond $\{-1,0,1\}$. A similar setting is studied by \cite{sildir2022mixed}, where piecewise linear approximations of non-linear activations are used, and integer weights are exploited to formulate the training problem as a MILP. Finally,  \cite{bernardelli2022bemi} rely on a multi-objective MIP model for training BNNs; from here, they create a BNN ensemble to produce robust classifiers. 

From these articles, we can conclude that the MILP-based approach to training their neural networks can result in high-quality neural networks, especially in terms of generalization. However, many of these MILP-based methods currently do not scale well, as opposed to gradient-descent-based methods. We believe that, even though there are some theoretical limitations to the efficiency of MILP-based methods, there is a considerable practical improvement potential with using them in neural network training.



% \section{Notation Commands}
%   \begin{tabular}[h]{lll}
% \hline
%     Concept  & Symbol & Command  \\
% \hline \hline
% Sample Size & $\samplesize$ & \verb!\samplesize! \\
% Sample (Input) Vector & $\sampleinput$ &\verb!\sampleinput! \\
% Sample (Output) Vector & $\sampleoutput$ &\verb!\sampleoutput! \\
% ``Real function'' to learn & $\functolearn$ & \verb!\functolearn! \\
% All weights of DNN & $\weights$ & \verb!\weights! \\
% Weight matrix for layer $i$ &  $\weights^i$ & \verb!\weights^i! \\
% All biases of DNN & $\biases$ & \verb!\biases! \\
% Number of hidden layers & $\hiddenlayers$ & \verb!\hiddenlayers! \\
% input dimension & $\inputdimension$ & \verb!\inputdimension! \\
% output dimension & $\outputdimension$ & \verb!\outputdimension! \\
% Width of $i$-th layer & $\layerwidth_i$ & \verb!\layerwidth_i! \\
% Max layer width & $\maxlayerwidth$ & \verb!\maxlayerwidth! \\
% Set of parameters allowed & $\parameterset$ & \verb!\parameterset! \\
% Dimension of parameters & $\parameternumber$ & \verb!\parameternumber! \\
% Maximum in-degree in a NN & $\maxindegree$ & \verb!\maxindegree! \\
% Neural network function & $\NNfunction{x}{\weights}{\biases}$ & \verb!\NNfunction{x}{\weights}{\biases}! \\
% Set of outputs with value 1 (classification) & $\setofones$ & \verb!\setofones!\\
% Generic loss (each summand)& $\loss$ & \verb!\loss! \\
% Overall sum $\frac{1}{D}\sum_{i=1}^D \ell (\cdots) $& $\ERMfunction(\weights,\biases)$ & \verb!\ERMfunction(\weights,\biases)!\\
% \hline
%   \end{tabular}
 