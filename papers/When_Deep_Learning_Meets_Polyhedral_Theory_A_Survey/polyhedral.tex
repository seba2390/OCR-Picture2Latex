

\section{The Polyhedral Perspective}\label{sec:poly}

A feedforward rectifier network models a piecewise linear function \citep{arora2018understanding} in which every such piece is a polyhedron \citep{raghu2017expressive}, 
and represents a special case among neural networks modeling piecewise polynomials \citep{balestriero2018spline}. 
Therefore, training a rectifier network is equivalent to performing a piecewise linear regression,  
and we can potentially interpret such neural networks in terms of what happens in each piece of the function that they model. 
However, we are only beginning to answer some of the questions entailed by such a remark. In this survey, we discuss how insights on this subject may help us answer the following questions.
%\todo[inline]{Q1: Section 3; Q2-Q4: Section 4; Q5-Q6: Section 5}
\begin{enumerate}
\item Which piecewise linear functions can or cannot be obtained from training a neural network given its architecture?
\item Which neural networks are more susceptible to adversarial exploitation?
\item Can we integrate the model learned by a neural network into a broader decision-making problem for which we want to find an optimal solution?
\item Is it possible to obtain a smaller neural network that models exactly the same function as another trained  neural network?
\item Can we exploit the polyhedral geometry present in neural networks in the training phase? 
\item Can we efficiently incorporate extra structure when training neural network, such as linear constraints over the weights?
\end{enumerate}

The first question complements the universal approximation results for neural networks. Namely, there is a limit to what functions can be well approximated when limited computational resources are translated into constraints on the depth and width of the layers of neural networks that can be used in practice. 
The functions that can be modeled depend on the particular choice of hyperparameters subject to the computational resources available, and in the long run that may also lead to a more principled approach for the choice of hyperparameters than the current approaches of neural architecture search. 
%
In Section~\ref{sec:LR}, 
we analyze how a rectifier network partitions the input space into pieces in which it behaves linearly, which we denote as \emph{linear regions}. 
We discuss the geometry of linear regions, the effect of parameters and hyperparameters on the number of linear regions of a neural network, and the extent to which such number of linear regions relates to the accuracy of the network. 
%\begin{itemize}
%    \item What is the effect of parameters and hyperparameters on the number of linear regions?
%    \item Does the number of linear regions defined by a neural network relate to its accuracy? 
%    \item Are some linear regions more important than others?
%\end{itemize}

The second question relies on formal verification methods to evaluate the robustness of neural networks, which can be approached with mathematical optimization formulations that are also relevant for the third and fourth questions. Such formulations are convenient since a direct inspection of every piece of the function modeled by a neural network is prohibitive given how quickly their number scale with the size of the network. 
The linear behavior of the network for every choice of active and inactive units implies that we can use linear formulations with binary variables corresponding to the activation of units to model trained neural networks using MILP.  Therefore, we are able to solve a variety of optimization problems over a trained neural network, such as the neural network verification problem, 
identifying the range of outputs for each ReLU of the network, and modeling a trained neural network as part of a larger decision-making problem. 
%In all of those cases, however, the sizes of the neural networks for which this is possible is still limited. 
%In fact, we can approach the third question through one such family of optimization problems. 
%
In Section~\ref{sec:optimizing}, we discuss how to formulate optimization problems over a trained neural network, the applications of such formulations, and the progress toward obtaining stronger formulations that scale more easily with the network size.

The fifth and sixth questions involve the training procedure of a DNN, where linear programming tools have been applied to partially answer them. In Section~\ref{sec:training}, we overview these developments. In terms of the fifth question ---exploiting polyhedrality in training neural networks--- we describe algorithms that use the polyhedral geometry induced by activation sets to solve training problems. We also cover a recently proposed polyhedral construction that can approximately encode multiple training problems at once, showing a strong relationship across training problems that arise from different datasets, for a fixed architecture. Additionally, we review some recent uses of mixed-integer linear programming in the training phase as an alternative to SGD when the weights are required to be integer. Regarding the sixth question ---the incorporation of extra structure when training--- we review multiple approaches that have included techniques related to linear programming within SGD to impose a desirable structure when training, or to find better step-lengths in the execution of SGD.

% Finally, we could potentially address the fourth question through approximation by  discretizing the weight space and enumerating all the possibilities. However, such a prohibitive approach is not necessary. 
%  we show that LP formulations can be used for that purpose in neural networks. In fact, there has been considerable work on LP-based training of neural networks with a single hidden layer in the past. Similar approaches have also been used in related learning problems, such as obtaining optimal classification trees with MILP~\cite{ClassificationTrees}. In addition, we can use polyhedra to characterize all the models that can be learned from datasets of a given size.
