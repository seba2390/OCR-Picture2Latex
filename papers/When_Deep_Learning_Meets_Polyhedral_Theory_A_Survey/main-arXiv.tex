\documentclass{article}
\usepackage[utf8]{inputenc}

% Another title idea:
% - Seeing Deep Learning through Discrete Optimization and Polyhedral Theory: A Tutorial

% The title below fits EJOR's template in 2 lines
% - Discrete Optimization and Polyhedral Theory in Deep Learning: Connections, Insights, and Opportunities
\title{When Deep Learning Meets Polyhedral Theory: A Survey}
\author{
Joey Huchette\\{\footnotesize Google Research, USA} \and
Gonzalo Mu\~{n}oz\\{\footnotesize Universidad de O'Higgins, Chile} \and 
Thiago Serra\\{\footnotesize Bucknell University, USA} \and
Calvin Tsay\\{\footnotesize Imperial College London, UK}
}
\date{September 2023}

\usepackage[comma]{natbib}
\bibliographystyle{abbrvnat}

\input{math_commands.tex}

\usepackage{array}
\usepackage{multirow}

\usepackage{todonotes}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\usepackage{lscape}

%\usepackage{pdflscape}
\usepackage{adjustbox}
\usepackage{url}

\usepackage{tikz-network}
\begin{document}

\maketitle

\begin{abstract}
\noindent 
% Motivation
In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. 
% Problem 
Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit~(ReLU), 
which became the most commonly used type of activation function in neural networks. 
That made certain types of network structure ---such as the typical fully-connected feedforward neural network--- amenable to analysis 
through polyhedral theory and to the application of methodologies such as Linear Programming~(LP) and Mixed-Integer Linear Programming~(MILP) for a variety of purposes. 
% Approach 
In this paper, 
we survey the main topics emerging from this fast-paced area of work, 
% 
which brings a fresh perspective to understanding neural networks in more detail as well as to applying linear optimization techniques to train, verify, and reduce the size of such networks.  
\end{abstract}

\input{introduction.tex}

\input{polyhedral.tex}

\input{linearregions.tex}

\input{optimizingover.tex}

\input{training.tex}

\input{conclusions.tex}

\paragraph{Acknowledgments} We thank Christian Tjandraatmadja and Toon Tran for early feedback on the manuscript and asking questions that helped shaping it. 

Thiago Serra was  supported by
the National Science Foundation (NSF) award IIS 2104583. 
Calvin Tsay was supported by the Engineering \& Physical Sciences Research Council (EPSRC) grant EP/T001577/1. 

%\bibliographystyle{siamplain}
\bibliography{references}

\end{document}

