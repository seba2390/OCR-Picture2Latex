
\section{Conclusion}

The rapid advancement of neural networks and their ubiquity has given rise to numerous new challenges and opportunities in deep learning: we need to design them in more reliable ways, to better understand their limits, and to test their robustness, among other challenges.
%
While, traditionally, continuous optimization has been the predominant technology used in the optimization tasks in deep learning, some of these new challenges have made discrete optimization tools gain a remarkable importance.

In this survey, we have reviewed multiple areas where polyhedral theory and linear optimization have played a critical role. For example, in understanding the expressiveness of neural networks, in optimizing trained neural networks (e.g. for verification purposes), and even in designing new training algorithms. 
%
We hope this survey can provide perspective in a rapidly-changing field, and motivate further developments in both deep learning and discrete optimization. There is still much to be explored in the intersection of these fields.
