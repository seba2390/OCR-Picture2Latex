We now consider the same multivariate stochastic volatility example as in \citet[Section E.3.]{finke2021csmc}. This model is classically used as a benchmark for high dimensional SMC related methods~\citep[see also][]{guarniero2017iterated}. It is given by homogeneous auto-regressive Gaussian latent dynamics
\begin{equation}
    p_t(x_t \mid x_{t-1}) = \mathcal{N}(x_t; F\, x_{t-1}, Q)
\end{equation}
and a potential defined as a multidimensional observation model
\begin{equation}
    g(x_{0:T}) = \prod_{t=0}^T h(y_t \mid x_t),\, \text{where}\quad h(y_t \mid x_t) = \prod_{d=1}^{d_x}\mathcal{N}(y_t(d); 0, \exp(x_t(d))).
\end{equation}

As per \citet{finke2021csmc}, we take $F = \phi I_d$, $Q_{ij} = \tau (\delta(i=j) + \delta(i \neq j) \rho)$ for $\phi = 90\%$, $\tau = 2$, and $\rho = 25\%$. Similarly, the initial distribution $p_0(x_0)$ is also taken to be the stationary distribution of the latent Gaussian dynamics and we take $d_x = 30$. However, we increase the number of time steps to $T=250$, rather than $50$ and we take the number of particles for all the auxiliary cSMC algorithms to be $N=25$. %

The different methods we compare here are the following: (i) auxiliary Kalman sampler with first order linearisation~\eqref{eq:gaussian-FK-proposal} (both on CPU and GPU), (ii) with second order linearisation~\eqref{eq:second-order} (both on CPU and GPU), (iii) auxiliary cSMC sampler with backward sampling for the proposals $\mathcal{N}(\cdot; u_t, \frac{\delta_t}{2}I)$ corresponding to \citet{finke2021csmc} (on CPU), (iv) auxiliary cSMC sampler with parallel-in-time~\citep{corenflos2022sequentialized} sampling for the proposals $\mathcal{N}(\cdot; u_t, \frac{\delta_t}{2}I)$ (on GPU), (v) auxiliary cSMC sampler with backward sampling for the gradient-informed proposals~\eqref{eq:differentiable-model} (on CPU), (vi) auxiliary cSMC sampler with parallel-in-time sampling for the gradient-informed proposals~\eqref{eq:differentiable-model} (on GPU), and (vii) the guided auxiliary cSMC sampler with backward sampling for both the proposals \eqref{eq:guided-prop} and \eqref{eq:guided-prop-grad} (on CPU).

In order to compare the samplers on this example, we generate $10$ different datasets. For every dataset, we run each sampler for $2\,500$ adaptation steps. After this, we run $10\,000$ more iterations to compute the empirical root expected squared jump distance~\citep[RESJD,][]{pasarica2010adaptively} for each sampler, defined as, for each time step $t$, the empirical value of $\sqrt{\frac{1}{L} \sum_{l=1}^{L-1}\sum_{i,j=1}^d\left[X^{A + l + 1}_{t+1}(i,j) - X^{A + l}_{t+1}(i,j)\right]^2}$. All samplers, in both the sequential and parallel case were targeting 50\% acceptance rate across all time steps and the effective acceptance rate ranged between 47 and 52\% for all samplers and time steps. The averaged (across the 10 experiments) RESJD is reported in Figure~\ref{fig:esjd-stoch-vol-cpu} for the sequential versions of the algorithm, and in Figure~\ref{fig:esjd-stoch-vol-gpu} for the parallel counterparts (noting that there is, as expected, no statistical difference between the sequential and parallel implementations of the Kalman samplers).
\begin{figure}[htb!]
    \centering
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-stoch-vol-cpu}}
        \caption{Average root expected squared jump distance per iteration for the sequential versions of the auxiliary first order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-1_cpu}, second order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-2_cpu}, cSMC sampler~\ref{line:stochvol-ESJD-cSMC_cpu}, gradient-informed cSMC~\ref{line:stochvol-ESJD-cSMC_grad_cpu} sampler, the guided cSMC~\ref{line:stochvol-ESJD-cSMC-guided_cpu} sampler, and the gradient-informed guided cSMC~\ref{line:stochvol-ESJD-cSMC-guided-grad_cpu} sampler.}
        \label{fig:esjd-stoch-vol-cpu}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-stoch-vol-gpu}}
        \caption{Average root expected squared jump distance per iteration for the parallel versions of the auxiliary first order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-1_gpu}, second order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-2_gpu}, cSMC sampler~\ref{line:stochvol-ESJD-cSMC_gpu}, and gradient-informed cSMC~\ref{line:stochvol-ESJD-cSMC_grad_cpu} sampler. The latter two are hard to distinguish, but the gradient-informed cSMC is generally above the non-informed.}
        \label{fig:esjd-stoch-vol-gpu}
    \end{subfigure}
    \caption{Average (across 10 different experiments) root expected squared jump distance per iteration for all the sampler considered on the stochastic volatility model of Section~\ref{subsec:stoch-vol}. The results are split between CPU~\ref{fig:esjd-stoch-vol-cpu} (where the classical sequential implementation of the cSMC with backward sampling was used) and GPU~\ref{fig:esjd-stoch-vol-gpu} (where the parallel-in-time cSMC was used).}
\end{figure}
As highlighted by Figure~\ref{fig:esjd-stoch-vol-cpu}, the gradient-informed auxiliary cSMC statistically dominates the alternatives for all time steps on both CPU and GPU (although this is less obvious on the GPU).

This picture, however, is modified when looking at the RESJD per second -- heuristically corresponding to the average cumulative distance travelled by the sampler in a unit of time --
rather than per iteration in Figures~\ref{fig:esjd-time-stoch-vol-cpu} and~\ref{fig:esjd-time-stoch-vol-gpu}. In this case, on the CPU, the method of \citet{finke2021csmc} dominates the other ones. This is because it offers reasonable statistical efficiency ($\sim70\%$ the RESJD of the most efficient sampler tested here) with a rather small time-complexity overall (no gradient calculation, and no matrix inversion like in the Kalman samplers is needed here). On the other hand, the Kalman samplers are here completely dominated by all Monte Carlo alternatives. The GPU picture is more mixed, and both the gradient informed and uninformed proposals seem to provide the same overall efficiency in this case but still completely dominate the Kalman alternatives here too.
\begin{figure}[htb!]
    \centering
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-stoch-vol-time_cpu}}
        \caption{Average root expected squared jump distance per second for the sequential versions of the auxiliary first order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-1_cpu}, second order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-2_cpu}, cSMC sampler~\ref{line:stochvol-ESJD-cSMC_cpu}, gradient-informed cSMC~\ref{line:stochvol-ESJD-cSMC_grad_cpu} sampler, the guided cSMC~\ref{line:stochvol-ESJD-cSMC-guided_cpu} sampler, and the gradient-informed guided cSMC~\ref{line:stochvol-ESJD-cSMC-guided-grad_cpu} sampler.}
        \label{fig:esjd-time-stoch-vol-cpu}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{\input{experiments/figures/RESJD-stoch-vol-time_gpu}}
        \caption{Average root expected squared jump distance per second for the parallel versions of the auxiliary first order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-1_gpu}, second order Kalman sampler~\ref{line:stochvol-ESJD-Kalman-2_gpu}, cSMC sampler~\ref{line:stochvol-ESJD-cSMC_gpu}, and gradient-informed cSMC~\ref{line:stochvol-ESJD-cSMC_grad_cpu} sampler. The latter two are hard to distinguish, with no clear difference in terms of performance.}
        \label{fig:esjd-time-stoch-vol-gpu}
    \end{subfigure}
    \caption{Average (across 10 different experiments) root expected squared jump distance per second for all the sampler considered on the stochastic volatility model.}
\end{figure}

This underwhelming performance of the Kalman sampler was in fact to be expected given the need to solve $250$ matrix systems of dimension $30$ per iteration (albeit some are done in parallel on GPU).
In fact, this had another deleterious effect: the parallel versions of the auxiliary Kalman sampler suffered from numerical divergence in this experiment when using single precision floats (32-bits representation). This problem, due to the numerical instability of the covariance matrices calculations is well known in the literature~\citep[see, e.g.][]{Bierman1977sqrt} and prompted the development of a square-root version of the parallel Kalman filtering and smoothing algorithms in \citet{Yaghoobi2022sqrt}. Here, we instead simply used double precision floats instead of the square-root method as this sufficed to fix the numerical instability. This numerical instability is an important drawback of Kalman methods in general, and is particularly salient on parallel hardware which is often optimised to run on lower precision arithmetic~\citep{jouppi2017datacenter}. The issue did not arise for the sequential version of the algorithms, and we therefore stuck to single float precision arithmetic for these. In the next section, we show how latent structure can be leveraged to bypass the dimensionality problem.