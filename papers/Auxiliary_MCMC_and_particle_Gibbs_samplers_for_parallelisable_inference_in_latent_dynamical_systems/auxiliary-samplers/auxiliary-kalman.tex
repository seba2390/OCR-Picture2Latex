The proposal \eqref{eq:aux-general} covers latent Gaussian models in general, and in particular covers models with latent Gaussian dynamics\footnote{This was in fact explicitly used in \citet[][Chap. 15]{Chopin2020book}, where the authors successfully apply the marginal sampler \eqref{eq:marginal-general} to a one-dimensional stochastic volatility model with latent Gaussian dynamics. The fact that the auxiliary sampler corresponded to a LGSSM was, however, not noted by the authors.}:
\begin{equation}
    \label{eq:gaussian-FK}
    \begin{split}
        \pi(\ft{x}{0}{T})
        &\propto g(x_0, \ldots, x_T)\\
        &\times \mathcal{N}(x_0; m_0, P_0)\, \prod_{t=1}^T \mathcal{N}(x_t; F_{t-1}\, x_{t-1} + b_{t-1}, Q_{t-1}).
    \end{split}
\end{equation}
However, directly treating these as latent Gaussian models would incur a computational complexity of $\bigO(T^2 d_x^2)$, with an initial pre-processing step that scales as $\bigO(T^3 d_x^3)$, and a memory cost of $\bigO(T^2 d_x^2)$ corresponding to the size of the underlying covariance matrix $C$. This is true even though the inverse of $C$ is sparse~\citep[see, e.g.,][Chap. 3]{barfoot2017state} due to the need to compute the eigen-decomposition of either $C$ or $C^{-1}$~\citep[][Supplementary material]{titsias2018}. Instead of doing this, it is possible, in the case of a model like \eqref{eq:gaussian-FK}, to preserve the Markovian structure of the model and formulate the auxiliary sampler as a LGSSM, which can then be used more efficiently.

In order to do so, we emulate \citet{titsias2018}, and consider the augmented target distribution
\begin{equation}
    \label{eq:augmented-gaussian-fk}
    \pi(\ft{x}{0}{T}, u_{0:T}) \propto \pi(\ft{x}{0}{T}) \prod_{t=0}^T \mathcal{N}\left(u_t; x_{t}, \frac{\delta}{2} \Sigma_t\right),
\end{equation}
where $\delta > 0$ and, for all $t=0, \ldots, T$, $\Sigma_t$ is some positive definite matrix in $\mathbb{R}^{d_x \times d_x}$. Note that when $\Sigma_t = I$ is the identity matrix for all $t$, this recovers the proposal \eqref{eq:augmented-proposal-general}.

Let us define $\gamma$ via $\exp(\gamma(x_0, x_1, \ldots, x_T)) := g(x_0, x_1, \ldots, x_T)$, and linearise it around the previously sampled trajectory $x_{0:T}$,
\begin{equation}
    \label{eq:linearised-log-potential}
    \gamma(z_{0:T}) \approx \gamma(x_{0:T}) + \langle v_{0:T}, z_{0:T} - x_{0:T}\rangle,
\end{equation}
where $v_t = {\pdv{\gamma}{x_t}}(x_{0:T})$ for all $t$, and $\langle a_{0:T}, b_{0:T} \rangle$ denotes the sum of inner products $\sum_{t=0}^T \langle a_{t}, b_{t} \rangle$.

Under these notations, we can define the auxiliary proposal
\begin{equation}
    \label{eq:gaussian-FK-proposal}
    \begin{split}
        q(z_{0:T} \mid u_{0:T}, x_{0:T}) \propto \mathcal{N}(z_0; m_0, P_0)
        &\left\{\prod_{t=1}^T \mathcal{N}(z_t; F_{t-1}z_{t-1} + b_{t-1}, Q_{t-1})\right\} \\
        &\left\{\prod_{t=0}^T\mathcal{N}\left(u_{t} + \frac{\delta}{2} \Sigma_t v_{t}; z_{t}, \frac{\delta}{2} \Sigma_t\right)\right\},
    \end{split}
\end{equation}
which corresponds to the pathwise smoothing distribution of a LGSSM with unchanged dynamics compared to \eqref{eq:gaussian-FK}, and observations given by $u_{t} + \frac{\delta_t}{2} \Sigma_t v_{t}$ for an observation model $\mathcal{N}\left(\cdot; z_{t}, \frac{\delta}{2} \Sigma_t\right)$, $t=0, 1, \ldots, T$.
Sampling from this distribution, and evaluating its likelihood can be done using Kalman primitives~\citep[see, e.g.][Ch. 4 and Ch. 8]{sarkka2013bayesian} in $\bigO(T)$ steps.
In fact, this representation is crucial to reduce the memory requirements to linear in $T$ as well as the computational complexity from cubic to linear or even logarithmic in $T$ for parallel hardware. We come back to this last point in Section~\ref{sec:PIT-sampling}.

We insist that this proposal is statistically equivalent to the auxiliary method of \citet{titsias2018} for a choice of constant $\Sigma_t = I$, but exhibits better computational complexity. Marginalising it, however, would destroy the latent Markovian structure, removing this advantage completely. Similarly, in general, a second order approximations of $\gamma$  would result in fully dependent observations, so that the proposal distribution would not correspond to a LGSSM anymore.
An important special case, however, is that of separable potentials, as is the case for state-space models. Indeed, when $g(x_{0:T}) = \prod_{t=0}^T g_t(x_t)$, or equivalently, when $\gamma(x_{0:T}) = \sum_{t=0}^T \gamma_t(x_t)$, we can write
\begin{equation}
    \label{eq:linearised-log-potential-second}
    \gamma(z_{0:T}) \approx \gamma(x_{0:T}) + \langle v_{0:T}, z_{0:T} - x_{0:T}\rangle + \frac{1}{2} \sum_{t=0}^T (z_t - x_t)^{\top} \Lambda_t (z_t - x_t),
\end{equation}
where $\Lambda_t$ is the Hessian matrix of $\gamma_t$ evaluated at $x_t$. By rearranging the terms, we can derive the resulting proposal distribution as
\begin{equation}
    \label{eq:second-order}
    \begin{split}
        q(z_{0:T} \mid u_{0:T}, x_{0:T}) &\propto \mathcal{N}(z_0; m_0, P_0) \\
        &\quad\left\{\prod_{t=1}^T \mathcal{N}(z_t ; F_{t-1}z_{t-1} + b_{t-1}, Q_{t-1})\right\}\,\left\{\prod_{t=0}^T\mathcal{N}\left(\omega_{t}; z_{t}, \Omega_t\right)\right\},
    \end{split}
\end{equation}
with
\begin{equation}
    \Omega_t = \left(\frac{2}{\delta} \Sigma_t^{-1} - \Lambda_t\right)^{-1} \quad \text{and} \quad \omega_t = \Omega_t\left(\frac{2}{\delta} \Sigma_t^{-1} u_t + v_t - \Lambda_t x_t\right).
\end{equation}
This proposal is well defined as a LGSSM as soon as $\delta$ is small enough.
We note that other choices for $\Lambda_t$ are possible (for example using low-rank approximations of the Hessian matrix), and that this formula can be used as an approximation of a second-order decomposition when the potentials are not separable. These would however be \textit{ad-hoc} methods, and we do not discuss them further in this article.

Finally, when the dynamics are not Gaussian, it is often possible to transform the model at hand into a equivalent representation of $q_{0:T}$ with Gaussian dynamics by setting
\begin{equation}
    \begin{split}
        p_0(x_0)                &\leftarrow \mathcal{N}(x_0; m_0, P_0) \\
        p_t(x_t \mid x_{t-1})   &\leftarrow \mathcal{N}(x_t ; F_{t-1}x_{t-1} + b_{t-1}, Q_{t-1}) \\
        g(x_{0:T})              &\leftarrow g(x_{0:T}) \frac{p_0(x_0)}{\mathcal{N}(x_0; m_0, P_0)} \prod_{t=1}^T \frac{p_t(x_t \mid x_{t-1})}{\mathcal{N}(x_t ; F_{t-1}x_{t-1} + b_{t-1}, Q_{t-1})}.
    \end{split}
\end{equation}
This family of techniques is at the core of guided and auxiliary particle filtering methods~\citep[see, e.g.,][Ch. 10]{Chopin2020book}. While this is sometimes a natural thing to do, for example when the transition kernel is the product of a linear Gaussian transition and another term, it can also happen that there is no natural way to make such a Gaussian appear in the model. This justifies the need for introducing a new class of auxiliary samplers.
