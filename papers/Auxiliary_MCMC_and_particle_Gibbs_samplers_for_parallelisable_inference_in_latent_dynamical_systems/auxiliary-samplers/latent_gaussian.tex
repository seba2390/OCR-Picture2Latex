Auxiliary gradient-based methods were introduced in \citet{titsias2018} as a way to construct posterior-informed proposals in MCMC samplers for Gaussian latent models with a density $\pi(x) \propto \exp(f(x))\,\mathcal{N}(x; 0, C)$\footnote{As well as, under a trivial change of variables, for models with non-zero prior mean.}, where $x \in \mathbb{R}^{d_x}$. They were shown to outperform classical preconditioned (prior-informed) and gradient-based (likelihood-informed) samplers for latent Gaussian models.


Auxiliary gradient-based samplers rely on augmenting the target $\pi$ with an auxiliary variable $u$:
\begin{equation}
    \label{eq:augmented-proposal-general}
    \pi(x, u) \propto \exp(f(x))\,\mathcal{N}(x; 0, C)\, \mathcal{N}\left(u; x, \frac{\delta}{2} I\right),
\end{equation}
where $\delta > 0$ is a step size, so that the marginal of $\pi(x, u)$ is $\pi(x)$. Auxiliary samplers then proceed by linearising $f$ around the current state $x$ of the Markov chain to obtain a Gaussian proposal distribution
\begin{equation}
    \label{eq:aux-general}
    \begin{split}
        q(y \mid x, u)
        &\propto \exp(\nabla f(x)^{\top}y)\,\mathcal{N}(y; 0, C)\, \mathcal{N}\left(u; y, \frac{\delta}{2} I\right)\\
        &= \mathcal{N}\left(y; \frac{2}{\delta} A \left(u + \frac{\delta}{2} \nabla f(x)\right), A\right),
    \end{split}
\end{equation}
where $A = \frac{\delta}{2} (C + \frac{\delta}{2} I)^{-1} C = (C^{-1} + \frac{2}{\delta} I)^{-1}$. Sampling from $\pi(x, u)$ (and therefore from $\pi(x)$ by discarding the intermediate auxiliary steps) is then done via Hastings-within-Gibbs~\citep{muller1993}:
\begin{enumerate}
    \item Sample $u \mid x \sim \mathcal{N}(u; x, \frac{\delta}{2} I)$.
    \item Propose $y \sim q(\cdot \mid x, u)$ targeting $\pi(\cdot \mid u) \propto \pi(\cdot, u)$, and accept the move with the corresponding acceptance probability.
\end{enumerate}
A more efficient counterpart of this, targeting $\pi(x)$ directly, can be given by integrating the proposal distribution \eqref{eq:aux-general} with respect to $\mathcal{N}(u; x, \frac{\delta}{2} I)$:
\begin{align}
    \label{eq:marginal-general}
    q(y \mid x) = \mathcal{N}\left(y; \frac{2}{\delta} A \left(x + \frac{\delta}{2} \nabla f(x)\right), \frac{2}{\delta} A^2 + A\right).
\end{align}

This marginalised version skips the intermediate sampling step of the auxiliary variable, and is provably better -- both empirically and in terms of Peskun ordering~\citep{peskun1973optimum,tierney1998note,leisen2008extension} -- than its auxiliary version, resulting in step sizes $\delta$ roughly twice larger~\citep[see Tables 1, 2, and 3 in][]{titsias2018} for the same acceptance rate, at virtually no additional computational complexity.

A crucial property of both these instances of the auxiliary sampler is that for all $\delta > 0$, %
the matrices $A$ and $C$ share the same eigenspace~\citep[Section 3.3]{titsias2018}. This ensures that after an initial spectral decomposition of $C$, changing the value of $\delta$ can be done at a negligible cost compared to the actual sampling process itself, making the algorithm easy to tune for a given target acceptance rate.

However, when $C$ depends on a parameter $\theta$, changing $\theta$ will not keep the eigenspace invariant. This means that when using either of these samplers within a Hastings-within-Gibbs routine targeting a joint model $\pi(x, \theta) \propto \exp(f(x))\,\mathcal{N}(x; 0, C_{\theta}) \, p(\theta)$, the spectral decomposition of $C_{\theta}$ has to be recomputed every time the value of $\theta$ changes. This is computationally prohibitive for large dimensional $x$. This, however, can be mitigated thanks to the following observation \citep{titsias2018}: under a reparametrisation of $u$, which corresponds to considering the augmented target
\begin{equation}
    \label{eq:augmented-proposal-z-form}
    \pi(x, u) \propto \exp(f(x))\,\mathcal{N}(x; 0, C)\, \mathcal{N}\left(u; x + \frac{\delta}{2} \nabla f(x), \frac{\delta}{2} I\right),
\end{equation}
rather than \eqref{eq:augmented-proposal-general},
the proposal distribution $q(y \mid x, u)$ can be made independent of the current state of the chain $x$. This allows doing joint updates of $x$ and $\theta$ in parametric models, rather than using Gibbs steps to sample $x$ conditionally on $\theta$, and $\theta$ conditionally on $x$, thereby improving the mixing rate of the sampled Markov chain. This improvement, however, does not change the need for updating the spectral decomposition of $C_{\theta}$ and comes at the price of a lower statistical efficiency than the non-reparametrised version. 