In Section~\ref{subsec:auxiliary-lgssm}, we have made an explicit link between the auxiliary samplers of \citet{titsias2018} and Kalman filtering when the latent model has Gaussian dynamics. This linearity of the latent model corresponds to the assumption of linear Gaussian dynamics in the case of state-space models. This is a rather strong modeling assumption that is not easily verified, or enforced, in practice. In this section, we present an approach which uses local approximations of the dynamics model by conditional Gaussians using statistical linear regression.

In order to present the method in its most general form, we consider the case of state-space models, where the potential function $g(x_{0:T})$ corresponds to a product of observation models $\prod_{t=0}^T h_t(y_t \mid x_t)$ so that we can also form the Gaussian approximation to the potential itself. Following \citet{Tronarp2018iterative}, we suppose that the first two conditional moments
\begin{align}
    \label{eq:conditional-moments}
    \begin{split}
        m^X(x_{t-1}) \coloneqq \bbE[X_t \mid X_{t-1} = x_{t-1}],\\
        V^X(x_{t-1}) \coloneqq \bbV[X_t \mid X_{t-1} = x_{t-1}],
    \end{split}
    &
    \begin{split}
        m^Y(x_{t}) \coloneqq \bbE[Y_t \mid X_{t} = x_t],\\
        V^Y(x_{t}) \coloneqq \bbV[Y_t \mid X_{t} = x_{t}],
    \end{split}
\end{align}
of, respectively, the transitions and observation models appearing in \eqref{eq:ssm} can easily be either computed in closed form, or approximated well enough. Similarly, we suppose that the two first moments $m_0$ and $P_0$ of $p_0$ are known at least approximately.

Similarly as in Section~\ref{subsec:auxiliary-lgssm}, we start by considering an augmented target distribution
\begin{equation}
    \label{eq:augmented-ssm}
    \begin{split}
        p(\ft{x}{0}{T}, \ft{y}{0}{T}, \ft{u}{0}{T})
        \coloneqq p(x_0)
        &\left\{\prod_{t=0}^T h_t(y_t \mid x_t) \,\mathcal{N}(u_t; x_t, \frac{\delta}{2}\Sigma_t)  \right\} \\
        &\left\{\prod_{t=1}^T p_t(x_t \mid x_{t-1}) \right\},
    \end{split}
\end{equation}
where $\delta > 0$, and for all $t$, $\Sigma_t$ is a positive definite matrix.

In order to form a proposal distribution $q(x_{0:T} \mid u_{0:T}, y_{0:T})$ for $p(\ft{x}{0}{T} \mid \ft{y}{0}{T}, \ft{u}{0}{T})$, we linearise the state-space model \eqref{eq:ssm} around the trajectory at hand.
Let $x_{0:T} \in \bbR^{T \times d_x}$ and $u_{0:T} \in \bbR^{T \times d_x}$ be the current states of the auxiliary Markov chain, and let $\Gamma_{0:T}$ be a set of reference covariance matrices in $\bbR^{T \times d_x \times d_x}$, by which we mean that $\Gamma_t \in \bbR^{d_x \times d_x}$ needs to be positive definite for all $t$. We can apply the generalised statistical linear regression (GSLR) framework of \citet{Tronarp2018iterative} for the reference random variables $\zeta_t \sim \mathcal{N}(x_t, \Gamma_t)$, $t=0, \ldots, T$ to derive Gaussian approximations of the transition and observation models as follows:
\begin{equation}
    \label{eq:general_approx}
    \begin{split}
        p_{t}(z_t \mid z_{t-1})
        &\approx \mathcal{N}(z_t ; F_{t-1} z_{t-1} + b_{t-1}, Q_{t-1}), \\
        h_{t}(y_t \mid z_{t})
        &\approx \mathcal{N}(y_t ; H_{t} z_{t} + c_{t}, R_{t}),
    \end{split}
\end{equation}
with,
\begin{align}
    \label{eq:coeffs}
    \begin{split}
        F_{t-1} &= C^X_{t-1} \Gamma_{t-1}^{-1},\\
        b_{t-1} &= \mu^X_{t-1} - F_{t-1} x_{t-1},\\
        Q_{t-1} &= S^X_{t-1} - F_{t-1} \Gamma_{t-1}^{-1} F_{t-1}^{\top},
    \end{split}
    &
    \begin{split}
        H_{t} &= C^Y_{t} \Gamma_{t}^{-1},\\
        c_{t} &= \mu^Y_{t} - H_{t} x_{t},\\
        R_{t} &= S^Y_{t} - H_{t} \Gamma_{t}^{-1} H_{t}^{\top},
    \end{split}
\end{align}
and where, for the sake of readability, we do not notationally emphasise the dependency on $x$ and $\Gamma$. These Gaussian approximations are known to minimise a forward KL divergence with respect to the transition and observation models for the Gaussian variational family. The coefficients appearing in \eqref{eq:coeffs} are in turn given by the general formulae
\begin{align}
    \label{eq:general-GSLR-coeffs}
    \begin{split}
        C^X_{t-1} &= \mathbb{C}\left[m^X(\zeta_{t-1}), \zeta_{t-1}\right], \\
        \mu^X_{t-1} &= \mathbb{E}\left[m^X(\zeta_{t-1})\right], \\
        S^X_{t-1} &= \mathbb{E}\left[V^X(\zeta_{t-1})\right],\\
    \end{split}
    &
    \begin{split}
        C^Y_t &= \mathbb{C}\left[m^Y(\zeta_{t}), \zeta_{t}\right], \\
        \mu^Y_t &= \mathbb{E}\left[m^Y(\zeta_{t})\right], \\
        S^Y_t &= \mathbb{E}\left[V^Y(\zeta_{t})\right].
    \end{split}
\end{align}
Clearly, the quantities in \eqref{eq:general-GSLR-coeffs} are not typically available in closed-form, and we instead need to resort to further approximations. Such approximations are given by, for example, Taylor series expansions or sigma-point methods, such as Gauss--Hermite or unscented methods~\citep[see, e.g.,][Ch. 5]{sarkka2013bayesian}.

\begin{example}
    \label{eq:gslr-example}
    The first-order Taylor approximation to  $C^X_{t-1}$ can be obtained by $m^X(\zeta_{t-1}) \approx m^X(x_{t-1}) + \nabla m^X(x_{t-1})(\zeta_{t-1} - x_{t-1})$, giving $C^X_{t-1} \approx \nabla m^X(x_{t-1}) \Gamma_{t-1}$, and finally $F_{t-1} \approx \nabla m^X(x_{t-1})$. When the model at hand has additive Gaussian noise: $X_t = f(x_{t-1}) + \epsilon_{t-1}$, this recovers the well-known extended Kalman filter linearisation. Similarly, all the other linearisation parameters will be independent of the choice of $\Gamma_t$, $t=0, \ldots, T$. This property however does not hold when using second-order Taylor or sigma-points approximations for the integrals appearing in \eqref{eq:general-GSLR-coeffs}, and the choice of $\Gamma_t$ will then impact the performance of the algorithm~\citep[see, e.g.,][Ch. 5-6, for the role of the reference covariance in classical Gaussian approximated filtering and smoothing]{sarkka2013bayesian}.
\end{example}

The linear approximations~\eqref{eq:general_approx}, together with the known (or approximated) first two moments $m_0$ and $P_0$ of $\mathbb{P}_0$, can then be used to form a proposal distribution defined as an auxiliary LGSSM smoothing distribution with density
\begin{equation}
    \label{eq:auxiliary-LGSSM}
    \begin{split}
        q(z_{0:T} \mid u_{0:T}, x_{0:T}, y_{0:T})
        \propto \mathcal{N}(z_0; m_0, P_0)
        &\left\{\prod_{t=0}^T \mathcal{N}(y_t ; H_t z_t + c_t, R_t)\, \mathcal{N}\left(u_t ; z_t, \frac{\delta}{2} \Sigma_t\right)\right\}\\
        &\left\{\prod_{t=1}^T \mathcal{N}(z_t ; F_{t-1}z_{t-1} + b_{t-1}, Q_{t-1})\right\}.
    \end{split}
\end{equation}
This proposal distribution is then included as part of a Metropolis--Rosenbluth--Teller--Hastings acceptance-rejection step.
The resulting sampler then corresponds to Algorithm~\ref{alg:gslr-sampler}.
\begin{algorithm}[!htb]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{General Auxiliary Kalman sampler}\label{alg:gslr-sampler}
    \KwResult{An updated trajectory $x_{0:T}$}
    \Fn{\textsc{AuxKalmanSampler}$\big(x_{0:T})$}{
        \tcp{Generate the auxiliary observations}
        \lForSample{$t=0, 1, \ldots, T$}{
            $u_t \mid x_t \sim \mathcal{N}(\cdot; x_t, \frac{\delta}{2} \Sigma_t)$
        }
        \tcp{Proposal part}
        \For{$t=0\ldots, T$}{
            \uIf{$t>0$}{
                Form an approximation $\mathcal{N}(z_t; F^*_{t-1} z_{t-1} + b^*_{t-1}, Q^*_{t-1}) \approx p_t(z_t \mid z_{t-1})$ around $x_{t-1}$\;\label{step:forw-1}
            }
            Form an approximation $\mathcal{N}(y_t; H^*_{t} z_{t} + c^*_{t}, R^*_{t}) \approx p_t(y_t \mid z_{t})$, around $x_{t}$\;\label{step:forw-2}
        }
        Sample $x^*_{0:T} \sim q(\cdot \mid y_{0:T}, u_{0:T}, x_{0:T})$ and compute $L^* = \frac{q(x^*_{0:T}, y_{0:T}, u_{0:T} \mid x_{0:T})}{q(y_{0:T}, u_{0:T} \mid x_{0:T})}$\;
        \tcp{MRTH step}
        Form the reversed proposal $q^*(x_{0:T} \mid y_{0:T}, u_{0:T}, x^*_{0:T})$ following steps~\ref{step:forw-1} and~\ref{step:forw-2} around $x^*_{0:T}$ and compute $L = \frac{q^*(x_{0:T}, y_{0:T}, u_{0:T} \mid x^*_{0:T})}{q^*(y_{0:T}, u_{0:T} \mid x^*_{0:T})}$\;
        With probability $\min\left(1, \frac{p(x^*_{0:T}, y_{0:T}, u_{0:T}) L}{p(x_{0:T}, y_{0:T}, u_{0:T}) L^*}\right)$, set $x_{0:T} = x^*_{0:T}$\;
        \Return{$x_{0:T}$}
    }
\end{algorithm}

Evaluating the augmented density \eqref{eq:augmented-ssm} appearing in the acceptance ratio is easily done. Therefore, to effectively implement the steps above we only need to understand how to sample from the smoothing distribution of the LGSSM at hand, and compute the corresponding smoothing density $\frac{q(x^*_{0:T}, y_{0:T}, u_{0:T} \mid x_{0:T})}{q(y_{0:T}, u_{0:T} \mid x_{0:T})}$. This is readily achieved by applying sequential Kalman filtering equations to compute the filtering densities as well as the marginal likelihood of the true and auxiliary observations~\citep[see, e.g.,][Chap. 4]{sarkka2013bayesian}, and then sampling from the pathwise smoothing distribution backwards~\citep{deJong1995ffbs}. We are therefore able to achieve an overall computational complexity of $\bigO(T \times d_x^3)$. This is to be compared with the $\bigO(T^2 \times d_x^2)$ complexity of using the latent Gaussian samplers directly~\citep[Section 3.3]{titsias2018} as well as with the $\bigO(T^3 \times d_x^3)$ cost of forming their samplers in the first place. In particular, when the model at hand depends on a parameter $\theta$ updating it will not increase the cost of the $x_{0:T}$ sampling step, as opposed to the algorithms of \citet{titsias2018}. This property is critical for deriving computationally efficient samplers targeting the joint distribution of the parameters and latent states.

We end this section by noting that, while we assumed that the model at hand was a state-space model for consistency and ease of exposition, the method developed in this section can also be applied to the generalised Feynman--Kac model \eqref{eq:gen-ssm}. To do so, it suffices to apply the first order linearisation of Section~\ref{subsec:auxiliary-lgssm} to the likelihood term, and, independently, the GSLR approach of this section to the latent dynamics model. Similarly, the linearisation method used for the observation and transition need not be the same one, and the choice thereof is fully left to the user; for instance, if the likelihood is separable, we can use a Laplace approximation for the individual terms $g_t$, but still an extended linearisation for the transitions.