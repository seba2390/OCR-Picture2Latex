As a summary of the sections above, the Gaussian approximated smoothing solutions, whilst being more robust than SMC methods (and extensions thereof), lack the unbiasedness and convergence properties of these. This motivates the following contributions of this article:
\begin{enumerate}
    \item In Section~\ref{sec:auxiliary_samplers}, we show that, in the case of generalised Feynman--Kac models~\eqref{eq:gen-ssm} with Gaussian dynamics, the auxiliary proposals of \citet{titsias2018} recover the posterior distribution of an auxiliary LGSSM. We leverage this to reduce their time and space complexity to $\bigO(T)$ rather than $\bigO(T^3)$. We then use the generalised statistical regression framework of \citet{Tronarp2018iterative} to derive a new class of LGSSM auxiliary samplers for non-Gaussian latent dynamical systems.
    \item In Section~\ref{sec:PIT-sampling}, we introduce two parallel-in-time samplers for LGSSM pathwise smoothing distributions, one based on a prefix-sum implementation akin to \citet{Sarkka2021temporal}, and the other based on a divide-and-conquer recursion. We use these to sample from the LGSSM proposals, resulting in an overall $\bigO(\log T)$ MCMC algorithm on parallel hardware.
    \item In Section~\ref{sec:pgibbs_samplers}, we extend the construction of Section~\ref{sec:auxiliary_samplers} to the context of particle MCMC. This will allow us to tackle models for which Gaussian approximations are not practical or under-performing. We show that \citet{finke2021csmc} is recovered as a special case of our method.
    \item In Section~\ref{sec:experiments}, we illustrate the proposed methods on a series of examples from the SMC and Gaussian approximated inference literature. Special attention is paid to understanding their statistical as well as time and memory trade-offs.
\end{enumerate}