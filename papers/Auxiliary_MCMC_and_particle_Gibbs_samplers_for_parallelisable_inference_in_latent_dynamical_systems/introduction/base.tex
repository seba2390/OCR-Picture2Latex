For a given finite horizon $T>0$, state-space models \citep[SSMs, see, e.g.,][]{DelMoral2004Book,sarkka2013bayesian,Chopin2020book}, also called hidden Markov models, are a class of models which are fully described by the joint distribution of the states and observations:
\begin{equation}
    \label{eq:ssm}
    \bbP(\dd{\ft{x}{0}{T}}, \dd{\ft{y}{0}{T}})
    \coloneqq \bbP_0(\dd{x_0}) \left\{\prod_{t=0}^T H_t(\dd{y_t} \mid x_t)\right\} \left\{\prod_{t=1}^T P_t(\dd{x_t} \mid x_{t-1}) \right\}.
\end{equation}
In this formulation, $\bbP_0$ represents the initial distribution of the state $X_0$, while $P_t$ and $H_t$ represent the Markov transition and emission kernels for the states $X_t \in \bbR^{d_x}$ and observations $Y_t \in \bbR^{d_y}$, respectively.

Inference in SSMs typically has different meanings depending on the context: filtering is concerned with representing, sampling, or computing expectations of the quantity $\bbP(\dd{x_t} \mid \ft{y}{0}{t})$, where $\ft{y}{0}{t} = \{y_i; i=0, 1, \dots,t\}$; marginal smoothing is concerned with the same problems for the quantity $\bbP(\dd{x_t} \mid \ft{y}{0}{T})$; and pathwise smoothing is concerned with sampling or computing expectations of the quantity $\bbP(\dd{\ft{x}{0}{T}} \mid \ft{y}{0}{T})$. Furthermore, in many cases, the ``true'' generative model, consisting of the initial distribution $\mathbb{P}_0$, the transition kernels $P_t$, and emission kernels $H_t$, is unknown, and one needs to estimate it from the observed data. A typical way is to assume parametric forms for $\mathbb{P}_0(\dd{x_0} \mid \theta)$, $P_t(\dd{x_t}\mid x_{t-1}, \theta)$, and $H_t(\dd{y_t}\mid x_t, \theta)$, as well as a prior distribution $\bbP(\dd{\theta})$ for the parameters, resulting in a joint distribution
\begin{equation}
    \label{eq:parametric-ssm}
    \bbP(\dd{\ft{x}{0}{T}}, \dd{\ft{y}{0}{T}}, \dd{\theta})
    \coloneqq \bbP_0(\dd{x_0}\mid \theta) \left\{\prod_{t=0}^T H_t(\dd{y_t} \mid x_t, \theta)\right\} \left\{\prod_{t=1}^T P_t(\dd{x_t} \mid x_{t-1}, \theta) \right\} \bbP(\dd{\theta}).
\end{equation}
The parameter estimation problem then consists in computing either deterministic or probabilistic estimates of the posterior distribution of parameters $\mathbb{P}(\dd{\theta} \mid y_{0:T})$. In this work, we will focus on computing probabilistic estimates for the pathwise smoothing distribution $\bbP(\dd{\ft{x}{0}{T}} \mid \ft{y}{0}{T})$, the parameters distribution $\mathbb{P}(\dd{\theta} \mid y_{0:T})$, and the joint posterior distribution $\mathbb{P}(\dd{\theta}, \dd{x_{0:T}} \mid y_{0:T})$.

Throughout the rest of the article, we will assume that all distributions and kernels considered have a density with respect to the corresponding Lebesgue measure, and we consequently will write
\begin{equation}
    \label{eq:ssm-density}
    p(\ft{x}{0}{T}, \ft{y}{0}{T}, \theta)
    \coloneqq p_0(x_0 \mid \theta) \left\{\prod_{t=0}^T h_t(y_t \mid x_t, \theta)\right\} \left\{\prod_{t=1}^T p_t(x_t \mid x_{t-1}, \theta) \right\} p(\theta).
\end{equation}
Moreover, when it is not harmful, the dependency on the parameters $\theta$ will be made implicit, and the methods will be presented for known models.

A natural generalisation of~\eqref{eq:ssm-density} is given by the class of models
\begin{equation}
    \label{eq:gen-ssm}
    \pi(\ft{x}{0}{T})
    \propto g(x_0, x_1, \ldots, x_T) \, p_0(x_0)\, \left\{\prod_{t=1}^T p_t(x_t \mid x_{t-1}) \right\},
\end{equation}
which recover, as a special case, the pathwise smoothing formulation $p(x_{0:T} \mid y_{0:T})$ of \eqref{eq:parametric-ssm} with
\begin{equation}
    g(x_0, x_1, \ldots, x_T) = \prod_{t=0}^T h_t(y_t \mid x_t).
\end{equation}
It also recovers the class of Feynman-Kac models~\citep[see, e.g.,][]{DelMoral2004Book} with tractable transitions
\begin{equation}
    \label{eq:feynman-kac}
    \pi(\ft{x}{0}{T})
    \propto g_0(x_0) \, p_0(x_0) \, \left\{\prod_{t=1}^T g_t(x_t, x_{t-1}) \, p_t(x_t \mid x_{t-1}) \right\},
\end{equation}
by setting $g(x_0, x_1, \ldots, x_T) = g_0(x_0) \, \prod_{t=1}^T g_t(x_t, x_{t-1})$.
Under this more general formalism, the marginal smoothing problem corresponds to computing the quantity $\pi(x_t)$ for a given $t$, while pathwise smoothing aims at the full distribution $\pi(\ft{x}{0}{T})$.

Two popular classes of methods for inference in SSMs are the Gaussian approximation based methods (i.e., Kalman filters and smoothers), and the sequential Monte Carlo (SMC) based methods (i.e, particle filter and smoothers). These classes of methods are briefly reviewed next in Sections~\ref{subsec:kalman-intro} and \ref{subsec:smc-intro}.



