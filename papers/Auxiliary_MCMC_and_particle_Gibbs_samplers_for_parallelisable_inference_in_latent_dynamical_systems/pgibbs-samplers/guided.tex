In the previous section, we have described an algorithm that recovers \citet[Algorithm 3 and extensions]{finke2021csmc}. However, explicitly introducing the auxiliary variable allows us to decouple the state of the Markov chain and the generative model, so that we can incorporate statistical information in the auxiliary particle Gibbs sampler beside simple locality. Formally, we can implement ``locally adapted'' particle filters for $\pi(x_{0:T} \mid u_{0:T})$ to be used in the forward pass of Algorithm~\ref{alg:csmc}. While this can be applied to many models, we demonstrate how this can be done for differentiable models and for those that have (approximately) conditional Gaussian transitions and arbitrary potential functions.

When the potential functions $g_t$ are differentiable, it is possible to incorporate first or second order information from the potential. Indeed, for $g_t = \exp(\gamma_t)$ and $g = \prod_{t=0}^T g_t = \exp(\gamma)$, we have
\begin{equation}
    \begin{split}
        g(x_{0:T}) &= \exp(\gamma(x_{0:T}))\\
        &\approx \exp\left(\gamma(u_{0:T}) + \sum_{t=0}^T\pdv{\gamma(u_{0:T})}{x_t}\cdot (x_t - u_t)\right) \eqqcolon \prod_{t=0}^T \hat{g}_t(x_t \mid u_{0:T}).
    \end{split}
\end{equation}
Now, similarly to Section~\ref{sec:auxiliary_samplers}, we can form the proposal distributions
\begin{equation}
    \label{eq:differentiable-model}
    \begin{split}
        \tilde{p}_t(x_t \mid x_{t-1}, u_{0:T})
        &\propto \mathcal{N}(x_t; u_t, \frac{\delta}{2} \Sigma_t) \, \hat{g}_t(x_t \mid u_{0:T})\\
        &\propto \mathcal{N}\left(x_t; u_t + \frac{\delta}{2} \Sigma_t \pdv{\gamma(u_{0:T})}{x_t}, \frac{\delta}{2} \Sigma_t\right)
    \end{split}
\end{equation}
and similarly for $\tilde{p}_0$.
Finally, when $p_t$ is also differentiable, we can also include information from it in the sampler by considering $\exp(\gamma) = \prod_{t=0} p_t \, g_t$ rather than simply using $g_t$. Interestingly, these new proposal distributions are separable in time, so that they can be used in the parallel-in-time particle Gibbs algorithm of \citet{corenflos2022sequentialized} at no additional cost.

When the conditional mean and covariances of the prior process are available, as in Section~\ref{subsec:auxiliary-lgssm}, we can easily design a model~\citep[this is called a guided proposal in][Section 10.3.2]{Chopin2020book} locally adapted to the auxiliary observation:
\begin{equation}
    \label{eq:guided-prop}
    \begin{split}
        \tilde{p}_t(x_t \mid x_{t-1}, u_t)
        &\propto \mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t \right) \,\mathcal{N}\left(x_t; m^X_{t-1}(x_{t-1}), C^X_{t-1}(x_{t-1})\right)\\
        &\propto \mathcal{N}\left(x_t; \mu_t, \Lambda_t\right),
    \end{split}
\end{equation}
where $K_{t-1} = C^X_{t-1}(x_{t-1}) \left[C^X_{t-1}(x_{t-1}) + \frac{\delta}{2}\Sigma_t\right]^{-1}$, $\mu_t = m^X_{t-1}(x_{t-1}) + K_{t-1}[u_t - m^X_{t-1}(x_{t-1})]$, and $\Lambda_t = C^X_{t-1}(x_{t-1}) - K_{t-1} C^X_{t-1}(x_{t-1})$. A similar form is available for $p_0$.
Using this new proposal, an equivalent Feynman--Kac model will then take the form
\begin{equation}
    \label{eq:re-re-modified-auxiliary-feynman-kac}
    \pi(\ft{x}{0}{T} \mid \ft{u}{0}{T})
    \propto \tilde{p}_0(x_0)\left\{\prod_{t=1}^T \tilde{p}_t(x_t \mid x_{t-1})\right\}\tilde{g_0}(x_0) \left\{\prod_{t=1}^T \tilde{g_t}(x_t, x_{t-1}) \right\},
\end{equation}
where $\tilde{p}$ is given by~\eqref{eq:guided-prop}, and
\begin{equation}
    \tilde{g}_0(x_0) = \frac{g_0(x_0) \, p_0(x_0)}{\tilde{p}_0(x_0)}\mathcal{N}\left(u_0; x_0, \frac{\delta}{2} \Sigma_0 \right), \quad \tilde{g}_t(x_t, x_{t-1}) = \frac{g_t(x_t, x_{t-1}) \, p_t(x_t \mid x_{t-1})}{\tilde{p}_t(x_t \mid x_{t-1})}\mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t \right).
\end{equation}

Using such a generative model, contrary to the independent auxiliary proposal cases, is not parallelisable in time, and will scale as $\bigO(T)$, even on parallel hardware. On the other hand, when the potential is weakly informative compared to the dynamics, we can expect them to have better statistical properties, in the sense that they are likely to have lower autocorrelation than their independent proposal counterparts. We also note that the construction proposed in \eqref{eq:guided-prop} and \eqref{eq:re-re-modified-auxiliary-feynman-kac} extends to other methods developed to leverage approximate Gaussian conjugacy relationships in state-space models, for instance, they are directly compatible to Laplace approximations of the potential function~\citep[see, e.g.][Section 10.5.3]{Chopin2020book} or Rao--Blackwellisation~\citep{Murphy2001}.

Finally, it is worth highlighting that the two approaches presented in this section are not mutually exclusive. Indeed, we can combine an approximately Gaussian transition model together with a differentiable potential to obtain hybrid adapted proposals that may work better than their individual components taken in isolation. With the notations above, this would, for example, correspond to
\begin{equation}
    \label{eq:guided-prop-grad}
    \begin{split}
        \tilde{p}_t(x_t \mid x_{t-1}, u_t)
        &\propto \mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t \right) \,\mathcal{N}\left(x_t; m^X_{t-1}(x_{t-1}), C^X_{t-1}(x_{t-1})\right) \hat{g}_t(x_t \mid u_{0:T})\\
        &\propto \mathcal{N}\left(u_t + \frac{\delta}{2}\Sigma_t \pdv{\gamma(u_{0:T})}{x_t}; x_t, \frac{\delta}{2} \Sigma_t \right) \,\mathcal{N}\left(x_t; m^X_{t-1}(x_{t-1}), C^X_{t-1}(x_{t-1})\right),
    \end{split}
\end{equation}
if the linearisation point of $\gamma$ was taken to be $u_{0:T}$.

Other linearisation/combination choices are possible, and the willing statistician is free to fully leverage the flexibility brought by introducing the auxiliary observations $u_{0:T}$. Understanding which is the best choice will typically be application specific, although we expect the methods presented in this section to provide a competitive test-bed for more advanced methods.
