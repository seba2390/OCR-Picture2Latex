Section~\ref{sec:auxiliary_samplers} offers a class of new samplers for latent dynamical systems with tractable moments. It is however not the case that all practical problems verify this assumption, or that the potential function is always differentiable.
In this section, we instead consider the case of Feynman--Kac models~\eqref{eq:feynman-kac} with tractable densities.
For this class of models, the auxiliary target corresponds to a model with an augmented potential function
\begin{equation}
    \label{eq:auxiliary-feynman-kac}
    \pi(\ft{x}{0}{T}, \ft{u}{0}{T})
    \propto g_0(x_0) \, p_0(x_0) \left\{\prod_{t=1}^T g_t(x_t, x_{t-1}) \, p_t(x_t \mid x_{t-1}) \right\}\left\{\prod_{t=0}^T \mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t\right)\right\}.
\end{equation}
In order to sample from $\pi(\ft{x}{0}{T}, \ft{u}{0}{T})$, it is therefore enough to implement an abstract algorithm given by Algorithm~\ref{alg:aux-pgibbs}.

\begin{algorithm}[!htb]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Auxiliary cSMC}\label{alg:aux-pgibbs}
    \KwResult{An updated trajectory $x^{k+1}_{0:T}$}
    \Fn{\textsc{aux-cSMC}$\big(x^k_{0:T}\big)$}{
        \ForSample{$t=0, 1, \ldots, T$}{$u_t \sim \mathcal{N}(\cdot; x^k_t, \frac{\delta}{2} \Sigma_t)$}\label{step:pgibbs-1}
        Sample $x^{k+1}_{0:T} \sim K(\cdot \mid x^k_{0:T})$ \tcp{from a \textsc{cSMC} kernel keeping $\pi(\ft{x}{0}{T} \mid \ft{u^{k+1}}{0}{T})$ invariant}\label{step:pgibbs-2}
        \Return{$x^{k+1}_{0:T}$}
    }
\end{algorithm}

Clearly, in Algorithm~\ref{alg:aux-pgibbs}, if $(x^k_{0:T}, u^k_{0:T})$ are distributed according to $\pi$, then $(u^{k+1}_{0:T}, x^k_{0:T})$ are too after line~\ref{step:pgibbs-1}, so that $x^k_{0:T}$ is distributed according to $\pi(\cdot \mid u^{k+1}_{0:T})$, and therefore $(x^{k+1}_{0:T}, u^{k+1}_{0:T})$ are still distributed according to $\pi$ after line~\ref{step:pgibbs-2}. Otherwise said, this algorithm can be seen as a ``true'' particle Gibbs algorithm~\citep{Andrieu2010particle} for the choice of an improper prior $\pi(u_{0:T}) = 1$ for the auxiliary variables.

At first sight this may seem like a very bad idea, and it appears like we have made the problem more difficult than it was originally, and this is probably the reason why (to the best of our knowledge) this has not been \emph{explicitly} proposed before.
Indeed, instead of considering the potential function $g_t(x_t, x_{t-1})$, we are now considering the potential function $g_t(x_t, x_{t-1}) \, \mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t\right)$ at each time step $t$. This new potential function becomes very informative as $\delta$ gets smaller, which is known to induce high variance weights in particle filtering and smoothing algorithms~\citep[see, e.g.][Section 10.3.1]{Chopin2020book}. However, rather than seeing $\mathcal{N}\left(u_t; x_t, \frac{\delta}{2} \Sigma_t\right)$ as describing an auxiliary observation, we can leverage the symmetry of Gaussian distributions to look at it as the generative model $\mathcal{N}\left(x_t; u_t, \frac{\delta}{2} \Sigma_t\right)$ instead. We can consider the model
\begin{equation}
    \label{eq:modified-auxiliary-feynman-kac}
    \pi(\ft{x}{0}{T}, \ft{u}{0}{T})
    \propto \tilde{p}_0(x_0)\, \left\{\prod_{t=1}^T \tilde{p}_t(x_t \mid x_{t-1})\right\}\tilde{g}_0(x_0) \left\{\prod_{t=1}^T \tilde{g}_t(x_t, x_{t-1}) \right\},
\end{equation}
for the modified dynamics $\tilde{p}_t(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; u_t, \frac{\delta}{2} \Sigma_t\right)$ and potential functions $\tilde{g}_t = g_t \cdot p_t$, $t=0, 1, \ldots, T$.
This change of perspective immediately makes the problem much simpler, as we are now given a model with an informative and separable prior for which we can implement Step~\ref{step:pgibbs-2} of Algorithm~\ref{alg:aux-pgibbs} via Algorithm~\ref{alg:csmc}.
Moreover, because the auxiliary prior model is separable across time, the method of \citet{corenflos2022sequentialized} applies directly\footnote{While in \citet{corenflos2022sequentialized} it was derived for likelihood terms $g_t(x_t)$ rather than $g_t(x_t, x_{t-1})$ this was a notational simplification, and all the results derived within in fact hold for bivariate potentials.}, and a parallel-in-time particle Gibbs can be implemented to reduce the computational complexity to $\bigO(\log T)$ on parallel hardware. We also note that, contrarily to \citet[][Section 5.2]{corenflos2022sequentialized}, in this specific case, doing so would not necessarily come at a loss of statistical efficiency compared to sequential conditional SMC counterparts. This is due to the fact that the sequential algorithms would also rely on sampling from independent proposals.

In hindsight, it is easy to see that this method is \emph{exactly} the same one as the one proposed in \citet[Algorithm 3 and extensions]{finke2021csmc} who instead phrase it as a form of conditional SMC with exchangeable proposals. Informally, rather than using a proposal $\tilde{p}_t(x_t \mid x_{t-1})$, they use a proposal $\tilde{p}_t(x^1_t, \ldots, x^N_t)$ which induces an exchangeable dependency across particles, that is, $\tilde{p}_t(x^1_t, \ldots, x^N_t) = \tilde{p}_t(x^{\sigma(1)}_t, \ldots, x^{\sigma(N)}_t)$ for any permutation $\sigma$. As done in \citet{finke2021csmc} -- and first introduced in the context of classical MCMC in \citet{tjelmeland2004using} --, in the case of Gaussians, taking a conditional sample $p(\ldots, x^{k-1}_t, x^k_t, \ldots\mid x^k_t)$ can for instance be achieved by first sampling a ``centering'' variable $u_t \sim \mathcal{N}(x^k_t, \frac{\delta_t}{2}I)$ and then the remainder of the variables from $\prod_{i\neq k} \mathcal{N}(x_t^i; u_t, \frac{\delta_t}{2}I)$. This directly corresponds to Algorithm~\ref{alg:aux-pgibbs} for the modified Feynman--Kac model \eqref{eq:modified-auxiliary-feynman-kac}.
\begin{proposition}
    \label{prop:finke-equivalence}
    The algorithms of \citet{finke2021csmc} implement Algorithm~\ref{alg:aux-pgibbs} with separable proposal distributions $\mathcal{N}\left(\cdot; u_t, \frac{\delta_t}{2} \Sigma_t\right)$ for different choices of kernels $K$: embedded HMM~\citep{neal2003embedded}, conditional SMC~\citep{Andrieu2010particle}, conditional SMC with forced move~\citep{Chopin2015particle}, and conditional SMC with backward sampling~\citep{whiteley2010discussion}.
\end{proposition}
They show that, for a given choice of a standard conditional SMC -- with and without backward sampling -- Algorithm~\ref{alg:aux-pgibbs} avoids the curse of dimensionality, that is, its mixing time increases linearly with respect to $d_x$ rather than exponentially.
This new perspective on their method is rich in consequences: the entirety of the literature on particle Gibbs can be applied to step~\ref{step:pgibbs-2} of Algorithm~\ref{alg:aux-pgibbs}, and we can expect that the curse of dimensionality can be controlled in this case too.

For instance, while the parallelisation in time of our method via \citet{corenflos2022sequentialized} is immediate given its ``pure'' particle Gibbs interpretation, it was not directly obvious (although likely) that it applied to the perspective of \citet[][Propositions 4.1, 4.2, and 4.3]{finke2021csmc}. In the following section, we see how we can further extend this new perspective on the algorithm of \citet{finke2021csmc} to develop statistically superior local particle Gibbs samplers.


