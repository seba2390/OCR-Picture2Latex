%\jgl{For each use case, let's first give enough background on the technique. Then, enumerate the specific challenges/issues we deal with. Then, present explain the solution to each challenge. Finally, a subsection with the evaluation (I would include the experimental methodology as the first subsection of the evaluation).}

%\outline{Re-introduce RowClone briefly.}

\label{sec:rowclone}

\revdel{RowClone~\cite{seshadri2013rowclone} proposes \atb{minor changes to DRAM {chips} to copy data in DRAM to mitigate data movement overheads}. 
%RowClone provides up to \todo{X}\% system performance improvement on average \atb{across} real workloads. 
ComputeDRAM~\cite{gao2020computedram} demonstrates in-DRAM copy operations on contemporary, off-the-shelf DDR3 chips. \atb{Their results show that c}urrent DRAM devices can \atb{reliably} perform copy operations {at different temperatures and supply voltage levels using a set of violated tRAS and tRP timing parameters} in DRAM row granularity. \Copy{R3/1}{{None of the relevant prior works~\cite{seshadri.micro17,seshadri2013rowclone,wang2020figaro,gao2020computedram} provide a clear description {or a real system demonstration (like we do)} of a working memory allocation mechanism that can be integrated into a real operating system to expose RowClone capability to the programmer.}}}

We implement support for ComputeDRAM-\new{based} (i.e., using carefully-engineered sequences of valid DRAM commands {with violated timing parameters}) \new{RowClone (in-DRAM copy/initialization)} operations on PiDRAM to conduct a detailed study \newnew{of}\revdel{ (i) the system performance benefits that RowClone can provide and (ii)} the challenges associated with implementing RowClone end-to-end on a real system. \new{\Copy{R3/1}{{None of the relevant prior works~\cite{seshadri.micro17,seshadri2013rowclone,wang2020figaro,gao2020computedram,seshadri2020indram,seshadri.bookchapter17.arxiv,seshadri.thesis16,hajinazarsimdram} provide a clear description {or a real system demonstration} of a working memory allocation mechanism that can be \new{implemented in} a real operating system to expose RowClone capability to the programmer.}}} \revdel{\new{Using our real system prototype, we study the performance benefits that RowClone can provide in detail.}}

\subsection{Implementation Challenges}
%\outline{Talk about the problems briefly:} 

\noindent
\textbf{{Data Mapping.}}
\label{sec:rowclone_alignment}
{RowClone has data mapping and alignment requirements that {cannot be} satisfied by current memory allocation mechanisms (e.g., malloc~\cite{malloc}). We identify four major issues that complicate the process of implementing support for RowClone in real systems. First, \newnew{the} source and destination operands \new{(i.e., \omi{page (4 KiB)-sized} arrays)} of the copy operation must reside in the same DRAM subarray. We refer to this as the \emph{mapping} problem. Second, the source and destination operands must be aligned to DRAM rows. We refer to this as the \emph{alignment} problem. Third, the size of the copied data must be a multiple of the DRAM row size. The size constraint defines the granularity at which we can perform bulk-copy operations using RowClone. We refer to this as the \emph{granularity} problem.} Fourth, \new{RowClone must operate on up-to-date data that reside\newnew{s} in main memory.} Modern systems employ caches to exploit locality in memory accesses and reduce memory latency. \new{Thus,} cache blocks \new{(typically 64 B)} of either the source or the destination operand\newnew{s} of the RowClone operation {may} have \newnew{cache block} copies present in the cache hierarchy. Before performing RowClone, {the} cached copies \newnew{of pieces of both source and destination operands} must be invalidated and written back to main memory\revdel{ if necessary}. We refer to this as the %\emph{coherency} 
{\emph{memory coherence}} problem.


\new{We explain the data mapping and alignment requirements of RowClone \newnew{using} Figure~\ref{fig:rowclone_alignment}.} \reve{\new{The figure} depicts a \new{simplified version of a} DRAM {chip} with two banks and two subarrays. The {operand} Source 1 cannot be copied to the {operand} Target 1 as \new{the operands} do not satisfy the \emph{granularity} constraint (\boldone). Performing such a copy operation would overwrite the \newnew{remaining (i.e., non-Target 1)} data in \new{Target 1's DRAM row} with \newnew{the remaining (i.e., non-Source 1)} data in \new{Source 1's DRAM row}. Source 2 cannot be copied to Target 2 as Target 2 is not \emph{aligned} to its DRAM row (\boldtwo). Source 3 cannot be copied to Target 3, as these {operands} are not \emph{mapped} to the same DRAM subarray (\boldthree). \new{In contrast}, Source 4 can be copied to Target 4 using in-DRAM copy \new{because these operands} are (i) \emph{mapped} to the same DRAM subarray, (ii) aligned to their DRAM rows and (iii) occupy their rows completely (i.e., the {operands} have sizes equal to DRAM row size) (\boldfour).}
%\jgl{Add the circled numbers that the figure has.}
\iffalse
\begin{figure*}[!h]
     \centering
     \begin{subfigure}[b]{0.55\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figures/rowclone_alignment.pdf}
          \caption{A DRAM {chip} with two banks and two subarrays. Only \omi{one} operation \omi{(i.e., operation~\newnew{\boldfour{}})} can succeed as its operands satisfy \omi{all of} \emph{mapping}, \emph{alignment} and \emph{granularity} constraints.}
          \label{fig:rowclone_alignment}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.40\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figures/memory-allocation-mechanism.pdf}
          \caption{Overview of our memory allocation mechanism\revdel{ Source and destination operands (Arrays A and B) \newnew{that are} comprised of page-sized memory blocks are placed in DRAM subarrays.}}
          \label{fig:memory-allocation-mechanism}
     \end{subfigure}
     \caption{\newnew{RowClone memory allocation requirements (left), memory allocation mechanism overview (right)}}
     \vspace{-2mm}
\end{figure*}
\fi
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/rowclone_alignment.pdf}
  \caption{A DRAM {chip} with two banks and two subarrays. Only \omi{one} operation \omi{(i.e., operation~\newnew{\boldfour{}})} can succeed as its operands satisfy \omi{all of} \emph{mapping}, \emph{alignment} and \emph{granularity} constraints.}
  \label{fig:rowclone_alignment}
\end{figure}

\subsection{Memory Allocation Mechanism}
\label{sec:rowclone_mechanism}

\new{C}omputing systems employ various layers of address mappings that obfuscate the DRAM row-bank-column address mapping from the programmer~\cite{helm2020Reliable,cojocar2020susceptible}, \new{which makes allocating source and target operands as depicted in Figure~\ref{fig:rowclone_alignment}-(\boldfour) difficult}. \new{DRAM manufacturers employ DRAM\newnew{-}internal address mapping schemes (Section~\ref{sec:background-dram}) that translate from logical (e.g., \newnew{memory-controller-visible} DRAM row, bank, column) \omi{addresses} to physical DRAM addresses.} {\new{G}eneral-purpose} processors use complex functions to map physical addresses to DDRx addresses \new{(e.g., DRAM banks, rows and columns)}~\cite{hillenbrand2017Physical}. \new{The} operating system (OS) maps virtual addresses to physical addresses to provide isolation between multiple processes\revdel{ and %an 
\juan{the} illusion of larger-than-available physical memory}. \new{Only these virtual addresses are exposed to the programmer}. Without control over the virtual address $\rightarrow{}$ DRAM address mapping, the programmer \emph{cannot} \new{easily} place data in a way %to satisfy 
\juan{that satisfies} the mapping and alignment requirements of \newnew{an} in-DRAM copy operation. 

%Either many layers of address mapping must be exposed to the programmer, or the OS should expose a memory allocation interface that satisfies the requirements of in-DRAM copy operations. 

%\todo{Emphasize that we can allocate data even if there is a complex DDRX mapping, we just need to find the DRAM rows that are in the same subarray}

\new{W}e implement a new memory allocation mechanism that {can perform} memory allocation for \new{RowClone (in-DRAM copy/initialization)} operations. \atb{\new{This} mechanism enables page-granularity \new{RowClone} operations (i.e., a virtual page can be copied to another virtual page using RowClone) \emph{without} introducing any changes to the programming model.} \new{The mechanism} places the operands of RowClone operations \new{in} the same DRAM subarray while maximizing the bank-level parallelism in regular DRAM accesses (reads \& writes) to these operands \new{(such that the commonly\newnew{-}performed streaming accesses to these operands benefit from bank-level parallelism in DRAM)}. {\atb{Figure~\ref{fig:memory-allocation-mechanism} depicts an overview of our memory allocation mechanism.}}

%\iffalse
\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figures/memory-allocation-mechanism.pdf}
  \caption{Overview of our memory allocation mechanism\revdel{ Source and destination operands (Arrays A and B) \newnew{that are} comprised of page-sized memory blocks are placed in DRAM subarrays.}}
  \label{fig:memory-allocation-mechanism}
\end{figure}
%\fi

\new{At a high level, o}ur \new{memory allocation} mechanism \new{(i)} splits the source and destination operands into page-sized virtually-addressed memory blocks, \new{(ii)} allocate\new{s} two physical pages in different DRAM rows in the same DRAM subarray,\revdel{ These physical pages are aligned to the same page-size boundary (i.e., satisfying the \emph{alignment} requirement).} \new{(iii) assigns} these physical pages to virtual pages that correspond to the source and destination memory blocks at the same index such that the source block can be copied to the destination block using RowClone. We repeat this process until we exhaust the page-sized memory blocks. \new{As the mechanism processes subsequent page-sized memory blocks of the \newnew{two} operands, it allocates} physical pages from a different DRAM bank \new{to maximize bank-level parallelism in streaming accesses to these operands}.

To overcome the \emph{mapping}, \emph{alignment}, and \emph{granularity} problems, we implement our memory management mechanism in the custom supervisor software of \X.
We expose \new{the allocation mechanism} \newnew{using} the \texttt{alloc\_align(N, ID)} system call. The system call returns a pointer to a contiguous array of \emph{N} bytes in the virtual address space {(i.e., one operand)}. \Copy{R5/5}{Multiple calls with the same \emph{ID} to \texttt{alloc\_align(N, ID)} place the allocated arrays in the same subarray in DRAM, such that they can be copied from one to another using RowClone. {If \emph{N} is too large such that it exceeds the size of available physical memory, \texttt{alloc\_align} fails and causes an exception.} \revdel{Figure~\ref{fig:alloc-align-walkthrough}\revdel{shows the components of our mechanism and} presents the workflow of \texttt{alloc\_align()}.}} \revd{Our implementation of\revdel{\texttt{alloc\_align}} \new{RowClone} requires application developers to directly use \texttt{alloc\_align} to allocate data instead of \texttt{malloc} and similar function calls.}

%We characterize DRAM rows for their RowClone success rates and initialize the SAMT with DRAM row address pairs that have 100\% RowClone success rate. \revd{We conduct a reliability study by repeatedly performing RowClone operations using randomly initialized source and destination DRAM rows, and validate the correctness of RowClone operations by comparing the data on the destination row with the data on the source row. We observe no errors after performing 1000 RowClone operations using all possible (source, destination) row pairs in one DRAM subarray.}
%\jgl{Say here that Figure 5 shows the workflow and components.}

\iffalse
\begin{figure}[!ht]
  \centering
  \includegraphics[width=.40\textwidth]{figures/SAMT.pdf}
  \caption{Organization of the subarray mapping table. Each physical address in a tuple in the list addresses one half of a DRAM row}
  \label{fig:SAMT}
\end{figure}
\fi

{\new{The \omi{custom supervisor software}} maintains three key structures \new{to make \texttt{alloc\_align()} work}: (i) Subarray Mapping Table (SAMT), (ii) Allocation ID Table (AIT), and (iii) Initializer Rows Table (IRT).}

\noindent
\textbf{{1) Subarray Mapping Table (SAMT).}} We use the \textbf{S}ubarray \textbf{Ma}pping \textbf{T}able (SAMT) to maintain a list of physical page addresses that point to DRAM rows that are in the same DRAM subarray. {\new{\texttt{alloc\_align()}} queries SAMT to find physical addresses that map to rows in one subarray.} 
%Figure~\ref{fig:SAMT} depicts the organization of the SAMT. 

\new{SAMT} contains the physical pages that point to DRAM rows in each \new{subarray}. \new{SAMT is} indexed using subarray identifiers (SA IDs) in the range \emph{[0, number of subarrays\revdel{ in a DRAM bank})}. \new{\new{SAMT}} contains an entry for every subarray\revdel{ in the DRAM \new{bank}}. \new{An} entry consists of two elements: (i) the number of free physical address tuples and (ii) a list of physical address tuples. Each tuple in the list contains two physical addresses that \newnew{respectively} point to the first and second hal\newnew{ves} of the same DRAM row. The list of tuples \newnew{contains} all the physical addresses that point to DRAM rows in the DRAM subarray indexed by the SAMT\revdel{ \new{sub-table}} entry. We allocate free physical pages \new{listed in an} entry and assign them to the virtual pages (i.e., memory blocks) that make up the {row-copy operands (i.e., arrays)} allocated by \texttt{alloc\_align()}. We slightly modify our \new{high-level memory} allocation mechanism to allow for two memory blocks \new{(4 KiB virtually-addressed pages)} of an array to be placed in the same DRAM row, as the page size in our system is 4 KiB, and the size of a DRAM row is 8 KiB. \atb{We call two memory blocks in {the same operand} that are placed in the same DRAM row \emph{sibling memory blocks} \omi{(also called sibling pages)}. The parameter \emph{N} \new{of} the \texttt{alloc\_align()} call defines this relationship: We designate memory blocks that are precisely {\emph{N/2}} bytes apart as \emph{sibling memory blocks}.}


\noindent
\textbf{{Finding \new{the} DRAM Rows in a Subarray.}} \Copy{R2/1A}{{Finding the DRAM row addresses that belong \newnew{to} the same subarray is not straightforward due to DRAM-internal mapping schemes employed by DRAM manufacturers (Section~\ref{sec:rowclone_alignment}). It is extremely difficult to learn which DRAM address (i.e., bank-row-column) is actually mapped to a physical location (e.g., a subarray) in the DRAM device, as these mappings are not exposed through publicly accessible datasheets or standard definitions~\cite{jedecDDR4,micron2016ddr4,patel2022case}. We make the key observation that the entire mapping scheme need \emph{not} be available to successfully perform RowClone operations.}}

\Copy{R2/1B}{We observe that for a set of \emph{\{source, destination\}} DRAM row address pairs, RowClone operations \atb{repeatedly} succeed with a 100\% probability. We hypothesize that these pairs of DRAM row addresses are mapped to the same DRAM subarray. {We identify these row address pairs by conducting a \emph{\newnew{RowClone success rate}} experiment where we repeatedly perform RowClone operations between every \emph{source, destination} row address pair in a DRAM bank. Our experiment works in three steps{:} we (i) initialize both the source and the destination row with random data, (ii) perform a RowClone operation from the source to the destination row, and (iii) compare the data in the destination row with the source row. \omi{RowClone success rate is calculated as the number of bits that differ between the source and destination rows' data divided by the number of bits stored in a row (8 KiB in our prototype).} If there is no difference between the source and the destination rows' data \omi{(i.e., the RowClone success rate for the source and the destination row is 100\%)}, we {infer that} the RowClone operation {was} successful. We repeat the experiment for 1000 iterations for each row address pair and if every iteration is successful, we store the address pair in the SAMT{, indicating that the row address pair is mapped to different rows in the same DRAM subarray}.}} \Copy{R4/9}{{\newnew{The same RowClone success rate experiment} could be conducted \revdel{to identify DRAM row address pairs {that are mapped to the same DRAM subarray (i.e., data can be copied from one row address to the other row address in the pair} using RowClone operations) }in \newnew{other} systems that are based on PiDRAM or in a PiDRAM prototype that uses a different DRAM {module}. Since \newnew{the RowClone success rate experiment} is a one-time process, its overheads (e.g., time taken to iterate over all DRAM rows using our experiment) \omi{are} amortized over the lifetime of such a system.}}

\noindent
\textbf{{2) Allocation ID Table (AIT).}}
To {keep track of different operands} that are allocated by \texttt{alloc\_align} using the same \emph{ID} \newnew{(used to place different arrays in the same subarray)}, we use the \textbf{A}llocation \textbf{I}D \textbf{T}able (AIT).\revdel{ AIT is logically partitioned into sub-tables for each DRAM bank.} AIT entries are indexed by \emph{allocation ID}s (the parameter \emph{ID} \new{of} the \emph{alloc\_align} call). Each AIT entry stores a pointer to an SAMT entry. The SAMT entry pointed by the AIT entry contains the set of physical addresses that were allocated using the same \emph{allocation ID}. AIT entries are used by the \texttt{alloc\_align} function to find which DRAM subarray can be used to allocate DRAM rows from, such that the newly allocated array can be copied to other arrays allocated using the same \emph{ID}.



\noindent
\textbf{{3) Initializer Rows Table (IRT).}}
To find which row in a DRAM subarray can be used as the source operand in zero-initialization (RowClone-Initialize) operations, we maintain the \textbf{I}nitializer \textbf{R}ows \textbf{T}able (IRT). The IRT is indexed using physical page numbers. RowCopy-Initialize operations query the IRT to obtain the physical address of the DRAM row that is initialized with zeros and \new{that belong} to the same subarray as the destination operand (i.e., the DRAM row to be initialized with zeros). 


\atb{Figure~\ref{fig:alloc-align-walkthrough} describes how \texttt{alloc\_align()} works over an end-to-end example. \newnew{Using the RowClone success rate experiment (\omi{described above}), the custom supervisor software (\omi{CSS for short})} find\newnew{s} the DRAM rows that are in the same subarray (\boldone) and initialize\newnew{s} the \newnew{Subarray Mapping Table (SAMT)}. The programmer allocates two 128 KiB arrays, A and B, via \texttt{alloc\_align()} using the same \emph{allocation id} (\textbf{0}), with the intent to copy from A to B (\boldtwo). \newnew{\omi{CSS}} allocate\newnew{s} contiguous ranges of virtual addresses to A and B, \newnew{and} then split\newnew{s the virtual address ranges} into page-sized memory blocks (\boldthree). \newnew{\omi{CSS}} assigns consecutive memory blocks to consecutive DRAM banks and access\new{es} the \newnew{Allocation ID Table (AIT)} with the \emph{allocation id} (\boldfour) for each memory block. \newnew{By accessing the AIT, \omi{CSS} retrieves} the \emph{subarray id} that points to a SAMT entry. The SAMT entry corresponds to the subarray that contains the arrays that are allocated using the \emph{allocation id} (\boldfive). \newnew{\omi{CSS}} access\newnew{es} the SAMT entry to retrieve two physical addresses that point to the same DRAM row. \newnew{\omi{CSS}} map\newnew{s} a memory block and its \emph{sibling \omi{memory block}} \new{(i.e., the memory block that is N/2 bytes away from this memory block, where N is the \emph{size} argument of the \texttt{alloc\_align()} call)} to these two physical addresses, such that they are mapped to the first and the second halves of {the same} DRAM row (\boldsix). \reva{Once allocated, these physical addresses are pinned to main memory and cannot be swapped out to storage.} Finally, \newnew{\omi{CSS}} update\newnew{s} the page table with the physical addresses to map the memory blocks to the same DRAM row (\boldseven).}

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=.8\textwidth]{figures/alloc-align-rcc.pdf}
  \caption{{\texttt{Alloc\_align() and RowClone-Copy (\texttt{rcc}, see Section~\ref{sec:rccrci}) workflow.}}} %\jgl{This figure may be better at the bottom of page 7}
  \label{fig:alloc-align-walkthrough}
  
\end{figure*}

\subsection{Maintaining \juan{Memory Coherence}}

\label{sec:coherency}
{Since memory instructions update the cached copies of data \newnew{(Section~\ref{sec:rowclone_alignment})},} a naive implementation of RowClone can potentially operate on stale data because cached copies of RowClone operands can be modified \new{by CPU store instructions}. 
%To maintain coherency, we flush dirty cache blocks of the operands of the RowClone operations.
\juan{\new{Thus,} we need to ensure memory coherence to prevent RowClone from operating on stale data.}

\new{W}e implement a new \juan{custom RISC-V} instruction, \juan{called \emph{CLFLUSH},} to flush dirty cache blocks to DRAM %, as 
(RISC-V does not implement any cache management operations~\cite{riscv-spec}) \juan{so as to ensure RowClone operates on up-to-date data}. %We implement a custom RISC-V cache flush instruction, CLFLUSH, in our infrastructure. We use CLFLUSH to flush 
\juan{\newnew{A} CLFLUSH \newnew{instruction} flushes \newnew{(invalidates)}} \newnew{a} {physically addressed} dirty \newnew{(clean)} cache block. \atb{CLFLUSH or other cache management operations with similar semantics are supported in X86~\cite{x86-manual} and ARM architectures~\cite{arm-cmos}. \new{Thus, the CLFLUSH instruction (that we implement) provides a minimally invasive solution (i.e., it requires no changes to the specification of commercial ISAs) to the memory coherence problem.}}

\revdel{ \new{This way, the effects of CLFLUSH on the performance of end-to-end RowClone (Section~\ref{sec:rowclone-experimental-methodology}) \revdel{likely}represents \new{a case} of a minimally invasive solution (i.e., it requires no changes to the specification of commercial ISAs).}} %Our solution to 
%\juan{With CLFLUSH, we address the \emph{memory coherence} problem %is 
%with a minimally invasive solution} (i.e., it requires no changes to the microarchitecture and the specification in commercial ISAs)}. 
%and likely represents the solution that would be implemented in a commercial system that implements RowClone.} 
%\jgl{I commented out the second part of the sentence because I find it unnecessary and risky.}
We modify the non-blocking data cache and the \omi{R}ocket core modules (defined in \textit{NBDCache.scala} and \textit{rocket.scala} in \omi{Rocket Chip}~\cite{asanovic2016rocket}, respectively) to implement CLFLUSH. We modify the RISC-V GNU compiler toolchain~\cite{riscv-gnu-toolchain} to expose CLFLUSH as an instruction to C/C++ applications. %We use the CLFLUSH instruction to flush dirty cache blocks that map to the source and destination rows in RowClone operations.
\juan{Before executing a RowClone \newnew{Copy or Initialization} operation \newnew{(see Section~\ref{sec:rccrci})}, \newnew{the \omi{custom supervisor software}} \new{flushes} \atb{(invalidate\new{s})} the cache blocks \new{of} the source \atb{(destination)} row of the RowClone operation \atb{using CLFLUSH}.}

\subsection{RowClone-Copy and RowClone-Initialize}
\label{sec:rccrci}
\new{W}e support the RowClone-Copy and RowClone-Initialize operations in our custom supervisor software via two functions: (i) \newnew{RowClone-Copy,} \texttt{rcc(void *dest, void *src, int size)} and (ii) \newnew{RowClone-Initialize,} \texttt{rci(void* dest, int size)}. \atb{\texttt{rcc} copies \emph{size} \newnew{number of} contiguous bytes in the virtual address space starting from the \emph{src} memory address to the \emph{dest} memory address. \texttt{rci} initializes \emph{size} \newnew{number of} contiguous bytes in the virtual address space starting from the \emph{dest} memory address.} We expose \texttt{rcc} and \texttt{rci} to user-level programs \newnew{using} system calls \newnew{defined in the \omi{custom supervisor software}}. 
% \jgl{Explain syntax and semantics of these APIs.}

\juan{\texttt{rcc}} \new{(i)} splits the source and destination operands into page-aligned, page-sized blocks, \new{(ii)} traverses the page table \atb{(Figure~\ref{fig:alloc-align-walkthrough}-\boldeight)} to find the physical address of each block (i.e., the address of a DRAM row), \new{(iii)} flushes all cache blocks corresponding to the source \newnew{operand} and \atb{invalidates all cache blocks corresponding to the} destination \newnew{operand}, \new{and (iv)}
%\jgl{Do we need to flush destination row or only source row?}, 
performs a RowClone operation from the source row to the destination row using pumolib{'s \texttt{copy\_row()} function}.

\juan{\texttt{rci}} \new{(i)} splits the destination operand into page-aligned, page-sized blocks, \new{(ii)} traverses the page table to find the physical address of \newnew{the destination operand}\new{, (iii)} queries the \newnew{Initializer Rows Table (IRT, see Section~\ref{sec:rowclone_mechanism})} to obtain the physical address of the initializer row \atb{(i.e., source operand)}, \new{(iv)} \atb{invalidates} the cache blocks corresponding to the \newnew{destination operand}, and \new{(v)} performs a RowClone operation from the initializer row to the destination row using using pumolib{'s \texttt{copy\_row()} function}. 
%\jgl{There is something confusing about CLFLUSH in combination with rcc. In 5.3, it seems that we expose CLFLUSH to the program. But here it seems that rcc calls CLFLUSH, and not the program directly. Please clarify.}


\subsection{Evaluation}

We evaluate our solutions for the challenges in implementing RowClone end-to-end on a real system using \X. We modify the custom memory controller to implement DRAM command sequences \atb{($ACT\rightarrow{}PRE\rightarrow{}ACT$)} to trigger RowClone operations. \atb{We set the $tRAS$ and $tRP$ parameters to 10 $ns$ {(below the manufacturer-recommended \SI{37.5}{\nano\second} for tRAS and \SI{13.5}{\nano\second} for tRP~\cite{micron2018ddr3})}.} We modify our custom supervisor software to implement our memory allocation mechanism and add support for RowClone-Copy \new{(\texttt{rcc})} and RowClone-Initialize \new{(\texttt{rci})} operations.

%\jgl{I don't know if "Evaluation" is the right place for this subsection. To me, this is related to the implementation.}

\subsubsection{Experimental Methodology}
\label{sec:rowclone-experimental-methodology}
\revdel{We use \X to implement RowClone end-to-end \juan{and evaluate our solutions to RowClone's system integration challenges, as explained in Section~\ref{sec:rowclone_alignment}}.} 
Table~\ref{table:system-configuration} describes the configuration of the components in our system. \atb{We use the pipelined and in-order \newnew{R}ocket core with 16 K\newnew{i}B L1 data cache and 4\newnew{-}entry TLB as the main processor of our system. We use the 1 GiB DDR3 module available on the ZC706 board as the main memory where we conduct PuM operations.} 

%\jgl{Describe the system briefly here. Place the table after this paragraph}. 

\iffalse
\begin{table}[!ht]
   
\caption{PiDRAM system configuration (left). Physical address to DRAM address mapping in \X (right). \newnew{B}yte offset is used to address the byte in the DRAM burst.}
    \centering
\begin{minipage}[h]{0.48\linewidth}\centering
\scriptsize

  \begin{tabular}{@{} l @{}}
  \toprule
  \textbf{CPU:} 50~MHz; in-order Rocket core \cite{asanovic2016rocket}; \textbf{TLB} 4 entries DTLB; LRU policy\\        
  \midrule
  \textbf{L1 Data Cache:} 16~KiB, 4-way; 
  64~B line; random replacement policy\\
  \midrule
  \textbf{DRAM Memory:} 1~GiB DDR3; 800MT/s; single rank; 8~KiB row size\\
  \midrule
  \label{table:system-configuration}
  \end{tabular}
  %}
  
\end{minipage}\hfill%
\begin{minipage}[h]{0.48\linewidth}\centering
\includegraphics[width=1.0\textwidth]{figures/rbc-mapping.pdf}
  \label{fig:DDR-address-mapping}
\end{minipage}

  \vspace{-8mm}
\end{table}
\fi

\begin{table}[!ht]
  \centering
  \caption{PiDRAM system configuration}
  
  \scriptsize
  %\tiny
  %\resizebox{\columnwidth}{!}{%
  \begin{tabular}{@{} l @{}}
  \toprule
  \textbf{CPU:} 50~MHz; in-order Rocket core \cite{asanovic2016rocket}; \textbf{TLB} 4 entries DTLB; LRU policy\\        
  \midrule
  \textbf{L1 Data Cache:} 16~KiB, 4-way; 
  64~B line; random replacement policy\\
  \midrule
  \textbf{DRAM Memory:} 1~GiB DDR3; 800MT/s; single rank; 8~KiB row size\\
  \midrule
  \end{tabular}
  %}
  
  \label{table:system-configuration}
\end{table}

\atb{Implementing RowClone require\newnew{s} an additional 198 lines of Verilog code over \X's existing Verilog design. We add 43 and 522 lines of C code to pumolib and to our custom supervisor software, respectively, to implement RowClone in the software components.}

%\jgl{I think this paragraph goes better after Table 2.}

Table~\ref{fig:DDR-address-mapping} describes the mapping scheme we use in our custom memory controller to translate from physical to DRAM row-bank-column addresses. We map physical addresses to DRAM columns, banks, and rows from lower-order bits to higher-order bits to exploit the bank-level parallelism in memory accesses to consecutive physical pages. We note that our memory management mechanism is compatible with other physical address $\rightarrow{}$ DRAM address mappings~\cite{hillenbrand2017Physical}. \Copy{R2/2}{{For example, for a mapping {scheme} where \omu{page offset bits (physical address (PA) [11:0]) include all or a subset of the bank address bits}\revdel{\omi{DRAM bank and DRAM row addresses, in that order, are mapped from the most significant to the least significant bits in physical addresses (i.e., physical address (PA) [29:27] maps to DRAM bank address and PA[26:13] maps to DRAM row address)}},\revdel{ bank address and the column address \newnew{in the address scheme depicted in Table~\ref{table:system-configuration}} {are} swapped\revdel{(Table~\ref{fig:DDR-address-mapping}},} a single RowClone operand \omu{(i.e., a 4 KiB page)} would be split across multiple DRAM banks. This \new{only} coarsens the granularity of RowClone operations as the \newnew{{sibling \omi{pages}}} that must be copied in unison, to satisfy the granularity constraint, increases.}} We expect that for \new{other complex or unknown} physical address $\rightarrow{}$ DRAM address mapping scheme\new{s}, the characterization of the DRAM device for RowClone success rate would take longer. In the worst case, DRAM row addresses that belong to the same DRAM subarray can be found by testing all combinations of physical addresses \newnew{for their RowClone success rate}. 
%\jgl{First, explain the mapping and why you use this one. Second, explain (if possible) that other mappings are possible.}. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.40\textwidth]{figures/rbc-mapping.pdf}
  \caption{Physical address to DRAM address mapping in \X. \newnew{B}yte offset is used to address the byte in the DRAM burst.}
  \label{fig:DDR-address-mapping}
\end{figure}
We evaluate \texttt{rcc} and \texttt{rci} operations under two configurations to understand the copy/initialization throughput improvements provided by \texttt{rcc} and \texttt{rci} \newnew{over traditional CPU-copy operations performed by the Rocket core}, and to understand the overheads introduced by end-to-end support for {commodity DRAM based} PuM operations. We test two configurations: (i) \emph{Bare-Metal}, to find the maximum RowClone throughput our implementation provides solely using pumolib, 
%(ii) \emph{E2E}, to understand the performance benefits our end-to-end implementation of RowClone can bring, 
and (ii) \emph{No Flush}, to understand the benefits our end-to-end implementation \new{(i.e., with system support)} of RowClone can provide in copy/initialization throughput when data in DRAM is up-to-date \omi{(i.e., when no coherence operations are needed)}.

%\jgl{Explain why different configurations and high level view of what is different}: 
% (i) \jgl{Start giving the name of the configuration. Then, explain} 
\noindent
\textbf{Bare-Metal.} We assume that RowClone operations always target data that is allocated correctly in DRAM (i.e., \newnew{there is} no overhead introduced by address translation, IRT accesses, and CLFLUSH operations). We directly issue RowClone operations via pumolib using physical addresses. \newnew{Traditional CPU-copy operations (executed on the Rocket core) also use physical addresses}.

%\textbf{E2E.} We assume that the programmer uses the \texttt{alloc\_align} function to allocate the operands of RowClone operations. We issue RowClone operations using the \texttt{rcc} and \texttt{rci} system calls.
\noindent
\textbf{No Flush.} We assume that the programmer uses the \texttt{alloc\_align} function to allocate the operands of RowClone operations. We use %modified 
{a} version of \texttt{rcc} and \texttt{rci} system calls that do not use CLFLUSH to flush cache blocks of source and destination operands of RowClone operations. We run the \emph{No Flush} configuration on our custom supervisor software. \newnew{Both \texttt{rcc} and \texttt{rci}, and traditional CPU-copy operations use virtual addresses}.

%we assume that data allocation and address translation incurs no performance overhead (i.e., we directly issue RowClone operations via pumolib using physical addresses), depicted as \textbf{Bare-Metal}, (ii) we only assume that data in DRAM is always up-to-date (i.e., we do not flush dirty cache blocks, but we allocate data for RowClone operations and translate from virtual to physical addresses), depicted as \textbf{No Flush} and (iii) we allocate data for RowClone operations and flush cache blocks to address the challenges described in Section~\ref{X}, depicted as \textbf{E2E}. We run the \textbf{Bare-Metal} configuration bare-metal on rocket (i.e., no supervisor software, no virtual memory). We run the other two configurations on our custom supervisor software. \jgl{I would have a paragraph for each configuration, with the name of the configuration with bold font in the beginning of the paragraph.}

\revcommon{\subsubsection{Workloads}}
\label{sec:methodology-workloads}
For the two configurations, we run a microbenchmark that consists of two programs, \emph{\newnew{copy}} and \emph{\newnew{init}}, on \new{our prototype}. Both programs take the argument $N$, where \emph{copy} copies an $N$-byte array to another $N$-byte array and \emph{init} initializes an $N$\newnew{-}byte array \newnew{to all} zeros. Both programs \atb{have two versions: (i) CPU-copy, which} copies/initializes data using memory loads and stores, (ii) RowClone, which uses RowClone operations to perform copy/initialization. \atb{\new{All programs use} \texttt{alloc\_align} \new{to allocate data}.}
%\jgl{Better say that each of the microbenchmarks has two versions: (1) a CPU version, and (2) a RowClone version}.
\atb{The performance results we present in this section are the average of a \omi{1000} runs.}
%\jgl{Bad wording. "The performance results we present in this section are the average of a thousand runs."}, 
\atb{To maintain the same initial system state for both CPU-copy and RowClone, \revdel{(i.e., initially the data is up-to-date in DRAM), }we flush all cache blocks \newnew{before} each \newnew{one} \new{of the \omi{1000}} runs.}
%\jgl{This should be a separate sentence. Say why you flush.}. 
We run each program for array sizes (\emph{N}) that are powers of two and {$8~KiB$} $< N < 8~MiB$,  
%\jgl{Say these are the size of the arrays. Say you test powers of 2 sizes between these two.} 
and find the average copy/initialization throughput \newnew{across all 1000 runs} (by measuring the \# of elapsed CPU cycles to execute copy/initialization operations) for CPU-copy, RowClone-Copy (\texttt{rcc}), and RowClone-Initialize (\texttt{rci}).\footnote{\Copy{R1/2}{{We tested RowClone operations using \texttt{alloc\_align()} with up to 8 MiB of allocation size {since} we observed diminishing returns on performance improvement provided by RowClone operations \newnew{on} {larger} array sizes.}}}


\atb{We analyze the overheads of CLFLUSH operations on copy/initialization throughput that \texttt{rcc} and \texttt{rci} can provide. We measure the execution time of CLFLUSH operations \new{in} our \new{prototype} to find how many CPU cycles it takes to flush a (i) dirty and (ii) clean cache block on average \newnew{across 1000 measurements}. We simulate various scenarios \new{(described in Figure~\ref{fig:system-flush-overhead})} where we assume a certain \newnew{fraction} of the operands of RowClone operations are cached and dirty.}

%\atb{Finally, we conduct a \emph{projection} study to approximate the copy throughput improvement RowClone can provide when enabled end-to-end in a contemporary system. We use a recent \textbf{AMD Zen2~\cite{X}} processor with a DDR4 module operating at \textbf{3200 MT/s} as main memory. We run the \emph{copy} microbenchmark for 8 MiB arrays to find the copy throughput the contemporary system can achieve. We run a microbenchmark to find the latency cost of flushing cache blocks in the contemporary system using the \texttt{clflush\_opt}\todo{<-check} instruction~\cite{X}. We tightly schedule the DDR4 commands that need to be issued to perform RowClone in DRAM to find the latency of performing one RowClone operation (i.e., copying 8 KBs). We pessimistically assume that the contemporary processor spends as many cycles as rocket does when running the \texttt{rcc} function.}

\subsubsection{Bare-Metal RowClone}

Figure~\ref{fig:bare-metal-speedup} \juan{shows} the throughput improvement \omi{provided by} \texttt{rcc} {and \texttt{rci}} for \emph{copy} {and \emph{initialize}} \omi{over CPU-copy and CPU-initialization}  
%and the relative increase in execution time \jgl{I'd better talk about "throughput improvement" and "latency increase".} of rcc operations between consecutive
for increasing array sizes. 

\begin{figure}[h] %[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/bare-metal-speedup.pdf}
  \caption{\omi{RowClone-Copy} and \omi{RowClone-Initialize} over traditional CPU-copy and -initialization for the Bare-Metal configuration%\todo{Fix y axis title, shrink height.}
  %\jgl{I changed the position of this figure. It better goes after the paragraph where first referenced.}
  %\jgl{There is no triangle mark for 8KiB. It should be "1".}
  }
  \label{fig:bare-metal-speedup}
\end{figure}
\iffalse
\begin{figure*}[!h]
     \centering
     \begin{subfigure}[b]{0.47\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figures/bare-metal-speedup.pdf}
          \caption{Throughput improvement provided by \newnew{\omi{RowClone-Copy} and \omi{RowClone-Initialize} over traditional CPU-copy and -initialization for the Bare-Metal configuration}%\todo{Fix y axis title, shrink height.}
          %\jgl{I changed the position of this figure. It better goes after the paragraph where first referenced.}
          %\jgl{There is no triangle mark for 8KiB. It should be "1".}
          }
          \label{fig:bare-metal-speedup}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/system-copy-init-speedup1.pdf}
         \caption{Throughput improvement provided by \newnew{\omi{RowClone-Copy} and \omi{RowClone-Initialize} over traditional CPU-copy and -initialization for the NoFlush configuration}}
         \label{fig:system-copy-speedup1}
     \end{subfigure}
     \caption{\newnew{\omi{RowClone-Copy} and \omi{RowClone-Initialize} throughput improvement for the Bare-Metal (left) and the NoFlush (right) configurations}}
     \vspace{-2mm}
\end{figure*}
\fi
We make two major observations. \newnew{First, we observe that \texttt{rcc} and \texttt{rci} provide significant throughput improvement over traditional CPU-copy and \omi{CPU}-initialization. The throughput improvement \omi{provided by \texttt{rcc}} ranges from 317.5$\times{}$ (for 8 KiB arrays) to 364.8$\times{}$ (for 8 MiB arrays). \omi{The throughput improvement provided by \texttt{rci} ranges} from 172.4$\times{}$ to 182.4$\times{}$.} \newnew{Second,} the throughput improvement provided by \texttt{rcc} and \texttt{rci} increases as the array size increases. This increase saturates when the array size \newnew{reaches} 1 MiB. 
\omi{The load/store instructions used by CPU-copy and CPU-initialization access the operands in a streaming manner. The eviction of dirty cache blocks (i.e., the destination operands of copy and initialization operations) interfere with other memory requests on the memory bus.\footnote{\omi{Because the data cache in our prototype employs random replacement policy, as the array size increases, the fraction of cache evictions among all memory requests also increase\omi{s}, causing \omu{larger} interference on the memory bus (i.e., more memory requests to satisfy all cache evictions). The interference saturates at 1 MiB array size.}} We attribute the observed saturation at 1 MiB array size to the interference on the memory bus.}

%We attribute this to the interference \omi{o}n the memory bus \newnew{in CPU-copy and -initialization} caused by the cache blocks that are evicted \newnew{(CPU-copy and -initialization operations make the cache blocks of the destination operands dirty)} from the cache as the \newnew{copy and init microbenchmarks} access the arrays in a streaming manner\changev{Because our cache is 16 KiB 4-way, and employs a random replacement policy, as the copied/inited array size increases, the fraction of cache evictions among all memory requests also increase. If this is not too much detail or the text is not satisfactory, I can integrate this explanation.}\changev{I removed some text that I deemed redundant/not important}.
\revdel{Second, the \emph{latency} of a RowClone-Copy operation to copy 8 KiB in DRAM (using pumolib) is only 58 CPU cycles.
%\jgl{Can you justify where these 74 come from? Precise breakdown is not needed though} 
%to perform one RowClone-Copy operation to copy an 8 KiB array. 
\atb{This time is spent on (i) running the pumolib function that executes memory requests (e.g., \omi{\emph{store}} instructions
%\jgl{SD? Do you mean STORE?}
) to store data in the instruction and flag registers in the POC to ask the memory controller to perform a RowClone operation, (ii) waiting for the memory controller to respond with an acknowledgment}. The latency of executing RowClone-Copy increases \emph{linearly} with the array size. {We make similar observations for RowClone-Initialize. RowClone-Initialize can provide nearly the half of the throughput improvement provided RowClone-Copy, 182.4$\times{}$ over the CPU-Initialization baseline.  Because the CPU needs to execute only half as many instructions compared to copy operations (one load and one store) in initialization operations (one store), it can perform an initialization operation approximately two times faster than a copy operation for the same array size.}}

%\atb{A constant factor constituting the execution time of rcc operations (e.g., function call overhead) \jgl{I do not understand this sentence}.} \atb{We attribute the increase in performance improvement provided by rcc across array sizes to the sub-linear increase in relative execution time of rcc operations.}\jgl{Can you explain why there is a sub-linear increase and there is no increase after 1MiB?}



\subsubsection{\juan{No Flush} RowClone}

We analyze the overhead in copy/initialization throughput introduced by system support (Section~\ref{sec:rowclone_mechanism})\revdel{that we implement to enable RowClone end-to-end}.
%, and CLFLUSH \jgl{Good instruction name? Isn't this x86?} instructions that we use to maintain coherency. 
\reva{Figure~\ref{fig:system-copy-speedup1}} shows the throughput improvement of copy and initialization provided in the \emph{No Flush} configuration by \texttt{rcc} and \texttt{rci} operations.


\begin{figure}[h] %[!ht]
  \centering
  \includegraphics[width=\linewidth]{figures/system-copy-init-speedup1.pdf}
  \caption{Throughput improvement provided by \newnew{\omi{RowClone-Copy} and \omi{RowClone-Initialize} over traditional CPU-copy and -initialization for the NoFlush configuration}.}
    %\jgl{Here it is "speedup", but it is "performance improvement" in Fig. 7. Be consistent. "Throughput improvement" is better, I think.}
  \label{fig:system-copy-speedup1}
\end{figure}

\revdel{\newnew{Figure~\ref{fig:system-copy-speedup2-abs} shows the execution time of \texttt{rcc} and \texttt{rci} operations in CPU cycles.} Figure~\ref{fig:system-copy-speedup2} shows the proportional increase in \texttt{rcc} and \texttt{rci}'s execution time between consecutive array sizes \newnew{(i.e., the y-axis value for an array size S (between 16 KiB and 8 MiB) shows the execution time of \texttt{rcc} or \texttt{rci} normalized to the execution time of \texttt{rcc} or \texttt{rci} for an array size of S/2)}. For example, the \newnew{circles} at 16 KiB \omi{(2 MiB)} shows the execution time of \texttt{rcc} \newnew{\texttt{and rci}} for 16 KiB \omi{(2 MiB)} arrays divided by the execution time of \texttt{rcc} \newnew{\texttt{and rci}} for 8 KiB \omi{(1 MiB)} arrays. 
%, and (ii) the \emph{E2E} configuration, where we flush all cache lines that map to source and destination DRAM rows prior to rcc operations. 
%\juan{Figure~\ref{fig:system-copy-ini-speedup} shows...}


\begin{figure}[ht]
     \centering
     \vspace{-2mm}
         \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/perf-absolute.pdf}
         \caption{{The execution time of \texttt{rcc} and \texttt{rci}}}
         \label{fig:system-copy-speedup2-abs}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/system-copy-init-speedup2.pdf}
         \caption{The increase in \texttt{rcc} and \texttt{rci}'s execution time}
         \label{fig:system-copy-speedup2}
     \end{subfigure}

     \vspace{-3mm}
     \caption{\texttt{rcc} and \texttt{rci} execution time (left) and increase in the execution time as array size increases (right)}
     \vspace{-5mm}
\end{figure}
}

\iffalse
\begin{figure}[h] %[!ht]
  \centering
  \includegraphics[width=.46\textwidth]{figures/system-copy-init-speedup2.pdf}
  \caption{\reva{The proportional increase in \texttt{rcc} and \texttt{rci}'s execution time for consecutive array sizes}%\jgl{Say throughput are bars, and increase in exec. time are lines. Otherwise, the reader can only guess that due to the proximity of each legend to the corresponding axis.}}
  .}
    %\jgl{Here it is "speedup", but it is "performance improvement" in Fig. 7. Be consistent. "Throughput improvement" is better, I think.}
  \label{fig:system-copy-speedup2}
\end{figure}
\fi

We make two major observations: 
First, \juan{\texttt{rcc}} improves the copy throughput by 58.3$\times{}$ for 8 KiB and by 118.5$\times{}$ for 8 MiB arrays, whereas \texttt{rci} improves initialization throughput by 31.4$\times{}$ for 8 KiB and by 88.7$\times{}$ for 8 MiB arrays.
%\jgl{Bad writing. Also, the numbers are swapped.}. 
\atb{Second, we observe that the throughput improvement provided by \texttt{rcc} and \texttt{rci} improves \emph{non-linearly} as the array size increases. The \newnew{\omu{execution time (in Rocket core clock cycles)} of} \texttt{rcc} and \texttt{rci} operations \omu{(not shown in Figure~\ref{fig:system-copy-speedup1})} \emph{does not increase linearly} with the array size. \omu{For example, the execution time of \texttt{rcc} is 397 and 584 cycles at 8 KiB and 16 KiB array sizes, respectively, resulting in a $1.47\times{}$ increase in execution time between 8 KiB and 16 KiB array sizes. However, the execution time of \texttt{rcc} is 92,656 and 187,335 cycles at 4 MiB and 8 MiB array sizes, respectively, resulting in a $2.02\times{}$ increase in execution time between 4 MiB and 8 MiB array sizes. We make similar observations on the execution time of \texttt{rci}.} For every RowClone operation, \texttt{rcc} and \texttt{rci} walk the page table to find the physical addresses corresponding to the source (\texttt{rcc}) and the destination (\texttt{rcc} and \texttt{rci}) operands. We attribute the non-linear increase in \texttt{rcc} and \texttt{rci}'s execution time to (i) the locality exploited by \newnew{the R}ocket core in accesses to the page table and (ii) the \revdel{proportionally }diminishing constant cost in \newnew{the} execution time \newnew{of both \texttt{rcc} and \texttt{rci}} due to common instructions executed to perform a system call.}


%Second, we observe that the speedups provided by rcc do not remain stable as array size nears 8 MiB (e.g., 175.5$\times{}$ at 4 MiB vs. 170.5$\times{}$ at 8 MiB for \emph{No Flush}), as opposed to our observation in the \emph{Bare-Metal} configuration. We attribute this to the additional pressure on the L1 cache and the TLB brought by rcc's accesses to the page table. We hypothesize that the memory access patterns of the page-table walk operation are more detrimental to performance at specific array sizes (e.g., 2, 8 MiB)  \jgl{These guesses sound quite random to me. If there is no intuition behind, better to not try to explain these fluctuations.}. Third, we observe that the \emph{No Flush} configuration performs 7.3$\times{}$ (8.4$\times{}$,4.3$\times{}$) better than the \emph{E2E} configuration on average (maximum,minimum) across all array sizes \jgl{Can you explain why such performance degradation in terms of bandwidth usage during the flush operation?}.

%\changev{\ref{q:r1q3}}
\subsubsection{CLFLUSH Overhead}
\label{sec:clflush-overhead}
%\todo[fancyline,linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,size=\scriptsize]{\ref{q:r1q3}}
\Copy{R1/3}{We find that our implementation of CLFLUSH takes 45 \new{Rocket core} \omi{clock} cycles to flush a dirty cache block and 6 \new{Rocket core} cycles to \newnew{invalidate} a clean cache block. {We estimate the throughput improvement of \newnew{\texttt{rcc} and \texttt{rci}} including the CLFLUSH overhead. We assume that all cache blocks of the source \newnew{and destination} operand\newnew{s are} cached, and that a \newnew{fraction} of the \newnew{all cached cache blocks} is dirty (quantified on the x-axis). We do not include the overhead of accessing the data \omi{(e.g., by using \omi{\emph{load}} instructions)} \emph{after} \omi{the data} \newnew{gets copied in DRAM}.} Figure~\ref{fig:system-flush-overhead} shows the {estimated} improvement in \newnew{copy and initialization} throughput \newnew{that} \newnew{\texttt{rcc} and \texttt{rci}} \newnew{provide} for 8 MiB arrays.}

\begin{figure}[!h] %[!ht]
  \centering
  \includegraphics[width=.47\textwidth]{figures/system-flush-overhead.pdf}
  \caption{Throughput improvement provided by \texttt{rcc} and \texttt{rci} \revb{with CLFLUSH} over \omi{R}ocket's CPU-copy.}
  %\vspace{-4mm}
  \label{fig:system-flush-overhead}
\end{figure}

We make three major observations. \newnew{First, even with inefficient cache flush operations, \texttt{rcc} and \texttt{rci} provide 3.2$\times{}$ and 3.9$\times{}$ higher throughput over the CPU-copy and \omi{CPU}-initialization operations, assuming 50\% of the cache blocks of the 8 MiB source operand \omi{are} dirty, respectively.}\revdel{ \newnew{Second,} the copy and initialization throughput provided by \texttt{rcc} and \texttt{rci} are 9.4$\times{}$ and 6.1$\times{}$ smaller when we invalidate clean cache blocks of the source and destination operands of the copy/initialization operations, compared to the \emph{No Flush} configuration, where we assume all data is up-to-date in DRAM. The throughput improvement provided by \texttt{rcc} and \texttt{rci} is 12.6$\times{}$ and 14.6$\times{}$ for 8 MiB arrays over the CPU-copy/initialization baselines.} \newnew{Second,} as the \newnew{fraction} of dirty cache blocks \newnew{increase\omi{s}}, the throughput improvement \newnew{provided by both \texttt{rcc} and \texttt{rci}} \newnew{decreases (down to}\revdel{. The improvement in throughput provided by \texttt{rcc} and \texttt{rci} goes down to} 1.9$\times{}$ for \texttt{rcc} and 2.3$\times{}$ \newnew{for \texttt{rci} \omi{for 100\% dirty cache block \omu{fraction}})}. Third, we observe that \texttt{rci} can provide better throughput improvement compared to \texttt{rcc} when we \omi{include the CLFLUSH overhead. This is because} \texttt{rci} flushes cache blocks of one operand (destination), whereas \texttt{rcc} flushes cache blocks of both operands (source and destination). 

\Copy{R1/4}{{We do not study the distribution of dirty cache block \omi{fractions} in real application\omi{s} as \omi{that} is not the goal of our CLFLUSH overhead analysis. However, if a large dirty {cache block} \omi{fraction} causes severe overhead in a real application, the system designer \newnew{or the user of the system} would likely decide not to offload the operation to PuM (i.e., performing \newnew{\texttt{rcc}} operations instead of CPU-Copy). PiDRAM's prototype can be useful for studies on different PuM system integration aspects, including such offloading decisions.}}

\omi{We observe that the CLFLUSH operations are inefficient in supporting coherence for RowClone operations. Even so, we see that RowClone-Copy and RowClone-Initialization provides throughput improvements ranging from 1.9$\times{}$ to 14.6$\times{}$. We expect the throughput improvement benefits to increase as coherence between the CPU caches and PIM accelerators become more efficient with new techniques~\cite{boroumand2019conda,boroumand2016pim,seshadri2014dirty}.}

\subsubsection{{Real Workload Study}}
\label{sec:real-workload-study}
{The benefit of \texttt{rcc} and \texttt{rci} on a full application depends on what fraction of execution time is spent on bulk data copy and initialization. We demonstrate the benefit of \texttt{rcc} and \texttt{rci} on \emph{forkbench}~\cite{seshadri2013rowclone} and \emph{compile}~\cite{seshadri2013rowclone} workloads with varying fractions of time spent on bulk data copy and initialization, to show that our infrastructure can enable end-to-end execution and estimation of benefits \omi{on real workloads}.\footnote{A full workload study \omi{(i.e., with system calls to a full operating system such as Linux)} of \emph{forkbench} and \emph{compile} is out of the scope of this paper. Our infrastructure currently cannot execute all \omi{possible} workloads due to \omu{the} limited library and system call functionality provided by the \omi{RISC-V Proxy Kernel~\cite{riscv-pk}}.} We especially study \emph{forkbench} in detail to demonstrate how the benefits vary with the time spent on data copying in the baseline for this workload.}

{\emph{Forkbench} first allocates N memory pages and copies data to these pages from a buffer in the process's memory and then accesses 32K random cache blocks within the newly allocated pages to emulate a workload that frequently spawns new processes. We evaluate \emph{forkbench} under varying bulk data copy sizes where we sweep N from 8 to {2048} in increasing powers of two. 

\emph{Compile} first zero-allocates (\texttt{calloc} or \texttt{rci}) two pages (\omi{8 KiBs}) and then executes a number of arithmetic and memory instructions to operate on the zero-allocated data. We carefully develop the \emph{compile} microbenchmark to maintain a realistic ratio between the number of arithmetic and memory instructions executed and zero-allocation function calls made, which we obtain by profiling \emph{gcc}~\cite{perfLinux}. We use the \emph{No-Flush} configuration of our RowClone implementation \omi{for both \emph{forkbench} and \emph{compile}}.}

{Figure~\ref{fig:fork-all} plots the speedup provided by \texttt{rcc} over \omi{the} CPU-copy (bars, left y-axis) baseline, and the proportion of time spent on \texttt{memcpy} functions by the CPU-copy baseline (blue curve, right y-axis), for various configurations of \emph{forkbench} on the x-axis.}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fork-all.pdf}
    \caption{\emph{Forkbench} speedup (bars, left y-axis) and time spent on \texttt{memcpy} by the CPU baseline (curve, right y-axis)}
    \label{fig:fork-all}
\end{figure}

\noindent
{\textbf{Forkbench.}} {We observe that RowClone-Copy can significantly improve the performance of \emph{forkbench} by up to {42.9}\%. \omi{RowClone-Copy's} performance improvement increases as the number of pages copied increase. This is because the copy operations accelerated by \texttt{rcc} contribute a larger amount to the total execution time of the workload. The \texttt{memcpy} function calls take {86\%} of the CPU-copy baseline's time during \emph{forkbench} execution for N = {2048}.}

\noindent
{\textbf{Compile.}} {RowClone-Initialize \omi{improves} the performance of \emph{compile} by 9\%. Only an estimated 17\% of the execution time of \emph{compile} is used for zero-allocation by the CPU-initialization baseline, \texttt{rci} reduces the overhead of zero-allocation by \omi{(i)} performing \omi{in-DRAM} bulk-initialization and \omi{(ii)} executing a smaller number of instructions.}

\noindent
{\textbf{Libquantum.} To demonstrate that \X can run real workloads, we run a SPEC2006~\cite{spec2006} workload (libquantum). We modify the \texttt{calloc} (allocates and zero initializes memory) function call to allocate data using \texttt{alloc\_align}, and initialize data using \texttt{rci} for allocations that are larger than 8 KiBs.}

% Libquantum test takes 295041863 instructions
% Initializing 512 KiB is going to take 512 K instructions
% libqt spends .2 of its instructions on initialization, therefore the low perf. improvement.
\revcommon{Using \texttt{rci} to bulk initialize data in libquantum improves end-to-end application performance by 1.3\% (compared to the baseline that uses CPU-Initialization). This improvement is brought by \texttt{rci}, which initializes a total amount of 512 KiBs of memory\footnote{\revcommon{\omi{In libquantum,} there are 16 calls to \texttt{calloc} that exceed the 8 KiB allocation size. \omi{W}e only bulk initialize data using \texttt{rci} for these 16 calls.}} using RowClone operations.} {We note that the proportion of store instructions executed by libquantum to initialize arrays in the CPU-initialization baseline is only 0.2\% of all dynamic instructions in the libquantum workload {which amounts to an estimated 2.3\% of the total runtime of libquantum.} \omi{T}hus, the \omi{1.3\% end-to-end} performance improvement provided by \texttt{rci}, which can \omi{ideally} speed up only 2.3\% of \omu{the} total runtime, is reasonable, and we expect it to increase with the initialization intensity of workloads.}

\noindent
\omi{\textbf{Summary.}}
\revd{We conclude from our {evaluation} that end-to-end implementations of RowClone (i) can be \omi{efficiently} supported in real systems by employing memory allocation mechanisms that satisfy the memory \emph{alignment}, \emph{mapping}, \emph{granularity} requirements (Section~\ref{sec:rowclone_alignment}) of RowClone operations, (ii) can greatly improve copy/initialization throughput in real systems, and (iii) require cache coherenc\omi{e} mechanisms (e.g., PIM-optimized coherenc\omi{e} management~\cite{boroumand2019conda,boroumand2016pim,seshadri2014dirty}) that can flush dirty cache blocks of RowClone operands efficiently to achieve optimal copy/initialization throughput improvement.} {\X{} can be used \omi{to} estimat\omi{e} end-to-end \omi{workload} execution benefits provided by RowClone operations.} \revcommon{Our experiment\omi{s} using libquantum\omi{, forkbench, and compile} show that (i) \X can run real workloads, (ii) our end-to-end implementation of RowClone operates correctly, and (iii) RowClone can improve the performance of real workloads \omi{in a real system, even when inefficient CLFLUSH operations are used to maintain memory coherence}.}

\iffalse
\subsubsection{Projection Study\todo{better title}}

We calculate the copy throughput of RowClone operations to be no less than 542.5 GiB/s by tightly scheduling the DRAM commands required to perform consecutive RowClone operations. We find that on average, it takes (i) 9.5 cycles to flush a dirty cache block and (ii) 6.8 cycles to flush (invalidate) a clean cache block using \texttt{clflush\_opt}\todo{<-check}. We find that RowClone cannot provide an improvement in copy throughput even when all cache blocks of source and destination operands are clean. 
\fi

\iffalse
\subsection{Interface}
To enable RowClone end-to-end we expose it to the user via the memcpy C library function.

\subsubsection{Hardware Interface}

\subsubsection{Software Library}

\subsection{Data Mapping}

We offer two mechanisms to manage the alignment problem: (i) a passive management mechanism and (ii) an active management mechanism that builds on top of the passive mechanism. To summarize, the passive management mechanism refers to a scheme where the run-time system (e.g. OS) does not spend additional effort on aligning user data. The user explicitly asks the run-time system to align a number of arrays in DRAM. In the active mechanism the OS spends effort on aligning data structures such that they can be copied using RowClone. 

\subsubsection{Passive Management}

We assume that the OS provides an interface for the user to align data in the DRAM address space. This interface can be a function call cpyalign where arr is an array of pointers to the data structures that the user wants to align, and n is the number of bytes that are aligned. For example, the user calls cpyalign(16384, A, B) to align the first 16KBs of data from two arrays in memory such that they can be copied to each other using RowClone.

We discuss the internals of the copy align function in this paragraph. To implement copyalign the OS must have control over the two address translation layers: (i) virtual to physical and (ii) physical to DRAM device. We assume that the OS already has \textbf{full} control over the virtual to physical address translation layer. Contemporary CPUs implement a non-intuitive address translation layer in the memory controller~\cite{X,Y,Z} that translates from physical addresses to DRAM channel, rank, bank, row and column addresses. The OS must have some knowledge over this mapping to implement the cpyalign() function. Specifically, the OS must know which physical addresses correspond to which DRAM subarrays in the DRAM device. The OS must group physical addresses that map to the rows in a subarray together. The OS stores these groups in a table that is indexed by subarray ids. Each element in this table (named subphy table) is a set of physical addresses that point to DRAM rows in the same subarray (the indexed subarray). The OS needs to have \textbf{partial} knowledge (as described) over the physical to DRAM address translation layer to support cpyalign(). 

We describe how cpyalign() works over an example in this paragraph. The user calls cpyalign(16384, A, B). The OS splits the arrays into 8KB large chunks. Since there are two arrays, there are two arrays of chunks of size 2 after the split. The OS then traverses these two arrays and for the chunks that are at the same offset it maps two physical addresses from a single entry of the subphy table to the virtual addresses of these chunks. The OS repeats this for all chunks.

The vendors can provide the OS with the subphy table contents, i.e. the physical addresses that map to the same subarrays. In the worst case, the OS can reverse engineer these mappings using RowClone once over a lifetime and store these mappings in a persistent storage device. Finally, existing works show that contemporary memory controller mappings can be decoded~\cite{X, Y}. This is not required since the OS does not need this mapping information at such fine granularity to implement cpyalign().

\subsubsection{Active Management}

The active management mechanism is built on top of the passive management mechanism as it requires the OS to provide the same functionality (the cpyalign() function). However, in this mechanism the user does not explicitly call the cpyalign() function. Depending on program behavior, the OS dynamically aligns data structures in DRAM to optimize for bulk-copy performance.

We discuss how the active management can be implemented in this paragraph.

\subsubsection{Effects of Cache Flushing}

We analyze the overhead of cache management operations that are required for correct RowClone operation as described in Section~\ref{sec:coherency}. We run a microbenchmark that uses RowClone-Copy to copy arrays with sizes ranging from 8KBs to 256MBs. Figure~\ref{fig:flush_vs_noflush} depicts the performance loss when we maintain coherency by using CLFLUSH instructions for differently sized source and destination operands for the RowClone operation. We study the effects of maintaining coherency on performance under two scenarios: (i) where we execute a CLFLUSH operation for each cache block in both source and destination operands, (ii) where we execute as many CLFLUSH operations required to evict all cache blocks in CPU caches. For the second scenario we assume 8MB large CPU caches. We compare the performance of these two scenarios against an ideal baseline where coherency is maintained with no cost, i.e. no cache flush operations are needed to maintain coherency. We observe that the cost of maintaining coherency goes up to 20X for an array size of \textcolor{red}{Y}. The performance compared to the baseline continuously worsens for the first scenario. This is due to ... For the second scenario, ... 

We conclude that in order to efficiently support RowClone operations, a system needs to employ more sophisticated coherency mechanisms~\cite{X,Y,Z} that reduce cache flush operations as much as possible. We leave the evaluation of such sophisticated mechanisms to future work.

\begin{figure*}[!th]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/PLACEHOLDER_flush_vs_noflush.pdf}
  \caption{\textcolor{red}{PLACEHOLDER} Relative performance of RowClone when we maintain coherency via CLFLUSH instructions compared to the ideal case where coherency is not a problem (higher is better).}
  \label{fig:flush_vs_noflush}
\end{figure*}
\fi


%Implementing support for RowClone on PiDRAM requires modifications across the whole HW/SW stack. We modify our custom memory controller to implement the carefully-engineered valid DRAM command sequence~\cite{computedram} that is required to perform in-DRAM copy operations. This in-DRAM copy support is exposed to the programmer via pidlib over system calls. To overcome the mapping, alignment and granularity problems, we implement our memory management mechanism (Section~\ref{sec:memory-management}) in RISC-V PK. We expose our mechanism over the \emph{alloc\_align} system call, which enables programmers to allocate data in a way that satisfies in-DRAM copy mapping and alignment requirements. 


%\textbf{Initializer Rows Table (IRT).} The IRT holds physical addresses of the DRAM rows that are initialized with zeros. The IRT is indexed with physical page numbers. RowCopy-Initialize operations obtain the physical address of the DRAM row that is initialized with zeros which belongs in the same subarray as the operand of the initialization operation from the IRT. 

%\textbf{Allocation ID Table (AIT). }

\iffalse
We expose RowClone-Copy (RCC) and RowClone-Initialize (RCI) over two system calls: (i) \emph{RCC(src*,dest*,n)}


Figure~\ref{fig:memory-management-mechanism} shows the organization of RowClone-enabler structures and depicts how the memory management mechanism works over an end-to-end example for a 32 KB copy (i.e., we allocate two 32 KB arrays and copy one to another) operation.


\outline{How we implement RowClone on PiDRAM at a high level.}

We implement \emph{rcc} and \emph{rci} system calls which expose the RowClone-Copy and the RowClone-Initialize primitives to the programmer respectively. Table~\ref{table:pidlib-syscalls} shows the semantics of the three functions.

Figure~\ref{fig:rcc-e2e} depicts the end-to-end execution of a RowClone-Copy operation. First, the program calls the \emph{alloc\_align} function to allocate two 32 KB large arrays (A and B) that can be copied to each other using RowClone. Second, the program modifies array A by performing regular memory store operations. Third, the program calls the \emph{rcc} function to copy 32 KBs starting from A to B. Fourth, \emph{rcc} traverses the page table to find physical addresses that map to the virtual pages of A and B. Fifth, \emph{rcc} flushes all cache blocks that maps to the DRAM rows indicated by these physical addresses. Finally, \emph{rcc} performs the in-DRAM copy operations via the IDO controller.

Currently, RISC-V does not implement any cache maintenance operations. RISC-V memory model specification~\cite{X} states that the system designer is responsible for maintaining coherency. To maintain coherency, we implement a custom RISC-V cache flush instruction, CLFLUSH, in our infrastructure. CLFLUSH flushes a \textit{physically addressed} cache block. We modify the pipeline in the non blocking data cache and the \omi{R}ocket core modules (defined in \textit{NBDCache.scala} and \textit{rocket.scala} in \omi{R}ocket \omi{C}hip~\cite{X} respectively) to implement CLFLUSH, and we modify the RISC-V GNU compiler toolchain~\cite{X} to expose CLFLUSH as an instruction to C/C++ applications. We use the CLFLUSH instruction to flush dirty cache blocks that map to source rows and invalidate cache blocks that map to destination rows in RowClone operations.


\subsubsection{RowClone-Copy and RowClone-Initialize}
\todo{Pseudo-code for the algorithm}
\fi

\iffalse
Figure~\ref{fig:system-init-speedup} depicts the improvement in execution time for the same configurations (\emph{No Flush} and \emph{E2E}) for rci. 

\begin{figure}[h] %[!ht]
  \centering
  \includegraphics[width=.49\textwidth]{figures/system-init-speedup.pdf}
  \caption{}
  \label{fig:system-init-speedup}
\end{figure}


We make similar observations \juan{to those for \texttt{rcc} (Figure~\ref{fig:system-copy-speedup})}. 
First, rcc improves the execution time of initialization operations by 31.0$\times{}$ \textendash{} 11.5$\times{}$ for 8 KiB and 94.1$\times{}$ \textendash{} 15.5$\times{}$ for 8 MiB array sizes for E2E \textendash{} No Flush configurations \jgl{Swap}. 
Second, we observe a similar performance improvement behavior provided by rci and rcc routines, as array size increases. This observation is attributed to the additional pressure on caches brought by rci's accesses to the page table, and to the IRT (Section~\ref{X}). 
Third, rci under \emph{No Flush} configuration improves initialization performance by 5.2$\times{}$ (6.1$\times{}$,2.7$\times{}$) compared to the \emph{E2E} configuration on average (maximum,minimum) across all array sizes. 
Finally, we observe that the relative performance improvement provided by rci is smaller to rcc due to two reasons: (i) initialization operations require executing a single instruction (e.g., store) on the CPU to initialize data word by word, copy operations require two (one load, one store), resulting in rci's baseline to perform better than rcc's baseline (ii) rci routine has to access the IRT to find the DRAM row that is used as the source operand for the RowClone operation.
\jgl{The observations for both figures are very similar. You can eventually merge them if space needed.}
\fi


%First, the overhead due to the system calls (page table traversal, accessing SAMT) decreases with increasing array sizes. This overhead is the smallest as 18\% at 1MB array size. We find that this overhead is correlated with \todo{cache hit rates}, as the \emph{rci} and \emph{rcc} system calls access many RowClone-enabler structures along with the page table that can benefit from caching. Second, the overhead introduced by CLFLUSHs increase as array size increases, achieving a maximum overhead in execution time of 8.84$\times{}$ at 8 MB array size. \todo{similar explanation as the one for the behavior in Figure 1}

%\todo{Where are the bottlenecks? How long does it take to perform copy operation, how long it takes to flush the data. More insight into how these components affect the performance. Performance assuming everything in (i) cache, (ii) DRAM. }

\iffalse
\begin{figure}[!ht]
  \centering
  \includegraphics[width=.49\textwidth]{figures/system-flush-overhead.pdf}
  \caption{\todo{Change vert. axis label} Degradation in execution time introduced by system support and CLFLUSH instructions for RowClone. Baseline is in-DRAM copy. Y-axis shows the degradation in performance (increase in execution time) proportional to the baseline, X-axis shows the array sizes ranging from 8 KBs to 8 MBs.}
  \label{fig:system-xxxxxxxx}
\end{figure}
\fi

\iffalse
Figure~\ref{fig:perf-improvement}-c shows the improvement in execution time when using in-DRAM copy to initialize arrays over CPU-copy baseline with size ranging from 8 KBs to 8 MBs. We make two observations: (i) CPU-copy performs better than \emph{rci} for DRAM row-size (8 KB) initialization operations. We find that at 8KB initialization size, system call's contribution to the overall overhead of \emph{rci} is proportionally larger than that of it's contribution at larger array sizes. \todo{We observe that at 8KBs, accessing the page table and pidlib structures take more time because memory accesses to these structures miss in the D\$ (i.e. the structures are not warmed-up).} (ii) \emph{rci} improves execution time by 26.6\%. This improvement saturates as the array size increases.

\iffalse
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{figures/overall-performance.pdf}
  \caption{}
  \label{fig:overall-performance}
\end{figure}
\fi

\outline{Describe results.
\begin{itemize}
    \item RowClone overheads breakdown: syscalls, address translation, flush.
    \item Mapping \& alignment overheads: subarray characterization, copy align overhead.
    \item Other results: hardware area, memory overhead of sw components, verilog + chisel lines of code.
\end{itemize}}

\outline{Discuss the challenges in implementing RowClone.}

Figure~\ref{fig:rci-perf-breakdown} depicts the breakdown of execution time for the three key components of a RowClone operation: (i) system call, (ii) flush and (iii) in-DRAM copy, for array sizes ranging from 8 KB to 8 MB. We observe that CLFLUSH contributes most to the execution time at all array sizes, taking up to 80.8\% of execution time for an 8 MB in-DRAM copy operation.
% Stale from here on

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{figures/rci-perf-breakdown.pdf}
  \caption{Breakdown of execution time for key components of RowClone operations.}
  \label{fig:rci-perf-breakdown}
\end{figure}
\fi

