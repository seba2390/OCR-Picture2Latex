\section{{\TLA} Methodology}
\label{sec.method}


\begin{figure}
\centering
\includegraphics[width=.55\textwidth]{Figures/3la-abstract-flow.pdf}
\caption{
\textbf{\TLA methodology flow.}
\S\ref{sec:fragmentmapping}: the developer provides one 
%compiler IR-to-ILA rewrite
%\emph{IR-accelerator mapping rule}
\emph{\mapping}
  for each %high-level 
  accelerator operator,
  %based on their prototype design specified in ILA,
\S\ref{sec.method.flexible}: a compiler extended with \emph{flexible matching}
  automatically maps unmodified source application fragments 
  to accelerator calls, 
\S\ref{sec:cosimulation}: the ILAng platform generates fast simulators, enabling end-to-end co-simulation.
%linked into the accelerated application binary.
These features enable a mostly automated workflow for end-to-end testing of prototype accelerator designs on unmodified applications. 
  %The ILA specification can also be used to formally verify accelerator RTL implementations (as shown in prior work~\cite{huang2018instruction}). 
  %\sm{(Need to change 3.1 to 3.A etc.)}
  }
\label{fig.abstract-flow}
\Description{}
\end{figure}


%In this section,
We now describe technical details of the {\TLA} methodology,
 %for end-to-end application evaluation on prototype accelerators and review 
 along with illustrative examples
  that demonstrate its various components.
% and how the prototype presented in \S\ref{sec.prototype} handles them.
The important steps in the \TLA flow are shown in Fig.~\ref{fig.abstract-flow}, and described in the related subsections. 
Our examples and prototype (\S\ref{sec.prototype}) 
%in this section 
  pertain to deep learning,
  but 
  %our methodology is not specific to this %any single
  %domain. 
  the {\TLA} techniques based on a formal SW/HW interface and flexible matching are general and can apply to other domains.
  % where accelerators are used.}


%Let us 
We consider the following two examples: 
\begin{inlinelist}
\item an LSTM word language model (LSTM-WLM), a text generation application 
  consisting of an LSTM recurrent neural network 
  (RNN) followed by a linear 
  layer~\cite{pt2020wlm, graves2014towards}, and
\item ResNet-20, a widely used image classification model
  featuring 2D convolutions 
  and residual connections~\cite{he2016deep}.
  %(dataflow connections between earlier and later layers).
\end{inlinelist}
Since the LSTM RNN comprises most of the computation in 
  LSTM-WLM, it is desirable to accelerate this application 
  using FlexASR~\cite{tambe20219},
  a natural language processing accelerator
  that includes
  support for both LSTM RNNs and linear layers in hardware.
%While 
  Thus, FlexASR can also be used to accelerate the linear layers 
  in ResNet-20. In addition, ResNet-20 can also be accelerated with HLSCNN~\cite{whatmough201916nm},
  an accelerator for 2D convolutions exclusively;
  hence, the two accelerators can be used in concert. 
  %(In addition to FlexASR and HLSCNN, we also include VTA~\cite{moreau2019hardware} in our evaluations.)

  
  However, compiling DL applications like these
  from a high-level DSL %for deep learning applications
  (e.g., PyTorch for LSTM-WLM)
  to coarse-grained accelerators like FlexASR and HLSCNN
  poses several challenges highlighted below:
\begin{enumerate}
    \item \textbf{Specialized accelerator interfaces.} 
      These accelerators, 
        like many others,
        are invoked using MMIO instructions over AXI interfaces,
        to configure the accelerator's state and signal when to begin operations,
        thus requiring a thorough knowledge of 
        both the accelerator architecture and functions of the MMIO instructions. %to use effectively.
    \item \textbf{Granularity mismatch.}
      The compiler must relate
        %the fine-grained, low-level instructions required to configure and operate the accelerators with 
        the accelerator operations' coarse-grained semantics
        %provided by the DL operators 
        (e.g., an LSTM RNN) with the possibly fine-grained corresponding representations in the compiler IR.
        %incurring further engineering complexity.
    \item \textbf{Numerical representations.}
      Accelerators often use specialized numerical representations for improved performance or reduced hardware costs. For example, 
        FlexASR uses a custom format called AdaptivFloat~\cite{tambe2020algorithm}
        and HLSCNN uses a mixed 8/16-bit fixed-point representation.
      %Such numerical representations trade off numerical 
      %  accuracy (compared to 32-bit floating point) 
       % for improved performance or reduced hardware costs.
      %This requires verifying 
      %It must be verified 
        One must check that these data types
        do not cause inaccuracies at the full application
        level, %level of end-to-end applications,
        particularly when values are cast between data types.
\end{enumerate}
In the descriptions of various {\TLA} techniques in this section, we will emphasize how they address these challenges. 
%These parts and their connections are highlighted in Figure~\ref{fig.abstract-flow}.

\iffalse
%moving this subsection to the background section
\subsection{ILA Specification}

The principal component required in the {\TLA} methodology
  is an ILA~\cite{huang2018instruction} specification of the target accelerator,
  e.g., FlexASR and HLSCNN in this case.
  %which is, in this case, FlexASR.
The ILA models accelerator operations
  in terms of instructions with associated state transition functions,
  similarly to an ISA for a general-purpose programmable processor.
The ILA is an ISA-like formal model 
  for specifying the functional behavior of accelerators.
It generalizes the notion of instructions to accelerators 
  and provides a modular functional specification 
  as a set of instructions.
Like processor ISAs, 
  it does so by specifying 
  how each instruction updates software-visible (viz., architectural) state
  while abstracting out implementation details.

The ILA presents many advantages
  for the task of evaluating 
  full applications on accelerators.
Most pertinently,
  the ILAng toolchain~\cite{huang2019ilang}
  can automatically generate
  a functional simulator
  for a given ILA specification---%
  note that this can be done at any stage of development,
  since an ILA specification can be written
  even without a complete hardware design.
Additionally,
  ILA instructions correspond to accelerator operations
  at a low level
  and can be directly mapped to the MMIO interfaces
  that program the accelerators,
  meaning the same approach can also be used to run trials 
  on FPGAs or ASICs.
An instruction-like representation for accelerator operations
  also makes it possible to adapt instruction selection techniques
  to find opportunities to invoke accelerator operations
  (by outputting ILA instructions).
  %allowing for greater automation in
  %offloading portions of applications to devices
  %and therefore also for portability of applications.

We develop the ILA formal models for accelerators
  by following the methodology proposed in prior work~\cite{huang2018instruction}.
Each instruction of an accelerator ILA corresponds 
  to a command at the accelerator interface, i.e., an MMIO load or store command.
The ILA captures formal semantics of accelerator behavior by specifying how each instruction 
  reads or updates the architectural state variables.
Essentially, the ILA is a modular (per-instruction) operational specification of an accelerator.

Though writing the actual specification can be a significant one-time effort,
  the ILA provides several benefits for validating an accelerator design
  in the context of the {\TLA} methodology.
The specification can increase the level of confidence in the design's correctness
  because ILA specifications can be verified against RTL implementations,
  as has been done successfully in prior work~\cite{huang2018instruction}.
The formally defined semantics of the ILA
  also make it possible to automatically generate
  an executable software model for ILA instructions,
  which is implemented in
  the ILAng platform~\cite{huang2019ilang}.
Additionally, a revision to the accelerator design can easily be tested
  simply by making changes to the comparatively high-level ILA specification,
  which is much simpler than directly changing the hardware design;
  indeed, it is possible to write an ILA specification for a device 
  whose design has not yet been finished.
Also, since
 ILA instructions correspond one-to-one 
 with the individual MMIO commands that operate the accelerates,
 %that are used to operate accelerators,
 %which means that ILA instructions can be easily lowered
 ILA instructions can be directly lowered
 to MMIO commands for really running on a device,
 as we also perform in our evaluation in Section~\ref{sec.eval}.

Figure~\ref{fig.ila-example} in Appendix~\ref{app.fragments} provides an example
  of the ILA specification for one of FlexASR's operations.
The ILA specifications for FlexASR and HLSCNN 
  are about 5600 and 1600 lines of code, respectively;
  for comparison, their HLS implementations
  are about 9300 and 5100 lines of code, respectively.
\fi

\subsection{The ILA as a Formal SW/HW Interface in {\TLA}}
\label{sec:ila-compiling}
The first step in the {\TLA} methodology is to develop the ILA formal models for accelerators (\S\ref{sec:ila}). We follow the techniques proposed in prior work~\cite{huang2018instruction,huang2019ilang}, where each instruction of an accelerator ILA corresponds 
  to a command at the accelerator interface. Some instructions are simple instructions that configure the accelerator, while others may trigger complex operations, e.g., FlexASR's linear layer operator.
  
The ILA for accelerators serves as a formal software/hardware interface in {\TLA}, similar to the Instruction Set Architecture (ISA) for processors, and effectively drives the following tasks required for compilation and end-to-end application testing with accelerators.
%\begin{inlinelist}
\begin{itemize}
\item {\it Accelerator Operation Selection:} A formal instruction representation for accelerator operations enables adapting existing instruction selection techniques
  to identify acceleration opportunities and 
  output the ILA instructions that correspond to functionally equivalent parts of the compiler IR representation. 
\item {\it Code Generation:} The 
 ILA instructions correspond one-to-one 
 with the MMIO commands that operate the accelerator.
 %that are used to operate acceler˝˝Ï˝ators,
 %which means that ILA instructions can be easily lowered
 Thus, the selected ILA instructions can be directly lowered
 to MMIO commands for invoking accelerator operations from the application code running on a host processor.
\item {\it SW/HW Co-Simulation}: The ILAng toolchain~\cite{huang2019ilang}
  can automatically generate
  a functional simulator
  given an ILA specification---%
  this can be done in the early design stages, 
  even without an RTL implementation.
  %ILA instructions correspond to accelerator operations at a low level and 
  %meaning the same approach can also be used to run trials on FPGAs or ASICs.
  \end{itemize}
%\end{inlinelist}
Further, note that a revision to the accelerator design can easily be tested
  simply by making changes to the comparatively high-level ILA specification.
  %which is much simpler than directly changing the hardware design.
  %allowing for greater automation in
  %offloading portions of applications to devices
  %and therefore also for portability of applications.

%We develop the ILA formal models for accelerators by following the methodology proposed in prior work~\cite{huang2018instruction}.

%Essentially, the ILA is a modular (per-instruction) operational specification of an accelerator.

The ILA model size is about 10-20\% of the size of the RTL implementation~\cite{huang2018instruction}. Although writing the ILA model can be a significant one-time effort, it carries benefits
beyond the context of the {\TLA} methodology.
The ILA specification has been used for checking the accelerator RTL implementation,
  both using formal verification~\cite{huang2018instruction} 
  and simulation-based validation~\cite{huang2019ilang}.
Thus, ILA-based accelerator simulations in {\TLA} can be made sound with respect to RTL. %implementations. 

%\subsection{Compiler IR--ILA Program Fragment Mapping}
\subsection{Compiler IR-to-Accelerator Mapping}
\label{sec:fragmentmapping}

%% Aarti: please use the macro \mapping uniformly 
%The first step in the {\TLA} methodology is to develop the ILA formal models for accelerators (\S\ref{sec:ila}). We follow the techniques proposed in prior work~\cite{huang2018instruction,huang2019ilang}, where each instruction of an accelerator ILA corresponds to a command at the accelerator interface. Some instructions are simple instructions that configure the accelerator, while others may trigger complex operations, e.g., FlexASR's linear layer operator.

To support compilation to accelerators, 
%Next, in order to compile to  accelerator operators, %ILA instructions,
  we require some means of mapping from the compiler IR  
  %used to express the application down 
  to the ILA instructions that specify the accelerator operations.
%In the {\TLA} methodology,
  This is accomplished by specifying
  %\emph{program fragments}, namely, a fragment of the compiler IR and its corresponding equivalent sequence of ILA instructions (which serves as an imperative description of the accelerator operations).
  %\aarti{
  an \emph{\mapping rule} (``mapping,'' in short). 
  In general, this is a many-to-many mapping, 
  i.e., where a program fragment with many instructions in the compiler IR are rewritten to a program fragment with many ILA instructions on the accelerator side.
  %In general, such a rule has a program fragment on the compiler IR side, and a sequence of ILA instructions on the accelerator side that serves as an imperative specification of the accelerator operation. %}
\iffalse
% 6.2 data movement
(For simplicity, our current prototype also includes data movement
  in the ILA instruction sequences.
  In principle,
  it may be better to handle data movement separately in the future,
  as it would allow for optimizations like maintaining persistent state on-accelerator
  instead of moving data on and off an accelerator with each operation.)
\fi
%Each side can have many instructions, allowing \TLA to handle granularity mismatches between the two. 
%Thus, these mappings can be \textit{many-to-many}, 
%i.e., a sequence of many instructions in the compiler IR can map to a sequence of many ILA instructions on the accelerator side. This allows 
%allowing for great flexibility in what functionality can be captured by the ILA fragments.
This provides a general way to handle different granularities in compiler IR intrinsics (e.g., dot products and convolutions) and in accelerator operations (e.g., fine-grained operations in VTA~\cite{moreau2019hardware} and coarse-grained operations in HLSCNN, FlexASR). Furthermore, the ILA instructions in the mapping provide a verifiable abstraction of the hardware accelerator operation in terms of updates to software-visible architectural state. 

Writing the %compiler IR--ILA program fragments
\mapping rule
  is a one-time effort per accelerator operation 
  and is reusable across applications.
Further, 
  the mapping rule %fragments
  %enables validation of individual operators
  %by facilitating comparisons between the compiler IR's semantics on the host device and the ILA-based simulation results.
  can be validated by comparing the results of the compiler IR fragment on the host device and the ILA-based simulation results. 
  
%it enables operator-level comparisons 
%in addition to the application-level validation 
%\hl{(we should clarify the benefit of operator-level comparisons also, since we've been talking only about end-to-end testing)} 
%between
  %the compiler IR's semantics 
  %for the application code
  %and the ILA-based simulation results %from the auto-generated ILA-based simulator 
  %for validating the individual operator mappings.
  %of running the accelerator through ILAng.

\textbf{Examples.} 
In our prototype,
  we use the Relay IR~\cite{roesch2019relay}
  in the TVM DL compiler stack~\cite{chen2018tvm}
  as the compiler IR.
TVM supports importing models from other DL frameworks
  by converting them into Relay,
  thus allowing our prototype to support these front-ends as well.
%\sm{(Mention the front end support value of this first. The BYOC value should come next.)} 
Further, it enables 
  leveraging the Bring Your Own Codegen (BYOC)~\cite{chen2021byoc} library for code generation (discussed later).
  % Removed because the prototype section describes this
  %and because TVM supports importing DL models from other frameworks
  %by translating those frameworks' IRs into Relay.
For LSTM-WLM on FlexASR,
  we provide %program fragments 
  a mapping from an LSTM RNN
  (a large construct in Relay if ``unrolled'')
  to a short sequence of FlexASR ILA instructions, 
  and another mapping for a linear layer, which we illustrate in Fig.~\ref{fig.fragment-mapping}.
ResNet-20 also uses the same mapping for linear layer
  and a straightforward %fragment 
  mapping
  from a single 2D convolution operator 
  to a sequence of HLSCNN ILA instructions
  for performing a convolution.
  
\iffalse
Note that merely expressing a compiler IR--ILA program fragment
  already enables direct comparisons between
  the compiler IR's semantics for the application code
  and the simulated results of
  running the accelerator through ILAng,
  which we perform in Section~\ref{sec.eval} as well,
  though full-application simulation is a stronger form of validation.
\fi

\begin{figure}
  \centering
  %\includegraphics[width=0.4\textwidth]{Figures/program-fragment-mapping.pdf}
  \input{Floats/lst-flexasr-mapping}
  \caption{
    \textbf{An example \mapping for the FlexASR linear layer operation.} 
    The compiler IR fragment (a) is mapped to a sequence of FlexASR ILA instructions (b) that configure the accelerator states and trigger the computation.
    The ILA instructions correspond one-to-one to the accelerator's MMIO commands (c).
    This example illustrates how the ILA instructions are used in mapping rules and code generation.
  }
  \label{fig.fragment-mapping}
  \Description{}
\end{figure}

\subsection{Flexible Matching for Accelerator Operator Selection}
\label{sec.method.flexible}

Given compiler \mapping rules, %IR--ILA program fragment mappings,
  we can identify all potential 
  %uses of 
  offloads to an accelerator %in an application 
  by finding portions of an application
  that match the given compiler IR fragments, syntactically or semantically.
  % from the previous step.

\subsubsection{Difficulties due to syntactic matching}
%\subsubsection{Exact Matching}
Searching the application
  for exact syntactic matches
  for the given compiler IR fragments
  %in the application
  (referred to as ``exact matching'')
  is simple to implement 
  (e.g., this is done by the BYOC library in TVM~\cite{chen2021byoc}).
%\recheck{commented out BYOC}
%BYOC proceeds by making an annotation wherever it encounters an exact syntactic match for a pattern (in the Relay IR) that corresponds to a specified accelerator operation, eventually handing off compilation of annotated regions to user-provided custom code generators that can target an accelerator's interface (e.g., MMIO loads and stores).
%This approach is essentially a simple term-rewriting system (Section~\ref{sec.rewrite}).
%Given a set of syntactic rewrite rules ($\ell \rightarrow r$), a term-rewriting system rewrites instances of pattern $\ell$ in the input program with pattern $r$ where applicable~\cite{dershowitz1993taste,baader1999term,blindell2016instruction}.
%If all the rules preserve semantic equality, then the application of multiple rewrite rules is also correct by construction, allowing for modular correctness checking by verifying the individual rewrite rules.
\iffalse
\aarti{skip the next? not really important}
In principle,
  such a system would suffice for compiling the given examples:
  We could specify patterns in Relay 
  corresponding to 
  an unrolled LSTM RNN, a linear layer, and a 2D convolution,
  use BYOC to match those patterns,
  and have our custom code generator
  produce ILA instructions and lower them to MMIO instructions that invoke the accelerator.
\fi
%\hl{Maybe give an example of the patterns?}
%In fact, an earlier version of our prototype proceeded this way.
However, exact matching faces difficulties
  as there is often no canonical way
  to represent an operation,
  necessitating either the addition of more patterns
  or manual modifications to the input program
  to match the expected patterns.
Developing a canonicalization for each given IR may be possible,
  but would require careful design per IR and further effort to prove that the program transformations preserve the canonicalization~\cite{newcomb20-halide-verif}.
  %\mh{Shall we mention why canonicalization pass does not help in our cases?}
  %\sslyu{Yes, the trouble with canonicalization is that you need to define a format for every IR and prove that transformations preserve it, and it's really hard (see Julie's paper on Halide)}
Application code can vary greatly in structure,
  particularly in the case of compiler IRs,
  which may be produced after several iterations
  of program transformations
  (as with TVM, its model importers
  may translate equivalent expressions
  from various frameworks
  into different Relay expressions).


%Classic term-rewriting systems 
  %often suffer from the phase-ordering problem,
  %wherein applying rewrites destructively
  %in a fixed order
  %may preclude opportunities to apply more rewrites
  %and adversely affect final performance~\cite{whitfield1997approach}.

\textbf{Examples.} 
In our LSTM-WLM, the compiler IR pattern for a linear layer is (as an S-expression~\cite{mccarthy1960sexp}):
  %In our implementation of LSTM-WLM, we specify the compiler IR pattern for a linear layer (as an S-expression~\cite{mccarthy1960sexp}): 
% \instrInText{bias_add(nn_dense(\%a,\%b),\%c)}.
%\vspace*{-0.1in}
\[ \texttt{(bias\_add (nn\_dense \%a \%b) \%c)}. \]
%
%\begin{lstlisting}[language=lisp]
%(bias_add (nn_dense %a %b) %c)
%\end{lstlisting}
However, 
  in ResNet-20,
  which was imported from MxNet,
  linear layers are equivalently expressed as: 
% \instrInText{add(nn_dense(\%a,\%b),reshape(\%c,\%s))} 
%\vspace*{-0.1in}
\[ \texttt{(add (reshape (nn\_dense \%a \%b) \%s) \%c)} \]
%\begin{lstlisting}[language=lisp]
%(add (reshape (nn_dense %a %b) %s) %c)
%\end{lstlisting}
%\vspace*{-0.3in}

when \instrInText{\%c} is a vector, for certain shapes \instrInText{\%s}.
%
The former pattern would fail to match it,
  thus missing an opportunity to invoke FlexASR's linear layer operation.

%\paragraph{Leveraging Term Rewriting.} 
\subsubsection{Semantic matching via term rewriting} 
\label{sec:semantic}

%In this work, per \S\ref{sec.rewrite}, we explore using term rewriting for identifying acceleration opportunities, namely by employing \emph{compiler IR--ILA rewrites} \emph{\mapping rules} to map IR intrinsics to accelerator operations.
%~\cite{dershowitz1993taste,baader1999term,blindell2016instruction}.
 Rather than attempt to enumerate all semantically equivalent patterns %that may be encountered 
 (a task that is tedious, error-prone, and likely to result in an incomplete enumeration), 
 or expect users to modify their application code to expose expected patterns (demanding knowledge of the model and patterns
  as well as engineering effort), 
  %, engineering time, and debugging effort),
   \TLA  aims to maximize the degree of automation
  by utilizing term-rewriting and equality saturation techniques to transform programs
  \emph{to expose the most  matching opportunities for accelerator operation selection}. We call this process ``flexible matching'', and describe how it is specialized for accelerators. 
  %a much larger set of matching opportunities. 
  %\hl{AG: what is initial?}

%\subsubsection{\re{Design of flexible matching}}
\re{
%To specialize term rewriting and equality saturation techniques for accelerators, flexible matching in 
%{\TLA} uses two kinds of rewrite rules in flexible matching:
Flexible matching uses two kinds of rewrite rules:
\begin{itemize}
\item Compiler IR rewrite rules: These are general-purpose rules, independent of the accelerator, and are reusable and composable for various applications. We have developed a general set in \TLA %that is used across all our benchmarks. 
including rules for, e.g., merging/splitting tensors, commutativity, associativity, and identities for common operators. 
%\recheck{skip? These rules can also be extended by the system designer, e.g., in case of compiler IR updates, but in our experience they have largely stabilized and we expect they will be complete-in-practice for most users.}

\item \mapping rules: These rewrite rules are accelerator-specific. Recall (\S\ref{sec:fragmentmapping}) that these mappings are many-to-many, providing a general way to handle different granularities in compiler IR intrinsics and accelerator operations.
When targeting new accelerators, accelerator designers are expected to provide these mappings. For our evaluation, we created these mappings for the operations supported by the accelerators. 
\end{itemize}

All rewrites in {\TLA} are polymorphic over tensor size, which requires specifying relationships between the input and output sizes for operations that merge, split, or broadcast over tensors. This also makes a given \mapping more general and provides support for applications using different block sizes, strides, etc., without changing any rules. 
%Note that this separation provides many benefits for compiler IR rewrites: (a) they become a one-time-cost that can be shared across accelerators, (b) one can use off-the-shelf term rewriting systems for implementing them, and (c) these rewrites can use purely functional IRs, without requiring bespoke compilation steps for state/effect analysis. 

One benefit of separating the two kinds of rewrite rules in flexible matching is that this allows the compiler IR rewrites to use purely functional IRs, without requiring bespoke compilation steps for state/effect analysis during those rewrites, while stateful effects are limited to mappings, where they are formally specified by ILA instructions. 
Another benefit is  
  that mappings for multiple accelerators
  can be \emph{simultaneously} included, 
  %in the equality saturation,
  thereby searching over all opportunities 
  to invoke all available accelerators in concert.
}

In the extraction phase of equality saturation, the rewritten program optimizing the cost function is chosen. % as the final version of the program. 
This provides flexibility in the criteria for selection among functionally equivalent candidates for accelerator offloads. In our evaluations where we focused on end-to-end functional testing, we used a simple cost function that maximizes the number of accelerator invocations. More sophisticated cost functions can incorporate information about performance or data movement costs, and thereby result in different offloads.

%% Aarti: \iffalse around submitted text below
\iffalse
%\paragraph{Flexible Matching.} 
Specifically, we utilize equality saturation
  to perform ``\textit{flexible matching}'':
  The search applies \mapping rules %IR--ILA rewrites 
  to insert accelerator operations into the program,
  as well as equivalent rewrites within the compiler IR (compiler IR--compiler IR rewrites)
  to expose even more acceleration opportunities. % to apply accelerator operations.
  \iffalse
Equality saturation avoids
  phase-ordering issues when applying rewrites
  by searching over many equivalent rewritings of the same program,
  including different choices of accelerator operations,
  which ultimately reduces the need for manual program restructuring
  and improves application portability.
Given an input program $p$, 
  equality saturation repeatedly applies 
  the given rewrite rules 
  to explore all equivalent ways to express $p$
  (with respect to the rewrite rules).
This is accomplished using an \textit{e-graph} data structure
  to efficiently represent an exponentially large set of equivalent program expressions~\cite{nelson1980fast,nieuwenhuis2005proof}.
Upon reaching a fixed point, 
  i.e., when no application of any rewrite rule can introduce a new program expression, 
  the optimal rewritten program
  can be extracted from an e-graph
  according to a given cost function,
  thus providing for searching over many candidate rewritings without sophisticated ordering considerations.
\fi
%The use of equality saturation
  %also exposes opportunities for optimizations,
Since all rewritings with respect to the given rules 
  are available for the search,
  choosing an optimal rewriting
  reduces to framing the right cost function and search heuristics.
This allows %for automatically testing 
the use of \textit{unmodified} off-the-shelf applications
  and exploring all possible mappings to the accelerator operations (as demonstrated in our evaluation, \S\ref{sec.compilation-stats})---%
  a very useful capability during early design exploration. 
  
  %---requiring only a \textit{one-time} effort per accelerator
  %in the form of providing the compiler IR--accelerator rewrite rules.
%,
 % requires a \textit{one-time} effort per accelerator (the addition of compiler IR--accelerator rewrite rules),
  %and allows for exploring all possible mappings to the accelerator---%
  %a useful capability during early design exploration.
\fi 

\iffalse
Two additional advantages of our flexible-matching approach
  are that the general-purpose rewrite rules
  can be used with \emph{all} target accelerators and applications,
  %and thus require only a one-time effort to specify
  and that rewrite rules for multiple accelerators
  can be \emph{simultaneously} included, 
  %in the equality saturation,
  thereby searching over all opportunities 
  to invoke all available accelerators in concert.
In the extraction phase of equality saturation,
  the rewritten version maximizing the cost function
  is chosen as the final version of the program.
  This provides flexibility in the criteria for selection among functionally equivalent candidates for accelerator offloads.
  \fi 
%, by designing a suitable cost function.
%Because this approach often exposes more opportunities to invoke accelerators (\S\ref{sec.compilation-stats}), we refer to this approach as ``flexible matching,'' in contrast to ``exact.''
  %\aarti{add a forward pointer to flexible matching results}


\textbf{Examples.}
In our prototype,
  we compile programs in Relay
  into another IR called Glenside~\cite{smith2021pure}, 
  which uses the \egg library~\cite{willsey2021egg}
  to implement equality saturation
  for tensor programs.
  %facilitates efficient implementation of equality saturation
  %via the \egg library~\cite{willsey2021egg}.
%We can 
In addition to matching the compiler \mapping %IR--ILA fragments
  by adding them as rewrite rules in Glenside,
  this approach also facilitates \emph{additional} matches
  through the inclusion of \textit{general-purpose rewrite rules}
  within Glenside,
  %often consisting of 
  such as tensor shape transformations and algebraic manipulations of combinators.
  %including algebraic manipulations of various combinators in the language. 
%  \aarti{mention shape transformations here?}
These general-purpose rules
  allow the term-rewriting system to conclude
  that different variations of an expression
  (like the linear layer examples above)
  are, in fact, equivalent.
In our examples, using the rewrite rules in Glenside
  exposed acceleration opportunities in both
  the LSTM-WLM and ResNet-20 programs
  through specifying \textit{only a single}
  %compiler IR--ILA rewrite 
  \mapping rule for each accelerator operator.
  %we are able to annotate desirable accelerator operations in
  %both the LSTM-WLM and ResNet-20 implementations,
  %specifying only a single compiler IR--ILA rewrite for each accelerator operation
  %and relying on Glenside's general-purpose rewrite rules 
  %to expose opportunities to invoke the former.

%Note that a simple cost function of maximizing accelerator invocations will not produce performant code in all cases due to different per-operator performance gains, the incurred communication overhead, etc.
  %as offloading operations onto an accelerator
  %can incur communication overhead.
  %so some potential invocations are less efficient than simply using the host processor.
\iffalse
Since the focus of this work
  is on evaluating the
  \textit{functional correctness}
  of accelerator designs on
  end-to-end applications,
  we did not investigate
  cost functions that
  would be more predictive
  of real-world performance.
Such cost functions would have to
  take into account
  timing information for operations
  in both the host and the accelerators,
  and also %take into account
  the possibility
  of overlapping communication
  with other computations.
\fi
%In the future, we could obtain more precise performance models for the accelerators. % from the performance information. 
%Incorporating the performance models into the cost function would enable flexible matching to also serve as an optimizing compiler to accelerators by extracting the optimal backend assignment---%
  %and would hence be an interesting direction for future work.
%\aarti{Refine the above wrt our current/future performance modeling}

%In our evaluations for end-to-end functional validation, we use the number of accelerator invocations as the cost function in flexible matching. In the future, we plan to consider more sophisticated cost functions that can take into account data communication or other costs, as briefly described next.

It was not clear \textit{a priori} whether flexible matching would be performant for accelerators with complex \mapping rules needed for available accelerator designs. Our evaluation results (\S\ref{sec.eval}) show that powerful compiler IR rewrites can be combined effectively with a few \mapping rewrites in flexible matching, which finds more matches than exact matching in reasonable time. 


\subsection{\TLA support for additional optimizations}
\label{sec:optimization}
The following \TLA design features provide support for additional optimizations:
%\begin{itemize}
\begin{inlinelist}
    \item 
\mapping: 
%Due to  the availability of several coarse-grained accelerator operations (e.g., Conv2D and LSTM), 
Recall that 
%we parameterize these mappings with arguments such as the sizes of the input/output data. 
our mappings are polymorphic over tensor size, i.e., we parameterize these mappings with arguments such as sizes of the input/output data.
On the accelerator side, the maximum sizes supported by a single accelerator offload are limited by the accelerator-controlled hardware resources (e.g., buffer sizes, memory layout, internal/external memory, etc.). 
    \item
Flexible matching: Our technique provides optimization capabilities by including performance or other criteria in the cost function, which is optimized for instruction selection. 
%In our current evaluations (\S\ref{sec.eval}), where we focus on end-to-end functional testing, we used a simple cost function that maximizes the number of accelerator invocations. More sophisticated cost functions can incorporate information about performance or data movement costs, and thereby result in optimized offloads.
\end{inlinelist}
%\end{itemize}

In ongoing \TLA work, these features have supported the design of a ``scheduler'' that decomposes applications with oversized layers to accelerator operations; i.e., it determines loop orders, tile sizes, etc., based on a specification of the accelerator-controlled memory/hardware resources. For our current set of accelerators, our scheduler generates a guaranteed optimal schedule with the least data movement between the host and accelerator for mapping single layers in an application. It includes several novel techniques to prune the large search space (beyond the scope of this paper). Importantly, it is fast enough (a few seconds in our experiments) to be used for cost estimation during flexible matching.

In future work, we plan to integrate this scheduler with flexible matching, thereby enabling optimization of data movement %in selection of the accelerator offloads, 
or for considering possible tradeoffs with other costs. 
%After a particular schedule has been selected, the code generator in D2A uses it to generate code for data movement, along with code for the selected accelerator operation. 
Another optimization opportunity we have identified is in removing redundant intermediate data transfers in back-to-back offloads to accelerator operations. This can be done in a pass after flexible matching and before code generation.

\subsection{Co-Simulation for Application-Level Results}
\label{sec:cosimulation}

%With the compiler IR fragments corresponding to ILA instruction sequences marked,
  %we can use an approach of co-simulation
  After the acceleration operation selection is done and specific portions of the application are marked as offloaded to an accelerator, 
  we can co-simulate the results at the full-application level  
  %to determine the results of applying an accelerator to a full application,
  rather than for only individual operators.
Namely, the portions of the applications that are not marked
  are directly executed on the host 
  (generally a CPU)
  and the marked portions converted into their corresponding ILA instruction sequences
  are 
  simulated
  %emulated \aarti{QEMU?, emulated is confusing} 
  via an ILAng-generated simulator.
\iffalse
Note that the ILAng-generated simulators also accept custom software libraries, which allows us to use \textit{software implementations of custom numerical data types} in simulation, thus faithfully simulating the semantics of the custom numerics.
  %thus ensuring that the simulated results faithfully 
  %reflect the semantics of the custom numerics.
  %validating these design choices in the context of a full application.
(Though these libraries must be trusted
  or be separately verified against the RTL.)
\byhuang{Accepting custom libraries is not the enabler of simulating numerics, as numerical semantics can already be modeled in ILA.}
\fi
Note that the ILAng-generated simulators faithfully simulate the custom numerics, either using semantics formally modeled in the ILA specification or by accepting trusted software libraries that implement custom numerical data types.


\textbf{Examples.} %\aarti{was an issue found in either of these two examples?}
In \S\ref{sec.eval}, we examine the application of the \TLA methodology on several DL applications.
%\hl{We can cut if we feel it's unnecessary but I would say it gives the punchline to our entire methodology.}
In the process,
  upon identifying a numerical accuracy issue with HLSCNN in ResNet-20 and MobileNet-V2,
  we rapidly explored revisions to the design
  by changing the ILA specification---%
  a much simpler task than modifying the RTL.
 % This emphasizes that our methodology provides for a \textit{software-like debugging feedback loop of executing an application, inspecting the results, and rapidly making design adjustments before testing again}.

\iffalse
In the case of LSTM-WLM on FlexASR,
  our end-to-end application level results
  closely matched those of running the original application in PyTorch
  on the host device;
  this was the first time FlexASR had been run on an entire application
  and provided some validation of its design and custom numerical data type.
With ResNet-20,
  the application-level results diverged significantly,
  with the simulated version losing much accuracy compared to the original application.
Fortunately,
  our co-simulation framework
  allowed us to easily instrument the invocations
  of the ILAng-generated simulators,
  %and examine intermediate execution results,
  which led us to determine that
  the accuracy loss stemmed from
  the loss of dynamic range
  due to HLSCNN's custom datatype.
When we reran the trial with a revised ILA specification
  for HLSCNN that used a 16-bit fixed point datatype
  with a different binary point position,
  the accuracy matched the original application's.
This required only a small change in the ILA specification,
  but making similar corrections in an RTL implementation
  would be a much larger undertaking,
  thus emphasizing the importance of doing such evaluation
  at early stages of development.
The case of ResNet-20
  illustrates that our methodology
  provides for \textit{a software-like debugging feedback loop},
  where we were able to test the accelerator's performance on a real application,
  inspect the results,
  quickly make adjustments to the ILA specification,
  and proceed with the the revised version.
\fi
%In this section, we describe the {\TLA} methodology for compiling DSL programs down to accelerator-rich platforms. 
%The methodology provides for portability of applications through retargetability to different accelerator platforms, and verifiable compilation.
%
%In this section, we explain three key aspects of the \TLA methodology:
%\begin{inlinelist}
  %\item adding accelerator support by specifying mappings between compiler IR intrinsics and accelerator operations,
  %\item compiling applications by searching within input programs for computations supported by accelerators, and
  %\item ensuring compilation-result validation and supporting early-stage software/hardware co-design.
%\end{inlinelist}
%
%In the sections below, we discuss in detail these three major aspects of the {\TLA} methodology.


%Adding compiler support for a new accelerator requires specifying mappings between compiler IR intrinsics and the accelerator operations such that they are functionally equivalent.
%
%In {\TLA}, we provide an \emph{IR-accelerator mapping} for each operation supported by the accelerator using the ILA formal.
%
%The ILAs capture formal semantics of both the compiler IR and the accelerator in the form of instructions --- providing a uniform model on both sides.
%
%This allows us to specify the mapping with a \emph{pair of program fragments}, where a program fragment is a \emph{sequence of ILA instructions} that performs a certain function. 
%
%Figure~\ref{fig.mapping} shows an IR-accelerator mapping example that maps a linear layer operation from a compiler IR pattern (part~(a)) to accelerator invocation MMIO commands (part~(d)).
%
%The pair of ILA program fragments for the compiler IR and the accelerator is shown in part~(b) and~(c), respectively.


%Once the IR-accelerator mappings are specified, compiling an application reduces to finding computations in the input program and replace them with the corresponding accelerator operations. 
%
%As depicted in Figure~\ref{fig.3la-flow}, we first translate the application into the compiler IR and then apply equality saturation (a term rewriting technique) with the rewrite rules derived from the IR-accelerator mappings.
%
%Besides searching for computations that exactly match the IR patterns specified in the IR-accelerator mappings, we incorporate general-purpose rewrite rules that transform input programs to expose potential matches that are syntactically different but semantically equivalent.
%
%We refer to this as \emph{flexible matching}.
% 
%The final rewritten program is used for code generation, where each accelerator instruction is subsequently lowered to the corresponding MMIO load/store for invoking the accelerator.


%The formal semantics of ILA provide the foundation for verifiable compilation.
%
%In {\TLA}, we check compilation correctness by verifying the IR-accelerator mappings.
%
%This consists of three verification tasks (VT1, VT2, and VT3), that check the different parts of the mapping, as illustrated in Figure~\ref{fig.mapping}.
%
%Further, the {\TLA} methodology provides fully automated end-to-end application-level co-simulation --- an essential capability in enabling software/hardware co-design at an early-stage.

% In the sections below, we discuss in detail these three major aspects of the {\TLA} methodology.
%\begin{enumerate}
%  \item Specifying IR-accelerator mappings using ILAs, 
%  \item Flexible matching for program compilation, and
%  \item Checking end-to-end compilation correctness.
%\end{enumerate}


%\input{sec-method-mapping.tex}
%\input{sec-method-rewrite.tex}
%\input{sec-method-verif.tex}


% \subsection{Summary of the {\TLA} Methodology}

% Early-stage co-design is not possible in API calls.
% no framework for end-to-end application-level co-simulation available --> often missed or ignored by the current community