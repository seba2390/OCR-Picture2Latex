%\subsection{End-to-End Testing}
\subsection{Application-Level Validation Through Co-Simulation}
\label{sec.end-to-end}

%Having validated the IR-accelerator mappings, we 
%We next explore how the small numerical differences at the level of individual operators affect the results of evaluating full applications.
%One main goal of the {\TLA} methodology is to
  %allows us to 
  %explore this through 
We  performed application-level co-simulation
  %namely 
  by using the
  ILAng-generated simulators for accelerator computations
  and the host CPU for the rest of the computation.
%want to check if minor deviations at the single operation level will influence application-level behavior.
%
%Therefore, we performed application-level co-simulation on several applications which offload various computations to FlexASR and HLSCNN, the two accelerators that utilize custom numerics. (By co-simulation we mean simulating the software running on the host processor along with the operations off-loaded on the accelerator using the ILA simulator.)
%
%Specifically, we examined models that we were able to train and deploy on \hl{all of the devices (SM: unclear)} supported in our prototype:
We considered three applications,
  which between them provide opportunities to use
  each of the three accelerators: %,
  %as well as even combinations of them:
\begin{inlinelist}
  \item LSTM-WLM, where we accelerate linear layer and LSTM operations on FlexASR;
  \item ResNet-20, where we accelerate convolutions on HLSCNN and linear layers on FlexASR; and
  \item MobileNet-V2, where we accelerate convolutions and linear layers as in ResNet-20 and additionally accelerate both these operations on VTA (due to the \texttt{im2col} rewrites).
\end{inlinelist}
In ResNet-20 and MobileNet-V2,
  we were able to
  \emph{explore using HLSCNN and FlexASR together and separately}, simply by varying which 
  %IR-accelerator rewrites 
  {\mapping}s
  we included in flexible matching.

%Each of the three accelerators considered is used in some application, and even combinations of these accelerators are used in individual applications:
%\begin{inlinelist}
  %\item LSTM-WLM, whose linear layer and LSTM operations are supported by FlexASR;
  %\item \hl{ResMLP, whose linear layers are also supported by VTA}; and
  %\item ResNet-20 and MobileNet-V2, whose convolutions are supported by HLSCNN and whose linear layers are also supported by FlexASR, \emph{allowing us to explore using the accelerators together and separately}, simply by varying which IR-accelerator rewrites we include in the flexible matching step.
%\end{inlinelist}
%LSTM-WLM, which offload to FlexASR the LSTM layer and linear layer operations, respectively.
%
%We also considered MobileNet and ResNet, which both offload 2D convolution and linear layer operations to HLSCNN and FlexASR, respectively.
%
%(We compiled to two target accelerators by including the IR-accelerator rewrite rules for both accelerators.)
%
% As a sanity check \hl{(Was it a sanity check? It seems like an interesting case in its own right. -- Safety net in case we don't have a fully-matched VTA e2e in time so that we can drop it.)}, we also evaluated a quantized MobileNet on VTA, comparing application results under \hl{integer numerics, which have no rounding error}.


% Although the IR-accelerator mappings simulation shows acceptable relative errors, we don't know the mapping errors on a specific trained model or how these errors will accumulate across the layers and more importantly, how the numerical difference of the outputs will hurt the application-level results, such as the inference accuracy. Therefore, it is important to perform an application-level co-simulation to measure the correctness of the program running on the heterogeneous platform. This can be done by the compilation flow of the \TLA methodology which generates instructions sequences of the accelerator calls to run on the ILAng-generated software models.
% We pick the applications shown in the Table~\ref{tab.verif-sim} from the \AppNum compilation examples in Table~\ref{tab.compilation} to perform an application-level co-simulation. 
% These selected applications are relatively time-efficient for training and simulation, and some of them can utilize more than one accelerators for computation.


We trained and validated the LSTM-WLM model using the WikiText-2 dataset~\cite{merity2016pointer}.
%
The image classification models (MobileNet-V2 and ResNet-20) were trained and validated using the CIFAR-10 dataset~\cite{cifar10}.

Table~\ref{tab.verif-sim} shows the application-level co-simulation results.
% Our quantization approach: Uniform quantization (calculated based on min and max value),
% we compute the scaling factor on the fly:
% we check the min and max values at run time
% and we compute the floating point reference value at run time to get a scaling factor for requantization
% Why is this morally defensible?
% Because in principle, we can get the scaling factor through a calibration run. We didn't bother doing that to simplify engineering tasks for ourselves. This may affect the results, but not in a huge way because, at the end of the day, this is not a paper about quantization
% \cite{jacob2017quantization}
%
%Columns~1 and~2 describe the application and the target processing platform under evaluation, respectively.
%
%We provide a reference result (perplexity for the text-generation task and inference accuracy for vision tasks) in Column~3 by running the application on the host processor, i.e., not offloading to accelerators.
%
%The application-level co-simulation 
%The validation result using original accelerator designs, labeled ``Original Result,'' is provided in Column~4.
%
%For cases where the ``Original Result'' was significantly poorer than the reference result, we reported it to the accelerator developers for their further investigation. 
%When they provided an accelerator design modification to address this, we provide an updated result, using this modified accelerator, in Column~5. 
%
%The average simulation time is reported in Column~6.
%
For LSTM-WLM,
  the application-level results using the accelerators
  did not differ greatly
  from the reference results.
In the case of FlexASR,
  this was the \textit{first time}
  it had been run end-to-end on a full application---%
  this provided validation for its AdaptivFloat data type.
For VTA on MobileNet-V2,
  there was a small decrease in accuracy
  that may be attributed
  to quantization error.%
\footnote{We apply a form of uniform quantization~\cite{jacob2017quantization},
  which involves scaling the results
  based on the floating point reference results.}
%Typical implementations
  %involve pre-computing a scaling factor
  %using a floating point calibration run
  %on a representative dataset.
%To simplify our runtime,
  %our implementation
  %dynamically computes the scaling factor
  %by performing the floating point computation
  %on the current input
  %rather than using a calibration run.
%This approach is very inefficient
  %and may have different results 
  %compared to fixing a single scaling factor,
  %but it is presented only as a proof of concept,
  %since quantization is not the focus of this work.}

\input{Floats/tab-verif-e2e}

However, the initial results for ResNet-20 and MobileNet-V2
  using HLSCNN
  revealed a large loss in accuracy.
%We were able to determine the root cause through subsequent experimentation:
We noticed that the linear layers 
  accelerated by FlexASR 
  did not impact the final accuracy,
  suggesting the issue stemmed from HLSCNN
  (for which this was also the first time it was run in an end-to-end application).
We then instrumented
  our {\TLA} prototype 
  %BYOC runtime 
  to record additional information
  for each accelerator invocation,
  such as input and output ranges.
%With this information,
This helped 
  the accelerator developers
  %were able to 
  determine that the loss of accuracy
  was due to a lack of dynamic range in the data type:
  weight data values 
  in HLSCNN's 2D convolutional layers
  were heavily quantized
  due to the narrow value range
  of their 8-bit fixed point representation.
%After we updated the ILA specification (an easier task than updating an RTL implementation) to use a 16-bit fixed point data type for weights with an adjusted binary point position (no longer using any 8-bit values)\mh{Do we need to explicitly say how much work was done?}, the applications recovered their accuracy.
After we updated the ILA specification (a much easier task than modifying the RTL implementation) based on the developers' suggestion to expand the fixed point representation to 16 bits and adjust the binary points in inputs' and accumulators' fixed point data types, the %full application 
accuracy recovered.
  This case study readily demonstrates
  %thus also emphasizing
  how the {\TLA} methodology
  \textit{facilitates debugging and improving accelerator designs
  with rapid turnaround.}

%We reported the original validation accuracy of ResNet-20, 
% 29.15\% accuracy, 
%which was much worse from the 91.55\% reference result, to the accelerator developers. 
%
%We also provided statistics for each accelerator invocation (e.g., error accumulation, input and output ranges, etc.), gathered by our compiler prototype and ILA simulators.
%
%With the information, the accelerator developers were able to identify the root cause: weight data values in HLSCNN's 2D convolutional layers were heavily quantized by its 8-bit fixed point data type due to a narrower value range.
%
%After updating the design by expanding the original 8-bit representation to 16 bits, the application-level result matched up to the reference result.
%
%The same tuning approach also resulted in improved accuracy for MobileNet-V2.


% In the ResNet-20 case study, we only achieve 29.15\% of inference accuracy initially with computation using the original configurations of custom numerical representation on the accelerators. This inference accuracy is far from the 91.55\% reference accuracy running on the host processor. To investigate the issue, we extracted the intermediate results of each layers offloaded to the accelerators and compared them to the reference results of the same layer to investigate how errors are accumulated across different offloading layers. In the per-layer analysis, we found out that several 2-D convolutional layers on HLSCNN shows greater relative errors, which is due to a significant narrower range of the weight data values that are heavily quantized by the 8-bit fixed point representation. As a result, we expand the original 8-bit representation into 16 bits, providing a finer-grain quantization to fit the data, and the final inference accuracy is improved to 91.85\%, very close to the reference accuracy. Similar tuning on the accelerator implementation also happens in our MobileNet case study, in which the initial accuracy is only 10.35\% compared with a 92.40\% reference. However, only expanding the bitwidth for weight data is not enough for this application. We also reassigned the bits in the 16-bit fixed-point representation for the activations in HLSCNN to better match its data distribution in the application. The final accuracy is improved to 91.20\% for MobileNet-V2 after the further tuning of the accelerator. 
% \hl{Discussion on LSTM and qnn-mobilenet.} 

The overall results in Table~\ref{tab.verif-sim} reaffirm the need for application-level validation, especially for accelerators utilizing custom numerics.
%
%However, without an end-to-end compilation flow, such application-level validation is prohibitively difficult for new accelerators.
%
%Our experiments demonstrate how \TLA provides systematic and largely automatic compilation-results validation at the application level and also show its usefulness in software/hardware co-design.
%
%Specifically, due to 
Thanks to formal ILA models, \TLA provides quick design space exploration and numerics tuning without hardware engineering overhead %(e.g., deploying to FPGA) 
in each design iteration.
%
%A repetitive
%Other than the accelerator ILAs and the compiler IR--ILA patterns (a one-time-per-accelerator effort independent of the application), integration into the compiler required only a small number of Glenside rewrite rules and a small code generator mapping matched patterns to ILA instructions.
%
Further, it provides handy debugging information and efficient simulation---%
%
for FlexASR, the ILA simulator yields a 30$\times$ speedup on average compared to RTL simulation.
%using a 
% state-of-the-art 
%commercial Verilog simulator.
%Synopsys VCS~\cite{vcs}.)

% These case studies shows our \TLA's great advantage in software/hardware co-design. The portability of \TLA enabling us to automatically support application level co-simulation on a variety of applications by using ILA as a formal software/hardware interface. By contrast, similar application-level co-simulation with accelerator APIs would require significant amount of human efforts to map each DSL application to the accelerator calls. 
% This co-simulation could provide valuable feedback for software/hardware co-design. For example, accelerators' numerical representation settings may need to be tuned to adapt to different applications to provide an reasonable accuracy with its power-performance benefits, as it is shown in our case studies. Moreover, running application-level simulation would help discover issues in the early-stage hardware design, which could possibly be hidden in the traditional per-instruction/layer validation method. On the other hand, software designers could tweak their applications based on the feedback of the results using the accelerators, such as running accelerator specific quantization-aware training or tuning the parameters in the models, to improve the target metrics.

% In the meantime, the automatically-generated software models from the hardware accelerator ILA models enable efficient co-simulation for the early stage software-hardeware co-design. As a comparison, the measured time of the generated FlexASR ILA simulation is about 10$\times$ faster than its High-level Synthesis (HLS) model simulation (using Catapult HLS \cite{catapult}) and has on average 30$\times$ speed-up against the RTL simulation (using Synopsys VCS \cite{vcs}). 
% The total simulation time of the case studies shown in the table can be easily reduced down to a few hours with a distributed computing setup. For example, in the ResNet-20 experiment, the simulation time is reduced down to 4 hours using 125 same cpu cores. 
% Moreover, the generated ILA simulation models can serve as functional model for early-stage design and validation when the HLS or RTL models of the accelerators are not ready.
% this cannot be done in previous approaches, due to the lack of an interface that is abstract while capturing formal semantics
% re-iterate the RTL simulation is slow and limits early stage codesign

%\hl{Gus:
%  We should add an asterisk
%  on VTA/Mobilenet
%  to say that we ran
%  a quantization pass
%  in order to be able to map
%  mobilenet to VTA.
%This is in lieu of
%  having a separate Q-Mobilenet
%  column
%  in the flexmatching table.}