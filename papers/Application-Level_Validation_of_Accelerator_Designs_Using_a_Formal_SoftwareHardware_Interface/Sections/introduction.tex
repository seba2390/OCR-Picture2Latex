\section{Introduction}
\label{sec.intro}

%\steven{Per meeting between Akash and me, perhaps we should reframe this more as "some approaches have solved one of these issues at a time, but not all of them simultaneously" as opposed to just describing the problem and how we solve it}

Hardware specialization 
  is the main technique 
  for improving power-performance efficiency in emerging compute platforms.
By customizing 
  compute engines, memory hierarchies, and data representations~\cite{chan2014itrs,fang2019understanding,lai2021programming},
  hardware accelerators provide 
  efficient computation in various application domains 
  like artificial intelligence, image processing, and graph analysis~\cite{han2016eie,chen2016eyeriss,reagen2016minerva,zhang2016cambricon,hameed2010understanding,ham2016graphicionado}.
%The demand for hardware acceleration
%  has created a corresponding 
%  need for \textit{software/hardware co-design},
%  the capability to design accelerators
%  that can closely integrate with
%  the needs of specific software applications.
%\hl{Despite progress
  %in tools for designing domain-specific accelerators,
  %it remains stubbornly difficult to 
  %compile high-level applications to
  %new devices,
  %making it especially difficult
  %to determine the impact of accelerators on full applications.
%}
However,
  despite significant recent progress in 
  design languages and tools for
  custom accelerators~\cite{nigam2021calyx, lai2019heterocl},
  many difficulties remain in
  developing domain-specific accelerators.

A particularly challenging aspect of accelerator development
  is validating early design prototypes
  on real applications.
Such validation is critical,
  as errors can arise from
  some of the techniques used
  to achieve maximum power-performance efficiency
  in accelerators,
  such as the use of custom numeric representations
  or reformulated operators.
  %as errors can arise from differences 
  %in precise operator definitions, including numeric representations, between high-level application code and accelerator operations.
  %between high-level application code
  %and accelerator operations,
  %which reformulate operators
  %or use custom numeric representations for
  %maximal power-performance efficiency.
\iffalse % the flow seems smoother without these three sentences here
This is unfortunate as
  end-to-end testing reveals aberrant behaviors
  from accumulated errors that per-operator unit tests miss and
  catches accelerator design issues early,
  saving substantial effort for downstream engineering tasks.
Several aspects of accelerator development
  frustrate early end-to-end testing efforts.
Testing an accelerator design requires both
  specialized compiler support to
  find and offload workload operations to the accelerator
  and also
  specialized simulator support to
  quickly and cheaply evaluate the prototype design.
Developing such specialized support is often challenging.
\fi
%These numeric representations are used to achieve maximal power-performance efficiency. 
%accelerators often feature custom data representations:
In domains like deep learning (DL), signal processing or graphics,
  an application-level result
  (like a DL-based classification)
  can remain within acceptable range
  even if the numerical results
  of individual operations change slightly,
  presenting an opportunity to trade numerical accuracy
  for efficiency.
  %-- thus significant power-performance benefits can be gained by using custom numeric representations without sacrificing application-level accuracy.
%\aarti{add other domains?}
However, these changes need to be 
  carefully validated 
  at the application level---%
  even small changes in numerical accuracy
  of individual operators
  have the potential to cascade throughout an application,
  making the application-level results unacceptable~\cite{zorn2021rounding}.
  %when a custom numeric representation has a seemingly small impact on the numerical accuracy of individual operations, 
  %these effects can sometimes cascade through an application, making the application-level results unacceptable~\cite{zorn2021rounding}.
  %and nevertheless \textit{may still} affect application-level results, necessitating further analysis~\cite{zorn2021rounding}.
  %Accelerators often make use of custom datatypes,
  %on the reasoning that a loss of numerical accuracy
  %may not affect the application-level result
  %(like a DL model classification),
  %but this nevertheless requires analysis to ensure that
  %the effects of truncation and rounding 
  %do not cascade throughout the application~\cite{zorn2021rounding}.
%Thus, while RTL simulation tools allow for conveniently testing individual accelerator operations against a reference implementation, this form of validation may \textit{not} be informative as to the device's behavior in a full end-to-end application.
%Unfortunately,
  %in many domains,
  %like deep learning (DL),
  %evaluating only individual device operations
  %or portions of applications
  %is \textit{not} informative as to the device's behavior
  %in a full end-to-end application.
% Considering the complexity and expense
%   of revising hardware designs at later design stages,
%   end-to-end application-level validation is essential
%   for identifying such issues at early stages of development.
% Since late stage hardware design changes are expensive and complex,
%   early end-to-end application level validation is essential.
Early end-to-end application level validation is essential
  for avoiding expensive and complex late stage hardware design changes.
\iffalse
\re{
Even after an accelerator prototype design is available, testing it with an application for end-to-end validation requires both
  \emph{specialized compiler support} to
  find and offload workload operations to the accelerator,
  and 
  \emph{specialized high-level simulator support} to
  quickly and cheaply evaluate the prototype design -- RTL simulation is too slow for this purpose.
  Our work addresses the challenges in developing such support for end-to-end validation of accelerators.}
\fi
%We surveyed accelerator designs published at six conferences in 2021\footnote{
%The International Symposium on Computer Architecture (ISCA),
%the International Symposium on Microarchitecture (MICRO),
%the IEEE Symposium on VLSI Technology \& Circuits (VLSI),
%the International Solid-State Circuits Conference (ISSCC),
%the Design Automation Conference (DAC), and
%the International Conference on Computer-Aided Design (ICCAD).}
  %and found that 
  %only 41\% of the accelerators 
  %for neural network applications
  %reported results on 
  %end-to-end evaluations over non-synthetic applications,
  %of which 68\% (28\% of the total) were from industrial teams.
%\hl{Can we have the raw data for this? We can put that in an appendix.}

%Among accelerator designs published in six 2021 conferences, 
%\footnote{We surveyed works that were published in 2021 in proceedings of the following conferences: 
%International Symposium on Computer Architecture (ISCA),
%International Symposium on Microarchitecture (MICRO),
%IEEE Symposium on VLSI Technology \& Circuits (VLSI),
%International Solid-State Circuits Conference (ISSCC),
%Design Automation Conference (DAC), and
%The International Conference on Computer-Aided Design (ICCAD).}
%only 41\% of the accelerators targeting neural network applications reported results on end-to-end evaluations over non-synthetic applications, while only 13\% were developed by non-industrial teams.
  %many recent papers on accelerator designs 
  %only evaluate on small application snippets, 
  %e.g., individual layers of deep neural networks~\cite{tambe20219,jia202031,park20219,rossi20214,schmidt20214,whatmough201916nm,fujii2018new,cao202065nm,giordano2021chimera,saito2021analog,wei2019overcoming, garofalo20211}.
%\hl{(Call forward to Bo-Yuan's analysis?)}

\subsection{Challenges and Goals for Application-level Validation}
Testing accelerators under development
  on complete applications
  requires two critical components: compiler support and application-level testing support.  
%Yet, testing accelerators under development
%  on complete applications
%  is a stubbornly difficult task
%  that requires two critical components. 
%\aarti{Maybe we should call out three distinct tasks shown in our comparison table: (1) accelerator operation specification and selection, (2) MMIO code generation, and (3) hardware-software co-simulation. Most of the text below can be binned into these 3 tasks and we explain the different approaches A, B, C as we go along.}
  %developing %a great deal of custom compiler infrastructure.
\begin{itemize}
  \item {\bf Custom compiler support:}
  An application 
    (likely written in a domain-specific language, or DSL)
    must be adapted to offload computations to an accelerator,
    which entails writing DSL compiler passes
    or manual modification of the source program.
  In common practice,
    invoking accelerator operations from software
    requires engineering effort,
    %from hardware experts,
    such as developing custom drivers to invoke
    accelerators via memory-mapped I/O (MMIO) interfaces.
  Such drivers are opaque to the compiler,
    difficult to debug,
    and often rely on low-level architectural details.
    %(i.e., changes to the hardware design require manual updates to drivers).
  The compilation tasks would be simplified 
    through \emph{greater automation} in:
  \begin{inlinelist}
      \item identifying acceleration offload opportunities in the application, and 
      \item generating the low-level code that invokes the requisite accelerator operations.
  \end{inlinelist}

  \item {\bf Application-level testing support:}
  This goal poses several difficulties with existing techniques. 
  Register-transfer level (RTL) designs (and thus RTL simulation) are not available in the early stages when the proposed end-to-end-testing is most useful. Even when prototype RTL designs are available, RTL simulation is only practically feasible for individual operations, 
  being too slow for full applications.
  FPGA-based emulation requires significant engineering effort and is typically not done until late design stages. %in the early-stage design.
  Faster high-level simulation (e.g., using SystemC) is feasible,
  but requires manually writing detailed simulation models and
  verifying later that they are sound with respect to the RTL implementation.
  %Using a faster high-level simulation model for the accelerator (such as a SystemC~\cite{SystemC} model) requires manually writing this model, and assurance that this is a sound with respect to the final RTL design. 
  The ideal for application-level testing %end to end validation
  is to \emph{automatically generate a sound high-level simulation model} for the accelerator that can be co-simulated with an application.
\end{itemize}


Note that the support components outlined above are specialized to a particular accelerator, and need to be updated every time an accelerator design is modified.
%with modifications to the accelerator design.
% and must be developed for each new accelerator (and updated with modifications to accelerator designs).
 % of these challenges,
In current practice, large industrial teams 
  invest substantial resources to develop bespoke infrastructure~\cite{jouppi2017datacenter, jouppi2020tpu},
  while smaller teams often
  do not pursue end-to-end evaluation,
  as illustrated by our literature survey in Fig.~\ref{fig.acc-survey}. %;
%\cite{tambe20219,jia202031,park20219,rossi20214,schmidt20214,whatmough201916nm,fujii2018new,cao202065nm,giordano2021chimera,saito2021analog,wei2019overcoming, garofalo20211}.


\begin{figure}[!ht]
  %\centering
  \begin{minipage}[h]{0.52\textwidth}
    \vspace{-5\fboxsep}
    \caption{
    \textbf{Gap in end-to-end evaluation of accelerators for neural network applications:} Our survey of $79$ papers in recent conferences (ISCA, MICRO, VLSI, and ISSCC in 2021 and ICCAD, DAC in 2020) that introduced new DL accelerator designs/methodologies, comparing how the accelerators were evaluated. Only 41\% of the works reported end-to-end evaluation on non-synthetic applications, of which 68\% (28\% of the total) were from industrial teams.
    }
    \label{fig.acc-survey}
  \end{minipage}\hfill
  \begin{minipage}[h]{0.43\textwidth}
    \includegraphics[width=\textwidth, right]{Figures/survey-compact.pdf}
  \end{minipage}
  \Description{}
\end{figure}


%One challenge in particular is 
  %obtaining end-to-end application-level results
  %using accelerators,
  %which requires compiler infrastructure
  %for identifying portions of applications that can be accelerated
  %and appropriately invoking the accelerator (or a simulator)
  %to determine the final results.
%To achieve maximal efficiency,
  %accelerators often feature custom data representations
  %or perform complex optimizations,
  %so testing the correctness of the application-level results
  %is an important form of validation for the designs.
%Yet, end-to-end results are difficult to obtain, 
  %especially in early stages of device development,
  %because of the amount of custom compiler infrastructure that must be developed:
%\begin{enumerate}
    %\item Opportunities to invoke the accelerator must be identified in the source application, which may require manually modifying applications to identify accelerator operations or developing compiler passes to do the same;
    %\item Once the accelerator operations have been identified, it is necessary to generate code to invoke the accelerator's memory-mapped I/O (MMIO) interfaces, which are low-level and require detailed knowledge of the specific architecture (in present practice, this is often achieved through driver-based APIs that are opaque to the compiler); and
    %\item Once the code has been generated, if the accelerator in question is still under development and has not yet been manufactured, results must be obtained through simulation---options include RTL simulation, which is very computationally expensive; emulation on an FPGA, \hl{which requires further engineering effort}; or developing a custom software simulator, which requires further implementation effort and must itself be validated to ensure correspondence with the underlying hardware.
%\end{enumerate}
%These steps are all costly, either in terms of engineering effort by experts
  %or computational expenses via RTL simulation,
%  and the infrastructure developed is often unique to a given accelerator.

%Large industrial teams 
  %invest substantial resources 
  %to address application-to-accelerator mapping challenges 
  %via bespoke infrastructure~\cite{jouppi2017datacenter, jouppi2020tpu}.
%While large industrial teams
  %have been able to invest substantial resources
  %to address these issues via bespoke infrastructure~\cite{jouppi2017datacenter, jouppi2020tpu},
  %for smaller teams, 
  %like in the academic research community,
  %end-to-end evaluation of new accelerator designs
  %can be prohibitively difficult:
  %many recent papers on accelerator designs 
  %only evaluate on small application snippets, 
  %e.g., individual layers of deep neural networks~\cite{tambe20219,jia202031,park20219,rossi20214,schmidt20214,whatmough201916nm,fujii2018new,cao202065nm,giordano2021chimera,saito2021analog,wei2019overcoming, garofalo20211}.
%\hl{(Maybe include Bo-Yuan's survey too? Is it present in an easily cite-able form?)}
%Unfortunately,
  %in many domains,
  %evaluating only individual device operations
  %or portions of applications
  %is \textit{not} informative as to the device's behavior
  %in a full end-to-end application.
%This manifests particularly in domains like deep learning (DL),
  %where accelerators commonly make use of custom numerical representations
  %on the principle that a loss of numerical accuracy
  %does not necessarily result in an incorrect application-level result
  %(such as a classification category),
  %ultimately a good trade-off if the new datatypes
  %lead to faster and more power-efficient in hardware.
%However, the effects of truncation and rounding
  %can cascade throughout an application,
  %resulting in large inaccuracies;
  %full-application analysis is necessary for detecting such inaccuracies
  %(\hl{Cite Titanic or FPBench work?}).
%Some of these inaccuracies can be mitigated
  %via software quantization techniques,
  %such as by inserting accumulation operations in larger datatypes
  %or including appropriate scaling factors,
  %but in other cases,
  %entirely different datatypes may be needed to obtain suitable results,
  %\textit{necessitating revisions to the hardware design}.
%\hl{(Insert good cites for quantization.)}
%Considering the complexity and potential expenses
  %of having to revise hardware designs,
  %end-to-end application evaluation is essential
  %for identifying issues at early stages of development.

%\aarti{We now explain the D2A row of our table and highlight the key points which provide the benefits.}
\subsection{Novel Contributions of our {\TLA} Approach}

We present a \emph{first-in-class} methodology 
  that supports
  end-to-end evaluation of accelerators
  {\it on unmodified full applications},
  which includes the ability to compile to
  and run simulations
  of 
  accelerator designs still in flux.
As a practical capability,
  \textit{
  this provides hardware designers with a feedback loop 
  similar to that of software debugging and testing.
  %This is the primary contribution of our work.
  }
%We observe that end-to-end application level testing
  %hinges on developing a custom compilation flow
  %from the high-level DSLs typically used to program applications
  %(like TensorFlow for deep learning~\cite{abadi2016tensorflow},
  %Halide for image processing~\cite{ragan2013halide},
  %and GraphIt for graph applications~\cite{zhang2018graphit})
  %to the specialized interfaces of accelerators under development,
  %a task which is very laborious in current practice.

Our methodology,
  termed ``\TLA,'' % (short for ``DSLs to Accelerators''),
  aims to reduce the manual engineering required for this feedback loop by effectively treating accelerator operations as extensions of processor instructions. 
  A novel contribution of \TLA is the use of a \emph{formal software/hardware (SW/HW) interface} that specifies an accelerator's
  operations and their semantics. Specifically, we leverage the Instruction-Level Abstraction (ILA), a formal specification for accelerators, that has been successfully used thus far for accelerator implementation verification~\cite{huang2018instruction} but not for compilation. In this work, we show how the ILA for accelerators serves as a SW/HW interface, similar to the Instruction Set Architecture (ISA) for processors, effectively serving as a ``single source of truth'' to drive various %all
  tasks required for compilation and end-to-end application testing. (The same high-level ILA model can then be used in a late-stage ILA/RTL validation step using existing techniques.) While the ISA has wide applications in computer architecture/compilers, there is no existing framework that uses an ISA-like formal SW/HW interface for accelerators: {\TLA} provides \emph{an existence proof} that this is feasible both conceptually and as a practical framework. 
  %It is truly novel in this regard.
  
  % to design compiler flows from high-level application DSLs to custom accelerators
  %by encoding the semantics of accelerator operations
  %formal software/hardware interface 
  
%   The key novelty of D2A is that the same ILA specification for an accelerator drives the various tasks required for compilation and end-to-end application testing. 
%The key novelty of \TLA is using an ILA accelerator specification as a ``single source of truth'' to drive all tasks required for compilation and end-to-end application testing.
  %In doing so, it  additionally makes the following contributions:
Our work makes the following novel contributions:
\begin{itemize}
%     \item automatically identifying acceleration offload opportunities
%     which enables \textit{evaluation of unmodified %off-the-shelf 
%     applications} on accelerators,
%     by using the accelerator operation semantics with term-rewriting techniques in the compiler pipeline,
%   in a technique we term ``\textit{flexible matching}'';

  \item Use of a formal SW/HW interface for the accelerator: We use the ILA accelerator specification 
   %as a formal software/hardware interface 
  to automate key tasks required for compilation and instruction-level simulation (\S\ref{sec:ila-compiling}). %required for application-level accelerator evaluation
  Thus far, the ILA had only been used for accelerator implementation and firmware verification.
  
  \item Design of  ``flexible matching'' (\S\ref{sec.method.flexible}): This new semantics-guided term rewriting technique specialized to accelerators adds 
  %that automatically discovers and offloads acceleration opportunities, thus enabling prototype accelerator evaluation on \textit{unmodified} applications;
  %In D2A this has been extended by  adding and using 
  custom rewrite rules for accelerators (\S\ref{sec:fragmentmapping}), and uses them in combination with  generic compiler intermediate representation (IR) rewrites. 
  This allows identifying, for the first time, semantically equivalent accelerator operations \emph{even without a direct syntactic match} %This enables 
  and significantly automates sophisticated operation offloading to accelerators without manually rewriting applications. 
  
    %using the accelerator operation semantics with term-rewriting techniques in the compiler pipeline,
  %in a manner we dub ``\textit{flexible matching},''
  %to automatically identify acceleration opportunities
  %and enable 
  %\textit{evaluating
  %unmodified off-the-shelf applications}
  %on accelerators; % in simulation.

%(requiring only rewrite rules from the compiler IR to accelerator operations),
  %which we dub ``\textit{flexible matching},''
%since it allows for flexibility in the precise representation of the source program.
%Flexible matching

  \item {\TLA} methodology and prototype: Combining the above techniques to achieve end-to-end mapping of unmodified applications to accelerators is another contribution. No existing tool (e.g., MLIR/CIRCT, PyMTL; see  \S\ref{sec:comparison}) 
  has attempted, much less achieved, the capabilities offered by the {\TLA} prototype (\S\ref{sec.prototype}) at such a level of automation. 
  %We hope that other frameworks will adopt the goal of providing 
  Our evaluation (\S\ref{sec.eval}) demonstrates 
  %the effectiveness of the capabilities in {\TLA}. 
  automatic identification of multiple acceleration opportunities in off-the-shelf DL models imported from publicly available implementations and benchmarks. 
  We evaluated these models end-to-end in simulation for three different accelerators; this was the \textit{first time} that full applications were evaluated for two of the accelerators, and the tests exposed a flaw in one design related to numerical representations, which the developers were able to correct.
  %\recheck{delete FPGA here?} We also use \TLA  to generate and execute code on an FPGA implementation of an accelerator, providing additional confidence in our compilation flow, above and beyond the simulation-based testing.

  %\item automated code generation using a formally specified interface for invoking accelerator operations, thus obviating the need for an expert-designed driver; and %an expert-designed driver; and
      
  %\item automated generation of correct-by-construction high-level simulators for accelerator operations, enabling reliable and efficient end-to-end testing of applications.
      
  %\item 
  %the compiler infrastructure
  %to automatically identify computations in the application that can be off-loaded to the accelerator using term-rewriting techniques
  %reason about the accelerator operations
  %and target
  %an abstract interface for accelerator operations
  %(obviating the need for an expert-designed driver).
  %The formal hardware-software interface also enables 

  \end{itemize}

%Our methodology reduces the task of testing applications on an early-stage accelerator design to that of 


%\paragraph{User inputs.}
The {\TLA} methodology requires two main inputs from the user:
\begin{inlinelist}
  \item ILA models: a formal ILA specification for accelerator operations (which can be reused for separate RTL verification), and
  \item \mapping rules: rewrite rules from the compiler IR to the accelerator operations (``mappings,'' for short).
\end{inlinelist}
Note that both of these are
  \textit{one-time efforts} per accelerator. 
Furthermore,
  this approach %also
  greatly lowers the effort after hardware
  %facilitates exploring
  design revisions,
  as it requires modifying only the accelerator operation specifications and rewrite rules, if necessary,
  and obviates the need for
  certain additional work, like updating the high-level simulators, which are generated automatically in {\TLA}.
  %thereby allowing for easier design exploration.
%\byhuang{Spell out what's not needed?} 
%For example, no additional effort is needed to update the high-level simulators, which are generated automatically. 


\subsection{Comparison with Existing Approaches and Tools}
\label{sec:comparison}

Although there have been many efforts in compiler flows to support accelerators~\cite{bahr2020creating,truong2020fault,lai2019heterocl,chen2021byoc,ragan2013halide,AtlPopl22,chen2018tvm,moreau2019hardware,lattner2021mlir,ExoPldi22}, none of them provides automated support for end-to-end testing of unmodified applications at the same level as {\TLA}. 
We start by providing a high-level comparison summarized in  
Table~\ref{figure.methodology} and provide a detailed comparison with specific tools at the end.

\subsubsection{Task-based Comparison}

Existing approaches use different techniques for three critical tasks:
%\begin{inlinelist}
%\item accelerator operation selection, 
%\item code generation, and 
%\item software-hardware co-simulation. 
%\end{inlinelist}
accelerator operation selection, code generation, and software-hardware co-simulation. 
We compare them against {\TLA} for each task.

% \zpara{Task 1: Accelerator Operation Selection} 
\paragraph*{Task 1: Accelerator Operation Selection} 
%The common current practice is for the software developer to manually insert API calls for accelerator invocations. This requires manually refactoring the model code, with little-to-no code portability and thus additional effort to change to a new accelerator. Some emerging tools like BYOC~\cite{chen2021byoc}  can invoke accelerator operations by emitting accelerator MMIO instructions for parts of the compiler IR that \textit{syntactically} match patterns for accelerator operations. However, this will miss  semantically equivalent matches that are not syntactically identical. Bespoke compilation efforts, possibly built on top of tools like BYOC and frameworks like MLIR~\cite{lattner2021mlir} or Exo~\cite{ExoPldi22},
%, and Glenside~\cite{glenside}, 
%make sophisticated compilation passes to identify operations for offloading. Such approaches require accelerator-specific customization and  compiler expertise. By contrast, \TLA overcomes the purely syntactic matching limitation of BYOC by using %ILA-accelerator instruction rewrites for each operation 
%IR-accelerator mappings in conjunction with compiler IR rewrites using equality saturation. This finds accelerator operation matches beyond syntactic matching in an automated way, \emph{not requiring the expertise and cost of bespoke compiler infrastructure}, and providing code portability through easy migration to a new accelerator.
A common practice is for the software developer to manually insert API calls for accelerator invocations, or to use syntactic patterns to identify possible matches (e.g., using BYOC~\cite{chen2021byoc}). 
Bespoke compilation efforts, possibly built on top of tools like BYOC and frameworks like MLIR~\cite{lattner2021mlir} or Exo~\cite{ExoPldi22},
%, and Glenside~\cite{glenside}, 
make sophisticated compilation passes to identify operations for offloading, %. They require accelerator-specific customization 
but require compiler expertise. 
In contrast, \TLA overcomes the limitations of purely syntactic matching to find \emph{semantically-equivalent matches} in an automated way, without requiring the expertise and cost of bespoke compiler infrastructure.

\paragraph*{Task 2: Code Generation} 
% \zpara{Task 2: Code Generation} 
This task involves emitting the actual instructions, i.e., the MMIO loads/stores, from the application program to invoke accelerator operations. 
%In the common practice of using API calls to invoke accelerator operations, these instructions are in the API implementation, often referred to as the ``device driver'' for the accelerator.
%This API implementation hence requires platform-specific knowledge.
%Further, the API calls and their hardware implementation do not have a formal software/hardware interface---the compiler has no built-in knowledge of the semantics of these API calls.
%Further, the API calls are opaque to the compiler in the absence of a formal software/hardware interface, i.e., the compiler has no knowledge of the semantics of these API calls.
%Therefore, the semantics of what has been implemented in the API calls in the HW is unclear. 
%By contrast, in \TLA this code generation is trivial following the operation selection step: each ILA instruction in the \mapping %IR-accelerator mapping %program fragment corresponds one-to-one with an MMIO instruction and is replaced accordingly. Moreover, the compiler has complete semantics of these instructions through the formal software/hardware interface.
%is replaced one-to-one with its corresponding MMIO instruction.
%Thanks to the close correspondence between the ILA instructions and MMIO instructions, this code generation is trivial following the operation selection step.
A common practice is to emit MMIO code in implementations of the API calls that invoke accelerator operations, often referred to as the ``device driver'' for the accelerator. However, these API calls are opaque, in that a compiler has no built-in knowledge of the semantics of the accelerator operations or the MMIO code.  
Alternatively, in bespoke compilation efforts, this knowledge is built into the compiler but requires significant expertise and does not use a formal hardware semantics.
In contrast, in \TLA this code generation task is trivial following the operation selection step: each ILA instruction in the \mapping %IR-accelerator mapping %program fragment
corresponds one-to-one with an MMIO instruction and is replaced accordingly. Moreover, the compiler has complete knowledge of their semantics via the formal SW/HW interface.
%is replaced one-to-one with its corresponding MMIO instruction.
%Thanks to the close correspondence between the ILA instructions and MMIO instructions, this code generation is trivial following the operation selection step.

%\vishal{This paragraph feels like it could be a bit more gelled. It seems to waver a bit.}
%Further, because the ILA is a formal interface specification, its instructions (and thus the MMIO instructions) have formal semantics  that can be verified against the hardware. This has been done in prior work for several accelerators~\cite{huang2018instruction} 
%%from domains such as image processing, machine learning, and security, 
%where refinement checking in the ILAng toolchain~\cite{huang2019ilang} is used to check the correspondence of each ILA instruction specification and the RTL implementation.

\paragraph*{Task 3: Software-Hardware Co-Simulation} 
% \zpara{Task 3: Software-Hardware Co-Simulation} 
Co-simulation is needed to validate the results of the computation being done by the accelerator offloads (the hardware) and the host processor (the software) for application-level testing. 
%The co-simulation is done using frameworks like QEMU~\cite{bellard2005qemu}, which integrate RTL simulation calls (e.g., via Verilator~\cite{verilator}) with host-processor execution.
%The main drawbacks are that 
%\begin{inlinelist}
%\item this can be done only in later design stages with RTL available and
%\item RTL simulation is very slow since it maintains cycle-level accuracy.
%\end{inlinelist}

%Consequently, the co-simulation is too slow for full-application simulation and is typically only done at the level of individual operators.
%Individual operator co-simulation does not enable exposing accumulated imprecision or errors across multiple operations or operator formulation mismatches. 
% (e.g., sigmoid). % <- what is this referring to?
%Higher-level system models in languages like SystemC~\cite{SystemC} provide simulation speedup and can be used in early-stage design, but these models may not be validated with respect to the RTL; there is a general lack of formal verification tools for checking general SystemC models against RTL.
%By contrast, the ILA model provides for automatic generation of high-level (instruction-level) simulation models using the ILAng toolchain~\cite{huang2019ilang}. 
%This toolchain also enables formal verification of the RTL implementation against the ILA model, 
%using refinement checking~\cite{huang2018formal}, 
%provides for 
%and thus ensures the soundness of the generated simulator. 
A common practice is to use frameworks like QEMU~\cite{bellard2005qemu}, which integrate RTL simulation calls (e.g., via Verilator~\cite{verilator}) with host processor execution. However, this is too slow for full-application testing.
Higher-level system models in languages like SystemC~\cite{SystemC} provide faster simulation but require significant effort for creating simulation models, and these models are difficult to validate against the RTL design.
In contrast, the ILA model in {\TLA} supports \emph{automatic generation of instruction-level simulation models} using the ILAng toolchain~\cite{huang2019ilang}. The ILA also allows separate verification against the RTL implementation, thereby ensuring soundness between the high-level simulation and the RTL implementation. 

%Different combinations of choices for these three tasks correspond to specific flows. The most common flow is manually inserting API calls in the application code to invoke accelerator operations, manual implementation of the accelerator driver, and a slow late-stage RTL simulation. An improved practical flow may replace RTL simulation with high-level SystemC simulation, but with drawbacks, as described earlier. An improved experimental flow may use BYOC for syntactic matching of accelerator operations, %which provides for limited automation, but may miss offloading opportunities that are beyond syntactic matches. Bespoke compilation efforts using high-expertise teams can overcome these drawbacks through specialized compilation passes, but require great resources and result in limited, if any, code portability. 
%even then they generally do not use
%Furthermore, none of these flows provides a formal software/hardware interface for reasoning about the offloaded code. The \TLA flow \re{is the first to use a formal software/hardware interface (the ILA) to provide for flexible matching in accelerator operation selection, automated code generation (driver code), and 
%potentially \aarti{remove potentially?} 
%sound high-level co-simulation.} 
%This capability is unmatched by any of the existing flows. 
%What this accomplishes is the ability to do application-level accelerator evaluation without accelerator-specific application rewriting %(through flexible matching) 
%and co-simulation based on a formal hardware model. 
%We will demonstrate the benefits of %this capability \TLA through a prototype implementation evaluated on multiple deep learning models and accelerators.
%an evaluation using multiple deep learning models and accelerators.

%\re{As summarized in Table~\ref{figure.methodology}, none of the existing approaches provides a formal software/hardware interface for reasoning about the offloaded code. The \TLA methodology is the first to use a formal software/hardware interface (the ILA) to support flexible matching in accelerator operation selection, automated code generation (driver code), and sound high-level co-simulation.} 
%We will demonstrate the benefits of %this capability \TLA through a prototype implementation evaluated on multiple deep learning models and accelerators. 


\input{Floats/tab-comparison}


\subsubsection{Detailed comparison with closely related tools}
We discuss details of some specific tools.

\paragraph{MLIR~\cite{lattner2021mlir}} 
% \zpara{MLIR~\cite{lattner2021mlir}} 
%MLIR itself cannot be compared directly with {\TLA}, as 
MLIR is a framework for building 
%compiler infrastructure that supports implementing 
compiler IRs (as ``dialects'') in a structured, reusable manner.
%, whereas {\TLA} is a methodology for compiling end-to-end applications to accelerators. 
Some MLIR dialects address tasks related to hardware design; these include CIRCT, which supports high-level synthesis (HLS) and hardware simulation but \emph{not compilation of applications to accelerators}. To the best of our knowledge, there is no SW/HW interface in MLIR that enables compiling applications to accelerators via CIRCT. Other dialects are intended to interface with specific accelerators (e.g., the TPU) and deep learning frameworks (e.g., ONNX).
However, compilation using these dialects still entails mapping between IRs at different granularities and other challenges, which are addressed by \TLA.
%However, compilation using these dialects still requires mapping between IRs at different granularities and other challenges addressed by \TLA. 

\paragraph{HLS tools (e.g., Catapult~\cite{siemens-catapulthls}) and PyMTL~\cite{batten2018pymtl}} 
% \zpara{HLS tools (e.g., Catapult~\cite{siemens-catapulthls}) and PyMTL~\cite{batten2018pymtl}} 
These tools/frameworks allow for describing hardware designs with a high-level software-like interface, which is useful for designing accelerators and high-level hardware simulation. However, the hardware design specifications in these frameworks do not address accelerator operation selection and code generation during compilation---two of the three tasks in Table~\ref{figure.methodology}, which 
are significantly automated by {\TLA}. 

\paragraph{Exo~\cite{ExoPldi22}} 
% \zpara{Exo~\cite{ExoPldi22}} 
Exo also addresses the problem of compiling applications to accelerators. It uses a notion similar to high-level instructions for interfacing with accelerators, but, unlike \TLA, relies on \emph{manually specified sequences of rewrites} and other bespoke compiler passes, to expose instances of those instructions and compile them to the devices using \emph{exact syntactic matching}. 
%Also unlike D2A, Exo does not provide a means to verify its high-level interfaces for accelerators against the RTL implementation (which can be done via the ILA).
\TLA introduces flexible matching to \emph{automatically discover} accelerator offload opportunities in the applications, given a few rules relating the behavior of accelerator operations to the compiler IR (as in Exo). 
%Exo is less automated, instead relying on ``performance engineers'' to manually specify explicit sequences of transformations for mapping \emph{each} application to \emph{each} accelerator using a separate language. 
%Thus, Exo also addresses the problem of compiling applications to accelerators, but is less automated. Analogously to D2A’s IR-accelerator mappings, Exo expresses “accelerator instructions” in terms of its own IR via user annotations. Unlike flexible matching in D2A, Exo relies on user-specified sequences of IR rewrites and exact syntactic matching to find instances of annotated instructions for accelerator offloads. 
Additionally, Exo does not provide a \emph{formal SW/HW interface} or any means to verify its accelerator instructions against the RTL implementation.
%At a high level, {\TLA} is designed to support hardware architects as they develop new accelerators by providing automated compilation, testing, and simulation support that works across many unmodified applications. In contrast, Exo is designed to support application programmers lowering their code to already-developed hardware accelerators by providing a DSL to program the lowering for each application-accelerator combination.

\subsection{Paper organization}
We first provide the background (\S\ref{sec.background}) on ILA~\cite{huang2018instruction} and 
equality saturation~\cite{joshi2002denali, tate2011equality}.
The techniques in {\TLA} are described in detail next (\S\ref{sec.method}), followed by a description of the
{\TLA} prototype\footnote{Our prototype, benchmarks, and evaluation infrastructure will be open-sourced under a permissive license.} (\S\ref{sec.prototype}). 
%that implements the {\TLA} methodology for DL applications, utilizing the ILAng platform~\cite{huang2019ilang}, the TVM DL compiler stack~\cite{chen2018tvm}, and the Glenside DSL~\cite{smith2021pure} and \egg library~\cite{willsey2021egg} for equality saturation on tensor programs.
We present detailed evaluation using our prototype (\S\ref{sec.eval}), and end with a discussion on more broadly related work (\S\ref{sec.related}) and conclusions (\S\ref{sec.conclusion}).
%% Aarti: have highlighted this as part of contributions, in Sec I.B 
%that demonstrate the capabilities of our prototype: for automatically identifying acceleration opportunities in off-the-shelf DL models imported from publicly available implementations and benchmarks, and for evaluating these models end-to-end in simulation for three different accelerators. This was the \textit{first time} that full applications were evaluated for two of the accelerators, and the tests exposed a flaw in one design related to numerical representations, which the developers were able to correct. We also use \TLA  to generate and execute code on an FPGA implementation of an accelerator, providing additional confidence in our compilation flow, above and beyond the simulation-based testing.
      
\iffalse
This paper is organized as follows:
\begin{itemize}
    \item In \S\ref{sec.background}, we provide the background 
    %on how the formal software/hardware interface specification can be realized via
      on ILA~\cite{huang2018instruction} and 
      %and how flexible matching can be implemented using 
      equality saturation~\cite{joshi2002denali, tate2011equality}.
    
      %dramatically reduce the amount of engineering effort required to evaluate accelerator designs on end-to-end applications.
    \item In \S\ref{sec.method}, we describe technical details of the {\TLA} methodology, using two common DL applications (LSTM-WLM~\cite{pt2020wlm} and ResNet-20~\cite{he2016deep}) as illustrative examples.
    \item In \S\ref{sec.prototype},
      we present a prototype\footnote{Our prototype will be open-sourced under a permissive license.} 
      that implements
      the {\TLA} methodology
      for DL applications,
      utilizing the ILAng platform~\cite{huang2019ilang}, the TVM DL compiler stack~\cite{chen2018tvm},
      and the Glenside DSL~\cite{smith2021pure} and \egg library~\cite{willsey2021egg} 
      for equality saturation on tensor programs.
    \item In \S\ref{sec.eval},
      we demonstrate the capabilities of our prototype
      by: (1) automatically identifying acceleration opportunities
      in off-the-shelf DL models imported 
      from publicly available implementations and benchmarks,
      and (2) evaluating these models end-to-end
      in simulation 
      for three different accelerators. % modeled using the ILA.
    This was the \textit{first time} that full applications 
      were evaluated for two of the accelerators,
      and the tests exposed a flaw in one design
      related to numerical representations,
      which the developers were able to correct.
    We also use \TLA %our compiler infrastructure 
    to generate and execute code on an FPGA implementation of an accelerator, 
    providing additional confidence in our compilation flow, above and beyond the simulation-based testing.
     % demonstrating that our approach \textit{also enables modular, extensible compilation to new accelerator designs}.
%     \item We provide discussion on future work, related work, and conclusions in \S\ref{sec.future}, \S\ref{sec.related}, and \S\ref{sec.conclusion}, respectively.
\end{itemize}
\fi 

\iffalse
While our contributions in this work focus on
  enabling end-to-end application evaluation on accelerators,
  this work also lays the foundations for further possibilities.
As the FPGA experiment demonstrates,
  our methodology 
  provides the foundations
  for building fully fledged compilers
  that modularly and extensibly incorporate accelerator support.
Additionally,
  the ILAng toolchain~\cite{huang2019ilang} also supports the lowering of ILA semantics
  to SMT queries,
  suggesting the possibility 
  of formally verifying compiler IR--accelerator rewrites,
  increasing confidence in the compiler's correctness.
It is the crucial step of
  \textit{abstracting over accelerator operations
  and exposing their semantics to the compiler}
  that raises these possibilities.
\fi

%Our approach is centered
  %on what we identify as the most crucial gap
  %in the task of extending compiler support to accelerators:
  %the lack of a formal software/hardware interface.
%High-performance applications are often
  %written in high-level domain-specific languages (DSLs) 
  %like TensorFlow for deep learning~\cite{abadi2016tensorflow},
  %Halide for image processing~\cite{ragan2013halide},
  %and GraphIt for graph applications~\cite{zhang2018graphit}.
%These DSLs may specify applications in terms of fine-grained operations
  %that make sense within the domain
  %(such as mapping operations over tensors),
  %while accelerator operations are coarse-grained ``hardware function calls,''
  %which often combine many operations based
  %on what is most efficient to implement in hardware.
%In present practice,
  %matching the fine-grained operations of the application DSL
  %or compiler IR
  %with coarse-grained operations
  %is often achieved using driver-based APIs for the devices
  %that are opaque to the compiler
  %and require manual effort to insert calls to accelerators
  %or otherwise bespoke compiler passes,
  %treating each accelerator as \textit{sui generis}.

%Our methodology, which we call ``{\TLA}'' (short for ``DSLs to Accelerators''),
  %proposes to remedy this gap through 
  %the use of a formal representation for accelerators.
%By exposing the semantics of accelerator operations,
  %our methodology enables further automation
  %in matching compiler IR operations to accelerator operations
  %and avoids the use of opaque APIs.

%Applying a formal software/hardware interface
  %for accelerators
  %allowed us to approach the task of 
  %evaluating full applications on accelerators
  %in a manner similar to the implementation of traditional compilers
  %for programmable, general-purpose devices.
%Representing the semantics of accelerator operations
  %enabled us to use term rewriting (via equality saturation)
  %to greatly increase the amount of automation in this process
  %compared to traditional accelerator workflows.
%While our contributions in this work consist of
  %the use of ILA specifications and equality saturation
  %for end-to-end application evaluation on accelerators
  %and the prototype implementation,
  %this work also lays the foundations for further possibilities.
%Our experiments focus on \textit{functional correctness}
  %for applications featuring accelerators,
  %but further improving the cost functions
  %for the equality saturation and modeling further details
  %of the system like data movement
  %could allow for building fully fledged optimizing compilers
  %that modularly and extensibly incorporate accelerator
  %support using our methodology.
%Additionally,
  %the ILAng toolchain also supports the lowering of ILA semantics
  %to SMT queries,
  %suggesting the possibility 
  %of formally verifying compiler IR--accelerator rewrites,
  %increasing confidence in the compiler's correctness.
%It is the crucial step of
  %\textit{abstracting over accelerator operations
  %and exposing their semantics to the compiler}
  %that raises these possibilities.
  
%\hl{need contributions / outline paragraph}

%\hl{intro needs to coin the term ``flexible matching''}

%\hl{We should try to wrap up intro by end of page 2. Move details into overview or later technical sections and just stick to the higher-level points.}

\iffalse

% \hl{need to define ``numerics'' sooner}

Hardware specialization is the main technique for improving power-performance efficiency in emerging compute platforms. 
%
By customizing compute engines, memory hierarchies, and data representations~\cite{chan2014itrs,fang2019understanding,lai2021programming}, hardware accelerators provide efficient computation in various application domains like artificial intelligence, image processing, and graph analysis~\cite{han2016eie,chen2016eyeriss,reagen2016minerva,zhang2016cambricon,hameed2010understanding,ham2016graphicionado}.
%
At the same time, there is a growing trend in using domain-specific languages (DSLs) for boosting development productivity, e.g., TensorFlow for deep learning~\cite{abadi2016tensorflow}, Halide for image processing~\cite{ragan2013halide}, and GraphIt for graph applications~\cite{zhang2018graphit}.
%
However, there are critical gaps in bridging these two distinct system trends in the current compilation flows that start from high-level DSLs and target accelerator-rich platforms.%
\footnote{
  Note that there are two different compilation problems that DSL compilers tackle: 
  \begin{inlinelist}
    \item adding compiler support for a given custom accelerator, and 
    \item generating the accelerator design to be implemented on an FPGA as part of the compilation~\cite{noronha2018leflow,lai2019heterocl,skalicky2018hot,wang2021autosa,hegarty2014darkroom,cfuplayground}.
  \end{inlinelist}
  This paper deals with the former. % problem.
}

%  l'epic wall of cites
%However, critical gaps frustrate combining these two distinct system trends in current compilation flows from high-level DSLs to accelerator-rich platforms.
%
Large industrial teams invest substantial resources to address DSL-to-accelerator mapping challenges via bespoke infrastructure~\cite{jouppi2017datacenter, jouppi2020tpu}.
%
For smaller teams, like in the academic research community, these gaps make end-to-end evaluation of new accelerator designs prohibitively difficult;
%
many recent papers on accelerator designs only evaluate on small application snippets, e.g., individual layers of deep neural networks~\cite{tambe20219,jia202031,park20219,rossi20214,schmidt20214,whatmough201916nm,fujii2018new,cao202065nm,giordano2021chimera,saito2021analog,wei2019overcoming, garofalo20211}.
%
Unfortunately, an accelerator's results for application snippets are often \textit{not} predictive of its  influence on overall system behavior, e.g., due to cumulative effects of custom numerical representations --- %
engineers need the ability to easily validate complete applications compiled onto accelerator platforms. This is lacking in current prevalent practice.
%what is needed is the ability to easily validate the results of the compilation of complete applications onto accelerator-platforms. This is lacking in the current prevalent practice.

%Next, we review the prevalent practice in DSL-to-accelerator mapping and its inherent limitations.
%

%\hl{small para and new subsection to make clear segue?}
%In this paper, we propose a new methodology based on formal software/hardware interfaces to mitigate the challenges of compiling DSL workloads to accelerator-rich target platforms.

\subsection{Prevalent Practice: Accelerator Drivers/APIs}
%
Effective accelerator utilization requires offloading application computations to those supported by accelerators. 
%
In current practice, this is generally accomplished by manually crafting device drivers to provide ``hardware function calls'' for specific operations.
%
This essentially provides an application programming interface (API) for accelerators, where each hardware function call consists of low-level accelerator invocation commands that configure, initiate, and check/return the results. 
%
Figure~\ref{fig.mmio} shows an example hardware function call, in which a layer reduction operation is implemented by a sequence of memory-mapped input/output (MMIO) loads/stores from the host processor to invoke the FlexASR accelerator~\cite{tambe20219}.
This MMIO-based API is the prevalent mechanism for accelerator invocation.

\input{floats/lst-mmio.tex}

Generally, hardware function calls are manually added
by application programmers, or %though 
they can be  %invoked 
added by compilers via handcrafted accelerator-specific extensions. Such bespoke compiler extensions demand tedious effort and deep expertise in the hardware and the compilation stack.%
%
In practice, this limits end-to-end deployment of DSLs to accelerator-rich platforms within reach for only a few --- large enterprises that can afford teams of hardware, software, and system experts for high-value applications~\cite{caulfield2016cloud,fowers2018configurable,jouppi2017datacenter}.
Further, this API-based approach has three critical gaps in mapping applications from DSLs to accelerator-rich platforms:

\begin{enumerate}[label=\textbf{G\arabic{*}}, leftmargin=*]

  \item \textbf{Lack of portability.} 
  \label{gap.portability}
  %
  Hardware function calls implemented in device drivers contain MMIO loads/stores for accelerator invocation,
  %
  %However, 
  but these low-level commands work only for a specific processing platform.
  %
  %Further, as these MMIO load/stores provide no semantics of the corresponding operations in the accelerator, they cannot be retargeted to a different accelerator.
  %
  %This limits the portability of device drivers across hardware processing platforms.
  When these calls %hardware function calls are invoked 
  are manually added by an application programmer, 
  %it limits the portability of the application program 
  the application is not portable to new accelerator platforms. 
  %Even if the hardware function calls are invoked 
  When these are added by a compiler through bespoke extensions, 
  %that can match application code to these function calls, 
  this task requires significant accelerator-specific restructuring of the compiler. This is challenging due to:
  \begin{inlinelist}
    \item the mismatch between fine-grained compiler intermediate representation (IR) intrinsics and coarse-grained accelerator operations (that are often used to deliver power-performance efficiency), and 
    \item the lack of a formal software/hardware interface specification for the accelerator that can be leveraged to automate this matching.
  \end{inlinelist}
  %\sm{Shouldn't the following be part of instruction selection - i.e. Gap 2?}
  %On the other hand, accelerator APIs provide hardware function calls for operations supported by accelerators, which tend to implement coarse-grained operations for achieving better power-performance efficiency.
  %
  %However, compiler intermediate representations (IR) are normally fine-grained for easy mapping to general-purpose CPUs and GPUs.
  % 
  %Such granularity mismatch makes it difficult to map the compiler IR intrinsics to accelerator operations, unlike the case of general-purpose processors where such a mapping is usually one-to-one or one-to-few between IR intrinsics and processor instructions.
  %
  % Consequently, compiler developers need to manually craft the IR and optimizations to target available hardware function calls provided by the accelerator API.
  %
  %Even so, application developers often still have to carefully restructure their programs to enable the compilation and generation of efficient code.
  %
  This limits portability across both compiler frameworks and hardware platforms.


  \item \textbf{Lack of integration into standard compiler flows.}
  \label{gap.integration}
  %
  % For implementing operations supported by accelerators, each hardware function call consists of a sequence of accelerator invocation commands to set up configurations, load/store data, and trigger accelerator operations.
  %
  Unlike general-purpose processors which have the ISA as the software/hardware interface, accelerators lack a formal software/hardware interface specification which makes the accelerator APIs
  opaque to the compiler stack. This makes it challenging to integrate them %accelerator computation 
  in standard compiler flows. 
  %
  For example, standard techniques for instruction selection~\cite{blindell2016instruction} 
  %which can select between alternate code implementations 
  become challenging with a ``black-box'' API,  
  % \hl{What do we mean by ``fixed API'' here? The problem is probably even harder with an extensible API.}
  %
  %Further, these API calls 
  which provides little flexibility in selecting and reusing parts of these APIs.
  %controlling the low-level accelerator invocation commands, 
  The fixed API also limits automation in exposing potential optimizations, e.g., operator fusion that may minimize data transfers.
  %) that require fine-grained knowledge of individual operations and their operands.
  %
  %With limited integration with standard compiler flows, adding accelerator support using accelerator APIs requires significant ad hoc manual effort in compiler instrumentation.


  \item \textbf{Lack of ability to validate compilation results.} 
  \label{gap.correctness}
  %
  Accelerators often incorporate novel techniques such as custom memory management schemes and numeric representations (referred to as numerics) to improve computational efficiency.
  %
  However, existing compiler platforms do not readily support these unconventional features, %but failing to support them 
  which can lead to the generation of inefficient or even incorrect code.
  %
  Unfortunately, the lack of a formal software/hardware interface specification of accelerator operations makes it very difficult to validate compilation results.
  %unlike an instruction set architecture (ISA) for a general-purpose processor, an accelerator generally lacks a formal specification at its software/hardware interface (the MMIO commands do not capture the accelerator operation semantics).
  %
  %Thus, there is no way to validate the code generated using the API calls without knowing the accelerator-side semantics for the MMIO operations.
  %
  In current practice, this is done using software/hardware co-simulation with an FPGA emulation of the accelerator or with a low-level register-transfer level (RTL) model for the accelerator. The former incurs significant engineering overhead, and the latter is very slow, making validation for full applications unrealistic. In practice, validation using RTL co-simulation is performed only for individual accelerator-supported operations.
  %
  This is insufficient to determine if small deviations at the operation level (e.g., due to different numerics) lead to acceptable results at the application level.
  %
  Further, the need for a fully functional RTL model limits early stage software/hardware co-design, i.e., software development before the hardware is implemented.

  % This is generally done using software/hardware co-simulation with a low-level register-transfer level (RTL) model for the accelerator, which is very slow and generally does not allow for full application co-simulation in reasonable time. Thus, in practice, this is done only for individual hardware function calls for each accelerator-supported operation, and not for the full application. In domains like machine learning, small deviations at the single operation level may be acceptable due to different numerics in the accelerator, but this is insufficient to determine if their accumulation at the application level will lead to acceptable results. Further, this RTL-level co-simulation limits early stage hardware-software codesign, i.e., software development before the hardware is implemented.

  %This leaves compilation results validation to ad hoc testing and limits early-stage software/hardware co-design, i.e., software development before the hardware is implemented.

\end{enumerate}

% As described above, t
The lack of a formal software/hardware interface specification that provides the semantics of individual accelerator operations is the root inadequacy across the three gaps.
%
In this work, we propose the {\TLA} (short for ``DSLs to Accelerators'') methodology which addresses these gaps via the use of an ISA-like formal model for accelerators, instead of using API-supported hardware function calls.
%
% By using an instruction-level formal software/hardware interface for accelerators, this methodology avoids coarse-grained black-box hardware function calls that are used in current practices.
%
The formal semantics of the accelerator operations are provided through an \emph{accelerator instruction set}.
%
This formal model and the finer-grained, instruction-level control it provides (in contrast to API calls) facilitates standard compiling techniques (e.g., instruction selection and optimization), facilitates portability, and enables validation of compilation results.

\subsection{{\TLA} Methodology: Key Ideas}

Identifying the computation in the application that can be offloaded to the accelerators is effectively seeking \emph{mappings} between compiler IR intrinsics and the accelerator operations such that they are functionally equivalent and lead to performant code. 
%
(Note that in certain domains like machine learning, small numerical differences may not affect the application-level results, such as a classification category, so notions of ``equivalence'' should reflect this property.)
%(Note that in domains like machine learning, the equivalence in results may allow some acceptable deviation due to different numerics in the accelerator.) 
%\hl{Is it acceptable because of the nature of ML applications or due to numerics of accelerator? --The former}
%
%As discussed, the prevalent current practice is to use hardware function calls (using an API) for mapping application operations to accelerator operations. These API calls are often too coarse and lack formal semantics, and therefore suffer from gaps~\ref{gap.portability},~\ref{gap.integration}, and~\ref{gap.correctness}.
%
While the semantics of the accelerator operations is available in the hardware RTL model, this is too low-level for the compiler to target directly and thus not an option.
% 
In the {\TLA} methodology, we use the Instruction-Level Abstraction (ILA)~\cite{huang2018instruction}, 
%an ISA-like formal model proposed for accelerators, 
for this purpose.
%
The ILA, like the ISA for processors, provides a  software/hardware interface specification for accelerators. 
It bridges the gap between the compiler IR intrinsics on one hand and the accelerator operations on the other --- providing the basis for specifying \emph{IR-accelerator mappings}.

\para{Compiler IR-Accelerator Mappings using ILAs}
%
The ILA model of an accelerator provides formal semantics at its software/hardware interface through a \emph{lifting} of the accelerator operations in the form of a set of instructions,
each of which reads/updates the accelerator architectural state. Similarly, the IR intrinsics can be defined as instructions that update program state.
%
% While the ILA is primarily intended for accelerators, it is also convenient to model both the compiler IR and the accelerator using ILAs.
%
This provides a uniform model on both sides --- the IR side and the accelerator side --- in the form of instructions. We
specify 
%hence allows specifying 
the mappings as pairs of \emph{program fragments}, where a program fragment comprises a sequence of instructions.
%
Unlike API calls or RTL models, program fragments using accelerator ILA instructions capture the underlying accelerator operation semantics while \emph{abstracting} out low-level hardware implementation details.
%
Further, these program fragments provide the flexibility to specify mappings at \emph{different levels of granularity} --- this helps bridge the granularity mismatch which may exist between IR intrinsics and coarse-grained accelerator operations.

%Most compiler flows are designed for general-purpose CPUs and GPUs, i.e., targeting processor instructions, which raises the impedance in integrating with black-box hardware function calls.
%
%The formal instruction-level interface for accelerators provided by its ILA enables better integration into existing compiler flows and thus the utilization of techniques in compilation and optimization (addressing gap~\ref{gap.integration}).

\para{Instruction Selection and Optimization}

With the instruction semantics specified by an ILA, the {\TLA} methodology facilitates standard instruction selection and optimization techniques in compilers.
%
In this work, we use term rewriting for instruction selection, i.e., using \emph{compiler IR-accelerator rewrites} to map IR intrinsics to accelerator instructions~\cite{dershowitz1993taste,baader1999term,blindell2016instruction}.
%
Specifically, we utilize equality saturation to optimally match equivalent rewrites of the program and therefore reduce the need for manual program restructuring (addressing gaps~\ref{gap.portability} and ~\ref{gap.integration})~\cite{tate2011equality,stepp2011equality}.
%
Further, the compiler IR-accelerator mappings have finer-grained knowledge and control of accelerator operations through the instruction-level interface provided by ILAs.
%
This enables optimizations that require understanding of accelerator behavior, e.g., operator fusion for reducing data movement (addressing gap~\ref{gap.integration}).

\para{ILA-based Compilation-Results Validation}

We use the term compilation-results validation for checking that the results of the compilation are correct, i.e., functionally equivalent or with acceptable deviation when there are differences in numerics.
%\hl{do we want ``correct'' or ``acceptable''?}.
This may be done at the level of a single accelerator-supported operation, i.e., checking the correctness of compiler IR-accelerator mappings, or for the entire application.
%
% Further, note that for domains like machine learning, it may be acceptable to have some deviation in the results produced by the compiled code with compared to the reference application model due to differences in numerics. 
%
The formal semantics of the ILA instructions provides the foundation for 
%
% With the formal definition of each instruction, the behavior of a program fragment is well-defined and therefore facilitates compilation results validation.
%
%The validation may be done using either 
proof-based formal verification or simulation-based validation. %where possible.
%
This enables systematic end-to-end compilation-results validation as well as early-stage software/hardware co-design (addressing gap~\ref{gap.correctness}).

\input{floats/fig-compile-flow.tex}

\para{\TLA Compilation Flow}

The overall compilation flow using the {\TLA} methodology is shown in Figure~\ref{fig.3la-flow}.
%
We first translate the application program, provided in a high-level DSL, into the compiler IR. 
%
Next, we perform equality saturation
%\hl{(using both accelerator-specific and general-purpose rewrite rules)}
to search a large space of equivalent programs given the compiler IR-accelerator mappings
(along with general-purpose rewrite rules).
%
Based on a given cost function, we then extract the
%best 
lowest-cost
program and pass it on for code generation, where each accelerator instruction is subsequently lowered to the corresponding MMIO load/store command. %that invokes the accelerator.
%
This generated program executes on the host processor and invokes accelerator operations through MMIO commands.
%
(Note that the ILA modeling and the validation of the IR-accelerator mappings are omitted from this figure.)

\subsection{Prototype Implementation and Case Studies}

As a demonstration of the {\TLA} methodology, we have implemented an end-to-end compilation flow for deep learning (DL) applications by integrating it with an existing compiler flow.
%
Specifically, our prototype compiler utilizes the DSL front-end and the code-generation capability provided by the TVM framework~\cite{chen2018tvm}. For instruction selection, it leverages the rewrite rules and the equality saturation engine provided by Glenside and \egg~\cite{smith2021pure,willsey2021egg}.
%
The ILA-models of accelerators, the validation of IR-accelerator mappings, and compilation-results validation at the application level are powered by ILA-based methods in ILAng~\cite{huang2019ilang}.

We show the generality of the {\TLA} methodology through multiple case studies.
%
At the front-end, we consider \AppNum DL applications for language processing and image recognition, e.g., Transformer~\cite{vaswani2017attention} and ResNet~\cite{he2016deep}.
% At the front-end, we consider six DL applications from MLPerf~\cite{mlperf} (e.g., BERT~\cite{devlin2018bert} for language processing and Resnet50~\cite{he2016deep} for image classification) and seven other popular applications such as MobileNets~\cite{howard2017mobilenets} and ResMLP~\cite{touvron2021resmlp} for object detection and image classification, respectively.
%
For the target accelerators, we add support for three custom accelerators that provide hardware operations at different levels of granularity: 
VTA~\cite{moreau2019hardware} is a fine-grained accelerator for general tensor operations; 
HLSCNN~\cite{whatmough201916nm} is a coarse-grained accelerator providing 2D convolutions; 
FlexASR~\cite{tambe20219} is an accelerator for speech recognition, specializing in coarse-grained operations such as a long short-term memory (LSTM) layer.

We added support for the three accelerators --- developed their ILA models, provided compiler IR-accelerator mappings for operations supported by them, and validated all the mappings.
Note that this work for supporting a new accelerator is a one-time effort that can be reused across different applications.
%
%\hl{Without manually restructuring any of the application programs}, 
Our prototype compiler successfully compiled all \AppNum DL applications (developed by different teams and programmed in different DSLs) for exploiting the three custom accelerators.
%-- demonstrating that the {\TLA} methodology provides %better portability. 
%
%
%\sm{Fix: Furthermore, we demonstrate ``some optimizations'' (baseline fusion to reduce data movement).
%
%We also demonstrate ``debugging/early-stage co-design'' through exposing an accuracy issue for HLSCNN found by application-level simulation.
%
%Checking for post-training quantization --- there's still difference between PyTorch and hardware.}
Our prototype and case studies demonstrate the key ideas in the {\TLA} methodology for end-to-end compilation with validated results. 
%
\emph{
  We do not claim this work provides a complete, fully optimized compiler for custom accelerators; rather, it establishes the foundations for validated compilation for such targets through the use of a formal instruction-level software/hardware interface for accelerators.
}

\subsection{Contributions}

Overall this paper makes the following contributions:
%
\begin{itemize}[leftmargin=*]

  \item We present the {\TLA} methodology that uses an instruction-level formal software/hardware interface specification for an accelerator, which abstracts away low-level implementation details
  %which presents a verifiable lifting 
  while providing a formal hardware semantics with the following capabilities:
  %
  \begin{itemize}[leftmargin=*]
    \item It bridges the granularity gap between compiler IR intrinsics and accelerator operations with flexible mappings between instruction-level program fragments.

    \item It enables integration into standard compiler flows and the application of standard techniques like instruction selection and optimization.

    \item It provides for end-to-end validation of compilation-results for accelerator-extensible compilers.
  \end{itemize}

  \item We describe a prototype\footnote{%
  Our prototype will be 
  released under a permissive open-source license.
} that implements the {\TLA} compilation flow on top of open-source frameworks TVM, Glenside, \egg, and ILAng.
  %
  % By integrating with existing flows and utilizing the provided compilation/optimization techniques, the prototype demonstrates end-to-end compilation for DL applications.

  \item We provide a set of case studies that demonstrate: 
  %
  \begin{itemize}[leftmargin=*]
  
  \item portability: through compiling \AppNum DL applications (programmed by different teams in different DSLs) 
  %\emph{without} manual \hl{program restructuring}.
  
  \item extensibility: through adding support for three custom accelerators which provide operations at different levels of granularity.
  
  \item compilation-results validation: for both IR-accelerator mappings, and at the application-level. The latter demonstrates how this methodology exposed several accuracy issues due to custom numerics.
  \end{itemize}

  % \item first framework for accelerator-extensible compilers

\end{itemize}

This article is organized as follows. 
%
In Sections~\ref{sec.method} and~\ref{sec.prototype}, we explain the {\TLA} methodology and our compiler prototype, respectively.
%
We then describe the case studies and evaluation results in Section~\ref{sec.eval}.
%
Future work, related work, and conclusions are discussed in Sections~\ref{sec.future},~\ref{sec.related}, and~\ref{sec.conclusion}, respectively.

\fi