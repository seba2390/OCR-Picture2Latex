\section{Related Work}
\label{sec.related}

%\input{floats/tab-compiler-framework-comparison}

%\hl{[Zach]}

% Limitations of current approaches
% E.g., what’re the challenges in using MLIR, for example, to build the {\TLA} or an end-to-end flow.
% Even if MLIR wants to do the same things, they should adopt the {\TLA} approach! (i.e., MLIR doesn’t immediately make addressing the challenges we are focusing on easier!)
% (We would need to figure out which MLIR integrations we’re referring to… CIRCT may be one. There is also a systolic array document)

%The key insights of the \TLA methodology for doing end-to-end application-level evaluation on accelerators are using (i) the ILA as a formal software/hardware interface for mapping DSL applications to specialized accelerators and (ii) using flexible matching to avoid rewriting applications to match an accelerator.
%
% Above, we demonstrated how \TLA's ILA interfaces enable simulating 
% %\hl{ \textit{unmodified}} 
% applications on custom accelerators, modular validation and verification of accelerator mappings, and flexible matching to search for semantically equivalent offload opportunities.
%
%Below we survey work related to the \TLA methodology.

% By adopting \TLA, engineers can simulate, debug, and even verify mappings from DSL workloads down to custom accelerator invocations.
% %
% Furthermore, \TLA's uniform accelerator interface enables flexible matching to automatically discover opportunities for accelerator invocations in unmodified source programs.
% %
% \TLA is also extensible along several key dimensions: adding support for new accelerators, enhancing flexible matching with additional rewrite rules, and incorporating new tools into the ecosystem (e.g., design space exploration or breakpoint debugging).
% %
% %To the best of our knowledge, no existing framework provides a similarly comprehensive set of features.
% %
% Below we survey past work addressing different aspects of the features \TLA provides.

\paragraph{Software/Hardware Co-Design}

Recent work on accelerator generation and integration~\cite{
    bahr2020creating, truong2020fault}
  has explored adding support in the Halide~\cite{ragan2013halide}
  compiler flow for specialized Coarse-Grained Reconfigurable Array (CGRA) accelerators.
That work composes an
  impressive array of custom tools to
  generate and verify specialized CGRA accelerators
  and also map Halide program fragments
  down to accelerator invocations.
HeteroCL~\cite{lai2019heterocl} also provides
  a similar custom flow.
%designed around the details of the HeteroCL compiler flow.
By contrast, the \TLA methodology supports % is designed to support
  software/hardware co-design by mitigating impedance mismatches
  between the granularity of high-level % \textit{independently developed}
  DSLs and \textit{near-arbitrary} accelerators;
  because of the flexibility of the ILA,
  the \TLA methodology is applicable to a
  broader class of compilers and accelerators.
  %,
  %rather than being focused on a single compiler flow
  %or class of accelerators.
%As such, \TLA complements the above past work in software/hardware co-design and could be combined with those approaches to further ease the development, integration, and verification of specialized accelerators. % supporting high-value applications.

\paragraph{Pattern Matching Accelerator Calls}

% \hl{This paragraph appears to clash with the section header.}
% Numerous DSLs have recently become popular
%   to facilitate programming productivity
%   in high-value applications, e.g.,
%   TVM~\cite{chen2018tvm},
%   Halide~\cite{ragan2013halide}, and
%   TACO~\cite{kjolstad2017tensor}.
% The \TLA methodology complements all these efforts;
%   \TLA is designed to ease adding accelerator support for
%   compilers in this class of high-performance DSLs
%   by providing a uniform interface that can be reused
%   across DSLs and compilers and additionally
%   introduce support for verifying compiler-to-accelerator
%   mappings via ILAng tooling~\cite{huang2019ilang}.

The most closely related work to flexible matching is from 
\begin{inlinelist}
  \item TVM BYOC~\cite{chen2021byoc}, which only provides exact syntactic matching as discussed in \S\ref{sec.background}, and
  \item Glenside~\cite{smith2021pure}, which, prior to this work, had not been integrated into a compilation pipeline nor used to target custom accelerators.
\end{inlinelist}
Past work has also explored rewrite-based techniques for
  automatically inferring instruction selection passes
  between ISAs~\cite{
    ramsey2011resourceable,
    dias2010automatically}
  and in the context of superoptimization~\cite{
    bonsal-so,
    bonsal-so-translate}.
Rewriting in \TLA instead operates on a high-level IR
  to expose opportunities to invoke code generators,
  rather than performing low-level code generation directly.
Equality saturation has been
  used in the context of
  ML and DSP compilers for
  optimization~\cite{
    yang2021equality,
    alexa-dsp-eqsat,
    caviar-cc22}.
There has also been significant work on
  ML and HPC compiler frameworks with
  varying degrees of support for
  targeting custom accelerators~\cite{
    ragan2013halide,
    AtlPopl22,
    chen2018tvm,
    moreau2019hardware,
    lattner2021mlir}.
To the best of our knowledge,
  none of these frameworks provides support for
  testing prototype accelerators
  designs end-to-end on
  unmodified source applications.
  


% In principle, many DSLs allow for supporting custom accelerators via bespoke translations from DSL operators to specific accelerator APIs,
% e.g., as in the original TVM~\cite{chen2018tvm} support for VTA~\cite{moreau2019hardware}.
% %
% TVM's BYOC~\cite{chen2021byoc} interface
% % provides a more convenient mechanism for
% eases incorporating custom accelerators
% %, especially for coarser-grained operations,
% by performing syntactic pattern matching to offload computations via user-provided code generators.
% % By building on TVM,
% %   BYOC inherits support for multiple frontends
% %   and additionally provides extensible pattern matching to facilitate
% %   adding new rules for mapping DSL fragments to new accelerators.
% %\hl{merge next two sentences?}
% However, BYOC leaves
%   all matters of code generation, e.g., MMIO invocations,
%   to the user,
%   while \TLA provides
%   more structure to code generation
%   via the ILA.
% %Furthermore, unlike \TLA, BYOC does not directly support custom
% % code generation for fine-grained MMIO invocations.
% In particular, 
%   the ILA provides useful simulation capabilities.
% %   that can be of practical use during compiler development, 
% %   whereas any comparable faculties (such as TVM's built-in VTA functional simulator) 
% %   would otherwise have to be separately developed and deployed.
% Additionally, BYOC's pattern matching
% %, while incorporating some dataflow analysis beyond exact matching,
% cannot search the space of programs equivalent to the input,
% limiting the number of potential accelerator invocations %it can find
% compared to flexible matching in our \TLA prototype.

% The MLIR framework~\cite{lattner2021mlir} provides a rich metalanguage and numerous tools for
%   developing, optimizing, and translating between custom compiler IRs,
%   but does not inherently provide direct support for
%   \TLA's features.
%   %--- MLIR serves more as a rich metalanguage
%   %in which users can develop custom IRs,
%   %potentially reusing tools and definitions.
%   %ideally reusing tools and definitions from related IRs also
%   %expressed in MLIR.
% It would be possible to realize the \TLA methodology
%   within an MLIR-based ecosystem; 
%   we implemented our DL-focused prototype using TVM %and BYOC
%   since it allowed leveraging 
%   % us to leverage 
%   more DL infrastructure.
%   %in the case of our DL-focused prototype, 
%   %we found it more convenient to use TVM and BYOC 
%   %we believe that our current approach using BYOC required less effort and derived more benefit
%   %from existing infrastructure. 
%   %\hl{Not sure what is the point being made in the later part of this sentence. --It is supposed to explain why we chose to implement our prototype via TVM rather than MLIR. Can rephrase to clarify}

% However, these instruction selection techniques
%   are complementary to \TLA's approach
%   and potentially can be used in the \TLA methodology
%   for generating instructions for fine-grained accelerators like VTA.
%\hl{maybe add sentence distinguishing from flex matching, and then cut what is now the last sentence of this para?}
%   and how equality saturation can be applied
%   to automatically discover accelerator opportunities~\cite{willsey2021egg}. \hl{Should the cite at the end be Glenside? Not sure what in the egg paper it is referring to.}
%These techniques provide an ideal complement to
%  \TLA's pattern-matching approach and could be
%  incorporated into the \TLA methodology in future work.
%   for generating instructions for fine-grained accelerators like VTA.

\paragraph{Validating and Verifying Accelerator Calls}

% Consider two
%   of \TLA's key benefits relative to  existing approaches:
%   providing a formal software/hardware interface and
%   built-in verification support via ILAng~\cite{huang2019ilang}.
% \hl{References to the table from CASES that hsould be removed or rephrased}
% Traditional, ad hoc approaches and implementations providing
%   custom accelerator support for a particular DSL
%   can map from high-level DSLs all the way
%   to MMIO-invoked custom accelerators,
%   but generally are too specialized to provide easy extensibility.
% However, such focused specialization also facilitates
%   good support for compiler optimizations, both
%   at higher levels (e.g., common subexpression elimination) and
%   at lower levels (e.g., operator fusion). \hl{Not clear what is the point being made here.}

Tools like Verilator~\cite{verilator} and Cuttlesim~\cite{pitclaudel2021cuttlesim} enable cycle-accurate RTL simulation,
%but do not provide \hl{reusable} interfaces or flexible matching to incorporate custom accelerators into existing compiler flows. Further, given the low-level RTL simulation, they 
but are too slow to enable application-level co-simulation. Co-simulation using faster high-level SystemC~\cite{SystemC} models partially address this gap; however, the SystemC models need to be independently written and, unlike ILA models, do not have a clear formal verification path to RTL. Further, 
%the ILA matches the MMIO interface and targets the highest level of abstraction by focusing on updates of the architecture state variables, as opposed to 
general SystemC models do not target MMIO interfaces and may have arbitrary levels of detail. 
%Recent work has also introduced the first approach to
  %formally verify fast-math style optimizations~\cite{cakeml-fastmath},
  %which may provide an avenue toward principled support for
  %custom accelerator numerics in \TLA.
Other work has targeted formal verification of code generation for accelerators~\cite{AtlPopl22,ExoPldi22}, but does not have a path to RTL design verification that is possible with ILAs. 
 %and
 %do not support %for %flexible matching and 
 %application-level co-simulation. \sm{Not sure how this fits here.} 
%

\paragraph{Support for Diverse DSLs}

Our \TLA prototype supports importing from various DL frameworks (including MxNet~\cite{chen2015mxnet}, PyTorch~\cite{paszke2019pytorch}, TensorFlow~\cite{abadi2016tensorflow}, ONNX~\cite{linux2019onnx}, and CoreML~\cite{apple2022coreml}) via TVM's importers to the Relay IR.
% , which convert those frameworks' representations into Relay. BYOC and Glenside also operate on Relay and thus also benefit from these importers. The ecosystem of shared dialects in
Similar to Relay, MLIR~\cite{lattner2021mlir} now also  supports importing models from 
%PyTorch, Tensorflow, ONNX, and CoreML 
various frameworks; the \TLA approach complements such flows as demonstrated in our prototype.
%using dialects that mirror those frameworks' representations
% and later compiling them into other dialects that can be lowered to hardware. 
% Halide also has some support for importing models from PyTorch and other frameworks~\cite{adams2020halideroadmap}. 
In contrast, Exo~\cite{ExoPldi22} presently has no support for importing other representations.
%and only limited support for importing ONNX and PyTorch models into Halide~\cite{adams2020halideroadmap}. 
%\sslyu{Where did the mention of PT-TC come from in the original table? I cannot find a reference to this.}
%\akash{Let's say PyTorch and cite the Halide development roadmap\cite{adams2020halideroadmap}? Technically, the relevant part of the roadmap seems to be about output to PyTorch, rather than input. However, the Halide source code seems to describe it as PyTorch integration -- you can use Halide within PyTorch to transform tensors.}
%\akash{We may also want to cite importing from ONNX, I found a github repo called onnx-halide that purports to do that.}

% \TLA is capable of supporting any accelerator back-end provided that it be represented in terms of ILA instructions. BYOC, MLIR, and Exo are similarly capable of supporting arbitrary back-ends, so long as users implement custom code generators. Several Halide extensions have implemented support for some accelerators, like stencils or systolic arrays, but these have required modifications to Halide's core (see Table~\ref{figure.methodology}).
% %and are not extensible with respect to new back-ends. 
% While Glenside can represent arbitrary accelerator operations as opaque predicates, no further code generation is implemented (hence, \TLA is its first use in code generation).

% Formally verified compilers such as  
%   CompCert~\cite{leroy2006formal} and CakeML~\cite{kumar2014cakeml}
%   can rigorously establish end-to-end equivalence
%   from high-level source code down to assembly for
%   various CPU back-ends via machine-checkable proofs,
%   but currently do not provide a general approach
%   for integrating new accelerator support and
%   provide no support for custom numerics.
% In contrast, \TLA enables validating
%   accelerator mappings via
%   end-to-end simulation handling custom numerics. 
  %and formally verifying individual rewrite rules from compiler IR patterns to accelerator invocations.
% \TLA provides an approach
%   to extend existing compilers
%   with mappings from DSLs workloads
%   to custom accelerator back-ends and
%   enables validating these mappings
%   via end-to-end simulation.
% Thus, \TLA complements
%   existing compiler verification efforts, and
%   may provide guidance for future work on formally
%   verifying ``accelerator-extensible'' compilers.


  
%   \hl{Need to make sure that we distinguish \TLA's validation of the compilation results from the CompCert style verifying the compiler itself.}

% \subsubsection{The Costs and Benefits of Abstraction in 3LA}
% %
% In order to provide modularity, extensiblity, and verification,
%   the 3LA methodology relies on ILA-based abstractions of both
%   compiler IR intrinsics and accelerator operations.
% Our earlier case studies (Section~\ref{sec.eval}) demonstrate
%   the numerous benefits such abstractions can provide,
%   but the approach is not a panacea.
% In particular, strictly respecting abstraction boundaries in 3LA
%   complicates adding support for lower-level optimizations like
%   operator fusion that require fine-grained knowledge of operator internals.
% %  operator fusion that requires first ``inlining'' low-level
%   %operator implementations to expose optimization opportunities.
% This added complexity does not impact coarse-grained accelerators
%   like FlexASR, but further work may be required to achieve
%   optimal performance for finer-grained accelerators.
%   %that rely
%   %on operator fusion.







  
  
  
  
  
  



% ---------- CASES ----------

% \section{Related Work} \label{sec.related}

% The 3LA methodology uses the ILA as a uniform interface
%   connecting DSLs to custom accelerators by
%   incorporating an extensible pattern matcher to verifiably
%   map application fragments to accelerator invocations.
% As such, 3LA is closely related to both
%   various mainstream systems and
%   past work on
%   software/hardware co-design,
%   extensible compiler frameworks, and
%   compiler verification.

% \subsection{Relation to Current Approaches}
% %
% Table~\ref{tab.comparison} provides
%   a high-level comparison of 3LA to
%   a selection of related methodologies, tools, and frameworks.
% Because 3LA spans several layers of the system stack,
%   these contrasting points are not all of the same sort;
%   we discuss the nuances of such ``cross-category''
%   feature comparisons below.

% \mysubsub{Representative Approaches and Key Features.}
% %
% Calling back to Section~\ref{subsec.challenges},
%   we consider how each related approach supports
%   formal software/hardware interfaces (Formal SW/HW),
%   verification (Verif),
%   end-to-end workflows (E2E),
%   multiple backends (Multi-Back),
%   multiple frontends (Multi-Front),
%   extensible pattern matching (PatMatch), and
%   compiler optimizations (Opts).
% Across these dimensions, we compare
%   traditional, manual approaches to customization (Ad Hoc),
%   the TPU support provided in the
%     TensorFlow~\cite{abadi2016tensorflow} (TPU: TF) and
%     PyTorch~\cite{paszke2019pytorch} (TPU: PT) frameworks,
%   the BYOC interface~\cite{chen2020byoc}, and
%   the MLIR framework for developing compiler IRs~\cite{lattner2021mlir}.


% \input{tab_comparison.tex}


% \mysubsub{Comparing Approaches.}
% %
% Table~\ref{tab.comparison} highlights two
%   of 3LA's key benefits relative to  existing approaches:
%   providing a formal software/hardware interface and
%   built-in verification support via ILAng~\cite{huang2019ilang}.
% Traditional, ad hoc approaches and implementations providing
%   custom accelerator support for a particular DSL
%   (TPU: TF, TPU: PT) can map from high-level DSLs all the way
%   to MMIO-invoked custom accelerators,
%   but generally are too specialized to provide easy extensibility.
% However, such focused specialization also facilitates
%   good support for compiler optimizations, both
%   at higher levels (HL, e.g., common subexpression elimination) and
%   at lower levels (LL, e.g., operator fusion).

% By building on the TVM framework,
%   BYOC inherits support for multiple backends and frontends and
%   additionally provides extensible pattern matching to facilitate
%   adding new rules for mapping DSL fragments to new accelerators.
% However, BYOC leaves
%   all matters of code generation, e.g., MMIO invocations,
%   to the user,
%   while 3LA provides
%   more structure to code generation
%   via the ILA.
% %However, unlike 3LA, BYOC does not directly support custom
% %  code generation for fine-grained MMIO invocations.

% The MLIR framework provides numerous tools for
%   developing, optimizing, and translating between custom compiler IRs,
%   but does not inherently provide direct support for any of
%   the features 3LA targets --- similarly to BYOC, MLIR serves more as a rich metalanguage
%   in which users can develop custom IRs,
%   potentially reusing tools and definitions.
%   %ideally reusing tools and definitions from related IRs also
%   %expressed in MLIR.
% It would likely also be possible to realize the 3LA methodology
%   within an MLIR-based ecosystem; in the case of our DL-focused
%   prototype, we believe that our current approach using BYOC required less effort and derived more benefit
%   from existing infrastructure.
 
% \mysubsub{The Costs and Benefits of Abstraction in 3LA}
% %
% In order to provide modularity, extensiblity, and verification,
%   the 3LA methodology relies on ILA-based abstractions of both
%   compiler IR intrinsics and accelerator operations.
% Our earlier case studies (Section~\ref{sec.eval}) demonstrate
%   the numerous benefits such abstractions can provide,
%   but the approach is not a panacea.
% In particular, strictly respecting abstraction boundaries in 3LA
%   complicates adding support for lower-level optimizations like
%   operator fusion that require fine-grained knowledge of operator internals.
% %  operator fusion that requires first ``inlining'' low-level
%   %operator implementations to expose optimization opportunities.
% This added complexity does not impact coarse-grained accelerators
%   like FlexASR, but further work may be required to achieve
%   optimal performance for finer-grained accelerators.
%   %that rely
%   %on operator fusion.

% \subsection{Broader Related Work}

% % 3LA additionally relates to broader research efforts
% %   across the system stack.
% % Below we briefly highlight some of
% %   the most closely related past work.

% \mysubsub{Software/Hardware (SW/HW) Co-Design.}
% %
% Recent work on accelerator generation and integration~\cite{
%     bahr2020creating, fault-truong-cav2020}
%   has also explored adding support in the Halide~\cite{ragankelley2013halide}
%   compiler flow for specialized Coarse-Grained Reconfigurable Array (CGRA) accelerators.
% In that approach, the authors compose an
%   impressive array of custom tools to
%   generate and verify specialized CGRA accelerators
%   and also map Halide program fragments
%   down to accelerator invocations.
% HeteroCL~\cite{lai2019heterocl} also provides
%   a similar custom flow.
% %designed around the details of the HeteroCL compiler flow.
% In contrast, the 3LA methodology is design to support
%   fast and flexible SW/HW co-design by mitigating impedance mismatches
%   between the granularity of \textit{independently developed}
%   DSLs and \textit{near-arbitrary} accelerators;
%   because of the flexibility of the ILA,
%   the 3LA methodology is applicable to a
%   broad class of compilers and accelerators,
%   rather than being focused on a single compiler flow
%   or class of accelerators.

% % As such, 3LA complements the above past work in
% %   HW/SW co-design and could be combined with
% %   those approaches to further ease the
% %   development, integration, and verification
% %   of specialized accelerators supporting high-value applications.

% \mysubsub{DSL Design, Extensible Compiler Frameworks, and Compiler Verification}
% %
% Numerous DSLs
%   to facilitate programming productivity
%   in high-value applications have recently become popular, e.g.,
%   TVM~\cite{chen2018tvm},
%   Halide~\cite{ragankelley2013halide}, and
%   TACO~\cite{taco}.
% The 3LA methodology complements all these efforts;
%   3LA is designed to ease adding accelerator support for
%   compilers in this class of high-performance DSLs
%   by providing a uniform interface that can be reused
%   across DSLs and compilers and additionally
%   introduce support for verifying compiler-to-accelerator
%   mappings via ILAng tooling~\cite{huang2019ilang}.

% Past work has also explored rewrite-based techniques for
%   automatically inferring instruction selection passes
%   between ISAs~\cite{dias-isel-popl10}
%   and how equality saturation can be applied
%   to automatically discover accelerator opportunities~\cite{willsey2021egg}.
% These techniques provide an ideal complement to
%   3LA's pattern-matching approach and could be
%   incorporated into the 3LA methodology in future work.

% Formally verified compilers like
%   CompCert~\cite{compcert} and CakeML~\cite{cakeml}
%   can rigorously establish end-to-end equivalence
%   from high-level source down to assembly for
%   various CPU back-ends via machine-checkable proofs,
%   but currently do not provide a general approach
%   for integrating new accelerator support.
% 3LA provides exactly such an approach
%   for extending existing compilers
%   with verifiable mappings from program fragments
%   to accelerator back-ends, and thus also complements
%   the state-of-the-art in formal compiler verification.


% % listing what different approaches provide.
% % Verification, pattern matching, co-design, numerics support, and 
% % integration with broader workflow.

% % Compare with Cosy (Lisa) and Target (nML), manual dev, BYOC, and 
% % MLIR~\cite{lattner2021mlir}.

% % Table~\ref{tab.comparison} listing what different approaches provide.
% % Verification, pattern matching, co-design, numerics support, and 
% % integration with broader workflow.

