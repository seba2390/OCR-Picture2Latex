\subsection{Target Accelerators}
\label{sec.eval-acc}

We added support for three DL accelerators that provide hardware operators at different levels of granularity:
\begin{enumerate}[leftmargin=*]

\item \textbf{FlexASR} is an accelerator for speech and natural language processing (NLP) tasks that supports various RNNs~\cite{tambe20219}.
%
It uses a custom numeric data type called \textit{AdaptivFloat} for boosting the accuracy of quantized computations~\cite{tambe2020algorithm}.
%
% \hl{Our prototype supports FlexASR's linear layer and LSTM operations [@Gus/Mike, should we discuss how the LSTM pattern matching is handled? Perhaps give a footnote?].}
%
% FlexASR receives MMIO load and store commands from the host processor through an AXI interface. 
%
% These AXI commands have 128-bit data for various configurations, control, and memory operations. 

% FlexASR supports a variety of computations common in NLP applications, including various types of RNNs (e.g., Vanilla RNN, GRU, LSTM), fully connected layers, layer reduction (e.g., max and mean pooling), layer normalization, and the attention mechanism. 

\item \textbf{HLSCNN} is an accelerator optimized for 2D convolutions~\cite{whatmough201916nm}.
%
% It provides programmable support for arbitrary batch sizes, input and filter dimensions, and strides. 
%
%HLSCNN is designed to operate on mixed 8/16-bit fixed point data 
It operates on mixed 8/16-bit fixed point data
(8 bits for storing weights and 16 bits for computations).
%, and the feature map tensors are expressed in the NHWC layout format for providing better performance through parallelization.
%
% \hl{Our prototype compiler rewrites non-grouped 2D convolutions of appropriate sizes to HLSCNN's convolution operation.}

\item \textbf{VTA} is a parameterizable accelerator for tensor operations featuring a processor-like design~\cite{moreau2019hardware}.
%with an ISA and configurable parameters
%
It supports element-wise arithmetic operations as well as generalized matrix multiplication,
operating on 8-bit integer data.
%
% \hl{(Cribbed from CASES, please review/shorten.)}
% We chose VTA in part because TVM has special code generation support for VTA and also that unlike HLSCNN and FlexASR, which provide coarse-grained operations, VTA provides fine-grained instructions --- supporting VTA stresses the \textit{generality} of the {\TLA} methodology.
% As with other processor-like designs, the VTA ILA is based on its ISA~\cite{huang2018instruction}.
% In our prototype, as a proof of concept, we map matrix multiplication and bias addition to fixed sequences of VTA ILA instructions.
% In principle, it would be possible to implement a VTA ILA code generator more similar to TVM's built-in support for VTA (which produces VTA instructions to implement arbitrary TVM operators).

\end{enumerate}
%
%\hl{(Please review.)} 
% \recheck{@Yi: please review (combined LHS and RHS LoC and gave total for mappings).}
For each accelerator, we defined an ILA model and a set of \mapping rules.
%
The ILA models for FlexASR, HLSCNN, and VTA are approximately 5600, 1600, and 2100 lines of ILAng code (C++), respectively. 
%--- note that these ILAs serve the dual purposes of enabling compilation via the \TLA methodology as well as validating the RTL design.
%Note that 
The high-level synthesis (HLS) implementations
  of the accelerators are about 9300 (SystemC), 5100 (SystemC), and 6900 (Chisel) LoC, %lines of code, 
  respectively;
  the ILA specifications are thus of modest size,
  compared even to the relatively compact HLS implementations. 
%
For each \mapping rule, we represent the compiler side in Glenside IR, and the accelerator side %(via a wrapper API) 
as a program composed of ILA instructions (in a Python-embedded DSL). 
%\recheck{What's this wrapper API? Is it not in the python-embedded DSL?}
The total size of mapping rules (both the compiler and accelerator sides) for FlexASR (5 mappings), HLSCNN (1 mapping), and VTA (1 mapping) was 186, 22, and 49 LoC, respectively.
Recall that these mappings are polymorphic over tensor size on both sides, leading to general and compact representations. 
% **Aarti's original version: **
% \re{The ILA models do not include the \mapping rules. 
% For mappings, the LHS is represented in Glenside IR, and  the RHS is represented (via a wrapper API) as a program composed of ILA instructions (in a python-embedded DSL). Recall that these mappings are polymorphic over tensor size on both sides, leading to general and compact representations. 
% For FlexASR, we used 5 mappings (totalling 119 LoC on LHS, 67 LoC on RHS); for HLSCNN, we used 1 mapping (12 LoC on LHS, 10 LoC on RHS); for VTA, we used 1 mapping (38 LoC on LHS, 11 LoC on RHS). 
% }
Additionally, the BYOC-based code generators and runtimes for these accelerators are approximately 450, 300, and 900 LoC of C++, %\recheck{/Python},
%lines of code, 
respectively. 
%\recheck{@Yi, would it be more accurate to include the LoC for the Python codegens here also?}
%\recheck{It might also be more confusing/TMI since we didn't say much about their diffs and roles.}
%\recheck{Based on previous discussion, we agreed that we just LoCs for the mappings, which is added before, for counting the manual effort for providing the mappings. I think we should avoid adding more implementation details such as "API wrapper" or "python" stuff here.}
These indicate the implementation of the code generation module in our prototype, as well as reusable utilities for data movement, handling custom numerics, and emitting the low-level MMIO code for each selected accelerator offload for end-to-end simulation of the application.

\iffalse
%(Note that the SystemC HLS models are more difficult to simulate compared to using ILAng.
Note that simulating the SystemC HLS models requires extra effort and is less efficient compared to using ILAng.
  %do not provide the simulation capabilities provided by the ILAng-generated simulators. 
First, the ILA matches the MMIO interface while the SystemC model requires a custom driver. 
Second, the ILA model targets 
  the highest (architecture) level of abstraction,
%the highest level of abstraction by focusing on updates of the architecture state variables, 
  whereas general SystemC models may reflect low-level design details and are slower to simulate.
  \fi
%
%Additionally, the BYOC-based code generators and runtimes for these devices are approximately 450, 300, and 900 LoC of C++,
%lines of code, 
%respectively.
%
%Please see Appendix~\ref{app.operations} for a complete list of supported operations for the three accelerators.
