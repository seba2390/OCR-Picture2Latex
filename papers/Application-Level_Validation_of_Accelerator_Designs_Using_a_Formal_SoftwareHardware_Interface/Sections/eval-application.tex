\subsection{Target Applications}
\label{sec.eval-app}

% The MLPerf inference benchmark~\cite{mlperf,reddi2020mlperf} is a standardized collection of machine learning inference workloads, developed by more than 200 ML researchers and engineers across academia and industry.
% For our evaluation, we select models from the MLPerf inference benchmark so as to show our methodology's utility on state-of-the-art, industry-accepted workloads.
% Note that we do not run a true MLPerf benchmarking session, which involves connecting to the benchmark's custom-built test harness.
% Rather, we perform our own evaluation and gather our own metrics.

% \hl{most of the networks below are not actually part of mlperf inference...-GS}


%For the case studies, we considered the below applications which are developed by different teams and programmed in different DSLs:
%For our evaluation experiments, 
We considered \AppNum DL applications corresponding to common neural network models for language and vision tasks that contain operators supported by the three target accelerators.
%Many of these models loosely correspond to those included in the MLPerf benchmark~\cite{mlperf,reddi2020mlperf}, though 
We selected applications with reasonable size for human inspection and in-depth analysis.
% We generally chose relatively small models to reduce simulation times and simplify development. 
%ResMLP was chosen as a recent model that comprised primarily of linear layers and hence would be simple to support on VTA and FlexASR, while the LSTM word language model was chosen for its simplicity and correspondence to FlexASR's LSTM instruction.
%
% \hl{Let's give our reason for selection with each model --- the hl command does not play well with enumerations.}
%
\begin{enumerate}[leftmargin=*]

%\item \textbf{BERT} is a language representation model, for NLP tasks, that utilizes transformers with variable number of encoder layers and self-attention heads~\cite{devlin2018bert}.

\item \textbf{EfficientNet} is a recent convolutional neural network (CNN) designed for image classification~\cite{tan2019efficientnet}. 
%that uniformly scales network width, depth, and resolution~\cite{tan2019efficientnet}. 
% We chose it because it contains convolutions that could be accelerated by VTA and HLSCNN.
It has convolutions that are supported by VTA and HLSCNN.
% We chose EfficientNet as a recent CNN and hence a model likely to contain convolutions that could be accelerated by EfficientNet or VTA.

%\item \textbf{GNMT} is a neural network designed for language translation~\cite{wu2016google}.
%At its core, GNMT is powered by a deep chain of LSTM layers.

%\item \textbf{LSTM-S2T} is a speech recognition application designed based on the previously proposed LSTM recurrent neural network architecture~\cite{graves2014towards,tambe20219}. 

\item \textbf{LSTM-WLM} is a text generation application~\cite{pt2020wlm} implemented using an LSTM recurrent neural network architecture~\cite{graves2014towards}. The LSTM layer in this model is supported by FlexASR.
% It is given as an example in PyTorch's official example repository~\cite{pt2020wlm}.
% We chose this model because it contains an LSTM layer that could be accelerated by FlexASR.
%As detailed in Appendix~\ref{app.models}, 

% \yl{Current codegen has already supported retrieving hidden state and cell state}
% Note that we removed the unused final hidden and cell state return values so that the custom runtime deals only with a single output.
%simplifying the custom runtime's development
%by only dealing with a single output.
%to keep this application with a single output and to simplify the custom runtime development.
%Note that we made a small modification to this model in order to simplify the design of our BYOC-based code generator; namely, to avoid having to deal with multiple outputs, we eliminated the final hidden and cell state return values (which were not needed in this application).
%We made a small modification to the model after importing into Relay to match the semantics of our FlexASR LSTM code generator, namely by not returning the final hidden and cell states (unused in this application).
% We chose this model due to its simplicity and the fact its main computation could be accelerated by FlexASR's LSTM layer.

\item \textbf{MobileNet-V2} is a common CNN designed for mobile applications~\cite{howard2017mobilenets, sandler2019mobilenetv2}. We chose MobileNet-V2 due to its wide use, especially on embedded devices.
% \item \textbf{MobileNet-V2} is a common CNN designed for mobile and embedded vision applications that uses depth-wise separable convolutions~\cite{howard2017mobilenets, sandler2019mobilenetv2}. We chose MobileNet due to its wide use, especially on embedded devices. %  \hl{and in 8-bit quantized settings (corresponding to VTA's 8-bit hardware)}~\cite{jacob2017quantization}.

\item \textbf{ResMLP} is a recent residual network for image classification, comprised only of multi-layer perceptrons ~\cite{touvron2021resmlp}. Its linear layers could be accelerated by VTA and FlexASR.
% We chose ResMLP for its preponderance of linear layers that could be accelerated by VTA and FlexASR (despite not being a language model).

% \item \textbf{ResMLP} is a recent residual network, designed for image classification, comprised only of multi-layer perceptrons and no convolutional layers~\cite{touvron2021resmlp}.
% We chose ResMLP for its preponderance of linear layers that could be accelerated by VTA and FlexASR (despite not being a language model).

\item \textbf{Transformer} is an NLP model comprised primarily of attention mechanisms~\cite{vaswani2017attention}. 
% which outperformed earlier models that featured more complex recurrent cell structures~\cite{vaswani2017attention}.
We chose Transformer as a representative of recent popular NLP models. 
% which FlexASR is specialized for. <- unfortunately, it was not designed for them

\item \textbf{ResNet} is a popular CNN designed for image classification~\cite{he2016deep}. 
Besides ResNet-20, which we use in most of the evaluation, in \S\ref{sec.compilation-stats}, we additionally compare various implementations of ResNet-50 from MLPerf~\cite{mlperf} for its availability of different reference implementations.
%We use ResNet-20 for our end-to-end testing because its small size reduces simulation times; in \S\ref{sec.compilation-stats} we use ResNet-50 for comparing multiple different implementations because of the easy availability of its reference implementations.


%\item \textbf{Q-MobileNet} is a quantized variant of the MobileNet neural network~\cite{howard2017mobilenets}.

\end{enumerate}
%
% With the exception of the minor change for LSTM-WLM's unused outputs, 
All applications were mapped to accelerators \emph{without any manual modifications}.
%\hl{(Decide if the appendix is necessary.)} 
%Detailed parameters for these applications are given in Appendix~\ref{app.models}.