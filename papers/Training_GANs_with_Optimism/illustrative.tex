
We consider the following very simple WGAN example: The data are generated by a multivariate normal distribution, i.e. $Q \triangleq N(v, I)$ for some $v\in \mathbb{R}^d$. The goal is for the generator to learn the unknown parameter $v$. In  Appendix \ref{sec:covariance} we also consider a more complex example where the generator is trying to learn a co-variance matrix. 

We consider a WGAN, where the discriminator is a linear function and the generator is a simple additive displacement of the input noise $z$, which is drawn from $F\triangleq N(0, I)$, i.e:
\begin{equation}
\begin{aligned}
D_w(x) =~& \langle w, x\rangle\\
G_{\theta}(z) =~& z + \theta
\end{aligned}
\end{equation}
The goal of the generator is to figure out the true distribution, i.e. to converge to $\theta = v$. The WGAN loss then takes the simple form:
\begin{equation}
L(\theta, w) =  \mathbb{E}_{x\sim N(v, I)}\left[ \langle w, x \rangle \right] - \mathbb{E}_{z\sim N(0,I)}\left[\langle w, z + \theta \rangle\right] 
\end{equation}
We first consider the case where we optimize the true expectations above rather than assuming that we only get samples of $x$ and samples of $z$. Due to linearity of expectation, the expected zero-sum game takes the form:
\begin{equation}
\inf_{\theta} \sup_{w}~\langle w, v-\theta \rangle
\end{equation}
We see here that the unique equilibrium of the above game is for the generator to choose $\theta=v$ and for the discriminator to choose $w = 0$. For this simple zero sum game, we have $\nabla_{w, t}=v-\theta_t$ and $\nabla_{\theta, t}=-w_t$. Hence, the GD dynamics take the form:
\begin{equation}
\begin{aligned}
w_{t+1} =& w_{t} + \eta (v - \theta_{t})\\
\theta_{t+1} =& \theta_{t} + \eta w_t  
\end{aligned}\tag{GD Dynamics for Learning Means}
\end{equation}
while the OMD dynamics take the form:
\begin{equation}
\begin{aligned}
w_{t+1} =& w_{t} + 2\eta\cdot (v - \theta_{t}) - \eta \cdot (v-\theta_{t-1})\\
\theta_{t+1} =& \theta_{t} + 2\eta\cdot w_t - \eta\cdot  w_{t-1} 
\end{aligned}\tag{OMD Dynamics for Learning Means}
\end{equation}

We simulated simultaneous training in this zero-sum game under the GD and under OMD dynamics and we find that GD dynamics always lead to a limit cycle irrespective of the step size or other modifications. In Figure \ref{fig:gd} we present the behavior of the GD vs OMD dynamics in this game for $v = (3, 4)$. We see that even though GD dynamics leads to a limit cycle (whose average does indeed equal to the true vector), the OMD dynamics converge to $v$ in terms of the last iterate. In Figure \ref{fig:sampling} we see that the stability of OMD even carries over to the case of Stochastic Gradients, as long as the batch size is of decent size. 

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[height=.7in]{sgd_example.png}
        \caption{GD dynamics.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[height=.7in]{omd_example.png}
        \caption{OMD dynamics.}
    \end{subfigure}
    \caption{Training GAN with GD converges to a limit cycle that oscilates around the equilibrium (we applied weight-clipping at $10$ for the discriminator). On the contrary training with OMD converges to equilibrium in terms of last-iterate convergence.}\label{fig:gd}
\end{figure}

In the appendix we also portray the behavior of the GD dynamics even when we add gradient penalty \citep{Gulrajani2017} to the game loss (instead of weight clipping), adding Nesterov momentum to the GD update rule \citep{Nesterov} or when we train the discriminator multiple times in between a train iteration of the generator. We see that even though these modifications do improve the stability of the GD dynamics, in the sense that they narrow the band of the limit cycle, they still lead to a non-vanishing limit cycle, unlike OMD. 

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[height=.7in]{stochastic_omd_batch_50.png}
        \caption{Stochastic OMD dynamics with mini-batch of $50$.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[height=.7in]{stochastic_omd_batch_200.png}
        \caption{Stochastic OMD dynamics with mini-batch of $200$.}
    \end{subfigure}
    \caption{Robustness of last-iterate convergence of OMD to stochastic gradients.}\label{fig:sampling}
\end{figure}

In the next section, we will in fact prove formally that for a large class of zero-sum games including the one presented in this section, OMD dynamics converge to  equilibrium in the sense of last-iterate convergence, as opposed to average-iterate convergence.