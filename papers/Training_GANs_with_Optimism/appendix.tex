\section{Variants of GD Training}\label{sec:GD-variants}

For ease of reference we briefly describe the exact form of update rules for several modifications of GD training that we have used in our experimental results. 

Adagrad: 
\begin{equation}\label{eqn:GDadaptive}
\begin{aligned}
\eta_{w,t} = ~&\frac{\eta}{\sqrt{\sum_{i=1}^t \nabla_{w,i}^2} + \epsilon} ~~~~&~~~~ 
\eta_{\theta,t} = ~&\frac{\eta}{\sqrt{\sum_{i=1}^t \nabla_{\theta,i}^2} + \epsilon} \\
w_{t+1} =~& w_t + \eta_{w,t} \cdot \nabla_{w,t}
~~~~&~~~~
\theta_{t+1} =~& \theta_{t} - \eta_{\theta,t} \cdot \nabla_{\theta,t}
\end{aligned}
\end{equation}

Momentum:
\begin{equation}\label{eqn:GDMomentum}
\begin{aligned}
v_{w, t+1} =~& \gamma \cdot v_{w, t} + \eta \cdot \nabla_{w,t} 
~~~~&~~~~
v_{\theta, t+1} =~& \gamma \cdot v_{\theta, t} + \eta \cdot \nabla_{\theta,t} \\
w_{t+1} =~& w_t + v_{w, t+1}
~~~~&~~~~
\theta_{t+1} =~& \theta_t - v_{\theta, t+1}
\end{aligned}
\end{equation}

Nesterov momentum:
\begin{equation}\label{eqn:GDNesterov}
\begin{aligned}
w_{\text{ahead}} =~& w_t + \gamma \cdot v_{w,t} 
~~~~&~~~~
\theta_{\text{ahead}} =~& \theta_t - \gamma \cdot v_{\theta,t} \\
v_{w, t+1} =~& \gamma \cdot v_{w, t} + \eta \cdot \nabla_{w} L(\theta_t, w_{\text{ahead}}) 
~~~~&~~~~
v_{\theta,t+1} =~& \gamma \cdot  v_{\theta,t} + \eta \cdot \nabla_{\theta} L(\theta_{\text{ahead}}, w_t) \\
w_{t+1} =~& w_t + v_{w, t+1}
~~~~&~~~~
\theta_{t+1} =~& \theta_t - v_{\theta,t+1}
\end{aligned}
\end{equation}


\section{Persistence of Limit Cycles in GD Training}
In Figure \ref{fig:persistence} we portray example Gradient Descent dynamics in the illustrative example described in Section \ref{sec:illustrative} under multiple adaptations proposed in the literature. We observe that oscillations persist in all such modified GD dynamics, though alleviated by some. We briefly describe the modifications in detail first.

\paragraph{Gradient penalty.} The Wasserstein GAN is based on the idea that the discriminator is approximating all $1$-Lipschitz functions of the data. Hence, when training the discriminator we need to make sure that the function $D_{w}(x)$ has a bounded gradient with respect to $x$. One approach to achieving this is weight-clipping, i.e. clipping the weights to lie in some interval. However, the latter might introduce extra instability during training. \cite{Gulrajani2017} introduce an alternative approach by adding a penalty to the loss function of the zero-sum game that is essentially the $\ell_2$ norm of the gradient of $D_{w}(x)$ with respect to $x$. In particular they propose the following regularized WGAN loss:
\begin{equation*}
L_{\lambda}(\theta, w) = \E_{x\sim Q}\left[ D_w(x) \right] - \E_{z\sim F}\left[D_w(G_\theta(z))\right] - \lambda \E_{\hat{x} \sim Q_{\epsilon}}\left[\left(\|\nabla_{x} D_w(\hat{x})\|-1\right)^2\right]
\end{equation*}
where $Q_{\epsilon}$ is the distribution of the random vector $\epsilon x + (1-\epsilon) G(z)$ when $x\sim Q$ and $z\sim F$. The expectations in the latter can also be replaced with sample estimates in stochastic variants of the training algorithms.

For our simple example, $\nabla_{x} D_w(x) = w$. Hence, we get the gradient penalty modified WGAN:
\begin{equation}
L_{\lambda}(\theta, w) = \langle w, v-\theta \rangle - \lambda \left(\|w\|-1\right)^2
\end{equation}
Hence, the gradient of the modified loss function with respect to $\theta$ remains unchanged, but the gradient with respect to $w$ becomes:
\begin{equation}
\nabla_{wt} = v-\theta_t - 2\lambda w_{t} \frac{\|w_t\|_2 -1}{\|w_t\|_2}
\end{equation}

\paragraph{Momentum.} GD with momentum was defined in Equation \eqref{eqn:GDMomentum}. For the case of the simple illustrative example, these dynamics boil down to:
\begin{equation}
\begin{aligned}
m_{w, t+1} =& \gamma \cdot m_{w, t} + \eta\cdot (v-\theta_{t})
~~~~&~~~~
m_{\theta, t+1} =& \gamma\cdot m_{\theta, t} - \eta\cdot  w_t\\
w_{t+1} =& w_{t} + m_{w, t+1}
~~~~&~~~~
\theta_{t+1} =& \theta_{t} - m_{\theta, t+1} 
\end{aligned}
\end{equation}

\paragraph{Nesterov momentum.} GD with Nesterov's momentum was defined in Equation \eqref{eqn:GDNesterov}. For the illustrative example, we see that Nesterov's momentum is identical to momentum in the absence of gradient penalty. The reason being that the function is bi-linear. However, with a gradient penalty, Nesterov's momentum boils down to the following update rule. 
\begin{equation}
\begin{aligned}
\hat{w}_t =& w_t + \gamma\cdot m_{w,t}
~~~~&~~~~ &\\
m_{w, t+1} =& \gamma\cdot m_{w, t} + \eta\cdot (v-\theta_{t}) - 2 \eta\cdot \lambda \hat{w}_t \frac{\|\hat{w}_t\|_2 -1}{\|\hat{w}_t\|_2}
~~~~&~~~~
m_{\theta, t+1} =& \gamma \cdot m_{w, t} - \eta \cdot w_t\\
w_{t+1} =& w_{t} + m_{w, t+1}
~~~~&~~~~
\theta_{t+1} =& \theta_{t} - m_{\theta,t+1}
\end{aligned}
\end{equation}

\paragraph{Asymmetric training.} Another approach to reducing cycling is to train the discriminator more frequently than the generator. Observe that if we could exactly solve the supremum problem of the discriminator after every iteration of the generator, then the generator would be simply solving a convex minimization problem and GD should converge point-wise. The latter approach could lead to slow convergence given the finiteness of samples in the case of stochastic training. Hence, we cannot really afford completely solving the discriminators problem. However, training the discriminator for multiple iterations, brings the problem faced by the generator closer to convex minimization rather than solving an equilibrium problem. Hence, asymmetric training could help with cycling. We observe below that asymmetric training is the most effective modification in reducing the range of the cycles and hence making the last-iterate be close to the equilibrium. However, it does not really eliminate the cycles, rather it simply makes their range smaller.
\newpage
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[height=1.4in]{gradient_penalty_example.png}
        \caption{GD dynamics with a gradient penalty added to the loss. $\eta=0.1$ and $\lambda=0.1$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[height=1.4in]{momentum_example.png}
        \caption{GD dynamics with momentum. $\eta=0.1$ and $\gamma=0.5$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[height=1.4in]{gd_momentum_and_penalty.png}
        \caption{GD dynamics with momentum and gradient penalty. $\eta=.1$, $\gamma=0.2$ and $\lambda=0.1$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[height=1.4in]{gd_momentum_and_penalty_train_every_15.png}
        \caption{GD dynamics with momentum and gradient penalty, training generator every $15$ training iterations of the discriminator. $\eta=.1$, $\gamma=0.2$ and $\lambda=0.1$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[height=1.4in]{gd_momentum_and_penalty_train_every_15.png}
        \caption{GD dynamics with Nesterov momentum and gradient penalty, training generator every $15$ training iterations of the discriminator. $\eta=.1$, $\gamma=0.2$ and $\lambda=0.1$.}
    \end{subfigure}
    \caption{Persistence of limit cycles in multiple variants of GD training.}\label{fig:persistence}
\end{figure}