\begin{table}[H]
\label{table:arch}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Operation}  &\multicolumn{1}{c}{\bf Kernel} &\multicolumn{1}{c}{\bf Output Shape} &\multicolumn{1}{c}{\bf BatchNorm?} &\multicolumn{1}{c}{\bf Nonlinearity}
\\ \hline \\
Gradient penalty: $\lambda=10$\\
Batch size: 64 \\
$G(z):$ \\
$z$ & - & 100 & - & - \\
Fully connected         & - & 1024 & no & LeakyReLU\\
Fully connected             & - & $8192$  & yes & LeakyReLU \\
Reshape  & - & $ 128 \times 8 \times 8 $ & - & -\\
TransposedConv             & $[5\times 5]\times 128$ & $128\times16\times 16$ & yes & LeakyReLU  \\
Convolution             & $[5\times 5]\times 64$ & $64\times16\times 16$ & yes & LeakyReLU  \\
TransposedConv             & $[5\times 5]\times 64$ & $64\times32\times 32$ & yes & LeakyReLU  \\
Convolution             & $[5\times 5]\times 3$ & $3\times32\times 32$ & no & tanh  \\
 
 $D(x):$ \\
$x$   & - & $3 \times  32 \times 32$  &- &-\\
  Convolution             & $[5\times 5] \times 64$ &  $64\times32\times32$ & no & LeakyReLU  \\
   Convolution             & $[5\times 5] \times 128$ &  $128\times14\times 14$ & no & LeakyReLU  \\
    Convolution             & $[5\times 5] \times 128$ &  $128\times 7\times 7$ & no & LeakyReLU  \\
  Fully connected             & - & 1024  & no & LeakyReLU \\
  Fully connected             & - & 1  & no & linear \\
\end{tabular}
\end{center}
\end{table}

\newpage