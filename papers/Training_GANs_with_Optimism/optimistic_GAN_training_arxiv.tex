\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference_arxiv,times}
\iclrfinaltrue

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{url}

%% packages
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs} % For formal tables
%\usepackage{color}
\usepackage{nicefrac}
%\usepackage{graphicx}
%% HERE arxiv
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{enumitem}


%\usepackage[]{color-edits}
\usepackage[suppress]{color-edits}
\addauthor{vs}{blue}
\addauthor{cd}{red}
\addauthor{hz}{brown}
\addauthor{ai}{pink}



\definecolor{amber}{rgb}{1.0, 0.01, 0.5}

\newcount\Comments 
\Comments=1
\newcommand{\kibitz}[2]{\ifnum\Comments=1{\color{#1}{#2}}\fi}
\newcommand{\zf}[1]{\kibitz{amber}{[ZF: #1]}}
\newcommand{\cp}[1]{\kibitz{red}{[CP: #1]}}
\newcommand{\vs}[1]{\kibitz{blue}{[VS: #1]}}
\newcommand{\todo}[1]{\kibitz{blue}{[TODO: #1]}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\arginf}{arg\,inf}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\st}{\sum{t=1}^{T}}
\newcommand{\eps}{\epsilon}
\newcommand{\G}{\mathcal{G}^\eps}
\newcommand{\kk}{\textbf{k}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\D}{\mathcal D}
\newcommand{\mbf}{\mathbf}
\newcommand{\p}{\mathbf{p}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\g}{\textbf{g}}
\newcommand{\w}{\mbf{w}}
\newcommand{\winexp}{\textsc{WIN-EXP}}
\newcommand{\winexpG}{\textsc{WIN-EXP-G}}
\renewcommand{\Pr}{\ensuremath{\mathrm{Pr}}}
\newcommand{\opt}{\ensuremath{\textsc{OPT}}}
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}

%% HERE arXiv
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[]{Example}
\newenvironment{prevproof}[2]{\noindent {\em {Proof of {#1}~\ref{#2}:}}}{$\hfill\qed$\vskip \belowdisplayskip}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}



\newcommand{\sgamma}{\ensuremath{\sqrt{\gamma}}}
\newcommand{\inner}[3]{\ensuremath{\langle {#1}, {#2} \rangle_{{#3}}}}
\newcommand{\innerab}[3]{\ensuremath{\inner{{#1}}{{#2}}{AM_{{#3}}^T}}}
\newcommand{\innerac}[3]{\ensuremath{\inner{{#1}}{{#2}}{A^TN_{{#3}}^T}}}
\newcommand{\inneraccostas}[3]{\ensuremath{\inner{{#1}}{{#2}}{A^TN_{{#3}}}}}

\newcommand{\norm}[2]{\ensuremath{\left|\left|{#1}\right|\right|^2_{{#2}}}}

\newcommand{\norma}[1]{\ensuremath{\left|\left|{#1}\right|\right|^2_{A^TA}}}
\newcommand{\normat}[1]{\ensuremath{\left|\left|{#1}\right|\right|^2_{AA^T}}}
\newcommand{\normab}[2]{\ensuremath{\norm{{#1}}{AM_{{#2}}^T}}}
\newcommand{\normac}[2]{\ensuremath{\norm{{#1}}{A^TN_{{#2}}^T}}}

\newcommand{\normabb}[2]{\ensuremath{\normm{{#1}}{AM_{{#2}}^T}}}
\newcommand{\normacc}[2]{\ensuremath{\normm{{#1}}{A^TN_{{#2}}^T}}}
\newcommand{\normm}[2]{\ensuremath{\left|\left|{#1}\right|\right|_{{#2}}}}


\newcommand{\normlt}[1]{\ensuremath{\left|\left|{#1}\right|\right|^2_2}}

\newcommand{\x}[3]{\ensuremath{{#1}x_{t{#2}} - 2\eta{#1}Ay_{t{#2}} +
\eta{#1}Ay_{t{#3}}}}
\newcommand{\xto}{\ensuremath{\x{}{-1}{-2}}}
\newcommand{\xtt}{\ensuremath{\x{}{-2}{-3}}}
\newcommand{\xtao}{\ensuremath{\x{A^T}{-1}{-2}}}

\newcommand{\y}[3]{\ensuremath{{#1}y_{t{#2}} + 2\eta{#1}A^Tx_{t{#2}} -
\eta{#1}A^Tx_{t{#3}}}}
\newcommand{\yto}{\ensuremath{\y{}{-1}{-2}}}
\newcommand{\ytt}{\ensuremath{\y{}{-2}{-3}}}
\newcommand{\ytao}{\ensuremath{\y{A}{-1}{-2}}}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\def\[#1\]{\begin{align*}#1\end{align*}}
\def\(#1\){\ensuremath{\left(#1\right)}}


\begin{document}
% \nipsfinalcopy is no longer used


\title{Training GANs with Optimism}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{Constantinos Daskalakis\thanks{These authors contribute equally to this work.}\\
MIT, EECS\\
\texttt{costis@mit.edu}\\
\And  
Andrew Ilyas\samethanks\\
MIT, EECS\\
\texttt{ailyas@mit.edu}\\
\And
Vasilis Syrgkanis\samethanks\\
Microsoft Research\\
\texttt{vasy@microsoft.com}\\
\AND	
Haoyang Zeng\samethanks\\
MIT, EECS\\
\texttt{haoyangz@mit.edu}}

\footnotetext[1]{Code for our models is available at \url{https://github.com/vsyrgkanis/optimistic_GAN_training}}

\date{\today}
\maketitle

\begin{abstract}
We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics.  Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.
\end{abstract}

\section{Introduction}
\input{introduction}

\section{Preliminaries: WGANs and Optimistic Mirror Descent}
\input{preliminaries}

\section{An Illustrative Example: Learning the Mean of a Distribution}\label{sec:illustrative}
\input{illustrative}

\section{Last-Iterate Convergence of Optimistic Adversarial Training}
\input{last_iterate_theorem}

\section{Experimental Results for Generating DNA Sequences}
\input{experimental}
\vspace{-.1in}
\section{Generating Images from CIFAR10 with Optimistic Adam}\label{sec:cifar10}
\input{cifar10}

\bibliographystyle{iclr2018_conference}
\bibliography{agt}

\newpage

\appendix

\input{appendix}

\section{Another Example: Learning a Co-Variance Matrix}\label{sec:covariance}
\input{covariance}

\section{Last Iterate Convergence of OMD in Bilinear Case}\label{sec:appendix:last-iterate}
\input{last-iterate}

\section{DNA-Generation WGAN Architecture}\label{sec:appendix-dna-arch}\label{sec:apdxdna}
\input{appendix-DNA_arch}

\section{CIFAR10 WGAN Architecture}\label{sec:appendix-cifar10-arch}
\input{appendix-cifar10_architecture}

\section{CIFAR10 Generator Image Samples}\label{sec:appendix-cifar10}
\input{appendix-cifar10}

\section{CIFAR10 Adam vs. Optimistic Adam Comparison}\label{sec:appendix-errorbars}
\input{appendix-errorbars}


\end{document}
