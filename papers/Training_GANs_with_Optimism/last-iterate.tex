 \label{sec:bilinear OMD convergence}

The goal of this section is to show that Optimistic Mirror Descent exhibits last iterate convergence to min-max solutions for bilinear functions. In Section~\ref{app:proof of special case minmax}, we provide the proof of Theorem~\ref{thm:convergence of OGD-main}, that OMD exhibits last iterate convergence to min-max solutions of the following min-max problem
\begin{align}
 \min_x \max_y x^T A y, \label{eq:our minmax}
\end{align}
where $A$ is an abitrary matrix and $x$ and $y$ are unconstrained. In Section~\ref{app:proof of general case minmax}, we state the appropriate extension of our theorem to the general case:
\begin{align}
    \inf_{x} \sup_{y} \left(x^TAy + b^Tx + c^Ty + d\right). \label{eq:general inf sup}
\end{align}

\subsection{Proof of Theorem~\ref{thm:convergence of OGD-main}} \label{app:proof of special case minmax}

\smallskip As stated in Section~\ref{sec:main:proof OMD converges}, for the min-max problem~\eqref{eq:our minmax}
%
%In this section, we show that Optimistic Gradient Descent  
%exhibits final-iterate, rather than only average-iterate convergence to min-max solutions for
%bilinear functions. 
%%In particular, we show that the $\ell_2$ norms of the gradients used by the dynamics shrinks in time.
%More precisely, we consider the problem $\min_x \max_y x^T A y$, for some matrix $A$, where $x$ and $y$ are unconstrained. 
Optimistic Mirror Descent takes the following form, for all $t \ge 1$:
\begin{align}
    x_{t} &= \xto \label{eq:OGD bilinear x}\\
    y_{t} &= \yto  \label{eq:OGD bilinear y}
\end{align}
where for the above iterations to be meaningful we need to specify $x_0,x_{-1},y_0,y_{-1}$. 

\smallskip As stated in Section~\ref{sec:main:proof OMD converges} we allow any initialization $x_0 \in \mathcal{R}(A)$, and $y_0\in\mathcal{R}(A^T)$, and set $x_{-1}=2x_0$ and $y_{-1}=2y_{0}$, where ${\cal R}(\cdot)$ represents column space. In particular, our initialization means that the first step taken by the dynamics gives $x_1=x_0$ and $y_1=y_0$.

Before giving our proof of Theorem~\ref{thm:convergence of OGD-main}, we need some further notation.
%\smallskip We will analyze Optimistic Gradient Descent under the assumption $\lambda_{\infty} \le 1$, where $\lambda_{\infty}=\max\{||A||,||A^T||\}$ and $||\cdot||$ denotes spectral norm of matrices. We can always enforce that $\lambda_{\infty} \le 1$ by appropriately scaling $A$. Scaling $A$ by some positive factor clearly does not change the min-max solutions $(x^*,y^*)$, only scales the optimal value $x^{*T}Ay^*$ by the same factor.
%
%\begin{remark}
%We remark that $(x,y)=(0,0)$ is always a solution to $\min_x \max_y x^T A y$. More generally, the solutions to the problem are pairs $(x,y)$ such that $x$ is in the null space of $A^T$ and $y$ is in the null space of $A$. In particular, finding a solution to $\min_x \max_y x^T A y$ is a trivial problem. So this section only serves the purpose of rigorously showing that Optimistic Gradient Descent converges to a min-max solution. This is interesting in light of the fact that Gradient Descent actually diverges, even in the special case where $A$ is the identity matrix, as per the following proposition whose proof is provided in Appendix~\ref{appendix:omitted proofs}.
%
%
%{\begin{proposition} \label{prop:gradient descent unstable}
%Gradient descent applied to problem $\min_x \max_y x^T A y$ diverges from any initialization $x_0, y_0$ such that $x_0,y_0 \neq 0$.
%\end{proposition}}
%\end{remark}
%
%
%\paragraph{Notation.} We start with some notation that will be handy later on. 
For all $i \in \mathbb{N}$, we set:
\begin{align*}
      &~~~~~~~~~~M_i = A^j(A^TA)^k, N_i = \(A^T\)^j \(AA^T\)^k \\ 
    &~~~~~~~~~~\Delta^i_t = \normlt{N_iAy_t} + \normlt{M_iA^Tx_t}\\
		&\text{where $k \in \mathbb{Z}$ and $j \in \{0, 1\}$ are such that:~}   i = 2k + j.
\end{align*}
\noindent With this notation, $\Delta_t^0 =
\normlt{A^Tx_t} + \normlt{Ay_t}$, $\Delta^1_t = \normlt{AA^Tx_t} +
\normlt{A^TAy_t}$, $\Delta^2_t = \normlt{A^TAA^Tx_t} +
\normlt{AA^TAy_t}$, etc.

We also use the notation $\langle u, v \rangle_X = u^TXX^Tv$, for vectors $u, v
\in \mathbb{R}^d$ and square $d \times d$ matrices $X$. We similarly define the
norm notation $||u||_X=\sqrt{\langle u, u \rangle_X}$. Given our notation, we
have the following claim, shown in Appendix~\ref{appendix:omitted proofs}.
\begin{claim} \label{claim:pushing A's around}
For all matrices $A$ and vectors $u,v$ of the appropriate dimensions:\\
$$\innerab{Au}{Av}{i} = \innerac{u}{v}{i+1};~~\innerac{A^Tu}{A^Tv}{i} = \innerab{u}{v}{i+1};~~\innerab{u}{Av}{i} = \innerac{v}{A^Tu}{i}.$$
\end{claim}


With our notation in place, we show (through iterated expansion of the update rule), the
following lemma, proved in Appendix~\ref{appendix:omitted proofs}:
\begin{lemma}  \label{lemma:first bound}
For the dynamics of Eq.~\eqref{eq:OGD bilinear x} and~\eqref{eq:OGD bilinear y} and any initialization ${1 \over 2}x_{-1}=x_0 \in
\mathcal{R}(A)$, and ${1 \over 2}y_{-1}=y_0\in\mathcal{R}(A^T)$ we have the following for all $i, t \in \mathbb{N}$ such that $i\ge 0$ and $t \ge 2$:
$$\Delta^i_t - \Delta^i_{t-1} = 4\eta^2\Delta^{i+1}_{t-1} -
5\eta^2\Delta^{i+1}_{t-2} - 2\eta^3\(\innerab{x_{t-2}}{Ay_{t-4}}{i+1} -
\innerac{y_{t-2}}{A^Tx_{t-4}}{i+1}\).$$
\end{lemma}


\medskip We are ready to prove Theorem~\ref{thm:convergence of OGD-main}. Its proof is implied by the following stronger theorem, and Corollary~\ref{cor:gradient becomes small}.

%\begin{theorem}\label{thm:convergence of OGD}
%Consider the dynamics of Eq.~\eqref{eq:OGD bilinear x} and~\eqref{eq:OGD bilinear y} and any initialization ${1 \over 2}x_{-1}=x_0 \in
%\mathcal{R}(A)$, and ${1 \over 2}y_{-1}=y_0\in\mathcal{R}(A^T)$. Let $\gamma = \max\(\left|\left|\(AA^T\)^{+}\right|\right|,
	    %\left|\left|\(A^TA\)^{+}\right|\right|\)$, where for a matrix $X$ we denote by	$X^{+}$ its generalized inverse and by $||X||$ its
    %spectral norm. Suppose that $\max\{||A||,||A^T||\}\equiv \lambda_{\infty}\le 1$ and $\eta<\gamma$. Then, for all $i \in \mathbb{N}$:
		%\begin{align}
		%\Delta^i_1 = \Delta^i_{0}, \label{eq:target condition 1}
		%\end{align}
		%and, for all $i,t\in \mathbb{N}$ such that $t \ge 2$, the following condition holds:
%\begin{align}
    %H(i,t):~~\Delta^i_t \leq \left(1-{\eta}\right)\Delta^i_{t-1} + O(\eta^3 \Delta^0_0). \label{eq:target condition 2}
%\end{align}
%\end{theorem}

\begin{theorem}\label{thm:convergence of OGD}
Consider the dynamics of Eq.~\eqref{eq:OGD bilinear x} and~\eqref{eq:OGD bilinear y} and any initialization ${1 \over 2}x_{-1}=x_0 \in
\mathcal{R}(A)$, and ${1 \over 2}y_{-1}=y_0\in\mathcal{R}(A^T)$. Let $$\gamma = \max\(\left|\left|\(AA^T\)^{+}\right|\right|,
	    \left|\left|\(A^TA\)^{+}\right|\right|\),$$ where for a matrix $X$ we denote by	$X^{+}$ its generalized inverse and by $||X||$ its
    spectral norm. Suppose that $\max\{||A||,||A^T||\}\equiv \lambda_{\infty}\le 1$ and $\eta$ is a small enough constant satisfying $\eta <1/(3\gamma^2)$. Then, for all $i \in \mathbb{N}$:
		\begin{align}
		\Delta^i_1 = \Delta^i_{0}, \label{eq:target condition 1}\\
		\Delta^i_2 \le (1+\eta)^2\Delta^i_{0}, \label{eq:target condition 1.5}
		\end{align}
		and, for all $i,t\in \mathbb{N}$ such that $t \ge 3$, the following condition holds:
\begin{align}
    H(i,t):~~\Delta^i_t \leq \left(1-{\eta^2 \over \gamma^2}\right)\Delta^i_{t-1} + 16\eta^3 \Delta^0_0. \label{eq:target condition 2}
\end{align}
\end{theorem}

\begin{proof}
Eq.~\eqref{eq:target condition 1} holds trivially as under our initialization $x_1=x_0$ and $y_1=y_0$. Eq.~\eqref{eq:target condition 1.5} is also easy to show by noticing the following. Given our initialization:
\begin{align*}
x_2=x_0-\eta A y_0\\
y_2=y_0+\eta A x_0
\end{align*}
Hence (using $j=i \mod 2$):
\begin{align}
M_iA^Tx_2=M_iA^Tx_0 - \eta M_iA^TA y_0\\~~~~~~~~~~~~\Rightarrow~~||M_iA^Tx_2||_2 &\le ||M_iA^Tx_0||_2+\eta ||M_iA^TA y_0||_2\\
&= ||M_iA^Tx_0||_2+\eta ||A^j(A^T)^{1-j}N_iA y_0||_2\\
&\le ||M_iA^Tx_0||_2+\eta \lambda_{\infty} ||N_iA y_0||_2\\
&\le ||M_iA^Tx_0||_2+\eta  ||N_iA y_0||_2 \label{eq:kourasi1}
\end{align}
Similarly:
\begin{align}
||N_iAy_2||_2 &\le ||N_iAy_0||_2+\eta  ||M_iA^T x_0||_2 \label{eq:kourasi2}
\end{align}
It follows from~\eqref{eq:kourasi1} and~\eqref{eq:kourasi2} that
$$\Delta^i_2 \le (1+\eta)^2 \Delta^i_0.$$

We use induction on $t$ to prove~\eqref{eq:target condition 2}. We start our proof by showing the inductive step, and postpone establishing the basis of our induction to the end of this proof. For the inductive step, we assume that $H(i,\tau)$ holds for all $i \ge 0$ and $1\le \tau < t$, for some $t>3$. Assuming this, we show next that $H(i,t)$ holds for all $i$. To do this, we make use of a few lemmas, whose proofs are given in Appendix~\ref{appendix:omitted proofs}. 
	
    \begin{lemma} \label{lemma:restated bound} Under the conditions of the theorem, for all $i \ge 0, t \ge 2$:
		%$$\Delta^i_t - \Delta^i_{t-1} \leq 4\eta^2\Delta_{t-1}^{i+1} -
    %5\eta^2\Delta_{t-2}^{i+1} +2\eta^3(9\Delta_{t-2}^{i+1} + 8\eta^2 \Delta_{t-3}^{i+1} + 8\eta^2 \Delta_{t-4}^{i+1} + 8\eta^2 \Delta_{t-5}^{i+1}).$$
$$\Delta^i_t - \Delta^i_{t-1} \leq 4\eta^2\Delta^{i+1}_{t-1} - 5\eta^2\Delta^{i+1}_{t-2} +
    \eta^3(\Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1}).$$

		
    %$$\Delta^i_t - \Delta^i_{t-1} \leq 4\eta^2\Delta_{t-1}^{i+1} -
    %5\eta^2\Delta_{t-2}^{i+1} +
    %\eta^3\((1+\lambda_\infty)\Delta_{t-2}^{i+1} +
    %4\eta\lambda^2_\infty\Delta_{0}^{i+1}\).$$
    \end{lemma}
		
		\begin{lemma} \label{lemma:semi-trivial upper bound}
		Under the conditions of the theorem, for all $i, t \ge 0$: $\Delta_t^{i+1} \le \Delta_t^{i}$.
		\end{lemma}

    \begin{lemma} \label{lemma:lower bound}  Under the conditions of the theorem, for all $i \ge 0, t \ge 0$:
	$$\Delta_{t}^{i+2} \geq \frac{1}{\gamma^2}\Delta_{t}^{i}.$$
    \end{lemma}
    
    Given these lemmas, we show our inductive step. So for $t \ge 4$:
		  \begin{align}
	\Delta^i_t - \Delta^i_{t-1} &\leq 4\eta^2\Delta^{i+1}_{t-1} - 5\eta^2\Delta^{i+1}_{t-2} +
    \eta^3(\Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1}) \label{eq:derivation1}\\
	&\leq -\eta^2 \Delta_{t-1}^{i+1} + \eta^3(\Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1})+ 80 \eta^5 \Delta_0^0 \\
	%&\leq \eta^2(2\eta-1)\Delta^{i+1}_{t-2} +
	%4\eta^4\Delta_0^0 +O(\eta^5 \Delta^0_0)\\
	%&= -\eta^2\Delta^{i+1}_{t-2} + 2\eta^3\Delta_{t-2}^{i+1} +O(\eta^5\Delta^0_0)\\
&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + \eta^3(\Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1})+ 80 \eta^5 \Delta_0^0\\
&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + \eta^3(\Delta_{t-2}^{0} + \Delta_{t-4}^{0})+ 80 \eta^5 \Delta_0^0 \\
&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + \(2\eta^3\Delta_{2}^{0} +2\eta^3 {\gamma^2 \over \eta^2}16\eta^3 \Delta^0_0\)+ 80 \eta^5 \Delta_0^0 \\
&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + \(2\eta^3(1+\eta)^2 +32\eta^3 {\gamma^2 \over \eta^2}\eta^3  + 80 \eta^5\)\Delta_0^0 \\
&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + \(2\eta^3(1+\eta)^2 +11\eta^3 + 80 \eta^5 \)\Delta_0^0 \\
	&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i-1}_{t-1} + 16\eta^3\Delta_0^0\\
	&\leq -\frac{1}{\gamma^2}\eta^2\Delta^{i}_{t-1} + 16\eta^3\Delta_0^0	\label{eq:derivation6}
    \end{align}
		
		
		  %\begin{align}
	%\Delta^i_t - \Delta^i_{t-1} &\leq 4\eta^2\Delta_{t-1}^{i+1} -
	%5\eta^2\Delta_{t-2}^{i+1} +
%2\eta^3(9\Delta_{t-2}^{i+1} + 8\eta^2 \Delta_{t-3}^{i+1} + 8\eta^2 \Delta_{t-4}^{i+1} + 8\eta^2 \Delta_{t-5}^{i+1}) \label{eq:derivation1}\\
	%&\leq \eta^2(18\eta - 1)\Delta_{t-2}^{i+1} +O(\eta^5\Delta^0_0) \\
	%%&\leq \eta^2(2\eta-1)\Delta^{i+1}_{t-2} +
	%%4\eta^4\Delta_0^0 +O(\eta^5 \Delta^0_0)\\
	%&= -\eta^2\Delta^{i+1}_{t-2} + 2\eta^3\Delta_{t-2}^{i+1} +O(\eta^5\Delta^0_0)\\
	%&\leq -\frac{1}{\gamma}\eta^2\Delta^{i}_{t-2} + 2\eta^3\Delta_0^0 +O(\eta^5\Delta^0_0)\\
	%&\leq -\frac{1}{\gamma}\eta^2\Delta^{i}_{t-1} + O(\eta^3\Delta_0^0)	\label{eq:derivation6}
    %\end{align}
    %%\begin{align}
	%\Delta^i_t - \Delta^i_{t-1} &\leq 4\eta^2\Delta_{t-1}^{i+1} -
	%5\eta^2\Delta_{t-2}^{i+1} +
	%\eta^3\((1+\lambda_\infty)\Delta_{t-2}^{i+1} +
	%4\eta\lambda^2_\infty\Delta_{0}^{i+1}\) \label{eq:derivation1}\\
	%&\leq \eta^2((1+\lambda_\infty)\eta - 1)\Delta_{t-2}^{i+1} +
	%4\eta^4\lambda_\infty^2\Delta_0^{i+1} +O(\eta^5\Delta^0_0) \\
	%&\leq \eta^2(2\eta-1)\Delta^{i+1}_{t-2} +
	%4\eta^4\Delta_0^0 +O(\eta^5 \Delta^0_0)\\
	%&= -\eta^2\Delta^{i+1}_{t-2} + \eta^3(2\Delta_{t-2}^{i+1}+4\eta\Delta_0^0) +O(\eta^5\Delta^0_0)\\
	%&\leq -\frac{1}{\gamma}\eta^2\Delta^{i}_{t-2} + 6\eta^3\Delta_0^0 +O(\eta^5\Delta^0_0)\\
	%&\leq -\frac{1}{\gamma}\eta^2\Delta^{i}_{t-1} + 6\eta^3\Delta_0^0 +O(\eta^4\Delta^0_0)	\label{eq:derivation6}
    %\end{align}
where for the first inequality we used Lemma~\ref{lemma:restated bound}, 
for the second inequality we used that $\Delta_{t-1}^{i+1} \le \Delta_{t-2}^{i+1}+16\eta^3\Delta^0_0$ (which is implied by the induction hypothesis), 
for the third inequality we used Lemma~\ref{lemma:lower bound},
for the fourth inequality we used Lemma~\ref{lemma:semi-trivial upper bound},
for the fifth inequality we applied the induction hypothesis iteratively, for the sixth inequality we used Eq.~\eqref{eq:target condition 1.5}, for the seventh and eighth inequality we used that $\eta$ is small enough, and for the last inequality we used Lemma~\ref{lemma:semi-trivial upper bound}.
%
 %and that $\Delta_{0}^{i+1} \le \Delta_{0}^{0}$ (which also easily follows from the fact that $\lambda_\infty \le 1$), 
%for the fourth inequality we used Lemma~\ref{lemma:lower bound} and that $\Delta^{i+1}_{t-2} \le \Delta_0^0 + O(\eta^2\Delta^0_0)$, which follows by first noting that $\Delta^{i+1}_{t-2} \le \Delta^{0}_{t-2}$ (which easily follows from the fact that $\lambda_\infty \le 1$) and then noting that $\Delta^{0}_{t-2}\le \Delta^{0}_{0}+O(\eta^2\Delta^0_0)$ (which follows by iteratively applying the inductive hypothesis and using~\eqref{eq:target condition 1}), and for the last inequality we used that $\Delta^{i}_{t-1}\le \Delta^{i}_{t-2}+O(\eta^3\Delta^0_0)$, which follows from the inductive hypothesis, and that $\eta < \gamma$. 
Hence:
$$\Delta^i_t \leq \(1-{\eta^2 \over \gamma^2 }\)\Delta^i_{t-1} + 16\eta^3\Delta^0_0.$$
This completes the proof of our inductive step.

It remains to show the basis of the induction, namely that $H(i,3)$ holds for all $i \in \mathbb{N}$. From Lemma~\ref{lemma:restated bound} we have:
 \begin{align}
	\Delta^i_3 - \Delta^i_{2} &\leq 4\eta^2\Delta^{i+1}_{2} - 5\eta^2\Delta^{i+1}_{1} +
    \eta^3(\Delta_{1}^{i+1} + \Delta_{-1}^{i+1})\\
 &\leq 4\eta^2\Delta^{i+1}_{2} - 5\eta^2\Delta^{i+1}_{0}+5\eta^3 \Delta^{i+1}_0\\
&= -\eta^2\Delta^{i+1}_{2}+ 5\eta^2(\Delta^{i+1}_{2}-\Delta^{i+1}_{0})+5\eta^3 \Delta^{i+1}_0\\
&\le -\eta^2\Delta^{i+1}_{2}+ 5\eta^3(2+\eta)\Delta^{i+1}_{0}+5\eta^3 \Delta^{i+1}_0\\
&= -\eta^2\Delta^{i+1}_{2}+ 5\eta^3(3+\eta)\Delta^{i+1}_{0}\\
 &\le -\eta^2\Delta^{i+1}_{2}+15\eta^3(1+\eta/3)\Delta^0_{0}\\
 &\leq -{\eta^2 \over \gamma^2}\Delta^{i-1}_{2}+15\eta^3(1+\eta/3)\Delta^0_{0}\\
 &\leq -{\eta^2 \over \gamma^2}\Delta^{i}_{2}+15\eta^3(1+\eta/3)\Delta^0_{0},
	\end{align}
where for the second equality we used that $0.5x_{-1}=x_0=x_1$ and $0.5y_{-1}=y_0=y_1$ (which follow from our initialization),
for the third inequality we used that~\eqref{eq:target condition 1.5},
for the fourth inequality we used Lemma~\ref{lemma:semi-trivial upper bound}, 
for the fifth inequality we used Lemma~\ref{lemma:lower bound}, 
and for the last inequality we used Lemma~\ref{lemma:semi-trivial upper bound}. Hence, for small enough $\eta$, we have:
$$\Delta^i_3 \le \(1-{\eta^2 \over \gamma^2}\) \Delta^i_{2} + 16 \eta^3 \Delta^0_0.$$
%To do this, we follow the  derivation in lines~\eqref{eq:derivation1}-\eqref{eq:derivation6} above, noticing that what we needed for this derivation to go through holds for $t=2$. In particular, the first inequality uses Lemma~\ref{lemma:restated bound}, which holds for $t=2$. The second inequality goes through because $\Delta_{1}^{i+1} = \Delta_{0}^{i+1}$, for all $i$, given~\eqref{eq:target condition 1}. The third inequality goes through for the same reasons that were used in the induction step. The fourth inequality goes through since $\Delta_{0}^{i+1} \geq \frac{1}{\gamma}\Delta_{0}^{i}$, which holds from Lemma~\ref{lemma:lower bound}, and $\Delta^{i+1}_{0}\le \Delta^{0}_{0}$, which holds since $\lambda_{\infty}\le 1$. The last inequality follows from the fact that $\Delta^{i}_{1} = \Delta^{i}_{0}$.
    \end{proof}

\begin{corollary} \label{cor:gradient becomes small}
Under the conditions of Theorem~\ref{thm:convergence of OGD}, $\Delta^0_t \equiv \normlt{A^Tx_t} + \normlt{Ay_t} \rightarrow O(\eta \gamma^2 \Delta^0_0)$ as $t \rightarrow +\infty$. In particular, for large enough $t$, the last iterate of OMD is within $O\(\sqrt{\eta} \cdot \gamma \sqrt{\Delta^0_0}\)$ distance from the space of equilibrium points of the game, where $\sqrt{\Delta^0_0}$ is the distance of the initial point $(x_0,y_0)$ from the equilibrium space, and where both distances are taken with respect to the norm $\sqrt{x^T A A^T x + y^T A^T A y}$.
%In particular, for large enough $t$, $x_t$ and $y_t$ are approximately optimal in the following sense:
%\begin{align}
%x_t^T A y_t \le \min_x x^T A y_t + O(\eta^2\Delta^0_0);\\
%x_t^T A y_t \ge \max_y x^T A y + O(\eta^2\Delta^0_0).
%\end{align}
\end{corollary}
\begin{prevproof}{Corollary}{cor:gradient becomes small}
It follows from~\eqref{eq:target condition 1},~\eqref{eq:target condition 1.5} and~\eqref{eq:target condition 2} that:
\begin{align*}
\Delta^0_t &\le \(1-{\eta^2 \over \gamma^2}\)^{t-2}  (1+\eta)^2 \Delta^0_0 + 16 \sum_{t=0}^{\infty}\(1-{\eta^2 \over \gamma^2}\)^t\eta^3 \Delta^0_0 \\&= \(1-{\eta^2 \over \gamma^2}\)^{t-2}  (1+\eta)^2 \Delta^0_0 + O\(\eta \gamma^2 \Delta^0_0\),
\end{align*}
which shows the first part of our claim. For the second part of our claim recall that the solutions to~\eqref{eq:our minmax} are all pairs $(x,y)$ such that $x$ is in the null space of $A^T$ and $y$ is in the null space of $A$. 
%For our second claim let us pick a $t$ such that $(1-\eta)^t=\eta^2$. For such $t$, $\Delta^0_t = O\(\eta^2 \Delta^0_0\)$.
\end{prevproof}

\subsection{General Bilinear Case} \label{app:proof of general case minmax}

\begin{theorem}\label{theorem:general}
    Consider OMD for the min-max problem~\eqref{eq:general inf sup}:
		\begin{align*}
    \inf_{x} \sup_{y} \left(x^TAy + b^Tx + c^Ty + d\right). \label{eq:general inf sup}
\end{align*}
Under the same conditions as Corollary~\ref{cor:gradient becomes small} and whenever~\eqref{eq:general inf sup} is finite, OMD exhibits last iterate convergence in the same sense as in Corollary~\ref{cor:gradient becomes small}. In particular, for large enough $t$, the last iterate of OMD is within $O\(\sqrt{\eta} \cdot \gamma \sqrt{\Delta^0_0}\)$ distance from the space of equilibrium points of the game, where $\sqrt{\Delta_0}$ is the distance of the point $(x_0+(A^T)^+c,y_0+A^+b)$ from the equilibrium space, and where both distances are taken with respect to the norm $\sqrt{x^T A A^T x + y^T A^T A y}$. Whenever~\eqref{eq:general inf sup} is infinite or undefined, the OMD dynamics travels to infinity and we characterize its motion.
\end{theorem}
\begin{prevproof}{Theorem}{theorem:general}
Trivially, we need only consider functions of the form $x^TAy + b^Tx + c^Ty$. We consider the following decompositions
of $b$ and $c$: 
\[
    b &= b_1 + b_2 &\text{where}~b_1 \in \mathcal{R}(A), b_2 \in \mathcal{N}(A^T)  \\ 
    c &= c_1 + c_2 &\text{where}~c_1 \in \mathcal{R}(A^T), c_2 \in \mathcal{N}(A) 
		\]
		Given the above we can also define $b_3$ and $c_3$ as follows:
\[
    Ac_3 &= b_1 &\text{ feasible since } b_1 \in \mathcal{R}(A) \\
    A^Tb_3 &= c_1&\text{ feasible since } c_1 \in \mathcal{R}(A^T)  
\]

Then, we can make the following variable substition:
\[
    \alpha_t &= x_t + \eta t b_2 + b_3 \\
    \beta_t &= y_t - \eta t c_2 + c_3 \\
    \text{so that: }& \\
    A^T\alpha_t &= A^Tx_t + \eta t A^T b_2 + A^T b_3 \\
    &= A^Tx_t + c_1~~~~\text{since $b_2 \in \mathcal{N}(A^T)$} \\
    A\beta_t &= Ay_t - \eta t A c_2 + A c_3  \\
    &= Ay_t + b_1~~~~\text{since $c_2 \in \mathcal{N}(A)$}  \\
\]

We also state the OMD dynamics for $x_t$ and $y_t$ for problem~\eqref{eq:general inf sup}:
\[
    x_t &= x_{t-1} - 2\eta (A y_{t-1} + b)  + \eta (A y_{t-2} + b) \\ 
    &= x_{t-1} - 2\eta A y_{t-1} + \eta A y_{t-2} - \eta b \\ 
    y_t &= y_{t-1} + 2\eta (A^T x_{t-1} + c)  - \eta (A^T x_{t-2} + c) \\
    &= y_{t-1} + 2\eta A^T x_{t-1}  - \eta A^T x_{t-2} + \eta c 
\]

Note that given this update step:
\[
    x_{t+1} &= x_{t} - 2\eta A y_t + \eta A y_{t-1} - \eta b \\
    x_{t+1} &= x_{t} - \eta b_2 - 2\eta A y_t + \eta A y_{t-1} - \eta A c_3 \\
    x_{t+1} &= x_t - \eta b_2 - 2\eta A (y_t + c_3) + \eta A (y_{t-1} + c_3) \\
    x_{t+1} &= x_t - \eta b_2 - 2\eta A (y_t - \eta c_2 t + c_3) + \eta A (y_{t-1} - \eta c_2 (t-1) + c_3) \\
    x_{t+1} + \eta b_2 (t+1) &= x_t + \eta b_2 t - 2\eta A (y_t - \eta c_2 t + c_3) + \eta A (y_{t-1} - \eta c_2 (t-1) + c_3) \\
    x_{t+1} + \eta b_2 (t+1) + b_3  &= x_t + \eta b_2 t + b_3 - 2\eta A (y_t -
    \eta c_2 t + c_3) + \eta A (y_{t-1} - \eta c_2 (t-1) + c_3) \\
    \alpha_{t+1} &= \alpha_t - 2\eta A \beta_t + \eta A \beta_{t-1} \\
    \text{Analogously: }& \\
    \beta_{t+1} &= \beta_t + 2\eta A^T \alpha_t - \eta A^T \alpha_{t-1}
\]
Note that these are precisely the dynamics for which we proved convergence in
Theorem~\ref{thm:convergence of OGD-main}. Thus, by invoking Theorem~\ref{thm:convergence of OGD} and Corollary~\ref{cor:gradient becomes small} on the sequence $(\alpha_t,\beta_t)$ and then substituting back $(x_t,y_t)$, we have that for all large enough $t$:
\[
    x_t &= -\eta b_2 t - b_3 + \epsilon_x(t) \\
    y_t &= \eta c_2 t - c_3 + \epsilon_y(t) \\
    &\text{such that } ||A^T\epsilon_x(t)||_2, ||A\epsilon_y(t)||_2 \in O\(\sqrt{\eta} \cdot \gamma \sqrt{\Delta^0_0}\),
\]
where $\Delta^0_0 = ||A^T(x_0+b_3)||_2^2 + ||A (y_0+c3)||_2^2$.

In particular, this shows that, whenever~\eqref{eq:general inf sup} is finite (i.e.~$b_2=c_2=0$), OMD exhibits last iterate convergence. For large enough $t$, the last iterate of OMD is within $O\(\sqrt{\eta} \cdot \gamma \sqrt{\Delta^0_0}\)$ distance from the space of equilibrium points of the game, where $\sqrt{\Delta^0_0}$ is the distance of $(x_0+b_3,y_0+c_3)$ from the equilibrium space in the norm $\sqrt{x^T A A^T x + y^T A^T A y}$. Whenever~\eqref{eq:general inf sup} is infinite or undefined, the OMD dynamics travels to infinity linearly, with fluctuations around the divergence specified as above. 
\end{prevproof}
%\begin{corollary} \label{cor:general bilinear functions}
%    OGD converges as in the last corollary for functions of the form:
%    $$f(x,y) = (x+b)^TA(x+c)$$
%\end{corollary}
%\begin{prevproof}{Corollary}{cor:general bilinear functions}
%    Let $\alpha_t = x_t + b$, and $\beta_t = y_t + c$. Then, note that the
%    function can be written simply as $f(\alpha, \beta) = \alpha^TA\beta$. Thus,
%    by the main result, we have that both $\alpha, \beta \rightarrow 0$, which
%    in turn implies that $x_t \rightarrow b$, $y_t \rightarrow c$ as required.
%\end{prevproof}

%\begin{remark}{}
%    The above corrollary actually holds for any function $f$ of the form:
%    $$f(x, y) = x^TAy + bx + cy + r$$
%    If $b \in \mathcal{R}(A), c \in \mathcal{R}(A^T)$. This is because if these
%    two hold, we can write $d, e$ s.t. $Ae = b$, $A^Td = c$, then:
%    $$f(x, y) = (x+d)^TA(y+e) - (d^Te + r)$$
%    The constant term can be disregarded since it is not present in any of the
%    gradients, and thus this reduces to the corollary above. Note that if one
%    of these conditions does not hold, the optimum may be ill-defined. For
%    example, $A = [1,0;0,0], b = [1,1], c = [1,1]$ is a configuration such that
%    $f(x,y) = x_1y_1 + x_1 + x_2 + y_1 + y_2 + r$; here, $x_2$ and $y_2$ can be
%    set to $+\infty$, $-\infty$ respectively, so the optimum itself is not
%    well-defined.
%\end{remark}

\subsection{Omitted Proofs} \label{appendix:omitted proofs}

\begin{prevproof}{Proposition}{prop:gradient descent unstable}
    To show this, we consider the $\ell_2$ distance of the solution at time $t$.
    First, recall the GD update step in the special case of $f(x, y) = x^Ty$:
    \[
	x_{t} = x_{t-1} - \eta y_{t-1} \\
	y_{t} = y_{t-1} + \eta x_{t-1}
    \]

    Then, note that the squared $\ell_2$ distance of the running iterate $(x_t,y_t)$ to the unique equilibrium solution $(0,0)$  is given
    by $d(t) := ||x_t||_2^2 + ||y_t||_2^2$, which we can calculate:
    \[
	||x_t||_2^2 &= ||x_{t-1}||^2_2 - 2\eta x_{t-1}^Ty_{t-1} +
	\eta^2||y_{t-1}||_2^2 \\
	||y_{t-1}||_2^2 &= ||y_{t-1}||^2_2 + 2\eta y_{t-1}^T x_{t-1} +
	\eta^2||x_{t-1}||_2^2 \\
	\therefore d(t) &= d(t-1) + \eta^2 d(t-1) \\
	&= (1+\eta^2)d(t-1)
    \]
    This indicates that for any value of $\eta>0$, the running iterate of 
    GD \textit{diverges} from the equilibrium.
\end{prevproof}

\begin{prevproof}{Claim}{claim:pushing A's around}
For our first claim, observe that:
\[
    \innerab{Au}{Av}{i} &= u^TA^TAM_i^TM_iA^TAv \\
			&= u^TA^TA(A^TA)^k(A^j)^TA^j(A^TA)^kA^TAv \\
   &= u^TA^T(AA^T)^kA(A^j)^TA^jA^T(AA^T)^kAv \\
   &= u^TA^T[(AA^T)^kA(A^j)^T][A^jA^T(AA^T)^k]Av \\
   &= u^TA^TN_{i+1}^TN_{i+1}Av \\
   &= \innerac{u}{v}{i+1}
\]
Our second claim, $\innerac{A^Tu}{A^Tv}{i} = \innerab{u}{v}{i+1}$, is proven analogously.

For our third claim:
\[
    \innerab{u}{Av}{i} &= u^TAM_i^TM_iA^TAv \\
		       &= u^TA(A^TA)^{k}(A^TA)^j(A^TA)^kA^TAv \\
    \text{if $j = 0$: }&  \\
		       &= u^TA(A^TA)^{k}(A^TA)^kA^TAv \\
	 &= u^TA[A^T(AA^T)^k][(AA^T)^kA]v \\
  &= u^TA [A^T N_{i}^T][N_{i}A]v \\
  &= \innerac{v}{A^Tu}{i}\\
    \text{otherwise: }& \\
		      &= u^TA(A^TA)^kA^TA(A^TA)^kA^TAv \\
					&= u^TA[(A^TA)^kA^TA][(A^TA)^kA^TA]v \\
					&= u^TA[A^T(AA^T)^kA][A^T(AA^T)^kA]v \\
										&= u^TA[A^TN_i^T][N_iA]v \\
 &= \innerac{v}{A^Tu}{i} 
\]
\end{prevproof}

\begin{prevproof}{Lemma}{lemma:first bound}
First, we note the following scaled update rule:
\[
    M_{i}A^Tx_t &= M_i\(\xtao\) \\
    N_{i}Ay_t &= N_i\(\ytao\)
\]

\noindent Then, taking the norm of both sides, and using the statements of Claim~\ref{claim:pushing A's around}:
\[
    \normab{x_t}{i} &= \normab{x_{t-1}}{i} + 4\eta^2\normac{y_{t-1}}{i+1} +
    \eta^2\normac{y_{t-2}}{i+1} - 4\eta\innerab{x_{t-1}}{Ay_{t-1}}{i} \\
    &\qquad+ 2\eta\innerab{x_{t-1}}{Ay_{t-2}}{i} -
    4\eta^2\innerac{y_{t-1}}{y_{t-2}}{i+1} \\[2ex]
    \normac{y_t}{i} &= \normac{y_{t-1}}{i} + 4\eta^2\normab{x_{t-1}}{i+1} +
    \eta^2\normab{x_{t-2}}{i+1} + 4\eta\innerac{y_{t-1}}{A^Tx_{t-1}}{i} \\
    &\qquad+ 2\eta\innerac{y_{t-1}}{A^T x_{t-2}}{i} -
    4\eta^2\innerab{x_{t-1}}{x_{t-2}}{i+1} \\[2ex]
    \therefore \Delta_t^i &= \normab{x_t}{i} + \normac{y_t}{i} \\
    &= \Delta^i_{t-1} + 4\eta^2\Delta^{i+1}_{t-1} +
    \eta^2\Delta^{i+1}_{t-2} + 2\eta(\innerab{x_{t-1}}{Ay_{t-2}}{i} -
    \innerac{y_{t-1}}{Ax_{t-2}}{i}) \\
    &\qquad\qquad - 4\eta^2(\innerab{x_{t-1}}{x_{t-2}}{i+1} +
    \innerac{y_{t-1}}{y_{t-2}}{i+1})
\]

\noindent Expanding the first pair of inner products above and using Claim~\ref{claim:pushing A's around} again:
\[
   \innerab{x_{t-1}}{Ay_{t-2}}{i} - \innerac{y_{t-1}}{A^T x_{t-2}}{i} &=
    \innerab{\xtt}{Ay_{t-2}}{i} \\
    &\qquad - \innerac{\ytt}{A^T x_{t-2}}{i} \\[2ex]
    = -2\eta(\normac{y_{t-2}}{i+1}+&\normab{x_{t-2}}{i+1}) + \eta(\innerab{x_{t-2}}{x_{t-3}}{i+1} +
    \innerac{y_{t-2}}{y_{t-3}}{i+1})
\]

Then, multiplying by $2\eta$ and substituting into the previous derivation yields:
\[
    \Delta^i_t - \Delta^i_{t-1} &= 4\eta^2\Delta^{i+1}_{t-1} -
    3\eta^2\Delta^{i+1}_{t-2} + 2\eta^2(\innerab{x_{t-2}}{x_{t-3}}{i+1} +
    \innerac{y_{t-2}}{y_{t-3}}{i+1}) \\
    &\qquad\qquad - 4\eta^2(\innerab{x_{t-1}}{x_{t-2}}{i+1} +
    \innerac{y_{t-1}}{y_{t-2}}{i+1})
\]

\noindent Now, consider the following inner product:
\[
    \innerab{x_{t-2}}{x_{t-1}}{i+1} + \innerac{y_{t-2}}{y_{t-1}}{i+1} &=
    \innerab{x_{t-2}}{\xtt}{i+1} \\
    &\qquad + \innerac{y_{t-2}}{\ytt}{i+1} \\[2ex]
    &= \Delta^{i+1}_{t-2} + \eta\(\innerab{x_{t-2}}{Ay_{t-3}}{i+1} -
    \innerac{y_{t-2}}{A^Tx_{t-3}}{i+1}\)
\]

\noindent Once again, we multiply by $-4\eta^2$ and substitute:
\[
    \Delta^i_t - \Delta^i_{t-1} &= 4\eta^2\Delta^{i+1}_{t-1} -
    7\eta^2\Delta^{i+1}_{t-2} + 2\eta^2(\innerab{x_{t-2}}{x_{t-3}}{i+1} +
    \innerac{y_{t-2}}{y_{t-3}}{i+1}) \\
    &\qquad + 4\eta^3(\innerac{y_{t-2}}{A^Tx_{t-3}}{i+1} -
    \innerab{x_{t-2}}{Ay_{t-3}}{i+1})
\]

Now, we use the update step for time $t-2$. For all $t \geq 1$, this is
well-defined, since $x_{-1}$ and $y_{-1}$ are defined. To ensure that this step
is sound for $t = 0$ requires we define the following, where $X^+$ denotes the
generalized inverse:
\[
    x_{-2} = 4x_0 + \frac{1}{\eta}(A^T)^{+} y_0 \\
    y_{-2} = 4y_0 - \frac{1}{\eta}A^+ x_0
\]
We define these such that: $A^Tx_{-2} = 4A^Tx_0 + \frac{y_0}{\eta}$ and
$Ay_{-2} = 4Ay_0 - \frac{x_0}{\eta}$ (since $x_0 \in R(A)$ and $y_0 \in R(A^T)$,
and thus the following equalities hold:
\[
    x_0 = x_{-1} - 2\eta Ay_{-1} + \eta Ay_{-2} \\
    y_0 = y_{-1} + 2\eta A^T x_{-1} - \eta A^Tx_{-2}
\]

\noindent This allows us to use the following expansion freely for all $t \geq 2$:
\[
    x_{t-2} &= \x{}{-3}{-4} &&\implies x_{t-3} - 2\eta Ay_{t-3} = x_{t-2} - \eta Ay_{t-4} \\
    y_{t-2} &= \y{}{-3}{-4} &&\implies y_{t-3} + 2\eta A^Tx_{t-3} = y_{y-2} + \eta A^Tx_{t-4}
\]

\noindent We can gather the inner product terms and use this update rule to get
our final desired result:
\[
    \Delta^i_t - \Delta^i_{t-1} &= 4\eta^2\Delta^{i+1}_{t-1} -
    7\eta^2\Delta^{i+1}_{t-2} + 2\eta^2(\innerab{x_{t-2}}{x_{t-3} - 2\eta
    Ay_{t-3}}{i+1} + \innerac{y_{t-2}}{y_{t-3} + 2\eta A^Tx_{t-3}}{i+1}) \\[1ex]
    &= 4\eta^2\Delta^{i+1}_{t-1} - 5\eta^2\Delta^{i+1}_{t-2} -
    2\eta^3(\innerab{x_{t-2}}{Ay_{t-4}}{i+1} -
    \innerac{y_{t-2}}{A^Tx_{t-4}}{i+1})
\]
\end{prevproof}

\begin{prevproof}{Lemma}{lemma:restated bound}
\noindent To prove this, first consider the following trivial inequality:
\[
    \normac{y_{t-2} - A^Tx_{t-4}}{i+1} &+
    \normab{x_{t-2} + Ay_{t-4}}{i+1} \\
    &= \normac{y_{t-2}}{i+1} -
    2\innerac{y_{t-2}}{A^Tx_{t-4}}{i+1} + \normac{A^Tx_{t-4}}{i+1} \\
    &\qquad+ \normab{x_{t-2}}{i+1} +
    2\innerab{x_{t-2}}{Ay_{t-4}}{i+1} + \normab{Ay_{t-4}}{i+1} \\
    &\geq 0
\]

\noindent Rearranging:
\[
    2\innerac{y_{t-2}}{A^Tx_{t-4}}{i+1} - 2\innerab{x_{t-2}}{Ay_{t-4}}{i+1}
    &\leq \Delta_{t-2}^{i+1} + \(\normac{A^Tx_{t-4}}{i+1} +
    \normab{Ay_{t-4}}{i+1}\) \\
				&\leq \Delta_{t-2}^{i+1} + \lambda_\infty^2\(\normab{x_{t-4}}{i+1} +
    \normac{y_{t-4}}{i+1}\) \\
    &\leq \Delta_{t-2}^{i+1} +
    \(\lambda_\infty^2\Delta_{t-4}^{i+1}\)  \\
		    &\leq \Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1}
\]

%\vscomment{Why is the above last inequality correct? Also can we maybe also write an intermediate observation that $\|y\|_{A^T N_{i+1}^T}^2 = \|N_{i+1} A y\|_2^2$ and similarly for the $x$.}
%
%\noindent Now, using the update rule we know that:
%\[
    %\normab{x_{t-2}}{i} &= \normab{x_{t} + 2\eta Ay_{t-1} + \eta Ay_{t-2} - \eta
    %Ay_{t-3}}{i}\\
    %&\leq 8\normab{x_{t}}{i} + 8\eta^2\normab{ Ay_{t-1}}{i}+8\eta^2\normab{ Ay_{t-2}}{i} + 8\eta^2\normab{Ay_{t-3}}{i}\\
		%&\leq 8\normab{x_{t}}{i} + 8\eta^2\lambda_{\infty}^2\normac{ y_{t-1}}{i}+8\eta^2\lambda_{\infty}^2\normac{ y_{t-2}}{i} + 8\eta^2\lambda_{\infty}^2\normac{y_{t-3}}{i}
%\]
%Similarly:
%\[
%\normac{y_{t-2}}{i} \leq 8\normac{y_{t}}{i} + 8\eta^2\lambda_{\infty}^2\normab{ x_{t-1}}{i}+8\eta^2\lambda_{\infty}^2\normab{ x_{t-2}}{i} + 8\eta^2\lambda_{\infty}^2\normab{x_{t-3}}{i}
%\]	
%Hence, recalling that $\lambda_{\infty}\le 1$:
%\[	
    %\Delta_{t-2}^i &\leq 8\Delta_{t}^i + 8\eta^2 \Delta_{t-1}^i + 8\eta^2 \Delta_{t-2}^i + 8\eta^2 \Delta_{t-3}^i
		%\\
		   %&\leq \Delta_{t}^i + 4\eta(1-\eta^2)^{t-3}\Delta_{0}^{i+1} \\
    %&\leq \Delta_{t}^i + 4\eta\lambda_\infty\Delta_{0}^{i}
%\]
%%
%%
%\noindent Plugging the above inequality (for time $t-4$ and index $i+1$ on the left hand side)\footnote{If needed (for time $t=2$), we use the same construction of $x_{-2},y_{-2}$ found in the Proof of Lemma~\ref{lemma:first bound}.} in to our earlier bound we get:
%%Applying this for $t-4$ 
%\[
    %2\innerac{y_{t-2}}{A^Tx_{t-4}}{i+1} - 2\innerab{x_{t-2}}{Ay_{t-4}}{i+1}
    %&\leq \Delta_{t-2}^{i+1} +
    %\lambda_\infty^2\(8\Delta_{t-2}^{i+1} + 8\eta^2 \Delta_{t-3}^{i+1} + 8\eta^2 \Delta_{t-4}^{i+1} + 8\eta^2 \Delta_{t-5}^{i+1}\)  \\
		%&\leq 9\Delta_{t-2}^{i+1} + 8\eta^2 \Delta_{t-3}^{i+1} + 8\eta^2 \Delta_{t-4}^{i+1} + 8\eta^2 \Delta_{t-5}^{i+1}
    %%&\leq (1+\lambda_\infty^2)\Delta_{t-2}^{i+1} +
    %%4\eta\lambda^2_\infty\Delta_0^{i+1}
%\]

\noindent Now, we can apply this bound to the result of Lemma~\ref{lemma:first bound}:
\[
    \Delta^i_{t} - \Delta^i_{t-1} &= 4\eta^2\Delta^{i+1}_{t-1} -
    5\eta^2\Delta^{i+1}_{t-2} - 2\eta^3(\innerab{x_{t-2}}{Ay_{t-4}}{i+1} -
    \innerac{y_{t-2}}{A^Tx_{t-4}}{i+1}) \\
    &\leq 4\eta^2\Delta^{i+1}_{t-1} - 5\eta^2\Delta^{i+1}_{t-2} +
    \eta^3(\Delta_{t-2}^{i+1} + \Delta_{t-4}^{i+1})
\]

%\[
    %\Delta^i_{t} - \Delta^i_{t-1} &= 4\eta^2\Delta^{i+1}_{t-1} -
    %5\eta^2\Delta^{i+1}_{t-2} - 2\eta^3(\innerab{x_{t-2}}{Ay_{t-4}}{i+1} -
    %\innerac{y_{t-2}}{A^Tx_{t-4}}{i+1}) \\
    %&\leq 4\eta^2\Delta^{i+1}_{t-1} - 5\eta^2\Delta^{i+1}_{t-2} +
    %2\eta^3(9\Delta_{t-2}^{i+1} + 8\eta^2 \Delta_{t-3}^{i+1} + 8\eta^2 \Delta_{t-4}^{i+1} + 8\eta^2 \Delta_{t-5}^{i+1}) \\
%\]

\noindent Which is what we sought out to prove.
\end{prevproof}

%\begin{prevproof}{Lemma}{lemma:lower bound}
%Now, note that choosing $x_0 \in \mathcal{R}(A) \implies x_t \in
%\mathcal{R}(A) = \mathcal{R}(AA^T)\ \forall\ t$, due to the update step.
%Similarly, $y_t \in \mathcal{R}(A^T)$. Thus, $x_t = AA^T(AA^T)^{+}x_t$ and
%$y_t = A^TA(A^TA)^{+}y_t$, for all $t$. Letting $Q = (AA^T)^{+}$ and $P =
%(A^TA)^{+}$, and recalling key properties of the generalized inverse: 
%\[
    %\Delta_t^{i} &= \norm{M_{i}A^Tx_t}{2} + \norm{N_{i}Ay_t}{2} \\
       %&= \norm{M_iA^TAA^TQx_t}{2} +
    %\norm{N_iAA^TAPy_t}{2}\\
    %&= \norm{M_{i+2}A^TQx_t}{2} +
	%\norm{N_{i+2}APy_t}{2} \\ 
    %&= \norm{(A^T)^jQ(AA^T)^{k+1}x_t}{2} +
	%\norm{A^jP(A^TA)^{k+1}y_t}{2} \\
    %&= \begin{cases}
	%\norm{QM_{i+1}A^Tx}{2} + \norm{PN_{i+1}Ay_t}{2} &\text{ if $j = 0$}\\[0.5ex]
	%\norm{PM_{i+1}A^Tx}{2} + \norm{QN_{i+1}Ay_t}{2} &\text{ if $j = 1$}
    %\end{cases} \\
    %&\leq \max \(||Q||, ||P||\)\cdot \Delta_t^{i+1}
%\]
%\end{prevproof}

		\begin{prevproof}{Lemma}{lemma:semi-trivial upper bound}
		Suppose $j=i \mod 2$ and $k=(i-j)/2$. Notice the following identities:
		\begin{align*}
		&M_i = A^j(A^TA)^k, N_i = \(A^T\)^j \(AA^T\)^k\\
		&M_{i+1} = (A^T)^jA(A^TA)^k, N_{i+1} = A^j A^T\(AA^T\)^k
		\end{align*}
		Now:
		\begin{align*}
		\Delta^{i+1}_t &= \normlt{N_{i+1}Ay_t} + \normlt{M_{i+1}A^Tx_t}\\
		&=\normlt{A^j A^T\(AA^T\)^kAy_t} + \normlt{(A^T)^jA(A^TA)^kA^Tx_t}\\
		&\le \lambda_{\infty}^2\(\normlt{(A^T)^j\(AA^T\)^kAy_t} + \normlt{A^j(A^TA)^kA^Tx_t}\)\\
		&\le \lambda_{\infty}^2\(\normlt{N_iAy_t} + \normlt{M_iA^Tx_t}\)\\
		&\le \Delta^{i}_t,
		\end{align*}
		where for the last inequality we used that $\lambda_{\infty}\le 1$.
		\end{prevproof}
		
\begin{prevproof}{Lemma}{lemma:lower bound}
Given our initialization, $x_0 \in \mathcal{R}(A)$. This implies $x_t \in
\mathcal{R}(A), \forall\ t$, due to the update step of the dynamics. Recalling key properties of the matrix pseudoinverse, this implies: $x_t \equiv A A^+ x_t = A A^T (A A^T)^+ x_t$, for all $t$.
Similarly, given our initialization, $y_t \in \mathcal{R}(A^T)$, for all $t$, which implies $y_t \equiv A^T (A^T)^+ y_t = A^T A (A^T A)^+ y_t$, for all $t$. Letting $Q = (AA^T)^{+}$ and $P =
(A^TA)^{+}$, we get the following (where $j = i \mod 2$ and $k= (i-j)/2$): 
\[
    \Delta_t^{i} &= \norm{M_{i}A^Tx_t}{2} + \norm{N_{i}Ay_t}{2} \\
       &= \norm{M_iA^TAA^TQx_t}{2} +
    \norm{N_iAA^TAPy_t}{2}\\
    &= \norm{M_{i+2}A^TQx_t}{2} +
	\norm{N_{i+2}APy_t}{2} \\ 
	&= \norm{A^j (A^TA)^{k+1}A^T (AA^T)^{+} x_t}{2} +
	\norm{(A^T)^j (AA^T)^{k+1}A(A^TA)^+y_t}{2} \\ 
    &= \norm{A^j (A^TA)^{+} A^T (AA^T)^{k+1}x_t}{2} +
	\norm{(A^T)^j (AA^T)^{+} A(A^TA)^{k+1}y_t}{2} \\
    &= \begin{cases}
	\norm{(A^TA)^{+} M_{i+2}A^Tx_t}{2} + \norm{(AA^T)^{+}N_{i+2}Ay_t}{2} &\text{ if $j = 0$}\\[0.5ex]
	\norm{(AA^T)^{+}M_{i+2}A^Tx_t}{2} + \norm{(A^TA)^{+} N_{i+2}Ay_t}{2} &\text{ if $j = 1$}
    \end{cases} \\
    &\leq \max \(||Q||, ||P||\)^2\cdot \Delta_t^{i+2},
\]
where for the fourth and fifth equality we used the following key property of pseudo-inverses: $A^+=(A^TA)^+A^T=A^T(AA^T)^+$.
\end{prevproof}

