
We demonstrate the benefits of using OMD over GD in another simple illustrative example. In this case, the example is does not boil down to a bi-linear game and therefore, the simulation results portray that the theoretical results we provided for bi-linear games, carry over qualitatively beyond the linear case.

Consider the case where the data distribution is a mean zero multi-variate normal with an unknown co-variance matrix, i.e., $x \sim N(0, \Sigma)$. We will consider the case where the discriminator is the set of all quadratic functions:
\begin{equation}
D_W(x) = \sum_{ij} W_{ij} x_i x_j = x^T W x 
\end{equation}
The generator is a linear function of the random input noise $z\sim N(0, I)$, of the form:
\begin{equation}
G_V(z) = V z
\end{equation}
The parameters $W$ and $V$ are both $d\times d$ matrices. The WGAN game loss associated with these functions is then:
\begin{equation}
L(V, W)= \mathbb{E}_{x\sim N(0, \Sigma)}\left[ x^T W x \right] - \mathbb{E}_{z\sim N(0,I)}\left[z^T V^T W V z \right] 
\end{equation}
Expanding the latter we get:
\begin{align*}
L(V, W)=& \mathbb{E}_{x\sim N(0, \Sigma)}\left[ \sum_{ij} W_{ij} x_i x_j \right] - \mathbb{E}_{z\sim N(0,I)}\left[ \sum_{ij} W_{ij} \sum_{k} V_{ik} z_k  \sum_{m} V_{jm} z_m\right] \\
=&
\mathbb{E}_{x\sim N(0, \Sigma)}\left[ \sum_{ij} W_{ij} x_i x_j \right] - \mathbb{E}_{z\sim N(0,I)}\left[ \sum_{ijkm} W_{ij} V_{ik} V_{jm} z_k z_m \right]\\
=& \sum_{ij} W_{ij} \mathbb{E}_{x\sim N(0, \Sigma)}\left[ x_i x_j \right] - \sum_{ijkm} W_{ij} V_{ik} V_{jm} \mathbb{E}_{z\sim N(0,I)}\left[ z_k z_m \right]\\
=& \sum_{ij} W_{ij} \Sigma_{ij} - \sum_{ijkm} W_{ij} V_{ik} V_{jm} 1\{k=m\}\\
=& \sum_{ij} W_{ij} \Sigma_{ij} - \sum_{ijk} W_{ij} V_{ik} V_{jk}\\
=& \sum_{ij} W_{ij} \left(\Sigma_{ij} - \sum_{k} V_{ik} V_{jk}\right)
\end{align*}
Given that the covariance matrix is symmetric positive definite, we can write it as $\Sigma = U U^T$. Then the loss simplifies to:
\begin{align}
L(V, W) = \sum_{ij} W_{ij} \left(\Sigma_{ij} - \sum_{k} V_{ik} V_{jk}\right) =& \sum_{ijk} W_{ij} \left(U_{ik} U_{jk} -  V_{ik} V_{jk}\right)
\end{align}
The equilibrium of this game is for the generator to choose $V_{ik} = U_{ik}$ for all $i,k$, and for the discriminator to pick $W_{ij}=0$. For instance, in the case of a single dimension we have $L(V,W) = W\cdot (\sigma^2 - V^2)$, where $\sigma^2$ is the variance of the Gaussian. Hence, the equilibrium is for the generator to pick $V=\sigma$ and the discriminator to pick $W=0$.

\paragraph{Dynamics without sampling noise.} For the mean GD dynamics the update rules are as follows:
\begin{equation}
\begin{aligned}
W_{ij}^t =& W_{ij}^{t-1} + \eta \left(\Sigma_{ij} - \sum_{k} V_{ik}^{t-1} V_{jk}^{t-1}\right) \\
V_{ij}^t =& V_{ij}^{t-1} + \eta \sum_{k} \left(W_{ik}^{t-1} + W_{ki}^{t-1}\right) V_{kj}^{t-1} 
\end{aligned}
\end{equation}
We can write the latter updates in a simpler matrix form:
\begin{equation}
\begin{aligned}
W_t =& W_{t-1} + \eta \left(\Sigma - V_{t-1} V_{t-1}^T\right)\\
V_t =& V_{t-1} + \eta (W_{t-1} + W_{t-1}^T) V_{t-1}
\end{aligned}\tag{GD for Covariance}
\end{equation}
Similarly the OMD dynamics are:
\begin{equation}
\begin{aligned}
W_t =& W_{t-1} + 2\eta \left(\Sigma - V_{t-1} V_{t-1}^T\right) - \eta \left(\Sigma - V_{t-2} V_{t-2}^T\right)\\
V_t =& V_{t-1} + 2\eta (W_{t-1} + W_{t-1}^T) V_{t-1} - \eta (W_{t-2} + W_{t-2}^T) V_{t-2}
\end{aligned}\tag{OMD for Covariance}
\end{equation}

Due to the non-convexity of the generators problem and because there might be multiple optimal solutions (e.g. if $\Sigma$ is not strictly positive definite), it is helpful in this setting to also help dynamics by adding $\ell_2$ regularization to the loss of the game. The latter simply adds an extra $2\lambda W_{t}$ at each gradient term $\nabla_W L(V_t, W_t)$ for the discriminator and a $2\lambda V_{t}$ at each gradient term $\nabla_{V} L(V_t, W_t)$ for the generator. In Figures \ref{fig:covariance} and \ref{fig:covariance2d} we give the weights and the implied covariance matrix $\Sigma^G=VV^T$ of the generator's distribution for each of the dynamics for an example setting of the step-size and regularization parameters and for two and three dimensional gaussians respectively. We again see how OMD can stabilize the dynamics to converge pointwise.

\paragraph{Stochastic dynamics.} In Figure \ref{fig:stoch_covariance} and \ref{fig:stoch_covariance2} we also portray the instability of GD and the robustness of the stability of OMD under stochastic dynamics. In the case of stochastic dynamics the gradients are replaced with unbiased estimates or with averages of unbiased estimates over a small minibatch. In the case of a mini-batch of one, the unbiased estimates of the gradients in this setting take the following form:
\begin{equation}
\begin{aligned}
\hat{\nabla}_{W, t} = x_{t} x_{t}^T - V_{t}z_{t} z_{t}^T  V_{t}^T\\
\hat{\nabla}_{V, t} = - (W_{t} + W_{t}^T) V_{t} z_t z_t^T
\end{aligned}\tag{Stochastic Gradients}
\end{equation}
where $x_t, z_t$ are samples drawn from the true distribution and from the random noise distribution respectively. Hence, the stochastic dynamics simply follow by replacing gradients with unbiased estimates:
\begin{equation}
\begin{aligned}
W_t =& W_{t-1} + \eta \hat{\nabla}_{W, t-1}\\
V_t =& V_{t-1} - \eta \hat{\nabla}_{V, t-1}
\end{aligned}\tag{SGD for Covariance}
\end{equation}
\begin{equation}
\begin{aligned}
W_t =& W_{t-1} + 2\eta \hat{\nabla}_{W, t-1} - \eta \hat{\nabla}_{W, t-2}\\
V_t =& V_{t-1} - 2\eta \hat{\nabla}_{V, t-1} + \eta \hat{\nabla}_{V, t-2}
\end{aligned}\tag{SOMD for Covariance}
\end{equation}

\newpage

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_gd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_gd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_gd_gen_Sigma.png}
			\end{subfigure}        
        \caption{GD dynamics. $\eta=0.1$, $T=500$, $\lambda=0.3$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_omd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_omd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{2d_covariance_omd_gen_Sigma.png}
			\end{subfigure}        
        \caption{OMD dynamics. $\eta=0.1$, $T=500$, $\lambda=0.3$.}
    \end{subfigure}
    \caption{Stability of OMD vs GD in the co-variance learning problem for a two-dimensional gaussian ($d=2$). Weight clipping in $[-1,1]$ was applied in both dynamics.}\label{fig:covariance2d}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_gd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_gd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_gd_gen_Sigma.png}
			\end{subfigure}        
        \caption{GD dynamics. $\eta=0.1$, $T=500$, $\lambda=0.3$.}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_omd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_omd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_omd_gen_Sigma.png}
			\end{subfigure}        
        \caption{OMD dynamics. $\eta=0.1$, $T=500$, $\lambda=0.3$.}
    \end{subfigure}
    \caption{Stability of OMD vs GD in the co-variance learning problem for a three-dimensional gaussian ($d=3$). Weight clipping in $[-1,1]$ was applied in both dynamics.}\label{fig:covariance}
\end{figure}

\newpage

\begin{figure}[H]
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_gd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_gd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_gd_gen_Sigma.png}
			\end{subfigure}        
        \caption{Stochastic GD dynamics with mini-batch size $50$. $\eta=0.02$, $T=1000$, $\lambda=0.1$.}
    \end{subfigure}
    \begin{subfigure}[b]{1.01\textwidth}
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_gd_true.png}
    		\caption{True Distribution}
			\end{subfigure}        
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_gd_iterate_minus_50.png}
    		\caption{Iterate $T-50$}
			\end{subfigure}   
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_gd_iterate_minus_35.png}
    		\caption{Iterate $T-35$}
			\end{subfigure}   
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_gd_iterate_minus_20.png}
    		\caption{Iterate $T-20$}
			\end{subfigure}  
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_gd_last_iterate.png}
    		\caption{Iterate $T$}
			\end{subfigure}  
    	\caption{Comparison of true distribution and distribution of generator at various points closer to the end of training.}
    \end{subfigure}
    \caption{Stochastic GD dynamics for covariance learning of a two-dimensional gaussian ($d=2$). Weight clipping in $[-1,1]$ was applied to the discriminator weights.}\label{fig:stoch_covariance}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_omd_disc.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_omd_gen_V.png}
			\end{subfigure}        
    		\begin{subfigure}[b]{.3\textwidth}
    		\includegraphics[height=1.7in]{covariance_stoch_omd_gen_Sigma.png}
			\end{subfigure}        
        \caption{Stochastic OMD dynamics with mini-batch size $50$. $\eta=0.02$, $T=1000$, $\lambda=0.1$.}
    \end{subfigure}
    \begin{subfigure}[b]{1.01\textwidth}
    		\begin{subfigure}[b]{.19\textwidth}   
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_omd_true.png}
    		\caption{True Distribution}
			\end{subfigure}        
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_omd_iterate_minus_50.png}
    		\caption{Iterate $T-50$}
			\end{subfigure}   
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_omd_iterate_minus_35.png}
    		\caption{Iterate $T-35$}
			\end{subfigure}   
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_omd_iterate_minus_20.png}
    		\caption{Iterate $T-20$}
			\end{subfigure}  
    		\begin{subfigure}[b]{.19\textwidth}
    		\hspace{-.3in}     	
    		\includegraphics[height=1.3in]{covariance_omd_last_iterate.png}
    		\caption{Iterate $T$}
			\end{subfigure}  
    	\caption{Comparison of true distribution and distribution of generator at various points closer to the end of training.}
    \end{subfigure}
    \caption{Stability of OMD with stochastic gradients for covariance learning of a two-dimensional gaussian ($d=2$). Weight clipping in $[-1,1]$ was applied to the discriminator weights.}\label{fig:stoch_covariance2}
\end{figure}


