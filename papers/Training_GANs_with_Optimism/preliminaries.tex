We consider the problem of learning a generative model of a distribution of data points $Q \in \Delta(X)$. Our goal is given a set of samples from $D$, to learn an approximation to the distribution $Q$ in the form of a deep neural network $G_{\theta}(\cdot)$, with weight parameters $\theta$, that takes as input random noise $z\in F$ (from some simple distribution $F$) and outputs a sample $G_{\theta}(z)\in X$. We will focus on addressing this problem via a Generative Adversarial Network (GAN) training strategy.

The GAN training strategy defines as a zero-sum game between a \emph{generator} deep neural network $G_{\theta}(\cdot)$ and a \emph{discriminator} neural network $D_{w}(\cdot)$. The generator takes as input random noise $z\sim F$, and outputs a sample $G_{\theta}(z)\in X$. A discriminator takes as input a sample $x$ (either drawn from the true distribution $Q$ or from the generator) and attempts to classify it as real or fake. The goal of the generator is to fool the discriminator.

In the original GAN training strategy \cite{Goodfellow14}, the discriminator of the zero sum game was formulated as a classifier, i.e. $D_w(x)\in [0,1]$ with a multinomial logistic loss. The latter boils down to the following expected zero-sum game (ignoring sampling noise). 
\begin{equation}
\inf_{\theta} \sup_{w} \E_{x\sim Q}\left[ \log(D_w(x))\right] + \E_{z\sim F}\left[\log(1- D_{w}(G_{\theta}(z)))\right]
\end{equation} 
If the discriminator is very powerful and learns to accurately classify all samples, then the problem of the generator amounts to minimizing the Jensen-Shannon divergence between the true distribution and the generators distribution. However, if the discriminator is very powerful, then in practice the latter leads to vainishing gradients for the generator and inability to train in a stable manner.

The latter problem lead to the formulation of Wasserstein GANs (WGANs) \cite{arjovsky2017wasserstein}, where the discriminator rather than being treated as a classifier (equiv. approximating the JS divergence) is instead trying to approximate the Wasserstein$-1$ or \emph{earth-mover} metric between the true distribution and the distribution of the generator. In this case, the function $D_w(x)$ is not constrained to being a probability in $[0,1]$ but rather is an arbitrary $1$-Lipschitz function of $x$. This reasoning leads to the following zero-sum game:
\begin{equation}\label{eqn:wgan}
\inf_{\theta} \sup_{w} \E_{x\sim Q}\left[ D_w(x) \right] - \E_{z\sim F}\left[D_w(G_\theta(z))\right]
\end{equation}
If the function space of the discriminator covers all $1$-Lipschitz functions of $x$, then the quantity $\sup_{w} \E_{x\sim D}\left[ D_w(x) \right] - \E_{z\sim F}\left[D_w(G_\theta(z))\right]$ that the generator is trying to minimize corresponds to the earth-mover distance between the true distribution $Q$ and the distribution of the generator. Given the success of WGANs we will focus on WGANs in this paper.


\subsection{Gradient Descent vs Optimistic Mirror Descent}

The standard approach to training WGANs is to train simultaneously the parameters of both networks via stochastic gradient descent. We begin by presenting the most basic version of adversarial training via stochastic gradient descent and then comment on the multiple variants that have been proposed in the literature in the following section, where we compare their performance with our proposed algorithm for a simple example.

Let us start how training a GAN with gradient descent would look like in the absence of sampling error, i.e. if we had access to the true distribution $Q$. For simplicity of notation, let:
\begin{equation}\label{eqn:WGAN-loss}
L(\theta, w) = \E_{x\sim Q}\left[ D_w(x) \right] - \E_{z\sim F}\left[D_w(G_\theta(z))\right]
\end{equation}
denote the loss in the expected zero-sum game of WGAN, as defined in Equation \eqref{eqn:wgan}, i.e. $\inf_{\theta}\sup_{w} L(\theta,w)$. The classic WGAN approach is to solve this game by running gradient descent (GD) for each player, i.e. for $t\in \{1,\ldots, T-1\}$: with $\nabla_{w,t}=\nabla_w L(\theta_t, w_t)$ and $\nabla_{\theta, t} =\nabla_{\theta} L(\theta_t, w_t)$
\begin{equation}\label{eqn:GD}
\begin{aligned}
w_{t+1} =~& w_t + \eta \cdot \nabla_{w,t}\\
\theta_{t+1} =~& \theta_t - \eta \cdot \nabla_{\theta,t}
\end{aligned}
\end{equation}
If the loss function $L(\theta, w)$ was convex in $\theta$ and concave $w$, \vsedit{$\theta$ and $w$ lie in some bounded convex set} and the step size $\eta$ is chosen of the order $\frac{1}{\sqrt{T}}$, then standard results in game theory and no-regret learning (see e.g. \cite{Freund1999}) imply that the pair $(\bar{\theta}, \bar{w})$ of average parameters, i.e. $\bar{w}=\frac{1}{T} \sum_{t=1}^T w_t$ and $\bar{\theta} = \frac{1}{T} \sum_{t=1}^T \theta_t$ is an $\epsilon$-equilibrium of the zero-sum game, for $\epsilon = O\left(\frac{1}{\sqrt{T}}\right)$. However, no guarantees are known beyond the convex-concave setting and, more importantly for the paper, even in convex-concave games, no guarantees are known for the last-iterate pair $(\theta_T, w_T)$.

Rakhlin and Sridharan \citep{Rakhlin} proposed an alternative algorithm for solving zero-sum games in a decentralized manner, namely  \emph{Optimistic Mirror Descent} (OMD), that achieves faster convergence rates to equilibrium of $\epsilon = O\left(\frac{1}{T}\right)$ for the average of parameters. The algorithm essentially uses the last iterations gradient as a predictor for the next iteration's gradient. This follows from the intuition that if the opponent in the game is using a stable (or regularized) algorithm, then the gradients between the two iterations will not change much. Later \cite{Syrgkanis} showed that this intuition extends to show faster convergence of each individual player's regret in general normal form games. 

Given these favorable properties of OMD when learning in games, we propose replacing GD with OMD when training WGANs. The update rule of a OMD is a small adaptation to GD. OMD is parameterized by a predictor of the next iteration's gradient which could either be simply last iteration's gradient or an average of a window of last gradient or a discounted average of past gradients. In the case where the predictor is simply the last iteration gradient, then the update rule for OMD boils down to the following simple form:
\begin{equation}\label{eqn:omd}
\begin{aligned}
w_{t+1} =~& w_t + 2\eta \cdot \nabla_{w, t} - \eta \cdot \nabla_{w, t-1} \\
\theta_{t+1} =~& \theta_t - 2 \eta \cdot \nabla_{\theta. t}
+ \eta \cdot \nabla_{\theta, t-1}
\end{aligned}
\end{equation}
The simple modification in the GD update rule, is inherently different than any of the existing adaptations used in GAN training, such as Nesterov's momentum, or gradient penalty.

\paragraph{General OMD and intuition.} The intuition behind OMD can be more easily understood when GD is viewed through the lens of the Follow-the-Regularized-Leader formulation. In particular, it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm with an $\ell_2$ regularizer (see e.g. \cite{Shalev2012}), i.e.:
\begin{equation}\label{eqn:ftrl-gen}
\begin{aligned}
w_{t+1} =& \arg\max_{w} \eta \sum_{\tau=1}^t \langle w, \nabla_{w, \tau}\rangle - \|w\|_2^2\\
\theta_{t+1} =& \arg\min_{\theta} \eta \sum_{\tau=1}^t \langle \theta, \nabla_{\theta, \tau}\rangle + \|\theta\|_2^2
\end{aligned}
\end{equation}
It is known that if the learner knew in advance the gradient at the next iteration, then by adding that to the above optimization would lead to constant regret that comes solely from the regularization term\footnote{The latter is a consequence of the be-the-leader lemma \cite{Kalai2005,Rigollet2015}}. OMD essentially augments FTRL by adding a predictor $M_{t+1}$ of the next iterations gradient, i.e.:
\begin{equation}\label{eqn:oftrl-gen}
\begin{aligned}
w_{t+1} =& \arg\max_{w}~ \eta \left(\sum_{\tau=1}^t \langle w, \nabla_{w,\tau}\rangle + \langle w, M_{w, t+1}\rangle \right) - \|w\|_2^2\\
\theta_{t+1} =& \arg\min_{\theta}~ \eta \left(\sum_{\tau=1}^t \langle \theta, \nabla_{\theta, \tau}\rangle + \langle \theta, M_{\theta, t+1}\rangle \right)+ \|\theta\|_2^2
\end{aligned}
\end{equation}
For an arbitrary set of predictors, the latter boils down to the following set of update rules:
\begin{equation}\label{eqn:omd-gen}
\begin{aligned}
w_{t+1} =~& w_t + \eta \cdot \left(\nabla_{w, t} + M_{w, t+1} - M_{w, t}\right)\\
\theta_{t+1} =~& \theta_t - \eta \cdot \left(\nabla_{\theta, t} + M_{\theta, t+1} - M_{\theta, t}\right)
\end{aligned}
\end{equation}
In the theoretical part of the paper we will focus on the case where the predictor is simply the last iteration gradient, leading to update rules in Equation \eqref{eqn:omd}. In the experimental section we will also explore performance of other alternatives for predictors.

\subsection{Stochastic Gradient Descent and Stochastic Optimistic Mirror Descent}

In practice we don't really have access to the true distribution $Q$ and hence we replace $Q$ with an empirical distribution $Q_n$ over samples $\{x_1,\ldots, x_n\}$ and $F_n$ of random noise samples $\{z_1,\ldots, z_n\}$, leading to empirical loss for the zero-sum game of:
\begin{equation}
L_n(\theta, w) = \E_{x\sim Q_n}\left[ D_w(x) \right] - \E_{z\sim F_n}\left[D_w(G_\theta(z))\right]
\end{equation}
Even in this setting it might be impractical to compute the gradient of the expected loss with respect to $Q_n$ or $F_n$, e.g. $\E_{x\sim Q_n}\left[ \nabla_w D_w(x)\right]$.

However, GD and OMD still leads to small loss if we replace gradients with unbiased estimators of them. Hence, we can replace expectation with respect to $Q_n$ or $F_n$, by simply evaluating the gradients at a single sample or on a small batch of $B$ samples. Hence, we can replace the gradients at each iteration with the variants:
\begin{equation}\label{eqn:omd-gen}
\begin{aligned}
\hat{\nabla}_{w, t} =~& \frac{1}{|B|} \sum_{i\in B}\left( \nabla_w D_{w_t}(x_i) - \nabla_{w} D_{w_t}(G_{\theta_t}(z_i))\right)\\
\hat{\nabla}_{\theta, t}  =~& - \frac{1}{|B|}\sum_{i\in B} \nabla_{\theta} (D_{w_t}(G_{\theta_t}(z_i)))
\end{aligned}
\end{equation}
Replacing $\nabla_{w,t}$ and $\nabla_{\theta,t}$ with the above estimates in Equation \eqref{eqn:GD} and \eqref{eqn:omd}, leads to Stochastic Gradient Descent (SGD) and Stochastic Optimistic Mirror Decent (SOMD) correspondingly.
