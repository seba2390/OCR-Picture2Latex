\vspace{-.1in}
In this section we applying optimistic WGAN training to generating images, after training on CIFAR10. Given the success of Adam on training image WGANs we will use an optimistic version of the Adam algorithm, rather than vanilla OMD. We denote the latter by \emph{Optimistic Adam}. Optimistic Adam could be of independent interest even beyond training WGANs. We present Optimistic Adam for (G) but the analog is also used for training (D).
\begin{algorithm}[h]
\begin{algorithmic}
\State Parameters: stepsize $\eta$, exponential decay rates for moment estimates $\beta_1, \beta_2\in [0,1)$, stochastic loss as a function of weights $\ell_t(\theta)$, initial parameters $\theta_0$
%\State Initialize parameters to $\theta_0$
\For{each iteration $t\in \{1,\ldots, T\}$}
\State Compute stochastic gradient: $\nabla_{\theta,t} = \nabla_{\theta} \ell_t(\theta)$
\State Update biased estimate of first moment: $m_t = \beta_1 m_{t-1} + (1-\beta_1) \cdot \nabla_{\theta,t}$
\State Update biased estimate of second moment: $v_t = \beta_2 v_{t-1} + (1-\beta_2) \cdot \nabla_{\theta,t}^2$
\State Compute bias corrected first moment: $\hat{m}_t = m_t/(1 - \beta_1^t)$
\State Compute bias corrected second moment: $\hat{v}_t = v_t/(1 - \beta_2^t)$
\State Perform \emph{optimistic gradient step}: $\theta_t = \theta_{t-1} - 2 \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon} + \eta \frac{\hat{m}_{t-1}}{\sqrt{\hat{v}_{t-1}}+\epsilon}$ 
\EndFor
\State Return $\theta_T$
\end{algorithmic}
\caption{\emph{Optimistic ADAM}, proposed algorithm for training WGANs on images. }\label{alg:opt-adam}
\end{algorithm}
\vspace{-.1in}
We trained on CIFAR10 images with Optimistic Adam with the hyper-parameters matched to \cite{Gulrajani2017}, and we observe that it outperforms Adam in terms of inception score (see Figure \ref{fig:optimistic-Adam}), a standard metric of quality of WGANs \citep{Gulrajani2017, salimans2016improved}. In particular we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training. We observe that for Optimistic Adam, training the discriminator once after one iteration of the generator training, which matches the intuition behind the use of optimism, outperforms the 1:5 generator-discriminator training scheme. We see that vanilla Adam performs poorly when the discriminator is trained only once in between iterations of the generator training. Moreover, even if we use vanilla Adam and train $5$ times (D) in between a training of (G), as proposed by \cite{arjovsky2017wasserstein}, then performance is again worse than Optimistic Adam with a 1:1 ratio of training. The same learning rate $0.0001$ and betas ($\beta_1=0.5, \beta_2=0.9$) as in Appendix B of \cite{Gulrajani2017}  were used for all the methods compared. We also matched other hyper-parameters such as gradient penalty coefficient $\lambda$ and batch size. For a larger sample of images see Appendix \ref{sec:appendix-cifar10}. 
\vspace{-.1in}
\begin{figure}[htpb]
    \centering
    \begin{subfigure}[b]{.67\textwidth}
        \centering
    		\includegraphics[height=1.7in]{optimAdam-eps-converted-to.pdf}        
		\caption{Inception score on CIFAR10, when training with Adam and Optimistic Adam. ``ratio1" means we performed $1$ iteration of training of (D) in between $1$ iteration of (G). Otherwise we performed $5$ iterations. We further test (averaging over 35 trials) the two top-performing optimizers, Adam (ratio 5) and Optimistic Adam with ratio 1, in Appendix~\ref{sec:appendix-errorbars}.}
    \end{subfigure}
    ~~
    \begin{subfigure}[b]{.3\textwidth}
        \centering
    		\includegraphics[height=1.7in]{optimAdam_v0_1e-04_ratio1_epoch93-eps-converted-to.pdf}
    	\caption{Sample of images from Generator of Epoch $94$, which had the highest inception score.}
    \end{subfigure}
    \caption{Comparison of Adam and Optimistic Adam on CIFAR10.}\label{fig:optimistic-Adam}
\end{figure}
