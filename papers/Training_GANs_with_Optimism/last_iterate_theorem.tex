\label{sec:main:proof OMD converges}
In this section, we show that Optimistic Mirror Descent  
exhibits final-iterate, rather than only average-iterate convergence to min-max solutions for
bilinear functions. 
%In particular, we show that the $\ell_2$ norms of the gradients used by the dynamics shrinks in time.
More precisely, we consider the problem $\min_x \max_y x^T A y$, for some matrix $A$, where $x$ and $y$ are unconstrained. In Appendix \ref{sec:appendix:last-iterate}, we also show that our convergence result appropriately extends to the general case, where the bi-linear game
also contains terms that are linear in the players' individual strategies, i.e.~games of the form:
\begin{equation}
    \inf_{x} \sup_{y} \left(x^TAy + b^Tx + c^Ty + d\right). \label{eq:inf sup problem general}
\end{equation}

In the simpler $\min_{x}\max_{y} x^T Ay$ problem, Optimistic Mirror Descent takes the following form, for all $t \ge 1$:
\begin{align}
    x_{t} &= \xto \label{eq:OGD bilinear x repeat}\\
    y_{t} &= \yto  \label{eq:OGD bilinear y repeat}
\end{align}
\noindent {\em Initialization:} For the above iteration to be meaningful we need to specify $x_0,x_{-1},y_0,y_{-1}$. We choose any $x_0 \in
\mathcal{R}(A)$, and $y_0\in\mathcal{R}(A^T)$, and set $x_{-1}=2x_0$ and $y_{-1}=2y_{0}$, where ${\cal R}(\cdot)$ represents the column space of $A$. In particular, our initialization means that the first step taken by the dynamics gives $x_1=x_0$ and $y_1=y_0$.

\smallskip We will analyze Optimistic Mirror Descent under the assumption $\lambda_{\infty} \le 1$, where $\lambda_{\infty}=\max\{||A||,||A^T||\}$ and $||\cdot||$ denotes spectral norm of matrices. We can always enforce that $\lambda_{\infty} \le 1$ by appropriately scaling $A$. Scaling $A$ by some positive factor clearly does not change the min-max solutions $(x^*,y^*)$, only scales the optimal value $x^{*T}Ay^*$ by the same factor.

We remark that the set of equilibrium solutions of this minimax problem  are pairs $(x,y)$ such that $x$ is in the null space of $A^T$ and $y$ is in the null space of $A$. 
%In particular, finding a solution to $\min_x \max_y x^T A y$ is a trivial problem. So 
In this section we rigorously show that Optimistic Mirror Descent converges to the set of such min-max solutions. This is interesting in light of the fact that Gradient Descent actually diverges, even in the special case where $A$ is the identity matrix, as per the following proposition whose proof is provided in Appendix~\ref{appendix:omitted proofs}.

\begin{proposition} \label{prop:gradient descent unstable}
Gradient descent applied to the problem $\min_x \max_y x^T y$ diverges starting from any initialization $x_0, y_0$ such that $x_0,y_0 \neq 0$.
\end{proposition}

Next, we state our main result of this section, whose proof can be found in Appendix~\ref{sec:appendix:last-iterate}, where we also state its appropriate generalization to the general case~\eqref{eq:inf sup problem general}.

\begin{theorem}[Last Iterate Convergence of OMD]\label{thm:convergence of OGD-main}
Consider the dynamics of Eq.~\eqref{eq:OGD bilinear x repeat} and~\eqref{eq:OGD bilinear y repeat} and any initialization ${1 \over 2}x_{-1}=x_0 \in
\mathcal{R}(A)$, and ${1 \over 2}y_{-1}=y_0\in\mathcal{R}(A^T)$. Let also
$$\gamma = \vsedit{\left\|\(AA^T\)^{+}\right\|},$$
%\max\(\left|\left|\(AA^T\)^{+}\right|\right|,
%	    \left|\left|\(A^TA\)^{+}\right|\right|\),$$
where for a matrix $X$ we denote by	$X^{+}$ its generalized inverse and by $||X||$ its spectral norm. Suppose that \vsedit{$\|A\|\equiv \lambda_{\infty}\le 1$} 
%$\max\{||A||,||A^T||\}\equiv \lambda_{\infty}\le 1$ 
and that $\eta$ is a small enough constant satisfying $\eta <1/(3\gamma^2)$. Letting $\Delta_t = \normlt{A^Tx_t} + \normlt{Ay_t}$, the OMD dynamics satisfy the following:
\begin{align*}
&\Delta_1=\Delta_0 \ge {1 \over (1+\eta)^2} \Delta_2\\
    \forall t\ge 3:~~&\Delta_t \leq \left(1-{\eta^2 \over \gamma^2}\right)\Delta_{t-1} + 16\eta^3 \Delta_0. 
\end{align*}
In particular, $\Delta_t \rightarrow O(\eta \gamma^2 \Delta_0)$, as $t \rightarrow +\infty$, and for large enough $t$, the last iterate of OMD is within $O(\sqrt{\eta} \cdot \gamma \sqrt{\Delta_0})$ distance from the space of equilibrium points of the game, where $\sqrt{\Delta_0}$ is the distance of the initial point $(x_0,y_0)$ from the equilibrium space, and where both distances are taken with respect to the norm $\sqrt{x^T A A^T x + y^T A^T A y}$.
\end{theorem}



