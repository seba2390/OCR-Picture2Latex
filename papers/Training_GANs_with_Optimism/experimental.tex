We take our theoretical intuition to practice, applying OMD to the problem of generating DNA sequences from an observed distribution of sequences. DNA sequences that carry out the same function can be viewed as samples from some distribution. For many important cellular functions, this distribution can be well modeled by a position-weight matrix (PWM) that specifies the probability of different nucleotides occuring at each position~\citep{stormo2000dna}. Thus, training GANs from DNA sequences sampled from a PWM distribution serves as a practically motivated problem where we know the ground truth and can thus quantify the performance of different training methods in terms of the KL divergence between the trained generator distribution  and the true distribution.

In our experiments, we generated 40,000 DNA sequences of six nucleotides according to a given position weight matrix. A random 10\% of the sequences were held out as the validation set. Each sequence was then embedded into a $4\times6$ matrix by encoding each of the four nucleotides with an one-hot vector. On this dataset, we trained WGANs with different variants of OMD and SGD and evaluated their performance in terms of the KL divergence between the empirical distribution of the WGAN-generated samples and the true distribution described by the position weight matrix. Both the discriminator and generator of the WGAN used in this analysis were chosen to be convolutional neural networks (CNN), given the recent success of CNNs in modeling DNA-protein binding~\citep{Zeng2016, alipanahi2015predicting}. The detailed structure of the chosen CNNs can be found in Appendix~\ref{sec:apdxdna}. 

To account for the impact of learning rate and training epochs, we explored two different ways of model selection when comparing different optimization strategies: (1) using the iteration and learning rate that yields the lowest discriminator loss on the held out test set. This is inspired by the observation in \cite{arjovsky2017wasserstein} that the discriminator loss negatively correlates with the quality of the generated samples. (2) using the model obtained after the last epoch of the training. To account for the stochastic nature of the initialization and optimizers, we trained 50 independent models for each learning rate and optimizer, and compared the optimizer strategies by the resulting distribution of KL divergences across 50 runs.

For GD, we used variants of Equation~\eqref{eqn:GD} to examine the effect of using momentum and an adaptive step size. Specifically, we considered momentum, Nesterov momentum and Adagrad.  The specific form of all these modifications is given for reference in Appendix \ref{sec:GD-variants}.

For OMD we used the general predictor version of Equation~\eqref{eqn:omd-gen} with a fixed step size and with the following variants of the next iteration predictor $M_{t+1}$:
(v1)  Last iteration gradient: $M_{t+1} = \nabla f_t$, (v2)  Running average of past gradients: $M_{t+1} = \frac{1}{t} \sum_{i=1}^t \nabla f_i$, (v3) Hyperbolic discounted average of past gradients: $M_{t+1} = \lambda M_{t} + (1-\lambda)\nabla f_t, \lambda \in (0,1)$. We explored two training schemes: (1) training the discriminator 5 times for each generator training as suggest in \cite{arjovsky2017wasserstein}. (2) training the discriminator once for each generator training. The latter is inline with the intuition behind the use of optimism: optimism hinges on the fact that the gradient at the next iteration is very predictable since it is coming from another regularized algorithm, and if we train the other algorithm multiple times, then the gradient is not that predictable and the benefits of optimism are lost.

\begin{figure}[htbp!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.4in]{experimental_lowestval-eps-converted-to.pdf}
        \caption{WGAN with the lowest validation discriminator loss}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.4in]{experimental_lastepoch-eps-converted-to.pdf}
        \caption{WGAN at the last epoch}
    \end{subfigure}
    \caption{KL divergence of WGAN trained with different optimization strategies. Methods are ordered by the median KL divergence. Methods in (a) are named by the category and version of the method. ``ratio 1" denotes training the discriminator once for every generator training. Otherwise, we performed 5 iterations. For (b) where we don't combine the models trained with different learning rates, the learning rate is appended at the end of the method name. For momentum and Nesterov momentum, we used $\gamma=0.9$. For Adagrad, we used the default $\epsilon=1e^{-8}$.}
    \label{fig:expt}
\end{figure}

For all afore-described algorithms, we experimented with their stochastic variants. Figure~\ref{fig:expt} shows the KL divergence between the WGAN-generated samples and the true distribution. When evaluated by the epoch and learning rate that yields the lowest discriminator loss on the validation set,  WGAN trained with Stochastic OMD (SOMD) achieves lower KL divergence than the competing SGD variants. \vsedit{Evaluated by the last epoch,  
%optimistic Adam achieve the best performance, while only Adam with a carefully chosen learning rate outperforms SOMD. %is less sensitive to the choice of learning rate than the SGD variants. 
the best performance across different learning rates is achieved by optimistic Adam (see Section \ref{sec:cifar10}).} We note that in both metrics, SOMD with 1:1 generator-discriminator training ratio yields better KL divergence than the alternative training scheme (1:5 ratio), which validates the intuition behind the use of optimism.