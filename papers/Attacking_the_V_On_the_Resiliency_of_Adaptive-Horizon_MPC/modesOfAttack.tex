\section{The Adaptive-Horizon MPC Algorithm} % {Attack Strategies}
\label{sec:ampc}

We now present our new \emph{adaptive-horizon} \emph{model-predictive-control} algorithm, we call AMPC. We will use this algorithm as the controller strategy in the stochastic game on MDPs. We will also consider attack strategies that use AMPC. Since AMPC is an adaptive MPC procedure based on particle-swarm optimization (PSO), we first
briefly present background material on MPC and PSO.
%\todo[inline]{The rest of this section does not belong here. It should be moved in the description of the controller. Similar text can be used then for the advanced attacker.}

\subsection{Background on Model-Predictive Control}
%One way to find an action that can take a flock to a V-formation is based on using model-predictive control (MPC).  
Model-predictive control (MPC) determines the control action at current time $t$ by looking $h$ steps into the future and finding the best $h$-length sequence of control actions that can take the system from its current state $s(t)$ to a new state that has the lowest fitness. (Since we assume existence of a fitness metric $J$ that we are trying to minimize, we specialize the description of MPC to this case.) 
If ${s}_{\va^h}(t+h)$ denotes the state reached from state ${s}(t)$ in time $h$ following the actions $\va^h$ of length $h$, then in the MPC approach, at each time step $t$, the following minimization is performed to find the optimal set of actions
%\begin{align}
%&\textbf{opt-$\va$}^{h}(t)=\{\textbf{opt-$\va$}_i^{h}(t)\}_{i=1}^{b}=\argmin_{\va^h(t)}J(\boldsymbol{c}_{\va^h}(t+h)).
%\label{eq:opt}
%\end{align}
\begin{align}
&\textbf{opt-$\va$}^{h}(t)=\argmin_{\va^h(t)}J(s_{\va^h}(t+h)).
\label{eq:opt}
\end{align}
Since the model is an approximation of the system, only the first action $\va(t) = \textbf{opt-$\va$}^{1}(t)$ is applied as the action at time $t$,
and the remaining future $h-1$ actions found by the optimizer are ignored.  After the control action $\va(t)$ is applied, the system is left to evolve, and the process is repeated at $t\,{+}\,1, t\,{+}\,2,$ and so on.
%
%
%\vspace*{-1mm}
%\begin{align}
%J(\boldsymbol{c}(t),\va^h(t),{h}) = (\CV(\boldsymbol{c}_{\va}^{h}(t))-\CV^*)^2 &+ 
%(\VM(\boldsymbol{c}_{\va}^{h}(t))-\VM^*)^2 \nonumber \\ & +(\UB(\boldsymbol{c}_{\va}^{h}(t))-\UB^*)^2,
%\label{eq:fitness}
%\end{align}
%\noindent{}where ${h}$ is the length of the receding prediction horizon (RPH), $\va^h(t)$ is a sequence of accelerations of length ${h}$, and $\boldsymbol{c}_{\va}^{h}(t)$ is 
%the configuration reached after applying $\va^h(t)$ to $\boldsymbol{c}(t)$.
%
%

The MPC approach can be used for achieving a V-formation, as was outlined in~\cite{yang2016bda,yang2016love}.  These earlier works, however, did not use an adaptive dynamic window, and did not consider the adversarial control problem.
%We perform a single flock-wide minimization of $J$ at each time-step $t$ to obtain an optimal plan of  length $h$ of acceleration actions:\vspace*{-3mm}
%
%\begin{align}
%&\textbf{opt-$\va$}^{h}(t)=\{\textbf{opt-$\va$}_i^{h}(t)\}_{i=1}^{b}=\argmin_{\va^h(t)}J(\boldsymbol{c}(t),\va^h(t),{h}).
%\label{eq:opt}
%\end{align}
%\vspace*{-3mm}\noindent{} 
%
%We apply the first acceleration $\va_i(t) = \textbf{opt-$\va$}_i^{1}(t)$ as the optimal acceleration for bird $i$ %at time $t$. 
%

In MPC, optimization problem (\ref{eq:opt}) is additionally subject to constraints that bound the set of possible actions and states. For example, in our flocking model, the
magnitude of velocity and acceleration for each of the $B$ birds is bounded: $||\vv_i(t)||\,{\leqslant}\,\vv_{max},
||\va^h_i(t)||\,{\leqslant}\,\rho||\vv_i(t)||$ $\forall$ $i\,{\in}\,\{1,\ldots,B\}$,
where $\vv_{max}$ is a predefined constant and $\rho\,{\in}\,(0,1)$. 
% The initial state is selected following the given initial distribution. In the investigation of V-formation conducted in~\cite{lukina2016arxiv}, the initial positions and velocities of each bird are selected at random 
% within certain ranges, and limited such that the distance between any 
% two birds is greater than a (collision) constant $d_{min}$, and small
% enough so that each bird finds itself in the upwash or downwash region of at least one other bird. 

We use a \emph{particle-swarm-optimization algorithm} to solve the optimization problems generated by the MPC procedure.
%\todo[inline]{I added the following assuming we want to refer to the ARES approach - this might to be replaced if we want to use MPC instead. (I am not sure what Jesse used for his experiments right now)}
%\todo[inline]{Anna: as far as I understand Jesse is using MPC level-based with receding horizon, however, there are no PSO clones or important splitting}

% ASHISH: I think we should comment out the following; otherwise, we will look to be proposing something very close to TACAS paper.
%To solve the optimization problem, Lukina et al.~\cite{lukina2016arxiv} present an approach using \emph{Particle Swarm Optimization (PSO)}~\cite{Kennedy95particleswarm} to find potential next actions, combined with the idea of \emph{Importance Splitting}~\cite{kahn1951} in order to increase the chance of reaching a V-formation. Furthermore, they introduce adaptive horizons to allow for temporary worse situation in the flock enabling them to overcome local minima in the fitness function. Additionally, they introduced an adaptive number of particles for the PSO allowing them to better exploit the heuristics of PSO in combination with Importance Splitting as well as achieve a speed-up in the performance of their approach. However, their approach generates a plan offline to traverses the deterministic MDP in order to reach a specific state. In contrast, we present an approach to control the flock of birds online. The resilience of our approach is demonstrated in the presence of adversaries and noise.
