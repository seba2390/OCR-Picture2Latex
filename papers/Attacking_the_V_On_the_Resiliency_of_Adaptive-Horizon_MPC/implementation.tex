\newcommand{\anoise}{{\mathcal{AN}}}
\newcommand{\pnoise}{{\mathcal{PN}}}
\section{Stochastic Games for V-Formation}
\label{sec:sgv}

We describe the specialization of the stochastic-game verification problem to
V-formation.  In particular, we present the AMPC-based control strategy for reaching a V-formation, and the various attacker strategies against which we evaluate the resilience of our controller.

The MDP $\M$ for V-formation was presented in Section~\ref{sec:background}. The state variables of the MDP are the positions and velocities of the birds, and the control variables (defining the actions) are the accelerations and displacements. In the transition relation given in equation~(\ref{eq:v}), the attacker chooses the displacement $\vec{d}(t)$ it needs to manipulate the position of the birds,
whereas the controller chooses the acceleration $\vec{a}(t)$ to apply. Together, the pair $(\vec{a}(t),\vec{d}(t))$ defines the action that transforms one MDP state to another. We now define the controller's and attacker's strategies.

\subsection{Controller's Adaptive Strategies}

Given current state $(\vec{x}(t),\vec{v}(t))$, the controller's strategy $\sigma_C$ returns a probability distribution on the space of all possible accelerations (for all birds).  As mentioned above, this probability distribution is specified implicitly via a randomized algorithm that returns an actual acceleration (again for all birds).  This randomized algorithm is the AMPC algorithm, which inherits its randomization from the randomized PSO procedure it deploys.  

When the controller computes an acceleration, it assumes that the attacker does {\em{not}} introduce any disturbances; i.e., the controller uses the following model:
\vspace*{-4mm}\begin{eqnarray}
 \xv_i(t + 1) &=& \xv_i(t) + \vv_i(t+1) \qquad \forall~i\,{\in}\,\{1,\ldots,B\}, \nonumber \\
 \vv_i(t + 1) &=& \vv_i(t) + \va_i(t), \label{eq:noattack} %\\[-6mm]
\end{eqnarray}
where $\va(t)$ is the only control variable. Note that the controller chooses its next action $\va(t)$ based on the current configuration $(\xv(t),\vv(t))$ of the flock using MPC. The current configuration may have been influenced by the disturbance $\vec{d}(t-1)$ introduced by the attacker in the previous time step.  Hence, the current state need not be the state predicted by the controller when performing MPC in step $t-1$. Moreover, depending on the severity of the attacker action $\vec{d}(t-1)$, the AMPC procedure dynamically adapts its behavior, i.e.\ the choice of horizon $h$, in order to enable the controller to pick the best control action $\vec{a}(t)$ in response.

\subsection{Attacker's Strategies}

We are interested in evaluating the resilience of our V-formation controller when it is threatened by an attacker that can remove a certain number of birds from the flock, or manipulate a certain number of birds by taking control of their actuators (modeled by the displacement term in equation~(\ref{eq:trans})).
We assume that the attack lasts for a limited amount of time, after which the controller attempts to bring the system back into the good set of states. When there is no attack, the system behavior is the one given by equation~(\ref{eq:noattack}).

Note that there can be many different criteria for evaluating the success of an attack,  %(see Remark~\ref{remark:criteria})
but in our experiments, the controller is declared the winner if it can bring the flock to V-formation.
We consider three attack strategies (but see the future work discussion in Section~\ref{sec:conclusion}), each of which defines a V-formation game.

\vspace*{-0.5mm}\paragraph{\bf Remove Birds Game.}
In an RBG, the attacker selects a subset of $R$ birds, where $R\,{\ll}\,B$, and removes them from the flock.  The removal of bird $i$ from the flock at time $t\,{=}\,0$ can be simulated in our framework by allowing the attacker to set the displacement $\vd_i(0)$ for bird $i$ to $\infty$.  We assume that the flock is in a V-formation at time $t\,{=}\,0$.  
Thus, the goal of the controller is to bring the flock back into a V-formation consisting of $B\,{-}\,R$ birds.
%he controller needs to find the best adjustments in velocity $a_i$ for all remaining birds $i \in N - R$ during its turn. %$i \in N \wedge i \notin R$.
%Essentially, this results in a single-move game for the adversary. 
In an RBG, the attacker plays only one move.
When picking birds, the attacker is able to decide which birds will have the greatest negative impact on the flock's fitness when removed from the flock. Apart from seeing if the controller can bring the flock back to a V-formation, we also analyze the time it takes the controller to do so. 
%return to a v-formation for $R \leq \lceil\log(N)\rceil$ and 

% \todo[inline]{SAS: I would only suggest that the size R of the subset of
% birds removed from the flock (of size N) be such that R << N.  O/w I am
% not sure how interesting this game is.  Jesse has simulation results for
% R=1 and N=7.  Also, we should consider this game with and without process
% noise (PN), as Jesse has shown that the resiliency of the flock to remain
% in a V is highly dependent on the magnitude of PN.  It does very well with
% no PN or small PN, but resilience seems to degrade with increasing PN.}
%
%\begin{theorem}
%For any birds picked by the attacker, where $\left\vert{N - R}\right\vert \geq 3$, the planner can find 
%accelerations for each remaining bird in $N$ that will finally lead to a state $s^{*}$ such that cost 
%$J(s^{*})\{\leqslant}\,\varphi$.
%\end{theorem}

\vspace*{-0.5mm}\paragraph{\bf Random Displacement Game.}
In an RDG, the attacker chooses the displacement vector for a fixed number $R$ of birds uniformly from the space $[0,M]\times[0,2\pi]$. This means that the magnitude of the displacement vector is picked from the interval $[0,M]$, and the direction of the displacement vector is picked from the interval $[0,2\pi]$. We vary $M$ in our experiments. The $R$ birds that are picked in different steps are not necessarily the same, as the attacker makes this choice uniformly at random at runtime as well.
%In our second game, each player has control over all birds in the flock. The flock starts in a V-formation. However, both players have different goals and strategies. While the controller wants to keep the flock in a V-formation, the adversarial player tries to disrupt the V. Both players use the same planning approach but the controller tries to minimize the fitness function while the adversary tries to maximize the fitness in each step.
%In our second game, the adversarial player introduces malicious birds into the flock. These birds are controlled by the other player and hence can perturb the flock. To do so, the adversary adds small amounts of noise to this bird to distract the flock and disturb the v-formation. If this alone is not successful, the adversary can use a greater amount of noise to achieve the goal. However, this allows the controller to identify the adversary and henceforth ignore the malicious bird. 
The game starts from an initial V-formation. The attacker is allowed a fixed number of moves, say $20$, after which the displacement vector is identically $0$ for all birds.  The controller, which has been running in parallel with the attacker, is then tasked with moving the flock back to a V-formation, if necessary.
%
\vspace*{-0.5mm}\paragraph{\bf{AMPC Game.}}
An AMPC game is similar to an RDG except that the attacker does not use a uniform distribution to determine the displacement vector. The attacker is advanced and calculates the displacement (that will be the worst for the controller) using the AMPC procedure. See Figure~\ref{fig:ampc}.  In detail, the attacker applies AMPC, but assumes the controller applies zero acceleration. Thus, the attacker uses the following model of the flock dynamics:
\vspace*{-1mm}\begin{eqnarray}
 \xv_i(t + 1) &=& \xv_i(t) + \vv_i(t+1) + \vd_i(t) \qquad \forall~i\,{\in}\,\{1,\ldots,B\}, \nonumber \\
 \vv_i(t + 1) &=& \vv_i(t). \label{eq:attack} %\\[-6mm]
\end{eqnarray}
Note that the attacker is still allowed to have $\vd_i(t)$ be nonzero for a small number $R$ of birds. However, it can choose which $R$ birds it picks in each step.  It uses the AMPC procedure to simultaneously pick the $R$ birds and their displacements.
%Being a fair game, both players have the same capabilities. This means the controller as well as the adversary are able to use receding horizons to try to predict the best moves for their individual birds.

%\begin{theorem}
%
%\end{theorem}

%\paragraph{\bf Game 3.}%: Interior Lines}
% In our third game the adversary has only access to a specific subset of the birds. One could consider the attacker to add a set of malicious birds $M$ to the existing flock $N$.  Additionally we assume the controller is able to detect the attacker and hence the adversarial player needs to wait for the opportune moment to perform the actual attack. This means, the adversarial player can disrupt the V-formation slightly but only has one single move to interrupt and perturb the V-formation permanently. 
% \todo[inline] {Lukas: some important questions: the ATTACKER-ARES only controls the malicious birds and the CTL-ARES only the 'good' birds. however, does the CTL-ARES consider the malicious birds in its planning as 'good' birds? same for the ATTACKER-ARES. To me it would make sense, that the ATTACKER-ARES knows which ones are malicious birds and which ones are 'good' birds, but the CTL-ARES does not. So the CTL-ARES would consider ALL birds ($M \cup N$) but only controls the 'good' ones ($N$) -- i hope this makes any sense.}
%The third game is very similar to the second. However, when performing the final move, the attacker can decide whether it is more beneficial to introduce noise with a great magnitude to the flock or simply remove a specific number of birds from the flock. Again, we consider this a fair game where both players are able to use receding horizons do identify potential moves. Furthermore, we allow the adversary to remove up to $\log(N)$ birds from the flock.
%\subsection{Implementation: the Game is on}
%\label{sec:implementation}
%
%\todo[inline]{The following section would be the new implementation of our algorithm that deals with stochastic MDP and two-player games.}
%
% For this work, we extended the original \emph{deterministic Markov Decision Process} presented by Lukina et al.~\cite{lukina2016arxiv} to a \emph{classical MDP}~\cite{russellnorvig} by adding noise to the transition relation of the MDP. By doing so, we improved the original model and made it more realistic.
%
%We added and analyzed two different types of noises, processing noise ($\pnoise$) and actuator noise ($\anoise$). $\pnoise$ is applied to the position of each bird in our flock and changes the transition relation as follows
%\vspace*{-1mm}\begin{eqnarray*}
%\label{eq:pnoise_model}
% \xv_i(t + 1) &=& \xv_i(t) + \vv_i(t+1) + \pnoise %\label{eq:x_anoise},\\
% \vv_i(t + 1) &=& \vv_i(t) + \va_i(t) \label{eq:v_anoise},\\[-6mm]
%\end{eqnarray*}
%where $\pnoise \sim \mathcal{N}(0, \sigma^2)$. Here, $\sigma$ 

%In contrast, actuator noise is added to the acceleration action of the transition relation.
%\vspace*{-1mm}\begin{eqnarray*}
%\label{eq:model}
 %\xv_i(t + 1) &=& \xv_i(t) + \vv_i(t+1)\label{eq:x_anoise},\\
 %\vv_i(t + 1) &=& \vv_i(t) + \va_i(t) + \anoise\label{eq:v_anoise},\\[-6mm]
%\end{eqnarray*}

%\noindent where $\anoise \sim \mathcal{N}(0, \sigma^2)$. For our experiments we tried different $\sigma$, i.e. $\sigma = 0.05, 0.1, 0.2, 0.25$ and $0.3$.

%\begin{remark}\label{remark:criteria}
%Even though we use reaching V-formation as our success criterion (for the controller), we could have also used other criteria to decide if the attacker has been successful. For example, one could have used following criteria.
%
%\begin{itemize}
%\item \emph{Energy attack} is considered successful when a flock is not traveling in a V-formation for a certain amount of time. 
%
%\vspace*{1mm}\item \emph{Collisions} occur when two birds are in dangerous proximity from each other. This may happen through spoofing of existing birds or adversarial birds deliberately trying to lead to collisions with the others.
%
%\vspace*{1mm}\item \emph{Heading change} brings success, when the entire flock is diverged from its original direction (mission target) by a certain degree. 
%\end{itemize}
%\end{remark}

\begin{theorem}[AMPC resilience in a C-A game]
\label{thm:resilience}
Given a controller-attacker game, there is a finite maximum horizon $h_{\mathit{max}}$ and a finite maximum number of game-execution steps $m$ such that AMPC controller will win the controller-attacker game in $m$ steps with probability one.
\end{theorem}

\begin{proof}
Since the flock MDP (defined by Equation~6) is controllable, the PSO algorithm we use is fair, and the attack has a bounded duration, the proof of the theorem follows from Theorem~\ref{thm:ampc}. 
\end{proof}

\begin{remark}
While Theorem~\ref{thm:resilience} states that the controller is expected to win with probability one, we expect winning probability to be possibly lower than one in many cases because: (1)~the maximum horizon $h_{\mathit{max}}$ is fixed in advance, and so is (2) the maximum number of execution steps $m$; (3) the underlying PSO algorithm is also run with bounded number of particles and time.
\end{remark}
