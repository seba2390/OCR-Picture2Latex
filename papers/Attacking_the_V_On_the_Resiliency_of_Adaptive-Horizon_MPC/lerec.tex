	\subsection{The Main Algorithm of AMPC}
\label{sec:lerec}
% Inspired by the ARES algorithm in~\cite{lukina2016arxiv}, we propose a level-based receding-horizon model-predictive control algorithm we call LEREC. 
We propose the main algorithm of AMPC. %short for level-based Adaptive-horizon Model-Predictive Control {Jesse: this is already mentioned in the beginning of the section}. 
This algorithm performs step-by-step control of a given MDP $\M$ by looking $h$ steps ahead and predicting the next best state to move to.
We use PSO to identify the potentially best actions $\va^h$ in the current state achieving the optimal value of the fitness function in the next state.  For bird flocking, the fitness function, \texttt{Fitness}$(\M,\va^h,h)$ of $\va^h$ is defined as the minimum fitness metric $J$ obtained within $h$ steps by applying $\va^h$ on $\M$. Formally, we have
\vspace*{-2mm}
\begin{align}
\texttt{Fitness}(\M,\va^h,h) = \min_{1\leqslant \tau \leqslant h}{J(s_{\va^h}^\tau)}
\end{align}
where $s_{\va^h}^\tau$ is the state after apply the $\tau$th action of $\va^h$ on $\M$. For horizon $h$, PSO searches for the best sequence of 2-dimensional acceleration vector of length $h$, thus having $2Bh$ parameters to be optimized. The number of particles used in PSO is proportional to the number of parameters, i.e., $p = 2\beta B h$.
%was defined in equation~(\ref{eq:fitness}). 

The pseudocode for the AMPC algorithm is given in Algorithm~\ref{alg:lerec}. A novel feature of AMPC is that, unlike classical MPC that uses a fixed horizon $h$, AMPC adaptively chooses an $h$ depending on whether it is able to reach a fitness value that is lower than the current fitness by our chosen quanta $\Delta_i$, $\forall~i\,{\in}\,\{0,\ldots,m\}$.
%we would have had to resort to local optima without any guarantee of reaching a stable state.

%Following~\cite{lukina2016arxiv} we introduce level-based horizon as a way to overcome shortcomings of MPC and increase effectiveness of the optimization process. 
%To overcome the shortcoming of MPC and in order to increase the effectiveness of the optimization process, we introduce 
AMPC is hence an adaptive MPC procedure that uses level-based horizons.
It employs PSO to identify the potentially best next actions.
%for our flock.
If the chosen actions improve (decrease) the fitness of the next state $J(s_{k+h})$, $\forall~k\,{\in}\,\{0,\ldots,m\cdot h_{\mathit{max}}\}$, in comparison to the fitness of the previous state $J(s_k)$ by the predefined $\Delta_i$, the controller considers these actions to be worthy of leading the flock towards or keeping it in the V-formation.%
\footnote{We focus our attention on bird flocking, since the details generalize naturally to other MDPs that come with a fitness metric.}

In this case, the controller applies the actions to each bird and transitions to the next state of the MDP.
The threshold $\Delta_i$ determines the next level $\ell_i\,{=}\,J(s_{k+\widehat{h}})$ of the algorithm, where $\widehat{h} \leqslant h$ is the horizon with the best fitness. The prediction horizon $h$ is increased iteratively if the fitness has not been decreased enough. Upon reaching a new level, the horizon is reset to one (see Algorithm~\ref{alg:lerec}). Having a horizon $\widehat{h}\,{>}\,1$ means it will take multiple transitions in the MDP in order to reach a solution with improved fitness. However, when finding such a solution with $\widehat{h}\,{>}\,1$, we only apply the first action to transition the MDP to the next state. This is explained by the need to allow the other player (environment or an adversary) to apply their action before we obtain the actual next state. 
%adjustment faster and deal with unforeseen situations arising during runtime.
%
If no new level is reached within $h_{\mathit{max}}$ horizons, the first action of the best $\va^h$ using horizon $h_{\mathit{max}}$ is applied. 

The dynamic threshold $\Delta_i$ is defined as in~\cite{lukina2016arxiv}. Its initial value $\Delta_0$ is obtained by dividing the fitness range to be covered into $m$ equal parts, that is, $\Delta_0\,{=}\,(\ell_0\,{-}\,\ell_m)\,{/}\,m$, where $\ell_0\,{=}\,J(s_0)$ and $\ell_m\,{=}\,\varphi$. Subsequently, $\Delta_i$ is determined by the previously reached level $\ell_{i-1}$, as $\Delta_i\,{=}\,\ell_{i-1}{/}(m\,{-}\,i\,{+}\,1)$. This way AMPC advances only if $\ell_i\,{=}\,J(s_{k+\widehat{h}})$ is at least $\Delta_i$ apart from $\ell_{i-1}\,{=}\,J(s_{k})$.

This approach allows us to force PSO to 
escape from a local minimum, even if this implies passing over a bump, by gradually increasing the exploration horizon $h$. We assume that the MDP is controllable and that the set $G$ of good states is not empty, which means, that from any state, it is possible to reach a state whose fitness decreased by at least $\Delta_i$. 
%Figure~\ref{fig:approach} illustrates our approach.
Algorithm~\ref{alg:lerec} illustrates our approach.

%, which terminates if the stable state has been reached or the time elapsed.
%\begin{figure}[t]
%\centering
%	\input{diagram2}
%\vspace*{-2mm}
%\caption{Graphical representation of AMPC.}
%	\label{fig:approach}
    %\vspace*{-5mm}
%\end{figure}

%\input{algo_RPH}
\input{algo_LEREC}

\begin{theorem}[AMPC Convergence]
\label{thm:ampc}
Given an MDP $\M\,{=}\,(S,A,T,J)$ with positive and continuous fitness function $J$, and a nonempty set of target states $G\,{\subset}\,S$ with $G\,{=}\,\{s\,|\,J(s)\,{<}\,\varphi\}$. If the transition relation $T$ is controllable with actions in $A$, then there is a finite maximum horizon $h_{\mathit{max}}$ and a finite number of execution steps $m$, such that AMPC is able to find a sequence of actions $a_1,\ldots,a_m$ that brings a state in $S$ to a state in $G$ with probability one.
\end{theorem}

\begin{proof}
In each (macro) step of horizon length $h$, from level $\ell_{i-1}\,{=}\,J(s_k)$ to level $\ell_i\,{=}\,J(s_{k+\widehat{h}})$, AMPC decreases the distance to $\varphi$ by $\Delta_i\,{\geqslant}\,\Delta$, where $\Delta\,{>}\,0$ is fixed by the number of steps $m$ chosen in advance. Hence, AMPC converges to a state in $G$ in a finite number of steps, for a properly chosen $m$. AMPC is able to decrease the fitness in a macro step by $\Delta_i$ by the controllability assumption and the fairness assumption about the PSO algorithm. Since AMPC is a randomized algorithm, the result is probabilistic.
\end{proof}

Note that AMPC is a general procedure that performs adaptive MPC using PSO for dynamical systems that are controllable, come with a fitness metric, and have at least one optimal solution. In an adversarial situation two players have opposing objectives. The question arises what one player assumes about the other when computing its own action, which we discuss next.
