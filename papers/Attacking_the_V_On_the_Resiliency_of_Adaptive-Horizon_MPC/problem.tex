\section{Controller-Attacker Games: Problem Definition}
\label{sec:problem}

%\todo[inline]{add a definition on what a game is! --> Russell Norvig!}
We are interested in games between a controller and an attacker, where the goal of the controller is to take the system to a desired set of states, and the goal of the attacker is to keep the system outside these states.  We formulate our problem by using a Markov Decision Processes (MDP) such that the controller and the attacker jointly determine the transition probabilities.

\begin{definition}A \textbf{Markov Decision Process (MDP)} $\M$ is a tuple $(S,A,T,J)$ consisting of: (1)~a set $S$ of states, (2)~a set $A$ of actions, (3)~a function $T: S\,{\times}\,A\,{\times}\,S\,{\mapsto}\,[0,1]$, where $T(s,a,s^\prime)$ is the probability of transitioning from state $s$ to state $s'$ under action $a$, and (4)~a function $J:S\,{\mapsto}\,\mathbb{R}$, where $J(s)$ is the reward (fitness) associated to state $s$.
%(with an initial state $s_0$), a set of actions $A$, a transition model $T$, and a cost function $J$. An MDP is called \textbf{deterministic} if $\forall s\in S,a\in A$ transition model $T$, such that $S\,{\times}\,{A}\,{\rightarrow}\,{S}$, specifies a unique state.
\end{definition}

\begin{figure}[t]
	\centering
	\input{game_diagram}
\vspace*{-2mm}
\caption{Controller-Attacker Game Architecture}
\label{fig:ampc}
     \vspace*{-3mm}
\end{figure}

In a \textbf{stochastic game}~\cite{shapley1953stochastic}, the transition probability from state $s$ to state $s^\prime$ is controlled jointly by two players, a controller and an attacker in our case. To view an MDP as a stochastic game, we assume that the set of actions $A$ is given as a product $C\,{\times}\,D$, where the controller chooses the $C$-component of an action $\va$ and the attacker chooses the $D$-component of $\va$. We assume that the game is played in parallel by the controller and the attacker; i.e., they both take the state
$s(t)\in {S}$ of the system at time $t$,  
compute their respective actions $c(t)\in C$ and $d(t)\in D$, 
and then use the composed action $(c(t),d(t))$
to determine the
next state $s(t+1)\in {S}$ of the system (based on the transition function $T$).
% ASHISH: I commented out the text that was flock-specific (below), and replaced it by generic text above.
%$(\xv(t),\vv(t))\in{S}$ at time $t$, compute the accelerations $\va(t)$ and displacements $\vd(t)$ and pass them to the flock, which than computes the next state $(\xv(t+1),\vv(t+1))$.  

We consider {\em{randomized strategies}} for both the controller and the attacker. A randomized strategy is a mapping taking every state $s$ to a probability distribution $P(a\,{|}\,s)$ over the (available) actions. 
Once we fix a randomized strategy for the controller, and a randomized strategy for the attacker, the MDP reduces to a Markov chain on the state space $S$. Thus, 
the controller and attacker jointly fix the probability of transitioning from a state $s$ to a state $s^\prime$. 

In this paper, we consider {\em{reachability games}} only. In other words, we are given a set $G$ of ``good'' states and the goal of the controller is to reach a state in $G$. Let $s_0\,{\rightarrow}\,s_1\,{\rightarrow}\,s_2\,{\rightarrow}\,\cdots$ be a sequence of states (a run of the system). The controller wins on this run if $\exists{i}: s_i \in G$, and the attacker wins otherwise.

%\vspace*{-1.5mm}\begin{remark}
We are interested in discrete-time continuous-space dynamical systems.  Formally, the state space $S$ is $\mathbb{R}^n$ and the action space $A$ is in $\mathbb{R}^m$. In the bird flocking example, $n\,{=}\,m\,{=}\,4\,{\cdot}\,B$, where $B$ is the number of birds. We have four state variables and four action variables, respectively for each bird. They represent the $x$- and the $y$-components of the position $\xv_i$, velocity $\vv_i$, acceleration $\va_i$, and displacement $\vd_i$ of each bird $i$, respectively.
%\end{remark}

%\vspace*{-2mm}\begin{remark}
A classical problem in the study of games pertains to determining the existence of an optimal winning strategy (e.g.~a Nash equilibrium) for a player.  We are {\em{not}} concerned with such problems in this paper.  Due to the uncountably many states in the state- and action-space, solving such problems for our games of interest is extremely challenging. Instead, we focus on the problem of determining the likely winner of a game where the strategy of the two players is fixed.  Since we consider randomized strategies, determining the likely winner is a statistical model checking problem.  In other words, we want to evaluate the resilience of certain controllers under certain attack models.
%\end{remark}

%\begin{definition}
%First introduced by~\cite{shapley1953stochastic}, a \textbf{stochastic game} is a dynamic process proceeding from state (position) to state according to transition probabilities controlled jointly by two players.
%\end{definition}

We are now ready to formally define the problem we would like to solve.
\begin{definition}[Stochastic-game verification problem]
Let $\M\,{=}\,(S,A,T,J)$ be an MDP, where 
$A\,{=}\,C\,{\times}\,D$, and randomized strategies
$\sigma_C: S\,{\mapsto}\,PD(C)$ and
$\sigma_D: S\,{\mapsto}\,PD(D)$ mapping states $S$ to probability distributions
over $C$ and $D$. 
%
The \emph{stochastic-game verification problem} is to
determine the probability of reaching a state in $G\,{\subset}\,S$
in $m$ steps, for a given $m$, starting from an initial state (taken from a given probability distribution) in the underlying Markov chain induced by
strategies $\sigma_C,\sigma_D$ on the MDP $\M$.
\end{definition}

%\begin{remark}
Let us specify the randomized strategies. For a strategy
$\sigma$, we assume that we are
given a randomized algorithm that takes a state $s$ and
returns an action consistent with the probability distribution $\sigma(s)$.
%\end{remark}

%\begin{remark}
Our main interest here is in evaluating the resilience of a 
specific controller algorithm $\sigma_C$. The key assumption that
the controller and the attacker algorithms make is 
the existence of a {\emph{fitness function}}
$J: S\,{\mapsto}\,\mathbb{R}^{+}$ such that 
\begin{eqnarray*}
G & := & \left\{ s \mid J(s) < \varphi \mbox{ for some very small $\varphi > 0$} \right\}.
\end{eqnarray*}
Given such a fitness metric $J$, the controller works by minimizing the fitness of states reachable, in one or more steps, as it is done in model-predictive control (MPC).  Since the fitness function is highly nonlinear,
the controller uses an optimization procedure based on randomization to search for a minimum. Hence, our controller is a randomized procedure. One possible attack strategy we consider (for an advanced attacker) is based on the fitness function as well: the attacker tries to maximize the fitness of reachable states. 
%\end{remark}

A key contribution of our work is an adaptive MPC procedure called AMPC. Recall that traditional MPC uses a fixed finite horizon to determine the best control action. The AMPC procedure chooses it dynamically. Thus, AMPC can adapt to the severity of the action played by its adversary by choosing its own horizon accordingly. 
The AMPC procedure is inspired by an adaptive optimization procedure recently presented in~\cite{lukina2016arxiv}, which dynamically changes the amount of the effort it uses to search for a better solution in each step.  
The motivation for adaptation in~\cite{lukina2016arxiv} however was different, namely to take the optimizer out of a local minimum, and thus, ensure convergence to a global optimum.



% \begin{remark}
% Ashish: I am done with first pass on Section 4. The text in this remark is ``old'' text from Section 4. Perhaps it is obsolete now. Check. ----------------------------------------------------\\
% We assume that a position in the game is determined by the formation of the flock and its velocities. Hence, the acceleration is a control variable for both players at each round.

% \todo[inline]{Anna: introducing stochastic games bellow (further elaboration might be needed)}
% Behavior of intelligent agents in physical environment results in a stochastic process, the basic model for which is a Markov chain. Introducing a controller that associates a set of action with each state of the Markov chain we extend it to a Markov decision process, which is a one-player stochastic game.

% In \cite{vrieze1997competitive} a notion of competitive Markov decision processes is defined as two-player zero-sum stochastic game (with perfect information). Each player has her own objective opposing the objective of the other. One player tries to prevent the other to fulfill her objective.

% \begin{definition}
% The hypothesis of \textbf{imperfect information}:
% \begin{enumerate}
% \item All players know the rules of the game.
% \item Each players when making a decision is partially informed about previous events (including her own actions).
% \end{enumerate}
% \end{definition}

% \begin{problem}
% \textbf{Reaching stable state (V-formation).} Given an MDP (a flock) in an arbitrary initial state $s_0$, obtain a control sequence of actions leading to a stable state (V-formation) within bounded number of levels.
% \end{problem}

% \begin{problem}
% \textbf{Attack-resilient control.} Two players, the controller and attacker, are given an MDP (a flock) in a stable state (V-formation), the algorithm LEREC, and opposing goals. Under imperfect information the controller has to win the sequential competitive game in discrete time with the attacker within bounded number of steps. 
% \end{problem}

% The games we are considering in Section~\ref{sec:modes} lead to enormous state-space and therefore we are not aiming at finding optimal solution to them.\\
% End of remark 3.-------------------------------------------------
% \end{remark}

