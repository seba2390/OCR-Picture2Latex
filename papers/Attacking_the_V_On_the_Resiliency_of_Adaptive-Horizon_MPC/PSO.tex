\subsection{Background on Particle Swarm Optimization}
\label{sec:swarmOptimization}

Particle Swarm Optimization (PSO) is a randomized approximation algorithm for determining the parameters that minimize a possibly nonlinear and possibly discontinuous cost (or fitness) function. PSO was first introduced by~\cite{Kennedy95particleswarm}. In an interesting twist of events, PSO took its original inspiration from bird flocking.


%As in~\cite{lukina2016arxiv}, our controller-PSO uses ``acceleration birds'' (these are the particles in the swarm). They should not be confused with the actual flocking birds. 
The PSO procedure is best described using the metaphor of a swarm of insects collaboratively trying to find the location of food. The insects, also called particles, live in the space defined by all possible valuations of the unknown parameters (of the optimization problem).  The food is located at the position where the objective function is minimized. PSO works by having a swarm of particles, which have the same goal of finding food (the reward) without knowing its location. Each particle is informed about its distance to the food (value of the objective function). The PSO algorithm repeatedly redistributes each particle towards the one closest to the food, with a speed proportional to the distance separating them, until all particles converge to the same position. 


AMPC employs Matlab's toolbox $\texttt{particleswarm}$, which performs the classical version of PSO. A swarm of $p$ particles is sampled uniformly at random within a given bound on their positions and velocities. In the bird flocking example, if we try to find acceleration vectors by optimization over horizon $h$, then {\em{one}} ``particle'' represents $h$ 2-dimensional vectors for each of the $B$ birds, along with a vector of values that determine how these $h\cdot B$ acceleration vectors will be updated. 
%
%In the games we are considering each particle represents either a flock of bird-accelerations sequence $\{\va_i^{h}\}_{i=1}^b$, or a a flock of displacements sequence $\{\vd_i^{h}\}_{i=1}^b$, where $h$ is the current length of the receding horizon. 
After choosing a neighborhood of random size for each particle $j$, $j\,{\in}\,\{1,\ldots,p\}$, PSO computes the value of the given fitness function for each particle, and stores two vectors for each particle $j$: its so-far personal-best position $\mathbf{x}_{P}^j(t)$, and the position of its fittest neighbor $\mathbf{x}_{G}^j(t)$. The positions and velocities of the particle swarm $j\,{\in}\,\{1,\ldots,p\}$ are updated the following way:

\vspace*{-4mm}
\begin{align}
\mathbf{v}^j(t+1) = \omega\cdot\mathbf{v}^j(t) &+ y_1\cdot \mathbf{u_1}(t+1)\otimes(\mathbf{x}_{P}^j(t)-\mathbf{x}^j(t))  \nonumber \\
&+ y_2\cdot \mathbf{u_2}(t+1)\otimes(\mathbf{x}_{G}^j(t)-\mathbf{x}^j(t)),
\label{eq:swarm}
\end{align}

\vspace*{-1mm}\noindent{}where $\omega$ is an \emph{inertia weight}, which quantifies the trade-off between global and local exploration of the swarm (the value of $\omega$ is proportional to the exploration range); $y_1$ and $y_2$ are the \emph{self adjustment} and the \emph{social adjustment}, respectively; $\mathbf{u_1},\mathbf{u_2}\,{\in}\,{\rm Uniform}(0,1)$ are random variables; and $\otimes$ is the vector dot product, that is, $\forall$ random vector $\mathbf{z}$: $(\mathbf{z}_1,\ldots,\mathbf{z}_b)\otimes(\mathbf{x}_1^j,\ldots,\mathbf{x}_b^j)=(\mathbf{z}_1\mathbf{x}_1^j,\ldots,\mathbf{z}_b\mathbf{x}_b^j)$.


If the value of the fitness computed at each step of the PSO
for $\mathbf{x}^j(t+1)\,{=}\,\mathbf{x}^j(t)\,{+}\,\mathbf{v}^j(t+1)$ falls below the one for $\mathbf{x}_{P}^j(t)$, then $\mathbf{x}^j(t+1)$ is reassigned to $\mathbf{x}_{P}^j(t+1)$. A global best for the next iteration is determined as the particle with the best fitness among $j\,{\in}\,\{1,\ldots,p\}$. The stopping criterion of the PSO algorithm is either reaching the maximum number of iterations set in advance, or reaching the set time bound, or satisfying the minimum criterion. 

PSO can be used to solve any optimization problem. We use it to solve the optimization problem generated in the MPC approach. In a V-formation game, it can be used to obtain the birds' best accelerations, or even the best displacements, at each time step -- depending on whether MPC/PSO is being used by the controller or the attacker.

\emph{Remark}. We assume that PSO is fair, in the sense that it has a chance to sample all the points in the parameter space, and therefore it has the chance to find the optimal solution with probability one, given enough time.

%In a similar spirit, our advanced-attacker-PSO uses so called "displacement birds" (particles).


