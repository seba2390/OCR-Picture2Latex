\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage[normalem]{ulem}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}



\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{eqparbox}
\usepackage{placeins}
\usepackage{mathtools}
\usepackage{color,enumitem}


\newcommand{\disc}{\textnormal{disc}}
\newcommand{\gan}{\textnormal{GAN}}
\newcommand{\Id}{\textnormal{Id}}

\renewcommand{\algorithmiccomment}[1]{\hfill\eqparbox{COMMENT}{// #1}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{lemma}{Lemma}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{473} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{A Two-Step Disentanglement Method}

\author{Naama Hadad\\
Tel Aviv University\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Lior Wolf\\
Facebook AI Research and Tel Aviv University\\
\and
Moni Shahar\\
Tel Aviv University
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at~\url{https://github.com/naamahadad/A-Two-Step-Disentanglement-Method}.%In order to evaluate the latter, we developed a hypothetical trading 
%strategy whose performance is affected by the performance of the disentanglement, namely, it trades better when 
%the factors are better separated. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

%Financial data analysis has attracted considerable attention in recent years, although the first quantitative models were presented 
%almost fifty years ago. In the early 1970s Black and Scholes~\cite{BS} gave the first formula for options pricing, where their theory was later extended by Merton~\cite{Mer}. For the microstructure of stock pricing, many models from economics~\cite{roll}, time series analysis~\cite{tsay} and random matrix theory~\cite{laloux} were applied. For an interesting survey on market microstructure and the statistical methods used to analyze it see~\cite{madha}.

The problem of identifying complementary factors and separating them from each other is ubiquitous. In face recognition and in object recognition, one would like to separate illumination and pose from identity or label. In handwriting recognition, we would like to separate the factors that define the content of the text written from those that define its style. The separation between what is spoken and who is the speaker in automatic speech recognition and multi-speaker speech synthesis is similar in nature. In each of these domains, specialized solutions have emerged, oftentimes emphasizing recognition and eliminating the other factors and sometimes employing compound labels from orthogonal domains. However, the task of separating the factors that generated the observations, which is called \textit{disentanglement}, is also being studied as an abstract pattern recognition problem.

In this work, we present a new algorithm for disentanglement of factors,  where the separation is based on whether the factors are relevant to a given classification problem. Following the terminology used in~\cite{disentanglement}, we call the factors that are relevant for the classification task \textit{specified factors}, and those which are not \textit{unspecified factors}. %The algorithm uses neural networks to solve both the classification and the disentanglement problems. 
In order to perform disentanglement, we present a new adversarial technique. First, a classifier $S$ is trained to predict the specified factors. The activations of $S$ are then used to capture the specified component of the samples. A second network $Z$ is then trained to recover the complimentary component. A first loss on $Z$ ensures that the original sample can be reconstructed from the output of both networks together ($S$ and $Z$). A second loss on $Z$, which is based on an adversarial network, ensures that $Z$ does not encode the specified factors. The algorithm has the advantage that is makes very weak assumptions about the distribution of the specified and the unspecified factors. 

We focus our experiments on the image-based benchmarks used in previous work. In addition to image data, we also evaluate our model on financial data. A simplified model for stock price changes is that the price change can be decomposed into two factors, market movement and idiosyncratic movement. A common assumption is that the market return is a Geometric Brownian motion and cannot be predicted. However, since different stocks have different correlations with the market, one can neutralize the market factor from her portfolio. In real-life trading, the correlations are non-stationary and there are other effects such as trading costs that should be taken into account. Despite all that, the disentanglement of driving factors is relevant both for prediction purposes as well as for data generation. 
%In reality, the price movements are much more complicated and there may be many factors that drive it. 
%One must bear in mind that, for financial data, simple economic factors, such as market correlation, sector correlation etc. explains only a small part of the variance, and more advanced statistical tools are needed.% For a non-representing sample of economical and statistical methods applied to extract factors see:~\cite{bilson},\cite{ludvigson},\cite{stock},\cite{back} and others. 

\subsection{Related work}

\noindent {\bf Disentanglement} was studied in many contexts and has a vast literature. Early attempts to separate text from graphics using basic computer vision tools were made in~\cite{fletch}. In~\cite{tenenbaum} voice data was analyzed. It was assumed that the data was generated by two sources and separation was done using a bilinear model. Manifold learning methods was used by ElGammal and Lee in order to separate the body configuration from the appearance~\cite{elgammal}. In recent years, few papers tackled this problem using neural networks. What-where encoders~\cite{huang} combine the reconstruction criteria with the discrimination in order to separate the factors that are relevant for the labels. In~\cite{kingma} variational auto encoders were used to separate the digit from the style. However their approach can not generalized to unseen identities. This restriction was relaxed in~\cite{disentanglement}, where they trained a conditional generative model by using an adversarial network to remove label information from the unspecified part of the encoding.  

Concurrently with our work, the Fader Networks~\cite{fader} employ an architecture that is closely related to the second step of our two-step architecture. While in our model a classifier is trained to capture the specified factors, in the architecture of~\cite{fader}, the labels are used directly. The main advantage of our architecture in comparison to the one step alternative, is its support of novel labels at test time, i.e., it is not limited to the set of labels seen during training. This quality of our architecture is crucial for the Norb and Sprites datasets we present later, where we use the disentanglement for new identities at test time. In the modeling of the financial data this quality also comes into effect. For this data, the specified factors (the labels) denote the market regime during train years, whereas during test years there may be different market regimes.

\noindent{\bf Generative Adversarial Networks} GAN~\cite{gan} is a method to train a generator network $G$ that synthesizes samples from a target distribution given noisy inputs. In this approach, a second network called the discriminator $D$ is jointly trained to distinguish between generated samples and data samples. This ``competition'' between the generator and the discriminator, induces a zero-sum game whose equilibrium is reached when the discriminator can no longer distinguish between the generated samples and the empirical ones. Since this approach was published, many variations on this idea has appeared, see for example~\cite{radford,denton2015deep,infogan}. 

\section{Method}

\paragraph{The Problem of Disentanglement}
We are given a set of labeled inputs $X$ with the matching labels $Y$. Our goal is to represent the data using two disjoint parts of a code, $S$, and $Z$. We require $S$ to contain all the information relevant for the class ids $Y$, and $Z$ to contain only the unspecified factors of the data. For the example of handwriting recognition, if $Y$ is the text written in the image samples $X$, then $S$ will contain the information about the textual content $Y$, whereas $Z$ will only contain information about the style of writing. 

\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
      \begin{tabular}{c}
  \includegraphics[width=.7\linewidth,trim={0 1cm 0 0},clip]{figures/NetAFix.jpg} \\
    (a) \\
\includegraphics[width=.9\linewidth,trim={0 1cm 0 0},clip]{figures/NetB3Fix.jpg} \\
    (b)\\
    \end{tabular}
  \caption{\label{fig0_Net} Network architecture: (a) We train the  $S$ encoder and its classification network on a pure classification task. (b) Once $S$ is given, we freeze its weights and train the Enc-Dec network and the adversarial classifier alternatively}
\end{figure}

\paragraph{The Model}
For the encoding, we chose $S$ and $Z$ to be vectors of real numbers rather than a one-hot vector. This idea, presented in~\cite{disentanglement}, enables the network to generalize to identities that did not appear in the training set.  

We define a new network architecture for the disentanglement of the factors. It is simpler and more straightforward than the one presented in~\cite{disentanglement}. The network contains two deterministic encoders to map $X$ to its specified and unspecified components $S=Enc_S(X)$ and $Z=Enc_Z(X)$ accordingly. To train the $S$ encoder $Enc_S$, we first use a sub-network for the classification task and train the $S$-classifier concurrently with $Enc_S$. This sub-network accepts $X$ as its input, encodes it to a vector $S$, and then runs the $S$-classifier on $S$ to obtain the labels $Y$, see Fig.~\ref{fig0_Net}(a). The result of this network is an encoding of the data that contains the information needed in order to predict the class identity. 

In a second step $Enc_S$ is kept fixed. To train the $Z$-encoder to ignore the specified factors and contain data only on the unspecified factors, we use a new variation of adversarial networks. The configuration of the network is given in Fig~\ref{fig0_Net}(b), and it is composed out of two network branches. The adversarial classifier (see the bottom part of the figure) is being trained to minimize the classification loss given $Z,Y$ as input, namely, it is trained to classify $Z$ to $Y$. The Enc-Dec network (the rest of the network) is trained to minimize the sum of two terms: (i) the reconstruction error (given $S$ and $Z$), and (ii) minus the adversarial network loss.

More formally, let $\theta_Z$ be the parameters of $Enc_Z(X)$ and let $\theta_X$ be the parameters of the reconstruction network with output $\tilde{X} = Dec(S,Z)$. Let $\theta_A$ be the parameters of the adversarial network. We define $L_{adv}(\{(Z,Y)\},\theta_A)$ to be the classification loss of the adversarial network and $L_{rec}(\{S,Z,X\},\theta_X)$ to be the reconstruction loss of $\tilde{X}$. When optimizing $\theta_A$, $L_{adv}$ is minimized. When optimizing the two other networks, $\theta_Z$ and $\theta_X$, the objective is to simultaneously minimize $L_{rec}$ and maximize $L_{adv}$. Hence, our objective is: 

\begin{equation}\label{eqn_loss}
	\underset{\theta_Z,\theta_X,\theta_A}{min} \{L_{rec} - \lambda *L_{adv}\}, \lambda>0
\end{equation}

Note that while GANs are typically used in order to improve the quality of generated output such as images, here we use an adversarial configuration to encourage the encoding to "forget" information about the labels, which, in turn, leads to the disentanglement. 

Training the $S$ encoder together with the $Z$ encoder and the subsequent decoder, could lead the network to converge to a degenerated solution, where all information is encoded in $S$, whereas $Z$ holds no information on any factor. By training the $S$ network in the first stage with a limited capacity, and then fixing the values of its parameters, this scenario is avoided. Since $S$ has a limited capacity it ignores most of the information on the unspecified factors, which is irrelevant for its goal.  
\paragraph{Training details}

We employ MSE for the $L_{rec}$ loss, and use categorical cross-entropy loss for both the $S$ classifier's loss and $L_{adv}$. The $\lambda$ for each dataset was chosen independently using few iterations on validation data. 

For the training of the $S$-network and the Enc-Dec network, we apply the Adam optimization method~\cite{adam} with a learning rate of 0.001 and beta of 0.9. For the adversarial classifier, we used SGD with a learning rate of 0.001. 

While training the $Z$-network, we have noticed that the adversarial part requires more steps to stabilize, since it should solve a complicated classification task on a changing input. %THIS IS NOT TRUE IN MANY CASESNote that usually GANs are used for discrimination only, whereas in our network the adversarial part is a classifier and hence more complicated. 
Therefore, we run, at each iteration, one mini-batch to train the Enc-Dec network, followed by three mini-batches to train the adversarial network. 

\subsection{Comparison to~\cite{disentanglement} on Toy Data}
To illustrate the advantages of our approach in comparison to the more involved method of~\cite{disentanglement}, we generated images of a gray rectangle in ten possible locations on a white or black background. We refer to the background color as the unspecified factor $Z$, whereas the location of the rectangle is the specified factor $S$. We denote the ten possible values of $S$ by $\{s_0,\dots,s_9\}$. All possible twenty images were drawn with equal probability. 

 We also generated similar images, where the unspecified factor consists of two binary variables - the first controls the upper half background color and the second controls the lower half background color. Where similar to the first case all forty images were drawn with equal probability. 
 
We refer to the sets as \textit{Synth1} and  \textit{Synth2}. 
For the encoding, we chose both $S$ and $Z$ to be vectors of size $4$. We run our network and the network in~\cite{disentanglement} to obtain $S,Z$ for \textit{Synth1} and  \textit{Synth2}. We then used a neural network classifier with three dense layers of size $8$ on $S$ to find the label. The obtained accuracy was $100\%$ for both networks. 
%The classifiers we used on $S$ and $Z$ are neural networks with {\color{red} XX} hidden layers.
% We run our network and the network in~\cite{disentanglement} to obtain $S,Z$ for \textit{Synth1} and  \textit{Synth2}.
%  In table~\ref{tbl_syntacc} we show that both networks converged to a disentangled solution. To see that we applied the classifiers and got that $Pr[s_i|Z] \approx 0.1$, and that the rectangle state $s_i$ was perfectly classified from $S$. Applying the classifier on the resultant $Z$ of our method gave perfect classification, whereas applying the classifier on the $Z$ of ~\cite{disentanglement} method gave $95\%$ accuracy. This is probably because of the built-in randomness of the variational encoder employed in the latter.

We then examined the $Z$ vectors that were obtained from both methods of disentanglement on \textit{Synth1} and \textit{Synth2}. First we verified using a classifier that the background color can be inferred from $Z$ perfectly, whereas inferring the location of the rectangle from $Z$ leads to accuracy that is close to random. Next we turned to examining the distribution of $Z$ for \textit{Synth1} and \textit{Synth2}, these distributions are presented in Fig.~\ref{fig_Synt1},\ref{fig_Synt2} respectively. The figures show the values of the components $Z_0,\dots,Z_3$ for all of the data points. Each color represents a different value of the latent variable $Z$. Note that since the method in~\cite{disentanglement} defines $Z$ to be a random vector drawn from a normal distribution specified by $\mu,\sigma$, we show a sample of the drawn $Z$ vectors. 

For the binary case (\textit{Synth1}, Fig.~\ref{fig_Synt1}) our encoding shows two narrow peaks well separated in $Z_3$, whereas all other components have one peak %. For the more complex \textit{Synth2} case, obth z[0] and z[1] display bimodal distributions 
(the coordinate that contains the information is completely arbitrary since all coordinates are treated the same way). In the VAE encoder, the information was separated only for $Z_0$, but even the best classifier (which is LDA in this case) will have some error, since the peaks were not disjoint. This simple experiment also demonstrates that our results are simpler to understand. 

The gap in the explicitness of the results as encoded in $Z$  is more apparent on \textit{Synth2}. In Fig.~\ref{fig_Synt2}, we see that our encoding of $Z$ is well separated on $Z_0$ and $Z_1$ while in the other method, one cannot tell the number of values without further analysis or prior knowledge about $Z$. Moreover, applying standard PCA on the sampled $Z$ vector of the auto encoder, gave four components with similar variance, as shown in Tab.~\ref{tbl_syntpca}.    

% \begin{table}[]
% \centering
% \caption{Accuracy results for classifiers of S/Z ids based on S/z encoding for z dimensions of size 1 and 2 for our model and for the reference model}
% \label{tbl_syntacc}
% \begin{tabular}{lccc}
% \hline
%  & \textbf{Our model} & \textbf{~\cite{disentanglement} model}  \\ \hline
% \textbf{Sid from encoded Z, \textit{Synth1}} & 9.80\%             & 12.50\%                                  \\
% \textbf{Sid from encoded S, \textit{Synth1}} & 100\%              & 100\%                                 &                                       \\
% \textbf{Zid from encoded Z, \textit{Synth1}} & 100\%              & 95.50\%                                  \\ \hline
% \textbf{}                                            & \textbf{Our model} & \textbf{~\cite{disentanglement} model}  \\ \hline
% \textbf{Sid from encoded Z, \textit{Synth2}} & 13\%               & 13.20\%                                   \\
% \textbf{Sid from encoded S, \textit{Synth2}} & 100\%              & 100\%                                    &                                       \\
% \textbf{Zid from encoded Z, \textit{Synth2}} & 100\%              & 94.30\%                               
% \end{tabular}
% \end{table}

% \begin{figure}[t]
%   \centering
%   %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%     \begin{tabular}{cc}
%   \includegraphics[width=.48\linewidth]{figures/MyNet1colorL.jpg}&%SyntMyl1.jpg}&
%   \includegraphics[width=.48\linewidth]{figures/DistNet1colorL.jpg}\\%/Dist095Samp.jpg}\\
%     (a) & (b)\\
%     \end{tabular}
%   \caption{\label{fig_Synt1}Unspecified factor of dimension 1 (a) Z different components histogram for the encoding from our model (b) Z different components histogram for the encoding from ~\cite{disentanglement} model. }
% \end{figure}

\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \begin{tabular}{c}
  \includegraphics[width=.948\linewidth]{figures/MyNet1colorLL.jpg}\\%SyntMyl1.jpg}&
  (a)\\
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/DistNet1colorLLFix.jpg}\\%/Dist095Samp.jpg}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig_Synt1} {\it Synth1} data: The dimension of the unspecified factors is $1$. (a) histogram of different $Z$ components of our model (b) same histogram for the encoding of~\cite{disentanglement}}
\end{figure}


\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \begin{tabular}{c}
  \includegraphics[width=.948\linewidth]{figures/MyNet2colorLLL.jpg}\\%TwoCompMy.jpg}&
  (a)\\
  \includegraphics[width=.948\linewidth]{figures/DistNet2colorLLLFix.jpg}\\%Dist095Samp2.jpg}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig_Synt2} {\it Synth2} data: The dimesnion of the unspecified factors is $2$. (a) An histogram of the components of $Z$ of our model. (b) Same histogram for the encoding from the model of~\cite{disentanglement}}
\end{figure}

% \begin{table}[]
% \centering
% \caption{Z PCA components explained variance for unspecified factor of dimension 1 and 2}
% \label{tbl_syntpca}
% \begin{tabular}{b{4cm}lcccc}
% \hline
% \textbf{PCA component sorted by explained variance}         & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\ \hline
% Our model, \textit{Synth1}                     & 1.000          &            &            &            \\
% ~\cite{disentanglement} model, \textit{Synth1}       & 0.252      & 0.251      & 0.25       & 0.247      \\
% Our model,  \textit{Synth2}                 & 0.610       & 0.390       &            &            \\
% ~\cite{disentanglement} model, \textit{Synth2} & 0.263      & 0.249      & 0.248      & 0.240       \\ \hline
           
% \end{tabular}
% \end{table}
% \begin{table}[]
% \centering
% \caption{Z PCA components explained variance for unspecified factor of dimension 1 and 2}
% \label{tbl_syntpca}
% \begin{tabular}{ccccccccc}
% \toprule
%        & \multicolumn{4}{c}{\textbf{\textit{Synth1}}}                       & \multicolumn{4}{c}{\textbf{\textit{Synth2}}}                       \\ \cmidrule{2-5} \cmidrule{6-9}
% \textbf{PCA component:}             & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\ \midrule
% Our model             & 1.000      &            &            &            & 0.610      & 0.390      &            &            \\
% ~\cite{disentanglement}  & 0.252      & 0.251      & 0.25       & 0.247      & 0.263      & 0.249      & 0.248      & 0.240      \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[]
\centering
\begin{tabular}{ccccc}
\toprule
       & \multicolumn{4}{c}{\textbf{\textit{Synth1}}}                       \\ \cmidrule{2-5} 
\textbf{PCA component:}             & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\ \midrule
Our model             & 1.000      &            &            &            \\
~\cite{disentanglement}  & 0.252      & 0.251      & 0.25       & 0.247 \\ 
\cmidrule{2-5}       & \multicolumn{4}{c}{\textbf{\textit{Synth2}}}                       \\ 
\textbf{PCA component:}             & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\ \midrule
Our model             & 0.610      & 0.390      &            &            \\
~\cite{disentanglement}  & 0.263      & 0.249      & 0.248      & 0.240      \\ 
\hline
\end{tabular}
\smallskip
\caption{The ratio of the variance of the $Z$-encoding projected on its PCA components. Note that the autoencoder split the information between all the components, whereas our encoding expressed the information using the minimum number of dimensions.}
\label{tbl_syntpca}
\end{table}

\section{Experiments}
% on various visual and financial datasets. Since there is no golden standard for measuring disentanglement we used few criteria to show that the $S$-part captures most of the identity data, and the $Z$-part captures other quantities of the data. These criteria will be explained in details in Section~\ref{sec-comparison}.

We evaluate our method on the visual disentanglement benchmarks used in previous work, as well as on simulated and real financial data. The detailed network architecture used for each of the experiments is described in Tab.~\ref{tab:tblarch}.

\begin{table}[t]
\centering
\resizebox{.995\linewidth}{!}{% <------ Don't forget this %
\begin{tabular}{b{1.6cm}lll}
\hline
& Image Datasets & \textbf{Stocks return} \\ \hline
\textbf{Encoders S,Z}                                                      & \begin{tabular}[c]{@{}l@{}}For MNIST and Sprites\\three 
5x5 convolotional, for \\ NORB and Extended YaleB three 3x3 \\ convolutional layers.\\ 
All convolutional layers with stride 2 \\ and a dense S/Z dimension layer. \\ all with ReLU non-linearities\end{tabular}& \begin{tabular}[c]{@{}l@{}}4 dense layers of sizes \\ 100,66,66,50 with ReLU \\ non-linearities\end{tabular}                              \\ \hline

\textbf{S classifier}                                                      & \begin{tabular}[c]{@{}l@{}} For MNIST and Sprites dense layers\\x 256 hidden units, \\ for NORB,Extended YaleB\\x 16 hidden units \\ Batch Normalization, ReLU\\ and  a softmax for the output\end{tabular}                                    & \begin{tabular}[c]{@{}l@{}}2 dense layers x\\ 50 hidden units, \\ Batch Normalization,\\ ReLU and a softmax output\end{tabular}  \\ \hline

\textbf{Decoder}                                                           & \begin{tabular}[c]{@{}l@{}}Mirroring network to the encoders: \\ dense layer and three convolutional\\ network with upsampling\end{tabular}                 & \begin{tabular}[c]{@{}l@{}}4 dense layers of sizes \\ 70,66,66,100 with\\ ReLU non-linearities\end{tabular}                              \\ \hline

\textbf{\begin{tabular}[c]{@{}l@{}}Adversarial \\ Classifier\end{tabular}} & \begin{tabular}[c]{@{}l@{}}3 dense layers x 256 hidden units, \\ Batch Normalization,ReLU \\ and a softmax for the output\end{tabular}                & \begin{tabular}[c]{@{}l@{}}3 dense layers x 50 \\ hidden units, \\ Batch Normalization, ReLU \\ and a softmax for the output\end{tabular} \\ \hline


\textbf{S, Z \#dims} & 
\begin{tabular}[c]{@{}l@{}}Mnist: 16,16, Sprites: 32,128, \\ NORB and Extended YaleB: 32,256\end{tabular}& 20,50\\ \hline
\end{tabular}
}
\smallskip
\caption{Networks architectures}
\label{tab:tblarch}
\end{table}

\subsection{Image Benchmarks}

We followed a previous work~\cite{disentanglement} and tested our model on four visual datasets - MNIST~\cite{mnist}, NORB~\cite{norb}, Sprites dataset~\cite{sprites} and the Extended-YaleB dataset~\cite{yaleB}.

%\subsection{Visual Performance Evaluation}
For measures of performance on the visual datasets, we also followed the ones suggested in~\cite{disentanglement}. Note that all these measures are subjective.%, since to the best of our knowledge there is no standard visual objective measure for disentanglement.

\begin{itemize}[leftmargin=*]
\item \textit{Swapping} - In swapping, we generate an image using $S$ from one image, $I_1$, and $Z$ from a different image, $I_2$. In a good disentanglement, the resultant image preserves the $S$-qualities of $I_1$ and the $Z$-qualities of $I_2$.

\item\textit{Interpolation} - Interpolation of two source images is a sequence of images generated by linearly interpolating the $S$ and $Z$ components  of the two different sources. The measure is again done by visually judging the resultant images. i.e., we expect to see "more" of the look of the second image, the bigger its weight gets. Interpolation is done in both the $S$ space and the $Z$ space.

\item\textit{Retrieval} - To assess the lack of correlation between the $S$ and the $Z$ components, we perform a query based on either the $S$ part or the $Z$ part, where in each case we retrieve its nearest neighbors in the corresponding space. 

\item\textit{Classification Score} - In addition to the qualitative measures above, we quantify the amount of information on the class that each part of the code ($S$ and $Z$) contains on the class. Since measuring this directly is a difficult task, we approximate it by running a classification algorithm. A good disentanglement is such that when running the classifier on the $S$ part it gives high accuracy, whereas when running it on the $Z$ part it gives nearly random results. 

\end{itemize}

%\subsection{Visual Datasets}

\textbf{MNIST - } For the MNIST data, the $S$ part is the digit and the $Z$ part is the style. In Fig.~\ref{fig1_Mnist}, we present the results for swapping and interpolation. The rows of the table in the left hand side of the figure shows the style ($Z$) and the columns the digit. To the best of our judgment, the style looks well separated from the content. 


\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \begin{tabular}{c}
  \includegraphics[width=.9948\linewidth,trim={0 1cm 0 0},clip]{figures/MnistW.JPG}\\
  (a)\\
  \includegraphics[width=.9948\linewidth,trim={0 1cm 0 0},clip]{figures/MnistWinter.JPG}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig1_Mnist}(a) Swapping the specified and unspecified components of MNIST images. The images are generated using Z from the left column and S from the top row in the decoder. The diagonal digits show reconstructions. (b) Interpolation results. the images in the top-left and bottom-right corners are from the test set. The other digits are generated by interpolation of S and Z gradually. Z interpolate along the rows and S through the columns. }
\end{figure}


%Swapping and interpolation results for Mnist are given in . For the Sprites dataset, we present retrieval results in Figure~\ref{fig3_Spritesret}.

% \begin{figure}[t]
%   \centering
%   %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%     \begin{tabular}{cc}
%   \includegraphics[width=.48\linewidth]{figures/MnistW.JPG}&
%   \includegraphics[width=.48\linewidth]{figures/MnistWinter.JPG}\\
%     (a) & (b)\\
%     \end{tabular}
%   \caption{\label{fig1_Mnist}(a) swapping the specified and unspecified components of MNIST images. The images are generated using Z from the left column and S from the top row in the decoder. The diagonal digits show reconstructions. (b) Interpolation results. the images in the top-left and bottom-right corners are from the test set. The other digits are generated by interpolation of S and Z gradually. Z interpolate along the rows and S through the columns. }
% \end{figure}


\textbf{Sprites dataset -} This dataset contains color images of sprites~\cite{sprites}\footnote{\url{http://lpc.opengameart.org/} (CC BY-SA 3.0).}. Each sprite character is defined by body type, gender, hair type, armor type, arm type and greaves type. Overall there are 672 different characters, from which we use 572 characters for the training set and 100 characters for the test set. For each character, there are five animations each from four viewpoints, each animation has between 6 and 13 frames. We use character's identity as the specified component. The results from swapping and interpolation are shown in Figure \ref{fig2_Sprites}. Our model learned to separate the character from its position and weapon and generalizes the separation to unseen characters.

Examining the retrieval results in Fig.~\ref{fig3_Spritesret}, it is possible to see that for the $Z$ part (sub-figure (b)), the characters in any row is random but its pose is kept. In the $S$ part (sub-figure (a)), the character is perfectly kept, whereas the pose is not. In~\cite{disentanglement}, it seems that $Z$ holds some information on $S$ because the hair style and color rarely changes between characters. 


\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \begin{tabular}{c}
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/SpritesW32.JPG}\\(a)\\
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/SpritesW32inter2.JPG}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig2_Sprites} (a) Swapping the specified and unspecified components of Sprites. The images are generated using $Z$ from the left column and $S$ from the top row in the decoder. (b) Interpolation results. the images in the top-left and bottom-right corners are from the test set. The other images are generated by gradual interpolation of $S$ and $Z$. $Z$ interpolates along the rows and $S$ through the columns.}
\end{figure}

\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \begin{tabular}{c}
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/SpritesRetS2.JPG}\\(a)\\
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/SpritesRetZ2.JPG}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig3_Spritesret}Sprites retrieval results (a) Querying on the specified component S. (b) Querying on the unspecified component Z. The components of the sprites on the left column are used as the query. }
\end{figure}

% \begin{figure}[t]
%   \centering
%   %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \begin{tabular}{cc}
%   \includegraphics[width=.48\linewidth]{figures/SpritesRetS2.JPG}&
%   \includegraphics[width=.48\linewidth]{figures/SpritesRetZ2.JPG}\\
%     (a) & (b)\\
%     \end{tabular}
%   \caption{\label{fig3_Spritesret} (a) Querying on the specified component s. (b) Querying on the unspecified component z. The components of the sprites on the left column are used as the query. }
% \end{figure}

\textbf{Small NORB dataset~\cite{norb} -} The NORB dataset contains images of $50$ toys belonging to five generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under six different illumination conditions, nine elevations and $18$ azimuths.
The training set is composed of five instances of each category and the test set of the remaining five instances. We use the instance identity as the specified component and have $25$ different labels for the training set. 

For this dataset the swapping results were not perfect. We succeeded in separating different azimuths and background from the instance.
However, for some of the categories, the reconstruction contained mistakes. This is probably due to the high variability between the instances in the train and the test. The numerical results support this hypothesis, since there are big difference between the train and the test errors. The results look good for the interpolation, see Figure~\ref{fig3_norb}. A similar degradation of results was also observed in~\cite{disentanglement}

\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \begin{tabular}{c}
    \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/NorbTestW2.JPG}\\(a)\\
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/NorbTestWinter4d.JPG}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig3_norb} (a) Swapping the specified and unspecified components of the NORB test set images. (b) Interpolation results. These are the same arrangements as in Figure~\ref{fig2_Sprites}.}
\end{figure}


\textbf{Extended-YaleB~\cite{yaleB} -} The extended-YaleB Face dataset contains $16095$ images of $28$ human subjects under nine poses and $64$ illumination conditions. 
The training (test) set contains $500$ (roughly $75$) images per subject. We use subject identity as the specified component. 

Results for swapping and interpolation for images from the test set shown in Figure~\ref{fig5_yale}. For swapping, one can see that illumination conditions are transferred almost perfectly, whereas the position is not perfectly transferred (see for example line 6, column 5). We again suspect that this is mainly because some of the positions were missing in the training set, and with more data we expect the results to improve. For the interpolation, some of the mixed identities do not resemble either sources. 

\begin{figure}[t]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \begin{tabular}{c}
    \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/Yale8.JPG}\\(a)\\
  \includegraphics[width=.948\linewidth,trim={0 1cm 0 0},clip]{figures/Yaletest7.JPG}\\
    (b)\\
    \end{tabular}
  \caption{\label{fig5_yale} (a) Swapping the specified and unspecified components of Yale test set images. (b) Interpolation results. These are the same arrangements as in Figure~\ref{fig1_Mnist}. In this case, the results are visibly inferior to the examples presented in~\cite{disentanglement}}
\end{figure}

%The results for swapping and interpolation are presented in the appendix.%Results for swapping and interpolation for images from the test set shown in figure \ref{fig5_yale}.
% For swapping one can see that illumination conditions are transfered almost perfectly, whereas the position is not perfectly transfered (see for example line 6, column 5). Again we suspect that this is mainly because some of the positions were missing in the training set, and with more data we expect the results to improve. For the interpolation we got results visually inferior to the one in ~\cite{disentanglement}, some of the mixed identities do not resemble each of the sources. 


% \begin{figure}[t]
%   \centering
%   %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%     \begin{tabular}{cc}
%     \includegraphics[width=.48\linewidth]{figures/Yale8.JPG}&
%   \includegraphics[width=.48\linewidth]{figures/Yaletest7.JPG}\\
%     (a) & (b)\\
%     \end{tabular}
%   \caption{\label{fig5_yale} (a) swapping the specified and unspecified components of Yale test set images. (b) Interpolation results. Same arrangements as in ~\ref{fig1_Mnist}. In this case, the results are visibly inferior to the examples presented in~\cite{disentanglement}. }
% \end{figure}

\paragraph{Quantitative results} 
The numerical results for all datasets are shown in Tab.~\ref{tbl1_numericresults}. One can see that the unspecified component is almost agnostic to the identity, while the classifier on the specified component achieves high accuracy.
For comparison with~\cite{disentanglement}, we added to the table the results that were reported in their paper.  For most cases our model achieves higher accuracy for $S$. This is expected, since we train the $S$-encoder for classification. As for the unspecified component $Z$, our performance on the train and the test set are similar, except for the NORB dataset where our error rate is slightly worse.
%One may claim that this method of evaluation since it depends on the training process of the classifier that measures the information. For the training set the error rate of our models is a little lower, maybe due to overfit of the classifier we trained for the analysis. This may change slightly the numerical values for both $S$ and $Z$, however since the train and test error are close for all datasets besides the $S$ classifier of NORB we think that we that this effect can be neglected for all other columns of the table. For a specific reference to the NORB results, see below. 
%For the Sprites dataset we trained two classifiers on $S$ and $Z$ to identify the armor which is one of the attributes of each character. This may not be the only possible choice for attribute, however we followed ~\cite{disentanglement} in order to be able to compare the errors. 
%For this NORB dataset, % the classifiers were trained to identify object category. T
For this dataset, the error rate of $S$ in the test set is much larger than that of the train set, and in~\cite{disentanglement} they explain this result by overfitting. Note that for this dataset, there are only five training instances per category, which makes generalization difficult.

% \begin{table}[]
% \centering
% \caption{Classification error rate based on $S$ or $Z$ for our model and as reported in (Mathieu et al. 2016) *While (Mathieu et al. 2016) reports 60.7\% chance, we observe 56\% in the test set, and 67\% in the train set.}
% %ADD A COLUMN FOR X?
% %S is trained as classifier on X, so unless we'll use different classification network for X we are supposed to get the same results 
% \label{tbl1_numericresults}
% \begin{tabular}{lllllllll}
% \toprule
%                                & \multicolumn{2}{c}{\textbf{Mnist}} & \multicolumn{2}{c}{\textbf{Sprites}}    & \multicolumn{2}{c}{\textbf{NORB}} & \multicolumn{2}{c}{\textbf{Extended-YaleB}} \\
% \cmidrule{2-3}
% \cmidrule{4-5}
% \cmidrule{6-7}
% \cmidrule{8-9}
%                                & Z               & S                & Z                  & S                  & Z               & S               & Z                    & S                    \\ \midrule
% Our (train)                          &   87.0\%              &    0.1\%              & 66.0\%               & 0.0\%                & 78.9\%          & 1.1\%           & 95.7\%               & 0.00\%                    \\
% Our (test)                           & 87.0\%            & 0.8\%            & 58.0\%               & 0.0\%                & 79.2\%          & 15.2\%          & 96.3\%               & 0.00\%                    \\
% \cite{disentanglement} (train) & -               & -                & 58.6               & 5.5\%              & 79.8\%          & 2.6\%           & 96.4\%               & 0.05\%               \\
% \cite{disentanglement} (test)  & -               & -                & 59.8               & 5.2\%              & 79.9\%          & 13.5\%          & 96.4\%               & 0.08\%               \\
% Random-chance                  & \multicolumn{2}{c}{-- 90.0\% --}           & \multicolumn{2}{c}{-- * --} & \multicolumn{2}{c}{-- 80.0\% --}          & \multicolumn{2}{c}{-- 96.4\% --}   \\
% \bottomrule
 
% \end{tabular}
% \end{table}


\begin{table}[]
\centering
%ADD A COLUMN FOR X?
%S is trained as classifier on X, so unless we'll use different classification network for X we are supposed to get the same results 
\label{tbl1_numericresults}
\begin{small}
\begin{tabular}{lllllllll}
\toprule
                               & \multicolumn{2}{c}{\textbf{Mnist}} & \multicolumn{2}{c}{\textbf{Sprites}}   \\
\cmidrule{2-3}
\cmidrule{4-5}
                               & Z               & S                & Z                  & S \\ \midrule
Our (train)                          &   87.0\%              &    0.1\%              & 66.0\%               & 0.0\%   \\
Our (test)                           & 87.0\%            & 0.8\%            & 58.0\%               & 0.0\%    \\
~\cite{disentanglement} (train) & -               & -                & 58.6\%               & 5.5\%       \\
~\cite{disentanglement} (test)  & -               & -                & 59.8\%               & 5.2\%       \\
Random-chance                  & \multicolumn{2}{c}{-- 90.0\% --}           & \multicolumn{2}{c}{-- * --}  \\
\bottomrule
~\\
\toprule
                               & \multicolumn{2}{c}{\textbf{NORB}} & \multicolumn{2}{c}{\textbf{Extended-YaleB}} \\
\cmidrule{2-3}
\cmidrule{4-5}
                               & Z               & S                & Z                  & S \\ \midrule
Our (train)                          & 78.9\%          & 1.1\%           & 95.7\%               & 0.00\%                    \\
Our (test)                           & 79.2\%          & 15.2\%          & 96.3\%               & 0.00\%                    \\
~\cite{disentanglement} (train) &  79.8\%          & 2.6\%           & 96.4\%               & 0.05\%               \\
~\cite{disentanglement} (test)  &  79.9\%          & 13.5\%          & 96.4\%               & 0.08\%               \\
Random-chance                  & \multicolumn{2}{c}{-- 80.0\% --}          & \multicolumn{2}{c}{-- 96.4\% --}   \\


\bottomrule
\end{tabular}
\end{small}
\smallskip
\caption{Classification error rate based on $S$ or $Z$ for our model and as reported in~\cite{disentanglement}. *While~\cite{disentanglement} reports 60.7\% chance, we observe 56\% in the test set, and 67\% in the train set.}
\end{table}

\subsection{Financial data}

%We start by briefly review two important models that we use. 

% %\paragraph{CAPM model} 
% The capital asset pricing model (CAPM)~\cite{sharpe} is one of the first
% quantitative models in finance theory. The model makes two assumptions: (i) one can 
% invest in a risk-free asset of gain $R_f$ (the risk free rate) and (ii) riskier assets have
% better expected compensation. We denote by $R_m,R$ the average return of the market and an asset. The model states the following relation:
% %\begin{equation}\label{eqn1_capm}
% $	\mathbb{E}[R] = R_f + \beta*(E[R_m] - R_f)$, 
% %\end{equation}
% where $\beta = Cov(R,R_m)/Var(R_m)$, is the systematic risk of the asset compared to the market.  There have been many works on fitting the parameters of this model on real data. For a discussion on the statistical aspects of the subject see~\cite{shan92}.

%REMOVED FROM CVPR
% \paragraph{Black-Scholes model} The Black-Scholes model (\cite{BS},~\cite{Mer}) is a theoretical model for options pricing. 
% Its assumptions are: (i) the market is continuous, free of commissions and permits short sell, 
% (ii) you can borrow and lend money at the risk free rate and (iii) the market
% movements follow a Geometric Brownian motion. 

% Based on these and the principle of no arbitrage one can prove that the price of an option with strike price $S$ and time to 
% expiration $T$ depends on the volatility.
% Intuitively, for a call (put) option whose strike price is far above (below) the current price,
% the probability that it would be exercised increases with the volatility, so its price should be higher. 

%The main difference of ~ref(~disentanglement) over previous work (and hence ours too) was the ability to generalize to unseen identities at test time. Here we give simpler model with no prior assumption about z to adjust to unknown distribution of financial or other data.

%Can use related work in InfoGan and LeCun disentanglement

% \subsubsection{Results on Simulated CAPM Data}

% In order to verify our evaluation measures we generated a synthetic data-set that followed the CAPM assumptions and checked how well can we learn the stock specific information and the market return from $S,Z$. The results, shown below, show that $Z$ holds the information about the $\beta$-groups and almost no information about the $E[R_m]$, whereas $S$ holds the complementary information.

% \textbf{The Simulation -} We define $150$ periods of $50$ days for training. For each period, we draw the period risk free rate $R_f$ from the uniform distribution $U[0.0025,0.03]$. The daily market return for each quarter $E[R_m]$ is drawn as $R_f/50+0.04/50*N(0,1)$. The daily returns verctor $R_m$ is drawn i.i.d. from $E[R_m]+0.0005\times N(0,1)$. Lastly, the $\beta$ values for $1500$ assets in each period are distributed $U[-1,3]$.

% For each stock, $X$ is calculated as $R_f/50+ \beta(R_m-R_f/50) + \epsilon$, where $\epsilon$ is distributed $0.0005\times N(0,1)$, 
% concatenated to the $R_m$ vector, plus added noise in order to encourage generalization.
 
% \textbf{Evaluation -} Using a PCA decomposition of size two for $Z, X$ and $S$ and running logistic regression to classify the results to four beta groups, leads to accuracy of $64\%$, $40\%$ and $33\%$ respectively. As expected, $Z$ holds the information about the beta, which is stock specific. $S$ accuracy is much lower than $Z$ accuracy.
 
% On the other hand, using the same PCA components to deduce $E[R_m]$ we get accuracy of $33\%$, $96\%$ and $95\%$ for $Z$, $X$ and $S$. As expected, since $E[R_m]$ is drawn per period, it is a specified factor and is mostly included in $S$.
 
% Optimizing ~\cite{disentanglement} on the same data we obtain accuracy of $54\%$ and $37\%$, trying to classify beta (unspecified) based on $Z$ and $S$ respectively and $75\%$, $95\%$ accuracy for $R_m$ (unspecified) based on $Z,S$. This is additional evidence that our method outperforms ~\cite{disentanglement}. As desired, our $Z$ is better at classifying beta and worse at classifying the specified factor and our $S$ is also worse at classifying beta.


% \subsubsection{Results on Real Financial Data}

We applied our method on the daily returns of stocks listed in NASDAQ, NYSE and AMEX, from the Center for Research in Security Prices (CRSP) database. For all datasets, the results were measured on the test set. Specifically, we used daily returns of stocks from the Nasdaq, NYSE and AMEX exchanges. The training set consists of the years 1976-2009 and the test set 2010-2016. Each year is divided into four quarters of approximately $63$ trading days. As an input to the network, we used for each stock the returns of the first $50$ days of each quarter, as well as the  market returns for the same $50$ days. In order to improve generalization, we added $\epsilon_i$ a random noise $N(0,0.0016)$.

The goal of the disentanglement is to separate market behavior from specific stock's movements. In order to do so, we labeled each quarter in the training set differently, so as to have 136 such labels. Next, we let $S$ encode the label information and $Z$ encode  the rest of the information. %To give further motivation for this decomposition, we assume a hypothetical world where the CAPM model is exact. An optimal representation would be that $Z$ encodes $\beta$ and $S$ encodes the risk-free interest rate $R_f$, as well as the market movements. Note that the adversarial part cannot reconstruct the market returns from $\beta$, but the decoder can perfectly reconstruct  the stock and market returns based on the CAPM model. 

%%We chose the encoding, so that the $S$-part is a vector of length $20$, and the $Z$-part is a vector of length $50$. The other parameters of the network are outlined in Tab.~\ref{}XXXXXX.

%\textbf{Evaluation -} The assessment of the results in this dataset is  not straightforward, since the CAPM model is far from being exact. Furthermore, stocks returns are known to be highly non-stationary and their market correlation is not deterministic. Therefore, 

For evaluation, we employed two metrics, (i) checking the stock specific information from $Z$ and (ii) evaluating a trading strategy based on the predictions that came from $Z$. 

For a sanity check, we start by showing that $S$ contains market information. We did a PCA on the $S$-part of the encoding and examined the first component. This component was correlated with the average return of the market during the tested period. The correlation coefficient between the market return on the test period and the first component of the PCA is $0.55$.   

%To analyze the specified component S we run PCA with two components on the training set and found a correlation coefficient of 0.55 between the first component and overall 50 days market return. Fig. \ref{fig7_pcas} include each stock specified part first component vs. market return.

% \begin{figure}[t]
%   \centering
%   %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%     \begin{tabular}{cc}
%     \includegraphics[width=.48\linewidth]{figures/Strans.jpg}\\
%     \end{tabular}
%   \caption{\label{fig7_pcas} First S PCA component to market return, correlation coefficient = 0.55 }
% \end{figure}

We then defined two stock specific measures based on the Capital Asset Pricing Podel (CAPM)~\cite{sharpe}, which is one of the fundamental quantitative models in finance theory: $\beta$, which is the systematic risk of a given asset, and $\rho$, which is the correlation coefficient with the market during the last year. We constructed a discrete version of these measures with four levels each. The classifier we used is logistic regression, since it dominates the econometrics literature. %, and it also gave good results on the synthetic CAPM-generated data. 
The predictive accuracy on the test set for each of the six models ($2$ measures times $3$ inputs) is given in Table~\ref{Tbl3_betarho}. From this table, we clearly see that we failed to reveal stock properties from $X$ and $S$, but managed to do it from $Z$. 
% \begin{table}[]
% \centering
% \caption{Accuracy for logistic regression classification of $\beta$ and the correlation coefficient $\rho$}
% \label{Tbl3_betarho}
% \begin{tabular}{lllll}
% \toprule
%                & \multicolumn{2}{l}{\textbf{$\beta$}} & \multicolumn{2}{l}{\textbf{$\rho$}} \\
%                  \midrule
% Z              & \multicolumn{2}{l}{34.5\%}          & \multicolumn{2}{l}{30.8\%}                            \\
% S              & \multicolumn{2}{l}{26.0\%}          & \multicolumn{2}{l}{25.9\%}                          \\
% Original input & \multicolumn{2}{l}{25.7\%}        & \multicolumn{2}{l}{25.8\%}                         \\
% Random chance & \multicolumn{2}{l}{25.0\%}        & \multicolumn{2}{l}{25.0\%}                         \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[]
\centering
\begin{small}
%\begin{minipage}{.49\linewidth}
%\centering
 %\begin{subtable}{.45\linewidth}
\begin{tabular}{l@{~}l@{~}l@{~}l@{~}l@{~}l}
\toprule
               & \multicolumn{2}{c}{\textbf{beta}} & \multicolumn{1}{c}{\textbf{rho}} & \multicolumn{1}{c}{\textbf{beta}\textbf{~\cite{disentanglement}}} & \multicolumn{1}{c}{\textbf{rho}\textbf{~\cite{disentanglement}}} \\ \midrule
Z              & \multicolumn{2}{c}{35\%}        & \multicolumn{1}{c}{31\%}                           & \multicolumn{1}{c}{31\%}                                 & \multicolumn{1}{c}{30\%}                               \\
S              & \multicolumn{2}{c}{26\%}        & \multicolumn{1}{c}{26\%}                           & \multicolumn{1}{c}{28\%}                                 & \multicolumn{1}{c}{28\%}                               \\
Raw & \multicolumn{2}{c}{26\%}        & \multicolumn{1}{c}{26\%}                           &                                        &                                       \\
Rand  & \multicolumn{2}{c}{25\%}        & \multicolumn{1}{c}{25\%}                           & \multicolumn{1}{c}{25\%}                                 & \multicolumn{1}{c}{25\%}                                \\ \bottomrule
\end{tabular}
\end{small}
\smallskip
\caption{Logistic regression accuracy for $\beta,\rho$}
\label{Tbl3_betarho}
\end{table}

 \begin{table}[t]
 %  \begin{minipage}{.45\linewidth}
\centering
\begin{small}
\begin{tabular}{l@{~}l@{~}l@{~}l@{~}l@{~}l}
\toprule
                              & \multicolumn{1}{c}{\textbf{NY}} & \multicolumn{1}{c}{\textbf{AM}} & \multicolumn{1}{c}{\textbf{NQ}} & \multicolumn{1}{c}{\textbf{All}} & \multicolumn{1}{c}{\textbf{~\cite{disentanglement}}} \\ \midrule
\textbf{Z-1}              & \multicolumn{1}{c}{31\%}                            & \multicolumn{1}{c}{37\%}                            & \multicolumn{1}{c}{30\%}                              & \multicolumn{1}{c}{31\%}                           & \multicolumn{1}{c}{30\%}                                \\
\textbf{S-1}               & \multicolumn{1}{c}{26\%}                            & \multicolumn{1}{c}{24\%}                            & \multicolumn{1}{c}{24\%}                              & \multicolumn{1}{c}{25\%}                           & \multicolumn{1}{c}{27\%}                                \\
\textbf{X-1} & \multicolumn{1}{c}{28\%}                            & \multicolumn{1}{c}{24\%}                            & \multicolumn{1}{c}{24\%}                              & \multicolumn{2}{c}{--- 25\% ---}                                               \\
\textbf{Rnd-1}  & \multicolumn{5}{c}{---------------- 25\% ----------------}                                                                                                                                                             \\ \midrule
\textbf{Z-5}             & \multicolumn{1}{c}{40\%}                            & \multicolumn{1}{c}{49\%}                            & \multicolumn{1}{c}{36\%}                              & \multicolumn{1}{c}{39\%}                           & \multicolumn{1}{c}{34\%}                                \\
\textbf{S-5}             & \multicolumn{1}{c}{26\%}                            & \multicolumn{1}{c}{27\%}                            & \multicolumn{1}{c}{25\%}                              & \multicolumn{1}{c}{26\%}                           & \multicolumn{1}{c}{30\%}                                \\
\textbf{X-5} & \multicolumn{1}{c}{25\%}                            & \multicolumn{1}{c}{29\%}                            & \multicolumn{1}{c}{25\%}                              & \multicolumn{2}{c}{--- 26\% ---}                                               \\
\textbf{Rnd-5}  & \multicolumn{5}{c}{---------------- 25\% ----------------}                                                                                                                                                             \\ \bottomrule
\end{tabular}
\end{small}
 %\end{minipage}%
 \smallskip
\caption{Logistic regression accuracy for next day/week  volatility. The rightmost column is the results of the model presented in (Mathieu et al. 2016), other columns are the results of our model.}
\label{vol_exc}
\end{table}
\begin{table}
\centering
\begin{small}
\begin{tabular}{l@{~}c@{~}c@{~}c}
\toprule
           & \textbf{Mean} & \textbf{SD} & \textbf{Traded days \%} \\ \midrule
\textbf{Z (Ours)} & 3.1\%          & 0.026       & 89.3\%   \\
\textbf{Z~\cite{disentanglement}} & 2.9\%          & 0.039 & 78.6\%   \\
\textbf{S (Ours)} & 2.4\%          & 0.031       & 82.1\%   \\
\textbf{S~\cite{disentanglement}} & 2.4\%          & 0.028 & 83.2\%   \\
\textbf{X} & 2.6\%          & 0.030        & 78.6\%  \\ \bottomrule                            
\end{tabular}
\end{small}
\smallskip
\caption{Options portfolio returns. The mean, std and percent of trading days with positive returns.}
\label{Tbl_portfoliovol}
\vspace{-5mm}
%\end{subtable}
%\end{minipage}%
 \end{table}

  
%We evaluate the disentanglement by trying to classify beta and rho groups (out of four equivalent size groups of increasing $\rho$/$\beta$) based on the original input, S and Z. Classification is done using logistic regression to avoid models calibration. Results are presented in Table \ref{Tbl3_betarho} \ref{Tbl3_betarhoexc}??, It can be seen that Z gives highest accuracy while classification based on S gives almost random-choice accuracy. This settles with the disentanglement since beta and rho are specific and differs between different stocks.
%We also try to see whether separating market movements from the original signal helped us to classify the following day/5 days return and volatility to four groups from lowest to highest return/volatility values. We present the results in Tab.\ref{vol_return} /Tables \ref{return_exc} \ref{vol_exc}??. For most of the cases S includes no direct information about the indicators while Z gives higher accuracy than the original input. We also perform the same analysis for Z and S encoded by a network we trained with the architecture described in \cite{disentanglement}. We did our best effort to calibrate this model to separate S from the data,from the results in Tab. \ref{tbl_DistNet} it can be seen that our model achieves higher accuracy and better separation for the financial data.
%Include the following?
%We further test the model ability to predict return by Using the probabilities predicted by the regression to build a portfolio. In day 51 of each quarter of the test set we buy (long) the 10 stocks with the highest probability to be in fourth group and sell (short) the 10 stocks with the lowest probability. Results are presented in Table \ref{Tbl_portfolioret} - S based portfolio achieves low return for both long and short as expected. Our long portfolio return for Z is lower than the original input while selling the short portfolio achieves 0.37\% vs. 0.33\% for the original input. Although we achieve higher accuracy for return classification on X over the original input, the performance of z based portfolio is inferior to the original input. This contradiction can be explained by the relatively high accuracy of Z based volatility prediction. While our model fails to predict next day return direction it includes some information about next day amplitude so we get higher accuracy from distinguish lowest and highest from middle return groups. From practical view, volatility prediction can be used for options trading.

A very important measure that is used in options trading is the volatility. Using a model on $Z$, we predicted the next day and next $5$-days volatility. The results are given in Table~\ref{vol_exc}. The accuracy of the different models changes between the stock groups, but the performance is significantly better for the model based on $Z$. 

% \begin{table}[]
% \centering
% \caption{Accuracy for logistic regression classification of the following day/week volatility}
% \label{vol_exc}
% \begin{tabular}{lllll}
% \hline
%                               & \textbf{NYSE} & \textbf{AMEX} & \textbf{Nasdaq} & \textbf{All} \\ \hline
% \textbf{Z - day}              & 31.2\%       & 37.2\%       & 30.0\%         & 31.0\%      \\
% \textbf{S -day}               & 26.3\%       & 24.2\%       & 23.6\%         & 24.9\%      \\
% \textbf{Original Input - day} & 27.6\%       & 24.2\%       & 23.5\%         & 25.4\%      \\
% \textbf{Random chance - day}  & \multicolumn{4}{c}{25.0\%}   \\ \hline
% \textbf{Z - week}             & 40.5\%       & 48.6\%       & 36.1\%         & 39.0\%      \\
% \textbf{S - week}             & 26.3\%       & 27.1\%       & 24.5\%         & 25.5\%      \\
% \textbf{Original Input -week} & 25.0\%       & 28.8\%       & 25.8\%         & 25.7\%      \\
% \textbf{Random chance -week} & \multicolumn{4}{c}{25.0\%}   \\ \hline
% \end{tabular}
% \end{table}

The volatility is an important component in options pricing models, such as Black-Scholes model~\cite{BS}. We developed the following theoretical options trading strategy: 
%\begin{itemize}[leftmargin=*]
(1) We estimated the volatility of a stock based on its volatility in the last fifty trading days. 
(2) We run a classification model for the stock based on either $X$ or $Z$. 
(3) For the ten stocks whose predicted volatility minus measured volatility is the highest, we bought a put and a call option. Similarly for the ten stocks whose predicted volatility minus measured volatility is the lowest, we sold one put option and one call option. The strike price of the options is $5\%$ higher than the current price. The time to expire is $60$ days for the high predicted volatility options and $5$ days for the low volatility ones. 
(4) We cleared position on the next day, i.e., sold options in the case where we bought options yesterday and vice-versa.
%\end{itemize}

Note that this strategy is market neutral and relies only on the volatility. We are aware of the fact that we ignored trading cost, liquidity and other technicalities that make this strategy unrealistic. However, we used it as a way to compare the classifier that used $X$ to the one that used $Z$ as an input. The  results are summarized in Table~\ref{Tbl_portfoliovol}. As one can see, using $Z$ is better. 
%As a concluding remark for this section, we tried using the algorithm in~\cite{disentanglement} on this data and repeated our measures. 
%We made a best effort to optimize the parameters, and still obtained inferior results in terms of our measures. However, we cannot overrule the possibility that with more calibrations we would get better results. 
The results from~\cite{disentanglement} for financial data are presented next to ours in tables~\ref{Tbl3_betarho},~\ref{vol_exc} and~\ref{Tbl_portfoliovol}. It can be seen that our accuracy and portfolio performance based on $Z$ are better and we also achieved better separation from $S$, since it is almost agnostic to specific stock properties.

\section{Conclusions}
This paper presents an adversarial architecture for solving the problem of disentanglement. Given labeled data, our algorithm encodes it as two separate parts, one that contains the label information and the other that is agnostic to it. We tested the network on visual and financial data, and found that it performed well compared to a leading literature method. Our architecture does not assume a distribution on the unspecified factors and the resultant encoding seemed both more interpretable and more suitable as a representation for learning various unspecified qualities.

\section*{Acknowledgements}

This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974).


\clearpage
{\small
\bibliographystyle{ieee}
\bibliography{gans}
}

\end{document}
