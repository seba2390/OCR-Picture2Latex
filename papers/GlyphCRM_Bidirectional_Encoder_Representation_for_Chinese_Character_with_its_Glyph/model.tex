\section{Our Methodology}


In what follows, we first introduce the data preprocessing and then mainly introduce the overall architecture of GlyphCRM, presented in Figure~\ref{fig:model}, containing the HanGlyph module and bidirectional encoder layer. Finally, we introduce the two-stage pre-training and fine-tuning methods of our model in detail.


\subsection{Data Preprocessing}
For the input text, we render each Chinese character into a single-channel $ 48\times48$ grayscale image\footnote{We use Python's fontTools and PIL library to set the font of Chinese characters and render Chinese characters into images.}. As the instance shown in Figure \ref{fig:axis}, the position on each character feature map~(i.e. grayscale image) is set to '1' where the stroke of Chinese character passes; otherwise, the position is set to '0'. After obtaining the sequential feature maps of character-level input text, we apply a special token $\rm[CLS]$ as the start symbol for all inputs. We also apply another special token $\rm[SEP]$ to separate sentences and as the ending symbol of input for pre-training and specific NLU tasks. The two special tokens are also converted to the grayscale image to obtain the corresponding feature maps. Hence, the input and output sequence can be separately denoted to $\mathbf{X} = (x_{cls}, x_1, x_2, ..., x_n, x_{sep})$, and $\mathbf{H} = (\mathbf{h}_{cls}, \mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_n, \mathbf{h}_{sep})$. 


To further capture the spatial structure of Chinese characters, we design the identical two-channel position maps for each character image, which have the same size as the feature maps of Chinese characters. As shown in Figure~\ref{fig:axis}, we set the coordinate axis with the center point of the Chinese character image as the origin, and the value range of the horizontal and vertical axis is between $-0.2$ and $0.2$. Hence, the two-channel position maps separately represent the abscissa and ordinate values of each pixel after being projected, respectively. 



\subsection{HanGlyph}
After obtaining the three-channel representation of each Chinese input character as shown in Figure \ref{fig:axis}, we adopt two residual convolutional blocks to extract its glyph feature, namely HanGlyph. As the orange box shown in Figure~\ref{fig:model}, for each residual convolutional block, the sequential input feature maps pass one convolutional layer with $3\times3$ kernel size, three-layer CNN and $2\times2$ max-pooling layer in turn. We take the second residual convolutional block for instance, which can be calculated by the equation as E.q.\ref{eq1}.
\begin{equation}
    \begin{array}{c}
    \mathbf{z}_{c}^{2} = \mathbf{ReLU}(\mathbf{w}^{2}\mathbf{z}^{1} + \mathbf{b}^{2})\vspace{1ex} \\
    \mathbf{z}_{r}^{2} = \mathbf{ReLU}( \mathbf{Max}_{2\times2}(\mathbf{z}_{c}^{2}) + \mathcal{F}(\mathbf{z}_{c}^{2}, \mathbf{W}_{r}^{2})) \vspace{1ex}\\
    \mathbf{z}^{2} = \mathbf{Max}_{2\times2}(\mathbf{z}_{r}^{2})
    
    \end{array}
    \label{eq1}
\end{equation} where $\mathbf{z}^{1}$ is the output of ResBlock 1 and $\mathbf{ReLU}$~\cite{relu} is the activation function widely used for deep convolutional neural networks (DCNN). $\mathbf{w}^{2}$ and $\mathbf{b}^{2}$ are the parameters for the first convolutional sub-layer where the padding width and stride length both are $1$ for each residual block. The output of the first sub-layer passes the 
three-layer CNN denoted as $\mathcal{F}(\mathbf{z}_{c}^{2}, \mathbf{W}_{r}^{2})$. 
Note that the core three-layer convolutional networks of ResBlock 1 have a large kernel window $9\times9$ to alleviate the issue of sparse image features, which is caused by the strokes of most Chinese characters occupying only a small number of pixels. Finally, one $2\times2$ max-pooling layer is used to obtain the final output for each residual block. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{model_graph.pdf}
    \caption{The overview of GlyphCRM. The HanGlyph module contains two identical residual convolutional blocks, yet with different parameters, namely 'ResBlock 1' and 'ResBlock 2'. 'PoE' is the position embeddings designed for the input sequence. The Transformer Block is used to capture the bidirectional contextual representation, including the Multi-Head Attention and Feed Forward Neural Network.}
    \label{fig:model}
\end{figure}


Besides entering the next layer, we convert the final output of each residual block into one hidden state with the same dimensions as the upper bidirectional encoder layer through the linear layer. We denote the hidden state to $\mathbf{G}=(\mathbf{g_1}, \mathbf{g_2}, ..., \mathbf{g_n})$, which can be directly fed into the Transformer block as shown in Figure~\ref{fig:model}. ResBlock 2 is followed by one linear function to convert the output of each Chinese character into the glyph vector with fixed dimensions. It allows us to obtain the sequential glyph vectors of Chinese text, denoted as $\mathbf{R}=(\mathbf{r}_1, ..., \mathbf{r}_i, ..., \mathbf{r}_n)$, where $\mathbf{r}_i$ is the single Chinese character representation and $n$ is the length of input.

\subsection{Bidirectional Encoder Layer}

After obtaining the glyph vector of each Chinese character through the HanGlyph module, we use the bidirectional encoder layer based on Transformer\cite{attention} to obtain the context-sensitive representation of Chinese characters. 
Before $R$ is input into the superstructure, we sum the sequential glyph vectors, position embeddings, and segment embeddings to construct the more reasonable input; otherwise, characters in all input positions are regarded as equally important by the attention mechanism. Yet Chinese characters in different positions usually play different roles for understanding sentences.       

We adopt the popular Transformer structure as the backbone of the bidirectional encoder layer due to its capability to capture long-distance dependency information between words~\cite{devlin-etal-2019-bert, debert, T5}. Concretely, each Transformer block, shown in Figure~\ref{fig:model}, includes the multi-head attention mechanism and feed-forward neural networks. We take the $l$ th Transformer block as an instance and the computation process can be presented by the following equations.
\begin{equation}
\begin{array}{c}
\mathbf{h}_{l}^{M}=\mathbf{LayerNorm}(\mathbf{h}_{l-1} + \mathbf{MultiHeadAttention}(\mathbf{h}_{l-1}))\vspace{1ex} \\
\mathbf{h}_{l} = \mathbf{LayerNorm}(\mathbf{h}_{l}^{M} + \mathbf{FFN}(\mathbf{h}_{l}^{M}))  
\end{array}
\label{eq2}
\end{equation} where $\mathbf{h}_{l-1}$ is the output of the $l-1$ th Transformer block. $\mathbf{LayerNorm}$ \cite{layernorm} is one way to reduce the training time by performing normalization on the feature dimension of input. $\mathbf{FFN}$ is the feed-forward neural network, which is similar to Multi-Layer Perceptrons~\cite{perceptron}. $\mathbf{MultiHeadAttention}$ is the core structure of each Transformer blo-ck, which is used to update the contextual representation of characters by way of calculating the similarity with aspect to other characters. Specifically, the process can be denoted as E.q.\ref{eq3}.
\begin{equation}
\begin{array}{c}
    \mathbf{MultiHeadAttention} = \mathbf{Concat}(head_1, head_2, ..., head_h)\mathbf{W}^{h}\vspace{1ex} \\
    head_i = \mathbf{Attention}(\mathbf{Q}\mathbf{W}_{i}^{Q}, \mathbf{K}\mathbf{W}_{i}^{K}, \mathbf{V}\mathbf{W}_{i}^{V})
\end{array}    
\label{eq3}
\end{equation} where $h$ is the number of heads. $\mathbf{W}_{i}^{Q}$, $\mathbf{W}_{i}^{K}$ and $\mathbf{W}_{i}^{V}$ are the projection parameters, and their dimensions are $\mathbb{R}^{d_{model}\times d_{h}}$. $\mathbf{W}^{h}\in \mathbb{R}^{h d_{h} \times d_{model}}$, where $d_{model}$ is the hidden size. In our work, we set $d_h = d_{model}/h$ for each Transformer block. The dimension of each head is reduced, so the total computational cost is almost identical to the single head attention with full dimensionality. For the dot-product $\mathbf{Attention}$, the computational method is denoted as E.q.\ref{eq4}.
\begin{equation}
    \mathbf{Attention(\mathbf{Q},\mathbf{K},\mathbf{V}) = \mathbf{softmax}(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{k}}})\mathbf{V}}
    \label{eq4}
\end{equation}
In practice, the queries $\mathbf{Q}$, keys $\mathbf{K}$ and values $\mathbf{V}$ present the whole input sequence in each block. However, when updating the representation of any character, the query vector is itself, and the keys and values are the hidden states of the whole input sequence.

In order to make full use of the glyph features of Chinese characters, we incorporate the glyph representation obtained in each residual block of the HanGlyph module into the Transformer Block of the first two layers. As the gray box shown in Figure~\ref{fig:model}, the output of the FFN of the underlying two Transformer blocks is not only integrated with the output of the multi-head attention module, but also with the output of the residual blocks of HanGlyph. The detailed process is presented in Figure~\ref{fig:add}. Concretely, we take the simple addition of the three input vectors because an overly complex integration method will significantly increase the computation cost of GlyphCRM, and this method is also proved effective~\cite{resnet}.
\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{add.pdf}
    \caption{The second Add and Norm sub-module of the bottom two Transformer blocks. 'IN-FFN' represents the hidden state before being input into the feed-forward neural network. 'Glyph' represents the glyph vector obtained in the HanGlyph layer. 'FFN' represents the output of feed-forward neural network.}
    \label{fig:add}
\end{figure}

From the overall architecture of GlyphCRM, the underlying four-layer networks are a symmetrical structure, which can have a stable information interaction way to capture and exploit glyph features. To summarize, GlyphCRM contains the glyph features of Chinese characters extracted from images and the contextual information of input text, which can be regarded as a pre-trained multi-modal model but is different from general cross-modality models such as VisualBERT~\cite{visualbert}, LXMERT~\cite{tan-bansal-2019-lxmert} and ERNIE-ViL~\cite{ernie_vil}. Without using the ID-based word embedding method, GlyphCRM will not be restricted by the unseen (out of fixed vocab) Chinese characters when fine-tuned on specific downstream tasks which contain unseen characters. Furthermore, the glyph features of Chinese characters can be used to infer the meaning of previously unseen or sparse Chinese characters by glyph similarity, compared to directly converting them into word vector according to word/character ID having no such inference information. 
%disabling Chinese characters to interact with each other by glyphs. 

\subsection{Training}
In this section, we introduce the detailed two-stage training process of our model, containing pre-training and fine-tuning.

\subsubsection{Pre-training} 
To avoid overfitting, models with huge parameters usually need large-scale corpus to train, but manual labeling will cost copious resources. Yet like BERT, the large neural language models are usually pre-trained on large-scale corpora with an unsupervised method to enable them have a detailed understanding of texts on specific languages, i.e., pre-trained language models have good initial parameters when used for specific tasks again. Hence, training on large-scale corpora with unsupervised methods is a relatively efficient approach for training large-param-eter models due to the unnecessary to spend enormous manpower and material resources, thereby achieving a remarkable success in natural language processing.      

\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{nsp.pdf}
    \caption{The overview of unsupervised next sentence prediction~(NSP) and masked language model~(Masked LM). The start symbol $\rm[CLS]$ is used for predicting the relationship between sentence A and B. $\rm[SEP]$ is used to divide sentences.}
    \label{fig:nsp}
\end{figure}

In this paper, we first pre-train our proposed model GlyphCRM with two unsupervised tasks, identical to those in BERT~\cite{devlin-etal-2019-bert}. The first one is the Masked LM task where we randomly choose 15\% of all Chinese characters for each input text to be replaced with one special token $\rm[MASK]$ 80\% of the time, a random character 10\% of the time, and the unchanged character 10\% of the time. It exploits other Chinese characters to predict the corrupted characters. For predicting the masked characters, we count the number of Chinese characters in the pre-training corpora and construct the corresponding vocabulary used for classification. 
We select the universally used Chinese characters which can be presented by Song typeface, monofont, or boldface. The sparse characters are replaced by another special token $\rm[UNK]$. The final vocab size is about $18,612$, less than that of BERT.
Specifically, for any input sequence $\mathbf{X}$, we first construct the corrupted input version $\hat{\mathbf{x}}$ by the above randomly masked method. We define the masked characters as $\overline{\mathbf{x}}$, and the training object of masked prediction can be presented as the following:
\begin{equation}
%\max _{\theta} \log p_{\theta}(\overline{\mathbf{x}} \mid \hat{\mathbf{x}}) 
\mathcal{L}_{mp} = \max _{\theta} \log p_{\theta}(\overline{\mathbf{x}} \mid \hat{\mathbf{x}}) \approx \sum_{t=1}^{n} m_{t} \log \left(\mathbf{H}_{\theta}(\hat{\mathbf{x}})_{t} \mathbf{e}\left(x_{t}\right)\right)
\label{eq5}
\end{equation}
where $m_{t} = 1$ represents $x_{t}$ is masked, and $\mathbf{H}_{\theta}$ is the top hidden state of our model. Thus, the final hidden states of the input text can be denoted as $\mathbf{H}_{\theta}(\mathbf{X}) = (\mathbf{H}_{\theta}(\mathbf{x})_{1}, \mathbf{H}_{\theta}(\mathbf{x})_{2}, ..., \mathbf{H}_{\theta}(\mathbf{x})_{n})$. $\mathbf{e(x)}$ indicates the final projection matrix that maps the final hidden state of masked characters to the vocabulary size.     



Besides, we adopt the next sentence prediction~(NSP) pre-training task to impel our model to understand the relationship between sentences, which is instrumental for being fine-tuned on some downstream tasks such as Question Answering~\cite{questiona2, wang-etal-2017-questiona} and Sentence Matching~\cite{sentence_m, NIPS2014_hu}. Concretely, when the training example is the composition of sentence A and B, we formulate that 50\% of the time B is the general next sentence that follows A~(regarded as the IsNext), 50\% of the time B is from the other training data~(regarded as the NotNext). While training, the top hidden state of the start token $\rm[CLS]$ is used to predict the relationship between two sentences as the blue arrow shown in Figure~\ref{fig:nsp}.



\subsubsection{Fine-tuning} Compared to pre-training, fine-tuning expends relatively fewer resources. In this paper, we fine-tune our proposed model on general natural language processing tasks, including single sentence classification, text classification, sequence labeling and sentence matching. For single sentence classification, sentence mat-ching, and multi-sentence text classification tasks, we feed the final output of the start token $\rm[CLS]$ to one task-specific output layer to predict the correct label. For sequence labeling, the final output of each Chinese character is fed into an output layer for classification. Notably, the self-attention mechanism in the Transform block of GlyphCRM ensures the almost seamless connection of the two stages of pre-training and fine-tuning, making the application of our model direct and effective in specific tasks. The detailed analyses and comparison with BERT will be presented in the following experiment section. 


