\section{Introduction}

%Various language symbols are important tools for the spread of human civilization and the expression of ideas with the cognitive developmental stages of human intelligence. Generally, to represent the discrete symbolic texts, researchers propose to encode various symbols into corresponding word embedding space such as one-hot encoding and distributed representation, making remarkable progress in the field of natural language processing. However, from the perspective of symbolic evolution, language symbols have always possessed their unique structural features and peculiarities, developing from the initial hieroglyphics to their present forms as shown in Figuer~\ref{fig:charater}. Word-ID based encoding method ignores the static glyph feature of language symbols and linguistic signs withsimilar structures usually have intrinsic linking, e.g. radicals of Chinese characters, suffixes of English.

%Hence, the text symbols and variants that humans observe through the eyes can be immediately seeked out their accurate meaning in the memory stored in the brain.

%On the other hand, current word representation methods usually confront the Out-of-Vocabulary~(OOV) problem especially for processing texts of some professional fields such as Biologic, Finance, etc. To alleviate the above problem, researchers adopt the methods of expanding vocabulary and unified representation of unknown words. Firstly, the approach of using a large vocabulary makes that the underlying word embedding table has huge parameters and consumes a lot of resources during the model training process, especially for large pre-trained models such as T5~\cite{T5}, Bart~\cite{lewis-etal-2020-bart} and GPT-3~\cite{gpt_3}. In addition, it is not necessarily a good method to represent unknown words with a unified vector during the big network era where new vocabularies (e.g. abbreviated words) are constantly emerging. As we all know, the emerging new words are created by humans based on existing word combination and the similarity of structural features between different words implies semantic connection. Hence, an natural question to ask is that whether we can utilize a basic framework with fewer parameters to represent overall language symbols.  
\begin{CJK}{UTF8}{gbsn}
Pre-trained neural language models, e.g., BERT~\cite{devlin-etal-2019-bert}, BART~\cite{lewis-etal-2020-bart}, and XLNET~\cite{xlnet}, have achieved extraordinary success in many natural language processing~(NLP) tasks, such as Information Retrieval~\cite{ir_bert}, Semantic Matching~\cite{semantic_bert,reimers-gurevych-2019-sentence_bert}, Question Answering~\cite{wang-etal-2017-questiona, questiona2} and Text Classification~\cite{text_classification}. In these models, each word is usually converted into a discrete vector representation by looking up the word embedding table, and then the context-sensitive word representation is learned by certain structures. However, for Chinese texts, these routine methods ignore that the static glyph of Chinese characters contains rich semantic information. For instance, Pictographs: the shape of '山'~(mountain), '日'~(sun) and '马'~(horse) is inextricably related to the shape of natural objects as shown in Figure~\ref{fig:charater}; Radicals: '崎'~(rough) and '岖'~(rugged), which have the same radical '山'~(mountain), are usually used to describe things related to mountains~(e.g., mountain road) together. Hence, the glyphs of Chinese characters can convey some meanings in many cases, and Chinese characters with similar structures can have intrinsic links. They intuitively indicate that the glyph features of Chinese characters have the potential to enhance their representations.
\end{CJK}
%People combine the names of two things with similar properties to make them express specific concepts.
%We can also infer the meaning of previously unseen Chinese characters according to the glyph similarities of Chinese characters.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{img1.pdf}
    \caption{An example of the evolution of some Chinese characters. Their glyphs are similar to the physical objects in nature and contain rich semantic information.}
    \label{fig:charater}
\end{figure}

Based on the above observations, some methods~\cite{learning_glyph, glyce, chen-etal-2020-glyph2vec} incorporate the glyph features to enhance the Chinese character representation already covered into character embeddings~(character ID-based), e.g., \citet{glyce} combine the glyph features extracted from various forms of Chinese characters with the BERT embeddings. They demonstrate that the glyph features of Chinese characters are authentically helpful to improve the performance of models. However, previous methods only use glyphs of Chinese characters as the additional features, and there is no pre-trained Chinese text representation framework based on glyphs. In this paper, as shown in Figure~\ref{fig:charater_2}, instead of using the ID-based character embedding method, we propose to only use the glyph vectors of Chinese characters as the their representations, obtained by the HanGlyph module. To capture the contextual information, we further adopt the bidirectional encoder Transformer~\cite{attention} as the superstructure and finally propose the Chinese pre-trained representation model named GlyphCRM, based entirely on glyphs.



%e.g., as the routine method shown in Figure~\ref{fig:charater_2}, Chinese character '我'~(me) is projected into a vector through the character embedding table, then aggregated with the glyph vector extracted by a simple convolutional neural network~(CNN).

%we argue that the full glyph representations of Chinese characters, rendered to the grayscale images, intuitively are enough expressive.
%In this paper, motivated by the aforementioned observations and the effective performance of pre-trained representation models~(e.g. BERT~\cite{devlin-etal-2019-bert}, XLNET~\cite{xlnet}), we mainly explore the identical pre-trained representation framework of Chinese due to the fact that Chinese characters have obvious and systematic structural features as shown in Figure~\ref{fig:charater}. \citet{glyce} propose the glyph-vectors for Chinese character representation recently, yet they integrate the glyph-vectors and word embeddings for Chinese characters in each task-specific model. We have the completely different motivations and argue that the full glyph representation intuitively are enough expressible. Hence, we propose to only use glyph features as the underlying representation of Chinese characters, obtained by the deep convolutional neural networks. Furthermore, we adopt popular Transformer~\cite{attention} based structure as the superstructure and propose the pre-trained model \textbf{Percept-BERT}: What You See Is What In Your Mind, similar to BERT~\cite{devlin-etal-2019-bert}, yet without word embeddings. 

Concretely, we design two residual convolutional blocks in the HanGlyph module to obtain the glyph representation of any Chinese character, which is converted into the grayscale image. Each block has similar sub-layer architectures, including convolutional neural networks (CNN) and ReLU~\cite{relu} activation function. Furthermore, we design two-channel position maps for the character image to reinforce the capture of the spatial structure of Chinese characters' glyphs. Meanwhile, to fully exploit the glyph features of Chinese characters, we incorporate the glyph features extracted by the HanGlyph module into the underlying two Transformer blocks by the skip-connection method. From the whole architecture of GlyphCRM, it does not use the ID-based word/character embedding method and can be fine-tuned for specific NLU tasks, where Chinese characters only need to be converted into grayscale images. Hence, the proposed model can address the out-of-vocabulary problem and alleviate the issue of huge parameters stemming from the word/character embedding table. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{original_dui.pdf}
    \caption{Left: The routine representation method of Chinese characters, Right: Our approach based exclusively on glyphs of Chinese characters.}
    \label{fig:charater_2}
\end{figure}
%It also alleviate the issue of training caused by the huge parameters of word embedding table. 

When pre-training GlyphCRM, we adopt the Masked Language Model (Masked LM) pre-training object and Next Sentence Prediction (NSP) task, identical to BERT~\cite{devlin-etal-2019-bert}. Furthermore, we evaluate models on 9 down-stream Chinese NLU tasks, including text classification, sentence matching, and tagging. Extensive experimental results demonstrate that GlyphCRM significantly outperforms the previous state-of-the-art pre-trained representation model BERT on a wide range of Chinese tasks. The in-depth analysis indicates that it converges faster than BERT during pre-training and has strong transferability and generalization on specialized fields and low-resource tasks.      

The contributions of our paper are three-fold:
\begin{itemize}
    \item We propose a Chinese pre-trained representation model Gly-phCRM based entirely on glyphs for the first time, where it replaces the ID-based word/character embedding method with the convolutional representation of Chinese glyphs. 
    \item GlyphCRM addresses the out-of-vocabulary problem by the HanGlyph module, which can generate the glyph representation of any character already converted into the grayscale image when fine-tuned on specific tasks.
    \item Extensive experiments demonstrate that our proposed model achieves better performance on a wide range of Chinese NLU tasks, especially on sequence labeling, compared to BERT with similar Transformer depth.
    %\item Our model has strong generalization and transferability on special field and low-resource tasks. 
    %\item Experimental results on low-resource sentiment classification and medical sentence semantic matching tasks profoundly indicate that GlyphCRM has strong generalization and transferability.
\end{itemize}

%They can not only fuse the left and right context and allow us to obtain deep bidirectional representation of Chinese characters, but also be adapted to down-stream pairwise sentence tasks.