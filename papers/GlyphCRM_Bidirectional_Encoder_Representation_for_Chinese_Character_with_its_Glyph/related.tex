\section{Related Work}

%In this section, we briefly introduce the pre-trained language representation models and the glyph vector used for Chinese natural language understanding~(NLU) tasks.
%\subsection{Pre-trained Language Representation Models}

\textbf{Pre-trained Language Representation Models:} As the distributional representation of words is proved more efficient and practical than independent representation that ignores the contextual information~\cite{sahlgren2008distributional}, how to obtain the rich context-sensitive representation of words has been attracting promising attention of many researchers. Early methods such as Word2Vec~\cite{word2vec} and GloVe~\cite{pennington-etal-2014-glove} learn the word embeddings with fixed dimensions through the co-occurrence of words in fixed windows on large-scale corpora. Recently, to alleviate the problem of insufficient representation of the above methods, some researchers study how to learn the word embeddings that contain more comprehensive contextual information and long-distance dependency information between words. \citet{elmo, elmo_b} proposed ELMo and its successors that utilize the language models to capture the contextual features with left-to-right and right-to-left methods. 

As the simple yet efficient Transformer~\cite{attention} architecture emerg-es, recently proposed pre-trained language models adopt it as the main architecture and have achieved significant performances on many NLU tasks. For instance, GPT and its successors~\cite{gpt,radford2019language_gpt_2, gpt_3} utilize the Transformer decoder-based architecture with the self-supervised left-to-right pre-training method to obtain the context-sensitive representation of words. Different from GPT, BERT~\cite{devlin-etal-2019-bert} utilizes the self-attention mechanism-based Transformer architecture and adopts the Masked Language Model pre-training object and Next Sentence Prediction task to obtain the bidirectional representation of words. Moreover, sequence-to-sequence pre-trained models such as T5 \cite{T5} and BART~\cite{lewis-etal-2020-bart}, and BERT-based representation models such as RoBERTa~\cite{liu2019roberta}, XLNET~\cite{xlnet}, DeBERT \cite{debert}, also achieve promising gains on NLP tasks.

Many researchers also proposed pre-trained language models for different languages such as CamemBERT for French~\cite{martin-etal-2020-french_bert}, BERT-wwm~\cite{scir_wwm} and ERNIE~\cite{zhang-etal-2019-ernie} for Chinese. Multi-language pre-trained models such as mT5~\cite{mt5} are also proposed to handle the compounded language tasks such as machine translation expediently. To summarize, the language models pre-trained on large-scale corpora are extremely useful for the development of natural language processing. In this paper, we also adopt the popular directional encoder Transformer as the superstructure of our model, which uses the cross attention mechanism to capture the context-sensitive information. We pre-train it on large-scale Chinese corpora.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{pixel_all_1.pdf}
    \caption{\begin{CJK}{UTF8}{gbsn}An preprocessing instance of Chinese character 'æˆ‘' (me). Each column of abscissa position map (the second one) has the same value. Each row of ordinate position map (the third one) has the same value.\end{CJK}}
    \label{fig:axis}
\end{figure}
%\subsection{Glyph Features of Chinese characters}
\noindent\textbf{Glyph Vector:} Generally, to represent the discrete symbolic texts, researchers proposed to encode various symbols into the corresponding word embedding space, such as one-hot encoding and distributed representation, making a remarkable progress in the field of natural language processing. However, from the perspective of symbolic evolution, Chinese symbols have always possessed their unique structural features and peculiarities, developing from the initial hieroglyphics to their present forms as the Chinese characters shown in Figure \ref{fig:charater}. Recent researches~\cite{learning_glyph, chen-etal-2020-glyph2vec} also demonstrate that the glyphs of Chinese characters contain rich semantic information and have the potential to enhance the word representation of them. \citet{glyce} first apply the glyph features of Chinese characters into the pre-trained model BERT and achieve significant performance on many Chinese NLU tasks, such as Named Entity Recognition~\cite{msra}, News Text Classification~\cite{thunews} and Sentiment Analysis~\cite{chnSentiCorp}. Among them, the typical method is to use the deep convolutional neural networks to extract the glyph features of Chinese characters after converting them into images. Then, the glyph features of Chinese characters and corresponding character embeddings are integrated to enrich the representation of Chinese characters. We argue that the full glyph of Chinese characters is expressive enough, and further propose the Chinese pre-trained representation model GlyphCRM, based entirely on glyphs.

