%!TEX root = main.tex
\section{Evaluation}
\label{sec:eval}


\begin{figure}[t]
		\hspace{7mm}
        \includegraphics[width=3.2in]{figure/doc_count}
        \vspace{-5mm}
        \caption{The number of documents on different days in the dataset.}
        \label{fig:docAmount}
        \vspace{-5mm}
\end{figure}



%!TEX root = main.tex
%\subsection{Experimental Setup}
%\label{subsec:exp-setup}

We evaluate the performance of our system based on 60 GB of Chinese news documents collected from all the major Internet news providers in China, such as Tencent and Sina, in a three-month period from October 1, 2016 to December 31, 2016 covering different topics in the open domain. Fig.~\ref{fig:docAmount} shows the amounts of documents on different days in the dataset. The average number of documents in one day during that period is $164,922$. For the following experiments, we use the data in the first $7$ days for parameter tuning. The remaining data serves as the test set.

\subsection{Evaluate Event Clustering}
\label{subsec:eval-clustering}

We first evaluate the performance of our two-layer graph-based document clustering procedure for event extraction. We manually annotated a test dataset that consists of $3500$ news documents with ground-truth event labels, and compare our algorithm with the following methods:
\begin{itemize}
	\item \textbf{LDA + Affinity Propagation}: extract the 1000-dimensional LDA vector of each document, and cluster them by the Affinity Propagation clustering algorithm \cite{guan2011text}.
	\item \textbf{KeyGraph}: the original KeyGraph algorithm \cite{sayyadi2013graph} for document clustering, without the second-layer clustering based on document graphs and document-pair relationship classifier.
\end{itemize}

We use the homogeneity, completeness, and V-measure score \cite{rosenberg2007v} as the evaluation metrics of clustering results.
Homogeneity is larger if each cluster contains only members from a single class.  The completeness is maximized if all members of a ground true class are assigned to the same cluster.
The V-measure is the harmonic mean between homogeneity and completeness:
$
  \text{V-measure} = \frac{2 \times \text{homogenity} \times \text{completeness}}{\text{homogenity} + \text{completeness}}
$

\begin{table}
  \caption{Comparing different event clustering methods.}
  \label{tab:clusterResult}
  \begin{tabular}{llll}
    \toprule
    Algorithm & Homogeneity & Completeness & \text{V-measure}\\
    \midrule
    Our approach & $\mathbf{0.960}$ & $0.965$ & $\mathbf{0.962}$ \\
    KeyGraph & $0.554$ & $\mathbf{0.989}$ & $0.710$\\
    LDA + AP & $0.620$ & $0.947$ & $0.749$\\
    \bottomrule
  \end{tabular}
  \vspace{-5mm}
\end{table}

Table \ref{tab:clusterResult} shows that our approach achieves the best V-measure compared with other methods,  partly due to the fact that our method achieves the highest homogeneity score, which is $0.96$. This implies that most of the document clusters (events) we obtain are pure: each event only contains documents that talk about the same event. In comparison, the homogeneity for the other two methods is much lower. The reason is that we adopt two layers of graph-based clustering to group documents into events with more appropriate granularity. 

Yet, the completeness of our approach is a little bit smaller than that of KeyGraph, which is reasonable, as we further split the clusters with the second layer document-graph-based clustering supervised by the document-pair relationship classifier. Considering the significant improvement in homogeneity, the loss in completeness is ignorable. 
% Besides, real world applications often emphasize more on homogeneity than completeness, as incorrect clustering results often lead to loss of customers.


\subsection{Story Forest vs. Other Story Structures}
\label{subsec:eval-storytree}

We evaluate different event timeline and story generation algorithms on the large 3-month news dataset through pilot user evaluation. To make fair comparisons, the same preprocessing and event extraction procedures before developing story structures are adopted for all methods, with 261 stories detected from the dataset.
The only difference is how to construct the story structure given a set of event nodes.
We compare our online Story Forest system with the following existing algorithms:
\begin{itemize}
	\item \textbf{Flat Cluster (Flat):} this method clusters related events into a story without revealing the relationships between events, which approximates some previous works in TDT \cite{yang2002multi, allan1998line}.
	\item \textbf{Story Timeline (Timeline):} this method organizes events linearly according to the timestamps of events \cite{sayyadi2009event,sayyadi2013graph}.
	\item \textbf{Story Graph (Graph):} this method calculates a connection strength for every pair of events and connect the pair if the score exceeds a threshold \cite{yang2009discovering}.
	\item \textbf{Event Threading (Thread):} this algorithm appends each new event to its most similar earlier event \cite{nallapati2004event}. The similarity between two events is measured by the TF-IDF cosine similarity of the event centroids.
\end{itemize}

\begin{figure*}[t]
                        \centering
                        \subfigure[Percentage of incorrect edges]{
                \includegraphics[width=2.3in]{figure/cdf_incorrect_edge_ratio}
                                \label{fig:edge}
                        }
                \hspace{-3mm}
                        \subfigure[Percentage of inconsistent paths]{
                \includegraphics[width=2.3in]{figure/cdf_inconsistent_path_ratio}
                                \label{fig:path}
                        }
                        \hspace{-3mm}
                        \subfigure[Number of times rated as the most readable structure]{
                \includegraphics[width=2.3in]{figure/bar_best_alg}
                                \label{fig:whole}
                        }
                        \vspace{-3mm}
                \caption{Comparing the performance of different story structure generation algorithms.}
                \label{fig:compareAlgs}
\vspace{-4mm}
\end{figure*}



\begin{figure*}[t]
                        \centering
                        \subfigure[Histogram of the number of events in each story]{
                \includegraphics[width=2.3in]{figure/hist_num_event}
                                \label{fig:num-event}
                        }
                \hspace{-3mm}
                        \subfigure[Histogram of the number of paths in each story]{
                \includegraphics[width=2.3in]{figure/hist_num_paths}
                                \label{fig:num-path}
                        }
                        \hspace{-3mm}
                        \subfigure[Numbers of different story structures]{
                \includegraphics[width=2.3in]{figure/bar_structure_type}
                                \label{fig:story-type}
                        }
                        \vspace{-3mm}
                \caption{The characteristics of the story structures  generated by the  Story Forest system.}
                \label{fig:analysisTree}
\vspace{-4mm}
\end{figure*}



\begin{table}
  \caption{Comparing different story structure generation algorithms.}
  \label{tab:structureResult}
  \begin{tabular}{llllll}
    \toprule
     & Tree & Flat & Thread & Timeline & Graph\\
    \midrule
    Correct edges & $\mathbf{82.8\%}$ & $73.7\%$ & $66.8\%$ & $58.3\%$ & $32.9\%$ \\
    Consistent paths & $\mathbf{77.4\%}$ & $-$ & $50.1\%$ & $29.9\%$ & $-$\\
    Best structure & $\mathbf{187}$ & $88$ & $84$ & $52$ & $19$\\
    \bottomrule
  \end{tabular}
  \vspace{-3mm}
\end{table}

  

We enlisted $10$ human reviewers, including product managers, software engineers and senior undergraduate students, to blindly evaluate the results given by different approaches. Each individual story was reviewed by $3$ different reviewers. When the reviewers' opinions are different, they will discuss to give a final result. For each story, the reviewers answered the following questions for each of the $5$ different structures generated by different schemes:
\begin{enumerate}
	\item Do all the documents in each story cluster truly talk about the same story (\textit{yes} or \textit{no})? Continue if \textit{yes}.
	\item Do all the documents in each event node truly talk about the same event (\textit{yes} or \textit{no})? Continue if \textit{yes}.
	\item For each story structure given by different algorithms, how many edges correctly represent the event connections?
	\item For each story structure given by story forest, event threading and story timeline, how many paths from ROOT to any leaf node exist in the graph? And how many such paths are logically coherent?
	\item Which algorithm generates the structure that is the best in terms of revealing the story's underlying logical structure?
\end{enumerate}

Note that for question (3), the total number of edges for each tree equals to the number of events in that tree. Therefore, to make a fair comparison, for the story graph algorithm, we only retain the $n$ edges with the top scores, where $n$ is the number of events in that story graph.


We first report the clustering effectiveness of our system in the pilot user evaluation on the $3$-month dataset. Among the 261 stories, 234 of them are pure story clusters (\textit{yes} to question $1$), and furthermore there are 221 stories only contains pure event nodes (\textit{yes} to question $2$). Therefore, the final accuracy to extract events (\textit{yes} to both question $1$ and $2$) is $84.7\%$.


Next, we compare the output story structures given by different algorithms from three aspects: the correct edges between events, the logical coherence of paths, and the overall readability of different story structures. Fig.~\ref{fig:edge} compares the CDFs of incorrect edge percentage under different algorithms. As we can see, Story Forest significantly  outperforms the other $4$ baseline approaches.
As shown in Table~\ref{tab:structureResult}, for $58\%$ story trees, all the edges in each tree are reviewed as correct, and the average percentage of correct edges for all the story trees is $82.8\%$. In contrast, the average correct edge percentage given by the story graph algorithm is $32.9\%$. 

An interesting observation is that the average percentage of correct edges given by the simple flat structure is $73.7\%$, which is a special case of our tree structures.
This can be explained by the fact that most real-world breaking news that last for a constrained time period are not as complicated as a novel with rich logical structure, and a flat structure is often enough to depict their underlying logic.
However, for stories with richer structures and a relatively longer timeline, Story Forest gives better result than other algorithms by comprehensively considering the event similarity, path coherence and time gap, while other algorithms only consider parts of all the factors. 


For path coherence, Fig.~\ref{fig:path} shows the CDFs of percentages of inconsistent paths under different algorithms.
Story Forest gives significantly more coherent paths: the average percentage of coherent paths is $77.4\%$ for our algorithm, and is $50.1\%$ and $29.9\%$, respectively, for event threading and story timeline. Note that path coherence is meaningless for flat or graph structure.

Fig.~\ref{fig:whole} plots overall readability of different story structures. Among the $221$ stories, the tree-based Story Forest system gives the best readability on $187$ stories, which is much better than all other approaches. Different algorithms can generate the same structure. For example, the Story Forest system can also generate a flat structure, a timeline, or a same structure as the event threading algorithm does. Therefore, the sum of the numbers of best results given by different approaches is bigger than $221$. It's worth noting that the flat and timeline algorithms also give $88$ and $52$ most readable results, which again indicates that the logic structures of a large portion of real-world news stories can be characterized by simple flat or timeline structures, which are special cases of story trees. And complex graphs are often an overkill. 

We further inspect the story structures generated by Story Forest. Fig.~\ref{fig:num-event} and Fig.~\ref{fig:num-path} plot the distributions of the number of events and the number of paths in each story tree, respectively. The average numbers of events and paths are $4.07$ and $2.71$, respectively. Although the tree structure includes the flat and timeline structures as special cases, among the $221$ stories, Story Forest generates $77$ flat structures and $54$ timelines, while the remaining $90$ structures generated are still story trees. This implies that Story Forest is versatile and can generate diverse structures for real-world news stories, depending on the logical complexity of each story. 

% We compare our tree model with null structure model by the following criteria: if both models have the same output structure, the performance are the same; if our tree model gives $n$ links between event nodes and $n_c$ links among them are recognized as correct by the reviewers, then our model performs better if $\frac{n_c}{n} > 0.5$, worse if $\frac{n_c}{n} < 0.5$, and equal otherwise. According to the answers from reviewers, our model achieves better performance for $30$ stories, equal performance for $80$ stories, and worse performance for the remaining $10$ stories. Both methods perform equally for majority of the stories due to the lack of clear and strong connections for real world news documents. Thus our system prefers not to connect two events if the calculated link scores are not high enough and our results are similar with that of the null structure model. However, when the event connections are strong enough, our algorithm is able to detect that, and thus gives better performance than the null structure model.

% We compare the results of our story tree model with linear structure, according to the answers  given by the reviewers.  Our model achieves better performance for 61 events, equal performance for 48 events and worse performance for the 11 remaining events. It shows that our system significantly outperforms the linear structure model. Our story tree algorithm takes multiple factors into account, including  not only time line sequence but also time gap between events, event TF similarity, and the story line coherence. However, the linear structure model only considers the time line sequence information to sort events. Therefore, our method is more effective to detect and drop incorrect event connections.

% We directly compare the number of correct links given by our method and DAG structure for each story, 
% as we restrict the two models to give the same number of event connections in a story. The results from reviewers show that there is only $1$ story that we got better result than graph structure and $1$ story that our performance is worse. For all the others the performances of two methods are the same. This actually indicates that our method performance is at least not worse than the graph structure model. However, when we restrict the number of links to be the same for both algorithms, we actually improves the performance of graph structure model with a dynamic threshold. IN addition, Our results are more interpretable and much logically clearer compared with a graph structure, as the intrinsic link structure within a graph could become quite complex when the number of event nodes increase.

\subsection{Algorithm Complexity and Overhead}
\label{subsec:complexity}

\begin{figure}[t]
		\hspace{7mm}
        \includegraphics[width=3.2in]{figure/date_time}
        \vspace{-5mm}
        \caption{The running time of our system on the 3-month	 news dataset.}
        \label{fig:timeComplexity}
        \vspace{-5mm}
\end{figure}

In this section,  we discuss the complexity of each step in our system. For a time slot (e.g., in our case is one day), let $N_d$ be the number of documents, $N_w$ the number of unique words in corpora, note $N_w << N_d$, $N_e$ the number of different events, $N_s$ the number of different stories, and $N_k$ represents the maximum number of unique keywords in a document. 

As discussed in \cite{sayyadi2013graph}, building keyword graph requires $\mathcal{O}(N_d N_k + N_w^2)$ complexity, and community detection based on betweenness centrality requires $\mathcal{O}(N_w^3)$. The complexity of assigning documents to keyword communities is $\mathcal{O}(N_d N_k  N_e)$. So by far the total complexity is $\mathcal{O}(N_dN_k N_e + N_w^3)$. There exist other community detection algorithms requiring only $\mathcal{O}(N_w^2)$, such as the algorithm in \cite{radicchi2004defining}. Thus we can further improve efficiency by using faster community detection algorithms.

After clustering documents by keyword communities, for each cluster the average number of documents is $\sfrac{N_d}{N_e}$. The pair-wise document relation classification is implemented in $\mathcal{O}((\sfrac{N_d}{N_e})^2)$. The complexity of the next document graph splitting operation is $\mathcal{O}((\sfrac{N_w}{N_e})^3)$. Therefore, the total complexity is $\mathcal{O}(N_e((\sfrac{N_d}{N_e})^2 + (\sfrac{N_w}{N_e})^3))$. Our experiments show that usually $1 \leq \sfrac{N_d}{N_e} \leq 100$. Combining with $N_w << N_d$, the complexity is now approximately $\mathcal{O}(N_e)$.

To grow story trees with new events, the complexity of finding the related story tree for each event is of $\mathcal{O}(N_s  T)$, where $T$ is the history length to keep existing stories and delete older stories. If no existing related story, creating a new story requires $\mathcal{O}{(1)}$ operations. Otherwise, the complexity of updating a story tree is $\mathcal{O}(T \sfrac{N_e}{N_s})$. In summary, the complexity of growing story trees is $\mathcal{O}(N_eT (N_s + \sfrac{N_e}{N_s})) \approx \mathcal{O}(T N_e  N_s)$, as our experience on the Tencent news dataset shows that $1 \leq \sfrac{N_e}{N_s} \leq 200$. Our online algorithm to update story structure requires $\mathcal{O}(\sfrac{N_e}{N_s})$ complexity and  delivers a consistent story development structure, while most existing offline optimization based story structure algorithms require at least $\mathcal{O}((\sfrac{N_e}{N_s})^2)$ complexity and disrupt the previously generated story structures.

Fig.~\ref{fig:timeComplexity} shows the running time of our \textit{Story Forest} system on the $3$ months news dataset. The average time of processing each day's news is around $26$ seconds, and increases linearly with number of days. 
For the offline keyword extraction module, the processing efficiency is approximately $50$ documents per second. The performance of keyword extraction module is consistent with time and doesn't require frequently retraining. The LDA model is incrementally retrained every day to handle new words. For keyword extraction, the efficiency of event clustering and story structure generation can be further improved by a parallel implementation.
