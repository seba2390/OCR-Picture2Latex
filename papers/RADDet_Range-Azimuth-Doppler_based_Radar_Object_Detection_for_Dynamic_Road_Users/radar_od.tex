
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
 \documentclass[10pt, conference, compsocconf]{IEEEtran}
% \documentclass[conference]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{graphicx}
\usepackage{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subcaption}
\usepackage[hyphens]{url}
\usepackage{hyperref}
%\usepackage{mathpazo} % Use the Palatino font by default
%\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{diagbox}
%\usepackage{textcomp}
%\usepackage{comment}
\usepackage{xcolor}

%\hypersetup{citecolor=DeepPink4}
%\hypersetup{linkcolor=DarkRed}
%\hypersetup{urlcolor=DarkBlue}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\etal}{\textit{et~al.}}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{RADDet: Range-Azimuth-Doppler based Radar Object Detection 
\\for Dynamic Road Users}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations
\author{\IEEEauthorblockN{Ao Zhang$^1$, Farzan Erlik Nowruzi$^{1, 2}$, Robert Laganiere$^{1, 2}$}
	\IEEEauthorblockA{
		\noindent\parbox{.27\textwidth}{\centering University of Ottawa$^1$}
		\noindent\parbox{.27\textwidth}{\centering Sensorcortek Inc$^2$}
		\\
		 \noindent\parbox{.27\textwidth}{\centering Ottawa, Ontario, Canada}
		 \noindent\parbox{.27\textwidth}{\centering Ottawa, Ontario, Canada}
		\\
		azhan085@uottawa.ca, fnowr010@uottawa.ca, laganier@eecs.uottawa.ca}
	
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
%\and
%\IEEEauthorblockN{Erlik /s per 2nd Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}

%\author{\IEEEauthorblockN{Ao per 1st Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
%\and
%\IEEEauthorblockN{Erlik /s per 2nd Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
%\and
%\IEEEauthorblockN{Robert per 3rd Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%	line 2: name of organization, acronyms acceptable\\
%	line 3: City, Country\\
%	line 4: Email: name@xyz.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
	
Object detection using automotive radars has not been explored with deep learning models in comparison to the camera based approaches. This can be attributed to the lack of public radar datasets. In this paper, we collect a novel radar dataset that contains radar data in the form of Range-Azimuth-Doppler tensors along with the bounding boxes on the tensor for dynamic road users, category labels, and 2D bounding boxes on the Cartesian Bird-Eye-View range map. To build the dataset, we propose an instance-wise auto-annotation 
%{\color{gray}algorithm}
method. Furthermore, a novel Range-Azimuth-Doppler based multi-class object detection deep learning model is proposed. The algorithm is a one-stage anchor-based detector that generates both 3D bounding boxes and 2D bounding boxes on Range-Azimuth-Doppler and Cartesian domains, respectively. Our proposed algorithm achieves 56.3\% AP with IOU of 0.3 on 3D bounding box predictions, and 51.6\% with IOU of 0.5 on 2D bounding box prediction. Our dataset and the code can be found at \textcolor{blue}{\textit{\url{https://github.com/ZhangAoCanada/RADDet.git}}}.

\end{abstract}

\begin{IEEEkeywords}

Radar; Range; Azimuth; Doppler; Cartesian; Object Detection; Auto-annotation; Deep Learning

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

Automotive radar, also known as Frequency Modulated Continuous Wave (FMCW) radar, has been widely adopted in Advanced Driver Assistance Systems (ADAS). Compared to other sensors such as camera and Lidar, radar is robust against adverse weather and lighting conditions, and is a less expensive~\cite{Ref:HighResolutionRadarDataset} sensor. The drawbacks of radar sensor are its low output resolution and high noise levels. Despite that, recent studies~\cite{Ref:ProbabilisticOriented, Ref:DeepRadarDetector, Ref:DeepOpenSpace} have shown the feasability of various tasks such as object detection and pose estimation using deep learning algorithms.


Raw data from FMCW radar, known as Analog-to-Digital (ADC) signals, are hardly readable for human observers. Thus, they are often passed into radar Digital Signal Processing (DSP) for interpretation. Sparse point cloud representation is the most popular output format for radar DSP due to its similarity to the output format from Lidar. Another widely used output format is a 3D tensor representation illustrating the information in range, angle and velocity. This format is often called Range-Azimuth-Doppler (RAD) spectrum. A sample is shown as the left two columns from Figure~\ref{F:DatasetSample}. RAD is the main representation that is used to extract the point-cloud representation. Since the Range-Azimuth dimensions are in the form of polar coordinates, they are often transformed into Cartesian coordinates for higher readability. There are two categories of approaches to extract RAD data from ADC signals, namely Fast Fourier Transform (FFT) and MUSIC~\cite{BG:MUSIC}. Although MUSIC provides higher precision, it is computationally expensive. In this research, we study the object detection with RAD spectrums achieved from applying FFT.


\begin{figure}[!t]
	\centering
	\includegraphics[width=3.3in]{./images/model_sample.png}
	\caption{Instances from our dataset along with the predictions of our model. 3D boxes represented in Rang-Doppler spectrum (A) and Range-Azimuth spectrum (B). (C) The 2D representation of boxes in Cartesian coordinates. (D) The camra view used for visualization. Ground truth boxes are labelled with facecolors and predictions are without facecolors.}
	\label{F:DatasetSample}
\end{figure}


In recent years, deep learning based object detection algorithms have been widely explored in the image domain. In the radar domain, although object detection has gained a certain level of popularity, it is still an area that has not been explored to the full extent. This is due to several reasons. First, due to various input and output representations of the radar data, the focus of the research papers~\cite{Ref:ProbabilisticOriented, Ref:DeepRadarDetector, Ref:3DRadarCube, Ref:ClusterSegmentation} are split between those. Second, there is shortage of public datasets that provide the raw input representations and capacity to benchmark the studies in this field. Thus, the researchers chose to build their own dataset. 
One common observation that can be found in most radar research projects is that they only target dynamic objects~\cite{Ref:ProbabilisticOriented, Ref:3DRadarCube}, since the detections of dynamic objects are richer than static ones.


In this paper, we introduce a novel publicly available dataset and propose a new object detection model for dynamic road users. Our contributions are as follows:

\begin{itemize}
	\item A novel dataset that contains radar data in the form of RAD representations with the corresponding annotations for various object classes is introduced. The dataset is available\footnote{\label{webpage}\url{http://www.site.uottawa.ca/research/viva/projects/raddet/index.html}} for future research in this field. 
	\item An automatic annotation method that generates ground-truth labels on all dimensions of RAD data and in the Cartesian form is proposed. 	
	\item A new radar object detection model is proposed. Our model employs a backbone based on ResNet~\cite{BG:ResNet}. The ultimate form of the backbone is achieved after systematic explorations of deep learning models on the radar data. Inspired by YOLO head~\cite{Ref:YOLOv4}, we propose a new dual detection head, with a 3D detection head on RAD data and a 2D detection head on the data in Cartesian coordinates.	
	\item Our proposed model is extensively evaluated against the well known image based object detection counter-parts. Results show that our proposal achieves the state of the art performance in the object detection task with radar data. The significance of our results shows that radar is a viable competitor to camera and lidar based object detection methods.
\end{itemize}

%\footnote{\url{https://drive.google.com/drive/folders/1v-AF873jP8p6waChF3pSSqz6HXOOZgkC?usp=sharing}}
%% no \IEEEPARstart
%This demo file is intended to serve as a ``starter file''
%for IEEE conference papers produced under \LaTeX\ using
%IEEEtran.cls version 1.7 and later.
%
%All manuscripts must be in English. These guidelines include complete descriptions of the fonts, spacing, and related information for producing your proceedings manuscripts. Please follow them and if you have any questions, direct them to the production editor in charge of your proceedings at Conference Publishing Services (CPS): Phone +1 (714) 821-8380 or Fax +1 (714) 761-1784.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%
%\subsection{Subsection Heading Here}
%Subsection text here.
%
%\begin{figure*}
%	\begin{center}
%		\framebox(200,300){}
%		%\includegraphics[height=1.4in]{figures/IterativeModel.png}
%	\end{center}
%	\caption{Double column.}
%	\label{fig:hierarchy}
%\end{figure*}
%
%\begin{figure}
%	\begin{center}
%		\framebox(200,300){}
%		%\includegraphics[height=1.4in]{figures/IterativeModel.png}
%	\end{center}
%	\caption{Single Column.}
%	\label{fig:hierarchy2}
%\end{figure}
%
%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.
%
%equations if need be...
%\begin{equation}
%		H_{i} = H_{i-1} - H_{i-1}^{*}
%\end{equation}




\section{Related Work}

One common issue of autonomous radar research is the lack of proper public datasets. This is due to the radar hardware variance and the highly configurable property of the radar itself. For example, a different number of on-board transmitters and receivers will result in different data resolutions. Also, with different frequency sweep magnitudes and sampling rates, the maximum detection range and range resolution will behaviour totally different. Thus, most radar researchers chose to create the dataset themselves using sensor fusion techniques.

%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=3.3in]{./images/dataset_sample.png}
%	\caption{An sample of our proposed dataset. Top left is the image view for visualization only. Top right consists of 3D boxes represented in Rang-Doppler spectrum and Range-Azimuth spectrum respectively. Bottom is the representation of 2D boxes on Cartesian coordinates.}
%	\label{F:DatasetSample}
%\end{figure}

In the previous work \cite{Ref:DeepRadarDetector, Ref:RadarCamFusion, Ref:DeepOpenSpace, Ref:3DRadarCube, Ref:RADVehicle, Ref:ProbabilisticOriented}, cameras and Lidars are used along with the radar for building their datasets.
% \cite{Ref:ClassificationRadarSpectra, Ref:VRUGNSS} use the Differential Global Positioning System (DGPS) for annotation.
%\cite{} uses stereo cameras and \cite{} use Lidars for generating the ground truth labels.
In order to improve the quality of the dataset, some researchers~\cite{Ref:HighResolutionRadarDataset} even use multiple high-precision sensors for annotating radar data, namely Lidar, Synthetic Aperture Radar (SAR) and cameras.

Except for employing different sensors, another big difference among all the datasets mentioned above is the input radar data format. Generally, it can be classified into two categories: cluster-based radar input format and spectrum-based radar input format.

Cluster-based input format is the data format that contains a cluster of points with each point illustrating the location of the object. To generate such format, some traditional cluster-based pre-processing methods need to be applied. Schumann~\etal~\cite{Ref:ClusterLSTM} use DBSCAN~\cite{BG:DBSCAN} to differentiate and extract all instances in radar frames, then annotate each instance with the corresponding object in the camera frames. 
%Afterwards, LSTM~\cite{BG:LSTM} and Random Forest~\cite{BG:RandomForest} are chosen for classification task. 
%\cite{Ref:RNNClassification} and \cite{Ref:MultiClassForRU} , whose contributions are based on the work of \cite{Ref:ClusterLSTM}, improve the performance furthermore.
One challenge of the cluster-based methods is that data could be mistakenly merged or separated due to the limited range resolution and the high noise level. Palffy~\etal~\cite{Ref:3DRadarCube} manage to overcome such problem by adding cropped spectrum-based data to the clusters and employ 3D convolutions for processing the input radar cube.

Spectrum-based input format such as RAD spectrum
%, known as Range-Doppler spectrum, Range-Azimuth spectrum, Range-Azimuth-Doppler spectrum, etc, 
keeps the consistency of the input data without any discretization. This will result in a performance boost for most deep learning algorithms. However, it also increases the complexity of the input data, which could potentially impact the runtime speed of those algorithms. Brodeski~\etal~\cite{Ref:DeepRadarDetector} successfully conduct 3D point-cloud segmentation and pose estimation using Range-Doppler-Angles spectrum, where angles stand for a combination of azimuth and elevation angles. Nowruzi~\etal~\cite{Ref:DeepOpenSpace} use Range-Azimuth-Doppler spectrums as inputs for free space segmentation. Major~\etal~\cite{Ref:RADVehicle} sperate Range-Azimuth-Doppler inputs along $3$ axes and feed them into the networks before integrating them together. %Zhang~\etal~\cite{Ref:3DEstimation} arrange Range-Doppler spectrums of different receivers as its input data, while 
The Range-Azimuth spectrum is used in \cite{Ref:RadarCamFusion, Ref:ClassificationRadarSpectra, Ref:ProbabilisticOriented} for object detection and classification. 
%The variation of spectrum-based input data is based on pre-processing methods, namely Fast Fourier Transform (FFT) and MUSIC~\cite{BG:MUSIC}.

Based on different types of input, various base models from computer vision domain were used with the Radar data. For cluster-based input,
%apart from the LSTM and Random Forest mentioned above,
Schumann~\etal~\cite{Ref:ClusterSegmentation} explore multi-scale grouping module (MSG) from PointNet++~\cite{Ref:PointNet++} for point-cloud based segmentation. For spectrum-based input, VGG~\cite{BG:VGG} is chosen as the base model in \cite{Ref:ClassificationRadarSpectra}. Brodeski~\etal~\cite{Ref:DeepRadarDetector} build the high-level architecture based on Faster-RCNN~\cite{Ref:FasterRCNN}. U-Net~\cite{Ref:U-Net} and FCN~\cite{Ref:FCN} architectures are the most popular ones among recent radar research projects~\cite{Ref:ProbabilisticOriented, Ref:RadarGhostObject, Ref:DeepOpenSpace}.
%In addition, \cite{Ref:RADVehicle} seperates Range-Azimuth-Doppler input along $3$ axes and builds its own model under the inspiration of traditional radar signal processing. 
%\cite{Ref:3DRadarCube} directly uses 3D convolution for processing input radar cube.

For object detection, the detection head is one of the most researched topics. Typical anchor-based one-stage detection heads~\cite{Ref:SSD, Ref:YOLOv4, Ref:FocalLoss} are still leading in this field. 
%on the popular benchmarks \cite{Benchmark:ImageNet, Benchmark:COCO, Benchmark:KITTI}. 
On the other hand, anchor-free one-stage detection heads~\cite{Ref:FCOS} have gained certain popularity recently and the performance is gradually catching up with anchor-based detection heads. Self-attention layers~\cite{Ref:SAGAN, Ref:SAUnet} show a growing potential in achieving further improvements in this task. Two-stage detection heads~\cite{Ref:FasterRCNN} are always considered to be able to achieve higher performance but much slower than one-stage heads. In radar research, \cite{Ref:ProbabilisticOriented} proposes a probabilistic oriented object detection head for oriented 2D box detection.

%{\color{red}\textbf{REPEATING yourself}} \textit{In conclusion, the recent radar researched can be generally split into cluster-based and spectrum-based. In spectrum-based researches, specifically for radar object detection, most researchers tend to use 2D spectrums instead of original 3D data. Inevitably, this will result in the loss of input information. On the other hand, although some researchers successfully conduct object detection on radar data with considerably high performance, only limited deep learning architectures have been explored.}
%
%In this paper, we introduce a novel dataset and propose a new model for object dataction. Our contributions are as follows:
%\begin{itemize}
%	\item A novel dataset that contains raw Radar observations with corresponding annotations for various object classes is introduced. The dataset is available\footnote{LINK GOES HERE} for future research in this field. 
%	
%	\item An automatic annotation method that generates ground-truth labels on all dimensions of Range-Azimuth-Doppler (RAD) data and in the Cartesian form is proposed. 
%	
%	\item A new radar object detection model is proposed. Our model employs a backbone based on ResNet~\cite{BG:ResNet}. The ultimate form of the backbone is achieved after a systematic explorations of deep learning models on the radar data. Inspired by YOLO head~\cite{Ref:YOLOv4}, we propose a new dual detection head, with a 3D detection head on RAD data and a 2D detection head on the data in Cartesian coordinates.
%	
%	\item Our proposed model is extensively evaluated against the well known image base object detection counter-parts. Results show that our proposal achieves the state of the art performance in the object detection task with Radar data. Significance of our results show that Radar is a viable competitor to camera and lidar based object detection methods.
%\end{itemize}
%Compared with the previous researches, our proposal not only takes full-size information of radar data but also avoids heavy computational power by using 2D convolutions. And hopefully, we can bridge the gap between radar object detection and image object detection.


\begin{figure*}[!t]
	\centering
%	\includegraphics[width=.9\textwidth]{./images/RAD_annotation_horizontal.jpg}
%	\includegraphics[width=.99\textwidth]{./images/RAD_annotation_horizontal2.jpg}
%	\includegraphics[width=.99\textwidth]{./images/RAD_annotation_horizontal_converge.jpg}
	\includegraphics[width=.99\textwidth]{./images/RAD_annotation_horizontal_converge2.jpg}
	\caption{The proposed auto-annotation method for the ground truth generation that includes an enhanced radar pre-processing and stereo depth estimation.}
	\label{F:InstancizeProposal}
\end{figure*}

Compared with the previous research projects, our proposed dataset takes full-size information of radar data, including velocity, range, angle and the corresponding position in Cartesian coordinates. In addition, we choose ResNet~\cite{BG:ResNet} and YOLO~\cite{Ref:YOLOv4} as the base models for our proposed deep learning algorithm. In return, it boosts the performance of object detection on radar data with the optimal run time speed.


\section{Dataset Preparation}


In this section, we introduce the hardware setups of our radar and stereo cameras. We propose a new auto-annotation method that tackles the challenging radar annotation task. Extracted annnotations are then manually validated to ensure the quality of dataset. Finally, we report the statistical analysis of our dataset.

%\begin{figure}[h]
%	\centering
%	\begin{subfigure}{0.25\textwidth}
%		\centering
%		\includegraphics[width=.5\linewidth]{images/sensor_setup.jpg}
%		%		\caption{A subfigure}
%		%		\label{fig:sub1}
%	\end{subfigure}%
%	\begin{subfigure}{0.25\textwidth}
%		\centering
%		\includegraphics[width=.58\linewidth]{images/calib_target.jpg}
%		%		\caption{A subfigure}
%		%		\label{fig:sub2}
%	\end{subfigure}
%	\caption{Left: Sensors setup, with stereo cameras on the top and radar in the yellow box. Right: trihedral corner reflector with a white foam plate attached.}
%	\label{F:SensorsSetup}
%\end{figure}

\subsection{Hardware Setup}
\label{Section:ProjectionMatrix}

{The sensors used in data collection consist of a \textit{Texas Instruments} AWR1843-BOOST\footnote{\url{www.ti.com}} radar and a pair of DFK 33UX273\footnote{\url{www.theimagingsource.com}} stereo cameras from \textit{The Imaging Source}. }
%Figure~\ref{F:SensorsSetup} shows our sensors setup. The configurations of the radar sensor is specified in Table~\ref{T1:radarConfigurations}. 
%{\color{red}Due to the page limitation, details of sensor configurations and registration setups are presented at the project web page\footnote{\url{http://www.site.uottawa.ca/research/viva/projects/raddet/index.html}}.}
%{\color{red}Based on the radar configurations, the raw ADC data are sized $(256, 8, 64)$. }
The sensors calibration 
%{\color{red}in this research} 
is implemented with a self-made trihedral corner reflector 
%{\color{red}and the projection matrix $P^{radar}_{stereo}$ from the 3D stereo plane to the 2D radar plane is computed} 
under the instruction of \cite{BG:Calibration}. 
{Random locations were set for the calibration target with distances to the sensors ranging from $5 m$ to $50 m$. The projection matrix from the 3D stereo frame to the bird-eye-view radar frame is computed with least square optimization.} Raw ADC data captured from the radar have a shape of $(256, 8, 64)$.




%As we can view the number of virtual antennas as a combination of the number of transmitters and receivers, the size of Analog-to-Digital Converter (ADC) data can be computed as $(256, 8, 64)$. Due to the limited resolution of elevation angle of the radar, we opt to only consider the two dimensional birds eye view information from it.

%\begin{table}[h]
%	\begin{center}
%		\begin{tabular}{ c | c }
%			\hline
%			\\[-1ex]
%			\textbf{Parameter} & \textbf{Value} \\
%			\\[-1ex]
%			\hline\hline
%			\\[-1ex]
%			Maximum range & $50$ m \\
%			Range resolution & $0.1953125$ m \\
%			Maximum velocity & $13.4297698$ m/s \\
%			Velocity resolution & $0.41968$ m/s \\
%			Number of transmitters enabled & $2$ \\
%			Number of receivers enabled & $4$ \\
%			Number of samples per period & $256$ \\
%			Number of chirps per frame & $64$ \\
%			Data capture frequency & $10$ Hz \\[1ex]
%			\hline
%		\end{tabular}
%	\end{center}
%	\caption{Configurations of Radar.}
%	\label{T1:radarConfigurations}
%\end{table}

%\begin{table}[h]
%	\begin{center}
%		\begin{tabular}{ c | c }
%			\hline
%			\\[-1ex]
%			\textbf{Parameter} & \textbf{Value} \\
%			\\[-1ex]
%			\hline\hline
%			\\[-1ex]
%			Image resolution & $1440 \times 1080$ \\
%			Data capture frequency & $10$ Hz \\[1ex]
%			\hline
%		\end{tabular}
%	\end{center}
%	\caption{Configurations of Stereo Cameras.}
%	\label{T2:stereoConfigurations}
%\end{table}

%For data synchronization, the timestamps are manually added to the radar outputs in order to synchronize with the cameras. The implementation is conducted using the Robotics Operating System (ROS). We also re-calibrated the recorded timestamps of both sensors during the data capture. 
%As a result, the time difference between the two sensors is tolerable. We thus consider the synchronization as durable for our research.

%The sensors calibration is implemented with a self-made trihedral corner reflector, shown in Figure~\ref{F:SensorsSetup}. In order to make the corner reflector easily recognizable for cameras, a coloured triangle shape foam plate is attached to the front of it. The projection matrix from stereo camera's frame to radar's frame is computed based on \cite{Ref:Calibration}.

\subsection{Annotation Method}

%As a common sense, the data collected from radars is hardly readable. Thus, various pre-processing algorithms have been developed over the years.

%Traditional Digital Signal Processing (DSP) for Frequency Modulated Continuous Wave (FMCW) radar is divided into two steps. First, Fast Fourier Transform (FFT) is performed on each dimension of received Analog-to-Digital Converter (ADC) signals. The primary output of this step is Range-Azimuth-Doppler (RAD) spectrum. Second, Constant False Alarm Rate (CFAR) is employed for filtering out the noise signals. There are two major CFAR algorithms, namely Cell-Averaging CFAR (CA-CFAR) and Order-Statistic (OS-CFAR). OS-CFAR is normally more preferable for academic usages due to its high-quality outputs, while CA-CFAR is often used in the industry because of the speed. The output of this step is often transformed to Cartesian coordinates and presented as a point-cloud, which is the base of  cluster-based radar data analysis for various applications. In our dataset, the radar data pre-processing employes the 2D OS-CFAR algorithm.

Traditional DSP for FMCW radar is divided into two steps. First, FFT is performed on each dimension of received ADC signals. The primary output of this step is RAD spectrum. 
In our research, zero-padding is used while computing FFT on Azimuth dimension and the RAD outputs are generated with a shape of $(256, 256, 64)$. 
Second, Constant False Alarm Rate (CFAR) is employed for filtering out the noise signals on Range-Doppler dimensions. There are two major CFAR algorithms, namely Cell-Averaging CFAR (CA-CFAR) and Ordered-Statistic CFAR (OS-CFAR). OS-CFAR is normally more preferable for academic usages due to its high-quality outputs, while CA-CFAR is often used in the industry because of the speed. 
%For the detetions on Azimuth dimension, peak values are extracted due to the limited resolution. 
The output of this step is often transformed to Cartesian coordinates and presented as a point-cloud., which is the base of  cluster-based radar data analysis in various applications.
% In our dataset, the radar data pre-processing employes the 2D OS-CFAR algorithm.

%\begin{figure}[!t]
%	\centering
%%	\includegraphics[width=3.3in]{./images/RAD_annotation_preprocess.jpg}
%	\includegraphics[width=3.3in]{./images/RAD_annotation_overall.png}
%	\caption{Our proposed radar pre-processing method for enriching the radar detection and extracting radar instances.}
%	\label{F:InstancizeProposal}
%\end{figure}


By analyzing the output of traditional radar DSP, we found that the detection rate of dynamic road users is significantly higher than static road users. This is due to the fact that dynamic road users usually have richer information on the Doppler axis. Therefore, we set our targets to dynamic road users only. On the other hand, traditional OS-CFAR can generate false detections in some cases even with fine-tuned parameters. For example, it could ignore points when the objects are too big, or it could mislabel noise points as detections. Our proposed annotation method can effectively solve these issues.

On the Range-Doppler (RD) outputs from 2D OS-CFAR, rigid bodies such as vehicles can be easily detected due to the speed coherence on the Doppler axis. For pedestrians, different motions of different body parts may result in various output patterns on the RD spectrum~\cite{Ref:DeepRadarDetector}. However, when the radar's range resolution reaches a certain level, the complexity of human bodies' motion can hardly be observed. Thus, they also can be considered as rigid bodies. An example is shown in Figure~\ref{F:InstancizeProposal}. One property of the rigid bodies appearing on the RD spectrum is that they are often presented as linear patterns, despite the angle differences between the objects and the radar. Thus, by connecting the discrete patterns on the RD spectrum, we successfully enrich the detection rate of traditional 2D OS-CFAR.

However, this method provides poor detections when objects are at the same range with a similar speed. DBSCAN~\cite{BG:DBSCAN} can effectively alleviate such problems. This way, we can precisely extract object instances from RAD data. 
%{\color{gray}Figure~\ref{F:InstancizeProposal} shows the whole pipeline of our proposed radar pre-processing method.}
The dataflow of the proposed radar pre-processing method is shown in Figure~\ref{F:InstancizeProposal}.

With our proposal, radar instances can be easily extracted from RAD spectrum. 
%However, {\color{red}as the categories of those radar instances are hardly recognizable for human observers,} 
However, classifying radar-only instances is a challenging task. In this research, we rely on stereo vision 
%we still need other sensors for classifying these instances. In this research, we used stereo vison 
for the category labelling. The whole process is described as follow. 
%First, stereo depth estimation is implemented with OpenCV~\footnote{https://opencv.org/} to generate disparity maps. 
First, disparity maps are generated from the rectified stereo image pairs using Semi-Global Block Matching algorithm with OpenCV\footnote{https://opencv.org/}. 
Then, the pre-trained Mask-RCNN~\cite{Ref:MRCNN} model is applied on the left images to extract instance-wise segmentation masks. The prediction masks are then projected onto the disparity maps. Finally, using the triangulation, the instance-wise point cloud outputs with predicted categories are generated. Afterwards, the point cloud instances are transformed to the radar frame using the projection matrix obtained in \ref{Section:ProjectionMatrix}.

%{\color{gray}With the radar instances $I_{radar}$ and stereo instances $I_{stereo}$ obtained, the rest of our auto-annotation method can proceed as described in Algorithm~\ref{A1:AnnotationMethod}.}
Our auto-annotation method is developed with the obtained radar and stereo instances. Figure~\ref{F:InstancizeProposal} shows the whole pipeline of the process. 
%{where \color{red}$\tt{findCenter}$ is defined as a Gaussian regression based center finding function. }
%{\color{gray}Although this algorithm performs well on labelling radar data,} 
Using this method, we can retrieve annotations for around $75\%$ of all the objects in the dataset. 
%{\color{red}Although this method reaches around $75\%$ accuracy on labelling radar data, }
%it still has a certain level of error rate. 
%Multiple factors can contribute to the errors in the annotations, such as noisy radar instances extraction, wrong depth estimation and error predictions from Mask-RCNN~\cite{Ref:MRCNN}. 
Multiple factors can contribute to the errors in the annotations, such as errors in radar instance extraction due to the sensor noise, errors in depth estimation from the stereo cameras, and errors in predictions of the Mask-RCNN~\cite{Ref:MRCNN}.  
In addition, since the Field of View (FOV) of the stereo cameras is considerably smaller than the radar's, a certain number of objects are left unlabelled. To solve these issues, manual corrections are conducted 
%{\color{red}with a self-developed annotation tool} 
%after the dataset is generated. 
after the auto-annotation process to generate the dataset. 
%All the instances are correctly labelled in the final dataset.

%\begin{algorithm}[h]
%	{\color{gray}
%	\DontPrintSemicolon
%	\KwIn{$I_{stereo}$, $I_{radar}$, $D_{threshold}$, $P^{radar}_{stereo}$} %\tcp*{this is a comment}
%	\KwOut{$Annotation_{radar}$}
%	Initialize $Annotation_{radar}$\;
%%	\KwData{Testing set $x$}
%%	$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}
%%	\tcc{Now this is an if...else conditional loop}
%	\For{$I_{s} \in I_{stereo}$}
%	{
%		Initialize $D_{min}$, $I_{target}$\;
%		$C_{stereo}$ = $P^{radar}_{stereo}$ $\times$ $\tt{findCenter}$($I_{s}[``cluster"]$)\;
%		\For{$I_{r} \in I_{radar}$}
%		{
%			$C_{radar}$ = $\tt{findCenter}$($I_{r}[``cluster"]$)\;
%			$d$ = $\tt norm$($C_{stereo} - C_{radar}$)\;
%			\If{$d \leq D_{min}$}
%			{
%				$D_{min}$ = $d$\;
%				$I_{target}$ = $I_{r}$\;
%			}
%		}
%		\If{$D_{min} \leq D_{threshold}$}
%		{
%			$class$ = $I_{s}[``class"]$\;
%			$box_{3D}$ = $\tt{getBox}$($I_{target}[``RAD\_mask"]$)\;
%			$box_{2D}$ = $\tt{getBox}$($I_{target}[``cluster"]$)\;
%			$Annotation_{radar} \leftarrow \{class, box_{3D}, box_{2D}\}$\;
%			$I_{radar}$$\tt .remove$($I_{target}$)
%		}
%	}
%	\caption{Auto-annotation algorithm}
%	\label{A1:AnnotationMethod}
%	}
%\end{algorithm}



%Despite all the issues mentioned above, it is obvious that the whole process uses a couple of other algorithms that are computational expensive, and thus takes considerably high computation power to be executed. Also, in order to make the algorithm work, more than one sensor should be employed. For such reasons, we propose our radar dataset for deep learning algorithms development to solve these issues.


\subsection{Dataset Analysis}

%{\color{gray}The size of the ADC data in our research is $(256, 8, 64)$. Zero-paddings are used when computing FFT on Azimuth dimension. Therefore, the RAD outputs from FFT, which are the inputs in our dataset, are sized $(256, 256, 64)$.}

The data capture sessions were conducted in sunny weather conditions at several random locations during the time span from September to October. The sensors were set up on the sidewalks and facing the main roads, as shown in Figure~\ref{F:DatasetSample}. 
%A total number of $10158$ frames were selected from over $60800$ frames. {\color{red}Consecutive frames were avoided in order to improve dataset generalization.} 
After removing the corrupted frames, a total number of $10158$ frames are collected to build our dataset.
As for the statistical analysis of the dataset, Figure~\ref{F:DatasetAnalysis} shows the details.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.25\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{images/total_number_objects.png}
%		\caption{A subfigure}
%		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{0.25\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{images/range_distribution_all_obj.png}
%		\caption{A subfigure}
%		\label{fig:sub2}
	\end{subfigure}
	\caption{Left: distribution of the number of objects over all categories. Right: range distribution over all objects.}
	\label{F:DatasetAnalysis}
\end{figure}

We split our dataset into train and test sets with the ratios of $80\%$ and $20\%$ respectively using class-wise random sampling. This ensures that both our train and test sets are following the overall distributions shown in Figure~\ref{F:DatasetAnalysis}. Furthermore, we split the train set to $90\%$ for training and $10\%$ for validation during the experiments to find the optimal coefficients and models.

\section{RADDet}

The state-of-the-art image-based object detection algorithms consist of $3$ parts, a backbone, a neck and a detection head~\cite{Ref:YOLOv4, Ref:FCOS, Ref:FocalLoss}. Inspired by that we formulate our backbone networks based on widely used ResNet~\cite{BG:ResNet}. In the image domain, the neck layers are used to extract ouputs in multiple levels in order to handle the scale variations of the objects. However, unlike images, where the distance changes the size of the objects due to pespective geometry, radars reveal the true scale of the objects. Therefore, multi-resolution neck layers are not considered in our research. Finally, we propose a novel dual detection head based on the well-known anchor-based algorithm YOLO~\cite{Ref:YOLOv4}. Figure~\ref{F:Architecture} shows the dataflow of our proposed architecture. Details are introduced in the following sections.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=.90\textwidth]{./images/NetworksArchitecture.jpg}
	\caption{Dataflow of our proposed networks architecture.}
	\label{F:Architecture}
\end{figure*}

\subsection{Global Normalization of the Input}

The raw input to our model is a Range-Azimuth-Doppler tensor of size $(256, 256, 64)$, represented in the complex number form. As done in traditional radar DSP, we first extract the magnitude of the raw input and take log of it. In order to normalize the input, the mean value $V_{mean}$ and the variance value $V_{variance}$ are searched over the entire dataset $D_{dataset}$. Then, each input tensor is normalized as Equation~\ref{Eq:Normalization}.
\begin{equation}
	{I_i}_{norm} = \frac{(I_{i} - V_{mean})}{V_{variance}},\,\, I_{i} \in D_{dataset}
	\label{Eq:Normalization}
\end{equation}
where, ${I_i}_{norm}$ is the input to our backbone in the next step.

\subsection{RadarResNet}

The input, as introduced above, is quite different from a colored image that is used in image based methods. In order to adapt 2D convolutional layers to our task, we set Range-Azimuth axes as the input dimensions and the Doppler axis as the original channel size. Channel expansion and feature map downsampling methods are considered during the architecture search. After a large set of architecture exploration and experiments, we finalized our backbone network based on ResNet~\cite{BG:ResNet} and named it RadarResNet. Figure~\ref{F:Architecture} shows the RadarResNet that consists of two types of blocks, namely the residual blocks and the downsampling blocks. The residual blocks are of the same structure as the basic residual blocks in ResNet and the downsample blocks consist of one max-pooling layer. Activation functions are set as Rectified Linear Unit and Batch Normalization~\cite{BG:BN} is applied after each convolutional layer in the residual blocks. The final output size of the proposed backbone is $(16, 16, 256)$.

%{\color{red}We choose max-pooling layers over stride two convolutional layers is that our input RAD matrices are generated with zero paddings during the FFT process.  DOESN"T MEAN ANYTHING AND IS NOT CORRECT.} As seen in Figure~\ref{F:DatasetSample}, the input data shows strong magnitudes over the entire Azimuth axis. {\color{red}This may caue a reduction in the accuracy of the box regression. WHAAT?} Using max-pooling over the Azimuth axis, such problems can be resolved and thus, a higher performance in achieved.

%As shown in figure~\ref{F:Architecture}, we choose max-pooling layers over the strided convolutional layers to downsample the feature maps. 
%{\color{gray}Unlike the strided convolution layers that skip over certain indexes of the feature maps, utilizing a max-pool on top of a convolutional layer ensures that all locations of the feature maps are visited during downsapling process. Given the size of the objects in these feature maps, it is shown to be crucial to not skip any data point in the feature map.}
%{\color{red}Since the input data are acquired from FFT with zero-paddings, the resolution is limited with high magnitudes across over a certain bandwidth on Azimuth dimension. Max-pooling layers, which function similarly to CFAR algorithms, can effectively sparse such data, and thus reduce the noise signals being forwarded to the next residual blocks.}

%\begin{figure}[!h]
%	\centering
%	\begin{subfigure}{0.25\textwidth}
%		\centering
%		\includegraphics[width=.95\linewidth]{images/anchor_kmeans.png}
%		%		\caption{A subfigure}
%		%		\label{fig:sub1}
%	\end{subfigure}%
%	\begin{subfigure}{0.25\textwidth}
%		\centering
%		\includegraphics[width=.95\linewidth]{images/anchor_kmeans_cartboxes.png}
%		%		\caption{A subfigure}
%		%		\label{fig:sub2}
%	\end{subfigure}
%	\caption{Left: K-means errors of 3D RAD anchor boxes over the dataset. Right: K-means errors of 2D Cartesian anchor boxes over the dataset.}
%	\label{F:AnchorsFinding}
%\end{figure}



%{\color{red}\textbf{Structure of section C and D is off.... you say dual detection head explain the 3D one, but then you keep the 2D head for the section D... HOW About doing this way}
%\subsection{Dual Detection Head}
%1st paragraph of section C
%\subsubsection{3D Detection Head}
%2nd paragraph of section C
%\subsubsection{2D Detection Head}
%Section D
%}


\subsection{Dual Detection Head}

Our detection head consists of two branches: a 3D and a 2D detection head. Both of them are anchor-based methods and with similar structures as YOLO~\cite{Ref:YOLOv4}. In our research, $6$ anchor boxes are defined for both 3D and 2D detection head branches
%. The sizes of anchor boxes are extracted 
by 
using K-means clustering on all the ground truth boxes with the error rate threshold set to $10\%$. 

%Figure~\ref{F:AnchorsFinding} illustrates the K-means errors for both detection heads. {\color{red} FIGURE MIGHT NEED MORE EXPLANATION}

\subsubsection{3D Detection Head}

Since the outputs of our 3D detection head are 3D bounding boxes, some modifications are applied to the traditional YOLO Head in order to fit our task. First, the 
%{\color{red}model} 
output size of the third 
%{\color{red}on Doppler} 
dimension, Doppler dimension, is calculated with the same 
%{\color{red}grid} 
stride as other dimensions. In our case, the output size of the Doppler axis is $4$. Second, instead of convolving the feature maps into $(16, 16, num\_of\_anchors \times (5 + num\_of\_classes))$~\cite{Ref:YOLOv4}, the 3D head processes the feature maps into $(16, 16, 4 \times num\_of\_anchors \times (7 + num\_of\_classes))$, where $7$ stands for the objectness and the 3D box information. The 3D box information consists of the 3D center point [$x$, $y$, $z$], and the size [$w$, $h$, $d$]. Finally, the raw detection output is reshaped into the final output format $(16, 16, 4, num\_of\_anchors, 7 + num\_of\_classes)$, before it is fed into Non-maximum Suppression (NMS). Other operations such as box location calculation and interpretation, are structured in the same form as YOLO~\cite{Ref:YOLOv4}. For convenience, we named our 3D detection head as RAD YOLO Head.

\subsubsection{2D Detection Head}
%\subsubsection{2D Detection Head}

The 2D detection head consists of two parts; a coordinate transformation layer that transforms the feature maps from polar representation to the Cartesian form, and a classic YOLO Head~\cite{Ref:YOLOv4}. The structure of the coordinate transformation layer is shown in Figure ~\ref{F:CoordinateTransformation}.

Traditionally, the coordinate transformation from range and azimuth domain $[r, \theta]$ to cartesian width and depth domain represented by $[x, z]$, is formulated by the following equations.
\begin{equation}
\begin{split}
	x &= r \cdot \text{cos}(\theta) \\
	z &= r \cdot \text{sin}(\theta) \\
	\theta &\in [-\pi/2, \pi/2]
\end{split}
\label{eq:Pol2Cart}
\end{equation}
This non-linear transformation can double the size of the input along the width dimension in cartesian form. %For example, if the input is sized $(16, 16)$, the output will be in shape $(32, 16)$.

Our Coordinate Transformation Layer was inspired by the traditional method. The input feature maps are in the form $[r, \theta]$ with the size of $(16, 16, 256)$. They can be interpreted as $256$ RA feature maps with the shape of $(16, 16)$. Each of the RA feature maps is fed individually into two fully connected layers for non-linear transformation. The outputs of these features are then reshaped to $(32, 16)$ and concatenated to build the Cartesian feature maps of size $(32, 16, 256)$. Finally, since they are low-level feature maps, one residual block, with the same structure as the basic residual block in ResNet~\cite{BG:ResNet}, is structured for post-processing. The transformed outputs are then directly fed into YOLO Head for 2D box predictions.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.3in]{./images/channel-wise_MLP.jpg}
	\caption{Coordinate Transformation Layer consists of Channel-wise fully connected layers acting as non-linear transformation for each feature map and a residual block to summarize the features.}
	\label{F:CoordinateTransformation}
\end{figure}

\subsection{Loss Functions}

The outputs of the YOLO Head are normally split into $3$ tasks, namely box regression, objectness prediction and classification \cite{Ref:YOLOv4}. Various loss functions have been explored for these $3$ tasks to improve the performance. In our research, The total loss $L_{total}$ is formulated as Equation~\ref{Eq1:TotalLossFunction}.

\begin{equation}
	L_{total} = \beta \cdot L_{box} + L_{obj} + L_{class}
	\label{Eq1:TotalLossFunction}
\end{equation}

The loss function for box regression loss $L_{box}$ is chosen from \cite{Ref:YOLOv1}. Focal Loss~\cite{Ref:FocalLoss} is applied for training the objectness prediction loss $L_{obj}$. However, in order to remove the dominance of non-object predictions, we set the $\alpha$ coefficient to $0.01$ for negative samples in the Focal Loss. For the classification loss $L_{class}$, we use Cross Entropy~\cite{BG:CrossEntropy} as the loss function. During the training, we found that the box loss can easily dominate the total loss values. Therefore, we set another coefficient $\beta = 0.1$ for the box loss. 



\section{Experiments}


We follow the same experimental setup of image based object detection literature for our model exploration and experiments. First, we explore several popular backbones, namely VGG~\cite{BG:VGG} and ResNet~\cite{BG:ResNet}. Next, usage of layers such as self-attention and channel-wise Multilayer Perceptrons (MLP) prior to extracting bounding boxes are explored. Among the various self-attention proposals from the literature, we selected two, SAGAN~\cite{Ref:SAGAN} and SAUNet~\cite{Ref:SAUnet}, for our experiments. Since our detection heads share the same backbone, we trained the backbones along with the RAD YOLO head as it is more complicated than the 2D detection head. Before training the 2D detection head, we froze the backbone layers after fine-tuned on the 3D detection head. The following sections show the comparisons of different backbones on both 3D detection head and 2D detection.

Hyper-parameters used in our experiments are defined as follows: batch size is set to $3$; initial learning rate is $0.0001$; the learning rate decay is $0.96$ with every $10$K steps after $60$K warm-up steps; the objectness threshold is $0.5$; the NMS thresholds for 3D and 2D detection heads are $0.1$ and $0.3$ respectively; the optimizer is set to Adam~\cite{BG:Adam} and the mean Average Precision is chosen as the metrics. All experiments are conducted with an RTX 2080Ti GPU and TensorFlow API\footnote{\url{https://www.tensorflow.org/}}.

\begin{table*}[!t]
	\begin{center}
		\begin{tabular}{ c | c c c c c c c c }
			\hline
			\\[-1ex]
			\textbf{Backbone Name} & \textbf{Detection Head} & \textbf{$\text{AP}_{0.1}$} & \textbf{$\text{AP}_{0.3}$} & \textbf{$\text{AP}_{0.5}$} & \textbf{$\text{AP}_{0.7}$} & \begin{tabular}{@{}c@{}} \textbf{Backbone} \\ \textbf{Params} \end{tabular} &  \begin{tabular}{@{}c@{}}\textbf{Head} \\ \textbf{Params} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Inference} \\ \textbf{Time}($\tt ms$)\end{tabular} \\
			\\[-1ex]
			\hline
			\\[-1ex]
			VGG  & RAD YOLO Head & 0.685 & 0.499 & 0.209 & 0.035 & \textbf{2.10M} & 1.34M &  \textbf{10.5} \\
			{RadarResNet {(strided convolution)}}  & RAD YOLO Head & 0.727 & 0.538 & 0.243 & 0.054 & 6.81M & 1.34M & 75.3 \\
			RadarResNet {(max-pool)} & RAD YOLO Head & \textbf{0.764} & \textbf{0.563} & \textbf{0.251} & \textbf{0.059} & 6.73M & 1.34M & 75.2 \\
			RadarResNet+SA (SAGAN~\cite{Ref:SAGAN}) & RAD YOLO Head & 0.738 & 0.498 & 0.219 & 0.044 & 4.11M & 1.34M & 50.9 \\
			RadarResNet+SA (SAUNet~\cite{Ref:SAUnet}) & RAD YOLO Head & 0.752 & 0.559 & \textbf{0.251} & 0.053 & 7.55M & 1.34M & 78.7 \\
			RadarResNet+MLP & RAD YOLO Head & 0.759  & 0.521 & 0.217 & 0.043 & 6.86M & 1.34M & 77.6\\
			\\[-1ex]
			\hline
			\\[-1ex]
			VGG   & 2D YOLO Head & 0.757 & 0.685 & 0.484 & 0.189 & \textbf{2.10M} & 2.86M & \textbf{12.8} \\
			{RadarResNet {(strided convolution)}}   & 2D YOLO Head & 0.770 & 0.699 & 0.502 & 0.198 & 6.81M & 2.86M & 76.7 \\
			RadarResNet {(max-pool)} & 2D YOLO Head & 0.796 & 0.727 & 0.516 & \textbf{0.205} & 6.73M & 2.86M & 76.8 \\
			RadarResNet+SA (SAGAN~\cite{Ref:SAGAN}) & 2D YOLO Head & 0.787 & 0.686 & 0.452 & 0.145 & 4.11M & 2.86M & 52.6 \\
			RadarResNet+SA (SAUNet~\cite{Ref:SAUnet}) & 2D YOLO Head & \textbf{0.801} & \textbf{0.730} & \textbf{0.530} & 0.202 & 7.55M & 2.86M & 79.1 \\[1ex]
			\hline
		\end{tabular}
	\end{center}
	\caption{Average Precisions of different backbones on both detection heads.}
	\label{T3:BackboneAP}
\end{table*}


%\textcolor{red}{Why can't compare to previous radar researches.}

\subsection{Backbone Model Search}

Before training on the entire train set, we set up a framework with a small portion of the training data to find a proper output size of our backbone. After several trials on both VGG~\cite{BG:VGG} and ResNet~\cite{BG:ResNet} with different output sizes such as $(8, 8)$, $(16, 16)$ and $(32, 32)$. We found that using size $(16, 16)$ provides us with the highest performance in the 3D detection task. Therefore, we first fixed the output size of our backbone as $(16, 16, 256)$. Based on that, The total number of channel expansion blocks and downsampling blocks are calculated as $2$ and $4$. To adapt the image-based backbones to such structure, we modified each of them with proper channel expansion rates and downsampling rates. In VGG, we set the channel expansion rates of the first three blocks to $1$ and removed the pooling layer in the first block.

During the unit tests, we also found that the general performance of ResNet based backbones is higher than VGG. In addition, using max-pooling layers other than residual blocks for downsampling can significantly improve the performance as well. We then formulated our RadarResNet as illustrated in Figure~\ref{F:Architecture}. However, for ResNet-like architectures, the repeat times of the residual blocks vary when it comes to different tasks. Thus, we trained our RadarResNet with various sizes on the entire train set to find the optimal model. Table~\ref{T4:BackboneSearch} shows some examples of the backbone size exploration process. We finalized our RadarResNet with repeat times $2$, $4$, $8$, $16$ before each max-pooling layer. 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{ c | c c c c }
			\hline
			\\[-1ex]
			\textbf{Backbone Name} & \textbf{$\text{AP}_{0.1}$} & \textbf{$\text{AP}_{0.3}$} & \textbf{$\text{AP}_{0.5}$} & \begin{tabular}{@{}c@{}} \textbf{Backbone} \\ \textbf{Params} \end{tabular}\\
			\\[-1ex]
			\hline
			\\[-1ex]
			RadarResNet [1, 2, 4, 8] & \textbf{0.766} & 0.554 & 0.238 & 3.68M \\
			RadarResNet [2, 4, 4, 8] & 0.752 & 0.555 & 0.247 & 3.91M \\
			RadarResNet [4, 8, 8, 8] & 0.753 & 0.557 & 0.247 & 4.70M \\
			RadarResNet [2, 4, 8, 16] & 0.764 & \textbf{0.563} & \textbf{0.251} & 6.73M \\
			RadarResNet [4, 8, 8, 16] & \textbf{0.766} & \textbf{0.563} & 0.247 & 7.21M \\[1ex]
			\hline
		\end{tabular}
	\end{center}
	\caption{Backbone size searching samples with numbers inside ``[]'' indicating the repeat times of residual blocks before downsampling.}
	\label{T4:BackboneSearch}
\end{table}



\subsection{Detection Head Comparisons}


We conduct systematic comparisons between different backbones on both 3D and 2D detection heads in our experiments. Apart from the mentioned models, self-attention layers~\cite{Ref:SAGAN, Ref:SAUnet} are also explored. Our experiments show that self-attention layers convey the potential to dominate in object detection. Although the implementation of self-attention layers varies among different studies, the mechanism remains the same. In order to study the general impact of self-attention layers on our model, we employed the SAGAN~\cite{Ref:SAGAN} and SAUNet~\cite{Ref:SAUnet} individually on our RadarResNet. For convenience, we named these two backbones as RadarResNet+SA (SAGAN) and RadarResNet+SA (SAUNet). In addition, we experimented the channel-wise Multilayer Perceptrons (MLP) at the end of RadarResNet to form another backbone named RadarResNet+MLP. Table~\ref{T3:BackboneAP} shows the performance of the different backbones on different detection heads. The inference time is calculated by averaging the run time over the entire test set with batch size set to $1$.

For the RAD YOLO Head, the RadarResNet outperforms all other backbones due to several potential reasons. First, similar to image based models, the residual blocks outperform sequential convolutional layers in radar object detection as the original input signals are amplified during the process. Second, both types of self-attention layers are designed for images and are not able to boost performance in radar-based tasks. Table~\ref{T3:BackboneAP} also shows that max-pooling layers are more preferable than the strided convolutional layers in our task. 
%{\color{gray}Due to lower AP of RadarResNet+MLP on the 3D detection experiments, }
As the 2D detection head already contains MLP, 
we excluded the RadarResNet+MLP from 2D detection experiments.

For the 2D YOLO Head, although RadarResNet+SA (SAUNet) indicates slightly better performance, the RadarResNet shows better overall precision. In addition, the inference speed of RadarResNet is considerably higher than the other one. This further proves that image-based self-attention layers may not be suitable for radar-based tasks.


\subsection{Discussion}

 Some sample results on the test set are shown in Figure~\ref{F:DatasetSample}. There are two common false detections found during our experiments, mis-classification and false positive detection. We believe both problems can be a resulting from the dataset. Although we have created and labeled a large dataset, the class labels of our dataset suffer from label imbalance shown in Figure~\ref{F:DatasetAnalysis}. In addition, the limited dataset size constrains the model to underperform and more performance will be achieavable with an expansion on the dataset size. Some ambiguities of the classes can also be found resulting from the sizes of the objects in the dataset. For example, assigning mini-vans to the ``truck'' class and assigning large SUVs to the ``car'' class confuses the model due to the fact that the signal traces of these objects look similar in radar frames.

%For example, it is hard to tell whether the small-size mini-vans belong to ``truck" or ``car" on the dataset. \textbf{this has to be solved already... I think you are trying to say that they are so similar that models are confused in assigning them to their classes}

%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=\textwidth]{./images/testset_samples.png}
%	\caption{Some samples on test set. Boxes with face colors are ground truth boxes; boxes without face colors are predictions. The last row shows some common false predictions of our model.}
%	\label{F:TestsetSamples}
%\end{figure*}



\section{Conclusion}

In this paper, we introduce a comprehensive publicly available dataset with raw radar observations for multi-class object detection of dynamic road users. Details of Range-Azimuth-Doppler (RAD) based auto-annotation method for generating annotated radar instances is provided. Furthermore, we propose a robust and fast deep learning model for RAD based object detection and benchmark it against the other well known models. To the best of our knowledge, this is the first model that inputs raw radar inputs and predicts the position, speed and category of the dynamic road users. Despite the large size of our proposed dataset, it is still much smaller than its image based counterparts. Collection of more targeted data can balance the object categories and result in better performance of the models. Our proposed model achieves fast inference speeds on an RTX 2080Ti. However, deploying it on embedded platforms will result in a drastic slow down. To address this, a hardware aware model exploration needs to be conducted. Lastly, we believe using the self-attention layers to target this specific data rather than employing them from image based literature could significantly enhance performance of such models.

We sincerely hope that this research and the dataset can bridge the gap between image or Lidar based object detection and radar-based object detection, and inspire the development of more algorithms on automotive radars.


%\textcolor{red}{Dataset is created by 1 person, so it is full of personal prejudice}

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank SensorCortek\footnote{\url{https://sensorcortek.ai/}} for their help and high quality radar data capture tools used in our dataset development.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\newcommand{\BIBdecl}{\setlength{\itemsep}{0.1 em}}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,IEEEtranBST/MyReference}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}


% that's all folks
\end{document}


%\begin{table}[h]
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|c|}
%			\hline
%			Model & \parbox[c]{0.2\linewidth}{\vspace{.1\baselineskip}\centering Parallel layer dimensions\vspace{.3\baselineskip}} & \parbox[c]{0.2\linewidth}{\vspace{.1\baselineskip}\centering Merged layer dimensions\vspace{.3\baselineskip}} \\
%			\hline\hline
%			\textit{64\_dims} & $32\times4$ & $64\times4$ \\
%			\textit{128\_dims} & $64\times4$ & $128\times4$ \\
%			\textit{256\_dims} \textit{deep}& $64\times4$ & $128\times4 + 256\times4$ \\
%			\hline
%		\end{tabular}
%	\end{center}
%	\caption{Model names and parameters.}
%	\label{tbl:modelParams}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\documentclass{article}
%\usepackage{array,booktabs,tabularx}
%\newcolumntype{C}{>{\centering\arraybackslash}X}
%\begin{tabularx}{\linewidth}{*{3}{C}}
%	\toprule
%	Header 1 & Header 2 & Header 3\\\midrule
%	This is some text which will be longer than the width of the column. & This is some more text which will be longer than the width of this second column. & And here is some more text for the third column.\\\bottomrule
%\end{tabularx}
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.


