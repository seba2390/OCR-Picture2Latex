We consider a prophet inequality variant, where a set of $n$ rewards, $v_1, \ldots, v_n$, are revealed online. While the values $v_1, \ldots, v_n$ are unknown from the outset, $v_t$ is drawn independently from a known probability distribution $F_t$, for $t\in[n]$, where $[n]=\{1,\ldots,n\}$. 
In the classical prophet setting, a single decision maker observes the realized reward $v_t$ at time $t$, and makes an immediate and irrevocable decision whether to take it or not. If she takes it, the game ends. Otherwise, the reward $v_t$ is lost forever, and the game continues with the next reward.

Unlike the classical prophet setting that involves a single decision maker, we consider a setting with $k$ decision makers (hereafter, agents) who compete over the rewards. 
Upon the revelation of reward $v_t$, every active agent (i.e., an agent who has not received a reward yet) may select it.
If a reward is selected by exactly one agent, then it is assigned to that agent. 
If the reward $v_t$ is selected by more than one agent, it is assigned to one of these agents either randomly (hereafter, {\em random tie-breaking}), or according to a predefined ranking (hereafter, {\em ranked tie-breaking}).
Agents who received rewards are no longer active.

%Given an instance of a game, the history at time $t$
%includes all the relevant information revealed up to time $t$; i.e., the realizations of $v_1,\ldots,v_t$, and the assignments up to time $t-1$.\footnote{In our setting, additional information, such as the history of selections (in contrast to assignments) is irrelevant for future decision making.}

A strategy of agent $i$, denoted by $S_i$, is a function that for every $t=1, \ldots, n$, decides whether or not to select $v_t$, based on $t$, the realization of $v_t$, and the set of active agents\footnote{One can easily verify that in our setting, additional information, such as the history of realizations of $v_1, \ldots, v_{t-1}$, and the history of selections and assignments, is irrelevant for future decision making.}. 
A strategy profile is denoted by $S=(S_1,\ldots,S_k)$.
We also denote a strategy profile by $S=(S_i,S_{-i})$, where $S_{-i}$ denotes the strategy profile of all agents except agent $i$.

Every strategy profile $S$ induces a distribution over assignments of rewards to agents. 
For ranked tie breaking, the distribution is with respect to the realizations of the rewards, and possibly the randomness in the agent strategies. For random tie breaking, the randomness is also with respect to the randomness in the tie-breaking. 
 
The utility of agent $i$ under strategy profile $S$, $u_i(S)$, is her expected reward under $S$; every agent acts to maximize her utility.

We say that a strategy $S_i$ guarantees agent $i$ a utility of $\alpha$ if $u_i(S_i,S_{-i}) \geq \alpha$ for every $S_{-i}$. 

\begin{definition}
A single threshold strategy $T$ is the strategy that upon the arrival of reward $v$, $v$ is selected if and only if the agent is still active and $v_t \geq T$.
\end{definition}

We also use the following equilibrium notions:

\begin{itemize}
	\item Nash Equilibrium (NE): A strategy profile $S=(S_1,\ldots, S_k)$ is a NE if for every agent $i$ and every strategy $S'_i$, it holds that $u_i(S'_i,S_{-i}) \leq u_i(S_i,S_{-i})$.
	
	\item Subgame perfect equilibrium (SPE): A strategy profile $S=(S_1,\ldots, S_k)$ is an SPE if $S$ is a NE for every subgame of the game. I.e. for every initial history $h$, $S$ is a NE in the game induced by history $h$. 
\end{itemize}

SPE is a refinement of NE; namely, every SPE is a NE, but not vice versa.

In the next sections, we let $y_j$ denote the random variable that equals the $j^{th}$ maximal reward among $\{v_1,\ldots,v_n\}$. 





