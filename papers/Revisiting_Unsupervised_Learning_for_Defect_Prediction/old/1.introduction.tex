\section{Introduction}

Defect prediction for software projects has been well studied 
over the past decade and lots of researchers focus on proposing new techniques to improve predictors' performance
or make predictors more feasible for practical situation~\cite{lessmann2008benchmarking,hassan2009predicting,fu2016tuning,graves2000predicting,menzies2007data,hall2012systematic,nagappan2005use,jiang2013personalized,kim2007predicting,kim2008classifying,jing2014dictionary,lee2011micro,moser2008comparative,nam2013transfer,wang2016automatically,yang2016effort,kamei2013large}. 
Traditional defect prediction studies build learners at the granularity of  modules or files level~\cite{lessmann2008benchmarking,menzies2007data,nam2013transfer,wang2016automatically,fu2016tuning,hall2012systematic}, which can be
used to identify whether the file or module has defects after applying prediction models. 
Even though learners
may have very high accuracy, it still requires a lot of efforts from quality assurance team 
to inspect each predicted file~\cite{kamei2013large} or
module and it also adds much difficulty to fix the defects since 
the owner of the related files might not be available ~\cite{anvik2006should} or the owner forgets 
details of the implementation~\cite{weiss2007long}. 

To address these problems, researchers proposed to build defect prediction learners on change level~\cite{kim2008classifying,mockus2000predicting,sliwerski2005changes,shihab2012industrial} and predict the defect-inducing changes at check-in time~\cite{kamei2013large,fukushima2014empirical,yang2016effort}. This type of just-in-time(JIT) defect prediction
has the following benefits over traditional counterpart~\cite{kim2008classifying,kamei2013large}:
\bi
\item Predict defects at a fine granularity: Compared to the whole file or module, a committed change includes small amount of codes, which will dramatically reduce the efforts for inspection if it is predicted as defective.
\item Predict defects in a timely manner: Traditional defect prediction usually happens after the software or the package is completed. However, JIT defect prediction could happen as soon as the change is committed, which
will make the defect fixed easily as the developer would still remember the details about the change.
\ei

Mockus et al.~\cite{mockus2000predicting} conduct the first study to predict defects on change metrics from a telecommunication software project, whereas Kim et al.~\cite{kim2008classifying} further evaluate the effectiveness of change metrics on 12 open source projects.
Shihab et al.~\cite{shihab2012industrial} designed a logistic regression model to predict the risk change on
industrial software. They find that  features, like lines of code added
by the change and the developer experience, are strongly correlated to risk change. To take the effort into account, Kamei et al.~\cite{kamei2013large}
used a modified linear regression to build an effort-ware JIT defect predictor.
They reported that with $20\%$ of the total effort required to inspect all the changes,
their method can identify $35\%$ of all the defect-inducing changes. Yang et al.~\cite{yang2016effort} argued
that data collection for training models is time consuming and 
building supervised learners is expensive. Therefore, they
proposed to build unsupervised learners directly on testing data~\cite{yang2016effort}.
In their study, they identify 12 different change metrics and build
12 unsupervised learners accordingly. Based on their evaluation results, they claimed that many unsupervised learners perform better than Kamei et al.'s supervised learner~\cite{yang2016effort}. However, since the unsupervised learners
are built on the testing data, to deploy unsupervised learners on actual software
project, we still have the following questions:
\bi 
\item Do all the $12$ unsupervised learners perform equally well so that we can randomly pick one and apply to future projects?
\item If not, without any prior knowledge on the future project,  which unsupervised learner we have to use?
\ei

According to our best knowledge, Yang et al. did not shed lights on these issues in ~\cite{yang2016effort}. Therefore, in this paper, we empirically study how to make such
simple unsupervised learners work on actual software projects. Specifically, we set the following research questions:

\bi 
\item \textbf{RQ1:Do all simple unsupervised learners perform better than the best supervised learner?}\\
Experimental results show that not all, actually only two unsupervised learners,  outperform than the best supervised learner for only two evaluation measures. This indicates that without any prior knowledge on the performance of unsupervised learners, we can't find randomly pick an supervised learner to apply. Based on this observation, we proposed a new supervised learner, {\it OneWay}.

\item \textbf{RQ2:Does {\it OneWay} perform  better than unsupervised learners?}\\
Experimental results show that {\it OneWay} has pretty stable and good performance on all data sets for all evaluation measures. When deploying to the actual software projects, it performs better than unsupervised learners on average for effort-aware JIT defect prediction. 

\item \textbf{RQ3:Does {\it OneWay} perform  better than supervised learners?} \\
Experimental results {\it OneWay} performs significantly better than all 4 supervised learners in terms of {\it Recall} and $P_{opt}$, but not for {\it Precision}.
\ei

The rest of this paper is organized as follows. Section 2 describes the background and related work on defect prediction.
Section 3 explains the effort-ware JIT defect prediction methods investigated in this study, while Section 4 describes the
experimental settings of our study, including research questions that motivate our study, data sets and experimental design.
Section 5 presents the results. Section 6 discusses the threats to the validity of our study. Section 7 concludes the paper.


