\section{Experimental Settings}

\subsection{Research Questions}
To further investigate how unsupervised learners work for 
Effort-Aware JIT defect prediction in practice and its limitations,
whether {\it OneWay} method perform better compared to unsupervised learners and traditional
supervised learners, we set the following research questions:
\bi 
\item Do all simple unsupervised learners perform better than the best supervised learners?
\item Does {\it OneWay} perform  better than unsupervised learners?
\item Does {\it OneWay} perform  better than supervised learners?
\ei

When reading the results from Yang et al~\cite{yang2016effort},
we find that they report the performance scores of each learner across all projects in one chart, which might miss some information about how each learner perform on each project. Are these unsupervised learners working consistently across all the project data? if not, how it might look like? Therefore, we want to investigate the RQ1 in this study firstly.

Another observation is that even though Yang et al. propose that 
simple unsupervised learners could work better than supervised learners for effort-aware defect prediction, they actually ignore the fact that it's hard to know which metric actually works for the defect prediction before testing their unsupervised learners except for the case where all the metrics work perfectly. Based on this fact, we propose a new method, {\it OneWay}, which is simple yet applicable to practical effort-ware
JIT defect prediction scenario. Therefore,
in RQ2 and RQ3, we want to evaluate how well our proposed {\it OneWay} method perform compared to the unsupervised learners and supervised learners.

Considering our goals and questions, we reproduce Yang et al's results
and report for each project to answer RQ1 and For RQ2 and RQ3, we implement our {\it OneWay}
methods, and compare it with unsupervised learners and supervised learners on different projects in terms of various evaluation metrics.

\subsection{Data Sets}
In this study, we conduct our experiment on the data sets
from six well-known open source projects, Bugzilla, 
Columba, Eclipse JDT, Eclipse Platform, Mozilla and PostgreSQL.
These data sets are shared by Kamei et al.~\cite{kamei2013large},
which are the same data sets used in Yang et al.'s study~\cite{yang2016effort}.
The statistics of the data sets are listed in ~\tab{datasets}.
From ~\tab{datasets}, we know that all these six data sets
cover at least 4 years historical information, and the longest one is PostgreSQL, 
which includes 15 years data. The total changes for these six data sets
are from 4620 to 98275, which are sufficient for us to conduct
empirical study. In this study, if a change introduces one or more defect then
this change is considered as defect-introducing change. The percentage of 
defect-introducing changes ranges from 5\% to 36\% and over all, the average
modified files per change for all six projects is around 4.

\begin{table}[]
    \centering
    \caption{Statistics of the Studied Data Sets}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{@{}c|c c c c c@{}}
    \hline
        \multirow{3}{*}{Project} &
        \multirow{3}{*}{Period} & 
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Total\\ Change\end{tabular}} & 
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}$\%$ of \\ Defects \end{tabular}} &
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Avg LOC \\ per \\ Change\end{tabular}}  &
        \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \# Modified \\ Files per\\ Change\end{tabular}} \\ \\  \\\hline
    Bugzilla & 08/1998 - 12/2006 & 4620 & 36\% & 37.5 & 2.3 \\
    Platform & 05/2001 - 12/2007 & 64250 & 14\%& 72.2 & 4.3 \\
    Mozilla & 01/2000 - 12/2006 & 98275 & 5\% & 106.5 & 5.3 \\
    JDT & 05/2001 - 12/2007 & 35386 & 14\% & 71.4 & 4.3 \\
    Columba & 11/2002 - 07/2006 & 4455 & 31\% & 149.4 & 6.2 \\
    PostgreSQL & 07/1996 - 05/2010 & 20431 & 25\% & 101.3 & 4.5 \\ \hline
    \end{tabular}}
    \label{tab:datasets}
\end{table}



\subsection{Experimental Design}\label{exp_design}

When applying data mining algorithms to build predictive models,
one important principle is not to test on the data used in training. 
There are many ways to design a experiment that satisfies this principle.
However, some of those methods have limitations. For example, 
leave-one-out is too slow for large data sets and cross-validation mixes up older and newer data
(such that data from the past may be used to test on future data). 

To avoid these problems, we used the time-wise-cross-validation method in~\cite{yang2016effort},
which will be easier for us to compare our results against theirs.
(Due to the space limitation,
we don't provide the cross-project prediction results in this paper).
The following experiment design ensure that the testing data was actually
created after traning data.
Firstly, we sort all the changes in each project based on the commit date. 
Then all the changes that submitted in the same month are grouped together.
For a given project data set that covers totally $N$ months history, when building
a defect predictor,
\bi 
\item The two consecutive months data,  $i$ and $i+1$, are used as the training data, which will be used
to train supervised learners and {\it OneWay} learner.
\item The other two months data, $i+4$ and $i+5$, which are
two months later than the training data was created, are used as
the testing data to test the supervised learners and {\it OneWay} learner.
\ei

By using this method, each training and testing data set
has two months data, which will include sufficient positive and negative instances
for the supervised learners to learn. For any project that includes $N$ months data,
we can perform $N-5$ different experiments to evaluate our learners when $N$ is greater than 5.
For all the unsupervised learners, only the testing data 
is used to build the model and evaluate the performance.

To statistically compare the differences between {\it OneWay} 
with supervised and unsupervised learners,
we use 
% Scott-Knott procedure recommended by \cite{mittas2013ranking}.
% Scott-Knott first looks for a break in the sequence that maximizes the expected
%     values in the difference in the means before and after the break.
%     More specifically,  it  splits $l$ values into sub-lists $m,n$ in order to maximize the expected value of differences  in the observed performances before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$, Scott-Knott divides the sequence at the break that maximizes:
%      \[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]
% Scott-Knott then applies some statistical hypothesis test $H$ to check if $m,n$ are significantly different.
% If so, Scott-Knott then recurses on each division; 
% Otherwise, Scott-Knott terminates and returns the ranks
% of sub-lists.
Wilcoxon single ranked test is applied to compare
the performance scores of the learners in this study. As in~\cite{yang2016effort},
to control the false discover rate, the Benjamini-Hochberg(BH) adjusted p-value 
is used to test whether two distributions are statistically significant
at the level of $0.05$~\cite{benjamini1995controlling}. 
To measure the effect size of performance scores among {\it OneWay} and supervised/unsupervised learners,
we compute Cliff's $\delta$ that is a non-parametric effect size measure~\cite{romano2006exploring}.
As Romano et al. suggested, we evaluate the magnitude of the effect size as follows:
negligible ($|\delta|<0.147$ ), small ($ 0.147<|\delta|<0.33$), medium ($0.33<|\delta|<0.474$ ), and large (0.474 $\leq|\delta|$)~\cite{romano2006exploring}.


\begin{figure}[!t]
    \centering
    \includegraphics[width=.47\textwidth]{pic/delta.pdf}
    \caption{Example of an effort-based cumulative lift chart for Bugzilla~\cite{kamei2013large}.}
    \label{fig:delta}
\end{figure}
\subsection{Evaluation Measures}

For effort-aware defect prediction, in addition to evaluate how learners correctly predict a defect-introducing change, we have to take account of the efforts that required to inspect prediction. Ostrand et al.~\cite{ostrand2005predicting}
report that given a project, $20\%$ files contain on average $80\%$  of all defects in the project.
Although nothing magical about the number $20\%$, it has been used as a cutoff
value to set the efforts required for the defect inspection when evaluating the defect learners ~\cite{yang2016effort,kamei2013large,mende2010effort,monden2013assessing}.
That is, given $20\%$ effort\footnote{Effort means total number
of files/codes that predicted as defective need to inspect in the project}, 
how many defects actually can be detected by the learner.
To be consistent with Yang et al, in this study, we restrict our efforts to $20\%$ of total efforts.

To evaluate the performance of effort-aware JIT defect
prediction learners in our study, 
we used the following 4 metrics: {\it Precision, Recall, F1-score} and $P_{opt}$,
which are widely used in defect prediction literature~\cite{menzies2007data,menzies2010defect,zimmermann2007predicting, kamei2013large,yang2016effort,monden2013assessing}.

\vskip 2ex
\begin{eqnarray}\nonumber
Precision &=& \frac{True \, Positive}{True \, Positive +False \, Positive}\\ \nonumber
Recall &=& \frac{True \, Positive}{True \, Positive +False \, Negative}\\ \nonumber
F1 &=& \frac{2*Precision*Recall}{Recall + Precision}
\end{eqnarray}
where {\it Precision} denotes the percentage of actual defective changes to all the predicted changes and {\it Recall} is the percentage of predicted defective changes to all actual defective changes. {\it F1 } is a measure that combines both {\it precision} and {\it Recall} which is the harmonic mean of {\it Precision} and {\it Recall}. As mentioned above, we use $20\%$ efforts to evaluate learners, which means
we only inspect $20\%$ of all the top ranked changes that are predicted as defective. Therefore, all the evaluation metrics, {\it Precision}, {\it Recall} and {\it F1} considered in the following sections are using $20\%$ efforts.  

The last evaluation metric used in this study is $P_{opt}$,
which is defined as $1- \Delta_{opt}$, where $\Delta_{opt}$ is the area
between the effort(code-churn-based) cumulative lift charts of the optimal
model and the prediction model (shown in \fig{delta}). 
In this chart, the x-axis is considered as the percentage of required effort to inspect the change and the y-axis
is the percentage of defect-introducing change found in the selected change. In the optimal model, all the changes are sorted by the actual defect density in descendant order, while for the predicted model, all the changes are sorted by 
the actual predicted value in  descendant order.

According to \cite{yang2016effort,kamei2013large,monden2013assessing}, the normalized $P_{opt}$ can be denoted as follows:
\begin{eqnarray} \nonumber
P_{opt}(m) = 1- \frac{S(optimal)-S(m)}{S(optimal)-S(worst)}
\end{eqnarray}
where $S(optimal)$, $S(m)$ and $S(worst)$ represent the area of the optimal model, predicted model, and worst model
in \fig{delta}, respectively. Note that the worst model is built by sorting all the changes according to the actual defect density in ascendant order, which is used to normalize $P_{opt}$. Since the value of normalized $\Delta_{opt}$ is in $[0,1]$, then the normalized $P_{opt}$ is also in $[0,1]$. Therefore, for any learner, it performs better than random predictor only if the ${P_{opt}}$ is greater than 0.5

As similar to the above three evaluation metrics, {\it Precision, Recall, F1-score}, we use $20\%$ of all effort when computing $P_{opt}$, which is consistent to Yang et al.'s work~\cite{yang2016effort}. Note that in this study, in addition to $P_{opt}$ and $ACC$(it's actually recall) that used in Yang et al's work\cite{yang2016effort}, we include 
$Precision$ and $F1$ measures and they provide more insights about all the learners evaluated in the study from very different perspectives, which will be shown in the next section.




