\section{Method}


\begin{table}[!htp]
\caption{Change Metrics Used in Our Defect Data Sets.}\label{tab:CM}
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|l}
\hline
\rowcolor{lightgray}
Metric & Description  \\\hline
NS & Number of modified subsystems~\cite{mockus2000predicting}.\\
ND & Number of modified directories~\cite{mockus2000predicting}. \\
NF & Number of modified directories~\cite{nagappan2006mining}. \\
Entropy & Distribution of the modified code across each file~\cite{d2010extensive,hassan2009predicting}.\\\hline
LA & Lines of code added~\cite{nagappan2005use}.\\
LD & Lines of code deleted~\cite{nagappan2005use}.\\
LT & Lines of code in a file before the current change~\cite{koru2009investigation}.\\\hline
FIX & Whether or not the change is a defect fix~\cite{guo2010characterizing,yin2011fixes}. \\\hline
NDEV & Number of developers that changed the modified files~\cite{matsumoto2010analysis}. \\
AGE & The average time interval between the last and the current \\
    & change~\cite{graves2000predicting}. \\
NUC & The number of unique changes to the modified files~\cite{d2010extensive,hassan2009predicting}. \\ \hline
EXP & The developer experience in terms of number of changes~\cite{mockus2000predicting}.\\
REXP & Recent developer experience~\cite{mockus2000predicting}. \\
SEXP & Developer experience a the subsystem~\cite{mockus2000predicting}. \\\hline

\end{tabular}}
\end{table}

\subsection{Unsupervised Learner} \label{unsupervised}
In this section, we describe the effort-aware 
just-in-time(JIT) unsupervised defect learner proposed by Yang et al. \cite{yang2016effort},
which serves as a baseline method in this study.

As described by Yang et al.\cite{yang2016effort},  their simple unsupervised
 defect learner is built on change metrics, which is shown in \tab{CM}. These
 14 different change metrics can be divided into 5 dimensions\cite{kamei2013large}:
 \bi
 \item Diffusion: NS, ND, NF and Entropy.
 \item Size: LA, LD and LT.
 \item Purpose: FIX.
 \item History: NDEV, AGE and NUC.
 \item Experience: EXP, REXP and SEXP.
 \ei 
The {\it diffusion} dimension characterizes how a change is 
distributed at different levels of granularity.
As discussed by Kamei et al.~\cite{kamei2013large},
a highly distributed change is harder to keep track
and more likely to introduce defects. The {\it size} dimension
characterize the size of a change, where it's believed that
the software size is positively related to defect proneness
~\cite{nagappan2005use,koru2009investigation}. Yin et al.~\cite{yin2011fixes}
report that the bug-fixing process can also introduce new bugs. 
Therefore, the {\it Fix} metric could be used as a defect evaluation metric.
The {\it History} dimension includes some historical information
about the change, which has been proven to be a good defect indicator.
For example, Matsumoto et al. ~\cite{matsumoto2010analysis} find that the files
previously touched by many developers are likely to contain more defects.
The {\it Experience} dimensions describe the experience of software programmers
for the current change because Mockus et al.~\cite{mockus2000predicting} show that
more experienced developers are less likely to introduce a defect. More details
about these metrics can be found in ~\cite{kamei2013large}.

In Yang et al.'s study, for each change metric $M$, 
they build an unsupervised learner that ranks all the changes
based on the corresponding value of $\frac{1}{M(c)}$in descendant order, 
where $M(c)$ is the value of the selected change metric for each change $c$.
Therefore, the changes with smaller change metric values will ranked higher.
Totally, for each project, there will be 12 simple
unsupervised learners~(LA and LD are excluded), 
 
 
%  The literature show that these factors perform well to in defect prediction
% \cite{mockus2000predicting,nagappan2006mining,nagappan2005use,
% koru2009investigation, hassan2009predicting,guo2010characterizing,
% matsumoto2010analysis,graves2000predicting,d2010extensive,kamei2013large}. 
% In particular, Kamei et al.\cite{kamei2013large}

\subsection{Supervised Learner}
To further evaluate the unsupervised learner, we selected some supervised learners
that already used in Yang et al.'s work. 

As reported in both Yang et al.'s~\cite{yang2016effort}
and Kamei et al.'s~\cite{kamei2013large} work, linear regression model (EALR) outperforms
all other supervised learners for effort-aware JIT defect prediction.
For the linear regression learner, instead of predicting $Y(x)$,
we predict $\frac{Y(x)}{Effort(x)}$, where $Y(x)$ indicates whether this change
is a defect or not($1$ or $0$) and $Effort(x)$ represents the effort required 
to inspect this change. Note that this is the same method to 
build EALR as Kamel et al. ~\cite{kamei2013large}.

In defect prediction literature, KNN, J48 and Random Forests methods are simple 
yet widely used to as defect learners and have been proven to perform, if not best, quite
well~\cite{lessmann2008benchmarking,menzies2007data,hall2012systematic}. These three
learners are also used in Yang et al's study. For these
supervised learners, $Y(x)$ was used as the dependant variable. 
For KNN method, we set $K=8$. To fully reproduce
Yang et al.'s results,  all the settings in this paper strictly follow
their work~\cite{yang2016effort} without further explanation.


\subsection{OneWay Learner}\label{OneWay}
% \wei{The following text probabily need to move to motivation part}


As described in section \ref{unsupervised}, given a new project,
Yang et al.'s unsupervised method actually builds 12
learners for each project according to each predictor(change metric).
However, without using some validation data and ground truth labels to test,
we don't know which simple unsupervised learner works well for this project.
Generally, there are two situations need to consider, where

\bi
\item all 12 learners actually perform better than supervised learners.
\item or only some learners actually perform better than supervised learners.
\ei

In the first situation, we have the confidence that there's no need to
care about choosing which change metric to build unsupervised learners on.
Simply choosing one out of those 12 metrics and building a simple
learner on it will work quite well for JIT effort-aware defect prediction. 
However, in the second situation, we have to come up a way to
figure out how to select good predictors. Otherwise, randomly selecting
one feature to build an unsupervised learner won't guarantee the performance.
In other words, without any training data or supervised data, we can't even
tell which situation fits our new project.


Actually, based on our our preliminary experiment results
shown in the following section, for those six projects
investigated by Yang et al, some of those 12 unsupervised
learners do perform worse than supervised learners. 
That means we can't say which simple unsupervised learner actually
works for the new project before doing testing. In this case,
we need to select the proper metric to build unsupervised learner.



To make simple unsupervised learner actually work in practice,
we propose {\it OneWay} learner, which is supervised learner built on
the idea of simple unsupervised learner. The pseudocode for OneWay
is shown in Algorithm~\ref{alg:OneWay}. In the following description,
We use the superscript numbers to denote the line number of pseudocode. 

The general idea of {\it OneWay} is to use the best metric obtained
from the supervised feature pruning process to build a single feature
unsupervised learner. Specifically, {\it OneWay} firstly builds simple
unsupervised learners for each metric on training data$^{L3-L4}$, then
evaluate each of those learners in terms of evaluation metrics$^{L5-L6}$,
like Popt, ACC, Precision and F. After that, if the desirable
evaluation goal is set, the metric which performs best on the corresponding
evaluation goal is returned as the best metric; otherwise, the metric
which gets the highest mean score over all evaluation metrics is returned$^{L13-L22}$.
Finally, a simple unsupervised learner like Yang et al's. is built only
on such best metric$^{L10}$.




 
\begin{algorithm}[!htp]
\small
\caption{Pseudocode for OneWay}
\begin{algorithmic}[1]
\Require $\mathit{data\_train}$, $\mathit{data\_test}$, $\mathit{eval\_goal} \in \{\mathit{F,Popt,ACC,Precision,...}\}$
\Ensure $\mathit{result}$
\vspace{2mm}
\Function{OneWay}{$\mathit{data\_train}$, $\mathit{data\_test}$, $\mathit{eval\_goal}$}
  \State $\mathit{all\_scores} \gets$ NULL
  \For{$\mathit{metric}$ in $\mathit{data\_train}$}
      \State $\mathit{learner}\gets$ buildUnsupervisedLearner($\mathit{data\_train}$, $\mathit{metric}$)
      \State $\mathit{scores} \gets $evaluate($\mathit{learner}$)\\
      \quad \qquad //$\mathit{scores}$ include all evaluation goals, e.g., $\mathit{Popt,ACC,...}$
      \State $\mathit{all\_scores}$.append($\mathit{scores}$)
  \EndFor
  \State $\mathit{best\_metric} \gets$ pruneFeature($\mathit{all\_scores}$, $\mathit{eval\_goal}$)
  \State $\mathit{result} \gets$ buildUnsupervisedLearner($\mathit{data\_test}$, $\mathit{best\_metric}$)
  \State \Return $\mathit{result}$
\EndFunction
\Function{pruneFeature}{$\mathit{all\_scores}$, $\mathit{eval\_goal}$}
  \If{$\mathit{eval\_goal}$ == NULL}
    \State $\mathit{ mean\_scores} \gets$ getMeanScoresForEachMetric($\mathit{all\_scores}$)
    \State $ \mathit{best\_metric} \gets$ getMetric(max($\mathit{mean\_scores}$)) 
    \State \Return $\mathit{best\_metric}$
   \Else
     \State $\mathit{best\_metric} \gets$ getMetric(max($\mathit{all\_scores[``eval\_goal"]}$))
     \State\Return $\mathit{best\_metric}$
  \EndIf
\EndFunction
\end{algorithmic}
\label{alg:OneWay}
\end{algorithm}




