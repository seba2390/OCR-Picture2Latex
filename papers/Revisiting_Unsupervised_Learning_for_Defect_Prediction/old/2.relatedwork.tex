\section{Background and Related Work}

\subsection{Traditional Defect Prediction}
In traditional defect prediction, we measure the complexity of
software project using McCabe metrics, Halstead's effort metrics and  CK object-oriented code mertics~\cite{kafura1987use,chidamber1994metrics,mccabe1976complexity,halstead1977elements} at
a coarse granularity, like file or package level. It usually happens after the project has completed. With the collected data instances as well as the corresponding labels(defective or non-defective), we can  build defect prediction models using supervised machine learning algorithms such as Decision Tree, Random Forests, SVM, Naive Bayes and Logistic Regression\cite{khoshgoftaar2001modeling,khoshgoftaar2003software,khoshgoftaar2000balancing,menzies2007data,lessmann2008benchmarking,hall2012systematic}. After that, such trained defect predictor can be applied
to predict the defects on future projects. This type of defect prediction, where training data and testing data are from the same project, is referred as within-project defect prediction(WPDP).

However, for a new software project or a project with limited historical data, researchers
proposed to conduct cross-project defect prediction(CPDP), where the training data is from a different software project with the same metric measured as the testing project. Turhan et al. proposed the nearest neighbor method for CPDP~\cite{turhan2009relative}, where the most similar
training data to the testing data is select for building the model. However the performance of this method is still worse than WPDP. Transfer learning  techniques were proposed  in ~\cite{nam2013transfer, ma2012transfer}, both of these techniques achieve comparable performance with WPDP. Zhang et al.~\cite{zhang2014towards} proposed to build a universal model instead of building models
for each individual target project. They cluster projects based on the similarity of the distribution of 26 predictors, and derive the rank transformations using quantiles of predictors for a cluster. In this way, this universal model was fitted on data sets from 1398 open source projects. Their results show that the universal model obtains prediction performance comparable to WPDP models~\cite{zhang2014towards}. Recently, Nam et al.~\cite{nam2015clami} have proposed
to use unsupervised learning methods, CLA and CLAMI, for defect prediction  on unlabeled data sets. The key idea of
the CLA/CLAMI approaches is to label an unlabeled data set by using the magnitude of metric values~\cite{nam2015clami}. The intuition of this method is based on the empirical observation that higher complexity causes more defect-proneness~\cite{menzies2007data,rahman2013and}. They report that CLA/CLAMI acheive 0.636 and 0.723 F1-measure and AUC, respectively. 

To make defect prediction more feasible in practice, Nam et al.~\cite{nam2015heterogeneous}consider a situation where the data extracted from the
target project has different sets of metrics from any available training data. In this case,
current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. They proposed a  heterogeneous defect prediction method, which conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Experimental results  from 28 subjects show that about $68\%$ of predictions outperform or are comparable to WPDP with statistical significance~\cite{nam2015heterogeneous}.


\subsection{Just-In-Time Defect Prediction}
In traditional defect prediction has some drawbacks such as prediction at a coarse granularity and started at very late stage of software development circle~\cite{kamei2013large},wheras in JIT defect prediction paradigm, the models use change metrics generated from each change as the predictors, which
could easily help developers to narrow down the code for inspection and JIT defect prediction could be conducted right before developers commit the change. JIT defect prediction becomes more practical method for practitioners to carry out.

Mockus et al.~\cite{mockus2000predicting} conduct the first study to predict 
software failures on 5ESS software updates(a telecommunication system)
by using logistic regression on 
data sets consisted of change metrics of the project. Kim et al.
further evaluate the effectiveness of change metrics  on open source projects in ~\cite{kim2008classifying}, where they propose to apply support vector machine to build a defect predictor based on software change metrics, where they achieved $78\%$ accuracy and $60\%$ recall on average.Since the training data might
not available when building the defect predictor, Fukushima et al.~\cite{fukushima2014empirical} introduced
cross-project models into JIT defect prediction. Their results showed that using data from other projects to build JIT defect predictor is feasible. 

However, all the previous JIT defect prediction studies do not consider the efforts required to inspect the predicted defect-introducing changes. Kamei et al.~\cite{kamei2013large} take the effort into account and conduct a large-scale study on the effectiveness of JIT defect prediction, where they claim that using $
20\%$ of efforts required to inspect all changes, their modified linear regression model could
detect $35\%$ defect-introducing changes. Inspired by Menzies et al's ManualUp model(i.e., small size of modules inspected first)~\cite{menzies2010defect}, Yang et al. propose to build $12$ unsupervised learners by sorting the reciprocal values of $12$ different change metrics on each testing data set in descendant order. They
report that the experimental results from six open source projects
show that with $20\%$ efforts, many
unsupervised learners perform better than the state-of-art supervised learner.

Yang et al.'s method is so simple and might benefit a lot for defect prediction if working properly. However, there some issues
when applying to actual project. Firstly, since these unsupervised learners are built directly on testing data set, this indicates that there is no way to tell which learner(s) will perform better than supervised learners or all of them work equally same. 
If all of these learners have similar performance, then we have a strong confidence to select only one, instead of all of them, to apply on future projects. Otherwise, there should be a way to figure out which learner to select. Secondly, in yang's study, they only evaluate learners for two evaluation measures and there is no way to show how these unsupervised
learners perform in terms of different evaluation measures. Without any training data, how could we believe that those learners would meet our evaluation goals. Unfortunately, Yang et al. did not consider these issues in their work~\cite{yang2016effort}. 






