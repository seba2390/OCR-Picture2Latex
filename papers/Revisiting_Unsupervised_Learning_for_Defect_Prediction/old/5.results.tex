\section{Empirical Results}
In this section, we present the experimental results to investigate how
simple unsupervised learners work in practice and evaluate the performance
of the proposed method, {\it OneWay}, compared with supervised and unsupervised learners.

Before we start off, we need a sanity check to see if we can fully 
reproduced Yang et al.'s results in ~\cite{yang2016effort}. Since 
there might be some details missed in boxplot figures, we prefer to
compare performance scores. In~\cite{yang2016effort}, Yang et al.
provide the median values of $P_{opt}$ and $Recall$ for the EALR
model and the best two simple unsupervised models, LT and AGE, from
time-wise cross evaluation. Therefore, we use those numbers to check
our results. Note that only EALR and unsupervised learners need to implement from scratch,
where some differences might introduce if not 
properly follow Yang et al.'s work~\cite{yang2016effort}.

\begin{table}[]
    \centering
    \caption{ Comparison in $P_{opt}$: Yang's method(A) vs. Our implementation(B)}
    % \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|c c| c c| c c}

    \hline
        \multirow{2}{*}{Project} &
        \multicolumn{2}{c|}{EALR} &
        \multicolumn{2}{c|}{LT} &\multicolumn{2}{c}{AGE} \\
        \cline{2-7}
        &A & B& A&B&A&B \\
        \hline
        Bugzilla & 0.59 & 0.59 &0.72 &  0.72& 0.67  & 0.67 \\
        Platform & 0.58 & 0.58 & 0.72 & 0.72 & 0.71 & 0.71\\
        Mozilla & 0.50 &0.50 &0.65 &0.65 &0.64&0.64\\
        JDT & 0.59 & 0.59& 0.71 &0.71 & 0.68  & 0.69\\
        Columba & 0.62 & 0.62& 0.73 & 0.73 & 0.79  & 0.79 \\
        PostgreSQL & 0.60 &0.60&0.74&0.74&0.73 &0.73\\ \hline
        Average & 0.58 & 0.58 & 0.71 & 0.71 & 0.70 &0.70 \\ \hline
    \end{tabular}
    % }
    \label{tab:comp_Popt}
\end{table}



\begin{table}[]
    \centering
    \caption{Comparison in $Recall$ : Yang's method(A) vs. Our implementation(B)}
    % \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|c c| c c| c c}
    \hline
        \multirow{2}{*}{Project} &
        \multicolumn{2}{c|}{EALR} &
        \multicolumn{2}{c|}{LT} &\multicolumn{2}{c}{AGE} \\
        \cline{2-7}
        &A & B& A&B&A&B \\
        \hline
        Bugzilla & 0.29 & 0.30 &0.45 &  0.45& 0.38  & 0.38 \\
        Platform & 0.31 & 0.30 & 0.43 & 0.43 & 0.43 & 0.43\\
        Mozilla & 0.18 &0.18 &0.36 &0.36 &0.28&0.28\\
        JDT & 0.32 & 0.34& 0.45 &0.45 & 0.41  & 0.41\\
        Columba & 0.40 & 0.42& 0.44 & 0.44 & 0.57  & 0.57 \\
        PostgreSQL & 0.36 &0.36&0.43&0.43&0.43 &0.43\\ \hline
        Average & 0.31 & 0.32 & 0.43 & 0.43 & 0.41 & 0.41 \\ \hline
    \end{tabular}
    % }
    \label{tab:comp_Recall}
\end{table}


As shown in \tab{comp_Popt} and \tab{comp_Recall}, for unsupervised learners,
LT and AGE, we get the exactly same performance scores on all projects in terms
of {\it Reall} and ${\it P_{opt}}$. This is reasonable because unsupervised learners
are very straightforward and easy to implement. For the supervised learner, EALR, 
these two implementations do not have differences in ${\it P_{opt}}$, while the maximum
difference in  {\it Recall} is only ${0.02}$. Since the differences are quite small,
then we believe that our implementation reflect the details about EALR and unsupervised
learners in~\cite{yang2016effort}. 

For other supervised learners used in this study, like {\it J48, IBk}, and {
\it RandomForest}, we use the same algorithms from Weka package and set the same parameters as in~\cite{yang2016effort}.

\textbf{RQ1: Do all simple unsupervised learners perform better than the best supervised learner?}


To answer this question, we build 4 supervised learners and 12
unsupervised learners on the six project data sets using incremental learning
method as described in Section~\ref{exp_design}. 
% For each project, every 6 consecutive
% months data is used for one experiment, where the first 2 months  data are used train
% the supervised defect learners and the last two 2 months data is used as 
% the test data to evaluate the performance for both supervised and unsupervised learners.


\fig{sup_unsup} shows the boxplot of {\it Recall}, {\it $P_{opt}$}, {\it F1} and {\it Precision} for
supervised learners and unsupervised learners on 6 data sets. For each learner, the boxplot shows the
25th percentile, median and 75 percentile  values for one data set. The horizontal dashed lines indicate
the median of best supervised learners, which is to help visualize the median differences between unsupervised
learners and supervised learners.  


\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=.45\textwidth,height=2.8in]{pic/hist-cv-tw-Sup-Unsup-ACC.pdf}\includegraphics[width=.45\textwidth,height=2.8in]{pic/hist-cv-tw-Sup-Unsup-Popt.pdf}
    \includegraphics[width=.45\textwidth,height=2.8in]{pic/hist-cv-tw-Sup-Unsup-F1.pdf}\includegraphics[width=.45\textwidth,height=2.8in]{pic/hist-cv-tw-Sup-Unsup-PREC.pdf}
\caption{Performance comparisons among supervised and unsupervised learners over 6 projects(from top to bottom are  bugzilla, platform, mozilla, jdt, columba, postgres). } 
\label{fig:sup_unsup}
\end{center}
\end{figure*}

The colors of the box indicate
the significant difference between learners. The blue color represents that the corresponding learner
is significantly {\it better} than the best supervised learner according to Wilcoxon signed-rank, where the
BH corrected p-value is less than 0.05 {\it and} the magnitude of the difference between these two learners is NOT trivial
according to Cliff's delta, where $|\delta| \geq 0.147$. The black color represents that the corresponding learner is not significantly {\it better} than the best supervised learner {\it or} the magnitude of the difference between these two learners is trivial, where $|\delta| \leq 0.147$. The red color represents that the corresponding learner is significantly {\it worse} than the best supervised learner {\it and} the  magnitude of the difference between these two learners is NOT trivial.

From \fig{sup_unsup},  we can clearly see that not all unsupervised
learners perform statistically better than the best supervised learner
across all different evaluation metrics. Specifically, for {\it Recall}, 
on one hand, there are only $\frac{2}{12}$,
$\frac{3}{12}$, $\frac{6}{12}$, $\frac{2}{12}$, $\frac{3}{12}$ and $\frac{2}{12}$ of
total unsupervised learners perform statistically {\it better} than
the best supervised learner on 6 data sets, respectively.
On the other hand, there are $\frac{6}{12}$, $\frac{6}{12}$, $\frac{4}{12}$, $\frac{6}{12}$,
$\frac{5}{12}$ and $\frac{6}{12}$ of
total unsupervised learners perform statistically {\it worse} than the best supervised learner on 6 data sets, respectively. This indicates that (1) about $50\%$ unsupervised learners actually 
perform worse than the best supervised learner on any data set;
(2) without any prior knowledge,  we can't even know which
unsupervised learner(s) actually work on the test data. The similar trend as {\it Recall}
can be found in $ P_{opt}$ as well.


For {\it F1}, our observation is that only LT on bugzilla and AGE on postgres
 perform statistically better than the best supervised learner. 
 Other than that, no unsupervised learner performs better on any data set. Furthermore, surprisingly,
 no unsupervised learner works significantly better than the best supervised learner on any 
data sets in terms of {\it Precision}. As we can see,{\it RandomForest} perform well on all six data sets.
This suggests that unsupervised learners have very low
precision for effort-ware JIT defect prediction and can't be deployed to the business situation where 
demanding a high precision.



\begin{figure*}[htbp]
\begin{center}
 \includegraphics[width=.45\textwidth,height=3in]{pic/hist-cv-tw-Unsup-OneWay-ACC.pdf}\includegraphics[width=.45\textwidth,height=3in]{pic/hist-cv-tw-Unsup-OneWay-Popt.pdf}
 \includegraphics[width=.45\textwidth,height=3in]{pic/hist-cv-tw-Unsup-OneWay-F1.pdf}\includegraphics[width=.45\textwidth,height=3in]{pic/hist-cv-tw-Unsup-OneWay-PREC.pdf}
\caption{Performance comparisons between the proposed {\it OneWay} learner and unsupervised learners over 6 projects(from top to bottom are  bugzilla, platform, mozilla, jdt, columba, postgres).}
\label{fig:unsup_oneway}
\end{center}
\end{figure*}


Overall, for a given data set, no one specific unsupervised learner works 
better than the best supervised learner across all evaluation
metrics; For a given evaluation measure, most unsupervised learners
didn't perform better across all data sets.
There are two exceptions, LT and AGE for both {\it Recall} and ${P_{opt}}$. 
But, we know that after evaluating all the learners with the ground truth.
Before, the unsupervised learning nature won't allow us know anything about the learner's performance.
The above results indicate that:

\vskip 1ex
 \begin{myshadowbox}
    Not all simple unsupervised learners perform better than supervised learners for each project and for different evaluation measures.
 \end{myshadowbox}

\textbf{RQ2: Does {\it OneWay} perform  better than unsupervised learners?}

To answer this question, we compare {\it OneWay} learner with 12
unsupervised learners. All these learners are tested on on the six project data 
sets using the same experiment scheme as we did in RQ1.


\begin{figure*}[htbp]
\begin{center}
 \includegraphics[width=.25\textwidth,height=3in]{pic/hist-cv-tw-Sup-OneWay-ACC.pdf}\includegraphics[width=.25\textwidth,height=3in]{pic/hist-cv-tw-Sup-OneWay-Popt.pdf}\includegraphics[width=.25\textwidth,height=3in]{pic/hist-cv-tw-Sup-OneWay-F1.pdf}\includegraphics[width=.25\textwidth,height=3in]{pic/hist-cv-tw-Sup-OneWay-PREC.pdf}
\caption{Performance comparisons between the proposed {\it OneWay} learner and supervised learners over 6 projects(from top to bottom are  bugzilla, platform, mozilla, jdt, columba, postgres).}
\label{fig:sup_oneway}
\end{center}
\end{figure*}

\fig{unsup_oneway} shows the boxplot for the performance distribution of
unsupervised learners and the proposed
{\it OneWay} learner on 6 data sets across 4 evaluation measures. The horizontal 
dashed line denotes the median value of {\it OneWay}. Firstly, according to {\it Recall},
only one unsupervised learner, LT, outperforms {\it OneWay} in $\frac{4}{6}$ data sets. However, {\it OneWay}
significantly outperform$\frac{9}{12}$, $\frac{9}{12}$, $\frac{9}{12}$, $\frac{10}{12}$,
 $\frac{8}{12}$ and $\frac{10}{12}$ of total unsupervised learners on 6 data sets, respectively.
 This observation indicates that {\it OneWay} works significantly better 
 than almost all learners on all 6 data sets in terms of {\it Recall}.
 
 Similarly, we observe that only LT learner works better than {\it OneWay} in
$\frac{3}{6}$ data sets in terms of {\it $P_{opt}$} and AGE outperforms {\it OneWay} only on platform data set.
For the remaining experiments, {\it OneWay} perform better than all the other learners(usually,9 out of 12 learners). In addition, according to {\it F1}, only three unsupervised learners EXP/REXP/SEXP 
perform better than {\it OneWay} on mozilla data set and
LT learner just perform as well as {\it OneWay} and does not have any advantage over{\it OneWay}. The same finding can be observed in {\it Precision} measure.

To get more insights, \tab{unsu_oneway} provides the median values of the best unsupervised learner with {\it OneWay}
for each evaluation measure on all 6 data sets. In that table, for each evaluation measure,
the number in blue background indicates that the best unsupervised learner has a large advantage
over {\it OneWay} according to the cliff's Delta; Similarly, the light blue background means moderate advantage
and the gray background means small advantage. 

From \tab{unsu_oneway}, we observe that in total 24 experiments on all evaluation measures, 
none of these best unsupervised learners outperform {\it OneWay} with a large advantage according
to the cliff's Delta. Specifically,  according to {\it Recall} and $P_{opt}$,
even though LT outperforms {\it OnWay} on four and three data sets, respectively. However, all of
these advantage are small. Meanwhile, REXP and EXP have a moderate improvement
over {\it OneWay} on one and two data sets for {\it F1} and {\it Precision}, respectively.
In terms of the average scores, the maximum magnitude of the difference between the best unsupervised 
learner and {\it OneWay} is $0.02$. In other words, {\it OneWay} is comparable with the best unsupervised
learners on all data sets for all evalution measures.




\begin{table}[]
    \centering
    \caption{Best Unsupervised Learner(A) vs. OneWay(B)}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|c c| c c| c c| c c}
    \hline
        \multirow{2}{*}{Project} &
        \multicolumn{2}{c|}{{\it Recall}} &
        \multicolumn{2}{c|}{$P_{opt}$} &
        \multicolumn{2}{c|}{{\it F1}} &
        \multicolumn{2}{c}{{\it Precision}}\\
        \cline{2-9}
        &A(LT) & B & A(LT)&B&A(REXP)&B &A(EXP)&B \\
        \hline
        Bugzilla &\colorbox{gray!50}{0.45} & 0.36 &\colorbox{gray!50}{0.72} &  0.65& 0.33  & 0.35 & 0.40 & 0.39  \\
        Platform & \colorbox{gray!50}{0.43} & 0.41 & \colorbox{gray!50}{0.72} & 0.69 & 0.16 & 0.17 &\colorbox{gray!50}{0.14} &0.11\\
        Mozilla & \colorbox{gray!50}{0.36} &0.33 &\colorbox{gray!50}{0.65} &0.62 &\colorbox{blue!20}{0.10}&0.08 &\colorbox{blue!20}{0.06} &0.04\\
        JDT & \colorbox{gray!50}{0.45}& 0.42& 0.71 & 0.70  & 0.18 & 0.18 &\colorbox{blue!20}{0.15} &0.12\\
        Columba & 0.44& 0.56 & 0.73 & 0.76 & 0.23 & 0.32 &0.24 &0.25\\
        PostgreSQL & 0.43 &0.44&0.74&0.74&0.24 & 0.29 &\colorbox{gray!50}{0.25} &0.23\\ \hline
        Average & 0.43 &0.42 &0.71& 0.69 & 0.21 & 0.23 &0.21 &0.19\\ \hline
    \end{tabular}
    }
    \label{tab:unsu_oneway}
\end{table}


Overall, we find that (1) no one unsupervised learner significantly
outperform {\it OneWay} on all data sets for a given evaluation measure;(2)usually, {\it OneWay}
works as well as the best unsupervised learner and has significant better performance than almost
all unsupervised learners on all data sets for all evaluation measures. Therefore, the above results suggest:
\vskip 1ex
 \begin{myshadowbox}
    As a simple supervised learner, {\it OneWay} has pretty stable and good performance on
    all data sets for all evaluation measures and
    it performs  better than unsupervised learners for effort-aware JIT defect prediction. 
 \end{myshadowbox}


\textbf{RQ3: Does {\it OneWay} perform  better than supervised learners?}

To answer this question, we compare {\it OneWay} learner with 4
supervised learners, including EALR, Randomforest, J48 and IBk. 
EALR is considered to be the state-of-art learner for effort-aware
JIT defect prediction and all the other three learners are widely used 
in defect prediction literature over past years. We evaluate all these
learners on the six project data 
sets using the same experiment scheme as we did in RQ1.


From \fig{sup_oneway}, we have the following observations. Firstly, 
the performance of {\it OneWay} is significantly better than all
these four supervised learners in terms of {\it Recall} and ${P_{opt}}$
on all 6 data sets. Also, EALR works better than Randomforest, J48 and IBk,
which is consistent to Kamei et al's finding~\cite{kamei2013large}. Secondly,
according to {\it F1}, randomforest and IBk perform slightly better than {\it OneWay}
in 2 out of 6 data sets. For most cases, {\it OneWay} has a similar performance
to these supervised learners and there is no much difference between them. However,
when reading {\it Precision} scores, we find that, in most cases, supervised
learners perform significantly better than {\it OneWay}. Specifically, Randomforest,
J48 and IBk outperform {\it OneWay} on all data sets and EALR is better on 3 data sets. 
This finding is consistent to the observation in RQ1 where all unsupervised learners
perform worse than supervised learnrs. Since {\it OneWay} is built on the idea of
unsupervised learners, it explains why {\it OneWay} does worse in {\it Precision}.
As an aside, EALR is never
the best supervised learner in any case, but it's still better or simiar to {\it OneWay}.

\begin{table}[]
    \centering
    \caption{Best Supervised Learner(A) vs. OneWay(B)}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|c c| c c| c c| c c}
    \hline
        \multirow{2}{*}{Project} &
        \multicolumn{2}{c|}{{\it Recall}} &
        \multicolumn{2}{c|}{$P_{opt}$} &
        \multicolumn{2}{c|}{{\it F1}} &
        \multicolumn{2}{c}{{\it Precision}}\\
        \cline{2-9}
        &A(EALR) & B & A(EALR)&B&A(IBk)&B &A(RF)&B \\
        \hline
        Bugzilla & 0.30 & 0.36 &0.59 &  0.65& 0.30  & 0.35 & \colorbox{blue!50}{0.59} & 0.39  \\
        Platform & 0.30 & 0.41 & 0.58 & 0.69 & \colorbox{blue!20}{0.23} & 0.17 &\colorbox{blue!50}{0.38} &0.11\\
        Mozilla & 0.18 &0.33 &0.50 &0.62 &\colorbox{blue!50}{0.18}&0.08 &\colorbox{blue!50}{0.27} &0.04\\
        JDT & 0.34& 0.42& 0.59 & 0.70  & \colorbox{gray!50}{0.22} & 0.18 &\colorbox{blue!50}{0.31} &0.12\\
        Columba & 0.42& 0.56 & 0.62 & 0.76 & 0.24 & 0.32 &\colorbox{blue!50}{0.50} &0.25\\
        PostgreSQL & 0.36 &0.44&0.60&0.74&0.21 & 0.29 &\colorbox{blue!50}{0.69} &0.23\\ \hline
        Average & 0.32 &0.42 &0.58& 0.69 & 0.23 & 0.23 &0.46 &0.19\\ \hline
    \end{tabular}
    }
    \label{tab:unsup_oneway}
\end{table}

From \tab{unsup_oneway},  we have the following observation.
First of all, the maximum  difference in median values between 
EALR and {\it OneWay} is from $0.15$ and $0.14$, which are $83\%$ and $23\%$ improvements
over $0.18$ and $0.60$ on mozilla and postgreSQL data sets in terms of {\it Recall} and ${P_{opt}}$.
For both measures, {\it OneWay} improve the average scores by $0.1$ and $0.11$, which are
$31\%$ and $19\%$ improvement over EALR. Secondly, according to {\it F1}, IBk outperform
{\it OneWay}  on three data sets with a large, moderate and small advantage, respectively.
The largest difference in median is $0.1$. Finally, as we discuss before, the best 
supervised learner for {\it Recall}, RandomForest, have very large advantage over
{\it OneWay} on all data sets. The largest difference is $0.46$.


Overall, we conclude that (1) given one project data, no one supervised learner works
better than {\it OneWay} for all evaluation measures;(2) {\it OneWay} outperforms all
4 supervised learners in terms of {\it Recall} and $P_{opt}$, respectively. but it performs
significantly worse than Randomforest, J48 and IBk. Therefore, our answer to RQ3 is:
\vskip 1ex
 \begin{myshadowbox}
    {\it OneWay} performs significantly better than all 4 supervised learners in terms of {\it Recall}
    and $P_{opt}$, but not for {\it Precision}.
 \end{myshadowbox}










