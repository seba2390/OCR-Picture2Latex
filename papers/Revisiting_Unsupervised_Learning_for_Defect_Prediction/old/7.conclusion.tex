\section{Conclusion and Future Work}

In this paper, we revisit Yang et al.'s work~\cite{yang2016effort}
about using unsupervised learners for effort-ware just-in-time 
defect prediction and find that not all unsupervised learners
work better than supervised learners on 6 data sets for different evaluation
measures. This suggests that we can't randomly pick any one unsupervised learner
to perform effort-ware defect prediction and it's necessary to figure out
how to pick the best metric(s) even before building the unsupervised learner.
Therefore, we propose a supervised learner, {\it OneWay}, which automatically
selects the potential best one metric to build a simple learner with training data.
Our results indicate that, 
\bi
\item in terms of {\it Recall}, $P_{opt}$ and {\it F1},
{\it OneWay} outperforms all most unsupervised learners and supervised learners
on 6 data sets. The largest improvement is $83\%$ over EALR. 
\item it is not doing very well for {\it Precision}.
\ei
Since there are many domains and applications where a high-recall-and-low-precision
learner is required, therefore, we believe that {\it OneWay} can be deployed for practical
purpose.

This study opens the new reserach direction of applying simple supervised 
techniques to perform the defect prediction. As shown in this
study as well as Yang et al.'s work~\cite{yang2016effort}, instead of using
traditional machine learning algorithms like J48 and Randomforest, simply sorting
data according to one metric can be a good defect predictor model, at least for
effort-ware just-in-time defect prediction. Therefore, we recommend the future
defect prediction research should more focus on simple techniques.

For the future work, we plan to extend this study on other software projects,
especially those developed by the other programming languages. After that,
we plan to investigate some new change features to see if that helps improve
{\it OneWay}'s performance.




