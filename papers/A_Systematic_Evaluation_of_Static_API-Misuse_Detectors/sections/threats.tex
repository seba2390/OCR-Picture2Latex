%!TEX root = ../paper.tex

\section{Threats to Validity}
\label{sec:threats_to_validity}

%In this section, we discuss possible threats to the validity of our methodology and experiments. 

\InlineSec{Construct Validity}
Any detector's performance is dependent on its configuration. 
%it is possible that the detectors we compared against can perform better on our benchmark, given a different configuration. 
Due to the high effort of reviewing findings, we could not try different configurations for each detector.
However, to give each detector a fair chance, we used the optimal configurations reported in the respective publications.

Our study focuses on static misuse detectors.
Approaches based on dynamic analyses may perform differently and have unique strengths and weaknesses.
To enable dynamic analyses of the project versions in \MUBench, we would have to ensure that the respective code is executable (which requires a sufficient run-time environment, in addition to compile-time dependencies) and to provide example inputs for the execution.
It is unclear how to do this such that it results in a fair comparison of both static and dynamic techniques, without resorting to comparing apples to oranges.
In this work, we focused only on static approaches.

Our experiments focus on detectors that detect misuses in Java code.
Therefore, the results may not generalize to detectors for other languages.
We decided to focus on this subset of detectors, because the majority of approaches we identified in our survey targets Java.
To include detectors that target other languages, we would have to either migrate them to Java or build up additional datasets for the respective languages, both of which is outside the scope of this work.

\InlineSec{Internal Validity}
Reviewing the detectors' findings was done by three of the authors and was not blind (i.e., we knew the detectors we were reviewing findings for).
We could not do blind reviewing, because each approach has a distinct representation of usages and violations that cannot be anonymized.
Moreover, two of the authors of this work are among the original authors of \GROUMiner.
We did our best to review objectively.
To avoid bias, every finding was independently reviewed by two authors and for all findings of \GROUMiner, at least one review was done by an author who was not involved in the original work.

% \sa{Can we roughly quantify our review effort?}~\sn{perhaps using an average of 2min per review is fair? a lot of them took less but some took a bit and if we include discussion then 2min is very fair}~\sa{Taking the number of potential hits ($122 + 230 + 42$) times 2 reviews times 2min/review means $26.3h$ of review work in total. The efforts per detector are 4h for \Jadet, 7.8h for \GROUMiner, 5h for \Tikanga, and 9h for \DMMC. Should we put this as an argument for not involving the original authors?}~\sn{yeah i guess we can. I had a feeling that it took more time than this, but maybe I'm mistaken :D}~\sa{It took more time, because we rereviewed (parts of) the results several times after doing changes, but this is not workload somebody else would have to assess our final results. We also reviewed MuDetect, which is not in here anymore.}
%
While we did ask the original authors to confirm our assessment of the conceptual capabilities of their tools, we did not ask them to confirm the empirical results of our experiments.
We estimate that, including discussions to resolve disagreements, it required each reviewer on average \checkNum{2 minutes to verify whether a detector identified one of the known misuses in Experiments RUB and R} and \checkNum{5 minutes to verify whether a detector's finding identifies an actual misuse in \nameref{e2}}, where we needed to understand the respective code, check documentation, and sometimes also look into transitively called methods.
This amounts to \checkNum{24.8 hours of review effort} per reviewer, \checkNum{4 hours for \Jadet}, \checkNum{7.2 hours for \GROUMiner}, \checkNum{4.7 hours for \Tikanga}, and \checkNum{8.9 hours for \DMMC}.
We decided it is unreasonable to expect the original authors to invest this amount of time in verifying our assessments.
We do, however, publish all our review data~\cite{artifact-page} to allow them and others to revisit our decisions.

% For \Jadet, we measure a similar precision as in the original evaluation (10% vs 9-12%)
% For \Tikanga, we measure a better precision than in the original evaluation (11% vs 6%)
% For \DMMC, we measure a much worse precision than in the original evaluation (9% vs. 85%)

\InlineSec{External Validity}
There may be violation categories we miss in \MUC.
The \MUBench dataset may also not have enough examples of all violations.
This may impact the detectors' comparisons.
However, the existing \MUBench dataset is based on over 1,200 reports from state-of-the-art bug datasets as well as developer input~\cite{ANNN+16} and the results of two empirical studies on API usage directives.
Our survey of existing detectors' capabilities also includes \checkNum{12 detectors}.
This makes it unlikely that we miss a prevalent violation category.

Our dataset may not be representative of all possible real-world API misuses, especially, because we could only compile \checkNum{29 (52\%)} of the \checkNum{55 project versions} and had to exclude the misuses in the other versions from our experiments.
Compiling arbitrary versions of projects from the source control history of project is a challenging task.
We invested two full weeks work of one of the authors and additional 3 months work of a student, to include as many project versions as possible.
Still, loosing the examples for which we could not compile the respective project versions may bias the results of our experiments.

Ideally, our experiments would include thousands of misuses from a large number of projects and in each individual project version, to give us greater confidence in the generalizability of our results.
However, currently, there is no such dataset.
We invested several months of effort to collect and prepare \MUBench in its current state, to make a first step towards a large benchmark.
Now that the we have the infrastructure in place, it is straightforward to extend \MUBench with misuse examples from different sources.

We publish \MUPipe and \MUBench~\cite{mubench} and encourage others to extend the dataset and repeat our experiments, also with other detectors and detector configurations.
