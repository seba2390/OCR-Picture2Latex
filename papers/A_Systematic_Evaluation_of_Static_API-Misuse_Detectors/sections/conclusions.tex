%!TEX root = ../paper.tex

\section{Conclusions} % (fold)
\label{sec:conclusions}

API-misuse detectors help developers write better software by warning them about potential misuses in their code.
Despite the existence of many such detectors, there has been no attempt to systematically study types of API misuses and design detectors accordingly.
%
In this paper, we addressed this gap by creating \MUC, based on a dataset of \checkNum{100 misuses}.
By evaluating the conceptual capabilities of \checkNum{12 existing detectors} against \MUC, we identified shortcomings qualitatively.
We then developed an automated benchmark pipeline, \MUPipe, to empirically evaluate \checkNum{four existing detectors}.
Our results reveal that misuse detectors are practically capable of detecting misuses, when explicitly provided with correct usages to mine patterns from.
However, they suffer from extremely low precision and recall in a realistic application setting.
%
We identify \checkNum{four root causes} for false negatives, \checkNum{seven root causes} for false positives, and \checkNum{two general problems} with the design of detectors and the commonly-used evaluation setup.
These lead us to several observations on how to advance the state-of-the-art in API-misuse detection in future work.
We publish all our tooling and our dataset~\cite{mubench} to encourage other researchers to join us along this path.
