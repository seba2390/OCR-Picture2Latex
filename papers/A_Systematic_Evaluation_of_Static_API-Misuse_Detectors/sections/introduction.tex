%!TEX root = ../paper.tex

\IEEEraisesectionheading{\section{Introduction} % (fold) 
\label{sec:introduction}}

% Motivation

\IEEEPARstart{I}{ncorrect} usages of an Application Programming Interface (API), or \emph{API misuses}, are violations of (implicit) \emph{usage constraints} of the API. 
An example of a usage constraint is having to check that \code{hasNext()} returns \code{true} before calling \code{next()} on an \code{Iterator}, in order to avoid a \code{NoSuchElementException} at runtime.
Incorrect usage of APIs is a prevalent cause of software bugs, crashes, and vulnerabilities~\cite{MM13,SHA15,ANNN+16,FHMB+12,EBFK13,NKMB16,GIJA+12}.
While high-quality documentation of an API's usage constraints could help, it is often insufficient, at least in its current form, to solve the problem~\cite{DH09}.
For example, a recent empirical study shows that Android developers prefer informal references, such as StackOverflow, over official API documentation, even though the former promotes many insecure API usages~\cite{ABFKMS16}.
We confirm this tendency for a non-security API as well:
Instances of \code{Iterator} may not be used after the underlying collection was modified, otherwise they throw a \code{ConcurrentModificationException}.
Even though this constraint and the consequences of its violation are thoroughly documented, a review of the \checkNum{top-5\%} of \checkNum{2,854} threads about \code{ConcurrentModificationException} on StackOverflow shows that \checkNum{57\%} of them ask for a fix of the above misuse~\cite{artifact-page}.

Ideally, development environments should assist developers in implementing correct usages and in finding and fixing existing misuses.
In this paper, we focus on tools that identify misuses in a given codebase, specifically, those that automatically infer~API-usage specifications and identify respective violations through static code analysis.
We refer to these tools as \emph{static API-misuse~detectors}.

% What happened before...
There have been many attempts to address the problem of API misuse.
Existing static misuse detectors commonly mine \textit{usage patterns}, i.e., equivalent API usages that occur frequently, and report any anomaly with respect to these patterns as potential misuse~\cite{LZ05,L07,WZL07,RGJ07,NNP+09,AX09,TX09,TX09b,WZ11,MM13,NPVN16}.
The approaches differ in how they encode usages and frequency, as well as in the techniques they apply to identify patterns and violations thereof.
Despite the vast amount of work on API-misuse detection, API misuses still exist in practice, as recent studies show~\cite{LHXRM16,ABFKMS16}.
To advance the state of the art in API-misuse detection, we need to understand how existing approaches compare to each other, and what their current limitations are.
This would allow researchers to improve API-misuse detectors by enhancing current strengths and overcoming weaknesses.

%We need a classification of API-misuses to assess each detector's capabilities and to systematically address the problem space.
%And we need means to empirically evaluate and compare detectors.

In this work, we propose the \textit{API-Misuse Classification} (\MUC) as a taxonomy for API misuses and a framework to assess the capabilities of static API-misuse detectors.
In order to create such a taxonomy, we need a diverse sample of API misuses.
In our previous work, we described \MUBench, a dataset of \checkNum{90 API misuses} that we collected by reviewing \checkNum{over 1200 reports from existing bug datasets} and conducting a developer survey~\cite{ANNN+16}.
\MUBench provided us with the misuse examples needed to create a taxonomy.
To cover the entire problem space of API misuses, for this paper, we add further misuses to this dataset by looking at examples from studies on API-usage directives~\cite{DH09,METM12}. 
Using \MUC, we qualitatively compare \checkNum{12 existing detectors} and identify their shortcomings.
For example, we find that only few detectors detect misuses related to conditions or exception handling.
We confirm this assessment with the detectors' original authors.

The previous step provides us with a conceptual comparison of existing detectors.
We also want to compare these API-misuse detectors empirically, by both their precision and recall.
This is a challenging task, due to the different underlying mechanisms and representations used by detectors.
To enable this empirical comparison, we build \MUPipe, the first automated pipeline to benchmark API-misuse detectors.
Our automated benchmark leverages \MUBench, and the additional misuses we collect in this work, and creates an infrastructure on top of it to run the detectors and compare their results.
We perform three experiments based on \checkNum{29 real-world projects} and \checkNum{25 hand-crafted examples} to empirically evaluate and compare \checkNum{four state-of-the-art detectors}.
We exclude the \checkNum{other eight detectors} since \checkNum{two} rely on the discontinued Google Code Search~\cite{farewellGoogle}, \checkNum{five} target C/C++ code, and \checkNum{one} targets Dalvik Bytecode, while our benchmark contains Java misuses. 
%
In \nameref{e2}, we measure the precision of the detectors in a per-project setup, where they mine patterns and detect violations in individual projects from \MUBench.
%
In \nameref{e1}, we determine upper bounds to the recall of the detectors with respect to the known misuses in \MUBench.
We take the possibility of insufficient training data out of the equation, by providing the detectors with crafted examples of correct usages for them to mine required patterns.
%
Finally, in \nameref{e3}, we measure the recall of the detectors against both the \MUBench dataset and the detectors' own confirmed findings from \nameref{e2} using a per-project setup.

Our conceptual analysis shows many previously neglected aspects of API misuse, such as incorrect exception handling and redundant calls.
Our quantitative results show that misuse detectors are capable of detecting misuses, when provided with correct usages for pattern mining.
However, they suffer from extremely low precision and recall in a realistic setting. 
%
We identify \checkNum{four root causes} for false negatives and \checkNum{seven root causes} for false positives. 
Most importantly, to improve precision, detectors need to go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse, for example, by building probabilistic models to reason about the likelihood of usages in their respective context.
To improve recall, detectors need to obtain more correct usage examples, possibly from different sources, and to consider program semantics, such as type hierarchies and implicit dependencies between API usages.
%
These novel insights are made possible by our automated benchmark.
Our empirical results present a wake-up call, unveiling serious practical limitations of tools and evaluation strategies from the field.
Foremost, detectors suffer from extremely low recall---which is typically not evaluated. 
Moreover, we find that the application of detectors to individual projects does not seem to give them sufficient data to learn good models of correct API usage.

%For the first time, we systematically measure the recall of detectors and unveil their poor performance.
%This should call researchers to action; we provide our tooling and data to enable replication of experiments for additional detectors.

%Experiment_3 shows that all detectors have very low recall, likely because they cannot mine the required patterns from within single projects.
%This calls for new cross-project mining approaches.


% Contribution

In summary, this paper makes the following contributions to the area of API-misuse detection:
\begin{itemize}
  \item A taxonomy of API misuses, \MUC, which provides a conceptual framework to compare the capabilities of API-misuse detectors.
  \item A survey and qualitative assessment of \checkNum{12 state-of-the-art} misuse detectors, based on \MUC.
  \item A publicly available automated benchmark pipeline for API-misuse detectors, \MUPipe, which facilitates systematic and reproducible evaluations of misuse detectors.
  \item An empirical comparison of both recall and precision of~\checkNum{four existing misuse detectors} using \MUPipe.
  Our work is the first to compare different detectors on both a conceptual and practical level and, more importantly, the first to measure the recall of detectors, unveiling their poor performance.
  \item A systematic analysis of the root causes for low precision and recall across detectors, to call researchers to action.
\end{itemize}

Our benchmarking infrastructure is publicly available~\cite{mubench} and our artifact Web page~\cite{artifact-page} provides full details on our results.

%\todo{Use formulations from rebuttal to strengthen introduction?}

% How this is awesome, saves lifes, and advances the state-of-the-art

% section introduction (end)
