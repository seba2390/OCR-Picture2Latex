%!TEX root = ../paper.tex

\section{\MUPipe}
\label{sub:the_pipeline}

To systematically assess and compare API-misuse detectors, we built \MUPipe, a benchmarking pipeline for API-misuse detectors.
\MUPipe automates large parts of the experimental setup presented in \autoref{sec:benchmark} and facilitates the reproduction of our study.
It also enables adding new detectors to the comparison, as well as benchmarking with different or extended datasets, in the future.
We publish the pipeline~\cite{mubench} for future studies.


\subsection{Automation}

Following the idea of automated bug-detection benchmarks for C programs, such as~\aName{BugBench}~\cite{LLQ+05} and \aName{BegBunch}~\cite{CHKL+09}, we facilitate the benchmarking of multiple detectors on our misuse dataset with an evaluation pipeline.
\MUPipe automates many of our evaluation steps, such as retrieval and compilation of target projects, running detectors, and collecting their findings.
\MUPipe provides a command-line interface to control these steps.
We subsequently describe the pipeline steps we implemented to facilitate our evaluation.

\InlineSec{Checkout}
\MUPipe uses the recorded commit Id from \MUBench to obtain the source code of the respective project version.
It supports SVN and Git repositories, source archives (zip), as well as a special handling for the hand-crafted examples that come with \MUBench.

\InlineSec{Compile}
For every project version, \MUPipe first copies the entire project source code, the individual files containing known misuses, and the respective crafted correct usages for \nameref{e1} each into a separate folder.
It then uses the respective build configuration from the dataset to compile all Java sources to Bytecode.
After compilation, it copies the entire project Bytecode, the Bytecode of the individual files containing known misuses, and the Bytecode of the respective crafted correct usages each into a separate folder.
This way, we may provide the detectors with the source code or Bytecode of each of these parts individually.

\InlineSec{Detect}
For each detector, we also built a \emph{runner} to have a unified command-line interface for all detectors.
For every project version, \MUPipe invokes the detector with the paths to the respective source code and Bytecode.
%The runners use these to provide the right input to the respective detector and to output its findings. 
All detectors are invoked with the best configuration reported in their respective publication.
Apart from adding some accessor methods that allow us to obtain the detectors' output, all detector implementations were left unchanged.

%After the detection, the runners convert the detectors' output into a unified format for findings to facilitate the following validation step.
%For each finding, this format specifies the name of the file and the name of the method that the finding is in.
%In addition, the runners add tool-specific data that helps with validation (e.g., the detector's confidence value).

\InlineSec{Validation}
To help with the manual review of findings, \MUPipe automatically publishes experiment results to a review website~\cite{artifact-page}.
For every detector finding, the website shows the source code it is found in along with any metadata the detector provides, such as the violated pattern, the properties of the violation, and the detector's confidence.

For Experiments RUB and R, \MUPipe automatically filters potential hits, by matching findings to known misuses by file and method name.
On the review website, a reviewer sees the description of the known misuse as well as its fix, along with the set of potential hits that need to be reviewed.
For \nameref{e2}, \MUPipe shows all findings of the detector on the review site.

The review website allows reviewers to save an assessment and comment for each finding.
It also ensures at least two reviews for each finding, before automatically computing the experiment statistics, such as precision, recall, and Cohen's Kappa scores.

\subsection{Reproduction, Replication, and Extension}

\MUPipe comes with a Docker image, which allows running reproducible experiments across platforms, without the need to ensure a proper environment setup.
Its review website comes with a second Docker image, which allows serving it standalone.
Moreover, it is based on PHP and MySQL, such that it can be hosted on any off-the-shelf webspace.
The review website facilitates independent reviews, even when researchers work from different locations, while ensuring review integrity using authentication.
The website may also directly be used as an artifact to publish review results and experiment statistics.
\MUPipe defines a simple data schema for misuse examples to facilitate extensions of \MUBench.
It also provides a convenient Java interface as a Maven dependency to enable plugging in additional detectors for evaluation on the benchmark.
For further details on how~to~use~or extend \MUPipe, we refer the readers to our project website~\cite{mubench}.
