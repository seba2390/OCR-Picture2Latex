%!TEX root = ../paper.tex

\input{sections/table-run-stats}

\section{Experimental Setup}
\label{sec:benchmark}

%As we see in \autoref{tab:survey_evaluation}, the common practice for evaluating an API-misuse detector is to apply it to a small number of real-world projects and to manually review the top-ranked findings.
%The reviewers then classify findings into true positives, false positives, and a varying set of further categories, such as code smells or hints for improvement.
%This procedure has two inherent problems: (1) Since the classification criteria vary and are not always clearly documented, reproduction of the results is not straightforward and often even infeasible, and (2) Since the detectors are applied to different sets of projects, we cannot compare them fairly to one another.

In \autoref{sec:misuse-detectors-related-work}, we conceptually compared detectors' capabilities.
In this section, we describe the experimental setup we use to \textit{empirically} compare their capabilities.
We design three experiments, to measure both the detectors' precision and recall.
We build these experiments on \MUBench as a ground-truth dataset.
This enables us to compare all detectors on the same target projects and with respect to the same known misuses.

\InlineSec{Subject Detectors}
In this study, we focus on misuse detectors for Java APIs, because \MUBench contains examples of Java-API misuses.
Our survey identifies \checkNum{seven such detectors}.
We contacted the respective authors and got responses from all of them.
However, we learned that we cannot run \CARMiner{} and \Alattin{}, because they both depend on Google Code Search, a service that is no longer available~\cite{farewellGoogle}.
We exclude \DroidAssist{}, because its implementation only supports Dalvik Bytecode,\footnote{A bytecode format developed by Google, which is optimized for the characteristics of mobile operating systems (especially for the Android~platform).} while the examples in \MUBench are general Java projects, which compile to Java Bytecode.
%while our benchmark contains many other Java projects. 
This leaves us with four detectors \Jadet{}, \GROUMiner{}, \Tikanga{}, and \DMMC{}.

\InlineSec{Misuse Dataset}
We use \MUBench, described in~\autoref{sec:definitions}, to find targets for our evaluations.
While \GROUMiner{} works on source code, \Jadet{}, \Tikanga{}, and \DMMC{} require Java Bytecode as input.
Thus, we can only compare them on project versions for which we have both source code and Bytecode.
Since Bytecode is not readily available for most project versions in the dataset, we resort to compiling them ourselves by adding necessary build files and fixing any dependency issues.
We exclude \checkNum{26 project versions (47\%)} with compilation errors that we could not fix.
In the end, we have \checkNum{29 compilable project versions} and \checkNum{25 hand-crafted examples}, with \checkNum{64 misuses} in total, for our experiments.
Note that some project versions contain multiple misuses.
The last three rows in \autoref{tab:datasets} describe the subsets of this dataset that we use in the individual experiments.
We publish the dataset~\cite{mubench} for others to use in future studies.

\subsection{Experiment~P}
\label{e2}

We design \nameref{e2} to assess the precision of detectors.

\InlineSec{Motivation}
Past studies show that developers rarely use analysis tools that produce many false positives~\cite{FLLNSSLNSS02,BBCCFHHKME10,JS13}.
Therefore, for a detector to be adopted in practice, it needs a high precision.

\InlineSec{Setup}
To measure precision, we follow the most-common experimental setup we found in the literature (cf. \autoref{tab:survey-evaluations}).
First, we run detectors on individual project versions.
In this setting, they mine patterns and detect violations on a per-project basis.
Second, we manually validate the \checkNum{top-20} findings per detector on each version, as determined by the respective detector's ranking strategies.
We limit the number of findings, because it seems likely that developers would only consider a fixed number of findings, rather than all of a potentially very large number of findings.
Hence, the precision in a detector's top findings is likely crucial for tool adoption.
Also, we need to limit the effort of reviewing findings of multiple detectors on each project version.

\InlineSec{Dataset}
Since manually reviewing findings of all detectors on all project versions is infeasible, we sample \checkNum{five project versions}.
To ensure a fair selection of projects, we first run all detectors on all project versions.
For practical reasons, we timeout each detector on an individual project version after \checkNum{two hours}.
The run statistics are summarized in \autoref{tab:run-stats}.

\input{sections/table-run-findings-correlation}

\Jadet and \Tikanga fail on \checkNum{one project version} and \DMMC fails on \checkNum{four project versions}, since the Bytecode contains constructs that the detectors' respective Bytecode toolkits do not support.
\GROUMiner times out on \checkNum{eight project versions} and produces an error on \checkNum{one other version}.
We exclude any project version where a detector fails.

For the remaining \checkNum{15 versions}, we observe that the total number of findings correlates across detectors.
\autoref{tab:run-findings-correlation} shows that the pairwise correlation (Pearson's $r$) is strong ($\geq 0.75$) or medium ($\geq 0.5$) for all pairs of detectors, except for \Jadet and \GROUMiner ($r = 0.49$).
This means that either all detectors report a relatively large or a relatively small number of findings on any given project version.
We hypothesise that the total number of findings might be related to the detectors' ability to precisely identify misuses in a given project version.
Therefore, we sample project versions according to the average normalized number of findings across all detector.
We normalize the number of findings per detector on all project versions by the maximum number of findings of that detector on any project version.
We sample the two projects with the highest average normalized number of findings across all detectors (\aName{Closure}~\cite{closure} v319 and \aName{iText}~\cite{itext} v5091) and the two projects with the lowest average normalized number of findings across all detectors (\aName{JMRTD}~\cite{jmrtd} v51 and \aName{Joda-Time}~\cite{jodatime} v1231).
Additionally, we randomly select one more project version (\aName{Apache Lucene}~\cite{lucene} v1918) from the remaining projects, to cover the middle ground.
%
Note that we select at most one version from each distinct project, because different versions of the same project may share a lot of code, such that detectors are likely to perform similarly on them.
%
This dataset for \nameref{e2} is summarized in Row \checkNum{3} of \autoref{tab:datasets}.

\InlineSec{Metrics}
We calculate the precision of the detector, i.e., the ratio between the number of true positives over the number of findings.

\InlineSec{Review Process}
Two authors independently review each of the \checkNum{top-20} findings of the sampled project versions and mark it as a misuse or not.
To determine this, they consider the logic and the documentation in the source code, the API's documentation, and its implementation if publicly available.
%
After the review, any disagreements between the reviewers are discussed until a consensus is reached.
We report Cohen's Kappa score as a measure of the reviewers' agreement.
%
Note that we follow a lenient reviewing process.
For example, assume a usage misses a check \code{if (iterator.hasNext())} before calling \code{iterator.next()}.
If the detector finds that \code{hasNext()} is missing, we mark the finding as a hit, even though this does not explicitly state that the call to \code{next()} should be guarded by a check on the return value of \code{hasNext()}.
This follows our intuition that such findings may still provide a developer with a valuable hint about the problem.


\subsection{Experiment~RUB}
\label{e1}

We design \nameref{e1} to assess the detection capabilities of our subject detectors, i.e., to measure an upper bound to their recall under the assumption that they always mine the required pattern.

\InlineSec{Motivation}
We argue that it is important for developers to know which misuses a particular tool may or may not find, in order to decide whether the tool is adequate for their use case and whether they must take additional measures.
Moreover, it is important for researchers to know which types of misuses existing detectors may identify, in order to direct future work.
Therefore, we measure detectors' recall while providing sufficiently many correct usages that would allow them to mine the required pattern.

\InlineSec{Dataset}
For this experiment, we use all compilable project versions from the \MUBench dataset with the respective known misuses, as well as the hand-crafted misuse examples.
This dataset for \nameref{e1} is summarized in Row \checkNum{4} of \autoref{tab:datasets}.

\InlineSec{Setup}
Recall that all our subject detectors mine patterns, i.e., frequently reoccurring API usages, and assume that these correspond to correct usages.
They use these patterns to identify misuses.
Recall further that each detector has a distinct representation of usages and patterns and its own mining and detection strategies.
If a detector fails to identify a particular misuse, this may be due to
\begin{enumerate*}[label=(\arabic*)]
  \item an inherent limitation of the detector, e.g., because it cannot represent some usage element such as conditions, or
  \item a lack of examples of respective correct usage for pattern mining, i.e., a limitation of the training data.
\end{enumerate*}
With \nameref{e1}, we focus on (1), i.e., we take (2) out of the equation and assess the detectors' general ability to identify misuses.
To this end, we provide the detectors with sufficiently many examples of correct usage corresponding to the misuses in question.
This guarantees that they could mine a respective pattern.
If the detector is unable to identify a misuse in this setting, we know the problem lies with the detector itself.

We manually create a correct usage for each misuse in the dataset, using the fixing commits recorded in \MUBench.
For each misuse, we take the entire code of the method with the misuse after the fixing commit and remove all code that has no data or control dependencies to the objects involved in the misuse.
We store the code of this \emph{crafted correct usage} in our dataset.

In the experiment, we run each detector once for each individual known misuse in the dataset.
In each run, we provide the detector with the file that contains the known misuse and with \checkNum{50} copies of the respective crafted correct usage.
We ensure that the detector considers each copy as a distinct usage.
We configure the detectors to mine patterns with a minimum support of \checkNum{50}, thereby ensuring that they mine patterns only from the code in the crafted correct usage.
We chose \checkNum{50} as a threshold, since it is high enough to ensure that no detector mines patterns from the code in the file with the misuse.

\InlineSec{Metrics}
We calculate two numbers for each detector.
The first is its  \emph{conceptual recall upper bound}, which is the fraction of the known misuses in the dataset that match its capabilities from \autoref{tab:detector_capabilities}.
Note that the conceptual recall upper bound is calculated offline, without running any experiments.
The second is the detector's \emph{empirical recall upper bound}, which is the fraction of misuses a detector actually finds from all the known misuses in the dataset.
An ideal detector should have an empirical recall upper bound equal to its conceptual recall upper bound.
Otherwise, its practical capabilities do not match its conceptual capabilities.
In such cases, we investigate the root causes for such mismatches.
Note that we use the term ``upper bound,'' because neither recall rate reflects the detectors' recall in a setting without guarantees on the number of correct usages for mining.

\InlineSec{Review Process}
To evaluate the results, we review all \emph{potential hits}, i.e., findings from each detector that identify violations in the same files and methods as known misuses.
Two authors independently review each such potential hit to determine whether it actually identifies one of the known misuses.
If at least one potential hit identifies a misuse, we count it as a \emph{hit}.
%
After the review, any disagreements between the reviewers are discussed until a consensus is reached.
We report Cohen's Kappa score as a measure of the reviewers' agreement.
%
We follow the same lenient review process as for \nameref{e2}.


\subsection{Experiment~R}
\label{e3}

We design \nameref{e3} to assess the recall of detectors.

\InlineSec{Motivation}
While \nameref{e1} gives us an upper bound to the recall of misuse detectors, we also want to assess their actual recall where we do not provide them with correct usages ourselves.
Due to the lack of a ground-truth dataset, such an experiment has not been attempted before in any of the misuse-detection papers we surveyed.

\InlineSec{Dataset}
As the ground truth for this experiment, we use all known misuses from real-world projects in \MUBench plus the true positives identified by any of the detectors in \nameref{e2}.
This means that~\nameref{e3} not only evaluates recall against the misuses of \MUBench, but also practically cross-validates the detector capabilities against each other.
We exclude the hand-crafted misuse examples from this experiment, since there is no corresponding code for the detectors to mine patterns from.
The dataset we use for \nameref{e3} is summarized in Row \checkNum{5} of~\autoref{tab:datasets}.

\InlineSec{Setup}
We run all detectors on all projects versions individually, i.e., we use the same per-project setup as for \nameref{e2}.

\InlineSec{Metrics}
We calculate the recall of the detectors, i.e., the number of actual hits over the number of known misuses in the dataset.

\InlineSec{Review Process}
We review all potential hits in the same process as for \nameref{e1}.
This gives us the detectors' recall with respect to a large number of known misuses from \MUBench.
