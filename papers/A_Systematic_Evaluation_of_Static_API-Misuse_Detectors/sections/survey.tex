%!TEX root = ../paper.tex

\section{Conceptual Classification of Existing Misuse Detectors}
\label{sec:misuse-detectors-related-work}


To advance the state of the art of API-misuse detection, we need to understand the capabilities and short-comings of existing misuse detectors.
To identify detectors, we started from the publications about API-misuse detection listed in a survey of automated API-property inference techniques by Robillard~\etal~\cite{RBMR13}.
For each publication, we looked at all publications they refer to as related work and all publication that cite them, according to the ACM Digital Library or the IEEE Xplore Digital Library.
We recursively repeated this process, until we found no new detectors.

We use \MUC to guide the comparison.
We provide a \textit{conceptual classification} of the \emph{capabilities} of each detector with respect to \MUC, summarized in \autoref{tab:detector_capabilities}.
We use the published description and results of each detector to identify which of \MUC categories they can, conceptually, detect.
To reduce subjectivity, we confirmed our capability assessment and the detector descriptions with the respective authors, except for \PRMiner and \Colibri, whose authors did not respond. 
We also describe the strategies used to evaluate each detector and summarize those in \autoref{tab:survey-evaluations}.

% Unless otherwise stated, the surveyed detectors perform both pattern mining and anomaly detection at the same time (i.e., not as two separate phases), and all evaluations focused on detecting project-specific violations.
% This means that patterns mined from a specific project are used to detect misuses in the same project.

% summarize the evaluation methodology and results of each detector in \autoref{tab:survey_evaluation}.
% Column shows the number of evaluated projects.
% Most authors followed a ``Top X'' approach for selecting the anomalies they review (Column 2), where X could be a fixed number or percentage.
% Column 3 shows the true positive rate of reviewed anomalies. 

\vspace{0.03in}

%\paragraph*{\PRMiner}
\PRMiner is a misuse detector for C~\cite{LZ05}.
It encodes usages as the set of all function names called within the same function and then employs frequent-itemset mining to find patterns with a minimum support of $15$ usages.
% Detection
Violations here are strict subsets of a pattern that occur at least ten times less frequently than the pattern.
To prune false positives, \PRMiner{} applies inter-procedural analysis, i.e., for each occurrence of a violation, it checks whether the missing call occurs within a called method.
This analysis follows the call path for up to 3 levels.
The reported violations are ranked by the respective pattern's support.
% Capabilities
\PRMiner focuses on detecting missing method-calls.
% Performance
The evaluation applied \PRMiner to three target projects individually, thereby finding violations of project-specific patterns.
The detector reported 1,601 findings (1,447, 147, and 7 on the individual projects).
The authors reviewed the top-60 violations reported across all projects and found 18.1\% true positives (26.7\%, 10.0\%, and 14.3\% on the individual projects).

\vspace{0.03in}

\Chronicler is a misuse detector for C~\cite{RGJ07b}.
It mines frequent call-precedence relations from an inter-procedural control-flow graph.
A relation is considered frequent, if it holds on at least 80\% of all execution paths.
Paths where such relations do not hold are reported as violations.
% Capabilities
\Chronicler detects missing method calls.
Since loops are unrolled exactly once, it cannot detect missing iterations.
% Performance
The evaluation applied \Chronicler to five projects, thereby finding violations of project-specific patterns.
The authors compared the identified protocols with the documented protocols for one API and discussed a few examples of actual bugs found by their tool.

\vspace{0.03in}

%\paragraph*{\Colibri{}}
\Colibri is another misuse detector for C~\cite{L07}.
It re-implements \PRMiner using \emph{Formal Concept Analysis}~\cite{GW99} to strengthen the theoretical foundation of the approach.
% Capabilities
Consequently, its capabilities are the same as \PRMiner's.
% Performance
The evaluation applied \Colibri to five target projects, thereby finding violations of project-specific patterns.
While some detected violations are presented in the paper, no statistics on the quality of the detector's findings are reported.

\vspace{0.03in}

\input{sections/table-capabilities}

\input{sections/table-survey-evaluation}

%\paragraph*{\Jadet}~\cite{WZL07}
\Jadet is a misuse detector for Java~\cite{WZL07}.
It uses \Colibri{}~\cite{L07}, but instead of only method names, it encodes method-call order and call receivers in usages.
It builds a directed graph whose nodes represent method calls on a given object and whose edges represent control flows.
From this graph, it derives a pair of calls for each call-order relationship.
%, e.g., \code{m() $\prec$ n()}. 
The sets of these pairs form the input to the mining, which identifies patterns, i.e., sets of pairs, with a minimum support of $20$.
% Detection
A violation may miss at most $2$ properties of the violated pattern and needs to occur at least ten times less frequently than the pattern.
Detected violations are ranked by $u \times s / v$, where $s$ is the violated pattern's support, $v$ is the number of violations of the pattern, and $u$ is a uniqueness factor of the pattern.
% Capabilities
\Jadet detects missing method calls.
It may detect missing loops as a missing call-order relation from a method call in the loop header to itself.
%However, it cannot detect violations of patterns that consist of only two calls since such a pattern would be encoded as a set of a single pair of method calls.
% The only strict subset of such a pattern is the empty set, which is by definition not a violation.
% Performance
The evaluation applied \Jadet to five target projects, thereby finding violations of project-specific patterns.
The authors reviewed the top-10 violations reported per project and found 6.5\% true positives (0\%, 0\%, 7.7\%, 10.5\%, and 13.3\% on the individual projects).
Other findings were classified as code smells (6.5\%) or hints (35.0\%).

In a subsequent study, \Jadet was applied in a cross-project setting where it was applied to 6,097 projects at once, using a minimum pattern support of 200~\cite{GWZ10}.
The authors reviewed the top-25\% findings from a random sample of 20 projects, a total of 50 findings, and found 8\% true positives.
Other findings were classified as code smells (14.0\%).

\vspace{0.03in}

%\paragraph*{\RGJ}~\cite{RGJ07}
\RGJ is a misuse detector for C~\cite{RGJ07}.
It encodes usages as sets of properties for each variable. % \code{v}.
Properties are
comparisons to literals,
%e.g., $(\neq, \code{null})$,
argument positions in function calls,
%e.g., $(\operatorname{arg}(2), \code{f})$ if \code{v} was passed as the second argument to a function \code{f},
and assignments.
%, e.g., $(:=, \operatorname{res}(\code{f}))$ if the \code{v} was assigned the result of a call to \code{f}.
For each call, it creates a group of the property sets of the call's arguments.
To all groups for a particular function, it applies sequence mining to learn common sequences of control-flow properties and frequent-itemset mining to identify all common sets of all other property types.
% Detection
Subsequently, it identifies violations of the common~property sequences and sets.
% Capabilities
\RGJ is designed to detect missing conditions.
From the properties it encodes, it can detect missing \code{null} checks and missing value or state conditions.
Since patterns contain preceding calls on arguments, it may also detect missing calls, if the respective call shares an argument with another call in the pattern.
% Performance
The evaluation applied \RGJ to a single project, thereby finding violations of project-specific patterns.
The authors discussed several examples of actual bugs their approach detects, but reported no statistics on the detection performance.

\vspace{0.03in}

%\paragraph*{\Alattin}~\cite{TX09b}
\Alattin is a misuse detector for Java~\cite{TX09b}, specialized in alternative patterns for condition checks.
For each target method \code{m}, it queries a code-search engine to find example usages.
From each example, it extracts a set of rules about pre- and post-condition checks on the receiver, the arguments, and the return value of \code{m}, e.g.,  ``\code{boolean} check on \code{return} of \code{Iterator.hasNext} before \code{Iterator.next}.''
% or ``\code{null} check on \code{argument} of \code{ArrayList.constructor} after \code{Iterator.next}.'' 
It then applies frequent-itemset mining on these rules to obtain patterns with a minimum support of $40\%$.
For each such pattern, it extracts the subset of all groups that do not adhere to the pattern and repeats mining on that subset to obtain infrequent patterns with a minimum support of $20\%$.
Finally, it combines all frequent and infrequent patterns for the same method by disjunction.
% Detection
An analyzed method has a violation if the set of rules that hold in it is not a superset of any of the alternative patterns.
Violations are ranked by the support of the respective pattern.
% Capabilities
\Alattin, therefore, detects missing \code{null}-checks and missing value or state conditions that are ensured by checks and do not involve literals.
It may also detect missing method-calls that occur in checks.
% Performance
The evaluation applied \Alattin to six projects.
Since it queries code-search engines for usage examples, it detects violations of cross-project patterns.
The authors manually reviewed all violations of the top-10 patterns per project, a total of 532 findings, and confirmed that 29.5\% identify missing condition checks (12.5\%, 26.2\%, 28.1\%, 32.7\%, 52.6\%, and 100\% for the individual projects).
Considering frequent alternative patterns reduced false positives by 15.2\% on average, which increased precision to 33.3\%.
Considering both frequent and infrequent alternatives even reduced false positives by 28.1\% on average, leading to a precision of 37.8\%, but introduced 1.5\% additional false negatives, because misuses that occur multiple times are mistaken for infrequent patterns.


\vspace{0.03in}

%\paragraph*{\AX}~\cite{AX09}
\AX is a misuse detector for C~\cite{AX09}, specialized in detecting wrong error handling, realized through returning (and checking for) error codes.
It distinguishes normal paths, i.e., execution paths from the beginning of the \code{main} function to its end, and error paths, i.e., paths from the beginning of the \code{main} function to an \code{exit} or \code{return} statement in an error-handling block.
\AX uses push-down model checking to generate such paths as sequences of method calls and applies frequent-subsequence mining to find patterns with a minimum support of 80\% (but at least 5 usages).
% Detection
It then uses push-down model checking to verify adherence to these patterns and identify respective violations.
Finally, it filters false positives by tracking variable values and excluding error cases that cannot occur.
% rank violations by pattern support
% Capabilities
It detects missing error-handling as well as missing method calls among error-handling functions.
%As a side effect of identifying
Since it identifies error-handling blocks through a predefined set of checks, it also detects missing \code{null}-checks and missing value or state conditions in the case of missing error-handling blocks.
% Performance
The evaluation applied \AX to three projects individually, thereby finding violations of project-specific patterns.
The authors manually reviewed all 292 findings and confirmed $90.4\%$ true positives (50.0\%, 90.3\%, and 93.5\% on the individual projects).

\vspace{0.03in}

%\paragraph*{\CARMiner}
\CARMiner is a misuse detector for C++ and Java~\cite{TX09}, also specialized in detecting wrong error handling.
For each analyzed method \code{m} in a given code corpus, it queries a code-search engine to find example usages.
From the examples, it builds an \emph{Exception Flow Graph} (EFG), i.e., a control-flow graph with additional edges for exceptional flow. 
%These graphs distinguish exception edges, i.e., edges to and within \code{catch} and \code{finally} blocks. 
From the EFG, it generates \emph{normal} call sequences that lead to the currently analyzed call and \emph{exception} call sequences that lead from the call along exceptional edges.
Subsequently, it mines association rules between normal sequences and exception sequences, with a minimum support of $40\%$.
% and ranks these rules by their support.
% Detection
To detect violations, \CARMiner extracts the normal call sequence and the exception call sequence for the target method call.
It then uses the learned association rules to determine the expected exception handling and reports a violation if the actual sequence does not include it.
% Capabilities
\CARMiner detects missing exception-handling as well as missing method calls among error-handling functions.
%Since it does not distinguish calls in \code{finally} blocks from calls in \code{catch} blocks, it cannot detect missing guarantees.
% Performance
The evaluation applied \CARMiner to five projects.
Since it queries code-search engines for usage examples, it detects violations of cross-project patterns.
The authors manually reviewed all violations of the top-10 association rules for each project, a total of 264 violations, and confirmed that 60.1\% identify wrong error handling (41.1\%, 54.5\%, 68.2\%, 68.4\%, and 82.3\% on the individual projects).
Other findings were classified as hints (3.0\%).

\vspace{0.03in}

%\paragraph*{\GROUMiner}
\GROUMiner is a misuse detector for Java~\cite{NNP+09}.
It creates a graph-based object-usage representation (\GROUM) for each target method.
A \GROUM is a directed acyclic graph whose nodes represent method calls, branchings, and loops and whose edges encode control and data flows.
\GROUMiner performs frequent-subgraph mining on sets of such graphs to detect recurring usage patterns with a minimum support of $6$.
% Detection
When at least 90\% of all occurrences of a sub-pattern can be extended to a larger pattern, but some cannot, those \emph{rare} inextensible occurrences are considered as violations.
Note that such violations have always exactly one node less than a pattern.
The detection of patterns and violations happens at the same time.
Violations are ranked by their \emph{rareness}, i.e., the support of the pattern over the support of the violation.
% Capabilities
\GROUMiner detects missing method calls.
It also detects missing conditions and loops at the granularity of a missing branching or loop node.
However, it cannot consider the actual condition.
% Performance
The evaluation applied \GROUMiner to nine projects individually, thereby finding violations of project-specific patterns.
The authors reviewed the top-10 violations per project, a total of 184 findings, and found $5.4\%$ true positives (three times 0\%, five times 6.7\%, and once 7.8\% on the individual projects).
Other findings were classified as code smells (7.6\%) or hints (6.0\%).

\vspace{0.03in}

%\paragraph*{DMMC}~\cite{MM13}
\DMMC is a misuse detector for Java~\cite{MBM10}, specialized in missing method calls.
The detection is based on type usages, i.e., sets of methods called on an instance of a given type in a given method.
% and the signature of the method the usage occurred in (i.e., the context)
Two usages are \emph{exactly similar} if their respective sets match and are \emph{almost similar} if one of them contains exactly one additional method.
% Detection
The detection is based on the assumption that violations should have only few exactly-similar usages, but many almost-similar ones.
The likelihood of a usage $x$ being a violation is expressed in the \emph{strangeness score} $= 1 - |E(x)| / (|E(x)| + |A(X)|)$, where $E(x)$ is the set of usages that are exactly similar to $x$ and $A(x)$ the set of those that are almost similar.
A usage is considered a violation if its strangeness score is above $0.97$.
Violations are ranked by the strangeness score.
% Capabilities
\DMMC detects misuses with exactly one missing method-call.
% Performance
The evaluation applied \DMMC to a single project, thereby finding project-specific violations.
The authors manually reviewed all findings with a strangeness score above 97\%, a total of 19 findings, and confirmed 73.7\% as true positives.
The evaluation was repeated later~\cite{MM13}, applying \DMMC to three projects individually, thereby finding project-specific violations for a predefined set of APIs.
The authors report that they manually reviewed approximately 30 findings, and confirmed 17 ($\approx 56.7\%$) as true positives.
Others were classified as workarounds for bugs inside a used API.

\vspace{0.03in}

%\paragraph*{\Tikanga}~\cite{WZ11}
\Tikanga is a misuse detector for Java~\cite{WZ11} that builds on
%the same algorithm as
\Jadet.
It extends the simple call-order properties
% of the form \code{m() $\prec$ n()} 
to general Computation Tree Logic
% (CTL) 
formulae on object usages.
Specifically, it uses formulae that require a certain call to occur,
% in a usage, 
formulae that require two calls in
%a certain
order, and formulae that require a certain call to happen after another.
It uses model checking to determine
% the subset of 
all those formulae with a minimum support of 20 in the codebase.
Violations are ranked by the \emph{conviction} measure~\cite{BMUT97} of the association between the set of present formulae and the set of missing formulae in the violating usage.
% Detection
It then applies Formal Concept Analysis~\cite{GW99} to obtain patterns and violations at the same time.
% Capabilities
\Tikanga's capabilities are the same as \Jadet's.
%, but it also detects violations of patterns with two or less calls.
% Performance
The evaluation applied \Tikanga to six projects individually, finding violations of project-specific patterns.
The authors manually reviewed the top-25\% of findings per project, a total of 121 findings, and confirmed 9.9\% as true positives (0\%, 0\%, 8.3\%, 20.0\%, 21.4\%, and 33.3\% on the individual projects).
Other findings were classified as code smells (29.8\%).

\vspace{0.03in}

%\paragraph*{\DroidAssist}~\cite{NPVN16}
\DroidAssist is a detector for Android Java Bytecode~\cite{NPVN16}.
It generates method-call sequences from source code and learns a Hidden Markov Model from them, %, using a modified version of the Baum-Welch algorithm~\cite{todo}.
%
to compute the likelihood of a particular call sequence.
If the likelihood is too small, the sequence is considered a violation.
\DroidAssist then explores different modifications of the sequence (adding, replacing, and removing calls) to find a slightly modified, more likely sequence.
% Capabilities
This allows it to detect missing and redundant method calls and even to suggest solutions for them.
%
An evaluation of this mechanism is not provided in the respective paper.

\vspace{0.04in}
\noindent{\textbf{Summary.}}
All detectors use code (snippets) as training and verification input.
Some require the code in a compiled format, such as Java Bytecode, while others directly work on source code.
Detectors typically encode usages as sets, sequences, or graphs.
Graph representations seem promising for simultaneously encoding usage elements, order, and data-flow relations.
With the exception of \DroidAssist and \DMMC, detectors mine patterns through frequent-itemset/subsequence/subgraph mining, according to their usage representation. 
To detect violations, they mine in-extensible parts of patterns that are themselves observed infrequently.
This implies that they cannot detect redundant elements, since a usage with such an element is never part of any pattern.
The exception is \DroidAssist, which might find redundant calls as being unlikely.

\autoref{tab:detector_capabilities} summarizes the detectors' capabilities with respect to \MUC.
Overall, we find that detectors cover only a small subset of all API-misuse categories.
While all detectors may, to some degree, identify missing method calls, only four detectors may identify missing \code{null} checks and missing value-or-state conditions, only three may identify missing iterations, and only two may identify missing exception handling.
None of the detectors targets all of these categories.

Existing detectors use both absolute and relative minimum support thresholds to identify patterns.
The exceptions are, again, \DroidAssist and \DMMC, which use probabilistic approaches.
Since many detectors produce a high number of false positives, they use a variety of ranking strategies.
Most of these rely mainly on the pattern support, but some use different concepts, such as \emph{rareness}, \emph{strangeness}, or \emph{conviction}.
A comparison of different ranking strategies is not reported in any of the publications.

\autoref{tab:survey-evaluations} summarizes the empirical evaluations of the surveyed detectors, as reported in their original papers.
Most evaluations apply detectors to target projects individually.
In this setting, the detectors learn project-specific patterns and identify respective violations
The number of projects ranges from \checkNum{1 to 20} (average 5.3; median 5).
The concrete projects samples are all distinct and mostly even disjunct.

To assess the detection performance, most authors review the top-X findings of their detectors, where X is a fixed number or percentage.
They then either present anecdotal evidence of true positives or measure the precision of detectors.
Many evaluations also present additional categories of findings, such as code smells, to distinguish false positives from other non-misuse findings that may still be valuable to developers.
The definitions of when a finding belongs to which category---if provided---differ between publications, even if they use the same label, e.g., ``bug'' or ``code smell.''
No evaluation considers the recall of the respective detector.

Overall, it appears that the detectors that focus on specific violations, such as error handling or missing method calls, have higher precision.
However, simply comparing detectors based on their reported empirical results would be unreliable, since the target projects, the review sample sizes, and the criteria to assess detector findings differ between the studies.
