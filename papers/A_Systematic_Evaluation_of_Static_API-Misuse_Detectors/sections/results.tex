%!TEX root = ../paper.tex

\section{Results}
\label{sec:results}

We now discuss the results of comparing \Jadet, \GROUMiner, \Tikanga, and \DMMC in our experiments.
All reviewing data is available on our artifact page~\cite{artifact-page}.

\subsection{\nameref{e2}}

\begin{table*}[tb]
  \centering
  \small
  \caption{\nameref{e2}: Precision of the Detectors on the \checkNum{Top-20 Findings} on \checkNum{5 Projects} and Root Causes for False Positives.}
  \input{sections/table-ex2-results}
  \label{tab:ex2-results}
\end{table*}

\autoref{tab:ex2-results} shows our precision results, based on reviewing the \checkNum{top-20 findings} per detector on each of our \checkNum{five sample projects}.
The second column shows the total number of reviewed findings, \checkNum{230} in total across all detectors.
Note that all detectors report less than \checkNum{20 findings} for some projects.
The third column shows the confirmed misuses after resolving disagreements, and the fourth column shows the precision with respect to the reviewed findings.
The fifth column shows the Kappa score for the manual reviews, and the remaining columns show the frequencies of root causes for false positives.
%
We find that the precision of all detectors is extremely low.
\Tikanga shows the best precision of only \checkNum{11.4\%}.
\Jadet and \DMMC follow immediately behind, with a precision of \checkNum{10.3\%} and \checkNum{9.9\%}, respectively.
\GROUMiner reports only false positives in its \checkNum{top-20 findings}.

\begin{obs}{e2-precision}
  All detectors have extremely low precision (below \checkNum{12\%}).
  On average, they report less than 1.5 actual misuses in their top-20 findings.
  %~\sn{where did you get this number from?}~\sa{the number was wrong. Its $17 \text{ confirmed misuses } / 230 \text{ reviewed findings } * \text{(top-)}20 = 1.478$}~\sn{So it is a precision of 1.5\% or the absolute number of true positives.. right now the number is confusing that it is the percentage but phrased as an absolute/average number of true positives}~\sa{I rephrased it. Better now?}
\end{obs}

The Kappa scores indicate high reviewer agreement, which shows that all detectors produced mostly clear false positives.
The score is a little lower for \Tikanga, because it reported one confirmed misuse twice, which one of the reviewers first accepted as an actual hit while the other did not.
The score is also lower for \DMMC, because we initially disagreed on several violations it identifies in \code{Iterator} usages that do not check \code{hasNext()}, but the underlying collection's size.

\subsubsection*{True Positives}

Out of the \checkNum{230 reported findings} we reviewed, we confirm \checkNum{17 true misuses}.
%
\DMMC reports \checkNum{8 misuses} of an iterator API where \code{hasNext()} is not checked.
\Jadet reports \checkNum{4 misuses} that access a collection without checking its size before.
Also for collections, \Tikanga reports \checkNum{4 misuses} with a missing \code{hasNext()} and \checkNum{1 misuse} with a missing size check.
One misuse is reported by both \Tikanga and \Jadet and another by both \Tikanga and \DMMC.
Additionally, \Jadet reports one misuse twice.
This leaves a total of \checkNum{14 unique misuses}, all different from the known misuses in \MUBench.
% \mm{Are you saying that you didn't find any of the misusages in MuBench? If this was true, than you come you found a recall of higher than 0? I am confused (which also might be due to being tired ...)}
%
Interestingly, \emph{all} these misuses are missing value or state conditions, for which the detectors report only missing calls to methods that should be used in the respective missing checks.
We accept these findings in our lenient review process.


\begin{obs}{e2-misuses-allCSV}
  All \checkNum{14 confirmed misuses} in \nameref{e2} are missing value or state condition checks before accessing the elements of a collection, either directly or through an iterator.
\end{obs}

\subsubsection*{False Positives}

To identify opportunities to improve the precision of misuse detectors, we systematically investigate the root causes for the false positives they report.
In the following, we discuss these root causes summarized across all detectors, in the order of their absolute frequency.

\vspace{0.03in}
\noindent {\bf 1. Uncommon.}
% description
Particular usages may violate the patterns that detectors learn from frequent usages, without violating actual API usage constraints.
Detectors cannot differentiate infrequent from invalid usage.
% example
For example, \DMMC and \Jadet learn that the methods \code{getKey()} and \code{getValue()} of \code{MapEntry} usually appear together in code.
They both report violations if a call to either of these methods is missing, or, in case of \Jadet, if the calls appear in a different order.
However, there is no requirement by the API to always call both getter methods, let alone in a specific order.
Across the reported violations we analyzed, the detectors falsely report \checkNum{42 missing method calls} in cases where one out of a number of getter methods is missing or invoked in a different order.
%
Another example is that \Jadet and \Tikanga learn that methods such as \code{List.add()} and \code{Map.put()} are usually invoked in loops and report \checkNum{five missing iterations} for respective invocations outside a loop, which are perfectly fine according to the API.
%
Approaches such as multi-level patterns~\cite{SBA+15} or \Alattin's alternative patterns~\cite{TX09b} may help to mitigate this problem.
Also note that the four detectors in our experiments all use absolute frequency thresholds, while some of the detectors from our survey in \autoref{sec:misuse-detectors-related-work} also used relative thresholds.
Future work should investigate how these two alternatives compare.

\begin{obs}{e2-frequent}
  Particular usages may be uncommon without violating API constraints.
  Neglecting this causes \checkNum{73 (34.3\%)} of the~detectors' false positives in their \checkNum{top-20 findings}.
%
This calls for research on detecting patterns without setting a hard threshold on occurrence frequencies.
%
Meanwhile, relaxing requirements on the co-occurrence of getter methods might reduce false positives significantly.
\end{obs}

\vspace{0.03in}
\noindent {\bf 2. Analysis.}
% description
The detectors use static analysis to determine the facts that belong to a particular usage.
Imprecisions of these analyses lead to false positives.
% example
For example, the detectors mistakenly report \checkNum{five missing elements} in code that uses multiple aliases for the same object and another \checkNum{17} in code with nested control statements.
In both cases, the analysis failed to capture all calls belonging to the same usage.
\GROUMiner reports \checkNum{two missing method calls}, because it cannot resolve the receiver types in the chained calls and, therefore, fails to match a call between the pattern and the usage.
% example: interpocedural
Another example is that the detectors report \checkNum{eight missing method calls} due to chained calls on a fluent API, such as \code{StringBuilder}, where their analyses cannot determine that all calls actually happen on the same object.
\Jadet, \GROUMiner, and \DMMC together report \checkNum{nine missing calls} that happen transitively in a helper method of the same class or through a wrapper object, such as a \code{BufferedStream}.
\DMMC reports \checkNum{a missing call} that is located in the enclosing method of an anonymous class instance and \checkNum{a missing \code{close()} call} on a parameter that is, by contract, closed by the callers.
Moreover, \GROUMiner reports \checkNum{four missing conditions} that are checked by assertion helper methods.
An inter-procedural detection strategy, as proposed by \PRMiner~\cite{LZ05}, could mitigate this problem.
% one additional case is where the same object is retrieved twice via get, so same as chained calls on fluent API

\begin{obs}{e2-analysis}
  Imprecisions of the detectors' static analyses cause \checkNum{51 (23.9\%)} of the false positives in their \checkNum{top-20 findings}.
  An inter-procedural detection strategy might be able to eliminate \checkNum{14 (6.6\%)} of these false positives.
\end{obs}

\vspace{0.03in}
\noindent {\bf 3. Alternative.}
%~\mm{Is this really different from "Frequent"? Aren't the alternative correct usages non-frequent usages?}~\sa{They don't necessarily are, there might be multiple frequent alterantives and they would still trigger this problem. However, we could double check whether we have such a case at all. I'm not sure.}
% description
The detectors often learn a pattern and then report instances of alternative usages as violations.
We define \emph{alternative usages} as a different functionally correct way to use an API, either to achieve the same or a different functionality.
Note that multiple alternatives may occur frequently enough to induce patterns.
% example
For example, \Jadet, \Tikanga, and \DMMC learn that before a call to \code{next()}, there should always be a call to \code{hasNext()} on an \code{Iterator}.
Consequently, they report \checkNum{16 violations} in usages that check either \code{isEmpty()} or \code{size()} on the underlying collection before fetching only the first element through the \code{Iterator}.
\DMMC reports \checkNum{another violation}, because \code{isEmpty()} is used instead of \code{size()} before accessing a \code{List}.
%
Another example is that \Jadet, \Tikanga, and \DMMC learn that collections are filled one element at a time, e.g., by calling \code{add()}, and report \checkNum{10 missing methods} in usages that populate a collection differently, e.g., through the constructor or using \code{addAll()}.
%
% Another example is that \Jadet and \Tikanga learn that the size of a collection is usually retrieved in a loop header, when iterating over the list.
% Consequently, they each report one violation where the size is assigned to an intermediate variable before the loop and checked in the loop header, even though this usage is equivalent---assuming that the collection is not modified in the loop.
%
\GROUMiner reports \checkNum{four usages} where an alternative control statement is used, e.g., a \code{for} instead of a \code{while}.

%\paragraph*{Obtain}
% description
A special case of this root cause is alternatives to obtain an instance of a type.
% example
For example, \GROUMiner mistakenly reports \checkNum{two missing constructor calls} where the instance is not created through a constructor call as in the pattern, but returned from a method call.
\Jadet and \DMMC{} \checkNum{each report one missing constructor call} where an instance is not created, but passed as a parameter.
While handling alternative patterns is an open problem, some tools such as \Alattin already propose possible~solutions~\cite{TX09b}.


\begin{obs}{e2-alternative}
  A violation of a pattern might be an instance of an alternative, correct way to use the respective API.
  Not considering this causes \checkNum{41 (19.2\%)} of the false positives in their \checkNum{top-20 findings}.
\end{obs}

\vspace{0.03in}
\noindent {\bf 4. Inside.}
% description
Objects that are stored in fields are often used across multiple methods of the field's declaring class.
The respective API usages inside the individual methods might then deviate from usage patterns without being actual misuses.
% example
\autoref{lst:partial-usage} shows an example of such a case, where two fields of type \code{Iterator}, \code{in} and \code{out}, are used to implement the class \code{NeighborIterator}.
When \code{in} yields no more elements (Line~\ref{line:in-empty}), the call to \code{next()} in Line~\ref{line:unsafe-next} happens on \code{out} without a prior check whether it has more elements.
While this appears to be a misuse of the \code{Iterator} API inside the enclosing method, it is a correct usage inside the enclosing class, since \code{NeighborIterator} itself implements \code{Iterator} and, thereby, inherits its usage constraints.
Correct usages of \code{NeighborIterator} need to check its \code{hasNext()} method (Line~\ref{line:hasNext}) before calling its \code{next()} method (Line~\ref{line:next}), which ensures that \code{out} has more elements when \code{next()} is called on it.
\DMMC and \GROUMiner report \checkNum{sixteen violations} for such usages of fields of a class.
% Thereof, \DMMC reports \checkNum{six violations} in constructors where fields are initialized, but the respective objects not used otherwise.
% Similarly, \DMMC and \GROUMiner report \checkNum{three violations} in named constructors, i.e., methods that only create, initialize, and return an object, but not use it otherwise.
%
% Finally, \GROUMiner reports \checkNum{three violations} where it mines the patterns from usages in other methods of the same class and finds a deviance in a certain method, e.g., while most methods of \code{RAMDirectory} check whether the underlying file is open and throw an exception if it is not, the method \code{fileExists()} deviates from this pattern in that it does not throw an exception, but rather returns a boolean.

A special case of this root cause is when a class uses part of its own API in its implementation.
For example, when a \code{Collection} calls its own \code{add()} method in the implementation of its \code{addAll()} method.
\DMMC and \GROUMiner report \checkNum{four such violations}.
This is particularly interesting, because these are actually self usages of the API, while the detectors target client usages.
Since any codebase likely contains such self usages, detectors should consider this.

\begin{figure}[tb]
  \begin{lstlisting}[language=java,breaklines=true,escapechar=/]
class NeighborIterator implements Iterator<GraphNode> {
  private final Iterator<DiGraphEdge> in = ...;
  private final Iterator<DiGraphEdge> out = ...;
  
  @Override
  public boolean  hasNext() {/\label{line:hasNext}/
    return in.hasNext() || out.hasNext();
  }

  @Override
  public GraphNode next() {/\label{line:next}/
    boolean isOut = !in.hasNext();/\label{line:in-empty}/
    Iterator<DiGraphEdge> curIterator =  isOut ? out : in;
    DiGraphEdge s = curIterator.next();/\label{line:unsafe-next}/
    return isOut ? s.getDestination() : s.getSource();
  }
  
  ...
}
  \end{lstlisting}
  \caption{Correct Usages of \code{Iterator} Instances in the \aName{Closure} Project that Violate Usage Patterns.}
  \label{lst:partial-usage}
\end{figure}

\begin{obs}{e2-inside}
  The implementation code of a class may contain partial usages of the class' own API or fields.
  Such usages cause \checkNum{26 (12.2\%)} of the detectors' false positives in their \checkNum{top-20 findings}.
\end{obs}

\vspace{0.03in}
\noindent {\bf 5. Dependent.}
% description
When two objects' states depend upon each other, usages sometimes check the state of one and implicitly draw conclusions about the state of the other.
The detectors do not consider such inter-dependencies.
% example
For example, when two collections are maintained in parallel, i.e., always have the same size, it is sufficient to check the size of one of them before accessing either.
The detectors falsely report \checkNum{14 missing size checks} in such usages.
In \checkNum{10 of these cases}, the equal size is ensured by construction of the collections in the same method.
In the \checkNum{remaining four cases}, it is ensured elsewhere in the same class.
We consider this a dangerous practice, because should the dependency between the collections ever change, it is easy to miss some of the code that relies on it.
Thus, warning developers might be justified.
Nevertheless, we count these cases as false positives, since the current usages are correct.

\begin{obs}{e2-dependent}
  Semantic dependencies between objects' states may implicitly ensure conditions.
  Not considering such inter-dependencies causes \checkNum{14 (6.6\%)} of the detectors' false positives in their \checkNum{top-20 findings}.
\end{obs}

\begin{table*}[tb]
  \centering
  \small
	\caption{\nameref{e1}: Recall of the Isolated Detection Strategies and Root Causes for Divergences.}
    \input{sections/table-ex1-results}
  \label{tab:ex1-results}
\end{table*}

% \paragraph*{Precondition}
% description
% The detectors treat the cooccurrence of facts as a mutual implication, while dependencies between facts may be directed.~\hn{now reading this description, I think this should be in inter-procedural analysis. I would imagine that the method containing this hasNext() will be followed by method containing next() or some other method calls. There is no point calling hasNext() only.}~\sa{In the case of Iterator I agree, however, I think this is a more general pattern, as, for example, it makes sense in some cases to only check the size of a list without iterating or accessing it.}
% example
% For example, all detectors associate \code{hasNext()} and \code{next()}. Consequently, they report actual misuses where only \code{next()} is called. However, they also report false positives where only \code{hasNext()} is called, e.g., to change output depending on wether the iterator is empty or not, without actually consuming it.

\vspace{0.03in}
\noindent {\bf 6. Multiplicity.}
% description
The detectors cannot handle methods that may be called arbitrarily often.
% example
\GROUMiner and \Jadet both learn a pattern where the \code{append()} method of \code{StringBuilder} is called twice and falsely report \checkNum{three missing method calls} where it is called only once.

\begin{obs}{e2-multiplicity}
  Detectors should distinguish methods that require a specific number of calls, from methods that require one or more calls, and methods that may be called arbitrarily often.
  Not considering this causes \checkNum{3 (1.4\%)} of the detectors' false positives in their \checkNum{top-20 findings}.
\end{obs}

\vspace{0.03in}
\noindent {\bf 7. Bug.}
% description
A few findings are likely caused by mistakes in the detector implementations.
% example
\DMMC reports \checkNum{four violations} with an empty set of missing methods.
These empty sets are produced when none of the potentially missing methods match \DMMC's prevalence criteria.
\DMMC should probably filter such empty-set findings before reporting.
%
\GROUMiner reports \checkNum{one missing \code{if}} that actually appears in all respective usages, because its graph mapping does not match the respective \code{if} node from one of the usages with the corresponding nodes of all the other usages.



\subsection{\nameref{e1}}

We run all detectors to see which of the \checkNum{64 known misuses} from \MUBench they can detect when given the respective crafted correct usages for pattern mining.
\autoref{tab:ex1-results} shows the results per detector.
%
The second and third columns show the number of potential hits and the number of actual hits, after resolving disagreements.
The fourth and fifth columns show the detectors' empirical recall upper bound and conceptual recall upper bound, respectively.
The sixth column shows the Kappa score for the manual reviews.
The remaining columns show the frequencies of root causes for divergences between a detector's conceptual capabilities from \autoref{tab:detector_capabilities} and its actual findings in this experiment.

We find that \GROUMiner has by far the best recall upper bound and also shows the best recall in \nameref{e1}.
This suggests that its graph representation is a good choice to capture the differences between correct usages and patterns.
However, the gap between \GROUMiner's conceptual upper bound recall and its empirical recall upper bound is quite noticeable.
Actually, \autoref{tab:ex1-results} shows that all four detectors fall considerably short of their conceptual recall upper bound in practice.

Generally, we observe two kinds of divergences between the actual findings and the conceptual capabilities:
\emph{Unexpected false negatives}, i.e., misuses that a detector should be able to~detect, but does not, and \emph{unexpected hits}, i.e., misuses that a detector supposedly cannot detect, but does.
We investigate the root causes of each divergence to identify actionable ways to improve detectors.

\begin{obs}{e1-capability-gap}
  All detectors' empirical recall upper bound is much lower than their conceptual recall upper bound.
  Detectors' findings frequently diverge from their conceptual capabilities.
\end{obs}

The Kappa scores indicate good reviewer agreement, albeit a little lower than in \nameref{e2}.
Since we only reviewed potential hits, i.e., findings in the same method as a known misuse, many potential hits were related to the known misuses.
Consequently, we had several disagreements on whether a particular potential hit actually identifies a particular misuse.
In total, we had \checkNum{18} such disagreements (\Jadet: 4; \GROUMiner: 6; \DMMC: 5; \Tikanga: 3), which led us to formulate the lenient review process described in \autoref{e1}.
We decided in favor of the detectors in \checkNum{eight} of these cases.
%
We observe that the Kappa score is a little lower for \Jadet, compared to the other detectors.
Since the absolute number of disagreements is comparable and \Jadet had relatively few potential hits, i.e., a small number of decisions as a basis for the Kappa score, we attribute the lower score to chance.
%We decided in favor of \Jadet in \checkNum{three} of the \checkNum{four} cases.

\subsubsection*{Unexpected False Negatives}

%We find the following root causes for unexpected false negatives:

\vspace{0.03in}
\noindent {\bf 1. Representation.}
% description
Current usage representations are not expressive enough to capture all details that are necessary to differentiate between misuses and correct usages.
% example
For example, \DMMC and \GROUMiner encode methods by their name only and, therefore, cannot detect a missing method call, when the usage calls an overloaded version of the respective method.
For example, assume that a pattern requires a call to \code{getBytes(String)}, but the target usage calls \code{getBytes()} instead.
An ideal misuse detector would still report a violation, since the expected method, with the correct parameters, is not called.
However, since only the method name is used for comparison in both these detectors, such a violation is not detected.
%
Another example is that, to use a \code{Cipher} instance for decryption, it must be in decrypt mode.
This state condition is ensured by passing the constant \code{Cipher.DECRYPT} to the \code{Cipher}'s \code{init()} method.
None of the detectors captures this way of ensuring that the condition holds, because they do not encode method-call arguments in their representations.

\begin{obs}{e1-capture}
 Inability to capture details necessary to differentiate misuses from correct usages in the usage representation is responsible for \checkNum{22 (45.8\%) of the unexpected false negatives}.
\end{obs}

\vspace{0.03in}
\noindent {\bf 2. Matching.}
% description
The detectors fail to relate a pattern and a usage.
Typically, detectors relate patterns and usages by their common facts.
If there are no or only few common facts, detectors report no violation.
% example
For example, \Jadet's facts are pairs of method calls.
In a scenario where \code{JFrame}'s \code{\code{setPreferredSize()}} method is accidentally called after its \code{pack()} method, \Jadet represents the usage with a pair $(\code{pack}, \code{setPreferredSize})$ and the pattern with the reverse pair.
Since it compares facts by equality, \Jadet finds no relation between the pattern and the usage.
Without common facts between a usage and a pattern, the detector assumes that these are two completely unrelated pieces of code and does not report a violation.
%
Another example is when the pattern's facts relate to a type, e.g., \code{List} in \code{List.size()}, while the usage's facts relate to a super- or sub-type such as \code{ArrayList.size()} or \code{Collection.size()}.
The detectors cannot relate these facts, since they are unaware of the type hierarchy.
%
Also, \Tikanga misses \checkNum{four misuses}, because the target misses more than two formulae of the pattern (\Tikanga's maximum distance for matching).
For example, \autoref{lst:tikanga-distance} shows a misuse that does not close a \code{Writer} and the corresponding correct usage.
In \Tikanga's representation, the difference between the misuse and the correct usage consists of three formulae:
\begin{enumerate*}[label=(\arabic*)]
  \item that \code{close()} follows \code{write()} in case of normal execution,
  \item that \code{close()} follows \code{write()} if the latter throws an exception, and
  \item that \code{close()} is preceded by a \code{null} check.
\end{enumerate*}

\begin{figure}[tb]
  \begin{subfigure}[t]{0.46\columnwidth}
    \begin{lstlisting}[language=java,numberblanklines=false,escapeinside=||,firstnumber=0]

writer.write(value);|\addtocounter{lstnumber}{-1}|



\end{lstlisting}
  \end{subfigure}
  \begin{subfigure}[t]{0.46\columnwidth}
    \begin{lstlisting}[language=java]
try {
  writer.write(value);
} finally {
  if (writer != null)
    writer.close();
}
    \end{lstlisting}
  \end{subfigure}
  \caption{Not Closing \code{Writer} vs. Correctly Closing \code{Writer}.}
  \label{lst:tikanga-distance}
\end{figure}

\begin{obs}{e1-relate}
  When matching patterns and misuses, detectors should consider the semantics of their representation, e.g., call order and the number of usage facts generated by adding specific usage constructs, as well as code semantics, e.g., subtype relations.
  Neglecting this is responsible for \checkNum{15 (31.3\%) of the unexpected false negatives}.
\end{obs}

\vspace{0.03in}
\noindent {\bf 3. Analysis.}
% description
The detectors rely on static analysis to extract their usage representations.
Imprecisions in these analyses may obscure relations between patterns and usages.
% example
For example, \GROUMiner fails to detect \checkNum{one missing \code{null} check}, because it cannot determine the receiver type for chained calls, such as for \code{m()} in \code{o.getX().m()}, which is not generally possible from source code alone.
%
Also, it fails to detect another \checkNum{four missing \code{null} checks}, because it overlooks dataflow dependencies.
\autoref{lst:GrouMiner-miss-df} shows such a case.
In addition to the \code{null} check, \GROUMiner also misses the dataflow from the \code{get()} calls to the \code{remove()} call in the misuse, which makes the pattern and usage differ by multiple facts.
\GROUMiner, however, only reports a violation if the difference is a single fact.
%
\Tikanga misses a call that occurs in the correct usage in \checkNum{one case} and fails to capture the call order between two calls from the correct usage in \checkNum{another case}.
We assume that the cause is a limitation of its analysis, but could not ultimately verify this, because the tool's developer is not available to confirm the implementation details.

\begin{figure}[tb]
  \begin{lstlisting}[language=java]
ArrayList markers;
if (layer == Layer.FOREGROUND) {
  markers = (ArrayList) this.fgMarkers.get(index);
}
else {
  markers = (ArrayList) this.bgMarkers.get(index);
}
// if (markers != null) { // <-- missing in misuse
boolean removed = markers.remove(marker);
// }
  \end{lstlisting}
  \caption{Example of an Analysis Problem of \GROUMiner.}
  \label{lst:GrouMiner-miss-df}
\end{figure}

\begin{obs}{e1-analysis}
  Imprecision of the analysis, which obscures the relation between patterns and misuses, causes \checkNum{9 (18.8\%) of the unexpected false negatives}.
\end{obs}

%\vspace{0.03in}
%\noindent {\bf 4. Boundary.}
% description
% The detectors confuse elements of multiple usages of the same API in the target code, i.e., they are not precise about the boundaries of individual usages.
% example
% For example, \DMMC assumes that all method calls on the same type within one method belong to one usage.
% If two objects of the same type occur in a method and the usage of only one of them misses a call to a method that is invoked on the other, \DMMC cannot detect this~\tn{Don't understand}.
%
% Another example is that \Jadet and \Tikanga cannot differentiate two sequential usages, if the second usage reuses the variable from the first, i.e., if the variable is reassigned.~\sn{hard to follow these examples.. any chance we have space for short code snippets?} Conceptually, they would have issues in dealing with interleaving patterns/usages as well.
%
% For example, \autoref{lst:boundary} shows one usage of \code{Cipher} in lines~\ref{line:u1-1} and \ref{line:u1-2} and another in lines~\ref{line:u2-1} and \ref{line:u2-2}.
% \DMMC sees only one usage here, because it generally assumes that all method calls on the same receiver type (here, \code{Cipher}) belong to the same usage.
% \Jadet and \Tikanga also see only one usage, because they assume that one variable (here, \code{cipher}) corresponds to one object and, therefore, one usage.~\sa{example clear now?}
% Conceptually, they have these issues with multiple sequential usages, as well as with interleaving usages.

%\begin{figure}[tb]
%  \begin{lstlisting}[language=java,escapechar=|]
%Cipher cipher = Cipher.getInstance(...);|\label{line:u1-1}|
%try {
%  cipher.init(Cipher.DECRYPT_MODE, publicKey);|\label{line:u1-2}|
%} catch (InvalidKeyException e) {
%  cipher = Cipher.getInstance(...);|\label{line:u2-1}|
%  cipher.init(Cipher.DECRYPT_MODE, privateKey);|\label{line:u2-2}|
%}
%  \end{lstlisting}
%  \caption{Two Usages of \code{Cipher} from \code{Alibaba/Druid}.}
%  \label{lst:boundary}
%\end{figure}

%\begin{obs}{e1-boundary}
%  Detectors need to separate usages more precisely, which requires a precise definition of the boundaries of usages.
%\end{obs}

\vspace{0.03in}
\noindent {\bf 4. Bug.}
% description
%A few false negatives are caused by mistakes in the implementations.
% examples
\DMMC skips the comparison of a usage and a pattern if the pattern contains fewer calls than the usage, presumably to improve performance.
The pattern for \code{AuthState} from Apache's \aName{HTTPClient}, for instance, requires three calls, of which the misuse scenario misses one.
However, if this misuse has an~additional, optional call  that is not in the pattern, \DMMC skips the comparison since now both the pattern and the target each contain 3 calls.
This causes two unexpected false negatives in our experiment.

\subsubsection*{Unexpected Hits}

\vspace{0.03in}
\noindent {\bf 1. Lenient.}
% description
In all but two cases, the reason for \emph{unexpected hits} is the lenient review process we use for \nameref{e1} (see \autoref{e1}).
% example
In most cases, the detectors report a missing call that indicates a missing condition check.
%
The only other case is that \GROUMiner detects a missing context condition, in a scenario where some \aName{swing} code is required to run on the Event-Dispatching Thread (EDT).
The delegation to the EDT is implemented by wrapping the code in an anonymous instance of \code{Runnable}, as shown in \autoref{lst:edt}.
\GROUMiner considers the code in \code{run()} as part of code of the enclosing method.
Consequently, it suggests the misuse by reporting a missing instantiation of \code{Runnable} before the instantiation of the \code{JFrame}.

\begin{figure}[tb]
  \begin{lstlisting}[language=java]
public static void main(String[] args) {
  SwingUtilities.invokeLater(new Runnable() {
    public void run() {
  	  JFrame f = new JFrame("Main Window");
      // add components...
      f.setVisible(true); 
    }
  });
}
  \end{lstlisting}
  \caption{Instantiating Swing Components on the Event-Dispatching Thread.}
  \label{lst:edt}
\end{figure}

\begin{obs}{e1-lenient}
  Missing method calls may indicate missing condition checks.
  Detectors that report these missing calls, despite not reporting the exact condition, find violations outside of their conceptual capabilities.
\end{obs}

\vspace{0.03in}
\noindent {\bf 2. Exception Handling.}
% description
% example
In the remaining two cases, \Jadet and \Tikanga correctly report missing exception handling.
For example, \autoref{lst:exc-handling} (left) shows a misuse where \code{close()} is not called when \code{write()} throws an exception.
A corresponding correct usage is shown on the right.
\Tikanga and \Jadet both represent the correct usage with two facts $\{ (\code{write}, \code{close}), (\code{write:EXC}, \code{close}) \}$, effectively encoding that \code{close()} is called after \code{write()} in normal execution and in case of an exception.
In the misuse, they find the second fact missing.
This capability of the implementation is not mentioned in the respective publications.

\begin{figure}[tb]
  \begin{subfigure}[t]{0.46\columnwidth}
    \begin{lstlisting}[language=java,numberblanklines=false,escapeinside=||,firstnumber=0]

writer.write(value);|\addtocounter{lstnumber}{-1}|

writer.close();
\end{lstlisting}
  \end{subfigure}
  \begin{subfigure}[t]{0.46\columnwidth}
    \begin{lstlisting}[language=java]
try {
  writer.write(value);
} finally {
  writer.close();
}
    \end{lstlisting}
  \end{subfigure}
  \caption{Closing \code{Writer} Without and With Exception Handling.}
  \label{lst:exc-handling}
\end{figure}

%\begin{obs}{e1-additional}
%  Some detector implementations find missing exception
%  handling, which is beyond the capabilities discussed in the
%  respective publications.
%\end{obs}

%\subsection*{Summary}
%
% However, we identify several unexpected false negatives whose root causes indiciate the need for more expressive usage representations (Capture), code-semantic-aware matching between patterns and targets (Relate), and more precise separation of usages, which entails a precise definition of the boundaries of patterns (Confuse).
%
% On the other hand, we find that missing method calls often indicate missing condition checks.
% Consequently, all detectors unexpectedly indicate misuses outside of their conceptual capabilities (Lenient).
% \sa{I feel we can draw a conclusion here, about cooccurrence of violations, for example, but I cannot put a finger on it right now...}



\subsection{\nameref{e3}}

In \nameref{e3}, we run all detectors to assess their recall without explicitly providing them with correct usages.
In addition to \MUBench's \checkNum{64 misuses}, we add the \checkNum{14 new misuses} from \nameref{e2} and exclude the \checkNum{25 hand-crafted examples} for which there is no project code to mine patterns from.
This leaves us with \checkNum{53 misuses} for \nameref{e3} (Row 5 of~\autoref{tab:datasets}).
%Ideally, a detector would find all these \checkNum{53 misuses}.

\autoref{tab:ex3-results} shows the results and \autoref{fig:recal-venn} visualizes the recall.
%
\Jadet finds only the \checkNum{three misuses} it already identified in \nameref{e2}.
%
\GROUMiner does not find any of the misuses.
%
\Tikanga finds the \checkNum{five misuses} it already identified in \nameref{e2}, \checkNum{one of the misuses} that \DMMC identified in \nameref{e2}, and \checkNum{one of the misuses} that \Jadet identified in \nameref{e2}.
%
\DMMC finds \checkNum{two misuses} from \MUBench (both missing method calls), the \checkNum{eight misuses} it reported in \nameref{e2}, and \checkNum{one misuse} both \Jadet and \Tikanga reported in \nameref{e2}.

\DMMC shows by far the best recall in \nameref{e3}.
This suggests that its relatively simple detection strategy works well when focusing on missing method calls.
However, the recall of all detectors in the realistic setting offered by \nameref{e3} is low.
Analyzing the root causes for their bad performance, we identify \checkNum{two general problems} with the design of the detectors and their evaluation setup.

\InlineSec{1. Ranking}
\nameref{e3} shows that the detectors identify additional misuses beyond their \checkNum{top-20 findings} that we considered in \nameref{e2}.
Unfortunately, they rank those misuses very low.
For example, the two \MUBench misuses \DMMC finds are ranked \checkNum{309} and \checkNum{613}.
This is far beyond the number of findings that we can reasonably expect a user to assess.
The four detectors in our experiments all use different ranking strategies, but none of the detectors from our survey in \autoref{sec:misuse-detectors-related-work} compared different strategies on the same detector.

\begin{obs}{rank}
  Detectors need better ranking strategies to report true positives within their top findings.
  Furthermore, researchers should compare alternative ranking strategies for single detectors.
\end{obs}

\InlineSec{2. Usage Examples}
 The huge difference in the detectors' performance between Experiments RUB and R suggests that the cause is a shortage of correct usage examples in the target projects.
%
One possibility is that the number of such examples is smaller than the detectors' minimal support for pattern mining, in which case we could simply lower these thresholds.
However, this would likely also increase the number of false positives as the mined patterns generally become less reliable, which underlines the need to effectively filter false positives (\obsref{e2-precision}) and improve ranking (\obsref{rank}).
%
Another possibility is that no, or only very few, such examples exist in the projects.
This would be a general problem with the evaluation setup of misuse detectors.
To solve it, we need additional sources of usage examples to mine patterns from.
Gruska~\etal~\cite{GWZ10} demonstrated one possible approach by applying \Jadet in a cross-project setting with 6,000 projects, but did not measure recall.
This strategy is also common in other recommender systems for software engineering, such as code-completion engines~\cite{PLM15}.
The misuse detectors \CARMiner~\cite{TX09} and \Alattin~\cite{TX09b} implement an alternative approach, by specifically searching for usage examples of the APIs used in the target project via a code-search engine.
Related to this, other lines of research proposed code-search engines to find usage examples in open source projects~\cite{GFXM+10,MGPXF11} or on StackOverflow~\cite{PBDO+14}.

\begin{table}[tb]
  \centering
  \small
  \caption{\nameref{e3}: Recall of the Detectors on \MUBench and the New Misuses from \nameref{e2}.}
  \input{sections/table-ex3-results}
  \label{tab:ex3-results}
\end{table}

\begin{obs}{e3-recall}
  All detectors have low recall, likely due to lack of correct usage examples in target projects.
  Adoption of existing code-search techniques and cross-project mining could mitigate this problem.
\end{obs}

The Kappa scores indicate mostly perfect reviewer agreement in \nameref{e3}.
This is because the detectors found almost exclusively the misuses that one of them also identified in \nameref{e2}, i.e., the misuses we already agreed on before.
The exception is \DMMC, where we initially disagreed on \checkNum{one} of its \checkNum{14} potential hits for misuses from the original \MUBench dataset.


%\subsection{Reviewer Agreement}

%Generally, the Kappa scores in all experiments indicate good reviewer agreement across detectors.\footnote{Landis and Koch~\cite{LK77} characterize values from $0.61$ to $0.80$ as substantial and from $0.81$ to $1$ as (almost) perfect agreement.
%Fleiss~\cite{F81} characterizes values over $0.75$ as excellent.}

\begin{figure}[tb]
  \centering
  \includegraphics[width=.75\columnwidth]{sections/diag-ex3-recall-eps-converted-to.pdf}
  \caption{Recall of the Detectors in \nameref{e3}}%~\sn{Really only 2 mubench misuses were found?}~\sa{Yes, sad but true. And those where even discovered by the simplest detector in our experiments...}
  \label{fig:recal-venn}
\end{figure}

\subsection{User Experience}

We now report on our experiences as users of our subject misuse detectors.
Our observations is based on the experience we gained while reviewing the detectors' findings in our experiments.

%
\DMMC simply reports present and missing method calls, along with the source line number of the first present call.
We find this output generally easy to interpret.
The line number helps, especially, to locate usages in large methods.
%
\GROUMiner reports pattern and usage graphs, which are more difficult to understand.
However, we find that the structural properties of the source code that the graph representation captures help with the interpretation.
%
\Jadet and \Tikanga report the present and missing facts of their respective representations.
We find that it is often difficult to relate the facts to each other, especially in the presence of multiple usages of the same API.
This might be, in part, due to the textual representation we look at.
%
While none of the detector implementations was intended to present their findings to end users, we still find it interesting to note that the challenge of explaining findings seems to correlate with the distance between the source code and the usage representation.

% \GROUMiner, which analyzes source code only,  reports a missing \code{toString()} call that is implicit in string concatenation.
% Byte-code based detectors, such as the other three subject detectors in our experiments, do not have these problems, since the compilation emits fully-qualified names and makes all calls explicit.
%
We also find that Bytecode-based detectors may report findings in code that the compiler introduces.
For example, the compiler translates foreach loops into \code{Iterator} usages.
\Tikanga reports a missing call in such a usage, i.e., it reports a missing call on \code{Iterator} in a method where \code{Iterator} does not appear in the source code.
This finding confused us at first.
While additional steps could be taken to assist the user in mapping such findings back to the source code, source-based detectors do not face this problem.

%\begin{obs}{source-vs-bytecode}
%  Source-based detectors may mistakenly report implicit program elements missing, while Bytecode-based detectors may require additional effort to map their findings back to the source~code.
%\end{obs}

Our lenient review process shows that missing method calls frequently indicate missing conditions (\obsref{e1-lenient} and \obsref{e2-misuses-allCSV}).
While such findings do not report the entire problem, we found it relatively easy to deduce their meaning.
In contrast, \GROUMiner reports only a missing \code{if} node, when it captures a missing condition.
While these findings more explicitly indicate the problem of a missing check, we feel that they are actually harder to act upon, because they give no information about \emph{what} should be checked.
This indicates a gap between a detector's capability to find a violation type and its ability to explain respective violations to users.

Above all, we believe that the detectors' precision is likely to be the biggest threat to their applicability in practice.
As a previous study by Johnson~\etal~\cite{JS13} shows, large numbers of false positives are a major barrier in the adoption of code analysis tools.
This problem is made worse by the low recall of the detectors. 
Even if developers would take the time to review all reported warnings, they would still likely miss the vast majority of misuses.

%\begin{obs}{explain}
%  Detectors need mechanisms to explain the misuses to users.
%\end{obs}

\subsection{Call to Action}

We find that misuse detectors are practically capable of detecting a considerable part of the misuses in \MUBench, when provided with the correct usages to compare to (\nameref{e1}).
However, even though the detectors are also capable of finding some misuses in a realistic setting (Experiments P and R), they suffer from extremely low precision (\obsref{e2-precision}) and recall (\obsref{e3-recall}).
%
We identify \checkNum{four root causes} for false negatives, \checkNum{seven root causes} for false positives, and \checkNum{two general problems} with the design of detectors and how they are typically evaluated.
This leads us to several observations on how to advance the state-of-the-art in API-misuse detection.
Therefore, we call researchers to action:

\begin{itemize}
  % definition of usage
  \item We first need a precise definition of API usages, considering usage properties, such as the usage location~(\obsref{e2-inside}) and call multiplicities~(\obsref{e2-multiplicity}).
  % representation of usage
  \item We need a representation of such usages that captures all code details necessary to distinguish correct usages from misuses (\obsref{e1-capture}) and more precise analyses to identify usages in code (\obsref{e1-analysis} and \obsref{e2-analysis}).
  % pattern mining
  \item We need detectors that retrieve sufficiently many usage examples using project-external sources, such as large project sets or code-search engines~(\obsref{e3-recall}).
  % misuse detection
  \item We need detectors that go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse (\obsref{e2-frequent}), but consider program semantics, such as type hierarchies~(\obsref{e1-relate}) and implicit dependencies between objects (\obsref{e2-dependent}).
  We hypothesize that probabilistic models might be a way to tackle this problem.
  \item We need strategies to properly match patterns and usages in the presence of violations~(\obsref{e1-capability-gap} and \obsref{e1-relate}).
  \item We need strategies to properly handle alternative patterns for the same API (\obsref{e2-alternative}).
  %ranking
  \item Finally, we need good ranking strategies, to reduce the cost of reviewing findings~(\obsref{rank}).
  % studies
\end{itemize}

In order to achieve all this, we need repeatable and replicable studies that enable systematic evaluation and analysis of alternative approaches and strategies.
We publish \MUBench and \MUPipe~\cite{mubench}, as a foundation for such work, and call researchers to use and contribute to this infrastructure, to advance the state of the art in API-misuse detection.
