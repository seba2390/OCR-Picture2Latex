%% LaTeX Template for ISIT 2021
%%
%% by Stefan M. Moser, October 2017
%%
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

% For ISIT version.
\documentclass[conference,letterpaper]{IEEEtran}

% depending on your installation, you may wish to adjust the top margin:
% \addtolength{\topmargin}{9mm}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\interdisplaylinepenalty=2500% As explained in bare_conf.tex
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[prependcaption,disable]{todonotes}
\presetkeys%
    {todonotes}%
    {inline}{}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{grffile}% To solve the "unknown graphic extension" issue.
\graphicspath{{figures/}}
\usepackage{subcaption}
\usepackage{bbm}
\usepackage{url}
\usepackage{etoolbox}
\newtoggle{isit}
% \toggletrue{isit}
\iftoggle{isit}{}{%
  \usepackage{hyperref}
}%
\usepackage[shortlabels]{enumitem}

\DeclareMathOperator*{\minimize}{minimize}%
\DeclareMathOperator*{\sgn}{sgn}%
\DeclareMathOperator*{\supp}{supp}%
\DeclareMathOperator*{\argmin}{arg\,min}%
\DeclareMathOperator*{\hessian}{H}%
\newcommand{\defeq}{\triangleq}
\newcommand{\reals}{\mathbb{R}}% Set of real numbers.
\newcommand{\naturalnumbers}{\mathbb N}% Natural numbers.
\renewcommand{\Pr}{\operatorname{\mathbb P}}% Probability.
\newcommand{\E}{\operatorname{\mathbb E}}% Mathematical expectation.
\newcommand{\law}{\operatorname{\mathcal L}}% Law.
\newcommand{\fedeq}{=:}
\newcommand{\ind}[2]{\left(#1\colon #2\right)} % Indexed family.
\newcommand{\cov}{\operatorname{Cov}}% Covariance.
\newcommand{\sigmabi}{\sigma_{\mathrm{in}}}
\newcommand{\sigmaco}{\sigma_{\mathrm{en}}}
\newcommand{\sigmate}{\sigma_{\mathrm{te}}}
\newcommand{\sigmabii}{\sigma_{\mathrm{in}, i}}
\newcommand{\sigmacoi}{\sigma_{\mathrm{en}, i}}
\newcommand{\sigmatei}{\sigma_{\mathrm{te}, i}}
\newcommand{\steSq}{\sigma_{\mathrm{te}}^2}%
\newcommand{\intNoisebi}[2]{W_{\mathrm{in}, #1}^{#2}}% Internal individual noise.
\newcommand{\intNoiseco}[2]{W_{\mathrm{en}, #1}^{#2}}% Internal environmental noise.
\newtheorem{exam}{Example}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newcommand{\biol}{individual}
\newcommand{\cond}{environmental}
\newcommand{\T}{^{*}}% Matrix transpose.
\newcommand{\indicator}[1]{\mathbb{I}(#1)}% Indicator function.
\newcommand{\set}[2]{\left\{#1\colon #2\right\}}% Set builder.
\renewcommand{\d}{\mathop{}\!\mathrm{d}}% by egrep at http://tex.stackexchange.com/questions/5511/good-practice-on-spacing
\newcommand{\detHes}{\phi}%
\newcommand{\bcSq}{\rho^2}%
\newcommand{\gramian}{g}%
\newcommand{\avgbc}{\frac 1{n^2}\sum_{i, j}\rho_{ij}^2}%
\newcommand{\trueAdjMatTer}{A_{\mathsf{sign}}^0}% True ternary adjacency matrix.
\newcommand{\trueAdjMat}{A^0}%
\newcommand{\erPrior}{\pi^0_{\mathsf{ER}}}%
\newcommand{\specRad}{r}% Spectral radius.
\newcommand{\figWidth}{0.48\textwidth}%
\newcommand{\subfigWidth}{0.45\textwidth}%
\newcommand{\sigmat}{\nu}% Technical standard deviation.
\newcommand{\sigmab}{\sigma}% Biological standard deviation.
\newcommand{\meanInit}{\mu}% Initial mean of system state.
\newcommand{\covInit}{Q}% Initial covariance matrix of system state.
% Note some occurrences do not use the following two commands.
\newcommand{\distA}{f}%
\newcommand{\distB}{g}%
\newcommand{\edgeFNR}{P^-}%
\newcommand{\edgeFPR}{P^+}%
\newcommand{\pmfPrior}{q}%
\newcommand{\fnrNet}{\epsilon^-}%
\newcommand{\fprNet}{\epsilon^+}%
\newcommand{\edgeCoeff}{a}%
\newcommand{\mixWeight}{\alpha}%
\newcommand{\trueSuppMat}{\chi^0}%
\newcommand{\suppMat}{\chi}%
\newcommand{\suppMatEst}{\widehat\chi}%
\newcommand{\fnrHT}{P^-}%
\newcommand{\fprHT}{P^+}%
\newcommand{\nsr}{N}% Noise signal ratio.
\newcommand{\LBDirect}{\mathsf{LB}_{\text{direct}}}%
\newcommand{\LBSide}{\mathsf{LB}_{\text{side-info}}}%
\newcommand{\meanDiff}{\Delta\mu}%
\newcommand{\uiuc}{University of Illinois at Urbana–Champaign}
\newcommand{\errorOpt}{p_e^{\mathrm{opt}}}
\newcommand{\piComp}{\bar\pi}
\newcommand{\alphaComp}{\bar\alpha}

\begin{document}
\listoftodos
% \clearpage
% \tableofcontents

\title{Lower Bounds on Information Requirements for Causal Network Inference\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
\thanks{This material is based upon work supported by the National Science Foundation under Grant No.\ CCF 19-00636.}}

% %%% Single author, or several authors with same affiliation:
\author{%
  \IEEEauthorblockN{Xiaohan Kang and Bruce Hajek}
  \IEEEauthorblockA{\uiuc\\
                    Electrical and Computer Engineering and Coordinated Science Laboratory\\
                    Urbana, Illinois\\
                    Email: xiaohan.kang1@gmail.com, b-hajek@illinois.edu}
}

\maketitle

\begin{abstract}
  Recovery of the causal structure of dynamic networks from noisy
  measurements has long been a problem of intense interest across many
  areas of science and engineering.  Many algorithms have been
  proposed, but there is no work that compares the performance of the
  algorithms to converse bounds in a non-asymptotic setting.  As a
  step to address this problem, this paper gives lower bounds on the
  error probability for causal network support recovery in a linear
  Gaussian setting.  The bounds are based on the use of the
  Bhattacharyya coefficient for binary hypothesis testing problems
  with mixture probability distributions.  Comparison of the bounds
  and the performance achieved by two representative recovery
  algorithms are given for sparse random networks based on the
  Erdős–Rényi model.
\end{abstract}

\iftoggle{isit}{%
\textit{A full version of this paper is accessible at:}
\url{https://arxiv.org/pdf/2102.00055.pdf}
}{}%

% \begin{IEEEkeywords}
%   System identification, Bhattacharyya coefficient, hypothesis testing, mixture distribution
% \end{IEEEkeywords}

\section{Introduction}
Causal networks refer to the directed graphs representing the causal
relationships among a number of entities, and the inference of sparse
large-scale causal networks is of great importance in many scientific,
engineering, and medical fields.  For example, the study of gene
regulatory networks in biology concerns the causal interactions
between genes and is vital for finding pathways of biological
functions.  Because of the scale of these networks, inference often
cannot be carried out for specific ordered pairs of the vertices
without significant prior knowledge about the networks.  Instead, it
is desirable to infer the sparse structure from observations on all
the vertices.  Time-series observations are especially useful due to
the nature of causality.  The problem of causal network inference is
then typically formulated as a sparse support recovery problem from
time-series vertex data.

Numerous algorithms have been applied to the problem of causal network
inference, and their performance have been evaluated using both
generative models with ground truths and real data with putative
truths (see, e.g., \cite{MarbachCostelloKuffner12} for gene regulatory
network reconstruction), but there is little work that studies the
theoretical converse bounds of the minimum information requirements.
The work \cite{SunTaylorBollt15} lays a
theoretical foundation for causal network inference by studying a
general dynamic Markovian model, and proposes the oCSE algorithm which
is shown to find the causal structure of the network when the exact
causation entropy information is available.  However, such information
is often unavailable due to the limited amount of data and noise in
the observations.

Motivated by \cite{SunTaylorBollt15}, as a first step to understand
the fundamental information requirements, we study the linear discrete
stochastic network in \cite{SunTaylorBollt15} as a special case of the
general Markovian model.  Unlike \cite{SunTaylorBollt15}, we consider
observation noise on the time-series measurements.

To get lower bounds on the error probability, we apply lower bounds
for binary hypothesis testing (BHT) based on the Bhattacharyya
coefficient (BC), which measures the similarity between two
probability distributions.  In addition, we use the fact that when
there is side information that has the same distribution under either
hypothesis, the conditional error probabilities given the side
information can be lower bounded using the BC for the conditional
distributions and then averaged to yield a lower bound for the
original BHT problem.
% \todo{Explain our partial recovery criterion and that the lower bounds
%   also apply to exact recovery.}

The contributions of this paper are three-fold.  First, two lower
bounds on error probabilities, a direct one for general hypothesis
testing and one based on side information for mixture hypothesis
testing, are given.  Second, the lower bound based on side information
is applied to the dynamic Erdős–Rényi (ER) networks from
\cite{SunTaylorBollt15} (see Proposition~\ref{prop:er}).  Third, the
lower bound based on side information is numerically compared with the
performance of lasso and oCSE\cite{SunTaylorBollt15}.

Problems similar to the causal network inference in this paper have
been studied in various settings, but nothing on converse bounds is
known for a non-asymptotic regime.  In a linear system identification
(i.e., a vector autoregression model) setting, the upper bound of this
problem was recently studied in \cite{SimchowitzManiaTu18}, with
sparsity constraint in \cite{FattahiSojoudi18a,FattahiMatniSojoudi19},
with observation noise in \cite{OymakOzay19}, and in a closed-loop
setting in \cite{LaleAzizzadenesheliHassibi20}, and in both
discrete-time and continuous-time settings in
\cite{BentoIbrahimiMontanari10}.  Notably, the mutual incoherence
property (see
\cite{Fuchs05,Tropp06,ZhaoYu06,MeinshausenBuhlmann06,Wainwright09}) is
often used in upper bound analysis.  Lower bounds for exact recovery
in asymptotic settings have been studied in
\cite{JedraProutiere19,BentoIbrahimiMontanari11,PerieraIbrahimi14}.
% This assumption ($\mathsf{MIP} < 1$ in (14) of
% \cite{FattahiSojoudi18a}) is shown to be satisfied on certain matrix
% families (see \cite{MeinshausenBuhlmann06,ZhaoYu06}), but does not
% hold in general for time series of dynamic ER graphs with sufficiently
% large number of times and large spectral radius, as can be seen in
% \iftoggle{isit}{%
%   Appendix~A of the technical report \cite{KangHajek21}.  }{%
%   Appendix~\ref{app:mip}.  }%
% \todo{Call the condition ERC instead of MIP?}
  The causal inference problem is also
closely related to compressed sensing, but unlike compressed sensing
it has an unknown design matrix (the time series of system states).

The organization of this paper is as follows.  Section~\ref{sec:model}
introduces the model of the causal network inference problem.
Section~\ref{sec:direct} gives a direct lower bound on the error
probability based on the BC for hypothesis testing and applies it to
the network setting.  Section~\ref{sec:side-info} presents the lower
bound based on side information, and Section~\ref{sec:er-app} applies
this bound to the dynamic ER networks.  Section~\ref{sec:num} shows
the numerical comparison of the lower bound and two representative
algorithms.

\section{Model}
\label{sec:model}
\subsection{Network dynamics}
Let $n$ be the number of network vertices and
$\trueAdjMat\in\reals^{n\times n}$ be the random weighted adjacency
matrix of the network with a prior distribution $\pi^0$.  Let $X(t)$
be an $n$-dimensional random row vector representing the system state
at time $t\in\{0, 1, 2, \dots, T\}$.  Assume
$X(0) \sim \mathcal N(\meanInit, \covInit)$ and
\[X(t) = X(t - 1)\trueAdjMat+W(t),\quad t = 1, 2, \dots, T,\]
where $W(t)\sim\mathcal N(0, \sigmab^2I)$ are independent driving
noises with variance $\sigmab^2$.  The noisy observations are
\[Y(t) = X(t)+Z(t),\quad t = 0, 1, \dots, T,\]
where $Z(t)\sim\mathcal N(0, \sigmat^2I)$ are observation noises with
variance $\sigmat^2$.  The observations
$Y = (Y(0), Y(1), \dots, Y(T))\in\reals^{n(T+1)}$ are jointly Gaussian
given $\trueAdjMat$.  The goal is to recover the support matrix
$\trueSuppMat$ from the obervations $Y$, where $\trueSuppMat$ is
defined by $\trueSuppMat_{ij} = 0$ if $\trueAdjMat_{ij} = 0$ and
$\trueSuppMat_{ij} = 1$ if $\trueAdjMat_{ij}\neq 0$.  This setting is
the same as the linear discrete stochastic network dynamics in
\cite{SunTaylorBollt15} and the discrete-time model in
\cite{BentoIbrahimiMontanari10}.  However the theoretical results in
\cite{SunTaylorBollt15} and \cite{BentoIbrahimiMontanari10} do not
consider observation noise.

To be definite, for the examples considered in this paper, two more
restrictions are imposed on the prior distribution $\pi^0$ and the
initial distribution $\mathcal N(\meanInit, \covInit)$.  Let
$\specRad(A)$ denote the spectral radious of $A$, defined by
$\specRad(A) \defeq\max_{x\in\reals^n\colon\|x\|_2 = 1}|xAx\T|$, where
$x\T$ denotes the transpose of $x$.  We assume
$\specRad(\trueAdjMat) < 1$ for stability.  Without such a stability
condition data at later time points could have much higher
signal-to-noise ratios, making early time points relatively useless.
We also assume the process $\ind{X(t)}{t\in\{0, 1, \dots, T\}}$ is
stationary; i.e., $\meanInit = 0$ and $\covInit$ satisfies
$\covInit = (\trueAdjMat)\T\covInit\trueAdjMat + \sigmab^2I$.
\subsection{Performance metrics}
In this section we define the performance metrics of the network
inference problem, and relate them to error probabilities for
testing hypotheses about the existence of individual edges.

We first define the network-level error probabilities.  Let
$\suppMatEst \colon\reals^{n(T+1)}\to\{0, 1\}^{n\times n}$ be the
support matrix estimator based on the observation $Y$.  Let
$\indicator{\cdot}$ be the indicator function.  On the network level,
following \cite{SunTaylorBollt15}, we define the false negative ratio
$\fnrNet$ and the false positive ratio $\fprNet$ for a given network
prior $\pi^0$ and an estimator $\suppMatEst$ by
\begin{equation}
  \label{eq:fnr}
  \fnrNet \defeq \frac{\E\sum_{i, j}\indicator{\{\trueAdjMat_{ij}\neq
      0, \suppMatEst_{ij}(Y) = 0\}}}{\E\sum_{i,
      j}\indicator{\{\trueAdjMat_{ij}\neq 0\}}},
\end{equation}
\begin{equation}
  \label{eq:fpr}
  \fprNet \defeq \frac{\E\sum_{i, j}\indicator{\{\trueAdjMat_{ij} = 0,
      \suppMatEst_{ij}(Y) = 1\}}}{\E\sum_{i,
      j}\indicator{\{\trueAdjMat_{ij} = 0\}}},
\end{equation}
provided the denominators are positive.  Here the summations are over
all ordered pairs, including the self-pairs.

Now we define the edge-level error probabilities.  For an ordered pair
$(i, j)$ given the prior $\pi^0$ on $\trueAdjMat$ and an estimator
$\suppMatEst$, the recovery of $\trueSuppMat_{ij}$ is a BHT problem
with the probability of miss and the probability of false alarm given
by
\begin{equation}
  \label{eq:fnr-edge}
  \edgeFNR_{ij} \defeq\Pr(\suppMatEst_{ij}(Y) = 0\mid\trueSuppMat_{ij} = 1)
\end{equation}
and
\begin{equation}
  \label{eq:fpr-edge}
  \edgeFPR_{ij}\defeq\Pr(\suppMatEst_{ij}(Y) = 1\mid\trueSuppMat_{ij} = 0).
\end{equation}
\begin{prop}
  \label{prop:network-edge}
  The network-level error probabilities are convex combinations of the
  edge-level error probabilities:
  \[\fnrNet = \sum_{i, j}\edgeFNR_{ij}w_{ij}^-,\quad\fprNet = \sum_{i,
      j}\edgeFPR_{ij}w_{ij}^+,\]
  where
  \[w_{ij}^- \defeq\frac{\Pr\{\trueAdjMat_{ij}\neq 0\}}{\sum_{k,
        l}\Pr\{\trueAdjMat_{kl}\neq 0\}},\quad w_{ij}^+\defeq
    \frac{\Pr\{\trueAdjMat_{ij} = 0\}}{\sum_{k,
        l}\Pr\{\trueAdjMat_{kl} = 0\}}.\]
\end{prop}
The proof of Proposition~\ref{prop:network-edge} follows immediately
by exchanging the summation and expectation in the numerators and the
denominators in \eqref{eq:fnr} and \eqref{eq:fpr}.
Proposition~\ref{prop:network-edge} implies in order to study the
network-level error probabilities it suffices to study the edge-level
error probabilities.

\begin{remark}
  The quantities $\fnrNet$, $\fprNet$, $w_{ij}^-$, and $w_{ij}^+$ can
  be interpreted as limits, assuming the number of instances of the
  support recovery problem converges to infinity. First, $\fnrNet$ is
  the limiting ratio of the number of false negatives (edges in the
  ground truth that are missed in the prediction) to the total number
  of edges in the ground truth.  Similarly, $\fprNet$ is the limiting
  ratio of the number of false positives (predicted edges that are not
  in the ground truth) to the total number of ordered pairs with no
  edges.  Likewise, the weight $w_{ij}^-$ is the limiting fraction of
  edges that appear on the ordered pair $(i, j)$ out of all edges, and
  $w_{ij}^+$ is the limiting fraction of non-edges on $(i, j)$ that
  appear out of all non-edges.
\end{remark}
\begin{remark}
  While one can alternatively define $\fnrNet$ and $\fprNet$ in
  \eqref{eq:fnr} and \eqref{eq:fpr} by taking the expectation of the
  ratios rather than the ratios of the expectations, the presented
  definitions do not get overly dominated by the variation of the
  denominators, and the denominators might even be zero.  In
  \cite{SunTaylorBollt15} the two quantities were originally defined
  for a pair of true and predicted networks.
\end{remark}
\begin{remark}
  \label{rem:symm}
  The weights $w_{ij}^-$'s and $w_{ij}^+$'s are determined by the
  prior $\pi^0$.  If the network prior $\pi^0$ is symmetric in the
  sense a) it is invariant under vertex permutation; and b)
  $\Pr\{\trueAdjMat_{11} = 0\} = \Pr\{\trueAdjMat_{12} = 0\}\in(0,
  1)$, then $w_{ij}^- = w_{ij}^+ = \frac 1{n^2}$.
\end{remark}
\begin{remark}
  Note \eqref{eq:fnr} and \eqref{eq:fpr} weigh the self-edges and the
  other edges equally, whereas they could be weighted differently, or
  self-edges could be excluded.
\end{remark}

\section{Direct lower bounds on error probability}
\label{sec:direct}
In this section we apply error bounds for BHT based on the BC directly
to obtain sample complexity lower bounds on the network inference
problem.

\subsection{Binary hypothesis testing}
\label{sec:bht}
For background, this section presents useful bounds from the
theory of detection that are used to provide lower bounds on the
network-level error probabilities $\fnrNet$ and $\fprNet$.

Consider a BHT problem with prior probabilities $\pi$ for $H_0$ and
$1 - \pi$ for $H_1$.  Suppose that the observation $Y\in\mathcal Y$
has probability density function (pdf) $\distA$ under hypothesis $H_0$
and $\distB$ under hypothesis $H_1$.  For any decision rule
$\delta\colon \mathcal Y \to \{0, 1\}$, let the probability of miss
and the probability of false alarm be
\[\fnrHT_\delta \defeq\int g(y)(1 - \delta(y))\d y \mbox{ and }
  \fprHT_\delta\defeq \int f(y)\delta(y)\d y.\]
The average error probability is
$\pi\fnrHT_\delta + (1 - \pi)\fprHT_\delta$.  A decision rule is Bayes
optimal (i.e.\ minimizes the average error probability) if and only if
it minimizes $\pi\distA(y)\delta(y) + (1-\pi)\distB(y)(1-\delta(y))$
for each $y$.  The corresponding minimum average error probability is
given by
\begin{equation}
  \label{eq:avg-prob-error}
  \errorOpt = \int (\pi\distA(y)) \wedge ((1 - \pi)\distB(y))\d y,
\end{equation}
where $a\wedge b \defeq \min\{a, b\}$.  Let the BC for a pair of
continuous probability distributions with pdfs $\distA$ and $\distB$
be defined by:
\[\rho(\distA, \distB) \defeq \int\sqrt{\distA(y) \distB(y)}\d y.\]
We shall often omit the arguments $\distA$ and $\distB$ when they are
clear from the context.  Note the BC is related to the Hellinger
distance $H(f, g)$ by $H(f, g) = \sqrt{1 - \rho}$.  For two jointly
Gaussian distributions with means $\mu_1$ and $\mu_2$ and covariance
matrix $\Sigma_1$ and $\Sigma_2$ it is known that
$\rho = \rho_G((\mu_1, \Sigma_1), (\mu_2, \Sigma_2))$ is given by
\begin{align*}
  \exp\left(-\frac 18 (\meanDiff)\T\left(\frac{\Sigma_1 +
  \Sigma_2}2\right)^{-1}(\meanDiff)\right) \cdot\sqrt{\frac{\sqrt{\det
  \Sigma_1\det\Sigma_2}}{\det\frac{\Sigma_1 + \Sigma_2}2}},
\end{align*}
where $\meanDiff = \mu_1 - \mu_2$.

The following lemma goes back at least as far as a report of Kraft in
the 1950's (see \cite{Kailath67}).\\[-1em]
\begin{lemma}
  \label{lem:bhatta-bounds}
  (a) For any two distributions $\distA$ and $\distB$, the minimum
  average error probability for the BHT problem with priors
  $(\pi, 1 - \pi)$ satisfies
  {\small
    \[\pi(1 - \pi)\rho^2 \le \frac 12(1 - \sqrt{1-4\pi(1 - \pi)\rho^2})
      \le \errorOpt \le \sqrt{\pi(1 - \pi)}\rho.\]
  }
  (b) The BC for tensor products is the product of the BCs:
  \[\rho(\otimes_{j = 1}^n \distA_j, \otimes_{j = 1}^n \distB_j) =
    \prod_{j = 1}^n \rho(\distA_j, \distB_j),\]
  where the $\distA_j$ and $\distB_j$ are pdfs.
\end{lemma}
A proof of Lemma~\ref{lem:bhatta-bounds} is shown in
\iftoggle{isit}{%
the technical report \cite{KangHajek21}.
}{%
Appendix~\ref{app:bhatta-bounds}.
}%

\subsection{Direct Bhattacharyya bound for network inference}
Now we return to the model in Section~\ref{sec:model}.  Let
$f_{Y\mid\trueAdjMat_{ij} = 0}$ and $f_{Y\mid\trueAdjMat_{ij}\neq 0}$
be the conditional pdfs of $Y$ given there is not an edge from $i$ to
$j$ or there is such an edge, respectively.  We have the following
bound on the average network-level error probability.\\[-1em]
\begin{prop}
  \label{prop:bc-mixed}
  For any estimator $\suppMatEst$ and any $\pi\in[0, 1]$,
  {\small
  \begin{align*}
    \label{eq:bc-mixed}
    \pi\fnrNet + (1 - \pi)\fprNet \ge \frac 12\sum_{i, j}\left(1 -
    \sqrt{1 - 4\pi(1-\pi)\rho_{ij}^2}\right)(w_{ij}^-\wedge w_{ij}^+),
  \end{align*}
  }%
  where $\rho_{ij} \defeq\rho(f_{Y\mid\trueAdjMat_{ij} \neq 0},
  f_{Y\mid\trueAdjMat_{ij} = 0})$, and $w_{ij}^-$ and $w_{ij}^+$ are
  defined in Proposition~\ref{prop:network-edge}.
\end{prop}
\begin{IEEEproof}
  Proposition~\ref{prop:network-edge} yields
  \begin{align*}
    \pi\fnrNet + (1-\pi)\fprNet & = \sum_{i, j}\pi\edgeFNR_{ij}w_{ij}^- + (1-\pi)\edgeFPR_{ij}w_{ij}^+\\
    & \ge \sum_{i, j}(\pi\edgeFNR_{ij} + (1-\pi)\edgeFPR_{ij})(w_{ij}^-\wedge w_{ij}^+).
  \end{align*}
  and $\pi\edgeFNR_{ij} + (1-\pi)\edgeFPR_{ij}$ is the average error
  probability for testing $\trueSuppMat_{ij} = 0$ vs.\
  $\trueSuppMat_{ij} = 1$.  Applying Lemma~\ref{lem:bhatta-bounds} to
  bound each of those terms completes the proof.
\end{IEEEproof}
We illustrate Proposition~\ref{prop:bc-mixed} with a simple example.
\begin{exam}
  Consider the following prior for network size $n = 2$.  With
  probability $\beta$ there is a single edge of coefficient
  $\edgeCoeff\neq 0$ from vertex $1$ to vertex $2$, and with
  probability $1-\beta$ there are no edges in the network.  In other
  words, $\pi^0$ is given by
  $\pi^0\left(\left\{A_0\right\}\right) = 1-\beta$ and
  $\pi^0\left(\left\{A_1\right\}\right) = \beta$, where
  $A_0 = \begin{pmatrix} 0 & 0\\0 & 0
  \end{pmatrix}$ and $A_1 = \begin{pmatrix}
    0 & \edgeCoeff\\0 & 0
  \end{pmatrix}$.

  Let $\nsr = \sigmat^2 / \sigmab^2$.  By the formula of $\rho_G$, the
  BC of the observations given the two possible networks as
  \begin{equation}
    \label{eq:two-by-two}
    \rho(f_{Y\mid \trueAdjMat = A_0}, f_{Y\mid \trueAdjMat = A_1}) =
    \rho_0\gamma^T,
  \end{equation}
  where
  \[\rho_0 \defeq \sqrt{\frac{\sqrt{(\nsr + 1)(\nsr + 1 +
          \edgeCoeff^2)}}{\nsr + 1 + \frac{\edgeCoeff^2}2}}\]
  and
  \[\gamma \defeq \left(\frac{(\nsr+1)\sqrt{\nsr^2 +
          (2+\edgeCoeff^2)\nsr + 1}}{\nsr^2 + (2+\frac{\edgeCoeff^2}2)\nsr + 1 +
        \frac{\edgeCoeff^2}4}\right)^{1/2}.\]
  Indeed, note that $Y_1(T)$, $Y_2(0)$, and the pairs
  $\ind{(Y_1(t-1), Y_2(t))}{1\le t\le T}$ are all mutually
  independent.  Then by Lemma~\ref{lem:bhatta-bounds}(b), the BC of
  $Y$ under the two hypotheses tensorizes to the product of BCs of the
  independent components.  The BC of $Y_2(0)$ is the first
  multiplicative term in \eqref{eq:two-by-two}, and the BCs of the
  i.i.d.\ pairs form the other term.  The random variable $Y_1(T)$ is
  identically distributed under either hypothesis so its BC is 1.

  For the given $\pi^0$ we can get $w^- =
  \begin{pmatrix}
    0 & 1\\0 & 0
  \end{pmatrix}$ and $w^+ =
  \displaystyle\frac 1{4-\beta}\begin{pmatrix}
    1 & 1-\beta\\1 & 1
  \end{pmatrix}$.  Then by Proposition~\ref{prop:bc-mixed}, for any $\pi\in[0, 1]$,
  \[\pi\fnrNet + (1-\pi)\fprNet \ge \frac 12\left(1 - \sqrt{1 -
        4\pi(1-\pi)\rho_0^2\gamma^{2T}}\right)\frac{1 - \beta}{4 - \beta}.\]
\end{exam}

\section{Lower bounds on error probability with side information}
\label{sec:side-info}
The direct Bhattacharyya bound in Proposition~\ref{prop:bc-mixed} can
however be hard to compute or estimate because of the integration
involving high-dimensional pdfs.  In this section we give a different
bound based on the side information that acts like a switch random
variable.  With proper choice of the side information, this new bound
can be easily estimated numerically.

Consider the same setting as in Section~\ref{sec:bht} with the
additional condition $f = \sum_{s = 1}^d\mixWeight_sf_s$ and
$g = \sum_{s = 1}^d\mixWeight_sg_s$, where $f_s$ and $g_s$ are pdfs
for any $s\in[d]\defeq\{1, 2, \dots, d\}$ and
$\ind{\mixWeight_s}{s\in[d]}$ satisfies
$\sum_{s = 1}^d\mixWeight_s = 1$ and $\mixWeight_s \ge 0$ for any $s$.
Recall by Lemma~\ref{lem:bhatta-bounds} we have
$\errorOpt \ge \LBDirect$, where $\LBDirect$ is the lower bound based
on the mixture distributions alone without decomposing them into
mixture components and is given by
\begin{equation}
  \label{eq:bhatta-mix-bound}
  \LBDirect \defeq\frac 12\left(1 - \sqrt{1-4\pi(1 - \pi)\rho^2(f, g)}\right).
\end{equation}
The following proposition gives a lower bound using the mixture
representation.\\[-1em]
\begin{prop}
  \label{prop:bc-coupling}
  For any $\pi\in[0, 1]$, we have $\errorOpt \ge \LBSide$, where
  \begin{equation}
    \label{eq:bhatta-comp-bound}
    \LBSide \defeq\frac 12\left(1 - \sum_s\mixWeight_s\sqrt{1 - 4\pi(1
        - \pi)\rho^2(f_s, g_s)}\right).
  \end{equation}
\end{prop}
\begin{IEEEproof}
  Let $S$ be a random variable such that, under either hypothesis,
  $\Pr\{S = s\} = \alpha_s$ for $s\in [d]$.  Suppose $X$ is jointly
  distributed with $S$ such that the conditional pdf of $X$ given
  $S = s$ is $f_s$ under $H_0$ and is $g_s$ under $H_1$.  Then $S$ can
  be thought of as a switch variable or side information.  For each
  $s\in [d]$, Lemma~\ref{lem:bhatta-bounds} applied to the BHT problem
  with densities $f_s$ and $g_s$ yields that the conditional average
  error probability, given $S = s$, is lower bounded by
  $\LBDirect(f_s, g_s)$.  Averaging these lower bounds using
  $\ind{\alpha_s}{s\in [d]}$ yields \eqref{eq:bhatta-comp-bound}.  The
  lower bound applies to decision rules $\delta$ that depend on both
  $S$ and $Y$, so it applies to decision rules depending on $Y$ alone.
\end{IEEEproof}

Numerical comparisons of $\LBDirect$, $\LBSide$, and $\errorOpt$
under the uniform prior $\pi = 1/2$ are shown in Fig.~\ref{fig:conj}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\figWidth]{conjecture-d2-n10.eps}
  \caption{Numerical comparison of lower bounds with $d = 2$, $X$
    taking 10 values, and $\pi = 1/2$.}
  \label{fig:conj}
\end{figure}%
For each trial, $d = 2$ and $\mixWeight_1$ is drawn from $[0, 1]$
uniformly at random, and $\mixWeight_2 = 1 - \mixWeight_1$
(equivalently, $(\mixWeight_1, \mixWeight_2)$ follow the 2-dimensional
symmetric Dirichlet distribution).  The distributions $f_1$, $f_2$,
$g_1$, $g_2$ follow the $10$-dimensional symmetric Dirichlet
distribution independently.  All these distributions are generated
independently for each trial.  The trials are sorted in the increasing
order of $\LBDirect$.  It is shown that $\LBDirect$ is usually better
than $\LBSide$ for random mixtures.  Note the following example shows
this is not always the case.  Equivalently, it shows that $\LBDirect$
is not a concave function of $(f, g)$.
\begin{exam}
  \label{exam:lbd-nonconcave}
  Consider discrete distributions with probability mass functions
  $f = (\frac 13, \frac 13, \frac 13)$ and
  $g = (\frac 12, \frac 13, \frac 16)$.  Note the two distributions
  can be decomposed into mixtures as
  \[f = \frac 23\left(\frac 12, 0, \frac 12\right) + \frac 13(0, 1, 0),\]
  \[g = \frac 23\left(\frac 34, 0, \frac 14\right) + \frac 13(0, 1,
    0).\]
  Then for $\pi = 1/2$, we have $\LBDirect \approx 0.4246$ and
  $\LBSide \approx 0.4385$.  So $\LBDirect < \LBSide$.
\end{exam}
\section{Application to dynamic ER networks}
\label{sec:er-app}
This section presents the random network prior used in
\cite{SunTaylorBollt15}.  As we will see, the direct lower bounds on
the average error probability for such a prior require
high-dimensional integration and are hard to compute even numerically,
while the calculation of the lower bounds based on side information
only involves the means and the determinants of the covariance
matrices of the Gaussian mixture components.
\begin{definition}
  \label{def:er}
  A \emph{dynamic ER prior distribution}, denoted by $\erPrior$, is
  the distribution of a signed ER graph scaled to a desired spectral
  radius if possible.  Here the signed ER graph is a random graph
  whose edge weights take values in $\{-1, 0, 1\}$, with the locations
  of the edges determined by an ER graph and the signs equally likely.
  Formally, for given $n\in\mathbb N$, $p\in[0, 1]$, and
  $r_0\in(0, 1)$, let $R^0\in\{-1, 1\}^{n\times n}$ be independent
  Rademacher random variables indicating the potential signs of the
  edges, and let $\trueSuppMat\in\{0, 1\}^{n\times n}$ be independent
  Bernoulli random variables with mean $p$ indicating the support.
  Then $\erPrior$ is defined to be the distribution of
  $s(R^0\circ \trueSuppMat)$, where $\circ$ denotes the Hadamard
  (entrywise) product and
  $s\colon\{-1, 0, 1\}^{n\times n}\to\reals^{n\times n}$ defined by
  (recall $r(\cdot)$ is the spectral radius)
  \[s(A) \defeq
    \begin{cases}
      r_0\frac{A}{\specRad(A)} & \text{if }\specRad(A) \neq 0,\\
      A & \text{if }\specRad(A) = 0.
    \end{cases}
  \]
\end{definition}
Note $R^0\circ\trueSuppMat$ can have a spectral radius of zero, in
which case scaling cannot achieve the desired spectral radius $r_0$.
Also note $\erPrior$ is symmetric in the sense of
Remark~\ref{rem:symm}.

To apply the direct lower bound in Proposition~\ref{prop:bc-mixed} one
needs to calculate the $\rho_{ij}$'s, which are based on
high-dimensional Gaussian components mixed with mixture weights from
the dynamic ER networks.  As a result, even numerical estimation is
highly non-trivial.

Alternatively, to use Proposition~\ref{prop:bc-coupling} we need to
find suitable side information for the BHT $\trueSuppMat_{ij} = 0$
vs.\ $\trueSuppMat_{ij} = 1$.  Borrowing terminology from game theory,
let $\trueSuppMat_{-ij}$ be all the entries except the $(i, j)$th in
$\trueSuppMat$, defined by
\[\trueSuppMat_{-ij}= (\trueSuppMat_{i'j'}: i',j' \in [n], ~(i',j')\neq (i,j)).\]
Because $R^0$ and $\trueSuppMat_{-ij}$ are independent of
$\trueSuppMat_{ij}$, a natural choice of the side information is
$(R^0, \trueSuppMat_{-ij})$.  With a slight abuse of notation we write
$\erPrior(R, \suppMat_{-ij}) \defeq\Pr\{R^0 = R, \trueSuppMat_{-ij} =
\suppMat_{-ij}\}$. Propositions~\ref{prop:network-edge} and
\ref{prop:bc-coupling} then yield the following bound on the average
network-level error probability for the dynamics ER networks.\\[-1em]
\begin{prop}
  \label{prop:er}
  Under $\erPrior$ for any estimator $\suppMatEst$ and any $\pi$,
  {\footnotesize
    \begin{align*}
      & \quad\pi\fnrNet + (1-\pi)\fprNet\\
      & \ge \frac 12\left(1 - \frac
        1{n^2}\sum_{i, j}\sum_{R, \suppMat_{-ij}}\erPrior(R, \suppMat_{-ij})\sqrt{1 -
        4\pi(1-\pi)\rho_{i, j, R, \suppMat_{-ij}}^2}\right),
    \end{align*}
  }%
  where $\rho_{i, j, R, \suppMat_{-ij}}$ is the BC for the following
  two mean zero Gaussian distributions: The conditional distribution
  of $Y$ given
  $R^0 = R, \trueSuppMat_{-ij} = \suppMat_{-ij}, \trueSuppMat_{ij} =
  0$ vs.\ the conditional distribution of $Y$ given
  $R^0 = R, \trueSuppMat_{-ij} = \suppMat_{-ij}, \trueSuppMat_{ij} =
  1$.
\end{prop}
The bound in Proposition \ref{prop:er} involves an expectation over
the probability distribution of the dynamic ER network, and can be
readily estimated by Monte Carlo simulation.
\section{Numerical results}
\label{sec:num}
\subsection{Algorithms}
Let
\[\Phi(0) =
  \begin{pmatrix}
    Y(0)\\Y(1)\\\vdots\\Y(T-1)
  \end{pmatrix},\quad \Phi(1) =
  \begin{pmatrix}
    Y(1)\\Y(2)\\\vdots\\Y(T)
  \end{pmatrix}.
\]

\subsubsection{lasso}
The lasso algorithm solves the optimization problem
\[\minimize_{A_j}\frac 1{2T}\|\Phi_j(1)-\Phi(0)A_j\|_2^2 + \lambda\|A_j\|_1,\]
where $A_j$ and $\Phi_j(1)$ are the $j$th columns of $A$ and
$\Phi(1)$, respectively, and $\lambda \ge 0$ is the regularization
parameter.  If $\Phi(0)\T\Phi(0)$ is invertible, the minimizer
$\hat A_j^{\mathsf{lasso}}$ is unique.  Write
$\hat A_j^{\mathsf{lasso}} = \ind{\hat
  A_{ij}^{\mathsf{lasso}}}{i\in[n]}$.  Then the estimated support
matrix is $\suppMatEst^{\mathsf{lasso}}$ defined by
$\suppMatEst_{ij}^{\mathsf{lasso}} = \indicator{\{\hat
  A_{ij}^{\mathsf{lasso}}\neq 0\}}$.  We implement lasso using
\textit{scikit-learn} \cite{PedregosaVaroquauxGramfort11}.
\subsubsection{oCSE}
oCSE was proposed in \cite{SunTaylorBollt15}.  For each target vertex
$j$, its parent set is discovered greedily one at a time by finding
the vertex whose column in $\Phi(0)$ together with the other chosen
columns fits $\Phi_j(1)$ the best in the least squares sense.  This
discovery stage terminates when the improvement in the residual fails
a permutation test with some threshold $\theta.$
\subsection{Dynamic ER graphs}
Comparisons of the receiver operating characteristic (ROC) curves of
lasso and oCSE by varying the parameters $\lambda$ and $\theta$ and
the upper bounds on the ROC curve resulted from
Proposition~\ref{prop:er} on dynamic ER graphs are shown in
Figs.~\ref{fig:ocse-v-lasso-w-bc-tight-n10-t20-s10-o0} and
\ref{fig:ocse-v-lasso-w-bc-tight-n10-t20-s10-o1}.
\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\figWidth]{ocse-v-lasso-w-bc-tight-num_genes_10-num_times_20-sims_100-prob_conn_0.2-obs_noise_0.eps}
  \caption{ROC curves of oCSE and lasso and the upper bound based on
    $\LBSide$ for dynamic ER graphs with $n = 10$, $p = 0.2$,
    $T = 20$, and $\sigmat^2 = 0$, averaged over 100 simulations.}
  \label{fig:ocse-v-lasso-w-bc-tight-n10-t20-s10-o0}
\end{figure}%
\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\figWidth]{ocse-v-lasso-w-bc-tight-num_genes_10-num_times_20-sims_100-prob_conn_0.2-obs_noise_1.eps}
  \caption{ROC curves of oCSE and lasso and the upper bound based on
    $\LBSide$ for dynamic ER graphs with $n = 10$, $p = 0.2$,
    $T = 20$, and $\sigmat^2 = 1$, averaged over 100 simulations.}
  \label{fig:ocse-v-lasso-w-bc-tight-n10-t20-s10-o1}
\end{figure}%
The simulation code can be found at \cite{Kang21}.

\section{Discussion}
There is a significant gap between the information lower bound (upper
bound on the ROC curve) and the algorithm performance, especially for
small spectral radius (high signal-to-noise ratio) settings.  Further
research to close the gap is warranted.

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.
\iftoggle{isit}{%
\IEEEtriggeratref{10}
}{}%

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{MarbachCostelloKuffner12}
D.~Marbach, J.~C. Costello, R.~K{\"u}ffner, N.~M. Vega, R.~J. Prill, D.~M.
  Camacho, K.~R. Allison, M.~Kellis, J.~J. Collins, and G.~Stolovitzky,
  ``Wisdom of crowds for robust gene network inference,'' \emph{Nat Methods},
  vol.~9, no.~8, pp. 796--804, Jul. 2012.

\bibitem{SunTaylorBollt15}
J.~Sun, D.~Taylor, and E.~M. Bollt, ``Causal network inference by optimal
  causation entropy,'' \emph{{SIAM} Journal on Applied Dynamical Systems},
  vol.~14, no.~1, pp. 73--106, Jan. 2015.

\bibitem{SimchowitzManiaTu18}
\BIBentryALTinterwordspacing
M.~Simchowitz, H.~Mania, S.~Tu, M.~I. Jordan, and B.~Recht, ``Learning without
  mixing: Towards a sharp analysis of linear system identification,''
  \emph{CoRR}, vol. abs/1802.08334, 2018. [Online]. Available:
  \url{http://arxiv.org/abs/1802.08334}
\BIBentrySTDinterwordspacing

\bibitem{FattahiSojoudi18a}
\BIBentryALTinterwordspacing
S.~Fattahi and S.~Sojoudi, ``Sample complexity of sparse system identification
  problem,'' \emph{CoRR}, vol. abs/1803.07753, 2018. [Online]. Available:
  \url{http://arxiv.org/abs/1803.07753}
\BIBentrySTDinterwordspacing

\bibitem{FattahiMatniSojoudi19}
\BIBentryALTinterwordspacing
S.~Fattahi, N.~Matni, and S.~Sojoudi, ``Learning sparse dynamical systems from
  a single sample trajectory,'' \emph{CoRR}, vol. abs/1904.09396, 2019.
  [Online]. Available: \url{http://arxiv.org/abs/1904.09396}
\BIBentrySTDinterwordspacing

\bibitem{OymakOzay19}
S.~Oymak and N.~Ozay, ``Non-asymptotic identification of {LTI} systems from a
  single trajectory,'' in \emph{2019 American Control Conference
  ({ACC})}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, Jul. 2019.

\bibitem{LaleAzizzadenesheliHassibi20}
\BIBentryALTinterwordspacing
S.~Lale, K.~Azizzadenesheli, B.~Hassibi, and A.~Anandkumar, ``Logarithmic
  regret bound in partially observable linear dynamical systems,'' \emph{CoRR},
  vol. abs/2003.11227, 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2003.11227}
\BIBentrySTDinterwordspacing

\bibitem{BentoIbrahimiMontanari10}
\BIBentryALTinterwordspacing
J.~Bento, M.~Ibrahimi, and A.~Montanari, ``Learning networks of stochastic
  differential equations,'' in \emph{Advances in Neural Information Processing
  Systems (NIPS)}, 2010, pp. 172--180. [Online]. Available:
  \url{http://papers.nips.cc/paper/4055-learning-networks-of-stochastic-differential-equations.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Fuchs05}
J.~Fuchs, ``Recovery of exact sparse representations in the presence of bounded
  noise,'' \emph{{IEEE} Transactions on Information Theory}, vol.~51, no.~10,
  pp. 3601--3608, Oct. 2005.

\bibitem{Tropp06}
J.~Tropp, ``Just relax: convex programming methods for identifying sparse
  signals in noise,'' \emph{{IEEE} Transactions on Information Theory},
  vol.~52, no.~3, pp. 1030--1051, Mar. 2006.

\bibitem{ZhaoYu06}
P.~Zhao and B.~Yu, ``On model selection consistency of {Lasso},'' \emph{J Mach
  Learn Res}, vol.~7, pp. 2541--2563, Nov. 2006.

\bibitem{MeinshausenBuhlmann06}
N.~Meinshausen and P.~B{\"u}hlmann, ``High-dimensional graphs and variable
  selection with the lasso,'' \emph{The Annals of Statistics}, vol.~34, no.~3,
  pp. 1436--1462, Jun. 2006.

\bibitem{Wainwright09}
M.~J. Wainwright, ``Sharp thresholds for high-dimensional and noisy sparsity
  recovery using $\ell_1$-constrained quadratic programming ({Lasso}),''
  \emph{{IEEE} Transactions on Information Theory}, vol.~55, no.~5, pp.
  2183--2202, May 2009.

\bibitem{JedraProutiere19}
Y.~Jedra and A.~Proutiere, ``Sample complexity lower bounds for linear system
  identification,'' in \emph{2019 {IEEE} 58th Conference on Decision and
  Control ({CDC})}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, Dec. 2019.

\bibitem{BentoIbrahimiMontanari11}
J.~Bento, M.~Ibrahimi, and A.~Montanari, ``Information theoretic limits on
  learning stochastic differential equations,'' in \emph{2011 {IEEE}
  International Symposium on Information Theory Proceedings}.\hskip 1em plus
  0.5em minus 0.4em\relax {IEEE}, Jul. 2011.

\bibitem{PerieraIbrahimi14}
J.~B.~A. Periera and M.~Ibrahimi, ``Support recovery for the drift coefficient
  of high-dimensional diffusions,'' \emph{{IEEE} Transactions on Information
  Theory}, vol.~60, no.~7, pp. 4026--4049, Jul. 2014.

\bibitem{Kailath67}
T.~Kailath, ``The divergence and {B}hattacharyya distance measures in signal
  selection,'' \emph{{IEEE} Transactions on Communications}, vol.~15, no.~1,
  pp. 52--60, Feb. 1967.

\bibitem{PedregosaVaroquauxGramfort11}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay, ``Scikit-learn:
  Machine learning in {P}ython,'' \emph{Journal of Machine Learning Research},
  vol.~12, pp. 2825--2830, 2011.

\bibitem{Kang21}
\BIBentryALTinterwordspacing
X.~Kang, ``Causal network inference simulations,'' Feb. 2021. [Online].
  Available: \url{https://github.com/Veggente/net-inf-eval}
\BIBentrySTDinterwordspacing

\bibitem{Poor94}
H.~V. Poor, \emph{An Introduction to Signal Detection and Estimation}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer-Verlag New York, 1994.

\bibitem{Shapiro99}
J.~H. Shapiro, ``Bounds on the area under the {ROC} curve,'' \emph{Journal of
  the Optical Society of America A}, vol.~16, no.~1, p.~53, Jan. 1999.

\end{thebibliography}
% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.
\iftoggle{isit}{}{%
\appendices
\section{Mutual incoherence property in dynamic ER networks}
\label{app:mip}
For a covariance matrix $\covInit$, let $\mathcal A_j$ be the nonzero
index set of column $j$ of $\covInit$.  The mutual incoherence
property in \cite{FattahiSojoudi18a} requires that
$\mathsf{MIP} \le 1 - \gamma_0$ for some positive $\gamma_0$, where
\[\mathsf{MIP} \defeq \max_{1\le j\le n}\max_{i\notin\mathcal A_j}\|\covInit_{i, \mathcal A_j}\covInit_{\mathcal A_j, \mathcal A_j}^{-1}\|_1.\]

For the dynamic ER networks as defined in Definition~\ref{def:er}
starting from an all-zero state, the average MIP at time $T$ is shown
in Fig.~\ref{fig:mip}.  It can be seen that the average MIP can be
greater than $1$.  As a result, the mutual incoherence property does
not hold in general.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\figWidth]{mip-w-time-spec-rad.eps}
  \caption{Average mutual incoherence parameters for dynamic ER
    networks with 200 vertices and 0.05 probability of connection.
    Each point is averaged over 10 simulations.}
  \label{fig:mip}
\end{figure}%
\section{Bhattacharyya bounds on average error probability for BHT}
\label{app:bhatta-bounds}
This section provides a proof of Lemma~\ref{lem:bhatta-bounds} and
discusses the (non)-concavity properties of the related quantities in
BHT.
\begin{IEEEproof}[Proof of Lemma~\ref{lem:bhatta-bounds}]
  Let $\piComp \defeq 1 - \pi$.  Integrating the identity
  $\pi \distA(y) + \piComp\distB(y) = 2((\pi \distA(y)) \wedge
  (\piComp \distB(y))) + |\pi \distA(y) - \piComp\distB(y)|$ over $y$
  yields
  \begin{equation}
    \label{eq:petotalvar}
    1 = 2\errorOpt  + \int|\pi \distA(y) - \piComp\distB(y)|\d y.
  \end{equation}
  The Cauchy–Schwarz inequality yields
  \begin{align}
    &\quad\int|\pi \distA(y) - \piComp\distB(y)|\d y\nonumber\\
    & = \int\left|\sqrt{\pi \distA(y)} +
      \sqrt{\piComp\distB(y)}\right|\cdot\left|\sqrt{\pi \distA(y)} -
      \sqrt{\piComp\distB(y)}\right|\d y\nonumber\\
    & \le \sqrt{\int\left(\sqrt{\pi \distA(y)} +
      \sqrt{\piComp\distB(y)}\right)^2 \d y}\nonumber\\
    &\quad\cdot \sqrt{\int \left(\sqrt{\pi \distA(y)} -
      \sqrt{\piComp\distB(y)}\right)^2\d y} \nonumber \\
    & = \sqrt{(1 + 2\sqrt{\pi\piComp}\rho)(1 - 2\sqrt{\pi\piComp}\rho)}\nonumber\\
    & = \sqrt{1 - 4\pi\piComp\rho^2}.   \label{eq:totvarBat}
  \end{align}
  Combining \eqref{eq:petotalvar} and \eqref{eq:totvarBat} yields
  \[\errorOpt  \geq \frac 1 2 \left[ 1 - \sqrt{1 - 4\pi\piComp\rho^2 } \right].\]
  For the other direction, note that
  $(\pi \distA(y)) \wedge (\piComp\distB(y)) \le
  \sqrt{\pi\piComp\distA(y)\distB(y)}$.  Integrating over $y$ yields
  $\errorOpt \le \sqrt{\pi\piComp}\rho$.

  Using the fact $\sqrt{1-u}\le 1 -\frac u 2$ for $0 \leq u \leq 1$
  (square both sides to check), one gets
  $\frac 12(1 - \sqrt{1 - 4\pi(1 - \pi)\rho^2}) \ge \pi(1 -
  \pi)\rho^2$ (see also eq.~(III.C.23) in Section~III.C.2 of
  \cite{Poor94}).

  Proof of the tensorization of BC is left to the reader.
\end{IEEEproof}

In the rest of this section we study the joint concavity of the
average error probability and its upper and lower bounds in
Lemma~\ref{lem:bhatta-bounds} in terms of the distributions $\distA$
and $\distB$.  The following two lemmas show that the minimum average
error probability and the upper bound are jointly concave in the two
distributions.  The two lower bounds are not concave in general per
Example~\ref{exam:lbd-nonconcave}, but
Proposition~\ref{prop:concave-special} shows $\rho^2$ is jointly
concave in a binary case.  Besides, Proposition~\ref{prop:single}
shows $\rho^2$ is concave in one distribution with the other fixed.

\begin{lemma}
  \label{lem:concave-error}
  For fixed $\pi$, the minimum average error probability $\errorOpt$
  in \eqref{eq:avg-prob-error} is jointly concave in $f$ and $g$.
\end{lemma}
\begin{IEEEproof}
  For any $f_1$, $f_2$, $g_1$, $g_2$, and $\alpha$, let
  $f' = \alpha f_1 + (1-\alpha)f_2$ and let
  $g' = \alpha g_1 + (1-\alpha)g_2$.  Then $f'$ and $g'$ are the
  mixture distributions of $f_1$, $f_2$ and $g_1$, $g_2$ with mixture
  weights $\alpha$ and $1-\alpha$.  It then suffices to show for any
  $f_1$, $f_2$, $g_1$, $g_2$, and $\alpha$, we have
  \begin{equation}
    \label{eq:avg-error-concave}
    \errorOpt(f', g') \ge \alpha \errorOpt(f_1, g_1) + (1-\alpha)\errorOpt(f_2, g_2).
  \end{equation}
  Indeed, the left-hand side of \eqref{eq:avg-error-concave} is the
  optimal average error probability based on an observation of the two
  mixture distributions, while the right-hand side of
  \eqref{eq:avg-error-concave} is the optimal average error
  probability based on the observation as well as the side information
  of a binary random variable indicating the mixture component.  Since
  side information cannot hurt, we obtain
  \eqref{eq:avg-error-concave}.
\end{IEEEproof}
\begin{lemma}
  \label{lem:rho-concave}
  The BC $\rho$ is jointly concave in $f$ and $g$.
\end{lemma}
\begin{IEEEproof}
  It is easy to see that the mapping
  $\phi\colon(x, y)\mapsto\sqrt{xy}$ is jointly concave by checking
  the Hessian.  Then $\rho$ is also jointly concave because a
  summation or integration of concave functions is still concave.
\end{IEEEproof}

Example~\ref{exam:lbd-nonconcave} shows $\LBDirect$ defined in
\eqref{eq:bhatta-mix-bound} is not concave.  The same example also
demonstrates that $\rho^2$ is not concave.  The following proposition
shows that $\rho^2$ is however indeed concave for discrete
distributions taking only two values.

\begin{prop}
  \label{prop:concave-special}
  For binary-valued distributions $\distA$ and $\distB$, the BC
  $\rho^2(\distA, \distB) = \sqrt{\distA_1\distB_1} +
  \sqrt{\distA_2\distB_2}$ is jointly concave in $(\distA, \distB)$.
\end{prop}
The proof can be done by checking the Hessian is negative
semi-definite and is left to the reader.

\begin{prop}
  \label{prop:single}
  $\rho^2(\distA, \cdot)$ is concave for fixed $\distA$.
\end{prop}
\begin{IEEEproof}
  Applying the reverse Minkowski inequality immediately yields the
  result.
\end{IEEEproof}

\section{Bounds on the area under the ROC curve}

This paper presents lower bounds on error probabilities which were
shown as upper bounds on the ROC curve.  This section briefly
describes the corresponding upper bounds on the area under the ROC
curve.  For the Bayes optimal decision rules $\delta(\pi)$ defined in
Section~\ref{sec:bht}, the area under the ROC curve, denoted by AUC,
is traced out by varying $\pi$ for the pair
$(1 - \fnrHT_{\delta(\pi)}, \fprHT_{\delta(\pi)})$.
Lemma~\ref{lem:bhatta-bounds} implies the following upper bound on
AUC.
\begin{cor}
  \label{cor:auc}
  $\mathsf{AUC} \le 1 - \rho^4/6$.
\end{cor}
\begin{IEEEproof}
  For any $\pi\in[0, 1]$ and any $\alpha\in[0, 1]$, let
  $\piComp = 1 - \pi$ and $\alphaComp = 1 - \alpha$.  By
  Lemma~\ref{lem:bhatta-bounds},
  \[\alpha\fnrHT_{\delta(\pi)} + \alphaComp\fprHT_{\delta(\pi)} \ge
    \alpha\alphaComp\rho^2.\]
  Using the fact that for any nonnegative $a$, $b$, and $c$,
  \[(\forall \alpha\in[0, 1], \alpha a + \alphaComp b \ge
    \alpha\alphaComp c) \iff \sqrt a + \sqrt b \ge \sqrt c,\]
  we can eliminate the parameter $\alpha$ to get
  $\fnrHT_{\delta(\pi)} \ge \left(\rho -
    \sqrt{\fprHT_{\delta(\pi)}}\right)^2$.  Then by excluding the
  impossible region we have
  $\mathsf{AUC} \le 1 - \int_0^{\rho^2}(\rho - \sqrt x)^2\d x = 1 -
  \rho^4 / 6$.
\end{IEEEproof}

Similarly, the tighter lower bound on $\errorOpt$ in
Lemma~\ref{lem:bhatta-bounds}(a) implies a tighter upper bound on AUC.
The tighter AUC bound does not have a nice analytical form, but can
nevertheless be numerically computed.  The two bounds and the bound
from Shapiro~\cite{Shapiro99}
($\mathsf{AUC} \le 1 - (1 - \sqrt{1 - \rho^2})^2 / 2$) are shown in
Fig.~\ref{fig:auc}.
\begin{figure}[thbp]
  \centering
  \includegraphics[width=\figWidth]{auc_bound.eps}
  \caption{Comparison of upper bounds on AUC.}
  \label{fig:auc}
\end{figure}%
It can be seen that our numerical bound outperforms Shapiro's bound
for all value of $\rho$, and our simple analytical bound in
Corollary~\ref{cor:auc} also slightly outperforms Shapiro's bound when
$\rho < 0.65$.
}
\end{document}
%%%%%%
%% To balance the columns at the last page of the paper use this
%% command:
%%
%\enlargethispage{-1.2cm}
%%
%% If the balancing should occur in the middle of the references, use
%% the following trigger:
%%
% \IEEEtriggeratref{4}
%%
%% which triggers a \newpage (i.e., new column) just before the given
%% reference number. Note that you need to adapt this if you modify
%% the paper.  The "triggered" command can be changed if desired:
%%
%\IEEEtriggercmd{\enlargethispage{-20cm}}
%%
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
