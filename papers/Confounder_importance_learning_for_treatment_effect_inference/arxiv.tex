\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{pgf,tikz}  % MIQUEL
\usetikzlibrary{positioning}  % MIQUEL
\usepackage{comment}  % MIQUEL
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}  % MIQUEL

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

% MIQUEL
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xr}

%\externaldocument{draft_jasacs_v9}

\def\cred{\textcolor{red}}
\def\cblue{\textcolor{blue}}
\def\cteal{\textcolor{teal}}

\newcommand{\E}{\mbox{\textup{E}}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\tr}[1]{#1^{\intercal}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\cind}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\indf}[1]{\textup{I}\left( #1 \right)}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\by}{{\mb{y}}}
\newcommand{\bd}{{\mb{d}}}
\newcommand{\bx}{{\mb{x}}}
\newcommand{\bX}{{\mb{X}}}
\newcommand{\bD}{{\mb{D}}}
\newcommand{\bI}{{\mb{I}}}
\newcommand{\bw}{{\mb{w}}}
\newcommand{\balpha}{{\bm{\alpha}}}
\newcommand{\bbeta}{{\bm{\beta}}}
\newcommand{\bgamma}{{\bm{\gamma}}}
\newcommand{\bdelta}{{\bm{\delta}}}
\newcommand{\blambda}{{\bm{\lambda}}}
\newcommand{\bpsi}{{\bm{\psi}}}
\newcommand{\brho}{{\bm{\rho}}}
\newcommand{\btheta}{{\bm{\theta}}}
\newcommand{\bmu}{{\bm{\mu}}}
\newcommand{\bomega}{{\bm{\omega}}}
\newcommand{\bpi}{{\bm{\pi}}}

\newcommand{\bthetay}{\btheta_{\text{y}}}
\newcommand{\bthetaD}{\btheta_{\textsc{d}}}
\newcommand{\hatbthetaD}{\hat{\btheta}_{\textsc{d}}}

\newcommand{\bthetaeb}{\btheta^{\textsc{eb}}}
\newcommand{\bthetaep}{\btheta^{\textsc{ep}}}

\newcommand{\py}{p}
\newcommand{\pdt}{p}
%\newcommand{\py}{p_{\textup{y}}}
%\newcommand{\pdt}{p_{\textup{d}^{t}}}

\newcommand{\hlambeb}{\hat{\lambda}^{\textsc{eb}}}
\newcommand{\hlambep}{\hat{\lambda}^{\textsc{ep}}}
\newcommand{\tlambep}{\tilde{\lambda}^{\textsc{ep}}}
\newcommand{\hthetaeb}{\hat{\btheta}^{\textsc{eb}}}
\newcommand{\hthetaep}{\hat{\btheta}^{\textsc{ep}}}
\newcommand{\tthetaep}{\tilde{\btheta}^{\textsc{ep}}}

\newtheorem{result}{Result}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}

% Load supplement file
%\externaldocument{supplement}
% END MIQUEL

\newcommand{\omcom}[1]{ {\color{blue} #1} }
\newcommand{\davidcom}[1]{{\color{red} [DR. #1]} }

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{ Confounder importance learning for treatment effect inference }
  \author{
Miquel Torrens i Dinar\`{e}s\\
    Department of Economics \& Business, Universitat Pompeu Fabra\\
    and \\
    Omiros Papaspiliopoulos \\
    Department of Decision Sciences, Universit\`{a} Bocconi\\
    and \\
    David Rossell\thanks{
    DR gratefully acknowledges support from Spanish Government grants Europa Excelencia EUR2020-112096, RYC-2015-18544, PGC2018-101643-B-I00.}\hspace{.2cm}\\
    Department of Economics \& Business, Universitat Pompeu Fabra \\ Data Science Center, Barcelona School of Economics
}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf  Confounder importance learning for treatment effect inference }
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}

We address applied and computational issues for the problem of multiple treatment effect inference under many potential confounders. While there is abundant literature on the harmful effects of omitting relevant controls (under-selection), we show that over-selection can be comparably problematic, introducing substantial variance and a bias related to the non-random over-inclusion controls. We provide a novel empirical Bayes framework to mitigate both under-selection problems in standard high-dimensional methods and over-selection issues in recent proposals, by learning whether each control's inclusion should be encouraged or discouraged. We develop efficient gradient-based and Expectation-Propagation model-fitting algorithms to render the approach practical for a wide class of models. A motivating problem is to estimate the salary gap evolution in recent years in relation to potentially discriminatory characteristics such as gender, race, ethnicity and place of birth. We found that, albeit smaller, some wage differences remained for female and black individuals. A similar trend is observed when analyzing the joint contribution of these factors to deviations from the average salary. Our methodology also showed remarkable estimation robustness to the addition of spurious artificial controls, relative to existing proposals.

%We address the problem of treatment effect inference in the presence of a high-dimensional set of potential confounders, and possibly multiple simultaneous treatments. While a lot of focus has been put on the effects of omitting relevant controls, we show that over-selection problems can become as severe in different contexts.
%problems related to over-selection have been partially overlooked. Here we show that over-selection problems can become as severe as under-selection

%The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:} multiple treatments, Bayesian model averaging, empirical Bayes, double machine learning, sparsity, variable selection, variational approximation
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!

\section{Introduction} \label{sec:intro}

This article addresses a problem of fundamental importance in applied research, that of evaluating the joint effect, if any, of multiple treatments on a response variable while controlling for a large number of covariates, many of which might be correlated with the treatments. We refer to the covariates as \textit{controls}, and when correlated to both the response and the treatments we call them \textit{confounders}.
A motivating application is to study the association between salary and multiple treatments such as gender, race and country of birth, after accounting for differential access to education, position at the work place, economic sector, region in the country, and many other potential controls. Studying which treatments are associated to salary, after accounting for said controls, helps quantify salary variation potentially due to discrimination, and how this evolved over time.

We model the dependence of the response $y_{i} \sim p(y_{i}; \eta_{i}, \phi)$ on $t=1,\ldots,T$ treatments $d_{i,t}$ and $j=1,\ldots,J$ controls $x_{i,j}$, via 
\begin{align}
\eta_{i} = \sum_{t=1}^{T} \alpha_{t} d_{i,t} + \sum_{j=1}^{J} \beta_{j} x_{i,j}, i=1,\ldots, n \label{eq:y_eq}
\end{align}
where $p(y_i; \eta_i, \phi)$ defines a generalized linear model with linear predictor $\eta_i$ and dispersion parameter $\phi$.
The distinction between treatments and controls is that we are primarily interested in inference for the former, i.e, for the set of $\alpha_t$'s in \eqref{eq:y_eq}, whereas the latter are included in the model to avoid omitted variable biases. When the controls are plenty, a shrinkage and selection estimation framework is required, either in terms of penalized likelihood or Bayesian model averaging (BMA). 
%However, these frameworks are geared towards prediction and consequently 
However, a key observation is that their naive application can yield imprecise estimates, especially when there are many confounders and/or the treatment effects are not very strong.
%, see for example Figure \ref{fig:intro2} later in this section. For example, 
For example, Figure \ref{fig:intro2} (described later in this section) shows that LASSO and BMA suffer from very high root mean squared error (RMSE) when all controls affecting the response $(\beta_j \neq 0)$ are confounders.
A further issue for LASSO and other likelihood penalties is that the treatment variables might not be selected, in which case the powerful post-selection inference framework of \cite{lee_jason:2016}, %implemented in the \texttt{R} package \texttt{selectiveInference}, 
is not applicable.
An alternative is to use methods based on de-biasing the LASSO \citep{vdgeer14, javanmard14}, which provide inference for all parameters, however these incur a loss of shrinkage that typically results in low power to detect anything but strong effects.
For example, in linear regression with orthogonal design matrix the debiased LASSO of \cite{vdgeer14} recovers the least-squares estimate, i.e. offers no shrinkage.

%\davidcom{I shuffled the order of Fig 1 and 2, to match the order in which they are presented in the text.}

To address this issue a number of penalized likelihood and Bayesian methods have been introduced. Section \ref{sec:other_methods} provides an overview. A popular one is the Double Machine Learning (DML) approach of \cite{Belloni14b}. %, which at a second stage fits a model like \eqref{eq:y_eq} by using OLS on a set of controls that have been selected at a first stage by, say, separate LASSO algorithms  that regress the response $y$  but also the treatments $d_t$ on the controls. 
In a first step, DML regresses separately the outcome and the treatment on the controls via penalized likelihood, and in a second step it fits a model like \eqref{eq:y_eq} via maximum likelihood on the set of controls selected in either regression in the first stage.
In a similar spirit, Bayesian Adjustment for Confounding (BAC)  in \cite{Wang12} models jointly the response and the treatment and uses a prior that encourages controls to be simultaneously selected in the two regression models. 
The key idea behind these approaches and related work is to encourage the inclusion of covariates that are associated with the treatments to the regression model for the response. %, as we detail in Section \ref{sec:other_methods}).

In this article we highlight an overlooked weakness that is relevant for the application of such methods, % for treatment effect estimation in presence of many controls, 
which relates to over-selection of controls. Adding controls to the regression model for $y_{i}$ that are correlated to the treatments, but are not conditionally related to the outcome, has two effects. The first one is an obvious \emph{over-selection variance}, an inflation of the standard errors of the treatment effects that leads to a reduced power to detect weaker effects. The second one is more subtle, and we call it \emph{control over-selection bias}. The inclusion of controls in \eqref{eq:y_eq} which were screened out to be correlated with the treatments leads to biased inference for the treatment effects. % that basic LASSO and Bayesian model averaging also suffer from, as we discussed above. 
Said over-selection bias and variance worsen as the number of treatments increases and as the level of confounding (the proportion of controls that are relevant both for the response and the treatments) decreases. In Figure \ref{fig:intro2} we consider a single treatment  simulated  according to linear regression on 6 active controls, and we varied the number of controls active in both models from 0 (no confounding) to 6  (full confounding, i.e., the same controls are used for generating the response and the treatment). While LASSO and BMA perform worse the stronger the confounding due to control under-selection, DML and BAC perform well in the presence of strong confounding but poorly in the lack of it. % is little overlap in the sets of controls that are active for response and treatment. On the other hand, CIL achieves good performance for the whole spectrum. 
In Figure \ref{fig:multitreat_uncounfounded} (described later) we show that these effects can be exacerbated in the presence of an increasing number of treatments.

We propose a new approach, Confounder Importance Learning (CIL), which can be easily implemented using existing software, that deals successfully with both over- and under-selection, in both high and low confounding situations, and in the presence of multiple simultaneous treatments. A first illustration of the merits of CIL is given in Figure \ref{fig:intro2}, where it achieves good performance across the spectrum. %Figures \ref{fig:intro1} and \ref{fig:intro2}. 
\begin{figure}[htbp]
\centering
\begin{tabular}{ccc}
$\alpha = 1$ & $\alpha = 1/3$ & $\alpha = 0$ \\
\includegraphics[scale=0.66]{fig1Ap1.pdf} &
\includegraphics[scale=0.66]{fig1Ap2.pdf} &
\includegraphics[scale=0.66]{fig1Ap3.pdf}
\end{tabular}
\caption{Parameter RMSE relative to an oracle OLS,  for a single treatment effect ($T=1$)  averaged over 250 simulated datasets, considering strong ($\alpha=1$), weak $(\alpha=1/3)$ and  no effect $(\alpha=0)$.  In all panels, $n=100$, $J=49$ and the response and treatment are simulated from a linear regression model based on 6 active controls each. The overlap between the two sets of active controls varies from 0 (no confounding) to 6 (full confounding). DML is double machine learning, BMA is Bayesian model averaging, BAC is Bayesian Adjustment for Confounding and CIL is confounder importance learning.} %\omcom{I have asked this before:  remove EP from label. }}
%, i.e., the same controls are used for generating both the response and the treatment). }%,  \omcom{Om2M: remove EP from label, remove LASSO from DML, remove inf from BAC, remove NLP from BMA and remove PCR since it is too early to also talk about this in the intro. we provide mode detail later}}
\label{fig:intro2}
\end{figure}

In Figure \ref{fig:intro1} we analyze data from the Current Population Survey, which has $T=204$ treatments and $J=278$ controls (see Section \ref{sec:cps}). % on the data, the methods and the conclusions from this analysis.
The former include four average treatment effects of particular interest, Figure  \ref{fig:intro1} focuses on two of them, a gender and a black race indicators, %the sex of the individual being female and their race being black, 
and compares a number of methods.  All methods show that the average log-salary is reduced for women, although this gap is less pronounced in 2019 relative to 2010. However, the methods differ in their conclusions for the black race. To understand better what drives these differences, we added 100 and 200 simulated controls that are dependent on the treatments but conditionally independent of the response. The figure shows a marked robustness of CIL to the addition of said controls, whereas other methods lose their ability to detect the weaker effects (e.g. gender in 2019 and race in 2010). %and its power to estimate weaker but existent effects. 
%Figure \ref{fig:intro2} gives further insights into the over-selection bias and variance, and the ability of CIL to perform well in a range of scenarios. Here we consider response and a single treatment simulated according to linear regressions on 6 active controls, but where we vary the number of controls active in both models from 0 (no confounding) to 6  (full confounding, i.e., the same controls are used for generating both the response and the treatment). As argued earlier, LASSO and basic Bayesian model averaging that do not account for the difference between treatments and controls perform the worse the stronger the confounding, which is due to under-selection. On the other hand, tailored existing methods, such as double machine learning and Bayesian adjustment for confounding, perform well in presence of strong confounding but poorly when there is little overlap in the sets of controls that are active for response and treatment. On the other hand, CIL achieves good performance for the whole spectrum. 

\begin{figure}[htbp]
\centering
%\includegraphics[scale=0.5]{figApp1}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{figApp1top.pdf} &
\includegraphics[width=0.48\textwidth]{figApp1bot.pdf}
\end{tabular}
\caption{Inference for treatments ``female'' (left) and ``black'' (right) in 2010 and 2019; see Section \ref{sec:cps}. We analyze Current Population Survey data with $J=278$ controls (left black point and bar in each panel) but also adding 100 (middle) and 200 (right) artificial controls correlated with the treatments and conditionally independent of  the response. Names of methods as in the caption of Figure \ref{fig:intro2}.}
\label{fig:intro1}
\end{figure}

The remainder of the introduction outlines our main ideas and the paper's structure.
Section \ref{sec:method} details our proposed approach and provides a deeper discussion of related literature. Our main contribution is a BMA framework where the prior inclusion probabilities, $\pi_j = P(\beta_j \neq 0)$, vary with $j$ in a manner that is learned from data. We build a model 
\begin{align}
\pi_{j}(\btheta) = \rho  + (1 - 2 \rho) \left( 1 + \exp \left\{ - \theta_{0} - \sum_{t=1}^{T} \theta_{t} f_{j,t} \right\} \right)^{-1} \label{eq:cilprior}
\end{align}
which expresses these probabilities in terms of \emph{features} $f_{j,t} \geq 0$ extracted from the treatment and control data, and hyperparameters $\rho \in (0, 1/2)$ and  $\btheta = (\theta_{0}, \theta_{1}, \dots, \theta_{T})$. The role of $\rho$ is to bound the probabilities away from 0 and 1 and, as we discuss in Section \ref{sec:model},  we propose the default choice $\rho = 1/(1+J^2)$. Our method relies on a good choice of the features $f_{j,t}$ %, one for each control and each treatment, 
and the hyperparameters $\theta_t$. %, one for each treatment variable. 
Section \ref{sec:model} describes high-dimensional approaches to obtain the former, and the main idea is to obtain rough estimates of the relative impact of each control to predict each treatment. Regarding the latter, when $\theta_t=0$ for $t=1,\ldots,T$ our approach assigns equal inclusion probabilities, when $\theta_t>0$ controls found to predict treatment $t$ are encouraged to be included in the response model, and discouraged when  $\theta_t<0$. 
This is in contrast to methods such as DML and BAC that encourage the inclusion of any control associated with any treatment, i.e. with large $f_{j,t}$.  

Section \ref{sec:comput} describes our computational methods, which are another main contribution. \color{black}
We design an empirical Bayes choice for $\btheta$ based on optimizing the marginal likelihood % using stochastic gradient descent. %, as shown in Section \ref{sec:ml}, but 
and a much cheaper alternative based on an expectation-propagation approximation. % variational approximation, % developed in Section \ref{sec:ep}, 
%which we used throughout the article.
%Under suitable conditions, there are consistency results available for BMS when hyperparameters such as $\btheta$ are estimated via empirical Bayes. Given the application-oriented nature of this paper we refer the interested reader to \cite{petrone:2014}.
In summary, our approach is based on \emph{learning} the importance of each possible confounder via \eqref{eq:cilprior}. %, while treating differentially treatments and controls. 
In doing so, it helps prevent both under-selection bias and over-selection bias and variance.
%Additionally, our approach scales well with the number of treatments, as it is not based on jointly modelling of treatments and response. 

Section \ref{sec:cps} applies the methodology to our motivating salary application. As discussed, our CIL detects negative salary gaps associated to the black race that might go otherwise unnoticed. 
We also illustrate how considering multiple treatments allows to portray, via posterior predictive inference, a measure of joint salary variation due to potentially discriminatory factors. Our results suggest that in 2019 said variation decreased nation-wide (from 5.4\% in 2010 to 1.5\% in 2019) and state-wide, with lesser disparities across states than in 2010.
%We also illustrate how, via posterior predictive inference, the framework can characterize functionals such as the average salary gap associated to joint variation in multiple treatments. For example, in 2010 the average salary gap associated to variation in the discriminatory treatments was 5.4\%, whereas in 2019 it was 1.5\%.
Finally, it also shows simulations that help understand practical differences between methods, in particular in terms of the amount of confounding present in the data.
All proofs and additional empirical results are in the supplementary material.
%\davidcom{Miquel, please prepare the R files so that we can submit them with the paper, and comply with JASA's blinded requirement. The unblinded version can link to GitHub}.
%THE LINK IS: https://github.com/mtorrens/rcil
R code to reproduce our results is available at \url{https://github.com/mtorrens/rcil}.

%\omcom{Om2D: here a paragraph summarizing the main findings from our analysis of salary data. I think for an applied oriented paper we should at least give some priority to this. maybe here even show the figure on posterior predictive}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Modelling Framework} \label{sec:method}

\subsection{Sparse Treatment and Control Selection}
\label{sec:model}

We model the dependence of the response $y_{i}$ on treatments $\mb{d}_{i} = (d_{i,1}, \ldots, d_{i,T})$ and controls $\mb{x}_{i} = (x_{i,1}, \dots, x_{i,J})$, according to \eqref{eq:y_eq}. %, where $\phi >0$ is a dispersion parameter. 
We are primarily interested in inference for $\balpha = (\alpha_{1}, \dots, \alpha_{T})$, i.e.  the \textit{treatment effects}. We call \textit{single treatment} to the special case $T=1$, while for $T>1$ we have  \textit{multiple treatments}.
%For concreteness, we focus on the normal linear model $y_{i} \sim \text{N}(eta_{i}, \phi)$, where $\phi$ represents the error variance. We postpone discussion of other choices to Section \ref{sec:discuss}.

We adopt a Bayesian framework where we   introduce variable inclusion indicators   $\gamma_{j} = \text{I}(\beta_{j} \neq 0)$ and $\delta_{t} = \text{I}(\alpha_{t} \neq 0)$, and define a model prior 
\begin{align}
p(\balpha, \bbeta, \bdelta, \bgamma, \phi \mid \btheta) = p(\balpha, \bbeta \mid \bdelta, \bgamma, \phi) p(\bgamma \mid \btheta) p(\bdelta) p(\phi), \label{eq:probmodel}
\end{align}
where $\btheta $ are the hyper-parameters in \eqref{eq:cilprior}, and $p(\phi)$ is dropped for models with known dispersion parameter (e.g. logistic or Poisson regression). For the regression coefficients, we assume prior independence, %Figure \ref{fig:dag1} includes a graphical model detailing the relation between the different variables and parameters in the model. For the linear coefficients, we assume prior independence,
\begin{align*}
p(\balpha, \bbeta \mid \bdelta, \bgamma, \phi) := \prod_{t=1}^{T} p(\alpha_{t} \mid \delta_{t}, \phi) \prod_{j=1}^{J} p(\beta_{j} \mid \gamma_{j}, \phi),
\end{align*}
and adopt the so-called product moment (pMOM) non-local prior of \cite{Johnson12}, according to which $\alpha_{t} = 0$ almost surely if $\delta_{t} = 0$, and
\begin{align*}
p(\alpha_{t} \mid \delta_{t} = 1, \phi) = \frac{\alpha_{t}^{2}}{\tau \phi} \text{N}(\alpha_{t}; 0, \tau \phi),
\end{align*}
with the analogous setting for every $\beta_{j}$. Figure \ref{fig:nlpmom} illustrates its density.
This prior involves a hyperparameter $\tau > 0$, that we set to $\tau = 0.348$, following \cite{Johnson10}, so that the prior signal-to-noise ratio $|\alpha_t|/\sqrt{\phi}$ is greater than 0.2 with probability 0.99. Non-local priors achieve model selection consistency on a range of high-dimensional linear and generalized linear regression models and play an important role in helping discard spurious predictors \citep{Johnson12, wu2016nonlocal, Shin18, Rossell21}. As for the dispersion parameter, where unknown,  we place a standard $\phi \sim \text{IGam}(a_{\phi} = 0.01, b_{\phi} = 0.01)$ prior, %commonly used for the Gaussian linear model, 
see e.g. \cite{Gelman06}.

For the inclusion indicators, we also assume prior independence, and set
\begin{eqnarray}
p(\bdelta) &=& \prod_{t=1}^{T} \text{Bern}(\delta_{t}; 1/2), \label{eq:pdelta} \\
p(\bgamma \mid \btheta) &=& \prod_{j=1}^{J}  \text{Bern}(\gamma_{j}; \pi_{j}(\btheta)). \label{eq:pgamma}
\end{eqnarray}
All treatments get a fixed marginal prior inclusion probability $P(\delta_t=1)=1/2$, as we do not want to favor their exclusion a priori, considering that there is at least some suspicion that any given treatment has an effect. %Such prior treatment inclusion probability leads to a uniform prior on $\bdelta$. 
This choice is a practical default when the number of treatments $T$ is not too large, else one may set $P(\delta_t=1)<1/2$ to avoid false positive inflation due to multiple hypothesis testing \citep{Scott10, Rossell21}.
%\cred{For example, setting $P(\delta_{t} = 1) = 1/?$ leads to prior inclusion probabilities of the same order as the $\text{Beta-Binomial}(1, 1)$, shown by \cite{Scott10} to prevent false positive inflation, see also \cite{Rossell21}.} \cblue{[\textbf{Miquel:} this comment confused me because the marginal for BBin(1,1) is 1/2 as well]} \davidcom{Maybe refer to $P(\delta_t=1)= 1/p$, which I think defines a Complexity prior?} 
Our software allows the user to set any desired $P(\delta_{t} = 1)$.

The main modelling novelty in this article is the choice of $\pi_{j}(\btheta)$, which we set according to \eqref{eq:cilprior}. A key part of the construction is the choice of features $f_{j,t}$. Our generic approach is to take $f_{j,t} = |w_{j,t}|$, where $\bw_t = (w_{1,t},\ldots, w_{J,t})$ are regression coefficients obtained via a high-dimensional regression of $\bd_t$ on the controls. We highlight two possibilities. First, a LASSO regression,
\begin{align}
\bw_{t} := \argmin_{(v_{t,1}, \dots, v_{t,J})} \left\{ \sum_{i=1}^{n} \log p\left( d_{i,t}; \sum_{j=1}^{J} x_{i,j} v_{t,j} \right) + \lambda \sum_{j=1}^{J} |v_{t,j}| \right\}, \label{eq:lasso}
\end{align}
%\mb{f}_{t}^{(\lambda)} := \arg \min_{(v_{t,1}, \dots, v_{t,J}) \in \mathbb{R}^{J}} \left\{ \sum_{n=1}^{N} \log p\left( d_{n,t}; \sum_{j=1}^{J} x_{n,j} v_{t,j} \right) + \lambda \sum_{j=1}^{J} |v_{t,j}| \right\}, \label{eq:lasso}
where $\lambda > 0$ is a regularization parameter, which we set by minimizing the BIC (we obtained similar results when using cross-validation). 
%, intuitively the reason is that values $\pi_j(\btheta)$ are mainly driven by the relative magnitudes of $f_{j,t}$, rather than their exact value. 
The choice in \eqref{eq:lasso} balances speed with reasonable point estimate precision, and is the option that we used in all our examples.
A second option, available when dealing with continuous treatments, is to use the minimum norm ridge regression,
\begin{align}
%\mb{f}_{t} := \tr{\bX} \left( \bX \tr{\bX} \right)^{-1} \bd_{t},
\mb{w}_{t} =  \left( \tr{\bX} \bX \right)^{+} \bd_{t}, \label{eq:pseudoinv}
\end{align}
where $\left( \tr{\bX} \bX \right)^{+}$ is the Moore-Penrose pseudo-inverse, and $\bX$ the $n \times J$ design matrix. For $J<n$ this is the familiar OLS estimator, but \eqref{eq:pseudoinv} is also well-defined when $J>n$, and it has been recently investigated in terms of the so-called benign overfitting property in \cite{Bartlett20}. \cite{Wang16} showed that when $J>n$, \eqref{eq:pseudoinv} ranks the coefficients consistently under theoretical conditions slightly broader than the LASSO.
This is appealing in our context, since $\pi_j(\btheta)$ are mainly driven by the relative magnitudes of $f_{j,t}$, rather than their exact value. 

The scalar $\rho$ in \eqref{eq:cilprior} ensures that prior probabilities are bounded away from 0 and 1. %, and we propose to set it according to general Bayesian variable selection asymptotic theory. 
In particular, we set it to $\rho = 1/(J^2 + 1)$. 
This sets a lower-bound $P(\beta_j \neq 0) \geq 1/(J^2+1)$ that is of the same order as $J$ grows as the Complexity priors in \citep{Castillo12}, which are sufficiently sparse to discard spurious predictors and attain minimax estimation rates \citep{Castillo15,Rossell21}.
%This contraction rate is sufficiently strong to attain to optimal minimax rates based on complexity prior theory \citep{Castillo12, Castillo15} using $c=1$ as the complexity parameter, although \cite{Rossell21} shows that lesser sparsity can in fact improve performance in smaller samples. 

The final element in \eqref{eq:cilprior} are the hyper-parameters $\theta_t$, which can encourage the inclusion or exclusion of controls associated to the treatment $t$. 
Figure \ref{fig:theta1} illustrates $\pi_{j}(\btheta)$ for three different values of $\theta_{1}$. Setting $\btheta$ is critical for the performance of our inferential paradigm, and in Section \ref{sec:comput} we introduce data-driven criteria and algorithms for its choice. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{cil_shape.pdf} 
\caption{Prior inclusion probability \eqref{eq:cilprior} as a function of $f_{j,1}$, a feature measuring correlation between control $j$ and treatment $t=1$, for $\theta_{0}=-1$, $\rho = (J^2 + 1)^{-1}$, and $J=99$ controls. Top and bottom dotted lines show the upper and lower bounds, $1 - \rho$ and $\rho$, respectively. The dotted line in the middle corresponds to $\theta_{1} = 0$.} %\omcom{Remove the absolute from f in the xaxis}}
\label{fig:theta1}
\end{figure}

\subsection{Connections to the literature}
\label{sec:other_methods}

%\davidcom{Miquel, many refs are missing Bibtex entries and proper cite commands, can you please add?}

We discuss approaches for treatment effect inference in the presence of many controls.
The main idea in both frequentist and Bayesian literatures is to encourage the inclusion of confounders in \eqref{eq:y_eq} to mitigate under-selection bias. 
\cite{Farrell15} adapted the DML framework of \cite{Belloni14b} by using a robust estimator to safeguard from mistakes in the double selection step, 
\cite{Shortreed17} employed a two-step adaptive LASSO approach,
\cite{Antonelli18} used propensity matching,
and \cite{Victor18} extended DML by introducing a de-biasing step, and cross-fitting to ameliorate false positive inclusion of controls. 
%See also \cred{Ghosh et al (2015)} with shared and difference Lasso for similar multi-step contributions. 
An an alternative to these two-step approaches, \cite{Ertefaie15} used a joint likelihood $L_1$ penalization on the outcome and treatment regressions.
%\davidcom{To discuss. Do \cite{Victor18} address any of the over-selection issues that we describe, or some other type of over-fitting?}
%Miquel: not really, the idea is to split the sample into, say, two halves and work out the parameter estimates of the "first" stage with one of them, and plug them in on the "second" stage, which is estimated with the second half. The cross-fitting basically flips the two halves and re-estimates everything (round 2), then averages the estimates of round 1 and round 2 ("avoiding" loss of efficiency).
%\cred{Ma et al. (2019)} combine regularization with sufficient dimension reduction into an estimator that achieves good asymptotic properties without requiring model selection consistency. 
%Yet, even if some of these proposals do allow for inference, most of them are essentially designed with sufficient control selection in mind and tackle point estimation only after selection has been conducted, generally without a focus on quantification of uncertainty. 
%Other proposals exist focused on inference for PL-related methods in this particular context, see \cred{Athey et al. (2016)} in the context of binary treatments, see also \cred{Vansteelandt, Bekaert and Claeskens (2012)}, as well as more recent work in \cred{Dukes and Vansteelandt (2019)}. Generally, PL-based proposals are heavily focused on asymptotic results regarding estimation efficiency and their distributional properties, which follow from attaining guarantees of sufficient control selection, i.e. avoiding detection errors of relevant controls connected to either outcome or treatment. In contrast, they are not too concerned with excess inclusion of any other spurious variable. This can be problematic based on the aforementioned reasons, which in turn often limit their oracle performance to a super-model of the true outcome model, inflated in size with unnecessary controls that may relate to the treatment only. 
%It is also worth mentioning that a number of these proposals are not designed for continuous or even multi-valued treatments, or sometimes outcomes.

Within a Bayesian framework, a natural approach is to build a joint model
\begin{align}
p(y_{i}, \bd_{i} \mid \bx_{i}) = p(y_{i} \mid \bd_{i}, \bx_{i}) p(\bd_{i} \mid \bx_{i}),
\label{eq:joint_bms}
\end{align}
where $p(y_i \mid \bd_i, \bx_i)$ is as in \eqref{eq:y_eq} and $p(\bd_i \mid \bx_i)$ adds $T \times J$ inclusion indicators $\xi_{tj}$ describing the dependence between each treatment $t$ and control $j$. 
%In the single treatment model, for example, this approach involves modelling a third set of inclusion indicators $\bm{\xi} \in \{ 0,1 \}^{J}$, with elements $\xi_{j} = \text{I}(v_{1,j} \neq 0)$ that capture the existence of a conditional relation between the treatment and every control. 
BAC \citep{Wang12} considers this approach only for $T=1$, setting
% This method uses a tuning hyperparameter $\omega \geq 1$ that regulates the dependence of treatment effect indicators $\bgamma$ on the controls, and forces the inclusion of the treatment in \eqref{eq:y_eq}. 
a prior for $\gamma_j$ where each control has two potential prior inclusion probabilities. %, depending on whether the associated $\xi_{tj}$ is 0 or 1. 
If a control $j$ is associated to the single treatment $t=1$ ($\xi_{tj}=1$), the prior inclusion probability $P(\gamma_j=1)$ increases by a factor determined by a hyperparameter $\omega$
%Posterior parameter estimation of the model is then conducted with standard BMA. 
that is be set by the user. % and, for large $\omega$, it can lead to over-selection problems, see Figures \ref{fig:fig1} through \ref{fig:fig3}. 
\cite{Lefebvre14} and \cite{Wang15} provided some theoretical support and proposals to set $\omega$,
and \cite{Wilson18} proposed a multiple treatment extension of BAC.
\cite{Talbot15} introduced Bayesian causal effect estimation, which incorporates informative priors to deter excess control inclusion, 
and \cite{Antonelli17} generalized BAC %\textit{guided} BAC 
to address treatment effect heterogeneity.
From a practical point of view, \eqref{eq:joint_bms} multiplies the size of the model space by a factor of $2^{JT}$, rendering the framework impractical even for moderate $T$.

%Similar methods have also been explored in propensity score analysis, as in \cred{Cefalu et al. (2015)}, but more generally around model uncertainty as well, see e.g. \cred{Ziegler and Dominici (2014)} and \cred{An (2010)}. See also \cred{Jacobi et al. (2016)} for methodological adaptations to dynamic effects in panel data. 
In a different thread, \cite{Hahn18} proposed a shrinkage prior framework based on re-parameterizing a joint outcome and treatment regression, designed to improve estimation biases,
%preserve the joint modelling approach but move away from model averaging, by addressing BAC using hierarchical priors, in an attempt to give some prior flexibility and ease computational difficulties via posterior sampling. This is a reparametrization technique designed to achieve debiased point-estimates using regularization priors. 
and \cite{Hahn20} considered non-parametric Bayesian causal forests.
\cite{Antonelli19} proposed a spike-and-slab Laplace prior on the controls that shrinks less those controls that are associated to the treatment, % places low shrinkage to controls with any association to the treatment (only encouragement is possible), 
and an empirical Bayes algorithm for hyper-parameter setting. %This method formulates model estimation as a penalized likelihood problem to recover the posterior mode of the model with its corresponding coefficient estimates, without recurring to a model averaging approach.

Our main contributions are of an applied, but relevant, nature: replacing the joint model \eqref{eq:joint_bms} by extracting features derived from $p(\bd_i \mid \bx_i)$ to render computations practical, and learning from data whether confounder inclusion should be encouraged, discouraged, or neither, to avoid over-selection issues.
%It also comes at sizeable computational cost derived from the joint modelling strategy, relative to our framework, which replaces an explicit model for $p(\bd_i \mid \bx_i)$ by features extracted from observations from this conditional distribution.
Another contribution is considering the multiple treatments problem ($T>1$), which has been less studied.
%The Penalized Credible Regions method 
%\cite{Wilson14} used $L_{1}$ penalization to find the simplest model among those in a given posterior credible region of the full model,
%and can consider multiple treatments, but provide point estimation rather than parameter inference.
%%Posterior quantities are generally computed using a flat prior on the coefficients and an inverse-gamma prior on the error variance. In $p>n$ situations, the flat prior is replaced by a Gaussian prior $\bbeta \sim \text{N}(\mb{0}, \phi/\tau \mb{I})$, adding a gamma prior on $\tau$. 
%%A posterior credible region for parameters of the outcome model is built with the estimates of the posterior mean and covariance, and then an $L_{1}$-type penalty is applied to find a sparse coefficient vector within such region, penalizing less those controls with a strong relation to any given treatment, i.e. restricting to positively discriminating them in terms of prior inclusion probability. 
%%For a fixed tuning parameter $\lambda \geq 0$, the resulting estimator is
%%\begin{equation}
%%\hat{\bbeta}^{\textsc{pcr}} := \arg \min_{\bbeta \in \mathbb{R}^{J}} \tr{(\bbeta - \hat{\bbeta})} \hat{\Sigma}^{-1} (\bbeta - \hat{\bbeta}) + \lambda \sum_{j=1}^{J} \frac{|\beta_{j}|}{|\hat{\beta}_{j}| + \sum_{t=1}^{T} |\hat{v}_{t,j}|}, \nonumber
%%\end{equation}
%%where $\hat{\bbeta}$ and $\hat{\Sigma}$ denote the posterior mean of $\bbeta$ and $\Sigma$, and $\hat{v}_{t,j}$ that of $\hat{v}_{t,j}$, representing the model coefficients of regressing $\mb{d}_{t}$ on $\bX$. 
%\cite{Wilson18} %proposed ACPME, which 
%incorporated correlations between controls and multiple treatments %, this time without imposing sudden jumps in prior probabilities. 
%to tilt prior probabilities to encourage confounder inclusion in \eqref{eq:y_eq}.
%Besides the discussed over-selection bias and variance issues, marginal prior inclusion probabilities are bounded below at 1/2, which does not help reduce false positive inclusion.


%CARVALHO
%ANTONELLI
%ACPME

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational Methodology} \label{sec:comput}

\subsection{Bayesian model averaging} \label{ssec:bma}

All expressions in this section are conditional on the observed $(\bx_i,\bd_i)$, we drop them from the notation for simplicity.
Inference for our approach relies on
%the density of the response given variable inclusion indicators with the all other unknown parameters integrated out, i.e., 
posterior model probabilities
\begin{align}
  %p(\bgamma, \bdelta \mid \by, \btheta) = \frac{p(\by \mid \bgamma, \bdelta) p(\bgamma \mid \btheta) p(\bdelta)}{p(\by \mid \btheta)},
\nonumber
p(\bgamma, \bdelta \mid \by, \btheta) \propto p(\by \mid \bgamma, \bdelta) p(\bgamma \mid \btheta) p(\bdelta),
\nonumber
\end{align}
where
\begin{align}
p(\by \mid \bgamma, \bdelta) = \int p(\by \mid \balpha, \bbeta, \phi, \bdelta, \bgamma) p(\balpha, \bbeta \mid \bdelta, \bgamma, \phi) p(\phi) \text{d}\balpha \text{d}\bbeta \text{d}\phi \label{eq:marglik2}
\end{align}
is the marginal likelihood of model $(\bgamma,\bdelta)$.
We set the hyperparameter $\btheta$ to a point estimate $\hat{\btheta}$ described in the next section.
Conditional on $\btheta$, our model prior $p(\bgamma \mid \btheta)$ is a product of independent Bernouilli's with asymmetric success probabilities defined by \eqref{eq:cilprior}. As a simple variation of standard BMA, one can exploit existing computational algorithms, which we outline next.

%\begin{equation}
%\begin{aligned}
%& p(y_i \mid \bx_i,\bd_i,\bgamma, \bdelta)  = \\ & \int p(y_i \mid \bx_i,\bd_i,\bgamma, \bdelta, \balpha, \bbeta, \phi ) p(\balpha \mid  \bdelta,  \phi) p(\bbeta \mid \bgamma,\phi) p(\phi) \text{d}\balpha \text{d}\bbeta \text{d}\phi.
%\end{aligned} \label{eq:marglik2}
%\end{equation}
Outside particular cases such as Gaussian regression under Gaussian priors, \eqref{eq:marglik2} does not have a closed-form expression.
To estimate \eqref{eq:marglik2} under our pMOM prior we adopt the approximate Laplace approximations of \cite{Rossell20a}, %using Proposition 1 in \cite{Rossell17}, 
see Section \ref{subsec:nlp_approx}.

We obtain point estimates using BMA,
\begin{align}
\hat{\balpha} := \sum_{\bgamma, \bdelta}  \text{E}(\balpha \mid \by, \bgamma, \bdelta) p(\bgamma, \bdelta \mid \by, \btheta), \label{eq:teffest}
\end{align}
and similarly employ the BMA posterior density $p(\balpha \mid \by, \bgamma, \bdelta, \btheta)$ to provide posterior credible intervals.
To this end we use posterior samples from this density  using a latent truncation representation described by \cite{Rossell17}. 
%\omcom{I think we need a supplement section with more details on how we do all this for reproducibility}
%\davidcom{Maybe we can just provide the R code to ensure reproducibility, for an applications paper we already have lots of computational details}
Expression \eqref{eq:teffest} is a sum across $2^{T+J}$ models, when it is unfeasible we use Markov Chain Monte Carlo to explore the posterior distribution $p(\bgamma,\bdelta \mid \by,\btheta)$,
%The computational methods we employ and introduce rely on samples from the posterior density
%$$p(\bgamma,\bdelta \mid \by,\btheta) \propto p(\by \mid \bgamma, \bdelta) p(\bgamma) p(\bdelta \mid \btheta).$$
see e.g. \cite{Clyde12} for a review. 

We used all the algorithms described above as implemented by the \texttt{modelSelection} function in \texttt{R} package \texttt{mombf} \citep{Rossell20}.

%\omcom{Om2D: Here is where I have stopped editing. I created the two subsections below and I think these two should be the only other thing in this section, I have left the material from the past below in case you want to copy and paste stuff easily.}

\subsection{Confounder importance learning via marginal likelihood}
\label{sec:ml}

Our main computational contribution is a strategy to learn the hyperparameter $\btheta$, which plays a critical role by determining prior inclusion probabilities. We devised an empirical Bayes approach maximizing the marginal likelihood, with
\begin{align}
\bthetaeb := \argmax_{\btheta \in \mathbb{R}^{T+1}} p(\by \mid \btheta)=
% \label{eq:eb_eq}
%\argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{\bgamma, \bdelta} p(\by \mid \bgamma, \bdelta) p(\bgamma) p(\bdelta \mid \btheta). 
\argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{(\bdelta, \bgamma)} p_{u}(\bdelta, \bgamma \mid \by) p(\bdelta, \bgamma \mid \btheta)
\label{eq:marglik1}
\end{align}
where the right-hand side follows easily, denoting by $p_{u}(\bdelta, \bgamma \mid \by)$ the posterior probabilities under a uniform model prior $p_{u}(\bdelta, \bgamma) \propto 1$.
The use of empirical Bayes for hyperparameter learning in variable selection has been well-studied, see \cite{George00, Scott10, petrone:2014}.
%\begin{align}
%p(\by \mid \btheta) = \sum_{(\bdelta, \bgamma)} p(\by \mid \bgamma, \bdelta) p(\bdelta, \bgamma \mid \btheta), \label{eq:marglik1}
%\end{align}

The main challenge is that one must evaluate the costly sum in \eqref{eq:marglik1} for each value of $\btheta$ considered by an optimization algorithm. 
%A second challenge is that evaluating \eqref{eq:marglik2} under non-local priors, as with our MOM prior, can be computationally cumbersome. 
Note that $p(\by \mid \bgamma,\bdelta)$ does not depend on $\btheta$, and hence can be re-used to evaluate \eqref{eq:marglik1} for any number of $\btheta$ values. 
%Section \ref{subsec:comput1} discusses  strategies to evaluate \eqref{eq:marglik2} and obtain posterior inference, given $\bthetaeb$. Section \ref{subsec:sgdeb} presents a strategy to evaluate the gradient of \eqref{eq:marglik1}, which can serve as the basis of standard optimization algorithms, and discusses issues related to the existence of local maxima. Section \ref{subsec:varapprox} proposes a fast Expectation Propagation mean-field approximation to $\bthetaeb$, which we denote $\bthetaep$. The latter provided practically indistinguishable results from $\bthetaeb$ in our numerical experiments, at a significantly lower computational cost. Alternatively, $\bthetaep$ can be used to initialize $\bthetaeb$ when optimizing \eqref{eq:marglik1}, ameliorating issues related to finding low-quality local maxima.
In fact, by Proposition \ref{prop:one} below, this provides grounds to use stochastic gradient methodology to maximize \eqref{eq:marglik1}.
\begin{prop} \label{prop:one}
If $p(\by \mid \bgamma, \bdelta, \btheta) = p(\by \mid \bgamma, \bdelta)$, then
\begin{align*}
\nabla_{\btheta} \log p(\by \mid \btheta) = \sum_{(\bdelta, \bgamma)} p(\bgamma, \bdelta \mid \by, \btheta) \nabla_{\btheta} \log p(\bgamma, \bdelta \mid \btheta).
\end{align*}
If, additionally, the model prior is separable such that
\begin{align*}
p(\bgamma, \bdelta \mid \btheta) = \prod_{t=1}^{T} p(\delta_{t}) \prod_{j=1}^{J} p(\gamma_{j} \mid \btheta),
\end{align*}
then
\begin{align}
\nabla_{\btheta} \log p(\by \mid \btheta) = \sum_{j=1}^{J} \E\left[ \nabla_{\btheta} \log p(\gamma_{j} \mid \btheta) \mid \by \right]. \label{eq:prop_one}
\end{align}
\end{prop}

\begin{corollary} \label{prop:two}
Under the model prior in \eqref{eq:pdelta} and \eqref{eq:pgamma}, and with $\pi_{j}(\btheta)$ as defined by \eqref{eq:cilprior},
\begin{align}
\nabla_{\btheta} \log p(\by \mid \btheta) = (1-2\rho) \sum_{j=1}^{J} \mb{f}_{j} \left[P(\gamma_{j} = 1 \mid \by, \btheta) - \pi_{j}(\btheta)\right], \label{eq:prop_two}
\end{align}
where $\mb{f}_{j} = \tr{(1, f_{j,1}, \dots, f_{j,T})}$.
\end{corollary}
%\omcom{Remove absolute values above and fix any proofs in Supplement}
Expressions \eqref{eq:prop_one} and \eqref{eq:prop_two} evaluate the gradient with a sum of $J$ terms, relative to the $2^{J+T}$ terms in \eqref{eq:marglik1}. Further, \eqref{eq:prop_two} only depends on $\by$ via marginal inclusion probabilities $P(\gamma_j = 1 \mid \by, \btheta)$, which can typically be estimated more accurately than the joint model probabilities in \eqref{eq:marglik1}. 
%The Gibbs sampling scheme introduced in Section \ref{subsec:comput1} provides estimates for the marginal posterior inclusion probabilities needed in \eqref{eq:prop_two}, which are Rao-Blackwellized to increase accuracy. Importantly, the result in Corollary \ref{prop:two} allows for the use of stochastic Newton-type optimization algorithms to approximate $\bthetaeb$. 
However, two problems remain unaddressed. First, one must compute $P(\gamma_j = 1 \mid \by, \btheta)$ for every considered $\btheta$, which is cumbersome when the optimization requires more than a few iterations. Second, $\log p(\by \mid \btheta)$ can be non-convex. Hence, standard algorithms may converge to low-quality local optima if $\btheta$ is poorly initialized. Figure \ref{fig:thEPEB} (left) shows an example of a multi-modal $p(\by \mid \btheta)$.
We next describe an Expectation Propagation approximation which, as illustrated in Figure \ref{fig:thEPEB}, typically provides a good approximation to the global mode.



\subsection{Confounder importance learning by Expectation-Propagation}
\label{sec:ep}

The use of Expectation Propagation \citep{Minka01a, Minka01b} is common in Bayesian machine learning, including in variable selection \citep{Seeger07, HdezLobato13, Xu14}. 
We propose a computationally tractable approximation to \eqref{eq:marglik1}, which can also serve as an initialization point for an algorithm to solve \eqref{eq:marglik1} exactly, if so desired.

We consider a mean-field approximation to the posterior probabilities in \eqref{eq:marglik1},
\begin{align}
\hat{p}_{u}(\bdelta, \bgamma \mid \by) = \prod_{t=1}^{T} \text{Bern}(\delta_{t}; s_{t}) \prod_{j=1}^{J} \text{Bern}(\gamma_{j}; q_{j}). \label{eq:mfapp}
\end{align}
where $\mb{s}=(s_1,\ldots,s_T)$ and $\mb{q}=(q_1,\ldots,q_J)$ are given in Proposition \ref{prop:three} to optimally approximate $p(\bdelta, \bgamma \mid \by)$. 
By Proposition \ref{prop:three} below, \eqref{eq:mfapp} leads to replacing \eqref{eq:marglik1} by a new objective function
\eqref{eq:ep_eq0} that only requires an inexpensive product across $J$ terms. These only depend on $\by$ via posterior inclusion probabilities $q_{j} = P(\gamma_{j} = 1 \mid \by, \btheta = \mb{0})$ that can be easily pre-computed prior to conducting the optimization exercise.

\begin{prop}  \label{prop:three}
Let $s_t$, $q_j$ and $\hat{p}_u(\bdelta,\bgamma \mid \by)$ be as defined in \eqref{eq:mfapp}. Then, $s_{t}^{\textsc{ep}} = P(\delta_{t} = 1 \mid \by, \btheta = \mb{0})$ and $q_{j}^{\textsc{ep}} = P(\gamma_{j} = 1 \mid \by, \btheta = \mb{0})$ minimize Kullback-Leibler divergence from $p_{u}(\bdelta, \bgamma \mid \by)$ to $\hat{p}_u(\bdelta,\bgamma \mid \by)$. Further
\begin{eqnarray}
\bthetaep_{u} &:=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{\bdelta, \bgamma} \hat{p}_{u}(\bdelta, \bgamma \mid \by) p(\bdelta, \bgamma \mid \btheta) \nonumber \\
&=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{j=1}^{J} \log \left( q_{j}^{\textsc{ep}} \pi_j(\btheta) + (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta)) \right). \label{eq:ep_eq0}
\end{eqnarray}
\end{prop}
%\omcom{There are two issues here. One is notation, there is both $q_j$ and $q_j^{EP}$. Other is that the $s_t$ do not show up anywhere which is confusing. if they play no role in this latter optimization why do we make the approximation in the first place? we could just use the actual marginal of the deltas. Anyway, Miquel please talk about this with David because I do not understand this. And fix the typos}
%Note that \eqref{eq:ep_eq0} has no closed form solution, but critically its objective function only depends on $\btheta$ through deterministic terms: its only stochastic quantities $\hat{\mb{q}}$ are independent of $\btheta$ and can be accurately estimated upfront with a single model space search under $\btheta = \mb{0}$. As a result, evaluating the expression in \eqref{eq:ep_eq0} becomes much faster to evaluate compared to \eqref{eq:eb_eq}, and it can be efficiently optimized deterministically for a given $\hat{\mb{q}}$ using a gradient descent method, as 
The gradient of the objective function \eqref{eq:ep_eq0} is in Section \ref{sec:gradEP}.
Since this function may not be concave, we conduct an initial grid search and subsequently use a quasi-Newton BFGS algorithm. See Section \ref{sec:algorithm} and Algorithm \ref{alg:one} therein for a full description of our algorithm to obtain $\hthetaeb$ and $\hthetaep$.
In most our examples $\hthetaeb$ and $\hthetaep$ provided virtually indistinguishable inference, the latter incurring a significantly lower computational cost, but the exact $\hthetaeb$ did provide slight advantages in some high-dimensional settings (see Section \ref{subsec:singleT}).
%\cred{While the parameter $\hthetaep$ itself might suggest a value of $\btheta$ that can be readily implemented in \eqref{eq:cilprior}, it also provides an initialization point to optimize \eqref{eq:eb_eq} that is fast to compute. A proper initialization point can help us escape local optima, as well as minimize the number of optimization steps.}


\section{Results} \label{sec:cps}

We studied the association between belonging to certain social groups and the hourly salary, and its evolution over the last decade (prior to the \textsc{covid}-19 pandemic), to assess progress in wage discrimination. We analyzed the USA Current Population Survey (CPS) microdata \citep{ipums}, which records many social, economic and job-related factors. %Surveys are administered monthly by the U.S. Bureau of the Census to over 65,000 households. %The microdata is made freely available by the Integrated Public Use Microdata Series (IPUMS) website.
The outcome is the individual log-hourly wage, rescaled by the consumer price index of 1999, and we considered four treatments: gender, black race, Hispanic ethnicity and Latin America as place of birth. These treatments are highly correlated to sociodemographic and job characteristics that can impact salary, i.e. there are many potential confounders.

Section \ref{sec:cps_data} describes the data and Section \ref{sec:cps_results} contains results on the treatment effects, both individually and in terms of a composite score measuring their joint association with salary.
These results support that methods designed for treatment effect inference may run into over-selection, whereas naive methods may run into under-selection.
To provide further insight, Section \ref{subsec:singleT} shows simulation studies, with particular attention on how the presence/absence of confounders affects each method.
%We design a set of simulations to illustrate the various effects discussed along this article, some of which will also be apparent in the real application presented in Section \ref{subsec:realapp}. In these synthetic datasets we compare the performance of a few of the methods discussed in a variety of situations. We divide them in two subsets: one for single treatment estimation, and one for multiple treatments.


In Section \ref{sec:cps_results} we compare our CIL (under the EP approximation) to three methods: OLS under the full model ($J=278$ controls), DML based on the LASSO \citep{Belloni14b}, and standard BMA with a $\text{Beta-Binomial}(1,1)$ model prior and the pMOM prior in Section \ref{sec:model}.
In Section \ref{subsec:singleT} using simulated data we also compare to BAC \citep{Wang12}, which was computationally unfeasible to apply to the salary data. We set its hyperparameter to $\omega = +\infty$, which encourages the inclusion of confounders relative to %$\omega = 1$, which corresponds to 
standard BMA. For completeness, we also considered a standard LASSO regression on the outcome equation \eqref{eq:y_eq}, setting the penalization parameter via cross-validation. 
%Finally, we report the Penalized Credible Regions approach (PCR) by \cite{Wilson14} on a number of figures in the Supplementary material, as despite not providing inference it exhibited good point estimation in some situations, setting the penalty parameter to minimize the Bayesian information criterion. 
%Debiased LASSO provides no shrinkage and thus was outside of our present scope. 
%\davidcom{I removed the mention to PCR, it was just confusing, please also remove from the suppl figures, PCR just makes them harder to read}
We compared these methods to the oracle OLS, i.e. based on the subset of controls truly featuring in \eqref{eq:y_eq}. 
%Regarding standard BMS, we used the same pMOM prior as in our CIL and set a Binomial model prior akin to \eqref{eq:pdelta}--\eqref{eq:pgamma}. We set the prior inclusion probabilities to the true proportion of active covariates, to assess the advantages of CIL over BMS in a most favorable setting for the latter.
These methods are implemented in \texttt{R} packages \texttt{glmnet} \citep{glmnet} for LASSO, \texttt{mombf} for BMA, \texttt{hdm} \citep{hdm} for DML and \texttt{BACprior} \citep{BACprior} for BAC. %, and \texttt{BayesPen} \citep{BayesPen} for PCR.


\subsection{Data}
\label{sec:cps_data}

We downloaded data %of 03-2010 and 03-2019, which include data from the Annual Social and Economic Supplement, 
from 2010 and 2019 and analyzed each year separately. %, which include data from the Annual Social and Economic Supplement. 
We selected individuals aged over 18, with a yearly income over \$1,000 and working 20-60 hours per week, giving $n=64,380$ and $n=58,885$ in 2010 and 2019, respectively. 
The controls included characteristics of the place of residence, education, labor force status, migration status, household composition, housing type, health status, financial and tax records, reception of subsidies, and sources of income (beyond wage). 
Overall, there were $J=278$ controls, after adding 50 binary indicators for state. 

Since every state has its own regulatory, sociodemographic and political framework, we captured state effects by adding interactions for each pair of treatment and state. On these interactions, we applied a sum to zero constraint, so that the coefficients associated to the four treatments remain interpretable as average effects across the USA, and the interactions as deviation from the average. Hence, overall, we have $T = 4+4 \times 50 = 204$ treatments, our main interest being in the first four. 
To simplify computation in our CIL prior we assumed a common $\theta_t$
%From a computational perspective, to deal with this amount of treatments, which include interactions, we make an amendment to our CIL prior: we assume that for a main treatment $d_{i,t}$ and every interaction with it we have one common $\theta_{t}$, i.e. $\theta_t$ is
shared between each main treatment and all its interactions with state, so that $\mbox{dim}(\btheta)=5$. %Thus, we limit the dimensionality of $\btheta$ to $4+1$.
A full list of the extracted variables can be found as a supplementary file.%\omcom{add the exact pointer here}.
%A full list of the extracted variables can be found in Section SXXX \davidcom{State what section, I did not see this info in the supplement}.
%\davidcom{Miquel, provide a list of the finally used variables in a supplementary file, and the R script doing the data pre-processing.} 
%We collected a total of 232 initial available control variables.  The number of controls is reduced to 162 after removing variables with insufficient variability or redundancies. After converting categorical variables into dummy variables, the number of control variables increases to 228, including the intercept. We added 50 dummy variables reporting which U.S. State the observation belongs to. 

%Our objective is two-fold. First, we want
To study issues related to under- and over-selection, we analyzed the original data and two augmented datasets where we added artificial controls correlated with the treatments but not the outcome. The augmented data incorporated 100 artificial controls in the first scenario, and 200 in the second one, which were split into four subsets of size 25 and 50, respectively. Each of these subsets was designed to correlate to one of the four main treatments. Section \ref{sec:fakepreds_supp} summarizes the simulation procedure.
The resulting average correlation between gender and its associated artificial variables was of $0.83$, and analogously of $0.69$, $0.76$ and $0.67$ for black race, ethnicity and place of birth with their corresponding correlated variables, respectively. %\omcom{can't follow the notation here and the meaning either. First, how does this construction give correlation with EACH of the treatments? the math suggest that it is with one of them (and then maybe implicitly with the others if they are between them correlated). second, why is bold z? Cannot follow this at all. can you please write carefully since it is important and is unclear}


\subsection{Salary survey results}
\label{sec:cps_results}

%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.5]{figApp1.pdf} 
%\caption{Inference on treatment effects of gender and race for selected methods, for 2010 and 2019. Vertical axes show the size of the effect on the log hourly wage for the respective treatment. A negative sign corresponds to the existence of a negative wage discrimination, and vice versa. Dots and squares correspond to point estimates, while braces show the corresponding 95\% confidence band. Dots represent significant point estimates, chosen as $p\text{-value} < 0.05$ for the frequentist methods, or marginal posterior inclusion probability above 0.5 for Bayesian methods. Results in black correspond the estimation under the true data, while the grey results correspond to the estimation for the artificially enhanced datasets with 100 and 200 fake predictors, sequentially.}
%\label{fig:fig6}
%\end{figure}

None of the methods pointed to an association between salary and ethnicity nor place of birth, see Figure \ref{fig:figApp1B}. Figure \ref{fig:intro1} reports the results for gender and race. The treatment effect for gender is picked up by all methods in both years with similar point estimates. All methods found a significant decrease of this effect in 2019. 
When adding the artificial controls, the confidence intervals for OLS and DML became notably wider, which in 2019 led to a loss of statistical significance. This points towards a relevant loss in power due to over-selecting irrelevant controls, with the associated variance inflation.
The CIL results were particularly robust to the artificial controls.

As for race, we obtained more diverse results. In 2010, DML, BMA and CIL found a negative association between black race and salary. However, in 2019 DML and BMA found no significant association, OLS found a small positive association, and CIL was the only method finding a negative association in both years.
Once we introduce the artificial controls, we observe that OLS and DML suffer a large variance inflation, and in 2010 BMA suffers a significant loss of power, failing to detect an effect that was confidently picked up in the original data. 
On the other end, CIL experiences no perceptible change to adding the artificial controls. These results seem to suggest that CIL has sufficient power to detect the difference that other methods miss in the original data in 2019. 

The full scope of our proposed approach is materialized when considering more complex functions of the parameters. We study a measure of overall treatment contribution to deviations from the average salary. For a new observation $n+1$, with observed treatments $\mb{d}_{n+1}$ and controls $\mb{x}_{n+1}$, let
\begin{align}
  & h_{n+1}(\mb{d}_{n+1}, \balpha, \mb{x}_{n+1}) = \nonumber \\
  & | \E(y_{n+1} \mid \mb{d}_{n+1}, \mb{x}_{n+1}, \balpha, \bbeta) - \E(y_{n+1} \mid \mb{x}_{n+1}, \balpha, \bbeta) |
%\nonumber \\
%=(\tr{\mb{d}_{n+1}} \balpha + \tr{\mb{x}_{n+1}} \bbeta) - (\tr{\bar{\mb{d}}_{i}} \balpha + \tr{\mb{x}_{n+1}} \bbeta) 
= | \tr{[\mb{d}_{n+1} - \E(\mb{d}_{n+1} \mid \mb{x}_{n+1})]} \balpha | \label{eq:app2}
\end{align}
be its expected salary minus the expected salary averaged over possible $\mb{d}_{n+1}$, given equal control values $\mb{x}_{n+1}$.
%Naturally, this quantity also depends on $\mb{x}_{n+1}$, which we drop from the notation for simplicity.
Since $y_{n+1}$ is a log-salary, we examine the posterior predictive distribution of $\exp \left\{ h_{n+1}(\mb{d}_{n+1}, \balpha, \mb{x}_{n+1}) \right\}$ as a measure of salary variation associated to the treatments. A value of 1 indicates no deviation from the average salary, relative to another individual with the same controls $\mb{x}_{n+1}$.


\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{figApp2.pdf} 
\includegraphics[scale=0.6]{figApp2B.pdf}
%\includegraphics[scale=0.6]{figApp2B_old.pdf}
\caption{The left panel shows the posterior predictive distribution of deviations from average salary as given by $\exp \left\{ h_{n+1}(\mb{d}_{n+1}, \balpha, \mb{x}_{n+1}) \right\}$ in \eqref{eq:app2}, for 2010 and 2019. The gray boxes represent 50\% posterior intervals and the black lines are 90\% intervals. The black dot is the posterior median. The right panel shows the posterior median of these deviations for every U.S. state in 2010 and 2019 on the horizontal axis, ordered by their value in 2019, with the corresponding 50\% posterior intervals for both years.}
\label{fig:salary_variation}
\end{figure}

To evaluate the posterior predictive distribution of \eqref{eq:app2} given $\by$, the observed $\bd$ and the set of controls, we obtained posterior samples from the model averaged posterior $p(\balpha \mid \by)$ associated to CIL (Section \ref{ssec:bma}). Given that we do not have an explicit model for $(\bd_{n+1}, \mb{x}_{n+1})$, we sampled pairs $(\mb{d}_{n+1}, \mb{x}_{n+1})$ from their empirical distribution, and estimated $\E(\mb{d}_{n+1} \mid \mb{x}_{n+1})$ from a logistic regression of $\bd$ on the set of controls.
Figure \ref{fig:salary_variation} shows the results. In 2010, joint variation in the treatments was associated to an average 5.4\% salary variation (90\% predictive interval [0.1\%, 18.3\%]). The posterior mean in 2019 dropped to 1.5\% and the 90\% predictive interval was [0\%, 4.9\%]. That is, the treatments not only played a smaller role in the 2019 average, but there was also a strong improvement in inequality, e.g. individuals whose salary was furthest from the average. %, conditional on their control values $\mb{x}_{n+1}$.

It is also of interest to study differences between states. This is possible in our model, which features 200 interaction terms for the 4 treatments and 50 states. Figure \ref{fig:salary_variation} (right) shows the results. The most salient feature is a lower heterogeneity across states in 2019 relative to 2010.
%At the state level, in 2010 we observe Wyoming (2.7\%), Maine (2.9\%) and South Dakota (3.1\%); and Pennsylvania (6.4\%), New York (6.0\%) and Ohio (6.0\%), as those states with smallest and largest posterior average deviations, respectively. In 2019, the states with smallest average deviations were the same three (at roughly 0.8\%), and those with largest ones were Pennsylvania (1.9\%), Illinois (1.7\%) and Kentucky (1.7\%). 
The states whose median improved the most were among the lowest ranking states in 2010: Pennsylvania (reducing it by 3.7\%), Indiana (3.7\%) and Ohio (3.6\%), while those improving the least (Nebraska aside, whose median is 0\% on both years) were Maine (0.2\%), Wyoming (0.2\%) and South Dakota (0.3\%), which were already among the top-ranking states in 2019. This points towards a gradual catch-up effect across U.S. states, although the intervals still show some variability within states.
%\omcom{There are still many issues with this part, apart from being clearly more poorly written than any of the previous ones. Please talk to David and in between the two of you polish this. I do not need to see this again but please try and improve. 1. I thought David was not using an average of d anymore but some typical values. clearly this changed again to average? just check with him. 2. we actually have a model for d given x, the one we use for CIL. why are we not using directly that? 3. you say use sample the x's from empirical but you are also the d's and it is confusing you don't write this so. 4. it is confusing why h is written as a function of alpha and d and not x. 5. Is there some reason why there is enormous variability in 2010 and so much less in 2019? are you sure you are not doing something wrong? }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simulation Studies} \label{subsec:singleT}
%\subsection{Single Treatment Model} \label{subsec:singleT}

To illustrate issues associated to under- and over-selection of controls, a key factor we focus on is the \textit{level of confounding}.
Our scenarios range from no confounding (no controls affect both $\by$ and $\bd$) to complete confounding (any control affecting $\by$ also affects $\bd$, and vice versa). We also considered the effect of dimensionality, treatment effect sizes $\alpha$, and true level of sparsity (number of active controls).

We considered a Gaussian outcome and a single treatment $(T=1)$, and an error variance $\phi = 1$. 
The controls were obtained as independent Gaussian draws $\bx_i \sim \text{N}(\mb{0}, \mb{I})$, and any active control had a coefficient $\beta_{j} = 1$.
The treatment $\bd$ was also Gaussian with its mean depending linearly on the controls, unit error variance,  and any control having an effect on $\bd$ had unit regression coefficient.
Throughout, the number of controls that truly had affect $\bd$ was set equal to the number of controls that affect the outcome $\by$.
We measured the RMSE of the estimated $\hat{\alpha}$. 
%A key factor we focus on is the \textit{level of confounding}, i.e. how many controls truly affect both the treatment and the outcome. 


Figures \ref{fig:intro2}, \ref{fig:singletreat_growingdim}, \ref{fig:fig1b} and \ref{fig:multitreat_uncounfounded} summarize the results.
%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.6]{fig1A.pdf} 
%\caption{Single treatment parameter RMSE (relative to Oracle OLS) based on $R=250$ simulated datasets for each level of confounding. On each $x$-axis we plot increasing levels of confounding from left (no confounding) to right (complete confounding). Left panel shows results for $\alpha = 1$ (strong signal), center for $\alpha = 1/3$ (weak signal), and right for $\alpha^{*} = 0$ (lack of effect). In all panels, $n=100$, $J+T=50$ and $\norm{\bgamma}_{0} = 6$.}
%\label{fig:fig1}
%\end{figure}
Figure \ref{fig:intro2} shows that the RMSE of BMA and LASSO worsens as confounding increases, this was due to a lower power to select all relevant confounders (see Figure \ref{fig:fig1b} for model selection diagnostics), i.e. an omitted variable bias. 
%We observe that generic methods perform as one might expect: as confounding increases they have more difficulties to pick up every correlated active control, and thus tend to under-select relevant controls, running into omitted variable bias. BMA suffers less than LASSO, in part because it has the advantage of knowing the true underlying sparsity via the model prior. 
These effects have been well studied. % and occur regardless of the different treatment effect strengths. 
Methods such as DML and BAC were designed to prevent omitted variables, but as shown in Figure \ref{fig:intro2} they can run into over-selection when there truly are few confounders.
%Specific methodology developed to address them, such as DL and BAC, are strongly focused on not losing relevant variables by encouraging inclusion of controls that correlate to the treatment. This is highly beneficial in high-confounding situations, however, with low-confounding it can lead to non-random over-selection of controls, causing a notable inflation of the variance of the estimator, and leading to a different source of bias. 
%Over-selection bias arises from the inclusion of covariates strongly correlated to an active treatment that are truly inactive on the outcome. Issues related to over-selection are often overlooked as generally it is preferred to err on the side of model inflation but, importantly, here we observe that the magnitude of these effects can be of similar order to those related to control under-selection, and hence should be taken equally into account. 
%\cred{PCR attained a quite satisfactory and steady performance in point estimation throughout.}
In contrast, our CIL performed well at all levels of confounding.
%When treatment size is weak, however, it can suffer more to detect it with precision, in the general line of Bayesian variable selection based methods, although it still performs close to competing methods. 
The Empirical Bayes and the Expectation Propagation versions of CIL provide nearly indistinguishable results (not shown).
It is worth noting that, when the treatment truly had no effect $(\alpha = 0)$, CIL provided a strong shrinkage that delivered a significantly lower RMSE than other methods.
%  shows the partition of the methods between those forcing treatment inclusion and those not forcing it. The latter methods are capable of achieving results better than oracle OLS in this scenario, thanks to detecting that the treatment is inactive with a precise point estimate at zero.


Figure \ref{fig:singletreat_growingdim} extends the analysis to consider a growing number of covariates, under a strong treatment effect ($\alpha=1$). As dimensionality grew, standard LASSO and BMA incurred a significantly higher RMSE under strong confounding. Our CIL generally provided marked improvements over BMA, except for the larger $J+T=200$.
%Figure \ref{fig:singletreat_growingdim} shows the results for different design sizes under a strong treatment effect. Overall results exhibited a similar trend to Figure \ref{fig:intro2}, with exacerbating under- and over-selection effects as dimensionality grows.
%\cred{PCR suffered the strongest deterioration in higher dimensions, with strong spikes in RMSE as $J$ grows relative to $n$. Their authors anticipate that PCR requires $n$ to grow at sufficiently fast rates relative to $J$ to achieve good performance.}
%Generally, CIL continued to perform very well for all confounding levels under $J \leq N$. As for the case with $J > N$, no method achieved satisfactory performance consistently. Our method suffered not to over-select as confounding levels decrease, although it had the smallest spike in RMSE relative to the rest of methods. 
Here we observed the only perceptible differences between the EB and EP approximations, with the former attaining better results, pointing to advantages of the EB approach in higher dimension. Figure \ref{fig:fig3} further extends the analysis to less sparse settings, with $\norm{\gamma}_{0}=6$, 12 and 18 active parameters. Overall, the results were similar to Figures \ref{fig:intro2} and \ref{fig:singletreat_growingdim}.
%Figure \ref{fig:singletreat_growingdim}  examines the situation in which the total amount of confounders varies, with very similar relative results as those exhibited in Figures \ref{fig:intro2} and \ref{fig:singletreat_growingdim}.


A focus in this paper is to understand over-selection issues in multiple treatment inference.
%To this end, we added a second treatment that was truly uncounfounded with any covariate. 
%Additionally, we extended some of these simulations to $T=2$, and hence $\balpha = (\alpha_{1}, \alpha_{2})$. We restricted confounding to the first treatment, where the subsets of controls associated to each treatment were disjoint and of equal size. The rest of elements in the design were kept equal to the design for $T=1$. 
To this end, we added a multiple treatments design with an increasing number of treatments, with a maximum of $T=5$. There, every present treatment was active, setting $\alpha_t = 1$ on all treatments. For all levels of $T$, we set $\beta_j = 1$ for $j = 1, \dots, 20$, denoting the set of active controls by $\bx_{1:20}$, and $\beta_j = 0$ for the rest of controls $\bx_{21:J}$. Regarding the association between treatments and controls, $\bx_{1:20}$ were divided into five disjoint subsets with four variables each, and each of these subsets was associated to a different treatment. When existing, the treatments were linearly dependent on every control of its associated subset. Additionally, each treatment also depended on a further subset of controls in the set $\bx_{21:J}$. In this case, the size of such subset was increasing by four with each added treatment: treatment 1 was associated to $\bx_{21:24}$, treatment 2 was associated to $\bx_{21:28}$, etc., up to treatment 5, which correlated to $\bx_{21:40}$. All controls affecting a treatment had a unit linear coefficient. The rest of the design, including the DGP of the controls and the error variances, is akin to that in Figure \ref{fig:intro2}.
We also replaced BAC with the ACPME method %the Adjustment for Confounding in the Presence of Multivariate Exposures method 
of \citep{Wilson18}, an extension of BAC for multiple treatments.

\begin{figure}[h]
\centering
%\begin{tabular}{cc}
%Treatment 1 & Average across Treatments 1 to 5 \\
%\includegraphics[scale=0.75]{figXp1.pdf} &
%\includegraphics[scale=0.75]{figXp2.pdf} 
%\end{tabular}
\includegraphics[scale=0.8]{figXp2.pdf} 
\caption{Treatment parameter RMSE (relative to oracle least-squares) based on $R=250$ simulated datasets at every value of $T$, for $n=100$, $J=95$, and $T \in \{2, 3, 4, 5 \}$. For every $T$ ($x$-axis), we show the average RMSE across Treatments $1,\ldots,T$.
%, i.e. at $T=2$ we show the average of two RMSE measures (one per treatment), at $T=3$ across three of them, etc. 
%Here the $x$-axis indicates the number of treatments in the model.
}%In both diagrams, the $x$-axis indicates the number of treatments in the model. The left panel shows the results for Treatment 1, while the right panel shows the average across present treatments at each level of $T$, i.e. at $T=2$ it shows the average RMSE across two treatments, at $T=3$ across three treatments, etc.}
\label{fig:multitreat_uncounfounded}
\end{figure}

Figure \ref{fig:multitreat_uncounfounded} shows the estimation results on $\balpha$ for the different values of $T$, akin to Figure \ref{fig:intro2}. We observe similar trends as before. Methods prone to over-selection recovered more inactive controls as $T$ increased, i.e. they included too many controls affecting the inference of those treatments they correlated to. Some of these controls were increasingly influential with $T$ as they were associated to more treatments, and so they became harder to discard. Despite that, under-selection (here suffered by BMA) was also problematic, as for larger $T$ the model became highly confounded, as a subset of the controls accounted for a larger proportion of the variance in the outcome, as well as for that of the treatment(s). This led to BMA discarding with high probability active but highly correlated variables between treatments and confounders. Additionally, we also observed stable but improving performance of ACPME, although in its best performing scenario its average RMSE still more than doubled that of oracle OLS. On the other end, our CIL proposal was able to achieve oracle-type performance for every examined level of $T$.

%Figure \ref{fig:multitreat_uncounfounded} shows that over-selection issues related to Treatment 1 carry over to Treatment 2: DML and ACPME incurred a larger RMSE, roughly the same as that for Treatment 1 under no confounding. These results illustrate how over-selection issues may propagate over treatments.
%We report the results on a set of strong treatment effects for a maximum of six active confounders in Figure \ref{fig:multitreat_uncounfounded}. \cred{As for the confounded treatment, we observed almost no change with respect to its counterpart in the single treatment case, although there was a drop in performance of our method only in the complete confounding case, where it fell short of LASSO and ACPME, despite still beating regular BMA. ACMPE showed very similar performance for $T=2$ relative to BAC in $T=1$. 
%On the second treatment, results are generally consistent through levels of confounding, as these levels refer only to the first treatment. Here our proposed method achieves practically oracle performance, alongside BMA. For neither treatment do we appreciate differences between EB and EP versions.}

%As for the confounded treatment, we see a general drop in performance of all methods with respect to oracle OLS, although the relative performances experience little variation, and all of the effects described before remain present. The ACPME method improves substantially the performance of BAC on a single treatment, in Section \ref{subsec:singleT}. All in all, our approach achieves better results than every compared method in most situations, and performs very close to the top performer in the remaining scenarios. On the second treatment, results are generally consistent through levels of confounding, as these levels refer only to the first treatment. We observe a similar relative performance among tested methods: here our proposed method achieves practically oracle performance, alongside BMA. For neither effect we appreciate differences between EB and EP versions. The problems encountered by \cred{DL} are very explicit here since the effect of the regressors dragged into the model by the first treatment propagate to harm inference on the effect of the second one, causing a sizeable RMSE inflation throughout the panels.

%\davidcom{Maybe remove Figure \ref{fig:fig4or} and Figure \ref{fig:fig5}? If we keep them we should explain why did we do the exercise, and explain the findings better, I couldn't make sense of the explanation. Frankly, it feels like the simulation section is long enough already without them.}
%Figure \ref{fig:fig5} examines the situation where the treatment effects were weak or not present, akin to panels in Figure \ref{fig:intro2}, with similar relative results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Related Literature}
% 
%Literature on single treatment effect estimation has grown considerably over the last decade, with the main objective of addressing pitfalls in parameter estimation that arise under general variable selection methods. Frequentist approaches particular to the single treatment problem concentrate around varied forms of penalized likelihood (PL), which make them computationally very appealing and suitable for high-dimensional settings. One of the most popular is the Double Machine Learning (DML) approach, presented in \cred{Belloni et al. (2014)}, a two-step procedure by which one analizes the output and exposure equations separately with the objective of identifying which controls affect either output, to then fits a regular model \textit{ex-post} using the union set of selected controls. Its main concern is to avoid estimation bias through fully adjusting for confounding, that is, including into the model at least any control relevant to the treatment or the outcome. \cred{Farrell (2015)} builds upon similar ideas with a doubly-robust estimator attempting to safeguard from model selection mistakes after the double selection step, in the context of multi-valued treatments, resonating also with \cred{Antonelli et al. (2018)}, who use matching on propensity and prognostic scores on a similar direction. \cred{Shortreed and Ertefaie (2017)} employ a two-step approach as well, in this case using adaptive Lasso on the exposure equation, showing improvement on confounder-selection results. See also \cred{Ghosh et al (2015)} with shared and difference Lasso for similar multi-step contributions. \cred{Ertefaie et al. (2015)} use joint likelihood $L_1$ penalization of both equations instead, in order to accommodate for the information shared between the two equations. \cred{Ma et al. (2019)} combine regularization with sufficient dimension reduction into an estimator that achieves good asymptotic properties without requiring model selection consistency. Yet, even if some of these proposals do allow for inference, most of them are essentially designed with sufficient control selection in mind and tackle point estimation only after selection has been conducted, generally without a focus on quantification of uncertainty. \cred{Chernozhukov et al. (2018)} elaborate on the DML technique with a more explicit attempt at inference seeking dependence reduction on the selection step, introducing an additional debiasing operation combining Neyman-orthogonal scores on the first equation, with cross-fitting to address overfitting concerns. Other proposals exist focused on inference for PL-related methods in this particular context, see \cred{Athey et al. (2016)} in the context of binary treatments, see also \cred{Vansteelandt, Bekaert and Claeskens (2012)}, as well as more recent work in \cred{Dukes and Vansteelandt (2019)}. Generally, PL-based proposals are heavily focused on asymptotic results regarding estimation efficiency and their distributional properties, which follow from attaining guarantees of sufficient control selection, i.e. avoiding detection errors of relevant controls connected to either outcome or treatment. In contrast, they are not too concerned with excess inclusion of any other spurious variable. This can be problematic based on the aforementioned reasons, which in turn often limit their oracle performance to a super-model of the true outcome model, inflated in size with unnecessary controls that may relate to the treatment only. It is also worth mentioning that a number of these proposals are not designed for continuous or even multi-valued treatments, or sometimes outcomes.
% 
%From a Bayesian perspective, a number of different proposals also exist. A widely referenced method is Bayesian Adjustment for Confounding (BAC) as presented in \cred{Wang, Parmigiani and Dominici (2012)}, a joint-modelling approach which essentially employs a Bayesian Model Averaging (BMA) approach under a specifically designed model space prior. This prior is separable across controls, and is governed by a hyperparameter $\omega \in [1, \infty)$ that represents the odds of including a control in the outcome model conditional on that same control being included in the exposure model. This is quite helpful since for any finite $\omega$ control inclusion on the outcome model is only encouraged and not forced, allowing for some model flexibility that combines with the accounting of model uncertainty in the averaged point estimate. This entanglement across equations poses a series of questions, however, mainly related as to how one should set such a sensitive hyperparameter, combined with the fact that resulting marginal prior inclusion probabilities are generally high, hence encouraging model size inflation. This can hinder performance notably in high-dimensional settings as over-selection problems may enter into play. Additionally, computational demands arising from joint equation modelling can quickly become insurmountable. Further contributions to BAC include \cred{Lefebvre et al. (2014)} and \cred{Wang et al. (2015)}, which provide some theoretical support as well as further proposals on how to set $\omega$. Other articles build on the approach based on control selection with model averaging: \cred{Talbot et al. (2015)} introduce Bayesian causal effect estimation, a similar method to BAC that incorporates informative priors aimed at deterring excess control inclusion; \cred{Antonelli et al. (2017)} contribute with \textit{guided} BAC as a generalized framework on BAC addressing treatment effect heterogeneity, as well as additional technical questions. Similar methods have also been explored in propensity score analysis, as in \cred{Cefalu et al. (2015)}, but more generally around model uncertainty as well, see e.g. \cred{Ziegler and Dominici (2014)} and \cred{An (2010)}. See also \cred{Jacobi et al. (2016)} for methodological adaptations to dynamic effects in panel data. More recently, \cred{Hahn et al. (2018)} preserve the joint modelling approach but move away from model averaging, by addressing BAC using hierarchical priors, in an attempt to give some prior flexibility and ease computational difficulties via posterior sampling. This is a reparametrization technique designed to achieve debiased point-estimates using regularization priors. \cred{Hahn et al. (2019)} also extend this notion to non-parametric setups with Bayesian Causal Forests. \cred{Antonelli et al. (2019)} propose a spike-and-slab prior formulation with a prior distribution that places low shrinkage to controls associated with the treatment, combined with an Empirical Bayes algorithm for hyperparameter setting. Addressing computational concerns in high-dimensional setups, \cred{Wilson and Reich (2014)} propose Penalized Credible Regions (PCR), stemming from \cred{Bondell and Reich (2012)}. This is a decision theoretic approach that can be formulated as a PL method. It essentially uses the posterior credible region of the outcome regression parameters to form a set of possible models to choose from, and then apply $L_1$-type penalization with lighter penalties to those covariates that are associated to the treatment, as a function of their strength. This relies strongly on a the quality of the posterior mean, and the fact that the treatment is always included casts some difficulty on its single parameter estimation ability as it might sacrifice precision on one parameter to favor models that aggregately perform better predictively. PCR tends to be conservative as well as to dropping variables, depending on the penalty parameter, whose setting is an open end itself. It also introduces the notion of strength of relation between controls and treatment as a determinant of relevance on the outcome equation. Additionally, it naturally allows for the inclusion of multiple simultaneous treatments, which is absent in previously reviewed methods. On the other hand, its PL nature disallows PCR as a method of uncertainty quantification. This proposal has strong ties to the Adaptive Lasso \cred{(Zou, 2006)}, as well as relation to Bayesian Lasso strategies employing shrinkage priors \cred{(Park and Casella, 2008; Hans, 2010)}. All in all, it is worh noting that most current Bayesian proposals are also heavily concerned with omission of relevant controls, and despite extra flexibility they can run into similar problems related to over-selection, as those described under the frequentist paradigm.
% 
%As for literature related to the multiple treatments problem, to the best of our knowledge, the list of available methodology is short. Beyond PCR, perhaps the most notable recent contribution is ACPME by \cred{Wilson et al. (2018)}, a method with strong ties to BAC. This is also a BMA-based algorithm with a specific model prior that incorporates, for each feature, a measure of correlation strength between each control and the set of treatments, this time without imposing sudden jumps in prior probabilities. Similarly to BAC, the philosophy of ACPME is to tilt prior probabilities towards models fully adjusting for confounding. Again, this can put a good fraction of prior mass on super-models of the true outcome model, as feature inclusion can only be encouraged and, hence, is not designed to overcome over-selection problems. More so taking into account that marginal prior inclusion probabilities are capped below at 1/2 by construction. It is also unclear whether it can perform well in high dimensions or with a large number of treatments. Finally, in the context of treatment heterogeneity, a mention to recent work on the Debiased Orthogonal Lasso by \cred{Semenova et al. (2020)} that can potentially extend the frequentist DML scheme to the multiple treatments, although it might not be the focus of its current formulation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion} \label{sec:discuss}
%\davidcom{I added a bit of discussion, this needs to be refined, do not refrain from butchering at your leisure.}

The two main ingredients in our proposal are learning from data whether and to what extent control inclusion/exclusion should be encouraged to improve multiple treatment inference, and a convenient computational strategy to render the approach practical.
This is in contrast to current literature, which encourages the inclusion of certain controls to avoid under-selection biases but can run into serious over-selection bias and variance, as we have illustrated.
By learning the relative importance of potential confounders, as in our CIL framework, one may bypass this problem.

These issues are practically relevant, e.g. in the salary data we showed that one may fail to detect a negative association between the black race and salary. Further, the proposed Bayesian framework naturally allows for posterior predictive inference on functions that depend on multiple parameters, such as the variation in salary jointly associated with multiple treatments. Interestingly, our analyses revealed a reduced association between salary and potentially discriminatory factors such as gender or race in 2019 relative to 2010, as well as a lesser heterogeneity across states.
These results are conditional on controls that include education, employment and other characteristics that affect salary. That is, our results reveal lower salary discrepancies in 2019 between races/genders, provided that two individuals have the same characteristics (and that they were hired in the first place).
This analysis offers a complementary view to analyses that are unadjusted by controls, and which may reveal equally interesting information.
For example, if females migrated towards lower-paying occupational sections in 2019 and received a lower salary as a consequence, this would not be detected by our analysis, but would be revealed by an unadjusted analysis.

To keep our exposition simple, we focused our discussion on linear treatment effects, but it is possible to extend our framework to non-linear effects and interactions between treatments, e.g. via splines or other suitable non-linear bases.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Appendix}
%
%Text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\bigskip
%%\begin{center}
%%{\large\bf SUPPLEMENTARY MATERIAL}
%%\end{center}
%% 
%%\begin{description}
%% 
%%\item[Title: \texttt{supplement}.] Supplementary material, including proofs, computational methodology, and further complementary numerical results. (.pdf file)
%% 
%%\item[Title: \texttt{cps\_00004.cbk}.] List and description of variables employed in the CPS salary data. (.txt file)
%% 
%%\item[\texttt{R}-code for CPS dataset and numerical experiments:] \texttt{R}-code to reproduce the results in this article, and to obtain the necessary CPS data. (.zip file, available upon request)
%% 
%%%\item[\texttt{R}-code to implement CIL:] available at \texttt{https://github.com/mtorrens/rcil}. %R-package MYNEW containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)
%% 
%%%\item[CPS data set:] \cred{Format TBD.} (.\cred{XXX} file) %Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)
%% 
%%\end{description}


\newpage
\section*{Supplementary material}



\renewcommand{\thethm}{S\arabic{theorem}}
\renewcommand{\theprop}{S\arabic{prop}}
\renewcommand{\thelemma}{S\arabic{lemma}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\theequation}{S\arabic{equation}}

\section{Proofs}

\subsection{Proof of Proposition \ref{prop:one}} \label{sec:proof_prop1}

Let $p(\by \mid \bgamma, \bdelta, \btheta) = p(\by \mid \bgamma, \bdelta)$, then
\begin{eqnarray}
\nabla_{\btheta} \log p(\by \mid \btheta) &=& \frac{\nabla_{\btheta} p(\by \mid \btheta)}{p(\by \mid \btheta)}  \nonumber \\
&=& \frac{\nabla_{\btheta} \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta, \btheta) p(\bgamma, \bdelta \mid \btheta)}{p(\by \mid \btheta)} \nonumber \\
&=& \frac{\sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) \nabla_{\btheta}  p(\bgamma, \bdelta \mid \btheta)}{p(\by \mid \btheta)} \nonumber \\
&=& \sum_{(\bgamma, \bdelta)} \frac{p(\by \mid \bgamma, \bdelta, \btheta)}{p(\by \mid \btheta)} \frac{p(\bgamma, \bdelta \mid \btheta)}{p(\bgamma, \bdelta \mid \btheta)} \nabla_{\btheta} p(\bgamma, \bdelta \mid \btheta) \nonumber \\
&=& \sum_{(\bgamma, \bdelta)} \frac{\nabla_{\btheta} p(\bgamma, \bdelta \mid \btheta)}{p(\bgamma, \bdelta \mid \btheta)} p(\bgamma, \bdelta \mid \by, \btheta) \nonumber \\
&=& \sum_{(\bgamma, \bdelta)} p(\bgamma, \bdelta \mid \by, \btheta) \nabla_{\btheta} \log p(\bgamma, \bdelta \mid \btheta).
\end{eqnarray}
If, further, the model prior satisfies $p(\bgamma, \bdelta \mid \btheta) = \prod_{t=1}^{T} p(\delta_{t}) \prod_{j=1}^{J} p(\gamma_{j} \mid \btheta)$, then
\begin{align*}
\nabla_{\btheta} \log p(\bgamma, \bdelta \mid \btheta) = \sum_{j=1}^{J} \nabla_{\btheta} \log p(\gamma_{j} \mid \btheta),
\end{align*}
and so
\begin{align*}
\nabla_{\btheta} \log p(\by \mid \btheta) = \sum_{j=1}^{J} \sum_{(\bgamma, \bdelta)} \nabla_{\btheta} \log p(\gamma_{j} \mid \btheta) p(\bgamma, \bdelta \mid \by, \btheta) = \sum_{j=1}^{J} \E\left[ \nabla_{\btheta} \log p(\gamma_{j} \mid \btheta) \mid \by, \btheta \right].
\end{align*}
\hfill $\blacksquare$

\subsection{Proof of Corollary \ref{prop:two}} \label{sec:proof_prop2}

The empirical Bayes estimate defined by \eqref{eq:marglik1} writes
\begin{align*}
\bthetaeb = \argmax_{\btheta \in \mathbb{R}^{T+1}} \log p(\by \mid \btheta) = \argmax_{\btheta \in \mathbb{R}^{T+1}} \log \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) p(\bgamma, \bdelta \mid \btheta).
\end{align*}
For short, denote $H(\btheta) = p(\by \mid \btheta)$ and $h_{j}(\btheta) = p_{j}(\gamma_j \mid \btheta)$, where generically $\nabla_{\btheta} \log H(\btheta) = \nabla_{\btheta} H(\btheta) / H(\btheta)$. Under the assumptions of Corollary \ref{prop:two}
\begin{eqnarray}
\nabla_{\btheta} H(\btheta) &=& \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) p(\bdelta) \nabla_{\btheta} \prod_{j=1}^{J} h_{j}(\btheta) \nonumber \\
&=& \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) p(\bdelta) \sum_{j=1}^{J} \left( \nabla_{\btheta} h_{j}(\btheta) \prod_{j \neq l} h_{l}(\btheta) \right).  \label{eq:prop2_eq1}
\end{eqnarray}
Denoting $\mb{f}_{j} = \tr{(1, f_{j,1}, \dots, f_{j,T})} $, direct algebra gives
\begin{align}
\nabla_{\btheta} h_{j}(\btheta) = \nabla_{\btheta}\left\{ \pi_{j}(\btheta)^{\gamma_j} (1-\pi_{j}(\btheta))^{1-\gamma_j} \right\} = (1-2\rho) \mb{f}_{j} (\gamma_j - \pi_{j}(\btheta)) h_{j}(\btheta), \label{eq:prop2_eq2}
\end{align}
since
\begin{align}
\nabla_{\btheta} \pi_{j}(\btheta) = (1-2\rho) \mb{f}_{j}  \pi_{j}(\btheta) (1-\pi_{j}(\btheta)). \label{eq:prop2_eq3}
\end{align}
Then, replacing \eqref{eq:prop2_eq2} into \eqref{eq:prop2_eq1}
\begin{eqnarray}
\nabla_{\btheta} H(\btheta) &=& \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) p(\bdelta) \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} (\gamma_j - \pi_{j}(\btheta)) \prod_{j=1}^{J} f_{j}(\btheta) \nonumber \\
%&=& \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} (\gamma_j - \pi_{j}(\btheta))  \sum_{(\bgamma, \bdelta)} p(\by \mid \bgamma, \bdelta) p(\bdelta) p(\bgamma \mid \btheta) \nonumber \\
&=& \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} \sum_{(\bgamma, \bdelta)} (\gamma_j - \pi_{j}(\btheta)) p(\by \mid \bgamma, \bdelta) p(\bdelta, \bgamma \mid \btheta) \nonumber \\
&=& \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} \left[ (1-\pi_{j}(\btheta)) \sum_{(\bgamma, \bdelta): \gamma_{j}=1} p(\by, \bdelta, \bgamma \mid \btheta) - \pi_{j}(\btheta) \sum_{(\bgamma, \bdelta): \gamma_{j}=0} p(\by, \bdelta, \bgamma \mid \btheta) \right]. \nonumber
\end{eqnarray}
Finally
\begin{eqnarray}
\nabla_{\btheta} \log H(\btheta) &=& \frac{\nabla_{\btheta} H(\btheta)}{H(\btheta)} \nonumber \\
&=& \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} \left[ (1-\pi_{j}(\btheta)) \frac{\sum_{(\bgamma, \bdelta): \gamma_{j}=1} p(\by, \bdelta, \bgamma \mid \btheta)}{\sum_{(\bgamma, \bdelta)} p(\by, \bdelta, \bgamma \mid \btheta)} - \pi_{j}(\btheta) \frac{\sum_{(\bgamma, \bdelta): \gamma_{j}=0} p(\by, \bdelta, \bgamma \mid \btheta)}{\sum_{(\bgamma, \bdelta)} p(\by, \bdelta, \bgamma \mid \btheta)} \right] \nonumber \\
&=& \sum_{j=1}^{J} (1-2\rho) \mb{f}_{j} \left[ (1-\pi_{j}(\btheta)) P(\gamma_{j} = 1 \mid \by, \btheta) - \pi_{j}(\btheta) (1 - P(\gamma_{j} = 1 \mid \by, \btheta)) \right] \nonumber \\
&=& (1-2\rho) \sum_{j=1}^{J} \mb{f}_{j} \left[ P(\gamma_{j} = 1 \mid \by, \btheta) - \pi_{j}(\btheta) \right]. \nonumber
\end{eqnarray}
\hfill $\blacksquare$

\subsection{Proof of Proposition \ref{prop:three}}  \label{sec:proof_prop3}

Consider the right-hand side in \eqref{eq:marglik1},
\begin{align}
\argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{(\bdelta, \bgamma)} p_{u}(\bdelta, \bgamma \mid \by) p(\bdelta, \bgamma \mid \btheta)
\label{eq:l1eq1}
\end{align}
where $p_{u}(\bdelta, \bgamma \mid \by)$ are the posterior probabilities under a uniform prior $p_{u}(\bdelta, \bgamma) \propto 1$.

We seek to set the parameters $s_t$ and $q_j$ in the approximation
\begin{align}
\hat{p}_{u}(\bdelta, \bgamma \mid \by) = \prod_{t=1}^{T} \text{Bern}(\delta_{t}; s_{t}) \prod_{j=1}^{J} \text{Bern}(\gamma_{j}; q_{j})
\nonumber
\end{align}
using Expectation Propagation. That is, setting and $\mb{q} = (q_1, \dots, q_J)$ such that
\begin{align*}
\mb{q}^{\textsc{ep}} = \argmax_{\mb{q} \in [0,1]^{J}} \sum_{(\bgamma, \bdelta)} p_{u}(\bdelta, \bgamma \mid \by) \log \left(\prod_{t=1}^{T} s_{t}^{\delta_{t}} (1-s_{t})^{1-\delta_{t}} \prod_{j=1}^{J} q_{j}^{\gamma_{j}} (1-q_{j})^{1-\gamma_{j}}  \right).
\end{align*}
and analogously for $\mb{s} = (s_1, \dots, s_T)$. Proceeding elementwise, we derive
\begin{eqnarray}
q_{j}^{\textsc{ep}} &:=& \argmax_{q_j \in [0,1]} \sum_{(\bgamma, \bdelta)} p_{u}(\bdelta, \bgamma \mid \by) \times \nonumber \\
&& \times \left( \sum_{j=1}^{J} [\gamma_j \log q_{j} + (1 - \gamma_{j}) \log(1-q_{j})] + \sum_{t=1}^{T} [\delta_t \log s_{j} + (1 - \delta_{t}) \log(1-s_{t})] \right) \nonumber \\
&=& \argmax_{q_j \in [0,1]} \sum_{(\bgamma, \bdelta)} p_{u}(\bdelta, \bgamma \mid \by) \left( \sum_{j=1}^{J} [\gamma_j \log q_{j} + (1 - \gamma_{j}) \log(1-q_{j})] \right) \nonumber \\
&=& \arg \max_{q_j \in [0,1]} \sum_{j=1}^{J} \sum_{(\bgamma, \bdelta)} p_{u}(\bdelta, \bgamma \mid \by) \left[ \gamma_j \log q_j + (1 - \gamma_j) \log (1 - q_j) \right]. \nonumber
\end{eqnarray}
Optimizing this expression yields
\begin{eqnarray}
\frac{\partial}{\partial q_j} = 0 &\Leftrightarrow& \sum_{(\bgamma, \bdelta)} p_{u}(\bdelta, \bgamma \mid \by) \left( \frac{\gamma_j}{q_{j}^{\textsc{ep}}} - \frac{1 - \gamma_j}{1 - q_{j}^{\textsc{ep}}} \right) = 0 \nonumber \\
&\Leftrightarrow& \frac{1}{q_{j}^{\textsc{ep}}} \sum_{(\bgamma, \bdelta): \gamma_j = 1} p_{u}(\bdelta, \bgamma \mid \by) - \frac{1}{1 - q_{j}^{\textsc{ep}}} \sum_{(\bgamma, \bdelta): \gamma_j = 0} p_{u}(\bdelta, \bgamma \mid \by) = 0 \nonumber \\
&\Leftrightarrow& \frac{P_{u}(\gamma_j = 1 \mid \by)}{q_{j}^{\textsc{ep}}} - \frac{P_{u}(\gamma_j = 0 \mid \by)}{1 - q_{j}^{\textsc{ep}}} = 0 \nonumber \\
&\Leftrightarrow& q_{j}^{\textsc{ep}} = P_{u}(\gamma_j = 1 \mid \by) = P(\gamma_j = 1 \mid \by, \btheta = \mb{0}). \label{eq:prop3_eq2}
\end{eqnarray}
With the same exact procedure one analogously obtains $s_{t}^{\textsc{ep}} = P_{u}(\delta_t = 1 \mid \by)$. Let
\begin{align*}
h(\bdelta) := \prod_{T=1}^{T} \text{Bern}(\delta_t; s_{t}^{\textsc{ep}}) \prod_{j=1}^{J} \text{Bern}(\delta_t; \pi_{t}) = \prod_{t=1}^{T} \left[ s_{t}^{\textsc{ep}} \pi_t \right]^{\delta_t} \left[ (1-s_{t}^{\textsc{ep}}) (1-\pi_t) \right]^{1-\delta_t},
\end{align*}
which is independent of $\btheta$, and where $\pi_{t}$ is the marginal prior inclusion probability within our framework. Then, implementing the approximation \eqref{eq:prop3_eq2} into \eqref{eq:l1eq1} gives
\begin{eqnarray}
\bthetaep &:=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{(\bgamma, \bdelta)} h(\bdelta) \prod_{j=1}^{J} \text{Bern}(\gamma_j; q_{j}^{\textsc{ep}}) \prod_{j=1}^{J} \text{Bern}(\gamma_j; \pi_j(\btheta)) \nonumber \\
&=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{(\bgamma, \bdelta)} h(\bdelta) \prod_{j=1}^{J} \left[ q_{j}^{\textsc{ep}} \pi_j(\btheta) \right]^{\gamma_j} \left[ (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta)) \right]^{1-\gamma_j}. \label{eq:prop3_eq3}
\end{eqnarray}
Note that the product in the RHS of \eqref{eq:prop3_eq3} defines a probability distribution on $(\delta_1, \dots, \delta_{T}, \gamma_1, \dots, \gamma_{J})$ with independent components, hence the sum is the normalizing constant of such distribution. Thus, this constant is just the product of the univariate normalizing constants. The univariate normalizing constant of each Bernouilli is then
\begin{align*}
q_{j}^{\textsc{ep}} \pi_j(\btheta) + (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta))
\end{align*}
for every $q_{j}$, and similarly $s_{t}^{\textsc{ep}} \pi_t + (1-s_{t}^{\textsc{ep}}) (1-\pi_t)$ for every $s_{t}$.
Hence, replacing into \eqref{eq:prop3_eq3} we obtain
\begin{eqnarray}
\bthetaep &:=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \prod_{j=1}^{J} \left\{ q_{j}^{\textsc{ep}} \pi_j(\btheta) + (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta)) \right\} \prod_{t=1}^{T} \left\{ s_{t}^{\textsc{ep}} \pi_{t} + (1-s_{t}^{\textsc{ep}}) (1-\pi_{t}) \right\}. \nonumber \\
&=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{j=1}^{J} \log \left( q_{j}^{\textsc{ep}} \pi_j(\btheta) + (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta)) \right). \nonumber
\end{eqnarray}
\hfill $\blacksquare$

%\subsection{Gradient of the log of function optimized in \eqref{eq:ep_eq0} in Proposition \ref{prop:three}} \label{sec:gradEP}
\subsection{Gradient of the function optimized in \eqref{eq:ep_eq0} in Proposition \ref{prop:three}} \label{sec:gradEP}

From \eqref{eq:ep_eq0}, for a given set of $q_{j}$ we have
%\begin{eqnarray}
\begin{align}
\bthetaep = \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{j=1}^{J} \log \left( q_{j}^{\textsc{ep}} \pi_j(\btheta) + (1-q_{j}^{\textsc{ep}}) (1-\pi_j(\btheta)) \right). \label{eq:prop3b_eq1}
\end{align}
%\bthetaep &=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \prod_{j=1}^{J} \left[ q_{j} \pi_{j}(\btheta) + (1-q_{j}) (1-\pi_{j}(\btheta)) \right] \nonumber \\
%&=& \argmax_{\btheta \in \mathbb{R}^{T+1}} \sum_{j=1}^{J} \log \left( q_{j} \pi_{j}(\btheta) + (1-q_{j}) (1-\pi_{j}(\btheta)) \right). \nonumber \label{eq:prop3_eq1}
%\end{eqnarray}
We are interested in computing the gradient of the function being optimized in \eqref{eq:prop3b_eq1}. Denote $h_{j}(\btheta) := q_{j} \pi_{j}(\btheta) + (1-q_{j}) (1-\pi_{j}(\btheta))$ for short. Simple algebra provides
\begin{align*}
\nabla_{\btheta} h_{j}(\btheta) = (2 q_j - 1) \nabla_{\btheta} \pi_{j}(\btheta).
\end{align*}
From \eqref{eq:prop2_eq3} we recover the remaining gradient in the last expression and derive
\begin{align*}
\nabla_{\btheta} \log h_{j}(\btheta) = \frac{\nabla_{\btheta} h_{j}(\btheta)}{h_{j}(\btheta)} = \frac{2 q_{j} - 1}{h_{j}(\btheta)} \left[ (1-2\rho) \mb{f}_{j}  \pi_{j}(\btheta) (1-\pi_{j}(\btheta)) \right],
\end{align*}
where $\mb{f}_{j} = \tr{(1, f_{j,1}, \dots, f_{j,T})}$, and so the gradient for the expression in \eqref{eq:prop3b_eq1} is simply
\begin{align*}
\nabla_{\btheta} \sum_{j=1}^{J} \log h_{j}(\btheta) = (1-2\rho) \sum_{j=1}^{J} \mb{f}_{j} \frac{\pi_{j}(\btheta) (1-\pi_{j}(\btheta))}{h_{j}(\btheta)}.
\end{align*}
\hfill $\blacksquare$

\section{Computational methods}

\subsection{Product MOM non-local prior}

Figure \ref{fig:nlpmom} illustrates the density of the product MOM non-local prior of \cite{Johnson12}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.85]{nlpmom.pdf} 
\caption{Prior density $p(\alpha_{t} \mid \delta_{t} = 1, \phi = 1)$ of the MOM non-local prior, with $\tau = 0.348$.}
\label{fig:nlpmom}
\end{figure}

\subsection{Numerical computation of the marginal likelihood for non-local priors} \label{subsec:nlp_approx}

Briefly, denote by $p^\textsc{n}(\alpha_t \mid \delta_t=1, \phi) = \text{N}(\alpha_t; 0, \tau \phi)$ independent Gaussian priors for $t=1,..,T$, and similarly $p^\textsc{n}(\beta_j \mid \gamma_j=1, \phi) = \text{N}(\beta_j; 0, \tau \phi)$ for $j=1,\ldots, J$. Proposition 1 in \cite{Rossell17} shows that the following identity holds exactly
\begin{align*}
p(\by \mid \bgamma, \bdelta)= p^\textsc{n}(\by \mid \bgamma, \bdelta) \E^\textsc{n} \left[ \prod_{t=1}^{T} \frac{\alpha_t^2}{\tau\phi} \prod_{j=1}^{J} \frac{\beta_j^2}{\tau\phi} \mid \by, \bgamma, \bdelta \right]
\end{align*}
where $p^\textsc{n}(\by \mid \bgamma, \bdelta)$ is the integrated likelihood under $p^\textsc{n}(\balpha, \bbeta)$, and $\E^\textsc{n}[\cdot]$ denotes the posterior expectation under $p^\textsc{n}(\balpha, \bbeta \mid \by, \bgamma, \bdelta)$.
%For certain models this is analytically tractable. The most common tractable case is when $y_i \mid \eta_i,\phi \sim N(\eta_i,\phi)$  and Gaussian prior is used for $\balpha$ and $\bbeta$ conditionally on being non-zero \omcom{add some reference for where this formula is to be found}. For the examples in this article, we are interested in the case of Gaussian likelihood but non-Gaussian priors, in particular the pMOM prior discussed in Section \ref{sec:model}, for which the expression is intractable \omcom{Om2D: I think it is expensive not entirely intractable right? please edit to make this correct}
To estimate $p^\textsc{n}(\by \mid \bgamma, \bdelta)$ for non-Gaussian outcomes we use a Laplace approximation. Regarding the second term, we approximate it by a product of expectations, which \cite{Rossell20a} showed leads to the same asymptotic properties and typically enjoys better finite-$n$ properties than a Laplace approximation.

\subsection{Numerical optimization in search of $\hat{\theta}^{\text{EB}}$ and $\hat{\theta}^{\text{EP}}$} \label{sec:algorithm}

%\davidcom{Miquel, please move the algorithm below and rest of this section to the supplement. Also, indicate the grid size in Step 3 of the algorithm}
Algorithm \ref{alg:one} describes our method to estimate $\hthetaep$ and $\hthetaeb$. We employ the quasi-Newton BFGS algorithm to optimize the objective function. For $\hthetaeb$, we use the gradients from Corollary \ref{prop:two}, while the Hessian is evaluated numerically using line search, with the \texttt{R} function \texttt{nlminb}. Note, however, that obtaining $\hthetaeb$ requires sampling models from their posterior distribution for each $\btheta$, which is impractical, to then obtain posterior inclusion probabilities required by \eqref{eq:prop_two}. Instead, we restrict attention to the models $M$ sampled for either $\btheta = \mb{0}$ or $\btheta = \hthetaep$ in order to avoid successive MCMC runs at every step, relying on the relative regional proximity between the starting point $\hthetaep$ and $\hthetaeb$. This proximity would ensure that $M$ contains the large majority of models with non-negligible posterior probability under $\hthetaeb$. For $\hthetaep$, we use employ the same BFGS strategy using gradient computed in \ref{sec:gradEP}, with numerical evaluation of the Hessian. This computation requires only one MCMC run at $\btheta = \mb{0}$, which allows us to use grid search to avoid local optima. As for the size of the grid, we let the user specify what points are evaluated. For $K$ points in the grid one must evaluate the log objective function $K^{T+1}$ times, so we recommend to reduce the grid density as $T$ grows. By default, we evaluate every integer in the grid assuming $T$ is not large, but preferably we avoid coordinates greater than 10 in absolute value, as in our experiments it is very unlikely that any global posterior mode far from zero is isolated, i.e. not reachable by BFGS by starting to its closest point in the grid. Additionally, even if that were the case, numerically it makes no practical difference, considering that marginal inclusion probabilities are bounded away from zero and one regardless.

\begin{algorithm}[H] \label{alg:one}
%\KwIn{a,b,c,d}
\KwOut{$\hthetaep$ and $\hthetaeb$}
Obtain $B$ posterior samples $(\bgamma, \bdelta)^{(b)} \sim p(\bgamma, \bdelta \mid \by, \btheta = \mb{0})$ for $b = 1,\ldots,B$. Denote by $M^{(0)}$ the corresponding set of unique models.\\

Compute $s_{t} = P(\delta_{t} = 1 \mid \by, \btheta = \mb{0})$ and $q_{j} = P(\gamma_{j} = 1 \mid \by, \btheta = \mb{0})$.\\

%Compute estimates $\hat{q}_{j}$ using $M^{(0)}$ as a model dictionary.\\

Condut a grid search for $\hthetaep$ around $\btheta={\bf 0}$. Optimize \eqref{eq:ep_eq0} with the BFGS algorithm initialized at the grid's optimum.\\

Obtain $B$ posterior samples $(\bgamma, \bdelta)^{(b)} \sim p(\bgamma, \bdelta \mid \by, \btheta = \hthetaep)$. Denote by $M^{(1)}$ the corresponding set of unique models. Set $M = M^{(0)} \cup M^{(1)}$.\\

Initialize search for $\hthetaeb$ at $\hthetaep$. Use the BFGS algorithm to optimize \eqref{eq:marglik1}, restricting the sum to $(\bdelta,\bgamma) \in M$.
\caption{Obtaining $\bthetaep$ and $\bthetaeb$}
\end{algorithm}

\begin{comment}

% Algorithm example
%\begin{algorithm}
\begin{algorithm}[H] \label{alg:one}
\SetAlgoLined
\KwResult{$\hthetaep$ and/or $\hthetaeb$}
 \STATE Set $\btheta = \mb{0}$ and obtain $B$ posterior samples $(\gamma,\delta)^{(b)} \sim p(\bgamma, \bdelta \mid \by, \btheta = \mb{0})$ for $b = 1,\ldots,B$. Denote by $M^{(0)}$ the corresponding set of unique models. $M^{(0)}$\;
 Compute estimates $\hat{q}_{j} = \sum_{(\bdelta, \bgamma): \gamma_j = 1; (\bdelta, \bgamma) \in M^{(0)}} p(\bdelta, \bgamma \mid \by, \btheta = \mb{0})$\;
 Approximate the optimum of (log) objective function in \eqref{eq:ep_eq0} $\longrightarrow \hthetaep$\;
 Conduct new model search conditional on $\btheta = \hthetaep$, visiting models $M^{(1)}$\;
 Set $M = M^{(0)} \cup M^{(1)}$, precompute and store $p(\by \mid \bdelta, \bgamma)$ for every $(\bdelta, \bgamma) \in M$\;
 Apply BFGS method on (log) objective function initialised at $\btheta = \hthetaep$\;
 %Return the optimum found by BFGS $\longrightarrow \hat{\btheta} := \hthetaeb$\;
 Return the optimum found by BFGS $\longrightarrow \hthetaeb$\;
 \caption{Quick approximation to $\bthetaeb$}
\end{algorithm}

\end{comment}

\section{Supplementary Results}


\subsection{Illustration of the EB and EP objective functions}

\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{thEB.pdf} 
\includegraphics[scale=0.45]{thEP.pdf} 
\caption{Empirical Bayes (left) and Expectation-Propagation (right) objective functions \eqref{eq:marglik1} and \eqref{eq:ep_eq0} in the single treatment case ($T=1$). Here, $\hthetaeb = (-2.43, 3.19)$ and $\hthetaep = (-2.34, 3.09)$, for $n=100$ and $J=49$, for the first data realization for the simulation design displayed in the center-left panel of Figure \ref{fig:intro2} with three confounders. See Section \ref{subsec:singleT} for further details.}
\label{fig:thEPEB}
\end{figure}


Figure \ref{fig:thEPEB} shows the Empirical Bayes objective function in \eqref{eq:marglik1} and \eqref{eq:ep_eq0} in a simulated dataset with a single treatment. A bimodality is appreciated in the left panel.

\subsection{Salary survey: generation of augmented datasets} \label{sec:fakepreds_supp}

For both amounts $K_1=100$ and $K_2=200$ of artificial predictors, the simulation protocol was the same. Every artificial control $\mb{z}_{k} \in \mathbb{R}^{n}$, for $k=1,\dots,100$ or $k=1,\dots,200$ respectively, was simulated to correlate to one individual treatment, according to which subset said control was assigned to, correlating only indirectly to the rest of treatments. In particular, we drew elements of $\mb{z}_{k}$ from $z_{i,k} \mid d_{i,t} = 1 \sim \text{N}(1.5, 1)$, and $z_{i,k} \mid d_{i,t} = 0 \sim \text{N}(-1.5, 1)$, where $\bd_{t}$ denotes the corresponding column in the treatment matrix associated to the given $\mb{z}_{k}$. 

\subsection{Further results on salary survey} \label{sec:salary_supp}

\begin{figure}[h]
\centering
\includegraphics[scale=0.75]{figApp1Btop.pdf}\\
\includegraphics[scale=0.75]{figApp1Bbot.pdf}
\caption{Inference for treatment variables ``hispanic'' (top) and ``born in Latin America'' (bottom) in 2010 and 2019; see Section \ref{sec:cps}. Read caption to Figure \ref{fig:intro1} to read this figure.}
\label{fig:figApp1B}
\end{figure}

Figure \ref{fig:figApp1B} follows Figure \ref{fig:intro1} by showing the results for the other two treatments: Hispanic ethnicity, and birthplace in Latin America.


\subsection{Model selection results in simulation study}

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
$\alpha = 1$ & $\alpha = 1/3$ & $\alpha = 0$ \\
\includegraphics[scale=0.57]{fig1Bp1.pdf} &
\includegraphics[scale=0.57]{fig1Bp2.pdf} &
\includegraphics[scale=0.57]{fig1Bp3.pdf} \\
\includegraphics[scale=0.57]{fig1Bp4.pdf} &
\includegraphics[scale=0.57]{fig1Bp5.pdf} &
\includegraphics[scale=0.57]{fig1Bp6.pdf}
\end{tabular}
\caption{To be read vertically in relation to Fig. \ref{fig:intro2}. The top panels show the average outcome model size across levels of confounding, divided by the true model size (i.e. 1 indicates that it matches the true model size). The bottom panels show the probability of selecting the treatment using a 0.05 P-value cut-off for DML, and for Bayesian methods the treatment is included when marginal posterior inclusion probability is $>$1/2. LASSO does not appear in these panels as its not designed for inference.}
\label{fig:fig1b}
\end{figure}

Figure \ref{fig:fig1b} summarizes model selection results for the simulations described in Figure \ref{fig:intro2}.


\subsection{Simulations under growing dimensionality ($T=1$)}

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
$n = 50$; $J+T = 25$ & $n = 100$; $J+T = 100$ & $n = 100$; $J+T = 200$ \\
\includegraphics[scale=0.66]{fig2Ap1.pdf} &
\includegraphics[scale=0.66]{fig2Ap2.pdf} &
\includegraphics[scale=0.66]{fig2Ap3.pdf} 
\end{tabular}
\caption{Single treatment parameter RMSE (relative to Oracle OLS) based on $R=250$ simulated datasets for each level of confounding.
%Here, leftmost panel shows results for $\{ N=50, J+T=25 \}$, center-left for $\{ N=100, J+T=50 \}$, center-right for $\{ N=100, J+T=100 \}$, and rightmost for $\{ N=100, J+T=200 \}$. 
In all panels, $\alpha = 1$ and $\norm{\bgamma}_{0} = 6$. We show the empirical Bayes version CIL only in the right panel, for the other panels results are undistinguishable relative to EP.}
\label{fig:singletreat_growingdim}
\end{figure}


Figure \ref{fig:singletreat_growingdim} studies the effect of growing number of covariates on inference, specifically for $J+T=25$, 100 and 200.


\subsection{Testing CIL to different amounts of confounders for $T=1$}

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
$\norm{\bgamma}_{0} = 6$ & $\norm{\bgamma}_{0} = 12$ & $\norm{\bgamma}_{0} = 18$ \\
\includegraphics[scale=0.66]{fig3Ap1.pdf} &
\includegraphics[scale=0.66]{fig3Ap2.pdf} &
\includegraphics[scale=0.66]{fig3Ap3.pdf}
\end{tabular}
\caption{Single treatment parameter RMSE (relative to Oracle OLS) based on $R=250$ simulated datasets for each level of confounding reported, as described in Figure \ref{fig:intro2}. In all panels, $n=100$, $J+T=100$ and $\alpha = 1$. Sudden general improvement at the right end of center and right panels is due to a sharper deterioration of oracle OLS RMSE at complete confounding relative to other methods.}
\label{fig:fig3}
\end{figure}

Figure \ref{fig:fig3} shows the effect of having various amounts of active confounders. The results look consistent to the effects reported in Figures \ref{fig:intro2} and \ref{fig:singletreat_growingdim}, which were magnified for large amounts of active confounders. These are really challenging situations to tackle since the tested methods aim at model sparsity, while the true model size is relatively large. Although our method still performed at oracle rates in low-confounding scenarios, its relative performance was compromised for the highest levels of confounding. This occurred in part because accurate point estimation in \eqref{eq:lasso} became increasingly harder as the correlation between covariates strengthened, which in turn influenced the ability of the algorithm to calibrate $\btheta$ reliably. Even in these hard cases, however, its performance was not excessively far to the best competing method, while it clearly outperformed BMA on all of them.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{References}

%We hope you've chosen to use BibTeX!\ If you have, please feel free to use the package natbib with any bibliography style you're comfortable with. The .bst file agsm has been included here for your convenience. 

%\bibliographystyle{Chicago}
\bibliographystyle{agsm}

%\bibliography{Bibliography-MM-MC}
\bibliography{references}
\end{document}
