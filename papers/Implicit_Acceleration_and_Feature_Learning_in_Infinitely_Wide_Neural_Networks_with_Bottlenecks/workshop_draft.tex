\documentclass{article}
\usepackage[title]{appendix}
% \usepackage[nonatbib]{neurips_2020}
% \usepackage[numbers]{natbib}
% \usepackage{arxiv}
% \usepackage[numbers]{natbib}
\usepackage[accepted]{icml2021_oppo}
% \usepackage{icml2021_oppo}
% \usepackage[nohyperref]{icml2021_oppo}
% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
\usepackage{mathtools}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, 
\usepackage{thm-restate}
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{subcaption,graphicx}
\usepackage{caption}
\usepackage{cleveref}
% \usepackage{subcaption}
\captionsetup[table]{skip=10pt plus 0.01pt}
\usepackage{wrapfig}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{amssymb,amsmath,amsthm}

\usepackage{thmtools}
\usepackage{thm-restate}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{fact}[thm]{Fact}
\newtheorem{qst}[thm]{Question}
\newtheorem{axm}[thm]{Axiom}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{claim}{Claim}[section]
\newtheorem{assm}[thm]{Assumption}
\newtheorem{heur}[thm]{Heuristic}
\newtheorem{setup}[thm]{Setup}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exmp}[thm]{Example}
\renewcommand{\[}{\begin{eqnarray}}
\renewcommand{\]}{\end{eqnarray}}
\newcommand{\greg}[1]{{\color{red}Greg: #1}}
\newcommand{\etai}[1]{{\color{blue}Etai: #1}}
% \usepackage{algorithm,algorithmic}
% \usepackage{algorithm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% \usepackage[options ]{algorithm2e}

\usepackage{etoolbox}
% \usepackage[nonatbib]{natbib}

\makeatletter
% \patchcmd\algocf@Vline{\vrule}{\vrule \kern-0.4pt}{}{}
% \patchcmd\algocf@Vsline{\vrule}{\vrule \kern-0.4pt}{}{}
\makeatother

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}
% \usepackage[fleqn]{amsmath}
% \usepackage{algpseudocode}
\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc,arrows,positioning, fit}
% \usepackage{mathpazo}
% \usepackage{amsmath}

% \usepackage[parfill]{parskip}


\usepackage{float}
\raggedbottom
\setlength{\textfloatsep}{0.31cm}


\renewcommand{\[}{\begin{eqnarray}}
\renewcommand{\]}{\end{eqnarray}}
\newcommand{\at}[2][]{#1|_{#2}}
% \DeclareMathOperator*{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb\bold{n}}
\newcommand{\sgn}{\textnormal{sgn}}
%\newcommand{\E}{\mathbb{E}}
\usepackage{mathbbol}
\newtheorem{theorem}{Theorem}
\usepackage{thm-restate}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\HE}{\mathcal{H}}
\newcommand{\G}{\mathcal{H}_g}
\newcommand{\K}{\mathcal{K}}
\newcommand{\KI}{\K_{\text{init}}}
% \title{Implicit Acceleration and Feature Learning in\\Infinitely Wide Neural Networks with Bottlenecks  }
\icmltitlerunning{Implicit Acceleration and Feature Learning in Infinitely Wide Neural Networks with Bottlenecks}
% \author{Authors}
% \date{May 2021}
% \author{
%   Etai Littwin \; Omid Saremi \; Shuangfei Zhai \; Vimal Thilak \; Hanlin Goh \; Josh Susskind, \\
%   \texttt{Apple Inc.} \\
%   Greg Yang, \texttt{MSR}
% }
% \author{
%   \texttt{Apple Inc.} \\
% }
% \author{
%   Etai Littwin \; Ben Myara \; Sima Sabah \; Joshua Susskind \; Shuangfei Zhai\;  \\
%   \texttt{Apple Inc.} \\
%   \texttt{\{elittwin, bmyara, sima, jsusskind, szhai\}@apple.com}
% }
\begin{document}

% \maketitle
\twocolumn[
\icmltitle{Implicit Acceleration and Feature Learning in\\Infinitely Wide Neural Networks with Bottlenecks}

\begin{icmlauthorlist}
\icmlauthor{Etai Littwin}{apple}
\icmlauthor{Omid Saremi}{apple}
\icmlauthor{Shuangfei Zhai}{apple}
\icmlauthor{Vimal Thilak}{apple}
\icmlauthor{Hanlin Goh}{apple}
\icmlauthor{Joshua M. Susskind}{apple}
\icmlauthor{Greg Yang}{MSR}
\end{icmlauthorlist}

\icmlaffiliation{apple}{Apple}
\icmlaffiliation{MSR}{MSR}

\icmlcorrespondingauthor{Etai Littwin. }{elittwin@apple.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{neural tangent kernel, NTK, kernel regime, feature learning, infinite neural networks, bottleneck networks}
\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
% In the lazy regime, training dynamics of increasingly wide neural networks under gradient descent converge to kernel gradient descent in function space. In the infinite width limit, the learned model post training becomes independent of the specific instantiation of the parameters, and depends only on the initial function computed by the initialized network (i.e the outputs of the network at initialization). In this limit, training can be analyzed in functional space using the Neural Tangent Kernel. Unfortunately, this limit does not allow actual data dependent feature learning. As we show, when parameterized accordingly (aka using the NTK parametrization), training of 
We analyze the learning dynamics of infinitely wide neural networks with a finite sized bottleneck. Unlike the neural tangent kernel limit, a bottleneck in an otherwise infinite width network allows data dependent feature learning in its bottleneck representation. We empirically show that a single bottleneck in infinite networks dramatically accelerates training when compared to purely infinite networks, with an improved overall performance. We discuss the acceleration phenomena by drawing similarities to infinitely wide deep linear models, where the acceleration effect of a bottleneck can be understood theoretically. 
\end{abstract}
% \setcounter{footnote}{1}
\section{Introduction}
The study of infinitely wide neural networks is one of the most actively researched topics in deep learning theory \cite{NTK,gp1,gp2,gp3,gp4,gp5,gp6,gp7,yang,yang2,yang3,TP2b,Littwin2020OnRK,Littwin2020OnTO}. Previous work identified distinct training regimes that are determined by the networks hyperparameters. In \cite{Yang2020FeatureLI}, it is shown that by correctly scaling hyperparameters such as learning rate, weight multipliers and initialization constants, neural network training dynamics under gradient descent exhibit a limiting behaviour as the width of the network tends to infinity. Two distinct training regimes are identified in the limit. 1) The kernel regime, where the network evolves like a linear model during training. In this regime, intermediate activations in infinite layers change by a vanishing amount, stripping the network of its ability to learn features. 
2) The feature learning regime, where the intermediate activations change in a nontrivial way during training, resulting in data dependent feature learning. \\
In \cite{Yang2020FeatureLI}, a clear dichotomy exists between these regimes in the infinite width limit, where a network is either in one regime or another, but not in both at the same time. A network undergoing training by gradient descent algorithms can be placed in one regime or the other depending on its parametrization at initialization. The underlying assumption in both cases however is that all hidden layers are infinitely wide. 
Some practical architectures incorporate narrow layers by design, or use varying width layers of which some are wide, while others are narrow. For example, bottleneck layers force a network to learn a low dimensional latent representation, and are typically used as part of a generally wide network. Perhaps the most prominent architecture which uses such layers is the autoencoder, where the encoder learns a typically low dimensional representation of its input, while the decoder reconstructs the input from its latent representation.
% A less intuitive example of a bottleneck resides in self attention layers, where the dimensionality of the attention map is preset and determined by the number of tokens in the input, hence can also be viewed as a bottleneck when the number of tokens is considerably smaller than the feature dimension. 
In these types of models, the discrepancy between wide and narrow layers is a carefully hand designed feature of the architecture. Therefore, the standard infinite width approximation cannot be applied without "assuming away" a prominent architectural feature.\\
In this work, we consider a different type of limit, where a bottleneck layer of finite width is inserted in an otherwise infinitely wide network. From a Bayesian perspective, such models have been investigated in \cite{Agrawal2020WideNN,Aitchison2020WhyBI}, however their learning dynamics under gradient descent have not yet been explored to the best of our knowledge. As we show, gradient descent on such models can be understood and simulated exactly in function space rather than parameter space, at a relatively cheap computational cost. This is in contrast to the feature learning limit in \cite{Yang2020FeatureLI}, where it is practically infeasible to simulate exact learning dynamics in the limit without approximations. Empirically, we show that adding a bottleneck layer can significantly boost speed of training convergence. We further speculate on possible reasons for this dramatic boost in convergence speed by drawing equivalence to infinitely wide deep linear models with bottlenecks, where acceleration of the bottlenecks is fully tractable. \\
% \begin{enumerate}
%     \item Theoretical analysis of wide nets with bottlenecks
%     \item Exact training in functional space for toy models
%     \item Exact training in functional space for CNNs and Transformers
%     \item Models trained in parameter space showing additional empirical support
% \end{enumerate}

% \section{Previous Work}
% \begin{itemize}
%     \item NTK theory work including limitations
%     \item Extensions of NTK to self-attention
%     \item Finite width corrections to NTK and Neural Tangent Hierarchy
% \end{itemize}

\section{Neural Networks With Bottlenecks}\label{sec:bottle}

A natural way of thinking about wide neural networks with bottlenecks is through composition of networks. Let $f_n(x;w):\R^{d} \to \R^{d_r}$ denote the (vector) output of a neural network given input vector $x \in \R^d$ and parameters $w$, where $n$ denotes the width of the hidden layers\footnote{We use the term "width" losely here. For MLPs, this means the size of the hidden layers, or the feature dimension in convolutional neural nets.}. 
% The study of infinite width neural networks revolves around the analysis of trained and untrained neural networks in the regime where the width $n$ of the hidden layers of $f_n$ tend to infinity $n \to \infty$. In \cite{NTK}, it is shown that for a wide range of architectures that are properly parametrized (i.e using the NTK parametrization), the outputs of the hidden layers change by an order of $\mathcal{O}(\frac{1}{\sqrt{n}})$ when trained by gradient descent. Roughly speaking, in the limit of $n \to \infty$ the coordinates of the hidden layers do not change, and the entire training dynamics can be fully described in functional space using the renowned NTK equation. 
In the present work, we consider the case where the input $x$ is itself given by the output of a neural network $x(\xi) = g_n(\xi;\theta): \R^{d_0} \to \R^d$, given input vector $\xi \in \R^{d_0}$ and parameters $\theta$, with hidden layers width parametrized by $m$. For simplicity, we assume the width of all hidden layers in both $f$ and $g$ is $n$. The output of the composed architecture is then given by $\F_n(\xi) = (f_n \circ g_n)(\xi)$. Note that even if $n$ is large, the output of $g_n(\xi)$ is still $d$ dimensional, hence $\F_n$ can be viewed as a wide network with a bottleneck of dimension $d$. \\
\paragraph{Setup}
\label{par:setup}
To make things concrete, we consider the case where $f_n$ implement an MLP of depths $2$ and width $n$. Due to technical considerations,\footnote{Rigorously generalizing the claims in this paper to deeper MLPs is not straightforward, and involves a considerable technical challenge. This difficulty arises due to the structure of the input-output Jacobian, which cannot be implemented as a Tensor Program \cite{yang,yang2,yang3} in its current form.} we restrict our discussion in the current paper to a 1 hidden layer MLP for $f$, and leave the rigorous analysis of deeper networks to future work. Given an input vector $g \in \R^d$ and parameters $w = \{u,v\}$, the output $f_n(g)$ is given by:
\begin{align}\label{eqn:mlp}
    f_n(x) = \frac{v}{\sqrt{n}}\phi(\frac{ux}{\sqrt{d}})
\end{align}
where $u \in \R^{n \times d},v \in \R^{d_r \times n}$ are the weight matrices sampled from a normal distribution, and
% For $f$, the parameter matrices are given by $\{w^l\}_{l=1}^{L+f+1}: w^1 \in \R^{n \times d},w^{L_f+1} \in \R^{d_r \times n}, \forall_{1<l\leq L_f}w^l \in \R^{n \times n}$. The output of $f$ is given by:
% \begin{align}\label{eqn:mlp}
% f(g) &= \frac{1}{\sqrt{n}}w^{L_f+1\top} z^{L_f}, z^l = \begin{cases}
% \phi(x^l) &  L\geq l>0\\
% g & l=0
% \end{cases}\\
% x^l &= \begin{cases} \frac{1}{\sqrt{n}}w^{l\top} z^{l-1} & l>1\\
% \frac{1}{\sqrt{d_0}}w^{l\top} z^{l-1} & l=1
% \end{cases}
% \end{align}
$\phi$ is a coordinate nonlinearity which we assume, for the sake of analysis, is twice differentiable with bounded derivatives. We let $g$ implement an arbitrary neural network function with parameter matrices $\theta = \{\theta^l\}_{l=1}^{L}$, with suitable dimensions. 
We are interested in theoretically understanding the behaviour of the composition $\F_n = f_n \circ g_n$ during training in the overparametrized scenario where the width $n$ of the hidden layers tend to infinity, while the bottleneck dimension $d$ remains fixed in size. 
% For the remainder of the paper, a statement involving an infinite width limit should be interpreted by the reader as the limit of $n\to \infty$ and a constant $d$, unless explicitly stated otherwise. \\
Our setup involves the training of the composition function $\F_n$ on a training dataset $\{\xi_i\}_{i=1}^N$, using a loss function $\mathcal{L}$ implicitly containing the labels. We state our results assuming infinitesimal learning rate (aka gradient flow), however we expect our results to carry over to SGD. Since $\F_n$ contains a bottleneck of finite size, we will have to reason about the evolution of forward and backward signals as they propagate through a finite sized layer. 
% In this section we consider a toy network with a single bottleneck layer. 
% An intuitive way of describing the computation graph implemented by the network is using two separate computation "modules", separated by the bottleneck. Given an input vector $\xi \in \R^{d_0}$, the first module $g(\xi):\R^{d_0} \to \R^d$ maps the input to some fixed dimensional representation (e.g the bottleneck), while the second module $f(g):\R^d \to \R$ takes the output $g(\xi)$ and maps it to the output space. In this section, both $f,g$ are implemented by a single hidden layer MLP initialized using the NTK parametrization, with a twice differentiable loss function $\phi: \R \to \R$:
% \[
% g(\xi) &=& \frac{v^\top x(\xi)}{\sqrt{n}},~~~x(\xi) = \phi \big(y(\xi)\big),~~~y(\xi) = u\xi\\
% f(g) &=& \frac{b^\top h(g)}{\sqrt{n}},~~~h(g) = \phi \big(z(g)\big),~~~z(g) = \frac{ag}{\sqrt{d}}\\
% \F(\xi) &=& (f\circ g)(\xi)
% \]
% Here, $u,v$ are the trainable parameters of module $g$ with dimensions $u \in \R^{n \times d_0}, v \in \R^{n \in d}$, and $a,b$ are the trainable parameters of module $f$ with dimensions $a \in \R^{n \times d},b \in \R^{n \times 1}$. Consequently, the dimensions of the intermediate layers are $y,x,z,h \in \R^n$, and $g \in \R^d$. We further assume all trainable parameters $u,v,a,b$ are initialized iid from a normal distribution as is common in practice. We are interested in theoretically understanding the behaviour of the composition $\F = f \circ g$ in the overparametrized scenario where the width $n$ of the hidden layers tend to infinity, while the bottleneck dimension $d$ remains fixed in size. For the remainder of the paper, a statement involving an infinite width limit should be interpreted by the reader as the limit of $n\to \infty$, unless explicitly stated otherwise. 
\paragraph{Notations}
We use $x,\tilde{x}$ to denote generic placeholder vectors to $f$, and $g(\xi)$ as a specific instantiation of $x$ by the output $g(\xi,\theta)$. We denote the input-output Jacobian $J(x) = \frac{\partial f(x)}{\partial x} \in \R^{d_r \times d}$, with the notation $\tilde{J} = J(\tilde{x})$. We use $\F,g,J$ for $\F(\xi), g(\xi), J = J\big(g(\xi)\big)$ where $\xi$ is some generic input. 
To remove clutter, we use subscripts $n,t,i$ to denote the value of a vector/matrix parametrized by $n$ at time $t$ given sample $\xi_i$.  (i.e $g_{n,t,i} = g_{n,t}(\xi_i),g_{n,t} = g_{n,t}(\xi)$ and same for $J$). We use superscripts to denote coordinates of a vector/matrix (i.e $g^\alpha$ is the $\alpha$ coordinate of vector $g$). The absence of a subscript $n$ implies we have taken $n \to \infty$. Finally, we use $\chi_i$ to denote the loss derivative  for sample $i$ (i.e $\chi_i = \frac{\partial \mathcal{L}}{\partial \F_i} \in \R^{d_r}$). 
% We write $f,g$ to denote the functions $f(g):\R^{d}\to \R,g(\xi):\R^{d_0}\to \R^d$ acting separately on their corresponding inputs. We denote a training set and test set by concatenated matrices $D \in \R^{N \times d_0}$ and $D' \in \R^{N' \times d_0}$ where $N,N'$ are the number of samples in the train (resp test) sets.


% That is, for any discussion relating to the function $f$ we assume its input is fixed and deterministic. We consider training the composition function $\F$ using gradient flow on a training dataset $\{\xi_i\}_{i=1}^N$ using a loss function $\mathcal{L}()$

% We use subscript to denote a dependency on sample (i.e $g_i = g(\xi_i)$). With a slight abuse of notation, we use subscript $t$ to denote a value of a tensor at time $t$ of training. We use a superscript to denote indexing


\subsection{Preliminaries}\label{sec:pre}
As width increases, pre-activations of initialized neural networks without bottlenecks approach a centered gaussian process (GP), independent across coordinates but possibly correlated across inputs. At the output level, the following hold at initialization:
\begin{align}\label{eqn:nngp}
\lim_{n \to \infty} g_n(\xi) \overset{d}{=} g(\xi),&~~~ \lim_{n \to \infty} f_n(x) \overset{d}{=} f(x)
\end{align}
where $g(\xi),f(x)$ are GPs with respect to their respective inputs. For $f$ as implemented in \cref{eqn:mlp}, we can write the defining covariance $\Lambda$ of the GP given input samples $x,\tilde{x}$:
\begin{align}
\forall_\alpha,\begin{pmatrix}
f^\alpha(x)\\
f^\alpha(\tilde{x}) 
\end{pmatrix} &\sim \mathcal{N}\big(0, \Lambda(x,\tilde{x})\big)\\
\Lambda(x,\tilde{x}) &= \begin{pmatrix} \Sigma(x,x) & \Sigma(x,\tilde{x})\\ \Sigma(\tilde{x},x) & \Sigma(\tilde{x},\tilde{x}) \end{pmatrix} \in \R^{2 \times 2}
% \Sigma(x,\tilde{x}) &= \E_{a,b \sim \mathcal{N}(\Lambda_0(x,\tilde{x}))}\big[\phi(a)\phi(b)\big]\\
% \Lambda_0(x,\tilde{x}) &= \begin{pmatrix} \frac{\|x\|^2}{d} & \frac{x^\top \tilde{x}}{d}\\ \frac{\tilde{x}^\top x}{d} & \frac{\|\tilde{x}\|^2}{d} \end{pmatrix} \in \R^{2 \times 2}
\end{align}

We assume both $f,g$ have empirical NTKs $\mathcal{K}_n ,\Theta_n$ defined as $\mathcal{K}_n(x,\tilde{x}) = \frac{\partial f(x)}{\partial w}\frac{\partial  f(\tilde{x})^\top}{\partial w} \in \R^{d_r \times d_r},\Theta_n(\xi,\tilde{\xi}) = \frac{\partial g(\xi)}{\partial \theta}\frac{\partial g(\tilde{\xi})^\top}{\partial \theta} \in \R^{d \times d}$
with corresponding limits $\mathcal{K}(x,\tilde{x})I_{d_r}, \Theta(\xi,\tilde{\xi})I_{d}$, where $I_{d_r},I_d$ are identity matrices of size $d_r,d$, and $\mathcal{K}(x,\tilde{x}),\Theta(\xi,\tilde{\xi}) \in \R$.
% Intuitively, the composition $\F(\xi) = (f \circ g)(\xi)$ should not be expected to exhibit a GP behaviour. Since $g$ is finite dimensional, the following layer $ \frac{ug}{\sqrt{d}}$ is given by a linear combination of the same finite stochastic vector $g$, hence the coordinates of $ug$ become statistically dependent even in the limit of $n \to \infty$. 
Consistent with intuition, it was rigorously shown in \cite{Agrawal2020WideNN} that as the width increases the output of an MLP with bottlenecks converges to a composition of GPs. \\
\paragraph{Training} We consider training the model described in \cref{sec:bottle}. Under gradient flow, the weights evolve according to $\dot{w}_t = -\nabla_{w_t}\mathcal{L}_t,~~~\dot{\theta}_t = -\nabla_{\theta_t}\mathcal{L}_t$. The evolution of the output of the composition function can be described by the following set of ODEs:  
\begin{align}\label{eqn:gf}
\dot{\mathbf{\F}}_{n,t}(\xi) &= \frac{\partial \F_{n,t}(\xi)}{\partial w_t}\dot{w}_t + \frac{\partial \F_{n,t}(\xi)}{\partial \theta_t}\dot{\theta}_t
\end{align}
Substituting the empirical kernel definitions:
\begin{align}\label{eqn:random_kernel}
\dot{\F}_{n,t}(\xi) &= -\sum_{i=1}^N\mathcal{K}_{n,t}\big(g_{n,t},g_{n,t,i}\big)\chi_{n,t,i} \\\nonumber
&-\sum_{i=1}^NJ_{n,t} \Theta_{n,t}(\xi,\xi_i)J^{\top}_{n,t,i} \chi_{n,t,i}
\end{align}
where $g_{n,t} = g_{n,t}(\xi), J_{n,t} = J_{n,t}(\xi)$. The above evolutionary equations can be interpreted as Kernel equations with random, evolving kernels that depend on the weights $w,\theta$. In \cite{NTK}, it was shown for infinite width networks (without bottlenecks) the output is fully deterministic at any time $t$ conditioned on the initial GP output at time $t=0$. In contrast, since both bottleneck embedding $g$ and input-output Jacobian $J$ are finite, we expect \cref{eqn:random_kernel} to remain random even when taking the limit $n \to \infty$. To get a more complete view of the evolution of the composition function $\F$ during training at the limit, we must reason about the dynamics of the Jacobian term $J$. 


\section{Dynamics in Function Space} 
A key observation in our analysis is that the input-output Jacobian $J(x)$ converges to a multivariate GP, and evolves as a linear function in the infinite width limit, similarly to outputs of an infinitely wide network. Here, $J(x)$ will have non trivial correlations across its coordinates, unlike layers in the NTK limit where the coordinates are independent. Our first result relating to the initial state of $J$ is stated in the following proposition:

\begin{restatable}[GP behaviour of the Jacobian]{lemma}{GP}\label{lemma:GP}
For $f_n(x)$ and its limit $f(x)$ as described in \cref{eqn:mlp,eqn:nngp}, the following holds at initialization for every pair of fixed inputs $x,\tilde{x}$:
\[
\lim_{n \to \infty}J_n(x) = J(x) \label{eqn:jac}
\]
where $J(x)$ is a multivariate GP with independent rows, and $J(x),f(x)$ are jointly Gaussian 
with:
\begin{align}
\begin{pmatrix}\label{JJ:cov}
J^{\alpha,\beta}(x)\\
J^{\alpha,\gamma}(\tilde{x})
\end{pmatrix} &\sim \mathcal{N}(\bm{0}, \begin{pmatrix} \Sigma_{(2)}^{\beta,\beta}(x,x) & \Sigma_{(2)}^{\beta,\gamma}(x,\tilde{x})\\ \Sigma_{(2)}^{\gamma,\beta}(\tilde{x},x) & \Sigma_{(2)}^{\gamma,\gamma}(\tilde{x},\tilde{x}) \end{pmatrix}\\
\begin{pmatrix}\label{fJ:cov}
f^\alpha(x)\\
J^{\alpha,\beta}(\tilde{x})
\end{pmatrix} &\sim \mathcal{N}(\bm{0}, \begin{pmatrix} \Sigma(x,\tilde{x}) & \Sigma_{(1)}^{\beta}(x,\tilde{x})\\ \Sigma_{(1)}^\beta(x,\tilde{x})^\top & \Sigma_{(2)}^{\beta,\beta}(\tilde{x},\tilde{x}) \end{pmatrix}
\end{align}
where:
\begin{align}\label{sig1:sig2:def}
\Sigma_{(1)}(x,\tilde{x}) &= \frac{\partial}{\partial b}\Sigma(x,b)\Big|_{b=\tilde{x}} \in \R^{1\times d}\\
\Sigma_{(2)}(a,b) &= \frac{\partial^2}{\partial a\partial b}\Sigma(a,b)\Big|_{a=x,b=\tilde{x}} \in \R^{d \times d}
\end{align}
\end{restatable}

\cref{lemma:GP} already illustrates a novel aspect of infinite width networks. As the input-output Jacobian is frequently used to derive sensitivity to perturbations, \cref{lemma:GP} shows that sensitivity to perturbations and outputs can be jointly modeled as a multivariate GP for sufficiently wide models.  \\
In the next theorem, we show that the $J(x)$ evolves linearly in wide models, in a similar fashion to the network outputs. Now we are ready to characterize the full training dynamics of $\F$ in the infinite width limit in the following theorem:
\begin{restatable}[Evolution of composed function]{thm}{main}\label{thm:main}
For networks $f,g$ as in \cref{sec:bottle}, the following ODEs describe the dynamics of $\F,g,J$ in the limit of $n \to \infty$:
\begin{align}
\dot{g}_t(\xi) &= -\sum_{i=1}^N\Theta(\xi,\xi_i)J_{t,i}^\top\chi_{t,i} \label{eqn:eq1}\\
\dot{J}_t(x) &= - \sum_{i=1}^N\chi_{t,i}\Xi(x,g_{t,i})^\top \label{eqn:eq2}\\
\dot{\F}_t(\xi) &= -\sum_{i=1}^N\Big[\mathcal{K}\big(g_t,g_{t,i}\big)I_{d_r} + 
\Theta(\xi,\xi_i)J_tJ_{t,i}^{\top}\Big]\chi_{t,i} \label{eqn:eq3}
\end{align}
where $\Xi(-,-)$ is a deterministic function defined as $\Xi(x,\tilde{x}) \in \R^{ d } = \lim_{n \to \infty} \frac{\partial^\top}{\partial x}\mathcal{K}_n^{\alpha,\alpha}(x,\tilde{x})$.
\end{restatable}
For $\phi = \text{ReLU}$, we give an explicit form for $\Xi$ in \cref{sec:exp}. 
ODEs in \cref{eqn:eq1,eqn:eq2,eqn:eq3} depend on deterministic, frozen kernels $\mathcal{K},\Theta,\Xi$. The evolution of $\F,g,J$ is hence completely deterministic after conditioning on initial states, and can therefore be expressed in functional space.
% The resulting ODEs however are non analytic in the general case, hence we must resort to numerical simulations.

% \paragraph{Stochastic Gradient Descent}
% To perform empirical experiments, we resort to the discrete time version of \cref{eqn:eq1,eqn:eq2,eqn:eq3} which will allow simulating SGD on the loss in function space. Let $\xi^t$ denote the sample fed into the SGD algorithm at step $t$. The discrete evolution of $\F,g,J$ is given by:
% \begin{align}
% \Delta g_{t+1}(\xi) &= -\sum_{i=1}^t\Theta(\xi,\xi_i)J_{t,i}^\top\chi_{t,i} \label{eqn:d1}\\
% \Delta J_{t+1}(x) &= - \sum_{i=1}^t\chi_{t,i}\Xi(x,g_{t,i})^\top \label{eqn:d2}\\
% \Delta \F_{t+1}(\xi) &= -\sum_{i=1}^t\Big[\mathcal{K}\big(g_t,g_{t,i}\big)I_{d_r} + 
% \Theta(\xi,\xi_i)J_tJ_{t,i}^{\top}\Big]\chi_{t,i} \label{eqn:d3}
% \end{align}
% where $\Delta \bullet_{t+1} = \bullet_{t+1} - \bullet_t$. 

\section{Implicit Acceleration by Bottlenecks in Linear Networks}\label{sec:implicit}
To intuitively understand the training aspects of introducing bottlenecks in infinite width networks, we draw inspiration from deep linear networks. Assume $f_n,g_n$ implement deep linear MLPs of depth $L_f,L_g$ respectively and width $n$, and let $w_{\text{eff}} \in \R^{d_r \times d} = \frac{1}{\sqrt{n^{(L_f-1)} d}}w^{L_f}w^{L_{f-1}}...w^1,~\theta_\text{eff} \in \R^{d \times d_0} = \frac{1}{\sqrt{n^{(L_g-1)} d_0}}\theta^{L_g}\theta^{L_{g-1}}...\theta^1$. Hence, we have that $g(\xi) = \theta_\text{eff} \xi$, $f(g) = w_{\text{eff}} g$ and $\F(\xi) = w_{\text{eff}}\theta_\text{eff} \xi$. In finite networks, recent results have shown that in some cases, the stacking of linear layers produces an acceleration effect, and a low rank bias when optimized by gradient descent \cite{Arora2018OnTO}. Moreover, the acceleration effect is akin to momentum, and cannot be reproduced by adding some regularizer to the objective.  However, the acceleration effect as outlined in \cite{Arora2018OnTO} disappears when considering the NTK regime. This is because in this regime, training speed is determined by the NTK itself, which does not change meaningfully with stacking of additional linear layers. Indeed, for a linear $g$ we have that $\Theta(\xi,\tilde{\xi}) = L_g\frac{\xi^\top\tilde{\xi}}{d_0}$. However, by introducing a bottleneck, we regain the lost acceleration effect, as illustrated in the following lemma.
\begin{restatable}{lemma}{linear}\label{lemma:linear}
In the limit of $n \to \infty$, optimizing $\F$ by running gradient flow on the weights $\{w^l\}_{l=1}^{L_f}$ with a learning rate $\epsilon_f$, and  $\{\theta^l\}_{l=1}^{L_g}$ with a learning rate $\epsilon_g$, is equivalent to running gradient flow directly on the effective weights $w_{\text{eff}}, \theta_\text{eff}$ with learning rates $L_f\epsilon_f,L_g\epsilon_g$.
\end{restatable}
 \cref{lemma:linear} suggests an infinitely wide deep linear network with a bottleneck is essentially reduced to a two layer, finite linear network with weight matrices $w_{\text{eff}}, \theta_\text{eff}$ under gradient descent. Therefore, under mild initialization conditions,\footnote{The acceleration effect formally requires a small initialization and learning rates to hold. Empirically, these conditions may sometimes be relaxed.} a bottleneck brings about accelerated learning in linear networks. It is worth noting, additional capacity cannot be attained by stacking additional layers in linear networks. Moreover, shallow and deep linear functions represent the same function class. Hence training speed can be directly attributed to trajectories of gradient descent. This does not hold for nonlinear finite networks, where changes in depth or width also affect capacity. However, in the infinite width regime, with bottlenecks or not, capacity is infinite. Hence, we can isolate the effect of bottlenecks on training and acceleration without confounding capacity.
 
%  However, we note that in the kernel regime a network has "infinite" capacity to fit the training data \footnote{Provided the NTK is positive definite}, which can only be reduced by the addition of a bottleneck. We therefore hypothesise that the implicit acceleration observed in both linear and nonlinear (wide) networks with bottlenecks is the cause of the same underlying implicit bias of depth.

\section{Experiments}

We provide empirical support for our theoretical contributions in three parts:
\begin{itemize}
    \item In part 1, we conduct simulations to numerically verify the theory in \cref{lemma:GP,lemma:linear,thm:main}. We present these results in \cref{verify}.
    \item  In part 2, we train infinite neural networks with bottlenecks on MNIST \cite{mnist} and CIFAR-10 \cite{cifar} datasets by simulating SGD on the loss in function space, investigating training acceleration effects and test performance. \Cref{fig:mnist_cifar_train_loss} summarizes the results from these experiments. We observe that the accelerated training predicted by  \cref{lemma:linear} for linear models is also visible when we train infinite width nonlinear networks with bottlenecks on the two real world datasets.\footnote{Figure~\ref{fig:cifar10_lr1k_metrics:train_loss_main} show loss for first 15K steps on CIFAR-10, as some training runs did not complete in time due to compute resource scarcity. Extended results showing loss trajectories for models trained longer are available in figure~\ref{fig:cifar10_lr1k_metrics} in \cref{sec:exp}.} We present additional results and analysis in \cref{sec:exp}.
    %\item In part 3, we run experiments with finite width networks trained with standard SGD to verify whether the acceleration effect holds in this setting. These results are presented in \cref{finiteapprox}.
    \item In part 3, we run experiments with finite width networks trained with standard SGD and verify that the acceleration effect holds in this setting. These results are presented in \cref{finiteapprox}.
\end{itemize}

% \subsection{Training infinite width bottlenecks in function space}
\label{expt:data:mnist}
\begin{figure}[!t]
    %\centering % Not needed
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{workshop_figures/MNIST/train_loss.pdf}
        \caption{MNIST training loss}
        % with learning rate 250
        \label{fig:mnist_lr5k_metrics:train_loss_main}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{workshop_figures/CIFAR10/train_loss_first10K.pdf}
        \caption{CIFAR-10 training}
        % with learning rate 1000
        \label{fig:cifar10_lr1k_metrics:train_loss_main}
    \end{subfigure}

    \caption{{Training loss for infinite width bottleneck models with widths from smallest to largest (inf indicates infinite width bottleneck). Loss is reduced in function space by simulating SGD with \cref{eqn:eq1,eqn:eq2,eqn:eq3}}}%
    \label{fig:mnist_cifar_train_loss}
\end{figure}

% \paragraph{Loss and accuracy evolution} Figure \ref{fig:mnist_cifar_train_loss} shows the evolution of training and test loss and accuracy over the course of training for architectures where the bottleneck width is varied from ``no bottleneck'' (labeled as inf) to 1000. The models are trained with a learning rate set to 250. We observe that training loss decays rapidly for networks with narrower bottlenecks. Figure \ref{fig:mnist_lr5k_heatmap} (\cref{sec:exp}) further examines training and test loss evolution for the first 4000 steps, confirming that training accelerates as bottleneck size narrows. Note that while narrower bottleneck models exhibit lower test loss compared to their wider bottleneck counterparts, this does not hold for test accuracy, potentially due to a loss-metric mismatch as is commonly observed in practice \cite{pmlr-v97-huang19f}.
% These results are inline with the analysis of linear networks with bottlenecks, that wide networks with bottlenecks accelerate training, and confirm empirically that this effect holds in practical nonlinear settings.


% Additional results are available in \cref{sec:exp} that include:
% \begin{itemize}
%     \item regression with syntehtic data in \cref{expt:data:synthetic}
%     \item binary classification with CIFAR-10 data in \cref{expt:data:cifar10}
% \end{itemize}

\section{Conclusion}
In this work we investigate the effect of applying a bottleneck in an otherwise infinite width network. We do this by first deriving the ODEs corresponding to optimization of such a model under gradient flow. Our theoretical analysis reveals novel insights regarding the behaviour of input-output Jacobians, both at initialization and training. Though stated for shallow, single hidden layer networks post bottleneck, we expect our results to hold in more general cases. Empirically, we observe that infinite width networks with bottlenecks train much faster than their fully infinite counterparts, while typically achieving better overall performance. We hope our results pave the way for new understanding of learning dynamics beyond kernel regimes.

\bibliography{reference}
\bibliographystyle{icml2021}

\newpage
\appendix
\onecolumn
\section{Proof of \cref{lemma:GP}}
\GP*
\begin{proof}
Intuitively, the derivative process of any centered GP with a covariance kernel function $\mathcal{K}(x,\tilde{x})$ is another GP, provided that the kernel function $\mathcal{K}$ is everywhere differentiable. Since the Jacobian is by definition the derivative of the output with respect to the input, this implies that \cref{eqn:jac} holds, along with the stated covariances. However, the subtlety here is to show that the empirical Jacobian indeed converges in distribution to the derivative of the NN-GP in the limit. For a single hidden layer MLP, the convergence of the Jacobian to a GP can be established by a straightforward application of the standard CLT theorem, as in the feed forward case. \\
The Jacobian takes the form:
\[
J_n(x) = \frac{\partial f(x)}{\partial x} = \frac{v}{\sqrt{nd}} \text{diag}\big(\phi'(\frac{ux}{\sqrt{d}})\big)u
\]
Each element $J_n^{\alpha,\beta}(x)$ is hence given by:
\[
J_n^{\alpha,\beta}(x) = \frac{1}{\sqrt{nd}}\sum_{\gamma = 1}^n v^{\alpha,\gamma}\phi'^{\gamma}(\frac{ux}{\sqrt{d}})u^{\gamma,\beta}
\]
Note that the sequence $\{v^{\alpha,\gamma}\phi'^{\gamma}(ux)u^{\gamma,\beta}\}_{\gamma=1}^n$ is a sequence of zero mean iid variables. From the polynomial boundness of $\phi$, the coordinates of $\phi'$ have bounded second moment. We can then apply the CLT theorem to establish a convergence to a GP with some kernel function $\Sigma^\star \in \R^{3 \times 3}$:
\[
\begin{pmatrix}
f^\alpha(x)\\
J^{\alpha,\beta}(\tilde{x})\\
J^{\alpha,\delta}(x')
\end{pmatrix}
\overset{p}{=} \lim_{n \to \infty} \frac{1}{\sqrt{n}}\begin{pmatrix}
\sum_{\gamma=1}^n v^{\alpha,\gamma} \phi^{\gamma}(\frac{ux}{\sqrt{d}})\\
\frac{1}{\sqrt{d}}\sum_{\gamma = 1}^n v^{\alpha,\gamma}\phi'^{\gamma}(\frac{u\tilde{x}}{\sqrt{d}})u^{\gamma,\beta}\\
\frac{1}{\sqrt{d}}\sum_{\gamma = 1}^n v^{\alpha,\gamma}\phi'^{\gamma}(\frac{ux'}{\sqrt{d}})u^{\gamma,\delta}
\end{pmatrix} \overset{p}{=} \mathcal{N}(\bm{0},\Sigma^\star)
\]
From the independence of $u,v$, it trivially holds that $\forall_{\alpha,\beta}\E[J^{\alpha,\beta}(x)] = 0$, $\forall_{\alpha \neq \gamma,\beta,\delta}\E[J^{\alpha,\beta}(x)J^{\gamma,\delta}(\tilde{x})] = 0$ and $\forall_{\alpha \neq \gamma,\beta}\E[J^{\alpha,\beta}(x)f(\tilde{x})^\gamma] = 0$. For any two inputs $x,\tilde{x}$ we have:
\begin{align}
\forall_{\alpha,\beta, \delta}\E[J^{\alpha,\beta}(x)J^{\alpha,\delta}(\tilde{x})] &= \frac{1}{d}\E\Big[ \big(v^{\alpha,\gamma}\big)^2\phi'(\frac{ux}{\sqrt{d}})^\gamma \phi'(\frac{u\tilde{x}}{\sqrt{d}})^\gamma u^{\gamma,\beta}u^{\gamma,\delta}\Big]\\
&= \frac{1}{d}\E\Big[ \phi'(\frac{ux}{\sqrt{d}})^\gamma \phi'(\frac{u\tilde{x}}{\sqrt{d}})^\gamma u^{\gamma,\beta}u^{\gamma,\delta}\Big]\\
&= \E\Big[ \frac{\partial}{\partial x^\beta}\frac{\partial}{\partial \tilde{x}^\delta}\phi(\frac{ux}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma \Big]
\end{align}

From our assumption that $\phi$ is differentiable with bounded derivatives, it holds that $\E\Big[ \big|\frac{\partial}{\partial x^\beta}\frac{\partial}{\partial \tilde{x}^\delta}\phi(\frac{ux}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma \big|\Big] < \infty$. 
By the dominated convergence theorem:
\[
\E\Big[ \frac{\partial}{\partial x^\beta}\frac{\partial}{\partial \tilde{x}^\delta}\phi(\frac{ux}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma \Big] = \frac{\partial}{\partial x^\beta}\frac{\partial}{\partial \tilde{x}^\delta}\E\Big[ \phi(\frac{ux}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma \Big] = \frac{\partial^2}{\partial x^\beta \partial \tilde{x}^\delta}\Sigma(x,\tilde{x})
\]
Similarly:
\begin{align}
\forall_{\alpha,\beta, \delta}\E[J^{\alpha,\beta}(x)f^\alpha(\tilde{x})] &= \E\Big[\big(v^{\alpha,\gamma}\big)^2\phi'(\frac{ug}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma u^{\gamma,\beta}\Big]\\
&= \frac{\partial}{\partial x^\beta}\E\Big[\phi(\frac{ux}{\sqrt{d}})^\gamma \phi(\frac{u\tilde{x}}{\sqrt{d}})^\gamma \Big] = \frac{\partial}{\partial x^\beta}\Sigma(x,\tilde{x})
\end{align}
which concludes the proof.
\end{proof}
% {\bf{Omid's note to be removed}s}
% Let's compute 
% \[
% \E\Big[ \phi'(\frac{ug}{\sqrt{d}})^\gamma \phi'(\frac{ug}{\sqrt{d}})^\gamma u^{\gamma,\beta}u^{\gamma,\delta}\Big] = I^{\beta, \delta}(g) 
% \]
% for a simpler case, where $g \in \matbb{R}^{2}$ to see what we get. Let's write
% \[
% \phi^{'}(\frac{ug}{\sqrt{2}})^{\gamma} = \Theta(\frac{u_{\gamma1}g_1 + u_{\gamma2}g_2}{\sqrt{2}})
% \]
% and focus on $\beta=\delta=1$ 
% \[
%  I^{1,1}(g) = \int  \Theta^2(\frac{u_{\gamma1}g_1 + u_{\gamma2}g_2}{\sqrt{2}}) u^2_{\gamma1}\exp(-\frac{1}{2}u^2_{\gamma1}-\frac{1}{2}u^2_{\gamma2})\frac{du_{\gamma1}}{\sqrt{2\pi}}\frac{du_{\gamma2}}{\sqrt{2\pi}}
% \]
% Using the following change of variable
% \[
% u_{\gamma1}=\frac{\sqrt{2}x}{g_1\sqrt{\frac{1}{g^2_1}+\frac{1}{g^2_2}}},
% u_{\gamma2} = \sqrt{2}s - \frac{\sqrt{2}x}{g_2\sqrt{\frac{1}{g^2_1}+\frac{1}{g^2_2}}}
% \]
% one ends up with
% \[
% I^{1, 1}=\frac{4}{2\pi}\frac{1}{g^3_1(\frac{1}{g^2_1}+\frac{1}{g^2_2})^{\frac{3}{2}}}\int\Theta^2(s) x^2 \exp(-x^2-s^2+2\cos\phi_0 s x)dxds
% \]
% Switching to polar coordinates in $(x, s)$-plane, one gets
% \[
% I^{1, 1} = \frac{4}{2\pi}\frac{1}{g^3_1(\frac{1}{g^2_1}+\frac{1}{g^2_2})^{\frac{3}{2}}}\int^{\frac{\pi}{2}}_{0} d\Phi \int^{\infty}_0 dr r^3\sin^2\Phi\exp[-r^2(1-2\cos\phi_0\sin\Phi\cos\Phi)],\\\nonumber  \cos\phi_0=\frac{g_1}{\sqrt{g^2_1+g^2_2}}
% \]
% where the condition integrand being non-zero only for $s>0$  determines the upper bound on the $\Phi$ integral. Performing the integral over $r$ first 
% \[
% I^{1, 1} = \frac{4}{2\pi}\frac{1}{g^3_1(\frac{1}{g^2_1}+\frac{1}{g^2_2})^{\frac{3}{2}}}\int^{\frac{\pi}{2}}_{0}\frac{\sin^2\Phi}{2(1-2\cos\phi_0\sin\Phi\cos\Phi)^2}d\Phi
% \]
% This remaining integral can be performed using {\it Weierstrass substitution}. Take $t=\tan{\Phi}$, one obtains
% \[
% I^{1, 1}= \frac{4}{4\pi}\frac{1}{g^3_1(\frac{1}{g^2_1}+\frac{1}{g^2_2})^{\frac{3}{2}}}\int^{\infty}_0\frac{t^2}{(1+t^2-2\cos\phi_0 t)^2}dt
% \]
% after performing the integral and some more algebra, one finds
% \[
% I^{1, 1}=\frac{1}{2\pi}(\pi-\phi_0+\sin\phi_0\cos\phi_0),\quad \cos\phi_0=\frac{g_1}{\sqrt{g^2_1+g^2_2}}
% \]
% Note that just based on symmetry
% \[
% I^{2, 2} = \frac{1}{2\pi}(\pi - \phi_0+\sin\phi_0\cos\phi_0), \quad \cos\phi_0=\frac{g_2}{\sqrt{g^2_1+g^2_2}}
% \]

\section{Proof of \cref{thm:main}}
\main*

\begin{proof}
The empirical dynamical equation for $\F,g$ are given in \cref{eqn:random_kernel}:
\begin{align}
\dot{g}_{n,t}(\xi) &= -\sum_{i=1}^N\Theta_{n,t}(\xi,\xi_i)J_{n,t,i}^\top\chi_{n,t,i}\\
\dot{\F}_{n,t}(\tilde{\xi}) &= -\sum_{i=1}^N\mathcal{K}_{n,t}\big(g_{n,t},g_{n,t,i}\big)\chi_{n,t,i} 
-\sum_{i=1}^NJ_{n,t} \Theta_{n,t}(\tilde{\xi},\xi_i)J^{\top}_{n,t,i} \chi_{n,t,i}
\end{align}

Prior results regarding convergence of NTK functions have been established pointwise on a fixed dataset \cite{yang2}. In our scenario, we have both the bottleneck embeddings and the Jacobian which evolve continuously during training. This would not pose a major problem to us since a pointwise convergence implies local uniform convergence over a closed region. All we need is insure both the bottleneck embedding terms $g$ and Jacobian terms $J$ do explode during training.
% Note that $J_{n,t,i}$ depends on the weights $u,v$ only through the outputs $g$. Hence, conditioned on the bottleneck embeddings $g_{n,0}$, $\Theta_{n,0}$ is independent of the Jacobian $J^{\top}_{n,0}$, and both functions $\mathcal{K}_{n,t},\Theta_{n,t}$ converge almost surely to their respective limits $\mathcal{K},\Theta$.
Assuming this is the case, it is straightforward that:
\begin{align}
    \lim_{n \to \infty } \sum_{i=1}^N\Theta_{n,t}(\xi,\xi_i)J_{n,t,i}^\top\chi_{n,t,i} &= \sum_{i=1}^N\Theta(\xi,\xi_i)J_{t,i}^\top\chi_{t,i}\\
    \lim_{n \to \infty}\sum_{i=1}^NJ_{n,t} \Theta_{n,t}(\tilde{\xi},\xi_i)J^{\top}_{n,t,i}\chi_{n,t,i} &= \sum_{i=1}^N \Theta(\tilde{\xi},\xi_i)J_tJ^{\top}_{t,i} \chi_{t,i} 
\end{align}

% As the convergence of the first term, known results regarding NTK convergence have been established for a fixed dataset, with the kernel functions converge pointwise almost surely \cite{yang2}. In our scenario, we have both the bottleneck embeddings and the Jacobian which evolve continuously during training. This would not pose a major problem to us since a pointwise convergence implies local uniform convergence over a closed region. All we need is insure both the bottleneck embeddings and Jacobians do explode during training. \\
We now have to show how the bottleneck outputs evolve by deriving the dynamical equation for the Jacobian. 
This is done by taking the input derivative of the dynamical equation for $f(x)$. Namely:
\begin{align}
    \dot{J}_{n,t}(x) = \frac{\partial}{\partial x}\dot{f}_{n,t}(x) = -\frac{\partial}{\partial x}\sum_{i=1}^N\mathcal{K}_{n,t}(x,g_{n,t,i})\chi_{n,t,i} 
\end{align}
We will now prove that $x,\tilde{x}$, $\lim_{n \to \infty} \frac{\partial \mathcal{K}_{n,t}(x,\tilde{x})}{\partial x} = \frac{\partial \mathcal{K}(x,\tilde{x})I_{d_r}}{\partial x}$.

Let $y(x) = \frac{ux}{\sqrt{d}}, z(x) = \phi \big(y(x)\big)$.
For our 1 hidden layer MLP, the kernel $\mathcal{K}_n$ is given by:
\begin{align}
    \mathcal{K}_n(x,\tilde{x}) = \frac{\partial f_n(x)}{\partial w}\frac{\partial  f_n(\tilde{x})^\top}{\partial w} = \frac{x^\top \tilde{x}}{d}\cdot \frac{1}{n}\frac{\partial f(x)}{\partial y_n(x)}\frac{\partial f_n(\tilde{x})^\top}{\partial y_n(\tilde{x})} + \frac{z_n(x)^\top z_n(\tilde{x})}{n}I_{d_r}
\end{align}
Hence, we have that:
\begin{align}
    \frac{\partial}{\partial x^\alpha}\mathcal{K}_n(x,\tilde{x}) = \frac{\tilde{x}^\alpha}{d}\cdot \frac{1}{n}\frac{\partial f_n(x)}{\partial y_n(x)}\frac{\partial f_n(\tilde{x})^\top}{\partial y_n(\tilde{x})} + \frac{x^\top \tilde{x}}{d}\cdot \frac{1}{n}\frac{\partial^2 f_n(x)}{\partial y_n(x) \partial x^\alpha}\frac{\partial f_n(\tilde{x})^\top}{\partial y_n(\tilde{x})} + \frac{1}{n}\frac{\partial z_n(x)^\top}{\partial x^\alpha}z_n(\tilde{x})I_{r_d}
\end{align}

We now handle each term separately. For the first term, we have the trivial limit which holds throughout training:
\begin{align}
    \frac{\tilde{x}^\alpha}{d}\cdot \frac{1}{n}\frac{\partial f_n(x)}{\partial y_n(x)}\frac{\partial f_n(\tilde{x})^\top}{\partial y_n(\tilde{x})} \to \frac{\tilde{x}^\alpha}{d}\dot{\Sigma}(x,\tilde{x})
\end{align}
where:
\begin{align}
    \dot{\Sigma}(x,\tilde{x}) &= \E_{a,b \sim \mathcal{N}(\bm{0},\Lambda_0(x,\tilde{x}))}\big[\dot{\phi}(a)\dot{\phi}(b)\big],~~~
    \Lambda_0(x,\tilde{x}) = \begin{pmatrix} \frac{\|x\|^2}{d} & \frac{x^\top \tilde{x}}{d}\\ \frac{\tilde{x}^\top x}{d} & \frac{\|\tilde{x}\|^2}{d} \end{pmatrix}
\end{align}
For the second term:
\begin{align}
    \frac{\partial f_n^\beta(x)}{\partial y_n^\gamma(g)} = v^{\beta,\gamma} \phi'^\gamma(\frac{ux}{\sqrt{d}}),~~~\frac{\partial^2 f_n^\beta(x)}{\partial y_n^\gamma(x) \partial x^\alpha} = \frac{1}{\sqrt{d}}v^{\beta,\gamma}u^{\gamma,\alpha} \phi''^\gamma(\frac{ux}{\sqrt{d}})
\end{align}
hence at the limit:
\begin{align}
    \frac{1}{n}\frac{\partial^2 f_n^\beta(x)}{\partial y_n(x) \partial x^\alpha}\frac{\partial f_n^\delta(\tilde{x})^\top}{\partial y_n(\tilde{x})} &= \frac{1}{\sqrt{d}}\frac{\sum_{\gamma=1}^n v^{\beta,\gamma}v^{\delta,\gamma}u^{\gamma,\alpha} \phi''^\gamma(\frac{ux}{\sqrt{d}})\phi'^\gamma(\frac{u\tilde{x}}{\sqrt{d}})}{n} \label{eqn:clt}\\
    &\to \frac{1}{\sqrt{d}}\E\Big[u^{\gamma,\alpha}\phi''^\gamma(\frac{ux}{\sqrt{d}})\phi'^\gamma(\frac{u\tilde{x}}{\sqrt{d}}) \Big]\mathbb{1}(\beta = \delta)\label{eqn:before}\\
    &= \E\Big[\frac{\partial}{\partial x^\alpha} \phi'^\gamma(\frac{ux}{\sqrt{d}})\phi'^\gamma(\frac{u\tilde{x}}{\sqrt{d}}) \Big]\mathbb{1}(\beta = \delta)\label{eqn:after}\\
    &= \frac{\partial}{\partial x^\alpha}\E\Big[ \phi'^\gamma(\frac{ux}{\sqrt{d}})\phi'^\gamma(\frac{u\tilde{x}}{\sqrt{d}}) \Big]\mathbb{1}(\beta = \delta)\\
    &= \frac{\partial}{\partial x^\alpha}\dot{\Sigma}(x,\tilde{x})\mathbb{1}(\beta = \delta)
\end{align}
where we used the independence of the coordinates of $\phi',\phi''$ to apply the LLN (law of large numbers) theorem on \cref{eqn:clt}, and used the dominated convergence theorem to get from \cref{eqn:before} to \cref{eqn:after}. We arrive at the result for the second term:
\begin{align}
    \frac{x^\top \tilde{x}}{d}\cdot \frac{1}{n}\frac{\partial^2 f_n(x)}{\partial y_n(x) \partial x^\alpha}\frac{\partial f_n(\tilde{x})^\top}{\partial y_n(\tilde{x})} \to \frac{x^\top \tilde{x}}{d}\frac{\partial}{\partial x^\alpha}\dot{\Sigma}(x,\tilde{x})I_{d_r}
\end{align}

For the final term we have similarly:
\begin{align}
    \frac{1}{n}\frac{\partial z_n(x)^\top}{\partial x^\alpha}z_n(\tilde{x})I_{r_d} = \frac{1}{\sqrt{d}}\frac{\sum_{\gamma =1}^n u^{\gamma,\alpha}\phi'^\gamma(\frac{ux}{\sqrt{d}})\phi^\gamma(\frac{u\tilde{x}}{\sqrt{d}})}{n} \to \frac{\partial}{\partial x^\alpha}\Sigma(x,\tilde{x})
\end{align}
where we again used the independence of the coordinates of $\phi$ and applied LLN to the sum. Wrapping up all three terms, we arrive at:
\begin{align}
    \lim_{n \to \infty}\frac{\partial}{\partial x^\alpha}\mathcal{K}_n(x,\tilde{x}) = \frac{\partial}{\partial x^\alpha}\Big(\frac{x^\top \tilde{x}}{d}\dot{\Sigma}(x,\tilde{x}) + \Sigma(x,\tilde{x})\Big) = \frac{\partial}{\partial x^\alpha}\mathcal{K}(x,\tilde{x}) = \Xi^\alpha(x,\tilde{x})
\end{align}
concluding the proof.
\end{proof}

\section{Proof of \cref{lemma:linear}}
\linear*
\begin{proof}
We prove the claim by deriving the update equations using \cref{thm:main} for both cases and show their equivalence.
Concretely, we must show that the functions $\Theta,\mathcal{K},\Xi$ are the same in both cases ups to constants. When optimizing the effective weights $w_{\text{eff}},\theta_{\text{eff}}$ directly, we trivially have that $\Theta(\xi,\tilde{\xi}) = \frac{\xi^\top \tilde{\xi}}{d_0}, \mathcal{K}(x,\tilde{x}) = \frac{x^\top \tilde{x}}{d}$ and $\Xi(x,\tilde{x}) = \frac{\tilde{x}}{d}$.\\
When optimizing the weights $\{\theta^l\}_{l=1}^{L_g},\{w^l\}_{l=1}^{L_f}$ instead, we use the recursive formulas for the NTK of MLPs in \cite{exact}, equipped with linear activations, and arrive at $\Theta(\xi,\tilde{\xi}) = L_g\frac{\xi^\top \tilde{\xi}}{d_0}, \mathcal{K}(x,\tilde{x}) = L_f\frac{x^\top \tilde{x}}{d}$ and $\Xi(x,\tilde{x}) = L_f\frac{\tilde{x}}{d}$, concluding the proof.
\end{proof}




% \section{Experimental Details}\label{sec:exp}
\section {Empirical support for implicit acceleration - setup and additional results}\label{sec:exp}
\paragraph{Stochastic Gradient Descent}
To perform empirical experiments, we resort to the discrete time version of \cref{eqn:eq1,eqn:eq2,eqn:eq3} which will allow simulating SGD on the loss in function space. Let $\xi_{i_s}$ denote the sample fed into the SGD algorithm at step $s$. The discrete evolution of $\F,g,J$ assuming a learning rate $\mu$ is given by:
\begin{align}
g_{t+1}(\xi) &= g_{0}(\xi) -\mu\sum_{s=1}^t\Theta(\xi,\xi_{i_s})J_{s,i_s}^\top\chi_{s,i_s} \label{eqn:d1}\\
J_{t+1}(x) &= J_{0}(x) -\mu\sum_{s=1}^t \chi_{s,i_s}\Xi(x,g_{s,i_s})^\top \label{eqn:d2}\\
\F_{t+1}(\xi) &= \F_{0}(\xi) -\mu\sum_{s=1}^t\mathcal{K}\big(g_{t+1}(\xi),g_{s,i_s}\big)I_{d_r}\chi_{s,i_s} \label{eqn:d3}
\end{align}

\subsection{Model description} 

We use 1 hidden layer MLPs with ReLU\footnote{While ReLUs dont meet the criteria of a twice differentiable activations, our results indeed hold still by approximating the ReLU function using the softplus function $\phi(a;m) = \frac{1}{m}\log(1 + \text{e}^{ma})$, and taking its limit $m \to \infty$ after taking $n \to \infty$.} activations for both $f,g$ in all experiments. For any pair of inputs $\xi,\tilde{\xi}$ with $D = \|\xi\|\|\tilde{\xi}\|, \lambda = \frac{\xi^\top \tilde{\xi}}{D}$, the NTK function $\Theta(\xi,\tilde{\xi})$ is given by $\Theta(\xi,\tilde{\xi}) = \frac{\xi^\top\tilde{\xi}}{d_0}\dot{\Sigma}(\xi,\tilde{\xi}) + \Sigma(\xi,\tilde{\xi})$
where:
\begin{align}
    \Sigma(\xi,\tilde{\xi}) = \frac{D}{d_0}\frac{\lambda(\pi - \text{arccos}(\lambda)) + \sqrt{1 - \lambda^2}}{2\pi},~~~
    \dot{\Sigma}(\xi,\tilde{\xi}) = \frac{\pi - \text{arccos}(\lambda)}{2\pi}
\end{align}
$\mathcal{K}(x,\tilde{x})$ computes the same function with the replacement $d_0 \to d$, and the inputs $D = \|x\|\|\tilde{x}\|, \lambda = \frac{x^\top \tilde{x}}{D}$.\\
The function $\Xi(x,\tilde{x})$ is computed by taking the derivative of $\mathcal{K}(x,\tilde{x})$ with respect to the first input $x$. This results in the following:
\begin{align}
    \Xi(x,\tilde{x}) = \frac{\tilde{x}}{d}\dot{\Sigma} + \frac{x^\top \tilde{x}}{d}\frac{1}{2\pi\sqrt{1 - \lambda^2}}\big(\frac{\tilde{x}}{D} - \lambda \frac{x}{\|x\|^2}\big) + \frac{D}{d}\dot{\Sigma}(x,\tilde{x})\big(\frac{\tilde{x}}{D} - \lambda \frac{x}{\|x\|^2}\big) + \frac{x\|\tilde{x}\|}{\|x\|D}\Sigma(x,\tilde{x})
\end{align}
We consider a range of bottleneck widths from infinite bottleneck (``no bottleneck'') (labeled as inf in plots) to a maximum value that varies per dataset as noted in the result section below. Finally, we use initialized networks with $n = 10000$ to sample $g_{0}(\xi), J_0(x), F_0(\xi)$ for all inputs. Experiments are implemented using the Tensorflow \cite{tensorflow2015-whitepaper} package.

\subsection{Tasks and datasets}

We consider a regression problem on synthetic dataset in \cref{expt:data:synthetic}, multi-class classification on MNIST dataset \cite{mnist} in \cref{expt:data:mnist} and a binary classification problem (dog vs deer) on CIFAR-10 dataset \cite{cifar} in \cref{expt:data:cifar10}. The networks are optimized with mean squared error (MSE) criterion where the targets are one-hot encoded vectors in classification experiments. 

\subsection{Compute budget}
We train all models for a fixed amount of time per set of hyperparameters that include learning rate and mini-batch size per dataset. This budget is necessary in order for us to manage the burden imposed on our compute resources. Consequently narrower finite width bottleneck models train for a larger number of steps compared to wider finite width bottleneck models.

\subsection{Results}
\label{appendix:expt:results}
We present the results of experiments with infinite width bottleneck nonlinear models in this section. Our main finding is that the acceleration effect suggested by \cref{lemma:linear} for linear models during optimization is also seen with nonlinear models on both synthetic and real datasets. Our presentation includes two types of plots that allows us to make observations about both training speed as well as test performance:
\paragraph{Loss evolution} We present plots that show the evolution of loss and accuracy for both training and test datasets. These plots allow us to make observations on the implicit acceleration effect in infinite width bottleneck models as well as its impact on test performance.

\paragraph{Training vs test loss} \label{appendix:expt:results:tvt} Additionally, we plot test loss as a function of training loss for several bottleneck widths. This plot allows us to compare test performance for a given training loss across different bottleneck widths. 

The procedure used to generate this plot is described below for completeness:
\begin{itemize}
    \item determine a range for training loss values that covers all bottleneck widths for a given dataset. We set this range to be the minimum and maximum recorded training losses across all bottleneck widths for each dataset.
    \item resample training loss values in the above range from training loss values recorded in our experiments.
    \item determine test loss values for the training loss values obtained in the above step again by resampling with linear interpolation.
    \item smooth both training and test loss values with a moving average filter of length 5.
\end{itemize}

\subsubsection{Synthetic dataset}
\label{expt:data:synthetic}

\begin{figure}[htb]
\centering
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/train_loss.pdf}&
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/test_loss.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Synthetic dataset:  metrics for finite bottleneck models of various widths. Widths are shown from smallest to largest where inf indicates infinite bottleneck (a) training loss  (b) test loss}}%
  \label{fig:synth_lr2k_metrics}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[htb]
\centering
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/train_loss_heatmap.pdf}&
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/test_loss_heatmap.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Synthetic dataset: heatmap view of (a) training loss evolution and (b) test loss evolution for the first 4000 steps}}%
  \label{fig:synth_lr2k_heatmap}
  %\vspace{-.2cm}
\end{figure}

% \begin{figure}[!b]
% \centering
% \includegraphics[width=.5\columnwidth]{workshop_figures/synth/train_vs_test.pdf}\\
%   \caption{{Synthetic dataset: training loss vs test loss plot}}%
%   \label{fig:synth_lr2k_tvt}
%   %\vspace{-.2cm}
% \end{figure}
\begin{figure}[!b]
\centering
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/train_vs_test.pdf}&
\includegraphics[width=.5\columnwidth]{workshop_figures/synth/train_vs_test_no_inf.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Synthetic dataset: training loss vs test loss plot for (a) finite and infinite width bottleneck models and (b) finite width bottleneck models}}%
  \label{fig:synth_lr2k_tvt}
  %\vspace{-.2cm}
\end{figure}

We first study the behavior of infinite width bottleneck models by running \cref{eqn:d1,eqn:d2,eqn:d3} on a regression problem with synthetic data. To this end, we generate a dataset that consists of 2000 training and test samples where the:
\begin{itemize}
    \item inputs are random vectors sampled from a standard multivariate normal distribution.
    \item targets are formed by passing these vectors through a deep Gaussian Process model.
\end{itemize}
 The widths considered for this dataset range from infinite width (inf) to a width of 500. The models are trained with SGD using a mini-batch size of 20 and learning rate set to 2000.


\paragraph{Loss evolution}
Figure~\ref{fig:synth_lr2k_metrics}a and figure~\ref{fig:synth_lr2k_metrics}b show the evolution of training and test loss for models with different bottleneck widths. Figure~\ref{fig:synth_lr2k_metrics}a clearly shows that architectures with narrower bottleneck train faster than their wider bottleneck counterparts. Figure~\ref{fig:synth_lr2k_heatmap} shows a heatmap view of the evolution of training and test loss for the first 4000 steps to zero in on the early phase of learning. Figure~\ref{fig:synth_lr2k_heatmap} clearly demonstrates the acceleration effect of training and test loss for small to intermediate bottleneck sizes, while no bottleneck and large bottleneck models exhibit slower learning.

\paragraph{Training vs test loss}
We observe from figure~\ref{fig:synth_lr2k_metrics}a that the infinite width bottleneck model exhibits a higher training loss compared to its finite width bottleneck counterparts. To make meaningful comparisons, we resort to using two plots for this dataset --- figure~\ref{fig:synth_lr2k_tvt}a shows performance of all models including infinite width bottleneck (labeled as inf) and figure~\ref{fig:synth_lr2k_tvt}b that shows performance of finite width bottlenecks only. We make the following observations from the two plots:
\begin{itemize}
    \item finite width bottleneck models lead to a lower test loss for the same training loss over infinite width bottlenecks models.
\item the widest bottleneck width model (500) has the best test performance as seen from figure~\ref{fig:synth_lr2k_tvt}b
\end{itemize}
The above observations suggest that using a bottleneck in infinite width models can lead to better test performance across a range of bottleneck widths over infinite width (``no bottleneck'') model. The narrowest bottleneck width (10) considered here in this experiment appears to show an increase in test loss. This, however, is an artifact that arises due the training loss range displayed in figure~\ref{fig:synth_lr2k_tvt}a. We see from figure~\ref{fig:synth_lr2k_tvt}b the the training vs test loss curve eventually decreases and matches values observed in models with bottleneck widths of 20, 30 and 50. This non-monotonic behavior may be caused by sub-optimal hyperparameter selection that we plan to address as future work.

\subsubsection{MNIST dataset}
\begin{figure*}[htb]
    %\centering % Not needed
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/MNIST/train_loss.pdf}
        \caption{train loss}
        \label{fig:mnist_lr5k_metrics:train_loss}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/MNIST/train_accuracy.pdf}
        \caption{train accuracy}
        \label{fig:mnist_lr5k_metrics:train_acc}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/MNIST/test_loss.pdf}
        \caption{test loss}
        \label{fig:mnist_lr5k_metrics:test_loss}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/MNIST/test_accuracy.pdf}
        \caption{test accuracy}
        \label{fig:mnist_lr5k_metrics:test_acc}
    \end{subfigure}\hfill
    \caption{{MNIST dataset: training metrics for bottleneck models with widths from smallest to largest (inf indicates infinite width bottleneck)}}%
    \label{fig:mnist_lr5k_metrics}
\end{figure*}
We study the behavior of infinite width neural networks on MNIST dataset \cite{mnist} in this section. We train models on a multi-class (10-class) classification problem using the MSE loss in this experiment. The bottleneck widths used here range from ``no bottleneck'' (labeled as inf) to 1000 and are trained with SGD using a learning rate of 250. As with the previous section, we discuss implicit acceleration during optimization followed by test performance below:
\label{expt:data:mnist}
\paragraph{Loss and accuracy evolution} Figure~\ref{fig:mnist_lr5k_metrics} shows the evolution of training and test loss and accuracy over the course of training. We observe that training loss decays rapidly for networks with narrower bottlenecks. Figure~\ref{fig:mnist_lr5k_heatmap} (\cref{sec:exp}) further examines training and test loss evolution for the first 4000 steps, confirming that training accelerates as bottleneck size narrows. Note that while narrower bottleneck models exhibit lower test loss compared to their wider bottleneck counterparts, this does not hold for test accuracy, potentially due to a loss-metric mismatch as is commonly observed in practice \cite{pmlr-v97-huang19f}.
These results are inline with the analysis of linear networks with bottlenecks, that wide networks with bottlenecks accelerate training, and confirm empirically that this effect holds in practical nonlinear settings.
\begin{figure}[htb]
\centering
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{workshop_figures/MNIST/train_loss_heatmap.pdf}&
\includegraphics[width=.5\columnwidth]{workshop_figures/MNIST/test_loss_heatmap.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{MNIST dataset: heatmap view of (a) training loss evolution and (b) test loss evolution for the first 4000 steps}}%
  \label{fig:mnist_lr5k_heatmap}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\columnwidth]{workshop_figures/MNIST/train_vs_test.pdf}\\
  \caption{{MNIST dataset: training loss vs test loss plot}}%
  \label{fig:mnist_lr5k_tvt}
  %\vspace{-.2cm}
\end{figure}

\paragraph{Training vs test loss} Figure~\ref{fig:mnist_lr5k_tvt} shows a plot of training vs test loss for MNIST dataset from which we make the following observations:
\begin{itemize}
    \item finite width bottleneck models achieve lower test loss for a given training loss over infinite width bottlenecks.
    \item narrower bottlenecks attain lower test loss compared to their wider finite width counterparts for all widths considered for this dataset.
\end{itemize}
The above observations suggest that using bottlenecks in infinite width nonlinear models can lead to a lower test loss and hence improved test performance on real datasets as well. However, we repeat our previous observation that lower test loss does not always lead to an improvement in test accuracy. 

\subsubsection{CIFAR-10 dataset}
Finally, we study minimizing loss in function space on CIFAR-10 \cite{cifar} which is another instance of real world data. We study a binary classification problem using two classes from CIFAR-10 in this set of experiments. As before, we first present our findings on acceleration effect during training followed by observations on test performance. The models are trained with SGD using a learning rate of 1000 in these experiments.
\label{expt:data:cifar10}
\paragraph{Loss and accuracy evolution} Figure~\ref{fig:cifar10_lr1k_metrics} shows training loss, training accuracy, test loss, test accuracy for experiments on CIFAR10 dataset.  Figure~\ref{fig:cifar10_lr1k_heatmap} shows the evolution of training loss for 4000 steps and clearly illustrates the acceleration effect in finite bottleneck models. The accelerated learning effect is stronger in narrower bottleneck architectures. Interestingly, we observe that lower loss value leads to better test accuracy in this dataset unlike our observation for MNIST in \cref{expt:data:mnist}.

\paragraph{Training vs test loss} We make the following observations from figure~\ref{fig:cifar10_lr1k_tvt} that shows a plot of training vs test loss for CIFAR-10 dataset:
\begin{itemize}
    \item Larger finite width bottleneck models attain a lower test loss compared to infinite width bottleneck model.
    \item Larger finite width bottleneck models achieve a lower test loss compared to their narrower width counterparts.
\end{itemize}
\begin{figure*}
    %\centering % Not needed
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/CIFAR10/train_loss.pdf}
        \caption{train loss}
        \label{fig:cifar10_lr1k_metrics:train_loss}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/CIFAR10/train_accuracy.pdf}
        \caption{train accuracy}
        \label{fig:cifar10_lr1k_metrics:train_acc}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/CIFAR10/test_loss.pdf}
        \caption{test loss}
        \label{fig:cifar10_lr1k_metrics:test_loss}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{workshop_figures/CIFAR10/test_accuracy.pdf}
        \caption{test accuracy}
        \label{fig:cifar10_lr1k_metrics:test_acc}
    \end{subfigure}\hfill
    \caption{{CIFAR-10 dataset: training metrics for bottleneck models with widths from smallest to largest (inf indicates infinite width bottleneck)}}%
    \label{fig:cifar10_lr1k_metrics}
\end{figure*}

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{workshop_figures/CIFAR10/train_loss_heatmap.pdf}&
\includegraphics[width=.5\columnwidth]{workshop_figures/CIFAR10/test_loss_heatmap.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{CIFAR-10 dataset: heatmap view of (a) training loss evolution and (b) test loss evolution for the first 4000 steps}}%
  \label{fig:cifar10_lr1k_heatmap}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=.5\columnwidth]{workshop_figures/CIFAR10/train_vs_test.pdf}\\
  \caption{{CIFAR-10 dataset: training loss vs test loss plot}}%
  \label{fig:cifar10_lr1k_tvt}
  %\vspace{1cm}
\end{figure}

% To prove the claim we first establish the convergence of the dynamics

% of $g_n,\F_n$. For $g_n$, the empirical dynamical equation is given by the following ODE:
% \[
% \dot{g}_{n,t}(\tilde{\xi}) = \sum_{i=1}^N\Theta_{n,t}(\tilde{\xi},\xi_i)J_{n,t,i}^\top\chi_{n,t,i}
% \]
% Note that we may naively apply standard infinite width results for MLPs since $J_{n,t,i}$ depends on the weights $u,v$ only through the outputs $g$. Hence, we can construct an effective loss $\hat{\mathcal{L}}(g) = \mathcal{L}\big(f(g)\big)$, with a derivative $\nabla_g  \hat{\mathcal{L}}(g) = J(g)^\top \chi$. This concludes the proof of the updates to $g$ in the limit. The empirical dynamical equation for $\F$ is given in \cref{eqn:random_kernel}:
% \[
% \dot{\F}_{n,t}(\tilde{\xi}) = -\sum_{i=1}^N\mathcal{K}_{n,t}\big(\tilde{g}_{n,t},g_{n,t,i}\big)\chi_{n,t,i} 
% -\sum_{i=1}^N\tilde{J}_{n,t} \Theta_{n,t}(\tilde{\xi},\xi_i)J^{\top}_{n,t}(g_{n,t,i}) \chi_{n,t,i}
% \]
% In the infinite width limit both functions $\mathcal{K}_{n,t},\Theta_{n,t}$ converge almost surely to their respective limits $\mathcal{K},\Theta$. Moreover, both $\mathcal{K}(\tilde{g},g),$


% Since the Jacobian is defined as the derivative of the output $f(g)$ with respect to the input $g$, 

% For any input $g$, the Jacobian is defined as $J(g) = \frac{\partial f(g)}{\partial g}$. From the existence of the NN-GP in the limit of $n \to \infty$, we have that for any fixed input $g$, $\lim_{n \to \infty} f(g) = \mathring{f}(g)$ where $\mathring{f}$ is a gaussian process defined in \cref{eqn:nngp}. 


\section{Experimental verification of the theory}\label{verify}
Here we present results of a number of numerical simulations, serving as sanity checks on the theoretical results at finite but large $n$.  In order to numerically verify the results stated in \cref{lemma:GP}, i.e., Jacobian's GP behavior at initialization, we numerically estimate the Jacobian covariance and its cross- covariance with $f$ for a two-layer \text{ReLU} network with a large number of hidden units $n$ by averaging over initializations (100K for $J-J$, 30K for $J-f$ and $f-f$ each). Deviation of the empirical Jacobian covariances from their theoretical counterparts, given by \cref{JJ:cov}, \cref{fJ:cov} and \cref{sig1:sig2:def}, is subsequently measured for randomly generated input pairs as $\lVert cov_{empirical}-cov_{theory}\rVert_{F}/\lVert cov_{theory} \rVert_{F}$. Each plot in Figure \cref{fig:jj_cov_devs} shows deviations of a given covariance structure for up to 50 randomly generated input data pairs of dimension two and for different values of (large) $n$.\footnote{The main objective here is to only verify deviations/fractional errors are small for large $n$. In particular, we do not attempt to numerically establish convergence as a function of $n$.}

Next, we shift our attention to showing the evolution equations in \cref{{thm:main}} hold numerically for sample parameter values. The following direct strategy is used: We train four-layer \text{ReLU} networks with finite bottlenecks and large number of hidden units $n$, with architecture described in \cref{sec:bottle}, using full batch gradient descent (\text{GD}) and a small learning rate $lr=10^{-3}$ (to approximate a gradient flow) on a regression task with an \text{L2}-loss. Training data is synthetically generated. Instances are taken to be standard normal-distributed, while labels are assigned by a fixed random projection of the training data. In order to numerically track the network in the functional space during training, we checkpoint values of the network function $f$, $\chi$ (loss response) and the bottleneck embedding $g$, all evaluated on the training instances, as well as the Jacobian $J$ (for a fixed sample bottleneck embedding) at every training step during \text{GD}. The idea is to numerically show these snapshots satisfy the concentrated evolution equations \cref{eqn:eq1}, \cref{eqn:eq2} and \cref{eqn:eq3} at their corresponding time steps, if the \text{NTK} limiting \text{ReLU} kernels $\mathcal{K}$ and $\Theta$ are used and time derivatives in the evolution equations are discretized by forward finite differences (with time step size set to be the learning rate). Equivalently, we plot the corresponding residual error per step. Figure (\ref{fig:eom:res:errors}) summarizes the simulation results for a choice of parameters.  Each point in the Figure (\ref{fig:eom:res:errors}) plots is a residual error measured at a time step $t$ as follows $\lVert (\psi[t+1]-\psi[t])/lr-EQ_{\psi}^{rhs}[t]\rVert/\lVert EQ_{\psi}^{rhs}[t]\rVert$, where $\psi\in\{f, g, J\}$.\footnote{Besides finite $n$, continuous time discretization error (finite learning rate) contributes to the error.} Color-coding corresponds to different training samples on which the functional evolution equations were evaluated.

To verify the main result in \cref{lemma:linear}, we train two linear networks with different architectures of the type described in \cref{sec:implicit} for sample parameter values. One of the networks is taken to have $f_n$ and $g_n$ maps as linear \text{MLPs} of depth two. The second network has the simple form $\F(\xi) = w_{\text{eff}}\theta_\text{eff} \xi$ (as if the depth-two components of the first network are collapsed into one {\it effective} each). Similar to the previous case, we record the two networks dynamics in functional space during \text{GD}. We then numerically show the evolution equations \cref{eqn:eq1}, \cref{eqn:eq2} and \cref{eqn:eq3} are satisfied by the empirical trajectories in the functional space, when the appropriate NTK kernels for each {\it linear} network is used.  \cref{fig:eom:res:errors:4-layer:lin} and \cref{fig:eom:res:errors:2-layer:lin} show the evolution equations' residual error for the four and two layer linear networks described above, where kernels $\Theta(\xi,\tilde{\xi}) = L_g\frac{\xi^\top \tilde{\xi}}{d_0}, \mathcal{K}(x,\tilde{x}) = L_f\frac{x^\top \tilde{x}}{d}$ for $L_f=L_g=2$ and $\Theta(\xi,\tilde{\xi}) = \frac{\xi^\top \tilde{\xi}}{d_0}, \mathcal{K}(x,\tilde{x}) = \frac{x^\top \tilde{x}}{d}$ were used in the evolution equations respectively. The learning rate was set to $10^{-4}$. 

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/exp_jacob_vs_theory_dev.pdf}&
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/exp_jacob_vs_theory_dev_ff.pdf}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/exp_jacob_vs_theory_dev_jf.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Deviation of the experimental covariances from their theoretical predictions in (\ref{JJ:cov}) for pairs of random input samples for (a) $cov_{JJ}$ (b) $cov_{ff}$ and (c) $cov_{Jf}$. See \cref{verify} for details.}}%
  \label{fig:jj_cov_devs}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_f_dot_eom_n_50k_d_3.pdf}&
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_g_dot_eom_n_50k_d_3.pdf}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_J_dot_eom_n_50k_d_3.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Residual error for $f$, $g$ and $J$ evolution equations for $n=50K$ and a bottleneck size of three. See \cref{verify} for details.}}%
  \label{fig:eom:res:errors}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_f_dot_eom_n_50k_d_3_l_4_linear_layers.pdf}&
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_g_dot_eom_n_50k_d_3_l_4_linear_layers.pdf}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_J_dot_eom_n_50k_d_3_l_4_linear_layers.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Residual error for $f$, $g$ and $J$ evolution equations for a four-layer linear network with $n=50K$ and a bottleneck size of three. See \cref{verify} for details..}}%
  \label{fig:eom:res:errors:4-layer:lin}
  %\vspace{-.2cm}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tabular}{cc}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_f_dot_eom_d_3_l_2_layer_effective.pdf}&
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_g_dot_eom_d_3_l_2_layer_effective.pdf}
\includegraphics[width=.3\columnwidth]{numerical_verification_figures/err_in_J_dot_eom_n_d_3_l_2_layer_effective.pdf}\\
(a) & (b)\\
\end{tabular}
  \caption{{Residual error for $f$, $g$ and $J$ evolution equations for a two-layer effective linear network described in \cref{verify}. Bottleneck size of three. Refer to \cref{verify} for details.}}%
  \label{fig:eom:res:errors:2-layer:lin}
  %\vspace{-.2cm}
\end{figure}

\newpage
\section{Finite approximation of infinite width networks}\label{finiteapprox}

We set up four-layer neural networks with a large number of dimensions $n$ (100K) to approximate infinite width layers, as well as a significantly smaller bottleneck layer between the wide layers. We used the standard batched stochastic gradient descent (SGD) with an L2-loss to train the model for a multioutput regression task on the MNIST dataset. The batchsize was reduced to 5 to allow for these wider networks to fit within GPU memory. We trained variants of the network with different bottleneck sizes and analyzed the change in losses during training (Figure~\ref{fig:finite_epoch}). The final loss and error rate (1-accuracy) at the end of training for 36 different bottleneck sizes: [1 to 9, 10 to 90, 100 to 900, 1000 to 9000] is shown in Figure~\ref{fig:finite_d} . For better clarity, Figure~\ref{fig:finite_epoch} shows the following selection of bottleneck sizes: [1, 2, 4, 8, 10, 20, 40, 80, 100, 200, 400, 800, 1000].

From the results, the best performing models have a relatively small bottleneck size. We observe that performance improves when the bottleneck size goes from wide to narrow, before degrading again when the width is extremely small as the representation power degrades. At the sweet spot, the model not only achieves lower training and test losses, it also trains faster. Interestingly, the bottleneck size with the lowest test loss did not result in the lowest test error (or highest accuracy). These observations are consistent with those in the infinite width bottleneck experiments (section~\ref{expt:data:mnist}).

% The best performing models have a relatively small bottleneck size, when measuring their loss or error rate.
%% SOME NOTES
% lower test/train loss as bottleneck as width goes from wide -> narrow
%finite width bottleneck models achieve lower test loss for a given training loss over infinite width bottlenecks
%narrower bottlenecks attain lower test loss compared to their wider finite width counterparts for all widths considered for this dataset
% lowest test loss doesn't correspond to best test accuracy
% Regarding the point that lowest test loss does not necessarily equate to highest test accuracy, we have two papers on that! https://arxiv.org/abs/1905.05895 https://arxiv.org/abs/2104.10631


%\begin{figure}[ht]
%\centering
%\includegraphics[trim=300 264 300 264,clip,width=0.5\textwidth]{workshop_figures/finite/finite_architecture.pdf}
%  \caption{{Architecture for approximating the infinitely wide network with very wide finite layers with large $n$ (100K).}}%
%  \label{fig:finite_architecture}
%\end{figure}

\begin{figure}[ht]
\centering
  \subfloat[Training loss]{\label{fig:finite_train_err}\includegraphics[trim=30 25 455 440,clip,scale=0.45]{workshop_figures/finite/finite_epoch_3d_type3.pdf}}
  \subfloat[Test loss]{\label{fig:finite_test_err}\includegraphics[trim=520 25 0 440,clip,scale=0.45]{workshop_figures/finite/finite_epoch_3d_type3.pdf}}\\
  %\subfloat[Training error rate]{\label{fig:finite_train_err}\includegraphics[trim=30 440 455 65,clip,scale=0.45]{workshop_figures/finite/finite_epoch_3d.pdf}}
  %\subfloat[Test error rate]{\label{fig:finite_test_err}\includegraphics[trim=520 440 0 65,clip,scale=0.45]{workshop_figures/finite/finite_epoch_3d.pdf}}
  \caption{Progression of training and test losses over training steps for $n=100K$ and bottleneck sizes from 1 to 1000.}
  \label{fig:finite_epoch}
\end{figure}


\begin{figure}[ht]
\centering
\subfloat[Loss]{\label{fig:finite_loss}\includegraphics[trim=0 0 425 0,clip,scale=0.45]{workshop_figures/finite/finite_bottleneck_size_type3.pdf}}
  \subfloat[Error rate (\%)]{\label{fig:finite_error}\includegraphics[trim=425 0 0 0,clip,scale=0.45]{workshop_figures/finite/finite_bottleneck_size_type3.pdf}}
  \caption{Loss and error rate w.r.t. bottleneck size for $n=100K$ and bottleneck sizes from 1 to 9000.}%
  \label{fig:finite_d}
\end{figure}


\end{document}
% \section{Self Attention As a Bottleneck}
% The relationship between GPs and neural networks have recently been extended to networks involving self attention layers. Consider an input $\xi \in \R^{d \times d_e}$ where $d$ is the spacial dimension and $d_e$ is the feature or embedding dimension. A simplified multi head self attention layer with $m$ heads is computed with matrices $\{w^{l,q}\}_{l=1}^m, \{w^{l,k}\}_{l=1}^m, \{w^{l,v}\}_{l=1}^m, \forall_l,w^{l,k},w^{l,q},w^{l,v} \in \R^{d_e \times n}$ by computing keys queries and values $K^l(\xi) = \frac{1}{\sqrt{d_e}}\xi w^{l,k}, Q^l(\xi) = \frac{1}{\sqrt{d_e}}\xi w^{l,q}, V^l(\xi) = \frac{1}{\sqrt{d_e}}\xi w^{l,v}$. The attention layer output is then:
% \begin{align}\label{eqn:atten}
%     A^l(\xi) = \frac{K^l(\xi)Q^l(\xi)^\top}{\sqrt{n}},~~~\text{atten}(\xi) = \frac{1}{\sqrt{n}}\sum_l\psi(A^l)V^l(\xi)w^{l,p}
% \end{align}
% where $\psi$ is a row wise softmax function.  Note that the key-query product $K(\xi)Q(\xi)^\top$ has dimensions $d \times d$ which is determined by the input spacial dimension. When taking the 'width', typically referring to the embedding dimension $n$ to infinity, the key-query product remains fixed in size as noted in (XXX). The output of the attention head is then a linear projection of a finite sized stochastic matrix, hence does not approach gaussianity even at the limit. Several approaches have been proposed to solve this issue. In (XX), it was proposed to replace the $\sqrt{n}$ denominator in \cref{eqn:atten} with $n$. When $n \to \infty$, this causes the product $A(\xi) = \frac{K(\xi)Q(\xi)^\top}{n})$ to converge to a constant matrix, preserving the gaussianity of attention outputs. This however strips the model from its expressive power since the attention head itself remains frozen in its initial constant state even during training. In (XXX), this issue was resolve by considering a multi-head attention layer, and taking the number of attention heads to infinity, along with the embedding dimension. By taking both to infinity, it was shown that the gaussianity of the attention layer is preserved, with well defined limiting NN-GP and NTK kernels. \\
% In contrast, we show in the present paper that the dynamics of infinite width attention networks during training can be examined through the lens of infinite width networks with bottlenecks.
% A typical attention model is comprised of stacked attention layers, with MLPs in between. Following (XXX), we consider a simplified model of a single attention layer (with a single attention head) followed by a global average pooling (GAP), and an MLP. As before, we break the overall model into a composition of two "modules" $g(\xi): \R^{d \times d_e}\to \R^{d \times d_e}$, $f(g):\R^{d \times d_e} \to \R$:
% \begin{align}
%     g(\xi) = \psi\big(A(\xi)\big)\xi,~~~f(g) = \text{MLP}(\text{GAP}(g)),~~~\F(\xi) = (f \circ g)(\xi)
% \end{align}

% ,~~~
% \dot{g}_t' = \sum_\alpha\frac{\partial g_t'}{\partial \theta_t^\alpha}\dot{\theta}^\alpha_t \label{eqn:ode1} \\
% \dot{w}_t &= -(\frac{\partial f_t}{\partial w_t})^\top\chi_{t},~~~\dot{\theta}_t = -\sum_{\alpha=1}^d(\frac{\partial g_t^\alpha}{\partial \theta_t})^\top D^\alpha_t  \chi_t\label{eqn:ode2} 
% \end{align}
% \cref{eqn:ode1,eqn:ode2} are random as they depend on the weights of the network at each step. In (XXX), it was shown for infinite width networks (without bottlenecks) the above ODEs concentrate, and and can be fully described by the values of the initial output GP at initialization. The introduction of the bottleneck has   



% \[\label{eqn:gf}
% \dot{\tilde{\F}}_t = \frac{\partial \tilde{\F}_t}{\partial w_t}\dot{w}_t + \frac{\partial \tilde{\F}_t}{\partial \theta_t}\dot{\theta}_t,~~~\dot{w}_t = -\nabla_{w_t}\mathcal{L}_t.
% \]
% Breaking the evolution of $\F$ to those of $f,g$ separately, we can alternatively write:
% \[
% \dot{\tilde{\F}}_t = \frac{\partial \tilde{f}_t}{\partial w_t}\dot{w}_t + \sum_{\alpha=1}^N\frac{\partial \tilde{f}_t}{\partial g'_t^\alpha},\dot{g'}_t^\alpha,~~~
% \dot{g'}_t^\alpha = \frac{\partial g'_t^\alpha}{\partial \theta_t}\dot{\theta}_t 
% \]
% Under gradient flow, the weights of each module at time $t$ evolve according to:
% \[
% \dot{w}_t = -(\frac{\partial f_t}{\partial w_t})^\top\chi_{t},~~~\dot{\theta}_t = -\sum_{\alpha=1}^d(\frac{\partial g_t^\alpha}{\partial \theta_t})^\top \text{diag}(J^\alpha_t)  \chi_t
% \]
% Denote the random kernels $\Theta_t^{\alpha,\beta}(\xi_i',\xi) = (\frac{\partial g_t'^\alpha}{\partial \theta_t})(\frac{\partial g_t^\beta}{\partial \theta_t})^\top \in \R^{N_t \times N}$. The outputs $g_t',\F_t'$ then evolve as:
% \begin{align}
% \dot{g}_t'^\alpha &= -\sum_{\beta=1}^d\Theta_t^{\alpha,\beta}(\xi',\xi) \text{diag}(J^\beta_t) \chi_t \label{eqn:ev_g}\\
% \dot{\F}'_t &= -\Big[\mathcal{K}_t\big(g'_t,g_t\big)
% +\sum_{\alpha,\beta=1}^d\text{diag}(J'^\beta_t)\Theta_{t}^{\beta,\alpha}(\xi',\xi)\text{diag}(J^\alpha_t)\Big] \chi_t \label{eqn:ev_f}
% \end{align}
% The above evolutionary equations can be interpreted as Kernel equations with random, evolving kernels that depend on the weights $w,\theta$.
% \paragraph{Taking the Limit} 
% Intuitively, known results concerning the infinite width limit of the neural tangent kernel can be applied separately to modules $f$ and $g$, which result in the  concentrations:
% \[
% \lim_{n \to \infty}\Theta_t^{\alpha,\beta}(x,y) &\overset{a.s}{=}& \mathring{\Theta}(xy^\top)\mathbb{1}(\alpha = \beta)\\
% \lim_{n \to \infty} \mathcal{K}(x,y) &\overset{a.s}{=}& \mathring{\mathcal{K}}_{f}(xy^\top)
% \]
% The infinite width limit of \cref{eqn:ev_g,eqn:ev_f} is summarized in the following proposition.
% \begin{prop}\label{prop:evo}
% Given the setup in \cref{sec:toy}, we have the following concentrations in the infinite width limit:
% \begin{align}
% \lim_{n \to \infty}\dot{g}_t'^\alpha &= -\mathring{\Theta}(\xi'\xi^\top)\text{diag}(J^\alpha_t)\mathring{\chi}_t\\
% \lim_{n \to \infty}\dot{\F}'_t &= -\Big[\mathring{\mathcal{K}}\big(\mathring{g}'_t \mathring{g}_t^\top\big)
% +(J'_tJ_t^\top) \odot \mathring{\Theta}(\xi'\xi^\top)\Big] \mathring{\chi}_t
% \end{align}
% \end{prop}
% \cref{prop:evo} crucially depends on the input output Jacobian $J$. hence, to fully describe the evolution of $\F$ at any time $t$ we must reason about the evolution of $J$ under gradient flow. In the following, we show that similar to forward pre-activation units in infinite width networks, the input output Jacobian exhibits a multivariate GP behaviour with dependent coordinates at initialization, and evolve as a linear function during training.    





% Perhaps a more     
% When training, the coordinates $d_g f(g)^\alpha$ evolve nontrivially with respect to their initial values, learning data dependent features in the process. Perhaps surprisingly, this evolution is simple in a sense that it is described by a fixed kernel function


% \begin{figure*}[t]
% \centering
% %   \includegraphics[height=3.4in]{NMK_to_NTK.pdf}\hspace{1em}%
%   \subcaptionbox{\label{fig:NMK2NTKa}}{\includegraphics[height=1.7in]{loss.png}}\hspace{1em}%
%   \subcaptionbox{\label{fig:NMK2NTKb}}{\includegraphics[height=1.7in]{act.png}}
%   \subcaptionbox{\label{fig:NMK2NTKb}}{\includegraphics[height=1.7in]{evol.png}}\\
%   \caption{\textbf{Empirical verification} . We conduct experiments on the analyzed toy model with $d_0 = d = 1$ by running SGD with a learning rate $\eta=0.01$ on a random dataset with and labels with a regression task, and width $n=10000$. (a) training loss. (b) $\frac{y(x)^\top y(x)}{n}$ (red) and $\frac{z(x)^\top z(x)}{n}$ (blue) as a function of steps. (c) $\delta g_{t+1}(x)$ as a function of steps. As can be seen, theory matches practice, first module is in kernel regime, however the second module which receives the output of the first module does not evolve like a kernel.}
%   \label{fig:NMK2NTK}
% \end{figure*}

% \subsection{Deep Kernel Processes}
% Previous work...

\section{}


% \subsection{Example - Finite Bottlenecks In Infinite Width Models}
% Consider the following model comprised of two consecutive modules:
% \[
% g(\xi):\mathbb{R}^{d_0} \to \mathbb{R}^d &=& \frac{v^\top x(\xi)}{\sqrt{n}},~~~x(\xi) = \phi \big(y(\xi)\big),~~~y(\xi) = \frac{u\xi}{\sqrt{d_0}}\\
% f\big(g(\xi)\big):\mathbb{R}^d \to \mathbb{R} &=& \frac{a^\top h(\xi)}{\sqrt{n}},~~~h(\xi) = \phi \big(z(\xi)\big),~~~z(\xi) = \frac{bg(\xi)}{\sqrt{d}}.
% \]
% where $\xi \in \mathbb{R}^{d_0},~u \in \mathbb{R}^{n \times d_0}, v \in \mathbb{R}^{d \times n},~b \in \mathbb{R}^{n \times d},~a \in \mathbb{R}^{n}$, and $\phi$ is a non linearity function. When $n \to \infty$, this model represents an infinite width model with a fixed width bottleneck layer, given by the intermediate output $g(\xi) \in \mathbb{R}^d$. Note that we have used the NTK parametrization for both modules.

% \paragraph{Training Dynamics} We now consider training the model using gradient flow (GF) on a training dataset $\{\xi_i\}_{i=1}^N$, and a loss function $\mathcal{L} = \sum_i \mathcal{L}_i$. We use $\chi \in \mathbb{R}^N$ to denote the vector of loss derivatives such that the $i$'th component of $\chi$ is given by $\chi_i = \frac{\partial \mathcal{L}_i}{\partial f_i}$. Finally, we use $w = \{u,v\}, \theta = \{a,b\}$ to denote the parameters of modules $g$ and $f$ respectively. 
% We use the following notations: We use $g_i = g(\xi_i) \in \mathbb{R}^d,~ f_i = f(\xi_i) \in \mathbb{R}$. We use $g \in \mathbb{R}^{N \times d}$ to denote the concatenated intermediate outputs such that row $i$ of $g$ is given by $g_{i:}$, and $D \in \mathbb{R}^{N \times d_0}$ is the data matrix where the $i$'th column is given by $\xi_i^\top$. We denote $\mathcal{G} = gg^\top, \mathcal{D} = DD^\top$. Similarly we use $f \in \mathbb{R}^N$ to denote the concatenation of all outputs, such that the $i$'th component of $f$ is given by $f_i$. We further define $dg \in \mathbb{R}^{N \times d}$ such that:
% \[
% dg_{i:} = \frac{\partial f_i}{\partial g_{i:}},~~dg_{ij} = \frac{\partial f_i}{\partial g_{ij}}
% \]




% \subsection{Gradient Flow}
% Under GF, the weights are updated according to:
% \[
% \dot {\theta} = -\nabla_\theta \mathcal{L},~~~ \dot{w}_t = -\nabla_w \mathcal{L}
% \]
% The evolution of the outputs $g$ in the limit $n \to \infty$ is given by:
% \[
% \dot{g} = -\mathcal{K}\frac{\partial \mathcal{L}}{\partial g},~~~\dot{g}_{i:} = -\frac{\partial \mathcal{L}}{\partial g}^\top\mathcal{K}_{:i}
% \]
% where $\mathcal{K} \in \mathbb{R}^{N \times N}$ is the limit NTK given by:
% \[
% \mathcal{K}_{ij} =  \lim_{n \to \infty }\frac{1}{d}\big\langle \nabla_w g_{i:},\nabla_w g_{j:} \big \rangle.
% \]
% $\frac{\partial \mathcal{L}}{\partial g} \in \mathbb{R}^{N \times d}$ is the loss derivative with respect to $g$, and $\mathcal{K}_{:i} \in \mathbb{R}^{N}$ is the $i$'th column of $\mathcal{K}$. Note that $f_i = f_i(\theta,g_{i:})$, and so its evolution is given by:
% \[
% \dot{f}_i = \big \langle\frac{\partial f_i}{\partial \theta} , \frac{\partial \theta}{ \partial t} \big\rangle + \big \langle dg_{i:},\dot{g}_{i:} \big\rangle
% \]
% % On some test sample $\xi'$:
% % \[
% % \dot{f}(\xi') = \big \langle\frac{\partial f(\xi')}{\partial \theta} , \frac{\partial \theta}{ \partial t} \big\rangle + \big \langle dg_{i:}(\xi'),\dot{g}_{i:}(\xi') \big\rangle
% % \]
% % in the ntk limit:
% % \[
% % f(\xi) =  (k(\xi,\xi_1)...k(\xi,\xi_N))K^{-1}y
% % \]
% The first term in the above has the following limit:
% \[
% \lim_{n \to \infty }\big \langle\frac{\partial f}{\partial \theta} , \frac{\partial \theta}{ \partial t} \big\rangle = -\Theta(\mathcal{G})\chi
% \]
% where $\Theta \in \mathbb{R}^{N \times N}$ is the limit NTK of module $f(g)$. The second term can be expressed:
% \[
% \big \langle dg_{i:},\dot{g}_{i:} \big\rangle = -dg_{i:}^\top\frac{\partial \mathcal{L}}{\partial g}^\top\mathcal{K}_{i:} = - \sum_j\big\langle dg_{i:},dg_{j:}\big\rangle\mathcal{K}_{i,j}\chi_j
% \]
% Let $\Xi \in \mathbb{R}^{N \times N}$ denote a matrix such that $\Xi_{i,j} = \big\langle dg_{i:},dg_{j:}\big\rangle$. The output $f$ on all samples evolves according to:
% \[
% \dot{f} = -\Theta(\mathcal{G})\chi - \big(\Xi \odot \mathcal{K}\big) \chi &=& -\Big[\Theta(\mathcal{G}) + \Xi \odot \mathcal{K}(\mathcal{D})\Big]\chi
% \]

% Recall that $\Xi$ is a gram matrix of the error signals which evolve over time. 
% In the limit of infinite width, it holds that at initialization $dg_{ij}$ is a Gaussian Process given by:
% \[
% dg_{ij} = \frac{\partial f_i}{\partial g_{ij}} = \frac{b_{:j}^\top (\phi'(bg_{i:})\odot a)}{\sqrt{n}} = \frac{a^\top}{\sqrt{n}}\text{diag}\Big(\phi'(bg_{i:})\Big)b_{:j}
% \]
% The evolution of $dg_{ij}$ can then be expressed using:
% \[
% \nabla_{b_{:j}} dg_{ij} = \frac{(\phi'(bg_{i:})\odot a)}{\sqrt{n}},~~~\nabla_a dg_{ij} = \frac{b_{:j} \odot \phi'(bg_{i:})}{\sqrt{n}}
% \]
% On the other hand:
% \[
% \dot{b_{:j}} &=& -\nabla_{b_{:j}}\mathcal{L} = -\sum_i\chi_i g_{ij}\frac{\phi'(bg_{i:})\odot a}{\sqrt{n}}\\
% \dot{a} &=& -\nabla_{a}\mathcal{L} = -\sum_i\chi_i \sum_j g_{ij}\frac{b_{:j} \odot \phi'(bg_{i:})}{\sqrt{n}}
% \]

% so it follows:
% \[\label{Lambda_terms}
% \dot{dg_{ij}} &=& \big\langle \nabla_{b_{:j}} dg_{ij}, \dot{b_{:j}} \big\rangle + \big\langle \nabla_{a} dg_{ij}, \dot{a} \big\rangle + \big\langle \nabla_{g_{i:}}dg_{ij},\dot{g}_{i:} \big\rangle\\
% &=& -\sum_k \chi_k g_{k,j}\frac{1}{n}\big\langle \phi'(bg_{i:})\odot a,\phi'(bg_{k:})\odot a \big  \rangle\\
% &-& \sum_{k,l}\chi_k g_{k,l}\frac{1}{n}\big\langle \phi'(bg_{i:})\odot b_{:j},\phi'(bg_{k:})\odot b_{:l} \big  \rangle\\
% \]
% In the infinite width limit, the vectors $a$ and $\phi'(bg_{i:})$ have iid independent coordinates throughout training. Using LLN we can take the limit:
% \[
% \frac{1}{n}\big\langle \phi'(bg_{i:})\odot a,\phi'(bg_{k:})\odot a \big  \rangle \to \Lambda(g_{i:}^\top g_{k:})
% \]
% where $\Lambda$ is a fixed function. Evaluating the second term, i.e $\frac{1}{n}\big\langle \phi'(bg_{i:})\odot b_{:j},\phi'(bg_{k:})\odot b_{:l} \big  \rangle$ is more problematic since the vectors $\phi'(bg_{i:})$ and $b_{:j}$ are obviously dependent. Explicitly we have that:
% \[
% \frac{1}{n}\big\langle \phi'(bg_{i:})\odot b_{:j},\phi'(bg_{k:})\odot b_{:l} \big  \rangle = \frac{1}{n}\sum_{m=1}^n b_{mj}b_{ml}\phi'(\sum_{q=1}^d b_{mq}g_{iq})\phi'(\sum_{p=1}^d b_{mp}g_{kp})
% \]
% Let $z = [z_1,z_2,...,z_d] \in \mathbb{R}^d$ denote a normal Gaussian vector with iid components. The above equation simplifies in the limit to the following expectation:
% \[\label{Lambda}
% \lim_{n \to \infty}\frac{1}{n}\big\langle \phi'(bg_{i:})\odot b_{:j},\phi'(bg_{k:})\odot b_{:l} \big  \rangle = 
% \begin{cases}
% \mathbb{E}\Big[z_jz_l \phi'(z^\top g_{i:})\phi'(z^\top g_{k:})\Big], &l\neq j\\
% \mathbb{E}\Big[z_j^2 \phi'(z^\top g_{i:})\phi'(z^\top g_{k:})\Big], &l= j
% \end{cases}
% \]
% For simplicity, assume the last layer $a$ is not trained. Then:
% \[
% dg_{ij,t} = dg_{ij,0} - \sum_i\int_{s=0}^t\Lambda(g_{t},g_{i,s})\chi_{i,s}ds,~~~dg_{:j,t} = dg_{:j,0} - \int_{s=0}^t\Lambda \chi ds
% \]


% % The GF evolution equations are therefore;

% % \[
% % \dot{f} &=& -\Big[(gg^\top)\odot\Theta + (dgdg^\top) \odot \mathcal{K}\Big]\chi\\
% % \dot{g} &=& -\mathcal{K} \text{diag}(\chi) dg\\
% % \dot{dg} &=& -\Lambda \text{diag}(\chi) g + \mathcal{O}(\frac{1}{d})
% % \]
% % where $\Lambda,\Theta,\mathcal{K}$ are fixed matrices.

% % \subsection{Solving the ODE}

% % For the MLP considered with ReLU activations, the matrices $\mathcal{K},\theta,\Lambda$ can be analytically computed. Let $\xi,\xi'$ denote two training samples with $\xi^\top \xi' = \lambda$. The NTK of a depth L MLP is given by:
% % \[
% % \mathcal{K}(\xi,\xi') = \sum_{l=1}^{L+1}\Big(\Sigma^{l}(\xi,\xi')\prod_{h=l}^{L+1}\dot{\Sigma}^h(\xi,\xi'\Big)
% % \]
% % where $\Sigma,\dot{\Sigma}$ are computed recursively:
% % \[
% % \Sigma^0(\xi,\xi') &=& \lambda \in \mathbb{R}\\
% % \Lambda^l &=& \begin{bmatrix}
% %     \Sigma^{l-1}(\xi,\xi) & \Sigma^{l-1}(\xi,\xi') \\
% %     \Sigma^{l-1}(\xi',\xi) & \Sigma^{l-1}(\xi',\xi')
% % \end{bmatrix} \in \mathbb{R}^{2 \times 2}\\
% % \Sigma^l(\xi,\xi') &=& \mathbb{E}_{u,v \sim \mathcal{N}(0,\Lambda^l)}\big[\phi(u)\phi(v)\big]\\
% % \dot{\Sigma}^l(\xi,\xi') &=& \mathbb{E}_{u,v \sim \mathcal{N}(0,\Lambda^l)}\big[\dot{\phi}(u)\dot{\phi}(v)\big]
% % \]
% % where we have defined $\phi$ as $\phi = \sqrt{2}\text{ReLU}$.
% % luckily, $\phi$ defined this way we can compute the above formulas in closed form. Denote:
% % \[
% % c_1^l = \Sigma^l(\xi,\xi),~~~ c_2^l = \Sigma^l(\xi',\xi'),~~~ \lambda^l = \frac{\Sigma^l(\xi,\xi')}{\sqrt{c_1^lc_2^l}}
% % \]
% % We have the following recursive relations:
% % \[
% % c_1^{l+1} &=& c_1^l\\
% % c_2^{l+1} &=& c_2^l\\
% % \lambda^{l+1} &=& \frac{\lambda^l(\pi - \arccos(\lambda)) + 2\sqrt{1 - \lambda^2}}{\pi}\sqrt{c_1^lc_2^l}\\
% % \dot{\Sigma}^{l+1}(\xi,\xi') &=& \frac{\pi - \arccos(\lambda^l)}{\pi}
% % \]


% \subsection{Stochastic Gradient Descent}
% Under SGD, any weight $w$ at time $t$ is updated according to $w_{t+1} - w_t = -\chi_t\nabla_{w_t} f_t$. Note that from the model design as an infinitely wide model with a bottleneck, intuitively each infinite module $f(g), g(\xi)$ receives a finite sized input, and outputs a finite output. So, using known results for infinite width networks we expect both modules to evolve as a linear model given its input. However, the composition $f(\xi) = f \circ g$ should not be expected to evolve linearly. This is because, unlike coordinates of infinite layers, the coordinates of $g$ evolve non-trivially during training. However, while the network function $f \circ g$ does not evolve linearly, the training dynamics of the architecture is fully specified by fixed (3) kernels functions, after conditioning on its initial state. Specifically, we will condition on the initial function outputs $f(g): \mathbb{R}^d \to \mathbb{R}$, $g(\xi):\mathbb{R}^{d_0} \to \mathbb{R}^d$ and $\frac{\partial f(g)}{\partial g}: \mathbb{R}^d \to \mathbb{R}^d$. \\
% \paragraph{notations} For any data dependent tensor $\bullet$ we denote $\bullet_t(\xi)$ to denote its value at step $t$ of SGD given sample $\xi$. WLOG we assume the SGD algorithm uses a batchsize of 1 and a learning rate of $1$, where $\xi_t$ denotes the training sample fed to SGD at time $t$. For any data dependent tensor $\bullet$ we omit the explicit dependency on data when it is implied by the time stamp (i.e $\bullet_t = \bullet_t(\xi_t)$). We will keep track of how the both modules $f,g$ evolve over the course of SGD on some arbitrary sample $\tilde{\xi}$. For every data dependent tensor $\bullet$ we denote $\tilde{\bullet}_t = \bullet_t(\tilde{\xi})$. We use a superscript $\alpha$ to denote the $\alpha$ component of a vector. Finally, we denote by:
% \[
% dg(\xi) = \frac{\partial f(g)}{\partial g}
% \]
% As with all other data dependent tensors, we have $dg_t = dg_t(\xi_t), dg'_t = dg_t(\tilde{\xi})$.
% We denote by $\theta_g$ the parameters of module $g$ (i.e $u,v$), and $\theta_f$ for the parameters of module $f$ (i.e $a,b$). As before, $\chi_t = \frac{\partial \mathcal{L}_t}{\partial f_t}$.  Informally, at each step $t$ each module evolves linearly given its input. We have that:
% \[
% g'_{t+1}^\alpha- g'_t^\alpha &=& -\chi_tdg_t^\alpha\Theta_g(\xi_t,\tilde{\xi})\\
% \tilde{f}_{t+1} - \tilde{f}_t &=& f_{t+1}(g'_{t+1}) - f_t(g'_t)\\
%  &=& f_{t+1}(g'_{t+1}) - f_t(g'_{t+1}) + f_t(g'_{t+1}) - f_t(g'_t)\\
%  f_{t+1}(g'_{t+1}) - f_t(g'_{t+1}) &=& -\chi_t\Theta_f(g_{t+1},g'_{t+1}) \\
%  f_t(g'_{t+1}) - f_t(g'_t) &=& f_0(g'_{t+1}) - f_0(g'_t) -\sum_{s=0}^{t-1}\chi_s\Big(\Theta_f(g'_{t+1},g_s) - \Theta_f(g'_t,g_s)\Big)
% \]
% where $\Theta_g,\Theta_f$ are the NTK functions of modules $g,f$ respectively.
% We are left with evaluating how $dg_t$ changes through time. Luckily, in the case where $f(g)$ is implemented as an infinite width MLP, $dg_t$ itself evolves linearly. To see this, recall $h = \frac{bg}{\sqrt{d}}$, and note that:
% \[
% dg^\alpha = \frac{\partial f(g)}{\partial g^\alpha} = \frac{b_{:\alpha}^\top (\phi'(h)\odot a)}{\sqrt{nd}} = \frac{b_{:\alpha}^\top (\phi'(\frac{1}{\sqrt{d}}\sum_\beta b_{:\beta}g^\beta)\odot a)}{\sqrt{nd}}
% \]
% Recall that:
% \[
% \nabla_b f = \frac{(\phi'(h)\odot a)g^\top}{\sqrt{nd}},~~~\nabla_a f = \frac{\phi(h)}{\sqrt{n}}
% \]

% In addition:
% \[
% \frac{\partial dg^\alpha}{\partial b_{:\alpha}} &=& \frac{(\phi'(h)\odot a)}{\sqrt{nd}} + g^\alpha\frac{b_{:\alpha}\odot \phi''(h)\odot a}{\sqrt{n}d}\\
% \forall_{\alpha\neq \beta},~~~\frac{\partial dg^\alpha}{\partial b_{:\beta}} &=& g^\beta\frac{b_{:\alpha}\odot \phi''(h)\odot a}{\sqrt{n}d}\\
% \frac{\partial dg^\alpha}{\partial a} &=& \frac{b_{:\alpha}\odot \phi'(h)}{\sqrt{nd}}
% \]

% For ReLU activations, we have that $\phi'' = 0$ almost everywhere, and $\phi(h) = h\odot \phi'(h)$. Hence, we have that for $\phi = \text{ReLU}$:
% \[
% \big\langle \nabla_b f(\hat{g}), \nabla_b dg^\alpha  \big\rangle &=& \frac{g^\alpha}{nd}\big\langle \phi'(\hat{h})\odot a, \phi'(h)\odot a \big\rangle\\
% \big\langle \nabla_a f(\hat{g}), \nabla_adg^\alpha  \big\rangle &=& \frac{1}{nd}\sum_\beta \hat{g}^\beta\big\langle b_{:\beta}\odot\phi'(\hat{h}) ,b_{:\alpha}\odot \phi'(h)\big\rangle
% \]
% In the limit, the first term converges to a known fixed function:
% \[
% \lim_{n \to \infty}\big\langle \nabla_{b_{:\alpha}} f(\hat{g}), \nabla_{b_{:\alpha}}dg^\alpha  \big\rangle = \Lambda(h,\hat{h})g^\alpha
% \]

% The second term can be computed as follows. Let $z = [z_1,z_2,...,z_d] \in \mathbb{R}^d$ denote a normal Gaussian vector with iid components. We have in the limit:
% \[\label{Lambda}
% \lim_{n \to \infty}\frac{1}{n}\big\langle b_{:\beta}\odot\phi'(\hat{h}) ,b_{:\alpha}\odot \phi'(h)\big\rangle = 
% \begin{cases}
% \mathbb{E}\Big[z_\alpha z_\beta \phi'(z^\top g)\phi'(z^\top \hat{g})\Big], &\alpha \neq \beta\\
% \mathbb{E}\Big[z_\alpha^2 \phi'(z^\top g)\phi'(z^\top \hat{g})\Big], &\alpha=\beta
% \end{cases}
% \]

% Hence, we conclude that:
% \[
% \lim_{n \to \infty}\Big[\big\langle \nabla_b f(\hat{g}), \nabla_b dg^\alpha  \big\rangle + \big\langle \nabla_a f(\hat{g}), \nabla_adg^\alpha  \big\rangle\Big] = \Xi(g,\hat{g})
% \]
% where $\Xi:\mathbb{R}^2 \to \mathbb{R}$ is some fixed kernel function. To simplify the rest of this analysis, we assume the last layer $a$ is not trained, and so $\Xi = \Lambda$.\\
% the final update equations are given by:
% \[\label{eqn:dynamics}
% g'_{t+1}^\alpha &=& g'_0^\alpha -\sum_{s=0}^t\chi_sdg_s^\alpha\Theta_g(\xi_s,\tilde{\xi})\\
% f_{t+1}(g'_{t+1}) &=& f_{0}(g'_{t+1}) -\sum_{s=0}^{t}\chi_s\Theta_f(g'_{t+1},g_s)\\
% dg_t^\alpha &=& dg_0(g_t)^\alpha - \sum_{s=0}^{t-1}\chi_s\Lambda(g_t,g_s)g_s^\alpha 
% \]


% \subsection{Batch training}

% For full batch training, we have the following set of equations (abusing notations):
% \[
% g'_{t+1}^\alpha - g'_{t}^\alpha &=&  -\Theta_g(\tilde{\xi}^\top \xi) (\chi_t \odot dg_t^\alpha)\\
% f_{t+1}(g'_{t+1}) &=& f_{0}(g'_{t+1}) -\sum_{s=0}^{t}\Theta_f(g'_{t+1}^\top g_s)\chi_s\\
% dg_t^\alpha &=& dg_0(g_t)^\alpha - \sum_{s=0}^{t-1}\Lambda(g_t^\top g_s)(\chi_s \odot g_s^\alpha) 
% \]


% Note that the evolution depends only on deterministic functions, and GP values as initialization $f_0,g_0,dg_0$. In other words, the evolution is completely deterministic given the initial GP values, which are random. 

% \subsection{Computing the difference equations}

% For the MLP considered with ReLU activations, the matrices $\mathcal{K},\theta,\Lambda$ can be analytically computed. Let $\xi,\xi'$ denote two training samples with $\xi^\top \xi' = \lambda$. The NTK of a depth L MLP is given by:
% \[
% \Theta(\xi,\xi') = \sum_{l=1}^{L+1}\Big(\Sigma^{l}(\xi,\xi')\prod_{h=l}^{L+1}\dot{\Sigma}^h(\xi,\xi'\Big)
% \]
% where $\Sigma,\dot{\Sigma}$ are computed recursively:
% \[
% \Sigma^0(\xi,\xi') &=& \lambda \in \mathbb{R}\\
% \Lambda^l &=& \begin{bmatrix}
%     \Sigma^{l-1}(\xi,\xi) & \Sigma^{l-1}(\xi,\xi') \\
%     \Sigma^{l-1}(\xi',\xi) & \Sigma^{l-1}(\xi',\xi')
% \end{bmatrix} \in \mathbb{R}^{2 \times 2}\\
% \Sigma^l(\xi,\xi') &=& \mathbb{E}_{u,v \sim \mathcal{N}(0,\Lambda^l)}\big[\phi(u)\phi(v)\big]\\
% \dot{\Sigma}^l(\xi,\xi') &=& \mathbb{E}_{u,v \sim \mathcal{N}(0,\Lambda^l)}\big[\dot{\phi}(u)\dot{\phi}(v)\big]
% \]

% luckily, $\phi$ defined this way we can compute the above formulas in closed form. Denote:
% \[
% c_1^l = \Sigma^l(\xi,\xi),~~~ c_2^l = \Sigma^l(\xi',\xi'),~~~ \lambda^l = \frac{\Sigma^l(\xi,\xi')}{\sqrt{c_1^lc_2^l}}
% \]
% We have the following recursive relations:
% \[
% c_1^{l+1} &=& c_1^l\\
% c_2^{l+1} &=& c_2^l\\
% \lambda^{l+1} &=& \frac{\lambda^l(\pi - \arccos(\lambda)) + \sqrt{1 - \lambda^2}}{2\pi}\sqrt{c_1^lc_2^l}\\
% \dot{\Sigma}^{l+1}(\xi,\xi') &=& \frac{\pi - \arccos(\lambda^l)}{2\pi}
% \]



% % The evolution on the training set in matrix form can be written as:
% % \[
% % g_{t+1}^\alpha- g_t^\alpha &=& -\Theta_g(DD^\top) (\chi_t\odot dg_t^\alpha)\\
% % f_{t+1} - f_t &=& -\Theta_f(\mathcal{G}_{t+1})\chi_t + f_0(g_{t+1}) - f_0(g_t) - \sum_{s=0}^{t-1}\Big(\Theta_f(g_{t+1}g_s^\top) - \Theta_f(g_tg_s^\top)\Big)\chi_s\\
% % dg_t^\alpha &=& dg_0^\alpha(g_t) - \sum_{s=0}^{t-1}\Lambda(g_tg_s^\top)\chi_s
% % \]

% % \subsection{Algorithm for computing the limit function}

% % To run SGD on th infinite width model with a finite bottleneck exactly, we may run the following algorithm:
% % \begin{enumerate}
% % \item For every sample $\xi$ (including test samples $\tilde{\xi}$) sample $g(\xi),f(g),dg$ from the corresponding multivariate GPs.
% % \item Sample training sample $\xi_0$, and compute $\chi_0$
% % \item compute $g'_{1}^\alpha- g'_0^\alpha &=& -\chi_0dg_0^\alpha\Theta_g(\xi_0,\tilde{\xi})$ for every $\alpha$.
% % \item 
% % \end{enumerate}










% \subsection{Convergence To Zero Training loss guaranteed?}
% Intuitively, if the minimum eigenvalue of $\mathcal{M} = \Theta(\mathcal{G}) + \Xi \odot \mathcal{K}(\mathcal{D})$ is greater then some $\epsilon >0$ throughout training, then $\|\dot{f}\|=0$ iff $\|\chi\|=0$, implying convergence to zero training loss. The important thing to notice is that if $g$ is not degenerate throughout training then $\Theta(\mathcal{G})$ "should" be positive definite (known property of the NTK). Under some mild assumptions this should hold.

% % The output $f^T$ at time $T$ conditioned on the initial outputs $f^0$ is given by the integral:
% % \[
% % f^T - f^0 = -\int_{t=0}^T\Theta(\mathcal{G}_t)\chi_tdt -  \int_{t=0}^T\big(\mathcal{K}(\mathcal{D})\odot \Xi_t\big)\chi_t dt.
% % \]
% % where we use $\odot$ in the above to denote element wise multiplication between matrices. Note that the evolution of $f$ depends on fixed kernel functions $\mathcal{K},\Theta$, while also involves feature learning through the evolution of $\mathcal{G}$ and $\Xi$.
% % Further, note that $\Theta(\mathcal{G})$, $\Xi$ and $\mathcal{K}(\mathcal{D})$ are PSD matrices. Hence, $\Xi \odot \mathcal{K}(\mathcal{D})$ is PSD (hadamard product of PSD matrices is PSD). 

% % \subsection{Training As Inference Of A Deep Kernel Process}
% % We now draw equivalence between training of the model and inference in a deep kernel process under some mild assumptions. More specifically, we show that any collection $\mathcal{W}_{t_1},\mathcal{W}_{t_2},...,\mathcal{W}_{t_T}|t_1<t_2<...<t_T$ is a deep wishart process. In the following analysis we assume $n = \infty$

% \paragraph{Initialization} At initialization, the columns $g_{:,i} \in \mathbb{R}^N$ are Gaussian vectors sampled from a GP:
% \[
% g_{:,i} \sim \mathcal{N}(\bf{0},\Sigma)
% \]
% where $\Sigma \in \mathbb{R}^{N\times N}$ is the covariance matrix given by the NNGP kernel. Therefore, the Gram matrix $\mathcal{G}$ is distributed according to the wishart distribution:
% \[
% \mathcal{G} = \sum_{i=1}^Ng_{:,i}g_{:,i}^\top \sim \mathcal{W}(\Sigma,N)
% \]
% where (assuming $N>d-1$):
% \[
% \mathcal{W}(\mathcal{G};\Sigma,N) = \frac{1}{2^{Nd}|\Sigma|\Gamma_d\frac{N}{2}}|\mathcal{G}|^{\frac{N-d-1}{2}}\exp\Big[{-\frac{1}{2}\text{trace}(\Sigma^{-1}\mathcal{G})}\Big]
% \]
% where $\Gamma_d$ is the multivariate gamma function. We now show how $\mathcal{G}$ evolves through time.




% \subsection{Generalization to Multiple Modules}
% We now assume our model is comprised of $m$ modules with parameters $w_m$, with $m-1$ bottlenecks of dimension $d$. In that case, it is straightforward to show that:
% \[
% \dot{f} =  - \Big[\Theta_m(\mathcal{G}_{m-1}) + \sum_{l=1}^{m-1}\Xi_l \odot \Theta_l(\mathcal{G}_{l-1})\Big]\chi
% \]
% here, $\{\Theta_l\}_{l=1}^m$ are fixed kernel functions, $\{\mathcal{G}_l\}_{l=1}^m$ are the Gram matrices of the bottleneck representations, and $\{\Xi_l\}_{l=1}^{m-1}$ are the Gram matrices of the error signals. As can be seen, the evolution only depends on the the bottleneck activations and error signals.








% \subsection{Wide Transformers}
% In this part we consider models comprised of stacked self attention modules. Consider an embedding $x \in \mathbb{R}^{d_s\times n}$ where $d_s$ is a spatial dimension, and $n$ is the feature embedding dimension. A simple self attention module is computed using weight matrices $w^v \in \mathbb{R}^{n\times n},w^q\in \mathbb{R}^{n\times n},w^k\in \mathbb{R}^{n\times n}$ as follows:
% \[
% Q(x) &=& \frac{1}{\sqrt{n}}xw^q,~~~K(x) = \frac{1}{\sqrt{n}}xw^k,~~~V(x) = \frac{1}{\sqrt{n}}xw^v\\
% h(x) &=& \psi(\frac{K(x)Q(x)^\top}{\sqrt{n}})V(x)
% \]
% We begin by analyzing a simple model with a single attention head. Let $u \in \mathbb{R}^{d \times n},v \in \mathbb{R}^{n}$ denote the first and last layers weights. Given an input $\xi \in \mathbb{R}^{d_s\times d}$, our model is given by:
% \[
% x(\xi) = \frac{1}{\sqrt{d}}\xi u \in \mathbb{R}^{d_s \times n},~~~a(\xi) &=& \frac{K(\xi)Q(\xi)^\top}{\sqrt{n}} \in \mathbb{R}^{d_s \times d_s},~~~z(\xi) = \psi(a)  \in \mathbb{R}^{d_s \times d_s}\\
% h(\xi) &=& z(\xi)V(\xi) \in \mathbb{R}^{d_s \times n},~~~f = \frac{1}{\sqrt{n}}h(\xi)v \in \mathbb{R}^{d_s}.
% \]
% For any matrix $\bullet \in \mathbb{R}^{d_1\times d_2}$, we denote its rows and columns $\bullet_{\alpha:} \in \mathbb{R}^{1\times d_2},\bullet_{:\alpha} \in \mathbb{R}^{d_1\times 1}$.
% We now analyze how each component $f_\alpha$ evolves over time. To that end, we reduce the above equations only to the relevant tensors that influence $f_\alpha$. Namely:
% \[
% f_\alpha &=& \frac{1}{\sqrt{n}}h_{\alpha:} v,~~~h_{\alpha:} = z_{\alpha:}V,~~~z_{\alpha:} = \psi(a_{\alpha:}),~~~a_{\alpha:} = \frac{K_{\alpha:}Q^\top}{\sqrt{n}}\\
% K_\alpha &=& \frac{1}{\sqrt{n}}x_{\alpha:}w^k,~~~x_{\alpha:} = \xi_{\alpha:} u
% \]
% Hence:
% \[
% f_\alpha = \frac{1}{n}z_{\alpha:}xw^vv = \frac{1}{n}\psi(a_{\alpha:})xw^vv = \frac{1}{n}\psi(\frac{K_{\alpha:}Q^\top}{\sqrt{n}})xw^vv = \frac{1}{n}\psi(\frac{x_{\alpha:}w^kw^{q\top}x^\top}{n\sqrt{n}})xw^vv
% \]

% Let $\psi'(x) \in \mathbb{R}^{d_s \times d_s}$ denote the derivative of $\psi$ such that $\psi'(x)_{\alpha\beta} = \frac{\partial \psi(x)_\alpha}{\partial x_\beta}$. We have the following identities:
% \[
% \frac{\partial f_\alpha}{\partial w^v} &=& \frac{1}{\sqrt{n}}x^\top z_{\alpha:} \frac{v^\top}{\sqrt{n}},~~~\frac{\partial f_\alpha}{\partial a_{\alpha\beta}}  =  \psi'(a_{\alpha:})_{:\beta}^\top V\frac{v}{\sqrt{n}}\\
% \frac{\partial a_{\alpha\beta}}{\partial K_{\alpha\gamma}} &=& \frac{1}{\sqrt{n}}Q_{\beta\gamma},~~~\frac{\partial K_{\alpha\gamma}}{\partial w^k_{:\gamma}} = \frac{1}{\sqrt{n}}x_{\alpha:}^\top\\
% \frac{\partial a_{\alpha\beta}}{\partial Q_{\beta\gamma}} &=& \frac{1}{\sqrt{n}}K_{\alpha\gamma},~~~\frac{\partial Q_{\beta\gamma}}{\partial w^q_{:\gamma}} = \frac{1}{\sqrt{n}}x_{\beta:}^\top
% \]
% Hence:
% \[
% \frac{\partial a_{\alpha\beta}}{\partial w^k} &=& \frac{1}{n}Q_{\beta:}x_{\alpha:}^\top,~~~\frac{\partial a_{\alpha\beta}}{\partial w^q} = \frac{1}{n}K_{\alpha:}x_{\beta:}^\top
% \]
% Finally:
% \[
% \frac{\partial f_\alpha}{\partial w^k} &=& \frac{1}{n}\sum_\beta \big(\psi'(a_{\alpha:})_{:\beta}^\top V\frac{v}{\sqrt{n}}\big) Q_{\beta:}x_{\alpha:}^\top\\
% \frac{\partial f_\alpha}{\partial w^q} &=& \frac{1}{n}\sum_\beta \big(\psi'(a_{\alpha:})_{:\beta}^\top V\frac{v}{\sqrt{n}}\big) K_{\alpha:}x_{\beta:}^\top
% \]

% We now add the following notation: For each data dependent tensor $\bullet$, we use superscript to denote a dependency on a specific sample $\bullet^i = \bullet(\xi^i)$. As before, a data dependent tensor without a superscript implies dependency on a generic sample. In addition, we use $da_{\alpha\beta} = \frac{\partial f_\alpha}{\partial a_{\alpha\beta}}$. It follows:
% \[
% \big\langle \frac{\partial f^i_\alpha}{\partial w^k},\frac{\partial f_\alpha}{\partial w^k} \big\rangle &=& \sum_{\beta,\gamma}da^i_{\alpha\beta}da_{\alpha\gamma}\frac{x^{i\top}_{\alpha:} x_{\alpha:}}{n}\frac{Q^{i\top}_{\beta:} Q_{\gamma:}}{n}\\
% \big\langle \frac{\partial f^i_\alpha}{\partial w^q},\frac{\partial f_\alpha}{\partial w^q} \big\rangle &=& \sum_{\beta,\gamma}da^i_{\alpha\beta}da_{\alpha\gamma}\frac{x^{i\top}_{\beta:} x_{\gamma:}}{n}\frac{K^{i\top}_{\alpha:} K_{\alpha:}}{n}
% \]
% In the infinite width limit, we have the following concentrations:
% \[
% \lim_{n \to \infty} \frac{x^{i\top}_{\alpha:} x_{\beta:}^\top}{n} &=& \lim_{n \to \infty}\frac{\xi^{i\top}_{\alpha:}uu^\top \xi_{\beta:}}{n} =  \xi^{i\top}_{\alpha:}\xi_{\beta:}\\
% \lim_{n \to \infty} \frac{Q^{i\top}_{\alpha:} Q_{\beta:}}{n} &=&   \lim_{n \to \infty}\frac{x^{i\top}_{\alpha:}w^qw^{q\top}x_{\beta:}}{n^2} = \lim_{n \to \infty} \frac{x^{i\top}_{\alpha:} x_{\beta:}}{n} = \xi^{i\top}_{\alpha:}\xi_{\beta:} \\
% \lim_{n \to \infty} \frac{K^{i\top}_{\alpha:} K_{\beta:}}{n} &=& \xi^{i\top}_{\alpha:}\xi_{\beta:}
% \]
% % and so:
% % \[
% % \lim_{n \to \infty} \Big[\big\langle \frac{\partial f^i_\alpha}{\partial w^k},\frac{\partial f_\alpha}{\partial w^k} \big\rangle + \big\langle \frac{\partial f^i_\alpha}{\partial w^q},\frac{\partial f_\alpha}{\partial w^q} \big\rangle\Big] &=& \sum_{\beta\gamma}da^i_{\alpha\beta}da_{\alpha\gamma} \xi^{i\top}_{\alpha:}\xi_{\alpha:}\xi^{i\top}_{\beta:}\xi_{\gamma:}\\
% % &=& da^{i\top}_{\alpha:}\xi^i\xi^\top da_{\alpha:}\xi^{i\top}_{\alpha:}\xi_{\alpha:}
% % \]
% In addition:
% \[
% \frac{\partial f_\alpha}{\partial u} &=& \sum_{\beta}da_{\alpha\beta}\frac{\partial a_{\alpha\beta}}{\partial u} = \sum_{\beta\gamma}da_{\alpha\beta}\frac{1}{\sqrt{n}}\frac{\partial K_{\alpha\gamma} Q_{\beta\gamma}}{\partial u}\\
% \frac{\partial K_{\alpha\gamma}}{\partial u} &=&  \frac{1}{\sqrt{dn}}\xi_{\alpha:}w_{:\gamma}^{k\top}\\
% \frac{\partial Q_{\beta\gamma}}{\partial u} &=&  \frac{1}{\sqrt{dn}}\xi_{\beta:}w_{:\gamma}^{q\top}\\
% \frac{\partial f_\alpha}{\partial u} &=& \frac{1}{n\sqrt{d}}\sum_{\beta\gamma}da_{\alpha\beta}\Big[\xi_{\alpha:}w_{:\gamma}^{k\top}Q_{\beta\gamma} + \xi_{\beta:}w_{:\gamma}^{q\top}K_{\alpha\gamma}\Big]
% \]
% It follows:
% \[
% \lim_{n \to \infty}\big\langle \frac{\partial f^i_\alpha}{\partial u}, \frac{\partial f_\alpha}{\partial u} \big\rangle = 2\sum_{\beta\gamma}da^i_{\alpha\beta}da_{\alpha\gamma}\xi^{i\top}_{\alpha:}\xi_{\alpha:}\xi^{i\top}_{\beta:}\xi_{\gamma:} 
% \]
% Finally:
% \[
% \lim_{n \to \infty} \Big[\big\langle \frac{\partial f^i_\alpha}{\partial w^k},\frac{\partial f_\alpha}{\partial w^k} \big\rangle + \big\langle \frac{\partial f^i_\alpha}{\partial w^q},\frac{\partial f_\alpha}{\partial w^q} \big\rangle + \big\langle \frac{\partial f^i_\alpha}{\partial u}, \frac{\partial f_\alpha}{\partial u} \big\rangle\Big] &=& 4\sum_{\beta\gamma}da^i_{\alpha\beta}da_{\alpha\gamma}\xi^{i\top}_{\alpha:}\xi_{\alpha:}\xi^{i\top}_{\beta:}\xi_{\gamma:}\\
% &=& 4da^{i\top}_{\alpha:} \xi^i\xi^\top da_{\alpha:}\xi^{i\top}_{\alpha:}\xi_{\alpha:}
% \]

% likewise it can be easily shown that:
% \[
% \lim_{n \to \infty }\big\langle \frac{\partial f^i_\alpha}{\partial w^v},\frac{\partial f_\alpha}{\partial w^v} \big\rangle &=& \lim_{n \to \infty }\big\langle \frac{\partial f^i_\alpha}{\partial v},\frac{\partial f_\alpha}{\partial v} \big\rangle =  z^{i\top}_{\alpha:}\xi^i\xi^\top z_{\alpha:}
% \]

% Finally:
% \[
% \dot{f}_\alpha = -2\sum_{i}\Big[2da^{i\top}_{\alpha:} \xi^i\xi^\top da_{\alpha:}\xi^{i\top}_{\alpha:}\xi_{\alpha:} + \psi(a^i_{\alpha:})^\top\xi^i\xi^\top \psi(a_{\alpha:})\Big]
% \]


% \subsection{Deep Self Attention Models}

% We now analyze deep self attention models. This can be done by analyzing how the outputs of a single self attention head evolves during training. Assume $a(x) = \frac{K(x)Q(x)^\top}{\sqrt{n}}$ where $x \in \mathbb{R}^{d_s\times n} = \phi(\xi;w_\phi)$, where $\phi$ is some infinite "module" with parameters $w_\phi$. We assume the output of the network is a scalar $f(\xi) \in \mathbb{R}$, and we denote $\frac{\partial f}{\partial a_{\alpha \beta}} = da_{\alpha\beta}$. Again, a superscript $i$ denotes dependency on input (input to the module) $\xi^i$. We have:
% \[
% \dot{a}_{\alpha\beta} &=&  \big\langle \nabla_{w_\phi} a_{\alpha\beta}, \dot{w}_t_\phi \big\rangle + \big\langle \nabla_{w^k} a_{\alpha\beta}, \dot{w}_t^k \big\rangle + \big\langle \nabla_{w^q} a_{\alpha\beta}, \dot{w}_t^q \big\rangle = A + B + C\\
% A &=& \sum_{i,\gamma,\delta,\mu,\nu,u,v} \frac{da^i_{uv}}{n}\big\langle \frac{\partial (x_{\alpha\mu}x_{\beta\nu})}{\partial w_\phi}\frac{\partial x_{\alpha:}^\top w^kw^{q\top}x_{\beta:}}{n\partial (x_{\alpha\mu}x_{\beta\nu})}, \frac{\partial (x^i_{u\gamma}x^i_{v\delta})}{\partial w_\phi}\frac{\partial x_{u:}^{i\top}w^kw^{q\top}x_{v:}^i}{n\partial (x^i_{u\gamma}x^i_{v\delta})} \big\rangle\\
% &=& \sum_{i,\gamma,\delta,\mu,\nu,u,v}da^i_{uv}\big\langle \frac{\partial (x_{\alpha\mu}x_{\beta\nu})}{\sqrt{n}\partial w_\phi}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{n}, \frac{\partial (x^i_{u\gamma}x^i_{v\delta})}{\sqrt{n}\partial w_\phi}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{n} \big\rangle\\
% &=& \frac{1}{n}\sum_{i,\gamma,\delta,\mu,\nu,u,v}da^i_{uv}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{n}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{n} \big\langle \frac{\partial (x_{\alpha\mu}x_{\beta\nu})}{\partial w_\phi}, \frac{\partial (x^i_{u\gamma}x^i_{v\delta})}{\partial w_\phi}\big\rangle\\
% &=&\frac{1}{n^2}\sum_{i,\gamma,\delta,\mu,\nu,u,v}da^i_{uv}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} \big\langle \frac{\partial (x_{\alpha\mu}x_{\beta\nu})}{\partial w_\phi}, \frac{\partial (x^i_{u\gamma}x^i_{v\delta})}{\partial w_\phi}\big\rangle
% % &=& \sum_{i,\gamma,\delta,\mu,\nu,u,v} da^i_{uv}\big\langle \frac{\partial (xx^\top)_{\gamma\delta}}{\sqrt{n}\partial w_\phi}\frac{w^k_{\alpha\gamma}w^q_{\beta\delta}}{n}, \frac{\partial (x^ix^{i\top})_{\mu\nu}}{\sqrt{n}\partial w_\phi}\frac{w^k_{u\mu}w^q_{v\nu}}{n} \big\rangle
% \]

% It follows:
% \[
% \big\langle \frac{\partial (x_{\alpha\mu}x_{\beta\nu})}{\partial w_\phi}, \frac{\partial (x^i_{u\gamma}x^i_{v\delta})}{\partial w_\phi}\big\rangle &=& \big\langle x_{\beta\nu}\frac{\partial x_{\alpha\mu}}{\partial w_\phi} + x_{\alpha\mu}\frac{\partial x_{\beta\nu}}{\partial w_\phi}, x^i_{u\gamma}\frac{\partial x^i_{v\delta}}{\partial w_\phi} + x^i_{v\delta}\frac{\partial x^i_{u\gamma}}{\partial w_\phi}\big\rangle\\
% &=& x_{\beta\nu}x^i_{u\gamma}\big\langle \frac{\partial x_{\alpha\mu}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle + x_{\beta\nu}x^i_{v\delta}\big\langle \frac{\partial x_{\alpha\mu}}{\partial w_\phi}, \frac{\partial x^i_{u\gamma}}{\partial w_\phi}\big\rangle\\
% &+& x_{\alpha\mu}x^i_{u\gamma}\big\langle \frac{\partial x_{\beta\nu}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle + x_{\alpha\mu}x^i_{v\delta}\big\langle \frac{\partial x_{\beta\nu}}{\partial w_\phi}, \frac{\partial x^i_{u\gamma}}{\partial w_\phi}\big\rangle
% \]

% Recall that in the infinite width limit $x_{\alpha\beta}$ is a GP, independent across $\beta$, and:
% \[
% \forall_{\alpha,\beta,\gamma,\delta|\beta = \delta}\lim_{n \to \infty} \big\langle \frac{\partial x_{\alpha\beta}}{\partial w_\phi}, \frac{\partial x^i_{\gamma\delta}}{\partial w_\phi}\big\rangle &=& \mathcal{O}(1)\\
% \forall_{\alpha,\beta,\gamma,\delta|\beta \neq \delta}\lim_{n \to \infty} \big\langle \frac{\partial x_{\alpha\beta}}{\partial w_\phi}, \frac{\partial x^i_{\gamma\delta}}{\partial w_\phi}\big\rangle &=& 0
% \]

% therefore, we have the following decomposition:
% \[
% &&\lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\mu,\nu}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\beta\nu}x^i_{u\gamma}\big\langle \frac{\partial x_{\alpha\mu}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &=& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\nu}\frac{w^{k\top}_{\delta:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\beta\nu}x^i_{u\gamma}\big\langle \frac{\partial x_{\alpha\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &=& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\delta,\nu\neq \gamma}\frac{w^{k\top}_{\delta:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\beta\nu}x^i_{u\gamma}\big\langle \frac{\partial x_{\alpha\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &+& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\delta,\nu =  \gamma}\frac{w^{k\top}_{\delta:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\beta\nu}x^i_{u\gamma}\big\langle \frac{\partial x_{\alpha\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &=& 0
% \]

% Similarly:
% \[
% &&\lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\mu,\nu}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\alpha\mu}x^i_{u\gamma}\big\langle \frac{\partial x_{\beta\nu}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &=& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\mu}\frac{w^{k\top}_{\mu:}w^q_{\delta:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\alpha\mu}x^i_{u\gamma}\big\langle \frac{\partial x_{\beta\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &=& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\delta,\mu\neq \gamma}\frac{w^{k\top}_{\mu:}w^q_{\delta:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\alpha\mu}x^i_{u\gamma}\big\langle \frac{\partial x_{\beta\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle\\
% &+& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\delta,\mu =  \gamma}\frac{w^{k\top}_{\mu:}w^q_{\delta:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}} x_{\alpha\mu}x^i_{u\gamma}\big\langle \frac{\partial x_{\beta\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle
% \]

% Using LLN, only the second term is non zero:
% \[
% E &=& \lim_{n \to \infty}\sum_{\mu , \delta}\frac{\frac{(w^{k\top}_{\mu:}w^q_{\delta:})^2}{n}x_{\alpha\mu}x^i_{u\mu}\big\langle \frac{\partial x_{\beta\delta}}{\partial w_\phi}, \frac{\partial x^i_{v\delta}}{\partial w_\phi}\big\rangle}{n^2} = \Sigma_{\alpha u}(x,x^i)\Theta_{\beta v}(x,x^i)
% \]

% Similarly:

% \[
% &&\lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\mu,\nu}\frac{w^{k\top}_{\mu:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}}x_{\beta\nu}x^i_{v\delta}\big\langle \frac{\partial x_{\alpha\mu}}{\partial w_\phi}, \frac{\partial x^i_{u\gamma}}{\partial w_\phi}\big\rangle\\
% &=& \lim_{n \to \infty}\frac{1}{n^2}\sum_{\gamma,\delta,\nu}\frac{w^{k\top}_{\gamma:}w^q_{\nu:}}{\sqrt{n}}\frac{w^{k\top}_{\gamma:}w^q_{\delta:}}{\sqrt{n}}x_{\beta\nu}x^i_{v\delta}\big\langle \frac{\partial x_{\alpha\gamma}}{\partial w_\phi}, \frac{\partial x^i_{u\gamma}}{\partial w_\phi}\big\rangle\\
% &=& \Sigma_{\beta v}(x,x^i)\Theta_{\alpha u}(x,x^i)
% \]

% Finally:
% \[
% A = \sum_{i,u,v}da^i_{uv}\Big[\Sigma_{\alpha u}(x,x^i)\Theta_{\beta v}(x,x^i) + \Sigma_{\beta v}(x,x^i)\Theta_{\alpha u}(x,x^i)\Big]
% \]

% Not to compute the limit of $B$:
% \[
% \big\langle \nabla_{w^k} a_{\alpha\beta}, \dot{w}_t^k \big\rangle &=& -\sum_{i,u,v}\frac{da^i_{uv}}{n}\big\langle \frac{\partial x_{\alpha:}w^kw^{q\top}x_{\beta:}}{n\partial w^k} , \frac{\partial x^i_{u:}w^kw^{q\top}x^i_{v:}}{n\partial w^k}\big\rangle\\
% &=& -\sum_{i,u,v}da^i_{uv}\frac{x_{\alpha:}^\top x^i_{u:}}{n}\frac{x_{\beta:}^\top w^{q\top}w^q x_{v:}}{n^2}\\
% \lim_{n \to \infty}\big\langle \nabla_{w^k} a_{\alpha\beta}, \dot{w}_t^k \big\rangle &=& -\sum_{i,u,v}da^i_{uv}\Sigma_{\alpha u}(x,x^i)\Sigma_{\beta v}(x,x^i)
% \]
% Similarly with $C$:
% \[
% \lim_{n \to \infty}\big\langle \nabla_{w^q} a_{\alpha\beta}, \dot{w}_t^q \big\rangle &=& -\sum_{i,u,v}da^i_{uv}\Sigma_{\alpha u}(x,x^i)\Sigma_{\beta v}(x,x^i)
% \]

% Putting it all together:
% \[
% \dot{a}_{\alpha\beta} =  -\sum_{i,u,v}da^i_{uv}\chi^i\Big[\Sigma_{\alpha u}(x,x^i)\Theta_{\beta v}(x,x^i) + \Sigma_{\beta v}(x,x^i)\Theta_{\alpha u}(x,x^i) + 2\Sigma_{\alpha u}(x,x^i)\Sigma_{\beta v}(x,x^i)\Big]
% \]
% In matrix form:
% \[
% \dot{a} = -\sum_i\chi^i\Big[ \Sigma^ida^i\Theta_i^\top + \Theta_ida_i\Sigma^{i\top} + 2\Sigma^ida^i\Sigma^{i\top}\Big]
% \]


% \subsection{Simplified Attention Model}
% To perform experiments, we introduce a simple attention model. Let $x \in \mathbb{R}^{d_s \times d}$ denote some input. Our simplified attention model consists of a single attention head, followed by global average pooling (GAP). Following the global average pooling is either a linear fully connected layer, or a 1 hidden layer MLP. Formally, our model is given by (for a linear layer following GAP):
% \[
% K &=& xw^k \in \mathbb{R}^{d_s\times n},~~~Q = xw^q\in \mathbb{R}^{d_s\times n},~~~V = xw^v\in \mathbb{R}^{d_s\times n}\\
% a &=& \frac{KQ^\top}{\sqrt{n}}\in \mathbb{R}^{d_s \times d_s},~~~z = \psi(a)\in \mathbb{R}^{d_s \times d_s},~~~h = zV\in \mathbb{R}^{d_s \times n}\\
% g &=& \frac{1}{d_s}\sum_jh_{j:} \in \mathbb{R}^n,~~~f = \frac{v^\top g}{\sqrt{n}}
% \]

% In this simplified setting, according to the previous subsection we have:
% \[
% \dot{a} = -2\sum_i\chi_i\Big[\Sigma^i da^i \Sigma^{i\top}\Big]
% \]
% where $\Sigma^i = xx^{i\top}$
% To complete the evolutionary equations, we compute the evolution of output $f$ and $da_i$. We have:
% \[
% \dot{f} = \big\langle \frac{\partial f}{\partial w^v},\dot{w}_t^v \big\rangle + \big\langle \frac{\partial f}{\partial v},\dot{v} \big\rangle + \big\langle \frac{\partial f}{\partial a},\dot{a} \big\rangle 
% \]
% The first terms may be readily computed in the limit:
% \[
% \dot{w_{uv}^v} &=& -\sum_{ipq}\chi_i\frac{\partial f^i}{\partial h^i_{pq}}\frac{\partial h^i_{pq}}{\partial w_{uv}^v} = -\sum_{ip}\chi_i\frac{v_v}{d_s\sqrt{n}}(z^ix^i)_{pu}\\
% &=& -\sum_{ip}\chi_i\frac{v_v}{d_s\sqrt{n}}z_{p:}^{i\top}x_{:u}^i
% \]
% hence:
% \[
% \big\langle \frac{\partial f}{\partial w^v},\dot{w}_t^v \big\rangle &=& -\sum_{iuv}\chi_i \frac{\partial f}{\partial w_{uv}^v}\frac{\partial f^i}{\partial w_{uv}^v}\\
% &=& -\sum_{iuvpq}\chi_i\frac{v_v}{d_s\sqrt{n}}z_{p:}^{i\top}x_{:u}^i\frac{v_v}{d_s\sqrt{n}}z_{q:}^{\top}x_{:u}\\
% &\to& -\frac{1}{d_s^2}\sum_{ipq}\chi_i (z^i\Sigma^iz^\top)_{pq} 
% \]
% Similarly:
% \[
% \big\langle \frac{\partial f}{\partial v},\dot{v} \big\rangle = -\sum_i\chi_i \frac{g^\top g^i}{n} = -\sum_i\chi_i \frac{\sum_{pq}h_{p:}^\top h_{q:}^i}{nds^2} \to -\frac{1}{d_s^2}\sum_{ipq}\chi_i (z^i\Sigma^iz^\top)_{pq} 
% \]

% finally:
% \[
% \dot{f} = -2\sum_i\chi_i\Big[\frac{1}{d_s^2}\sum_{pq} (\psi(a^i)\Sigma^i\psi(a)^\top)_{pq} +  \big\langle \Sigma^i da^i \Sigma^{i\top}, da \big\rangle\Big]
% \]

% also:
% \[
% dz_{uv} &=& \frac{\partial f}{\partial z_{uv}} = \sum_{pq}\frac{\partial f}{\partial h_{pq}}\frac{\partial h_{pq}}{\partial z_{uv}} = \sum_{q}\frac{v_q}{d_s\sqrt{n}}V_{vq} = \sum_{q}\frac{v_q}{d_s\sqrt{n}}x_{v:}^\top w^v_{:q}\\
% &=& \frac{1}{d_s\sqrt{n}}x_{v:}^\top w^v v\\
% dz_{u:} &=& \frac{1}{d_s\sqrt{n}}x^\top w^v v
% \]




% We assume the network output is given by:
% \[
% f(\xi) = \big\langle \frac{w^o}{\sqrt{n}},h(\xi) \big\rangle,~~~h(\xi) = z(\xi)V(\xi),~~~z(\xi) = \psi(a(\xi))
% \]

% where $w^o \in \mathbb{R}^{d \times n}$. Then:

% \[
% \dot{f}(z) &=& \big\langle \frac{\partial f}{\partial w^o},\dot{w}_t^o \big\rangle + \big\langle \frac{\partial f}{\partial w^v},\dot{w}_t^v \big\rangle\\
% &=& -\sum_i\chi_i\frac{1}{n}\big\langle h,h^i \big\rangle + \big\langle \frac{\partial f}{\partial w^v},\dot{w}_t^v \big\rangle\\
% &&\frac{\partial f}{\partial w_{uv}^v} = \sum_{ij}\frac{\partial f}{\partial h_{ij}}\frac{\partial h_{ij}}{\partial w_{uv}^v} = \sum_{i}\frac{w^o_{iv}}{n}(zx)_{iv} =  \sum_{i}\frac{w^o_{iv}}{n}z_{i:}^\top x_{:u}
% \]
% and so:
% \[
% \big\langle \frac{\partial f}{\partial w^v},\dot{w}_t^v \big\rangle &=& -\sum_{i,u,v}\chi_i\sum_{uv} \frac{\partial f}{\partial w_{uv}^v}\frac{\partial f^i}{\partial w_{uv}^v} \\
% &=& -\sum_{i,u,v}\chi_i\sum_{pq}\frac{w^o_{pv}}{n}z_{p:}^\top x_{:u}\frac{w^o_{qv}}{n}z^{i\top}_{q:} x^i_{:u}\\
% &\to& -\sum_{i,u}\chi_i\sum_{p}\frac{(z_{p:}^\top x_{:u})(z_{p:}^{i\top} x^i_{:u})}{n}
% \]
% \[
% dz_{\alpha\beta}^j &=& \sum_{i=1}\chi^i\frac{\partial f^i}{\partial z_{\alpha\beta}^i} = 
% \]


% % In einstein notations (index appearing twice implies summation):
% % \[
% % x(\xi)_{\alpha\beta} &=& u_{\alpha\gamma}\xi_{\gamma\beta},~~~a(\xi)_{\alpha\beta} = \frac{K(\xi)_{\alpha\gamma}Q(\xi)_{\beta\gamma}}{\sqrt{n}},~~~z(\xi)_{\alpha\beta} = (\psi(a))_{\alpha\beta}\\
% % h(\xi)_{\alpha\beta} &=& z(\xi)_{\alpha\gamma}V(\xi)_{\gamma\beta},~~~f_\alpha = \frac{1}{\sqrt{n}}h(\xi)_{\alpha\beta}v_\beta.
% % \]
% % Let $\theta$ denote the collection of all parameters in the model $\{w^v,w^q,w^k,u,v\}$.\\
% % For any data dependent tensor $\bullet$, we omit the dependency on sample to denote it symbolic form (i.e its dependency on a generic sample, $\bullet = \bullet(\tilde{\xi})$).  Let $f(\xi)_\alpha \in \mathbb{R}$ denote the $\alpha$ coordinate of $f(\xi)$, and $da(\xi)_{\alpha\beta} = \frac{\partial f(\xi)_\alpha}{\partial a(\xi)_{\alpha\beta}} \in \mathbb{R}, da(\xi)_{\alpha:} = \frac{\partial f(\xi)_\alpha}{\partial a(\xi)_{\alpha:}} \in \mathbb{R}^{d_s}$. Similarly, for any data dependent tensor $\bullet$, we denote by $d\bullet(\xi)_{\alpha\beta} = \frac{\partial f(\xi)_\alpha}{\partial \bullet(\xi)_{\alpha\beta}}$. 

% % For any two tensors $\mathcal{T}^1 \in \mathbb{R}^{d_1,d_2,...,d_m},\mathcal{T}^2 \in \mathbb{R}^{n_1,n_2,...,n_l}$, the derivative $\mathcal{D} = \frac{\partial \mathcal{T}^1}{\partial \mathcal{T}^2} \in \mathbb{R}^{d_1,d_2,...,d_m,n_1,n_2,...,n_l}$ is such that $\mathcal{D}_{p_1,p_2,...,p_m,p_{m+1},...,p_{m+l}} = \frac{\partial \mathcal{T}^1_{p_1,p_2,...,p_m}}{\partial \mathcal{T}^2_{p_{m+1},p_{m+2},...,p_{m+l}}}$.\\
% % Therefore, we have that:

% % \[
% % \frac{\partial f_{\alpha}}{\partial h_{\alpha\beta}} &=& \frac{1}{\sqrt{n}}v_\beta,~~~\frac{\partial h}{\partial V} = I_{d_s} \otimes v \in \mathbb{R}^{d_s \times d_s \times n}\\
% % \frac{\partial f}{\partial z} &=& \frac{1}{\sqrt{n}}V \otimes v \in \mathbb{R}^{d_s \times d_s \times d_s}
% % \]

% % \[
% % \dot{a}_{\alpha\beta} &=& \big\langle\frac{\partial a_{\alpha\beta}}{\partial K_{\alpha:}}  , \dot{K}_{\alpha:} \big\rangle + \big\langle\frac{\partial a_{\alpha\beta}}{\partial Q_{\beta:}}  , \dot{Q}_{\beta:} \big\rangle\\
% % \frac{\partial a_{\alpha\beta}}{\partial K_{\alpha:}} &=& \frac{1}{\sqrt{n}}Q_{\beta:},~~~ \frac{\partial a_{\alpha\beta}}{\partial Q_{\beta:}} = \frac{1}{\sqrt{n}}K_{\alpha:}
% % \]
% % To compute the time derivative, we first compute the gradients:
% % \[
% % \nabla_{w^v}f_\alpha &=& \frac{\partial f_\alpha}{\partial }
% % \]


% % The time derivatives under gradient flow are given by:
% % \[
% % \dot{K}_{\alpha\beta} = \big\langle \frac{}{} \big\rangle
% % \]




% \subsection{Things to do}

% 1.Linear setting\\
% 2.Get explicit form for transformers/with without res/mlps...\\
% 3.performance when increasing only some layers width\\
% 4.local attention - implicitly a bottleneck?
% 5.Experiments on MLPs
% 6.efficient nets.

% \begin{appendices}


% \section{Explicit form for $\Lambda$: Special cases}
% {\bf Omid note:} Here we derive the explicit form of (the fixed) function $\Lambda$ in (\ref{Lambda}) for the special case, where the nonlinearity in the activation $\phi$ is the Gauss error function. The explicit form could come handy when performing numerical simulations. More precisely, we compute the following expected value, where $z=[z_1, z_2, z_3, \cdots,z_d] \in \mathbb{R}^d$ where $z_i$s {\it iid} distributed as $z_i\sim \mathcal{N}(0, \Sigma^2_{z})$ and $\Upsilon, \chi \in \mathbb{R}^{d}$ are taken to be fixed vectors
% \[\label{lamb}
% \Lambda_{rs} = \mathbb{E}_{\vec{z}}[z_rz_s\phi'(\frac{z^{\top}\Upsilon}{\sqrt{2}})\phi'(\frac{z^{T}\chi}{\sqrt{2}})].
% \]
% Expanding the right-hand side of (\ref{lamb})
% \[
% \Lambda_{rs} = \frac{2^{\frac{4-d}{2}}}{\Sigma^d_{z}\pi^{\frac{d}{2}+1}}\int z_r z_s \exp{[-\frac{1}{2}\sum_{ij}z_i(\frac{1}{\Sigma^2_{z}}\delta_{ij} + \Upsilon_{i}\Upsilon_{j} + \chi_{i}\chi_{j})z_j]}\prod^{d}_{k=1} dz_{k}.
% \]
% Consider the generator $\mathcal{Z(\{\bf{\mathcal{\varrho}}\})}$ instead 
% \[
% \mathcal{Z(\{\bf{\mathcal{\varrho}}\})}= \int\exp{[-\frac{1}{2}\sum_{ij}z_i(\frac{1}{\Sigma^2_{z}}\delta_{ij} + \Upsilon_{i}\Upsilon_{j} + \chi_{i}\chi_{j})z_j+\sum_{i}\varrho_{i}z_i]}\prod^{d}_{k=1} dz_{k},
% \]
% where $\varrho$ is the {\it source}. Then $\Lambda_{rs}$ can be written as 
% \[\label{2nd_der}
% \Lambda_{rs} = \frac{2^{\frac{4-d}{2}}}{\Sigma^d_{z}\pi^{\frac{d}{2}+1}} \partial^2_{\varrho_r\varrho_s}\mathcal{Z(\{\bf{\mathcal{\varrho}}\})}_{|\varrho=\vec{0}}.
% \] 
% We can write done a closed form for the Gaussian generator above. Defining
% \[
% [\Gamma]_{ij} = [\frac{1}{\Sigma^2_{z}}\mathbb{1} + \Upsilon\Upsilon^{\top} + \chi\chi^{\top}]_{ij},
% \]
% then 
% \[
% \mathcal{Z(\{\bf{\mathcal{\varrho}}\})} = \frac{(2\pi)^{\frac{d}{2}}}{[\text{det}(\Gamma)]^{\frac{1}{2}}}\exp[\frac{1}{2}\varrho^{\top}\Gamma^{-1}\varrho].
% \]
% Using this and (\ref{2nd_der})
% \[
% \Lambda_{rs} = \frac{2^{\frac{4-d}{2}}}{\Sigma^d_{z}\pi^{\frac{d}{2}+1}} \partial^2_{\varrho_r\varrho_s}\mathcal{Z(\{\bf{\mathcal{\varrho}}\})}_{|\varrho=\vec{0}}= \frac{4}{\pi\Sigma^d_{z}}\frac{1}{[\text{det}(\Gamma)]^{\frac{1}{2}}}[\Gamma^{-1}]_{rs}.
% \]
% The special form of $\Gamma$ allows us to write down a closed form for $\Lambda$. Using determinant lemma, one can write 
% \[\label{DL}
% \text{det}(H + \chi\chi^{T}) = \text{det}(H)(1+\chi^{\top}H^{-1}\chi),
% \]
% where we specialize to $H=\frac{1}{\Sigma^2_{z}}\mathbb{1} + \Upsilon\Upsilon^{\top}$. Now applying the Sherman-Morrison formula, leaves us with 
% \[\label{SM}
% \frac{1}{\Sigma^{2}_{z}}H^{-1} = \mathbb{1} - \frac{\Sigma^2_{z}\Upsilon\Upsilon^{\top}}{1 + \Sigma^2_{z}\Upsilon^{\top}\Upsilon}, 
% \]
% while another application of the determinant lemma gives 
% \[
% \text{det}(H) = \frac{(1+\Sigma^2_{z}\Upsilon^{\top}\Upsilon)}{\Sigma^{2d}_{z}}.
% \]
% At this point, using (\ref{DL}) and (\ref{SM}), one arrives at a closed form for $\text{det}(\Gamma)$ as follows
% \[
% \text{det}({\Gamma})= \frac{1}{\Sigma^{2d}_{z}}(1+ \Sigma^{2}_{z}\Upsilon^{\top}\Upsilon)(1+\Sigma^{2}_{z}\chi^{\top}\chi)-\Sigma^{4-2d}_{z}\chi^T\Upsilon\Upsilon^{\top}\chi.
% \]
% To obtain a closed form for $\Gamma^{-1}$, one simply applies the Sherman-Morrison formula again
% \[
% \Gamma^{-1} = H^{-1} - \frac{H^{-1} \chi\chi^{\top}H^{-1}}{1+\chi^{\top}H^{-1}\chi},
% \] 
% where $H^{-1}$ is given in (\ref{SM}). This concludes the derivation for the error function activation.

% We will also write down explicit form of $\Lambda$ for the case of $ReLu$ networks (useful for numerical simulations).
% There are two terms in eq(\ref{Lambda_terms}). Define 
% \[\label{lambda_defs}
% \Lambda^1 = \mathbb{E}_{\{a,b\}}[\phi'(bg_{i:})\odot a \cdot \phi'(bg_{k:})\odot a], \\\nonumber 
% \Lambda^2 = \mathbb{E}_{\{a,b\}}[\phi'(bg_{i:})\odot b_{:j} \cdot \phi'(bg_{k:})\odot b_{:l}],
% \]
% where $b\sim \mathcal{N}(0, \Sigma^{2}_b)$ and $a\sim\mathcal{N}(0, \Sigma^2_a)$. Will Focus on $\Lambda^1$ first. Writing the expected values above more explicitly in Einstein notation
% \[
% \Lambda^{1} = \sum_p\Lambda^{1}_p = \sum_p \int \Theta(\sum_m b_{pm}g_{im})\Theta(\sum_m b_{pm}g_{km})a^2_p \mathbb{P}(a, b)\prod_{p} da_p\prod_{pm} db_{pm}
% \]
% where $\Theta=\Theta(x)$ is the Heaviside function and $\mathbb{P}= \mathbb{P}(a, b)$ is the following joint Gaussian distribution
% \[
% \mathbb{P}(a, b) = \frac{1}{\Sigma^{n}_{a}\Sigma^{nd}_b(2\pi)^{\frac{n}{2}+\frac{nd}{2}}}\exp[-\frac{1}{2\Sigma^2_a}\sum_i a_i - \frac{1}{2\Sigma^2_b}\sum_{pm} b^2_{pm}].
% \]
% Note that one can choose a coordinate system in the $b$-space such that vector $g_{i}$ aligns with one of the dimensions (call it dimension 1) and the vector $g_{k:}$ lives in a plane spanned by direction 1 and some other dimension we call 2. Using this fact, one can define 
% \[
% x = b_{p1}g_{i1}, \\ \nonumber
% y = b_{p1}g_{k1} + b_{p2}g_{k2}.
% \]  
% This implies except for $b_{p1}, b_{p2}$, every other $b_{pm}$ integral decouples and integrates to 1 ($\mathbb{P}$ is normalized). 
% Under the above coordinate transformation, one obtains 
% \[
% \Lambda^{1}_{p} = \frac{\Sigma^2_{a}}{2\pi}\sin{\phi_0}\int^{\infty}_{-\infty}\int^{\infty}_{-\infty} d\tilde{x} d\tilde{y} \exp[-\frac{1}{2}\tilde{x}^2 -\frac{1}{2}\tilde{y}^2 +\cos{\phi_0} \tilde{x}\tilde{y}],
% \]
% where 
% \[
% \tilde{x} = x\frac{\sqrt{g^2_{k1}+g^2_{k2}}}{\Sigma_b g_{i1}g_{k2}}, \\ \nonumber
% \tilde{y} = \frac{1}{\Sigma_b g_{k2}} y, \\ \nonumber
% \sin{\phi_0} = \frac{g_{k2}}{\sqrt{g^2_{i1}+ g^2_{k2}}}, \quad
% \cos{\phi_0} = \frac{g_{k1}}{\sqrt{g^2_{i1}+ g^2_{k2}}}.
% \]
% This can be further simplified by switching to polar coordinates in $x-y$ plane as follows
% \[
%  \Lambda^{1}_{p}  =-\frac{\Sigma^2_a}{2\pi}\sin{\phi_0}\int^{\frac{\pi}{2}}_0d\Phi\int^{\infty}_{0} dr \frac{\partial_r{\exp[-\frac{1}{2}r^2 + r^2\cos{\phi_0}\sin{\Phi}\cos{\Phi}]}}{1- 2 \cos{\phi_0}\cos{\Phi}\sin{\Phi}}.
% \]
% Performing the integral over $r$, gives 
% \[
%  \Lambda^{1}_{p}=\frac{\Sigma^2_a}{2\pi}\sin{\phi_0}\int^{\frac{\pi}{2}}_0  \frac{d\Phi}{1-\cos{\phi_0}\sin{2\Phi}}
% \]
% This definite integral can be performed analytically using {\it Weierstrass substitution}. The change of variable $t = \tan{\Phi}$ gives
% \[
%  \Lambda^{1}_{p}  =  \frac{\Sigma^2_a}{2\pi}\sin{\phi_0}\int^{\infty}_{0}\frac{dt}{(t - \cos{\phi_0})^2 + \sin^2{\phi_0}}.
% \]
% A further change of variable in the following form $U = \frac{(t - \cos{\phi_0})}{\sin{\phi_0}}$ leads to
% \[
%  \Lambda^{1}_{p} = \frac{\Sigma^2_a}{2\pi}\tan^{-1}{U} \biggr|^{\infty}_{-\cot{\phi_0}}=\frac{\Sigma^2_a}{2\pi}(\frac{\pi}{2} + \tan^{-1}{\cot{\phi_0}}).
% \]
% Simplifying this further one finds
% \[
% \Lambda^{1}_{p} = \frac{\Sigma^2_a}{2\pi}(\pi - \phi_0).
% \]

% Let us briefly quote the results for the second expected value in (\ref{lambda_defs}) as well. Note that 
% \[
% Z(\alpha, \beta) = \mathbb{E}_{z}[\text{Relu}(z^{\top}\alpha)\text{Relu}(z^{\top}\beta)],
% \]
% can be seen as the generator for the second expected value in (\ref{lambda_defs}). Expanding this out, one gets
% \[
% Z=\frac{1}{\Sigma^2_z(2\pi)^{\frac{d}{2}}}\int z^{\top}\alpha z^{\top}\beta \quad\Theta(z^{\top}\alpha)\Theta(z^{\top}\beta) \exp[-\frac{1}{2\Sigma^2_z}\sum_{i}z^2_i]\prod_i dz_{i}.
% \]
% Where $\Theta$ is the step function. Similar arguments to the ones used above can be used to performed the integral ({\bf{TODO}}: fill in the steps if necessary)
% \[
% Z= \frac{\Sigma^2_{z}}{2\pi} |\alpha||\beta|[\sqrt{1-s^2} + s(\pi - \arccos{(s)})], \quad  s = \frac{\vec{\alpha}\cdot\vec{\beta}}{|\vec{\alpha}| |\vec{\beta}|},
% \]
% and 
% \[
% \Lambda^2_{lj} = \partial^{2}_{\alpha_l\beta_j}Z.
% \] 
% After some algebra
% \[
% \Lambda^2_{lj}  = \frac{\Sigma^2_{z}}{2\pi}[\frac{\alpha_l\beta_j}{|\alpha||\beta|}\sqrt{1-s^2}+\frac{|\alpha|\beta|}{\sqrt{1-s^2}}\partial_{\alpha_l}s\partial_{\beta_j}s + \delta_{lj}(\pi - \arccos(s))],
% \]
% where
% \[
% \partial_{\beta_j}s = \frac{1}{|\beta|}(\frac{\alpha_j}{|\alpha|}-\frac{\beta_j}{\beta}s), \\ \nonumber 
% \partial_{\alpha_l}s = \frac{1}{|\alpha|}(\frac{\beta_l}{|\beta|}-\frac{\alpha_l}{|\alpha|}s),
% \]
% which further simplifies into 
% \[
% \Lambda^2_{lj}  = \frac{\Sigma^2_z}{2\pi\sqrt{1-s^2}}(\frac{\beta_l\alpha_j+\beta_j\alpha_l}{|\alpha||\beta|} - \frac{\beta_l\beta_j }{|\beta|^2}s - \frac{\alpha_j\alpha_l }{|\alpha|^2}s) +  \frac{\Sigma^2_z}{2\pi}\delta_{lj}(\pi - \arccos(s)).
% \]


% % \section{verification}
% % recall the model is given by:
% % \[
% % f &=& \frac{v^\top z(x)}{\sqrt{n}},~~~z(x) = \phi(h(x)),~~~h(x) = ux\\
% % \nabla_v \mathcal{L} &=& -\sum_i\chi_i\frac{z_i}{\sqrt{n}} = -\sum_i\chi_i\frac{\phi'(h_i)\odot (\sum_\alpha u_\alpha x_{i,\alpha})}{\sqrt{n}}\\
% % \nabla_u \mathcal{L} &=& -\sum_i\chi_i \frac{(\phi'(h_i))\odot v)x_i^\top}{\sqrt{n}}\\
% % \nabla_{u_\alpha} \mathcal{L} &=& -\sum_i\chi_i \frac{(\phi'(h_i))\odot v)x_{i,\alpha}}{\sqrt{n}}\\
% % df_i^\alpha &=& \frac{1}{\sqrt{n}}u_\alpha^\top (\phi'(h_i)\odot v)\\
% % \nabla_v df_i^\alpha &=& \frac{1}{\sqrt{n}}u_\alpha \odot \phi'(h_i)\\
% % \nabla_{u_\alpha} df_i^\alpha &=& \frac{1}{\sqrt{n}}\phi'(h_i)\odot v
% % \]
% % now:
% % \[
% % \langle \nabla_v df_i^\alpha, \nabla_v \mathcal{L} \rangle &=& -\sum_j\chi_j\frac{1}{n}\Big(u_\alpha \odot \phi'(h_i)\Big)^\top \Big(\phi'(h_j)\odot (\sum_\beta u_\beta x_{j,\beta})\Big)\\
% % &\to& -\sum_j\chi_j\sum_\beta x_{j,\beta}\E\Big[z_\alpha z_\beta \phi'(z^\top x_i)\phi'(z^\top x_j)\Big]\\
% % &=& -\sum_j\chi_j\sum_\beta x_{j,\beta}\frac{\partial }{\partial x_{i,\alpha}}\frac{\partial }{\partial x_{j,\beta}}\E\Big[ \phi(z^\top x_i)\phi(z^\top x_j)\Big]\\
% % &=& -\sum_j\chi_j\frac{\partial}{\partial x_{i,\alpha}} x_j^\top \frac{\partial}{\partial x_j}\Sigma(x_i,x_j)
% % \]

% % Finally:
% % \[
% % \dot{df}_i &=& -\sum_j \chi_j\Big[x_j\dot{\Sigma}(x_i,x_j) + \frac{\partial}{\partial x_i} x_j^\top \frac{\partial}{\partial x_j}\Sigma(x_i,x_j)\Big]\\
% % &=& -\sum_j \chi_j\Big[x_j\dot{\Sigma}(x_i,x_j) + \frac{\partial}{\partial x_i} \Sigma(x_i,x_j)\Big]
% % \]

% % We have:

% % \[
% % g'_{t+1} - g'_{t} &=&  -\Theta_g(\tilde{\xi}, \xi) ((\chi_t \otimes \mathbb{1}) \odot dg_t)\\
% % f_{t+1}(g'_{t+1}) &=& f_{0}(g'_{t+1}) -\sum_{s=0}^{t}\Theta_f(g'_{t+1}, g_s)\chi_s\\
% % dg_t &=& dg_0(g_t) - \sum_{s=0}^{t-1}\Lambda(g_t, g_s)\chi_s
% % \]
% % where:
% % \[
% % \Lambda(g_t, g_s) = g_s\dot{\Sigma}(g_t,g_s) + \frac{\partial}{\partial g_t} \Sigma(g_t,g_s)
% % \]

% \end{appendices}


% % At initialization, the initial $\mathcal{G}^0$ is distributed according to the wishart distribution, and is PD with probability 1. Assuming that $\mathcal{G}^0>\epsilon$ at initialization, we now show that $\mathcal{G}^t>0$ at any step $t$. Note that $\mathcal{G}^t = g^tg^{t\top}$. Recall that:
% % \[
% % g^t &=& g^0 + \int_{s=0}^t \dot{g}^sds,~~~ \dot{g}_i^s = -\mathcal{K}\frac{\partial \mathcal{L}_i^s}{\partial g_i^s} = \mathcal{K}\text{diag}(\chi^s)\frac{\partial f_i^s}{\partial g_i^s}\\
% % \mathcal{G}^t &=& (g^0 - \int_{s=0}^t \dot{g}^sds)(g^0 - \int_{s=0}^t \dot{g}^sds)^\top
% % \]




% % with dimensions $\xi,g(\xi),f(\xi) \in \mathbb{R}, y(\xi),x(\xi),z(\xi),h(\xi) \in \mathbb{R}^n$ and trainable parameters $u,v,a,b \in \mathbb{R}^n$ with coordinates sampled iid from a normal distribution.
% % Note that this model is comprised of two identical modules with different weights (the first module computes $\xi \to y \to x \to g$, the second module computes $g \to z \to h \to f$). Each module projects its scalar input nonlinearly into an $\mathbb{R}^n$ vector, which is then projected back to a scalar. This model can be thought of as a wide model (when $n$ is large), with a bottleneck layer in the middle.
% % Further note that we have used the standard NTK parametrization here for all layers.  

% % \paragraph{Initialization} At initialization, it is straightforward to show that the coordinates of $y(x)$ for a given input $\xi$, as well as the output of the bottleneck layer $g(\xi)$ are all \emph{Gaussian Processes} in the limit of $n \to \infty$, and of order $\sim \Theta(1)$ (That is, they have a finite non-zero variance in the limit). Non-trivially, the coordinates of $z(\xi)$, as well the output $f(\xi)$ also converge to a GP in the limit as was shown recently, and are also of order $\sim \mathcal{O}(1)$.





% % \paragraph{Training}
% % We now train the model using \emph{stochastic gradient descent} (SGD) with a learning rate $\eta$. Let $\{x_i\}_{i=1}^N$ denote a training dataset, and let $\mathcal{L}_i = \mathcal{L}\big(f(x_i)\big)$ denote the loss (implicitly containing the label) given example
% % $x_i$, with a derivative $\chi_i = \frac{\partial \mathcal{L}_i}{\partial f(x_i)}$. \\
% % We now analyze the first gradient step. We assume sample $\xi$ is the first sample fed to the SGD algorithm, and we wish to track the evolution of $g(\tilde{\xi}),f(\tilde\xi)$ given some arbitrary example $\tilde{\xi}$ after the first step. For any quantity $\bullet(\xi)$, we use a subscript $\bullet_1(\xi)$ to denote its value after the first gradient step. To reduce notation clutter, we remove the explicit dependency on the sample by using $\bullet = \bullet(\xi), \tilde{\bullet} = \bullet(\tilde{\xi})$ for any quantity $\bullet$ which depends on the input. For each function $\bullet$, we define $d\bullet = \sqrt{n}\frac{\partial f}{\partial \bullet}$ if $\bullet$ is an $\mathbb{R}^n$ vector, and $d\bullet = \frac{\partial f}{\partial \bullet}$ if $\bullet$ is a scalar. Namely:
% % \[
% % dg &:=& \frac{\partial f}{\partial g},~~~dy := \sqrt{n}\frac{\partial f}{\partial y},~~~dx:= \sqrt{n}\frac{\partial f}{\partial x}\\
% % dz &:=& \sqrt{n}\frac{\partial f}{\partial z},~~~dh:= \sqrt{n}\frac{\partial f}{\partial h}.
% % \]
% % We likewise define:
% % \[
% % \delta g &=& g_1 - g,~~~\delta y = \sqrt{n}(y_1 - y),~~~\delta x = \sqrt{n}(x_1 - x).
% % \]
% % Note that $\delta y,\delta x$ are scaled by $\sqrt{n}$.\\
% % We will now track difference $\deltag',\delta\tilde{f}$ given a generic sample $\tilde{\xi}$ after the first gradient step in the limit of $n \to \infty$.
% % \paragraph{First SGD Step}
% % We have the following simple identities:
% % \[
% % dy &=& dg\phi'(y)\odot v,~~~
% % dz = \phi'(z)\odot a,~~~
% % dg = \frac{b^\top (\phi'(z)\odot a)}{\sqrt{n}}.
% % \]
% % We note that at initialization $dg$ and the coordinates of $dy$ are of order $\Theta(1)$ in the large width limit.
% % The gradients of the loss with respect to the parameters are given by:
% % \[
% % \nabla_u \mathcal{L} &=& \chi\frac{dy\xi^\top}{\sqrt{n}},~~~\nabla_v\mathcal{L} =  \chi\frac{dgx}{\sqrt{n}}
% % \]

% % It follows:
% % \[
% % \delta \tilde{y} &=& \sqrt{n}(\tilde{y}_1 - \tilde{y}) = -\eta \chi dy\xi^\top \tilde{\xi}\\
% % \delta \tilde{x} &=& \sqrt{n}(\tilde{x}_1 - \tilde{x}) = \sqrt{n}\Big(\phi(\tilde{y} + \frac{\delta \tilde{y}}{\sqrt{n}}) - \phi(\tilde{y}) \Big).
% % \]
% % Note that the coordinated of $\delta \tilde{y}$ are $\sim \Theta(1)$ in the large width limit, and therefore the coordinates of $\frac{\delta \tilde{y}}{\sqrt{n}}$ are of order $\Theta(\frac{1}{\sqrt{n}})$. Hence, we may naively expand $\delta \tilde{x}$ assuming $\phi$ is smooth:
% % \[
% % \delta \tilde{x} &\approx& -\eta\chi\phi'(\tilde{y})\odot dy \xi^\top \tilde{\xi}.
% % \]
% % hence:
% % \[
% % \delta g' &=& g'_1 - g' =  \frac{v_1^\top \tilde{x}_1 - v^\top \tilde{x}}{\sqrt{n}}\\
% % &=& \frac{v^\top \delta \tilde{x}}{n} - \eta\chi\frac{dgx^\top \tilde{x}}{n} - \eta\chi\frac{dgx^\top \delta \tilde{x}}{n\sqrt{n}}.
% % \]
% % where in the large width limit only the first two terms remain.
% % At the infinite width limit, the vectors $y,x$ are Gaussian with iid components when conditioned on the output GP $g$ at initialization. Let $Z^{y}$ denote the coordinate distribution of $y$, with the following identities:
% % \[
% % \lim_{n \to \infty} \frac{y^\top \tilde{y}}{n} &=& \mathbb{E}\big[Z^{y}Z^{\tilde{y}}\big]\\
% % \lim_{n \to \infty} \frac{x^\top \tilde{x}}{n} &=& \mathbb{E}\big[\phi(Z^{y})\phi(Z^{\tilde{y}})\big]\\
% % \lim_{n \to \infty} \frac{\phi'(y)^\top \phi'(\tilde{y})}{n} &=& \mathbb{E}\big[\phi'(Z^{y})\phi'(Z^{\tilde{y}})\big].
% % \]

% % We have:
% % \[
% % \lim_{n \to \infty}\frac{v^\top \delta \tilde{x}}{n} &=& \lim_{n \to \infty}-\eta \chi v^\top \big[\phi'(\tilde{y})\odot \big(dg\frac{\phi'(y)\odot v}{n}\xi^\top \tilde{\xi}\big)\big]\\
% % &=& -\eta \chi dg \mathbb{E}\big[\phi'(Z^{\tilde{y}})\phi'(Z^{y})\big]\xi^\top \tilde{\xi}\\
% % \lim_{n \to \infty }\delta g'  &=& -\eta \chi dg \Big(\mathbb{E}\big[\phi'(Z^{\tilde{y}})\phi'(Z^{y})\big]\xi^\top \tilde{\xi}  + \mathbb{E}\big[\phi(Z^{\tilde{y}})\phi(Z^{y})\big]\Big)\\
% % &=& -\eta\chi dg \mathcal{K}(\xi,\tilde{\xi}).
% % \]
% % Note that $\delta g'$ does not vanish in the limit, and is of order $\mathcal{O}(1)$, and depends only on the weights $u,v$ through the kernel $\mathcal{K}$ when conditioned on $g$.\\
% % Now for the next block, the gradients are given by:
% % \[
% % \nabla_b \mathcal{L} &=& \chi \frac{dz g}{\sqrt{n}},~~~ \nabla_a \mathcal{L} = \chi\frac{h}{\sqrt{n}}.
% % \]
% % Therefore (we use $\mathcal{K} = \mathcal{K}(\xi,\tilde{\xi})$ here):
% % \[
% % \tilde{z}_1 - \tilde{z}  &=& b_1g'_1 - bg'\\
% % &=& b\delta g' + (b_1 - b)g' + (b_1 - b)\delta g'\\
% % &=& -\eta\chi dg \mathcal{K}b - \eta\chi \frac{dz gg'}{\sqrt{n}} + \eta^2\chi^2 dg\frac{dz g}{\sqrt{n}}\mathcal{K}.
% % \]
% % Note that the first term has components of order $\Theta(1)$, while the rest have components of order $\Theta(\frac{1}{\sqrt{n}})$.
% % We now make the following definitions. We define $ \alpha_z$ as the $\Theta(1)$ contributions to the coordinates of $z$,, and $\beta_z$ as the $\Theta(\frac{1}{\sqrt{n}})$ contributions to the coordinates of $z$ after the first gradient step, scaled by $\sqrt{n}$. Namely:
% % \[
% % \alpha_{\tilde{z}} &=& -\eta \chi dg \mathcal{K} b,~~~ \beta_{\tilde{z}} = - \eta\chi dz gg' + \eta^2\chi^2 dgdz \mathcal{K}.
% % \]
% % Note that we have $\tilde{z}_1 = \tilde{z} + \alpha_{\tilde{z}} + \frac{\beta_{\tilde{z}}}{\sqrt{n}}$. It follows:
% % \[
% % \tilde{h}_1 - \tilde{h} &=& \phi (\tilde{z} + \alpha_{\tilde{z}}) - \phi (\tilde{z})
% % + \sqrt{n}\frac{\phi ( \tilde{z} + \alpha_{\tilde{z}}  + \frac{\beta_{\tilde{z}}}{\sqrt{n}}) - \phi (\tilde{z} + \alpha_{\tilde{z}})}{\sqrt{n}}\\
% % &\approx&  \phi (\tilde{z} + \alpha_{\tilde{z}}) - \phi (\tilde{z}) + \frac{1}{\sqrt{n}}\phi'(\tilde{z} + \alpha_{\tilde{z}})\odot \beta_{\tilde{z}}.
% % \]
% % The output prediction then evolves as (assuming the parameters $a$ are not trained):
% % \[
% % \tilde{f}_1 - \tilde{f} &\approx& \frac{a^\top (\tilde{h}_1 - \tilde{h})}{\sqrt{n}} = \frac{a^\top (\phi (\tilde{z} + \alpha_{\tilde{z}}) - \phi(\tilde{z}))}{\sqrt{n}}
% % + \frac{a^\top \big[\phi'(\tilde{z} + \alpha_{\tilde{z}})\odot \beta_{\tilde{z}}\big]}{n}\\
% % g'_1  - g' &=&  - \eta\chi dg\mathcal{K}_g(\xi,\tilde{\xi})\\
% % f_1(g'_1) - f(g'_1) &=& -\eta \chi \mathcal{K}_f(g'_1,g) 
% % % &=&
% % % &=& f\big(g' - \eta\chi dg\mathcal{K}_g(\tilde{\xi},\xi)\big)  - \eta \chi \mathcal{K}_f(g'_1,g_1) - \tilde{f}
% % \]
% % \section{Gradient Flow}
% % Consider a dataset $\{x_i,y_i\}_{i=1}^N$. In this setting we optimize the network with Gradient flow (GF) using full batch. Let $g,dg,\chi \in \mathbb{R}^N$, and $\mathcal{K}_g,\mathcal{K}_f \in \mathbb{R}^{N \times N}$. Under GF we have $\dot{w}_t = -\nabla_w \mathcal{L}$:
% % \[
% % dy &=& dg\phi'(y)\odot v,~~~
% % dz = \phi'(z)\odot a,~~~
% % dg = \frac{b^\top (\phi'(z)\odot a)}{\sqrt{n}}\\
% % \nabla_u \mathcal{L} &=& \chi\frac{dy\xi^\top}{\sqrt{n}},~~~\nabla_v\mathcal{L} =  \chi\frac{dgx}{\sqrt{n}}\\
% % \nabla_b \mathcal{L} &=& \chi \frac{dz g}{\sqrt{n}},~~~ \nabla_a \mathcal{L} = \chi\frac{h}{\sqrt{n}}.
% % \]
% % Yielding:
% % \[
% % \dot{f} &=& -\sum_i \chi_i\Big[\frac{dy_i^\top dy \xi\xi_i}{n} + \frac{x_i^\top x dgdg_i}{n} + \frac{dz_i^\top dz gg_i}{n} + \frac{h_i^\top h}{n}\Big]
% % \]
% % it holds:
% % \[
% % \frac{dy_i^\top dy \xi\xi_i}{n} &\to& dgdg_i \E\big[\phi'(Z^y)\phi'(Z^{y_i})\big]\xi\xi_i\\
% % \frac{x_i^\top x dgdg_i}{n} &\to& dgdg_i \E \big[Z^x Z^{x_i}\big]\\
% % \frac{dz_i^\top dz gg_i}{n} &=& gg_i \E[\phi'(Z^{z(g)})\phi'(Z^{z_i(g_i)})]\\
% % \frac{h_i^\top h}{n} &=& \E\big[Z^{h(g)}Z^{h_i(g_i)}\big].
% % \]
% % \[
% % g^t - g = -\sum_i \mathcal{K}(\xi,\xi_i)\int_{t'=0}^tdg^{t'}dg_i^{t'}\chi_i^{t'}dt' 
% % \]

% % \section{Reformulation By Kernel Composition}
% % We can understand the output of $f$ by analysing both modules separately. Starting from the last module.
% % We use:
% % \[
% % g &\in& \mathbb{R}^{N \times d},~~~\frac{\partial f}{\partial g} \in \mathbb{R}^{N \times d},~~~ \frac{\partial f}{\partial g_i} \in \mathbb{R}^{1 \times d},~~~\chi \in \mathbb{R}^N,~~~f \in \mathbb{R}^N\\
% % \mathcal{K}_g &\in& \mathbb{R}^{N \times N},~~~\mathcal{K}_{i,g} \in \mathbb{R}^{1 \times N},~~~\frac{\partial \mathcal{L}}{\partial g} \in \mathbb{R}^{N \times d},~~~g(\xi_i) = \in \mathbb{R}^{1 \times d}
% % \]


% % It follows:
% % \[
% % \dot g_i &=& -\mathcal{K}_{g,i}\frac{\partial \mathcal{L}}{\partial g}\\
% % \big\langle \dot g_i, \frac{\partial f_i}{\partial g_i}\big\rangle &=& -\big\langle\mathcal{K}_i \frac{\partial \mathcal{L}}{\partial g}, \frac{\partial f_i}{\partial g_i}\big\rangle = -\big\langle(\mathcal{K}_i \odot\chi)\frac{\partial f}{\partial g}, \frac{\partial f_i}{\partial g_i}\big\rangle
% % \]
% % The evolution of $f(\xi)$ is given by the following equation:
% % \[
% % \dot{f}(\xi) = -\sum_i \chi(\xi_i)\mathcal{K}_f\big(g(\xi)g(\xi_i)^\top \big) - \sum_i \chi(\xi_i)\mathcal{K}_g(\xi^\top \xi_i)\big \langle \frac{\partial f}{\partial g},  \frac{\partial f_i}{\partial g_i}\big \rangle
% % \]

% % Denote $\Xi = \frac{\partial f}{\partial g} \frac{\partial f}{\partial g}^\top \in \mathbb{R}^{N \times N} $, we have that:
% % \[
% % \dot{f} = -\Big[ \mathcal{K}_f(gg^\top) + \mathcal{K}_g(\xi\xi^\top) \odot \Xi\Big]\chi
% % \]



% % \[
% % \dot{f}|g = -\mathcal{K}_f(gg^\top) \chi.
% % \]
% % Similarly, we have for module $g$:
% % \[
% % \dot{g} = -\mathcal{K}_g \frac{\partial \mathcal{L}}{\partial g}.
% % \]

% % The final evolution of $f$ is given by:
% % \[
% % \dot{f} &=& -\mathcal{K}_f(gg^\top) \chi - \text{diag}(\frac{\partial f}{\partial g})\odot \dot{g}\\
% % &=& -\mathcal{K}_f(gg^\top) \chi - \text{diag}(\frac{\partial f}{\partial g})\odot \big[\mathcal{K}_g [\chi \odot \text{diag}(\frac{\partial f}{\partial g})]\big]\\
% % &=& -\Big[\mathcal{K}_f(gg^\top) + \frac{\partial f}{\partial g} \mathcal{K}_g(\xi\xi^\top)\frac{\partial f}{\partial g}\big]\chi\\
% % &=& -\Big[\mathcal{K}_f(gg^\top) + \mathcal{K}_g(\xi\xi^\top)\odot \big(\text{diag}(\frac{\partial f}{\partial g})\text{diag}(\frac{\partial f}{\partial g})^\top \big)\big]\chi
% % \]
% % We note that at initialization, $\text{diag}(\frac{\partial f}{\partial g})$ is a Gaussian vector. We now denote this vector by $dg$. Note that $dg$ can be expressed as:
% % \[
% % dg_i = \frac{a^\top}{\sqrt{n}}\psi(b;g_i).
% % \]
% % where $\psi(x;y) = \phi'(xy)\odot x$.
% % It follows:
% % \[
% % \dot{dg} = 
% % \]

% % Similarly, we have for module $g$:
% % \[
% % \dot{g} = -\mathcal{K}_g [\chi \odot \text{diag}(\frac{\partial f}{\partial g})] = -\mathcal{K}_g \frac{\partial f}{\partial g} \chi
% % \]

% % The final evolution of $f$ is given by:
% % \[
% % \dot{f} &=& -\mathcal{K}_f(gg^\top) \chi - \text{diag}(\frac{\partial f}{\partial g})\odot \dot{g}\\
% % &=& -\mathcal{K}_f(gg^\top) \chi - \text{diag}(\frac{\partial f}{\partial g})\odot \big[\mathcal{K}_g [\chi \odot \text{diag}(\frac{\partial f}{\partial g})]\big]\\
% % &=& -\Big[\mathcal{K}_f(gg^\top) + \frac{\partial f}{\partial g} \mathcal{K}_g(\xi\xi^\top)\frac{\partial f}{\partial g}\big]\chi\\
% % &=& -\Big[\mathcal{K}_f(gg^\top) + \mathcal{K}_g(\xi\xi^\top)\odot \big(\text{diag}(\frac{\partial f}{\partial g})\text{diag}(\frac{\partial f}{\partial g})^\top \big)\big]\chi
% % \]
% % We note that at initialization, $\text{diag}(\frac{\partial f}{\partial g})$ is a Gaussian vector. We now denote this vector by $dg$. Note that $dg$ can be expressed as:
% % \[
% % dg_i = \frac{a^\top}{\sqrt{n}}\psi(b;g_i).
% % \]
% % where $\psi(x;y) = \phi'(xy)\odot x$.
% % It follows:
% % \[
% % \dot{dg} = 
% % \]

% % The first term on the RHS of the above depends on the parameters $a,b$. However, the second can be shown to converge to a deterministic value when conditioning on $f,g,dg$ (TO SHOW). \\
% % To conclude, the first module evolves according to the kernel equation:
% % \[
% % g'_{t+1} - g' = -\eta\sum_{s=0}^t\chi_s dg_s\mathcal{K}(\xi_s,\tilde{\xi}).
% % \]
% % where $\xi_s$ is the sample fed to the SGD algorithm at step $s$, while the second module evolves non-trivially by learning features.
% % \subsection{Conditioning on $f,g,dg$} 
% % In the following calculations we assume that $g,f,dg$ are fixed by conditioning on their initial values. Using TP rules:




% % \paragraph{Subsequent SGD Steps} The subsequent steps may be similarly derived using Tensor Programs. In conclusion, we conclude that the output of the first module evolves according to:

% % while the second block learns features which depend on the initialized values of $a,b,g$.\\




% % \paragraph{Training}
% % We now train the model using \emph{stochastic gradient descent} (SGD) with a learning rate $\eta$. Let $\{x_i\}_{i=1}^N$ denote a training dataset, and let $\mathcal{L}_i = \mathcal{L}\big(f(x_i)\big)$ denote the loss (implicitly containing the label) given example
% % $x_i$, with a derivative $\chi_i = \frac{\partial \mathcal{L}_i}{\partial f(x_i)}$. \\
% % We use the following notations: Let $x_t$ denote the training sample fed to the SGD algorithm at step $t$ (first sample is $x_0$). We use $\bullet_t(x)$ to denote the  value of the function $\bullet$ at step $t$ given example $x$. To reduce clutter, we remove the explicit dependency on $t$ when it is implied by the subscript (i.e $\bullet_t = \bullet_t(x_t)$) Further, for $\bullet$ we remove the subscript $t$ altogether to denote its value at initialization (i.e $\bullet = \bullet_0$). For each function $\bullet(x)$, we define $d\bullet(x) = \sqrt{n}\frac{\partial f(x)}{\partial \bullet(x)}$ if $\bullet(x)$ is an $\mathbb{R}^n$ vector, and $d\bullet(x) = \frac{\partial f(x)}{\partial \bullet(x)}$ if $\bullet(x)$ is a scalar. Similarly we define $\delta \bullet_{t+1}(x) = \sqrt{n}\big(\bullet_{t+1}(x) - \bullet_t(x)\big)$ if is an $\mathbb{R}^n$ vector, and $\delta \bullet_{t+1}(x) = \bullet_{t+1}(x) - \bullet_t(x)$ if 
% % it is a scalar. Namely:
% % \[
% % dg(x) &:=& \frac{\partial f(x)}{\partial g(x)},~~~dy(x):= \sqrt{n}\frac{\partial f(x)}{\partial y(x)},~~~dz(x):= \sqrt{n}\frac{\partial f(x)}{\partial z(x)}\\
% % \delta g_{t+1}(x) &=& g_{t+1}(x) - g_t(x),~~~\delta y_{t+1}(x) = \sqrt{n}\big(y_{t+1}(x) - y_t(x)\big)\\
% % \delta z_{t+1}(x) &=& \sqrt{n}\big(z_{t+1}(x) - z_t(x)\big).
% % \]
% % We will now track how the outputs $g_t(x),f_t(x)$ given a generic sample $x$ evolve during training in the limit of $n \to \infty$.
% % \paragraph{First SGD Step}
% % We have the following simple identities:
% % \[
% % dy(x) &=& dg(x)\phi'(y(x))\odot v,~~~
% % dz(x) = \phi'(z(x))\odot a\\
% % dg(x) &=& \frac{b^\top (\phi'(z(x))\odot a)}{\sqrt{n}}.
% % \]
% % We note that at initialization $dg(x)$ and the coordinates of $dy(x)$ are of order $\Theta(1)$ in the large width limit.
% % The gradients of the loss with respect to the parameters are given by:
% % \[
% % \nabla_u \mathcal{L}(x) &=& \chi(x)\frac{dy(x)x^\top}{\sqrt{n}},~~~\nabla_v\mathcal{L}(x) =  \chi(x)\frac{dg(x)\phi(y(x))}{\sqrt{n}}
% % \]

% % It follows:
% % \[
% % \delta y_1(x) &=& -\eta \chi dyx_0^\top x\\
% % \delta \phi(y_1(x)) &=& \sqrt{n}\Big(\phi(y(x) + \frac{\delta y(x)}{\sqrt{n}}) - \phi(y(x)) \Big)
% % \]
% % Note that the coordinated of $\delta y_1(x)$ are $\sim \Theta(1)$ in the large width limit, and therefore the coordinates of $\frac{\delta y_1(x)}{\sqrt{n}}$ are of order $\Theta(\sqrt{n}^{-1})$. Hence, we may naively expand $\delta \phi(y_1(x))$ assuming $\phi$ is smooth:
% % \[
% % \delta \phi(y_1(x)) &\approx& -\eta\chi\phi'(y(x))\odot dy x_0^\top x.
% % \]
% % hence:
% % \[
% % \delta g_1(x) &=& \frac{v_1^\top \phi(y_1(x)) - v^\top\phi(y(x))}{\sqrt{n}}\\
% % &=& \frac{v^\top \delta \phi(y_1(x))}{n} - \eta\chi\frac{dg\phi(y)^\top \phi(y(x))}{n} - \eta\chi\frac{dg\phi(y)^\top \delta\phi(y_1(x)}{n\sqrt{n}}.
% % \]
% % where in the large width limit only the first two terms remain.
% % At the infinite width limit, the vectors $y(x)$ are Gaussian with iid components when conditioned on the output GP $g$ at initialization. Let $Z^{y(x)}$ denote the coordinate distribution of $y(x)$, with the following identities:
% % \[
% % \lim_{n \to \infty} \frac{y(x)^\top y(\tilde{x})}{n} &=& \mathbb{E}(Z^{y(x)}Z^{y(\tilde{x})})\\
% % \lim_{n \to \infty} \frac{\phi(y(x))^\top \phi(y(\tilde{x}))}{n} &=& \mathbb{E}(\phi(Z^{y(x)})\phi(Z^{y(\tilde{x})}))\\
% % \lim_{n \to \infty} \frac{\phi'(y(x))^\top \phi'(y(\tilde{x}))}{n} &=& \mathbb{E}(\phi'(Z^{y(x)})\phi'(Z^{y(\tilde{x})})).
% % \]

% % We have:
% % \[
% % \lim_{n \to \infty}\frac{v^\top \delta \phi(y_1(x)}{n} &=& \lim_{n \to \infty}-\eta \chi v^\top \big[\phi'(y(x))\odot \big(dg\frac{\phi'(y)\odot v}{n}x_0^\top x\big)\big]\\
% % &=& -\eta \chi dg \mathbb{E}\big[\phi'(Z^{y(x)})\phi'(Z^{y(x_0)})\big]x^\top x_0\\
% % \lim_{n \to \infty }\delta g_1(x)  &=& -\eta \chi dg \Big(\mathbb{E}\big[\phi'(Z^{y(x)})\phi'(Z^{y(x_0)})\big]x^\top x  + \mathbb{E}\big[\phi(Z^{y(x)})\phi(Z^{y(\tilde{x})})\big]\Big)\\
% % &=& -\eta\chi dg \mathcal{K}(x,x_0).
% % \]
% % Note that $\delta g_1(x)$ does not vanish in the limit, and is of order $\mathcal{O}(1)$, and depends only on the weights $u,v$ through the kernel $\mathcal{K}$ when conditioned on $g_0$.
% % Now for the next block, the gradients are given by:
% % \[
% % \nabla_b \mathcal{L}(x) &=& \chi(x) \frac{dz(x) g(x)}{\sqrt{n}},~~~ \nabla_a \mathcal{L}(x) = \chi(x)\frac{\phi(z(x))}{\sqrt{n}}.
% % \]
% % Therefore (we use $\mathcal{K} = \mathcal{K}(x,x_0)$ here):
% % \[
% % z_1(x) - z(x)  &=& b_1g_1(x) - bg(x)\\
% % &=& b\delta g_1(x) + (b_1 - b)g(x) + (b_1 - b)\delta g_1(x)\\
% % &=& -\eta\chi dg \mathcal{K}b - \eta\chi \frac{dz gg(x)}{\sqrt{n}} + \eta^2\chi^2 dg\frac{dz g}{\sqrt{n}}\mathcal{K}.
% % \]
% % Note that the first term has components of order $\Theta(1)$, while the rest have components of order $\Theta(\frac{1}{\sqrt{n}})$.
% % We now make the following definitions. For every $\mathbb{R}^n$ vector $\bullet$, we define $ \alpha_{t+1}^\bullet(x)$ as the $\Theta(1)$ contributions to the coordinates of $\bullet_t(x)$ at step $t$, and $\beta_{t+1}^\bullet(x)$ as the $\Theta(\frac{1}{\sqrt{n}})$ contributions to the coordinates of $\bullet_t(x)$ at step $t$, scaled by $\sqrt{n}$. Namely:
% % \[
% % \alpha_{t+1}^{z}(x) &=& -\eta \chi dg \mathcal{K} b,~~~ \beta_{t+1}^{z}(x) = - \eta\chi dz gg(x) + \eta^2\chi^2 dgdz \mathcal{K}.
% % \]
% % Note that we have $z_1(x) = z(x) + \alpha_{t+1}^{z}(x) + \frac{\beta_{t+1}^{z}(x)}{\sqrt{n}}$. It follows:
% % \[
% % \phi(z_1(x)) - \phi(z(x)) &=& \phi \big( z(x) + \alpha_1^z(x) \big) - \phi \big(z(x)\big)\\
% % &+& \sqrt{n}\frac{\phi \big( z(x) + \alpha_1^z(x)  + \frac{\beta_1^z(x)}{\sqrt{n}}\big) - \phi \big( z(x) + \alpha_1^z(x) \big)}{\sqrt{n}}.
% % \]
% % The output prediction is given by:
% % \[
% % f_1(x) - f(x) = \frac{a^\top \big(\phi(z_1(x)) - \phi(z(x))\big)}{\sqrt{n}} = 
% % \]


% % \subsection{Conditioning on $f,g,dg$} 
% % In the following calculations we assume that $g,f,dg$ are fixed by conditioning on their initial values. Using TP rules:



% % Hence, the coordinates of $z_1(x) - z(x)$ are non vanishing in the limit (unlike the coordinates of $y_1(x) - y(x)$ which vanish in the limit). 
% % Therefore, the second module is not in the kernel regime, and indeed learns features.

% % \paragraph{Subsequent SGD Steps} The subsequent steps may be similarly derived using Tensor Programs. In conclusion, we conclude that the output of the first module evolves according to:
% % \[
% % \delta g_{t+1}(x) = -\eta\sum_{s=0}^t\chi_s dg_s\mathcal{K}(x_s,x)).
% % \]
% % while the second block learns features which depend on the initialized values of $a,b,g$.\\



% % \[
% % &=& -\eta\chi dg b \mathcal{K}^1(x,\tilde{x}) - \eta\chi\frac{dz g}{\sqrt{n}}g' + \eta^2\chi^2\frac{dz g}{\sqrt{n}}dg k \sim \mathcal{O}(1). \\
% % \phi(\tilde{z}_1) - \phi(\tilde{z}) &=& \phi \Big(\tilde{z} + \tilde{z}_1 - \tilde{z}\Big) - \phi(\tilde{z})\\
% % \nabla_a f &=& \chi\frac{\phi(z)}{\sqrt{n}}\\
% % \tilde{f}_1 - \tilde{f} &=& \frac{a_1^\top \phi(\tilde{z}_1) - a^\top \phi(\tilde{z})}{\sqrt{n}}\\
% % &=& \frac{a^\top \big(\phi(\tilde{z}_1) - \phi(\tilde{z})\big) }{\sqrt{n}} + \frac{(a_1 - a)^\top \big(\phi(\tilde{z}_1) - \phi(\tilde{z})\big) }{\sqrt{n}}\\
% % &+& \frac{(a_1-a)^\top\phi(\tilde{z})}{\sqrt{n}}.\\
% % \]
% % It seems that the first block is in the kernel learning regime, while the second block is in the feature learning regime!
% % \subsection{Linear Second Block}
% % We have that:
% % \[
% % dz&=& a,~~~dg = \frac{b^\top a}{\sqrt{n}}\\
% % \tilde{z}_1 - \tilde{z} &=& -\eta\chi \frac{b^\top a}{\sqrt{n}}b\mathcal{K}(x,\tilde{x}) - \eta\chi \frac{ag}{\sqrt{n}}g' + \eta^2\chi^2 \frac{ag}{\sqrt{n}}\frac{a^\top b}{\sqrt{n}}\mathcal{K}(x,\tilde{x})\\
% % \tilde{f}_1 - \tilde{f} &=& -\eta\chi \frac{(b^\top a)^2}{n}\mathcal{K} - \eta\chi \frac{a^\top a}{n}g'
% % \]
% % \subsection{Dual Kernel Regime}


% % The purpose of this document is to investigate the applicability of RMT statistics to the hessian of neural matrix. Consider the output of a neural network $f(\xi;w) \in \R$ where $\xi$ is the input and $w \in \R^{p}$ are the weights. Furthermore, consider the L2 loss $\mathcal{L}(\xi) = \frac{1}{2}(f(\xi;w) - y)^2$. Our network is trained on a dataset $\{\xi_i,y_y\}_{i=1}^N$ with a loss $\sum_i\mathcal{L}(\xi_i)$ using batch GD. Let $\chi(\xi_i) = f(\xi_i;w) - y_i$, The hessian is decomposed into two parts:
% % \[
% % \HE &=& \sum_i \nabla_w f(\xi_i;w)\nabla_w^\top f(\xi_i;w) + \sum_i \chi(\xi_i)\nabla^2_w f(\xi_i;w)\\
% % &=& \G + \HE_e.
% % \]
% % We turn our attention to $\G$ which represents the Gauss Newton approximation of $\HE$. Note that this approximation is exact as we get closer to the global minimum where $\forall_i,~\chi(x_i) = 0$.  
% % The Gauss Newton approximation can be written as the outer product of the Jacobian matrix:
% % \[
% % \G = JJ^\top
% % \]
% % where $J \in \R^{p \times N}$ is a matrix where each column represents the gradient $\nabla_wf(\xi_i;w)$. From elementary algebra, the spectrum of $\G$ matches the spectrum of the NTK given by $\K = J^\top J$. Therefore, in the following we investigate the spectrum of the NTK as it evolves during training under some set of assumptions.

% % \section{Empirical NTK of a Deep MLP}

% % In our working example we will work with a depth L MLP with the NTK parametrization of the following form:
% % \[
% % f(\xi) = \frac{v^\top}{\sqrt{n}} x^L(\xi),~~ x^l(\xi) = \phi\big(g^l(\xi)\big),~~g^l(\xi) = \frac{w^l}{\sqrt{n}}x^{l-1}(\xi),~~ g^0(\xi) = u^\top \xi.
% % \]
% % where $\phi$ is a smooth nonlinearity, the parameters $u \in \R^{n \times d},v \in \R^{n},\{w^l\}_{l=1}^L \in \R^{n \times n}$ are all initialized iid from a normal distribution, and we assume the first and last layers ($u,v$) are not trained for simplicity. 
% % The NTK of the architecture is given by:
% % \[
% % \K(x,\tilde{x}) = \sum_{l=1}^L\frac{x(\xi)^{l\top}x(\tilde{\xi})^l}{n}\frac{dg(\xi)^{l\top}dg(\tilde{\xi})^l}{n}
% % \]
% % where $dg^l(\xi) = \sqrt{n}\frac{\partial f(\xi;w)}{\partial g^l(\xi)}$. \\
% % Note that the NTK is time dependent, and hence we resort to its approximation using a first order truncation of the full NTH.

% % \section{Truncated NTH approximation}
% % We now use a first order truncation of the NTH. Essentially, we assume the NTK changes in the order of $1/n$ throughout training. Under gradient descent with a learning rate $\eta$, the NTK changes in the following fashion:
% % \[
% % \Delta \K(\xi,\tilde{\xi};w) &=& -\frac{\eta}{n}\sum_i \chi(\xi_i)\Big(\nabla_w^\top f(\xi_i)\nabla_w^2 f(\xi)\nabla_w f(\tilde{\xi})  + \nabla_w^\top f(\xi)\nabla_w^2 f(\tilde{\xi})\nabla_w f(\xi_i)\Big)\\
% % &+& \mathcal{O}(\frac{1}{n^2})\\
% % &=& -\frac{\eta}{n}\sum_i \chi(\xi_i)\K^{(1)}(\xi,\tilde{\xi},\xi_i) + \mathcal{O}(\frac{1}{n^2})
% % \]
% % We now consider only terms up to order $\mathcal{O}(\frac{1}{n})$, and treat the values of $\K^{(1)}$ as constant. Let $\K\in \R^{N \times N}$, and $\K_i^{(1)} \in \R^{N \times N} $ denote the correction matrix such that $\K_i^{(1)}(\xi,\tilde{\xi}) = \K^{(1)}(\xi,\tilde{\xi},\xi_i)$ as defined above. The evolution of the NTK $\K_t$ is given by:
% % \[
% % \K_t = \KI -\frac{\eta}{n}\sum_i \K^{(1)}_i\sum_{s=0}^t\chi_s(\xi_i) + \mathcal{O}(\frac{1}{n^2}).
% % \]
% % Note that both $\KI$ and $\{\K^{(1)}_i\}$ are the empirical and depend on the width $n$. However, we can exchange them with their limit value (using the notation $\lim_{n \to \infty }\bullet = \mathring{\bullet}$) by adding additional corrections (to be rigorously justified):

% % \[
% % \KI &=& \mathring{\KI} + \frac{\epsilon}{\sqrt{n}} + o(\frac{1}{\sqrt{n}})\\
% % \K^{(1)}_i &=& \mathring{\K}^{(1)}_i + o(1).
% % \]
% % where $\epsilon$ is a symmetric Gaussian matrix of order $\Theta(1)$. We then have (assuming $\frac{1}{\eta} \sim o(1)$):
% % \begin{align}\label{corrected_ntk}
% % \K_t &= \mathring{\KI} + \frac{\epsilon}{\sqrt{n}} -\frac{\eta}{n}\sum_i \mathring{\K^{(1)}}_i\sum_{s=0}^t\chi_s(\xi_i) + o(\frac{\eta}{n}).
% % \end{align}

% % For the time being, we will ignore the term $\frac{\epsilon}{\sqrt{n}}$ and focus our attention on the following equation:
% % \[
% % \K_t &= \mathring{\KI}  -\frac{\eta}{n}\sum_i \mathring{\K^{(1)}}_i\sum_{s=0}^t\chi_s(\xi_i).
% % \]

% % We will now show that $\mathring{\K^{(1)}}_i$ are GPs for any $i$.

% % \section{The GP behaviour of First Order NTK Corrections}

% % Our objective now is to show that $\mathring{\K^{(1)}}_i$ is a GP for any $i$. Recall the definition of $\mathring{\K^{(1)}}_i$:
% % \[
% % \mathring{\K^{(1)}}_i(\xi,\tilde{\xi}) = \lim_{n \to \infty}n\Big(\nabla_w^\top f(\xi_i)\nabla_w^2 f(\xi)\nabla_w f(\tilde{\xi})  + \nabla_w^\top f(\xi)\nabla_w^2 f(\tilde{\xi})\nabla_w f(\xi_i)\Big).
% % \]
% % We will show that both terms on the RHS of the above converge to a GP in the limit.\\
% % Re- writing:
% % \[
% % \nabla_w^\top f(\xi_i)\nabla_w^2 f(\xi)\nabla_w f(\tilde{\xi}) &=& \sum_{u=1}^L\sum_{v=1}^L \Big\langle \nabla_{w^u}f(\xi_i)\otimes \nabla_{w^v}f(\tilde{\xi}),\nabla^2_{w^u,w^v}f(\xi)  \Big\rangle\\
% % &=& \sum_{u=1}^L\sum_{v=1}^L \mathcal{T}^i_{u,v}
% % \]
% % where we have defined $\mathcal{T}^i_{u,v} := \Big\langle \nabla_{w^u}f(\xi_i)\otimes \nabla_{w^v}f(\tilde{\xi}),\nabla^2_{w^u,w^v}f(\xi)  \Big\rangle$.
% % Let $D^u(\xi) = \text{diag}\big[\phi'\big(g^u(\xi)\big)\big]$ The following equalities hold (WLOG $u<v$ and assume $\phi$ is piece-wise linear):
% % \[
% % \nabla_{w^u}f(\xi) &=& \frac{dg^u(\xi)x^{u-1}(\xi)^\top}{n}\\
% % \frac{\partial x^{v}(\xi)}{\partial g^{u}(\xi)}^\top &=& D^u(\xi)W^{u+1\top}D^{u+1}(\xi)W^{u+2\top}...D^v(\xi)\\
% % dg^u(\xi) &=& \begin{cases}
% % D^L(\xi) v & u = L\\
% % \frac{\partial x^{L}(\xi)}{\partial g^{u}(\xi)} v & 1\leq u<L\\
% % \end{cases}\\
% % % \nabla_{w^v}dg^u(\xi) &=& \frac{1}{\sqrt{n}}\Big(\frac{\partial x^{v-1}(\xi)}{\partial g^{u}(\xi)}\Big)^\top \otimes dg^v(\xi) \\
% % \nabla^2_{w^u,w^v}f(\xi) &=& \frac{1}{n\sqrt{n}}dg^v(\xi) \otimes \frac{\partial x^{v-1}(\xi)}{\partial g^{u}(\xi)} \otimes x^{u-1}(\xi)
% % \]
% % It follows:
% % \[
% % \mathcal{T}^i_{u,v} &=& \frac{1}{n\sqrt{n}}\Big\langle \nabla_{w^v}f(\tilde{\xi})\otimes \nabla_{w^u}f(\xi)   ,dg^v(\xi_i) \otimes \frac{\partial x^{v-1}(\xi_i)}{\partial g^{u}(\xi_i)} \otimes x^{u-1}(\xi_i)\Big\rangle \\
% % &=& \frac{dg^v(\xi_i)^\top dg^v(\tilde{\xi})}{n}\frac{x^{u-1}(\xi_i)^\top x^{u-1}(\xi)}{n}\frac{1}{n\sqrt{n}}\Big \langle x^{v-1}(\tilde{\xi})\otimes dg^u(\xi),\frac{\partial x^{v-1}(\xi_i)}{\partial g^{u}(\xi_i)} \Big \rangle\\
% % &=& \frac{dg^v(\xi_i)^\top dg^v(\tilde{\xi})}{n}\frac{x^{u-1}(\xi_i)^\top x^{u-1}(\xi)}{n}\frac{1}{n\sqrt{n}} x^{v-1}(\tilde{\xi})^\top \frac{\partial x^{v-1}(\xi_i)}{\partial g^{u}(\xi_i)} dg^u(\xi).
% % \]
% % Therefore it follows:
% % \[
% % \lim_{n \to \infty} n\mathcal{T}^i_{u,v} &=& \Big[\lim_{n \to \infty}\frac{dg^v(\xi_i)^\top dg^v(\tilde{\xi})}{n}\Big] \times \Big[\lim_{n \to \infty}\frac{x^{u-1}(\xi_i)^\top x^{u-1}(\xi)}{n}\Big]...\\
% % &\times& \Big[\lim_{n \to \infty}\frac{1}{\sqrt{n}}x^{v-1}(\tilde{\xi})^\top \frac{\partial x^{v-1}(\xi_i)}{\partial g^{u}(\xi_i)} dg^u(\xi)\Big].
% % \]
% % The first two terms in the above have well established infinite width limits, denoted by:
% % \[
% % \lim_{n \to \infty}\frac{dg^v(\xi)^\top dg^v(\tilde{\xi})}{n} &:=& \Gamma^v(\xi,\tilde{\xi})\\
% % \lim_{n \to \infty}\frac{x^{u}(\xi)^\top x^{u}(\tilde{\xi})}{n} &:=& \Sigma^u(\xi,\tilde{\xi}).
% % \]
% % The last term can be written as follows:
% % \[
% % &&\frac{1}{\sqrt{n}}x^{v-1}(\tilde{\xi})^\top \frac{\partial x^{v-1}(\xi_i)}{\partial g^{u}(\xi_i)} dg^u(\xi)\\ &=& V^\top D^L(\xi)...W^{u+1}D^u(\xi)D^u(\xi_i)...W^{v-1\top}D^{v-1}(\xi_i)D^{v-1}(\tilde{\xi})W^{v-1}...D^0(\tilde{\xi})U\tilde{\xi}
% % \]
% % This term can be easily expressed as a \emph{Tensor Program}, and approaches a GP as $n \to \infty$. $\lim_{n \to \infty} n\mathcal{T}^i_{u,v}$ therefore takes the form:
% % \[
% % \lim_{n \to \infty} n\mathcal{T}^i_{u,v} = \Gamma^v(\xi_i,\tilde{\xi})\Sigma^u(\xi_i,\xi)\mathcal{G}_{u,v}(\tilde{\xi},\xi_i,\xi).
% % \]
% % where $\mathcal{G}_{u,v}(\tilde{\xi},\xi_i,\xi)$ is a GP. Hence:
% % \[
% % \mathring{\K^{(1)}}_i(\xi,\tilde{\xi}) = \Gamma^v(\xi_i,\tilde{\xi})\Sigma^u(\xi_i,\xi)\mathcal{G}_{u,v}(\tilde{\xi},\xi_i,\xi) + \Gamma^v(\xi_i,\xi)\Sigma^u(\tilde{\xi},\xi)\mathcal{G}_{u,v}(\xi,\tilde{\xi},\xi_i).
% % \]

% % \section{Example - 1 hidden layer}
% % Consider a model of the form:
% % \[
% % f(\xi) = V^\top x(\xi),~~~x(\xi) = \phi\big(g(\xi)\big),~~~g(\xi) = U\xi.
% % \]
% % where $\xi \in \R^d,~~~V = \frac{v}{\sqrt{n}} \in \R^{n},U = u \in \R^{n \times d}$ and $u,v$ are trainable with iid normally distributed components. Let $\{\xi_i,y_i\}_{i=1}^N$ denote the training dataset where $\forall_i,~\|\xi_i\| = 1$.  
% % Let $D(\xi) = \text{diag}[\phi'\big(g(\xi)\big)]$. We have:
% % \[
% % dg(\xi) &=& D(\xi)v\\
% % \nabla_u f(\xi) &=& \frac{dg(\xi) \xi^\top}{\sqrt{n}}\\
% % \nabla_v f(\xi) &=& \frac{x(\xi)}{\sqrt{n}}\\
% % \nabla_{u,v}f(\xi) &=& \frac{D(\xi)\otimes \xi}{\sqrt{n}}. 
% % \]
% % It follows:
% % \[
% % \mathring{\K^{(1)}}_i(\xi,\tilde{\xi}) &= \lim_{n \to \infty}n\Big(\nabla_u^\top f(\xi_i)\nabla_{u,v} f(\xi)\nabla_v f(\tilde{\xi})  + \nabla_u^\top f(\xi)\nabla_{u,v} f(\tilde{\xi})\nabla_v f(\xi_i)\Big)\\
% % &+ \lim_{n \to \infty}n\Big(\nabla_v^\top f(\xi_i)\nabla_{v,u} f(\xi)\nabla_u f(\tilde{\xi})  + \nabla_v^\top f(\xi)\nabla_{v,u} f(\tilde{\xi})\nabla_u f(\xi_i)\Big).
% % \]
% % Let $D(\xi,\xi_i,\tilde{\xi}) = D(\xi_i)D(\xi)D(\tilde{\xi})$. We have:
% % \[
% % n\nabla_u^\top f(\xi_i)\nabla_{u,v} f(\xi)\nabla_v f(\tilde{\xi}) &= \xi_i^\top \tilde{\xi} \Big[VD(\xi,\xi_i,\tilde{\xi})U\tilde{\xi}\Big] = \xi_i^\top \tilde{\xi}\mathcal{G}(\xi_i,\xi,\tilde{\xi})\\
% % n\nabla_u^\top f(\xi)\nabla_{u,v} f(\tilde{\xi})\nabla_v f(\xi_i) &= \xi_i^\top \xi \Big[VD(\xi,\xi_i,\tilde{\xi})U\xi_i\Big] = \xi_i^\top \xi \mathcal{G}(\xi,\tilde{\xi},\xi_i).
% % \]
% % Finally:
% % \[
% % \mathring{\K^{(1)}}_i(\xi,\tilde{\xi}) &= \xi_i^\top \tilde{\xi}\mathcal{G}(\xi_i,\xi,\tilde{\xi}) + \xi_i^\top \tilde{\xi}\mathcal{G}(\tilde{\xi},\xi,\xi_i) + \xi_i^\top \xi \mathcal{G}(\xi,\tilde{\xi},\xi_i) + \xi_i^\top \xi \mathcal{G}(\xi_i,\tilde{\xi},\xi)\\
% % &= \xi_i^\top \tilde{\xi}\mathcal{G}(\xi_i,\xi,\tilde{\xi}) + \xi_i^\top(\xi + \tilde{\xi})\mathcal{G}(\tilde{\xi},\xi,\xi_i) +  \xi_i^\top \xi \mathcal{G}(\xi_i,\tilde{\xi},\xi).
% % \]

% % \subsection{2 samples}
% % Assume we have two samples such that $\|\xi_1\| = \|\xi_2\| = 1$, and $\xi_1^\top \xi_2 = \Sigma$. Let $D_1 = D(\xi_1),D_2 = D(\xi_2), D = D_1D_2$. We have:
% % \[
% % \mathring{\K^{(1)}}_1(\xi_1,\xi_2) = \Sigma \big(\mathcal{G}(\xi_1,\xi_2) + \mathcal{G}(\xi_2,\xi_1)\big) + 2\mathcal{G}(\xi_2,\xi_1). 
% % % \mathring{\K^{(1)}}_i(\xi_1,\xi_2) = 2(\Sigma + 1) \Big(\mathcal{G}(\xi_2,\xi_1) + \mathcal{G}(\xi_1,\xi_2)\Big). 
% % \]

% % \[
% % f(x) = \frac{a^\top \phi(z)}{\sqrt{m}},~~~z = bg,~~~   g(x) = \frac{v^\top \phi( y)}{\sqrt{n}},~~~y = ux\\
% % \]
% % \[
% % dg &=& b^\top (\phi'(z)\odot a)\\
% % dy &=& dg\frac{\phi'(y)\odot v}{\sqrt{n}}\\
% % \nabla_u &=& \chi\frac{dyx^\top}{\sqrt{n}}\\
% % \delta \phi(\tilde{y}) &=& \sqrt{n}\Big(\phi(\tilde{y} - \eta \chi \frac{dyx^\top \tilde{x}}{\sqrt{n}}) - \phi(\tilde{y}) \Big)\\
% % &=& -\eta \chi \phi'(\tilde{y})\odot (dyx^\top \tilde{x})\\
% % \nabla_v &=& \chi\frac{dg\phi(y)}{n}\\
% % g'_1 - g' &=& \frac{v^\top \delta \phi(\tilde{y})}{\sqrt{n}} - \chi\frac{dg\phi(y)^\top \phi(\tilde{y})}{n\sqrt{n}} + \chi\frac{dg\phi(y)^\top \delta\phi(\tilde{y})}{n^2}\\
% % \frac{v^\top \delta \phi(\tilde{y})}{\sqrt{n}} &=& -\eta \chi v^\top \big[\phi'(\tilde{y})\odot \big(dg\frac{\phi'(y)\odot v}{n}x^\top \tilde{x}\big)\big]\\
% % &=& -\eta \chi dg \mathbb{E}\big[\phi'(Z^y)\phi'(Z^{\tilde{y}})\big]x^\top \tilde{x}
% % \]



% % \section{Kernel Distilation}

% % Consider a dataset $\{x_i\}_{i=1}^n,x_i\in \mathbb{R}^{d}$. Our setup is a teacher student setup where we are given a teacher (trained) network, and we would like to distill the knowledge from the teacher model to a student model. Let us consider the simplest possible setup where the teacher model contains a single linear convolutional layer, while the student model is a linear fully connected model with a single layer. $f_s(x) = vux$ denote the student network, and $f_t(x) = v_tu_t*[x]$ where $u \in \mathbb{R}^{ n \times d},v \in \mathbb{R}^{k \times n}$. Let us compare the two NTKs:
% % \[
% % k_1(x,x') &=& x^\top x \|v\|^2 + x^\top ww^\top x'\\
% % k_2(x,x') &=& \sum_{i,j}[x_i]^\top[x'_j]\sum_kv_{i,k}v_{j,k} + \sum_i [x_i]^\top \sum_j\theta_j \theta_j^\top [x'_i].
% % \]


% % \section{Example - Mean Field parametrization}
% % We will use the same working example:
% % \[
% % f(\xi) = V^\top x(\xi),~~~x(\xi) = \phi\big(g(\xi)\big),~~~g(\xi) = U\xi.
% % \]
% % where $U = u, V = \frac{v}{n}$, and a learning rate $\eta = n$. The gradients are given by:
% % \[
% % dg(\xi) &=& D(\xi)\frac{v}{\sqrt{n}}\\
% % \nabla_u f(\xi) &=& \frac{dg(\xi) \xi^\top}{\sqrt{n}}\\
% % \nabla_v f(\xi) &=& \frac{x(\xi)}{n}\\
% % \nabla_{u,v}f(\xi) &=& \frac{D(\xi)\otimes \xi}{n}.
% % \]
% % At initialization in the infinite width limit, it is clear that $\mathring{\K} = 0$:
% % \[
% % \]
