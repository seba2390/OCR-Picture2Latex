%\section{Modeling Time Series Data with Model-driven and Data-driven Components}
\section{Modeling Time Series Data with Mathematical Models and Data-driven Learning}

%\note{ Don't put method, scheme, etc in the title. people knows it is a method, or a scheme. Don't put novel, which only gives people bad impression. You need to convince people it is novel with your description, but not to claim it is novel.  It triggers the anger.}



%\note{You need to pay attention how English sentences are written formally from paper reading. Your writing is too oral and informal. Also comparing my writing and your previous writing to see the difference. See why I reorganize your writing each time to make the logic more natural.}

%Generally, statistical models can well represent stable data with slow variations, but are not good at tracking dynamic data with fast changes. \rev{On the other hand,  Purely relying on machine learning techniques to track data, however, may suffer from long training time and low interpretability. In order to more accurately model the time series data while reducing the complexity and increasing the interpretability, we explore the use of both  model-driven  and data-driven approaches. 
 
 Generally, statistical models can well represent stable data with slow variations, but are not good at tracking dynamic data with fast changes. 
On the other hand,  without assuming the data format, learning directly from data can more flexible represent data. However, if there is no knowledge on the data,  it may suffer from long training time. It is also hard to control the learning process and explain the results.
 %\rev{On the other hand,  without assuming the data format, learning directly from data can more flexible represent data. However, if there is no knowledge on the data,  it may suffer from long training time. It is also hard to control the learning process and explain the results.} 
 In order to more accurately model the (time series) data while reducing the complexity and increasing the interpretability, we explore the use of both  model-driven  and data-driven approaches. 
 
 
 

%\note{You are not decomposing a data set to multiple parts, but each data item to multiple parts, right?} \note{Yes! Not  decomposing a data set but for each data item to multiple parts.}
%\rev{In order to more accurately model the data while reducing the complexity and increasing  interpret-ability, fully explore the information of the data itself and enhance the interpret-ability of the model, some model-driven + data-driven approaches can be considered. In this section, assuming we just have one time series data, our major goal is to boost the current model-driven or data-driven methods just based on the single time series data we have without introducing any other information. Primarily we have two different directions to achieve our goal: The first one is to introduce the data decomposing method. \sout{According to the learning abilities  of statistical time series model and machine learning model}, we decompose the data set into different parts. For each part, we use the specific model with the best learning capacity to train and model. The motivation here is we believe that statistical methods and machine learning methods both have their own advantages and limitations. After the data is decomposed, choosing the optimal modeling method according to the characteristics of different parts can make full use of the advantages of different learners, improve the learning ability of the entire time series data, and make the prediction more accurate. }


We first introduce a decomposition-based method to exploit both types of techniques.  We decompose each data item into multiple parts, and for each part, we choose a specific model or learning format that can best represent its data features. Given the tradeoff between methods that are purely based on models and purely driven by data, if the statistics of data are known, it may help to %speed up the training process and  
better understand the data characteristics. Therefore, as a second method, we propose to extract the statistics of data and incorporate them into the pure data-driven learning framework to improve the performance. 
%For the third method, we will design a mixed model that can concurrently exploit the decomposition-based method and the extraction-based method.

%We first introduce a decomposition-based method to exploit both types of techniques.  We decompose each data item into multiple parts.  For each part, we choose a specific model that can best represent the data based on the features of the part. \rev{The performance of statistic models is often limited by pre-setting the parameters of models. While} neural networks are known to be able to more flexibly model the data \rev{without assuming the} data format, without any knowledge on the data, \rev{it} may take a long time for the training process to converge and it is \rev{also} hard to explain and control the network learnt. If the statistics of the data are known, \rev{it may} help to speed up the training process \rev{and  better understand the} data characteristics. \rev{Therefore,} as a second method,  we propose to extract the statistics of data and incorporate them into the pure neural-network based models to improve the learning capability. For the third method, we will design a mixed model that can concurrently exploit the decomposition-based method and the extraction-based method.



%\note{You may need to compare the onverge time when the error threshold is set up, not the number of epoches is fixed. Also, you may need to compare with the case the model-based method with the model parameters are learning, not obtained through statistics.}  


%Another direction is to use statistical models to mine new valuable information or statistics to improve the learning ability of machine learning models. The usual data-driven supervised learning method mainly lies in learning the mapping relationship between input variables and output variables. While Model-based statistical methods are very concerned about the generation of certain statistics, and use different statistics to measure the distribution of the population or the sample and certain conditions of the sample itself. For example, statistical researchers will use average to assess overall quality, variance to measure fluctuations in things, or mode to judge mainstream opinions, and so on. Therefore, we want to use the statistical model to find valuable statistics first then feed these statistics into the data-driven learner to enhance the learning ability.This is our motivation of second direction. }

%\rev{In this section, section 3.1 is the method of first direction, section 3.2 is the method of second direction and section 3.3 is a boosting method based on the two directions. The performance of different methods will be shown on Section 4.}


\subsection{Method 1: Modeling Time Series Data with Decomposition}


Time series data can be divided into a stable part and an unstable part. Generally, the stable part can be more easily modeled, while it is often hard to  accurately model the unstable part. Further, the stable part can be categorized  into two types, linearly stable  and non-linearly stable. From the model features we introduce earlier, we could choose to model the linearly stable part of the data with ARIMA and the non-linearly stable part with GARCH. In order to capture all possible features in practical data, we can represent the data with  ARIMA-GARCH and Machine Learning techniques (ML) together, so both stable part and and unstable part of the data can be tracked. 
%Here we use "ML" to represent the Machine Learning techniques.   

A few ML approaches have been introduced to represent time series data. 
%\del{From our preliminary studies, adding ML into the model does not always help. An improper learning method may produce negative effect. Thus, the selection of viable ML method is crucial.}
LSTM is a  popular ML method to deal with time series with a recurrent neural network. We use  LSTM to learn the unstable part,
%\del{and SVR can better deal with complex and chaotic time series data due to its use of kernel techniques.} 
%\del{For the learning of unstable part, as it is more chaotic and irregular, SVR may outperform LSTM.} 
%We use  LSTM as the primary machine learning technique to model the unstable part. %\del{compare the results with single used SVR and LSTM, respectively.}\del{but will still compare the results with the use of LSTM.} 
%del{Taking $LSTM$ model as an example, if we include the parameters into the model representation, data can be} 
and represent the overall data with $ARIMA(p,d,q)-GARCH(P,Q)-LSTM$. It is not difficult to see that this complete data representation can reduce to $ARIMA(p,d,q)-LSTM$ when there are only linear stable part and unstable part in the time series, that is ($P=0,Q=0$). The complete $ARIMA-GARCH-LSTM$ model  will reduce to the $GARCH($P$,$Q$)-LSTM$ model if there is not linearly stable part in the time series, that is ($p=0,q=0$). Next we we introduce the detailed formats of these three representations. 

%\rev{However machine learning techniques with the ability to solve time series problems are suitable here. The improper learning method may produce negative effect. Therefore, the selection of viable ML method is a vital point. In this paper, we prefer to use the LSTM and SVR(Support Vector Regression) as the primary machine learning techniques. LSTM is a typical recurrent neural network with good ability to handle time series problem; while SVR has a strong ability to deal with the more complex and chaotic series data due to the kernel techniques it used. Because the unstable part is more chaotic and irregular, SVR may outperform LSTM.  If we include the parameters into the model representation, this data can be represented with $ARIMA(p,d,q)-GARCH(P,Q)-ML$. It is not difficult to see that this complete data representation can reduce to $ARIMA(p,d,q)-LSTM$ when there are only linear stable part and unstable part in the time series, that is ($P=0,Q=0$). The complete ARIMA-GARCH-LSTM model  will reduce to the GARCH($P,Q$)-LSTM model if there is no linear stable part in the time series, that is ($p=0,q=0$). Next we we introduce the detailed formats of these three representations.}


%Finally we compare ARIMA-GARCH-SVR with ARIMA-GARCH-LSTM, ARIMA-GARCH, LSTM and SVR to show the decomposition method with right machine learning algorithm will have the best performance.

%LSTM If we include the parameters into the model representation, this data can be represented with $ARIMA(p,d,q)-GARCH(P,Q)-ML$. Next we we introduce the detailed formats of these three representations. 

%The stable part refers to that can be modeled by the mathematical model like ARIMA and GARCH models; while the unstable part is that can't be modeled by those mathematical model. Further, the stable part can be divided into linear stable part and nonlinear stable part. Linear stable part is that can be explained by ARIMA model while the nonlinear stable part is that can be explained by GARCH model. Genrally, a hybrid ARIMA-GARCH and LSTM method can be used to model the  time series. However, ARIMA(p,d,q)-GARCH(P,Q)-LSTM model will reduce to ARIMA(p,d,q)-LSTM when there are only linear stable part and unstable part in the time series, that is (P=0,Q=0); and ARIMA-GARCH-LSTM model will reduce to GARCH(P,Q)-LSTM model if there is no linear stable part in the time series, that is (p=0,q=0).

\paragraph{A. Complete data representation with ARIMA-GARCH-LSTM}

As a general format, a time series $y_t$ can be represented as:
\begin{eqnarray}
     y_t = S_t + N_t = LS_t + NS_t + N_t
\end{eqnarray}
where $S_t$ is the  stable part, $N_t$ is the unstable part, $LS_t$ is the linearly stable part and $NS_t$ is the non-linearly stable part.  $S_t$ can be modeled by ARIMA-GARCH and $N_t$ can be learnt through $LSTM$. In this case, $y_t$ can be expressed more completely as:
\begin{eqnarray}
&& y_t = \mu_0  + u_t + \widehat{LSTM}, \nonumber \\
&& u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}},\nonumber\\
&& e_t = z_t{\sqrt{h_t}}, \quad  z_t\sim N(0,1), \nonumber \\
&& h_t = w+\sum_{j=1}^{P}{\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
\end{eqnarray}

where $y_t$ is a stationary time series, and $\mu_0$ is a constant. The stable part $u_t$ follows the $ARIMA-GARCH$ model and the unstable part  is learnt through $LSTM$. The random error $e_t$ is a function of $z_t$, which follows the normal distribution,  and $h_t$ is the GARCH term  (i.e. conditional variance). The parameters $\alpha_i$, $\theta_j$, $\beta_i$ and $\lambda_j$ are the weights. 

                    
\paragraph{B. ARIMA-LSTM}

If a time series just includes a linearly stable part and an unstable part, the time series can be represented as
\begin{eqnarray}
     y_t = LS_t +  N_t,
\end{eqnarray}
where $LS_t$ can be modeled by ARIMA model. The equation above can be expressed as
\begin{eqnarray}
&& y_t = \mu_0 + u_t + \widehat{LSTM} \nonumber\\
&& u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}
\end{eqnarray}
%\begin{eqnarray}
%    u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}, e_t\sim N(0,1) 
%\end{eqnarray}
\paragraph{C. GARCH-LSTM}

If a time series just includes a non-linearly stable part and an unstable part, the time series can be represented as
\begin{eqnarray}
     y_t = NS_t +  N_t
\end{eqnarray}
where $NS_t$ can be modeled by the GARCH model. The equation above can be expressed as
\begin{eqnarray}
&& y_t = \mu_0 + u_t + \widehat{LSTM}, u_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber \\
&& h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}} 
\end{eqnarray}

%\begin{eqnarray}
 %     u_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber
%\end{eqnarray}
%\begin{eqnarray}
 %     h_t = w+\sum_{j=1}^{P} %{\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
%\end{eqnarray}
%\rev{If LSTM model is sued to track the residual part, the expression is similar, and} we only need to  replace $\widehat{SVR}$ with $\widehat{LSTM}$ in the equations above \rev{to represent} the the ARIMA-GARCH-LSTM model.
%To investigate the efficiency of our decomposition method, we explore both one-step and multi-step prediction. For one-step prediction, we only make prediction for the next step. While for multi-step prediction, a \del{rolling}\rev{recursive} %\note{ or calling it "recursive"?} 
%procedure is taken, where the predicted value of the next step is used as the new input for the following prediction. %\del{Figure X shows the multi-steps rolling prediction \del{algorithm} based on our decomposition method.} 
%\note{Yes, A rolling forecast is a type of financial model that predicts the future performance of a business over a continuous period, based on historical data.That is, it relies on an add/drop approach to forecasting that drops a month/period as it passes and adds a new month/period automatically.}







\subsection{Method 2: Sequential Neural Network with Statistic Extraction}
 Good statistical values of a time series can help to better understand and capture the characteristics  of data. The average, median, mode and quantile are some example statistics parameters that can reflect the distribution of a data set and are used the most. 
 %However, in time series, if one wants to use some statistics to help us model and predict, it is difficult to get help from these commonly used statistics. An important reason is that they all have only one value.
 However, if these statistics are only drawn from existing data and set to fixed values, they can not well reflect the time varying features of time series data. If the statistics themselves can be modeled as  time series,  they may be fed as additional input variables to help better train the sequential neural network.
 %But if there exits a statistics that changes over time, in another word, if there is a statistics as time series data, then this kind of statistics can be fed as another input variable into the sequential neural network for training. 
 
 Apart from variables commonly used as inputs to the sequential neural network, we add in the statistics extracted from the data series as additional inputs. The GARCH term $h_t$ is a good candidate to choose. As a conditional variance, it can reflect the volatility of the sequence at different time and is essential for describing and predicting the changes of sequence. To explain our design principles, we use LSTM as an example neural network and the GARCH term $h_t$ as the example statistics. 
 %a conditional variance that can be  attracts our attention. 


  We use the past values of both $y_t$ and $h_t$ to train LSTM and predict the future values of $y_t$.  That is, besides past values of $y_t$ (i.e., ${y_{t-1}, y_{t-2},...,y_{t-k}}$), we add past values of the GARCH term $h_t$ (i.e., ${h_{t-1}, h_{t-2},...,h_{t-k}}$) as another input: %\note{For prediction, you may start from $y_{t+1}$} \note{The reason I use $y_t$ here is to be consistent with the previous expression. In time series, it's  normal to express in the way of $y_t$ = ....}
 
% \paragraph{Method I of Multi-variables LSTM based on statistics extraction}
% \rev{Apart from the variable of time series data in the past, the GARCH term $h_t$ will be another variable to train the LSTM. In method I, past values of $y_t$ are the first input and the past values of $h_t$ will be the second input. Specifically, ${y_{t}, y_{t-2},...,y_{t-k1}}$ is the first input and ${h_{t-1}, h_{t-2},...,h_{t-k2}}$ is the second input. Method I is designed by the common sense, where we use both the past information of $y_t$ and $h_t$ to train the LSTM and predict $y_t$ in the future. The equation can be expressed as below:}
 \begin{eqnarray}
    {y_{t}} = LSTM(y_{t-1},y_{t-2},...,y_{t-k};h_{t-1},h_{t-2},...h_{t-k}) + \varepsilon_t
\end{eqnarray}
where $\varepsilon_t$ is the random error. For the convenience of expression, we call the model above as LSTM-GARCH model. 

%\note{we don't do the recursive method in method 2 cause LSTM didn't have a good performance}

%\note{You only used LSRM in method 2.}

%\rev{Like that in method 1, we explore both  one-step  and  multi-step  predictions to show the performance of  method 2.} 

 %\rev{In method II, besides past values of $y_t$ and $h_t$, we will also add the future values of $h_t$ to train the sequential neural network. As a statistic variable, $h_t$ reflects the fluctuation of $y_t$, but is also more stable thus can be better predicted than the raw data.
 %a current to current mapping between $h_t$ and $y_t$ will better capture the current data states. 
 %In this case, to predict ${y_{t+k},y_{t+k-1},...,y_{t}}$,} we will not only use ${y_{t-1},y_{t-2},...,y_{t-k1}}$ and ${h_{t-1},h_{t-2},...h_{t-k2}}$, but also ${h_{t+k},h_{t+k-1},...,h_{t}}$. \rev{Although this design is easy to realize in the training phase, if multi-step prediction is needed,  we don't have the future values of ${h_{t+k},h_{t+k-1},...,h_{t}}$ in the testing phase and practical applications. To address the issue, we apply another LSTM to predict the future values of $h_t$ based on past $h_t$ values.} 
 %we train another multi-steps single-variable LSTM to  predict $h_t$ in the future. Therefore, in method II, we have trained two LSTM. The first one is trained to predict $h_t$; while the second one utilizes  the past values of  $y_t$ and the past to current values of $h_t$ as variables to train to predict $y_t$.  
 %The algorithm of method II is shown below and the structure is shown in Fig. 
 
 

% \paragraph{Method II of Multi-variables LSTM based on statistics extraction}
% \rev{In the Method I above, the past information both of $y_t$ and $h_t$ are used to predict the future $y_t$. But in method II, we want to add the present values of $h_t$ into the past $h_t$ to train our model. The reason to do so is that $h_t$ itself reflects the  fluctuation in current time of $y_t$. A current to current mapping between $h_t$ and $y_t$ will be a more logical relationship. For example, to predict ${y_{t+k},y_{t+k-1},...,y_{t}}$, we use not only  ${y_{t-1},y_{t-2},...,y_{t-k1}}$ and ${h_{t-1},h_{t-2},...h_{t-k2}}$, but ${h_{t+k},h_{t+k-1},...,h_{t}}$. In the training phase, this design is easy to implement but in the testing phase, we don't have the future values of ${h_{t+k},h_{t+k-1},...,h_{t}}$ because we assume we just have the information before t. \sout{However, to predict the future in test stage, the future values of $h_t$ are needed but we have no information of future $h_t$.} In order to solve this problem, we train another multi-steps single-variable LSTM to  predict $h_t$ in the future. Therefore, in method II, we have trained two LSTM. The first one is trained to predict $h_t$; while the second one utilizes  the past values of  $y_t$ and the past to current values of $h_t$ as variables to train to predict $y_t$.  The algorithm of method II is shown below and the structure is shwon in Fig. }
 
 
 
 
 
 
% The mathematical expression of method II is as follows:

 %\begin{equation}
   % {y_t} = LSTM(y_{t-1},y_{t-2},...,y_{t-k};h_t,h_{t-1},h_{t-2},...h_{t-k}) + \varepsilon_t
%\end{equation}
 
 
 
 
 %\note{ I still need some time to determine Method 3}
%\subsection{Method 3: Hybrid  Time Series Model \rev{with  Data Decomposition and Statistic Feature Extraction}}

%With data decomposition,  ARIMA-GARCH is applied to model the stable part of the data, and LSTM is used to track the unstable part of the data. In the method of exploiting statistic feature extraction, LSTM-GARCH takes the statistic-based time series $h_t$ as an additional input to predict $y_t$. To take full advantage of both types of method, we introduce a hybrid  model  Hybrid-LSTM-GARCH to exploit the use of both.   In this model,  a time series $y_t$ is first divided into a stable part $S_t$ and an unstable part $N_t$. Then $S_t$ is modeled by ARIMA-GARCH and  $N_t$ is modeled by LSTM-GARCH. Rather than predicting $y_t$ directly, the statistics $h_t$ is applied with LSTM to predict  $N_t$. In the example case that   LSTM-GARCH is used,  the hybrid model is \rev{briefly} represented as

%\rev{Hybrid  model here takes advantage of both the decomposing and statics extracting method. In section 3.2, the LSTM-GARCH method utilizes the new statistics $h_t$ to help predict $y_t$; and in section 3.1, for the complete version of model, after decomposing the data, ARIMA-GARCH is used to deal with stable part and LSTM is used to deal with unstable part. After carefully thinking about the two methods above, we found a new method that can take full advantage of the  two methods. Take the LSTM-GARCH-I model in section 3.2 for example. First of all, still dividing the time series $y_t$ into stable part $S_t$ and unstable part $N_t$. $S_t$ is molded by ARIMA-GARCH first; then the $N_t$ (not $y_t$) are modeled by LSTM-GARCH.That is, we use the new statistics $h_t$ to help predict  $N_t$ instead of $y_t$. We call this model as Hybrid-LSTM-GARCH model, which is represented as follosw:}

%\begin{equation}
 %   y_t = S_t + N_t
%\end{equation}
%\begin{equation}
 %   N_t = LSTM(N_{t-1},N_{t-2},...,N_{t-k};h_{t-1},h_{t-2},...,h_{t-k})+\varepsilon_t
%\end{equation}

%Where $\varepsilon_t$ is the random error.
%\note{Why here you only predict one step, while in the previous section you predict multiple steps? The paper does not have consistent notations everywhere.}



%\subsection{\sout{\rev{Sequential neural network based on statistical statistics feature extraction II}}}
%\sout{Based on method , a hybrid LSTM-GARCH model will be builded up to predict $y_t$. The difference between method IV and method II is method IV will take the current value of $h_t$ into consideration to predict $y_t$,} \note{Why?} that is,
%\begin{equation}
    %y_t = LSTM(y_{t-1},y_{t-2},...,y_{t-n};h_t,h_{t-1},h_{t-2},...h_{t-n}) + \varepsilon_t
%\end{equation}
%\sout{For the multiple steps prediction, method IV can be expressed as:}
%\begin{equation}
    %{y_t,y_{t+1},...,y_{t+m}} = LSTM(y_{t-1},y_{t-2},...,y_{t-n};h_{t+m},...,h_t,h_{t-1},h_{t-2},...h_{t-n}) + \varepsilon_t
%\end{equation}

%\sout{Take one step prediction for example, GARCH values $h_t$ is a single time series like $y_t$. ARIMA-GARCH model was built up first to $y_t$; then make prediction for $h_t$ by LSTM model. Next, the current GARCH value $h_t$ was added into LSTM-GARCH to predict $y_t$, which is shown in (23).} \note{At the beginning of this section you need to introduce problem and motivation and design consideration.}