\section{Performance Evaluation}
We compare the performance of our model with several reference models on the prediction of stock  data. 

\subsection{Experimental Setup}
\subsubsection{Data sets}
As typical time series data, stock prices are commonly used in data analyses. We choose three data sets on "Open Price" of different stocks in our performance studies, with each  set containing the data of one year. All the data set can be obtained from  \url{https://eoddata.com/stockquote/NASDAQ/AMZN.htm.}   %The stock data  \rev{have} large fluctuations as well as randomness.
The first data set is the daily open stock price of ”AMZON(AMZN)” from 01-02-2019 to 12-18-2019. The second data set is the daily open stock price for "Fidelity National Information Services (FIS)" from 01-02-2019 to 12-18-2019. The third one contains the "Federal Home Loan Mortgage Corp(FMCC)" Company from 01-02-2018 to 12-04-2018. We divide each data set into two parts, $91\%$ data are put in the training set and the remaining $9\%$ is used as the test set. Each training set has 214 samples and each test set has 21 samples. In order to further investigate the performance of our proposed methods, we  divide the test data into three subgroups: test set 1 with one week's data from day 1 to day 7, test set 2 with two weeks' data from day 1 to day 14 and test set 3 with three weeks' data from day 1 to day 21. The first data sample in the test set is considered as the data from Day 1. The first data set is represented as $y_t$ and used as the primary example to show the performance of our methods. 

\subsubsection{Preparation for ARIMA-GARCH Model } Statistic models are often impacted by the stationary level of the data. We use ARIMA-GARCH model in our method 1. As a first step, we check if data are stationary.
 A time series {$x_t$} is strictly stationary if $f(x_1,...,x_n) = f(x_{1+h},...,x_{n+h})$, where $n$ and $h$ are positive integer numbers and $f(\bullet)$ denotes a function. ARMA-GARCH can only deal with stationary data, while ARIMA-GARCH can transfer  non-stationary data into stationary ones.


In statistics, Augmented  Dickey-Fuller Test(ADF) test is the most popular method to help check if a time series is stationary, and the null hypothesis of ADF test indicates that the time series is not stationary.  The p-values obtained by the test is used to decide whether we should reject the null hypothesis, so the times series is considered to be stationary. If p-value is bigger than 0.05, we will accept the null hypothesis; otherwise, we will reject the null hypothesis. In our ADF test of data, the p-value is 0.6135,  so we accept the null hypothesis and conclude that the data set is not stationary. Then we transfer the data set $y_t$ to the difference form $\triangle$$y_t$. From the ADF test of $\triangle$$y_t$,  we get the p-value 0.01, so we can reject the null hypothesis and conclude that $\triangle$$y_t$ is stationary. Although $y_t$ is not stationary, its 1 time difference is, so the parameter $d$ in the ARIMA(p,d,q)-GARCH(P,Q) model is 1. Next we introduce our procedures in constructing the ARIMA-GARCH model for $y_t$. 

\subsubsection{ARIMA(p,d,q)-GARCH(P,Q) Model}
 We use R Studio to build up the ARIMA-GARCH model for the data. Using the "auto.arima" function from 'tseries' library in the studio and the AIC criterion, we determine that $p=5, d=1, q=2$ for ARIMA (p,d,q). We further check whether  the variance of the residuals are homogeneous to determine if there exists an arch effect. If the variance is not homogeneous,  we need to build an GARCH model to capture the data dynamics. For the dataset ${y_t}$, we form the ARIMA(5,1,2)-GARCH(1,1) model based on the AIC criterion.



%\rev{We further conduct} the model diagnostic test to see whether this model is suitable and the residuals of this model have no correlation. \note{how to determine suitable? What residual of the model means?}
%The correlation here refers to  linear correlation but the higher order correlations like squared correlation are not included. \note{Why only needs linear check? All these statements read random.} Ljung-Box test is a powerful tool to \rev{perform} this check. The null hypothesis of Ljung-Box test is the residuals have no auto-correlation. \note{I don't know what you want to say in this sentence. This sentence also has a wrong grammar, and not readable.} The p-values of the Ljung-Box test \note{What is the relationship between this test and ADF test?}  from lag 2 to 20 are all bigger than 0.05. We will accept the Ljung-Box test and conclude \rev{that} ARIMA(5,1,2) model is suitable. \note{It is unclear what you mean acceptable, and why it is suitable, and what you mean suitable.}

%\note{You put many buzz words without explanations, and also don't provide any reference.}




To use the model for prediction, we first build the ARIMA(p,d,q)-GARCH(P,Q)  model using the training data. More specifically,  we first use the training data ${y_1,y_2,...,y_m}$ to predict $y_{m+1}$. Then the true value of $y_{m+1}$ will be added and we use ${y_1,y_2,...,y_m,y_{m+1}}$ to predict $y_{m+2}$, %which is  still based on the same ARIMA(p,d,q)-GARCH(P,Q) structure obtained in the training stage. 
where the coefficients $\alpha_i$ and $\theta_j$ of ARIMA(p,d,q)-GARCH(P,Q) are updated but not the model structure $(p,d,q)$ and $(P,Q)$.
%\subsubsection{Proposed decomposition methods and baselines}

\subsubsection{Setting of the decomposition method}

%\note{I am working on revising the this section now}

%\note{Are you done with the beginning part of this section?}

%\note{ what I left is the performances studies part}

We use LSTM  as  the baseline method to directly learn the data distribution and make one step forward prediction.  The "Keras" version '2.3.1', a high-level neural network API written in Python, is used to build up LSTM. 
 %Keras" is a high-level neural networks API, written in Python, which is capable of running on top of Tensorflow. 
%\del{ In LSTM method, given 
%the input and the output,  the weights of input gates, cell states, forget gates and output gates need to be learnt }\note{People know this.} 
We choose " Mean squared Error" as the loss function and  $tanh$ as activation function.
%to do the training.  %In the SVR method, the penalty parameter C of the error term, the  $\epsilon-$insensitive loss function and the kernel function are the three key parameters need to be learnt. 

In our proposed ARIMA-GARCH-LSTM, we apply ARIMA-GARCH to model the stable data components, and LSTM  to learn the distribution of the residual data ${r_t}$.  
%\del{In the training stage, LSTM is used to train ${r_t}$, where ${r_1,r_2,...,r_k}$ serve as the input and ${r_{k+1}}$ is the output. }
To predict ${y_{t+1}}$, the predicted ${r_{t+1}}$ is added to the part of ${y_{t+1}}$ that is predicted with ARIMA-GARCH.
%to get the final data.



\subsubsection{Setting of the method based on statistics extraction} The GARCH term--$h_t$, obtained from the equation 8, is a statistics that can reflect the volatility of a time sequence at the time $t$. In our proposed statistics extraction method, to help capture the dynamic of $y_t$, the ARIMA-GARCH model is first built up for the data to obtain $h_t$, which is then used as a new feature to input into the LSTM framework with $y_t$.

%\subsubsection{Metrics}


To evaluate the effectiveness of our proposed method, we adopt four common metrics:  Mean Squared Error(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE),and Mean Absolute Percentage Error (MAPE).  For all the metrics, the lower the value, the better the performance of the corresponding models. Using $y$ to denote the ground truth and $\hat{y}$ to denote the corresponding forecast,  the four metrics based on $n$ predictions are defined as follows: 
\begin{eqnarray}
&&  MSE=\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2,  MAE = \frac{1}{n}\sum_{i=1}^{n}{\mid\hat{y_i}-y_t\mid}, \nonumber\\
&& RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}({\hat{y_i}-y_i})^2} \nonumber\\
&& MAPE = \frac{100\%}{n}\sum_{i=1}^{n} \mid\frac{\hat{y_i}-y_i}{y_i}\mid
\end{eqnarray}
%\rev{where $y$ denotes all the ground truths and $\hat{y}$ are the corresponding forecasts. $n$ is total number of predictions.}
%\note{Do you need to introduce how you get the statistics and what are the statistics? Or these statistics are varying over time as well? }
\subsection{Result Analysis}

%\note{You need to introduce you experiment setup, metrics and datasets used that are common used for all studies at the beginning. You need to cite the dataset if it is publicly available.}

\subsubsection{Results of Decomposition Method}
%\note{This belongs of experiment setting. If it is common to all methods, move it to the beginning part.} 
%\del{We compare the performance of different methods in making one-step forward prediction. Figure show the predictions' curves  and  Table \ref{tab1} shows the prediction results of our proposed models and the baseline models for all the four test sets.}
%\del{Tables \ref{tab1}  compares the prediction accuracy of  ARIMA-GARCH-LSTM with the results of ARIMA-GARCH and LSTM.}
Table \ref{tab1} shows that ARIMA-GARCH-LSTM  outperforms existing purely model-based scheme  ARIMA-GARCH and purely data-driven scheme LSTM  in all metrics. Taking results in data set 1 as an example, MSE, RMSE, MAE and MAPS of our proposed model improve $66.56\%$, $42.17\%$, $50.57\%$ and $50.35\%$ %and  $38.63\%$, $21.66\%$, $27.97\%$ and $27.61\%$ 
respectively compared with LSTM and $30.97\%$, $16.92\%$, $20.80\%$ and $20.71\%$ respectively compared with ARIMA-GARCH.    The results demonstrate that our decomposition-based method can effectively integrate the statistic model with the purely data-driven scheme to largely improve learning performance.
%\note{Why  ARIMA-GARCH-LSTM is better than ARIMA-GARCH-SVR? Explain the possible reason.}
%ARIMA-GARCH-LSTM is the best model compared with ARIMA-GARCH and LSTM. The reaso why our proposed models will outperform the baselines is we always use the suitable models to deal with the suitable data.  




 %\rev{Table \ref{tab2} shows the result of the following week.} \note{Why do you show the following week? Any particular reason, or just another set of data?}
%Compared with the Table 1, 

%\rev{Further, comparing the results in other test sets of Table \ref{tab1}.} In test set 3, MSE, RMSE, MAE and MAPS of the proposed model improve \rev{$66.56\%$, $42.17\%$, $50.57\%$ and $50.35\%$} %and  $38.63\%$, $21.66\%$, $27.97\%$ and $27.61\%$ 
%\rev{respectively compared with LSTM and} \rev{$30.97\%$, $16.92\%$, $20.80\%$ and $20.71\%$} respectively compared with ARIMA-GARCH.  

Comparing the results across the subsets of the test data, the performance improvement of the test set 1 is the biggest.   For the test set 3, the improvements of MSE, RMSE, MAE and MAPE are $40.19\%$, $22.66\%$, $28.77\%$ and $28.42\%$ respectively compared with LSTM and $8.43\%$, $4.31\%$, $11.30\%$ and $11.22\%$ respectively compared with ARIMA-GARCH model. This is because we build our model on the base of  ARIMA(p,d,q)-GARCH(P,Q). From the assumption of ARIMA-GARCH model  equation (6) and our preliminary study, its residual has a zero mean. The predicted value  $\hat{y_t}$ is equal to the summation of predicted ARIMA-GARCH value $\hat{AG}$ and the predicted residual $\hat{r_t}$, i.e., $\hat{y_t}$ = $\hat{AG}$ + $\hat{r_t}$. 
%Using $E(\bullet)$ to denote the expectation function, 
Accordingly, we have $E(\hat{y_t})$ = $E(\hat{AG})$ + $E(\hat{r_t})$ = $E(\hat{AG})$. If the test set has a long time duration, the overall performance of ARIMA-GARCH-LSTM will approach that of ARIMA-GARCH.% \del{In a word, in test set 1 with short length of data, the proposed model shows much more superiority than ARIMA-GARCH model  while in test set 3(the whole data set) it will be  close to the performance of ARIMA-GARCH. }
%\rev{This result shows  if the  test set is long, the expectation of ARIMA-GARCH-LSTM predictions will approach to that of ARIMA-GARCH, so the overall performance of ARIMA-GARCH-LSTM will approach to that of ARIMA-GARCH. In a word, in test set 1 with short length of data, the proposed model shows much more superiority than ARIMA-GARCH model  while in test set 3(the whole data set) it will be  close to the performance of ARIMA-GARCH. }



Figures \ref{AGL7} and \ref{AGL28}  show the prediction over time for the data in the test sets 1 and  3 respectively. 
%\rev{Figure 2 shows the predictions curves in test set 3.} \
ARIMA-GARCH-LSTM is shown to be the closest to the ground-true curve in all results, and the accuracy is higher in the prediction of data in the testset 1.
%Therefore, our proposed method has better performances in all test sets especially in the first test set.}
Moreover, we observe a first-order-lag in all curves, which means the prediction on the day $t$ is more close to the result in the day $t-1$. Taking figure ~\ref{AGL7} as an example, if we move the true data from 1-6 to the position 2-7, the results from all methods have the same trend of change. The lag problem is  common  in the time series analysis, as the prediction results of all methods have a tendency to approach those of the previous step.  




%\rev{the MSE, RMSE, MAE and MAPS of the proposed model improve $66.56\%$, $42.17\%$, $50.57\%$ and $50.35\%$ compared with LSTM model and $30.97\%$, $16.92\%$, $20.80\%$ and $20.71\%$ compared with ARIMA-GARCH model.} \rev{The different performance improvement} is because that our proposed \rev{method is} built up on the base of ARIMA(p,d,q)-GARCH(P,Q) model.  When \rev{new data are} added, we just recompute the coefficients of ARIMA(5,1,2)-GARCH(1,1) model, i.e., \rev{$\alpha_i$,$\theta_j$,}but we \rev{don't} change the structure, i.e., $(p,d,q)$ and $(P,Q)$ of this model. \note{Aren't the parameters of these structures changed?} Nevertheless, as time goes by, the structure may have some changes \rev{due to the fact that financial data have more volatility. When long sequence of the new data added into the training data, the whole structure of this data may change}, so the accuracy of  ARIMA-GARCH model reduces and the residual data may have a large variation. \note{Your parameters for the statistics are not from historical data but only from recent data? Then you need to justify it and mention the finance data re volatile and you cannot use too long history data.} All these factors will affect the performances of  ARIMA-GARCH-LSTM \del{and ARIMA-GARCH-SVR} model. %But for the one step forward prediction of next one week, few data are added to ARIMA-GARCH, so the performance of ARIMA-GARCH will not be affected a lot.   \note{This is confusing. You just said for the next week, it changes a lot. Now you said it does not change a lot. Can't you be more strict in your writing?} 

%\note{Again, you are attracting people to attack you by emphasizing show-period prediciton. People will ask what the result for long-term prediction?} Table \ref{tab3} lists the percentage improvement between different methods. \rev{Compared to  ARIMA-GARCH, the improvement of ARIMA-GARCH-SVR is larger than that of ARIMA-GARCH-LSTM, which indicates that SVR can better track the variation of residual data than LSTM.} 

%\note{I am not sure if you want to emphasize one-step prediction, if you don't study multi-step. When you said one step-forwarding of a week, it is unclear you predict a whole week or for each day you make a prediction.}


 


%And that is why the predictions for the following one week are have more improvements. 

%Figure ~\ref{LSTM38}shows the prediction results from "11/05/2018" to "01/01/2019" for ARIMA-GARCH-LSTM, ARIMA-GARCH and LSTM models. Figure ~\ref{LSTM7} shows the next one week prediction  results from "11/05/208" to "11/13/2018". 

%The percentage improvements of our proposed methods compared with the baseline models are given in Table \ref{tab3}. From  Table \ref{tab3}, one can see LSTM and SVR both have  huge improvements when combined with ARIMA-GARCH method Although LSTM seems having worst performance compared with other models, it still has good ability to predict the tendency of the complex data. In our proposed method, we use LSTM to predict the unstable part, which 

% Please add the following required packages to your document preamble:
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
% Please add the following required packages to your document preamble:
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 156.93 & 12.53 & 10.13 & 0.57 \\ \cline{2-6} 
Test Set 1 & LSTM & 323.92 & 18.00 & 16.23 & 0.90 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{108.32} & \textbf{10.41} & \textbf{8.02} & \textbf{0.45} \\ \hline
 & AG & 155.33 & 12.46 & 10.56 & 0.60 \\ \cline{2-6} 
Test Set 2 & LSTM & 280.57 & 16.75 & 14.38 & 0.81 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{108.06} & \textbf{10.40} & \textbf{8.34} & \textbf{0.47} \\ \hline
  & AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{263.40} & \textbf{16.23} & \textbf{12.35} & \textbf{0.70} \\ \hline
\end{tabular}
\vspace{-5pt}
\caption{\label{tab1} Comparison of the decomposition-based ARIMA-GARCH-LSTM with reference schemes on different test sets}
\end{table}

%\caption{\label{tab1}Metrics Analysis of Test Set 1 and Test Set 2}


%\begin{table}[]
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
% &  & MSE & RMSE & MAE & MAPE \\ \hline
% & AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
%Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
% & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{263.40} & %\textbf{16.23} & \textbf{12.35} & \textbf{0.70} \\ \hline
% & AG & 238.25 & 15.44 & 12.39 & 0.70 \\ \cline{2-6} 
%Test Set 4 & LSTM & 367.18 & 19.16 & 15.71 & 0.88 \\ \cline{2-6} 
% & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{225.32} & %\textbf{15.01} & \textbf{11.32} & \textbf{0.64} \\ \hline
%\end{tabular}
%\caption{\label{tab2}Metrics Analysis of Test Set 3 and Test Set 4}
%\vspace{-15pt}
%\end{table}

%Metrics Analysis of Test Set 3 and Test Set 4
%\begin{table}[]
%\caption{\label{tab1}Metrics Analysis of Test Set Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.001318 & 0.036298 & 0.024794 & 2.142506 \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%AG-LSTM & \textbf{0.001241} & \textbf{0.035234} & \textbf{0.022233} & \textbf{1.941214} \\ \hline
%SVR & 0.001501 & 0.038747 & 0.029667 & 2.510208 \\ \hline
%AG-SVR & \textbf{0.001287} & \textbf{0.035871} & \textbf{0.023604} & \textbf{2.051793} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}[]
%\caption{\label{tab2}Metrics Analysis of Following week Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.000282 & 0.016798 & 0.013751 & 1.076193 \\ \hline
%LSTM & 0.001359 & 0.036868 & 0.033778 & 2.576865 \\ \hline
%AG-LSTM & \textbf{0.000244} & \textbf{0.015636} & \textbf{0.011847} & \textbf{0.929590} \\ \hline
%SVR & 0.000273 & 0.016518 & 0.012974 & 1.007235 \\ \hline
%AG-SVR & \textbf{0.000182} & \textbf{0.013501} & \textbf{0.009888} & \textbf{0.772916} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}[]
%\caption{\label{tab3}Percentage Improvement of the Following week Predictions}
%\vspace{-10pt}
%\footnotesize
%\begin{center}

%\begin{tabular}{|c|c|c|c|c|}
%\hline
%$\%$ & MSE & RMSE & MAE & MAPE \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-LSTM \\ vs. AG\end{tabular} & 13.35 & 6.92 & 13.85 & 13.62 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-LSTM \\ vs. LSTM\end{tabular} & 82.01 & 57.59 & 64.93 & 63.93 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-SVR \\ vs. AG\end{tabular} & 35.40 & 19.63 & 28.09 & 28.18 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-SVR \\ vs. SVR\end{tabular} & 33.20 & 18.27 & 23.78 & 23.26 \\ \hline
%\end{tabular}
%\end{center}
%\end{table}

\begin{figure}
  \vspace{-10pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/AGL7.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of AG-LSTM and its baselines for test set 1}
\label{AGL7}
  \vspace{-5pt}
\end{figure}




\begin{figure}
  \vspace{-5pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/AGL28.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of AG-LSTM and its baselines for test set 3}
\label{AGL28}
  \vspace{-5pt}
\end{figure}


\begin{figure}
  \vspace{-5pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/LG7.png}
  \end{center}
  \vspace{-10pt} 
  \caption{\footnotesize Predictions of LSTM-GARCH and its baselines for test set 1}
\label{LG7}
  \vspace{-10pt}
\end{figure}

\begin{figure}
  \vspace{-10pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/LG28.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of LSTM-GARCH and its baselines for test set 3}
\label{LG28}
  \vspace{-20pt}
\end{figure}



\subsubsection{Results of Learning with Statistic Extraction}
We compare the performance  of LSTM-GARCH model with those of ARIMA-GARCH model and LSTM model in  Table \ref{tab2}. shows the prediction results for LSTM,ARIMA-GARCH and LSTM-GARCH for all the test sets. LSTM-GARCH is seen to perform the best for all metrics in all testsets. For the testset 1, compared with the LSTM model, the improvement of MSE, RMSE, MAE and MAPE are 
$61.41\%$,$37.88\%$,$47.48\%$,$47.29\%$ respectively. This demonstrates that the introduction of additional statistic features into the learning platform has a big impact on the performance of LSTM model. Compared with  ARIMA-GARCH, the improvements of MSE, RMSE, MAE and MAPE are 
$20.34\%$,$10.77\%$,$15.85\%$,$15.82\%$, which are still big. 
%LSTM-GARCH still has bigger improvement. }

%The LSTM-GARCH model's prediction curve is the closet one to the true values
\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 156.93 & 12.53 & 10.13 & 0.57 \\ \cline{2-6} 
Test Set 1 & LSTM & 323.92 & 18.00 & 16.23 & 0.90 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{125.01} & \textbf{11.18} & \textbf{8.53} & \textbf{0.48} \\ \hline
 & AG & 155.33 & 12.46 & 10.56 & 0.60 \\ \cline{2-6} 
Test Set 2 & LSTM & 280.57 & 16.75 & 14.38 & 0.81 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{149.39} & \textbf{12.22} & \textbf{10.05} & \textbf{0.57} \\ \hline
 & AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{285.41} & \textbf{16.89} & \textbf{13.70} & \textbf{0.77} \\ \hline
\end{tabular}
\vspace{-5pt}
\caption{\label{tab2} Comparison of the extraction-based LSTM-GARCH with it's baselines on different test sets}
\vspace{-20pt}
\end{table}



%\caption{\label{tab3}Metrics Analysis of LSTM-GARCH and it's baselines for test set 1 and test set 2}


%\begin{table}[]
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
 %&  & MSE & RMSE & MAE & MAPE \\ \hline
 %& AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
%Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
% & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{285.41} & \textbf{16.89} & \textbf{13.70} & \textbf{0.77} \\ \hline
% & AG & 238.25 & 15.44 & 12.39 & 0.70 \\ \cline{2-6} 
%Test Set 4 & LSTM & 367.18 & 19.16 & 15.71 & 0.88 \\ \cline{2-6} 
% & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{231.80} & \textbf{15.22} & \textbf{11.95} & \textbf{0.67} \\ \hline
%\end{tabular}
%\caption{\label{tab4}Metrics Analysis of LSTM-GARCH and it's baselines for %test set 3 and test set 4}
%\end{table}

%Metrics Analysis of LSTM-GARCH and it's baselines for test set 3 and test set 4



%\del{The last row of Table 4 is the percentage improvement the LSTM-GARCH model has compared with LSTM model for the whole test data.} \rev{ The improvement of RMSE is $27\%$, while the improvements of MSE, MAE and MAPE are all over $40\%$. 
Further, comparing the results in different test sets of table \ref{tab2}, the performance improvement of test set 1 is  the biggest. Over the test set 3,  compared with the LSTM model, the values of MSE, RMSE, MAE and MAPE  improve  $35.2\%$, $19.50\%$, $20.95\%$, and $20.54\%$.  Although the improvements are smaller than those for the test set 1, they are still large. This shows our second method has a  great potential in improving the performance of LSTM model. %The improvement reduction over long data set is also the result of utilizing the statistics of ARIMA-GARCH in LSTM-GARCH.}  
%and it will be affected by ARIMA-GARCH to some extent. 
%When the test set is long, the overall effects of ARIMA-GARCH model to our model will increase, \note{what do you mean the overall effects of ARIMA-GARCH model to our model will increase?} so the performance of LSTM-GARCH has the tendency to approach ARIMA-GARCH. 


%The MSE, RMSE, MAE and MAPE of ARIMA-GARCH increased a lot from test 2 to test 3 in Table 2, which shows a large fluctuation of the real data in this periods. The ARIMA-GARCH prediction reduced 


%$61.41\%$,$37.88\%$,$47.48\%$,$47.29\%$ compared with LSTM model and %$20.34\%$,$10.77\%$,$15.85\%$,$15.82\%$ compared with ARIMA-GARCH. 


%The reason is similar to that in the last part. When a long sequence of the new data added into the training data, the whole structure of this data may change, so the accuracy of  ARIMA-GARCH model reduces and the GARCH term($h_t$) obtained by ARIMA-GARCH model may have a large variation.
%This demonstrates that the introduction of additional statistic feature into the learning platform has a big impact on the performance. \del{}LSTM model.}

In Figures \ref{LG7} and \ref{LG28},  our proposed method has the best performance and is closest to the curves with true values in all testsets,  and the improvement is more obvious in the first testset.
%shows the predictions curves in test set 1 and  }\rev{Figure \ref{LG28} shows the predictions curves in test set 3.} \rev{Both figures \ref{LG7} and \ref{LG7} show the curve of LSTM-GARCH is the closest one to the true values' curve. Therefore, our proposed method has better performances in all test sets especially in the first test set.}

In Table \ref{tab3}, we further compare the performance between the methods 1 and 2.  ARIMA-GARCH-LSTM not only outperforms the baselines ARIMA-GARCH and LSTM in all metrics, but  is also superior to our proposed  LSTM-GARCH model. This is because that the decomposition-based method could better exploit the features of data to represent different components with appropriate strategies,  while LSTM-GARCH just utilizes the GARCH term to help improve the LSTM performance. However, the second method is more concise and convenient to use, while the first method needs more data pre-processing.% \note{both need to build arima-garch, right?   Yes!} 

%extracts the data information more sufficiently while LSTM-GARCH just utilizes the GARCH term to help improve the LSTM performance. However, the second method is more concise and easy to use while the first method need more pre-work to do. 

%while LSTM-GARCH just utilizes the GARCH term to help improve the LSTM performance. However, the second method is more concise and easy to use while the first method need more pre-work to do. 

%\rev{Compared with the decomposition-based method, the method of learning with statistic extraction has more freedom and can better track the practice data.} Finally, we conclude that the extraction method not only  improves the performance of LSTM but have better sustainability compared with the decomposition method. \note{Why?You cannot simply claim.}

%We can conclude our statistics extraction method did improve a lot. 



%\begin{table}[]
%\caption{\label{tab4}Metrics Analysis of Test Set Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
 %& MSE & RMSE & MAE & MAPE \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & 0.001142 & 0.033791 & 0.021945 & 1.897688 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\\ vs.LSTM($\%$)\end{tabular} & 47.26 & 27.37 & 45.73 & 43.35 \\ \hline
%\end{tabular}
%\vspace{-5pt}
%\end{table}


%\begin{table}[]
%\caption{\label{tab5}Metrics Comparisons among extraction, decomposition and baseline methods }
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.001318 & 0.036298 & 0.024794 & 2.142506 \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%AG-LSTM & 0.001241 & 0.035234 & 0.022233 & 1.941214 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSGM\\ -GARCH\end{tabular} & \textbf{0.001142} & \textbf{0.033791} & \textbf{0.021945} & \textbf{1.897688} \\ \hline
%\end{tabular}
%\vspace{-10pt}
%\end{table}


%\begin{figure}
 % \vspace{5pt}
 % \begin{center}
%\includegraphics[width=2.5 in]{images/LG38.png}
 % \end{center}
 % \vspace{-10pt}
 % \caption{\footnotesize Open Price Predictions}
%\label{LG38}
 % \vspace{-10pt}
%\end{figure}




\subsubsection{Robust Analysis}
We use two more data sets to test the superiority of our schemes and the robustness of the performance. The first one is the stock open Price of "Fidelity National Information Services(FIS)" from 01-02-2019 to 12-18-2019 and the second one is the stock open Price of "Walmart(WMT)" from 01-02-2019 to 12-18-2019. %\del{The metrics analysis of method 1, method 2 and their baselines for all the test sets are put into one table.} 
Table 4 and 5 shows the result of "FIS" and "WMT", respectively.  %\del{One can see}\note{Don't use "one can see"} 
For all the test sets, our proposed two schemes perform much better than the ones purely based on model or purely learnt from data.  
%\del{Therefore, we conclude that both the decomposition method and the statistics extraction method 
%are exceptional and deserve more attention.}

\subsubsection{Conclusion}
The increasing demands on big data analysis call for more advanced and accurate learning techniques. Model-based learning and data-based learning are two tools that have been playing important roles. We systematically study the features of data and tools and propose two different methods to concurrently exploit the powerful statistical models and machine learning techniques. We illustrate our scheme using time series data as an example. In the first method, we decompose the time series data into two parts, and use statistical models to represent the stable part and the data-driven learning to track the remaining unstable part. In the second method, we first extract statistic features of data, and then feed them together data as input into the machine learning model. Extensive performance studies demonstrate the superiority of our proposed methods in all experimental settings.  We expect our work will motivate more studies to explore the seamless integration of mathematical models and data-driven schemes to enable reliable and interpretable learning.      
%Both our proposed methods
%, either based on decomposition or based on statistics extraction, 
%are shown to be promising in bridging the gap between model-based and data-driven schemes and integrate the two to provide an overall higher learning performance.
 
%\pagebreak


%The increasing demands on big data analysis call for more advanced and accurate techniques. Model based learning and data based learning are two tools playing an important role in digital era. They are not opposed to each other but can promote each other. The research on how to take  advantages of their own strengths and put them together to play  more greater value is attracting and promising. Taking time series as the research object, we propose two different methods to incorporate the powerful statistical models and machine learning models. The first one is a decomposition based method. The time series is decomposed different parts, statistical models will model the stable part first and then hand to learning models the left unstable parts. The second one is a statistics extraction based machine learning method. Statistics models are first established but not for direct prediction but to extract valuable statistics. Then we feed the extracted statistics as new feature into machine learning models to produce better results. Empirical studies demonstrate the superiority of the proposed methods in all experimental settings.  Both our proposed methods, either based on decomposition or based on statistics extraction, are promising to bridge the gap between model-based and data-driven schemes and integrate the two to provide an overall higher learning performance.
 





%We use two more data sets \note{Did you introduce this somewhere?} to test the superiority of our proposed \rev{methods. Tables} 6 and 7 show the prediction results \rev{for a  longer time period and one week respectively. Both our methods perform better than the baseline models.} \del{both in Table 6 and Table 7.} The results in Table 7 have much more \rev{performance} improvement compared with that in Table 6. The reason is similar to data set 1.  \note{You didn't explain this earlier, between a long data set and short one. I didn't put whole data set to avoid problem.} \note{Also, why you didn't show the performance of extraction-based method?} 

\begin{table}[]
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
{Test Set 1} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 108.32 & 10.41 & 8.02 & 0.45 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 125.01 & 11.18 & 8.53 & 0.48 \\ \hline
{Test Set 2} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 108.06 & 10.40 & 8.34 & 0.47 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 149.39 & 12.22 & 10.05 & 0.57 \\ \hline
{Test Set 3} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 263.40 & 16.23 & 12.35 & 0.70 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 285.41 & 16.89 & 13.70 & 0.77 \\ \hline
\end{tabular}
\vspace{-5pt}
\caption{\label{tab3} Comparison between AG-LSTM and LSTM-GARCH for all test sets.}
\vspace{-5pt}
\end{table}

\begin{table}[]
\vspace{-10pt}
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 2.33 & 1.53 & 1.30 & 0.98 \\ \cline{2-6} 
 & LSTM & 2.29 & 1.52 & 1.20 & 0.90 \\ \cline{2-6} 
{Test Set 1} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 2.03 & 1.43 & 1.17 & 0.88 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 1.70 & 1.30 & 1.18 & 0.89 \\ \hline
& AG & 1.95 & 1.40 & 1.16 & 0.87 \\ \cline{2-6} 
 & LSTM & 2.55 & 1.60 & 1.27 & 0.95 \\ \cline{2-6} 
{Test Set 2} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 1.93 & 1.39 & 1.06 & 0.79 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 1.54 & 1.24 & 1.04 & 0.77 \\ \hline
 & AG & 2.11 & 1.45 & 1.13 & 0.84 \\ \cline{2-6} 
 & LSTM & 2.41 & 1.55 & 1.28 & 0.95 \\ \cline{2-6} 
{Test Set 3} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 2.10 & 1.45 & 1.05 & 0.78 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 1.93 & 1.38 & 1.11 & 0.82 \\ \hline
\end{tabular}
\vspace{-5pt}
\caption{\label{tab4} Comparison of different schemes over data set 2}
\vspace{-15pt}
\end{table}

\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 2.82e-4 & 1.68e-2 & 1.37e-2 & 1.08 \\ \cline{2-6} 
 & LSTM &  0.84e-4 & 2.90e-2 & 2.47e-2 & 1.89 \\ \cline{2-6} 
{Test 1} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 1.99e-4 & 1.41e-2 & 1.20e-2 & 0.94 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 1.60e-4 & 1.27e-2 & 1.13e-2 & 0.89 \\ \hline
& AG & 2.32e-4 & 1.52e-2 & 1.27e-2 & 1.01 \\ \cline{2-6} 
 & LSTM & 8.71e-4 & 2.95e-2 & 2.68e-2 & 2.11 \\ \cline{2-6} 
{Test 2} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 1.77e-4 & 1.33e-2 & 1.13e-2 & 0.91 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 1.69e-4 & 1.30e-2 & 1.15e-2 & 0.92 \\ \hline
 & AG & 4.82e-4 & 2.19e-2 & 1.67e-2 & 1.39 \\ \cline{2-6} 
 & LSTM & 9.20e-4 & 3.03e-2 & 2.75e-2 & 2.22 \\ \cline{2-6} 
{Test 3} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 4.43e-4 & 2.11e-2 & 1.59e-2 & 1.33 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 4.19e-4 & 2.05e-2 & 1.57e-2 & 1.31 \\ \hline
\end{tabular}
\vspace{-5pt}
\caption{\label{tab5} Comparison of different schemes over data set 3}
\end{table}


