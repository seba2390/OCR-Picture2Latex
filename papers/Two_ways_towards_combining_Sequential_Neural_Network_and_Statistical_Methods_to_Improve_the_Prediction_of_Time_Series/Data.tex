\subsection{Data-driven Learning}

Rather than following a pre-established model, modern machine learning and deep learning methods derive the knowledge directly from data  
%Taking deep supervised learning as an example, it mainly starts from the data itself, sets the corresponding activation function and loss function, and passes the given input and output during the training phase. 
without assuming their distribution format, and can well follow nonlinear data. However, they often suffer from large computational cost, weak interpretability,  and bias when data are unbalanced. In the case of time series, %the Support Vector Regression(SVR) \note{Is SVR for time series data only or general data?} \note{SVR can solve general nonlinear regression problems and it can be extend to solve time series problem}
sequential neural networks, such as Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are often applied. 

%\paragraph{A. Support Vector Regression}

%Support vector machine (SVM) is originally proposed to solve the binary classification problems,  and it follows the VC dimension theory and the principle of minimizing the structural risk.
%It was originally used (support vector machine classification), and 
%It is extended to solve the function approximation problem, and called  support vector regression (SVR). 
%With SVR, \del{a kernel technique can be used to convert an input non-linear sample set into a high-dimensional space to improve the sample separation.}
%With SVR, the input \rev{data samples can be converted into ones in a higher} dimensional space that \rev{a nonlinear regression problem in the original space becomes the linear regression problem} in the new space.  

%\note{It is better to put the remaining in the model design part. My earlier question was because you were presenting it as a classification. I haven't modified this part, and will after your moving to fit in the context} 
%\rev{For a time series, the  relationships between ${y_t}$ and its previous values ${y_{t-1},y_{t-2},...,y_{t-k}}$ is actually a regression problem. However, through the ARIMA-GARCH model we know that this is not a linear regression problem, let alone the relationship between unstable parts. As for the unstable part, represented by $N_t$, the relationships between $N_t$ and ${N_{t-1},N{t-2},...,N_{t-k}}$ are more complicated and chaotic.} \rev{For a data set $S$ with a complex nonlinear regression relationship, it} can be mapped to the high dimensional feature space(Hilbert space) through the nonlinear mapping $\phi(x)$ \rev{and the kernel techniques to make their relationships linear. Therefore, SVR becomes our choice to deal with time series problems. }

%the feature sets $X = {x_1,x_2,..,x_k}$
%For \rev{a} sample set $S$ that is linearly inseparable in the original space, the feature $x$ can be mapped to the high-dimensional feature space (Hilbert space) through the nonlinear mapping $\phi(x)$ \rev{so that $\phi(x)$ is Linearly separable.} \del{in the  the high-dimensional feature space. Therefore, the nonlinear problem of S in the original space is well solved. This is a support vector machine nonlinear regression, called support vector regression (SVR).}



\paragraph{A. Recurrent Neural Network}

RNN \cite{manaswi2018rnn}\cite{tokgoz2018rnn}\cite{poulos2017rnn}includes a neural network to provide a very straight-forward but effective way of handling time series or other sequential data. RNN is recurrent, where the same function is performed for each time stage and the output of the previous time stage is the input of the next stage. RNN can model a sequence of data so that each sample  depends on previous ones. Backward Propagation Algorithm is often used to train RNN. 

%Fig~\ref{RNN} shows a typical RNN structure.  Given the sequence of input x = (x1,...xt), RNN iteratively calculates the hidden layer \rev{vector} \note{multiple vectors? Why do you use "vectors"? You use this plural or single form randomly everywhere. It gives people the bad impression of very lousy writing. } $h^* = (h^*_1;...; h^*_t)$ and the output layers \rev{vector} \note{multiple vectors?} $y = (y_1;...; y_t)$ as follows  for $t = 1$ to $T $:
%\begin{equation}
 %    h^*_t = H(W_{xh^*}x_t + W_{h^*h^*}h^*_{t-1}+b_h^*), \hspace{0.2in}  y_t = W_{h^*y}h^*_t + b_y,
%\end{equation}
%\begin{equation}
%    y_t = W_{hy}h_t + b_y
%\end{equation}
%where  $h^*_t$ is the  hidden vector,  $b_h^*$ is the hidden bias, $H$ is the hidden layer function, $W_{h^*h^*}$ is the weight at previous hidden state and $W_{xh^*}$ is the weight at the current state, $y_t$ is the output state, $W_{h^*y}$ is the weight at the current hidden state and $b_y$ is the output bias. RNN can model a sequence of data so that each sample can be assumed to be dependent on previous ones. 
%Backward Propagation Algorithm is often used to train RNN. 

%\begin{wrapfigure}[10]{r}{2.5in}
  %\vspace{-25pt}
 % \begin{center}
%\includegraphics[width=2.5 in]{images/RNN.png}
  %\end{center}
  %\vspace{-20pt}
  %\caption{\footnotesize RNN Structure}
%\label{RNN}
  %\vspace{-10pt}
%\end{wrapfigure}

\paragraph{B. Long Short-Term Memory}
As the back-propagated error  either explodes or vanishes during the propagation in the training process, RNN cannot process very long time sequence.  To alleviate the problem, 
LSTM \cite{song2020time}\cite{sagheer2019time}
is introduced. It has two transmission states, the  cell state $c_t$  and the hidden state $h^*_t$. $c_t$  changes slowly, while $h^*_t$ changes faster. 
%Fig~\ref{LSTM} shows the structure of LSTM with a single layer cell.
An LSTM time stage consists of many recurrently
memory cells. Each cell contains three multiplicative gate units, named input gate, forget gate and output gate.

%\begin{wrapfigure}[10]{r}{2.5in}
  %\vspace{-45pt}
 % \begin{center}
%\includegraphics[width=2.5 in]{images/LSTM.png}
 % \end{center}
  %\vspace{-20pt}
  %\caption{\footnotesize Basic LSTM Structure}
%\label{formulas}
 % \vspace{-10pt}
%\end{wrapfigure}

%Simply put, LSTM can perform better in longer sequences than normal RNN.
%LSTM has two transmission states, the  cell state $c_t$  and the hidden state $h^*_t$. $c_t$  changes slowly, while $h^*_t$ changes faster. 
%Fig~\ref{LSTM} shows the structure of LSTM with a single layer cell.
%An LSTM \rev{temporal} layer \note{temporal or spatial layer?}consists of many recurrently memory cells and each cell contains three multiplicative gate units, including the input, forget and output.

%The LSTM architecture is implemented through the following composite functions:
%\begin{equation}
   % i_t = \sigma(W_{xi}x_t + W_{h^*i}h^*_t-1 + b_i), \hspace{0.1in}  f_t = %\sigma(W_{xf}x_t + W_{h^*f}h^*_{t-1} + b_f), \hspace{0.1in} \mbox{and}  %\hspace{0.1in} o_t = \sigma(W_{xo}x_t + W_{h^*o}h^*_{t^*-1}+b_o)
%\end{equation}
%\begin{equation}
%    f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)
%\end{equation}
%\begin{equation}
    %c_t = f_tc_{t-1} + i_ttanh(W_{xc}x_t + W_{h^*c}h^*_{t-1}+b_c), %\hspace{0.1in} \mbox{and}  \hspace{0.1in}  h^*_t = o_ttanh(c_t). 
%\end{equation}
%\begin{equation}
%    o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1}+b_o)
%\end{equation}
%\begin{equation}
%    h_t = o_ttanh(c_t)
%\end{equation}
%where $\sigma$ is the logistic sigmoid function, and $i$, $f$, $o$ and $c$ stand for input gates, forget gates, output gates and cell vectors. In LSTM, weights are divided into 3 sets, input weights ($W_{xi}$, $W_{xf}$, $W_{xc}$, $W_{xo}$ $\in{R^{N \times M}}$), recurrent weights ($W_{h^*i}$,$W_{h^*f}$ ,$W_{h^*c}$,$W_{h^*o}$ $\in{R^{N \times N}}$) and bias weights ($b_i$,$b_f$,$b_c$,$b_o$ $\in{R^N}$).  $N$ is the number of LSTM cells, and $M$ is the dimension of the input.
