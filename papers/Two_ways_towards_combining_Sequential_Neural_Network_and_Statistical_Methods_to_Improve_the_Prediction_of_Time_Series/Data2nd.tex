\subsection{Data-driven \del{Learning}\rev{Neural Networks}}
\vspace{-0.10in}
In modern machine learning and deep learning methods, rather than following a pre-established model, knowledge is directly derived from the data with \rev{various types of neural networks}.  
%Taking deep supervised learning as an example, it mainly starts from the data itself, sets the corresponding activation function and loss function, and passes the given input and output during the training phase. 
\rev{Although this method does not make assumption on the data distribution format and can} well follow nonlinear data, it suffers from large computational cost, weak interpretability,  \rev{and bias when  data  are unbalanced.} In the case of time series, the sequential neural networks, such as Recurrent Neural Network (RNN)\cite{madan2018predicting}, Long Short-Term Memory (LSTM)\cite{reddy2018prediction} and Gated Recurrent Units (GRU)\cite{fu2016using}, are often applied. 
\paragraph{Recurrent Neural Network}
\vspace{-0.10in}

\begin{figure}
%  \vspace{-25pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/RNN.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize RNN Structure}
\label{RNN}
  \vspace{-10pt}
%\end{wrapfigure}
\end{figure}


RNN \del{includes a  neural network that} can provide a very straight-forward but effective way of handling time series or other sequential data. \rv{As shown in Fig~\ref{RNN}, RNN follows a recurrent structure,} where the same function is performed for each time stage and the output of the previous time stage is the input of the next stage.  Given the sequence of input x = (x1,...xt), RNN iteratively calculates  \rev{the vector $h^* = (h^*_1;...; h^*_t)$ of the hidden layer and  $y = (y_1;...; y_t)$ of the output layer \del{for $t = 1$ to $T $} as follows }:
\begin{equation}
     h^*_t = H(W_{xh^*}x_t + W_{h^*h^*}h^*_{t-1}+b_h^*), \hspace{0.05in}  y_t = W_{h^*y}h^*_t + b_y,
\end{equation}
%\begin{equation}
%    y_t = W_{hy}h_t + b_y
%\end{equation}
where  $h^*_t$ is the  hidden vector,  $b_h^*$ is the hidden bias, $H$ is the hidden layer function, $W_{h^*h^*}$ is the weight at previous hidden state and $W_{xh^*}$ is the weight at the current state, $y_t$ is the output state, $W_{h^*y}$ is the weight at the current hidden state and $b_y$ is the output bias. RNN can model a sequence of data so that each sample \del{can be assumed to be dependent on} \rev{depends on} previous ones. Backward Propagation Algorithm is often used to train RNN. 

%\begin{wrapfigure}[10]{r}{2.5in}

\paragraph{Long Short-Term Memory}
\vspace{-0.15in}

\begin{figure}
%\begin{wrapfigure}[10]{r}{2.5in}
%  \vspace{-45pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/LSTM.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Basic LSTM Structure}
\label{LSTM}
  \vspace{-10pt}
\end{figure}

RNN cannot process very long sequences, because \rev{the} back-propagated error  either explodes or vanishes \del{during the propagation} in the training process.  To alleviate the problem, 
LSTM is introduced. 
%Simply put, LSTM can perform better in longer sequences than normal RNN.
LSTM has two transmission states, the  cell state $c_t$  and the hidden state $h^*_t$. $c_t$  changes slowly, while $h^*_t$ changes faster. 
Fig~\ref{LSTM} shows the structure of LSTM with a single layer cell. \note {Temporal layer to spatial layer?} 
An LSTM layer \note{temporal or spatial layer?} consists of many \rev{recurrent}
memory cells and each cell contains three multiplicative gate units, \rev{{\em input,
forget and output.}} An LSTM architecture \rev{can be expressed with} the following composite functions:
\begin{equation}
    i_t = \sigma(W_{xi}x_t + W_{h^*i}h^*_t-1 + b_i), \hspace{0.1in}  f_t = \sigma(W_{xf}x_t + W_{h^*f}h^*_{t-1} + b_f), \hspace{0.1in} \mbox{and}  \hspace{0.1in} o_t = \sigma(W_{xo}x_t + W_{h^*o}h^*_{t^*-1}+b_o)
\end{equation}


%\begin{equation}
%    f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)
%\end{equation}
\begin{equation}
    c_t = f_tc_{t-1} + i_ttanh(W_{xc}x_t + W_{h^*c}h^*_{t-1}+b_c), \hspace{0.1in} \mbox{and}  \hspace{0.1in}  h^*_t = o_ttanh(c_t). 
\end{equation}
%\begin{equation}
%    o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1}+b_o)
%\end{equation}
%\begin{equation}
%    h_t = o_ttanh(c_t)
%\end{equation}
where $\sigma$ is the logistic sigmoid function, and $i$, $f$, $o$ and $c$ stand for input gates, forget gates, output gates and cell vectors. In LSTM, weights are divided into 3 sets, input weights ($W_{xi}$, $W_{xf}$, $W_{xc}$, $W_{xo}$ $\in{R^{N \times M}}$), recurrent weights ($W_{h^*i}$,$W_{h^*f}$ ,$W_{h^*c}$,$W_{h^*o}$ $\in{R^{N \times N}}$) and bias weights ($b_i$,$b_f$,$b_c$,$b_o$ $\in{R^N}$).  $N$ is the number of LSTM cells, and $M$ is the dimension of the input.
\note{Format your equation. It does not see that each row cannot hold two.}