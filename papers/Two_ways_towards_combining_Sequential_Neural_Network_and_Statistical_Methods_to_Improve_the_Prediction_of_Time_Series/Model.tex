\section{Preliminaries}
Various types of statistical models have been developed in the literature to represent the time series data for different applications. In recent years, increasing attentions are drawn to apply neural networks and \rev{machine} learning to represent complex data distributions. Before presenting our proposed methods, we review some basic statistical models commonly used to model financial data, and basic neural network structure \rev{and machine learning technique} used to represent time series data. 

\subsection{Statistic Models}
To represent the time series data, statistic models have been developed in the literature. Compared to methods using the neural networks, they can better explain the relationship between data and reveal the mechanism of change among data. In addition, model-base methods have good analytical properties, and can better calculate and theoretically prove the existence and convergence of errors. Taking daily financial stock data as an example, the data constantly change over time. Parameters, such as daily open price, close price, maximum price and minimum price, always change but have some relationships with the data of previous days. For example, the daily close price in day $t$ may be related with those in day $t-1, t-2…t-10$. Generally, the influences from closer days are bigger.  The task for model-based methods is to find and capture the strong relationship among the data in day $t$ and the data in previous days. A few models are commonly used to represent financial data. 

\paragraph{ARMA Model}

ARMA (Auto-Regressive Moving Average) is a general model used to forecast a stationary time series. ARMA ($p,q$) is a combination of Auto-Regressive(AR($p$)) process and Moving Average(MA($q$)) process. AR($p$) is generally written as 
\begin{equation}
				y_t = \mu + \alpha_1{y_{t-1}}+\alpha_2{y_{t-2}}+...+\alpha_p{y_{t-p}}+e_t,
\end{equation}
where $y_t$ is a stationary time series, $\mu$ is a constant, $\alpha_1,\alpha_2,...\alpha_p$ are the auto-correlation coefficients at lags 1,2…$p$. \rev{The residual $e_t$ is often assumed to be Gaussian white noise with the mean zero and the variance $\sigma_t$}.  To improve \rev{the accuracy of time series prediction, MA($q$) forecasts} the lagged values of the errors:
\begin{equation}
	y_t = \mu + e_t+\theta_1{e_{t-1}}+\theta_2{e_{t-2}}+....+\theta_q{e_{t-q}},
\end{equation}
where $\mu$ is the expectation of $y_t$ (usually assumed equal to zero), $\theta$ terms are the weights for prior stochastic term in time series. $e_t$ is often assumed to follow Gaussian white noises with mean zero and variance $\sigma_t$. 
%where $p$ is number of lagged observations for the auto-regression model, which uses the dependencies between an observation and the lagged observations; $q$ is number of lagged forecast residual errors for the moving average model, which uses the dependency between observations and the lagged forecast errors. 
Integrating AR($p$) and MA($q$), ARMA($p$,$q$) model is expressed as
\begin{equation}
   y_t = \mu + \sum_{i=1}^{p} {\alpha_i{y_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}.
\end{equation}

\paragraph{ARIMA Model}

 To model the non-stationary time series data,  Auto-Regressive Integrated Moving Average(ARIMA) is used to generalize ARMA model with ARMA($p$,$d$,$q$), where  differences or some nonlinear transformation (including deflating and logging) is introduced. $d$ is the number of differences needed to convert a non-stationary time series into a stationary \rev{one. An  ARIMA($p$,$d$,$q$) time series will follow ARMA($p$,$q$) model after $d$ times differences. For example, if a time series $y_t$ follows the ARIMA($p$,$d$,$q$) model, then $\triangle^d$$y_t$ follows the} ARMA($p$,$q$) model, where $\triangle^d$$y_t$ is the sequence of $y_t$ after $d$ times differences. \note{By difference, it also means d lags?}\note{ D times differences are not same with d lags.  A “lag” is a fixed amount of passing time. The $k_{th}$ lag is the time period that happened “k” time points before time i. Lag1(Y2) = Y1 and Lag4(Y9) = Y5}

\paragraph{ARCH Model}

In the ARIMA model, we assume the errors follow the homogeneous Gaussian distribution, while conditional variance is used in Auto-Regressive Conditional Heteroscedasticity (ARCH) model. The ARCH($P$) model has the following structure:
\begin{equation}
\label{ARCH}
    \begin{cases}
      y_t = \mu_t + u_t \\
      u_t = z_t{\sqrt{h_t}}\quad  z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}
    \end{cases}
\end{equation}
where the constant $\mu_t$ is usually assumed to equal the expectation of the time series. The random error $u_t$ is a function of $z_t$ with normal distribution  and the GARCH term $h_t$ (i.e. conditional variance).  $\lambda_j$ is a weight. 

\paragraph{GARCH Model and ARMA-GARCH Model}

The conditional variance functions of some residual time series often have the feature of auto-regression, where the current conditional variance is also affected by the previous conditional variance values. As a generation of the ARCH model, the GARCH($P,Q$) model follows
\begin{equation}
    \begin{cases}
      y_t = \mu_t + u_t \\
      u_t = z_t{\sqrt{h_t}}\quad z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
    \end{cases}
\end{equation}
where $h_t$ is the conditional variance. GARCH model can be integrated with ARMA model below
\begin{equation}
    \begin{cases}
      y_t = \mu_t + u_t \\
     u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}\\
     e_t = z_t{\sqrt{h_t}} \quad  z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
    \end{cases}
\end{equation}
to form ARMA($p$,$q$)-GARCH($P$,$Q$) model. If $y_t$ is not stationary, it can be replaced with the $d$ times difference series $\triangle^d$$y_t$  to form the ARIMA($p$,$d$,$q$)-GARCH($P$,$Q$) model.



