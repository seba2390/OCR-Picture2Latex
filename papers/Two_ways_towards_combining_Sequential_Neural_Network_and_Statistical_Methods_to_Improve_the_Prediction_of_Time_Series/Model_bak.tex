\section{Model-driven Learning}
\vspace{-0.10in}
Model based learning aiming to find the appropriate model contained in the data can better explain the relationship between the data and reveal the mechanism of change among the data. At the same time, it can better Calculate and prove the existence and convergence of theoretical errors and has good analytical properties. Taking financial daily stock data as an example. The financial stock is keep changing over time. For the daily stock data such as daily open price, close price, maximum price and minimum price, it will always change but have some relationships with the former days. For example, the daily close price in day $t$ may related with that in day $t-1, t-2…t-10$. However, the influences the previous daily close prices have on day $t$ may be different. $t-1, t-2$ may have the closer relationships than $t-9, t-10$ have to $t$ since $t-1, t-2$ are closer to $t$ than $t-9, t-10$. The task for model-driven learning here is to find and capture the strong relationship among the data in day $t$ and the data in previous days.
\subsection{ARMA Model}
\vspace{-0.05in}
ARMA, the acronym standing for Auto-Regressive Moving Average, is a general model aiming to forecast a stationary time series. ARMA(p,q) model is a combination of Auto-Regressive(AR($p$)) process and Moving Average(MA($q$)) process. $p$ is number of lagged observations for the auto-regression model, which uses the dependencies between an observation and the lagged observations; $q$ is number of lagged forecast residual errors for the moving average model, which uses the dependency between observations and the lagged forecast errors. 
The general expression for AR($p$) model can be written in the following linear form:
\begin{equation}
				y_t = \mu + \alpha_1{y_{t-1}}+\alpha_2{y_{t-2}}+...+\alpha_p{y_{t-p}}
\end{equation}
Where $y_t$ is a stationary time series, $c$ is a constant, $\alpha_1,\alpha_2,...\alpha_p$ are the auto-correlation coefficients at lags 1,2…$p$, $e_t$ are residuals that are Gaussian white noises series with mean zero and variance $\sigma_t$.

Moving average(MA($q$)) model improves the time series prediction through forecasting the lagged values of the errors, which can be written in the following form:
\begin{equation}
	y_t = \mu + e_t+\theta_1{e_{t-1}}+\theta_2{e_{t-2}}+....+\theta_q{e_{t-q}}
\end{equation}
Where $\mu$ is the expectation of $y_t$(usually assumed equal to zero), $\theta$ terms are the weights for prior stochastic term in time series.$e_t$ are often assumed to follow Gaussian white noises with mean zero and variance $\sigma_t$. 
Combining AR($p$) and MA($q$) models, one can get the ARMA($p$,$q$) model as follow:
\begin{equation}
   y_t = \mu + \sum_{i=1}^{p} {\alpha_i{y_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}
\end{equation}

\subsection{ARIMA Model}
\vspace{-0.05in}
Auto-Regressive Integrated Moving Average(ARIMA) model is a generalized model of ARMA($p$,$d$,$q$). ARIMA($p$,$d$,$q$) model can deal with the non-stationary time series data via differences or some nonlinear transformation including deflating and logging. $d$ is the number of differences needed to convert a non-stationary time series into a stationary. A time series following ARIMA($p$,$d$,$q$) model is equivalent to follow ARIMA($p$,$d$,$q$) model after d times differences for this series. 

\subsection{ARCH Model}
\vspace{-0.05in}
In the ARIMA model, we assume the errors follow the homogeneous Gaussian distribution; while in Auto-Regressive Conditional Heteroscedasticity (ARCH) model. In the ARCH model, the assumption of homogeneity of variance is no longer used, but conditional variance is used to build the model. The ARCH(P) model has the following structure:
\begin{equation}
    \begin{cases}
      y_t = \mu_t + u_t \\
      u_t = z_t{\sqrt{h_t}}\quad  z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}
    \end{cases}
\end{equation}
where $\mu_t$ is the constant term(usually assumed equal to the expectation of the time series), $u_t$ is the random error terms at time t, $z_t$ is a normal variable, $h_t$ is the GARCH term, which is also called conditional variance, $u_{t-j}$ is the random error at time $t-j$ and $\lambda_j$ is the corresponding weight. 

\subsection{GARCH Model}
\vspace{-0.05in}
Further, in reality, the conditional variance functions of some residual time series have the feature of auto-regression. Namely, the current conditional variance is also affected by the previous conditional variance values. Therefore, a generalized ARCH model, the GARCH model developed by Engle and Bollerslov, is  proposed to model the time series. The GARCH(P,Q) model has the following formulation.
\begin{equation}
    \begin{cases}
      y_t = \mu_t + u_t \\
      u_t = z_t{\sqrt{h_t}}\quad z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
    \end{cases}
\end{equation}
Where $h_{t-i}$ is the conditional variance at time $t-i$.
GARCH model can be integrated with ARMA model, that is:
\begin{equation}
    \begin{cases}
      y_t = \mu_t + u_t \\
     u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}\\
     e_t = z_t{\sqrt{h_t}} \quad  z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
    \end{cases}
\end{equation}
The model above is called ARMA($p$,$q$)-GARCH($P$,$Q$) model. If $y_t$ is not stationary, replace $y_t$ with its $d$ times difference series $\triangle^d$,$y_t$ and get ARIMA($p$,$d$,$q$)-GARCH($P$,$Q$) model.



