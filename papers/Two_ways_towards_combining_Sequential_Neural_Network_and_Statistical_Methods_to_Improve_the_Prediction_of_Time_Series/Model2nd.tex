\section{Preliminaries}

Various types of statistical models\cite{aamir2016modelling}\cite{gupta2019very}\cite{shbier2017swgarch}\cite{zhu2020high} have been developed in the literature to represent the time series data for different applications. In recent years, increasing attentions are drawn to apply neural networks and deep learning\cite{kashiparekh2019convtimenet}\cite{sangiorgio2020robustness}\cite{hewamalage2020recurrent} \cite{zhu2021news}\cite{zhu2020adaptive} represent complex data distributions. Before presenting our proposed methods, we review some basic statistical models commonly used to represent financial data, and basic neural network structure applied to track time series data. 

\subsection{Statistic Models}
%\vspace{-0.10in}
Compared to methods using the neural networks, statistic models can better explain the relationship between data and reveal the mechanism of change among data. In addition, model-based methods have good analytical properties, and can better calculate and theoretically prove the existence and convergence of errors. Taking daily financial stock data as an example, the data constantly change over time. Parameters, such as daily open price, close price, maximum price and minimum price, always change but have some relationships with the data of previous days. For example, the daily close price in day $t$ may be related with those in day $t-1, t-2…t-10$. Generally, the influences from closer days are bigger.  Time series models intend to find and capture the strong relationship among the data in day $t$ and the data in previous days. A few models are commonly used to represent financial data. 
\paragraph{A. ARMA Model}
\vspace{-0.05in}
Auto-Regressive Moving Average (ARMA)\cite{brockwell2002introduction} is a general model used to forecast a stationary time series. ARMA ($p,q$) combines the Auto-Regressive (AR ($p$)) process and the Moving Average (MA($q$)) process\rev{.} AR($p$) represents a stationary time series
\begin{eqnarray}
				y_t = \mu + \alpha_1{y_{t-1}}+\alpha_2{y_{t-2}}+...+\alpha_p{y_{t-p}}+e_t,
\end{eqnarray}
where $\mu$ is a constant, $\alpha_1,\alpha_2,...\alpha_p$ are the auto-correlation coefficients at lags 1,2…$p$, and the residual $e_t$ is often assumed to be Gaussian white noise with the mean zero and the variance $\sigma_t$.  To improve the accuracy of time series prediction,  MA ($q$) model takes into account  the historical impact of white noise to predict the sequential value: %\note{I am not sure why this form of MA ($q$) represents a moving average. When you first mentioned the model, it is a part related to data, which implies the data moving average, while obviously your data part didn't use moving average.}

%\note{The moving average refers to the $e_t$. From $e_t$ to $e_{t-q}$, all of them have their weights $\theta$. This is the standard form of moving average in time series}

\begin{eqnarray}
	y_t = \mu + e_t+\theta_1{e_{t-1}}+\theta_2{e_{t-2}}+....+\theta_q{e_{t-q}},
\end{eqnarray}
where $\mu$ is the expectation of $y_t$ (usually assumed equal to zero), $\theta$ terms are the weights for prior stochastic term in time series. $e_t$ is often assumed to follow Gaussian white noises with mean zero and variance $\sigma_t$. 
%where $p$ is number of lagged observations for the auto-regression model, which uses the dependencies between an observation and the lagged observations; $q$ is number of lagged forecast residual errors for the moving average model, which uses the dependency between observations and the lagged forecast errors. 
Integrating AR($p$) and MA($q$), ARMA($p$,$q$) model is expressed as
\begin{eqnarray}
   y_t = \mu + \sum_{i=1}^{p} {\alpha_i{y_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}.
\end{eqnarray}

\paragraph{B. ARIMA Model}
\vspace{-0.05in}
 To model the non-stationary time series data,  Auto-Regressive Integrated Moving Average (ARIMA)\cite{weisang2008vagaries} is used to generalize ARMA model with ARIMA ($p$,$d$,$q$), where  a difference factor or some nonlinear transformation (including power transformation and logarithmic transformation) is introduced. $d$ is the number of difference items needed to convert a non-stationary time series into a stationary one. An  ARIMA($p$,$d$,$q$) time series will follow the ARMA($p$,$q$) model after $d$ times of difference. For example, if a time series $y_t$ follows the ARIMA($p$,$d$,$q$) model, then $\triangle^d$$y_t$ follows the ARMA($p$,$q$) model, where $\triangle^d$$y_t$ is the sequence of $y_t$ after $d$ times differences. 
 
\paragraph{C. ARCH Model}
\vspace{-0.05in}
In the ARIMA model, we assume the errors follow the homogeneous Gaussian distribution, while Auto-Regressive Conditional Heteroscedasticity (ARCH) model\cite{engle1995arch} is used when errors do not follow the same distribution but change over time. The variance of the errors at time $t$ is affected by the errors before $t$, and  is thus considered to be conditional. The ARCH($P$) model has the following structure:

\begin{eqnarray}
&& y_t  =  \mu_t +  u_t,   \quad u_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1), \nonumber\\
&& h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}. 
\end{eqnarray}


%\begin{eqnarray}
%y_t  =  \mu_t +  u_t, \nonumber
%\end{eqnarray}
%\begin{eqnarray}
%u_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber
%\end{eqnarray}
%\begin{eqnarray}
%h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}.
%\end{eqnarray}

%\note{The regular way for organize the equation sets doesn't work in this template.} 

%\note{This is because you did not use "eqnarray"  format, but treat them as individual equations. You may also use the format with left braket. Search "latex math"} 
where the constant $\mu_t$ is usually assumed to equal the expectation of the time series. The random error $u_t$ is a function of $z_t$ (which follows a normal distribution)  and the GARCH term $h_t$ (which represents a conditional variance).  $\lambda_j$ is a weight. 

%\note{The paragraph below is new added to explain what is the conditional variance} 

%\note{You don't need to put these details.}

%\note{A simple proof to show the relationship of $h_t$ and the conditional variance of $u_t$}

%From the equation above, $ u_t = z_t{\sqrt{h_t}}\quad $, then $E(u_t^2) = E(z_t^2h_t).$ $z_t$ and $h_t$ are independent variables, where $z_t\sim N(0,1)$. Thus, $E(z_t^2h_t) = E(z_t^2)E(h_t)=E(h_t)$, where $E(z_t^2) = Variance(z_t) - {E(z_t)}^2=1$. If we remove the intercept term $w$ in equation $h_t$, the variance of $u_t$ can be expressed as $variance(u_t) = E(u_t^2) - {E(u_t)}^2 = E(u_t^2) = E(h_t) = E(\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}})$, where $E(u_t) = 0$. Therefore, one can conclude that the variance of random error $u_t$ at time $t$ is determined by the values of $u_t$ before $t$. Further, given the previous p steps information of $u_t$, the conditional variance of $u_t$ at time t is expressed as $variance(u_t/{u_{t-1,t-2,...,t-P}}) = E(h_t) = E(\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}) = \sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}} = h_t$. We call $h_t$ the conditional variance of $u_t$. 

\paragraph{D. GARCH Model and ARMA-GARCH Model}
\vspace{-0.05in}
The conditional variance functions of some residual time series often have the feature of auto-regression, where the current conditional variance is also affected by the previous  variance values. As a generation of the ARCH model, the GARCH($P,Q$) model\cite{francq2019garch} follows

\begin{eqnarray}
 &&  y_t = \mu_t + u_t ,   u_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber\\
 && h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}}}.
\end{eqnarray}

where $h_t$ is the conditional variance. GARCH model can be integrated with ARMA model as
\begin{eqnarray}
&& y_t = \mu_t + u_t,  u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}},\nonumber\\
&& e_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber\\
&& h_t = w+\sum_{j=1}^{P} {\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}}}.
\end{eqnarray}

%\begin{eqnarray}
 %    e_t = z_t{\sqrt{h_t}},\quad z_t\sim N(0,1),\nonumber
%\end{eqnarray}
%\begin{eqnarray}
%      h_t = w+\sum_{j=1}^{P} {\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}}}.
%\end{eqnarray}
to form an ARMA($p$,$q$)-GARCH($P$,$Q$) model. If $y_t$ is not stationary, it can be replaced with a $d$ times difference series $\triangle^d$$y_t$  to form the ARIMA($p$,$d$,$q$)-GARCH($P$,$Q$) model.



