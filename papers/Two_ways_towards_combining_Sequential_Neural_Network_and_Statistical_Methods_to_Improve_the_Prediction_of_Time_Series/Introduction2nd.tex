\section{Introduction}


In recent years, we are seeing a rapid growth of data from the increasing deployment of sensors and sources of various types. The availability of massive data calls for new and more advanced schemes to analyze the data, and apply the knowledge to  enable more intelligent and powerful applications.  Data-based (model-less) applications attempt to extract information directly from the data without preprocessing through complex models. Machine learning systems are becoming increasingly employed in complex  settings such as medicine (e.g. radiology, drug development), financial technology (e.g. stock price prediction, digital financial advisor), and even in law enforcement (e.g. case summarization, litigation prediction). Despite this increased utilization, there is still a lack of sufficient techniques to explain and interpret the decisions of these deep learning algorithms.

The convergence of data and model has been reflected from at least two aspects. First, massive data have transformed the way how system states can be estimated and predicted, which is essential for reliable system operations and decision making. Second, data also provide a venue to critically examine and refine traditional “model-based” methods for many applications. These have led to the “data-driven” methods that have greatly empowered the applications. However, a subtle issue is how to properly integrate data-driven methods with existing knowledge or models. After all, many application fields have been extensively studied for the last almost a century, and has well-established theories and models. Data, big or small, can better reflect the knowledge or help discover new knowledge, but are not expected to fundamentally change the established knowledge. Therefore,maximizing the potential of data-driven methods while at the same time respecting basic theories and knowledge to enable new and powerful applications is currently a grand challenge.

The introduction of useful new information may be helpful to  improve the performance of both the model-based and data-based learning. For example,  to predict the daily "opening price" of a stock\cite{sureshkumar2012performance}\cite{ariyo2014stock}\cite{di2016artificial}, it would be helpful to introduce additional information such as the daily "closing price", "highest price", "lowest price" and "trading volume". However, a more valuable research is how to improve the learning performance by making the best use of models and existing data without requiring new information to add in the cost and delay. Given a model already established to describe an application and the large amount of data that reflect practical states, the question is if these two can be integrated and how they can work together to provide an overall new information paradigm. 

To fundamentally advance the learning performance while being able to control and interpret the results,  we propose two methods to explore two directions of effectively integrating the model and data. We use the learning of sequential data distribution as an example to explain our design principles, while our methods can be extended to more general learning scenarios. In the first method,  data are decomposed into three parts, {\em linear stable, nonlinear stable and unstable.} The first two parts are represented with mathematical models, while the  unstable part is learnt  through pure data-based methods. Rather than purely relying on mathematical model or machine learning, we expect the model can better represent and interpret the stable data with prior knowledge of the data features, while the pure data-driven learning can help to capture the uncertainty and dynamics of data in a practical scenario.  The second method exploits data-driven learning to more flexibly capture the data distribution, while taking advantage of mathematical models to extract statistics of data and feed them as new features into the machine learning platform. The focus is on the finding of proper statistics and the way to integrate model-based and pure data-driven learning. It does not require capturing new data to obtain the additional features. %\del{Taking the time series data as an example, the ARIMA-GARCH model is often used to model the data, where the GARCH term provides the conditional variance. We  use the GARCH term as a new feature to feed into the machine learning platform to improve the model performance. }\note{I am not sure we need to put Garch here as an example.} 

In the rest of the paper, we first provide background knowledge on statistical models and machine learning methods commonly used to represent time series. We then present the details of our two methods of integrating mathematical models with the data-driven learning scheme. We compare the performance of our schemes with those purely using models or relying on data, and the results show that our schemes are very effective and achieve the best performance in all data sets.  

%\rev{In the remaining of the sections, we first provide background knowledge on some statistical models and machine learning methods. We then present the details of our two methods of integrating mathematical models with the data-driven learning scheme. In the performance study part, 3 data sets are used to test our models. We compare the performance of our models with some baselines and the results show the proposed models have the best performance in all data sets. } 

%In the performance study part, 3 data sets are used to test our models.
%\rev{In the third method, we concurrently exploit the previous two methods to further} improve the performance of the learners. On the basis of the  decomposition method, method 2 is applied to model and make prediction for the unstable part. 
%\del{ The second method starts from a completely different perspective from the first method. Not based on the decomposition of the data, but} Relying on the extraction of statistics come from mathematical models,\rev{the second method still uses} mathematical statistics models to model first, then extracts valuable statistics information, and finally feeds the extracted statistics as new features into the machine learning learner for training. The contribution here is not the method of adding new features into the machine learning learner but the thought of how to \note{incarcerate or incorporate} model based and data based learning and the way to generate and extract new features. Different from the  ways of adding new features by introducing new information to help boost the learning ability, our method acquires new features without any new information. Take time series data as an example, after the ARIMA-GARCH model established, the GARCH term, that is conditional variance, is noticed by us due to the Time-varying characteristics. Then we propose to utilize the GARCH term as a new feature to be fed into the machine learner to improve model performance. In the third method, we consider to incorporate both the first two methods to better improve the performance of the learners. On the basis of  decomposition method, method 2 is applied to model and make prediction for the unstable part. \note{I am confused again. Isn't the GARCH term part of the model? What do you mean extract statistics?} \note{Garch term is produced from Garch model, it is a type of statistics with time series characteristic.}
%The introduction of new information may be helpful to  improve the performance of both the model-based and data-based learning. However, a more valuable research is how to integrate models without introducing new information to improve the performance of the model. To achieve this goal, in this paper, we propose three methods, which give 3 directions to integrate both model based and data based learning to produce "1+1 $>$ 2" effect. The first one presents a  way of decomposing the data into different parts, such as linear stable part, nonlinear stable part  and unstable part. The mathematical models will model both the linear and nonlinear stable parts while the machine learning method will handle the unstable part. The mathematical models can account for most of data; as for the unstable part, machine learning method will take effect. The merit of this method is that it improves the predictive ability of the model without reducing model interpretation, and has better performance than the single used mathematical models and machine learning methods. The second method starts from a completely different perspective from the first method. Not based on the decomposition of the data, but relying on the extraction of statistics come from mathematical models. It still uses mathematical statistics models to model first, then extracts valuable statistics information, and finally feeds the extracted statistics as new features into the machine learning learner for training. The contribution here is not the method of adding new features into the machine learning learner but the thought of how to incarcerate model based and data based learning and the way to generate and extract new features. Different from the  ways of adding new features by introducing new information to help boost the learning ability, our method acquires new features without any new information. Take time series data as an example, after the ARIMA-GARCH model established, the GARCH term, that is conditional variance, is noticed by us due to the Time-varying characteristics. Then we propose to utilize the GARCH term as a new feature to be fed into the machine learner to improve model performance. In the third method, we consider to incorporate both the first two methods to better improve the performance of the learners. On the basis of  decomposition method, method 2 is applied to model and make prediction for the unstable part.}