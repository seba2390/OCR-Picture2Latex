\section{\rev{Modeling Time Series Data with Model-driven and Data-driven Components}}
\vspace{-0.10in}
\note{ Don't put method, scheme, etc in the title. people knows it is a method, or a scheme. Don't put novel, which only gives people bad impression. You need to convince people it is novel with your description, but not to claim it is novel.  It triggers the anger.}



\note{You need to pay attention how English sentences are written formally from paper reading. Your writing is too oral and informal. Also comparing my writing and your previous writing to see the difference. See why I reorganize your writing each time to make the logic more natural.}

\rev{In order to more accurately model the time series data while reducing the complexity and increasing  interpretability, we explore the use of both  model-driven  and data-driven approaches. Generally, statistical models can well represent stable data with slow variations, but are not good at tracking dynamic data with fast changes. Purely relying on neural network to track data, however, may suffer from long training time and low interpretability. We first introduce a decomposition-based method to exploit both types of technique.  We decompose each data item into multiple parts.  For each part, we choose a specific model that can best represent the data based on the features of the part.}


\note{You are not decomposing a data set to multiple parts, but each data item to multiple parts, right?} \note{Yes! Not  decomposing a data set but for each data item to multiple parts.}
%\rev{In order to more accurately model the data while reducing the complexity and increasing  interpret-ability, fully explore the information of the data itself and enhance the interpret-ability of the model, some model-driven + data-driven approaches can be considered. In this section, assuming we just have one time series data, our major goal is to boost the current model-driven or data-driven methods just based on the single time series data we have without introducing any other information. Primarily we have two different directions to achieve our goal: The first one is to introduce the data decomposing method. \sout{According to the learning abilities  of statistical time series model and machine learning model}, we decompose the data set into different parts. For each part, we use the specific model with the best learning capacity to train and model. The motivation here is we believe that statistical methods and machine learning methods both have their own advantages and limitations. After the data is decomposed, choosing the optimal modeling method according to the characteristics of different parts can make full use of the advantages of different learners, improve the learning ability of the entire time series data, and make the prediction more accurate. }


\rev{Statistic models are limited if the the parameters associated with the models are pre-determined. Neural networks are known to be able to more flexibly model the data without any assumption on data format. 
Without any knowledge on the data, however, it may take a long time for the training process to converge, and it is hard to explain and control the network learnt. If the statistics of the data is known, it will help speed up the training process and allow for better understanding of the data characteristics. As a second method,  we propose to extract the statistics of data and incorporate them into the pure neural-network based models to improve the learning capability. As a third method, we will design a mixed model that can concurrently exploit decomposition-based method and extraction-based method.}  

\note{This paragraph is somewhat different from what I want to express. The training time of the learning part of the first method is indeed reduced, but the training time of the second method does not decrease but will increase slightly, because it adds new features to the training. Time must be increased. So what I want to emphasize here is that after new features are added, the model's prediction accuracy will increase and the prediction ability will increase.}



%Another direction is to use statistical models to mine new valuable information or statistics to improve the learning ability of machine learning models. The usual data-driven supervised learning method mainly lies in learning the mapping relationship between input variables and output variables. While Model-based statistical methods are very concerned about the generation of certain statistics, and use different statistics to measure the distribution of the population or the sample and certain conditions of the sample itself. For example, statistical researchers will use average to assess overall quality, variance to measure fluctuations in things, or mode to judge mainstream opinions, and so on. Therefore, we want to use the statistical model to find valuable statistics first then feed these statistics into the data-driven learner to enhance the learning ability.This is our motivation of second direction. }

%\rev{In this section, section 3.1 is the method of first direction, section 3.2 is the method of second direction and section 3.3 is a boosting method based on the two directions. The performance of different methods will be shown on Section 4.}


\subsection{\rev{Modeling time series data with decomposition}}
\vspace{-0.10in}

Time series data can be divided into \rev{a} stable part and \rev{an} unstable part. \note{before a single noun, it needs to put article words a, an or the.} Generally, the stable part can be more easily modeled, while it is often hard to  accurately model the unstable part. Further, the stable part can be categorized  into two types, linear stable  and nonlinear stable. From the model features we introduce earlier, we could choose to model the linear stable part of the data with ARIMA and the nonlinear stable part with GARCH. In order to capture all possible features in practical data, we can represent the data with  ARIMA-GARCH and LSTM together, so both stable part and and unstable part of the data can be tracked. If we include the parameters into the model representation, this data can be represented with $ARIMA(p,d,q)-GARCH(P,Q)-LSTM$.  It is not difficult to see that this complete data representation can reduce to $ARIMA(p,d,q)-LSTM$ when there are only linear stable part and unstable part in the time series, that is ($P=0,Q=0$). The complete ARIMA-GARCH-LSTM model  will reduce to the GARCH($P,Q$)-LSTM model if there is no linear stable part in the time series, that is ($p=0,q=0$). Next we we introduce the detailed formats of these three representations. 

%The stable part refers to that can be modeled by the mathematical model like ARIMA and GARCH models; while the unstable part is that can't be modeled by those mathematical model. Further, the stable part can be divided into linear stable part and nonlinear stable part. Linear stable part is that can be explained by ARIMA model while the nonlinear stable part is that can be explained by GARCH model. Genrally, a hybrid ARIMA-GARCH and LSTM method can be used to model the  time series. However, ARIMA(p,d,q)-GARCH(P,Q)-LSTM model will reduce to ARIMA(p,d,q)-LSTM when there are only linear stable part and unstable part in the time series, that is (P=0,Q=0); and ARIMA-GARCH-LSTM model will reduce to GARCH(P,Q)-LSTM model if there is no linear stable part in the time series, that is (p=0,q=0).

\paragraph{Complete data representation with ARIMA-GARCH-LSTM}
\vspace{-0.10in}
As a general format, a time series $y_t$ can be represented as:
\begin{equation}
     y_t = S_t + N_t = LS_t + NS_t + N_t
\end{equation}
where $S_t$ is the  stable part, $N_t$ is the unstable part, $LS_t$ is the linear stable part and $NS_t$ is the nonlinear stable part.  $S_t$ can be modeled by ARIMA-GARCH and $N_t$ can be learnt through $LSTM$. In this case, $y_t$ can be expressed more completely as:
\begin{equation}
\begin{cases}
    y_t = \mu_0  + u_t + \widehat{LSTM} \\
    u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}}\\
    e_t = z_t{\sqrt{h_t}}, \quad  z_t\sim N(0,1)\\
    h_t = w+\sum_{j=1}^{P}{\lambda_j{e_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
\end{cases}
\end{equation}
where $y_t$ is a stationary time series, and $\mu_0$ is a constant. \rev{The stable part $u_t$ follows the $ARIMA-GARCH$ model and the unstable part $\widehat{LSTM}$ is learnt through $LSTM$. The random error $e_t$ is a function of $z_t$ which follows the normal distribution,  and $h_t$ is the GARCH term  (i.e. conditional variance).} $\alpha_i$, $\theta_j$, $\beta_i$ and $\lambda_j$ are the weights. 

                    
\paragraph{ARIMA-LSTM}
\vspace{-0.10in}
If a time series just includes linear stable part and unstable part, the time series can be represented as:
\begin{equation}
     y_t = LS_t +  N_t
\end{equation}
where $LS_t$ can be modeled by ARIMA model. The equation above can be expressed as:
\begin{equation}
\begin{cases}
    y_t = \mu_0 + u_t + \widehat{LSTM} \\
    u_t = \sum_{i=1}^{p} {\alpha_i{u_{t-i}}} + e_t+ \sum_{j=1}^{q} {\theta_j{e_{t-j}}} \\
    e_t\sim N(0,1) \\
\end{cases}
\end{equation}

\paragraph{GARCH-LSTM}
\vspace{-0.10in}
If a time series just includes nonlinear stable part and unstable part, the time series can be represented as:
\begin{equation}
     y_t = NS_t +  N_t
\end{equation}
where $NS_t$ can be modeled by GARCH model. The equation above can be expressed as:
\begin{equation}
    \begin{cases}
      y_t = \mu_0 + u_t + \widehat{LSTM}\\
      u_t = z_t{\sqrt{h_t}}\quad z_t\sim N(0,1)\\
      h_t = w+\sum_{j=1}^{P} {\lambda_j{u_{t-j}^2}}+\sum_{i=1}^{Q}{\beta_i{h_{t-i}^2}}
    \end{cases}
\end{equation}
\subsection{\rev{Sequential neural network with statistics extraction}}
\note{I will combine the two method and reorganize this section later}
 \rev{Good statistics values of a time series can help to better understand and capture the change of data. The average, median, mode and quantile can reflect the distribution of a data set and are used the most.} 
 %However, in time series, if one wants to use some statistics to help us model and predict, it is difficult to get help from these commonly used statistics. An important reason is that they all have only one value.
 \note{We cannot say these statistics do not have any help. Conventional statistics based method just calculate these values based on existing data and use these values in the model for the future data, right?}
 \rev{However, if these statistics are only drawn from existing data and set to fixed values, they can not well reflect the time varying features of time series data. If the statistics themselves can be modeled as time series,  they may be fed as additional input variables to help better train the sequential neural network.}
 %But if there exits a statistics that changes over time, in another word, if there is a statistics as time series data, then this kind of statistics can be fed as another input variable into the sequential neural network for training. 
 
 \rev{Apart from variables commonly used as inputs to the sequential neural network, we add in the statistics extracted from the data series as additional inputs. The GARCH term $h_t$ is a good candidate to choose. As a conditional variance, it can reflect the volatility of the sequence at different time and is essential for describing and predicting the changes of sequence. To explain our design principles, we use LSTM as an example neural network and the GARCH term $h_t$ as the example statistics. We propose two methods to incorporate the statistics into the sequential neural networks. } 
 %a conditional variance that can be  attracts our attention. 


 \rev{In method I, we use the past values of both $y_t$ and $h_t$ to train LSTM and predict the future values of $y_t$.  That is, besides past values of $y_t$ (i.e., ${y_{t}, y_{t-2},...,y_{t-k1}}$), we add past values of the GARCH term $h_t$ (i.e., ${h_{t-1}, h_{t-2},...,h_{t-k2}}$) as another input:} \note{For prediction, you may start from $y_{t+1}$}
 
% \paragraph{Method I of Multi-variables LSTM based on statistics extraction}
% \rev{Apart from the variable of time series data in the past, the GARCH term $h_t$ will be another variable to train the LSTM. In method I, past values of $y_t$ are the first input and the past values of $h_t$ will be the second input. Specifically, ${y_{t}, y_{t-2},...,y_{t-k1}}$ is the first input and ${h_{t-1}, h_{t-2},...,h_{t-k2}}$ is the second input. Method I is designed by the common sense, where we use both the past information of $y_t$ and $h_t$ to train the LSTM and predict $y_t$ in the future. The equation can be expressed as below:}
 \begin{equation}
    {y_{t}} = LSTM(y_{t-1},y_{t-2},...,y_{t-k1};h_{t-1},h_{t-2},...h_{t-k2}) + \varepsilon_t
\end{equation}
Where $\varepsilon_t$ is the random error. For the convenience of expression, we call the model above as LSTM-GARCH-I model. \note{On the left side of the above equation, will write the expression in sequential order, $y_{t}, y_{t+1}$? In the second method, you use sequential order. These two are not consistent. } 


\note{You are not using only using the current, but also future states?}

 \rev{In method II, besides past values of $y_t$ and $h_t$, we will also add the future values of $h_t$ to train the sequential neural network. As a statistic variable, $h_t$ reflects the fluctuation of $y_t$, but is also more stable thus can be better predicted than the raw data.
 %a current to current mapping between $h_t$ and $y_t$ will better capture the current data states. 
 In this case, to predict ${y_{t+k},y_{t+k-1},...,y_{t}}$,} we will not only use ${y_{t-1},y_{t-2},...,y_{t-k1}}$ and ${h_{t-1},h_{t-2},...h_{t-k2}}$, but also ${h_{t+k},h_{t+k-1},...,h_{t}}$. \rev{Although this design is easy to realize in the training phase, if multi-step prediction is needed,  we don't have the future values of ${h_{t+k},h_{t+k-1},...,h_{t}}$ in the testing phase and practical applications. To address the issue, we apply another LSTM to predict the future values of $h_t$ based on past $h_t$ values.} 
 %we train another multi-steps single-variable LSTM to  predict $h_t$ in the future. Therefore, in method II, we have trained two LSTM. The first one is trained to predict $h_t$; while the second one utilizes  the past values of  $y_t$ and the past to current values of $h_t$ as variables to train to predict $y_t$.  
 The algorithm of method II is shown below and the structure is shown in Fig. 
 
 
 \note{Normally, people use left side as past, right side as future. Your notations seem to be not used commonly.} \rev{In time series expression, people more likely to express the data from t-1 to t-n}

 
% \paragraph{Method II of Multi-variables LSTM based on statistics extraction}
% \rev{In the Method I above, the past information both of $y_t$ and $h_t$ are used to predict the future $y_t$. But in method II, we want to add the present values of $h_t$ into the past $h_t$ to train our model. The reason to do so is that $h_t$ itself reflects the  fluctuation in current time of $y_t$. A current to current mapping between $h_t$ and $y_t$ will be a more logical relationship. For example, to predict ${y_{t+k},y_{t+k-1},...,y_{t}}$, we use not only  ${y_{t-1},y_{t-2},...,y_{t-k1}}$ and ${h_{t-1},h_{t-2},...h_{t-k2}}$, but ${h_{t+k},h_{t+k-1},...,h_{t}}$. In the training phase, this design is easy to implement but in the testing phase, we don't have the future values of ${h_{t+k},h_{t+k-1},...,h_{t}}$ because we assume we just have the information before t. \sout{However, to predict the future in test stage, the future values of $h_t$ are needed but we have no information of future $h_t$.} In order to solve this problem, we train another multi-steps single-variable LSTM to  predict $h_t$ in the future. Therefore, in method II, we have trained two LSTM. The first one is trained to predict $h_t$; while the second one utilizes  the past values of  $y_t$ and the past to current values of $h_t$ as variables to train to predict $y_t$.  The algorithm of method II is shown below and the structure is shwon in Fig. }
 
 
 
 
 
 
% The mathematical expression of method II is as follows:
 \begin{equation}
    {y_t} = LSTM(y_{t-1},y_{t-2},...,y_{t-k1};h_t,h_{t-1},h_{t-2},...h_{t-k2}) + \varepsilon_t
\end{equation}
 
 
 
 \rev{We call this model as LSTM-GARCH-II.}
 

\subsection{Hybrid  time series model \rev{exploiting both  data decomposition and statistic feature extraction}}

\rev{With data decomposition,  ARIMA-GARCH is applied to model the stable part of the data, and LSTM is used to track the unstable part of the data. In the method exploiting statistic feature extraction, LSTM-GARCH takes the statistic-based time series $h_t$ as an additional input to predict $y_t$. To take full advantage of both types of method, we introduce a hybrid  model  Hybrid-LSTM-GARCH to exploit the use of both.   In this model,  a time series $y_t$ is first divided into a stable part $S_t$ and an unstable part $N_t$. Then $S_t$ is modeled by ARIMA-GARCH and  $N_t$ is modeled by LSTM-GARCH. Rather than predicting $y_t$ directly, the statistics $h_t$ is applied with LSTM to predict  $N_t$. The hybrid model is \rev{briefly} represented as}

%\rev{Hybrid  model here takes advantage of both the decomposing and statics extracting method. In section 3.2, the LSTM-GARCH method utilizes the new statistics $h_t$ to help predict $y_t$; and in section 3.1, for the complete version of model, after decomposing the data, ARIMA-GARCH is used to deal with stable part and LSTM is used to deal with unstable part. After carefully thinking about the two methods above, we found a new method that can take full advantage of the  two methods. Take the LSTM-GARCH-I model in section 3.2 for example. First of all, still dividing the time series $y_t$ into stable part $S_t$ and unstable part $N_t$. $S_t$ is molded by ARIMA-GARCH first; then the $N_t$ (not $y_t$) are modeled by LSTM-GARCH.That is, we use the new statistics $h_t$ to help predict  $N_t$ instead of $y_t$. We call this model as Hybrid-LSTM-GARCH model, which is represented as follosw:}

\begin{equation}
    y_t = S_t + N_t
\end{equation}
\begin{equation}
    N_t = LSTM(N_{t-1},N_{t-2},...,N_{t-k1};h_{t-1},h_{t-2},...,h_{t-k2})+\varepsilon_t
\end{equation}

Where $\varepsilon_t$ is the random error.
\note{Why here you only predict one step, while in the previous section you predict multiple steps? The paper does not have consistent notations everywhere.}



%\subsection{\sout{\rev{Sequential neural network based on statistical statistics feature extraction II}}}
%\sout{Based on method , a hybrid LSTM-GARCH model will be builded up to predict $y_t$. The difference between method IV and method II is method IV will take the current value of $h_t$ into consideration to predict $y_t$,} \note{Why?} that is,
%\begin{equation}
    %y_t = LSTM(y_{t-1},y_{t-2},...,y_{t-n};h_t,h_{t-1},h_{t-2},...h_{t-n}) + \varepsilon_t
%\end{equation}
%\sout{For the multiple steps prediction, method IV can be expressed as:}
%\begin{equation}
    %{y_t,y_{t+1},...,y_{t+m}} = LSTM(y_{t-1},y_{t-2},...,y_{t-n};h_{t+m},...,h_t,h_{t-1},h_{t-2},...h_{t-n}) + \varepsilon_t
%\end{equation}

%\sout{Take one step prediction for example, GARCH values $h_t$ is a single time series like $y_t$. ARIMA-GARCH model was built up first to $y_t$; then make prediction for $h_t$ by LSTM model. Next, the current GARCH value $h_t$ was added into LSTM-GARCH to predict $y_t$, which is shown in (23).} \note{At the beginning of this section you need to introduce problem and motivation and design consideration.}