\section{Performance Studies}

\note{Don't start a section with title. Write something here to introduce you work experiments etc. Say ho other people wrote.} 
\subsection{Experimental Setup}
\subsubsection{Data sets}
\rev{As typical time series data, stock prices are commonly used in data analyses. We choose three data sets on "Open Price" of different stocks in our performance studies, with each  set containing the data of one year.} \note{Any reference or links?}\note{I will do all the reference work later}  %The stock data  \rev{have} large fluctuations as well as randomness.
\rev{Stock data are typical time series in our lives. Therefore, choosing stock data for research has  good practical application value.} %The t does have a strong relationships among the current time and its past.
\del{Thus, dealing with the chaotic financial data will be a challenging but meaningful work.} The first data set is the daily open stock price for ”AMZON(AMZN)” from 01-02-2019 to 12-18-2019. \rev{The second data set is the daily open stock price for "Fidelity National Information Services(FIS)" from 01-02-2019 to 12-18-2019.}\rev{The third one is "WARMALT(WMT)" from 01-02-2019 to 12-18-2019.} We divide the data into two subsets, $88\%$ belongs to the training set and the remaining $12\%$ is used as the test set. \rev{The training set has 214 samples and test set has 28 samples. In order to further investigate the performance of our proposed methods, we  divide the test data into four subgroups: test set 1(with one week's data from day 1 to day 7);test set 2(with two weeks' data from day 1 to day 14);test set 1(with three weeks' data from day 1 to day 21);test set 1(with one weeks' data from day 1 to day 28).Day 1 here is the first sample's date in test set.}
\rev{The first data set} is represented as $y_t$ and is used as the primary example to show the performance of our methods. \note{You need to introduce all datasets, and performance metrics you use in this study.}

\subsubsection{Preparation for ARIMA-GARCH Model } Statistic models are often impacted by the stationary level of the data. We use ARIMA-GARCH model in our method 1. As a first step, we check if data are stationary.
 A time series {$x_t$} is strictly stationary if $f(x_1,...,x_n) = f(x_{1+h},...,x_{n+h})$, where $n$ and $h$ are positive integer numbers and $f(\bullet)$ denotes the function relationship. ARMA-GARCH can only deal with stationary data, while ARIMA-GARCH can \del{deal with non-stationary data by} transfer  non-stationary data into stationary ones.


In statistics, Augmented  Dickey-Fuller Test(ADF) test is the most popular method to help check if a time series is stationary, and the null hypothesis of ADF test indicates that the time series is not stationary.  The p-values obtained by the test is used to decide whether we should reject the null hypothesis, so the times series is considered to be stationary. If p-value is bigger than 0.05, we will accept the null hypothesis; otherwise, we will reject the null hypothesis. In our ADF test of data, the p-value is 0.6135,  so we accept the null hypothesis and conclude that the data set is not stationary. Then we transfer the data set $y_t$ to the difference 
form $\triangle$$y_t$. \note{If you work on the difference form. How to get the original data? Difference data have no physical meaning.}\note{Next paragraph show this process is just used to find the value of "d". We still build ARIMA-GARCH model for the original data set.} We do the ADF test for $\triangle$$y_t$  and get p-value of 0.01. As the null hypothesis is rejected, we conclude that $\triangle$$y_t$ is stationary. %\note{See how I made changes to write things more clearly.}

\subsubsection{ARIMA(p,d,q)-GARCH(P,Q) Model}
Based on the ADF test on our data, $y_t$ is not stationary, but its 1 time difference is stationary. Therefore, the parameter $d$ in \rev{the} ARIMA(p,d,q)-GARCH(P,Q) model for $y_t$ is 1. We use R Studio to build up the ARIMA-GARCH model for the data. To determine the parameters of the ARIMA(p,d,q) part, we use the "auto.arima" function in R studio, \note{Any reference?} and get ARIMA (5,1,2) for $y_t$ based on the AIC criterion. %The "auto.arima" will return the model with lowest AIC, BIC or AICc. Here we choose AIC as the criterion. 



%\rev{We further conduct} the model diagnostic test to see whether this model is suitable and the residuals of this model have no correlation. \note{how to determine suitable? What residual of the model means?}
%The correlation here refers to  linear correlation but the higher order correlations like squared correlation are not included. \note{Why only needs linear check? All these statements read random.} Ljung-Box test is a powerful tool to \rev{perform} this check. The null hypothesis of Ljung-Box test is the residuals have no auto-correlation. \note{I don't know what you want to say in this sentence. This sentence also has a wrong grammar, and not readable.} The p-values of the Ljung-Box test \note{What is the relationship between this test and ADF test?}  from lag 2 to 20 are all bigger than 0.05. We will accept the Ljung-Box test and conclude \rev{that} ARIMA(5,1,2) model is suitable. \note{It is unclear what you mean acceptable, and why it is suitable, and what you mean suitable.}

%\note{You put many buzz words without explanations, and also don't provide any reference.}

Next, the arch effect will be checked to see whether  the variance of the residuals are homogeneous. If  an arch effect exists in the model, which means the variance of the residuals are not homogeneous,  GARCH model needs to be built up to capture the data dynamics. Finally, we build up ARIMA(5,1,2)-GARCH(1,1) model for ${y_t}$ based on the AIC criterion.



\rev{As for the prediction of this model, We use the training data to build up the ARIMA(p,d,q)-GARCH(P,Q) and make prediction. Assuming the training data are ${y_1,y_2,...,y_m}$,the one step forward prediction will predict $y_{m+1}$. Next, the true value of $y_{m+1}$ will be added into ${y_1,y_2,...,y_m}$ as} ${y_1,y_2,...,y_m,y_{m+1}}$ to predict $y_{m+2}$, %which is  still based on the same ARIMA(p,d,q)-GARCH(P,Q) structure obtained in the training stage. 
\rev{The coefficients of ARIMA(p,d,q)-GARCH(P,Q) will be recomputed but the structure, i.e., $\alpha_i$,$\theta_j$,but the model structure, i.e., $(p,d,q)$ and $(P,Q)$ won't be changed.}
%\subsubsection{Proposed decomposition methods and baselines}

\subsubsection{\rev{Setting of the decomposition method}}

%\note{I am working on revising the this section now}

%\note{Are you done with the beginning part of this section?}

%\note{ what I left is the performances studies part}

 In this study, we use LSTM  as  baseline method to directly learn the data distribution and make one step forward prediction. \del{ certain number of sequence data will be took as input to predict the next. For example,} We take ${y_1,y_2,...,y_k}$ as an input and ${y_{k+1}}$ as an output to perform the training. The "Keras" with version of '2.3.1' is used to built up LSTM. "Keras" is a high-level neural networks API, written in Python, which is capable of running on top of Tensorflow. \del{LSTM or SVR.} \del{The value of $k$ is \rev{an} important parameter \rev{to learn from} the data.}\note{Do people normally learn k? If this is not predetermined, how to even perform training? } In LSTM method, \rev{given 
the input and the output,  the weights of input gates, cell states, forget gates and output gates need to be learnt }\note{What do you mean the number of units?} \del{is another parameter need to be learnt.} \note{Do people normally don't set up the number of units in advance, but learn? Then how could training be performed? Also, don't you need to learn gate parameters?} In addition, we choose " Mean squared Error" as the loss function and  $tanh$ \rev{as activation function} to do the training.  %In the SVR method, the penalty parameter C of the error term, the  $\epsilon-$insensitive loss function and the kernel function are the three key parameters need to be learnt. 

In our proposed decomposition method, we apply ARIMA-GARCH to model the stable data components to get the residual ${r_t}$ of the data. We use LSTM  as \del{two} candidate method to learn the distribution of the residual data, and have \rev{method as  ARIMA-GARCH-LSTM} \del{and have two methods ARIMA-GARCH-LSTM and ARIMA-GARCH-SVR respectively.} In the training stage, \rev{LSTM} is used to train ${r_t}$, where ${r_1,r_2,...,r_k}$ serve as the input and ${r_{k+1}}$ is the output. In the prediction stage, if we want to predict ${y_{t+1}}$, the predicted ${r_{t+1}}$ will be added to the predicted value of ${y_{t+1}}$ from ARIMA-GARCH to get the final prediction of ${y_{t+1}}$. 



\subsubsection{\rev{Setting of the method based on statistics extraction}}
\rev{The GARCH term--$h_t$, obtained from equation 8, is a statistics that can reflect the volatility of a time sequence at each time point. In our proposed statistics extraction method, the ARIMA-GARCH model will be built up for the data first, then $h_t$ can be got from this model. Next, as a new feature, $h_t$ will be put into the LSTM with $y_t$ to help capture the dynamic of $y_t$.}


%\subsubsection{Metrics}
\rev{\subsection{Metrics}} 
\note{Or any suitable name?}
We adopt four common metrics to evaluate the effectiveness of the proposed frameworks, which include  Mean Squared Error(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE),and Mean Absolute Percentage Error (MAPE). \note{If you have space, you may give basic definition for each.} \rev{For all the metrics, the lower the value, the better the performance of the corresponding models.} The definitions of the four metrics are as follows:

%\note{Do you need to introduce how you get the statistics and what are the statistics? Or these statistics are varying over time as well? }
\subsection{\rev{Performance Studies}}

%\note{You need to introduce you experiment setup, metrics and datasets used that are common used for all studies at the beginning. You need to cite the dataset if it is publicly available.}

\subsubsection{Results of Decomposition Method}
%\note{This belongs of experiment setting. If it is common to all methods, move it to the beginning part.} 
We \del{first} compare the performance of different methods in making one-step forward prediction.\rev{Figure show the predictions' curves  and  Table \ref{tab1} shows the prediction results of our proposed models and the baseline models for all the four test sets.}
\del{for the first week of the data from the test set}. Compared with ARIMA-GARCH, \del{SVR} and LSTM, \del{both ARIMA-GARCH-SVR and} ARIMA-GARCH-LSTM  achieve\rev{s} better performance in all metrics.\rev{Taking results in data set 4(the whole test set) as an example, the MSE, RMSE, MAE and MAPS of the proposed model improve $38.63\%$, $21.66\%$, $27.97\%$ and $27.61\%$ compared with LSTM model and $5.43\%$, $2.76\%$, $8.64\%$ and $8.53\%$ compared with ARIMA-GARCH model, }which demonstrates the effectiveness of the decomposition method in improving the prediction accuracy. 
%\note{Why  ARIMA-GARCH-LSTM is better than ARIMA-GARCH-SVR? Explain the possible reason.}
%ARIMA-GARCH-LSTM is the best model compared with ARIMA-GARCH and LSTM. The reaso why our proposed models will outperform the baselines is we always use the suitable models to deal with the suitable data.  

\note{Does the statistic model use all data or testset data only?} \note{Answered in ARIMA-GARCH part in left column}
 %\rev{Table \ref{tab2} shows the result of the following week.} \note{Why do you show the following week? Any particular reason, or just another set of data?}
%Compared with the Table 1, 
\rev{Further, comparing the results in different test sets of Table \ref{tab1}}, the performance improvement of \del{Table \ref{tab2}} test set 1 is bigger. \rev{Figure 2 shows the predictions' curves in test set 2. This plot clearly shows the curve of ARIMA-GARCH-LSTM is the closest one to the true values' curve.}\rev{the MSE, RMSE, MAE and MAPS of the proposed model improve $66.56\%$, $42.17\%$, $50.57\%$ and $50.35\%$ compared with LSTM model and $30.97\%$, $16.92\%$, $20.80\%$ and $20.71\%$ compared with ARIMA-GARCH model.} \rev{The different performance improvement} is because that our proposed \rev{method is} built up on the base of ARIMA(p,d,q)-GARCH(P,Q) model.  When \rev{new data are} added, we just recompute the coefficients of ARIMA(5,1,2)-GARCH(1,1) model, i.e., \rev{$\alpha_i$,$\theta_j$,}but we \rev{don't} change the structure, i.e., $(p,d,q)$ and $(P,Q)$ of this model. \note{Aren't the parameters of these structures changed?} Nevertheless, as time goes by, the structure may have some changes \rev{due to the fact that financial data have more volatility. When long sequence of the new data added into the training data, the whole structure of this data may change}, so the accuracy of  ARIMA-GARCH model reduces and the residual data may have a large variation. \note{Your parameters for the statistics are not from historical data but only from recent data? Then you need to justify it and mention the finance data re volatile and you cannot use too long history data.} All these factors will affect the performances of  ARIMA-GARCH-LSTM \del{and ARIMA-GARCH-SVR} model. %But for the one step forward prediction of next one week, few data are added to ARIMA-GARCH, so the performance of ARIMA-GARCH will not be affected a lot.   \note{This is confusing. You just said for the next week, it changes a lot. Now you said it does not change a lot. Can't you be more strict in your writing?} 
Therefore, our proposed methods have better performances \rev{in all test sets especially in the first test set}

%\note{Again, you are attracting people to attack you by emphasizing show-period prediciton. People will ask what the result for long-term prediction?} Table \ref{tab3} lists the percentage improvement between different methods. \rev{Compared to  ARIMA-GARCH, the improvement of ARIMA-GARCH-SVR is larger than that of ARIMA-GARCH-LSTM, which indicates that SVR can better track the variation of residual data than LSTM.} 

%\note{I am not sure if you want to emphasize one-step prediction, if you don't study multi-step. When you said one step-forwarding of a week, it is unclear you predict a whole week or for each day you make a prediction.}


 


%And that is why the predictions for the following one week are have more improvements. 

%Figure ~\ref{LSTM38}shows the prediction results from "11/05/2018" to "01/01/2019" for ARIMA-GARCH-LSTM, ARIMA-GARCH and LSTM models. Figure ~\ref{LSTM7} shows the next one week prediction  results from "11/05/208" to "11/13/2018". 
\rev{Moreover},All the predictions in the figures shows \rev{a} first-order-lag, which means the prediction on \rev{the} day $t$ is more close to the result in \rev{the} day $t-1$. \del{To better understand the lag problem,} Taking figure ~\ref{AGL7} as an example, if we move the \rev{true} data from 1-6 to the position 2-7, you will \rev{find the results from all methods have the same trend of change.} The lag problem is a common \del{problem} in time series analysis \rev{as} the prediction results of all methods have a tendency to approach \rev{those of the} previous step.  

%The percentage improvements of our proposed methods compared with the baseline models are given in Table \ref{tab3}. From  Table \ref{tab3}, one can see LSTM and SVR both have  huge improvements when combined with ARIMA-GARCH method Although LSTM seems having worst performance compared with other models, it still has good ability to predict the tendency of the complex data. In our proposed method, we use LSTM to predict the unstable part, which 

% Please add the following required packages to your document preamble:
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
% Please add the following required packages to your document preamble:
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 156.93 & 12.53 & 10.13 & 0.57 \\ \cline{2-6} 
Test Set 1 & LSTM & 323.92 & 18.00 & 16.23 & 0.90 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{108.32} & \textbf{10.41} & \textbf{8.02} & \textbf{0.45} \\ \hline
 & AG & 155.33 & 12.46 & 10.56 & 0.60 \\ \cline{2-6} 
Test Set 2 & LSTM & 280.57 & 16.75 & 14.38 & 0.81 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{108.06} & \textbf{10.40} & \textbf{8.34} & \textbf{0.47} \\ \hline
\end{tabular}
\caption{\label{tab1}Metrics Analysis of Test Set 1 and Test Set 2}
\end{table}

%\caption{\label{tab1}Metrics Analysis of Test Set 1 and Test Set 2}


\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{263.40} & \textbf{16.23} & \textbf{12.35} & \textbf{0.70} \\ \hline
 & AG & 238.25 & 15.44 & 12.39 & 0.70 \\ \cline{2-6} 
Test Set 4 & LSTM & 367.18 & 19.16 & 15.71 & 0.88 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}AG-\\ LSTM\end{tabular} & \textbf{225.32} & \textbf{15.01} & \textbf{11.32} & \textbf{0.64} \\ \hline
\end{tabular}
\caption{\label{tab2}Metrics Analysis of Test Set 3 and Test Set 4}
\vspace{-15pt}
\end{table}

%Metrics Analysis of Test Set 3 and Test Set 4
%\begin{table}[]
%\caption{\label{tab1}Metrics Analysis of Test Set Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.001318 & 0.036298 & 0.024794 & 2.142506 \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%AG-LSTM & \textbf{0.001241} & \textbf{0.035234} & \textbf{0.022233} & \textbf{1.941214} \\ \hline
%SVR & 0.001501 & 0.038747 & 0.029667 & 2.510208 \\ \hline
%AG-SVR & \textbf{0.001287} & \textbf{0.035871} & \textbf{0.023604} & \textbf{2.051793} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}[]
%\caption{\label{tab2}Metrics Analysis of Following week Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.000282 & 0.016798 & 0.013751 & 1.076193 \\ \hline
%LSTM & 0.001359 & 0.036868 & 0.033778 & 2.576865 \\ \hline
%AG-LSTM & \textbf{0.000244} & \textbf{0.015636} & \textbf{0.011847} & \textbf{0.929590} \\ \hline
%SVR & 0.000273 & 0.016518 & 0.012974 & 1.007235 \\ \hline
%AG-SVR & \textbf{0.000182} & \textbf{0.013501} & \textbf{0.009888} & \textbf{0.772916} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}[]
%\caption{\label{tab3}Percentage Improvement of the Following week Predictions}
%\vspace{-10pt}
%\footnotesize
%\begin{center}

%\begin{tabular}{|c|c|c|c|c|}
%\hline
%$\%$ & MSE & RMSE & MAE & MAPE \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-LSTM \\ vs. AG\end{tabular} & 13.35 & 6.92 & 13.85 & 13.62 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-LSTM \\ vs. LSTM\end{tabular} & 82.01 & 57.59 & 64.93 & 63.93 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-SVR \\ vs. AG\end{tabular} & 35.40 & 19.63 & 28.09 & 28.18 \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG-SVR \\ vs. SVR\end{tabular} & 33.20 & 18.27 & 23.78 & 23.26 \\ \hline
%\end{tabular}
%\end{center}
%\end{table}


\begin{figure}
  \vspace{-10pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/AGL28.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of AG-LSTM and its baselines for test set 4}
\label{AGL28}
  \vspace{-10pt}
\end{figure}

\begin{figure}
  \vspace{-10pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/AGL7.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of AG-LSTM and its baselines for test set 1}
\label{AGL7}
  \vspace{-10pt}
\end{figure}

\begin{figure}
  \vspace{0pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/LG28.png}
  \end{center}
  \vspace{-10pt}
  \caption{\footnotesize Predictions of LSTM-GARCH and its baselines for test set 4}
\label{LG28}
  \vspace{-20pt}
\end{figure}

\begin{figure}
  \vspace{-45pt}
  \begin{center}
\includegraphics[width=2.5 in]{images/LG7.png}
  \end{center}
  \vspace{-10pt} 
  \caption{\footnotesize Predictions of AG-LSTM and its baselines for test set 1}
\label{LG7}
  \vspace{-20pt}
\end{figure}

\subsubsection{Results of \rev{Learning with Statistics Extraction}}
\rev{We compare the performance in  making  one-step  for-ward  prediction. Figure 3 shows the curve of true values and  the predictions curves of LSTM-GARCH model, ARIMA-GARCH model and LSTM model. The LSTM-GARCH model's prediction curve is the closet one to the true values. } Table 3 shows the prediction results for LSTM,ARIMA-GARCH adn LSTM-GARCH \rev{ for all the four test sets.}LSTM-GARCH is seen to be perform \note{or performing} better in all metrics. \rev{Taking the test set 4(the whole test set) as an example, The improvement of MSE, RMSE, MAE and MAPE are 
$36.87\%$,$20.55\%$,$23.91\%$,$23.55\%$ compared with LSTM model.}This demonstrates that the introduction of additional statistic feature into the learning platform has a big impact on the performance. of LSTM model.\rev{ In addition, the improvement of MSE, RMSE, MAE and MAPE are 
$2.71\%$,$1.36\%$,$3.50\%$,$3.40\%$ compared with ARIMA-GARCH model. LSTM-GARCH still has better performance than ARIMA-GARCH model.}

\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 156.93 & 12.53 & 10.13 & 0.57 \\ \cline{2-6} 
Test Set 1 & LSTM & 323.92 & 18.00 & 16.23 & 0.90 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{125.01} & \textbf{11.18} & \textbf{8.53} & \textbf{0.48} \\ \hline
 & AG & 155.33 & 12.46 & 10.56 & 0.60 \\ \cline{2-6} 
Test Set 2 & LSTM & 280.57 & 16.75 & 14.38 & 0.81 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{149.39} & \textbf{12.22} & \textbf{10.05} & \textbf{0.57} \\ \hline
\end{tabular}
\caption{\label{tab3}Metrics Analysis of LSTM-GARCH and it's baselines for test set 1 and test set 2}
\end{table}



%\caption{\label{tab3}Metrics Analysis of LSTM-GARCH and it's baselines for test set 1 and test set 2}


\begin{table}[]
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
 & AG & 287.66 & 16.96 & 13.92 & 0.78 \\ \cline{2-6} 
Test Set 3 & LSTM & 440.38 & 20.99 & 17.34 & 0.97 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{285.41} & \textbf{16.89} & \textbf{13.70} & \textbf{0.77} \\ \hline
 & AG & 238.25 & 15.44 & 12.39 & 0.70 \\ \cline{2-6} 
Test Set 4 & LSTM & 367.18 & 19.16 & 15.71 & 0.88 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & \textbf{231.80} & \textbf{15.22} & \textbf{11.95} & \textbf{0.67} \\ \hline
\end{tabular}
\caption{\label{tab4}Metrics Analysis of LSTM-GARCH and it's baselines for test set 3 and test set 4}
\end{table}

%Metrics Analysis of LSTM-GARCH and it's baselines for test set 3 and test set 4



%\del{The last row of Table 4 is the percentage improvement the LSTM-GARCH model has compared with LSTM model for the whole test data.} \rev{ The improvement of RMSE is $27\%$, while the improvements of MSE, MAE and MAPE are all over $40\%$. 
\rev{Further, comparing the results in different test sets of table 3, the performance  of test set 1 is has the biggest improvement.}The MSE, RMSE, MAE and MAPE in our proposed method has improved $61.41\%$,$37.88\%$,$47.48\%$,$47.29\%$ compared with LSTM model and $20.34\%$,$10.77\%$,$15.85\%$,$15.82\%$ compared with ARIMA-GARCH. The reason is similar to that in the last part. When a long sequence of the new data added into the training data, the whole structure of this data may change, so the accuracy of  ARIMA-GARCH model reduces and the GARCH term($h_t$) obtained by ARIMA-GARCH model may have a large variation.
%This demonstrates that the introduction of additional statistic feature into the learning platform has a big impact on the performance. \del{}LSTM model.}
\note{I will not put baseline results again in table 5 due to the page limits}


In Table 5, we further compare the performance between the methods 1 and 2. \del{in preference to the base lines ARIMA-GARCH and LSTM.} ARIMA-GARCH-LSTM \rev{not only} outperforms the baselines ARIMA-GARCH and LSTM in all metrics \rev{but also} is superior to our proposed  LSTM-GARCH model. The reason is that the decomposition method  extracts data information more sufficiently while LSTM-GARCH just utilizes the GARCH term to help improve the LSTM performance. However, the second method is more concise and easy to use while the first method need more pre-work to do. 


%\rev{Compared with the decomposition-based method, the method of learning with statistic extraction has more freedom and can better track the practice data.} Finally, we conclude that the extraction method not only  improves the performance of LSTM but have better sustainability compared with the decomposition method. \note{Why?You cannot simply claim.}

%We can conclude our statistics extraction method did improve a lot. 



%\begin{table}[]
%\caption{\label{tab4}Metrics Analysis of Test Set Predictions}
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
 %& MSE & RMSE & MAE & MAPE \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\end{tabular} & 0.001142 & 0.033791 & 0.021945 & 1.897688 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSTM-\\ GARCH\\ vs.LSTM($\%$)\end{tabular} & 47.26 & 27.37 & 45.73 & 43.35 \\ \hline
%\end{tabular}
%\vspace{-5pt}
%\end{table}


%\begin{table}[]
%\caption{\label{tab5}Metrics Comparisons among extraction, decomposition and baseline methods }
%\vspace{-5pt}
%\footnotesize
%\begin{tabular}{|c|c|c|c|c|}
%\hline
% & MSE & RMSE & MAE & MAPE \\ \hline
%AG & 0.001318 & 0.036298 & 0.024794 & 2.142506 \\ \hline
%LSTM & 0.002165 & 0.046528 & 0.040433 & 3.349739 \\ \hline
%AG-LSTM & 0.001241 & 0.035234 & 0.022233 & 1.941214 \\ \hline
%\begin{tabular}[c]{@{}c@{}}LSGM\\ -GARCH\end{tabular} & \textbf{0.001142} & \textbf{0.033791} & \textbf{0.021945} & \textbf{1.897688} \\ \hline
%\end{tabular}
%\vspace{-10pt}
%\end{table}


%\begin{figure}
 % \vspace{5pt}
 % \begin{center}
%\includegraphics[width=2.5 in]{images/LG38.png}
 % \end{center}
 % \vspace{-10pt}
 % \caption{\footnotesize Open Price Predictions}
%\label{LG38}
 % \vspace{-10pt}
%\end{figure}




\subsubsection{Robust Analysis}
\rev{We use two more data sets to test the superiority of our proposed. The first one is the stock open Price of "Fidelity National Information Services(FIS)" from 01-02-2019 to 12-18-2019 and the second one is the stock open Price of "Walmart(WMT)" from 01-02-2019 to 12-18-2019.} The metrics analysis of method 1, method 2 and their baselines for all the test sets are put into one table. Table 6 and 7 shows the result of "FIS" and "WMT",respectively.  One can see our proposed models have best performance compared with the baselines for all the test sets. Therefore, we conclude that both the decomposition method and the statistics extraction method 
are exceptional and deserve more attention.

\subsubsection{Conclusion}







%We use two more data sets \note{Did you introduce this somewhere?} to test the superiority of our proposed \rev{methods. Tables} 6 and 7 show the prediction results \rev{for a  longer time period and one week respectively. Both our methods perform better than the baseline models.} \del{both in Table 6 and Table 7.} The results in Table 7 have much more \rev{performance} improvement compared with that in Table 6. The reason is similar to data set 1.  \note{You didn't explain this earlier, between a long data set and short one. I didn't put whole data set to avoid problem.} \note{Also, why you didn't show the performance of extraction-based method?} 

\begin{table}[]
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 &  & MSE & RMSE & MAE & MAPE \\ \hline
{Test Set 1} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 108.32 & 10.41 & 8.02 & 0.45 \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 125.01 & 11.18 & 8.53 & 0.48 \\ \hline
{Test Set 2} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 108.06 & 10.40 & 8.34 & 0.47 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 149.39 & 12.22 & 10.05 & 0.57 \\ \hline
{Test Set 3} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 263.40 & 16.23 & 12.35 & 0.70 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 285.41 & 16.89 & 13.70 & 0.77 \\ \hline
{Test Set 4} & \begin{tabular}[c]{@{}l@{}}AG-\\ LSTM\end{tabular} & 225.32 & 15.01 & 11.32 & 0.64 \\ \cline{2-6} 
 & \begin{tabular}[c]{@{}l@{}}LSTM-\\ GARCH\end{tabular} & 231.80 & 15.22 & 11.95 & 0.67 \\ \hline
\end{tabular}
\caption{\label{tab5} Metrics comparisons of AG-LSTM and LSTM-GARCH for all test sets.}
\end{table}


