\begin{algorithm}[H]
    \small
    \caption{BetaZero offline policy iteration.}
    \label{alg:betazero}
    \begin{algorithmic}[1]
        \Require $\mathcal{P} \defeq \langle \mathcal{S}, \mathcal{A}, \mathcal{O}, T, R, O, \gamma \rangle$: POMDP
        \Require $\psi$: Parameters (includes $n_{\text{\rm iterations}}$, $n_{\text{\rm data}}$, $n_{\text{\rm online}}$, and $d$)
        \Function{BetaZero$(\mathcal{P}, \psi)$}{}
            \State $f_\theta \leftarrow \textproc{InitializeNetwork}(\psi)$
            \State $P_\theta, V_\theta \leftarrow f_\theta$ \GrayComment{where $(\mathbf{p}, v) \leftarrow \big(P_\theta(\tilde{b}), V_\theta(\tilde{b})\big)$}
            \For {$i \leftarrow 1 \textbf{ to } n_\text{iterations}$}
                \State $\mathcal{D} \leftarrow \textproc{CollectData}(\mathcal{P}, f_\theta, \psi)$ \GrayComment{policy evaluation}
                \State $f_\theta \leftarrow \textproc{Train}(f_\theta, \mathcal{D})$ \GrayComment{policy improvement}
            \EndFor
            \State \Return $\beta_0^\pi(\mathcal{P}, f_\theta)$ \GrayComment{BetaZero online policy (uses alg. \ref{alg:mcts-top-lvl})}
        \EndFunction
    \end{algorithmic}
\end{algorithm}
