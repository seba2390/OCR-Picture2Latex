\begin{algorithm}[H]
    \small
    \caption{Collect MCTS data offline for policy evaluation.} 
    \label{alg:collect_data}
    \begin{algorithmic}[1]
        \Function{CollectData$(\mathcal{P}, f_\theta, \psi)$}{}
        \State $\mathcal{D} = \emptyset$
        \ParallelFor{$i \leftarrow 1 \textbf{ to } n_\text{data}$} \GrayComment{parallelize MCTS runs across available CPUs} \label{line:parallel}
            \For {$t \leftarrow 1 \textbf{ to } T$}
                \State $a_t \leftarrow \textproc{MonteCarloTreeSearch}(\mathcal{P}, f_\theta, b_t, \psi)$ \GrayComment{select next action through online planning}
                \State $\mathcal{D}_i^{(t)} \leftarrow \mathcal{D}_i^{(t)} \cup \big\{(b_t, \pi_\text{tree}^{(t)}, g_t) \big\}$ \GrayComment{collect belief and policy data (placeholder for returns)}
                \State $s_{t+1} \sim T(\cdot \mid s_t, a_t)\phantom{\big\}}$ % \GrayComment{transition the true state of the POMDP}
                \State $o_t \sim O(\cdot \mid a_t, s_{t+1})$
                \State $b_{t+1} \leftarrow \textproc{Update}(b_t, a_t, o_t)$
                \hspace*{3em}%
                \rlap{\raisebox{\dimexpr.5\normalbaselineskip+.5\jot}{\smash{$\left.\begin{array}{@{}c@{}}\\{}\\{}\\{}\end{array}\color{commentgray}\right\}%
                \color{commentgray}\begin{tabular}{l}transition the original POMDP\end{tabular}$}}}
                \State $r_t \leftarrow R(s_t, a_t)$ or $R(s_t, a_t, s_{t+1})$
            \EndFor
            % [sum(γ^(k-t) * R[k] for k in t:T) for t in 1:T]
            % or
            % [sum(γ^i*R[t+i] for i in 0:T-t) for t in 1:T]
            \State $g_t \leftarrow \sum_{k=t}^T \gamma^{(k-t)} r_k$ \textbf{ for } $t \leftarrow 1 \textbf{ to } T$ \GrayComment{compute returns from observed rewards}
        \EndParallelFor
        \State \Return $\mathcal{D}$
    \EndFunction
    \end{algorithmic}
\end{algorithm}