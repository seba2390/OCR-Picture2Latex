%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix

\section*{Appendix}

This section contains material detailing the POMDP environments and experiments, the ablation studies, additional analysis of bootstrapping and DPW, the network architectures, the hyperparameters and tuning, computational resources, information regarding open-source code for reproducibility, and the full BetaZero algorithm pseudocode.

\section*{POMDP Environments}
This section describes the benchmark POMDPs in detail.

\paragraph{Light dark.}

The \textsc{LightDark}$(y)$ POMDP is a one-dimensional localization problem \cite{platt2010belief}.
The objective is for the agent to execute the \texttt{stop} action at the goal, which is at $\pm 1$ of the origin.
The agent is awarded $100$ for stopping at the goal and $-100$ for stopping anywhere else; using a discount of $\gamma = 0.9$.
The agent receives noisy observations of their position, where the noise is minimized in the ``light'' region defined by $y$.
In the $\textsc{LightDark}(5)$ problem used by \citeauthor{wu2021adaptive}, the noise is a zero-mean Gaussian with standard deviation of $|y - 5|/\sqrt{2} + 10^{-2}$.
For the $\textsc{LightDark}(10)$ problem used by \citeauthor{sunberg2018online}, the noise is a zero-mean Gaussian with standard deviation of $|y - 10| + 10^{-4}$.
In both problems, we use a restricted action space of $\mathcal{A} = [-1, 0, 1]$ where $0$ is the \texttt{stop} action.
The expected behavior of the optimal policy is first to localize in the light region, then travel down to the goal.
The BetaZero policy exhibits this behavior which can be seen in \cref{fig:lightdark_trajectories} (where circles indicate the final location).


The approximately optimal solution to the light dark problems used \textit{local approximation value iteration} (LAVI) \cite{dmubook} over the discretized belief-state space (i.e., mean and std).
The belief mean was discretized between the range $[-3, 12]$ and the belief std was discretized between the range $[0, 5]$; each of length $25$.
The LAVI solver used $100$ generative samples per belief state and ran for $25$ value iterations with a Bellman residual of $\num{1e-3}$.

\paragraph{Rock sample.}

In the $\textsc{RockSample}(n,k)$ POMDP introduced by \citeauthor{smith2012heuristic}, an agent has full observability of its position on an $n \times n$ grid but has to sense the $k$ rocks to determine if they are ``good'' or ''bad''.
The agent knows \textit{a priori} the true locations of the rocks (i.e., the rock locations $\mathbf{x}_\text{rock}$ are a part of the problem, not the state).
The observation noise is a function of the distance to the rock: % and decreases exponentially:
\begin{equation}
   \frac{1}{2}\left(1 + \exp\left(-\frac{\lVert \mathbf{x}_\text{rock} - \mathbf{x}_\text{agent} \rVert_2 \log(2)}{c}\right)\right)
\end{equation}
where $c=20$ is the sensor efficiency.
The agent can move in the four cardinal directions, sense the $k$ rocks, or take the action to \texttt{sample} a rock when it is located under the agent.
The agent receives a reward of $10$ for sampling a ``good'' rock and a penalty of $-10$ for sampling a ``bad'' rock.
The terminal state is the exit at the right edge of the map, where the agent gets a reward of $10$ for exiting.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/results/lightdark_traj.pdf}
    \caption{$\textsc{LightDark}(10)$ trajectories from $50$ episodes.
    BetaZero (dark blue) learned to first localize in the light region at $y=10$ before heading to the goal (origin).}
    \label{fig:lightdark_trajectories}
\end{figure}


\paragraph{Mineral exploration.}

The \textsc{Mineral Exploration} POMDP introduced by \citeauthor{mern2023intelligent} is an information gather problem with the goal of deciding whether a subsurface ore body is economical to mine or should be abandoned.
The agent can drill every fifth cell of a $32 \times 32$ plot of land to determine the ore quality at that location.
Therefore, the action space consists of the $36$ drill locations and the final decisions to either \texttt{mine} or \texttt{abandon}.
The agent receives a small cost for each \texttt{drill} action, a reward proportional to the extracted ore if chosen to \texttt{mine} (which is negative if uneconomical), and a reward of zero if chosen to \texttt{abandon}:
\begin{equation*}
R(s,a) = \begin{cases}
    -c_\text{drill} & \text{if } a=\text{drill}\\
    \sum\mathds{1}(s_\text{ore} \ge h_\text{massive}) - c_\text{extract} & \text{if } a=\text{mine}\\
    0 & \text{otherwise}
\end{cases}\label{eq:minex_reward}
\end{equation*}
where $c_\text{drill}=0.1$, $h_\text{massive}=0.7$, and $c_\text{extract}=71$.
The term $\sum\mathds{1}(s_\text{ore} \ge h_\text{massive})$ indicates the cells that have an ore quality value above some massive ore threshold $h_\text{massive}$ (which are deemed valuable).
\Cref{fig:minex_policy} and \cref{fig:minex_2d} show an example of four steps of the mineral exploration POMDP.


\newcommand*{\SHOWAPPENDIXPLOTS}{}
\ifdefined\SHOWAPPENDIXPLOTS
\begin{figure*}[hp!]
    \centering
    \begin{minipage}{0.65\textwidth}
        \input{data/appendix/minex_data1}% load minex data files (BetaZero)
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/minex}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/appendix/belief1}
        }
    \end{minipage}
    % 
    \begin{minipage}{0.65\textwidth}
        \input{data/appendix/minex_data2}% load minex data files (BetaZero)
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/minex}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/appendix/belief2}
        }
    \end{minipage}
    % 
    \begin{minipage}{0.65\textwidth}
        \input{data/appendix/minex_data3}% load minex data files (BetaZero)
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/minex}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/appendix/belief3}
        }
    \end{minipage}
    % 
    \begin{minipage}{0.65\textwidth}
        \input{data/appendix/minex_data4}% load minex data files (BetaZero)
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/minex}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/appendix/belief4}
        }
    \end{minipage}

    \begin{minipage}[t]{0.65\textwidth}
        \caption{The BetaZero policy shown over belief mean for four steps. BetaZero first prioritizes the edges of the belief mean, corresponding to the belief uncertainty (right-most plots), then explores the outer regions of the subsurface; ultimately gathering information from actions with high mean and std, matching heuristics. At the initial step, abandoning and mining have near-equal probability (bottom left graphs) but by the fourth action, abandoning is much more likely.}
        \label{fig:minex_policy}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \caption{The selected drill actions over belief uncertainty, showing that uncertainty collapses after drilling.}
        \label{fig:minex_2d}
    \end{minipage}
\end{figure*}
\fi


\input{figures/ablations/ablation-figures}


\subsection*{Experiment details}

Experiment parameters for each problem can be seen in tables \ref{tab:ld_params}--\ref{tab:minex_params} under the ``online'' column.
For the baseline algorithms, the heuristics follow \citeauthor{wu2021adaptive}.
Problems that failed to run due to memory limits followed suggestions from \citeauthor{adaops2021review} to first use the MDP solution and then use a fixed upper bound of $r_\text{correct}=100$ for the light dark problems and the following for the rock sample problems:
\begin{equation}
    V_\text{max} =  r_\text{exit} + \sum_{t=1+n-k}^{2k-n} \gamma^{t-1}r_\text{good}
\end{equation}
where $r_\text{good} = r_\text{exit} = 10$ and the sum computes an optimistic value assuming the rocks are directly lined between the agent and the goal and assuming $n \ge k$ for simplicity.

For problems not studied by \citeauthor{wu2021adaptive}, we use the same heuristics as their easier counterpart (i.e., $\textsc{LightDark}(10)$ uses $\textsc{LightDark}(5)$ heuristics and $\textsc{RockSample}(20,20)$ uses $\textsc{RockSample}(15,15)$ heuristics).
For mineral exploration, the baselines used the following heuristics.
POMCPOW used a value estimator of $\max(0, R(s, a=\text{mine}))$ and when using ``no heuristic'' used a random rollout policy to estimate the value.
Both AdaOPS and DESPOT used a lower bound computed as the returns if fully drilled all locations, then made the decision to abandon:
\begin{equation}
    V_\text{min} = -\sum_{t=1}^{T-1} \gamma^{t-1}c_\text{drill}
\end{equation}
The upper bound comes from an oracle taking the correct final action without drilling, computed over $10{,}000$ states:
\begin{align}
    V_\text{max} &= \operatorname*{\mathbb{E}}_{s_0 \in \mathcal{S}} \bigg[ \max\Big(0, R(s_0, a=\text{mine})\Big) \bigg] \\
                 &\approx \frac{1}{n}\sum_{i=1}^n \max\Big(0, R(s_0^{(i)}, a=\text{mine})\Big) \nonumber
\end{align}

\paragraph{Particle filtering.}
Both BetaZero and the baseline algorithms update their belief with a bootstrap particle filter using a low-variance resampler \cite{gordon1993novel}, with $n_\text{particles} \in [500,1000,1000]$ for the light dark, rock sample, and mineral exploration problems, respectively.
The particle filter follows an update procedure of first reweighting then resampling.
In mineral exploration, the observations are noiseless which could quickly result in particle depletion.
Therefore, approximate Bayesian computation (ABC) is used to reweight each particle using a Gaussian distribution centered at the observation with a standard deviation of $\sigma_\text{abc} = 0.1$ \cite{csillery2010approximate}.

The belief representation takes the mean and standard deviation across the $n_\text{particles}$. In the light dark problems, this is computed across the $500$ sampled $y$-state values to make up the belief.
The initial $y$-value state distribution---which makes up the initial belief---follows a Gaussian distribution and thus the parametric representation is a good approximation of the belief.
For the rock sample problem, the belief is represented as the mean and standard deviation of the good rocks from the $1000$ sampled states (appending the true position as it is deterministic).
The rock qualities are sampled uniformly in $\{0,1\}$ indicating if they are ``good'', which makes the problem non-Gaussian but the parametric belief approximation can model a uniform distribution by placing the mean at the center of the uniform range and stretching the variance to match the uniform.
Lastly, the mineral exploration problem flattens the $1000$ subsurface $32\times32$ maps that each have associated ore quality per-pixel between $[0,1]$ into two images: a mean and standard deviation image of the ore quality that is stacked and used as input to a CNN.
The initial state distribution for the massive ore quantity closely follows a Gaussian, making the parametric belief approximation well suited.
For problems where Gaussian approximations do not capture the belief, the parameters of other distributions could be used as a belief representation or the particles themselves could be input into a network---first passing through an order-invariant layer \cite{igl2018deep}.
Scaling to larger observation spaces will not be an issue as BetaZero plans over belief states instead of observations.


\begin{table*}[t!]
    \centering
    \begin{threeparttable}
        \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{@{}lrrrrrrrrrr@{}}
            \arrayrulecolor{black} % revert
            \toprule
                & \multicolumn{2}{c}{$\text{LightDark}(5)$}  &  \multicolumn{2}{c}{$\text{LightDark}(10)$}  &  \multicolumn{2}{c}{$\text{RockSample}(15,15)$}  &  \multicolumn{2}{c}{$\text{RockSample}({20,20})$}  &  \multicolumn{2}{c}{Mineral Exploration} \\
            \arrayrulecolor{lightgray}
            \cmidrule{2-11}
            \arrayrulecolor{black} % revert
                & returns & time [s] & returns & time [s] & returns & time [s] & returns & time [s] & returns & time [s] \\
            \midrule
            \arrayrulecolor{white}
            BetaZero                 &  $\num{4.22 \pm 0.31}$  &  $\num{0.014}$  &  $\num{14.45 \pm 1.15}$  &  $\num{0.34}$  &  $\mathBF{20.15 \pm 0.71}$  &  $\num{0.48}$  &  $\mathBF{13.09 \pm 0.55}$  &  $\num{1.11}$  &  $\num{10.32 \pm 2.38}$  &  $\num{6.27}$  \\
            \midrule
            BetaZero (no bootstrap)  &  $\mathBF{4.47 \pm 0.28}$  &  $\num{0.014}$  &  $\mathBF{16.77 \pm 1.28}$  &  $\num{0.33}$  &  $\num{19.50 \pm 0.71}$  &  $\num{0.42}$  &  $\num{11.00 \pm 0.54}$  &  $\num{0.57}$  &  $\mathBF{10.67 \pm 2.25}$  &  $\num{4.46}$  \\
            \arrayrulecolor{black} % revert
            \bottomrule
        \end{tabular}
        \end{adjustbox}
        \begin{tablenotes}
            \small
            \item[\phantom{*}] {Reporting mean $\pm$ standard error over $100$ seeds (i.e., episodes); timing is average per episode.}
        \end{tablenotes}
    \end{threeparttable}
    \caption{Effect of $Q$-value bootstrapping in online \textit{BetaZero} performance (reporting returns and online timing).}\label{tab:bootstrap}
\end{table*}


\section*{Additional Analysis}

This section briefly describes the ablation studies and additional analyses omitted from the main body of the paper.

\subsection*{Ablation studies}

This section tests the effect of each contribution.
The influence of value and visit count information when selecting an action is shown in \cref{fig:z_sweep}.
Each cell is the mean return for the \textsc{RockSample}$(20,20)$ problem over $100$ online trials; selecting root-node actions via the $\argmax$ of \cref{eq:policy_q_weight} given $z_q$ and $z_n$.
The cell at $(0,0)$ corresponds to a uniform policy and thus samples actions instead.
Using only the visit counts (bottom cells) or only the values (left cells) to make decisions is worse than using a combination of the two.
The effect of the $Q$-weighting is also shown in the leftmost \cref{fig:ablations}, which suggests that it helps learn faster in \textsc{LightDark}$(10)$.

Unsurprisingly, using the state uncertainty encoded in the belief is crucial for learning as indicated in the middle of \cref{fig:ablations}.
Future work could directly input the particle set into the network, first passing through an order invariant layer \cite{zaheer2017deep}, to offload the belief approximation to the network itself.
Finally, the rightmost plot in \cref{fig:ablations} suggests that when branching on actions using progressive widening, it is important to first prioritize the actions suggested by the policy network.
Offline learning fails if instead we sample uniformly from the action space.


\subsection*{Bootstrapping analysis}

When adding a state-action pair $(b,a)$ to the MCTS tree, initializing the $Q$-values via bootstrapping with the value network may improve performance when using a small MCTS budget.
\Cref{tab:bootstrap} shows the results of an analysis comparing BetaZero with bootstrapping $Q_0(b,a) = R_b(b,a) + \gamma V_\theta(\tilde{b}')$ where $\tilde{b}' = \phi(b')$ and without bootstrapping $Q_0(b,a)=0$.
Each domain used the online parameters described in tables \ref{tab:ld_params}--\ref{tab:minex_params}.
Results indicate that bootstrapping was only helpful in the rock sample problems and incurs additional compute time due to the belief update done in $b' \sim T_b(b,a)$.
Note that bootstrapping was not used during offline training.
In problems with high stochasticity in the belief-state transitions, bootstrapping may be noisy during the initial search due to the transition $T_b$ sampling a single belief.
Further analysis could investigate the use of multiple belief transitions to better estimate the value; at the expense of additional computation.
The value estimate of $b$ could instead be used as the bootstrap but we would expect similar results to the one-step bootstrap as many problems we study have sparse rewards.


\subsection*{Limitations of double progressive widening}

\begin{figure}[b!]
    \captionsetup{font={small}}
    \centering
    \begin{minipage}{0.49\linewidth}
        \resizebox{\linewidth}{!}{
            \input{figures/appendix/spw_sweep_ld10}
    }
    \end{minipage}% <- Important.
    \hspace*{3mm} % \hfill
    \begin{minipage}{0.49\linewidth}
        \resizebox{\linewidth}{!}{%
            \input{figures/appendix/apw_sweep_ld10}
        }
    \end{minipage}

    \begin{minipage}[t]{0.49\linewidth}
        \caption{Sensitivity analysis of belief-state progressive widening in $\textsc{LightDark}(10)$.}
        \label{fig:spw_sensitivity_ld10}
    \end{minipage}% <- Important.
    \hspace*{3mm} % \hfill
    \begin{minipage}[t]{0.49\linewidth}
        \caption{Sensitivity analysis of action progressive widening in $\textsc{LightDark}(10)$.}
        \label{fig:apw_sensitivity_ld10}
    \end{minipage}
\end{figure}

Double progressive widening (DPW) is a straightforward approach to handle large or continuous state and action spaces in Monte Carlo tree search.
It is easy to implement and only requires information available in the tree search, i.e., number of children nodes and number of node visits.
It is known that MCTS performance can be sensitive to DPW hyperparameter tuning and \citeauthor{sokota2021monte} show that DPW ignores information about the relation between states that could provide more intelligent branching.
\citeauthor{sokota2021monte} introduce \textit{state abstraction refinement} that uses a distance metric between states to determine if a similar state should be added to the tree; requiring a state transition every time a state-action node is visited.
For our work, we want to reduce the number of expensive belief-state transitions in the tree and avoid the use of problem-specific heuristics required when defining distance metrics.
Using DPW in BetaZero was motivated by simplicity and allows future work to innovate on the components of belief-state and action branching.

To analyze the sensitivity of DPW, \cref{fig:spw_sensitivity_ld10}--\ref{fig:apw_sensitivity_ld10} show a sweep over the $\alpha$ and $k$ parameters for DPW in $\textsc{LightDark}(10)$.
\Cref{fig:spw_sensitivity_ld10} shows that the light dark problem is sensitive to belief-state widening and \cref{fig:apw_sensitivity_ld10} indicates that this problem may not require widening on all actions---noting that when $k=0$, the only action expanded on is the one prioritized from the policy head $a\sim P_\theta(\tilde{b}, \cdot)$.
The light dark problems have a small action space of ${|\mathcal{A}|=3}$, therefore this prioritization leads to good performance when only a single action is evaluated (left cells in \cref{fig:apw_sensitivity_ld10} when $k=0$).

In $\textsc{RockSample}(20,20)$, \cref{fig:spw_sensitivity_rs20}--\ref{fig:apw_sensitivity_rs20} indicates that this problem benefits from a higher widening factor (top right of the figures) as the action space ${|\mathcal{A}|=25}$ is larger and the belief-state transitions operate over a much larger state space.
DPW uses a single branching factor throughout the tree search and research into methods that adapt the branching based on learned information would be a valuable direction to explore.

\begin{figure}[b!]
    \captionsetup{font={small}}
    \centering
    \begin{minipage}{0.49\linewidth}
        \resizebox{\linewidth}{!}{
            \input{figures/appendix/spw_sweep_rs20}
    }
    \end{minipage}% <- Important.
    \hspace*{3mm} % \hfill
    \begin{minipage}{0.49\linewidth}
        \resizebox{\linewidth}{!}{%
            \input{figures/appendix/apw_sweep_rs20}
        }
    \end{minipage}

    \begin{minipage}[t]{0.49\linewidth}
        \caption{Sensitivity analysis of belief-state progressive widening in $\textsc{RockSample}(20,20)$.}
        \label{fig:spw_sensitivity_rs20}
    \end{minipage}% <- Important.
    \hspace*{3mm} % \hfill
    \begin{minipage}[t]{0.49\linewidth}
        \caption{Sensitivity analysis of action progressive widening in $\textsc{RockSample}(20,20)$.}
        \label{fig:apw_sensitivity_rs20}
    \end{minipage}
\end{figure}

\citeauthor{lim2023optimality} introduce a class of POMDP planning algorithms that use a fixed number of samples to branch on instead of progressive widening.
The bottom row of figures \ref{fig:spw_sensitivity_ld10}--\ref{fig:apw_sensitivity_rs20} (where $\alpha = 0$) can be interpreted as a fixed branching factor compared to progressive widening in the other cells. The analysis in the figures shows that there are cases where BetaZero has better performance when using progressive widening (show in the lighter colors).


\begin{figure*}[ht!]
    \centering
    \begin{minipage}{0.22\textwidth}
        \input{data/appendix/minex_data1}% load minex data files (BetaZero)
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/nn-lightdark}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.27\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/nn-rocksample}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.25\textwidth}
        \resizebox{\textwidth}{!}{%
            \input{figures/diagrams/nn-minex}
        }
    \end{minipage}

    \begin{minipage}[t]{0.22\textwidth}
        \caption{Light dark neural network architecture.}
        \label{fig:nn_lightdark}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.27\textwidth}
        \caption{Rock sample neural network architecture.}
        \label{fig:nn_rocksample}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.25\textwidth}
        \caption{Mineral exploration CNN architecture.}
        \label{fig:nn_minex}
    \end{minipage}
\end{figure*}


\section*{Network Architectures}

Figures \ref{fig:nn_lightdark}--\ref{fig:nn_minex} specify the neural network architectures for the three problem domains.
The networks were designed to be simple so that future work could focus on incorporating more complicated architectures such as residual networks.
Mineral exploration does not normalize the inputs and is the only problem where the input is treated as an image, thus we use a convolutional neural network (CNN).
Training occurs on normalized returns and an output denormalization layer is added to the value head to ensure proper magnitude of the predicted values.


\subsection*{Return scaling for output normalization}

For general POMDPs, the return can be an unbounded real-value and not conveniently in $[0,1]$ or $[-1,1]$; as is often the case with two player games. 
\citeauthor{schrittwieser2020mastering} use a categorical representation of the value split into a discrete support to make learning more robust \cite{schrittwieser2020intuition}.
We instead simply normalize the target before training as
\begin{equation}
    \bar{g}_t = (g_t - \mathbb{E}[G_\text{train}])/\sqrt{\Var[G_\text{train}]} % ,
\end{equation}
where $G_\text{train}$ is the set of returns used during training; keeping running statistics of all training data.
Intuitively, this ensures that the target values have zero mean and unit variance which is known to stabilize training \cite{lecun2002efficient}.
After training, a denormalization layer is added to the normalized output $\bar{v}$ of the value network as
\begin{equation}
    v_t = \bar{v} \sqrt{\Var[G_\text{train}]} + \mathbb{E}[G_\text{train}]
\end{equation}
to properly scale value predictions when the network is evaluated (which is done entirely internal to the network).


\begin{table*}[pt!]
    \centering
    \begin{adjustbox}{max width=0.92\textwidth}
    \begin{threeparttable}
        \begin{footnotesize}
        \begin{tabular}{@{}clrrrrm{9cm}@{}}
            \arrayrulecolor{black} % revert
            \toprule
             & \multirow{2}{*}{Parameter\tnote{*}}  &  \multicolumn{2}{c}{$\text{LightDark}(5)$}  &  \multicolumn{2}{c}{$\text{LightDark}(10)$}  &  \multirow{2}{*}{Description} \\
            \arrayrulecolor{lightgray}
            \cmidrule{3-6}
            \arrayrulecolor{black} % revert
                    &  & Offline & Online & Offline & Online & \\
            \midrule
            \multirow{3}{*}{\makecell{BetaZero policy\\iteration parameters}} & $n_\text{iterations}$ & \num{30} & --- & \num{30} & --- & Number of offline BetaZero policy iterations. \\
             & $n_\text{data}$ & \num{500} & --- & \num{500} & --- & Number of parallel MCTS data generation episodes per policy iteration. \\
             & bootstrap $Q_0$ & false & false & false & false & Use bootstrap estimate for initial $Q$-value in MCTS. \\
            \arrayrulecolor{lightgray}
            \midrule
            \multirow{3}{*}{\shortstack{Neural network\\parameters}} & $n_\text{epochs}$ & \num{50} & --- & \num{50} & --- & Number of training epochs. \\
             & $\alpha$ & \num{1e-4} & --- & \num{1e-4} & --- & Learning rate. \\
             & $\lambda$ & \num{1e-5} & --- & \num{1e-5} & --- & $L_2$-regularization parameter. \\
             \midrule
             \multirow{10}{*}{\shortstack{MCTS\\parameters}} & $n_\text{online}$ & \num{100} & \num{1300} & \num{100} & \num{1000} & Number of tree search iterations of MCTS.\\
              & $c$ & \num{1} & \num{1} & \num{1} & \num{1} & PUCT exploration constant. \\
              & $k_\text{action}$ & \num{2.0} & \num{2.0} & \num{2.0} & \num{2.0} & Multiplicative action progressive widening value.\\
              & $\alpha_\text{action}$ & \num{0.25} & \num{0.25} & \num{0.25} & \num{0.25} & Exponential action progressive widening value.\\
              & $k_\text{state}$ & \num{2.0} & \num{2.0} & \num{2.0} & \num{2.0} & Multiplicative belief-state progressive widening value.\\
              & $\alpha_\text{state}$ & \num{0.1} & \num{0.1} & \num{0.1} & \num{0.1} & Exponential belief-state progressive widening value.\\
              & $d$ & \num{10} & \num{10} & \num{10} & \num{10} & Maximum tree depth. \\
              & $\tau$ & \num{0} & \num{0} & \num{0} & \num{0} & Exploration temperature for final root node action selection. \\
              & $z_q$ & \num{1} & \num{1} & \num{1} & \num{1} & Influence of $Q$-values in final criteria. \\
              & $z_n$ & \num{1} & \num{1} & \num{1} & \num{1} & Influence of visit counts in final criteria. \\
            \arrayrulecolor{black} % revert
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \item[*] {Entries with ``---'' denote non-applicability and ``$\cdot$'' denotes they are disabled.}
        \end{tablenotes}
        \end{footnotesize}
    \end{threeparttable}
    \end{adjustbox}
    \caption{\textit{BetaZero} parameters for the $\textsc{LightDark}$ problems.}\label{tab:ld_params}
\end{table*}


\begin{table*}[hp!]
    \centering
    \begin{adjustbox}{max width=0.92\textwidth}
    \begin{threeparttable}
        \begin{footnotesize}
        \begin{tabular}{@{}clrrrrm{9cm}@{}}
            \arrayrulecolor{black} % revert
            \toprule
             & \multirow{2}{*}{Parameter}  &  \multicolumn{2}{c}{$\text{RockSample}(15,15)$}  &  \multicolumn{2}{c}{$\text{RockSample}(20,20)$}  &  \multirow{2}{*}{Description} \\
            \arrayrulecolor{lightgray}
            \cmidrule{3-6}
            \arrayrulecolor{black} % revert
                    &  & Offline & Online & Offline & Online & \\
            \midrule
            \multirow{3}{*}{\shortstack{BetaZero policy\\iteration parameters}} & $n_\text{iterations}$ & \num{50} & --- & \num{50} & --- & Number of offline BetaZero policy iterations. \\
             & $n_\text{data}$ & \num{500} & --- & \num{500} & --- & Number of parallel MCTS data generation episodes per policy iteration. \\
             & bootstrap $Q_0$ & false & true & false & true & Use bootstrap estimate for initial $Q$-value in MCTS. \\
            \arrayrulecolor{lightgray}
            \midrule
            \multirow{3}{*}{\shortstack{Neural network\\parameters}} & $n_\text{epochs}$ & \num{10} & --- & \num{10} & --- & Number of training epochs. \\
             & $\alpha$ & \num{1e-3} & --- & \num{1e-3} & --- & Learning rate. \\
             & $\lambda$ & \num{1e-5} & --- & \num{1e-5} & --- & $L_2$-regularization parameter. \\
             \midrule
             \multirow{10}{*}{\shortstack{MCTS\\parameters}} & $n_\text{online}$ & \num{100} & \num{100} & \num{100} & \num{100} & Number of tree search iterations of MCTS.\\
              & $c$ & \num{50} & \num{50} & \num{50} & \num{50} & PUCT exploration constant. \\
              & $k_\text{action}$ & $\cdot$ & \num{5.0} & $\cdot$ & $\cdot$ & Multiplicative action progressive widening value.\\
              & $\alpha_\text{action}$ & $\cdot$ & \num{0.9} & $\cdot$ & $\cdot$ & Exponential action progressive widening value.\\
              & $k_\text{state}$ & $\cdot$ & \num{1.0} & \num{1.0} & \num{1.0} & Multiplicative belief-state progressive widening value.\\
              & $\alpha_\text{state}$ & $\cdot$ & \num{0.0} & \num{0.0} & \num{0.0} & Exponential belief-state progressive widening value.\\
              & $d$ & \num{15} & \num{15} & \num{4} & \num{4} & Maximum tree depth. \\
              & $\tau$ & \num{1.0} & \num{0} & \num{1.5} & \num{0} & Exploration temperature for final root node action selection. \\
              & $z_q$ & \num{1} & \num{0.4} & \num{1} & \num{0.5} & Influence of $Q$-values in final criteria. \\
              & $z_n$ & \num{1} & \num{0.9} & \num{1} & \num{0.8} & Influence of visit counts in final criteria. \\
            \arrayrulecolor{black} % revert
            \bottomrule
        \end{tabular}
        \end{footnotesize}
    \end{threeparttable}
    \end{adjustbox}
    \caption{\textit{BetaZero} parameters for the $\textsc{RockSample}$ problems.}\label{tab:rs_params}
\end{table*}


\begin{table*}[pb!]
    \centering
    \begin{adjustbox}{max width=0.80\textwidth}
    \begin{threeparttable}
        \begin{footnotesize}
        \begin{tabular}{@{}clrrm{9cm}@{}}
            \arrayrulecolor{black} % revert
            \toprule
             & Parameter  &  Offline & Online  &  Description \\
            \midrule
            \multirow{3}{*}{\shortstack{BetaZero policy\\iteration parameters}} & $n_\text{iterations}$ & \num{20} & --- & Number of offline BetaZero policy iterations. \\
             & $n_\text{data}$ & \num{100} & --- & Number of parallel MCTS data generation episodes per policy iteration. \\
             & bootstrap $Q_0$ & false & false & Use bootstrap estimate for initial $Q$-value in MCTS. \\
            \arrayrulecolor{lightgray}
            \midrule
            \multirow{3}{*}{\shortstack{Neural network\\parameters}} & $n_\text{epochs}$ & \num{10} & --- & Number of training epochs. \\
             & $\alpha$ & \num{1e-6} & --- & Learning rate. \\
             & $\lambda$ & \num{1e-4} & --- & $L_2$-regularization parameter. \\
             \midrule
             \multirow{10}{*}{\shortstack{MCTS\\parameters}} & $n_\text{online}$ & \num{50} & \num{50} & Number of tree search iterations of MCTS.\\
              & $c$ & \num{57} & \num{57} & PUCT exploration constant. \\
              & $k_\text{action}$ & \num{41.09} & \num{41.09} & Multiplicative action progressive widening value.\\
              & $\alpha_\text{action}$ & \num{0.57} & \num{0.57} & Exponential action progressive widening value.\\
              & $k_\text{state}$ & \num{37.13} & \num{37.13} & Multiplicative belief-state progressive widening value.\\
              & $\alpha_\text{state}$ & \num{0.94} & \num{0.94} & Exponential belief-state progressive widening value.\\
              & $d$ & \num{5} & \num{5} & Maximum tree depth. \\
              & $\tau$ & \num{1.0} & \num{0} & Exploration temperature for final root node action selection. \\
              & $z_q$ & \num{1} & \num{1} & Influence of $Q$-values in final criteria. \\
              & $z_n$ & \num{1} & \num{1} & Influence of visit counts in final criteria. \\
            \arrayrulecolor{black} % revert
            \bottomrule
        \end{tabular}
        \end{footnotesize}
    \end{threeparttable}
    \end{adjustbox}
    \caption{\textit{BetaZero} parameters for the $\textsc{Mineral Exploration}$ problem.}\label{tab:minex_params}
\end{table*}


\section*{Hyperparameters and Tuning}

The hyperparameters used during offline training and online execution are described in tables \ref{tab:ld_params}--\ref{tab:minex_params}.
Offline training refers to the BetaZero policy iteration steps that collect parallel MCTS data (\textit{policy evaluation}) and then retrain the network (\textit{policy improvement}).
The online execution refers to using the BetaZero policy after offline training to evaluate its performance through online tree search.
The main difference between these two settings is the final criteria used to select the root node action in MCTS.
During offline training of problems with large action spaces (e.g., rock sample and mineral exploration), sampling root node actions according to the $Q$-weighted visit counts with a temperature $\tau$ ensures exploration.
To evaluate the performance online, root node action selection takes the maximizing action of the $Q$-weighted visit counts.
During training, we also evaluate a holdout set that uses the $\argmax$ criteria to monitor the true performance of the learned policy.

The MCTS parameters for the mineral exploration problem were tuned using Latin hypercube sampling based on the lower-confidence bound of the returns.
During training, the rock sample problems disabled progressive widening to expand on all actions and transition to a single belief state.
Then for online execution, we tuned the DPW parameters as shown in figures \ref{fig:spw_sensitivity_ld10}--\ref{fig:spw_sensitivity_rs20}. %; which also highlights the limitations of DPW and the tuning sensitivity.
The problems train with a batch size of $1024$ over $80\%$ of $100{,}000$ samples from one round of data collection (${n_\text{buffer}=1)}$ using $p_\text{dropout}$ of $0.2$, $0.5$, $0.7$, respectively.
Mineral exploration used a value function loss of MAE (otherwise MSE was used), light dark used the Adam optimizer \cite{kingma2014adam}, and the other problems used RMSProp \cite{hinton2014rmsprop} and batch norm regularization with a momentum of $0.7$.


\addtocounter{algorithm}{1}
\begin{figure*}[b!]
    \centering
    \begin{minipage}{0.8\textwidth}
        \input{algorithms/collect-data}
    \end{minipage}
\end{figure*}

\begin{figure*}[b!]
    \centering
    \begin{minipage}{0.8\textwidth}
        \input{algorithms/mcts-outer}
    \end{minipage}
\end{figure*}


\section*{Compute Resources}

BetaZero was designed to use a single GPU to train the network and parallelize MCTS evaluations across available CPUs.
Evaluating the networks on the CPU is computationally inexpensive due to the size of the neural networks (see \cref{fig:nn_lightdark}--\ref{fig:nn_minex}).
This design was chosen to enable future research without a computational bottleneck.
A single NVIDIA A100 was used with 80GB of memory on an Ubuntu 22.04 machine with 500 GB of RAM.
Parallel data collection processes were run on $50$ processes split evenly over two separate Ubuntu 22.04 machines: 1) with 40 Intel Xeon 2.3 GHz CPUs, and 2) with 56 Intel Xeon 2.6 GHz CPUs.
\Cref{alg:collect_data} (line \ref{line:parallel}) show where CPU parallelization occurs.
In practice, the MCTS data generation simulations are the bottleneck of the offline component of BetaZero and not the network training---thus, parallelization is useful.

\section*{Open-Sourced Code and Experiments}

The BetaZero algorithm has been open sourced and incorporated into the Julia programming language POMDPs.jl ecosystem \cite{pomdps_jl}.
Fitting into this ecosystem allows BetaZero to access existing POMDP models and can easily be compared to various POMDP solvers.
The user constructs a \texttt{BetaZeroSolver} that takes parameters for policy iteration and data generation, parameters for neural network architecture and training, and parameters for MCTS (described in the tables above).
The user may choose to define a method that inputs the belief $b$ and outputs the belief representation $\tilde{b}$ used by the neural network (where the default computes the belief mean and std).

All experiments, including the experiment setup for the baseline algorithms with their heuristics, are included for reproducibility.
Code to run MCTS data collection across parallel processes is also included.
The code and experiments presented in this work are available online: 
https://github.com/sisl/BetaZero.jl


\section*{BetaZero Algorithm}

Algorithms \ref{alg:betazero}--\ref{alg:mcts-top-lvl} detail the full BetaZero policy iteration algorithm, including data collection and $Q$-weighted MCTS.
Descriptions of parameters $\psi$ are listed in tables \ref{tab:ld_params}--\ref{tab:minex_params}.

\addtocounter{algorithm}{-3}
\begin{figure}[h!]
    \centering
    \input{algorithms/betazero}
\end{figure}
