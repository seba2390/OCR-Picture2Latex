\section{Experiment}
\label{sec:exp}
In this section, we will present extensive offline and online A/B experiment results of TransAct. We compare TransAct with baseline models using Pinterest's internal training data. 
\subsection{Experiment Setup}
% We provide more details in this section for reproducibility.
\subsubsection{Dataset}
% reference doc: https://docs.google.com/document/d/1Tv8gbaXLfh2elTA20rCn6t99qswB0B_9xRSK3CI5PHw/edit#heading=h.pjp5lpc31bzn
We construct the offline training dataset from three weeks of Pinterest Homefeed view log (FVL). The model is trained on the first two weeks of FVL and evaluated on the third week. The training data is sampled based on user state and labels. 
% For example, the log is naturally dominated by frequent users. So we intentionally raise the sampling ratio for new and casual users to prevent underfitting for those less frequent users. 
For example, we design the sampling ratio for different label actions based on their statistical distribution and importance. 
In addition, since users only engage with a small portion of pins shown on their Homefeed page, most of the training samples are negative samples. To balance the highly skewed dataset and improve model accuracy,
we employ downsampling on the negative samples and set a fixed ratio between the positive and negative samples.
Our training dataset contains 3 billion training instances of 177 million users and 720 million pins.

In this paper, we conduct all experiments with the Pinterest dataset. We do not use public datasets as they lack the necessary realtime user action sequence metadata features, such as item embeddings and action types, required by TransAct.  
Furthermore, they are incompatible with our proposal of realtime-batch hybrid model, which requires both realtime and batch user features. 
And they cannot be tested in online A/B experiments. 

% \td{we do not use public dataset because they do not have realtime user features, they do not release any learned user embeeding to support our hybrid setup}
\subsubsection{Hyperparameters}
Realtime user sequence length is $|S| = 100$ and the dimension of action embedding $d_{action} = 32$. 
The encoded sequence feature is passed through a transformer encoder composed of 2 transformer blocks, with a default dropout rate of 0.1. 
The feed forward network in the transformer encoder layer has a dimension of $d_{hidden}=32$, and positional encoding is not used. 
The implementation is done using PyTorch. We use an Adam~\cite{adam} optimizer with a learning rate scheduler. The learning rate begins with a warm-up phase of 5000 steps, gradually increasing to 0.0048, and finally reduced through cosine annealing. The batch size is 12000.
% Performance of variants and different hyper-parameters is examined below.

\subsection{Offline Experiment}

\subsubsection{Metrics}

The offline evaluation data, unlike training data, is randomly sampled from FVL to represent the true distribution of the real-world traffic. With this sampling strategy, the offline evaluation data is representative of the entire population, reducing the variance of evaluation results.
% , which provides a more accurate evaluation of the model's performance. 
% Unlike when training, we evaluate our model on a dataset with minimal bias. Specifically, we strive to eliminate bias from two sources: sampling and the current ranking model's predictions. Debiasing against sampling is relatively simple; we randomly choose a small fraction of Homefeed requests to use for evaluation. To avoid position bias introduced by the existing ranking model, we randomize the order of pins in requests that will be used for evaluation.

In addition to sampling bias, we also eliminate position bias in offline evaluation data.
Position bias refers to the tendency for items at the top of a recommendation to receive more attention and engagement than the items lower down the list. This can be a problem when evaluating a ranking model, as it can distort the evaluation results and make it difficult to accurately assess the model's performance. To avoid position bias, we randomize the order of pins in a very small portion of Homefeed recommendation sessions. This is done by shuffling the recommendations before presenting them to users. We gather the FVL for those randomized sessions and only use randomized data to perform the offline evaluation.


Our model is evaluated on HIT@3.
A chunk $\vc = [p_1, p_2, \dots, p_n]$ refers to a group of pins that are recommended to a user at the same time. 
Each input instance to the ranking model is associated with a user id $u\_{id}$, a pin id $p\_{id}$, and a chunk id $c\_{id}$. 
The evaluation output is grouped by $(u\_{id}, c\_{id})$ so that it contains the model output from the same ranking request. We sort the pins from the same ranking request by a final ranking score $\mathcal{S}$, which is a linear combination of Pinnability output heads $f(\vx)$. 
\begin{equation}
\mathcal{S} = \sum_{h \in H } u_h f(\vx)_h
\end{equation}
Then we take the top $K$ ranked pins in each chunk and calculate the hit@K for all heads, denoted by $\beta_{c,h}$, which is defined as the number of topK-ranked pins whose labels of $h$ are 1. For example, if a chunk $\vc = [p_1, p_2, p_3,\dots , p_n]$ is sorted by $\mathcal{S}$, and the user repins $p_1$ and $p_4$, then hit@K of repin $\beta_{c,repin}=1$ when $K=3$. 

We calculate the aggregated HIT@3  for each head $h$ as follows:


\begin{equation}
HIT@3/h = \frac{\sum_{u \in U }\sum_{c \in C_u } \beta_{c,h}}{\left| U \right|}
\end{equation}

It is important to note that for actions indicating positive engagement, such as repin or click, a higher HIT@K score means better model performance. Conversely, for actions indicating negative engagement, such as hide, a lower HIT@K/hide score is desirable.


% At Pinterest, users are divided into two categories: core and non-core. Core users are those who have actively saved pins to boards within the past 28 days, while the rest are considered non-core.
At Pinterest, a non-core user is defined as a user who has not actively saved pins to boards within the past 28 days.
% At Pinterst, a non-core user is defined as a user who has not performed the repin action within at least 4 of the past 28 days.
Non-core users tend to be less active and therefore pose a challenge in terms of improving their recommendation relevance due to their limited historical engagement. This is also referred to as the cold-start user problem in recommendation~\cite{natarajan2020resolving}. Despite the challenges, it is important to retain non-core users as they play a crucial role in maintaining a diverse and thriving community, contributing to long-term platform growth.

All reported results are statistically significant (p-value $ < 0.05$) unless stated otherwise.
\subsubsection{Results}



\begin{table}
\caption{Offline evaluation of comparing existing methods with TransAct. ($^*$ statistically insignificant)} \label{tab:offline_res}
  \resizebox{.45\textwidth}{!}{%
  \begin{tabular}{ccccc}
    \toprule
    % Methods & HIT@3/hide& hide& HIT@3/repin& repin\\
    \multirow{2}{*}{Methods} & \multicolumn{2}{c}{HIT@3/repin}& \multicolumn{2}{c}{HIT@3/hide}\\ \cmidrule{2-5}%
    % \midrule
       & all  & non-core  & all & non-core  \\
    \midrule
    WDL + seq & +0.21\% & +0.35\%& -1.61\% &  -1.55\% \\
    BST (all actions) & +4.41\% &+5.09\% &  +2.33\% &+3.59\% \\
    BST (positive actions) & +7.34\% &+8.16\% &  -1.12\%$^*$ & -3.14\%$^*$ \\
    TransAct & \textbf{+9.40\%} & \textbf{+10.42\%} & \textbf{-14.86}\% & \textbf{-13.54\%}\\
  \bottomrule
\end{tabular}%
}
\end{table}

% \td{describe competitors and results analysis}
We compare TransAct with existing methods of sequential recommendation. 
The first baseline is the WDL model~\cite{cheng2016wide} that incorporates sequence features as part of its wide features. Due to the large size of the sequence features, the number of parameters in the feature cross layer would grow quadratically, making it unfeasible for both training and online serving. 
Therefore, we used an averaging pooling for PinSage embeddings of user actions to encode the sequence.
The second baseline is Alibaba's behavior sequence transformer (BST) model~\cite{alibaba_seq_tfmr}. 
We trained 2 BST model variants here: one with only positive actions in user sequence, the other with all actions. 
We opted not to compare our results with DIN~\cite{DIN} as BST has already demonstrated its superiority over DIN. Additionally, we did not compare with variants like BERT4Rec~\cite{BERT4Rec} as the problem formulations are different and a direct comparison is not feasible.


The results of the model comparison are presented in Table~\ref{tab:offline_res}. It is evident that BST and TransAct outperform the WDL model, demonstrating the necessity of using a specialized sequential model to effectively capture short-term user preferences through real-time user action sequence features. 
BST performs well when only positive actions are encoded, however, it struggles to distinguish negative actions. 
In contrast, TransAct outperforms BST, particularly in terms of hide prediction, due to its ability to distinguish between different actions by encoding action types. 
Furthermore, TransAct also exhibits improved performance in HIT@3/repin compared to BST, which can be attributed to its effective early fusion and output compression design. 
A common trend across all groups is that the performance for non-core users is better than for all users, this is due to realtime user action features being crucial for users with limited engagement history on the platform, as they provide the only source of information for the model to learn their preferences.
\subsection{Ablation Study}



\subsubsection{Hybrid ranking model}
First, we investigate the effect of the realtime-batch hybrid design by examining the individual impact of TransAct(realtime component) and Pinnerformer(batch component). 
Table~\ref{tab:hybrid} shows the relative decrease in offline performance from the model containing all user features as we remove each component.
TransAct captures users' immediate interests, which contribute the most to the user's overall engagement, while PinnerFormer (PF)~\cite{pinnerformer} extracts users' long-term preferences from their historical behavior.
We observe that TransAct is the most important user understanding feature in the model, but we still see value from the large-scale training and longer-term interests captured by PinnerFormer, showing that longer-term batch user understanding can complement a realtime engagement sequence for recommendations.
In the last row of Table~\ref{tab:hybrid}, we show that removing all user features other than TransAct and PinnerFormer only leads to a relatively small drop in performance, demonstrating the effectiveness of our combination of a realtime sequence model with a pre-trained batch model.
% chatgpt suggestion
% First, we evaluate the impact of our key personalization features on recommendation quality. Table~\ref{tab:hybrid} presents the reduction in offline performance as we eliminate each feature from a model incorporating all user features. Our proposed TransAct approach captures users' realtime interests, which have the most significant effect on user engagement, while PinnerFormer (PF)\cite{pinnerformer} extracts users' long-term preferences from their historical behavior. Our results indicate that TransAct is the most crucial user understanding feature in the model, however, we also find that PinnerFormer, with its large-scale training and long-term interests, adds complementary value. The last row of Table\ref{tab:hybrid} demonstrates that removing all user features except for TransAct and PinnerFormer only results in a minor decrease in performance, emphasizing the efficacy of our hybrid approach combining a real-time sequence model with a pre-trained batch model.





% The hybrid ranking model uses user action sequences to understand user preferences in realtime and batch approaches. TransAct captures users' immediate interests from realtime user action sequences, while PinnerFormer (PF)~\cite{pinnerformer} effectively extracts users' long-term preferences from their historical behavior. 
% % By integrating TransAct and PinnerFormer with other user features, we build a highly effective hybrid ranking model that leverages both realtime and batch information.


% Table~\ref{tab:hybrid} illustrates the effectiveness of the hybrid model design.
% % We observe that realtime component (TransAct) contributes to the majority of engagement gain. In addition, the batch component (PF) is also indispensable, which contributes to 25\% of the HIT@3/repin gain. 
% TransAct significantly improved repins and hides prediction, while the batch approach using Pinnerformer complements TransAct and lifted the metrics further. \td{rewrite based on data}



% \begin{table}
% \begin{threeparttable}
%   \caption{Ablation study of realtime-batch hybrid model}
%   \label{tab:hybrid}
%   \begin{tabular}{crr}
%     \toprule
%     RS setting & HIT@3/repin & HIT@3/hide \\
%     \midrule
%     Hybrid\tnote{*}  & - & - \\
%     No TransAct  & -8.6\% & +7.21\%\\
%     No PF & -3\% & +2\% \\
%     TransAct + PF only  & -1\% & +0.5\%\\
%     % Vanilla Transformer & \textbf{-8.45\%} & \textbf{+1.56\%}\\
%     % TransAct & \textbf{-14.86}\% & \textbf{+9.40\%}\\
%   \bottomrule
% \end{tabular}
% \begin{tablenotes}\footnotesize
% \item[*] Hybrid includes PF, TransAct, and other categorical/numerical user features
% \end{tablenotes}
% \end{threeparttable}
% \end{table}
\
\begin{table}
\caption{Ablation study of realtime-batch hybrid model}
  \label{tab:hybrid}
\resizebox{.4\textwidth}{!}{% 
  \begin{tabular}{ccccc}
    \toprule
    \begin{tabular}{@{}c@{}}TransAct\end{tabular} & PF & \begin{tabular}{@{}c@{}}Other User\\Features\end{tabular} & HIT@3/repin & HIT@3/hide \\
    \midrule
    \checkmark & \checkmark & \checkmark  & --- & --- \\
    \checkmark & \ding{53} & \checkmark  & -2.46\% & +3.61\% \\
    \ding{53} & \checkmark & \checkmark  & -8.59\% & +17.45\% \\
    % \ding{53} & \ding{53} & \checkmark  & -9.95\% & +14.36\% \\
    \checkmark & \checkmark & \ding{53}  & -0.67\% & +1.40\% \\
    % No TransAct  & -8.6\% & +7.21\%\\
    % No PF & -3\% & +2\% \\
    % TransAct + PF only  & -1\% & +0.5\%\\
    % Vanilla Transformer & \textbf{-8.45\%} & \textbf{+1.56\%}\\
    % TransAct & \textbf{-14.86}\% & \textbf{+9.40\%}\\
  \bottomrule
\end{tabular}%
}
\end{table}


\subsubsection{Base sequence encoder architecture}

We perform an offline evaluation on different sequential models that process realtime user sequence features. We use different architectures to encode the PinSage embedding sequence from users' realtime actions.


\textbf{Average Pooling}: use the average of PinSage embeddings in user sequence to present the user’s short-term interest

\textbf{CNN}: use a 1-d CNN with 256 output channels to encode the sequence. Kernel size is 4 and stride is 1. 

\textbf{RNN}: use 2 RNN layers with a hidden dimension of 256, to encode a sequence of PinSage embeddings. 

\textbf{LSTM}: use Long Short-Term Memory (LSTM)~\cite{lstm}, a more sophisticated version of RNN that better captures longer-term dependencies by using memory cells and gating. We use 2 LSTM layers with the hidden size of 256.

\textbf{Vanilla Transformer}: encodes only PinSage embeddings sequence directly using the Transformer encoder module. We use 2 transformer encoder layers with a hidden dimension of 32.
 
% \begin{itemize}
% \item Average Pooling: uses the average of PinSage embeddings in user sequence to present user’s short term interest
% \item Convolutional Neural Network (CNN): uses a 1-d CNN with 256 output channels to encode the sequence of PinSage embeddings. Kernal size is 4 and stride is 1. 
% % CNN is suitable to capture the dependent relationship across local information
% \item Recurrent Neural Network (RNN): uses 2 RNN layers with hidden dimension of 256, to encoder a sequence of PinSage embeddings. 
% % Compared to CNN, RNN better captures longer term dependencies.
% \item Long Short-Term Memory (LSTM)\cite{lstm}: uses LSTM, a more sophisticated version of RNN that captures longer-term dependencies even better than RNN by using memory cells and gating. We use 2 LSTM layers with hidden size of 256.
% \item Vanilla Transformer: encodes only PinSage embeddings sequence directly using the Transformer encoder module. We use 2 transformer encoder layer with hidden dimension of 32.
% % \item Dedicated user sequence transformer: our architecture design as illustrated in \td{fix} \ref{fig:seq_tfmr}.

% \end{itemize}
The baseline group is the Pinnability model without realtime user sequence feature.
% Table~\ref{tab:seq_encoder} shows the offline performance of different sequence encoder architecture. 
% For repin, we try to improve the HIT@3. For hide, the goal is to decrease HIT@3. 
From Table~\ref{tab:seq_encoder}, we learned that using realtime user sequence features, even with a simple average pooling method, improves engagement. 
Surprisingly, more complex architectures like RNN, CNN, and LSTM do not always perform better than average pooling. However, the best performance is achieved with the use of a vanilla transformer, as it significantly reduces HIT@3/hide and improves HIT@3/repin.



\begin{table}
  \caption{Offline evaluation of sequence encoder architecture}
  \label{tab:seq_encoder}
  \begin{tabular}{crr}
    \toprule
    Sequence Encoder & HIT@3/repin & HIT@3/hide\\
    \midrule
    Average Pooling & +0.21\%& -1.61\% \\
    CNN & +0.08\% & -1.29\% \\
    RNN & -1.05\% & -2.46\% \\
    LSTM & -0.75\% & -2.98\% \\
    Vanilla Transformer & \textbf{+1.56\%} & \textbf{-8.45\%} \\
    % BST\cite{alibaba_seq_tfmr} & +3.59\% &+4.41\%\\
    % TransAct & \textbf{+9.40\%} & \textbf{-14.86}\% \\
  \bottomrule
\end{tabular}
\end{table}

% https://docs.google.com/spreadsheets/d/1HjHALNRgX00wH_XB6I7wFuBYUZoNZr0rNzgSADIsUn8/edit#gid=0


\subsubsection{Early fusion and sequence length selection}

As discussed in Section~\ref{sec:earlyfusion}, early fusion plays a crucial role in the ranking model.  
By incorporating early fusion, the model can not only take into account the dependency between different items in the user's action history but also explicitly learn the relationship between the ranking candidate pin and each pin that the user has engaged with in the past.

% Our approach of concatenating candidate pin embedding with each of the user sequence pin (concat) outperforms existing early fusion method (append)\cite{alibaba_seq_tfmr}.

Longer user action sequences naturally are more expressive than short sequences. 
To learn the effect of input sequence length on the model performance, we evaluate the model on different lengths of user sequence input.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/seq_len_concat_exp.png}
  \caption{Effect of early fusion and sequence length on ranking model performance (HIT@3/repin, HIT@3/hide) }
  \label{fig:seq_len}
  \Description{seq length experiment}
\end{figure}

An analysis of Figure~\ref{fig:seq_len} reveals that there is a positive correlation between sequence length and performance. The performance improvement increases at a rate that is sub-linear with respect to the sequence length. The use of concatenation as the early fusion method was found to be superior to the use of appending. Therefore, the optimal engagement gain can be achieved by utilizing the maximum available sequence length and employing concatenation as the early fusion method.

\subsubsection{Transformer hyperparameters}
% \td{layer, dim of tfmr. Performance vs cost (GPU benchmark).}
We optimized TransAct's transformer encoder by adjusting its hyperparameters. As shown in Figure~\ref{fig:tfmr_hpt},
increasing the number of transformer layers and feed forward dimension leads to higher latency and also better performance.
While the best performance was achieved using 4 transformer layers and 384 as the feed forward dimension, this came at the cost of a 30\% increase in latency, which does not meet the latency requirement.
To balance performance and user experience, we chose 2 transformer layers and 32 as the hidden dimension.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/size_vs_perf_and_latency_new.pdf}
  \caption{Effect of transformer hyperparameters on model performance and latency}
  \label{fig:tfmr_hpt}
  \Description{seq length experiment}
\end{figure}


\subsubsection{Transformer output compression}
\label{sec:compression}
% The output of the transformer encoder is $\vO = (\vo_0, \vo_1, \dots, \vo_{|S|-1} ) \in \mathbb{R}^{|S| \times d_{hidden}}$. 
The transformer encoder produces $\vO \in \mathbb{R}^{d \times |S|}$, with each column corresponding to an input user action. 
However, directly using $\vO$ as input to the DCN v2 layers for feature crossing would result in excessive time complexity, which is quadratic to the input size.

To address this issue, we explored several approaches to compress the transformer output.
Figure~\ref{fig:seq_encoder} shows that the highest HIT@3/repin is achieved by combining the first K columns and applying max pooling to the entire sequence. 
The first K column represents the most recently engaged pins and the max pooling is an aggregated representation of the entire sequence,
Although using all columns improved HIT@3/hide slightly, the combination of the first K columns and max pooling provided a good balance between performance and latency. We use K=10 for TransAct.
% Overall, the TransAct approach provided satisfactory results with low latency.
% Since the input user sequence feature is sorted by action timestamp in descending order, the first output columns represents the most recently engaged pins. And the maxpooling can be viewed as the aggregated presentation of the whole sequence.
% \td{dive into the results more}
% \begin{itemize}
% \item the first output token, which is the most recent entry of user aciton sequence $\vo_0$
% \item a random output token $\vo_r$
% \item the first K output tokens of \texttt{CONCAT}$(\vo_0:\vo_{k-1})$
% \item the max pooling of $\vO$: \texttt{MAXPOOL}$(\mO)$. 
% \item the max pooling of $\vO$ and the first K output tokens: \texttt{CONCAT}$(\vo_0:\vo_{k-1}) + $ \texttt{MAXPOOL}$(\mO)$
% % \item Dedicated user sequence transformer: our architecture design as illustrated in \td{fix} \ref{fig:seq_tfmr}.
% \end{itemize}



% \begin{table*}
%   \caption{Offline evaluation of different transformer output compression (K=10). All p-values are less than 0.05. *All values in HIT@3/hide column are multiplied by 1e4 \nikil{remove absolute numbers} \nikil{i think we need the max pooling alone here too} \nikil{I'd consider moving the notation part to the appendix}}
%   \label{tab:tfmr_output}
%   \begin{tabular}{ccrrrr}
%     \toprule
%     Transformer output compression & Notation & HIT@3/hide\textsuperscript{*} & $\Delta$ & HIT@3/repin & $\Delta$ \\
%     \midrule
%     no user sequence (baseline)      & -- & 9.49 &  -      & 0.0547 & -\\
%         a random output token            & $\vo_{random}$ & 8.45   & -10.96\%    & 0.0585 & +6.80\%\\
%         first output token               & $\vo_0$ & 8.42   & -11.28\% &    0.0591     &+7.82\%\\
%             random K output tokens           & $(O_{\text{random } K \text{ out of } |S|})$   & 8.34 &     -12.12\%         & 0.0588  &   +7.42\%  \\
%     first K output tokens            & $(\vo_0:\vo_{k-1})$ & 8.13 & -14.33\% &0.0599 & +9.38\%\\
%     all output tokens     & $(\vo_0:\vo_{|S|-1})$  &    \textbf{8.00}   & \textbf{-15.70\%} &  0.0596  & +8.86\%\\
%     max pooling      & \texttt{MAXPOOL}$(\mO)$  &    8.14   & -14.15\% &  0.0583  & +6.38\%\\
%         first K output + max pooling (TransAct)     & $(\vo_0:\vo_{k-1})+$ \texttt{MAXPOOL}$(\mO)$ &  8.08 & -14.86\% & \textbf{0.0599} & \textbf{+9.41\%}\\
%     all output tokens + max pooling  & $(\vo_0:\vo_{|S|-1})+$ \texttt{MAXPOOL}$(\mO)$  &    8.29   & -12.64\% &  0.0595  & +8.67\%\\


%   \bottomrule
% \end{tabular}
% \end{table*}

\begin{table}
  \caption{Ablation study of transformer output compression}
  \label{tab:tfmr_output}
\resizebox{.45\textwidth}{!}{% 
\begin{tabular}{ccrr}
    \toprule
    Output Compression &  Size & HIT@3/repin &  HIT@3/hide   \\
    \midrule
    % no user sequence (baseline)      &  -      & -\\
        a random  col    & $d$ &+6.80\%  & -10.96\%   \\
        first  col       & $d$ &+7.82\%     & -11.28\%  \\
    random K  cols     & $Kd$&  +7.42\%     & -12.12\%       \\
    first K cols    & $Kd$ & +9.38\%   & -14.33\% \\
    all cols    & $|S|d$&+8.86\%   & \textbf{-15.70\%}  \\
    max pooling      & $d$ &+6.38\%   & -14.15\% \\
    \textbf{first K cols + max pool} & $(K+1)d$ &\textbf{+9.41\%}  & -14.86\% \\
    all cols + max pool   & $(|S|+1)d$ &+8.67\%  & -12.64\% \\
  \bottomrule
\end{tabular}%
% \begin{tablenotes}\footnotesize
% \item[1] $d$ is short for $d_{hidden}$
% % \item[2] TransAct approach
% \end{tablenotes}
}
\end{table}

% \begin{table}
% \begin{threeparttable}
%   \caption{Offline evaluation of different transformer output compression (K=10). All p-values are less than 0.05. }
%   \label{tab:tfmr_output}
%   \begin{tabular}{crr}
%     \toprule
%     Output Compression &  HIT@3/repin &  HIT@3/hide   \\
%     \midrule
%     % no user sequence (baseline)      &  -      & -\\
%         a random  column    & +6.80\%  & -10.96\%   \\
%         first  column       &+7.82\%     & -11.28\%  \\
%     random K  columns     &   +7.42\%     & -12.12\%       \\
%     first K columns    & +9.38\%   & -14.33\% \\
%     all columns    & +8.86\%   & \textbf{-15.70\%}  \\
%     max pooling      & +6.38\%   & -14.15\% \\
%         first K columns + max pooling\tnote{*} & \textbf{+9.41\%}  & -14.86\% \\
%     all columns + max pooling    & +8.67\%  & -12.64\% \\
%   \bottomrule
% \end{tabular}
% \begin{tablenotes}\footnotesize
% \item[*] TransAct approach
% \end{tablenotes}
% \end{threeparttable}
% \end{table}



\subsection{Online Experiment}
% Most of the time, offline experiment results can accurately predict online A/B experiment performance. However, there are some aspects that offline experiments are unable to cover. For example, one advantage of online experiments in recommendation tasks is that they can be run on live user data, allowing the model to be tested in a more realistic and dynamic environment. This can provide a more accurate representation of the model's performance in a production setting. In this section, we presents the online results.
Compared with offline evaluation, one advantage of online experiments in recommendation tasks is that they can be run on live user data, allowing the model to be tested in a more realistic and dynamic environment. 
For the online experiment, we serve the ranking model trained on the 2-week offline training dataset. We set the control group to be the Pinnability model without any realtime user sequence features. The treatment group is Pinnability model with TransAct. Each experiment group serves 1.5\% of the total users who visit Homefeed page.



\subsubsection{metrics}
On Homefeed, one of the most important metrics is \textbf{Homefeed repin volume}. Repin is the strongest indicator that users find the recommended pins relevant, and is usually positively correlated to the amount of time users spend on Pinterest. Empirically, we found that offline HIT@3/repin usually aligns very well with Homefeed online repin volume. Another important metric is \textbf{Homefeed hide volume}, which measures the proportion of recommended items that users choose to hide or remove from their recommendations. High hide rates indicate that the system is recommending items that users do not find relevant, which can lead to a poor user experience. Conversely, low hide rates indicate that the system is recommending items that users find relevant and engaging, which can lead to a better user experience. 





\subsubsection{Online engagement}

We observe significant online metric improvement with TransAct introduced to ranking. Figure~\ref{tab:online_metric} shows that we improved the Homefeed repin volume by 11\%.  It’s worth noting that engagement gains for non-core users are higher because they do not have a well-established user action history. And realtime features can capture their interest in a short time. Using TransAct, the Homefeed page is able to respond quickly and adjust the ranking results timely. We see hide volume dropped and that the overall time spent on Pinterest is increased.
\begin{table}
  \caption{Online evaluation of TransAct }
  \label{tab:online_metric}
  \begin{tabular}{crr}
    \toprule
    Online Metrics & All Users & Non-core Users\\
    \midrule
    Homefeed Repin Volume & +11.0\% & +17.0\%\\
    Homefeed Hide Volume & -10.0\% & -10.5\%\\
    Overall Time Spent & +2.0\% & +1.5\%\\
    % LSTM & -2.98\% & -0.75\%\\
    % Vanilla Transformer & \textbf{-8.45\%} & \textbf{+1.56\%}\\
    % Dedicated Transformer & \textbf{-13.49}\% & \textbf{+8.87\%}\\
  \bottomrule
\end{tabular}
\end{table}

\subsubsection{Model retrain}
\label{sec:retrain_exp}
One challenge observed in the TransAct group was the decay of engagement metrics over time for a given user.
As shown in Figure~\ref{fig:retrain1}, we compare the Homefeed repin volume gain of TransAct to the baseline, with both groups either fixed or retrained.
We observed that if TransAct was not retrained, despite having a significantly higher engagement on the first day of the experiment, it gradually decreased to a lower level over the course of two weeks. 
However, when TransAct was retrained on fresh data, there was a noticeable increase in engagement compared to not retraining the model. 
This suggests that TransAct, which utilizes realtime features, is highly sensitive to changes in user behavior and requires frequent retraining. 
% On the other hand, the baseline model is less affected by these changes and is able to maintain a consistent engagement volume without the need for retraining.
% To test this hypothesis, we retrained both the control group model (which does not use realtime user action features) and the treatment group model (TransAct) simultaneously and compared the impact of retraining on both models. We observed that the treatment model displayed a much greater improvement after retraining in comparison to the control model. 
Therefore, it is desired to have a high retrain frequency when using TransAct. In our production, we set the retrain frequency to twice a week and this retrain frequency has been proven to keep the engagement rate stable.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/retrain.png}
  \caption{Effect of retraining on TransAct}
  \label{fig:retrain1}
\end{figure}





\subsubsection{Random time window masking}

% https://w.pinadmin.com/display/CONTENT/Interest+Taxonomy#InterestTaxonomy-Whatisit?
Another challenge observed was dropping diversity in recommendations. Diversity measures the broadness and variety of the items being recommended to a user. Previous literature\cite{gg_surrogate} finds diversity is associated with increasing user visiting frequency. However, diversity is not always desirable as it can lead to a drop in relevance. Therefore, it is crucial to find the right balance between relevance and diversity in recommendations.


At Pinterest, we have a 28k-node hierarchical interest taxonomy~\cite{interest_blog} that classifies all the pins. The top-level interests are coarse. Some examples of top-level interests are art, beauty, and sport. Here, we measure the \textbf{impression diversity} as the summation of the number of unique top-level interests viewed per user. We observe that with TransAct introduced to Homefeed ranking, the impression diversity dropped by 2\% to 3\%. The interpretation is that by adding the user action sequence feature, the ranking model learns to optimize for the user’s short-term interest. And by focusing on mainly short-term interest, the diversity of the recommendation dropped. 

We mitigate the diversity drop by using a random time window mask in the transformer as mentioned in Section~\ref{sec: seq_model}. This random masking encourages the model to focus on content other than only the most recent items a user engaged with. With this design, the diversity metric drop was brought back to only -1\% without influencing relevance metrics like repin volume. 
We also tried using a higher dropout rate in the transformer encoder layer and randomly masking out a fixed percentage of actions in the user action sequence input. However, neither of these methods yielded better results than using random time window masking. They increased the diversity at the cost of engagement drop. 
% chatgpt output, adjusted by nikil
% In order to address the issue of diversity drop, we employ a random time window mask in the transformer, as described in Section~\ref{sec: seq_model}. This random masking strategy encourages the model to focus on a diverse time range of actions, rather than just the most recent items a user interacted with. Online results showed this approach decreased the diversity drop to only -1%, without negatively impacting relevance metrics like repin volume. We also explored alternative methods, such as increasing the dropout rate in the transformer encoder layer and randomly masking a fixed percentage of actions in the user action sequence input. However, these methods failed to produce better results than the random time window masking, as they caused a decrease in engagement while improving diversity.





