\section{Methodology}
\label{sec:method}
% In the ranking stage, we model the recommendation task as a multi-label classification problem, which can be defined as
% follows: given a user’s behavior sequence S(u) = {v1,v2, ...,vn }
% clicked by a user u, we need to learn a function, F, to predict
% the probability of u clicking the target item vt
% , i.e., the candidate
% one. Other Features include user profile, context, item, and cross
% features.


In this section, we introduce TransAct, our realtime-batch hybrid ranking model. 
% TransAct is a transformer-based architecture to process realtime user action sequence. 
We will start with an overview of the Pinterest Homefeed ranking model, Pinnability. We then describe how to use TrancAct to encode the realtime user action sequence features in Pinnability for the ranking task.

\subsection{Preliminary: Homefeed Ranking Model}
\label{sec:pinnability}
In Homefeed ranking, we model the recommendation task as a pointwise multi-task prediction problem, which can be defined as follows: given a user $u$ and a pin $p$, we build a function to predict the probabilities of user $u$ performing different actions 
% $\vH = \{h_1, h_2, \dots , h_k\}$ 
on the candidate pin $p$. The set of different actions contains both positive and negative actions, e.g. click, repin\footnote{A "repin" on Pinterest refers to the action of saving an existing pin to another board by a user.} and hide.

We build \textit{Pinnability}, Pinterest's Homefeed ranking model, to approach the above problem.
The high-level architecture is a Wide and Deep learning (WDL) model~\cite{cheng2016wide}.  
% Pinnability consumes a $\{u, p\}$ pair and predicts the probability $u$ will take a specified action on $p$.
The Pinnability model utilizes various types of input signals, such as user signals, pin signals, and context signals.
These inputs can come in different formats, including categorical, numerical, and embedding features.

We use embedding layers to project categorical features to dense features, and perform batch normalization on numerical features. We then apply a feature cross using a full-rank DCN V2~\cite{DCNv2} to explicitly model feature interactions.
At last, we use fully connected layers with a set of output action heads $\vH = \{h_1, h_2, \dots , h_k\}$ to predict the user actions on the candidate pin $p$. Each head maps to one action.
As shown in Figure~\ref{fig:pinnability}, our model is a realtime-batch hybrid model that encodes the user action history features by both realtime (TransAct) and batch (PinnerFormer) approaches and optimizes for the ranking task~\cite{10.1145/3523227.3547394}. 



% Note that the actions in label $\vy$ is a super set of actions in the heads, i.e. $\vH \in \vA$ .


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/high_level_v2.pdf}
  \caption{Pinterest Homefeed ranking model (Pinnability)}
  \label{fig:pinnability}
  \Description{Pinterest Homefeed ranking model (Pinnability)}
\end{figure}

Each training sample is $(\vx, \vy)$, where $\vx$ represents a set of features, and $\vy \in \{0,1\} ^{|\vH|}$. Each entry in $\vy$ corresponds to the label of an action head in $\vH$.
The loss function of Pinnability is a weighted cross-entropy loss, designed to optimize for multi-label classification tasks.
We formulate the loss function as:

\begin{equation}
\label{eq:loss}
\mathcal{L} = w_u \sum_{h \in H } \left\{ - w_h\left[y_h \log f(\vx)_h + (1-y_h)(1 - \log f(\vx)_h) \right] \right\}
\end{equation}
where $f(\vx)\in (0, 1)^H$, and $f(\vx)_h$ is the output probability of head $h$. $y_h \in \{0,1\}$ is the ground truth on head $h$.

% Given a $\{u, p\}$ pair, for each action head \textbf{h}, the model outputs a probability $f(\vx)_h$.
A weight $w_h$ is applied on the cross entropy of each head's output $f(\vx)_h$.
$w_h$ is calculated using the ground truth $\vy$ and a label weight matrix  $\mM \in \mathbb{R} ^{|H| * |H|}$ as follows:


\begin{equation}
w_h = \sum_{a \in H }\mM_{h,a} \times y_a
\end{equation}



The label weight matrix $\mM$ acts as a controlling factor for the contribution of each action to the loss term of each head\footnote{For more details, see Appendix ~\ref{appendix:head_weights}}.
Note that if $\mM$ is a diagonal matrix, Eq~\eqref{eq:loss} reduces to a standard multi-head binary cross entropy loss.
But selecting empirically determined label weights $\mM$ improves performance considerably.
% This manually tuned label weight matrix improves performance considerably.


In addition, each training example is weighted by a user-dependent weight $w_u$, which is determined by user attributes, such as the user state\footnote{User states are used to group users of different behavior patterns, for example, users who engage daily are in one group, while those who engage once a month have a different user state}, gender and location.
% We compute $w_u$ by multiplying user state weight $w_{\text{state}}$, user gender weight $w_{\text{gender}}$, and user location weight $w_{\text{location}}$, with these weights adjusted based on specific business needs.
% \begin{equation}
%   w_u = w_{\text{state}} \times w_{\text{location}} \times w_{\text{gender}}
% \end{equation}
We compute $w_u$ by multiplying user state weight, user gender weight, and user location weight: $w_u = w_{\text{state}} \times w_{\text{location}} \times w_{\text{gender}}$. These weights are adjusted based on specific business needs.


% where $f(\vx)\in (0, 1)^H$, and $f(\vx)_h$ the probability of user $u$ performing action $h$ on the candidate pin $p$.  $y_h \in \{0,1\}$, is the label denoting usesr $u$ performing action $h$ or not.
% A user's weight $w_u$ is determined by certain user attributes, such as the user state, gender and location. User state is used to represent users' visiting pattern, e.g. new user, casual user, or core user.
% We compute $w_u$ as the product of user state weight $w_{\text{state}}$, user gender weight $w_{\text{gender}}$,  and user location weight $w_{\text{location}}$.
% All of the above weights are hand-tuned according to experiment results and business needs.






\subsection{Realtime User Action Sequence Features}

User's past action history is naturally a variable length feature -- different users have different amounts of past actions on the platform. 
% Figure~\ref{fig:action_dist} illustrates the long tail distribution of user actions on Pinterest over the course of a year. The analysis focuses on important actions, such as clicks, hides, and repins. While a small number of users have more than 10,000 actions in a year, the median number of actions is around 90.
% \begin{figure}[h]
%   \centering
  
%   \includegraphics[width=0.8\linewidth]{figures/user_action_dist.pdf}
%   \caption{The distribution of the number of user actions in a year }
%   \label{fig:action_dist}
%   \Description{The distribution of the number of user actions in a year}
% \end{figure}
Although a longer user action sequence usually means more accurate user interest representation, in practice, it is infeasible to include all user actions. Because the time needed to fetch user action features and perform ranking model inference can also grow substantially, which in turn hurts user experience and system efficiency. Considering infrastructure cost and latency requirements, we choose to include each user's most recent 100 actions in the sequence. For users with less than 100 actions, we pad the feature to the length of 100 with 0s. The user action sequence features are sorted by timestamp in descending order, i.e. the first entry being the most recent action.
% reference form PS dataset: https://docs.google.com/document/d/1TmU1ZyFpjhmYInhKnev2s41J79MkaOg_0CBlf1ao43c/edit






All actions in the user action sequence are pin-level actions. For each action, we use three primary features: the timestamp of the action, action type, and the 32-dimensional PinSage embedding \cite{PinSage} of the pin. 
% \td{any reference to PS's low dim loss design? answer: https://arxiv.org/abs/2205.13147}
PinSage is a compact embedding that encodes a pin's content information. 
% One advantage of using PinSage is that the dimension can be easily reduced from 256 to 16, 32, or 64 dimensions because it is learned by a process similar to ~\cite{kusupati2022matryoshka}.


\subsection{Our Approach: TransAct}
% Given a user’s realtime action sequence $\vS(u) = [a_1,a_2, ...,a_n]$, by utilizing Pinnability as the high-level ranking framework, we aim to increase the accuracy of multi-task action prediction task on  $\{u, p\}$ pair, as stated in section \ref{sec:pinnability}. 

Unlike static features, the realtime user action sequence feature $\vS(u) = [a_1,a_2, ...,a_n]$ is handled using a specialized sub-module called TransAct. TransAct extracts sequential patterns from the user's historical behavior and predicts $(u, p)$ relevance scores.

\subsubsection{Feature encoding}
The relevance of pins that a user has engaged with can be determined by the types of actions taken on them in the user's action history. For example, a pin repinned to a user's board is typically considered more relevant than one that the user only viewed. If a pin is hidden by the user, the relevance should be very low. To incorporate this important information, we use trainable embedding tables to project action types to low-dimensional vectors. The user action type sequence is then projected to a user action embedding matrix $\mW_{actions} \in \mathbb{R} ^{|S| \times d_{action}}$, where $d_{action}$ is the dimension of action type embedding.

As mentioned earlier, the content of pins in the user action sequence is represented by PinSage embeddings \cite{PinSage}. Therefore, the content of all pins in the user action sequence is a matrix $\mW_{pins} \in \mathbb{R} ^{|S| \times d_{PinSage}}$. 
% We do not use the timestamp features as a direct input to the transformer layers in TransAct. 
% We construct the final encoded user action sequence feature by concatenating $\mW_{actions}$ and $\mW_{pins}$ into a single matrix as the representation of realtime user action sequence. The final size of user action sequence content is \texttt{CONCAT}$(\mW_{actions},\mW_{pins}) \in \mathbb{R} ^{|S| \times (d_{PinSage}+d_{action})}$.
% By concatenating $\mW_{actions}$ and $\mW_{pins}$, w
The final encoded user action sequence feature is \texttt{CONCAT}$(\mW_{actions},\mW_{pins}) \in \mathbb{R} ^{|S| \times (d_{PinSage}+d_{action})}$.

\subsubsection{Early fusion}
\label{sec:earlyfusion}
One of the unique advantages of using  user action sequence features directly in the ranking model is that we can explicitly model the interactions between the candidate pin and the user's engaged pins. 
Early fusion in recommendation tasks refers to merging user and item features at an early stage of the recommendation model. 
% For the candidate pin, we have the PinSage embedding in the same hidden space $\mathbb{R}^{d_{PinSage}}$ as engaged pin embeddings. 
Through experiments, we find that early fusion is an important factor to improve ranking performance. Two early fusion methods are evaluated:
\begin{itemize}
% \item adding candidate pin embedding to each entry of the user action sequence
\item \texttt{append}: Append candidate pin's PinSage embedding to user action sequence as the last entry of the sequence, similar to BST~\cite{alibaba_seq_tfmr}. Use a zero vector to serve as a dummy action type for candidate pin.
\item \texttt{concat}: For each action in the user action sequence, concatenate the candidate pin's PinSage embedding with user action features.
\end{itemize}
We choose  \texttt{concat}  as our early fusion method based on the offline experiment results. The resulting sequence feature with early fusion is a 2-d matrix $\mU \in \mathbb{R} ^{|S| \times d}$, where $d = (d_{action} + 2d_{PinSage})$



\subsubsection{Sequence Aggregation Model}
\label{sec: seq_model}
With the user action sequence feature $\mU$ prepared, the next challenge is to efficiently aggregate all the information in the user action sequence to represent the user's short-term preference. 
Some popular model architectures for sequential modeling in the industry include CNN\cite{cnn}, RNN~\cite{rnn} and recently transformer~\cite{tfmr}, etc. 
We experimented with different sequence aggregation architectures and choose transformer-based architectures. We employed the standard transformer encoder with 2 encoder layers and one head. The hidden dimension of feed forward network is denoted as $d_{hidden}$. Positional encoding is not used here because our offline experiment showed that position information is ineffective\footnote{For more details about positional encoding, see Appendix ~\ref{appendix:pe}.}.


\subsubsection{Random Time Window Mask}
Training on all recent actions of a user can lead to a rabbit hole effect, where the model recommends content similar to the user's recent engagements.
This hurts the diversity of users' Homefeeds, which is harmful to long-term user retention.
To address this issue, we use the timestamps of the user action sequence to build a time window mask for the transformer encoder. 
This mask filters out certain positions in the input sequence before the self-attention mechanism is applied. 
In each forward pass, a random time window $T$ is sampled uniformly from 0 to 24 hours. All actions taken within $(t_{request} - T, t_{request})$ are masked, where $t_{request}$ stands for the timestamp of receiving the ranking request. It is important to note that the random time window mask is only applied during training, while at inference time, the mask is not used. 
% \nikil{at training or inference time or both? Answer: only training}
% The reason is that we found that using all the past 100 actions can lead to a rabbit hole effect where users keep getting similar content as their recent engagement, keep engaging with similar content and are unable to see diversified recommendation. Dropped diversity is not desired for long term user retention.

\subsubsection{Transformer Output Compression}
% We feed the encoded user action sequence feature $\mU$ to the transformer encoder with random time window mask. 
The output of the transformer encoder is a matrix $\mO = (\vo_0:\vo_{|S|-1})\in \mathbb{R} ^ {|S| \times d}$. We only take the first $K$ columns $(\vo_0:\vo_{K-1})$, concatenated them with the max pooling vector \texttt{MAXPOOL}$(\mO) \in \mathbb{R}^{d}$, and flattened it to a vector $\vz \in \mathbb{R} ^{(K + 1) * d}$. The first $K$ output columns capture  users’ most recent interests and \texttt{MAXPOOL}$(\mO)$ represents users' longer-term preference over $S(u)$. Since the output is compact enough, it can be easily integrated into the Pinnability framework using the DCN v2~\cite{DCNv2}  feature crossing layer.




\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/transAct_v3.pdf}
  \caption{TransAct architecture. Note that this is a submodule that can be plugged into any similar architecture like Pinnability}
  \label{fig:seq_encoder}
  \Description{TransAct model architecture}
\end{figure}


\subsection{Model Productionization}

\subsubsection{Model Retraining}
Retraining is important for recommender systems because it allows the system to continuously adapt to changing user behavior and preferences over time. Without retraining, a recommender system's performance can degrade as the user's behavior and preferences change, leading to less accurate recommendations~\cite{tech_debt}. 
This holds especially true when we use realtime features in ranking. The model is more time sensitive and requires frequent retraining. 
Otherwise, the model can become stale in a matter of days, leading to less accurate predictions. 
We retrain Pinnability from scratch twice per week. We find that this retraining frequency is essential to ensure a consistent engagement rate and still maintain a manageable training cost.
We will dive into the importance of retraining in Section~\ref{sec:retrain_exp}.


\subsubsection{GPU serving} Pinnability with TransAct is 65 times more computationally complex compared to its predecessors in terms of floating point operations. Without any breakthroughs in model inference, our model serving cost and latency would increase by the same scale. GPU model inference allows us to serve Pinnability with TransAct at neutral latency and cost\footnote{For more details about model effiency, see Appendix ~\ref{appendix:efficiency}.}. 

The main challenge to serve Pinnability on GPUs is the CUDA kernel launch overhead. The CPU cost of launching operations on the GPU is very high, but it is often overshadowed by the prolonged GPU computation time. However, this is problematic for Pinnability GPU model serving in two ways. First, Pinnability and recommender models in general process hundreds of features, which means that there is a large number of CUDA kernels. Second, the batch size during online serving is small and hence each CUDA kernel requires little computation. With a large number of small CUDA kernels, the launching overhead is much more expensive than the actual computation. We solved the technical challenge through the following optimizations:

\textbf{Fuse CUDA kernels.} An effective approach is to fuse operations as much as possible. We leverage standard deep learning compilers such as nvFuser\footnote{\url{https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/}} but often found human intervention is needed for many of the remaining operations. One example is our embedding table lookup module, which consists of two computation steps: raw id to table index lookup and table index to embedding lookup. This is repeated hundreds of times due to the large number of features. We significantly reduce the number of operations by leveraging cuCollections\footnote{\url{https://github.com/NVIDIA/cuCollections}} to support hash tables for the raw ids on GPUs and implementing a custom consolidated embedding lookup module to merge the lookup for multiple features into one lookup. As a result, we reduced hundreds of operations related to sparse features into one.

% https://docs.google.com/document/d/1MLY1oYCo-PyFP-vWCYX3yH688G1K0NIWMjTUM37FR9I/edit 
\textbf{Combine memory copies.} For every inference, hundreds of features are copied from the CPU to the GPU memory as individual tensors. The overhead of scheduling hundreds of tensor copies becomes the bottleneck. To decrease the number of tensor copy operations, we combine multiple tensors into one continuous buffer before transferring them from CPU to GPU. This approach reduces the scheduling overhead of transferring hundreds of tensors individually to transferring one tensor. 

\textbf{Form larger batches.} For CPU-based inference, smaller batches are preferred to increase parallelism and reduce latency. However, for GPU-based inference, larger batches are more efficient~\cite{sze2017efficient}. This led us to re-evaluate our distributed system setup. Initially, we used a scatter-gather architecture to split requests into small batches and run them in parallel on multiple leaf nodes for better latency. However, this setup did not work well with GPU-based inference. Instead, we use the larger batches in the original requests directly. To compensate for the loss of cache capacity, we implemented a hybrid cache that uses both DRAM and SSD.

\textbf{Utilize CUDA graphs.} We relied on CUDA Graphs\footnote{\url{https://developer.nvidia.com/blog/cuda-graphs/}} to completely eliminate the remaining small operations overhead. CUDA Graphs capture the model inference process as a static graph of operations instead of individually scheduled ones, allowing the computation to be executed as a single unit without any kernel launching overheads. 

\subsubsection{Realtime Feature Processing}

When a user takes an action, a realtime feature processing application based on Flink\footnote{https://flink.apache.org/}
% Apache Flink is Pinterest's choice of real time data processing frameworks. It consume streams(e.g. Kafka) and produce data into streams, databases.}
 consumes user action Kafka\footnote{https://kafka.apache.org/}
 % \footnote{Apache Kafka is Pinterest's choice of distributed streaming system used for stream processing, real-time data pipelines, and data integration at scale.} 
 streams generated from front-end events. It validates each action record, detects and combines duplicates, and manages any time discrepancies from multiple data sources. The application then materializes the features and stores them in Rockstore~\cite{rockstore_blog}. At serving time, each Homefeed logging/serving request triggers the processor to convert sequence features into a format that can be utilized by the model.
 