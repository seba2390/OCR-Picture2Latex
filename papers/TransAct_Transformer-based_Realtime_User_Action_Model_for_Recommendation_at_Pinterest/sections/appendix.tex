


\section{Head Weighting}\label{appendix:head_weights}

We illustrate how head weighting helps the multi-task prediction task here. Consider the following example, of a model using 3 actions: repins, clicks, and hides. The label weight matrix is set as Table~\ref{tab:labelweight}.

\begin{table}[!ht]
  \caption{An example of label weight matrix $\mM$ with 3 actions}
  \label{tab:labelweight}
  \begin{tabular}{|c|ccc|}
  \hline
    \diagbox{Head}{Action} &click & repin  & hide\\
    \hline
    click & 100 & 0 & 100\\
    % \midrule
    repin & 0 & 100 & 100 \\
    hide & 1 & 5 & 10\\
    % Throughput & -76\% & +23\%\\
  \hline
\end{tabular}
\end{table}



Hides are a strong negative action, while repins and clicks are both positive engagements, although repins are considered a stronger positive signal than clicks.
We set the value of $\mM$ manually, to control the weight on cross-entropy loss. Here, we give some examples of how this is achieved.
% Consider the head predicting the likelihood of a user hiding a pin.
\begin{itemize}
% nikil's suggestions of potential rewording
% \item Assume a user only hides a pin, and does not repin or click ($\vy_{hide}=1$, $\vy_{repin}=\vy_{click}=0$). This hide provides a strong signal that the pin is one that user would never repin, but provides a weaker signal that they would never click the pin (clicks sometimes can lead to hides, if the clicked pin is low quality). This yields the following weights: $w_{hide}=\mM_{hide, hide} = 100$, $w_{repin}=\mM_{repin, hide} = 100$, $w_{click}=\mM_{click, hide} = 20$
% \item Assume a user repins a pin, but does not hide or click ($\vy_{repin}=1$, $\vy_{hide}=\vy_{click}=0$). We then know the pin is one a user is unlikely to hide (resulting in $w_{hide}=\mM_{hide, repin}=10$), but because both repins and clicks are positive signals, we don't penalize the model for predicting a click ($w_{click}=\mM_{click, click}\cdot \vy_{click} + \mM_{click, repin}\cdot \vy_{repin} + \mM_{click, hide}\cdot \vy_{hide} = 100 \cdot 0 + 0 \cdot 1 + 20 \cdot 0 = 0$)

\item If a user only hides a pin ($\vy_{hide}=1$), and does not repin or click ($\vy_{repin}=\vy_{click}=0$). Then we want to penalize the model more if it predicts repin or click, by setting the $\mM_{repin, hide}$ and $\mM_{click, hide}$ to a large value. 
% In this case, we could set $\mM_{hide, repin}$ to a medium number (say 5), $\mM_{hide, click}$ to a smaller number (say 1), and $\mM_{repin, hide}$ to a large number (say 10).

% $w_{hide}=\mM_{hide, hide}=10$.

$w_{repin}=\mM_{repin, hide} * \vy_{hide} =100$.

$w_{click}=\mM_{click, hide}  * \vy_{hide}  =100$.
% \begin{equation*}
% w_{hide}=\mM_{hide, hide}=10
% \end{equation*}


\item If a user only repins a pin ($\vy_{repin}=1$), but does not hide or click ($\vy_{hide}=\vy_{click}=0$). We want to penalize the model if it predicts hide: $w_{hide} = \mM_{hide,repin} * \vy_{repin} = 5$. 

But we do not need to penalize the model if it predicts a click, because a user could repin and click the same pin.
$w_{click} =  \mM_{click, repin} * \vy_{repin} = 0$
 \end{itemize}

% In this case, we could set $\mM_{hide, repin}$ to a medium number (say 5), $\mM_{hide, click}$ to a positive but smaller number (say 1), and $\mM_{hide, hide}$ to a large number (say 10).
% Then, the behavior can be illustrated in the following examples:
% \begin{itemize}
%     \item The user hid this Pin (and did not repin or click) $\vy_{hide}=1$, $w_{hide}=\mM_{hide, hide}=10$, , $w_{repin}=\mM_{hide, repin}=10$
%     \item The user repinned this Pin, but did not click or hide. $\vy_{hide}=0$, $w_{hide}=\mM_{hide, repin}=5$
%     \item The user repinned and clicked this Pin, but did not hide. $\vy_{hide}=0$, $w_{hide}=\mM_{hide, repin} + \mM_{hide, click}=5+1=6$
%     \item The user repinned, clicked, and hid this Pin $\vy_{hide}=1$, $w_{hide}=\mM_{hide, repin} + \mM_{hide, click} + \mM_{hide, hide}=5+1 + 10=16$ \nikil{is this correct}
% \end{itemize}




% \section{GPU serving}
% \begin{table}[!ht]
%   \caption{\td{explain each columns more} GPU serving improvement}
%   \label{tab:gpu_serving}
%   \begin{tabular}{crr}
%     \toprule
%     & Out-of-the-box GPU & Optimized GPU\\
%     \midrule
%     Model Size & 30x & 70x \\
%     Latency & +332\% & -29\%\\
%     Throughput & -76\% & +23\%\\
%   \bottomrule
% \end{tabular}
% \end{table}


% \section{Retrain}
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/retrain2.png}
%   \caption{\td{update?} Retrain Comparison Between Control and Treatment}
%   \label{fig:retrain2}
%   \Description{Retrain Comparison Between Control and Treatment}
% \end{figure}


\section{Positional Encoding}
We tried several positional encoding approaches: learning positional embedding from scratch, sinusoidal positional encoding~\cite{tfmr}, and linear projection positional encoding as proposed in~\cite{alibaba_seq_tfmr}.
Table\ref{tab:pe} shows that positional encoding does not add much value.
\label{appendix:pe}
\begin{table}[!ht]
  \caption{Offline evaluation of different positional encoding methods compared with TransAct}
  \label{tab:pe}
  \begin{tabular}{crr}
    \toprule
    Positional encoding method &  HIT@3/hide &  HIT@3/repin  \\
    \midrule
    None (TransAct)      &  -      & -\\
    From scratch & +0.86\%   & -0.61\%\\
    Sinusoidal  & +0.78\%     &-0.13\%\\
    Linear projection $^*$  & +2.29\%       &   +0.19\%  \\
  \bottomrule
\end{tabular}
\end{table}

\section{Model Efficiency}\label{appendix:efficiency}

Table~\ref{tab:efficiency} shows more detailed information on the efficiency of our model, including number of flops, model forward latency per batch (batch size = 256), and serving cost. The serving cost is not linearly correlated with model forward latency because it is also related to server configurations such as time out limit, batch size, etc. GPU serving optimization is important to maintain low latency and serving cost. 
\begin{table}[!ht]
  \caption{Model Efficiency Numbers from Serving Optimization}
  \label{tab:efficiency}
  \begin{tabular}{cccc}
    \toprule
     &  Baseline(CPU) &  TransAct(CPU) & TransAct(GPU)  \\
    \midrule
    Parameters      & 60M  & 92M & 92M\\
    flops & 1M   & 77M & 77M\\
    Latency  & 22ms     & 712ms & 8ms\\
    Serving Cost  & 1x       &   32x & 1x  \\
  \bottomrule
\end{tabular}
\end{table}
