\section{Related Work}
\label{sec:related_work}

% In this section, we summarize the development path of recommender systems. 
% We then dive deeper into sequential recommendation methods and discuss challenges and limitations of existing models.

\subsection{Recommender System}
Collaborative filtering (CF)~\cite{sarwar2001item, CF, CF2} makes recommendations based on the assumption that a user will prefer an item that other similar users prefer. 
It uses the user behavior history to compute the similarity between users and items and recommend items based on similarity. 
This approach suffers from the sparsity of the user-item matrix and cannot handle users who have never interacted with any items. 
Factorization machines~\cite{FM, rendle2010factorization}, on the other hand, are able to handle sparse matrices.

More recently, deep learning (DL) has been used in click-through rate (CTR) prediction tasks. 
For example, Google uses Wide \& Deep~\cite{cheng2016wide} models for application recommendation. 
The wide component achieves memorization by capturing the interaction between features, while the deep component helps with generalization by learning the embedding of categorical features using a feed forward network.
DeepFM~\cite{guo2017deepfm} makes improvements by learning both low-order and high-order feature interactions automatically. 
DCN~\cite{dcn} and its upgraded version DCN v2~\cite{DCNv2} both aim to automatically model the explicit feature crosses.
The aforementioned recommender systems do not work well in capturing the short-term interests of users since only the static features of users are utilized. 
These methods also tend to ignore the sequential relationship within the action history of a user, resulting in an inadequate representation of user preferences. 

%% nikil's proposed (approximate) rewrite of above paragraph
% More recently, click-through rate (CTR) tasks have been tackled through deep learning. 
% One key problem in this task is the question of computing feature interactions; traditional feedforward neural networks struggle to represent simple interactions between categorical and continuous features. % need citation probably
% To address this issue, several architectures have been proposed.
% One earlier example is the Wide \& Deep~\cite{cheng2016wide} architecture, which factorizes CTR prediction into a linear model based on categorical interaction features, and a deep model based on other categorical and continuous features.
% Because explicit feature interactions require extra manual feature engineering and/or infrastructure, several other methods that implicitly learn feature interactions have since been proposed, including DeepFM~\cite{guo2017deepfm}, DCN~\cite{dcn}, and its upgraded version, DCN v2~\cite{DCNv2}.
% The aforementioned architectures are effective when handling many features, but they do not have inductive biases that are often important when modeling sequnces; there would be no shared processing of separate sequence elements if each element was considered a feature in the model


\subsection{Sequential Recommendation}
% \nikil{address the difference between a pure sequential recommendation problem and our problem; reviewers will otherwise wonder why we're not comparing to things like bert4rec}
To address this problem, sequential recommendation has been widely studied in both academia and the industry. 
A sequential recommendation system uses a behavior history of users as input and applies recommendation algorithms to suggest appropriate items to users.
Sequential recommendation models are able to capture users' long-term preferences over an extended period of time, similar to traditional recommendation methods. Additionally, they also have the added benefit of being able to account for users' evolving interests, which enables higher quality recommendations.
% A user's current intent can be inferred from a small set of recent interactions. 


Sequential recommendation is often viewed as a next item prediction task, where the goal is to predict a user's next action based on their past action sequence. 
% In this work, we do not formulate the ranking problem the same way. 
We are inspired by the previous sequential recommendation method \cite{alibaba_seq_tfmr} in terms of encoding users' past action into a dense representation. 
Some early sequential recommendation systems use machine learning techniques, such as Markov Chain~\cite{he2016fusingMC} and session-based K nearest neighbors (KNN)~\cite{hu2020modelingknn} to model the temporal dependencies among interactions in users' action history. 
These models are criticized for not being able to fully capture the long-term patterns of users by simply combining information from different sessions. 
Recently, deep learning techniques such as recurrent neural networks (RNN)~\cite{rnn} have shown great success in natural language processing and have become increasingly popular in sequential recommendation. 
As a result, many DL-based sequential models~\cite{donkers2017sequential, hidasi2015session, tan2016improved, zhou2019deep} have achieved outstanding performance using RNNs. 
Convolutional neural networks (CNNs)~\cite{cnn} are widely used for processing time-series data and image data. 
% Their architectures typically contain convolutional layers, pooling layers, and feed-forward layers. 
% CNNs are effective at capturing the dependency relationship across local information, such as the correlation between pixels in a certain part of an image or the dependencies between several adjacent words in a sentence. 
% \nikil{probably don't need this much detail on CNNs; can cut if we need space} 
In the context of sequential recommendation, CNN-based models can effectively learn dependency within a set of items users recently interacted with, and make recommendations accordingly~\cite{tang2018personalized, tuan20173d}.
Attention mechanism is originated from the neural machine translation task, which models the importance of different parts of the input sentences on the output words~\cite{bahdanau2014neural}.
Self-attention is a mechanism known to weigh the importance of different parts of an input sequence~\cite{tfmr}. 
There have been more recommender systems that use attention~\cite{DIN} and self-attention~\cite{zhang2019next, alibaba_seq_tfmr, li2020time, SASRec, sun2019bert4rec}. 
% \td{https://arxiv.org/pdf/1905.01997.pdf}


% Many previous works, such as SASRec~\cite{SASRec}, Ti-SASRec~\cite{li2020time}, and BERT4Rec~\cite{sun2019bert4rec}, 
Many previous works~\cite{SASRec,li2020time,sun2019bert4rec}
only perform offline evaluations using public datasets. However, the online environment is more challenging and unpredictable. Our method is not directly comparable to these works due to differences in the problem formulation. Our approach resembles a Click-through Rate (CTR) prediction task.
Deep Interest Network (DIN) uses an attention mechanism to model the dependency within users' past actions in CTR prediction tasks. 
Alibaba's Behavior Sequence Transformer (BST)~\cite{alibaba_seq_tfmr} is the improved version of DIN and is closely related to our work.
They propose to use Transformer to capture the user interest from user actions, emphasizing the importance of the action order.
% information in the user action sequences to improve the accuracy of CTR prediction task.
% \td{add PE experiment table to appendix} 
However, we found that positional information does not add much value. We find other designs like better early fusion and action type embedding are effective when dealing with sequence features.
% In addition, BST work does not consider the practical challenges of deploying such a model in production that requires high throughput and low latency. 
% In this paper, we will address various challenges that arise in real-world recommender systems and provide solutions.




% Deep interest network \cite{DIN}
% Some model the recommendation problem as a sequence prediction task with self-attention mechanism, where the model’s input is users' past action sequence $(S_1, S_2, ..., S_{n-1})$ its expected output as a ‘shifted’ version of the same sequence $(S_2, S_3, ..., S_{n})$.


% BERT4Rec \cite{BERT4Rec}

