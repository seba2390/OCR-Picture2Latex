
\section{Preliminaries\label{sec:preliminaries}}

Let 
 $G=(V,E,w)$ be an undirected   graph with $n$ vertices,   $m$ edges, and weight function $V\times V\rightarrow \mathbb{R}_{\geq 0}$. 
The set of neighbors of a vertex $v$ is represented by $N(v)$,
and its degree is $d_v=\sum_{u\sim v} w(u,v)$. The maximum degree of $G$ is defined to be $\Delta(G)=\max_{v}\{d_v\}$. 
For any set $S\subseteq V$, let $\mu(S)\triangleq\sum_{v\in S} d_v$.
For any sets $S, T\subseteq V$, we define
$w(S,T)\triangleq \sum_{u\in S, v\in T} w(u,v)$ to be the total weight of edges crossing $S$ and $T$.
We define the conductance of any set $S$ by
\[
\phi(S)=\frac{w(S, V\setminus S)}{\mu(S)}.
\]
For two sets $X$ and $Y$, the symmetric difference of $X$ and $Y$ is defined
as $X\triangle Y\triangleq (X\setminus Y)\cup (Y\setminus X)$. 
%For any two graphs $G_1=(V, E_1,w_1)$ and $G_2=(V, E_2,w_2)$ defined on the same vertex set, we write $G_1\boxplus G_2= (V,E,w)$ where $E=E_1\cup E_2$ and the weight function $w$ is defined by $w(u,v)=w_1(u,v)+w_2(u,v)$ for any $u,v\in V$.


For any matrix $A\in\mathbb{R}^{n\times n }$, let $\lambda_1(A)\leq\cdots \leq \lambda_n(A)=\lambda_{\max}(A)$ be the eigenvalues of $A$. For any two matrices $A, B\in\mathbb{R}^{n\times n}$, we write $A\preceq B$ to represent $B-A$ is positive semi-definite~(\textsf{PSD}). Notice that this condition implies that $x^{\rot}Ax\leq x^{\rot}Bx$ for any $x\in\mathbb{R}^n$. Sometimes we also use a weaker notation
$(1-\varepsilon)A\preceq_r B\preceq_r (1+\varepsilon)A$ to indicate that 
\[
(1-\varepsilon)x^{\rot}Ax\leq x^{\rot}Bx\leq (1+\varepsilon)x^{\rot}Ax
\] for all $x$ in the row span of $A$.

%\medskip

\subsection{Graph Laplacian}  The Laplacian matrix of $G$ is an $n\times n$ matrix $L_G$ defined by 
$L_G=D_G-A_G$, where $A_G$ is the adjacency matrix of $G$ defined by $A_G(u,v)=w(u,v)$, and $D_G$ is the $n\times n$ diagonal matrix with $D_G(v,v)=d_v$ for any $v\in V[G]$.  Alternatively, we can write $L_G$ with respect to a \emph{signed edge-vertex incidence matrix}:  we assign every edge $e=\{u,v\}$ an arbitrary orientation, and let $B_G(e,v)=1$ if $v$ is $e$'s head, $B_G(e,v)=-1$ if $v$ is $e$'s tail, and $B_G(e,v)=0$ otherwise.
We further define a diagonal matrix $W_G\in\mathbb{R}^{m\times m}$, where $W_G(e,e)=w_e$ for any edge $e\in E[G]$. Then, we can write $L_G$ as $L_G=B_G^{\rot}W_GB_G$. The \emph{normalized Laplacian matrix} of $G$ is defined by
$
\calL_G\triangleq \mat{D}_G^{-1/2}\mat{L}_G\mat{D}_G^{-1/2} = \mat{I}-\mat{D}_G^{-1/2}\mat{A}_G\mat{D}_G^{-1/2}$.
We sometimes drop the subscript $G$ when the underlying graph is clear from the context. 

%By definition, $B_G$'s each row, expressed by $b_i$, corresponds to an edge of $G$, and we define the \emph{leverage score} of $b_i$ by  $ \tau_i=b_i^{\rot}L^{+}_G b_i$, where $L^+_G$ is the \emph{pseudo-inverse} of the Laplacian matrix of $G$.



%\medskip

\subsection{Spectral sparsification}   For any  undirected and weighted graph $G=(V,E,w)$, we say a subgraph $H$ of $G$ with proper reweighting of the edges is a $(1+\varepsilon)$-spectral sparsifier if
\begin{equation}\label{eq:ssproperty}
(1-\varepsilon)L_G  \preceq  L_{H} \preceq  (1+\varepsilon)L_G.
\end{equation}
By definition, it is easy to show that, if we decompose the edge set of a graph $G=(V,E)$ into $E_1,\ldots,E_{\ell}$ for a constant
$\ell$ and $H_i$ is a  spectral sparsifier of $G_i=(V, E_i)$ for any $1\leq i\leq \ell$, then the graph formed by the union of  edge sets from $H_i$ is a spectral sparsifier of $G$.  It is known that, for any undirected graph $G$ of $n$ vertices, there is a $(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n/\varepsilon^2)$ edges, and it can be constructed in almost-linear time~\cite{LS15}. 

The following lemma shows that a spectral sparsifier preserves the clustering structure of a graph. 

 

\begin{lem}\label{lem:sssubset}
Let $H$ be a $(1+\varepsilon)$-spectral sparsifier of $G$ for some $\varepsilon\leq 1/3$. Then, it holds for any set $S\subseteq V$ that   $\phi_H(S)\in \left(\frac{1}{2},2\right)\phi_G(S)$.
\end{lem}

\begin{proof}
Let $x_u\in\mathbb{R}^n$ be the indicator vector of vertex $u$, i.e., $x_u(v)=1$ if $u=v$, and $x_u(v)=0$ otherwise. We have that \[
(1-\varepsilon)\cdot x_u^{\rot} L_Gx_u \leq x_u^{\rot} L_H x_u  \leq (1+\varepsilon)\cdot x_u^{\rot} L_G x_u,
\]
which  implies that  $
(1-\varepsilon)\cdot\mu_G(S)\leq \mu_H(S)\leq (1+\varepsilon)\cdot \mu_G(S)$ for any subset $S$.

Similarly, for any set $S\subseteq V$ we define the indicator vector of $S$ by $x_S\in\mathbb{R}^n$, where $x_S(u)=1$ if $u\in S$, and $x_S(u)=0$ otherwise. Hence, $x_S^{\rot}L_Gx_S=w_G(S,V\setminus S)$, and $x_S^{\rot}L_Hx_S=w_H(S,V\setminus S)$. Combining these with \eq{ssproperty}, we have that 
\[
(1-\varepsilon)\cdot w_G(S, V\setminus S)\leq w_H(S, V\setminus S)\leq (1+\varepsilon)\cdot w_G(S, V\setminus S).
\]
Hence, for any subset $S$ we have that 
\[
\phi_H(S) =\frac{w_H(S, V\setminus S)}{\vol_H(S)} \leq \frac{(1+\varepsilon) w_G(S, V\setminus S)}{(1-\varepsilon) \vol_G(S) } \leq 2\cdot\phi_G(S),
\]
where the last inequality holds by assuming $\varepsilon\leq 1/3$. Similarly, we have that
\[
\phi_H(S) =\frac{w_H(S, V\setminus S)}{\vol_H(S)} \geq \frac{(1-\varepsilon) w_G(S, V\setminus S)}{(1+\varepsilon) \vol_G(S) } \geq \frac{1}{2}\cdot\phi_G(S).
\]
Hence,   $\phi_H(S)$ and $\phi_G(S)$ differ by at most a factor of 2 for any vertex set $S$. 
\end{proof}

%\begin{lem}\label{lem:ssadd}
%Let $G_1$ and $G_2$ be two  graphs on the same vertex set, and $H_1$ and $H_2$ be  $(1+\varepsilon)$-spectral sparsifiers of $G_1$ and $G_2$ respectively, for some  $\varepsilon<1$. Then, $H_1\boxplus H_2$ is a $(1+\varepsilon)$-spectral sparsifier of $G_1\boxplus G_2$. 
%\end{lem} 
 
%\begin{proof}
%The statement holds by noticing that the Laplacian matrix of $G_1\boxplus G_2$~(as well as $H_1\boxplus H_1$) is the sum of Laplacian matrices of $G_1$ and $G_2$~(as well as $H_1$ and $H_2$).
%\end{proof}
 

\subsection{Models of computation}  We  study distributed clustering in two models for distributed data: the message passing model and the blackboard model. The message passing model represents those distributed computation systems with point-to-point communication, and the blackboard model represents those where messages can be broadcast to all parties. 

More precisely, in the message passing model there are $s$ sites $\mathcal{P}_1,\ldots, \mathcal{P}_s$, and one coordinator. These sites can talk to the coordinator through a two-way private channel. In fact, this is referred to as the coordinator model in Section \ref{sec:intro}, where it is shown to be equivalent to the point-to-point model up to small factors. The input is initially distributed at the $s$ sites. The computation is in terms of rounds: at the beginning of each round, the coordinator sends a message to some of the $s$ sites, and then each of those sites that have been contacted by the coordinator sends a message back to the coordinator.  At the end, the coordinator outputs the answer.   In the alternative blackboard model, the coordinator is simply a blackboard where these $s$ sites $\mathcal{P}_1,\ldots, \mathcal{P}_s$ can share information; in other words, if one site sends a message to the coordinator/blackboard then all the other $s-1$ sites can see this information without further communication. The order for the sites to speak is decided by the contents of the blackboard.

For both models we measure the \emph{communication cost} as the total number of bits sent through the channels.  %We comment that for most problems, including those in this paper, if the input is distributed into the $s$ sites such that each , then the communication cost between each site and the coordinator will also be balanced.  %
The two models are now standard in multiparty communication complexity (see, e.g., \cite{beopv13,pvz16,wz12}).  They are similar to the congested clique model~\cite{LPPP03}  studied in the distributed computing community; the main difference is that in our models we do not post any bandwidth limitations at each channel but instead consider the total number of bits communicated.




\subsection{Communication complexity}  For any problem $\mathcal{A}$ and any protocol $\Pi$ solving $\mathcal{A}$, the \emph{communication complexity} of a protocol $\Pi$ is the maximum communication cost of $\Pi$ over all possible inputs $X$.  When the protocol is randomised, we define 
the \emph{error} of $\Pi$   by 
\[
\max_X\mathbb{P}\left(\mbox{the coordinator outputs an incorrect answer on $X$}\right),
\]
where the $\max$ is over all inputs $X$ and the probability is over all random strings of the coordinator and $s$ sites.
 The \emph{$\delta$-error randomised communication complexity} $\mathsf{R}_{\delta}(\mathcal{A})$ of a problem $\mathcal{A}$ in the message passing model  is the minimum communication complexity of any randomised protocol $\Pi$ that solves $\mathcal{A}$ with error at most $\delta$.  
 
Let $\mu$ be an input distribution on $X$. We call a deterministic protocol $(\delta, \mu)\text{-error}$ if it gives the correct answer for $\mathcal{A}$ on at least a $1 - \delta$ fraction of all input pairs, weighted by the distribution $\mu$. We denote $\mathsf{D}_{\delta, \mu}(\mathcal{A})$ as the cost of the minimum-communication $(\delta, \mu)\text{-error}$ protocol. A standard lemma in communication complexity called Yao's minimax lemma shows that 
$\mathsf{R}_{\delta}(\mathcal{A}) \ge \max_\mu \mathsf{D}_{\delta, \mu}(\mathcal{A}).$
%Thus to prove a lower bound for randomized communication complexity we can just find a hard input distribution and prove deterministic communication complexity for that distribution.

\subsection{Information complexity}  We abuse notation by using $\Pi$ for both the protocol and its transcript (its concatenation of messages). In the message passing model, let $\Pi_i\ (i \in [s])$ be the transcript (set of messages exchanged) between the $i$-th site and the coordinator.  Then $\Pi$ can be seen as a concatenation $\Pi_1 \circ \Pi_2 \circ \ldots \circ \Pi_s$ ordered by the timestamps of the messages.
We define the information complexity of a problem $\mathcal{A}$ in the message passing model by 
\[
\mathsf{IC}_{\mu, \delta}(\mathcal{A}) = \min_{(\delta, \mu)\text{-error}\ \Pi} \sum_{i \in [s]} I(X_1, \ldots, X_s; \Pi_i),
\]where $I(\cdot\ ;\ \cdot)$ is the mutual information function. It has been shown in \cite{HRVZ15} that $\mathsf{R}_{\delta}(\mathcal{A}) \ge \mathsf{IC}_{\delta, \mu}(\mathcal{A})$ for any input distribution $\mu$.





