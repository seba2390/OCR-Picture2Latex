
\subsection{The blackboard model\label{sec:bb}}
Next we present a graph clustering algorithm with $\tilde{O}(n+s)$ bits of communication cost in the blackboard model. 
Our result is based on the observation that a spectral sparsifier preserves the structure of clusters, which was used for proving \thmref{gcss}. So it suffices to design a  distributed algorithm for constructing a spectral sparsifier in the blackboard model. 


Our distributed algorithm  is based on constructing a chain of coarse sparsifiers~\cite{MP12}, which is described as follows: for any input \textsf{PSD} matrix $K$ with $\lambda_{\max}(K)\leq \lambda_u$ and all the non-zero eigenvalues of $K$  at least $\lambda_{\ell}$,   we define $d=\lceil \log_2(\lambda_u/\lambda_{\ell})\rceil$ and construct a chain of $d+1$ matrices
\begin{equation}\label{eq:chain}
[K(0),K(1),\ldots, K(d)],
\end{equation}
where $ 
\gamma(i)=\lambda_u/2^i$ and $K(i)= K+\gamma(i)I$.
Notice that in the chain above every $K(i-1)$
 is obtained by adding weights to the diagonal entries of $K(i)$, and 
 $K(i-1)$ approximates $K(i)$ as long as the weights added to the diagonal entries are  small.  
We will construct this chain recursively, so that $K(0)$ has heavy diagonal entries and can be approximated by a diagonal matrix. Moreover, since $K$ is the Laplacian matrix of a graph $G$, it is easy to see that $d=O(\log n)$ as long as the edge weights of $G$  are polynomially upper-bounded in $n$.


\begin{lem}[\cite{MP12}] \label{lem:MP12}
The chain~\eq{chain} satisfies the following relations: (1) 
 $K\preceq_r K(d)\preceq_r 2K$;
(2)  $K(\ell) \preceq K(\ell-1) \preceq 2K(\ell)$ for all $\ell\in\{1,\ldots, d\}$;
(3)  $K(0)\preceq 2\gamma(0)I\preceq 2K(0)$.
\end{lem}




%Such a chain can be constructed recursively so that $K(0)$ has heavy diagonal entries, and can be approximated by a diagonal matrix. Moreover, it is easy to prove that $d=O(\log n)$ if $K$ is the Laplacian matrix of a graph $G$, provided $G$'s edge weights are polynomially bounded.


Based on \lemref{MP12}, we will construct a chain of matrices  
\begin{equation}\label{eq:aptchain}
\left[ \tilde{K}(0), \tilde{K}(1),\ldots, \tilde{K}(d) \right]
\end{equation}
in the blackboard model, 
such that every $\tilde{K}(\ell)$ is a spectral sparsifier of $K(\ell)$, and 
every $\tilde{K}(\ell+1)$ can be constructed from $\tilde{K}(\ell)$.  The basic idea behind our construction is to use the relations among different $K(\ell)$ shown in \lemref{MP12} and the fact that, for any $K=B^{\rot}B$, sampling rows of $B$ with respect to their leverage scores can be used to obtain a matrix approximating $K$.


%It is well known that, for  $K(\ell)\triangleq B^{\rot}B$,  sampling rows $b_i$ of $B$ according to their leverage scores, denoted by$\tau_i\triangleq b_i^{\rot}K^{+}b_i$, will give a matrix approximating $K$. Formally, we assume that $\tilde{\tau}$ is the vector of leverage score overestimate for $B$'s rows such that $\tilde{\tau}_i\geq \tau_i$ for all $i\in[m]$, $0<\varepsilon<1$, and $c$ is a fixed constant. We sample every row $b_i$ with  probability $p_i=\min\left\{1, c\varepsilon^{-2}\tilde{\tau}_i\log n\right\}$, and define a diagonal matrix $W$ with $W_{i,i}=\frac{1}{p_i}$ with probability $p_i$, and $W(i,i)=0$ otherwise. Then, it holds with high probability that $(1-\varepsilon)K(\ell)\preceq \tilde{K}(\ell)=B^{\rot}WB \preceq (1+\varepsilon)K(\ell)$.Moreover, $W$ has $O\left( \|\tilde{\tau} \|_1\varepsilon^{-2}\log n \right)$ non-zeros with high probability. We prove in  Appendix~\ref{sec:b3} that the above sampling procedure can be implemented in the blackboard model, and this gives the following result:

\begin{thm}\label{thm:gcblackboard}
Let $G$ be an undirected graph on $n$ vertices, where the edges of $G$ are allocated among $s$ sites, and the edge weights are polynomially upper bounded in $n$. Then, a spectral sparsifier of $G$ can be constructed with $\tilde{O}(n+s)$ bits of communication in the blackboard model. That is, the chain \eq{aptchain} can be constructed with $\tilde{O}(n+s)$ bits of communication in the blackboard model.
\end{thm}



\begin{proof} Let $K= B^{\rot}B$  be the Laplacian matrix of the underlying graph $G$, where $B\in\mathbb{R}^{m\times n}$ is the  edge-vertex incidence matrix of $G$. We will  prove  that 
 every $\tilde{K}(i+1)$ can be constructed based on $\tilde{K}(i)$ with  $\tilde{O}(n+s)$ bits of communication. This implies that $\tilde{K}(d)$, a $(1+\varepsilon)$-spectral sparsifier of $K$, can be constructed with $\tilde{O}(n+s)$ bits of communication, as the length of the chain $d=O(\log n)$.  
 
 
 First of all, notice that  $\lambda_u\leq 2n$, and the value of $n$ can be obtained with communication cost $\tilde{O}(n+s)$~(different sites sequentially write the new IDs of the vertices on the blackboard). In the following  we assume that $\lambda_u$ is the upper bound of $\lambda_{\max}$ that we actually obtained in the blackboard.
 
\emph{Base case of $\ell=0$:} 
 By definition, $K(0)=K+\lambda_u\cdot  I$, and 
$
 \frac{1}{2}\cdot K(0)\preceq \gamma(0)\cdot I \preceq K(0)$, 
due to Statement~3 of  \lemref{MP12}.  Let $\oplus$ denote appending the rows of one matrix to another. We define $B_{\gamma(0)}=B\oplus \sqrt{\gamma(0)}\cdot I$, and write $K(0)=K+\gamma(0)\cdot I=B^{\rot}_{\gamma(0)}B_{\gamma(0)}$. By defining $\tau_i=b_i^{\rot}\left(K(0)\right)^{\rot}b_i$ for each row of $B_{\gamma(0)}$, we have
$
\tau_i\leq b_i^{\rot}\left(\gamma(0)\cdot I\right) b_i\leq 2\cdot \tau_i$.
Let $\tilde{\tau}_i=b_i^{\rot}\left( \gamma(0)\cdot I \right)^{+}b_i$ be the leverage score of $b_i$ approximated using $\gamma(0)\cdot I$, and let $\tilde{\tau}$ be the vector of approximate leverage scores, with the leverage scores of the $n$ rows corresponding to $\sqrt{\gamma(0)}\cdot I$ rounded up to 1. Then, with high probability sampling $O(\varepsilon^{-2}n\log n)$ rows of $B$ will give a matrix $\tilde{K}(0)$ such that $
(1-\varepsilon)K(0)\preceq \tilde{K}(0)\preceq (1+\varepsilon)K(0)$.
Notice that, as every row of $B$ corresponds to an edge of $G$, the approximate leverage scores $\tilde{\tau}_i$ for different edges can be computed locally by different sites maintaining the edges, and the sites only need to send the information of the sampled edges to the blackboard, hence the communication cost is $\tilde{O}(n+s)$ bits.

\emph{Induction step:} We assume that 
$
(1-\varepsilon)K(\ell)\preceq_r\tilde{K}(\ell) \preceq_r (1+\varepsilon)K(\ell)$,
and the blackboard maintains the matrix $\tilde{K}(\ell)$. This implies that
$
(1-\varepsilon)/(1+\varepsilon)\cdot K(\ell)\preceq_r 1/(1+\varepsilon)\cdot \tilde{K}(\ell) \preceq_r K(\ell).
$
Combining this with Statement~2 of \lemref{MP12}, we have that 
\[
\frac{1-\varepsilon}{2(1+\varepsilon)}K(\ell+1)\preceq_r \frac{1}{2(1+\varepsilon)}\tilde{K}(\ell) \preceq K(\ell+1).
\]
We apply the same sampling procedure as in the base case, and obtain a matrix $\tilde{K}(\ell+1)$ such that $
(1-\varepsilon) K(\ell+1) \preceq_r\tilde{K}(\ell+1) \preceq_r (1+\varepsilon) K(\ell+1)$.
Notice that, since $\tilde{K}(\ell)$ is written on  the blackboard, the probabilities used for sampling individual edges can be computed locally by different sites, and in each round only the sampled edges will be sent to the blackboard in order for the blackboard to obtain $\tilde{K}(\ell+1)$. Hence, the total communication cost in each iteration is $\tilde{O}(n+s)$ bits. Combining this with the fact that the chain length $d=O(\log n)$ proves the theorem. 
\end{proof}




Combining \thmref{gcblackboard} and the fact that a spectral sparsifier preserves the structure of clusters, 
we obtain a distributed  algorithm in the blackboard model with total communication cost $\tilde{O}(n+s)$ bits, and the performance of our algorithm is the same as in the statement of \thmref{gcss}. Notice that $\Omega(n+s)$ bits of communication are needed for graph clustering in the blackboard model, since the output of a clustering algorithm contains $\Omega(n)$ bits of information and each site needs to communicate at least one bit. Hence the communication cost of our proposed algorithm is optimal up to a poly-logarithmic factor.

 
