\section{Distributed geometric clustering\label{sec:geo}}
We now consider geometric clustering, including $k$-median, $k$-means and $k$-center.  Let $P$ be a set of points of size $n$ in a metric space with distance function $d(\cdot, \cdot)$, and let $k \le n$ be an integer. In the $k$-center problem we want to find a set $C\ (|C| = k)$ such that $\max_{p \in P} d(p, C)$ is minimized, where $d(p, C) = \min_{c \in C} d(p, c)$.  In $k$-median and $k$-means we replace the objective function $\max_{p \in P} d(p, C)$ with $\sum_{p \in P} d(p, C)$ and $\sum_{p \in P} (d(p, C))^2$, respectively.
%We will assume that the input consists of points in the Euclidean space.

\subsection{The message passing model}
\label{sec:geo-mp}

As mentioned, for constant dimensional Euclidean space and a constant $c > 1$, there are algorithms that $c$-approximate $k$-median and $k$-means using $\tilde{O}(sk)$ bits of communication~\cite{BEL13}.  For $k$-center, the folklore parallel guessing algorithms (see, e.g., \cite{CMZ07}) achieve a $2.01$-approximation using $\tilde{O}(sk)$ bits of communication.  

The following theorem states that the above upper bounds are tight up to logarithmic factors. % Due to space constraints we defer the proof to the full version of this paper. %Appendix~\ref{app:proof-geometric}.  
The proof uses tools from multiparty communication complexity.  We in fact can prove a stronger statement that any algorithm that can differentiate whether we have $k$ points or $k+1$ points in total in the message passing model needs $\Omega(sk)$ bits of communication.

\begin{thm}
\label{thm:geometric}
For any $c > 1$, computing $c$-approximation for $k$-median, $k$-means or $k$-center correctly with probability $0.99$ in the message passing model needs $\Omega(sk)$ bits of communication.
\end{thm}

\begin{proof}
We can in fact prove a more general result: we can show that the $\Omega(sk)$ lower bound holds for any {\em eligible} function which evaluates to $0$ if there are at most $k$ points, and evaluates to a value greater than $0$ if there are at least $k+1$ points.  Note that $k$-median, $k$-means and $k$-center are all eligible functions.  We prove this by a simple reduction from \DISJL\ where $\ell = (k+1)/2$ (w.l.o.g., assuming $k$ is odd). 

The reduction is as follows. Given an $s$-player set-disjointness instance of size $\ell$ (i.e., \DISJL), let $X_i = (X_i^1, \ldots, X_i^\ell)$ be the $i$-th row of the input matrix $X$.  Let $p^1, \ldots, p^\ell$ and $q^1, \ldots, q^\ell$ be $2\ell$ distinct point locations on a line under Euclidean distance.  Each site $i$ does the following: for each coordinate $j$, if $X_i^j = 0$ then it put a point $u_i^j$ at location $q^j$; otherwise if $X_i^j = 1$ it put a point at location $p^j$.  It is easy to see that $\mathsf{DISJ}_{s,\ell} = 1$ if and only if the number of distinct points in $\bigcup_{i \in [s], j \in [\ell]} u_i^j$ is $2(\ell - 1) + 1 = k$; and $\mathsf{DISJ}_{s,\ell} = 1$ if and only if the number of distinct points in $\bigcup_{i \in [s], j \in [\ell]} u_i^j$ is $2(\ell - 1) + 2 = k  + 1$.  The lower bound follows from the definition of eligible function and Theorem~\ref{thm:DISJ}.
\end{proof}


A number of works on clustering consider {\em bicriteria} solutions (e.g., \cite{KPR98,CKMN01}). An algorithm is a $(c_1, c_2)$-approximation $(c_1, c_2 > 1)$ if the optimal solution costs $W$ when using $k$ centers, then the output of the algorithm costs at most $c_1 W$ when using at most $c_2 k$ centers.  We can show that for $k$-median and $k$-means, the $\Omega(sk)$ lower bound  holds even for algorithms with bicriteria approximations.  
\begin{thm}
\label{thm:bicriteria}

For any $c \in [1, 1.01]$, computing $(7.1 - 6c, c)$-bicriteria-approximation for $k$-median or $k$-means  correctly with probability $0.99$ in the message passing model needs $\Omega(sk)$ bits of communication.
\end{thm}


%\subsection{Proof of Theorem~\ref{thm:bicriteria}}
\label{app:proof-bicriteria}
%\he{To Qin: I copied everything you wrote from the appendix of our NIPS submission here. It seems unusual to put the lemmas/theorems inside a proof. Can you make some changes here?}
Before proving Theorem \ref{thm:bicriteria}, we first show the following technical lemma.  
\begin{lem}
\label{lem:direct-sum}
In the message-passing model, $\Omega(s \ell)$ bits of communication is needed for computing at least a $0.8$ fraction of $j \in [\ell]$ \ONE$(X^j)$ correctly with probability $0.99$ under the input distribution $X \sim \mu_\ell$.
\end{lem}

\begin{proof}
By a Markov inequality, there must exist $\Omega(s)$ coordinates $j$ such that the algorithm computes \ONE$(X^j)\ (X^j \sim \nu)$  with error probability at most $0.24$. Call each of these coordinates $j$ {\em good}. Let $\Pi$ be the protocol transcript. We have
\begin{eqnarray*}
I(X; \Pi) &=& \sum_{j \in [\ell]} I(X^j; \Pi\ |\ X^{-j}) \\
&\ge& \sum_{j \in [\ell]} I(X^j; \Pi) \quad (X^j \text{ and } X^{-j} \text{ are independent})\\
&\ge& \sum_{\text{good } j} I(X^j; \Pi)  \\
&\ge& \Omega(s) \cdot \mathsf{IC}_{0.24, \nu}(\text{\ONE})\\
&\ge& \Omega(s \ell). \quad \quad (\text{Theorem}\ \ref{thm:DISJ})
\end{eqnarray*}
\end{proof}



%We again prove by a reduction from the $s$-player set-disjointness, but the arguments are more involved.   Our proof holds for both $k$-median and $k$-means.

Now we are ready to prove the theorem.
%{\bf The reduction.}  
\begin{proof}[Proof of  \thmref{bicriteria}]
We consider $8 \ell$ point locations on a line (under Euclidean distance) with $x$-coordinates being $1, 2, \ldots, 8 \ell$. We put a point with {\em infinite} weight at every even point location. We name the $4 \ell$ odd point locations from left to right as 
$$p^1, q^1, p^2, q^2, \ldots, p^{\ell}, q^{\ell}, z^1, z^2, \ldots, z^{2\ell}.$$  
For each site $i \in [s]$ and each column $j \in [\ell]$, if $X_i^j = 0$ then we put a point with weight $1$ at location $q^j$; otherwise if $X_i^j = 1$ then we put a point with weight $1$ at location $p^j$.  We also put a point with weight $1/2$ at each of the ``dummy'' locations $z^1, \ldots, z^{2\ell}$.  Let the weight of a {\em location} be the sum of the weights of points falling into that location.

Given such an input $X$, for both $k$-median and $k$-means, the optimal solution ($\OPT$) which is allowed to use $k = 6\ell$ centers will include all locations $p^j$ and $q^j$ whose weights are at least $1$ (note that there are {\em at most} $2\ell$ such locations), the $4 \ell$ even point locations, and as many as dummy locations that it can still include.  The cost of the optimal solution will be precisely the cost of linking the points in the rest of the dummy locations to their nearest centers (at the even locations), which can be written as
$$\OPT = 1/2 \cdot (k/3 - (k/3 - F_0)) = 1/2 \cdot F_0 \le \ell,$$
where $F_0$ is the number of locations in $\{p^1, q^1, \ldots, p^\ell, q^\ell\}$ that have weights at least $1$.

Now suppose our solution ($\SOL$) outputs $c k$ centers for a constant $c \in [1, 1.01]$. Each time we include a  location $q^j$ as a center when there is no $0$-coordinate in the input column $X^j$, we have a loss of $1/2$ since we miss out on including a dummy location (i.e., we can take one more dummy location instead of taking $q^j$ as a center). Similarly, each time we do not include a location $q^j$ as a center when there is a $0$-coordinate in $X^j$, we have a loss of $1/2$ since a point at $q^j$ has weight at least $1$ but a point at a dummy location has weight at $1/2$. Therefore, even if we are allowed to output $c k$ medians, we will still need to figure out whether there is any point at location $q^j$ for at least an $\alpha = 0.9$ fraction of the coordinates $j \in [\ell]$. If not, then
\begin{eqnarray}
\SOL - \OPT &\ge& {1}/{2} \cdot (1 - \alpha) \ell - 1 \cdot (c - 1) k \label{eq:a-1} \ \ \ \ \\
&= & \frac{(1 - \alpha) - 12(c - 1)}{2} \cdot \ell \nonumber \\
&\ge& (6.1 - 6c) \OPT, \nonumber
\end{eqnarray}
where the first term in the RHS of (\ref{eq:a-1}) counts the loss of incorrectly computing the (at least) $(1-\alpha) \ell$ coordinates $j \in [\ell]$, and the second term counts the maximum gain of the extra $(c - 1) k$ centers $\SOL$ can use (compared with $\OPT$). 

By Lemma~\ref{lem:direct-sum}, we have that for any $c \in [1, 1.01]$, computing $(7.1 - 6c, c)$-bicriteria-approximation for $k$-median or $k$-means in the message passing model correctly with probability $0.9$ under distribution $X \sim \mu$ needs $\Omega(sk)$ bits of communication.  The theorem follows by Yao's minimax principle.
\end{proof}




\subsection{The blackboard model}
\label{sec:geo-bb}
%In Appendix~\ref{app:alg-geometric} 
We can show that there is an algorithm that achieves an $O(1)$-approximation using $\tilde{O}(s + k)$ bits of communication for $k$-median and $k$-means.  %Due to space constraints we defer the description of the algorithm to the full version of this paper.
For $k$-center, it is straightforward to implement the parallel guessing algorithm in the blackboard model using $\tilde{O}(s + k)$ bits of communication.

%\begin{thm}
%\label{thm:geometric-bb}
%There are algorithms that compute $O(1)$-approximations for $k$-median, $k$-means and $k$-center correctly with probability $0.9$ in the blackboard model using $\tilde{O}(s+k)$ bits of communication.
%\end{thm}
%\he{we should write the following discussion as a proof. Currently there is no proof of Theorem 4.4. We should also add some discussion on the lower bound in the blackboard model.}


%\he{The following is from the appendix of our NIPS submission. I did not make any changes.}

Our algorithm for $k$-median/means is an easy adaptation of the {\em successive sampling} algorithm proposed by Mettu and Plaxton~\cite{MP04} in the (centralized) RAM model.  We first summarize their algorithm and then describe how to port it to the blackboard model.

Let $X_1, \ldots X_s$ be the point sets at sites $\mathcal{P}_1, \ldots, \mathcal{P}_k$ respectively. The successive sampling algorithm proceeds in rounds. At each round $j$ it does the following:
\begin{enumerate}
\item $s$ sites jointly sample $O(k)$ point centers, denoted by $Y_j$;

\item $s$ sites grow balls from each of the point centers in $Y_j$ synchronously until a time step when a $0.9$ fraction of points in $\bigcup_{i \in [s]} X_i$ are covered;

\item each site $\mathcal{P}_i$ updates $X_j$ by removing those points that are covered by any of the balls centered at points in $Y_j$;

\item $s$ sites remove all the points covered by balls centered at points in $Y_j$, and proceed to the next round $j+1$.
\end{enumerate}
It is easy to see that the computation will finish in $r = O(\log n)$ rounds since at each round we remove a constant fraction of points.  At the end we compute an $O(1)$-approximation of $k$-median or $k$-means on the $O(k \log n)$ points $\bigcup_{j \in [r]} Y_j$.  In \cite{MP04} it has been shown that this algorithm gives an $O(1)$-approximation to $k$-median or $k$-means with high probability.

We now describe how to implement this centralized algorithm in the blackboard model.  We first consider each round.  Step 1 can be done by the distributed sampling algorithm in \cite{CMYZ12} using $\tilde{O}(k+s)$ bits of communication; note that at the end of this step the sampled points in $Y_j$ are written on the blackboard.
Step $2$ can be done by a binary search for the minimum ball radius $t_j$ such that $\bigcup_{p \in Y_j} \mathtt{Ball}(p, t_j)$ covers at least a $0.9$ fraction of points in  $\bigcup_{i \in [s]} X_i$, where $\mathtt{Ball}(p, t_j)$ denotes the ball centered at $p$ with radius $t_j$;  this binary search can be done using $\tilde{O}(1)$ bits of communication.  Step $3$ and $4$ can be done locally without any communication.   After $r$ rounds, the final clustering step can be done by any of the $s$ sites since all points in $\bigcup_{j \in [r]} Y_j$ have already been written on the blackboard.
Therefore the total communication cost can be bounded by $\tilde{O}(k+s)$.

Finally, we would like to mention that $\Omega(k + s)$ is an obvious lower bound, and thus our upper bound is tight up to logarithmic factors.  To see this, notice that $k$ is the size of the output, and the coordinator has to communication with each of the $s$ sites for at least $1$ bit.









