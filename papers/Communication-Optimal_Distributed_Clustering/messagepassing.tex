
\section{Distributed graph  clustering\label{sec:graphmsg}}

In this section we study distributed graph clustering.   We assume that the vertex set of  the input graph $G=(V,E)$ can be partitioned into $k$ clusters, where vertices in each cluster $S$ are highly connected to each other, and there are fewer edges between $S$ and $V\setminus S$.  To formalize this notion, 
 we  define the \emph{$k$-way expansion constant} of graph $G$ by 
\[
\rho(k)\triangleq \min_{\mbox{\footnotesize partition $A_1,\ldots, A_k$}}\max_{1\leq i\leq k}\phi_G(A_i).
\] Notice that a graph $G$ has $k$ clusters if the value of $\rho(k)$ is small. It was shown in \cite{conf/stoc/LeeGT12} that $\rho(k)$ closely relates to  $\lambda_k(\mathcal{L}_G)$ by the following higher-order Cheeger inequality:
\begin{equation*}
\frac{\lambda_k(\mathcal{L}_G)}{2}\leq \rho(k)\leq O(k^2) \sqrt{\lambda_k(\mathcal{L}_G)}.
\end{equation*}
Hence, a large gap between $\lambda_{k+1}(\mathcal{L}_G)$ and $\rho(k)$ implies (i) the existence of 
a $k$-way partition $\{S_i\}_{i=1}^k$ such that every $S_i$ has small conductance
 $\phi_G(S_i)\leq \rho(k)$, and (ii) any $(k+1)$-way partition of $G$ contains a subset with  high conductance $\rho(k+1)\geq \lambda_{k+1}(\mathcal{L}_G)/2$. Therefore, a large gap between $\lambda_{k+1}(\mathcal{L}_G)$ and $\rho(k)$ ensures that $G$ has \emph{exactly} $k$ clusters. In the following, we assume that 
\[
\Upsilon\triangleq \lambda_{k+1}(\mathcal{L}_G)/\rho(k)=\Omega(k^3)
\]  
to ensure that the input graph $G$ has exactly $k$ clusters. The same assumption has been used 
 in the literature for studying graph clustering in the centralized setting~\cite{PSZ15}.

Both algorithms presented in the section are based on the following \emph{spectral clustering} algorithm: (i) compute the  $k$ eigenvectors $f_1,\ldots, f_k$ of $\mathcal{L}_G$ associated with $\lambda_1(\mathcal{L}_G),\ldots,\lambda_k(\mathcal{L}_G)$; (ii) embed every vertex $v$ to a point in $\mathbb{R}^k$ through the embedding 
\[
F(v)=\frac{1}{\sqrt{d_v}}\cdot(f_1(v),\ldots, f_k(v));
\]
 (iii) run $k$-means on the embedded points $\{F(v)\}_{v\in V}$, and group the vertices of $G$ into $k$ clusters according to the output of $k$-means.

\subsection{The message passing model\label{sec:gcmessage}}

We assume  the edges of the input graph $G=(V,E)$ are arbitrarily allocated among $s$ sites $\mathcal{P}_1,\cdots, \mathcal{P}_s$, and we use  $E_i$ to denote the edge set maintained by site $\mathcal{P}_i$. Our proposed algorithm consists of two steps: (i) every  $\mathcal{P}_i$ computes a linear-sized $(1+\eps)$-spectral sparsifier $H_i$ of $G_i\triangleq (V, E_i)$, 
for a small constant $\eps\leq 1/10$,
and sends the edge set of  $H_i$, denoted by $E_i'$, to the coordinator; (ii) the coordinator runs a spectral clustering algorithm on the union of received graphs $H\triangleq \left(V,\bigcup_{i=1}^k E_i' \right)$. The theorem  below summarizes the performance of this algorithm, and shows the approximation guarantee of this algorithm is as good as the provable guarantee of spectral clustering known in the centralized setting, which is shown in the lemma below.


\begin{lem}[\cite{PSZ15}]\label{lem:PSZ}
Let $G$ be a graph satisfying the condition $\Upsilon=\Omega(k^3)$, and $k\in \mathbb{N}$.
Then, a spectral clustering algorithm outputs sets $A_1,\ldots, A_k$ such that $\vol(A_i\triangle S_i)= O\left(k^3\cdot\Upsilon^{-1}\cdot\vol(S_i)\right)$ holds for any $1\leq i\leq k$, where $S_i$ is the optimal cluster corresponding to $A_i$.
\end{lem}



\begin{thm}\label{thm:gcss}
Let $G=(V,E)$ be an $n$-vertex graph with $\Upsilon=\Omega(k^3)$, and suppose the edges of $G$ are
arbitrarily allocated among $s$ sites.  Assume  $S_1,\cdots, S_k$ is an optimal partition that achieves $\rho(k)$.
Then, the algorithm above computes  a partition $A_1,\ldots, A_k$ satisfying $\vol(A_i\triangle S_i)= O\left( k^3\cdot\Upsilon^{-1}\cdot\vol(S_i)\right)$  for any $1\leq i\leq k$. The total communication cost of this algorithm is $\tilde{O}(ns)$ bits. 
\end{thm}


\begin{proof}
By the definition of the Laplacian matrix, we have that $L_G=\sum_{i=1}^s L_{G_i}$. Since every $H_i$ is a $(1+\eps)$-spectral sparsifier of graph $G_i$,   we have that 
$(1-\eps) L_{H_i}\preceq L_{G_i} \preceq (1+\eps) L_{H_i}$.
This implies that $
(1-\eps) L_{H}\preceq L_{G} \preceq (1+\eps) L_{H}$,
by the definition of $H_i$ and graph Laplacians. 
Now we show that our assumption on $\Upsilon$ is preserved in $H$. By \lemref{sssubset}, we have for any $1\leq i \leq k$ that 
$\phi_H(S_i)\in \left(\frac{1}{2},2\right) \phi_G(S_i)$,
which implies that $S_i$ has low conductance in $H$, and $\rho_H(k)\in \left(\frac{1}{2},2\right)\rho_G(k)$. To show that $\lambda_k(\mathcal{L}_H)$ is a constant approximation of $\lambda_k(\mathcal{L}_G)$, notice that \[(1-c)\cdot x^{\rot} L_Gx \leq x^{\rot} L_H x \leq (1+c)\cdot x^{\rot} L_G x\] holds for any $x\in\mathbb{R}^n$. Hence
it holds for any $x\in\mathbb{R}^n$ that 
\[
(1-\varepsilon)\cdot x^{\rot}D_G^{-1/2} L_GD_G^{-1/2}x \leq x^{\rot} D_G^{-1/2}L_H D_G^{-1/2}x \leq (1+\varepsilon)\cdot x^{\rot}D_G^{-1/2} L_GD_G^{-1/2} x.\]
Since $D_G^{-1/2} L_GD_G^{-1/2}=\mathcal{L}_G$ and  $\frac{1}{2} D_G^{-1}\preceq D_H^{-1}\preceq 2 D_G^{-1}$, we have that $\lambda_i\left(\mathcal{L}_H\right)=\Theta\left(\lambda_i\left(\mathcal{L}_G\right)\right)$, and the  assumption on $\Upsilon$ in $H$ is preserved from $G$ up to a constant factor.  By \lemref{PSZ}, the output of a spectral clustering algorithm on $H$ satisfies the claimed properties.
The total communication cost of $\tilde{O}(ns)$  bits follows from the fact that every $H_i$ has  $O(n)$ edges.
\end{proof}

Next we show that the communication cost of our proposed algorithm is optimal up to a logarithmic factor. 
 Our analysis is based on a reduction from graph clustering to the Multiparty Set-Disjointness problem~(\DISJN): for any $s$ sites  $\mathcal{P}_1,\ldots, \mathcal{P}_s$, where each  $\mathcal{P}_i$ has a set $S_i\subseteq [n]$, let $X_i = (X_i^1, \ldots, X_i^n)$ be the characteristic vector of $S_i$, and let $X = (X_1, \ldots, X_s)$ be the input matrix with $X_i$ being the $i$-th row.  Let $X^j = (X_1^j, \ldots, X_s^j)$ be the $j$-th column of the input matrix $X$. 
We define a function \ONE\ on an $s$-bit vector $Y = (Y_1, \ldots, Y_s)$ as
$
\text{\ONE} (Y)  =  \bigwedge_{i \in [s]} Y_i$, and $ \text{\DISJN}(X) = \bigvee_{j \in [n]} \text{\ONE} (X^j)$. Then the \DISJN\ problem asks the value of $\text{\DISJN}(X)$.
We  introduce two hard input distributions for \ONE\ and \DISJN\ respectively.  
\begin{itemize}
\item {\em Hard input distribution $\nu$ on $Y \in \{0,1\}^s$ for \ONE:} with probability $1/2$, we choose each $Y_i\ (i \in [s])$ to be $0$ or $1$ with equal probability; with probability $1/4$ we choose $Y$ to be an all-$1$ vector; and with the remaining probability $1/4$ we choose $Y$ to be a random vector with $n-1$ coordinates being $1$'s and a random coordinate being $0$.
\item {\em Hard input distribution $\mu_n$ on $X \in \{0,1\}^{s \times n}$ for \DISJN:}  For each $j \in [n]$, we choose $X^j \sim \nu$.  
\end{itemize}
%We will use the following results by Braverman et al.



\begin{thm}[\cite{beopv13}] \label{thm:DISJ} 
It holds that $\mathsf{IC}_{0.49, \nu}(\text{\ONE}) = \Omega(s)$, and 
  $\mathsf{IC}_{0.49, \nu}(\text{\DISJN}) = \Omega(sn)$.
\end{thm}



\begin{lem} \label{lem:DISJ}
In the message passing model, any randomized algorithm that computes \DISJN\ correctly with probability $0.9$ needs $\Omega(sn)$ bits of communication.
\end{lem}


\begin{proof}
The lemma follows from \thmref{DISJ} and Yao's minimax lemma.
\end{proof}

\begin{thm}\label{thm:lbmsg}
Let $G$ be an undirected graph with $n$ vertices, and suppose the edges of $G$ are distributed among $s$ sites. Then, any algorithm that correctly outputs a constant fraction of a cluster in $G$ requires $\Omega(ns)$ bits of communication. This holds even if each cluster has constant expansion.  
\end{thm}



\begin{proof}
Our proof is based on the reduction from graph clustering to the Multiparty Set-Disjointness problem~(\DISJN).
For any item $j$ and site $\mathcal{P}_i$, we set $X^j_i=0$ if item $j$ appears in site $\mathcal{P}_i$, and $X^j_i=1$ otherwise.
Then $\text{\DISJN}(X)=1$ if there is some item not appearing in any site.  Now we construct a graph $G$ based on the hard instance $X$ of 
$\mathsf{DISJ}_{n,s}$ as follows: initially,  graph $G$ consists of  $n$ isolated vertices $\ell_1,\ldots, \ell_n$, and $r$ isolated vertices $r_1,\ldots, r_s$. Then, we add an edge between $\ell_j$ and $r_i$ if item $j$ appears in site $\mathcal{P}_i$. With this construction, it is easy to see that $\text{\DISJN}(X)=0$ if every vertex $\ell_j$ is connected to some $r_i$, and $\text{\DISJN}(X)=1$ if there are some isolated vertices $\ell_j$. 


We will show that, when $\text{\DISJN}(X)=0$, our constructed  graph $G$ is a bipartite expander, i.e., $G$ has only $1$ cluster.   To prove this, notice that, from the hard input distribution $\mu$ on $Y\in\{0,1\}^s$  described above, with probability $1/2$ we choose each $Y_i(i\in [s])$ to be $0$ or $1$ with equal probability. This implies that, for any $\ell_i$ and $r_j$, there is an edge between $\ell_i$ and $r_j$ independently with probability at least $1/4$. By standard results on constructing expanders, this implies $G$ is a bipartite expander with constant expansion, and in particular is connected.   

On the other side, when $\text{\DISJN}(X)=1$, every isolated vertex $\ell_j$ itself forms a cluster with conductance $0$ and constant expansion, and the giant component of $G$ forms a cluster with conductance $0$ and constant expansion (since, as argued in the previous paragraph, it is a bipartite expander). Let $k$ be the number of connected  components in graph $G$. Then, $\rho(k)=0$, and our assumption on $\Upsilon=\lambda_{k+1}(\mathcal{L}_G)/\rho(k)=\Omega(k^3)$ holds trivially. Hence any clustering algorithm  that is able to find a constant fraction of each cluster in graph $G$ satisfying $\Upsilon=\Omega(k^3)$ can be used to solve \DISJN. The lower bound on the communication complexity of graph clustering follows from the lower bound for \DISJN.  
\end{proof}




As a  remark, it is easy to see that this  lower bound also holds for constructing spectral sparsifiers: for any $n\times n$ $\mathsf{PSD}$ matrix $A$ whose entries are arbitrarily distributed among $s$ sites, any distributed algorithm that constructs a $(1+\Theta(1))$-spectral sparsifier of $A$ requires $\Omega(ns)$ bits of communication. This follows since such a spectral sparsifier can be used to solve the spectral clustering problem. Spectral sparsification has played an important role in designing fast algorithms from different areas, e.g., machine learning, and  numerical linear algebra. Hence our lower bound result for constructing spectral sparsifiers may have applications to studying other distributed learning algorithms.


 

