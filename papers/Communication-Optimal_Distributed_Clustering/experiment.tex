\newcommand{\twomoons}{{\tt Twomoons}}
\newcommand{\gauss}{{\tt Gauss}}
\newcommand{\sculpture}{{\tt Sculpture}}
\newcommand{\baseline}{{\tt Baseline}}
\newcommand{\MM}{{\tt MsgPassing}}
\newcommand{\blackboard}{{\tt Blackboard}}
\newcommand{\ncut}{\text{ncut}}
\newcommand{\chensays}[2][]{\textcolor{blue} {\textsc{Jiecao #1:} \emph{#2}}}

\section{Experiments}
In this section we present experimental results for  graph clustering in the message passing and blackboard models. We will compare the following three algorithms. (1) \baseline: each site sends all the data to the coordinator directly; (2) \MM: our algorithm in the message passing model (Section~\ref{sec:gcmessage}); (3) 
\blackboard: our algorithm in  the blackboard model (Section~\ref{sec:bb}).


%Since both of our algorithms are crucially based on the use of spectral scarification, our main focus in the experiments is to investigate to what extend the quality of the spectral clustering algorithms will be affected by using spectral sparsification, the saving of communication costs by using spectral sparsificaion, ...
%
%
%The goal of this experiment is not to demonstrate the effectiveness of the spectral clustering algorithm. We mainly want to investigate the following, 
%\begin{itemize}
%\item to what extend the quality of clustered results will be affected by using spectral sparsification.
%\item saving of communication costs by using spectral sparsifier.
%\item the affect of constants in algorithms of the message passing/blackboard model.
%\end{itemize}
%
%
%\subsection{The Setup}
%\paragraph{Reference Algorithms}
%We compare different algorithms in our experiment.

%Note that we can also run \MM~ in the blackboard model.

Besides giving the visualized results of these algorithms on various datasets, we also measure the qualities of the results via the {\em normalized cut}, defined as 
\[
\ncut(A_1, \ldots, A_{k}) = \frac{1}{2}\sum_{i\in[k]}\frac{w(A_i, V\backslash A_i)}{\vol(A_i)},
\]
 which is a standard objective function to be minimized for spectral clustering algorithms. 
%We will compare the communication costs of these algorithms in different settings.

%We also compare the total communication costs of different algorithms/models. As the unit does not matter in our case, we normalize all communication costs by the cost of \baseline.  Whenever possible, we will visualize the clustered results.

We implemented the algorithms using multiple languages, including Matlab, Python and C++. Our experiments were conducted on an IBM NeXtScale nx360 M4 server, which is equipped with 2 Intel Xeon E5-2652 v2 8-core processors, 32GB RAM and 250GB local storage.


\subsection{Datasets.}
We test the algorithms in the following real and synthetic datasets, which is visualized in \figref{visualization}.


\begin{figure}[h]
     \centering
     \subfigure[\twomoons]{\includegraphics[width=0.23\textwidth]{twomoons-14000-original.png}\label{fig:twomoons}}
     ~~
     \subfigure[\gauss]{\includegraphics[width=0.23\textwidth]{gauss-10000-original.png}\label{fig:gauss}}
     ~~
     \subfigure[\sculpture]{\includegraphics[width=0.13\textwidth,height=0.16\textwidth]{sculpture-11680-original.jpg}\label{fig:sculpture}}
     \caption{Visualization of the datasets for our experiments.}
     \label{fig:visualization}
\end{figure}



\vspace{-1mm}
\begin{itemize}
\item \twomoons : this dataset contains $n=14,000$ coordinates in $\mathbb{R}^2$. We consider each point to be a vertex. For any two vertices $u, v$, we add an edge with weight $w(u,v) = \exp\{-\|u-v\|_2^2/\sigma^2\}$ with $\sigma = 0.1$ when one vertex is among the $7000$-nearest points of the other.  This construction results in a graph with about $110,000,000$ edges.

\item  \gauss : this dataset contains $n = 10,000$ points in $\mathbb{R}^2$. There are $4$ clusters in this dataset, each generated using a Gaussian distribution. We construct a complete graph as the similarity graph.  For any two vertices $u, v$, we define the weight $w(u,v) = \exp\{-\|u-v\|_2^2/\sigma^2\}$ with $\sigma = 1$. The resulting graph has about $100,000,000$ edges.

\item \sculpture : a photo of \textit{The Greek Slave}~\footnote{Available in e.g., \url{http://artgallery.yale.edu/collections/objects/14794}}. We use an $80\times 150$ version of this photo where each pixel is viewed as a vertex. To construct a similarity graph, we map each pixel to a point in $\mathbb{R}^5$, i.e., $(x, y, r, g, b)$, where the latter three coordinates are the RGB values. For any two vertices $u, v$, we  put an edge between $u, v$ with weight $w(u,v) = \exp\{-\|u-v\|_2^2/\sigma^2\}$ with $\sigma = 0.5$ if one of $u, v$ is among the $5000$-nearest points of the other. This results in a graph with about $70,000,000$ edges.
\end{itemize}
\vspace{-1mm}
In the distributed model edges are randomly partitioned across $s$ sites. 

%\vspace{-1.5mm}



\subsection{Results on clustering quality}
%{\em Quality.} \
\begin{figure*}[ht]
     \centering
     \subfigure[\baseline]{\includegraphics[width=0.2\textwidth]{twomoons-14000-original-clustered.png}\label{fig:twomoons-clustered-original}}
     \subfigure[\MM]{\includegraphics[width=0.2\textwidth]{twomoons-14000-sparsify-clustered-15.png}\label{fig:twomoons-clustered-sparsify}}
     \subfigure[\blackboard]{\includegraphics[width=0.2\textwidth]{twomoons-14000-chain-clustered.png}\label{fig:twomoons-clustered-chain}}
     \caption*{\twomoons, $k = 2$;}

\subfigure[\baseline]{\includegraphics[width=0.2\textwidth]{gauss-10000-original-clustered.png}\label{fig:gauss-clustered-original}}
     \subfigure[\MM]{\includegraphics[width=0.2\textwidth]{gauss-10000-sparsify-clustered-15.png}\label{fig:gauss-clustered-sparsify}}
     \subfigure[\blackboard]{\includegraphics[width=0.2\textwidth]{gauss-10000-chain-clustered.png}\label{fig:gauss-clustered-chain}}
     \caption*{\gauss, $k = 4$}


     \subfigure[\baseline]{\includegraphics[width=0.2\textwidth,height=0.2\textwidth]{sculpture-11680-original-clustered.png}\label{fig:sculpture-clustered-original}}  
     \subfigure[\MM]{\includegraphics[width=0.2\textwidth,height=0.2\textwidth]{sculpture-11680-sparsify-clustered-15.png}\label{fig:sculpture-clustered-sparsify}}
     \subfigure[\blackboard]{\includegraphics[width=0.2\textwidth,height=0.2\textwidth]{sculpture-11680-chain-clustered.png}\label{fig:sculpture-clustered-chain}}
     \caption*{\sculpture, $k = 3$. }


     
     \caption{Visualization of the results on \twomoons, \gauss\ and \sculpture. In the message passing model each site samples $5 n$ edges; in the blackboard model all sites jointly sample $10n$ edges (in \twomoons~ and \gauss) or $20n$ edges (in \sculpture) and the chain has length $18$. $s = 15$.}
     \label{fig:quality-1}
\end{figure*}

We visualize the clustered results for 
the \twomoons, \gauss\ and \sculpture\ in Figure~\ref{fig:quality-1}.
% and visualize the clustered results for \gauss\ and \sculpture in Figure~\ref{fig:quality-2}.
It can be seen that \baseline, \MM\ and \blackboard\ give results of very similar qualities.  For simplicity, here we only present the visualization for $s=15$. Similar results were observed when we varied the values of $s$.  
%\he{To Qin: Do you plan to have two titles (Results \& Quality)?}


% \begin{figure*}[h]
%      \centering
% \subfigure[\baseline]{\includegraphics[width=0.3\textwidth]{gauss-10000-original-clustered.png}\label{fig:gauss-clustered-original}}
%      \subfigure[\MM]{\includegraphics[width=0.3\textwidth]{gauss-10000-sparsify-clustered-15.png}\label{fig:gauss-clustered-sparsify}}
%      \subfigure[\blackboard]{\includegraphics[width=0.3\textwidth]{gauss-10000-chain-clustered.png}\label{fig:gauss-clustered-chain}}
%      \caption*{\gauss, $k = 4$}


%      \subfigure[\baseline]{\includegraphics[width=0.2\textwidth]{sculpture-11680-original-clustered.png}\label{fig:sculpture-clustered-original}}  
%      \subfigure[\MM]{\includegraphics[width=0.2\textwidth]{sculpture-11680-sparsify-clustered-15.png}\label{fig:sculpture-clustered-sparsify}}
%      \subfigure[\blackboard]{\includegraphics[width=0.2\textwidth]{sculpture-11680-chain-clustered.png}\label{fig:sculpture-clustered-chain}}
%      \caption*{\sculpture, $k = 3$. }

%      \caption{Visualization of results on \gauss\ and \sculpture; in the message passing model each site samples $5 n$ edges; in the blackboard model all sites jointly sample $10n$ (in \gauss) or $20n$ (in \sculpture) edges and the chain has length $18$.}
%      \label{fig:quality-2}
% \end{figure*}


We also compare the normalized cut (ncut) values of the clustering results of different algorithms.  The results are presented in Figure \ref{fig:quality}. In all datasets, the ncut values of different algorithms are very close. The ncut value of \MM\ slightly decreases when we increase the value of $s$, while the ncut value of \blackboard\ is independent of $s$.
%We comment that in general, it is difficult to compare \MM\ and \blackboard\ directly because they are affected by different parameters.


\begin{figure*}[!ht]
  \centering
  \subfigure[\twomoons]{\includegraphics[width=0.33\textwidth]{twomoons-14000-ncut.png}\label{fig:twomoons-quality}}\hspace*{-1.1em}
  \subfigure[\gauss]{\includegraphics[width=0.31\textwidth]{gauss-10000-ncut.png}\label{fig:gauss-quality}}\hspace*{-1.1em}
  \subfigure[\sculpture]{\includegraphics[width=0.31\textwidth]{sculpture-11680-ncut.png}\label{fig:sculpture-quality}}\hspace*{-1.1em}
  \subfigure{\includegraphics[width=0.14\textwidth]{legend.png}}
     \caption{Comparisons on normalized cuts. In the message passing model, each site samples $5n$ edges; in each round of the algorithm in the blackboard model, all sites jointly sample $10n$ edges (in \twomoons~and \gauss) or $20n$ edges (in \sculpture) edges and the chain has length $18$.}
     \label{fig:quality}
\end{figure*}

%\textcolor{red}{To Jiecao: Can you put the color lines indicating baseline, message passing, and blackboard within one row in Pic 2? Withthis we can save some space.}

%\vspace{-1.5mm}

\subsection{Results on communication costs} 
\begin{figure*}[!ht]
     \centering
     \subfigure[\twomoons]{\includegraphics[width=0.3\textwidth]{twomoons-14000-communication.png}\label{fig:twomoons-communication}}
     \subfigure[\gauss]{\includegraphics[width=0.3\textwidth]{gauss-10000-communication.png}\label{fig:gauss-communication}}
     \subfigure[\sculpture]{\includegraphics[width=0.3\textwidth]{sculpture-11680-communication.png}\label{fig:sculpture-communication}}


     \subfigure[\twomoons]{\includegraphics[width=0.32\textwidth]{twomoons-14000-communication-2.png}\label{fig:twomoons-communication-2}}
     \subfigure[\gauss]{\includegraphics[width=0.32\textwidth]{gauss-10000-communication-2.png}\label{fig:gauss-communication-2}}
     \subfigure[\sculpture]{\includegraphics[width=0.32\textwidth]{sculpture-11680-communication-2.png}\label{fig:sculpture-communication-2}}
     \caption{Comparisons on communication costs. In the message passing model, each site samples $5n$ edges; in each round of the algorithm in the blackboard model, all sites jointly sample $10n$ (in \twomoons~and \gauss) or $20n$ (in \sculpture) edges and the chain has length $18$. }
     \label{fig:communication}
\end{figure*}

We compare the communication costs of different algorithms in Figure \ref{fig:communication}. We observe that while achieving similar clustering qualities as \baseline, both \MM\ and \blackboard\ are significantly more communication-efficient (by one or two orders of magnitudes in our experiments). We also notice that the value of $s$ does not affect the communication cost of \blackboard, while the communication cost of \MM\ grows almost linearly with $s$; when $s$ is large, \MM\ uses significantly more communication than \blackboard. These confirm our theory.  %In Figure~\ref{fig:mm-const} and Figure~\ref{fig:blackboard-const}   in Appendix~\ref{sec:parameters} we present how the performance of \MM\ and \blackboard\ are affected by their parameters.

%
%
%\vspace{-1.5mm}
%\paragraph{Summary.}  From our experimental results we conclude that \MM\ and \blackboard\ achieve similar clustering quality as the native algorithm \baseline, while significantly reduce the communication cost.  When the number of sites is large, \blackboard\ is more communication efficient than \MM, as predicted by our theory.



\subsection{Parameters in \MM\ and \blackboard}
\label{sec:parameters}

Figure \ref{fig:mm-const} shows in \MM how the value of ncut is affected by the number of sites and the number of edges sampled in each site. 
Here, each site samples $cn$ edges. 
When $c=3$ and $s=1$, the ncut value diverges in all datasets. This is because with such a small $c$, the algorithm does not generate a valid sparsifier. In general, increasing $c$ or $s$ will slightly decrease the ncut value. But once they are above some thresholds, the ncut values of \MM\ and \baseline\ become very close.

Figure \ref{fig:blackboard-const} shows in \blackboard  how the ncut value is affected by the number of iterations and the number of edges sampled. When the number of iterations is set to be $5$, ncut values diverge in all datasets. This is because we cannot expect to generate a valid sparsifier by using such few iterations. It can be seen from \ref{fig:bb-gauss-constant} that for a fixed $c$, performing more iterations will help to reduce ncut values. From the same figure, one can also conclude that for fixed iterations, increasing $c$ also helps to reduce the ncut values.



\begin{figure*}[h!t]
     \centering
     \subfigure[\twomoons]{\includegraphics[width=0.3\textwidth]{twomoons-c.png}\label{fig:mm-twomoons-constant}}
     \subfigure[\gauss~dataset]{\includegraphics[width=0.3\textwidth]{gauss-c.png}\label{fig:mm-gauss-constant}}
     \subfigure[\sculpture]{\includegraphics[width=0.3\textwidth]{sculpture-c.png}\label{fig:mm-sculpture-constant}}
     \caption{The pictures above show the $\ncut$ values with respect to the values of $c$ and $s$ for the \MM\ algorithm. Here  
 each site samples $c n$ edges.}
     \label{fig:mm-const}
\end{figure*}


\begin{figure*}[h!t]
     \centering
     \subfigure[\twomoons]{\includegraphics[width=0.3\textwidth]{twomoons-iter.png}\label{fig:bb-twomoons-constant}}
     \subfigure[\gauss]{\includegraphics[width=0.3\textwidth]{gauss-iter.png}\label{fig:bb-gauss-constant}}
     \subfigure[\sculpture]{\includegraphics[width=0.3\textwidth]{sculpture-iter.png}\label{fig:bb-sculpture-constant}}
     \caption{The pictures above show how the $\ncut$ values are affected by the number of iterations and the value of $c$ for the \blackboard\ algorithm. Here 
all sites jointly sample $c n$ edges. }
     \label{fig:blackboard-const}
\end{figure*}





