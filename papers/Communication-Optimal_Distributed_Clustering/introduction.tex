
\section{Introduction}\label{sec:intro}

Clustering is a fundamental task in machine learning with widespread applications in data mining, computer vision, and social network analysis. Example applications of clustering include grouping similar webpages by search engines,  finding users with common interests in a social network, and identifying different objects in a picture or video. For these applications, one can model the objects that need to be clustered as points in Euclidean space $\mathbb{R}^d$, where the similarities of two objects are represented by the Euclidean distance between the two points. 
Then the task of clustering is to choose $k$ points as centers, so that the total distance between all input points to their corresponding closest center is minimized. Depending on different distance objective functions, three typical problems have been studied: $k$-means, $k$-median, and $k$-center.

The other popular approach for clustering is to model the input data as vertices of a graph, and the similarity between two objects is represented by the weight of the edge connecting the corresponding vertices. For this scenario, one is asked to partition the vertices into clusters so that the ``highly connected'' vertices belong to the same cluster. A widely-used approach for graph clustering  is \emph{spectral clustering}, which embeds the vertices of a graph into the points in $\mathbb{R}^k$ through the bottom $k$ eigenvectors of the graph's Laplacian matrix, and applies $k$-means on the embedded points.

%For instance, one typical scenario for clustering is to group the points in the geometric space. Here, one is given $n$ points in the Euclidean space, and asked to choose $k$ points so that the total distance between every input point to its closest center is minimized. Depending on different distance functions, one can define the $k$-means problem, $k$-median problem, and the $k$-center problem.

%Another typical example for clustering is to group the vertices in a graph, where the edge wight between any pair of vertices represents the similarity of two vertices, and one is asked to partition the vertices so that highly-connected vertices belong to the same cluster. A widely used algorithm for graph clustering in practice is the spectral clustering: the algorithm embeds vertices to points in a high-dimensional Euclidean space through the bottom eigenvectors of the Laplacian matrix associated with the graph, and applies $k$-means algorithm on the embedded points.

Both the spectral clustering and the geometric clustering algorithms mentioned above have been widely used in practice, and have been the subject of extensive theoretical and experimental studies over the decades. However, these algorithms are designed for the centralized setting, and are not applicable in the setting of large-scale datasets that are maintained remotely by different sites.
In particular, collecting the information from all the remote sites and performing a centralized clustering algorithm is infeasible due to high communication costs, and new distributed clustering algorithms with low communication cost need to be developed.

There are several natural communication models, and we focus on two of them: (1) a point-to-point model, and (2) a model with a broadcast channel. In the former, sometimes referred to as the {\it message-passing model}, there is a communication channel between each pair of users. This may be impractical, and the so-called {\it coordinator model} can often be used in place; in the coordinator model there is a centralized site called the coordinator, and all communication goes through the coordinator. This affects the total communication by a factor of two, since the coordinator can forward a message from one server to another and therefore simulate a point-to-point protocol. There is also an additional additive $O(\log s)$ bits per message, where $s$ is the number of sites, since a server must specify to the coordinator where to forward its message. In the model with a broadcast channel, sometimes referred to as the {\it blackboard model}, the coordinator has the power to send a single message which is received by all $s$ sites at once. This
can be viewed as a model for single-hop wireless networks.

In both models we study the total number of bits
communicated among all sites. Although the blackboard model is at least as powerful as the
message-passing model, it is often unclear how to exploit its power to obtain better bounds for specific problems.
Also, for a number of problems the communication complexity is the same in both models, such as computing
the sum of $s$ length-$n$ bit vectors modulo two, where each site holds one bit vector \cite{pvz16}, or estimating large moments \cite{wz12}. Still,
for other problems like set disjointness it can save a factor of $s$ in the communication
\cite{beopv13}.
%Understanding for which problems the broadcast channel can help is an important goal.  
%Both the spectral algorithms and algorithms for clustering points in the Euclidean space are centralized, and  require the global knowledge of the dataset. However, as most of the nowadays' big datasets are maintained in a distributed way by multiple sites, it is unrealistic for one site to gather the information of the whole datasets and perform a centralized algorithm. Hence, we need to develop distributed clustering algorithms, for which the overall communication cost among all sites needs to be considered. 

\subsection{Our contributions}  
%We study both graph clustering and geometric clustering problems in the two distributed models above. 
%
We present algorithms for graph clustering: for any $n$-vertex graph whose edges are arbitrarily partitioned across $s$ sites, our algorithms have communication cost $\tilde{O}(ns)$ in the message passing model, and have communication cost $\tilde{O}(n+s)$ in the blackboard model, where the $\tilde{O}$ notation suppresses polylogarithmic factors. The algorithm in the message passing model has each site send a {\it spectral sparsifier} of its local data to the coordinator, who then merges them in order to obtain a spectral sparsifier of the union of the datasets, which is sufficient for solving the graph clustering problem. Our algorithm in the blackboard model is technically more involved, as we show a particular recursive sampling procedure for building a spectral sparsifier can be efficiently implemented using a broadcast channel. It is unclear if other natural ways of building spectral sparsifiers can be implemented with low communication in the blackboard model. Our algorithms demonstrate the surprising power of the blackboard model for clustering problems. Since our algorithms compute spectral sparsifiers, they also have applications to solving symmetric diagonally dominant linear systems in a distributed model. Any such system can be converted into a system involving a Laplacian (see, e.g., \cite{ACKQWZ16}), from which a spectral sparsifier serves as a good preconditioner. 

Next we show that $\Omega(ns)$ bits of communication is necessary in the message passing model to even recover a constant fraction of a cluster, and $\Omega(n+s)$ bits of communication is necessary in the blackboard model. This shows the optimality of our algorithms up to poly-logarithmic factors. 

We then study clustering problems in constant-dimensional Euclidean space. We show for any $c>1$, computing a $c$-approximation for $k$-median, $k$-means, or $k$-center correctly with constant probability in the message passing model requires $\Omega(sk)$ bits of communication. We then strengthen this lower bound, and show even for {\it bicriteria} clustering algorithms, which may output a constant factor more clusters and a constant factor approximation, our $\Omega(sk)$ bit lower bound still holds. Our proofs are based on
communication and information complexity. 
Our results imply that existing algorithms~\cite{BEL13} for $k$-median and $k$-means with $\tilde{O}(sk)$ bits of communication, as well as the folklore parallel guessing algorithm for $k$-center with $\tilde{O}(sk)$ bits of communication, are optimal up to poly-logarithmic factors. For the blackboard model, we present an algorithm for $k$-median and $k$-means that achieves an $O(1)$-approximation using $\tilde{O}(s+k)$ bits of communication. This again separates the models.  


We give empirical results which show that using spectral sparsifiers preserves the quality of spectral clustering surprisingly well in real-world datasets.
%In the message passing model and the blackboard model, we show that our algorithms constructing spectral sparsifiers save a huge amount of communication while achieving similar results compared to a centralized algorithm.
For example, when we partition a graph with over $70$ million edges (the {\tt Sculpture} dataset) into $30$ sites, only $6\%$ of the input edges are communicated in the blackboard model and $8\%$ are communicated in the message passing model, while the values of the normalized cut (the objective function of spectral clustering) given in those two models are at most $2\%$ larger than the ones given by the centralized algorithm, and the visualized results are almost identical. This is strong evidence that spectral sparsifiers can be a powerful tool in practical, distributed computation. When the number of sites is large, the blackboard model incurs significantly less communication than the message passing model, e.g., in the {\tt Twomoons} dataset when there are $90$ sites, the message passing model communicates $9$ times as many edges as communicated in the blackboard model, illustrating the strong separation between these models that our theory predicts.


%We give empirical results which show that using spectral sparsifiers preserves the quality of spectral clustering surprisingly well in real-world datasets. In the message passing model and the blackboard model, we show that our algorithms constructing spectral sparsifiers save a huge amount of communication while achieving similar results compared to a centralized algorithm. This is strong evidence that spectral sparsifiers can be a powerful tool in practical, distributed computation. When the number of sites is large, the blackboard model incurs significantly less communication than the message passing model, illustrating the strong separation between these models that our theory predicts. 
%Our experiments also show how algorithms in those models can be affected by different parameters.

\subsection{Related work}  There is a rich literature on spectral and geometric clustering algorithms from various aspects~(see, e.g.,~\cite{k-means++,nips02,PSZ15,luxburg07}). Balcan et al.~\cite{BEL13,BKLW14} and Feldman et al. \cite{FSS13} study distributed $k$-means (\cite{BEL13} also studies $k$-median), and present provable guarantees on the clustering quality. Very recently Guha et al. \cite{GLZ17} studied distributed $k$-median/center/means with outliers.  %Lammersen et al.~\cite{journals/mst/Lammersen0S15} study probabilistic geometric clustering problem in data streams.
Cohen et al.~\cite{journals/corr/CohenEMMP14} study dimensionality reduction techniques for the input data matrices that can be used for distributed $k$-means. The main takeaway  is that there is no previous work which develops protocols for spectral clustering in the common message passing and blackboard models, and lower bounds are lacking as well. For geometric clustering, while upper bounds exist (e.g.,~\cite{BEL13,BKLW14,FSS13}), no provable lower bounds in either model existed, and our main contribution is to show that previous algorithms are optimal. We also develop a new protocol in the blackboard model.  
%
%
%\textcolor{red}{Most of these previous studies only considered the clustering problem in the non-distributed setting, while a few reference~(e.g.,~\cite{BEL13}) presented distributed geometric clustering algorithms but did not optimize the communication complexity.}
%
%
%
%Czumaj and Sohler~\cite{journals/mst/CzumajS10} studied sublinear-time approximation algorithms for the geometric clustering problem in metric spaces. 


%\textcolor{red}{add some related works. also add all the work by Balcan, Sohler, Cohen, Musco, etc.}

