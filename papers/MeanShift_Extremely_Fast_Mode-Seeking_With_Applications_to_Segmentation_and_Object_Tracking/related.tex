\section{Related Works}

Since MeanShift is a very popular procedure, there have been a number of approaches to speed up the algorithm and other mode-seeking based clustering algorithms in general. 

Yang et al. (2003) \cite{yang2003improved} propose a speedup of MeanShift by using a fast gauss transform to efficiently compute the kernel density estimator, reducing the computational complexity down to linear per iteration; however, they found the fast gauss transform to be impractical for any dimension higher than $3$. Yang et al. (2005) \cite{yang2005efficient} then applies this technique to a modified similarity function specifically for color histograms and show its effectiveness on frame-tracking in image sequences. Elgamma (2003) \cite{elgammal2003efficient} also leverage fast gauss transform for color modeling and tracking.

Then there are other computer vision application specific speedup methods for MeanShift. Yin et al. (2011) \cite{yin2011fast} leverage frame-differences to speed up MeanShift in the specific application of target tracking. Carreira-Perpinan (2006) \cite{carreira2006acceleration} shows that a spatial discretization strategy can accelerate Gaussian MeanShift image segmentation by one to two orders of magnitude while attaining almost the same segmentation. Carreira-Perpinan \cite{carreira2006fast} also provides similar results for general Gaussian MeanShift; however, in this paper, we show that our method can achieve a far better improvement compared to MeanShift.

Another set of approaches leverage space-partitioning data structures in order to speed up the density estimation calculations. Wang et al. (2007) \cite{wang2007fast} propose using a dual-tree to obtain a faster approximation of MeanShift with provable accuracy guarantees. Xiao et al. (2010) \cite{xiao2010efficient} propose a heuristic to make the computations more efficient by approximating MeanShift using a greatly reduced feature space via applying an adaptive Gaussian KD-Tree.

Freedman et al. (2009)  \cite{freedman2009fast} propose speeding up MeanShift by randomly sampling data points when computing the kernel density estimates. This approach only reduces the runtime by small orders and does not address the underlying quadratic runtime issue unless only a small number of samples are used, but this leads to high error in the density estimates. Our method is both linear runtime and utilizes all of the data points to construct an optimal density estimator.

Vedaldi and Soatto (2008) \cite{vedaldi2008quick} propose a procedure called QuickShift, which is modification of MeanShift in which the trajectories of the points are restricted to the original examples. The same procedure was proposed later by Rodriguez and Laio (2014) \cite{rodriguez2014clustering}. The procedure comes with theoretical guarantees \cite{jiang2017consistency}, and GPU-based speedups have been proposed for the algorithm \cite{fulkerson2010really}. We will show later in the experimental results that QuickShift is indeed much faster than MeanShift, but MeanShift++ is still orders of magnitude faster than QuickShift.