\section{Theory}
In this section, we give guarantees on our grid-based approach. Suppose there is some underlying distribution $\mathcal{P}$ with corresponding density function $p : \mathbb{R}^d \rightarrow \mathbb{R}_{\ge 0}$ from which our data points $X_{[n]} = \{x_1,...,x_n\}$ are drawn i.i.d. We show guarantees on the density estimator based on the grid cell counts.

We need the following regularity assumptions on the density function. The first ensures that the density function has compact support with smooth boundaries and is lower bounded by some positive quantity, and the other ensures that the density function has smoothness. These are standard assumptions in analyses on density estimation e.g. \cite{gine2002rates,jiang2017uniform,chen2017tutorial,singh2009adaptive}.
\begin{assumption}\label{assumption1}
$p$ has compact support $\mathcal{X} \in \mathbb{R}^d$ and there exists $\lambda_0, r_0, C_0 > 0$ such that $p(x) \ge \lambda_0$ for all $x \in \mathcal{X}$ and $\text{Vol}(B(x, r) \cap \mathcal{X}) \ge C_0 \cdot \text{Vol}(B(x, r))$ for all $x \in \mathcal{X}$ and $0 < r \le r_0$, where $B(x, r) := \{x' \in \mathbb{R}^d: |x-x'| \le r\}$.
\end{assumption}
\begin{assumption}\label{assumption2}
$p$ is $\alpha$-HÃ¶lder continuous for some $0 < \alpha \le 1$: i.e. there exists $C_\alpha > 0$ such that $|p(x) - p(x')| \le C_\alpha \cdot |x - x'|^\alpha$ for all $x, x' \in \mathbb{R}^d$.
\end{assumption}

We now give the result, which says that for $h$ sufficiently small depending on $p$ (if $h$ is too large, then the grid is too coarse to learn a statistically consistent density estimator), and $n$ sufficiently large, there will be a high probability finite-sample uniform bound on the difference between the density estimator and the true density. The proof can be found in the Appendix.
\begin{theorem}\label{theorem}
Suppose Assumption~\ref{assumption1} and~\ref{assumption2} hold. Then there exists constants $C, C_{1} > 0$ depending on $p$ such that the following holds.
Let $0 < \delta < 1$, $0 < h < \text{min}\{\left(\frac{\lambda_0}{2\cdot C_\alpha}\right)^{1/\alpha}, r_0\}$, $nh^d \ge C_1$. Let $\mathcal{G}_h$ be a partitioning of $\mathbb{R}^d$ into grid cells of edge-length $h$ and for $x \in \mathbb{R}^d$. Let $G(x)$ denote the cell in $\mathcal{G}_h$ that $x$ belongs to.  Then, define the corresponding density estimator $\widehat{p}_h$ as:
\begin{align*}
    \widehat{p}_h(x) := \frac{|X_{[n]} \cap G(x)|}{n\cdot h^d}.
\end{align*}
Then, with probability at least $1 - \delta$:
\begin{align*}
    \sup_{x \in \mathbb{R}^d} |\widehat{p}_h(x)  - p(x)| \le C\cdot \left( h^\alpha + \frac{\sqrt{\log(1/(h\delta)}}{\sqrt{n\cdot h^d}} \right).
\end{align*}
\end{theorem}


\begin{remark}
In the above result, choosing $h \approx n^{-1/(2\alpha+d)}$ optimizes the convergence rate to $\tilde{O}(n^{-\alpha/(2\alpha+d)})$, which is the minimax optimal convergence up to logarithmic factors for the density estimation problem as established by Tsybakov \cite{tsybakov1997nonparametric,tsybakov2008introduction}.
\end{remark}
In other words, the grid-based approach statistically performs at least as well as any estimator of the density function, including the density estimator used by MeanShift. It is worth noting that while our results only provide results for the density estimation portion of MeanShift++ (i.e. the grid-cell binning technique), we prove the near-minimax optimality of this estimation. This implies that the information contained in the density estimation portion serves as an approximately sufficient statistic for the rest of the procedure, which behaves similarly to MeanShift, which operates on another, also nearly-optimal density estimator. Thus, existing analyses of MeanShift e.g. \cite{arias2016estimation,chen2015convergence,xiang2005convergence,li2007note,ghassabeh2015sufficient,ghassabeh2013convergence,subbarao2009nonlinear} can be adapted here; however, it is known that MeanShift is very difficult to analyze \cite{dasgupta2014optimal} and a complete analysis is beyond the scope of this paper.
