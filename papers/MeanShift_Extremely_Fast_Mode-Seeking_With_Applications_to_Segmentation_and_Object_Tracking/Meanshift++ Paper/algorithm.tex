\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\section{Algorithm}



We first introduce MeanShift in Algorithm~\ref{alg:meanshift} to cluster data points $X_{[n]} := \{x_1,..,x_n\}$. The most popular version uses a unit flat kernel (i.e. $K(x) := 1[\lVert x\rVert \le 1]$ \cite{cheng1995mean}), but other kernels can be used as well including the Gaussian kernel \cite{carreira2007gaussian}. At each iteration, it moves points to its kernel-weighted mean w.r.t. the last iteration's points until convergence. This computation costs $O(n^2\cdot d)$ time per iteration, even if space-partitioning data structures are used to speed up the search to find the $h$-radius neighborhood in the flat kernel case \cite{xiao2010efficient}.

\begin{algorithm}[H]
\caption{MeanShift \cite{cheng1995mean,comaniciu1999mean,fukunaga1975estimation}}
\label{alg:meanshift}
\begin{algorithmic}[H]
  \State Inputs: bandwidth $h$, tolerance $\eta$, kernel $K$, $X_{[n]}$.
  \State Initialize $y_{0, i} := x_i$ for $i \in [n]$, $t = 1$.
   \Do
  \State For $i \in [n]$: \begin{align*}
      y_{t,i} \leftarrow \frac{\sum_{j \in [n]} K\left(\frac{\lVert y_{t-1, i} - y_{t-1, j} \rVert}{h}\right) y_{t-1, j} }{\sum_{j \in [n]} K\left(\frac{\lVert y_{t-1, i} - y_{t-1, j} \rVert}{h}\right) }.
  \end{align*} 
  \State $t \leftarrow t + 1$.
   \doWhile{$\sum_{i=1}^n \lVert y_{t, i} - y_{t-1, i} \rVert \ge \eta$.}
   \State \Return $\{y_{t,1},...,y_{t, n}\}$.
\end{algorithmic}
\end{algorithm}

We now introduce MeanShift++ (Algorithm~\ref{alg:meanshiftpp}). In each iteration, it first performs a preprocessing step of parititioning the input data points into appropriate grid cells, which are hypercubes of side length $h$. A graphical comparison with MeanShift is shown in Figure~\ref{fig:example}. In practice, we keep two hash tables $\mathcal{C}$ (to store the count in each cell) and $\mathcal{S}$ (to store the sum in each cell). $\mathcal{C}$ and $\mathcal{S}$ are defined as initially empty mappings from lattice points in $D$-dimension (corresponding to grid cells) to non-negative integers and $\mathbb{R}^D$ respectively. We show how they are updated via a single pass through the dataset. 

To compute which grid cell an example belongs to, we first divide each entry by $h$ and then take the element-wise floor function, which gives a $d$-dimensional integer vector index of the grids. Then, we can simply use the preprocessed information in the point's grid cell and neighboring grid cells to compute the shifted point. As a result, each point is moved to the average of all the points within its cell and neighboring cells. This grid-based approach is a much faster approximation of the density at low dimensions. The runtime of MeanShift++ grows linearly with $n$, while MeanShift is quadratic (Figure~\ref{fig:gaussians}).

\begin{algorithm}[H]
\caption{MeanShift++}
\label{alg:meanshiftpp}
\begin{algorithmic}[H]
  \State Inputs: bandwidth $h$, tolerance $\eta$, $X_{[n]}$.
  \State Initialize $y_{0, i} := x_i$ for $i \in [n]$, $t = 1$.
   \Do
  \State Initialize empty hash tables $\mathcal{C}: \mathcal{Z}^d \rightarrow \mathcal{Z}_{\ge 0}$ (stores cell count), $\mathcal{S}: \mathcal{Z}^d \rightarrow \mathbb{R}^d$ (stores cell sum).
  \State $\mathcal{C}(\lfloor y_{t-1, i} / h \rfloor) \leftarrow \mathcal{C}(\lfloor y_{t-1, i} / h \rfloor) + 1$ for $i \in [n]$.
  \State $\mathcal{S}(\lfloor y_{t-1, i} / h \rfloor) \leftarrow \mathcal{S}(\lfloor y_{t-1, i} / h \rfloor) + y_{t-1, i}$ for $i \in [n]$.
  \State Next, for all $i \in [n]$: \begin{align*}
      y_{t,i} \leftarrow \frac{\sum_{v \in \{-1, 0, 1\}^d} \mathcal{S}(\lfloor y_{t-1, i} / h \rfloor + v)  }{\sum_{v \in \{-1, 0, 1\}^d} \mathcal{C}(\lfloor y_{t-1, i} / h \rfloor + v) }.
  \end{align*} 
  \State $t \leftarrow t + 1$.
   \doWhile{$\sum_{i=1}^n \lVert y_{t, i} - y_{t-1, i} \rVert \ge \eta$.}
   \State \Return $\{y_{t,1},...,y_{t, n}\}$.
\end{algorithmic}
\end{algorithm}


\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{example.png}
\end{center}
   \caption{\label{fig:example}\textit{2D example illustrating the difference between MeanShift and MeanShift++.} The red circle is the point we want to shift to the mean of its neighbors. It takes $O(n)$ for MeanShift to find the neighbors of a single point versus $O(3^d)$ for MeanShift++ using grid cells. The location of the new point is indicated by the red triangle.}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{gaussians.png}
\end{center}
   \caption{\label{fig:gaussians}\textit{Comparison of MeanShift++ and MeanShift on 2D mixtures of Gaussians.} The runtime of MeanShift is quadratic in the number of data points and quickly becomes infeasible to run, whereas the runtime of MeanShift++ grows linearly with $n$ (shown here in log space). The clustering performances are comparable.}
\end{figure}