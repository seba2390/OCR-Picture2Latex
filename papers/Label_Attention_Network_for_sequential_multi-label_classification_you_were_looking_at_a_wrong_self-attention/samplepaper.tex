% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage[hidelinks]{hyperref}
\newcommand{\vecE}{\mathbf{e}}
\newcommand{\vecX}{\mathbf{x}}
\newcommand{\vecY}{\mathbf{y}}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
% %
% \title{Contribution Title\thanks{Supported by organization x.}}
% %
% %\titlerunning{Abbreviated paper title}
% % If the paper title is too long for the running head, you can set
% % an abbreviated paper title here
% %
% \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
% %
% \authorrunning{F. Author et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
% %
% \maketitle              % typeset the header of the contribution
% %
% \begin{abstract}
% The abstract should briefly summarize the contents of the paper in
% 15--250 words.

% \keywords{First keyword  \and Second keyword \and Another keyword.}
% \end{abstract}
% %
% %
% %
% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
% %
% % \bibliographystyle{splncs04}
% % \bibliography{mybibliography}
% %
% %
\title{Label Attention Network for sequential multi-label classification: you were looking at a wrong self-attention}
%\thanks{This work was supported in part by the Russian Foundation for Basic Research under Grant 21-51-12005 NNIO\_a, and in part by the
% German Research Foundation (DFG) under Project 448795504.}
%
\titlerunning{Label Attention Network for sequential multi-label classification}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

% \author{Anonymous}

\author{Elizaveta Kovtun$^*$\inst{1} \and
Galina Boeva$^*$\inst{1} \and
Artem Zabolotnyi\inst{1} \and
Evgeny Burnaev\inst{1, 2} \and 
Martin Spindler\inst{3} \and
Alexey Zaytsev\inst{1}}
%
\def\thefootnote{*}\footnotetext{Equal contribution}

\authorrunning{E. Kovtun et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Skoltech, Moscow, Russia \and 
AIRI, Moscow, Russia \and
University of Hamburg, Hamburg, Germany}
\maketitle              % typeset the header of the contribution

\begin{abstract}
Most of the available user information can be represented as a sequence of timestamped events. Each event is assigned a set of categorical labels whose future structure is of great interest. For instance, our goal is to predict a group of items in the next customer's purchase or tomorrow's client transactions. This is a multi-label classification problem for sequential data. Modern approaches focus on transformer architecture for sequential data introducing self-attention for the elements in a sequence. In that case, we take into account events' time interactions but lose information on label inter-dependencies. Motivated by this shortcoming, we propose leveraging a self-attention mechanism over labels preceding the predicted step. As our approach is a Label-Attention NETwork, we call it LANET. Experimental evidence suggests that LANET outperforms the established models' performance and greatly captures interconnections between labels. For example, the micro-AUC of our approach is 0.9536  compared to 0.7501 for a vanilla transformer. We provide an implementation of LANET to facilitate its wider usage.
\end{abstract}

\keywords{Multi-label classification, attention mechanism, sequential data}
 

% %%
% %% Keywords. The author(s) should pick words that accurately describe
% %% the work being presented. Separate the keywords with commas.
% \keywords{datasets, neural networks, gaze detection, text tagging}

% %% A "teaser" image appears between the author and affiliation
% %% information and the body of the document, and typically spans the
% %% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

% 1. Intro - Artem, rewrite the intro, Liza - review Artem and rewrite after him
% 2. Conclusions - Artem, I suppose. They should be in line with abstract and intro
% 3. Details on preprocessing on all datasets, not only Orders - add to the appendix. Galina, if possible. If not - Liza, 08.06
% 4. Remove focus on Orders data (materials instead of labels etc.) - Galina +
% 5. Metrics - Liza
% 6. Attention matrix - Liza TODO
% 7. Ablation study - Liza 08.06



% \author{\IEEEauthorblockN{1\textsuperscript{st} Elizaveta Kovtun*}
% \IEEEauthorblockA{\textit{RAIC} \\
% \textit{Skoltech}\\
% Moscow, Russia \\
% Elizaveta.Kovtun@skoltech.ru}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Galina Boeva*}
% \IEEEauthorblockA{\textit{RAIC} \\
% \textit{Skoltech}\\
% Moscow, Russia \\
% G.Boeva@skoltech.ru}
% \and
% \IEEEauthorblockN{3\textsuperscript{nd} Artem Zabolotnyi*}
% \IEEEauthorblockA{\textit{RAIC} \\
% \textit{Skoltech}\\
% Moscow, Russia \\
% Artem.Zabolotnyi@skoltech.ru}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Evgeny Burnaev}
% \IEEEauthorblockA{\textit{RAIC} \\
% \textit{Skoltech}\\
% Moscow, Russia \\
% e.burnaev@skoltech.ru}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Martin Spindler}
% \IEEEauthorblockA{\textit{Department of Business Administration} \\
% \textit{University of Hamburg}\\
% Hamburg, Germany \\
% martin.spindler@uni-hamburg.de}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Alexey Zaytsev}
% \IEEEauthorblockA{\textit{RAIC} \\
% \textit{Skoltech}\\
% Moscow, Russia \\
% a.zaytsev@skoltech.ru}
% }



\section{Introduction}

% multilabel classification for sequences 
The multi-label classification is a more natural setting than a binary or multiclass classification since everything that surrounds us in the real world is usually described with multiple labels~\cite{liu2021emerging}. The same logic can be transferred to the sequence of timestamped events. Events in a sequence are prone to be characterized by several categorical values instead of one mark. There are numerous approaches to deal with the multi-label classification in computer vision~\cite{durand2019learning}, natural language processing~\cite{xiao2019label}, or classic tabular data framework~\cite{tarekegn2021review}. However, the multi-label problem statement for event sequences tends to receive less attention. So, we aim to confront such a lack of focus and solve the labels' set prediction problem for sequential timestamped data. In short, it is a multi-label classification for sequential data. 

Importantly, the model should predict a set of labels that correspond to the next step by taking into account the content of previous label groups for a sequence of events related to an object.
The problem at hand is illustrated by an example in Figure~\ref{fig:multi_label_data}.

% \begin{table}[!h]
% \caption{
% A dataset sample with timestamped events that can be used within the multi-label classification task. Our model should \textcolor{blue}{predict labels for the $28$th of August}, given history of previous label sets. There are multiple labels to predict, so it is a multi-label classification problem.}
% \label{tab:multi_label_data}
% \centering
% \begin{tabular}{cccc}
% \hline
% & Date & Labels \\
% \hline
% & 12-08-2016 & [1, 18, 89] \\
% & 21-08-2016 & [3, 8] \\
% & 23-08-2016 & [1, 18] \\
% & 28-08-2016 & \textbf{\textcolor{blue}{[?, ?, ...]}} \\
% \hline
% \end{tabular}
% \end{table}

\begin{figure}[h!]
\begin{minipage}{0.47\linewidth}
\centering
\begin{tabular}{cccccc}
\hline
& ID & Date & Labels & Features\\
\hline
& 1 & 12-08-2016 & [1, 18, 89] & [2.1, 0.4, 0.7] \\
& & 21-08-2016 & [3, 8] & [0.3, 1.5] \\
& & 23-08-2016 & [1, 18] & [1.7, 0.5] \\
& & 28-08-2016 & \textbf{\textcolor{blue}{[?, ?, ...]}} & \\
\hline
\end{tabular}
\end{minipage}
\begin{minipage}{0.53\linewidth}
\centering
\label{fig:general_scheme}
\includegraphics[width=\textwidth]{images/general_scheme.pdf}
\end{minipage}
\caption{A dataset sample of timestamped events that can be used within the multi-label classification task and its visualization. ID refers to a considered client identifier, while features are additional characteristics of labels in a set. For example, if a set of labels are a group of transactions, then features represent the amounts of particular transactions. Our model should \textcolor{blue}{predict labels for the $28$th of August}, given the history of previous label sets. There are multiple labels to predict, so it is a multi-label classification problem. The right figure provides a visual representation of the problem in the left table.}
\label{fig:multi_label_data}
\end{figure}

% \begin{table}[!h]
% \caption{
% A dataset sample with timestamped events that can be used within the multi-label classification task. Our model should \textcolor{blue}{predict labels for the second ID for the $28$th of August}, given previous information related to this ID. There are multiple labels to predict, so it is a multi-label classification problem. We can use additional available information as input to the problem. For example, this information can include amounts associated with labels in past observations.}
% \label{tab:multi_label_data}
% \centering
% \begin{tabular}{cccccc}
% \hline
% & ID & Date & Labels & Amounts \\
% \hline
% & 1 & 01-05-2015 & [1, 15, 94] & [1.2, 0.7, 1.1] \\
% & - & 07-05-2015 & [55] & [0.5] \\
% & - & 11-05-2015 & [2] & [1.5] \\
% \hline
% & 2 & 12-08-2016 & [1, 18, 89] & [2.1, 0.4, 0.7] \\
% & - & 21-08-2016 & [3, 8] & [0.3, 1.5] \\
% & - & 23-08-2016 & [1, 18] & [1.7, 0.5] \\
% & - & 28-08-2016 & \textbf{\textcolor{blue}{[?, ?, ...]}} & \\
% \hline
% \end{tabular}
% \end{table}

The interaction between an object's states at different timestamps is essential for solving tasks with sequential data~\cite{hartvigsen2020recurrent}. Therefore, expressive and powerful models should be able to learn such interactions. Several neural network architectures, such as transformers or recurrent neural networks, are capable of doing this. For example, a transformer directly defines an attention mechanism that measures how different timestamps in a sequence are connected. However, the applications of modern deep learning methods are limited~\cite{zhang2020multi}, and they primarily focus on predicting labels for a sequence in general. 
We refer to the graph of connections between states of an object at different timestamps as \textit{a timestamp interaction graph}.

Another connection worth exploring is the connection between different labels and a need to consider the correlation between them~\cite{hang2021collaborative}. This capability is not inherent for the majority of models. With a large number of possible labels, this problem becomes close to a recommender system problem.
We name the graph of connections between different labels \textit{a label interaction graph}.

For single label-event data by using a position encoder, we take into account both interaction between labels and timestamped events~\cite{li2020time,ying2018sequential}.
For sequential multi-label prediction, simultaneous consideration of both \textit{timestamp interaction graph} and \textit{label interaction graph} can be crucial. 
Typically, articles explore a particular side of the dependence that can be explainable by domain bias. In sequential recommender systems, there is a focus on connections between labels~\cite{quadrana2018sequence} with the incorporation of convolutional neural networks~\cite{tang2018personalized} as well as the attention mechanism~\cite{zhang2019next}.
Direct models for event sequences~\cite{hawkes1971spectra} also prefer identification of interactions between timestamps~\cite{zhuzhel2023continuoustime}.
So, in time series and natural language processing, it is more common to handle \textit{a timestamp interaction graph} via an attention mechanism or recurrent neural networks. 

\begin{table}[th]
\centering
\small
\begin{tabular}{p{2cm}p{1.5cm}p{2.8cm}p{2.5cm}p{2.4cm}}
\hline
  & LSTM & TransformerBase & CLASS2C2AE & LANET (ours) \\
\hline
Micro-AUC & \underline{2} & 3.2 & 3.6 & \textbf{1.2} \\
Macro-AUC & \underline{2} & 3.2 & 3.6 & \textbf{1.2} \\
Micro-F1 & \underline{1.8} & 3.4 & 3.4 & \textbf{1.6} \\
Macro-F1 & \textbf{1.8} & 3.2 & 2.8 & \underline{2.2} \\
\hline
\end{tabular}
\centering
\caption{Mean rank for different metrics averaged over $5$ considered datasets. We want to minimize rank, as the best method has rank $1$. The best values are in \textbf{bold} and the second best values are \underline{underlined}.}
\label{table:rank_results}
\end{table}

Our LANET aims at recovering \textit{label interaction graphs}, as we believe it is more vital in many problems.
The algorithm aggregates past information in a specially constructed set of label representations.
The set serves as input to a transformer encoder. 
The transformer encoder updates label embeddings via self-attention, uncovering their interactions. 
Finally, we predict a set of labels for the current model based on the model's output that encompasses knowledge of label relationships. Moreover, we can process long sequences this way, as now the attention evaluation is quadratic in the number of labels, not the sequence length.
As our model does not use external information, it produces self-supervised embeddings.



\paragraph{\textbf{Contributions.}}
We have developed a transformer-based architecture with self-attention between labels to deal with multi-label classification for event sequences. Our main contributions are the following:
\begin{itemize}
    \item We introduce LANET architecture for the prediction of a label set for the current event taking the information from previous events. The architecture's peculiarity is a calculation of self-attention between label views. The scheme of our approach is presented in Figure~\ref{fig:transf}.
    \item LANET outperforms transformer-based models that focus on self-attention between timestamps. We evaluate all metrics on various datasets to prove the insufficiency of a common sequential placement-oriented perspective. See Table~\ref{table:rank_results} for a high-level comparison of different approaches. Furthermore, we provide an ablation study of the proposed architecture. 
    \item Due to LANET structure, it efficiently captures label interactions via the attention matrix. 
\end{itemize}

\begin{figure}[h!]
    \centering
    % \includegraphics[scale=0.65]{images/orders_paper_diagram.pdf}
    \includegraphics[scale=0.50]{images/OrderProcessing-LANET_ver2.drawio.pdf}
    \caption{LANET architecture for multi-label classification of event sequences with a total number of labels $K$. The model input comprises the concatenation of the object's label, time, feature, and ID embeddings. After self-attention layers, the model outputs a vector of confidence scores, where each element is associated with the probability of the presence of a particular label in the next event. The self-attention layer in LANET acts between labels, not temporal representations.}
    \label{fig:transf}
\end{figure}

% \begin{table}[b]
% \centering
% \small
% \begin{tabular}{lcccccc}
% \hline
% Approach  & \multicolumn{2}{c}{ROC---AUC} & \multicolumn{2}{c}{F1} \\
%           & micro & macro & micro & macro \\ 
% \hline
% LSTM & 3.2 & 3.4 & 2.8 & 3.2 \\
% TransformerBase & \underline{2.4} & \underline{2.2} & \underline{2.6} & \underline{2.2} \\
% CLASS2C2AE & 3.4 & 3.4 & 3 & 2.6 \\
% LANET (ours) & \textbf{1} & \textbf{1} & \textbf{1.6} & \textbf{2} \\
%     \hline
% \end{tabular}
% \centering
% \caption{Mean rank for different metrics average over $5$ considered datasets. We want to minimize rank, as the best methods has rank $1$ with respect to all metrics. The best values are in \textbf{bold}, the second best values are \underline{underlined}}
% \label{table:rank_results}
% \end{table}

\section{Related Work}
\label{sec:related}
% let start from links to recent reviews, than focus on specific papers related to our research/
% more order is better here, reviews of different articles make little sense. They should serve a general idea on what have be done and what is missing (so we should do our research)

The multi-label classification problem statement emerges in many diverse domains, e.g., text categorization or image tagging, all of which entail their own peculiarities and challenges. The review~\cite{zhang2013review} explores foundations in multi-label learning, discussing the well-established methods as well as the most recent approaches.
Emerging trends are covered in a more recent review~\cite{liu2021emerging}.


Firstly, we examine loss functions tailored for the multi-label setting and into some methods for composing label set prediction. 
Secondly, we overview the usage of RNNs in the multi-label classification task. Thirdly, we review how to capture the label dependencies.
Then, we discuss an association with a sequential recommendation problem.
Lastly, we summarise the literature analysis and present the identified research gap. 


% Lastly, we provide an overview of several articles that are closely related to the concepts discussed in our work and conclude with an existing research gap.

% TODO why did you comments anything else?
% TODO include the paper presented by Lisa

\paragraph{\textbf{Loss functions and ways for label set composition in multi-label problem.}}
The paper~\cite{menon2019multilabel} studies the theoretical background for main approaches to the reduction of multi-label classification problem to a series of binary or multi-class problems. In particular, they show that considered reductions implicitly optimize for either Precision@k or Recall@k. The choice of the correct reduction should be based on the ultimate performance measure of interest. In \cite{Li_2017_CVPR}, the authors propose an improved loss function for pairwise ranking in multi-label image classification task that is easier to optimize. Also, they discuss an approach based on the estimation of the optimal confidence thresholds for the label decision part of the model that determines which labels to include in the final prediction. The task of multi-label text classification is the topic of \cite{du2019ml}.
The authors construct an end-to-end deep learning framework called ML-Net. ML-Net consists of two parts: a label prediction network and a label count prediction network. In order to get the final set of labels, confidence scores generated from the label prediction network are ranked, and then the top $K_{top}$ labels are predicted. A separate label count network predicts $K_{top}$. 

\paragraph{\textbf{Neural networks for multi-label classification.}}
In~\cite{yazici2020orderless}, the authors use the RNN model to solve a multi-label classification problem. They address the issue that RNNs produce sequential outputs, so the target labels should be ordered. The authors propose to dynamically order the ground truth labels based on the model predictions, which contributes to faster training and alleviates the effect of duplicate generation. In turn, \cite{tsai2020order} consider the transformation of a multi-label classification problem to a sequence prediction problem with an RNN decoder. They propose a new learning algorithm for RNN-based decoders which does not rely on a predefined label order. Consequently, the model explores diverse label combinations, alleviating the exposure bias. 
The work~\cite{shou2023concurrent} examines the same problem statement of multi-label classification in an event stream as we do. The authors' model targets capturing temporal and probabilistic dependencies between concurrent event types by encoding historical information with a transformer and then leveraging a conditional mixture of Bernoulli experts.

\paragraph{\textbf{Approaches to leveraging label dependencies.}}
The authors of \cite{Lanchantin_2021_CVPR} construct a model called C-Tran for a multi-label image classification task that leverages Transformer architecture that encourages capturing the dependencies among image features and target labels. The key idea is to train the model with label masking. The authors in~\cite{yeh2017learning} propose DNN architecture for solving the multi-label classification task, which incorporates the construction of label embeddings with feature and label interdependency awareness. A label-correlation sensitive loss improves the efficiency of the constructed model. Another popular way to consider label relationships is to use Graph Neural Networks as a part of the pipeline. Namely, \cite{pal2020multi} captures the correlation between the labels in the task of Multi-Label Text Classification by adopting Graph Attention Network (GAT). They predict the final set of labels combining feature vectors from BiLSTM and attended label features from GAT.  
Event sequences processing also tries to derive dependencies between different event types and consider specific attention mechanisms~\cite{mei2021transformer}.
The closest to our LANET approach~\cite{liu2021gated} in the literature explores the relationship between different series for multivariate time series classification. The authors propose using attention in step-wise and channel-wise fashion to produce embeddings, which are then forwarded by a classification head. 

% TODO sequential recommender systems?
\paragraph{\textbf{Sequential recommender systems.}}
Another close neighbour of our problem statement is the problem of sequential recommender system construction~\cite{wang2019sequential,quadrana2018sequence}.
In this case, we have a lot of possible labels and should sort them with respect to the probability of occurrence next time.
Typically, the estimation of embeddings for all possible labels/items is a part of a pipeline.
Existing approaches use neural networks for sequential data such as LSTM~\cite{wu2017recurrent} as well as attention mechanism~\cite{wang2017perceiving}.
We want to highlight the statement related to the usage of only recent past data for prediction~\cite{li2020time}.
However, millions of possible labels typically lead to more classic techniques in this area with specific loss functions and methods.


% HERE GOES A PARAGRAPH ON WHAT IS MISSING RIGHT NOW
\paragraph{\textbf{Research gap.}}
Almost all existing methods are applied to multi-label learning for images, natural language texts or time series. 
However, there is a need for approaches to the multi-label classification of event sequences. Such characteristics of events as time-dependency and heterogeneous attributes introduce additional difficulties as well as opportunities for their processing and learning of label correlations. Notably, such data structure encourages the usage of the attention mechanism, as the sets of possible labels in the past and the future coincide, and it is natural to leverage such a connection in an architecture. 


\section{Methods} 
\label{sec:methods}


\subsection{Multi-label sequential classification}

% We consider a multi-label classification for a sequence $S = \{(\vecX_{i}, Y_{i})\}_{i = 1}^{t - 1}$.
% It consists of a set of labels $Y_i$ and features $\vecX_i$ specific for each timestamp from $1$ to $(t - 1)$. The index corresponds to the time of event, so $(\vecX_{1}, Y_{1})$ is the information about the first event, and $(\vecX_{t - 1}, Y_{t - 1})$ is the information about the last observed event.
% A set $Y_i \subseteq \mathcal{Y}$, where $\mathcal{Y} = \{1, 2, \dots, K\}$ is the set of all possible labels. 
% A vector of features $\vecX_i \in \mathbb{R}^d$.
% Our goal is to predict a set of labels $Y_t$ for the next timestamp.
% We also can have additional set of features for a sequence $\mathbf{z}$ related to a general information of an object e.g. ID, marital status or education.

% Version in which I fix a logic for event features
We consider a multi-label classification for a sequence $S = \{(X_{i}, Y_{i})\}_{i = 1}^{t-1}$.
It consists of a set of labels $Y_i$ and a set of features $X_{i}$ specific for each timestamp from $1$ to $t-1$. The index corresponds to the time of the event, so $(X_{1}, Y_{1})$ is the information about the first event, and $(X_{t-1}, Y_{t-1})$ is the information about the last observed event.
A set $Y_i \subseteq \mathcal{Y}$, where $\mathcal{Y} = \{1, 2, \dots, K\}$ is the set of all possible labels. A set size of $X_{i}$ is equal to a set size of $Y_{i}$. Each label from $Y_{i}$ is entailed with the numerical or categorical feature from $X_{i}$ in the corresponding position. 
We also can have an additional feature vector $\mathbf{z}$ describing a considered sequence $S$ as a whole, e.g. user ID.
The goal of sequential multi-label classification is to predict a set of labels $Y_{t}$ for the next timestamp.


% We construct a function $f(\cdot) = \{f_1(\cdot), \ldots, f_K(\cdot)\} \in [0, 1]^K$ that takes the information relevant to the currently considered event $(S, \mathbf{z})$ as input and outputs the vector of scores for each of $K$ labels. Each score is an estimate of the probability of a label to be presented in the next event-related label set.
% In our setting, we limit the size of the past available to our model.
% $S = \{(\vecX_{j}, Y_{j})\}_{j = t - \tau}^{t - 1}$, where $\tau$ means a number of events preceding the considered event with the timestamp $t$, which is attributed with a target label set $Y_t$.
% So, more formally $f(\cdot)$ has the form:
% $$
% f(\vecX_{t - \tau}, \ldots, \vecX_{t - 1}, Y_{t - \tau}, \ldots, Y_{t - 1}, \mathbf{z}) \in [0, 1]^K 
% $$
% to prepare scores for the prediction of $Y_t$.

We construct a function $f(\cdot) \in [0, 1]^K$ that takes the historical information on events as input and outputs the vector of scores for each of $K$ labels. Each score is an estimate of the probability of a label to be present in the next event-related label set.
In our setting, we limit the size of the past available to our model.
$S^t = \{(X_{j}, Y_{j})\}_{j = t - \tau}^{t-1}$, where $\tau$ means a number of events preceding the considered event with the timestamp $t$, which is attributed with a target label set $Y_{t}$.
So, more formally $f(\cdot)$ has the form:
$$
f(X_{t - \tau}, \ldots, X_{t-1}, Y_{t - \tau}, \ldots, Y_{t-1}, \textbf{z}) \in [0, 1]^K 
$$
to prepare scores for the prediction of $Y_{t}$.

To complete a prediction, we need a separate label decision model $g(f(\cdot))$, that transforms confidence scores into labels.
For example, we compare the score for $k$-th label to a selected threshold $\beta_k$: if $f_k(\cdot) > \beta_k, k = 1, \dots, K$, then the model predicts that the $k$-th label is present. Thus, the model $g$ produces a label set $\hat{Y}_{t} \subseteq \mathcal{Y}$ from the input confidence~scores.

The resulting quality of the model depends both on the method $g(\cdot)$ for selecting labels to a final set and the performance of $f(\cdot)$ to produce confidence scores while we focus on working with $f(\cdot)$. 

\subsection{Dataset structure} 
\label{sec:data_structure}

We consider problems with the following dataset structure. See Figure~\ref{fig:multi_label_data} for an example of a data sample.
% To be able to use the developed architectures in the diverse fields, the input features to these architectures should follow the fixed structure. Besides, such features should have general concepts that can be easily extended to other domains. Thus, we convert all datasets to the structure presented in ~\autoref{tab:multi_label_data}. 
\begin{itemize}
    \item \textbf{ID}. Each sequence that corresponds to a client or an organization has a unique identifier. So in our case, the feature vector for a sequence description is just a scalar value $z$. There may be thousands of unique IDs in the dataset. Nevertheless, we as well provide the results of experiments without the usage of ID information.
    \item \textbf{Date}. Each event is associated with a date. 
    \item \textbf{Labels}. The label's column provides a set of categorical variables $Y_i$ that describes an event. It might be a sequence of codes of transactions made by a client in a particular day or the categories of the sold items.
    \item \textbf{Features}. Each event is also characterized by a feature set $X_i$. It is represented as a set of categorical or numerical values. Each component in a set describes the corresponding label from a label set. Values of a feature set may have a meaning of the sums of paid money per transaction code or the number of the sold items per category.
\end{itemize}

% We have a single ID for each sequence of events, while all other features are specific for each event in a sequence. 
% Our goal is still to predict $Y_t$ as close as possible.

% We assume that we have access to all historical information for a particular ID and want to predict a categorical set for the current timestamp related to the same ID. We split each sequence in several subsequences with rolling window of size \textit{$look\_back + 1$} shifting it by one timestamp. 
% % For considered dataset it is often useful to process only \textit{$look\_back$} previous moments instead of full sequences.

% Therefore, an input to the model is all information from the last \textit{$look\_back$} timestamps related to one ID and the output target is a categorical set of the next timestamp. 

\subsection{Sample structure}

% We work within a sequential multi-label classification problem in the panel setting. 
% The training and test datasets have the structure $\mathcal{D} = \{(S_i, \mathbf{z}_i, Y_{t_i})\}_{i = 1}^n$.
% Each $S_i$ is a sequence of pairs of features and corresponding multi-label sets.
% The input to the model $f(\cdot)$ is a sequence of vectors, which describe the consecutive events preceding the considered timestamp $(S_i, \mathbf{z}_i)$, the length of $S_i$ being $t_i - 1$. 
% The output are scores that allow us to reconstruct the label set $Y_{t_i}$. 

We work within a sequential multi-label classification problem in the panel setting. 
The considered datasets have the structure $\mathcal{D} = \{(S_j, z_j)\}_{j = 1}^n$.
Each $S_j$ is a sequence of pairs of feature sets and corresponding multi-label sets. We need to split sequences $S_j, j=1, \dots, n$ into samples for model training and testing. The part of a sample that is passed to the model's input $f(\cdot)$ incorporates information on $\tau$ consecutive events from $S_j$ preceding the considered timestamp $t$. The sample's target is a label set of the event from $S_j$ with the considered timestamp~$t$. 
% The input to the model $f(\cdot)$ is a sample that incorporates information on $\tau$ consecutive events from $S_j$ preceding the considered timestamp $t$ for the particular $z_j$. 
% The output are scores that allow us to reconstruct the label set $Y_{t_i}$. 
Thus, to construct train and test samples from the initial sequence~$S_j$, we need to apply a sliding window approach. 
Importantly, train and test datasets that share the samples related to the same $z_j, j = 1, \dots, n$ organized in a non-overlapping and chronological way.

\subsection{Our LANET approach} 
\label{sec:transf_label}

Most of the transformer-related models used for sequential multi-label prediction use self-attention computation between consecutive input timestamps representations. 
The LANET instead uses the self-attention between label representations. 
So, it has the input that consists of $K$ vectors.
Below, we describe how to aggregate a sequence of size $\tau$ to $K$ vectors via an \textbf{Embedding layer}. Then we define the \textbf{Self-attention layer}.
To get the predictions, we apply a \textbf{Prediction layer}.

\paragraph{\textbf{Embedding layer.}} We use the following approach to use different parts of input data for multi-label event sequences:
\begin{itemize}
    \item \textbf{\emph{ID embedding:}} For IDs we learn an embedding matrix;
    \item \textbf{\emph{Time embedding:}} For each timestamp, we know the value of $dt$, which is equal to the difference in days between the considered and the previous timestamp. We train an embedding for each observed $dt$ value. We also take into account the order of events, so we look at the embeddings for positions: $1, \dots,$ \textit{$\tau$} to add them to the $dt$ representation of the corresponding timestamp; 
    % To get an embedding for $\tau$ timestamps, we use the mean pooling operation;
    \item \textbf{\emph{Amount embedding:}} We transform all amounts into bins, breaking down the continuous sums of amounts on the interval. Each interval is assigned a unique number. Then, for each unique number, we create embedding. A vector of representations is learned similar to~\cite{fursov2021adversarial}. 
    % To get an embedding for $\tau$ timestamps, we use the mean pooling operation;
    \item \textbf{\emph{Transformer label encoder:}} To construct the LANET's input, we use the data associated with a specific ID for \textit{$\tau$} timestamps. We concatenate the representation of the labels that are encountered during \textit{$\tau$} timestamps with the corresponding time and amount embeddings. If a label does not belong to a history of \textit{$\tau$} last timestamps, we add vectors of zeros to it as time and amount views. If a particular label occurs several times during previous \textit{$\tau$} steps, then we construct time embeddings for each individual occurrence and then sum them up to get the ultimate time view for that label. We do the same when constructing amount embedding in the situation of label reoccurrence.
\end{itemize}

So, as a result of the embedding layer, we have $K + 1$ embedding vectors. 
The first vector $\vecE_0$ corresponds to the embeddings of general (ID) features for a sequence.
All other vectors $\vecE_k, k = 1, \dots, K$ are concatenation of the label embedding with the corresponding time and amount embeddings. It turns out that vectors for not historically involved labels are just label embeddings, as we are summing them up with zero vectors of time and and amount views.
Under the training of all embedding weights, they are initialized from normal distribution $\mathcal{N}(0, 1)$ and then optimized.
% All other vectors are concatenations of the label embedding from a dictionary and mean poolings of amounts and timestamps for a particular label.  

\paragraph{\textbf{Self-attention layer.}} After getting representations from our data, we move on to the composition of our architecture. 
Let $E = \{\vecE_0, \dots, \vecE_{K}\}$, $\vecE_i \in \mathbb{R}^d$, be the sequence of input embeddings to the Transformer encoder, where $\vecE_0$ corresponds to the embedding of $\mathbf{z}$ and all other $\vecE_i$ correspond to embeddings captured historical information from a label point of view. 
In Transformer architecture, the influence of embedding $\vecE_j$ on embedding $\vecE_i$ is obtained via self-attention. The attention weight $\alpha_{ij}$ and an embedding update $\vecE_i^{\prime}$ are calculated as:
$$ \vecE_i^{\prime} = \sum_{j = 0}^{K} \alpha_{ij} (W^v \vecE_j);\hspace{3mm} \alpha_{ij} = \mathrm{softmax}\left ( (W^q \vecE_i)^T (W^k \vecE_j) / \sqrt{d} \right),$$
    where $W^k$ is the key weight matrix, $W^q$ is the query weight matrix, and $W^v$ is the value weight matrix. Such a procedure of embedding updates can be repeated several times by specifying the number of Transformer encoder layers.
% On top of the self-attention sublayer, we apply routine Transformer encoder layer procedures~\cite{transformer_encoder_torch}.
% In particular, we TODO.


\paragraph{\textbf{Prediction layer.}} The updated embeddings are processed with one fully-connected linear layer to get final embeddings for each label $\{\vecE^{(final)}_j\}_{j = 1}^K$.
In this way, we obtain $\{f_j\}_{j = 1}^K$ that are used in a multi-label classifier with threshold $t_j$ selected separately for each label using a validation sample.

\section{Experiments} 
\label{sec:experiments}

In this section, we present comparison of our approach with others and relevant ablation studies.
The code is available at GitHub repository\footnote{\url{https://github.com/E-Kovtun/order_prediction/tree/common_model}}.

\subsection{Datasets}

% \begin{table}
% \label{tab:multi_datasets}
% \centering
% \begin{tabular}{cccccc}
% \hline
% & Dataset &  Median label  & Max label & Label  \\
% & number & number & number \\
% \hline
% & Sales & 16 & 48  & 84 \\
% & Demand & 13 & 24  & 33 \\
% & Liquor & 14 & 66  & 107 \\
% & Transactions & 3 & 23  & 77 \\
% & Orders & 2 & 13 & 61 \\
% \hline
% \end{tabular}
% \caption{Characteristics of the datasets used in sequential multi-label classification problems.}
% \label{tab:multi_datasets}
% \end{table}

\begin{table}[t!]
\centering
% \begin{tabular}{ccccccc}
\begin{tabular}{p{0.3cm}p{2.2cm}p{1.7cm}p{1.6cm}p{1.6cm}p{1.8cm}p{1.1cm}}
\hline
& Dataset & \# events & Median & Max & \# unique & Diff \\
& &  & set size & set size &  labels \\
\hline
& Sales & 47 217 & 16 & 48  & 84 & 0.0632 \\
& Demand & 5 912 & 13 & 24  & 33 & 0.0957 \\
& Liquor & 291 029 & 14 & 66 & 107 & 0.0413 \\
& Transactions & 784 520 & 3 & 23  & 77 & 0.1079 \\
& Orders & 226 522 & 2 & 13 & 61 & 0.0518 \\
\hline
\end{tabular}
\caption{Characteristics of the datasets used in sequential multi-label classification problems.}
\label{tab:multi_datasets}
\end{table}

For a more versatile study of the algorithm we built, we conducted experiments on five diverse datasets. 
\textbf{Sales dataset}~\cite{sales} is historical sales data from different shops. The labels relate to the item categories, and the amount is the number of sold items for a particular category. 
\textbf{Demand dataset}~\cite{demand} describes historical product demand of several warehouses. The label feature means the product category, and the amount feature relates to the corresponding demand.
\textbf{Liquor dataset}~\cite{liquor} presents information on the liquor sales of the different stores. Each liquor label is identified by its category. The amount of sold liquor is measured in litres.
\textbf{Transactions dataset}~\cite{gender} contains histories of clients transactions. Each transaction is described with the special MCC code and an involved amount of money. In the original paper, the dataset has the name \textit{Gender}.
To demonstrate the applicability of the proposed approach in the industry, we also consider a private \textbf{Orders dataset}. 
It provides information on the restaurant orders of different materials for beer production. The amount feature is related to the volume of ordered materials in hectolitres. 

The overall statistics for the considered datasets are given in Table~\ref{tab:multi_datasets}.
We present the number of observed events, Median set size of all available label sets $\mathrm{median}({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, Maximum size of the label set that is encountered in the dataset $\max({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, the number of unique labels $K$, and \textit{Diff}. 
\textit{Diff} measures the imbalance of labels: we calculate 5$\%$ and 95$\%$ quantiles for frequencies of labels and take the difference between them.
The representation of labels is imbalanced in the majority of datasets, but we use metrics that take into account this effect.
All datasets are transformed to a general structure described in~\autoref{fig:multi_label_data}.

% We provide details on the preprocessing of the considered datasets in Appendix.
% \begin{table}
% \label{tab:stats_data}
% \centering
% % \begin{tabular}{ccccccc}
% \begin{tabular}{p{0.3cm}p{2cm}p{1.5cm}p{1.5cm}p{1.5cm}p{2cm}p{1.5cm}p{0.3cm}}
% \hline
% & Dataset & Sales & Demand & Liquor & Transactions & Orders \\

% \hline
% & Median & 0.0024 & 0.0051 & 0.0020 & 0.0002  & 0.0078 \\
% & Mean & 0.0119 & 0.0303 & 0.0093  & 0.0130 & 0.0164\\
% & Diff & 0.0632 & 0.0957 & 0.0413 & 0.1079 & 0.0518 \\

% \hline
% \end{tabular}
% \caption{Analysis of the frequency of occurrence of each unique label in the dataset. The table shows the \textit{Median} and \textit{Mean} values for the frequency. \textit{Diff} means that 5$\%$ and 95$\%$ quantiles were calculated for the frequency and the difference between them was taken. }
% \label{tab:multi_dataset_imbalance}
% \end{table}

\begin{table*}[t] % table
\centering
\small
\begin{tabular}{ccccccc}
\hline
Dataset & Model & Micro-AUC$\uparrow$ & Macro-AUC$\uparrow$ & Micro-F1$\uparrow$ & Macro-F1$\uparrow$\\
\hline
& LSTM & \underline{0.8670} & \underline{0.7346} & \underline{0.5600} & \underline{0.4389} \\
& TransformerBase     & 0.8604 & 0.7206 & 0.5348 & 0.3751 \\
% & Gradient Boosting   & 0.7807  & 0.6156 & 0.4275 & 0.4151\\
& CLASS2C2AE          & 0.8528 & 0.6881 & 0.5116 & 0.4217 \\
\multirow{-5}{*}{Sales} & LANET (ours) & \textbf{0.9069} & \textbf{0.7627} & \textbf{0.6235} & \textbf{0.4901} \\ \hline

& LSTM & \textbf{0.8829} & \textbf{0.7633} & \underline{0.6746} & \textbf{0.5929} \\
& TransformerBase & 0.8624  & 0.7240 & 0.6491 & 0.5678 \\
% & Gradient Boosting & 0.7784  & 0.6219  & 0.6010 & 0.5466\\
& CLASS2C2AE & 0.8342  & 0.7079 & 0.6738 & 0.5581\\
\multirow{-5}{*}{Demand} & LANET (ours) & \underline{0.8806} & \underline{0.7373} & \textbf{0.7038} & \underline{0.5908} \\ \hline

& LSTM & 0.7520 & 0.7253 & \underline{0.4729} & 0.2218 \\
& TransformerBase & 0.7501 & 0.7197 & 0.4674 & 0.2279 \\
& CLASS2C2AE & \underline{0.9442}  & \underline{0.8058}  & 0.4088 & \underline{0.3638}\\
\multirow{-4}{*}{Liquor} & LANET (ours) & \textbf{0.9536}  & \textbf{0.8728} & \textbf{0.7089} & \textbf{0.4508} \\ 
\hline

& LSTM &  \underline{0.9719}  & \underline{0.8189} & \underline{0.2133} & \textbf{0.2349} \\
& TransformerBase & 0.9714  & 0.8098 & 0.1982 & \underline{0.2285} &\\
& CLASS2C2AE & 0.9347 & 0.7324 & 0.1131 & 0.1501 \\
\multirow{-4}{*}{Transactions} & LANET (ours) & \textbf{0.9743}  & \textbf{0.8328} & \textbf{0.2373} & 0.1067 \\ \hline

& LSTM & \underline{0.9800}  & \underline{0.9562} & \textbf{0.6827} & \textbf{0.6280} \\
& TransformerBase & 0.9601 & 0.9316 & 0.6302 & 0.5631 \\
% & Gradient Boosting & 0.8105  & 0.7820 & \underline{0.6475} & \underline{0.6115} \\
& CLASS2C2AE & 0.9218 & 0.8964 & \underline{0.6324} & \underline{0.5979} \\
\multirow{-5}{*}{Orders} & LANET (ours) & \textbf{0.9847} & \textbf{0.9665} & 0.5773 & 0.5856 \\ 

\hline

\end{tabular}
\centering
\caption{Comparison of our method LANET with the baselines on five different datasets.
% Micro-AUC and macro-AUC are calculated on the basis of confidence scores. In turn, to evaluate micro-F1 and macro-F1, we compose final label set by comparing confidence scores with thresholds calculated on the validation set. 
Best values are highlighted, and second-best values are underlined.
}
\label{table:diff_data}
\end{table*}

\subsection{Baselines and training details} 
\label{sec:based_transf}

\textbf{TransformerBase} utilizes the transformer architecture~\cite{vaswani2017attention}. Similar to many previous works, it considers the self-attention between input vectors that correspond to consecutive points in a sequence. We also note that it can be viewed as a simplification of next-item-recommendation models~\cite{li2020time}.
\emph{The amount vector for each timestamp} is the input vector, whose dimension is equal to the number of the unique label values. Each position of the vector contains the corresponding amount value.
There is no need for label embedding, as the information on historically involved labels is introduced through the amount vector. Time and ID embeddings are constructed in the same way as in LANET.
We also use two self-attention layers.
The output of the transformer encoder is an updated time representation. Subsequently, Mean Pooling is applied to this output to obtain the final vector of confidence estimates, based on which we make a prediction on a set of labels for the current timestamp.

% \begin{figure}[!th]
%     \centering
%     \includegraphics[scale=0.6]{images/transformer_base_upd2.pdf}
%     \caption{TransformerBase architecture for multi-label classification of time-dependent events.
%     The input to this model is a sequence of vectors which are obtained by the concatenation of representations of different features related to the particular timestamp.}
%     \label{fig:transformer_base}
% \end{figure}

We also use two RNN-based approaches. \textbf{RNN (LSTM)} replaces the transformer encoder with LSTM-type encoder~\cite{hochreiter1997long}. The model takes one by one point in time and updates the hidden representation, which contains information about previous elements. 
\textbf{CLASS2C2AE} is a model for multilabel-classification presented in~\cite{yeh2017learning}. This model combines LSTM and autoencoder to improve its performance.

For all models, we use the same data input given in Subsection~\ref{sec:data_structure}. 
The following manually selected hyperparameters were used for all experiments: dimension of the embedding space is 128, batch size is 32. We use Adam optimizer with learning rate initialized to $0.001$ and reduced via reduce-LR-on-plateau scheduler. 

\paragraph{LANET training details.} 
%\label{sec:train}
Our LANET model consists of two Transformer encoder layers with four heads in each layer. Self-attention and layer implementation are inherited from the basic PyTorch Transformer layer~\cite{transformer_encoder_torch}. After receiving the updated vector representations, we run them through the dropout layer, which is initialized with the probability $0.3$. In the end, a fully-connected linear layer is applied to the obtained vectors to get final label scores. LANET is trained with the cross-entropy loss adapted for the multi-label task through independent consideration of each label score.  


\paragraph{Validation procedure.} \label{sec:valid}
% \subsection{Validation procedure} \label{sec:valid}

The original datasets are divided into train, validation, and test sets. For each ID, some part of the consecutive events go to the train set, and the rest is allocated for validation and test. 
Importantly, the time periods in the train, valid, and test sets do not overlap and 
are arranged in the order of increasing time. 
% in a way that validation data contain more recent information than the training set. In turn, the validation data contain timestamps that precede dates from the test set. 
We take 70\% of the data samples for model training, 17\% for validation, and 13\% for testing. 
All experiments were also carried out for five random seeds, and the results after the experiments were averaged.

% In the following discussion, we refer to an example, as the combination of $\tau$ consecutive rows from the structured dataset and the target of such example is the categorical set from the following timestamp. We compose such examples by going through the initial dataset with one row-size steps. 

%\subsection{Metrics} \label{sec:metric}
The article~\cite{wu2017unified} provides an overview of metrics for multi-label classification. We consider the most frequently used and comprehensive metrics for the estimation of multi-label classification quality: Micro-AUC, Macro-AUC, Micro-F1, and Macro-F1.
% TODO include one metric as an example?

 
%  TODO: check, if we use another metrics in experiments in the final version of the article

\subsection{Main results} 
\label{sec:comp_qual}
% \begin{figure}[h!]
%      \centering
%      \begin{subfigure}[b]{0.47\textwidth}
%          \centering
%          \includegraphics[width=1\linewidth]{images/Percentage_of_occurrence_of_each_label_in_the_dataset.pdf}
%          \caption{ }
%          \label{fig:balance_1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.47\textwidth}
%          \centering
%          \includegraphics[width=1\linewidth]{images/Percentage_of_the_length_of_the_label_sequence_for_each_time.pdf}
%          \caption{ }
%          \label{fig:balance_2}
%      \end{subfigure}
%      \caption{The analysis of the unbalance of the label sample is presented in two ways: (a) The dependence of percentage of occurrence of each label in the dataset on the labels; (b) The dependence of percentage of the length of the label sequence for each time on the length of the label sequence for each time. }
% \end{figure}
 
For all datasets, the models are trained with five different random seeds to understand the value of results deviation from the result depending on the selected random state. The standard deviation influences the metrics in the third decimal place, while the superiority in performance is observed in the first or second decimal place. To make our tables more readable, we omit the standard deviation values.
To calculate micro-F1 and macro-F1, we search for thresholds to compare them with the obtained confidence scores and compose the final label set. Thresholds are selected on the validation set by optimizing F1-score for each label separately. 

 
Table~\ref{table:diff_data} presents the sequential multi-label classification metrics for considered approaches.
Our LANET gives the best results for diverse datasets. 
The LSTM baseline is also suitable for capturing the label relationships in general. We denote that it gives better results than the used version of transformer.
The reason can be the limited amount of data for training available, so there are too many parameters for a transformer with timestamp-based self-attention. Besides, we test the model suggested in~\cite{shou2023concurrent} for our problems. However, the results were not satisfying. We associate it with a more complex structure and, consequently, greater sensitivity to model parameters. 

% On the contrary, to evaluate Precision@k, Recall@k, F1@k, and MAP@k metrics, we select labels with top-k confidence scores in predicted label set. 
% The results are in Table~\ref{table:top_k_order}.
% LANET model shows the best results for all presented metrics with top-k scoring.

 
\subsection{Ablation study}
\label{sec:qual}

In this section, we provide results of ablation studies. 
The results are for Demand dataset, if not mentioned otherwise, as for most of the other datasets conclusions are similar.

\paragraph{\textbf{Usage of both label-attention and timestamp-attention.}}
It is worth exploring what perspective of self-attention calculation is more critical: between labels or between timestamps.
Table~\ref{table:attents} answers this question.
Label-attention is our basic implementation. Time-attention is a case when we consider attention only between timestamp views. Concat-attention implies getting confidence scores from concatenation of label-attention and time-attention. We learn the importance coefficients of two forms of attentions and use them as weights when summing attention views in the case of gated-attention. Absence-indication experiment consists in adding a learnable vector to the input embeddings if the particular label is not involved in the considered history.
As a result, Label-attention is the most beneficial, and the inclusion of both types of attention decreases the quality.
% due to overfitting.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lcccccc}
\hline
 & Micro-AUC$\uparrow$ & Macro-AUC$\uparrow$ & Micro-F1$\uparrow$ & Macro-F1$\uparrow$\\ 
\hline
Label-attention & \underline{0.881} $\pm$ 0.007 & \underline{0.737} $\pm$ 0.017 & \underline{0.704} $\pm$ 0.018 & \textbf{0.591} $\pm$ 0.003\\
Time-attention & 0.837 $\pm$ 0.004 & 0.682 $\pm$ 0.007 & 0.674 $\pm$ 0.009  & 0.588 $\pm$ 0.002 \\
Concat-attention & 0.835 $\pm$ 0.001 & 0.681 $\pm$ 0.010 & 0.666 $\pm$ 0.000 & 0.587 $\pm$ 0.001 \\
Gated-attention & 0.829 $\pm$ 0.030 & 0.668 $\pm$ 0.048 & 0.672 $\pm$ 0.010 & 0.578 $\pm$ 0.011 \\
\hline
 Absence-indication & \textbf{0.882} $\pm$ 0.004 & \textbf{0.742} $\pm$ 0.003 & \textbf{0.704} $\pm$ 0.010 & \underline{0.589} $\pm$ 0.004 \\
\hline
\end{tabular}
\centering
\caption{Comparison of metrics when calculating attentions between different views in LANET. }
\label{table:attents}
\end{table}

\paragraph{\textbf{The contribution of different feature embeddings to model performance.}}
The input to our LANET model consists of a label, time, amount, and ID that we embed to input to the self-attention layer. All these parts include information on the events preceding the considered one. We want to understand the contribution of each feature embedding to the model prediction. So, in each experiment, we drop either amount, time, and ID one at a time. A significant quality drop after removal indicates the importance of the model part for the prediction. The results of such an experiment are in~\autoref{table:no_embeds}.    

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lcccccc}
\hline
Inputs & Micro-AUC$\uparrow$ & Macro-AUC$\uparrow$ & Micro-F1$\uparrow$ & Macro-F1$\uparrow$\\ 
\hline
All & \textbf{0.881} $\pm$ 0.007 & \textbf{0.737} $\pm$ 0.017 & \textbf{0.704} $\pm$ 0.018 & \textbf{0.591} $\pm$ 0.003\\
No amount & 0.825 $\pm$ 0.051 & 0.705 $\pm$ 0.039 & 0.698 $\pm$ 0.010 & 0.574 $\pm$ 0.014 \\
No time & 0.869 $\pm$ 0.007 & 0.721 $\pm$ 0.020 & 0.698 $\pm$ 0.004 & \underline{0.590} $\pm$ 0.003 \\
No ID & \underline{0.880} $\pm$ 0.003 & \underline{0.732} $\pm$ 0.006 & \underline{0.703} $\pm$ 0.022 & 0.588 $\pm$ 0.004 \\
    \hline
\end{tabular}
\centering
\caption{How different types of embeddings in the architecture affect LANET.}
\label{table:no_embeds}
\end{table}

A substantial drop in quality happens when we drop the amount embeddings. We attribute this effect to the fact that amounts are essential for quantifying the scale of interdependencies.
Time embeddings are helpful and make models capture label connections better. 
IDs contribute little to the performance of the model. It seems that it is enough to look at other types of available information. 

\paragraph{\textbf{Dependence of model quality on $\tau$.}}
The model considers only $\tau$ last timestamps for prediction. Thus, a natural question arises about how the model quality depends on the number of timestamps used for prediction. 
The dependence of micro-AUC metric on $\tau$ is presented in \autoref{fig:look_back}.


% \begin{figure}[!t]
%     \centering
%     \includegraphics[scale=0.23]{images/look_back.pdf}
%     \caption{The dependence of micro-AUC metric on the \textit{look\_back} parameter, which characterizes the number of timestamps used for the prediction. Processing of sequences with more historical information is beneficial for the model performance. The demand dataset is considered.}
%     \label{fig:look_back}
% \end{figure}


% \begin{figure}
% \centering
% \begin{minipage}[b]{0.3\textwidth}
%   \centering
%   \includegraphics[width=\textwidth]{images/look_back.pdf}
%   \caption{The dependence of micro-AUC metric on the \textit{look\_back} parameter, which characterizes the number of timestamps used for the prediction. Processing of sequences with more historical information is beneficial for the model performance. The demand dataset is considered.}
%   \label{fig:look_back}
% \end{minipage}
% \begin{minipage}[b]{0.3\textwidth}
%   \centering
%   \includegraphics[width=\textwidth]{images/emb_dim.pdf}
%   \caption{The dependence of micro-F1 metric in the dimension of embedding vectors. There exists the optimal vector size. Further increase in embedding size leads to a quality drop. The demand dataset is considered.}
%   \label{fig:emb_dim}
% \end{minipage}
% \end{figure}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=0.9\linewidth]{images/tau.pdf}
         \caption{The dependence of micro-AUC on the \textit{$\tau$} parameter.}
         \label{fig:look_back}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=0.9\linewidth]{images/emb.pdf}
         \caption{The dependence of micro-AUC on the dimension of embeddings.}
         \label{fig:emb_dim}
     \end{subfigure}
    \caption{The dependence of metrics on model parameters for Demand dataset.}
\end{figure}

The experiment supports the evidence that the more information we provide to our model, the better prediction we obtain for the considered timestamp. 
% However, we can face computational problems if the sequences are too long.
However, we should find the balance between the desired quality and the computational cost since the number of involved labels increases with boosting parameter value. Therefore, for the optimality of the solution and the speed of the method, we selected the parameter $\tau = 3$.

\paragraph{\textbf{Dependence of the model performance on embedding size.}}
As we are working with learnable embeddings for label, amount, time, and ID features, we need to set the dimensionality of their representations. For each feature, we construct embeddings of the same size. The dimensionality of the embedding vector defines the capacity of information that a model should capture and the overall number of learnable parameters. The results are in~\autoref{fig:emb_dim}. We can observe that the model can not effectively learn large-dimension representations.

% The optimal size of the embedding vectors is equal to $128$. 
% Further increase of the dimensionality leads to the deterioration of the micro-F1 metric. 
% We conclude, that the model can't effectively learn large-dimension representations  with available amount of data.

\paragraph{\textbf{Attention matrix for vector representations of labels}} 
Additionally, we constructed an attention matrix for the data labels. This matrix reflects label importances for making predictions. 
The model pays attention to various sequence objects with different weights. 
Figure~\ref{fig:abl} shows three attention matrices for three datasets: Order, Transactions, and Liquor. The size of the matrix is the number of unique labels in the dataset.
% Not surprisingly, we pay attention to labels presented in the last timestamps, so we have clear dominance of some columns in the attention matrix.
We notice that the attention matrix has a clear dominance of the certain labels encountered in the sequence over those that are not in it, which is clearly expressed through the weights.
Looking deeper, we see that small-scale variations in attention describe the connection between particular event types.

% \begin{figure*}[!htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{images/attention_orders_log_0.pdf}
%     \includegraphics[width=.3\linewidth]{images/attention_gender_log_0.pdf}
%     \includegraphics[width=.3\linewidth]{images/attention_liquor_log_0.pdf}
%     \caption{Ablation study results for the Orders, Gender and Liquor datasets including attention matrix studies, which shows the importance of a label for the same sequence of data.}
%     \label{fig:abl}
% \end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[scale=0.3]{images/attention_all_log_0_1_2.pdf}
    
    \caption{Attention matrix studies for the Orders, Transactions and Liquor datasets from left to right. They show the representation importances.}
    \label{fig:abl}
\end{figure*}
% \begin{table*}[h!] % table
% \centering
% \small
% \begin{tabular}{ p{2cm} p{2cm} p{2cm} p{2cm} p{2cm} }
% \hline
%      k &  Precision@k $\uparrow$ & Recall@k $\uparrow$ & F1@k $\uparrow$ & MAP@k $\uparrow$  \\
% \hline
%     \multicolumn{5}{c}{\textbf{\emph{TransformerBase}}}\\
%     \hline
%     1	    & \underline{0.9240} & \underline{0.5166} & \underline{0.6627} & \underline{0.9240}\\
%     2	    & \underline{0.7273} & \underline{0.7203} & \underline{0.7237} & \underline{0.8256}\\
%     3	    & \underline{0.5906} & \underline{0.8265} & \underline{0.6889} & \underline{0.7473}\\
%     4       & \underline{0.4936} & \underline{0.8890} & \underline{0.6347} & \underline{0.6839}\\
%     \hline
%     \multicolumn{5}{c}{\textbf{\emph{Gradient Boosting}}}\\
%     \hline
%     1	    & 0.8713 & 0.5192 & 0.6506 & 0.8713\\
%     2	    & 0.6523 & 0.6537 & 0.6529 & 0.7618\\
%     3	    & 0.4955 & 0.7108 & 0.5839 & 0.6731\\
%     4       & 0.3899 & 0.7306 & 0.5084 & 0.6023\\
%     \hline
%     \multicolumn{5}{c}{\textbf{\emph{CLASS2C2AE}}}\\
%     \hline
%     1	    & 0.7422 & 0.4016 & 0.5211 & 0.7422\\
%     2	    & 0.6304 & 0.6278 & 0.6290 & 0.6863\\
%     3	    & 0.5221 & 0.7422 & 0.6129 & 0.6316\\
%     4       & 0.4364 & 0.8013 & 0.5650 & 0.5828\\
%     \hline
%     \multicolumn{5}{c}{\textbf{\emph{LSTM}}}\\
%     \hline
%     1	    & 0.9207 & 0.5148 & 0.6603 & 0.9207\\
%     2	    & 0.7236 & 0.7171 & 0.7203 & 0.8222\\
%     3	    & 0.5838 & 0.8187 & 0.6815 & 0.7427\\
%     4       & 0.4850 & 0.8765 & 0.6244 & 0.6783\\
% \hline
%     \multicolumn{5}{c}{\textbf{\emph{LANET}}}\\
%   \hline
%     1	    & \textbf{0.9279} & \textbf{0.5193} & \textbf{0.6659} & \textbf{0.9279}\\
%     2	    & \textbf{0.7349} & \textbf{0.7279} & \textbf{0.7313} & \textbf{0.8314}\\
%     3	    & \textbf{0.5989} & \textbf{0.8366} & \textbf{0.6981} & \textbf{0.7539}\\
%     4       & \textbf{0.5006} & \textbf{0.8994} & \textbf{0.6432} & \textbf{0.6906}\\
%   \hline
% \end{tabular}
% \centering
% \caption{Model quality for different approaches on Order Dataset. Hyperparameter $k$ is responsible for the number of labels that we take at the maximum score. Best values are highlighted, second best values are underlined.
% }
% \label{table:top_k_order}
% \end{table*} 


\section{Conclusions and discussion}

We consider the sequential multi-label classification problem: given previous multi-label vectors, what a multi-label vector will be next. 
For this problem, we propose the LANET architecture. It explores a connection between past multi-label vectors in a  specific architecture.
In particular, LANET uses the self-attention between labels to encourage capturing historical label interdependencies.
In our pipeline, we can apply any model architecture based on any similar self-attention. 
So, it can easily absorb attention-related innovations making it possible to add interpretability~\cite{serrano2019attention} or efficiency~\cite{tay2022efficient} to a model.

LANET demonstrates the best metrics on the five considered datasets. In particular, it improves the micro-AUC metric from $0.7501$ to $0.9536 $ on one of the datasets. 
However, the current approach is limited to the case where no long-term memory is required for prediction, as we aggregate only several recent timestamps and general embeddings of a particular sequence.
Further studies are welcomed to fill this gap.

It is also interesting to note that the proposed approach naturally fits into the paradigm of self-supervised learning and directly outputs an embedding of a client in general and at a particular time. The intricacy of the self-supervised problem is easy to tune by changing the used time horizon or including masking during training~\cite{he2022masked, kenton2019bert}. By aggregating labels collected during a day, we also make learning event sequences models more stable, avoiding the over-complicated problem of predicting the next event type~\cite{babaev2022coles} and challenging likelihood optimization for neural temporal points processes~\cite{shchur2021neural}.


% \section{Conclusions and limitations}

% Sequential data arises in numerous applied areas. For event sequences, for example, we aggregate all event types that happened on a certain date. A multi-label vector for a date has zeros at positions where no event of this type has happened and ones at positions that correspond to occurred events. We solve the sequential multi-label classification problem: given previous multi-label vectors, what a multi-label vector will be next. 


% We propose the LANET architecture for sequential multi-label classification. It explores a connection between past multi-label vectors in a  specific architecture.
% In particular, LANET uses the self-attention between labels to encourage capturing historical label interdependencies.

% LANET shows the best metrics on the five datasets we have considered, in particular, improving micro-AUC metric from $0.7501$ to $0.9847$ on one of the datasets. 
 
% However, our results have some limitations.
% We expect, that for sequences of multi-label data in some cases it can important to include both cross-time and cross-label attention, while it requires significant computational resources to implement. 
% The current approach is limited to the case, where no long-term memory is required for prediction, we need only a couple of recent timestamps and general embeddings of a particular sequence.


\bibliographystyle{splncs04}
\bibliography{references}
\clearpage
\newpage



% \appendix

% \subsection{Data preprocessing}

 
% \textbf{\textcolor{red}{TODO}} review
% An important step of a pipeline related to working with tabular data is preprocessing. All features must be transformed in a certain way for the correct operation of the entire model. 
% %The structure of our table is described in ~\autoref{sec:data_structure}. %We receive data already divided into train, test and valid data(section~\ref{sec:valid}). 
% First, we convert our data to a a general structure described in ~\autoref{tab:multi_label_data}. Then, we encode the categorical features, such as ID and labels, using Ordinal Encoder. Amounts are discretized with the purpose of construction of embeddings for them in the future. 
% % Fixing a certain id and date, we unite the labels into arrays, and the sum of amounts corresponding to each label. Having fixed the value of the identifier, we go through the data and calculate the difference between the dates to construct the dt column. We also implemented a function that, for a certain value of the id, walks through the resulting table with a step equal to the $look\_back$.

% Each dataset has its own specific features, which will be discussed in the sections below.

% \textbf{\textcolor{red}{TODO}}

% \textbf{\emph{Orders}} consists of $477 533$ orders of products in general. In total, this dataset has a number of restaurant identifier values equal to $..$ , the number of unique labels $61$ and the amount takes values from $0.06$ to $78.0$ . After combining for a certain restaurant id and date, we receive $201 406$ higher-level orders in total. The minimum length of the label sequence is $1$, and the maximum is $13$. It is also worth noting that the values in the column of the differences between dates we have constructed for a certain restaurant identifier are multiples of 7.

% \paragraph{Sales}

% \paragraph{Demand}

% \paragraph{Gender}

% \paragraph{Liquor}

% The first dataset we deal with is the data on orders of beers by restaurants from a manufacturer.
% In total, it consists of $477 533$ orders of products in total. 

 
% Each order consists of 12 variables.
% Thy describe restaurant index (Ship.to), postal code (PLZ), material from which the products are made (Material), date of the order delivery (DeliveryDateWeek), type of restaurant (HKUNNR), restaurant status (Status), strength of beer (MaterialGroup.2), product types (MaterialGroup.4), license (MaterialGroup.1), group (group), order year (DeliveryYear), purchased product quantity (AmountHL).
% The purchases quantity lies in range from $0.06$ to $78.0$.
% Table~\ref{table:unique} shows the number of unique values for different features and identify a feature as either categorical or continuous.

% \begin{table}
% \centering
% \small
% \begin{tabular}{ccc}
% \hline
%     Name          & Type  & N of unique values\\
%   \hline
%     Material	    &categorical           & 61    \\
%     PLZ	            &categorical           & 808   \\
%     Ship.to	        &categorical           & 1517  \\
%     Status          &categorical           & 2     \\
%     MaterialGroup.1	&categorical           & 3     \\
%     MaterialGroup.2	&categorical           & 3     \\
%     MaterialGroup.4	&categorical           & 5     \\
%     Delivery year   &categorical           & 6     \\
%     Week	        &categorical           & 5     \\	Month           &categorical           & 12  \\
%     Day	            &categorical           & 31    \\
%     Amount HL	    &continuous            & 340   \\
% \hline
% \end{tabular}
% \centering
% \caption{Names of all features with data types and numbers of unique values
% }
% \label{table:unique}
% \end{table}

 

 

% To create predictive models on the base of these data, we follow the preprocessing procedure.
% We start with considering only relevant features
% ids (Ship.to),
% labels (Material), 
% amounts (AmountHL), 
% dates (DeliveryDateWeek).
% Then we group orders to higher-level orders by restaurants-day pairs,
% as a restaurant each day forms a list of products with corresponding amounts it wants to purchase.
% So, we obtain $201 406$ higher-level orders in total.
% We also include $\mathrm{d}t$ time difference between two consecutive orders. 
% To better prepare inputs for neural network processing, we scale AmountHL and dt with MinMaxScaler, taking the minimum and maximum among all values encountered in the dataset and normalizing them to the range $[0, 1]$ by subtracting min value and dividing by the difference between max value and min value.

% Each order here is a list of purchased products with amounts specified for each.
% Prediction a list of products that fall into this order is then a multi-label classification problem.
% A part of our dataset after preprocessing is in Figure~\ref{fig:data_after_preprocessing}.

% The original datasets are divided into train, validation, and test sets. For each ID some part of the consecutive events go to the train set, and the rest one is allocated to valid and test. Importantly, the time periods in train, valid, and test sets do not overlap and are arranged in a way that validation data contain more recent information than train set. In turn, the validation data contain timestamps that precede dates from the test set. We take 70\% of the initial data size for model training, 17\% for validation, and 13\% for testing.
% 
% \subsection{ClassificationNet} \label{sec:class_net}
% \textbf{\textcolor{red}{TODO}}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/Gradient_Boosting-Classification_Net.png}
    
%     \centering
%     \caption{ClassificationNet.The main base architecture, which is also used in the model C2AE.}
%     \label{fig:l}
% \end{figure}

%  ClassificationNet consists of several main parts. We take the last hidden state from the first LSTM model that takes as input the sequence of sums of labels embeddings for each timestamp and the last hidden state from the second LSTM that takes the sequence of the concatenated sums of amounts and dt variables. These two vectors plus the restaurant id vector are combined. After concatenation, the vector is reduced to the size of a dictionary of unique values for labels. During training, a vector is also compiled, where there are units in the positions of the labels we need, in the rest - zeros. We begin to train our vector using PAL loss(or some other loss), which implies maximization of the scores where one should be.
%  

% \section{CLASS2C2AE} \label{c2ae}
% To compare our LANET model, we construct a competitive baseline called CLASS2C2AE that incorporates the parts in the architecture responsible for capturing label relationships.
% The overall model architecture is shown in ~\autoref{fig:c2ae}. 
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{images/orders_paper_diagram_CLASS2C2AE.pdf}
    
%     \centering
%     \caption{CLASS2C2AE. Model that enhance the quality of confidence scores that come from LSTM model with encoder-decoder processing. The adoption of the encoding-decoding mechanism that implies leveraging target multi-label vector during training leads to learning of label interdependencies. Better viewed in zoom.}
%     \label{fig:c2ae}
% \end{figure}

% The LSTM model(the architecture of this model is described in~\autoref{sec:based_transf}) has been upgraded by us to work with the avntoecoder by changing the input vectors to this representations:
% \begin{itemize}
%     \item \textbf{\emph{Representation of time}} - we make representations of dt values for each moment of time, where dt are the values of the time difference between the previous and the considered timestamp;
%     \item \textbf{\emph{Sum of vectors of the sum of amount}} - for each moment of time we make representations of the sums of the ordered labels and sum up the vectors of representations (we have discrete sums in this model);
%     \item \textbf{\emph{Sum of label vectors}} - for each moment of time we get representations of the ordered labels and sum up the representation vectors
% \end{itemize}

% The identifier vector is concatenated to the resulting latent state of the LSTM, and from the resulting vector we obtain confidence estimates for labels. This model is called ClassificationNet.


% The output of the ClassificationNet model is fed to the C2AE input. Inside C2AE(the idea of using this model and its training was proposed in~\cite{yeh2017learning}), it is first passed to the encoder, which translates it into the hidden space. The same procedure is performed for the label vector for the timestamp in question and for the label vector from the previous timestamp. Subsequently, the resulting vectors in this hidden space are used in a loss function that tries to minimize the distance between them:
% \[Loss_1 =||F_{x}(X) - F_{e}(Y_t)|| + \lambda ||F_{e}(Y_t) - F_{e}(Y_{t-1})||\]

% The next stage of this architecture is decoding, after which a vector of labels in the considered timestamp is obtained. Interestingly, using a loss function of the form:
% \[Loss_{2_{i}} = \frac{1}{|Y_i^{+}| |Y_i^{-}|}\sum_{(l,k)\in Y_i^{+} \times Y_i^{-}} exp(F_d(F_e(Y_i))^k - F_d(F_e(Y_i))^l),
% \]

% where $Y_i^{+}$ denotes a set of positive labels in $Y_i$ for the $i$-th instance of $x_i$, and $Y_i^{-}$ is a set of negative labels. Given the input data $x_i$, $F_d(F_e(x_i))^l$ returns the $l$-th record of the output CLASS2C2AE.

% Thus, minimizing the above loss function is equivalent to maximizing the prediction results of all pairs of attributes of a positive and negative label, which implicitly preserves information about the comparison of labels.The indicators are calculated using the obtained vector of confidence estimates for the considered timestamp.

% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
% %
% % \bibliographystyle{splncs04}
% % \bibliography{mybibliography}
% %
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}

% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
