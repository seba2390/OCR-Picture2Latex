\vspace{-0.1cm}
\section{Method}
\vspace{-0.1cm}
\label{sec:method}
The proposed architecture is comprised of three pre-trained and fixed encoders, namely: (i) content encoder; (ii) F0 encoder; (iii) speaker identity encoder; and a decoder network. The first two encoders extract discrete representation from the raw audio while the latter extracts a single global representation. The overall architecture is depicted in Figure~\ref{fig:arch}. 

\vspace{-0.2cm}
\subsection{Encoders}
\vspace{-0.1cm}
%In the following subsections we provide a detailed description for each of the modules.  
% \paragraph*{Content Encoder.}
Denote the domain of audio samples by $\Xc \subset \R$. The representation for a raw signal is therefore a sequence of samples $\vx = (x_1,\ldots, x_T)$, where  $x_t\in\Xc$ for all $1\leq t \leq T$. 

\noindent{\bf Content Encoder \quad} 
% Consider an encoder network, $E_c$, that gets as input the speech utterance and outputs 
The input to a content encoder network, $E_c$, is a speech utterance, $\vx$, and the output is 
a sequence of spectral representations sampled at a low frequency as follows $E_c(\vx) = (\vv_1, \dots, \vv_{T'})$. We evaluated three state-of-the-art unsupervised representation learning functions as $E_c$. Specifically, we experimented with: (i) CPC~\cite{oord2018representation} which attempts to predict the future states of the encoder based on the past and optimizes a contrastive loss comparing the actual future from that of random sequences; (ii) HuBERT~\cite{hsu2020hubert} which was trained with a masked prediction task similar to BERT~\cite{devlin-etal-2019-bert} on masked continuous audio signals as inputs. The targets are obtained through clustering of raw speech features or learned features from earlier iterations; (iii) and VQ-VAE~\cite{van2017neural} which performs similarly to a Variational Auto Encoder~\cite{kingma2013auto} where the encoder's output is discrete rather than continuous. 

Since the representations learned by CPC and HuBERT are continuous, a k-means algorithm is applied over the models' outputs to generate discrete units, denoted as $\vz_c = (z_1,\ldots,z_{L})$. Each element $z_i$ in $\vz_c$ is a positive integer, $z_i\in\{0,1,..,K\}$ for $1\le i \le L$, where $K$ is the number of discrete units. We did not follow the same approach for VQ-VAE as its representations are already quantized.

% We did not follow the same approach for VQ-VAE as its representations are already quantized. 

% \paragraph*{F0 Encoder.}
% \smallskip
% \noindent{\bf F0 Encoder.\quad} 
% To generate low frequency discrete F0 representations a separate encoder, denote as $E_{F_0}$, is applied over the F0 extracted from the input signal. The YAAPT~\cite{yaapt} algorithm is used to extract the F0,~$\vp = (p_i, \dots, p_{T'})$. 

% Specifically, the VQ-VAE module is used as $E_{F_0}$. Denote its output as $Z_{F_0} = (z_s, \dots, z_L)$. Similar to Jukebox~\cite{jukebox}, the VQ-VAE employs a one dimensional convolutional encoder which extracts a sequence of latent vectors ${\bf{h}} = (h_s, \dots, h_L)$, a bottleneck which maps each latent vector in ${\bf{h}}$ to its nearest vector $e_{z_s}$, from the learned codebook $C = (e_k, \dots, e_K)$, denoted as $\ve$, and a decoder $D_{F_0}(\ve)$ that reconstructs the original signal from the embedded latent vectors. Overall, the VQ-VAE is trained by minimizing the following loss:

% Specifically, the VQ-VAE module is used as $E_{F_0}$. Denote its output as $\vz_{F_0} = (z_s, \dots, z_{L'})$, where $z_s\in\{0,1,..,K'\}\subset\Z^+$ for $1\le s \le L'$, where $K'$ is $E_{F_0}$ dictionary size. The VQ-VAE employs a convolutional encoder, a bottleneck with a learned codebook $C = (e_k, \dots, e_{K'})$, and a decoder $D_{F_0}$. The encoder extracts a sequence of latent vectors ${\bf{h}} = (h_s, \dots, h_{L'})$ from the raw audio, where $h_s \in \R^128$. Then, the bottleneck maps each a latent vector in ${\bf{h}}$ to its nearest vector $e_{z_s} \in \R^128$, from the learned codebook $C$. The embedded latent vectors, $\ve$, are then fed to the decoder $D_{F_0}(\ve)$ which reconstructs the original signal. Overall, the VQ-VAE is trained by minimizing the following loss:

% \smallskip
\noindent{\bf F0 Encoder \quad} 
To generate low frequency discrete F0 representation, $\vz_{F_0}=(z_1, \dots, z_{L'})$, a separate encoder, $E_{F_0}$, is applied over the F0 extracted from the input signal. Each element in $\vz_{F_0}$ is an integer $z_s\in\{0,1,..,K'\}$, where $K'$ is the encoder dictionary size. The YAAPT~\cite{yaapt} algorithm is used to extract the F0 from the input signal, $\vx$, generating~$\vp = (p_1, \dots, p_{T'})$. 

$E_{F_0}$ is trained using the VQ-VAE framework. VQ-VAE employs a convolutional encoder, $E_{F_0}$,  a bottleneck with a learned codebook $C = (\ve_1, \dots, \ve_{K'})$, where each item in $C$ is a 128-dimensional vector, and a decoder $D_{F_0}$. The encoder extracts a sequence of latent vectors $E_{F_0}(\vp)=({\bf{h}}_1, \dots, {\bf{h}}_{L'})$ 
from the raw audio, where ${\bf{h}}_i \in \R^{128}$, for all $1 \le i \le L'$. 
Then, the bottleneck maps each latent vector to its nearest vector in the codebook $C$. The embedded latent vectors are then being fed into the decoder $D_{F_0}(\ve_{z_1}, \dots, \ve_{z_{L'}}) = \vph$ which reconstructs the original F0 signal. Similar to~\cite{jukebox}, we use Exponential Moving Average updates to learn the codebook and employ random restarts for unused embeddings. 

% Overall, the VQ-VAE is trained by minimizing the following loss:
% \begin{equation}
% \begin{aligned}
%     &L(E_{F_0}, C, D_{F_0}) = L_{recon} + \beta L_{commit}, \\
%     &L_{recon}(E_{F_0}, C, D_{F_0}) = \frac{1}{T'}\sum_{t=1}^{T'} \| p_t - D_{F_0}(e_{t}) \|^2_2 \\
%     &L_{commit}(E_{F_0}, C) = \frac{1}{L'}\sum_{s=1}^{L'} \| h_s - \text{sg}[e_{z_s}] \|^2_2,
% \end{aligned}
% \end{equation}
% where $\texttt{sg}$ is the stop gradient operation, detaching the embedding from the loss backpropagation. %The discrete representation is achieved by taking the indices of the embedded latent vectors $\ve$.

% The reconstruction term reduces the distance between the input and the generated signal, while the commitment term ensures the latent representation remains close to the codebook vectors. 
To generate $\vz_{F0}$, we use the indices of the mapped latent vectors rather than the vectors. 

% \paragraph*{Speaker Encoder.}
% \smallskip
\noindent{\bf Speaker Encoder \quad} 
\label{sec:spkr}
Lastly, a speaker encoder, $E_{spk}$, is used to extract speaker embedding. A pre-trained speaker verification model similar to the one proposed in~\cite{heigold2016end} is used. Formally, $E_{spk}$ gets as input the speech utterance, $\vx$, extracts the Mel-spectrogram and outputs a \emph{d-vector} speaker representation, denoted as $\vz_{spk} \in \R^{256}$. We experimented with learning the speaker embedding via a lookup-table. Although such method performs slightly better, it is limited to speakers seen during training.

% \begin{table}[t!]
% \centering
% \caption{Speech resynthesis results. MOS results are reported with 95\% confidence interval. Except MOS for all other evaluation metrics the lower is the better. We did not report EER for LJ since it is a single speaker dataset.}
% \label{tab:recon}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c@{~} | c@{~} | c@{~} c@{~} | c@{~}c@{~}c@{~} | c@{~} | c@{~}}
% \toprule
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Content} & \multicolumn{3}{c|}{F0} &  Speaker & Overall Quality \\
% \cmidrule{3-9}
% \cmidrule{3-9}
%  &  & PER~$\downarrow$ & WER~$\downarrow$ & GPE~$\downarrow$ & VDE~$\downarrow$ & FFE~$\downarrow$ &  EER~$\downarrow$ & MOS~$\uparrow$ \\
% \midrule
% \multirow{5}{*}{LJ}
% & GT        & 6.93 & 5.60 & -- & -- & -- & -- & 4.33$\pm$0.20 \\
% & {\color{red}ASR-TTS}   & {\color{red}9.78}  & {\color{red}7.43} & -- & -- & -- & -- &  \\
% & CPC       & 9.66  & 8.51 & 4.25 & 13.48 & 15.19 & -- & 3.31$\pm$0.33 \\
% & HuBERT    & 9.52  & 6.96 & 6.84 & 13.09 & 15.00 & -- & 3.66$\pm$0.33\\
% & VQ-VAE    & 12.77 & 8.85 & 3.04 & 7.19 & 8.54 & -- & 3.66$\pm$0.31\\
% \midrule 
% \multirow{5}{*}{VCTK} 
% & GT        & 17.16 & 4.32 & -- & -- & -- & 3.25 & 4.08$\pm$0.66\\
% & {\color{red}ASR-TTS}   & {\color{red}29.75} & {\color{red}8.93} & -- & -- & -- & {\color{red}7.27} &  \\
% & CPC       & 23.01 & 14.49 & 2.66 & 10.56 & 11.13 & 4.25 & 3.33$\pm$0.61\\
% & HuBERT    & 19.66 & 11.44 & 3.12 & 9.77  & 10.43 & 5.79 & 3.41$\pm$0.66\\
% & VQ-VAE    & 31.97 & 19.80 & 1.53 & 5.20 & 5.59 &  4.28 & 3.39$\pm$0.58\\
% \bottomrule
% \end{tabular}
% }
% \end{table}

\begin{table}[t!]
\centering
\caption{Speech resynthesis results. MOS results are reported with 95\% confidence interval. Except MOS for all other evaluation metrics the lower is the better. We did not report EER for LJ since it is a single speaker dataset.}
\label{tab:recon}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c@{~} | c@{~} | c@{~} c@{~} | c@{~}c@{~} | c@{~} | c@{~}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Content} & \multicolumn{2}{c|}{F0} &  Speaker & Overall Quality \\
\cmidrule{3-8}
\cmidrule{3-8}
 &  & PER~$\downarrow$ & WER~$\downarrow$ & VDE~$\downarrow$ & FFE~$\downarrow$ &  EER~$\downarrow$ & MOS~$\uparrow$ \\
\midrule
\multirow{4}{*}{LJ}
& GT        & 6.93 & 5.60 & -- & -- & -- & 4.33$\pm$0.20 \\
\cmidrule{2-8}
% & {\color{red}ASR-TTS}   & {\color{red}9.78}  & {\color{red}7.43} & -- & -- & -- &  \\
& CPC       & 9.66  	& 8.51 		& 13.48 		& 15.19 		& -- & 3.31$\pm$0.33 \\
& HuBERT    & \bf 9.52  & \bf 6.96 	& 13.09 		& 15.00 		& -- & \bf 3.66$\pm$0.33\\
& VQ-VAE    & 12.77 	& 8.85 		& \bf 7.19 	& \bf 8.54 	& -- & \bf 3.66$\pm$0.31\\
\midrule 
\multirow{4}{*}{VCTK} 
& GT        & 17.16 & 4.32 & -- & -- & 3.25 & 4.08$\pm$0.66\\
\cmidrule{2-8}
% & {\color{red}ASR-TTS}   & {\color{red}29.75} & {\color{red}8.93} & -- & -- & {\color{red}7.27} &  \\
& CPC       & 23.01 	& 14.49 		& 10.56 		& 11.13 		& \bf 4.25 	& 3.33$\pm$0.61\\
& HuBERT    & \bf 19.66 & \bf 11.44 & 9.77  		& 10.43 		& 5.79 		& \bf 3.41$\pm$0.66\\
& VQ-VAE    & 31.97 	& 19.80 		& \bf 5.20 	& \bf 5.59 	&  4.28 		& 3.39$\pm$0.58\\
\bottomrule
\end{tabular}
}
\vspace{-0.4cm}
\end{table}

\vspace{-0.2cm}
\subsection{Decoder}
\vspace{-0.1cm}
% To decode the speech signal from its encoded representation, a neural vocoder is employed. 
A neural vocoder is employed to decode the speech signal from the discrete representation.
This study considers the decoder to be a modified version of the HiFi-GAN~\cite{kong2020hifi} neural vocoder. 

The HiFi-GAN architecture is comprised of a generator, $G$, and a set of discriminators, $D$. The generator is built from a set of look-up tables~(LUT) that embed the discrete representation and a series of blocks composed of transposed convolution and a residual block with dilated layers. The transposed convolutions upsample the encoded representation to match the input sample rate, while the dilated layers increase the 
% model's 
receptive field. 

As an input, the generator receives the encoded representation $(\vz_c, \vz_{F_0}, \vz_{spk})$. The discrete content sequence, $\vz_c$, and the discrete pitch sequence, $\vz_{F_0}$, are converted to a continuous representation via $LUT_{c}$ and $LUT_{F_0}$ accordingly. The sequences are up-sampled and concatenated together. The speaker embedding, $\vz_{spk}$, is concatenated to each frame in the up-sampled sequence.  
% The concatenation is then fed to the series of transposed convolutions.

The discriminator comprises two networks, a Multi-Period Discriminator~(MPD) and a Multi-Scale Discriminator~(MSD). The MPD consists of multiple sub-discriminators operating on equally spaced samples from the input signal. The period sub-discriminators differ from each other based on the space between the samples. Similar to~\cite{kong2020hifi}, the MPD employs a total of five-period discriminators with a period hops of $[2, 3, 5, 7, 11]$. Multi-scale discriminator (MSD) employs multiple sub-discriminators operating at different scales of the input signal. Specifically, we use three scales: the original input scale, $\times2$ downsampled scale, and $\times4$ downsampled scale. Overall each sub-discriminator $D_j$ is tasked with minimizing the following,
\begin{equation}
\label{eq:discriminator}
\begin{aligned}
    &L_{adv}(D_j, G) = \sum_{\vx}|| 1 - D_j(\vxh) ||_2^2, \\
    &L_D(D_j, G) = \sum_{\vx}{[|| 1 - D_j(\vx) ||_2^2 + || D_j(\vxh) ||_2^2]},
\end{aligned}
\end{equation}
where $\vxh=G(LUT_{c}(\vz_c), LUT_{F_0}(\vz_{F_0}), \vz_{spk})$, is the resynthesized signal from the encoded representation.

% In parallel, $G$ optimizes two more auxiliary loss functions. 
Additionally, two terms are added to the loss function. 
The first one is a reconstruction term computed between the Mel-spectrogram of the input signal and the generated signal, 
% $L_{recon}(G) = \sum_{\vx}|| \phi(\vx) - \phi(\vxh) ||_1$,
\begin{equation}
\label{eq:recon}
 L_{recon}(G) = \sum_{\vx}|| \phi(\vx) - \phi(\vxh) ||_1,
\end{equation}
where $\phi$ is a spectral operator computing Mel-spectrogram. The second term is a feature-matching loss~\cite{larsen2016autoencoding} which measures the distance between discriminator activations of the real signal and those of the resynthesized signal,
% $L_{fm}(D_j, G) = \sum_{\vx}\sum_{i=1}^{R}\frac{1}{M_i}|| \psi_i(\vx) - \psi_i(\vxh) ||_1$,
\begin{equation}
\label{eq:fm}
L_{fm}(D_j, G) = \sum_{\vx}\sum_{i=1}^{R}\frac{1}{M_i}|| \psi_i(\vx) - \psi_i(\vxh) ||_1,
\end{equation}
where $\psi_i$ is an operator which extracts the activations of the discriminator $i$-th layer, $M_i$ is the number of features in layer $i$, and $R$ is the total number of layers in $D_j$. 

% The objective function of the neural vocoder, $G$, is to minimize the following,
% \begin{equation}
% \label{eq:generator}
% \begin{aligned}
%     L_{G}(D_j, G) = &L_{adv}(D_j, G) + \\ 
%     &\lambda_{fm}L_{fm}(D_j, G) + \lambda_{r}L_{recon}(G),
% \end{aligned}
% \end{equation}
% where we set $\lambda_{fm}=2$ and $\lambda_{r}=45$. 

% Finally, we update Eq.~\ref{eq:discriminator} and Eq.~\ref{eq:generator} with respect to the sub-discriminators composing discriminator, $D$:
% \begin{equation}
% \begin{aligned}
% L_G^{multi}(D, G) = &\sum_{j=1}^{J}[L_{adv}(G, D_j) + \lambda_{fm}L_{fm}(G, D_j)], \\ 
%                     & + \lambda_{r}L_{recon}(G), \\
% L_D^{multi}(D, G) = &  \sum_{j=1}^{J} L_D(G, D_j).
% \end{aligned}
% \end{equation}
% \vspace{-0.3cm}
% where $D_j$ is the $j$-th sub-discriminator of $D$.

The final loss with respect to the sub-discriminators composing discriminator $D$ and generator $G$ is:
\begin{equation}
\begin{aligned}
L_G^{multi}(D, G) = &\sum_{j=1}^{J}[L_{adv}(G, D_j) + \lambda_{fm}L_{fm}(G, D_j)], \\ 
                    & + \lambda_{r}L_{recon}(G), \\
L_D^{multi}(D, G) = &  \sum_{j=1}^{J} L_D(G, D_j),
\end{aligned}
\vspace{-0.2cm}
\end{equation}
\vspace{-0.1cm}
where we set $\lambda_{fm}=2$ and $\lambda_{r}=45$.
% where $D_j$ is the $j$-th sub-discriminator of $D$.
