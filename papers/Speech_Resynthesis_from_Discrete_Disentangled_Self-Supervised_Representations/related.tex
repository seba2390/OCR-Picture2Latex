\begin{figure}[t!]
\centering
\includegraphics[scale=0.4,trim={50 20 70 20}]{figures/fig_thin.png}
\caption{\textbf{The overall proposed speech resynthesis architecture.} Three parallel encoders extract discrete representations from the raw input signal. These are then being used as a conditioning to reconstruct the signal using a decoder network.} 
\label{fig:arch}
\vspace{-0.6cm}
\end{figure}

\vspace{-0.1cm}
\section{Related Work}
\vspace{-0.1cm}
% start with related work regarding speech representations (VAE, CPC, WAV2VEC, HuBERT, etc.) and the tacl paper, and the zero speech challenge 
% \paragraph*{Unsupervised Speech Representation Learning.} 
\noindent{\bf Unsupervised Speech Representation Learning\quad}
Studies on unsupervised speech representation learning can roughly be divided into reconstruction and self-supervised learning methods. 
%  The common approach for signal reconstruction is auto-encoding, 
Auto-encoding is the common approach for signal reconstruction,
where speech is first encoded into a low-dimensional latent representation, and then decoded back to speech. Various constraints can be imposed on the encoded space, such as temporal smoothness~\cite{ebbers2017hidden}, discreteness~\cite{van2017neural}, and hierarchy~\cite{hsu2017unsupervised}.

SSL methods have shown remarkable results for ASR~\cite{schneider2019wav2vec, baevski2020wav2vec}, phoneme segmentation~\cite{kreuk2020self}, and GSLM~\cite{lakhotia2021generative}. The authors in~\cite{oord2018representation, schneider2019wav2vec} suggested training a convolutional neural network to distinguish true future samples from random distractor samples using a Contrastive Predictive Coding (CPC) loss function. Similar to CPC, the authors in~\cite{baevski2020wav2vec} use an encoder and a predictor, which is trained contrastively to distinguish positive and negative samples. However, unlike~\cite{schneider2019wav2vec} it discretizes and masks segments of the encoder's output. In HuBERT~\cite{hsu2020hubert}, the model is trained with a masked prediction task similar to BERT~\cite{devlin-etal-2019-bert} but with masked continuous audio signals. 

% text to speech
% describe some disentangled speech representation work 
% Controllable neural text-to-speech synthesis using intuitive prosodic features
% \paragraph*{Speech Resynthesis.}
\noindent{\bf Speech Resynthesis \quad}
Recent advancements in neural-based vocoders enabled generating natural-sounding speech and music~\cite{oord2016wavenet, prenger2019waveglow, kong2020hifi}. These are often conditioned on the log Mel-spectrogram for the generation process. 
% Learning low bitrate speech representations in an unsupervised manner is widely explored recently, mainly under the Zero-Resource Challenge~\cite{dunbar2019zero, dunbar2020zero}.
Recently, unsupervised learning of low bitrate speech representations was explored under the Zero-Resource Challenge~\cite{dunbar2019zero, dunbar2020zero}.
Vector-Quantized Variational Auto-Encoder (VQ-VAE)~\cite{van2017neural} employs a learned fixed-sized codebook, decoded by a WaveNet model for speech synthesis. In~\cite{eloff2019unsupervised} the authors proposed a VQ-VAE model followed by an FFTNet vocoder model~\cite{jin2018fftnet}. The authors in~\cite{tjandra2020transformer} suggested using transformer~\cite{trans} together with a VQ-VAE model for unsupervised unit discovery, and in~\cite{van2020vector} they combine vector quantization with contrastive predictive coding for acoustic unit discovery. The study by~\cite{zhao2020improved} is the closest to our work. In which, the authors suggest training a VQ-VAE with two encoders, one for the waveform and the other for F0. The authors demonstrated how such modeling improves overall generation quality. In contrast, we study SSL-based speech encoders and empirically show these representations are better disentangled, and apply them as an ultra-low bitrate speech codec. Another line of work suggests using intermediate representations obtained from an ASR acoustic model. These representations are being used together with the identity and prosodic information for voice conversion~\cite{polyak2019tts, polyak2020unsupervised, polyak2021high}. Unlike all of the above, we suggest synthesizing speech directly from the discrete units. Moreover, the resynthesis process sheds light on the encoded information in each of the evaluated representations. 

% \paragraph*{Speech Codec.}
% \smallskip
\noindent{\bf Speech Codec \quad}
% lastly, some speech codec work (both AI and non-AI based)
Speech codecs typically employ a carefully hand-engineered pipeline combining an encoder and a decoder 
% that considers the physics of speech production 
which is influenced by speech production physics
to remove redundancies in the data and yield a compact bitstream. Low bitrate parametric speech codecs have long been studied~\cite{atal1971speech}, but their quality has been severely limited. Despite some advances~\cite{griffin1985new, mccree19962}, modeling the excitation signal has remained a challenge. Neural speech codecs have been recently proposed and demonstrated promising results~\cite{kleijn2018wavenet, lim2020robust}. 
In~\cite{valin2019real} an LPCNet~\cite{valin2019lpcnet} vocoder was conditioned on hand-crafted features and a uniform quantizer. In~\cite{garbacea2019low} a WaveNet model was conditioned on discrete units obtained from a VQ-VAE model, while in~\cite{skoglund2019improving} the Opus codec~\cite{valin2012definition} was fed to WaveNet. % module.