% our contribution: 
% 1) Demonstrate the usage of SSL units for synthesis purposes (no Tacotron model)
% 2) Directly evaluate SSL units from a TTS point of view, Voice Conversion, and F0 manipulation
% 3) Demonstrate the usage of such units for speech codec

\vspace{-0.1cm}
\section{Introduction}
\vspace{-0.1cm}
Learning unsupervised speech representations, both continuous and discrete, has seen a significant leap in performance following the recent success of Self-Supervised Learning (SSL) methods~\cite{oord2018representation, schneider2019wav2vec, baevski2020wav2vec, hsu2020hubert}. In the self-supervised setting, unlabeled inputs define an auxiliary task that can generate pseudo-labeled training data. This data can then be used to train a model using supervised techniques. %The learned representations can then be used for downstream tasks with or minimal amount of supervised data. For example, with 10 minutes of transcribed speech and 53K hours of untranscribed speech, wav2vec 2.0 enables speech recognition models at a word error rate (WER) of 8.6 percent on noisy speech and 5.2 percent on clean speech on the standard LibriSpeech benchmark~\cite{baevski2020wav2vec}.
The learned representations are often used for downstream tasks with a minimal amount of supervised data. For example, learning  speech recognition with only 10 minutes of transcribed speech and 53K hours of untranscribed speech as in wav2vec 2.0~\cite{baevski2020wav2vec} and HuBERT~\cite{hsu2020hubert}. The learned self-supervised discrete representations also showed impressive performance on the conditional and unconditional spoken generative language modeling (GSLM) task~\cite{lakhotia2021generative}.

Despite its success, most studies on SSL for speech are focused on generating and evaluating the quality of the learned representations in the context of Automatic Speech Recognition (ASR). It remains unclear how suitable these representations are for speech synthesis. Moreover, in the context of expressive and controllable generation, it is unknown to what extent the speaker identity and F0 information are encoded in the learned representations. 

% Traditionally, speech synthesis and text-to-speech models get as input textual features and generate Mel-spectrogram autoregressively. 
Traditionally, speech synthesis and text-to-speech models produce Mel-spectrogram autoregressively given textual features as input.
Next, a vocoder is applied to reconstruct the phase from the Mel-spectrogram (e.g., Griffin-Lim~\cite{griffin1984signal}, WaveNet~\cite{oord2016wavenet}, WaveGlow~\cite{waveglow}, or HiFi-GAN~\cite{kong2020hifi}). In this study, we suggest using the learned speech units as an input to a vocoder module with no spectrogram estimation. We additionally augment the learned units with quantized F0 representation and a global speaker embedding. Figure~\ref{fig:arch} presents the overall proposed method.
% This allows to evaluate the encoded properties in the learned units when considering speech content, speaker identity, and F0 information, as well as better control the audio synthesis. 
This allows the evaluation of the learned units with respect to speech content, speaker identity, and F0 information, as well as better control the audio synthesis. 
We experiment with signal reconstruction, voice conversion, and F0 manipulation using several datasets and encoder models. Finally, equipped with our previous findings, we demonstrate how the learned units can function as an ultra-lightweight speech codec. Following the proposed method, we can reach an encoding rate of 365 bits per second~(bps) while being superior to the baseline methods by a significant margin, including lightweight and heavyweight codecs. 

% \paragraph*{Our Contribution:}
\noindent{\bf Our Contribution:\quad}
% for the first time 
(i) We demonstrate the usage of discrete speech units, learned in a self-supervised manner, for high-quality synthesis purposes (no Mel-spectrogram estimation); (ii) We provide an extensive evaluation of the SSL speech units from a synthesis point of view, i.e., signal reconstruction, voice conversion, and F0 manipulation; (iii) We build an ultra-lightweight speech codec from the obtained speech units.
