\begin{table}[t!]
\centering
\caption{Voice conversion \& F0 manipulation results. MOS results are reported with 95\% confidence interval. VDE, and FFE are reported for F0 manipulation while PER, WER, EER, and MOS are reported for voice conversion. Notice, for VDE, and FFE higher is the better since F0 was flattened.}
\label{tab:conv}

\resizebox{1\columnwidth}{!}{
\begin{tabular}{c@{~} | c@{~} | c@{~}c@{~} | c@{~} | c@{~} ||  c@{~}c@{~} }
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{4}{c||}{Voice Conversion} & \multicolumn{2}{c}{F0 Manipulation} \\
\cmidrule{3-8}
& & PER~$\downarrow$ & WER~$\downarrow$ & EER~$\downarrow$ & MOS~$\uparrow$ & VDE~$\uparrow$ & FFE~$\uparrow$ \\
\midrule
VCTK & GT  & 17.16 & 4.32 & 3.25 & 4.11$\pm$0.29 & -- & -- \\
\midrule 
\multirow{3}{*}{LJ}
% & ASR-TTS   & 50.74  & --     & 66.08 & 32.96 & 1.46 \\
& CPC       & 22.22 	& 16.11 		& 0.46 		& 3.57$\pm$0.15 		& \bf 46.68 & \bf 48.71\\
& HuBERT    & \bf 19.09 & \bf 12.23 & \bf 0.31  & \bf 3.71$\pm$0.24 & 39.20 		& 48.42\\
& VQ-VAE    & 40.88 	& 36.96 		& 9.65 		& 2.90$\pm$0.17 		& 10.54 	& 12.08 \\
\midrule 
\multirow{3}{*}{VCTK} 
% & ASR-TTS   & 68.88  & --    & 41.77 & 13.55 & 6.48 \\
& CPC       &  23.58 		& 15.98 		& \bf 4.83  &  3.42 $\pm$ 0.24 		& \bf 25.29 & \bf 26.97 \\
& HuBERT    &  \bf 20.85 	& \bf 12.72 & 6.01  		& \bf  3.58 $\pm$ 0.28 	& 23.46 	& 26.67 \\
& VQ-VAE    & 36.88  		& 29.44 		& 11.56 		& 3.08 $\pm$ 0.34 		& 7.03  	& 7.80  \\
\bottomrule
\end{tabular}}
\vspace{-0.4cm}
\end{table}

\vspace{-0.1cm}
\section{Results}
\vspace{-0.1cm}
Our results cover
% We report results for 
three different settings: (i) speech reconstruction experiments; (ii) speaker conversion and F0 manipulation; (iii) bitrate analysis with subjective tests for speech codec evaluation. We employ two datasets: LJ~\cite{ljspeech17} single speaker dataset and VCTK~\cite{vctk} multi-speaker dataset. All datasets were resampled to a 16kHz sample rate.

% \paragraph*{Implementation Details.}
% \smallskip
\noindent{\bf Implementation Details\quad} 
\label{sec:impl}
We follow the same setup as in~\cite{lakhotia2021generative}. For CPC, we used the model from~\cite{Riviere2020}, which was trained on a ``clean'' 6k hour sub-sample of the LibriLight dataset~\cite{Kahn2020,Riviere2020}. We extract a downsampled representation from an intermediate layer with a 256-dimensional embedding and a hop size of 160 audio samples. For HuBERT we used a \textsc{Base} 12 transformer-layer model trained for two iterations~\cite{hsu2020hubert} on 960 hours of LibriSpeech corpus~\cite{Panayotov2015}. 
% This model encodes every 320 raw audio samples into a 768-dimensional vector. 
This model downsamples the raw audio $\times320$ into a sequence of 768-dimensional vectors. Similarly to~\cite{lakhotia2021generative}, activations were extracted from the sixth layer.

%CPC: We use a dictionary of 100 units, leading to a bitrate of 700bps.
%HuBERT: A dictionary of 100 units is used, leading to a bitrate of 350bps. 
%VQVE: The VQ-VAE discrete code operates at a bitrate of 800bps.
% For both CPC and HuBERT, the k-means algorithm is applied to convert continuous frames to discrete codes, using the LibriSpeech clean-100h~\cite{Panayotov2015} dataset. 
For CPC and HuBERT, the k-means algorithm is trained on LibriSpeech clean-100h~\cite{Panayotov2015} dataset to convert continuous frames to discrete codes. We quantize both learned representations with $K=100$ centroids. Leading to a bitrate of 700bps for CPC and 350bps for HuBERT.

% VQ-VAE
Similarly to CPC models, we trained the VQ-VAE content encoder model on the ``clean'' 6K hours subset from the LibriLight dataset. We use an encoder operating on the raw signal to extract discrete units, similar to~\cite{jukebox}. In addition, ``random restarts'' were performed when the mean usage of a codebook vector fell below a predetermined threshold. Finally, we used HiFiGAN (architecture and objective) as the decoder instead of a simple convolutional decoder, as it improved the overall audio quality. This model encodes the raw audio into a sequence of discrete tokens from 256 possible tokens~\cite{garbacea2019low} with a hop size of 160 raw audio samples. The VQ-VAE discrete code operates at a bitrate of 800bps. We additionally experimented with 100 discrete units for VQ-VAE, however results were the best for 256. This finding is consistent with~\cite{garbacea2019low}.

% verification model
The speaker verification network uses the architecture proposed in~\cite{heigold2016end}. It was trained on the VoxCeleb2~\cite{voxceleb2} dataset, achieving a 7.4\% Equal Error Rate (EER) for speaker verification on the test split of the VoxCeleb1~\cite{Nagrani17} dataset.

% pitch
Only a single F0 representation is considered across all evaluated models, trained on the VCTK dataset.
% The F0 is extracted from the raw audio using YAAPT~\cite{yaapt} algorithm, using a window size of 20ms and a 5ms hop. 
The F0 is extracted from the raw audio using a window size of 20ms and a 5ms hop. 
As a result, the F0 sequence is sampled at 200Hz. 
% We apply the quantization described at Sec.~\ref{sec:method}, using a pitch codebook of $K'=20$ tokens and an encoder that downsamples the pitch by $\times16$. 
The quantization described at Sec.~\ref{sec:method}, is applied using an F0 codebook of $K'=20$ tokens and an encoder that downsamples the signal by $\times16$. Hence, the discrete F0 representation is sampled at 12.5Hz, leading to a bitrate of 65bps. The final bitrate of the evaluated codecs is the sum of the pitch code bitrate with the content code bitrate.

% \paragraph*{Evaluation Metrics}
% \smallskip
\noindent{\bf Evaluation Metrics\quad} 
We consider both subjective and objective evaluation metrics. For subjective tests, we report the Mean Opinion Scores (MOS). In which human evaluators rate the naturalness of audio samples on a scale of 1--5. Each experiment, included 50 randomly selected samples rated by 30 raters. For objective evaluation, we consider: (i) Equal Error Rate~(EER) as an automatic speaker verification metric obtained using a pre-trained speaker verification network. We report EER between test utterances and enrolled speakers; (ii) Voicing Decision Error (VDE)~\cite{nakatani2008method}, which measures the portion of frames with voicing decision error; (iii) F0 Frame Error (FFE)~\cite{chu2009reducing}, measures the percentage of frames that contain a deviation of more than 20\% in pitch value or have a voicing decision error; (iv) Word Error Rate (WER) and Phoneme Error Rate (PER), proxy metrics to the intelligibility of the generated audio. We used a pre-trained ASR network~\cite{baevski2020wav2vec} on both reconstructed and converted samples to calculate both metrics. %To generate target phonemes, the g2p-en~\cite{g2pE2019} Grapheme2Phoneme module was used.

% \vspace{-0.1cm}
% \smallskip
\noindent{\bf Reconstruction \& Conversion}
% \vspace{-0.1cm}
We start by reporting the reconstruction performance. Results are summarized in Table~\ref{tab:recon}. When considering the intelligibility of the reconstructed signal HuBERT reaches the lowest PER and WER scores across all models, where both CPC and HuBERT are superior to VQ-VAE. However, when considering F0 reconstruction VQ-VAE outperforms both HuBERT and CPC by a significant margin. This results are somewhat intuitive, bearing in mind VQ-VAE objective is to fully reconstruct the input signal. In terms of subjective evaluation, all models reach similar MOS scores, with one exception of CPC on LJ. 

%Notice, since the same F0 units are used for each method, this result implies the VQ-VAE units contain some information about the F0 of the signal, enabling better reconstruction. Regarding speaker information, the CPC gets the lowest EER. 

To better evaluate the disentanglement properties of each method with respect to speaker identity and F0, we conducted an additional set of experiments aiming at speaker conversion and F0 manipulation. For voice conversion, we converted each test utterance into five random target speakers. Next, we employed a speaker verification network, which extracts \emph{d-vector} representation to evaluate speaker-converted utterances' similarity to real speaker utterances (low error-rate indicates good conversion), providing measurement to the speaker identity's disentanglement from the evaluated coding method. The error-rate is reported between converted test utterances and enrolled speakers. For the LJ speech single speaker dataset, we converted samples from the VCTK dataset to the single speaker and enrolled all VCTK speakers together with the single speaker. Results are summarized in Table~\ref{tab:conv} (left). Unlike resynthesis results, on voice conversion CPC and HuBERT outperform VQ-VAE on both LJ and VCTK datasets, indicating VQ-VAE contains more information about the speaker in the encoded units, hence producing more artifacts. Notice, this also affects WER, PER, and the overall subjective quality (MOS). 

Next, to evaluate the presence of F0 in the discrete units, we flattened the F0 units before synthesizing the signal and calculated VDE and FFE with respect to the original F0 values. F0 flattening was done by setting the speakers' mean F0 value across all voiced frames. In this experiment, we expected units that contain F0 information to be better at F0 reconstruction over disentangled units. Results are summarized in Table~\ref{tab:conv} (right). Notice VQ-VAE can still reconstruct the F0 almost at the same level as when using the original F0 as conditioning (5.2 vs 7.03, and 5.59 vs 7.8), in contrast to CPC and HuBERT.

\begin{figure}[t!]
\centering
\includegraphics[width=0.65\columnwidth, trim={50 20 70 20}]{figures/codec_2.pdf}
% \caption{MUSHRA subjective listening test results as a function of bitrate per second for various methods. Purple dots denote the baseline methods, and green dots the proposed SSL based method.} 
\caption{MUSHRA subjective quality results as a function of bitrate per second. Purple dots denote the baseline methods, and green dots the proposed SSL based method.} 
\label{fig:codec}
\vspace{-0.5cm}
\end{figure}

% \vspace{-0.1cm}
% \smallskip
\noindent{\bf Speech Codec}
Our final experiment evaluates the obtained speech units as a low bitrate speech codec. 
% Therefore, we evaluate how the performance varies as a function of the number of discrete units. Changing the number of units is equivalent to varying the bitrate of the encoded signal. 
We use a subjective MUSHRA-type listening test~\cite{series2014method} to measure the perceived quality of the proposed speech codec with regard to its bitrate constraints. In MUSHRA evaluations, listeners are presented with a labeled uncompressed signal for reference, a set of test samples to rate, a copy of the uncompressed reference, and a low-quality anchor. Listeners are asked to rate each test utterance and the copy of the uncompressed reference with respect to the labeled reference in a scale of 1-100.

The experiment is performed on the VCTK dataset~\cite{vctk}. For evaluation, we used 20 utterances from 5 speakers. The set of speakers in the test data is disjoint with those in the training data. For this experiment, HuBERT models with 50, 100, and 200 units were trained as described in Sec.~\ref{sec:impl}. For comparison, we included other speech codecs in our evaluation: Opus~\cite{valin2012definition} wideband at 9 kbps VBR, Codec2~\cite{rowe2011codec} at 2.4 kbps and LPCNet~\cite{valin2019real} operating at 1.6 kbps. The LPCNet model was trained from scratch on the VCTK dataset following the experimental setup in~\cite{valin2019real}. The VQ-VAE model employs the HiFiGAN decoder trained on the LibriLight dataset to match the amount of data reported in~\cite{garbacea2019low}. We compressed the anchor sample with Speex~\cite{valin2016speex} at 4 kbps as a low anchor. Fig.~\ref{fig:codec} depicts the results. HuBERT with 50 units reaches the best MUSHRA score while its bitrate is only 365bps, which is significantly lower than the baseline methods.