\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig/latent-executor-overview.pdf}
    \caption{\small (a) An overview of \ours{} model architecture. (b), (c), and (d) present the details of the program decoder, latent executor, and the operation predictor. Note that the operation predictor is specialized for numerical calculation, and thus is not used for the Karel domain.}
    \label{fig:model-architecture}
    \vspace{-1em}
\end{figure}

\def\cL{\mathcal{L}}

\section{Program Synthesis with Learned Execution}
\vspace{-0.1in}
\label{sec:model-architecture}
%\yuandong{Follow the story of introduction and rewrite it. Provide a high-level introduction about different components of the architecture. Emphasize the core design which is neural executor.}
In this section, we present \ours{} which learns to represent the execution of partial programs to guide the synthesis process. Fig.~\ref{fig:model-architecture}(a) provides an overview of \ours{} model architecture which consists of two components, the \emph{program decoder} and the \emph{latent executor}. We present the core design below, and defer more details to Appendix~\ref{app:model-architecture} and Appendix~\ref{app:implementation-details}. 

\subsection{Model Overview}
\vspace{-0.1in}
At a high level, the program decoder (Fig.~\ref{fig:model-architecture}(b)) takes a latent vector $h_{t-1}$ that represents the generated partial program, the previous (generated) program token $p_{t-1}$, and outputs the latent vector $h_t$ and the next program token $p_t$ to be generated at time step $t$: 
\begin{equation}
(h_t, p_t) = \mathrm{ProgramDecoder}(h_{t-1}, p_{t-1}; IO_{t-1}) \label{eq:program-decoder}
\end{equation}
Here the recurrent model is conditioned on the IO pair $IO_{t-1}$. When $IO_t = IO := (I, O)$ for every $t$, i.e., $IO_t$ remains \emph{constant} over the entire recurrent generation process, Eqn.~\ref{eq:program-decoder} represents the standard recurrent architecture used in most autoregressive natural language models~\cite{hochreiter1997long,vaswani2017attention}, and is also used in prior works on program synthesis from input-output examples~\cite{devlin2017robustfill,bunel2018leveraging}. 

For program decoding, the decoder first takes two attention vectors $s_t^I$ and $s_t^O$ computed from IO pairs and latent vector $h_{t-1}$ via double attention~\cite{devlin2017robustfill}, and utilizes a max pooling layer to compute an aggregated embedding $m_t$ for all IO pairs
(Fig.~\ref{fig:model-architecture}(b)):
\begin{equation}
m_t=\mathrm{MaxPool}_{j \in \{1,2,...,K\}}(\mathrm{tanh}(W[s_t^{I(j)}; s_t^{O(j)}])) \label{eq:wo-operation-predictor}
\end{equation}
Here the superscript $(j)$ indicates that the representation is for the $j$-th IO pair, $[a; b]$ is vector concatenation of $a$ and $b$, and $W$ is a trainable matrix. To facilitate the prediction of long programs, we compute an attention vector $d_t$ over previously generated program tokens using the standard attention mechanism~\cite{bahdanau2014neural,luong2015effective}: 
\begin{equation} 
d_t=\mathrm{Attention}(m_t, \{p_0, ..., p_{t-1}\}) \label{eq:d-attention}
\end{equation}
Finally, the next token $p_t$ is sampled from $\mathbb{P}[p_t]=\mathrm{Softmax}(Vd_t)_{p_t}$ where $V$ is a trainable matrix.

\subsection{Latent Executor Design}
\vspace{-0.1in}
As shown in our experiments (Sec.~\ref{sec:exp}), the standard program decoder architecture may not be able to achieve strong performance in program synthesis when the program complexity increases. One main reason is that the standard program decoder only takes the initial IO pairs as the input without considering the program execution, thus the learned representation for the partial program does not effectively guide the synthesis process. Motivated by prior works that utilize execution traces for Karel program synthesis~\cite{chen2018execution,shin2018improving,sun2018neural}, in this paper, we introduce \emph{latent executor} (Fig.~\ref{fig:model-architecture}(c)) which maintains a second representation $\hat I_t$ during program decoding. Intuitively, $\hat I_{t-1}$ models the \emph{hypothetical input} of the partial program $p_{t\ldots T}$ so that its output becomes $O$. Given the estimated input $\hat I_{t-1}$ and the latent vector $h_t$, the latent executor returns $\hat I_{t}$ at the next time step $t$: 
\begin{equation}
    \hat I_t = \mathrm{LatentExecutor}(\hat I_{t-1}, h_t)
\end{equation}
The collection of $\{\hat I_t\}_{t=0}^T$ is the \emph{latent execution trace (\laet{})}. With the help of latent executor, we now use the IO pairs $IO_{t-1} := (\hat I_{t-1}, O)$ instead of $(I, O)$ for the program decoder (Eqn.~\ref{eq:program-decoder}).


%\yuandong{maybe we can mention more detailed observations why they do not work well?} \xinyun{Added.}
\subsection{End-to-end Training}
\vspace{-0.1in}
We train our model with supervised learning, by minimizing the sum of token prediction loss $\cL_{Prog}$, and the latent executor loss $\cL_{Exec}$: 
\begin{equation}
\cL=\cL_{Prog} + \cL_{Exec}
\end{equation}
Specifically, $\cL_{Prog} :=\sum_{t=1}^T\mathrm{Loss}(p_t, p^\star_t)$ is the step-by-step cross-entropy loss between the predicted programs $p_{1\ldots T}$ and the ground truth programs $p^\star_{1\ldots T}$.

For latent executor, since the semantics of partial programs (e.g., partial C programs) are not always well-defined, there is no step-by-step training supervision. However, the output of the executor should be consistent with the program specification after taking the annotated ground truth program as the input. Therefore, we set $\hat I_0 = I$ (true input) and minimize the distance between $\hat I_T$ and $O$ (true output) after the program finishes:
\begin{equation}
\cL_{Exec}=\mathrm{Loss}(\hat{I}_T, O)
\end{equation}
Note that $\cL_{Exec}$ does not rely on any assumptions of the partial program semantics, and thus is applicable to both domain-specific languages and general-purpose programming languages such as C. In our evaluation, equipping with the latent executor significantly improves the program prediction performance, where each program could include up to 256 tokens. \yuandong{People may be curious about whether this training would break if the program is too long? What are typical length of a program in the training set.} \xinyun{Added. The length of random programs is much larger, but the decoded programs mostly have around 50 tokens.} \yuandong{In the C section, the maximal token length is 256. So what happens to if the program is much longer than that?}

\def\cD{\mathcal{D}}

\subsection{Data Regeneration and Iterative Retraining}
\label{sec:iterative-retraining}
Interestingly, once our model is trained on the initial random generated programs $\cD_0$, the predicted program becomes more concise and resembles human-written code. While the exact token match accuracy is low even on the training set, the model still satisfies the IO pairs for many problems. We leverage such a phenomenon to construct a new dataset $\cD_1$ with higher-quality programs from $\cD_0$. Specifically, we run beam search on the trained model to predict program $p_{0\ldots T}$ given input-output pairs in the training set. If model prediction $p_{0\ldots T}$ satisfies all the input-output examples and held-out cases, we replace the original program $p^\star_{0\ldots T}$ with $p_{0\ldots T}$ in $\cD_1$, and keep $p^\star_{0\ldots T}$ otherwise. Afterward, we re-train the model on $\cD_1$. In Sec.~\ref{sec:exp}, we will demonstrate that the retraining process further improves the model performance, especially with smaller training datasets.

\section{Restricted C Program Synthesis Domain}
\vspace{-0.1in}

\begin{table}[t]
\centering
\caption{\small The comparison between our restricted C domain and existing programming by example tasks.}
\label{tab:task-comparison}
\begin{tabular}{ccccc}
\toprule
& Control flow  & Variables & Arithmetics & No helper functions \\
\midrule
Restricted C (Ours) & \cmark & \cmark & \cmark & \cmark \\
Karel~\cite{bunel2018leveraging} & \cmark & $-$ & $-$  &  $-$  \\
DeepCoder~\cite{balog2016deepcoder} &  $-$  & \cmark & \cmark &  $-$  \\
FlashFill~\cite{gulwani2011automating} &  $-$  &  $-$  &  $-$  &  $-$  \\
\bottomrule
\end{tabular}
\end{table}

In this section, we discuss our restricted C program synthesis domain, and our operation predictor design for improving the numerical reasoning ability of program synthesis models.

\subsection{Data Generation}
\vspace{-0.1in}

Collecting large-scale high-quality datasets for program synthesis requires a lot of human efforts, and we aim to reduce the manual work for dataset construction.

Our data generator is built upon Csmith~\cite{yang2011finding}, a random C code generation tool originally designed for finding bugs in compilers. Following the common practice of generating input-output pairs, for each program, we randomly sample 5 numerical lists as the program inputs, and execute the program to obtain the corresponding output lists. This is similar to existing works on PBE problems that sample programs based on a probabilistic context-free grammar, randomly generate valid inputs for the programs and obtain the outputs~\cite{parisotto2016neuro,devlin2017neural,balog2016deepcoder}. This creates infinite samples for synthesizing programs in domain-specific languages. While the programs sampled in this way differ from human-written code, Sec.~\ref{sec:iterative-retraining} shows that they can be converted to be more concise and human-like.

\textbf{The subset of language features used}. Our generated program has variable declaration, variable assignment, and expressions with addition or subtraction operations. The programs also have non-sequential statements, including \texttt{If} statements, \texttt{For} loops, \texttt{Continue} and \texttt{Break} statements. Except for the input argument which is a list, all variables declared are integers, and all program statements are integer manipulation. Each expression has at most 2 mathematical operations, and chaining the full C program could perform multi-step numerical calculation (e.g., \texttt{p0 = p0 - p1 + p2; p0 = p0 - 1;}). Looping statements other than \texttt{For} (i.e., \texttt{While} or \texttt{Do-While} loops) are not supported. Note that we only constrain the final program length ($\le 256$ tokens) and the program can have nested for-loops and complicated if-conditions. 

\textbf{Post-processing}. We perform a few post-processing steps to obtain our final programs from programs generated by Csmith (see Fig.~\ref{fig:ex-c} for an example). We resample different components of the program, so that (1) each constant numerical value lies in $[-4, 4]$, (2) mathematical operators only contain addition and subtraction, and (3) upper/lower limits of \texttt{For} loops are positive and within the length of the list. Programs are discarded if they are trivial (e.g., constant or identity mappings), or the input-output examples include values out of the range $[-4, 4]$. \yuandong{Reviewer may complain that $[-4,4]$ is too restrictive. So we might want to give like 2 pages of generated examples in supplementary materials?} 

\textbf{Final dataset}. We reweight the program distribution so that at least half of them include \texttt{For} loops. Our full dataset includes $500K$ samples in the training set, $1K$ samples in the validation set, and $1K$ samples in the test set. As shown in Fig.~\ref{fig:ex-c}, the randomly sampled program may contain redundant statements, which can be easily avoided by human programmers. We compare our restricted C domain to prior datasets of programming by example in Table~\ref{tab:task-comparison}.

\subsection{Program Decoding with the Operation Predictor}
\vspace{-0.1in}
For program decoder, predicting the next program token $p_t$ is non-trivial, especially when mathematical reasoning is required~\cite{saxton2019analysing,lample2019deep}. To improve the program synthesis performance for domains involving numerical calculation, such as our restricted C domain, we design an associative memory structure named \emph{operation predictor} (Fig.~\ref{fig:model-architecture}(d)), based on the following intuition: given the input $I=2$ and output $O=4$, human would infer that ``$O=I+2$'' might be the desired operation and write down the code accordingly. To materialize such an intuition, we create a pre-computed table that covers all possible integer addition and subtraction operations for valid input and output list values. We defer the details of the model architecture to Appendix~\ref{app:operation-predictor}. The program decoding process remains similar to the one described in Sec.~\ref{sec:model-architecture}, and we highlight the key differences as follows. 

The operation predictor takes two attention vectors $s_t^I$ and $s_t^O$ as the representations of input-output examples, and yields an operator embedding $\hat{op}_t$. To compute the aggregated embedding vector for all input-output examples, we modify Eqn.~\ref{eq:wo-operation-predictor} to also take $\hat{op}_t$ as an input of the max pooling layer:
\begin{equation}
m_t=\mathrm{MaxPool}_{j \in \{1,2,...,K\}}(\mathrm{tanh}(W[s_t^{I(j)}; s_t^{O(j)}; \hat{op}_t^{(j)}])) \label{eq:with-operation-predictor}
\end{equation}

To train the operation predictor, we add an additional loss $\cL_{Op}$: 
\begin{equation}
\cL=\cL_{Prog} + \cL_{Exec} + \cL_{Op}
\end{equation}
$\cL_{Op}$ is designed to ensure that the operation predictor predicts operations related to IO pairs, and we defer the details to Appendix~\ref{app:operation-predictor}.

\textbf{Limitations.} In our current implementation of the operation predictor, the operation table is only able to enumerate the arithmetic operations over a pre-defined constant set, thus it requires that the set of possible numerical values in input-output pairs is finite. One way of extending our operation predictor to support potentially unbounded numerical calculation is to combine it with the subword tokenizer, which has been commonly used in recent language models~\cite{devlin2018bert,chen2021evaluating,austin2021program}. We consider designing general-purpose number representation for better mathematical reasoning as future work.




