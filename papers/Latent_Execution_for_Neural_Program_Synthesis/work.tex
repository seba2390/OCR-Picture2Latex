\section{Related Work}
\label{sec:work}

\textbf{Programming by example.} Programming by example problems have been widely studied with various applications, and recent works have developed deep neural networks as program synthesizers~\cite{gulwani2012spreadsheet,parisotto2016neuro,devlin2017robustfill,bunel2018leveraging}. Most prior works focus on synthesizing programs in domain-specific languages, such as FlashFill~\cite{parisotto2016neuro,devlin2017robustfill,vijayakumar2018neural} for string transformation, Karel~\cite{bunel2018leveraging,shin2018improving,chen2018execution,gupta2020synthesize} for simulated robot navigation, and LISP-style languages for list manipulation~\cite{balog2016deepcoder,polosukhin2018neural,Zohar2018AutomaticPSExtendExecution,nye2019learning}. In this work, we make the first attempt of synthesizing C code in a restricted domain from input-output examples only, and we focus on the list manipulation domain.

Some recent works investigate the limitations of synthetic datasets and the ambiguity in program specifications for neural program synthesis~\cite{shin2019synthetic,clymo2020data,suh2020creating,laich2019guiding}. These works focus on reducing the bias of data distributions and generating more diverse input-output pairs, while our data regeneration aims to improve the quality of programs. We consider incorporating both lines of work to further improve the dataset quality as future work. In addition, drawing the inspiration from self-training and bootstrapping techniques developed for other applications~\cite{mooney1993bootstrapping,abney2002bootstrapping,mcclosky2006effective,xie2020self} to extend our iterative retraining scheme is also another future direction.

\textbf{Execution-guided program synthesis.} To learn better program representations, some recent works incorporate the execution information to guide the synthesis process~\cite{sun2018neural,Zohar2018AutomaticPSExtendExecution,shin2018improving,chen2018execution,Ellis2019WriteEAExtendExecution,tian2019learning,balog2020neural,gupta2020synthesize,odena2020bustle,nye2020representing,mandal2021learning}. In particular, leveraging partial program execution states improves the performance for several program synthesis tasks~\cite{chen2018execution,Zohar2018AutomaticPSExtendExecution,Ellis2019WriteEAExtendExecution,nye2020representing}. However, existing approaches rely on program interpreters to provide the intermediate execution results whenever applicable. In contrast, we demonstrate that our latent executor learns the latent execution traces (\laet{}) without such a requirement. Besides program synthesis, execution traces have also been utilized for other software engineering applications~\cite{alam2019zero,mendis2019ithemal}.

\textbf{Neural execution.} Our latent executor is related to prior works on learning to execute algorithms~\cite{zaremba2014learning,velivckovic2019neural,yan2020neural} and programs~\cite{bieber2020learning}. They focus on predicting execution results for full algorithms and programs, but do not utilize them for program synthesis. Latent state prediction has also been studied in other applications such as task-oriented dialogue systems~\cite{min2020dsi,zhang2020probabilistic} and robotics~\cite{paxton2019prospection}.

