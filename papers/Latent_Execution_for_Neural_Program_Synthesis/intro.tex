\section{Introduction}
\vspace{-0.1in}
Program synthesis from input-output (IO) pairs, also called programming by example (PBE), requires high-level reasoning and remains a challenging problem for deep models. Unlike Natural Language Processing (NLP)~\cite{bahdanau2014neural,devlin2018bert} and perceptual tasks such as Computer Vision (CV)~\cite{deng2009imagenet,he2016deep}, the mapping from IO pairs to the program itself is hard to model. Many works attempt to learn a direct mapping from training samples, but often found that it is already difficult to achieve a low training error, and generalization to new problems is even harder. Alternatively, one might choose to formulate program synthesis as a search problem: to find the program that satisfies IO pairs. Unfortunately, the search space of programs is often vast and highly non-smooth, i.e., a small perturbation of the program often leads to a complete change of the output.  

While there are many previous works on programming by example tasks~\cite{balog2016deepcoder,devlin2017robustfill,bunel2018leveraging}, they mainly focus on Domain Specific Languages (DSLs), and cannot be easily applied to popular general-purpose programming languages. For example, to synthesize C programs, we need to deal with both high-level control flows (e.g., branching and loop) and low-level operations (e.g., which variable is the target of assignment). Moreover, unlike DSLs (e.g., Karel) for which it is feasible to implement a per-line interpreter, C programs need compilation and a partial C program cannot execute. On the other hand, some recent works investigate natural language descriptions as the auxiliary information of the program specification, and they evaluate neural program synthesis models on constrained or simplified competitive programming problems~\cite{kulal2019spoc,alet2021large,hendrycks2021measuring,chen2021evaluating,austin2021program}. Although some of these works demonstrate promising results for synthesizing Python or C code, they require manual annotations of natural language specifications~\cite{kulal2019spoc} or large-scale pre-training on human-written programs~\cite{chen2021evaluating,austin2021program}, and the performance significantly degrades when only input-output examples are fed into the synthesizer~\cite{alet2021large}.

To synthesize C programs from input-output examples only, we propose \ours{}, which generates the program in a recurrent and token-by-token manner. As the first contribution on model architectures for program synthesis, we propose to use two latent \emph{parallel representations} in the recurrent model. One representation is learned from regular recurrent models as in autoregressive language models~\cite{hochreiter1997long}, with the double attention mechanism over IO pairs proposed in RobustFill~\cite{devlin2017robustfill} and an operation predictor that models the arithmetic relationship between the program input and output. The second representation, named \emph{Latent Execution Trace (\laet{})}, models the hypothetical input signal for the remaining partial program to execute to get to the desired output. Motivated by the line of work on execution-guided program synthesis~\cite{sun2018neural,Ellis2019WriteEAExtendExecution,Zohar2018AutomaticPSExtendExecution,chen2018execution}, we learn a latent representation for C programs which are not executed via interpreters, and train the model given only IO pairs without the intermediate program execution states. The two parallel representations are trained end-to-end. 

\iffalse
To extract potentially complicated relationship between input and outputs, we use an external pre-computed addition and subtraction table with attention mechanism to guide generation of next token. 
\fi

\iffalse
The program is generated in a token-by-token manner: at each time step $t$, the \emph{latent executor} $\phi(\cdot)$ takes the learned representation $h_t$ and the previously generated token $a_t$ as the inputs, and generate $h_{t+1} = \phi(h_t, a_t; IO)$ as the next latent representation of the partial program (here $IO$ is the IO pairs). This enables the algorithm to ``imagine'' how the partial program looks like and how it is related to the output, and thus guides the synthesis effectively. While for program with ground truth interpreter (e.g., Karel), we train $\phi$ to make sure $h_t$ can reconstruct the current execution trace, for C program, we only use the supervision that the last representation $h_T$ should reconstruct the output, and train the entire pipeline in an end-to-end manner. 
\fi

\iffalse
With latent execution trace, \ours{} can generate not only sequential program but also programs with branching and loops. 
\fi

As the second contribution on dataset construction, we demonstrate that it is possible to automatically construct a C codebase that is of high quality, controllable and concise through our proposed program synthesis procedure. Specifically, starting from randomly generated C programs that might contain a lot of redundant statements, we show that via \emph{iterative retraining}, the subsequent generated code from our learned model becomes more concise and similar to human-written ones. Moreover, learning directly from the generated code leads to better performance given the same amount of samples, and improves the sample efficiency. We observe similar results when applying our iterative retraining technique to Karel~\cite{bunel2018leveraging}, another programming by example benchmark consisting of randomly generated programs. Although the initial Karel dataset includes a large proportion of complicated programs with different control flow constructs, we demonstrate that nearly half of the problems can be solved by straight-line programs, which again confirms that randomly generated programs tend to be unnecessarily complicated. We envision that the iterative retraining procedure could greatly reduce laborious efforts in human codebase collection in future research. 

As the third contribution, we show for the first time that short C code in a restricted domain (tens of tokens, no library call) with sequential, branching, loop and simple arithmetic operations can be effectively synthesized from IO pairs only. In particular, while \ours{} tends to generate more concise programs (and does not have exact token match with random generated ground truth code), when measuring whether the program execution outputs match the IO pairs, \ours{} achieves $55.2\%$ accuracy, and outperforms existing neural program synthesis models by around $20\%$. These results demonstrate the effectiveness of learning latent execution traces.

%Second, we are the first to show that a learned latent space plus a planning procedure plays a critical role in program synthesis. Our contribution is three-fold. First, w