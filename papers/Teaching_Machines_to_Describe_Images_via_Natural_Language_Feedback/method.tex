\vspace{-3mm}
\section{Our Approach}
\label{sec:approach}
\vspace{-1mm}

Our framework consists of a new phrase-based captioning model trained with Policy Gradients that incorporates natural language feedback provided by a human teacher. 
While a number of captioning methods exist, we design our own which is phrase-based, allowing for natural guidance by a non-expert. 
In particular, we argue that the strongest learning signal is provided when the feedback describes one mistake at a time, e.g. a single wrong word or a phrase in a caption. An example can be seen in Fig.~\ref{fig:web_feedback}. This is also how one most effectively teaches a learning child. To avoid parsing the generated sentences at test time, we aim to predict phrases directly with our captioning model. We first describe our phrase-based captioner, then describe our feedback collection process, and finally propose how to exploit feedback as a guiding signal in policy gradient optimization. 

\vspace{-2mm}
\subsection{Phrase-based Image Captioning}
\vspace{-1mm}

\begin{figure}[t!]
\vspace{-2mm}
\begin{minipage}{0.54\linewidth}
  \includegraphics[width=1\linewidth,trim=10 0 0 0,clip]{figs/phrase_word_rnn}
  \end{minipage}
  \hspace{2mm}
  \begin{minipage}{0.42\linewidth}
  \caption{\small Our hierarchical phrase-based captioning model, composed of a phrase-RNN at the top level, and a word-level RNN which outputs a sequence of words for each phrase. The useful property of this model is that it directly produces an output sentence segmented into linguistic phrases. We exploit this information while collecting and incorporating human feedback into the model. Our model also exploits attention, and linguistic information (phrase labels such as noun, preposition, verb, and conjunction phrase). Please see text for details. }
  \label{fig:phrasemodel}
  \end{minipage}
  \vspace{-2mm}
\end{figure}

Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to~\cite{Tan16,Krause17}. In~\cite{Krause17}, the authors use a two-level LSTM to generate paragraphs, while~\cite{Tan16} uses it to generate sentences as a sequence of phrases. The latter model shares a similar overall structure as ours, however, our model additionally reasons about the type of phrases and exploits the attention mechanism over the image. 

The structure of our model is best explained through Fig.~\ref{fig:phrasemodel}. The model receives an image as input and outputs a caption. It is composed of a \emph{phrase RNN} at the top level, and a \emph{word RNN} that generates a sequence of words for each phrase. One can think of the phrase RNN as providing a ``topic'' at each time step, which instructs the word RNN what to talk about. 

%Let ${\bf a}=({\bf a}_1,\dots,{\bf a}_n)$ denote image feature.
Following~\cite{Xu15}, we use a convolutional neural network in order to extract a
set of feature vectors ${a}=({\bf a}_1,\dots,{\bf a}_n)$, with ${\bf a}_j$ a feature in location $j$ in the input image. We denote the hidden state of the phrase RNN at time step $t$ with $h_t$, and $h_{t,i}$ to denote the $i$-th hidden state of the word RNN for the $t$-th phrase. Computation in our model can be expressed with the following equations: 
%The phrase RNN predicts the label of the phrase using a linear layer on top of its hidden state, followed by a softmax. %That is, $l=\mathrm{softmax}(W_{hl})$  

\hspace{-1mm}\begin{minipage}{0.03\linewidth}
\vspace{-5.8mm}
\begin{tabular}{p{0.03cm}p{0.03cm}}
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\footnotesize phrase-RNN}}} & \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{-90}{$\underbrace{\qquad\qquad\ \ \ }$}}}\\ & \\ & \\[7.8mm]
\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\footnotesize word-RNN}}}& \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{-90}{$\underbrace{\qquad\quad\ \ \,}$}}}\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{align*}
h_t&=f_{phrase}(h_{t-1},l_{t-1},c_{t-1},e_{t-1})\\
l_{t}& = \mathrm{softmax}(f_{phrase-label}(h_t))\\
c_t&=f_{att}(h_{t},l_t, {a})\\
h_{t,0} &= f_{phrase-word}(h_t,l_t,c_t)\\[1mm]
h_{t,i} &= f_{word}(h_{t,i-1},c_t,w_{t,i})\\
w_{t,i} &= f_{out}(h_{t,i}, c_t, w_{t,i-1})\\
e_{t}&=f_{word-phrase}(w_{t,1},\dots,w_{t,end})
\end{align*}
\end{minipage}
\hspace{-2mm}
\begin{minipage}{0.4\linewidth}
\vspace{5.0mm}
{\tabulinesep=0.67mm
\begin{tabu}{|l|c|}
\hline
$f_{phrase}$ & {\small LSTM, dim $256$}\\
$f_{phrase-label}$ & {\small 3-layer MLP}\\% $256\times 5$}\\
$f_{att}$ & {\small 2-layer MLP with ReLu}\\
$f_{phrase-word}$ & {\small 3-layer MLP with ReLu}\\[0.7mm]
$f_{word}$ & {\small LSTM, dim $256$}\\
$f_{out}$ & {\small deep decoder~\cite{deepRnn}}\\
$f_{word-phrase}$ & {\small mean+3-lay. MLP with ReLu}\\
\hline
\end{tabu}
}
\vspace{2mm}
\end{minipage}

As in~\cite{Xu15}, $c_t$ denotes a context vector obtained by applying the attention mechanism to the image. This context vector essentially represents the image area that the model ``looks at'' in order to generate the $t$-th phrase. This information is passed to both the word-RNN as well as to the next hidden state of the phrase-RNN. We found that computing two different context vectors, one passed to the phrase and one to the word RNN, improves generation by $0.6$ points (in weighted metric, see Table~\ref{MLE-table}) mainly helping the model to avoid repetition of words. 
Furthermore, we noticed that the quality of attention significantly improves ($1.5$ points, Table~\ref{MLE-table}) if we provide it with additional linguistic information. In particular,  at each time step $t$ our phrase RNN also predicts a phrase label $l_{t}$, following the standard definition from the Penn Tree Bank. For each phrase, we predict one out of four possible phrase labels, i.e., a noun (NP), preposition (PP), verb (VP), and a conjunction phrase (CP). We use additional <EOS> token to indicate the end of the sentence. By conditioning on the NP label, we help the model look at the objects in the image, while VP may focus on more global image information.

Above, $w_{t,i}$ denotes the $i$-th word output of the word-RNN in the $t$-th phrase, encoded with a one-hot vector. Note that we use an additional <EOP> token in word-RNN's vocabulary, which signals the end-of-phrase. Further, $e_t$ encodes the generated phrase via simple mean-pooling over the words, which provides additional word-level context to the next phrase. Details about the choices of the functions are given in the table. Following~\cite{Xu15}, we use a deep output layer~\cite{deepRnn} in the LSTM and double stochastic attention.

%The bottom layer is a word RNN that conditions on the hidden state of the phrase RNN, and generates a sequence of words for the corresponding phrase. One can think of the phrase RNN as providing a ``topic'' at each time step, which instructs the word RNN what to talk about. We use $h_{ti}$ to denote the $i$-th hidden state of the word RNN for the $t$-th phrase. 

\begin{table}[t!]
\vspace{-2mm}
  \centering
  \scriptsize
  \begin{minipage}{0.49\linewidth}
  \addtolength{\tabcolsep}{-3pt}
  \begin{tabular}{ |c|p{1.8cm}|p{1.2cm}|p{1.6cm}| }
    \hline
    Image &  Ref. caption & Feedback &  Corr. caption\\ \hline
    \begin{minipage}{.2\textwidth}
      \vspace{0.5mm}
      \includegraphics[width=\linewidth]{figs/2911}
      \vspace{-7mm}
    \end{minipage}
    &
    ( a woman ) ( is sitting ) ( on a bench ) ( with a plate ) ( of food . )
    & 
      What the woman is sitting on is not visible.
    & 
       ( a woman ) ( is sitting ) ( with a plate ) ( of food . )
    \\ \hline
        \begin{minipage}{.2\textwidth}
      \vspace{0.5mm}
      \includegraphics[width=\linewidth]{figs/3993}
      \vspace{-7mm}
    \end{minipage}
    &
    ( a horse ) ( is standing ) ( in a barn ) ( in a field . )
    & 
       There is no barn. There is a fence. 
    & 
       ( a horse ) ( is standing ) ( in a fence ) ( in a field . )
    \\ \hline
  \end{tabular}
  \end{minipage}\hspace{1mm}
 \begin{minipage}{0.49\linewidth}
 \addtolength{\tabcolsep}{-3pt}
  \begin{tabular}{ | c |p{1.5cm}|p{1.3cm}|p{1.8cm}|}
  \hline
   Image &  Ref. caption & Feedback &  Corr. caption\\ \hline
        \begin{minipage}{.2\textwidth}
      \vspace{0.5mm}
      \includegraphics[width=\linewidth]{figs/3003}
      \vspace{-7mm}
    \end{minipage}    
    &
    ( a man ) ( riding a motorcycle ) ( on a city street . )
    & 
       There is a man and a woman.
    & 
       ( a man  and a woman ) ( riding a motorcycle ) ( on a city street . )
    \\ \hline
        \begin{minipage}{.2\textwidth}
      \vspace{0.5mm}
      \includegraphics[width=\linewidth]{figs/2186}
      \vspace{-7mm}
    \end{minipage}    
    &
    ( a man ) ( is swinging a baseball bat ) ( on a field . )
    & 
       The baseball player is not swinging a bate.
    & 
       ( a man ) ( is playing baseball ) ( on a field . )
    \\ \hline
  \end{tabular}
  \end{minipage}
  \caption{\small Examples of collected feedback. Reference caption comes from the MLE model. }\label{tbl:myLboro}
  \vspace{-3mm}
\end{table}




\begin{table}[t!]
 \centering
      \caption{\small Statistics for our collected feedback information. The table on the right shows how many times the feedback sentences mention words to be corrected and suggest correction.}
        \small
        \vspace{-2mm}
  \label{tab:feedback-data}
  \begin{minipage}{0.5\linewidth}
  \addtolength{\tabcolsep}{0.5pt}
  \begin{tabular}{cc}
    \toprule
    Num. of evaluated examples (annot. round 1) & 9000\\
    Evaluated as containing errors & 5150\\
    To ask for feedback (annot. round 2) & 4174\\
         \hline
             Avg. num. of feedback rounds per image & 2.22 \\ 
             Avg. num. of words in feedback sent. & 8.04 \\
    Avg. num. of words needing correction & 1.52 \\
    Avg. num. of modified words & 1.46 \\
     \bottomrule
     \end{tabular}
    \end{minipage}
    \hspace{4mm}
    \begin{minipage}{0.46\linewidth}
    \begin{tabular}{lc}
    \toprule
    Something should be replaced & 2999 \\
    \hspace{7mm}mistake word is in description & 2664\\
    \hspace{7mm}correct word is in description & 2674\\
         \hline
   Something is missing & 334 \\
    \hspace{7mm}missing word is in description & 246\\
    \hline
    Something should be removed & 841 \\
     \hspace{7mm}removed word is in description & 779\\
    \bottomrule
  \end{tabular}
   \end{minipage}
   \footnotesize
   feedback round: number of correction rounds for the same example, %(we paid the annotator 1 round, more than this was good will), 
   description: natural language feedback\\[1mm]
  \end{table}

\begin{figure}[t!]
\vspace{-3mm}
\centering
\begin{minipage}{0.3\linewidth}
  \includegraphics[width=1\linewidth,trim=38 24 50 25,clip]{figs/before.pdf}
  \end{minipage}
    \hspace{1mm}
  \begin{minipage}{0.30\linewidth}
  \includegraphics[width=1\linewidth,trim=38 24 50 25,clip]{figs/after.pdf}
  \end{minipage}
  \hspace{2mm}
 % \vspace{-1.5mm}
 \begin{minipage}{0.33\linewidth}
 \vspace{-2mm}
  \caption{\small Caption quality evaluation by the human annotators. Plot on the left shows evaluation for captions generated with our reference model (MLE). The right plot
  shows evaluation of the human-corrected captions (after completing at least one round of feedback).}
  \label{fig:eval}
  \end{minipage}
  \vspace{-2mm}
\end{figure}

\iffalse
It's composed of a phrass RNN, a word RNN. The phrase RNN is responsive for predicting currect phrase label. Then we pass the phrase label and phrase RNN hidden state into a attention layer which will draw attention based on phrase type. We also pass hidden state into a further-use-attention layer, which is responsive for predicting what we should look next. The word RNN generates the words of that phrase. It will also feed all generated word into a phrase encoding layer and generate a phrase embedding vector. This will give next phrase-word RNN information about what we have already generated.


\textbf{phrase RNN } Phras RNN is single layer LSTM with hidden state size 256, at each time step, phrase RNN receives output of phrase encoding layer, phrase label from last time step and output attention context from last further-use-attention layer and output a hidden state h. We use this hidden state h in multiply ways. \\
Firstly, we pass h into a multilayer perceptron, then do a classification task upon on : '<END>': caption end sign , '<PP>': preposition phrase ,'<NP>': noun phrase ,'<VP>': verb phras, '<CP>': conjunction phraes.\\
Secondly, we pass embedded prediced phrase label and h into attention layer to decide what we should look to genreate words.\\
Finally, we pass h into a multilayer perceptron to generalte a topic vector.\\\\

\textbf{word RNN } Phras RNN is single layer LSTM with hidden state size 256, at the first time step, it receives topic vector, output of atterntion layer and embedded prediced phrase label and gerenate first word in this phrase. In the following steps , it receives topic vector, output of atterntion layer and predicted word from last step. The generation process terminates when word RNN generate </EOP> which stands for end of phrase.\\\\

\textbf{phrase encoding layer } After generation process in a phrase, we pass all generated words into a phrase encoding layer, which takes sum of word embedding vector and pass it into next phrase RNN to provide history of generation.\\
\fi

\vspace{-3mm}
\paragraph{Implementation details. }
To train our hierarchical model, we first process MS-COCO image caption data~\cite{lin2014microsoft} using the Stanford Core NLP toolkit \cite{stanfordCore}. We flatten each parse tree, separate a sentence into parts, and label each part with a phrase label (<NP>, <PP>, <CP>, <VP>). To simplify the phrase structure, we merge some NPs to its previous phrase label if it is not another NP.  %For both phrase and word RNNs, we use a LSTM with hidden state size 256.For word embedding, we use embedding vectors which has size 256 as well.

\vspace{-3mm}
\paragraph{Pre-training.} We pre-train our model using the standard cross-entropy loss.  We use the ADAM optimizer~\cite{kingma2014adam} with learning rate $0.001$. We discuss Policy Gradient optimization in Subsec.~\ref{sec:policy}. 



\vspace{-1mm}
\subsection{Crowd-sourcing Human Feedback}
\label{sec:feedback}

We aim to bring a human in the loop when training the captioning model. Towards this, we create a web interface that allows us to collect feedback information on a larger scale via AMT. Our interface is akin to that depicted in Fig.~\ref{fig:web_feedback}, and we provide further visualizations in the Appendix. We also provide it online on our project page. In particular, we take a snapshot of our model and generate captions for a subset of MS-COCO images~\cite{lin2014microsoft} using greedy decoding. In our experiments, we take the model trained with the MLE objective. 

We do two rounds of annotation. In the first round, the annotator is shown a captioned image and is asked to assess the quality of the caption, by choosing between: \emph{perfect}, \emph{acceptable}, \emph{grammar mistakes only}, \emph{minor} or \emph{major} errors. We asked the annotators to choose minor and major error if the caption contained errors in semantics, i.e., indicating that the ``robot'' is not understanding the photo correctly. We advised them to choose \emph{minor} for small errors such as wrong or missing attributes or awkward prepositions, and go with major errors whenever any object or action naming is wrong. %\SF{Fig.~\ref{} shows statistics of the obtained results.} %If the semantics is captured correctly, the annotator can 



For the next (more detailed, and thus more costly) round of annotation, we only select captions which are not marked as either perfect or acceptable in the first round. Since these captions contain errors, the new annotator is required to provide detailed feedback about the mistakes. We found that some of the annotators did not find errors  in some of these captions, pointing to the annotator noise in the process. 
The annotator is shown the generated caption, delineating different phrases with the ``('' and ``)'' tokens. We ask the annotator to 
1) choose the type of required correction, 
2) write feedback in natural language, 
3) mark the type of mistake,
4) highlight the word/phrase that contains the mistake, 
5) correct the chosen word/phrase, 
6) evaluate the quality of the caption after correction. We allow the annotator to submit the HIT after one correction even if her/his evaluation still points to errors. However, we plea to the good will of the annotators to continue in providing feedback. In the latter case, we reset the webpage, and replace the generated caption with their current correction. 
 
The annotator first chooses the type of error, i.e., something `` should be replaced'', ``is missing'', or ``should be deleted''. (S)he then writes a sentence providing feedback about the mistake and how it should be corrected. We require that the feedback is provided sequentially, describing a single mistake at a time. We do this by restricting the annotator to only  select mistaken words within a single phrase (in step 4). In 3), the annotator marks further details about the mistake, indicating whether it corresponds to an error in \emph{object}, \emph{action}, \emph{attribute}, \emph{preposition}, \emph{counting}, or \emph{grammar}. 
For 4) and 5) we let the annotator highlight the area of mistake in the caption, and replace it with a correction.%This information provides us with ground-truth localization of the mistake and its correction, which we use in order to train a neural network to do this automatically based on the feedback description.  

%\SF{Examples of collected feedback are shown in Fig.~\ref{}, with more examples in the Suppl. material. 
The statistics of the data is provided in Table~\ref{tab:feedback-data}, with examples shown in Table~\ref{tbl:myLboro}. An interesting fact is that the feedback sentences in most cases mention both the wrong word from the caption, as well as the correction word. Fig.~\ref{fig:eval} (left) shows evaluation of the caption quality of the reference (MLE) model. Out of 9000 captions, 5150 are marked as containing errors (either semantic or grammar), and we randomly choose 4174 for the second round of annotation (detailed feedback). Fig.~\ref{fig:eval} (left) shows the quality of all the captions after correction, i.e. good reference captions as well as 4174 corrected captions as submitted by the annotators. Note that we only paid for one round of feedback, thus some of the captions still contained errors even after correction. Interestingly, on average the annotators still did 2.2 rounds of feedback per image (Table~\ref{tab:feedback-data}).

\vspace{-2mm}
\subsection{Feedback Network}
\label{sec:feedbacknet}
 \vspace{-1mm}

\begin{figure}[t!]
 \vspace{-1mm}
\centering
  \includegraphics[width=0.7\linewidth,trim=35 40 0 30,clip]{figs/feedback_net}
  \vspace{-1mm}
  \caption{\small The architecture of our feedback network (FBN) that classifies each phrase (bottom left) in a sampled sentence (top left) as either \emph{correct}, \emph{wrong} or \emph{not relevant}, by conditioning on the feedback sentence.}
  \label{fig:feedbacknet}
  \vspace{-2mm}
\end{figure}

Our goal is to incorporate natural language feedback into the learning process. The collected feedback contains rich information of how the caption can be improved: it conveys the location of the mistake and typically suggests how to correct it, as seen in Table~\ref{tab:feedback-data}. This provides strong supervisory signal which we want to exploit in our RL framework. %Note that RL relies on sampling during training, meaning that our 
In particular, we design a neural network which will provide additional reward based on the feedback sentence. We refer to it as the \emph{feedback network} (FBN). We first explain our feedback network, and show how to integrate its output in RL.

%We exploit our collected data to train the feedback network to recognize whether a particular phrase is correct, wrong, or not related to the feedback sentence. 
Note that RL training will require us to generate samples (captions) from the model. Thus, during training, the sampled captions for each training image will change (will differ from the reference MLE caption for which we obtained feedback for). 
The goal of the feedback network is to read a newly sampled caption, and judge the correctness of each phrase conditioned on the feedback. We make our FBN to only depend on text (and not on the image), making its learning task easier. In particular, our FBN performs the following computation:\\
\begin{minipage}{0.52\linewidth}
\vspace{-1mm}
\begin{align}
\label{eq:feedbacknet}
h^{caption}_t &= f_{sent}(h^{caption}_{t-1},w^c_t)\\
h^{feedback}_t &= f_{sent}(h^{feedback}_{t-1},w^f_t)\\
q_i&=f_{phrase}(w^c_{i,1},\dots,w^c_{i,N})\\
o_i&=f_{fbn}(h^c_T,h^f_{T'},q_i,m)
\end{align}
\end{minipage}
\hspace{8mm}
\begin{minipage}{0.4\linewidth}
\vspace{4mm}
{\tabulinesep=0.8mm
\begin{tabu}{|l|c|}
\hline
$f_{sent}$ & LSTM, dim $256$\\
$f_{phrase}$ & linear+mean pool\\
$f_{fbn}$ & 3-layer MLP with dropout\\
& +3-way softmax\\
\hline
\end{tabu}
}
\vspace{2mm}
\end{minipage}
Here, $w^c_t$ and $w^f_t$ denote the one-hot encoding of words in the sampled caption and feedback sentence, respectively. By $w^c_{i,\cdot}$ we denote words in the $i$-th phrase of the sampled caption. FBN thus encodes both the caption and feedback using an LSTM (with shared parameters), performs mean pooling over the words in a phrase to represent the phrase $i$, and passes this information through a 3-layer MLP. The MLP additionally accepts information about the mistake type (e.g., wrong object/action) encoded as a one hot vector $m$ (denoted as ``extra information'' in Fig.~\ref{fig:feedbacknet}). The output layer of the MLP is a 3-way classification layer that predicts whether the phrase $i$ is \emph{correct}, \emph{wrong}, or \emph{not relevant} (wrt feedback sentence). An example output from FBN is shown in Table~\ref{table:feedbacknetTable}.

%In this section we introduce a feedback network. The feedback net recives a image caption sentence, a feedback sentence and a particular phrase from the caption sentence. It returns a -1 is there is an error in this particular phraes, it returns 0 if this phrase is not relevant to feedback, 1 if the error described in feedback has been fixed. For example, as in Figure3, if phrase is "a cat", it will return "good", if phrae is "on a sidewalk", it will return "not relevant". Sentence LSTM and feedback LSTM are a share paremeter LSTM with hidden state 256. Phrase word mean pool takes mean of all words embedding vector in this phrase. Then we pass two output hidden states and output of phrase word mean pool into a three layer MLP with dropout then do 3-classes classfication.\\
%We also provide extra information into MLP layer. We use error type, detail error class type. We decode those informations into one hot vector and feed into MLP together with hidden stats, words mean pool.
\begin{table}[t!] % placement parameter H
\vspace{-0mm}
    \centering % if you want to center the table
    \small
    \addtolength{\tabcolsep}{4pt}
      \begin{tabular}{llll}
    \toprule
  %  \multicolumn{2}{c}{Input}                   \\
    %\cmidrule{1-3}
    Sampled caption     & Feedback & Phrase     & Prediction\\
    \midrule
    A cat on a sidewalk. & & A cat  &  wrong    \\
    A dog on a sidewalk.     & There is a dog on a sidewalk not a cat.& A dog&  correct     \\
    A cat on a sidewalk.     &      &on a sidewalk& not relevant \\
    \bottomrule
  \end{tabular}
  \vspace{1mm}
      \caption{\small Example classif. of each phrase in  a newly sampled caption into correct/wrong/not-relevant conditioned  on the feedback sentence. Notice that we do not need the image to judge the correctness/relevance of a phrase.}
    \label{table:feedbacknetTable}
    \vspace{-2mm}
\end{table}

\vspace{-2mm}
\paragraph{Implementation details.} We train our FBN with the ground-truth data that we collected. In particular, we use (reference, feedback, marked phrase in reference caption) as an example of a \emph{wrong} phrase, (corrected sentence, feedback, marked phrase in corrected caption) as an example of the \emph{correct} phrase, and treat the rest as the \emph{not relevant} label. Reference here means the generated caption that we collected feedback for, and marked phrase means the phrase that the annotator highlighted in either the reference or the corrected caption. We use the standard cross-entropy loss to train our model. We use ADAM~\cite{kingma2014adam} with learning rate 0.001, and a batch size of 256. When a reference caption has several feedback sentences, we treat each one as independent training data. % and word embedding size 256.


\subsection{Policy Gradient Optimization using Natural Language Feedback}
\label{sec:policy}

We follow~\cite{Selfcritical,Mixer} to directly optimize for the desired image captioning metrics using the Policy Gradient technique. For completeness,  we briefly summarize it here~\cite{Selfcritical}. 

One can think of an caption decoder as an agent following a parameterized policy $p_\theta$ that selects an action at each time step. An ``action'' in our case requires choosing a word from the vocabulary (for the word RNN), or a phrase label (for the phrase RNN). An ``agent'' (our captioning model) then receives the reward after generating the full caption, i.e., the reward can be any of the automatic metrics, their weighted sum~\cite{Selfcritical,Spider}, or in our case will also include the reward from feedback. 

%As desctibed in ~\cite{Selfcritical,Spider,Mixer}, we can optimize image caption metrics(such as BLEU, ROGER) directly by Policy Gradient method. For paramater $\theta$, We defind our policy $p_\theta$ .For generated sequence $w^s = w_1^s w_2^sw_3^s...w_T^s$ where $w_t^s$ is word or phrase label sampled at time step t, we define reward of it to be $r(w^s) =  \lambda_{1} BLEU_1+ \lambda_{2}BLEU_2+ \lambda_{3} BLEU_3+  \lambda_{4}BLEU_4 $, then we aim at minimize: 
The objective for learning the parameters of the model is the expected reward received when completing the caption $w^s=(w^s_1,\dots,w^s_T)$ ($w^s_t$ is the word sampled from the model at time step $t$):
\begin{equation}
\label{eq:expreward}
\begin{aligned}
L(\theta) = - E_{w^s\sim p_\theta}[r(w^s)]
 \end{aligned}
\end{equation}
\iffalse
Typically, the above expectation is approximated by taking a single sample from the model:
\begin{equation}
\begin{aligned}
L(\theta) \approx - r(w^s), \quad w^s \sim p_\theta
 \end{aligned}
\end{equation}
\fi
To optimize this objective, we follow the reinforce algorithm~\cite{RL1992}, as also used in~\cite{Selfcritical,Mixer}. The gradient of~\eqref{eq:expreward} can be computed as 
\begin{equation}
\begin{aligned}
\nabla_{\theta} L(\theta) = - E_{w^s\sim p_\theta}[r(w^s) \nabla_{\theta} \log p_\theta(w^s)],
 \end{aligned}
\end{equation}
which is typically estimated by using a single Monte-Carlo sample:
\begin{equation}
\begin{aligned}
\nabla_{\theta} L(\theta)  \approx   - r(w^s) \nabla_{\theta} \log p_\theta(w^s)
 \end{aligned}
\end{equation}
%Proven by \cite{RL1998}, introduce a baseline b which is not dependent on action of sampling words sequence $w$ can decrease variance and does not change the expected gradient. As in \cite{Selfcritical}, we define self critical baseline :
We follow~\cite{Selfcritical} to define the baseline $b$ as the reward obtained by performing greedy decoding: \begin{equation}
\begin{aligned}
&b =   r(\hat{w}), \quad \hat{w_t} = \mathrm{arg\,max}\ p(w_t|h_t)\\
&\nabla_{\theta} L(\theta)  \approx   - (r(w^s)-r(\hat{w}))\nabla_{\theta} \log p_\theta(w^s)
&
 \end{aligned}
\end{equation}
Note that the baseline does not change the expected gradient but can drastically reduce its variance.

\iffalse
Using chain rule, the final gradient formula is:\\
\begin{equation}
\begin{aligned}
&\nabla_{\theta} L(\theta) = \sum_{t=1}^{T} \frac{\partial L(\theta)}{\partial s_t}\frac{\partial \partial s_t}{\partial \theta}\\
&\frac{\partial L(\theta)}{\partial s_t}  \approx (r(w^s)-r(\hat{w}))(p_\theta(w_t|h_t) - 1_{w_t^s})
&
 \end{aligned}
\end{equation}
\fi



%\subsection{Policy Gradient Optimization using feedback net}
{\bf Reward.} We define two different rewards, one at the sentence level (optimizing for a performance metrics), and one at the phrase level. We use human feedback information in both. We first define the sentence reward wrt to a reference caption as a weighted sum of the BLEU scores:
\begin{equation}
\begin{aligned}
&r(w^s) = \beta \sum_i \lambda_{i} \cdot BLEU_i (w^s,ref) % f(w^s, feedback, p_m)
&
 \end{aligned}
\end{equation}
%Whenever we will assume to have access to a ground-truth caption (provided by the dataset)
In particular, we choose $\lambda_{1}=\lambda_{2}=0.5$, $\lambda_{3}=\lambda_{4}=1$, $\lambda_{5}=0.3$. As reference captions to compute the reward, we either use the reference captions generated by a snapshot of our model which were evaluated as not having minor and major errors, or ground-truth captions. %Note that these may still contain errors as shown in Fig.~\ref{}. 
The details are given in the experimental section. 
%In cases where the corrected sentences still had errors, we use the ground-truth captions as our RL targets. 
We weigh the reward by the caption quality as provided by the annotators. In particular, $\beta=1$ for \emph{perfect} (or GT), $0.8$ for \emph{acceptable}, and $0.6$ for \emph{grammar/fluency issues only}. %If we also have access to the ground-truth captions (from the dataset), we simply average the two rewards (we use $\beta=1$ when using the GT captions). 

We further incorporate the reward provided by the feedback network. In particular, our FBN allows us to define the reward at the phrase level (thus helping with the credit assignment problem). Since our generated sentence is segmented into phrases, i.e., $w^s = w^p_1w^p_2\dots w^p_P$, where $w^p_t$ denotes the (sequence of words in the) $t$-th phrase, we define the combined phrase reward as:
\begin{equation}
\begin{aligned}
&r(w^p_t) = r(w^s)+ \lambda_{f} f_{fbn}(w^s, feedback, w^p_t)
&
 \end{aligned}
\end{equation}
%\HL{Note that $P$ is total number of phrases, our final gradient becomes:}
Note that FBN produces a classification of each phrase. We convert this into reward, by assigning \emph{correct} to $1$, \emph{wrong} to $-1$, and $0$ to \emph{not relevant}. We do not weigh the reward by the confidence of the network, which might be worth exploring in the future. 
Our final gradient takes the following form:
\begin{equation}
\begin{aligned}
&\nabla_{\theta} L(\theta) = - \sum_{p=1}^{P} (r(w^p) - r(\hat{w}^p)) \nabla_{\theta} \log p_\theta(w^p)\\
&
 \end{aligned}
\end{equation}

\vspace{-6.5mm}
\paragraph{Implementation details.}
We use Adam with learning rate $1e^{-6}$ and batch size 50. As in~\cite{Mixer}, we follow an annealing schedule.  
We first optimize the cross entropy loss for the first $K$ epochs, then for the following $t=1,\dots,T$ epochs, we use cross entropy loss for the first $(P - floor(t/m))$ phrases (where $P$ denotes the number of phrases), and the policy gradient algorithm for the remaining $floor(t/m)$ phrases. We choose $m = 5$. When a caption has multiple feedback sentences, we take the sum of the FBN's outputs (converted to rewards) as the reward for each phrase. When a sentence does not have any feedback, we assign it a zero reward.

\iffalse
In this section, we define a new reward function $r_H(w^s)$ ,which stands for reward from human feedback. we define $w^s = p_1p_2...p_k...p_K$ and $p_m$ stands for m-th phrase in generated sentence.\\As discussed in section3.3, we have $f(w^s, feedback, p_m)$.  Together with  section3.4, our final reward function is:\\
\begin{equation}
\begin{aligned}
&r(p_m) = \lambda_{1} BLEU_1(w^s)+\lambda_{2} BLEU_2(w^s)+ \lambda_{3} BLEU_3(w^s)+ \lambda_{4}BLEU_4(w^s) + \lambda_{5} f(w^s, feedback, p_m)
&
 \end{aligned}
\end{equation}
In particular, we choose $\lambda_{1}=\lambda_{2}=0.5$, $\lambda_{3}=\lambda_{4}=1$, $\lambda_{5}=0.3$.
\fi
