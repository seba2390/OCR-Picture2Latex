\vspace{-2mm}
\section{Experimental Results}
\label{sec:results}
\vspace{-1mm}

To validate our approach we use the MS-COCO dataset~\cite{lin2014microsoft}. We use 82K images for training, 2K for validation, and 4K for testing. In particular, we randomly 
chose 2K val and 4K test images from the official validation split. To collect feedback, we randomly chose 7K images from the training set, as well as all 2K images from our validation. In all experiments, we report the performance on our (held out) test set. 
For all the models (including baselines) we used a pre-trained VGG~\cite{vgg} network to extract image features. We use a word vocabulary size of 23,115. 

\vspace{-2mm}
\paragraph{Phrase-based captioning model.} We analyze different instantiations of our phrase-based captioning in Table~\ref{MLE-table}, showing the importance of predicting phrase labels. To sanity check our model we compare it to a flat approach (word-RNN only)~\cite{Xu15}. %The flat model is~\cite{Xu15}, but uses the same CNN as we do. 
Overall, our model performs slightly worse than~\cite{Xu15} ($0.66$ points). However, the main strength of our model is that it allows a more natural integration with feedback. Note that these results are reported for the models trained with MLE.

\begin{table}[t!]
\vspace{-1mm}
\small
  \centering
   \addtolength{\tabcolsep}{-1.6pt}
  \begin{tabular}{c|cccccc}
    \toprule
     & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE-L& Weighted metric   \\
     \hline
    flat (word level) with att &65.36 & 44.03 & 29.68 & 20.40 & 51.04& {\bf 104.78}\\
    \hline
    phrase with att. &64.69 & 43.37 & 28.80 & 19.31 & 50.80& 102.14\\
    phrase with att +phrase label &65.46 & 44.59 & 29.36 & 19.25& 51.40& 103.64\\
    phrase with 2 att +phrase label &65.37 & 44.02 & 29.51 & 19.91 & 50.90& 104.12 \\
    \bottomrule
  \end{tabular}
    \caption{\small Comparing performance of the flat captioning model~\cite{Xu15}, and different instantiations of our phrase-based captioning model. All these models were trained using the cross-entropy loss.}
  \label{MLE-table}
  \vspace{-2mm}
\end{table}



%In all of our experiments , we used a pretrained Vgg\cite{vgg} net to extract image features. We use all mscoco training data plus 2K images from val set as our training set. We use another 4K images from val set as our testing set. We notice that using phrase label informantion to draw attention will give us one percent improvement. And use two context model described in section3.1 gives us aroung 0.5 percent improvement.



\vspace{-2mm}
\paragraph{Feedback network.} As reported in Table~\ref{tab:feedback-data}, our dataset which contains detailed feedback (descriptions) contains 4173 images. We randomly select 9/10 of them to serve as a training set for our feedback network, and use 1/10 of them to be our test set. The classification performance of our FBN is reported in Table~\ref{feedback-table}. %We analyze which types of additional information 
We tried exploiting additional information in the network. The second line reports the result for FBN which also exploits the reference caption (for which the feedback was written) as input, represented with a LSTM. The model in the third line uses the type of error, i.e. the phrase is ``missing'', ``wrong'', or ``redundant''. 
We found that by using  information about what kind of mistake the reference caption had (e.g., corresponding to misnaming an \emph{object}, \emph{action}, etc) achieves the best performance. We use this model as our FBN used in the following experiments. 

\begin{table}[t!]
\vspace{-1mm}
  \centering
  \small
  \begin{minipage}{0.5\linewidth}
  \begin{tabular}{cc}
    \toprule
     Feedback network & Accuracy\\
     \hline
    no extra information & 73.30 \\
    use reference caption & 73.24 \\
    use "missing"/"wrong"/"redundant"  &72.92 \\
    use  "action"/"object"/"preposition"/etc &{\bf 74.66} \\
    \bottomrule
  \end{tabular}
  \end{minipage}
  \hspace{3mm}
  \begin{minipage}{0.45\linewidth}
    \caption{\small Classification results of our feedback network (FBN) on a held-out feedback data. The FBN predicts \emph{correct}/\emph{wrong}/\emph{not relevant} for each phrase in a caption. See text for details.}
  \label{feedback-table}
   \end{minipage}
   \vspace{-2mm}
\end{table}

%As described in Table1, we used all 4173 feedback data to train our feedback net. Because our feedback net is doing classification on phrases, we train each phrase together with sentence to be one data. We randomly select 9/10 of them to be training set and 1/10 of them to be testing set.
%As shown in Table4, we also experiment on extra information. From AMT we gathered error category(missing/wrong/redundant) as well as error type(action error/object error ...). We decoded those labels into one hot and pass into MLP. Our best result comes from using error type as extra information.






\vspace{-2mm}
\paragraph{RL with Natural Language Feedback.} In Table~\ref{sample-table} we report the performance for several instantiations of the RL models. All models have been pre-trained using cross-entropy loss (MLE) on the full MS-COCO training set. For the next rounds of training, all the models are trained only on the 9K images that comprise our full  evaluation+feedback dataset from Table~\ref{tab:feedback-data}. 
In particular, we separate two cases. In the first, standard case, the ``agent'' has access to 5 captions for each image. We experiment with different types of captions, e.g. ground-truth captions (provided by MS-COCO), as well as feedback data. For a fair comparison, we ensure that each model has access to (roughly) the same amount of data. This means that we count a feedback sentence as one source of information, and a human-corrected reference caption as yet another source. %Strictly speaking, we have feedback sentences for only 4.1K images, thus not adding up to the full ``budget''.  
We also exploit reference (MLE) captions which were evaluated as correct, as well as corrected captions obtained from the annotators. In particular, we tried two types of experiments. We define ``C'' captions as all captions that were corrected by the annotators and were not evaluated as containing \emph{minor} or \emph{major} error, and ground-truth captions for the rest of the images. For ``A'', we use all captions (including reference MLE captions) that did not have \emph{minor} or \emph{major} errors, and GT for the rest. A detailed break-down of these captions is reported in Table~\ref{info}.
%Note that ``C'' in Table~\ref{sample-table} denotes all captions that were not evaluated as containing \emph{minor} or \emph{major} errors from our feedback session, and GT captions for those that were, plus GT for remaining images which were not sent to feedback session. This includes  938 \emph{perfect}, 1502 \emph{acceptable}, 234 \emph{grammar mistakes only}, and 6326 \emph{GT captions} and \HL{that ``P'' in Table~\ref{sample-table} denotes all captions that were not evaluated as containing \emph{minor} or \emph{major} errors from our feedback session, and GT captions for those that were, plus generated for remaining images which were not sent to feedback session (already perfect ones). This includes  2661 \emph{perfect}, 2790 \emph{acceptable}, 442 \emph{grammar mistakes only}, and 3107 \emph{GT captions}}

We first test a model using the standard cross-entropy loss, but which now also has access to the corrected captions in addition to the 5GT captions.  This model (MLEC) is able to improve over the original MLE model by $1.4$ points. We then test the RL model by optimizing the metric wrt the 5GT captions (as in~\cite{Selfcritical}). This brings an additional point, achieving $2.4$ over the MLE model. Our RL agent with feedback is given access to 3GT captions, the ``C" captions and feedback sentences. We show that this model outperforms the no-feedback baseline by $0.5$ points. Interestingly, with ``A'' captions we get an additional $0.3$ boost. If our RL agent has access to 4GT captions and feedback descriptions, we achieve a total of $1.1$ points over the baseline RL model and $3.5$ over the MLE model. Examples of generated captions are shown in Fig.~\ref{fig:examples}.

We also test a more realistic scenario, in which the models have access to either a single GT caption, or in our case ``C" (or ``A'') and feedback. This mimics a scenario in which the human teacher observes the agent and either gives feedback about the agent's mistakes, or, if the agent's caption is completely wrong, the teacher writes a new caption. 
Interestingly, RL when provided with the corrected captions performs better than when given GT captions. Overall, our model outperforms the base RL (no feedback) by $1.2$ points. We note that our RL agents are trained (not counting pre-training) only on a small (9K) subset of the full MS-COCO training set. Further improvements are thus possible. 

\vspace{-3mm}
\paragraph{Discussion.} These experiments make an important point. Instead of giving the RL agent a completely new target (caption), a better strategy  is to ``teach'' the agent about the mistakes it is doing and suggest a correction. Natural language thus offers itself as a rich modality for providing such guidance not only to humans but also to artificial agents. 


\definecolor{bl}{rgb}{0.7,0.85,1}
\definecolor{or}{rgb}{1,0.85,0.7}

\begin{table}[t!]
\vspace{-2mm}
  \caption{\small Comparison of our RL with feedback information to baseline RL and MLE models.}
  \vspace{-1.5mm}
  \label{sample-table}
  \centering
  \small
   \addtolength{\tabcolsep}{-1.6pt}
  \begin{tabular}{p{1.7mm}c|cccccc}
    \toprule
    & & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE-L  & Weighted metric \\
    \hline
   \cellcolor{bl} & MLE (5 GT) &65.37 & 44.02 & 29.51 & 19.91 & 50.90 & 104.12 \\
    \cellcolor{bl}& MLEC (5 GT + C) &66.85 & 45.19 & 29.89 & 19.79 & 51.20 & 105.58 \\
    \cellcolor{bl}& MLEC (5 GT + A) &66.14 & 44.87 & 30.17 & 20.27 & 51.32 & 105.47 \\
    \cellcolor{bl}& RLB (5 GT) & 66.90  & 45.10 & 30.10 &  20.30 & 51.10 & 106.55  \\
\cellcolor{bl}& RLF (3GT+FB+C) & 66.52  & 45.23 & 30.48 & {\bf 20.66} & 51.41 & 107.02  \\
\cellcolor{bl}& RLF (3GT+FB+A) & 66.98  & 45.54 & 30.52 & 20.53 & {\bf51.54} & 107.31  \\
    \cellcolor{bl} \parbox[t]{2.5mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{\footnotesize 5 sent.}}} & RLF (4GT + FB)  &  {\bf 67.10}  & {\bf 45.50} & {\bf 30.60} & 20.30 & 51.30 & {\bf 107.67}    \\
    \hline
    \cellcolor{or} & RLB (1 GT)     &  65.68  & 44.58 & 29.81 & 19.97 & 51.07 & 104.93    \\
    \cellcolor{or}& RLB (C) & 65.84  &  44.64 & 30.01 & 20.23 & 51.06 & 105.50   \\
    \cellcolor{or}& RLB (A) &  65.81  & 44.58 & 29.87 & 20.24 & 51.28 & 105.31   \\
    \cellcolor{or}& RLF (C + FB) &  65.76  &  44.65 & {\bf 30.20} & {\bf 20.62} &  51.35 & 106.03  \\
    \cellcolor{or}\parbox[t]{2.5mm}{\multirow{-3}{*}{\rotatebox[origin=c]{90}{\footnotesize 1 sent.}}} & RLF (A + FB) &  {\bf66.23} & {\bf45.00} & 30.15 & 20.34 & {\bf51.58} & {\bf106.12} \\[-0.5mm]

    \bottomrule
  \end{tabular}
  
\small {\bf GT}: ground truth captions; \hspace{1.5mm} {\bf FB}: feedback; \hspace{1.5mm} {\bf MLE(A)(C)}: MLE model using five GT sentences + either C or A captions (see text and Table~\ref{info}); 
\hspace{1.5mm} {\bf RLB}: baseline RL (no feedback network); \hspace{1.5mm} {\bf RLF}: RL with feedback (here we also use C or A captions as well as FBN); 
 
\end{table}


\begin{table}[t!]
\vspace{-1mm}
  \centering
  \small
  \begin{minipage}{0.58\linewidth}
  \addtolength{\tabcolsep}{-1.6pt}
  \begin{tabular}{ccccc}
    \toprule
      & \emph{ground-truth} & \emph{perfect} & \emph{acceptable} & \emph{grammar error only}\\
     \hline
    A & 3107 & 2661 & 2790 & 442\\
    C & 6326 & 1502  & 1502 & 234\\
    \bottomrule
  \end{tabular}
  \end{minipage}
  \hspace{3mm}
   \begin{minipage}{0.38\linewidth}
    \caption{\small Detailed break-down of what captions were used as ``A'' or ``C'' in Table~\ref{sample-table} for computing additional rewards in RL.}
  \label{info}
  \end{minipage}
   \vspace{-2mm}
\end{table}

%As shown in Table1, we randomly select 7K data from mscoco training set and another 2K from mscoco validation set. Then we send those 9K data to AMT to evaluate, we use those 9K data as our training and another 4K images from mscoco validation set as our testing set. Firstly, we run several epochs on mannually corrected sentences which is evluated as "perfect" or "acceptable". This gives us one percent improvment, which shows our data can help improve our model.  Secondly, we run RL model using five ground truths per image without feedback net. It gives us two percents improvment on our MLE baseline model. Then
 %we run a RL model with feedback net reward plus four ground truths. This gives us three percent improvement which proves our feedback RL is strong.\\
%Furthermore, as shown in Table5 from line five to seven. We run RL model targets on one ground truth, RL model targets on corrected sentnece which is evluated as "perfect" or "acceptable" and RL model with feedback targets on corrected sentnece. We get 0.8 percent, 1.4 percent, 2 percent improvement respectly. This proves our corrected sentence is easier to learn than one ground truth, and feedback net can help our model find the error part to fix.


\begin{figure}[t!]
\vspace{-1mm}
\scriptsize
    \begin{minipage}{.13\textwidth}
      \vspace{3mm}
      \includegraphics[width=\linewidth]{figs/18}
      \vspace{1mm}
    \end{minipage}
    \hspace{1mm}
    \begin{minipage}{.33\textwidth}
      {\bf MLE}: ( a man ) ( walking ) ( in front of a building ) ( with a cell phone . )\\
       {\bf RLB}: ( a man ) ( is standing ) ( on a sidewalk ) ( with a cell phone . )\\
        {\bf RLF}: ( a man ) ( wearing a black suit ) ( and tie ) ( on a sidewalk . )\\
        \end{minipage}
        \hspace{4mm}
              \begin{minipage}{.13\textwidth}
      \vspace{3mm}
      \includegraphics[width=\linewidth]{figs/33}
      \vspace{1mm}
    \end{minipage}
    \hspace{1mm}
    \begin{minipage}{.33\textwidth}
        {\bf MLE}: ( two giraffes ) ( are standing ) ( in a field ) ( in a field . )\\
        {\bf RLB}: ( a giraffe ) ( is standing ) ( in front of a large building . )\\
        {\bf RLF}: ( a giraffe ) ( is ) ( in a green field ) ( in a zoo . )\\
         \end{minipage}\\[-5mm]
             \begin{minipage}{.13\textwidth}
      \vspace{3mm}
      \includegraphics[width=\linewidth]{figs/24}
      \vspace{1mm}
    \end{minipage}
    \hspace{1mm}
    \begin{minipage}{.33\textwidth}
      {\bf MLE}: ( a clock tower ) ( with a clock ) ( on top . )\\
       {\bf RLB}:  ( a clock tower ) ( with a clock ) ( on top of it . )\\
        {\bf RLF}: ( a clock tower ) ( with a clock ) ( on the front . )\\
        \end{minipage}
        \hspace{4mm}
              \begin{minipage}{.13\textwidth}
      \vspace{3mm}
      \includegraphics[width=\linewidth]{figs/35}
      \vspace{1mm}
    \end{minipage}
    \hspace{1mm}
    \begin{minipage}{.33\textwidth}
        {\bf MLE}: ( two birds ) ( are standing ) ( on the beach ) ( on a beach . )\\
        {\bf RLB}: ( a group ) ( of birds ) ( are ) ( on the beach . )\\
        {\bf RLF}: ( two birds ) ( are standing ) ( on a beach ) ( in front of water . )\\
         \end{minipage}
         \vspace{-5mm}
         \caption{\small Qualitative examples of captions from the MLE and RLB models (baselines), and our RBF model.}
         \label{fig:examples}
         \vspace{-2mm}
      \end{figure}


