\vspace{-3.5mm}
\section{Introduction}
\label{sec:intro}


In the era where A.I. is slowly finding its way into everyone's lives, be in the form of social bots~\cite{Vinyals15,tay}, personal assistants~\cite{Mataric17,Westlund16,SimoCVPR15}, or household robots~\cite{herb}, it becomes critical to allow non-expert users to teach and guide their robots~\cite{Weston16,Weston16b}. For example, if a household robot keeps bringing food served on an ashtray thinking it's a plate, one should ideally be able to educate the robot about its mistakes,  possibly without needing to dig into the underlying software. 

Reinforcement learning has become a standard way of training artificial agents that interact with an environment. There have been significant advances in a variety of domains such as games~\cite{Silver_2016,mnih2015humanlevel}, robotics~\cite{Levine:2016}, and even fields like vision and NLP~\cite{Selfcritical,Li16}.
%RL agents typically balance between exploitation and exploration in order to learn good policies. This is particularly hard in environments with large action spaces and sparse rewards. 
RL agents optimize their action policies so as to maximize the expected reward received from the environment. Training typically requires a large number of episodes, particularly in environments with large action spaces and sparse rewards. 


Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster~\cite{Thomaz06,Knox12,Knox13,Judah10,shaping}. In most cases, a human teacher observes the agent act in an environment, and is allowed to give additional guidance to the learner. This feedback typically comes in the form of a simple numerical (or ``good''/``bad'')  reward which is used to either shape the MDP reward~\cite{Thomaz06} or directly shape the policy of the learner~\cite{shaping}.  

In this paper, we aim to exploit natural language as a way to guide an RL agent. We argue that a sentence provides a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes occur and suggests how to correct them.  Such descriptive feedback can thus naturally facilitate solving the credit assignment problem as well as to help guide exploration. Despite its clear benefits, very few approaches aimed at incorporating language in Reinforcement Learning. In pioneering work,~\cite{Maclin94} translated natural language advice into a short program which was used to bias action selection. While this is possible in limited domains such as in navigating a maze~\cite{Maclin94} or learning to play a soccer game~\cite{Kuhlmann04}, it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback. 

Here   our goal is to allow a non-expert human teacher to give feedback to an RL agent in the form of natural language, just as one would to a learning child. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.

%We spend a significant portion of our lives learning from other humans. While some tasks can be learned through demonstration, most information that we get is via natural language. Teachers describe new concepts, as well as provide feedback to the students through language. A sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes occur and suggest how to correct them.  
%In the era where A.I. is starting to play roles such as personal or household assistants~\cite{Mataric17,Westlund16,herb,SimoCVPR15}, or chat bots~\cite{Vinyals15}, it becomes critical to allow non-expert users to guide the machine learning algorithms. %In this paper, we aim to exploit language as a natural link between a learning artificial agent and a human teacher. 

\iffalse
Image captioning techniques are slowly gearing towards human-level performance as measured by standard benchmarks~\cite{Karpathy15,Xu15,DBLP:journals/corr/KirosSZ14}. These methods, which typically rely on a powerful sequence decoder, are most often trained via maximum likelihood estimation (MLE). In MLE, one learns the model parameters 
by maximizing
the conditional log-likelihood of the training samples, i.e., by encouraging the decoder to produce the ground-truth word token at each time step. 
However, language is extremely versatile and there are many way to express the same semantic meaning, making this objective overly rigid. Recently, improvements have been obtained via Reinforcement Learning which allows for direct optimization of the desired, non-differentiable metrics~\cite{Selfcritical,Spider,Dai17}. %Since the reward is very sparse (one gets it only once the full caption is generated), 
MCMC rollouts~\cite{Spider,Dai17} or clever computation of the baseline~\cite{Selfcritical} have to be employed to deal with credit assignment and control the variance of the estimated gradients. 
\fi

%Here we aim to put a human in the loop to help a captioning algorithm recover from its mistakes.  In particular, our goal is to allow a non-expert human teacher to give feedback to the algorithm in the form of natural language, just as one would to a learning child. %Natural language feedback is much more powerful than a numeric reward. 
%A descriptive feedback can naturally facilitate solving the credit assignment problem as well as to guide exploration. 

%We argue that feedback from the human provides a stronger learning signal as it helps the algorithm localize the mistake in its current prediction, thus solving the credit assignment problem. Furthermore, it may convey that some sentences are already correct even though they do not directly match any of the ``ground-truth'' captions, thus helping the model focus on the more urgent examples. 

Towards this goal, we make several contributions. We propose a hierarchical phrase-based RNN as our image captioning model, as it can be naturally integrated with human feedback. We design a web interface which allows us to collect natural language feedback from human ``teachers'' for a snapshot of our model, as in Fig.~\ref{fig:web_feedback}. We show how to incorporate this %natural language feedback 
information in Policy Gradient RL~\cite{Selfcritical}, and show that we can improve over RL that has access to the same amount of ground-truth captions. %To the best of our knowledge, our work is one of the first to guide an RL agent using natural language.  
Our code and data will be released ({\color{magenta}{\small\url{http://www.cs.toronto.edu/~linghuan/feedbackImageCaption/}}}) to facilitate more human-like training of captioning models. 


\begin{figure}[t!]
\vspace{-0.mm}
\hspace{1mm}\begin{minipage}{0.3\linewidth}
\centering
%\includegraphics[width=\linewidth,trim=0 50 0 8,clip]{figs/intro1/Slide3}
\includegraphics[width=1\linewidth,trim=0 0 7 0,clip]{figs/web}
\end{minipage}
\hspace{3mm}
\begin{minipage}{0.65\linewidth}
%\includegraphics[width=1\linewidth,trim=0 0 00 0,clip]{figs/web_correction1}
%{\bf Caption:}\\[0.8mm]
\begin{tcolorbox}[colback=white, colframe=black!90,colbacktitle=gray!50!red!30!orange,left=3pt,right=3pt,top=0.1pt,bottom=0.0pt,boxrule=1.0pt,title=Machine]
{( a cat ) ( sitting ) ( on a sidewalk ) ( next to a street . )}%\\[-1.2mm]
\end{tcolorbox}
\vspace{1mm}
%\line(1,0){100}\\[1mm]
\begin{tcolorbox}[colback=white, colframe=black!90,colbacktitle=gray!68!blue,left=3pt,top=1pt,bottom=-0.8pt,boxrule=1.0pt,title=Human Teacher]
\begin{small}
\vspace{0.2mm}
%{\bf Correction}\\[1mm]
%Type of mistake: \emph{Something should be replaced in the caption}\\[1mm]
{\bf Feedback:} \emph{ There is a dog on a sidewalk, not a cat.}\\[0.1mm]
{\bf Type of mistake:} \emph{wrong object}\\[0.1mm]
{\bf Select the mistake area:}\\[0.1mm]
 \emph{( a  \xmybox{\bf cat} ) ( sitting ) ( on a sidewalk ) ( next to a street . )}\\[0.4mm]
 {\bf Correct the mistake:}\\[0.1mm]
 \emph{( a \xmybox[green]{\bf dog} ) ( sitting ) ( on a sidewalk ) ( next to a street . )}
\end{small}
\end{tcolorbox}
\end{minipage}
\vspace{-2mm}
\caption{\small Our model accepts feedback from a human teacher in the form of natural language. We generate captions using the current snapshot of the model and collect feedback via AMT. The annotators are requested to focus their feedback on a single word/phrase at a time. Phrases, indicated with brackets in the example, are part or our captioning model's output. 
We also collect information about which word the feedback applies to and its suggested correction. This information is used to train our \emph{feedback network}.} 
\label{fig:web_feedback}
\vspace{-3mm}
\end{figure}
