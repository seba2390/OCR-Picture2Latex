\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib,final]{nips_2017}

% \usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{array}
\usepackage{float}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{xcolor,colortbl}

\newcommand{\HL}[1]{{\color{blue}{[Huan: #1]}}}
\newcommand{\SF}[1]{{\color{red}{[Sanja: #1]}}}

\newtcbox{\mybox}[1][red]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black,
boxsep=0pt,left=1pt,right=1pt,top=2pt,bottom=2pt,
boxrule=0pt,bottomrule=1pt,toprule=1pt}

\newtcbox{\xmybox}[1][red]{on line,
arc=4pt,colback=#1!15!white,colframe=#1!50!black,
before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1.2pt,
boxsep=0pt,left=2.5pt,right=2.5pt,top=0.4pt,bottom=0.4pt}

\newtcbox{\ymybox}[1][black]{on line,
arc=3pt,colback=white,colframe=#1!50!black,
before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1.0pt,
boxsep=0pt,left=3pt,right=5pt,top=2pt,bottom=1.5pt}

%\title{Phrase-based Image Captioning with Natural Language Feedback}
\title{Teaching Machines to Describe Images via Natural Language Feedback}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Huan Ling$^1$, Sanja Fidler$^{1,2}$ \\
  %Department of Computer Science\\
  University of Toronto$^1$, Vector Institute$^{2}$\\
  \texttt{\{linghuan,fidler\}@cs.toronto.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\t
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\vspace{-2.5mm}
\begin{abstract}
%In this work we aim at describing image caption in a hierarchical approach. Current image captioning methods are usually trained to generated a whole sentence by maximum likelihood estimation, and use metrics, such as BLEU, METEOR and ROUGE to evaluate the whole generted sentence. In this paper, based on the intuition that people can always describe an image by combining short phrases, such as NP(who) + VP(doing what) + PP(at where), we introduce a Phrase-Rnn to generate phrase level guidance about how to describe the image and a Word-Rnn to generate detail words for each phrases. Also, inspired by recent work of reinforcement learning image caption, we propose a more intuitive evaluation metric,which combine existing evaluation metrics and human feedbacks, to evaluate phrase grammer correctness, attention accuracy, caption accuracy. We also show how to use PG method to directly optimize this metric.
 
 Robots will eventually be part of every household. 
It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a \emph{feedback network} that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions. 

\end{abstract}

\input{intro}
\input{related}
\input{method}
\input{results}
\input{conc}
\input{appendix}


\begin{thebibliography}{10}

\bibitem{herb}
Cmu's herb robotic platform, \url{http://www.cmu.edu/herb-robot/}.

\bibitem{tay}
Microsoft's tay, \url{https://twitter.com/tayandyou}.

\bibitem{Dai17}
Bo~Dai, Dahua Lin, Raquel Urtasun, and Sanja Fidler.
\newblock Towards diverse and natural image descriptions via a conditional gan.
\newblock In {\em arXiv:1703.06029}, 2017.

\bibitem{Das16}
A.~Das, S.~Kottur, K.~Gupta, A.~Singh, D.~Yadav, J.~M. Moura, D.~Parikh, and
  D.~Batra.
\newblock Visual dialog.
\newblock In {\em arXiv:1611.08669}, 2016.

\bibitem{shaping}
Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles~L. Isbell, and
  Andrea~Lockerd Thomaz.
\newblock Policy shaping: Integrating human feedback with reinforcement
  learning.
\newblock In {\em NIPS}, 2013.

\bibitem{Judah10}
K.~Judah, S.~Roy, A.~Fern, and T.~Dietterich.
\newblock Reinforcement learning via practice and critique advice.
\newblock In {\em AAAI}, 2010.

\bibitem{Kaplan17}
Russell Kaplan, Christopher Sauer, and Alexander Sosa.
\newblock Beating atari with natural language guided reinforcement learning.
\newblock In {\em arXiv:1704.05539}, 2017.

\bibitem{Karpathy15}
A.~Karpathy and L.~Fei-Fei.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In {\em CVPR}, 2015.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{DBLP:journals/corr/KirosSZ14}
Ryan Kiros, Ruslan Salakhutdinov, and Richard~S. Zemel.
\newblock Unifying visual-semantic embeddings with multimodal neural language
  models.
\newblock {\em CoRR}, abs/1411.2539, 2014.

\bibitem{Knox13}
W.~Bradley Knox, Cynthia Breazeal, , and Peter Stone.
\newblock Training a robot via human feedback: A case study.
\newblock In {\em International Conference on Social Robotics}, 2013.

\bibitem{Knox12}
W.~Bradley Knox and Peter Stone.
\newblock Reinforcement learning from simultaneous human and mdp reward.
\newblock In {\em Intl. Conf. on Autonomous Agents and Multiagent Systems},
  2012.

\bibitem{Westlund16}
Jacqueline Kory~Westlund, Jin~Joo Lee, Luke Plummer, Fardad Faridi, Jesse Gray,
  Matt Berlin, Harald Quintus-Bosz, Robert Hartmann, Mike Hess, Stacy Dyer,
  Kristopher dos Santos, Sigurdhur \"{O}rn Adhalgeirsson, Goren Gordon, Samuel
  Spaulding, Marayna Martinez, Madhurima Das, Maryam Archie, Sooyeon Jeong, and
  Cynthia Breazeal.
\newblock Tega: A social robot.
\newblock In {\em International Conference on Human-Robot Interaction}, 2016.

\bibitem{Krause17}
Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li~Fei-Fei.
\newblock A hierarchical approach for generating descriptive image paragraphs.
\newblock In {\em CVPR}, 2017.

\bibitem{Kuhlmann04}
G.~Kuhlmann, P.~Stone, R.~Mooney, and J.~Shavlik.
\newblock Guiding a reinforcement learner with natural language advice: Initial
  results in robocup soccer.
\newblock In {\em AAAI Workshop on Supervisory Control of Learning and Adaptive
  Systems}, 2004.

\bibitem{Lebret15}
Remi Lebret, Pedro~O. Pinheiro, and Ronan Collobert.
\newblock Phrase-based image captioning.
\newblock In {\em arXiv:1502.03671}, 2015.

\bibitem{Levine:2016}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em J. Mach. Learn. Res.}, 17(1):1334--1373, 2016.

\bibitem{Weston16b}
Jiwei Li, Alexander~H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason
  Weston.
\newblock Dialogue learning with human-in-the-loop.
\newblock In {\em arXiv:1611.09823}, 2016.

\bibitem{Li16}
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan
  Jurafsky.
\newblock Deep reinforcement learning for dialogue generation.
\newblock In {\em arXiv:1606.01541}, 2016.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, pages 740--755. 2014.

\bibitem{Spider}
Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy.
\newblock Improved image captioning via policy gradient optimization of spider.
\newblock In {\em arXiv:1612.00370}, 2016.

\bibitem{Maclin94}
Richard Maclin and Jude~W. Shavlik.
\newblock Incorporating advice into agents that learn from reinforcements.
\newblock In {\em National Conference on Artificial Intelligence}, pages
  694--699, 1994.

\bibitem{stanfordCore}
Christopher~D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven~J.
  Bethard, and David McClosky.
\newblock The {Stanford} {CoreNLP} natural language processing toolkit.
\newblock In {\em ICLR}, 2014.

\bibitem{Mataric17}
Maja~J. Matari\v{c}.
\newblock Socially assistive robotics: Human augmentation vs. automation.
\newblock {\em Science Robotics}, 2(4), 2017.

\bibitem{mnih2015humanlevel}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{Ng99}
Andrew~Y. Ng, Daishi Harada, and Stuart~J. Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In {\em ICML}, pages 278--287, 1999.

\bibitem{deepRnn}
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.
\newblock How to construct deep recurrent neural networks.
\newblock In {\em Association for Computational Linguistics (ACL) System
  Demonstrations}, pages 55--60, 2014.

\bibitem{Mixer}
Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba.
\newblock Sequence level training with recurrent neural networks.
\newblock In {\em arXiv:1511.06732}, 2015.

\bibitem{Selfcritical}
Steven~J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava
  Goel.
\newblock Self-critical sequence training for image captioning.
\newblock In {\em arXiv:1612.00563}, 2016.

\bibitem{Silver_2016}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  van~den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, 2016.

\bibitem{SimoCVPR15}
Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, and Raquel Urtasun.
\newblock Neuroaesthetics in fashion: Modeling the perception of beauty.
\newblock In {\em CVPR}, 2015.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock 23, 2015.

\bibitem{Tan16}
Ying~Hua Tan and Chee~Seng Chan.
\newblock phi-lstm: A phrase-based hierarchical lstm model for image
  captioning.
\newblock In {\em ACCV}, 2016.

\bibitem{Thomaz06}
A.~Thomaz and C.~Breazeal.
\newblock Reinforcement learning with human teachers: Evidence of feedback and
  guidance.
\newblock In {\em AAAI}, 2006.

\bibitem{Vinyals15}
Oriol Vinyals and Quoc Le.
\newblock A neural conversational model.
\newblock In {\em arXiv:1506.05869}, 2015.

\bibitem{Weston16}
Jason Weston.
\newblock Dialog-based language learning.
\newblock In {\em arXiv:1604.06045}, 2016.

\bibitem{RL1992}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock In {\em Machine Learning}, 1992.

\bibitem{Xu15}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhutdinov, Richard Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In {\em ICML}, 2015.

\bibitem{yu16}
Yanchao Yu, Arash Eshghi, and Oliver Lemon.
\newblock Training an adaptive dialogue policy for interactive learning of
  visually grounded word meanings.
\newblock In {\em Proc. of SIGDIAL}, 2016.

\bibitem{yu17}
Yanchao Yu, Arash Eshghi, Gregory Mills, and Oliver Lemon.
\newblock The burchak corpus: a challenge data set for interactive learning of
  visually grounded word meanings.
\newblock In {\em Workshop on Vision and Language}, 2017.

\end{thebibliography}

\small{\bibliographystyle{plain}}
\end{document}