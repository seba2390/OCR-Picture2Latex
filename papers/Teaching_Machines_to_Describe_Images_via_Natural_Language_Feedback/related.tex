\vspace{-3.5mm}
\section{Related Work}
\label{sec:related}
\vspace{-1.5mm}

%Robots will eventually be part of every household. %It is unrealistic to expect that the robot 
%It is thus critical to enable algorithms to learn from and be guided by non-expert users. 
Several works incorporate human feedback to help an RL agent learn faster.  %Common ways include reward shaping~\cite{Thomaz06} which adds the human reward  to the MDP reward at every time step. 
~\cite{Thomaz06} exploits humans in the loop to teach an agent to cook in a virtual kitchen. The users watch the agent learn and may intervene at any time to give a scalar reward. Reward shaping~\cite{Ng99} is used to incorporate this information in the MDP.
%by adding both the human and the MDP reward at every time step. 
~\cite{Judah10} iterates between ``practice'', during which the agent interacts with the real environment, and a critique session where a human labels any subset of the chosen actions as good or bad. In~\cite{Knox12}, the authors compare different ways of incorporating human feedback, including reward shaping, Q augmentation, action biasing, and control sharing. The same authors implement their TAMER framework on a real robotic platform~\cite{Knox13}.~\cite{shaping} proposes policy shaping which incorporates right/wrong feedback by utilizing it as direct policy labels.
These approaches mostly assume that humans provide a numeric reward, unlike in our work where the feedback is given in natural language.

A few attempts have been made to advise an RL agent using language.~\cite{Maclin94}'s pioneering work translated advice to a short program which was then implemented as a neural network. The units in this network represent Boolean concepts, which recognize whether the observed state satisfies the constraints given by the program. In such a case, the advice network will encourage the policy to take the suggested action.~\cite{Kuhlmann04} incorporated natural language advice for a RoboCup simulated soccer task. They too translate the advice in a formal language which is then used to bias action selection. Parallel to our work,~\cite{Kaplan17} exploits textual advice to improve training time of the A3C algorithm in playing an Atari game. %While both theirs and our work exploit sentence embeddings to provide additional reward, the domains are very different and thus hard to compare. %Advice is used both as an additional reward, as well as to bias action selection. Our work differs in the domain (caption generation)
Recently,~\cite{Weston16,Weston16b} incorporates human feedback to improve a text-based QA agent. Our work shares similar ideas, but applies them to the problem of image captioning.

%We review recent related work in image captioning. %In particular, our base phrase captioning model shares similarity with~\cite{Lebret15} that generates phrases conditioned on the image. Most similar captioning model to ours is~\cite{Lebret15} who propose a two-level LSTM to generate sentence. 
Captioning represents a natural way of showing that our algorithm understands a photograph to a non-expert observer. This domain has received significant attention~\cite{Karpathy15,Xu15,DBLP:journals/corr/KirosSZ14}, achieving impressive performance on standard benchmarks.  
Our phrase model shares the most similarity with~\cite{Lebret15}, but differs  in that exploits attention~\cite{Xu15}, linguistic information, and RL to train. 
Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics~\cite{Spider,Selfcritical,Dai17}. We follow this line of work. However, to the best of our knowledge, our work is the first to incorporate natural language feedback into a captioning model. Related to our efforts is also work on dialogue based visual representation learning~\cite{yu16,yu17}, however this work tackles a simpler scenario, and employs a slightly more engineered approach.

We stress that our work differs from the recent efforts in conversation modeling~\cite{Li16} or visual dialog~\cite{Das16} using Reinforcement Learning. These models aim to mimic human-to-human conversations while in our work the human converses with and guides an artificial learning agent. %We also emphasize the difference with learning-from-demonstration~\cite{Duan17} where an expert typically provides a trajectory which the agent tries to imitate, but the expert does not remain in the loop when the agent learns. 