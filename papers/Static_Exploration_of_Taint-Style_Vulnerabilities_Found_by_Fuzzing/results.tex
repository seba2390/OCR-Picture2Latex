\begin{table*}[!tbh]
  \centering
  \begin{tabular}{p{6cm}lrr}
  \toprule
  Fuzzer-Discovered Vulnerability & CVE ID & Explored Matches & True Positives \\
  \toprule
 Out-of-bounds read (IP) & CVE-2016-10377~\cite{cve201610377} & 5 & 0 \\
 Out-of-bounds read (TCP) & CVE-2017-9264~\cite{cve20179264}  & 10 & 0 \\
 Out-of-bounds read (UDP) & CVE-2017-9264 & 2 & 1 \\
 Out-of-bounds read (IPv6)& CVE-2017-9264 & 3 & 0 \\
 Remote DoS due to assertion failure & CVE-2017-9214~\cite{cve20179214} & 22 & 0 \\
 Remote DoS due to unhandled packet & CVE-2017-9263~\cite{cve20179263} & 34 & 0 \\
 Out-of-bounds read & CVE-2017-9265~\cite{cve20179265} & 1 & 0 \\
  \bottomrule
  \multicolumn{2}{c}{Total} & 96 & 1 \\
  \bottomrule
  \end{tabular}
  \caption{Summary of static vulnerability exploration carried out on vulnerabilities found by fuzzing Open vSwitch. For each fuzzer-discovered vulnerability, our prototype generate a vulnerability template, and matched it against the entire codebase.}
  \label{tab:matches}
\end{table*}

\begin{table*}[!tbh]
  \centering
  \begin{tabular}{lrrr}
  \toprule
  CVE ID & Explored matches & Ranked high (untested) & Reduction in FP (in \%)\\
  \toprule
  CVE-2016-10377 & 5 & 0 & 100\\
  CVE-2017-9264  & 10 & 0 & 100 \\
  CVE-2017-9264 & 2 & 2 & 0 \\
  CVE-2017-9264 & 3 & 0 & 100 \\
  CVE-2017-9214 & 41 & 17 & 59 \\
  CVE-2017-9263 & 34 & 17 & 50 \\
  CVE-2017-9265 & 1 & 0 & 100 \\
  \bottomrule
  Total & 96 & 36 & 62 \\
  \bottomrule
  \end{tabular}
  \caption{Effectiveness of our matching ranking algorithm in highlighting untested code, and assisting in fast review of matches.}
  \label{tab:ranking}
\end{table*}

We evaluated our approach on multiple versions of Open vSwitch, an open-source virtual switch used in data centers.
We chose Open vSwitch for evaluation because (i) it is a good representative of production code; (ii) it has insufficient test harnesses suitable for fuzzing, resulting in program edge coverage of less than 5\%.

Our evaluations were performed using afl-fuzz for fuzzing, AddressSanitizer for fault localization, falling back to our implementation of differential slice-based fault localization, and our implementation of static template generation, matching, and ranking algorithms.
Experiments were carried out on a 64-bit machine with 80 CPU threads (Intel Xeon E7-4870) clocked at 2.4 GHz, and 512 GB RAM.

\paragraph{Fuzzing and Fault Localization}
Using the baseline fuzzer, we discovered multiple out-of-bounds reads and assertion failures in packet parsing code in Open vSwitch.
All the discovered flaws were triaged to ascertain their security impact, and subsequently reported upstream and fixed.
For each unique vulnerability, we used our fault localization module comprising AddressSanitizer, and differential execution slicing, to determine the lines of code triggering the vulnerability.

\paragraph{Template Matching}
Using localized code, we automatically generated a template suitable for matching similar code patterns elsewhere in the codebase.
For example, the AST snippet shown in Listing~\ref{lst:ovs_ast} was parsed to derive a template for CVE-2017-9264.
Subsequently, we used the tool {\tt clang-query} to perform template matching using the derived template.
Listing~\ref{lst:ovs_query} shows the outcome of template matching for one of the bugs comprising CVE-2017-9264.
For each vulnerability that the fuzzer discovered, we counted the number of matches (excluding the known vulnerability itself) returned using template matching.

We used semantic template matching only when syntactic template matching was too broad to capture the code pattern underlying the vulnerability.
For example, if a program crash was caused by a failed assertion, syntactic templates (that matched calls to all assertion statements), were augmented with semantic templates (that matched a smaller subset of assertion statements involving tainted data types).

\paragraph{Ranking}
The returned matches were ranked using our proposed ranking algorithm (see Algorithm~\ref{alg:ranking}), and the ranked output was used as a starting point for manual security audit.
Matches ranked high were reviewed first.
This enabled us to devote more time to audit untested code, than the code that had already undergone testing.

\begin{table*}[!tbh]
  \centering
  \begin{tabular}{lrrrrrp{2cm}}
  \toprule
  CVE ID & Localization & Syntactic & Semantic & Ranking & Total Run Time & Normalized \\
  \toprule
  CVE-2016-10377 & 82ms & 1.66s & -- & 63ms & 1.80s & 0.20x \\
  CVE-2017-9264 (TCP)  & 84ms & 3.20s & -- & 64ms & 3.34s & 0.25x \\
  CVE-2017-9264 (UDP) & 86ms & 4.77s & -- & 59ms & 4.91s & 0.37x\\
  CVE-2017-9264 (IPv6) & 91ms & 4.71s & -- & 60ms & 4.86s & 0.36x\\
  CVE-2017-9214 & 9ms & 8.44s & 44.17s & 60ms & 52.67s & 5.51x \\
  CVE-2017-9263 & 9ms & 11.88s & 44.26s & 59ms & 57.09s & 5.97x \\
  CVE-2017-9265 & 111ms & 5.74s & -- & 56ms & 5.9s & 0.62x\\
  \bottomrule
  \end{tabular}
  \caption{Run times of fault localization, template matching, and match ranking for all statically explored vulnerabilities in Open vSwitch. The absolute and relative (to code compilation) run times for our end-to-end analysis is presented in the final two columns. A normalized run time of $2$x denotes that our end-to-end analysis takes twice as long as code compilation.}
  \label{tab:runtime}
\end{table*}

\subsection{Analysis Effectiveness}

We evaluated the effectiveness of our approach in two ways: Quantifying (i) the raw false positive rate of our analysis; (ii) the benefit of the proposed ranking algorithm in reducing the effective false positive rate after match ranking was done.

To quantify the number of raw false positives, we counted the total number of statically explored matches, and the number of true positives among them.
A match was deemed a true positive if manual audit revealed that the tainted instruction underwent no prior sanitization and was thus potentially vulnerable.
Table~\ref{tab:matches} summarizes our findings.
Our prototype returned a total of 96 matches for the 7 vulnerabilities found by fuzzing (listed in column 1 of Table~\ref{tab:matches}).
Out of 96 matches, only one match corresponding to CVE-2017-9264 was deemed a new potential vulnerability.
This was reported upstream and subsequently patched~\cite{ovsmatch}.
Moreover, the reported (potential) vulnerability helped OvS developers uncover another similar flaw in the DHCPv6 parsing code that followed the patched UDP flaw.

Our ranking algorithm ranked untested code over tested code, thereby helping reduce the manual effort involved in validating potential false positives.
Although it is hard to correctly quantify the benefit of our ranking algorithm in bringing down the false positive rate, we employ a notion of {\it effective} false positive rate.
We define the effective false positive rate to be the false positive rate only among highly ranked matches.
This is intuitive, since auditing untested code is usually more interesting to a security analyst than auditing code that has already undergone testing.
Table~\ref{tab:ranking} summarizes the number of effective false positives due to our analysis.
In total, there were 36 matches (out of 96) that were ranked high, bringing down the raw false positive rate by 62\%.
Naturally, we confirmed that the single true positive was among the highly ranked matches.

Match ranking helps reduce, but not eliminate the number of false positives.
Indeed, 1 correct match out of 36 matches is very low.
Having said that, our approach has borne good results in practice, and has helped advance the tooling required for secure coding.
The additional patch that our approach contributed to is not the only way in which our approach met this objective.
We discovered that the template derived from the vulnerability CVE-2016-10377 present in an earlier version of Open vSwitch (v2.5.0), could have helped eliminate a similar vulnerability (CVE-2017-9264) in a later version (v2.6.1), perhaps during software development itself.
This shows that our approach is suitable for regression testing.
Indeed, OvS developers noted in personal communications with the authors that the matches returned by our tooling not only encouraged reasoning about corner cases in software development, but helped catch bugs (latent vulnerabilities) at an early stage.

\subsection{Analysis Runtime}

We quantified the run time of our tooling by measuring the total and constituent run times of our work-flow steps, starting from fault localization, and template matching, to match ranking.
Table~\ref{tab:runtime} presents our analysis run times for each of the fuzzer-discovered vulnerabilities in Open vSwitch.
Since fault localization was done using dynamic tooling (AddressSanitizer/coverage tracing), it was orders of magnitude faster (ranging between 9--111 milliseconds) than the time required for static template matching.
For each fuzzer-discovered vulnerability, we measured the template matching run time as the time required to construct and match the vulnerability template against the entire codebase.
Template matching run time comprised between 92--99\% of the end-to-end runtime of our tooling, and ranged from 1.8 seconds to 57.09 seconds.
Syntactic template matching was up to 4x faster than semantic template matching.
This conformed to our expectations, as semantic matching is slower due to the need to encode (and check) program data and control flow in addition to its syntactic properties.
Nonetheless, our end-to-end vulnerability analysis had a normalized run time (relative to code compilation time) of between 0.2x to 5.97x.
The potential vulnerability that our analysis pointed out in untested UDP parsing code, was returned in roughly a third of the time taken for code compilation of the codebase.
This shows that our syntactic analysis is fast enough to be applied on each build of a codebase, while our semantic analysis is more suitable to be invoked during daily builds.
Moreover, given the low run time of our analysis, templates derived from a vulnerability discovered in a given release may be continuously applied to future versions of the same codebase as part of regression testing.