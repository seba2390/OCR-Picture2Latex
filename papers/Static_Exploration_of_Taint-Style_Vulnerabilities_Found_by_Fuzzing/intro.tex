Software exploitation is asymmetric, requiring only a single flaw to compromise a system, and at times a network of systems.
The complexity inherent to contemporary software increases their attack surface, and plays into the hands of attackers.
Consequently, it is imperative that each and every software module be well analyzed and tested, before a release.
This is especially true for applications that routinely handle untrusted user input such as network and data parsers.
Fuzz testing has been the tool of choice for conducting security assessments of these classes of applications.

Although fuzz testing is effective at uncovering software vulnerabilities, it has two practical limitations.
First, fuzzing may encounter coverage bottlenecks such as cryptographic code, and non-atomic comparison operations that limit the test coverage achieved, and impede the discovery of latent vulnerabilities.
Second, several code bases do not contain test harnesses for security-critical program APIs, making thorough testing dependent on writing new test cases.
Writing test cases is a manual process that requires domain-specific knowledge pertaining to the software under analysis.
{\it Even} well-written unit tests do not necessarily permit a thorough systems evaluation.
For example, networking stacks contain asynchronous, and stateful API calls that are invoked in an event-driven fashion.
Without a practical set-up that injects the right sequence of messages, it becomes difficult to test these APIs.
Having said that, simple pre-existing test cases can provide a starting point for a wider exploration of the codebase.

\begin{figure*}[t]
  \centering
  \includegraphics[scale=.6]{workflow.eps}
  \caption{Work-flow of static vulnerability exploration. Templates generated from fault localized code are used to find recurring instances of a fuzzer-discovered vulnerability. The resulting matches are ranked to focus attention on potential recurring vulnerabilities in untested code.}
  \label{fig:workflow}
\end{figure*}

In this paper, we build on the idea that static analysis can perform a broader search for vulnerable code patterns, starting from a handful of fuzzer-discovered program failures.
Our working hypothesis is that {\it any} readily available fuzzable test harness can be used to bootstrap our analysis, reducing the burden of test writing.
Therefore, we begin by fuzzing an existing test harness packaged with a codebase and expect to find a handful of program crashes.
Subsequently, our analysis proceeds in three steps.
%Most often, open-source software already contain functional test cases that may be leveraged for fuzzing.
First, we narrow down the root cause of the uncovered crashes using a memory error detector such as AddressSanitizer, falling back to execution slice based fault localization when the fault is not memory-based.
Fault localization not only narrows the search for vulnerable code patterns, but also provides syntactic and semantic information about the underlying fault.
Second, we automatically generate vulnerability templates using localized faulty code.
Vulnerability templates encode both syntactic, as well as semantic features of the faulty code, making our approach superior to na{\"i}ve text-based pattern matchers such as {\tt grep}.
% Vulnerability templates match those code snippets that are syntactically, or semantically equivalent to a fuzzer-discovered vulnerability.
Third, we rank matching code snippets (returned by template matching) by using fuzzer test coverage data: Matches comprising untested code is ranked higher than those that do not.
Such a ranking system helps prioritize manual audit of untested code over code that has already undergone fuzz testing.
We use {\tt afl-fuzz}, a contemporary coverage-guided fuzzer, and Clang/LLVM instrumentation and static analysis framework for prototyping our approach.

We evaluate our prototype using a case study of Open vSwitch (OvS), an open-source virtual switch implementation used in data centers.
We chose Open vSwitch because (i) it routinely handles untrusted input, and (ii) its existing fuzzable test cases achieve a test coverage of less than 5\% providing a low assurance on software security.
Results from our case study are promising.
Our prototype has uncovered a potential recurring vulnerability in a portion of OvS that lacked a test harness.
Moreover, in one instance, template matching has proved to be helpful in flagging a recurring vulnerability that originated in an older release of OvS.
This shows that static analysis can not only complement fuzzing, but enable security assessments to be made during software development.
To facilitate independent evaluation, we have open-sourced our prototype, that is available at \url{https://www.github.com/test-pipeline}.

\noindent
{\bf Contributions:}
\begin{itemize}
\item We present an approach to improve the effectiveness of source code security audit that benefits from both the precise diagnostics of a fuzzer, and the breadth of analysis of a static analyzer.
\item We prototype our approach using {\tt afl-fuzz}, a contemporary coverage-guided fuzzer, and the Clang/LLVM compiler toolchain.
Our prototype automatically generates vulnerability templates from a fuzzer corpus, ranking the matches returned by template matching based on novelty.
\item We evaluate our prototype using a case study of Open vSwitch codebase.
Our approached has (i) helped discover one potential vulnerability in a portion of Open vSwitch that lacked a test harness; (ii) facilitated vulnerability checks at an early stage; and (iii) reduced false alarms by 50-100\% in most cases demonstrating that coverage-based match ranking is effective in combating false positives.
\end{itemize}

\begin{figure*}[t]
  \centering
\begin{minipage}[t]{.43\linewidth}
\lstinputlisting[style=minipage,framexrightmargin=0em,label=lst:example,caption=A representative fuzzer test harness in which two synthetic denial of service vulnerabilities have been introduced by calling the {\tt abort()} function. Fault localized code is shown in red.]{test.c}
\end{minipage}
\quad
\begin{minipage}[t]{.46\linewidth}
\lstinputlisting[style=minipage,xleftmargin=2em, numbers=none,label=lst:query, caption=Template derived from fault localized code (green text) is matched against the test harness source code. Match \#2 lists the line of code containing a similar fault pattern that is likely untested by a fuzzer.]{output.txt}    
\end{minipage}
\end{figure*}
