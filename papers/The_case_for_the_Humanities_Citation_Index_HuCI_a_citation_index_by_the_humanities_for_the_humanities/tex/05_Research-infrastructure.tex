\section{Research infrastructure}\label{sec:research-infrastructure}

We propose to adopt a federated and distributed approach to design the research infrastructure required to create the Humanities Citation Index. Such an approach implies that the creation of citation data is delegated to a federation of cooperating institutions rather than being carried out by a single, central entity. The scenario we envisage (see Fig. \ref{fig:huci-high-level}) is having a network of GLAM institutions, each of them contributing citation data extracted from their digitised collections through a common open source software platform. These data will then be harvested, aggregated and consolidated to become the HuCI citation corpus, available to researchers both via search and exploration interfaces, as well as data dumps to be further analysed and visualised through external tools. In what follows we describe in more detail the proposed architecture, as well as the challenges related to its implementation.

\begin{figure}[h]%
\centering
\includegraphics[width=0.4\textwidth]{img/layers.png}
\caption{The 4-layer architecture of the Humanities Citation Index. The solid arrows show that an item in one layer (ending node) uses the information provided by an item in another layer (starting node). The dashed line highlights existence of a federation mechanism between the items of the same layer.
\label{fig:huci-high-level}}
\end{figure}

\subsection{Distributed and federated approach}

A distributed and federated approach recognizes the central role that libraries and other cultural heritage institutions could play with respect to the curation of their digitized collections. A modern notion of collection curation, we argue, ought to include the extraction of structured contents (e.g., citations) from digitized materials. This could take the form of manual verification, carried out by librarians, of automatically extracted information, as advocated by \cite{lauscher_linked_2018} for the specific case of citations.   

At a technical level, a federated model has the advantage that it gives individual institutions a certain degree of freedom in deciding what can and should be made openly accessible -- thus harvested from a federation of partner institutions -- and what, on the contrary, should remain accessible only internally, within the boundaries of institutional access. We can call this access model ``open by default and closed by necessity''. A typical case where it proves useful is the display of contextual information about citations (i.e., an excerpt of the text surrounding the citation), which can be complicated due to copyright restrictions. A library holding digitized materials under copyright will want to give exclusively to its users full access to citation contexts from these publications, while still sharing with the wider community data extracted from these publications that do not fall under copyright, such as citation data. It is worth mentioning on this respect that citation data are just facts, and as such cannot be copyrighted. Thus, following the guidelines in \cite{peroni_open_2018} and the Initiative for Open Citations (I4OC)\footnote{\url{https://i4oc.org}.}, they should be released as public domain material using appropriate solutions, such as the CC0 waiver.

Moreover, a distributed approach makes sense not only for the collection of citation data but also for the hosting of the resulting citation corpus. In fact, collecting all the citation data from the whole scientific knowledge in one single, centralized repository -- albeit feasible -- would raise considerable issues in terms of maintenance and performances. For instance, consider that the data available in the OpenCitations Corpus includes over 7.5 million bibliographic resources, mainly (~80\%) journal articles and their issues, volumes, and journals \cite{damato_one_2017}. Their bibliographic metadata and their provenance information occupy more than 2 billion triples. Supposing that, roughly, 45 million new journal articles are published every year \cite{van_harmelen_end_2017} and considering that 10\% belong to the A\&H, we can estimate 4.5 million new articles in the A\&H every year. This amount of data can be stored in 230 gigabytes using the model adopted by OpenCitations -- thus, we will need more than 5.7 terabytes for storing the metadata of all the A\&H articles published in the ``Web-era'', since 1994. While such amount of bytes can be even manageable in a big file system, these figures can drastically increase if we want to keep track of all the citation links among articles, if we start to ingest data coming from books (that are the primary publication object for the A\&H which contains more citation links than any other scholarly medium), and if we consider publications that are older than 25 years. 

The availability and cost of storage is something that will become more manageable in time. However, there is another, probably more pressing issue concerning the scenario mentioned above: having an infrastructure that guarantees efficient data querying is something very demanding when a large amount of data is actually available in a single and centralised database. As rough estimate considering the figures above, we would require 30 billions triples for handling the 4.5 million articles in A\&H in the past 25 years. It is worth mentioning that this figure does not include books and all the older literature, nor any further extension of the kinds of data to store – for instance, the figures above are based on what is currently stored in the OpenCitations Corpus and, as such, do not account for abstracts or authors’ affiliations. Therefore it emerges that a centralised solution like storing all the data in a single database, is not feasible in the long-term.

\subsection{Architecture}

The HuCI infrastructure that we propose consists of four main layers, whose interplay is schematically depicted in Fig. \ref{fig:huci-high-level}:
\begin{enumerate}
    \item The \textbf{data collection layer} consists of a network of content providers, who hold digitised materials (be they public or private) and contribute to the growth and coverage of the citation index by making openly available the citation data extracted from their holdings.
    \item The \textbf{data federation layer} is conceived as a federation of decentralised citation databases based on RDF technologies where the HuCI citation data is actually stored.
    \item The \textbf{service layer} provides HTTP APIs that allow for standardised access to HuCI data (e.g., via SPARQL endpoints and common REST Web APIs), external resources (e.g., those included in library or archive metadata) and services (e.g., author disambiguation).
    \item Finally, the \textbf{application layer} is an ecosystem of tools and software components, plugged on HuCI's virtual triplestore (via the previous layer or by consuming directly the data from the data federation layer), that allow A\&H researchers to discover and identify relevant literature for their research, and provides bibliometric insights into the citation data.
\end{enumerate}

In what follows we discuss in greater detail each of these infrastructure components. In addition to such components, it is important that the various providers of citation data are compliant as much as possible with the Principles of Open Scholarly Infrastructures (POSI) introduced in \cite{bilder_principles_2015}. These principles are organised in three themes: Governance, Sustainability and Insurance. The latter theme specifies technological dimensions that should be guaranteed: \textbf{open source} (of all software required to run the infrastructure), \textbf{open data} (of all relevant data necessary to replicate it), \textbf{available data} (i.e., the availability of underlying data as periodic data dumps) and \textbf{patent non-assertion} (i.e., avoid using patents to prevent the community to replicate an infrastructure). If followed strictly, POSI should guarantee the long term sustainability of infrastructures that provide open scholarly data and open source software that can be used to build service new and innovative services. Several infrastructures (including OpenCitations, Crossref and DataCite) have run self-assessment exercises to measure their compliance with POSI, as introduced in the POSI website at \url{https://openscholarlyinfrastructure.org/posse}.

\subsubsection{Data Collection}

The first layer of HuCI's architecture is constituted by a network of GLAM institutions playing an active role in the production and curation of citation data. While each institution is responsible for the extraction of citation data from their digitised holdings, we envisage the development of a common open source platform that can ease the tasks of extracting citations from publications as well as the manual curation of such citations. 

An example of software that could be deployed in the data collection layer is the Scholar Library (SL) platform\footnote{\url{https://github.com/ScholarIndex/ScholarLibrary}.} \cite{colavizza_linked_2018}. While it provides the typical functionalities of any digital library software (e.g., display of image and OCR), SL integrates specific components that perform the extraction of bibliographic references from digitized publications, and their disambiguation against bibliographic databases. In particular, it includes two components for the enrichment of publications with citation data: a machine learning-based citation extractor as well as a component to match bibliographic references against the unified catalogue of Italian libraries \cite{colavizza_references_2018}. 

The SL was designed to be \textit{deployed locally} while staying \textit{connected globally}: the local deployment ensures that digitized materials that cannot be shared openly remain private; APIs allow to harvest citation data from each local instance of SL and to connect them into a global citation index. As such, this platform could be deployed by partner institutions to facilitate the extraction and sharing of open citation data from their digitised holdings.

%In the future, we will enable the integration of a wider range of external components that perform specific tasks on citation data (extraction, matching, author disambiguation, record deduplication, etc.). To this end we will develop an API specification to be implemented by providers of mining services and other components.

\subsubsection{Data Federation}

As a long-term solution to devising a scalable infrastructure for the storage of A\&H’s citation data we propose the HuCI virtual triple store, a federation of decentralised citation databases that can cooperate with each other by means of Web technologies, in particular RDF. Along the aforementioned lines, the interlinked databases of open citation data mentioned before have been recently released in order to address this aspect. The idea, in this aspect, is to organise existing and future open citations and scholarly metadata repositories (e.g., OpenCitations’ datasets, Wikidata, OpenAIRE) as part of a bigger and interlinked graph of open repositories\footnote{Something strongly supported by the 2017 report of the Confederation of Open Access Repositories (COAR), available at \url{https://www.coar-repositories.org/files/NGR-Final-Formatted-Report-cc.pdf}.}, which would allow them to scale in terms of their infrastructure and the amount of data they need to handle. This can be implemented by means of appropriate Web and Semantic Web technologies, such as RDF triplestores, which natively are able to handle federation in storing the data. The use of such technologies is also crucial for enabling the development of a decentralised network of interoperable Linked Open Data (LOD), which are hosted in several places. In particular, such interoperability should be guaranteed by using the same data model for exposing the citation data involved, as introduced below. In essence, HuCI is a virtual database, since it must be implemented as a federation of repositories which provide access to their citation data via the HTTP protocol according to a particular shared data model, and which enable to expose the data in multiple formats (CSV, JSON, RDF-based) to foster maximum understandability for both humans and machines. 
%Other examples of proposed protocols or standards that rely on distributed architectures are the International Image Interoperability Framework (IIIF) for sharing image data, the Distributed Text Services (DTS) API for sharing textual data, and the InterPlanetary File System (IPFS), a protocol for  distributed file systems.

The feasibility of handling multiple and decentralised repositories of citation data effectively should be guaranteed by adopting a general metadata model in which the citation data will be described. If a different data model will be used by one of the repositories in the federated system, an explicit alignment to such general metadata model must be provided so as to make the federation possible.

Among the possible candidate data models for describing citation data there is the OpenCitations Data Model (OCDM) \cite{peroni_opencitations_2018}. OCDM is fully based on the Semantic Publishing and Referencing (SPAR) ontologies \cite{rutkowski_spar_2018} and other standard vocabularies (FOAF, PROV, etc.) for the specification of additional information about agents and provenance data. The data model is implemented by means of the OpenCitations Ontology (OCO)\footnote{\url{https://w3id.org/oc/ontology}.}, which is not yet another bibliographic ontology, but rather simply a mechanism for grouping together existing complementary ontological entities from several other ontologies, for the purpose of providing descriptive metadata all in one place. As introduced in \cite{pan_opencitations_2020}, the OCDM has already been adopted by several projects in the scholarly domain for organising bibliographic information such as the Venice Scholar Index\footnote{\url{https://venicescholar.dhlab.epfl.ch}.} \cite{colavizza_linked_2018}, the Linked Open Citations Database (LOC-DB)\footnote{\url{https://locdb.bib.uni-mannheim.de}.} \cite{lauscher_linked_2018} and the EXCITE Project\footnote{\url{http://excite.west.uni-koblenz.de}.} \cite{hosseini_excite_2019}.

\subsubsection{Service Layer}

In addition to the two layers dedicated to data collection and federation, HuCI will comprise a layer of services (e.g. Web APIs) that will enable and regulate the flow of data between HuCI, its network of data providers, external providers of bibliographic metadata, and providers of services for the enrichment of citation data (both internal and external). In particular, we envisage three types of services:
\begin{enumerate}
    \item services to harvest citation data from the network of data providers; 
    \item services to provide standardised access to external resources (e.g., archive and library catalogues);
    \item services to enrich the aggregated citation data (e.g., interlinking, deduplication).
\end{enumerate}

To the first type of services belong the APIs that will allow participating institutions to share the citation data extracted from their digitized collections. Citation data will be exposed by using the shared data model (such as the OpenCitations Data Model discussed above) and harvested via either SPARQL-based APIs or common Web REST APIs acting as a proxy to a SPARQL endpoint -- that can be easily set up using software such as RAMOSE \cite{daquino_creating_2020}, BASIL \cite{daga_basil_nodate}, grlc \cite{sack_grlc_2016}, OBA \cite{pan_oba_2020}, and SPARQL.anything \cite{daga_facade-x_2021}. These APIs could be available as part of HuCI or be offered by external providers. Provenance information, which includes the identification of the attribution, sources, activities and additional change tracking data, is also attached to the related citation data in order to allow trackability and restorability of citation data due to some, even unpredictable, changes \cite{peroni_document-inspired_2016}.

The second type of services aims to provide unified and standardised access to bibliographic metadata present in external resources, such as archive and library catalogues. These resources can be extremely valuable in various steps of the citation mining process (citation linking, author disambiguation), yet the heterogeneity of formats in which they are exposed hampers their reuse (see Section \ref{sec:metadata-ecosystem-requirements}). These services will facilitate the access to external resources by defining a common API specification for data exchange, as well as a common data format towards which individual bibliographic formats can be mapped.

Finally, a third type of services will provide enrichment of citation data, especially through interlinking and deduplication. In fact, due to the federated nature of HuCI, it may happen that the same bibliographic entity and its citations are stored multiple times in different repositories. Thus, it is crucial to provide mechanisms and algorithms for dealing with deduplication appropriately, both for live access to data for \textit{streaming} purposes, using a particular entry-point (e.g., a certain SPARQL endpoint), and to download full \textit{dumps} of citation data available in different federated repositories.

The resolution of these conflicts could be handled by using persistent identification schemas (like DOI, Handle, ORCID or VIAF) for uniquely identifying the various resources, or by applying disambiguation mechanisms based on entities’ metadata. Of course, the more persistent identifiers are specified for a bibliographic entity, the easier its disambiguation will be and, consequently, the deduplication of bibliographic resources coming from different repositories.

A good example of integrating remote services into a common research infrastructure comes from the recent project Open Mining INfrastructure for TExt and Data (OpenMinTeD)\footnote{\url{http://openminted.eu}.}. Their API specification for processing Web services defines a protocol that allows remote NLP components to be seamlessly integrated into processing pipelines (see e.g., \cite{ba_interoperability_2016}). Similarly, an API specification will need to be developed for external services that can be used to enrich HuCI's citation data. 

\subsubsection{Application Layer}

A crucial aspect of creating a citation index for the A\&H concerns the development of user interfaces allowing researchers to explore and exploit citation data. In the technical infrastructure we propose, search and visualization tools for the citation index will plug directly into HuCI’s virtual triples store and will constitute its application layer. 
This layer will comprise user interfaces for search and visualization, as well as software components that are meant to facilitate access to citation data stored in HuCI's virtual triples store via SPARQL API or via REST APIs built upon SPARQL endpoints. 

Several tools have been developed to date to display, analyze and visualize citation data. They differ substantially with respect to the platform where they run (Web or desktop), their main purpose (analysis, visualization, search), as well as the data sources for which they offer support (e.g., Web of Science, Scopus, PubMed, Microsoft Academics, OpenCitations, etc.). In particular, among these tools, there are VOSviewer \cite{van_eck_software_2010}, Sci2 \cite{sci2_team_sci2_2009}, CiteSpace \cite{chen_searching_2004}, Cytoscape \cite{shannon_cytoscape:_2003}, Bibliographic EXplorer (BEX) \cite{di_iorio_exploring_2015}, CiteWiz \cite{elmqvist_citewiz_2007}, Docudipity \cite{poggi_exploiting_2019}, CRExplorer (CREx) \cite{thor_introducing_2016}, Science Citation Knowledge Extractor (SCKE) \cite{lent_science_2018}, Scholia \cite{nielsen_scholia_2017}, the Scholar Index (SI) \cite{colavizza_linked_2018}, OSCAR \cite{heibi_enabling_2019}, and LUCINDA\footnote{\url{https://github.com/opencitations/lucinda}.}.
  
%The Scholar Index is an open source web application, originally developed for the citation index of literature on the History of Venice: the Venice Scholar\footnote{\url{https://venicescholar.dhlab.epfl.ch}}. In addition to the two aspects highlighted above, its tight integration with the Scholar Library makes it the perfect choice as the main front-end for the future HuCI. 

%As for its functionalities, users of the SI are welcomed by a simple yet powerful textual search, which allows for searching over a) author names; b) titles of cited/citing publications; c) names of archival documents (primary sources) and d) the textual content of extracted references. The search results are organized by resource type and can be further filtered based on criteria like publication date, language or provenance of digitized materials. 

%The main avenues for exploring citation data in the SI are author and publication profiles. Author profiles consist of a list of publications by the author (with links to full-text, if available and accessible to the user) and a list of citations given/received by the author in all of her publications, summarised in a timeline that shows their distribution over time (see Fig. \ref{fig:vs-cit-prof}). Similarly, publication profiles are available for any cited/citing resource, be it a monograph, journal article or a primary source (e.g., an archival source).  This means that scholars can consult the profile of an archival fund or document and immediately see what has been published on the topic, when and by whom -- a functionality that, if provided for any humanities discipline and not only Venetian historiography, has the potential to radically change information retrieval practices of scholars in the A\&H. 

%\begin{figure}[h]%
%\centering
%\includegraphics[width=0.5\textwidth]{img/vs_2.png}
%\caption{Venice Scholar Index: the citation profile view.\label{fig:vs-cit-prof}}
%\end{figure}

%Another feature of the SI that is worth highlighting here is its \textit{interconnectivity} with other existing resources (cfr. Section \ref{sec:interlinking-collections-via-citations}). Author profiles are linked to the Virtual International Authority File (VIAF)\footnote{\url{http://viaf.org/}} whenever possible, and cited sources (primary and secondary) link back to the online library catalogues or archival information systems that do provide more detailed information about them. Additionally, an experimental feature enriches the profile page of sources and authors with a dynamically-generated list of relevant objects contained in Europeana. This feature was developed to add an aspect of serendipity to the user experiences of the SI, and uses keywords extracted from publication titles in order to find the most relevant objects among the millions of digital artifacts available in Europeana. 
