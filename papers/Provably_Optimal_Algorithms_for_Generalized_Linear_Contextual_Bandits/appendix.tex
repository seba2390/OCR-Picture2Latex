\section{Proof of \thmref{thm:glm_ucb}}
\label{sec:glm_ucb_proof}

In the following, for simplicity, we will drop the subscript $n$ when there is no ambiguity.  Therefore, $V_n$ is denoted $V$ and so on.

To prove normality-type results of the maximum likelihood estimator $\hat{\theta}$, typically we first show the $n^{-1/2}$-consistency of $\hat{\theta}$ to $\theta^*$. Then, by using a second-order Taylor expansion or Newton-step, we can prove the desired normality of $\hat{\theta}$.
More details can be found in standard textbooks such as \citet{van2000asymptotic}. %Although the proof structure is conceptually straightforward, the details are quite technical.

Since $m$ is twice differentiable with $\ddot{m} \ge 0$, the maximum-likelihood estimation can be written as the solution to the following equation
\begin{equation} \label{eq:score}
\sum_{i=1}^{n} \left(Y_i-\mu(X_i'\theta)\right) X_i= 0\,.
\end{equation}
Define 
$G(\theta) \defeq \sum_{i=1}^{n} \left(\mu(X_i'\theta)-\mu(X_i'\theta^*)\right) X_i$,
and we have 
\begin{equation} \label{eq:mle}
G(\theta^*)=0 \;\; \text{and }\; G(\hat{\theta})=\sum_{i=1}^{n} \epsilon_i X_i\,,
\end{equation} 
where the noise $\epsilon_i$ is defined in \eqnref{eq:model1}.  For convenience, define $Z \defeq G(\hat{\theta})=\sum_{i=1}^{n} \epsilon_i X_i$.

\paragraph{Step 1: Consistency of $\hat{\theta}$.} We first prove the consistency of $\hat{\theta}.$ For any $\theta_1, \theta_2 \in \R^d$, mean value theorem implies that there exists some $\bar{\theta}=v\theta_1+(1-v)\theta_2$ with $0<v<1$, such that
\begin{equation} \label{eq:taylor1}
G(\theta_1) - G(\theta_2) =\left [\sum_{i=1}^{n}\dot{\mu}(X_i' \bar{\theta})X_iX_i' \right](\theta_1-\theta_2):=F(\bar{\theta})(\theta_1-\theta_2)
\end{equation}
Since $\dot{\mu}>0$ and $\lmin(V) > 0$, we have
\[
(\theta_1-\theta_2)'(G(\theta_1)-G(\theta_2)) \ge (\theta_1-\theta_2)' (\kappa V) (\theta_1-\theta_2) > 0
\]
for any $\theta_1 \neq \theta_2$. Hence, $G(\theta)$ is an injection from $\R^d$ to $\R^d$, and so $G^{-1}$ is a well-defined function. Consequently, \eqnref{eq:score} has a unique solution $ \hat{\theta} = G^{-1}(Z)$.

Let us consider an $\eta$-neighborhood of $\theta^*$, $\B_\eta \defeq \{\theta ~:~ \norm{\theta-\theta^*} \le \eta\}$, where $\eta>0$ is a constant that will be specified later. Note that $\mathcal{B}_{\eta}$ is a convex set, thus $\bar{\theta} \in \mathcal{B}_{\eta}$ as long as $\theta_1,\theta_2\in\B_\eta$. Define $\kappa_{\eta} \defeq \inf_{\theta \in \B_\eta} \dot{\mu}(x'\theta)>0$. From \eqnref{eq:taylor1},  for any $\theta \in \mathcal{B}_\eta$,
\begin{eqnarray*}
\norm{G(\theta)}_{V^{-1}}^2 &=& \norm{G(\theta)-G(\theta^*)}_{V^{-1}}^2 \\
&=& (\theta-\theta^*)' F(\bar{\theta}) V^{-1} F(\bar{\theta}) (\theta-\theta^*) \\
&\ge& \kappa_{\eta}^2 \lmin(V) \norm{\theta - \theta^*}^2,
\end{eqnarray*}
where the last inequality is due to the fact that $F(\bar{\theta}) \mge \kappa_\eta V$.

On the other hand, Lemma~A of \citet{chen1999strong} implies that
\[
\left \{\theta ~:~\norm{G(\theta)}_{V^{-1}} \le  \kappa_{\eta} \eta \sqrt{\lmin(V)} \right \}  \subset \mathcal{B}_{\eta}\,.
\]
Now it remains to upper bound $\norm{Z}_{V^{-1}}=\norm{G(\hat{\theta})}_{V^{-1}}$ to ensure $\hat{\theta}\in\B_\eta$.  
To do so, we need the following technical lemma, whose proof is deferred to \secref{sec:lemmas}. 
\begin{lemma} \label{lm:hoeffding}
Recall $\sigma$ which is the constant in \eqnref{eq:glm2}.  For any $\delta>0$, define the following event:
\begin{equation*}
\EvtG \defeq \left\{\norm{Z}_{V^{-1}} \le  4\sigma \sqrt{d+\log(1/\delta)}\right\}\,.
\end{equation*}
Then, $\EvtG$ holds with probability at least $1-\delta$.
\end{lemma}

Suppose $\EvtG$ holds for the rest of the proof.  Then, $\eta \ge \frac{4\sigma}{\kappa_{\eta}} \sqrt{\frac{d + \log(1/\delta)}{\lmin(V)}}$ implies $\norm{\hat{\theta}_t-\theta} \le \eta$. Since $\kappa=\kappa_1$, we have $\kappa_\eta \ge \kappa$ as long as $\eta \le 1$. Thus, we have
\begin{equation} \label{eq:intial} 
\norm{\hat{\theta}-\theta} \le \frac{4\sigma}{\kappa} \sqrt{\frac{d  + \log(1/\delta)}{\lmin(V)}} \le 1\,,
\end{equation}
when $\lmin (V) \ge 16\sigma^2\left[d + \log(1/\delta)\right]/\kappa^2$. 

\paragraph{Step 2: Normality of $\hat{\theta}$.} Now, we are ready to precede to prove the normality result. 
The following assumes $\EvtG$ holds (which is high-probability event, according to \lemref{lm:hoeffding}).

Define $\Delta \defeq \hat{\theta}-\theta^*$. It follows from \eqnref{eq:taylor1} that there exists a $v \in [0,1]$ such that
\[
Z=G(\hat{\theta})-G(\theta^*)=(H+E)\Delta\,,
\] 
where $\tilde{\theta} \defeq v \theta^*+(1-v)\hat{\theta}$, $H \defeq F(\theta^*)=\sum_{i=1}^{n}\dot{\mu}(X_i' \theta^*)X_iX_i' $ and $E \defeq F(\tilde{\theta})-F(\theta^*)$. Intuitively, when $\hat{\theta}$ and $\theta^*$ are close, elements in $E$ are small.  By the mean value theorem, 
\begin{eqnarray*}
E =\sum_{i=1}^{n} \left(\dot{\mu}(X_i'\tilde{\theta}) - \dot{\mu}(X_i'{\theta^*})\right)X_iX_i'= \sum_{i=1}^{n} \ddot{\mu}(r_i)X_i'\Delta X_iX_i'
\end{eqnarray*}
for some $r_i \in \R$. Since $\ddot{\mu} \le M_\mu$ and $v \in [0,1]$, for any $x \in \R^d \setminus \{\zerovec\}$, we have
\begin{eqnarray*}
x' H^{-1/2}EH^{-1/2} x &=& (1-v) \sum_{i=1}^{n} \ddot{\mu}(r_i) X_i' \Delta \norm{x' H^{-1/2} X_i}^2 \\
&\le& \sum_{i=1}^{n} M_\mu \norm{X_i} \norm{\Delta} \norm{x' H^{-1/2} X_i}^2 \\
&\le& M_\mu \norm{\Delta}  \left( x' H^{-1/2} \left(\sum_{i=1}^{n} X_i X_i' \right) H^{-1/2} x \right) \\ 
&\le& \frac{M_\mu }{\kappa} \norm{\Delta} \norm{x}^2\,,
\end{eqnarray*}
where we have used the assumption that $\norm{X_i} \le 1$ for the second inequality. Therefore,
\begin{equation}
\norm{H^{-1/2}EH^{-1/2}} \le \frac{M_\mu}{\kappa} \norm{\Delta} \le  \frac{4M_\mu \sigma}{\kappa^2} \sqrt{\frac{d + \log(1/\delta)}{\lmin(V)}}\,. \label{eqn:normality}
\end{equation}
When $\lmin(V) \ge 64 M_\mu^2 \sigma^2(d + \log(1/\delta))/\kappa^4$, we have
\begin{equation}
\norm{H^{-1/2}EH^{-1/2}} \le 1/2\,.
\label{eqn:half}
\end{equation}

Now we are ready to prove the theorem. For any $x \in \R^d$, 
\begin{equation} 
x' (\hat{\theta}-\theta^*) \,=\, x' (H+E)^\mi Z \,=\, x' H^\mi Z - x' H^\mi E (H+E)^\mi Z\,.
\label{eqn:decomposition}
\end{equation}
Note that the matrix $(H+E)$ is nonsingular, so its inversion exists.

For the first term, $\{\epsilon_i\}$ are sub-Gaussian random variables with sub-Gaussian parameter $\sigma$.  Define
\[
D \defeq \left[X_1, X_2, \ldots, X_n \right]' \in \R^{n \times d}
\]
to be the design matrix.  Hoeffding inequality gives
\begin{equation}
\Prob\{|x' H^{-1} Z| \ge t\} \le 2 \exp\left\{-\frac{t^2}{2\sigma^2 \norm{x' H^{-1} D'}^2}\right\}\,.
\label{eqn:first-term-hoeffding}
\end{equation}
Since $H \mge \kappa V = \kappa D' D$, we have 
\[
\norm{x' H^{-1} D'}^2 = x' H^{-1} D' D H^{-1} x \le \frac{1}{\kappa^2} x' V^{-1} x = \frac{1}{\kappa^2} \norm{x}_{V^\mi}^2\,,
\]
so \eqnref{eqn:first-term-hoeffding} implies
\[
\Prob\{|x' H^{-1} Z| \ge t\} \le 2 \exp\left\{-\frac{t^2 \kappa^2}{2\sigma^2 \norm{x}_{V^\mi}^2}\right\}\,.
\]
Let the right-hand side be $2\delta$ and solve for $t$, we obtain that with probability at least $1-2\delta$,
\begin{equation}
|x' H^{-1} Z| \le \frac{\sqrt{2}\sigma}{\kappa} \sqrt{\log(1/\delta)} \norm{x}_{V^{-1}}\,.
\label{eqn:first-term-bound}
\end{equation}

For the second term, 
\begin{eqnarray}
 |x' H^{-1} E (H+E)^{-1} Z|  \nonumber &\le& \norm{x} _{H^{-1}} \norm{H^{-1/2}E (H+E)^{-1} Z} \\
 \nonumber &\le& \norm{x}_{H^{-1}} \norm{H^{-1/2}E (H+E)^{-1}H^{1/2}} \norm{Z}_{H^{-1}} \\
 \label{eq:term2} &\le& \frac{1}{\kappa} \norm{x}_{V^{-1}} \norm{H^{-1/2}E (H+E)^{-1}H^{1/2}} \norm{Z}_{V^{-1}}\,,
\end{eqnarray}
where the last inequality is due to the fact that $H \mge \kappa V$. Since $(H+E)^{-1}=H^{-1}-H^{-1}E(H+E)^{-1}$, we have
\begin{eqnarray*}
\norm{H^{-1/2}E (H+E)^{-1}H^{1/2}} &=& \norm{H^{-1/2}E \left(H^{-1} - H^{-1} E (H+E)^{-1}\right)H^{1/2}} \\
&=& \norm{H^{-1/2} E H^{-1/2} + H^{-1/2} E H^{-1} E (H+E)^{-1} H^{1/2}} \\
&\le& \norm{H^{-1/2} E H^{-1/2}} + \norm{H^{-1/2} E H^{-1/2}} \norm{H^{-1/2}E (H+E)^{-1}H^{1/2}}\,.
\end{eqnarray*}
By solving this inequality, we get
\begin{equation*} \label{eq:matrixinv}
\norm{H^{-1/2}E (H+E)^{-1}H^{1/2}} \le \frac{\norm{H^{-1/2} E H^{-1/2}}}{1-\norm{H^{-1/2} E H^{-1/2}}} \le 2 \norm{H^{-1/2} E H^{-1/2}} \le \frac{8 M_\mu \sigma}{\kappa^2} \sqrt{\frac{d + \log(1/\delta)}{\lmin(V)}}\,,
\end{equation*}
where we have used \eqnref{eqn:half} and \eqnref{eqn:normality} in the second and third inequalities, respectively.
Combining it with \eqnref{eq:term2} and the bound in $\EvtG$, we have
\begin{equation} 
|x' H^{-1} E (H+E)^{-1} Z| \,\le\, \frac{32 M_\mu \sigma^2}{\kappa^3} \frac{d + \log(1/\delta)}{\sqrt{\lmin(V)}} \norm{x}_{V^{-1}}.
\label{eqn:second-term-bound}
\end{equation}
From \eqnref{eqn:decomposition}, \eqnref{eqn:first-term-bound} and \eqnref{eqn:second-term-bound}, one can see that \eqnref{eq:normality} holds as long as the lower bound \eqnref{eq:lmin1} for $\lmin(V)$ holds.  Finally, an application of a union bound on two small-probability events (given in \lemref{lm:hoeffding} and \eqnref{eqn:first-term-bound}, respectively) asserts that \eqnref{eq:normality} holds with probability at least $1-3\delta$.

\section{Proof of \proref{pro:spectral}}

In the following, for simplicity, we will drop the subscript $n$ when there is no ambiguity.  Therefore, $V_n$ is denoted $V$ and so on.

\iffalse
We will need a technical lemma, which is an existing result in random matrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from \citet{vershynin2010introduction}.
\begin{lemma}
\label{lem:rmsv}
%[Theorem~5.39 from \citet{vershynin2010introduction}
%\footnote{The form presented here differs slightly from the version in \citet{vershynin2010introduction}.  Our form is from Equation~5.23 in their proof, which is equivalent to their Theorem~5.39.}]
Let $A\in\R^{n\times d}$ be a matrix whose rows $A_i$ are independent sub-Gaussian isotropic random vectors in $\R^d$ with parameter $\sigma$, namely, $\E \exp(x' (A_i - \E A_i)) \le \exp(\sigma^2 \norm{x}^2/2)$ for any $x \in \R^d$.  Then, there exist positive, universal constants $C_1$ and $C_2$ such that, for every $t \ge 0$, the following holds with
probability at least $1-2\exp(-C_2t^2)$:
\[
 \left\|\frac{1}{n} A'A - \mathbf{I}_d \right\| \le \max\{\delta, \delta^2\}, \quad \textrm{where } \delta = \sigma^2 \left( C_1 \sqrt{\frac{d}{n}} + \frac{t}{\sqrt{n}} \right). 
\]
\end{lemma}
\fi

Let $X$ be a random vector drawn from the distribution $\nu$.  Define $Z\defeq \Sigma^{-1/2}X$.  Then $Z$ is isotropic, namely, $\E[ZZ'] = \Id_d$.  Define $U=\sum_{t=1}^n Z_tZ_t' = \Sigma^{-1/2}V\Sigma^{-1/2}$.
From \lemref{lem:rmsv}, we have that, for any $t$, with probability at least $1-2\exp(-C_2t^2)$,
\[
\lmin(U)\ge n - C_1 \sigma^2 \sqrt{nd} - \sigma^2 t \sqrt{n} \,. 
\]
where $\sigma$ is the sub-Gaussian parameter of $Z$, which is upper-bounded by $\norm{\Sigma^{-1/2}} = \lmin^{-1/2}(\Sigma)$ (see, e.g., \citet{vershynin2010introduction}).  
%\lihong{Can Yu double-check?  Is there a better bound?  Maybe make the constants' dependence on $\norm{Z}_{\Psi_2}$ explicit?} {\color{red} Yu: Yes}
We thus can rewrite the above inequality (which holds with probability $1-\delta$ as
\[
\lmin(U) \ge n - \lmin^{-1}(\Sigma) \left(C_1 \sigma^2 \sqrt{nd} + t \sqrt{n} \right) \,.
\]

We now bound the minimum eigenvalue of $V$, as follows:
\begin{eqnarray*}
\lmin(V) &=& \min_{x\in\B^d} x'Vx \\
&=& \min_{x\in\B^d}x'\Sigma^{1/2}U\Sigma^{1/2}x \\
&\ge& \lmin(U) \min_{x\in\B^d}x'\Sigma x \\
&=& \lmin(U) \lmin(\Sigma) \\
&\ge& \lmin(\Sigma) \left( n - \lmin^{-1}(\Sigma) (C_1 \sigma^2 \sqrt{nd} + t \sqrt{n} ) \right) \\
&=& \lmin(\Sigma) n - C_1 \sqrt{nd} - C_2 \sqrt{n \log(1/\delta)}\,.
\end{eqnarray*}

Finally, it can be verified (\lemref{lem:quad-ineq}) that the last expression above is no less than $B$ as long as
\[
n \ge \left(\frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lmin(\Sigma)}\right)^2 + \frac{2B}{\lmin(\Sigma)} \,,
\]
finishing the proof.

% Comment this out if main proofs (Thms 2 & 3) are given in the main text.
%\input{appendix_mainproofs}

\section{Technical Lemmas and Proofs} \label{sec:lemmas}

\subsection{Proof of \lemref{lm:hoeffding}}

Noting that 
$$\|Z\|_{V^{-1}}=\|V^{-1/2}Z\|_2 = \sup_{\|a\|_2 \le 1}\iprod{a}{V^{-1/2}Z},$$
let $\hat{\B}$ be a $1/2$-net of the unit ball $\B^d$. Then $|\hat{\B}| \le 6^d$~\citep[Lemma 4.1]{pollard1990empirical}, and for any $x \in \B^d$, there is a $\hat{x} \in \hat{\B}$ such that $\norm{x-\hat{x}} \le 1/2$. Consequently, 
\begin{eqnarray*} 
\iprod{x}{V^{-1/2}Z} &=& \iprod{\hat{x}}{V^{-1/2}Z} + \iprod{x-\hat{x}}{V^{-1/2}Z} \\
&=& \iprod{\hat{x}}{V^{-1/2}Z} +  \norm{x-\hat{x}} \iprod{\frac{x-\hat{x}}{\norm{x-\hat{x}}}}{V^{-1/2}Z} \\
&\le& \iprod{\hat{x}}{V^{-1/2}Z} + \frac{1}{2} \sup_{z \in \B^d} \iprod{z}{V^{-1/2}Z}.
\end{eqnarray*}
Taking supremum on both sides, we get
$$ \sup_{x \in \B^d} \iprod{x}{V^{-1/2}Z} \le 2 \max_{\hat{x} \in \hat{\B}} \iprod{\hat{x}}{V^{-1/2}Z}\,.$$
Then a union bound argument implies
\begin{eqnarray*}
\mathbb{P}\left\{\|Z\|_{V^{-1}} > t\right\} &\le& \mathbb{P}\left \{ \max_{\hat{x} \in \hat{\B}} \iprod{\hat{x}}{V^{-1/2}Z} > t/2\right\} \\
&\le& \sum_{\hat{x} \in \hat{\B}} \mathbb{P}\left\{ \iprod{\hat{x}}{V^{-1/2}Z} > t/2 \right \} \\
&\le& \sum_{\hat{x} \in \hat{\B}} \exp \left\{ - \frac{t^2}{8\sigma^2 \norm{\hat{x}'V^{-1/2} X'}^2} \right\} \\
&\le& \exp \left\{ - t^2/(8\sigma^2) +  d \log 6 \right\},
\end{eqnarray*}
where we have used Hoeffding's inequality for the third inequality and $|\hat{\B}| \le 6^d$ for the last inequality. A choice of $t=4\sigma \sqrt{d+\log(1/\delta)}$ completes the proof.

\subsection{Proof of \lemref{lm:widthsum}}

By \citet[Lemma~11]{abbasi2011improved}, we have
$$ \sum_{t=m+1}^{m+n} \norm{X_t}_{V_{t}^{-1}}^2 
\le 2 \log \frac{\det V_{m+n+1}}{\det V_{m+1}} 
\le 2 d \log \left( \frac{\tr{V_{m+1}} + n}{d} \right) - 2 \log \det V_{m+1}\,. $$
Note that $\tr{V_{m+1}} = \sum_{t=1}^{m} \tr{X_tX_t'} = \sum_{t=1}^{m} \|X_t\|^2 \le m$ and that $\det V_{m+1} = \prod_{i=1}^{d} \lambda _i \ge \lmin^d(V_{m+1}) \ge 1$, where $\{\lambda_i\}$ are the eigenvalues of $V_{m+1}$. Applying Cauchy-Schwartz inequality yields
$$ \sum_{t=m+1}^{m+n} \norm{X_t}_{V_{t}^{-1}} \le \sqrt{n \sum_{t=m+1}^{m+n} \norm{X_t}_{V_{t}^{-1}}^2} \le \sqrt{2nd \log {\left(\frac{n+m}{d} \right)}}\,. $$

\subsection{Proof of \lemref{lm:deltatbound}}

Define $G_t(\theta) = \sum_{i=1}^{t-1}(\mu(X_i'\theta)-\mu(X_i'\theta^*))X_i$ and $Z_t=\sum_{i=1}^{t-1}\epsilon_i X_i$. Following the same argument as in the proof of \thmref{thm:glm_ucb}, we have $G_t(\hat{\theta}_t)=Z_t$ and
\begin{equation} \label{eq:aaa}
\|G_t(\theta)\|_{V_t^{-1}}^2 \ge \kappa^2 \|\theta-\theta^*\|_{V_t}^2 
\end{equation}
for any $\theta \in \{\theta~:~\norm{\theta-\theta^*} \le 1\}$.  Combining \eqnref{eq:aaa} with the following lemma and the equality $Z_t=G_t(\hat{\theta}_t)$ completes the proof.

\begin{lemma} \label{lm:self}
Suppose there is an integer $m$ such that $\lmin(V_m) \ge 1$, then for any $\delta \in (0,1)$, with probability at least $1-\delta$, for all $t>m$,
\[
\norm{Z_t}_{V_t^{-1}}^2 \le 4 \sigma^2 \left(\frac{d}{2} \log(1+2t/d) + \log(1/\delta) \right) \,. \]
\end{lemma}

\begin{proof}
For convenience, fix $t$ such that $t>m$, and denote $V_t$ and $Z_t$ by $V$ and $Z$, respectively.  Furthermore, define $\bar{V} \defeq V+\lambda I$ and let $\onevec$ be the vector of all $1$s.  It is easy to observe that
\begin{equation}
\norm{Z}_{V^{-1}}^2 = \norm{Z}_{\bar{V}^{-1}}^2 + Z'(V^{-1} - \bar{V}^{-1})Z\,.
\label{eqn:znorm-decomposition}
\end{equation}
%
We start with bounding the second term.  The Shermanâ€“Morrison formula gives
\[
\bar{V}^{-1} = V^{-1} - \frac{\lambda V^{-2}}{1+\lambda \onevec'V^{-1}\onevec}\,.
\]
Since $\onevec'V^{-1}\onevec\ge0$, the above implies that
\begin{eqnarray*}
0 &\le& Z'(V^{-1} - \bar{V}^{-1})Z \\
&\le& \lambda Z' V^{-2} Z \\
&\le& \lambda \norm{V^{-1}} \norm{Z}_{V^{-1}}^2 \\
&=& \frac{\lambda}{\lmin(V)}\norm{Z}_{V^{-1}}^2\,.
\end{eqnarray*}
Since $\lmin(V) \ge \lmin(V_m) \ge 1$, we now have
\[
0 \le Z'(V^{-1} - \bar{V}^{-1})Z \le \lambda \norm{Z}_{V^{-1}}^2\,.
\]
The above inequality together with \eqnref{eqn:znorm-decomposition} implies that
\[
\norm{Z}_{V^{-1}}^2 \le (1-\lambda)^{-1} \norm{Z}_{\bar{V}^{-1}}^2\,.
\]
The proof can be finished by applying Theorem~1 and Lemma~10 from \citet{abbasi2011improved} to bound $\norm{Z}_{\bar{V}^{-1}}^2$, using $\lambda=1/2$.
\end{proof} 

\subsection{Proof of \lemref{lm:propSup}}

We will prove the first part of the lemma by induction. It is easy to check the lemma holds for $s=1$. Suppose we have $a_t^* \in A_s$ and we want to prove $a_t^* \in A_{s+1}$. Since the algorithm proceeds to stage $s+1$, we know from step 2b that
$$|m_{t,a}^{(s)} - x_{t,a}'\theta^*| \le w_{t,a}^{(s)} \le 2^{-s}$$
for all $a \in A_{s}$. Specially, it holds for $a=a_t^*$ because $a_t^* \in A_{s}$ by our induction step. Then the optimality of $a_t^*$ implies
$$ m_{t,a_t^*}^{(s)} \ge  x_{t,a_t^*}'\theta^* - 2^{-s} \ge x_{t,a}'\theta^* - 2^{-s} \ge m_{t,a}^{(s)} - 2\cdot2^{-s}$$
for all $a \in A_s$. Thus we have $a_t^* \in A_{s+1}$ according to step 2d.

Suppose $a_t$ is selected at stage $s_t$ in step 2b. If $s_t=1$, obviously the lemma holds because $0 \le \mu(x) \le 1$ for all $x$. If $s_t>1$, since we have proved $a_t^* \in A_{s_t}$, again step 2b at stage $s_t-1$ implies 
$$|m_{t,a}^{(s_t-1)} - x_{t,a}'\theta^*| \le 2^{-s_t+1}$$
for $a=a_t$ and $a=a_t^*$. Step 2d at stage $s_t-1$ implies
$$ m_{t,a_t^*}^{(s_t-1)} -  m_{t,a_t}^{(s_t-1)} \le 2\cdot 2^{-s_t+1}\,.$$
Combining above two inequalities, we get 
$$ x_{t,a_t}'\theta^* \ge m_{t,a_t}^{(s_t-1)} - 2^{-s_t+1} 
\ge m_{t,a_t^*}^{(s_t-1)} - 3\cdot 2^{-s_t+1} \ge x_{t,a_t^*}'\theta^*- 4\cdot 2^{-s_t+1}\,.$$
When $a_t$ is selected in step 2c, since $m_{t,a_t}^{(s_t)} \ge m_{t,a_t^*}^{(s_t)}$, we have
$$ x_{t,a_t}'\theta^* \ge m_{t,a_t}^{(s_t)} - 1/\sqrt{T} 
\ge m_{t,a_t^*}^{(s_t)} - 1/\sqrt{T} \ge x_{t,a_t^*}'\theta^*- 2/\sqrt{T}\,.$$
Using the fact that $\mu(x_1)-\mu(x_2) \le L_\mu (x_1-x_2)$ for $x_1 \ge  x_2$, we will get the desired result.

\subsection{Proof of \lemref{lem:quad-ineq}}

\begin{lemma}
\label{lem:quad-ineq}
Let $a$ and $b$ be two positive constants.  If $m \ge a^2 + 2b$, then $m - a\sqrt{m} - b \ge 0$.
\end{lemma}

\begin{proof}
The function $t \mapsto t^2-at-b$ is monotonically increasing for $t \ge a/2$.  Since $m \ge a^2 + 2b$, we have $\sqrt{m} \ge a/2$, so
\begin{eqnarray*}
m - a\sqrt{m} - b &\ge& a^2 + 2b - a \sqrt{a^2 + 2b} - b \\
&\ge& a^2 + b - a \sqrt{a^2 + 2b + b^2/a^2} \\
&=& a^2 + b - a \sqrt{(a + b/a)^2} \\
&=& a^2 + b - a (a + b/a) \\
&=& 0\,.
\end{eqnarray*}
\end{proof}
