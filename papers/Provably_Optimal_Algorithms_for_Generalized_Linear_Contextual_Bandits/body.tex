\section{Introduction}
Contextual bandit problems are originally motivated by applications in clinical trials~\cite{woodroofe1979one}. When a standard treatment and a new treatment are available for a certain disease, the doctor needs to decide, in a sequetial manner, which of them to use based on the patient's profiles such as age, general physical status or medicine history. With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search~\cite{agarwal2009online, li2010contextual, liunbiased}. In the problem of personalized news recommendation, the website must recommend news articles that are most interesting to users that visit the website.  The problem is especially challenging for breaking news, as little data are available to make good prediction about user interest.
%one needs to decide which news article to recommend to a given user whether to recommend breaking news to a given user, especially when the news first emerges and little data are available.
A trade-off naturally occurs in this kind of sequential decision making problems.  One needs to balance \emph{exploitation}---choosing actions that performed well in the past---and \emph{exploration}---choosing actions that may potentially give better outcomes.

In this paper, we study the following stochastic, $K$-armed contextual bandit problem. Suppose at each of the $T$ rounds, an agent is presented with a set of $K$ actions, each of which is associated with a context (a $d$-dimensional feature vector). By choosing an action based on the rewards obtained from previous rounds and on the contexts, the agent will receive a stochastic reward generated from some unknown distribution conditioned on the context and the chosen action. The goal of the agent is to maximize the expected cumulative rewards over $T$ rounds. The most studied model in contextual bandits literature is the linear model \cite{auer2003using,dani2008stochastic,rusmevichientong2010linearly,chu2011contextual,abbasi2011improved}, in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work with. However, in practice, we usually have binary rewards (click or not, treatment working or not).  Logistic regression model based algorithms have been shown to have substantial improvements over linear models~\cite{liunbiased}.  We therefore consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases. 
%The mathematical formulation of the setting will be given in section \ref{sec:probsetting}.

The celebrated work of \citet{lai1985asymptotically} first introduces the upper confidence bound (UCB) approach to efficient exploration. Later, the idea of confidence bound has been successfully applied to many stochastic bandits problems, from $K$-arm bandits problems \cite{auer2002finite, bubeck2012regret} to linear bandits~\cite{auer2003using,abbasi2011improved}. UCB-type algorithms are both efficient and provable optimal in $K$-arm bandits and $K$-armed linear bandits. However, most study are limited to the linear case. While some UCB-type algorithms using GLMs perform well empirically~\cite{liunbiased}, there is little theoretical study of them. A natural question arises: can we find an efficient algorithm to achieve the optimal convergence rate for generalized linear bandits?

\paragraph{Our Contributions} In this paper, we propose a GLM version of the UCB algorithm called SupCB-GLM that achieves a regret over $T$ rounds of order $\tilde{O}(\sqrt{dT})$. This rate improves the state-of-the-art results of \citet{filippi2010parametric} by a $\sqrt{d}$ factor, assuming the number of actions is fixed.  Moreover, it matches the GLM bandits problem's minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM  is inspired by the seminal work of \citet{auer2003using}, which introduced a technique to construct independence samples in linear contextual bandits.  A key observation in proving this result is that the $\ell_2$ confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in \emph{all} directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To the best of our knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. 

\paragraph{Related Work} The study of GLM bandits problem goes back at least to \citet{sarkar1991one},
%\todo{Not Lai-Robbins?}
who considered \emph{discounted} regrets rather than cumulative regerts. They prove that a myopic rule without exploration is \emph{asymptotically} optimal. Recently,  \citet{filippi2010parametric} study the same stochastic GLM bandit problem considered here. They propose the GLM-UCB algorithm, similar to our \algref{alg: UCB}, which achieves a regret of ${\small \tilde{O}(d\sqrt{T})}$ after $T$ rounds. However, as we believe the optimal regret for stochastic GLM bandits should be the same as linear case when the number of actions is small, their rates misses a $\sqrt{d}$ term than the optimal rates.

Another line of research focuses on using EXP-type algorithms, which can be applied to almost any model classes~\cite{auer2002nonstochastic}. These algorithms, which choose actions using a carefully randomized policy, use importance sampling to reduce a bandit problem to its full-information analogue.
%address the  exploitation and exploration dilemma by doing importance sampling on the actions.
Later variants of the EXP4 algorithm~\cite{beygelzimer2010contextual,agarwal2014taming} give an $\tilde{O}(\sqrt{dKT})$ regret that is near-optimal with respect to $T$. However, these regret bounds have a $\sqrt{K}$ dependence. Moreover, these algorithms can be expensive to run: they either have a computational complexity exponential in $d$ for our GLM case, or need to make a large number of calls to a nontrivial optimization oracle.
%even with recent algorithmic advances~\cite{agarwal2014taming}.
%Exp-type algorithm are far less efficient than the UCB algorithm because they would have a running time exponential in $d$.
%\todo{Review the first section again}

\paragraph{Organization} 
%The rest of this paper is organized as follows. 
\secref{sec:probsetting}  introduces the generalized linear bandit problem.  \secref{sec:glm} gives a brief review of the statistical properties of generalized linear model, and gives a sharp non-asymptotic normality-type result for GLM parameter estimation which can be of independent value.  With this tool, \secref{sec:alg} presents our algorithms and the main theoretical results.  \secref{sec:discussions} concludes the paper with further discussions, including several open problems.
All proofs are given in the supplementary materials.
%In Section 5, we discuss some related issues in the paper. Section 6 is devoted to theoretical analysis and the proofs of some technical lemmas defer to Section 7.
%\todo{Finish this}


\paragraph{Notations} 
%We close this section by introducing some notations. 
For a vector $x \in \R^{d}$, we use $\norm{x}$ to denote its $\ell_2$- norm and $x'$ its transpose. 
%Denote the $d$-dimensional unit ball centered at the origin by 
$\B^d\defeq\{x\in\R^d ~:~ \norm{x}\le1\}$ is the unit ball centered at the origin.  The weighted $\ell_2$-norm associated with a positive-definite matrix $A$ is defined by $\norm{x}_A \defeq \sqrt{x' Ax}$. The minimum and maximum singular values of a matrix $A$ are written as $\lmin(A)$ and $\norm{A}$, respectively.  The trace of a matrix $A$ is $\tr{A}$.  For two symmetric matrices $A$ and $B$ of the same dimensions, $A \mge B$ means that $A-B$ is positive semi-definite.   For a real-valued function $f$, we use $\dot{f}$ and $\ddot{f}$ to denote its first and second derivatives.  Finally, $[n] \defeq \{1,2,\ldots,n\}$.
% is abbreviated as $[n]$.

\section{Problem Setting} \label{sec:probsetting} 

We consider the stochastic $K$-armed contextual bandit problem.  Let $T$ be the number of total rounds. At round $t$, the agent observes a context consisting of a set of $K$ feature vectors, $\{x_{t,a} \mid a\in[K]\} \subset \R^d$, which is drawn IID from an unknown distribution $\nu$, with $\norm{x_{t,a}} \le 1$.  %{\color{red} \{Yu: it makes more sense to me that we assume $\norm{x_{t,a}} \le d. \} $} 
Each feature vector $x_{t,a}$ is associated with an unknown stochastic reward  $y_{t,a} \in [0,1]$.  The agent selects one action, denoted $a_t$, and observes the corresponding reward $y_{t,a_t}$.  Finally, we make a regularity assumption about the distribution $\nu$: there exists a constant $\sigma_0 > 0$ such that $\lmin(\E[\frac{1}{K}\sum_{a\in[K]}x_{t,a}x_{t,a}']) \ge \sigma_0^2$ for all $t$.

In this paper, we are concerned with the generalized linear model, or GLM, in which there is an unknown $\theta^* \in \R^d$ and a fixed, strictly increasing \emph{link function} $\mu: \R \to \R$ such that $\E[Y \mid X] = \mu(X'\theta^*)$, where $X$ is the chosen action's feature and $Y$ the corresponding reward.
One can verify that linear and logistic models are special cases of GLM with $\mu(x)=x$ and $\mu(x)=1/(1+e^{-x})$, respectively. 

The agent's goal is to maximize the cumulative expected rewards over $T$ rounds. Suppose the agent takes action $a_t$ at round $t$. Then the agent's strategy can be evaluated by comparing its expected reward to the best expected reward. To do so, define the optimal action at round $t$ by $a_t^*=\argmax_{a \in [K]} \mu(x_{t,a}'\theta^*)$. Then, the agent's total regret of following strategy $\pi$ can be expressed as follows
\vspace{-1mm}
\begin{equation*}
R_T(\pi) \defeq \sum_{t=1}^{T} \left( \mu(x_{t,a_t^*}'\theta^*) - \mu(x_{t,a_t}'\theta^*) \right).
\end{equation*}
%Although we consider the expected cumulative regret here, 
Note that $R_T(\pi)$ is in general a random variable due to the possible randomness in $\pi$. Denote by $X_{t}=x_{t,a_t}$, $Y_{t}=y_{t,a_t}$, and our model can be written as 
\begin{equation} \label{eq:model1}
Y_{t} = \mu(X_t'\theta^*) + \epsilon_t\,,
\end{equation}
where $\{\epsilon_t, t \in [T]\}$ are independent zero-mean noise. Here, $X_t$ is a random variable because the agent chooses current action based on previous rewards. Formally, we assume there is an increasing sequence of sigma fields $\{\mathcal{F}_{n}\}$ such that 
%$X_t$ is $\mathcal{F}_{t-1}$-measurable and 
$\epsilon_t$ is $\mathcal{F}_{t}$-measurable with $\E \left[\,\epsilon_t \mid \mathcal{F}_{t-1}\,\right]=0$.  An example of $\mathcal{F}_{n}$ will be the sigma-field generated by $\{X_1,Y_1,\ldots,X_n, Y_n\}$. Also, we assume the noise $\epsilon_t$ is sub-Gaussian with parameter $\sigma$, where $\sigma$ is some positive, universal constant; that is, for all $t$,
\begin{equation} \label{eq:glm2}
\E \left [\, e^{\lambda \epsilon_t} \mid \mathcal{F}_{t-1}\,\right] \le e^{\lambda^2 \sigma^2/2}.
\end{equation}
In practice, when we have bounded reward $Y_t \in [0,1]$,
% and bounded $\mu$, 
the noise $\epsilon_t$ is also bounded and hence satisfies \eqnref{eq:glm2} with some appropriate $\sigma$ value. In addition to the boundedness assumption on the rewards and feature vectors, we also need the following assumption on the link function $\mu$. 

\begin{assumption} \label{ass:kappa}
$\kappa := \inf_{\{\norm{x}\le 1,\,\,\norm{\theta-\theta^*} \le 1\}} \dot{\mu}(x'\theta)  > 0 $.
\end{assumption}

As we shall see in \secref{sec:glm}, the asymptotic normality of maximum-likelihood estimates implies the necessity of this assumption. Note that this assumption is weaker than Assumption 1 in \citet{filippi2010parametric}, as it only requires to control the \emph{local} behavior of $\dot{\mu}(x'\theta)$ near $\theta^*$. 

\begin{assumption} \label{ass:smooth}
$\mu$ is twice differentiable. Its first and second order derivatives are upper-bounded by $L_{\mu}$ and $M_\mu$, respectively.
\end{assumption}

It can be verified that \assref{ass:smooth} holds for the logistic link function, where we may choose $L_{\mu}=M_\mu=1/4$.

\section{Generalized Linear Models} \label{sec:glm}

To motivate the algorithms proposed in this paper, we first briefly review the classical likelihood theory of generalized linear models. In the canonical generalized linear model \cite{mccullagh1989generalized}, the conditional distribution of $Y$ given $X$ is from the exponential family, and its density, parameterized by $\theta\in\Theta$, can be written as
\begin{equation}
\Prob(Y \mid X) = \exp\left \{\frac{Y X'\theta^* - m(X'\theta^*)}{g(\eta)} + h(Y,\eta) \right\}. \label{eq:glm1}
\end{equation}
Here, $\eta \in \R^+$ is a known scale parameter; $m$, $g$ and $h$ are three normalization functions mapping from $\R$ to $\R$. The exponential family \eqnref{eq:glm1} is a very broad family of distributions including the Gaussian, binomial, Poisson, gamma and inverse-Gaussian distributions. It follows from standard properties of exponential families \cite{brown1986fundamentals} that $m$ is infinitely differentiable satisfying $\dot{m}(X'\theta^*)=\E [\,Y \mid X\,] = \mu(X'\theta^*)$ and $\ddot{m}(X'\theta^*)=\V(Y \mid X)$. It can be checked that the data generated from \eqnref{eq:glm1} automatically satisfies the sub-Gaussian condition \eqnref{eq:glm2}.

Suppose we have independent samples of $Y_1, Y_2, \ldots, Y_n$ condition on $X_1,X_2,\ldots,X_n$. The log-likelihood function of $\theta$ under model \eqnref{eq:glm1} is
\begin{eqnarray*}
\log \ell(\theta) &=& \sum_{t=1}^n \left[\frac{Y_t X_t'\theta - m(X_t'\theta)}{v(\eta)} + c(Y_t,\eta)\right] \\
&=&
\frac{1}{v(\eta)} \sum_{t=1}^{n} \left[ Y_t X_t'\theta - m(X_t'\theta) \right] + \mathrm{constant}\,.
\end{eqnarray*}
Consequently, the maximum likelihood estimate (MLE) may be defined by
\[
\hat{\theta}_n \in \argmax_{\theta\in\Theta} \sum_{t=1}^{n} \left[ Y_t X_t'\theta - m(X_t'\theta) \right]\,.
\]
From classical likelihood theory~\cite{lehmann1998theory}, we know that when the sample size $n$ goes to infinity, the MLE $\hat{\theta}_n$ is asymptotically normal, that is,
$\hat{\theta}_n - \theta^* \to \mathcal{N}(0, \mathcal{I}^{-1}_{\theta^*})$, where $\mathcal{I}_\theta= \sum_{t=1}^{n} \dot{\mu}(X_t'\theta)X_tX_t'$ is the Fisher Information Matrix. Note that if $\dot{\mu}(X_t'\theta^*) \to 0$, the asymptotic variance of $x'\hat{\theta}$ can go to infinity for some $x$. This suggests the necessity of \assref{ass:kappa}.
%to assume that $\dot{\mu}(X_t'\theta^*)$ is lower bounded by some positive constant (\assref{ass:kappa}). The asymptotic normality implies a certain asymptotic normality result along any direction $x \in \R^d$: $x'(\hat{\theta} - \theta^*) \to \mathcal{N}(0, \norm{x}_{\mathcal{I}^{-1}_{\theta^*}}^2)$.

As we will see later, the normality result is crucial in our regret analysis of GLM bandits. However, to the best of our knowledge, there is no non-asymptotic normality results of the MLE for GLM. In the following, we present a finite-sample version of the classical asymptotic normality results, which can be of independent interest.
 
\begin{theorem} \label{thm:glm_ucb}
Define $V_n = \sum_{t=1}^{n} X_t X_t'$, and let $\delta>0$ be given.   Furthermore, assume
%$\kappa=\inf_{\norm{x} \le 1, \|\theta-\theta^*\| \le 1} \dot{\mu}(x'\theta) >0$ and
that
\begin{equation}
\lmin(V_n) \ge  \frac{512 M_\mu^2 \sigma^2}{\kappa^4} \left(d^2 + \log\frac{1}{\delta}\right)\,.
% = \Omega  \left( \frac{\sigma^2}{\kappa^4} \max \left\{d^2, \log(1/\delta) \right \}\right)\,.
\label{eq:lmin1}
\end{equation} 
Then, with probability at least $1-3\delta$,  the maximum-likelihood estimator satisfies, for any $x \in \R^d$, that
\begin{equation} \label{eq:normality}
|x'(\hat{\theta}_n-\theta^*)| \le \frac{3\sigma}{\kappa} \sqrt{\log(1/\delta)} \norm{x}_{V_n^{-1}}\,.
\end{equation}
\end{theorem}
This theorem characterizes the behavior of MLE on \emph{every} direction. It implies that $x'(\hat{\theta}_n-\theta^*)$ has a sub-Gaussian tail bound for any $x \in \R^d$. It also provides a rigorous justification of the asymptotic upper confidence bound derived heuristically by \citet[Section~4.2]{filippi2010parametric}.

The proof of the theorem is given in the appendix.  It consists of two main steps, as is typical for proving normality-type results of MLEs~\citep{van2000asymptotic}.  We first show the $n^{-1/2}$-consistency of $\hat{\theta}$ to $\theta^*$. Then, by using a second-order Taylor expansion or Newton-step, we can prove the desired normality of $\hat{\theta}$.

The condition \eqnref{eq:lmin1} on $\lmin(V_n)$ is necessary for the consistency of estimating linear models~\cite{lai1982least,bickel2009simultaneous} and generalized linear models~\cite{fahrmeir1985consistency,chen1999strong}. It can be satisfied under mild conditions
%For example, when we have independent bounded random designs $X_i, i \in [n]$, $\lmin(V_n)$ can be lower bounded by $c(n-d)$ with probability at least $1-\exp(-c'n)$ for some universal constant $c$ and $c'$~\cite{vershynin2010introduction} (c.f., \lemref{lem:rmsv}). \lihong{To check: constants are not universal unless RVs are isotropic?}
such as the proposition below, which will be useful for our analysis.
%The proof of the proposition, given in the appendix, makes use of concentration results of random matrices~\citep{vershynin2010introduction} to show that the singular values of $V_n$ and $\E[V_n]$ are close for large $n$.

\begin{proposition}
\label{pro:spectral}
Define $V_n=\sum_{t=1}^n X_t X_t'$, where $X_t$ is drawn iid from some distribution $\nu$ with support in the unit ball, $\B^d$.  Furthermore, let $\Sigma\defeq\E[X_tX_t']$ be the second moment matrix, and $B$ and $\delta>0$ be two positive constants.  Then, there exist positive, universal constants $C_1$ and $C_2$ such that $\lmin(V_n) \ge B$ with probability at least $1-\delta$, as long as
\begin{eqnarray*}
n &\ge& \left(\frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lmin(\Sigma)}\right)^2 + \frac{2B}{\lmin(\Sigma)}\,.
%&=& O\left( \frac{d+\log(1/\delta) + B\lmin(\Sigma)}{\lmin^2(\Sigma)} \right) \,.
\end{eqnarray*}
\end{proposition}

\begin{proof}[Proof Sketch]
We give a proof sketch here, and the full proof is found in the appendix.
In the following, for simplicity, we will drop the subscript $n$ when there is no ambiguity.  Therefore, $V_n$ is denoted $V$ and so on.
%
We will need a technical lemma, which is an existing result in random matrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from \citet{vershynin2010introduction}.
\begin{lemma}
\label{lem:rmsv}
%[Theorem~5.39 from \citet{vershynin2010introduction}
%\footnote{The form presented here differs slightly from the version in \citet{vershynin2010introduction}.  Our form is from Equation~5.23 in their proof, which is equivalent to their Theorem~5.39.}]
Let $A\in\R^{n\times d}$ be a matrix whose rows $A_i$ are independent sub-Gaussian isotropic random vectors in $\R^d$ with parameter $\sigma$, namely, $\E \exp(x' (A_i - \E A_i)) \le \exp(\sigma^2 \norm{x}^2/2)$ for any $x \in \R^d$.  Then, there exist positive, universal constants $C_1$ and $C_2$ such that, for every $t \ge 0$, the following holds with
probability at least $1-2\exp(-C_2t^2)$, where $\varepsilon = \sigma^2 (C_1 \sqrt{d/n} + t/\sqrt{n})$: $\left\|\frac{1}{n} A'A - \mathbf{I}_d \right\| \le \max\{\varepsilon, \varepsilon^2\}\,.$
\end{lemma}

Let $X$ be a random vector drawn from the distribution $\nu$.  Define $Z\defeq \Sigma^{-1/2}X$.  Then $Z$ is isotropic, namely, $\E[ZZ'] = \Id_d$.  Define $U=\sum_{t=1}^n Z_tZ_t' = \Sigma^{-1/2}V\Sigma^{-1/2}$.
From \lemref{lem:rmsv}, we have that, for any $t$, with probability at least $1-2\exp(-C_2t^2)$, $\lmin(U)\ge n - C_1 \sigma^2 \sqrt{nd} - \sigma^2 t \sqrt{n}$,
where $\sigma$ is the sub-Gaussian parameter of $Z$, which is upper-bounded by $\norm{\Sigma^{-1/2}} = \lmin^{-1/2}(\Sigma)$ (see, e.g., \citet{vershynin2010introduction}).  
%\lihong{Can Yu double-check?  Is there a better bound?  Maybe make the constants' dependence on $\norm{Z}_{\Psi_2}$ explicit?} {\color{red} Yu: Yes}
We thus can rewrite the above inequality (which holds with probability $1-\delta$ as
\[
\lmin(U) \ge n - \lmin^{-1}(\Sigma) \left(C_1 \sigma^2 \sqrt{nd} + t \sqrt{n} \right) \,.
\]
This implies the following lower bound:
\[
\lmin(V) \ge \lmin(\Sigma) n - C_1 \sqrt{nd} - C_2 \sqrt{n \log(1/\delta)}\,.
\]
Finally, simple calculations show that the last expression is no less than $B$ as long as $n$ is no smaller than the expression stated in the proposition, finishing the proof.
\end{proof}

\section{Algorithms and Main Results} \label{sec:alg}

In this section, we are going to present two algorithms. While the first algorithm is computationally more efficient, the second algorithm has a provable optimal regret bound.

\subsection{Algorithm UCB-GLM} \label{sec:UCB-GLM}

The idea of upper confidence bounds (UCB) is highly effective in dealing with the exploration and exploitation trade-off in many parametric bandit problems, including $K$-arm bandits~\cite{auer2002finite} and linear bandits~\cite{abbasi2011improved,auer2003using,chu2011contextual,dani2008stochastic}. 
For the generalized linear model considered here, since $\mu$ is a strictly increasing function, our goal is equivalent to choosing $a \in [K]$ to maximize $x_{t,a}'\theta^*$ at round $t$. Suppose $\hat{\theta}_t$ is our current estimator of $\theta^*$ after round $t$. An exploitation action is to take the action that maximizes the estimated mean value, while an exploration action is to choose the one that has the largest variance. Thus, to balance exploitation and exploration, we can simply choose the action that maximizes the sum of estimated mean and variance, which can be interpreted as an upper confidence bound of $x_{t,a}'\hat{\theta}_t$.  This leads to the algorithm UCB-GLM (\algref{alg: UCB}).
%\todo{Relation to \cite{abbasi2011improved}? Rename it? Phase 1 exploration.}

\begin{algorithm}
\caption{UCB-GLM}\label{alg: UCB}
\vspace{0.05in}
\textbf{Input}: the total rounds $T$, tuning parameter $\tau$ and $\alpha$. \vspace{0.00in}

\textbf{Initialization}: randomly choose $a_t \in [K]$ for $t \in [\tau]$, set $V_{\tau+1}=\sum_{i=1}^{\tau} X_{t}X_{t}'$ \\ \vspace{0.05in}
\textbf{For} {$t=\tau+1,\tau+2,\ldots,T$} \textbf{do} \vspace{-0.15in}
 \begin{itemize}
 \item[1.] Calculate the maximum-likelihood estimator $ \hat{\theta}_t$ by solving the equation \vspace{-3mm}
 \begin{equation} \label{eq:algquasi}
 \sum_{i=1}^{t-1} (Y_i - \mu(X_i'\theta)) X_i =0
 \end{equation}\vspace{-5mm}
 \item[2.] Choose $a_t = \argmax_{a \in [K]} \left ( X_{t,a}'\hat{\theta}_t + \alpha \norm{X_{t,a}}_{V_{t}^{-1}} \right)$ \vspace{-5mm}
 \item[3.] Observe $Y_t$, let $X_t \leftarrow X_{t,a_t}$, $V_{t+1} \leftarrow V_t+X_{t}X_{t}'$
 \end{itemize} \vspace{-2mm}
\textbf{End For} \vspace{0.05in}
\end{algorithm}

UCB-GLM take two parameters. At the initialization stage, we randomly choose actions to ensure a unique solution of \eqnref{eq:algquasi}.  The choice of $\tau$ in the theorem statement follows from \proref{pro:spectral} with $B=1$.  It should be noted that the IID assumption about contextual (i.e., the distribution $\nu$) is only needed to ensure $V_{\tau+1}$ is invertable (similar to the first phase in the algorithm of \citet{filippi2010parametric}); the rest of our analysis does not depend on this stochastic assumption.  The same may be achieved by using regularization (see, e.g., \citet{abbasi2011improved}).
%
%Usually we can choose $\tau$ of the same order as $dK$ (see \proref{pro:spectral}). \lihong{Explain more.  Define $\Sigma$ needed below.} 
Another tuning parameter $\alpha$ is used to control the amount of exploration. The larger the $\alpha$ is, the more exploration will be used.

As mentioned earlier, the feature vectors $X_t$ depend on the previous rewards. Consequently, the rewards $\{Y_i, i \in [t]\}$ may not be independent given $\{X_i, i \in [t]\}$.  We instead use results on self-normalized martingales~\cite{abbasi2011improved}, together with a finite-time normality result like \thmref{thm:glm_ucb}, to prove the next theorem.

\begin{theorem} \label{thm:main1}
Fix any $\delta>0$.  There exists a universal constant $C>0$, such that if we run UCB-GLM with $\alpha=\frac{\sigma}{\kappa} \sqrt{\frac{d}{2} \log(1+2T/d) + \log(1/\delta)}$ and $\tau=C\sigma_0^{-2}(d+\log(1/\delta))$, then, with probability at least $1-2\delta$, the regret of the algorithm is upper bounded by
\begin{equation*}
R_T \le \tau + \frac{2 L_{\mu} \sigma d}{\kappa} \log \left( \frac{T}{d\delta}\right) \sqrt{T} \,.
\end{equation*}
\end{theorem}

%The proofs of \thmref{thm:main1} can be found in the supplementary material. 
The theorem shows an $\tilde{O}(d\sqrt{T})$ regret bound that is independent of $K$. Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems \cite{dani2008stochastic}.  By choosing $\delta=1/T$ and using the fact that $R_T \le T$, this high-probability result implies a bound on the \emph{expected} regret: $\E [R_T] = \tilde{O}(d\sqrt{T})$.
%
Our result improves the previous regret bound of \citet{filippi2010parametric} by a $\sqrt{\log T}$ factor. Moreover, the algorithm proposed in \citet{filippi2010parametric} involves a projection step, which is computationally more expensive comparing to UCB-GLM.  Finally, this algorithm works well in practice. We give a heuristic argument for its strong performance in \secref{sec:discussions}, under a specific condition that sometimes are satisfied.

\iffalse
Here we randomly choose $2d$ actions at the beginning to ensure $V_t$ is invertible when $t \ge 2d$. It is a technical assumption. Such assumption has also been made in \citet{filippi2010parametric}. In practice, we may choose a much smaller $\tau$. The choice of $\alpha$ is crucial in determining the regret $R_T$. A smaller $\alpha$ will sometimes leads to much smaller regret.
\lihong{May not need any more if IID assumption is made.}
\fi

\begin{proof}[Proof of \thmref{thm:main1}]
We first bound the one-step regret.  To do so, fix $t$ and let $X_t^* = x_{t, a_t^*}$ and $\Delta_t=\hat{\theta}_t-\theta^*$, where $a_t^* \in \arg\max_{a\in[K]}\mu(x_{t,a}'\theta^*)$ is an optimal action at round $t$.
The selection of $a_t$ in UCB-GLM implies
\begin{equation} \label{eq:pitstar}
\iprod{X_t^*}{\hat{\theta}_t} + \alpha \norm{X_t^*}_{V_t^{-1}} \le \iprod{X_t}{\hat{\theta}_t} + \alpha \norm{X_t}_{V_t^{-1}}\,.
\end{equation}
Then, we have
\begin{eqnarray*} 
\lefteqn{\iprod{X_t^*}{\theta^*} - \iprod{X_t}{\theta^*} = \iprod{X_t^* - X_t}{\hat{\theta}_t} - \iprod{X_t^* - X_t}{ \hat{\theta}_t - \theta^*}} \\
\label{eq:basic1}&\le&  \alpha \norm{X_t}_{V_t^{-1}} - \alpha \norm{X_t^*}_{V_t^{-1}}- \iprod{X_t^* - X_t}{ \Delta_t} \\
&\le&  \alpha \big ( \norm{X_t}_{V_t^{-1}} - \norm{X_t^*}_{V_t^{-1}} \big) + \norm{X_t^* - X_t}_{V_t^{-1}} \norm{\Delta_t}_{V_t}\,,
%\\&\ge& \iprod{X_t^*}{\theta^*} - \frac{\alpha}{\sqrt{\lmin(V_t)}} - 2 \|\Delta_t\|_2.
\end{eqnarray*}
where the last inequality is due to Cauchy-Schwartz inequality. We have the following two lemmas to bound $\norm{\Delta_t}_{V_t}$ and $\norm{X_t}_{V_t^{-1}}$, respectively.  Their proofs are deferred to the appendix.

\begin{lemma} \label{lm:widthsum}
Let $\{X_t\}_{t=1}^{\infty}$ be a sequence in $\R^d$ satisfying $\norm{X_t} \le 1$.  Define $X_0=\zerovec$ and $V_t=\sum_{s=0}^{t-1} X_s X_s'$. Suppose there is an integer $m$ such that $\lmin(V_{m+1}) \ge 1$, then for all $n>0$,
$$ \sum_{t=m+1}^{m+n} \norm{X_t}_{V_{t}^{-1}} \le \sqrt{2nd \log {\left(\frac{n+m}{d} \right)}}\,. $$
\end{lemma}

\begin{lemma} \label{lm:deltatbound}
Suppose $\lmin(V_{\tau+1}) \ge 1$.  
%\lihong{Why $\tau=2d$?}  
For any $\delta \in [1/T,1)$, define event
\begin{equation*}
\EvtD \defeq \left\{ \norm{\Delta_t}_{V_t} \le \frac{\sigma}{\kappa} \sqrt{\frac{d}{2} \log(1+2t/d) + \log(1/\delta)} \right\}\,.
\end{equation*}
Then, event $\EvtD$ holds for all $t \ge \tau$ with probability at least $1-\delta$.
\end{lemma}

%\lihong{To be fixed: is $\lmin(V_\tau)>0$ sufficient?  Should we assume something like \eqnref{eq:lmin1}?} 
%{\color{red} Yu: Thanks to Lemma 8, or more precisely, Theorem 1 in [1], we only need $\lmin(V_\tau) \ge 1$.}  \lihong{Double check it.}

We now choose $\alpha=\frac{\sigma}{\kappa} \sqrt{\frac{d}{2} \log(1+2T/d) + \log(1/\delta)}$.  If event $\EvtD$ holds for all $t \ge \tau$, then,
\begin{eqnarray*}
\lefteqn{\iprod{X_t^*}{\theta^*} - \iprod{X_t}{\theta^*}} \\
&\le& \alpha \left( \norm{X_t}_{V_t^{-1}} -  \norm{X_t^*}_{V_t^{-1}} + \norm{X_t^*-X_t}_{V_t^{-1}} \right) \\
&\le& 2\alpha \norm{X_t}_{V_t^{-1}}\,.
\end{eqnarray*}
Combining the above with \lemref{lm:widthsum} yields
\begin{align}
\sum_{t=\tau+1}^{T} \big(\iprod{X_t^*}{\theta^*} - \iprod{X_t}{\theta^*}\big) &\le  2\alpha \sqrt{2Td\log \left( \frac{T}{d}\right)} \nonumber \\
&\le
\frac{2d\sigma}{\kappa} \log\left(\frac{T}{d\delta}\right)\sqrt{T}\,.
\label{eqn:ucbglm-phase2-bound}
\end{align}
%
%\le \frac{4\sigma}{\kappa} d  \sqrt{T \log \left( \frac{T}{d\delta}\right) \log\left(\frac{T}{d}\right)}\,.

Note that $\mu$ is an increasing Lipschitz function with Lipschitz constant $L_\mu$ and the $\mu$ function is bounded between $0$ and $1$. The regret of algorithm UCB-GLM can be upper bounded as
\begin{eqnarray*}
R_T
&=& \sum_{t=1}^{\tau} \Big( \mu \left(\iprod{X_t^*}{\theta^*}\right) -   \mu \left(\iprod{X_t}{\theta^*}\right) \Big) \\
&& + \sum_{t=\tau+1}^{T} \Big(\mu \left(\iprod{X_t^*}{\theta^*}\right) - \mu \left(\iprod{X_t}{\theta^*} \right)\Big) \\
&\le & \tau  + L_{\mu} \sum_{t=\tau+1}^{T} \Big( \iprod{X_t^*}{\theta^*} - \iprod{X_t}{\theta^*} \Big)\,.
% \\
%&\le & \frac{5 L_{\mu} \sigma}{\kappa} d  \sqrt{T \log \left( \frac{T}{d\delta}\right) \log\left(\frac{T}{d}\right)}.   \qquad\qquad\text{\lihong{TO check}}
\end{eqnarray*}
The proof can be finished by applying \eqnref{eqn:ucbglm-phase2-bound} and the specified value of $\tau$ to the bound above.
\end{proof}

\subsection{Algorithm SupCB-GLM}

While the algorithm UCB-GLM performs sufficiently well in practice~\cite{liunbiased},  it is unclear whether it can achieve the optimal rates of ${O}(\sqrt{dT\log K})$, when $K$ is fixed and small. As mentioned in \secref{sec:UCB-GLM}, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by \citet{auer2003using} to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (\algref{alg: SUPUCB}), which uses algorithm CB-GLM (\algref{alg: CB}) as a sub-routine.


\begin{algorithm}
\caption{CB-GLM} \label{alg: CB} \vspace{0.05in}
\textbf{Input}: parameter $\alpha$, index set $\Psi(t)$, and candidate set $A$. \vspace{-0.1in}
\begin{enumerate}
\item  Let $\hat{\theta}_t$ be the solution of
$$\sum_{i \in \Psi(t)} \left[Y_i - \mu(X_i'\theta)\right]X_i = 0$$
\item $ V_t=\sum_{i \in \Psi(t)} X_i X_i'$
\item \textbf{For} {$a \in A$}, \textbf{do}
 $$ w_{t,a} = \alpha \norm{x_{t,a}}_{V_t^{-1}}, \quad m_{t,a} =  \iprod{x_{t,a}}{\hat{\theta}_t}$$
\textbf{End For}
\end{enumerate}
\end{algorithm}


\begin{algorithm}[h]
\caption{SupCB-GLM} \label{alg: SUPUCB}
\vspace{0.05in} \textbf{Input}: tuning parameter $\alpha$, $\tau$, the number of trials $T$.\vspace{0.05in}

\noindent\textbf{Initialization}: \\ \vspace{0.03in}
\indent\indent ~~~\textbf{for} $t\in[\tau]$, randomly choose $a_t \in [K]$. \\ \vspace{0.0in}
\indent\indent ~~ Set $S=\lfloor \log_2{T} \rfloor$, $\mathcal{F}=\{a1, \cdots, a{\tau}\}$ and $\Psi_0=\Psi_1=\cdots=\Psi_S=\varnothing$. \vspace{0.0in}

\textbf{For} {$t=\tau+1,\tau+2,\cdots,T$} \textbf{do}
 \begin{itemize}
 \item[1.] Initialize $A_1=[K]$ and $s=1$.
 \item[2.] While $a_t=$Null
    \begin{itemize}
    \item[a.] \vspace{-0.05in} Run CB-GLM with $\alpha$ and $\Psi_s \cup \mathcal{F}$ to calculate $m_{t,a}^{(s)}$ and $w_{t,a}^{(s)}$ for all $a \in A_s$.
    \item[b.] If $w_{t,a}^{(s)}>2^{-s}$ for some $a \in A_s$, $$ \textrm{set }  a_t=a \textrm{, ~~update } \Psi_s=\Psi_s \cup \{t\} $$
  \item[c.] Else if $w_{t,a}^{(s)} \le 1/\sqrt{T}$ for all $a \in A_s$, $$\textrm{set } a_t=\argmax_{a \in A_s} m_{t,a}^{(s)} \textrm{, ~~update }\Psi_0=\Psi_0 \cup \{t\} $$ 
  \vspace{-0.1in}  \item[d.] Else if $w_{t,a}^{(s)} \le 2^{-s}$ for all $a \in A_s$,
    $$A_{s+1}=\{a \in A_s, m_{t,a}^{(s)} \ge \max_{j \in A_s} m_{t,j}^{(s)} - 2 \cdot 2^{-s}\},$$
    $\quad s \leftarrow s+1\,.$
    \end{itemize}
 \end{itemize} \vspace{-0.05in}
\textbf{End For} \vspace{0.05in}
\end{algorithm}

This algorithm also relies on the idea of confidence bound to do exploration.  At round $t$, the algorithm screens the candidate actions based on the value of $w_{t,a}^{(s)}$ through $S$ stages until an action is chosen. At stage $s$, we set the confidence level at stage $s$ to be $2^{-s}$. If $w_{t,a}^{(s)}>2^{-s}$ for some $a$, we need to do more exploration on $x_{t,a}$ and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than $2^{-s}$, if $m_{t,a}^{(s)}<m_{t,j}^{(s)}-2\cdot2^{-s}$ for some $j \in A_s$, the action $a$ can not be the optimal action. The filter process terminates when we have already got accurate estimate of all $x_{t,a}'\theta^*$ up to the $1/\sqrt{T}$ level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value. 

Our algorithm is different from the algorithm SupLinRel in \citet{auer2003using} that we directly maximize the mean, rather than the upper confidence bound, in steps~c and d. This modification leads to a simpler algorithm and a cleaner regret analysis. Also, we would like to point out that, unlike SpectralEliminator~\cite{valko2014spectral}, the algorithm can easily handle a changing action set.
%\todo{Double check.}


The following result, adapted from \citet[lemma 14]{auer2003using}, shows how the algorithm SupCB-GLM will give us independent samples. For the sake of completeness, we also present its proof here. 
\begin{lemma}\label{lm:independence}
For all $s \in [S] $ and $t \in [T]$, given $\{x_{i,a_i}, i \in \Psi_s(t)\}$, the rewards $\{y_{i,a_i}, i \in \Psi_s(t)\}$ are independent random variables.
\end{lemma}
\begin{proof} [Proof of \lemref{lm:independence}]
Since a trial $t$ can only be added to $\Psi_s(t)$ in step 2b of algorithm SupCB-GLM, the event $\{t \in \Psi_s\}$ only depends on the results of trials $\tau \in \cup_{\sigma<s} \Psi_{\sigma}(t)$ and on $w_{t,a}^{(s)}$. From the definition of $w_{t,a}^{(s)}$, we know it only depends on the feature vectors $x_{i,a_i}, i \in \Psi_s(t)$ and on $x_{t,i}$. This implies the lemma.
\end{proof}

With \lemref{lm:independence}, we are able to apply the non-asymptotic normality result \eqnref{eq:normality} and thus to prove our regret bound of Algorithm SupCB-GLM. 

\begin{theorem} \label{thm:main}
For any $0 < \delta < 1$, if we run the SupCB-GLM algorithm with $\tau=\sqrt{dT}$ and $\alpha=\frac{3\sigma}{\kappa} \sqrt{2\log(TK/\delta)}$ for $T \ge T_0$ rounds, where
\begin{equation} \label{eq:condT}
T_0 = \Omega \left( \frac{\sigma^2}{\kappa^4}\max \left \{d^3, \frac{\log(TK/\delta)}{d} \right\} \right),
\end{equation}
the regret of the algorithm is bounded as
\[ R_T \le  45 (\sigma L_{\mu}/\kappa) \sqrt{\log T \log(TK/\delta)\log(T/d)}  \sqrt{dT}\,,\]
with probability at least $1-\delta$. With $\delta = 1/T$, we obtain \[ \E[R_T] = O\left ((\log T)^{1.5} \sqrt{dT \log K} \right). \]
\end{theorem}

%The proof of \thmref{thm:main} will be collected in the supplementary material.
The theorem demonstrates an $\tilde{O}(\sqrt{dT\log K})$ regret bound for the algorithm SupCB-GLM. It has been proved in \citet[Theorem 2]{chu2011contextual} that $\sqrt{dT}$ is the minimax lower bound of the expected regret for $K$-armed linear bandits, a special of the GLM bandits considered here. 
%Since then linear bandit is a special case of generalized linear bandit, $\sqrt{dT}$ is also the minimax lower bound for the generalized linear bandits problem we consider.
Therefore, the regret of our SupCB-GLM algorithm is optimal up to logarithm terms of $T$ and $K$. To the best of our knowledge, this is the first algorithm which achieves the (near-)optimal rate of GLM bandits.

% The assumption on $T$ is natural since one can not expected to get an accurate estimation of $\theta^*$ if $T \le d$. In practice, particularly in Internet service related bandits problems, the number of rounds $T$ will be sufficiently large and those assumptions on $T$ will usually be satisfied.  

It is worthwhile to compare \thmref{thm:main} with the result in \thmref{thm:main1}. When $K =o(2^{d})$ is small, the rate of SupCB-GLM is faster, and we improve the previous rates by a $\sqrt{d}$ factor. Here, we give a briefly illustration of how we get rid of the extra $\sqrt{d}$ factor. Both in \thmref{thm:main1} and in \citet{filippi2010parametric},  $|x'(\hat{\theta}_n-\theta^*)|$ is upper bounded by using the Cauchy-Schwartz inequality,
\begin{equation} \label{eq:cauchy}
|x'(\hat{\theta}_n-\theta^*)| \le \norm{x}_{V_n^{-1}} \norm{\hat{\theta}_n - \theta^*}_{V_n}\,.
\end{equation}
\lemref{lm:deltatbound} in the supplementary material establishes that
\[ \norm{\hat{\theta}_n - \theta^*}_{V_n} 
%\le C_1\norm{Z}_{{V_n^{-1}}} 
\le C_2 \sqrt{d\log(T/\delta)}.\] 
This will lead to an extra $\sqrt{d}$ factor compared to \eqnref{eq:normality}. By using the Cauchy-Schwartz inequality~\eqnref{eq:cauchy}, we only make use of the fact that $\hat{\theta}_n$ is close to $\theta^*$ in the $\ell_2$ sense. However, \eqnref{eq:normality} tells us that actually $\hat{\theta}_n$ is close to $\theta^*$ in \emph{every} direction. This is the reason why we are able to remove the extra $\sqrt{d}$ factor to achieve a near-optimal regret.  It also explains why the bound in \thmref{thm:main1} is tight when $K$ is large. As $K$ goes large, it is likely there is a direction $x$ for which \eqnref{eq:cauchy} is tight.

\begin{proof}[Proof of \thmref{thm:main}]
To facilitate our proof, we first present two technical lemmas.  \lemref{lm:ucb} follows from  \lemref{lm:independence}, \thmref{thm:glm_ucb},  Theorem~5.39 of \citet{vershynin2010introduction} and a union bound.  The proof of \lemref{lm:propSup} is deferred to the appendix.

\begin{lemma} \label{lm:ucb}
Fix $\delta>0$. Choose in SupCB-GLM $\tau=\sqrt{dT}$ and $\alpha=\frac{3\sigma}{\kappa} \sqrt{2\log(TK/\delta)}$.  Suppose $T$ satisfies condition \eqnref{eq:condT}.  Define the following event:
\begin{eqnarray}
\EvtX &\defeq& \{|m_{t,a}^{(s)} - x_{t,a}'\theta^*| \le w_{t,a}^{(s)}, 
\label{eq:ucb} \\
&& \forall t\in[\tau+1,T], s\in[S], a\in[K]\} \nonumber
\end{eqnarray}
Then, event $\EvtX$ holds with probability at least $1-\delta $.
\end{lemma}

\begin{proof} [Proof of \lemref{lm:ucb}]
By \lemref{lm:independence}, we have independent samples now. Then to apply \thmref{thm:glm_ucb}, the key is to lower bound the minimum eigenvalue of $V_t$. Note that we randomly select the feature vectors at the first $\tau=\sqrt{dT}$ rounds, that is, they are independent. Moreover, the feature vectors are bounded. Thus, $X_1, X_2, \ldots, X_\tau$ are independent sub-Gaussian with parameter $1$.  It follows from Proposition \ref{pro:spectral} that
%Theorem 5.39 in \cite{vershynin2010introduction}, 
%we have
\[
\lmin(V_t) \ge \lmin(V_\tau) \ge c\sqrt{dT}
\]
for some constant $c$ with probability at least $1-\exp(-\sqrt{dT})$. By Theorem \ref{thm:glm_ucb} and union bound, we have the desired result under condition  \eqnref{eq:condT}.
\end{proof}

\begin{lemma} \label{lm:propSup}
Suppose that event $\EvtX$ holds, and that in round $t$, the action $a_t$ is chosen at stage $s_t$.  Then, $a_t^{*} \in A_s$ for all $s \le s_t$. Furthermore, we have
\begin{align*}
&\mu(x_{t,a_t^*}' \theta^*) - \mu(x_{t,a_t}' \theta^*) \\
&\le \left\{ \begin{array}{ll}
 (8 L_\mu)/2^{s_t} & \textrm{if $a_t$ is selected in step 2b}\\
 (2L_\mu)/\sqrt{T} & \textrm{if $a_t$ is selected in step 2c}\,.
  \end{array} \right.
\end{align*}
\end{lemma}

Define $V_{s,t}=\sum_{t \in \Psi_s(T)} X_tX_t'$, then by \lemref{lm:widthsum}, 
\begin{eqnarray*}
\sum_{t \in \Psi_s(T)} w_{t,a_t}^{(s)} &=& \sum_{t \in \Psi_s(T)} \alpha(\delta) \|x_{t,a_t}\|_{V_{s,t}^{-1}} \\
&\le& \alpha(\delta) \sqrt{2 d \log(T/d) |\Psi_s(n)|}\,.
\end{eqnarray*}
On the other hand, by the step 2b of SupCB-GLM,
\begin{equation*}
\sum_{t \in \Psi_s(T)} w_{t,a_t}^{(s)} \ge 2^{-s} |\Psi_s(T)|.
\end{equation*}
Combining the above two inequalities gives us
\begin{equation} \label{eq:cardbound}
|\Psi_s(T)| \le 2^s \alpha(\delta) \sqrt{2 d\log{(T/d)} |\Psi_s(T)| }.
\end{equation}
Let $\Psi_0$ be the collection of trials such that $a_t$ is chosen in step 2c. Since we have chose $S=\log_2 T$, each $t \in [\tau+1, T]$ must be in one of $\Psi_s$ and hence, $\{\tau,\tau+1,\ldots,T\}=\Psi_0\cup \left(\cup_{s=1}^{S}\Psi_s(T)\right)$. If we set $\tau=\sqrt{dT}$, we have
\begin{eqnarray*}
\lefteqn{R_T = \sum_{t=1}^{\tau} \left( \mu(x_{t,a_t^*}' \theta^*) - \mu(x_{t,a_t}' \theta^*) \right)} \\
&& + \sum_{t=\tau+1}^{T} \left( \mu(x_{t,a_t^*}' \theta^*) - \mu(x_{t,a_t}' \theta^*) \right) \\
&\le & \tau + \sum_{t \in \Psi_0}  \left( \mu(x_{t,a_t^*}' \theta^*) - \mu(x_{t,a_t}' \theta^*)\right) \\
&& + \sum_{s=1}^{S}  \sum_{t \in \Psi_s(T)} \left( \mu(x_{t,a_t^*}' \theta^*) - \mu(x_{t,a_t}' \theta^*)\right) \\
&\le& \sqrt{dT} + T \cdot  \frac{2L_{\mu}}{\sqrt{T}} + \sum_{s=1}^{S}  L_\mu \cdot 2^{3-s} \cdot |\Psi_s(T)| \\
&\le& \sqrt{dT} + 2L_\mu \sqrt{T} \\
& & + 8L_\mu \alpha(\delta)  \sum_{s=1}^{S} \sqrt{2d \log\frac{T}{d} |\Psi_s(T)| } \\
&\le& \sqrt{dT} + 2L_\mu \sqrt{T} + 8L_\mu \alpha(\delta)   \sqrt{2d \log(T/d)} \sqrt{ST} \\
&\le& 45 (\sigma L_{\mu}/\kappa) \sqrt{\log T \log(TK/\delta)\log(T/d)} \sqrt{dT}\,,
\end{eqnarray*}
with probability at least $1 - \delta$. Here, the first inequality is due to the assumption that $0 \le \mu \le 1$. The second inequality is \lemref{lm:propSup}. The third inequality is the inequality \eqnref{eq:cardbound} and the fourth inequality is implied by Cauchy-Schwartz. This completes the proof of the high-probability result. 
\end{proof}


\section{Discussions}
\label{sec:discussions}

In this paper, we propose two algorithms for $K$-armed bandits with generalized linear models. While the first algorithm, UCB-GLM, achieves the optimal rate for the case of infinite number of arms, the second algorithm SupCB-GLM is provable optimal for the case of finite number actions at each round. However, it remains open whether UCB-GLM can achieve the optimal rate for small $K$.

\subsection{A better regret bound for UCB-GLM}

A key quantity in determining the regret of UCB-GLM is the minimum eigenvalue of $V_t$. If we make an addition assumption on the minimum eigenvalue of $V_t$, we will be able to prove an $O(\sqrt{dT})$ regret bound for UCB-GLM.

\begin{theorem} \label{thm:maindis}
We run algorithm UCB-GLM with $\tau=\frac{8\sigma^2}{\kappa^2}d\log T$ and $\alpha \le L_{\mu} \sigma/\kappa$. For any $\delta \in [1/T,1)$, suppose there is an universal constant $c$ such that
\begin{equation} \label{eq:assump4}
\sum_{t=\tau+1}^{T} \lmin^{-1/2}(V_t) \le  c \sqrt{T}.
\end{equation}
holds with probability at least $1-\delta$, and 
\begin{equation} \label{eq:condition1}
T=\Omega \left(\frac{\sigma R}{ \kappa L_{\mu} } d \log^2 T \right).
\end{equation}
Then, the regret of the algorithm is bounded by
\begin{equation*}
R_T \le \frac{C L_{\mu} \sigma}{\kappa} \sqrt{dT\log(T/\delta)}
\end{equation*}
with probability at least $1-2\delta$, where $C$ is a positive, universal constant. \end{theorem}

This theorem provides some insights of why UCB-GLM performs well in practice.  Although the condition in \eqnref{eq:assump4} is hard to check and may be violated in some cases, for example, in $K$-armed bandits, we provide a heuristic argument to justify this assumption in a range of problems. When $t$ is large enough, our estimator $\hat{\theta}_t$ is very close to $\theta^*$. If we assume there is a positive gap between $\iprod{x_{t,a_t^*}}{\theta^*}$ and $\iprod{x_{t,a}}{\theta^*}$ for all $a \neq a_t^*$, we will have $a_t=a_t^*$ after, for example, $\sqrt{T}$ steps. Since $\left \{x_{t,a},a \in [K] \right \}$ are independent for $t \in [T]$, $\{x_{t,a_t^*}\}$ are also independent samples. Then $V_t/t$ will be well-approximated by the covariance matrix of $x_{t,a_t^*}$, which we denote by $\Sigma_0$.  In many problem in practice, especially when features are \emph{dense}, it is unlikely the feature vector $x_{t,a_t^*}$ lies in a low-dimensional subspace of $\R^d$.  It implies that $\Sigma_0$ has full rank, and that we will have $\lmin(V_t)=\Theta(t \cdot \lmin(\Sigma_0))$ when $t$ is large enough.  It follows that,
\begin{equation*} 
\sum_{t=\tau+1}^{T} \frac{1}{\sqrt{\lmin(V_t)}} = \sum_{t=\tau+1}^{T} \Theta(t^{-1/2}) = O(\sqrt{T}).
\end{equation*}

It should be cautioned that, since we do not know the distribution of our feature vectors, we cannot assume the above gap exists.  It is therefore challenging to make the above arguments rigorous.
% Note that whether assumption \eqnref{eq:assump4} will be satisfied or not depends on the choice of $\alpha$. It might be violated sometimes. 
In fact, when studying the ARIMA model in time series, \citet[Example 1]{lai1982least} provide an example such that $\lmin(V_t)=O(\log t)$.
%\todo{Relation to Sarkar?}

\subsection{Open Questions}

\paragraph{Computational efficient algorithms.}

While UCB-GLM and SupCB-GLM enjoy good theoretical properties, they can be expensive in some applications.  First, they require inverting a $d\times d$ matrix in every step, a costly operation when $d$ is large.  Second, at step $t$, the MLE is computed using $\Theta(t)$ samples, meaning that the per-step complexity grows at least linearly with $t$ for a straightforward implementation of the algorithms.  It is therefore interesting to investigate more scalable alternatives.  It is possible to use a first-order, iterative optimization procedure to amortize the cost, analogous to the approach of \citet{agarwal2014taming}.
%
%. and have a complexity polynomial in the feature dimension $d$, they are still expensive to run when $d$ is large
%they are computationally inefficient because they both need to calculate the inverse of a $d \times d$ matrix at every round.
%In practice, the dimensionality of the data is usually very high and may makes those two algorithms computationally intractable. So it is a very interesting question to explore efficient and provable optimal algorithms for the generalized linear bandits.

\paragraph{$K$-dependent lower bound.}
Currently, all the lower bound results on (generalized) linear bandits have no dependence on $K$, the number of arms. The minimax lower bound will be of particularly interest because all current lower bound results assume that $K \le d$. Although it will at most be a logarithm dependence on $K$, it is still a theoretically interesting question.

\paragraph{Randomized algorithms with optimal regret rate.}
As opposed to the deterministic, UCB-style algorithms studied in this paper, randomized algorithms like EXP4~\cite{auer2002nonstochastic} and Thompson Sampling~\cite{Thompson33Likelihood} have advantages in certain situations, for example, when reward observations are delayed~\cite{Chapelle12Empirical}.  Recently developed techniques for analyzing Bayes regret in BLM bandits~\cite{Russo14Learning} may be useful to analyze the cumulative regret considered here.
