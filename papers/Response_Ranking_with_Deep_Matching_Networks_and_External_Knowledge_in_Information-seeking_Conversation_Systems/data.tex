%\section{Task Definition \& Data Sets}
%\label{sec:data_task}
%% Present it in a matchmatical way
%% Problem Formalization
%% Should define dialogs, context, candidate response, external collection (CQA), external collection (Wiki), etc...
%
%\subsection{Problem Formulation}
%The research problem of response ranking in information-seeking conversations is defined as following. We are given an information-seeking conversation data set $\mathcal{D} = \{(\mathcal{U}_i, \mathcal{R}_i,  \mathcal{Y}_i)\}_{i=1}^N$, where $ \mathcal{U}_i = \{u_i^1, u_i^2, \dots, u_i^{t-1} , u_i^t\} $ in which $ \{u_i^1, u_i^2, \dots, u_i^{t-1}\} $ is the dialog context and $u_i^t$ is the  input utterance in the $t$-th turn. $\mathcal{R}_i$ and $\mathcal{Y}_i$ are a set of response candidates $ \{r_i^1, r_i^2, \dots, r_{i}^k\}_{k=1}^M $ and the corresponding binary labels $ \{y_i^1, y_i^2, \dots, y_{i}^k\} $, where $y_i^k=1$ denotes $r_i^k$ is a true response for $\mathcal{U}_i$. Otherwise $y_i^k=0$. In order to integrate external knowledge, we are also given an external collection  $\mathcal{E}$, which is related to the topics discussed during the conversation $\mathcal{U}$. Our task is to learn a ranking model $f(\cdot)$ with $\mathcal{D}$ and $\mathcal{E}$. For any given $\mathcal{U}_i$, the model should be able to generate a rank list for the candidate responses $\mathcal{R}_i$ with $f(\cdot)$.  The external collection $\mathcal{E}$ could any massive text corpus. In our paper, $\mathcal{E}$ are historical QA pairs in Stack Overflow data dump \footnote{\url{https://stackoverflow.com/}} for MS Dialog data, AskUbuntu data dump  \footnote{\url{https://askubuntu.com/}} for Ubuntu dialog corpus or dialog related product QA pairs for eCommerce data. The summary of experimental data sets used in this paper is shown in Table \ref{tab:data_summery}.
%%
%\begin{table*}[th]
%	\centering
%	\begin{scriptsize}
%		\caption{Summary of experimental datasets in this paper.}
%		\label{tab:data_summery}
%		\begin{tabular}{ l|| l l l}
%			\hline \hline
%			Data & QA Dialog (QAD) & External QA Collection (QAC) & Source  \\
%			\hline 
%			UDC & Ubuntu QA dialogs extracted from IRC network & AskUbuntu data dump & Both QAD and QAC are publicly available
%\\
%			MSDialog & Customer service dialogs on Microsoft products & Stack Overflow data dump & QAD is crawled from Web; QAC is publicly available \\
%			 E-commerce & Customer service dialogs from a large eCommerce company & Internal product QAC data from the same company & Both QAD and QAC are proprietary data \\
%			\hline \hline
%		\end{tabular}
%	\end{scriptsize}
%\end{table*}



%The comparison of different formulations for single-turn conversation, multi-turn conversation and multi-turn conversation with external collection is presented in Table \ref{tab:problem_format}. 
%
%In addition to MSDialog and UDC, we also use a real commercial technical support service data from a large eCommerce company. All these three data sets contain a set of information-seeking conversation dialog and a external QA pairs collection as the source of external knowledge. 

%\begin{table*}[th]
%	\centering
%		\caption{Different formulations for single-turn conversation, multi-turn conversation and multi-turn conversation with external collection.}
%		\label{tab:problem_format}
%		\begin{tabular}{ l ||  l |  l |  l  }
%			\hline \hline
%			  & Single-Turn & Multi-Turn & Multi-Turn with External Collection  \\ \hline
%			\hline
%			Input & $u_i^t$ & $ \{u_i^1, u_i^2, \dots, u_i^t\} $ & $ \{u_i^1, u_i^2, \dots,  u_i^t\}, \mathcal{E}$ 
%			\\
%			Output & $r_i$& $r_i$ &$r_i$ \\
%			Object Func. & $r_i^* = \argmax_{r_i^k}f(r_i^k|u_i^t)$ & $r_i^* = \argmax_{r_i^k}f( r_i^k| \{u_i^1, u_i^2, \dots, u_i^t\} )$ &  $r_i^* = \argmax_{r_i^k}f(r_i^k| \{u_i^1, u_i^2, \dots, u_i^t\} , \mathcal{E})$ \\
%			\hline\hline
%		\end{tabular}
%\end{table*}

%\subsection{Informaiton-seeking Conversation Data Collection and Analysis}
%
%\ylcomment{Can replace this section with some sentences and percentage of QA related utterances. The table can be put into another paper.}
%
%\ylcomment{Can merge section 3 and section 4 into one section.}
%
%\begin{table}[]
%	\centering
%	\caption{The user intent label distribution of randomly sampled MSDialog and UDC data.}
%	\label{tab:user_intent}
%	\begin{tabular}{l | | l | l | l | l | l}
%		\hline \hline
%		\multicolumn{2}{l|}{Label Types} & \multicolumn{2}{l|}{MSDialog} & \multicolumn{2}{l}{UDC} \\ \hline
%		Code    & Label                   & Count       & Percent      & Count    & Percent    \\ \hline \hline
%		GG      & Greetings/Gratitude     & 4031        & 21.8\%         & 105      & 2.0\%        \\ \hline
%		PA      & Potential Answer        & 4011        & 21.7\%         & 1015     & 18.9\%       \\ \hline
%		FD      & Further Details         & 2505        & 13.5\%         & 1751     & 32.6\%       \\ \hline
%		OQ      & Original Question       & 2352        & 12.7\%         & 184      & 3.4\%        \\ \hline
%		IR      & Information Request     & 1083        & 5.9\%          & 525      & 9.8\%        \\ \hline
%		PF      & Positive Feedback       & 1086        & 5.9\%          & 99       & 1.8\%        \\ \hline
%		FQ      & Follow Up Question      & 890         & 4.8\%          & 510      & 9.5\%        \\ \hline
%		CQ      & Clarifiying Question    & 753         & 4.1\%          & 511      & 9.5\%        \\ \hline
%		NF      & Negative Feedback       & 761         & 4.1\%          & 230      & 4.3\%        \\ \hline
%		RQ      & Repeat Question         & 612         & 3.3\%          & 41       & 0.8\%        \\ \hline
%		JK      & Junk                    & 261         & 1.4\%          & 355      & 6.6\%        \\ \hline
%		O       & Other                   & 149         & 0.8\%          & 54       & 1.0\%        \\ \hline 
%		Total & 12       & 18494       & 100.0\%        & 5380     & 100.0\%      \\ \hline \hline
%	\end{tabular}
%\end{table}
%
%% Data collection
%As presented in Section \ref{sec:intro}, we target on studying information-seeking conversations. We created a large technical support conversation data MSDialog, which is about Microsoft products. Specifically, we crawled over $35,000$ dialogs from Microsoft Answer Community, an online forum that provides technical support for a wide range of Microsoft products. This well-moderated forum contains user-generated questions with high-quality answers provided by Microsoft staff, community moderators, article authors, and other experienced users including Microsoft Most Valuable Professionals (MVPs). In addition to this, we also consider another popular open benchmark data Ubuntu Dialog Corpus (UDC) \cite{DBLP:journals/corr/LowePSP15}, which consists of almost one million two-person technical support conversations about Ubuntu extracted from the chat logs on the Freenode Internet Relay Chat (IRC) network.
%
%% Data annotation to verfify most utterances are on Q&A content
%In order to verify the MSDialog and UDC data are information-seeking conversation data, we sample  $10,032$ MSDialog utterances and $4,063$ UDC utterances in the dialogs. Then we use Amazon Mechanical Turk \footnote{\url{https://www.mturk.com/}} to hire workers to label the finer-grained user intents of each utterance. We defined the following $12$ user intent classes: Original Question (OQ), Repeat Question (RQ), Clarifying Question (CQ), Further Details (FD), Information Request (IR), Potential Answer (PA), Positive Feedback (PF), Negative Feedback (NF), Greetings/ Gratitude (GG), Junk (JK) and Others (O). Each utterance can have multiple labels. Each dialog is annotated by two different workers and we calculate the inter-rater agreement using Fuzzy Kappa \cite{Kirilenko2016Inter} for this one-to-many classification task. Annotated dialogs with small kappa scores are filtered. The label distributions of these two annotated data are shown in Table \ref{tab:user_intent}. We can see that most user intent labels in both MSDialog and UDC are relevant to questions (OQ, FQ, CQ, RQ, IR), answers (PA, FD) and user feedback (PF, NF). Althogh we can see more utterances expressing greetings/ gratitude in MSDialog, the other labels relevant to junk/others are minor parts in both data sets. The annotation results show that MSDialog and UDC represent the typical content in information-seeking conversations. Thus they are proper data sets for our investigation in this paper.
%
%In addition to MSDialog and UDC, we also use a real commercial technical support service data from a large eCommerce company. All these three data sets contain a set of information-seeking conversation dialog and a external QA pairs collection as the source of external knowledge. The summary of experimental data sets used in this paper is shown in Table \ref{tab:data_summery}.
%
%\begin{table*}[th]
%	\centering
%	\begin{scriptsize}
%		\caption{Summary of experimental datasets in this paper.}
%		\label{tab:data_summery}
%		\begin{tabular}{ l|| l l l}
%			\hline \hline
%			Data & QA Dialog (QAD) & External QA Collection (QAC) & Source  \\
%			\hline 
%			UDC & Ubuntu QA dialogs extracted from IRC network & AskUbuntu data dump & Both QAD and QAC are publicly available
%\\
%			MSDialog & Customer service dialogs on Microsoft products & Stack Overflow data dump & QAD is crawled from Web; QAC is publicly available \\
%			 E-commerce & Customer service dialogs from a large eCommerce company & Internal product QAC data from the same company & Both QAD and QAC are proprietary data \\
%			\hline \hline
%		\end{tabular}
%	\end{scriptsize}
%\end{table*}


%\textbf{Informaiton-seeking Conversation Data Analysis}
%
%\ylcomment{Main statistics and analysis on MSDialog/Ubuntu data to indicate most utterances in these data are indeed questions\&answers, which is what information-seeking conversations are about.}

 