\section{Related Work}
\label{sec:rel}

% The google doc link for the related work survey
% Refer to the realted work section of the NEUIR17 paper and SIE paper

%\begin{table*}[th]
%	\centering
%	\begin{footnotesize}
%		\caption{Summary of experimental datasets in this paper.}
%		\vspace{-0.15in}
%		\label{tab:data_summery}
%		\begin{tabular}{ l|| l l l}
%			\hline \hline
%			Data & QA Dialog (QAD) & External QA Collection (QAC) & Source  \\
%			\hline 
%			UDC & Ubuntu QA dialogs extracted from IRC network & AskUbuntu data dump & Both QAD and QAC are publicly available
%			\\
%			MSDialog & Customer service dialogs on Microsoft products & Stack Overflow data dump & QAD is crawled from Web; QAC is publicly available \\
%			E-commerce & Customer service dialogs from a large eCommerce company & Internal product FAQ data & Both QAD and QAC are proprietary data \\
%			\hline \hline
%		\end{tabular}
%	\end{footnotesize}
%\end{table*} 

Our work is related to research on conversational search, neural conversational models and neural ranking models.

%\textbf{Information-seeking Conversations}

\textbf{Conversational Search.}
Conversational search has received significant attention with the emerging of conversational devices in the recent years. Radlinski and Craswell described the basic features of conversational search systems \cite{radlinski2017theoretical}. Thomas et al. \cite{thomas2017misc} released the Microsoft Information-Seeking Conversation (MISC) data set, which contains information-seeking conversations with a human intermediary, in a setup designed to mimic software agents such as Siri or Cortana. But this data is quite small (in terms of the number of dialogs) for the training of neural models. Based on state-of-the-art advances on machine reading, Kenter and de Rijke \cite{kenter-attentive-2017} adopted a conversational search approach to question answering. Except for conversational search models, researchers have also studied the medium of conversational search. Arguello et al. \cite{arguello2017factors} studied how the medium (e.g., voice interaction) affect user requests in conversational search. Spina et al. studied the ways of presenting search results over speech-only channels to support conversational search \cite{spina2017extracting,trippas2015towards}. Yang et al. \cite{DBLP:journals/corr/YangZZGC17} investigated predicting the new question that the user will ask given the past conversational context. Our research targets at the response ranking of information-seeking conversations, with deep matching networks and integration of external knowledge.

% Vakulenko et al. \cite{vakulenko2017conversational} adopted interactive storytelling as a tool to enable exploratory search within a conversational interface.

\textbf{Neural Conversational Models.}
Recent years there are growing interests on research about conversation response generation and ranking with deep learning and reinforcement learning \cite{DBLP:conf/acl/ShangLL15,DBLP:conf/sigir/YanSW16,DBLP:conf/cikm/YanSZW16,DBLP:conf/sigir/YanZE17,DBLP:conf/acl/LiGBSGD16,DBLP:conf/emnlp/LiMRJGG16,DBLP:conf/naacl/SordoniGABJMNGD15,bordes2017learning}. Existing work includes retrieval-based methods \cite{DBLP:conf/acl/WuWXZL17,DBLP:conf/emnlp/ZhouDWZYTLY16,DBLP:conf/sigir/YanSW16,DBLP:conf/sigir/YanZE17,DBLP:journals/corr/JiLL14,DBLP:journals/corr/LowePSP15} and generation-based methods \cite{DBLP:conf/acl/ShangLL15,DBLP:conf/acl/TianYMSFZ17,DBLP:conf/emnlp/RitterCD11,DBLP:conf/naacl/SordoniGABJMNGD15,DBLP:journals/corr/VinyalsL15,DBLP:conf/emnlp/LiMRJGG16,bordes2017learning,P17-1045,alime-chat}. Sordoni et al. \cite{DBLP:conf/naacl/SordoniGABJMNGD15} proposed a neural network architecture for response generation that is both context-sensitive and data-driven utilizing the Recurrent Neural Network Language Model architecture. %Bordes et al. \cite{bordes2017learning} proposed a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications based on Memory Networks  \cite{weston2015memory,sukhbaatar2015end,graves2014neural}. Shang et al. \cite{DBLP:conf/acl/ShangLL15} proposed Neural Responding Machine (NRM), which is a RNN encoder-decoder framework for short text conversation and showed that it outperformed retrieved-based methods and SMT-based methods for single round conversation.
 Our work is a retrieval-based method. There are some research on multi-turn conversations with retrieval-based method. Wu et al. \cite{DBLP:conf/acl/WuWXZL17} proposed a sequential matching network that matches a response with each utterance in the context on multiple levels of granularity to distill important matching information. The main difference between our work with their research is that we consider external knowledge beyond dialog context for multi-turn response selection. We show that incorporating external knowledge with pseudo-relevance feedback and QA correspondence knowledge distillation is important and effective for response selection.

% Yan et al. \cite{DBLP:conf/sigir/YanSW16} proposed a retrieval-based conversation system with the deep learning-to-respond schema by concatenating context utterances with the input message as reformulated queries. 
% Li et al. \cite{DBLP:conf/emnlp/LiMRJGG16} apply deep reinforcement learning to model future reward in chatbot dialogs towards building a neural conversational model based on the long-term success of dialogs. Zhou et al. \cite{DBLP:conf/emnlp/ZhouDWZYTLY16} proposed a multi-view response selection model that integrates information from two different views including word sequence view and utterance sequence view with deep neural networks. 
 %\ylcomment{Dhingra et al. \cite{P17-1045} proposed KB-InfoBot, which is a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. }

%Recently, neural approaches to dialog systems have attracted much attention in the NLP community \cite{henderson2013deep,bordes2017learning,wen2016network}. With the ability of logical inference on textual inputs, Memory Network (MemNN) \cite{weston2015memory,sukhbaatar2015end,graves2014neural} has been one of the state-of-the-art approaches to dialog systems \cite{bordes2017learning,li2017learning,dodge2016evaluating}, question answering \cite{li2017learning,bordes2015large,xiong2016dynamic,kumar2016ask}, and machine reading \cite{miller2016key,shen2017reasonet}. Memory networks exhibit close alignment with the conversational search task -- by learning word embeddings and using attention mechanisms to select important signals from query and item, it helps to alleviate vocabulary mismatch between user responses and machine knowledge, and to improve search performance by focusing on the important signals for the current conversation.

%\textbf{Multi-turn Question Answering}



\textbf{Neural Ranking Models.}
Recently a number of neural ranking models have been proposed for information retrieval, question answering and conversation response ranking. These models could be classified into three categories \cite{Guo:2016:DRM:2983323.2983769}. The first category is the representation focused models. These models will firstly learn the representations of queries and documents separately and then calculate the similarity score of the learned representations with functions such as cosine, dot, bilinear or tensor layers. A typical example is the DSSM \cite{DBLP:conf/cikm/HuangHGDAH13} model, which is a feed forward neural network with a word hashing phase as the first layer to predict the click probability given a query string and a document title. The second category is the interaction focused models, which build a query-document term pairwise interaction matrix to capture the exact matching and semantic matching information between the query-document pairs. Then the interaction matrix will be fed into deep neural networks which could be CNN \cite{DBLP:conf/nips/HuLLC14,DBLP:conf/aaai/PangLGXWC16,alime-tl}, term gating network with histogram or value shared weighting mechanism \cite{Guo:2016:DRM:2983323.2983769,Yang:2016:ARS:2983323.2983818} to generate the final ranking score. In the end, the neural ranking models in the third category combine the ideas of the representation focused models and interaction focused models to joint learn the lexical matching and semantic matching between queries and documents \cite{Mitra:2017:LMU:3038912.3052579,alime-tl}. The deep matching networks used in our research belong to the interaction focused models due to their better performances on a variety of text matching tasks compared with representation focused models \cite{DBLP:conf/nips/HuLLC14,DBLP:conf/aaai/PangLGXWC16,Guo:2016:DRM:2983323.2983769,Yang:2016:ARS:2983323.2983818,DBLP:conf/acl/WuWXZL17, Xiong:2017:ENA:3077136.3080809}. We study different ways to build the interaction matching matrices to capture the matching patterns in term spaces, sequence structures and external knowledge signals between dialog context utterances and response candidates.

%  In this way, the model could capture the matching pattern between the raw word embedding representation of queries and documents before the final representations become mature. 



