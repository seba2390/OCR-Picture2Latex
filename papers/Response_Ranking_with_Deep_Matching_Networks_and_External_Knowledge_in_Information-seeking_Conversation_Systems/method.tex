

\section{Our Approach}
\subsection{Problem Formulation}
The research problem of response ranking in information-seeking conversations is defined as follows. We are given an information-seeking conversation data set $\mathcal{D} = \{(\mathcal{U}_i, \mathcal{R}_i,  \mathcal{Y}_i)\}_{i=1}^N$, where $ \mathcal{U}_i = \{u_i^1, u_i^2, \dots, u_i^{t-1} , u_i^t\} $ in which $ \{u_i^1, u_i^2, \dots, u_i^{t-1}\} $ is the dialog context and $u_i^t$ is the  input utterance in the $t$-th turn. $\mathcal{R}_i$ and $\mathcal{Y}_i$ are a set of response candidates $ \{r_i^1, r_i^2, \dots, r_{i}^k\}_{k=1}^M $ and the corresponding binary labels $ \{y_i^1, y_i^2, \dots, y_{i}^k\} $, where $y_i^k=1$ denotes $r_i^k$ is a true response for $\mathcal{U}_i$. Otherwise $y_i^k=0$. In order to integrate external knowledge, we are also given an external collection  $\mathcal{E}$, which is related to the topics discussed in conversation $\mathcal{U}$. Our task is to learn a ranking model $f(\cdot)$ with $\mathcal{D}$ and $\mathcal{E}$. For any given $\mathcal{U}_i$, the model should be able to generate a ranking list for the candidate responses $\mathcal{R}_i$ with $f(\cdot)$.  The external collection $\mathcal{E}$ could be any massive text corpus. In our paper, $\mathcal{E}$ are historical QA posts in Stack Overflow data dump \footnote{\url{https://stackoverflow.com/}} for MSDialog, AskUbuntu data dump  \footnote{\url{https://askubuntu.com/}} for Ubuntu Dialog Corpus and product QA pairs for AliMe data. % from Alibaba group. %The summary of experimental data sets used in this paper is shown in Table \ref{tab:data_summery}.

  \begin{table}[!t]
  	\footnotesize
	\caption{A summary of key notations in this work. Note that all vectors are denoted with bold cases.}
	\vspace{-10pt}
	\begin{tabular}
		{ p{0.06\textwidth} | p{0.38\textwidth}} \hline  \hline
		$\mathcal{D}$ & The conversation data set used for training/validation/testing\\\hline
		$\mathcal{E}$ & The collection for the retrieval and distillation of external knowledge \\ \hline
		$u_i^t, \mathcal{U}_i, \mathcal{U}$ & The $t$-th utterance of the $i$-th dialog, all utterances of the $i$-th dialog and the set of all dialog utterances\\\hline
		$r_i^k, \mathcal{R}_i, \mathcal{R}$ & The $k$-th response candidate for the  $i$-th dialog, all response candidates of the  $i$-th dialog and the set of all candidate responses\\\hline
		$r_i^{k'}$ & The $k$-th expanded response candidate for the  $i$-th dialog \\ \hline
		$y_i^k, \mathcal{Y}$ & The label for the $k$-th response candidate for the $i$-th dialog and the set of all labels \\ \hline
		$f(\cdot)$ & The ranking model learnt with $\mathcal{D}$ and $\mathcal{E}$ \\ \hline
		$f( \mathcal{U}_i, r_i^k ) $ & The predicted matching score between $\mathcal{U}_i$ and $r_i^k$ \\ \hline
		$N$ & The total number of dialogs in $\mathcal{D}$\\ \hline
		$M$ & The total number of response candidates for $\mathcal{U}_i$\\ \hline
		$W$ & The number of expanded words in response candidates\\ \hline
		$\theta$ & The language model constructed from the pseudo relevance feedback document set for response candidate expansion\\ \hline
		$P, \mathcal{P} $ & The number of top ranked QA posts  retrieved from $\mathcal{E}$ and  the top ranked QA post set \\ \hline
		$l_r, l_u$ & The length of a response candidate and the length of an utterance \\ \hline
		$d$ & The number of dimensions of word embedding vectors \\ \hline
		$\mathbf{M}_1$,$\mathbf{M}_2$,$\mathbf{M}_3$ & Interaction matrices between dialog utterance $u_i^t$ and candidate response $r_i^k$ or  $r_i^{k'}$ for word embedding similarity, sequence hidden representation similarity and QA  correspondence matching similarity\\ \hline
		$m_{1,i,j}$ & The $(i,j)$-th element in the interaction matrix $\mathbf{M}_1$\\ \hline
		$c$ & The window size for the utterances in dialog context, which is the maximal number of previous utterances modeled \\ \hline  \hline
	\end{tabular}\label{tab:notation}
	\vspace{-10pt}
\end{table}

% Present in a mathmatical way
\subsection{Method Overview}
 In the following sections, we describe the proposed learning framework built on the top of deep matching networks and external knowledge for response ranking in information-seeking conversations. A summary of key notations in this work is presented in Table \ref{tab:notation}. In general, there are three modules in our learning framework:
 
 %\begin{enumerate}
 (1) \textbf{Information retrieval (IR) module:} Given the information seeking conversation data $\mathcal{D}$ and external QA text collection $\mathcal{E}$, this module is to retrieve a small relevant set of QA pairs $\mathcal{P}$ from $\mathcal{E}$ with the response candidate $\mathcal{R}$ as the queries. These retrieved QA pairs $\mathcal{P}$ become the source of external knowledge. % to be integrated into the neural models.
 	
 (2) \textbf{External knowledge extraction (KE) module:} Given the retrieved QA pairs $\mathcal{P}$  from the IR module, this module will extract useful information as term distributions, term co-occurrence matrices or other forms as external knowledge.
 	
 (3) \textbf{Deep matching network (DMN) module:} This is the module to model the extracted external knowledge from $\mathcal{P}$ , dialog utterances $ \mathcal{U}_i$ and the response candidate $ r_i^k$ to learn the matching pattern, over which it will accumulate and predict a matching score $f( \mathcal{U}_i, r_i^k ) $ for $ \mathcal{U}_i$ and $ r_i^k$. %The response candidates $\mathcal{R}$  will be ranked according to these matching scores.
% \end{enumerate}
 
 We explore two different implementations under this learning framework as follows: 1) Incorporating external knowledge into deep matching networks via pseudo-relevance feedback (DMN-PRF). The architecture of DMN-PRF model is presented in Figure \ref{fig:dmn-prf}. 2) Incorporating external knowledge via QA correspondence knowledge distillation (DMN-KD). The architecture of DMN-KD model is presented in Figure \ref{fig:dmn-kd}. We will present the details of these two models in Section \ref{sec:method_dmn_prf} and Section \ref{sec:method_dmn_kd}.
 
 
 %\vspace{-0.3cm}
 \begin{figure*}[th]
 	\center
 	\includegraphics*[viewport=0mm 0mm 280mm 90mm, scale=0.50]{figures/DMN-PRF-Rexpand-V41.pdf}
 	%\includegraphics[width=6.8in, height=3.0in]{figures/blstm-cnn}\\
 	\vspace{-0.4cm}
 	\caption{The architecture of DMN-PRF model for conversation response ranking.}\label{fig:dmn-prf}
 	%\vspace{-0.4cm}
 \end{figure*}

%\ylcomment{Until 19:47 am 01/29/2018, 5th round of checking/updating to here.}

\subsection{Deep Matching Networks with Pseudo-Relevance Feedback}
\label{sec:method_dmn_prf}
% Present in a mathmatical way

%\ylcomment{Until 19:25 01/24/2018, double check and revise the paper/reduce the length to here.}
\subsubsection{\textbf{Relevant QA Posts Retrieval}} %\linebreak
We adopt different QA text collections for different conversation data (e.g. Stack Overflow data for MSDialog, AskUbuntu for UDC). The statistics of these external collections are shown in Table \ref{tab:ext_collection_statistics}. We download the data dumps for Stack Overflow and AskUbuntu from archive.org\footnote{\url{https://archive.org/download/stackexchange}}. We index the QA posts in Stack Overflow in most recent two years and all the QA posts in AskUbuntu. Then we use the response candidate $ r_i^k$ as the query to retrieve top $P$ \footnote{In our experiments, we set $P=10$.} QA posts with BM25 as the source for external knowledge. %  Then we use these indexes to perform QA posts retrieval in the next step for response candidate expansion.

\begin{table}
	\centering
	\footnotesize
	\caption{Statistics of external collections for QA pairs retrieval and knowledge extraction. Note that ``\#QWithAcceptedA'' means  ``number of questions with an accepted answer''. The other names use similar abbreviations.}
	\vspace{-0.1in}
	\label{tab:ext_collection_statistics}
	\begin{tabular}{l|l|l}
		\hline \hline
		Collection Name     & SOTwoYears & AskUbuntu \\ \hline  \hline
		StartDate           & 12/4/2015    & 7/28/2010   \\ \hline
		EndDate             & 9/1/2017     & 9/1/2017    \\ \hline
		\#QAPosts           & 9,563,530  & 629,198   \\ \hline
		\#Time              & 2 Years    & 7 years   \\ \hline
		XMLFileDiskSize     & 17GB       & 799MB     \\ \hline
		\#Question          & 4,188,937  & 271,233   \\ \hline
		%\#QWithTitle & 4,188,937  & 271,233   \\ \hline
		%\#QWithAcceptedA    & 1,751,787  & 92,259    \\ \hline
		%\#QWithAtLeastOneA  & 3,178,814  & 213,830   \\ \hline
		\%QWithAcceptedA    & 41.82\%    & 34.01\%   \\ \hline
		\%QWithAtLeastOneA  & 75.89\%    & 78.84\%   \\ \hline   \hline
		%\#Answer            & 5,362,237  & 357,965   \\ \hline  \hline
		%\#AWithTitle   & 296        & 0         \\ \hline \hline
	\end{tabular}
\end{table}
%\vspace{-0.1in}

%\ylcomment{Until 14:53pm, 3rd round of checking and revising to here.}
\subsubsection{\textbf{Candidate Response Expansion}} %\linebreak
\label{sec:response_expansion}
The motivation of Pseudo-Relevance Feedback (PRF) is to extract terms from the top-ranked documents in the first retrieval results to help discriminate relevant documents from irrelevant ones \cite{Cao:2008:SGE:1390334.1390377}. The expansion terms are extracted either according to the term distributions (e.g. extract the most frequent terms) or extracted from the most specific terms (e.g. extract terms with the maximal IDF weights) in feedback documents. Given the retrieved top QA posts $\mathcal{P}$ from the previous step, we compute a language model $\theta = P(w| \mathcal{P})$ using $\mathcal{P}$. Then we extract the most frequent $W$\footnote{In our experiments, we set $W = 10$. }  terms from $\theta$ as expansion terms for response candidate $r_i^k$ and append them at the end of $r_i^k$. For the query $r_i^k$, we perform several preprocessing steps including tokenization, punctuation removal and stop words removal.  QA posts in both Stack Overflow and AskUbuntu have two fields: ``Body'' and ``Title''. We choose to search the ``Body'' field since we found it more effective in experiments. 

% Only report the best setting of DMN-PRF, which is DMN-PRF-Body
%  \footnote{According to the statistics in Table \ref{tab:ext_collection_statistics}ï¼Œ all the question posts have ``Body'' and ``Title'' fields. Only a few answer posts in Stack Overflow have the ``Title'' field. }
% all the question posts have ``Body'' and ``Title'' fields. Only a few answer posts in Stack Overflow have the ``Title'' field. }, we generate two versions of retrieval results for candidate response expansion, which is corresponding to searching the ``Body'' or ``Title'' field of the QA posts. We choose to re
%We refer the models using these two different retrieval strategies as ``DMN-PRF-Body'' and ``DMN-PRF-Title'' respectively. We will compare the effectiveness of these two retrieval strategies in Section \ref{sec:exps}.

\subsubsection{\textbf{Interaction Matching Matrix}}
% Present in a mathmatical way
The expanded response candidates and dialog contexts will be modeled by a deep neural matching network. Given an expanded response $r_i^{k'}$ and an utterance $u_i^t$ in the context $\mathcal{U}_{i}$, the model firstly looks up a global embedding dictionary to represent  $r_i^{k'}$ and $u_i^t$ as two sequences of embedding vectors $\mathbf{E}(r_i^{k'})=[\mathbf{e}_{r,1}, \mathbf{e}_{r,2}, \cdots, \mathbf{e}_{r,l_r}]$ and  $\mathbf{E}(u_i^t)=[\mathbf{e}_{u,1}, \mathbf{e}_{u,2}, \cdots, \mathbf{e}_{u,l_u}]$, where $\mathbf{e}_{r,i} \in \mathbb{R}^d$, $\mathbf{e}_{u,i} \in \mathbb{R}^d$  are the embedding vectors of the $i$-th word in $r_i^{k'}$ and $u_i^t$ respectively. Given these two word embedding sequences, there are two different methods to learn matching patterns: representation focused methods and interaction focused methods \cite{Guo:2016:DRM:2983323.2983769}. Here we adopt the interaction focused methods due to their better performances over a number of text matching tasks \cite{DBLP:conf/nips/HuLLC14,DBLP:conf/aaai/PangLGXWC16,DBLP:conf/aaai/WanLGXPC16,Yang:2016:ARS:2983323.2983818}. Specifically, the model builds two interaction matrices with $\mathbf{E}(r_i^{k'}) \in \mathbb{R}^{d\times l_r}$ and $\mathbf{E}(u_i^t) \in \mathbb{R}^{d\times l_u}$: a word pairwise similarity matrix $\mathbf{M}_1$ and a sequence hidden representation similarity matrix $\mathbf{M}_2$.  $\mathbf{M}_1$ and $\mathbf{M}_2$ will be two input channels of a convolutional neural network (CNN) to learn important matching features, which will be aggregated by the final BiGRU layer and a multi-layer perceptron (MLP) to generate a matching score.
% \ylcomment{In the current implementation, these two matrices are the dot-product matrices between word embedding and sequence representations learned in BiGRU. I will try more variations like indicator, cosine, bi-linear or single direction GRU, etc later.}

Specifically, in the input channel one, $\forall i, j$, the element $m_{1,i,j}$ in the $\mathbf{M}_1$ is defined by $m_{1,i,j} = \mathbf{e}_{r,i}^T \cdot \mathbf{e}_{u,j}$.
%\begin{footnotesize}	
%\begin{equation} \label{Eqn:matrix1_element_ij}
%\end{equation}
%\end{footnotesize}
$\mathbf{M_1}$ models the word pairwise similarity between $r_i^{k'}$ and $u_i^t$ via the dot product similarity between the embedding representations.

For input channel two, we firstly employ bidirectional gated recurrent units (BiGRU) \cite{DBLP:journals/corr/ChungGCB14} to encode $r_i^{k'}$ and $u_i^t$ into two hidden representations. A BiGRU consists two GRUs that run in opposite directions on sequence $\mathbf{E}(r_i^{k'})$: a forward GRUs processing the sequence as it is ordered, and another backward GRUs processing the sequence in its reverse order. These two GRUs will generate two sequences of hidden states $(\vec{\mathbf{h}_1}, \cdots, \vec{\mathbf{h}_{l_r}})$ and $(\overset{{}_{\shortleftarrow}}{\mathbf{h}_1}, \cdots, \overset{{}_{\shortleftarrow}}{\mathbf{h}_{l_r}}  )$. BiGRU then concatenates the forward and the backward hidden states to form the final hidden vectors for $r_i^{k'}$ as $\mathbf{h}_i = {[\vec{\mathbf{h}_i}, \overset{{}_{\shortleftarrow}}{\mathbf{h}_i} ]}_{i=1}^{l_r}$. More specifically, $\forall i$, the hidden state vector $\vec{\mathbf{h}_i}\in \mathbb{R}^O$ is calculated by the following formulas:

% Similar to the LSTM unit \cite{Hochreiter:1997:LSM:1246443.1246450}, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate memory cells. It has similar performances with the LSTM unit on sequence modeling, but with lower computational costs. 
 \begin{footnotesize}
\begin{equation}\label{Eqn:gru}
\begin{aligned}
\mathbf{z}_i&=\sigma(\mathbf{W}_z\mathbf{e}_{r,i}+\mathbf{U}_z\vec{\mathbf{h}}_{i-1}+\mathbf{b}_z)\\
\mathbf{r}_i&=\sigma(\mathbf{W}_r\mathbf{e}_{r,i}+\mathbf{U}_r \vec{\mathbf{h}}_{i-1}+\mathbf{b}_r)\\
\tilde{\mathbf{h}}_i&=\tanh(\mathbf{W}_h\mathbf{e}_{r,i}+\mathbf{U}_h(\mathbf{r}_i\circ\vec{\mathbf{h}}_{i-1})+\mathbf{b}_h)\\
\vec{\mathbf{h}}_i&=(\mathbf{1}-\mathbf{z}_i)\circ\vec{\mathbf{h}}_{i-1}+\mathbf{z}_i\circ\tilde{\mathbf{h}}_i
\end{aligned}
\end{equation}
\end{footnotesize}
where $\mathbf{z}_i$ and $\mathbf{r}_i$ are an update gate and a reset gate respectively. $\textbf{e}_{r,i}, \vec{\textbf{h}}_i$ are the input and hidden state output of the network at time step $i$. $\mathbf{W}_z,\mathbf{W}_r,\mathbf{W}_h,\mathbf{U}_z,\mathbf{U}_r,\mathbf{U}_h$ and $\mathbf{b}_z,\mathbf{b}_r,\mathbf{b}_h $ are parameter matrices and bias vectors to be learned. The backward hidden state $ \overset{{}_{\shortleftarrow}}{\mathbf{h}_i} \in \mathbb{R}^O$ is computed in a similar way according to Equation \ref{Eqn:gru}. The hidden vectors for the dialog utterance $u_i^t$ can be obtained in the same procedure. Given the hidden vectors of $r_i^{k'}$ and $u_i^t$, we calculate element $m_{2,i,j}$ in the sequence hidden representation similarity matrix $\mathbf{M}_2$ by $m_{2,i,j} = \mathbf{h}_{r,i}^T \cdot \mathbf{h}_{u,j}$.
% \begin{footnotesize}
%\begin{eqnarray} \label{Eqn:matrix2_element_ij}
%\end{eqnarray}
%\end{footnotesize}
BiGRU models the neighbor context information around words from two directions and encode the text sequences into hidden vectors. Thus $\mathbf{M}_2$ matches $r_i^{k'}$ and $u_i^t$ with local sequence structures such as phrases or text segments. %\ylcomment{will update the the computation of $m_{2,i,j} $ as the bi-linear product and add one more parameter matrix here. Can also add one more non-linear transformation here.}
 
\subsubsection{\textbf{Convolution and Pooling Layers}}
% Present in a mathmatical way
The interaction matrices $\mathbf{M}_1$ and $\mathbf{M}_2$ are then fed into a CNN to learn high level matching patterns as features. CNN alternates convolution and max-pooling operations over these input channels. Let $\mathbf{z}^{(l,k)}$ denote the output feature map of the l-th layer and k-th kernel, the model will do convolution operations and max-pooling operations according to the following equations.
% in an interleaved way

\textbf{Convolution.} Let $r_w^{(l,k)} \times r_h^{(l,k)}$  denote the shape of the k-th convolution kernel in the $l$-th layer, the convolution operation can be defined as:
 \begin{footnotesize}
\begin{equation} \label{Eqn:2d_conv}
\begin{aligned}
\mathbf{z}_{i,j}^{(l+1, k)}=\sigma(\sum_{k'=0}^{K_l -1}\sum_{s=0}^{r_w^{(l,k)} - 1}\sum_{t=0}^{r_h^{(l,k)} - 1} \mathbf{w}_{s,t}^{(l+1,k)} \cdot z_{i+s, j+t}^{(l,k')} + b^{(l+1,k)}  )  \quad  \\ 
\forall l =0,2,4,6,\cdots,  
\end{aligned}
\end{equation}
\end{footnotesize}
where $\sigma$ is the activation function ReLU, and $\mathbf{w}_{s,t}^{(l+1,k)}$ and $b^{(l+1,k)}$ are the parameters of the $k$-th kernel on the $(l+1)$-th layer to be learned.  $K_l$ is the number of kernels on the $l$-th layer. 

\textbf{Max Pooling.} Let $p_w^{(l,k)} \times p_h^{(l,k)}$  denote the shape of the k-th pooling kernel in the $l$-th layer, the max pooling operation can be defined as:
 \begin{footnotesize}
\begin{equation} \label{Eqn:2d_max_pooling}
\begin{aligned}
\mathbf{z}_{i,j}^{(l+1,k)} = \max_{0\le s < p_w^{l+1, k}} \max_{0\le t < p_h^{l+1, k}} \mathbf{z}_{i+s,j+t}^{(l,k)} \quad \forall  l = 1,3,5,7, \cdots, 
\end{aligned}
\end{equation}
\end{footnotesize}
\subsubsection{\textbf{BiGRU Layer and MLP}}
% Present in a mathmatical way

%\ylcomment{Cite the ACL'17 paper on SMN and analyze the similarity and difference between DMN/DMN-PRF/DMN-KD with SMN.}

Given the output feature representation vectors learned by CNN for utterance-response pairs $(r_i^{k'}, u_i^t)$, we add another BiGRU layer to model the dependency and temporal relationship of utterances in the conversation according to Equation \ref{Eqn:gru} following the previous work \cite{DBLP:conf/acl/WuWXZL17}.  The output hidden states $\mathbf{H}_c = [\mathbf{h'}_1, \cdots, \mathbf{h'}_c]$ will be concatenated as a vector and fed into a multi-layer perceptron (MLP) to calculate the final matching score $f( \mathcal{U}_i, r_i^{k'} ) $  as
 \begin{footnotesize}
\begin{equation}
f( \mathcal{U}_i, r_i^{k'} )  = \sigma_2( \mathbf{w}_2^T \cdot \sigma_1 (\mathbf{w}_1^T\mathbf{H}_c + \mathbf{b}_1    ) + \mathbf{b}_2)
\end{equation}
\end{footnotesize}
where $\mathbf{w}_1, \mathbf{w}_2,\mathbf{b}_1,\mathbf{b}_2$ are model parameters. $\sigma_1$ and $\sigma_2$ are tanh and softmax functions respectively.

%\vspace{-0.3cm}
\begin{figure*}[th]
	\center
	\includegraphics*[viewport=0mm 0mm 310mm 95mm, scale=0.50]{figures/DMN-KD-CQA-V42.pdf}
	%\includegraphics[width=6.8in, height=3.0in]{figures/blstm-cnn}\\
	%\vspace{-0.4cm}
	\caption{The left figure shows the architecture of DMN-KD model for conversation response ranking. The input channel $\mathbf{M}_3$ denoted as blue matrices capture the correspondence matching patterns of utterance terms and response terms in relevant external QA pairs retrieved from $\mathcal{E}$. Note that we omit the details for CNN layers here to save spaces as they have been visualized in Figure \ref{fig:dmn-prf}. The right figure shows the detailed pipeline of external relevant QA pairs retrieval and QA correspondence matching knowledge distillation in DMN-KD model. }\label{fig:dmn-kd}
	\vspace{-0.4cm}
\end{figure*}

% We visualize the correspondence match matrices between $3$ dialog utterances and the response candidate $r_i^k$.

\subsubsection{\textbf{Model Training}}
% Present in a mathmatical way
For model training, we consider a pairwise ranking learning setting. The training data consists of triples $(\mathcal{U}_i, r_i^{k+}, r_i^{k-})$ where $r_i^{k+}$ and $r_i^{k-}$ denote the positive and the  negative response candidate  for dialog utterances $\mathcal{U}_i$. Let $\Theta$ denote all the parameters of our model. The pairwise ranking-based hinge loss function is defined as:
 \begin{footnotesize}
\begin{equation}
\mathcal{L}(\mathcal{D}, \mathcal{E};\Theta) = \sum_{i=1}^{I} \max(0, \epsilon - f( \mathcal{U}_i, r_i^{k+} )  +  f( \mathcal{U}_i, r_i^{k-} )  ) + \lambda ||\Theta||^2_2
\end{equation}
\end{footnotesize}
where $I$ is the total number of triples in the training data $\mathcal{D}$. $\lambda ||\Theta||^2_2$ is the regularization term where $\lambda$  denotes the regularization coefficient. $\epsilon$ denotes the margin in the hinge loss. The parameters of the deep matching network are optimized using back-propagation with \textit{Adam} algorithm~\cite{DBLP:journals/corr/KingmaB14}.  % For neural network regularization, we employ Dropout\cite{DBLP:journals/jmlr/SrivastavaHKSS14} in the model training process.

\subsection{Deep Matching Networks with QA Correspondence Knowledge Distillation}
\label{sec:method_dmn_kd}
% Present in a mathmatical way

 %\subsubsection{Knowledge Distillation from Relevant QA  Pairs}
 % Present in a mathmatical way
In addition to the DMN-PRF model presented in Section \ref{sec:method_dmn_prf}, we also propose another model for incorporating external knowledge into conversation response ranking via QA correspondence knowledge distillation, which is referred to as DMN-KD model in this paper. The architecture of DMN-KD model is presented in Figure \ref{fig:dmn-kd}. Compared with DMN-PRF, the main difference is that the CNN of DMN-KD will run on an additional input channel $\mathbf{M}_3$ denoted as blue matrices in Figure \ref{fig:dmn-kd}, which captures the correspondence matching patterns of utterance terms and response terms in relevant external QA pairs retrieved from $\mathcal{E}$. Specifically, we firstly use the response candidate $r_i^k$ as the query to retrieve a set of relevant QA pairs\footnote{Note that we want QA pairs here instead of question posts or answer posts, since we would like to extract QA term co-occurrence information with these QA pairs.}  $\mathcal{P}$. Suppose $\mathcal{P} = \{ \mathcal{Q}, \mathcal{A}\}  = \{(\mathbf{Q}_1, \mathbf{A}_1), (\mathbf{Q}_2, \mathbf{A}_2), \cdots, (\mathbf{Q}_P, \mathbf{A}_P) \}$, where $(\mathbf{Q}_p, \mathbf{A}_p)$ denotes the $p$-th QA pair. Given a response candidate $r_i^k$ and a dialog utterance $u_i^t$ in dialog $\mathcal{U}_i$, the model will compute the term co-occurrence information as the \textit{Positive Pointwise Mutual Information} (PPMI) of words of $r_i^k$ and  $u_i^t$ in retrieved QA pair set $\{ \mathcal{Q}, \mathcal{A}\} $. Let $[w_{r,1}, w_{r,2}, \cdots, w_{r, l_r}]$ and $[w_{u,1}, w_{u,2}, \cdots, w_{u, l_u}]$ denote the word sequence in $r_i^k$ and  $u_i^t$. We construct a QA term correspondence matching matrix $\mathbf{M}_3$ as the third input channel of CNN for $r_i^k$ and  $u_i^t$ with the PPMI statistics from $\{ \mathcal{Q}, \mathcal{A}\} $. More specifically, $\forall i, j$, the element $m_{3,i,j}$ in  $\mathbf{M}_3$ is computed as 

\vspace{-0.1in}
 \begin{footnotesize}
\begin{eqnarray}\label{Eqn:kd_co_occurrence_matrix}
m_{3,i,j} &=& PPMI(w_{r,i}, w_{u,j}|\{\mathcal{Q}, \mathcal{A}\})    \\
&=& \max(0, \log \frac{ \sum_{p'=1}^P p(w_{r,i} \in \mathbf{A}_{p'}, w_{u,j} \in \mathbf{Q}_{p'}|\mathbf{Q}_{p'}, \mathbf{A}_{p'})  }{p(w_{r,i}|\mathcal{A}) \cdot p(w_{u,j}|\mathcal{Q})}) \nonumber
\end{eqnarray}
\vspace{-0.1in}
\end{footnotesize}

%\begin{footnotesize}
%	\begin{eqnarray}
%	&&p(z_{c} = z, e_{c} = e|\mathbf{Z}_{\neg_{c}}, \mathbf{W}, \mathbf{E}_{\neg_{c}},\mathbf{V}, \mathbf{T}, \Theta) \nonumber \\
%	&\propto& \frac{p(\mathbf{Z}, \mathbf{W}, \mathbf{E},\mathbf{V}, \mathbf{T} | \Theta)}
%	{p(\mathbf{Z}_{\neg_{c}}, \mathbf{W}, \mathbf{E}_{\neg_{c}},\mathbf{V}, \mathbf{T} | \Theta)} \nonumber \\
%	&=& \frac{\Delta(C_{u}^\mathbf{k} + \alpha)}{\Delta(C_{u,\neg_{c}}^\mathbf{k} + \alpha)} \cdot
%	\frac{\Delta(C_{z}^\mathbf{w} + \gamma)}{\Delta(C_{z,\neg_{c}}^\mathbf{w} + \gamma)} \cdot
%	\frac{\Delta(C_{z}^\mathbf{t} + \eta)}{\Delta(C_{z,\neg_{c}}^\mathbf{t} + \eta)} \nonumber \\
%	&&   \cdot \frac{\Delta(C_{z,u}^\mathbf{e} + \beta)}{\Delta(C_{z,u,\neg_{c}}^\mathbf{e} + \beta)} \cdot \mathcal{N}(v_c | \mu_e, \Sigma_e)  \nonumber \\
%	&=& \frac{C_{u,\neg_c}^{z} + \alpha}{\sum_{k=1}^{K}C_{u,\neg_c}^{k} + K \alpha} \cdot
%	\frac{\prod_{w=1}^{V} \prod_{i=1}^{n_c^w} (C_{z,\neg_c}^{w} + \gamma + i - 1)}
%	{\prod_{j=1}^{n_c^{\mathbf{w}}} \sum_{w=1}^V (C_{z,\neg_c}^{w} + V\gamma + j - 1)} \nonumber \\
%	&&  \cdot \frac{\prod_{t=1}^{T} \prod_{p=1}^{n_c^t} (C_{z,\neg_c}^{t} + \eta + p - 1)}
%	{\prod_{q=1}^{n_c^{\mathbf{t}}} \sum_{t=1}^T (C_{z,\neg_c}^{t} + T\eta + q - 1)} \nonumber \\
%	&&  \cdot \frac{C_{z,u,\neg_c}^{e} + \beta}{\sum_{e=1}^{E}C_{z,u,\neg_c}^{e} + E \beta} \cdot
%	\mathcal{N}(v_c | \mu_e, \Sigma_e),  \label{eqn_gibbsRuleRes}
%	\end{eqnarray}
%\end{footnotesize}
where $w_{r,i}$ and $w_{u,j}$ denote the $i$-th word in the response candidate and $j$-th word in the dialog utterance.  The intuition is that the PPMI between $w_{r,i}$ and $w_{u,j}$ in the top retrieved relevant QA pair set $\{\mathcal{Q}, \mathcal{A}  \}$ could encode the correspondence matching patterns between $w_{r,i}$ and $w_{u,j}$ in external relevant QA pairs . Thus $\mathbf{M}_3$ is the extracted QA correspondence  knowledge from the external collection $\mathcal{E}$ for $r_i^k$ and $u_i^t$. These correspondence matching knowledge capture relationships such as \textit{``(Problem Descriptions, Solutions)'', ``(Symptoms, Causes)'', ``(Information Request, Answers)''}, etc. in  the top ranked relevant QA pair set $\{\mathcal{Q}, \mathcal{A}  \}$. They will help the model better discriminate a good response candidate from a bad response candidate given the dialog context utterances. To compute the co-occurrence count between $w_{r,i}$ and $w_{u,j}$, we count all word co-occurrences considering $\mathbf{A}_p$ and $\mathbf{Q}_p$ as bag-of-words as we found this setting is more effective in experiments.

% \subsubsection{Knowledge Distillation from Relevant Web documents}

%\subsubsection{QA Correspondence  Matching Representation}
% Present in a mathmatical way

%%\vspace{-0.3cm}
%\begin{figure}[th]
%	\center
%	\includegraphics*[viewport=0mm 0mm 340mm 120mm, scale=0.50]{figures/IR-KD-Pipeline.pdf}
%	%\includegraphics[width=6.8in, height=3.0in]{figures/blstm-cnn}\\
%	%\vspace{-0.4cm}
%	\caption{}\label{fig:ir_kd_pipeline}
%	%\vspace{-0.4cm}
%\end{figure}

%\ylcomment{\subsubsection{[Seperated] Incorporate User Intent as External Knowledge  in Conversation - (Put this part and motivations in another separate paper submission.)} Deep Matching Networks with User Intent Modeling}

%\begin{table}[h]
%	\centering
%	\caption{Notations and descriptions.}
%	\begin{scriptsize}
%		\begin{tabular}{ll}
%			\toprule
%			Notations & Descriptions \\
%			\midrule
%			$U$ & the total number of users  \\
%			$N_u$ & the total number of Q\&A posts of user $u$ \\
%			$L_{u,n}$ & the total number of words in $u$'s $n$-th post  \\
%			$P_{u,n}$ & the total number of tags in $u$'s $n$-th post  \\
%			$K$ & the total number of topics  \\
%			$E$ & the total number of expertise levels \\
%			$T$ & the total number of unique tags \\
%			$V$ & the total number of unique words \\
%			%$V_v$ & the total number of unique votes \\
%			$\mu$ & mean of Gaussian distribution \\
%			$\Sigma$ & precision of Gaussian distribution \\
%			$w, t, v, e, z$ & label for word, tag, vote, expertise, topic \\
%			$\mathbf{W}, \mathbf{T}, \mathbf{V}, \mathbf{E}, \mathbf{Z}$ & vector for words, tags, votes, expertise, topics \\
%			\midrule
%			$\theta_u$ & user specific topic distribution \\
%			$\mathcal{N}(\mu_e,\Sigma_e)$ & expertise specific vote distribution \\
%			$\psi_k$ & topic specific tag distribution \\
%			$\varphi_{e}$ & topic specific word distribution \\
%			$\phi_{k,u}$ & user topical expertise distribution \\
%			$\alpha, \beta, \eta, \gamma$ & Dirichlet priors \\
%			$\alpha_0, \beta_0, \mu_0, \kappa_0$ & Normal-Gamma parameters \\
%			$\mathcal{NG}(\alpha_0, \beta_0, \mu_0, \kappa_0)$ & Normal-Gamma distribution \\
%			\bottomrule
%		\end{tabular}
%	\end{scriptsize}
%	\label{tab:notion}
%\end{table}

















































