\section{Introduction}
\label{sec:intro}

% Present in a logical way

%1. What is the task and Why it is important
%The recent boom of artificial intelligence has witnessed the

%The fast development of artificial intelligence contributes to the emerging and flourishing of many intelligent personal assistant systems, including Amazon Alexa, Apple Siri, Alibaba AliMe, Microsoft Cortana and Google Now, among wider population around the world. 

Personal assistant systems, such as Apple Siri, Google Now, Amazon Alexa, and Microsoft Cortana, are becoming ever more widely used\footnote{For example, over 100M installations of Google Now (Google, \url{http://bit.ly/1wTckVs}); 15M sales of Amazon Echo (GeekWire, \url{http://bit.ly/2xfZAgX}); more than 141M monthly users of Microsoft Cortana (Windowscentral, \url{http://bit.ly/2Dv6TVT}).}. These systems, with either text-based or voice-based conversational interfaces, are capable of voice interaction, information search, question answering and voice control of smart devices. This trend has led to an interest in developing conversational search systems, where users would be able to ask questions to seek information with conversation interactions. Research on speech and text-based conversational search has also recently attracted significant attention in the information retrieval (IR) community.

%Both speech and text based conversational search systems are emerging topics among the information retrieval (IR) community in recent years\footnote{\url{https://sites.google.com/view/cair-ws/}}\footnote{ \url{http://scai.info/}}, attracting enormous attention of researchers from both academia and industry. 

%2. What is the problems/missing parts of current state-of-art work

% Review current main research on generation based and retreival based methods for conversation system. Retreivel based methods have the advantages of ...
% For retreivel based methods, most current methods are on single-turn conversation, e.g. Wang et al. 2013

Existing approaches to building conversational systems include generation-based methods \cite{DBLP:conf/emnlp/RitterCD11,DBLP:conf/acl/ShangLL15} and retrieval-based methods \cite{DBLP:journals/corr/JiLL14,DBLP:conf/sigir/YanSW16,DBLP:conf/cikm/YanSZW16,DBLP:conf/sigir/YanZE17}. Compared with generation-based methods, retrieval-based methods have the advantages of returning fluent and informative responses. Most work on retrieval-based conversational systems studies response ranking for \textit{single-turn conversation} \cite{DBLP:conf/emnlp/WangLLC13}, which only considers a current utterance for selecting responses. Recently, several researchers have been studying \textit{multi-turn conversation} \cite{DBLP:conf/sigir/YanSW16,DBLP:conf/emnlp/ZhouDWZYTLY16,DBLP:conf/acl/WuWXZL17,DBLP:conf/sigir/YanZE17}, which considers the previous utterances of the current message as the conversation context to select responses by jointly modeling context information, current input utterance and response candidates. However, existing studies are still suffering from the following weaknesses: 
%  Due to the rapid growth of user generated conversation data on social platforms ( e.g., Twitter, Facebook, Weibo) and community question answering sites (e.g. Quora, Stack Overflow, Yahoo Answers), it is become increasingly practical as the increased amount of data makes it more likely that a suitable response will be found.  
%  although achieved great progress on building human computer conversation systems, 
% \yfcomment{We may be claiming too much for retrieval-based approaches, this may annoy a reviewer if he happens to work on generation-based approaches}.

% Most recently, some researchers studied multi-turn conversation. e.g, Yan et al. DL2R, Tian et al. Zhou et al. Multi-View, Wu et al. SMN.  However, the current research and methods in this area have a few weakness:
% 2.1  Most current works on multi-turn conversation are mainly looking at open domain chit-chat conversations
% 2.2 Most currrent open domain conversation models are merely model the matching patterns between the conversational context with response candidates ignoring external knowledge beyond the dialog. Whether external knowledge is useful for conversatioin resonponse ranking and how to do this effectively have not been well studied.

%\begin{enumerate}
(1) \textbf{Most existing studies are on open domain chit-chat conversations or task / transaction oriented conversations}. Most current work \cite{DBLP:conf/emnlp/RitterCD11,DBLP:conf/acl/ShangLL15, DBLP:journals/corr/JiLL14,DBLP:conf/sigir/YanSW16,DBLP:conf/cikm/YanSZW16,DBLP:conf/sigir/YanZE17} is looking at open domain chit-chat conversations as in microblog data like Twitter and Weibo. There is some research on task oriented conversations \cite{Young:2010:HIS:1621140.1621240,wen2016network,bordes2017learning}, where there is a clear goal to be achieved through conversations between the human and the agent. However, the typical applications and data are related to completing transactions like ordering a restaurant or booking a flight ticket. Much less attention has been paid to \textit{information oriented conversations}, which is referred to as \textit{information-seeking conversations} in this paper. Information-seeking conversations, where the agent is trying to satisfy the information needs of the user through conversation interactions, are closely related to conversational search systems. More research is needed on response selection in information-seeking conversation systems. 
%\ylcomment{will try to give a clearer definition of ``information-seeking conversations''}%\yfcomment{Are the concepts `transaction oriented conversations', `information oriented conversations', and `information-seeking conversations' already existing concepts or newly created concepts in this paper? If already existing, better give reference, otherwise, we should give a clearer definition. Besides, `information oriented conversations' and `information-seeking conversations' are repetitions of the same concept, usually a principle is to define as few terminologies as possible and be consistent throughout the text.}

(2) \textbf{Lack of modeling external knowledge beyond the dialog utterances}. Most research on response selection in conversation systems are purely modeling the matching patterns between user input message (either with context or not) and response candidates, which ignores external knowledge beyond the dialog utterances. Similar to Web search, information-seeking conversations could be associated with massive external data collections that contain rich knowledge that could be useful for response selection. This is especially critical for information-seeking conversations, since there may be not enough signals in the current dialog context and candidate responses to discriminate a good response from a bad one due to the wide range of topics for user information needs. An obvious research question is how to utilize external knowledge effectively for response ranking. This question has not been well studied, despite the potential benefits for the development of information-seeking conversation systems. 

%\yfcomment{For (1) and (2), I think the discussion of chit-chat systems can be moved to related work; after introducing multi-turn conversation in the second paragraph, we can only briefly introduce chit-chat and use it to introduce the concept of information-seeking conversation, and then quickly go to the key of the paper -- using external knowledge for info-seeking conversation.}

%\end{enumerate}

%3.What is your contribution/findings/ the novel parts of the model or archtecture/  summary of experiment results

To address these research issues, we propose a learning framework on top of deep neural matching networks that leverages external knowledge for response ranking in information-seeking conversation systems. We study two different methods on integrating external knowledge into deep neural matching networks as follows: %To this end, we created a new data set MSDialog, which contains more than $35,000$ technical support dialogs on Microsoft products from the Microsoft answers Community\footnote{\url{answers.microsoft.com}}. We perform fine-grained user intent annotation and analysis with MSDialog and another popular benchmark data Ubuntu Dialog Corpus (UDC) \cite{DBLP:journals/corr/LowePSP15}, which consists of one million two-person technical support dialogs. The data analysis results show that most utterances from these two data sets are on questions, potential answers, information requests and user feedback, which represent typical content in information-seeking conversations. With these two conversation data, we build learning models for response ranking.
%Unlike many previous studies on microblog conversation data, we are looking at effective methods for response selection in information-seeking conversations.

% \yfcomment{We do not have to talk about so much details of data preparation in introduction, just briefly introduce the data with a few sentences, and leave the details to Section 3.}
%  (\ylcomment{add percentage here later}) 
% 3.1 Focuse on information-seeking conversations, which is different from previous works on chit-chat Twitter data
% In this paper, we are looking at information-seeking conversations, which is targeting on satisifying user information needs with conversation interactions. 
% To this end, we create a data set MSDialog...

% 3.2 Incorporate external knowledge in neural models to enchance response ranking performance
% This is especially critical for information-seeking converstaions since ... To show this, we give an example dialog in Table ...

% There are serveral chanllenges on incorporating external knowledge into response ranking in conversational system. e.g. (naive solutions won't work) it is very challenge to directly extract responses from external collections. For example,...  We did the following to overcome these chanllenges

%\begin{itemize} \end{itemize}
(1) \textbf{Incorporating external knowledge via pseudo-relevance feedback}. Pseudo-relevance feedback (PRF) has been proven effective in improving the performance of many retrieval models \cite{Lavrenko:2001:RBL:383952.383972,Lv:2009:CSM:1645953.1646259,Zamani:2016:PFB:2983323.2983844,Zhai:2001:MFL:502585.502654,rocchio71relevance,Cao:2008:SGE:1390334.1390377}. 
The motivation of PRF is to assume a certain number of top-ranked documents from the initial retrieval run to be relevant and use these feedback documents to improve the original query representation. For conversation response ranking, many candidate responses are much shorter compared with conversation context,  which could have negative impacts on deep neural matching models. Inspired by the key idea of PRF, we propose using the candidate response as a query to run a retrieval round on a large external collection. Then we extract useful information from the (pseudo) relevant feedback documents to enrich the original candidate response representation. 

%Important terms will be extracted to expand the candidate response and then the deep matching networks match the conversation context with expanded candidate responses. 
%  In Web search, users often issue very short queries, which lacks important terms to better represent user information seeking intent, leading to poor retrieval results. PRF is an effective technique to get better query representations. 

(2) \textbf{Incorporating external knowledge via QA correspondence knowledge distillation}. Previous neural ranking models enhanced the performance of retrieval models such as BM25 and QL, which mainly rely on lexical match information, via modeling semantic match patterns in text \cite{Guo:2016:DRM:2983323.2983769,DBLP:conf/cikm/HuangHGDAH13,Mitra:2017:LMU:3038912.3052579}. For response ranking in information-seeking conversations, the match patterns between candidate responses and conversation context can be quite different from the well studied lexical and semantic matching. Consider the following sample utterance and response from the conversations in the Microsoft Answers community \footnote{\url{https://answers.microsoft.com/}} shown in Table \ref{tab:qapost_example}. A Windows user proposed a question about the windows update failure on ``restart install''. An expert replied with a response pointing to a potential cause ``Norton leftovers''. The match signals between the problem ``restart install'' and the cause ``Norton leftovers'' may not be captured by simple lexical and semantic matching. To derive such match patterns, we need to rely on external knowledge to distill QA correspondence information. We propose to extract the ``correspondence'' regularities between question and answer terms from retrieved external QA pairs. We define this type of match patterns as a ``\textit{correspondence match}'', which will be incorporated into deep matching networks as external knowledge to help response selection in information-seeking conversations. 

%\yfcomment{Also, because the introduction is already very long, we'd better shorten (1) and (2) here, only introduce the key idea, and leave the details to the main text.}

% https://answers.microsoft.com/en-us/windows/forum/windows_10-update-winpc/windows-update-failure/b3ac8dbf-4ad1-4431-b31d-e67185f91851

\begin{table}
	\scriptsize
	%\fontfamily{ppl}\selectfont
	\begin{ppl}
		\caption{Sample utterance and response from the conversations in the Microsoft Answers community. This figure could be more readable with color print. Note that the purpose of this figure is to illustrate examples and differences among these three types of matches instead of exhaustively labeling all three types of matches between the two texts.}
		\vspace{-0.1in}
		 %\ylcomment{Will update this table later to highlight the QA term co-occurrences in the retrieved QA pairs from external collection.  Then link this match information back to the words in utterances/responses in the dialog
		\begin{tabular}{|p{8.0cm}|}
			\hline
			\multicolumn{1}{|l|}{\emph{QA Dialog Title: }: \textbf{Windows Update Failure }}\\
			\multicolumn{1}{|l|}{\emph{Dialog Tags}: \textbf{Windows, Windows 10, Windows update, recovery, backup, PC}}\\
		    \underline{USER:} I have \textcolor{blue}{Windows10}, version 1511, OS Build 10586.1106. For the past year I have tried to \textcolor{magenta}{upgrade} from this without success. \textcolor{magenta}{Upgrade} download OK but on installing only get to 85 - 93\% and then on \textcolor{red}{restart install} previous version of \textcolor{blue}{windows} (the 1511 version), I have  \textcolor{blue}{Windows} update assistant installed. Any help or advice on this would be most welcome. \\
			David\\
			\hline
			\multicolumn{1}{|l|}{\emph{Responses}} %& \multicolumn{1}{c|}{Votes~(\textcolor{blue})}\\
			\\
			\hline
			\underline{AGENT: James (Microsoft MVP - Windows Client) }:\newline
			\textbf{Response}:There's not a doubt in my mind that those \textcolor{red}{Norton ``leftovers''} is your troublemaker here - but now that the \textcolor{red}{Norton Removal Tool} has been deprecated and especially since the new-fangled \textcolor{red}{Norton Remove and Reinstall tool} doesn't get rid of the \textcolor{red}{leftovers}, a manual \textcolor{magenta}{upgrade} or a clean install of \textcolor{blue}{Microsoft Win10} appears to be your only possible resolution here. Feel free to give \textcolor{red}{Norton/Symantec} a piece of your mind! \\
			\hline
			Term Match: \textcolor{magenta}{Magenta} \qquad  Semantic Match:  \textcolor{blue}{Blue} \qquad    Correspondence Match:  \textcolor{red}{Red}\\
			\hline
		\end{tabular} \label{tab:qapost_example}
	\end{ppl}
	\vspace{-0.3cm}
\end{table}

We conduct extensive experiments with three information-seeking conversation data sets: the MSDialog data which contains crawled customer service dialogs from Microsoft Answers community , a popular benchmark data Ubuntu Dialog Corpus (UDC) \cite{DBLP:journals/corr/LowePSP15}, and another commercial customer service data AliMe from Alibaba group. We compare our methods with various deep text matching models and the state-of-the-art baseline on response selection in multi-turn conversations. Our methods outperform all baseline methods regrading a variety of metrics. % for response ranking in information-seeking conversations.

To sum up, our contributions can be summarized as follows:
% The novel properties of the proposed model and our contribution can be summerized as follows:

%\begin{enumerate}
 (1) \textbf{Focusing on information-seeking conversations and building a new benchmark data set.} We target information-seeking conversations to push the boundaries of conversational search models. To this end, we create a new information-seeking conversation data set MSDialog on technical support dialogs of Microsoft products and released it to the research community \footnote{The MSDialog dataset can be downloaded from ~\url{https://ciir.cs.umass.edu/downloads/msdialog}.  We also released our source code at ~\url{https://github.com/yangliuy/NeuralResponseRanking} .}.

 (2) \textbf{Integrating external knowledge into deep neural matching networks for response ranking.}  We propose a new response ranking paradigm for multi-turn conversations by incorporating external knowledge into the matching process of dialog context and candidate responses. Under this paradigm, we design two different methods with pseudo relevance feedback and QA correspondence knowledge distillation to integrate external knowledge into deep neural matching networks for response ranking.
	
(3) \textbf{Extensive experimental evaluation on benchmark / commercial data sets and promising results.} Experimental results with three different information-seeking conversation data sets show that our methods outperform various baseline methods including the state-of-the-art method on response selection in multi-turn conversations. We also perform analysis over different response types, model variations and ranking examples to provide insights.
%\end{enumerate}

%Our contributions can be summarized as follows:
%%\vspace{-1.5cm}
%% Highlight contributions/ what we have done and the differences between our work with previous works

%4.Roadmap: the structure of the whole paper
%\textbf{Roadmap}. 
%The rest of our paper is organized as follows. We will review related work in ...
