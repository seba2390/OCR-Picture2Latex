\section{Existence of An Optimal Strategy and Suboptimal Algorithm for A Finite Game}
\label{sec:algorithm_finite}

Based on the game formulation, in this section we discuss the existence of an optimal solution for the finite form of the hybrid stochastic game, and present an algorithm to compute a suboptimal system strategy.%\PA{what do you mean by ``approximation computing algorithm''}
 
\subsection{Existence of the System's Optimal Strategy}

%\iffalse 
We define the concatenation of strategies for $K$-stage game of each player ($\mathbf{f}$ for attacker and $\mathbf{g}$ for system) as $\mathbf{f}=\mathbf{f}_1\cdots \mathbf{f}_K,\quad\mathbf{f}_{k} \in \mathbf{F}_{k},\quad\mathbf{f}\in \mathbf{F},$
$\mathbf{g}=\mathbf{g}_{1}\cdots\mathbf{g}_K,\quad\mathbf{g}_{k} \in \mathbf{G}_{k},\quad\mathbf{g}\in\mathbf{G}$,
$k=1, 2,\dots,K$.
\begin{defn}
Let the random variable $\zeta_{k}$ describe the discrete state of the hybrid game at stage $k$, we define the conditional expected total payoff till $\tilde{K}$ for any $\mathbf{f},\mathbf{g}$ as%\begin{align*}
\\$\text{ }\quad R_{\tilde{K}}(s, \mathbf{f}, \mathbf{g})$
\\\centerline{$
=\sum \limits^{\tilde{K}}_{k=1}\sum_{l=1}^{3} p(\zeta_{k}=\delta_{l}|\zeta_{1} = s)[\mathbf{f}_{k}(s_{kl})]^{T}\tilde{r}_{k}(s_{kl})\mathbf{g}_{k}(s_{kl}),
$}
where $p(\zeta_{k}=\delta_{l}|\zeta_{1} = s)$ is the probability that the discrete state of the hybrid game is $\delta_{l}$ at stage $k$ given its initial discrete state $\zeta_{1}=s$.
\label{R_K}
\end{defn}
Since the immediate payoff of each stage satisfies that $0 \leq \tilde{r}_{k}^{ij}(s_{kl})< \infty,\ \text{for all}\ k,i,j,$
we have that $R_{\tilde{K}}(s, \mathbf{f}, \mathbf{g})$ is a nonnegative real-valued, nondecreasing function with $\tilde{K}$. 
%\PA{when you cut an old sentence to save space, make sure that the new one makes sense; for example you should have said - from the immediate payoff definition we have that ...' or ``since...''}  
Furthermore, for finite $K$%a finite stage game
%\PA{shouldn't it be ``for a finite game'' or ``for a finite stage game''}
%\vspace{-4pt}
\begin{align}
%\sup_{\mathbf{f} \in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}^{*}) < \infty,s \in S.
%\sup_{\mathbf{f}}
R_{K}(s,\mathbf{f}, \mathbf{g}) < \infty, \forall s \in S, \mathbf{f}\in \mathbf{F}, \mathbf{g} \in \mathbf{G}.
\label{fR}
\end{align}
Similarly as the definition of value and optimal strategy for a zero-sum, finite discrete state, finite stage stochastic game, we define the value and optimal strategy for the hybrid state stochastic game defined in this work as the following.
\begin{defn}
A two-person zero-sum $K$-stage stochastic game is said to have a value vector $v^{*}_{K}$ if $v^{*}_{K,s}=\underbar{v}_{K,s}=\bar{v}_{K,s},$ for any initial cyber state $s\in S$, where
\\\centerline{$
\underbar{v}_{K,s}= \sup_{\mathbf{f}\in\mathbf{F}}\inf_{\mathbf{g}\in\mathbf{G}}R_{K}(s,\mathbf{f}, \mathbf{g}),
$}
\centerline{$
\bar{v}_{K,s}=\inf_{\mathbf{g}\in\mathbf{G}}\sup_{\mathbf{f}\in\mathbf{F}}R_{K}(s,\mathbf{f},\mathbf{g}).
$}
For the finite value $K$-stage stochastic game, strategies $\mathbf{g}^{*}$ and $\mathbf{f}^{*}$ are called optimal at the saddle-point equilibrium for player two (the system) and player one (the attacker), respectively, if for all $s\in S$,
\centerline{$
v^*_{K,s} = \inf\limits_{\mathbf{g}\in \mathbf{G}}R_{K}(s, \mathbf{f}^{*}, \mathbf{g}),\quad v^*_{K,s} = \sup\limits_{\mathbf{f}\in \mathbf{F}}R_{K}(s, \mathbf{f}, \mathbf{g}^{*}).
$}
\end{defn}
%The existence conditions of the value and optimal strategies for a general finite horizon, finite state, zero-sum stochastic game are. 
The game defined in this paper has finite action spaces, finite strategy space, finite discrete cyber modes and satisfies~\eqref{fR} with bounded total payoff in finite horizon. Therefore, there exists the value of the considered  game and an saddle-point equilibrium or optimal strategy for the system shown in~\cite{dgt_Basar}.
\subsection{Suboptimal algorithm for the finite game}
Existing value iterative algorithms or dynamic programming algorithms for finite stochastic games cannot be used to solve the finite hybrid stochastic game defined in this work, since the discrete time dynamics $x_{[k-T,k]}$ of the game at stage $k$ depends on that of the stage $k-1$, which is only available in the future algorithm iterations. Hence, we design a suboptimal algorithm based on the value iteration method for a finite horizon, finite discrete state stochastic game~(\cite{plangame}) and robust game techniques~(\cite{RGT}). The value iteration algorithm for a finite horizon, discrete state stochastic game (with fixed payoff $r$ and state transition probability $P$ at every stage) works in the way that if a player knew how to play in the game optimally from the next stage on, then, at the current stage, he would play with such strategies. The value of $K$-stage game is finally provided by the last step of iteration. 



For a multi-stage game, to calculate the game value, we define the auxiliary matrix at stage $k$ for every cyber state $\delta_l$ with system dynamics $x_{[k-T,k]}$ as $Q(s_{kl}) \in \mathbb{Q}_{k} \subset \mathbb{R}^{M \times N}$, and each element of $Q(s_{kl})$ for action pair $(a_{ik}, u_{jk})$ is defined as
\begin{align}
\begin{split}
&Q^{ij}(s_{kl})\\
=&r^{ij}(s_{kl})
                         +\sum_{\delta_h\in S} {P}^{ij}(s_{(k+1)h} |s_{kl})\cdot {v}_{k+1}(s_{(k+1)h}), 
\end{split}
\label{Q_k}
\end{align} 
where ${v}_{k+1}(s_{(k+1)h})$ is the game value from stage $k+1$, state $s_{(k+1)h}$ (with cyber mode $\delta_h$) to the final stage $K$. For the final stage $K$, we define $Q(s_{Kl})=r(s_{Kl})$. We define a one-shot game at stage $k$ as a finite action space, zero-sum game between the system and the attacker with payoff matrix $Q(s_{kl})$, i.e., $Q^{ij}(s_{kl})$ is the payoff for action pair $(a_{ik},u_{jk})$ of stage $k$. In each one-shot game, the system only consider a strategy $f_k(s_{kl})$ to minimize the worst case payoff caused by the attacker according to matrix $Q(s_{kl})$. Here $Q(s_{kl})$ is defined based on the the system dynamics and the state transition probability provided by the detector. An alternative algorithm with unknown transition matrix or payoffs will be our future work.

Similarly as the value iteration algorithm for a discrete state stochastic game~(\cite{plangame}), Algorithm~\ref{finite} of the finite hybrid state stochastic game starts from the last stage, then gets the optimal one-stage strategy and the upper bound of game value at each stage. By calculating values of all stages until backwards to the first stage, Algorithm~\ref{finite} returns an upper bound for the value of the total payoff in $K$-stages. 

To estimate the values at each step, we consider the immediate payoff $r(s_{kl})$, the state transition probability ${P}(s_{(k+1)h}|s_{kl})$ and the game value estimated at the previous step uncertain parameters for the one shot robust game (\cite{RGT}). Then approximate each iteration value as the value of the robust one shot zero sum game. 
Algorithm~\ref{finite} provides an upper bound for the game value and the corresponding suboptimal strategy for the system. The idea is to solve a robust game at each iteration step -- i.e., minimize the worst-case caused by extreme points of the set of auxiliary matrix $\mathbb{Q}_k$ defined for all possible dynamics $x_{[k-T,k]}$. 


To quantify the boundary of the set of auxiliary matrix $\mathbb{Q}_k$ %($r_{k}$, $\mathbb{P}_{k}$), 
we need the expected values of system dynamics $\mathbf{x}_{k}, \mathbf{u}_{k}$, $\mathbf{y}_{k}, k=1,\cdots,K$ defined in equations~\eqref{dynamicgame}, which is determined by the strategies from stage $1$ till stage $k$. 
We first analyze the uncertain sets of the immediate payoff function at stage $k$, and the extreme points for the uncertain set $\mathbb{Q}_k$ depend on pure strategies . Let $\mathbf{f}^p_{k-1}$, $\mathbf{g}^p_{k-1}$ be the concatenation of previous pure strategies of the attacker and the system till stage $k \geqslant 2$, respectively, where
\\\centerline{
$\mathbf{f}^{p}_{k-1}=\mathbf{f}^{p}_{1}\cdots\mathbf{f}^{p}_{k-1},\quad  \mathbf{g}^{p}_{k-1}=\mathbf{g}^{p}_{1}\cdots\mathbf{g}^{p}_{k-1}$
}
satisfies that all $\mathbf{f}^{p}_{t}(s)$ ($\mathbf{g}^{p}_{t}(s)$) for $t=1,2,\dots, k$ have only one non-zero element, i.e., the player chooses the corresponding action or the \emph{pure} strategy.

Define a pure strategy auxiliary matrix $Q^p(s_{kl}) \in \mathbb{Q}^p_{k}$ as:
\begin{align}
\begin{split}
%\vspace{-5pt}
&Q^{p}(s_{kl}) \\
=&r^{p}(s_{kl})+\sum_{\delta_h\in S} {P}^{p} ( s_{(k+1)h} |s_{kl})\cdot \bar{v}^p_{k+1}(s_{(k+1)h}),
\label{eq:Q}
\end{split}
\end{align}
%\normalsize
for stages $k=1,\dots,K-1$, and for the final stage $k=K$,
\begin{align}
Q^{p}(s_{Kl})=r^{p}(s_{Kl}).
\label{Q_p}
\end{align}
For each stage $k$, $\bar{v}^p_{k}(s_{kl})$ is defined as
% and relates to matrix games defined by $\mathbb{Q}_{(k+1)p}$.
\begin{align}
\bar{v}_{k}^{p}(s_{kl})=\max_{Q^{p}(s_{kl})\in \mathbb{Q}^p_{k}}v^*[Q^{p}(s_{kl})],%v_{k}^{s_{l}}(h_{k}^{p})&=\max_{(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(h^{p}_{k},s_{l}))}
%v^*[r^{p}_{k}(h^{k}_{p},s_{l})+\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v_{k+1}^{s'}(h^{p}_{k+1})],
\label{pickv}
\end{align}
where $v^*$ is the function that yields the value of a zero-sum matrix game. Then the value $\bar{v}^p_{k+1}(s_{(k+1)h}) \geq 0$ to calculate the auxiliary matrix~\ref{eq:Q} is the upper bound of robust game value from stage $k+1$ till stage $K$, resulting from the iteration at stage $k+1$. This value iteration process is the key idea of the following~Algorithm~\ref{finite}. 

\begin{alg}
%\vspace{-5pt}
\textbf{: Suboptimal Algorithm for A Finite Hybrid Stochastic Game}\\
\textbf{Input}: System model parameters and game parameters.
\\\textbf{Initialization}:
             Compute the set of $\mathbb{Q}^p_k$ for every stage $k=T,\dots,T+ K$ given $\hat{\mathbf{x}}_{[0,T]}$;
              %For all $s_{l} \in S, l =1,2,3, h^{p}_{K}\in H^{p}_{K} :$ get %$4^{K-1}$ 
              %backup matrix set $\{Q_{k}^{p}(h_{k}^{p},s_{l})\}$:
             get the robust game value and corresponding strategies at stage $K$: $Q^{p}(s_{(K+T)l})=r^{p}(s_{(K+T)l})$,
                    $f^{*}(s_{(K+T)l}), g^{*}(s_{(K+T)l}), \bar{v}_{K+T}^{p}(s_{(K+T)l}) \leftarrow \pi(Q^{p}(s_{(K+T)l})).$
\\\textbf{Iteration}: For $k=(K+T-1), \cdots, T$, obtain a set of auxiliary matrices $\mathbb{Q}_{k}^{p}$ for all $\mathbf{f}^{p}_{k}$, $\mathbf{g}^p_k$, where each matrix is defined in~\eqref{eq:Q}, then calculate:
%\[Q_{k}^{p}(h^{p}_{k},s_{l}) = r^{p}_{k}(h^{p}_{k},s_{l})+\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v_{k+1}^{s'}(h^{p}_{k+1}),\]

 $f^{*}(s_{kl}), g^{*}(s_{kl}),\bar{v}_{k}^{p}(s_{kl}) \leftarrow \pi(Q^{p}(s_{kl}))$.
 
%$\mathbf{f}^*_k=[\mathbf{f}^{*}(h^{p}_{k}, s_{l}),\mathbf{f}^{*}(h^{p}_{k}, s_{2}), \mathbf{f}^{*}(h^{p}_{k}, s_{3})],$ $ \mathbf{g}^{*}_k=[\mathbf{g}(h^{p}_{k}, s_{1}),\mathbf{g}(h^{p}_{k}, s_{2}),\mathbf{g}(h^{p}_{k}, s_{3})].$ 

$\mathbf{f}^*_k=[\mathbf{f}^{*}(s_{kl}),l=1,2,3],$ $ \mathbf{g}^{*}_k=[\mathbf{g}^*(s_{kl}), l=1,2,3].$ 

\textbf{Return}:strategies $\mathbf{f}_{a}=\mathbf{f}^{*}_T\cdots\mathbf{f}^*_{K+T},$ $\mathbf{g}_{a}=\mathbf{g}^{*}_T\cdots\mathbf{g}^*_{K+T}$ and the value upper bound $\bar{v}_1^p(s_{1l}),l=1,2,3$.
\label{finite}
\end{alg}

Now consider the iteration for calculating $\bar{v}^p_{k}(s_{kl})$ from all matrix games $Q^{p}(s_{kl})\in \mathbb{Q}_{k}^p$ applying Algorithm~\ref{finite}. We define any strategy concatenations to stage $k-1$ with at most one non-pure strategy at stage $(k-1)$ as
\begin{align}
\begin{split}
\mathbf{f}^{np}_{k-1}&=\mathbf{f}^p_{k-2}\mathbf{f}_{k-1},\quad \mathbf{f}_{k-1} \in \mathbf{F}_{k-1},\\ \mathbf{g}^{np}_{k-1}&=\mathbf{g}^p_{k-2}\mathbf{g}_{k-1},\quad \mathbf{g}_{k-1} \in \mathbf{G}_{k-1},
\end{split}
\label{np_strategy}
\end{align}
where $\mathbf{f}^p_{k-2},\mathbf{g}^p_{k-2}$ are concatenations of pure strategies to stage $(k-1)$. We denote the corresponding auxiliary matrix as $\tilde{Q}(s_{kl}) \in \tilde{\mathbb{Q}}_k$ for cyber state $\delta_l$, the one shot game value based on payoff matrix $\tilde{Q}(s_{kl})$ as $\tilde{v}_{k}(s_{kl})$, i.e.,
\begin{align}
\begin{split}
&\tilde{Q}(s_{kl}) \\
=&\tilde{r}(s_{kl})+\sum_{\delta_h\in S} \tilde{P}( s_{(k+1)h} |s_{kl})\cdot \tilde{v}_{k+1}(s_{(k+1)h}).
\end{split}
\label{tilde_Q}
\end{align}
Here each possible hybrid state $s_{kl}$ for time instant $k$ is calculated from a none pure strategy defined as~\eqref{np_strategy}. Similarly, the value is defined as
\begin{align}
\tilde{v}_{k}(s_{kl})=\max_{\tilde{Q}(s_{kl})\in \mathbb{\tilde{Q}}_k}v^*[\tilde{Q}(s_{kl})].
\label{tilde_v}
\end{align}
The following theorem shows that at every stage $k$, $\bar{v}_{k}^{p}(s_{kl})$ is greater than or equal to $\tilde{v}_{k}(s_{kl})$.
\\
\begin{thm}
Consider the value iteration for stage $k$ as a one shot robust game. %given a pure strategy history $h^{p}_{k}$, 
Based on $\bar{v}_{k}^{p}(s_{kl}) \geq 0$ of previous iteration, 
we define the robust game value  obtained at $k$ as~\eqref{pickv}. Then for $k=2,\cdots, K$, $\tilde{v}_{k}(s_{kl})$~\eqref{tilde_v} is upper bounded by $\bar{v}_{k}^{p}(s_{kl})$, i.e., $\tilde{v}_{k}(s_{kl}) \leqslant \bar{v}_{k}^{p}(s_{kl}).$
\label{robust}
\end{thm}
\begin{pf}
Since $\bar{v}_{k+1}^{p}(s_{(k+1)h})$ is a nonnegative scalar value, the extreme points of the set $\mathbb{Q}_{k}$ is a subset of the extreme points  of set $\mathbb{Q}^p_k$. Hence, by considering the value of matrix game $Q^{p} (s_{kl})\in \mathbb{Q}^p_k$ defined in~\eqref{eq:Q}, we will get the upper bound of the maximum game value from extreme points of $\mathbb{Q}_k$. 

Consider the following optimization problem for the system with constraint inequality~\eqref{optr} for any possible attacker's strategy vector $\mathbf{f}$ at each stage $k$
%From the system's perspective, to compute the value of the matrix game is equivalent with:
%\vspace{-5pt}
\begin{align}
%\begin{split}
%\vspace{-5pt}
\min_{\mathbf{g}} \quad & z\label{ob_z}\\
\text{subject to}\quad &z \geq \max_{\tilde{Q}(s_{kl})\in \tilde{\mathbb{Q}}_{k}}\mathbf{f}^{T} [\tilde{Q}(s_{kl})]\mathbf{g}.
%\end{split}
\label{optr}
\end{align}
As proven by~Lemma 5 in \cite{RGT},~\eqref{optr} is equivalent to the following constraint that considers only the extreme points
\begin{align}
\quad z \geq \max_{Q^{p}(s_{kl})\in \mathbb{Q}_{k}^p}\mathbf{f}^{T} [Q^{p}(s_{kl})]\mathbf{g},
\end{align}
For the worst-case $f$, the above is also true. Hence, let 
\begin{align}
\label{eq1}
v^p_{k}(s_{kl})
%=\max_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}}v^*[Q_{k}^{p}(h^{p}_{k},s_{l})]
=\max_{Q^{p}(s_{kl})\in \mathbb{Q}_k^p} \min\limits_{\mathbf{g}}\max\limits_{\mathbf{f}}\mathbf{f}^T[Q^{p}(s_{kl})]\mathbf{g}.
%\label{pickv}
\end{align}
For optimal policies $\mathbf{f}^{*}(s_{kl})$ and $\mathbf{g}^{*} (s_{kl})$, the above optimization problem~\eqref{eq1} results in a cost
\\\centerline{$
%\begin{align*}
 \max\limits_{Q^{p}(s_{kl})\in \mathbb{Q}_{k}^p}v^*[Q^{p}(s_{kl})].
%\end{align*} 
$}
However, $(\mathbf{f}^{*}(s_{kl}),\mathbf{g}^{*}(s_{kl}))$ can be non-pure strategies, meaning that when we apply  $(\mathbf{f}^{*}(s_{kl}),\mathbf{g}^{*}(s_{kl}))$  to calculate system dynamics such as equations~\eqref{dynamicgame}, they will not result in any extreme point of set $\mathbb{Q}_{k+1}$. 

Now consider the final stage $K$, we have
\\\centerline{$
%\begin{align*}
Q^p(s_{Kl})=r^p(s_{Kl}), \tilde{Q}(s_{Kl})=\tilde{r}(s_{Kl}),
%\end{align*}
$}
and use the $Q^p(s_{Kl})$ and $\tilde{Q}(s_{Kl})$ in the above proof, value $\tilde{v}_k(s_{Kl})$ from $\tilde{Q}(s_{Kl})$ is smaller than $\bar{v}_k^p(s_{Kl})$ from the extreme points auxiliary matrix $Q^p(s_{Kl})$, i.e., for $K$, the following inequality holds
\\\centerline{$
%\begin{align*}
\tilde{v}_k(s_{Kl}) \leqslant \bar{v}_k^p(s_{Kl}).
%\end{align*}
$}
Then, by induction, with the value $\tilde{v}_{k+1}(s_{(k+1)h})$ of iteration for stage $k+1, 2\leqslant k \leqslant K-1$ satisfies %%%%%(from $\mathbb{Q}_{(k+1)p}$):
%\begin{align*}
\\\centerline{$\tilde{v}_{k+1}(s_{(k+1)h})\leqslant \bar{v}_{k+1}^{p}(s_{(k+1)h}),$} 
%\end{align*}
and nonnegative payoff and state transition probability $r_{k}^{ij}\geqslant 0$ and $\tilde{P}_{k}^{ij}\geqslant 0$, replacing $\tilde{v}_{k+1}(s_{(k+1)h})$ by $v_{k+1}^{p}(s_{(k+1)h})$ in~\eqref{eq:Q} will make every entry of matrix $\tilde{Q}(s_{kl})$ smaller than matrix $Q^{p}(s_{kl})$. 
% since $r_{k}^{ij}\geqslant 0$ and $\tilde{P}_{k}^{ij}\geqslant 0$. %%% by definition. 
%The system is a minimizer, and is possible to get a smaller value at time k, with some %optimal strategy history $h^{*}_{k} \neq h^{p}_{k}$, 
With a similar argument in the next iteration for stage $k-1$, we have 
%\begin{align*}
\\\centerline{$
\tilde{v}_{k}(s_{kl}) \leqslant \bar{v}_{k}^{p}(s_{kl}).
$}
%\end{align*} 
%(Note that here $v_{k}^{s_{l}}(h^{p}_{k-1})$ is not the game value at $k$ either, because the strategies from 1 to $k-2$ must be pure to get it).
\end{pf}

Based on the above observation, we arrive at the suboptimal algorithm to compute the equilibrium solutions, illustrated in the Algorithm~\ref{finite}. Note that for keeping the physical state $x_{[k-T,k]}$ of the first stage of the game starts at $\hat{x}_0$, in the above Algorithm~\ref{finite} the $K$-stage game  starts at $k=T$. This does not affect our proofs in this section for considering $k=1,\dots, T$.
According to~Theorem~\ref{robust}, we use Algorithm~\ref{finite} to compute an upper bound of the value and the corresponding suboptimal strategy for every step. The % Nash Equilibrium selection 
function $\pi$ computes the strategy and robust value as defined in~\eqref{pickv}. 

The values of the finite stage game $\tilde{v}_k(s_{kl})$ and $\bar{v}_k^p(s_{kl})$ resulting from two auxiliary matrices $\tilde{Q}(s_{kl})$ $Q^p(s_{kl})$ are based on strategy concatenations that only differ at stage $k-1$ (i.e., the same and pure strategies from stages 1 to $(k-2)$). By value iteration backward to stage $1$, we compare the game value for all possible strategies and the robust game value $\bar{v}_1^p(s_{1l})$ of~Algorithm~\ref{finite} in the following theorem.
\begin{cor}
%Moreover, by following this iteration method,
~Algorithm~\ref{finite} yields an upper bound $v_1(s_{1l})$ for the value of the $K$-stage game, together with suboptimal strategies $\mathbf{f}_{a}$ and $\mathbf{g}_{a}$. 
\end{cor}
%\begin{pf}
The strategies $\mathbf{f}_{a}, \mathbf{g}_{a}$ of~Algorithm~\ref{finite} are possibly not pure. %strategy history $h^{p}_{K} \in H^{p}_{K}$. 
According to~Theorem~\ref{robust}, we obtain $\tilde{v}_{k}(s_{kl})\leqslant \bar{v}^{p}_{k}(s_{kl}),$ and the proof holds for every $k=2,\cdots,K$. Consider the value iteration for $k=1$, with 
$\tilde{v}_{2}(s_{2l})\leqslant \bar{v}^p_k(s_{2l})$, and $\text{\ }\quad Q^{ij}( s_{1l})$
$=r^{ij}(s_{1l})+\sum\limits_{\delta_h\in S}{P}^{ij}(s_{2h}|s_{1l})v^{p}_{2}(s_{2h}) \leqslant Q^{p,ij}(s_{2l}),$
%\end{align*}
%\normalsize
thus the true value of the K-stage game $v^{*}[Q(s_{1l})] \leqslant \bar{v}^{p}_{1}(s_{1l})$. The iterative value based on pure strategy auxiliary matrix sets $\mathbb{Q}_{k}^p, k=1,\cdots, K,$ obtained from~Algorithm~\ref{finite} is an upper bound for the game value.
%\end{pf}
Let $v^*[Q(s_{na})]$ represent the minimum total payoff of the system when the strategy is calculated given that there is no attack at all in $K$ stages, then $\bar{v}^{p}_{1}(s_{1l})-v^{*}[Q(s_{1l})] \leqslant \bar{v}^{p}_{1}(s_{1l})-v^*[Q(s_{na})]$, since  $v^*[Q(s_{na})] \leqslant v^{*}[Q(s_{1l})]$ when the system operates in normal state without sacrificing any control cost to play against attacks. The sub-optimality of value $\bar{v}^{p}_{1}(s_{1l})$ calculated from Algorithm 1 is then bounded though we do not know the true value $v^{*}[Q(s_{1l})]$ of the game.
