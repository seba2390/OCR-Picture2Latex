\section{Comparison of Algorithms}
\label{sec:simulation}
One advantage of the moving horizon Algorithm~\ref{finite_new} is its faster computation speed. Table~\ref{alg_compare} shows Matlab simulation time for different $K$-stage games, all with the same size of action space for the system and attacker. When $K$ increases, the difference between algorithm speed also increases.
We compare the cost of the strategies provided by the suboptimal Algorithm~\ref{finite} and~Algorithm~\ref{finite_new}.
The example studied is an unstable batch reactor, a four dimensional system~(see \cite{ncs}, Section IV.A for model parameters).

We first show the case under replay attacks, when the system is equipped with two controllers, one steady state Kalman filter, and the corresponding $\chi^2$ detector. An optimal LQG controller $u^*_K$ is denoted as controller $1$, and a non-optimal controller $(u^{*}_{k} + \Delta u_{k})$~(\cite{replay}) with higher replay detection rate as controller $2$. System's action space includes: subsystem $u_{1k}$ with controller $1$ and subsystem  $u_{2k}$ with controller $2$. For illustration, we show the case when the attacker's action space are discretized replay attack time window size $\{10s, 20s, 30s, 40s\}$ in simulation. We design switched control policy for the system under replay attacks with initial mode $\delta_{2}$, (i.e.,~$p(\delta^{1}_{2})=1$), we compare the system's strategies and total payoff when applying suboptimal strategies of Algorithm~\ref{finite} and real-time receding horizon Algorithm~\ref{finite_new} in a finite game of stage $K=50$.



Figure~\ref{ads} shows the probability of switching to Controller~$2$ at every stage according to different algorithms.
Three cases are shown in Figure~\ref{cost2_c2}--when the system applies the strategy of Algorithm~\ref{finite}, the strategy of Algorithm~\ref{finite_new}, and only the subsystem $2$ with higher replay detection rate through all stages. 
Figure~\ref{ps1_c1} shows the probability that system being at mode $\delta_{1}$ (successfully detected an attack), when applying strategies obtained from the two algorithms and always choosing subsystem $2$. Applying a game strategy, randomly switching between subsystems results in a lower cost, while not sacrificing the detection rate significantly. 

\begin{table}%[t]
\centering
\begin{tabular}{|c|c|c|}
  \hline
   K  & \iffalse Time of \fi real time algorithm & \iffalse Time of \fi suboptimal algorithm \\ \hline
   20 &   1.8054s    &  6.7346s    \\  \hline
   50 &   4.9968s    &  58.6144s  \\ \hline  
   100&  8.3827s    &  2073.2928s   \\ \hline
    500&  41.0342s  & 20h    \\ \hline
\end{tabular}
\caption{Elapsed time comparison of two algorithms} %for different total stage numbers}
\label{alg_compare}
\end{table}

For game strategies designed for multiple types of attack, Figure~\ref{long_time} shows the case when attacks are successfully detected and the system reaches the cyber mode $\delta_1=safe$, the quadratic cost of the system converge. When replay finally occurs at $T_2=100s$, with a game-theoretic strategy, the cost of the system is smaller than the cost when system always applies a controller with higher cost and higher detection rate. Data injection attacks shown in Figure~\ref{long_time} appear during $k=30,31,\dots,50$.   
\begin{figure}[b!]
%\vspace{-5pt}
\centering
\includegraphics [width=0.42\textwidth]{ads.pdf}
\vspace{-8pt}
\caption{Strategies comparison of two algorithms for system under replay attack--the probability of switching to subsystem 2 at mode $\delta_{2}$ of every $k$. }
\label{ads}
\vspace{-5pt}
\end{figure}
\begin{figure}[b!]
\vspace{-5pt}
\centering
\includegraphics [width=0.42\textwidth]{cost2_c2.pdf}
\vspace{-15pt}
\caption{Cost comparison of system applying different strategies at mode $\delta_{2}$. Applying the suboptimal strategy provides the smallest cost, and the strategy of the real time algorithm is better than the one of a non-game approach.}
\label{cost2_c2}
\end{figure}
\begin{figure}[t!]
%\vspace{-5pt}
\centering
\includegraphics [width=0.42\textwidth]{ps1_c1.pdf}
\vspace{-10pt}
\caption{Comparison of the probability of the system being at mode $\delta_{1}$ for different strategies. 
Game strategies provide similar detection rate with the non-switching policy.}
\label{ps1_c1}
\vspace{-5pt}
\end{figure}


\begin{figure}[t!]
\centering
\includegraphics [width=0.42\textwidth]{long_time.pdf}
\vspace{-15pt}
\caption{Cost comparison when system and the attacker apply different strategies. The replay attack occurs at $T_2=100s$.}
\label{long_time}
\vspace{-5pt}
\end{figure}

These figures illustrate that the real-time strategy results a higher cost than the suboptimal system strategy, and they both provide lower control costs compared to the non-game-theoretic approach. The non-game-theoretic approach provides only a slightly higher probability of being at the safe mode in $K$ stages. By introducing the game strategy, i.e., switching between multiple subsystems, we do not sacrifice the payoff of the system while providing an acceptable detection rate, even we discretize the attacker's action space in the game framework. For instance, Figures~\ref{cost2_c2} and~\ref{ps1_c1} show the result when the actual replay attack occurs at $T_2=25s$, and the game strategies are calculated with action space $A_t=\{10s,20s,30s,40s\}$. Since $25$ is in between $[20,30]$, and the error of the action space discretization is bounded, a game strategy calculated via finite action space improves the system's performance.


\iffalse
The linearized model parameters are
\footnotesize
%\begin{align*}
\centerline{$
\mathbf{A}&=\begin{bmatrix}1.38 &   -0.2077 &  6.715 &  -5.676\\
   -0.5814 &-4.29   &    0  &    0.675\\
   1.067 &  4.273 &    -6.654&  5.893\\
   0.048  & 4.273 &    1.343 &  -2.104 \end{bmatrix},$}\\
\centerline{$
\mathbf{B}&=\begin{bmatrix}0 & 0\\
   5.679 & 0\\
   1.136 & -3.14\\
   1.136 & 0\end{bmatrix}, \quad \mathbf{C}=\begin{bmatrix}1 &  0 &  1 &  -1\\ 0&  1 & 0 &  0\end{bmatrix},\quad \mathbf{D}=\mathbf{0}.
   $}
%\end{align*}
\normalsize
\fi
