%\section{Algorithms for Finite and Infinite Horion Game}
\section{Suboptimal Algorithm for Finite Horizon Game}
\label{sec:algorithm_finite}

Based on the game formulation, in this section we discuss the existence of an optimal finite horizon solution for the system, and present an algorithm to compute a suboptimal strategy for the system. 
In this section, we propose a moving-horizon algorithm to compute the saddle-point equilibrium strategy of the hybrid stochastic game.  Illustrated in Fig. 2, a time window of size $T$ is used, and an equilibrium strategy is computed at each stage $k$ by looking back $T$ stages of the physical state $x_{[k-T, k]}$ and its associated cyber state $\delta_l$. Detailed process of moving the horizon to obtain predicted future stage information is described in Subsection~\ref{move_horizon}.  Algorithm~\ref{finite_new} is developed based on this concept, and provides a scalable and real-time computation process, which allows us to analyze the convergence property of the strategies of the hybrid stochastic game in Subsection~\ref{converge}. 

\subsection{A Moving-Horizon Approach for Hybrid Stochastic Game}
\label{sec:algorithm}
%Based on the game formulation, in this section, we define an objective function and the optimal strategy for both players, and 
In this section, we propose a moving-horizon algorithm to compute the saddle-point equilibrium strategy of the hybrid stochastic game.  Illustrated in Fig. 2, a time window of size $T$ is used, and an equilibrium strategy is computed at each stage $k$ by looking back $T$ stages of the physical state $x_{[k-T, k]}$ and its associated cyber state $\delta_l$. Detailed process of moving the horizon to obtain predicted future stage information is described in Subsection~\ref{move_horizon}.  Algorithm~\ref{finite_new} is developed based on this concept, and provides a scalable and real-time computation process, which allows us to analyze the convergence property of the strategies of the hybrid stochastic game in Subsection~\ref{converge}. 
%and the stability property of the system.
\subsubsection{A Moving-Horizon Algorithm for Game Strategies}
\label{move_horizon}
The saddle-point equilibrium strategy and the value of the moving-horizon game at each stage involves solving finite zero-sum matrix games. 
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this paper, we consider an objective function that reflects the payoff of the game at the current stage $k$, and also the expected payoff from the future stage. By looking one stage ahead of the game state at $k$, predicting the physical dynamics $\mathbf{x}_{k+1}$ given any action pair, we move the information horizon to stage $k+1$ and obtain future expectation for computing the strategies at stage $k$. The moving horizon process is illustrated as Figure~\ref{sg}. 
%We obtain a zero-sum game at stage $k$ to solve as in Definition~\ref{zero_sum}, 
Detailed process to construct the payoff matrix of a zero-sum game for stage $k$ is described, and Algorithm~\ref{finite_new} presents the complete equilibrium computation process of the hybrid stochastic game.
    
\iffalse When we calculate the expected value of the game at stage $k+1$, treat it as a termination. \fi
Given any action pair $(a_{ik},u_{jk})$ at stage $k$, we first update the state space form of the system dynamics $\mathbf{x}_{k+1}$ based on $\mathbf{x}_{[k-T,k]}$ as~\eqref{dynamicgame}. We view $\mathbf{x}_{k+1}$ as a function of $(\mathbf{x}_{[k-T, k]}, a_{ik},u_{jk})$, the immediate payoff function $\tilde{r}_{k+1} (s_{(k+1)h})$ (for stage $k+1$) defined as~\eqref{payoff} is a function of $s_{(k+1)h}=(\mathbf{x}_{[k-T+1,k+1]}, \delta_h)$, thus $r_{k+1}$ is a function of $(\mathbf{x}_{[k-T, k]}, a_{ik},u_{jk}, \delta_h)$, as shown in the following equation~\eqref{prev}.
Then, we compute the value of the matrix game at stage $k+1$, for every $r_{k+1}(\mathbf{x}_{[k-T,k]},a_{ik},u_{jk},\delta_h)$, $h=1,2,3,$ $i\in \{1,\cdots, M\}, j \in \{1,\cdots, N\}$ as~\eqref{prev}:
%$v^{ij}_{k+1}(s_{(k+1)h}) \leftarrow \pi (r_{k+1}(s_{(k+1)h}))$;\\
\begin{align}
v^{ij}_{k+1}(x_{[x-T,k]},\delta_h)= \min\limits_{\mathbf{g}}\max\limits_{\mathbf{f}}(r_{k+1}(\mathbf{x}_{[k-T,k]},a_{ik},u_{jk},\delta_h))
\label{prev}
\end{align}

With the predicted value from the next stage, define the auxiliary matrix for stage $k$ as:
\footnotesize
\begin{align}
\begin{split}
Q_{k}(s_{kl})=r_{k}(s_{kl})
                         +\sum_{s_h\in S} \tilde{P}_{k}( s_{(k+1)h} |s_{kl})\cdot v_{k+1}(x_{[k-T,k]},\delta_h),
\end{split}
\label{Q_k}
\end{align}
\normalsize
where the matrix $v^{ij}_{k+1}(x_{[x-T,k]},\delta_h)$ is defined by~\eqref{prev}, and it is the element of the $i$th row, $j$th column of the matrix $$v_{k+1}(x_{[k-T,k]},\delta_h) \in \mathbb{R}^{M \times N}.$$ %Then the optimal solution for stage $k$ of the real time Algorithm~\ref{finite_new}.
The dot products between two matrices $\tilde{P}_{k}( s_{(k+1)h} |s_{kl})$, $v_{k+1}(x_{[k-T,k]},\delta_h)$ is an element wise product of two elements at the same position of the two matrices.
\iffalse we first predict the payoff matrix of stage $k+1$ for every action pair $(a_{ik}, u_{jk})$, which is denoted as $r_{k+1}(\mathbf{x}_{[k-T,k]},a_{ik},u_{jk},\delta_h), h=1,2,3$. For getting a payoff matrix at state $s_{kl}$, \fi

The value and stationary equilibrium strategies that Algorithm~\ref{finite_new} calculates at each stage $k$ is defined as following:
\begin{definition}
Given $s_{kl}$, $v_{k+1}(x_{[k-T,k]},\delta_h)$ as~\eqref{prev}, and auxiliary matrix $Q_{k}(s_{kl})$ as~\eqref{Q_k}, the value and equilibrium strategies at $k$ are defined as the following equation: 
\begin{align}
v(s_{kl})= \min\limits_{\mathbf{g}_k(s_{kl})}\max\limits_{\mathbf{f}_k(s_{kl})} \mathbf{f}_k(s_{kl})^T Q_{k}(s_{kl}) \mathbf{g}_k(s_{kl}).
\label{v_k}
\end{align}
Where we treat the auxiliary matrix $Q_{k}(s_{kl})$ as the payoff matrix of a zero-sum game of stage $k$. 
%The value and equilibrium strategies of the matrix game $Q_{k}(s_{kl})$ consist with the meaning of value and equilibrium strategies in Definition~\ref{zero_sum}.
\end{definition}

At each stage $k$, we repeat calculating $Q_{k}(s_{kl})$ and the corresponding value and equilibrium strategies, then update the system dynamics by the strategies for computation of next stage. The complete process is summarized as Algorithm~\ref{finite_new}. 
%\newtheorem{algorithm}{Algorithm}
\begin{algorithm}
%\label{finite_new}
\caption{\textbf{: Moving-Horizon Algorithm for A Hybrid Stochastic Game}}
%\begin{algorithmic}
\textbf{Input}: System model parameters and game parameters.
\\\textbf{Initialization}: $\hat{\mathbf{x}}_{1|0}, \mathbf{x}_1$.
\\\textbf{Iteration}: For $k=T, \cdots, K+T-1$,                                                                                 
 $s_{kl}=(x_{[k-T,k]},\delta_l),$  $l=1,2,3$:  
get  the auxiliary matrix~\eqref{Q_k} for stage k;
%\[Q_{k}(s_{kl}) = r_{k}(s_{kl})+\sum_{s_h\in S} \tilde{P}_{k}( s_{(k+1)h} |s_{kl})\cdot v_{k+1}(x_{[k-T,k]},\delta_h),\]
compute the value and equilibrium strategies of every matrix game:\\
$v(s_{kl})= \min\limits_{\mathbf{g}(s_{kl})}\max\limits_{\mathbf{f}(s_{kl})} \mathbf{f}(s_{kl})^T Q_{k}(s_{kl}) \mathbf{g}(s_{kl})$,\\
$\mathbf{f}_k^{*}(s_{kl})=\arg \max\limits_{\mathbf{f}_k(s_{kl})}\mathbf{f}_k(s_{kl})^T Q_{k}(s_{kl} )\mathbf{g}_k^*(s_{kl})$, \\
$\mathbf{g}_k^{*}(s_{kl})=\arg \min\limits_{\mathbf{g}_k(s_{kl})} [\mathbf{f}_k^{*}(s_{kl})]^T Q_{k}(s_{kl}) \mathbf{g}_k(s_{kl})$.\\
Update the system dynamics with strategies $\mathbf{f}_k^{*}(s_{kl}),\mathbf{g}_k^{*}(s_{kl}),$ $l=1,2,3$ as described in~\ref{dynamicgame} for the next stage.
\\\textbf{Return}: the concatenation of strategies for both players $\mathbf{f}=\{f_k^{*}(s_{kl})\},\mathbf{g}=\{\mathbf{g}^*_k(s_{kl})\}$ and the value sequence $v_{k}(s_{kl}),k=1,\cdots, K, l=1,2,3$.
\label{finite_new}
\end{algorithm}
To get the total payoff till stage $k$ by Algorithm~\ref{finite_new}, we plug in the strategies $\mathbf{f}, \mathbf{g}$ to the system dynamics and calculate the sum of payoff for all stages.%as Definition~\ref{R_K} for $k$ stages.
\iffalse
Miao~\textit{et al.} designed a suboptimal value iterative algorithm and proved a value upper bound for the stochastic game in~\cite{cdc_replay}. The suboptimal algorithm is exponential time with respect to K, since before value iteration, it computes the extreme points for the payoff and state transition probability set--the number of extreme points increases exponentially with the stage number $K$.  When running offline, this algorithm works well in the sense that total payoff of $K$ stages  are considered for the game.   
\fi
\begin{remark}
It is worth noting that~Algorithm~\ref{finite_new} reduces the computation overhead for the hybrid stochastic game, since it looks one stage ahead with a moving-horizon information window. The complexity of Algorithm~\eqref{finite_new} is $O(K)$. For a large total stage number of the hybrid stochastic game $\tilde{T}$, it is necessary to examine the strategy trend of Algorithm~\ref{finite_new}, such as convergence property. As a contrast, the suboptimal algorithm in~\cite{cdc_replay} takes the total expected payoff as an objective function. The complexity of suboptimal algorithm is exponential with stage number $K$, because the algorithm looks $K$ stages ahead at once and compute a robust game for every iteration. The advantage of suboptimal algorithm in~\cite{cdc_replay} is to provide an upper bound of the total finite cost. However, for a large $\tilde{T}$,  the suboptimal algorithm in~\cite{cdc_replay} is computationally expensive. Numerical comparisons are shown in Section~\ref{sec:simulation}.
\iffalse
To get a suboptimal total payoff of time $\tilde{T}$ faster, we can view this as a $K$ stage game, and at each stage the physical dynamics run $[\tilde{T}/K]$ steps as~\eqref{eq:xu}. The average payoff and state transition probability in the $[\tilde{T}/K]$ discrete time slots are the payoff and state transition probability of one game stage.  
In particular, when we consider the average of infinite horizon time $\tilde{T}$, the game can reduce to a stationary stochastic game. For example, with the infinite time average LQG cost as the payoff function, and the average detection rate, false alarm trigger rate as the state transition probability, we actually reach the optimal strategy of a stationary stochastic game.  
\fi

\end{remark}
\subsubsection{Convergence Analysis of the Algorithm}
\label{coverge}
Given the sets of models for each component of the subsystems and attacks, the system dynamics are defined by a sequence of action pairs $(a_{ik},u_{jk}), k \in \{k+T, \cdots, K+T\}$ randomly chosen by the attacker and the system. 
Then, the system dynamics with the stochastic game strategies (for the system and the attacker) are equivalent with a switched system -- the system model randomly switches among $N$ subsystems, according to strategies $\mathbf{f}_k(s_{kl})$. %[\mathbf{g}_k(s_{kl})]^T$. 
The following theorem shows the existence condition of convergent strategies when $k\to \infty$. 
When there exists such strategies, the switched system can be described as a Markov jump system, since the state transition probability also converges. 
%\newtheorem{theorem}{Theorem}
\begin{proposition}
%The nonstationary stochastic game has a stationary strategy
The strategy sequences $\mathbf{f}_k^*(s_{kl})$, $\mathbf{g}_k^*(s_{kl})$ of the stochastic game converge to $\mathfrak{f}^l,\mathfrak{g}^l $, $l=1,2,3$, i.e.,
\begin{align*}
\mathfrak{f}^l=\lim\limits_{k\to\infty}\mathbf{f}_k^*(s_{kl}), \mathfrak{g}^l=\lim\limits_{k\to\infty}\mathbf{g}_k^*(s_{kl}), l=1,2,3,
\end{align*}
if updating system dynamics at stage $k+1$ by ($\mathfrak{f}^l,\mathfrak{g}^l$) results in:
\begin{align}
\lim \limits_{k \to \infty} Q_{k}(s_{kl})=\lim \limits_{k \to \infty} Q_{k}(s_{(k+1)l}), l=1,2,3.
\label{station}
\end{align}
%\end{equation}
\end{proposition}
\begin{proof}
According to Algorithm~\ref{finite_new}, the strategies $\mathbf{f}^*_k(s_{kl})$, $\mathbf{g}^*_k(s_{kl}),l=1,2,3$ are the saddle-point equilibrium strategies for the payoff matrices $Q_{k}(s_{kl}),l=1,2,3$. 
\iffalse After calculating the strategies at stage $k$, the next step of the Algorithm~\ref{finite_new} is updating the system dynamics, and then getting the auxiliary matrices $Q_{k}(s_{(k+1)l}),l=1,2,3$ for stage $k+1$. 
The saddle-point equilibrium strategies at stage $k+1$ are decided by $Q_{k}(s_{(k+1)l}),l=1,2,3$. 
Thus, if updating the system dynamics for the auxiliary matrix $Q_{k}(s_{kl})$ and at each mode $Q_{k}(s_{kl})$  converges with an optimal strategy for both players, \fi
Thus, if~\eqref{station} holds, the auxiliary matrix $Q_{k}(s_{kl})$ converges, and we get convergent strategies for both players. 
%The nonstationary stochastic game then converges to a stationary one, the switching probability of the system then does not change with $k$.%
%turns to a homogeneous Markov jump system--the switching probability distribution is determined by 
%%
%$\tilde{P}_k(s_{(k+1)h}|s_{kl})$.
\end{proof}
\iffalse
\begin{proposition}
If the control input $\gamma(a_{ik}, u_{jk})=L_j\hat{\mathbf{x}}_{t}, t\in[k-T, k]$ for any action pair $(a_{ik}, u_{jk})$, then the system is not asymptotically stable when the strategy sequences $\mathbf{f}_k^*(s_{kl})$, $\mathbf{g}_k^*(s_{kl})$ of the stochastic game are not convergent.
\end{proposition}
%\begin{proof}

Sketch of the proof: 
When the system is asymptotically stable, $\lim_{k\to\infty}\mathbf{Ex}_k=0$, by the definition of immediate payoff function~\eqref{payoff}, we gave $$
After calculating the strategies at stage $k$, the next step of the Algorithm~\ref{finite_new} is updating the system dynamics, and then getting the auxiliary matrices $Q_{k}(s_{(k+1)l}),l=1,2,3$ for stage $k+1$. 
%\end{proof}
\fi
\begin{remark}
When the strategy sequences of both players converge, the switched system dynamics converge to a discrete-time Markov jump linear system (with delays when the attacker's strategies include replay attacks), then we analyze the stability properties of the system based on conclusions of previous work~\cite{delay_mlj}.
\end{remark}
%Assume that every subsystem is mean square exponentially stable (i.e., attacks can increase the cost of every subsystem, but can not destabilize the system), 
 %\cite{dtls_mj},~\cite{dtass_mj}. 
%When the strategy sequences do not converge, condition of system stability is considered as future work. 

It is possible that some subsystems $u_{jk}, j \in \{1,\cdots, N\}$  are unstable under specific types of attacks.
When this is the case, the system switches among stable and unstable subsystems. Stability properties of continuous time linear switched systems including unstable modes are analyzed in~\cite{switch_unstable}. To guarantee exponential stability, the total activation time of unstable subsystems need to be relatively small compared with that of stable subsystems. Given the stochastic game strategy, we get the switched dynamic process of the system under different types of attacks, and check whether stability conditions are violated. 
More analysis of system stability conditions based on the moving horizon stochastic game framework will be an avenue of future work.
%For the discrete time switched system switches among stable and unstable subsystems, where the transition probability is determined by the game strategy, the stability property still needs future analysis.
\iffalse the system is still Markov jump, and properties of piecewise homogeneous Markov jump systems are discussed in~\cite{H_mj}. \fi

\iffalse There are several stable conceptions in literature, like the means square exponentially stable and mean square stable. \fi
\iffalse
\begin{definition}
The stochastic differential Equation is said to be exponentially stable in mean square if there is a pair of positive constants $\lambda$ and $M$ such that, for all initial data $x_0$, such that
\begin{align}
\mathbb{E}\|x_k\|^2 \leq M\mathbb{E}\|x_0\|^2 e^{-\lambda k}, \text{for all}\ k\geq 0.
\end{align}
\end{definition}

\begin{definition}
The stochastic differential equation is said to be mean square stable if 
\begin{align}
\lim\limits_{k\to\infty}\mathbb{E}\|x_k\|^2=0, \text{for any initial}\ \mathbf{x}_0, \sigma_0,
\end{align}
where $\sigma_0$ is the probability distribution of the initial subsystem. 
\end{definition}
\fi

%%%%%%%
%%%%%%%%%%
\iffalse
\begin{theorem}
When the Markov jump discrete time system is mean square stable, the nonstationary stochastic game has a stationary strategy as $k\to\infty$. 
\end{theorem}
\begin{proof}
\end{proof}
\fi
%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{proof}
Consider the pure strategy of always applying the $j$-th controller (detector) at each stage for any attacker's strategy as a subsystem. Then the stochastic game strategy for the system is equivalent with randomly switching between $N$ controllers/detectors with any linear combination of probability $g_j, j \in \{1,\cdots, N\}, \sum_{j} g_j =1$, for any joint state. We assume every subsystem that equipped with a controller/detector can make the closed loop system mean square exponentially stable, even under worst case attack strategies (i.e., attacks can increase the system cost, but can not distablize the system), then for system always applying controller $j$, there exists a pair of positive constants $\lambda_j$ and $M_j$ such that, for all initial data $x_0$, such that
\begin{align}
\mathbb{E}\|x_k\|^2 \leq M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}, \text{for all}\ t\geq 0.
\end{align}
Then let $\lambda = min_{j \in N} \{\lambda_j\}, M= N max_{j\in N} \{M_j\}$, since $0\leq g_j \leq 1, 0\leq g_j^2\leq 1$,
\begin{align}
\begin{split}
\mathbb{E}\|\sum_j g_j x_k\|^2 &\leq \sum_j g_j^2 M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}\\
&\leq \sum_j M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}\\
&\leq M \mathbb{E}\|x_0\|^2 e^{-\lambda k},
\text{for all}\ k\geq 0.
\end{split}
\end{align}
The switching system according to the game strategy is also exponentially mean square stable.
\end{proof}
\fi
