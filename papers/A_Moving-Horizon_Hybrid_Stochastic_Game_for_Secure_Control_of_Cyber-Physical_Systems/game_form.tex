\section{A Hybrid Stochastic Game Model}
\label{sec:game_form}
To obtain a switching policy that minimizes the expected real-time worst case payoff for the given subsystems, 
we formulate a zero-sum, hybrid stochastic game between the system and the attacker. System dynamics knowledge are combined with the game definition, and the quantitative process for the game parameters will be introduced in this section. We assume that one game stage $k$ is also one time step of the physical system. The total stage number is $K$. The hybrid game state space $(X_{[k-T,k]}\times S)$ contains information about both the system dynamics $\mathbf{x}_k$ and the discrete modes $\delta_l, l=1,2,3$. Here, $T$ is the window size of system dynamics needed to keep the state transition between stages $k$ and $(k+1)$ Markov. The joint state includes information we need to compute the game strategy at the current stage. This is the main difference compared with the previous work~(\cite{cdc_replay}), while the latter is not Markov since it needs to consider all the possible histories of strategies for deciding the physical dynamics and getting a strategy. At each stage $k \in \{T,\cdots, K+T\}$, parameters include the action space for the attacker (system) $A_{t}$ ($A_{s}$), the state transition probability matrix $\mathbb{P}_{k}$, and the immediate payoff matrix $r_{k}$. The solution set of the game is mixed strategies $\mathbf{F}_{k}$ for the attacker, and $\mathbf{G}_{k}$ for the system. Formally, the game is defined as a sequence of tuples:
 $\{(X_{[k-T,k]} \times S),A_{t},A_{s}, \mathbf{F}_{k},\mathbf{G}_{k}, P, r\}$.

\iffalse
\begin{table*}
\centering
%\caption{Parameters of the hybrid stochastic game between the system and the attacker}
\begin{tabular}{|c|c|}
  \hline
   $s_{kl}=(x_{[k-T,k]}, \delta_l)$& Joint game state: sequence of physical dynamics, and piecewise constant mode \\ \hline
   $A_{t}$ & Attacker's action space \\ \hline
   $A_{s}$ & System's action space \\ \hline  
   $\mathbf{f}_k(s_{kl})$ & Strategy of the attacker at stage $k$, 
                                           state $s_{kl}, l=1,2,3$  \\ \hline     
      $\mathbf{g}_k(s_{kl})$ & Strategy of the system at stage $k$, 
                                            state $s_{kl}, l=1,2,3$  \\ \hline     
    ${P}(s_{(k+1)h}|s_{kl})$ & Probability that system transits from state $s_{kl}$ 
                                              at stage $k$ to state $s_{(k+1)l}$ at stage $k+1$\\ \hline
    ${r}(s_{kl})$ & Immediate payoff matrix at stage $k$ \\ 
   \hline
\end{tabular}
\centering
%\captionsetup{justification=centerlast}
\caption{Parameters of the hybrid stochastic game between the system and the attacker}
\label{game_parameter}
\end{table*} 
\fi

\textbf{Game State Space}: The joint state of the system at stage $k$ is described by the pair $s_{kl}=(x_{[k-T,k]}, \delta_l)$, where
\centerline{$
x_{[k-T,k]}=(x_{k-T}, x_{k-T+1}, \cdots, x_k ) \in X_{[k-T,k]}$} is the discrete-time dynamics of the physical process provided to the system--the state estimations $\hat{x}_{k-T},\cdots, \hat{x}_k$, $\delta_l \in S=\{\delta_1,\delta_2, \delta_3\}$ denote the cyber state of the system. We assume that once the game reach $\delta_1$, the system wins and will not enter other modes till next game, i.e., $\delta_1$ is an absorbing state. The moving-horizon transition of the joint states on stage axis is shown as Figure~\ref{sg}. The window size of system dynamics $T$ keeps the state transition between time $k$ and $k+1$ Markov. For instance, if the detector of the system requires system dynamics $\hat{x}_{[k-T_1,k]}$, and we consider sensor data injection attacks and replay attacks with replay windows less than $T_2$ steps, then $T=max\{T_1, T_2\}$. 
%With the system dynamics $\hat{x}_{[k-T, k]}$ denoted as $x_{[k-T,k]}$ for game stage $k$,  information needed to define the following action space of two players, the payoff and state transition probability is included. 

%i.e., the detector needs information for T steps to decide whether the alarm should be triggered. 
%\FM{In replay attack, we can say: once alarm is triggered, the system can stop the execution and check whether attack occurred, is this true for other attacks? Can the system distinguish between successfully detection and false alarm trigger?}
\begin{figure}[t!]
%\vspace{-5pt}
\centering
\includegraphics [width=0.32\textwidth]{xk.pdf}
\vspace{-8pt}
\caption{Joint state transition of the hybrid stochastic game when moving the horizon of game state one step ahead. When the state transits from stage $k$ to $k+1$, we slice the window of the sequence of physical dynamics one step ahead, add $x_{k+1}$ and remove $x_{k-T}$,  thus $x_{[k-T,k]} \to x_{[k-T+1,k+1]}$. The piecewise constant modes $\delta_l$, $\delta_h$ describe the cyber states provided by the detector at stage $k$, respectively.}
\label{sg}
\vspace{-5pt}
\end{figure}

%For simplicity, we omit the subscript k and just write state at every time k as $s_i,  i=1,2,3$.
\textbf{Attacker's Action Space}: We assume that the system is vulnerable to different attack models described by the action space $A_{t}$, where 
\\\centerline{$
A_{t}=\{a_{1}(x_{[k-T,k]}), a_{2}(x_{[k-T,k]}), \cdots, a_{M}(x_{[k-T,k]})\}
$}
is the attacker's action space at stage $k$, and $a_{1}$ means no attack. Here we only consider discretized action space of the attacker for computational efficiency. For the LTI system dynamics considered in this work, the distance of a continuous point to its nearest discrete point in action space is bounded. With bounded error of the dynamics by discretized continuous action space, the quality of game solutions under different conditions is analyzed by work~\cite{disaction}. 

The actions can describe both multiple types of attacks and the same type attack with different values. For instance, when considering only sensor data injection attacks with different norms of injection value, we will denote $a_i (x_{[k-T,k]}), i=2, 3,\dots$ as changing the sensor value from $\mathbf{y}_k=\mathbf{Cx}_k+\mathbf{v}_k$ to $\mathbf{y}'_k= \mathbf{y}_k+\mathbf{y}_{k,i}^a$, where any injection $\mathbf{y}_k^a$ is classified as $a_i (x_{[k-T,k]}), i=inf\{i:\ \|\mathbf{y}_k^a-\mathbf{y}_{k,i}^a\|_2\}$ in attacker's action space.
Similarly, for replay attack only, the action space is discretized as changing sensor values from $\mathbf{y}_k=\mathbf{Cx}_k+\mathbf{v}_k$ to $\mathbf{y}'_k= \mathbf{y}_{k-T_i}$ for action index $a_i (x_{[k-T,k]})$, where any replay time length $T_a$ is classified as $a_i (x_{[k-T,k]}), i=inf\{i:|T_a - T_i|\}$. Considering multiple types of attacks, we assume that the system is valnerable under $m_a$ types of attacks, and attack type $A_i$ is corresponding to $M_{a,i}$ discretized actions in the action space, then there are $\sum_{i=1}^{m_a} M_{a,i}+1$ actions in total within the attacker's action space $A_t$.
 

% with discretized, bounded norm $\|\mathbf{y}_{k,i}^a\|_2 \leqslant b$, since the attacker has limited energy for every data injection. This means any injection data that satisfies $\|\mathbf{y}_{k}^a\|_2 \leqslant b$ is considered as $inf\{i:\ \|\mathbf{y}_k^a-\mathbf{y}_{k,i}^a\|_2\}$ in attacker's action space. 

%For example, 
%when considering replay attacks and false data injection attacks, we take $a_{2k}$ as~\eqref{replay_y} for a given replay window size, and $a_{3k}$ as~\eqref{attackmodel} for a given data injection range. 

\iffalse by any given controller/estimator/detector combination of the system \fi
%; here, $y_{k}$ is the real sensor value and $\mathbf{y}_{k-t_{i}}$ denotes any replay sensor value in the strategy set. %\FM{here we assume during time $k \in \{1,...,K\}$ the replay window size $T$ does not change for simulation.}
%\item
%\FM{rewrite the action space definition}

\textbf{System's Action Space}: The system's action space at stage $k$ is defined as
\\\centerline{$
A_{s}=\{u_{1}(x_{[k-T,k]}), u_{2}(x_{[k-T,k]}),\cdots, u_{N}(x_{[k-T,k]})\},
$}  %= \{\mathbf{u}^{*}_{k}, \mathbf{u}^{*}_{k} + \Delta \mathbf{u}_{k}\}$
where $u_{j}$ is the index for the $j$th subsystem. We assume that the $N$ subsystems (a model for each component in Figure~\ref{system}) are determined priorly. For example, a subsystem can be the plant with a given optimal LQG controller, a Kalman filter and a $\chi^2$ detector. A subsystem can also be the plant with an optimal LQG controller, a resilient state estimator~\cite{res_estimator} and its corresponding estimation residual checking component. We assume that the attacker's action space is defined, with corresponding system's action or a subsystem that the detection rate is greater than $0$. A switched system does not ensure performance under the attack outside the action space of the game.

\textbf{Mixed Strategy}: Let $f^{i}_{k}(s_{kl})$ ($g^{j}_{k}(s_{kl})$) be the probability that the attacker (system) chooses action $a_{i}(x_{[k-T,k}) \in A_{t}$ ($u_{j}(x_{[k-T,k}) \in A_{s}$) at state $s_{kl}\in (X_{[k-T,k]}\times S)$. Define $\mathbf{F}_{k}$ and $\mathbf{G}_{k}$ as the mixed strategy sets of the attacker and the system for stage $k$:
$\mathbf{F}_{k} :=\{\mathbf{f}_{k}= [\mathbf{f}_{k}(s_{k1}), \mathbf{f}_{k}(s_{k2}), \mathbf{f}_{k}(s_{k3})]
|f_{k}^{i}(s_{kl})\geq 0, \mathbf{f}_k \in [0,1]^{M\times 3},
\sum \limits_{a_{ik} \in A_{tk}}f_{k}^{i}(s_{kl}) = 1,\mathbf{f}_{k}(s_{kl})\in \mathbb{R}^{M}, \forall s_{kl} \in(X_{[k-T,k]}\times S)\},$
$\mathbf{G}_{k}:=\{\mathbf{g}_{k}= [\mathbf{g}_{k}(s_{k1}), \mathbf{g}_{k}(s_{k2}), \mathbf{g}_{k}(s_{k3})]|$
$g_{k}^{j}(s_{kl})\geq 0, \mathbf{g}_k \in [0,1]^{N \times 3}, %&\forall u_{jk} \in A_{sk},%k \in \{1,...,K\},\\
\sum \limits_{u_{jk} \in A_{sk}}g_{k}^{j}(s_{kl}) = 1, \mathbf{g}_{k}(s_{kl}) \in \mathbb{R}^{N}, \forall s_{kl} \in (X_{[k-T,k]}\times S)\}. $ Note that $\mathbf{x}_{[k-T,k]}$ provides exogenous information for the strategy $\mathbf{f}_k (\mathbf{g}_k)$, since for every $l$, $\mathbf{f}_{k}(s_{kl}) (\mathbf{g}_{k}(s_{kl}))$ is the strategy at mode $\delta_l$ for the same $\mathbf{x}_{[k-T,k]}$ at stage $k$. Hence, $\mathbf{g}_k$ and $\mathbf{f}_k$ are finite dimensional vectors, that the stationary strategy chosen by each player at stage $k$ depends on the cyber state. %Mixed strategy set $\mathbf{F}_k$ also include the case that the attacker only implement one specific type of attack in the action space at time instance $k$, since we do not have know the strategy of the attacker, we are able to consider all possible combinations of attacks by exploiting mixed strategies. 

%\textbf{}:
%\label{dynamicgame}
%With all the above definition, %for any strategy history $h_{k}$ (a sequence of switching policy), 


\textbf{System and Subsystem Dynamics under game framework}: Given the subsystem and attack models in Section~\ref{sec:replay1} and the game definition,  
%we can transform it to a new system dynamic model decided by both players' actions. 
we show the dynamics at stage $k$ given an action pair $(a_{i}(x_{[k-T,k}),u_{j}(x_{[k-T,k}))$ (assume initial $\mathbf{\hat{x}}_{1|0}=\bar{\mathbf{x}}_{0}$, $\mathbf{x}_{1}=\mathbf{x}_{0}$). Each action pair $(a_{i}(x_{[k-T,k]}),u_{j}(x_{[k-T,k]}))$ defines the corresponding system dynamics at $k$. For instance, when we focus on sensor attacks (like replay or false data injection), let $\mathbf{\gamma}_{k}(a_{i}(x_{[k-T,k]}), u_{j}(x_{[k-T,k]}))$ be the control input with $(a_{i}(x_{[k-T,k]}),u_{j}(x_{[k-T,k]}))$, a subsystem $u_{j}(x_{[k-T,k]})$ with a Kalman filter, an optimal LQG controller has the following dynamics (we denote $(a_{i}(x_{[k-T,k]}),u_{j}(x_{[k-T,k]}))$ as $(a_{ik}, u_{jk})$ for convenience): 
\begin{align}
\begin{split}
&\mathbf{x}_{k}=\mathbf{Ax}_{k-1}+ \mathbf{Bu}_{k-1}+\mathbf{w}_{k-1},\\
& \mathbf{y}_{k}=\begin{cases}a_{1k} = \mathbf{Cx}_k+\mathbf{v}_{k},\ \text{without attack}\\
a_{ik}, i=2,\cdots, M, \ \ \text{with attack,} \end{cases}\\
&\hat{\mathbf{x}}_{k|k-1}= \mathbf{A\hat{x}}_{k-1|k-1}+\mathbf{Bu}_{k-1},\\
%&\mathbf{z}_{k+1}(h_{k},a_{ik},u_{jk})=a_{ik}(h_{k}) - \mathbf{C\hat{x}}_{k+1|k}(h_{k},a_{ik},u_{jk}),\\
&\hat{\mathbf{x}}_{k|k}(a_{ik}) =\hat{\mathbf{x}}_{k|k-1}+ \mathbf{K}(a_{ik} - \mathbf{C\hat{x}}_{k|k-1}),\\
%\end{split}
&\mathbf{\hat{x}}_{k+1|k}(a_{ik},u_{jk})=\mathbf{A\hat{x}}_{k|k}(a_{ik})+\mathbf{B\gamma}_{k}(a_{ik},u_{jk}),\\
& \mathbf{\gamma}_{k}(a_{ik}, u_{jk}) = \mathbf{L\hat{x}}_{k|k} (a_{ik}),\\%+\Delta \mathbf{u}_{k},\\
&\mathbf{z}_{k+1}(a_{ik},u_{jk})=a_{ik} - \mathbf{C\hat{x}}_{k+1|k}(a_{ik},u_{jk}).
\label{dynamicgame}
\end{split}
\end{align}
\textbf{State Transition Probability}: Given a set of subsystem models, define the state transition probability $P$ as a function of the state of the game and both players' actions $P:\ (X_{[k-T,k]}\times S) \times A_{t} \times A_{s}\to [0, 1],$
where
\\\centerline{$
P(s_{(k+1)h}|s_{kl},a_{ik}, u_{jk}), h=1,2,3
$}
%\end{align*}
is the probability that system transits from state $s_{kl}$ to state $s_{(k+1)h}$ at stage $k+1$, given both players' action $(a_{ik},u_{jk})$ at stage $k$. Given the current game state $s_{kl}=(x_{[k-T,k]}, \delta_l)$ and an action pair $(a_{ik},u_{jk})$, the dynamics of the system at stage $k+1$ is described as $x_{[k-T+1,k+1]}$ for all possible cyber modes $\delta_h \in S$, hence the dimension of state transition probability $P(s_{(k+1)h}|s_{kl},a_{ik}, u_{jk})$ is determined by the number of cyber modes of the game. We denote $P(s_{(k+1)h}|s_{kl}, a_{ik}, u_{jk})$ as $P^{ij}(s_{(k+1)h}|s_{kl})$ for short.
%and $\tilde{P}^{ij}(s_{(k+1)h}|s_{kl})$ is the entry at the $i$-th row and $j$-the column  of the state transition matrix $\tilde{P}(s_{(k+1)h}|s_{kl})$ of the game at hybrid state $s_{kl}$.
 As a state transition probability, this function should also satisfy
%\begin{align*}
\\\centerline{$\sum_{\delta_h \in S} {P}^{ij}(s_{(k+1)h}|s_{kl}) = 1,\quad \forall (a_{ik},u_{jk}) \in A_{t} \times A_{s},$}
\\\centerline{$s_{(k+1)h} \in (X_{[k-T+1,k+1]}\times S), s_{kl} \in(X_{[k-T,k]}\times S).$}
%\end{align*}
The transition probability is provided by intrusion detectors of the subsystem. 
%For computational efficiency, we assume that every element of the state transition matrix is a convex function of the system dynamics $x_{[k-T,k]}$ or can be convexified with bounded error. 
%For example, if a $\chi^{2}$ detector is the detector component of subsystem $u_{j}$, we apply~\eqref{alarm} to decide the state transition probability.

\textbf{Immediate Payoff Function}: The immediate payoff matrix at stage $k$ is a $\mathbb{R}^{M\times N}$ matrix for given game state and every action pair $(a_{ik}, u_{jk})$. We define the immediate payoff function as a continuous, convex function of the hybrid game state and the actions of both players
\\\centerline{$r: (X_{[k-T,k]}\times S) \times A_{t} \times A_{s} \to \mathbb{R}^{M \times N},$}
where $r(s_{kl}, a_{ik}, u_{jk}) \geqslant 0$ is the payoff at joint state $s_{kl}$ given action pair $(a_{ik}, u_{jk})$. For definition convenience, we denote ${r}(s_{kl}, a_{ik}, u_{jk})$ as ${r}^{ij}(s_{kl})$ for short, since it is the element on the $i$-th row and $j$-th column of the payoff matrix ${r}(s_{kl})$. It is a zero-sum game between the system and the attacker, and we assume the system is the minimizer and the attacker is the maximizer, hence the payoff function for the attacker and the system is defined as
\centerline{$
{r}^{ij}(s_{kl})={r}_t^{ij}(s_{kl})=-{r}_s^{ij}(s_{kl}).
$}
For instance, when the linear quadratic cost is a metric of system performance, let $\gamma_{k}(a_{ik}, u_{jk})$ be the control input given action pair $(a_{ik}, u_{jk})$, then the payoff function is defined as
\begin{align}
\begin{split}
{r}^{ij} (s_{k1}) =&\mathbb{E}[\mathbf{\hat{x}}^{T}_{k}]\mathbf{W}\mathbb{E}[\mathbf{\hat{x}}_{k}]+\mathbb{E}[\mathbf{\gamma}^{T}_{k}(a_{1k},u_{jk})]\mathbf{U}\mathbb{E}[\mathbf{\gamma}_{k}(a_{1k},u_{jk})],\\
{r}^{ij} (s_{k2}) =&\mathbb{E}[\mathbf{\hat{x}}^{T}_{k}]\mathbf{W}\mathbb{E}[\mathbf{\hat{x}}_{k}]+\mathbb{E}[\mathbf{\gamma}^{T}_{k}(a_{ik},u_{jk})]\mathbf{U}\mathbb{E}[\mathbf{\gamma}_{k}(a_{ik}, u_{jk})],\\
{r}^{ij} (s_{k3}) =& p_f,
\end{split}
\label{payoff}
\end{align}
where $p_f$ is the false alarm trigger penalty, the cost that the system needs to stop execution, check the reason of an alarm, and restart later; $\mathbf{x}_{k}$ is the physical state under the game framework. At mode $\delta_{1}$ the system wins, so the payoff is a normal system payoff with correct sensor data. The larger $p_f$ is, the less probable it is for the system to choose a strategy to transit to state $s_{k3}$.

\textbf{System dynamics update with strategies at stage k}:
 Let $p(s_{kl})$ be the probability system is at state $s_{kl}$ at stage $k$. The initial state distribution $p(s_{1l})$ is given. With  a strategy $\mathbf{f}_{k},\mathbf{g}_{k}$, the attacker and the system randomly sample an action pair $(a_{ik}, u_{jk})$ according to the probability distribution. Then, the control input and sensor value for calculating expectation cost are: 
%\begin{align*}
\centerline{$
\mathbf{u}_{k}=\sum\limits_{j=1}^{N}\sum\limits_{i=1}^{M} \sum\limits_{l=1}^{3}p(s_{kl})f_{k}^{i}(s_{kl})g^{j}_{k}(s_{kl})\mathbf{\gamma}_{k}(a_{ik},u_{jk}),
$}
$\text{ }\quad\quad\mathbf{y}_{k} =\sum\limits_{i=1}^{M}\sum\limits_{l=1}^{3} p(s_{kl})f_{k}^{i}(s_{kl}) a_{ik}.$
%\end{align*}
\\The probability that system is at state $s_{(k+1)h}$ for $k+1$ is:
\\\centerline{$
%\begin{align*}
p(s_{(k+1)h})= \sum\limits_{l=1}^{3}p(s_{kl})[\mathbf{f}_{k}(s_{kl})]^{T}{P}_{k}(s_{(k+1)h}|s_{kl})\mathbf{g}_{k}(s_{kl}). 
%\end{align*}
$}

\iffalse
\begin{align*}
\mathbf{F}_{k} :=\{&\mathbf{f}_{k}= [\mathbf{f}_{k}(s_{k1}), \mathbf{f}_{k}(s_{k2}), \mathbf{f}_{k}(s_{k3})]
|f_{k}^{i}(s_{kl})\geq 0,\\& \mathbf{f}_k \in [0,1]^{M\times 3} %&\forall a_{ik} \in A_{tk},  %k\in \{1,...,K\},\\
\sum \limits_{a_{ik} \in A_{tk}}f_{k}^{i}(s_{kl}) = 1,\mathbf{f}_{k}(s_{kl})\in \mathbb{R}^{M},\\&\forall s_{kl} \in(X_{[k-T,k]}\times S)\},\\
\mathbf{G}_{k}:=\{&\mathbf{g}_{k}= [\mathbf{g}_{k}(s_{k1}), \mathbf{g}_{k}(s_{k2}), \mathbf{g}_{k}(s_{k3})]|
g_{k}^{j}(s_{kl})\geq 0,\\& \mathbf{g}_k \in [0,1]^{N \times 3}, %&\forall u_{jk} \in A_{sk},%k \in \{1,...,K\},\\
\sum \limits_{u_{jk} \in A_{sk}}g_{k}^{j}(s_{kl}) = 1, \mathbf{g}_{k}(s_{kl}) \in \mathbb{R}^{N},\\ &\forall s_{kl} \in (X_{[k-T,k]}\times S)\}. 
\end{align*} 
\fi