\section{Existence of An Optimal Strategy and Suboptimal Algorithm}
%\label{sec:algorithm_finite}

Based on the game formulation, in this section we discuss the existence of an optimal solution for the system, and present an algorithm to compute a suboptimal system strategy.%\PA{what do you mean by ``approximation computing algorithm''}
 
\subsection{Existence of the System's Optimal Strategy}

%Let the total number of game stages, $K$,  be a fixed positive integer, 
We define the concatenation of strategies for $K$-stage game of each player ($\mathbf{f}$ for attacker and $\mathbf{g}$ for system) as%\PA{notation is a bit confusing}
%\begin{align*}
%F^{K}:=\{\mathbf{f}=[\mathbf{f}^1,\cdots, \mathbf{f}^K]\},%\mathbf{f}^{k} \in \mathbf{F}_{k}\},\\ 
%G^{K}:=\{\mathbf{g}=[\mathbf{g}^{1},\cdots,\mathbf{g}^K]\}.%,\mathbf{g}^{k} %\in \mathbf{G}_{k}\}.
%\mathbf{F}=
\vspace{-4.5pt}
\begin{equation*}
\vspace{-4.5pt}
\mathbf{f}=\mathbf{f}_1\cdots \mathbf{f}_K,\mathbf{f}_{k} \in \mathbf{F}_{k},
%\mathbf{G}=
~~~~\mathbf{g}=\mathbf{g}_{1}\cdots\mathbf{g}_K,\mathbf{g}_{k} \in \mathbf{G}_{k}.
\end{equation*}
%\end{align*}
%Let $\zeta_{k}: H_{k} \to S$ denote the projections from $H^{k}$ onto the state space $S$ at stage $k$, then 
Let the random variable $\zeta_{k}$ describe the state of the game at stage $k$, and  
%The transition law $\mathbb{P}^{k}$, a strategy pair $f\in F^{K},g\in G^{K}$ define a conditional probability $P_{fg}(s^{1},.) : S\to A_{t1}\times A_{s1} \times \cdots A_{tK}\times A_{sK}$.$P_{fg} (s^{1},.)$ associates with each initial state $s^{1} \in S$ a probability measure on $H_{K}$, according to theorem of Ionescu-Tulcea ~\cite{Tulcea}. 
let us define the conditional expected total payoff till $\tilde{K}$ for any $\mathbf{f},\mathbf{g}$, given initial state $\zeta_{1}=s$ as%initial state $s$ as: 
\footnotesize
\begin{equation*}\vspace{-5pt}
R_{\tilde{K}}(s, \mathbf{f}, \mathbf{g})%=\sum \limits^{K}_{k=1} \mathbb{E}_{\mathbf{f}, \mathbf{g}}[\tilde{r}(h_k,\zeta_{k})|\zeta_{1} = s],\\
%& = -R_{K}^{s}(s,\mathbf{f}, \mathbf{g}) = R_{K}^{t}(s,\mathbf{f}, \mathbf{g})
%=\sum \limits^{K}_{k=1} \mathbb{E}_{\mathbf{f}, \mathbf{g}}[r(\zeta_{k},\mathbf{f},\mathbf{g})|\zeta_{1} = s],\\
%&\mathbb{E}_{\mathbf{f}, \mathbf{g}} [r(\zeta^{k},\mathbf{f},\mathbf{g})|\zeta^{1} = s]
=\sum \limits^{\tilde{K}}_{k=1}\sum_{l=1}^{3} p(\zeta_{k}=s_{l}|\zeta_{1} = s)[\mathbf{f}_{k}(s_{l})]^{T}\tilde{r}_{k}(h_{k},s_{l})\mathbf{g}_{k}(s_{l}).
\end{equation*}
\normalsize
%where For any given $\zeta_{1}=s$, strategy concatenation $\mathbf{f}, \mathbf{g}$, h_{k}=s\mathbf{f}^{1}\mathbf{g}^{1}\cdots\mathbf{f}^{k-1}\mathbf{g}^{k-1}$,
%\begin{align*}
%\mathbb{E} [r(\zeta_{k},\mathbf{f},\mathbf{g})]=\sum_{l=1}^{3} p(\zeta_{k}=s_{l})[\mathbf{f}^{k}(s_{l})]^{T}\tilde{r}^{k}(h^{k},s_{l})\mathbf{g}^{k}(s_{l}).
%\end{align*}
Since the immediate payoff satisfies $0 \leq \tilde{r}_{k}^{ij}(h_{k},s_{l})< \infty$, we have that $R_{\tilde{K}}(s, \mathbf{f}, \mathbf{g})$ is a nonnegative real-valued, nondecreasing function with $\tilde{K}$. 
%\PA{when you cut an old sentence to save space, make sure that the new one makes sense; for example you should have said - from the immediate payoff definition we have that ...' or ``since...''}  
Furthermore, for finite $K$%a finite stage game
%\PA{shouldn't it be ``for a finite game'' or ``for a finite stage game''}
\vspace{-4pt}
\begin{align}
%\sup_{\mathbf{f} \in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}^{*}) < \infty,s \in S.
%\sup_{\mathbf{f}}
R_{K}(s,\mathbf{f}, \mathbf{g}) < \infty, \forall s, \mathbf{f}, \mathbf{g}.
\label{fR}
\end{align}
%%%%%%%%%%%
%%%%%%%%%%%%
\iffalse
Define the lower (upper) value function $\underbar{$v$}_{K,s}$ ($\bar{v}_{K,s}$) of the stochastic game as:
\begin{align*}
\underbar{$v$}_{K,s}=\sup_{\mathbf{f}\in F^{K}}\inf_{\mathbf{g}\in G^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}),\\
\bar{v}_{K,s}=\inf_{\mathbf{g}\in G^{K}}\sup_{\mathbf{f}\in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g})
\end{align*}  
\fi
%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{definition}[\cite{finite_exist}]
A two-person zero-sum $K$-stage stochastic game 
%$\mathbf{\tilde{R}}(s) = [\tilde{r}(s, ]_{a_{j}\in A_{t}, u_{i} \in A_{s}}$ 
%at a particular state s 
is said to have a value vector $v^{*}_{K}$ if %or any $s\in S$,%\PA{not clear}
\begin{align*}
\begin{split}
&v^{*}_{K,s}=\underbar{$v$}_{K,s}=\bar{v}_{K,s},\ \text{for any}\ s\in S,\ \ \text{where}\\
&\underbar{$v$}_{K,s}= \sup_{\mathbf{f}}\inf_{\mathbf{g}}R_{K}(s,\mathbf{f}, \mathbf{g}),
~~~~\bar{v}_{K,s}=\inf_{\mathbf{g}}\sup_{\mathbf{f}}R_{K}(s,\mathbf{f},\mathbf{g}).
\end{split}
\end{align*}

%A strategy $\mathbf{f}^{*} \in F^{K}$ is called $\epsilon$-optimal for player one (the attacker) of the K-stage stochastic game if  for all $s\in S$
%\[\bar{v}_{K,s} \leq \inf_{\mathbf{g} \in G^{K}}R_{K}(s, \mathbf{f}^{*}, \mathbf{g}) + \epsilon,\]

For the finite value $K$-stage stochastic game, strategies $\mathbf{g}^{*}$ and $\mathbf{f}^{*}$ are called optimal for player two (the system) and player one (the attacker), respectively, if for all $s\in S$\\
%\vspace{-2pt}
%\begin{align*}
%\underbar{$v$}_{K,s} &\geq \sup_{\mathbf{f}}R_{K}(s, \mathbf{f}, \mathbf{g}^{*}) - \epsilon,\\
%\bar{v}_{K,s} &\leq \inf_{\mathbf{g}}R_{K}(s, \mathbf{f}^{*}, \mathbf{g}) + \epsilon.
$v^*_{K,s} = \sup\limits_{\mathbf{f}}R_{K}(s, \mathbf{f}, \mathbf{g}^{*}),%\\ - \epsilon,\\
~~v^*_{K,s} = \inf\limits_{\mathbf{g}}R_{K}(s, \mathbf{f}^{*}, \mathbf{g}). $%+ \epsilon.
%\end{align*}%The 0-optimal strategies are called optimal.
\end{definition}
%\begin{remark}
%The objective of security problems is usually ensuring the performance of the "weakest" link in the system. The value of the game defined in this paper is the minimum of the worst case cost for the system, which is consistent with security goal.
%\end{remark}

%\begin{lemma}%\cite{finite_exist}
The existence conditions of the value and optimal strategies for a general finite horizon zero-sum nonstationary stochastic game are shown in~\cite{finite_exist}. The game defined in this paper has a finite state space, finite action spaces, and satisfies~\eqref{fR}.
%the state transition probability and immediate payoff functions are defined to calculate the expectation total payoff, 
%and $\sup_{\mathbf{f} \in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}^{*})$ is bounded.
%Our game model 
%\PA{adopted?} 
Therefore, using the same approach as in the proofs from~\cite{finite_exist} we can prove the following theorem:

\begin{theorem} 
%The value of the game and an optimal strategy for each player exist for the considered problem formulation.
There exists the value of the considered game and an optimal strategy for the system.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%
%%%%%%%%%%%%%%%
\iffalse
The game we define in this paper has properties: (i) for any stage k, the state space S, action space $A_{tk}$ for attacker and $A_{sk}$ for system are finite sets.   (ii) The state transition probability matrix $\tilde{P}^{k}(s_{h}|h_{k}, s_{l})$ together with the strategy history $h_{k}$ defines a conditional distribution for computing the expected total payoff. %at stage $k+1$, given the strategy history $h_{k} \in H_{k}$ and both players' actions at stage $k$. 
(iii) The immediate payoff $\tilde{r}^{k} (h_{k},s_{l})$ is nonnegative, and conditional expected total payoff function $R_{k}(s,\mathbf{f,g}): H_{K} \to R$ is a bounded below (nonnegative), nondecreasing, for $k\in\{1,\cdots,K\}$. %lower semicontinuous function on $H_{K}$. 
%(iv)In finite time, \[\sup_{\mathbf{f} \in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}^{*}) < \infty,s \in S.\]
The game formulation in this paper can be interpreted by the abstract nonstationary game model in~\cite{finite_exist}, and a detail proof is provided in~\cite{finite_exist}.
%%%%\end{proof}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Suboptimal Algorithm For the Nonstationary Game}

Existing value iterative algorithms for stationary stochastic games can not be used to solve our game, since the nonstationary game parameters depend on the previous history, which is only available in the future algorithm iterations.
Hence, we design a suboptimal algorithm based on the  value iteration method for finite horizon stationary stochastic game from~\cite{plangame} and robust game techniques from~\cite{RGT}. 
Algorithm~\ref{finite} provides an upper bound for the game value and the corresponding nonstationary suboptimal strategy for the system. The idea is to solve a robust game at each iteration step -- i.e., minimize the worst-case caused by extreme points of the nonstationary payoff and state transition probability polyhedra, or $(r_{k},\mathbb{P}_{k})$, defined for all possible histories. 
%Algorithm~\ref{finite} for the finite horizon nonstationary stochastic game 

%The iteration works in the way, that if a player knew how to play in the robust stochastic game optimally from the next stage on, then, at the current stage, he would play with such strategies. Similar to a stationary stochastic game, this value iteration minimizes the maximum expected immediate cost at the current stage and expected costs possibly incurred in future stages.    
 
%
%The value iteration is: %such that for given initial $v^{0}_{s}$ the value of the game at time t+1 is:
%\begin{align*}
 %                     $v^{t}_{s}=\text{value}[\tilde{r}_{s}+\sum_{s'\in S} \tilde{P}(s'|s)v^{t+1}_{s'}].$
%\end{align*}
The value iteration algorithm for finite horizon stationary stochastic game (with fixed payoff $r$ and state transition probability $P$ at every stage) works in the way that if a player knew how to play in the game optimally from the next stage on, then, at the current stage, he would play with such strategies~\cite{plangame}. The value of $K$-stage game is finally provided by the last step of iteration. Similarly, the Algorithm~\ref{finite} of the nonstationary stochastic game starts from the last stage, gets the matrix game value and the optimal strategy related to nonstationary $(r_{k},\mathbb{P}_{k})$  at each stage, and returns an upper bound for the value of the total payoff in $K$-stages. 
To estimate the values %%%%iteration of 
at each step, we consider the payoff and state transition probability set ($r_{k}$, $\mathbb{P}_{k}$) as an uncertain parameter set for the one shot robust game~\cite{RGT}. %Then approximate each iteration value as the value of the robust one shot zero sum game. 

To quantify the bounded polyhedra ($r_{k}$, $\mathbb{P}_{k}$), we need the expected system dynamics $\mathbf{x}_{k}, \mathbf{u}_{k}$, $\mathbf{y}_{k}, k=1,\cdots,K$ defined in Section~\ref{dynamicgame}, which is determined by the strategy history.  
%(this approximation also holds for other attack game quantification model with bounded payoff and transition probability). 
The extreme points for the uncertain set  ($r_{k}$, $\mathbb{P}_{k}$) depend on pure strategy histories. Let $H^p_k \subset H_k$ be the concatenation of pure strategies until stage $k$, where  a pure history $h^{p}_{k} \in H^{p}_{k}$, $h^{p}_{k}=s_{1}\mathbf{f}^{p}_{1}\mathbf{g}^{p}_{1}\cdots\mathbf{f}^{p}_{k-1}\mathbf{g}^{p}_{k-1}$ satisfies that all $f^{p}_{t}(s), g^{p}_{t}(s)$ have only one non-zero element (i.e., they choose the corresponding action or the \emph{pure} strategy).  
By using any $h^{p}_{k}$ in the game model of Section~\ref{dynamicgame}, we get the corresponding $r^p_k(h^p_k,s_l)$ and $P^p_k(h^p_k,s_l)$ for stages $1$ to $K$. Thus, the extreme points set $(r_{kp},\mathbb{P}_{kp})$ for $(r_{k}, \mathbb{P}_{k})$ is:%which we define as: \\
%\vspace{-4pt}
\begin{equation*}
\{(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(h^{p}_{k},s_{l})) |~h^{p}_{k}\in H^{p}_{k},l\in\{1,2,3\}\}.
\end{equation*}
%\vspace{-4pt}
%%%%%%%%%%%%%%%


\iffalse
Now, consider the iteration for calculating the pure history value $v_{k}^{s_{l}}(h^{p}_{k})$ (from extreme points $(r_{kp},\mathbb{P}_{kp})$). %based on $v_{k+1}^{s'}(h^{p}_{k+1})$ of the previous inductive value iteration process at $k+1$.
We denote the non-pure history value of $k$ as $v_{k}^{s_{l}}(h^{p}_{k-1})$, which is calculated from the $(r_k,\mathbb{P}_k)$ that have pure strategies in all stages $1,\cdots,k-2$, and any strategy at stage $k-1$.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%



We define the pure strategy backup matrix set $\mathbb{Q}_{kp}$, $k=1,\cdots,K-1$ as:
%for each $(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(h^{p}_{k},s_{l}))\in (r_{kp},\mathbb{P}_{kp})$,
\vspace{-5pt}
\footnotesize
\begin{align}
\vspace{-5pt}
\mathbb{Q}_{kp}=\{Q_{k}^{p}(h^{p}_{k},s_{l}) = r^{p}_{k}(h^{p}_{k},s_{l})+\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v_{k+1}^{s'}(h^{p}_{k+1})\},
\label{eq:Q}
\end{align}
\normalsize
where $v^{s'}_{k+1}(h^p_{k+1})\geq 0$ is the robust game value resulting from the iteration at stage $k+1$ of~Algorithm~\ref{finite}, and relates to matrix games defined by $\mathbb{Q}_{(k+1)p}$.
 In addition,  we define the backup matrix set~$\mathbb{Q}_{k}$ for all possible~$h_{k}$,  as
 
\vspace{-8pt}
\footnotesize
\begin{align*}
\vspace{-5pt}
\mathbb{Q}_{k}= 
\{&\tilde{Q}_{k}(h_{k},s_{l})=\tilde{r}_{k}(h_{k},s_{l})+\sum_{s'\in S}\tilde{P}_{k}(s'|h_{k},s_{l})v^{s'}_{k+1}(h^p_{k+1})\}.
%&h_{k} \in H_{k}, l=1,2,3\}.
\end{align*} 
\normalsize
Finally, for the stage $K$ we define $Q_{K}^{p}(h^{p}_{K},s_{l})= r^{p}_{K}(h^{p}_{K},s_{l})$ and $\tilde{Q}_{K}(h_{K},s_{l})=\tilde{r}_{K}(h_{K},s_{l})$. 

Now, consider the iteration for calculating $v_{k}^{s_{l}}(h^{p}_{k})$ from all matrix games $Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}$ in Algorithm~\ref{finite}. 
We denote the non-pure history value of $k$ as $v_{k}^{s_{l}}(h^{p}_{k-1})$, which is calculated from the $\tilde{Q}_{k}(h_{k},s_{l})$ that have the same pure strategies as $h^p_k$ in all stages $1,\cdots,k-2$, and any strategy at stage $k-1$.
We use the following result to show that  at every $k$, $v_{k}^{s_{l}}(h^{p}_{k})$ is greater than or equal to $v_{k}^{s_{l}}(h^{p}_{k-1})$.


\begin{theorem}
Consider the value iteration for stage $k$ as a one shot robust game. %given a pure strategy history $h^{p}_{k}$, 
Based on $v_{k+1}^{s'}(h^{p}_{k+1}) \geq 0$ of previous iteration, 
We define the robust game value  obtained at $k$ as %to compute $v^{k-1}_{s}(h^{p}_{k-1})$ at $k-1$ as:
\begin{align}
v^{s_{l}}_{k}(h_{k}^{p})=\max_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}}v^*[Q_{k}^{p}(h^{p}_{k},s_{l})],%v_{k}^{s_{l}}(h_{k}^{p})&=\max_{(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(h^{p}_{k},s_{l}))}
%v^*[r^{p}_{k}(h^{k}_{p},s_{l})+\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v_{k+1}^{s'}(h^{p}_{k+1})],
\label{pickv}
\end{align}
where $v^*$ is the function that yields the value of a zero-sum matrix game. Then for $k=2,\cdots, K$, $v_{k}^{s_{l}}(h^{p}_{k-1})$ is upper bounded by $v_{k}^{s_{l}}(h_{k}^{p})$ (i.e.,~$v^{s_{l}}_{k}(h^{p}_{k-1})\leq v_{k}^{s_{l}}(h_{k}^{p})$).%&\max_{(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(\cdot|h^{p}_{k},s_{l}))\in (r^{kp},\mathbb{P}^{kp})}%\in (M^{p}_{ks_{l}}(h^{p}_{k}), P^{p}_{k}( . |h^{p}_{k},s_{l}))}\\
%\text{value}[r^{p}_{k}(h^{k}_{p},s_{l})+\\
%&\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v^{k+1}_{s'}(h^{p}_{k})].
%\end{align*}
 %and we have exact 4 $v^{k+1}_{s'}(h^{p}_{k})$ for every $s' \in S$ as candidate backup value at every stage k. 
\label{robust}
\end{theorem}


\begin{proof}
%In the following proof every $h_{k}^{p}$ is a branch for the same $h^{p}_{k-1}$, $h_{k}=h_{k-1}^{p}\mathbf{f}^{k}\mathbf{g}^{k}$, and we omit the subscript $h^{p}_{k-1}$. 
\iffalse
The backup matrix set $\mathbb{Q}_{k}$ for all possible $h_{k}$ is %for $s_l$ :
\begin{align*}
%Q_{k}= 
\{&\tilde{Q}_{k}(h_{k},s_{l})=\tilde{r}_{k}(h_{k},s_{l})+\sum_{s'\in S}\tilde{P}_{k}(s'|h_{k},s_{l})v^{s'}_{k+1}(h_{k+1}^{p})\},
%&h_{k} \in H_{k}, l=1,2,3\}.
\end{align*}
\fi
%%%%%%%%%%%%%%%%%%%%%
%$\tilde{Q}_{k}\in \mathbb{Q}_k$. 
Since $v^{s'}_{k+1}(h_{k+1}^{p})$ is a nonnegative scalar, the extreme points of $\mathbb{Q}_{k}$ can only come from the extreme points of the tuple $(r_{k}, \mathbb{P}_{k})$, i.e., by considering the matrix game value of every $Q_{k}^{p} (h_{k}^{p},s_{l})$ defined in~\eqref{eq:Q}, we will get the maximum value from extreme points of $\mathbb{Q}_k$. 
%\[\tilde{Q}_{k}^{p}=\{ Q_{k}^{p} (h_{k}^{p},s_{l}), (r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(h^{p}_{k},s_{l}))\in (r^{kp},\mathbb{P}^{kp})\}.\] 
%we only need to consider every extreme point  $(r^{p}_{k}(h^{p}_{k},s_{l}), P^{p}_{k}(\cdot|h^{p}_{k},s_{l}))$ in
% 
Now consider the following optimization problem for the system~\eqref{optr} and for any attacker's strategy vector $\mathbf{f}$
%From the system's perspective, to compute the value of the matrix game is equivalent with:
\vspace{-5pt}
\begin{align}
%\begin{split}
\vspace{-5pt}
\min_{\mathbf{g}} \quad & z\label{ob_z}\\
\text{subject to}\quad &z \geq \max_{\tilde{Q}_{k}(h_{k},s_{l})\in \mathbb{Q}_{k}}\mathbf{f}^{T} [\tilde{Q}_{k}(h_{k},s_{l})]\mathbf{g}.
%\end{split}
\label{optr}
\end{align}
As proven by~Lemma 5 in \cite{RGT},~\eqref{optr} is equivalent to the following constraint that considers only the extreme points
\begin{align}
\quad z \geq \max_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}}\mathbf{f}^{T} [Q_{k}^{p}(h^{p}_{k},s_{l})]\mathbf{g},
\end{align}
%meaning that it is sufficient to consider only the extreme points.


For the worst-case $f$, the above is also true. Hence, let 
\begin{align}
\label{eq1}
v^{s_{l}}_{k}(h_{k}^{p})
%=\max_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}}v^*[Q_{k}^{p}(h^{p}_{k},s_{l})]
=\max_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}} \min\limits_{\mathbf{g}}\max\limits_{\mathbf{f}}\mathbf{f}^T[Q_{k}^{p}(h^{p}_{k},s_{l})]\mathbf{g}.
%\label{pickv}
\end{align}
For optimal policies $\mathbf{f}^{*}(h^{p}_{k},s_{l})$ and $\mathbf{g}^{*} (h^{p}_{k},s_{l})$, the above optimization \eqref{eq1} results in cost $\max\limits_{Q_{k}^{p}(h^{p}_{k},s_{l})\in \mathbb{Q}_{kp}}v^*[Q_{k}^{p}(h^{p}_{k},s_{l})]$. 
%we can compute the value from four different $\tilde{Q}_{ks_{l}}^{p}(h^{p}_{k})$separately, and get the value follow the pure strategy $v^{k}_{s'}(h^{p}_{k-1})$. 
However, $(\mathbf{f}^{*}(h^{p}_{k},s_{l}),\mathbf{g}^{*}(h^{p}_{k},s_{l}))$ can be non-pure strategies, meaning that when used in~\ref{dynamicgame}, they will not result in extreme points of $\mathbb{Q}_{k+1}$. The non-pure history value $v_{k+1}^{s'}(h_{k}^{p})$ of iteration for stage $k+1$ satisfies %%%%%(from $\mathbb{Q}_{(k+1)p}$):
%\begin{align*}
$
v_{k+1}^{s'}(h_{k}^{p})\leq v_{k+1}^{s'}(h_{k+1}^{p}). 
$
%\end{align*}
Replacing $v_{k+1}^{s'}(h_{k+1}^{p})$ by $v_{k+1}^{s'}(h_{k}^{p})$ in~\eqref{eq:Q} will decrease every element in matrix $Q^{p}_{k}$, since $r_{k}^{ij}\geq0$ and $P_{k}^{ij}\geq 0$. %%% by definition. 
%The system is a minimizer, and is possible to get a smaller value at time k, with some %optimal strategy history $h^{*}_{k} \neq h^{p}_{k}$, 
With a similar argument in the next iteration for stage $k-1$, we have $v_{k}^{s_{l}}(h^{p}_{k-1})\leq v^{s_{l}}_{k}(h_{k}^{p})$. 
%(Note that here $v_{k}^{s_{l}}(h^{p}_{k-1})$ is not the game value at $k$ either, because the strategies from 1 to $k-2$ must be pure to get it).
\end{proof}
%\squeezeup
%\vspace{-5pt}
\begin{algorithm}
%\vspace{-5pt}
\caption{\textbf{: Suboptimal Algorithm for A Finite Nonstationary  Stochastic Game }}
\textbf{Input}: System model parameters and game parameters.
\\\textbf{Initialization}:
             Compute the set of $(r_{kp}, \mathbb{P}_{kp})$ for $k=1,...K$.
              %For all $s_{l} \in S, l =1,2,3, h^{p}_{K}\in H^{p}_{K} :$ get %$4^{K-1}$ 
              %backup matrix set $\{Q_{k}^{p}(h_{k}^{p},s_{l})\}$:
              
             Compute the game at stage $K$: $Q_{K}^{p}(h^{p}_{K},s_{l})=r_{K}^{p}(h^{p}_{K},s_{l})$, \\ 
                    $f^{*}(h^{p}_{K}, s_{l}), g^{*}(h^{p}_{K}, s_{l}), v_{K}^{s_{l}}(h^{p}_{K}) \leftarrow \pi(Q_{K}^{p}(h^{p}_{K},s_{l}).$
\\\textbf{Iteration}: For $k=(K-1), \cdots, 1$,

get the backup matrix set $\mathbb{Q}_{kp}$ for all $h^{p}_{k}\in H^{p}_{k}$, where each matrix is defined in~\eqref{eq:Q}, then calculate:
%\[Q_{k}^{p}(h^{p}_{k},s_{l}) = r^{p}_{k}(h^{p}_{k},s_{l})+\sum_{s'\in S} P^{p}_{k}( s' |h^{p}_{k},s_{l})v_{k+1}^{s'}(h^{p}_{k+1}),\]

 $f^{*}(h^{p}_{k}, s_{l}), g^{*}(h^{p}_{k}, s_{l}),v_{k}^{s_{l}}(h^{p}_{k}) \leftarrow \pi(Q_{k}^{p}(h^{p}_{k},s_{l}))$.
 
%$\mathbf{f}^*_k=[\mathbf{f}^{*}(h^{p}_{k}, s_{l}),\mathbf{f}^{*}(h^{p}_{k}, s_{2}), \mathbf{f}^{*}(h^{p}_{k}, s_{3})],$ $ \mathbf{g}^{*}_k=[\mathbf{g}(h^{p}_{k}, s_{1}),\mathbf{g}(h^{p}_{k}, s_{2}),\mathbf{g}(h^{p}_{k}, s_{3})].$ 

$\mathbf{f}^*_k=[\mathbf{f}^{*}(h^{p}_{k}, s_{l}),l=1,2,3],$ $ \mathbf{g}^{*}_k=[\mathbf{g}^*(h^{p}_{k}, s_{l}), l=1,2,3].$ 

\textbf{Return}: the strategy concatenation pair $\mathbf{f}_{a}=\mathbf{f}^{*}_1\cdots\mathbf{f}^*_K,\mathbf{g}_{a}=\mathbf{g}^{*}_1\cdots\mathbf{g}^*_K$ and the value upper bound $v^{s_{l}}_{1},l=1,2,3$.
\label{finite}
%\vspace{-5pt}
\end{algorithm}
%\squeezeup

According to~Theorem~\ref{robust}, we use Algorithm~\ref{finite} to compute an upper bound of the value and the corresponding suboptimal strategy for every step. The % Nash Equilibrium selection 
function $\pi$ computes the strategy and robust value as defined in~\eqref{pickv}. Note that for the replay attacks considered in this paper, at state $s_{1}$ the system wins the game and replay is harmless. Thus, the optimal strategies of $s_{1}$ is $\mathbf{g}_{k}(s_{1})=\begin{bmatrix}1&0\end{bmatrix}^{T}$, $\mathbf{f}_{k}(s_{1})=\begin{bmatrix}1&0&\cdots&0\end{bmatrix}^{T}$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%the Input can be: $\mathbf{A,B,C,Q,R,\bar{x}_{0}},\Sigma,$ $\mathbf{K},$ $\mathbf{W,U,L}$, $\tau, \alpha, \Delta \mathbf{u}_{k}\sim N(0,\mathcal{L})$; replay window size $\{t_{2},\cdots,t_{M}\}$, total stage number $K$, system initial state distribution $p(s^{1})$, false alarm trigger penalty.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The nonstationary game values $v_k^{s_l}(h^p_{k-1})$ and $v_k^{s_l}(h^p_k)$ that result from value iteration of two strategies only differ at stage $k-1$ (i.e.,~same and pure from stages 1 to $k-2$). By value iteration backward to stage 1, we compare the game value (for all possible strategies) and the robust game value $v_1^{s_l}$ of~Algorithm~\ref{finite} in the following theorem.
\begin{theorem}
%Moreover, by following this iteration method,
~Algorithm~\ref{finite} results  in an upper bound $v_{1}^{s_{l}}$ for the value of the $K$-stage game, together with suboptimal strategies $\mathbf{f}_{a}$ and $\mathbf{g}_{a}$. 
\end{theorem}
\vspace{3pt}
\begin{proof}
The strategies $\mathbf{f}_{a}, \mathbf{g}_{a}$ of~Algorithm~\ref{finite} are possibly not pure. %strategy history $h^{p}_{K} \in H^{p}_{K}$. 
According to~Theorem~\ref{robust}, $v_{k}^{s_{l}}(h^{p}_{k-1})\leq v^{s_{l}}_{k}(h_{k}^{p})$, %(may write this proof more detailed depends on the paper requirement)
and the proof holds for every $k=2,\cdots,K$.  \iffalse Using induction,\fi Consider the value iteration for $k=1$, with $v_{2}^{s_{l}}(h^{p}_{1})\leq v^{s_{l}}_{2}(h_{2}^{p})$, 
\footnotesize
\begin{equation*}
\vspace{-5pt}
Q_1^{ij}(h_1, s_l)=\tilde{r}_1^{ij}(h_1,s_l)+\sum\limits_{s'\in S}\tilde{P}^{ij}_{1}(s'|h_{1},s_{l})v^{s'}_{2}(h^p_{1})\leq Q_1^{ij}(h_2^p,s_l),
\end{equation*}
\normalsize
thus $v^{*}[Q_1(h_1, s_l)] \leq v^{s_{l}}_{1}$.  Iterative value based on pure strategy back up matrix sets $\mathbb{Q}_{kp}, k=1,\cdots, K$ obtained from~Algorithm~\ref{finite} is an upper bound for the game value.
\end{proof}

\iffalse
\begin{remark} %%%%%\FM{remove this remark to the extension paper?}
Algorithm~\ref{finite} is exponential with K, and for large control system running time $\tilde{T}$, the game stage $K$ does not need to be $\tilde{T}$--% We can formulate a $K$-stage game, 
each stage we compute $\tilde{T}/K$ steps of control system dynamic, and apply the average cost and transition probability to quantify the game parameters. 
%keeps the same during each stage ($\tilde{T}/K$ time).  
In particular, if we consider the infinite horizon problem, the game formulation can reduce to stationary. 
For example, we can define the infinite time average LQG cost as the payoff function, and the average detection rate/false alarm trigger rate as state transition probability, and find the optimal strategy of a stationary stochastic game.
%Or assume the replay window size does not change during K.
\end{remark}%\FM{we can put this remark at the end of section 3?}
\fi

\iffalse
Besides finding the optimal or suboptimal strategy, we can also use the stochastic game framework to find the `best reply' strategy for a specific attacker behavior. For example, the attacker's strategy can be to always apply a certain type of attack before being detected. When we want a faster iteration, we can calculate the best reply strategy with respect to one attack type from the attacker's action space, run the algorithm for a different action type each time, 
and find the worst-case cost. This approximation is reasonable, since when we do not have expectation that a certain attack type will appear, then our goal is to keep the system performance acceptable under the worst-case attack events.\fi

  %$(4\times2)$ action space (i.e., the attacker has 4 actions and the system has 2 actions).
  %This is because the computation complexity of the real-time algorithm grows linearly with the total stage number $K$, whereas the suboptimal algorithm is exponential with respect to $K$. 
  %As shown in Table~\ref{alg_compare}, the computation overhead of the suboptimal algorithm in~\cite{cdc_replay} increases faster with the total stage $K$ than that of Algorithm~\ref{finite_new}.

\iffalse
To get a suboptimal total payoff of time $\tilde{T}$ faster, we can view this as a $K$ stage game, and at each stage the physical dynamics run $[\tilde{T}/K]$ steps as~\eqref{eq:xu}. The average payoff and state transition probability in the $[\tilde{T}/K]$ discrete time slots are the payoff and state transition probability of one game stage.  
In particular, when we consider the average of infinite horizon time $\tilde{T}$, the game can reduce to a stationary stochastic game. For example, with the infinite time average LQG cost as the payoff function, and the average detection rate, false alarm trigger rate as the state transition probability, we actually reach the optimal strategy of a stationary stochastic game.  
\fi

%For the discrete time switched system switches among stable and unstable subsystems, where the transition probability is determined by the game strategy, the stability property still needs future analysis.
\iffalse the system is still Markov jump, and properties of piecewise homogeneous Markov jump systems are discussed in~\cite{H_mj}. \fi

\iffalse There are several stable conceptions in literature, like the means square exponentially stable and mean square stable. \fi
\iffalse
\begin{definition}
The stochastic differential Equation is said to be exponentially stable in mean square if there is a pair of positive constants $\lambda$ and $M$ such that, for all initial data $x_0$, such that
\begin{align}
\mathbb{E}\|x_k\|^2 \leq M\mathbb{E}\|x_0\|^2 e^{-\lambda k}, \text{for all}\ k\geq 0.
\end{align}
\end{definition}

\begin{definition}
The stochastic differential equation is said to be mean square stable if 
\begin{align}
\lim\limits_{k\to\infty}\mathbb{E}\|x_k\|^2=0, \text{for any initial}\ \mathbf{x}_0, \sigma_0,
\end{align}
where $\sigma_0$ is the probability distribution of the initial subsystem. 
\end{definition}
\fi

%%%%%%%
%%%%%%%%%%
\iffalse
\begin{theorem}
When the Markov jump discrete time system is mean square stable, the nonstationary stochastic game has a stationary strategy as $k\to\infty$. 
\end{theorem}
\begin{proof}
\end{proof}
\fi
%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{proof}
Consider the pure strategy of always applying the $j$-th controller (detector) at each stage for any attacker's strategy as a subsystem. Then the stochastic game strategy for the system is equivalent with randomly switching between $N$ controllers/detectors with any linear combination of probability $g_j, j \in \{1,\cdots, N\}, \sum_{j} g_j =1$, for any joint state. We assume every subsystem that equipped with a controller/detector can make the closed loop system mean square exponentially stable, even under worst case attack strategies (i.e., attacks can increase the system cost, but can not distablize the system), then for system always applying controller $j$, there exists a pair of positive constants $\lambda_j$ and $M_j$ such that, for all initial data $x_0$, such that
\begin{align}
\mathbb{E}\|x_k\|^2 \leq M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}, \text{for all}\ t\geq 0.
\end{align}
Then let $\lambda = min_{j \in N} \{\lambda_j\}, M= N max_{j\in N} \{M_j\}$, since $0\leq g_j \leq 1, 0\leq g_j^2\leq 1$,
\begin{align}
\begin{split}
\mathbb{E}\|\sum_j g_j x_k\|^2 &\leq \sum_j g_j^2 M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}\\
&\leq \sum_j M_j\mathbb{E}\|x_0\|^2 e^{-\lambda_j k}\\
&\leq M \mathbb{E}\|x_0\|^2 e^{-\lambda k},
\text{for all}\ k\geq 0.
\end{split}
\end{align}
The switching system according to the game strategy is also exponentially mean square stable.
\end{proof}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{proposition}
If the control input $\gamma(a_{ik}, u_{jk})=L_j\hat{\mathbf{x}}_{t}, t\in[k-T, k]$ for any action pair $(a_{ik}, u_{jk})$, then the system is not asymptotically stable when the strategy sequences $\mathbf{f}_k^*(s_{kl})$, $\mathbf{g}_k^*(s_{kl})$ of the stochastic game are not convergent.
\end{proposition}
%\begin{proof}

Sketch of the proof: 
When the system is asymptotically stable, $\lim_{k\to\infty}\mathbf{Ex}_k=0$, by the definition of immediate payoff function~\eqref{payoff}, we gave $$
After calculating the strategies at stage $k$, the next step of the Algorithm~\ref{finite_new} is updating the system dynamics, and then getting the auxiliary matrices $Q_{k}(s_{(k+1)l}),l=1,2,3$ for stage $k+1$. 
%\end{proof}
\fi

\iffalse After calculating the strategies at stage $k$, the next step of the Algorithm~\ref{finite_new} is updating the system dynamics, and then getting the auxiliary matrices $Q_{k}(s_{(k+1)l}),l=1,2,3$ for stage $k+1$. 
The saddle-point equilibrium strategies at stage $k+1$ are decided by $Q_{k}(s_{(k+1)l}),l=1,2,3$. 
Thus, if updating the system dynamics for the auxiliary matrix $Q_{k}(s_{kl})$ and at each mode $Q_{k}(s_{kl})$  converges with an optimal strategy for both players, \fi


\iffalse
If $(A+BL)(I-KC) \triangleq \mathcal{A}$ is stable, the $\chi^2$ detector is useless, since 
the expectation of detector statistics for the compromised residues $\mathbf{z}'_k=\mathbf{y}_{k-T}-\mathbf{C\hat{x}}_{k|k-1}$ will converge to the same value as that for $\mathbf{z}_k=\mathbf{y}_{k} -\mathbf{C\hat{x}}_{k|k-1}$~\cite{replay}
\begin{align}
\begin{split}
&\lim \limits_{k \to \infty} \mathbb{E}[(\mathbf{z}'_k)^{T} \mathcal{P}^{-1}\mathbf{z}'_k]=\lim \limits_{k \to \infty} \mathbb{E}[(\mathbf{z}_k)^{T} \mathcal{P}^{-1}\mathbf{z}_k]=m
%&\lim \limits_{k \to \infty} \mathbb{E}[(\mathbf{y}_{k} -\mathbf{C\hat{x}}_{k|k-1})^{T} \mathcal{P}^{-1} (\mathbf{y}_{k} - \mathbf{C\hat{x}}_{k|k-1})]=m
%&\lim \limits_{k \to \infty} E[(\mathbf{y}_{k-T} - \mathbf{C\hat{x}}_{k|k-1})^{T} \mathcal{P}^{-1} (\mathbf{y}_{k-T} - \mathbf{C\hat{x}}_{k|k-1})]=m,
\end{split}
\label{Er}
\end{align}
\normalsize
\fi

\iffalse the Kalman filter residue $\mathbf{z}'_k$ satisfy:
\begin{align*}
\begin{split}
&\lim \limits_{k \to \infty} \mathbb{E}[(\mathbf{z}'_k)^{T} \mathcal{P}^{-1}\mathbf{z}'_k]=m + 2 trace(\mathbf{C}^{T}\mathcal{P}^{-1}\mathbf{C}\mathcal{U}), \ \ \text{while}\\
&\lim \limits_{k \to \infty} \mathbb{E}[(\mathbf{z}_k)^{T} \mathcal{P}^{-1}\mathbf{z}_k]=m,
\mathcal{U} - \mathbf{B}\mathcal{L} \mathbf{B}^{T} = \mathcal{AUA}^{T}.
\end{split}
\end{align*}
\fi


\iffalse
The game we define in this paper has properties: (i) for any stage k, the state space S, action space $A_{tk}$ for attacker and $A_{sk}$ for system are finite sets.   (ii) The state transition probability matrix $\tilde{P}^{k}(s_{h}|h_{k}, s_{l})$ together with the strategy history $h_{k}$ defines a conditional distribution for computing the expected total payoff. %at stage $k+1$, given the strategy history $h_{k} \in H_{k}$ and both players' actions at stage $k$. 
(iii) The immediate payoff $\tilde{r}^{k} (h_{k},s_{l})$ is nonnegative, and conditional expected total payoff function $R_{k}(s,\mathbf{f,g}): H_{K} \to R$ is a bounded below (nonnegative), nondecreasing, for $k\in\{1,\cdots,K\}$. %lower semicontinuous function on $H_{K}$. 
%(iv)In finite time, \[\sup_{\mathbf{f} \in F^{K}}R_{K}(s,\mathbf{f}, \mathbf{g}^{*}) < \infty,s \in S.\]
The game formulation in this paper can be interpreted by the abstract nonstationary game model in~\cite{finite_exist}, and a detail proof is provided in~\cite{finite_exist}.
%%%%\end{proof}
\fi