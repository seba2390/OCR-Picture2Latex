%\vspace{-0.1in}
\section{Preliminary results}
\label{sec:results}

DREMS\footnote{\vspace{-0.1in}http://www.isis.vanderbilt.edu/drems} is a software
infrastructure for designing, implementing, configuring, deploying and
managing distributed real-time embedded systems that consists of two
major subsystems: (1) a design-time toolsuite for modeling, analysis,
synthesis, implementation, debugging, testing, and maintenance of
application software built from reusable components, and (2) a
run-time software platform for deploying, managing, and operating
application software on a network of computing nodes. The platform is
tailored towards a managed network of computers and distributed
software applications running on that network of nodes, i.e. a cluster.

The toolsuite supports a model-based paradigm of software development
for distributed, real-time, embedded systems where modeling tools and
generators automate the tedious parts of software development and also
provide a design-time framework for the analysis of software
systems. The run-time software platform reduces the complexity and
increases the reliability of software applications by providing
reusable technological building blocks in the form of an operating
system, middleware, and application management services.

\subsection{DREMS Architecture}

DREMS is a complete, end-to-end solution for software development:
from modeling tools to code to deployed applications. It is open and
extensible, and relies on open industry standards, well-tested
functionality and high-performance tools. It focuses on the
architectural issues of the software, and promotes the modeling of
application software, where the models are directly used in the
construction of the software.

Software applications running on the DREMS platform are distributed:
an application consists of one or more \textit{actors} that run in parallel,
typically on different nodes of a network. Actors specialize the
concept of processes: they have identity with state, can be
migrated from node to node.  Actors are created,
deployed, configured, and managed by a special service of the run-time
platform: the deployment manager - a privileged, distributed, and
fault tolerant actor, present on each node of the system, that
performs all management functions for application actors. An actor can
also be assigned a set of limited resources of the node on which it runs:
memory and file space, a share of CPU time, and a share of the network
bandwidth.

Applications are built from software components - hosted by actors -
that only  interact via  well-defined interaction patterns using
security-labeled messages, and are allowed to use specific sets of
services provided by the operating system, including messaging and 
thread synchronization operations. Note that components use these 
indirectly: via the middleware.

The middleware libraries implement the high-level communication
abstractions: synchronous and asynchronous interactions, on top of the
low-level services provided by the underlying distributed hardware
platform. Interaction patterns include (1) point-to-point interactions
(in the form of synchronous and asynchronous remote method
invocations), and (2) group communications (in the form of
anonymous publish-subscribe interactions). Component operations can
be event-driven or time-triggered, enabling time-driven
applications. Message exchanges via the low-level messaging services
are time-stamped, thus message receivers are aware of when the message
was sent. Hence temporal ordering of events can be established
(assuming the clocks of the computing nodes are synchronized).

Specialized, verified platform actors provide system-wide high-level
services: application deployment, fault management, controlled access
to I/O devices, etc. Each application actor exposes the interface(s)
of one or more of its components that the components of applications
can interact with using the same interaction patterns. Applications
can also interact with each other the same way: exposed interfaces and
precisely defined interaction patterns.

The DREMS Operating System - a set of extensions to the Linux kernel -
implements all the critical low-level services to support resource
sharing (including spatial and temporal partitioning), actor management,
secure (labeled and managed) information flows, and fault tolerance. A
key feature of the OS layer is support for temporal partitions
(similar to the ARINC-653 standard): actors can be assigned to a
fixed duration, periodically repeating interval of the CPU's time so
that they have a guaranteed access to the processor in that
interval. In other words, the actors can have an assured bandwidth to
utilize the CPU and actors in separate temporal partitions cannot
inadvertently interfere with each other via the CPU.


\vspace{-0.04in}
\subsection{Run-time Software Platform}

The implementation of the run-time software platform has several layers. 
Practically all layers are based on existing and proven open-source
technology. Starting from the bottom, the operating system layer
extends the Linux kernel with a number of specific services, but it
strongly relies on the code available in the Linux kernel (currently:
version 3.2.17). This permits the use of DREMS services for the actors,
but also keeps the Linux system calls for debugging and monitoring 
purposes. These extensions are in the form of 120+ new system
calls.

The C and C++ run-time support libraries (based on uClibc\footnote{\url{www.uclibc.org}}
and libstdcpp\footnote{\url{http://gcc.gnu.org/libstdc++/}} 
implement the conventional
support services needed by the typical C and C++ programs. The C
run-time library has entry points to access the DREMS OS system
calls. These calls utilize data structures that have been defined
using the standard OMG Interface Definition Language (IDL), and can be
created and manipulated using generated constructor and manipulation
operators. The implementation of the DREMS operating system calls
checks the integrity of all data structures passed on the
interface. This enables validation of the data structures on the
interface, preventing potential abuse of the system calls.

Layered on the C and C++ run-time libraries the Adaptive Communication
Environment (ACE) libraries provide a low-overhead isolation layer for
the higher level middleware elements that support CORBA and DDS. The
CORBA implementation is based on The ACE ORB (TAO, currently: version
6.1.4) that implements a subset of the CORBA standard for facilitating
point-to-point interactions between distributed objects. Such
interactions are in the form of Remote Method Invocations (RMIs) or
Asynchronous Method Invocations (AMIs). RMIs follow the call-return
semantics, where the caller waits until the server responds, while the
AMIs follow the call-return-callback semantics, where the caller
continues immediately and the response from the server is handled by a
registered callback operation of the client. The CORBA subset
implemented by the middleware has been selected to support a minimal
set of core functions that are suitable for resource-constrained
embedded systems.  The DDS implementation is based on the OpenDDS
(currently: version 3.4) that implements a subset of the DDS standard
for facilitating anonymous publish/subscribe interactions among
distributed objects. 
%In these interactions publishers send typed
%messages of specific topics via the middleware which then distributes
%them to subscribers interested in those topics. Subscribers can be
%anywhere on the network, they can join and leave the system at any
%time - the distribution middleware decouples publishers from the
%subscribers. 
There are several quality-of-service attributes
associated with publishers and subscribers that control features like
buffering, reliability, delivery rate, etc. DDS is designed to be
highly scalable, and its implementations meet the requirements of
mission-critical applications.

CORBA and DDS provide for data exchange and basic interactions between
distributed objects, but in DREMS objects are packaged into
higher-level units called components. A component
\cite{ISIS_F6_ISORC:13} publishes and subscribes to various topics
(possibly many), implements (provides) interface(s), and expects
(requires) implementations of other interfaces. Note that a component
may contain several, tightly coupled objects.  Components may expose
parts of their observable state via read-only state variables,
accessible through specific methods. Components are configured via
configurable parameters. Their operations are
scheduled based on events or elapse of time. An event can be the
arrival of a message the component has subscribed to or an incoming
request on a provided interface. Time triggering is done by
associating a timer with the component that invokes a selected
operation on the component when a set amount of time elapses,
possibly periodically repeating the operation. Component operations
can perform computations, publish messages, and call out to other
components via the required interfaces. To avoid having to write
complex locking and synchronization logic for components, component
operations are always single threaded: inside of one component at most
one thread can be active at any time.  Actors are formed from
interacting components, and applications are formed from actors that
interact with each other via their interacting components. Actors
(together with their components) can be deployed on different nodes of
a network, but their composition and interactions are always clearly
defined: they must happen either via remote method invocations or via
publish/subscribe interactions.

\begin{figure}[t]
\centering \includegraphics[width=0.9\columnwidth]{pfigs-comps}
\caption{Component-based distributed system example}
\label{fig:component_architecture}
\vspace{-0.1in}
\end{figure}

Figure~\ref{fig:component_architecture} shows an application where a
Sensor component periodically (P) publishes a message that a GPS
component subscribes to, and which, in turn, sporadically (S) publishes
another message that a NAVDisplay component consumes. This last
component invokes the GPS component via a provided interface, when it
needs to refresh its own state. The messages published can be quite
small, while the method invocation (that happens less frequently, and
on demand) may transfer larger amounts of data. 
%The number of possible
%combination of interactions among components is quite large, but each
%interaction pattern is precisely defined, allowing the application
%writer to understand all operational scenarios. Note that applications
%are multi-threaded, and only individual components are single threaded.

%Interactions are realized by connectors~\cite{Connectors} that support
%specific interaction patterns. In addition to the two main ones
%described above, components can interact with network sockets (for
%conventional message oriented networking using POSIX standard socket
%APIs), timers, and I/O devices. For each case, the synchronization
%between component code execution and the events of the external world
%are precisely defined, and allow the implementation of various
%interactions to enable high degree of asynchrony and responsiveness.

The run-time software platform includes a key platform actor: the
Deployment Manager (DM) that instantiates, configures, activates, deactivates,
and dismantles applications. Every node on a network has a copy of the DM that acts
as a controller for all applications on that node. The DMs communicate
with each other, with one being the lead  `cluster' DM. This, cluster
leader orchestrates the deployment of applications across cluster
with the help of the node DMs. For deployment, the binaries of
application components should be installed on each node, then the
cluster lead DM is provided a deployment plan that is generated from
application models and executes the plan, coordinating the activities
of node level DMs which start the actors, installs components,
configures the network connections among the components, etc., and
finally activates the components. This last step releases the
execution threads of the components. When the applications need to be
removed, the DM stops the components, withdraws the network
configuration, and stops the actors. A key feature of the deployment
process is that the network connections among the parts: i.e. actors
and components of the distributed application are managed: the
application business logic does not have to deal with this problem;
everything is set up based on the deployment plan.

\subsection{Design-time Development Platform}
Configuring the middleware and writing code that takes advantage of
the component framework is a highly non-trivial and tedious task. To
mitigate this problem and to enable programmer productivity, a
model-driven development environment is available that simplifies the
tasks of the application developers and system integrators.

In this environment, developers define via graphical and textual
models various properties of the application, including: interface and
message types, components types (in terms interfaces and
publish/subscribe message types), component implementations, component
assemblies, and applications (in terms interacting components and
actors containing them). Additionally, the hardware platform for the
cluster are modeled: processors, network and device interfaces,
network addresses, etc. Finally, the deployment of the application(s)
on the hardware platform are also modeled, as the mapping of actors onto
hardware nodes, and information flows onto network links. 
%The deployment can change over time but the deployment is stable and static for most of the time.  
Models are processed by code generators
that produce several artifacts from them: source code, configuration
files, build system artifacts that facilitate the automated
compilation and linking of the components, and other documents. The
application developer is expected to provide the component
implementation code in the form of C++ code (currently, in the future:
any other, supported executable language) and add it to the generated
code. The compilation and debugging of the applications can happen
with the help of a conventional development environment
(currently: Eclipse) that supports editing, compiling, and debugging
the code.  The result of this process is a set of component
executables and the deployment plan - ready to be deployed on a
cluster of nodes.

The model-driven approach has several benefits. (1) The model serves
as the single source of all structural and configuration information
for the system. (2) The tedious work of crafting middleware `glue'
code and configuration files for deployment is automated: everything
is derived programmatically from the models. (3) The models provide an
explicit representation of the architecture of all the applications
running on the system - this enables architectural and performance
analysis on the system before it is executed. (4) Models can also be
used for rapidly creating `mockup' components and applications for
rapid prototyping and evaluation.

\subsection{Example: Cluster flight control and sensor processing}

We have evaluated the DREMS prototype on several examples. The graphical modeling
tool runs on Windows, the code development and cross-compilation tools on a Linux platform, 
while DREMS is on a set of networked embedded x86-based devices (3 iBX-530 industrial computers). 
Deployment and configuration is done from the Linux machine, via the network. 
Several small scale tests were used to validate that the platform is functional. A more
realistic application involved a distributed flight control software applications (2 actors on each node,
with 3-4 components each), and a sensor processing application (dissimilar actors on each node). The flight control
actors share critical, but low-bandwidth data, the sensor application shares high bandwidth, but low criticality data. The two sets of applications run in different security domains, and in different temporal partitions. We were scheduling the applications in partitions of 100 msec duration, and were experimenting with variable bandwidth between the nodes. All designed and implemented features were functional, including component interactions, partition scheduling, security labeling and information flow separation, application deployment and control. The applications have been constructed using the model-driven development toolchain; the model had about 100 distinct elements. Component code was hand inserted into the skeleton code generated by the software generators, followed by compilation using Eclipse with a cross-compiler for the platform. The model-driven generation produced all the infrastructure code, simplifying the task of the developer. A detailed report of this experiments can be found in \cite{drems-rtss-2013}.
