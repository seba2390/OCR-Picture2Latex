\section{Platform architecture}
\label{sec:architecture}
\begin{figure}[t]
\centering \includegraphics[width=0.8\columnwidth]{figs-arch}
%\includegraphics[width=0.45\textwidth]{layered_architecture2}
\caption{Layered architecture of the software platform running in each
  node of the distributed system. The guarantees (assumed and
  provided) are indicated between the layers.}
\label{fig:layered_architecture}
\vspace{-0.2in}
\end{figure}
We aim at building a reusable software platform that can be applied
across many application domains, and many processing and communication
platforms. The software platform should provide solutions to core
resource management problems, support security, and provide services
that are application independent.

This platform  can be built as a multi-layer architecture that 
addresses these issues, as shown on Fig~\ref{fig:layered_architecture}. 
At the lowest level, an operating system kernel provides the core resource sharing and
management functions, as well as the isolation from hardware specific
details. The kernel is typically accessed via `system libraries' that
provide a convenient interface to kernel services. Layered upon this
foundation there is a middleware layer to provide higher-level, reusable
communication (i.e. messaging) and resource management services. The
next layer up provides the component abstractions, in order to support
component-oriented development or distributed applications. The
platform should provide services for application configuration
and lifecycle management (the deployment manager), for application
control (the mission manager), for the handling of faults arising
during operations (the fault manager), and for the management of
resources (the resource manager). Note that by `managers' here we mean
critical, privileged applications that run outside of the core
operating system, and provide complex and long-term management services.

Note that a layered architecture helps with establishing assurances
for the overall systems. At the bottom, the hardware layer provides
guarantees about correct behavior (that was verified by the hardware
vendor). The kernel can assume these and provide its own guarantees to
the higher layers that, in turn, provide their guarantees to the higher 
layers, etc.



\subsection{Platform kernel}

At the lowest layer of the software platform, the kernel  encapsulates device drivers and provides processor scheduling and
networking features, but it also needs to address the real-time,
resilience, and security requirements of the application
domain. Borrowing language from the computer security community, the
kernel has to be part of the Trusted Computing Base (TCB) that
provides guarantees and is built and verified to high-assurance standards
\cite{Rushby84atrusted}.

Real-time requirements can be addressed by a number of
factors. Interrupt latencies (i.e. the worst-case delay elapsed
between the arrival of an interrupt and the release of an activity the
responds to that interrupt) should be bounded and known. System calls
should always be configurably time-bounded, and should return with a 
timeout error in case of unexpected delays to prevent the caller application from 
being unacceptably delayed.  The kernel should support a number of
scheduling policies that provide verifiable guarantees for timeliness
of task execution. Furthermore, it should allow application tasks of
different criticality levels to share the CPU. All tasks of an
application should run at the same criticality level and this should
be reflected in the available scheduling policies.  Because of the
often critical nature of the applications, the scheduling models
provided by the kernel must support timing analysis.

Schedulability analysis is very problematic in the most general case, with
completely unconstrained task behaviors. However, it can become feasible
when restrictions are placed on the behavior of and the interaction
amongst the tasks. As discussed below, an application-level software
\textit{component model} can provide such restrictions, such that the
component-level schedulability becomes manageable. However, the kernel
should be kept simple and provide only core scheduling services that
support potentially many component models. According to experience,
tasks should be able to operate within a shared address space
(i.e. threads) as well as separated address spaces
(i.e. processes). Finally, the kernel scheduler should be able to take
advantage of multi-core architectures and be able to schedule tasks
on different cores, possibly under application control.

Communication links in a distributed system are a critical resource,
especially when they are scarce and highly dynamic, as in mobile
ad-hoc networks. Hence the communication facilities, including the
protocol stack, should be implemented accordingly. As a minimum, the
kernel should support a multitude of transport protocols, preferably
above a common network protocol.

Real-time support must be available for the communications as
well. Datagrams (message blocks) should be time stamped by the kernel
such that message recipients are aware of the message transfer
delays. This necessitates clock synchronization across the nodes;
IEEE or IEEE 802.1AS can serve as the facility to support that. 
Furthermore, the network layer should support time-constrained
real-time communications with guarantees. For some classes of network
traffic existing best-effort approaches (like TCP/IP) are
insufficient, and real-time protocols are needed. The solution here 
necessitates a multitude of network traffic classes, including sporadic but highly critical
traffic, guaranteed-bandwidth time triggered traffic, rate constrained
traffic, and best effort traffic\footnote{\vspace{-0.05in}The standard
SAE AS6802: Time-Triggered Ethernet has similar traffic classes}. The kernel, as the ultimate 
resource manager is to support the sharing of the communication link(s) and 
is to permit applications to  select the traffic classes needed for 
their specific network flows. Furthermore, if the
communication channel is not able to provide the expected performance
anymore the kernel should signal the application so that it can adapt to this change.

Security (i.e. confidentiality, integrity, and authenticity) 
of communications is a critical issue in some of the application domains. 
On the lowest layers, features for secure communication should
be available, possibly supported by the communication hardware
itself and cryptography engines. However, as the
applications running on the platform are not necessarily trusted,
their communication capabilities need to be constrained as
well. Mandatory Access Control (MAC) with Multi Level Security (MLS)
\cite{BellLaPadula} on the network and the messages may be necessary,
in which case the kernel has to provide support for (1) the
trustworthy configuration of network communications and (2) labeled
communications between parties.  The first means that only privileged,
trusted service processes are permitted to configure the network and
the communication flows in the network. We expect that untrusted
processes are not permitted to simply open a communication channel to the network and
talk to any network address -- only trusted service processes can
create the network connections, and once initiated the
communication endpoints are handed over to the untrusted processes for use. The
second means that, following the principles of labeled communications,
each message transmitter and receiver is provided with a label set, by
an external authority. These labels are to be used in each communication
operation by the application, and their correct use is validated and enforced 
by the TCB.  Note that while these technologies have been 
originally developed for government applications, security awareness on a 
shared computing platform necessitates their use.

A communication flow is valid only between parties with labels that
satisfy the rule that information can flow only from lower to
higher or between equal labels (according to the domination relation). 
Assuming an increasing order of sensitivity: \texttt{Confidential} <
\texttt{CompetitionSensitive} < \texttt{ManagementOnly}, 
\emph{e.g.}, a \texttt{CompetitionSensitive} process for mission A can read \texttt{Confidential} or
\texttt{CompetitionSensitive} data for mission A, but not  \texttt{ManagementOnly} data for mission A or
\texttt{CompetitionSensitive} data for mission B.  When the transmitter wishes to transfer a
message, it has to supply a message label that must match with one of
the labels in its own set, and satisfies the MLS rule. The kernel,
which is part of the TCB, performs this check on each message -- both
on the transmitter and the receiver side. This machinery can ensure
that processes always follow the communication constraints defined by
a security policy.

The operating system runs processes; both application and service
processes. To distinguish between these a capability mechanism is
needed that controls what operating system services a process can
use. For example, in order to prevent the unchecked proliferation of
application processes, only privileged processes should be permitted
to create new processes. When a process is created, its parent process 
should specify what capabilities the child has, which can only be a subset of
the capabilities of the parent.

\vspace{-0.05in}
\subsection{Platform services}

As mentioned above, platform services are needed to perform management
functions on the running system that are outside of the scope of
typical applications. Note that platform services perform critical
functions that require privileges, hence the platform services are
part of the TCB. We envision at least four kinds of management
services:

%\begin{itemize}
%\item
  \textbf{Deployment management:} As stated in the introduction,
  the envisioned systems are managed by some management authority,
  presumably over a network connection. Each node in the system has to
  have a service that can download, install, configure, activate,
  teardown, and remove the distributed applications. This service is
  essentially the top-level configuration manager for the node. Note
  that it itself should be fault-tolerant (i.e. able to manage faults during
  the deployment process), should obey and enforce security policies,
  and should be responsive (per real-time requirements).

%\item
 \textbf{Mission management:} Beyond deployment, there is a need
  for a service to manage the execution of applications. One should be
  able to activate and de-activate applications based on triggering
  events or the elapse of time. Triggering events can be generated by
  applications or the services. Mission management should include support
  for system auditing (including logging control) and debugging.

%\item
 \textbf{Fault management:} Resilience to faults is a core
  requirement for the system. We envision that the fault management is
  autonomous: the system attempts to restore functionality, if
  possible, without external intervention. Obviously, it may be
  necessary that the system cannot manage a fault on its own, and it
  has to contact its management authority.  While fault management is
  inherently a shared responsibility of all layers (including
  applications), there are some system-level issues that
  can be addressed by a dedicated service. For instance, if an
  application unexpectedly terminates, a fault management reaction
  could be an attempt to restart the application, and if that fails
  then attempt to restart the application on another node, 
  the capability facilitated by a fault management service. Note
  that the software platform is not to define fixed policies for
  fault management (e.g. try restart five times, then re-allocate),
  rather it is to provide mechanisms that allow implementing any such
  policies (e.g. by scripting the behavior of the fault manager
  service).

%\item
 \textbf{Resource management:} Embedded systems are typically
  resource constrained, hence unbounded resource usage cannot be
  permitted. This can be strictly managed by a static quota system,
  where developers declare the resource needs of their applications,
  then a system integrator verifies that such resource needs are
  acceptable (i.e. the application is `admissible'), and then the
  software platform enforces these quotas. If the application attempts
  to obtain more resources than it was declared, the request will fail
  (and the application has to handle this failure). This method is too
  strict, however, and may use resources very inefficiently. A
  resource manager service can implement a more complex, dynamic
  resource allocation policy, where applications can dynamically
  request and release resources, and the service honors or rejects
  these requests while maximizing system utility. Note that a critical
  resource is network bandwidth (if it is limited and/or fluctuating),
  and the dynamic management of communication bandwidth that maximizes
  system utility is a challenge.
%\end{itemize}
%Note that the above list is just a starting point for services, and
%many other, more specialized services can be anticipated.


\subsection{Middleware}
All modern distributed software systems are built using middleware
libraries that provide core communication abstractions for
object-based systems. These abstractions are to facilitate
prototypical component interactions. Industry standards and
pragmatic experience shows that a well-defined, small set of 
interaction patterns can provide a solid foundation for building
applications. The set includes: (1) Point-to-point interactions when
when an object wants to invoke specific services of another object. 
The interaction can be synchronous (call-return) or  asynchronous (call-callback). 
Note that the client and server are coupled and are involved in bi-directional messaging.
(2) Publish-subscribe interactions when a publisher generates data samples, 
which are then asynchronously consumed by interested subscribers. Note that the
publishers are loosely coupled, and not directly known to each other. 
While additional, more complex interactions may also be needed, the interactions
should be facilitated in conjunction with overall system requirements.
For instance, the interactions can be subject to timing constraints, and the scheduling
of the message exchanges should be done accordingly. The interactions
are also subject to the security policies - only permitted information
flows can be utilized to facilitate an interaction. The interactions
have to be implemented in conjunction with the fault management
architecture: objects participating in the interactions must become
aware of faults (originating from the network, for instance), and
should be able to rely on fault tolerant services, if available.

\vspace{-0.05in}
\subsection{Component model}

We envision a component-oriented software development for the
platform. Obviously, this necessitates a precisely defined abstract
component model that helps developers to build robust systems from
reusable components. The implementation of the component model 
must rely on a robust component framework that
facilitates and mediates all interactions among the components. The
component model should clearly define how component activities are
scheduled, based on events or the elapse of time, and what the
component lifecycle is.

The component model is subject to all requirements mentioned above. It
has to support real-time requirements: we want to be able to predict
the timing properties of the system based on the timing properties of
the components and their specific interactions. The component model
should support security policies, and should provide for
fault management, including anomaly detection, diagnosis, and fault
mitigation.
