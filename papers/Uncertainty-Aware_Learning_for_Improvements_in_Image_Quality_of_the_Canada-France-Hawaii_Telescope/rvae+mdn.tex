\subsection{Probabilistic Predictions with a Mixture Density Network}
\label{sec:mdn}

Mixture density networks (MDNs) are composed of a neural network, the output of which are the parameters of a mixture model \citep{bishop_mdn}. They are of interest here because the relationship between the feature vectors $\mathbf{x}$ and target labels $\mathbf{y}$ can be thought of stochastic nature. Therefore, MDNs express the probability distribution parameters of MPIQ as a function of the input sensor features. In a one-dimensional mixture model, the overall probability density function (PDF) is a weighted sum of $M$ individual PDF $p_\theta^m(\mathbf{y}|\mathbf{x})$ parameterized by a neural network of parameters $\theta$ \footnote{In their initial form~\cite{bishop_mdn}, MDNs used a Gaussian mixture model (GMM). They can easily be generalized to other distributions.}:

%$p(y)=\sum_{j=0}^{M-1} \alpha_{j} p_{j}(y)$
%is a weighted sum of $M$ PDFs $p_{j}(y)$.  The weights $\Alpha=\left\{\alpha_{0}, \ldots, \alpha_{M-1}\right\}$ are non-negative and $\sum_{j=0}^{M-1} \alpha_{j}=1$.  In a (one-dimensional) GMM each distribution $p_j$ is a Gaussian, parameterized by $\theta_j = (\mu_j, \sigma_j)$ where $\mu_j$ is the mean and $\sigma_j$ the standard deviation.  Writing $\theta = \{\theta_0, \ldots, \theta_{M-1}\}$ we sometimes explicitly indicate the parameterization as

\begin{equation*}
p_\theta(\mathbf{y}|\mathbf{x})=\sum_{m=1}^{M} \alpha_\theta^m(\mathbf{x}) p_\theta^m(\mathbf{y}|\mathbf{x}) \quad \textrm{with} \quad
\sum_{m=1}^{M} \alpha_\theta^m(\mathbf{x})=1.
\end{equation*}

Under the assumption of $N$ independent samples $\mathbf{x}$ from the features distribution, and the corresponding conditional samples $\mathbf{y}$ of MPIQ, we minimize over the negative log-likelihood of the density mixture to obtain the neural network weights:
\begin{equation}
\theta^* = \mathop{\mathrm{argmin}}_{\theta} -\frac{1}{N}\sum_{n=1}^{N} \log p_\theta(\mathbf{y}_n|\mathbf{x}_n)
\label{eqn:mdn_loss}
\end{equation}

To train the neural network we take as the network input the data record of sensor readings, observatory operating conditions, etc.  The network outputs are the (per-vector) mixture model parameters modeling the MPIQ conditional distribution, implicitly parameterized by the neural network. %We ensure that the mixture parameters are properly normalized so as to be valid.  In particular, the mixture weight vectors $\alpha$ is normalized so it sums to one and the standard deviations $\sigma_i$ are constrained to be positive.  %The loss for each data record is the log-likelihood of the corresponding MPIQ reading, calculated according to the corresponding $\Alpha$, $\Theta$. 
In our experiments, we use $\beta$ distributions, and set $M=5$ as it gave sensible results.

%The number of GMM components in an MDN is a hyperparameter that needs to be tuned on the validation set; we leave this exercise to future work and use $m=5$ since it produces sensible results. %Figure~\ref{fig:rvae} provides an overview of the MDN construction.

%Fitting can then typically proceed using expectation-maximisation [20] \yst{Is this correct? For our case, there are no classes, the training should only be a maximum likelihood. Am I missing something here? Or do you mean classical GMM here. I would shorten this.}. \Simon{It looks like what is described above is a density estimation using GMM and not an MDN ? I would expect an MDN to model the distribution of MPIQ conditionally on the inputs ?}
%For our purposes, we have an individual-based model $M,$ with some input $\alpha,$ that produces stochastic realisations $y \sim M(\alpha) .$

\iffalse
\tcr{(SCD:Suggest we delete the rest of this section)}

\tcr{+++++START DELETE++++++++}

We wish to derive a relationship among the inputs $\textbf{x}$, the mixture density weights $\omega_{j}(\textbf{s})$, and the density parameterisations $\theta_{j}(\textbf{x})$. While this could potentially be done with a separate regression for each of the density parameters and weights, this would fail to capture the corresponding relationships that would exist between each parameter and weight. %\Simon{ OK that sounds more like an MDN, which is actually regressing the GMM parameters as a function of the input features. I would therefore directly introduce the MDN above and skip the general discussion on GMMs}.
We can therefore model these using a neural network, which is able to provide flexible fitting for arbitrarily complex relationships. %\Simon{ The following is a bit long and hard to follow, especially in the absence of a figure }
This is exactly what an MDN is -- a mixture model, where the mixture components are modelled using a neural network. Provided that $p_{j}$s are differentiable with respect to $\theta_{j}$s, the loss in Equation (\ref{eqn:mdn_loss}) represents a differentiable function. Standard techniques based on stochastic gradient descent can then be applied in order to optimise the weights of the network with respect to this loss.

In our experiments the GMM has $m=50$ components. The MDN is then used to find the $\mu$s, $\sigma$s, and associated weights $\omega$s. Figure~\ref{fig:rvae} provides an overview of the MDN construction. 

\tcr{(SCD: Will clean this up, but in~(\ref{eqn:mdn_loss}) we use $x$ for the MPIQ, I think in the following the $x_{\rm trn}$ may be the input to the neural net?  Ned to clarify.)} The tuple of inputs and output MPIQ from the training set -- $\mathbf{x_{\rm trn}}$ and $y_{\rm trn}$ -- are passed through a number of hidden densely connected layers in the neural network, which provide a compact representation of the relationship between them. These learned distribution parameters are then passed through a normalisation layer; the weights of the mixture ($\omega$s)are transformed such that they sum to 1, and the scale parameters ($\sigma$s)are transformed so that they are positive. \tcr{(SCD: Probably will need Sankalp to review my edits when I make them here.)} These parameters are used to construct the mixture model, where one can draw samples or calculate statistics, such as mean and variance, for a given input. We note that the choice of the number of components in the mixture model, 50, and the parameterization of the mixture model components itself -- normal -- has not been optimized, and provides an avenue for future research.

\tcr{+++++END DELETE++++++++}
\fi

\subsection{Complementary Predictions and Interpretation with Gradient Boosted Decision Trees}
\label{sec:gbdt}

We complement the MDN IQ predictions by another algorithm to secure our results: a gradient boosted decision tree (GBDT) to predict IQ from the sensor data. This is in fact one of the main reason so much of feature engineering was performed on the sensor data.
A set of consecutive decision trees is fit where each successive model is fit to obtain less overall residuals than the previous ones by weighting more the larger sample residuals. Once converged, we can obtain a final predictions from the trained boosted tree as the weighted mean of all models. The optimization can be performed with gradient descent. Several implementations of this popular algorithm exist and we selected the \textsc{catboost}\footnote{\url{https://catboost.ai/}} one for our modelling, with a loss optimized for both the mean and the variance of the predictions. We first perform nested cross-validation as for the MDN, obtain the best hyper-parameters. We then train ten  GBDT models with the same hyper-parameters with a stochastic optimization, each with a different initialization of the model parameters. Aleatoric and epistemic uncertainties are estimated with a simple ensemble averaging method \citep{malinin2021uncertainty} of each model predictions. We show our results in Figure \ref{fig:MDN_moneyplot_catboost}, and discuss them in detail in Section \ref{sec:results}.

%A trained GBDT model can be probed to explore which decisions were selected at each tree split to ultimately give the best IQ predictions. Our tabular data is quite large with many columns, and after hyper-parameter optimization and regularization, the final solution ended into a fairly large tree which limits a human-readable interpretation. Nerveless, feature ranking and attributions are possible and we explore these results further on.


%%%%%%%%%%%%%%%%%%%%
%\subsection{Mixture Density Robust $\beta$ Variational Autoencoder with $\beta$ Divergence}
\subsection{Density Estimation with a Robust Variational Autoencoder}
\label{sec:rvae}
%\seb{My take: put this section into an appendix}

%From Table \ref{tab:datasets_overview} we observe that $\mathcal{D_{F_S,S_S}}$ and $\mathcal{D_{F_L,S_S}}$ have an order of magnitude {\it fewer} samples than their respective counterparts, $\mathcal{D_{F_S,S_L}}$ and $\mathcal{D_{F_L,S_L}}$. As the training of neural networks is extremely data intensive, this provides strong motivation to find a way to be able to bring the larger data sets to bear on network training. The major hurdle is that the larger data sets have missing values and missing data is not easily handled by neural networks. Our approach is to implement self-supervised learning. By developing an approximation of the underlying multivariate distribution from which the input samples are drawn then, for any given sample with missing observations, we can query the approximate distribution to fill in the observed values. We next describe how we accomplish this using one of the most common tools for self-supervised learning, the autoencoder~\citep{hinton2006reducing_autoencoderintroduction}.

%As a `freebie', we would also have the distribution's best guesses for the missing observations, and we could then proceed with using such a sample in a neural network. This is precisely what we do in what follows.

An autoencoder~\citep{hinton2006reducing_autoencoderintroduction} is a neural network that takes high dimensional input data, encode it into a common efficient representation (usually of lower dimension), and then recreates a full-dimensional approximation at the other end. Through its mapping of the input to a smaller or sparser, and more manageable, but information-dense, latent vector, the autoencoder finds application in many areas including  compression, filtering, and accelerated search. A variational encoder \citep{sgvb, vae_intro_2013, vae2} is a probabilistic version of the autoencoder.  Rather than mapping the input data to a specific (fixed) approximating vector, it maps the input data to the parameters of a probability distribution, e.g., the mean and variance of a Gaussian. VAEs produce a latent variable $\mathbf{z}$, useful for data generation. We refer to the \textit{prior} for this latent variable by $p(\mathbf{z})$, the observed variable (\textit{input}) by $\mathbf{x}$, and its conditional distribution ({\it likelihood}) as $p_{\theta}(\mathbf{x} | \mathbf{z})$. In a Bayesian framework, the relationship between the \textit{input} and the \textit{latent variable} can be fully defined by the \textit{prior}, the \textit{likelihood}, and the \textit{marginal} $p_{\theta}(\mathbf{x})$ as:
\begin{align}
p_{\theta}(\mathbf{x})=\int_{\mathcal{Z}} p_{\theta}(\mathbf{x} | \mathbf{z}) p(\mathbf{z}) d \mathbf{z}. \label{eq:fullDist}
\end{align}

%Given $n$ independent samples of data vectors $x^{(i)}$, the maximum likelihood parameter estimate $\theta^{\ast}$ is the parameter value that maximizes the probability of the data.  Typically we equivalently minimize the negative log-likelihood to the data:
%\begin{align}
%\theta^{*}=\arg \min _{\theta} \sum_{i=1}^{n} - \log p_{\theta}\left(\mathbf{x}^{(i)}\right). \label{eq:MLest}
%\end{align}

It is not not easy to solve Equation~(\ref{eq:fullDist}) as the integration across $\mathbf{z}$ is most often computationally intractable, especially in high dimensions.  To tackle this, variational inference \citep[e.g.][]{Jordan1999} is used to introduce an approximation $q_{\phi}(\mathbf{z}|\mathbf{x})$ to the posterior $p_{\theta}(\mathbf{z}|\mathbf{x})$. In addition to maximizing the probability of generating real data, the goal now is also to minimize the difference between the real $p_{\theta}(\mathbf{z}|\mathbf{x})$ and estimated $q_{\phi}(\mathbf{z}|\mathbf{x})$ {\it posteriors}. We state without proof (see \cite{kingma2019introduction} for detailed derivation):%Expanding the Kullback-Lieibler (KL) divergence $D\left(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z}|\mathbf{x})\right)$, where the expectation in the KL-divergence is taken only with respect to $\mathbf{z}$ and not the (fixed) $\mathbf{x}$ one finds that:
\begin{align}
    &-\log p_{\theta}(\mathbf{x}) + D_{\rm KL}\left(q_{\phi}(\mathbf{z}|\mathbf{x}\right) \| p_{\theta}(\mathbf{z}|\mathbf{x})) \nonumber \\ 
    &= -\mathbb{E}_{\mathbf{z} \sim q_{\phi}(\mathbf{z}|\mathbf{x})} [ \log p_\theta(\mathbf{x} | \mathbf{z})] + D_{\rm KL}\left(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z})\right) \label{eq:VAEeq}
\end{align}

The approximating distribution $q$ is chosen to make the right-hand-side of Equation~(\ref{eq:VAEeq}) tractable and differentiable. Taking the right-hand-side as the objective to simultaneously minimize both the divergence term on the left-hand-side (making $q$ a good approximation to $p$) and $-\log p(x)$. This is exactly the loss function that we want to minimize via backpropagation:%This makes each term in Equation~(\ref{eq:MLest}) equal to 
\begin{align}
L_{\mathrm{VAE}}(\theta, \phi; \mathbf{x}) & = -\mathbb{E}_{\mathbf{z} \sim q_{\phi}(\mathbf{z} | \mathbf{x})} \left[\log(p_{\theta}(\mathbf{x} | \mathbf{z}))\right] \nonumber \\
&+D_{\mathrm{KL}}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z})\right) \nonumber \\
-L_\mathrm{ELBO} &= L_{\mathrm{REC}}(\theta, \phi; \mathbf{x}) + D_{\mathrm{KL}}(\theta, \phi; \mathbf{x}) \label{eq:VAEeq2}
\end{align}
where $\theta^{*}, \phi^{*} = \mathop{\mathrm{argmin}}_{\theta, \phi} L_{\mathrm{VAE}}$. Since KL-divergence is non-negative, Equation (\ref{eq:VAEeq2}) above can be thought of as the lower bound of $p_{\theta}(\mathbf{x})$, and is the loss function to minimize. It is commonly called the ELBO, short for {\it {evidence based lower bound}}. $L_{\rm{REC}}$ minimizes the difference between input and encoded samples, while $D_{\rm{KL}}$ acts as a regularizer \citep{KLregularizer}.

The typical choice for $q$ (that we also make) is an isotropic conditionally Gaussian distribution whose mean and (diagonal) covariance depend on $\mathbf{x}$.  The result is that the divergence term has a closed-form expression where the mean and variance are learned, for example by using a neural network.  To be able to backpropagate through the first term (the expectation) in the loss function, a reparameterization  is introduced.  For each sample from $\mathbf{x}$ take one sample of $\mathbf{z}$ from the conditionally Gaussian distribution $q_\phi(\mathbf{z}|\mathbf{x})$.  Without loss of generality we can generate an isotropic Gaussian $\mathbf{z}$ by taking a Gaussian source $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathrm{\mathbf{I}})$ shifting it by the $\mathbf{x}$-dependent mean $\mathbf{\mu}$ and scaling by the standard deviation $\boldsymbol{\sigma}$ to get $\mathbf{z} = \boldsymbol{\mu}+\boldsymbol{\sigma}\odot \boldsymbol{\epsilon}$, where $\odot$ is the element-wise product.  Approximating the first (expectation) term in the objective with a single term using this value for $\mathbf{z}$ allows one to backpropagate gradients through this objective.  Note that, in the terminology of autoencoders, the $q$ and $p$ functions play the respective roles of encoder and decoder; $q_{\phi}(\mathbf{z} | \mathbf{x})$ generate the latent representation from a data point and  $p_{\theta}(\mathbf{x} | \mathbf{z})$ defines a generative model.

%model the random variable $\mathbf{z}$ as a deterministic variable $\mathbf{z}=\mathcal{T}_{\phi}(\mathbf{x}, \boldsymbol{\epsilon}),$ where $\boldsymbol{\epsilon}$ is an auxiliary independent random variable, and the transformation function $\mathcal{T}_{\phi}$ parameterized by $\phi$ converts $\epsilon$ to $\mathbf{z}$. A common choice of the form of $q_{\phi}(\mathbf{z} | \mathbf{x})$ -- and the one employed in this paper -- is a multivariate Gaussian with a diagonal covariance structure:

%\begin{align}
%\mathbf{z} &\sim q_{\phi}\left(\mathbf{z} | \mathbf{x}^{(i)}\right)=\mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} \boldsymbol{I}\right) %\\
%\mathbf{z} &= \boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \text { where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I})
%\end{align}
%where $\odot$ refers to element-wise product.

%While this reparameterization works for other types of distributions, if we particularlize to multivariate isotropic Gaussians, we need only learn the mean and variance of the distribution.    The  stochasticity is restricted to the random variable $\epsilon \sim \mathcal{N}(0, \boldsymbol{I})$.


%For instance, we find that recorded temperature values can be glitchy, reaching values as high as $999^{\circ}$ and as low as $-999^{\circ}$, as can be seen from Table \ref{tab:summarystats}. 

The VAE described so far, which we refer to as the `vanilla' VAE, is not the optimal model for our purposes.% for two reasons.  First, the data sets $\mathcal{D_{F_S,S_L}}$ and $\mathcal{D_{F_L,S_L}}$ contain missing observations, where sensor values were not recorded.
This is because our data set $\mathcal{D_{F_S,S_S}}$ %and its derivative $\mathcal{D_{F_L,S_S}}$ 
can contain outliers caused mostly by sensor failures, and sometimes by faulty data processing pipelines. The ELBO for `vanilla' VAE contains a log-likelihood term (first term in RHS of Equation \ref{eq:VAEeq}) that will give high values for low-probability samples \citep{akrami2019robustvae}. %To accommodate these aspects of the data we modify the VAE in two ways. First, following the work of \cite{vae_missingvalues}, we introduce a mask $M$ as an additional input to the encoder, . This mask encodes the positions of the missing observations (`NaN' values), allowing the modified network to impute the missing values. Second,

We state without proof (see \citet{akrami2019robustvae, akrami2020robustvaetabular} for details) that $L_{\rm {REC}}$ for a single sample can be re-written as:
\begin{align}
L_{\rm {REC}}^{(i)} = \mathbb{E}_{\mathbf{z} \sim q_{\phi}(\mathbf{z} | \mathbf{x}^{(i)})}\left[D_{\rm {KL}} \left(\hat{p}(\mathbf{X}) | p_\theta(\mathbf{X}|\mathbf{Z})\right)\right], \label{eq:lrec_mod}
\end{align}
where $\hat{p}(\mathbf{X}) = \frac{1}{N} \sum_{i=1}^{N} \delta (\mathbf{X}- \mathbf{x}^{(i)})$ is the empirical distribution of the input matrix $\mathbf{X}$, and $N$ is the number of samples in a mini-batch. We then substitute the KL-divergence with the $\beta$-cross entropy \citep{ghosh2016robust_variationalbayesianinference} which is considerably more immune to outliers:
\begin{align}
    L_{\rm{REC}, \beta}^{(i)} = \mathbb{E}_{\mathbf{Z} \sim q_{\phi}(\mathbf{Z} | \mathbf{x}^{(i)})}\left[H_{\beta}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{Z}))\right], \label{eq:lrec_beta}
\end{align}
where the $\beta$ cross-entropy is given by \cite{formula_betacrossentropy,adaptiveoptics0,rvae_orig}:
\begin{align}
    &H_{\beta}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{Z})) = \nonumber \\
    &-\frac{\beta+1}{\beta}\!\int\!\hat{p}(\mathbf{X})\left(p_{\theta}(\mathbf{X} | \mathbf{Z})^{\beta}-1\right) d \mathbf{X}+\int\!p_{\theta}(\mathbf{X} | \mathbf{Z})^{\beta+1} d \mathbf{X} \label{eq:ce_beta_def}
\end{align}
Here $\beta$ is a constant close to 0. This makes the total loss function for a given sample:
\begin{align}
L_{\beta}\left(\theta, \phi ; \mathbf{x}^{(i)}\right) =& \mathbb{E}_{\mathbf{Z} \sim q_{\phi}(\mathbf{Z} | \mathbf{x}^{(i)})}\left[H_{\beta}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{Z}))\right] \nonumber \\
%\frac{\beta+1}{\beta}\left(\frac{1}{\left(2 \pi \sigma^{2}\right)^{\beta D / 2}} \exp \left(-\frac{\beta}{2 \sigma^{2}} \sum_{d=1}^{D}\left\|\hat{\mathbf{x}}_{d}-\mathbf{x}_{d}^{(i)}\right\|^{2}\right)-1\right) \nonumber \\
&+ D_{K L}\left(q_{\phi}\left(\mathbf{Z} | \mathbf{x}^{(i)}\right) \| p_{\theta}(\mathbf{Z})\right) \label{eq:loss_beta_prelim}
\end{align}
%\tcr{where $D$ is the number of features. $\hat{\mathbf{x}}_{d}$ is the output from the decoder at the $d^{\rm{th}}$ position, for the input sample $\mathbf{x}$. (SCD: Sankalp there is no $D$ nor $\hat{x}_d$ in the above.)}  \tcb{(SCD: I find the following extremely hard to follow.  It seems you are drawing a single sample $S=1$ below, which is akin to the VAE stuff, can you maybe just say that after the definition of $L_\beta$ and leave it at that?  Should also indicate what value of $\beta$ is used -- that is left open below.)}

\iffalse
\begin{align}
    &H_{\beta}\left(\hat{p}(\mathbf{X}), p_{\theta}(\mathbf{X} | \mathbf{z})\right) = \nonumber \\
    &-\frac{\beta+1}{\beta}\!\int\!\hat{p}(\mathbf{X})\left(p_{\theta}(\mathbf{X} | \mathbf{z})^{\beta}-1\right) d \mathbf{X}+\int\!p_{\theta}(\mathbf{X} | \mathbf{z})^{\beta+1} d \mathbf{X}
\end{align}
\fi

To draw from the continuous $\mathbf{Z}$, we use an empirical estimate of the expectation, and convert the above into a form of the Stochastic Gradient Variational Bayes (SGVB) cost \citep{sgvb} with a single sample $\mathbf{z}^{(j=1)}$ from $\mathbf{Z}$.
\iffalse
:
\begin{align}
L_{\beta}\left(\theta, \phi ; \mathbf{x}^{(i)}\right) = &H_{\beta} \left(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{z}^{(j)})\right) \nonumber \\
&+ D_{K L}\left(q_{\phi}\left(\mathbf{z}^{(j)} | \mathbf{x}^{(i)}\right) \| p_{\theta}(\mathbf{z}^{(j)})\right) \label{eq:loss_beta_sgvb}
\end{align}
\fi
Next, for each sample we calculate $H_{\beta}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{z}^{(1)}))$ when $\mathbf{x}^{(i)} \in [0,1]$. We substitute $\hat{p}(\mathbf{X})=\delta\left(\mathbf{X}-\mathbf{x}^{(i)}\right)$ and model $p_{\theta}(\mathbf{X} | \mathbf{z}^{(1)})$ with a mixture of Beta distributions with weight vector $\mathbf{\omega}$. That is,
\begin{align}
    p_{\theta}(\mathbf{X} | \mathbf{z}^{(1)}) = \sum_{k=1}^{k} \omega_k (\mathbf{X}^{p_k-1})(1-\mathbf{X})^{q_k-1} \times \frac{\Gamma(p_k+q_k)}{\Gamma(p_k)\Gamma(q_k)} \label{eq:pxgivenz_mixturedensity}
\end{align}
Using Equations (\ref{eq:ce_beta_def}) and (\ref{eq:pxgivenz_mixturedensity}), we obtain:
\begin{align}
    &H_{\beta}(\delta(\mathbf{X}-\mathbf{x}^{(i)}),p_{\theta}(\mathbf{X} | \mathbf{Z})) = \nonumber \\
    &-\frac{\beta+1}{\beta} \left(\sum_{d=1}^{D}\left(\sum_{k=1}^{K} \omega_k (\mathbf{x}_{d}^{(i) \cdot p_k-1})(1-\mathbf{x}_{d}^{(i)})^{q_k-1} \Lambda_{d,k}\right)\right) \nonumber \\
    &+\sum_{d=1}^{D} \sum_{k=1}^{K}\frac{\left((p_{d,k}-1)^{1+\beta}+1\right) \left((q_{d,k}-1)^{1+\beta}+1\right)}{(p_{d,k}-1)^{1+\beta}+(q_{d,k}-1)^{1+\beta}+2} \label{eq:ce_final}
\end{align}
where $\Lambda_{d,k} = \frac{\Gamma(p_{d,k}) + \Gamma(q_{d,k})}{\Gamma(p_{d,k})\Gamma(q_{d,k})}$, $D$ is the number of dimensions in a single sample, and $K$ is the number of components in the mixture. Equations (\ref{eq:loss_beta_prelim}) and (\ref{eq:ce_final}) together give us the total loss across all $N$ samples in a given mini-batch:
\begin{align}
L_{\beta}\left(\theta, \phi ; \mathbf{X}\right) = &\frac{1}{N}\sum_{i=1}^{N} \left[H_{\beta}^{(i)}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{Z})) \right. \nonumber \\
& \left. + D_{K L}\left(q_{\phi}\left(\mathbf{z}^{(1)} | \mathbf{x}^{(i)}\right) \| p_{\theta}(\mathbf{z}^{(1)})\right)\right], \label{eq:loss_beta_final}
\end{align}
where the superscript $^{(1)}$ implies a single draw from \textbf{z} from \textbf{Z}.

\iffalse
Using empirical estimates of expectation we form the Stochastic Gradient Variational Bayes (SGVB) \citep{sgvb}:
\begin{align}
L_{\beta}\left(\theta, \phi ; \mathbf{x}^{(i)}\right) \approx &\frac{1}{S}\sum_{j=1}^{S}H_{\beta}^{(i)}(\hat{p}(\mathbf{X}),p_{\theta}(\mathbf{X} | \mathbf{z}^{(j)})) \nonumber \\
&+ D_{K L}\left(q_{\phi}\left(\mathbf{Z} | \mathbf{x}^{(i)}\right) \| p_{\theta}(\mathbf{Z})\right),
\end{align}
where $S$ is the number of samples drawn from $q_{\phi}\left(\mathbf{Z} | \mathbf{X}}\right)$. In practice we pick $S=1$; as long as the minibatch size is large enough, this is a good approximation
We pick $\beta$ to be \tcr{xxx} by trial and error. 
\fi

The final, robust variational autoencoder architecture is denoted in the left of Figure \tcr{\ref{fig:rvae_plus_mdn}}.

\iffalse
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{vae.png}
    \caption{Caption}
    \label{fig:rvae}
\end{figure*}
\fi

%\fi


\iffalse
Let us call the underlying, unknown probability distribution $p^{*}(x)$. We would like to estimate it from its independent samples $\mathbf{x}_{1: N}=\left\{\mathbf{x}_{i}\right\}_{i=1}^{N} .$ To formulate inference as an optimization problem, we need to choose an approximating family $\mathcal{Q}$ and an optimization objective $J(q) .$ This objective needs to capture the similarity between $q$ and $p ;$ the field of information theory provides us with a tool for this called the Kullback-Leibler $(\mathrm{KL})$ divergence.

To this end, we consider a parametric model $p(x ; \theta)$ with parameter $\theta,$ and minimize the generalization error measured by the KL divergence $D_{\mathrm{KL}}$ from $p^{*}(x)$ to $p(x ; \theta)$\footnote{The main idea of variational methods is to cast inference as an optimization problem over a class of tractable distributions $\mathcal{Q}$ in order to find a $q \in \mathcal{Q}$ that is most similar to $p$.} :
\begin{align}
D_{\mathrm{KL}}\left(p^{*}(x) \| p(x ; \theta)\right)=\int p^{*}(x) \log \left(\frac{p^{*}(x)}{p(x ; \theta)}\right) d x
\end{align}
However, since $p^{*}(x)$ is unknown in practice, it is replaced with
\begin{align}
D_{\mathrm{KL}}(\hat{p}(x) \| p(x ; \theta))=\mathrm{Const.}-\frac{1}{N} \sum_{i=1}^{N} \ln p\left(x_{i} ; \theta\right)
\end{align}
where $\hat{p}(x)=\frac{1}{N} \sum_{i=1}^{N} \delta\left(x, x_{i}\right),$ is the empirical distribution and $\delta$ is the Dirac delta function. Minimizing this empirical Kullback-Leibler divergence is equivalent to maximum likelihood estimation. Equating the partial derivative of Eq.(2) with respect to $\theta$ to zero, we obtain the following estimating equation:

In our VAE example, we use two small ConvNets for the encoder and decoder networks. In the literature, these networks are also referred to as inference/recognition and generative models respectively. We use tf.keras.Sequential to simplify implementation. Let  and  denote the observation and latent variable respectively in the following descriptions.

Encoder network
This defines the approximate posterior distribution , which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation . In this example, we simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance parameters of a factorized Gaussian. We output log-variance instead of the variance directly for numerical stability.

Decoder network
This defines the conditional distribution of the observation , which takes a latent sample  as input and outputs the parameters for a conditional distribution of the observation. We model the latent distribution prior  as a unit Gaussian.

Reparameterization trick
To generate a sample  for the decoder during training, we can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation . However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.

To address this, we use a reparameterization trick. In our example, we approximate  using the decoder parameters and another parameter  as follows:

where  and  represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the decoder output. The  can be thought of as a random noise used to maintain stochasticity of . We generate  from a standard normal distribution.

The latent variable  is now generated by a function of ,  and , which would enable the model to backpropagate gradients in the encoder through  and  respectively, while maintaining stochasticity through .

Network architecture
For the encoder network, we use two convolutional layers followed by a fully-connected layer. In the decoder network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.
\fi
