\section{Conclusions and Future Work} \label{sec:conclusion}

\iffalse
\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/sigma_al_vs_mll.pdf}
    \caption{Aleatoric uncertainty ($68\%$ spread) v/s pseudo MLL. Vertical line is the $95^{\rm th}$ percentile value of the pseudo MLL histogram.}
    \label{fig:alvsmll}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/sigma_epis_vs_mll.pdf}
    \caption{Epistemic uncertainty ($68\%$ spread) v/s pseudo MLL. Vertical line is the $95^{\rm th}$ percentile value of the pseudo MLL histogram.}
    \label{fig:episvsmll}
\end{subfigure}
\caption{For one of the 10 separate test sets, uncertainty from the mixture density network is plotted against the pseudo marginal log likelihood from the robust variational autoencoder. On the x-axis, greater is better. By and large, there is a 1:1 correspondence between each uncertainty and MLL}
\label{fig:sigma_vs_mll}
\end{figure*}
\fi

In this paper we present what we envisage to be the first in a series of studies that will ultimately lead to dynamically optimized scheduling at the Canada-France-Hawaii Telescope. We have initiated that program herein by developing machine-learning based data-driven methods of image quality (IQ) prediction.  
We present results for two models.  The first is a feed-forward mixture density network (MDN) used in conjunction with a robust variational autoencoder. We trained both on a new dataset that comprises eight years of data collected at CFHT since the installation of the dome vents.  The MDN produces probabilistic predictions of image quality, while the autoencoder estimates the marginal distribution of the data. On average, IQ can be predicted to within 0.07'' accuracy based on environmental conditions and telescope operating parameters. By varying the configuration of the dome vents (in an in-distribution way) in response to environmental conditions, our model predicts that IQ can be improved by about 10\% over historical patterns, with the gains increasing when the nominal IQ value is large. For SNR-based observations, this represents gains of up to 10-15\%.  These gains, in turn, can be equated to  approximately 1M USD in operating costs per year of SNR-based observing.  Such gains would be realized in the form of additional observations made and experiments conducted; additional science accomplished.

We see several important avenues for further inquiry. Perhaps most immediate, the improvements in IQ that we present are predicted by extrapolating over hypothetical vent configurations.  While the uncertainties predicted by our model suggests that these out-of-distribution predictions are robust, we need to verify our predictions by collecting additional observation data in these operating regimes. By collecting such data we will be able to extend our model and robustly predict IQ for the more intermediate vent configurations ``half-way'' between all-open and all-closed.  In doing so we aim, finally, to realize the full utility of the dome vents.

Second, in this study we have treated each data record as an independent sample.  In reality of course, exposures are temporally related.  Numerous exposures are collected each night. By treating data records as independent, we do not leverage what we anticipate are quite important temporal relationships extant in the data. By incorporating temporal models, our aim is be to be able to produce real-time robust forecasts of IQ some five-to-twenty minutes into the future.  The realization of such capabilities will enable the adaptive reorganization of a nominal observation schedule, the real-time scheduling protocols we mention in the introduction. 

Connected to the second point, in this paper we work exclusively with data records from MegaCam.  Going forward we plan to augment our data set with records from other CFHT instruments. While MegaCam's IQ measurements are the most accurate, other instruments also measure IQ with acceptable accuracy, and equally importantly, operate when MegaCam is offline. CFHT schedules instruments in blocks or ``runs'' of several consecutive nights, e.g., swapping out instruments twice a month according to their sensitivity to moonlight. The consequence of this is that training data from MegaCam is not temporally contiguous. This makes it more difficult to use in training a scheduler.

Fourth, we do not currently take into account the physical locations of the different sensors. While discarding this spatial information made modeling and data analysis easier for this initial study, it leaves out useful side  information that can connect the placement of sensors and their relative values.  Going forward we will incorporate such information into our models.

%In future work we plan to leverage temporal correlations through specialized architectures such as temporal convolutional neural networks~\citep{temporal_cnns}.  
%In the future, our aim will be to be able to safely forecast IQ by some five-to-twenty minutes out in real-time.  Such a capability will enable the adaptive reorganization of a nominal observation schedule. 
%As the work of~\cite{milli2019nowcasting_paramal} shows, the high frequency behavior of atmospheric turbulence means that off-the-shelf networks are able to perform only %\tcr{(SCD: Sankalp, can you rework the rest of this paragraph, seems not so optimistic, not sure...)} \sg{(SG: Stark, not sure how to do that... I want to emphasize that past work has shown that it can be done, but that using simple models such as LSTMs isn't going to cut it.)} modestly well with respect to a constant prediction scenario; we thus expect to engage is substantial feature engineering and network architecting to be able to squeeze out maximal performance. 
%We also do not take into account the physical locations of the different sensors. While losing this resolution makes modeling and data analysis easier, it leaves helpful information about connections between placement of sensors and their values; in future work on the table, we plan on incorporating this information in our modeling.

%Next, we did not incorporate any hyperparameter optimization (HPO) in this work. Number of layers, number of nodes per layer, batch sizes, learning rate schedules, number of mixture components, feature normalization schemes -- everything can and should be optimized for obtaining minimum loss values. %We did not . As shown in \cite{akrami2019robustvae} and \cite{akrami2020robustvaetabular}, the choice of this constant controls the degree to which samples are classified as in-distribution or out-of-distribution. While our choice of $beta=0.005$ is `good enough', a detailed exploration of the parameter space is necessary to choose the optimal value. We relegate this to future work.

%\sg{(replace beta with gamma when model is pushed into production.)}

Fifth, the approach we take to {\it post-hoc} calibration of epistemic uncertainties -- CRUDE \citep{crude_probability_calibration} -- has its own set of limitations. While state-of-the-art in terms of improving sharpness and calibration, CRUDE implicitly assumes a symmetric distribution of uncertainty.  In our context this means we do not fully leverage the {\it asymmetric} uncertainties output by our $\beta$ posteriors. CRUDE also assumes that, once normalized (by their standard deviations), all  errors are drawn from the same distribution.  Therefore CRUDE weights all data points equally. In our context one implication is that while calibrating the probability distribution function (PDF) for a test sample with nominal (true) IQ of $2''$, which is near the right tail of the distribution (see the red curve in the left sub-figure of Figure \ref{fig:hist_preliminaries}), we are strongly impacted by samples with nominal IQ values near 0.6 arc-seconds, which is the mode of the distribution. This uniformity of treatment is not ideal. We plan to address this shortcoming in the future.

Sixth, we note that we are cautious about the thresholding method we apply to detect out-of-distribution samples (see~Figure \ref{fig:vae_hist_mll_ood}). In this work we take the $95^{\rm th}$ percentile of the pseudo marginal log-likelihoods for the training set samples as the out-of-distribution threshold. However, this is an {\it ad-hoc} choice based on intuition.  We do not claim that it is the optimal method to filter out out-of-distribution samples. We also show that log-likelihood regret is a more accurate metric than is log-likelihood when aiming to separate the two types of distributions.  We refrain from using regret in this work due to practical concerns about run-time. In future work, we will leverage distributed computing to  integrate this superior metric into our pipeline, and we will explore more principled ways to set the threshold.


Finally, in a slightly different direction, we note that a subset of the authors are collaborating with a concurrent and complimentary study of dome seeing at CFHT. In that study direct measurements of local in-dome optical turbulence are being collected using AIRFLOW instruments~\cite{lai2019}. AIRFLOW sensors are always-on optical turbulence sensors and, as discussed in the introduction, turbulence is  highly correlated with instrument IQ, the metric of interest herein. The current work informs the AIRFLOW study in that it can provide insight into which locations sensors should be placed.  Conversely,  data from the AIRFLOW study can provide a new data stream for the current study.   Taken together, these two studies will offer unique insights into the nature of dome seeing and ways that effects which degrade seeing can be mitigated.

%In future work we will also explore use of variational autoencoders for feature imputation \citep{vae_missingvalues}, since it is known that dropping samples negatively impacts a model's predictive ability \citep{imputation_missingvalues}. This will allow us to include measurements from MKAM, as described in Section \ref{sec:data}; we expect this will significantly improve our predictive ability. 
%In addition, we plan to include records from additional CFHT instruments. MegaCam's field of view and resolution yield extremely accurate IQ values, but other instruments also measure IQ with sufficient accuracy. CFHT schedules instruments in blocks or runs of several consecutive nights, alternating instruments according to sensitivity to moonlight. The consequence of this is that training data from MegaCam is quite widely dispersed across time. This makes it more difficult to use in training a scheduler.
%We expect that adding data sources in a semi-supervised~\citep{saas1} fashion will allow us to increase the size of our training data many fold. We also realize that by using self-supervised and supervised learning based models separately (RVAE and MDN, respectively), we are leaving money on the table. In future, we will explore the use of a comprehensive end-to-end model with multi-task learning to tackle this shortcoming.

%In a future publication, we also plan on expanding on our work on feature attributions. While summary plots like those in Figure \ref{fig:attplots} provide valuable information about the impact of individual features on IQ, they lack in resolution. For one, they are global plots, and average over variances in feature attributions from individual samples, some of which could be quite informative. For another, they do not educate us about feature {\it interactions} -- how two features act together to affect the output. Accounting for both these shortcomings will allow us to cluster feature attributions and interactions and identify different operating modes for CFHT for the first time. This can also be leveraged in predictive maintenance, to understand exactly which combinations of features is likely to cause our measurements to drift.

%\sg{(I don't quite understand this next line - whoever put this in, can you please explain it to me?)}
%Looking forward, we also plan to use the insight gained by teasing out feature importance and telescope operating modes to develop low-dimensional dynamic models that will aid us in predicting IQ further into the future and to high temporal resolution. %, and predict IQ not just for MegaCam, but also all the other instruments.

%As a final direction, we hope to expand our Mauna Kea data set beyond CFHT. Multiple observatories and weather monitoring instruments collect potentially relevant environmental data. By leveraging sensor data from multiple sites we may be able to improve significantly the temporal resolution of our predictions.
