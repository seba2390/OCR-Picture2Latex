\subsection{Probability Calibration}\label{sec:DL_probability_calibration}

In Section~\ref{sec:uncertainty_quantification} we describe how to derive both aleatoric and epistemic errors. While these variance estimates yield a second-order statistical characterization of the distribution of output errors, they can at times mislead the practitioner into a false sense of overconfidence%. At the same time, the distribution of error may not actually be Gaussian
~\citep{lakshminarayanan_probability_calibration0, probability_calibration1, crude_probability_calibration}.  

It therefore becomes imperative to {\it calibrate} our uncertainty estimates to more closely match the true distribution of errors. In other words, we ensure that 68\% confidence intervals for MPIQ predictions (derived from the epistemic uncertainty) contain the true MPIQ values $\sim 68\%$ of times. The confidence interval is the range about the point prediction of IQ in which we expect, to some degree of confidence, the true IQ value will lie.  For example, if our error were conditionally Gaussian, centered on our point prediction, then we would expect that with about $68.2$\% probability the true IQ value would lie within $\pm 1\sigma$ of our IQ prediction where $\sigma$ is the standard deviation of the Gaussian. To accomplish this we reserve some of our data which we use to estimate the distribution of errors -- this is the validation set. Using the inverse cumulative distribution function of this estimated distribution, scaled by the predicted standard deviation and shifted by the predicted mean, allows us to obtain a calibrated estimate of the output realization corresponding to any particular percentile of the distribution. The specific approach we use is the CRUDE method~\cite{crude_probability_calibration}.

However, calibrating the error estimates is not the only thing we care about if, through the calibration process we loose substantial accuracy. For instance, one can increase predicted uncertainties to arbitrarily high values to obtain a perfectly calibrated model; however, this would make these predictions useless for practically any downstream task. Therefore, CRUDE not only calibrates our post-processed predictions, but also ensures that they are  {\it sharp}~\citep{measuring_calibration_in_deep_learning}. Sharpness refers to the concentration of the predictions, akin to the inverse of the posterior error variance.  The more peaked (the sharper) the predictions are, the better, provided the sharpness does not come at the expense of calibration.  

%\tcb{In Section~\ref{sec:uncertainty_quantification} we describe how we obtain estimates of both aleatoric and epistemic error. However, not all errors are created equal -- in practice, most uncertainty estimates often fail to capture the underlying data distribution \citep{lakshminarayanan_probability_calibration0, probability_calibration1, crude_probability_calibration}. For example, a 95 \% confidence interval generally does not contain the true outcome 95\% of the time. When this happens, the model is said to be \emph{miscalibrated}. This is typically caused by model overconfidence, if the model is not powerful enough to map the right probability to its confidence interval. Calibration, or lack thereof, is also a function of the loss function the model is trained to minimize. Negative log-likelihood (Equation (\ref{eqn:mdn_loss})), is known to result in over-confident predictions \citep{measuring_calibration_in_deep_learning}. Thus, we need to \emph{post hoc} correct the uncalibrated probabilities our model outputs.}

%\tcb{Here, we use the CRUDE method of \cite{crude_probability_calibration} to re-calibrate our model's predictions \emph{post-hoc}. CRUDE ensures that not only are the post-processed predictions calibrated, but that they are also \emph{sharp} \citep{measuring_calibration_in_deep_learning}. Sharpness refers to the concentration of the predictions.  The more peaked (the sharper) the predictions are, the better (provided it is not at the expense of calibration).}
\iffalse
Above, we describe the procedure for obtaining both aleatoric and epistemic errors from a deep neural network. However, not all errors are created equal--in practice, most uncertainty estimates often fail to capture the underlying data distribution \citep{lakshminarayanan_probability_calibration0, probability_calibration1, crude_probability_calibration}. For example, a 95 \% confidence interval generally does not contain the true outcome 95\% of the time. When this happens, the model is said to be \emph{miscalibrated}. This is caused by model overconfidence: just as a deterministic model is unable to determine its own ignorance \textbf{CITATION NEEDED}, a probabilistic model may not be powerful enough to map the right probability to its confidence interval. Calibration is not directly measured by proper scoring rules like negative log likelihood \citep{measuring_calibration_in_deep_learning}. Since this is precisely the error which is the error metric we use (see \S\ref{sec:DL_MDN}), we need to correct the uncalibrated probabilities our model outputs.

Here, we use the CRUDE method of \cite{crude_probability_calibration} to \emph{post-hoc} re-calibrate our model's \textbf{INSERT MODEL NAME HERE} predictions. This method ensures that not only are the post-processed predictions \emph{calibrated}, but that they are also \emph{sharp}. Ideally, each expected calibration percentile should match the fraction of values observed below the predictions of that percentile \citep{measuring_calibration_in_deep_learning}. Calibration is thus a joint property of the predicted and the ground truth values. Sharpness, on the other hand, refers to the concentration of the predictions--the more concentrated the predictive distributions are, the sharper the forecasts, and the sharper the better (provided it is not at the expense of calibration). \cite{crude_probability_calibration} show that CRUDE not only results in better calibrated predictions compared to previously proposed methods, but also sharper predictions.

Finally, to measure the degree of calibration, we use two metrics (described in detail in Section \ref{sec:DL_metrics}): Average Coverage Error (ACE) and Prediction Interval Normalized Average Width (PINAW). To measure the sharpness of the predicted probability distributions, we utilize Interval Sharpness (IS). Finally, we use the Continuous Ranked Probability Score (CRPS) to simultaneously evaluate both the degree of calibration and sharpness. % cite \textbf{`CuttingEdge_Technologies_for_Renewable_Energy_Production_and_Storage chapter 4'}.
\fi