\section{Algorithm for probabilistic MPIQ prediction}
\label{sec:DL_putting_it_all_together}

\begin{algorithm}
\caption{Probabilistic MPIQ Prediction}
\label{algo:cont_norm}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\renewcommand{\algorithmicinitialize}{\textbf{Initialize:}}
\REQUIRE matrices consisting of environmental and dome measurements for the labelled and unlabelled subsets ($\boldsymbol{\mathrm{X_{L}}} = \{x_{n}^{(1:d)}\}^{N_L}_{n=1}$ and $\boldsymbol{\mathrm{X_{U}}} = \{x_{n}^{(1:d)}\}^{N_U}_{n=1}$, respectively, where $N_L$ and $N_U$ are the number of samples in the labelled and the unlabelled subsets, and $d$ is the number of covariates/features), and MPIQ measurements for the labelled subset ($y_L = \{y_{n}\}^{N_L}_{n=1}$).
\ENSURE (i) predicted probability densities for both the labelled and the unlabelled datasets: $y_{L}\vert_\mathrm{pred} =\{y_{n}\vert_\mathrm{pred}^{(1),(2)}\}^{N_L}_{n=1}$ and $y_{U}\vert_\mathrm{pred} =\{y_{n}\vert_\mathrm{pred}^{(1),(2)}\}^{N_U}_{n=1}$. Each predicted PDF is characterized by a mean and a standard deviation. \\
(ii) feature importance matrices for both labelled and unlabelled subsets: $\boldsymbol{\mathrm{F_{L}}} = \{f_{n}^{(1:d)}\}^{N_L}_{n=1}$ and $\boldsymbol{\mathrm{F_{U}}} = \{f_{n}^{(1:d)}\}^{N_U}_{n=1}$.
\FOR{iteration $i_\mathrm{test}$ in range [1, 10]}
\STATE Divide $\boldsymbol{\mathrm{X_{L}}}$ into two subsets,a training subset $\boldsymbol{\mathrm{X_{L(train)}}}\vert_{i_\mathrm{test}}$, and a testing subset $\boldsymbol{\mathrm{X_{L(test)}}}\vert_{i_\mathrm{test}}$,in a $9:1$ ratio (i.e., 90\% training and 10\% testing); do this in a stratified cross validated manner (\S\ref{sec:DL_putting_it_all_together}).
\FOR{iteration $i_\mathrm{val}$ in range [1, 5]}
\STATE Sub-divide the training set into training and validation sets in a $4:1$ ratio; that is, $\boldsymbol{\mathrm{X_{L(train)}}}$ $\rightarrow$ $\boldsymbol{\mathrm{X_{L(train2)}}}\vert_{i_\mathrm{val}}$ and $\boldsymbol{\mathrm{X_{L(val)}}}\vert_{i_\mathrm{val}}$.
\STATE Combine $\boldsymbol{\mathrm{X_{L(train2)}}}\vert_{i_\mathrm{val}}$ with the unlabelled subset $\boldsymbol{\mathrm{X_{U}}}$, and train \textbf{INSERT MODEL NAME} on this collated dataset using the SaaS algorithm \textbf{cite SaaS algo here}. Obtain $y_{L(\mathrm{val})}\vert_\mathrm{pred, i_\mathrm{val}}$ by predicting on the validation set.
\ENDFOR
\STATE Use the collated predictions on all 10 validation sets to tune the hyperparameters of \textbf{insert model name} (\S\ref{sec:DL_putting_it_all_together}). Call this tuned model \textbf{insert model name}$_\mathrm{tuned}$.
\FOR{iteration $i_\mathrm{val}$ in range [1, 5]}
\STATE Using \textbf{insert model name}$_\mathrm{tuned}$, run $2,500$ iterations with Concrete Dropout (\S\ref{sec:DL_errors}) during inference, obtaining mixture model $\mu$ and $\sigma$ for samples in $\boldsymbol{\mathrm{X_{L(val)}}}\vert_\mathrm{i_\mathrm{val}}$.
\STATE Combine the $2,500$ $\mu$'s and $2,500$ $\sigma$'s for each sample using Equation \textbf{insert equation number} to obtain six values: $50^{th},$ $2.5^{th},$ and $97.5^{th}$ percentile confidence intervals for the mean $\mu$, lower prediction level $\mu - 1.96\sigma$, and upper prediction level $\mu + 1.96\sigma$.
\ENDFOR
\STATE Using \textbf{insert model name}$_\mathrm{tuned}$, make predictions on $\boldsymbol{\mathrm{X_{L(test)}}}\vert_{i_\mathrm{test}}$, and obtain six values per sample like above.
\STATE Obtain corresponding $2500$ feature importances during inference, and combine them using Equation (). Call the combined matrix $\boldsymbol{\mathrm{F_{L}}}\vert_{\mathrm{i_{test}}}$.
\STATE Use the collated predictions $y_{L}\vert_\mathrm{pred, al+epis, train}$ to calibrate the predictions on the test set, $y_{L}\vert_\mathrm{pred, al+epis, i_{test}}$, with Equation (). Call these $y_{L}\vert_\mathrm{pred, al+epis, cal, i_{test}}$.
\ENDFOR
\STATE Collate all $y_{L}\vert_\mathrm{pred, al+epis, cal, i_{test}}$ to obtain $y_{L}\vert_\mathrm{pred, al+epis, cal}$, and rename it $y_{L}\vert_\mathrm{pred}$.
\STATE Collate all $\boldsymbol{\mathrm{F_{L}}}\vert_{\mathrm{i_{test}}}$ and to obtain $\boldsymbol{\mathrm{F_{L}}}$.
\end{algorithmic} 
\end{algorithm}
