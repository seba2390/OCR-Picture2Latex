\subsection{Deep Learning for Tabular Data}\label{sec:DL_tabnet}
Given the ease of use and conventionally superior performance of tree-based models on tabular data, it is natural to ask--why consider DNNs at all?
\begin{itemize}
    \item DNNs are powerful feature pre-processors. Using back-propagation, they learn a fine-tuned heirarchical representation of data by mapping input features to the output label(s). This allows us to shift our focus from feature engineering to fine-tuning the architecture, designing better loss functions, and generally experimenting with the mechanics of our neural network. This exactly functionality allows us to get away with merely using the pairwise differences of the temperature values in \S\ref{sec:data_cleaning_feature_engineering} as additional, hand-crafted features.
    \item There is empirical proof that DNNs result in improved performance with larger sized datasets \citep{airbnb}.
    \item Deep learning allows us to train systems end-to-end, which unlocks the possibility of using unlabelled samples to better predict the labelled ones (\emph{semi-supervised learning}, \S\ref{sec:DL_SSL}).
    \item It is easier to use DNNs in an online-mode, with data points `streaming in' rather than arriving in batches. This is because tree-based models need access to all the data to determine best split points, and hence need to be trained every time a new observation comes in. While this work does not use streaming data, in preparation for our future work on real-time predictions we believe it best to move to a neural network from the get-go.
\end{itemize}