
\subsection{Performance Metrics}\label{sec:DL_metrics}
For each input sample \textbf{x} we derive the predicted IQ, the aleatoric uncertainty, and the epistemic uncertainty, respectively, $\mu$, $\sigma_a$, and $\sigma_e$, cf.~Equations (\ref{eq.calcIQpredict}), (\ref{eq.calcAleaUncert}), (\ref{eq.calcEpisUncert}). In Section \ref{sec:metrics_det} we compare the median of predicted IQ values against their ground truth values. In Section \ref{sec:metrics_prob} we evaluate the quality of the predicted PDF.

\subsubsection{Metrics for Deterministic Predictions}\label{sec:metrics_det}
We present three measures to quantify the quality of the IQ prediction, Root-mean-square error (RMSE), mean absolute error (MAE), and bias error (BE).  Respectively, these three measures are defined as
\begin{align*}
        \mathrm{RMSE} & = \sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(\mu_i-y_i\right)^{2}},\\  \mathrm{MAE} & = \frac{1}{N} \sum_{i=1}^{N}\left|\mu_i-y_i\right|,\\
        \mathrm{BE} & = \frac{1}{N} \sum_{i=1}^{N} \left(\mu_i-y_i\right).
    \end{align*}
    %\item Normalized mean absolute percentage error (NMAPE):
    %\begin{align}
    %    \mathrm{NMAPE}=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\tilde{y}(i)-y(i)\right|}{\max _{i \in 1, \mathrm{n}]} y(i)} \times 100 \%,
    %\end{align}
In the above definitions, $y_i$ and $\mu_i$ are the true and predicted IQ values corresponding to an input sample and $N$ is the number of samples.

%%%%%%%%%%%%%%
\subsubsection{Metrics for Probabilistic Predictions}\label{sec:metrics_prob}

%We assess probabilistic predictions by evaluating Confidence Intervals (CI).  The confidence interval is the range about the point prediction of IQ in which we expect, to some degree of confidence, the true IQ value will lie.  For example, if our error were conditionally Gaussian, centered on our point prediction, then we would expect that with about $68.2$\% probability the true IQ value would lie within $\pm 1\sigma$ of our IQ prediction where $\sigma$ is the standard deviation of the Gaussian.  We say that $CI = 0.682$ for this example.  Since our model makes predictions only of first and second-order statistics, we can effectively consider the model to make conditionally Gaussian predictions.  Before introducing our measures of the quality of the probabilistic predictions, we first connect to our prediction model.

As discussed, for each sample our model yields a prediction tuple $\{\mu, \sigma_a, \sigma_e\}$.  We further use $\sigma$ to denote total uncertainty where $\sigma^2 = \sigma_{a}^2 + \sigma_{e}^2$.  Considering $68^{\rm th}$ percentile (``one-sigma'') confidence intervals, the lower and upper bounds of the interval are $L_\alpha = \mu - \sigma_e$ and $U_\alpha = \mu+ \sigma_e$ where, for this example of a one-sigma confidence interval, $\rm{CI} = 0.682$ and $\alpha = 1-\rm{CI} = 1 - 0.682 = 0.318$. The parameter $\alpha$ is the fraction of time the model predicts the true IQ will fall outside the confidence interval. We denote by $l_\alpha$ and $u_\alpha$ the cumulative distribution function of the (assumed Gaussian) PDF respectively evaluated at $L_\alpha$ and $U_\alpha$, i.e., $l_\alpha = 0.5 - \rm{CI}/2 = 0.159$, $u_\alpha = 0.5 + \rm{CI}/2 = 0.841$. 

%Then, our confidence is $68.2$\% that the true MPIQ value is within one standard deviation of the prediction $\mu(i)$ (we take the mean of the predicted distribution as our point prediction of MPIQ).  

We are now ready to introduce our two measures of the quality of our probabilistic predictor: average coverage area (ACE) and interval sharpness (IS).
Given $N$ predictions, let the true IQ for one sample be denoted $y$.  We also define an indicator function $\mathds{1}_\alpha$ that evaluates to 1 if the true IQ of a sample falls within the corresponding predicted confidence intervals, and zero elsewhere:
\begin{align*}
        \mathds{1}_\alpha=\left\{\begin{array}{lll}
1 & {\rm if} &  y \in\left[L_{\alpha}, U_{\alpha}\right] \\
0 & {\rm else}
\end{array}\right. .
    \end{align*}
The average coverage estimator is defined as for all samples:
\begin{align*}
        \mathrm{ACE}_{\alpha}=\frac{1}{N} \sum_{i=1}^{N} \mathds{1}_\alpha^{i} - (1-\alpha)
\end{align*}
and is a measure of the how well the confidence interval captures the realized distribution of predictions.  A value of zero tells us that exactly a fraction $1-\alpha$ of the predicted confidence intervals encapsulate the respective true IQs.  Generally if $\mathrm{ACE}_\alpha$ is small in magnitude then the prediction interval is well matched to the realized distribution of predictions.


 %   ACE is effectively the ratio of target values falling within the confidence interval to the total number of predicted samples.
    
While the average coverage area gives us a sense of the match between the predicted and realized distributions, it doesn't give us a sense of the concentration of the error.  By letting $\alpha \rightarrow 0$ all data points will fall in the bounds and so $\mathrm{ACE}_\alpha \rightarrow 0$ too.  Therefore we need a second measure of probabilistic prediction.  We use interval sharpness/interval score (IS) as this second measure \citep{gneiting_metrics, bracher_metrics}. Interval sharpness for a single sample is defined as:
%\begin{align*}
%    \mathrm{IS}_{\alpha}=\frac{1}{n} \sum_{t=1}^{n}\left\{\begin{array}{ll}
%-2 \alpha\Delta_{\alpha}(i) -4\left[l_{\alpha}(i)-y(i)\right], & y(i)<l_{\alpha}(t) \\
%-2 \alpha\Delta_{\alpha}(i) -4\left[y(i)-u_{\alpha}(i)\right], & y(i)>u_{\alpha}(t) \\
%-2 \alpha\Delta_{\alpha}(i), & \mathrm{otherwise}\end{array}\right.
%\end{align*}
\begin{align*}
    \mathrm{IS}_{\alpha}=\left\{\begin{array}{lll}
 \alpha( U_{\alpha}-L_{\alpha}) + 2\left[L_{\alpha}-y\right] \ \ {\rm if} \ \ y<L_{\alpha}, \\
 \alpha( U_{\alpha}-L_{\alpha}) \hspace{4.5em} {\rm if} \ \ L_{\alpha} \leq y \leq U_{\alpha}, \\
 \alpha( U_{\alpha}-L_{\alpha}) +2\left[y-U_{\alpha}\right] \, \ {\rm if} \ \ y>U_{\alpha}, 
\end{array}
\right.
\end{align*}
We normalize this against similar values for all samples, such that the final value lies between 0 and 1:
\begin{align}
    \mathrm{IS}_{\alpha, \rm{norm}} = \frac{\mathrm{IS}_\alpha - \min{(\mathrm{\mathbf{IS}}_\alpha)}}{\max{(\mathrm{\mathbf{IS}}_\alpha)} - \min{(\mathrm{\mathbf{IS}}_\alpha)}} \nonumber
\end{align}
and finally average the normalized values across the samples in the test set:
\begin{equation}
\overline{\mathrm{IS}}_\alpha = \frac{1}{N} \sum_{i=1}^N \mathrm{IS}_{\alpha, \rm{norm}}^i. \label{eq.defIntSharp}
\end{equation}
%In the preceding definitions $y(i) = F(Y(i); \mu(i), \sigma(i))$ where $F(\cdot, \mu, \sigma)$ is the cumulative distribution function of a Gaussian distribution of mean $\mu$ and variance $\sigma^2$.
To understand Equation~(\ref{eq.defIntSharp}) we note first that $0 \leq \overline{\mathrm{IS}}_\alpha \leq1$ and higher sharpness (less positive) corresponds to more concentration and therefore more useful predictions.  The first term, $\alpha( u_{\alpha}-l_{\alpha})$, is a constant, parameterized by $\alpha$. In our experiments we set $\alpha = 0.318$ corresponding to $\pm 1$ standard deviation. Then a smaller variance will lead to a narrower confidence interval and a smaller $\mathrm{IS}_\alpha$ {\it if} the sample falls within the confidence interval.  The sharpness is decreased ($\mathrm{IS}_\alpha$ increases) if the prediction $y$ falls outside of the confidence interval, and the penalty applied is proportional to the distance between the ground truth value and the nearest interval limit. %The factors multiplying the first and second term ($1$ and $2$) can be tweaked to the particular application.
Generally a $\mathrm{IS}_\alpha$ small in magnitude means the estimates both fall in the confidence interval {\it and} the confidence interval is narrow.

We calculate ACE and IS for all three uncertainties -- aleatoric, epistemic, and total.












\iffalse
\textbf{\underline{Metrics for Deterministic Predictions}}: For determining the `goodness' of point predictions, we adopt the following four performance criteria:

\begin{itemize}
    \item Root mean square error (RMSE):
    \begin{align}
        \mathrm{RMSE}=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(\tilde{y}(i)-y(i)\right)^{2}},
    \end{align}
    \item Mean absolute error (MAE):
    \begin{align}
        \mathrm{MAE}=\frac{1}{n} \sum_{i=1}^{n}\left|\tilde{y}(i)-y(i)\right|,
    \end{align}
    \item Normalized mean absolute percentage error (NMAPE):
    \begin{align}
        \mathrm{NMAPE}=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\tilde{y}(i)-y(i)\right|}{\max _{i \in 1, \mathrm{n}]} y(i)} \times 100 \%,
    \end{align}
    \item Bias:
    \begin{align}
        \mathrm{Bias} = \sum_{i=1}^{n} \tilde{y}(i)-y(i),
    \end{align}
\end{itemize}
where $y(i)$ and $\tilde{y}(i)$ are the true and predicted values of the $i^{th}$ sample in the test set, while $n$ is the total number of samples in the test set. A perfect prediction would result in RMSE, MAE, NMAPE, and Bias of 0.

\textbf{\underline{Metrics for Probabilistic Predictions}}: Probabilistic predictions are assessed by constructing Confidence Intervals (CI) and using them in evaluation criteria. In this work, we adopt the following four:
\begin{itemize}
    \item Average Coverage Error. ACE is an indicator to appraise the reliability of prediction interval, which has the following equation:
    \begin{align}
        \mathrm{ACE}_{\alpha}=\frac{1}{n} \sum_{i=1}^{n} c_{i} \times 100 \%-100 \cdot(1-\alpha) \%,
    \end{align}
    where $c_i$ is the indicative function of coverage and its calculation equation is as:
    \begin{align}
        c_{i}=\left\{\begin{array}{l}
1, y(i) \in\left[l_{\alpha}(i), u_{\alpha}(i)\right] \\
0, y(i) \notin\left[l_{\alpha}(i), u_{\alpha}(i)\right]
\end{array}\right.
    \end{align}
    and $\alpha$ controls the width of the prediction interval interval (PI) that we wish to obtain. In this work, we use $\alpha = 15.87$, corresponding to $1\sigma$ intervals. ACE is effectively the ratio of target values falling within the prediction interval to the total number of predicted samples.
    %\yst{What interval(s) CRUDE adopt for the calibration?}
    \item Prediction Interval Normalized Average Width (PINAW):
    \begin{align}
        \mathrm{PINAW} = \frac{\sum_{i=1}^n U_i - L_i}{nr},
    \end{align}
    where $r = y_{max} - y_{min}$. PINAW is the mean of PI widths normalized by the range of testing targets. %\yst{I think this needs to be elaborated. Like what is $U_i$. I am not sure I follow this part.}
    \item Interval Sharpness (IS):
    \begin{align}
        \mathrm{IS}_{\alpha}=\frac{1}{n} \sum_{t=1}^{n}\left\{\begin{array}{ll}
-2 \alpha\Delta_{\alpha}(i)-4\left[l_{\alpha}(t)-y(t)\right], & y(t)<l_{\alpha}(t) \\
-2 \alpha\Delta_{\alpha}(i)-4\left[y(t)-u_{\alpha}(t)\right], & y(t)>u_{\alpha}(t) \\
-2 \alpha\Delta_{\alpha}(i), & \mathrm{otherwise}\end{array}\right.
    \end{align}
    where $\Delta_{\alpha}(i) = u_{\alpha}(i)-l_{\alpha}(i)$. \yst{same for here..}
    \item Continuous Ranked Probability Score (CRPS):
    \begin{align}
        \mathrm{CRPS}=\int_{-\infty}^{\infty}(F(y)-\mathbb{1}(y-x))^{2} d y,
    \end{align}
    where $F$ is the cumulative distribution function (CDF) associated with the probabilistic forecast, and $\mathbb{1}$ is the Heaviside step function.
\end{itemize}

From the equation of ACE, a value close to zero denotes the high reliability of prediction interval. As an infinitely wide prediction interval is meaningless, IS is another indicator contrary to the coverage rate of interval, which measures the accuracy of probabilistic forecasting. $\mathrm{IS}_{\alpha}$ is always $<0$; a great prediction interval obtains high reliability with a narrow width, which has a small absolute value of IS. PINAW is the mean of PI \yst{predictive interval?} widths normalized by the range of testing targets. Our goal is to ensure that PI is not too wide, since that would make it useless as a metric. The CRPS is a measure of the integrated squared difference between the cumulative distribution function of the forecasts and the corresponding cumulative distribution function of the
observations. It CRPS generalizes the mean absolute error (MAE), and reduces to it if the forecast is deterministic. It simultaneously considers the reliability and sharpness, and is a commonly used metric to evaluated the performance of the predicted conditional PDF. Note that CRPS is the only metric that uses the raw predicted PDF, and not the predicted intervals obtained by using Equation ().
%\yst{The overall intuition is straightforward, but I think this section requires some serious rewriting such that it is clear as a standalone piece.} \yst{If IS and CRPS are not used in CRUDE, but rather we are only using them as a post-validation, I would recommend removing them. I guess it is more visual just to show the plot.}
%\textbf{MAKE SURE THIS ABOVE PARA ISN'T PLAGIARIZED; I HAVE A VAGUE RECOLLECTION OF COPYING SOME BITS FROM SOMEWHERE WITH THE INTENTION OF LATER MODIFYING THEM.}

\fi
