\subsection{Uncertainty Quantification}
\label{sec:uncertainty_quantification}


\iffalse
\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    %\includegraphics[width=.98\linewidth]{figures/cfht_iq_exptime.pdf}
    %\caption{Exposure difference for}
    %\label{fig:exptime_feature}
    \includegraphics[width=.98\linewidth]{figures/find_beta.pdf}
    \caption{Finding optimal $\beta$ for the robust variational autoencoder. The lower the reconstruction loss, the better. The larger the separation between training and noisy dataset reconstruction, the better. $\beta=0.005$ is determined to be the optimal value.\tcr{(SCD: I think we need to discuss what is the difference between uniform and constant noise.  In the caption we only refer to "noisy dataset".  Also in y-legend should be the "sign" and the "NLL" should not be italicised.  Plus the "NLL" in the log should be something like log (|NLL|) I assume -- i.e., include absolute values.)}}
    \label{fig:findbeta_elbo_vs_beta}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_rl_noise_temp.pdf}%hist_noise.pdf}
    \caption{Histogram of reconstruction losses along with the $95^{\rm th}$ percentile cut-off.}
    \label{fig:findbeta_hist0}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_mll_noise_temp.pdf}%hist_noise.pdf}
    \caption{Histogram of marginal log likelihoods along with the $5^{\rm th}$ percentile cut-off.}
    \label{fig:findbeta_hist1}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_regret_noise_temp.pdf}%hist_noise.pdf}
    \caption{Histogram of likelihood regrets \citep{likelihood_regret} along with the $95^{\rm th}$ percentile cut-off.  \tcr{(SCD: I think this data is not from Xiao et al, so the caption is misleading -- maybe push that citation to the discussion in text?)}}
    \label{fig:findbeta_hist2}
\end{subfigure}
\caption{In (a), we determine the optimal $\beta$ based on marginal log likelihood's ability to separate the medians of each of the three histograms.  In (b)-(d) we present histograms for three distinct metrics: (b) reconstruction loss, (c) marginal log likelihood, and (d) likelihood regret.  In each plot we plot histograms of the respective metric for three distinct training sets: the actual training set, simulated uniform noise, and simulated constant noise. \tcr{(SCD: what is ``constant'' noise -- seems any oxymoron.)} By injecting synthetically generated noise through an RVAE trained on the training set, we can asses the ability of different metrics to separate in-distribution (ID) from out-of-distribution (OOD) sets. \tcr{(SCD: Is this all detailed in the text?)} }
\label{fig:findbeta}
\end{figure*}
\fi


%\begin{figure*}
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_episstd_features.pdf}
%    \caption{Histograms of uncalibrated epistemic uncertainties from the MDN, for various data sets. Vertical line is the $95^{\rm th}$ percentile value for the training set.}
%    \label{fig:vae_hist_epis}
%\end{subfigure}
%\hfill
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_calepisstd_features.pdf}
%    \caption{Histograms of calibrated epistemic uncertainties from the MDN, for various data sets. Vertical line is the $95^{\rm th}$ percentile value for the training set.}
%    \label{fig:vae_hist_calepis}
%\end{subfigure}
%\newline
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_mll_noise_features.pdf}
%    \caption{Histograms of the pseudo marginal log likelihood (-L$_{\rm ELBO}$) from the RVAE. Vertical line is the $95^{\rm th}$ percentile value for the training set.}
%    \label{fig:vae_hist_mll}
%\end{subfigure}
%\hfill
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_regret_noise_features.pdf}
%    \caption{Histograms of log likelihood regret for the same data sets, from the RVAE. Vertical line is the $95^{\rm th}$ percentile value for the training set.}
%    \label{fig:vae_hist_regret}
%\end{subfigure}
%\newline
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_mll_noise_zoomed_features.pdf}
%    \caption{Zoom-in of \textbf{(c)}, to focus on the hard OoD data sets.}
%    \label{fig:vae_hist_mll_zoomed}
%\end{subfigure}
%\hfill
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/hist_regret_noise_zoomed_features.pdf}
%    \caption{Zoom-in of \textbf{(d)}, to focus on the hard OoD data sets.}
%    \label{fig:vae_hist_regret_zoomed}
%\end{subfigure}
%\label{fig:common_ood_detections}
%\caption{We visualize the discriminative ability of various metrics to separate Out-of-Distribution (OoD) samples from In-Distribution (ID) samples. Surprisingly, we notice that epistemic uncertainties, whether calibrated or uncalibrated, are poor metrics for OoD detection. This was our motivation to design and use the RVAE, which directly captures the likelihood of the training data-generating distribution. \textbf{(c)} and \textbf{(e)} show that the log likelihood is an excellent -- if not perfect -- separator of ID training and test data, from slightly yet certainly OOD synthetic data created by adding Gaussian noise with the indicated $\sigma$ to the normalized (between 0 and 1) training data. In \textbf{(d)} and \textbf{(f)} we calculate the log likelihood regret \citep{likelihood_regret}, as explained in Section \ref{sec:rvae}. Comparing \textbf{(e)} and \textbf{(f)}, we see that regret is a slightly better OoD detector -- the training data histogram is more concentrated, and the modes of the histograms for the two noisy datasets are farther away from the mode of the histogram for the training data set.}
%\end{figure*}

Our predictions will be safer for decision making if for each input vector, in addition to the prediction of IQ, we also predict the degree of (un)certainty. This is especially true since we aim to toggle the twelve vents based on our predictions, which is an expensive manoeuvre -- a configuration of vents that ends up increasing observed IQ as opposed to decreasing it would require re-observation of the target, when CFHT is already oversubscribed by a factor of $\sim 3$. For this reason, we predict a probability density function of MPIQ for every input sample, as described in Section \ref{sec:mdn}.


%Any model mapping a set of covariates to outputs should realistically be probabilistic in nature. In other words, for each input vector associated with an output label, the model should predict not just a measure of central tendency, but also a measure of spread (for example, the mean and standard deviation if one assumes normal distribution for the output variable). This uncertainty naturally arises due to a number of reasons:
%\begin{itemize}
%    \item It is possible that in spite of feature engineering (described in Section \ref{sec:feature_engineering}), our data set does not have every feature required for confident prediction of MPIQ values. This could simply be because the data set with fewer features, $\mathcal{D_{F_S, S_S}}$, is lacking some strongly informative features. As an exaggerated example, consider the task of training a model to predict the digit (from 0 to 9) present in an image. If one were to feed this model only the right half of each image for training, we would intuitively expect it to be very uncertain if an image contains a 3 or an 8.
%    \item Every sensor and measuring device has a certain intrinsic measurement error, which would carry forward to all downstream calculations.
%    \item Any machine learning algorithm is only as good as the data on which it is trained. It might well be the case that a future observation lies in isolation in the multi-dimensional parameter space, i.e., be an outlier. In such a case, a well-behaved model should intuitively be able to detect this \emph{out-of-distribution} behavior.
 %   \item Even if our model is able to satisfactorily predict MPIQ, it is very well possible that there exist other models which can also make predictions just as, if not more, accurately and precisely. It is also possible that for our chosen model, a different data pre-processing scheme or choice of hyperparameters %%, or choice of scheme for balancing data \textbf{insert section}
 %   might result in better predictions.
%\end{itemize}

%Owing to these factors, a highly desirable behavior of any predictive model would be to not only return a point prediction, but also some quantity conveying the model's belief or confidence in its output \citep{kendall2017uncertainties, choi2018uncertainty}. 

Higher error (corresponding to lower model belief or confidence in the estimate) can result from absence of predictive features, error or failure in important sensors, or an input vector that value has drifted from the training distribution. We decompose the sources of predictive uncertainties into two distinct categories: {\it aleatoric} and {\it epistemic}.
%\cite{kendall2017uncertainties}\footnote{The word epistemic comes from the Greek ``episteme", meaning ``knowledge" and corresponds to... Aleatoric comes from the Latin ``aleator", meaning ``dice player"; aleatoric uncertainty is the ``dice player's" uncertainty \citep{gal_thesis}}. 
Aleatoric uncertainty captures the uncertainty inherent to the data generating process. To analogize using an everyday object, this is the entropy associated with an independent toss of a fair coin. Epistemic uncertainty, on the other hand captures the uncertainty associated with improper model-fitting. In contrast to its aleatoric counterpart, given a sufficiently large data set epistemic uncertainty can theoretically be reduced to zero\footnote{The word ``aleatoric'' derives from the Latin ``aleator'' which means ``dice player''. The word ``epistemic'' derives from the Greek ``episteme'' meaning ``knowledge''~\citep{gal_thesis}.}. Aleatoric uncertainty is thus sometimes referred to as {\it irreducible} uncertainty, while epistemic as the {\it reducible} uncertainty. High aleatoric uncertainty can be indicative of noisy measurements or missing informative features, while high epistemic uncertainty for a prediction could be a pointer to the outlier status of the associated input vector.

%and can indeed be explained away given an a sufficiently large training dataset\footnote{If we are given a biased coin where we do not know the probability $p$ of turning up heads, we can determine it to arbitrary precision by flipping the coin an arbitrarily large number of times and counting the number of times it turns up heads.}. 



%measurement noise in filters, or the inherent randomness of a coin flipping experiment. It cannot be reduced by collecting more training data

%\footnote{We know that a fair coin has a 0.5 probability of landing heads, yet we cannot assert with 100\% certainty the outcome of the next flip, no matter how many times we have already flipped the coin.}; however, it can be reduced by collecting more informative features. %\footnote{Uncertainty in galaxy mass estimation in the absence of H and K band information can be drastically reduced by collecting high quality data in these bands.}.


%On the other hand, epistemic uncertainty models the ignorance of the predictive model, and can indeed be explained away given an a sufficiently large training dataset\footnote{If we are given a biased coin where we do not know the probability $p$ of turning up heads, we can determine it to arbitrary precision by flipping the coin an arbitrarily large number of times and counting the number of times it turns up heads.}. 



%Aleatoric uncertainty covers first two of the four issues above, while epistemic uncertainty subsumes the last two. High aleatoric uncertainty can be indicative of noisy measurements or missing informative features, while high epistemic uncertainty could be a pointer to its status as an outlier.

The architecture of the MDN (Section \ref{sec:mdn}) allows us to predict a PDF of MPIQ for each sample. For each sample and mixture model component, let $\mu_m$, $(\sigma_m)^2$, and $\alpha_m$ respectively denote the mean, variance, and normalized weight (weights for all mixture model components must sum to 1) in the mixture model. We obtain the predicted IQ value as the weighted mean of the individual means: 
\begin{equation}
\mu = \sum_{m=1}^M \alpha_m \mu_m \label{eq.calcIQpredict}
\end{equation}
Aleatoric uncertainty is the weighted average of the mixture model variances, calculated as \citep{choi2018uncertainty}:
\begin{equation}
\sigma_{\mathrm{al}}^2 =  \sum_{m=1}^{M} \alpha_m\sigma_m^2, \label{eq.calcAleaUncert}
\end{equation}
while epistemic uncertainty is the weighed variance of the mixture model means: 
\begin{equation}
\sigma_{\mathrm{epis}}^2 = \sum_{m=1}^{M} \alpha_m \mu_m^{2} - \mu^{2} \label{eq.calcEpisUncert}
\end{equation}
The total uncertainty is computed by adding Equations ~(\ref{eq.calcAleaUncert}) and~(\ref{eq.calcEpisUncert}) in quadrature.%  \tcr{(SCD: Note the notation needs to be unified with earlier -- here $\sigma_{i,j}$ while earlier $\sigma^{(i)}$.  Also, we don't use the term "in quadrature" (means something different in communication theory) -- I guess you just are convolving two Guassians so taking the square root of the squares of the standard deviations -- basically Pythoagorean's Theorem?  Is ``in quadrature'' standard in astronomy?)}