\section{Data} \label{sec:data}

\iffalse
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/exposuretime_hist.pdf}
    \caption{\textbf{Top:} Histogram of $\log_{10}$ exposure times of the $\sim$60,000 samples/exposures used in this paper. \textbf{Bottom:} Histogram of $\log_{10}$ of the time difference between two consecutive samples. Vertical lines mark the $16^{\rm th}$, $50^{\rm th}$, and $84^{\rm th}$ percentile values. \tcr{(SCD: I don't see any "vertical lines'' -- perhaps those were in an earlier version?)} \seb{is log scale necessary?} \sg{(Sankalp: Drat, I meant to recreate these WITH the vertical lines, totally forgot to do it. Will fix. Sebastien: I tried without log scale, but the figures didn't really look very good -- I'll share with you privately figures made with linear scale.)}}
    \label{fig:exptime}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/filter_hist.pdf}
    \caption{\textbf{Left:} Collected observations per year, broken down by quarter. \textbf{Right:} Same observations split per filter represented by their central wavelength.}
    \label{fig:yearonyear}
    \end{subfigure}
    %\caption{Distribution of samples in the post-processed data set that is input into our machine learning models, as a function of different variables.}
    %\label{fig:exptimeplusyearonyear}
    \caption{\tcr{SCD: Needs a figure caption (so we can have a figure number)} \sg{(Sankalp: Not sure what a good caption would be. Help?)}}
    \label{fig:NeedAFigNumber}
\end{figure*}
\fi

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/filter_hist.pdf}
    \caption{\textbf{Left:} Histogram of exposure times of the $\sim$60,000 samples/exposures used in this paper. \textbf{Middle:} Collected observations per year, broken down by quarter. \textbf{Right:} Same observations group by filter as represented by the filter central wavelength.}
    %\caption{\tcr{SCD: Needs a figure caption (so we can have a figure number)} \sg{(Sankalp: Not sure what a good caption would be. Help?)}}
    \label{fig:dataexploration}
\end{figure*}

In this section we discuss how we curated and prepared the data for use in our models. As mentioned already, our efforts began with  almost a decade's worth of sensor measurements archived at CFHT, together with IQ measured on the MegaCam exposures retrieved from the Canadian Astronomy Data Center (CADC) web services. 
At the start pertinent variables were spread across multiple data sets, sensor measurements were missing due to sensor failures,  data records contained errant values due to mis-calibrated data reduction pipelines. We therefore spent substantial effort cleaning the data.
In Section~\ref{sec:rawData} we discuss the various data sources that we collate to form our final data set. We then discuss our data cleaning and feature engineering procedures in Section~\ref{sec:data_cleaning} and Section~\ref{sec:feature_engineering}.


%Data curation is the first step of any data analysis pipeline and is an umbrella term for three sequential steps: data discovery, data integration, and data cleaning \citep{datacuration}. Data curation starts with finding data sets germane to the task at hand,  merging them into a single workable unit, accounting for their often disparate and multi-modal natures. Data cleaning includes identification and of removal of data repetitions, outliers, and errant values, finding ways to work with missing data, and using existing predictive variables to create novel ones which could provide additional predictive ability to the model. 




\subsection{Data sources}\label{sec:rawData}
Our first step in data collection was to build a data archive that contains one record per MegaCam exposure. In the remainder of this paper, we refer to each exposure and its associated sensor measurements interchangeably as a ``sample'', a ``record'', or an ``observation''. Each record contains three distinct types of predictive variables: 
\begin{enumerate}
    \item \textit{Observatory parameters}. These can be divided into operating (controllable) and non-operating (fixed) parameters. The former include the configurations of the twelve dome vents (open or closed), and the windscreen setting (degrees open).  These are examples of the variables that we can adjust the settings of in real time. The non-operating features include measurements of the telescope altitude and azimuth (which correspond to pointing of the astronomical object being observed) and the central wavelength of the observing filter.
    \item \textit{Environmental parameters}. These include exposure-averaged wind speed, wind direction, barometric pressure, and temperature values at various points both inside and outside the observatory. 
    \item \textit{Ancillary parameters}. Each exposure come with metadata. Relevant to our work are the date and time of the observation and the length of exposure. All predictive variables have been summarized in Table \ref{table:megaCamData}.
%\tcr{(SCD: This is a strange paragraph -- mixing exposure time, number observation, number of variables, and something about data records.  Also here we list 86 variables while in the data set table it lists 119.  What's the difference?  Maybe we should delete this paragraph or move the relevant information elsewhere?)}
The median time of each exposure is $\sim$150 seconds, while the median time between two consecutive exposures made on the same night is $\sim$240 seconds\footnote{We emphasize that in this work we forego temporal dependencies and treat all exposures as independent. We provide the time between exposures for the sake of context.}. 
\end{enumerate}

In total, there are 160,341 observations, and 86 variables (\textit{including} image quality) are provided with each exposure. The records span February 2005 to March 2020. An overview of the data is provided in Table~\ref{tab:datasets_overview}, where we note the expanded number of features created using {\it feature engineering}, which we expound upon below.

%Table \ref{tab:summarystats} lists summary statistics of the ten most important features.
%\kw Moved this to the end because it's a separate topic

%\tcr{(SCD: Do we need this paragraph on MKAM?)} MKAM is a smaller telescope on a  separate structure adjacent to the CFHT dome wherein MegaCam is housed.  MKAM houses a DIMM (differential image motion monitor) that provides estimates of $\iqAtmo$ (but not of $\iqDome$ or $\iqOpt$) every 60 seconds.  MKAM became operational in 2009.  When MKAM measurements are available, we compute the average and standard deviation of the MKAM DIMM IQ estimates over the duration of the MegaCam exposure to provide a point of comparison.



%We used this dataset in \S\ref{sec:DL_SL} in our supervised results. \yst{Maybe a table summarizing the variables.}
%Our second step in data collection was to complement the initial archive with seeing data from other instruments. Since instruments are routinely swapped to meet differing science goals, no single instrument is uninterruptedly in operation at CFHT. These other instruments are online in period for which MegaCam was offline and include SPIRou, SITELLE, and WIRCam \seb{probably ESPaDOnS as well?}.  SPIRou is a near-infrared spectropolarimeter. SITELLE is a wide-field Fourier transform spectrograph. WIRCam is composed of four infrared detectors optimized for the J, H, and K spectral bands.  Respectively these instruments add \tcr{$150,000$, $85,000$ and $100,000$} records to the data archive.  \tcr{(SCD: I think the above may be old numbers)} Each of these instruments  produces an IQ measurement \seb{might be worth mentioning how those IQ measurements are performed - and when relatively to MCAM}.  And each data record here contains one such IQ measurement plus the same predictive variables on environmental conditions and operating parameters as for MegaCam.  This data set consists of \tcr{xxx} records, spanning observations made from \tcr{month/year} to \tcr{month/year}. As the IQ produced by these instruments is not the same as that produced by MegaCam, we use these latter data in \S\ref{sec:DL_SSL} in our semi-supervised results.  
%Our archive combines the measurements of all instruments in a single unified formatting.
%When a measurement is not available, e.g., there is no MegaCam IQ data, or when a sensor has failed, we populate the missing field with ``NaN''.  \yst{Did we end up doing data inputation?} We also note that the cadence of the data varies both across instruments and for each instrument.  For example, MegaCam exposure length varies from around \tcr{xxx} seconds on the short end to \tcr{xxx} seconds on the long end.  \tcr{We describe these steps Figure ()() through (). }
%The dome vents are particularly important to understand. As we describe in \S\ref{sec:relatedWork}, they were installed in \tcr{} to allow for colder mountain air to `flush out' the heated air inside the dome, in an attempt to reduce \emph{dome seeing}--the portion of the IQ that can be attributed to turbulent air between the telescope mirror and the dome shutter. An important goal of this work is to discover relationships between the ON/OFF positions of these vents, and the observed Megacam image quality, with a goal to find optimal vent poistions as a function of other features. (\sg{Ask Andy or Billy about the UW wint tunnel test, and get a reference.})Since \tcr{X}, the vent positions have barely been changed--see Figure () comparing the different vent configurations.The default position so far has been to MKAM DIMM and MASS values during integration \bm {in my math, i use MKAM(dimm+mass) of the moment, and not the 'average' fields in the headers - I don't trust those data and don't have the understanding of how they are provided.}, 
%\bm{my comment here is still valid.  in the data i provide, the MKAM measurements are of the moment (+-60s) of the Megacam (or any other instrument) start time.  There is no summary mathematics applied as I think the previous writings indicated.}

%this somewhat a rough all data table structure: epoch, environmental (outside temperature, wind speed, wind direction, barometric pressure, etc.), CFHT facility ([instrument],[inside temperatures],vents[open or closed],telescope pointing, dome position or config, science data [iq, sky-background, etc.]), MKAM data (telescope pointing, seeing, mass[mass seeing profiles]) and this is what we're looking to populate to drive the analysis with: epoch, MKAM seeing, MP IQ, WC IQ, ESP IQ, SIT IQ, SPI IQ,

\subsection{Data Cleaning}
\label{sec:data_cleaning}
We now list the data cleaning we performed.  In short these included the removal of data records corresponding to (i) non-sidereal targets, (ii) data records associated with too-short or too-long exposures, (iii) data records associated with IQ estimates deemed unrealistic, and (iv) data records containing missing or errant data values. 

% SF: moved to feature engineering section.
%In the remaining data records we (v) removed a the contribution to $\iqMea$ from instrument optics $\iqOpt$, (vi) normalized the frequency band of measurement to make the resulting $\iqMea$ comparable, and (vii) made atmospheric corrections that depend on instrument pointing. We provide additional details below.

\begin{enumerate}
%    \item The data records corresponding to the most recent 18 months of experiments are  proprietary. As these data are not yet in the public domain we remove proprietary data including identifying information such as investigator name and program and target identity.
    %%%%%%%%%%%%%%%%%%
    \item \textit{ Non-sidereal:} We remove moving, non-sidereal, targets.   The IQ measurements for these data records are not valid as the data pipeline that calculates IQ assumes sidereal observation.  Therefore these records are not appropriate for training.  We note that as part of the configuration data recorded along with each observation the astronomer specifies whether the observation is sidereal or not. Hence, these data records are easy to remove.
    %%%%%%%%%%%%%%
    %\item {\bf Too-short or too-long exposures:} We remove data records corresponding either to very short ( $< 10$ seconds) or very long ($> 30$ minutes) exposures.  \tcr{These are removed because...} \sg{(Billy:add citation -- can we also say *why* these were removed? Do we think the big end is non-sidereal?)} 
    %%%%%%%%%%%
    \item \textit{Non-trustworthy IQ estimates:} We remove MegaCam exposures associated with IQ estimates of less than $0.15''$ or greater than $2''$.  Such  IQ numbers are deemed unrealistic.  It is believed that an IQ of $\sim 0.2''$ is the best possible at Maunakea.  Anything below this is deemed to result from an erroneous calculation when converting from the raw exposure data. On the other hand, IQ $>2''$ is too large for useful  science.
    %%%%%%%%%%%%%%%
% SF: these are too obvious: biases and darks have no photons, dome flats no stars, and sky flats not really any seeing.
%\item {\bf Calibration records:} Certain data records are made for calibration purposes and post-acquisition analysis, e.g. ``biases'', ``darks'' or ``flats''. These targets don't yield  IQ estimates so we exclude them from the archive.
    %%%%%
    \item \textit{Missing and errant measurements:} Not all sensor measurements are available at all times of the exposure.  We refer to these as "missing data". As is tabulated in the first row of Table~\ref{tab:datasets_overview}, prior to considering missing data, our cleaned data set (cleaned of non-sidereal and non-trustworthy) contains $120$ features ($86$ original + rest engineered features + 1 MPIQ, see Section~\ref{sec:feature_engineering}) and $160,341$ samples. Of these, just under $100,000$ samples \textit{do not} contain all measurements; we specify the fraction of missing measurements in the last column of Table~\ref{tab:datasets_overview}. We refer to the dataset with {\it all} samples as $\mathcal{D_{F_S,S_L}}$, i.e., $\mathcal{D}$ata set with a $\mathcal{S}$mall number of $\mathcal{F}$eatures, and a $\mathcal{L}$arge number of $\mathcal{S}$amples. By removing those samples that contain at least one missing feature, we obtain $\mathcal{D_{F_S,S_S}}$: $\mathcal{D}$ata set with a $\mathcal{S}$mall number of $\mathcal{F}$eatures, and a $\mathcal{S}$mall number of $\mathcal{S}$ samples. This latter dataset consists of 63,082 samples (second and third rows in Table~\ref{tab:datasets_overview}. % SF: shift to the feature engineering %In addition to the raw features from the data, in Sec.~\ref{sec:feature_engineering}, we hand-craft a number of features that helps the learning system to extract the information embedded in the data.  This expands the number of features from $119$ to $1200$.  In the third line of Table~\ref{tab:datasets_overview} we denote this dataset as %$\mathcal{D_{F_L,S_S}}$: $\mathcal{D}$ata set with a $\mathcal{L}$arge number of $\mathcal{F}$eatures, and a $\mathcal{S}$mall number of $\mathcal{S}$amples.
    In this paper, we train our models on $\mathcal{D_{F_S,S_S}}$, since feed-forward neural networks cannot handle missing values without non-trivial modifications. In future work, we will use a variational autoencoder capable of imputing missing values \citep{vae_missingvalues} to enable us to leverage the larger dataset, $\mathcal{D_{F_L,S_L}}$: $\mathcal{D}$ata set with a $\mathcal{L}$arge number of $\mathcal{F}$eatures, and a $\mathcal{L}$arge number of $\mathcal{S}$amples.

\begin{table}
    \caption{Summary statistics of data sets described in Section \ref{sec:data_cleaning}. `\#Original Features' includes the MegaPrime Image Quality (MPIQ), while `\#Engineered Features' are additional hand-crafted ones added to enhance the predictive capability of our models (see Section \ref{sec:feature_engineering} for details). However for the remainder of this paper, we use `features' to refer to the union of original and engineered features less the MPIQ column: these are predictive, independent variables. Similarly, going forward MPIQ -- the dependent variable -- is referred to as the `target'.}
    \label{tab:datasets_overview}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \toprule
        \thead{Dataset \\ Identifier} & \#Samples & \thead{\#Original \\ Features} & \thead{\#Engineering \\ Features} & \thead{Percentage \\ Missing} \\ \midrule
        $\mathcal{D_{F_S,S_L}}$ & 160,341 & 86 & 34 & 62\% \\
        $\mathcal{D_{F_S,S_S}}$ & 63,082 & 86 & 34 & 0\% \\
        $\mathcal{D_{F_L,S_S}}$ & 63,082 & 86 & 1115 & 0\% \\
        $\mathcal{D_{F_L,S_L}}$ & 160,341 & 86 & 1115 & 62\% \\        
        \bottomrule
    \end{tabular}
\end{table}

\end{enumerate}
%In the rest of this paper, we will refer to the corrected MPIQ values (output of Equation (\ref{eqn:iq_filter_plus_zenith_correction})) as MegaPrime IQ, or MPIQ.  %\tcr{(SCD: I am confused, why do we want the corrected atmospheric seeing?  Don't we care about the measured seeing or the dome seeing (if we are trying to improve it)?)}

\begin{figure*}
   \centering
   \includegraphics[width=1.0\linewidth]{figures/cfht_rvae+mdn.pdf}
   \caption{The architectures of the two networks used in this study. To the \textbf{left} of the dashed vertical line, we show the overview of a variational autoencoder, but note that the \textit{robust} VAE (i.e. RVAE) used in this work uses a special reconstruction loss (comparing \textbf{x} and \textbf{x'}) which is not depicted in this cartoon. On the \textbf{right} we show a dense feed-forward network with skip connections, mish activations \citep{mish_activation}, positional normalization \citep{positional_normalization_pono}, batch normalization \citep{batchnorm},  %group normalization \citep{group_normalization},
   and momentum exchange \citep{moex} augmentation layers. This mixture density network (MDN) has 5 components. Near the right edge of the figure, we indicate in cyan colored rectangles the output shapes after an input sample has passed through each layer.}% This choice was based on an initial trial-and-error analysis, and will be optimized in the future with hyperparameter optimization.}
   \label{fig:rvae_plus_mdn}
\end{figure*}

\subsection{Feature Engineering}
\label{sec:feature_engineering}
Feature engineering is the process of modifying existing features, using either domain expertise, statistical analysis, or intuition derived from scientific expertise.  The goal is to create predictive variables that are more easily understood by an ML algorithm. We now describe the feature engineering we performed. %As discussed above, and as is tabulated in Table~\ref{tab:datasets_overview}, we apply the feature engineering steps to $\mathcal{D_{F_S,S_S}}$ to produce $\mathcal{D_{F_L,S_S}}$. 

\begin{enumerate}
\item \textit{Optics IQ correction:} We remove the fixed, but wavelength dependent, contributions of the telescope optics to IQ, $\iqOpt$. These corrections are based on work by \cite{salmon2009cfht}, and range from $0.31''$ in the $i$-band to $0.53''$ in the $u$-band \citep{racine2018}; cf. second column of Table \ref{tab:per_filter_contrib}. 
After removing the contribution of optics, we are  left with a convolution of dome seeing and atmospheric seeing. This is because dome seeing, referred here to as ${\rm IQ}_{\rm Dome}$, is enmeshed with ${\rm IQ}_{\rm Measured}$ in a complicated way that does not lend itself to easy separation; the relationship between these is governed by Equation~(\ref{eqn:iq_optics_correction}), a rearranged version of Equation~(\ref{eqn:iq_measured}):

\begin{align}
&\iqAtmo^{5/3} + \iqDome^{5/3} = \iqMea^{5/3} - \iqOpt^{5/3}, \nonumber \\
&\rm{IQ}_{\rm Atmospheric}^{'} = \left(\iqMea^{5/3} - \iqOpt^{5/3}\right)^{3/5}
\label{eqn:iq_optics_correction}
\end{align}

%\seb{TODO: move around and summarize this section once we agree on the related work/introduction, there are many overlaps.}  \tcr{(SCD: As Sebastien notes, the following strongly overlaps with the 3rd-to-last paragraph of Sec. 1.  Also the numbers are different here.  Finally the calculation here is both different from before and not quite right -- rather than $0.20''$, $(0.43^{5/3} - 0.37^{5/3})^{3/5} = 0.174$.  Maybe just delete the following paragraph but finish with a comment on how~(\ref{eqn:iq_optics_correction}) will be used -- maybe in the loss function?)}

\begin{table}
    \centering
    \caption{IQ$_{\rm Optics}$ for different bands, calculated according to the prescription of \citet{salmon2009cfht}. The average seeing across all bands is about $0.33''$, as noted in  Table 4 of \citet{salmon2009cfht}.}
    \label{tab:per_filter_contrib}
    \begin{tabular}{ccc}
        \toprule
        \toprule
        Filter & Central $\lambda$ (nm) & IQ$_{\rm Optics} ('')$ \\
        \midrule
        Ha.MP7605 &	645.3 &	0.284 \\
        HaOFF.MP7604 &	658.4 &	0.280 \\
        Ha.MP9603 &	659.1 &	0.280 \\
        HaOFF.MP9604 &	671.9 &	0.276 \\
        TiO.MP7701 &	777.7 &	0.260 \\
        CN.MP7803 &	812 &	0.260 \\
        u.MP9301 &	374.3 &	0.441 \\
        u.MP9302 &	353.8 &	0.459 \\
        CaHK.MP9303 &	395.2 &	0.424 \\
        g.MP9401 &	487.2 &	0.358 \\
        g.MP9402 &	472 &	0.368 \\
        OIII.MP7504 &	487.2 &	0.358 \\
        OIII.MP9501 &	487.2 &	0.358 \\
        OIIIOFF.MP9502 &	500.7 &	0.350 \\
        r.MP9601 &	628.2 &	0.290 \\
        r.MP9602 &	640.4 &	0.285 \\
        gri.MP9605 &	610.68 & 0.296 \\
        i.MP9701 &	777.6 &	0.261 \\
        i.MP9702 &	764.4 &	0.261 \\
        i.MP9703 &	776.4 &	0.261 \\
        z.MP9801 &	1170.2 & 0.397 \\
        z.MP9901 &	925.6 &	0.276 \\
        \bottomrule
    \end{tabular}
\end{table}

At the risk of being redundant with information presented towards the tail-end of Section \ref{sec:relatedWork}, we remind the readers that \cite{racine2018} and \cite{salmon2009cfht} estimate ${\rm IQ}_{\rm Atmospheric}$ to be in the range of $0.43''$ to $0.45''$, and ${\rm IQ}_{\rm Atmospheric}^{'}$ to be about $0.55''$. They demonstrate that opening all 12 vents completely allows one to reduce ${\rm IQ}_{\rm Atmospheric}^{'}$ to about $0.51''$, which leaves a residual median $0.20''$ $\left((0.51^{5/3} - 0.43^{5/3})^{3/5}\right)$ on the table, which is what we aim to capture in this paper. These numbers also agree with our own calculations, as described in Section \ref{sec:relatedWork} and visualized in the top two sub-figures of Figure \ref{fig:hist_preliminaries}. Our argument, introduced in Section \ref{sec:introduction} and expounded upon in Section \ref{sec:introduction}, is that for any given observation, there is an optimal set of vent configuration, somewhere between all-open and all-closed, that allows us to bite into this $0.20''$ residual ${\rm IQ}_{\rm Dome}$. 

\item \textit{Wavelength IQ correction:}
%\seb{do we know this correction was useful for our prediction metrics? update: we post-justify all this feature engineering thanks to the boosting tree results which we do not use :)}
Each MegaCam exposure is taken using one of 22 band-pass filters.  The right-hand subfigure in Figure~\ref{fig:dataexploration} plots a histogram of  observations across  bands. The use of the filters results in a wavelength-dependent IQ variation.   To make IQ measurements consistent we scale IQ to a common wavelength of $500$nm. The formula for the scaling is provided in Equation~(\ref{eqn:iq_filter_plus_zenith_correction}), which we present in conjunction with a zenith angle correction, discussed next.

\item \textit{Zenith angle correction:} IQ is also affected by the amount of atmosphere through which the observation is made. The contribution of air mass is, to first degree, predictable, and can be removed together with the wavelength correction via~(\ref{eqn:iq_filter_plus_zenith_correction}), where $z$ is the zenith angle in degrees and $\lambda$ is the central wavelength of a given filter in nm.

\begin{align}
\iqCorr = \iqAtmoPrime \times \left(\frac{\lambda}{500}\right)^{1/5} \times \left(\cos{z}\right)^{3/5} \label{eqn:iq_filter_plus_zenith_correction}
\end{align}
    
\item \textit{Cyclic encoding of time-of-day and day-of-year:}
Every observation has an associated time stamp, indicating the beginning of image acquisition. Using this `timestamp' feature, we derive two time-features, the hour-of-day and the day-of-year.  These features better capture latent cyclical relationship between weather events and IQ. We represent each of these two features into a pair cyclical `sinusoidal' and `co-sinusoidal' component. For example, for the day-of-week feature values -- which can range from 0 to 6 -- we encode it as day-of-week-sine and day-of-week-cosine.  These can each respectively take on values from $\sin{\left(0\times180\degree/6\right)}$, to $\sin{\left(6\times180\degree/6\right)}$, and $\cos{\left(0\times180\degree/6\right)}$, to $\cos{\left(6\times180\degree/6\right)}$. In this way we replace the timestamp feature with four new, and more easily digestible features.

\item \textit{Cyclic encoding of azimuth:} Similar to the temporal information, we cyclically encode the telescope azimuth, splitting it into two features. We note that since the altitude of observation ranges from $0$ to $90$ degrees, and is not cyclical in nature, we leave that feature unmodified.
    
\item \textit{Temperature differences features:} 
As argued in our discussion, and evidenced by the prior work, temperature differences are the prime source of turbulence. In recognition of this key generative process, we engineer new temperature features that consist of the pairwise differences of existing temperature measurements.

We note that, given sufficient data, a deep enough neural network should be able to discover that temperature differences are important features.  We engineer in such features as, from our knowledge of physics, we understand temperature difference are important and providing them explicitly to the network eases the inference task faced by the network. In addition, unlike a neural network, the boosted-tree model that we use for comparative analysis is, by design, unable to create new features. The boosted-tree therefore benefits quite significantly from increased feature representation.

We implement two different flavors of engineering here. First, for every temperature feature in our two data sets of 160,341 and 63,082 samples, we subtract it from every other temperature feature.  We calculate the Spearman correlation of these newly generated features with the MPIQ values.  We then rank them by magnitude in descending order and pick the top three features. This increases our original 85 input features to 119, and this is how we get $\mathcal{D_{F_S,S_L}}$ and $\mathcal{D_{F_S,S_S}}$. For the second variation, we do not pick the top 3, but retain all the newly generated temperature-difference features.  This increases the number of features from 86 to 1115. This is how we get $\mathcal{D_{F_L,S_L}}$ and $\mathcal{D_{F_L,S_S}}$. This is summarized in Table~\ref{tab:datasets_overview}. As a reminder, in this work, we only use $\mathcal{D_{F_S,S_S}}$; empirical results showed that our neural networks' performance did not significantly improve by using $\mathcal{D_{F_L,S_S}}$.
\end{enumerate}


%\begin{figure*}
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/one_to_one.pdf}
%    \caption{Predicted (y-axis) v/s true (x-axis) MPIQ. For the former, we show the calibrated median, and upper and lower quantiles for calibrated uncertainties.}
%    \label{fig:mdn_one_to_one}
%\end{subfigure}
%\hfill
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/one_to_one_hist.pdf}
%    \caption{Histogram of prediction errors, along with three metrics comparing performance with and without calibration.}
%    \label{fig:mdn_one_to_one_hist}
%\end{subfigure}
%\newline
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/spread_vs_mpiq1.pdf}
%    \caption{68\% spread in calibrated and uncalibrated aleatoric and epistemic uncertainties in predicted MPIQ, as a function of true MPIQ.}
%    \label{fig:mdn_CI_vs_mpiq1}
%\end{subfigure}
%\hfill
%\begin{subfigure}{0.49\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/spread_vs_mpiq2.pdf}
%    \caption{68\% spread in total uncertainties in predicted MPIQ, as a function of true MPIQ.}
%    \label{fig:mdn_CI_vs_mpiq2}
%\end{subfigure}
%\newline
%\begin{subfigure}{0.32\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_aleatoric.pdf}
%    \caption{Calibration curve for aleatoric uncertainties.}
%    \label{fig:mdn_calibration_curve_al}
%\end{subfigure}
%%\hfill
%\begin{subfigure}{0.32\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_epistemic.pdf}
%    \caption{Calibration curve for epistemic uncertainties.}
%    \label{fig:mdn_calibration_curve_epis}
%\end{subfigure}
%%\hfill
%\begin{subfigure}{0.32\textwidth}
%    \centering
%    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_total.pdf}
%    \caption{Calibration curve for total uncertainties.}
%    \label{fig:mdn_calibration_curve_total}
%\end{subfigure}
%\caption{Prediction results from the Mixture Density Network. In \textbf{(a)} we plot the predicted MPIQ, characterized by the $16^{\rm th}, 50^{\rm th}$, and $84^{\rm thn}$ percentile values of their respective calibrated PDFs, against the ground truth MPIQs. In \textbf{(b)} we subtract the ground-truth MPIQs from the $50^{\rm th}$ percentile predictions, both from the raw uncalibrated and the calibrated PDFs, and plot their histograms. We also quantify the quality of predictions by indicating the root mean squared error (RMSE), mean absolute error (MAE), and the bias error (BE) for both calibrated and uncalibrated predictions, to the left and right of the vertical bars, respectively. As can be seen, calibration results in better BE. In \textbf{(c)} and \textbf{(d)}, we plot the rolling means and standard deviations of the aleatoric, epistemic, and total uncertainties as a function of the true MPIQs. All three uncertainties increase when $\sigma_{\rm epis}$ is calibrated, are the highest near the lowest and highest MPIQ values where there are the least number of observations, and are the lowest near the mode of the histogram for measured MPIQs (see Figure \ref{fig:hist_moneyontable}). Finally, in \textbf{(e)}, \textbf{(f)} and \textbf{(g)} we visualize the benefits of calibrating $\sigma_{\rm epis}$. Closer to the 1:1 line is better. Also shown are the interval sharpness (IS) and the average calibration error (ACE) metrics with and without calibration. While CRUDE \citep{crude_probability_calibration} certainly improves the predicted epistemic uncertainties, this comes as the cost of poorer aleatoric and total uncertainties; we plan on addressing this in future work.}
%\label{fig:MDN_moneyplot}
%\end{figure*}



\iffalse
\begin{table}
    \caption{Summary statistics for the 10 most important features in $\mathcal{D_{F_S,S_L}}$}
    \label{tab:summarystats}
    \centering
    \begin{tabular}{ccccccccccc}
        \toprule
        \toprule
         & & & & & & & & & & \\
         & & & & & & & & & & \\
    \end{tabular}
\end{table}
\fi

%\subsection{Mauna Kea Atmospheric Monitor (MKAM)}\label{sec:mkam}
%MKAM is a collaborative project between IFA (Institute for Astronomy, University of Hawaii), CFHT and the W.M. Keck Observatory. The goal of which is to install a seeing monitor on the CFHT site for use by all the observatories on the summit.  Provides a measure of the Fried parameter (r0) at a wavelength of 0.5 microns with a goal of an precision of better than $5\%$ at the observed zenith angle and azimuth and an estimate of r0 corrected to the zenith. The r0 value should be corrected for the finite exposure time of the instrument. Goal of an accuracy of $5\%$ in r0 and the measurements will be from at least six meters above grade. Fried's parameter r0 quantitatively expresses the image degradation due to atmospheric turbulence.  r0 represents the diameter of the coherent cells in the incident wave at the telescope pupil. \url{http://articles.adsabs.harvard.edu//full/1981SoPh...69..223R/0000227.000.html} the MASS profiler is an instrument in addition to the DIMM in MKAM. MASS Turbulence Profiler. MASS is an instrument to measure the vertical distribution of turbulence in terrestrial atmosphere by analysing the scintillation (twinkling) of bright stars. When stellar light passes through a turbulent layer and propagates down, its intensity fluctuates.

%\emph{describe missing values; plot of number of samples vs number of missing values}
%\emph{structured data}

%MKAM comes online end of 2009.  
%Vents come online end of 2013.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Data Imputation}\label{sec:dataImputation}

\tcr{(SCD: Should this perhaps go into the section on data?)}

As we note in \S\ref{sec:data}, our dataset of 180,000 samples contains () samples with one or more missing values. This is not a concern when dealing with tree-based models, as they are natively able to handle splits with missing observations. Hence, we do not find it necessary to implement any kind of data imputation. However, for sake of completeness, we compare the predictions with and without implementing a K-Nearest Neighbors based imputation scheme \yst{If this is not necessary, I would omit. To explain K-nearst neighbours, you first need to define a metric, and how you define the distance if both data has missing values. All these are unclear to me.} \seb{don't we need the data-imputation from the NN, or SG is masking the missing values?} from Pandas\footnote{(insert hyperlink)}. As we show in Figure (), .....

\tcr{SCD: Note the two sources of data imputation need to be combined into one}
\fi
