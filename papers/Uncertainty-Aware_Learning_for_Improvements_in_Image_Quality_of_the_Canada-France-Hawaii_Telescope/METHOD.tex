\section{Methodology}\label{sec:method}

The raw sensor data is a collection of time series and ultimately it would best to model the multiple sensors in their native data structure. In the analysis we perform in this paper, we compiled the sensor data into a large table to ease exploration, consisting of heterogeneous and categorical data. The heterogeneity is caused by the wide variety of sensors (wind speed, temperature, telescope pointing) each recorded in specific units. Categorical features emerged because certain measurements values were binned. For instance, due to the unreliability of wind speed measurement, we have binned these values -- wind speed below $5$ knots, $5$-$10$ knots, etc. Similarly, for simplicity, each of the twelve vents have been encoded into either completely open or completely closed. These characteristics induce a discontinuous feature space. Our training data set is thus tabular in nature. 
At hand with our curated data set, we are equipped to work on our two objectives: making accurate predictions of  MegaPrime Image Quality, and use our predictor to, on a per-sample basis, explore the importance of each feature on IQ. 

%The structure introduced by humans when tabulating the data (humans decide, e.g., to put features in columns, observations in rows, and what ordering to use) makes tabulated data different from unstructured audio- or image-based data sets (where sound samples or pixels have a naturally occurring order). 

%In the following we discuss how the structure of our data set influences our exploration of tree-based and neural-network-based inference architectures.

% SF: moved from separate file.
Decision tree-based models ~\citep{decision_trees} and their popular derivatives such as random forests~\citep{randomforests}, and gradient boosted trees \citep{gradient_boosted_decision_trees_gbdt} are well-matched to tabular data. and often are the best performers. Tree-based models select and combine features greedily to whittle down the list of pertinent features to include only the most predictive ones. Feature sparsity and missing data is naturally accommodated by tree models, they simply do not include feature cells containing such values in their splits. We show below our implementation of a variant of gradient boosted tree with uncertainty quantification, and feature exploration.
%In this paper we implement an off-the-shelf model gradient boosting tree model to provide a performance baseline. To extract feature importance and interactions from this tree-based model, we use the SHAP package \citep{shap1,shap2}.

However tree-based models require the human process of feature engineering and are known (e.g. \cite{bengio2010decision}) to poorly generalize.
In contrast to tree-based models, deep neural networks (DNNs) are powerful feature pre-processors. Using back-propagation, they learn a fine-tuned hierarchical representation of data by mapping input features to the output label(s). This allows us to shift our focus from feature engineering to fine-tuning the architecture, designing better loss functions, and generally experimenting with the mechanics of our neural network. In reported comparison cases, DNNs yield improved performance with larger sized datasets \citep{airbnb}. As we will show, our neural network implementation, with the feature engineering steps described above, performs better than the alternative tree-based boosted model. We therefore deepen our analysis of the deep neural networks further: we quantify its probabilistic predictions, and we attempt to model the feature space.

%In this paper we provide results for both neural-network based inference architectures and light gradient boosed machines (LGBM, \citealt{lightgbm}).

% SF: suggestion: remove this paragraph to make it shorter?
%We discuss in this section our modelling process. We begin with a short summary of traditional machine-learning based methods and deep learning methods, and justify our decision to use the latter in this work. We describe the two types of neural networks we utilize in Sections \ref{sec:mdn} and \ref{sec:rvae}. Later, in section~\ref{sec:uncertainty_quantification} we present a case for making probabilistic predictions, with accurate accounting of both \textit{aleatoric} and \textit{epistemic} uncertainties. In Section \ref{sec:DL_probability_calibration}, we describe our post-processing methodology to make both models' prediction errors more accurate. Finally, in Section~\ref{sec:DL_metrics}, we list the metrics we use to evaluate the performance of our models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%\subsection{Deep Learning}
%\label{sec:DL}

\iffalse

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{cfht_mdn.PNG}
    \caption{Finding lower and upper learning rates.}
    \label{fig:mdn}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{cfht_rvae.PNG}
    \caption{Calibration Curve}
    \label{fig:vae}
\end{subfigure}
\end{figure*}
\fi

%\input{ml_vs_dl}
%\input{ml_baseline}

\input{rvae+mdn}
\input{uncertainty_quantification}
\input{calibration}
\input{metrics}
\input{feature_ranking}
\input{putting_it_all_together}
%%%%%%%%%%%%
%\subsection{Semi-supervised}
%\label{sec:semiSupervised}


