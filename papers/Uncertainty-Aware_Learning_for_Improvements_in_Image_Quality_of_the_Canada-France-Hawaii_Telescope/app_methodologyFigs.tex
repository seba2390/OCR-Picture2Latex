\section{Workflow and Training Details} \label{sec.workflowFigs}

In this appendix we present a few figures that we anticipate will help the reader better understand our implementation of the machine learning techniques we use.  In particular, in Figure~\ref{fig:cfht_mdn_overview} we illustrate our MDN training process.  In Figure~\ref{fig:cfht_rvae_overview} we illustrate the approach taken to identify in-distribution vent configurations amongst all possible $2^{12}$ configurations.  Our predictions are restricted to  only in-distribution vent configurations.  Finally, in Figures~\ref{fig:mdn_ancillary} and~\ref{fig:vae_ancillary} we present some details on how we select our learning rate.

\begin{figure*}
    \centering
    \includegraphics[width=0.89\textwidth]{figures/cfht_mdn_overview.pdf}
    \caption{Workflow for generating predictions for MPIQ using our mixture density network (MDN, Figure \ref{fig:rvae_plus_mdn}). \textbf{First}, we divide the input data set $\mathbf{X}$ into $N_{\rm CV,1} = 10$ cross-validation folds, at any point referring to the collation of $N_{\rm CV,1}-1$ of them as $\mathbf{X_{TRAIN}}$, and the remaining fold as $\mathbf{X_{TEST}}$. We repeat this $N_{\rm CV,1}$ times to cover all samples in $\mathbf{X}$, but depict only one such iteration here for illustration. \textbf{Second}, we sub-divide $\mathbf{X_{TRAIN}}$ into $N_{\rm CV,2} = 3$ CV folds. Same as before, $N_{\rm CV,2}-1$ folds are collated while the remaining fold, referred to as the validation set $\mathbf{V}$, is set aside. \textbf{Third}, each of the $N_{\rm CV,2}$ CV folds is used to create $N_{\rm bags} = 3$ `bags' by randomly shuffling its data and picking the same number of samples with replacement. The MDN is trained on one of such folds, and predictions on the validation set give us the $16^{\rm th}$, $50^{\rm th}$, and $84^{\rm th}$ quantile predictions, plus the epistemic uncertainty per sample in $\mathbf{V}$. This process is repeated $N_{\rm CV,2} - 1$ more times to get predictions for all samples in $\mathbf{X_{TRAIN}}$.  \textbf{Fourth}, the MDN is now trained on the entire training set, and predictions collected for samples in $\mathbf{X_{TEST}}$. \textbf{Fifth}, we use {\sc CRUDE} \citep{crude_probability_calibration}, and the predictions from the third step to calibrate the predicted values from the fourth.}
    \label{fig:cfht_mdn_overview}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.89\textwidth]{figures/cfht_rvae_overview.pdf}
    \caption{Workflow for identifying acceptable, in-distribution (ID) vent configurations, for each sample in the test set. \textbf{First}, we isolate samples with all twelve vents open. \textbf{Second}, we further pick only those samples for which the predicted $84^{\rm th}$ and $16^{\rm th}$ quantiles, generated by adding the epistemic and aleatoric uncertainties in quadrature, envelope the true MPIQ. `MDN' stands for Mixture Density Network, see Figure \ref{fig:cfht_mdn_overview}. \textbf{Third}, we use the training set with our robust VAE, calculate the $5^{\rm th}$ percentile of the pseudo-marginal log likelihood loss, and use that as a lower cut-off to separate in-distribution test samples from out-of-distribution (OoD) ones. \textbf{Fourth}, for the test samples thus filtered, we generate $2^{12} - 1$ samples by toggling the 12 vents into open and close positions, but skip the all-open configuration since that is the  base case. From these hypothetical samples, we find the ID ones by repeating the procedure of step 3. \textbf{Finally}, we throw out those test samples for which none of the hypothetical cases passed the cut-off test.}
    \label{fig:cfht_rvae_overview}
\end{figure*}



\begin{figure*}
    \centering
    \begin{minipage}{.97\textwidth}
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/learning_rate_finder_mdn.pdf}
            \caption{Choosing lower and upper learning rates for the cyclic learning rate scheduler, as described in Section \ref{sec:putting_it_all_together}. The vertical lines indicate the chosen limits of $10^{-6}$ and $10^{-3}$.}
            \label{fig:mdn_find_lr}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/training_curve.pdf}
            \caption{Training and validation curves for one of ten folds when using the Mixture Density Network, as described in Section \ref{sec:putting_it_all_together}. The vertical line indicates the epoch of minimum validation loss.}
            \label{fig:mdn_training_curve}
        \end{subfigure}
        \caption{Loss as a function of learning rate and epochs for one of ten folds when training using the MDN. In $\textbf{(b)}$, the training loss is higher than the validation loss owing to the MoEx augmentation \citep{moex}, as we explain in Section \ref{sec:mdn}.}
        \label{fig:mdn_ancillary}
    \end{minipage}

    \begin{minipage}{.97\textwidth}
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/learning_rate_finder_vae.pdf}
            \caption{Choosing lower and upper learning rates for the cyclic learning rate scheduler. Since we also anneal W$_{\rm KL}$, it is important to find LR limits that encourage quick convergence over the entire domain of W$_{\rm KL}$.}
            \label{fig:vae_find_lr}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/beta_vs_epoch.pdf}
            \caption{Annealing W$_{\rm KL}$ from 0 to 1 in each cycle of 20 epochs.Finding optimal $\beta$ for the robust variational autoencoder. This prevents L$_{\rm KL}$ from collapsing to 0, and is based upon findings of \cite{cyclical_wkl_annealing}.}
            \label{fig:vae_weightkl_vs_epoch}
        \end{subfigure}
        \newline
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/rvae_training1.pdf}
            \caption{Training and validation curves for the total loss and the reconstruction loss for one of ten folds.}
            \label{fig:vae_trainingcurve1}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=.98\linewidth]{figures/rvae_training2.pdf}
            \caption{Training and validation curves for the KL-divergence loss for one of ten folds.}
            \label{fig:vae_trainingcurve2}
        \end{subfigure}
        \caption{Losses as a function of learning rate, W$_{\rm KL}$, and epochs, for one of ten folds when training using the RVAE. While the MDN has a single loss (see Figure \ref{fig:mdn_ancillary}), the RVAE has two individual losses.  In conjunction these form the final loss that is minimized by mini-batch gradient descent (-L$_{\rm ELBO}$ = L$_{\rm REC}$ + L$_{\rm KL}$, see Section \ref{sec:rvae}). Different from Figure \ref{fig:mdn_training_curve}, \textbf{(c)} and \textbf{(d)} here are not `live' plots, but constructed once the RVAE has been fully trained.}
        \label{fig:vae_ancillary}
    \end{minipage}
\end{figure*}   
    

\iffalse
\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/learning_rate_finder_mdn.pdf}
    \caption{Choosing lower and upper learning rates for the cyclic learning rate scheduler, as described in Section \ref{sec:putting_it_all_together}. The vertical lines indicate the chosen limits of $10^{-6}$ and $10^{-3}$.}
    \label{fig:mdn_find_lr}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/training_curve.pdf}
    \caption{Training and validation curves for one of ten folds when using the Mixture Density Network, as described in Section \ref{sec:putting_it_all_together}. The vertical line indicates the epoch of minimum validation loss.}
    \label{fig:mdn_training_curve}
\end{subfigure}
\caption{Loss as a function of learning rate and epochs for one of ten folds when training using the MDN. In $\textbf{(b)}$, the training loss is higher than the validation loss owing to the MoEx augmentation \citep{moex}, as we explain in Section \ref{sec:mdn}.}
\label{fig:mdn_ancillary}
\end{figure*}

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/learning_rate_finder_vae.pdf}
    \caption{Choosing lower and upper learning rates for the cyclic learning rate scheduler. Since we also anneal W$_{\rm KL}$, it is important to find LR limits that encourage quick convergence over the entire domain of W$_{\rm KL}$.}
    \label{fig:vae_find_lr}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/beta_vs_epoch.pdf}
    \caption{Annealing W$_{\rm KL}$ from 0 to 1 in each cycle of 20 epochs.Finding optimal $\beta$ for the robust variational autoencoder. This prevents L$_{\rm KL}$ from collapsing to 0, and is based upon findings of \cite{cyclical_wkl_annealing}.}
    \label{fig:vae_weightkl_vs_epoch}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/rvae_training1.pdf}
    \caption{Training and validation curves for the total loss and the reconstruction loss for one of ten folds.}
    \label{fig:vae_trainingcurve1}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/rvae_training2.pdf}
    \caption{Training and validation curves for the KL-divergence loss for one of ten folds.}
    \label{fig:vae_trainingcurve2}
\end{subfigure}
\label{fig:vae_ancillary}
\caption{Losses as a function of learning rate, W$_{\rm KL}$, and epochs, for one of ten folds when training using the RVAE. While the MDN has a single loss (see Figure \ref{fig:mdn_ancillary}, the RVAE has two individual losses, which in conjunction form the final loss that is minimized by mini-batch gradient descent (-L$_{\rm ELBO}$ = L$_{\rm REC}$ + L$_{\rm KL}$, see Section \ref{sec:rvae}). Different from Figure \ref{fig:mdn_training_curve}, \textbf{(c)} and \textbf{(d)} here are not `live' plots, but constructed once the RVAE has been fully trained.}
\end{figure*}
\fi