\subsubsection{Mixture Density Network}\label{sec:DL_MDN}

Mixture density networks (MDNs) are composed of a neural network and a mixture model\footnote{In their initial form~\cite{bishop_mdn}, MDNs used a Gaussian mixture model (GMM); they can  easily be generalized to other distributions.}. We begin by recapping the concept of a mixture model--a model of probability distributions built up from a weighted sum of more simple distributions. More concretely, we consider a 

In a one-dimensional mixture model with $m$ components the overall probability density function (PDF) 
\[
p(x)=\sum_{j=0}^{m-1} \omega_{j} p_{j}(x)
\]
is weighted sum of $m$ PDFs $p_{j}(x)$ with non-negative weights $\Omega=\left\{\omega_{0}, \ldots, \omega_{m-1}\right\}$; where $\sum_{j=0}^{m-1} \omega_{j}=1$.

Typically these probability distributions will be parameterised by a series of parameters that reflect the shape and location of the distribution $\Theta=\left\{\theta_{0}, \ldots, \theta_{m-1}\right\} .$ The full parameterised model may therefore be written as
\[
p(x \mid \Omega, \Theta)=\sum_{j=0}^{m-1} \omega_{j} p_{j}\left(x \mid \theta_{j}\right)
\]
As an example, each $p_{j}$ could be a normal distribution parameterised by a mean $\mu_{j}$ and a variance $\sigma_{j}$. The mixture model would then have the following form,
\[
p(x \mid \Omega, \Theta)=\sum_{j=0}^{m-1} \frac{\omega_{j}}{\sqrt{2 \pi \sigma_{j}^{2}}} \exp \left(\frac{-1}{2 \sigma_{j}^{2}}\left(x-\mu_{j}\right)^{2}\right)
\]
In general, these mixture distributions are multi-modal, including those in the form of Eq
$(4),$ and can be fitted directly to some data $\mathbf{x}=\left(x_{0}, \ldots, x_{n-1}\right) .$ Assuming independence, the corresponding likelihood is calculated as
\[
l(\mathbf{x} \mid \Omega, \Theta)=\prod_{i=0}^{n-1}\left[\sum_{j=0}^{m-1} \omega_{j} p_{j}\left(x_{i} \mid \theta_{j}\right)\right]
\]
Fitting can then typically proceed using expectation-maximisation [20] \yst{Is this correct? For our case, there are no classes, the training should only be a maximum likelihood. Am I missing something here? Or do you mean classical GMM here. I would shorten this.}. For our purposes, we have an individual-based model $M,$ with some input $\alpha,$ that produces stochastic realisations $y \sim M(\alpha) .$ We therefore wish to derive a relationship between the input parameters $\alpha,$ and the mixture density weights $\omega_{j}(\alpha)$ and density parameterisations $\theta_{j}(\alpha) .$ This could potentially be done with a separate regression for each of the density parameters and weights, although this would fail to capture the corresponding relationships that would exist between each parameter and weight. We can therefore model these using a neural network, which is able to provide flexible fitting for arbitrarily complex relationships by the universal approximation theorem
$[22] .$ An MDN is therefore defined as a mixture model, where the mixture components are modelled using a neural network. Fig 1 provides an overview of the MDN construction. The inputs of the model $\alpha$ are initially fed into the MDN (three such inputs in the example diagram). These are then passed through a number of hidden layers in the neural network, which provide a compact representation of the relationship between the inputs and the unnormalised inputs into the mixture model. These distribution parameters are then passed through a normalisation layer; the weights of the mixture are transformed such that they sum to one and the shape parameters are transformed so that they are positive. These parameters are used to construct the mixture model, where one can draw samples or calculate statistics, such as mean and variance, for a given input. For multiple outputs the final layer can be copied with independent parameters for the number of outputs being considered \yst{Not sure I follow the last sentence here?}. Note that a number of aspects of the MDN need to be specified including the number of input parameters, the dimension of the output, the distributions used in the mixture density, and the number and size of the hidden layers.
The MDN can then be fit to the following objective loss function, which is equivalent to maximising the likelihood given in $\mathrm{Eq}(5)$
\[
f(\mathbf{x} \mid \Omega, \Theta)=-\sum_{i=0}^{n-1} \log \left(\sum_{j=0}^{m-1} \omega_{j} p_{j}\left(x_{i} \mid \theta_{j}\right)\right)
\]
Note that provided $p_{j}$ is differentiable with respect to $\theta_{j}$, this loss represents a differentiable function. Standard techniques based on stochastic gradient descent can then be applied in order to optimise the weights of the network with respect to this loss [23]

Mixture density networks (MDN) (Bishop, 1994) are a class of models obtained by combining a conventional neural network with a mixture density model.
We use a mixture of 20 normal distributions parameterized by a feedforward network. That is, the membership probabilities and per-component mean and standard deviation are given by the output of a feedforward network.


The standard normal--or Gaussian--distribution acts as the
foundation for countless modeling approaches in statistics
and machine learning. As opposed to a single Gaussian,
which is a very limited model, \emph{Gaussian mixture models} can represent or approximate almost any density
of interest while remaining intuitive and easy to handle. A Gaussian is characterized by two parameters--mean $\mu$ and standard deviation $\sigma$. We see from Figure () that our output label of interest, MPIQ, does not resemble a Gaussian

\begin{figure*}
   \centering
    \includegraphics[width=\textwidth]{Fig3.png}
    \caption{Optimizing the vent configuration can dramatically improve the image quality of the telescope. \textbf{(3a)} shows the nominal vent configurations, and \textbf{(3b)} the (predicted) optimal vent configuration that leads to the IQ improvements shown in \textbf{(3c)}. The red dashed lines in \textbf{(3c)} delineate the assumed improvements for the exposure time calculation.}
    \label{fig:actionable}
\end{figure*}

\iffalse
\begin{figure*}
    \centering
    \begin{subfigure}
    \includegraphics[width=.49\textwidth]{shap_heatmap_goodsmall.png}
    \caption{Caption}
    \label{fig:my_label}
    \end{subfigure}
\end{figure*}
\fi
\yst{I think here you also need to explain how you define aleatoric and epistemic uncertainties from the MDN output. E.g, sigma of the mean, and mean of the sigma.}
