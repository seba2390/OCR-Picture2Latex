\subsection{Putting It All Together}\label{sec:putting_it_all_together}

%\seb{This section is very technical, should we just give a high level summary and push the training details onto the appendix?}
\iffalse
As we note above, tabular data has traditionally--and to great success--been analysed using machine learning models, which have shown superior performance over deep learning models. Here, we utilize a host of tree-based machine learning models available in widely known packages, including--Extremely Randomized Trees \citep{extremelyrandomizedtrees}, Random Forests \citep{randomforests}, XGBoost \citep{xgboost}, Gradient Boosting Machines/LightGBM \citep{lightgbm}, and NGBoost \citep{ngboost}. \yst{only discuss the best model? Or if TPOT uses all of these, then you can say we use AutoML which implicitly considers all these variations} We choose to use tree-based models since they are usually more interpretable than linear models due to model-mismatch effects (where the modelâ€™s form does not match its true relationships in the data, \cite{model_mismatch_linear_models}. \yst{the last sentence is not clear to me. Please elaborate.} In \citet{shap2}, the authors highlight that even when simpler high-bias linear models achieve high accuracy, low-bias ones (such as tree-based models) are usually preferable, and even more interpretable, since they are likely to better represent the true data-generating mechanism and depend more naturally on their input features. NGBoost is a recently released method based on deicision trees \citep{decision_trees}allows us to automatically obtain both the mean and the standard deviation of predictions for all samples. Instead of using it as the sole machine learning model trained on the CFHT-dataset, we separately train a number of different tree-based models and use their predictions as inputs to an NGBoost model. It has been widely shown that using such an ensemble of models/committee of experts enables one to overcome the limitations of individual models in different parts of the parameter space, and increase generalization \yst{briefly explain how these tree-based methods differ.} \citep{stacking}.
\fi

{\bf Training and test sets:} For both the MDN (Section \ref{sec:mdn}) and the RVAE (Section \ref{sec:rvae}) we partition $\mathcal{D_{F_S,S_S}}$ into two unequally-sized subsets -- a {\it training super-set} containing 90\% of the samples and a {\it test set} containing the rest. We are following a nested cross-validation scenario. We partition the data sets carefully, to ensure that the distribution of MPIQ values in both the test and training sets reflect the distribution in the original data set. To accomplish this we sort the samples by MPIQ values and, starting from the lowest MPIQ value allocate each sample in a round-robin fashion to one of ten buckets generated. We then iterate this process for the training super-set -- again producing a 90-10 split -- to respectively produce the final {\it training set} and the {\it validation set}. We train the models on the training set and record its predictions on the validation and test sets. The validation set guards against over-fitting -- we want our models to learn patterns from the training set, but not to the extent where they fail to generalize to unseen samples. Before making predictions on the test set, we revert the weights of both the MDN and RVAE models to their respective epochs where their respective losses on the validation data set were minimal, as shown in Figure \ref{fig:mdn_training_curve} for the MDN. As a quick reminder, a `prediction' for the MDN is a three-tuple consisting of mean $\mu$, aleatoric uncertainty $\sigma_a$, and epistemic uncertainty $\sigma_e$ for the MPIQ, whereas for the RVAE it is the reconstructed input sample.

{\bf Learning rate and optimizer:} We use a cyclical learning rate scheduler to vary the learning rate from an initial high to a final low value, in multiple cycles; this has been shown to result in a considerably better convergence than using step-wise or constant learning rate schedules \citep{cyclical_learning_rate}. To determine these limits for the MDN and the RVAE, we pick arbitrarily high ($10^{-1}$) and low ($10^{-7}$) limits, exponentially increase the learning rate from the latter to the former in a mere 20 epochs, and evaluate the behavior of the respective loss functions. For the MDN, we determine that at $10^{-3}$ and $10^{-6}$, the loss begins to plateau, as can be seen from Figure \ref{fig:mdn_find_lr}. We thus pick these as the higher and lower limits, respectively, and indicate them by dashed vertical lines. Similarly, from Figure \ref{fig:vae_find_lr} we can see that these limits for the RVAE are $10^{-3}$ and $10^{-5}$. We use the Yogi optimizer \citep{yogi_optimizer} for stochastic gradient descent; this optimizer is an improvement over the commonly used Adam \citep{adam_optimizer}, and we find experimentally that it provides faster convergence. We wrap this optimizer in the Stochastic Weight Averaging optimizer \citep{swa_stochastic_weight_averaging} -- accessible via the \texttt{TensorFlow Addons} library\footnote{\url{https://github.com/tensorflow/addons}} -- and average the model weights every 20 epochs, to overlap with the length of a training cycle. The batch size when using both models is 128.

{\bf Feature normalization and data augmentation:} Finally, we apply strong feature normalization and data augmentation to regularize against over-fitting. Specifically, we use Positional Normalization \citep[PONO]{positional_normalization_pono} layers to capture both the first and second moments of latent feature vectors, and use Momentum Exchange \citep[MoEx]{moex} to mix the moments of one input sample with that of another, to encourage our models to draw out training signal from the moments as well as from the normalized features. In each mini-batch of 128 samples, every feature vector for every sample is added with the feature vector for a randomly picked sample; the probability that this happens is set to 0.5 - this is, half the times, there is no mixing. In case of mixing, the weight assigned to the original sample is picked from a $\beta$ distribution with both concentration parameters set to 100, while the weight of the randomly picked sample is the difference of this from 1 (so that both weights sum to unity). The same random ordering of samples and the same weights are carried over to the model outputs as well (MPIQ for the MDN, the reconstructed input for the RVAE). This augmentation scheme has shown to produce state-of-the-art results, and our own experiments confirm excellent performance. This can be seen in Figure \ref{fig:mdn_training_curve}, where we plot the training and validation losses for one of ten folds; the training loss is significantly higher than the validation loss for a large part of the training process. We insert a PONO layer after each Dense layer in the MDN, and after the penultimate encoding layer in the RVAE. The MoEx layers are inserted before the ultimate Dense layer in the MDN, and the ultimate layer in the RVAE. Each PONO layer is followed by a Group Normalization layer \citep[GN]{group_normalization} with a channel size of 16 (see the MDN in Figure \ref{fig:rvae_plus_mdn}), except when a MoEx layer directly follows the PONO layer, where the former is followed by a Batch Normalization layer \citep{batchnorm}.

{\bf Calibration:} For the MDN, we implement additional steps to calibrate the predicted MPIQ PDFs. We treat each of the 10 training sets (these are obtained after splitting the respective training {\it super-sets} into training and validation sets, as explained at the beginning of this section) as a training {\it super-set}, and the associated validation set as the test set. In other words, we sub-divide the training set into 10 training and validation sets , train on the new training data sets and use the new validation sets as guardrails against over-fitting, and predict MPIQ on the new test sets. After repeating this process a total of 10 times, we now have predictions for the mean and both uncertainties for all samples in the original training set. Finally, we {\it calibrate} our model's predictions on the original test set by using the predictions on the original training set, by following the method described in \cite{crude_probability_calibration}. This is the post-processing step discussed in Section~\ref{sec:DL_probability_calibration}. We repeat this entire process a total of 10 times to cover all samples in $\mathcal{D_{F_S,S_S}}$. We illustrate this workflow in Figure~\ref{fig:cfht_mdn_overview} in Appendix~\ref{sec.workflowFigs}, where in the interest of saving space we show only 3 splits instead of 10.

{\bf RVAE tuning:} For the RVAE, there are a couple of additional considerations. For one, we adopt an annealing methodology to handle the problem of vanishing KL-divergence \citep{cyclical_wkl_annealing}. It is known that the KL-divergence loss term in Equation \ref{eq:VAEeq2} very quickly collapses to 0 if both L$_{\rm REC}$ and L$_{\rm KL}$ are equally weighted. We therefore adopt the methodology suggested by \cite{cyclical_wkl_annealing}: we modify Equation \ref{eq:VAEeq2} by multiplying the second term by a weight scalar W$_{\rm KL}$, and vary this from 0 to 1 in a cyclical fashion, as shown in Figure \ref{fig:vae_weightkl_vs_epoch}. Next, there is the requirement to choose an appropriate $\beta$ in Equation \ref{eq:ce_final}. We choose $\beta=0.005$ based as suggested by \cite{rvae_orig}, and leave the task of finding an optimal $\beta$ to future work. %\sg{(the previous statement will most likely change, as soon as I am able to figure out why all betas give good results and thus how to pick the best among them.)}
Finally, since W$_{\rm KL}$ is annealed with epochs, we need to ensure that our lower and upper learning rates help with convergence for all values of this scalar. From Figure \ref{fig:vae_find_lr}, we see that between learning rates of $10^{-5}$ and $10^{-3}$, the total loss decreases for all values of W$_{\rm KL}$. %since empirical observations show that this offers a good trade-off between the need for high marginal log likelihood for the training samples and low MLL for the OoD samples.reconstruction loss for the training samples

{\bf Overall workflow:} Our overall workflow is as follows:
\begin{enumerate}
    \item For a given train-test split (out of a total of 10) of $\mathcal{D_{F_S,S_S}}$, we use the training set with the MDN, record predictions on the test set, and calibrate them using the methodology described above. We save the weights of the MDN at the epoch of minimum validation loss -- this is shown by the dashed vertical line in Figure \ref{fig:mdn_training_curve}, and for the specific split shown, occurs at epoch 38.
    \item Next, we train the RVAE using the same training set. Similar to the process with the MDN, we revert the model weights back to the epoch of minimum loss, and make predictions on the test set. We gather for the training, validation, and test sets the total loss -L$_{\rm ELBO}$, reconstruction loss L$_{\rm REC}$, and the KL-divergence loss L$_{\rm KL}$. These are plotted in Figures \ref{fig:vae_trainingcurve1} and \ref{fig:vae_trainingcurve2}. We save the $95^{\rm th}$ percentile of -L$_{\rm ELBO, Train}$ as the L2; this is our cut-off between ID and OoD samples. 
    \item Next, we create a small data set of only those samples from the test set where all twelve vents are open. While our goal is to hypothesize the gains in seeing/MPIQ we could have gotten had the vents been in their optimal configuration instead of in the all-open configuration, we believe it is important to be conservative in our estimates. Thus we select only those samples for further processing where we are confident that there were no mechanical malfunctions, high wind conditions, or other system errors that could have prevented the telescope operator from opening all vents.
    \item As a first filter, we select only those samples for which L$_{\rm ELBO, Test} <$ L2, with the intention of filtering out samples for which we are not extremely confident about the ID characteristic.
    \item From this newly created test set, we further only select those samples where our MDN from Step (i) predicts that the true MPIQ is covered by $68^{th}$ percent spread about the median in the predicted MPIQ PDF. This is again enacted in the interest of obtaining conservative predictions downstream.
    \item From the filtered test set in Step (v), we create a permutated data set by toggling all twelve vents ON (==1) and OFF (==0). For a total of 12 vents, this results in $4095$ new samples for each input sample, where the remaining 107 features remain unchanged. The $4096^{\rm th}$ sample is the input test sample itself, since its vents are already in the all-open configuration. For each of the these $4095$ samples, we again apply the same filter as in Step (iv) -- filtering out those vent configurations which, given the training set, are OoD.
    \item Finally, we obtain MPIQ predictions using the MDN for all samples in the permutated data set, created by collation ID permutations for all selected test samples.
\end{enumerate}

{\bf Identifying predictable vent variations \& separating in-distribution from out-of-distribution samples:} In Figure \ref{fig:vae_hist_mll_ood}, we demonstrate our methodology for separating ID samples from OoD ones. As should be expected, most test samples are ID, as are $95\%$ training samples (by definition). A striking yet expected result is that only a very small sample of possible permutations are ID. The reason for this becomes clear from Figure \ref{fig:vae_hist_hds}, where we plot histograms of the different vent configurations in the training set -- 0 on the x-axis corresponds to the all-open configuration, while 1 to all-closed. The vast majority of samples, $\sim80\%$, have all vents closed, while $\sim20\%$ have either all vents or most vents open. Thus the vast majority of samples in the permutated dataset, where the twelve vents can take arbitrary configurations -- say half open and half closed, corresponding to a Hamming distance (x-axis in Figure \ref{fig:vae_hist_mll_ood}) of 0.5 -- are those that the RVAE has not seen before, and thus classifies as OoD.

{\bf Process illustration:} Finally, we illustrate the workflow delineated in Steps (ii) through (vii) above in Figure \ref{fig:cfht_rvae_overview}.

%Using the scheduler thus defined, we train both the RVAE defined in Section \ref{sec:rvae}, and the MDFN defined in Section \ref{sec:mdn}, for 3 cycles, each being 20 epochs long, and evaluate the loss on training and validation splits. In Figure \ref{fig:mdn_training_curve} we visualize this procedure for MDFN, for 1 of 10 training-validation splits, where the training+validation data itself is 1 of 10 folds of the training-test splits of $\mathbf{X_{\rm train}}$ (see Figure \ref{fig:cfht_mdn_overview} for an overview of the workflow). We track the training and validation losses, and revert the model weights back to their values at the epoch corresponding to minimum of the validation loss. For the particular nested split in Figure \ref{fig:mdn_training_curve}, this epoch is number 47, as is indicated by the dashed vertical line. Finally, we apply strong data augmentation to regularize against over-fitting. Specifically, we use the momentum exchange \citep{moex} layer in the penultimate layers of both our networks, with probability $p=1$ and the concentration parameters $\alpha$ and $\beta$ of the $\beta-$ distribution equal to 1 (see xxx for details about the meanings of these parameters).%We use the same process for both $\mathcal{D_{F_S,S_S}}$ and $\mathcal{D_{F_L,S_S}}$.


\iffalse
\begin{algorithm}
\caption{Probabilistic MPIQ Prediction}
\label{algo:putting_it_all_together}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\renewcommand{\algorithmicinitialize}{\textbf{Initialize:}}
\REQUIRE (i) The matrix consisting of environmental and dome measurements $\mathbf{X} = \{\mathbf{x}_{n}^{(1:d)}\}^{N}_{n=1}$ where $N$ is the number of samples, and $d$ is the number of features.\\ Also, (ii) the vector consisting of MPIQ measurements $\mathbf{y} = \{y_{n}\}^{N}_{n=1}$.
\ENSURE (i) predicted probability densities for the input dataset: \tcr{(SCD: I think the following should have $\mu$ and $\sigma_a$ and $\sigma_e$ in it)} \tcb{$\mathbf{y}_\mathrm{pred} =\{y_{n}\vert_\mathrm{pred}^{(1),(2)}\}^{N}_{n=1}$.} \\
%Each predicted PDF is characterized by a mean and a standard deviation. \\
(ii) Feature importance matrix: $\boldsymbol{\mathrm{F}} = \{f_{n}^{(1:d)}\}^{N}_{n=1}$.
\FOR{iteration $i_\mathrm{test}$ in range [1, 10]}
\STATE Divide $\mathbf{X}$ into two subsets, a training subset $\boldsymbol{\mathrm{X_{train}}}\vert_{\hat{i_\mathrm{test}}}$, and a testing subset $\boldsymbol{\mathrm{X_{test}}}\vert_{i_\mathrm{test}}$,into the $9:1$ ratio (90\% training 10\% testing) described above. Here the $\hat{i_{\rm{test}}}$ represents the collation of all \tcr{folds} \tcb{(What is a ``fold''?)} besides $i_{\rm{test}}$. \tcr{(SCD: What is the $\vert_{\hat{i_\mathrm{test}}}$ for?)}
\FOR{iteration $i_\mathrm{val}$ in range [1, 10]}
\STATE Sub-divide the training set into training and validation sets in a $4:1$ ratio \tcr{(SCD: In text says this is alsp 90/10)}; that is, $\boldsymbol{\mathrm{X_{train}}}\vert_{\hat{i_\mathrm{test}}}$ $\rightarrow$ $\boldsymbol{\mathrm{X_{train}}}\vert_{\hat{i_\mathrm{val}},\hat{i_\mathrm{test}}} + \boldsymbol{\mathrm{X_{train}}}\vert_{i_\mathrm{val},\hat{i_\mathrm{test}}}$.
\STATE Train on $\boldsymbol{\mathrm{X_{train}}}\vert_{\hat{i_\mathrm{val}},\hat{i_\mathrm{test}}}$ and obtain predictions $y_{\mathrm{val}}\vert_\mathrm{pred, i_\mathrm{val}}$ by predicting on $\boldsymbol{\mathrm{X_{train}}}\vert_{i_\mathrm{val},\hat{i_\mathrm{test}}}$.
\ENDFOR
\STATE Collate predictions on all 10 validation sets and sample from the resulting PDFs to obtain $3\times50$ values per sample: $50^{th},$ $15.9{\rm th},$ and $84.1^{\rm st}$. \tcr{(SCD: what is 15.9 and 84.1?  Not following here...)} Combine them as described in Section \ref{sec:uncertainty_quantification} to obtain three quantities per sample: mean, epistemic uncertainty, and aleatoric uncertainty.% lower prediction level $\mu - 1.96\sigma$, and upper prediction level $\mu + 1.96\sigma$. %Call this tuned model \textbf{insert model name}$_\mathrm{tuned}$.
\STATE Train the model on $\boldsymbol{\mathrm{X_{train}}}\vert_{\hat{i_\mathrm{test}}}$ and predict on $\boldsymbol{\mathrm{X_{test}}}\vert_{i_\mathrm{test}}$, to obtain three values per sample just like above.
\STATE Use the collated predictions on the super-set of all validation sets obtained from Step 7, to calibrate the predictions on the test set from Step 8, by using the CRUDE method of \cite{crude_probability_calibration}.
\STATE Use the same model trained in Step 8, to obtain feature importances for samples in $\boldsymbol{\mathrm{X_{test}}}\vert_{i_\mathrm{test}}$. Call the matrix $\boldsymbol{\mathrm{F}}_{i_{\rm test}}$.
\ENDFOR
\STATE Collate all predictions from Steps 8 and 9 to obtain uncalibrated and calibrated predictions, respectively, for all samples in $\mathbf{X}$.
\STATE Collate all $\boldsymbol{\mathrm{F}}_{i_{\rm test}}$s to obtain $\boldsymbol{\mathrm{F}}$.
\end{algorithmic} 
\end{algorithm}
\fi

