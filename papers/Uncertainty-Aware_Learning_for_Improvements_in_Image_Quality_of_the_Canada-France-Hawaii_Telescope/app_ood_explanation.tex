\iffalse

\section{Out of Distribution}
\label{sec:ood}

\iffalse
\begin{figure*}
\centering
\begin{subfigure}{0.98\textwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/oodplot_0_20201011-042027.pdf}
    \caption{IQ$_{\textrm{Megacam}}$ x IQ$_{\textrm{MKAM}}$ after dome vent installation. IQ as a function of Wind Direction minus Dome pointing prior to dome vents.}
    \label{fig:my_label}
\end{subfigure}
\hfill
\begin{subfigure}{0.98\textwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/oodplot_1_20201011-042027.pdf}
    \caption{IQ$_{\textrm{Megacam}}$ x IQ$_{\textrm{MKAM}}$ after dome vent installation. IQ as a function of Wind Direction minus Dome pointing after dome vents.}
    \label{fig:my_label}
\end{subfigure}
\hfill
\begin{subfigure}{0.98\textwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/oodplot_8_20201011-042027.pdf}
    \caption{IQ$_{\textrm{Megacam}}$ x IQ$_{\textrm{MKAM}}$ after dome vent installation. IQ as a function of Wind Direction minus Dome pointing when dome vents are OPEN or CLOSED.}
    \label{fig:my_label}
\end{subfigure}
\end{figure*}
\fi

\begin{figure*}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/oodplot_large_20201011-141451.pdf}
    \caption{Caption}
    \label{fig:ood}
\end{figure*}

Here, we analyze and visualize the impact that variable feature importance have on a model's predictions of the true mean and uncertainties. 

Assume that we have three categorical features $x_1$, $x_2$, and $x_3$ with 9 values each, for a total of 729 possible feature combinations.The target depends on the features as:
\begin{align}\label{eq:ood_distribution}
    y = \mathcal{N}\left(\mu=\mathcal{U}(x_1, x_2) + \lambda x_3, \sigma^2=\mathcal{C}(x_1, x_2)\right),
\end{align}
The output is normally distributed with a mean that depends on the first two features in a fixed way, and on the third feature in a variable fashion determined by the value of the modulating factor $\lambda$. A trivial value of $\lambda$ implies $x_3$ is a redundant feature with no predictive power, whereas an infinitely high (in magnitude) value signifies that $x_1$ and $x_2$ are relatively unimportant. $\sigma^2$ is the input aleatoric (or data) uncertainty, is dependent only on $x_1$ and $x_2$, and takes only two values -- 0.01 and 0.05 -- that are distributed as shown in the bottom-right square in Figure \ref{fig:ood}.

We consider four different cases with increasing values of the modulating factor $\lambda = 0, 1, 7, 255$. For each of these cases, the training set contains only 25 of the possible 81 configurations of $x_1$ and $x_2$, to simulate a scenario where we have access to only a subset of the entire parameter space. We place no such restriction on $x_3$, which is allowed to take all values from 0 to 8. The training set has 100 draws for each of the 225 ($25 \times 9$) points in the underlying 3-D parameter space (Equation \ref{eq:ood_distribution}), for a total of 22,500 samples. The test set consists of 729 samples -- a single draw per possible point in the parameter space. We use a  As can be seen from Figure \ref{fig:ood}, the impact of missing samples in the training set becomes less and less important as the underlying features $x_1$ and $x_2$ themselves becomes relatively less important in predicting the output $y$. 

\fi

\iffalse
Machine learning has been widely applied to a range of tasks. However, in certain high-risk applications, such as autonomous driving, medical diagnostics, and financial forecasting, a mistake can lead to either a fatal outcome or large financial loss. In these applications, it is important to detect when the system makes a mistake and take safer actions. Furthermore, it is also desirable to collect these “failure scenarios”, label them, and teach the system to make the correct prediction through active learning.
Predictive uncertainty estimation can be used for detecting errors . Ideally, the model indicates a high level of uncertainty in situations where it is likely to make a mistake. That allows us to detect errors and take safer actions. Crucially, the choice of action can depend on why the model is uncertain. There are two main sources of uncertainty: data uncertainty (also known as aleatoric uncertainty) and knowledge uncertainty (also known as epistemic uncertainty). If our goal is to detect errors, it is not necessary to separate these two uncertainties. However, if our goal is active learning, then we would like to detect novel inputs, and knowledge uncertainty can be used for that.
Data uncertainty arises due to the inherent complexity of the data, such as additive noise or overlapping classes. In these cases, the model knows that the input has attributes of multiple classes or that the target is noisy. Importantly, data uncertainty cannot be reduced by collecting more training data.
Knowledge uncertainty arises when the model is given an input from a region that is either sparsely covered by the training data or far from the training data. In these cases, the model knows very little about this region and is likely to make a mistake. Unlike data uncertainty, knowledge uncertainty can be reduced by collecting more training data from a poorly understood region.
\fi