\subsection{Data Imputation}\label{sec:DL_imputation}
Since neural networks can not automatically handle samples with different numbers of missing values, this calls for feature imputation. In this work, we accomplish this in the \emph{pre-training} phase--we train the complete subset $D_{comp}$ in a \emph{self-supervised} manner, using a Sliced Wasserstein Autoencoder (SWAE, Figure (), \cite{sliced_wasserstein_autoencoder_swae})\footnote{The SWAE is a simpler version of the Wasserstein Autoencoder \citep{wasserstein_autoencoder}, and has been shown to perform almost as well but with a much simpler architecture (\textbf{check this})}. Self-supervised learning is a regime of deep learning that enables us to reconstruct the input data without the use of labels. Here, we do not care about precise prediction of MPIQ, but are interested in learning a good intermediate representation with the expectation that this can approximate well the `true', underlying generative distribution. As can be seen from Figure (), of the total () samples/observations in our corpus, () (or $\approx x \%$) contain no missing values. We use these () samples to train our SWAE using the mean absolute error (MAE) loss function. Once training converges (based on cross-validation results), we use the method proposed by \cite{autoencoder_imputation} to impute the missing values for the remaining () out-of-sample observations. In brief, this process is as follows: (a) For the out out-of-sample observations, use a relatively simple imputing scheme to fill in the missing values. Here we use k-NN imputation () from \textsc{pandas}, which fills missing features with mean values of those features cmoputed from the $k$ nearest training samples (we used $\text{k} = 5$). (b) Use the auto-encoder network (SWAE) trained on the complete sub-set, for inference on the incomplete sub-set. Iteratively update $\hat{x}_{J}$ using $\hat{x}_{J}^{t+1}=\hat{x}_{J}^{t}+h\left[f\left(\hat{x}^{t}\right)-\hat{x}^{t}\right]_{J}$, where $f$ is the trained SWAE, $h$ is a step size and $\hat{x}^{t}=\left(\hat{x}_{J}^{t}, x_{J^{\prime}}\right) \in \mathbb{R}^{d}$ denotes a complete point where the observed components are fixed at the observed values and the missing ones are replaced by the current estimate. We use h=0.001, and in Figure () plot $\hat{x}_{J}^{t+1} - \hat{x}_{J}^{t}$ averaged over all samples in the incomplete set. We stop updating at time step (), when gains start diminishing.%\yst{Then you need to explain what is self-supervised. If most of the data does not require inputating, and the additional self-supervised learning does not help much, this part should be simplified with simple supervised learning}
\yst{I am not sure I fully follow the connection between the (a) and (b). My naive understanding is that here you train an auto-encoder, and each time you find k-nearest neighbour in the latent space, and use the mean to update the inputted values. In this case, I am not sure I understand $\hat{x}_{J}^{t}+h\left[f\left(\hat{x}^{t}\right)-\hat{x}^{t}\right]_{J}$. For me it seems like the two terms are not the tensor of same dimension. Maybe elaborate?}