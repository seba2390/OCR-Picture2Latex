\section{Results}
\label{sec:results}
%\tcr{(Need to add introductory paragraph outlining results in the section.  Added a couple stubs below.)}
In Section~\ref{sec:resultsPredictingIQ} we present results on using our model to predict the image quality given the current environmental and dome operating conditions. In Section~\ref{sec:resultsImprovingIQ} we discuss how we might better operate the dome to improve IQ.  In particular, we investigate the potential improvement that could result from smart actuation of the configuration of the dome vents. In Section~\ref{sec:resultsRelContribIQ} we present results on the relative contribution of different features to the predicted mean MPIQ.  Through these results we verify observations by earlier groups and we start to understand better what information our models use in its inference process.

%Figures~\ref{fig:one-to-one} and~\ref{fig:abc} we describe the learning process. \tcr{(SCD: Sankalp, I think some text needs to be inserted here about how you perform the training -- these two figures seem to have to do with the training process.  Maybe describe what algorithm you use -- do you use adaptive step-size, say with ADAM, something like that?  I'm not exactly sure what to say about each figure.  I suppose the second just says that the training and validation data hield identical results?  What happens at epoch 50 -- change in the learning rate?)}

\subsection{Predicting image quality}
\label{sec:resultsPredictingIQ}

In Figure~\ref{fig:MDN_moneyplot} we present our main results on the accuracy of probabilistic predictions of MPIQ using the MDN.  In Figure~\ref{fig:MDN_moneyplot_catboost} we present comparative results for the graient-boosted tree model.  Table~\ref{tab:ml_vs_dl_results} tabulates summary results.  We describe each set of results in turn.

Figure~\ref{fig:mdn_one_to_one} quantifies the accuracy of our predictions.  The horizontal axis displays measured (a.k.a. nominal) MPIQ, while the y-axis displays predicted MPIQ.  The units of both are arc-seconds ($''$). Perfect prediction is represented by the red $45\degree$  line. True MPIQ varies from a bit below $0.5''$ to just over $2''$.  The blue dots depict the point-predictions (the medians of the output PDFs).  The light blue bars plot the estimated aleatoric uncertainties ($\sigma_a$) of the point predictions.  These are superimposed on the total uncertainty, the differences are the epistemic uncertainties ($\sigma_e$), visible in orange.   
As is tabulated in Table~\ref{tab:ml_vs_dl_results}, the mean absolute error (MAE) between the true MPIQ values and the medians of our calibrated predictions is $\sim 0.07''$. 

Figure \ref{fig:mdn_one_to_one_hist} help us understand the improvement due to calibration. We plot the histograms of the differences between the calibrated predictions and the true MPIQ values, and between the uncalibrated predictions and the true MPIQ values.  These histograms are respectively plotted in pink and black. We use three metrics (cf., Section \ref{sec:metrics_det}) to quantify the improvement resulting from calibration: root mean squared error (RMSE), mean absolute error (MAE), and bias error (BE). The values in the first row are for uncalibrated medians while those in the second row are for calibrated models.  We remind the reader that the calibration using CRUDE \citep{crude_probability_calibration} is enacted only for the epistemic uncertainties, $\sigma_{\rm epis}$, which we observe is significantly decreased for the calibrated model.


\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/one_to_one.pdf}
    \caption{Predicted versus measured MPIQ. For the former we show the calibrated median, and upper and lower quantiles for calibrated uncertainties.}
    \label{fig:mdn_one_to_one}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/one_to_one_hist.pdf}
    \caption{Histogram of prediction errors, along (in inset box) with three metrics that compare performance with and without calibration.}
    \label{fig:mdn_one_to_one_hist}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/spread_vs_mpiq1.pdf}
    \caption{68\% spread in calibrated and uncalibrated aleatoric and epistemic uncertainties in predicted MPIQ, plotted as a function of measured MPIQ.}
    \label{fig:mdn_CI_vs_mpiq1}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/spread_vs_mpiq2.pdf}
    \caption{68\% spread in total uncertainties in predicted MPIQ, plotted as a function of measured MPIQ.}
    \label{fig:mdn_CI_vs_mpiq2}
\end{subfigure}
\newline
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_aleatoric.pdf}
    \caption{Calibration curves for aleatoric uncertainty.}
    \label{fig:mdn_calibration_curve_al}
\end{subfigure}
%\hfill
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_epistemic.pdf}
    \caption{Calibration curves for epistemic uncertainty.}
    \label{fig:mdn_calibration_curve_epis}
\end{subfigure}
%\hfill
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/Calibration_curve_total.pdf}
    \caption{Calibration curves for total uncertainty.}
    \label{fig:mdn_calibration_curve_total}
\end{subfigure}
\caption{Predictions using the Mixture Density Network. $\sigma \implies 0.5\times(84^{\rm th} - 16^{\rm th})$ quantiles. In \textbf{(a)} we plot the predicted MPIQ, characterized by the $16^{\rm th}, 50^{\rm th}$, and $84^{\rm thn}$ percentiles of their respective calibrated PDFs.  These are plotted versus measured (i.e., true) MPIQs. In \textbf{(b)} we subtract the ground-truth MPIQ from the $50^{\rm th}$ percentile predictions, from both the raw uncalibrated, and the calibrated PDFs, and plot their histograms. We also quantify the quality of both calibrated and uncalibrated predictions using root mean squared error (RMSE), mean absolute error (MAE), and  bias error (BE); in the inset box read calibrated on left and uncalibrated on right.  Calibration results in better BE. In \textbf{(c)} and \textbf{(d)}, we plot the smoothed mean and standard deviations of the aleatoric, epistemic, and total uncertainties as a function of the measured MPIQs. All three uncertainties increase when $\sigma_{\rm epis}$ is calibrated.  Uncertainty is highest near the low-end and high-end MPIQ values; in these regimes we have the least number of observations.  Uncertainty is lowest near the mode of the histogram of measured MPIQs where data is plentiful (cf., the histograms in Figure~\ref{fig:hist_preliminaries}). Finally, in \textbf{(e)}, \textbf{(f)} and \textbf{(g)} we visualize the benefits of calibrating $\sigma_{\rm epis}$. The ideal is the 1:1 line; closer is better. In the inset box the interval sharpness (IS) and average calibration error (ACE) metrics, with and without calibration, are provided.}% While CRUDE \citep{crude_probability_calibration} improves the predicted epistemic uncertainties, the improvement comes at the cost of worst aleatoric and total uncertainty. We plan on addressing this in future work.}
\label{fig:MDN_moneyplot}
\end{figure*}


\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/one_to_one_cat.pdf}
    \caption{Predicted versus measured MPIQ. We do not calibrate the predictions from the boosted tree model.}
    \label{fig:mdn_one_to_one_cat}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/one_to_one_hist_cat.pdf}
    \caption{Histogram of prediction errors.  In the inset box we provide three metrics that compare performance with and without calibration.}
    \label{fig:mdn_one_to_one_hist_cat}
\end{subfigure}
\caption{Prediction results using gradient boosted decision trees (GBDTs).}
\label{fig:MDN_moneyplot_catboost}
\end{figure*}

\begin{table*}
    \iffalse
    \begin{tabular}{ccc}
        \toprule
        \toprule
        Metric & $\mathcal{D_{F_S,S_S}}$ & $\mathcal{D_{F_L,S_S}}$ \\ \midrule
        MAE$_{\rm {uncal}}$ & \tcr{xxx}, \tcr{xxx} & xxx, xxx \\
        RMSE$_{\rm {uncal}}$ & xxx, xxx & xxx, xxx \\
        BE$_{\rm {uncal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {al, uncal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {epis, uncal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {total, uncal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {al, uncal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {epis, uncal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {total, uncal}}$ & xxx, xxx & xxx, xxx \\
        \midrule
        MAE$_{\rm {cal}}$ & xxx, xxx & xxx, xxx \\
        RMSE$_{\rm {cal}}$ & xxx, xxx & xxx, xxx \\
        BE$_{\rm {cal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {al, cal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {epis, cal}}$ & xxx, xxx & xxx, xxx \\
        ACE$_{\rm {total, cal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {al, cal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {epis, cal}}$ & xxx, xxx & xxx, xxx \\
        IS$_{\rm {total, cal}}$ & xxx, xxx & xxx, xxx \\
        \bottomrule
    \end{tabular}
    \fi
     \caption{Comparative performance of methods, across all four data sets (cf. Section \ref{sec:data}), and all six metrics (cf. Section \ref{sec:DL_metrics}). The top row shows uncalibrated results for the MDN and the GBDT models.  To ease direct comparison we present results as an ordered tuple (MDN, GBDT).  The bottom row shows calibrated results for the MDN.  We do not provide calibrated results for the GBDT.  For each metric the performance of the best-performing model is highlighted in bold; in all cases the MDN performs at least as well as the GBDT.}
    \begin{tabular}{cccccccccc}
        \toprule
        \toprule
        $\rm{Metric}$ & RMSE & MAE & BE & ACE$_{\rm {al}}$ & ACE$_{\rm {epis}}$ & ACE$_{\rm {total}}$ & IS$_{\rm {al}}$ & IS$_{\rm {epis}}$ & IS$_{\rm {total}}$ \\ \midrule
        Uncalibrated & ({\bf 0.11}, {\bf 0.11}) & ({\bf 0.07}, 0.08) & ({\bf 0.00}, {\bf 0.00}) & (\textbf{-0.01}, -0.09) & (\textbf{-0.31}, -0.59) & (\textbf{0.04}, -0.08) & (\textbf{0.03}, 0.06) & (\textbf{0.04}, 0.08) & (\textbf{0.03}, 0.06) \\
        \midrule
        Calibrated & 0.11 & 0.07 & 0.03 & -0.02 & -0.10 & 0.11 & 0.02 & 0.04 & 0.03 \\
        \bottomrule
    \end{tabular}
    \label{tab:ml_vs_dl_results}
\end{table*}


\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_episstd_features.pdf}
    \caption{Histograms of {\it uncalibrated epistemic} uncertainty from the MDN, for various data sets. The vertical line is the $95^{\rm th}$ percentile for the training set.}
    \label{fig:vae_hist_epis}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_calepisstd_features.pdf}
    \caption{Histograms of {\it calibrated epistemic} uncertainty from the MDN, for various data sets. The vertical line is the $95^{\rm th}$ percentile  for the training set.}
    \label{fig:vae_hist_calepis}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_mll_noise_features.pdf}
    \caption{Histograms of the pseudo marginal log likelihood (-L$_{\rm ELBO}$) from the RVAE. The vertical line is the $95^{\rm th}$ percentile  for the training set.}
    \label{fig:vae_hist_mll}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_regret_noise_features.pdf}
    \caption{Histograms of log likelihood regret for the same data sets, from the RVAE. The vertical line is the $95^{\rm th}$ percentile  for the training set.}
    \label{fig:vae_hist_regret}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_mll_noise_zoomed_features.pdf}
    \caption{Zoom-in of subfigure~\textbf{(c)}, focusing on the hard OoD data sets.}
    \label{fig:vae_hist_mll_zoomed}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/hist_regret_noise_zoomed_features.pdf}
    \caption{Zoom-in of subfigure~\textbf{(d)}, focusing on the hard OoD data sets.}
    \label{fig:vae_hist_regret_zoomed}
\end{subfigure}
\caption{We visualize the discriminative ability of various metrics to separate Out-of-Distribution (OoD) samples from In-Distribution (ID) samples. Perhaps surprisingly, we observe that epistemic uncertainties, whether calibrated or uncalibrated, are poor metrics for OoD detection. This is our motivation for the RVAE, which directly captures the likelihood of the training data-generating distribution. \textbf{(c)} and \textbf{(e)} show that  log-likelihood is an excellent, if not perfect, discriminator that can separate ID training and test data from even slightly OoD synthetic data created by adding Gaussian noise of the indicated standard deviation ($\sigma$) to normalized training data. In \textbf{(d)} and \textbf{(f)} we calculate the log-likelihood regret \citep{likelihood_regret}, as explained in Section \ref{sec:rvae}. Comparing \textbf{(e)} and \textbf{(f)}, we see that regret is a slightly better OoD detector.  In these plots the training data histogram is more concentrated, and the modes of the histograms of the two noisy data sets are farther away from the mode of the histogram of the training data set.}
\label{fig:common_ood_detections}
\end{figure*}


Figures \ref{fig:mdn_CI_vs_mpiq1} and \ref{fig:mdn_CI_vs_mpiq2} show smoothed averages of the aleatoric, epistemic, and total uncertainties, for both calibrated and uncalibrated models. We highlight a few important aspects. First, as expected, $\sigma_{\rm al}$ is unchanged by calibration since we do not calibrate aleatoric uncertainty. (The light-blue and dark-blue plots coincide so we don't see both.)  Second, as a function of increasing MPIQ, $\sigma_{\rm al}$ (and the identical $\sigma_{\rm al, cal}$ curve) start from a low value, decrease slightly, and then increases almost $3\times$. The initial dip can be attributed to the high density of data points near the mode of the MPIQ distribution at $\sim0.7''$. The  increase at higher MPIQ is likely due to the decreasing density of data points (see the red curve in the left sub-figure of Figure \ref{fig:hist_preliminaries}).  As the model has access to fewer and fewer points it becomes challenging to learn latent representations discriminative enough to be able make good predictions.  Hence, the aleatoric error increases. Third, comparing $\sigma_{\rm epis}$ to $\sigma_{\rm epis, cal}$ we observe that calibration increases epistemic uncertainty.  This justifies our suspicion that the probabilistic MPIQ predictions are over-confident, and that our decision to calibrate them {\it post-hoc} was sensible. Fourth, $\sigma_{\rm epis}$ and $\sigma_{\rm epis, cal}$ follow the same pattern as the aleatoric uncertainty;  they initially dip to a minimum and then rise  with increasing true MPIQ. That said, relative to their starting values, they dip down to lower levels, and rise asymptotically to about $1.25\times$ their respective starting levels. 
Since epistemic uncertainty quantifies the degree to which a sample is out-of-distribution (OoD), these curves imply that, compared to the samples near the median MPIQ of $0.7''$, samples at both the low and high ends of the measured MPIQ distribution are slightly OoD. (We do note that using predicted epistemic uncertainties is not an reliable way to filter out OoD samples, as expounded upon in Section \ref{sec:resultsImprovingIQ} and Figure \ref{fig:common_ood_detections}). We believe that both $\sigma_{\rm al}$ and $\sigma_{\rm epis}$ can be reduced by weighing the loss function for the MDN so that samples with poorer predictions are given more attention by the network. Another strategy would be to over- and under-sample data points near the ends and the mode of the MPIQ distribution, respectively.  This will make the curve be less peaked. By attacking the class-imbalance problem at both the algorithm- and data-level, we expect to de-bias our predictions.

Finally in Figures \ref{fig:mdn_calibration_curve_al}, \ref{fig:mdn_calibration_curve_epis} and \ref{fig:mdn_calibration_curve_total} we demonstrate the effect of probability calibration on the three uncertainties. The $x$- and $y$-axes respectively quantify the expected and observed confidence levels.  If we sample the 50\% CI spread around the median of the predicted MPIQ PDF from the MDN, 50\% of samples should have their measured MPIQ values be covered by the predicted intervals. Hence the black dashed 1:1 line in all three plots is the ideal calibration plot. In the inserts, we also quantify the difference that calibration makes via the ACE and IS metrics, defined in Section \ref{sec:metrics_prob}.  The values to the left of the vertical bar (`$\vert$') in the wheat-colored inserts are for calibrated results, while those to the right for uncalibrated results. Since we only calibrate $\sigma_{\rm epis}$, only Figure \ref{fig:mdn_calibration_curve_epis} shows an improvement. This comes at the cost of poorer post-calibration results for $\sigma_{\rm al}$ and $\sigma_{\rm total}$.

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hist_mll_ood.pdf}
    \caption{Histograms of the pseudo marginal log likelihood (-L$_{\rm ELBO}$) for three data sets based on one of ten training folds: the training set, the subset of the test set where all vents are open, and the permutated data set constructed by toggling each of the twelve vents for each sample in the test subset. The dashed line indicates the $95^{\rm th}$ percentile  for -L$_{\rm ELBO}$ of the training data set.}
    \label{fig:vae_hist_mll_ood}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hist_hds_train.pdf}
    \caption{For the training set (purple curve in \textbf{(a)}), we visualize the percentage of observed vent configurations by plotting their Hamming distance from the all-open configuration. With twelve vents, each of which can be either open or closed, we can only have a total of thirteen Hamming distances, with 0 denoting all-open and 1 denoting all-closed.}
    \label{fig:vae_hds_train}
\end{subfigure}
\caption{Identifying valid, ID samples from the corpus of all toggled vent configurations. From samples in the test set with all twelve vents open, we select only those about which we can make confident predictions of MPIQ using our MDN. Only a very small subset of the blue curve is ID.  This makes sense, since the training set consists mostly of samples where the vents are almost all-open or all-closed.  Hence, most samples in the toggled data set with other vent configurations are classified as OoD by our RVAE.}
\label{fig:vae_moneyplot}
\end{figure*}

\begin{figure*}
\begin{subfigure}{0.99\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/barplot.pdf}
    \caption{Predicted change in MPIQ corresponding to each possible vent configuration for $100$ randomly selected samples, actuating vent configurations across all in-distribution settings for each. Lower is better. The general trend is that the higher the measured IQ the more room there is for improvement.}
    \label{fig:common_barplot}
\end{subfigure}
\begin{subfigure}{0.99\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/barplot_robust.pdf}
    \caption{A similar plot to \textbf{(a)} except that now we restrict ourselves to a robust sample, as predicted by total uncertainty.  Of the roughly $600$ samples, according to this measure only $62$ are robust.  For these we see that the predicted change is an improvement (a decrease) in IQ.  Note that this subset of $62$ samples need not overlap significantly or at all with those samples in \textbf{(a)}.}
    \label{fig:common_barplot_robust}
\end{subfigure}
\caption{Visualizing the predicted effect on IQ of optimizing vent configurations.}
\label{fig:common_barplots}
\end{figure*}

In Figure~\ref{fig:MDN_moneyplot_catboost} we show comparative plots for predictions made using our gradient-boosted tree models.  This model is described in Section~\ref{sec:gbdt}. Comparing Figure~\ref{fig:mdn_one_to_one_cat} to Figure~\ref{fig:mdn_one_to_one}, we see that GBDT models significantly underestimate $\sigma_{\rm epis}$. Calibrating the GBDT model using CRUDE does not result in substantial improvement. This is why we use the MDN as our workhorse for MPIQ predictions. For sake of completeness, we compare predictions from {\sc catboost} with those from MDN, and hypothesize reasons for deficient performance of {\sc catboost}, in Appendix \ref{sec:app_mdnvscatboost}.

In Table~\ref{tab:ml_vs_dl_results} we collate the results on the five metrics, for both calibrated and uncalibrated predictions from the MDN. We compare these predictions from those from the boosted-tree GBDT model.  These results demonstrate that the MDN outperforms the GBDT, again supporting the choice to use it as the workhorse model for MPIQ prediction.

\subsection{Actuating dome parameters to improve IQ}\label{sec:resultsImprovingIQ}
\subsubsection{Separating in-distribution (ID) from out-of-distribution (OoD) actuations}\label{sec:results_idvsood}
In addition to predicting MPIQ, one of our driving motivations is to learn how to actuate observatory operating parameters to improve MPIQ.  One set of easily actuatable parameters is the dome vents. Indeed, as mentioned in the discussion of related work in Section~\ref{sec:relatedWork}, fluid flow models were developed in the vent design process to predict the effect on MPIQ of various vent configurations. These models predicted that the optimal MPIQ is achievable with intermediate vent configurations, where the 12 vents are neither all-open nor all-closed \citep{wind_tunnel_test}.
In contrast, in most usage to date vents have been configured either to the all-open or to the all-closed setting. We therefore explore what our MDN model predicts -- how much improvement a modified vent configuration might have on MPIQ reduction. We note that we must be cautious when pursuing this exercise as some vent configurations are not within the training sample. As we describe in Section~\ref{sec:method} and Figure~\ref{fig:vae_hist_mll}, we use the pseudo marginal log likelihood, -L$_{\rm ELBO}$, from the RVAE model as a filter to discriminate in-distribution samples from out-of-distribution ones. In Figure~\ref{fig:common_ood_detections} we justify our choice to use this metric to detect distribution shift. 

%In Figures \ref{fig:vae_hist_epis} and \ref{fig:vae_hist_calepis}, we see that even though epistemic uncertainties from the MDN should hypothetically be able to separate ID and OoD samples, in practice they fail to do so. 
In Figure \ref{fig:vae_hist_epis} the pink and cyan curves are the histograms for $\sigma_{\rm epis}$ for the training and test sets for one of ten folds. To simulate out-of-distribution data, we synthesize four data sets. The uniform noise data set, depicted in green, is generated by drawing $50,000 \times 119$ samples, independently, from the uniform distribution between 0 and 1. The constant noise data set, depicted in blue, is generated by drawing $50,000 \times 1$ samples, independently, from the uniform distribution between 0 and 1, and copying this over $119$ times. The orange and red curves are noisy versions of the training data set, where we add Gaussian noise with $\mu = 0$ and $\sigma = 0.10$ and $0.05$, respectively. Since we do not train the MDN with noisy versions of the training data (we use the MoEx data augmentation method only, as described in Section \ref{sec:method}), the uncertainty in predicting MPIQ as a result of noisy versions of training data is classified as epistemic and not aleatoric. The dashed vertical black line marks the $95^{\rm th}$ percentile value for $\sigma_{\rm epis, train}$ -- we classify all values to its right as out-of-distribution. We plot the density in log scale to better capture different ranges. Figure \ref{fig:vae_hist_calepis} is the same as Figure \ref{fig:vae_hist_epis}, except it plots histograms for calibrated epistemic uncertainty. In both figures, it is apparent that epistemic uncertainty, whether calibrated or uncalibrated, is a poor detector of a distribution shift. Distribution shift identification using discriminative models such as the MDN is an area of active research, and we relegate further exploration of this limitation to future work. In this paper, we instead use the RVAE as a proxy for our data distribution, and justify our decision in Figures \ref{fig:vae_hist_mll}, \ref{fig:vae_hist_regret}, \ref{fig:vae_hist_mll_zoomed}, and \ref{fig:vae_hist_regret_zoomed}.

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/deltaoptimalnominal_vs_nominal_iqs_all.pdf}
    \caption{Improvement in MPIQ predicted by the MDN for the optimal vent configurations.  We plot with respect to the measured MPIQ. Negative is better.}
    \label{fig:common_deltaoptimalnominal_vs_nominal_iqs}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/deltaoptimalnominal_vs_nominal_iqs_robust.pdf}
    \caption{Same as \textbf{(a)}, but  only for those samples where the $84^{\rm th}$ percentile of predicted optimal MPIQ $\leq$ measured MPIQ.}
    \label{fig:common_deltaoptimalnominal_vs_nominal_iqs_robust}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/deltaoptimalreference_vs_nominal_iqs_all.pdf}
    \caption{Same as \textbf{(a)}, with the change that here the improvement is calculated with respect to the predicted MPIQ for the \textit{all-open} samples.}
    \label{fig:common_deltaoptimalreference_vs_nominal_iqs}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/deltaoptimalreference_vs_nominal_iqs_robust.pdf}
    \caption{Same as \textbf{(c)}, but for only those samples where the $84^{\rm th}$ percentile of predicted optimal MPIQ $\leq$ $50^{\rm th}$ percentile of predicted reference MPIQ.}
    \label{fig:common_deltaoptimalreference_vs_nominal_iqs_robust}
\end{subfigure}
\caption{We visualize the gains in terms of improved MPIQ prediction that can be achieved by our proposed methodology of toggling the twelve vents open and close individually as a function of environmental and observatory characteristics. The baseline configuration is all-open. After restricting ourselves to a subset of in-distribution ``togglings'', in \textbf{(a)} and \textbf{(b)} we plot the improvement over the measured MPIQ values, whereas in \textbf{(c)} and \textbf{(d)} we plot the improvement over MPIQ values predicted for the same samples in \textit{all-open} configuration. In \textbf{(b)} and \textbf{(d)}, we sub-sample the data points from \textbf{(a)} and \textbf{(c)} respectively, only presenting those samples for which we are quite confident in our estimates. In \textbf{(c)}, the several $y=0$ red dots in the left-half of the plot signify that for those samples, the \textit{all-open} vent configuration is in fact the optimal vent configuration. Finally, we present third-order polynomial fits in \textbf{(c)} and \textbf{(d)}, and estimate total gains achievable using our predicted, optimal vent configurations, using weighted mean and weighted median metrics. These fits are used to calculate average expected reduction in observing times to achieve a fixed signal-to-noise ratio (SNR).}
\label{fig:common_moneyplot1}
\end{figure*}

In Figures \ref{fig:vae_hist_mll} and \ref{fig:vae_hist_mll_zoomed}, we plot the L$_{\rm ELBO}$ for the same data sets described above; the latter figures focuses on the ``hard'' cases of noisy training data. The samples to the left are out-of-distribution. It is immediately clear that the marginal likelihood is a much better distribution shift detector than is the epistemic uncertainty.  This follows because the uniform and constant noise data sets are very easily separable, and the supports of the two Gaussian datasets are both also almost completely to the left of the vertical line. In Figures \ref{fig:vae_hist_regret} and \ref{fig:vae_hist_regret_zoomed}, instead of using L$_{\rm ELBO}$ as the discriminative metric, we instead use the {\rm pseudo log likelihood regret}. Proposed in \cite{likelihood_regret}, likelihood regret for a sample is derived by passing it through the trained RVAE and caching L$_{\rm ELBO}$. We then fix the weights of the decoder and train the encoder to minimize L$_{\rm ELBO}$ for  that single sample. The difference between L$_{\rm ELBO, sample}$ and L$_{\rm ELBO}$ is the pseudo log likelihood regret.  In~\cite{likelihood_regret} this is shown to be a better detector of out-of-distribution samples than L$_{\rm ELBO}$. By definition, regret is always non-negative. Comparing Figures \ref{fig:vae_hist_mll_zoomed} and \ref{fig:vae_hist_regret_zoomed}, we verify that regret is indeed a better separator -- the pink curve is less spread out, and the modes of both the red and orange curves are farther away from the black line. Given these results, it is natural to question our design choice of using L$_{\rm ELBO}$ as the metric we used to identify out-of-distribution samples. We use L$_{\rm ELBO}$ rather than regret because while pseudo log likelihood regret is more robust, it also takes about $50$ times longer to calculate than L$_{\rm ELBO}$. This is because the calculation of regret  requires retraining of the encoder, once per input sample.  Even if we divide our GPU\footnote{NVIDIA Titan RTX 24Gb} into multiple virtual cores for parallel processing, it took about two hours to calculate regret for $4096$ samples ($4096 = 2^{12}$ is the number of permuted vent configurations possible for each input sample). For these computational reasons we use L$_{\rm ELBO}$.  Depending on computing resources, in the future we may move to distributed computing framework to make the use of regret practical.

\subsubsection{Predicted reduction in MPIQ using only on in-distribution (ID) vent configurations}\label{results_idventconfigs}
We now use L$_{\rm ELBO}$ to identify identify the vent settings that are not too ``out-of-distribution'' for which our model will be able to make reliable MPIQ predictions.  As we will develop in the following, these robust predictions indicate that substantial MPIQ improvement is possible by optimizing the vent configuration.  In future work we plan to extend our dataset to reduce the set of out-of-distribution vent configurations, thereby enabling a wider range of reliable predictions, and helping us to realize even greater MPIQ improvements.

Figure \ref{fig:vae_moneyplot} demonstrates  results from the process we use to identify, among all possible vent configurations, those for which we can make reliable MPIQ predictions.  By this process we filter out those data records that are OoD. (The workflow that led to these results was described towards the end of Section \ref{sec:putting_it_all_together} and is illustrated in Figure \ref{fig:cfht_rvae_overview}.)
Is is this restricted, or ``filtered'', set of vent configurations that we use to assess the possible improvement.  In Figure \ref{fig:vae_hist_mll_ood} we show  results for one of the ten splits of $\mathcal{D_{F_S,S_S}}$ and predict the MPIQ that would results for all possible vent settings. On average, each test split of $\sim6000$ data records results in $\sim600$ samples.  For each of these $6000$ samples on average only about $4$  other vent configurations (out of a possible $2^{12} -1 = 4095$) are not OoD given the training distribution. For each of these vent configurations we use the MDN to predict the MPIQ three-tuple ($\mu$, $\sigma_{\rm al}$, and $\sigma_{\rm epis}$).  We compare these predictions to the MPIQ three-tuple predictions for the respective base samples (with all vents open).
The results of this exercise, which we will discuss next, are presented in Figures \ref{fig:common_barplots}, \ref{fig:common_moneyplot1} and \ref{fig:common_moneyplot2}. 

In Figure~\ref{fig:common_barplot} we plot bar-charts for the change in MPIQ with optimal vent configurations, with respect to the predicted (calibrated) mean MPIQs for the respective reference test samples with all vents open. For each of the 10 train-test splits of $\mathcal{D_{F_S,S_S}}$ we recall that there are $\sim600$ viable all-open data samples.  Of these viable samples we randomly sub-sample $100$ and, for each of these, predict their mean MPIQ for the all-open configuration using our MDN. We then calibrate these predictions using the CRUDE method, as described in Section \ref{sec:DL_probability_calibration}. We note this prediction will be somewhat different from the measured MPIQ associated with these data records. Finally, we make predictions for each of the ID vent configurations for all 100 samples (roughly $100 \times 4 = 400$) and subtract each of these MPIQ predictions from the predicated calibrated median MPIQs for the all-open configuration.  In Figure~\ref{fig:common_barplot} we plot these difference against the measured MPIQ values. Values above dashed zero-level imply a worsened (predicted) MPIQ in comparison to the baseline of keeping all twelve vents open.  Values below the dashed zero-level suggest that another ID vent configuration will likely result in reduced seeing. Note that we predict the difference in {\it predicted} seeing levels as these are the levels the model would predict were it {\it not} to have a measurement of MPIQ for the baseline all-open configuration.  While in our data set we {\it do} have the baseline MPIQ, in real-time operation that baseline MPIQ value would not be available prior to the observation when the observer would be using the model to decide how to actuate the vent configurations.

To better understand how to read the vent configurations that lead to an improvement in predicted MPIQ, we have color-coded Figure~\ref{fig:common_barplot}. Bars that are dark purple correspond to the all-close configuration; dark brown to all-open. The color gradient corresponds to Hamming distances of the configuration vectors from all-open.  As one would suppose, the model predicts all-closed to be a worse setting more often than not.  This is in keeping with the original motivation for installing the vents, discussed in Section \ref{sec:relatedWork}.  By and large, opening vents improves MPIQ by allowing air currents built up inside the dome to flush. All-close is the same as having no vents, thus represents the  scenario that was meant to improve upon by installing the vents. As we consider higher measured MPIQs (moving from left to right on the x-axis), we see that the optimal configurations tend to be closer to all-close. This is also in line with intuition previously developed at CFHT.  Higher measured MPIQs are typically obtained in high wind speed scenarios, where it makes sense to close the vents.

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hds_vs_iqs.pdf}
    \caption{Hamming distance of optimal vent configurations from the all-open configuration.  We plot versus measured MPIQ. Low values on the y-axis (close to 0) indicate that majority of the vents are open.  High values (close to 1) indicate that most of vents are closed.}%\tcr{(Need to replot x-label as "measured" or "predicted MPIQ", replot y-axis from 0 to 12, i.e., non-normalized distance)}}
    \label{fig:common_hds_vs_iqs}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hds_vs_iqs_robust.pdf}
    \caption{Same as \textbf{(a)}, but only for those samples where the $84^{\rm th}$ percentile of the predicted optimal MPIQ probability distribution function $\leq 50^{\rm th}$ percentile of the predicted reference MPIQ PDF. These are the same samples used to plot Figure \ref{fig:common_deltaoptimalreference_vs_nominal_iqs_robust}. We call these {\it robust} samples.}% \tcr{(Need to replot x-label as "measured" or "predicted MPIQ", replot y-axis from 0 to 12, i.e., non-normalized distance)}}
    \label{fig:common_hds_vs_iqs_robust}
\end{subfigure}
\newline
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hist_hds.pdf}
    \caption{Histogram of Hamming distances of predicted optimal vent configurations. This is the histogram of the y-axis values in \textbf{(a)}.}% \tcr{Need to replot x-axis to run from 0 to 12, i.e., non-normalized distance.  Also I have changed to ``all-open'' lowercase.}}
    \label{fig:vae_hist_hds}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hist_hds_robust.pdf}
    \caption{Histogram of Hamming distances of predicted optimal vent configurations for the {\it robust} subset. This is the histogram of the y-axis values in \textbf{(b)}.}%\tcr{Need to replot x-axis to run from 0 to 12, i.e., non-normalized distance.  Also I have changed to ``all-open'' lowercase.}}
    \label{fig:vae_hist_hds_robust}
\end{subfigure}
\caption{Distribution of predicted optimal vent configuration.  Distribution is measured as Hamming distance from the all-open baseline.}%Optimizing the vent configuration can dramatically improve the image quality of the telescope. In \textbf{(a)} and \textbf{(b)}, we plot this improvement against the nominal IQ. The former sub-plot contains all samples in the test set, whereas the latter only those w Two major take-aways from these plot are: (i)  the potential for improvement increases with increase in seeing (worse IQ), and (ii) even if we consider the $84^{\rm th}$ percentile predictions, we see that `ALL OPEN' is rarely the optimal configuration for the twelve vents. This justifies our efforts towards modeling \textbf{(b):} For the same optimal configurations in \textbf{(a)}, we plot their Hamming distance from the `ALL OPEN' configuration. Each vent is assigned a 0 or a 1 depending on whether it is closed or open, respectively. \textbf{(c):} Same y-axis as before, plotted against the negative log likelihood loss. A lower value on the x-axis (going to left) indicates better prediction. We see that most improvement is possible for samples where our model does not predict very well \sg{(need to expand on this.)}\textbf{(d):} A violin plot of the same y-axis as the first three sub-plots, now plotted against the nominal wind speed. As a reminder, in this work we have binned the wind speed into four separate categories.}
\label{fig:common_moneyplot2}
\end{figure*}

Figure~\ref{fig:common_barplot_robust} consists of \textit{only} those test samples from the same $\sim600$ that were used to draw from in Figure~\ref{fig:common_barplot}, where the predicted, calibrated upper $84^{\rm th}$ quantiles of the total uncertainties ($\mu_{\rm cal} + \sigma_{\rm total, cal}$) are $\leq$ the predicted calibrated medians for the respective test samples with all vents open. Our idea here is to create a {\it robust} subset. On average, for all 10 train-test splits, this results in $62$ \textit{robust} samples. We emphasize that that this is a second, distinct sampling, with no guarantees of overlap with the $100$ randomly selected samples used to plot Figure \ref{fig:common_barplot}. If we were to mis-actuate the vents and decrease the MPIQ, there would be a negative effect on the downstream science applications.  To mitigate this risk here we chart only those instances where, if our models were to be put into production, we would be very confident in directing the telescope operator to move the vents according to our predictions. From Figure~\ref{fig:common_barplot_robust} we observe that all-open is not the optimal configuration in most situations.  This is true even at lower measured MPIQ values. In fact, significant gains in MPIQ can be realized  by switching each observation from the all-open configuration to the best configuration (from out limited choice of in-distribution and therefore ``viable'' configurations) is significant. When we consider higher MPIQs ($\sim \geq 1''$), the optimal configuration is likely to be all-close. 

\subsubsection{Best achievable MPIQ: A new regime}\label{results_bestmpiq}
In Figure \ref{fig:common_moneyplot1} we present results on the improvement in MPIQ predicted by our MDN given the (hypothetically) optimal vent configurations selected from the restricited set of in-distribution configurations selected by the RVAE.  Figures~\ref{fig:common_deltaoptimalnominal_vs_nominal_iqs} and~\ref{fig:common_deltaoptimalnominal_vs_nominal_iqs_robust} plot the improvement versus measured IQ, while Figures~\ref{fig:common_deltaoptimalreference_vs_nominal_iqs} and~\ref{fig:common_deltaoptimalreference_vs_nominal_iqs_robust} plot the improvement versus predicted and calibrated median MPIQ. Further, the two right-hand plots, Figures~\ref{fig:common_deltaoptimalnominal_vs_nominal_iqs_robust} and~\ref{fig:common_deltaoptimalreference_vs_nominal_iqs_robust} plots the improvement for {\it robust} sub-samples (the samples from the $84^{\rm th}$ quantiles) discussed in the last paragraph.
We note two differences here, compared to Figure \ref{fig:common_barplots}. First, we use all $\sim600$ viable test samples (for Figures \ref{fig:common_deltaoptimalnominal_vs_nominal_iqs} and \ref{fig:common_deltaoptimalreference_vs_nominal_iqs}) in a given train-test split, and not sub-sample of 100; in Figure \ref{fig:common_barplots} we were forced to sub-sample due to space constraints. Second, here we show the difference in predicted optimal MPIQs with respect to both the predicted all-open MPIQs, and the measured MPIQs. For Figures \ref{fig:common_deltaoptimalnominal_vs_nominal_iqs} and \ref{fig:common_deltaoptimalreference_vs_nominal_iqs}, we pick the optimal vent configuration for each of the $\sim600$ viable samples, use the MDN to derive calibrated predictions of MPIQ, and plot the difference between predicted calibrated optimal median and predicted calibrated median for the corresponding test sample with all twelve vents open. For Figures \ref{fig:common_deltaoptimalnominal_vs_nominal_iqs_robust} and \ref{fig:common_deltaoptimalreference_vs_nominal_iqs_robust}, we do the same, but only for the \textit{robust} samples from the $\sim600$ samples. We observe that as we consider larger measured MPIQ values there is increased advantage to optimizing the vent configuration.

In the four sub-figures in Figure~\ref{fig:common_moneyplot2} we visualize the distribution of Hamming distances for the optimal vent configurations chosen for the samples in Figures \ref{fig:common_moneyplot1}. Either from the density of points plotted in Figure~\ref{fig:common_hds_vs_iqs}, or from the distribution of Hamming distances from the all-open configuration plotted in Figure~\ref{fig:vae_hist_hds}, we observe that most optimal vents configurations are close to either all-open or all-closed.  This should not come as a surprise.  As discussed earlier, most vent configurations that are {\it not} close to either the all-open or all-closed configurations were OoD and so were filtered out of the ``viable'' set of configurations for which we consider the MPIQ predictions.  At the risk of repeating some of the broader context provided earlier, the fact that the intermediate vent configurations are OoD is a result of the way the observatory has been operated to date.  Most often the vents have been configured either all-open or all-closed, to a large degree because observers have had no reasoned methodology to follow to choose alternate configurations.  The training data we have access to therefore clusters around the all-open and all-closed configurations.  In a sense then, Figure~\ref{fig:common_moneyplot2} is  another illustration of a main motivation for our work; we want to expand the range of options for the observers so they can better tune observatory performance.

An important observation from Figures~\ref{fig:common_hds_vs_iqs} and~\ref{fig:vae_hist_hds} is that the model predicts about 60\% of samples would have resulted in improved MPIQ had a different setting been chosen.  Figures~\ref{fig:common_hds_vs_iqs_robust} and~\ref{fig:vae_hist_hds_robust}, which are the predictions for the ``robust'' subset discussed earlier are even more definitive -- a ful 85\% of samples would have benefitted from a different vent configuration.  About half of the adjustments would have be to close a {\it single} vent, while the other half would have closed all 12 vents.  Only a smattering of predictions falls between these two choices.  Of course, as just discussed, the intermediate range is mostly OoD.\footnote{At this point is is very helpful to refer back to Figure~\ref{fig:vae_hds_train} and observe that `single-vent-closed' is the third most frequent vent setting in the training set. This explains why our model finds that this type of close-a-single-vent adjustment is in-distribution.
Further, the $\sim 38\%$ prevalence of one-closed-vent configurations in the robustified results of Figures~\ref{fig:common_hds_vs_iqs_robust} and~\ref{fig:vae_hist_hds_robust} tells us that this option is of great use in improving the (robustly predicted) MPIQ resulst plotted in
Figures~\ref{fig:common_deltaoptimalnominal_vs_nominal_iqs_robust} and~\ref{fig:common_deltaoptimalreference_vs_nominal_iqs_robust}.
} That said, the peakiness at one-closed-vent and at all-12-closed-vents is, for us, a strong indication that the true optimal configuration lies somewhere in the middle.  Not till we can collect additional data on this intermediate range to bring it in distribution will we be able to make robust predictions in that range that we can use to advise -- with confidence -- how the observer might more productively operate the telescope. 

\subsection{Quantifying feature importance in prediction of MPIQ}
\label{sec:resultsRelContribIQ}

As a final contribution, we quantify the relative importance of each of the 119 features in predicted median MPIQ. By leveraging the integrated gradients technique~\citep{explaining_explanations_hessians}, we can attribute a \textit{Shapley} score to each feature for each sample. This score measures the linear change in the predicted output (with respect to the average of the MPIQs across all samples in the training set, which is called the `offset') that is induced by a small change in any given feature (i.e., the gradient). A positive score for a feature \textit{f} in sample \textit{x} implies that in \textit{x}, \textit{f} acts to increase the predicted MPIQ, while a negative score points to \textit{f}'s role in decreasing the predicted MPIQ. The larger the magnitude is of this Shapley'' score, the bigger the role of \textit{f} in determining MPIQ for \textit{x}. We average such scores for all 119 features across all samples of interest, and plot {\it attribution plots} in Figure \ref{fig:attplots}. In Figure \ref{fig:attplot0}, we carry out this exercise for all test samples for each train-test split of  $\mathcal{D_{F_S,S_S}}$, and collate the results from all 10 cycles (assuming the number of splits is 10). In Figure \ref{fig:attplot1}, we repeat this exercise, but only for the significantly smaller test set samples ($\sim600$ per train-test split) that are in-distribution (ID). 

Concentrating on in-distribution samples (\ref{fig:attplot1}), we can see that the most important features in the prediction of MPIQ can be grouped roughly into three groups, related respectively to dome convection, seasonal variations and filter central wavelengths. In the first group, additional dome turbulence can be sourced by air convection, which is sourced by (positive) internal temperature gradients within the dome, with respect to altitude in the dome. Therefore, MPIQ is expected to increase with the mirror temperature that acts as a source of convection, and decrease with the temperature of the upper structures of the telescope (truss), that tends to reduce the temperature gradient. This is precisely what we see in the attribution plots of the two most important features. The third feature in importance shows a correction of the predicted MPIQ with respect to the $\lambda^{1/5}$ law used in Equation~\ref{eqn:iq_filter_plus_zenith_correction}, with a predicted MPIQ larger at smaller wavelength compared to the theoretical scaling. Finally, the fourth ranked feature shows the seasonal variation of image quality, with better average seeing during the summer months due to more clement weather. Further features in the list can most of the time be attributed to one of the groups described above.
%We must be cautious about the predictions made in Fig.~\ref{fig:common_barplot} as the data records do not densely cover the feature space of all $4096$ vent settings.  Most often the observers operate the vents all-open or all-closed.  To illustrate this in Figure~\ref{fig:vae_hist_hds} we present a plot of the Hamming weights of vent configurations.  The weight corresponds to the number of vents (out of 12) that are closed in each of our data records.  Note that the distribution is heavily peaked at weight zero (all-open) and weight 12 (all-closed). 

%\begin{figure*}
%    \centering
%    \includegraphics[width=\linewidth]{figures/hist_hds.pdf}
%    \caption{\tcr{(To be added)} Histogram of Hamming weight of vent configurations.}
%    \label{fig:histOfVentConfigHammWgt}
%\end{figure*}

%Although caution is necessary, in Fig.~\ref{fig:vae_hist_mll_ood} and~\ref{fig:hist_hds} we argue that many of our predictions of possible MPIQ improvement are not far out-of-distribution and therefore the predictions can be trusted.  In particular, in Fig.~\ref{fig:hist_all} we plot histograms of the reconstruction error, i.e., \tcr{\ref{eq.xxx}} \tcr{(SCD: Sankalp -- please point to an equation number here)} for the training and test set that we used to develop the model.  We superimpose on this the reconstruction loss for each of the \tcr{600} data records on which we performed the vent-adjustment experiments (i.e., those plotted in Fig.~\ref{fig:common_barplot}) when we set the vents to the all-open configuration for each of these records (regardless of what the measured vent configuration was).  The is the blue stair-step plot.  We then only used those data records which fell in the left-hand tail of the distribution, to the left of the vertical dashed line, and overlapping with the green and purple plots.  These are the data records plotted in Fig.~\ref{fig:common_barplot}.  For each of these data records the predicted optimal setting is the lowest setting (i.e., greatest improvment in MPIQ) on each of the bars plotted in Fig.~\ref{fig:common_barplot}.  The Hamming distance from the all-open configuration for each of these is plotted in Fig.~\ref{fig:hist_hds}.  We see that the predictions are slight tweaks from the all-open configuration with the mode at $4$ meaning that four of the 12 vents are closed.  \tcr{(SCD: Sankalp. please replot with x-label 1, 2, 3, ... ,12.  You plotted the normalized Hamming distance to 2 significant digits.  Unnormalized -- the actual distance --  is more natural here)}


%\tcr{(SCD: Sankalp, can you put in text for Fig~\ref{fig:optimal_vs_nominal_iqs}--\ref{fig:optimal_minus_nominal_iqs_vs_reconstructionloss}.  The first is hard for me to reconcile with the previous bar-plot because in the bar-plot we didn't see improvmeent for each point.  Here all the points are below the line.  The second plot I guess shows how far from open we set the vents as a function of IQ.  Here again I would use distnace (0-12) rather than fractional max-distance.  I suppose the point is that at low nominal IQs the vents are more open?  Then the plot of reconstruction loss vs improvement I don't know what to say -- though this seems to contrast with he first plot as here some are above the x-axis.  Is one calibrated and the other is not?  The final one -- IQ vs normalized wind speed seems to say that the improvement is uniform across wind speeds?  Again, why "normalized" wind speed,  How about "binned" wind speed -- like 0-3kph, 3-7kph 7-15kph, etc?  Normalizations are harder for a reader to understand.)}

\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/attribution_plot.pdf}
    \caption{Attribution plot for all samples. This gives a bird's eye view of which features the model sees as most predictive.}
    \label{fig:attplot0}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/attribution_plot_ventsallopen.pdf}
    \caption{Attribution plot for in-distribution (ID) samples with all vents open. This enables us to isolate those features which make samples ID.}
    \label{fig:attplot1}
\end{subfigure}
\caption{Attribution (a.k.a. summary) plots. By using expected gradients \citep{explaining_explanations_hessians}, we obtain the impact of each feature on the mixture density network's predicted output. These are then collated for all samples in the test set (for a given fold), and collated again for all test sets from all folds. \textbf{(a):} Using \textit{all} $\sim60000$ samples. \textbf{(b):} Using only the $\sim6000$ in-distribution (ID) samples for which there exist vent configurations that result in lower predicted median MPIQ than their counterparts with all 12 vents open.}% Pictured above are the average feature attributions, which smooth over variations from individual samples.}
\label{fig:attplots}


\captionof{table}{Most predictive features identified in Figure \ref{fig:attplots}.}
%\centering
\iffalse
\begin{tabular}{cccc}
\toprule
Abbreviation &                                        Feature & Abbreviation &                                      Feature \\
\midrule
       F:000 &                             Barometric Pressure &          &                                \\
       F:003 &                         Catwalk Temperature East &         F:052 &  Mirror Surface Temperature South Underside Spigot \\
       F:004 &                        Catwalk Temperature North &         F:056 &                   Observing Room Air Temperature \\
       F:007 &                                 Filter Central Wavelength &         F:060 &      Thrust Bearing Surface Temperature South Beam \\
       F:015 &                                     Current Altitude &         F:063 &                     Top Ring Air Temperature West \\
       F:023 &                             Dome Top Temperature &         F:065 &         Truss Surface Temperature North Halfway-up \\
       F:027 &                        Dome Wall Temperature West &         F:069 &          Truss Surface Temperature West Halfway-up \\
       F:029 &           Electrical box Surface Temperature South &         F:076 &                                       Vent L3 \\
       F:034 &  Fifth floor Surface Temperature North Pier Concrete &         F:094 &                                       Vent R3 \\
       F:038 &      Horseshoe Surface Temperature East-bearing Pad &         F:097 &                                       Vent R4 \\
       F:040 &      Horseshoe Surface Temperature West-bearing Pad &         F:107 &                      Weather Tower Temperature \\
       F:041 &                                $\cos$(hour of day) &         F:111 &                              $\cos$(week of year) \\
       F:042 &                                $\sin$(hour of day) &         F:112 &                              $\sin$(week of year) \\
\bottomrule
\end{tabular}
\fi
\begin{tabular}{cccc}
\toprule
Abbreviation &                                        Feature & Abbreviation &                                      Feature \\
\midrule
       F:000 &                             Barometric pressure &          F:057 &           Rear observing room air temperature \\
       F:004 &                        Catwalk temperature, north &         F:060 &      Thrust bearing surface temperature, south beam \\
       F:007 &                                 Filter central wavelength &         F:061 &                Top ring air temperature, east \\
       F:015 &                                     Current altitude &         F:063 &                     Top ring air temperature, west \\
       F:023 &                             Dome top temperature &         F:065 &         Truss surface temperature, north halfway-up \\
       F:027 &                        Dome wall temperature, west &         F:069 &          Truss surface temperature, west halfway-up \\
       F:037 &           Fourth floor crawlspace air temperature &         F:070 &                                   Vent L1 \\
       F:042 &                                $\sin$(hour of day) &         F:107 &                      Weather tower temperature \\
       F:051 &         Mirror surface temperature, east underside &         F:110 &                    Weather tower wind speed \\
       F:052 &  Mirror surface temperature, south underside spigot &         F:111 &                              $\cos$(week of year) \\
       F:056 &                   Observing room air temperature &         F:112 &                              $\sin$(week of year) \\
\bottomrule
\end{tabular}
\label{tab:attribution_features}
%\end{table*}
\end{figure*}

\iffalse
\begin{table}
    \caption{abc}
    \label{tab:attribution_features}
\begin{tabular}{cc}
\toprule
Abbreviation &                                        Feature \\
\midrule
       F:000 &                             Barometric Pressure \\
       F:003 &                         Catwalk Temperature East \\
       F:004 &                        Catwalk Temperature North \\
       F:007 &                                 Filter Central Wavelength \\
       F:015 &                                     Current Altitude \\
       F:023 &                             Dome Top Temperature \\
       F:027 &                        Dome Wall Temperature West \\
       F:029 &           Electrical box Surface Temperature South \\
       F:034 &  Fifth floor Surface Temperature North Pier Concrete \\
       F:038 &      Horseshoe Surface Temperature East-bearing Pad \\
       F:040 &      Horseshoe Surface Temperature West-bearing Pad \\
       F:041 &                                $\cos$(hour of day) \\
       F:042 &                                $\sin$(hour of day) \\
       F:052 &   Mirror Surface Temperature South Underside Spigot \\
       F:056 &                    Observing Room Air Temperature \\
       F:060 &       Thrust Bearing Surface Temperature South Beam \\
       F:063 &                      Top Ring Air Temperature West \\
       F:065 &          Truss Surface Temperature North Halfway-up \\
       F:069 &           Truss Surface Temperature West Halfway-up \\
       F:076 &                                        Vent l3 \\
       F:094 &                                        Vent r3 \\
       F:097 &                                        Vent r4 \\
       F:107 &                       Weather Tower Temperature \\
       F:111 &                               $\cos$(week of year) \\
       F:112 &                               $\sin$(week of year) \\
\bottomrule
\end{tabular}
\end{table}
\fi

\iffalse
\begin{table}
    \centering
    \caption{Comparative performance of machine learning and deep learning based methods, across all four datasets (Section \ref{sec:data}), and all six metrics (Section \ref{sec:DL_metrics}). In each cell, the top figure is the result from RVAE+MDN, while the bottom figure is the result from GBDT; the better value is in bold. We can see two clear trends from these numbers:}
    \label{tab:ml_vs_dl_results}
    \begin{tabular}{ccccccc}
        \toprule
        \toprule
        Data Set & RMSE & MAE & MBE & ACE$_{0.33}$ & IS$_{0.33}$ & CRPS \\ \midrule
        $\mathcal{D_{F_S,S_L}}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ \\
        $\mathcal{D_{F_S,S_S}}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ \\
        $\mathcal{D_{F_L,S_L}}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ \\
        $\mathcal{D_{F_L,S_S}}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ & $\frac{xx}{yy}$ \\
        \bottomrule
    \end{tabular}
\end{table}
\fi


\iffalse
\begin{figure*}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/aleatoric_uncal.pdf}
    \caption{Uncalibrated Aleatoric Uncertainty}
    \label{fig:learning_rate}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/aleatoric.pdf}
    \caption{Calibrated Aleatoric Uncertainty}
    \label{fig:training_curve}
\end{subfigure}
\caption{caption}
\label{fig:abc}
\end{figure*}
\fi