\subsection{Semi-Supervised Learning}\label{sec:DL_SSL}
Semi-supervised learning (SSL) is a domain of machine learning which allows us to combine (typically a small amount) of labeled data with (typically a larger amount) of unlabeled data during training. SSL falls somewhere between unsupervised learning--with no labeled training data--and supervised learning--with only labeled training data. The primary goal while employing an SSL framework is to use the unlabeled dataset to guide our model's predictions on the labeled dataset. A secondary goal shared by some methods--including the one we employ in this work--is to label the unlabeled data using the labeled information set.

While there exists a plethora of methods, both simple and complex, in this domain, and the development of SSL algorithms to solve problems in real-time is an area of active research \textbf{CITATION NEEDED}, we use an algorithm of the type that solves SSL by generating pseudo-labels for the unlabelled data. `Speed as a Supervisor' (SaaS, \cite{saas1}) enables us to generate pseudo-labels concurrently with estimating the parameters of our deep learning model. Ussing SaaS we are able to obtain (potentially accurate) estimates for the unlabeled data at the end of training. \cite{saas1} empirically show that corollary to widely known fact that accurate labels lead to fast training \citep{good_labels_leadsto_fast_training}, fast training is also a good indicator of good labels. Specifically, the algorithm consists of two loops: in the inner loop, SaaS aims to find a set of pseudo-labels that decreases the loss function over a small number of epochs, the most; in the outer loop, we optimize ofer the posterior distribution of the pseudo labels \cite{saas2} \yst{what is being optimized here? Is it the similarity between the label distribution of the labelled training set vs. the pseudo labels? If so, why is that a good assumption?}. We show this process via a cartoon in Figure()(), where we use our modified TabNet in the inner loop.

%figure and algo, consider https://openaccess.thecvf.com/content_CVPR_2020/papers/Lokhande_Generating_Accurate_Pseudo-Labels_in_Semi-Supervised_Learning_and_Avoiding_Overconfident_Predictions_CVPR_2020_paper.pdf
