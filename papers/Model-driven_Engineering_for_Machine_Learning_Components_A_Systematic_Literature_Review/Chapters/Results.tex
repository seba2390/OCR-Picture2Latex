\section{Results}~\label{sec:Results}
This section presents the results of our SLR on MDE approaches for systems with ML components. We organize these findings based on the four previously mentioned research questions.
\subsection{Publication Trends}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=\textwidth]{Images/PubTrends/year.png}
    \caption{Study Distribution by Year}
    \label{fig:yearDist}
    \vspace*{-1em}
    \end{subfigure}
\hfill
    \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/PubTrends/studytype.png}
    \caption{Study Distribution by Publication Type}
    \label{fig:studyType}
    \vspace*{-1em}
\end{subfigure}
    \vspace*{1em}
 \caption{Publication Trends}
\end{figure}

We selected 46 primary studies on MDE4ML, published over a span of 16 years, as shown in Figure~\ref{fig:yearDist}. Appendix A lists the full citation details of these primary studies. There are no studies included from 2009, 2010, and 2013. The number of papers published from 2011 to 2018 was low compared to 2019 and onward. There was a drop in 2020, which could have been due to the COVID-19 outbreak. However, we are not sure. The high number of studies in 2021 and 2022 show the increasing research interest in MDE for ML and we hope to see the same trend in the future. Our search process was conducted in March 2023 and could thus be the reason for the low numbers in 2023.  

Most of the studies were published in conferences and journals, contributing to ~39\% and 37\% of the total paper count, followed by workshops and symposiums, making up for the remaining ~20\% and ~4\%, respectively. Figure \ref{fig:studyType} provides more detail about the type of publications included in this study. The selected 46 studies belonged to 34 different venues; the venues with two or more published studies are shown in Table \ref{table:pubTrends}. Model Driven Engineering Languages and Systems (MODELS) conference, co-located workshops, and Software and Systems Modeling (SoSym) journal have the highest number of papers in each publication type category. This is unsurprising as these are all highly reputed publication venues dedicated to MDE. 

\begin{table}[htbp]
\centering
\caption{Publication Venues with two or more research studies}
\footnotesize
\begin{tabular}{ p{11cm} p{2cm} p{2cm}} 
\hline
\textbf{Publication Venue} & \textbf{Type} & \textbf{No. of Studies}\TBstrut \\
\hline
Model Driven Engineering Languages and Systems (MODELS) & Conference & 4 \TBstrut \\ 
Model-Based Software and Systems Engineering (MODELSWARD) & Conference & 2 \TBstrut \\
Model Driven Engineering Languages and Systems (MODELS) - Companion  & Workshop  & 6 \TBstrut \\
Computer Languages & Journal & 2  \TBstrut \\
Software and System Modeling (SoSym) & Journal & 3  \TBstrut \\
\hline
\end{tabular}
\label{table:pubTrends}
\end{table}

In the following sections, we describe the features of MDE solutions for ML-based systems with respect to our RQs. Figure~\ref{fig:features} provides an overview of these features, including the goal, end users, modeling, supported ML aspects and more. We extracted data corresponding to these features from our selected primary studies and reported our findings.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Images/PubTrends/featuresnew.png}
    \caption{Features of Selected Primary Studies}
    \label{fig:features}
    \vspace*{-1em}
\end{figure*}

\subsection{RQ1 - Motivation for MDE4ML approaches}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{Images/RQ1/goals.png}
    \caption{Goal Distribution in Studies}
    \label{fig:goals}
\end{figure}

\subsubsection{Motivation, Goals, and Objectives}
All the analyzed studies described their goals, objectives, and motivations for applying MDE techniques to systems with ML components. We divided these goals into three high-level categories \textit{effort reduction}, \textit{increased stakeholder understanding}, and \textit{quality improvement}. These categories are not mutually exclusive and many studies fall under more than one category, as shown in the Venn diagram in Figure \ref{fig:goals}. The full breakdown of goals, sub-goals, and relevant studies can be seen in Table \ref{table:goals}. \\

\begin{table}[htbp]
\centering
\caption{Goals of Primary Studies}
    \label{table:goals}
    \footnotesize
        \begin{tabular}{p{3cm} p{3.5cm} p{8.5cm}}
        \hline
        \textbf{Goal} & \textbf{Sub-goal} & \textbf{Studies} \TBstrut \\
            \hline
            \multirow{10}{*}{Effort Reduction}  & \multicolumn{1}{p{2.5cm}}{Abstraction} & \multicolumn{1}{p{8.5cm}}{P1, P4, P5, P6, P7, P8, P9, P10, P14, P16, P19, P21, P22, P23, P25, P27, P28, P29, P30, P33, P35, P36, P40, P41, P42, P43, P44, P46} \TBstrut\\
                                                & \multicolumn{1}{p{2.5cm}}{Automation} & \multicolumn{1}{p{8.5cm}}{P2, P4, P5, P9, P11, P12, P13, P16, P17, P18, P19, P21, P23, P24, P25, P27, P28, P30, P31, P32, P33, P34, P35, P36, P37, P38, P39, P41, P42, P43, P44, P46} \TBstrut\\
                                                & \multicolumn{1}{p{2.5cm}}{Integration} & \multicolumn{1}{p{8.5cm}}{P1, P5, P8, P11, P20, P22} \TBstrut\\
                                                & \multicolumn{1}{p{2.5cm}}{Monitoring} & \multicolumn{1}{p{8.5cm}}{P3, P6, P13} \TBstrut\\
                                                & \multicolumn{1}{p{3.5cm}}{System Management} & \multicolumn{1}{p{8.5cm}}{P3, P13} \TBstrut\\
                                                & \multicolumn{1}{p{2.5cm}}{Data management} & \multicolumn{1}{p{8.5cm}}{P11, P12} \TBstrut\\

            \hline
            \multirow{9}{*}{Quality Improvement}  & \multicolumn{1}{p{2.5cm}}{Reusability} & \multicolumn{1}{p{8.5cm}}{P8, P19, P23, P25} \TBstrut\\&{Extensibility} & \multicolumn{1}{p{8.5cm}}{P1, P8, P25, P26} \TBstrut\\&
            {Standardisation} & \multicolumn{1}{p{8.5cm}}{P1, P7, P10} \TBstrut\\
                                                     & \multicolumn{1}{p{3.5cm}}{Responsible ML} & \multicolumn{1}{p{8.5cm}}{P2, P3, P10} \TBstrut\\
                                                     & \multicolumn{1}{p{2.5cm}}{Interoperability} & \multicolumn{1}{p{8.5cm}}{P7, P45} \TBstrut\\

                                                     
                                                     &\multicolumn{1}{p{2.5cm}}{Maintainability} & \multicolumn{1}{p{8.5cm}}{P11} \TBstrut\\
                                                     & \multicolumn{1}{p{2.5cm}}{Scalability} & \multicolumn{1}{p{8.5cm}}{P16} \TBstrut\\
                                                     & \multicolumn{1}{p{2.5cm}}{Reliability} & \multicolumn{1}{p{8.5cm}}{P16} \TBstrut\\
                                                     
             \hline
             \multirow{2}{3cm}{Increased Stakeholder Understanding}  & \multicolumn{1}{p{3.5cm}}{Support non-ML Experts} & \multicolumn{1}{p{8.5cm}}{P2, P17, P24, P28, P34, P39} \TBstrut\\
                                                     & \multicolumn{1}{p{3.5cm}}{Common Language} & \multicolumn{1}{p{8.5cm}}{P14, P15, P32, P35, P36} \TBstrut\\
            \hline
        \end{tabular}
\end{table} 

\sectopic{Effort Reduction}
was the most common aim mentioned in (43 out of 46 studies) MDE4ML papers, and focuses on effort reduction in development (e.g., P4, P9, P13), integration (e.g., P1, P8, P20), monitoring (e.g., P3, P6, P13) and managing systems (e.g., P3, P11, P12) with ML components. \textit{Abstraction} is one way to achieve effort reduction by creating models of complex systems that hide unnecessary details to focus only on the required aspects. For example, the goal of P5 and P9 is to use models to reduce complexity when developing ML solutions for cyber-physical systems. \textit{Automation} is another way to reduce development effort, artifacts such as code, configurations, and documentation can be generated from the models without any manual effort. For example, the goal of P10 is to generate dataset description documents from models with dataset details such as structure, provenance, and social concerns. P28 aims to automatically generate code for neural networks by transforming MDE models into code. \textit{Integration} of ML components into the rest of the system is not a trivial task; the objective of P11 is to ease this process through an ML artifact model. Another example is P8, which aims to simplify integration between neural network components and non-ML components to create reusable neural networks. \textit{Monitoring} refers to observing a system at runtime to ensure desired behavior. Despite being important, setting up monitoring mechanisms is a complex task. P6 aims to support ML experts through MDE by setting up a monitoring solution to detect performance drops. \textit{System management} and \textit{data management} are two other goals mentioned in the studies that correspond to less effort required for management. P12 identifies the challenges of managing dynamic datasets and presents a model-driven approach with the goal of automating data retraining and version management. For system management, a model-driven approach is described in P13 with the objective of managing ML-based analytics services on IoT devices.\\


\sectopic{Quality Improvement}
is the second common category (13 out of 46 studies) that contains studies that intend to improve the quality of ML-based systems using MDE. These qualities include \textit{reusability}, the ability to be used in different contexts without needing significant effort  (e.g., P8, P23); \textit{extensibility}, the ability to be extended with new features or functionalities (e.g., P25, P26); \textit{standardization}, the adherence to standards or best practice guidelines (e.g., P1, P10); \textit{responsible ML}, developing ML systems in a manner that maximize benefit and minimize risk  (e.g., P2, P3); \textit{interoperability}, the ability to easily communicate with other systems (e.g., P7);  \textit{maintainability}, ease of incorporating changes (e.g., P11); \textit{scalability}, the capability of handling increased workloads  (e.g., P16); and \textit{reliability}, the ability to perform consistently as required without failures (e.g., P16). In this category, we found the largest number of papers for reusability and extensibility, followed by standardization and responsible ML. P2 is an example of a responsible ML study and aims to measure and mitigate biases in ML models using a DSL. P45 is an example of an interoperability study and aims to create models for manufacturing intelligence that can seamlessly operate across various devices and systems. Through modeling various aspects, study P10 intends to standardize machine learning datasets. \\

\sectopic{Increased Stakeholder Understanding} 
is the least common category (11 out of 46 studies) with papers that aim to support collaboration (e.g., P35, P36) and improve system understanding in stakeholders other than ML experts (e.g., P2, P17, P34). We found six studies that \textit{support non-ML experts} in developing ML-based systems; these studies use domain-specific terminology and hide complex technical details. In P39, the goal is to provide a DSL to support software engineers in specifying requirements for neural networks. This is, otherwise, a challenging task since most software engineers are not experienced in deep learning~\cite{ahmad2023requirements}. We also found five studies focused on providing a \textit{common language} for easier communication and stakeholder collaboration. For example, P35 aims to facilitate multi-disciplinary teams developing data analytics and machine learning solutions using DSLs.  \\

\subsubsection{Machine Learning Techniques}

\begin{table}[htbp]
\centering
    \caption{Machine Learning Techniques Used in Studies}
    \label{table:MLtypes}
        \begin{tabular}{p{4.5cm} p{3.5cm} p{7cm}}
        \hline
        \textbf{Machine Learning Technique} & \textbf{Sub-type} & \textbf{Studies} \TBstrut \\
            \hline
            Generic Machine Learning \TBstrut	& - & P9, P10, P13, P15, P21, P22, P23, P32, P33, P41, P43 \TBstrut \\ 
            \hline
            \multirow{5}{*}{Supervised Machine Learning	}  & \multicolumn{1}{p{2.5cm}}          {Traditional} & \multicolumn{1}{p{7cm}}{P2, P6, P16, P17, P19, P35, P38} \TBstrut\\
                                                & \multicolumn{1}{p{2.5cm}}{Neural Networks} & \multicolumn{1}{p{7cm}}{P1, P3, P4, P7, P8, P11, P12, P14, P20, P24, P25, P26, P28, P29, P36, P39, P42, P44, P46} \TBstrut\\
                                                & \multicolumn{1}{p{3.5cm}}{Traditional and Neural Networks} & \multicolumn{1}{p{7cm}}{P27, P31, P34, P37, P45} \TBstrut\\
            
            \hline
                        Unsupervised Machine Learning \TBstrut	& - & - \TBstrut \\ 
            \hline
            Reinforcement Learning & - & P5, P18, P30, P40 \TBstrut\\
            \hline
        \end{tabular}
\end{table}

Table~\ref{table:MLtypes} shows the different subsets of ML techniques considered in the included primary studies. Of the 46 primary studies, 31 studies (67\%) explicitly focused on supervised ML, and 4 studies (9\%) on reinforcement learning. None of the studies focused explicitly on unsupervised learning or clustering. The remaining 11 studies (24\%) are termed as \emph{generic} ML studies, i.e., (i) the studies did not explicitly specify the type of ML their solution targets (e.g., P41, P43); (ii) the studies target both supervised and unsupervised learning (e.g., P22, P32), and (iii) the studies target supervised, unsupervised and reinforcement learning (e.g., P13). For instance, P22 offers an MDE solution for IoT devices using \emph{generic} ML and supports various supervised and unsupervised ML algorithms. \textit{Supervised machine learning} papers are further categorized into \textit{traditional supervised learning}, where studies use models such as decision trees, linear regression and naive Bayes (e.g., P19, P35, P38); \textit{neural networks}, where studies use neural networks and deep learning (e.g., P29, P44, P46) and \textit{traditional and neural networks}, where the studies support traditional supervised learning models and neural networks (e.g. P27, P34, P45). An example of a study exclusively for reinforcement learning is P40, which presents a DSL to model multi-agent reinforcement learning in distributed systems. %We also found four papers on MDE solutions for \textit{reinforcement learning}, for example, P40 presents a DSL to model multi-agent reinforcement learning in distributed systems. 
%Overall, the highest number of studies (19 out of 46 studies) focused exclusively on MDE  approaches for neural networks and deep learning. 

%in contrast to 67\% of studies on supervised learning and 9\% of studies on reinforcement learning. We did not find any study that was only for unsupervised learning. \textit{Generic ML} refers to the group of papers presenting MDE solutions for supervised and unsupervised learning (e.g., P22, P32), additionally, some of these papers also support reinforcement learning (e.g., P13). This category also contains papers that do not explicitly specify the type of ML their solution targets (e.g., P41, P43). The authors of study P22, for instance, offer an MDE solution for IoT devices using ML, the study supports various supervised and unsupervised ML algorithms. 

\subsubsection{Application Domain}
\begin{figure}[htbp]
    \centering

    \centering
    \includegraphics[width=0.5\textwidth]{Images/RQ1/domain.png}
    \caption{Application Domain Distribution}
    \label{fig:domain}
\end{figure}

\begin{table}[htbp]
\centering
    \caption{End Users Mentioned in Studies}
    \label{table:users}
    \footnotesize
        \begin{tabular}{p{3.5cm} p{5cm} p{6.5cm}}
        \hline
        \textbf{User Category} & \textbf{End User} & \textbf{Studies} \TBstrut \\
            \hline
            \multirow{3}{*}{ML-related Roles}  & \multicolumn{1}{p{4.5cm}}{ML Engineer} & \multicolumn{1}{p{6.5cm}}{P1, P5, P6, P7, P8, P10, P11, P12, P13, P15, P25, P37, P40, P41, P42, P43, P44, P46} \TBstrut\\
                                                & \multicolumn{1}{p{4.5cm}}{\mbox{Data Analyst/ Engineer/ Scientist}} & \multicolumn{1}{p{6.5cm}}{P2, P10, P12, P14, P16, P18, P22, P24, P32, P34, P35, P36, P38} \TBstrut\\
            \hline
            
            \multirow{6}{*}{Software \& Systems Roles}  & \multicolumn{1}{p{4.5cm}}{Software Engineer} & \multicolumn{1}{p{6.5cm}}{P1, P2, P3, P4, P9, P11, P13, P19, P20, P22, P23, P27, P28, P29, P30, P39} \TBstrut\\
                                                & \multicolumn{1}{p{4.5cm}}{Systems Engineer} & \multicolumn{1}{p{6.5cm}}{P9, P31, P33} \TBstrut\\
                                                & \multicolumn{1}{p{4.5cm}}{Business Analyst} & \multicolumn{1}{p{6.5cm}}{P35} \TBstrut\\
                                                & \multicolumn{1}{p{4.5cm}}{Formal Methods Analyst} & \multicolumn{1}{p{6.5cm}}{P14} \TBstrut\\
            \hline
            \multirow{2}{*}{Other Roles}  & \multicolumn{1}{p{4.5cm}}{Domain Expert} & \multicolumn{1}{p{6.5cm}}{P17, P21, P24, P25, P26, P28, P30, P32, P35, P36, P45} \TBstrut\\
            \hline
        \end{tabular}
\end{table}

Figure \ref{fig:domain} highlights the eleven application domains found in the selected studies. 52\% of studies mention an application domain with \textit{cyber physical systems (CPS)} and subsets of it being the most common. We found seven studies for generic \textit{CPS} (P5, P8, P9, P12, P22, P23, and P31), two for \textit{manufacturing systems} (P36 and P45), two for \textit{autonomous vehicles} (P1 and P3), one for \textit{smart homes} (P20), one for \textit{traffic signal control} (P30), one for \textit{satellite communication system} (P33) and one for \textit{network planning} (P25). The second most common application domain found in the primary studies was \textit{big data analytics} in five studies (P13, P17, P21, P35, and P43) and \textit{data analytics} in three studies (P32, P34, and P38). We also found one study (P24) for developing \textit{social bots}. The MDE solutions proposed in almost half the studies (P2, P4, P6, P7, P10, P11, P14-P16, P18, P19, P26-P29, P37, P39-P42, P44, and P46) were generic and could be applied in any domain.

\subsubsection{End Users}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{Images/RQ1/users.png}
    \caption{End Users Distribution in Studies}
    \label{fig:users}
\end{figure}

Figure \ref{fig:users} provides an overview of the end user distribution in the primary studies. The proposed MDE solutions for ML-based systems are intended for three different categories of users, as shown in Table \ref{table:users}. The \textit{ML-related roles} category includes \textit{ML engineer} and \textit{data analyst/ engineer/scientist}; the \textit{software and systems roles} category includes \textit{software engineer}, \textit{system engineer}, \textit{business analyst}, and \textit{formal methods analyst}; and the last category \textit{other roles} includes \textit{domain expert}. Of the 46 selected studies, 13 presented approaches were for more than one user category, while the remaining 33 focused on a single end-user category. Of all the studies (including overlaps), 18 focused on MDE solutions for ML engineers, followed by solutions for software engineers in 16 studies, and 11 focused on approaches for domain experts. An example of each of these three main categories is: P1 proposes an approach for ML engineers to model deep neural network architectures for autonomous vehicle perception; P9 presents a modeling approach for software engineers to model ML components integrated on edge IoT devices for data analytics; 
and P17 enables domain experts to represent ML problems as models and derive code from them.
%For instance, P1 proposes an approach for ML engineers to model deep neural network architectures for autonomous vehicle perception. 
%A fair number of studies (11 out of 46 studies) were found on approaches for domain experts to leverage ML. For example, P17 enables domain experts to represent ML problems as models and derive code from them.


\begin{table}[htbp]
\centering
    \caption{Contributions of Studies}
    \label{table:contributions}
    \footnotesize
        \begin{tabular}{p{4.5cm} p{2.5cm} p{2.5cm} p{5.5cm}}
        \hline
        \textbf{Contribution} &  & & \textbf{Studies} \TBstrut \\

            \hline
            \multirow{8}{*}{Tool}  & \multirow{6}{2.5cm}{Model-to-Text Transformer}  & \multicolumn{1}{p{2.5cm}}{Code Generator}& \multicolumn{1}{p{5.5cm}}{P1, P2, P4, P5, P7, P8, P9, P11, P12, P13, P14, P16, P17, P19, P20, P22, P23, P24, P25, P27, P28, P30, P31, P32, P34, P35, P37, P38, P39, P41, P42, P43, P44, P45, P46} \TBstrut\\
                                                & &\multicolumn{1}{p{2.5cm}}{Text Generator} & \multicolumn{1}{p{5.5cm}}{P3, P6, P10, P15, P21, P26, P35, P36} \TBstrut\\ \cline{2-4}
                                    & \multirow{1}{2.5cm}{Model-to-Model Transformer} & \multicolumn{1}{p{2.5cm}}{Model Generator} & \multicolumn{1}{p{5.5cm}}{P1, P3, P4, P18, P25, P29, P33, P40, P41, P42} \TBstrut\\
            \hline
            \TBstrut Domain-specific Language (DSL) \TBstrut	& & & P1, P2, P6, P7, P10, P11, P13, P14, P15, P16, P19, P21, P25, P26, P27, P28, P29, P30, P32, P35, P36, P37, P38, P39, P40, P41, P42, P43, P44, P46 \TBstrut \\ 
            \hline   
            Framework \TBstrut	& & & P1, P3, P5, P7, P8, P12, P13, P18, P19, P23, P24, P25, P26, P27, P29, P31, P34, P36, P37, P38, P44 \TBstrut \\ 
            \hline
            Model \TBstrut & & & P4, P11, P13, P20, P31, P37 \TBstrut\\
            \hline
            Modeling Approach \TBstrut	& & & P9, P11, P14, P16, P20, P30 \TBstrut \\ 
            \hline
            Modeling Language Extension \TBstrut	& & & P5, P17, P22, P33, P45 \TBstrut \\ 
            \hline
            ML Knowledge Base \TBstrut & & & P15, P16, P20, P34 \TBstrut\\
            \hline     
            Data Synthesizer \TBstrut & & & P31, P39, P45 \TBstrut\\
            \hline
            OCL Constraints \TBstrut & & & P27 \TBstrut\\
            \hline
            API \TBstrut & & & P40 \TBstrut\\
            \hline
            Meta-modeling Language  \TBstrut	& & & P23 \TBstrut \\ 
            \hline
        \end{tabular}
\end{table}




\subsubsection{Contributions}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{Images/RQ1/tool.png}
    \caption{Tool Distribution in Studies}
    \label{fig:tools}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Images/RQ1/bubbleChart.jpg}
    \caption{Bubble Chart for Study Goal, Study Contribution and Machine Learning Aspects}
    \label{fig:GoalvsContribution}
    \vspace*{-1em}
\end{figure*}


We identified eleven different contributions in the analysed MDE4ML approaches, tabulated in Table \ref{table:contributions}. We show tool-specific contributions in the Venn diagram in Figure~\ref{fig:tools}. The most common contribution (in 35 out of 46 studies) is a \textit{code generator} that transforms model(s) into code. For example, in P23, the code generator transforms CPS domain models to code for domain classes and then weaves ML code into it. An alternate approach in P17 reads a domain-specific model for baseball analytics using binary classification and produces ML classification code. Other kinds of generators presented in studies include \textit{model generator} (10 out of 46 studies) that transforms an input model(s) into an output model(s) and \textit{text generator} (8 out of 46 studies) that transforms model(s) into text other than code, e.g., documentation. The second common contribution (in 30 out of 46 studies) is a domain-specific language (\textit{DSL}), which includes graphical and textual modeling languages intended for a particular domain. A graphical DSL is proposed in P1 to model deep learning-based computer vision tasks in autonomous vehicles. A textual DSL is presented in P2 to mitigate biases in ML datasets, to model the dataset, training methods, bias metrics, and bias mitigation methods. Another common contribution (in 21 out of 46 studies) is an MDE \textit{framework} for systems with ML components. A framework to monitor ML components in CPS is introduced in P3. The study leverages goal models to evaluate the ML components at runtime and ensure correct behavior in uncertain environments. Several other outcomes were also identified such as \textit{models} (6 out of 46 studies), \textit{modeling approach} (6 out of 46 studies), 
\textit{modeling language extension} (5 out of 46 studies), 
\textit{ML knowledge base} (4 out of 46 studies),
\textit{data synthesizer} (3 out of 46 studies),
\textit{OCL constraints} (1 out of 46 studies study), \textit{APIs} (1 out of 46 studies study), and \textit{meta-modeling language} (1 out of 46 studies study). However, in comparison, these remaining contributions were significantly lower in number. Most papers make multiple contributions, e.g., P25 provides a framework, DSL, model generator, and code generator.




Figure~\ref{fig:GoalvsContribution} shows a bubble chart of the study goals and contributions against their relevant ML aspect. The size of the bubble depicts the frequency of studies in that category. For example, related to the \textit{requirements engineering} aspect of ML components, the goal of three studies is to achieve automation. In comparison, three studies provide a code generator as their contribution. While analyzing this figure, we found that in the selected studies, the preferred ML aspects are the \textit{design and development} of ML components, and \textit{training} of ML components. In contrast, \textit{monitoring} of ML components, and \textit{documentation} were the most neglected aspects.


\begin{center}
\begin{myframe}[width=45em,top=5pt,bottom=5pt,left=5pt,right=5pt,arc=10pt,auto outer arc,title=\centering\textbf{RQ1 Answer Summary}]
\footnotesize
The majority of studies aim to use MDE for ML-based systems to reduce effort through automation and abstraction, and provide code generators, DSLs, and frameworks. Considerably fewer studies have focused on quality improvement and increased stakeholder understanding as a goal of their research. Nearly half of the studies were not limited to any specific application domain, while most of the remaining studies focused on cyber-physical systems. The more significant part of the studies provides MDE solutions for users with ML-related roles like ML engineers and data scientists, whereas few are intended for domain experts. The most common type of ML technique found in the primary studies was deep learning, whereas the least common was unsupervised learning. We also found that the goals and contributions of most studies were related to the design, development, and training aspects of ML, whereas monitoring and documentation were ignored.
    \end{myframe}
\end{center}

\subsection{RQ2 - MDE4ML approaches and tools used} 

\subsubsection{Modeling Characteristics}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/RQ2/representation.png}
    \caption{Model Representations in Studies}
    \label{fig:ModelRepresentation}
    \end{subfigure}
\hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
\includegraphics[width=\textwidth]{Images/RQ2/language.png}
    \caption{Language Types in Studies}
    \label{fig:ModelingLanguages}
\end{subfigure}
\hfill
    \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/RQ2/automation.png}
    \caption{Automation Levels in Studies}
    \label{fig:AutomationLevels}
    \end{subfigure}
    \caption{MDE Solution Characteristics in Studies}
\end{figure}

\sectopic{Model Representation} refers to the \textit{graphical notations} or \textit{textual notations} used for expressing a model. Figure~\ref{fig:ModelRepresentation} shows our findings; half of the studies (23 out of 46 studies) use graphical models (P1, P3, P4, P13-P19, P21, P24-P27, P30, P32-P38), nearly half of the studies (21 out of 46 studies) use textual models (P2, P5-P12, P20, P23, P28, P31, P39-P46) and only a small number of studies (2 out of 46 studies) use both graphical and textual models (P22 and P29). An example of graphical representation is P33. It generates graphical SysML models of the architecture of autonomous systems with ML and non-ML components using restricted natural language requirements. An example of textual model representation is P41, wherein the authors build a textual domain-specific language (OptiML) to generate ML code for heterogeneous hardware platforms. P29 uses both graphical and textual representations to model artificial neural networks.\\

\sectopic{Modeling Languages} refer to specific languages or notations for creating abstract representations of systems in graphical, textual, or combined form. As discussed in Section~\ref{subsec:MDEBackground}, modeling languages can be classified as \textit{general purpose languages} (GPLs) such as UML and iStar, \textit{domain-specific languages} (DSLs), or \textit{extensions of an existing language}, such as UML profiles. As shown in Table~\ref{fig:ModelingLanguages}, we found the majority of studies (34 out of 46 studies) propose a new DSL (P1, P2, P6-P8, P10-P16, P19, P21, P23, P25-P30, P32, P34-P44, and P46), a significantly fewer fraction (9 out of 46 studies) use a GPL (P3, P4, P9, P17, P18, P20, P24, P31, and P45), and only 3 out of 46 studies extend an existing language (P5, P22, and P33). A DSL for MLOps is introduced in P37 to automate the ML pipeline. The authors use Kubernetes and a blockchain-based infrastructure. P6 presents a DSL for monitoring ML models and selects Kubernetes as a platform.  In contrast, P3 leverages GSN and KOAS general-purpose goal models to monitor ML components. An example of a study that extends an existing modeling language is P5. The authors extend the deep learning DSL family MontiAnna to support reinforcement learning, particularly for cyber-physical systems. \\

\sectopic{Information Modeled, Model Types and Levels.} Table~\ref{table:modelinfo} provides an overview of the models proposed in the selected studies. Models in MDE can be categorized into three levels: computation-independent models (CIMs), platform-independent models (PIMs), and platform-specific models (PSMs). CIMs contain high-level information about requirements or business processes without implementation details (e.g., P3, P18, P35). PIMs contain design or solution details without platform information (e.g., P5, P26, P39), and PSMs contain low-level platform-specific implementation details (e.g., P9, P35, P42). As shown in Table~\ref{table:modelinfo}, a significant fraction of studies model PIMs (42 out of 46 studies), of which 35 are only PIMs, six studies model PIMs and PSMs, and only one study (P35) models all three categories. Only two studies exclusively model CIMs (P3 and P18) and PSMs (P4 and P37).

%A large fraction of studies (43 out of 46 studies) present PIMs, out of which 36 studies only model at this level. Comparatively, a much smaller portion of studies (6 out of 46 studies)  use PIMs and PSMs together. Interestingly, we only found one study using PSMs in isolation, two studies using CIMs in isolation, and only one study using models at all three levels together.

{In terms of model types, 39 out of 46 studies were design-level models, followed by requirements-level models (6 out of 46 studies), and finally data-representation models (5 out of 46 studies). These results are not surprising since most studies focus on the design, development, and training of ML components. Other types of models, such as feature models, process models, and deployment models, were only found in a few studies.}

\begin{table}[htbp]
\centering
%\footnotesize
    \caption{Modeled Information, Model Level and Model Type in studies}

    \resizebox{\textwidth}{!}{ % Specify the width and height (height is automatically adjusted to maintain aspect ratio)
        \begin{tabular}{l l l l }
        \hline
       \TBstrut  \textbf{Paper ID} \TBstrut  & \textbf{Model Level(s)} & \textbf{Model Type(s)} & \textbf{Information Modeled}  \TBstrut\\
            \hline
            \TBstrut P1 \TBstrut	& PIM & Design model & DL framework application domains, CNN model, layers, datasets, training, hyper-parameters and evaluation \TBstrut \\
            \TBstrut P2 \TBstrut	& PIM & Design model & ML bias metrics, mitigation algorithms, datasets, training methods and hyper-parameters \TBstrut \\ 
            \TBstrut P3 \TBstrut	& CIM & Requirements model & Assurance cases for design and runtime, System goals, sub-goals, agents, objectives, and utility functions for goals\TBstrut \\
            \TBstrut P4 \TBstrut	& PSM   & Feature model, Design model & DL library features, API elements (i.e., classes, methods, and constructors) related to features, feature interactions\TBstrut \\
            \TBstrut P5 \TBstrut	& PIM    &  Design model & NN model, layers, training methods, and hyper-parameters\TBstrut \\
            \TBstrut P6 \TBstrut	& PIM    & Design model  & ML model to monitor, ML framework, data and concept drift detection algorithms, input features and output\TBstrut \\
            \TBstrut P7 \TBstrut	& PIM    & Design model &  ML problem, dataset, training method, NN model, neurons, layers, and connections\TBstrut \\
            \TBstrut P8 \TBstrut	& PIM    & Design model, Dataset model & NN architecture, layers, training method, hyper-parameters, input features, and output \TBstrut \\
            \TBstrut P9 \TBstrut	& PIM, PSM    &  Design model &  IoT service structure and behavior, data analytics model, data pre-processing, training methods, and prediction  \TBstrut \\
            \TBstrut P10 \TBstrut	& PIM    & Dataset model & Dataset description including metadata, composition, data instance, provenance, and social concerns \TBstrut \\
            \TBstrut P11 \TBstrut	& PIM    & Design model, Artifact model & Software, source code, dataset, pre-trained model and training environment archive, pipelines and hyper-parameters\TBstrut \\
            \TBstrut P12 \TBstrut	& PIM    & Design model, Dataset model  & NN architecture, layers, dataset, training methods, retraining, hyper-parameters, input features and output\TBstrut \\
            \TBstrut P13 \TBstrut	& PIM   & Design model  & ML models, frameworks, data pre-processing, training methods, hyper-parameters, deployment, and evaluation\TBstrut \\
            \TBstrut P14 \TBstrut	& PIM   & Requirements model & Dataset structure, properties, invariant properties, data, and equivalence classes\TBstrut \\
            \TBstrut P15 \TBstrut	& PIM    & Design model  & ML Knowledge source, version, relevance, reliability, decisions, ML algorithms and characteristics\TBstrut \\
            \TBstrut P16 \TBstrut	& PIM    & Design model  & ML pipeline, activities, ML model, ML component architecture, dataset, pipeline experiment, and justification \TBstrut \\
            \TBstrut P17 \TBstrut	& PIM    & Probabilistic Graphical model  & Classification problem, observed variables (input features), random variables, nodes, gates, plates and factors \TBstrut \\
            \TBstrut P18 \TBstrut	& CIM    & Requirements model & Goals, tasks, qualities, effects, preconditions, actor boundaries and links, \TBstrut \\
            \TBstrut P19 \TBstrut	& PIM    & Design model  & Fog layer, fog nodes, ML layer, ML algorithms, and rule-based algorithms \TBstrut\\
            \TBstrut P20 \TBstrut	& PIM    & Entity model & Physical entities, capabilities, states, location, users, activities, preferences, ML models\TBstrut \\
            \TBstrut P21 \TBstrut	& PIM    & Probabilistic Graphical model & Observed variables (input features), random variables, parameters, relationships, nodes, gates, plates and factors\TBstrut\\
            \TBstrut P22 \TBstrut	& PIM    & Design model  & ML features, ML models, data analytics libraries labels, results, datasets, timestamps, and AutoML support. \TBstrut \\
            \TBstrut P23 \TBstrut	& PIM    & Design model  & Specified, learned and derived properties, relations, parameters, ML algorithms, and features\TBstrut \\
            \TBstrut P24 \TBstrut	& PIM    & Design model  & Virtual learning environment instance, users, bots, user and bot actions, parameters, DL classifier, and triggers \TBstrut \\
            \TBstrut P25 \TBstrut	& PIM    & Design model  &  Wireless network plan properties, prediction problems, datasets, features, ML models, parameters, and evaluation \TBstrut \\
            \TBstrut P26 \TBstrut	& PIM    & Design model  & Data organized by Projects, project files (e.g. training data, testing data, validation data), runs and parameters\TBstrut \\
            \TBstrut P27 \TBstrut	& PIM    & Design model  & Entities, context, observations, notifications, properties, associations, ML models, inputs, output, and evaluation\TBstrut \\
            \TBstrut P28 \TBstrut	& PIM    & Design model  & NN architecture, layers, inputs, output, and training methods\TBstrut \\
            \TBstrut P29 \TBstrut	& PIM    & Design model  & ANN model, system, layers, links (weights), and bias\TBstrut \\
            \TBstrut P30 \TBstrut	& PIM    & Design model  & Agents, agent, decision and learning capabilities, entities, attributes, states, and decision options\TBstrut \\
            \TBstrut P31 \TBstrut	& PIM    & Design model  & NN architecture, layers, neurons in layers, inputs, weights, bias, and ML model meta data\TBstrut \\
            \TBstrut P32 \TBstrut	& PIM    & Process model & Dataflow process, sub-processes, function interface, inputs, output, constants, and ML algorithms\TBstrut \\
            \TBstrut P33 \TBstrut	& PIM    & Design model, Requirements model & System structure and behavior, properties, data, ML components, ML algorithms, and evaluation metrics \TBstrut \\
            \TBstrut P34 \TBstrut	& PIM    & Design model  & Dataset, dataset fields, domain, ML algorithms, parameters, data mining tasks, meta-features and predicted fields\TBstrut \\
            \TBstrut P35 \TBstrut	& CIM, PIM, PSM & Design model, Requirements model, & Big data analytics high-level tasks, sub-tasks, processes, stakeholders, operations, conditions, ML models, training  \TBstrut\\
            \TBstrut \TBstrut       &        & Process model, Artifact model, & methods, data processing techniques, data and artifacts, and deployment details\TBstrut \\
            \TBstrut \TBstrut       &        & Deployment model & \TBstrut \\
            \TBstrut P36 \TBstrut	& PIM    & Design model  & Manufacturing flows, processes, equipment and resources, NN model, layers, inputs, output, bias and edges\TBstrut \\
            \TBstrut P37 \TBstrut	& PSM    & Design model  & NN architecture, ML Pipeline, ML tasks, dataset import, training, evaluation, Kubernetes clusters, and nodes \TBstrut \\
            \TBstrut P38 \TBstrut	& PIM    & Design model, Dataset model & Classification problem, ML algorithm, features, evaluation metrics, labels, dataset, and hyper-parameters\TBstrut \\
            \TBstrut P39 \TBstrut	& PIM    & Design model, Requirements model & Requirements, NN behavior, inputs, output, datasets, equivalence classes, properties, and evaluation metrics \TBstrut \\
            \TBstrut P40 \TBstrut	& PIM    & Design model  & Goals, actors, actions, parameters, reward functions, properties, identifiers and messages \TBstrut \\
            \TBstrut P41 \TBstrut	& PIM, PSM  & Design model  & Vectors (vertices, edges, indices), Matrices, and Graphs to support operations in ML algorithms \TBstrut \\
            \TBstrut P42 \TBstrut	& PIM, PSM  & Design model  & NN model, layers, hyper-parameters, loops, tensor and scalar expressions, and functions\TBstrut \\
            \TBstrut P43 \TBstrut	& PIM    & Design model  & Inputs, output, functions, hyper-parameters, messages, graphs, nodes, expressions, and loops \TBstrut \\
            \TBstrut P44 \TBstrut	& PIM, PSM  & Design model  & NN model, layers, arrays and tensor operations, training, and evaluation\TBstrut \\
            \TBstrut P45 \TBstrut	& PIM, PSM  & Design model  & Regression model, NN model, inputs, layers, ML algorithms, functions, and bias \TBstrut \\
            \TBstrut P46 \TBstrut	& PIM, PSM  & Design model  & Sets, maps, iterations, functions, schedule, data dependencies, parameters, matrices, expressions and  statements
\TBstrut \\
            \hline
        \end{tabular}
        }
    \label{table:modelinfo}
\end{table}

\begin{figure}[htbp]
\centering
    \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{Images/RQ2/MLaspects.png}
    \caption{Machine Learning Aspects Supported in Studies}
    \label{fig:MLaspects}
    \end{subfigure}
\hfill
    \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/RQ2/genLanguages.png}
    \caption{Generated Code and Text Languages in Studies}    
    \label{fig:generatedlanguages}
\end{subfigure}
\caption{ML aspects and Generated Text Languages}
\end{figure}

\subsubsection{Supported Machine Learning Aspects}
From the included primary studies we found 17 different ML aspects that were addressed by studies, 30 out of 46 studies focused on two or more ML aspects, whereas the remaining 16 studies only focused on one ML aspect. Figure \ref{fig:MLaspects} shows the distribution of ML aspects in studies, these include the development stages of ML components and other related aspects. The development stages consist of \textit{requirements engineering}, to gather, analyse and specify requirements for ML-based systems (e.g., P3, P35, P39); \textit{data preprocessing}, cleaning and transforming data into a suitable format for input to the ML model (e.g., P13, P14); \textit{design and development of ML models/components}, algorithm selection, feature engineering and coding for the ML component (e.g., P7, P24, P36);  \textit{training}, feeding data to a ML model for it to learn patterns and relationships (e.g., P22, P25, P42); \textit{evaluation}, evaluating the ML model's performance on unseen (test) data before deployment (e.g., P1, P27, P39); \textit{deployment}, releasing the ML component into the production environment (e.g., P9, P35, P37); \textit{integration}, integrating ML components with other software or system components (e.g., P20, P23, P31); \textit{inference}, using a trained ML model to make predictions on unseen data after deployment (e.g., P3, P6, P21); \textit{monitoring}, observing the ML component at runtime for correct behaviour (e.g., P3, P6) and \textit{management}, managing ML components after deployment through retraining, parameter adjustment, version control and maintenance (e.g., P12, P26, P31). Other related ML aspects consist of  \textit{data generation}, artificially creating data for training ML models (e.g., P39, P45); \textit{data storage}, storing and organizing datasets (e.g. P13); \textit{data visualization}, creating visual representations of data to uncover patterns, relationships, and insights (e.g., P26); \textit{documentation}, recording various aspects of the ML component such as dataset details, architecture, training setup and more (e.g., P10); \textit{ML pipeline development}, building the process for developing a ML component from data pre-processing to deployment (e.g., P4, P16, P32); and \textit{ML knowledge base development}, creating a repository of information about ML models and resources (e.g., P15, P16, P34). More than half the studies (28 out of 46 studies) were targeted toward designing and developing ML models or components, while nearly half (22 out of 46 studies) were for training the ML models. A reasonable portion of studies (10 out of 46 studies) addressed deploying ML components with a primary focus on practices such as DevOps and MLOps. The least explored ML aspects were documentation, data storage, visualization with only one study, and monitoring and data generation with two studies.

An ML framework is a comprehensive platform that offers a structured foundation to build, train, and deploy ML models. In other words, they offer various tools and functionalities that cover the entire ML model development lifecycle. An ML library is a collection of functions and methods that allow developers to perform specific ML tasks, e.g., splitting datasets into training and test sets. Libraries are generally more lightweight than frameworks and focus on particular ML pipeline aspects. We examined the ML frameworks and libraries used in the studies, our results summarised in Table \ref{table:metaTools}. Based on our review of the 46 studies, Tensorflow was the most used ML framework, followed by MXNet, whereas Weka was the most used ML library, followed by Scikit-learn and Numpy.



\begin{table}[htb]
\centering
\caption{Machine Learning Frameworks and Libraries}
\footnotesize
\begin{tabular}{ p{4cm} p{3.5cm} | p{4cm} p{3.2cm} } 
\hline
\textbf{\mbox{Machine Learning Frameworks}} & \textbf{Studies} & \textbf{Machine Learning Libraries} & \textbf{Studies}\TBstrut \\
\hline
AI-toolbox & P40  &                             Encog	& P20, P28  \TBstrut \\ 
Caffee & P8, P11, P12  &                        Keras	& P1, P22  \TBstrut \\                                                       
\mbox{Deep Learning for Java (DL4J)} &	P4 &    NetLogo for Reinforcement Learning	& P30   \TBstrut \\
Infer.NET &	P17, P21 &                          NumPy	& P35, P37, P44 \TBstrut \\
ZenML	& P37    &                              Neuroph &	P7  \TBstrut \\ 
MXNET	& P5, P8, P11, P12 &                    OpenAI Gym &	P18  \TBstrut \\ 
PyTorch	& P6, P25, P37    &                     Pandas	& P17, P35      \TBstrut \\ 
Tensorflow	 & P5, P6, P8, P11, P12, P13, P22,P24, P31 &   Scikit-learn & P13, P22, P35 \TBstrut \\ 
Tensorflow Lite	& P9 &                         Weka	& P16, P34, P36, P45 \TBstrut \\

\hline
\end{tabular}
\label{table:MLframeworks}
\end{table}

\subsubsection{Tool Support}
\sectopic{Model Transformations.} MDE involves transforming models into text or different kinds of models. We refer to these as \textit{model-to-text (M2T)} and \textit{model-to-model (M2M)} transformations respectively. M2T transformations include model transformations into code, documentation, or any kind of textual artifact whereas M2M transformations comprise model transformations into different kind of models. ~\cite{brambilla2017model}. We found the largest portion of studies (35 out of 46 studies) using solely M2T transformations (P2, P5-P8, P10-P17, P19-P24, P26-P28, P30-P32, P34-P39, P43-P46). For example, P7 generates a Predictive Model Markup Language (PMML) file from a model with ML model details. A small portion of studies (4 out of 46 studies) use only M2M transformations (P18, P29, P33, and P40). For instance, P18 converts goal models into formal specification models to generate domain simulations for reinforcement learning. We also found some studies (7 out of 46 studies) that apply both M2M and M2T transformations in their MDE solutions (P1, P3, P4, P9, P25, P41, and P42). For example, P25 applies M2T transformation to convert a model with ML prediction and dataset details into deep learning code, and M2M transformation to convert models with wireless network plan details into models with component allocation and resource usage. Another example is P35 in which models with big data analytics tasks are converted into ML code and documentation. We also found that all 46 included studies use only forward engineering.\\

\sectopic{Generated Artifacts.} Table \ref{table:genArtifacts} summarises the artifacts generated by the studies. These artifacts include \textit{ML model code or training code} in 36 studies, software or intermediate \textit{models} in 15 studies, \textit{deployment configurations} in 8 studies, and \textit{datasets} or subsets of datasets in 4 studies. The remaining were \textit{text files}, \textit{API code}, \textit{recommendation rules or queries}, and \textit{meta-models} in 2, 2, 2, and 1 studies, respectively. We further examined the languages in which text and code were generated. Our findings are shown in figure \ref{fig:generatedlanguages}, the highest number of studies generated artifacts in \textit{Python} (15 out of 46 studies), \textit{Java} (10 out of 46 studies) and \textit{C++} (4 out of 46 studies). Study P11 generates ML components in Python and C++ using artifact and reference models. \\

\sectopic{Automation Levels.} Figure \ref{fig:AutomationLevels} shows the automation categories supported by the transformations, \textit{fully automated} and \textit{partially automated}. The former category works independently while the latter requires some manual effort. Transformations in the majority of studies (38 out of 46 studies) were fully automated (P1, P2, P4-P10, P12-P18, P20, P22-P25, P27-P37, P40-P44, and P46), whereas, in a few studies (8 out of 46 studies) they were partially automated (P3, P11, P19, P21, P26, P38, P39, P45).\\


\begin{table}[htbp]
\centering
\caption{Generated Artifacts}
\label{table:genArtifacts}
\footnotesize
\begin{tabular}{ p{5cm} p{10.5cm}  } 
\hline
\textbf{Generated Artifacts} & \textbf{Studies} \TBstrut \\
\hline
ML Model/Training code	  \TBstrut  & P1, P2, P3, P5, P7, P8, P9, P11, P12, P13, P14, P16, P17, P19, P20, P21, P22, P23, P24, P25, P27, P28, P30, P31, P32, P35, P36, P37, P38, P39, P41, P42, P43, P44, P45, P46\TBstrut \\ 
Model	                     & P1, P4, P6, P7, P9, P14, P18, P20, P25, P28, P29, P33, P40, P41, P42 \TBstrut \\
Deployment configurations    &	P3, P6, P13, P16, P26, P31, P34, P37\TBstrut \\ 
Dataset	                     & P31, P36, P39, P45\TBstrut \\ 
Text files	                 & P10, P35 \TBstrut \\ 
Recommendation rules/Queries & P15, P38 \TBstrut \\ 
API code	                 &  P4, P8\TBstrut \\ 
Meta-model	                 & P26\TBstrut \\ 
\hline
\end{tabular}

\end{table}

\sectopic{Tool Availability.} We searched primary studies for details of developed tool development details, with our findings shown in Figure \ref{fig:ToolType}. An open-source tool was provided in 17 studies (P1, P2, P4, P8, P10, P13, P14, P16, P17, P22, P23, P32, P34, P35, P39, P42, and P44), a proprietary tool was mentioned in 6 studies (P7, P26, P27, P31, P33, and P45) and no tool was mentioned in 23 studies (P3, P5, P6, P9, P11, P12, P15, P18-P21, P24, P25, P28-P30, P36-P38, P40, P41, P43, and P46).\\


\sectopic{Meta Tools and Frameworks.} 
Tools and frameworks that facilitate the development of modeling languages, modeling tools and frameworks are known as \textit{meta} tools and frameworks, for example, the Eclipse Modeling Framework (EMF). Table \ref{table:metaTools} provides an overview of the modeling frameworks, meta tools, and model transformation languages found in the selected studies. Among modeling frameworks, the most frequent was the \textit{Eclipse modeling framework (EMF)} in 15 out of 46 studies. The second most frequent was the \textit{MontiAnna/MontiArc} framework in 4 out of 46 studies. Considering meta tools, \textit{Sirius} was the most common tool found in 10 out of 46 studies. The next most common tools were \textit{MontiAnna/MontiArc} and \textit{Eclipse IDE} both used in 4 studies. For model transformation languages, \textit{XTend} and \textit{Epsilon Generation Language (EGL)} were most common, found in 5 and 4 studies, respectively. The studies absent from the table did not mention the framework, meta-tool, or model transformation language.


\begin{table}[htbp]
\centering
\caption{Modeling Frameworks, Meta tools and Model Transformation Languages}
\footnotesize
        \begin{tabular}{p{2.6cm} p{2.8cm} | p{2.4cm} p{2.1cm} | p{2.5cm} p{1.5cm} }
        \hline
       \TBstrut  \textbf{Modeling Framework} \TBstrut  & \textbf{Studies} & \textbf{Meta Tool} & \textbf{Studies} & \textbf{Model Transformation Language } & \textbf{Studies}   \TBstrut \\
       
            \hline
            \TBstrut Eclipse modeling framework (EMF) \TBstrut	& P1, P2, P4, P6, P7, P9, P14, P22, P27, P28, P30, P32, P33, P34, P37 & Sirius & P1, P7, P9, P14, P25, P27, P30, P34, P37, P38 & XTend & P9, P14, P22, P28, P39\TBstrut \\

             xText \TBstrut	& P9, P22, P27, P28, P39 & Eclipse IDE  & P22, P28, P29, P39  &Epsilon Generation Language (EGL)  & P2, P6, P7, P17   \TBstrut \\
       
           \TBstrut MontiAnna/MontiArc framework \TBstrut	& P5, P8, P11, P12 & MontiArc/MontiAnna & \mbox{P5, P8, P11, P12} & MontiAnna/MontiArc generators  & \mbox{P5, P8, P11,} P12 \TBstrut \\

            \TBstrut PyEcore 	& P25, P38 & Papyrus & P17, P33 &  Acceleo & P1,P27,P34 \\

            Generic Modeling Environment (GME) 	& P13, P36 \TBstrut & Flexmi  & P2, P6 & \mbox{Atlas Transformation} language (ATL) & P25, P29, P30\TBstrut\\


            \TBstrut Meta object facility (MOF) framework  \TBstrut	& P17 &  TouchCore  & P4 & TouchCore & P4 \TBstrut \\


             \TBstrut GreyCat (extension of KMF)	& P23 & IntelliJ IDE  & P23 & Apache Velocity & P23 \TBstrut \\
                 
              Langium  \TBstrut	& P10 & \mbox{Langium Workbench} & P10 & Langium  & P10 \TBstrut \\            SyncMeta \TBstrut	& P24 &  SyncMeta  & P24 & ANTLR & P28 \TBstrut \\
                  
            JastAdd \TBstrut	& P20 & Pyro  & P32 & JastAdd  & P20 \TBstrut \\
             

            \TBstrut i* framework \TBstrut	& P18 & MetaEdit+ & P35 & Xpand language & P30 \TBstrut \\ 

            CINCO framework \TBstrut	& P32 & CINCO Workbench   & P32 & ENLIL & P3 \TBstrut \\
                      

            OPC UA framework \TBstrut	& P45 &  OPC UA modeler & P45 & OPC UA code \mbox{generator} & P45 \TBstrut \\
                   
      KM3 framework \TBstrut	& P29 & DL LDM tool  & P26 &  &  \TBstrut \\
                     
                             \TBstrut	&  & WebGME  & P13 &  &  \TBstrut \\
            \hline
        \end{tabular}
    \label{table:metaTools}
\end{table}


\begin{center}
\begin{myframe}[width=45em,top=5pt,bottom=5pt,left=5pt,right=5pt,arc=10pt,auto outer arc,title=\centering\textbf{RQ2 Answer Summary}]
\footnotesize

A variety of ML aspects have been covered in studies, frequent ones being the design, development, and training of ML components with TensorFlow as the most used ML framework. Over 93\% of studies propose models at the PIM level with most being design models. The majority of studies provide new DSLs with model representation almost equally divided between graphical and textual. In transformations, 91\% of studies apply M2T transformations in their solutions, and 85\% of transformations are fully automated. However, only tools from 50\% of the studies are available. Upon examining the generated artifacts, we found that 78\% of studies generated ML model code or training code, and Python was the preferred choice of programming language. Further, we found EMF for modeling framework, Sirius for meta-tool, and XTend for model transformation language were chosen by most studies.
       
    \end{myframe}
\end{center}

\subsection{RQ3 - MDE4ML Studies Evaluation}
\subsubsection{Target Area} 
We looked at the domain examples and evaluation context in each primary study - this can be \textit{academia}, \textit{industry}, or both. We classify studies as academia if examples and evaluations occur in controlled environments like labs and as industry when conducted in real-world settings. Figure \ref{fig:TargetAreas} shows the distribution of target areas of studies. A large portion of studies (89\%) were from an academic context (P1-P11, P13, P14, P16-P30, P32, P34, and P36-P46) and a small portion (9\%) were from an industrial context (P12, P15, P31, P33). Interestingly, one study, P35 with domain-specific visual languages for Big Data Analytics and ML had evaluations in academic and industrial contexts. 

\subsubsection{Evaluation Methods} 
We analyzed the evaluation methods in studies and categorized them under the following five categories: \textit{case study}, \textit{experiment}, \textit{survey}, \textit{criteria-based assessment}; \textit{no evaluation}. 
We borrow our categorization of empirical studies from Wohlin et al.~\cite{wohlin2012experimentation}. We note that for case studies, we classify them otherwise if the primary study terms their evaluation method as a case study, albeit it does not match the SE definition~\cite{wohlin2012experimentation}. We also added a new category here ``criteria-based assessment'' where the primary studies are compared against certain well-established guidelines/criteria in the area.
%
Figure \ref{fig:evalMethods} shows the majority of MDE solutions (23 out of 46 studies) were evaluated on case studies (P1-P3, P9, P11-P15, P17, P19, P20, P22, P23, P27, P29, P30, P33, P35, P36, P39, P40, and P45). Out of these, a few (4 of the 23) were industrial case studies (P12, P15, P33, and P35). The second most common evaluation method in studies (17 out of a total 46 studies) was experiments (P4, P7, P10, P16, P22, P24, P25, P28, P31, P34, P35, P37, P38, P41, P42, P44, and P46). Among these, only one was an industrial experiment study (P31), and four were `user studies' (P10, P22, P24, and P35). We note that the user studies also included a post-experiment interview or questionnaire (survey) with the participants; however, the experiment was the main method of evaluation.
The criteria-based assessment was done in two studies (P28 and P35). For example, P35 performed an evaluation against the `physics of notation' guidelines in their study. 
Three primary studies (P22, P28, and P35) used multiple methods to evaluate their proposed MDE solution. For example, P22 evaluated their work with an IoT case study and a user study with four volunteers. No evaluation was found in eight studies (P5, P6, P8, P18, P21, P26, P32 and P43). 



\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Images/RQ3/target_area.png}
    \caption{Target Area of Studies}
    \label{fig:TargetAreas}
    \vspace*{-1em}
    \end{subfigure}
\hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/RQ3/evaluationtype.png}
    \caption{Evaluation Methods in Studies}
    \label{fig:evalMethods}
    \vspace*{-1em}
\end{subfigure}
\hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/RQ3/tool_type.png}
    \caption{Types of Tools in Studies}
    \label{fig:ToolType}
    \vspace*{-1em}
\end{subfigure}
    \vspace*{1em}
 \caption{Evaluation and Tools in Studies}
\end{figure}

\subsubsection{Evaluation Metrics and Datasets} 

Evaluation metrics in studies are divided into metrics for ML and metrics for MDE, as shown in Figure \ref{fig:MLmetrics} and Figure \ref{fig:MDEmetrics}. The category \textit{not mentioned} includes studies that perform evaluation but do not mention the metrics. We found  10 studies (P14, P15, P20, P24, P29, P30, P33, P35, P36, and P40) that do not mention any ML metrics and 18 studies (P1, P3, P7, P9, P11, P13-P15, P20, P22, P23, P27, P29, P34, P36, P39, P42, and P46) that do not mention any MDE metrics. The \textit{not applicable (N/A)} category contains studies that either have no evaluation or the solution cannot be evaluated through such metrics. For instance, P10 provides a DSL to model dataset descriptions and then transform the models into HTML documents. Since this study has no relevant ML evaluation, we categorized it as N/A in Figure \ref{fig:MLmetrics}. There are ten studies (P5, P6, P8, P10, P18, P21, P26, P32, and P43) for which ML metrics are not applicable and eight studies (P5, P6, P8, P18, P21, P26, P32, and P43) for which MDE metrics are not applicable.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{Images/RQ3/MLmetrics.jpg}
    \caption{ML Evaluation Metrics in Studies}
          \begin{center}{\footnotesize A) Classification B) Regression C) Time and Resource D) Fairness E) No metrics} \end{center}
    \label{fig:MLmetrics}
    \vspace*{-0.3em}

    \end{subfigure}
\hfill
    \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/RQ3/MDEmetrics.jpg}
    \caption{MDE Evaluation Metrics}
     \begin{center}{ \footnotesize A) Quality B) Time and Resource C) Code metrics D) No metrics}\end{center}
    \label{fig:MDEmetrics}
\end{subfigure}
\caption{Evaluation Metrics}
\end{figure}

We examined the primary studies and found ML evaluation metrics related to \textit{classification}, \textit{regression}, \textit{time and resource}, and \textit{fairness}. Among all studies, classification metrics were used the most, with the frequently occurring ones being \textit{accuracy} of the ML component in 16 studies' evaluation (P1, P2, P7, P9, P11, P13, P16, P17, P22, P23, P27, P34, P37-P39, and P44) and \textit{precision} of the ML component in 7 studies' evaluation (P1, P4, P9, P13, P22, P27, and P44). Other classification evaluation metrics include \textit{recall} in six studies (P1, P3, P4, P9, P22, and P27), \textit{f measure} in five studies (P13, P22, P27, P34, and P38) and \textit{area under ROC curve} in one study (P16). The second highest evaluation metrics were time and resource metrics, with \textit{execution time} in eight studies (P13, P19, P23, P25, P41, P42, P44, and P46) and \textit{training time} in six studies (P9, P12, P22, P37, P38, and P44). Other time and resource evaluation metrics such as \textit{resource usage} were found in four studies (P19, P25, P42, and P44), \textit{execution latency} in one study (P13) and \textit{inference time} in one study (P44). In comparison, regression and fairness metrics were found in much fewer studies' evaluations. Among evaluation metrics for regression tasks, \textit{loss} was found in three studies (P1, P13, and P39), \textit{root mean square error} in two studies (P31 and P45), \textit{mean absolute error} in two studies (P12 and P31), \textit{mean relative error} in one study (P37), \textit{prediction error} in one study (P31),  \textit{relative absolute error} in one study (P45), and \textit{R\textsuperscript{2} score} in one study (P13). Among evaluation metrics for fairness tasks, \textit{mean difference} and \textit{average odds difference} were both found in one study (P2).

MDE metrics found in the primary studies were related to \textit{quality}, \textit{time and resource}, and \textit{code}. Quality metrics were often used to evaluate MDE approaches, especially \textit{productivity increase} when developing an ML solution found in six studies (P25, P30, P31, P37, P38, and P41), and \textit{usability} of the MDE approach found in three studies (P10, P24, and P35).  Other quality-related evaluation metrics such as \textit{scalability} were found in two studies (P19 and P44), \textit{learnability} in one study (P35), \textit{desirability} in one study (P35), \textit{completeness} in one study (P33), \textit{effectiveness} in one study (P30), \textit{correctness} in one study (P4), \textit{expressiveness} in one study (P10), \textit{usefulness} in one study (P10), \textit{reduced complexity}	in one study (P10), \textit{generated code quality} in one study (P17), and \textit{flexibility} in one study (P19). The second highest was time and resource metrics, with \textit{generation time} and \textit{modeling time} used the most in five studies (P2, P25, P37, P38, and P45) and two studies (P25 and P45), respectively. Other time and resource-related evaluation metrics found in the primary studies were \textit{execution time} in one study (P2) and \textit{re-training time reduction} in one study (P12). Code-related evaluation metrics were found in relatively fewer studies. Among these metrics, we found \textit{lines of code} in four studies (P2, P28, P30, and P40), \textit{number of words} in one study (P28), \textit{number of characters} in one study (P28), and \textit{number of generated pipelines} in one study (P16).

During the analysis of the evaluations described in the primary studies, 33 datasets were identified. The datasets most frequently used in the evaluations were the \textit{MNIST} handwritten digits dataset (7 out of 46 studies) and the \textit{Iris} flowers dataset (3 out of 46 studies) -- we note that both these datasets are widely used for ML classification.

\begin{center}
\begin{myframe}[width=45em,top=5pt,bottom=5pt,left=5pt,right=5pt,arc=10pt,auto outer arc,title=\centering\textbf{RQ3 Answer Summary}]
\footnotesize
Out of 46 primary studies, only five evaluate their MDE solution in an industrial context while the remaining studies provide examples or evaluations in academic contexts. The evaluation method identified in 23 out of 46 studies is a case study. The occurrence of other evaluation methods such as experiments and user studies was relatively low. Analysis of ML and MDE evaluation metrics shows that MDE metrics were mentioned in a few studies or MDE aspects were evaluated in a few studies. The dataset most often used for evaluations in the primary studies was MNIST. The results from the studies show that MDE approaches for ML are seldom evaluated in industrial contexts and during evaluation more emphasis is placed on ML aspects.
    \end{myframe}
\end{center}

\subsection{RQ4 - Limitations and future work of existing MDE4ML studies}

\subsubsection{Limitations in the Primary Studies}
In our analysis of the selected primary studies we found several limitations and we classified these into three high-level categories: approach, evaluation, and solution quality. Among the selected 46 studies, the following 19 studies, P3, P5, P7, P8, P11, P16, P18, P26, P27, P29, P31, P33, P37, P40-P44, and P46, did not mention any limitations.\\

\sectopic{Limitations in Approach.} A key limitation found in the approach taken by several of the selected primary studies is the manual effort required to configure the generated artifacts (studies P19, P20, P22, P24, P36, and P39). Whereas considerable manual effort goes into modeling for P15 and code generator implementation for P21. Limited ML models are supported in studies P7, P22, P23, P30, P33, P34, and P37. This may restrict the applicability of these approaches to a broader range of ML techniques. P4 has the limitation that a single error in the model can defeat the purpose of the entire solution. The approach in P21 is not generic and is only relevant to one ML framework (Infer.NET).  

\sectopic{Limitations in Evaluation.} We discovered evaluation-related limitations in several primary studies. However, only a few of them were actually discussed in the studies. P17, P28, P30 mention the absence of a user study to evaluate the approach with real-world users. This raises concerns about the practical usability of the approach, as user feedback is never obtained. Another limitation found in two studies (P39 and P40) was the absence of an industrial evaluation. This signifies an important gap regarding the application and usefulness of MDE solutions for ML-based systems in the industry. Interestingly, among the studies that do provide an evaluation, ten (10) studies (P9, P12, P14, P19, P20, P23, P24, P38, P42 and P45) describe their evaluations as being limited to simple scenarios or single case studies. Furthermore, P18 highlights the lack of an evaluation of the proposed solution.

\sectopic{Limitations in Quality.} The quality of a proposed solution is of high importance; while analyzing the studies we found quality limitations related to scalability and accessibility. P14 and P39 propose approaches that are difficult to scale, making them less suitable for large-scale applications. The study P35 describes accessibility issues during evaluation due to the MetaEdit+ meta-tool. These issues include the lack of a web interface and the need for a tool license. 

\subsubsection{Future Work Suggested in the Primary Studies.} 

While analyzing our selected primary studies, we were interested in examining the key future work and challenges described. We have classified these future works into three high-level categories related to enhancements in the approach, solution quality, and evaluation. P5, P11, P13, P26, P29, P41, and P44 do not mention any future work.


\sectopic{Improvement or Extension of Approach.} Many kinds of improvements to the approach or extensions to add new features have been grouped under this category. The addition of new features and improvements like a recommender and new modeling concepts were proposed in many studies, such as P6, P9, P10, P12, P14-P16, P18-P20, P22, P24, P25, P28, P34-P36, P39, and P42. For example, the authors of P9 intend to add support for future collaborative training of ML models. Studies P1, P21, P27, P32, and P35 plan to support additional platforms like embedded systems and websites to make their solutions compatible with a wider range of platforms.  Similarly, in P7, P8, P22, and P36 it is suggested that the code generators will support more programming languages. Currently, limited ML models are supported in P7, P22, P23, P30, P33, P34, P37, and P45; future work involves supporting a wider range of ML models to enhance system capabilities. In P3, P6, P15, and P36, the future goal involves considering more complex scenarios in the approach. Some studies like P9, P13, P14, P17, P22, and P34 aim to add training data processing and preparation as a part of their solutions. A future goal described in P16 is the development of a DSL, whereas the authors of P14 and P27 suggest the creation of a textual DSL to support the existing graphical one. P15 states tool implementation of their approach as a future work, although we note that 50\% of studies do not specify if they have developed a tool for their approach.
%50\% of the studies do not specify a tool developed for their solution, and only one study, P15, states tool implementation as a future work.\\

\sectopic{Further Evaluation.} As discussed in the primary study limitations section, the evaluations performed in many of the selected primary studies have limitations. To address this gap, thirteen studies P9, P12, P14, P17, P19, P20, P23, P24, P38-P40, P42, and P45, intend to perform additional evaluations. For example, in P23 multiple use cases for ML models were planned to be evaluated on high-power computers and P40 considered industrial evaluation an important next step. Surprisingly, from the eight studies that do not provide any sort of evaluation, only P18 states evaluation as a future task. Similarly, out of the 42 studies that do not conduct a user study, only three studies, P17, P28, and P30, report it as work to be done in the future. 

\sectopic{Quality Enhancement.} A small portion of the primary studies mention quality enhancements as a future target. The two more frequent quality improvements include integration with other languages and tools (P1, P8, P24, P35, P43) and interoperability support (P24, P32, P45). For instance, in P1 a future goal is to integrate with EAST-ADL an architectural language for automotive embedded systems. Other enhancements mentioned are optimization of generated code, in P2 and P33, optimal resource allocation in P37, support for model checking in P32, and improved scalability, reusability, and adaptability in P40, P20, and P25 respectively. \\

\begin{center}
\begin{myframe}[width=45em,top=5pt,bottom=5pt,left=5pt,right=5pt,arc=10pt,auto outer arc,title=\centering\textbf{RQ4 Answer Summary}]
\footnotesize
The key limitations in the selected primary studies are related to the evaluation with over 88\% of studies having no industrial evaluation and user study. Furthermore, 48\% of studies evaluate only one aspect of their approach (either MDE or ML), and 17\% of studies do not provide any kind of evaluation. In terms of solution quality, some limitations mentioned in the selected primary studies are relevant to scalability and accessibility. For future work, the majority of papers (46\%) state additional features and enhancements in the proposed approach along with further evaluations (28\%).
    \end{myframe}
\end{center}
\clearpage




