\section{Research Methodology}~\label{sec:Method}
We conducted a Systematic Literature Review (SLR) on Model-driven Engineering (MDE) approaches for systems with machine learning (ML) components (MDE4ML). This review aims to analyze existing primary studies and synthesize significant findings to guide future research and practice. This literature review has been conducted in conformance with the systematic literature review guidelines for SE presented by Kitchenham et al. \cite{kitchenham2009systematic, kitchenham2007guidelines}. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.875\textwidth]{Images/SLR_Process_Diagram.png}
    \caption{Systematic Literature Review Process}
    \label{fig:SLR process}
    \vspace*{-1em}
\end{figure*}

Figure~\ref{fig:SLR process} provides an overview of our review process. We divide the work into three stages: planning, conducting, and reporting. During the \textit{planning} stage, we identified the need for this SLR, formulated research questions (RQs), and defined the SLR protocol. %The protocol was reviewed by the other authors all highly experienced in conducting SLRs in the domain of software engineering. 
In the \textit{conducting} stage, all authors collaborated to formulate search strings and select databases. The first author searched all selected databases and removed duplicate studies to create an initial pool of papers. Over multiple iterations, the first author filtered studies based on predefined criteria. Cross-validation was used to check searches and selection and to resolve ambiguities with the other authors. Forward and backward snowballing was used to identify other highly relevant primary studies. This resulted in 46 primary studies for analysis. Data extraction and synthesis activities for the final 46 papers, listed in Appendix A, were performed by the first author under the close supervision of the other authors. In the final stage \textit{reporting}, we documented all the significant findings and analyzed threats to the validity of the SLR.

\subsection{Research Questions}
The objectives of this study were to identify the motivations, solutions, evaluation techniques, and limitations of MDE4ML. Following the PICOC (population, intervention, comparison, outcomes, and context) approach, we developed four research questions (RQs)~\cite{petticrew2008systematic}. The PICOC for RQs in this SLR is shown in Table~\ref{table:picoc}.

\begin{table}[htbp]
\centering
\caption{PICOC for Research Questions}
\label{table:picoc}
\footnotesize
\begin{tabular}{ p{2cm} p{13cm}} 
 \hline
 \textbf{Population} & Systems with machine learning (ML) components \TBstrut \\ 
 \textbf{Intervention} & Model-driven engineering (MDE) approaches for systems with ML components  \TBstrut \\
 \textbf{Comparison} & Not applicable  \TBstrut \\
 \textbf{Outcomes} & The consequence of using MDE for systems with ML components \TBstrut \\
 \textbf{Context} & Include: MDE approaches for systems with ML components \TBstrut \\ & Exclude: AI approaches automating, recommending, or enhancing the MDE process, MDE approaches for systems with AI components
other than ML, pre-deployment Model-based testing approaches for ML systems \TBstrut \\ 
\hline
\end{tabular}
\end{table}

\par\textbf{RQ1. What is the motivation behind applying MDE approaches to systems with ML components?} -- RQ1 explores the key motivations and goals for applying MDE for ML-based systems in our selected primary studies. We further looked into other relevant aspects, such as the ML techniques, application domain, end users, and outcomes. 

\par\textbf{RQ2. Which MDE approaches and tools are presented in the literature for systems with ML components?} -- RQ2 examines the MDE solutions presented in the literature for ML-based systems. We identified and classified the modeling languages applied, transformations and automation levels supported, and the artifacts generated. We also explore the MDE tools available and the underlying meta-tools and frameworks. From the ML perspective, we looked at the ML phases, training data, and ML libraries considered in the primary studies.

\par\textbf{RQ3. How are existing studies on MDE4ML evaluated?} -- RQ3 investigates the evaluation techniques, metrics, datasets, and settings applied in the selected primary studies. 

\par\textbf{RQ4. What are the limitations and future work of existing studies on MDE4ML?} -- RQ4 identifies the limitations of current studies and key future research challenges of MDE4ML. In this context, we further examine the studies in terms of limitations in their approach, evaluation, and solution quality. 

\subsection{Study Selection}
Our database search results contain a number of irrelevant studies. Therefore, we developed detailed selection criteria as part of our SLR protocol. These criteria are divided into inclusion and exclusion criteria (shown in Table \ref{table:inclusion}) that we used to filter primary studies. For this review, we only consider primary English-language studies that focus on MDE for ML-based systems. The studies must be academic and have their full text available online. We excluded any irrelevant papers or did not have enough information to extract, such as vision papers, posters, magazine articles, keynotes, opinion papers, and experience papers. This SLR focuses on MDE4ML approaches, so we did not include any AI4MDE papers. These criteria were applied to all papers to select the most relevant ones, and discussions between all authors resolved any ambiguities in the study filtering process.

\begin{table}[htbp]
\centering
\caption{Inclusion and Exclusion Criteria}
\label{table:inclusion}
\footnotesize
\begin{tabular}{ p{2cm} p{13cm}} 
\hline
\textbf{Criteria ID} & \textbf{Criterion} \TBstrut \\
\hline
I01 & Papers about MDE for systems with ML components \TBstrut \\ 
I02 & Full text of the article is available  \TBstrut \\
I03 & Peer-reviewed studies that have been used in academia with references from literature  \TBstrut \\
I04 & Papers written in English language  \TBstrut \\
\hline
\hline

E01 & Papers about ML (or its subsets) that do not use MDE \TBstrut \\ 
E02 & Papers about MDE for any subset of AI other than ML  \TBstrut \\
E03 & Papers about pre-deployment model-based testing of systems with ML components.  \TBstrut \\
E04 & Studies leveraging AI to improve, enhance, or automate MDE \TBstrut \\
E05 & Short papers that are less than four pages \TBstrut \\
E06 & Conference or workshop papers if an extended journal version of the same paper exists \TBstrut \\
E07 & Papers with inadequate information to extract \TBstrut \\
E08 & Non-primary studies (Secondary or Tertiary Studies) \TBstrut \\
E09 & Vision papers and grey literature (unpublished work), books (chapters), posters, discussions, opinions, keynotes, magazine articles, opinion, experience and comparison papers \TBstrut \\
\hline
\end{tabular}
\end{table}

\subsection{Search Strategy}
Figure \ref{fig:searchProcess} provides an overview of our search process, with all the steps detailed below.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Images/SLR_Study_selection.png}
    \caption{Study Search Process}
    \label{fig:searchProcess}
    \vspace*{-1em}
\end{figure*}

\subsubsection{Search String Formulation}
For automated search in online databases, a search string was defined using key terms from the PICOC in Table \ref{table:picoc} and alternative terms. We realize that model-based software engineering and low code/no code are not the same as MDE; however, we included them in the search string since they are sometimes used interchangeably in the literature. This string was tested on seven online databases, IEEE Xplore, ACM Digital Library, Springer, Wiley, Scopus, Web of Science, and Science Direct, and refined over several iterations to get the most relevant results. Each time the search was executed, the first author randomly selected a sample of 5-7 papers from the search results and skimmed through them to ensure relevance and further refine the string. We combine search string keywords in the same category using the OR operator and keywords in different categories using the AND operator. For example, all ML keywords are joined by the OR operator, whereas the AND operator joins the ML and MDE keywords. The final search string below was slightly modified for some databases to get the best results. 
 
\subsubsection{Automated Search and Duplicate Removal}
The automated search was conducted in March 2023. The final search string was executed on the selected online database search engines to extract an initial pool of 3,934 papers. The search was restricted to academic articles, including journals, conference papers, and workshop papers, with no time range specified. We used the default search for all databases except Scopus, where we performed an advanced search on titles, abstracts, and keywords. The list of resultant papers was downloaded, and a Python script was developed to remove duplicates. The script also removed conference or workshop papers from the list if a journal version was found with the same title and authors. After duplicate removal, we were left with 3,570 papers for further screening.

\subsubsection{Filtering Studies}
We filtered the 3,570 potentially relevant papers in three iterations by applying the inclusion and exclusion criteria. The first screening based on title and abstract yielded 72 potentially relevant papers. In the second screening, we skimmed through the entire paper and further narrowed it down to 55 papers. The third and final screening was done during data extraction, resulting in 32 highly relevant papers. During the filtering process, we maintained a list of all papers on \textit{Google Sheets} and color-coded them on relevance, i.e., green for `relevant', yellow for `might be relevant', and red for `irrelevant'. The color codes helped authors focus and classify the papers that needed further attention. 

\begin{center}
\begin{myframe}[width=45em,top=10pt,bottom=10pt,left=10pt,right=10pt,arc=10pt,auto outer arc,title=\centering\textbf{Search String}]
\footnotesize
\begin{center}
("machine learning" OR "supervised learning" OR "unsupervised learning" OR "semi-supervised
learning" OR "reinforcement learning" OR "deep learning" OR "ensemble learning" OR "neural network" OR
"naive bayes" OR "decision tree" OR "deep boltzmann machine" OR "deep belief network" OR "convolutional
neural network" OR "recurrent neural network" OR "generative adversarial network" OR "auto encoder"
OR "gradient boosted regression trees" OR "adaboost" OR "gradient boosting machine" OR "random forest"
OR "perceptron" OR "back propagation" OR "hopfield network" OR "radial basis function network" OR
"linear regression" OR "stepwise regression" OR "logistic regression" OR "least square regression" OR
"adaptive regression" OR "locally estimated scatterplot" OR "k means clustering" OR "k medians clustering"
OR "hierarchical clustering" OR "mean shift clustering" OR "expectation maximization" OR "gaussian naïve
bayes" OR "multinomial naïve bayes" OR "bayesian network" OR "bayesian belief network" OR "k nearest
neighbour" OR "learning vector quantization" OR "self organizing map" OR "locally weighted learning" OR
"transfer learning" OR "support vector machines" OR "classification and regression tree" OR "CHAID" OR
"conditional decision tree" OR "decision stump" OR "long short term memory network" OR "gaussian
mixture" OR "hidden markov model" OR "Q learning" OR "temporal difference learning" OR "dimensionality
reduction") \\ AND \\ ("model driven development" OR "model driven engineering" OR "model based software
engineering" OR "model driven software engineering" OR "model transformation" OR "model driven
architecture" OR "low code solution" OR "low code application" OR "low code applications" OR "low code
paradigm" OR "low code approach" OR "low code platform" OR "low code development" OR "low code/no
code" OR "no code/low code" OR "no code development platform")
        \end{center}
    \end{myframe}
\end{center}


\subsubsection{Snowballing}
To account for any missed papers in the automated search process, we performed a manual search following the snowballing guidelines by Wohlin~\cite{wohlin2014guidelines}. The 32 selected papers and related work papers were snowballed in forward and backward directions over three iterations until no new suitable papers were found. We gathered eight (8) papers through our forward snowballing and six (6) papers through our backward snowballing.

\subsection{Data Extraction and Synthesis}
We created a Google Form with 40 questions corresponding to our four RQs to ensure all required data was extracted from the papers. This form was divided into five sections: the first section for general information and publication trends, including title, authors, publication venue, and citation count; the second section for motivations, goals, application domain, and users; the third section for MDE approaches for ML-based systems; the fourth section for evaluation techniques and tools and the last section for limitation and future challenges. The answer options for the questions in the form consisted of 23 short answers, ten long answers, two checkboxes and 14 radio buttons. For quality assessment, we ran pilot tests; the first author extracted data for six papers and compared it with data extracted by the other authors for the same papers. A close match was found between both, after which the first author extracted data for all the remaining papers. During pilot tests, small updates were made to improve the Google Form, for example, adding check boxes for common answers and improving the structure of the question. From the data extraction, we gathered qualitative and quantitative data for the next stage of data synthesis. The first author performed synthesis using various graphs, figures, and tables under the guidance of the other authors. 

\subsection{Quality Assessment}
We devised a five-point scoring system to assess the quality of the selected primary studies to answer five predefined questions. The scores range from very poor~(1) and inadequate~(2) to moderate~(3), good~(4), and excellent~(5). Such QA scoring for evaluating the quality of primary studies is commonplace in SLRs~\cite{shamsujjoha2021developing, hidellaarachchi2021effects}. The quality assessment questions we used are based on Kitchenham's guidelines ~\cite{kitchenham2007guidelines} and are shown below: \\
\textbf{QA1:} Are the aims clearly stated? \\
\textbf{QA2:} Is the solution clearly defined?\\
\textbf{QA3:} Are the measures used clearly defined? \\
\textbf{QA4:} Does the report have implications for practice? \\
\textbf{QA5:} Does the report discuss how the results add to the literature? \\

\begin{table}[htb]
    \centering
        \caption{Quality Assessment of Selected Primary Studies}
\footnotesize
    \begin{tabular}{cccccc|cccccc}
    \hline
      \textbf{ID}&\textbf{QA1} &\textbf{QA2} & \textbf{QA3}  & \textbf{QA4}  & \textbf{QA5}  &\textbf{ID}  & \textbf{QA1}  & \textbf{QA2} & \textbf{QA3}  & \textbf{QA4}  & \textbf{QA5}  \\
    \hline
    
       P1  & 5 & 5 & 3 & 4 & 5 & P24 & 4 & 2 & 1 & 2 & 2 \\
       P2  & 5 & 5 & 3 & 5 & 5 & P25 & 5 & 5 & 1 & 5 & 5 \\
       P3  & 5 & 4 & 5 & 3 & 3 & P26 & 2 & 3 & NA & NA & NA \\
       P4  & 5 & 5 & 4 & 3 & 5 & P27 & 3 & 4 & 5 & 3 & 1 \\
       P5  & 5 & 5 & NA & NA & NA & P28 & 4 & 4 & 2 & 3 & 1\\
       P6  & 5 & 3 & NA & NA & NA & P29 & 3 & 2 & 1 & 2 & 1\\
       P7  & 3 & 3 & 1 & 2 & 1 & P30 & 5 & 5 & 5 & 4 & 3 \\
       P8  & 5 & 5 & NA & NA & NA & P31 & 5 & 5 & 5 & 5 & 5\\
       P9  & 4 & 4 & 1 & 4 & 4 & P32 & 4 & 3 & 1 & 1 & 1 \\
       P10  & 5 & 5 & 2 & 5 & 5 & P33 & 5 & 4 & 4 & 2 & 2 \\
       P11  & 5 & 4 & 1 & 4 & 2 & P34 & 5 & 4 & 3 & 4 & 1\\
       P12  & 5 & 4 & 1 & 4 & 2 & P35 & 4 & 5 & 2 & 5 & 5\\
       P13  & 5 & 3 & 1 & 4 & 3 & P36 & 4 & 3 & 1 & 2 & 1\\
       P14  & 5 & 3 & 1 & 3 & 3 & P37 & 5 & 4 & 1 & 3 & 1 \\
       P15  & 3 & 3 & 1 & 1 & 2 & P38 & 4 & 3 & 1 & 2 & 1 \\
       P16  & 4 & 4 & 1 & 4 & 5 & P39 & 5 & 5 & 5 & 1 & 4\\
       P17  & 4 & 3 & 1 & 3 & 5 & P40 & 4 & 3 & 1 & 3 & 1 \\
       P18  & 4 & 3 & NA & NA & NA & P41 & 4 & 3 & 1 & 4 & 4 \\
       P19  & 4 & 3 & 4 & 3 & 3 & P42 & 5 & 3 & 1 & 3 & 4\\
       P20  & 5 & 4 & 1 & 4 & 4 & P43 & 2 & 2 & NA  & NA & NA\\
       P21  & 3 & 3 & NA & NA & NA & P44 & 5 & 4 & 1 & 4 & 4 \\
       P22  & 5 & 5 & 4 & 5 & 5 & P45 & 4 & 4 & 1 & 3 & 3 \\
       P23  & 5 & 5 & 1 & 5 & 4 & P46 & 3 & 4 & 1 & 3 & 1 \\
    \hline
    \end{tabular}
    \label{tab:quality}
\end{table}

The results of our quality assessment are captured in Table \ref{tab:quality}. Questions QA3-QA5 are only applicable to studies that provide an evaluation and are marked as not applicable (NA) for all other studies without an explicit evaluation component. From our quality assessment, we found that 19/46 included papers were of good quality, 15/46 were of average quality, and 12/46 were poor quality. We did not exclude any papers since MDE4ML is an emerging research area and our selection of 46 studies was already limited. Furthermore, we aimed to minimize any publication bias in our research.