\section{The Proposed Framework}
\label{method}
In this section, we first introduce our proposed light-weight backbone RFDNet in Section~\ref{sec:DS-SqueezeNet}. To enrich features, we present a multi-RPN to combine different level feature maps, as introduced in Section~\ref{MRPN}. Then, we propose MLPS score maps and RoI pooling to better use the multi-level feature maps, as introduced in Section~\ref{MLPS}. The detailed architecture of the proposed framework is depicted in Fig.~\ref{fig:pipeline}. The proposed framework takes an image as input, generates hundreds of fault region proposals via multi-RPN from RFDNet, and then scores each proposal using MLPS score maps and RoI pooling.

\subsection{Light-weight Backbone}
\label{sec:DS-SqueezeNet}
In original R-FCN, the backbone (\textit{i.e.} ResNet~\cite{He2016Deep}) needs a large number of parameters and floating point operations (FLOPs) to achieve a satisfactory accuracy, thus requiring a huge amount of computations. However, as discussed in the previous section, fault detection task cannot meet the demand of tremendous computing power in the wild. Compared to the ResNet, a light-weight network usually has fewer parameters and lower computations with approximate precision, which is more suitable for real-time fault detection. SqueezeNet~\cite{iandola2016squeezenet} is an efficient network which uses a bottleneck approach to design a small-size network. Nevertheless, there is still a large accuracy gap between these networks and those of full-sized counterparts for detection~\cite{yuxi2018tinydsod}. For traditional SqueezeNet, it is a challenge to increase accuracy while reducing computing cost. In Fig.~\ref{fig:pipeline}, the core of SqueezeNet is Fire module which consists of a squeeze layer and an expand layer. The expand layer contains two layers: 3$\times$3 convolutional layer and 1$\times$1 convolutional layer. However, it is unreliable to preset the kernel numbers of 1$\times$1 and 3$\times$3 convolutional layers in expand layers. As an alternative, we remove the 1$\times$1 convolutional and retain the 3$\times$3 convolutional, and the network is adjusted to be a streamline.
In general, we define a $K_c\times K_c\times P\times Q$ convolutional (Conv.) kernel $K(\cdot)$, where $K_c$ is the spatial dimension of the kernel assumed to be square. \textit{P} is the number of input channels, and \textit{Q} is the number of output channels. The kernel $K(\cdot)$ slides on an input feature map $F(\cdot)$ to extract output features maps $G(\cdot)$ as follows~\cite{sandler2018inverted}:
\begin{equation}
G_{m, n, q}=\sum_{i,j,p}K_{i, j, p, q}\cdot F_{m+i-1, n+j-1, p} \;.
\end{equation}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.03}
\centering
\caption{RFDNet detailed architecture}
\begin{tabular}{l|l|l}
\toprule
Layer name & Type / Stride & Filter shape \\
\midrule
Conv 1                                                   &Conv / s2  &3$\times$3$\times$3$\times$64   \\
\hline
MP 1                                             &MaxPooling / s2  &Pool 3$\times$3    \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 2 / 3 \end{tabular}}}    &Conv / s1   & 1$\times$1$\times$64$\times$64   \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$64 dw         \\ \cline{2-3}
           &Conv / s1    & 1$\times$1$\times$64$\times$64  \\
\hline
MP 3                                             &MaxPooling / s2  & Pool 3$\times$3    \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 4 \end{tabular}}}    &Conv / s1    & 1$\times$1$\times$64$\times$128  \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$128 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$128$\times$128   \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 5 \end{tabular}}}    &Conv / s1    & 1$\times$1$\times$128$\times$128  \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$128 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$128$\times$128   \\
\hline
MP 5                                             &MaxPooling / s2  & Pool 3$\times$3    \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 6 \end{tabular}}} &Conv / s1  & 1$\times$1$\times$128$\times$256      \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$256 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$256$\times$256   \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 7 \end{tabular}}} &Conv / s1  & 1$\times$1$\times$256$\times$256      \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$256 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$256$\times$256   \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 8 \end{tabular}}}  &Conv / s1  & 1$\times$1$\times$256$\times$512     \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$512 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$512$\times$512   \\
\hline
\multirow{3}{*}{{\begin{tabular}[l]{@{}c@{}} DSF 9 \end{tabular}}}  &Conv / s1  & 1$\times$1$\times$512$\times$512     \\ \cline{2-3}
           &Dw-Conv / s1 & 3$\times$3$\times$512 dw         \\ \cline{2-3}
           &Conv / s1  & 1$\times$1$\times$512$\times$512   \\
\hline
Conv 10                                                  &Conv / s1  & 1$\times$1$\times$512$\times$1000   \\
\hline
Avgpooling                                              &Average Pooling  / s1  & Pool 14$\times$14  \\
\hline
SoftmaxWithLoss                                         &Softmax / s1  & Classifier  \\
\bottomrule
\end{tabular}
\label{Dsqunetarchitecture}
\end{table}

Moreover, depthwise separable Conv.~\cite{sandler2018inverted} has shown computing efficiency in generic image classification tasks, drastically reducing computational cost and model size. In RFDNet, we use depthwise separable Conv. to improve the performance of Fire module, called as depthwise separable Fire (DSF) module. The proposed DSF contains a depthwise Conv. (Dw-Conv) and a pointwise Conv. layer. We use 3$\times$3 Dw-Conv to replace original expand layers of each Fire module in SqueezeNet. The Dw-Conv~\cite{sandler2018inverted} is defined as:
\begin{equation}
\hat{G}_{m, n, p}=\sum_{i,j} \hat{K}_{i, j, p}\cdot F_{m+i-1, n+j-1, p} \;,
\end{equation}
where $\hat{K}(\cdot)$ is the Dw-Conv kernel of size $K_c\times K_c\times P$, and $\hat{G}(\cdot)$ is the filtered output feature map. Such an approach achieves 8$\times$ less computation than standard Conv.~\cite{sandler2018inverted}. Pointwise Conv., namely a simple 1$\times$1 Conv., is then applied to create a linear combination of the output of depthwise layer. Both batch normalization (BN)~\cite{Ioffe2015Batch} and rectified linear unit (ReLU) nonlinearities are used for all layers in the DSF. The detailed architecture can be found in Table~\ref{Dsqunetarchitecture}. The RFDNet\footnote{For real-time fault detection, we remove the average pooling and the final layer, and only use the Conv.1 layer and DSF2$\sim$9 as the backbone.} begins with a standalone Conv. layer (Conv1) and eight DSF modules (DSF2$\sim$9), followed by a global average pooling, ending with a 1000-d 1$\times$1 Conv. layer.

To verify the advantage of our DSF intuitively, we calculate the average feature maps over all channels from Fire3, Fire5, and Fire7 in SqueezeNet, and the corresponding DSF3, DSF5, and DSF7 in our RFDNet, respectively. The visualization comparisons of extracted feature maps are demonstrated in Fig.~\ref{fig:RFDNet_vis}. The results indicate that DSF produces more salient features, while Fire misses some valuable information. In addition, the feature maps from DSF remain more textural property in the low-level layer. In this case, our DSF can show better fine-grained object details. Furthermore, our proposed DSF can capture more semantic cues in the high-level layer. The effectiveness of our DSF in RFDNet will be further described in the following Section~\ref{analysisframework}.

\begin{figure}[!t]
	\centering
    \includegraphics[width=3.3in]{Figures/Figure3.jpg}
	\caption{Visualization comparison of average feature maps extracted by Fire modules in SqueezeNet (top) and by the corresponding DSF modules in RFDNet (bottom). The average feature maps over all channels from Fire3, Fire5, and Fire7 are shown from left to right, respectively. The average feature maps over all channels from DSF3, DSF5, and DSF7 are shown from left to right, respectively. DSF modules remain more textural property in the low-level layer which present better fine-grained object details. DSF modules also capture more semantic cues in the high-level layer.}
	\label{fig:RFDNet_vis}
\end{figure}

Besides the rich salient features, our proposed RFDNet can generate feature description which is invariant to illumination. Aiming at the main variation in illumination for freight train images, its robustness can be proved by feature maps extracted from different layers in this paper. In Fig.~\ref{fig:conv}, we firstly obtain the images under different illumination intensity. The feature maps are then extracted from different intermediate layers (MP1, MP3 and MP5) of RFDNet. We can observe that feature maps derived from different inputs are similar at the same stage of networks. This means that RFDNet is robust to the change of illumination for freight train images. We attribute this success to abundant input data and self-learning capacity of RFDNet, which can automatically learn to obtain better feature maps. Therefore, the introduction of RFDNet is advisable, because the weather and sunlight would make a great difference in light intensity for freight train images, which is common in practice. %Moreover, some modules in CNNs such as batch normalization (BN)~\cite{Ioffe2015Batch} can enhance generalization ability of network by normalizing samples to a certain distribution range.

\begin{figure}[!t]
 \centering
 \subfloat[]{
 \includegraphics[width=1.05in]{Figures/Figure4a.jpg}}
 \subfloat[]{
 \includegraphics[width=1.05in]{Figures/Figure4b.jpg}}
 \subfloat[]{
 \includegraphics[width=1.05in]{Figures/Figure4c.jpg}}
 \caption{Illustration of RFDNet for freight train images in varying light conditions. (a) Input image in low illumination with average feature maps of MP1, MP3 and MP5 from RFDNet. (b) Normal illumination image with the corresponding average feature maps. (c) High illumination image with the corresponding average feature maps. The feature maps derived from different inputs are similar at the same stage of networks, which means that RFDNet is robust to the change of illumination for fault detection.}
 \label{fig:conv}
\end{figure}

\subsection{Multi-scale Feature Utilization}
Lower-level to higher-level layers in CNNs usually possess diverse distinguishing features for different size of objects. It can be seen from Figs.~\ref{fig:RFDNet_vis} and~\ref{fig:conv} that lower-level layers with higher resolution can capture more fine-grained information, which is helpful for recognizing small objects. Higher-level layers are more sensitive to semantic cues than lower-level layers. Therefore, multi-scale features can better represent all objects by incorporating multiple spatial resolutions in images. For freight train images, the detected parts have a range in size so that a single feature map cannot support for a satisfactory detection performance. So, we apply a multi-scale feature to produce more powerful feature maps of fault region, which can help to detect different size of objects.

\subsubsection{Multi-RPN}
\label{MRPN}
The function of RPN is to quickly select some candidate regions for target objects, which can greatly decrease the computation burden for inference process. A set of rectangular object proposals are usually generated by a fully Conv. network on feature maps. How to build an accurate RPN is important for two-stage detectors, and one potential way to improve its performance is employing multi-scale features.

We propose a novel RPN using a multi-scale feature fusion (MFF) block (see details of MFF\_1 in Fig.~\ref{fig:pipeline}) to apply multi-scale sliding windows over multi-level DSFs, which associates a set of prior anchors with each sliding position to generate fault region proposals. Specially, according to the size of fault regions, we use a 3$\times$3 sliding window who carries 9 anchors with 3 scales and 3 aspect ratios over the MFF\_1 block to produce multiple spatial features. To adjust multi-level feature maps to the same resolution for combination, different DSFs are processed by different sampling strategies. For DSF4, a 2$\times$2 max pooling layer is added to carry out subsampling. Then, we use 192-d 1$\times$1 Conv. to extract local feature over the above processed DSF4, DSF7, and DSF9, respectively. We normalize multiple feature maps using BN and then concatenate them. We encode the above concatenated feature maps using a 512-d 3$\times$3 Conv. layer which not only extracts more semantic features but also compresses them into a uniform space. The 512-d feature is then entered into two output layers: a classification layer that predicts the score of fault region, and a regression layer that refines the location for each prior anchor.
We define a bounding box as $t=(t_x,t_y,t_w,t_h)$ with the score $s$, and our loss function defined on each RoI is the summation of cross-entropy loss $L_{cls}$ and box regression loss $L_{reg}$~\cite{RenHGS15}:
\begin{equation}
\begin{split}
L(s,t_{x,y,w,h})&=L_{cls}(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*) \\
&=-log(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*),
\end{split}
\end{equation}
where $c^*$ denotes the ground-truth label of a RoI, and $t^*$ is the ground-truth bounding box. $\lambda$ is a balance weight which is set as 1. $[c^*>0]$ is an indicator that equals to 1 if the argument is true and 0 otherwise. Besides that, all local features are pre-computed before multi-RPN and detection without redundant computation~\cite{8911418}. The effectiveness of multi-RPN will be further described in the following Section~\ref{analysisframework}. %In addition, we apply a linear nonmaximum suppression (NMS)~\cite{BodlaSCD17} with a threshold 0.7 to rapidly suppress the lower score boxes and retain the highest scoring anchor in the neighborhood.

\subsubsection{MLPS}
\label{MLPS}
To better use the multi-level features and enrich the different information of each anchor, we perform position-sensitive RoI pooling over MLPS score maps. Before encoding position information into each RoI, we use another MFF block (see details of MFF\_2 in Fig.~\ref{fig:pipeline}) and encode the concatenated feature with a 512-d 1$\times$1 Conv. layer to combine the multi-level features. We then attach a 256-d 1$\times$1 Conv. layer for reducing dimension.
After that, the multi-level weighted fusion feature is accessed to produce $k^2$ position-sensitive score maps for each of the $C$ categories ($k$ is set to 7 in practice~\cite{RenHGS15}), correspondingly all RoIs also are evenly divided into $k^2$ grid areas. The MLPS scores vote on the RoI by averaging the scores, which is MLPS RoI pooling that can be denoted as:
\begin{equation}
P_{c}|\mathcal{A}_{(m,n)}=\sum_{i=1}^{N} \frac{1}{N}p_{(i)}^{(c)}|L_{m,n,c} \quad p\in \mathcal{A}_{(m,n)},
\end{equation}
where the $\mathcal{A}_{(m,n)}$ is an area within $k^2$ grids in each RoI, and it represents the location for specified area (0$\leq$ $m,n$ $\leq$  $k-1$). $P_{c}|\mathcal{A}_{(m,n)}$ is the pooling result for category $C$ at $\mathcal{A}_{(m,n)}$, and $p$ is the pixel in $\mathcal{A}_{(m,n)}$. $N$ denotes total number of pixels in $\mathcal{A}_{(m,n)}$, while $L_{m,n,c}$ is one of score maps that corresponds to $\mathcal{A}_{(m,n)}$ in $k^2$ score maps for $C$.

Finally, a ($C+1$)-d vector is produced for classification, and an average vote is used over the vector as follows
\begin{equation}
P_{c}= \sum _{m,n} P_{c}|\mathcal{A}_{(m,n)},
\end{equation}
where $P_{c}$ is the final score for category $C$, and we then calculate the softmax responses across categories:
\begin{equation}
s_c=\frac{e^{P_{c}}}{\sum_{c'=0}^{C} e^{P_{c'}}}.
\end{equation}
These are used for computing the cross-entropy loss $L_{cls}$ during training and for ranking the RoIs during inference.

Aiming at achieving bounding box regression, a sibling 4$k^2$-d Conv. layer is then appended for bounding box regression. The MLPS RoI pooling is performed on this bank of 4$k^2$ maps as well. Then, it is aggregated into a 4-d vector by average voting which is used to parameterize a bounding box. There is no learnable layer after the RoI, enabling nearly cost-free region-wise computation and speeding up both training and inference~\cite{RenHGS15}. The visualization results of the multi-level feature concatenation are demonstrated in Fig.~\ref{fig:vis_MLPS}. The average feature maps of DSF9 are extremely scarce for different freight train images, which only contain semantic cues with low resolution. The MLPS score maps and RoI Pooling will be unreliable to detect faults only based on the feature maps processed by DSF9. Nevertheless, the multi-level fusion feature has rich object characteristic such as shape and contour, which is helpful to improve the detection accuracy. The applicability of MLPS score maps and RoI Pooling will be further described in the following Section~\ref{analysisframework}.

\begin{figure}[!t]
	\centering
    \subfloat[]{\includegraphics[width=1.05in]{Figures/Figure5a.jpg}}\hspace{0.1em}
    \subfloat[]{\includegraphics[width=1.05in]{Figures/Figure5b.jpg}}\hspace{0.1em}
    \subfloat[]{\includegraphics[width=1.05in]{Figures/Figure5c.jpg}}
	\caption{Visualization results of the multi-level feature concatenation. (a) Input images; (b) Average feature maps extracted by DSF9; (c) Average feature maps concatenated by DSF4, DSF7, and DSF9. The multi-level fusion feature has richer object characteristic than a single, such as shape and contour.}
	\label{fig:vis_MLPS}
\end{figure} 