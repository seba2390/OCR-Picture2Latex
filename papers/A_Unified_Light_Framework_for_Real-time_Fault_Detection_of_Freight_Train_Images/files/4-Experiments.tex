\section{Experiments and Analysis}
\label{experiment}
In this section, we evaluate the effectiveness of our framework on the problem of real-time fault detection for freight train images. To this end, we firstly evaluate our proposed light-weight backbone on three datasets, ImageNet ILSVRC 2012~\cite{SimonyanZ14a}, PASCAL visual object classes (VOC) 2007~\cite{EveringhamEGWWZ15} and MS COCO~\cite{2014Microsoft}. Then we compare the proposed framework with state-of-the-art fault detectors and well-known object detection methods on six fault datasets~\cite{zhang2018,8911418}. We conduct all of our experiments using Caffe~\cite{JiaSDKLGGD14} on a single NVIDIA GeForce GTX1080Ti GPU.
%\begin{table}[!t]
%\renewcommand{\arraystretch}{1.1}
%\centering
%\caption{The datasets of fault detection for freight train images}
%\begin{tabular}{lcccc}
%\toprule
%\multirow{2}{*}{Datasets}  & \multirow{2}{*}{Training set} & \multicolumn{3}{c}{Testing set}
%\\ \cline{3-5}
%                    &              & Non-fault   & Fault & Total \\
%\midrule
%Angle cock          & 2002         & 1049        & 975   & 2024  \\
%Bogie block key     & 5440         & 2530        & 367   & 2897  \\
%Brake shoe key      & 5636         & 2000        & 2000  & 4000  \\
%Cut-out cock        & 815          & 671         & 179   & 850   \\
%Dust collector      & 815          & 798         & 52    & 850   \\
%Fastening bolt      & 1724         & 445         & 1257  & 1902  \\
%\bottomrule
%\end{tabular}
%\label{database}
%\end{table}

\subsection{Experimental Setup}
\subsubsection{Implementation Details}
In ImageNet experiments, to make a fair comparison, all the hyper-parameters follow SqueezeNet~\cite{iandola2016squeezenet}. We use BN after each Conv. layer before ReLU activation, and the initial learning rate is set to 0.04. We use the polynomial decay learning rate scheduling strategy in the batch size of 32. The momentum and weight decay are set as 0.9 and 0.0002, respectively. Finally, we use the validation set of ImageNet ILSVRC 2012 to validate our backbone.

In PASCAL VOC experiments, we use the same hyper-parameters as SqueezeNet to make a fair comparison. Based on a pre-trained model from the ImageNet experiments, we fine-tune the resulting model using RMSProp with 0.0001 initial learning rate, 0.9 momentum, and 0.0005 weight decay. We set 120K training steps and execute multi-scale training in the batch size of 64. We use the step decay learning rate scheduling strategy and multiply with a factor 0.1 at the 20K, 50K, and 100K steps, respectively. Finally, the VOC 2007 test set is used to verify our RFDNet following the protocol in~\cite{RenHGS15}.

In MS COCO experiments, we also use the same hyper-parameters as SqueezeNet for fair comparison. Based on a pre-trained model from the ImageNet experiments, we fine-tune the resulting model using SGD with 0.001 initial learning rate, 0.9 momentum, and 0.0005 weight decay. We set 480K training steps and execute multi-scale training in the batch size of 56. We use the step decay learning rate scheduling strategy and multiply with a factor 0.1 at the 280K, and 360K steps, respectively. Finally, the COCO minival set is adopted to evaluate our backbone following the standard protocol.

In fault detection experiments, our method is trained via back-propagation and stochastic gradient descent (SGD). We use a basic learning rate of 0.001 and it is divided by 10 for each 40K mini-batch until convergence. The batch sizes of  multi-RPN and MLPS RoI are 256 and 512, respectively. A pre-trained RFDNet model for ImageNet is first used to initialize shared Conv. layers of our backbone network, and then the new layers are initialized with a zero mean and a standard deviation of 0.01 Gaussian distribution. We train the network with 70K iterations in total. The momentum and weight decay are set as 0.9 and 0.0005, respectively. The confidence score in the detecting stage is 0.9.

\subsubsection{Fault Datasets} To evaluate the performance of our method, six fault datasets~\cite{zhang2018,8911418} for freight train images are directly used in this study, including angle cock, bogie block key, brake shoe key, cut-out cock, dust collector, and fastening bolt on brake beam. Some typical samples of freight train images are shown in Fig~\ref{fig:detection}(b).
\begin{itemize}
  \item \textbf{Angle cock} is a key component of the air brake system of freight trains, and its role is to ensure the smooth flow of air in the main pipeline. For this dataset, training and evaluation are performed on the 2002 images in the trainval and the 2024 images in the test, respectively.
  \item \textbf{Bogie block key} is a very small part used to prevent the wheel set from getting out of the bogie. This dataset is divided into two sets, training and testing with 5440 and 2897 images, respectively.
  \item \textbf{Brake shoe key} is also a small component equipped in brake shoe, which is vital for safe operation of braking system. The dataset provides more than 5600 images for training, and 4000 images for its test set.
  \item \textbf{Cut-out cock} is a key part that cuts off the air from main reservoir to the brake pipe, which is used to shut down the brake pipe. The images are divided into a train set of 815 images and a test set of 850 images.
  \item \textbf{Dust collector} is usually installed next to the cut-out cock and its role is to filter impurities towards compressed air. So, the images in this dataset are annotated directly on the images in cut-out cock dataset.
  \item\textbf{Fastening bolt} is an important part for train brake. When the train brakes, the fastening bolts may break or fall off because of a large horizontal force generated from brake beam. There are 1724 images in the train set and another 1902 images in the test set.
\end{itemize}

\begin{table}[!t]
\renewcommand{\arraystretch}{1}
\centering
\caption{Classification results on ImageNet ILSVRC 2012}
\begin{tabular}{lcccc}
\toprule
 \multirow{2}{*}{{\begin{tabular}[l]{@{}c@{}} Model \end{tabular}}}  & \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Computational \\ cost (FLOPs) \end{tabular}}}
& \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Model size\\(Parameters)\end{tabular}}} & \multirow{2}{*}{Top-1} & \multirow{2}{*}{Top-5}\\
&&&&\\
\midrule
SqueezeNet                   & 833M  & 4.8MB  & 57.5\% & 80.3\% \\
RFDNet                       & 580M  & 7.1MB  & 64.4\% & 85.8\% \\
\bottomrule
\end{tabular}
\label{imagenet}
\end{table}

\begin{table}[!t]
\renewcommand{\arraystretch}{1}
\centering
\caption{Detection results on PASCAL VOC 2007 dataset. The “07+12” means VOC07 trainval union with VOC12 trainval}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{{\begin{tabular}[l]{@{}c@{}} Model \end{tabular}}}  & \multirow{2}{*}{{\begin{tabular}[l]{@{}c@{}} Training \\ data \end{tabular}}}  & \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Input \\dimension \end{tabular}}}
& \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Model size\\(Parameters)\end{tabular}}} & \multirow{2}{*}{{\begin{tabular}[l]{@{}c@{}} mAP \end{tabular}}} \\
&&&& \\
\midrule
SqueezeNet-SSD  & 07+12  & 300$\times$300  & 21.1MB  & 64.3 \\
RFDNet-SSD      & 07+12  & 300$\times$300  & 17.2MB  & 70.1 \\
\bottomrule
\end{tabular}
\label{voc2007}
\end{table}

\begin{table}[!t]
\renewcommand{\arraystretch}{1}
\centering
\caption{Detection results on MS COCO dataset}
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{{\begin{tabular}[l]{@{}c@{}} Model \end{tabular}}}     & \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Input \\dimension \end{tabular}}}
& \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Model size\\(Parameters)\end{tabular}}} & \multicolumn{3}{c}{Avg. Precision, IoU:} \\
&&&0.5:0.95&0.5&0.75 \\
\midrule
SqueezeNet-SSD    & 300$\times$300  & 55.4MB  & 8.4  & 15.2 & 8.2\\
RFDNet-SSD        & 300$\times$300  & 40.1MB  & 11.7 & 19.7 & 12.1\\
\bottomrule
\end{tabular}}
\label{coco2015}
\end{table}

\begin{table}[!t]
	\renewcommand{\arraystretch}{1}
	\centering
	\caption{Detection results of different DSFs on six datasets}
	\begin{tabular}{lcccc}
		\toprule
		Modules  & Width & mCDR/\%$\uparrow$      & mMDR/\%$\downarrow$     & mFDR/\%$\downarrow$ \\
		\midrule
		DSF9      &512$\times$1   & 98.09  & 1.26   & 0.65  \\
		DSF(4,9)    &256$\times$2   & 98.39  & 0.92   & 0.69  \\
		DSF(5,9)    &256$\times$2   & 94.13  & 4.27   & 1.60  \\
		DSF(6,9)    &256$\times$2   & 96.64  & 2.88   & 0.48  \\
		DSF(7,9)    &256$\times$2   & 97.90  & 1.64   & 1.46  \\
		DSF(8,9)    &256$\times$2   & 98.37  & 1.40   & 0.23  \\
		DSF(4,6,9)  &192$\times$3   & 97.99  & 1.64   & 0.37  \\
		DSF(4,7,9)  &192$\times$3   & 98.60  & 0.94   & 0.46  \\
		DSF(4,8,9)  &192$\times$3   & 98.51  & 0.86   & 0.63  \\
		DSF(5,6,9)  &192$\times$3   & 89.90  & 5.22   & 4.88  \\
		DSF(5,7,9)  &192$\times$3   & 95.15  & 3.44   & 1.41  \\
		\bottomrule
	\end{tabular}
	\label{diffmod}
\end{table}

\begin{table*}[!t]
	\renewcommand{\arraystretch}{1}
	\centering
	\caption{Detection results of connecting different modules, including SqueezeNet, RFDNet, MRPN, and MLPS}
	\setlength{\tabcolsep}{2.9mm}{
		\begin{tabular}{cccccccccccc}
			\toprule
			\multirow{2}{*}{SqueezeNet} & \multirow{2}{*}{RFDNet} & \multirow{2}{*}{MRPN} & \multirow{2}{*}{MLPS}    & \multirow{2}{*}{mCDR/\%$\uparrow$}  & \multirow{2}{*}{mMDR/\%$\downarrow$} & \multirow{2}{*}{mFDR/\%$\downarrow$} &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Training \\ speed/s\end{tabular}}}   &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Testing \\ speed/s\end{tabular}}}   &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Memory \\ usage/MB\end{tabular}}}  &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Model \\size/MB\end{tabular}}} \\
			& & & & & & & & & &  \\
			\midrule
			$\surd$ &--      &--      &--       & 97.12 & 1.15 & 1.73  & 0.085 & 0.026 & 745 & 20.7  \\
			--      &$\surd$ &--      &--       & 98.09 & 1.26 & 0.65  & 0.105 & 0.024 & 683 & 13.8  \\
			$\surd$ &--      &$\surd$ &--       & 97.90 & 0.91 & 1.19  & 0.114 & 0.027 & 770 & 21.6  \\
			--      &$\surd$ &$\surd$ &--       & 98.45 & 1.14 & 0.41  & 0.126 & 0.025 & 698 & 17.4  \\
			$\surd$ &--      &$\surd$ &$\surd$  & 98.36 & 0.70 & 0.94  & 0.118 & 0.028 & 795 & 25.1  \\
			--      &$\surd$ &$\surd$ &$\surd$  & 98.60 & 0.94 & 0.46  & 0.135 & 0.026 & 713 & 19.6  \\
			\bottomrule
	\end{tabular}}
	\label{modules}
\end{table*}


\begin{table*}[!t]
	\renewcommand{\arraystretch}{1}
	\caption{Detection results of six typical faults in comparison with state-of-the-art methods}
	\centering
	\setlength{\tabcolsep}{3.5mm}{
		%\resizebox{\textwidth}{!}{
		\begin{tabular}{lccccccccccccccc}
			\toprule
			\multirow{2}{*}{Methods}    & \multirow{2}{*}{mCDR/\%$\uparrow$}  & \multirow{2}{*}{mMDR/\%$\downarrow$} & \multirow{2}{*}{mFDR/\%$\downarrow$} &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Training \\ speed/s\end{tabular}}}   &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Testing \\ speed/s\end{tabular}}}   &
			\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Batch \\ size\end{tabular}}} &
            \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Model\\size/MB\end{tabular}}}&
            \multicolumn{2}{c}{\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Memory \\ usage/MB\end{tabular}}}} \\
			& & & & & & & &\\
			\midrule
			Cascade detector(LBP)       & 87.55   & 6.33   & 6.12   & --    & 0.048 & --  & 0.12  & \multicolumn{2}{c}{--}   \\
			HOG+Adaboost+SVM            & 93.32   & 3.25   & 3.43   & --    & 0.049 & --  & 0.11  & \multicolumn{2}{c}{--}   \\
			FAMRF+EHF                   & 94.96   & 1.00   & 4.04   & --    & 0.725 & --  & --    & \multicolumn{2}{c}{--}   \\
			\midrule
			SSD(VGG16)                  & 96.32   & 0.88   & 2.80   & 0.747 & 0.047 & 16  & 95.5  & \multicolumn{2}{c}{1173} \\
			YOLOv3                      & 88.85   & 2.58   & 8.57   & 3.537 & 0.026 & 64  & 246.3 & \multicolumn{2}{c}{1501} \\
			RefineDet(VGG16)            & 96.06   & 0.74   & 3.20   & 1.742 & 0.056 & 16  & 135.8 & \multicolumn{2}{c}{1415} \\
			RON(VGG16)                  & 98.15   & 0.47   & 1.38   & 0.892 & 0.029 & 32  & 157.9 & \multicolumn{2}{c}{1143} \\
			DSOD(DenseNet)              & 95.62   & 2.13   & 2.25   & 0.517 & 0.109 & 2   & 50.8  & \multicolumn{2}{c}{4429} \\
			\midrule
			MLKP(VGG16)                 & 98.21   & 0.68   & 1.11   & 0.722 & 0.147 & 128 & 596.1 & \multicolumn{2}{c}{3711} \\
			Faster R-CNN(VGG16)         & 98.19   & 0.96   & 0.85   & 0.289 & 0.065 & 128 & 546.8 & \multicolumn{2}{c}{1817} \\
			%CoupleNet(ResNet101)        & 97.51   & 0.22   & 2.27   & 0.851 & 0.112 & 128 & 409.6 & \multicolumn{2}{c}{3443} \\
			R-FCN(ResNet101)            & 94.68   & 1.71   & 3.61   & 0.524 & 0.096 & 128 & 199.9 & \multicolumn{2}{c}{3114} \\
			FTI-FDet(VGG16)             & 99.41   & 0.37   & 0.22   & 0.336 & 0.071 & 128 & 557.3 & \multicolumn{2}{c}{1823} \\
			Light FTI-FDet(VGG16)       & 99.22   & 0.32   & 0.46   & 0.318 & 0.058 & 128 & 89.7  & \multicolumn{2}{c}{1533} \\
            Cascade R-CNN(ResNet101)    & 97.34   & 0.96   & 1.70   & 0.615 & 0.203 & 2   & 220.8 & \multicolumn{2}{c}{3818} \\
			\midrule
			MobileNetV2-SSD             & 97.97   & 0.58   & 1.45   & 0.561 & 0.034 & 8   & 15.2  & \multicolumn{2}{c}{1343} \\
			MobileNetV2-SSDLite         & 94.65   & 0.29   & 5.06   & 0.101 & 0.018 & 16  & 12.3  & \multicolumn{2}{c}{827}  \\
			ShuffleNetV2-SSD            & 96.24   & 0.51   & 3.25   & 0.254 & 0.028 & 16  & 11.8  & \multicolumn{2}{c}{850}  \\
			Tiny-DSOD                   & 95.74   & 0.31   & 3.95   & 0.467 & 0.057 & 4   & 3.5   & \multicolumn{2}{c}{1469} \\
			Pelee(PeleeNet)             & 96.34   & 0.92   & 2.74   & 0.757 & 0.051 & 16  & 20.2  & \multicolumn{2}{c}{1412} \\
			\midrule
            Light FTI-FDet(RFDNet)      & 98.17   & 0.94   & 0.89   & 0.178 & 0.034 & 128 & 27.8  & \multicolumn{2}{c}{857}  \\
			RFDNet-SSD                  & 97.98   & 0.32   & 1.70   & 0.809 & 0.036 & 24  & 11.4  & \multicolumn{2}{c}{905}  \\
			LR FTI-FDet(RFDNet)         & 98.60   & 0.94   & 0.46   & 0.135 & 0.026 & 256 & 19.6  & \multicolumn{2}{c}{713}  \\
			\bottomrule
	\end{tabular}}
	\label{meanaccuray}
\end{table*}


\subsubsection{Evaluation Metrics} There are seven indexes: correct detection rate (CDR), missing detection rate (MDR), false detection rate (FDR), training speed, testing speed, test memory usage and model size (parameters) to evaluate the effectiveness of fault detectors. The indexes of CDR, MDR, and FDR are all used to measure the accuracy of detectors, which are calculated based on the method directly from~\cite{8911418}. For example, there is a test set which contains $m$ fault images and $n$ normal (non-fault) images, through the work of the detector, $a$ images are detected as fault, among them $b$ images are detected by error, meanwhile, $c$ images are detected as normal, among them $d$ images are detected by error. In this case, the indexes will be defined as:
\begin{equation}
  CDR = \frac{a+c}{m+n},\ MDR = \frac{b}{m+n},\ FDR = \frac{d}{m+n}.
\end{equation}
The mean value of CDRs, MDRs, and FDRs are calculated as mCDR, mMDR, and mFDR respectively to indicate the accuracy of fault detection for different datasets. Both model size and accuracy report the impact of CNN architectural designs~\cite{iandola2016squeezenet} on fault detectors. Both memory usage and training/testing speed reflect the dependence of detectors on hardware. Especially, we use the computational time for each iteration in training and testing phase for each image as training and testing speed, respectively. Memory usage is collected from a detector's memory usage on a single GPU in the testing phase.

\subsection{Performance Analysis}
\label{analysisframework}

\subsubsection{Backbone}
To verify the effectiveness of our RFDNet, we give a detailed discussion on the performance of RFDNet in comparison with the baseline light-weight network SqueezeNet. It can be seen from Table~\ref{imagenet} that RFDNet achieves a baseline of 64.4\% top-1 and 85.8\% top-5 accuracy on ImageNet, which is 6.9\% and 5.5\% higher than SqueezeNet with 1.4$\times$ less computation at the same size. The proposed RFDNet can also be deployed as an effective base network in object detection. We then perform experiments on VOC 2007 and MS COCO for detailed analysis of our RFDNet based on the SSD. Detection accuracy is measured by mean Average Precision (mAP) with 300 input resolutions. The experimental results on VOC2007 test set are summarized in Table~\ref{voc2007}. Our RFDNet achieves 70.1\% mAP, and its accuracy is higher than that of SqueezeNet by 5.8\% at only 81.2\% of model size. Moreover, the results on COCO minival set are summarized in Table~\ref{coco2015}. Our proposed RFDNet achieves 19.7\%/12.1\% with 0.5/0.75 IoU, which outperforms the SqueezeNet with a large margin. We observe that our [0.5:0.95] result is 3.3\% higher than the SqueezeNet at 72.4\% of model size. This indicates that our predicted locations are more accurate than the SqueezeNet with lower computational cost.

\subsubsection{Multi-scale Feature Utilization}
An important property of our method is that it combines coarse-to-fine information across deep CNN models. As an example, we compare different Conv. feature maps on six datasets to illustrate the superiority of the proposed multi-scale feature utilization (MFF\_1 and MFF\_2). Table~\ref{diffmod} shows the detection performance for connecting different DSF modules. “DSF(4,9)" means connecting DSF4 and DSF9 in both multi-RPN and MLPS score maps. “192$\times$3" means that we apply the 192-d 1$\times$1 Conv. layer on each of three DSF modules, respectively. In Table~\ref{diffmod}, the combination of DSF4, DSF7, and DSF9 works the best. The results indicate that the multi-layer combination performs roughly better than a single layer, and further verify the effectiveness of low-to-high combination strategy.

\subsubsection{Different Modules}
We analyze RFDNet, multi-RPN, and MLPS score maps by conducting experiments on six datasets. With the aforementioned computer, we only change the configuration of modules for a fair comparison. In Table~\ref{modules}, our RFDNet has higher accuracy and less computation than SqueezeNet. The combination of three modules in our framework can achieve the best performance. The index of mCDR significantly improves from 97.12\% to 98.60\%, and the testing speed is 0.026s. The results reveal that both MRPN and MLPS score maps can improve detection performance with few redundant computations. These two modules are able to learn more effective and comprehensive features than a single DSF for distinguishing faults from complex backgrounds.

\subsection{Comparison with State-of-the-art Methods}
%To illustrate the superiority of our method, we compare our framework called as Light-weight Real-time FTI-FDet (LR FTI-FDet) with traditional detectors (Cascade detector with local binary pattern (LBP)~\cite{Sun2018Railway}, FAMRF + EHF~\cite{Sun2018Railway}, histogram of oriented gradient (HOG) + Adaboost + SVM\footnote{https://github.com/pdollar/toolbox}~\cite{DollarABP14}), one-stage detectors (YOLOv3\footnote{https://pjreddie.com/darknet/}~\cite{redmon2018yolov3}, SSD\footnote{https://github.com/weiliu89/caffe/tree/ssd}~\cite{LiuAESRFB16}, RefineDet\footnote{https://github.com/sfzhang15/RefineDet}~\cite{zhang2018single}, RON\footnote{https://github.com/taokong/RON}~\cite{kong2017ron}, DSOD\footnote{https://github.com/szq0214/DSOD}~\cite{shen2017dsod}), two-stage detectors (Faster R-CNN\footnote{https://github.com/rbgirshick/py-faster-rcnn}~\cite{RenHGS15}, MLKP\footnote{https://github.com/Hwang64/MLKP}~\cite{wang2018multi}, R-FCN\footnote{https://github.com/daijifeng001/R-FCN}~\cite{DaiLHS16}, Cascade R-CNN\footnote{https://github.com/zhaoweicai/cascade-rcnn}~\cite{cai2018cascade}, FTI-FDet~\cite{zhang2018}, Light FTI-FDet\footnote{https://github.com/Yangzhangcst/Light-FTI-FDet}~\cite{8911418}), and light-weight detectors (MobileNetV2-SSD~\cite{sandler2018inverted}, MobileNetV2-SSDLite\footnote{https://github.com/chuanqi305/MobileNetv2-SSDLite}~\cite{sandler2018inverted}, ShuffleNetV2-SSD~\cite{ma2018shufflenet}, Tiny-DSOD\footnote{https://github.com/lyxok1/Tiny-DSOD}~\cite{yuxi2018tinydsod}, Pelee\footnote{https://github.com/Robert-JunWang/Pelee}~\cite{wang2018pelee}).
To illustrate the superiority of our method, we compare our framework called as Light-weight Real-time FTI-FDet (LR FTI-FDet) with traditional detectors (Cascade detector with local binary pattern (LBP)~\cite{Sun2018Railway}, FAMRF + EHF~\cite{Sun2018Railway}, histogram of oriented gradient (HOG) + Adaboost + SVM~\cite{DollarABP14}), one-stage detectors (YOLOv3~\cite{redmon2018yolov3}, SSD~\cite{LiuAESRFB16}, RefineDet~\cite{zhang2018single}, RON~\cite{kong2017ron}, DSOD), two-stage detectors (Faster R-CNN~\cite{RenHGS15}, MLKP~\cite{wang2018multi}, R-FCN~\cite{DaiLHS16}, Cascade R-CNN~\cite{cai2018cascade}, FTI-FDet~\cite{zhang2018}, Light FTI-FDet~\cite{8911418}), and light-weight detectors (MobileNetV2-SSD~\cite{sandler2018inverted}, MobileNetV2-SSDLite~\cite{sandler2018inverted}, ShuffleNetV2-SSD~\cite{ma2018shufflenet}, Tiny-DSOD~\cite{yuxi2018tinydsod}, Pelee~\cite{wang2018pelee}).  In addition, we compare RFDNet-SSD with all above methods to discuss the performance of RFDNet and depthwise separable Conv.-based networks (\textit{e.g.} MobileNetV2) on fault detection. Specially, the related parameters in each detector are tuned to the best performance. %CoupleNet\footnote{https://github.com/tshizys/CoupleNet}~\cite{zhu2017couplenet},

\textbf{Accuracy and model size.}
As shown in Table~\ref{meanaccuray}, LR FTI-FDet achieves 98.60\% mCDR which outperforms RFDNet-SSD, all traditional methods, one-stage, light-weight and most two-stage detectors. The accuracy of both FTI-FDet and Light FTI-FDet are slightly higher than our method, but their model size is too large. Although the model size of each traditional methods is the smallest, but their accuracy is the lowest. The model sizes of our RFDNet-SSD and LR FTI-FDet are 11.4MB and 19.6 MB respectively, which is comparable to light-weight detectors and far less than all one- and two-stage detectors. After replacing backbone (VGG16) with RFDNet in Light FTI-FDet, our method achieves 0.43\% higher mCDR with 1.4$\times$ smaller than the Light FTI-FDet. Especially, the model size of LR FTI-FDet is 28.4/4.6$\times$ smaller than FTI-FDet/Light FTI-FDet with VGG16.
However, our method is unsatisfactory for the robustness of noise and the disturbance from other similar structures without faults. The comparisons between ground-truths and failure examples obtained by our method are shown in Fig.~\ref{QFresults}. We will solve it by expanding the datasets through adding more samples and performing data augmentation in the future. These operations will also improve the generalization ability of our method.

\textbf{Computational cost and speed.}
In Table~\ref{meanaccuray}, both training and testing speeds of our method are faster than traditional methods, one- and two-stage detectors. The testing speed ($>$38 fps) of our LR FTI-FDet is the same as YOLOv3 and 2.7/2.2$\times$ faster than FTI-FDet/Light FTI-FDet with 2.6/2.2$\times$ less memory usage. Our RFDNet-SSD has a comparable performance with MobileNetV2-SSD on fault detection while our method has smaller model size and memory usage. The speed of MobileNetV2-SSDLite is slight faster than our method, but its memory usage is higher. The main reason is that our LR FTI-FDet is a two-stage detector containing RPN and position-sensitive RoI pooling, which needs more computations than a one-stage detector MobileNetV2-SSDLite. But there are efficient DSF modules in RFDNet and many shared layers among RFDNet, multi-RPN, and MLPS score maps, so that the memory usage of our LR FTI-FDet is smaller, which merely needs 713 MB.

The experimental results confirm that our method achieves a much better trade-off between resources and accuracy than the state-of-the-art methods. The experiments on six typical fault datasets also indicate that our method is robust to the illumination variation with high versatility. Therefore, our method is the most suitable for real-time fault detection of freight train images, even though under strict memory and computational budget constraints.

%\begin{figure*}[!t]
%  \centering
%  \includegraphics[width=6.9in]{Figures/Figure4.jpg}
%  \caption{Qualitative results of our method.}
%  \label{QCresults}
%\end{figure*}

\begin{figure}[!t]
  \centering
  \includegraphics[width=3.3in]{Figures/Figure6.jpg}
  \caption{Visualization of ground-truths (top) and failure examples (bottom) obtained by our method. Green bounding boxes mean normal parts, and red bounding boxes are fault areas. Our method is unsatisfactory for the robustness of noise and the disturbance from other similar structures without faults.}
  \label{QFresults}
\end{figure}
