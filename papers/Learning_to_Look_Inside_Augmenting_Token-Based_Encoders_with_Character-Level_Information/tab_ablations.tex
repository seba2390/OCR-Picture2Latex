\begin{table}
    \centering
    \small
    \begin{tabular}{lrrr}
        \toprule
        Ablation & BERT & GPT2 & RoBERTa \\
        \midrule
        Full & 37.17 & 38.27 & 41.97 \\
        No FT & $-$0.31 & $-$0.25 & $-$0.80 \\
        Wiki \ppt{} & $-$3.11 & $-$2.67 & $-$2.17 \\
        % Wiki+Emb loss &  & $-$4.83 &  \\
        No loops & $+$0.82 & $+$0.26 & $-$1.12 \\
        All multi & $-$1.39 & $+$0.44 & $-$1.81 \\	
        \bottomrule
    \end{tabular}
    \caption{Dev set average effect of model variants compared with the \textit{All no-suff} condition: % on a single random seed:
    \say{No-FT} --- pre-trained model used only for feature extraction;
    \say{Wiki} --- \tokdetok{} trained on Wikipedia data instead of Twitter;
    % \say{Emb loss} --- single-token words in downstream data fed for training \tok{};
    \say{No loops} --- trained without the cycle dependency loops;
    \say{All multi} --- common-suffix words also inferred using \tok{}.}
    \label{tab:ablations}
\end{table}

% \begin{table}
%     \centering
%     \small
%     \begin{tabular}{lrr}
%         \toprule
%         Model / Noise & Random case & Repeat \\
%         \midrule
%         Base & 42.73 & 43.57 \\
%         Base + \ppt{} & 43.54 & 45.07 \\
%         Scaffolding & 43.15 & 44.31 \\
%         Stochastic & 42.54 & 43.92 \\
%         All-no-suff & 41.72 & 42.33 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Noising experiments on RoBERTa models on the QA dataset.}
%     \label{tab:ablations}
% \end{table}


