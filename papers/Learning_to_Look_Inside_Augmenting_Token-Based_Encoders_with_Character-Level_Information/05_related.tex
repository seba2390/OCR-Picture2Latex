\section{Related Work}
\label{sec:related}

CharBERT~\cite{ma-etal-2020-charbert} is a method which incorporates character-level encodings in \llm{}s, while re-designing the transformer stack to include a \say{character channel} distinct from the parallel token channel.
Char2Subword~\cite{aguilar2020char2subword} integrates multiple losses in a system trained to predict subword tokens from the character level, all originating in distance metrics between target and prediction.
CharacterBERT~\cite{el-boukkouri-etal-2020-characterbert} uses an ELMo-style character convolutional network to encode input into a transformer MLM.
CANINE~\cite{clark2021canine} is a method for training \llm{}s which removes the need for a subword tokenizer, by utilizing character-level representations which are pooled into inputs for the main transformer module.
ByT5~\cite{xue2021byt5} ventures yet deeper by training a T5-architecture model~\cite{raffel2020exploring} over byte representations, reducing the required embedding table size by one more order of magnitude.
Unlike \tokdetok{}, these systems all require training an \llm{} from scratch and do not support using a \ppt{} step for tuning existing large models.
In addition, with the exception of CANINE, they all resort to softmax prediction over a subword vocabulary for the generative portion of the pre-training phase, and do not offer a character-level decoder which can also be tied to the encoding component.



