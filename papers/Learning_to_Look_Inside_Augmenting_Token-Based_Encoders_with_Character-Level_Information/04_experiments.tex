\section{Experiments}
\label{sec:exp}


We evaluate the effect of \tokdetok{} when included in various \llm{}s. 
We choose \textsc{BERT-base-cased}~\cite{devlin-etal-2019-bert}, \textsc{GPT2-small}~\cite{radford2019language}, and \textsc{RoBERTa-base}~\cite{liu2019roberta} as the base \llm{}s to be manipulated.\footnote{Models loaded from the Huggingface repository~\cite{wolf-etal-2020-transformers}.}
The former contains roughly 108M parameters, and the latter two roughly 125M, a difference accounted for by their larger subword vocabulary (50k vs. 29k) and the resulting larger embedding table.
All models are case sensitive, but they differ in their strategy for preserving the original space-delimited word sequence: BERT's tokenizer marks word-non-initial tokens with a \say{\#\#} string, while GPT-2's tokenizer marks spaces with a special underline character and appends them to the following word.
The difference manifests itself in sequence-initial words, whose initial token in the GPT-2 representation shares the form of sequence-medial word-medial tokens, rather than that of sequence-medial word-initial tokens; and in symbols pre-tokenized without a preceding space, such as punctuation and apostrophes.
RoBERTa's tokenizer adopts the GPT-2 marking strategy but avoids the first pitfall by internally prepending all input sequences with a space character.
We perform the \ppt{} phase for each model on a collection of English tweets from 2016 obtained from the Firehose and preprocessed to replace all `@'-mentions with \texttt{@user}.
We later ablate this domain change effect by training models with \ppt{} text from the English Wikipedia March 2019 dump (see \S\ref{ssec:abl}).
We sample both resources to create pre-training corpora of roughly 725MB (unzipped), several orders of magnitude smaller than what contemporary models use for the first pre-training phase, and train for a single epoch.

In preliminary experiments on several character-level \tok{} architectures, we found that a convolutional net outperforms bidirectional LSTMs and small transformer stacks.
We pass the input characters through three separate convolution layers of width 2, 3, and 4 (characters), then pass the outputs through max-pooling layers and a \texttt{ReLU} activation, and finally project the concatenation of the results onto the base models' embedding dimension.
% \todo{Add figure if feeling masochistic.}
This \tok{} component contains $\sim$1M parameters, negligible compared to the transformers' parameter count.\footnote{The Wikipedia-trained models contain $\sim$2.8M parameters, the difference owing to language-ID filtering performed on the Twitter data, leading to a much smaller character set.} % \detok{}, which uses a 2-layer MLP with hidden layer same-dim as output dim, has 81M params for Wiki and 540K params for Twitter (but we don't use it in our tasks).
% exact numbers: 1071760, 2819360, 81470895, 539539
We implement \detok{} as a 2-layer unidirectional LSTM whose hidden layer is initialized by projecting the context vector $\vh_i$ output from \mmod{} into the hidden dimension.
Characters are generated by projecting the LSTM's output through two linear layers with a \texttt{tanh} activation.

During \ppt{}, we insert a cycle dependency batch every 5,000 LM steps.
For the replacement policies we choose $\pi^t_{(l)}$ to sample uniformly random sets of tokens representing 15\% of space-delimited words to pass as a target loss
for \tok{}, and $\pi^t_{(u)}$ to replace the embedding input to \mmod{} with \tok{}'s output for all multi-token words.
$\pi^g$ selects all tokens to pass as target losses for \detok{}, calculated as a sum of the cross-entropy loss for each character in the target sequence.
Cycle batches consist of sampling $k$ words out of the $K$ most frequent words from the training corpus, with replacement, and $k$ vectors from a Gaussian distribution blown to concentrate around the surface of the unit sphere in hidden-dimension space:
\[ \tilde{\ve} \sim \frac{1}{\sqrt{d}}\cdot\mathcal{N}(0,\vI^{(d)}). \]
\tdloop{} loops are optimized for a
character-level cross-entropy
loss, whereas \dtloop{} loops target a
euclidean distance loss.
When a \tok{} embedding correspond to multiple $\tau$ tokens, the learning target is created by max-pooling their embeddings.\footnote{This $\agg$ function outperformed average-pooling and first-token selection in preliminary experiments.}
We set $K=$25,000 and $k=$1,000.

\input{041_intrinsic}

\subsection{Downstream Evaluation}
\label{ssec:finetune}

During task fine-tuning and inference,
since we do not evaluate on generative tasks, we do not use \detok{}.
We perform minimal hyperparameter search for each base model + task combination, and fine-tune model parameters during downstream training in all tasks but NYTWIT.
For these tasks, we also experimented with setups where \mmod{} and \tok{} are used as feature extractors only, and where \tok{} training is supplemented by an additional embedding loss (as described in \S\ref{ssec:pretr} for the \ppt{} phase) computed against embeddings of the task input.
Neither setup provided improvement on any task during our tuning experiments (see \S\ref{ssec:abl}).

Downstream models are implemented as follows: for sequence classification and ranking, a two-layer perceptron with \texttt{ReLU} activation is trained to make the prediction from the top-layer representation of the initial \texttt{[CLS]} token (in BERT and RoBERTa models) or of the final token in the sequence (in GPT-2).
For NER, an LSTM is run over the sequence of each word's top-layer representation, followed by a single linear layer which makes the prediction.
For NYTWIT, \mmod{}'s contextualized vector for the target word is used as input for a single logistic layer.
In cases of multi-token words, the prediction from the first token is selected.
We set \tok{}'s character embedding dimension to 200 and the convolutional layers to 256 channels.
Following \newcite{sun2019fine}, we set the maximum learning rate to $10^{-3}$ for the task models and $2\times 10^{-5}$ for fine-tuning, and perform warm-up for 10\% of the total expected training steps before linearly decaying the rates to zero.
All parameters are optimized using Adam~\cite{adam} with default settings.
We run all NER models for twenty epochs and sequence-level task models for three, evaluating on the validation set after each epoch using the metrics reported below, and stopping early if performance has not improved for four epochs.
In order to avoid unfairly favorable conditions for \tokdetok{} models, task hyperparameters are all tuned on the base models, with \tokdetok{} models using only values on which their base equivalents have also been evaluated.

We evaluate the effectiveness of \tokdetok{}'s concepts and components by comparing the following setups:\footnote{A setup where all words are represented by \tok{} obtained noncompetitive results on all tasks.}
\begin{itemize}
    \item \textsc{\textbf{None}} uses only the base model;
    \item \textsc{\textbf{None+\ppt{}}} uses a version of the base model that was further pre-trained on the same Twitter corpus on which \tokdetok{} is trained (adaptive fine-tuning~\cite{ruder2021lmfinetuning}), in order to control for the increase in total unlabeled text seen by the model;
    \item \textsc{\textbf{Scaffolding}} is a model trained with \tokdetok{} in a \ppt{} phase, but only using base model embeddings during task fine-tuning;
    \item \textsc{\textbf{Stochastic}} samples 10\% of the words in the downstream datasets, calling \tok{} on their character sequences while using the base model's embedding(s) for the remaining 90\%;
    \item \textsc{\textbf{All no-suff}} calls \tok{} on all multi-token words which are not of the form \texttt{[token suff]}, where \texttt{suff} is a member of the \textsc{Suffixes} set described in \S\ref{sec:model}, and uses the base model's embedding on the rest.
\end{itemize}

\input{tab_results}


We present the results of the downstream prediction tasks in \autoref{tab:main_results}.
All transformer models except for \textsc{None} were 2nd-phase pre-trained three separate times using different random seeds, and the mean results are reported.

The first observation we make is the dominance of RoBERTa, a thoroughly optimized masked language model, over the other models on the NER datasets in all its variants.
This suggests RoBERTa has captured fine-grained information about individual words that it was able to retain in its representations for them; the struggle of the \tokdetok{} model to provide improvement over the \textsc{None} versions of the model strengthens this hypothesis.
GPT-2, despite having access to only left-side context of each word, still outperforms the basic BERT model on most setups in the NER tasks, perhaps due to its larger subword vocabulary size.
At the same time, its gains from the \tok{} representations are much more considerable, suggesting that its left-context-only inference may in fact be detrimental to the its \mmod{}'s performance as a whole.
In general, models perform better on Emerging NER than on Twitter NER, which we attribute to several possible causes or their combination: first, the source shift in the Emerging NER test set from Twitter to Reddit is meant to encumber the models, but given the extensive pre-training they undergo they may actually benefit from the fact that test sequences are longer on average than those in the Twitter NER dataset;
second, more prosaically, the Emerging NER dataset contains fewer entity types (6 vs. 10), making the task itself somewhat easier.

The other word-level task, NYTWIT, demonstrates substantial gains made by the \tokdetok{} training regime: the \textsc{Scaffolding} setup preforms best in all three base models, and in both masked langugage models the \textsc{Stochastic} setup outperforms both \textsc{None} variants.
Together with the NER results, this suggests that \tokdetok{} succeeds in providing transformer models with \textbf{word-level} representations that allow coarse-grained classifications (such as named entity type or novel word origin), better than default subword segmentations, for both edited and user-generated text.

We find that \tokdetok{} is less successful in improving sequence classification and ranking performance than on word-level tasks.
The \textsc{None+\ppt} setup obtains the best results in most models on the Emoji and QA datasets, suggesting the improvements seen in \tokdetok{}-based models is mostly attributable to the domain shift introduced by the Twitter pre-training corpus.\footnote{We note the complete inconsistency in both model performance and comparative model ranking present in the Emoji dataset, which calls back the systemic issue we identified in~\S\ref{sec:tasks}:
The time span over which the test set was collected contained a fundamental shift in Twitter's properties --- doubling the tweet length limit from 140 to 280 characters --- and so exhibits an unpredictable corpus incompatible with the train and dev partitions.
As a result, model performance over the dev set does not predict the test set results (a large difference on performance in this task between dev and test sets was also observed in macro-F1 scores by~\newcite{barbieri-etal-2020-tweeteval}).
In fact, under such specific data shift circumstances, it could be the case that the simpler base model is better equipped for facing longer test data, which scales generalizations made over the training and dev sets, as opposed to \tok{}-augmented models which have more levels of generalization to acquire during training and cannot anticipate the scale change.
Post-hoc inspection of specific model outputs resulted in some more concrete hypotheses for the cause of discrepancy in the major categories of confusion, for example the distributions of \say{@} presence in heart emoji
%{\NotoEmoji\symbol{"2764}}
tweets and heart-eyes emoji
%{\NotoEmoji\symbol{"1F60D}}
tweets shifted to a degree which could explain the models' growing confusion between the two, but no signals accounted for the entire difference in performance and we conclude that the main reason remains the change in sequence length distributions.}
This suggests that \tok{} may be a good learner for word-internal phenomena, picking up structural cues as to their roles within or without context, making it more useful \textbf{locally} than subword tokens' uninformed embeddings, while not being strong enough to provide a better semantic prior for \mmod{} to aggregate together with surrounding well-formed words, making its \textbf{global} utility limited.


\input{tab_ablations}

\subsection{Ablations}
\label{ssec:abl}
We compare the dev set results of the \textsc{All no-suff} condition on several modified versions of the model, presented in Table~\ref{tab:ablations}.
First, we find that fine-tuning \mmod{} and \tokdetok{} parameters during downstream task application is beneficial for results across base models, indicating susceptibility of \tok{}'s network to tune itself on task data and not solely on LM and the vectorization signal.
Next, and most substantially, we note the vast improvement of Twitter-trained models compared with \ppt{} performed over a Wikipedia corpus with comparable size;
even though some tasks are on edited text, the overall effect of domain change during second pre-training is apparent (and, indeed, least impactful on the NYTWIT task which features the best-edited text).
Other decisions made during the \ppt{} phase appear to be less decisive: removing the dependency loops helps performance on BERT and GPT2, but makes a large dent in the best-performing RoBERTa, indicating potential gains to be made by applying \detok{} in generative tasks not pursued within our scope;
the suffix-based dialing down of inference in pre-training helps the masked models but hurts GPT2 performance, possibly because its autoregressive application prohibits it from looking at a simply-inflected word's suffix when processing its stem, a problem not incurred in masked modeling.


