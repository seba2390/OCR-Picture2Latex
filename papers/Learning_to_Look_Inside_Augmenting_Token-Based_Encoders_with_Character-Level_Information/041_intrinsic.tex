
\subsection{Intrinsic Assessment}

Across all base models and both \ppt{} corpora selected for our experiments, we observed a steady decrease in the \llm{} models' built-in loss metrics (masked prediction / autoregressive prediction) until stabilizing at roughly half the initial value before the end of the \ppt{} epoch.
This indicated that the transformer layers are able to process inputs from both the embedding table and \tok{} and reconcile them.
\autoref{fig:updates} depicts the parameter updates in RoBERTa by parameter type, across layers, comparing parameter values before and after the \ppt{} phase on Twitter data.
It shows that the change along the model layers is fairly stable, with mildly more extensive updates in the bottom and top layers.
The former is to be expected given the introduction of inputs from \tok{}; the latter can also be influenced by encountering Twitter data, which is substantially different than what RoBERTa is \say{used to}.

\begin{figure}
    \centering
    \includegraphics[width=8.25cm]{figures/layer_updates.png}
    \caption{Euclidean distance between RoBERTa weight parameter values before and after a \ppt{} training phase on the Twitter corpus.}
    \label{fig:updates}
\end{figure}

\input{tab_generations}

Another artifact of \ppt{} is the outputs of the \detok{} module.
While monitoring the training procedure, we periodically sample words from random locations centered around the surface of the unit sphere of the embedding space, to see what \say{priors} the generative net is learning from the vectors encountered during training.
\autoref{tab:gen} presents some of these samples for different models and different corpora at different points in the training phase.
Masked models appear to be learning well-formed fractions of English or pseudo-English words at early stages of the training phase from both Wikipedia and Twitter data.
As training continues, fewer sequences containing repetitions are observed, fewer generations occur across  samples, and more in-vocabulary words appear, suggesting convergence of the vector space towards representing well-formed and diverse English vocabulary.
The autoregressive GPT-2, on the other hand, struggles to produce meaningful sequences beyond short words and punctuation symbols when trained on the informal Twitter input, suggesting a difficulty in learning a mapping of language from vector space without availability of a two-sided context.

