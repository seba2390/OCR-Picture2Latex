\begin{table*}
    \centering
    \small
    \begin{tabular}{lcccccc} \toprule
        Dataset & Instances & Tokens & Types & TTR & Multitoks & Token mass \\
         & & \multicolumn{2}{c}{(Space-delimited)} & & & increase \\
        \midrule
        CoNLL-2003 NER & 14,986 & 204,563 & 23,624 & .115 & 16.08\% & 29.50\% \\
        Wiki1 & --- & 204,564 & 28,092 & .137 & 8.64\% & 12.85\% \\
        \midrule
        NYTWIT & 1,903 & 94,403 & 26,028 & .276 & 24.04\% & 38.04\% \\
        Wiki2 & --- & 94,403 & 16,903 & .179 & 9.11\% & 13.75\% \\
        \midrule
        Twitter NER & 2,394 & 46,469 & 10,586 & .228 & 17.23\% & 43.21\% \\
        Wiki3 & --- & 46,509 & 10,855 & .233 & 9.02\% & 13.77\% \\
        \midrule
        Emerging NER & 3,394 & 62,730 & 14,878 & .237 & 19.15\% & 54.25\% \\
        Wiki4 & --- & 62,757 & 13,392 & .213 & 9.05\% & 13.74\% \\
        \midrule
        Emoji Prediction & 427,458 & 4,973,813 & 504,644 & .101 & 32.96\% & 64.90\% \\ % test set: 21.53\% & 63.49\%
        Wiki5 & --- & 4,973,813 & 194,240 & .039 & 9.07\% & 13.58\% \\ % 95.52\% of types are multitoken! % prev version had 440,689 types, idk why
        \midrule
        \marco{} (10\% sample) & 80,704 & 46,956,674 & 1,355,049 & .029 & 20.58\% & 31.91\% \\ % 97.78\% of types are multitoken!
        Wiki6 & --- & 46,956,703 & 818,712 & .017 & 9.48\% & 14.37\% \\
        \bottomrule
    \end{tabular}
    \caption{Surface statistics for the datasets used for evaluation (training sets), against Wikipedia text of comparable token count sizes. Tokenization performed with GPT-2.}
    \label{tab:stats}
\end{table*}
