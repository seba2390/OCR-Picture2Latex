\section{Conclusion}
\label{sec:conclusion}

We present \tokdetok{}, an adaptive encoding-decoding system which allows large pre-trained language models to handle representation of words learned compositionally from their orthographic manifestation without resorting to hard-coded subword embeddings which fare poorly on the thick distribution tails of domain-shifted data, while still benefiting from token representations learned during the models' original pre-training phase.
We demonstrate \tokdetok{}'s efficacy across three pre-trained model architectures when trained on a corpus from a domain new to them, showing that with a relatively small amount of processing improvements can be reached on word-level tasks; with sequence-level task performance holding out in powerful pre-trained systems.
In future work, we wish to extend the scope of \tokdetok{} to non-English languages and to multilingual models, as well as to make use of the \detok{} module to assist in generative tasks.


\section*{Acknowledgments}
We thank Jon Clark, Dan Garrette, Mark Riedl, Diyi Yang, Dan Roth, and Wei Xu for their helpful comments on earlier drafts.

