\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{tacl2018v2}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{plex-mono}

\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{dsfont}

\usepackage{algorithm}
\usepackage{algpseudocode}


%%%% table stuff
\usepackage{array,etoolbox}
\preto\tabular{\setcounter{magicrownumbers}{0}}
\newcounter{magicrownumbers}
\def\rownumber{}

%%%% graphics
\usepackage{tikz}
\usetikzlibrary{arrows,matrix,positioning,fit,calc,shapes,decorations.pathreplacing,svg.path,shadows,patterns}
\pgfdeclarelayer{background}
\pgfdeclarelayer{backgroundoverlay}
\pgfsetlayers{background,backgroundoverlay,main}
\usepackage{pgfplots}
\pgfplotsset{compat=1.13}
\usepackage{adjustbox}
\usepackage{ocr}
\newcommand{\circnum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.5pt}{\scalebox{.8}{\(#1\)}}}}}
\newcommand{\ofw}[1]{\ensuremath{{#1}\raisebox{.06em}{\scalebox{.7}{$(w)$}}}}
\newcommand{\charword}[1]{\scalebox{.9}{\ocrfamily #1}}
\newcommand{\spelling}{s}
\newcommand{\wughighlight}[1]{{\ocrfamily\color{black} #1}}
%%%% end of graphics

\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{dirtytalk}
\usepackage{microtype}

\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{array}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\usepackage{bm}

\newcommand{\tokdetok}[0]{\textsc{XRayEmb}}
\newcommand{\tok}[0]{\textsc{XR-Enc}}
\newcommand{\detok}[0]{\textsc{XR-Dec}}
\newcommand{\tdloop}[0]{E$\rightarrow$D}
\newcommand{\dtloop}[0]{D$\rightarrow$E}
% \newcommand{\tokdetok}[0]{\textsc{TokDetok}}
% \newcommand{\tok}[0]{\textsc{Tok}}
% \newcommand{\detok}[0]{\textsc{Detok}}
\newcommand{\marco}[0]{\textsc{MarcoQA}}
\newcommand{\unilm}[0]{Unigram LM}
\newcommand{\llm}[0]{LPLM}
\newcommand{\wpc}[0]{WordPiece}
\newcommand{\ppt}[0]{\textsc{2pt}}
\newcommand{\mmod}[0]{\textsc{$M$}}
\newcommand{\gen}[0]{\textsc{Pred}}
\DeclareMathOperator*{\agg}{agg}

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\tribracket}[1]{\textless#1\textgreater}
\newcommand{\vE}{\ensuremath \vec{E}}
\newcommand{\vI}{\ensuremath \vec{I}}
\newcommand{\vc}{\ensuremath \vec{c}}
\newcommand{\vh}{\ensuremath \vec{h}}
\newcommand{\vb}{\ensuremath \vec{b}}
\newcommand{\ve}{\ensuremath \vec{e}}
\newcommand{\vr}{\ensuremath \vec{r}}
\newcommand{\vw}{\ensuremath \vec{w}}
\newcommand{\vx}{\ensuremath \vec{x}}
\newcommand{\vy}{\ensuremath \vec{y}}
\newcommand{\vf}{\ensuremath \vec{f}}


\taclpubformattrue % Uncomment this line to deanonymize

\setlength\titlebox{8cm} % comment out for review version

\title{Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information}

\author{Yuval Pinter\thanks{Work done as an intern at Bloomberg LP and as a Bloomberg PhD Data Science Fellow.} \\
  School of Interactive Computing \\
  Georgia Institute of Technology \\
  Atlanta, GA, USA \\
  \texttt{uvp@gatech.edu} \\\And
  Amanda Stent \\
  Bloomberg \\
  New York, NY, USA \\
  \texttt{astent@bloomberg.net} \\\AND 
  Mark Dredze \\
  Bloomberg \\
  Department of Computer Science \\
  Johns Hopkins University \\
  \texttt{mdredze@cs.jhu.edu} \\\And
  Jacob Eisenstein \\
  Google Research \\
  \texttt{jeisenstein@google.com} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Commonly-used transformer language models depend on a tokenization schema which sets an unchangeable subword vocabulary prior to pre-training, destined to be applied to all downstream tasks regardless of domain shift, novel word formations, or other sources of vocabulary mismatch.
    Recent work has shown that ``token-free'' models can be trained directly on characters or bytes, but training these models from scratch requires substantial computational resources, and this implies discarding the many domain-specific models that were trained on tokens. In this paper, we present \tokdetok{}, a method for retrofitting existing token-based models with character-level information.  
    \tokdetok{} is composed of a character-level ``encoder'' that computes vector representations of character sequences, and a generative component that decodes from the internal representation to a character sequence.
    We show that incorporating \tokdetok{}'s learned vectors into sequences of pre-trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on non-standard English text.
\end{abstract}


\input{01_intro}

\input{02_model}

\input{03_task}

\input{04_experiments}

\input{05_related}

\input{06_conclusion}


\bibliography{anthology,tdt}
\bibliographystyle{acl_natbib}

\clearpage

% \appendix

% \input{99_appendix}

\end{document}
