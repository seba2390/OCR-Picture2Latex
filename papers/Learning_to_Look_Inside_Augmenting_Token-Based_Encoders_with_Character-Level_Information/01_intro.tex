\section{Introduction}
\label{sec:intro}

The use of subword representations in NLP applications has become the default choice in recent years, most notably within the framework of large pre-trained language models (\llm{}s) where the LM objective requires a vocabulary that is \textit{computationally feasible} for a softmax operation, typically interpreted as no larger than $10^{5}$ types, while also \textit{exhaustive}, such that it allows production of arbitrary space-delimited tokens in order to mitigate the out-of-vocabulary (OOV) problem.
Informative subword vocabularies are pre-computed by methods such as byte-pair encoding~\cite[BPE;][]{sennrich-etal-2016-neural} and its variant \wpc{} used in BERT~\cite{devlin-etal-2019-bert}, or Unigram LM~\cite{kudo-2018-subword}.
Such vocabularies normally make use of explicit signalling for either word-medial or word-boundary tokens, so that recompiling the original space-delimited words is a deterministic process.\footnote{In this paper, we refer to space-delimited sections of text as \say{words} even when they are not linguistic words per se, in order to distinguish them from the model-centric \say{tokens} which are represented and operated over by \llm{}s.}

\input{tab_example}

However, processes for constructing subword vocabularies have high variance: perturbing the dataset used to train them leads to significant changes in the resulting vocabulary.
In a preliminary experiment, we sampled two sets of 1 million sentences from the same Wikipedia dump and trained a Unigram LM model of 32,000 tokens on both.
The resulting vocabularies presented a discrepancy of 27\% (8,620 unshared tokens).
Similarly, \newcite{lazaridou2021pitfalls} found that collecting corpus data from different timestamps of the same source shifts the vocabulary towards terms used more frequently over different times.
Nevertheless, this creation procedure is irreversible: once a subword vocabulary has been set, the \llm{} is tied down to it regardless of data encountered in subsequent LM training and fine-tuning.

\input{figures/fig_algo}

When shifting between domains, topics, and registers, this property produces undesired consequences~\cite{blitzer-etal-2007-biographies}.
One effect of training or fine-tuning an \llm{} over a dataset extracted from a domain other than the one on which its subword vocabulary was created is the formation of long input sequences with different words receiving markedly different treatment.
For example, in the tweet shown in \autoref{tab:example}, taken from a task where an emoji is to be matched to a tweet (see \S\ref{sec:tasks}), the intended emoji is the US flag; % {\NotoEmoji \symbol{"1F1FA}\symbol{"1F1F8}};
but the excessive number of \wpc{} tokens, and the breaking down of the most informative word \say{\#4thofjuly} into four unrecognizable segments, including the splitting of \say{July} due to its mid-word location, causes a classifier trained over a fine-tuned BERT model to falsely predict the red heart emoji. %, {\NotoEmoji \symbol{"2764}}.
In sequence labeling tasks like named entity recognition or morphological tagging, the breaking down of sequence-atomic words into tokens presents a nontrivial decision point as to how multi-token words' output representations are to be used for predicting labels~\cite{acs2021subword}.
In languages with non-concatenative morphology such as Hebrew, the contiguous nature of subword tokens has been shown to render the representations of the transformer stack all but useless~\cite{klein-tsarfaty-2020-getting}.

In this paper we present \tokdetok{}, a system which provides \llm{}s the option to represent novel and rare words with a single vector composed over their character sequence, while retaining the knowledge learned for well-recognized word tokens during pre-training.
Our method bridges the gap between the two representation modules via an additional pre-training sequence where the language modeling objective is supplemented with training a character-level encoding component which provides vector inputs to the contextualized model apparatus, as well as a generative component which learns to output target words from their contextualized representations.
We show how these lightweight auxiliary components can be trained so that they regularize each other, resulting in a character-to-vector model that approximates representations well for single- and multi-token words, and in a word-generation model which produces well-formed sequences approximating English from arbitrary points in vector space.
We examine the performance of various \llm{}s trained with a \tokdetok{} module over text from both user-generated and edited sources on both sequence classification and sequence labeling tasks, finding that our system's strength lies mostly in the latter.
We then analyze the effect of individual components and decision points in our system, such as our choice of corpus for the second pre-training phase and our regularization regime.\footnote{We will release our code and model checkpoints.}


