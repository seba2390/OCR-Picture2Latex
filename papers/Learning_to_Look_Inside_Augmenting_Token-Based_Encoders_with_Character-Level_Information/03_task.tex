\section{Tasks}
\label{sec:tasks}

To evaluate the advantages of character-sequence awareness in large-scale transformers, we chose a diverse set of datasets which reflect unedited user-generated language in English, as well as its interaction with edited text.
We report results on a sequence classification task (emoji prediction), a sequence tagging task (named entity recognition, or NER) in both an in-domain (Twitter NER) and cross-domain (emerging entities NER) setting, a sequence ranking task based on information retrieval in a hybrid edited-unedited textual setting (MARCO-QA ranking), and a task where a single word's class is predicted within a sequence (NYTWIT).

\paragraph{Emoji prediction.}
The English portion of the Multilingual Emoji Prediction dataset \cite{barbieri-etal-2018-semeval,ma-etal-2020-multi} is composed of tweets containing one of the twenty most common emoji symbols.
For the task, the emoji are stripped from the tweet text and the system is asked to predict which one appeared in the original tweet, allowing it to be construed as a self-annotated, fine-grained sentiment analysis task.
All tweets are identified as geographically originating in the US.
We note that there is a significant qualitative difference between the training (+ development) set, and the test set of this corpus, hinted at by one of the participant teams in the original task~\cite{chen-etal-2018-peperomia} but not explored.\footnote{Another team identified a seasonal shift affecting the distribution of the Christmas tree 
% {\NotoEmoji\symbol{"1F384}} 
emoji~\cite{coster-etal-2018-hatching}.}
The partitions were extracted based on a temporal split, with all test set tweets post-dating all training set tweets by at least three months. %(February 2017 -- May 2017).
More crucially, the test set dates (May 2017 -- January 2018) overlap with Twitter's increase of the tweet character limit from 140 to 280 characters, phased in mostly during November 2017.
As a result, test set tweets have on average $\sim$10\% more words, increasing the amount of information within and diverging their textual feature distribution from that of the training set by more than is customary in NLP tasks.
The label distribution remains more or less the same.

\paragraph{Twitter NER.}
The Twitter NER dataset \cite{strauss-etal-2016-results} is a prime example of sequence tagging in noisy user-generated data settings.
It is composed of randomly sampled English tweets from 2016, annotated for ten different entity types, including music artists and sports teams, thus representing topics prevalent in social media.

\paragraph{Emerging entities recognition.}
The emerging entities dataset \cite{derczynski-etal-2017-results} shares most of its training set with the same Twitter source as the Twitter NER dataset, but as a domain-adaptation setup it includes a more ambitious evaluation fold: the development set is extracted from YouTube comments, and the test set from StackExchange and Reddit.
The taskmasters claim that the dataset contains mostly rare and hitherto-unseen entities, but do not provide exact statistics.

\paragraph{Community question answering.}
The MS-MARCO question answering dataset~\cite[\marco;][]{nguyen2016ms} was collected by mining a commercial search engine log for user queries and asking humans to answer them, supplying the answer writers with a set of passages retrieved automatically from edited text by the search engine which the answer writers then marked as \say{selected} if they helped them formulate the answer.
We recast this dataset into a selection / ranking problem, not pursued in the original challenge: given the query and the set of possibly helpful passages, which is the passage the answer writer selected?
To this end, we filtered out queries with more or fewer than one selected passage, and evaluated each system based on the mean reciprocal ranks (MRR) of the true selected passages in the rankings it produced.
Due to its size, we uniformly sampled 10\% of the queries and associated passage collections from each partition in this dataset, and ran all analyses and experiments on this new sampled dataset.

\paragraph{Novel word classification.}
The NYTWIT dataset~\cite{pinter-etal-2020-nytwit} includes passages in the New York Times where a word appears which has not appeared in the publication before.
The system is tasked to classify the novel word into one of eighteen types of novelty sources, such as \say{inflection of known word} or \say{lexical blend}.
The dataset is not partitioned into train/dev/test, and so we use the 10-fold partition from \newcite{pinter-etal-2020-nytwit} and report accuracy results aggregated over all instances, each from a model trained on the other nine folds.


\input{tab_stats}


\subsection{Analysis}
\label{ssec:stats}

We begin with an analysis of the datasets and their subword properties in order to gauge the direct effects of tokenization on the dataset's genre and domain.
In \autoref{tab:stats}, we present surface-level statistics reflecting the challenges posed by the datasets' sources, comparing each task's training set with comparably-sized sets of sentences sampled uniformly from English Wikipedia.
We use the GPT-2 tokenizer, which boasts the largest subword vocabulary of all models considered in this work, to obtain a lower bound on the added token mass expected by our models on these datasets, and report the following measures: the number of unique word types and type/word ratio, the percent of types which are subword-tokenized by GPT-2 (\textit{Multitoks}), and the overall increase in number of tokens over the corpus compared to a single-token-per-word representation (i.e., strictest application of \tokdetok{} considered in this work).

We find a striking disparity between the well-edited Wikipedia corpora, themselves far from being completely in-vocabulary, and the datasets at hand.
Wikipedia itself proves to be scale-invariant on the metrics that are not TTR, maintaining a token-OOV rate of roughly 9\% and a tokenization overhead of $\sim$13\%.\footnote{This number is very close to the one found by \newcite{acs2021subword} using multilingual transformer models; other languages' corpora boast overheads ranging from 28\% (French) to 95\% (Japanese).}
A control dataset in the form of CoNLL-2003's English NER portion~\cite{tjong-kim-sang-de-meulder-2003-introduction}, extracted from newswire text, exhibits a marked increase in complex words, possibly mostly named entities, or time-sensitive terms compared to the tokenizer's training set, alongside a decrease in type count which reflects its narrow source domain.
NYTWIT, despite also being extracted from newswire text, is sourced from news items that contain novel forms and so are often written in a high register or involve niche domains; as a result, it contains a larger overall token mass, split rather evenly across individual words (i.e., unknown words have fairly \say{regular} structure).
The Twitter NER datasets, exhibiting tweet language, do not exceed CoNLL's multitoken type proportion by much, but its OOVs tend to be completely unexpected forms, leading to a much higher raw post-tokenization count.
In the emoji dataset, which has not been pre-processed according to NER standards and instead was directly scraped off Twitter, almost a third of all unique forms are multi-token, and their presence enlarges the total token count by nearly two thirds.
\marco{} data, most of which is text from highly-ranked web pages, but also including user-generated queries, assumes a middle position between these two extremes.





