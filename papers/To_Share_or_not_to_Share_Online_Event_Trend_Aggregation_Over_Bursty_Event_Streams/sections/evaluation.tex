\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Experimental Setup}

\textbf{Infrastructure}.
%
We have implemented \app\ in Java with JDK 1.8.0\_181 running on Ubuntu 14.04 with 16-core 3.4GHz CPU and 128GB of RAM. Our code is available online~\cite{hamlet_code}. We execute each experiment three times and report their average results here.

%--------------------------
\textbf{Data Sets}. 
%
We evaluate \app\ using four data sets.

% (291GB)
$\bullet$~\textit{New York city taxi and Uber real data set}~\cite{uber1} contains 2.63 billion taxi and Uber trips in New York City in 2014--2015. Each event carries a time stamp in seconds, driver and rider identifiers, pick-up and drop-off locations, number of passengers, and price. The average number of events per minute is 200.

% (135GB)
$\bullet$~\textit{Smart home real data set}~\cite{smarthome} contains 4055 million measurements for 2125 plugs in 40 houses. Each event carries a timestamp in seconds, measurement, house identifiers, and voltage measurement value. The average number of events per minute is 20K.

$\bullet$~\textit{Stock real data set}~\cite{stockStream} contains up to 20 years of stock price history. Our sample data contains 2 million transaction records of 220 companies for 8 hours. Each event carries a time stamp in minutes, company identifier, price, and volume. The average number of events per minute is 4.5K.

$\bullet$~\textit{Ridesharing data set} was created by our stream generator to control the rate and distribution of events of different types in the stream. This stream contains events of 20 event types such as request, pickup, travel, dropoff, cancel, etc. Each event carries a time stamp in seconds, driver and rider ids, request type, district, duration, and price. The attribute values are randomly generated. The average number of events per minute is 10K.

%--------------------------
\textbf{Event Trend Aggregation Queries}.
%
For each data set, we generated workloads similar to queries $q_1$--$q_3$ in Figure~\ref{fig:queries}. We experimented with the  two types of workloads described below.

$\bullet$~The first workload focuses on sharing Kleene closure because this is the most expensive operator in event trend aggregation queries (Definition~2.2). 
Further, the sharing of Kleene closure is a much overlooked topic in the literature; while the sharing of other query clauses (windows, grouping, predicates, and aggregation) has been well-studied in prior research and systems~\cite{AW04, GSCL12, KWF06, LMTPT05, LRGGWAM11}. Thus, queries in this workload are similar to Examples~\ref{ex:template_one_query}--\ref{ex:search_space}. Namely, they have different patterns but their sharable Kleene sub-pattern, window, groupby clause, predicates, and aggregates are the same. We evaluate this workload in Figures~\ref{fig:executor1}--\ref{fig:executor2}.

$\bullet$~The second workload is more diverse since the queries have sharable Kleene patterns of length ranging from 1 to 3, windows sizes ranging from 5 to 20 minutes, different aggregates (e.g., COUNT, AVG, MAX, etc.), as well as groupbys and predicates on a variety of event types. We evaluate this workload in Figures~\ref{fig:optimizer}--\ref{fig:opt-memory}.
%
%{\ear{what is meant by These -  do you mean All the above queries?}}
%{\ear{Why not instead say that the queries in workload 2 are count-based windows!!!!  say that this allows us to control better how many events per query are to be processed at a time.}}

The rate of events differs in different real data sets~\cite{uber1, smarthome, stockStream} that we used in our experiments. The window sizes are also different in the query workloads per data set. To make the results comparable across data sets, we vary the number of events per minute by a speed-up factor; which corresponds to the number of events per window divided by the window size in minutes.
%
The default number of events per minute per data set is included in the description of each data set.
%X for the New York city data set,
%Y for the smart home data set,
%Z for the stock data set, and 
%V for the ridesharing data set.
Unless stated otherwise, 
the workload consists of 50 queries.
We vary  major cost factors (Definition~\ref{def:dynamic-benefit}), namely,
the number of events and
the number of queries.

%\lei{In the figures we only vary #events per window and #queries.}
% olga: yep, fixed

%--------------------------
\textbf{Methodology}. 
%
We experimentally compare \app\ to the following state-of-the-art approaches:

$\bullet$~\textit{MCEP}~\cite{KS19} is the most recently published state-of-the-art shared two-step approach. MCEP constructs all event trends prior to computing their aggregation. As shown in~\cite{KS19}, it shares event trend construction. It outperforms other shared two-step approaches SPASS~\cite{RLR16} and MOTTO~\cite{ZVDH17}.

$\bullet$~\textit{\sharon}~\cite{PRLRM18} is a shared approach that computes event sequence aggregation online. That is, it avoids sequence construction by incrementally maintaining a count for each pattern. \sharon\ does not support Kleene closure. To mimic Kleene queries, we flatten them as follows. For each Kleene pattern $E+$, we estimate the length $l$ of the longest match of $E+$ and specify a set of fixed-length sequence queries that cover all possible lengths up to $l$. 

$\bullet$~\textit{\greta}~\cite{PLRM18} supports Kleene closure and computes event trend aggregation online, i.e, without constructing all event trends. It achieves this online event trend aggregation by encoding all matched events and their adjacency relationships in a graph. However, \greta\ does not optimize for sharing a workload of queries. That is, each query is processed independently as described in Section~\ref{sec:non-shared-baseline}.

%--------------------------
\textbf{Metrics}.
%
We measure \textit{latency} in seconds as the average time difference between the time point of the aggregation result output by a query in the workload and the arrival time of the latest event that contributed to this result.
%
\textit{Throughput} corresponds to the average number of events processed by all queries per second.
%
\textit{Peak memory consumption}, measured in bytes, corresponds to the maximal memory required to store snapshot expressions for \app, the current event trend for MCEP, aggregates for \sharon, and matched events for \app, MCEP, and \greta.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental Results}
\label{sec:exp_results}

%Due to space constraints, we describe the most important experimental results below, while a complete set of experiments is available in the technical report~\cite{PLMRR20}.

\textbf{\app\ versus State-of-the-art Approaches}.
%
In Figures~\ref{fig:executor1} and \ref{fig:memory}, we measure all metrics of all approaches while varying the number of events per minute from 10K to 20K and the number of queries in the workload from 5 to 25. We intentionally selected this setting to ensure that the two-step approach MCEP, the non-shared approach \greta, and the fixed-length sequence aggregation approach \sharon\ terminate within hours.

\input{sections/charts_executor}

With respect to throughput, \app\ consistently outperforms 
\sharon\ by 3--5 orders of magnitude,
\greta\ by 1--2 orders of magnitude, and
MCEP 7--76-fold
(Figures~\ref{fig:e1-throughput-events} and \ref{fig:e1-throughput-queries}).
%
We observe similar improvement with respect to latency in Figures~\ref{fig:e1-latency-events} and \ref{fig:e1-latency-queries}. 
While \app\ terminates within 25 milliseconds in all cases, 
\sharon\ needs up to 50 minutes, 
\greta\ up to 3 seconds, and 
MCEP up to 1 second.
% 
With respect to memory consumption, \app, \greta, and MCEP perform similarly, while \sharon\ requires 2--3 orders of magnitude more memory than \app\ in Figure~\ref{fig:memory}.

Such poor performance of \sharon\ is not surprising because \sharon\ does not natively support Kleene closure. To detect all Kleene matches, \sharon\ runs a workload of fixed-length sequence queries for each Kleene query. As Figure~\ref{fig:executor1} illustrates, this overhead dominates the latency and throughput of \sharon. 
In contrast to \sharon, \greta\ and MCEP terminate within a few seconds in this low setting because both approaches not only support Kleene closure but also optimize its processing. In particular, \greta\ computes trend aggregation without constructing the trends but does not share trend aggregation among different queries in the workload.
MCEP shares the construction of trends but computes trend aggregation as a post-processing step. Due to these limitations, \app\ outperforms both \greta\ and MCEP with respect to all metrics.

\begin{figure}[!htb]
% \vspace{-10pt}
	\centering
    \subfigure[Memory vs $\#$events]{
    	\includegraphics[width=0.3\columnwidth]{experiments/synthetic/lower_setting/epw/memory.png}
    	 \label{fig:memory-queries}
	}
	\subfigure[Memory vs $\#$queries]{
    	\includegraphics[width=0.3\columnwidth]{experiments/synthetic/lower_setting/queries/memory.png}
    	\label{fig:memory-events}
	}
% \vspace{-10pt}
    \caption{\app\ vs state-of-the-art (Ridesharing)}
    \label{fig:memory}
% \vspace{-10pt}
\end{figure}


\begin{figure}[!htb]
% \vspace{-10pt}
	\centering
    \subfigure[Latency vs $\#$events (NYC)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/latency-nyc.png}
    	 \label{fig:e2-latency-nyc-events}
	}\hspace{-8pt}
	\subfigure[Latency vs $\#$events (SH)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/latency-sh.png}
    	 \label{fig:e2-latency-sh-events}
	}\\
%
	\subfigure[Throughput vs $\#$events (NYC)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/throughput-nyc.png}
    	 \label{fig:e2-throughput-nyc-events}
	}\hspace{-8pt}
	\subfigure[Throughput vs $\#$events (SH)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/throughput-sh.png}
    	\label{fig:e2-throughput-sh-events}
	}\\
	\subfigure[Memory vs $\#$events (NYC)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/memory-nyc.png}
    	 \label{fig:e2-memory-nyc-events}
	}\hspace{-8pt}
	\subfigure[Memory vs $\#$events (SH)]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/epw/memory-sh.png}
    	\label{fig:e2-memory-sh-events}
	}\\
%
	\subfigure[Latency vs $\#$queries]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/queries/latency.png}
    	 \label{fig:e2-latency-queries}
	}\hspace{-8pt}
	\subfigure[Throughput vs $\#$queries]{
    	\includegraphics[width=0.3\columnwidth]{experiments/real/queries/throughput.png}
    	\label{fig:e2-throughput-queries}
	}
% \vspace{-10pt}
    \caption{\app\ versus state-of-the-art approaches (NY City Taxi (NYC) and Smart Home (SH) data sets)}
    \label{fig:executor2}
% \vspace{-10pt}
\end{figure}

However, the low setting in Figures~\ref{fig:executor1} and \ref{fig:memory} does not reveal the full potential of \app. Thus in Figure~\ref{fig:executor2}, we compare \app\ to the most advanced state-of-the-art online trend aggregation approach \greta\ using two real data sets. We measure latency and throughput, while varying the number of events per minute and the number of queries in the workload. 
%
\app\ consistently outperforms \greta\ with respect to throughput and latency by 3--5 orders of magnitude. In practice this means that the response time of \app\ is within half a second, while \greta\ runs up to 2 hours and 17 minutes for 400 events per minute in Figure~\ref{fig:e2-latency-nyc-events}.

%-----------------------------------------------------------
\textbf{Dynamic versus Static Sharing Decision}.
%
Figures~\ref{fig:optimizer} and \ref{fig:opt-memory} compare the effectiveness of \app\ dynamic sharing decisions to static sharing decisions. 
Each burst of events that can be shared contains 120 events on average in the stock data set. Our \app\ dynamic optimizer makes sharing decisions at runtime per each burst of events (Section~\ref{sec:dynamic-benefit}). The \app\ executor splits and merges graphlets at runtime based on these optimization instructions (Section~\ref{sec:decisions}). The number of all graphlets ranges from 400 to 600, while the number of shared graphlets ranges from 360 to 500. In this way, \app\ efficiently shares the beneficial Kleene sub-patterns within a subset of queries during its execution.

\input{sections/charts_optimizer}

In Figures~\ref{fig:opt-latency-events}, \ref{fig:opt-throughput-events} and \ref{fig:opt-memory-events}, as the number of events per minute increases from 2K to 4K, the number of snapshots maintained by the \app\ executor grows from 4K to 8K. As soon as the overhead of snapshot maintenance outweighs the benefits of sharing, the \app\ optimizer decides to stop sharing. The \app\ executor then splits these shared graphlets (Section~\ref{sec:decisions}). The \app\ dynamic optimizer shares approximately 90\% of bursts. The rest 10\% of the bursts are not shared which substantially reduces the number of snapshots by around 50\% compared to the shared execution. 

In contrast, the static optimizer decides to share certain Kleene sub-patterns by a fixed set of queries during the entire window. Since these decisions are made at compile time, they do not incur overhead at runtime. However, these static decisions do not take the stream fluctuations into account. Consequently, these sharing decisions may do more harm than good by introducing significant CPU overhead of snapshot maintenance, causing non-beneficial shared execution. During the entire execution, the static optimizer always decides to share, and the number of snapshots grows dramatically from 10K to 20K. 
% do may miss beneficial sharing opportunities, and, worse yet, introduce overhead due to non-beneficial sharing. In this case, the static optimizer decides to share during the entire window. Thus, the number of snapshots grows from 10K to 20K and sharing does more harm than good. 
Therefore, our \app\ dynamic sharing approach achieves 
21--34\% speed-up and
27--52\% throughput improvement
compared to the executor that obeys to static sharing decisions.

% Each shared graphlet contains Y events on average. 

%-------------------------------------------------------------
\begin{figure}[!htb]
\vspace{-10pt}
	\centering
    \subfigure[Memory vs $\#$events]{
    	\includegraphics[width=0.3\columnwidth]{experiments/optimizer/epw/memory.png}
    	 \label{fig:opt-memory-events}
	}
	\subfigure[Memory vs $\#$queries]{
    	\includegraphics[width=0.3\columnwidth]{experiments/optimizer/queries/memory.png}
    	\label{fig:opt-memory-queries}
	}
\vspace{-10pt}
    \caption{Dynamic versus static sharing decisions (Stock data set)}
    \label{fig:opt-memory}
\vspace{-5pt}
\end{figure}

We observe similar gains of \app\ with respect to memory consumption in Figure~\ref{fig:opt-memory-events}. \app\ reduces memory by 25\% compared to the executor based on static sharing decisions because the number of snapshots introduced by \app\ dynamic sharing decisions is much less than the number of snapshots introduced by the static sharing decisions. 
% Note that our \app\ dynamic sharing optimizer, in principle, could consume more memory when splitting a large burst. In this case, the memory of the duplicates of the events could outweigh the memory of snapshots created by the sharing decision.

% original version
% However, \app\ is latency optimized and does not always reduce memory. For example, the \app\ optimizer decides to split because the CPU overhead of snapshot maintenance is higher than the CPU overhead of recomputation.  ($k < s_p$ in Equation~\ref{eq:dynamic-benefit}). But if the burst has a large number of events, the memory of the duplicates of the events could outweigh the memory of snapshots created by the sharing decision. In this case, even dynamic decision is winning by splitting, it could spend more memory than a slower sharing decision. 

We also vary the number of queries in the workload from 20 to 100, and we observe similar gains by \app\ dynamic sharing optimizer in terms of latency, throughput, and memory (depicted in Figures~\ref{fig:opt-latency-queries}, \ref{fig:opt-throughput-queries}, and \ref{fig:opt-memory-queries}). \app\ can effectively leverage the beneficial sharing opportunities within a large query workload.

%\lei{I am not sure only giving numbers can offer a clear explanation here. The important thing is the portion of \#decision snapshot in the \#static snapshots. From the output, we have only 10\% bursts choose to split but have reduced 50\% of the total number of snapshots, which is the reason of the throughput increase. 
% olga: Lets discuss this on Monday. I am not following

%Also, about the memory, even in our experiment, dynamic is saving memory, that doesn't mean that dynamic is always cheaper in memory. If the split bursts have fewer snapshots, the size of duplicates of the burst could outweigh the size of the extra snapshots created by static, which means the dynamic could take more memory than the static }
% olga: i added this


Lastly, we measured the runtime overhead of the \app\ dynamic sharing decisions. Even though the number of sharing decisions ranges between 400 and 600 per window, the latency incurred by these decisions stays within 20 milliseconds (less than 0.2\% of total latency per window) because these decisions are light-weight (Section~\ref{sec:decisions}). Also, the latency of one-time static workload analysis (Section~\ref{sec:tempalte}) stays within 81 milliseconds. Thus, we conclude that the overhead of dynamic decision making and static workload analysis are negligible compared to their gains.

% and adaptively selects the set of queries that share each Kleene pattern

%\lei{compares the dynamic and static optimizer? Shouldn't that be the dynamic and static hamlet?}
% olga: rephrased

%\lei{the static optimizer decides which queries share which kleene? I think it should be the workload analyzer? In figure 2 there is only one Hamlet optimizer which includes workload analyzer and dynamic decision maker, the new names may cause confusion here.}
% olga:rephrased






