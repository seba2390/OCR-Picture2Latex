\subsection{\app\ Approach in a Nutshell}
\label{sec:overview}

Given a workload of event trend aggregation queries $Q$ and a high-rate event stream $I$, the \textbf{\textit{Multi-query Event Trend Aggregation Problem}} is to evaluate the workload $Q$ over the stream $I$ such that the average query latency of all queries in $Q$ is minimal. The latency of a query $q \in Q$ is measured as the difference between the time point of the aggregation result output by the query $q$ and the arrival time of the last event that contributed to this result.

\begin{figure}[!ht]
\centering
\includegraphics[width=.6\columnwidth]{figures/framework.png}
\vspace{-5pt}
\caption{\app\ Framework}
\label{fig:framework}
\end{figure}


We design the \app\ framework in Figure~\ref{fig:framework} to tackle this problem.
% 
To reveal all sharing opportunities in the workload at compile time, the \textbf{\textit{\app\ Optimizer}} identifies sharable queries and translates them into a Finite State Automaton-based representation, called the merged query template (Section~\ref{sec:tempalte}).
%\ear{I am worried here above; because this states in blue that sets of sharable queries are merged into one FSA - meaning there is no sharing across such mutually exclusive subsets. can we make it less explicit?}
% olga: made it less explicit
Based on this template, the optimizer reveals which sub-patterns could potentially be shared by which queries.
% \ear{This above sentence seems vague but fine - meaning
% that nothing but kleene is deemed sharable? this sounds
% not right; but probably just leave it as is. this
% section 3.2 which is at the very high-level may be
% the most dangerous section of reviewers easily
% finding limitations and what they don't like...}.
% chuan - removed "Kleene" from the above sentence to make it more generic
% {\color{blue} EAR: at what granularity? they will still think it is either the complete query, or not at all? at what granularity can one jump between alternate decisions at runtime is fuzzy}
% {\color{blue} EAR: Below, the "for how long"  is important; we need to convey that it's not for the complete query or even complete window, but rather for a  SUBSTREAM of EVENTS (a batch). right now as written, things are a bit fuzzy here in this section.}
% olga: fixed
At runtime, the optimizer estimates the sharing benefit depending on the current stream characteristics to make fine-grained sharing decisions. Each sharing decision determines which queries share the processing of which Kleene sub-patterns and for how long (Section~\ref{sec:runtime}). 
% {\color{blue} EAR: Below, it is not clear if the  "for how long" is encoded, so that you beforehand decide that you will stop sharing after the one substream data, or  will you stay and evaluate if to continue to share, i.e., stay in current mode of share or not share????}
These decisions along with the template are encoded into the runtime configuration to guide the executor.

% chuan - in this figure, we need to add a statistic monitor to close the runtime executor loop

\textbf{\textit{\app\ Executor}}  partitions the stream by the values of grouping attributes. To enable shared execution despite different windows of sharable queries, the executor further partitions the stream into panes that are sharable across overlapping windows~\cite{AW04, GSCL12, KWF06, LMTPT05}. Based on the merged query template for each set of sharable queries, the executor compactly encodes matched trends  
% {\ear{CAN   WE DROP the text "BY A QUERY"  - BECAUSE WOULD IT BE BY
% ANY OF all/any QUERIES RELEVANT TO THE DATA IN THAT PANE?} } 
% chuan - fixed
within a pane into the \app\ graph.
More precisely, matched events are modeled as  nodes, while event adjacency relations in a trend are edges of the graph.
%
%{\color{blue} EAR: This from x to y is confusing. how do those matched events relate to the hamlet graph is not clear. also, not clear what is kept in the graph ; namely, both all matched events are stored in the graph, plus associated meta data capturing the current partial aggregate result values.}
% olga: fixed
%
Based on this graph, we incrementally compute trend aggregates by propagating intermediate aggregates along the edges from previously matched events to new events
-- without constructing the actual trends. 
This reduces the time complexity of trend aggregation from exponential to quadratic in the number of matched events compared to two-step approaches~\cite{hong2009rule,KS19,PLAR17,ZDI14}.



The \app\ graph is partitioned into sub-graphs, called graphlets, by event type and time stamps to maximally expose runtime opportunities to share these graphlets among  queries. 
% {\color{blue} EAR: Why or how does decomposition help? there is an infinite # of decompositions; including a graph of size 1, size 2? so above seems a bit too vague and unclear. how decide what to all package inside a graphlet should be specified, or pointed at later where this is specified.}
Since the value of aggregates may be differ  for distinct
 queries, we capture these aggregate values per query as "snapshots" and share the propagation of snapshots through  shared graphlets (Section~\ref{sec:shared-approach}). 
% In this way, the graphlet $G$ can be executed only once by means of "snapshot propagation", while being equivalent to having processed  all queries in $Q$ on $G$ (Section~\ref{sec:shared-approach}). 
% We then share snapshot propagation in the graphlet $G$ among all queries in $Q$ (Section~\ref{sec:shared-approach}). %

Lastly, the executor implements the sharing decisions imposed by the optimizer. This may involve dynamically splitting a shared graphlet into several non-shared graphlets or, vice-versa, merging several non-shared graphlets into one shared graphlet (Section~\ref{sec:decisions}).
% \ear{What is section 4.1 and other parts of sec 4 about?}
% chuan - Sec. 4.1 is about benefit model, and Sec. 4.3 is about query set selection.


