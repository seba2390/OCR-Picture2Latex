\section{Introduction}
\label{sec:introduction}

Sensor networks, web applications, and smart devices produce high velocity event streams. Industries use Complex Event Processing (CEP) technologies to extract insights from these streams using Kleene queries~\cite{ADGI08,Giatrakos2020,ZDI14},
i.e., queries with Kleene plus ``+'' operator that matches event sequences of any length, a.k.a. \textit{event trends}~\cite{PLAR17}. Since these trends can be arbitrarily long and complex and there also tends to be a large number of them, they are typically aggregated to derive summarized insights~\cite{QCRR14}. CEP systems must thus process large workloads of these event trend aggregation queries over high-velocity streams in near real-time.

%------------------------------
\begin{example}
Complex event trend aggregation queries are  used in Uber and DoorDash for price computation, forecasting, scheduling, and routing~\cite{uber-athenax}. With hundreds of users per district, thousands of transactions, and millions of districts nationwide, real-time event analytics has become a challenging task.

In Figure~\ref{fig:queries}, the query workload  computes various trip statistics such as the number, total duration, and average speed of trips per district. Each event in the stream is of a particular event type, e.g., \textit{Request}, \textit{Pickup}, \textit{Dropoff}. Each event is associated with attributes such as a time stamp, district, speed, driver, and rider identifiers.

%
Query $q_1$ focuses on trips in which the driver drove to a pickup location but did not pickup a rider within 30 minutes since the request. Each trip matched by $q_1$ corresponds to a sequence of one ride \textit{Request} event, followed by one or more \textit{Travel} events (expressed by the Kleene plus operator ``+''), and not followed by a \textit{Pickup} event. All events in a trip must have the same driver and rider identifiers as required by the predicate [driver, rider].
%
Query $q_2$ targets \textit{Pool} riders who were dropped off at their destination. 
Query $q_3$ tracks riders who cancel their accepted requests while the drivers were stuck in slow-moving traffic.
%
All three queries contain the expensive Kleene sub-pattern $T+$ that matches arbitrarily long event trends. Thus, one may conclude that sharing $T+$ always leads to computational savings. However, a closer look reveals that the actual sharing benefit depends on the current stream characteristics. Indeed, trips are affected by many factors, from time and location to specific incidents, as the event stream fluctuates. 
%
%If most ride requests from the same district are of type \textit{Pool}, the benefit of sharing $q_1$--$q_3$ would be substantial. 
%
% {\color{blue} (probably ok? but can we ignore all the other aspects here? such as different group-by? different window? etc)}
%
%However, if pool requests are rare and frequently intermixed with other types of requests in the stream, sharing all three queries would require validation of trends per query which can incur significant computational overhead.
\label{ex:motivating}
\end{example}

\vspace*{-2mm}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\columnwidth]{figures/queries.png}
\caption{Event trend aggregation queries}
\label{fig:queries}
\end{figure}
\vspace*{-2mm}

% A wide range of streaming applications from fraud detection to targeted advertising involve large time-sensitive workloads consisting of similar trend aggregation queries over high-velocity data streams.

%------------------------------
\textbf{Challenges}. To enable shared execution of trend aggregation queries, we must tackle the following open challenges. 

%(1) efficiently execute the shared event trend aggregation queries, and (2) adaptively make a sharing decision at runtime, depending on the stream characteristics.

%\textit{Efficient shared query execution}. 
\textit{Exponential complexity versus real-time response}.
Construction of event trends matched by a Kleene query has exponential time complexity in the number of matched events~\cite{PLAR17,ZDI14}. To achieve real-time responsiveness, shared execution of trend aggregation queries should thus adopt online strategies that compute trend aggregates on-the-fly while avoiding this expensive trend construction~\cite{PLRM18,PLRM19}. However, shared execution applied to such online trend aggregation incurs additional challenges not encountered by the shared construction of traditional queries~\cite{KS19}. In particular, we must avoid constructing these trends, while  capturing  critical connections among shared sub-trends compactly to validate predicates of each query. 
%
For example, query $q_1$ in Figure~\ref{fig:queries} may match all events of type \textit{Travel}, while queries $q_2$ and $q_3$ may only match some of them due to their predicates. Consequently, different trends will be matched by these queries.
% one trend $(r_1,t_2,t_3,t_4)$ may be detected by $q_1$, another trend $(r_1,t_2,c_5)$ may be matched by $q_3$, but $q_2$ may not have any matches (Figure~\ref{fig:challenges}).
%
On first sight it appears that result validation requires the construction of all trends per query, which would defeat the goal of online aggregation. To address this dilemma, we must  develop  a correct yet efficient shared online trend aggregation strategy.

%\vspace{-2mm}
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.5\columnwidth]{figures/challenges.png}
%\caption{{\color{blue}Challenges of shared online trend aggregation}}
%\label{fig:challenges}
%\end{figure}
%\vspace{-2mm}

%\textit{Sharing benefit model}.
\textit{Benefit versus overhead of sharing}.
One may assume that the more sub-patterns are shared, the greater the performance improvement will be. However, this assumption does not always hold due to the overhead caused by maintaining intermediate aggregates of sub-patterns to ensure correctness of results. The computational overhead incurred by  shared query execution does not always justify the savings achievable compared to  baseline non-shared execution.
%
For example, sharing query $q_1$ with the other two queries in Figure~\ref{fig:queries} will not be beneficial if there are only few \textit{Pool} requests and the travel speed is above 10 mph.
% \ear{Did you mean NOT slow moving, or rather if yes slow moving?}
% chuan - fixed.
%For example, only two events $r_1$ and $t_2$ are matched by queries $q_1$ and $q_3$ in Figure~\ref{fig:challenges} and savings are negligible.
%
Hence, we need to devise a lightweight benefit model that accurately estimates the benefit of shared execution of multiple trend aggregation  at runtime.

%\textit{Adaptive sharing decision making}.
\textit{Bursty event streams versus light-weight sharing decisions}.
The actual sharing benefit can vary over time due to the nature of bursty event streams. Even with an efficient shared execution strategy and an accurate sharing benefit model, a static sharing solution may not always lead to computational savings. Worse yet, 
in some cases,  a static sharing decision may do more harm than good. 
%
Due to different predicates and windows of queries in Figure~\ref{fig:queries}, one may decide at compile time that these queries should not be shared. However, a large burst of \textit{Pool} requests may arrive and the traffic may be moving slowly (i.e., speed below 10 mph) in rush hour, making sharing of these queries beneficial.
% \ear{Why does slow traffic help us? if data is
% slow and thus low volume, then such opt becomes
% typically unnecessary?!}
% chuan - data is not slow, cars move slowly.
For this, a dynamic sharing optimizer, capable of adapting to changing arrival rates, data distribution, and other cost factors, must be designed. Its runtime sharing decisions must be light-weight to ensure real-time responsiveness.

%------------------------------
% \ear{Fig/Table 1 should be completely in BLUE font. also
% behind Hamlet, you may want to put the word (ours).}
% chuan - fixed

\input{sections/soa}
\vspace*{-5mm}

\textbf{State-of-the-Art Approaches}. 
While there are approaches to shared execution of multiple Kleene queries~\cite{hong2009rule,KS19}, they first construct all trends and then aggregate them. Even if trend construction is shared, its exponential complexity is not avoided~\cite{PLAR17,ZDI14}. Thus, even the most recent approach, \mcep~\cite{KS19} is 76--fold slower than \app\ as the number of events scales to 10K events per window (Figure~\ref{fig:e1-latency-events}).
%
% \ear{Table 1 is all about state-of-art,
% and thus should be cited and referred to
% in the state-of-art section!}
% chuan - fixed
Recent work on event trend processing~\cite{PLRM18,PLRM19,PRLRM18} addresses this performance bottleneck by pushing the aggregation computation into the pattern matching process. Such online methods manage to skip the trend construction step and thus reduce time complexity of trend aggregation from exponential to quadratic in the number of matched events. Among these online approaches, \greta~\cite{PLRM18} is the only approach that supports Kleene closure. Unfortunately, \greta\ neglects sharing opportunities in the workload and instead processes each query independently from others.
On the other hand, while \sharon~\cite{PRLRM18} considers  sharing among  queries, it does not support Kleene closure. Thus, it is restricted to fixed-length event sequences. Further, its shared execution strategy is static and thus misses runtime sharing opportunities. Our experiments confirm that these existing approaches fail to cope with high velocity streams with 100K events per window (Figures~\ref{fig:e2-latency-nyc-events} and \ref{fig:e2-latency-sh-events}). Table~\ref{tab:approaches} summarizes the approaches mentioned above with respect to the challenges of shared execution of multiple trend aggregation queries.

% olga: can we move the paragraph below to Section 7? Or better to technical report?
% Adaptive query processing over data streams has been well studied in the past two decades~\cite{DBLP:journals/ftdb/DeshpandeIR07}. However, it has received only limited attention in the CEP domain~\cite{KA18-2,KSS15,MM09}. Methods introduced in the CEP context decide whether and how the evaluation strategy should be re-optimized according to the stream statistics. Unfortunately, these methods either incur a substantial computational overhead of plan re-optimization~\cite{KSS15} or fail to support a shared execution of multiple queries~\cite{KA18-2,MM09}. Further, these approaches do not support online aggregation of Kleene pattern matches. Thus, their responsiveness would be delayed by exponential time complexity of trend construction for the targeted class of queries.



%------------------------------
\textbf{Proposed Solution}. 
To address these challenges, 
we now propose the \app\  approach that supports  online aggregation over Kleene closure while dynamically deciding which subset of sub-patterns should be shared by which trend aggregation queries and for how long depending on the current characteristics of the event stream.
% \ear{Citing Table 1 here seems WRONG.
% table 1 does not show much of any stream
% characteristics at all; stream characteristics would
% be distributions and frequencies of different
% event types and patterns of arrival. }
% chuan - removed the reference to table 1
The \app\ optimizer leverages these stream characteristics to estimate the runtime sharing benefit.  Based on the estimated benefit, it instructs the \app\ executor to switch between shared and non-shared execution strategies. Such fine-grained decisions allow \app\ to maximize the sharing benefit at runtime. 
%
% Driven by these sharing decisions, \app\ executor compactly encodes the events matched by any query in the workload into a graph-based structure, called the \app\ graph. 
The \app\ runtime executor propagates shared trend aggregates from previously matched events to newly matched events 
% in the graph 
in an online fashion, i.e., without constructing event trends.

%------------------------------
\textbf{Contributions}.
%
\app\ 
%tackles all open challenges described above by
offers the following key innovations.
%(Table~\ref{tab:overview}).

1. We present a novel framework \app\ for optimizing a workload of queries computing aggregation over Kleene pattern matches, called event trends. To the best of our knowledge, \app\ is the first to seamlessly integrate the power of online event trend aggregation and adaptive execution sharing among queries.

%decision-making.

2. We introduce the \app\ graph to compactly capture  trends matched by queries in the workload. We partition the graph into smaller graphlets by event types and time. \app\ then selectively shares trend aggregation in some graphlets among multiple queries. 

3. We design a lightweight sharing benefit model to quantify the trade-off between the benefit of sharing and the overhead of maintaining the intermediate trend aggregates per query at runtime. 

4. Based on the benefit of sharing sub-patterns, we propose an adaptive sharing optimizer.
%decision technique. 
It selects a subset of queries among which it is beneficial to share this sub-pattern and determines the time interval during which this sharing remains beneficial.


5. Our experiments on several real world  stream  data sets  demonstrate that \app\ achieves up to five orders of magnitude performance improvement over state-of-the-art approaches.

%-------------------------------------------
\textbf{Outline.} 
Section~\ref{sec:basic} describes preliminaries.
Sections~\ref{sec:executor} and \ref{sec:runtime} describe the core \app\ techniques: online trend aggregation and dynamic sharing optimizer. 
%We generalize our approach to a broad class of queries in Section~\ref{sec:other}.
We present experiments,
review related work  and 
conclude the paper in Sections ~\ref{sec:experiments}, 
~\ref{sec:related}, and 
~\ref{sec:conclusions}, respectively.

