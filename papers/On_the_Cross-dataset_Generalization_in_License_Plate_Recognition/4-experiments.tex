\section{\uppercase{Experiments}}
\label{sec:experiments}

In this section, we describe the setup adopted in our experiments. 
We first list the models we implemented for our assessments, explaining why they were chosen and not others.
Afterward, we provide the implementation details, for example, which framework was used to train/test each model and the respective hyperparameters.
We then present and briefly describe the datasets used in our experiments, as well as the data augmentation techniques explored to avoid overfitting.
Lastly, we detail the evaluation protocols adopted by us, that is, which images from each dataset were used for training or testing in each experiment, and how we evaluate the performance of each~method.

\subsection{Methods}
\label{sec:experiments-methods}

In this work, we evaluate $\numbaselines$ \gls*{ocr} models for \gls*{lp} recognition:
\rare~\citep{shi2016robust}, \rtwoam~\citep{lee2016recursive}, \starnet~\citep{liu2016starnet}, \crnn~\citep{shi2017endtoend}, \grcnn~\citep{wang2017deep}, \holistic~\citep{spanhel2017holistic}, \multitaskgabriel~\citep{goncalves2019multitask}, \rosetta~\citep{borisyuk2018rosetta}, \trba~\citep{baek2019what}, \crnet~\citep{silva2020realtime}, \fastocr~\citep{laroca2021towards}, and \vitstrbase~\citep{atienza2021vitstr}. 
Table~\ref{tab:models} presents an overview of these methods, listing the original \gls*{ocr} application for which they were designed as well as the framework we used to train and evaluate~them.

\begin{table}[!htb]
\centering
\caption{\gls*{ocr} models explored in our experiments.}
\label{tab:models}

\vspace{-0.75mm}

\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{@{}lllc@{}}
\toprule
\multicolumn{3}{c}{Model}                            & Original Application       \\ \midrule
\multicolumn{4}{l}{Framework: PyTorch\footnotemark[3]} 
\\
& & \rtwoam~\citep{lee2016recursive}                             & Scene Text Recognition       \\
& &  \rare~\citep{shi2016robust}                            & Scene Text Recognition       \\
& & \starnet~\citep{liu2016starnet}                         & Scene Text Recognition       \\
& & \crnn~\citep{shi2017endtoend}                             & Scene Text Recognition       \\
& & \grcnn~\citep{wang2017deep}                            & Scene Text Recognition       \\
& & \rosetta~\citep{borisyuk2018rosetta}                          & Scene Text Recognition       \\
& & \trba~\citep{baek2019what}                            & Scene Text Recognition       \\
& & \vitstrbase~\citep{atienza2021vitstr}                      & Scene Text Recognition      \\ \midrule
\multicolumn{4}{l}{Framework: Keras\footnotemark[4]} 
\\
& & \holistic~\citep{spanhel2017holistic} & License Plate Recognition     \\ 
& & \multitaskgabriel~\citep{goncalves2019multitask}                        & License Plate Recognition      \\ \midrule
\multicolumn{4}{l}{Framework: Darknet\footnotemark[5]} 
\\
& & \crnet~\citep{silva2020realtime}                           & License Plate Recognition  \\   
& & \fastocr~\citep{laroca2021towards}                        & Image-based Meter Reading               \\ \bottomrule
\end{tabular}%
}
\end{table}

These models were chosen/implemented by us for two main reasons: 
(i)~they have been employed for \gls*{ocr} tasks with promising/impressive results~\citep{baek2019what,atienza2021vitstr,laroca2021towards}, and (ii)~we believe we have the necessary knowledge to train/adjust them in the best possible way in order to ensure fairness in our experiments, as the authors provided enough details about the  architectures used, and also because we designed/employed similar networks in previous works (even the same ones in some cases)~\citep{goncalves2018realtime,goncalves2019multitask,laroca2019convolutional,laroca2021towards}.
Note that we are not aware of any work in the \gls*{alpr} literature where so many recognition models were explored in the~experiments.
\footnotetext[3]{\url{https://github.com/roatienza/deep-text-recognition-benchmark/}}
\footnotetext[4]{\url{https://keras.io/}}
\footnotetext[5]{\url{https://github.com/AlexeyAB/darknet/}}
\setcounter{footnote}{5}

The CR-NET and Fast-OCR models are based on the YOLO object detector~\citep{redmon2016yolo}.
Thus, they are trained to predict $35$ classes (0-9, A-Z, where `O' and `0' are detected/recognized jointly) using the bounding box of each \gls*{lp} character as input.
Although these methods have been attaining impressive results, they require laborious data annotations, i.e., each character's bounding box needs to be labeled for training them~\citep{wang2022rethinking}.
All the other $10$ models, on the other hand, output the \gls*{lp} characters in a segmentation-free manner, i.e., they predict  the characters holistically from the \gls*{lp} region without the need to detect/segment~them.
According to previous works~\citep{goncalves2018realtime,atienza2021vitstr,hasnat2021robust}, the generalizability of such segmentation-free models tends to improve significantly through the use of data augmentation.

\subsection{Setup}
\label{sec:experiments-setup}

All experiments were carried out on a computer with an AMD Ryzen Threadripper $1920$X $3.5$GHz CPU, $96$~GB of RAM ($2133$ MHz), HDD $7200$ RPM, and an NVIDIA Quadro RTX~$8000$ GPU~($48$~GB). 

Although run-time analysis is considered a critical factor in the ALPR literature~\citep{lubna2021automatic}, we consider such analysis beyond the scope of this work since we used different frameworks to implement the recognition models and there are probably differences in implementation and optimization between them --~we implemented each method using either the framework where it was originally implemented or well-known public repositories.
For example, the YOLO-based models were implemented using Darknet\footnotemark[5] while the models originally proposed for scene text recognition were trained and evaluated using a fork\footnotemark[3] of the open source repository of Clova AI Research (PyTorch) used to record the $1$st place of ICDAR2013 focused scene text and ICDAR2019~ArT, and $3$rd place of ICDAR2017 COCO-Text and ICDAR2019 ReCTS~(task1)~\citep{baek2019what}.

For completeness, below we list the hyperparameters used in each framework for training the \gls*{ocr} models; we remark that these hyperparameters were defined based on previous works fas well as on experiments performed in the validation set.
In Darknet, we employed the following parameters:~\gls*{sgd} optimizer, $90$K iterations (max batches), batch size~=~$64$, and learning rate~=~[$10$\textsuperscript{-$3$},~$10$\textsuperscript{-$4$},~$10$\textsuperscript{-$5$}] with decay steps at $30$K and $60$K~iterations.
In Keras, we used the Adam optimizer, initial learning rate~=~$10$\textsuperscript{-$3$} (with \textit{ReduceLROnPlateau}'s patience = $5$ and factor~=~$10$\textsuperscript{-$1$}), batch size~=~$64$, max epochs~=~$100$, and patience~=~$11$ (patience refers to the number of epochs with no improvement after which training is stopped).
In PyTorch, we adopted the following parameters: Adadelta optimizer, whose decay rate is set to $\rho= 0.99$,  $300$K iterations, and batch size~=~$128$.

\subsection{Datasets}
\label{sec:experiments-datasets}

Our experiments were conducted on images from the \dataset dataset and eight publicly available datasets that are often employed to benchmark \gls*{alpr} algorithms:
\caltech~\citep{caltech}, \englishlp~\citep{englishlp}, \stills~\citep{ucsd}, \chineselp~\citep{zhou2012principal}, \aolp~\citep{hsu2013application}, \openalpreu~\citep{openalpreu}, \ssigsegplate~\citep{goncalves2016benchmark}, \ufpralpr~\citep{laroca2018robust}.
Table~\ref{tab:experiments:overview_datasets} shows an overview of these datasets.
They were introduced over the last 22 years and have considerable diversity in terms of the number of images, acquisition settings, image resolution, and \gls*{lp} layouts.
As far as we know, there is no other work in the \gls*{alpr} literature where experiments were carried out on images from so many public~datasets.

\begin{table}[!htb]
\centering
\caption{The datasets used in our experiments. In this work, the ``Chinese'' layout refers to \glspl*{lp} of vehicles registered in mainland China, while the ``Taiwanese'' layout refers to \glspl*{lp} of vehicles registered in the Taiwan region.%
}
\label{tab:experiments:overview_datasets}

\vspace{-1mm}

\resizebox{0.95\columnwidth}{!}{ 
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Year} & \textbf{Images} & \textbf{Resolution} & \textbf{LP Layout} \\ \midrule
\caltech & $1999$ & $126$ & $896\times592$ & American \\
\englishlp & $2003$ & $509$ & $640\times480$ & European \\
\stills & $2005$ & $291$ & $640\times480$ & American \\
\chineselp & $2012$ & $411$ & Various & Chinese \\
\aolp & $2013$ & $2049$ & Various & Taiwanese \\
\openalpreu & $2016$ & $108$ & Various & European \\
\ssigsegplate & $2016$ & $2000$ & $1920\times1080$ & Brazilian \\
\ufpralpr & $2018$ & $4500$ & $1920\times1080$ & Brazilian \\ 
\dataset & $2022$ & $20000$ & $1280\times720$ & Brazilian/Mercosur \\ \bottomrule
\end{tabular}
}
\end{table}

Figure~\ref{fig:samples-public-datasets} shows the diversity of the chosen datasets in terms of \gls*{lp} layouts.
It is clear that even \glspl*{lp} from the same country can be quite different, e.g., the \caltech and \stills datasets were collected in the same region (California, United States), but they have images of \glspl*{lp} with significant differences in terms of colors, aspect ratios, backgrounds, and the number of characters.
It can also be observed that some datasets have \glspl*{lp} with two rows of characters and that the \glspl*{lp} may be tilted or have low resolution due to camera quality or vehicle-to-camera~distance.

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{captionskip=-0.25pt,font={scriptsize},justification=centering} 
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\caltech]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/caltech/002150_image_0063_caltech_us_UFD69K.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/caltech/002144_image_0057_caltech_us_1SCX833.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/caltech/002175_image_0089_caltech_us_VZW818.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\englishlp]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/englishlp/002705_070603_P6070021_englishlp_eu_AICMM14.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/englishlp/002656_040603_P6040044_englishlp_eu_ZG162LC.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/englishlp/002615_040603_P1010003_englishlp_eu_819KXH75.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\stills]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/stills/025423_cars008_ucsd-stills_us_RMZ61H.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/stills/025407_cars4_123_ucsd-stills_us_JSR113.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/stills/025432_cars017_ucsd-stills_us_7A68689.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\chineselp\label{fig:samples-public-datasets-chineselp}]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/chineselp/002361_chineselp_151_chineselp_cn_D00958.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/chineselp/002512_chineselp_299_chineselp_cn_B2B250.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/chineselp/002252_chineselp_042_chineselp_cn_AWW623.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\aolp]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/aolp/001802_rp_324_aolp_tw_IL9170.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/aolp/000694_le_013_aolp_tw_7960ZR.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/aolp/000651_ac_651_aolp_tw_2R4231.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\openalpreu]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/openalpr-eu/003182_test_052_openalpr-eu_eu_BY649AG.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/openalpr-eu/003125_eu08_openalpr-eu_eu_WSQ3021.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/openalpr-eu/003122_eu03_openalpr-eu_eu_FWE50.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\ssigsegplate]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ssig-segplate/025117_Track099_03__ssig_br_HDN6806.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ssig-segplate/023647_Track022_32__ssig_br_GMF2862.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ssig-segplate/024656_Track075_26__ssig_br_OPJ5305.jpg}} \hspace{1mm} 
    }
    
    \vspace{0.75mm}
    
    \resizebox{0.9\linewidth}{!}{
	\subfloat[][\ufpralpr]{
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ufpr-alpr/025875_track0105_01__ufpr-alpr_br_AEK0977.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ufpr-alpr/029624_track0080_30__ufpr-alpr_br_APJ3829.jpg}
    \includegraphics[height=4.5ex]{imgs/4-experiments/datasets/ufpr-alpr/027779_track0018_15__ufpr-alpr_br_BAA2384.jpg}} \hspace{1mm} 
    }

    \vspace{-0.25mm}
    
    \caption{Some \gls*{lp} images from the public datasets used in our experimental evaluation.
    We show some \gls*{lp} images from the \dataset dataset in the last column of Fig~\ref{fig:samples-dataset}.
    }
    \label{fig:samples-public-datasets}
\end{figure}

In order to eliminate biases from the public datasets, we also used $772$ images from the internet --~those labeled and provided by \cite{laroca2021efficient}~-- to train all models.
These images include $257$ American \glspl*{lp}, $347$ Chinese \glspl*{lp}, and $178$ European \glspl*{lp}.
We chose not to use two datasets introduced recently: \karplate~\citep{henry2020multinational} and \ccpd~\citep{xu2018towards}.
The former cannot currently be downloaded due to legal problems.
The latter, although already available, was not employed for two main reasons: (i)~it contains highly compressed images, which significantly compromises the readability of the \glspl*{lp}~\citep{silva2022flexible};
and (ii)~it has some large errors in the corners' annotations~\citep{meng2020accelerating} --~this was somewhat expected since the corners were labeled automatically using RPnet~\citep{xu2018towards}. 
Additionally, we could not download the \clpd dataset~\citep{zhang2021robust_attentional}, as the authors made it available exclusively through a Chinese website where registration --~using a Chinese phone number or identity document~-- is required (we contacted the authors requesting an alternative link to download the dataset, but have not received a response so~far).

\subsubsection{Data Augmentation}

As shown in Table~\ref{tab:experiments:overview_datasets}, two-thirds of the images used in our experiments are from the \dataset dataset.
In order to prevent overfitting, we initially balanced the number of images from different datasets through data augmentation techniques such as random cropping, random shadows, conversion to grayscale, and random perturbations of hue, saturation and brightness.
We used Albumentations~\citep{albumentations}, which is a well-known Python library for image augmentation, to apply these transformations.
Nevertheless, preliminary experiments showed that some of the recognition models were prone to predict only \gls*{lp} patterns that existed in the training set, as some patterns were being fed numerous times per epoch to the networks -- especially from small-scale datasets, where many images were created from a single original one.
Therefore, inspired by~\cite{goncalves2018realtime}, we also randomly permuted the position of the characters on each \gls*{lp} to eliminate such biases in the learning process (as illustrated in Figure~\ref{fig:data-aug-preprocessing-ocr}).
As the bounding box of each \gls*{lp} character is required to apply this data augmentation~technique --~these annotations are very time-consuming and laborious~-- we do not augment the training images from the RodoSol-ALPR dataset.
We believe this is not a significant problem as the proposed dataset is much larger than the~others.
The images from the other public datasets were augmented using the labels provided by~\cite{laroca2021efficient}.

\begin{figure}[!htb]
    \centering
    
    \resizebox{0.95\linewidth}{!}{ %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/tw-orig.jpg} \hspace{-1.25mm}
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/eu-orig.jpg} \hspace{-1.25mm}
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/us-orig.jpg}
    } %
    
    \vspace{0.1mm}
    
    \resizebox{0.95\linewidth}{!}{ %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/tw-2.jpg} \hspace{-1.25mm} %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/eu-2.jpg} \hspace{-1.25mm}
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/us-2.jpg} %
    } %
    
    \vspace{0.1mm}
    
    \resizebox{0.95\linewidth}{!}{ %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/tw-3.jpg}  \hspace{-1.25mm} %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/eu-3.jpg} \hspace{-1.25mm}
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/us-3.jpg} %
    } %
    
    \vspace{0.1mm}
    
    \resizebox{0.95\linewidth}{!}{ %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/tw-4.jpg} \hspace{-1.25mm} %
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/eu-4.jpg} \hspace{-1.25mm}
    \includegraphics[height=4.5ex]{imgs/4-experiments/permutation/us-4.jpg} %
    } %
    
    \vspace{0.25mm}
    
    \caption{
    Illustration of the character permutation-based data augmentation technique~\citep{goncalves2018realtime} we adopted to avoid overfitting.
    The images in the first row are the originals, while the others were generated~automatically.}
    \label{fig:data-aug-preprocessing-ocr}
\end{figure}

In this process, we do not enforce the generated \glspl*{lp} to have the same arrangement of letters and digits of the original \glspl*{lp} so that the recognition models do not memorize specific patterns from different \gls*{lp} layouts.
For example, as described in Section~\ref{sec:dataset}, all Brazilian \glspl*{lp} consist of $3$ letters followed by $4$ digits, while Mercosur \glspl*{lp} have $3$ letters, $1$ digit, $1$ letter and $2$ digits, in that order.
Considering that \glspl*{lp} of these layouts are relatively similar (in size, shape, etc.), the segmentation-free networks would probably predict $3$ letters followed by $4$ digits for most Mercosur \gls*{lp} when holding the \dataset dataset out in a leave-one-dataset-out evaluation, as none of the other datasets have vehicles with Mercosur~\glspl*{lp}.

\subsection{Evaluation Protocols}
\label{sec:experiments-protocol}

In our experiments, we consider both traditional-split and leave-one-dataset-out protocols.
In the following subsections, we first describe them in~detail.
Then, we discuss how the performance evaluation is carried~out.

\subsubsection{Traditional Split}
\label{sec:experiments-protocol:traditional-split}

The traditional-split protocol assesses the ability of the models to perform well in seen scenarios, as each model is trained on the union of the training set images from all datasets and evaluated on the test set images from the respective datasets.
In recent works, the authors have chosen to train a single model on images from multiple datasets (instead of training a specific network for each dataset or \gls*{lp} layout as was commonly done in the past) so that the proposed models are robust for different scenarios with considerably less manual effort since their parameters are adjusted only once for all datasets~\citep{selmi2020delpdar,laroca2021efficient,silva2022flexible}.

For reproducibility, it is important to make clear how we divided the images from each of the datasets to train, validate and test the chosen models.
The \stills, \ssigsegplate, \ufpralpr and \dataset datasets were split according to the protocols defined by the respective authors, while the other datasets, which do not have well-defined evaluation protocols, were divided following previous works.
In summary, as in~\citep{xiang2019lightweight,henry2020multinational}, the \caltech dataset was randomly split into $80$ images for training/validation and $46$ images for testing.
Following~\citep{panahi2017accurate,beratoglu2021vehicle}, the \englishlp dataset was randomly divided as follows: $80$\% of the images for training/validation and $20$\% for testing.
For the \chineselp dataset, we employed the same protocol as \cite{laroca2021efficient}: $40$\% of the images for training, $20$\% for validation and $40$\% for testing.
We split each of the three subsets of the \aolp dataset (i.e., AC, LE, and RP) into training and test sets with a $2$:$1$ ratio, following~\citep{xie2018new,liang2021egsanet}, with $20$\%
of the training images being used for validation.
Finally, as most works in the literature~\citep{masood2017sighthound,laroca2021efficient,silva2022flexible}, we used all the $108$ images from the \openalpreu dataset for~testing (this division has been considered as a mini leave-one-dataset-out evaluation in recent works).
Table~\ref{tab:results:overview_datasets_protocols} lists the exact number of images used for training, validating and testing the chosen~models.

\begin{table}[!htb]
\centering
\caption{An overview of the number of images from each dataset used for training, validation, and testing.}
\label{tab:results:overview_datasets_protocols}
\vspace{-0.75mm}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Training} & \textbf{Validation} & \textbf{Testing} & \textbf{Discarded} & \textbf{Total} \\ \midrule
\caltech & $61$ & $16$ & $46$ & $3$ & $126$\\
\englishlp & $326$ & $81$ & $102$ & $0$ & $509$ \\
\stills & $181$ & $39$ & $60$ & $11$ & $291$\\
\chineselp & $159$ & $79$ & $159$ & $14$ & $411$\\
\aolp & $1{,}093$ & $273$ & $683$ & $0$ & $2{,}049$\\
\openalpreu & $0$ & $0$ & $108$ & $0$ & $108$ \\
\ssigsegplate & $789$ & $407$ & $804$ & $0$ & $2{,}000$\\
\ufpralpr & $1{,}800$ & $900$ & $1{,}800$ & $0$ & $4{,}500$ \\
\dataset & $8{,}000$ & $4{,}000$ & $8{,}000$ & $0$ & $20{,}000$ \\\bottomrule
\end{tabular}
}
\end{table}

As also detailed in Table~\ref{tab:results:overview_datasets_protocols}, a few images ($0.01$\%) were discarded in our experiments because it is impossible to recognize the \gls*{lp}(s) on them due to occlusion, lighting or image acquisition problems\footnote{\scriptsize The list of discarded images can be found at \discarded}.
Such images were also discarded by~\cite{masood2017sighthound} and~\cite{laroca2021efficient}.

\subsubsection{Leave-one-dataset-out}
\label{sec:experiments-protocol:leave-one-dataset-out}

The leave-one-dataset-out protocol evaluates the generalization performance of the trained models by testing them on the test set of an unseen dataset; that is, no images from that dataset are available during training.
For each experiment, we hold out the test set of one dataset as the unseen data, and train every model on all images from the other datasets.
As an example, if \aolp's test set is the current unseen data, the models are trained on all images from \caltech, \englishlp, \stills, \chineselp, \openalpreu, \ssigsegplate, \ufpralpr and \dataset, in addition to the images taken from the internet and provided by~\cite{laroca2021efficient}.

We evaluate the models only on the test set images from each unseen dataset, rather than including the training and validation images in the evaluation, so that the results achieved by each model on a given dataset are fully comparable with those achieved by the same model under the traditional-split~protocol.

\subsubsection{Performance Evaluation}
\label{sec:experiments-protocol:performance-evaluation}

As mentioned in Section~\ref{sec:introduction}, in our experiments, the \glspl*{lp} fed to the recognition models were detected using YOLOv4~\citep{bochkovskiy2020yolov4} --~with an input size of $672\times416$ pixels~-- rather than cropped directly from the ground truth. 
This procedure was adopted to better simulate real-world scenarios, as the \glspl*{lp} will not always be detected perfectly, and certain \gls*{ocr} models are not as robust in cases where the region of interest has not been detected so precisely~\citep{goncalves2018realtime}.
We employed the YOLOv4 model for this task because impressive results are consistently being reported in the \gls*{alpr} context through YOLO-based models~\citep{weihong2020research}.
Indeed, as detailed in Section~\ref{sec:results}, YOLOv4 reached an average recall rate above $99.5$\% in our experiments (we considered as correct the detections with \gls*{iou} $\ge0.5$ with the ground~truth).

For each experiment, we report the number of correctly recognized \glspl*{lp} divided by the number of \glspl*{lp} in the test set.
A correctly recognized \gls*{lp} means that all characters on the \gls*{lp} were correctly recognized, as a single incorrectly recognized character can result in the vehicle being incorrectly~identified.

Note that the first character in Chinese \glspl*{lp} is a Chinese character that represents the province in which the vehicle is affiliated~\citep{xu2018towards,zhang2021robust_attentional}.
Even though Chinese \glspl*{lp} are used in our experiments (see Figure~\ref{fig:samples-public-datasets-chineselp}), the evaluated models were not trained/adjusted to recognize Chinese characters; that is, only digits and English letters are considered.
This same procedure was adopted in previous works~\citep{li2019toward,selmi2020delpdar,laroca2021efficient} for several reasons, including scope reduction and the fact that it is not trivial for non-Chinese speakers to analyze the different Chinese characters in order to make an accurate error analysis or to choose which data augmentation techniques to~explore.
Following~\cite{li2019toward}, we denoted all Chinese characters as a single class~`*' in our experiments.
According to our results, the recognition models learned well the difference between Chinese characters and others --~i.e., digits and English letters~-- and this procedure did not affect the recognition rates~obtained.