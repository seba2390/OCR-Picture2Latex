\begin{table*}[!htb]
\centering
\setlength{\tabcolsep}{7pt}
\caption{Recognition rates obtained by all models under the \ul{leave-one-dataset-out} protocol, which assesses the generalization performance of the models by testing them on the test set of an unseen dataset.
For each dataset (columns), we trained the recognition models (rows) on all images from the other datasets.
The best recognition rates achieved are shown in bold.
}
\label{tab:results-leave-one-dataset-out}

\vspace{-1mm}

\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\diagbox[trim=l,innerrightsep=28.5pt]{Approach}{Test set}    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\caltech\\\# $46$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\englishlp\\\# $102$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\stills\\\# $60$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\chineselp\\\# $159$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\aolp\\\# $683$\phantom{\#}\end{tabular}}    & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\openalpreu\\\# $108$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\ssigsegplate\\\# $804$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\ufpralpr\\\# $1{,}800$\phantom{\#}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\dataset\\\# $8{,}000$\phantom{\#}\end{tabular}} & Average \\ \midrule
\crnet~\citep{silva2020realtime}   & $93.5$\%      & $\textbf{96.1}$\textbf{\%}   & $\textbf{96.7}$\textbf{\%}     & $88.2$\%   & $76.9$\% & $\textbf{96.3}$\textbf{\%}     & $94.7$\%       & $61.8$\%      & $45.4$\%      & $\textbf{83.3}$\textbf{\%} \\ 
\crnn~\citep{shi2017endtoend}        & $91.3$\%      & $62.7$\%   & $75.0$\%     & $76.4$\%   & $59.4$\% & $88.0$\%     & $91.3$\%       & $61.7$\%       & $38.8$\%      & $71.6$\% \\
\fastocr~\citep{laroca2021towards}     & $93.5$\%      & $91.2$\%   & $95.0$\%    & $90.1$\%   & $77.0$\% & $94.4$\%     & $91.2$\%       & $53.2$\%      & $\textbf{47.8}$\textbf{\%}      & $81.5$\% \\
\grcnn~\citep{wang2017deep}       & $\textbf{95.7}$\textbf{\%}      & $65.7$\%   & $90.0$\%     & $80.7$\%   & $53.9$\% & $88.9$\%     & $90.3$\%       & $60.8$\%       & $39.8$\%      & $74.0$\% \\
\holistic~\citep{spanhel2017holistic}        & $80.4$\%      & $40.2$\%   & $73.3$\%     & $81.4$\%   & $59.7$\%  & $83.3$\%     & $93.4$\%       & $61.8$\%      & $33.4$\%      & $67.4$\% \\
\multitaskgabriel~\citep{goncalves2019multitask}        & $82.6$\%     & $34.3$\%    & $66.7$\%      & $77.6\%$    & $50.8$\%   & $79.6$\%     & $89.9$\%       & $57.9$\%      & $44.8$\%      & $64.9$\% \\
\rtwoam~\citep{lee2016recursive}        & $89.1$\%      & $52.9$\%   & $66.7$\%     & $74.5$\%   & $52.5$\%  & $80.6$\%     & $93.5$\%       & $57.9$\%       & $40.7$\%      & $67.6$\% \\
\rare~\citep{shi2016robust}        & $84.8$\%      & $50.0$\%   & $85.0$\%     & $88.8$\%   & $62.9$\% & $91.7$\%     & $93.5$\%       & $71.3$\%       & $40.1$\%      & $74.2$\% \\
\rosetta~\citep{borisyuk2018rosetta}     & $89.1$\%      & $63.7$\%   & $68.3$\%     & $83.2$\%   & $51.1$\% & $81.5$\%     & $94.4$\%       & $61.8$\%        & $42.5$\%      & $70.6$\% \\
\starnet~\citep{liu2016starnet}    & $89.1$\%      & $80.4$\%   & $91.7$\%     & $\textbf{95.0}$\textbf{\%}   & $\textbf{79.3}$\textbf{\%} & $93.5$\%     & $94.0$\%       & $69.1$\%      & $43.6$\%      & $81.8$\% \\
\trba~\citep{baek2019what}        & $\textbf{95.7}$\textbf{\%}      & $66.7$\%   & $93.3$\%     & $\textbf{95.0}$\textbf{\%}   & $70.0$\% & $92.6$\%     & $96.9$\%       & $\textbf{73.2}$\textbf{\%}      & $42.6$\%      & $80.7$\% \\
\vitstrbase~\citep{atienza2021vitstr} & $89.1$\%      & $58.8$\%   & $90.0$\%     & $\textbf{95.0}$\textbf{\%}   & $59.2$\%  & $89.8$\%     & $\textbf{97.9}$\textbf{\%}       & $69.6$\%      & $41.7$\%      & $76.8$\% \\[2pt] \cdashline{1-11} \\[-7.5pt]
Average   & $89.5$\%      & $63.6$\%   & $82.6$\%     & $85.5$\%   & $62.7$\% & $88.3$\%     & $93.4$\%       & $63.3$\%      & $41.8$\%      & $\acclodo$\% \\ %
Average (traditional-split protocol)    & $90.0$\%      & $80.4$\%   & $90.4$\%     & $91.5$\%   & $90.8$\% & \phantom{$^\dagger$}$85.1$\%$^\dagger$     & $94.1$\%       & $68.7$\%       & $50.8$\%      & $\acctraditional$\% 
\\ \midrule
Sighthound~\citep{masood2017sighthound}  & $87.0$\%      & $94.1$\%   & $90.0$\%     & $84.5$\%   & $79.6$\% & $94.4$\%     & $79.2$\%       & $52.6$\%       & $51.0$\%      & $79.2$\% \\
OpenALPR~\citep{openalprapi}$^\ast$           & $95.7$\%      & $99.0$\%   & $96.7$\%     & $93.8$\%   & $81.1$\% & $99.1$\%     & $91.4$\%       & $87.8$\%       & $70.0$\%      & $90.5$\% \\ \bottomrule \\[-2.2ex]
\multicolumn{11}{l}{\small $^{\dagger}$Under the traditional-split protocol, no images from the \openalpreu dataset were used for training. This is the protocol commonly adopted in the literature~\citep{laroca2021efficient,silva2022flexible}.} \\
\multicolumn{11}{l}{\small $^{\ast}$OpenALPR contains specialized solutions for \glspl*{lp} from different regions and the user must enter the correct region before using its API. Hence, it was expected to achieve better results than the other methods.}
\end{tabular}%
}
\end{table*}