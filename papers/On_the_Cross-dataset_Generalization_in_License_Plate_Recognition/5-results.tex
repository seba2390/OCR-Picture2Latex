\input{4-experiments/table-lp-detection}
\input{4-experiments/table-traditional-split}
\input{4-experiments/table-leave-one-dataset-out}

\section{\uppercase{Results and Discussion}}
\label{sec:results}

First, we report in Table~\ref{tab:results-lp-detection} the recall rates obtained by the YOLOv4 model in the \gls*{lp} detection stage.
As can be seen, it reached surprisingly good results in both protocols.
More specifically, recall rates above $99.9$\% were achieved in $14$ of the $18$ assessments. 
As in previous works~\citep{laroca2018robust,goncalves2018realtime,silva2020realtime}, the detection results are slightly worse for the \ufpralpr dataset due to its challenging nature, as (i)~it has images where the vehicles are considerably far from the camera;
(ii)~some of its frames have motion blur because the dataset was recorded in real-world scenarios where both the vehicle and the camera --~inside another vehicle~-- are moving;
and (iii)~it also contains images of motorcycles, where the backgrounds can be much more complicated due to different body configurations and mixtures with other background~scenes~\citep{hsu2015comparison}.

Considering the discussion above, we assert that deep models trained for \gls*{lp} detection on images from multiple datasets can be employed quite reliably on images from unseen datasets (i.e., leave-one-dataset-out protocol).
Of course, this may not hold true in extraordinary cases where the test set domain is very different from training ones, but this was not the case in our experimental evaluation carried out on images from nine datasets with different~characteristics.

Regarding the recognition stage, the results achieved by all models across all datasets on the traditional-split and leave-one-dataset-out protocols are shown in Table~\ref{tab:results-traditional-split} and Table~\ref{tab:results-leave-one-dataset-out}, respectively.
In Table~\ref{tab:results-leave-one-dataset-out}, we included the results obtained by Sighthound~\citep{masood2017sighthound} and OpenALPR~\citep{openalprapi}, which are two commercial systems frequently used as baselines in the \gls*{alpr} literature, since in principle they are trained on images from large-scale private datasets and not from the public datasets explored here (i.e., leave-one-dataset-out~protocol).

The first observation is that, as expected, the best results --~on average for all models~-- were attained when training and evaluating the models on different subsets from the same datasets (i.e., traditional-split protocol).
The only case where this did not occur was precisely in the \openalpreu dataset, where no images are used for training even under the traditional-split protocol (see Table~\ref{tab:results:overview_datasets_protocols}).
We kept this division for three main reasons: (i)~to better evaluate the recognition models on European \glspl*{lp}; (ii)~to maintain consistency with previous works~\citep{masood2017sighthound,laroca2021efficient,silva2022flexible}, which also used all images from that dataset for testing; and (iii)~to analyze how the models perform with more training data from other datasets, which in this case corresponds to the leave-one-dataset-out protocol since all images from the other datasets --~and not just the training set ones~-- are used for~training.
Although some studies have shown that the performance on the test set of a particular dataset often decreases when the training data is augmented with data from other datasets~\citep{torralba2011unbiased,khosla2012undoing}, the recognition rates reached in the \openalpreu dataset were higher with more training data from other datasets.
In the same way, \crnet performed better in the \englishlp dataset when using all images from the \openalpreu dataset for~training (both datasets contain images of European \glspl*{lp}). 

The average recognition rate across all datasets decreased from $\acctraditional$\% under the traditional-split protocol to $\acclodo$\% under the leave-one-dataset-out protocol.
This drastic performance drop is accentuated by the poor results achieved on the \englishlp and \aolp datasets under the leave-one-dataset-out protocol.
For instance, the average recognition rate of $90.8$\% obtained in the \aolp dataset under the traditional-split protocol drops to $62.7$\% under the leave-one-dataset-out protocol.
These results caught us by surprise, as both datasets have been considered to be relatively simple due to the fact that recent works have reported recognition rates close to 97\% for the \englishlp and above 99\% for the \aolp dataset~\citep{henry2020multinational,laroca2021efficient,silva2022flexible,wang2022rethinking}.
According to our analysis, most of the recognition errors under the leave-one-dataset-out protocol occurred due to differences in the fonts of the \gls*{lp} characters in the training and test images, as well as because of specific patterns in the \gls*{lp} (e.g., a coat of arms between the \gls*{lp} characters or a straight line under them).
To better illustrate, Figure~\ref{fig:samples-errors-aolp} shows three \glspl*{lp} from the \aolp dataset where the \trba model, which performed best on that dataset, recognized at least one character incorrectly under the leave-one-dataset-out protocol but not under the traditional split.
Such analysis highlights the importance of performing cross-dataset experiments in the \gls*{alpr}~context.

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={normalsize},captionskip=0.3pt}
    
    \resizebox{\linewidth}{!}{
    \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{LODO:}} \texttt{7615\wrong{B}G}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Trad.:}} \texttt{7615RG}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/000692_le_011_aolp_tw_7615RG.jpg}} \, %
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{LODO:}} \texttt{P\wrong{G}379\wrong{I}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Trad.:}} \texttt{P63791}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/001672_rp_194_aolp_tw_P63791.jpg}} \, %
    \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{LODO:}} \texttt{\wrong{0}X7655}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Trad.:}} \texttt{DX7655}]{
    		\includegraphics[width=0.32\linewidth]{imgs/5-results/000587_ac_587_aolp_tw_DX7655.jpg}} \hspace{1.5mm}
	}
	
    \caption{The predictions obtained by \trba on three images of the \aolp dataset. In general, the errors (outlined in red) under the leave-one-dataset-out~(LODO) protocol did not occur in challenging cases (e.g., blurry or tilted images); therefore, they were probably caused by \emph{differences in the training and test images}. Trad.: traditional-split protocol.}
    \label{fig:samples-errors-aolp}
\end{figure}

The second observation is that, regardless of the evaluation protocol adopted, no recognition model achieved the best results in every single dataset we performed experiments on.
For instance, although the \crnet model obtained the best average recognition rates, corroborating the state-of-the-art results reported recently in \citep{laroca2021efficient,silva2022flexible}, it did not reach the best results in the \chineselp, \aolp, \ssigsegplate and \dataset datasets in either protocol.
These results emphasize the importance of carrying out experiments on multiple datasets, with \glspl*{lp} from different countries/regions, especially under the leave-one-dataset-out protocol because six different models obtained the best result in at least one~dataset.

The third observation is that the \dataset dataset proved very challenging since all the recognition models trained by us, as well as both commercial systems, failed to reach recognition rates above $70$\% on its test set images. 
The main reason for such disappointing results is the large number of motorcycle images, which are very challenging in nature (as discussed in Section~\ref{sec:related_work}).
For example, OpenALPR correctly recognized $3{,}772$ of the $4{,}000$ cars in the test set ($94.3$\%) and only $1{,}827$ of the $4{,}000$ motorcycles in the test set ($45.7$\%).
These results accentuate the importance of the proposed dataset for the accurate evaluation of \gls*{alpr} systems, as it avoids bias in the assessments by having the same number of ``easy'' (cars with single-row \glspl*{lp}) and ``difficult'' (motorcycles with two-row \glspl*{lp})~samples.

We also did not rule out challenging images when selecting the images for the creation of the dataset.
Figure~\ref{fig:samples-dataset-challenging} shows some of these images along with the predictions returned by \trba (traditional-split) and OpenALPR, which were the model trained by us and the commercial system that performed better on this dataset.
The results are in line with what was recently stated by~\cite{zhang2021robust_attentional}: that recognizing \glspl*{lp} in complex environments is still far from~satisfactory.

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={small},captionskip=0.25pt}
    
    \resizebox{\linewidth}{!}{
    \subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{HLP\wrong{A}594}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{HLP4594}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{HLP4594}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/img_004226-gt-HLP4594-pred-HLPA594.jpg}} \, %
	\subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{PPY6\wrong{0}26}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{PPY6\wrong{0}26}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{PPY6C26}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/img_031372-gt-PPY6C26-pred-PPY6026.jpg}} \, %
    \subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{QRE4E6\wrong{Z}}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{QRE4E62}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{QRE4E62}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/011561_img_013352_rodosol-alpr_me_QRE4E62.jpg}} \, %
	}
	
	\vspace{2mm}
	
	\resizebox{\linewidth}{!}{
    \subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{QRG6D57}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{\wrong{-------}}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{QRG6D57}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/018662_img_027972_rodosol-alpr_me_QRG6D57.jpg}} \, %
	\subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{O\wrong{O}M8060}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{O\wrong{O}M8060}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{ODM8060}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/img_000722-gt-0DM8060-pred-00M8060.jpg}} \, %
    \subfloat[][\centering \phantom{\,} \resizebox{\adjj}{!}{\phantom{AAAA}\textbf{\trba:}} \texttt{MR\wrong{O}3095}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\textbf{OpenALPR:}} \texttt{MR\wrong{O}3095}\hspace{\textwidth} \phantom{\,}  \resizebox{\adjj}{!}{\phantom{AAAAa}\textbf{GT:}} \texttt{MRU3095}]{
		\includegraphics[width=0.32\linewidth]{imgs/5-results/img_020142-gt-MRU3095-pred-MR03095.jpg}} \, %
	}
	
    \caption{Some \gls*{lp} images from \dataset along with the predictions returned by \trba and OpenALPR. Note that one character may become very similar to another due to factors such as blur, low/high exposure, rotations and occlusions.
    For correctness, we checked if the ground truth (GT) matched the vehicle make and model on the \gls*{denatran}~database.}
    \label{fig:samples-dataset-challenging}
\end{figure}

Lastly, it is important to highlight the number of experiments we carried out for this work.
We trained each of the $\numbaselines$ chosen \gls*{ocr} models $10$ times: once following the split protocols traditionally adopted in the literature (see Table~\ref{tab:results-traditional-split}) and nine for the leave-one-dataset-out evaluation (see Table~\ref{tab:results-leave-one-dataset-out}).
We remark that a single training process of some models (e.g., \trba and \vitstrbase) took several days to complete on an NVIDIA Quadro RTX $8000$ GPU.
In fact, we believe that this large number of necessary experiments is precisely what caused a leave-one-dataset-out evaluation to have not yet been performed in the~literature.

