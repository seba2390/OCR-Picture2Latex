\abstract{
\gls*{alpr} systems have shown remarkable performance on \glspl*{lp} from multiple regions due to advances in deep learning and the increasing availability of datasets.
The evaluation of deep \gls*{alpr} systems is usually done within each dataset; therefore, it is questionable if such results are a reliable indicator of generalization ability.
In this paper, we propose a traditional-split \textit{versus} leave-one-dataset-out experimental setup to empirically assess the cross-dataset generalization of $12$ \gls*{ocr} models applied to \gls*{lp} recognition on nine publicly available datasets with a great variety in several aspects (e.g., acquisition settings, image resolution, and \gls*{lp} layouts).
We also introduce a public dataset for end-to-end \gls*{alpr} that is the first to contain images of vehicles with Mercosur \glspl*{lp} and the one with the highest number of motorcycle images.
The experimental results shed light on the limitations of the traditional-split protocol for evaluating approaches in the \gls*{alpr} context, as there are significant drops in performance for most datasets when training and testing the models in a leave-one-dataset-out~fashion.
\ifscitepress
\else
\vspace{3mm}
\fi
}