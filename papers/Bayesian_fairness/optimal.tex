\section{Algorithms} 
\label{sec:optim-balanc-bayes}
 
The algorithms we employ in this paper are based on gradient descent. We compare the full Bayesian framework with the simpler approach of assuming that the marginal model is the true one. In particular, for the Bayesian framework, we directly optimize~\eqref{eq:bayes-decision-problem-weights}. Using the marginal simplification, we maximize~\eqref{eq:oracle-decision-problem-weights} with respect to the marginal model $\Pr_\bel$.
%\footnote{For sequential allocation, we employ the same approach, and we do not explicitly consider feedback effects, i.e. we only consider a time horizon $T=1$ in the optimisation problem. However, the data we observe depend on our decisions $a_t$.} 


\subsection{Balance gradient descent}
\label{sec:credible-fairness}

Again, as in the Bayesian setting, we have a family of models
$\set{P_\param}$ with a corresponding subjective distribution
$\bel(\param)$. In order to derive algorithms, we shall focus on the quantity:
\begin{equation}
  C(\pol, \param)
   \defn
   \sum_{y,z} \big\|\sum_x \pol(a \mid x)  \Delta_\param(x,y,z)\big\|_p,
\end{equation}
to be the deviation from balance for decision rule $\pol$ under
parameter $\param$, where
\begin{equation}
  \Delta_\param(x,y,z) \defn P_\param(x, z \mid y)
  - P_\param(x \mid y) P_\param(z \mid y).
\end{equation}
Then the Bayesian balance of the policy is $f(\pol) = \int_\Param C(\pol,  \param) \dd \bel(\param)$.

In order to find a rule trading off utility for balance, we can
maximize a convex combination of the expected utility and
deviation specified in \eqref{eq:bayes-decision-problem-weights}. In particular, we can look for a parametrized rule $\pol_w$
solving the following unconstrained maximization problem.
\begin{align}
  \max_{\pol_w} &
           \int_\Param
           \val_\param(\pol_w)
           \dd \bel(\param),
\nonumber \\
&
  \val_\param(\pol_w) \defn 
                (1 - \lambda) \E_\param^{\pol_w} \util 
                - \lambda C(\pol_w, \param)
  \label{eq:penalty}
\end{align}
To perform this maximization we use parametrized policies and
 stochastic gradient descent.  In particular, for a finite set
$\CX$ and $\CY$, the policies can be defined in terms of parameters
$w_{xa} = \pol(a \mid x)$. Then we can perform stochastic gradient
descent as detailed in Section \emph{Gradient calculations for optimal balance decision} of supplementary materials, by sampling $\param \sim \bel$ and calculating the gradient for each sampled $\param$.

For the \emph{marginal} decision rule, we employ the same approach, but instead of sampling the parameters from the posterior, we use the parameters of the marginal model. The approach is otherwise identical.

%\dcp{following doesn't look useful, ... and don't think we want to talk
%about $k$ samples or Thompson.} 
%For instance, as in previous
%Section, if we only take $k < \infty$
%samples to perform the gradient ascent over, the policy becomes an
%instance of $k$-Thompson sampling. When $p = q = 2$, the gradient
%procedure is quite simple, \ifdefined \longver and is given in
%App.~\ref{sec:gradient}, \else but we omit for brevity. 
% \fi
%


%\emph{Marginal fairness}
%\label{sec:marginal-fairness}
%
%\dcp{careful, because I think we discredited this approach of using the marginal
%model earler in the paper}\yang{not addressed}

%As an approximation, we can maximize with respect to \ifdefined
%\longver the marginal model, i.e.
%\[
%  f_\bel(w) \defn 
%  (1 - \lambda) \E_\bel^\pol \util 
%  - \lambda \sum_{z,y} \abs*{\sum_x \pol_w(a \mid x)
%  D_\bel(x,y,z)}^p,
%\]
%where $\E_\bel, D_\bel$ are defined in terms of the marginal distribution 
% $\Pr_\bel \defn \int_\Param P_\param
%\dd{\bel}(\param)$.
%\else
%the marginal
%distribution
% $\Pr_\bel \defn \int_\Param P_\param
%\dd{\bel}(\param)$.
%\fi
%This could be a useful formulation if calculating the marginal is easier
%than sampling.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "subjective-fairness-nips.tex"
%%% End:
