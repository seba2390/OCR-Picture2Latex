\subsection{Bayesian Balance}
\label{sec:bayesian-balance}

In the Bayesian setting, we would like our decisions to take into
account the impact on all possible models.
While perfect balance is generally achievable, it turns out that
sometimes only
a trivial decision rule can satisfy it
in a setting with model
uncertainty (where balance
must hold exactly, for all possible model
parameters).
%
%We begin with a lemma that shows that perfect balance is generally unachievable.
%
\begin{theorem}
  A trivial decision rule of the form $\pol(a \mid x) = p_a$ can always satisfy balance for a Bayesian decision problem. However, it may be the only balanced decision rule, even when a non-trivial balanced policy can be found for every possible $\param \in \Param$.
  \label{lem:trivial-balance}
\end{theorem}

The proof, as well as  an example illustrating this
result, are in the supplementary materials. For this reason, we consider the the $p$-norm of the deviation from fairness with respect to our belief $\bel$:
\begin{definition}[Bayesian Balance]
  We say that a decision rule $\pol(\cdot)$ is $(\alpha,p)$-Bayes-balanced with respect to $\bel$ if:
  \begin{align}
    f(\pol) &\defn 
    \int_{\Param }
    \sum_{a, y, z}
    \biggl |\sum_x \pol(a | x)
    [P_\param(x, z | y)
    \nonumber \\
    &- P_\param(x | y) P_\param(z | y) ] 
    \biggr |^p
    \dd \bel(\param) 
    \leq \alpha^p.
      \label{eq:balanced-bayes-norm}
  \end{align}
\end{definition}

This definition captures the expected deviation from balance of  policy $\pol$, for a Bayesian DM under their belief $\bel$. It measures the deviation of the specific policy $\pol$ from perfect balance with respect to each possible parameter $\param$, and weighs it according to the probability of that model. This
provides a graceful trade-off between achieving near-balance in the most likely models, while avoiding extreme unfairness in less likely ones.
\iffalse
Achieving perfect fairness in \eqref{eq:balanced-bayes-norm},
i.e. ensuring $f(\pol) = 0$, suggests two sufficient conditions. One
is $x \indep z \mid y$, which is something that depends entirely on
$P_\param$ rather than the decision rule itself.  However, we cannot
rely on this condition to hold for all models $\param$.  A second
sufficient condition for $\alpha = 0$
in~\eqref{eq:balanced-bayes-norm} is that the distribution on actions
$a$ is orthogonal to the difference between the joint distribution of
$z, y$ and their marginals conditioned on $x$. This difference is
essentially measuring the amount of dependence, so to achieve perfect
independence we should be orthogonal for every observation $x$ and
parameter setting $\param$, which is generally not possible, as we
already argued in Lemma~\ref{lem:trivial-balance}.
\fi

Why not use a single point estimate for the model, instead of the full Bayesian approach? This would entail simply measuring balance (and utility) with respect to the marginal model
$\Pr_\bel \defn \int_\Param P_\param \dd \bel(\param)$.
\begin{definition}[Marginal balance]
A decision rule $\pol(\cdot)$ is $(\alpha,p)$-marginal-Balanced if  $\forall a, y,  z$:
\begin{align}
  \sum_{a, y, z}  \biggl |\sum_x \pol(a | x)
    \left[\Pr_\bel(x, z | y)
    - \Pr_\bel(x | y) \Pr_\bel(z | y) \right]\biggl|^p &\leq \alpha.
  \label{eq:marginal-balance}
\end{align}
\label{def:marginal-balance}
\end{definition}

One problem with this, which we will see in our experimental results,
is that the DM would assume that the marginal model is the correct one, and
may be very unfair towards other high-probability models.

Still, both balance conditions can provide a bound on balance with respect
to the true model.
%This balance definition easily leads to a theorem later in this section characterizing how the noise in understanding the model (from the data) amounts to the violation of fairness.
For this, denote the true underlying model as $\theta^*$, and define the $(\epsilon,\delta)$-accurate belief.
%
\begin{definition}
  We call $\bel(\param)$ an $(\epsilon,\delta)$-accurate belief with respect to the true model $\param^* \in \Param$, if with $\bel$-probability at least $1-\delta$, $\forall x,y,z$: $$|P_{\theta}(x|y,z) -P_{\theta^*}(x|y,z)| \leq \epsilon,~| P_{\theta}(x|y)-P_{\theta^*}(x|y)| \leq \epsilon,$$  i.e. that set $\Param_\epsilon$ for which the above conditions hold has measure $\bel(\Param_\epsilon) \geq 1 - \delta$.
\end{definition}

Under some conditions the balance achieved through either definition
provides an approximation to balance
under the true model, as shown by the following theorem.
%
\begin{theorem}\label{noise:model}
  If a decision rule satisfies either $(\alpha,1)$-marginal-balance or $(\alpha,1)$-Bayes-balance for $\bel$ or both, and $\bel$ is $(\epsilon,\delta)$-accurate, then the resulting decision rule is a $$(\alpha+2|\Act| \cdot |\Sns| \cdot |\Out|\cdot(\epsilon+\delta),1) \text{-balanced}$$ decision rule w.r.t. the true model $\theta^*$.
\end{theorem}

This theorem says that if our belief $\bel$ is concentrated around the
true model $P_{\theta^*}$, and our decision rule is fair with respect
to either definition, then it is also fair with respect to the true
model.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "subjective-fairness-nips"
%%% End:
