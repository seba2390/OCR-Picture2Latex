\section{Introduction}
\label{sec:introduction}
Fairness is a desirable property of policies applied to a
population of individuals. For example, college admissions should be
decided on variables that inform about merit, but fairness may also
require taking into account the fact that certain communities are
inherently disadvantaged.  At the same time, a person should not feel
that another in a similar situation obtained an unfair advantage.  All
this must be taken into account while still optimizing a decision
maker's utility function.

Much of the recent work on fairness in machine learning has focused on
analysing sometimes conflicting definitions. In this paper we do not
focus on proposing new definitions or algorithms. We instead take a
closer look at informational aspects of fairness. In particular, by adopting a Bayesian viewpoint,
we can explicitly take into account model uncertainty, something that
turns out to be crucial for fairness.

Uncertainty about the underlying reality has two main
effects. Firstly, most notions of fairness are defined with respect to
some latent variables, including model parameters. This means that we
need to take into account uncertainty in order to be fair. Secondly,
in many problems our decisions determine what data we will collect in
the future. Ignoring uncertainty may magnify subtle biases in our model.

By viewing fairness through a Bayesian decision theoretic perspective,
we avoid these problems.  In particular, we demonstrate that Bayesian
policies can optimally trade off utility and fairness by explicitly
taking into account uncertainty about model parameters.

We consider a setting where a decision maker (DM) makes a sequence of
decisions through some chosen policy $\pol$ to maximise her expected
utility $\util$. However, the DM must trade off utility with
some fairness constraint $\fair$. We assume the existence of some
underlying probability law $P$, so that the decision problem, when $P$
is known, can be written as:
\begin{align}
  \max_\pol  (1 - \lambda) \E^\pol_P \util - \lambda \E^\pol_P \fair,
  \label{eq:oracle-decision-problem-weights}
\end{align}
where $\lambda$ is the DM's trade-off between fairness and utility.\footnote{We do not consider the alternative constrained problem i.e.  $\max \cset{\E^\pol_P \util}{\E^\pol_P \fair \leq \epsilon}$, in the present paper.} In this paper we adopt a Bayesian viewpoint and assume the DM has some belief $\bel$ over some family of distributions $\family \defn \cset{P_\param}{\param \in \Param}$, which may contain the actual law, i.e. $P_{\theta^*} = P$ for some $\theta^*$.


The DM's policy $\pol$ defines what actions $\act_t \in \Act$ the DM
takes at different (discrete) times $t$ depending on the available
information. More precisely, at time $t$ the DM observes some data
$\obs_t \in \Obs$, and depending on her belief $\bel_t$ makes a
decision $\act_t \in \Act$, so that $\pol(\act_t \mid \bel_t, \obs_t)$
defines a probability over actions for every possible belief and
observation.  The DM's objective is to maximize her expected
utility. We model this as a function with structure $\util : \Act
\times \Out \to \Reals$, where $\Out$ is a set of \emph{outcomes}.
The fairness concept we focus on in this paper is a Bayesian version
of {\em balance}~\citep{kleinberg2016inherent}, which depends on the
policy at time $t$.  In the Bayesian setting, information is
central. The amount of uncertainty about the model parameters directly
influences how fairness can be achieved. Informally, the more
uncertain we are, the more stochastic the decision rule is.

\paragraph{Our contributions.} In this paper, we develop a framework for fairness that is defined as being
appropriate to the available information for the DM. The motivation for the Bayesian framework is that there can be a high degree of uncertainty, particularly when not a lot of data has been collected, or in sequential settings. This
informational notion of fairness is central to our discussion. It
entails that the DM should take into account how unfair she would be
under all possible models, weighted by their probability. While the
fairness concepts we use are grounded in conditional
independence~\citep{chouldechova2016fair,kleinberg2016inherent,HardtPNS16} type of
notions of fairness, we employ a Bayesian decision theoretic methodology. In particular, we cleanly separate model parameters from the DM's information, and the decision rule used by the DM. Fairness can thus be seen as a property of the decision rule with respect to the true model (which is used to \emph{measure} fairness), while
achieving it depends
on the DM's information (which is used to derive \emph{algorithms}).
\rev{
The Bayesian approach we adopt for fair decision making is generally applicable. In this paper, however, we focus on a simple setting so that we can work without model approximations, and proceed directly to the effect of uncertainty on fairness. The policies we obtain are qualitatively and quantitatively different when we consider uncertainty (by being Bayesian) compared to when we do not.}

The Bayesian algorithms we develop, based on gradient descent, take
into account uncertainty by considering fairness with respect to the
DM's information.  This inherent modeling of uncertainty allows us to
select better policies when those policies influence the data we
collect, and thus our knowledge about the model.  This is an important
informational feedback effect, that a Bayesian methodology can provide
in a principled way. We provide experimental results on the COMPAS
dataset~\citep{compas:dataset} as well as artificial data, showing the
robustness of the Bayesian approach, and comparing against methods that define fairness measures according to a single, marginalized model (e.g. \citep{HardtPNS16}). While we mainly treat the non-sequential setting, where the data is fixed, we can also accommodate sequential, bandits-style settings, as explained in Sections~\emph{The Sequential setting} and ~\emph{Sequential allocation}. The results provide a vivid
illustration of what can go wrong with a certainty-equivalent
approach to achieving fairness. 

All missing proofs and details can be found in our supplementary materials. 



\paragraph{Related work.}
%Fairness has long been a topic of study in game theory, and
%particularly in social choice theory, but there have been some novel
%developments in fairness arising from statistical problems.
%
%\iffalse
%In \textbf{game theory}, fairness is captured by a number of different
%concepts.  In resource allocation problems, envy-free allocation is a
%popular concept, but fairness can also be measured by other functions,
%such as the maximin fair share, or some global function of social
%welfare, such as the minimum utility received by an agent.  Another
%relevant concept to algorithmic fairness, is cost sharing.  In
%\textbf{behavioural game theory}, fairness is usually linked to
%players having a preference for calibrated outcomes.
%\citet{fehr1999theory} present a simple model linking fairness to
%selfish behaviour and co-operation motives. \citet{fehr2000fairness}
%examine fairness in the context of reciprocity and fair
%division. Finally, the subject of being fair to future generations
%also appears in problems of intertemporal omptimization in
%economics~\citep{arrow:consumption}. In this paper, however, we
%mainly focus of fairness in simple statistical decision rules.
%\fi


%We discuss existing fairness definitions in the non-subjective
%framework.  

%dcp dropped this. don't think it is needed
%Many existing fairness definitions amount to conditional
%independence assumptions between sensitive variables, decisions, and
%potential outcomes.


% Whether or not these assumptions can be satisfied
%depends on the exact nature of these assumptions.

%\yang{I moved the setting to later; now the discussions involving mathematics looks a bit unclear}
Recently algorithmic fairness has been studied quite extensively in
the context of statistical decision making.
But we are not aware of work that adopts
a Bayesian perspective. For instance,
\citep{dwork2012fairness,chouldechova2016fair,corbett2017algorithmic,kleinberg2016inherent,kilbertus2017avoiding}
studied fairness under the one-shot statistical decision making
framework. \citep{jabbari2016fair,joseph2016rawlsian} kicked off the
study of fairness in sequential decision making settings. Besides,
there is also a trending line of research on fairness in other machine
learning topics, such as clustering~\citep{Chierichetti2017fair},
natural language processing~\citep{BlodgettO17} and recommendation
systems~\citep{CelisV17}.  While the aforementioned works focused on
fairness in a specific context, such as classification,
\citep{corbett2017algorithmic} have considered how to satisfy some of
the above fairness constraints while maximizing expected utility.  For
a given model, they find a decision rule that maximizes expected
utility while satisfying fairness constraints. %s.t.~fairness. 
\citep{dwork2012fairness} consider
an individual-fairness approach,
and look for decision
rules that  are smooth in a sense that
similar \emph{individuals} are treated similarly.
\rev{
Finally, we'd like to mention the recent work of \citep{russell2017worlds}, which considers the problem of uncertainty from the point of view of causal modeling, with the three main differences being (a) They consider a PAC-like setting, rather than the Bayesian framework; (b) We show that the effect of uncertainty remains important even without varying the counterfactual assumptions, which is the main focus of that paper; (c) the Bayesian framework easily admits a sequential setting.}


In this paper, we focus on notions of fairness related to notions of
conditional independence, discussed next.
%. The next section provides more details on the most
%closely related works in that area.
%\yang{need to be more concrete. will add more discussions
%  later. }

\section{Preliminaries}

\label{sec:preliminaries}
\citep{chouldechova2016fair} considers the problem of fair prediction
with disparate impact. She defines an action\footnote{Called a
  ``statistic'' in their paper.} $\act$ as \emph{test-fair} with respect
to the outcome $y$ and the sensitive variable $z$ if $y$ is
independent of $z$ under the action and parameter $\param$, i.e. if
$y \indep z \mid \act, \param$.  While the author does not explicitly
discuss the distribution $P_\param$, it is implicitly assumed to be that of the true model. We slightly generalize it as follows:
\begin{definition}[Calibrated decision rule]
  A decision rule $\pol(a \mid x)$ is \emph{calibrated} with respect to some distribution $P_\param$ if $y, z$ are independent for all actions $a$ taken, i.e. if
  \begin{equation}
    \label{eq:calibrated-rule}
    P^\pol_\param(y, z \mid a) =  P^\pol_\param(y \mid a) P^\pol_\param(z \mid a),
  \end{equation}
  where $P_\param^\pol$ is the distribution induced by $P_\param$ and the decision rule $\pol$.
  \label{def:calibrated-rule}
\end{definition}
\iffalse
Note in particular that in the above definition there is only an indirect dependency on the decision rule itself. 
\cd{Not sure if the following is useful}
\begin{align*}
&\sum_\obs \pol(\act | \obs) P_\param(\obs | \out, \sns) P_\param(\out | \sns)
\\
\times
&\sum_\obs \pol(\act | \obs) P_\param(\obs)
\\
=&
\sum_\obs \pol(\act | \obs) P_\param(\obs | \out) P_\param(\out)
\\
\times&
\sum_\obs \pol(\act | \obs) P_\param(\obs | \sns).
\end{align*}
\fi


\citep{kleinberg2016inherent} also consider two balance conditions, which we re-interpret as follows:
\ifdefined \longver
\begin{align}
  v(\act) = P_\param^\pol(y = 1 \mid \act, z)  \tag{calibration}\\
  \E_\param^\pol(\util \mid y, z) = \E_\param^\pol(\util \mid y, z') \tag{balance}.
\end{align}
The authors show that these cannot can be simultaneously achieved
under the distribution $P_\param^\pol$ induced by the underlying
parameter $\param$ and the decision rule $\pol$.
A sufficient condition for
\emph{balance} is the conditional independence :
$\util \indep z \mid y, \param, \pol$, i.e. when
$P_\param^\pol(\util \mid y , z) = P_\param^\pol(\util \mid y)$. This
will be the basis of our own balance definitions. 

\goran{Chouldechova (extended version) and Kleinberg conditions seems to be in spirit similar. If so, 
can we compress the above and simply state our version of them? (Or we consider this our contribution?)}
\cd{Yes, we can remove the above.}
\fi
%Apart from the calibration condition,
%they consider two ``balance'' conditions, which we re-interpret as:
\begin{definition}[Balanced decision rule]
  A decision rule~\footnote{Here we simplified the notation of the decision rule so that $\pol(a \mid x)$ corresponds to the probability of taking action $a$
given observation $x$.} $\pol(a \mid x)$ is balanced with respect to some distribution $P_\param$ if $a, z$ are independent for all $y$, i.e. if
  \begin{equation}
    \label{eq:balanced-rule}
    P_\param^\pol(a, z \mid y) =  P_\param^\pol(a \mid y) P_\param^\pol(z \mid y),
  \end{equation}
  where $P_\param^\pol$ is the distribution induced by $P_\param$ and the decision rule $\pol$.
  \label{def:balanced-rule}
\end{definition}
These authors also work with the true model, while we will
slightly generalize the definition, stating balance with respect to
any model parameter.

Unfortunately, the calibration and balanced conditions cannot be
achieved simultaneously for non-trivial environments
\citep{kleinberg2016inherent}. This is also true for our more general
definitions, as we show in Theorem~\ref{thm:impossible} in the
Supplementary material.  From a practitioner's perspective,
we must choose either the calibration condition or the balanced conditions
in order to find a fair decision rule.
We work with the balanced
condition, because it gracefully degrades to settings with
uncertainty. In particular, balance involves equality in the
expectation of a score function (when writing the probabilities as the
expectations of 0-1 score functions; also depending on an observation
$x$) under different values of a sensitive variable $z$, conditioned
on the true (but latent) outcome $y$. Consequently, balance can always
be satisfied---by using a trivial, for example randomized
decision rule, being independent of $x$.  The same, however, does not
hold for the calibration condition under model uncertainty. Note that there also exist other fairness notions that go beyond disparate treatment \citep{zafar2017fairness}. This merits future studies, and is out of the scope of the current draft.
 
%Our work also conceptually relates to the fairness definition of \citet{dwork2012fairness}, which we elaborate in the appendix. 

%In this paper, we further explore the extension of the balance
%mechanism to a Bayesian setting in Section~\ref{sec:bayesian-balance}.

% \paragraph{Decision problems.}
% %While the aforementioned works focused on fairness in a specific
% %context, such as classification, 
% \citet{corbett2017algorithmic} have considered how to satisfy some of
% the above fairness constraints while maximizing expected utility.  For
% a given parameter value $\param$, they find a decision rule that
% maximizes expected utility under a hard constraint for independence.
%  \citet{dwork2012fairness} consider decision rules that maximize expected utility,
% but are smooth in a sense that similar individuals are treated similarly. Due to the page limit we explain the 
% the relation of this condition to our work in the appendix. 
%and  suggest that the correct notion of smoothness is with respect to the
%distribution induced by our beliefs and the observations on the
%variable $y$. \dcp{why call out the $y$ part of the data, specifically?}

%\yang{Cite more. general fairness results for AI/ML systems.}
%\dcp{check Berk Ustun work?}
%\yang{I checked out Berk's paper, doesn't seem to be quite relevant. or maybe I'm missing something.}

\ifdefined \longver On \textbf{sequential decision
  problems}, such as multi-armed bandits and reinforcement learning,
\citet{joseph:fair-bandits} define an algorithm as fair if it plays
arms with highest means most of the time. \citet{jabbari:fair-mdp}
study a similar notion for Markovian environments, whereby the
algorithm's is fair the algorithm is more likely to play actions that
have a higher utility under the optimal policy. However, this notion
of fairness relies on oracle knowledge, while we focus on
subjectivity.  \fi


% \paragraph{Our contribution.}
% While the Bayesian setting we are interested in is quite general, the focus of this paper is a balance condition in 
% analogous to \cite{kleinberg2016inherent}. This involves equality in the
% expectation of a score function (when writing the probabilities as the
% expectations of 0-1 score functions; also depending on an observation
% $x$) under different values of a sensitive variable $z$, conditioned
% on the true (but latent) outcome $y$. That said, the appendix contains material 
% demonstrate a link between balance and smoothness, as well as an algorithm for the \emph{calibration} definition of fairness.
%
%The paper is structured as follows. In the next section, we introduce the standard Bayesian decision
%setting. In Section~\ref{sec:bayesian-balance}, we introduce a Bayesian notion of balance, while Section~\ref{sec:fair-smoothness}, we relate smoothness to available information, and in turn to balance. Section~\ref{sec:optim-balanc-bayes} introduces two methods to trade off utility and fairness in a Bayesian framework, which are tested experimentally in Section~\ref{sec:experiments}. Section~\ref{sec:conclusion} concludes the paper. Technical material and secondary contributions are relegated to the appendix.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "subjective-fairness-nips.tex"
%%% End:
