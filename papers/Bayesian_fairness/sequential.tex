\subsection{The Sequential setting}
\label{sec:sequential}
\rev{We can extend the approach to a sequential setting,
where the information learned depends on the action.
For example, if we grant a loan application, we will only later discover if the loan is going to be paid off. This will affect our future decisions.
%
Analogous to other sequential decision making problems such as Markov decision processes\citep{Puterman:MDP:1994},} we need to solve the following optimization problem over a
time horizon $T$:
%
\begin{align}
  \max_\pol \E_{\bel_1} \left[\sum_{t=1}^T (1 - \lambda) U_t - \lambda F_t\right],
\end{align}
where $\pol$ now must explicitly map future beliefs $\bel_t$ to probabilities over actions.
 If the data that the DM obtains depends on her
decisions $a_t$, then she must consider adaptive policies, as the next
belief depends on what the data obtained by the policy was.

We can reformulate the maximization problem so as to explicitly include the future changes in belief:
\begin{align}
  \val^*(\bel_t) &\defn \sup_{\pol_t} \E^{\pol_t}_{\bel_t} \left[(1 - \lambda) U_t - \lambda F_t\right]
          \nonumber \\
          &+ \sum_{\bel_{t+1}} V^*(\bel_{t+1}) \Pr_{\bel_t}^{\pol_t} (\bel_{t+1}),
\end{align}
%
under the mild assumption that the set of reachable next beliefs is
finite (easily satisfied when the set of outcomes is finite). This formulation is not different from standard MDP formulation (e.g., the 
reinforcement learning settings) that features the trade-offs between \emph{exploration} (obtaining new knowledges) and \emph{exploitation} (maximizing utilities). We know in these settings a myopic policy will lead to sub-optimal solutions. 

However, just as in the bandits case \citep[c.f.][]{duff2002olc}), the
above computation is intractable, as the policy space is exponential in
$T$. For this reason, in this paper we only consider
{\em myopic policies}
that select a policy (and decision) that is optimal for the current
step $t$,
trading utility and fairness
as well as the value of `single-step' information.
%
%without explicitly considering possible future consequences.
%
A specific instance of this type of sequential version of the problem
is experimentally studied in Section {\em Sequential allocation}.%~\ref{sec:sequential-experiment}.
%However, when the same data is observed no matter what the DM's
%decision is, then myopic policies are optimal as the next belief is
%independent of the DM's decision.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "subjective-fairness-nips"
%%% End:
