\documentclass{INTERSPEECH2023}
\interspeechcameraready 

\usepackage{tabularx}

\title{AfriNames: Most ASR models ``butcher" African Names}

\name{
    Tobi Olatunji\textsuperscript{$\dagger$}\thanks{$\dagger$ Equal contribution.}$^{1,2,*}$, Tejumade Afonja$\textsuperscript{$\dagger$}^{3,4,*}$, 
    Bonaventure F. P. Dossou $^{5,6,7,8,*}$, 
    Atnafu Lambebo Tonja$^{9,*}$,
    Chris Chinenye Emezue$^{6,8,10,*}$, 
    Amina Mardiyyah Rufai$^{11,*}$, 
    Sahib Singh$^{12,*}$
}

\address{
    \small
    $^{1}$ Intron Health Inc
    $^{2}$ Georgia Institute of Technology
    $^{3}$ AI Saturdays Lagos
    $^{4}$ CISPA Helmholtz Center for Information Security
    $^{5}$ McGill University
    $^{6}$ Mila Quebec AI Institute
    $^{7}$ Lelapa AI
    $^{8}$ Lanfrica
    $^{9}$ Instituto PolitÃ©cnico Nacional
    $^{10}$ Technical University of Munich
    $^{11}$ Idiap Research Institute
    $^{12}$ Ford Motor Company
    $^*$Masakhane NLP
}

\email{
    tobi@intron.io, 
    tejumade.afonja@cispa.de, 
    bonaventure.dossou@mila.quebec, 
    atnafu.lambebo@wsu.edu.et, 
    chris.emezue@gmail.com, 
    amina.rufai@idiap.ch, 
    sahibsingh570@gmail.com
}


\begin{document}

\maketitle
 
\begin{abstract}
% 1000 characters. ASCII characters only. No citations.
Useful conversational agents must accurately capture named entities to minimize error for downstream tasks, for example, asking a voice assistant to play a track from a certain artist, initiating navigation to a specific location, or documenting a laboratory result for a patient. However, where named entities such as ``Ukachukwu" (Igbo), ``Lakicia" (Swahili), or ``Ingabire" (Rwandan) are spoken, automatic speech recognition (ASR) models' performance degrades significantly, propagating errors to downstream systems. We model this problem as a distribution shift and demonstrate that such model bias can be mitigated through multilingual pre-training, intelligent data augmentation strategies to increase the representation of African-named entities, and fine-tuning multilingual ASR models on multiple African accents. The resulting fine-tuned models show an 81.5\% relative WER improvement compared with the baseline on samples with African-named entities.
\end{abstract}

% TODO update tags
\noindent\textbf{Index Terms}: Speech recognition, named entity recognition, distribution shift, accented speech




\section{Introduction and Motivation}

Automatic Speech Recognition (ASR) powers voice assistants, which use machine learning and other artificial intelligence techniques to automatically interpret and understand spoken languages for conversational purposes. With the advent of breakthroughs such as 
% Google's Assistant, 
Amazon's Alexa, and Apple's Siri, 
% Samsung Bixby, Microsoft Cortana, 
etc., voice assistant technology has increasingly become a widespread technology with diverse applications  \cite{siegert2021speaker}.
% which range from businesses, service delivery, to healthcare applications, to education, etc. In recent years, the integration of conversational AI into smart devices has become common. 
However, as these devices gain adoption beyond the demographics of their training data, there is a need for more inclusive and robust AI agents with better spoken language understanding (SLU) and accent recognition capabilities
% However, as these devices gain adoption beyond the demographics of their training data, there is a growing need for more robust and inclusive agents with improved spoken language understanding (SLU) and accent recognition 
\cite{desot2019towards, adelani2021masakhaner}\footnote{https://techxplore.com/news/2022-09-effective-automatic-speech-recognition.html}.


%Automatic Speech Recognition (ASR) technology has grown tremendously in recent years, but there still exists a significant issue with its accuracy when it comes to recognizing African names. Despite the diversity of African names, ASR systems often struggle to transcribe them accurately. This issue is rooted in the limited training data used to develop ASR systems, which frequently lack representation from diverse language communities. For example, \cite{adelani2021masakhaner} addressed this challenge by developing NER datasets, models, and evaluations covering ten widely spoken African languages and found that ASR systems performed poorly when transcribing African names, often replacing them with incorrect transcriptions. The authors attributed this issue to the underrepresentation of African languages in the training data used to develop ASR systems.

Useful conversational agents must accurately capture named entities to minimize errors for downstream tasks. For example, in the command, ``Play Billie Jean by Micheal Jackson", conversational agents need to excel at 3 core tasks: Speech Recognition, Named Entity Recognition, and Entity Linking, to appropriately respond to commands. The ASR component of the system must correctly transcribe the speech, laying a good foundation for Named Entity Recognition (NER) \cite{Nguyen_Yu}, which is, in turn, necessary for effective Entity Linking. 

However, in the command ``Play `Trouble Sleep Yanga Wake Am' by Fela Anikulapo Kuti"\footnote{Fela is one of Africa's most legendary artists} spoken by a Nigerian with a thick Yoruba accent, the phonetic and linguistic variability of the heavily accented speech presents a double dilemma for such systems. Firstly, the heavy accent and tonality can be difficult for the system to recognize, and secondly, the use of out-of-vocabulary words can confuse the model, making it nearly impossible for the system to generate a correct response. Siri responds ``I couldn't find `trouble sleep younger we' by Fela and Kolapo Coochie in your library", effectively ``butchering" \footnote{To "butcher" a name means to mispronounce it, resulting in a significant deviation from the correct pronunciation.} the name, typifying the failures of similar agents on out-of-distribution named entities. More examples in Table \ref{tab:failure_examples}.
% 1) The heavily accented/tonal speech; and 2) the out-of-vocabulary words confuse the model making it nearly impossible for the system to divine a plausible response. 

%Recent research demonstrates the growing popularity of End-to-end (E2E) model approaches over hybrid models \cite{li2022recent}. These approaches also established the benefits of unsupervised or semi-supervised pre-trained models, trained on large corpora, as these models have proven capable of achieving human-level accuracy for high-resource languages and decent performance in downstream fine-tuning tasks such as multilingual and low-resource language applications \cite{conneau2020unsupervised, radford2022robust}


We hypothesize that the underrepresentation (and sometimes complete lack of) African named-entities in their training data may partly explain the model bias and eventual ``butchering" of African names by many voice assistants and conversational agents. %We investigate state-of-the-art (SOTA) ASR models' performance on accented African speech with African and non-African named entities. Furthermore, we design a data augmentation strategy to increase the representation of African-named entities in speech corpora and demonstrate the effectiveness of our strategy through fine-tuning experiments on the augmented data.

%We investigate state-of-the-art (SOTA) ASR models' performance on accented African speech with African-named entities by evaluating samples with African and non-African named entities, we design a data augmentation strategy to increase the representation of African-named entities in the speech corpora. We fine-tune ASR models on the augmented data and report results.

Our contributions are as follows:

%\section{Contribution}
%\begin{enumerate}
%    \item First to do speech NER for African accented English ASR
%\item Handcrafted Templating strategy
%\item Automated NER annotation strategy
%\item Improve African names ASR
%\end{enumerate}


%Problem
%ASR models "butcher" our names
%Demonstration of Problem
%We show results of "un-finetuned" ASR models on AfriNER entities in different datasets. -> AfriSpeech, Cv, SautiDB
%Solution
%AfriSpeech dataset
%Demonstration of solution
%Results of models "finetuned" on AfriSpeech dataset



%\begin{enumerate}
%    \item We are the first to investigate speech named-entity recognition for African accented English ASR. We design an effective strategy to evaluate ASR models on speech datasets with no prior NER annotations and demonstrate failures of existing SOTA and commercial ASR models on samples with African named entities
%    \item We create a novel speech corpus rich in African named-entities using a templating framework to augment existing corpora with native African named entities 
 %   \item We finetune pretrained models on the augmented accented speech dataset and show significant improvement in ASR performance and provide our best models as publicly available pre-trained checkpoints.
    
%\end{enumerate}




%\textbf{Another variant}

\begin{enumerate}
    \item We investigate the performance of state-of-the-art (SOTA) ASR models on African named-entities. To do this, we design an effective strategy to evaluate ASR models on speech datasets with no prior NER annotations. Our study highlights the failure of existing SOTA and commercial ASR models on samples with African named-entities
    \item We develop a data augmentation strategy to increase the representation of African-named entities, creating a novel speech corpus rich in African named-entities, and show that by fine-tuning pre-trained models on the augmented accented data, we significantly improve the ability of pre-trained models to recognize African named entities. We open-source the dataset and fine-tuned models\footnote{https://huggingface.co/datasets/tobiolatunji/afrispeech-200}.
    %\item We present AfriSpeech, a novel speech corpus rich in African named-entities, and show that by finetuning pretrained models on the augmented accented AfriSpeech data, we significantly improve the ability of pretrained models to recognize African entities.
    
\end{enumerate}



%NER research using speech data dates back as far as NER using text data (\cite{Nguyen_Yu}). Undeniably, Numerous research advancements have been made in this area, with the majority of them addressing the issues of NER from speech data's low performance in comparison to text, and the large amounts of annotated data required for training(\cite{Galibert_Rosset_Grouin_Zweigenbaum_Quintard,mdhaffar2022end}). A recent research by \cite{porjazovski2020end} showed work on accented speech for NER. 
%\begin{enumerate}
%    \item Voice assistant use cases and their adoption outside their primary markets
    
%\item Recent advances in ASR

%\item ASR performance on named entities
%\item Common african names
%\end{enumerate}

\begin{table*}[t]
\caption{Model behavior examples on native African named entities}
\centering
\small
\begin{tabularx}{\textwidth}{l|X}
\toprule
Model & Sentence\\
\hline
reference & \textbf{Ifeadigo} has been living at \textbf{Kaduna} with his wife \textbf{Chiamaka Orajimetochukwu} \\
\hline
%\multicolumn{2}{l}{\textbf{Commercial APIs}}\\

azure & \underline{if you're diego}. \\

% gcp & \underline{diego} has been living at his wife \\

aws & \underline{if you did good} has been living at \underline{kaduna} with his wife, \underline{she america or raji mo to} \\
%\arrayrulecolor{gray}\hline
%\multicolumn{2}{l}{\textbf{Monolingual finetuning}}\\

%hubert-large-ls960-ft & \underline{ifia di gun} has been living at \underline{cardonal} with his wife \underline{shia maca orraji mo tu truku} \\

% hubert-xlarge-ls960-ft & \underline{ifia di gun} ha been living at \underline{cardona} with his wife \underline{shia macca or ragi mo tu truco} \\

w2v2-lg-960h-lv60-self & \underline{fia digo} has been living at \underline{cadna} with his wife \underline{shi maca orajimo to truco o} \\

% w2v2-lg-960h & \underline{ifia digoun} not been living at \underline{caduna} with his wife \underline{shea macca or ra gi mou toul tru coul} \\

% w2v2-ft-swbd-300h & \underline{ifia digo} has been living at \underline{cadona} with his wife \underline{shi meca or ragimo too tro qo} \\
%\hline
%\multicolumn{2}{l}{\textbf{Multilingual finetuning}}\\

w22-lg-xlsr-53-en & \underline{ifia digu} has been living at \underline{kaduna} with his wife \underline{shiamaka orajimo tutruku} \\

% w2v2-xls-r-1b-english & \underline{ifia digo} has been living at \underline{kaduna} with his wife \underline{shiamaka orajimu tutuku} \\

% whisper-base & \underline{if you are de-goon}, that's when living at \underline{kaduna}, which is wife, \underline{shia makka, or rajimu, to kuku} \\

% whisper-small & \underline{ifya digo} has been living at \underline{kaduna} with his wife, \underline{shiyamaka orajimu tochuku} \\

% whisper-medium & \underline{ifeia digun} has been living at \underline{kaduna} with his wife, \underline{shiamaka or rajimu, to chuku} \\

whisper-large & \underline{ifeardigun} has been living at \underline{kaduna} with his wife, \underline{shiamaka or rajimu, to chukwu} \\

xlsr-general (Ours) & \underline{ifiadigo} has been living at \underline{kaduna} with his wife \underline{chiamaka orajimotochukwu} \\
Whisper-general (Ours) & \underline{ifeadigo} has been living at \underline{kaduna} with his wife \underline{chiamaka orahjimu tochukwu} \\
% Whisper-all (Ours) & \underline{ifeadigo} has been living at \underline{kaduna} with his wife \underline{chiamaka orajimotochukwu} \\
%\arrayrulecolor{black}
\bottomrule
\end{tabularx}
\label{tab:failure_examples}
\end{table*}

\section{Related work}
Developing ASR systems for low-resource languages remains challenging due to the scarcity of training data and resources. As a result, models trained on high-resource languages, such as English, do not perform well on low-resource languages \cite{lepak2021generalisation}. To address this, researchers have proposed several solutions such as cross-lingual representations where the system learns a shared representation for multiple languages \cite{conneau2017word}, data augmentation techniques \cite{feng2021survey}, and fine-tuning ASR model trained on high-resource languages on low-resource languages \cite{anaby2020not}.
Recent SOTA multilingual ASR models such as Whisper \cite{radford2022robust} -- trained on over 680K hours labeled speech samples, including multilingual speech corpora such as Common Voice \cite{commonvoice} -- have significantly improved the ASR landscape, outperforming their monolingual counterparts such as HuBERT \cite{hsu2021hubert}, wavLM \cite{chen2022wavlm}, and wav2vec2 \cite{baevski2020wav2vec} in various downstream tasks. 
Despite these breakthroughs, both open source and commercial ASR systems still exhibit racial bias \cite{koenecke2020racial}, higher error on accented speech \cite{hinsvark2021accented}, and incorrect transcriptions of named entities. Previous studies have highlighted challenges with named entity recognition (NER) for ASR and have investigated various methods to improve NER performance. For instance, French researchers \cite{galliano2009ester} outlined steps for assessing NER in french transcripts of radio broadcasts, while \cite{xiao2021automatic} evaluated Chinese accent ASR on an automatic speech query service (AVQS), highlighting the severe limitations of such systems for Mandarin users with multiple accents. More recently, \cite{mdhaffar2022end, caubriere2020we} attempted to extract semantic information directly from speech signals using a single end-to-end model that learns ASR and NER tasks together. However, none of this work focuses on named entities in African datasets, which presents a new area of research and its unique challenges.

% This work sheds light on the challenges of developing ASR systems for African laguages with a particular focus on named entity recognition. We identify new methods to improve ASR and NER performance on African languages and overcome the challenges. 


% % The onset of multilingual speech corpora like Common Voice \cite{commonvoice} led to the increased trend towards multilingual ASR and the release of large, robust multilingual pre-trained ASR models such as Whisper (90 languages) \cite{radford2022robust} and Wav2vec-xlsr (10 languages) \cite{Babu2022XLSRSC} along with several variations such as SpeechStew \cite{chan2021speechstew} and \cite{ritchie2022large} trained on multilingual speech corpora. These models achieved SOTA performance on many downstream tasks, outperforming monolingual models like HuBERT \cite{hsu2021hubert}, wavLM \cite{chen2022wavlm}, and wav2vec2 \cite{baevski2020wav2vec}, which were pre-trained or fine-tuned exclusively on monolingual corpora such as Librispeech \cite{panayotov2015librispeech}, LibriVox \cite{kearns2014librivox}, SWBD \cite{godfrey1992switchboard}, or WSJ \cite{paul1992design}. Other efforts achieved high ASR performance in other low resource languages such as Indian \cite{indicwav2vec}, Irish \cite{faste2022wav2vec}, Swedish \cite{al2021self}, and Mandarin, Japanese, Arabic, German \cite{yi2020applying}.


% % \subsection{Advances in accented English ASR}
% The challenge isn't limited to low-resource languages. Based on recent survey \cite{hinsvark2021accented}, the linguistic variability of accents presents hard challenges in accented English ASR systems in both data collection and modeling strategies. Promising approaches include training accent-specific models \cite{vergyri2010automatic, najafian2014unsupervised}, data augmentation such as speed perturbation \cite{fukuda2018data}, model generalization through multi-task learning \cite{jain2018improved, radford2022robust, li2018multi}, domain expansion \cite{ghorbani2019domain, houston2020continual}, pronunciation modification \cite{goronzy2004generating, lehr2014discriminative}, adaptation using auxiliary acoustic features \cite{grace2018occam, li2018multi, zhu2020multi}, accent embeddings \cite{viglino2019end, turan2020achieving}, and adversarial training \cite{sun2018domain, chen2020aipnet}. Our work focuses on domain adaptation to accented speech.  %Accent embeddings and adversarial training are more recent techniques that leverage deep learning to address the issue of accented speech recognition. Significant progress made in addressing the challenges posed by linguistic variability holds promise for improving the performance of ASR systems for a wide range of accents.

% % \subsection{Racial Bias in ASR} 
% % Racial bias is an important problem that needs to be addressed to ensure that ASR systems are fair and accessible to all individuals, regardless of racial or ethnic background. A study \cite{koenecke2020racial} found large racial disparities in the performance of five popular commercial ASR systems -- Amazon, Apple, Google, IBM, and Microsoft -- when transcribing structured interviews conducted with 42 white speakers and 73 black speakers. They found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. This bias is representative of their underlying training data. Most ASR systems work best for native English speakers but their accuracy plummets drastically with non-native English speakers \cite{hassan2022improvement, prasad-jyothi-2020-accents}. Similarly, performance gaps with accented English have been well demonstrated by \cite{doumbouya2021using, siminyu2021ai4d, babirye2022building, ogayo2022building}, with multiple parallel efforts \cite{gutkin2020developing, dossou2021okwugb, afonja2021learning, kamper2011multi} to create accented English datasets. 

% \subsection{Improving ASR for named entities and Speech NER} 
% Many studies have demonstrated progress and challenges in the field of NER for ASR and explored various methods to improve NER performance. French researchers \cite{galliano2009ester} outlined steps for evaluating NER on radio broadcast transcripts.  \cite{xiao2021automatic} evaluated Chinese accented ASR on an Automatic Voice Query Service (AVQS), highlighting the severe limitation of such systems for multi-accented Mandarin users. They improve the final quality of the AVQS system by pairing an end-to-end Transformer-CTC ASR model with fuzzy logic \cite{xiao2021automatic}. \cite{katerenchuk2014improving} and \cite{rangarajan2006detection} leverage prosodic features, \cite{ramabhadran2004use} integrate document metadata to improve NER. More recently, \cite{mdhaffar2022end, caubriere2020we} attempted to extract semantic information directly from speech signals with a single end-to-end model jointly learning ASR and NER tasks.

\section{Methodology}
\begin{figure}[h]
\includegraphics[width=\columnwidth]{images/AfriNames.drawio_new.pdf} %
\centering
\caption{AfriNames dataset augmentation process.}
\label{fig:method}
\end{figure}

\subsection{African Named-Entity Augmentation Workflow}
\textbf{Western vs African-named entities:} We use the term ``Western named entities" to refer to names that are commonly used in Western cultures and languages, such as Laura and Buenos Aires, and that may not have direct translations in African languages \footnote{Due to the influences of colonization and globalization, many Western names have been adopted in African cultures. Therefore, while these names may not have direct translations in African languages, they can still be used and recognized in African contexts. Our work focus specifically on African named entities that are derived from African languages. }. In contrast, we use the term ``African named entities" to denote names, places, and cultural references that are derived from African languages and cultures, and that may not be commonly used or recognized outside of those contexts.

\textbf{Approach:} 
We address the generalization problem as a domain shift, depicted in Figure \ref{fig:method}. Our initial dataset, denoted as $D_1$, consists of Western audio samples $X^{E_{1}}$ and their corresponding transcripts $Y^{E_{1}}$. We employ a pre-trained named entity recognition (NER) model $R_1$ to extract named entities (NEs) from $Y^{E_{1}}$, resulting in the predominantly Western named entity list $E_1$. To inject African named entities, we mask tokens in randomly selected samples from $Y^{E_{1}}$ that match the entities in $E_1$. This process generates the modified dataset $D_1'$ with modified transcripts $Y'^{E_{1}}$. We then randomly insert tokens from a curated African named-entity list $E_2$ to replace the masked tokens in $Y'^{E_{1}}$, creating an augmented dataset $D_2$ with modified transcripts $Y^{E_{2}}$. These transcripts are sent to African crowd-sourced workers for recording, resulting in a new corpus named $D_2'$ with augmented pairs $\{(X^{E_{2}},Y^{E_{2}})\}$. This novel dataset comprises accented audio samples and augmented transcript pairs, combining distributions from $D_1$ and $D_2$ with Anglo-centric named entities $E_1$ and African named entities $E_2$. Next, we use a specialized NER model $R_2$ to annotate all western and African named entities (called $E_3$) present in $D_2$. Using these NER annotations, we select the subset of $D_2$ with NEs. This NE subset $D_3$ (called AfriNER) contains accented speech $X^{E_{3}}$ and corresponding transcripts $Y^{E_{3}}$ with named entities extracted from both $Y^{E_{1}}$ and $Y^{E_{2}}$. Additionally, using curated African NE list $E_2$, we also filter $Y^{E_{3}}$ to create $D_4$ confirmed to contain African NEs (called AfriVal).


% We address the generalization problem as a domain shift, depicted in Figure \ref{fig:method}. Our initial dataset, denoted as $D_1$, consists of Western audio samples $X^{E_{1}}$ and their corresponding transcripts $Y^{E_{1}}$. We employ a pre-trained named entity recognition (NER) model $R_1$ to extract named entities (NEs) from $Y^{E_{1}}$, resulting in the predominantly Western named entity list $E_1$. To inject African named entities, we mask tokens in randomly selected samples from $Y^{E_{1}}$ that match the entities in $E_1$. This process generates the modified dataset $D_1'$ with modified transcripts $Y'^E_{1}$. We then randomly insert tokens from a curated African named-entity list $E_2$ to replace the masked tokens in $Y'^{E_{1}}$, creating an augmented dataset $D_2$ with modified transcripts $Y^{E_{2}}$. These transcripts are sent to African crowd-sourced workers for recording, resulting in a new corpus named $D_2'$. This novel dataset $D_2'$ comprises accented audio samples and augmented transcript pairs $\{(X^{E_{2}},Y^{E_{2}})\}$ along with the audio and transcri$D_1$ and $D_2$ with Anglo-centric named entities $E_1$ and African named entities $E_2$. Next, we use a specialized NER model $R_2$ to annotate all western and African named entities (called $E_3$) present in $D_2$. Using these NER annotations, we select the subset of $D_2$ with NEs. This NE subset $D_3$ (AfriNER) contains accented speech $X^{E_{3}}$ and corresponding transcripts $Y^{E_{3}}$ with named entities extracted from both $Y^{E_{1}}$ and $Y^{E_{2}}$. Additionally, using curated African NE list $E_2$, we also filter $Y^{E_{3}}$ to create $D_4$ confirmed to contain African NEs (AfriValidated).

% A real-world example of $D^{E_{1}}$ is LibriSpeech \cite{panayotov2015librispeech}, a 1,000-hours speech-text dataset from English-only  audiobooks. The resulting ASR model, such as Wav2vec2 \cite{baevski2020wav2vec}, therefore, generalizes poorly to African named entities (Table \ref{tab:failure_examples}).

% We model the generalization problem as a domain shift and illustrate the workflow of the proposed solution in Figure \ref{fig:method}. $D_1$ is a predominantly western dataset $\{(X^{E_{1}},Y^{E_{1}})\}$ with audio and transcript pairs, originating from a distribution $D^{E_{1}}$ induced by Anglo-centric named-entities $E_1$. Model $M_1$ is randomly initialized and trained on $D_1$, learning the mapping $f: X^{E_{1}} \longrightarrow Y^{E_{1}}$, leading to a pretrained model $M_{1}^{E_{1}}$. 
% Pre-trained NER model $R_1$ extracts named entities from $D_1$ transcripts  resulting in predominantly western named entities list $E_1$. Masking $E_1$ tokens from randomly selected samples in $D_1$ produces $D_1'$. Tokens from curated African named-entity list $E_2$ randomly replace masked tokens in $D_1'$. Augmented subset $D_1'$ + $D_1$ creates $D_2$ transcripts which are sent to African crowd-sourced workers for recording to create $D_2'$, a novel corpus of accented audio and augmented transcript pairs originating from a distribution $D^{E_{1}}$ and $D^{E_{2}}$ induced by African and Anglo-centric named-entities $E_1 + E_2 = E_3$. 
% A Specialized NER model $R_2$ extracts African and western named entities $E_3$ from $D_2$ including any African named entities originally in $D_1$. Accented audio recordings of $D_1'$ prompts and $D_1'$ transcripts are isolated to create $D_3$, the subset of $D_2'$ confirmed to contain African named entities (AfriValidated).



%$D^{E_{2}}$ due to the wide distribution shift.
%$M_1$ is an ASR model trained on a predominantly Western dataset $D_1 = \{(X^{E_{1}},Y^{E_{1}})\}$ with audio and transcript pairs, $X^{E_{1}}$ and $Y^{E_{1}}$, originating from a distribution $D^{E_{1}}$ induced by Anglo-centric named-entities $E_1$. The ASR model then learns the mapping $f: X^{E_{1}} \longrightarrow Y^{E_{1}}$, leading to a pretrained model $M_{1}^{E_{1}}$. 
%On the other end of the spectrum are Afro-centric entities $E_2$ like `Fela' and `Ifeadigo'. 
%Pre-trained NER model $R_1$ extracts named entities from the transcripts of a predominantly western speech corpus $D_1$ resulting in a corpus with masked named entities $D_1'$. African-named entities $E_2$ randomly replace masked tokens for a curated subset of $D_1'$. Augmented subset $D_1'$ + $D_1$ creates $D_2$ transcripts which are sent to African crowd-sourced workers for recording. Specialized NER model $R_2$ extracts African and western named entities from $D_2$ which are further filtered by $E_2$ to create $D_3$ the AfriValidated dataset


%Such a dataset does not have  represented as $E_1x$ in speech signal and $E_1y$ in transcripts, learns a function but predicts over a different distribution $D_2$ with African named-entities.
%\begin{equation}\label{eq1}
%    \theta = P(Y_1|X_1,E_1x) 
%\end{equation}


%Chris version............


%We model the generalization problem as a domain shift where an ASR model $M_1$ trained on a predominantly western dataset $D_1 = \{(x_{i},y_{i})\}_{i=1}^{N}$ made of $N$ pairs, $(x_{i},y_{i})$, where $x_i$ is an audio sample and $y_{i}$ is the corresponding transcript. and named-entities $E_1$ represented as $E_1x$ in speech signal and $E_1y$ in transcripts, learns a function (eq \ref{eq1}) but predicts over a different distribution $D_2$ with African named-entities.
%\begin{equation}\label{eq1}
 %   \theta = P(Y_1|X_1,E_1x) 
%\end{equation}


% chris version.............



% is tested on speech from a different data distribution D2 of accented speech with X2, Y2 pairs where X2 = (x1, x2, ..., xT) and Y2=(y1, y2,..., yT) with African named-entities E2.

\subsection{Datasets}
In this study, we primarily explore the AfriSpeech-200 dataset, a 200.91 hours novel accented English speech corpus rich with African-named entities, curated for clinical and general domain ASR using the augmentation process described above. 67,577 prompts were recorded by 2,463 unique crowdsourced African speakers from 13 Anglophone countries across sub-Saharan Africa and the United States. The average audio duration was 10.7 seconds (Table \ref{tab: dataset stats}).

%We explore two additional datasets: (1) \textbf{SautiDB} \cite{afonja2021sautidb}, a dataset of Nigerian accent recordings with 919 audio samples, a sampling rate of 48kHz each, for a total amount of 59 minutes of recordings; (2) \textbf{Common Voice English Accented Dataset}, a subset of English Common Voice (version 10) \cite{commonvoice} with majority American and European English accents removed.
% and (3) \textbf{Medical Speech}\footnote{\url{https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent}} which contains 6,661 audio utterances for common medical symptoms like \textit{knee pain} or \textit{headache}, for a total of 8 hours of recordings;

%\begin{table}[b]
%\small
%\centering
%\begin{tabular}{l|l|l|l|l|l}
%\hline
%\textbf{Test Samples} & \#n & \#entities & \multicolumn{3}{c}{Categories}\\
% & & & PER & ORG & LOC  \\
%\hline
%AfriSpeech & 2723 & 2478 & 1347 & 398 & 733\\
%SautiDB & 138 & 92& 71 & 3 & 18 \\
% MedSpeech & 622& 2463 & 9 & 0 & 2 & 7 & 0  \\
%CV-En-Accented & 334 & 170 & 76 & 38 & 56  \\

%\hline
%\end{tabular}
%\caption{Dataset entity category counts.}
%\label{tab:splits}
%\end{table}

% \paragraph{\color{red}- Tables for dataset split and samples with NER Tags} 
\begin{table}[h]
\centering
\small
\caption{AfriSpeech-200 Dataset statistics}
\begin{tabular}{l|l|l|l}
\toprule 
%\multicolumn{4}{c}{\textbf{Dataset Stats}} \\
%\hline 
& Train & Dev & Test\\
\midrule
Duration (hrs) & 173.4  &  8.74 & 18.77  \\
\# General domain clips & 21682  &  1407 & 2723  \\
Unique Speakers & 1466   &  247 & 750 \\
Accents & 71 &  45 & 108 \\
%Average Audio duration & 10.7 seconds  \\
\hline
\multicolumn{4}{c}{\textbf{Named Entities Category Counts}}\\
%\multicolumn{4}{c}{\textbf{NER Category Counts}}\\
\hline
% Entities & 20527  &  1233 & 1869  \\
PER & 11011 &  669 & 1064  \\
ORG & 6322 &  372 & 279 \\
LOC & 3194 &  192 & 526  \\
\bottomrule
%\multicolumn{2}{c}{\textbf{Speaker Gender Ratios - \# Clip \%}}\\
%\hline
%Female & 57.11\%  \\
%Male & 42.41\%  \\
%Other/Unknown & 0.48\% \\
%\hline
%\multicolumn{2}{c}{\textbf{Speaker Age Groups - \# Clips}}\\
%\hline
%$<$18yrs & 1,264 (1.88\%) \\
%19-25 & 36,728 (54.58\%)  \\
%26-40 & 18,366 (27.29\%)  \\
%41-55 & 10,374 (15.42\%) \\
%$>$56yrs & 563 (0.84\%)  \\
%\hline
%\multicolumn{2}{c}{\textbf{Clip  Domain - \# Clips}}\\
%\hline
%Clinical & 41,765 (61.80\%)  \\
%General & 25,812 (38.20\%) \\
%\hline
\end{tabular}
\label{tab: dataset stats}
\end{table}

\subsection{AfroAug: African Named-Entity Augmentation}\label{section:name-aug}
%Neural networks learn concepts from training data. Where transcripts or prompts in training data are predominantly Western (e.g. Common Voice and LibriSpeech \cite{ardila2019common, panayotov2015librispeech}) and African-named entities are sparse, such ASR systems fail at correctly transcribing African names like ``Ogochukwu" (Igbo), ``Malaika" (Swahili), or ``Uwimana" (Rwandan), while excellently transcribing Western names like ``Lauren" and ``Bryan"-- representative of the bias in their training corpora. 

To increase the representation of African named entities, we start with a corpus $D_1$ using large open-source predominantly western corpora: Wikitext-103 \cite{merity2016pointer} and scrape African entertainment and news websites to increase the representation of African content. We augment this dataset using two main strategies. 
We curate a list $E_2$ of approximately 100k African names using a database of 90,000 African names from \cite{anderson2013using}, 965 Nigerian Igbo names from \cite{okagbue2017personal}, and 1,000 African names obtained from freely available textbooks, online baby name websites, oral interviews, published articles, and online forums like Instagram and Twitter; and African cities list from Wikipedia \footnote{https://en.wikipedia.org/wiki/List\_of\_cities\_in\_Africa\_by\_population}. We augment $D_1'$ in three key steps:
\begin{enumerate}
    \item \textbf{Named-Entity Extraction with NER Models:} We leverage off-the-shelf pre-trained NER models \cite{conneau2019unsupervised} and annotate all named-entities in corpus $Y^{E_{1}}$ to extract the list $E_1$, tokens tagged with [PER], [LOC], or [ORG]. We mask these tokens $e_i \in E_1$ for a randomly sampled subset of transcripts. 
    \item \textbf{Template Selection:} We manually review, select and validate 140 of these sentences where the replacement of masked tokens with African named entities sounds natural and retains meaning in context. These curated sentences with masked tokens are selected as final templates.
    \item \textbf{Named Entity Replacement:} We randomly (uniformly) replace all [LOC] tags with African cities from $E_2$, and all [PER] and [ORG] tags with African names from $E_2$. We repeat this process 200 times to create text corpus $Y^{E_{2}}$ consisting of 28,000 novel augmented transcripts combined with transcripts from $Y^{E_{1}}$ (100,000+ sentences). $Y^{E_{2}}$ is recorded by crowd-sourced workers.
    % creating a new dataset $D_2' = \{(X^{E_{2}},Y^{E_{2}})\}$ with distribution $D^{E_{2}}$ induced by African and Anglo-centric named entities $E_{2}$. 
    We sample a subset of users from train/dev/test splits for this work. 
    
    %Several studies have demonstrated the utility of "templates" as an effective way to create richer, more expressive training datasets, especially for Question-Answering and prompt engineering \cite{pawar2016question, brown2020language, yao2022prompt} and named entity recognition \cite{DBLP:conf/tsd/DavodyAKK22}. Inspired by this approach, we expand our dataset by creating templates from sentences selected from curated corpora described
\end{enumerate}

A real-world example of $D^{E_{1}}$ is LibriSpeech \cite{panayotov2015librispeech}, a 1,000-hours speech-text dataset from English-only  audiobooks. The resulting ASR model $M_{1}^{E_{1}}$, such as Wav2vec2 \cite{baevski2020wav2vec}, therefore, generalizes poorly to African named entities ${E_{2}}$ (Table \ref{tab:failure_examples}). The pretrained ASR model  $M_{1}^{E_{1}}$ is thus fine-tuned on the new augmented training dataset $D_2'$,  and learns a new mapping $f: X^{E_{2}} \longrightarrow Y^{E_{2}}$ resulting in a more robust model $M_{1}^{E_{2}}$, adapted to the target distribution $D^{E_{2}}$.

%, is finetuned Model $M_1$, thus needs to update its weights to $M_1'$  where $Y'$ and $X'$ represent augmented samples derived from $X$ and $Y$ respectively by randomly replacing named entities from $E_1$ with entities from $E_2$.

%\begin{equation}\label{eq2}
%    \theta = \theta - \alpha\nabla_{\theta}f(\theta, X', Y')
%\end{equation}

% \begin{equation}\label{eq2}
%     \theta_2 = P(Y'|X',E_2) %= \Sigma_i P(y^2_i|X^2,E^2)
% \end{equation}


%Since $D_2$ contains both Western and African-named entities and our primary task is to evaluate NER on African named-entities, to isolate African named-entities, we extract a subset $D_3$ from $D_2$ where exists any $e_i \in E_2$, essentially, the subset of $D_2$ where African-named entities from $E_2$ exists. We note that this process is not perfect as it could omit samples in $D_2$ with African names not captured in $E_2$. However, it guarantees that we can evaluate a subset of test samples with African-named entities.

%\begin{enumerate}
%    \item Candidate Datasets
%    \begin{itemize}
%        \item Fine-tuned
%        \begin{itemize}
%            \item AfriSpeech
%\item Common Voice*
%\item Appen
%\item SautiDB
%\item librispeech

%        \end{itemize}
%        \item Test
%        \begin{itemize}
%            \item AfriSpeech
%\item SautiDB
%\item EduSTT
%\item Prct
%\item lwazi
%\item Nchlt
%\item Common Voice accented
%\item librispeech*

%        \end{itemize}
%    \end{itemize}
%    \item AfriSpeech Templates
%\item Audio recordings
%\item Samples with NER vs non-NER samples

%\end{enumerate}

\section{Experiments}


\subsection{Benchmarks}\label{section:benchmarks}
We compare SOTA open-source pre-trained ASR models: Whisper \cite{radford2022robust}, Wav2vec2 \cite{baevski2020wav2vec}, XLSR \cite{grosman2021xlsr53-large-english}, Hubert \cite{hsu2021hubert}, and WavLM \cite{chen2022wavlm}, with commercial ASR systems. We refer readers to the respective papers for details on pre-training corpora, model architecture, and hyperparameters. We compare 4 model categories: (1) \textbf{Monolingual Models} pre-trained or fine-tuned exclusively on predominantly western transcripts, western English speech, and western named-entities (2) \textbf{Multilingual Models} pre-trained on transcripts from multiple domains, western and accented speech, but with minimal amounts of African named-entities (3) \textbf{Commercial ASR APIs} (4) \textbf{Ours} finetuned on western and African-named entities paired with audios in accented African English.
% (4) \textbf{Models fine-tuned for robustness} on a combination of transcripts with western and African-named entities paired with audios in accented African English.

% \begin{enumerate}
%     \item Monolingual Models pre-trained or fine-tuned exclusively on predominantly western transcripts, western English speech, and western named-entities
%     \item Multilingual Models pre-trained on transcripts from multiple domains, western and accented speech, but with minimal amounts of African named-entities
%     %\item Models fine-tuned on predominantly western transcripts, accented English speech, and predominantly western named entities
%     \item Commercial ASR APIs
%     \item Models fine-tuned for robustness on a combination of transcripts with western and African-named entities paired with audios in accented African English
% \end{enumerate}

\subsection{Fine-tuning}
We select two best-performing open-source models from section \ref{section:benchmarks} and fine-tune them on an accented speech corpus dense with African and western-named entities to achieve robustness to western and African-named entities. We compare pre-trained model performance with fine-tuned checkpoints. Selected model architectures include:
\begin{itemize}
    \item wav2vec2-large-xlsr-53 \cite{grosman2021xlsr53-large-english}: an encoder-decoder architecture with a CNN-based feature extractor, code book, and transformer-based encoder, 378.9M parameters; learning rate of 1e-4.
    \item whisper-medium \cite{radford2022robust}: a decoder-only multi-task architecture, 789.9M parameters; learning rate of 2.5e-4. (We do not fine-tune whisper-large because of computational resource constraints)
\end{itemize}

For each model, we fine-tuned with FP16 \cite{micikevicius2017mixed}, AdamW \cite{loshchilov2019decoupled}, batch size of 16, for 10 epochs, with a linear learning rate decay to zero after a warmup over the first 10\% of iterations. XLSR was trained on a single Tesla T4 GPU with 16GB GPU memory while Whisper was trained on RTX8000 GPU with 48GB GPU memory. Fine-tuning took 24-48 hrs.


%For selected pre-trained and commercial ASR models $M_1$ to $M_n$, as well as fine-tuned models $M_1'$ to $M_n'$, we evaluate WER on samples containing one or more named entities and present single run results in Table \ref{tab:models_benchmarks}. 

%\begin{enumerate}
%    \item Finetune and test on 
%    \begin{itemize}
%        \item western alone (librispeech, common voice, Appen)
%\item accented alone (Afrispeech, SautiDB)
%\item western + african (Common Voice accented)
%    \end{itemize}
%   \item  Benchmarks
%\item Evaluation
%\item Results
%\end{enumerate}

\begin{table*}[t]
\tiny
\centering
\caption{WER results on Afrispeech test samples. \textbf{All} is mean WER across all test samples. \textbf{No-NER} is mean WER across samples with NO predicted named entities (NEs). \textbf{AfriNER} is mean WER across all sentences WITH predicted NEs. \textbf{AfriVal} is mean WER across AfriValidated samples. \textbf{char-AfriNER} and \textbf{char-AfriVal} are mean CER on AfriNER and AfriVal respectively. \textbf{char-AfriNER} and \textbf{char-AfriVal} concatenates the NEs in the predicted and reference transcripts.}

\begin{tabular}{l|l|l|l|l|l|l|l|l}
\toprule 
Model & Params & Training or Finetuning data & \multicolumn{4}{c|}{WER}  & \multicolumn{2}{c}{CER} \\
  & & & All (\#2364) & No-NER  (\#1029) & AfriNER  (\#971) & AfriVal  (\#229) &  char-AfriNER & char-AfriVal \\ 
\midrule
\multicolumn{9}{l}{Baseline}\\
\hline
wav2vec2-large-960h & 317M & Monolingual   & 0.641  & 0.565  & 0.696  & 0.802 & 0.861 & 0.986  \\
\midrule
\multicolumn{6}{l}{Monolingual Fine-tuning: Open-Source SOTA pre-trained Models} & \multicolumn{3}{l} {0.718 Monolingual Mean WER}\\
\hline
wav2vec2-large-960h-lv60-self & 317M & Monolingual   & 0.533  & 0.458  & 0.584  & 0.683  & 0.808 & 0.978  \\

%hubert-large-ls960-ft & 317M & Monolingual   & 0.557  & 0.494  & 0.607  & 0.613  & - & -  \\
hubert-xlarge-ls960-ft & 317M & Monolingual  & 0.562  & 0.487  & 0.613  & 0.701  & 0.803 & 0.986  \\

wavlm-libri-clean-100h-large & 317M & Monolingual   & 0.631  & 0.562  & 0.680  & 0.769  & 0.864 & 0.984  \\

%w2v2-lg-robust-ft-libri-960h & 317M & Monolingual  & 2478& 2364 & 0.3 & 3024 & 0.3 & 5 & 0.3  \\

%wav2vec2-large-robust-swbd-300h & 317M & Monolingual   & 0.733  & 0.660  & 0.796  & 0.844  & - & -  \\
\midrule
\multicolumn{6}{l}{Multilingual Fine-tuning: Open-Source SOTA pre-trained Models } & \multicolumn{3}{l}{0.506 Multilingual Mean WER}\\
\hline
whisper-large & 1550M & Multilingual  & 0.240  & 0.187  & 0.300  & 0.412 & \textbf{0.565} & 0.855 \\
whisper-medium & 769M & Multilingual   & 0.276  & 0.206  & 0.352  & 0.488  & 0.607 & 0.913  \\
%whisper-small & 244M & Multilingual   & 0.330  & 0.257  & 0.405  & 0.462  & - & -  \\
wav2vec2-large-xlsr-53-english & 317M & Multilingual   & 0.506  & 0.447  & 0.550  & 0.617  & 0.772 & 0.965  \\
% wav2vec2-xls-r-1b-english & 317M & Multilingual    & 0.521  & 0.468  & 0.568  & 0.581  & - & -  \\
% nvidia/stt-en-conformer-ctc-large & 118M &  Multi, 10 & AfriSpeech &3024& 2364 & 0.3 & 3024 & 0.3 & 5 & 0.3  \\

\midrule
\multicolumn{6}{l}{Commercial ASR APIs} & \multicolumn{3}{l}{0.588 Commercial Mean WER}\\
\hline
Azure\cite{azure}  & - & -     & 0.340  & 0.273  & 0.402  & 0.509  & 0.674 & 0.946 \\
GCP\cite{gcp}  & - & -     & 0.534  & 0.464  & 0.603  & 0.700  & 0.827 & 0.991 \\
AWS\cite{aws}  & - & -     & 0.354  & 0.279  & 0.426  & 0.556  & 0.735 & 0.970  \\

% (Azure\footnote{\url{https://speech.microsoft.com/portal/speechtotexttool}}, GCP\footnote{\url{https://cloud.google.com/speech-to-text/}}, and AWS\footnote{\url{https://aws.amazon.com/transcribe/}})

\midrule
%\multicolumn{9}{l}{Western named entities fine-tuned on accented speech}\\
%\hline
%wav2vec2-large-xlsr-53-english  & 317M & Multilingual  &  SDB  & 92& 138 & 0.3 & - & - & 15 & 0.3 & 0 & -  \\
%xlsr-53-english-SautiDB & 317M & SDB  & SDB  & 92 & 138 & 0.5 & 48 & - & 90 & 0.157 & 0 & -  & - & -  \\
%xlsr-53-english-CV & 317M & CV  & CV & 170 & 334 & 0.3 & 190 & - & 144 & 0.253 & 3 & -  & - & -  \\

%\midrule
\multicolumn{6}{l}{AfriSpeech Finetuning (Ours)} & \multicolumn{3}{l}{0.160 AfriSpeech Mean WER} \\
\hline
whisper-medium-AfriSpeech & 769M & Monolingual, AfriSpeech     & \textbf{0.186}  & \textbf{0.172}  & \textbf{0.198}  & \textbf{0.108}  & 0.576 & \textbf{0.704} \\
xlsr-53-english-AfriSpeech & 317M & Monolingual, AfriSpeech     & 0.236  & 0.211  & 0.258  & 0.212  & 0.622 & 0.816  \\

% wav2vec2-large-960h-medspeech & 317M & MedSpeech & Medspeech  &  & 622 & 0.3 & 9 & 0.09 & 0 & 0.3  \\
\bottomrule
\end{tabular}

\label{tab:models_benchmarks}
\end{table*}

% Mean WER per model category: Monolingual 0.681; Multilingual 0.479; Commercial 0.554; Ours 0.141 (80.4\% improvement over baseline)

\subsection{Evaluation}
Word Error Rate (WER) and Character Error Rate (CER) are common metrics for evaluating ASR models. WER measures word errors, CER measures character errors. Lower values are better for both.

\subsubsection{AfriNER: Named-Entity Evaluation}\label{section:ner}
To evaluate ASR performance on named entities (NEs), we need a reliable way to identify samples in $Y_2$ with NEs. Ground truth transcripts $Y_2$ contain $E_1$ and $E_2$ entities, jointly called $E_3$. To extract all samples in $Y_2$ with NEs in $E_3$, we run NER inference on all test samples in $Y_2$ using a specialized performant NER model $R_2$ \footnote{https://huggingface.co/masakhane/afroxlmr-large-ner-masakhaner-1.0\_2.0} from \cite{Adelani2022MasakhaNER2A} that jointly predicts the set of African and western named entities $E_3$. We select test sentences where an entity is detected with confidence (score) greater than 0.8. This seemed to be a reasonable threshold based on ad-hoc analysis. $R_2$ is also able to identify unknown African named entities in $Y_2$ not sourced from $E_2$ (but present in $Y_1$). We denote this subset $Y_3$ (Afri-NER). For each model, we compute WER on corresponding model predictions $Z_3$.


%Since ground truth NER labels do not exist for this novel dataset, we identify samples with named entities  To evaluate model 


%$M_{1}^{E_{2}}$ predicted transcripts ($Z_2$) with NEs, we need a reliable way to identify $z_i \in Z_2$ where NEs exist. ASR errors make it unreliable to exactly match tokens from $E_3$ in $Z_2$. Therefore,  We select test sentences where an entity is detected with confidence (score) greater than 0.8. This seemed to be a reasonable threshold based on ad-hoc manual analysis.  $R_2$ is also able to identify African named entities in $Y_2$ not sourced from $E_2$ (present in $Y_1$). Afri-NER ($Z_3$) is the subset of predicted transcripts $Z_2$ where $R_2$ predicts the presence of one or more named entities with a confidence score above 0.8.

%$D_1$ and $D_2'$, $D_2$, therefore, contains sentences with named entities from both $E_1$ and $E_2$, jointly called $E_3$. To evaluate NER on ASR-predicted transcripts from $D_2'$, we need a reliable way to identify named entities in $D_2$ since $D_2$ has no prior ground truth NER annotations. To achieve this, we run NER inference on all test samples in $D_2$ using a specialized performant NER model $R_2$ from \cite{Adelani2022MasakhaNER2A} that jointly predicts the set of African and western named entities $E_3$ in $D_2$. %This process identifies samples from $D_2$ with Western and African named entities.

\subsubsection{Sentence-level AfriValidation: African Named-Entity Validation}\label{section:afriVal}
Our primary goal is to evaluate $M_{1}^{E_{1}}$s and $M_{1}^{E_{2}}$ on transcripts with ``African" NEs. To isolate samples with African NEs, we extract the subset of $Y_2$ from the test partition with any NEs from $E_2$ to create the AfriVal subset. Because these sentences are known to contain African NEs, they are Afri-Validated, and guarantee we can reliably evaluate ASR models on predicted transcripts with African NEs. For each model, we compute WER on corresponding model predictions. % beyond the subset of $D_2$ augmented transcripts. 

% \footnote{WER is based on string edit distance, and penalizes all differences between the model's output and the reference transcript. The lower the value, the better.}
% \footnote{CER is a common metric whose value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better.}

\subsubsection{Character-level AfriValidation}\label{section:char-afriVal}
Since sentence-level WER is impacted by non-NE tokens, we compute CER on NE tokens by isolating them as follows: 1) We run $R_2$ on model predicted transcript $Z_2$ and $Z_3$ to obtain predicted NE tokens with $>0.8$ confidence score. To mitigate the impact of NER errors from $R_2$, for each ground truth and predicted sentence, we concatenate all NER tokens $e_i \in E_3$ from $Y_3$ and all $z_i \in Z_3$ removing all spaces and compute CER.
%Since ground truth NER labels do not exist for this novel dataset, we identify samples with named entities using the process described in \ref{section:ner}. This subset, with named entities from $E_3$, is called Afri-NER. We evaluate Word Error Rate (WER) on corresponding test set model predictions $Z_3$. Furthermore, we extract the subset of $Y_2$ sentences with named entity tokens from $E_2$. This subset is called Afri-Val and evaluate WER on corresponding model predictions. Furthermore, from Afri-Val, we extract named entity tokens from ground truth and model predictions using the process described in \ref{section:afriVal} and compute Character Error Rate (CER). 
For selected pre-trained and commercial ASR models $M_{1}^{E_{1}}$, as well as fine-tuned models $M_{1}^{E_{2}}$, we evaluate WER and CER on samples containing one or more named entities and present single run results in Table \ref{tab:models_benchmarks}.


%we infer NER labels using a specialized NER model \cite{Adelani2022MasakhaNER2A} as described in section \ref{section:afriVal} above. We select test sentences where an entity is detected with confidence (score) greater than 0.8 and evaluate using Word Error Rate (WER). 

%For ``ground truth" transcripts, we extract the subset of $Y_2$ with any named entity from $E_2$ and evaluate Word Error Rate (WER) on corresponding model predictions. Furthermore, we extract named entities $E_3$ from each $y_i in Y_2$ using $R_2$. For each model $M$ with predicted named entities $Z_3$ identified by $R_2$. To mitigate NER errors from $R_2$, for each sentence, we concatenate all $e_i \in E_3$ and all $z_i in Z_3$ and compute Character Error Rate (CER).




\section{Results and Discussion}
\subsection{African named entities are challenging}
The baseline model in Table \ref{tab:models_benchmarks} demonstrates the dominant trend in our results. WER on all samples (column 4, All) improves by 13.6\% (relative) when samples with named entities are EXCLUDED (column 5, No-NER), worsens by 11.7\% (relative) when samples with named entities (western + African) are isolated (column 6, AfriNER). Performance sinks by 29.7\% (relative) on the subset of Afrivalidated examples (column 7, AfriVal)-- samples with African-named entities from $E2$. This pattern is consistent across all model categories except Ours where we observed a 41.9\% (whisper) and 10.2\% (xlsr) relative WER improvement on AfriVal sentences.

\subsection{Training data bias} 
As shown in Table \ref{tab:models_benchmarks}, multilingual/multitask pre-training outperforms monolingual pre-training/fine-tuning. Multilingual/multitask models \cite{radford2022robust, grosman2021xlsr53-large-english, gulati2020conformer} learn more useful representations, are more linguistically diverse, robust, and generalize better to accented speech when compared with monolingual models fine-tuned on datasets (e.g. Librispeech  \cite{panayotov2015librispeech} and Switchboard \cite{godfrey1992switchboard}) with predominantly western NEs and western accents. After fine-tuning on AfriSpeech with African NEs and accented speech, our best model, whisper-medium improves on the baseline by 81.5\% compared to 16.4\% for the pre-trained model.


%\subsection{NER models and AfriValidation}
% Figure \ref{fig:distribution} shows the distribution of named entities in the AfriSpeech dataset. 
%Although NER models are imperfect, we manually validated several outputs that gave us confidence that the specialized NER model \cite{Adelani2022MasakhaNER2A} was in fact correctly identifying African and Western-named entities. Afrivalidation also guaranteed sentences with African-named entities were isolated. However, there are caveats. The African slave name database \cite{anderson2013using} contained western names like George and John which were initially picked up during Afrivalidation. To mitigate this, we limit Afrivalidation to a set of nearly 2k Nigerian names described in section \ref{section:name-aug}. 
% Several [PER], [LOC], and [ORG] examples can be seen in Appendix Tables \ref{apdx:nigeria_ner1} and \ref{apdx:nigeria_ner2}.


\subsection{Multilingual pretraining is insufficient}  
% Appendix Tables \ref{apdx:nigeria_ner1} and \ref{apdx:nigeria_ner2} show several examples with African-named entities, comparing pre-trained vs finetuned versions of our best model, Whisper-medium. 
Despite extensive pretraining on 680k hours of multilingual data (90 languages), the fine-tuned model outperforms the pre-trained model by 77.9\% (relative). Our results demonstrate that multilingual/multi-task pretraining is inadequate as these SOTA models make several mistakes with African-named entities. Fine-tuning results show that our approach is effective in mitigating bias in these large models. 
% Appendix Table \ref{apdx:nigeria_ner1} shows Afrivalidated named entity examples where the pre-trained whisper had a WER $<$ 0.20 (low error) along with the improved transcripts after fine-tuning.  Appendix Table \ref{apdx:nigeria_ner2} shows high error (WER $>$ 0.8) examples along with improved transcripts by the fine-tuned whisper model.

%\subsection{Accented speech on Western named entities} 
%Table \ref{tab:models_benchmarks} shows WER on transcripts with predominantly western named-entities for models finetuned on accented speech. Because these models haven exposed to multiple English accents (including African accents), we expect lower WER on the subset of samples with named entities. %These models show an X\% improvement over the baseline.

%\subsection{Speech characteristics} 
%Maybe look at Spectrograms or MFCCs of samples with western vs African-named entities

\subsection{Character-Level analysis} 
When named entities are isolated as described in Section \ref{section:afriVal}, we observe that our fine-tuned whisper-medium model worsens by 1.9\% (relative) in comparison to the pre-trained whisper-large model (column 8, char-AfriNER). This may be due to the significantly higher number of parameters in whisper-large generalizing better to certain named entities.
However, when evaluated on the Afrivalidated dataset (column 9, char-AfriVal), our fine-tuned whisper-medium model outperforms both pre-trained whisper-large and medium models (relative gain of 17.7\%, and 22.9\% respectively). These results further support our claim that the presence of African-named entities is crucial for achieving better performance in ASR models. 

\subsection{Use of language models} 
Table \ref{tab:failure_examples} shows some of the difficulties with commercial APIs where a language model (LM) is likely used to rescore the raw ASR transcript. This is especially destructive for African-named entities. Because these named entities (e.g.``Ifeadigo") are missing from LM training data, where the probability of sequences with African NEs is effectively zero, and such transcripts are downranked by the algorithm in favor of more likely tokens like ``Diego" as seen in the example in Table \ref{tab:failure_examples}. Prediction score thresholds may also be in use under the hood in these commercial systems, limiting the ASR output where confidence is low resulting in truncated output as seen in Table \ref{tab:failure_examples}. 

\section{Conclusion}
Automatic speech recognition (ASR) for African-named entities is a challenging task for most state-of-the-art (SOTA) ASR models including those trained with multilingual data and multitask objectives. We demonstrate that this bias can be mitigated by fine-tuning these models on accented speech corpora rich in African-named entities, shifting the distribution for robustness in the African context.
% \section{Limitation and Future work}

% \textbf{Limitation and Future work:}
% Table \ref{tab:failure_examples} provides examples where the pre-trained model transcribed some African NEs even without explicit fine-tuning, as well as cases where African NEs are not well transcribed by the fine-tuned model. While this approach works well for African NEs, empirical results are needed to generalize this approach to other races or domains.
% %\section{Limitations \& Future work}



% \subsection{Tables}

% An example of a table is shown in Table~\ref{tab:example}. The caption text must be above the table. Tables must be legible when printed in monochrome on DIN A4 paper; a minimum font size of 8 points is recommended.



%\section{Ethical considerations}


%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done in many journals. This is optional and at the discretion of the authors.

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.


% \input{appendix}

\bibliographystyle{IEEEtran}
\bibliography{mybib}


\end{document}
