\documentclass[amsmath,amssymb,aps,prb,superscriptaddress,onecolumn,floatfix]{revtex4}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}
\usepackage[usenames]{color}
\usepackage{comment}

\usepackage{hyperref} 
%\usepackage{titlesec}
\usepackage{physics}
\usepackage{bookmark}
\usepackage{doubleangle}
\usepackage{mathtools}
\usepackage{subfigure}



\begin{document}
\title{Supplemental Material - Hamiltonian reconstruction as metric for variational studies}
%\author{Kevin Zhang, Samuel Lederer, Kenny Choo, Titus Neupert, Giuseppe Carleo, and Eun-Ah Kim}
\author{Kevin Zhang}
\affiliation{Laboratory of Atomic and Solid State Physics, Cornell University, 142 Sciences Drive, Ithaca NY 14853-2501, USA}
\author{Samuel Lederer}
\affiliation{Laboratory of Atomic and Solid State Physics, Cornell University, 142 Sciences Drive, Ithaca NY 14853-2501, USA}
\author{Kenny Choo}
\affiliation{Department of Physics, University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Switzerland}
\author{Titus Neupert}
\affiliation{Department of Physics, University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Switzerland}
\author{Giuseppe Carleo}
\affiliation{Computational Quantum Science Laboratory, \'{e}cole polytechnique f\'{e}d\'{e}rale de Lausanne, Route Cantonale, 1015 Lausanne, Switzerland}
\author{Eun-Ah Kim}
\affiliation{Laboratory of Atomic and Solid State Physics, Cornell University, 142 Sciences Drive, Ithaca NY 14853-2501, USA}
\date{January 2021}


\maketitle

\section{Neural network architectures and training}
\label{sec:wf}

In this section, we will explain how we constructed and trained our variational ansatze.
First, we describe a basic neural network to motivate the more complex architectures that we used.
Then, we explicitly construct the convolutional neural network (CNN) and restricted Boltzmann machine (RBM) architectures that we used in our study.
Finally, we explain how we trained and symmetrized the wavefunctions.

One of the simplest examples of a neural network that can be used to parametrize a variational wavefunction is a single-layer perceptron.
Although we did not use this architecture, it serves as an illustrative example of neural network wavefunctions in general.
The single-layer perceptron consists of one transformation (known as a dense layer)
\begin{equation}
    w_i(\{ v_{i'} \}) = g\left(\sum_j \bm{W}_{ij} v_j + b_i\right),
\end{equation}
where $W$ is a complex matrix of tunable parameters (``weights"), $\vec{b}$ is a complex vector of tunable parameters (``biases"), and $g$ represents a non-linear firing function.
The wavefunction corresponding to this single-layer network is
\begin{equation}
    \bra{\{\sigma_i\}}\ket{\Psi} = \sum_i w_i(\{ \sigma_{i'} \})
\end{equation}
Generically, a multi-layer perceptron can be constructed by composing several of these ``linear" layers.
Examples of common algorithms used to optimize these constructions are stochastic gradient descent or Adam.
In our training, we used stochastic gradient descent with a learning rate of $0.001$.

\subsection{Convolutional neural network}
In a convolutional layer, both the input and output have an additional ``channel" dimension on top of the spatial indices.
Thus, each stage of intermediate values is a three-dimensional tensor, with one channel index and two spatial indices.
The input values (spin configuration) are interpreted as just consisting of a single channel.
A convolutional layer from $N$ channels to $M$ channels is given by
\begin{equation}
    w_{m,(i,j)}(\{v_{n,i',j'}\}) = g\left(\sum_{n}^N \sum_{x=0}^{L-1} \sum_{y=0}^{L-1} K_{n,(x,y)} v_{n,(i+x,j+y)} + b_m\right),
\end{equation}
where the indices of the output vector $w$ are $m$ for the channel, and $(i, j)$ for the spatial position.
The input vector $v$ is similarly indexed.
As with the dense layer, $g(x) = \ln{\cosh{x}}$ is the nonlinearity.
The $K$ parameters are called the ``kernel'' of the transformation, and there is one kernel for each output channel.
Each kernel is a $L$ by $L$ by $N$ tensor of weights, which attempts to capture some spatially local feature of the input values.
$L$ is the size of the kernel.
The construction of the convolutional layer is explicitly translationally invariant, which respects the symmetry of the periodic J1-J2 model we are solving.
The full convolutional network is given by the composition of convolutional layers,
\begin{equation}
    \bra{\sigma_{x, y}}\ket{\Psi} = \sum_{w,i,j} w_{m,(i,j)}\left(\{v_{n,(i',j')}(...(\sigma_{x,y}))\}\right)
\end{equation}
The outputs of the final layer were summed to obtain the output of the network.
The network we used consists of three convolutional layers, with channel counts 6, 4, 2, and kernel sizes 4, 4, 2, respectively, amounting to a total of 236 parameters.

\subsection{Restricted Boltzmann machine}
The single-layer restricted Boltzmann machine is characterized by the wavefunction
\begin{equation}
    \bra{\{ \sigma_i \}}\ket{\Psi} = e^{\sum_i a_i \sigma_i} \prod_j^M \cosh \left(\sum_i W_{ij} \sigma_i + b_j \right)
\end{equation}
where $M$ represents the number of hidden nodes.
We imposed the spatial symmetries of translation and rotation on the parameters $a_i$, $W_{ij}$, $b_j$, reducing the total number of free parameters by a factor of 16 (the size of the system).
With a hidden node count of 160, this wavefunction has a total of 171 parameters.

\subsection{Sign rotation and symmetrization}

Since the successful optimization of wavefunctions depends on a sign structure being pre-imposed, the wavefunctions were rotated in-plane:
\begin{equation}
     \ket{\Psi} \to \left( \bigotimes_{i \in A} e^{-i \pi \sigma^i_z / 2} \right) \ket{\Psi},
\end{equation}
where $A$ denotes a subset of lattice sites corresponding to either N\'{e}el or stripe order (\autoref{fig:spin}).
\begin{figure*}
\centering
    \subfigure[] {
    \centering
    \includegraphics[width=.3\textwidth]{1b.png}
    }
    \subfigure[] {
    \centering
    \includegraphics[width=.3\textwidth]{1c.png}
    }
    \caption{Diagrams of the classical spin structure, used for in-plane wavefunction rotation, for a) N\'{e}el and b) stripe order.}
    \label{fig:spin}
\end{figure*}
The N\'{e}el rotation was used for $J_2 \leq 0.5$ and the striped rotation used otherwise.

Finally, to ensure that the final wavefunctions respect point group symmetries and time reversal, we used the same procedure as in \cite{PhysRevB.100.125124}.
For an Abelian symmetry $C$, with an irrep $\Gamma$, the (unnormalized) symmetrization of the wavefunction $\ket{\Psi}$ under $\Gamma$ is given by
\begin{equation}
    \ket{\Psi_S} = \sum_{r=0}^{|C| - 1} \omega^r c^r \ket{\Psi},
\end{equation}
where $\omega$ is the character of $\Gamma$ and $c$ is the generator of $\Gamma$.
For the $C_4$ rotation group, there are four possible irreps with characters $\pm 1$ and $\pm i$; for time reversal symmetry, there are two irreps with characters $\pm 1$.
We chose the combination of representations that yielded the lowest ground state energy, and in all cases, the energy was lowest with the irrep with character 1 for both $C_4$ and $T$.

\section{Hamiltonian reconstruction}
\subsection{Benchmarks}
With the exact ground state of the Heisenberg model ($J_2=0$) obtained via exact diagonalization, Hamiltonian reconstruction indeed yields the correct Hamiltonian in all three subspaces we considered down to machine precision, as shown in Table~I.

\begin{table}[ht]
\centering
\begin{tabular}{ |c||c| }

 \hline
 $\mathcal{H}[\mathcal{O}]$ & Reconstructed parameter \\
 \hline
 $H[\delta J_2]$  & $\delta J_2 = -6.5 \times 10^{-15} $ \\
 $H[J_3]$  & $J_3 = -3.18 \times 10^{-15} $ \\
 $H[\alpha]$ & $\alpha = 3.73 \times 10^{-15} $ \\
 \hline
\end{tabular}

\caption{Benchmark reconstructions for wavefunctions obtained via exact diagonalization for the $J_1$-$J_2$ model in the limit $J_2 = 0$.
In all cases, the reconstruction yields the original $J_1$-$J_2$ Hamiltonian to within machine precision.}
\label{tab:table}
\end{table}


\subsection{Multi-dimensional reconstructions}

In our studies, we chose to restrict the target Hamiltonian spaces to two-dimensional spaces.
Attempting to perform reconstruction into large Hamiltonian spaces led to unidentifiable features.
Here, we summarize the results of such a high-dimensional reconstruction, in the space given by

\begin{equation}
     H[\alpha, \beta, \delta J_2] = H_{J_1J_2} + \alpha \sum_{\left< i, j\right>} S^z_i S^z_{j} + \beta \sum_{\llangle i, j\rrangle} S^z_i S^z_{j} + \delta J_2 \sum_{\llangle i, j\rrangle} \vec{S}_i \cdot \vec{S}_j,
\end{equation}

\begin{figure*}[htp]
\subfigure[] {
    \centering
    \includegraphics[width=.3\textwidth]{ex3.1.png}
    }\subfigure[] {
    \centering
    \includegraphics[width=.3\textwidth]{ex3.2.png}
    }\subfigure[] {
    \centering
    \includegraphics[width=.3\textwidth]{ex3.3.png}
    }
    \caption{Parameters of the reconstructed Hamiltonians into a higher-dimensional target space of CNN wavefunctions.
    a) Reconstructed nearest-neighbor coupling $\delta J_2/J_1$.
    b) The reconstructed nearest neighbor easy-axis anisotropy $\alpha/J_1$.
    c) The reconstructed next-nearest neighbor easy-axis anisotropy $\beta/J_1$.}
    \label{fig:H1}
\end{figure*}

\autoref{fig:H1} shows reconstruction results for this four-dimensional Hamiltonian space. Here, an additional unexplained peak at $J_2/J_1 = 0.75$ can be observed in the $\alpha$ parameter which is not present in reconstructions of two-dimensional target spaces.

Further, multi-dimensional target spaces including a $J_3$ parameter, e.g.
\begin{equation}
    H[\alpha, \beta, \delta J_2, J_3] = H[\alpha, \beta, \delta J_2] + J_3 \sum_{\langle i, j\rangle_3} \vec{S}_i \cdot \vec{S}_j,
\end{equation}
result in degeneracy of the correlation matrix's lowest eigenvalues. In these cases, there is no unique best Hamiltonian making interpretation of the reconstruction difficult.
Thus, for clarity, we restricted our analysis to two-dimensional target spaces.

\subsection{Monte Carlo sampling}

Various methods can be used when evaluating the elements of the quantum covariance matrix.
Here, we present results of Hamiltonian reconstruction using Monte Carlo sampling for correlation functions.
The model in question is the same $4 \times 4$ square lattice $J_1$-$J_2$ Heisenberg model, with $J_2/J_1 = 0.5$, i.e., the classical high frustration point.
We only present results here for the reconstruction into the space $H[\delta J_2]$ using the convolutional neural network wavefunction.

\begin{figure*}[htp]
\centering
    \subfigure[] {
    \centering
    \includegraphics[width=0.3\textwidth]{ex4.1.png}
    }\subfigure[] {
    \centering
    \includegraphics[width=0.3\textwidth]{ex4.2.png}
    }\subfigure[] {
    \centering
    \includegraphics[width=0.3\textwidth]{ex4.3.png}
    }
    
    \caption{Results of Hamiltonian reconstruction using Monte Carlo sampling for operator correlation functions at $J_2/J_1 = 0.5$, into the space $H[\delta J_2]$. In all plots, the horizontal red line represents the true value as obtained through direct integration of the correlation functions.
    a) Correlation function $\langle{H_{J1J2} \sum_{\llangle i, j\rrangle} \vec{S}_i \cdot \vec{S}_j \rangle}$ as measured by Monte Carlo sampling. The error bars represent the standard deviation of the estimates for the correlation function.
    b) The reconstructed $\delta J_2$ parameter, which converges slower than the previous correlation function.
    c) The variance of the variational wavefunction $\ket{\Psi}$ under the reconstructed Hamiltonian. Here, the variance does not converge within the number of Monte Carlo samples that we used.}
    \label{fig:mc}
\end{figure*}

The reconstructed $\delta J_2$ parameter showed a slower rate of convergence (\autoref{fig:mc}(b)) than that of the shown correlation function (\autoref{fig:mc}(a)) with the number of Monte Carlo samples.
Further, the variance of the input wavefunction under the reconstructed wavefunction did not converge for the range of Monte Carlo samples that we used (\autoref{fig:mc}(c)).
Considering these points, we chose for our study to restrict our analysis to correlation functions evaluated explicitly. As a result, we limited ourselves to systems small enough that they were amenable to exact diagonalization.

% \subsection{Reconstruction as a metric of quality for noisy wavefunctions}
% The standard approach of evaluating the quality of a variational ansatz relies primarily on two quantities: energy and energy variance.
% These indicators are useful when the ground state energy is the main object of interest.
% However, very often, one is also interested in other observables such as order parameters or correlation functions.
% In these cases, looking only at energy-related quantities may give misleading results.
% The Hamiltonian reconstruction method provides an additional evaluation benchmark which can potentially give valuable insight.

% In the following toy example, we start from the exact ground state $\psi_g$ of the $J_1$-$J_2$ model, at the classical high frustration point $J_2 = 0.5$.
% We perturb the ground state by adding random noise to the wavefunction amplitudes, i.e.
% \begin{equation}
%     \psi(\{ \sigma_i \}) = \psi_g(\{ \sigma_i \}) + \epsilon
% \end{equation}
% where $\epsilon$ is an i.i.d. random variable drawn from either a normal or exponential distribution with varying parameters.

% \begin{figure*}[htp]
%     \subfigure[] {
%     \centering
%     \includegraphics[width=5.5cm]{ex1.5.png}
%     }\subfigure[] {
%     \centering
%     \includegraphics[width=5.5cm]{ex1.png}
%     }\subfigure[] {
%     \centering
%     \includegraphics[width=5.5cm]{ex2.png}
%     }
    
%     \caption{Comparison of various metrics used to evaluate wavefunctions. Trial wavefunctions were generated by adding a fixed amount of noise to the exact ground state of the $J_2 = 0.5$ model, and their energy, energy variance, and $J_3$ error were calculated.
%     a) Comparison of ground state energy gap for wavefunctions with fixed overlap, at various noise levels.
%     b) Comparison of energy variance.
%     c) Comparison of the absolute value of the reconstructed $J_3$ parameter. The $J_3$ metric most closely tracks the wavefunction overlap.}
%     \label{fig:noise}
% \end{figure*}

% In this example, the quality of the noisy wavefunctions is not known a priori (without relying on knowledge of the underlying distribution of $\epsilon$).
% Then, the goal is to estimate the wavefunction overlap from accessible quantities, such as energy, variance, or correlation functions.
% As can be seen from \autoref{fig:noise}(a-c), the wavefunction overlaps decrease monotonically with all metrics, but the issue arises when trying to compare wavefunctions across the two noise families: there exist pairs of points where a comparison of energy or variance belies the actual relationship between wavefunction overlap.

% The reconstructed $J_3$ parameter, in \autoref{fig:noise}(c), is most accurate in providing a metric of wavefunction quality, i.e., the slopes of the lines are most similar for the different distributions of wavefunction noise.
% Compared to the energy variance and ground state energy gap, this shows that the $J_3$ parameter from Hamiltonain reconstruction can be used as a more reliable indicator of wavefunction closeness to the true ground state.

\section{Correlations in the $J_1$-$J_2$-$J_3$ model}

Here, we present measurements of spin-spin correlation functions on the ground states of $J_1$-$J_2$-$J_3$ models obtained via exact diagonalization.
We define the spin-spin correlation function for wavevector $q$ as:
\begin{equation}
\label{eq:corr}
    S(\vec{q}) = \frac{1}{N} \sum_{i, j} e^{i \vec{q}\cdot (\vec{r}_i - \vec{r_j})} \langle \vec{S}_i \cdot \vec{S}_j \rangle,
\end{equation}
where $N=16$ is the system size.

\begin{figure*}[htp]
    \centering
    \subfigure[] {
    \centering
    \includegraphics[width=.4\textwidth]{j2_order_pizero.png}
    }
    \subfigure[] {
    \centering
    \includegraphics[width=.4\textwidth]{j2.1.pizero.png}
    }
    
    \caption{Spin-spin correlation functions, as defined in \autoref{eq:corr}, of the exact ground state of $J_1$-$J_2$-$J_3$ models. a) $S(\pi, 0)$ for $J_3 = 0$, showing how the stripe order is enhanced with increasing $J_2$. b) Similarly, $S(\pi, 0)$ at $J_2/J_1 = 1$ for ferromagnetic $J_3$ is enhanced.
    }
    \label{fig:corr}
\end{figure*}

As can be seen from \autoref{fig:corr}, an increase in the $J_2$ parameter in the large $J_2$ regime leads to enhanced stripe order (\autoref{fig:corr}(a)). Also, small negative values of $J_3$ of similar magnitude to those reconstructed from our variational wavefunctions enhance the stripe order for states in the same regime (\autoref{fig:corr}(b)).
The stripe order parameter here is the spin-spin correlation function with ordering vector $(\pi, 0)$ or $(0, \pi)$.
These observations show that a ferromagnetic $J_3$ parameter, as well as positive $\delta J_2$, act to suppress quantum fluctuations and enhance the ground state order of the model in the large $J_2$ regime.


\bibliographystyle{apsrev4-2}
\bibliography{refs}

\end{document}
