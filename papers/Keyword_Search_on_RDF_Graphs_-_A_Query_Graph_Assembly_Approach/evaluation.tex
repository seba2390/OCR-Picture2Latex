\section{Evaluation}\label{sec:exp}

We evaluate both effectiveness and efficiency of our approach, and compare with a typical data graph based approach DPBF \cite{ding2007finding}, and a summary graph based approach SUMG \cite{tran2009top}, which are mentioned in Section \ref{sec:related_work}.
%The former is to find top-k optimal connected Steiner Trees, and the latter aims to generate top-k optimal query graphs by exploration over a summary RDF graph.
Due to the lack of schema information in RDF graphs, we can not compare with schema graph based approaches. Our system participates the QALD-6 competition and we report the results of our system with regard to other NL-QA participants. Experimental studies are also conducted on Freebase. All implementations are in Java, and all experiments are performed on a Linux server with Intel Xeon E5-2640v3@2.6GHz, 128GB memory, and 4T disk.

\vspace{-0.1in}
\subsection{Dataset}
%\textbf{DBpedia + QALD-6:}
%DBpedia \cite{lehmann2015dbpedia} is a well-known open-domain RDF repository,
%which extracts structured information from Wikipedia.
%QALD-6 \cite{unger20166th} collects 100 queries over DBpedia.
%For each query, it provides both NL question sentences and keywords.
%%full version:
%%Some sample inputs are listed in Table \ref{apptab:qald_question_sample} (see Appendix \ref{appsec:qald_queries}). 
%we only take each query's keywords as the input to our system, DPBF, and SUMG, while other QALD-6 participants can leverage both of NL sentences and keywords.
%Keyword sequences in QALD-6 have been segmented into terms by commas, such as ``scientist, graduate from, university, USA''. In practice, we believe that users will not split their inputs into terms by commas. Thus we remove these comma separators in order to test the segmentation accuracy of our approach. However, for the comparative methods DPBF and SUMG, we assume that the keywords have been correctly segmented using commas. Thus, the precision error of DPBF and SUMG does not come from the segmentation mistakes. 

\textbf{DBpedia + QALD-6:}
QALD-6 \cite{unger20166th} collects 100 queries over DBpedia.
For each query, it provides both NL question sentences and keywords.
we only take each query's keywords as the input to our system, DPBF, and SUMG, while other QALD-6 participants can leverage both of NL sentences and keywords.

%\noindent\textbf{Freebase + Free917:}
%We also conduct the comparative experiments on Freebase \cite{bollacker2008freebase}, which is a large collaborative KB composed mainly by communities. Free917 \cite{cai2013large} is an open QA benchmark which consists of NL question and answer pairs over Freebase. We select 80 questions of it and rewrite them into keyword queries artificially. 

\noindent\textbf{Freebase + Free917:}
Free917 \cite{cai2013large} is an open QA benchmark which consists of NL question and answer pairs over Freebase. We select 80 questions of it and rewrite them into keyword queries artificially. 

Table \ref{tab:statistic_rdf_graph} lists the statistics of the two RDF datasets.
\begin{table} [h]
\begin{tabular}
{  r | c | c }
%{ | c | c | }
\hline
 & DBpedia & Freebase \\ 
\hline
entities & 6.7M & 153M \\
relations & 1.4K & 19K\\
%property relations & 55,986 \\
classes & 0.6K  & 15K\\
%yago classes & 369,144 \\
%total triples & 162M & 596M \\
total triples & 583M & 1.9B \\
\hline
\end{tabular}
\caption{Statistics of RDF Datasets.}
\label{tab:statistic_rdf_graph}
\vspace{-0.3in}
\end{table}

%\subsection{Offline Preprocessing}
%%\vspace{-0.1in}
%\subsubsection{Building Dictionaries}
%
%\Paragraph{Entity/Class dictionary:} For entities, we collect the literal values of name properties, such as \emph{foaf:name}\footnote{PREFIX foaf: <http://xmlns.com/foaf/0.1/>}, \emph{dbo:alternativeName}, \emph{dbo:alias}, \emph{dbo:abbreviation} from DBpedia, and \emph{fb:type.object.name}, \emph{fb:common.topic.alias} from Freebase. For classes, we extend each class's relevant terms by the synonym set from Wordnet\cite{miller1995wordnet}. For example, both of ``movie'' and ``film'' are matched to the class \emph{dbo:Film}, because they are synonymous.
%
%\Paragraph{Paraphrase dictionary:} Patty\cite{nakashole2012patty} provides us a large number of matchings between relation terms(also called \emph{paraphrases}) and DBpedia predicates with confidence score. Thus, we integrate Patty and some verb phrases synonymous with the predicate from Wordnet to construct the paraphrase dictionary.
%
%A sample of DBpedia entity/class dictionary and paraphrase dictionary are shown in Table \ref{apptab:entity_class_dict} and Table \ref{apptab:relation_dict} (See Appendix \ref{appsec:dict}). We index the three dictionaries in key-value format by Apache Lucene\footnote{https://lucene.apache.org/}, where the key stands for keyword terms, and the value is the matched RDF entity/class/predicate along with confidence score. 
%%The indexes cost 442.5 MB space of disk in total.

%\vspace{-0.1in}
\noindent\textbf{TransE Vectors:}
%We classify RDF triples into three classes:
%(1) relation triple, whose subject and object are all entities,
%and describes the relation between two entities. 
%(e.g. $\langle$res:Alan\_Turing, dbo:almaMater, res:Priceton\_University$\rangle$).
%(2) literal triple, whose object is a literal string,
%and describes an entity's attribute value.
%(e.g. $\langle$res:Alan\_Turing, dbo:deathDate, "1954-06-07"$\rangle$). 
%(3) class triple, which describes an entity's class information.
%(e.g. $\langle$res:Alan\_Turing, rdf:type, dbo:Scientist$\rangle$).
%
%Firstly, we collect all the relation triples as the training data $\mathcal{T}$.
%Literal and class triples are omitted because they do not contain structural information among entities. In order to calculate vectors of class vertices, we need to enlarge the training data as follows: for each relation triple $\langle s,p,o \rangle$, suppose entity $s$(or $o$) is belong to the class $t_s$(or $t_o$), we add the class-grained triples $\langle t_s,p,o \rangle$, $\langle s,p,t_s \rangle$, and $\langle t_s,p,t_o \rangle$ into $\mathcal{T}$. 
We adopt an open-sourced TransE implementation
\footnote{https://github.com/thunlp/Fast-TransX} to train vectors.
We use a grid search strategy to find an optimal parameter configuration: learning rate $\lambda=10^{-5}$, number of iterations $n=3000$, vector dimension $d=200$. The training process costs 2.1h and 5.8h on DBpedia and Freebase, respectively.

\vspace{-0.1in}
\subsection{Evaluating Effectiveness}

\begin{table} [t]
	\scalebox{1.0}{
		\begin{tabular}
			{ l | r | c c c c}
			\hline
			& & \textbf{N} & \textbf{R} & \textbf{P} & \textbf{F-1} \\
			\hline
			\multirow{5}{*}{\begin{minipage}{0.5in} Keyword Search Systems\end{minipage}}
			& \textbf{\textit{QGA(+TransE)}} & 100 & 0.59 & 0.85 & \textbf{\textit{0.70}} \\ 
			& \textit{QGA(+Cooccur)}          & 100 & 0.49 & 0.76 & 0.59 \\
%			& \textit{SimpQGA(+TransE)}      & 100 & 0.41 & 0.68 & 0.51 \\
			& \textit{QGA(+TFIDF)}           & 100 & 0.37 & 0.64 & 0.47 \\			
			& DPBF                         & 100 & 0.46 & 0.21 & 0.29 \\
			& SUMG                         & 100 & 0.31 & 0.22 & 0.23 \\  
			\hline
			\multirow{6}{*}{\begin{minipage}{0.5in} NL-QA Systems\end{minipage}}
			& CANaLI                       & 100 & 0.89 & 0.89 & 0.89 \\ 
			& UTQA                         & 100 & 0.69 & 0.82 & 0.75 \\ 
			& \textbf{\textit{QGA(+TransE)}} & 100 & 0.59 & 0.85 & \textbf{\textit{0.70}} \\  
			& NBFramework                  & 63  & 0.54 & 0.55 & 0.54 \\ 
			& SemGraphQA                   & 100 & 0.25 & 0.70 & 0.37 \\ 
			& UIQA                         & 44  & 0.28 & 0.24 & 0.25 \\ 
			%& UIQA(without manual)        & 36  & 0.19 & 0.15 & 0.17 \\
			\hline
		\end{tabular}
	}
	\begin{tablenotes}
		\item \textbf{N}: Number of Processed Queries; \textbf{R}: Recall; \textbf{P}: Precision.
%full version:
%		\item {\small Note: \textbf{R}, \textbf{P}, and \textbf{F-1} calculation formula are given in Appendix \ref{appsec:effect_measure}.}
	\end{tablenotes}
	%\vspace{-0.1in}
	\caption{QALD-6 Evaluation Result}
	\label{tab:testres}
	\vspace{-0.3in}
\end{table}
%\begin{table} [h]
%\scalebox{0.90}{
%\begin{tabular}
%{ l | r | c c c c c}
% \hline
%  & & \textbf{N} & \textbf{R} & \textbf{P} & \textbf{F-1} & \textbf{F-1 Global}\\
% \hline
%\multirow{6}{*}{\begin{minipage}{0.5in} Keyword Search Systems\end{minipage}}
%& \textbf{\textit{QGA+TransE}} & 100 & 0.59 & 0.85 & 0.70& \textbf{\textit{0.70}} \\ 
%& \textit{QGA+Cooccur}  & 100 & 0.49 & 0.76 & 0.59 & 0.59 \\
%& \textit{QGA+TFIDF}  & 100 & 0.37 & 0.64 & 0.47 & 0.47 \\
%& \textit{SimpQGA+TransE}  & 100 & 0.41 & 0.68 & 0.51 & 0.51 \\
%& DPBF      & 100 & 0.46 & 0.21 & 0.29 & 0.29 \\
%& SUMG        & 100 & 0.31 & 0.22 & 0.23 & 0.23 \\  
%  \hline
%  \multirow{6}{*}{\begin{minipage}{0.5in} NL-QA Systems\end{minipage}}
%& CANaLI         & 100 & 0.89 & 0.89  & 0.89 & 0.89 \\ 
%& UTQA      & 100 & 0.69 & 0.82 & 0.75 & 0.75 \\ 
%& \textbf{\textit{QGA+TransE}} & 100 & 0.59 & 0.85 & 0.70 & \textbf{\textit{0.70}} \\  
%& NBFramework         & 63 & 0.85 & 0.87 & 0.86 & 0.54 \\ 
%& SemGraphQA   & 100 &  0.25 & 0.70 & 0.37 & 0.37 \\ 
%& UIQA       & 44 & 0.63 & 0.54 & 0.58 & 0.25 \\ 
%%& UIQA(without manual)       & 36 & 0.53 & 0.43 & 0.48 & 0.17 \\
%\hline
%\end{tabular}
%}
%\begin{tablenotes}
%\item \textbf{N}: Number of Processed Queries; \textbf{R}: Recall; \textbf{P}: Precision.
%\item {\small Note: \textbf{R}, \textbf{P}, and \textbf{F-1} calculation formula are given in Appendix \ref{appsec:effect_measure}.}
%\end{tablenotes}
%%\vspace{-0.1in}
%\caption{QALD-6 Evaluation Result}
% \label{tab:testres}
%\end{table}
%\vspace{-0.1in}
\begin{table} [h]
	\begin{tabular}
		{r | c c c c}
		\hline
		& \textbf{N} & \textbf{R} & \textbf{P} & \textbf{F-1} \\
		\hline
		\textbf{\textit{QGA(+TransE)}} & 80 & 0.62 & 0.65 & \textbf{\textit{0.63}} \\
%		\textit{SimpQGA(+TransE)}      & 80 & 0.38 & 0.62 & 0.47 \\
		\textit{QGA(+Cooccur)}          & 80 & 0.32 & 0.55 & 0.40 \\
		DPBF                         & 80 & 0.55 & 0.28 & 0.37 \\
		\textit{QGA(+TFIDF)}           & 80 & 0.26 & 0.52 & 0.35 \\
		SUMG                         & 80 & 0.24 & 0.19 & 0.21 \\
		\hline
	\end{tabular}
	\caption{Free917 Evaluation Result}
	\label{tab:freebaseres}
	\vspace{-0.4in}
\end{table}
%\begin{table} [h]
%\begin{tabular}
%{r | c c c c c}
% \hline
% & \textbf{N} & \textbf{R} & \textbf{P} & \textbf{F-1} & \textbf{F-1 Global} \\
% \hline
%\textbf{\textit{QGA+TransE}} & 80 & 0.63 & 0.66 & 0.64 & \textbf{\textit{0.64}} \\
%\textit{SimpQGA+TransE} & 80 & 0.38 & 0.62 & 0.47 & 0.47 \\
%\textit{QGA+Cooccur} & 80 & 0.32 & 0.55 & 0.40 & 0.40 \\
%DPBF & 80 & 0.55 & 0.28 & 0.37 & 0.37 \\
%\textit{QGA+TFIDF} & 80 & 0.26 & 0.52 & 0.35 & 0.35 \\
%SUMG & 80 & 0.24 & 0.19 & 0.21 & 0.21 \\
%\hline
%\end{tabular}
%\caption{Free917 Evaluation Result}
% \label{tab:freebaseres}
%\end{table}

\subsubsection{Overall Effectiveness of Our Approach}
For the keyword search systems, we compare our approach with DPBF and SUMG ,in Table \ref{tab:testres} and \ref{tab:freebaseres}. DPBF uses a dynamic programming strategy to find top-k GST over the data graph, and returns the tree nodes as answers. In DPBF, the answer set's size influences its accuracy significantly, because it varies widely on different queries.
%For example, \textit{Q10:``oldest child, Meryl Streep''} from QALD-6 only has one correct answer, while \textit{Q4:``animal, critically endangered''} has 1645 correct answers.
To favor the comparison, we assume that DPBF knows the answer set's size in advance. For each query, if its standard answer set's size is $|R|$, we set DPBF to return $3 \times |R|$ candidate answers. Then we calculate different F-1 by resizing the answer set from $|R|$ to $3\times |R|$ and find the highest one as the measure value. Despite such a favorite measurement for DPBF, it still cannot achieve desirable accuracy.
%For example, our approach achieves 0.70 at F-1 measure over QALD-6 benchmark, which is much higher than 0.29 of DPBF. 

SUMG explores the subgraph that connects keywords' mapping candidates with a minimum cost over a class based summary graph $G_S$ \cite{tran2009top}.
%This subgraph is transformed to a query graph as the optimal result. Under the summary graph's definition in \cite{tran2009top}, we summarize each RDF graph $G$ to its summary graph $G_S$.
%DBpedia summary graph consists of 452 class vertices and 116K edges. Freebase summary graph is even larger, which has 15K class vertices and 855K edges.
Because $G_S$ loses detailed information about relations between entities of the same class, SUMG performs even worse than DPBF in accuracy.

%Our approach achieves 0.63 F-1 over Free917, which is slightly lower than 0.70 over QALD-6. The main reason is that Freebase's hierarchy is much more complicated. As listed in Table \ref{tab:statistic_rdf_graph}, Freebase has more classes and relations than DBpedia, which brings more ambiguous candidates when mapping keywords to RDF items.

We also compare our approach with some NL-QA systems participating QALD-6. Table \ref{tab:testres} lists the top-6 systems that are ranked by F-1. Our system ranks at top-3 even that we only use keywords instead of the complete NL question sentences. Actually, the top-1 system CANaLI \cite{atzori2016answering} requires manual efforts for disambiguation. Our approach is better than the rank-2 system UTQA in the precision, but the recall of our approach is worse than that. As mentioned before, NL-QA system can leverage the syntactic structure of the question sentences, but it is unavailable for the keyword search system. Even so, our approach is still competitive with these NL-QA systems in effectiveness.

\vspace{-0.05in}
\subsubsection{Comparing with Different Assembly Metrics}
To show the superiority of graph embedding metrics, we also compare with traditional link prediction methods in Table \ref{tab:testres} and \ref{tab:freebaseres}. Given a triple $\langle s,p,o \rangle$, \emph{Cooccur} measures the co-occurrence frequency of $\langle s,p \rangle$, $\langle p,o \rangle$, and $\langle p,o \rangle$. \emph{TFIDF} adopts the tf-idf formula to measure the selectivity of $p$ on $\langle s, o \rangle$. As we can see, both of the two metrics are inferior to TransE on accuracy, because they can hardly capture the structural information of the RDF graph.
%There are some other similar graph embedding models, such as TransH \cite{wang2014knowledge} and TransR \cite{lin2015learning}. However, the training process of TransH and TransR are much slower than TransE (about 5\textasciitilde30 times), so it is impractical to employ them on large RDF datasets.

\subsubsection{Tuning Parameter $N$ and $k$}

In Section \ref{sec:phase1}, we mentioned that a keyword token sequence $RQ$ will be interpreted as top-$N$ $AQ$.
%The number of candidate $AQ$ generated in Phase-I (size $N$) influences our approach's overall precision and performance.
Along with $N$ growing larger, Phase-II needs to process more $AQ$, leading to a trade-off between system's accuracy and response time. Thus we tune $N \in [1,10]$  in Figure \ref{fig:chooseoptn}, and find that the growth of F-1 is almost stagnant when $N$ is larger than $5$.

In QGA, we suppose that each term is matched to at most $k$ query graph elements. Similarly, threshold $k$ also leads to a trade-off between precision and performance. Thus we tune $k \in [1,20]$  in Figure \ref{fig:lb_f1}, and find that the growth trend of F-1 will stop when $k$ is larger than $10$, while the running time still raises. Therefore, we set $N=5$ and $k=10$ in the comparative experiments.


\vspace{-0.1in}
\begin{figure} [h]
	\newcommand{\mywidth}{0.22\textwidth}
	\centering
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.45}
		{
			\includegraphics{pics/tune_aq_size}
			
		}
		\vspace{-0.2in}
		\caption{DBpedia}
		\label{fig:chooseoptn_dbpedia}
	\end{subfigure}
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.45}
		{
			\includegraphics{pics/tune_aq_size_fb}
		}
		\vspace{-0.2in}
		\caption{Freebase}
		\label{fig:chooseoptn_freebase}
	\end{subfigure}
	\vspace{-0.1in}
	\caption{Tuning Candidate AQ Size $N$}
	\vspace{-0.2in}
	\label{fig:chooseoptn}
\end{figure}

%\vspace{-0.1in}
\begin{figure} [h]
	\newcommand{\mywidth}{0.22\textwidth}
	\centering
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.45}
		{
			\includegraphics{pics/lb_f1_compare}
			
		}
		\vspace{-0.2in}
		\caption{DBpedia}
		\label{fig:lb_f1_dbpedia}
	\end{subfigure}
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.45}
		{
			\includegraphics{pics/lb_f1_compare_fb}
		}
		\vspace{-0.2in}
		\caption{Freebase}
		
		\label{fig:lb_f1_freebase}
	\end{subfigure}
	\vspace{-0.1in}
	\caption{Tuning $k$ and Comparing Different Lower Bounds}
	\label{fig:lb_f1}
	\vspace{-0.1in}
\end{figure}

%\begin{figure}[h]
%\centering
%\scalebox{0.85}
%{
%	\includegraphics[scale=1.0]{pics/tune_aq_size}
%}
%%\caption{Accuracy and Efficiency under Different $N$}%
%\vspace{-0.05in}
%\caption{Tuning Candidate AQ Size $N$}%
%\label{fig:chooseoptn}
%\end{figure}

%\begin{table} [h]
%	\begin{tabular}
%		{r | c c c | c c c}
%		\hline
%		& \multicolumn{3}{c|}{DBpedia} & \multicolumn{3}{c}{Freebase} \\
%		\cline{2-7}
%		& \textbf{R} & \textbf{P} & \textbf{F-1} & \textbf{R} & \textbf{P} & \textbf{F-1}\\
%		\hline
%		\textit{QGA+TransE}  & 0.59 & 0.86 & 0.70 & 0.63 & 0.66 & \textbf{\textit{0.64}} \\
%		\textit{QGA+Cooccur}  & 0.49 & 0.76 & 0.59 & 0.32 & 0.55 & 0.40 \\
%		\textit{QGA+TFIDF}   & 0.37 & 0.64 & 0.47 & 0.26 & 0.52 & 0.35 \\
%		\textit{SimpQGA+TransE}     & 0.41 & 0.68 & 0.51 & 0.38 & 0.62 & 0.47 \\
%		\hline
%	\end{tabular}
%	\caption{Comparing with SimpQGA and Different Cost Function}
%	\label{tab:compare_simg_costfunc}
%\end{table}

%\vspace{-0.10in}
\subsection{Evaluating Efficiency} \label{sec:efficiency}
%The system's online response time is another important criteria to evaluate the keyword search system. We concentrate on evaluating the system's performance in this subsection. 

%\vspace{-0.10in}
\subsubsection{Evaluating QGA Algorithm and Lower Bounds} \label{sec:evaluate_qga_bounds}
We evaluate the efficiency of QGA Algorithm, with three different lower bounds proposed in Section \ref{sec:lower_bound}. We count the average number of search states under the three lower bounds in Table \ref{tab:search_state_cnt}. The QGA running time with different lower bounds is illustrated in Figure \ref{fig:lb_f1}. We can see that KM-LB gives us the tightest lower bound, but it runs slowest, because the lower bound computation by KM Algorithm is expensive. Both of Naive-LB and Greedy-LB have linear time complexity, while Greedy-LB is tighter. In summary, Greedy-LB provides a trade-off between the other two and runs fastest.
%Therefore, we adopt Greedy-LB in the following efficiency experiments.

%\begin{table} [h]
%\begin{tabular}
%{ r | c  c  c c }
%\hline
%\multirow{2}{*}{} & \multicolumn{4}{c}{Number of Search State} \\
%\cline{2-5}
% & $k=5$ & $k=10$ & $k=15 $& $k=20$ \\
%\hline
%\textbf{Naive-LB}        & 566.9 & 2374.7 & 8973.0 & 15243.2 \\
%\textbf{KM-LB}     & 176.4 & 524.2 & 942.6 & 1737.0 \\
%\textbf{Greedy-LB} & 214.4 & 620.9 & 1062.9 & 2098.3 \\
%\hline
%\end{tabular}
%%\vspace{-0.1in}
%\caption{Evaluating Pruning Power of Lower Bounds. }
%\label{tab:search_state_cnt}
%\end{table}
\vspace{-0.1in}
\begin{table} [h]
\begin{tabular}
{ l | r | c  c  c c }
\hline
\multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{4}{c}{Number of Search State} \\
\cline{3-6}
& & $k=5$ & $k=10$ & $k=15 $& $k=20$ \\
\hline
\multirow{3}{*}{\begin{minipage}{0.38in}DBpedia\end{minipage}} 
& \textbf{Naive-LB}  & 566.9 & 2374.7 & 8973.0 & 15243.2 \\
& \textbf{KM-LB}     & 176.4 & 524.2  & 942.6  & 1737.0 \\
& \textbf{Greedy-LB} & 214.4 & 620.9  & 1062.9 & 2098.3 \\
\hline
\multirow{3}{*}{\begin{minipage}{0.38in}Freebase\end{minipage}} 
& \textbf{Naive-LB}  & 847.8 & 3730.5 & 13371.4 & 21689.9 \\
& \textbf{KM-LB}     & 276.5 & 867.0  & 1535.6  & 2318.2 \\
& \textbf{Greedy-LB} & 284.9 & 1175.2  & 2376.1 & 3709.2 \\
\hline
\end{tabular}
%\vspace{-0.1in}
\caption{Evaluating Pruning Power of Lower Bounds. }
\label{tab:search_state_cnt}
\vspace{-0.4in}
\end{table}


%\vspace{-0.15in}
%\begin{figure}[h]
%\centering
%\scalebox{1.0}
%{
%	\includegraphics[scale=1.0]{pics/each_step_time_new}
%}
%\vspace{-0.15in}
%\caption{Average Time Cost of Each Step.}%
%\label{fig:steptime}
%\end{figure}

\begin{table} [h]
\scalebox{0.92}
{

\begin{tabular}
{ l | r | c  c}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{2}{c}{Avg. Time Cost (ms)} \\
\cline{3-4}
& & DBpedia & Freebase \\
\hline
\multirow{3}{*}{Phase-I} 
& RDF Item Mapping               & 523.6 & 867.0 \\
& Build Candidate Term Graph     & 83.5  & 64.2 \\
& Find Top-$N$ Maximal Clique    & 11.4  & 10.9 \\
\hline
\multirow{3}{*}{Phase-II} 
& Build Assembly Bipartite Graph & 27.6  & 35.1 \\
& Run QGA Algorithm       & 29.1  & 78.5 \\
& Execute SPARQL Query           & 34.7  & 26.7\\
\hline
& \textbf{Overall Response Time} & 734.9 & 1103.2\\

\hline
\end{tabular}
}
%\vspace{-0.1in}
\caption{Average Time Cost of Each Step. }
\label{tab:time_cost_each_step}
\vspace{-0.4in}
\end{table}


\subsubsection{Time Cost of Each Step}
%full version:
%Generally, our system consists of two major phases, and the whole pipeline can be further divided into six steps, which is illustrated in Figure \ref{appfig:pipeline} of Appendix \ref{appsec:pipeline}.
Generally, our system's two-phase framework can be further divided into six detailed steps.
We report each step's time cost in Table \ref{tab:time_cost_each_step}. The most time-consuming step is KB Item Mapping, since it involves in expensive I/O operations.
Because Freebase contains more entities, classes and relations than DBpedia, it spends even more time on KB Item Mapping.
Although QGA is NP-complete, our algorithm gains high-efficiency in practice (less than 80ms per query).
%Although both of the top-$N$ maximal clique problem and QGA problem are NP-complete, the search algorithms gain high-efficiency (less than 80ms per query), because (1) the scale of Candidate Term Graph and Assembly Bipartite Graph are small; 
%full version:
%(2) the pruning strategies in Algorithm \ref{alg:bfmatch} and Algorithm \ref{alg:maxclique} perform very well. 
%(2) the pruning strategies in Algorithm \ref{alg:bfmatch} perform very well. 
We use an open-sourced SPARQL query engine gStore \footnote{https://github.com/Caesar11/gStore.git} \cite{zou2014gstore} to execute the generated SPARQL queries.
%The average time costs per SPARQL query from QALD-6 and Free917 are 34.7ms and 26.7ms, respectively.
Note that any other SPARQL query engine can be used here, which is orthogonal to our task in this paper. 


\begin{figure} [t]
	\newcommand{\mywidth}{0.48\textwidth}
	\centering
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.50}
		%        \resizebox{1.0\linewidth}{!}
		{
			\includegraphics{pics/overall_time_compare}
			
		}
		\vspace{-0.1in}
		\caption{DBpedia + QALD-6}
		\label{fig:timecompare_db}
		\vspace{-0.1in}
	\end{subfigure}
	\begin{subfigure}[t]{\mywidth}
		\centering
		\scalebox{0.50}
		%		\resizebox{1.0\linewidth}{!}
		{
			\includegraphics{pics/overall_time_compare_fb}
		}
		\vspace{-0.1in}
		\caption{Freebase + Free917}
		%        \vspace{0.1in}
		\label{fig:timecompare_fb}
	\end{subfigure}
	\vspace{-0.15in}
	\caption{Overall Response Time Comparison}    
	\label{fig:timecompare}
	\vspace{-0.2in}
\end{figure}

\vspace{-0.05in}
\subsubsection{Comparing with DPBF and SUMG}
We also compare the overall response time.
In Figure \ref{fig:timecompare}, we list the overall response time of 
10 typical queries from QALD-6 and Free917, respectively, and the average response time over all the queries.
Although DPBF and SUMG perform better than our approach on some special cases, such as $Q5$ and $F18$, our approach runs faster than DPBF and SUMG by 1\textasciitilde3 times on average (for all 180 benchmarking queries).


%\begin{figure}[t]
%\centering
%\scalebox{0.53} [0.53]
%{
%	\includegraphics[scale=1.0]{pics/overall_time_compare}
%}
%
%\caption{Overall Response Time Comparison}%
%\label{fig:timecompare}
%\end{figure}


%\vspace{-0.04in}


