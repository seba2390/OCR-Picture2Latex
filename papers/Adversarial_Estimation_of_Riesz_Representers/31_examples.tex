\section{Example Function Spaces}\label{sec:examples}

We now instantiate our two main theorems for several function classes of interest. Throughout this section we will use the following convenient characterization of the critical radius of a function class. Corollary~14.3 and Proposition~14.25 of \cite{wainwright2019high} imply that the critical radius of any function class $\mcF$, uniformly bounded in $[-b,b]$, is of the same order as any solution to the inequality:
\begin{equation}\label{eqn:metric-entropy-critical}
    \frac{64}{\sqrt{n}} \int_{\frac{\delta^2}{2b}}^{\delta} \sqrt{\log\left(N_n(\epsilon; B_n(\delta; \mcF)\right)} d\epsilon \leq \frac{\delta^2}{b}
\end{equation}
where $B_n(\delta; \mcF)=\{f\in \mcF: \|f\|_{2,n}\leq \delta\}$ and $N_n(\epsilon; \mcF)$ is the empirical $\ell_2$-covering number at approximation level $\epsilon$, i.e. the size of the smallest $\epsilon$-cover of $\mcF$, with respect to the empirical $\ell_2$ metric.

\subsection{Sparse Linear Functions}
Consider the class of $s$-sparse linear function classes in $p$ dimensions, with bounded coefficients, i.e., 
\begin{equation}
\mcA_{\splin} :=\{x \to \ldot{\theta}{x}: \|\theta\|_{0} \leq s, \|\theta\|_{\infty}\leq b\},    
\end{equation}
then observe that $\mcF$ is also the class of $s$-sparse linear functions, with bounded coefficients in $[-2b,2b]$. Moreover, suppose that the $\ell_1$-norm of the covariates $x$ is bounded. The critical radius $\delta_n$ is of order $O\left(\sqrt{\frac{s\log(p\,n)}{n}}\right)$. It is easy to see that the $\epsilon$-covering number of such a function class is of order $N_n(\epsilon; \mcF)=O\left(\binom{p}{s} \left(\frac{b}{\epsilon}\right)^{s}\right)\leq O\left(\left(\frac{p\,b}{\epsilon}\right)^s\right)$, since it suffices to choose the support of the coefficients and then place a uniform $\epsilon$-grid on the support. Thus we get that Equation~\eqref{eqn:metric-entropy-critical} is satisfied for $\delta=O\left(\sqrt{\frac{s\log(p\, b)\,\log(n)}{n}}\right)$. Moreover, observe that if $m(Z;f)$ is $L$-Lipschitz in $f$ with respect to the $\ell_{\infty}$ norm, then the covering number of $m\circ \mcF$ is also of the same order. Thus we can apply Corollary~\ref{cor:main-error} to get:
\begin{corollary}[Sparse Linear Riesz Representer]
    The estimator presented in Corollary~\ref{cor:main-error}, with $\mcA=\mcA_{\splin}$, satisfies w.p. $1-\zeta$:
    \begin{equation}
        \|\hat{a}-a_0\|_{2} \leq O\left(\min_{a\in A_{\splin}} \|a-a_0\|_2 + \sqrt{\frac{s\log(p\, b)\,\log(n)}{n}} + \sqrt{\frac{\log(1/\zeta)}{n}}\right)
    \end{equation}
\end{corollary}

The latter Theorem required a hard sparsity constraint. However, our second main theorem, Theorem~\ref{thm:reg-main-error-2}, allows us to prove a similar guarantee for the relaxed version of $\ell_1$-bounded high-dimensional linear function classes. For this corollary we require a restricted eigenvalue condition which is typical for such relaxations. 

\begin{corollary}[Sparse Linear Riesz Representer with Restricted Eigenvalue]\label{cor:sparse-linear-reg-ell1}
Suppose that $a_0(x)=\ldot{\theta_0}{x}$ with $\|\theta_0\|_0\leq s$ and $\|\theta_0\|_1\leq B$ and $\|\theta_0\|_{\infty}\leq 1$. Moreover, suppose that the covariance matrix $V=\E[xx']$ satisfies the restricted eigenvalue condition:
\begin{equation}
    \forall \nu\in \R^p \text{ s.t. } \|\nu_{S^c}\|_1 \leq \|\nu_S\|_1 + \delta_{n,\zeta}/\lambda: \nu^\top  V\nu \geq \gamma \|\nu\|_2^2
\end{equation}
Let $\mcA = \{x\to \ldot{\theta}{x}: \theta \in \R^p\}$, $\|\ldot{\theta}{\cdot}\|_{\mcA}=\|\theta\|_1$, and $\mcF=\{x \to  \xi x_i: i\in [p], \xi\in \{-1, 1\}\}$. Then the estimator presented in Equation~\eqref{eqn:reg-estimator-2} with $\lambda\leq \frac{\gamma}{8s}$, satisfies that w.p. $1-\zeta$:
\begin{equation}
    \textstyle{\|\hat{a}-a_0\|_2 \leq O\left( \max\left\{1, \frac{1}{\lambda}\frac{\gamma}{s}\right\} \sqrt{\frac{s}{\gamma}} \left((\|\theta_0\|_1 + 1)\sqrt{\frac{\log(p)}{n}} + \sqrt{\frac{\log(p/\zeta)}{n}}\right)\right)}
\end{equation}
\end{corollary}


\begin{remark}[Restricted Eigenvalue]
We note that if we have that the unrestricted minimum eigenvalue of $V$ is at least $\gamma$, then the restricted eigenvalue condition always holds. Moreover, observe that we only require a condition on the population covariance matrix $V$ and not on the empirical covariance matrix.
\end{remark}

\subsection{Neural Networks}

Suppose that the function class $\mcA$ can be represented as a RELU activation neural network with depth $L$ and width $W$, denoted as $\mcA_{\nnet(L,W)}$. Then observe that functions in $\mcF$ can be represented as neural networks with depth $L+1$ and width $2W$. Moreover, we assume that functions in $m\circ \mcF$ are also representable by neural networks of depth $O(L)$ and width $O(W)$. Finally, suppose that the covariates are distributed in a way that the outputs of $\mcF$ and $m\circ\mcF$ are uniformly bounded in $[-b,b]$. 

Then by the $L_1$ covering number for VC classes of \cite{haussler1995sphere}, the bounds of theorem 14.1 of \cite{anthony2009neural} and Theorem~6 of \cite{bartlett2019nearly}, one can show that the critical radius of $\mcF$ and $m\circ \mcF$ is of the order of $\delta_n=O\left(\sqrt{\frac{L\, W\, \log(W)\,\log(b)\, \log(n)}{n}}\right)$ (c.f. Proof of Example~3 of \cite{foster2019orthogonal} for a detailed derivation). Thus we can apply Corollary~\ref{cor:main-error} to get:
\begin{corollary}[Neural Network Riesz Representer]
    Suppose that $\mcA=\mcA_{\nnet(L,W)}$, and that $m\circ \mcF$ is representable as a neural network with depth $O(L)$ and width $O(W)$. Moreover, the input covariates are such that functions in $\mcF$ and $m\circ \mcF$ are uniformly bounded in $[-b,b]$. Then the estimator presented in Corollary~\ref{cor:main-error}, satisfies w.p. $1-\zeta$:
    \begin{equation}
        \|\hat{a}-a_0\|_{2} \leq O\left(\min_{a\in A_{\nnet(L,W)}} \|a-a_0\|_2 + \sqrt{\frac{L\, W\, \log(W)\,\log(b)\, \log(n)}{n}} + \sqrt{\frac{\log(1/\zeta)}{n}}\right)
    \end{equation}
\end{corollary}

If the true Riesz representer $a_0$ is representable as a RELU neural network, then the first term vanishes and we achieve an almost parametric rate. For non-parametric Holder function classes, one can easily combine the latter corollary with approximation results for RELU activation neural networks presented in \cite{yarotsky2017error,yarotsky2018optimal}. These approximation results typically require that the depth and the width of the neural network grow as some function of the approximation error $\epsilon$, leading to errors of the form: $O\left(\epsilon + \sqrt{\frac{L(\epsilon)\, W(\epsilon)\, \log(W(\epsilon))\,\log(b)\, \log(n)}{n}} + \sqrt{\frac{\log(1/\zeta)}{n}}\right)$. Optimally balancing $\epsilon$ then typically leads to almost tight non-parametric rates, of the same order as those presented in Theorem~1 of \cite{farrell2018DeepNeural}.

\subsection{Reproducing Kernel Hilbert Spaces}

Suppose that $a_0$ lies in a Reproducing Kernel Hilbert Space (RKHS) with kernel $K$, denoted as $\mcA_{\rkhs(K)}$ and with the norm $\|\cdot\|_{\mcA}$ being the RKHS norm. Then observe that $\mcF$ is the same function space. Moreover, we assume that $m\circ \mcF$ also lies in an RKHS with a potentially different kernel $\tilde{K}$. Finally, suppose that the input covariates are such that for some constant $B$, functions in $\mcF_B$ and $m\circ \mcF_B$ are bounded in $[-1, 1]$. 

Let $\{\hat{\lambda}_j\}_{j=1}^n$ be the eigenvalues of the $n\times n$ empirical kernel matrix, with $K_{ij}=K(x_i, x_j)/n$. Similarly, let $\{\hat{\mu}_j\}_{j=1}^n$ be the eigenvalues of the empirical kernel matrix $\tilde{K}$. Then by Corollary~13.18 of \cite{wainwright2019high}, we can derive the following corollary of Theorem~\ref{thm:reg-main-error}:
\begin{corollary}[RKHS Riesz Representer]
    Suppose that $\mcA=\mcA_{\rkhs}$, $a_0\in \mcA_{\rkhs}$, and that $m\circ \mcF\in \mcA_{\rkhs(\tilde{K})}$. Let $\{\hat{\lambda}_j\}_{j=1}^n$ and $\{\hat{\mu}_j\}_{j=1}^n$ be  the egienvalues of the empirical kernel matrices of $K$ and $\tilde{K}$, correspondingly. Let $\delta_n$ be any solution to the inequalities:
    \begin{align}
        B\sqrt{\frac{2}{n}}\sqrt{\sum_{j=1}^\infty \max\{\hat{\lambda}_j, \delta^2\}}\leq~& \delta^2 &
        B\sqrt{\frac{2}{n}}\sqrt{\sum_{j=1}^\infty \max\{\hat{\mu}_j, \delta^2\}}\leq~& \delta^2
    \end{align}
    Moreover, the input covariates are such that functions in $\mcF_B$ and $m\circ \mcF_B$ are uniformly bounded in $[-1, 1]$. Then the estimator presented in Theorem~\ref{thm:reg-main-error}, satisfies w.p. $1-\zeta$:
    \begin{equation}
        \|\hat{a}-a_0\|_{2} \leq O\left(\|a_0\|_{\mcA} \left( \delta_n + \sqrt{\frac{\log(1/\zeta)}{n}}\right)\right)
    \end{equation}
\end{corollary}

We note that the latter estimator does not need to know the RKHS norm of the true function $a_0$. Instead it automatically adapts to the unknown RKHS norm. Moreover, note that the bound $\delta_n$ is solely based on empirically observable quantities, as it is a function of the empirical eigenvalues. Thus these empirical quantities can be used as a data-adaptive diagnostic of the error.

Finally, we note that for particular kernels a more explicit bound can be derived as a function of the eigendecay. For instance, for the Gaussian kernel, which has an exponential eigendecay, Example~13.21 of \cite{wainwright2019high} derives that the solution to the eigenvalue inequality scales as $O\left(\sqrt{\frac{\log(n)}{n}}\right)$, thus leading to almost parametric rates: $\|\hat{a}-a_0\|_2 \leq O\left(\|a_0\|_{\mcA} \sqrt{\frac{\log(n)}{n}}\right)$.
