\section{Introduction}

Many problems in econometrics, statistics, causal inference, and finance involve linear functionals of unknown functions:
\begin{equation}
\theta(g)=\E[m(Z; g)]
\end{equation}
where $Z$ denotes a random vector, and $g: \mcX\to \R$ is a function in some space $ \mcG$. A continuous linear functional that is mean square continuous with respect to $\ell_2$ norm can be written in a more benign and useful manner. Formally, for a given linear functional $\theta(\cdot)$, there exists a function $a_0$ such that for any $g\in \mcG$:\footnote{For simplicity of exposition, throughout the paper we consider scalar-valued functions $g$. All our results naturally extend to vector-valued functions $g$, and estimate a vector valued Riesz representer that satisfies that $\theta(g)=\E[a(X)'g(X)]$.}
\begin{equation}
    \theta(g) = \E[a_0(X)\, g(X)]
\end{equation}
This result is known as the Riesz representation theorem, and the function $a_0$ is the Riesz representer of the linear functional. Evaluation of a linear functional $\theta(g)$ can be achieved by simply taking the inner product between $a_0$ and $g$.

Knowing the Riesz representation of a linear functional is a critical building block in a variety of learning problems. For instance, in semi-parametric models, $g_0$ is an unknown regression function and $\theta(g_0)$ is a causal or structural parameter of interest. The Riesz representer $a_0$ of the functional $\theta(\cdot)$ can be used to debias the plug-in estimator and construct semi-parametrically efficient estimators of the parameter $\theta(g_0)$. In asset pricing applications, the Riesz representer corresponds to the stochastic discount factor, which is of primary interest when pricing financial derivatives.

Irrespective of the downstream application, the goal of this paper is to derive an estimator for the Riesz representer of any linear functional, when given access to $n$ samples of the random vector $Z$ and a target function space $\mcA$ that can well approximate the function $a_0$. We propose and analyze an estimator $\hat{a}$, with small mean-squared-error. Formally, with probability (w.p.) $1-\zeta$:
\begin{equation}
    \|\hat{a}-a_0\|_2 = \sqrt{\E\left[\left(\hat{a}(X) - a_0(X)\right)^2\right]} \leq \epsilon_{n,\zeta}
\end{equation}

We consider estimation of the Riesz representer within some function space $\mcA$ and propose an adversarial estimator based on regularized variants of the following min-max criterion:
\begin{equation}
    \hat{a} = \argmin_{a\in \mcA} \max_{f\in \mcF} \frac{1}{n}\sum_{i=1}^n \left(m(Z_i;f) - a(X_i)\cdot f(X_i) - f(X_i)^2\right)
\end{equation}
We derive oracle inequalities for this estimator as a function of the localized Rademacher complexity of the function space $\mcA$ and the approximation error $\epsilon = \min_{a\in \mcA} \|a-a_0\|_{2}$.

We show that as long as the function class $\mcF$ contains the star-hull of differences of functions in $\mcA$, i.e. $\mcF:= \{r(a-a'): a, a'\in \mcA, r\in [0, 1]\}$, then the estimation rate of the adversarial estimator achieves w.p. $1-\zeta$:
\begin{equation}
    \|\hat{a} - a_0\|_2 = O\left(\epsilon + \delta_n + \sqrt{\frac{\log(1/\zeta)}{n}}\right)
\end{equation}
where $\delta_n$ is the critical radius of the function classes $\mcF$ and $m\circ \mcF=\{Z\to m(Z; f): f\in \mcF\}$. The critical radius of a function class is a widely used quantity in statistical learning theory that allows one to argue fast estimation rates that are nearly optimal. For instance, for parametric function classes, the critical radius is of order $n^{-1/2}$, leading to fast parametric rates (as compared to $n^{-1/4}$ which would be achievable via looser uniform deviation bounds).

Moreover, the critical radius has been analyzed and derived for a variety of function spaces of interest, such as neural networks, high-dimensional linear functions, reproducing kernel Hilbert spaces, and VC-subgraph classes. Thus our general theorem allows us to appeal to these characterizations and provide oracle rates for a family of Riesz representer estimators. Prior work on estimating Riesz representers only considered particular high-dimensional parametric classes and derived specialized estimators for the function space of interest. Our adversarial estimator provides a single approach that tackles generic function spaces in a uniform manner.

We also examine the computational aspect of our estimator. We provide examples of how estimation can be achieved in a computationally efficient manner for several function spaces of interest.

Finally, we show how our estimator can be used in the context of estimating causal or structural parameters in semi-parametric models. Specifically, our mean square rate for the Riesz representer is sufficiently fast to achieve semi-parametric efficiency and asymptotic normality of the causal or structural parameter.

\subsection{Applications: Causal Inference and Asset Pricing}\label{sec:intro_examples}

This learning problem arises in two important domains for economic research: causal inference and asset pricing.

\paragraph{Automated De-biasing of Causal Estimates.} In causal inference, a variety of treatment effects and policy effects can be formulated as functionals--i.e., scalar summaries--of an underlying regression \cite{chernozhukov2016locally}. Formally, the causal parameter $\theta_0=\theta(g_0)=\mathbb{E}[m(Z;g_0)]$ is a functional $\theta(\cdot)$ of the nuisance parameter $g_0(x):=\mathbb{E}[Y|X=x]$. In this paper, we consider a variety of treatment and policy effects including
\begin{enumerate}
    \item Average treatment effect (ATE): $\theta_0=\mathbb{E}[g_0(1,W)-g_0(0,W)]$, where $X=(D,W)$ consists of treatment and covariates.
    \item Average policy effect: $\theta_0=\int g_0(x)d\mu(x)$ where $\mu(x)=F_1(x)-F_0(x)$
    \item Policy effect from transporting covariates: $\theta_0=\mathbb{E}[g_0(t(X))-g_0(X)]$
    \item Cross effect: $\theta_0=\mathbb{E}[Dg_0(0,W)]$, where $X=(D,W)$ consists of treatment and covariates.
    \item Regression decomposition: $\mathbb{E}[Y|D=1]-\mathbb{E}[Y|D=0]=\theta_0^{response}+\theta_0^{composition}$
    where
    \begin{align}
        \theta_0^{response}&=\mathbb{E}[g_0(1,W)|D=1]-\mathbb{E}[g_0(0,W)|D=1] \\
        \theta_0^{composition}&=\mathbb{E}[g_0(0,W)|D=1]-\mathbb{E}[g_0(0,W)|D=0]
    \end{align}
    \item Average treatment on the treated (ATT): $\theta_0=\mathbb{E}[g_0(1,W)|D=1]-\mathbb{E}[g_0(0,W)|D=1]$, where $X=(D,W)$ consists of treatment and covariates.
    \item Local average treatment effect (LATE): $\theta_0=\frac{\mathbb{E}[g_0(1,W)-g_0(0,W)]}{\mathbb{E}[h_0(1,W)-h_0(0,W)]}$, where $X=(V,W)$ consists of instrument and covariates and $h_0(x):=\mathbb{E}[D|X=x]$ is a second regression.
\end{enumerate}
More generally, our results extend to parameters defined implicitly by $0=\mathbb{E}[m(Z;g_0;\theta_0)]$, such as partially linear regression and partially linear instrumental variable regression.

    If the regression $g_0$ is learned by a regularized estimator $\hat{g}$, then estimation of the causal parameter $\theta_0$  by a plug-in estimator $\mathbb{E}_n[m(Z;\hat{g})]$ is badly biased. The solution is to use a de-biased formulation of the causal parameter instead: $\theta_0=\mathbb{E}[m(Z;g_0)+a_0(X)\{Y-g_0(X)\}]$. Observe that $a_0$ arises in the bias correction term. We re-visit this example in Section~\ref{sec:debiasing}.

%

\paragraph{Fundamental Asset Pricing Equation.} In asset pricing, a variety of financial models deliver the same fundamental asset pricing equation. This equation is of both theoretical and practical interest. Theoretically, it elucidates why asset prices or returns are what they are. Practically, it can be used to identify trading opportunities when assets are mis-priced. The asset pricing equation follows from two weak assumptions: free portfolio formation, and the law of one price.  In Appendix~\ref{sec:finance}, we review the derivation for a general audience.\footnote{The same asset pricing equation can be derived from either a model of complete markets for contingent claims, or a model of investor utility maximization. Free portfolio formation is a weaker assumption on market structure than the existence of complete markets for contingent claims. The law of one price is a weaker assumption on preference structure than investor utility maximization. We present these additional derivations in Appendix~\ref{sec:finance}.}

Formally, the fundamental asset pricing equation is $p_{t,i}=\mathbb{E}_t[m_{t+1}x_{t+1,i}]$ where $p_{t,i}$ is the price of asset $i$ at time $t$, $x_{t+1,i}$ is payoff of asset $i$ at time $t+1$, and $m_{t+1}$ is the market-wide stochastic discount factor (SDF) at time $t+1$.\footnote{The SDF has many additional names: marginal rate of substitution, state price density, and pricing kernel. Each name corresponds to a different derivation of the asset pricing equation, starting from different first principles.} The expectation is conditional on information $(I_t,I_{t,i})$ known at time $t$:  $I_t$ are macroeconomic conditioning variables that are not asset specific, e.g. inflation rates and market return; $I_{t,i}$ are asset-specific characteristics, e.g. the size or book-to-market ratio of firm $i$ at time $t$. The asset pricing equation encompasses stocks, bonds, and options. We clarify its many instantiations below, where $d_{t+1}$ is dividend, $C$ is the call price, $S_T$ is the stock price at expiration, $K$ is the strike price. 

\begin{table}[H]
       \centering
       \begin{tabular}{|c||c|c|}
        \hline 
            Asset & Price $p_t$ & Payoff $x_{t+1}$ \\
             \hline 
            \hline
            Stock &$p_t$& $p_{t+1}+d_{t+1}$ \\
              Bond &$p_t$&$1$\\
             Option &$C$&$\max\{S_T-K,0\}$ \\
             \hline 
            Return & $1$& $R_{t+1}$ \\
            Excess return &0&$R^e_{t+1}$ \\
            \hline 
       \end{tabular}
       \caption{Generality of asset pricing equation}
       \label{tab:my_label}
   \end{table}
 
 The fundamental asset pricing equation can also be parametrized in terms of returns. If an investor pays one dollar for an asset $i$ today, the gross rate of return $R_{t+1,i}$ is how many dollars the investor receives tomorrow; formally, the price is $p_{t,i}=1$ and the payoff is $x_{t+1,i}=R_{t+1,i}$ by definition. Next consider what happens when an investor borrows a dollar today at the interest rate $R_{t+1}^f$ and buys an asset $i$ that gives the gross rate of return $R_{t+1,i}$ tomorrow. From the perspective of the investor, who paid nothing out-of-pocket, the price is $p_{t,i}=0$ while the payoff is the excess rate of return $R_{t+1,i}^e:=R_{t+1,i}-R_{t+1}^f$, leading to the asset pricing equation: $0=\mathbb{E}_t[m_{t+1}R^e_{t+1,i}]$.
 
 
 Following \cite{chen2019deep}, we focus on the latter excess return parametrization of the asset pricing equation. Taking expectations yields the unconditional moment restriction
$$
0=\mathbb{E}[m_{t+1}R^e_{t+1,i}z(I_t,I_{t,i})]=\mathbb{E}[\mathbb{E}[m_{t+1}|R^e_{t+1,i},I_t,I_{t,i}]R^e_{t+1,i}z(I_t,I_{t,i})],\quad \forall z(\cdot)
$$
Our framework nests this final expression. Specifically,
$$
\theta(g)=0,\quad g(R^e_{t+1,i},I_t,I_{t,i})=R^e_{t+1,i}z(I_t,I_{t,i}),\quad a_0(R^e_{t+1,i},I_t,I_{t,i})=\mathbb{E}[m_{t+1}|R^e_{t+1,i},I_t,I_{t,i}]
$$
By estimating $a_0$, which is the projection of the SDF onto excess returns and other available information, one can pin down the price of any hypothetical asset. 

%
%
%
%

\subsection{Related Work}

\textbf{Classical Semi-parametric Statistics.} Classical semi-parametric statistical theory studies the asymptotic properties of statistical quantities that are functionals of a density or a regression over a low-dimensional domain \cite{levit1976efficiency,hasminskii1979nonparametric,ibragimov1981statistical,pfanzagl1982lecture,klaassen1987consistent,robinson1988root,van1991differentiable,bickel1993efficient,newey1994asymptotic,robins1995semiparametric,vaart,bickel1988estimating,newey1998undersmoothing,ai2003efficient,newey2004twicing,ai2007estimation,tsiatis2007semiparametric,kosorok2007introduction,ai2012semiparametric}. Any continuous linear functional has a Riesz representer. In this classical theory, the Riesz representer appears in the influence function and therefore in the asymptotic variance of semi-parametric estimators \cite{newey1994asymptotic}. We depart from classical theory by considering the high-dimensional setting.

\textbf{De-biased Machine Learning and Targeted Maximum Likelihood.} Because the Riesz representer appears in the asymptotic variance of semi-parametric estimators, it can be incorporated into estimation to ensure semi-parametric efficiency. In practice, this can be achieved by introducing a de-biasing term into the estimating equation \cite{hasminskii1979nonparametric,bickel1988estimating,zhang2014confidence,belloni2011inference,belloni2014inference,belloni2014uniform,belloni2014pivotal,javanmard2014confidence,javanmard2014hypothesis,javanmard2018debiasing,van2014asymptotically,ning2017general,chernozhukov2015valid,neykov2018unified,ren2015asymptotic,jankova2015confidence,jankova2016confidence,jankova2018semiparametric,bradic2017uniform,zhu2017breaking,zhu2018linear}. In doubly robust estimating equations for regression functionals, the de-biasing term is the product between the Riesz representer and the regression residual \cite{robins1995analysis,robins1995semiparametric,van2006targeted,van2011targeted,luedtke2016statistical,toth2016tmle}. The more general principle at play is Neyman orthogonality: the learning problem for the functional of interest becomes orthogonal to the learning problems for both the regression and the Riesz representer \cite{neyman1959,neyman1979c,vaart,robins2008higher,zheng2010asymptotic,belloni2014uniform,belloni2014pivotal,chernozhukov2016locally,belloni2017program,chernozhukov2018double,foster2019orthogonal}.

De-biased machine learning and targeted maximum likelihood combine the algorithmic insight of doubly-robust moment functions with the algorithmic insight of sample splitting \cite{bickel1982adaptive,schick1986asymptotically,klaassen1987consistent,vaart,robins2008higher}.  In doing so, these frameworks facilitate a general analysis of residuals such that the target functional is $\sqrt{n}$-consistent under minimal assumptions on the estimators used for the regression and Riesz representer \cite{scharfstein1999adjusting,rubin2005general,rubin2006extending,van2006targeted,zheng2010asymptotic,van2011targeted,diaz2013targeted,van2014targeted,kennedy2017nonparametric,kennedy2020optimal}. In particular, any machine learning estimators are permitted that satisfy $\sqrt{n}\|\hat{g}-g_0\|_2\cdot\|\hat{a}-a_0\|_2\rightarrow 0$ \cite{chernozhukov2018double,chernozhukov2016locally}.

The Riesz representer may be a difficult object to estimate. Even for simple regression functionals such as policy effects, its closed form involves ratios of densities. In restricted models, where the regression is known to belong to a certain function class, there is the further difficulty of projecting the Riesz representer accordingly. A recent literature explores the possibility of directly estimating the Riesz representer, without estimating its components or even knowing its functional form \cite{robins2007comment,newey2018cross,athey2018approximate,chernozhukov2018global,chernozhukov2018learning,hirshberg2018debiased,hirshberg2019augmented,singh2019biased,rothenhausler2019incremental}. A crucial insight, on which we build, is that the Riesz representer is directly identified from data. 

\cite{hirshberg2019augmented} observe that to debias an average moment, it is sufficient to estimate an empirical analogue of the Riesz representer that approximately satisfies the Riesz representer moment equation on the $n$ samples. They propose a parametric min-max criterion to estimate $n$ parameters corresponding to the $n$ evaluations of the empirical Riesz representer. Unlike \cite{hirshberg2019augmented}, we provide a guarantee on learning the true Riesz representer, we approximate the Riesz representer within non-parametric function spaces, and our result therefore has broader application beyond causal inference. Importantly, \cite{hirshberg2019augmented} require that the same sample used to estimate the $n$ parameters is used in final stage estimation of the causal parameter. As such, the analysis requires that the regression function $g$ lies in a Donsker class--a restriction that precludes many machine learning estimators. By contrast, our adversarial estimator provides fast estimation rates with respect to the true Reisz representer and hence can be used in combination with cross-fitting and sample splitting to eliminate the Donsker assumption.


\textbf{Adversarial Estimation.} Riesz representation theorem can be viewed as a continuum of unconditional moment restrictions. The non-parametric instrumental variable problem, based on a conditional moment restriction, also implies a continuum of unconditional moment restrictions \cite{newey2003instrumental,hall2005nonparametric,blundell2007semi,chen2009efficient,darolles2011nonparametric,chen2012estimation,chen2015sieve,chen2018optimal}. A central insight of this work is that the min-max approach for conditional moment models may be adapted to the problem of learning the Riesz representer. In a min-max approach, the continuum of unconditional moment restrictions is enforced adversarially over a set of test functions \cite{goodfellow2014generative,arjovsky2017wasserstein,dikkala2020minimax}. 

The fundamental advantage of the min-max approach is its unified analysis over arbitrary function classes. In particular, via local Rademacher analysis, one can derive an abstract bound that encompasses sparse linear models, neural networks, and RKHS methods \cite{koltchinskii2000rademacher,bartlett2005local}. As such, the min-max approach is actually a family of algorithms adaptive to a variety of data settings with a unified guarantee \cite{negahban2012,lecue2017regularization,Lecue2018}. 

\textbf{Machine Learning Approaches to Causal Inference and Asset Pricing.} By pursuing a min-max approach, our work relates to previous work that incorporates a variety of machine learning methods into causal inference. Much work on de-biased machine learning focuses on sparse and approximately sparse models \cite{chernozhukov2018global,chernozhukov2018learning,chernozhukov2018plug}. A neural network estimator with mean square rate has been successfully used to learn the nuisance regression in semiparametric estimation \cite{chen1999improved,farrell2018deep} and to learn the structural function in nonparametric instrumental variable regression \cite{deepiv,bennett2019deep,dikkala2020minimax}. A more recent literature incorporates RKHS methods into causal inference due to their convenient closed form solutions and strong performance on smooth designs \cite{nie2017quasi,singh2019kernel,muandet2019dual,singh2020kernel,muandet2020kernel}.

Finally, our works provides a theoretical foundation for a growing literature that incorporates machine learning into asset pricing. We follow the asset pricing literature in framing the problem of learning a stochastic discount factor as the problem of learning a Riesz representer \cite{hansen1997assessing}. Specifically, we propose a deep min-max approach based on free portfolio formation and the law of one price \cite{bansal1993no,chen2019deep}. This approach differs from deep learning approaches that predict asset prices via nonparametric regression \cite{messmer2017deep,feng2018deep,gu2020autoencoder,bianchi2020bond}. Unlike previous work, we prove mean square rates for the stochastic discount factor, and we prove $\sqrt{n}$-consistency and semiparametric efficiency for expected asset prices.