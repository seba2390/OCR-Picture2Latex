\section{Proofs from \Cref{sec:computation}}

\subsection{Proof of \Cref{prop:sparse-optimization-ell1}}

\begin{proposition}
Consider an online linear optimization algorithm over a convex strategy space $S$ and consider the OFTRL algorithm with a $1$-strongly convex regularizer with respect to some norm $\|\cdot\|$ on space $S$:
\begin{equation}
    f_t = \argmin_{f \in S} f^\top \left(\sum_{\tau\leq t} \ell_{\tau} + \ell_t\right) + \frac{1}{\eta} R(f)
\end{equation}
Let $\|\cdot\|_*$ denote the dual norm of $\|\cdot\|$ and $R=\sup_{f\in S} R(f) - \inf_{f\in S} R(f)$. Then for any $f^*\in S$:
\begin{equation}
    \sum_{t=1}^T (f_t-f^*)^\top \ell_t \leq \frac{R}{\eta} + \eta \sum_{t=1}^T \|\ell_t - \ell_{t-1}\|_* - \frac{1}{4\eta} \sum_{t=1}^T \|f_t - f_{t-1}\|^2
\end{equation}
\end{proposition}
\begin{proof}
The proof follows by observing that Proposition~7 in \cite{syrgkanis2015fast} holds verbatim for any convex strategy space $S$ and not necessarily the simplex.
\end{proof}

\begin{proposition}\label{prop:appendix-minimax}
Consider a minimax objective: $\min_{\theta\in \Theta} \max_{w\in W} \ell(\theta, w)$. Suppose that $\Theta, W$ are convex sets and that $\ell(\theta, w)$ is convex in $\theta$ for every $w$ and concave in $\theta$ for any $w$. Let $\|\cdot\|_\Theta$ and $\|\cdot\|_W$ be arbitrary norms in the corresponding spaces. Moreover, suppose that the following Lipschitzness properties are satisfied:
\begin{align}
    \forall \theta\in \Theta, w, w'\in W: \left\|\nabla_{\theta}\ell(\theta, w)  - \nabla_{\theta}\ell(\theta, w')\right\|_{\Theta, *} \leq L \|w-w'\|_W\\
    \forall w\in W, \theta, \theta'\in \Theta: \left\|\nabla_{w}\ell(\theta, w)  - \nabla_{w}\ell(\theta', w)\right\|_{W, *} \leq L \|\theta-\theta'\|_\Theta
\end{align}
where $\|\cdot\|_{\Theta, *}$ and $\|\cdot\|_{W, *}$ correspond to the dual norms of $\|\cdot\|_{\Theta}, \|\cdot\|_W$. Consider the algorithm where at each iteration each player updates their strategy based on:
\begin{align}
    \theta_{t+1} =~& \argmin_{\theta\in \Theta} \theta^\top \left(\sum_{\tau\leq t} \nabla_{\theta}\ell(\theta_\tau, w_\tau) + \nabla_{\theta} \ell(\theta_t, w_t)\right) + \frac{1}{\eta} R_{\min}(\theta)\\
    w_{t+1} =~& \argmax_{w\in W} w^T \left(\sum_{\tau \leq t} \nabla_{w} \ell(\theta_\tau, w_\tau) + \nabla_w \ell(\theta_t, w_t)\right) - \frac{1}{\eta} R_{\max}(w)
\end{align}
such that $R_{\min}$ is $1$-strongly convex in the set $\Theta$ with respect to norm $\|\cdot\|_\Theta$ and $R_{\max}$ is $1$-strongly convex in the set $W$ with respect to norm $\|\cdot\|_W$ and with any step-size $\eta \leq \frac{1}{4L}$. Then the parameters $\bar{\theta} = \frac{1}{T} \sum_{t=1}^T \theta_t$ and $\bar{w}=\frac{1}{T}\sum_{t=1}^T w_t$ correspond to an $\frac{2 R_*}{\eta \cdot T}$-approximate equilibrium and hence $\bar{\theta}$ is a $\frac{4 R_*}{\eta T}$-approximate solution to the minimax objective, where $R$ is defined as:
\begin{equation}
    R_* := \max\left\{ \sup_{\theta\in \Theta} R_{\min}(\theta) - \inf_{\theta\in \Theta} R_{\min}(\theta), \sup_{w\in W} R_{\max}(w)-\inf_{w\in W} R_{\max}(w)\right\}
\end{equation}
\end{proposition}
\begin{proof}
The proposition is essentially a re-statement of Theorem~25 of \cite{syrgkanis2015fast} (which in turn is an adaptation of Lemma~4 of \cite{Rakhlin2013}), specialized to the case of the OFTRL algorithm and to the case of a two-player convex-concave zero-sum game, which implies that the if the sum of regrets of players is at most $\epsilon$, then the pair of average solutions corresponds to an $\epsilon$-equilibrium (see e.g. \cite{FREUND199979} and Lemma~4 of \cite{Rakhlin2013}).
\end{proof}

\paragraph{Proof of \Cref{prop:sparse-optimization-ell1}} Let $R_E(x)=\sum_{i=1}^{2p} x_i \log(x_i)$. For the space $\Theta:=\{\rho\in \R^{2p}: \rho \geq 0, \|\rho\|_1\leq B\}$, the entropic regularizer is $\frac{1}{B}$-strongly convex with respect to the $\ell_1$ norm and hence we can set $R_{\min}(\rho)=B\, R_{E}(\rho)$. Similarly, for the space $W:=\{w\in \R^{2p}: w\geq 0, \|w\|_1=1\}$, the entropic regularizer is $1$-strongly convex with respect to the $\ell_1$ norm and thus we can set $R_{\max}(w)=R_E(w)$. For this choice of regularizers, the update rules can be easily verified to have a closed form solution provided in \Cref{prop:sparse-optimization-ell1}, by writing the Lagrangian of each OFTRL optimization problem and invoking strong duality. Further, we can verify the lipschitzness conditions. Since the dual of the $\ell_1$ norm is the $\ell_{\infty}$ norm,  $\nabla_{\rho}\ell(\rho, w) = \E_n[VV^\top] w + \lambda$ and thus:
\begin{align}
    \left\|\nabla_{\rho}\ell(\rho, w) - \nabla_{\rho}\ell(\rho, w')\right\|_{\infty} =\|\E_n[VV^\top] (w-w')\|_{\infty} \leq \|\E_n[VV^\top]\|_{\infty} \|w-w'\|_1\\
    \left\|\nabla_{w}\ell(\rho, w) - \nabla_{w}\ell(\rho', w)\right\|_{\infty} =\|\E_n[VV^\top] (\rho-\rho')\|_{\infty} \leq \|\E_n[VV^\top]\|_{\infty} \|\rho-\rho'\|_1\\
\end{align}
Thus we have $L=\|\E_n[VV^\top]\|_{\infty}$. Finally, observe that:
\begin{align}
    \sup_{\rho\in \Theta} B\, R_{E}(\rho) - \inf_{\rho\in \Theta} B\, R_E(\rho) =~& B^2 \log(B\vee 1) + B \log(2p)\\
    \sup_{w\in W} R_{E}(w) - \inf_{w\in W} R_E(w) =~& \log(2p)
\end{align}
Thus we can take $R_*=B^2 \log(B\vee 1) + (B+1) \log(2p)$. Thus if we set $\eta = \frac{1}{4\|\E_n[VV^\top]\|_{\infty}}$, then we have that after $T$ iterations, $\bar{\theta}=\bar{\rho}^+-\bar{\rho}^-$ is an $\epsilon(T)$-approximate solution to the minimax problem, with \begin{equation}
\epsilon(T)=16\|\E_n[VV^\top]\|_{\infty} \frac{4B^2 \log(B\vee 1) + (B+1) \log(2p)}{T}.
\end{equation}
Combining all the above with \Cref{prop:appendix-minimax} yields the proof of \Cref{prop:sparse-optimization-ell1}.


\subsection{Proof of \Cref{prop:oracle}}


Observe that the loss function $-\ell(a, \cdot)$ is strongly convex in $f$ with respect to the $\|\cdot\|_{2,n}$ norm, i.e.:
\begin{equation}
    -\frac{1}{2} D_{ff} \ell(a, f)[\nu, \nu] \geq \E_n[\nu(X)^2]
\end{equation}
and that the difference:
\begin{equation}
    \ell(a, f) - \ell(a', f) = \E_n[(a(X)-a'(X))\cdot f(X)]
\end{equation}
is an $\|a-a'\|_{2,n}$-Lipschitz function with respect to the $\ell_{2,n}$ norm (via a Cauchy-Schwarz inequality). Thus we can conclude that (see Lemma~1 in \cite{syrgkanis}):
\begin{equation}
    \|f_t - f_{t+1}\|_{2,n} \leq \|\bar{a}_{<t} - \bar{a}_{<t+1}\|_{2, n}
\end{equation}
Moreover, we know that the cumulative regret of the FTL algorithm is at most (see proof of Theorem~1 in \cite{syrgkanis}):
\begin{equation}
    R(T) \leq \sum_{t=1}^T \left|\ell(a_t, f_t) - \ell(a_t, f_{t+1})\right|
\end{equation}
Since $\|a_t\|_{\infty}, \|f_t\|_{\infty} \leq 1$, each summand of the latter is upper bounded by:
\begin{equation}
    \left|\E_n[m(Z; f_t - f_{t+1})]\right| + 3\|f_t - f_{t+1}\|_{1,n}
\end{equation}
We will assume that the empirical operator $E_n[m(Z; f)]$ is also a bounded linear operator, with a bound of $M_n$. Thus we have: 
\begin{equation}
    \left|\E_n[m(Z; f_t - f_{t+1})]\right| \leq M_n \|f_t - f_{t+1}\|_{2,n}
\end{equation}
Thus overall we get:
\begin{equation}
     \left|\ell(a_t, f_t) - \ell(a_t, f_{t+1})\right| \leq (M_n+3) \|f_t - f_{t+1}\|_{2,n} \leq (M_n+3) \|\bar{a}_{<t} - \bar{a}_{<t+1}\|_{2,n} \leq \frac{2\,(M_n+3)}{t}
\end{equation}
where we used the fact that $\left|\bar{a}_{<t}(X) - \bar{a}_{<t+1}(X)\right|\leq \frac{2}{t}$, since $\|a\|_{\infty}\leq 1$. Thus we conclude that:
\begin{equation}
    R(T) \leq 2\,(M_n+3)\sum_{t=1}^T \frac{1}{t} = O(M_n\, \log(T))
\end{equation}
Thus after $T=\Theta\left(\frac{M_n\, \log(1/\epsilon)}{\epsilon}\right)$ iterations, of the algorithm, the $f$-player has regret of at most $\epsilon$. By standard results in solving convex-concave zero-sum games, this then implies that the average solutions: $f_*=\frac{1}{T}\sum_{t=1}^T f_t$ and $a_* = \frac{1}{T} \sum_{t=1}^T a_t$ are an $\epsilon$-equilibrium and therefore also that $a_*$ is an $\epsilon$-approximate solution to the minimax problem. This concludes the proof of the proposition.

\subsection{Proof of Proposition~\ref{prop:kernel_matrices}}

\begin{proof}
For example for ATE
\begin{align}
    [K^{(3)}]_{ij}&=[\Phi^{(m)}\Phi']_{ij} \\
    &=\langle M^*\phi(x_i),\phi(x_j)\rangle\\
    &=\langle \phi(x_i),M\phi(x_j)\rangle \\
    &= \langle \phi(d_i,w_i),\phi(1,w_j)-\phi(0,w_j)\rangle \\
    &=k((d_i,w_i),(1,w_j))-k((d_i,w_i),(0,w_j))
\end{align}
Likewise
\begin{align}
    [K^{(4)}]_{ij}&=[\Phi^{(m)}(\Phi^{(m)})']_{ij}\\
    &=\langle M^*\phi(x_i),M^* \phi(x_j)\rangle\\
    &=\langle \phi(x_i),M M^* \phi(x_j)\rangle\\
    &=\langle \phi(x_i),M^* \phi(1,w_j)-M^*\phi(0,w_j)\rangle \\
    &=\langle M\phi(x_i),\phi(1,w_j)-\phi(0,w_j) \rangle \\
    &=\langle \phi(1,w_i)-\phi(0,w_i),\phi(1,w_j)-\phi(0,w_j) \rangle \\
    &=k((1,w_i),(1,w_j))-k((1,w_i),(0,w_j))-k((0,w_i),(1,w_j))+k((0,w_i),(0,w_j))
\end{align}
\end{proof}

\subsection{Proof of Proposition~\ref{prop:rep1}}

\begin{proof}
Write the objective as 
$$
\mathcal{E}_{1}(f):=\frac{1}{n}\sum_{i=1}^n \langle f,M^*\phi(x_i)\rangle_{\mathcal{H}}-a(x_i)\langle f,\phi(x_i)\rangle_{\mathcal{H}}-\langle f,\phi(x_i)\rangle_{\mathcal{H}}^2-\lambda\|f\|^2_{\mathcal{H}}
$$
Recall that for an RKHS, evaluation is a continuous functional represented as the inner product with the feature map. Due to the ridge penalty, the stated objective is coercive and strongly convex w.r.t $f$. Hence it has a unique maximizer $\hat{f}$ that obtains the maximum.

Write $\hat{f}=\hat{f}_n+\hat{f}^{\perp}_n$ where $\hat{f}_n\in row(\Psi)$ and $\hat{f}_n^{\perp}\in null(\Psi)$. Substituting this decomposition of $\hat{f}$ into the objective, we see that
$$
\mathcal{E}_{1}(\hat{f})=\mathcal{E}_{1}(\hat{f}_n)-\lambda \|\hat{f}_n^{\perp}\|^2_{\mathcal{H}}
$$
Therefore
$$
\mathcal{E}_{1}(\hat{f})\leq \mathcal{E}_{1}(\hat{f}_n)
$$
Since $\hat{f}$ is the unique maximizer, $\hat{f}=\hat{f}_n$.
\end{proof}

\subsection{Proof of Proposition~\ref{prop:closed1}}

\begin{proof}
Write the objective as
\begin{align}
    \mathcal{E}_1(f)&= \frac{1}{n}\sum_{i=1}^n \langle Mf,\phi(x_i) \rangle -\langle a,\phi(x_i)\rangle \langle f,\phi(x_i)\rangle -\langle f,\phi(x_i)\rangle^2-\lambda \langle f,f \rangle   \\
    &= f' M' \hat{\mu} -f'\hat{T} a- f' \hat{T} f-\lambda f' f
\end{align}
where $\hat{\mu}:=\frac{1}{n}\sum_{i=1}^n \phi(x_i)$ and $\hat{T}:=\frac{1}{n}\sum_{i=1}^n \phi(x_i)\otimes \phi(x_i)$. Appealing to the representer theorem
\begin{align}
    \mathcal{E}_1(\gamma)&= \gamma'\Psi M' \hat{\mu} -\gamma'\Psi\hat{T} a- \gamma'\Psi \hat{T} \Psi'\gamma-\lambda \gamma'\Psi \Psi'\gamma \\
    &= \gamma'\Psi M' \hat{\mu} -\frac{1}{n}\gamma'\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a- \frac{1}{n}\gamma'\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}\gamma-\lambda \gamma'K\gamma
\end{align}
The FOC yields
$$
\Psi M' \hat{\mu} -\frac{1}{n}\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a- \frac{2}{n}\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}\hat{\gamma}-2\lambda K\hat{\gamma}=0
$$
Hence
\begin{align}
    \hat{\gamma}&=\frac{1}{2}\left[\frac{1}{n}\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}+\lambda K\right]^{-1}\left[\Psi M' \hat{\mu} -\frac{1}{n}\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right] \\
    &=
\frac{1}{2}\left[\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}+n\lambda K\right]^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right]
\end{align}

\end{proof}

\subsection{Proof of Proposition~\ref{prop:rep2}}

\begin{proof}
Observe that
\begin{align}
    \hat{f}(x)&=\langle \hat{f} ,\phi(x) \rangle=\phi(x)'\Psi' \hat{\gamma}=\frac{1}{2}\phi(x)'\Psi'\Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right] \\
    m(x;\hat{f})&=\langle M\hat{f},\phi(x) \rangle =\frac{1}{2} \phi(x)'M\Psi' \Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right] \\
   \|\hat{f}\|_{\mathcal{H}}^2 &=\hat{\gamma}'\Psi \Psi'\hat{\gamma}
   = \frac{1}{4}  \Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right]'
   \Delta^{-1}
   K  
   \Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right]
\end{align}

Write the objective as 
\begin{align}
    \mathcal{E}_{2}(a)&=\frac{1}{n}\sum_{i=1}^n m(x_i;\hat{f})-\langle a,\phi(x_i)\rangle \hat{f}(x_i)- \hat{f}(x_i)^2-\lambda\|\hat{f}\|^2_{\mathcal{H}}+\mu\|a\|^2_{\mathcal{H}}
\end{align}
where the various terms involving $\hat{f}$ \textit{only} depend on $a$ in the form $\Phi a$. Due to the ridge penalty, the stated objective is coercive and strongly convex w.r.t $a$. Hence it has a unique maximizer $\hat{a}$ that obtains the maximum.

Write $\hat{a}=\hat{a}_n+\hat{a}^{\perp}_n$ where $\hat{a}_n\in row(\Phi)$ and $\hat{a}_n^{\perp}\in null(\Phi)$. Substituting this decomposition of $\hat{a}$ into the objective, we see that
$$
\mathcal{E}_{2}(\hat{a})=\mathcal{E}_{2}(\hat{a}_n)+\mu \|\hat{a}_n^{\perp}\|^2_{\mathcal{H}}
$$
Therefore
$$
\mathcal{E}_{2}(\hat{a})\geq \mathcal{E}_{2}(\hat{a}_n)
$$
Since $\hat{a}$ is the unique minimizer, $\hat{a}=\hat{a}_n$.
\end{proof}

\subsection{Proof of Proposition~\ref{prop:closed2}}

\begin{proof}
Write the objective as 
\begin{align}
    \mathcal{E}_2(a)&=\hat{f}' M' \hat{\mu} -\hat{f}'\hat{T} a- \hat{f}' \hat{T} \hat{f}-\lambda \hat{f}' \hat{f}+\mu a'a \\
    %
    \mathcal{E}_2(\beta)
    %
    &=\hat{\gamma}'\Psi M'\hat{\mu}-\hat{\gamma}'\Psi\hat{T}\Phi'\beta- \hat{\gamma}'\Psi\hat{T}\Psi'\hat{\gamma}-\lambda \hat{\gamma}'\Psi\Psi'\hat{\gamma}+\mu \beta'\Phi\Phi'\beta \\
    %
    &=\hat{\gamma}'\Psi M'\hat{\mu}
    -\frac{1}{n}\hat{\gamma}'\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta
    - \frac{1}{n}\hat{\gamma}'\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}\hat{\gamma}
    -\lambda \hat{\gamma}'K\hat{\gamma}
    +\mu \beta'K^{(1)}\beta  \\
    &=\sum_{j=1}^5 E_j
\end{align}
where 
\begin{align}
    E_1&=\hat{\gamma}'\Psi M'\hat{\mu} \\
    E_2&= -\frac{1}{n}\hat{\gamma}'\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta \\
    E_3&=- \frac{1}{n}\hat{\gamma}'\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}\hat{\gamma} \\
    E_4&= -\lambda \hat{\gamma}'K\hat{\gamma} \\
    E_5&=\mu \beta'K^{(1)}\beta
\end{align}

Recall that
$$
\hat{\gamma}
=\frac{1}{2}\Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right]
=\frac{1}{2}\Delta^{-1}
\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]
$$
Hence
$$
\hat{\gamma}'=\frac{1}{2} \left[n \hat{\mu}' M \Psi' -\beta'\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}' \right] \Delta^{-1}
$$
We analyze each term
\begin{enumerate}
    \item $E_1$
    
    $$
    E_1=\frac{1}{2} \left[n \hat{\mu}' M \Psi' -\beta'\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'\right] \Delta^{-1}\Psi M'\hat{\mu}
    $$
    Hence
    $$
    \frac{\partial E_1}{\partial \beta}=-\frac{1}{2} \begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'\Delta^{-1}\Psi M'\hat{\mu}
    $$
    
    \item $E_2$
    
   \begin{align}
        E_2&=\frac{1}{2n} \left[\beta' \begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'-n \hat{\mu}' M \Psi'\right] \Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta \\
        &= \frac{1}{2n} \beta'\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'\Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta-\frac{1}{2} \hat{\mu}' M \Psi'\Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta
   \end{align}
 Hence
  $$
    \frac{\partial E_2}{\partial \beta}=\frac{1}{n} \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'\Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta-\frac{1}{2}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'\Delta^{-1} \Psi M'\hat{\mu}
    $$
    \item $E_3$
    
    \begin{align}
        E_3&=-\frac{1}{4n} 
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]'
        \Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        \Delta^{-1}
\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]
    \end{align}
Note that
$$
\frac{\partial }{\partial s} [x-As]'W[x-As]=-2A'W(x-As)
$$
Therefore
$$
\frac{\partial E_3}{\partial \beta}=\frac{1}{2n}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        \Delta^{-1}
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]
$$
    
    \item $E_4$
    
     \begin{align}
        E_4&=-\frac{\lambda}{4} 
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]'
        \Delta^{-1}
        K
        \Delta^{-1}
\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]
    \end{align}
Note that
$$
\frac{\partial }{\partial s} [x-As]'W[x-As]=-2A'W(x-As)
$$
Therefore
$$
\frac{\partial E_4}{\partial \beta}=\frac{\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'   
\Delta^{-1}
       K
        \Delta^{-1}
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right]
$$
    
    \item  $E_5$
    
    $$
    \frac{\partial E_1}{\partial \beta_1}=2\mu\cdot K^{(1)}\beta
    $$
    
\end{enumerate}

Collecting these results gives the FOC
\begin{align}
    0&= -\frac{1}{2} \begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'\Delta^{-1}\Psi M'\hat{\mu} \\
    %
    &+\frac{1}{n} \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'\Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta-\frac{1}{2}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'\Delta^{-1} \Psi M'\hat{\mu} \\
    %
    &+\frac{1}{2n}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        \Delta^{-1}
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right] \\
        %
        &+\frac{\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'   
\Delta^{-1}
       K
        \Delta^{-1}
        \left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta\right] \\
        %
        &+2\mu\cdot K_{XX}\hat{\beta}
\end{align}

Grouping terms
\begin{align}
    &\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'\Delta^{-1}\Psi M'\hat{\mu} \\
    &-\frac{1}{2n}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        \Delta^{-1}
        n\Psi M' \hat{\mu}  \\
    &-\frac{\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'   
\Delta^{-1}
       K
        \Delta^{-1}
        n\Psi M' \hat{\mu} \\
    &= 
    \frac{1}{n} \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'\Delta^{-1}\begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}\beta
    \\
    &-\frac{1}{2n}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        \Delta^{-1}
        \begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta  \\
    &-\frac{\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'   
\Delta^{-1}
       K
        \Delta^{-1}
        \begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix} \beta \\
    &+2\mu\cdot K^{(1)}\beta
\end{align}

Define
$$
\Omega:=\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'
%
-\frac{1}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        %
        -\frac{n\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}' \Delta^{-1}K 
$$
We simplify each side of the equation
\begin{enumerate}
    \item LHS
 \begin{align}
&\left\{
\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'
%
-\frac{1}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        %
        -\frac{n\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}' \Delta^{-1}K 
        \right\}\Delta^{-1}\Psi M'\hat{\mu}\\
        &=\Omega\Delta^{-1}\Psi M'\hat{\mu}
 \end{align}
    \item RHS 
 \begin{align}
&\bigg\{
       \left(\frac{1}{n} \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}'
       -\frac{1}{2n}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}-\frac{\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'   
\Delta^{-1}
       K\right) \Delta^{-1}
        \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix} \\
    &\quad +2\mu\cdot K_{XX}\bigg\}\hat{\beta}\\
    &=\bigg\{
       \frac{1}{n}\Omega  \Delta^{-1}
        \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}
    +2\mu\cdot K^{(1)}\bigg\}\hat{\beta}
 \end{align}
\end{enumerate}
\end{proof}

\subsection{Proof of Corollary~\ref{cor:RKHS}}

\begin{proof}
\begin{align}
    \hat{a}(x)&=\langle \hat{a}, \phi(x)\rangle  \\
    &=\phi(x)'\Phi'\hat{\beta} \\
    &=K_{xX}\bigg\{
       \frac{1}{n}\Omega  \Delta^{-1}
        \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}
    +2\mu\cdot K^{(1)}\bigg\}^{-1}\Omega\Delta^{-1}\Psi M'\hat{\mu}
\end{align}

What remains is an account of how to evaluate $V:=\Psi M' \hat{\mu}\in\mathbb{R}^{2n}$.

There are two cases
\begin{enumerate}
    \item $j\in [n]$
    
    Observe that the $j$-th element of $V$ is
$$
v_j=\phi(x_j)' M' \hat{\mu}=\frac{1}{n}\sum_{i=1}^n\phi(x_j)' M' \phi(x_i)
$$
Moreover
$$
\phi(x_j)' M' \phi(x_i)= \langle \phi(x_j) ,M^*\phi(x_i) \rangle=[K^{(2)}]_{ji}
$$
Therefore 
$$
v_j=\frac{1}{n}\sum_{i=1}^n   [K^{(2)}]_{ji}
$$
    
    \item $j\in\{n+1,...,2n\}$
    
     Observe that the $j$-th element of $V$ is
$$
v_j=\phi(x_j)' M M' \hat{\mu}=\frac{1}{n}\sum_{i=1}^n\phi(x_j)'M M' \phi(x_i)
$$
Moreover
$$
\phi(x_j)' M M' \phi(x_i)= \langle M^*\phi(x_j) ,M^*\phi(x_i) \rangle= [K^{(4)}]_{ji}
$$
Therefore 
$$
v_j=\frac{1}{n}\sum_{i=1}^n [K^{(4)}]_{ji}
$$
\end{enumerate}
\end{proof}