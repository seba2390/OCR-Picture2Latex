
\section{Debiasing Average Moment}\label{sec:debiasing}

Suppose our goal is to estimate $\theta_0 = \theta(g_0)$, where $g_0=\E[Y\mid X]$. We have access to an estimate $\hat{g}$ of $g_0$. We consider the de-biased moment:
\begin{equation}
    m_{a}(Z; g) = m(Z; g) + a(X)'\, (Y - g(X))
\end{equation}
For simplicity of exposition, we present the remainder of the section for the case of a single-valued regression function.

\subsection{Asymptotic Normality with Sample Splitting}

Consider the following cross-fitted estimate:
\begin{itemize}
    \item Partition $n$ samples into $K$ folds $P_1, \ldots, P_K$
    \item For each partition, estimate $\hat{a}_k, \hat{g}_k$ based on all out-of-fold data.
    \item Construct estimate:
    \begin{equation}
        \hat{\theta} = \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} m_{\hat{a}_k}(Z_i; \hat{g}_k)
    \end{equation}
\end{itemize}

\begin{lemma}\label{lem:debias}
Suppose that $K=\Theta(1)$ and that:
\begin{align}\label{eqn:main-cond}
    \forall k \in [K]: \sqrt{n}\, \E[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] \rightarrow_p 0
\end{align}
and that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that for all $k\in [K]$: $\|\hat{a}_k-a_*\|_2 \stackrel{L^2}{\to} 0$ and $\|\hat{g}_k-g_*\|_2\stackrel{L^2}{\to} 0$. Assume that Condition~\ref{ass:strong-smooth} is satisfied and the variables $Y, g(X), a(X)$ are bounded a.s. for all $g\in \mcG$ and $a\in \mcA$.\footnote{This condition can be relaxed to simply assuming bounded fourth moments of $Y, g(X), a(X)$, as long as we strengthen the requirement to assume $4$-th moment convergence to $a_*, g_*$, i.e. that $\|\hat{a}_k-a_*\|_4, \|\hat{g}_k-g_*\|_4 \to_p 0$.} Then if we let $\sigma_*^2 := \Var(m_{a_*}(Z; g_*))$
\begin{equation}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) \to_d N\left(0, \sigma_*^2\right)
\end{equation}
\end{lemma}

A sufficient condition for Condition~\ref{eqn:main-cond} is that $\sqrt{n} \|\hat{a}-a_0\|_2 \|\hat{g} - g_0\|_2 \to_p 0$, which is a condition on the product of the two RMSE rates. However, observe that Condition~\eqref{eqn:main-cond} is much weaker as it implies that our Riesz estimate $\hat{a}$ only needs to approximately satisfy the representer moment for test functions of the form: $\hat{g}-g_0$. Thus, if we assume that $\hat{g}$ satisfies an RMSE consistency rate that $\|\hat{g}-g_0\|_2\leq r_n$, then it suffices that it satisfies the moment for any $g\in {\cal G}$, with $\|g-\hat{g}\|_2\leq r_n$, i.e. it suffices that it is a local Riesz representer around $\hat{g}$. This can potentially make the Riesz estimation task much simpler than estimating a global Riesz representer. We formalize this observation in Appendix~\ref{sec:local_RR}.

Moreover, observe that the theorem does not require consistency of both nuisance functions. Only one of the two nuisance functions needs to be consistent, while the other must simply converge to some limit function. For instance, as long as $\sqrt{n} \|\hat{a}-a_0\|_2\to 0$ or $\sqrt{n}\|\hat{g}-g_0\|_2 \to 0$, then the result holds. Inconsistency will only impact the limit variance, which will not be equal to the efficient variance; nonetheless, confidence intervals will be asymptotically valid. The required rate for the latter scenario is implausible as it asks for faster than root-$n$ rate for either $a$ or $g$. However, we can still show that the de-biased moment satisfies a double robustness property: if one nuisance is inconsistent, as long as the other is root-$n$ consistent, then asymptotic normality of the causal parameter still holds. This result is presented in \Cref{sec:inconsistent}. The result is analogous to the one provided in \cite{Benkeser2017}, where an estimator with such a property was presented within the targeted maximum likelihood framework.


\subsection{Asymptotic Normality without Sample Splitting}

Consider the algorithm where no cross-fitting or sample splitting is employed:
\begin{itemize}
    \item Estimate $\hat{a}, \hat{g}$ on all the samples
    \item Construct estimate:
    \begin{equation}
        \hat{\theta} = \E_n\left[m_{\hat{a}}(Z; \hat{g})\right]
    \end{equation}
\end{itemize}

\begin{lemma}[Normality via Localized Complexities]\label{lem:debias-nocross}
Suppose that:
\begin{align}\label{eqn:main-cond-reuse}
    \forall k \in [K]: \sqrt{n}\, \E[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] \rightarrow_p 0
\end{align}
and that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that: $\|\hat{a}_k-a_*\|_2, \|\hat{g}_k-g_*\|_2 = o_p(r_n)$. Assume that Condition~\ref{ass:strong-smooth} is satisfied and the variables $Y, g(X), a(X)$ are bounded a.s. for all $g\in \mcG$ and $a\in \mcA$. Moreover, assume that with high probability $\|\hat{g}\|_{\mcG} \leq B_1$ and $\|\hat{a}\|_{\mcA} \leq B_2$. Let $\delta_{n,*} = \delta_{n} + c_0 \sqrt{\frac{\log(c_1\, n)}{n}}$ for some appropriately defined universal constants $c_0, c_1$, where $\delta_n$ is a bound on the critical radius of $\mcG_{B_1}$, $m\circ \mcG_{B_1}$ and $\mcA_{B_2}$ and also at least $\sqrt{\frac{\log\log(n)}{n}}$. If \begin{equation}
\sqrt{n}\left(\delta_{n,*}\, r_n + \delta_{n,*}^2\right)\to 0
\end{equation}
then if we let $\sigma_*^2 := \Var(m_{a_*}(Z; g_*))$
\begin{equation}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) \to_d N\left(0, \sigma_*^2\right)
\end{equation}
\end{lemma}


Suppose we use for both $\hat{a}$ and $\hat{g}$ an $\ell_1$ constrained linear function class in $p$ dimensions and that $a_*, g_*$ are sparse linear functions with support size $s$. Moreover if $B_1=\|a_*\|_1 + o(1)$ and $B_2=\|g_*\|_1 + o(1)$, and the covariates satisfy a restricted eigenvalue condition, then we could show that $\delta_{n, *}=O\left(\sqrt{\frac{s\log(p)}{n}}\right)$ (a simplification by assuming $s\log(p)>\log(n)$). Then as long as $r_n \to 0$, the condition is satisfied. Moreover, for such function classes, we will typically have that $r_n = O\left(\sqrt{\frac{s\log(p)}{n}}\right)$. Therefore, the required condition is that: $\frac{s\log(p)}{\sqrt{n}}=o(1)$ or equivalently $s=o\left(\sqrt{n}/\log(p)\right)$. 

Of theoretical interest, it seems that without sample splitting, the analysis essentially goes through for general function classes that are not Donsker. With sample splitting, we would require from Condition~\eqref{eqn:main-cond-reuse} that $\frac{\sqrt{s_{a} s_{g}} \log(p)}{n} = o(n^{-1/2})$, where $s_a, s_g$ are the sparsity bounds on $a$ and $g$, respectively. Simplifying, with sample splitting we require $\sqrt{s_a\, s_g} = o\left(\sqrt{n}/\log(p)\right)$. By contrast, without sample splitting, we require this condition for both $s_a$ and $s_g$. Beyond this difference, the conditions on the sparsity of the function classes seem comparable.

We also provide a proof of asymptotic normality without sample splitting for uniformly stable estimators. This proof technique handles cases beyond Donsker classes or classes with small critical radius, since stability is not only a property of the function class but also of the estimation algorithm. Thus, it could be potentially apply to large neural net classes trained via few iterations of stochastic gradient descent \cite{Hardt2016} or sub-bagged ensembles of overfitting estimators \cite{elisseeff2003leave}.


\begin{lemma}[Normality via Uniform Stability]\label{lem:debias-nocross-stability}
Suppose that:
\begin{align}\label{eqn:main-cond-reuse}
    \forall k \in [K]: \sqrt{n}\, \E[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] \rightarrow_p 0
\end{align}
and that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that:
\begin{equation}
\E\left[\|\hat{a}_k-a_*\|_2^2\right], \E\left[\|\hat{g}_k-g_*\|_2^2\right] = O(r_n^2)    
\end{equation}
Assume that Condition~\ref{ass:strong-smooth} is satisfied and the variables $Y, g(X), a(X)$ are bounded a.s. for all $g\in \mcG$ and $a\in \mcA$. Suppose that the algorithm for estimating $\hat{h}:=(\hat{a}, \hat{g})$ is symmetric across samples and satisfies $\beta_n$-mean-squared stability, i.e.:\footnote{The notion was originally defined in \cite{Kale11cross-validationand} and used to derive imporved bounds on $k$-fold cross-validation. It is weaker than the well-studied uniform stability \cite{Bousquet2002StabilityAG}. See \cite{elisseeff2003leave,Celisse2016,pmlr-v98-abou-moustafa19a} for more discussion.}
\begin{equation}
    \E_Z\left[\left\|\hat{h}(Z) - \hat{h}^{-i}(Z)\right\|_{\infty}^2\right] \leq \beta_n
\end{equation}
where $\hat{h}^{-i}$ is the function that the estimation algorithm would produce if sample $i$ was removed from the training set. If
\begin{equation}
r_{n-1}^2 + n\,\beta_{n-1} r_{n-2}\to 0
\end{equation}
then if we let $\sigma_*^2 := \Var(m_{a_*}(Z; g_*))$
\begin{equation}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) \to_d N\left(0, \sigma_*^2\right)
\end{equation}
\end{lemma}


\paragraph{Uniform stability of sub-bagged ensemble estimators.} If we use sub-bagging and return as an estimate the average of a base estimator over subsamples of size $s<n$, then the sub-bagged estimate is $\beta_n:=\frac{s}{n}$-uniformly stable (see e.g. \cite{elisseeff2003leave}). If the bias of the base estimator decays as some function $\textsc{bias}(s)$, then typically sub-bagged estimators will achieve $r_n = \sqrt{\frac{s}{n}} + \textsc{bias}(s)$ (see e.g. \cite{Athey2016,Khosravi2019,Syrgkanis2020}). Thus we need that $n\beta_n r_n = \sqrt{\frac{s^3}{n}} + s\, \textsc{bias}(s) \to 0$. As long as $s=o(n^{1/3})$ and $\textsc{bias}(s)=o(1/s)$, then the conditions of the latter theorem hold. The recent work of \cite{Syrgkanis2020} shows that in a high-dimensional regression setting, with $p\gg n$ and only $r\ll p,n$ of the variables being $\mu$-\emph{strictly} relevant variables, i.e. leading to a decrease in explained variance of at least $\mu$, (for some constant $\mu>0$), the bias of a deep Breiman tree trained on $s$ data points decays as $\exp(-s)$. Moreover, a deep Breiman forest where each tree is trained on $s=O\left(\frac{2^r \log(p)}{\mu}\right)=o(n^{1/3})$ samples, drawn without replacement, will achieve $r_n = O\left(\sqrt{\frac{s 2^r}{n}}\right)$. Thus sub-bagged deep Breiman random forests satisfy the conditions of the theorem in the case of sparse high-dimensional non-parametric regression.

