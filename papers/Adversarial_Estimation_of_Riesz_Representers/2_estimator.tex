\section{Adversarial Estimator}

For any function space $\mcG$, let $\text{star}(\mcG):=\{r\, g: g\in \mcG, r \in [0, 1]\}$, denote the star hull. Let $\partial \mcG:= \{g-g': g, g'\in \mcG\}$ denote the space of differences. We will consider estimators that estimate Riesz representers within some function space $\mcA$, equipped with some norm $\|\cdot\|_{\mcA}$. Moreover, let $\ldot{\cdot}{\cdot}_2$ be the inner product associated with the $\ell_2$ norm, i.e. $\ldot{a}{a'}_2 := \E_{X}[a(X)\, a'(X)]$.\footnote{In Appendix~\ref{sec:restricted}, we examine the relationship between $\mcG$ and the Riesz representer space $\mcA$.} Given this notation, we define the class:
\begin{equation}
\mcF:=\text{star}(\partial\mcA):=\{r\, (a-a'): a, a'\in \mcA, r\in [0,1]\}    
\end{equation}
and assume that the norm $\|\cdot\|_{\mcA}$ extends naturally to the larger space $\mcF$. Moreover, let $\E_n[\cdot]$ denote the empirical average and $\|\cdot\|_{2,n}$ the empirical $\ell_2$ norm, i.e. 
$$\|g\|_{2,n} :=\sqrt{\E_n[g(X)^2]}.$$
Consider the following adversarial estimator:
\begin{equation}\label{eqn:reg-estimator}
    \hat{a} = \argmin_{a\in \mcA} \max_{f\in \mcF} \E_n[m(Z; f) - a(X)\cdot f(X)] - \|f\|_{2,n}^2 - \lambda \|f\|_{\mcA}^2 + \mu \|a\|_{\mcA}^2
\end{equation}

\begin{remark}[Population limit]
Consider the population limit of our criterion where $n\to \infty$ and $\lambda,\mu \to 0$. Then our criterion is:
\begin{equation}
    \max_{f\in \mcF} \E[m(Z; f) - a(X)\cdot f(X)] - \|f\|_{2}^2
\end{equation}
By the definition of the Riesz representer we thus have:
\begin{align}
    \max_{f\in \mcF} \E\left[m(Z; f) - a(X)\cdot f(X)\right] - \|f\|_{2}^2 =~& \max_{f\in \mcF} \E\left[(a_0(X) - a(X))\cdot f(X) -  f(X)^2\right]\\
    =~& \frac{1}{4}\E\left[(a_0(X) - a(X))^2\right] =: \frac{1}{4} \|\hat{a}-a_0\|_2^2
\end{align}
Thus our empirical criterion converges to the mean-squared-error criterion in the population limit, even though we don't have access to unbiased samples from $a_0(X)$.
\end{remark}

\begin{remark}[Norm-Based Regularization] 
The extra vanishing norm-based regularization can be avoided if one knows a bound on the norm of the true $a_0$. In that case, one can impose a hard norm constraint on the hypothesis space $\mcA$ and $\bar{\mcA}$ and optimize over these norm-constrained sub-spaces. However, regularization allows the estimator to be adaptive to the true norm of $a_0$, without knowledge of it.
\end{remark}

\begin{remark}[Mis-specification]
We in fact allow for $a_0\notin \mcA$, and incur an extra bias part in our estimation error of the form of: $\min_{a\in \mcA} \|a-a_0\|_{2}$. Thus $\mcA$ need only be an $\ell_2$-norm approximating sequence of function spaces.
\end{remark}