\section{Computation}\label{sec:computation}

In this section we discuss computational aspects of the optimization problem implied by our adversarial estimator. We show how in many cases, the min-max optimization problem can be solved computationally efficiently and also discuss practical heuristics for cases where the problem is non-convex (e.g. in the case of neural networks).

\subsection{Sparse Linear Function Spaces}

For the case of sparse linear functions, the estimator in \Cref{thm:reg-main-error-2} requires solving the following optimization problem:
\begin{equation}\label{eqn:minimax-ell1}
    \min_{\theta\in \R^p: \|\theta\|_1\leq B}\,\, \max_{i\in [2p]} \E_n\left[m(Z; f_i) - f_i(X)\, \ldot{\theta}{X} \right] + \lambda \|\theta\|_1
\end{equation}
where $f_i(X) = X_i$ for $i\in \{1, \ldots, p\}$ and $f_i(X)=-X_i$ for $i\in \{p+1, \ldots, 2p\}$.
This can be solved via sub-gradient descent, which would yield an $\epsilon$-approximate solution after $O\left(p/\epsilon^2\right)$ steps. This can be improved to $O\left(\log(p)/\epsilon\right)$ steps if one views it as a zero-sum game and uses simultaneous gradient descent, where the $\theta$-player uses Optimistic-Follow-the-Regularized-Leader with an entropic regularizer and the $f$-player uses Optimistic Hedge over probability distributions on the finite set of test functions (analogous to Proposition~13 of \cite{dikkala2020minimax}).

To present the algorithm it will be convenient to re-write the problem where the maximizing player optimizes over distributions in the $2p$-dimensional simplex, i.e.:
\begin{equation}
    \min_{\theta\in \R^p: \|\theta\|_1\leq B}\,\, \max_{w\in \R_{\geq 0}^{2p}:\|w\|_1=1} \E_n\left[m(Z; \ldot{w}{f}) - \ldot{w}{f}(X)\, \ldot{\theta}{X} \right] + \lambda \|\theta\|_1
\end{equation}
 where $f=(f_1, \ldots, f_{2p})$, denote the vector of the $2p$ functions. Moreover, to avoid the non-smoothness of the $\ell_1$ penalty it will be convenient to introduce the augmented vector $V=(X; -X)$ and for the minimizing player to optimize over the positive orthant of a $2p$-dimensional vector $\rho=(\rho^+; \rho^-)$, with an $\ell_1$ bounded norm, such that in the end: $\theta=\rho^+ - \rho^-$. Then we can re-write the problem as:
\begin{equation}
    \min_{\rho\in \R_{\geq 0}^{2p}: \|\rho\|_1\leq B}\,\, \max_{w\in \R_{\geq 0}^{2p}:\|w\|_1=1} \E_n\left[m(Z; \ldot{w}{f}) - \ldot{w}{V}\, \ldot{\rho}{V} \right] + \lambda \sum_{i=1}^{2p} \rho_i
\end{equation}
where we also noted that $\ldot{w}{f}(X) = \ldot{w}{V}$.
\begin{proposition}\label{prop:sparse-optimization-ell1}
Consider the algorithm that for $t=1, \ldots, T$, sets:
\begin{align}
    \tilde{\rho}_{i, t+1} =~& \tilde{\rho}_{i, t} e^{- 2\frac{\eta}{B}\, \left(- \E_n[V_i\, \ldot{V}{w_t}] + \lambda\right) + \frac{\eta}{B}\, \left(- \E_n[V_i\, \ldot{V}{w_{t-1}}] + \lambda\right)} &
    \rho_{t+1} =~& \tilde{\rho}_{t+1}\, \min\left\{1, \frac{B}{\|\tilde{\rho}_{t+1}\|_1}\right\}\\
    \tilde{w}_{i, t+1} =~& w_{i, t} e^{2\, \eta\, \E_n[m(Z; f_i) - V_i \ldot{V}{\rho_t}] - \eta\, \E_n[m(Z; f_i) - V_i \ldot{V}{\rho_{t-1}}]} & w_{t+1} =~& \frac{\tilde{w}_{t+1}}{\|\tilde{w}_{t+1}\|_1}
\end{align}
with $\tilde{\rho}_{i,-1}=\tilde{\rho}_{i,0}=1/e$ and $\tilde{w}_{i,-1}=\tilde{w}_{i,0}=1/(2p)$ and returns $\bar{\rho}=\frac{1}{T} \sum_{t=1}^T \rho_t$. Then for $\eta=\frac{1}{4\|\E_n[VV^\top]\|_{\infty}}$,\footnote{For a matrix $A$, we denote with $\|A\|_{\infty}=\max_{i, j} |A_{ij}|$} after
\begin{equation}
T=16\|\E_n[VV^\top]\|_{\infty} \frac{4B^2 \log(B\vee 1) + (B+1) \log(2p)}{\epsilon}
\end{equation}
iterations, the parameter $\bar{\theta}=\bar{\rho}^{+} - \bar{\rho}^-$ is an $\epsilon$-approximate solution to the minimax problem in Equation~\eqref{eqn:minimax-ell1}.
\end{proposition}


\subsection{Neural Nets with Simultaneous Stochastic Gradient Descent}

When the function space $\mcA$ and $\mcF$ is represented as a deep neural network then the optimization problem is highly non-convex. This is the case even if we were just solving a square loss minimization problem. On top of this we also need to deal with the non-convexity and non-smoothness introduced by the min-max structure of our estimator. 

Luckily, the optimization problem that we are facing is similar to the optimization problem that is encountered in training Generative Adversarial Networks, i.e. we need to solve a non-convex, non-concave zero-sum game, where the strategy of each of the two players are the parameters of a neural net. Luckily, there has been a surge of recent work proposing iterative optimization algorithms inspired by the convex-concave zero-sum game theory (see, e.g. the Optimistic Adam algorithm of \cite{Daskalakis2017}, also utilized in the recent work of \cite{bennett2019deep,dikkala2020minimax} in the context of solving moment equations, or the work of \cite{Hsieh2019,Mishchenko2019} on the extra-gradient or stochastic extra-gradient algorithm). All these new algorithms for solving differentiable non-convex/non-concave zero-sum games can be deployed for our problem.

Recent work of \cite{liao2020provably} contributes to a literature on over-parameterized neural network training for square losses \cite{AllenZhu2018,du2018gradient,Soltanolkotabi2019}. The authors show that even for min-max losses that are very similar to the loss of our estimator, neural nets that are sufficiently wide and appropriately randomly initialized essentially behave like linear functions in an appropriate reproducing kernel Hilbert space, typically referred to as the neural tangent kernel space. Given this intuition, the authors show that a simple simultaneous gradient descent/ascent algorithm and subsequent averaging of the parameters converges to the solution of the min-max problem. In this regime neural networks behave like linear functions, so one can invoke analysis similar to the analysis we invoke for sparse linear function spaces, and then carefully account for the approximation error. The intuition and results of the work of \cite{liao2020provably} can be appropriately adapted for our loss function too so as to show that the average path of the simultaneous gradient descent/ascent algorithm also converges in our setting. One caveat is that growing the width of the neural net to facilitate optimization deteriorates the statistical guarantee, since the critical radius grows as a function of the width.

\subsection{Reproducing Kernel Hilbert Space}\label{sec:RKHS}


%

Recall the estimator is
$$
\hat{a} = \argmin_{a\in \mcA} \max_{f\in \mcF} \E_n[m(Z; f) - a(X)\cdot f(X)] - \|f\|_{2,n}^2 - \lambda \|f\|_{\mcA}^2 + \mu \|a\|_{\mcA}^2
$$
In this section, we derive a closed form solution for $\hat{a}$ that can be computed from matrix operations.

Towards this end, we impose additional structure on the problem. If $\mathcal{G}=\mathcal{F}=\mathcal{H}$ is a reproducing kernel Hilbert space (RKHS), then the projection $a_0^{\min}$ of any RR $a_0$ into $\mathcal{G}$ is clearly an element of $\mathcal{H}$ as well, so we can take $\mathcal{A}=\mathcal{H}$. Also assume that the functional $m$ satisfies $m(z;f)=m(x;f)$. Moreover, let the functional be such that it evaluates the function in some arguments. For example, in ATE, $m(z;f)=f(1,w)-f(0,w)$ where $z=x=(d,w)$. This property holds for treatment effects and policy effects, and it ensures that $m(\cdot;f)\in\mathcal{H}$.

Denote the kernel $k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$, and denote the feature map $\phi:x\mapsto k(x,\cdot)$. Denote the kernel matrix $K_{XX}$ with $(i,j)$-th entry $k(x_i,x_j)$. Denote the feature matrix $\Phi$ with $i$-th row $\phi(x_i)'$. Hence $K_{XX}=\Phi\Phi'$.

By the reproducing property
$
f(x)=\langle f,\phi(x) \rangle_{\mathcal{H}}
$.  Moreover, since $m$ is a linear functional, we can define the linear operator
$
M:\mathcal{H}\rightarrow \mathcal{H},\; f(\cdot)\mapsto m(\cdot;f)
$
whereby
$$
m(x;f)=[Mf](x)=\langle Mf,\phi(x) \rangle_{\mathcal{H}} = \langle f, M^*\phi(x) \rangle_{\mathcal{H}}
$$
where $M^*$ is the adjoint of $M$. Define the matrix $\Phi^{(m)}:=\Phi M$ with $i$-th row $\phi(x_i)'M$. Finally define $\Psi$ as the matrix with $2n$ rows that is constructed by concatenating $\Phi$ and $\Phi^{(m)}$. We denote the induced kernel matrix by $K:=\Psi\Psi'$. Formally,
$$
\Psi:=\begin{bmatrix} \Phi\\ \Phi^{(m)}\end{bmatrix},\quad K:=\begin{bmatrix} K^{(1)} & K^{(2)} \\ K^{(3)}  & K^{(4)} \end{bmatrix}:=\begin{bmatrix} \Phi \Phi' & \Phi (\Phi^{(m)})' \\ \Phi^{(m)}\Phi' & \Phi^{(m)}  (\Phi^{(m)})'  \end{bmatrix} 
$$
Note that $\{K^{(j)}\}_{j\in[4]}\in \mathbb{R}^{n\times n}$ and hence $K\in\mathbb{R}^{2n\times 2n}$ can be computed from data, though they depend on the choice of moment. 
\begin{proposition}[Computing kernel matrices]\label{prop:kernel_matrices}
For example, for ATE
\begin{align}
[K^{(1)}]_{ij}&=k((d_i,w_i),(d_j,w_j)) \\
[K^{(2)}]_{ij}&=k((1,w_i),(d_j,w_j))-k((0,w_i),(d_j,w_j)) \\
    [K^{(3)}]_{ij}&=k((d_i,w_i),(1,w_j))-k((d_i,w_i),(0,w_j)) \\
    [K^{(4)}]_{ij} &=k((1,w_i),(1,w_j))-k((1,w_i),(0,w_j))-k((0,w_i),(1,w_j))+k((0,w_i),(0,w_j))
\end{align}
\end{proposition}

We proceed in steps. First we prove the existence of a closed form for the maximizer $\hat{f}=\argmax_{f\in\mathcal{H}} \E_n[m(X;f) - a(X)\cdot f(X)] - \|f\|_{2,n}^2 - \lambda \|f\|_{\mcH}^2$ by extending the classic representation theorem of \cite{kimeldorf1971some,scholkopf2001generalized}.

%

\begin{proposition}[Representation of maximizer]\label{prop:rep1}
$\hat{f}=\Psi'\hat{\gamma}$ for some $\hat{\gamma}\in\mathbb{R}^{2n}$
\end{proposition}

Appealing to this abstract result, we derive the closed form expression for the maximizer in terms of kernel matrices.

\begin{proposition}[Closed form of maximizer]\label{prop:closed1}
$\hat{\gamma}=\frac{1}{2}\Delta^{-1}\left[n\Psi M' \hat{\mu} -\begin{bmatrix}K^{(1)} \\ K^{(3)} \end{bmatrix}\Phi a\right]$ 
where
$$ \Delta:=\begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}+n\lambda K\in\mathbb{R}^{2n\times 2n},\quad \hat{\mu}:=\frac{1}{n}\sum_{i=1}^n \phi(x_i)$$
\end{proposition}

%

Next we prove the existence of a closed form for the minimizer $\hat{a}=\argmin_{a\in\mathcal{H}} \E_n[m(X;\hat{f}) - a(X)\cdot \hat{f}(X)] - \|\hat{f}\|_{2,n}^2 - \lambda \|\hat{f}\|_{\mcH}^2 + \mu \|a\|_{\mcH}^2$ by appealing to the classic representation theorem of \cite{kimeldorf1971some,scholkopf2001generalized}.

\begin{proposition}[Representation of minimizer]\label{prop:rep2}
$\hat{a}=\Phi'\hat{\beta}$ for some $\hat{\beta}\in\mathbb{R}^n$
\end{proposition}

Again, with this abstract result in hand, we derive the closed form expression for the minimizer in terms of kernel matrices.

\begin{proposition}[Closed form of minimizer]\label{prop:closed2}
$\hat{\beta}=\bigg\{
       \frac{1}{n}\Omega  \Delta^{-1}
        \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}
    +2\mu\cdot K^{(1)}\bigg\}^{-1}\Omega\Delta^{-1}\Psi M'\hat{\mu}$
    where
    $$
 \Omega:=\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'
%
-\frac{1}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}'    
\Delta^{-1}
       \begin{bmatrix} K^{(1)}K^{(1)} & K^{(1)} K^{(2)} \\ K^{(3)} K^{(1)} & K^{(3)}K^{(2)}\end{bmatrix}
        %
        -\frac{n\lambda}{2}\begin{bmatrix}K^{(1)}K^{(1)} \\ K^{(3)}K^{(1)} \end{bmatrix}' \Delta^{-1}K \in\mathbb{R}^{n\times 2n}
    $$
\end{proposition}

For practical use, we require a way to evaluate the minimizer using only kernel operations. Evaluation directly follows from the closed form expression.

\begin{corollary}[Evaluation of minimizer]\label{cor:RKHS}
$$
\hat{a}(x)=K_{xX}\bigg\{
       \frac{1}{n}\Omega  \Delta^{-1}
        \begin{bmatrix} K^{(1)} K^{(1)} \\ K^{(3)} K^{(1)}\end{bmatrix}
    +2\mu\cdot K^{(1)}\bigg\}^{-1}\Omega\Delta^{-1}V
$$
where $V\in\mathbb{R}^{2n}$ is defined such that
$$
v_j=
\begin{cases} 
\frac{1}{n}\sum_{i=1}^n [K^{(2)}]_{ji} &\text{ if }j\in[n] \\
\frac{1}{n}\sum_{i=1}^n  [K^{(4)}]_{ji} &\text{ if }j\in\{n+1,...,2n\}\end{cases}
$$
\end{corollary}

\subsection{Oracle Based Training}



Consider the estimator with $\lambda=\mu=0$:
\begin{equation}\label{eqn:unreg-minimax}
    \min_{a\in \mcA} \max_{f\in \mcF} \E_n\left[m(Z;f) - a(X)\cdot f(X) - f(X)^2\right] =: \ell(a, f)
\end{equation}
We can solve this optimization problem by treating it as a zero-sum game, where one player controls $a$ and the other player controls $f$. Observe that the game is convex $a$ (in fact linear) and concave in $f$. Thus, we can solve this zero-sum game by having the $f$-player run a no-regret algorithm at each period $t\in \{1,\ldots, T\}$ and the $a$-player best responding to the current choice of the $f$ player.

Observe that for any fixed $f$, the best-response of the $a$-player is the solution to:
\begin{equation}
    a_t = \argmin_{a\in \mcA} -\E_n[a(X) \cdot f(X)] = \argmax_{a\in \mcA} \E_n[a(X) \cdot f(X)]
\end{equation}
In other words, the $a$-player wants to match the sign of the function $f$. Thus the best-response of the $a$-player is equivalent to a weighted classification oracle, where the label is $Y_i = \sign(f(X_i))$ and the weight is $w_i = |f(X_i)|$.

Finally, we need to solve the no-regret problem for the $f$ player. If the function space $\mcF$ is a convex space,\footnote{i.e. if $f,f'\in \mcF$, then $\gamma f+(1-\gamma)f'\in \mcF$ for any $\gamma\in [0, 1]$} then we can simply run the follow the leader (FTL) algorithm, where at every period the algorithm maximizes the empirical past reward:
\begin{equation}
    f_t = \argmax_{f\in \mcF} \E_n\left[m(Z;f) - \bar{a}_{< t}(X)\cdot f(X) - f(X)^2\right] = \argmin_{f\in \mcF} -\ell(\bar{a}_{<t}, f)
\end{equation}
where $\bar{a}_{<t} = \frac{1}{t-1} \sum_{\tau<t} a_{\tau}$.

\begin{proposition}\label{prop:oracle}
Suppose that the empirical operator $\E_n[m(Z; \cdot)]$ is bounded with operator norm upper bounded by $M_n\geq 1$ and that the function class $\mcF$ is convex. Consider the algorithm where at each period $t\in \{1, \ldots, T\}$:
\begin{align}
    f_t =~& \argmax_{f\in \mcF} \ell(\bar{a}_{<t}, f) & a_t =~& \argmin_{a\in \mcA} \ell(a, f_t)
\end{align}
Then for $T=\Theta\left(\frac{M_n\, \log(1/\epsilon)}{\epsilon}\right)$, the function $a_*=\frac{1}{T}\sum_{t=1}^T a_t$ is an $\epsilon$-approximate solution to the empirical minimax problem in Equation~\eqref{eqn:unreg-minimax}.
\end{proposition}

The above algorithm requires a weighted classification oracle for the $a$-player and an oracle for the $f$-player that solves the problem $\max_{f\in \mcF} \ell(a, f)$, for any $a$.

\begin{example}[$f$-player oracle for ATE]
For the case of ATE this problem is:
\begin{align}
    f_* =~& \argmin_{f\in \mcF} \E_n\left[f(T, X)^2 + a(T, X) f(T, X) - f(1, X) + f(0, X)\right]
\end{align}
\end{example}
