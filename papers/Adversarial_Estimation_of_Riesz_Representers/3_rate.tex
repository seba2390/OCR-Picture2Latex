
\section{Fast Convergence Rate}\label{sec:estimation}

We now provide fast convergence rates of our regularized minimax estimator, parameterized by the critical radii of the function classes:
\begin{align}
    \mcF_B:=~&\{f\in \mcF: \|f\|_{\mcA}^2\leq B\}\\
    m\circ \mcF_B:=~&\{m(\cdot; f): f\in \mcF_B\}
\end{align}
for some appropriately defined constant $B$. The critical radius of a function class $\mcF$ with range in $[-1, 1]$ is defined as any solution $\delta_n$ to the inequality:
\begin{align}
    {\cal R}(\delta; \mcF)\leq~& \delta^2 &
    \text{with: } {\cal R}(\delta; \mcF) =~& \E\left[\sup_{f\in \mcF: \|f\|_2\leq \delta} \frac{1}{n} \sum_{i=1}^n \epsilon_i f(X_i)\right]
\end{align}
with $\epsilon_{1:n}$ are independent Rademacher random variables drawn equiprobably in $\{-1, 1\}$. For VC-subgraph function classes with constant VC dimension the critical radius is of the order of $\sqrt{\log(n)/n}$. The critical radius has been characterized by many other function classes such as reproducing kernel Hilbert spaces, neural networks and high-dimensional linear functions (c.f. \cite{wainwright2019high} and Section~\ref{sec:examples}).

We will also require the following norm-dominance condition:
\begin{assumption}[Mean-Squared Continuity]\label{ass:strong-smooth}
For some constant $M\geq 0$, the following property holds:
\begin{equation}
    \forall f\in \mcF: \sqrt{\E\left[m(Z;f)^2\right]} \leq M\, \|f\|_2 
\end{equation}
\end{assumption}
Observe that the fact that the operator $\theta(g)$ is bounded, implies that $|\E[m(Z;g)]| \leq M\, \|g\|_2$. Mean-squared continuity is a stronger condition than boundedness, since: $|\E[m(Z;g)]|\leq \E[|m(Z;g)|] \leq \sqrt{\E[m(Z;g)^2]}$. In Appendix~\ref{sec:continuity}, we verify this condition for a variety of popular functionals.

\begin{example}[Mean-Squared Continuity for ATE] Let $X=(D,W)$ consist of treatment and covariates. In the case of treatment effect estimation, the above is implied by a non-parametric overlap condition, i.e. $\Pr[D=1\mid w] \in (1/M, 1-1/M)$ for some $M\in (1, \infty)$. Then observe that:
\begin{align}
\E[(g(1, W) - g(0, W))^2]\leq~& 2\E[g(1,W)^2 + g(0, W)^2]\\
\leq~& 2M \E\left[\Pr[D=1\mid W]\, g(1,W)^2 + \Pr[D=0\mid W]\, g(0, W)^2\right] = 2M \|g\|_2^2
\end{align}
\end{example}

\begin{theorem}\label{thm:reg-main-error}
Assume that mean-squared continuity holds for some constant $M\geq 1$ and that for some $B\geq 0$, the functions in $\mcF_B$ and $m\circ \mcF_B$ have uniformly bounded ranges in $[-1, 1]$. Let:
\[
\delta:=\delta_n + \epsilon_n + c_0 \sqrt{\frac{\log(c_1/\zeta)}{n}},
\]
for universal constants $c_0, c_1$, where $\delta_n$ upper bounds the critical radii of $\mcF_{B}, m\circ \mcF_B$
and $\epsilon_n$ upper bounds the bias $\min_{a\in \mcA} \|a-a_0\|_2$. 
Let $a_*=\argmin_{a\in \mcA} \|a-a_0\|_2$. Then the estimator in Equation~\cref{eqn:reg-estimator}, with $\mu \geq 6\lambda \geq 12\delta^2/B$, satisfies w.p. $1-\zeta$:
\begin{equation}
    \|\hat{a} - a_0\|_2 \leq O\left(M^2 \delta + \frac{\mu}{\delta} \|a_*\|_{\mcA}^2\right)
\end{equation}
For $\mu\leq C \delta^2/B$, for some constant $C$, the latter is: $O\left(\delta\,\max\left\{M^2, \frac{\|a_*\|_{\mcA}^2}{B}\right\}\right)$.
\end{theorem}

\begin{remark}
Suppose we only want to approximate the Riesz representer with respect to the weaker distance metric $\|\cdot\|_{\mcF}$ defined as:\footnote{The metric $\|\cdot\|_{\mcF}$ satisfies the triangle inequality:
\begin{equation}
    \|a + b\|_{\mcF} \leq \sqrt{\sup_{f\in \mcF} \ldot{a}{f} - \frac{1}{4} \|f\|_2^2 + \sup_{f\in \mcF} \ldot{b}{f} - \frac{1}{4} \|f\|_2^2} \leq \|a\|_{\mcF} + \|b\|_{\mcF}
\end{equation}
and is positive definite i.e. $\|0\|_{\mcF}=0$, but not necessarily homogeneous, i.e. $\|\lambda a\|_{\mcF}=? |\lambda| \|a\|_{\mcF}$ for $\lambda\in \R$.}
\begin{equation}
    \|a\|_{\mcF}^2 = \sup_{f\in \mcF}\, \ldot{a}{f}_2 - \frac{1}{4}\|f\|_2^2 \leq \|a\|_2^2
\end{equation}
Then \Cref{thm:reg-main-error} can be adapted to show that: $\|\hat{a}-a_0\|_{\mcF} \leq \delta \max\left\{M^2, \|a_*\|_{\mcA}^2/B\right\}$, where now the approximation rate is $\epsilon_n = \inf_{a\in \mcA}\|a-a_0\|_{\mcF}$. Observe that $\|\cdot\|_{\mcF}$ satisfies:
\begin{equation}
    \|a\|_{\mcF}^2 = \inf_{f\in \mcF} \frac{1}{4} \|f\|_2^2 - \ldot{a}{f}_2 + \|a\|_2^2 - \|a\|_2^2 = \inf_{f\in \mcF} \left\|a - f/2\right\|_2^2 - \|a\|_2^2 \leq \inf_{f \in \mcF} \|a - f\|_2^2 - \|a\|_2^2
\end{equation}
where in the last inequality we used the fact that $\mcF$ is star-convex. Thus it is at most the projection of $a$ on $\mcF$. Hence, it is sufficient that $\mcA$ approximates $a_0$ in this weak sense that for some $a_*\in \mcA$ the projection of $a_*-a_0$ on $\mcF$ is at most $\epsilon_n$. Thus any component of $a_0$ that is orthogonal to $\mcF$ can be ignored, since if we denote with $a_0 = a_0^{\perp} + a_0^{\parallel}$ with $\sup_{f\in \mcF} \ldot{a_0^{\perp}}{f}_2=0$, then $\|a_0 - a_*\|_{\mcF} = \|a_0^{\parallel} - a_*\|_{\mcF}$.
\end{remark}

Our proof uses similar ideas as in the proof of Theorem~1 of \cite{dikkala2020minimax}, where an adversarial estimator was considered for the case of non-parametric instrumental variable regression. Theorem~1 of \cite{dikkala2020minimax} provides bounds on a weaker metric than the mean-squared-error metric and requires bounds on the critical radius of more complicated function spaces.

As a corollary of \Cref{thm:reg-main-error}, we can obtain a bound for the un-regularized estimator with $\lambda=\mu=0$, where the function classes $\mcF$ and $\mcG$ are already norm constrained, e.g. $\|f\|_{\mcA}\leq U$ for all $f\in \mcF$, which also implies that $\|a\|_{\mcA} \leq U$ for all $a\in \mcA$, such that functions in $\mcF$ and $\mcG$ have uniformly bounded range. This can be achieved by using the above norm-constrained definitions of $\mcF$ and $\mcG$ and taking the limit of \Cref{thm:reg-main-error-2} when $B\to \infty$. In that case, $\mcF_B \to \mcF$, $m\circ \mcF_B\to m\circ \mcF$ and $\lambda, \mu$ are allowed to take zero value. This leads to the corollary:

\begin{corollary}\label{cor:main-error}
Assume that mean-squared continuity holds for some constant $M\geq 1$ and that the functions in $\mcF$ and $m\circ \mcF$ have uniformly bounded ranges in $[-1, 1]$. Let:
\[
\delta:=\delta_n + \epsilon_n + c_0 \sqrt{\frac{\log(c_1/\zeta)}{n}},
\]
for universal constants $c_0, c_1$, where $\delta_n$ upper bounds the critical radii of $\mcF, m\circ \mcF$
and $\epsilon_n$ upper bounds the bias $\min_{a\in \mcA} \|a-a_0\|_2$. 
The estimator in Equation~\cref{eqn:reg-estimator}, with $\lambda=\mu=0$, satisfies:
\begin{equation}
    \|\hat{a} - a_0\|_2 \leq O\left(M^2 \delta\right)
\end{equation}
\end{corollary}

\subsection{Fast Rates without $\ell_2$-Penalty}

We will use the following notation:
\begin{equation}
\spanF_{\kappa}(\mcF) := \left\{\sum_{i=1}^p w_i f_i: f_i\in \mcF, \|w\|_1\leq \kappa, p\leq\infty\right\}
\end{equation}

\begin{theorem}\label{thm:reg-main-error-2}
Consider a set of test functions $\mcF:=\cup_{i=1}^d \mcF^i$, that is de-composable as a union of $d$ symmetric test function spaces $\mcF^i$ and suppose that $\mcA$ is star-convex. Consider the adversarial estimator:
\begin{equation}\label{eqn:reg-estimator-2}
    \hat{a} = 
    \argmin_{a\in \mcA} \,\,\,\,\sup_{f\in \mcF}\,\, \E_n[m(Z; f) - a(X)\cdot f(X)] + \lambda \|a\|_{\mcA}
\end{equation}
Let $m\circ \mcF^i = \{m(\cdot; f): f\in \mcF^i\}$ and
\[
\delta_{n,\zeta}:=2\max_{i=1}^d \left(\mcR(\mcF^i) + \mcR(m\circ \mcF^i)\right) + c_0 \sqrt{\frac{\log(c_1\, d/\zeta)}{n}},
\]
for some universal constants $c_0, c_1$ and $B_{n,\lambda,\zeta}:= \left(\|a_0\|_{\mcA} + \delta_{n,\zeta}/\lambda\right)^2$. Suppose that $\lambda\geq \delta_{n,\zeta}$ and:
\begin{equation}\label{cond:normalized-span}
\textstyle{\forall a\in \mcA_{B_{n,\lambda,\zeta}} \text{ with } \|a-a_0\|_2\geq \delta_{n,\zeta}: \frac{a - a_0}{\|a-a_0\|_2} \in \spanF_{\kappa}(\mcF)}
\end{equation}
Then $\hat{a}$ satisfies that w.p. $1-\zeta$:
\begin{equation}
    \|\hat{a}-a_0\|_2 \leq \kappa \left( 2\left(\|a_0\|_{\mcA}+1\right) \mcR(\mcA_1) + \delta_{n,\zeta} + \lambda \left(\|a_0\|_{\mcA}-\|\hat{a}\|_{\mcA}\right)\right)
\end{equation}
\end{theorem}




