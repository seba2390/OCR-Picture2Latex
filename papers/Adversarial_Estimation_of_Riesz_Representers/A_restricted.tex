\section{Unrestricted and Restricted Models}\label{sec:restricted}

In the context of semi-parametric statistics, recall that the causal parameter $\theta_0=\theta(g_0)=\mathbb{E}[m(Z;g_0)]$ is a functional $m$ of the underlying regression $g_0(x):=\mathbb{E}[Y|X=x]$. In an unrestricted model, we assume $g_0\in L^2(\mathbb{P})$, the space of square integrable functions. In a restricted model, additional information about $g_0$ can be encoded by the restriction $g_0\in\mathcal{G}_0\subset L^2(\mathbb{P})$, where $\mathcal{G}_0$ is some convex function space. In this section, we give an account of Riesz representation in restricted models, following the notation and technical lemmas of \cite{chernozhukov2018global}. 

Denote $\mathcal{G}:=span(\mathcal{G}_0)$ and $\bar{\mathcal{G}}:=closure(\mathcal{G})$. Define the modulus of continuity of $g\mapsto \theta(g)$ by
$$
L:=\sup_{g\in \mathcal{G}\backslash \{0\}} \frac{|\theta(g)|}{\|g\|_2}
$$
\begin{definition}[RR and minimal RR]
A RR of the functional $\theta(g)$ is $a_0\in L^2(\mathbb{P})$ s.t.
$$
\theta(g)=\mathbb{E}[g(X)a_0(X)],\quad \forall g\in \mathcal{G}
$$
If $a_0\in \bar{\mathcal{G}}$, then it is the minimal RR and we denote it by $a_0^{\min}$. Any RR can be reduced to the minimal RR by projecting it onto $\bar{\mathcal{G}}$.
\end{definition}

\begin{lemma}[Lemma 1 of \cite{chernozhukov2018global}]
We have the following results
\begin{enumerate}
    \item If $L<\infty$ then there exists a unique minimal RR $a_0^{\min}$ and $L=\|a_0^{\min}\|_2$
    \item If there exists a RR $a_0$ with $\|a_0\|_2<\infty$ then $L=\|a_0^{\min}\|_2\leq \|a_0\|_2<\infty$, where $a_0^{\min}$ is the unique minimal RR obtained by projecting $a_0$ onto $\bar{\mathcal{G}}$
\end{enumerate}
In both cases, $g\mapsto\theta(g)$ can be extended to $\bar{\mathcal{G}}$ or to all of $L^2(\mathbb{P})$ with modulus of continuity $L$
\end{lemma}

To interpret these results, consider a toy example of vectors in $\mathbb{R}^3$ rather than functions in $L^2(\mathbb{P})$. Suppose the functional of interest is 
$$
\theta:\mathbb{R}^3 \rightarrow\mathbb{R},\quad (x,y,z)\mapsto x+2y+3z
$$
Moreover, assume $g_0\in \mathcal{G}$ where $\mathcal{G}$ is the $(x,y)$-plane, though the ambient space is $\mathbb{R}^3$. Then any vector of the form $a_0=(1,2,c)$ with $c\in\mathbb{R}$ is a valid RR. The unique minimal RR is $a_0^{\min}=(1,2,0)$. As an aside, the vector $a_0=(1,2,3)$ is a universal RR; it holds for any choice of $\mathcal{G}\subset \mathbb{R}^3$, not just the $(x,y)$-plane. From any RR, we can obtain the minimal RR by projection onto the $(x,y)$-plane.

In \cite[Theorem 2]{chernozhukov2018global}, we see that it is better to use $a_0^{\min}$ rather than any $a_0$ to attain full semi-parametric efficiency (unless of course $\mathcal{G}=L^2(\mathbb{P})$ so there is no difference). By the stated lemma, we know how to obtain $a_0^{\min}$ from any $a_0$: projection onto $\bar{\mathcal{G}}$.

When do these technical issues arise? In the semi-parametric literature, a popular restricted model is the additive model. It is an important setting where $\mathcal{G}$ is not dense in $L^2(\mathbb{P})$. We present a definition of the additive model, then a technical lemma about the minimal RR in an additive model.

\begin{definition}[Additive model]
Suppose that 
\begin{enumerate}
    \item the regression $g_0$ is additive in components $x=(x^{(1)},x^{(2)})$:
$
g_0(x)=g^{(1)}_0(x^{(1)})+g^{(2)}_0(x^{(2)})
$
    \item $g_0^{(1)} \in \mathcal{G}_0^{(1)}$, a dense subset of $L^2(\mathbb{P}^{(1)})$, where $\mathbb{P}^{(1)}$ is the distribution of $X^{(1)}$
    \item the functional depends on only the first component: $m(z;g)=m(z;g^{(1)})$
\end{enumerate}

\end{definition}

\begin{lemma}[Lemma 6 of \cite{chernozhukov2018global}]
Assume an additive model. Consider any RR $a_0\in L^2(\mathbb{P})$. Then $\forall g\in\mathcal{G}$
$$
\theta(g)=\theta(g^{(1)})=\int a_0^{\min}(x^{(1)})g^{(1)}(x^{(1)})d\mathbb{P}^{(1)},\quad a_0^{\min}(x^{(1)})=\mathbb{E}[a_0(X)|X^{(1)}=x^{(1)}]
$$
and
$$
\|a_0^{\min}\|_q\leq \|a_0\|_q,\quad\forall q\in[1,\infty]
$$
\end{lemma}
This preservation of order and contraction of norm is helpful in analysis.

Finally, we quote some projection geometry for sumspaces from \cite[Appendix A.4]{bickel1993efficient}. Suppose $\mathcal{H}_1$ and $\mathcal{H}_2$ are closed subspaces of a Hilbert space $\mathcal{H}$. 

\begin{lemma}
If $\mathcal{H}_1 \perp \mathcal{H}_2$ then the projection onto the sumspace $\mathcal{H}_1 + \mathcal{H}_2$ is the sum of the projections onto $\mathcal{H}_1$ and $\mathcal{H}_2$
\end{lemma}

More generally, $\mathcal{H}_1$ may not be orthogonal to $\mathcal{H}_2$. Denote by $P_i$ the orthogonal projection onto $\mathcal{H}_i$, and denote by $Q_i:=I-P_i$ the projection onto $\mathcal{H}_i^{\perp}$. Denote by $\Pi$ the projection onto the closure of $\mathcal{H}_1+\mathcal{H}_2$

\begin{lemma}[Corollary 1 of \cite{bickel1993efficient}]
For any $h\in\mathcal{H}$
$$
[I-(Q_1Q_2)^m]h\rightarrow \Pi h,\quad m\rightarrow\infty
$$
\end{lemma}

Stronger versions of this result are available that provide quantitative rates of convergence and that allow for $r\geq 2$ subspaces.