
\section{Proofs from \Cref{sec:debiasing}}

\subsection{Proof of \Cref{lem:debias}}
\begin{proof}
Observe that $\theta_0 = \E[m_{a}(Z; g_0)]$ for all $a$. Moreover, we can decompose:
\begin{align}
    \hat{\theta}-\theta_0 =~& \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \left(m_{\hat{a}_k}(Z_i; \hat{g}) - \E_Z[m_{\hat{a}_k}(Z; \hat{g}_k)]\right) + \frac{1}{K} \sum_{k=1}^K \left(\E_Z[m_{\hat{a}_k}(Z; \hat{g}_k)] - \E_Z[m_{\hat{a}_k}(Z; g_0)]\right)\\
    =~& \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \left(m_{\hat{a}_k}(Z_i; \hat{g}_k) - \E_Z[m_{\hat{a}_k}(Z; \hat{g}_k)]\right) + \frac{1}{K} \sum_{k=1}^K \E_X[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))]
\end{align}

Thus as long as $K=\Theta(1)$ and:
\begin{equation}
    \sqrt{n} \E_X[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] \rightarrow_p 0
\end{equation}
we have that:
\begin{align}
    \sqrt{n}\left(\hat{\theta}-\theta_0\right) =~& \sqrt{n} \underbrace{\frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \left(m_{\hat{a}_k}(Z_i; \hat{g}_k) - \E_Z[m_{\hat{a}_k}(Z; \hat{g}_k)]\right)}_{A} + o_p(1)
\end{align}
Suppose that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that: $\|\hat{a}_k-a_*\|_2 \to_p 0$ and $\|\hat{g}_k-g_*\|_2\to_p 0$. Then we can further decompose $A$ as:
\begin{align}
    A =~& \E_n[m_{a_*}(Z; g_*)] - \E_Z[m_{a_*}(Z; g_*)] + \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \underbrace{m_{\hat{a}_k}(Z_i; \hat{g}_k) - m_{a_*}(Z_i; g_*) - \E_Z[m_{\hat{a}_k}(Z; \hat{g}_k) - m_{a_*}(Z; g_*)]}_{V_i}
\end{align}
Denote with: 
\begin{equation}
    B := \frac{1}{n}\sum_{k=1}^K \sum_{i\in P_k} V_i =: \frac{1}{n} \sum_{k=1}^{K} B_k
\end{equation}
As long as $n\,\E[B^2]\to 0$, then we have that $\sqrt{n} B \to_p 0$. The second moment of each $B_k$ is:
\begin{equation}
    \E\left[B_k^2\right] = \sum_{i, j\in P_k} \E[V_i V_j] = \sum_{i, j\in P_k} \E[\E[V_i V_j \mid \hat{g}_k]] = \sum_{i\in P_k} \E\left[V_i^2\right] 
\end{equation}
where in the last equality we used the fact that due to cross-fitting, for any $i\neq j$, $V_i$ is independent of $V_j$ and mean-zero, conditional on the nuisance $\hat{g}_k$ estimated on the out-of-fold data for fold $k$. 
Moreover, by Jensen's inequality with respect to $\frac{1}{K}\sum_{k=1}^K B_k$
\begin{equation}
    \E[B^2] = \E\left[\left(\frac{1}{n} \sum_{k=1}^K B_k\right)^2\right] = \frac{K^2}{n^2} \E\left[\left(\frac{1}{K} \sum_{k=1}^K B_k\right)^2\right] \leq \frac{K}{n^2} \sum_{k=1}^K \E[B_k^2] = \frac{K}{n^2} \sum_{k=1}^K \sum_{i\in P_k} \E[V_i^2] = \frac{K}{n^2} \sum_{i=1}^n \E[V_i^2]
\end{equation}
Finally, observe that $\E[V_i^2] \to_p 0$, by mean-squared-continuity of the moment and by boundedness of the Riesz representer function class, the function class $\mcG$ and the variable $Y$. More elaborately:
\begin{align}
    \E[V_i^2] \leq~& \E\left[\left(m_{\hat{a}}(Z_i; \hat{g}_k) - m_{a_*}(Z_i; g_*)\right)^2\right] \\
    \leq~& 2\,\E\left[\left(m(Z_i; \hat{g}_k) - m(Z_i; g_*)\right)^2\right] + 2\, \E[\left(\hat{a}_k(X)\,(Y - \hat{g}_k(X)) - a_*(X)\,(Y-g_*(X))\right)^2]
\end{align}
The latter can further be bounded as:
\begin{align}
    4\E[\left(a_k(X) - a_*(X)\right)^2 (Y-g_k(X))^2] + 4\E[a_*(X)^2 (g_*(X) - g_k(X))^2] \leq 4 C\, \left(\E\left[\|\hat{a}_k - a_*\|_2^2 + \|\hat{g} - g_*\|_2^2\right]\right)
\end{align}
assuming that $(Y-\hat{g}_k(X))^2 \leq C$ and $a_*(X)^2 \leq C$ a.s..
Finally, by linearity of the operator and mean-squared continuity, we have:
\begin{equation}
    \E[\left(m(Z_i; \hat{g}_k) - m(Z_i; g_*)\right)^2] = \E[\left(m(Z_i; \hat{g}_k - g_*)\right)^2] \leq M\, \E\left[\|\hat{g}_k - g_*\|_2^2\right]
\end{equation}
Thus we have:
\begin{equation}
    \E[V_i^2] \leq (2M + 4C)\, \left(\E\left[\|\hat{a}_k - a_*\|_2^2 + \|\hat{g} - g_*\|_2^2\right]\right) \to 0
\end{equation}
Thus as long as $K = \Theta(1)$, we have that:
\begin{equation}\label{eqn:crucial-normality}
    n\, \E[B^2] = \frac{K}{n} \sum_{i=1}^n \E[V_i^2] \leq (2M + 4C)\, K\, \E\left[\|\hat{g}-g_*\|_2^2 + \|\hat{a}-a_*\|_2^2\right] \to 0
\end{equation}
and we can conclude the result that:
\begin{align}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) = \sqrt{n} \left(\E_n[m_{a_*}(Z; g_*)] - \E_Z[m_{a_*}(Z; g_*)]\right) + o_p(1)
\end{align}
where the latter term can be easily argued, invoking the Central Limit Theorem, to be asymptotically normal $N(0, \sigma_*^2)$ with $\sigma_*^2 =\Var(m_{a_*}(Z; g_*))$.
\end{proof}


\subsection{Proof of Normality without Consistency}\label{sec:inconsistent}

\begin{lemma}\label{lem:debias-inconsistent}
Suppose that $K=\Theta(1)$ and that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that for all $k\in [K]$: $\|\hat{a}_k-a_*\|_2 \stackrel{L^2}{\to} 0$ and $\|\hat{g}_k-g_*\|_2\stackrel{L^2}{\to} 0$.
Assume that:
\begin{align}
    \forall k \in [K]: \sqrt{n}\, \E[(a_*(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_*(X))] \rightarrow_p 0
\end{align}
and that $\hat{g}_k$ admits an asymptotically linear representation around the truth $g_0$, i.e.:
\begin{equation}
    \sqrt{|P_k|}\left(\hat{g}_k(X) - g_0(X)\right) = \frac{1}{\sqrt{|P_k|}} \sum_{i\in P_k} \psi(X, Z_i; g_0) + o_p(1)
\end{equation}
with $\E[\psi(X, Z_i; g_0)\mid X]=0$ and let:
\begin{equation}
    \sigma_*^2 :=\Var_{Z_i}(m_{a_*}(Z_i; g_*) + \E_X\left[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)\right])
\end{equation}
 Assume that Condition~\ref{ass:strong-smooth} is satisfied and the variables $Y, g(X), a(X)$ are bounded a.s. for all $g\in \mcG$ and $a\in \mcA$. Then:
\begin{equation}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) \to_d N\left(0, \sigma_*^2\right)
\end{equation}
Similarly, if $\hat{a}_k$ has an asymptotically linear representation around the truth, then the statement above holds with:
\begin{equation}
    \sigma_*^2 :=\Var_{Z_i}(m_{a_*}(Z_i; g_*) + \E_X\left[\psi(X, Z_i; a_0)\, (g_0(X) - g_*(X))\right])
\end{equation}
\end{lemma}
\begin{proof}
Observe that $\theta_0 = \E[m_{a}(Z; g_0)]$ for all $a$. Moreover, we can decompose:
\begin{align}
    \hat{\theta}-\theta_0 =~& \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \left(m_{\hat{a}_k}(Z_i; \hat{g}) - \E[m_{\hat{a}_k}(Z; \hat{g}_k)]\right) + \frac{1}{K} \sum_{k=1}^K \left(\E[m_{\hat{a}_k}(Z; \hat{g}_k)] - \E[m_{\hat{a}_k}(Z; g_0)]\right)\\
    =~& \underbrace{\frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \left(m_{\hat{a}_k}(Z_i; \hat{g}_k) - \E[m_{\hat{a}_k}(Z; \hat{g}_k)]\right)}_{A} + \underbrace{\frac{1}{K} \sum_{k=1}^K \E[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))]}_{C}
\end{align}
Suppose that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that: $\|\hat{a}_k-a_*\|_2 \to_p 0$ and $\|\hat{g}_k-g_*\|_2\to_p 0$. Then we can further decompose $A$ as:
\begin{align}
    A =~& \E_n[m_{a_*}(Z; g_*)] - \E[m_{a_*}(Z; g_*)] + \frac{1}{n} \sum_{k=1}^K \sum_{i\in P_k} \underbrace{m_{\hat{a}_k}(Z_i; \hat{g}_k) - m_{a_*}(Z_i; g_*) - \E[m_{\hat{a}_k}(Z; \hat{g}_k) - m_{a_*}(Z; g_*)]}_{V_i}
\end{align}
Denote with: 
\begin{equation}
    B := \frac{1}{n}\sum_{k=1}^K \sum_{i\in P_k} V_i =: \frac{1}{n} \sum_{k=1}^{K} B_k
\end{equation}
As long as $n\,\E[B^2]\to 0$, then we have that $\sqrt{n} B \to_p 0$. The second moment of each $B_k$ is:
\begin{equation}
    \E[B_k^2] = \sum_{i, j\in P_k} \E[V_i V_j] = \sum_{i, j\in P_k} \E[\E[V_i V_j \mid \hat{g}_k]] = \sum_{i\in P_k} \E[V_i^2] 
\end{equation}
where in the last equality we used the fact that due to cross-fitting, for any $i\neq j$, $V_i$ is independent of $V_j$ and mean-zero, conditional on the nuisance $\hat{g}_k$ estimated on the out-of-fold data for fold $k$. 
Moreover, by Jensen's inequality with respect to $\frac{1}{K}\sum_{k=1}^K B_k$
\begin{equation}
    \E[B^2] = \E\left[\left(\frac{1}{n} \sum_{k=1}^K B_k\right)^2\right] = \frac{K^2}{n^2} \E\left[\left(\frac{1}{K} \sum_{k=1}^K B_k\right)^2\right] \leq \frac{K}{n^2} \sum_{k=1}^K \E[B_k^2] = \frac{K}{n^2} \sum_{k=1}^K \sum_{i\in P_k} \E[V_i^2] = \frac{K}{n^2} \sum_{i=1}^n \E[V_i^2]
\end{equation}
Finally, observe that $\E[V_i^2] \to_p 0$, by mean-squared-continuity of the moment and by boundedness of the Riesz representer function class, the function class $\mcG$ and the variable $Y$. More elaborately:
\begin{align}
    \E[V_i^2] \leq~& \E\left[\left(m_{\hat{a}}(Z_i; \hat{g}_k) - m_{a_*}(Z_i; g_*)\right)^2\right] \\
    \leq~& 2\,\E\left[\left(m(Z_i; \hat{g}_k) - m(Z_i; g_*)\right)^2\right] + 2\, \E[\left(\hat{a}_k(X)\,(Y - \hat{g}_k(X)) - a_*(X)\,(Y-g_*(X))\right)^2]
\end{align}
The latter can further be bounded as:
\begin{align}
    4\E[\left(a_k(X) - a_*(X)\right)^2 (Y-g_k(X))^2] + 4\E[a_*(X)^2 (g_*(X) - g_k(X))^2] \leq 4 C\, \E\left[\|\hat{a}_k - a_*\|_2^2 + \|\hat{g} - g_*\|_2^2\right]
\end{align}
assuming that $(Y-\hat{g}_k(X))^2 \leq C$ and $a_*(X)^2 \leq C$ a.s..
Finally, by linearity of the operator and mean-squared continuity, we have:
\begin{equation}
    \E[\left(m(Z_i; \hat{g}_k) - m(Z_i; g_*)\right)^2] = \E[\left(m(Z_i; \hat{g}_k - g_*)\right)^2] \leq M\, \E\left[\|\hat{g}_k - g_*\|_2^2\right]
\end{equation}
Thus we have:
\begin{equation}
    \E[V_i^2] \leq (2M + 4C)\, \E\left[\|\hat{a}_k - a_*\|_2^2 + \|\hat{g} - g_*\|_2^2\right] \to 0
\end{equation}
Thus as long as $K = \Theta(1)$, we have that:
\begin{equation}\label{eqn:crucial-normality}
    n\, \E[B^2] = \frac{K}{n} \sum_{i=1}^n \E[V_i^2] \leq (2M + 4C)\, K\, \left(\|\hat{g}-g_*\|_2^2 + \|\hat{a}-a_*\|_2^2\right) \to_p 0
\end{equation}
and we can conclude the result that:
\begin{align}
    \sqrt{n}\, A = \sqrt{n} \left(\E_n[m_{a_*}(Z; g_*)] - \E[m_{a_*}(Z; g_*)]\right) + o_p(1)
\end{align}


Now we analyze term $C$. We will prove one of the two conditions in the ``or'' statement, when $\hat{g}_k$ has an asymptotically linear representation, i.e. \begin{equation}
\sqrt{|P_k|}\left(\hat{g}_k(X) - g_0(X)\right) = \frac{1}{\sqrt{|P_k|}} \sum_{i\in P_k} \psi(X, Z_i; g_0) + o_p(1)
\end{equation}
with $\E[\psi(X, Z_i; g_0)\mid X]=0$. The case when $\hat{a}_k$ is asymptotically linear can be proved analogously.

Let:
\begin{equation}
    C_k := \E[(a_0(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] 
\end{equation}
We can then write:
\begin{align}
    C_k =  \E[(a_*(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))] + \E[(a_0(X) - a_*(X))\, (\hat{g}_k(X) - g_0(X))]
\end{align}
Since:
\begin{equation}
\sqrt{|P_k|}\E[(a_*(X) - \hat{a}_k(X))\, (\hat{g}_k(X) - g_0(X))]\leq \sqrt{|P_k|} \|a_* - \hat{a}_k\|_2 \, \|\hat{g}_k - g_0\|_2 = \|a_* - \hat{a}_k\|_2\, O_p(1) = o_p(1)
\end{equation}
we have that:
\begin{align}
    \sqrt{|P_k|} C_k =~& \sqrt{|P_k|}\E[(a_0(X) - a_*(X))\, (\hat{g}_k(X) - g_0(X))] + o_p(1)\\
    =~& \frac{1}{\sqrt{|P_k|}} \sum_{i\in P_k} \E_X[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)] + o_p(1)
\end{align}
Since $K=\Theta(1)$ and $n/|P_k| \to K$, we then also have that:
\begin{align}
    \sqrt{n} C =~& \frac{\sqrt{n}}{K} \sum_{k=1}^K C_k =  \frac{\sqrt{K}}{K} \sum_{k=1}^K \sqrt{|P_k|} C_k + o(1)\\
    =~& \frac{1}{\sqrt{K}} \sum_{k=1}^K \frac{1}{\sqrt{|P_k|}} \sum_{i\in P_k} \E_X[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)] + o_p(1)\\
    =~& \frac{1}{\sqrt{n}} \sum_{k=1}^K \sum_{i\in P_k} \E_X[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)] + o_p(1)\\
    =~& \frac{1}{\sqrt{n}} \sum_{i\in [n]} \E_X[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)] + o_p(1)\\
    =~& \sqrt{n} \E_n\left[\E_X[(a_0(X) - a_*(X))\,\psi(X,Z_i; g_0)]\right] + o_p(1)
\end{align}
\begin{align}
    \sqrt{n}\left(\hat{\theta}-\theta_0\right) =~& \sqrt{n} \left(\E_n\left[m_{a_*}(Z; g_*) + \E_X\left[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)\right]\right] - \E[m_{a_*}(Z; g_*)]\right)  + o_p(1)
\end{align}
where the latter term can be easily argued, invoking the Central Limit Theorem, to be asymptotically normal $N(0, \sigma_*^2)$ with $\sigma_*^2 =\Var_{Z_i}(m_{a_*}(Z_i; g_*) + \E_X\left[(a_0(X) - a_*(X))\,\psi(X, Z_i; g_0)\right])$.
\end{proof}


\subsection{Proof of \Cref{lem:debias-nocross}}

\begin{proof}
Observe that $\theta_0 = \E[m_{a}(Z; g_0)]$ for all $a$. Moreover, we can decompose:
\begin{align}
    \hat{\theta}-\theta_0 =~& \E_n[m_{\hat{a}}(Z; \hat{g})] - \E[m_{\hat{a}}(Z; \hat{g})] + \E[m_{\hat{a}}(Z; \hat{g})] - \E[m_{\hat{a}}(Z; g_0)]\\
    =~& \E_n[m_{\hat{a}}(Z; \hat{g})] - \E[m_{\hat{a}}(Z; \hat{g})]  + \E[(a_0(X) - \hat{a}(X))\, (\hat{g}(X) - g_0(X))]
\end{align}

Thus as long as $K=\Theta(1)$ and:
\begin{equation}
    \sqrt{n} \E[(a_0(X) - \hat{a}(X))\, (\hat{g}(X) - g_0(X))] \rightarrow_p 0
\end{equation}
we have that:
\begin{align}
    \sqrt{n}\left(\hat{\theta}-\theta_0\right) =~& \sqrt{n} \underbrace{\E_n[m_{\hat{a}}(Z; \hat{g})] - \E[m_{\hat{a}}(Z; \hat{g})]}_{A} + o_p(1)
\end{align}
Suppose that for some $a_*$ and $g_*$ (not necessarily equal to $a_0$ and $g_0$), we have that: $\|\hat{a}_k-a_*\|_2 \to_p 0$ and $\|\hat{g}_k-g_*\|_2\to_p 0$. Then we can further decompose $A$ as:
\begin{align}
    A =~& \E_n[m_{a_*}(Z; g_*)] - \E[m_{a_*}(Z; g_*)] + \E_n\left[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)\right] - \E[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)]
\end{align}
Let $\delta_{n,\zeta}=\delta_n + c_0 \sqrt{\frac{\log(c_1/\zeta)}{n}}$, where $\delta_n$ upper bounds the critical radius of function classes $\mcG_B$ and $m \circ \mcG_B$ and $\mcA_B$, where $B$ is set such that these sets contain functions that are bounded in $[-1, 1]$. By a concentration inequality, almost identical to that of Equation~\eqref{eqn:reg-concentration}, we have that w.p. $1-\zeta$: $\forall f\in \mcF, a\in \mcA$
\begin{multline}
     \left|\E_n\left[m_{a}(Z; g) - m_{a_*}(Z; g_*)\right] - \E[m_{a}(Z; a) - m_{a_*}(Z; g_*)]\right| \\
     \leq O\left(\delta_{n,\zeta} \left(\|m\circ (g - g_*)\|_2 \| \|a\|_{\mcA} + \|a-a_*\|_2 \|g\|_{\mcG} + \|g-g_*\|_2 \|a\|_{\mcA}\right) + \delta_{n,\zeta}^2 \|a\|_{\mcA} \|g\|_{\mcG}\right)
\end{multline}
Applying the latter for $\hat{g}, \hat{a}$ and invoking the MSE continuity, w.p. $1-\zeta$:
\begin{multline}
     \left|\E_n\left[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)\right] - \E[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)]\right| \\
     \leq O\left(\delta_{n,\zeta} M \left(\|\hat{a}-a_*\|_2\, \|\hat{g}\|_{\mcG} + \|\hat{g}-g_*\|_2\, \|\hat{a}\|_{\mcA}\right) + \delta_{n,\zeta}^2\, \|\hat{g}\|_{\mcG} \, \|\hat{a}\|_{\mcA}\right)
\end{multline}
If we let $\delta_{n,*} = \delta_n + c_0 \sqrt{\frac{c_1 n}{n}}$, then we have that:
\begin{multline}
    \left|\E_n\left[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)\right] - \E[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)]\right| \\
     = O_p\left(\delta_{n,*} M \left(\|\hat{a}-a_*\|_2\, \|\hat{g}\|_{\mcG} + \|\hat{g}-g_*\|_2\, \|\hat{a}\|_{\mcA}\right) + \delta_{n,*}^2\, \|\hat{g}\|_{\mcG} \, \|\hat{a}\|_{\mcA}\right)
\end{multline}
If $\|\hat{a} - a_*\|_2, \|\hat{g}-g_*\|_2= O_p(r_n)$ and $\|\hat{a}\|_{\mcA}, \|\hat{g}\|_{\mcG} = O_p(1)$, we have that:
\begin{equation}
    \left|\E_n\left[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)\right] - \E[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)]\right| 
     = O_p\left(M\, \delta_{n,*} r_n  + \delta_{n,*}^2\right)
\end{equation}
Thus as long as: $\sqrt{n}\left(\delta_{n, *} r_n + \delta_{n,*}^2\right) \to 0$, we have that:
\begin{equation}
    \sqrt{n}\left|\E_n\left[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)\right] - \E[m_{\hat{a}}(Z; \hat{g}) - m_{a_*}(Z; g_*)]\right| 
     = o_p(1)
\end{equation}
Thus we conclude that:
\begin{align}
    \sqrt{n}\left(\hat{\theta} - \theta_0\right) = \sqrt{n} \left(\E_n[m_{a_*}(Z; g_*)] - \E[m_{a_*}(Z; g_*)]\right) + o_p(1)
\end{align}
where the latter term can be easily argued, invoking the Central Limit Theorem, to be asymptotically normal $N(0, \sigma_*^2)$ with $\sigma_*^2 =\Var(m_{a_*}(Z; g_*))$.
\end{proof}


\subsection{Proof of \Cref{lem:debias-nocross-stability}}

\begin{proof}
Let $h=(a, g)$ and $V(Z; h)=m_{a}(Z; g) - m_{a_*}(Z; g_*) - \E[m_{a}(Z; g) - m_{a_*}(Z; g_*)]$. We argue that:
\begin{equation}
    \sqrt{n}\, \E_n\left[V(Z; \hat{h})\right] = o_p(1)
\end{equation}
The remainder of the proof is identical to the proof of \Cref{lem:debias-nocross}. For the above property it suffices to show that $n\, \E\left[\E_n\left[V(Z; \hat{h})\right]^2\right] \to 0$.

First we re-write the differences $V(Z; h) - V(Z;h')$: 
\begin{align}
    V(Z; h) - V(Z;h') 
    =~& m(Z; g-g') + (a(X) - a'(X))\, Y - a(X) g(X) + a'(X) g'(X)\\
    & - \left(\ldot{a_0}{g-g'}_2 - \ldot{a}{g}_2 + \ldot{a'}{g'}_2 + \ldot{a-a'}{g_0}_2\right)
\end{align}
By MSE continuity of the the moment and boundedness of the functions we have that:
\begin{equation}
\E\left[\left(V(Z; h) - V(Z; h')\right)^2\right] \leq c_0 \E\left[\|h(X) - h'(X)\|_{\infty}^2\right]
\end{equation}
for some constant $c_0$. Moreover, since, for every $x, y$: $x^2 \leq y^2 + |x|\, |x-y| + |y|\, |x-y|$:
\begin{align}
    \E\left[\E_n[V(Z; \hat{h})]^2\right] =~& \frac{1}{n^2} \sum_{i, j}\E\left[V(Z_i; \hat{h})  V(Z_j;\hat{h})\right]\\
    \leq~& \frac{1}{n^2}\sum_{i, j} \left(\E\left[V(Z_i;\hat{h}^{-i,j})  V(Z_j;\hat{h}^{-i,j})\right] + 2\, \E\left[\left|V(Z_i;\hat{h}^{-i,j})\right|\, \left|V(Z_j;\hat{h}^{-i,j}) - V(Z_j; \hat{h})\right|\right]\right)\\
    \leq~& \frac{1}{n^2}\sum_{i, j} \left(\E\left[V(Z_i;\hat{h}^{-i,j})  V(Z_j;\hat{h}^{-i,j})\right] + 2\, \sqrt{\E\left[V(Z_i;\hat{h}^{-i,j})^2\right]}\, \sqrt{\E\left[\left(V(Z_j;\hat{h}^{-i,j}) - V(Z_j; \hat{h})\right)^2\right]}\right)\\
    \leq~& \frac{1}{n^2}\sum_{i, j} \left(\E\left[V(Z_i;\hat{h}^{-i,j})  V(Z_j;\hat{h}^{-i,j})\right] + 2\, c_0\,  \sqrt{\E\left[V(Z_i;\hat{h}^{-i,j})^2\right]}\, \sqrt{\E\left[\|\hat{h}^{-i,j}(X_j) - \hat{h}(X_j)\|_{\infty}^2\right]}\right)\\
    \leq~& \frac{1}{n^2}\sum_{i, j} \left(\E\left[V(Z_i;\hat{h}^{-i,j})  V(Z_j;\hat{h}^{-i,j})\right] + 8\, c_0\, \beta_{n-1} \sqrt{\E\left[V(Z_i;\hat{h}^{-i,j})^2\right]}\right)
\end{align}
For every $i\neq j$ we have:
\begin{align}
    \E[V(Z_i; \hat{h}^{-i,j})  V(Z_j; \hat{h}^{-i,j})] =~& \E\left[\E\left[V(Z_i; \hat{h}^{-i})  V(Z_j; \hat{h}^{-j}) \mid \hat{h}^{-i,j}\right]\right]\\ 
    =~& \E\left[\E\left[V(Z_i;\hat{h}^{-i,j}) \mid \hat{h}^{-i,j}\right]  \E\left[V(Z_j; \hat{h}^{-i,j}) \mid \hat{h}^{-i,j}\right]\right] = 0
\end{align}
and
\begin{align}
\sqrt{\E[V(Z; \hat{h}^{-i,j})^2]} \leq~& O\left(\sqrt{\E\left[\|\hat{a}^{-i,j} - a_*\|_2^2 + \|\hat{g}^{-i,j} - g_*\|_2^2\right]}\right) = O(r_{n-2})\\
\E[V(Z; \hat{h}^{-i})^2] \leq~&  O\left(\E\left[\|\hat{a}^{-i} - a_*\|_2^2 + \|\hat{g}^{-i} - g_*\|_2^2\right]\right) = O(r_{n-1}^2)
\end{align}
Thus we get that:
\begin{equation}
    n\, \E\left[\E_n[V(Z; \hat{h})]^2\right] = \frac{1}{n} \sum_{i=1}^n \E[V(Z_i; \hat{h}^{-i})^2]  + O\left(\beta_{n-1} r_{n-2}\right)= O\left(r_{n-1}^2 + n\,\beta_{n-1} r_{n-2}\right)
\end{equation}
Thus it suffices to assume that
\begin{equation}
r_{n-1}^2 + n\,\beta_{n-1} r_{n-2}\to 0
\end{equation}
\end{proof}