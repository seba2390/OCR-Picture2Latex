\section{Experiment}
\label{sec:experiment}
We start by exhibiting the effectiveness of our new training objective over the baseline objective in~\cref{sec:experiment_baseline}. Then, we compare with prior literature of various types of generative models in~\cref{sec:experiment_literature}. Finally, we conduct analysis studies to further support our method in~\cref{sec:experiment_analysis}. Samples generated with our models are shown in~\cref{fig:teaser}.


\input{tables/various}

\subsection{Comparison to the Baseline}
\label{sec:experiment_baseline}
\textbf{Quantitative comparison.  } 
We trained diffusion models by optimizing training objectives with both baseline and our weighting scheme on FFHQ~\cite{stylegan}, AFHQ-dog~\cite{choi2020stargan}, MetFaces~\cite{karras2020training}, and CUB~\cite{wah2011caltech} datasets. These datasets contain approximately 70k, 50k, 1k, and 12k images respectively.  We resized and center-cropped data to 256$\times$256 pixels, following the pre-processing performed by ADM~\cite{dhariwal2021diffusion}.

\cref{table:fid_various} shows the results. Our method consistently exhibits superior performance to the baseline in terms of FID and KID. The results suggest that our weighting scheme imposes a good inductive bias for training diffusion models, regardless of the dataset. Our method outperforms the baseline by a large margin especially on MetFaces, which contains only 1k images. Hence, we assume that wasting the model capacity on learning imperceptible details is very harmful when training with limited data. 


\textbf{Qualitative comparison.  }
We observe that diffusion models trained with the baseline objective are likely to suffer color shift artifacts, as shown in~\cref{fig:quality_compare}. We assume that the baseline training objective unnecessarily focuses on the imperceptible details; therefore, it fails to learn global color schemes properly. In contrast, our objective encourages the model to learn global and holistic concepts in a dataset.

\subsection{Comparison to the Prior Literature}
\label{sec:experiment_literature}
We compare diffusion models trained with our method to existing models on FFHQ~\cite{stylegan}, Oxford flowers~\cite{nilsback2008automated}, and CelebA-HQ~\cite{karras2017progressive} datasets, as shown in~\cref{table:fid_ffhq2}. We use 256$\times$256 resolutions for all datasets. We achieve state-of-the-art FIDs on the Oxford Flowers and CelebA-HQ datasets. While our models are trained with $T=1000$, we already achieve state-of-the-art with reduced sampling steps; 250 and 500 steps respectively. On FFHQ, we achieve a superior result to most models except StyleGAN2~\cite{karras2020analyzing}, whose architecture was carefully designed for FFHQ. We note that our method brought the diffusion model closer to the state-of-the-art, and scaling model architectures and sampling steps will further improve the performance.


\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/qualitative_compare.png}
  \caption{\textbf{Qualitative comparison.} Uncurated samples generated during training. The number of images seen for training displayed on the top. We observed that the baseline suffer color shift problem at early iterations of the training (FFHQ) or even at the convergence (MetFaces). The baseline weighting scheme fails to focus on global coherence and waste model capacity on the imperceptible details.}
  \label{fig:quality_compare}
\end{figure*}


\input{tables/ffhq2}

\subsection{Analysis}
\label{sec:experiment_analysis}
In this section, we analyze whether our weighting scheme is robust to the model configurations, number of sampling steps, and sampling schedules.

\textbf{Model configuration matters?  }
Previous experiments are conducted using our default model for fair comparisons. Here, we show that P2 weighting is effective regardless of the model configurations. \cref{table:architecture} shows that our method achieves consistently superior performance to the baseline for various configurations. We investigated for following variations: replacing the BigGAN~\cite{brock2018large} residual block with the residual block of Ho et al.~\cite{ho2020denoising}, removing self-attention at 16$\times$16, using two BigGAN~\cite{brock2018large} residual blocks, and training our default model with a learning rate of $2.5\times10^{-5}$. Our default model contains a single BigGAN~\cite{brock2018large} residual block and is trained with a learning rate $2\times10^{-5}$. Our weighting scheme consistently improves FID and KID by a large margin, across various model configurations. Our method is especially effective when the self-attention is removed ((c)), indicating that P2 encourages learning global dependency.
\vspace{0.5em}

\textbf{Sampling step matters?  }
We trained our models on 1000 diffusion steps following the convention of previous studies. However, it requires more than 10 min to generate a high-resolution image with a modern GPU. Nichol et al.~\cite{nichol2021improved} have shown that their sampling strategy maintains performance even when reducing the sampling steps. They also observed that using the DDIM~\cite{song2020denoising} sampler was effective when using 50 or fewer sampling steps.

\cref{fig:speed} shows the FID scores of various sampling steps with models trained on the FFHQ. A model trained with our weighting scheme consistently outperforms the baseline by considerable margins. It should be noted that our weighting scheme consistently achieves better performance with half the number of sampling steps required by the baseline.

\textbf{Why not schedule sampling steps?  } 
In addition to the consistent improvement across various sampling steps, we sweep over the sampling steps in~\cref{table:sweep}. Sweeping sampling schedules slightly improves FID and KID but does not reach our improvement. Our method is more effective compared to scheduling sampling steps, as we improve the \textit{model training}, which benefits predictions at all steps.



\input{tables/architecture}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/speed.png}
  \caption{\textbf{Reducing sampling steps.} FID as a function of sampling steps. Our method is superior to the baseline regardless of the sampling steps. Samples generated following~\cite{nichol2021improved} and DDIM~\cite{song2020denoising}. Models trained on FFHQ dataset.}
  \label{fig:speed}
\end{figure}

\input{tables/sweep}


