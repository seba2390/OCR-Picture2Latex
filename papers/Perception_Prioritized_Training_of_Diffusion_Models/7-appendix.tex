\clearpage
\newpage

\twocolumn[
]

\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{\Alph{figure}}
\setcounter{equation}{0}
\renewcommand{\theequation}{\Alph{equation}}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/weight.png}
  \caption{Unnormalized or normalized weights as functions of diffusion steps or signal-to-noise ratio (SNR). Large $t$ and small SNR indicates noisy image $x_t$ near random noise $x_T$, whereas small $t$ and large SNR indicates $x_t$ near a clean image $x_0$.}
  \vspace{6.5em}
  \label{fig:weight}
\end{figure*}

\section{Weighting Schemes}
\subsection{Additional Visualizations}
In the main text, we showed weights of both our new weighting scheme and the baseline, as functions of signal-to-noise ratio (SNR). In \cref{fig:weight} (left), we show weights as functions of time steps ($t$). To exhibit relative changes of weights, we show normalized weights as functions of both time steps (\cref{fig:weight} (middle)) and SNR (\cref{fig:weight} (right)). We normalized so that the sum of weights for all time steps become 1. Normalized weights suggest that larger $\gamma$ suppresses weights at steps near $t=0$ and uplifts weights at larger steps. Note that weights of VLB objective are equal to a constant, as such objective does not impose any inductive bias for training. In contrast, as discussed in the main text, our method encourages the model to learn rich content rather than imperceptible details.




\subsection{Derivations}
In the main text, we wrote the baseline weighting scheme $\lambda_t$ as a funtion of SNR, which characterizes the noise level at each step $t$. Below is the derivation:

\begin{align}\label{eq:derivation}
\lambda _t=~&
(1-\beta _t)(1-\alpha _t)/\beta _t\nonumber \\
=~&(\alpha _t / \alpha_{t-1})(1-\alpha _t)/(1-\alpha _t/\alpha _{t-1})\nonumber \\
=~&\alpha _t(1-\alpha _t)/(\alpha _{t-1}-\alpha _t)\nonumber \\
=~&\alpha _t(1-\alpha _t)/((1-\alpha _t)-(1 - \alpha _{t-1}))\nonumber \\
=~&\frac{\text{SNR}(t)}{(1+\text{SNR}(t))^2}/(\frac{1}{1+\text{SNR}(t)}-\frac{1}{1+\text{SNR}(t-1)})\nonumber \\
=~&\frac{\text{SNR}(t)}{(1+\text{SNR}(t))^2}/\frac{\text{SNR}(t-1)-\text{SNR}(t)}{(1+\text{SNR}(t))(1+\text{SNR}(t-1))}\nonumber \\
=~&\frac{\text{SNR}(t)(1+\text{SNR}(t-1))}{(1+\text{SNR}(t))(\text{SNR}(t-1)-\text{SNR}(t))}\nonumber \\
\approx~&\frac{-\text{SNR}(t)}{\text{SNR}'(t)}~(T\to \infty),
\end{align}
which is a differential of log-SNR($t$) regarding time-step $t$.

\section{Discussions}
\subsection{Limitations}
Despite the promising performances achieved by our method, diffusion models still need multiple sampling steps. Diffusion models require at least 25 feed-forwards with DDIM sampler, which makes it difficult to use diffusion models in real-time applications. Yet, they are faster than autoregressive models which generate a pixel at each step. In addition, we have observed in section 4.3 that our method enables better FID with half the number of steps required by the baseline. Along with our method, optimizing sampling schedules with dynamic programming~\cite{watson2021learning} or distilling DDIM sampling into a single step model~\cite{luhman2021knowledge} might be promising future directions for faster sampling.

\subsection{Broader Impacts}
The proposed method in this work allows high-fidelity image generation with diffusion-based generative models. Improving the performance of generative models can enable multiple creative applications~\cite{choi2021ilvr,meng2021sdedit}. However, such improvements have the potential to be exploited for deception. Works in deepfake detection~\cite{wang2020cnn} or watermarking~\cite{yu2021artificial} can alleviate the problems. Investigating invisible frequency artifacts~\cite{wang2020cnn} in samples of diffusion models might be promising approach to detect fake images.


\section{Implementation Details}

For a given time-step $t$, the input noisy image $x_t$ and output noise prediction $\epsilon$ and variance $\sigma _t$ are images of the same resolution. Therefore, $\epsilon _\theta$ is parameterized with the U-Net~\cite{ronneberger2015u}-style architecture of three input and six output channel dimensions. We inherit the architecture of ADM~\cite{dhariwal2021diffusion}, which is a U-Net with large channel dimension, BigGAN~\cite{brock2018large} residual blocks, multi-resolution attention, and multi-head attention with fixed channels per head. Time-step $t$ is provided to the model by adaptive group normalization (AdaGN), which transforms $t$ embeddings to scales and biases of group normalizations~\cite{wu2018group}. However, for efficiency, we use fewer base channels, fewer residual blocks, and a self-attention at a single resolution (16$\times$16). 

Hyperparameters for training models are in \cref{table:config}. We use $\gamma = 0.5$ for FFHQ and CelebA-HQ as it achieve slightly better FIDs than $\gamma = 1.0$ on those datasets. Models consist of one or two residual blocks per resolution and self-attention blocks at 16$\times$16 resolution or at bottleneck layers of 8$\times$8 resolution. Our default model has only 94M parameters, while recent works rely on large models (larger than 500M)~\cite{dhariwal2021diffusion}. While recent works use 2 or 4 blocks per resolution, we use only one block, which leads to speed-up of training and inference. We use dropout when training on limited data. We trained models using EMA rate of 0.9999, 32-bit precision, and AdamW optimizer~\cite{loshchilov2017decoupled}.

\input{tables/config}

\section{Additional Results}
\textbf{Qualitative.  } Additional samples for all datasets mentioned in the paper are in \cref{fig:sample_appendix}.

\textbf{Quantitative.  } In Fig. 1 of the main text, we measured perceptual distances to investigate how the diffusion process corrupts perceptual contents. In Fig. 2, we qualitatively explored what a trained model learned at each step (Fig. 2). Here, we reproduce Fig. 1 at various datasets and resolutions in \cref{fig:generalize} and show the quantitative result of Fig. 2 in \cref{fig:recon_appendix}. These results indicate that our investigation in Sec. 3.1 holds for various datasets and resolutions.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/fig1-generalize.png}

  \caption{\textbf{Generalization of sec. 3.} Results with CelebA-HQ, LSUN-Church, and CUB at $256^2$ and $64^2$ resolutions.}

  \label{fig:generalize}
\end{figure*}


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.80\linewidth]{figures/recon.png}
  \caption{\textbf{Stochastic reconstruction.} Perceptual distance between input and reconstructed image as a function of signal-to-noise ratio, measured with random 200 images from FFHQ.}
  \label{fig:recon_appendix}
\end{figure}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.90\linewidth]{figures/appendix-sample.pdf}
  \caption{Additional samples generated with our models traind on various datasets.}
  \label{fig:sample_appendix}
\end{figure*}


\newpage



