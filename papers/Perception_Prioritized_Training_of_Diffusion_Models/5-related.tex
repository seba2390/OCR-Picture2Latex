\section{Related Work}
\label{sec:related}

\subsection{Diffusion-Based Generative Models}
Diffusion models~\cite{sohl2015deep,ho2020denoising,nichol2021improved,dhariwal2021diffusion} and score-based models~\cite{song2019generative,song2020score} are two recent families of generative models that generate data using a learned denoising process. Song et al.~\cite{song2020score} showed that both families can be expressed with stochastic differentiable equations (SDE) with different noise schedules. We note that score-based models may enjoy different hyperparamters of P2 ($\gamma$ and $k$) as noise schedules are correlated with weighting schemes (\cref{eq:vlb}).
Recent studies~\cite{saharia2021image,dhariwal2021diffusion,song2020score} have achieved remarkable improvements in sample quality. However, they rely on heavy architectures, long training and sampling steps~\cite{song2020score}, classifier guidance~\cite{dhariwal2021diffusion}, and a cascade of multiple models~\cite{saharia2021image}. In contrast, we improved the performance by simply redesigning the training objective without requiring heavy computations and additional models. Along with the success in the image domain, diffusion models have also shown effectiveness in speech synthesis~\cite{kong2020diffwave,chen2020wavegrad}.

\subsection{Advantages of Diffusion Models}

Diffusion models have several advantages over other generative models. First, their sample quality is superior to likelihood-based methods such as autoregressive models~\cite{salimans2017pixelcnn++,oord2016conditional}, flow models~\cite{dinh2014nice,dinh2016density}, and variational autoencoders (VAEs)~\cite{kingma2013auto}. Second, because of the stable training, scaling and applying diffusion models to new domains and datasets is much easier than generative adversarial networks (GANs)~\cite{goodfellow2014generative}, which rely on unstable adversarial training. 

Moreover, pre-trained diffusion models are surprisingly easy to apply to downstream image synthesis tasks.
Recent works~\cite{choi2021ilvr,meng2021sdedit} have demonstrated that pre-trained diffusion models can easily adapt to image translation and image editing. Compared to GAN-based methods~\cite{isola2017image,zhu2017unpaired,abdal2019image2stylegan}, they adapt a single diffusion model to various tasks without task-specific training and loss functions. They also show that diffusion models allow stochastic (one-to-many) generation in those tasks, while GAN-based methods suffer deterministic (one-to-one) generations~\cite{isola2017image}. 


\subsection{Redesigning Training Objectives}
Recent works \cite{kingma2021variational,song2021maximum,vahdat2021score} introduced new training objectives to achieve state-of-the-art likelihood. However, their objectives suffer from degradation of sample quality and training instability, therefore rely on importance sampling~\cite{song2021maximum,vahdat2021score} or sophisticated parameterization~\cite{kingma2021variational}. Because the likelihood focuses on fine-scale details, their objectives impede understanding global consistency and high-level concepts of images. For this reason, \cite{vahdat2021score} use different weighting schemes for likelihood training and FID training. Our P2 weighting provides a good inductive bias for perceptually rich contents, allowing the model to achieve improved sample quality with stable training. 



