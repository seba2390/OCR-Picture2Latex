\section{Introduction}
\label{sec:intro}

Comparison-based algorithms have been thoroughly studied in computer
science. This includes algorithms for problems such as
\textsc{Minimum}, \textsc{Median}, \textsc{Sorting},
\textsc{Searching}, \textsc{Dictionaries}, \textsc{Priority Queues},
and many others. The cost measure analyzed is almost always the total
number of comparisons performed by the algorithm, either in the worst
case or the expected case. Recently, another type of cost measure has
been introduced~\cite{afshani:fragile-ESA19} which instead considers
how many comparisons each individual element is subjected during the
course of the algorithm. In~\cite{afshani:fragile-ESA19}, a
comparison-based algorithm is defined to have {\em \frag} $\f{n}$ if
each individual input element participates in at most $\f{n}$
comparisons. The \frag of a computational problem is the best possible
\frag of any comparison-based algorithm solving the problem.
\stefan{Define out-\frag complexity}

This cost measure has both theoretical and practical motivations. On
the theoretical side, it raises the question of to what extent the
comparisons necessary to solve a given problem can be spread evenly
across the input elements. On the practical side, this question is
relevant in any real world situation where comparisons involve some
amount of destructive impact on the elements being compared (hence the
name of the cost measure). As argued in~\cite{afshani:fragile-ESA19},
one example of such a situation is ranking of any type of consumable
objects (wine, beer, food, produce), where each comparison reduces the
available amount of the objects compared. Here, an algorithm like
\textsc{QuickSort}, which takes a single object and partitions the
whole set with it, may use up this pivot element long before the
algorithm completes. Another example is sports, where each comparison
constitutes a match and takes a physical toll on the athletes
involved. If a comparison scheme subjects one contestant to many more
matches than others, both fairness to contestants and quality of
result are impacted---finding a winner may not be very useful if this
winner has a high risk of being injured in the process. The negative
impact of comparisons may also be of non-physical nature, for instance
when there is a privacy risk for the elements compared, or when bias
grows if few elements are used extensively in comparisons.

% \begin{definition}
%   We say that a comparison-based algorithm $\A$ has {\em \frag}
%   $\f{n}$ if each individual input element participates in at most
%   $\f{n}$ comparisons.  We also say that $\A$ has {\em \work} $\w{n}$
%   if it performs at most $\w{n}$ comparisons in total.  We say that a
%   particular element~$e$ has \frag $f_e(n)$ in~$A$ if $e$ participates
%   in at most $f_e(n)$ comparisons.
% \end{definition}

%We define the \emph{\frag} of
%comparison-based algorithms as the maximal number of comparisons any
%individual element takes part in, and we give a number of upper and
%lower bounds on the \frag for fundamental problems.

\subsection{Previous work}
In~\cite{afshani:fragile-ESA19}, the study of algorithms' \frag was
initiated and a number of upper and lower bounds on the \frag for
fundamental problems was given. The problems studied included
\textsc{Minimum}, the \textsc{Selection}, \textsc{Sorting}, and
\textsc{Heap Construction}, and both deterministic and randomized
settings were considered.
%
In the deterministic setting, \textsc{Minimum} was shown to have \frag
$\Omega(\log n)$ and \textsc{Sorting} to have \frag $O(\log n)$. Since
\textsc{Sorting} can solve \textsc{Selection}, which can solve
\textsc{Minimum}, the \frag of all three problems is $\Theta(\log
n)$. The authors then consider randomized algorithms, as well as a
more fine-grained notion of \frag, where the objective is to protect
selected elements such as the minimum or median (i.e., the element to
be returned by the algorithm), possibly at the expense of the
remaining elements.
%In this paper, we will denote this notion as output \frag.
Among other results, it is shown in~\cite{afshani:fragile-ESA19} that
\textsc{Minimum} can be solved incurring expected $O(1)$ comparisons
on the minimum element itself, at a price of incurring expected
$O(n^{\varepsilon})$ on each of the rest. Also a more general
trade-off between the two costs is shown, as well as a close to
matching lower bound. For \textsc{Selection}, similar results are
given, including an algorithm incurring expected $O(\log \log n)$
comparisons on the returned element itself, at a price of incurring
expected $O(\sqrt{n})$ on each of the rest.

An earlier body of work relevant for the concept of \frag is the study
of sorting networks, started in 1968 by Batcher~\cite{Batc68}. In
sorting networks, and more generally comparator networks, the notion
of depth (the number of layers, where each layer consists of
non-overlapping comparators) and size (the total number of
comparators) correspond to fragile complexity and standard worst case
complexity, respectively, in the sense that a network with
depth~$f(n)$ and size~$s(n)$ can be converted into a comparison-based
algorithm with fragile complexity~$f(n)$ and standard
complexity~$s(n)$ by simply simulating the network.

Batcher, as well as a number of later
authors~\cite{Dowd-Perl-Rudolph-Saks/89,Parberry/92,IPL::ParkerP1989,books/garland/Pratt72}, gave sorting networks with
$\Oh(\log^2 n)$ depth and $\Oh(n\log^2 n)$ size.
% based on clever variants of the \textsc{MergeSort} paradigm.
For a long time it was an open question whether better results were
possible. In 1983, Ajtai, Koml{\'o}s, and
Szemer{\'e}di~\cite{aks-halvers,aks} answered this in the affirmative
by constructing a sorting network of $\Oh(\log n)$ depth and
$\Oh(n\log n)$ size. This construction is quite complex and involves
expander graphs~\cite{HooLinWig06,journals/fttcs/Vadhan12}. It was
later modified by
others~\cite{Chvatal/92,conf/stoc/Goodrich14,Paterson/90,journals/algorithmica/Seiferas09},
but finding a simple, optimal sorting network, in particular one not
based on expander graphs, remains an open problem.  Comparator
networks for other problems, such as selection and heap construction
have also been studied
~\cite{alekseev:selection-69,brodal:heap-98,Jimbo,Pippenger91,Yao.merging}.

While comparator networks are related to \frag in the sense that
results for comparator networks can be transfered to the \frag setting by
simple simulation, it is demonstrated in~\cite{afshani:fragile-ESA19}
that the two models are not equivalent: there are problems where one
can construct fragile algorithms with the same \frag, but with
strictly lower standard complexity (i.e., total number of comparisons)
than what is possible by simulation of comparison networks. These
problems include \textsc{Selection} and \textsc{Heap Construction}.

\subsection{Our Contribution}
In many settings, the classical worst case complexity of
comparison-based algorithms can be lowered if additional information
on the input is known. For instance, sorting becomes easier than
$\Theta(n \log n)$ if the input is known to be close to
sorted. Another example is searching in a sorted set of elements,
which becomes easier than $O(\log n)$ if we know an element of rank
close to the element searched for. Such algorithms may be described as
\emph{adaptive} to input restrictions (using the terminology from the
sorting setting~\cite{DBLP:journals/csur/Estivill-CastroW92}). Given
that the total number of comparisons can be lowered in such
situations, the question arises whether also reductions in the \frag
are possible under these types of input restrictions.

In this paper, we expand the study of the \frag of comparison-based
algorithms to consider the impact of a number of classic input
restrictions. We show that searching for the predecessor in a sorted
array has \frag $\Theta(\log k)$, where $k$ is the rank of the query
element, both in a randomized and a deterministic setting. For
predecessor searches, we also show how to optimally reduce the
amortized \frag of the elements in the array. We also prove the
following results: Selecting the $k$th smallest element has expected
\frag $O(\log\log k)$ for the element selected.  Deterministically
finding the minimum element has \frag $\Theta(\log(\INV))$ and
$\Theta(\log(\RUNS))$, where $\INV$ is the number of inversions in a
sequence and $\RUNS$ is the number of increasing runs in a sequence.
Deterministically finding the median has \frag
$O(\log(\RUNS) + \log\log n)$ and $\Theta(\log (\INV))$.
Deterministic sorting has \frag $\Theta(\log (\INV))$ but it has \frag
$\Theta(\log n)$ regardless of the number of runs.
