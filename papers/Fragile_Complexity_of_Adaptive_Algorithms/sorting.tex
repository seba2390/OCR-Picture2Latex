\section{Sorting}
\label{sec:sorting}

When the input is known to have some amount of existing order, sorting
can be done faster than $\Theta(n \log n)$. Quantifying the amount of
existing order is traditionally done using measures of
disorder~\cite{DBLP:journals/csur/Estivill-CastroW92}, of which \INV{}
and \RUNS{} are two classic examples.\footnote{The measure \INV{} is
  defined as the total number of inversions in the input, where each
  of the ${n\choose 2}$ pairs of elements constitute an inversion if
  the elements of the pair appear in the wrong order. The measure
  \RUNS{} is defined as the number of runs in the input, where a run is a maximal consecutive ascending subsequence.}
%
A sorting algorithm is adaptive to a measure of disorder if it
is faster for inputs with a smaller value of the measure. For the
above measures, run times of $O(n \log (\INV/n))$ and
$O(n \log (\RUNS))$ can be achieved. These results are best possible
for comparison-based sorting, by standard information-theoretic
arguments based on the number of different inputs having a given maximal
value of the measure.

The fact~\cite{aks,afshani:fragile-ESA19} that we can sort all inputs
in $\Theta(n \log n)$ time and $\Theta(\log n)$ \frag can be
viewed as being able to distribute the necessary comparisons evenly
among the elements such that each element takes part in at most
$\Theta(\log n)$ comparisons.
% the
% worst case and the average case number of comparison that an element
% takes part in become the same, namely $\log n$.
%
Given the running times for adaptive sorting stated above, it is
natural to ask if for an input with a given value of \INV{} or
\RUNS{} we are able to sort in a way that distributes the necessary
comparisons evenly among the elements, i.e., in a way such that each
element takes part in at most $O(\log (\INV))$ or $O(\log (\RUNS))$
comparisons, respectively.
% the worst case and the average case
% number of comparison that an element takes part in are the same,
% namely $\log (\INV/n)$ and $\log (\RUNS)$, respectively.
In short, can we sort in \frag $O(\log (\INV))$ and
$O(\log (\RUNS))$?  Or more generally, what problems can we solve with
\frag adaptive to \INV{} and \RUNS{}? In this section, we study
the \frag of deterministic algorithms for \textsc{Minimum},
\textsc{Median}, and \textsc{Sorting} and essentially resolve their
adaptivity to \INV{} and \RUNS{}.

\begin{theorem}
\textsc{Minimum} has \frag $\Theta(\log (\RUNS))$.
\end{theorem}
\begin{proof}
%\riko{also true also for expected  / WHP by exchanging the log}
For the upper bound: identify the runs in $O(1)$ \frag by a scan of the input. Then, use a
tournament on the heads of the runs since the minimum is the minimum of the heads of the runs. 
For the lower bound: apply the logarithmic lower
bound for \textsc{Minimum}~\cite{afshani:fragile-ESA19} on the heads
of the runs. \qed
\end{proof}

\begin{theorem}
\textsc{Sorting} has \frag $\Theta(\log n)$, no matter what value of
\RUNS{} is assumed for the input.
\end{theorem}
\begin{proof}
The upper bound follows from general sorting. For the lower bound: the input
consisting of a run~$R$ of length $n-1$ and one more element~$x$ has
$\RUNS = 2$, but $\log n$ comparisons on $x$ can be forced by an
adversary before the position of $x$ in $R$ is determined.\qed
\end{proof}

\begin{theorem}
\textsc{Median} has \frag $O(\log (\RUNS) + \log\log n)$.
\end{theorem}
% {\color{gray} [Very high-level: the algorithm is a mix of
%   our deterministic median algorithm from the ESA 2019 paper and
%   methods inspired by the classic median-of-medians algorithm.]}
\begin{proof}
Assume that $4 \cdot \RUNS \cdot \log n < n/2$, since otherwise the
claimed \frag is $O(\log n)$ for which we already have a median
algorithm~\cite{afshani:fragile-ESA19}. Consider the rank space
$[1,n]$ (i.e., the indices of the input elements in the total sorted
order) of the input elements and consider the rank interval $[a,b]$
around the median defined by $a = n/2 - 4 \cdot \RUNS \cdot \log n$
and $b = n/2 + 4 \cdot \RUNS \cdot \log n$. In each step of the
algorithm, elements are removed in two ways: type~A removals and
type~B removals. A removal of type~A is a balanced removal, where a
number of elements with ranks in $[1,a-1]$ are removed and the same
number of elements with ranks in $[b+1,n]$ are removed. The key
behind the type~A removal is that the median element of the set
prior to the removal is the same as the median of the set after the
removal, if the median prior to the removal has a rank in $[a,b]$.

A removal of type~B is a removal of elements with arbitrary
rank. However, the total number of elements removed by type~B removals
is at most $7 \cdot \RUNS \cdot \log n$ during the\ entire run of the
algorithm. Hence, repeated use of type~A and type~B removals will
maintain the invariant that the median of the remaining elements has a
rank in $[a,b]$.

We now outline the details of the algorithm. The first step is to
identify all the runs in $O(1)$ \frag by a scan. A run will be
considered {\em short} if the run consists of fewer than
$7 \cdot \log n$ elements and it will be considered {\em long} otherwise. A
step of the algorithm proceeds by first performing a type~B removal
followed by a type~A removal.  A type~B removal consists of removing
all short runs that are present. The short runs that are removed will
be reconsidered again at the end once the number of elements under
consideration by the algorithm is less than
$64 \cdot \RUNS \cdot \log n$.

Once a type~B removal step is completed, only long runs remain under
consideration.  We now describe a type~A removal step. Note that a
long run may become short after a type~A removal step, in which case
it will be removed as part of the next type~B removal step. Each run
can become short (and be removed by a type~B removal) only once, hence
the total number of elements removed by type~B removals will be at
most $7 \cdot \RUNS \cdot \log n$, as claimed.

In the following, let $\mathcal{N}$ denote the elements under
consideration just before a type~A removal (i.e., the elements of the
remaining long runs), and let $N = |\mathcal{N}|$. The algorithm stops
when $N \le 64 \cdot \RUNS \cdot \log n$.

To execute the type~A removal step, the algorithm divides each long
run $R$ into blocks of length $\log n$. The blocks of a run are
partitioned by a {\em partitioning} block. The partitioning block has
the property that there are at least $|R|/7$ elements of $R$ whose
values are less than the values in the partitioning block and at least
$5|R|/7$ elements of $R$ whose value are greater than the elements in
the partitioning block. One element~$x_R$ is selected from the
partitioning block. We will refer to this element as a {\em
partitioning} element.
These partitioning elements are then sorted
%$\{x_R \mid \mbox{$R$ is a long run} \}$ of such elements
into increasing order, which incurs a cost of $O(\log (\RUNS))$ \frag
on each of the partitioning elements.  The runs are then arranged in
the same order as their partitioning elements.  Label this sequence of
runs as $R_1, R_2, \dots, R_k$, and let $t$ be the largest index such
that $\sum_{i=1}^{t-1} |R_i| < N/8.$

Since the partitioning element $x_{R_t}$ is smaller than all the elements in the blocks with values greater than their
respective partitioning blocks
in $R_t, R_{t+1},$ $\dots, R_k$, we have that $x_{R_t}$ is smaller than $(7/8)(5N/7) = 5N/8$ of the
remaining elements. Hence in rank it is at least $N/8$ below the median
of the remaining elements. By the invariant on the position in rank
space of this median and the fact that $N > 64 \cdot \RUNS \cdot \log n$, we note
that $x_{R_t}$ has a rank below~$a$. We also note that all the elements
below the partitioning blocks in $R_1, R_{2}, \dots, R_t$ have value less than 
$x_{R_t}$. This constitutes at least $(1/8)(N/7) = N/56$
elements in~$\mathcal{N}$ with rank below~$a$. Therefore, 
we can remove $N/56$ elements with rank below~$a$. In a similar manner, we can find
at least $N/56$ elements in~$\mathcal{N}$ with rank
above~$b$. Removal of these $2N/56 = N/28$ elements
in~$\mathcal{N}$ constitutes a type~A removal step.

Since the number of elements under consideration, i.e. $N$, 
decreases by a constant factor at each step, the algorithm
performs $O(\log n)$ type~A and type~B removal steps before we have
$N \le 64 \cdot \RUNS \cdot \log n$.  Since each block under 
consideration in a type~A removal step has size $\log n$, we can guarantee that each element in a
partitioning block
only needs to be selected as a partitioning element $O(1)$ times. This implies
that a total cost of $O(\log (\RUNS))$ \frag is incurred on each
element once we have that $N \le 64 \cdot \RUNS \cdot \log n$.
%choose a new element $x_R$ from a block after that and still be able
%to perform the $O(\log n)$ steps of the algorithm.\footnote{Instead of
%  having the block system, we can also just remove the elements being
%  sorted in a step. Then the definition of a short run will just be a
%  length below eight. The analysis ends up similar, as we for each of
%  the \RUNS{} runs can have $\log n$ elements removed like that.}

We now describe the final step of the algorithm.
At this point, the algorithm combines the last $\mathcal{N}$ elements
with all the short runs removed during its execution up to this point, forming the
set~$\mathcal{S}$. This set is the original elements
subjected to a series of type~A removals, each of which are balanced
and outside the rank interval $[a,b]$. Hence, the median
of~$\mathcal{S}$ is the global median. As
$|\mathcal{S}| = O(\RUNS \cdot \log n)$, we can find this median in
$O(\log(\RUNS \cdot \log n)) = O(\log (\RUNS) + \log\log n)$
\frag~\cite{afshani:fragile-ESA19}, which dominates the total
\frag of the algorithm.

We note that for $\RUNS = 2$, we can improve the above result to
$O(1)$ \frag as follows. Let the two runs be $R_1$ and $R_2$, with
$|R_1| \le |R_2|$.  Compare their middle elements $x$ and $y$ and
assume \ $x \le y$. Then the elements in the first half of $R_1$ are
below $n/2$ other elements, and hence are below the median. Similarly,
the elements in the last half of $R_2$ are above the median. Hence, we
can remove $|R_1|/2$ elements on each side of the median by removing
that many elements from one end of each run. The median of the
remaining elements is equal to the global median. By recursion, we in
$\log |R_1|$ steps end up with $R_1$ reduced to constant length. Then
$O(1)$ comparisons with the center area of $R_2$ will find the median.
Because both runs lose elements in each recursive step, both $x$ and
$y$ will be new elements each time. The total \frag of the
algorithm is therefore $O(1)$. \qed
\end{proof}

\begin{theorem}
\textsc{Minimum} has \frag $\Theta(\log (\INV))$.
\end{theorem}
\begin{proof}
Lower bound: For any $k$, consider the instances composed of
$\sqrt{k}$ elements in random order followed by $n - \sqrt{k}$
larger elements in sorted order. These instances have $\INV \le
k$. Finding the minimum is equal to finding the minimum on the first
$\sqrt{k}$ elements, which has a lower
bound~\cite{afshani:fragile-ESA19} of
$\log \sqrt{k} = \Omega(\log k)$ on its \frag.

For the upper bound, we will remove a subset~$I$ of size $O(\INV)$
which leaves a single sorted run~$R$. We can find the mininum in~$I$
in $O(\log(\INV))$ \frag by a tournament tree, which can then be
compared to the head of~$R$ for the final answer.

We find~$I$ and~$R$ in $O(1)$ \frag during a scan of the input as
follows, using~$R$ as a stack. For each new element~$e$ scanned, we
compare it to the current top element~$f$ of~$R$. If $e$ is larger, we
push~$e$ to the top of~$R$. If~$e$ is smaller, it forms an inversion
with~$f$, and we include $e$ in $I$. We also put a mark on~$f$. If an
element on the stack gets two marks, we pop it, include it in~$I$,
remove one of its marks (which will account for its inclusion in~$I$)
and move the second mark to the new top element~$f'$ of~$R$. If $f'$
now has two marks, this process continues until an element with only a
single mark is created (or the stack gets empty). An element is
compared with exactly one element residing earlier in the input (when
the element is scanned). To count comparisons with elements residing
later in the input, call such a comparison large or small, depending
on whether the other element is larger or smaller. It is easy to see
that elements on the stack always have between zero and one marks,
that an element with zero marks has participated in one large
comparison, and that an element with one mark has either participated
in at most two larger comparisons or one smaller comparison. Hence,
the \frag of the process is~$O(1)$. By the accounting scheme, $I$
is no larger than $\INV$ plus the number of marks, which is also at
most~$\INV$. \qed
\end{proof}

\begin{theorem}
\textsc{Median} has \frag $\Theta(\log (\INV))$.
\end{theorem}
\begin{proof}
As \textsc{Median} solves \textsc{Minimum} via padding with $n$
elements of value~$-\infty$, the lower bound follows from the lower
bound on~\textsc{Minimum}. For the upper bound, find~$R$ and $I$ as in
the upper bound for \textsc{Minimum}, sort~$I$ in
\frag~$O(\log(\INV))$ and use the algorithm for \textsc{Median}
for $\RUNS = 2$. \qed
\end{proof}

\begin{theorem}
\textsc{Sorting} has \frag $\Theta(\log (\INV))$.
\end{theorem}
\begin{proof}
The lower bound follows from the lower bound on~\textsc{Minimum}.
For the upper bound, find~$R$ and $I$ as in the upper bound for
\textsc{Minimum} and let each element recall its position in the
input. Divide the sorted sequence~$R$ into contiguous blocks of size
$|I|$ and let $R_i$ be the set of $i$'th elements of all blocks.
With the $i$'th element of~$I$ we perform an exponential search on
$R_i$, starting from the block where the element's position in the
input is. If the search moves a distance~$k$, the element from~$I$
participated in at least~$k$ inversions in the input, so
$k \le \INV$ and hence the incurred \frag for the element
is~$O(\log k) = O(\log(\INV))$. A \frag of~$O(1)$ is incurred on the
elements of~$R$, as each~$R_i$ is used once. After this, each
element from~$I$ knows its position in~$R$ within a window of
size~$|I|$. If the window of an element and a block overlaps, we
call the element and the block associated. Each block of~$R$ is
associated with at most~$|I| = O(\INV)$ elements, and each element
is associated with at most two blocks. For each block in turn, we
now sort its associated elements and merge them into the block
(except for tail elements overlapping the next block). The sorting
incurs~$O(\log(\INV))$ \frag, as does the merging if we use
exponential merging~\cite{afshani:fragile-ESA19}. We remove all
inserted elements from any association with the next block. Then we
continue with the next block. \qed
\end{proof}
