
\newcommand{\lrp}[1]{\left( #1 \right)}
\newcommand{\lrc}[1]{\lceil #1 \rceil}

\section{Searching}
\label{sec:searching}

The problem of predecessor searching is, given a sorted array~$A$ with
$n$ elements, $A[0]..A[n-1]$, answer queries of the form ``What is the
index of the largest element in $A$ smaller than $x$?''  Binary search
is the classic solution to the predecessor search problem. It achieves
$\log n$ \frag for~$x$, and \frag at most one for each element of~$A$.
We can improve on this in two ways.  The first is where we try to keep
the \frag of~$x$ small, which is possible if we know something about
the rank of~$x$. We show that the optimal dependency on
the rank of~$x$ is $\Theta(\log k)$ where $k$ is its rank, both for deterministic and
randomized algorithms.\footnote{For simplicity of exposition, we assume the
  rank is close to one, but the result clearly holds for rank distance
  to other positions in~$A$.}
%% Rolf: All our results are stated for sorted arrays, so I removed
%% this:
% In that setting we can assume that the other elements are sorted
% because we don't care about their \frag.
The second setting is where we are concerned with the
\frag of the other elements.  While there is no way to improve a
single search, classical deterministic binary search will always do
the first comparison with the same element (typically the median).
Hence we consider deterministic algorithms that improve the amortized
\frag of any element of the array~$A$ over a sequence of searches.

\subsection{Single search}
%\subsection{The rank of element $x$}

\begin{theorem}\label{thm:rank}
  Let $A$ be a sorted array.
  Determining the predecessor of an element $x$ within~$A$ has \frag $\Theta(\log k)$ for deterministic and randomized algorithms, where $k$ is the rank of $x$ in~$A$. 
\end{theorem}
\begin{proof}
The upper bound follows from standard exponential
search~\cite{conf/stoc/Fredman75}: We compare~$x$ to
$A[2],A[4],A[8],\ldots$ until we find the smallest~$i$ such that
$x<A[2^i]$.  We perform a binary search with the initial interval
$[2^{i-1},2^i]$.  If $x$ has the predecessor $A[k]$, this requires
$O(\log k)$ comparisons.

% First, we show the upper bound.  We can be sensitive to the rank~$k$
% of~$x$ by performing an exponential search, i.e., we compare~$x$ to
% $A[2],A[4],A[8],\ldots$ until we find the smallest~$i$ such that
% $x<A[2^i]$.  We perform a binary search with the initial interval
% $[2^{i-1},2^i]$.  If $x$ has the predecessor $A[k]$, this requires
% $O(\log k)$ comparisons involving~$x$.

For the lower bound assume we have a deterministic algorithm to
determine the rank of an element~$x$.  If the answer of the algorithm
is~$k$, let $B_k$ be the bit-string resulting from concatenating the
sequence of the outcomes of the comparisons performed by the
algorithm, the $i$-th bit $B_k[i]=0$ for $x<A[k]$, otherwise it is~1.
Because the algorithm is deterministic and correct, all these
bit-strings are different and they are a code for the numbers
$1,\ldots,n$.  Now, for any~$k$, consider the uniform distribution on
the numbers~$0,\ldots,k-1$, a distribution with entropy~$\log k$.  By
Shannon's source coding theorem, the average code length must be at
least $\log k$, i.e., $\sum_{i=0}^{k-1} |B_i| \ge k\log k$.

For a contradiction, assume there would be an algorithm with $|B_i|\le \log i$ (the binary logarithm itself). 
Then for $k> 1$, $\sum_{i=0}^{k-1} |B_i| < k\log k$, in contrast to Shannon's theorem.

The bound $\sum_{i=0}^{k-1} |B_i| \ge k\log k$ also holds for randomized algorithms if the queries are drawn uniformly from~$[1,\ldots,k]$, following Yao's principle:
Any randomized algorithm can be understood as a collection of deterministic algorithms from which the 'real' algorithm is drawn according to some distribution. 
Now each deterministic algorithm has the lower bound, and the average number of comparisons of the randomized algorithm is a weighted average of these. 
Hence the lower bound also holds for randomized algorithms.\qed 
\end{proof}
%
\subsection{Sequence of searches}

As mentioned, in binary search, the median element of the array will be compared with every query element. Our goal here is to develop a search strategy so as to ensure that data far away from the query will only infrequently be involved in a comparison.
Data close to the query must be queried more frequently. While we prove this formally in Theorem~\ref{t:lb}, it is easy to see that predecessor and successor of a query must be involved in comparisons with the query in order to answer the query correctly. 

\begin{theorem} \label{t:search}
There is a search algorithm that 
for any sequence of predecessor searches $x_1, x_2, \ldots, x_m$ in a sorted array $A$ of size $n$
the number of comparisons with any $y\in A$ is $O\lrp{ \log n+ \sum_{i=1}^m \frac{1}{d(x_i,y)}}$ 
 where $d(x,y)$ is the number of elements between $x$ and $y$ in $A$, inclusive. 
The runtime is $O(\log n)$ per search and the structure uses $O(n)$ bits of additional space.
\end{theorem}

\def\offset{\text{\sl offset}}
\begin{proof}
We use the word interval to refer to a contiguous range of $A$; when we index an interval, we are indexing $A$ relative to the start of the interval.
%
Call an aligned interval $I$ of $A$ of rank $i$ to be $( A[k\cdot 2^i]\ldots A[(k+1) \cdot 2^i])$ for some integer $k$, i.e., the aligned intervals of $A$ are the dyadic intervals of $A$. There are $O(n)$ aligned intervals of $A$, and for each aligned interval $I$ of rank $i$ we store an offset $I.\offset$ which is in the range $[0,2^i)$, and it is initialized to 0.

The predecessor search algorithm with query $x$ is a variant of recursive binary search, where at each step an interval $I_q$ of $A$ is under consideration, and the initial recursive call considers the whole array $A$. Each recursive call proceeds as follows: Find the largest $i$ such that there are at least three rank-$i$ aligned intervals in $I_q$, use $I_m$ to denote  the middle such interval (or an arbitrary non-extreme one if there are more than three), and we henceforth refer to this recursive call as a rank-$i$ recursion. Compare $I_m[I_m.\offset]$ with $x$, and then increment $I_m.\offset$ modulo $2^i$. Based on the result of the comparison, proceed recursively as in binary search.
The intuition is by moving the offset with every comparison, this prevents a single element far from the search from being accessed too frequently.
We note that the total space used by the offsets is $O(n)$ words, which can be reduced to $O(n)$ bits if the offsets are stored in a compact representation.

\pagebreak[3]
\noindent First, several observations:\nopagebreak
%
\begin{compactenum}
\item \label{p1} In a rank-$i$ recursion, $I_q$ has size at least $3 \cdot 2^i$ (since there must be at least three rank-$i$, size $2^i$ aligned intervals in $I_q$) and at most $8 \cdot 2^i$, the latter being true as if it was this size there would be three rank-$i+1$ intervals in $I_q$, which would contradict $I_m$ having rank $i$. 

\item If $I_q$ has size $k$ then if there is a recursive call, it is called with an interval of size at most~$\frac{7}{8}k$. This is true by virtue of $I_m$ being rank-$i$ aligned with at least one rank-$i$ aligned interval on either side of $I_m$ in $i$. Since $I_q$ has size at most $8\cdot 2^i$, this guarantees an eighth of the elements of $I_q$ will be removed from consideration as a result of the comparison in any recursive call.

\item \label{p3} From the previous two points, one can conclude that for a given rank $i$, during any search there are at most 7 recursions with of rank $i$. This is because after eight recursions any rank-$i$ search will be reduced below the minimum for rank $i$:  $8 \cdot 2^i \cdot \lrp{\frac{7}{8}}^8 <3 \cdot 2^i$.
\end{compactenum}

For the analysis, we fix an arbitrary element $y$ in $A$ and use the potential method to analyse the comparisons involving $y$. 
Let $\mathcal{I}_y=\{I_y^1,I_y^2\ldots\}$ be the $O(\log n)$ aligned intervals that contain $y$, numbered such that $I_y^i$ has rank $i$.
Element $y$ will be assigned a potential relative to each aligned interval $I_y^i\in \mathcal{I}_y$ which we will denote as $\varphi_y(I_y^i)$. 
Let $t_y(I_y^i)$ be number of times $I_y^i.\offset$ needs to be incremented before $I_y^i[I_y^i.\offset]=y$, which is in the range $[0,2^i)$. 
The potential relative to $I^i_y$ is then defined as $\varphi_y(I_y^i)\coloneqq \frac{2^i-t_y(I^i_y)}{2^i}$, and the potential relative to $y$ is defined to be the sum of the potentials relative to the intervals in $\mathcal{I}_y$: $\varphi_y \coloneqq \sum_{I^i_y\in\mathcal{I}_y} \varphi_y(I_y^i)$.

How does $\varphi_y(I_y^i)$ change during a search? 
First, if there is no rank-$i$ recursive call during the search to an interval containing $y$, it does not change as $I_y^i.\offset$ is unchanged.
Second, observe from point \ref{p3} that a search can increase $\varphi_y (I_y^i)$ by only $\frac{7}{2^i}$.
Furthermore if $y$ was involved in a comparison during a rank-$i$ recursion, there will be a loss of $1-\frac{1}{2^i}$ units of potential in $\varphi_y(I_y^i)$ as the offset of $I_y^i$ changes from 0 to $2^i-1$.

Following standard potential-based amortized analysis, the amortized number of comparisons involving $y$ during a search is the actual number of comparisons (zero or one) plus the change in the potential $\varphi_y $. 
Let $i_{\min}$ be the smallest value of $i$ for which there was a rank-$i$ recursion that included $y$. As the maximum gain telescopes, the potential gain is at most $\frac{14}{2^{i_{\min}}}$, minus 1 if $y$ was involved in a comparison. 
Thus the amortized number of comparisons with $y$ in the search is at most $\frac{14}{2^{i_{\min}}}$.

Observe that if there was a rank-$i$ recursion that included $y$, that $d(x,y)$ is at most $8\cdot 2^i$ by point~\ref{p1}.
This gives $d(x,y)\leq 8\cdot 2^i \leq 8\cdot 2^{i_{\min}}$.
Thus the amortized cost  can be restated as being at most
$ \frac{14}{2^{i_{\min}}}\leq \frac{112}{d(x,y)}$. 

To complete the proof, the total number of comparisons involving $y$ over a sequence of searches is the sum of the amortized costs plus any potential loss. As the potential $\varphi_y$ is always nonnegative and at most $\lrc{\log n}$ (1 for each $\varphi_y(I^i_y)$), this gives the total cost as $O\lrp{ \log n+ \sum_{i=1}^m \frac{1}{d(x_i,y)}}$. \qed
\end{proof}

Note that the above proof was designed for easy presentation and not an optimal constant. Also note that this theorem implies that if the sequence of searches is uniformly random, the expected fragility of all elements is $O(\frac{\log n}{n})$, which is asymptotically the best possible since random searches require $\Omega(\log n)$ comparisons in expectation.

\subsection{Lower Bounds.} It is well-known that comparison-based searching requires $\Omega(\log n)$ comparisons per search. In our method, taking a single search $x_i$ summing over the upper bound on amortized cost of the number of comparisons with $y$, $\frac{42}{d(x_i,y)}$, for all $y$ yields a harmonic series which sums to $O(\log n)$. But we can prove something stronger:

\begin{theorem} \label{t:lb}
There is a constant $c$ such that if a predecessor search algorithm has an amortized number of comparisons of $f(d(x_i,y))$ for an arbitrary $y$ for every sequence of predecessor searches $x_1, x_2, \ldots x_m$, then $\sum_{k=1}^p f(k) \geq c \log p$ for all $p \leq n$.
\end{theorem}

\begin{proof}
This can be seen by looking at a random sequence of predecessor searches for which the answers are uniform among $A[0]\ldots A[p-1]$, if the theorem was false, similarly to the proof of Theorem~\ref{thm:rank}, this would imply the ability to execute such a sequence in $o(\log p)$ amortized time per operation. \qed
\end{proof}
\riko{perhaps we could do this even nicer}

This shows that a flatter asymptotic tradeoff between $d(x_i,y)$ and the amortized comparison cost is impossible; more comparisons are needed in the vicinity of the search than farther away.
For example, a flat amortized number of comparisons of $\frac{\log n}{n}$ for all elements would sum up to $O(\log n)$ amortized comparisons over all elements, but yet would violate this theorem. 

\subsection{Extensions.}

Here we discuss extensions to the search method above. We omit the proofs as they are simply more tedious variants of the above.

One can save the additional space used by the offsets of the intervals through the use of randomization. The offsets force each item in the interval to take its turn as the one to be compared with, instead one can pick an item at random from the interval. This can be further simplified into a binary search where at each step one simply picks a random element for the comparison amongst those (in the middle half) of the part of the array under consideration. 

To allow for insertions and deletions, two approaches are possible. The first is to keep the same array-centric view and simply use the packed-memory array \cite{DBLP:conf/icalp/ItaiKR81,DBLP:conf/sigmod/Willard86,DBLP:journals/iandc/Willard92} to maintain the items in sorted order in the array. This will give rise to a cost of $O(\log^2 n)$ time which is inherent in maintaining a dynamic collection of items ordered in an array \cite{DBLP:journals/siamcomp/BulanekKS15} (but no additional fragility beyond searching for the item to insert or delete as these are structural changes).
The second approach would be to use a balanced search tree such as a red-black tree \cite{DBLP:conf/focs/GuibasS78}. This will reduce the insertion/deletion cost to $O(\log n)$ but will cause the search cost to increase to $O(\log^2 n)$ as it will take $O(\log n)$ time to move to the item in each interval indicated by the offset, or to randomly choose an item in an interval. The intervals themselves would need to allow insertions and deletions, and would, in effect be defined by the subtrees of the red-back tree.
It remains open whether there is a dynamic structure with the fragility results of Theorem~\ref{t:search} where insertions and deletions can be done in $O(\log n)$ time.


