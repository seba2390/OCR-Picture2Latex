\section{Empirical Evaluation}
Given our focus on synthetic text generation, our primary interest lies in faithfully evaluating different synthetic text generation approaches under few-shot scenarios, rather than engaging in a ``state-of-the-art" race with general few-shot learning approaches (\ie we never claim that we achieve \emph{state-of-the-art} performance on these tasks). In this context, the following questions particularly intrigue us:
\textbf{RQ1}: How does {\ours} perform when compared with baselines on different downstream tasks?  
\textbf{RQ2}: How do different factors such as LLM generators and the size of synthetic data affect the performance of {\ours}? 
\textbf{RQ3}: How is the quality of the synthetic datasets generated by {\ours} and baselines?
These questions are addressed in Sec~\ref{sec:model_perf}, Sec~\ref{sec:ablation} and Sec~\ref{sec:quality_analysis}, respectively.

% GPT annotation~\citep{wang2021want,ding-etal-2023-gpt,gu2023distilling}

% In pursuing few-shot synthetic data generation, we aim to delve into practical applications in clinical scenarios. We committed to conducting a thorough evaluation that 
\subsection{Experiment Setup}
We conduct experiments in the few-shot settings with 5 examples available for each class. We employ ChatGPT~\citep{chatgpt} (\texttt{gpt-3.5-turbo-0301}) as the generator for both {\ours} and baselines for a fair comparison. The pre-trained PubMedBERT~\citep{gu2021domain} is then applied to fine-tune on the generated synthetic data for evaluation, where we consider both the \texttt{Base} and \texttt{Large} variants. See Appendix~\ref{sec:implementation_details} for implementation details.

\textbf{Datasets and Tasks.}
In our exploration of few-shot synthetic data generation, we undertake a comprehensive evaluation of \textbf{16 datasets} across a diverse array of tasks typically encountered in clinical NLP benchmarks~\citep{blue,fries2022bigbio}. Specifically, we consider 2 text classification, 3 relation extraction (RE), 3 natural language inference (NLI), 2 fact verification, 1 sentence similarity (STS), 4 NER, and 1 attribute extraction tasks. Please see Appendix~\ref{sec:dataset_description} for detailed dataset descriptions and the statistics of each dataset.

% The evaluation tasks and datasets are summarized in Table \ref{tab:datastats}. Note that the number of training samples indicates the size of the \textit{original} training set. Please see Appendix~\ref{sec:dataset_description} for detailed dataset descriptions.

\textbf{Baselines.}
We compare {\ours} with \textbf{9 baselines} in total, including
6 data augmentation methods and 3 LLM-based data generation techniques. The data augmentation models include Word Substitution~\citep{checklist}, Back Translation~\citep{uda}, Mixup~\citep{chen2020mixtext,seqmix}, Transformer~\citep{kumar2020data,melm}, LightNER~\citep{lightner}, and KGPC~\citep{chen2023few}.
% Note that LightNER and KGPC are designed specifically for NER tasks. Back Translation cannot be applied to NER and RE tasks as it's non-trivial to locate the related entities in the generated sentences. 
For LLM-based generation models, we consider ZeroGen~\citep{ye2022zerogen}, DemoGen~\citep{meng2023tuning,gpt3mix} and ProGen~\citep{ye2022progen} as representative methods. See Appendix~\ref{sec:baseline_details} for details.


\subsection{Model Performance with the Synthetic Data}
\label{sec:model_perf}
\input{sections/04-exp-table}
Table~\ref{tab:main-table} summarizes the experimental results on different datasets. We also conduct supervised learning on the original training data and the extracted few-shot examples, denoted as ``Supervised-Full" and ``Supervised-Few", respectively.
Due to space limits, we report the average performance over all datasets for each task, but provide the detailed results for each dataset in Tables~\ref{tab:single-sent}, \ref{tab:sent-pair}, \ref{tab:token-class} in Appendix~\ref{sec:more_experimental_results}. 
Based on the experimental results, we have the following findings:

\noindent $\diamond$ Our proposed approach, {\ours}, consistently outperforms the baselines across all tasks. The average performance gain over all \textit{main} metrics is 8.98\% at \texttt{Base} scale and 7.27\% at \texttt{Large} scale. In addition, methods utilizing LLMs have better performance than traditional data augmentation techniques, illustrating the capacity of LLMs to extract valuable information from limited examples. The performance gain of DemoGen and ProGen over ZeroGen further demonstrates the positive influence of few-shot examples on overall comprehension.

\noindent $\diamond$ In \textit{token classification tasks}, {\ours} performs better with KG compared to LLM. This improvement stems from the strong alignment between the task's target and the generated domain knowledge, where the extracted topics serve as direct labels for these datasets. The \textit{single-sentence} and \textit{sentence-pair tasks}, on the other hand, display an advantage for the LLM-based knowledge extraction. This can be attributed to two potential reasons: first, these tasks prioritize understanding entire sentences over specific terminologies, and some specialized terms might even impede LLM comprehension. Second, KGs may not always contain the required information. For example, in a RE task involving chemicals and proteins, some types of the relations are absent from the KG, thus the performance gain is rather limited.
% as this information is absent from the KG, we have to extract drug-gene relations as a substitute.

\noindent $\diamond$ Some data augmentation methods are task-specific, limiting their generalizability. For example, LightNER and KGPC are designed specifically for NER tasks. It is also non-trivial to apply Back Translation to NER or RE tasks, as it requires locating related entities in the generated sentence accurately.
In contrast, {\ours} is flexible and can be effectively applied to various tasks.


% \begin{itemize}[leftmargin=0.5cm]

% \item \textbf{In Terms of Model Types}\\
% $\diamond$ Methods utilizing LLMs consistently outperform traditional data augmentation techniques, underscoring the capacity of LLMs to extract valuable information from limited examples while preserving diversity in terms of content, structures, and lexicons. This observation is further supported by the findings presented in Section~\ref{sec:diversity_measures}.

% $\diamond$ The consistent superiority of DemoGen and ProGen over ZeroGen underscores the positive influence of few-shot examples on overall comprehension.

% $\diamond$ Our proposed approach, {\ours}, consistently outperforms the alternatives across all 16 datasets, showcasing relative improvements of up to 14.03\%.

% \item \textbf{In Terms of Clinical Task Types}\\
% $\diamond$ The performance gain in \textit{token classification tasks} is particularly significant, which predominantly involves the identification of disease or drug entities. The strong alignment between this task target and our generated prior knowledge guarantees the exceptional quality of the synthetic dataset created by {\ours}.

% % $\diamond$ \textit{single-sentence tasks}

% $\diamond$ The proposed model, {\ours}, is flexible and can be effortlessly and effectively applied to various tasks.

% \item \textbf{The Way of Clinical Knowledge Extraction}\\
% $\diamond$ {\ours} with KG or LLM demonstrates comparable performance on \textit{in-depth token classification tasks}, with a slight advantage for the KG-based model. This can be attributed to the enhanced diversity and comprehensiveness of entities derived from the knowledge graph.

% $\diamond$ When combined with LLMs, the proposed approach {\ours} demonstrates superior performance in \textit{sentence-pair tasks} compared to its performance with KG. The reason for this difference can be attributed to the inherent ability of LLM  to generalize from few-shot examples and to capture nuanced semantic relationships, which is beneficial in sentence-pair tasks that require understanding contextual information and complex inference.

% \end{itemize}

% LLM better than DA
% DemoGen and ProGen better than ZeroGen
% Ours the best (prior knowledge)
% KG and LLM
% NER knowledge related tasks -> more gain

% \paragraph{Single Sentence}

% \paragraph{Sentence Pair}

% \paragraph{Token Classification}
\subsection{Ablation and Parameter Studies}
\label{sec:ablation}
\textbf{Effect of Different LLM Generators.}
To investigate the impact of various LLMs on {\ours}, we leverage other models in the GPT-family as the text generator. Specifically, we utilize InstructGPT (\texttt{text-curie-001})~\citep{ouyang2022training} and GPT-4~\citep{gpt4}. Note that we only generate 500 samples in the GPT-4 setting due to budget constraints, but we provide the results of GPT-3.5 with same amount of synthetic samples for a fair comparison. 
From Figure~\ref{fig:generator} we observe that {\ours} generally outperforms the best baseline in all settings.
% except for the NCBI-Disease dataset. 
Additionally, we observe generally improved performance with larger models, as they often have better capabilities to follow our designed instructions for the given prompts. See Appendix~\ref{sec:add_ablation_para} for more figures.

\begin{figure}[t!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[HOC]{
            \includegraphics[width=0.48\textwidth]{figures/generator-HOC.pdf}
            \label{fig:generator-HOC}
        } \hspace{-3mm}
        \subfigure[MEDIQA-RQE]{
            \includegraphics[width=0.48\textwidth]{figures/generator-MEDIQA-RQE.pdf}
            \label{fig:generator-MEDIQA-RQE}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different generators at \texttt{Base}.}\label{fig:generator}}
    \end{minipage}%
    % \begin{minipage}{0.33\textwidth}
    %     \centering
    %     \vspace{3.5mm}
    %     \includegraphics[width=0.96\textwidth]{figures/size-HOC.pdf}
    %     \caption{\textit{Accuracy vs. Confidence score.}}
    %     \label{fig:confscore}
    % \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[HOC]{
            \includegraphics[width=0.5\textwidth]{figures/size-HOC.pdf}
            \label{fig:size-HOC}
        } \hspace{-6mm}
        \subfigure[MEDIQA-RQE]{
            \includegraphics[width=0.5\textwidth]{figures/size-Mediqa-rqe.pdf}
            \label{fig:size-Mediqa-rqe}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different proportion of data at \texttt{Base}.}\label{fig:size-synthetic}}
    \end{minipage}%
    \vspace{-0.5ex}
\end{figure}



\textbf{Effect of Size of Synthetic Data.}
In Figure~\ref{fig:size-synthetic} (and more in Appendix~\ref{sec:add_ablation_para}), we study the effect of the size of synthetic data. The result shows that {\ours} consistently outperforms the best baseline, using only around 10\% of the synthetic examples. This illustrates that incorporating domain knowledge and increasing the diversity of the prompts could be an effective way to improve the sample efficiency, and narrow the gap between the performance of synthetic and ground-truth dataset.
% \paragraph{How Many Clean Data Points is Synthetic Data Worth?}

\textbf{Comparison with few-shot inference via prompting ChatGPT.}
% How Does the Value of Synthetic Data Compare to Clean Data Points?
\input{sections/04-table-icl-inference}
As we employ ChatGPT as our backbone model for generating synthetic data, we also evaluate its ability for direct inference from the original training sets using few-shot in-context learning. Due to budget limits, we only run experiments on datasets with few testing samples for each task.  
As presented in Table~\ref{tab:gpt_inference}, {\ours} at PubMedBERT$_{\texttt{Large}}$ scale achieves better results on 5 out of 6 datasets than ChatGPT few-shot learning, which uses $\sim 530 \times$ more parameters. 
One exception is for PUBHEALTH, as it requires complex reasoning abilities that PubMedBERT$_{\texttt{Large}}$ may not fully possess. 
% Overall, the performance gain verifies that the synthetic dataset generated by our model effectively captures task-relevant information, enabling PubMedBERT$_{\texttt{Large}}$ to generalize well across these tasks.  
Overall, {\ours} offers cost-effective and time-efficient advantages. 
While it entails a one-time investment in both money and time for synthetic training data generation, subsequent prediction relying on a moderate-sized pretrained language model is much more efficient. 
Besides, the continued use of ChatGPT for inference on new testing data 
incurs ongoing time and financial costs, while our model requires zero additional costs for querying APIs. The price information is exhibited in Appendix \ref{sec:apd_cost}.

\textbf{Effect of Topic Extraction and Style Suggestion.}
We inspect different components of {\ours} in Table~\ref{tab:ablation}. It is observed that both Topics Extraction and Style Suggestion contribute to model performance as they enhance the relevance of generated samples to domain knowledge and introduce greater structural diversity. Different from other datasets, MEDIQA-RQE shows more performance gain incorporating writing style than topics. It is because NLI tasks focus on capturing the relationships between two sentences while incorporating additional knowledge entities does not directly help the model improve the reasoning ability.

% \joyce{Style seemingly plays a key role in MEDIQA-RQE. Thoughts as to why?}

\input{sections/04-table-ablation}
% \vspace{-1ex}
\section{Quality Analysis of the Synthetic Data}
% \vspace{-1ex}
\label{sec:quality_analysis}
\textbf{Data Distribution Measures.}
In this section, we present the data distribution and diversity measurement of the synthetic dataset generated by {\ours}.
Figure~\ref{fig:bc5cdr_disease_sentencebert_ours} shows the t-SNE plot of data generated by {\ours} and baselines compared with the ground truth. This visualization clearly demonstrates that {\ours} exhibits a greater overlap with the ground truth, indicating a similar distribution as the original dataset.
In addition, as depicted in Figure \ref{fig:cmd-all}, the embedding of \textit{\ours} aligns more closely with the ground truth distribution than other baselines across all six datasets, further justifying the efficacy of {\ours} for mitigating the distribution shift issue.
% Notably, \textit{\ours} w/ LLM tends to replicate the ground truth more precisely than \textit{\ours} w/ KG. 


\textbf{Diversity Measures.}
Table~\ref{table:aps} calculates the average cosine similarity for sample pairs using SentenceBERT embeddings.
Compared to baselines, the dataset generated with {\ours} exhibits lower cosine similarity and the average similarity is close to that of the ground truth training data, which shows {\ours} could render more diverse data.  
% Moreover, while {\ours} w/ KG envelops more entities, {\ours} w/ LLM delivers a data distribution that is most congruent with the ground truth, further verifying the observations from Figure \ref{fig:cmd-all} and \ref{fig:avg-entity-all}.
Moreover, Figure \ref{fig:avg-entity-all} highlights the ability of \textit{\ours} to cover a broader range of entities in comparison to the baselines. We find that {\ours} w/ KG captures a larger variety of entities than {\ours} w/ LLM, because KG tends to cover more extensive knowledge, including relatively uncommon information that may not be present in LLMs.
% attributed to the rich information, especially rarer knowledge, contained in the knowledge graph.
Figure \ref{fig:bc5cdr_disease_freq} reflects that the entity frequency distribution of {\ours} is more in line with the ground truth, having a relatively balanced distribution among all entities. This ensures that {\ours} generates synthetic data with a wide range of diverse topics.


\label{sec:diversity_measures}

\begin{figure}
	\centering
	\vspace{-2ex}
	\subfigure[t-SNE plot]{
		\includegraphics[width=0.26\linewidth]{figures/bc5cdr_disease_sentencebert_ours.pdf}
		\label{fig:bc5cdr_disease_sentencebert_ours}
	} %\hfill
         \hspace{-2ex}
	% \subfigure[MEDIQA-RQE]{
	% 	\includegraphics[width=0.25\linewidth]{figures/mediqa_rqe_sentencebert_bsl.pdf}
	% 	\label{fig:mediqa_rqe_sentencebert_bsl}
	% }\hspace{-1.5ex}
     \subfigure[Case study of generated examples]{
		\includegraphics[width=0.68\linewidth]{figures/case_study_llm.pdf}
		\label{fig:case_study_llm}
	}
	\caption{Data distribution and diversity measures on {\ours}. (a) is from BC5CDR-Disease and (b) is from MEDIQA-RQE using {\ours} with LLM. \vspace{-1.5ex}}
	\vspace{-1ex}
\label{fig:quality_ana1}
\end{figure}

 \begin{figure}
	\centering
	\vspace{-2ex}
	\subfigure[CMD]{
		\includegraphics[width=0.362\linewidth]{figures/cmd-all.pdf}
		\label{fig:cmd-all}
	} %\hfill
         \hspace{-1.5ex}
	% \subfigure[MEDIQA-RQE]{
	% 	\includegraphics[width=0.25\linewidth]{figures/mediqa_rqe_sentencebert_bsl.pdf}
	% 	\label{fig:mediqa_rqe_sentencebert_bsl}
	% }\hspace{-1.5ex}
     \subfigure[Entity Coverage]{
		\includegraphics[width=0.362\linewidth]{figures/avg-entity-all.pdf}
		\label{fig:avg-entity-all}
	}
 \hspace{-1.5ex}
      \subfigure[Entity Frequency]{
		\includegraphics[width=0.248\linewidth]{figures/bc5cdr_disease_freq.pdf}
		\label{fig:bc5cdr_disease_freq}
	}
	\caption{Data distribution and diversity measures on {\ours}. (c) is from BC5CDR-Disease.\vspace{-2ex}}
	\vspace{-1ex}
\label{fig:quality_ana2}
\end{figure}


% rare diseases

\textbf{Case Study.}
In Figure~\ref{fig:case_study_llm}, we present a case study of examples generated by {\ours} with LLM on MEDIQA-RQE dataset, which consists of consumer health queries. The showcased examples reveal that the sentences generated by {\ours} include more extensive contextual information compared with the baseline as shown in Figure~\ref{fig:case_study_prelim}. These sentences closely resemble the queries people might pose in real-life scenarios.