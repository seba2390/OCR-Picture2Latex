\section{Related Work}
Generating additional training data enables a more precise analysis of medical text, and has gained more attention in the past years. 
Earlier research has employed data augmentation techniques to generate similar samples to existing instances with word substitution~\citep{kang2021umls,checklist}, back translation~\citep{uda}, interpolation between samples~\citep{chen2020mixtext,kan2023r}, pretrained transformers~\citep{kumar2020data,zhou2021flipda,melm} for enhancing model generalization. But they often yield rigid transformations and the quality of the augmented text cannot be always guaranteed. 
Another line of work focuses on leveraging external knowledge to create weak labels~\citep{ratner2017snorkel,fries2017swellshark,wang2019clinical,yu-etal-2021-fine,dunnmon2020cross,gao2022classifying,wander}\footnote{Some works also name it as `distant supervision'~\citep{mintz2009distant,min2013distant,liang2020bond}.}. 
These methods typically require domain expertise and additional task-specific corpora, which can be resource-intensive to obtain for low-resource clinical tasks~\citep{zhang2021wrench}. 
% .

The emergence of LLMs has presented new possibilities, and some studies attempt to use LLM to generate training data~\citep{meng2022generating,meng2023tuning,ye2022zerogen,yu2023large,chung2023increasing}, often with few demonstrations~\citep{gpt3mix}. However, these methods often use generic and simple prompts that may not fully capture domain-specific knowledge, thus potentially limiting the quality of the generated data. 
\citet{liu2022wanli,chung2023increasing} employ interactive learning to generate additional instances to refine the existing dataset, at the cost of additional human efforts.
One recent study \citep{tang2023does} explores synthetic data generation for clinical NLP. Nevertheless, their proposed approach relies on a \emph{much larger training set} to generate candidate entities, which {disregards the practical low-resource setting}~\citep{perez2021true}. Furthermore, their study is limited to {a narrow range of tasks (2 tasks and 4 datasets only)}, lacking breadth in terms of exploring a diverse set of clinical NLP applications.

On the other hand, several works aimed at optimizing prompts using LLMs~\citep{mishra-etal-2022-reframing,zhou2023large,yang2023large} or knowledge graphs~\citep{chen2022knowprompt,hu-etal-2022-knowledgeable,liu-etal-2022-generated,kan2021zero}, yet they mainly focus on refining prompts to obtain the answer for the given input, and the prompt template often remains unchanged. 
Instead, we focus on the different task of generating training instances. By composing different topics and styles together, we are able to generate diverse templates for prompting LLMs to improve the quality of the synthetic data.