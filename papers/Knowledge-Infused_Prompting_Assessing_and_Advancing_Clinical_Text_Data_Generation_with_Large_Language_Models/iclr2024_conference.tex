
\documentclass{article} % For LaTeX2e
\usepackage{pifont}

\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\iclrfinalcopy
\usepackage[colorlinks,citecolor=teal]{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{floatrow}
\usepackage{graphicx} % For adding images
\usepackage{smile}
\usepackage{enumitem}
\usepackage{colortbl}

% \title{Clinically-informed Prompting Makes Large Language Models Better Few-shot Clinical Text Generators}
% \title{Assessing and Advancing LLM on Clinical Text Generation via Knowledge-infused Prompting}
\title{Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ran Xu$^{\heartsuit}$, Hejie Cui$^{\heartsuit}$, Yue Yu{$^\spadesuit$}, Xuan Kan$^{\heartsuit}$, Wenqi Shi{$^\spadesuit$}, Yuchen Zhuang{$^\spadesuit$}, \\ \bf Wei Jin$^{\heartsuit}$, Joyce C. Ho$^{\heartsuit}$, Carl Yang$^{\heartsuit}$ \\
${\heartsuit}$ Emory University \quad {$^\spadesuit$} Georgia Institute of Technology \\
Atlanta, GA, USA \\
\texttt{\{ran.xu,hejie.cui,xuan.kan,wei.jin,joyce.c.ho,j.carlyang\}@emory.edu} \\
\texttt{\{yueyu,wshi83,yczhuang\}@gatech.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{xspace}
\usepackage{xcolor,colortbl}
\usepackage{subfigure}
\usepackage{caption}
\usepackage[normalem]{ulem} % Load the ulem package
\usepackage{listings}


\newcommand{\wei}[1]{\textcolor{magenta}{@Wei:~#1@}}
\newcommand{\joyce}[1]{\textcolor{blue}{@J:~#1@}}
\newcommand{\ours}{\textsc{ClinGen}\xspace}
\newcommand{\ran}[1]{{\color{cyan}[Ran: #1]}}

\lstdefinestyle{mystyle}{
  basicstyle=\ttfamily,
  frame=single,
  breaklines=true,
  breakindent=0pt,
  backgroundcolor=\color{gray!10}, % Change the background color
}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. 
Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. 
To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, {\ours}, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. 
Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that {\ours} consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.
\end{abstract}





\input{sections/01-intro}
\input{sections/02-related}
\input{sections/03-method}
\input{sections/04-exp}
\input{sections/05-discussion}


\bibliography{iclr2024_conference,dataset}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\section{Limitation, Future Works and Ethics Issues}
In this work, we propose {\ours} to better harness the LLM for synthetic text data generation. 
Despite the strong performance of {\ours} on 16 clinical NLP tasks, we mainly verify their efficacy from their empirical performance, sample diversity, and distribution gaps. 
However, there still exist gaps between the performance of the model $\cC_\theta$ fine-tuned using our generated synthetic data and ground-truth data. 
To further improve {\ours}, there are several avenues of future works:

\textbf{Using Clinical LLMs as Data Generator}:
Our method {\ours} relies on an LLM with instruction following ability. We mainly evaluate {\ours} using GPT-family models as the LLM.
Recently, there are many LLMs that have been fine-tuned on additional clinical contexts as well as instructions (e.g. Med-PALM\footnote{\url{https://sites.research.google/med-palm/}}), and achieved superior performance on challenging clinical NLP benchmarks.
However, they are not open-sourced, thus we cannot run them in our experiments. An interesting future work could be how to leverage these Clinical LLMs as Data Generator  to further boost the performance.

\textbf{Data Evaluation}:
In this work, we consider the distribution gap and sample diversity as our optimization objective. However, there might be many other aspects for synthetic quality estimation \citep{pmlr-v162-alaa22a}. 
We need more tools to capture, analyze, and improve this new aspect of data-centric AI.

% Additional Tasks and Methods

\textbf{Factuality}: One issue with LLM-based synthetic data generation is the phenomenon of \emph{hallucination}, wherein the model generates information that does not ground in reality~\citep{zhang2023siren}. This can lead to the propagation of misinformation, which may have negative impacts on the clinical domain.  
It is crucial to cross-verify the generated text with a reliable knowledge base or dataset. 
% Furthermore, incorporating an additional layer of human review can also help in mitigating hallucinations and ensuring the faithfulness of LLM-generated synthetic outputs~\citep{zhou2023context}. 


\section{Additional Preliminary Studies}
\label{sec:add_prelim}
We present additional preliminary studies of the t-SNE plots in Figure~\ref{fig:add_prelim_tsne} and the regularized entity frequencies in Figure~\ref{fig:add_prelim_freq}.
These results further justify the distribution shift issue mentioned in section \ref{sec:limitations}, demonstrating that the limited diversity as well as the distribution shift issue generally exists for a broad range of clinical NLP tasks.
\clearpage
 \begin{figure}[H]
	\centering
	% \vspace{-2ex}
	\subfigure[LitCovid]{
		\includegraphics[width=0.31\linewidth]{figures/litcovid_sentencebert_bsl.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[GAD]{
		\includegraphics[width=0.31\linewidth]{figures/gad_sentencebert_bsl.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CDR]{
		\includegraphics[width=0.31\linewidth]{figures/cdr_sentencebert_bsl.pdf}
	}
 	\subfigure[MEDIQA-RQE]{
		\includegraphics[width=0.31\linewidth]{figures/mediqa_rqe_sentencebert_bsl.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[MQP]{
		\includegraphics[width=0.31\linewidth]{figures/mqp_sentencebert_bsl.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CHEMDNER]{
		\includegraphics[width=0.31\linewidth]{figures/chemdner_sentencebert_bsl.pdf}
	}
	\caption{The t-SNE plots of datasets generated by ZeroGen and DemoGen compared with the ground truth.}
\label{fig:add_prelim_tsne}
\end{figure}

 \begin{figure}[H]
	\centering
	\vspace{-2ex}
	\subfigure[LitCovid]{
		\includegraphics[width=0.31\linewidth]{figures/litcovid_freq_bsl.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[GAD]{
		\includegraphics[width=0.31\linewidth]{figures/gad_freq_bsl.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CDR]{
		\includegraphics[width=0.31\linewidth]{figures/cdr_freq_bsl.pdf}
	}
 	\subfigure[MEDIQA-RQE]{
		\includegraphics[width=0.31\linewidth]{figures/mediqa_rqe_freq_bsl.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[MQP]{
		\includegraphics[width=0.31\linewidth]{figures/mqp_freq_bsl.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CHEMDNER]{
		\includegraphics[width=0.31\linewidth]{figures/chemdner_freq_bsl.pdf}
	}
	\caption{The regularized entity frequencies of datasets generated by ZeroGen and DemoGen compared with the ground truth in log scale.}
\label{fig:add_prelim_freq}
\end{figure}


\section{Implementation Details}
\label{sec:implementation_details}
For implementation, we use PyTorch~\citep{paszke2019pytorch} and HuggingFace~\citep{wolf2019huggingface}. For each dataset, we randomly sample 5 examples from each class to provide few-shot demonstrations and keep a validation set of the same size. In the experiments, We generate 5000 synthetic training data for both {\ours} and the baselines and report the average performance over 3 random seeds for all the results.

During the data generation process when we call the ChatGPT APIs~\citep{chatgpt}, we set the parameter $\operatorname{top\_p}=1.0$ and temperature $t=1.0$ to balance between the quality of the generated text as well as diversity~\citep{chung2023increasing,yu2023large}\footnote{We do not further increase $t$, as previous analysis  \citep{chung2023increasing,yu2023large} has shown that increasing $t$ to larger value does not help with additional performance gain.}. 
With the generated synthetic dataset, we follow the common few-shot learning setting~\citep{perez2021true} to train all the models for 6 epochs and use the model with the best performance on the validation set for evaluation.

During the PubMedBERT fine-tuning, we adopt AdamW~\citep{loshchilov2017decoupled} for optimization with a linear warmup of the first 5\% steps and linear learning rate decay. The learning rate is set to 2e-5 for \texttt{Base} and 4e-5 for \texttt{Large}, and the maximum number of tokens per sequence is 256. 

\section{Dataset Description}
\label{sec:dataset_description}

\input{sections/03-data_table}
The evaluation tasks and datasets are summarized in Table \ref{tab:datastats}. Note that the number of training samples indicates the size of the \textit{original} training set. Specifically, we consider the following datasets:

\begin{itemize}[leftmargin=0.3cm]

\item \textbf{Single-Sentence Tasks}
\begin{itemize}[label=$\circ$]
% \noindent$\bullet$ \textbf{Text Classification}: 
\item \uline{Text Classification}: 

\begin{itemize}
    \item The \textit{LitCovid} dataset~\citep{litcovid} consists of COVID-19-related publications from PubMed. The task is to predict the topics of the sentences, including ``Epidemic Forecasting", ``Treatment", ``Prevention", ``Mechanism", ``Case Report", ``Transmission", and ``Diagnosis".
    \item The \textit{HOC} dataset~\citep{hoc} also extracts sentences from PubMed articles, each annotated at the sentence level. The task is to predict the topics of the sentences, including ``evading growth suppressors", ``tumor promoting inflammation", ``enabling replicative immortality", ``cellular energetics", ``resisting cell death", ``activating invasion and metastasis", genomic instability and mutation", ``inducing angiogenesis", ``sustaining proliferative signaling", and ``avoiding immune destruction".
\end{itemize}

% Our evaluation aligns with prior research~\citep{blue}, focusing on the document-level F1-score for the multi-label text classification task.

% \noindent$\bullet$ \textbf{Relation Extraction}: 
\item \uline{Relation Extraction}:
\begin{itemize}
    \item The \textit{GAD}~\citep{gad} dataset is to predict whether there is a relation between the given disease and gene in the sentences. Note that the original annotation for this dataset is Noisy. To remedy this issue, we \emph{relabel} 350 examples from the original test set to form a clean subset for faithful evaluation.
    \item The \textit{CDR}~\citep{cdr_dataset} dataset is to predict whether the provided chemical can induce the disease in the sentences.
    \item The \textit{ChemProt}~\citep{chemprot} dataset focuses on the chemical-protein relations, and the labels include ``Upregulator", ``Downregulator", ``Agonist", ``Antagonist", ``Product\_of" and ``No relation".
\end{itemize}
% The relation extraction tasks involve identifying relationships and their associated types among entities within sentences. These tasks encompass the detection of gene-disease relation in the \textit{GAD}~\citep{gad} dataset, chemical-disease relation in the \textit{CDR}~\citep{cdr_dataset} dataset, and chemical-protein interactions in the \textit{ChemProt}~\citep{chemprot} dataset. 
% Performance evaluation hinges on a comparison of the predicted relation types against the annotated data, utilizing metrics precision, recall, and F1-score.
\end{itemize}

\item \textbf{Sentence-Pair Tasks}
\begin{itemize}[label=$\circ$]
% \noindent$\bullet$ \textbf{Natural Language Inference (NLI)}: 
\item \uline{Natural Language Inference (NLI)}: 

\begin{itemize}
    \item The \textit{MedNLI}~\citep{mednli} dateset consists of sentences pairs derived from MIMIC-III, where we predict the relations between the sentences. The labels include ``entailment", ``neutral" and ``contradiction".
    \item The \textit{MEDIQA-NLI}~\citep{mediqa-nli} dataset  comprises text-hypothesis pairs. Their relations include ``entailment", ``neutral" and ``contradiction".
    \item The \textit{MEDIQA-RQE}~\citep{abacha2016recognizing} dataset contains NIH consumer health question pairs, and the task is to recognize if the first question can entail the second one. 
\end{itemize}
% To identify inference relations between sentences, we leveraged three clinical NLI datasets: \textit{MedNLI*}~\citep{mednli} with 14,049 pairs derived from MIMIC-III, \textit{MEDIQA-NLI}~\citep{mediqa-nli} comprising 405 text-hypothesis pairs, and \textit{MEDIQA-RQE}~\citep{abacha2016recognizing} with 230 NIH consumer health question pairs for recognizing question entailment (RQE). 
% We evaluate NLI and RQE tasks using accuracy as our primary evaluation metric.

% \noindent$\bullet$ \textbf{Fact Verification}: 
\item \uline{Fact Verification}:
\begin{itemize}
    \item The \textit{PUBHEALTH}~\citep{PUBHEALTH} encompasses claims paired with journalist-crafted explanations. The task is to predict the relations between the claim and evidence, including ``Refute", ``Unproven", ``Support", and ``Mixture".
    \item The \textit{HealthVer}~\citep{healthver} contains evidence-claim pairs from search engine snippets regarding COVID-19 questions. The relations between claims and evidences are chosen from ``Refute", ``Unproven", and ``Support".
\end{itemize}
% In verifying claims against credible evidence, our investigation delves into two real-world evidence-based fact-checking datasets: \textit{PUBHEALTH}~\citep{PUBHEALTH}
% encompassing 11.8K claims paired with journalist-crafted explanations, and \textit{HealthVer}~\citep{healthver} containing 14,330 evidence-claim pairs from search engine snippets regarding COVID-19 questions.
% Our evaluation of the verification prediction task is based on accuracy and F1-score as the key performance metrics.

% \noindent$\bullet$ \textbf{Sentence Similarity (STS)}: 
\item \uline{Sentence Similarity (STS)}:
\begin{itemize}
    \item the \textit{MQP}~\citep{mqp} dataset comprises a collection of medical question pairs designed for identifying semantically similar questions. The task is to predict whether the two questions are equivalent or not.
\end{itemize}
% \textit{MQP}~\citep{mqp} dataset comprises a collection of 4,567 unique medical question pairs designed for identifying semantically similar questions. Our evaluation of model performance involves comparing the predicted similarity scores with the ground truth labels, and we report accuracy as the comparison metric.
\end{itemize}

\item \textbf{Token Classification Tasks}
\begin{itemize}[label=$\circ$]
% \noindent$\bullet$ \textbf{Named Entity Recognition (NER)}: 
\item \uline{Named Entity Recognition (NER)}:
\begin{itemize}
    \item The \textit{BC5CDR-Disease}~\citep{li2016biocreative} is to recognize diseases in the sentences.
    \item The \textit{BC5CDR-Chemical}~\citep{li2016biocreative} is to recognize chemicals in the sentences.
    \item The \textit{NCBI-Disease}~\citep{ncbi-disease} is to recognize diseases in the sentences.
    \item The \textit{CHEMDNER}~\citep{chemdner} is to recognize chemicals in the sentences.
\end{itemize}
% For recognizing and predicting entities (e.g., diseases, chemicals) from text, we examine three datasets commonly used for biomedical NER: \textit{BC5CDR}~\citep{li2016biocreative}, \textit{NCBI-Disease}~\citep{ncbi-disease}, \textit{CHEMDNER}~\citep{chemdner}. Our evaluation compares annotated mention spans in the documents to model predictions, utilizing precision, recall, and F1-score.

% \noindent$\bullet$ \textbf{Attribute Extraction (MedAttr)}:
\item \uline{Attribute Extraction (MedAttr)}:
\begin{itemize}
    \item The \textit{CASI} dataset~\citep{agrawal2022large,claim} aims to identify interventions including medication, dosage, route, freq, reason, duration
\end{itemize}
% For biomedical evidence extraction, we concentrate on identifying interventions from the manually re-annotated \textit{CASI} dataset~\citep{agrawal2022large,claim}, using the F1-score.
\end{itemize}
\end{itemize}

\section{Baseline Details}
\label{sec:baseline_details}
\textbf{Data Augmentation Methods:}
\begin{itemize}[leftmargin=0.5cm]
    \item \textbf{DA-Word Sub}: Word substitution, following Checklist~\citep{checklist}, maintains a word list to generate new examples by filling in a template. 
    \item \textbf{DA-Back Translation}: Following UDA~\citep{uda}, we employ back translation to augment the training data, including translating text from the target language to the source language and then back to the target language. 
    \item \textbf{DA-Mixup}~\citep{chen2020mixtext,seqmix}: We use the TMix version of MixText for data interpolation on the few-shot labeled dataset. For token-level classification tasks, we employ SeqMix~\citep{seqmix} that augments the queried samples by generating additional labeled sequences iteratively.
    \item \textbf{DA-Transformer (MELM)}~\citep{kumar2020data,melm}: It introduces a conditional data augmentation technique that prepends class labels to text sequences for pre-trained transformer-based models. For token-level classification tasks, it incorporates token labels into the sentence context and predicts masked entity tokens by explicitly conditioning on their labels.
    \item  \textbf{LightNER}~\citep{lightner}: It adopts a seq2seq framework, generating the entity span sequence and entity categories under the guidance of a self-attention-based prompting module. It is designed specifically for NER tasks.
    \item  \textbf{KGPC}~\citep{chen2023few}: Leveraging the semantic relations of the knowledge graph, it performs knowledge-guided instance generation for few-shot biomedical NER. It also only applies to NER tasks.
\end{itemize}


\textbf{LLM-based Generation Methods.} 
% In this study, we consider both zero-shot and few-shot learning data generation methods as baselines. 

% In zero-shot learning,
\begin{itemize}[leftmargin=0.5cm]
\item \textbf{ZeroGen}~\citep{ye2022zerogen}: It generates a dataset using a carefully designed instruction and then trains a tiny task-specific model for zero-shot inference. 
We follow the prompting method mentioned in their original paper as implementation, which \emph{does not consider} any style information as well as domain knowledge.
\item \textbf{DemoGen} ~\citep{meng2023tuning,gpt3mix}: It leverages LLMs to synthesize novel training data by  feeding few-shot samples as demonstrations. Note that we focus on using the black-box LLM as the generator, thus we do not tune the LLM as \citep{meng2023tuning}.
\item \textbf{ProGen}~\citep{ye2022progen}: It divides the entire dataset generation process into multiple phases. In each phase, feedback from the previously generated dataset guides the generation towards higher quality. It is also in the few-shot setting.
\end{itemize}

We do not compare with \cite{tang2023does} in the main experiments as it leverages entities extracted from the entire training set and violates the true few-shot learning setting.
% The comparison is listed in Appendix~\ref{}.



\section{Prompt Format}
\label{sec:prompt_format}
\subsection{The prompts for Writing Styles Suggestion with {\ours}}
\label{sec: style_prompt}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for writing styles suggestion with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to generate a synthetic clinical text dataset on <@\textcolor{blue}{[task]}@> tasks. Here are a few examples from the original training set:
<@\textcolor{blue}{[demonstrations]}@>
Please write three potential sources, speakers or authors of the sentences.

\end{lstlisting}

\texttt{[task]}: The task names for each specific task.
\texttt{[demonstrations]}: The few-shot demonstrations from the original training set.


\subsection{The prompts for Data Generation with {\ours}}
\label{sec:generation_prompt}
In the following prompt format, \texttt{[topic]} and \texttt{[style]} are randomly sampled from the topics candidate set and styles candidate set we formulate in the knowledge extraction step, respectively.

\textbf{Named entity recognition tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for NER tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a dataset for <@\textcolor{blue}{[domain]}@> recognition. Your task is to:
1. generate a sentence about <@\textcolor{blue}{[domain]}@>,
2. output a list of named entity about <@\textcolor{blue}{[domain]}@> only,
3. the sentence should mimic the style of <@\textcolor{blue}{[style]}@>,
4. the sentence should mention the <@\textcolor{blue}{[domain]}@> named <@\textcolor{blue}{[topic]}@>.
\end{lstlisting}

\texttt{[domain]}: ``disease" for BC5CDR-Disease and NCBI-Disease; ``chemical" for BC5CDR-Chemical and CHEMDNER.

\textbf{Medication attributes tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for medication attributes tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a dataset for clinical attributes recognition. Your task is to:
1. generate a sentence about clinical attributes, The Clinical Attributes you need to extract include "Medication", "Dosage", "Route", "Frequency", "Reason", "Duration". For each attribute class, please return a list of attributes within the class that occurs in the Sentence.
2. the sentence should mimic the style of <@\textcolor{blue}{[style]}@>,
3. the sentence should be relevant to <@\textcolor{blue}{[topic]}@>.
\end{lstlisting}


\textbf{Text classification tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for text classification tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
 Suppose you are a writer for <@\textcolor{blue}{[domain]}@>. Your task is to:
 1. give a synthetic <@\textcolor{blue}{[domain]}@> about <@\textcolor{blue}{[class\_name]}@>. 
 2. discuss about the subtopic of <@\textcolor{blue}{[topic]}@> for <@\textcolor{blue}{[class\_name]}@> in the <@\textcolor{blue}{[domain]}@>.
 3. the sentence should mimic the style of <@\textcolor{blue}{[style]}@>.
\end{lstlisting}
\texttt{[domain]}: ``COVID-19 Literature" for LitCovid and ``Cancer Document" for HOC.

\texttt{[class\_name]}: the label name for this generated sample.

\textbf{Relation extraction tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for relation extraction tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to generate synthetic data for the biomedical <@\textcolor{blue}{[domain]}@> task. Your task is to:
1. give a sentence about <@\textcolor{blue}{[class\_name]}@> relation between <@\textcolor{blue}{[entity0]}@> and <@\textcolor{blue}{[entity1]}@>
2. the sentence should discuss the <@\textcolor{blue}{[entity0]}@>: <@\textcolor{blue}{[topic0]}@> and <@\textcolor{blue}{[entity1]}@>: <@\textcolor{blue}{[topic1]}@> with the relation <@\textcolor{blue}{[label\_desc]}@>.
3. the sentence should mimic the style of <@\textcolor{blue}{[style]}@>.
\end{lstlisting}
\texttt{[domain]}: ``Disease Gene Relation" for GAD, ``Chemical Disease Relation" for CDR, and ``Chemical Protein Relation" for ChemProt.

\texttt{[entity0]} and \texttt{[entity1]}: ``disease" and ``gene" for GAD, ``chemical" and ``disease: for CDR, and ``chemical" and ``protein" for ChemProt.

\texttt{[class\_name]}: the label name for this generated sample.

\texttt{[label\_desc]}: the description of the selected label. For example, the label ``upregulator" in ChemProt has a description of ``the chemical activates expression of the protein."

\textbf{Natural language inference tasks:}
\begin{lstlisting}[style=mystyle, caption={Prompt Format for generating the first sentence in NLI tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a set of <@\textcolor{blue}{[content]}@>. Your task is to:
1. generate one sentence for a <@\textcolor{blue}{[content]}@>.
2. the <@\textcolor{blue}{[content]}@> should be relevant to <@\textcolor{blue}{[topic]}@>,
3. The <@\textcolor{blue}{[content]}@> should mimic the style of <@\textcolor{blue}{[style]}@>.
\end{lstlisting}
\texttt{[content]}: ``health question" for MEDIQA-RQE, ``claim" for MEDIQA-NLI, MedNLI and MQP, and ``health news" for PUBHEALTH and HealthVer.


% \vspace{8ex}
\begin{lstlisting}[style=mystyle, caption={Prompt Format for generating the second sentence in NLI tasks with {\ours}.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a pair of sentences for the <@\textcolor{blue}{[domain]}@> task with the label '<@\textcolor{blue}{[class\_name]}@>'. Given the <@\textcolor{blue}{[content]}@>: '<@\textcolor{blue}{[first\_sentence]}@>', Your task is to:
1. generate one short <@\textcolor{blue}{[content]}@> about <@\textcolor{blue}{[topic]}@> so that <@\textcolor{blue}{[label\_desc]}@>.
2. The <@\textcolor{blue}{[content]}@> should mimic the style of the first sentence.
\end{lstlisting}
\texttt{[domain]}: ``Question Entailment" for MEDIQA-RQE, ``Natural Language Entailment" for MEDIQA-NLI and MedNLI, ``Fact Verification" for PUBHEALTH and HealthVer, and ``Sentence Similarity Calculation" for MQP.

\texttt{[content]}: ``health question" for MEDIQA-RQE, ``hypothesis" for MEDIQA-NLI, MedNLI, ``evidence" for PUBHEALTH and HealthVer, and ``sentence" for MQP.

\texttt{[class\_name]}: the label name for this generated sample.

\texttt{[label\_desc]}: the description of the selected label.

\texttt{[first\_sentence]}: the first sentence we generate

\subsection{Prompts for ZeroGen, DemoGen, ProGen}
\label{sec:prompt_format_bsl}
We use the same set of prompts for ZeroGen, DemoGen and ProGen, while DemoGen and ProGen have additional demonstrations augmented to the prompts. DemoGen uses the few-shot examples in the training set as demonstrations, and ProGen leverages feedbacks from previous rounds to iteratively guide the generation.

\textbf{Named entity recognition tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for NER tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a dataset for <@\textcolor{blue}{[domain]}@> recognition. Your task is to generate a sentence about <@\textcolor{blue}{[domain]}@> and output a list of named entity about <@\textcolor{blue}{[domain]}@> only.
\end{lstlisting}

\texttt{[domain]}: ``disease" for BC5CDR-Disease and NCBI-Disease; ``chemical" for BC5CDR-Chemical and CHEMDNER.

\textbf{Medication attributes tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for medication attributes tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a dataset for clinical attributes recognition. Your task is to generate a sentence about clinical attributes, The Clinical Attributes you need to extract include "Medication", "Dosage", "Route", "Frequency", "Reason", "Duration". For each attribute class, please return a list of attributes within the class that occurs in the Sentence.
\end{lstlisting}


\textbf{Text classification tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for text classification tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
 Suppose you are a writer for <@\textcolor{blue}{[domain]}@>. Your task is to give a synthetic <@\textcolor{blue}{[domain]}@> about <@\textcolor{blue}{[class\_name]}@>. 
\end{lstlisting}
\texttt{[domain]}: ``COVID-19 Literature" for LitCovid and ``Cancer Document" for HOC.

\texttt{[class\_name]}: the label name for this generated sample.

\textbf{Relation extraction tasks:}

\begin{lstlisting}[style=mystyle, caption={Prompt Format for relation extraction tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to generate synthetic data for the biomedical <@\textcolor{blue}{[domain]}@> task. Your task is to give a sentence about <@\textcolor{blue}{[class\_name]}@> relation between <@\textcolor{blue}{[entity0]}@> and <@\textcolor{blue}{[entity1]}@> so that <@\textcolor{blue}{[label\_desc]}@>.
\end{lstlisting}
\texttt{[domain]}: ``Disease Gene Relation" for GAD, ``Chemical Disease Relation" for CDR, and ``Chemical Protein Relation" for ChemProt.

\texttt{[entity0]} and \texttt{[entity1]}: ``disease" and ``gene" for GAD, ``chemical" and ``disease: for CDR, and ``chemical" and ``protein" for ChemProt.

\texttt{[class\_name]}: the label name for this generated sample.

\texttt{[label\_desc]}: the description of the selected label. For example, the label ``upregulator" in ChemProt has a description of ``the chemical activates expression of the protein."

\textbf{Natural language inference tasks:}
\begin{lstlisting}[style=mystyle, caption={Prompt Format for generating the first sentence in NLI tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a set of <@\textcolor{blue}{[content]}@>. Your task is to generate one sentence for a <@\textcolor{blue}{[content]}@>.
\end{lstlisting}
\texttt{[content]}: ``health question" for MEDIQA-RQE, ``claim" for MEDIQA-NLI, MedNLI and MQP, and ``health news" for PUBHEALTH and HealthVer.


% \vspace{8ex}
\begin{lstlisting}[style=mystyle, caption={Prompt Format for generating the second sentence in NLI tasks with baselines.}, label=lst:prompt, escapeinside={<@}{@>}]
Suppose you need to create a pair of sentences for the <@\textcolor{blue}{[domain]}@> task with the label '<@\textcolor{blue}{[class\_name]}@>'. Given the <@\textcolor{blue}{[content]}@>: '<@\textcolor{blue}{[first\_sentence]}@>', Your task is to generate one short <@\textcolor{blue}{[content]}@> so that <@\textcolor{blue}{[label\_desc]}@>.
\end{lstlisting}
\texttt{[domain]}: ``Question Entailment" for MEDIQA-RQE, ``Natural Language Entailment" for MEDIQA-NLI and MedNLI, ``Fact Verification" for PUBHEALTH and HealthVer, and ``Sentence Similarity Calculation" for MQP.

\texttt{[content]}: ``health question" for MEDIQA-RQE, ``hypothesis" for MEDIQA-NLI, MedNLI, ``evidence" for PUBHEALTH and HealthVer, and ``sentence" for MQP.

\texttt{[class\_name]}: the label name for this generated sample.

\texttt{[label\_desc]}: the description of the selected label.

\texttt{[first\_sentence]}: the first sentence we generate

\section{Additional Experimental Results}
In this section, we present additional experimental results on every dataset in Tables~\ref{tab:single-sent}, ~\ref{tab:sent-pair}, ~\ref{tab:token-class}.
\label{sec:more_experimental_results}
\input{sections/04-exp-table-single-sent}
\input{sections/04-exp-table-sent-pair}
% \input{sections/04-exp-table-sent-pair-base}
% \input{sections/04-exp-table-sent-pair-large}
\input{sections/04-exp-table-token-class}

\clearpage
\section{Additional Ablation and Parameter Studies}
\label{sec:add_ablation_para}

Figure~\ref{fig:generator-add} and \ref{fig:size-synthetic-add} show the effect of different generators and the effect of the proportion of data on two additional datasets, respectively. Overall, our method generally outperform the best baseline. One interesting finding for the NCBI-Disease dataset is that {\ours} performs worse than the best on one variant. We hypothesize that it is because this task involves more complex input and output, potentially posing a challenge for moderate-size LLMs to follow the instructions. 


Besides, as few-shot sample selection is important for the final performance, we show the performance of different 3 random seeds (with different seed examples/training process), and observe that our method {\ours} generally outperforms the baselines with non-negligible margins, which indicates the robustness of {\ours} as it does not rely on a specific subset of few-shot training examples to perform well. 

\begin{figure}[t!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[CDR]{
            \includegraphics[width=0.48\textwidth]{figures/generator-CDR.pdf}
            \label{fig:generator-CDR}
        } \hspace{-3mm}
        \subfigure[NCBI-Disease]{
            \includegraphics[width=0.48\textwidth]{figures/generator-NCBI.pdf}
            \label{fig:generator-NCBI}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different generators at \texttt{Base}.}\label{fig:generator-add}}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[CDR]{
            \includegraphics[width=0.5\textwidth]{figures/size-CDR.pdf}
            \label{fig:size-CDR}
        } \hspace{-6mm}
        \subfigure[NCBI-Disease]{
            \includegraphics[width=0.5\textwidth]{figures/size-NCBI.pdf}
            \label{fig:size-NCBI}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different proportion of data at \texttt{Base}.}\label{fig:size-synthetic-add}}
    \end{minipage}%
    \vspace{-0.5ex}
\end{figure}

\begin{table}[t]
% \floatconts
% \vspace{-1ex}
  \caption{Performance with Different Random Seeds using PubMedBERT$_{\texttt{Base}}$.}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{l ccc | ccc | ccc| ccc}
  \toprule
  & \multicolumn{3}{c}{\textbf{HOC}} & \multicolumn{3}{c}{\textbf{CDR}} & \multicolumn{3}{c}{\textbf{MEDIQA-RQE}} & \multicolumn{3}{c}{\textbf{NCBI-Disease}}\\
  % \midrule
  \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
  & Best Baseline & {\ours}-KG& {\ours}-LLM & Best Baseline & {\ours}-KG& {\ours}-LLM& Best Baseline & {\ours}-KG& {\ours}-LLM & Best Baseline & {\ours}-KG& {\ours}-LLM\\
  \midrule
  1 &70.04 &	74.30 &	77.30	 &61.52 &	61.66 &	63.34	 & 68.30	 &76.85 &	74.50 &	56.12	 &60.22	 &54.51 \\
2 & 75.30	& 79.73& 	73.63	& 60.69& 	63.77	& 64.66	& 64.20	& 71.80	& 71.19	& 54.19& 	60.64& 	57.81 \\
3 & 71.41& 	74.81	& 78.33& 	57.82& 	59.79	& 62.02& 	67.18	& 75.90	& 71.51	& 53.85& 	57.52& 	55.50\\
  \bottomrule
  \end{tabular}
  }
  \label{tab:random_seed}
  \vspace{-1ex}
\end{table}


\section{Additional Quality Analysis}
We present additional quality analysis of the synthetic dataset with t-SNE plots in Figure~\ref{fig:add_quality_tsne} and the regularized entity frequencies in Figure~\ref{fig:add_quality_freq}.

 \begin{figure}[h]
	\centering
	\vspace{-2ex}
	\subfigure[LitCovid]{
		\includegraphics[width=0.31\linewidth]{figures/litcovid_sentencebert_ours.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[GAD]{
		\includegraphics[width=0.31\linewidth]{figures/gad_sentencebert_ours.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CDR]{
		\includegraphics[width=0.31\linewidth]{figures/cdr_sentencebert_ours.pdf}
	}
 	\subfigure[MEDIQA-RQE]{
		\includegraphics[width=0.31\linewidth]{figures/mediqa_rqe_sentencebert_ours.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[MQP]{
		\includegraphics[width=0.31\linewidth]{figures/mqp_sentencebert_ours.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CHEMDNER]{
		\includegraphics[width=0.31\linewidth]{figures/chemdner_sentencebert_ours.pdf}
	}
	\caption{The t-SNE plots of datasets generated by {\ours}, ZeroGen and DemoGen compared with the ground truth.}
\label{fig:add_quality_tsne}
\end{figure}

 \begin{figure}[h]
	\centering
	\vspace{-2ex}
	\subfigure[LitCovid]{
		\includegraphics[width=0.31\linewidth]{figures/litcovid_freq.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[GAD]{
		\includegraphics[width=0.31\linewidth]{figures/gad_freq.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CDR]{
		\includegraphics[width=0.31\linewidth]{figures/cdr_freq.pdf}
	}
 	\subfigure[MEDIQA-RQE]{
		\includegraphics[width=0.31\linewidth]{figures/mediqa_rqe_freq.pdf}
	} %\hfill
         % \hspace{-1.5ex}
     \subfigure[MQP]{
		\includegraphics[width=0.31\linewidth]{figures/mqp_freq.pdf}
	}
        % \hspace{-1.5ex}
      \subfigure[CHEMDNER]{
		\includegraphics[width=0.31\linewidth]{figures/chemdner_freq.pdf}
	}
	\caption{The regularized entity frequencies of datasets generated by {\ours}, ZeroGen and DemoGen compared with the ground truth in log scale.}
\label{fig:add_quality_freq}
\end{figure}


\section{Monetary Cost}
\label{sec:apd_cost}
We display the monetary cost of {\ours} for calling the OpenAI APIs, with a comparison with prompting GPT-3.5 for direct inference and DemoGen. From the values shown in Figure~\ref{tab:money_cost}, we observe that inference via GPT-3.5 generally has a higher cost, as it needs to input all the testing samples for prompting. In contrast, DemoGen has a relatively lower cost, because it does not include the topics and writing styles to the prompts as {\ours} does.

\begin{table}[h]
% \floatconts
  \caption{The average cost (in US dollars) of running {\ours} on various datasets per 1000 samples, compared with prompting GPT-3.5 for inference and DemoGen.}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lccccccc}
  \toprule
  & \bfseries HOC & \bfseries GAD & \bfseries ChemProt & \bfseries MEDIQA-RQE & \bfseries PUBHEALTH & \bfseries NCBI-Disease & \bfseries CASI\\
  % \midrule
  \midrule
  GPT-3.5 Inference & 1.09 & 1.05 & 5.75 & 2.15 & 2.80 & 0.90 & 1.30 \\ 
  % \hline
  DemoGen & 0.59 & 0.66 & 1.35 & 0.81 & 0.92 & 1.12 & 1.28 \\
  \rowcolor{teal!10} {\ours} w/ KG & 0.65 & 0.73 & 1.47 & 0.86 & 1.01 & 1.41 & 1.55 \\
  \rowcolor{teal!10} {\ours} w/ LLM & 0.72 & 0.84 & 1.51 & 0.90 & 1.34 & 1.49 & 1.62 \\
  \bottomrule
  \end{tabular}
  }
  \label{tab:money_cost}
  \vspace{-1ex}
\end{table}

\end{document}
