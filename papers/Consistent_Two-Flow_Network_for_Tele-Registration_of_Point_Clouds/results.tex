\section{Results and Evaluation} \label{sec:results}
\input{figures/realscan.tex}
\input{figures/realscan_pix3d.tex}

We first introduce the details of our training data generation in Section~\ref{sec:data}, then, we show results of pair registration and partial completion obtained with our CTF-Net and present a set of qualitative results to demonstrate the capabilities of our method in Section~\ref{sec:qualitative}.
A quantitative evaluation and ablation studies are described in Sections~\ref{sec:quantitative} and~\ref{sec:ablation}. We present results on both synthetic and real data.


\subsection{Data preparation}
\label{sec:data}

Our dataset contains $31,742$ shapes in eight categories from ShapeNet v2~\shortcite{chang2015shapenet}. The training, validating and testing split is similar to the dataset of Tchapmi et al.~\shortcite{tchapmi2019topnet}. Each shape in our dataset is represented as a point cloud which contains $16,384$ points, and all the shapes are normalized into a unit cube centered at the origin.

For each shape $\mathcal{S}^*$ in the dataset, we crop two parts from it and apply random transformations to get the training pairs $(\mathcal{P}_1, \mathcal{P}_2)$. Specifically, we first randomly generate two spheres centered at the surface of the bounding sphere of each shape, with the radius sampled from $[0.3,1.3]$, and then use these two spheres to crop two parts out of the shape. We use this method to simulate the real scan cases when a camera is held and scan around the shape. Note that we set a minimum distance ($0.3$ in our setting) between each pair of cropped parts to avoid too much overlap between those two parts, where the distance is calculated by the Euclidean distance between the centers of those two parts. We also ensure that each cropped part and the remaining part both have more than $N=4096$ points, so that they can be down-sampled to $N=2048$ points.
Each part is first translated to the origin and then rotated about its center. 

The rotation and translation matrix for pairs of parts are denoted as $\mathcal{R}_{1}$, $\mathcal{T}_{1}$, $\mathcal{R}_{2}$ and $\mathcal{T}_{2}$ respectively, and the complete transformations are then denoted as $\mathcal{M}_{1} = \mathcal{R}_{1} \mathcal{T}_{1}$ and $\mathcal{M}_{2} = \mathcal{R}_{2} \mathcal{T}_{2}$. 
Therefore, the corresponding ground truth for completed shapes are $\mathcal{S}_{1}^* = \mathcal{M}_{1} \mathcal{S}^*$ and $\mathcal{S}_{2}^* = \mathcal{M}_{2} \mathcal{S}^*$,
and the corresponding ground truth for relative transformations between those two parts are $\mathcal{M}_{21}^{*} = \mathcal{M}_1\mathcal{M}_2^{-1}$ and $\mathcal{M}_{12}^{*} = \mathcal{M}_2\mathcal{M}_1^{-1}$.
Note that in the following experiments, CTF-Net is trained individually for each of those eight categories. For a fair comparison, all the compared methods are trained category-by-category.


\subsection{Qualitative results}

 \label{sec:qualitative}
We show results for synthetic shapes on eight categories. To verify the generality of our method, we also test our method on real scans.

\subsubsection{Results on synthetic data}

Fig.~\ref{fig:gallery} shows visual examples of the registration and  completion results. %
The input pair of parts are centered at the origin point, as shown in the first two columns.
By taking either part as the anchor, our method is able to transfer the other part to make it align with the anchor well, as shown in the third and fourth columns respectively. 
We can see that even when there is little overlap between the input parts, like most of the examples in the results, CTF-Net can still register them correctly. 
Also, the union shapes after registration with different anchor parts are quite similar thanks to the effect of $L_{\text{s}}^{\text{T}}$.
The fifth and sixth columns show the completed shape for each part, and the last two columns are the corresponding ground truth complete shape. 
We can see that the completion module in our CTF-Net can work well for parts given in two different orientations, and by solving registration and completion together, the parts do not need to be pre-aligned before completion.
Note that we take registration results from the \textit{C-R flow} and completion results from the \textit{R-C flow}, mainly because each will improve the performance for the other, and the end output of each flow is more reliable even with the consistency loss. 


\subsubsection{Results on real data}
To demonstrate the generality of our method, we use 3D scanners to manually scan several objects from all those eight categories for testing. Specifically, for small objects with detailed textures, in particular, \texttt{Plane}, \texttt{Car}, \texttt{Watercraft}, and \texttt{Lamp}, we place each object on a turnplate and use an Artec Spider scanner for scanning which directly produces the reconstructed model; for other larger objects, we use a Microsoft Kinect v2 scanner to do the scanning, then utilize bundle fusion~\cite{dai2017bundle} to obtain the reconstructed model. %
The reconstruction results are shown in the last column in Fig.~\ref{fig:realscan}, denoted as $\mathcal{F}$.

To generate the input pairs, we first normalize the reconstructed full model into a unit box and put it in a virtual environment, and then randomly place two cameras pointing at the model to capture RGB-D images. Two partial point clouds are then obtained by back-projecting the depth images to the 3D reconstructed model, and those two single-view point clouds are taken as input to test our CTF-Net. 
Note that here we do not use the single-view point cloud directly from the scanner since there exist some affections such as reflect light which could make the single-view point cloud extremely sparse.

\input{figures/table_result.tex}
Fig.~\ref{fig:realscan} shows the results on our real scanned data. Note that the input partial parts can still be correctly aligned even the point clouds are noisy and non-uniformly distributed. 
For example, although the two parts of the couch shown on the fifth row are quite noisy and covers different region of the couch, our method is still able to successfully align them and complete the missing region in the front.

To further justify the generality of our method, we also tested our method on RedWood~\cite{choi2016large} and Pix3D~\cite{sun2018pix3d} datasets, both of which provided the full reconstructed model.
Similar to our real scanned data, the input pairs of these two datasets are generated by single-view scanning.
Some example results are shown in Fig.~\ref{fig:realscan_pix3d}. %
For each example, the full reconstructed model is shown in the last column, denote as $\mathcal{F}$, for comparison. 
We see that although the single-view input data is quite different from the synthetic training data, the predicted registration is still quite accurate and the completion results are also quite reasonable.
For example, in the fourth row, our method predicts the missing leg of the table, and in the fifth row, we can notice that the entire chair back is successfully reconstructed.
Note that for each real scan example, we slightly rotated the view during visualization to show the single-view inputs more clearly.


\subsection{Quantitative evaluation} \label{sec:quantitative}

We perform quantitative evaluation of both registration and completion networks by measuring the errors of predicted transformation parameters and reconstructed point clouds.

For registration, we calculate the error for the final output of the \textit{C-R flow}. \textit{Rotational error} $E_{\theta}$ is calculated as the absolute error between the predicted and ground truth angle in degrees, and \textit{Translation error} $E_{\text{t}}$ is calculated by the L2 distance between the predicted and ground truth translation (in normalized units):
\begin{align}
E_{\theta} &= \left(|\theta(\mathcal{R}_{12}^{\text{C-R}}, \mathcal{R}_{12}^{\text{C-R}*})| + |\theta(\mathcal{R}_{21}^{\text{C-R}}, \mathcal{R}_{21}^{\text{C-R}*})|\right) / 2,  \\ 
E_{t} &= \left(|t(\mathcal{T}_{12}^{\text{C-R}}, \mathcal{T}_{12}^{\text{C-R}*})| + |t(\mathcal{T}_{21}^{\text{C-R}}, \mathcal{T}_{21}^{\text{C-R}*})|\right) / 2 \times 10^3. 
\end{align}
where $\theta$ denotes the angle derived from the relative quaternion from the predicted quaternion $\mathcal{R}^{\text{C-R}}$ to the ground truth quaternion $\mathcal{R}^{\text{C-R}*}$, and $t$ denotes the distance between the translation decomposed from the predicted transformation $\mathcal{T}^{\text{C-R}}$ and the ground truth transformation $\mathcal{T}^{\text{C-R}*}$.

For completion, we also calculate the error for the completion obtained from the \textit{C-R flow}.
Since our completion network generates the missing region without modifying the given part, we use two measures $E_{\text{emd}}^g$ and $E_{\text{emd}}^f$ to compute the errors in the generated region and full shape, respectively, where $E_{\text{emd}}^g$ is calculated as the EMD between the reconstructed and corresponding ground truth point cloud of the missing region, and  $E_{\text{emd}}^f$ is calculated as the EMD between the reconstructed shape concatenate with the input part and ground truth full shape:
\begin{equation}
E_{emd}^g = \left(\text{D}_{emd}(\mathcal{G}_{1}^{\text{C-R}}, \mathcal{G}_{1}^{\text{C-R}*}) + \text{D}_{emd}(\mathcal{G}_{2}^{\text{C-R}}, \mathcal{G}_{2}^{\text{C-R}*})\right) / 2 \times 10^3, \label{compeval1}
\end{equation}
\begin{equation}
E_{emd}^f = \left(\text{D}_{emd}(\mathcal{S}_{1}^{\text{C-R}} , \mathcal{S}_{1}^{\text{C-R}*} ) + \text{D}_{emd}(\mathcal{S}_{2}^{\text{C-R}} , \mathcal{S}_{2}^{\text{C-R}*})\right) / 2 \times 10^3. \label{compeval2}
\end{equation}

Table~\ref{tab:result} shows the errors of all eight categories. Note that $E_t$, $E_{emd}^g$ and $E_{emd}^f$ values have been scaled by $10^3$ to amplify the error.
In most categories, our CTF-Net is able to predict accurate registration together with completion.
We observe that the registration error of \texttt{Lamp} is significant higher than other categories due to the ambiguity arising from its strong symmetry. For example, the stands of most lamps are cylindrical, which could lead to the rotational ambiguity. Furthermore, the variety and complexity of \texttt{Lamp} category are very large, e.g., a lamp with multi-fold stand, or a pendant lamp with cluttered accessories.
The rotational error of other categories are all lower than 20 degrees; meanwhile, the full shape completion errors of these categories are lower than $3$, meaning that the good reconstruction quality is achieved.

Overall, we achieve an average of 15.12 degrees rotational and 5.97 translation error in registration, and 2.40 completion error for the whole shape.


\subsubsection{Comparison on registration}

We compare the registration results of our method to five other options:
\begin{enumerate}
	\item A classic registration method 4PCS~\cite{aiger20084pcs}, which can globally register complete or partial point sets;
	\item A deep-learning based registration method PRNet~\cite{wang2019prnet}, which focuses on partial-to-partial registration, with self-supervised learning;
	\item A deep-learning based registration method DCP~\cite{wang2019dcp}, can be seen as state-of-the-art;
	\item Baseline registration network which predicts the rotation and translation parameters at once from a single decoder, denoted as BL-Regi. The detailed network structure can be found in the supplementary material;
	\item Our registration network alone, denoted as Regi.
\end{enumerate}

\input{figures/overlap.tex}

\input{figures/table_comparison_regi.tex}
\input{figures/chart_overlap}

The geometry-based method 4PCS is directly tested on our dataset, while the remaining three learning-based methods are trained/tested on the same dataset as ours. 
The comparison of prediction error is reported in Table~\ref{tab:comparison_regi}.
We can see that the prediction error of 4PCS is the highest since it assumes that there are certain-level overlap between the input pair, while our data are mainly non-overlap. 
The errors obtained using PRNet are lower than 4PCS but higher than others, meaning that PRNet can not works well on our data with little overlap between the input pairs. DCP performs better comparing to our BL-Regi method in rotational error, however, the translation error of BL-Regi is 23\% lower. Comparing DCP to our single registration module Regi, we can see that the rotational error is quite close, but the translation error of Regi is 68\% lower. The key idea of DCP is to find the correspondences between two point sets, which also fails on our dataset.
The baseline registration method, which predicts the rotation and translation parameters simultaneously from the same decoder, has slightly higher rotation error to our registration network, however, the translation error is more than twice as much as ours. This proves that splitting the prediction of rotation and translation could achieve better performance.
Last but not least, our method with two complete flows gets the best results comparing to all five other options, which shows the benefit of combining the registration and completion tasks.

In order to assess the advantage of our method on non-overlap data, we further compare our method with 4PCS, PRNet and DCP on the data with different levels of overlap.
To generate testing pairs with a certain overlap $\eta$, we modify our data preparation procedure slightly. In more detail, for each shape in our testing set, we randomly generate two spheres with fixed centers $(0, 0.75, 0)$ and $(0, -0.75, 0)$, respectively, and the radius randomly sampled from $[0.3,1.3]$ to crop two parts from a complete shape. We then calculate the IoU of the cropped parts and keep the ones with IoU in $[0.9\eta,1.1\eta]$. Finally, we downsample each part to $N=2048$ points.

We take the \texttt{Plane} category for testing, and quantitative comparisons of the rotation and translation errors are shown in Fig.~\ref{fig:chart_overlap}. 
We can see that both the rotation and translation errors of almost all the methods keep increasing as the overlap region decreases.
Regards to rotation error, 4PCS performs best when the overlap region is 80\%, however, as the overlap decreases, the performance of 4PCS dropped significantly and the error is lager than 50 degrees from 40\% overlap to 0\%. 

\input{figures/compcompare}

PRNet and DCP performs more stable than 4PCS,  and the rotational error of DCP is slightly lower than ours at 80\% and 60\% overlap, but CTF-Net keeps the lowest error from 40\% to 0\%.
For translation error, the four methods are quite similar at 80\% overlap, however, the error of 4PCS increases rapidly when overlap region decreases. 
The error of DCP is slightly lower than PRNet but significantly higher than ours at 20\% and 0\%.
PRNet is supposed to be able to deal with a partial-to-partial registration, however, it shows worse performance than DCP in our setting. The main reason is that PRNet assumes a large overlap between the input pair data and requires a good initial alignment for the ensuing iteration, while for our input data with quite little overlap, they fail to find a sufficiently good initial alignment.

We observe that the translation error of our method decreases a bit even the overlap region drop from 80\% to 20\%, implying that our network has higher capability in learning data with little overlap.

Fig.~\ref{fig:overlap} shows the visual comparison of our method to 4PCS, PRNet, and DCP.
To highlight the differences in the overlap regions, we rotate all the input pairs and align their first part (shown in blue), and use a lighter color to show the overlap region between the input pair, shown in the second row. %
From the results, we can see that 4PCS fails at 20\% and 0\% overlap, since the algorithm assumes that two parts should have a large overlapped region. For the deep-learning based methods PRNet and DCP, we observe that the rotational error of PRNet is large at and 60\%, 20\% and 0\% overlap. DCP produces more accurate rotation angle, however, the translation error is still high.
Our method performs stable on different overlap data. We observe that even there is no overlap between the head and tail of the plane, our method is still able to align two parts in the correct position, with a certain margin in the middle.
This experiment also shows that our method are robust when the area of the input data variants, which is helpful for the use of real scanned data.


\subsubsection{Comparison on completion}
\input{figures/table_comparison_comp.tex}

We compare the completion results of our method to five other options:
\begin{enumerate}
    \item TopNet~\cite{tchapmi2019topnet}, which direct reconstruct the whole shape using a structural decoder;
    \item MSN~\cite{liu2020morphing}, which predicts the whole shape in a coarse to fine manner;
    \item The original PF-Net~\cite{huang2020pf}, which can be seen as state-of-the-art; %
	\item Our completion network alone without orientation module, denoted as BL-Comp;
	\item Our completion network alone, denoted as Comp;
\end{enumerate}

All the five methods are trained/tested on the same dataset as ours, TopNet and PF-Net are trained using chamfer distance (CD) as proposed in their paper, and the remaining methods are trained using EMD. For fair comparison, we add another two completion quality measures based on CD instead of EMD, denoted as $E_{\text{cd}}^g$ and $E_{\text{cd}}^f$, by substituting $D_{emd}$ by $D_{cd}$ in Equation~\ref{compeval1} and Equation~\ref{compeval2}. Specifically,
\begin{align}
E_{cd}^g &= \left(\text{D}_{cd}(\mathcal{G}_{1}^{\text{C-R}}, \mathcal{G}_{1}^{\text{C-R}*}) + \text{D}_{cd}(\mathcal{G}_{2}^{\text{C-R}}, \mathcal{G}_{2}^{\text{C-R}*})\right) / 2 \times 10^4, \\
E_{cd}^f &= \left(\text{D}_{cd}(\mathcal{S}_{1}^{\text{C-R}} , \mathcal{S}_{1}^{\text{C-R}*} ) + \text{D}_{cd}(\mathcal{S}_{2}^{\text{C-R}} , \mathcal{S}_{2}^{\text{C-R}*})\right) / 2 \times 10^4. 
\end{align}
Note that $E_{\text{cd}}^g$ and $E_{\text{cd}}^f$ values have been scaled by $10^4$ to amplify the error.
The reconstruction error is reported in Table~\ref{tab:comparison_comp}.
TopNet and MSN predict the whole shape directly, thus we only compute $E_{emd}^f$ and $E_{cd}^f$ for them. For all other methods, only points on the missing regions are generated, so $E_{emd}^g$ and $E_{cd}^g$ are also computed. 
We can see that TopNet gets the highest error $E_{emd}^f$ since it doesn't keep the points from the input part and it is trained using CD. 
For comparison, MSN also predicts the whole shape, but it is trained using EMD, so both $E_{emd}^f$ and $E_{cd}^f$ are lower than TopNet. 
PF-Net achieves the lowest error in $E_{cd}^g$ and $E_{cd}^f$, since it keeps the input region and is trained using CD. 
The main difference between BL-Comp and PF-Net is that BL-Comp is trained using EMD measure, thus both $E_{emd}^g$ and $E_{emd}^f$ are lower than PF-Net.
To further improve the results on input part pairs with randomly 3D rotation, we added an orientation module to BL-Comp, denoted as Comp, and all the errors get lower comparing to BL-Comp. %
Our method obtains the best result in EMD measure, with 76\%, 32\%, and 51\% drop on $E_{emd}^f$ comparing to TopNet, MSN and PF-Net, thanks to the consistent two flow network.

Fig.~\ref{fig:compcompare} shows two examples of different completion methods. We see that the prediction results of TopNet are quite blurred and lack fine details. Similar results can be seen in the fourth column, i.e., PF-Net keeps the original input but the predicted parts are sparse, leading to the inconsistent results. These two methods are trained using CD measure, which is not able to effectively constrain the sparseness of the point cloud. The results of MSN are obviously more compact, however, we can see that there are some outlier points distributed in the blank region. For example, there are some unnecessary points  located between the back and the seat region in the second row. Our baseline completion network performs better than PF-Net, but incurs some distortions in the output. The completion method, which includes the orientation module to the baseline completion, predicts points more accurately. Our method achieves the best results comparing to others, and the generated points are evenly distributed in the missing regions. Note that here we take the completion results from \textit{C-R flow} to compare with the other methods.


\input{figures/chart_stress.tex}
\input{figures/stress.tex}

\subsubsection{Stress test}
To better simulate the scenario of real scanning, we generate the data with different noise level to test the robustness of our CTF-Net. Specifically, to generate the data with noise level $\zeta$, we randomly sample $N \times 3$ values range in $[0, \zeta]$ as the noise, then add the sampled noise to the original input partial pair $\mathcal{P}_1$ and $\mathcal{P}_2$. We set five different $\zeta$ to test our method, and the results are shown in Fig.~\ref{fig:chart_stress}. Note that our original data generated by virtual scanning have the noise level at 0.005.
We observe that all the errors increase quite slightly as the noise level increases from 0.005 to 0.09, i.e., $E_\theta$, $E_t$, $E_{emd}^g$ and $E_{emd}^f$ keep lower than $20$, $10$, $8$ and $5$ respectively. 
At noise level of 0.12 and 0.15, which rarely appear in real scanning, the errors increasing slightly faster, but the rotational error keeps lower than $27$ degree. 
The results show that our method is able to maintain low prediction error given noisy data.

Fig.~\ref{fig:stress} shows how the registration and completion errors change with the increase of noise level added to the input parts.
The registration of our method is correct even the input partial data are quite noisy, as shown in the examples in the last row. Besides, the missing parts generated by our method are recognizable at all noise levels. 

Other than points with random and small noise, outliers can also exist in the captured point cloud data, thus we perform another experiment to show how outliers affect the performance of our CTF-Net.
Specifically, as shown in Fig.~\ref{fig:outlier}, for each testing point cloud data $\mathcal{P}$, we randomly select $K$ points and each of them is then translated by a random displacement to simulate the outliers. The direction of each displacement is uniformly sampled in $SE(3)$ space, and the length of each displacement is uniformly sampled between $[0.1,0.5]$. In this experiment, we set $K$ to be 50 and 100, respectively, to test our method. We denote the processed data as $\mathcal{P}_{ol}$. 

\input{figures/outlier.tex}
\input{figures/chart_stress_outlier.tex}
Furthermore, since automatic outlier removal algorithms are widely used in many works related to point cloud processing, we also test our CTF-Net on the outlier data after being processed by an outlier filter. Specifically, we utilize the radius outlier removal method from Zhou et al.~\shortcite{zhou2018open3d}, which removes points that have few neighbors in a given sphere around them,  and we use the default parameters setting proposed in~\shortcite{zhou2018open3d}. The processed data are denoted as $\mathcal{P}_{ol}\textit{ with filter}$.

The results are reported in Fig.~\ref{fig:chart_stress_outlier}. We observe that the errors are relatively high when the outliers are added to the input scans without filtering, especially for $E_t$. Such amount of random outliers will confuse the network when distinguishing the part poses, thus lead to high errors. However, when we apply a filter to the data contaminated with outliers, the errors remain very close compared to those of the results on clean data, meaning that our method is able to perform well on the data cleaned up by a simple and automatic outlier removal filter.


\subsection{Ablation studies} \label{sec:ablation}

To justify the network structure and loss functions designed in our method, we perform several ablation studies for our CTF-Net.
For network structure, we take single \textit{R-C flow} and single \textit{C-R flow} to perform the comparison, the results are shown in Table~\ref{tab:ablation_net}. Since both \textit{R-C flow} and \textit{C-R flow} can produce the transformation parameters and completed shapes, we use the same measure as described in Section~\ref{sec:quantitative}.
Since our CTF-Net contains twice parameters comparing to each single flow, for fair comparison, we slightly modify \textit{R-C flow} and \textit{C-R flow} to make them have comparable parameters to ours, denoted as $\textit{R-C}^1$ and $\textit{C-R}^1$.
Specifically, we double the output dimension of each layer (except for the last layer) of all the decoders, and then obtain the network with almost twice parameters as the original single flow network.
In addition, the registration network in our CTF-Net is trained with both partial point clouds and (predicted) complete point clouds and the completion network in our CTF-Net is trained with both partial input point clouds and (predicted) registered point clouds, so we compare our method to each single flow which trained with augmented data.
Specifically, for single \textit{R-C flow}, we train the completion network with both input partial point cloud and combined point cloud after registration, denoted as $\textit{R-C}^2$, while for single \textit{C-R flow}, we train the registration network with both input partial point cloud and predicted complete point cloud, denoted as $\textit{C-R}^2$.

From the results, we can see that for single \textit{R-C flow}, the errors of both rotational and translation is significantly higher than our method, which shows that the direct registration of two non-overlap partial shapes is quite challenging. For the version with double parameters $\textit{R-C}^1$, the errors are quite similar to \textit{R-C flow}. For $\textit{R-C}^2$, we observe that training the completion network with additional input can improve the results of completion comparing to single \textit{R-C flow}, but the errors are still higher than our method.
The errors of single \textit{C-R flow} are also higher than ours, the registration error of $\textit{C-R}^1$ is lower than \textit{C-R}, but the completion error increases. For $\textit{C-R}^2$, the error of registration is lower than \textit{C-R} and $\textit{C-R}^1$, however, the errors are higher than CTF-Net.
The experiments show that the performance of either single flow is not comparable to our CTF-Net, even with the doubled parameters,  and only the combination of two flows with consistency loss can actually improve the results.

\input{figures/table_ablation_net.tex}

\input{figures/table_ablation_loss.tex}

We design four consistency losses after combining the \textit{R-C flow} and \textit{C-R flow} together: $L_{\text{s}}^{\text{O}}$, $L_{\text{s}}^{\text{C}}$, $L_{\text{s}}^{\text{R}}$, $L_{\text{s}}^{\text{T}}$.
To show the effectiveness of our consistency losses, we perform comparisons of our method with and without each loss term, as shown in Table~\ref{tab:ablation_loss}. By comparing the errors of the last five columns, we validate that the consistency losses are able to make the two flows strengthen each other, which leads to the lowest error.
We can see that both registration and completion errors are higher without the reconstruction consistency loss. Removing the parameter consistency loss results in the registration between two parts being less consistent, and thus leads to larger errors in prediction.


