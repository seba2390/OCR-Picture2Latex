\section{Discussion and Future Work} \label{sec:future}

We presented CTF-Net for tele-registration of two partial point clouds. Our method excels where the surfaces represented by point clouds have little or no overlap. The success of our method is attributed to the competence of neural network to learn a prior of a class of shapes, which allows predicting complete shapes from partial observations and registering non-overlapping parts. The key architectural design comes in the form of consistency between the \emph{register-and-complete} and the \emph{complete-and-register} networks.

The consistency of the two network flows encourages the two networks to predict reliably and performs surprisingly well. Given that parts $\mathcal{P}_1$ and $\mathcal{P}_2$ are disjoint, then their completions $\mathcal{S}_1$ and $\mathcal{S}_2$ should agree \textit{without} conditioning. We expect $\mathcal{S}_1$ to trivially agree with $\mathcal{P}_1$ in the overlap regions, but the rest of $\mathcal{S}_1$ is ambiguous. Here, we also expect $\mathcal{S}_1$ to agree with $\mathcal{P}_2$ in the overlapping region, which is a hard task.
Thus, the completion task by itself is ambiguous, while the registration with no overlap is ill-posed. Solving both tasks jointly and consistently, however, makes the problems more well-posed and less ambiguous.

\input{figures/faliure.tex}

{\em Limitations} of data-driven methods are naturally carried over to our method. The range by which our method works well is directly derived by the training data and the capacity of the network, which includes the variety of the expected input geometries and their mutual pre-disposition. 
As shown in Fig.~\ref{fig:faliure}, our CTF-Net may fail in registration prediction if the input pairs are sufficiently ambiguous. The upper row shows a table example, where the matching parts have very similar geometry and the ground truth degree of overlap between two input parts are ambiguous, which can cause different registration results. If we assume that the overlap region is larger, the two parts will be placed closer to each other as the result shown in the figure. 
For the chair shown in the second row, since the complete version of $\mathcal{P}_1$ that captures the leg and seat information can be symmetric, so the back part (complete version) of $\mathcal{P}_2$ can be put on any side to make it a valid chair, which introduce the ambiguity of registration between $\mathcal{P}_1$ and $\mathcal{P}_2$. As shown in the figure, the back in placed on the right side instead of back side of the seat.
Another limitation is that our dataset is generated by sphere cropping, since the ground truth for each network is obtained in this way. However, a more natural way to create training data might be by back-projecting depth images to 3D.
Also, currently, our method is trained category-by-category, which may limit the generality. We are planning to enable cross-category training by improving the network structures and loss functions in the future.
Further, our current implementation considers the coordinates of the point cloud, and ignores the additional attributes that can be associated with scanned data, like normals and colors. We leave this for future research.

Another direction is to leverage the tele-registration of our method to align the parts from different objects. For example, given a chair without legs and a partial cabinet, one may register them to form a new object that have both the functionality of chair and cabinet. This part-based modeling requires well designed dataset and network.

In the context of continuous scanning (e.g., using Kinect Fusion), data fusion works better in short sequences than on longer ones, where tracking and registration errors accumulate. 
In our tests, we found CTF-Net to
complement scanning performance by successfully stitching shorter bursts of fused results. This emphasizes the importance of tele-registration for disjoint scans in the context of autonomous (or semi-autonomous) robots and drones.
