\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{square, numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{multirow}
% \usepackage[square,numbers]{natbib}
\usepackage{changes}

\bibliographystyle{abbrvnat}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{float}
\floatplacement{figure}{htbp}

\title{Smooth Normalizing Flows}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jonas Köhler\thanks{J.K and A.K. contributed equally to this work.}~~\footnotemark[2] \And Andreas Krämer\footnotemark[1]~~\footnotemark[2] \And Frank Noé~\footnotemark[2]~~\footnotemark[3]~~\footnotemark[4] \AND
  ~ \\
  \footnotemark[2] ~ Department of Mathematics and Computer Science, Freie Universität Berlin \\
  \footnotemark[3] ~ Department of Physics, Freie Universität Berlin \\
  \footnotemark[4] ~ Department of Chemistry, Rice University, Houston, TX \\
  \texttt{\{jonas.koehler, andreas.kraemer, frank.noe\}@fu-berlin.de}
  
%   Jonas Köhler \footnote{\label{equal-contrib}J.K and A.K. contributed equally to this work.}  \\
% %   David S.~Hippocampus
% %   \thanks{Use footnote for providing further information
% %     about author (webpage, alternative address)---\emph{not} for acknowledging
% %     funding agencies.} \\
% %   Department of Mathematics and Computer Science\\
% %   Freie Universität Berlin\\
%   \texttt{jonas.koehler@fu-berlin.de} \\
%   % examples of more authors
%   \And
%   Andreas Krämer \thanks{} \\
% %   Department of Mathematics and Computer Science \\
% %   Freie Universität Berlin \\
%   \texttt{andreas.kraemer@fu-berlin.de} \\
%   % Address \\
%   % \texttt{email} \\
%   \AND
%   Frank Noé \\
%   Department of Mathematics and Computer Science \\
%   Freie Universität Berlin \\
%   \texttt{frank.noe@fu-berlin.de}
%   Department of Physics \\
%   Freie Universität Berlin \\
%   Department of Chemistry \\
%   Rice University \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

% --- TODO: mark equal contribution 

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand{\jonas}[1]{{\color{red} \textbf{jonas:} #1}}
\newcommand{\andreas}[1]{{\color{blue} \textbf{andreas:} #1}}

\newcommand{\TODO}[1]{{\color{orange} \textbf{TODO:} #1 }}
\newcommand{\CITE}[1]{{\color{purple} \textbf{CITE:} #1 }}

\begin{document}

\maketitle




\begin{abstract}
Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to compute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. 
A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. 
In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.
Mixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem.
We demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations. 
\end{abstract}
 
\section{Introduction}

Generative learning using \textit{normalizing flows} (NF) \cite{tabak2010density, rezende2015variational, papamakarios2019normalizing} has become a widely applicable tool in the physical sciences which has e.g. been used for sampling lattice models \cite{nicoli2020asymptotically, nicoli2021estimation, li2018neural, boyda2021sampling, albergo2019flow}, approximating the equilibrium density of molecular systems \cite{noe2019boltzmann, kohler2020equivariant, wu2020snf, xu2021learning} or estimating free energy differences \cite{wirnsberger2020targeted, ding2021deepbar}. 

Such models approximate a target density $\mu$ via diffeomorphic maps $f(\cdot; \bm \theta) \colon \Omega \subset \mathbb{R}^d \rightarrow \Omega$ by transforming samples $\bm z \sim p_{0}(\bm z)$ of a base density into samples $\bm x = f(\bm z; \bm \theta)$ such that they follow the push-forward density
\begin{align}
    \bm x \sim p_{f}(\bm x; \bm \theta) := p_{0}\left(f^{-1}(\bm x; \bm \theta)\right) \left|\det \partial_{\bm x} f^{-1}(\bm x; \bm \theta) \right|.
\end{align}
Flows can be trained on data by maximizing the likelihood or via minimizing of the reverse KL divergence $D_{KL}\left[p_{f}(\cdot; \bm \theta)\|\mu\right]$ if $\mu$ is known up to a normalizing constant.

% A lot of recent work on NFs concentrated on improving expressivity of transformations, improving sample quality or satisfying symmetry and topological constraints of the domain.
While NFs are usually introduced as \textit{smooth} diffeomorphisms, most applications like density estimation or sampling only require $C^{1}$-smooth transformations.
Higher-order smoothness of flows has not been discussed so far and can become a challenge as soon as multi-modal transformations on other topologies than $\mathbb{R}^d$ are discussed. 

$C^{k}$-smoothness of NFs with $k > 1$ is especially important for applications in physics. Physical models usually come in the form of differential equations, where the derivatives bear a physical meaning that is often crucial to fit, evaluate, or interpret the model. Thus, the construction of expressive, smooth flow architectures will likely open up new avenues of research in this domain. 

\paragraph{Boltzmann Generators}
A recent application of NFs to a physical problem are Boltzmann Generators (BG) \cite{noe2019boltzmann}, which we see as the main application area for the methods introduced in this paper. They are generative models trained to sample conformations of molecules in equilibrium, which follow a Boltzmann-type distribution $\mu(\bm x) \propto \exp(-u(\bm x))$. Here $u$ is the dimensionless potential energy defined by the molecular system and the thermodynamic state (e.g. Canonical ensemble at a certain temperature) it is simulated in. BGs can be trained by a combination of MLE on possibly biased trajectory data and simultaneous minimization of the reverse KL divergence to the target density $\mu(\bm x)$. This bi-directional training scheme can achieve sampling of low-energy equilibrium structures. After training, BGs can be used e.g. for importance sampling or for providing efficient proposals when being used in MCMC applications \cite{sbailo2021neural}. 

In the context of BGs, the negative gradient of the log push-forward density with respect to $\bm x$ corresponds to the atomistic forces. Access to well-behaved forces is pivotal in most classical molecular computations: they simplify optimization of molecular models, drive molecular dynamics (MD) simulations, enable computations of macroscopic observables, and facilitate multiscale modeling.

In this work, we will primarily focus on two important implications of smooth flow forces. First, we show that they enable the training of NFs via force matching (FM), i.e. minimizing the force mean-squared error with respect to reference data. In combining FM with density estimation, NFs are pushed to match the distributions and their derivatives, which implicitly leads to favorable regularization. Second, we apply flow forces to drive dynamics simulations. Such simulations are required in many applications to not only sample the target distribution but also compute dynamical observables such as transition rates or diffusivities.


\paragraph{Respecting Topological Constraints} 
Many physical models operate on nontrivial topologies, i.e. the $d$-dimensional hyper-torus $\mathbb{T}^d$, which can become an obstacle in constructing smooth flows.

An important example are BGs for peptides and small proteins which require an internal coordinate (IC) transformation to achieve low-energy sampling of structures \cite{noe2019boltzmann}. This non-trainable layer transforms Euclidean coordinates $\bm x \in \mathbb{R}^{n \times 3}$ into a representation given by distances $\bm d \in [a_{1}, b_{1}] \times \ldots \times [a_{n-1}, b_{n-1}]$, angles $\bm \alpha \in [0, \pi]^{n-2}$ and dihedral torsion angles $\bm \tau \in \mathbb{T}^{n-3}.$ 
As molecular energies are often invariant to translation and rotation, IC transformations are useful as they are bijective with the equivalence class of all-atom coordinates that are identical up to global rotations and translations. The learning problem can then be reduced to modeling the joint distribution $\mu(\bm d, \bm \alpha, \bm \tau)$ which is supported on the nontrivial topological space $\mathcal{X}_{\text{IC}} := \mathbb{I}^{2n-3} \times \mathbb{T}^{n-3}$, where $\mathbb{I} = [0,1]$ denotes the closed unit interval.

Recent work \cite{noe2019boltzmann, wu2020snf} suggested to model the density within an open set $\Omega \subset\mathcal{X}_{\text{IC}}$, leverage $C^{\infty}$-smooth normalizing flows defined on $\mathbb{R}^d$ and then prevent significant mass around singular points using regularizing losses. Such an approach however can lead to bias and ill-behaved training and requires a-priori knowledge of the support of the densities. Later work \cite{dibak2020temperature} approached the problem using $C^{1}$-smooth splines flows which leads to accurate samples, however results in broken forces.

To overcome these limitations while still benefiting from the merits of prior work, such as bi-directional training and fast forward/inverse transformations we formulate the following desiderata for flow transformations on-top of IC layers:
(A) They must have support on $\mathbb{I}^{d}$ and $\mathbb{T}^d$.
(B) They should be $C^{\infty}$-smooth.
(C) They must allow bi-directional training.
(D) Forward and inverse direction must be efficient to evaluate.

% \textcolor{red}{[FN: this list could be inline to save space if needed]}

% \begin{enumerate}[label=(\Alph*)]
%     \item They must have support on $\mathbb{I}^{d}$ and $\mathbb{T}^d$.
%     \item They should be $C^{\infty}$-smooth.
%     \item They must allow bi-directional training.
%     \item Forward and inverse direction must be efficient to evaluate.
% \end{enumerate}

Satisfying (D) can be achieved using coupling layers \cite{dinh2014nice, dinh2016rnvp}. This reduces the problem to finding element-wise conditional transformations satisfying (A-C).




% \TODO{
% Current architectures ...
% \begin{itemize}
%     \item augmentation/SNF -> noisy forces
%     \item affine flows -> weird forces
%     \item spline flows -> discontinuous forces
%     \item Moebius -> only on circles
%     \item nODE, Res.Flows etc. -> too expensive
%     \item mixtures -> not defined on circles; no mixed KL/NL training (yet!)
% \end{itemize}

% Other applications?
% \begin{itemize}
%     \item Can we (in principle) use smooth flows for training via score matching or is that a stupid idea?
%     \item Is there any cool application of the forces in SNFs? Something like: instead of propagating with a weighted average of target energy and prior energy we propagate with a weighted average of target energy and some intermediate flow layer.
% \end{itemize}


% % Our Contributions are thus given by
% % \begin{itemize}
% %     \item A novel flow architecture based on smooth mollifiers that is capable to model $C^{\infty}$-smooth densities on $(0,1)^d \cong \mathbb{R}^d, [0,1]^d$ and $\mathbb{T}^d$.
% %     \item A novel algorithm allowing to optimize flows in the inverse direction even if the inverse can only be obtained by black-box root-finding methods, such as a bisection search.
% %     \item Showing that these methods allow \TODO{what we can show in experiments}.
% % \end{itemize}

% }

\paragraph{Contributions} 
In this work we propose the following novelties:
\begin{itemize}
    \item We present a new $C^{\infty}$-smooth transformation with compact support which can simultaneously be used for expressive transformations on $\mathbb{I}^{d}$ as well as the hypertorus $\mathbb{T}^d$. This satisfies (A) and (B).
    \item 
    We present novel algorithm which allows optimizing non-analytic inverse directions of flows that can only be evaluated via black-box root finding methods. This satisfies (C).
    \item We show that training of smooth flows through combinations of force matching, density estimation, and energy-based training can achieve nearly perfect equilibrium densities.
    \item We show that forces of such smooth flows can be used in dynamical simulations.
\end{itemize}
% \section{Using normalizing flows as physical force-field}

% \paragraph{Normalizing Flows}
% Normalizing Flows \cite{tabak2010density, rezende2015variational, papamakarios2019normalizing} (NF) are generative models approximating a target distribution $q$ via diffeomorphic maps $f_{\theta} \colon \Omega \subset \mathbb{R}^d \rightarrow \Omega$ by transforming samples $\bm z \sim \rho_{0}(\bm z)$ of a base distribution into samples $\bm x = f_{\theta}(\bm z)$ such that they follow the push-forward distribution
% \begin{align}
%     \bm x \sim \rho_{f_{\bm \theta}}(\bm x) = \rho_{0}\left(f^{-1}_{\bm \theta}(\bm x)\right) \left| \frac{\partial}{\partial \bm x} f^{-1}_{\bm \theta}(\bm x) \right|.
% \end{align}
% If the target distribution is unknown and only given by samples, $f_{\theta}$ can be trained by maximizing the likelihood on data (MLE) which corresponds to minimizing the forward Kullback-Leibler (KL) divergence $D_{KL}\left[q\|\rho_{f_{\bm \theta}}\right]$.
% If the target distribution is known up to a normalizing constant, training can be done by minimizing the reverse KL divergence $D_{KL}\left[\rho_{f_{\bm \theta}}\|q\right]$.

% \paragraph{Boltzmann Generators}

% The equilibrium distribution of molecules such as peptides is usually given by a Boltzmann-type distribution $q(x) \propto \exp(-u(\bm x))$ where $u$ is the potential energy of the system. Boltzmann Generators (BG) \cite{noe2019boltzmann} use NFs and approximate these distributions combining MLE on biased trajectory data while simultaneously minimizing the reverse KL divergence in a bi-directional training scheme. After training BGs can be used e.g. for importance sampling or for providing efficient proposals when being used in MCMC applications.

% \paragraph{Flow densities as molecular force fields}

% In contrast to arbitrary energy-based models, the potential energy of a molecular system has a semantics that goes beyond purely statistical interpretations. When considered a classical mechanical system, it allows us to compute forces $\bm f = -\nabla_{\bm x} u(\bm x)$ and thus integrate the equations of motion e.g. by running molecular dynamics simulations. \TODO{connect to CG problem, explain smoothness of forces

% \begin{itemize}
%     \item kinetics
%     \item forces during training (e.g. if given biased data, regularized learning?)
% \end{itemize}
% }


% \TODO{Quick primer on the statistics/mechanics duality of potential energies}



% \section{Normalizing Flows for Molecular Conformations}

% BGs for peptides and small proteins have been studied in \citet{noe2019boltzmann}, who introduced a flow layer based on an internal coordinate (IC) transformation  which transforms Euclidean coordinates $\bm x \in \mathbb{R}^{n \times 3}$ into a representation given by distances $\bm d \in [a_{1}, b_{1}] \times \ldots \times [a_{n-1}, b_{n-1}]$, angles $\bm \alpha \in [0, \pi]^{n-2}$ and dihedral torsion angles $\bm \tau \in \mathbb{T}^{n-3}.$ This so-called \textit{Z-matrix} representation is non-diffeomorphic for all-atom coordinates but diffeomorphic for the equivalence classes of structures that are identical up to global rotations and translations. The density estimation / variational inference problem can then be reduced to modeling the joint distribution $p(\bm d, \bm \alpha, \bm \tau)$. While density estimation in all-atom representation could be done with any flow defined on $\mathbb{R}^{3n}$, density estimation in the IC representation requires respecting the topology of $[0,1]^{2n-3} \times \mathbb{T}^{n-3}$.

% In recent work, such as \citet{noe2019boltzmann, wu2020snf}, this joint distribution has been modeled using affine coupling layers in combination with regularizing losses forcing the density to be concentrated within an open interval $(a, b)$ and thus avoiding significant mass around the singular points. This however can cause problems \TODO{some argument why that is bad}.

% In combination we have the following desiderata for flow transformations being on-top of IC layers:
% \begin{enumerate}
%     \item[R1] having support on $[0, 1]^d$ and $\mathbb{T}^d$
%     \item[R2] being $C^{\infty}$-smooth
%     \item[R3] allowing for bi-directional training
%     \item[R4] being able to express multi-modal densities
%     \item[R5] efficient evaluation of forward / inverse
%     \item[R6] being able to capture correlations
% \end{enumerate}

% Satisfying (R6) as well as (R5) can be achieved using coupling layers \cite{dinh2014nice, dinh2016rnvp}. This reduces the problem to finding scalar/diagonal conditional transformations satisfying (R1-4).

% In order to satisfy topological constraints in combination with the smoothness requirement on the flow forces we thus need flows which can be trained bi-directionally in an efficient way

% In the following section we discuss merits and shortcomings of existing approaches to flows defined for the topological spaces\footnote{Please note that while $(0,1)^d$ and $\mathbb{T}^d$ are smooth manifolds, $[0,1]^d$ is none. By letting $(0,1)^d$ be its interior and $\{0, 1\}^d$ be its boundary we can see $[0,1]^d$ becomes a smooth manifold with boundary.} $(0,1)^d, [0, 1]^d,$ and $\mathbb{T}^d$ and introduce our new approach based on smooth mollifiers which are capable of modeling $C^{\infty}$-smooth densities with support on all three of them. 

\section{Related Work}
% \textcolor{red}{[FN: open with a statement such as: while related work exists for each of the requirements A-D above, none of them provides a framework that addresses all of them. Specifically...]}
While related work exist for each of the requirements (A-D) above, none of them provides a framework that addresses all of them.
Specifically, multi-modality in element-wise flow transformations has been approached using splines \cite{durkan2019neural, muller2018neural}, monotonoic polynomials \cite{jaini2019sum, ramasinghe2021robust}, monotonic neural networks which directly approximate an inverse CDF transformation \cite{huang2018neural, de2019block}, mixtures of unimodal transformations \cite{ ho2019flow++, rezende2020normalizing} and stochastic transition steps \cite{wu2020snf, cornish2020relaxing}.

Flows on non-trivial manifolds have been discussed for hypertori and -spheres \cite{rezende2020normalizing}, Lie groups \cite{falorsi2019reparameterizing, boyda2021sampling}, hyperbolic spaces \cite{bose2020latent} as well as on general manifolds \cite{falorsi2021continuous, gemici2016normalizing, mathieu2020riemannian, lou2020neural, brehmer2020flows, kalatzis2021multi}.
Applications of flows for molecular systems include approximation of the equilibrium density \cite{noe2019boltzmann, wu2020snf, kohler2020equivariant}, generation of molecular conformations \cite{xu2021learning, satorras2021n} and free energy differences \cite{wirnsberger2020targeted, ding2021deepbar}.
Using forces to train neural network potentials of molecules was done e.g. in  \cite{wang2019machine, husic2020coarse}.

Backpropagation through black-box functions was generally discussed in \cite{grathwohl2017backpropagation}. More related to our work is \citet{bai2019deep} who discuss training models whose outputs are obtained through fix-point equations. Finally \citet{shirobokov2020black} discuss backpropagation through black-box simulators using surrogate models.

Using force-matching for training generative models is rarely used in machine learning due to absence of an (unnormalized) target density. However, a common method to train unnormalized energy-based models purely on data is given by \emph{score-matching} (SM) \cite{hyvarinen2005estimation, song2019generative}. SM assumes an unknown density for the data distribution and attempts to match forces implicitly, e.g. via sliced \cite{song2020sliced} or denoising \cite{vincent2011denoising} score-matching. A survey on score-matching and its relation to other approaches of training energy-based models on data is e.g. given in \citet{song2021score}.

We discuss related work on flows for $\mathbb{I}$ and the unit circle in detail in  Section \ref{sec:bump-functions}.

\section{Incorporating Forces into Flow Training}
NFs are most commonly trained by minimizing the negative log likelihood (NLL), 
\begin{align}
    \mathcal{L}_{\mathrm{NLL}}(\bm \theta) := -\mathbb{E}_{\bm x \sim \mu(\bm x)} [\log p_{f}(\bm x; \bm\theta)] = 
    D_\mathrm{KL}[\mu||p_{f}(\cdot; \bm\theta)]+ \mathrm{const},
\end{align}
or by minimizing the reverse KL divergence (KLD)
\begin{align}
    \mathcal{L}_{\mathrm{KLD}}(\bm \theta) :=
    D_\mathrm{KL}[p_{f}(\cdot; \bm\theta)||\mu] + \mathrm{const}.
\end{align}
BGs as discussed in \citet{noe2019boltzmann} use a convex combination of the two in order to avoid mode-collapse while still being able to achieve low-energy samples.
If reference forces $\mathbf{f}(\bm x) = - \partial_{\bm x} u(\bm x)$ corresponding to samples $\bm{x}$ are available and $f(\cdot; \bm \theta)$ is at least $C^{2}$-smooth, the optimization can naturally be augmented by the force mean-squared error:
\begin{align}
    \mathcal{L}_{\mathrm{FM}}(\bm \theta) := \mathbb{E}_{\bm x \sim \mu(\bm x)} \left[\left\| \mathbf{f}(\bm x) - \partial_{\bm{x}} \log p_{f}(\bm x; \bm \theta) \right\|_{2}^{2}\right].
\end{align}
As was shown in \citet{wang2019machine} such \textit{force-matching} can lead to unbiased potential surfaces even if samples are not sampled from equilibrium. Similarly to \citet{noe2019boltzmann} we can thus define a loss function for smooth flows as the convex combination
\begin{equation}
    \mathcal{L}(\bm \theta) = \omega_n \mathcal{L}_{\mathrm{NLL}}(\bm \theta)
        + \omega_k \mathcal{L}_{\mathrm{KLD}}(\bm \theta)
        + \omega_{\mathrm{f}}  \mathcal{L}_{\mathrm{FM}}(\bm \theta).
\label{eq:loss}
\end{equation}

% \TODO{I think the loss function over the model ensemble should also be unbiased under certain regularity conditions. So in principle we should be able to add a fourth term. But probably we just leave this topic alone.}


\section{Smooth flows on the closed interval and the unit circle}
\label{sec:bump-functions}

We now discuss how to achieve smooth flows on $\mathbb{I}^d$ and $\mathbb{T}^{d}$. To unify the discussion we consider the unit circle $S^1$ to be the quotient space $\mathbb{I}/\sim$ using the relation $x \sim x' \Leftrightarrow (x = x') \lor (x = 0 \land x' = 1) \lor (x' = 0 \land x = 1)$. The $d$-dimensional hypertorus is given by the direct product $\mathbb{T}^d = S^{1} \times \ldots \times S^{1}$. Following the usual definition we say that $f$ is $C^{k}$-smooth on a compact interval $[a, b]$ iff there exists a $C^{k}$-smooth  continuation of $f$ on an open set $\Omega \supset [a,b]$. 

\paragraph{Smooth flows on $\mathbb{I}$}
Any smooth diffeomorphism on $\mathbb{R}$ or an open interval $(a, b) \supset \mathbb{I}$ can be restricted to $\mathbb{I}$ and re-scaled to satisfy this requirement. Let $f \colon \Omega \rightarrow \Omega$ be $C^{k}$-smooth for some open set $\mathbb{I} \subset \Omega \subset \mathbb{R}$. Then $\tilde f(x) = (f(x) - f(0)) / (f(1) - f(0))$
% \begin{align}
%     \tilde f(x) = \frac{f(x) - f(0)}{f(1) - f(0)}
% \end{align}
defines a $C^{k}$-smooth diffeomorphism on $\mathbb{I}$. Simple $C^{\infty}$ transformations on $\mathbb{R}$ are given by affine transformations \cite{dinh2016rnvp}. After re-scaling any affine map will result in the same identity mapping and thus will not be able to model complex densities. Powerful and computationally efficient multi-modal transformations on $\mathbb{I}$ can be achieved using rational-quadratic splines \cite{durkan2019neural}. Those are $C^{1}$-smooth and thus will result in discontinuous forces which can be disadvantageous for physical applications. Possible other $C^{\infty}$-smooth candidate transformations with analytic forward evaluation are given by mixtures of logistic transformations \cite{ho2019flow++} or deep sigmoid/deep dense sigmoid flows \cite{huang2018neural}. Those are only continuous on $(0,1)$ and thus special care has to be taken to avoid problematic behavior on the tails. Finally, multi-modal $C^{\infty}$ transformations on $\mathbb{R}$ can be achieved using non-analytic methods \cite{chen2018neural, wehenkel2019unconstrained}. Yet, computational costs (solving and backpropagating over ODEs, inverting quadrature integrations via bisections) and numerical accuracy (non-anlytic forward passes) limit their applicability e.g. when used in deep coupling layers which are required to capture multivariate correlations or when trying to match densities up to high accuracy e.g. as necessary for molecular modeling.% within the ICs.

\paragraph{Smooth flows on $S^1$}
Flows on the hypertorus were discussed in \citet{rezende2020normalizing} who introduced necessary conditions for the transformations to define $C^{0}$-continuous densities, such that they can be used in density estimation tasks.
In their work \citet{rezende2020normalizing} introduced three candidates satisfying this condition: mixtures of non-compact projections (NCP), rational-quadratic circular spline flows (CSF) and mixtures of Moebius transformations (MoMT). While NCPs and CSFs satisfy $C^1$-continuity and thus render forces discontinuous, only MoMTs define smooth densities. While MoMTs trivially also define $C^{\infty}$-flows on $\mathbb{I}$ their periodicity would be limiting when applied to non-periodic densities.

\paragraph{Smooth compact bump functions}
In addition to this previous work we leverage a third way to construct smooth transformations which works for both $\mathbb{I}$ and $S^1$. Here we follow a general construction principle for smooth bump functions e.g. as explained in \citet{Tu2008}. All proofs can be found in the supplementary material.

First, define a $C^{k}$-smooth and strictly increasing ramp function $\rho \colon \mathbb{I} \rightarrow \mathbb{I}$ with $\rho(0)=0$ and $\rho(1)=1$. Second, define the generalized sigmoid 
\begin{align}
    \sigma[\rho](x) := \frac{\rho(x)}{\rho(x) + \rho(1 - x)}.
\end{align}
Then $\sigma[\rho]$ will be a $C^{k}$-diffeomorphsim on $\mathbb{I}$ and furthermore all derivatives up to order $k$ vanish at the boundary.
% Then $\sigma[\rho]$ will be a $C^{k}$-diffeomorphsim on $[0,1]$ and furthermore for all $l \leq k$ we have 
% \begin{align}
%     \lim_{x \rightarrow 0} \left(\frac{\partial}{\partial x}\right)^{l} \sigma[\rho](x) = \lim_{x \rightarrow 0}\left(\frac{\partial}{\partial x}\right)^{l} \sigma[\rho](x)= 0.
% \end{align}
The first derivative defines a non-negative smooth bump function with
% $\text{supp}\left(\frac{\partial}{\partial x}\sigma[\rho]\right) = [0,1]$ 
support $[0,1]$
and maximum at $0.5$.
We can introduce a concentration parameter $a \in \mathbb{R}_{>0}$  and a location parameter $b\in [0,1]$, which makes $g(x) := \sigma[\rho]\left(a \cdot (x - b) + \tfrac{1}{2}\right)$
% \begin{align}
    % g(x) &:= \sigma[\rho]\left(a \cdot (x - b) + \tfrac{1}{2}\right)
% \end{align}
a smooth bijection from $[-\frac{1}{2a} + b, \frac{1}{2a} + b]$ to $[0, 1]$ with all values and higher-order derivatives vanishing outside the domain.
Furthermore, by introducing $c \in (0, 1]$ and setting 
\begin{align}
    f(x) &:= (1 - c) \cdot \left( \frac{g(x) - g(0)}{g(1) - g(0)} \right) + c \cdot x \label{eq:bump-function-transformation},
\end{align}
we can define flexible unimodal $C^k$-diffeomorphisms on $[0,1]$. When used within coupling layers, we let $a, b, c$ be the output of some not further constrained neural network.


\paragraph{$C^{k}$ and $C^\infty$ ramp functions}
There are many possible choices for ramp functions satisfying above mentioned properties. A simple $C^{k}$ ramp function is given by the $k$-th order monomial $\rho(x) = x^{k}$. More interestingly, $C^{\infty}$ smoothness on $[0,1]$ can be achieved using the ramp $\rho(x) = \exp\left(-\frac{1}{\alpha\cdot x^\beta}\right)$ for $x > 0$ and $\rho(x) = 0$ for $x \leq 0$.
% \begin{align}
%     \rho(x) := \begin{cases}
%         \exp\left(-\frac{1}{\alpha\cdot x^\beta}\right) & x > 0 \\
%         0 & \text{o.w.}
%     \end{cases},
% \end{align}
Here we set $\alpha > 0$ and $\beta > 1$. We make $\alpha$ a trainable parameter. Optimizing $\beta$ is possible in principle, yet fixing $\beta \in \{1, 2\}$ and treating it as a hyper-parameter stabilized training and led to better results.

\paragraph{Smooth and efficient circular wrapping}
As discussed in \citet{rezende2020normalizing} and \citet{falorsi2019reparameterizing} we can turn any $C^{k}$-smooth density $p(x)$ with support on $\mathbb{R}$ into a $C^{k}$-smooth density on $S^1$ using the marginalization $\tilde p(x) = \sum_{k \in \mathbb{Z}} p(x + k)$.
% \begin{align}
%     \tilde \rho(x) = \sum_{k \in \mathbb{Z}} \rho(x + k).
% \end{align}
This construction is generally problematic due to two reasons: (i) non-vanishing tails (e.g. if $p(x)$ is Gaussian) require evaluating an infinite series which in most cases has no analytic expression and can be numerically challenging (ii) smooth densities with compact support in some interval $[a, b]$, for example sigmoidal transforms, can introduce non-smooth behavior at the interval boundaries.

Smooth and compactly supported transformations as introduced above do not suffer from this: (i) due to compact support the series will always have a finite number of non-vanishing contributions, (ii) as the transformations are $C^{k}$ on all of $\mathbb{R}$ and all derivatives are compactly supported wrapping will always result in a $C^{k}$-smooth density on $S^1$.

\paragraph{Multi-modality via mixtures}
Similarly, as discussed in prior work \cite{ho2019flow++, rezende2020normalizing} any convex sum of $C^{k}$-diffeomorphisms on $\mathbb{I}$ defines a $C^{k}$-diffeomorphism on $\mathbb{I}$. Thus we can combine multiple of those unimodal transformations defined in Eq. \eqref{eq:bump-function-transformation} to obtain arbitrarily complex multi-modal transformations on $\mathbb{I}$ and $S^1$, see Fig. \ref{fig:schema}.
% To wrap such a compact bump function around the circle let $a > 1$ and set
% \begin{align}
%     z & := a \cdot ( x - b ) + \tfrac{1}{2} \text{mod} ~ 1\\
%     \tilde g(z) &:= \sigma[\rho]\left(z\right) \\
%     \tilde f(x) &:= (1 - c) * \tilde g(z) + \left\lfloor ( x - b ) + \tfrac{1}{2a} \right\rfloor
% \end{align}
% we can also define $C^{k}$-diffeomorphisms  on $S^1$.
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/bumps_normal.pdf}
    \includegraphics[width=0.49\textwidth]{figures/bumps_wrapped.pdf}
    \caption{Construction of mixture transformations on compact intervals (left) and hypertori (right). The upper and lower row depict probability densities and cumulative distribution functions, respectively. Multiple unimodal bump functions [a) and d)] are added to a small but finite density [b) and e)] and combined to yield bijective multimodal transformations [c) and f)].}
    \label{fig:schema}
\end{figure}
% \TODO{Details of circle wrapping into the appendix}

% \TODO{
% \begin{enumerate}
%     \item Explain possible approaches to model densities on $[0,1]^{d}$
%     \begin{enumerate}
%         \item Use spline flows. Results in non-continuous forces.
%         \item Wrap in some open interval $(-\epsilon, 1+\epsilon)^{d}$ and use density estimator on the open interval. Results and problematic tails on the boundary.
%     \end{enumerate}
%     \item Explain possible approaches to model densities on $\mathbb{T}^{d}$
%     \begin{enumerate}
%         \item Refer to results of \cite{rezende2020normalizing}. Only Moebius flows satisfy constraints.
%     \end{enumerate}
% \end{enumerate}
% }

% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c}
%             & \multicolumn{2}{c}{Analytic} & \multicolumn{3}{c}{Smoothness} \\
%           & forward & inverse & $(0,1)^d \cong \mathbb{R}^d$ & $[0,1]^d$ &  $\mathbb{T}^d$ \\
%          \hline
%          Unconstrained monotonic flows \cite{wehenkel2019unconstrained} & \xmark$^{*}$ & \xmark$^{*}$ & $C^{\infty}$ & \textbf{?}  & \textbf{?}  \\
%          Residual Flows \cite{chen2018neural, behrmann2018invertible, chen2019residual}& \xmark$^{*}$ & \xmark$^{*}$ &  $C^{\infty}$ & \textbf{?} & \textbf{?} \\ 
%          Neural Autoregressive Flows \cite{huang2018neural} & \cmark & \xmark$^{**}$ & $C^{\infty}$ & \xmark  & \xmark\\
%          Neural \cite{rezende2020normalizing} \& Circular \cite{durkan2019neural, wirnsberger2020targeted} Spline Flows & \cmark & \cmark & $C^{1}$ & $C^{1}$ & $C^{1}$\\
%          \hline
%          Mixture of Sigmoids \cite{ho2019flow++} & \cmark & \xmark$^{**}$ & $C^{\infty}$ & \xmark & \xmark \\
%          Mixture of Moebius Transformations \cite{rezende2020normalizing} & \cmark & \xmark$^{**}$ & \xmark & \xmark & $C^{\infty}$\\
%          Mixture of NCPs \cite{rezende2020normalizing}&  \cmark &  \xmark$^{**}$ & \xmark &  \xmark & $C^{1}$ \\
%          \hline
%          \textbf{Ours} & \cmark & \xmark$^{**}$ & $C^{\infty}$ & $C^{\infty}$ & $C^{\infty}$ \\
%     \end{tabular}
%     \caption{An overview over recent work modeling multi-modal one-dimensional inverse CDF transformations for downstream use in autoregressive flow architectures. An \textbf{?} indicates that such model is possible in principle but yet has not been realized. An * indicates that while being non-analytic there exist approximate methods to compute gradients and train the method along the direction of evaluation. An ** indicates that the model can be optimized using the black-box approach introduced in section \ref{sec:black-box-inverse-optimization}.}
%     \label{tab:my_label}
% \end{table}


% \jonas{
% Roadmap for this part: show that we can construct smooth flows on the hyper-torus with the following cooking recipe:
% \begin{enumerate}
%     \item Show that we can get a smooth CDF transform on $[0,1]$ based on a smooth ramp (Theorem \ref{thm:smooth-sigmoid}).
%     \item Apply it with the smooth ramp defined in lemma \ref{lemma:smooth-ramp}.
%     \item Show that we can shift and scale it and wrap it around a circle without losing the smoothness property (TODO).
%     \item Show that we can create smooth forces by regularizing the density with some $\epsilon$ (lemma \ref{lemma:smooth-forces-on-circle}).
%     \item Show how we can make this a full parameterizable multi-modal transform by taking a convex combination (TODO).
% \end{enumerate}
% }




\section{Optimizing non-analytic inverse flows}
\label{sec:black-box-inverse-optimization}

A drawback of mixture-based flows is their lack of an analytic inverse. Prior work such as \cite{ho2019flow++, rezende2020normalizing} suggested to rely on black-box inversion methods like a bisection search to sample from trained models. 
While this leads to accurate samples, the discrete nature of such black-box oracles does not allow to minimize $\mathcal{L}(\bm \theta)$ as defined in Eq. \eqref{eq:loss}.
% \paragraph{Optimizing black-box inversions via the inverse function theorem}

A possible remedy to this can be derived using the \textit{inverse function theorem}. Let $f(\cdot; \bm\theta) \colon \Omega \subset \mathbb{R} \rightarrow \Omega$ be a scalar diffeomorphism and let furthermore $ x = f^{-1}(y; \bm\theta)$ be obtained via a black-box inversion algorithm. To minimize losses depending on $x(y; \bm \theta)$ or $\log |\partial_{y} x(y; \bm \theta)|$ we need to compute gradients with respect to $y$ and $\bm \theta$. Here we derive the following relations for the derivatives (all details in supplementary material):
\begin{align}
    \partial_{y} x(y; \bm \theta) &= \left( \partial_{x} f( x; \bm \theta) \right)^{-1} \label{eq:bp1}\\
    \partial_{\bm \theta} x(y; \bm \theta) &= -\left(\partial_{x} f( x; \bm \theta)\right)^{-1} \partial_{\bm\theta} f( x; \bm \theta)\\
    \partial_{y} \log \left| \partial_{y} x(y; \bm \theta) \right| &= -\left(\partial_{x} f( x; \bm \theta)\right)^{-1} \log \left| \partial_{x} f( x; \bm \theta) \right|\\
    \partial_{\bm \theta} \log \left| \partial_{y} x(y; \bm \theta) \right| &= -\left(\partial_{x} f( x; \bm \theta)\right)^{-1} \left( \log \left| \partial_{x} f( x; \bm \theta) \right| \partial_{\bm \theta} f( x; \bm \theta) - \partial_{\bm \theta}\partial_{x} f( x; \bm \theta) \right) \label{eq:bp4}
\end{align}
% \begin{align}
%     \partial_{y} f^{-1}_{\bm \theta}( y) &= \left( \partial_{x} f_{\bm \theta}( x) \right)^{-1} \label{eq:bp1}\\
%     \partial_{\bm \theta} f^{-1}_{\bm \theta}( y) &= -\left(\partial_{x} f_{\bm \theta}( x)\right)^{-1} \partial_{\bm\theta} f_{\bm \theta}( x)\\
%     \partial_{y} \log \left| \partial_{y} f^{-1}_{\bm \theta}( y) \right| &= -\left(\partial_{x} f_{\bm \theta}( x)\right)^{-1} \log \left| \partial_{x} f_{\bm \theta}( x) \right|\\
%     \partial_{\bm \theta} \log \left| \partial_{y} f^{-1}_{\bm \theta}( y) \right| &= -\left(\partial_{x} f_{\bm \theta}( x)\right)^{-1} \left( \log \left| \partial_{x} f_{\bm \theta}( x) \right| \partial_{\bm \theta} f_{\bm \theta}( x) - \partial_{\bm \theta}\partial_{x} f_{\bm \theta}( x) \right) \label{eq:bp4}
% \end{align}
We remark that \eqref{eq:bp1} corresponds to the \emph{implicit reparameterization gradient} as introduced in \citet{figurnov2018implicit}. Using these rules, we can first compute $x$ via a black-box oracle for any input $y$ and then compute all necessary gradients using forward evaluations and derivatives of $f(\cdot; \bm\theta)$. 
Derivatives of $f(\cdot; \bm\theta)$ are usually accessible for computing the log Jacobian of the transformation. We can obtain higher-order derivatives using automatic differentiation.
It is easy to see that this construction extends to multivariate diffeomorphisms with diagonal Jacobian as used in coupling layers.
It can be generalized to arbitrary higher order derivatives using the Faà di Bruno formula, e.g. when aiming to do force matching through a black-box inversion algorithm. As our experiments only require losses involving second order derivatives for the inverse direction we leave this for future work.
Note that Equations \eqref{eq:bp1} - \eqref{eq:bp4} can be applied to any other element-wise flow transformation that lacks an analytic inverse and is not tied to mixture models.

%  \paragraph{Faster inverses via multi-bin bisection}
Another problem of bisection is its sequential execution which can become prohibitively slow during training.
E.g. achieving an error of 10$^{-6}$ requires around 20 iterations. 
To obtain speedup on GPUs we suggest to generalize the classic binary search to searching in a grid of $K$ bins simultaneoulsy (see details in Supp. Mat.). For $K=2$ it results in the usual bisection method. However, by leveraging vectorized execution we can reduce the number of iterations by a factor $O\left(1 / \log K\right)$ at the expense of increasing memory by a factor $O\left(K\right)$. Practical speedup depends on the actual number of parallel workers and thus depending on dimensionality, batch size and number of mixture components the optimal choice of $K$ varies.

% To obtain speedup on GPUs we suggest to generalize the classic binary search by the following grid-search procedure:
% \begin{enumerate}
%     \item Let $[a, b]$ be a closed interval which is known to contain $x$ s.t. $f(x) = y$.
%     \item For $k=0\ldots K$ define $s_{k} = \tfrac{k}{K} (b-a) + a$.
%     \item Find $k$ such that $f(s_{k}) - y < 0$ and $f(s_{k+1}) - y > 0$.
%     \item If $\left|f(s_{k}) - y\right| < \epsilon$ return $x \approx s_{k}$. Else set $a:= s_{k}, ~ b:= s_{k+1}$ and continue with step 2.
% \end{enumerate}
% We can see that $K=2$ results in the usual bisection method. However, each step can be executed in vectorized form. Thus for a fixed precision we can reduce the number of iterations by a factor $O\left(\tfrac{1}{\log K}\right)$ at the expense of increasing memory by a factor $O\left(K\right)$. In practice optimal speedup will depend on the actual number of parallel workers that can be utilized when computing the inverse on a GPU. Depending on dimensionality of the problem, chosen batch size and number of mixture components the optimal choice of $K$ varies.
%  

% \jonas{Roadmap:

% \begin{enumerate}
%     \item Explain the idea as algorithm (TODO).
%     \item Provide the theoretical underpinning (\ref{thm:backward-gradients}).
%     \item Discuss the specific implementation of Newton's method? (using an annealed step size rather than unconstrained gradient steps to avoid numerical overblow).
% \end{enumerate}
% }

\section{Experiments}
The numerical experiments in this work are tailored to show the benefits of smooth flows over non-smooth flows. Therefore, we mainly compare mixtures of bump functions to neural spline flows \cite{durkan2019neural}, which exhibit state-of-the-art generative performance but whose densities are only first-order continuously differentiable.

\subsection{Illustrative Toy Example}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/toy_example.pdf}
    \caption{a) Reference density and corresponding force field as approximated by b) a $C^{1}$-smooth NSF and c) a $C^{\infty}$-smooth flow using the mixture of bump functions introduced in Sec. \ref{sec:bump-functions}.}
    \label{fig:toy}
\end{figure}
To highlight the difference in terms of smoothness, the two flow architectures were applied to a two-dimensional toy example. Fig. \ref{fig:toy} a) depicts the reference energy and forces on samples from the ground-truth distribution. Both flows were trained through density estimations on those samples (see SI for details). The smooth flows and spline flows matched the density well, which demonstrates their expressivity. However, the force field of the spline flow contained dramatic outliers, while the forces of the smooth flow matched the regular behavior of the reference forces.


\subsection{Runtime Comparison}


While the rational-quadratic splines are analytically invertible, mixture transformations obviously increase computational cost of the inversion due to the iterative root-finding procedure. To quantify this gap in performance, the inverse evaluation of a spline flow was compared with a modified version, where the analytic inverse was replaced by the multi-bin bisection from Section \ref{sec:black-box-inverse-optimization}. The performance of the bisection was evaluated for different numbers of bins $K=2^m,\ m=1,\dots,8,$ and the most efficient $K$ was picked for each input size (``dim''). Both transformations were coupled to a conditioner with the same input size (dim) and 64 hidden neurons. 

For small tensor dimensions (2-32), the optimal multi-bin bisection employed up to 256 bins on the GPU, which resulted in only a factor of 2-3 slowdown compared to analytic inversion. For larger dimensions (2048), the parallelization over multiple bins became less effective, leading to one order of magnitude difference in computational cost. The compute times and optimal bin sizes were comparable for the inverse network pass and its backward-pass. More details on these runtime comparisons can be found in the supplementary material.

% \subsection{Boltzmann Generators}


\subsection{Smooth Flows Enable Boltzmann Generator Training through Force Matching}
\label{sec:smooth_fm}
To demonstrate the advantages of smooth flows, we train a Boltzmann generator (BG) for a small molecule, alanine dipeptide, which is described in the supplementary material. %in Appendix \ref{sec:ala2}. 
This is a common test case for molecular computations and was previously considered for sampling with normalizing flows in \cite{wu2020snf, dibak2020temperature, kramer2020training}. The molecular system has 60 degrees of freedom (global rotation and translation excluded). Its potential energy surface is highly sensitive to small displacements \cite{kramer2020training} and contains singularities. 

\label{sec:fm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/ala2_figure.pdf}
    \caption{a) Generated sample structures. b) Torsion distribution after training through root-finding. c) Smooth normalizing flow trained on alanine dipeptide through a combination of force matching and density estimation. The top and bottom row show the performance on 10,000 samples each from the holdout test set and from the flow, respectively. Left: joint distribution of backbone torsion angles. 
    Center: scatter plot of flow vs. target force components.
    Right: energy histograms for the flow energy and the target energy. (Flow energies were shifted by a constant so that the minimum energy matched with the minimum energy from the molecular potential). 
    }
    \label{fig:ala2_bg}
    \label{fig:ala2_samples}
    \label{fig:invbg}
\end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/structures_inverse_phi_psi.pdf}
%     \caption{a) Generated sample structures. b) Torsion distribution after training through root-finding.}
    
% \end{figure}

In previous work \cite{wu2020snf, dibak2020temperature, kramer2020training}, BGs for alanine dipeptide used stochastic augmentation of the base space. The introduction of noise variables facilitates creating expressive normalizing flows but prevents computation of deterministic forces. Some have also used spline flows to sample molecular configurations \cite{wirnsberger2020targeted, ding2021deepbar}.
%While neural spline flows have proven to be effective samplers for molecular configurations, their forces are inherently non-smooth as discussed above.
%In contrast to spline flows, smooth flows can be trained using force matching to complement the customary density estimation. 

As a proof-of-concept for the force matching loss, we trained smooth flows for alanine dipeptide using a 1000:1 weighting between the force matching residual and the negative log likelihood, 
% see Appendices \ref{sec:ala2bg} and \ref{sec:ala2training} 
see supplementary material
for details on the flow and training setup. No stochastic augmentation or energy-based training was used and no reweighting was conducted for postprocessing.

Figure \ref{fig:ala2_bg} c) compares the flow distribution with the target Boltzmann distribution on the test data from MD and on flow samples. The left column shows that the flow has almost perfectly learned the nontrivial joint distribution between the two backbone torsion angles $\phi$ and $\psi$, including the sparsely populated metastable region around $\phi\approx 1.$ In contrast to previous work that used affine coupling layers \cite{wu2020snf}, the modes are cleanly separated, see also Fig. \ref{fig:affine_bg} for a direct comparison. 
The center column compares flows forces with target forces. Those forces are highly sensitive to small perturbations of the molecular configurations. Nevertheless, they matched up to a root-mean square deviation of 25 and 46 $k_B T$/nm in the target ensemble and the generated ensemble, respectively. Note that the training was stopped after 10 epochs, where the validation force matching residual had not yet fully converged, so that even further improvements may be possible with longer training or extensive hyperparameter optimization. 
A more detailed analysis of the forces and sampling efficiency of the BGs is shown in supplementary material, Figure \ref{fig:forces_se}. Smooth flows trained by a combination of density estimation and FM attain a favorable sampling efficiency of 42\%, compared to 25\% and $<1$\% for spline and RealNVP flows, respectively.

Finally, the right column of Figure \ref{fig:ala2_bg} c) depicts the distribution of flow and target energies evaluated on the same samples. The flow energies were shifted so that the minimum energy matched with the molecular mechanics energy. It has not escaped our notice that this constant offset (189 $k_B T$ on the test set and 190 $k_B T$ on the flow samples) corresponds to the log partition function, a quantity whose intractability has complicated research in statistical mechanics for decades. 

% \begin{figure}
%     % \begin{subfigure}[t]{0.63\linewidth}
%         % \centering
%         \includegraphics[width=0.49\linewidth]{ala_samples.png}
%         % \caption{Generated sample structures.}
%         \label{fig:ala2_samples}
%     % \end{subfigure}
%     % \begin{subfigure}[t]{0.36\linewidth}
%         % \centering
%         \includegraphics[width=0.49\linewidth]{phi_psi_inverse.png}
%         % \caption{Backbone torsion distribution after training through root-finding.}
%         \label{fig:invbg}
%     % \end{subfigure}
%     % \caption{\TODO{Place as 2 Fig.s without a common caption.}}
%     \label{fig:ala2_samples}
% \end{figure}


The energy distribution of the flow tracked the ground truth exponential distribution almost perfectly even though no target energies or forces were evaluated on the flow samples during training. This close-to-perfect match demonstrates that including force information into the training process presents an efficient means for regularization. 
Figure \ref{fig:ala2_samples} a) shows representative conformations generated by the BG, which confirm the high quality of the molecular structures.


\subsection{Boltzmann Generator Training through Black-Box Root-Finding}

\begin{table}[htbp]
    \centering
    \caption{Test set metrics of alanine dipeptide training with different losses: negative log likelihood (NLL), force matching error (FME), and reverse Kullback-Leibler divergence (KLD). Weighted loss functions were used as defined in Eq. \eqref{eq:loss} with weight factors $\omega_k,$ $\omega_{\mathrm{f}}$ and $\omega_n = 1-\omega_k-\omega_{\mathrm{f}}.$ Metrics were recorded after 10 training epochs. Statistics are means and $2\times$ standard errors over 10 replicas for each experiment. The lowest value with respect to each metric is highlighted in bold type.
    } 
    \label{tab:metrics}
    \begin{tabular}{lccccc}
    \toprule
     &   &
     $\omega_{k}=0$ & $\omega_{k}=0$ & $\omega_{k}=0.1$ & $\omega_{k}=0.1$ \\
     Metric & Method & $\omega_{\mathrm{f}}=0$ & $\omega_{\mathrm{f}}=0.001$ & $\omega_{\mathrm{f}}=0$ & $\omega_{\mathrm{f}}=0.001$ \\
    \midrule
        \multirow{4}{*}{NLL} & \multirow{2}{*}{spline} & -210.32  & - & -196.40  & 48.13    \\
        & & ($\pm$0.16) & & ($\pm$ 1.53) & ($\pm$ 95.37) \\
        \cmidrule(lr){2-6}
           & \multirow{2}{*}{smooth} & -211.04 & \textbf{-211.40} & -206.33  & -208.70  \\
        & & ($\pm$ 0.09) &  ($\pm$ 0.05) & ($\pm 2.04$) & ($\pm$ 1.93)  \\
        \cmidrule(lr){2-6}
        \multirow{4}{*}{FME $\times 10^4$}  & \multirow{2}{*}{spline} & 12.13  & - &  313.64  & 1498.98 \\
        & & ($\pm$ 4.94) & & ($\pm$ 107.92) & ($\pm$ 1935.72)  \\
        \cmidrule(lr){2-6}
        & \multirow{2}{*}{smooth} & 1.04  & \textbf{0.32}  & 5.80  & 0.48   \\
        &&($\pm$ 0.08)& ($\pm$ 0.02)& ($\pm$ 2.30) & ($\pm$ 0.05)\\
        \cmidrule(lr){2-6}
        \multirow{4}{*}{KLD} & \multirow{2}{*}{spline}& 263.77 & - & 230.08  & 1205.69 \\
        &&  ($\pm$ 3.21) & & ($\pm$ 12.38) & ($\pm$ 36.25) \\
        \cmidrule(lr){2-6}
            & \multirow{2}{*}{smooth} & \textbf{193.91}  & 195.17  & 219.03  & 207.81 \\
        && ($\pm$ 0.39) & ($\pm$ 0.43) & ($\pm$ 22.16) & ($\pm$ 11.64)\\
    \bottomrule
    \end{tabular}
    % \begin{tablenotes}
    % \item[a] 10/10 runs failed due to non-numeric values in the parameters
    % \item[b] 2/10 runs failed due to non-numeric values in the parameters
    % \item[c] 7/10 runs failed due to non-numeric values in the parameters
    % \end{tablenotes}
    % \label{tab:my_label}
\end{table}

% \begin{threeparttable}[htbp]
% \begin{small}
%     \centering
%     \caption{Test set metrics of alanine dipeptide training with different losses: negative log likelihood (NLL), force matching error (FME), and reverse Kullback-Leibler divergence (KLD). Weighted loss functions were used as defined in Eq. \eqref{eq:loss} with weight factors $\omega_k,$ $\omega_{\mathrm{f}}$ and $\omega_n = 1-\omega_k-\omega_{\mathrm{f}}.$ Metrics were recorded after 10 training epochs. Statistics are means and $2\times$ standard errors over 10 replicas for each experiment. The lowest value with respect to each metric is highlighted in bold type.}
%     \label{tab:metrics}
% \begin{tabular}{lrrrrrrrr}
% \toprule
%  &  
%  \multicolumn{2}{c}{$\omega_{k}=0$} & \multicolumn{2}{c}{$\omega_{k}=0$} & \multicolumn{2}{c}{$\omega_{k}=0.1$} & \multicolumn{2}{c}{$\omega_{k}=0.1$} \\
%  & \multicolumn{2}{c}{$\omega_{\mathrm{f}}=0$} & \multicolumn{2}{c}{$\omega_{\mathrm{f}}=0.001$} & \multicolumn{2}{c}{$\omega_{\mathrm{f}}=0$} & \multicolumn{2}{c}{$\omega_{\mathrm{f}}=0.001$} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
% Metric & spline & smooth& spline\tnote{a} & smooth& spline\tnote{b} & smooth& spline\tnote{c} & smooth \\
% \midrule
% NLL   &           -210.32 &            -211.04 &               -- &            \textbf{-211.40} &           -196.40 &            -206.33 &             48.13 &            -208.70 \\
%       &             ($\pm$0.16) &              ($\pm$0.09)&               -- &              ($\pm$0.05)&             ($\pm$1.53)&             ($\pm$2.04)&            ($\pm$95.37)&              ($\pm$1.93 )\\
% FME / $10^4$  &         12.13 &           1.04 &               --&           \textbf{0.32} &        313.64 &           5.80 &       1498.98 &            0.48 \\
%      &          ($\pm$4.94) &            ($\pm$0.08) &               -- &             ($\pm$0.02) &        ($\pm$107.92) &           ($\pm$2.30) &       ($\pm$1935.72) &             ($\pm$0.05) \\
% KLD   &            263.77 &             \textbf{193.91} &               -- &             195.17 &            230.08 &             219.03 &           1205.69 &             207.81 \\
%      &              ($\pm$3.21) &               ($\pm$0.39) &               -- &               ($\pm$0.43) &             ($\pm$12.38) &              ($\pm$22.16) &             ($\pm$36.25) &              ($\pm$11.64) \\
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
% \item[a] 10/10 runs failed due to non-numeric values in the parameters
% \item[b] 2/10 runs failed due to non-numeric values in the parameters
% \item[c] 7/10 runs failed due to non-numeric values in the parameters
% \end{tablenotes}
% \end{small}
% \end{threeparttable}

Two experiments were conducted to demonstrate that flows can be trained through a black-box root-finding algorithm.

First, the BG from section \ref{sec:fm} was trained with all smooth transformations operating in the inverse direction. Consequently, inverse problems had to be solved when computing the negative log likelihood with respect to data, while the sampling occurred analytically. Figure \ref{fig:invbg} b) shows the joint marginal distribution of the backbone torsions on the BG samples. The distribution matches the data and the smooth flows that were trained in forward mode (see Fig. \ref{fig:ala2_bg}). This result shows that training with gradients computed from the inverse function theorem is feasible.

Second, flows were trained by different combinations of density estimation, force matching, and energy-based training, where computing $\partial_{\bm{\theta}}\mathcal{L}_{\mathrm{KLD}}$ requires the gradients of the inverse flow.
%. All transformations in this experiment were set up in the natural (forward) mode so that the negative log likelihood and force matching loss could be computed without root-finding. However, computation of the reverse KLD (energy-based training) requires the gradients of the inverse flow.
The training of spline flows with inclusion of the force matching error ($\omega_{\mathrm{f}}>0$) was notoriously unstable. This led to 10/10 and 7/10 failed runs for $\omega_{k}=0$ and $\omega_{k}=0.1,$ respectively. This is understandable from the discontinuity of the spline forces. Even the combination of NLL and KLD led to instabilities for 2/10 runs. (Gradients were not clipped during training to enable a mostly unbiased comparison). In contrast, all training runs with our smooth flows concluded successfully. 
 
Table \ref{tab:metrics} shows the metrics on the test set after 10 training epochs using spline flows and smooth flows (see SI for specifics about the flow architecture and training).
With pure density estimation ($\omega_k=\omega_{\mathrm{f}}=0$), both architectures achieved similar NLL. However, the smooth flow achieved much lower FME and KLD.
%, indicating better generalization properties. 
The NLL and FME were further improved when force data was presented to the flow during training. Including reverse KLD in the loss yielded reasonable metrics for the smooth flows, indicating that the backpropagation through root-finding was numerically stable. However, the metrics were consistently worse than with pure NLL training for both types of flows (spline and smooth) and were subject to large fluctuations. In contrast, including the FME proved to be a more stable approach to include information about the target energy into the training.

\subsection{Smooth Flows as Potentials in Molecular Dynamics}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{md_energies.png}
    \caption{Total energy fluctuation in simulations with a classical force field and with two flows that employed smooth transforms and neural spline transforms, respectively. For each potential, 10 simulations were run starting from 10 different initial configurations (grey color). One run each is highlighted in black. Subfigures c) and d) show identical data on different scales.}
    \label{fig:md_energies}
\end{figure}

A challenging test for the smoothness of a flow is its use as a potential energy in a dynamics simulation. Especially simulations in the microcanonical ensemble are extremely sensitive to numerical errors as the symplectic integration does not impose any additional stabilizing mechanism on the total energy. This means that any inconsistencies between energies and forces on the scale of one integration step easily cause drifting, strongly fluctuating, or exploding energies. 

Therefore, we ran MD simulations using the flow energy $u_\theta = -\log p_{f}(\cdot; \bm{\theta}),$ from Section \ref{sec:fm} as the potential. For comparison, simulations were also run with spline flows that were trained purely by density estimation. Simulations started from stable structures of an MD simulation and were equilibrated in each potential for 1 ps using a Langevin thermostat with 10/ps friction coefficient. 

Figure \ref{fig:md_energies} depicts the evolution of the total energy over 5 ps simulations in the microcanonical ensemble. As expected, the classical simulation kept the total energy roughly constant within a standard deviation of $6\times 10^{-4}$ kJ/mol per degree of freedom. Astoundingly, the smooth flow potentials also maintained the energies within $8\times 10^{-4}$ kJ/mol per degree of freedom using the same 1 fs time step. In contrast, the simulations with spline flow potentials quickly fell apart with potential and kinetic energies growing out of bounds. Those instabilities persisted even with an order-of-magnitude smaller time step (not shown).

While the infeasibility of spline flows for this task was expected, the competitive behavior of smooth flows with molecular mechanics force fields that were tailored for dynamics highlights their regularity and the consistency of their forces. 

% \andreas{There is probably too much discussion in those experiments. Maybe we can get a bit of a cleaner separation between results and discussion.}

\section{Discussion}

\paragraph{Limitations and outlook}
Despite the promising results we mention the following possible limitations of the method in the present state and discuss possible improvements for future work:

    First, smooth flows as implemented in this work impose numerical overhead in comparison to non-smooth alternatives such as spline flows. While this can be considered a question of concrete engineering it also opens search for more efficient smooth alternatives.
    
    In addition, the differentiation rules for black-box inverses provide correct algebraic gradients. Yet for considerably multi-modal distributions maintaining numerical stability can become a challenge. Future work should thus focus on studying and improving numerical stability of the bidirectional training.
    
    While potentials can be trained up to an accuracy where they allow for energy-conserving simulations, it is yet to show that they also provide accurate equilibrium distribution for long-run simulations. At this point a major bottleneck is the inferior evaluation performance per step of a flow-based force-field compared to common force-field implementations.
    
    Furthermore, internal coordinates are a very efficient representation space for smaller poly-peptides. However, they are difficult to scale to large proteins, protein complexes or systems with non-trivial topologies, e.g. proteins with disulfide bonds. Integrating recent advances in graph-based molecular representations with force-matched flows will likely be required for a succesful modeling of such systems.
    
    Finally, training with forces improves the resulting potential compared to just training with samples. However, for many MD data forces have not been stored and recomputing them requires a significant computational overhead. 
% \end{itemize}

\paragraph{Conclusion}
% \begin{itemize}
    
% \item 
We introduced a range of contributions which can improve upcoming work on applying flows to physical problems.

The proposed approach for backpropagation through black-box root-finders can help to obviate the search for analytically invertible transformations if bi-directional training is necessary. As we show the method works especially well for relatively low-dimensional problems. Scaling the approach, generalizing it to non-diagonal Jacobians and improving its numerics are interesting questions for future work.
% \item 

The MD simulation example shows that $C^{\infty}$-smooth flows on non-trivial topologies can open new avenues of research as well as new applications for flows. By carefully respecting the topological domain as well as the smoothness of the target potential we can approximate the equilibrium density of a small peptide nearly perfectly.
% \item 
Finally, we showed that incorporating force information together with MLE outperforms the state-of-the-art approach of combining MLE with minimizing of the reverse KL divergence. It does not suffer from mode-collapse, yet includes information of the target potential which is required to achieve low-energy samples. 

% \end{itemize}

Combined, these results could improve methods used for learning the mean potential surface of coarse-grained molecules, pave the way to multi-scale flows for modeling large protein systems and eventually help to accelerate simulations and sampling.

\newpage
% \TODO{
%     \begin{itemize}
%         \item Backprop through root finders obviates the search for analytically invertible trafos.
%         \item The backprop stuff is especially great for relatively low-dimensional problems.
%         \item Opens avenues of research as exemplified by the MD simulation -> new applications for flows.
%         \item Those smooth BG samples, energies, and forces are crazy good compared to previous work.
%         \item Force matching + NLL works better than KLD + NLL; no loss of modes, yet inclusion of target force information -- and in a way that regularizes the solution instead of fucking everything up
%     \end{itemize}
% }
\section*{Acknowledgements}
We thank the anonymous reviewers at NeurIPS 2021 for their insightful comments. Special thanks to Yaoyi Chen for setting up the alanine dipeptide system and Manuel Dibak, Leon Klein, Oana-Iuliana Popescu, Leon Sixt, and Michael Figurnov for helpful discussions and feedback on the manuscript and pointers to related work. We acknowledge funding from the European Commission (ERC CoG 772230 ScaleCell), Deutsche Forschungsgemeinschaft (GRK DAEDALUS, SFB1114/A04), the German Ministry for Education and Research (Berlin Institute for the Foundations of Learning and Data BIFOLD), and the Berlin Mathematics center MATH+ (Project AA1-6 and EF1-2).


% \TODO{
%     Yaoyi Chen for setting up the alanine dipeptide system. Grants.
% }

% \section{Submission of papers to NeurIPS 2021}

% Please read the instructions below carefully and follow them faithfully.

% \subsection{Style}

% Papers to be submitted to NeurIPS 2021 must be prepared according to the
% instructions presented here. Papers may only be up to {\bf nine} pages long,
% including figures. Additional pages \emph{containing only acknowledgments and
% references} are allowed. Papers that exceed the page limit will not be
% reviewed, or in any other way considered for presentation at the conference.

% The margins in 2021 are the same as those in 2007, which allow for $\sim$$15\%$
% more words in the paper compared to earlier years.

% Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
% NeurIPS website as indicated below. Please make sure you use the current files
% and not previous versions. Tweaking the style files may be grounds for
% rejection.

% \subsection{Retrieval of style files}

% The style files for NeurIPS and other conference information are available on
% the World Wide Web at
% \begin{center}
%   \url{http://www.neurips.cc/}
% \end{center}
% The file \verb+neurips_2021.pdf+ contains these instructions and illustrates the
% various formatting requirements your NeurIPS paper must satisfy.

% The only supported style file for NeurIPS 2021 is \verb+neurips_2021.sty+,
% rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
%   Microsoft Word, and RTF are no longer supported!}

% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.

% \paragraph{Preprint option}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to NeurIPS.

% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.

% The file \verb+neurips_2021.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.

% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.

% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.

% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.

% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.

% \section{Headings: first level}
% \label{headings}

% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.

% First-level headings should be in 12-point type.

% \subsection{Headings: second level}

% Second-level headings should be in 10-point type.

% \subsubsection{Headings: third level}

% Third-level headings should be in 10-point type.

% \paragraph{Paragraphs}

% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone.

% \subsection{Citations within the text}

% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%   \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2021+ package:
% \begin{verbatim}
%   \PassOptionsToPackage{options}{natbib}
% \end{verbatim}

% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%   \usepackage[nonatbib]{neurips_2021}
% \end{verbatim}

% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''

% \subsection{Footnotes}

% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).

% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}

% \subsection{Figures}

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}

% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.

% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.

% \subsection{Tables}

% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.

% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.

% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \section{Final instructions}

% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PDF files}

% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''

% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.

% \begin{itemize}

% \item You should directly generate PDF files using \verb+pdflatex+.

% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.

% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for NeurIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.

% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%   \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%   \newcommand{\RR}{I\!\!R} %real numbers
%   \newcommand{\Nat}{I\!\!N} %natural numbers
%   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

% \end{itemize}

% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.

% \subsection{Margins in \LaTeX{}}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%   \usepackage[pdftex]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.

% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2021/PaperInformation/FundingDisclosure}.

% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}

% \section*{References}


\bibliography{references}


% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip

% {
% \small

% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.

% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.

% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section*{Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% % The checklist follows the references.  Please
% % read the checklist guidelines carefully for information on how to answer these
% % questions.  For each question, change the default \answerTODO{} to \answerYes{},
% % \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% % justification to your answer}, either by referencing the appropriate section of
% % your paper or providing a brief inline description.  For example:
% % \begin{itemize}
% %   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
% %   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
% %   \item Did you include the license to the code and datasets? \answerNA{}
% % \end{itemize}
% % Please do not modify the questions and only use the provided macros for your
% % answers.  Note that the Checklist section does not count towards the page
% % limit.  In your paper, please delete this instructions block and only keep the
% % Checklist section heading above along with the questions/answers below.
% %%% END INSTRUCTIONS %%%

% \begin{enumerate}

% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerYes{}
%   \item Did you describe the limitations of your work?
%     \answerYes{}
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerNA{This work is merely technical and studies smooth normalizing flows applied to physics research.}
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerYes{}
% \end{enumerate}

% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerYes{All mathematical details can be found in the supplementary material.}
% 	\item Did you include complete proofs of all theoretical results?
%     \answerYes{All mathematical details can be found in the supplementary material.}
% \end{enumerate}

% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerYes{They can be found in the supplementary material.}
%   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \answerYes{Training details are partly explained in the experimental part. More details can be found in the supplementary material.}
% 	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerYes{}
% 	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
%     \answerYes{Details regarding compute requirements can be found in the supplementary material.}
% \end{enumerate}

% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerYes{We report used libraries and used code of prior works in the supplementary material.}
%   \item Did you mention the license of the assets?
%     \answerNo{All used code that is not written by us has been published under non-restrictive open source licenses}
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerNA{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerNA{}
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerNA{No personal data are used in this work.}
% \end{enumerate}

% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerNA{No crowdsourcing / human subject research was conducted in this work.}
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerNA{No crowdsourcing / human subject research was conducted in this work.}
%   \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
%     \answerNA{No crowdsourcing / human subject research was conducted in this work.}
% \end{enumerate}

% \end{enumerate}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
% \TODO{
% \begin{itemize}
%     \item \textbf{Code}:
%     \begin{itemize}
%         \item Minimal runable version that reproduces the results. No need for beauty, documentation or a clean API.
%         \item Core code for the library (can be sliced together in local imports) (Andreas)
%         \item Batch scripts producing the quantitative results for Ala 2 (Andreas)
%         \item Notebook reproducing our figures (Jonas, Andreas).
%     \end{itemize}
    
%     \item \textbf{Theory}:
%     \begin{itemize}
%         \item Smooth compact bump functions:
%         \begin{itemize}
%             \item \deleted{Align notation (Jonas)}
%             \item \deleted{Details on circular wrapping (Jonas)}
%             \begin{itemize}
%                 \item \deleted{Implementation details (Jonas)}
%                 \item \deleted{Proof of smoothness (Jonas)}
%             \end{itemize}
%         \end{itemize}
%         \item Non-analytic inverse flows
%         \begin{itemize}
%             \item \deleted{Align notation (Jonas)}
%             \item \deleted{Lifting to diagonal Jacobians (Jonas)}
%             \item \deleted{Algorithmic implementation (in AD) (Jonas)}
%             \item \deleted{Grid bisection details (Jonas)}
%         \end{itemize}
%     \end{itemize}
    
%     \item \textbf{Experimental Details}:
%     \begin{itemize}
%         \item Toy examples
%         \begin{itemize}
%             \item Additional periodic density in 2D (Jonas)
%             \item 1D density (Jonas)
%         \end{itemize}
%         \item \deleted{Run-time Comparisons}
%         \begin{itemize}
%             \item \deleted{Add out-commented section to the appendix including the run-time table. (Andreas)}
%         \end{itemize}
%         \item \deleted{6.3 BG training through force matching}
%         \begin{itemize}
%             \item \deleted{Ala2 explanation is there - figure explaining Rachamachandran plot (Andreas)}
%             \item \deleted{make \texttt{bgmol} package public } $\to$ \textcolor{blue}{no need to if we can't use the links anyway}
%             \item \deleted{Model architecture - clean up / figure (Andreas)}
%             \item \deleted{Training details - hyperparams (Andreas)}
%         \end{itemize}
%         \item \deleted{6.4 BG Training through Black-Box root finding}
%         \begin{itemize}
%             \item \deleted{Table 1 - explain numbers / slicing of data in detail. (Andreas)}
%             \item \deleted{Model architecture - same + blackbox inverse}
%             \item \deleted{Training details - hyperparams (Andreas), linlogcut, number of grid points for inverse, KLD batch size, learning rate scheduling}
%         \end{itemize}
       
%         \item \deleted{6.5 Smooth flows as potentials in MD}
%         \begin{itemize}
%             \item \deleted{Brief intro on what is happneing in this simulation}
%             \item \deleted{Simulation details}
%             \item \deleted{explain MD background (Andreas)}
%         \end{itemize}
%     \end{itemize}
% \end{itemize}
% }

% \TODO{
% Revision / Rebuttal notes
% \begin{itemize}
%     \item 
%     \item
% \end{itemize}
% }

%\appendix
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand\thesection{SI-\arabic{section}}
\renewcommand\thetable{SI-\arabic{table}}
\renewcommand\thefigure{SI-\arabic{figure}}

\newpage
%\setcounter{page}{1}
\pagenumbering{roman}
\section*{SUPPLEMENTARY INFORMATION\\ "Smooth Normalizing Flows"}
\section{Proofs}
\subsection{Smooth Bump Functions}

We first show that the smooth ramp as defined in Sec. \ref{sec:bump-functions} is indeed as smooth as promised. 

\begin{lemma}
\label{lemma:smooth-ramp}
Let $\alpha > 0$ and $\beta \geq 1$. The function given by 
\begin{align}
    \rho(x) := \begin{cases}
        \exp\left(-\frac{1}{\alpha\cdot x^\beta}\right) & x > 0 \\
        0 & \text{o.w.}
    \end{cases}
\end{align}
is smooth. Furthermore, for all $n \in \mathbb{N}$ we have $\lim_{x\rightarrow 0} \partial_{x}^{(n)} \rho(x) \rightarrow 0$.
\end{lemma}
\begin{proof}
    We follow classic text-book examples and give the proof for completeness. First note that the cases for $x > 0$ and $x < 0$ are trivial. We thus have to consider $x = 0$. We see that all left-sided derivatives vanish at $x=0$. Thus, it is sufficient to consider only right-sided derivatives. For $x > 0$ we have
    \begin{align}
        \partial_{x}\rho(x) = \alpha \beta \cdot x^{-(\beta+1)} \rho(x).
    \end{align}
    By induction, we can see that the $n$-th derivative is given by 
    \begin{align}
        \partial^{n}_{x}\rho(x) = \mathcal{P}\left(x^{-1}\right) \rho(x),
    \end{align}
    where $\mathcal{P}\left(x^{-1}\right)$ is some polynomial in $x^{-1}$. For any polynomial we have
    \begin{align}
        \lim_{y\rightarrow\infty} \frac{\mathcal{P}(y)}{\exp\left(\frac{y^{\beta}}{\alpha}\right)} = 0.
    \end{align}
    Thus, we end up with
    \begin{align}
        \lim_{x \rightarrow 0} \partial^{n}_{x}\rho(x) = 0.
    \end{align}
    % Now we use the fact, that for any $y$ and $n \in \mathbb{N}$ we have
    % \begin{align}
    %     \lim_{y\rightarrow\infty} y^{n} \exp(-y) = 0.
    % \end{align}
\end{proof}

Now with the following theorem, we see that the smooth bump functions satisfy all necessary conditions.
\begin{theorem}
\label{thm:smooth-sigmoid}
Let $\rho \colon [0,1] \rightarrow [0, 1]$ be a $n$-times differentiable and strictly monotonously increasing function with $\lim_{x \rightarrow 0} \rho(x)=0$ and $\lim_{x \rightarrow 1} \rho(x)=1$. Then the function
\begin{align}
    \sigma[\rho](x) := \frac{\rho(x)}{\rho(x) + \rho(1-x)}
\end{align}
is $n$-times differentiable, strictly monotonously increasing and satisfies
\begin{align}
    \sigma[\rho](0) &= 0, \\
    \sigma[\rho](1) &= 1.
\end{align}
If furthermore, $\lim_{x \rightarrow 0} \partial_{x}^{k} \rho(x) = 0$ for all $k \leq n$ we also have
\begin{align}
    \lim_{x \rightarrow 0} \partial^{n}_{x} \sigma[\rho](x) &= \lim_{x \rightarrow 1}  \partial^{n}_{x}  \sigma[\rho](x) = 0.
\end{align}
\end{theorem}
\begin{proof}
    We quickly see that
    \begin{align}
        \lim_{x \rightarrow 0}  \sigma[\rho](x) 
        &= \lim_{x \rightarrow 0} \frac{\rho(x)}{\rho(x) + \rho(1-x)} \\
        &= \frac{\lim_{x \rightarrow 0} \rho(x)}{\lim_{x \rightarrow 0} \rho(x) + \lim_{x \rightarrow 0} \rho(1 - x)} \\
        &= \frac{0}{0 + 1} = 0.
    \end{align}
    Analogously, we can show $\lim_{x \rightarrow 1}  \sigma[\rho](x) = 1$.
    For the first derivative we compute
    \begin{align}
        \partial_{x} \sigma[\rho](x) &= \frac{\partial_{x} \rho(x)}{\rho(x) + \rho(1-x)} - \frac{\rho(x) (\partial_{x} \rho(x) - \partial_{x} \rho(1-x))}{(\rho(x) + \rho(1-x))^{2}} \\
        &= \frac{(\partial_{x} \rho(x)) \rho(1-x) + (\partial_{x} \rho(1-x)) \rho(x)}{(\rho(x) + \rho(1-x))^{2}} \label{eq:first-derivative-of-general-sigmoid},
    \end{align}
    from which we see that $\sigma[\rho]$ is strictly monotonously increasing within $(0, 1)$ and thus on all of $[0,1]$.
    As the denominator does not vanish in all of $[0, 1]$ and $\rho$ is $k$-times differentiable, we can conclude that $\sigma[\rho]$ is $k$-times differentiable as well.
    \\  ~ \\
    Now let $\lim_{x \rightarrow 0} \partial_{x}^{k} \rho(x) = 0$ for all $k<n$. We first note that
    \begin{align}
        C_{k} &:= \lim_{x \rightarrow 0} \partial_{x}^{k} (\rho(x) + \rho(1-x)) \\
        &= \lim_{x \rightarrow 0} \partial_{x}^{k} \rho(x) + (-1)^{k} \partial_{x}^{n} \rho(1-x)\\
        &= (-1)^{k} \lim_{x \rightarrow 1} \partial_{x}^{k} \rho(x),
    \end{align}
    exists.
    
    Then using the following recursive formula for the $n$-th derivative of the quotient of two functions given by \citet{xenophontos2007formula}
    \begin{align}
        \label{eq:xenophontos-formula}
        \partial^{n}_{x}\frac{u(x)}{v(x)} = \frac{1}{v(x)} \left[ \partial^{n}_{x} u(x) - n! \sum_{j=1}^{n} \frac{\partial^{n+1-j}_{x}v(x) \cdot \partial^{j-1}_{x} \frac{u(x)}{v(x)}}{(n+1-j)!(j-1)!} \right]
    \end{align}
    and setting $u(x) = \rho(x), v(x) = \rho(x) + \rho(1-x)$, we obtain
    \begin{align}
        \lim_{x \rightarrow 0} \partial^{n}_{x} \sigma[\rho](x) 
        &=  \lim_{x \rightarrow 0} \Bigg( \frac{1}{\rho(x) + \rho(1-x)} \\
        &\quad \cdot \left[ \partial^{n}_{x} \rho(x) - n! \sum_{j=1}^{n} \frac{\partial^{n+1-j}_{x}(\rho(x) + \rho(1-x)) \cdot \partial^{j-1}_{x} \sigma[\rho](x) }{(n+1-j)!(j-1)!} \right] \Bigg).
    \end{align}
    We will prove the rest of the theorem by induction. Assume that
    \begin{align}
        \lim_{x \rightarrow 0} \partial^{k}_{x} \sigma[\rho](x) = 0
    \end{align}
    for all $k < n$. As $\rho(x) + \rho(1-x) > 0$ for all $x \in [0, 1]$ and all limits exist we obtain
    \begin{align}
        \lim_{x \rightarrow 0} \partial^{n}_{x} \sigma[\rho](x) 
        &= \frac{1}{1} \left[0 - n! \sum_{j=1}^{n} \frac{C_{n+1-j}  }{(n+1-j)!(j-1)!} \cdot \lim_{x \rightarrow 0} \partial^{j-1}_{x} \sigma[\rho](x)\right] \\
        &= \frac{1}{1} \left[0 - n! \sum_{j=1}^{n} \frac{C_{n+1-j}  }{(n+1-j)!(j-1)!} \cdot 0\right] = 0, \\
    \end{align}
    which proves the induction step. For the base case we evaluate \eqref{eq:first-derivative-of-general-sigmoid} at $x=0$ which completes the proof. To show that also $\lim_{x \rightarrow 1}  \partial^{n}_{x}  \sigma[\rho](x) = 0$ we use that
    \begin{align}
        \sigma[\rho](x) &= \frac{\rho(x)}{\rho(1-x) + \rho(x)}\\
                        &= 1 - \frac{\rho(1-x)}{\rho(1-x) + \rho(x)} \\
                        &= 1 - \sigma[\rho](1-x) \\
    \end{align}
    from which we have for $n > 0$
    \begin{align}
        \lim_{x\rightarrow 1} \partial^{n}_{x} \sigma[\rho](x) 
        &= \lim_{x\rightarrow 0} \partial^{n}_{x} \sigma[\rho](1-x) \\
        &= \lim_{x\rightarrow 0} \partial^{n}_{x} (1-\sigma[\rho](x)) \\
        &= - \lim_{x\rightarrow 0} \partial^{n}_{x} \sigma[\rho](x) \\
        &= 0.
    \end{align}
\end{proof}



% \begin{lemma}
%     \label{lemma:smooth-forces-on-circle}
%     Let $f \in \mathcal{C}^{n+1}(S^{1}, S^{1})$ be bijection and $\partial_{x} f(x) \geq 0$ for all $x \in S^{1}$. Define for $\epsilon > 0$
%     \begin{align}
%         f_{\epsilon}(x) := (1-\epsilon) f(x) + \epsilon x.
%     \end{align}
%     Then $f_{\epsilon}$ is a $\mathcal{C}^{n}$-bijection and both and $g_{\epsilon} :=\log \partial_{x} f_{\epsilon}$ are in $\mathcal{C}^{n}(S^{1}, S^{1})$.
% \end{lemma}
% \begin{proof}
%     We show this by induction. For the first derivative we compute
%     \begin{align}
%         -\partial_{x} \log \partial_{x} f_{\epsilon}(x)
%         &= - \partial_{x} \log (1-\epsilon) \partial_{x} f (x) + \epsilon \\ 
%         &= - \frac{\partial^{2}_{x} f (x)}{\partial_{x} f (x) + \epsilon}.
%     \end{align}
%     As the denominator does not vanish this is continuous on $S^{1}$. For the induction step we again utilize formula \eqref{eq:xenophontos-formula} with $u(x)=-\partial^{2}_{x} f (x)$ and $v(x)=\partial_{x} f (x) + \epsilon$ to conclude
%     \begin{align}
%         \partial^{n+1}_{x} \log \partial_{x} f_{\epsilon}(x)
%         &= \partial^{n}_{x} \frac{u(x)}{v(x)}  \\
%         &= \frac{1}{\partial_{x} f (x) + \epsilon}\left( -\partial^{n}_{x} \partial^{2}_{x} f (x) + \sum_{j=1}^{n} \frac{\partial_{x}^{n+1-j} \partial_{x} (f(x) + \epsilon)}{(n + 1 - j)!} 
%         \frac{\partial_{x}^{j-1} \frac{u(x)}{v(x)}}{(j-1)!}  \right) \\
%         &= \frac{1}{\partial_{x} f (x) + \epsilon}\left( - \partial^{n + 2}_{x}  f (x) + \sum_{j=1}^{n} \frac{\partial_{x}^{n+2-j} f(x)}{(n + 1 - j)!} 
%         \frac{\partial_{x}^{j-1} \frac{u(x)}{v(x)}}{(j-1)!}  \right),
%     \end{align}
%     thus $\partial^{n}_{x} \log \partial_{x} f_{\epsilon}(x)$ is continuous as long as $f$ is $n$-times differentiable.
% \end{proof}

\subsection{Circular Wrapping}
Here we explain, how the former construction of $\sigma[\rho]$ can be used to define smooth bijections on the circle (and the hypertorus). We first explain the general recipe which works for any compactly supported smooth transformation and then how it is instantiated for the functions as defined in Sec. \ref{sec:bump-functions}.

\paragraph{General wrapping construction for smooth compact bump functions}
Let $p$  a smooth density with support on $[a, b]$, such that $b - a \leq 1$ and $[a, b] \cap [0, 1] \neq \emptyset$. Let furthermore $\partial^{n}_{x} p(a) = \partial^{n}_{x} p(b) = 0$ for all $n$. Then we can define a smooth density $\hat p$ on $[0,1]$ using the following cases:
\begin{itemize}
    \item For $[a, b] \subset [0,1]$ set
    \begin{align}
        \hat p(x) = \begin{cases}
        p(x) & x \in [a, b] \\
        0 & \mathrm{o.w.}
        \end{cases}
    \end{align}
    \item For $a < 0$ set
    \begin{align}
        \hat p(x) = \begin{cases}
        p(x) & x \in [0, b] \\
        p(x - 1) & x \in [1 + a, 1] \\
        0 & \mathrm{o.w.}
        \end{cases}
    \end{align}
    \item For $b > 1$ set
    \begin{align}
        \hat p(x) = \begin{cases}
        p(x) & x \in [a, 1] \\
        p(1+x) & x \in [0, b - 1] \\
        0 & \mathrm{o.w.}
        \end{cases}
    \end{align}
\end{itemize}
It is easy to see that $\partial^{n}_{x} \hat p(0) = \partial^{n}_{x} \hat p(1)$ for all $n$.
For some $c > 0$ we can now define the smooth non-vanishing density $\bar p$ on $[0,1]$ as
\begin{align}
    \bar p(x) = (1-c) \hat p(x) + c.
\end{align}
Then the function
\begin{align}
    f(x) = \int_{0}^{x} \bar p(z) ~ dz. \label{eq:smooth-circular-transformation-as-density-integral}
\end{align}
is a smooth bijection on $[0,1]$ with $\partial^{n}_{x} f(0) = \partial^{n}_{x} f(1)$. By this, we obtain a smooth bijection on $S^1$ with periodic derivative $\bar p$.

\paragraph{Instantiation for the smooth bump functions of Sec \ref{sec:bump-functions}}

As discussed in Sec. \ref{sec:bump-functions} we can choose $b \in [0,1]$ and $a \geq 1$ and set
\begin{align}
    g(x) = \sigma[\rho]\left(a (x - b) + \tfrac{1}{2}\right)
\end{align}
to obtain the smooth $[b - \tfrac{1}{2a}, b + \tfrac{1}{2a}]$-supported bump function $\partial_{x} g$. This satisfies the assumptions on $p$ of the former paragraph. Thus, we can set $p := \partial_{x} g$ which gives us the desired smooth transformation $f \colon S^1 \rightarrow S^1$. Note, that due to the piece-wise construction of $\bar p$, we can evaluate the integral \eqref{eq:smooth-circular-transformation-as-density-integral} in closed form.

\subsection{Differentiation through Blackbox Root-Finding}

\paragraph{Gradient relations}
Here we derive the gradient relations as discussed in Sec. \ref{sec:black-box-inverse-optimization}. This is a generalization of the usual inverse function theorem in 1D to the case of scalar bijections conditioned on some parameter $\bm \theta$. For reasons of brevity in the main text and precision in the proof, we used a different notation in Sec. \ref{sec:black-box-inverse-optimization} compared to the one we will use here. Specifically, we will denote the forward transform as $\alpha(\cdot; \bm \theta)$ while we denote the inverse transform as $\beta(\cdot; \bm \theta)$ for some parameters $\bm \theta \in \mathbb{R}^{d}$.
\begin{theorem}
\label{thm:backward-gradients}
Let $\Omega \subset \mathbb{R}$ open and
\begin{align*}
    \alpha \colon \Omega \times \mathbb{R}^{d} \rightarrow \Omega, \qquad
    \beta \colon \Omega \times \mathbb{R}^{d} \rightarrow \Omega
    %  \qquad
    % \beta \colon (y; \bm \theta) \mapsto x
\end{align*}
be $\mathcal{C}^{2}$ functions such that for all $\bm \theta \in \mathbb{R}^{d}, x \in \Omega$  and $y := \alpha(x, \bm \theta)$
\begin{align} 
    \alpha(\beta(y; \bm \theta); \bm \theta) = y \qquad
    \beta(\alpha(x; \bm \theta); \bm \theta) = x \label{eq:local-inverse-rule}
\end{align}
and 
\begin{align}
    \partial_{x} \alpha(x; \bm \theta) > 0 \qquad 
    \partial_{y} \beta(y; \bm \theta) > 0. \label{eq:monotonicity-of-forward-and-inverse-function}
\end{align}
Define further
\begin{align}
    g_{\alpha}(x,\bm \theta) := \log \left| \frac{\partial \alpha(x; \bm \theta) }{\partial x } \right|, \quad g_{\beta}(y,\bm u) := \log \left| \frac{\partial \beta(y; \bm u) }{\partial y } \right|.
\end{align}
Then we have the following gradient relations:
\begin{align}
    \frac{\partial \beta(y; \bm \theta)}{\partial y} &= \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x}\right)^{-1} \label{eq:grad-out-in} \\
    \frac{\partial \beta(y; \bm \theta)}{\partial \bm \theta} &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x}\right)^{-1} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} \label{eq:grad-out-param} \\
    \frac{\partial g_{\beta}(y; \bm \theta)}{\partial y} &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x}\right)^{-1} \frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x} \label{eq:grad-density-in} \\
    \frac{\partial g_{\beta}(y; \bm \theta)}{\partial \bm \theta} &= \left(\frac{\partial \alpha(y; \bm \theta)}{\partial x}\right)^{-1} \left(  \frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} - \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x} \right) \label{eq:grad-density-param}
\end{align}
\end{theorem}
\begin{proof}
    We have
    \begin{align}
        \frac{d}{d x} \beta(\alpha(x; \bm \theta); \bm \theta) = \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial x} = \frac{d x}{d x} = 1.
    \end{align}
    and thus
    \begin{align}
        \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y}  &= \left( \frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-1}.
    \end{align}
    
    Now 
    \begin{align}
        \frac{d}{d \bm \theta} \beta(\alpha(x; \bm \theta); \bm \theta) = \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} + \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial \bm \theta} = \frac{\partial x}{\partial \bm \theta} = 0.
    \end{align}
    which gives
    \begin{align}
        \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial \bm \theta} 
        &= \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta}\\
        &= \left( \frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-1} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta}
    \end{align}
    
    Combining
    \begin{align}
        \frac{d }{d x} \frac{d }{d x} \beta(\alpha(x; \bm \theta); \bm \theta) &= \frac{d }{d x} \frac{d x}{d x} = 0
    \end{align}
    and 
    \begin{align}
        \frac{d }{d x} \frac{d }{d x} \beta(\alpha(x; \bm \theta); \bm \theta) &= \frac{d}{dx} \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial x} \\
        &= \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y\partial y} \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{2} + \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial x \partial x} 
    \end{align}
    we obtain
    \begin{align}
    \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y \partial y}
          &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial x \partial x} \\
          &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-3} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial x \partial x}\\
          &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \frac{\partial}{\partial x} \log \left ( \frac{\partial \alpha(x; \bm \theta)}{\partial x}  \right)\\
          &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x}
    \end{align}
    and thus
    \begin{align}
        \frac{\partial}{\partial y} g_{\beta}(\alpha(x; \bm \theta); \bm \theta) 
        &= \left(\frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \right)^{-1} \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y \partial y} \\
        &= - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-1} \frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x}
    \end{align}
    Finally, combining
    \begin{align}
        \frac{d }{d \bm \theta} \frac{d }{d x} \beta(\alpha(x; \bm \theta); \bm \theta)  &= \frac{d }{d \bm \theta} \frac{d x}{d x} = 0
    \end{align}
    and
    \begin{align}
        \frac{d }{d \bm \theta} \frac{d }{d x} \beta(\alpha(x; \bm \theta); \bm \theta) 
        &= \frac{d}{d\bm \theta} \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial x} \\
        &= \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} \frac{\partial \alpha(x; \bm \theta)}{\partial x}  \\
        &\quad + \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x} \nonumber  \\
        &\quad + \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial \bm \theta  \partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial x} \nonumber 
    \end{align}
    gives us
    \begin{align}
        \frac{\partial^{2}\beta(\alpha(x; \bm \theta); \bm \theta)}{\partial \bm \theta \partial y}
        &= - \frac{\partial^{2} \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y\partial y} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} \\
        &\quad - \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x} \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x}\right)^{-1} \nonumber \\
        &= \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta} \\
        &\quad - \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x}  \nonumber\\
        &= \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-2} \left(\frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta}  - \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x} \right)
    \end{align}
    And thus we get
    \begin{align}
        \frac{\partial}{\partial \bm \theta} g_{\beta}(\alpha(x; \bm \theta); \bm \theta) 
        &= \left( \frac{\partial \beta(\alpha(x; \bm \theta); \bm \theta)}{\partial y}  \right)^{-1} \frac{\partial^{2}\beta(\alpha(x; \bm \theta); \bm \theta)}{\partial \bm \theta \partial y} \\
        &= \left(\frac{\partial \alpha(x; \bm \theta)}{\partial x} \right)^{-1} \left(\frac{\partial g_{\alpha}(x; \bm \theta)}{\partial x} \frac{\partial \alpha(x; \bm \theta)}{\partial \bm \theta}  - \frac{\partial^{2} \alpha(x; \bm \theta)}{\partial \bm \theta \partial x} \right)
    \end{align}
\end{proof}

\paragraph{Use in component-wise transformations}
Now we discuss component-wise transformations, i.e. \emph{transformers} of coupling layers. \footnote{Here we refer to the terminology introduced in \citet{huang2018neural}. We do \textbf{not} refer to permutation equivariant models leveraging dot-product attention or similar.} Let the multivariate transformations $\bm \alpha$, $\bm \beta$ factorize as
\begin{align}
    \bm \alpha(\bm x, \bm \theta)_{i} &= \alpha_{i}(x_i, \bm \theta) \\
    \bm \beta(\bm y, \bm \theta)_{i} &= \beta_{i}(y_i, \bm \theta)
\end{align}
for some component bijections $\alpha_i, \beta_i$, which is equivalent to $\partial_{\bm x} \bm \alpha$ and $\partial_{\bm y} \bm \beta$ being diagonal.

First, note that all component-wise transformations within automatic differentation graphs (e.g. component-wise applied \texttt{relu}, \texttt{exp} or \texttt{log} operations) only require the computation of component-wise derivatives in order to compute vector-jacobian products (VJP) with respect to output gradients $\bm v$.

As such, computing the terms $\bm v^T \partial_{\bm y} \beta(\bm y; \bm \theta)$ and $\bm v^T \partial_{\bm \theta} \beta(\bm y; \bm \theta)$ can be reduced to computing the component-wise VJPs $v_{i} \cdot \partial_{y_{i}}  \beta(y_i; \bm \theta)_{i}$ and $v_{i} \cdot \partial_{\bm \theta}  \beta(y_i; \bm \theta)_{i}$. If we have access to the Jacobian diagonal $\mathrm{diag}\left( \partial_{\bm x} \alpha(\bm x; \bm \theta)\right)$, this can be done using eqs. \eqref{eq:grad-out-in} \eqref{eq:grad-out-param} with one VJP call (see next paragraph for an implementation example).

For the log determinant of the jacobian we use
\begin{align}
    \log \det \partial_{\bm y} \beta(\bm y, \bm \theta) 
    &= \log \prod_{i} \mathrm{diag}(\partial_{\bm y} \beta(\bm y, \bm \theta))_{i} \\
    &=\sum_{i} \log \partial_{y_{i}} \beta(y_{i}, \bm \theta)\\
    &= \sum_{i} g_{\beta_{i}}(y_i, \bm \theta).
\end{align}
which reduces computing the VJPs $v \cdot \partial_{\bm y} \log \det \partial_{\bm y} \beta(\bm y, \bm \theta)$, $v \cdot \partial_{\bm \theta} \log \det \partial_{\bm y} \beta(\bm y, \bm \theta)$ to computing the component-wise VJPs $v \cdot \partial_{\bm y_{i}} g_{i}(y_{i}, \bm \theta)$, $v \cdot \partial_{\bm \theta} g_{i}(y_{i}, \bm \theta)$. Similarly as before, this can be done with two VJP calls if we have access to the Jacobian diagonal $\mathrm{diag}\left( \partial_{\bm x} \alpha(\bm x; \bm \theta)\right)$ and using eqs. \eqref{eq:grad-density-in}, \eqref{eq:grad-density-param} (see next paragraph for an implementation example).

% We now discuss the multi-variate case in which $\bm x \in \mathbb{R}^{d}$ is transformed into $\bm y$ using component-wise flow. Here we mean flow transformation with diagonal Jacobian, as 





% Let us now consider $\bm\beta$ and $\bm\alpha$ to be multi-variate bijections with diagonal Jacobian (i.e. they act component-wise on the input). Here we mean precisely that
% \begin{align}
%     \bm \alpha(\bm x, \bm \theta)_{i} &= \alpha_{i}(x_i, \bm \theta) \\
%     \bm \beta(\bm y, \bm \theta)_{i} &= \beta_{i}(y_i, \bm \theta)
% \end{align}
% for some component functions $\alpha_i, \beta_i$.
% We can define $\bm g_{\bm \alpha}, \bm g_{\bm \beta}$ for the multi-variate case accordingly defining the component-functions
% \begin{align}
%     {\bm g_{\bm \alpha}(\bm x; \bm \theta)}_{i} &:= g_{\alpha_{i}}(x_i, \bm \theta) \\
%     {\bm g_{\bm \beta}(\bm y, \bm \theta)}_{i} &:= g_{\beta_{i}}(y_i, \bm \theta).
% \end{align}
% Evaluating the black-box inversion in the forward pass returns outputs $\bm x = \bm \beta(\bm y; \bm \theta)$ and $\log \det \left|\partial_{\bm y} \bm x\right| = \sum_{i} g_{\beta_{i}}(y_{i}; \bm \theta)$. We thus have to show, that the backward pass can be expressed via backward passes of the forward flow.

% Let $z(\bm x)$ be some (scalar) output node that depends on $\bm x$. Then we have
% \begin{align}
%     \frac{\partial z}{\partial y_{i}} 
%     &= \frac{\partial z}{\partial \bm x}^T \frac{\partial \bm x}{\partial y_{i}} \\
%     &= \frac{\partial z}{\partial x_{i}} \frac{\partial \beta_{i}}{\partial y_{i}} \\
%     \frac{\partial z}{\partial \theta_{i}} 
%     &= \frac{\partial z}{\partial \bm x}^T \frac{\partial \bm x}{\partial \theta_{i}} \\
%     &= \sum_{k} \frac{\partial z}{\partial x_{k}} \frac{\partial \beta_{k}}{\partial \theta_{i}}
% \end{align}

% For any $\ell \colon \mathbb{R}^{d} \rightarrow \mathbb{R}$ and any partial derivative operator $\partial_{z}$ we have 
% \begin{align}
%     \partial_{y_{l}} \ell(\bm \beta(\bm y; \bm \theta))
%     &= \sum_{i} \partial_{\beta_i} \ell(\bm \beta(\bm y; \bm \theta)) \partial_{y_{l}} \beta_{i}(\bm y; \bm \theta)\\
%     &= \sum_{i} \partial_{\beta_i} \ell(\bm \beta(\bm y; \bm \theta)) \sum_{k} \partial_{y_{k}}\beta_{i}(\bm y; \bm \theta) \partial_{y_l} y_{k}\\
%     &= \sum_{i} \partial_{\beta_i} \ell(\bm \beta(\bm y; \bm \theta)) \sum_{k} \partial_{y_{k}}\beta_{i}(\bm y; \bm \theta) \partial_{y_l} y_{k}\\
% \end{align}


% Then we see that 
% \begin{align}
%     \log \det \partial_{\bm y} \beta(\bm y, \bm \theta) 
%     &= \log \prod_{i} \mathrm{diag}(\partial_{\bm y} \beta(\bm y, \bm \theta))_{i} \\
%     &=\sum_{i} \log \partial_{y_{i}} \beta(y_{i}, \bm \theta)\\
%     &= \sum_{i} g_{\beta_{i}}(y_i, \bm \theta).
% \end{align}
% Finally, let $\ell$ be some function of $\beta(\bm y, \bm \theta) / g_{\beta}(\bm y, \bm \theta)$. Then we have
% \begin{align}
%     \partial_{\bm z} \ell(\beta(\bm y, \bm \theta)) = \sum_{i} \partial_{\beta_{i}} \ell(\cdots) \partial_{\bm z} \beta_{i}(\bm y, \bm \theta)
% \end{align}

\paragraph{Algorithmic implementation}
The procedure above is easy to implement in many deep learning frameworks such as \texttt{PyTorch}. We give a pseudo-code implementation below. Here \texttt{vjp} denotes the vector-jacobian product e.g. as implemented in \texttt{PyTorch} via the \texttt{torch.autograd.grad} function.
\begin{verbatim}
def forward_pass(root_finder, bijection, output, params):
    ''' computes forward pass via black-box root finder '''
    
    # compute inverse via black-box method
    input = root_finder(bijection, output, params)
        
    # compute forward log det jacobian
    ldj = jacobian_log_determinant(bijection, input, params)
    
    # return input and corresponding log det jacobian
    return input, -ldj

def backward_pass_x(bijection, output_grad_x, output_grad_ldj, input, params):
    ''' computes gradients with respect to inputs '''
    
    # compute diagonal jacobian and its log
    jac = diagonal_jacobian(bijection, input, params)
    log_jac = log(jac)
    
    # reweigh output gradients with inverse diagonal jacobian (eqs. 53 & 55)
    output_grad_x = output_grad_x / jac
    output_grad_ldj = output_grad_ldj / jac
    
    # compute dg/dx
    f = vjp(output=log_jac, input=input, output_grad=ones_like(jac))
    
    g1 = output_grad_x
    g2 = f * output_grad_ldj
    
    g = g1 + g2
    
    return g
    
    
def backward_pass_params(bijection, output_grad_x, output_grad_ldj, input, params): 
    ''' computes gradients with respect to parameters '''
    
    # compute output, diagonal jacobian and its log
    output = bijection(input, params)
    jac = diagonal_jacobian(bijection, input, params)
    log_jac = log(jac)
    
    # reweigh output gradients with inverse diagonal jacobian (eqs. 54 & 56)
    output_grad_x = output_grad_x / jac
    output_grad_ldj = output_grad_ldj / jac
    
    # compute dg/dx
    f = vjp(output=jac, input=input, output_grad=ones_like(jac))
    
    # compute sum of eqs. 54 & 56 in one operation
    u = [output, output, jac]
    v = [-output_grad_x, -f * output_grad_ldj, -output_grad_ldj]
    g = vjp(output=u, input=params, output_grad=v)
    
    return g
\end{verbatim}

\subsection{Details on multi-bin bisection}
Here we give additional details to the multi-bin bisection as sketched in Sec. \ref{sec:black-box-inverse-optimization}.

Instead of just keeping track of a left and right bound of the search interval (1) we split it into $K$ bins (2) identify the right bin (3) recursively apply the search to this smaller bin. Algorithmically, the procedure is given as follows:
\begin{enumerate}
    \item Let $[a, b]$ be a closed interval which is known to contain $x$ s.t. $f(x) = y$.
    \item For $k=0\ldots K$ define $s_{k} = \tfrac{k}{K} (b-a) + a$.
    \item Find $k$ such that $f(s_{k}) - y < 0$ and $f(s_{k+1}) - y > 0$.
    \item If $\left|f(s_{k}) - y\right| < \epsilon$ return $x \approx s_{k}$. Else set $a:= s_{k}, ~ b:= s_{k+1}$ and continue with step 2.
\end{enumerate}
We can see that $K=2$ results in the usual bisection method. However, each step can be executed in vectorized form. Thus for a fixed precision we can reduce the number of iterations by a factor $O\left(\tfrac{1}{\log K}\right)$ at the expense of increasing memory by a factor $O\left(K\right)$.

% \paragraph{Additional derivations for forces}

% \begin{align}
%     d^3_{x} \beta 
%     &= d_{x} \left( 
%         \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y} \beta \partial_{x}^{2} \alpha 
%     \right) \\
%     &= \partial_{y}^{3} \beta \left(\partial_{x} \alpha \right)^3 
%     + 2  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial^2_{y} \beta \partial_{x} \alpha \partial_{x}^{2} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha\\
%     &= \partial_{y}^{3} \beta \left(\partial_{x} \alpha \right)^3 
%     + 3  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha = 0\\
%     \implies&\\
%     \partial_{y}^{3} \beta &= -\left(\partial_{x} \alpha \right)^{-3}\left[ 3  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha \right] \\
%      &= -\left(\partial_{x} \alpha \right)^{-4}\left[ 3   \partial_{x} g_{\alpha}   \partial^2_{x} \alpha
%     + \partial_{x}^{3} \alpha \right] \\
% \end{align}

% \begin{align}
%     d_{\bm \theta} d^2_{x} \beta 
%     &= d_{\bm \theta} \left( 
%         \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y} \beta \partial_{x}^{2} \alpha 
%     \right) \\
%     &= 
%         \partial_{\bm \theta} \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{3} \beta  \partial_{\bm \theta} \alpha \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{2} \beta 2 \left(\partial_{x} \alpha \right) \partial_{\bm \theta} \partial_{x} \alpha
%         \\ &\quad
%         + \partial_{\bm \theta} \partial_{y} \beta \partial_{x}^{2} \alpha 
%         + \partial^2_{y} \beta \partial_{\bm \theta} \alpha \partial_{x}^{2} \alpha 
%         + \partial_{y} \beta \partial_{\bm \theta} \partial_{x}^{2} \alpha = 0
%      \\
%     \implies&\\
%     \partial_{\bm \theta} \partial_{y}^{2} \beta &= - \left(\partial_{x} \alpha \right)^{-2} \Bigg[ 
%         \partial_{y}^{3} \beta  \partial_{\bm \theta} \alpha \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{2} \beta 2 \left(\partial_{x} \alpha \right) \partial_{\bm \theta} \partial_{x} \alpha
%         \\ &\qquad\qquad\qquad
%         + \partial_{\bm \theta} \partial_{y} \beta \partial_{x}^{2} \alpha 
%         + \partial^2_{y} \beta \partial_{\bm \theta} \alpha \partial_{x}^{2} \alpha 
%         + \partial_{y} \beta \partial_{\bm \theta} \partial_{x}^{2} \alpha
%     \Bigg] 
% \end{align}
% \begin{theorem}
% \label{thm:backward-gradients}
% Let 
% \begin{align*}
%     \alpha \colon (x, \bm w) \mapsto y, \qquad
%     \beta \colon (y, \bm w) \mapsto x
% \end{align*}
% be $\mathcal{C}^{2}$ functions satisfying
% \begin{align} \label{eq:inverse-condition}
%     \alpha(\beta(y, \bm w), \bm w) = y, \qquad
%     \beta(\alpha(x, \bm w), \bm w) = x
% \end{align}
% and 
% \begin{align}
%     \partial_{x} \alpha(x, \bm w) > 0, \qquad
%     \partial_{y} \beta(y, \bm w) > 0.
% \end{align}
% Let further
% \begin{align}
%     g_{\alpha}(x,\bm w) = \log \left| \frac{\partial \alpha(x, \bm w) }{\partial x } \right|, \quad g_{\beta}(y,\bm u) = \log \left| \frac{\partial \beta(y, \bm u) }{\partial y } \right|.
% \end{align}
% For $y = \alpha(x, \bm w)$, we have the following gradient relations
% \begin{align}
%     \frac{\partial \beta(y, \bm w)}{\partial y} &= \left(\frac{\partial \alpha(x, \bm w)}{\partial x}\right)^{-1} \label{eq:grad-out-in} \\
%     \frac{\partial \beta(y, \bm w)}{\partial \bm w} &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x}\right)^{-1} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} \label{eq:grad-out-param} \\
%     \frac{\partial g_{\beta}(y, \bm w)}{\partial y} &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x}\right)^{-1} \frac{\partial g_{\alpha}(x, \bm w)}{\partial x} \label{eq:grad-density-in} \\
%     \frac{\partial g_{\beta}(y, \bm w)}{\partial \bm w} &= \left(\frac{\partial \alpha(y, \bm w)}{\partial x}\right)^{-1} \left(  \frac{\partial g_{\alpha}(x, \bm w)}{\partial x} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} - \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x} \right) \label{eq:grad-density-param}
% \end{align}
% \end{theorem}
% \begin{proof}
%     We have
%     \begin{align}
%         \frac{d}{d x} \beta(\alpha(x, \bm w), \bm w) = \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial \alpha(x, \bm w)}{\partial x} = \frac{d x}{d x} = 1.
%     \end{align}
%     and thus
%     \begin{align}
%         \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y}  &= \left( \frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-1}.
%     \end{align}
    
%     Now 
%     \begin{align}
%         \frac{d}{d \bm w} \beta(\alpha(x, \bm w), \bm w) = \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} + \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial \bm w} = \frac{\partial x}{\partial \bm w} = 0.
%     \end{align}
%     which gives
%     \begin{align}
%         \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial \bm w} 
%         &= \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial \alpha(x, \bm w)}{\partial \bm w}\\
%         &= \left( \frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-1} \frac{\partial \alpha(x, \bm w)}{\partial \bm w}
%     \end{align}
    
%     Combining
%     \begin{align}
%         \frac{d }{d x} \frac{d }{d x} \beta(\alpha(x, \bm w), \bm w) &= \frac{d }{d x} \frac{d x}{d x} = 0
%     \end{align}
%     and 
%     \begin{align}
%         \frac{d }{d x} \frac{d }{d x} \beta(\alpha(x, \bm w), \bm w) &= \frac{d}{dx} \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial \alpha(x, \bm w)}{\partial x} \\
%         &= \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial y\partial y} \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{2} + \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial^{2} \alpha(x, \bm w)}{\partial x \partial x} 
%     \end{align}
%     we obtain
%     \begin{align}
%     \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial y \partial y}
%           &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial^{2} \alpha(x, \bm w)}{\partial x \partial x} \\
%           &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-3} \frac{\partial^{2} \alpha(x, \bm w)}{\partial x \partial x}\\
%           &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \frac{\partial}{\partial x} \log \left ( \frac{\partial \alpha(x, \bm w)}{\partial x}  \right)\\
%           &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \frac{\partial g_{\alpha}(x, \bm w)}{\partial x}
%     \end{align}
%     and thus
%     \begin{align}
%         \frac{\partial}{\partial y} g_{\beta}(\alpha(x, \bm w), \bm w) 
%         &= \left(\frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \right)^{-1} \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial y \partial y} \\
%         &= - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-1} \frac{\partial g_{\alpha}(x, \bm w)}{\partial x}
%     \end{align}
%     Finally, combining
%     \begin{align}
%         \frac{d }{d \bm w} \frac{d }{d x} \beta(\alpha(x, \bm w), \bm w)  &= \frac{d }{d \bm w} \frac{d x}{d x} = 0
%     \end{align}
%     and
%     \begin{align}
%         \frac{d }{d \bm w} \frac{d }{d x} \beta(\alpha(x, \bm w), \bm w) 
%         &= \frac{d}{d\bm w} \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial \alpha(x, \bm w)}{\partial x} \\
%         &= \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial y\partial y} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} \frac{\partial \alpha(x, \bm w)}{\partial x}  \\
%         &\quad + \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x} \nonumber  \\
%         &\quad + \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial \bm w  \partial y} \frac{\partial \alpha(x, \bm w)}{\partial x} \nonumber 
%     \end{align}
%     gives us
%     \begin{align}
%         \frac{\partial^{2}\beta(\alpha(x, \bm w), \bm w)}{\partial \bm w \partial y}
%         &= - \frac{\partial^{2} \beta(\alpha(x, \bm w), \bm w)}{\partial y\partial y} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} \\
%         &\quad - \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y} \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x} \left(\frac{\partial \alpha(x, \bm w)}{\partial x}\right)^{-1} \nonumber \\
%         &= \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \frac{\partial g_{\alpha}(x, \bm w)}{\partial x} \frac{\partial \alpha(x, \bm w)}{\partial \bm w} \\
%         &\quad - \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x}  \nonumber\\
%         &= \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-2} \left(\frac{\partial g_{\alpha}(x, \bm w)}{\partial x} \frac{\partial \alpha(x, \bm w)}{\partial \bm w}  - \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x} \right)
%     \end{align}
%     And thus we get
%     \begin{align}
%         \frac{\partial}{\partial \bm w} g_{\beta}(\alpha(x, \bm w), \bm w) 
%         &= \left( \frac{\partial \beta(\alpha(x, \bm w), \bm w)}{\partial y}  \right)^{-1} \frac{\partial^{2}\beta(\alpha(x, \bm w), \bm w)}{\partial \bm w \partial y} \\
%         &= \left(\frac{\partial \alpha(x, \bm w)}{\partial x} \right)^{-1} \left(\frac{\partial g_{\alpha}(x, \bm w)}{\partial x} \frac{\partial \alpha(x, \bm w)}{\partial \bm w}  - \frac{\partial^{2} \alpha(x, \bm w)}{\partial \bm w \partial x} \right)
%     \end{align}
% \end{proof}

% \paragraph{Additional derivations for forces}

% \begin{align}
%     d^3_{x} \beta 
%     &= d_{x} \left( 
%         \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y} \beta \partial_{x}^{2} \alpha 
%     \right) \\
%     &= \partial_{y}^{3} \beta \left(\partial_{x} \alpha \right)^3 
%     + 2  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial^2_{y} \beta \partial_{x} \alpha \partial_{x}^{2} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha\\
%     &= \partial_{y}^{3} \beta \left(\partial_{x} \alpha \right)^3 
%     + 3  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha = 0\\
%     \implies&\\
%     \partial_{y}^{3} \beta &= -\left(\partial_{x} \alpha \right)^{-3}\left[ 3  \partial_{y}^{2} \beta \partial_{x} \alpha \partial^2_{x} \alpha
%     + \partial_{y} \beta \partial_{x}^{3} \alpha \right] \\
%      &= -\left(\partial_{x} \alpha \right)^{-4}\left[ 3   \partial_{x} g_{\alpha}   \partial^2_{x} \alpha
%     + \partial_{x}^{3} \alpha \right] \\
% \end{align}

% \begin{align}
%     d_{\bm w} d^2_{x} \beta 
%     &= d_{\bm w} \left( 
%         \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y} \beta \partial_{x}^{2} \alpha 
%     \right) \\
%     &= 
%         \partial_{\bm w} \partial_{y}^{2} \beta \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{3} \beta  \partial_{\bm w} \alpha \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{2} \beta 2 \left(\partial_{x} \alpha \right) \partial_{\bm w} \partial_{x} \alpha
%         \\ &\quad
%         + \partial_{\bm w} \partial_{y} \beta \partial_{x}^{2} \alpha 
%         + \partial^2_{y} \beta \partial_{\bm w} \alpha \partial_{x}^{2} \alpha 
%         + \partial_{y} \beta \partial_{\bm w} \partial_{x}^{2} \alpha = 0
%      \\
%     \implies&\\
%     \partial_{\bm w} \partial_{y}^{2} \beta &= - \left(\partial_{x} \alpha \right)^{-2} \Bigg[ 
%         \partial_{y}^{3} \beta  \partial_{\bm w} \alpha \left(\partial_{x} \alpha \right)^2 
%         + \partial_{y}^{2} \beta 2 \left(\partial_{x} \alpha \right) \partial_{\bm w} \partial_{x} \alpha
%         \\ &\qquad\qquad\qquad
%         + \partial_{\bm w} \partial_{y} \beta \partial_{x}^{2} \alpha 
%         + \partial^2_{y} \beta \partial_{\bm w} \alpha \partial_{x}^{2} \alpha 
%         + \partial_{y} \beta \partial_{\bm w} \partial_{x}^{2} \alpha
%     \Bigg] 
% \end{align}

\section{Additional Details for Experiments}

\subsection{Illustrative Toy Examples}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/toy-example-periodic.png}
    \caption{a) Reference density and corresponding force field as approximated by b) a $C^{1}$-smooth NSF and c) a $C^{\infty}$-smooth flow using the mixture of bump functions introduced in Sec. \ref{sec:bump-functions}.}
    \label{fig:toy-periodic}
\end{figure}

In addition to the simple example presented in Fig. \ref{fig:toy} we give another example for the periodic case in Fig. \ref{fig:toy-periodic}. As in the non-periodic case, the periodic splines introduce dramatic outliers in the forces, whereas the force field of the $C^{\infty}$ periodic flows remains smooth.

\paragraph{Compared architecture}
In both experiments (periodic and non-periodic), we used flows made of four coupling layers where we swapped first and second dimension between each layer. Each conditional component-wise transformation used 40 bins (spline case) or 40 mixture components (smooth cases). For the conditioner networks we used simple dense nets with two hidden layers of size 100 and Swish activations. To satisfy the periodic boundary condition in the periodic case we expand the inputs to these conditioner nets in a cosine basis. 

\paragraph{Potentials and data}
We use two simple potentials allowing us to compute the ground truth density and forces analytically.

For the non-periodic case presented in Fig. \ref{fig:toy} we used the energy 
\begin{align}
    u(\bm x) = -\log\left(\sum_{i} \alpha_{i} \exp\left(-\frac{\| \|\bm x\|_2 - r_{i}\|_2^2}{2 \sigma}\right) \right)
\end{align}
with $\sigma = 0.06$ and $\alpha =\left[ 1, 0.8, 0.6, 0.4 \right]$.

For the periodic case presented in Fig. \ref{fig:toy-periodic} we used the energy
\begin{align}
    \beta(\bm x) &= \left( \cos\left(x_{1}^{2}\right) + \cos\left(x_{2}^{2}\right) + 2 \right)^{\frac{1}{2}} \\
    u(\bm x) &= -\log\left(\sum_{i} \alpha_{i} \exp\left(-\frac{\| \beta(\bm x) - r_{i}\|_2^2}{2 \sigma}\right) \right)
\end{align}
with $\sigma = 0.05$ and $\alpha =\left[ 1, 0.8, 0.6, 0.4 \right]$.

We generated data from $u$ by first running 1,000  Metropolis-Hastings  steps using 10,000 parallel chains. The the initial configurations fo the chain were sampled in a regular grid. Then we ran each chain for another 10 steps providing us with 100,000 samples in total.

\paragraph{Training}
We trained all flows with MLE using a batch size of 1,000 for 8,000 iterations and using Adam \cite{kingma2014adam} optimizer with learning rate of $0.0005$.

\subsection{Runtime Comparison}


Table \ref{tab:performance} shows the difference in performance between a neural spline flow with analytic inversion and root-finding inversion. For small tensor dimensions, the optimal multi-bin bisection can employ up to 256 bins on the GPU, which results in only a factor of 2-3 slowdown compared to analytic inversion. For larger dimensions, the parallelization over multiple bins becomes less effective, leading to one order of magnitude difference in computational cost.

\begin{table}[ht]
  \caption{Runtimes (in ms) of analytic inversion and root-finding inversion of neural spline flows averaged over 10 runs with 100 evaluations each. $K_\mathrm{opt}$ denotes the number of bins that yielded the fastest root-finding inversion. The computations were performed on an NVIDIA GeForce GTX1080.}
  \label{tab:performance}
  \centering
  \begin{tabular}{llrrrlrrr}
    \toprule
    {} & \multicolumn{4}{c}{Inversion} & \multicolumn{4}{c}{Inversion + Backpropagation} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    dim & $K_\mathrm{opt}$ &  analytic &  root-finding &  factor & $K_\mathrm{opt}$ &  analytic &  root-finding &  factor \\
    \midrule
2& 156.8  &  11.8  &  22.6  &  2.1  &  112.0  &  13.3  &  25.4  &  2.0     \\ 
&  ($\pm$54.6)  &  ($\pm$2.0)  &  ($\pm$2.2)  &  ($\pm$0.5)  &  ($\pm$41.5)  &  ($\pm$1.7)  &  ($\pm$2.6)  &  ($\pm$0.4)     \\ 
8& 60.0  &  11.3  &  25.0  &  2.4  &  46.4  &  12.4  &  24.8  &  2.1     \\ 
& ($\pm$30.5)  &  ($\pm$1.9)  &  ($\pm$2.1)  &  ($\pm$0.5)  &  ($\pm$21.0)  &  ($\pm$1.5)  &  ($\pm$2.3)  &  ($\pm$0.3)    \\ 
32& 27.6  &  11.2  &  28.3  &  2.7  &  48.0  &  11.8  &  31.5  &  3.0     \\ 
&($\pm$9.8)  &  ($\pm$1.7)  &  ($\pm$2.7)  &  ($\pm$0.4)  &  ($\pm$20.2)  &  ($\pm$1.9)  &  ($\pm$2.8)  &  ($\pm$0.8)     \\ 
128& 14.4  &  12.6  &  44.1  &  3.7  &  12.4  &  12.1  &  40.7  &  3.5   \\ 
&($\pm$4.4)  & ($\pm$1.9)  &  ($\pm$5.4)  &  ($\pm$0.6)  &  ($\pm$2.9)  &  ($\pm$1.5)  &  ($\pm$4.9)  &  ($\pm$0.6)    \\ 
512& 6.0  &  12.2  &  76.0  &  6.5  &  5.6  &  10.6  &  75.7  &  7.8     \\ 
&($\pm$1.3)  &  ($\pm$1.5)  &  ($\pm$5.3)  &  ($\pm$1.1)  &  ($\pm$1.2)  &  ($\pm$1.9)  &  ($\pm$3.5  &  ($\pm$1.5)     \\ 
2048& 4.0  &  15.8  &  209.8  &  13.4  &  4.0  &  16.1  &  211.1  &  13.2    \\ 
&($\pm$0.0)  &  ($\pm$1.0)  &  ($\pm$3.6)  &  ($\pm$0.9)  &  ($\pm$0.0)  &  ($\pm$0.9)  &  ($\pm$4.1)  &  ($\pm$0.7)     \\ 
    \bottomrule
\end{tabular}
\end{table}


\subsection{Alanine Dipeptide Target Energy and Data}
\label{sec:ala2}
The molecular system contained one alanine dipeptide molecule (a 22-atom molecule) in implicit solvent. Its energy function was defined by the Amber ff99SB-ILDN force field with the Amber99 Generalized Born (OBC) solvation model. All covalent bonds were flexible.

Training samples were generated in OpenMM by running molecular dynamics (MD) simulations in the canonical ensemble at 300 K. The 1 $\mu$s simulation used Langevin integration with a 1/ps friction coefficient and a 1 fs time step. Coordinates and forces were saved in 1 ps intervals. 

%The benchmark dataset is available from https://github.com/noegroup/bgmol as \texttt{bgmol.datasets.AImplicitUnconstrained} \jonas{anonymize links!}.

The signature feature of the Boltzmann distribution for alanine dipeptide is the joint marginal density over its two backbone dihedrals. This distribution is depicted in the so-called Ramachandran plots in Fig. \ref{fig:ala2_bg}, which are log-scaled heatmaps of the populations. Rotations around these two dihedrals have relatively long transition times and determine the global shape of the molecule, see the exemplary samples in Fig. \ref{fig:ala2_samples} a).


\subsection{Alanine Dipeptide Boltzmann Generator}
\label{sec:ala2bg}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{network.png}
    \caption{Network architecture -- (a) Coupling block: a dense conditioner network $\tilde{c}$ generates shape parameters that define an elementwise bijective transform $t$ from the first input channel. The transform is then applied to the second input channel. (b) Conditioner network for spline transforms returning bin widths, bin heights, and slopes at the support points. Circular inputs are first wrapped around the unit circle. For circular outputs, the first and last slope are enforced to be identical. (c) Conditioner network for smooth transforms (mixtures of bump functions) returning the shape parameters for each bump function. (d) Boltzmann generator architecture for alanine dipeptide using coupling blocks as in (a). The global translation and rotation required for the coordinate transform are fixed. Torsions are circular, angles and bonds are not.}
    \label{fig:network}
\end{figure}

The normalizing flow was set up in a physically informed way, see Fig. \ref{fig:network}. All learnable transforms operated in an internal coordinate framework, where deeper parts of the network corresponded to torsion angles. This is a natural choice, as rotation around torsion angles determine the global structure of the molecules, while bond lengths and 1-3 angles are comparatively stiff.

The base density for intramolecular bond lengths, angles, and torsion angles is defined as uniform distributions on the unit hypercube, $\mathcal{U}\left([0,1]^{21} \times [0,1]^{20} \times [0,1]^{19}\right)$. The latent variables corresponding to torsion angles were split into two input channels. In the first 8 coupling layers, the two torsion channels were conditioned on each other using either neural spline transforms or mixtures of smooth bump functions. After this transformation, the two channels were merged back into a single torsion channel. Bond and angle channels were conditioned on each other using four subsequent coupling layers. Finally, the angle channel was conditioned on all torsions and the bond channel on all angles and torsions in an autoregressive manner.

The number of bins and components was 8 for spline and bump transforms, respectively. All elementwise transforms were defined as bijections on the unit interval, where torsions were considered as circular elements. All conditioners had two fully connected hidden layers with 64 features each and $sin$ activations. All circular degrees of freedom (torsions) were mapped onto the unit sphere before being passed into the dense conditioner network. Torsional degrees of freedom were transformed with circular transforms, bonds and angles with non-circular transforms.

The final two layers were designed as follows. First, all IC channels were mapped from $[0,1]$ to their respective domains by fixed linear transformations, where bonds and angles were constrained to physically reasonable values, [0.05 nm, 0.3 nm] for bonds and [0.15$\pi$, $\pi$] for angles, which bracket the values encountered in the data. Finally, the ICs were transformed to Cartesian coordinates through a global internal coordinate transform.
The global origin and rotation of the system were fixed in the forward pass and ignored in the inverse pass. The flows with spline transformers had 398,691 parameters in total. The flows with smooth transformers had 758,168 parameters as the shape parameters of the bump functions are also learnable.

\subsection{Boltzmann Generator Training}
\label{sec:ala2training}
The loss function \eqref{eq:loss} was minimized using 90\% of the data set and a batch size of 128. The inversion of smooth flows was performed with multi-bin root finding and $K=100$ bins. The Adam optimizer \cite{kingma2014adam} was initialized with a learning rate $5\times 10^{-4}$ and the learning rate was scaled by $0.7$ after each epoch. Optimization was run for 10 epochs. For validation, the negative log likelihood, force matching error, and reverse KL divergence (up to an additive constant) were computed over the remaining 10\% of the dataset. 

Singularities of the target energy function can negatively affect energy-based training. To stabilize the computation of the reverse KL divergence, the loss was cut off for exploding energies. Concretely, the reverse KL divergence was replaced by $\Lambda \left(\mathcal{L}_{\mathrm{KLD}}(\bm{\theta}) \right)$ with

\begin{equation*}
\Lambda(x) := \min \left\{ 
     x, \ 
     10^3 + \log(1 + x - 10^3), \ 
     10^9
\right\}.
\end{equation*}

The training runs for the comparison in Table \ref{tab:metrics} were done over a reduced dataset with only every tenth trajectory snapshot present in both the train and test set and with a constant learning rate of $5\times10^{-4}$ over 10 epochs.




\subsection{Affine Boltzmann Generator}
To compare with a previously published method, a Boltzmann generator with affine transforms was trained using density estimation and the same hyperparameters as the smooth flow depicted in Fig. \ref{fig:ala2_bg}. In comparison, the affine flow does not as cleanly separate modes on the torsion marginal and produces inaccurate forces and energies.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/affine_bg.png}
    \caption{Boltzmann generators with affine (RealNVP) transforms trained through likelihood maximization. See Fig. \ref{fig:ala2_bg} (c) for comparison.} 
    \label{fig:affine_bg}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/sampling_efficiency.pdf}
    \caption{Force residues on the test set (upper row) and on BG samples (lower row) for different flow transforms and training methods. The numbers in the last row denote the sampling efficiency. \label{fig:forces_se}}
\end{figure}

Figure \ref{fig:forces_se} compares forces and sampling efficiency between Boltzmann generators using different types of transforms and training methods via the protocol described in Section \ref{sec:ala2training}.
The sampling efficiency is evaluated as the Kish effective sample size divided by the number of samples.


\subsection{Molecular Simulations}
MD simulations of alanine dipeptide were run for three different potential energy functions. The first potential was the molecular target energy as defined by the Amber ff99SB-ILDN force field with the OBC implicit solvent model. As typical for MD potentials, the energy is defined as a sum over individual terms that correspond to covalent bond stretching, angle bending, rotation around proper and improper dihedrals, pairwise Coulomb and Lennard-Jones interactions, as well as an Generalized-Born implicit solvent term that takes into account the solvent-accessible surface area of each atom and the solvent dielectric constant (78.5 for water). The energy and forces were evaluated using OpenMM. 

%The OpenMM system and a PyTorch-compatible energy model are available from https://github.com/noegroup/bgmol \jonas{anonymize links!}.

The other two potential energies were defined by normalizing flows that were trained on the full dataset for 10 epochs each. The energy of a flow is given by $u_\theta = -\log p_f(\cdot; \bm{\theta}),$ where $p_f$ denotes the push-forward distribution of the prior under the learned bijection $f.$ Both flows used the same architecture except all flow transforms were smooth transforms (mixtures of 8 smooth bump functions) in one case and neural spline transforms (rational-quadratic splines \cite{rezende2020normalizing, wirnsberger2020targeted} with 8 bins) in the other case. 
The smooth flow was the one described in Sec. \ref{sec:smooth_fm}, which was trained by a combination of force matching and MLE. The spline flow was trained by MLE only since spline flows trained with a mixed loss performed poorly on the validation set (cf. Table \ref{tab:metrics}). Otherwise, all hyperparameters were identical. From each training, the flow with the smallest validation loss was selected for the MD simulation.

The first 10 samples from the dataset were chosen as starting configurations. Initial velocities were sampled randomly from a Maxwell-Boltzmann distribution at 300 K. First,  1000 integration steps were performed with a Langevin integrator in each potential to equilibrate the kinetic energy. Next, the thermostat was removed and the system was propagated for 5000 steps using a velocity Verlet integrator, which keeps the total energy constant up to numerical errors. 

Consequently, the energy fluctuation and drift are a measure for numerical errors accumulated in the simulations. For the classical force field, these errors arise primarily from the fastest degrees of freedom (in this case the covalent hydrogen bond vibration) that are not sufficiently resolved by the 1 fs time step. The fact that smooth flows exhibit energy fluctuations in a similar range as the MD force field indicates that the roughness of the potential energy surface is comparable.

\section{Computing Requirements}
The MD simulation and network training was performed on an in-house cluster equipped with NVIDIA GeForce GTX 1080 GPUs. The net runtime for all sampling and training reported in this work was approximately (30 experiments x 10 replicas x 5h) = 1500 h.

\section{Used Third-Party Software}
All our models were implemented in \texttt{PyTorch} \cite{paszke2017automatic}. For the Neural Spline Flows, we used the \texttt{nflows} library provided by \citet{durkan2019neural}. We further used \texttt{OpenMM} \cite{eastman2017openmm} for the simulations and \texttt{mdtraj} \cite{mcgibbon2015mdtraj} for analyzing the MD data. Beyond this we used \texttt{NumPy}\cite{harris2020array} for all non-GPU related numerical computations and \texttt{Matplolib} \cite{Hunter:2007} for plotting.

% \bibliography{references}

\end{document}