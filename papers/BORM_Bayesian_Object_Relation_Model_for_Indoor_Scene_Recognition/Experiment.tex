

\subsection{Experimental Settings}
We evaluated the proposed models on reduced SUN RGB-D and Places365 dataset. To be noticed, aim to investigate the generalizability of our model, we evaluate our model pretrained on Places365-7 dataset on the SUN RGB-D dataset without retraining it. We'll introduce the implementation details and training procedure and different experiment settings.

\subsubsection{Implementation Details}
For the PlacesCNN model, ResNet18 or ResNet50 architecture is adopted in our experiment for ablation study. The optimizer used is the Stochastic Gradient Descent (SGD) with an initial learning rate of 0.01, the momentum of 0.9, and the weight decay of 0.0001. We decrease the learning rate 10 times every 10 epoch, and every time when updating the learning rate, we reload the parameters which have the best accuracy before this timestamp. The total number of epoch during training is 40. To be noticed, we use the training sets of Places365-7 and Places365-14 dataset for learning the BORM statistically, while testing on the SUN RGB-D dataset, where the BORM is the same as the Places365-7 dataset. 


\subsubsection{Dataset Settings}

\begin{table}[]
\centering
	\caption{Dataset split setting, where the number of training set and number of test set are listed below.}
	\label{tab:dataset_split}
	\begin{tabular}{l|l|l}
		\hline
		Dataset      & Training & Test  \\ \hline
		Places365-7  &  35000   & 701     \\ 
		Places365-14 &  75000   & 1500    \\ 
		SUN RGB-D    &  35000(from Places365-7)        & 2077    \\ \hline
	\end{tabular}
\end{table}
\ \\
\textbf{Places365 Dataset:}
In this paper, we use the reduced Places365 \cite{zhou2017places}  dataset to test our methods, since it is the most largest and challenging scene classification dataset yet, and it contains broad categories in the indoor environment. In the experiment, we only consider the indoor scene recognition. There are two different settings on the reduced Places365 dataset. The one is Places365 with 7 classes includes Corridor, Dinning Room, Kitchen, Living Room, Bedroom, Office, and Bathroom, denoted as Places365-7.  The test set setting is the same as the official dataset and described in \cite{pal2019deduce}.
In addition, we use the reduced Places365 with 14 indoor scenes in Home environment includes Wet bar, Home theater, Balcony, Closet, Kitchen, Bedroom, Playroom, Laundromat, Bathroom, Living Room, Home office, Dining room, Staircase, and Garage denoted as Places365-14. The dataset splitting follows the same setting as described in \cite{chen2019scene}. The dataset splition can be seen from Table. \ref{tab:dataset_split}.

\textbf{SUN RGB-D Dataset:}
SUN RGB-D dataset \cite{song2015sun} is a challenging dataset for scene understanding that contains not only RGB images but also depth information of each image. It contains 3784 images collected by Kinect V2 and 1159 collected by Intel RealSense. Moreover, it incorporates 1449 images from the NYUDepth V2 \cite{silberman2012indoor}, and 554 images from the Berkeley B3DO Dataset \cite{janoch2013category}, both captured by Kinect V1. Finally, it takes 3389 manually selected distinguished frames without significant motion blur from the SUN3D videos \cite{xiao2013sun3d} captured by Asus Xtion.

In our experiment, we mainly consider the indoor environment understanding. Therefore, we use the reduced SUN RGB-D dataset includes Office, Kitchen, Bedroom, Corridor, Bathroom, Living room, and Dining room, where the test set split is the same as the official dataset. There are 3741 RGB images in total for testing. And we test our model pretrained on the Place365-7 on SUN RGB-D dataset without retraining.



\subsection{Experimental Results}
\subsubsection{Effect of Object Knowledge}
As illustrated in Fig. \ref{fig:om_ablation}, a group of ablation studies have been conducted for evaluating the effect of object knowledge to indoor scene recognition on the Places365-14, Places365-7, and SUN RGB-D dataset, respectively. The x-axis represent the number of object information IOM have about the indoor scene and is added by 20 from 90 to 150 sequentially selected from the vector. Plus, IOM-80 is the baseline accuracy. Obviously, as the number of object information increases, the much better scene recognition accuracy is achieved, e.g., on the Places365-14 dataset, the IOM-150 reaches 74.1\% accuracy, which is \textbf{10.0\%} higher than IOM-80. This improvement shows the number of object information is proportional to scene recognition accuracy. Moreover, the comparison experiments between the IOM and OM on three datasets, shows an average of \textbf{20.5\%} improvements can be achieved. After analyzing the object categories of OM pretrained on the MS COCO, we observed there are only half of the object categories are related to indoor scenes. In contrast, the other half is related to outdoor scenes. Therefore, the relevance of object categories of object model with the scenes is essential for scene recognition, e.g., the information of elephant and giraffe in OM will not be valuable for indoor scene recognition. Similarly, the bus and train are not beneficial to indoor scene recognition.

\subsubsection{{Analysis of BORM}}

\begin{figure}[tbp]
	\centering
	\includegraphics[width=0.4\textwidth]{Fig//om_abl.jpg}
	\caption{Ablation study of improved object model (IOM) with different number of object knowledge ranging from 80 to 150 categories as shown in horizontal axis. The vertical axis shows the accuracy on percentage.}
	\label{fig:om_ablation}
\end{figure}

%\begin{table*}[]
%	\centering
%	\scriptsize
%	\caption{Ablation study of accuracy on percentage of improved object model (IOM) with different number of object knowledge ranging from 80 to 150 categories.}
%	\label{tab:om_ablation}
%	\begin{tabular}{l|c|c|cccccccc}
%		\hline
%		\textbf{} & {$\Phi_{obj}$ \cite{pal2019deduce}} & OM-80 & {IOM-80} & {IOM-90} & {IOM-100} & {IOM-110} & {IOM-120} & {IOM-130} & {IOM-140} & {IOM-150} \\ \hline 
%		{Places365-14} & - & 47.0 & 64.1 & 64.4 & 66.1 & 71.5 & 71.9 & 72.5 & 73.7 & \textbf{74.1} \\ 
%		{Places365-7} & 62.6 & 73.0 & 80.9 & 81.6 & 81.6 & 81.9 & 81.7 & 82.0 & 82.1 & \textbf{82.6} \\ 
%		{SUN RGBD} & 53.6 & 59.2 & 65.9 & 66.8 & 66.7 & 67.0 & 66.8 & 67.3 & 67.8 & \textbf{68.1} \\ \hline
%	\end{tabular}
%\end{table*}


%\begin{table*}[htbp]
%\centering
%\scriptsize
%\caption{Scene recognition accuracy in percentage on the reduced Places365-7 dataset}
%\label{tab:place365_7}
%\begin{tabular}{cccccccccc}
%\hline
%            & ResNet18 & ResNet50 & $\Phi_{obj}$ \cite{pal2019deduce} & OM  & IOM  & BORM  & CBORM \\ \hline
%Bathroom    & 87 & 94 & 65 & 71  & 87      &  88     &  95   \\
%Bedroom     & 82 & 83 & 74 & 84  & 92      &  92     &  81   \\
%Corridor    & 96 & 93 & 90 & 92  & 91      &  89     &  95   \\
%Dining room & 81 & 71 & 94 & 74  & 85      &  80     &  93   \\
%Kitchen     & 83 & 84 & 62 & 65  & 73      &  83     &  94   \\
%Living room & 55 & 66 & 25 & 66  & 73      &  72     &  92   \\
%Office      & 79 & 88 & 29 & 58  & 76      &  78     &  81   \\ \hline
%Avg         & 80.4 & 82.7 & 62.6 & 72.9   &  82.4   & \textbf{83.1}  & \textbf{90.1}    \\ \hline
%\end{tabular}
%\end{table*}



We conduct experiments for BORM and IOM in the reduced Places365-7 dataset, and results are displayed in Table \ref{tab:place365_7_sota}. The experiment results show the BORM and IOM model has an advantage over the OM and yields an average accuracy of 83.1\% and 82.4\%, surpassing the OM model about \textbf{20\%} accuracy. The experiment result proves that with more object knowledge about the surrounding environment, the greater scene recognition accuracy can be reached.  Then, we test the model pretrained on the Places365-7 dataset on the SUN RGB-D test set, and similar conclusion can be drawn. Moreover, the BORM outperforms the IOM with 0.7\% and 1.1\% accuracy on the Places365-7 and SUN RGB-D dataset, respectively, which validates the knowledge of the co-occurrences between object pairs and their probabilistic relation forms an important indoor scene representation.


Similarly, in the Table \ref{tab:place365_14_sota}, we have conducted experiments on the reduced Places365-14 dataset, and experiment results show the IOM and BORM tremendously improves the performance over OM with \textbf{27\%} accuracy. The results suggest the effectiveness of BORM and IOM over the OM especially when the number of scene classes of dataset is large.



\subsubsection{Performance Comparison}


\begin{table}[htbp]
	\centering
	\scriptsize
	\begin{minipage}[t]{.44\textwidth}
		\centering
		\caption{Comparison with the state-of-the-art methods on the reduced Places365-7 Dataset and SUN dataset of scene recognition accuracy}
		\label{tab:place365_7_sota}
		\begin{tabular}{c|ccc}
			\hline
			Method     & Config   & Acc(Places365-7) & Acc(SUN) \\ \hline
			\multirow{2}{*}{PlacesCNN \cite{zhou2017places}}     & ResNet18 & 80.4  & 63.3  \\
			& ResNet50 & 82.7 & 67.2   \\ \hline
			\multirow{3}{*}{Deduce \cite{pal2019deduce}}     &  $\Phi_{obj}$ (OM)      & 62.6 & 53.6      \\
			& $\Phi_{scene}$    & 87.3 & 66.8    \\
			& $\Phi_{comb.}$ & 88.1 & 70.1    \\ \hline
			%\multirow{1}{*} {Baseline}
			%& OM   & 72.9 & 59.2   \\   \hline 
			\multirow{3}{*}{Ours} 	
			& IOM  & 82.4 & 68.1   \\
			& BORM & 83.1 & 69.2   \\ \cdashline{2-4}
%			& CIOM & 90.1 & 71.8   \\ 
			& CBORM& \textbf{90.1} & \textbf{72.1\%}   \\	\hline
		\end{tabular}
	\end{minipage}

	\hspace{1cm}

	\begin{minipage}[t]{.44\textwidth}
		\centering
		\caption{Comparison with the state-of-the-art methods on the reduced Places365-14 Dataset of scene recognition accuracy, the * indicates the re-implement of the method. }
		\label{tab:place365_14_sota}
		\begin{tabular}{c|cc}
			\hline
			Method                    & Config           & Acc \\ \hline
			\multirow{2}{*}{PlacesCNN \cite{zhou2017places}}     & ResNet18 & 76.0  \\
			& ResNet50 & 80.0   \\ \hline
			\multirow{1}{*}{Word2Vec \cite{chen2019scene}} 
			%& ResNet50         & 83.5    \\
			& ResNet50+Word2Vec         & 83.7    \\ \hline
%			\multirow{1}{*}{Deduce \cite{pal2019deduce}}     & Obj(re-implement)      & 47.0     \\ \hline
            \multirow{1}{*}{{*}Deduce \cite{pal2019deduce}} 
            
            & $\Phi_{obj}$ (OM) 		   & 47.0	 \\ \hline
			\multirow{3}{*}{Ours}  
			& IOM          & 74.1    \\
			& BORM         & 74.9    \\ \cdashline{2-3}
%			& CIOM & 85.5    		\\
			& CBORM & \textbf{85.8}				\\	\hline
		\end{tabular}
	\end{minipage}
\end{table}



As shown in Table \ref{tab:place365_7_sota}, we conduct the ablation study of using only the BORM model and the CBORM model. We found the CBORM yield an average accuracy of \textbf{90.1\%}, which greatly outperforms the ResNet18 and ResNet50 of PlacesCNN baselines about \textbf{10\%} and \textbf{8\%} respectively. Meanwhile, CBORM outperforms the BORM with 7\% accuracy and 3\% on the Places365-7 and SUN RGB-D dataset, respectively.

%Also, it surpass the Scene (ResNet18 model that pretrained on Places365 and finetuned on the Places365-7 dataset). 

In comparison with the state of the art, Table \ref{tab:place365_7_sota} shows that the CBORM improves the scene recognition by \textbf{2.0\%} in the reduce Places365-7 dataset.  Also, as shown in Table \ref{tab:place365_14_sota}, the CBORM improve the recognition accuracy by \textbf{2.1\%} in the reduced Places365-14 dataset. Both results show the combined model achieves comparable results to some recent approaches that use the word-embedding method to extract the semantic meaning of the environment, or use the combination of scene and object representations for better scene understanding. Moreover, Table \ref{tab:place365_7_sota} shows the performance of our method over the method in \cite{pal2019deduce} with \textbf{2\%}, showing the excellent generalization ability of CBORM over other methods on the Reduced SUN RGB-D dataset.

These results demonstrate that CBORM is successful in recognizing the scene images with a competitive accuracy. This improved effectiveness of CBORM over the state-of-the-art justifies our reasonable assumption that relation of object pairs is an essential complementary information for indoor scene recognition.



