%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper
                                                          % needed if you want to
                                                          % use the \thanks
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document
\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command



% The following packages can be found on http:\\www.ctan.org
\usepackage{amsmath,epsfig}
\usepackage{graphicx} %use graph format
\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{multirow}
%\usepackage{booktabs} % Publication quality tables
\usepackage{threeparttable}
\usepackage{subfigure}
\PassOptionsToPackage{bookmarks=false}{hyperref}
%\usepackage[american]{babel}
%\usepackage[colorlinks,linkcolor=blue]{hyperref}
\renewcommand{\arraystretch}{1.0} %?????§Ú?
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\usepackage{amsmath} % assumes amsmath package installed
%\renewcommand\thesection{\arabic {section}}
%\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\usepackage{arydshln}

\usepackage{cite}
\renewcommand\citepunct{,~}
\renewcommand\citedash{-}

%\usepackage[square, comma, sort&compress, numbers]{natbib}
%\biboptions{numbers,sort&compress}

\usepackage[colorlinks=true,
            linkcolor=red,
            urlcolor=blue,
            citecolor=gray]{hyperref}

\definecolor{dgreen}{rgb}{0, 0.6, 0}
\newcommand{\wzxrevise}[1]{\textcolor{blue}{#1}}
\newcommand{\torevise}[1]{\textcolor{dgreen}{#1}}
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
BORM: Bayesian Object Relation Model for Indoor Scene Recognition  
}
\author{~~~~~{Liguang Zhou$^{1,2}$, Jun Cen$^{2,3}$, Xingchao Wang$^{1,2}$, Zhenglong Sun$^{1,2}$, Tin Lun Lam$^{1,2,\dagger}$, Yangsheng Xu$^{1,2}$}
%\thanks{*The first and second authors contributed equally to this work.}
%\thanks{$^{\dagger}$ The corresponding author of this work is }%
\thanks{$^{\dagger}$ Corresponding Author: Tin Lun Lam (tllam@cuhk.edu.cn)}
\thanks{This work was supported in part by the funding AC01202101025 and 2019-INT007 from the Shenzhen Institute of Artificial Intelligence and Robotics for Society.}
\thanks{\textsuperscript{1} School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen}
\thanks{\textsuperscript{2}  Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen.}
\thanks{\textsuperscript{3} Hong Kong University of Science and Technology}
%\thanks{\tt\small {liguangzhou }@link.cuhk.edu.cn}
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%{Recently, the object model (OM) has been proposed for scene recognition with the knowledge of the few objects categories pretrained on MS COCO as indoor scene representations. However, half of the classes in MS COCO is not related to the indoor scene.Moreover, the object pair relations, an important representation that shows co-occurrences of object pairs in different scenes, are not studied.}

\begin{abstract}
Scene recognition is a fundamental task in robotic perception. For human beings, scene recognition is reasonable because they have abundant object knowledge of the real world. The idea of transferring prior object knowledge from humans to scene recognition is significant but still less exploited. 
In this paper, we propose to utilize meaningful object representations for indoor scene representation. First, we utilize an improved object model (IOM) as a baseline that enriches the object knowledge by introducing a scene parsing algorithm pretrained on the ADE20K dataset with rich object categories related to the indoor scene. To analyze the object co-occurrences and pairwise object relations, we formulate the IOM from a Bayesian perspective as the Bayesian object relation model (BORM). Meanwhile, we incorporate the proposed BORM with the PlacesCNN model as the combined Bayesian object relation model (CBORM) for scene recognition and significantly outperforms the state-of-the-art methods on the reduced Places365 dataset, and SUN RGB-D dataset without retraining, showing the excellent generalization ability of the proposed method. Code can be found at \href{https://github.com/hszhoushen/borm}{\textcolor{magenta}{https://github.com/FreeformRobotics/BORM}}.
\end{abstract}


%\begin{keywords}
%Object pair relations for scene recognition, mobile robot perception.
%\end{keywords}


\section{Introduction}
\label{sec:intro}


Scene understanding is a fundamental cognitive ability for robots to conduct tasks in an unknown environment. To carry out the task commanded by controllers, the most fundamental problem is recognizing the environment robots are traveling in, such as the living room, bedroom, or kitchen.

Recognizing scene about the current environment is quite essential for robots to make more intelligent behaviors. Fortunately, scene recognition has been an important research area among computer vision and robotics for decades. There have been many scene recognition algorithms proposed that using visual information for scene understanding. Liu et al. \cite{liu2009scene}\cite{Liu2013} propose a color and geometric features based adaptive descriptor for scene recognition. Also, indoor scenes can be characterized by global spatial features \cite{quattoni2009recognizing}. However, these methods are based on the traditional lower-level features, which are not accurate enough, and not interpretable at the semantic level.

\begin{figure}[tbp]
	\centering
	\includegraphics[width=0.48\textwidth]{Fig//Fig1.pdf}
	\caption{The left part of the figure shows the input image and the corresponding scene parsing result. We have conducted a Bayesian probabilistic analysis using the BORM on the Places365-14 dataset. The middle part shows the conditional probability of $P(scene|object)$, and the top 6 scene given the object ``bed" and ``lamp" are presented, respectively. The right part shows the joint conditional probability of $P(scene | (object, object))$, and the top 6 scene given the object pair $(bed, lamp)$. As observed from the figure, the proposed BORM suppresses the probability of scene-common object pairs while highlights the probability of the scene-specific object pairs.}
	\label{fig:joint_prob}
\end{figure}




Recently, many deep learning based methods have demonstrated impressive performance over various computer vision tasks. For example, ResNet has been shown a convincing ability in image classification tasks for years \cite{He2016}. However, the classification of the image using ResNet is like using a black box to interpret the image, which is not really in an interpretable manner.  Chen et al. \cite{chen2019scene} consider the word-embedding model to reformulate the scene understanding problem in a more semantic meaningful perspective. The output of ResNet module, object detection module, and scene parsing module have been encoded in the one global vector for scene understanding. However, the performance of the word embedding method is just slightly higher than ResNet, as reported. Besides, there are three streams used for scene recognition, which is relatively inefficient.
The utilization of semantic information for scene recognition is important. To model the descriptors probabilities, a Bayesian filtering method is proposed \cite{wu2009visual}, but the object information is not explicitly used. To use the object information, a object classifier is used to classify low-level visual features to objects \cite{espinace2010indoor}, while the relation among objects is not exploited. To utilize the object relation for scene recognition, spatial object-to-object relation is studied for RGB-D scene recognition \cite{song2019image}. Besides, a Long Short-Term Memory modeling method is proposed to investigate the object relation with ROI selection \cite{laranjeira2019modeling}. To utilize the object information in the scene, the object model \cite{pal2019deduce} is proposed as complementary semantic information of the scene combined with ResNet to better interpret the given scene. However, using only the object vector might not be discriminative enough for scene understanding \cite{zeng2019learning}. A Object-to-Scene model is proposed, where the object features and object relation are learned by object feature aggregation module and object attention module \cite{Miao2021ots}, respectively. Overall, these above-mentioned method lack the statistically analysis of object distribution in the scene. Specifically, there are some scene-common objects across various scene and scene-specific objects only appear in the particular scene. Therefore, we propose the Bayesian object relation model (BORM) for improving the accuracy of scene recognition by highlighting the scene-specific objects while preserving the scene-common objects for scene understanding \cite{cheng2018scene}. However, all of these methods neglect the probabilistic relation among object pairs, which contains an essential message for scene recognition.


In this paper, we consider the probabilistic relation between the object pairs as shown in Fig. \ref{fig:joint_prob}. We can observed that there is a high probability of the specific object pair appear in the particular scene, e.g., the bed and lamp is most likely (75.2\%) appeared in the bedroom, and can be regarded as a scene-specific object pair. 

To utilize this conditional object pair relation w.r.t the various scenes, the BORM that derives the joint conditional probability of the object pairs given the various scenes. With the BORM, the joint conditional probability of the scene-specific object pairs will be enhanced and the joint conditional probability of scene-common object pairs will be suppressed. 


In summary, our main contributions of this paper are as follows:

\begin{itemize}
	\item We utilize an IOM as baseline, based on scene parsing algorithm pretrained on ADE20K dataset that contains more relevant object classes for indoor scene representation. Surprisingly, the IOM surpasses the object model over \textbf{20.5\%} accuracy on average. 
	
	\item Meanwhile, To conduct an in-depth study of object pair relations, we propose a Bayesian object relation model (BORM) that enhances the scene-specific object pair relation and suppresses the scene-common object pair relation in a Bayesian probability manner for indoor scene representation.
	
	\item We combine the proposed BORM and the PlacesCNN model as CBORM, which significantly outperforms the state-of-the-art methods on the Places365-7 and Places365-14 dataset over 2.0\% and 2.1\% accuracy, and SUN RGB-D dataset over 2.0\% without retraining the model, showing the excellent generalization ability of the proposed method.

	
\end{itemize}

The rest of the paper is organized as follows. Section~\ref{sec:related_work} introduces the related work of scene recognition and object knowledge for indoor scene representation. Section~\ref{sec:method} describes IOM that pretrained on ADE20K with 150 categories based on the scene parsing algorithm, and the BORM that conduct an in-depth study of object relation in a Bayesian perspective. In Section~\ref{sec:cBORM}, we discuss the PlacesCNN model for scene recognition. Moreover, we combine the proposed BORM and the PlacesCNN model as the CBORM for scene recognition.  Section~\ref{sec:exp} shows the experimental settings and results, and numerical experiments have been conducted to verify the effectiveness of our proposed method. Finally, the conclusions and future directions are pointed out in Section~\ref{sec:conclusion}.

\begin{figure*}[htb]
	\centering
	\includegraphics[width=0.9\textwidth]{Fig/BORM_one.pdf}
	\caption{BORM that based on the baseline IOM is shown in the diagram, where blue and red arrow shows the flow of the IOM and BORM, respectively.}
	\label{fig:avom}
\end{figure*}

\section{Related Work}
\label{sec:related_work}
In this section, we review the research works related to our paper in two aspects: scene recognition, and object knowledge from object detection or scene parsing algorithm for indoor scene representation. We also discuss the differences and connections between these related works and our method. 



\subsection{Scene Recognition}
Scene recognition has been an important research problem in robotics area for decades, which can be utilized for topological map construction and mobile robot localization \cite{liu2009scene}\cite{Liu2013}\cite{Lin2018}. Moreover, it has the potential to be applied for robot to perform efficient recognition of functional areas \cite{Ye2017}, identification of the person \cite{wang2019learning}, and execute task accordingly. The early methods, mainly focus on extraction of local features like color descriptors \cite{Liu2013}\cite{VanDeSande2009}. 
To better recognize indoor images, the combination of local and global features are utilized \cite{quattoni2009recognizing}. However, these methods only captures lower-level features of the scene while the high-level semantic structures are difficult to capture \cite{Zhu2016}. 

To utilize the high-level semantic information, some methods propose to leverage the mid-level concepts. e.g., Zhou et al. \cite{Zhou2014}\cite{zhou2017places} propose a CNN based classifier to learn deep features for scene recognition on the Places Dataset. Liao et al. \cite{Liao2016} use deep learning with a multi-task training method that incorporate both semantic segmentation and scene recognition tasks. Zhu et al. \cite{Zhu2016} propose a discriminative multi-modal feature fusion framework for scene recognition. However, these methods neglect the critical object information for scene recognition.

To incorporate object information, Li et al. \cite{Li2014} represent images by using objects appearing in them as object bank (OB) method. Brucker et al. \cite{Brucker2018} counts the co-occurrence frequencies as potentials in conditional random field for scene labeling.
In DEDUCE \cite{pal2019deduce}, one hot object vector is used as complementary information for scene recognition, where the object information is separately considered. In context based Word Embeddings \cite{chen2019scene}, there are three streams for scene recognition, one stream is the scene parsing model pretrained on ADE20K, the other is ResNet50 pretrained on reduced Places365 dataset, and a Word Vectors Module computes the content of two modules and refines the results. Song et al. \cite{Song2017} considers spatial object-to-object relations with the intermediate (object) representations. In Semantic-aware method \cite{Lopez-Cifuentes2020}, the image representation and context information are combined, where context information consists of scene objects and stuff, and their relative locations. However, all of these methods have not considered the probabilistic relation of the object pairs given the various scene.
In this work, we consider the object pair co-occurrences given various scene in a Bayesian Perspective with proposed Bayesian object relation model (BORM). To the best of our knowledge, in the scene recognition task, the object relation modeled in a probabilistic perspective is less exploited.  

\subsection{Object Knowledge for Indoor Scene Representation}
Recently, object detection is a popular research area in computer vision, thanks to the emerging of the large-scale labeled data and advanced GPUs. Many excellent algorithms have been developed for object detection like the one-stage object detector, YoloV3 \cite{redmon2018yolov3}, SSD \cite{liu2016ssd} and two-stage object detector, like Joint SSD \cite{yi2018long}, Faster R-CNN \cite{Ren2015}, Mask RCNN \cite{he2017mask}, Cascaded RCNN \cite{cai2018cascade}. The one-stage detector has a higher speed while maintain the similar performance compared with two-stage detector, therefore, the YoloV3 is adopted as the part of object model in DEDUCE \cite{pal2019deduce}. To incorporate the object knowledge for scene understanding, the object detection algorithm is first pretrained on the public available dataset. There are several mainstream dataset for object detection. PASCAL VOC \cite{Everingham2015} contains 20 categories of objects, which is very limited, and most labels are like car, bus, bicycle, airplane, and all of these are not relevant to indoor scene representation. To better represent indoor scene, DEDUCE use the YoloV3 pretrained on the MS COCO dataset \cite{lin2014microsoft}, which contains 80 object categories. However, the half of objects in MS COCO are outdoor objects like giraffe, elephant, which are not relevant to indoor scene representations. 

Scene parsing algorithm is used to segment objects and stuff in the still image, which has demonstrated surprisingly performance recently \cite{Zhou2017}. Zhou et al. \cite{Zhou2019} propose a scene parsing algorithm to detect a wide range of object and stuff in the pixel level with ADE20K Dataset, which contains 150 classes of object knowledge in pixel level. To utilize the rich object knowledge of the ADE20K Dataset, we adapt the scene parsing model pretrained on the ADE20K dataset as the improved object model (IOM), which shows a significantly improvements over the OM pretrained on the MS COCO dataset. 



\section{Bayesian Object Relation Model}
\label{sec:method}

In Section \ref{sec:method}, we present the baseline model IOM that uses more object categories for object representation and the BORM that models the object pair relation, as shown in Fig.~\ref{fig:avom}. Since the object model proposed in Deduce \cite{pal2019deduce} only contains information about few categories of objects pretrained on the MS COCO dataset, which is very limited for indoor scene recognition because most of the object categories are not relevant to indoor scene representation. e.g.,  the object categories of elephant and giraffe from the super-category of animal, airplane and bus from the super-category of vehicle, traffic light and stop sign from the super-category of outdoor, and so on. In total, there are half of the objects can be categorized as outdoor objects. We believe that if the object model possesses the more rich and relevant object knowledge about the scene, the better performance of scene recognition can be obtained. Therefore, we utilize the IOM pretrained on the ADE20K dataset with rich and relevant object categories as a baseline model for indoor scene representation. To consider the probabilistic relation of object pairs, we assume some object pairs are scene-specific, and some object pairs are scene-common. To this end, we propose a novel BORM that highlights the scene-specific object pairs while suppresses the scene-common object pairs from a Bayesian probabilistic perspective.



%\subsection{Object Model (OM)}
%
%In Deduce \cite{pal2019deduce}, the YoloV3 is adopted as the object detector to infer the objects because of its real-time performance. The YoloV3 \cite{redmon2018yolov3} is pretrained on the MS COCO dataset \cite{lin2014microsoft} to detect 80 objects. The output of YoloV3 is converted to an object vector $\mathbf{X_{om-80}}$ with 80 dimensions, where element 1 represents the detected objects, and element 0 means the objects not appear in the image. Different from Deduce \cite{pal2019deduce}, where a codebook of most common COCO-objects for seven different scene classes is created for scene recognition. We feed the object vector $\mathbf{X_{om-80}}$ to a two-layer fully connected network for classification with the size of 32 and the number of scenes, respectively. The major problem of the object model is the use of MS-COCO that contains half categories that irrelevant to indoor scene recognition like the bus and train from vehicle, traffic light and stop sign from outdoor, elephant and giraffe from animal super-category.

\subsection{Improved Object Model (IOM)}

Different from the basic OM that only have 80 objects information of environment based on the YoloV3 pretrained on the MS COCO dataset, we present an IOM based on the scene parsing algorithm pretrained on ADE20K \cite{Zhou2017} dataset that convert the output the scene parsing algorithm to an object vector $\mathbf{X_{iom-150}}$ with 150 dimensions where the 1 means the detected objects, while 0 means the objects are not in the given image. Compared with the OM, we now have a new scene representation of 150 dimension $\mathbf{X_{iom-150}}$. The new scene representation $\mathbf{X_{iom-150}}$ will  be fed to a two-layer fully connected network for classification with the size of 32 and the number of scenes, respectively.


\begin{figure}[t]
	\centering
	\subfigure{
		\begin{minipage}{0.35\textwidth}
			\centering
			\includegraphics[width=3.6cm,height=3.6cm]{Fig//discriminative.jpg}
			\label{exp:mnist:1}
	\end{minipage}}
	\subfigure{
		\begin{minipage}{0.35\textwidth}
			\centering
			\includegraphics[width=3.6cm,height=3.6cm]{Fig//nondiscriminative.jpg}
			\label{exp:mnist:2}
	\end{minipage}}
	\caption{The BORM contains the discriminative value of object pairs. We have conducted statistical analysis on the Places365-7 dataset. The top figure shows the $p(scene|(bed, curtain))$, the probability distribution of object pair (bed, lamp) over the seven indoor scenes, and the standard deviation is 0.30, which means the $(bed, curtain)$ is a scene-specific object pair for indoor scene recognition. The bottom figure shows the $p(scene|(wall, floor))$, the probability distribution of object pair $(wall, floor)$ among seven indoor scenes,  which are almost the same. The standard deviation is 0.01, which indicates the $(wall, floor)$ is a scene-common object pair for indoor scene recognition, because this object pair appears in every scene equally.}
	\label{fig:object_pair}
\end{figure}



\subsection{Bayesian Co-occurrence Probability Analysis}
\label{sec:BORM}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.48\textwidth]{Fig//heatmap.jpg}
	\caption{The Bayesian object relation matrix of first 20 objects. e.g., the  $(bed,curtain)$ is the scene-specific object pair, in which will mostly leading to the bedroom, while $(wall,floor)$ forms the scene-common object pair because it is appear in everywhere with almost same probability.}
	\label{fig:BORM}
\end{figure}

Instead of using one hot object vector as the feature representations for the scene, which lacks the probabilistic relation between different object pairs, we propose a novel BORM method that measures the probabilistic relation of object pairs in a Bayesian perspective.
The scene recognition problem is shaped by the fact that a few object pairs are scene-common, but most object pairs are scene-specific. As observed in Fig.~\ref{fig:object_pair}, the probability of scene-specific object pair $(bed, curtain)$ and scene-common object pair $(wall, floor)$ are presented.

The scene-specific object pairs indicate the object pairs that have a high probability at a particular scene and have low probability at others. e.g., the object pairs like $(bed,curtain)$ tend to be scene-specific, which means their probability $p(scene|(bed, curtain))$ can be quite distinctive among the various scene categories and has the highest probability that appears in the bedroom. Therefore, they have a large standard deviation (0.31). In contrast, scene-common object pairs distributed at various scenes with a similar probability. e.g., the common object pairs like $(wall,floor)$ frequently appear in many different scenes, which means their joint conditional probability $p(scene|(wall, floor))$ is quite similar across various scenes, i.e., they have an extremely small standard deviation (0.01) compared with those scene-specific object pairs. 


\begin{figure*}[htbp]
	\centering
	\includegraphics[width=18cm]{Fig/Combined_Model.pdf}
	\caption{The proposed combined Bayesian object relation model (CBORM) contains two streams. The first stream with the red arrow is the proposed BORM that utilizes the scene parsing algorithm for scene segmentation. The second stream with the black arrow is the PlacesCNN model for feature extraction with the ResNet backbone network. The output of the BORM will first be converted to a discriminative object feature. Meanwhile, the results of the BORM and feature extraction result of PlacesCNN will be concatenated as one combined scene feature and fed to the two-layer FC network for scene classification.}
	\label{fig:combined_model}
\end{figure*}

We adopt a pipeline to estimate the posterior probability $P(c_j |o_h, o_i)$. First, we use the Places365-7 and Places365-14 dataset for learning the BORM statistically with only the training set, while testing on the SUN RGB-D dataset, where the probability distribution is regarded similar to the Places365-7 dataset. 

Given a set of images $I_{c_j}$ from a scene category $c_j$, the conditional probability of object $o_i$ appears on the scene $c_j$ is:
\begin{equation}
\begin{split}
P\left(o_{i} | c_{j}\right) = N_{o_i} / N_{I_{c_j}} \\
i \in [1,N_{objs}], j \in [1,N_{scenes}]
\end{split}
\end{equation}

where $N_{o_i}$ is the total number of i-th object $o_i$ appears in $I_{c_j}$ and $N_{I_{c_j}}$ is total number of images of $I_{c_j}$. Meanwhile, where $N_{objs}$, and $N_{scenes}$ represent the number of objects in the pretrained model and number of scene categories we have in the dataset respectively.
Assume the statistical independence of each object, we obtain the joint conditional probability $P\left(o_{h}, o_{i} | c_{j}\right)$ of $o_h$ and $o_i$ appear in the scene $c_j$:

\begin{equation}
\begin{split}
P\left(o_{h}, o_{i} | c_{j}\right) = P\left(o_{h} | c_{j}\right) P\left(o_{i} | c_{j}\right)\label{joint probability} \\
h, i \in [1,N_{objs}], j \in [1,N_{scenes}]
\end{split}
\end{equation}

The $P(c_{j}|o_{h},o_{i})$, the posterior probability of scene class $c_{j}$ given an object pair ($o_{h},o_{i}$), can be derived by the Bayes Rule and Law of Total Probability. 
\begin{equation}
\begin{split}
P\left(c_{j} | o_{h},o_{i}\right)& =\frac{P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)}{P(o_{h},o_{i})} \\
&=\frac{P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)}{\sum_{j} P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)} \\
\end{split}
\end{equation}

where the $P(c_j)$ is the probability of scene $c_j$ in the dataset, and $\sum_{j=1}^{N_{scenes}}P(c_j)=1$. We construct the posterior probability matrix $P(c_j|o,o)$ by calculating relation of each object pairs:

\begin{equation}
%\begin{split}
\scriptsize
P\left(c_{j} | o_{}, o_{}\right) =
\begin{bmatrix}
P\left( c_{j} | o_{1}, o_{1}\right) & \cdots & P\left( c_{j} | o_{1},o_{k}\right) & \cdots & P\left(c_{j} | o_{1}, o_{n}\right) \\
\vdots &  & \vdots &  & \vdots \\
P\left(c_{j}|o_{j}, o_{1}\right) & \cdots & P\left(c_{j} | o_{j},o_{k}\right) & \cdots & P\left(c_{j} | o_{j},o_{n}\right) \\
\vdots &  &  &  & \vdots \\
P\left(c_{j} | o_{n}, o_{1}\right) & \cdots & P\left(c_{j} | o_{n},o_{k}\right) & \cdots  & P\left(c_{j} | o_{n}, o_{n}\right)
\end{bmatrix}
%\end{split}
\end{equation}

%\begin{equation}
%P(o_{h},o_{i})=\sum_{j} P\left(o_{h},o_{i} \cap c_{j}\right)=\sum_{j} P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)
%\end{equation}

%\begin{equation}
%P\left(c_{j} | o_{h},o_{i}\right)=\frac{P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)}{\sum_{j} P\left(o_{h},o_{i} | c_{j}\right) P\left(c_{j}\right)}
%\end{equation}



First, the posterior probability $P(c_j|o_h, o_i)$ is calculated. To calculate the discriminative value of the object pairs, the standard deviation is applied to posterior probabilities among scene categories, denoted as $std(P(c_j|o_h, o_i))$. The discriminative value of the posterior probabilities among scene categories is defined as $dis(o_h, o_i)$:

%The BORM is chosen by replacing the object pair matrix with the their corresponding discriminative index $dis(o_h, o_i)$ derived from Bayesian analysis and standard deviation function. 

\begin{equation}
\operatorname{dis}\left(o_{h}, o_{i} \right)= std _{c \in 1, \ldots, N_{scenes}}(P(c_j|o_h, o_i))
\end{equation}

The $dis(o_h, o_i)$ is the discriminative value for measuring the discriminalibility of the object pair to the scene categories. Instead of using object vector as the feature representations for the scene, which dismiss the relationship between different objects, we first construct an object relation matrix $\mathbf{X_{orm}}=\mathbf{X_{iom}} \mathbf{X_{iom}^\mathrm{T}}$ for representing the given scene. The value of $\mathbf{X_{orm}}$ will be replaced by the discriminative value $dis(o_h, o_i)$ at the same position. Therefore, the Bayesian object relation matrix is obtained by element-wise matrix multiplication, $\mathbf{X_{borm}}=\mathbf{X_{orm}}*\mathbf{dis(o_h, o_i)}$, with the size of 150x150. After that, the matrix will be stretched to a one dimensional vector with the size of 22500x1, and will be fed into the three-layer fully connected network with the size of 8192, 2048, and number of scene classes. 



As shown in Fig. \ref{fig:BORM}, the discriminative matrix of the first 20 object pairs are displayed. The $(bed,curtain)$ is a scene-specific object pair that mostly appear in the bedroom, and they form a scene-specific object pair with a discriminative value of 0.30. The $(wall, floor)$ is a common object pair appear everywhere and have an extremely small discriminative value of 0.01, which means they form a scene-common object pair. 

\section{CBORM Model}
\label{sec:cBORM}


\subsection{PlacesCNN Model}
In order to obtain the scene representation, we use the PlacesCNN model \cite{zhou2017places} with the base architecture ResNet \cite{He2016} as a backbone network, which is pretrained on the ImageNet \cite{deng2009imagenet}  dataset. There are two versions of ResNet according to the settings in DEDUCE \cite{pal2019deduce} and Word2Vec \cite{chen2019scene}. ResNet18 is used for Places365-7 and SUN RGB-D dataset, while ResNet50 is used for Places365-14 dataset. Specifically, we preserve the output of ResNet18 with 512 dimensions or ResNet50 with 2048 dimensions, namely $F_{scene}$, the feature of the PlacesCNN model for the scene representation.

\subsection{Combined Model}
The Bayesian object relation matrix of the BORM is with the size of 150x150, which is first stretched to a vector of size 22500x1. The vector of BORM will be fed into the two fully connected layers and an discriminative object feature  $F_{BORM}$ with 512 dimensions (for Places365-7) or 2048 (For Places365-14) will be the feature representation of BORM.

Meanwhile, we combine the PlacesCNN model of the ResNet backbone with BORM, as the combined Bayesian object relation model (CBORM).

\subsection{Combined Classifier}
For the Places365-7 dataset, there are two streams. To fair compare with the object model \cite{pal2019deduce}, one stream is based on ResNet18 pretrained on the Places365 and finetuned on the Places365-7 dataset. The other stream is BORM. The feature representations of ResNet18 and BORM will be concatenated and fed into a two-layer FC network with a dimension of 512, and 7 respectively. 

Similarly, for the Places365-14 dataset, there are two streams. To fair compare with the word-embedding \cite{chen2019scene} model, while one stream is ResNet50, and the other is BORM.
The feature representations of ResNet50 and BORM will be fed into a two-layer FC network with a dimension of 512, and 14 respectively.  


\section{Experimental Results}
\label{sec:exp}

\input{Experiment.tex}

\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we aim to transfer object knowledge from humans to indoor scene recognition. First, we propose the IOM that with rich and relevant object categories for indoor scene representation. Besides, inspired by the nature of scene-common and scene-specific object pairs, we establish a BORM that obtains the probabilistic relations among object pairs given various scene, where the probability of scene-specific object pairs will be enhanced and the probability of scene-common object pairs will be suppressed.  
Moreover, we utilize the PlacesCNN model with ResNet as a backbone network for classification, which demonstrates a substantial result on the scene understanding. Hence, we combine the PlacesCNN and proposed BORM as CBORM for a more interpretable scene recognition algorithm, and experiment results show our proposed method significantly outperforms the state-of-the-art methods. 

In the future, we plan to integrate our algorithm to real robots like mobile robots, and flying robots for the construction of the semantic map includes the label of the scene and detailed semantic information of the environment. The construction of a semantic map using the proposed scene recognition algorithm would be useful for navigation in unknown places for robots and humans because both the semantic label of the region and semantic meaning of the environment are provided. Furthermore, our system could be applied to autonomous robots and enabling them to assist humans in safety and rescue missions inside a house or a building.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}

%\bibliographystyle{plain}
\bibliography{Ref}



\end{document}
