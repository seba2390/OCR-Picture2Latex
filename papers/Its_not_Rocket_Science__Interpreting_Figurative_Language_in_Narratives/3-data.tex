\input{figures/example_idioms_similes}

We build datasets aimed at testing the understanding of figurative language in narratives, focusing on idioms (Section~\ref{sec:dataidioms}) and similes (Section~\ref{sec:datasimile}). We posit that a model which truly understands the meaning of a figurative expression, like humans do, should be able to infer or decide what happens next in the context of a narrative. Thus, we construct a dataset in the form of the story-cloze test.

\subsection{Idioms}
\label{sec:dataidioms}

We compile a list of idioms, automatically find narratives containing these idioms, and then elicit plausible and implausible continuations from crowdsourcing workers, as follows.

\paragraph{Collecting Idioms.} We compile a list of 554 English idioms along with their definitions from online idiom lexicons.\footnote{\href{http://www.theidioms.com/}{www.theidioms.com},~\href{http://idioms.thefreedictionary.com/}{idioms.thefreedictionary.com}} Table~\ref{tab:example_expressions} presents a sample of the collected idioms.


\paragraph{Collecting Narratives.} We use the Toronto book corpus \cite{zhu2015aligning}, a collection of 11,038 indie ebooks extracted from \url{smashwords.com}. We extract sentences from the corpus containing an idiom from our list, and prepend the 4 preceding sentences to create a narrative. We manually discarded paragraphs that did not form a coherent narrative. We extracted 1,455 narratives with an average length of 80 words, spanning 554 distinct idioms.


\paragraph{Collecting Continuations.} We collected plausible and implausible continuations to the narrative. We used Amazon Mechanical Turk to recruit 117 workers. We provided these workers with the narrative along with the idiom definition, and instructed them to write plausible and implausible continuations that are pertinent to the context, depend on the correct interpretation of the idiom, but which don't explicitly give away the meaning of the idiom. We collected continuations from 3 to 4 workers for each narrative. The average plausible continuation contained 12 words, while the implausible continuations contained 11 words.

To ensure the quality of annotations, we required that workers have an acceptance rate of at least 99\% for 10,000 prior HITs, and pass a qualification test. We then manually inspected the annotations to identify workers that performed poorly in the initial batches, disqualified them from further working on the task, and discarded their annotations. 


Our automatic approach for collecting narratives doesn't account for expressions that may be used figuratively in some contexts but literally in others. For example, the idiom ``run a mile'', i.e. avoiding something in any way possible, may also be used literally to denote running a distance of one mile. To avoid including literal usages, we instructed the workers to flag such examples, which we discard from the dataset. We further manually verified all the collected data. Overall we removed 12 such narratives. 

The final idiom dataset contains 5,101 $\lt$narrative, continuation$\gt$ tuples, exemplified in the top part of Figure~\ref{fig:idiomdata}. We split the examples to train (3,204), validation (355) and test (1,542) sets. To test models' ability to generalize to unseen idioms, we split the data such that there are no overlaps in idioms between train and test.

\subsection{Similes}
\label{sec:datasimile}

A simile is a figure of speech that usually consists of a topic and a vehicle (typically noun phrases) which are compared along a certain property using comparators such as ``like'' or ``as'' \cite{hanks2013lexical,niculae-danescu-niculescu-mizil-2014-brighter}. The property may be mentioned (\emph{explicit} simile) or hidden and left for the reader to infer (\textit{implicit} simile). We focus on implicit similes, that are less trivial to interpret than their explicit counterparts \cite{qadir-etal-2016-automatically}, and test a model's ability to recover the implicit property.

\paragraph{Collecting Similes.} Since there are no reliable methods for automatically detecting implicit similes, we first identify explicit similes based on syntactic cues, and then deterministically convert them to implicit similes. We look for sentences in the Toronto Book corpus containing one of the syntactic structures ``as ADJ/ADV as'' or ``ADJ/ADV like'' as a heuristic for identifying explicit similes. We additionally add the constraint of the vehicle being a noun phrase to avoid examples like ``I worked as hard as him''. We remove the adjectival property to convert the simile to implicit, as demonstrated below:

\input{figures/explicit_implicit_simile}

We collected 520 similes along with their associated property. We asked workers to flag any expression that was not a simile, and manually verified all the collected data. Table~\ref{tab:example_expressions} presents a sample of the collected similes. Many of the similes are original, such as ``like a street-bought Rolex'' which implies that the subject is fake or cheap.

\paragraph{Collecting Narratives.} Once we identified the explicit simile and converted it to its implicit form, we similarly prepend the 4 previous sentences to form narratives. The average length of the narrative was 80 words.

\paragraph{Collecting Continuations.} We repeat the same crowdsourcing setup as for idioms, providing the explicit simile property as the definition. Each narrative was annotated by 10 workers. The average length of continuations was identical to the idiom dataset (12 for plausible and 11 for implausible).

The simile dataset contains 4,996 $\lt$narrative, continuation$\gt$ tuples, exemplified in the bottom part of Figure~\ref{fig:idiomdata}. We split the examples to train (3,100), validation (376) and test (1,520) sets with no simile overlaps between the different sets.