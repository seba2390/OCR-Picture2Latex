\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/data_examples.pdf}
    \caption{Example narratives from our datasets, containing an idiom (top) or a simile (bottom), along with human-written plausible and implausible continuations.}
    \label{fig:idiomdata}
\end{figure*}

Figurative language is a medium for making language expressive, communicating abstract ideas otherwise difficult to visualize, and provoking emotions \cite{roberts1994people,fussell1998figurative}. Despite the ubiquity of figurative language across various forms of speech and writing, the vast majority of NLP research focuses primarily on literal language. Figurative language is often more challenging due to its implicit nature and is seen as ``a  bottleneck in automatic text understanding'' \cite{shutova2011computational}. 

In recent years, transformer-based language models (LMs) achieved substantial performance gains across various NLP tasks, however, they still struggle with figurative language. In particular, one of the challenges is that figurative expressions are often non-compositional, i.e. the phrase meaning deviates from the literal meanings of its constituents. For instance, the idiom ``chicken feed'' in Figure~\ref{fig:idiomdata} denotes ``a ridiculously small sum of money'' instead of ``food for poultry''. By design, transformer-based LMs compute a word representation as a function of the representation of its context. LM-based phrase representations encode the meanings of the constituent words but hardly capture any meaning that is introduced by the composition itself \cite{yu-ettinger-2020-assessing}. Even though LMs may recognize when a word is used non-literally, and potentially attend to it less, they still struggle to represent the implied, non-literal meaning of such phrases \cite{shwartz-dagan-2019-still}. 

While LMs potentially memorize familiar idioms, we can expect them to further struggle with similes, which are often created ad hoc \cite{carston_wearing_2011}. For example, in Figure~\ref{fig:idiomdata}, the person is compared to ``a high mountain lake without a wind stirring it'' to imply calmness. Many such figurative expressions compose in a non-trivial way, and introduce implicit meaning that requires multiple reasoning steps to interpret. 
 


In this paper we work on interpreting idioms and similes in narratives, where they are especially abundant. Existing work on narrative understanding focuses on literal stories, testing models on their ability to answer questions about a narrative \cite{kocisky-etal-2018-narrativeqa} or continue an incomplete narrative \cite[Story Cloze Test;][]{mostafazadeh-etal-2016-corpus}. We follow the latter setup. We extracted short narratives from the Toronto book corpus \cite{zhu2015aligning}, each containing a figurative expression, and crowdsourced plausible and implausible continuations that rely on correct interpretation of the figurative expression. We defined two tasks: a discriminative setting, where the goal is to choose the plausible continuation among two candidates, and a generative setting, where the goal is to generate a plausible continuation that is coherent with the narrative and complies with the meaning of the figurative expression. 

We report the performance of an extensive number of state-of-the-art LMs on both tasks, in zero-shot, few-shot and supervised settings. Our results show that pre-trained LMs including GPT-3 \cite{brown2020language} perform poorly in the zero-shot and few-shot settings. While the supervised model's performance is closer to humans, the gap is still substantial: in the discriminative tasks, the gap from human performance was 10 and 14.6 points in accuracy for idioms and similes, respectively. In the generative tasks, there was a striking 24 and 28 points difference in human evaluation of the plausibility of generated continuations. 

To further close this gap, we developed knowledge-enhanced models inspired by two human strategies for interpreting unknown idioms, as studied by \newcite{10.2307/3587719} and discussed in \newcite{shwartz-dagan-2019-still}. The first strategy is to infer the expression's meaning from its \emph{context}, for which we incorporate event-centered inferences from ParaCOMET \cite{gabriel-etal-2021-discourse}. The second relies on the \emph{literal} meanings of the constituent words, using concept-centered knowledge from COMET-ConceptNET \cite{Hwang2021COMETATOMIC2O}. Additionally similes are often interpreted by humans using the \emph{literal} property of the vehicle or object of comparison and thus we use concept-centered knowledge here as well. The knowledge-enhanced models consistently outperformed other models on both datasets and settings, with a substantial gap on the generative tasks. 

Furthermore, different strategies were favored for each case: the generative context model performed well on idioms, in line with \citeauthor{10.2307/3587719}'s findings, while the literal model was favored for similes, which are by design based on a constituent's literal attribute (e.g. calm lake). The knowledge-enhanced models leave room for improvement on our dataset. We hope that future work will use additional techniques inspired by the properties of figurative language and human processing of it. Our code and data and code available is available at \url{https://github.com/tuhinjubcse/FigurativeNarrativeBenchmark} 
and our leaderboard is available at \url{https://leaderboard.allenai.org/idiom-simile/}.


% are available at: \footnote{\tiny\url{https://github.com/tuhinjubcse/FigurativeNarrativeBenchmark}}

% {\tiny \url{https://github.com/tuhinjubcse/FigurativeNarrativeBenchmark}.}}
