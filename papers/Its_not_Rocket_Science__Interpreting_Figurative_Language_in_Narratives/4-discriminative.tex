\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/knowledge_extraction.pdf} 
    \caption{Extracting inferences from COMET regarding the context (previous sentences in the narrative) and the literal meaning of the content words among the idiom constituents.}
        % \vspace{-2ex}
    \label{fig:knowledge_extraction}
\end{figure*}


The first task we derive from our dataset is of discriminative nature in the setup of the story cloze task. Given a narrative N and two candidate continuations $\{\text{C}_1, \text{C}_2\}$, the goal is to choose which of the continuations is more plausible. 

\subsection{Methods}
\label{sec:disc:methods}

For both idioms and similes, we report the performance of several zero-shot, few-shot and supervised methods as outlined below. Most of our experiments were implemented using the transformers package \cite{wolf-etal-2020-transformers}. 


\paragraph{Zero-shot.} The first type of zero-shot models is based on standard language model score as a proxy for plausibility. We use GPT-2 XL \cite{Radford2019LanguageMA} and GPT-3 \cite{brown2020language} to compute the normalized log-likelihood score of each continuation given the narrative, predicting the continuation with the highest probability: $ \operatorname{argmax}_i P_{LM}(\text{C}_i |\text{N})$. 




We also use UnifiedQA \cite{khashabi-etal-2020-unifiedqa}, a T5-3B model \cite{raffel2020exploring} trained on 20 QA datasets in diverse formats. We don't fine-tune it on our dataset, but instead use it in a zero-shot manner, with the assumption that the model's familiarity with QA format and with the narrative domain through training on the NarrativeQA dataset \cite{kocisky-etal-2018-narrativeqa} would be useful. To cast our task as a QA problem we format the input such that the question is ``Which is more plausible between the two based on the context?''. 


\paragraph{Few-shot.} Language models like GPT-3 have shown impressive performance after being prompted with a small number of labelled examples. A prompting example in which the correct continuation is the first is given in the following format: \texttt{Q: N (1) C$_1$ (2) C$_2$ A: (1)}.


We provided the model with as many prompting examples as possible within the GPT-3 API limit of 2,048 tokens, which is 6 examples. The test examples are provided without the answer and the model is expected to generate \texttt{(1)} or \texttt{(2)}. 

We also use the recently proposed Pattern Exploiting Training model \cite[PET;][]{schick-schutze-2021-just}. PET reformulates the tasks as a cloze question and fine-tunes smaller masked LMs to solve it using a few training examples.\footnote{Specifically, it uses ALBERT XXL V2 \cite{lan2019albert}, which is 784 times smaller than GPT-3.} We use the following input pattern: ``\texttt{N. C$_1$. You are \textunderscore}'' for idioms and ``\texttt{N. C$_1$. That was \textunderscore}'' for similes. PET predicts the masked token and maps it to the label inventory using the verbalizer \{``right'', ``wrong''\} for idioms and \{``expected'', ``unexpected''\} for similes respectively mapping  them to \{\textsc{true}, \textsc{false}\}.\footnote{ We also experimented with the pattern and verbalizer used by \newcite{schick-schutze-2021-just} for MultiRC \cite{khashabi-etal-2018-looking}, with the pattern: ``\texttt{N. Question: Based on the previous passage is C$_1$ a plausible next sentence? \textunderscore}.'' and the verbalizer \{``yes'', ``no''\}, but it performed worse.} We provide each model 100 training examples, train it for 3 epochs, and select the model that yields the best validation accuracy.

\paragraph{Supervised.} We fine-tune RoBERTa-large \cite{liu2019roberta} as a multiple-choice model. For a given instance, we feed each combination of the narrative and a continuation separately to the model in the following format: \texttt{N $\lt\text{s/}\gt\text{C}_i$}. 

We pool the representation of the start token to get a single vector representing each continuation, and feed it into a classifier that predicts the continuation score. The model predicts the continuation with the higher score. We fine-tune the model for 10 epochs with a learning rate of \num{1e-5}  and a batch size of 8, and save the best checkpoint based on validation accuracy.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/knowledge_integration_disc.pdf}
    \vspace{-4ex}
    \caption{Integrating commonsense inferences into a RoBERTa-based discriminative model.}
    % \vspace{-2ex}
    \label{fig:architecture_discriminative}
\end{figure*}


\paragraph{Knowledge-Enhanced.} Inspired by how humans process figurative language, we develop RoBERTa-based models enhanced with commonsense knowledge. We develop two models: the first model obtains additional knowledge to better understand the narrative (\emph{context}), while the second seeks knowledge pertaining to the \emph{literal} meaning of the constituents of the figurative expression (Section~\ref{sec:processing}). In both cases, in addition to the narrative and candidate continuations, the model is also provided with a set of inferences: \{Inf$_1$, ..., Inf$_n$\} that follow from the narrative, as detailed below and demonstrated in Figure~\ref{fig:knowledge_extraction}.

The literal model uses the COMET model \cite{Hwang2021COMETATOMIC2O}, a BART-based language model trained to complete incomplete tuples from ConceptNet. As opposed to extracting knowledge from ConceptNet directly, COMET can generate inferences on demand for any textual input. For an idiom, we retrieve knowledge pertaining to the content words among its constituents, focusing on the following relations: \texttt{UsedFor}, \texttt{Desires}, \texttt{HasProperty}, \texttt{MadeUpOf}, \texttt{AtLocation}, and \texttt{CapableOf}. For each content word, we extract the top 2 inferences for each relation using beam search. For example, given the idiom ``run the gauntlet'', we obtain inferences for ``run'' and ``gauntlet''. We convert the inferences to natural language format based on the templates in \citet{guan2019story}. Given the nature of the simile task, we focused solely on the vehicle's \texttt{HasProperty} relation and obtain the top 12 inferences. For example, given the simile ``like a psychic whirlpool'', we obtain inferences for the phrase ``psychic whirlpool''.

The context model is enhanced with knowledge from ParaCOMET \cite{Gabriel2021ParagraphLevelCT}, trained on ATOMIC. We feed into ParaCOMET all but the last sentence from the narrative, excluding the sentence containing the figurative expression. We generate inferences along ATOMIC dimensions pertaining to the narrator (\texttt{PersonX}), namely: \texttt{xIntent}, \texttt{xNeed}, \texttt{xAttr}, \texttt{xWant}, \texttt{xEffect}, and  \texttt{xReact}. Again, we extract the top 2 inferences for every relation using beam search.

In both models, as demonstrated in Figure~\ref{fig:architecture_discriminative}, the input format $X_{i,j}$ for continuation C$_i$ and inference Inf$_j$ is: \texttt{Inf$_j \lt\text{s/}\gt$ N <s/> C$_i$}.

We compute the score of each of these statements separately, and sum the scores across inferences to get a continuation score:
\begin{equation*}
s_{i} = \sum_{j=1}^{12} s_{i,j} = \sum_{j=1}^{12} \operatorname{scorer}(\operatorname{RoBERTa}(X_{i,j}))
\end{equation*}

\noindent where $\operatorname{scorer}$ is a dropout layer with dropout probability of 0.1 followed by a linear classifier. Finally, the model predicts the continuations with the higher score. We fine-tune the context and literal models for 10 epochs with a learning rate of \num{1e-5} and an effective batch size of 16 for idioms and 64 for similes, and save the best checkpoint based on validation accuracy. 


\subsection{Results}
\label{sec:disc:results}

Table~\ref{tab:discriminative_results} shows the performance of all models on the discriminative tasks. For both similes and idioms, supervised models perform substantially better than few-shot and zero-shot models, but still leave a gap of several points of accuracy behind human performance. Human performance is the average accuracy of two native English speakers on the task. We did not provide them with the idiom definition, and we assume they were familiar with the more common idioms. The models performed somewhat better on idioms than on similes, possibly due to the LMs' familiarity with some common idioms as opposed to the novel similes.

Among the zero-shot models, GPT-2 performed worse than GPT-3 and UnifiedQA, each of which performed best on one of the tasks. In particular, UnifiedQA performed well on idioms, likely thanks to its familiarity with the QA format and with the narrative domain.  

In the idiom task, PET outperformed few-shot GPT-3 by a large margin of 12 points in accuracy for idioms and 3.5 points for simile, which we conjecture is attributed to the different number of training examples: 6 for GPT-3 vs. 100 for PET. The small number of examples used to prompt GPT-3 is a result of the API limit on the number of tokens (2,048) as well as the setup in which all prompting examples are concatenated as a single input. 

Overall, few-shot models performed worse than zero-shot models on both datasets. We conjecture that this is due to two advantages of the zero-shot models. First, the GPT-2 and GPT-3 models performed better than the majority baseline thanks to the similarity between the task (determining which continuation is more plausible) and the language model objective (guessing the next word). Second, the UnifiedQA model performed particularly well thanks to its relevant training. At the same time, both few-shot models had to learn a new task from just a few examples.

\input{figures/discriminative_results}

The supervised models leave some room for improvement, and the knowledge-enhanced models narrow the gap for idioms. For similes we see a minor drop in the context model and nearly comparable performance for the literal model. 

\paragraph{Annotation Artifacts.} Human-elicited texts often contain stylistic attributes (e.g. sentiment, lexical choice) that make it easy for models to distinguish correct from incorrect answers without solving the actual task \cite{schwartz-etal-2017-effect,cai-etal-2017-pay,gururangan-etal-2018-annotation,poliak-etal-2018-hypothesis}. Following previous work, we trained a continuation-only baseline, which is a RoBERTa-based supervised model that was trained only on the candidate continuations without the narrative. The results in Table~\ref{tab:discriminative_results} (\texttt{-narrative}) show that the performance is above majority baseline, indicating the existence of \emph{some} bias. However, the performance of this baseline is still substantially worse than the supervised baseline that has access to the full input, with a gap of 17 points for idioms and 12 points for similes, indicating that this bias alone is not enough for solving the task. 


\subsection{Analysis}
\label{sec:disc:analysis}
 
The knowledge-enhanced models provide various types of inferences corresponding to different relations in ConceptNet and ATOMIC. We are interested in understanding the source of improvements from the knowledge-enhanced models over the supervised baseline, by identifying the relations that were more helpful than others. To that end, we analyze the test examples that were incorrectly predicted by the supervised baseline but correctly predicted by each of the knowledge-enhanced models. We split the examples such that every example consists of a single inference, and feed the following input into the model to predict the plausible continuation: \texttt{Inf <s/> N <s/> C}. We focus on the idiom dataset, since for the literal model for similes the only used relation was \texttt{HasProperty} and the context model performed slightly worse than the baseline. 

\input{figures/disc_analysis}

Table~\ref{tab:disc_analysis} shows the percents of successful test set predictions for each relation type. The relations in the context model perform similarly, with the best relation \texttt{xReact} performing as well as all of the relations (Table~\ref{tab:discriminative_results}). In the literal model, it seems that the combination of all relations is beneficial, whereas the best relation, \texttt{CapableOf}, performs slightly worse than the full model. For a narrative snippet ``Since Dominic isn't \textit{up for grabs} anymore, I figure that I will concentrate on something else, Carmen declares'', the inference ``grabs is capable of hold on to'' was compliant with the meaning of ``up for grabs'' (available or obtainable), and led to the correct prediction of the plausible continuation ``The good news is that there are many other available bachelors out there''. Conversely, the inference corresponding to the \texttt{Desires} relation was ``grab desires making money'' which was irrelevant and led to an incorrect prediction. 



