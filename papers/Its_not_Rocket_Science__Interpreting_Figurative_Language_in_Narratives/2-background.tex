\subsection{Idioms}
\label{sec:bg_idioms}

Idioms are figurative expressions with a non-literal meaning. For instance, ``break a leg'' is a good luck greeting before a performance and shouldn't be taken literally as wishing someone to injure themselves. Idioms are typically non-compositional, i.e. the meaning of an idiom is not derived from the meanings of its constituents, and fixed, i.e. allowing little variance in syntax and lexical choice.\footnote{The meaning of some idioms may be derived from the non-literal meanings of their constituents. For example, in ``spill the beans'', the non-literal meaning of spill is ``reveal'' and the beans signify the secret \cite{sag2002multiword}.} Idiomatic expressions include proverbs (``actions speak louder than words''), clich\'{e}s (``what goes around comes around''), euphemisms (``rest in peace''), and more. 

Prior work on idioms largely focused on identifying the idiomaticity of a multi-word expression. This is a classification task, defined either at the token-level (is the phrase idiomatic within a given context?), or the type-level (may the phrase be idiomatic in some context?)  \cite{fazly-etal-2009-unsupervised,li-sporleder-2009-classifier,verma-vuppuluri-2015-new,peng-feldman-2016-experiments,salton-etal-2016-idiom,AAAI1714939}. Compared to identification, the interpretation of idioms has been less-explored. Approaches for representing idiomatic expressions include substituting idioms with literal paraphrases \cite{liu-hwa-2016-phrasal,zhou-etal-2021-pie}, representing them as a single token, or learning to compose them at the character rather than word-level \cite{liu-etal-2017-idiom}. 


With the rising popularity of pre-trained LMs, several recent papers studied their capacity to accurately represent idioms. \newcite{shwartz-dagan-2019-still} found that while LMs excelled at detecting non-literal word usage (e.g. ``flea'' in ``flea market''), the representation of idiomatic expressions was of lower quality than that of literal ones. \newcite{yu-ettinger-2020-assessing} showed that LMs encode the words that appear in a given text, but capture little information regarding phrase meaning. Finally, \newcite{garcia-etal-2021-assessing} studied the compositionality of noun compounds in English and Portuguese, and found that LM-based models did not perform well on detecting compositionality, and represented idiomaticity differently from humans. 


\subsection{Similes}
\label{sec:bg_similes}

Similes are a figure of speech that compares two things, usually with the intent to make the description more emphatic or vivid, and spark the reader's imagination \cite{definition}. Similes may either be explicit, i.e. specify the topic, vehicle, and similarity property, as in ``The house was cold like Antarctica'' (where the topic is ``house", the vehicle is ``Antarctica" and the property of comparison is ``cold"), or implicit, i.e. omitting the property, as in ``the house was like Antarctica'' (Section \ref{sec:datasimile}). Most work in NLP has focused on simile detection, i.e. distinguishing literal from
figurative comparisons. Earlier work relied on semantic and syntactic characteristics, i.e. higher semantic similarity between the topic and the vehicle in literal comparisons than in figurative comparisons \cite{niculae-danescu-niculescu-mizil-2014-brighter,qadir-etal-2015-learning,mpouli-2017-annotating}, and dictionary definitions \cite{qadir-etal-2016-automatically}, while more recent work is based on neural methods \cite{liu-etal-2018-neural,zeng2020neural}. Simile interpretation focused on inferring the implicit property \cite{qadir-etal-2016-automatically}. In other lines of work, \newcite{chakrabarty-etal-2020-generating} and \newcite{Zhang2021WritingPW} proposed methods for generating similes from their literal counterparts, while \newcite{chakrabarty-etal-2021-figurative} showed that state-of-the-art NLI models fail on pragmatic inferences involving similes.  

\subsection{Human Processing of Figurative Language}
\label{sec:processing}

The ways in which humans process figurative language may inspire computational work on figurative language interpretation. \newcite{10.2307/3587719} studied how L2 English speakers interpret unfamiliar English idioms. He found that the leading strategy was to infer the meaning from the given context, which led to successful interpretation in 57\% of the times, followed by relying on the literal meaning of the constituent words (22\% success rate). For example, a participant asked to interpret ``robbing the cradle'' in the context ``Robert knew that he was robbing the cradle by dating a sixteen-year-old girl'' used the literal meaning of cradle to associate the meaning with babies and indirectly with young age, and along with the context inferred that it meant to ``date a very young person''. \newcite{Asl2013TheIO} repeated the same experiment with stories, and concluded that longer contexts improved people's ability to interpret unknown idioms. With respect to novel similes and metaphors, they are interpreted through shared literal attributes between the topic and vehicle (e.g. ``Antarctica is cold, can a house also be cold?'') \cite{Wolff2000EvidenceFR,carston_wearing_2011}. 

\subsection{Narrative Understanding}
\label{sec:bg:narratives}
Early computational work on narrative understanding extracted chains of subevents and their participants from narratives \cite{chambers-jurafsky-2009-unsupervised}. An alternative task is machine reading comprehension, i.e. answering multiple-choice questions based on a narrative, such as MCTest \cite{richardson-etal-2013-mctest} and NarrativeQA \cite{kocisky-etal-2018-narrativeqa}. 

The most commonly used benchmark for narrative understanding today is ROCStories \cite{mostafazadeh-etal-2016-corpus}, a collection of 50k five-sentence commonsense stories pertaining to everyday life. The story cloze task requires models to identify the plausible continuation sentence among two candidate continuations in its discriminative form, or generate a plausible sentence, in its generative form. Since the release of this dataset, many computational approaches for the task were developed \cite[][\emph{inter alia}]{chaturvedi-etal-2017-story,schwartz-etal-2017-story,cai-etal-2017-pay,srinivasan-etal-2018-simple,ijcai2019-249,cui2020discriminative,brown2020language}. In this paper, we follow the story cloze benchmark setup, and collect benchmarks particularly aimed at testing the comprehension of figurative language in narratives. 

\subsection{Commonsense Knowledge Models}
\label{sec:bg:knowledge_models}

Many language tasks require relying on implicit commonsense knowledge that is never mentioned explicitly because it is assumed to be known by everyone. To that end, commonsense knowledge bases (KBs) record such facts. Notably, ConceptNet \cite{speer2017conceptnet} is a large-scale concept-centric KB, while ATOMIC \cite{atomic} contains event-centric knowledge about causes, effects, and the mental states of the participants. To overcome the sparsity of KBs, knowledge models such as COMET \cite{bosselut-etal-2019-comet,Hwang2021COMETATOMIC2O} fine-tuned an LM on structured KB triplets. COMET is capable of providing inferences for new events or concepts. ParaCOMET \cite{Gabriel2021ParagraphLevelCT} is an extension of ATOMIC-COMET which works at the paragraph level and generates discourse-aware commonsense knowledge. Recently several works have used such commonsense knowledge models for improved natural language understanding or generation such as \newcite{bhagavatula2019abductive} for abductive reasoning, \newcite{shwartz-etal-2020-unsupervised} for QA, \newcite{guan2019story,ammanabrolu2020automated} for story generation, \newcite{majumder-etal-2020-like} for dialog generation and \citeauthor{chakrabarty-etal-2020-r}~(\citeyear{chakrabarty-etal-2020-r,chakrabarty-etal-2020-generating,chakrabarty-etal-2021-mermaid}) for creative text generation.

In our work we use the knowledge models COMET \cite{Hwang2021COMETATOMIC2O} and ParaCOMET \cite{Gabriel2021ParagraphLevelCT} respectively to provide more information about the literal meaning of constituent words or the narrative context useful to infer the figurative expressions meaning.