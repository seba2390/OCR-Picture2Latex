\begin{figure*}[t]
    \centering
    \includegraphics[width=1.03\textwidth]{figures/knowledge_integration_gen.pdf}
    \caption{Integrating commonsense inferences into a GPT2-based generative model.}
    \label{fig:architecture_generative}
    % \vspace{-3ex}
\end{figure*}

In the generative task, given a narrative N, the goal is to generate a plausible next sentence that is coherent with the context and consistent with the meaning of the figurative expression. Each instance consists of a reference plausible continuation C.

\subsection{Methods}
\label{sec:generative_methods}

We similarly experiment with zero-shot, few-shot, and supervised models. 

\paragraph{Zero-shot.} We use standard LMs, GPT-2 XL and GPT-3, to generate the next sentence following the narrative. We let the models generate up to 20 tokens, stopping when an end of sentence token was generated. Following preliminary experiments, for GPT-2 XL and the rest of the models we use top-k sampling \cite{fan-etal-2018-hierarchical} as the decoding strategy with $k = 5$ and a softmax temperature of 0.7, while for GPT-3 we use the method provided in the API which is nucleus sampling \cite{holtzman2020curious} with a cumulative probability of $p = 0.9$.

\paragraph{Few-shot.} We prompt GPT-3 with 4 training examples of the form \texttt{Q: N A: C} followed by each individual test example, and decode the answer. 

\paragraph{Supervised.} We fine-tune GPT-2 XL with a language model objective for 3 epochs with a batch size of 2. We also trained T5 large \cite{raffel2020exploring} and BART large \cite{lewis-etal-2020-bart} as encoder-decoder models. Both were trained for 5 epochs for idioms and 20 epochs for similes, with an effective batch size of 64. For each model, we kept the best checkpoint based on the validation set perplexity, and used top-k decoding with $k = 5$ and a temperature of 0.7. 


\paragraph{Knowledge-Enhanced.} We followed the same intuition and inferences we used for the knowledge-enhanced discriminative models (Section~\ref{sec:disc:methods}). We fine-tune the models for one epoch as the effective data size is multiplied by the number of inferences per sample. The overall architecture of the generative knowledge-enhanced model is depicted in Figure~\ref{fig:architecture_generative}. The models are based on GPT-2 XL and trained with a language model objective to predict the next sentence given the narrative and \emph{a single inference}. The input format for inference Inf$_j$ is: \texttt{Inf$_j$ <sep1> N <sep2>}, where \texttt{<sep1>} and \texttt{<sep2>} are special tokens, and the expected output is the plausible continuation C. During inference, we combine the generations from all inferences pertaining to a given narrative. Inspired by \newcite{liu-etal-2021-dexperts}, who ensemble logits from multiple LMs, we ensemble the logits predicted for multiple input prompts using the same model. 

A standard decoding process gets at each time step an input prompt text $x_{\lt t}$ of length $t-1$. The prompt is encoded and the model outputs the logits for the next ($t^{th}$) token, denoted by $z_{t} \in {\rm I\!R}^{|V|}$, where V is the vocabulary. To get a discrete next token, $z_{t}$ is normalized and exponentiated to resemble a probability distribution over the vocabulary: $P(X_t|x_{\lt t}) = \operatorname{softmax}(z_t)$, and the next token $x_{t}$ is sampled from $P(X_{t}|x_{<t})$. This token is then appended to the prompt and the process iteratively continues until a predefined length or until an end of sentence token had been generated.

Our decoding process differs in that at time step t, we compute the logits ${z_t}_{j=1}^{12}$ corresponding to the prompts derived from each of the inferences: \texttt{Inf$_j$ <sep1> N <sep2>} for $j = 1 ... 12$. We sum the logits vectors to obtain $z_t = \sum_{j=1}^{12} {z_t}_{j}$, from which we decode the next token as usual.

\input{figures/generative_results}

\subsection{Results}
\label{sec:generative:results}

\input{figures/example_generations}
\input{figures/human_eval_results}
\input{figures/error_analysis_examples}

\paragraph{Automatic Evaluation.} Table~\ref{tab:generative_results} shows the performance of all the models on the generative tasks in terms of automatic metrics. We report the performance of the recall-oriented n-gram overlap metric Rouge-L \cite{lin-2004-rouge}, typically used for summarization tasks, and the similarity-based BERT-Score \cite{zhang2019bertscore}. We use the latest  implementation to date which replaces BERT with \texttt{deberta-large-mnli}, which is a DeBERTa model \cite{he2020deberta} fine-tuned on MNLI \cite{williams-etal-2018-broad}. In terms of automatic evaluation, the best-performing knowledge-enhanced model (context for idioms and literal for similes) perform similarly to the GPT-2 XL supervised baseline, with slight preference to the baseline for idioms and to the knowledge-enhanced model for similes. Both types of supervised models outperform the zero-shot and few-shot models. 



\input{figures/error_analysis}


\paragraph{Human Evaluation.}
While automatic metrics provides an estimate of relative model performance, these metrics were often found to have very little correlation with human judgements \cite{novikova-etal-2017-need,krishna2021hurdles}. To account for this we also performed human evaluation of the generated texts for a sample of the test narratives. The human judgements were collected using Amazon Mechanical Turk. Workers were shown a narrative, the meaning of the idiom (or the property of the simile), and a list of 3 generated continuations, one from each of the supervised GPT-2 model, the context model, and the literal model. We performed two types of evaluations. In the absolute evaluation, we randomly sampled 50 narratives for each task, and asked workers to determine for each of the generated continuations along with the human references whether it is plausible or not. In the comparative evaluation, we randomly sampled 100 narratives for idioms and 75 for similes, and presented the workers with a randomly shuffled list of continuations, asking them to choose the most plausible one (or indicate that ``neither of the generations were good'' or ``all are equally good''). In both evaluations, workers were instructed to consider whether the generation is sensical, coherent, follows the narrative, and consistent with the meaning of the figurative expression. Each example was judged by 3 workers and aggregated using majority voting. The inter-annotator agreement was moderate with Krippendorff's $\alpha = 0.68$ and $\alpha = 0.63$ for the absolute and comparative evaluations respectively \cite{Krippendorff2011ComputingKA}. 

In both absolute and comparative performance, Table~\ref{tab:humaneval} shows that for each of the tasks, a knowledge-enhanced model outperformed the baseline GPT-2 model. What makes a more compelling case is that the context model was favored for idioms while the literal model was favored for similes, complying with prior theoretical grounding on these figurative language types. Figure~\ref{fig:generations} shows examples generated by the baseline and the best model for each task. We note that 80\% of the human-written continuations for idioms and 88\% of those in the simile task were judged as plausible. Based on our analysis, the gap from 100\% may be explained by the ambiguity of the narratives that leaves room for subjective interpretation. 

\subsection{Error Analysis}
\label{sec:generative_analysis}


We analyze the continuations labeled as implausible by the annotators, for the best model in each task: context for idioms and literal for similes. We found the following error categories, with percent details in Table~\ref{tab:error_analysis} and exemplified in Table~\ref{tab:error_analysis_examples}: 

\noindent \paragraph{\textcircled{1} Inconsistent with the figurative expression:} The continuation is inconsistent or contradictory to the figurative expression. For instance, the simile in the first row in Table~\ref{tab:error_analysis_examples} is ``like a sloppily designed ink blot test'', for which the property of comparison is ``a pattern of dark blue, purple, and black'', but the generated continuation mentions brownie, which has a \emph{brown} color. Similarly for the idiom ``thick as thieves'' the model generates a literal continuation without understanding its actual meaning ``closest of friends". 


\noindent \paragraph{\textcircled{2} Inconsistent with the narrative:} The continuation is inconsistent or contradictory to the flow of the narrative. For instance, the narrative in the second row in Table~\ref{tab:error_analysis_examples} states that ``the owners who are humans are \emph{standing}'', while the continuation states they are jumping. The model further predicts that the \emph{humans} are barking, instead of the \emph{dogs}. In general, across multiple examples we have found that models tend to confuse the various characters in the narrative.


\noindent \paragraph{\textcircled{3} Spelling or grammar errors:} some generations contained spelling mistakes or introduced grammar errors such as starting with a punctuation or having extra blank spaces. Although we instructed the crowdsourcing workers to ignore such errors, they may have affected their plausibility judgements.
