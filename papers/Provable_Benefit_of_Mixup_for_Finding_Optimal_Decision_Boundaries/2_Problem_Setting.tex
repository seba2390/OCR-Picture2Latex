\vspace*{-10pt}
\section{Problem Setting and Notation}\label{section:setting}
In this section, we introduce our formal problem setting and notation. 
We consider a binary linear classification problem with training dataset $S = \{(\vx_i,y_i)\}_{i=1}^n$ where $\vx_i\in \mathbb{R}^d$ are data points and $y_i\in \{0,1\}$ are labels.
The vanilla empirical risk minimization (ERM) loss under training set $S$ can be formulated as follows:
\begin{equation}
\label{eq:1}
   \loss(\vw) :=  \frac{1}{n} \sum_{i=1}^n y_i l(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i), 
\end{equation}
where $l(\cdot)$ is the logistic loss $l(z) := \log(1+\exp(-z))$.

\paragraph{Data Generating Distribution.} We mainly focus on the following family of data generating distributions, where we obtain positive and negative examples from two symmetric Gaussian distributions. More precisely, for a given constant $\kappa\in (0, \infty)$, we define a data generating distribution $\mathcal{D}_\kappa$ as the following: we say that $(\vx,y) \sim \mathcal{D}_\kappa$ when $y$ is uniformly drawn from $\{0, 1\}$ and
\begin{align*}
    \vx \mid {y=1} &\sim N( \vmu , \kappa^{-1} \mSigma ), \\
    \vx \mid {y=0} &\sim N(-\vmu , \kappa^{-1}\mSigma ).
\end{align*}
where $\vmu \in \mathbb{R}^d$ is nonzero, $\mSigma \in \mathbb{R}^{d \times d}$ is positive definite, and $\lVert \vmu \rVert^2 = \lVert \mSigma \rVert$. Here, $\lVert \cdot \rVert$ denotes the $\ell_2$ norm for a vector and the spectral norm for a matrix. We further define a data generating distribution $\mathcal{D}_\infty$ as the limiting behavior of $\mathcal{D}_\kappa$ as $\kappa \to \infty$; i.e., $(\vx,y) \sim \mathcal{D}_\infty$ implies $y$ follows uniform distribution on $\{0,1\}$ and $\vx = (2y-1)\vmu$. 

\paragraph{Separability Constant $\bm{\kappa}$.}
Our data generating distribution $\mathcal{D}_\kappa$ with larger $\kappa$ is more \emph{separable} in the sense that the two Gaussian distributions overlap less and they are more likely to generate well-separated training data. This separability constant $\kappa$ is of great importance in our analysis, as we will demonstrate the curse of separability phenomenon based on the dependency of sample complexity on $\kappa$.

\paragraph{Bayes Optimal Classifier.}
The Bayes optimal classifier is a classifier achieving the lowest possible test error on the data population. In other words, any other classifier cannot outperform the Bayes optimal classifier in terms of test accuracy. 
For the specific form of data generating distribution $\mathcal{D}_\kappa$ that we consider in this paper, it is well-known that for any $\kappa \in (0,\infty)$, decision boundary of the Bayes optimal classifier for $\mathcal{D}_\kappa$ is a hyperplane with normal vector $\mSigma^{-1} \vmu$. For completeness, we provide proof for this in Appendix~\ref{proof:Bayes_optimal}. The fact that we have a closed-form solution of the optimal decision boundary motivates us to study finding a decision boundary close enough to that optimal one. Since we are mainly interested in linear models, for our ``closeness'' metric, we use cosine similarity between the two linear decision boundaries, or equivalently, the two normal vectors.

\paragraph{Comparison to the Setting of \citet{chidambaram2021towards}.}
\citet{chidambaram2021towards} also consider the linear model and a special type of data generating distributions to compare training ERM loss vs.\ Mixup loss. Our setting is different from theirs in several aspects. First, there is a difference in data generating distributions; we draw positive and negative data from two symmetric Gaussian distributions while \citet{chidambaram2021towards} consider the distribution that obtains data from a single spherical Gaussian distribution. \citet{chidambaram2021towards} show that training ERM loss and Mixup loss can lead to the same solution by considering the highly overparametrized regime. On the other hand, we consider the underparametrized regime and show the gap between the two training methods. In addition, \citet{chidambaram2021towards} only consider the optimal classifier in terms of training loss while we investigate the number of required training data to get close to the optimal solution of population loss.

\paragraph{Notation.}
We denote taking expectation on training data $S = \{(\vx_i, y_i)\}_{i=1}^n \overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ and any other randomness, if any, 
as $\mathbb{E}_\kappa$ for each $\kappa \in (0, \infty]$. We use $[k]$ for the index set $\{1, 2, \dots, k\}$ for each $k \in \mathbb{N}$. For two nonzero vectors $\vu,\vv \in \mathbb{R}^d$, let us denote their cosine similarity as $\cosim(\vu,\vv) = \frac{\vu^\top \vv}{\lVert \vu\rVert\lVert \vv\rVert}$ and the angle between them as $\angle(\vu,\vv) = \cos^{-1}(\cosim(\vu,\vv))$. We use $\support(\cdot)$ to denote the support of a probability distribution or a random variable and $\vone_{(\cdot)}$ denotes the $0$-$1$ indicator function. We also use $\odot$ to denote element-wise multiplication between two vectors or two matrices having the same size. We use $\mathcal{O}(\cdot), \Theta(\cdot), \Omega(\cdot)$ to represent asymptotic behavior as $\kappa$ grows and to hide terms related to $d, \vmu, \mSigma$. 
In addition, whenever we express the $\kappa$ dependency in $\mathcal{O}(\cdot), \Theta(\cdot), \Omega(\cdot)$ notation, we only write the most dominant factor; for example, we say $\kappa^m\mathrm{polylog}(\kappa) = \Theta(\kappa^m)$ and $\exp\left(c \kappa^m\right) \mathrm{poly}(\kappa) = \exp(\Theta(\kappa^m))$.