\subsection{Proof of Theorem~\ref{thm:mask_convergence}}\label{proof:mask_convergence}
Since our sample complexity bound contains $\Omega (1)$ which can hide $d$, we may assume $n \geq d$ and $n \geq 2$. 
Let $R$ be the upper bound on $\{ \lVert \wmask \rVert : \kappa \in (0,\infty] \}$ which we defined in Appendix~\ref{proof:mask}. Next, define a compact set $\mathcal{C}:= \{\vw \in \mathbb{R}^d \mid \lVert \vw \rVert \leq 2R\}$, which trivially contains $\wmask$ for all $\kappa \in (0, \infty]$. For any $\vw\in \mathbb{R}^d$ and nonzero $\vv \in \mathbb{R}^d$, we have
\begin{align*}
    \vv \nabla^2_\vw \lossmask (\vw) \vv &= \frac{1}{n^2} \sum_{i,j=1}^n (\tilde{y}_{i,j}^\mathrm{mask} l''\left(w^\top \tilde{\vx}_{i,j}^\mathrm{mask}\right)\left(\vv^\top \tilde{\vx}_{i,j}^\mathrm{mask}\right)^2 +  \left(1-\tilde{y}_{i,j}^\mathrm{mask}\right)l''\left(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}\right)\left(\vv^\top \tilde{\vx}_{i,j}^\mathrm{mask}\right)^2)\\
    &\geq\frac{1}{n^2} \sum_{i=1}^n (y_i l''(\vw^\top \vx_i)(\vv^\top \vx_i)^2 +  (1-y_i)l''(-\vw^\top \vx_i)(\vv^\top \vx_i)^2)>0,
\end{align*}
almost surely, since $\{\vx_i\}_{i \in [n]}$ spans $\mathbb{R}^d$ almost surely. Therefore, $\lossmask (\vw)$ is strictly convex almost surely and we conclude that $\lossmask(\vw)$ has a unique minimizer $\hat{\vw}_{\mathrm{mask}, S}^*$ on $\mathcal{C}$ almost surely. Also, if $\hat{\vw}_{\mathrm{mask},S}^*$ belong to interior of $\mathcal{C}$, it is minimizer of $\lossmask(\vw)$ on $\mathbb{R}^d$. We will prove high probability convergence of $\hat{\vw}_{\mathrm{mask}, S}^*$ to $\wmask$ using Lemma~\ref{lemma:minimizer_dependent} and convert $\ell_2$ convergence into directional convergence.  For simplicity, we define
\begin{equation*}
f_{i,j}(\vw) : = \tilde{y}_{i,j}^\mathrm{mask}l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask}) l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}),
\end{equation*}
for each $i,j \in [n]$.
We start with the following claim which is useful for estimating quantities described in assumptions of Lemma~\ref{lemma:minimizer_dependent} for our setting.
\begin{claim}\label{claim:mask}
For any  $t>0$, we have
\begin{equation*}
\mathbb{E}_\kappa \left[ e^{t \left \lVert \tilde{\vx}_{i,j}^\mathrm{mask}\right \rVert} \right] \leq \max \left \{  2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert} ,  \sum_{k=1}^p 2c_k \left(2^{d/2} + e^{4\kappa^{-1} t^2 \left \lVert \mSigma^{(k)} \right \rVert} \right)  \right \}e^{t\lVert \vmu \rVert} ,
\end{equation*}
for all $i,j \in [n]$.
\end{claim}
\begin{proof}[Proof of Claim~\ref{claim:mask}]
We first consider the case $i = j$. By invoking triangular inequality and Lemma~\ref{lemma:ineq4}, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{t \left \lVert \tilde{\vx}_{i,i}^\mathrm{mask}\right \rVert} \right] 
&= \mathbb{E}_\kappa \left[ e^{t \left \lVert \vx\right \rVert} \right] = \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx\right \rVert} \right]\\
&= \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx\right \rVert} \right] \leq \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx - \vmu \right \rVert} \right] e^{t\lVert \vmu \rVert}\\
&= \mathbb{E}_{\rvz \sim N(\vzero, \kappa^{-1}t^2 \mSigma )} \left[e^{\lVert \rvz \rVert} \right] e^{t\lVert \vmu \rVert} \\
&= \left( 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert}\right)e^{t\lVert \vmu \rVert}.
\end{align*}
for each $i \in [n]$. Next, we handle the case $i \neq j$. From \Eqref{eqn:mask_dist}, for any $i,j\in[n]$ with $i \neq j$, we have
\begin{align*}
&\quad \mathbb{E}_\kappa \left[ e^{t \left \lVert \tilde{\vx}_{i,j}^\mathrm{mask}\right \rVert} \right]\\
&= \sum_{k=1}^p c_k \left( \mathbb{E}_{\rvx \sim N \left (\vmu, \kappa^{-1}\mSigma^{(k)} \right)}\left [e^{t\lVert \rvx \rVert}\right] + \mathbb{E}_{\rvx \sim N\left( \vmu^{(k)} , \kappa^{-1} \mSigma^{(k)}\right)}\left [e^{t\lVert \rvx \rVert}\right]\right)\\
&\leq \sum_{k=1}^p c_k \left( \mathbb{E}_{\rvx \sim N \left (\vmu, \kappa^{-1}\mSigma^{(k)} \right)}\left [e^{t\lVert \rvx -\vmu \rVert}\right]e^{t\lVert \vmu \rVert} + \mathbb{E}_{\rvx \sim N\left( \vmu^{(k)} , \kappa^{-1} \mSigma^{(k)}\right)}\left [e^{t\lVert \rvx -\vmu^{(k)}\rVert}\right]e^{t\lVert \vmu^{(k)}\rVert} \right)\\ 
&= \sum_{k=1}^p 2c_k \mathbb{E}_{\rvz \sim N\left (\vzero, \kappa^{-1}t^2 \mSigma^{(k)}\right)} \left [e^{\lVert \rvz \rVert} \right]e^{t \lVert \vmu \rVert}.
\end{align*}
By applying Lemma~\ref{lemma:ineq4},
\begin{equation*}
\mathbb{E}_{\rvz \sim N \left (\vzero, \kappa^{-1}t^2 \mSigma^{(k)}\right) }\left [e^{\lVert \rvz \rVert} \right] \leq 2^{d/2} + e^{4\kappa^{-1}t^2 \left \lVert \mSigma^{(k)} \right\rVert },
\end{equation*}
and we have our conclusion.
\end{proof}

\paragraph{Step 1: Estimate Upper Bound of $\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right]$ on $\mathcal{C}$ }\quad

For any $\vw \in \mathcal{C}$ and $i,j \in [n]$, we have
\begin{align*}
    |f_{i,j}(\vw)| &= |\tilde{y}_{i,j}^\mathrm{mask}l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask})l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})|\\
    &\leq l(-|\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}|)\\
    &\leq l(-2R \lVert \tilde{\vx}_{i,j}^\mathrm{mask} \rVert)\\
    &= \log \left( 1+e^{2R\left\lVert \tilde{x}_{i,j}^\mathrm{mask} \right\rVert} \right).
\end{align*}
By applying Claim~\ref{claim:mask} for $t = 2R$, there exists $M_\kappa'$ such that $M_\kappa' = \Theta(1)$ and
\begin{equation*}
    \mathbb{E}_\kappa \left[ e^{|f_{i,j}(\vw)|}\right] \leq \mathbb{E}_\kappa \left[ 1+ e^{2R \lVert \tilde{\vx}_{i,j}^\mathrm{mask}\rVert} \right]< M_\kappa'.
\end{equation*}
From triangular inequality and Jensen's inequality, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right] 
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw)\right| + \left|\mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right]\\
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw)\right| }\right]^2\\
&\leq {M_\kappa'}^2.
\end{align*}
Letting $M_\kappa = {M'_\kappa}^2$, it follows that $M_\kappa = \Theta(1)$ and $\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right] < M_\kappa$ for all $\vw \in \mathcal{C}$.

\paragraph{Step 2: Estimate Upper Bound of $\lVert \nabla_\vw f_{i,j}(\vw)\rVert$ and $\lVert \nabla_\vw \mathbb{E}_\kappa[f_{i,j}(\vw)]\rVert$} \quad

For each $\vw \in \mathcal{C}$ and $i,j \in [n]$, 
\begin{align*}
\lVert \nabla_\vw f_{i,j}(\vw) \rVert 
&= \left \lVert \nabla_\vw (\tilde{y}_{i,j}^\mathrm{mask}l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask}) l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})) \right \rVert \\
&= \left \lVert \tilde{y}_{i,j}^\mathrm{mask}l'(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\tilde{\vx}_{i,j}^\mathrm{mask} - (1-\tilde{y}_{i,j}^\mathrm{mask}) l'(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\tilde{\vx}_{i,j}^\mathrm{mask} \right \rVert\\
&\leq  \left \lVert \tilde{\vx}_{i,j}^\mathrm{mask} \right \rVert.
\end{align*}
In addition, by Lemma~\ref{lemma:gradient&hessian},
\begin{align*}
\lVert \nabla_\vw \mathbb{E}[f_{i,j}(\vw)] \rVert 
&= \left \lVert \nabla_\vw \mathbb{E}\left[(\tilde{y}_{i,j}^\mathrm{mask}l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask}) l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}))\right] \right \rVert \\
&= \left \lVert \mathbb{E}\left[\nabla_\vw(\tilde{y}_{i,j}^\mathrm{mask}l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask}) l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})) \right] \right \rVert \\
&= \left \lVert \mathbb{E}\left[ \tilde{y}_{i,j}^\mathrm{mask}l'(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\tilde{\vx}_{i,j}^\mathrm{mask} - (1-\tilde{y}_{i,j}^\mathrm{mask}) l'(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\tilde{\vx}_{i,j}^\mathrm{mask} \right]\right \rVert\\
&\leq  \mathbb{E}\left[ \left  \lVert \tilde{\vx}_{i,j}^\mathrm{mask} \right \rVert \right].
\end{align*}
Also, by applying Claim~\ref{claim:mask} with $t=1$, there exists $L_\kappa$ such that $\mathbb{E}_\kappa \left[e^{ \left  \lVert \tilde{\vx}_{i,j}^\mathrm{mask} \right \rVert} \right]<L_\kappa$ and $L_\kappa = \Theta(1)$.

\paragraph{Step 3: Estimate Strong Convexity Constant of $\mathbb{E}_\kappa[\lossmask (\vw)]$ on $\mathcal{C}$} \mbox{}

By Lemma~\ref{lemma:gradient&hessian}, Lemma~\ref{lemma:ineq1} and \Eqref{eqn:mask},  we have 
\begin{align*}
    \vv \nabla^2_\vw \mathbb{E}_\kappa\left[\lossmask (\vw)\right] \vv &\geq \frac{n-1}{n} \sum_{k=1}^p a_k \mathbb{E}_{\rvx^{(k)} \sim N\left (\vmu^{(k)}, \kappa^{-1} \mSigma^{(k)}\right)}\left[l''\left(\vw^\top \rvx^{(k)}\right) \left(\vv^\top \rvx^{(k)}\right)^2 \right]\\
    &\geq \frac{1}{8} \min_{k\in [p]}\{a_k\} \sum_{k=1}^p \mathbb{E}_{\rvx^{(k)} \sim N\left (\vmu^{(k)}, \kappa^{-1}\mSigma^{(k)}\right)}\left[e^{-\left(\vw^\top \rvx^{(k)}\right)^2/2} \left(\vv^\top \rvx^{(k)}\right)^2 \right],
\end{align*}
for any $\vw\in \mathcal{C}$ and unit vector $\vv \in \mathbb{R}^d$.

For each $k \in [p]$,
\begin{align*}
    &\quad \vv^\top \mSigma^{(k)} \vv\\
    &= \left(\mM^{(k)} \odot \vv\right)^\top \mSigma \left(\mM^{(k)} \odot \vv\right) + \left(\left(\vone-\mM^{(k)}\right)\odot \vv\right)^\top \mSigma \left(\left(\vone-\mM^{(k)}\right) \odot \vv\right) \\
    &\geq \left\lVert \mSigma ^{-1} \right\rVert^{-1} \left \lVert \mM^{(k)} \odot \vv  \right\rVert ^2 + \left\lVert \mSigma ^{-1} \right\rVert^{-1} \left \lVert \left(\vone-\mM^{(k)} \right) \odot \vv \right \rVert ^2\\
    &= \left \lVert \mSigma^{-1} \right\rVert^{-1} \lVert \vv \rVert^2.
\end{align*}
Hence, $\mSigma^{(k)}$ is positive definite for all $k \in [p]$ and by Lemma~\ref{lemma:ineq3},
\begin{align*}
    &\quad \mathbb{E}_{\rvx^{(k)} \sim N\left (\vmu^{(k)}, \kappa^{-1}\mSigma^{(k)}\right)}\left[e^{-\left(\vw^\top \rvx^{(k)}\right)^2/2} \left(\vv^\top \rvx^{(k)}\right)^2 \right]\\
    &\geq \left(\frac{1}{2} \left(\vv^\top \vmu^{(k)}\right)^2 - \kappa^{-2} \left \lVert \mSigma^{(k)} \right \rVert^2 \lVert \vw \rVert^4 \left \lVert \vmu^{(k)} \right \rVert^2 \right)\\
    &\quad \cdot \left(\kappa^{-1} \left \lVert \mSigma^{(k)} \right \rVert\left(\kappa \left\lVert \left(\mSigma^{(k)}\right)^{-1} \right\rVert+ \lVert \vw \rVert^2 \right)\right)^{-d/2} \exp \left( -\lVert \vw \rVert^2 \left\lVert \left(\mSigma^{(k)}\right)^{-1}\right\rVert \left\lVert \mSigma^{(k)}\right\rVert \left \lVert \vmu^{(k)} \right \rVert^2 \right)\\
    &\geq \left(\frac{1}{2} \left(\vv^\top \vmu^{(k)}\right)^2 - 16\kappa^{-2} \left \lVert \mSigma^{(k)} \right \rVert^2 R^4 \left \lVert \vmu^{(k)} \right \rVert^2 \right)\\
    &\quad \cdot \left(\kappa^{-1} \left \lVert \mSigma^{(k)} \right \rVert\left(\kappa \left\lVert \left(\mSigma^{(k)}\right)^{-1} \right\rVert+ 4R^2 \right)\right)^{-d/2} \exp \left( -4R^2 \left\lVert \left(\mSigma^{(k)}\right)^{-1}\right\rVert \left\lVert \mSigma^{(k)}\right\rVert \left \lVert \vmu^{(k)} \right \rVert^2 \right)\\
    &= \left(\vv^\top \vmu^{(k)}\right)^2 \Theta(1).
\end{align*}
By Assumption~\ref{Assumption:mask}, $\sum_{k=1}^p \left(\vv^\top \vmu^{(k)}\right)^2 >0$ for each unit vector $\vv \in \mathbb{R}^d$ and since $\vv \mapsto \sum_{k=1}^p \left(\vv^\top \vmu^{(k)}\right)^2$ is continuous, we conclude that $\mathbb{E}_\kappa [\lossmask (\vw)]$ is $\alpha_k$-strongly convex on $\mathcal{C}$ where $\alpha_\kappa = \Theta(1)$. In addition, we can choose $\alpha_\kappa$ small enough so that for sufficiently large $\kappa$, $\lVert \wmask \rVert \leq R < \alpha_\kappa^{-1/2}$ since $\lVert \wmask \rVert = \Theta(1)$. This choice of $\alpha_\kappa$ makes it possible to apply Lemma~\ref{lemma:minimizer_dependent}.

\paragraph{Step 4: Lower Bounds on $\lVert \wmask \rVert$ for Sufficiently Large $n$ and $\kappa$} \quad

We need lower bounds on $\lVert \wmask \rVert$ that are independent of $n$ when we apply Lemma~\ref{lemma:minimizer_dependent} in our final step. However, finding such lower bounds is challenging since we do not know the exact direction of $\wmask$ unlike $\w$ and $\wmix$. In addition, the fact that $\lossmask(\vw)$ is dependent on $n$ also makes it hard. 
Instead, we will focus on sufficiently large $n$ and $\kappa$ and look for lower bounds independent of $n$, which is sufficient for our analysis.

We introduce a function $\mathcal{L}_\infty^\mathrm{mask} :  \R^d \to \R$ which corresponds to the limit case of $\mathbb{E}_\infty [\lossmask (\vw)]$ as $n \rightarrow \infty$ and defined as follows (i.e., the limit when both $n$ and $\kappa$ approach $\infty$):
\begin{equation}\label{eqn:mask_infty_limit}
    \mathcal{L}_\infty^\mathrm{mask}(\vw) := \lim_{n \rightarrow \infty} \mathbb{E}_{\infty}[\lossmask(\vw)]= \sum_{k=1}^p \left(a_k l\left (\vw^\top \vmu^{(k)}\right) + b_k l\left (- \vw^\top \vmu^{(k)}\right) + c_k l\left (\vw^\top \vmu\right) \right).
\end{equation}
Analyzing a minimizer of $\mathcal{L}_\infty^\mathrm{mask}(\vw)$ is helpful because it is independent of $n$ and approximates  $\lossmask(\vw)$ for large enough $n$ and $\kappa$.

Recall that we choose $i_1, \dots, i_d \in [m]$ such that $\left \{ \vmu^{(i_1)}, \dots, \vmu^{(i_d)} \right \}$ spans $\mathbb{R}^d$ in Appendix~\ref{proof:mask}. For any $t\in [0,1]$ and $\vw_1,\vw_2 \in \mathbb{R}^d$ with $\vw_1 \neq \vw_2$, at least one of $k \in [d]$ satisfies 
\begin{equation*}
    t l\left(\vw_1^\top \vmu^{(i_k)}\right) + (1-t) l\left(\vw_2^\top \vmu^{(i_k)}\right) >  l\left((t\vw_1 + (1-t)\vw_2)^\top \vmu^{(i_k)}\right),
\end{equation*}
and
\begin{equation*}
    t l\left(- \vw_1^\top \vmu^{(i_k)}\right) + (1-t) l\left(-\vw_2^\top \vmu^{(i_k)}\right) >  l\left(-(t\vw_1 + (1-t)\vw_2)^\top \vmu^{(i_k)}\right).
\end{equation*}
We can conclude the strict convexity of $\mathcal{L}_\infty^\mathrm{mask}(\vw)$. Also, since $l(z) + l(-z) \geq |z|$ for each $z \in \R$, we have
\begin{align*}
    \mathcal{L}_\infty^\mathrm{mask}(\vw)
    &\geq \frac{1}{2} \sum_{k=1}^d \min\{a_{i_k}, b_{i_k}\} \left(l\left(\vw^\top \vmu^{(k)}\right) + l\left(-\vw^\top \vmu^{(k)}\right)\right)\\
    &\geq \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k},b_{i_k}\} \left|\vw^\top \vmu^{(i_k)}\right|.
\end{align*}
In Appendix~\ref{proof:mask}, we have shown that for any unit vector $\vu \in \mathbb{R}^d$, $\sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vu^\top \vmu^{(i_k)}\right| >0$ and $\vu \mapsto \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vu^\top \vmu^{(i_k)}\right|$ has the minimum value $m>0$. If $\lVert \vw \rVert > R$, where we previously defined $R:=\frac{2 \log 2}{m}$, then we have
\begin{align*}
    \mathcal{L}_\infty^\mathrm{mask}(\vw)  &\geq \frac{1}{2} \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vw^\top \vmu^{(i_k)}\right|\\ 
    &= \lVert \vw \rVert \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\left(\frac{\vw}{\lVert \vw \rVert}\right)^\top \vmu^{(i_k)}\right|\\ 
    &\geq \frac{1}{2}\lVert \vw \rVert m \geq \log 2\\
    &= \mathcal{L}_\infty^\mathrm{mask}(\vzero).
\end{align*}
Hence, a minimizer of $\mathcal{L}_\infty^\mathrm{mask}(\vw)$ contained in the ball centered origin with radius $R$. Together with the strict convexity, we can conclude that $\mathcal{L}_\infty^\mathrm{mask}(\vw)$ has the unique minimizer $\vw_{\mathrm{mask}}^*$ and it satisfies $\lVert \vw_{\mathrm{mask}}^* \rVert \leq R$.

In addition, we would like to prove that $\vw_{\mathrm{mask}}^*$ is nonzero. This will make our lower bounds on $\lVert \wmask \rVert$ positive. Since $\vmu$ is nonzero, without loss of generality, we assume 1st coordinate of $\vmu$, namely $\mu_1$, is nonzero. We consider a weight $r\ve_1$, where $\ve_1$ is 1st standard basis and $r>0$ which will be chosen later. We have
\begin{align*}
    &\quad \mathcal{L}_\infty^\mathrm{mask}(r\ve_1) - \mathcal{L}_\infty^\mathrm{mask}(\vzero)\\
    &= \sum_{k=1}^p \left[ a_k \left( l\left( r \ve_1^\top \vmu^{(k)} \right) - l(0) \right) + b_k \left(  l\left( -r \ve_1^\top \vmu^{(k)} \right) - l(0)\right) + c_k \left(  l\left( r \ve_1^\top \vmu \right) - l(0)\right)\right]\\
    &= \sum_{k \in \mathcal{I} }\Big[ a_k \big( l\left( r \mu_1 \right) - l(0) \big) + b_k \big(  l\left( -r \mu_1 \right) - l(0)\big) + c_k \big(  l\left( r \mu_1 \right) - l(0)\big)\Big],
\end{align*}
where $\mathcal{I} \subset [p]$ is the index set satisfying 1st coordinate of $\mM^{(k)}$ is 1 for each $k \in \mathcal{I}$. From our definition of $a_k,b_k,c_k$'s, $a_k + b_k = c_k$, thus we have
\begin{align*}
    &\quad \sum_{k \in \mathcal{I} }\Big[ a_k \big( l\left( r \mu_1 \right) - l(0) \big) + b_k \big(  l\left( -r \mu_1 \right) - l(0)\big) + c_k \big(  l\left( r \mu_1 \right) - l(0)\big)\Big]\\
    &= \sum_{k \in \mathcal{I} }\Big[ a_k \big( l\left( r \mu_1 \right) -l\left( -r \mu_1 \right) \big)  + c_k \big(  l\left( r \mu_1 \right) - l(0)\big)\Big]\\
    &= \sum_{k \in \mathcal{I} }\Big[- a_k r\mu_1  + c_k \big(  l\left( r \mu_1 \right)+l\left(- r \mu_1 \right) - 2l(0)\big)\Big]\\
    &= \sum_{k \in \mathcal{I} } c_kr\mu_1\left (  \frac{l\left( r \mu_1 \right)+l\left(- r \mu_1 \right) - 2l(0)}{r\mu_1} - \frac{a_k}{c_k}\right).
\end{align*}
Since $\lim_{z \rightarrow 0} \frac{l(z)+l(-z) - 2l(0)}{z} = 0$, we can choose $r>0$ small enough so that $\frac{l\left( r \mu_1 \right)+l\left(- r \mu_1 \right) - 2l(0)}{r\mu_1} < \frac{a_k}{c_k}$ for all $k \in \mathcal{I}$. Then, we obtain $\mathcal{L}_\infty^\mathrm{mask}(r\ve_1) - \mathcal{L}_\infty^\mathrm{mask}(\vzero)<0$ and thus $\vw_{\mathrm{mask}}^*$ is nonzero.

Next, we would like to characterize the strong convexity of  $\mathcal{L}_\infty^\mathrm{mask}(\vw)$.
For each $\vw \in \mathbb{R}^d$ with $\lVert \vw \rVert \leq R$ and unit vector $\vv \in \mathbb{R}^d$,  \Eqref{eqn:mask_infty_limit} implies
\begin{align*}
 &\quad \vv^\top \nabla_\vw^2 \mathcal{L}_\infty^\mathrm{mask}(\vw)\vv \\
 &=  \sum_{k=1}^p \left[ \left(a_k l''\left(\vw^\top \vmu^{(k)}\right) + b_k l''\left(-\vw^\top \vmu^{(k)}\right)\right) \left(\vv^\top \vmu^{(k)}\right)^2 + c_k l''\left(\vw^\top \vmu \right)(\vv^\top \vmu)^2 \right]\\
 &\geq \frac{1}{2} \sum_{k=1}^p (a_k + b_k) l''\left (R\lVert \vmu^{(k)}\rVert \right) \left(\vv^\top \vmu^{(k)} \right)^2.
\end{align*}
Recall that we have shown that $\vv \mapsto \frac{1}{2} \sum_{k=1}^p (a_k + b_k) l''\left ( R\lVert \vmu^{(k)}\rVert \right) \left(\vv^\top \vmu^{(k)} \right)^2$ has minimum $\alpha>0$ on the unit sphere $\{ \vv \in \mathbb{R}^d:\lVert \vv\rVert=1 \}$ in Appendix~\ref{proof:mask}. Hence, $\mathcal{L}_\infty^\mathrm{mask}(\vw)$ is $\alpha$-strongly convex on $\left \{\vw\in \mathbb{R}^d : \lVert \vw \rVert \leq R \right \}$. Since $ \wmask[\infty]$ and $\vw_{\mathrm{mask}}^*$ are contained in $ \left \{\vw\in \mathbb{R}^d : \lVert \vw \rVert \leq R \right \}$, we have
\begin{align*}
    &\quad \frac{\alpha}{2} \lVert \wmask[\infty] - \vw_{\mathrm{mask}}^* \rVert^2\\
    &\leq \mathcal{L}_\infty^{\mathrm{mask}}(\wmask[\infty]) - \mathcal{L}_\infty^{\mathrm{mask}}(\vw_{\mathrm{mask}}^*)\\
    &= \Big( \mathcal{L}_\infty^{\mathrm{mask}}\big(\wmask[\infty]\big)-\mathbb{E}_\infty \left [\lossmask \big (\wmask[\infty] \big) \right ]\Big) \\
    &\quad + \Big(\mathbb{E}_\infty \left [\lossmask \big (\wmask[\infty] \big) \right ]-\mathbb{E}_\infty \left [\lossmask (\vw_{\mathrm{mask}}^*  ) \right ] \Big)\\
    &\quad +\Big( \mathbb{E}_\infty \left [\lossmask\big(\vw_{\mathrm{mask}}^*\big) \right ] - \mathcal{L}_\infty^{\mathrm{mask}}\big(\vw_{\mathrm{mask}}^*\big) \Big)\\
    &\leq \Big( \mathcal{L}_\infty^{\mathrm{mask}}\big(\wmask[\infty]\big)-\mathbb{E}_\infty[\lossmask\big(\wmask[\infty]\big)]\Big)
    +\Big( \mathbb{E}_\infty[\lossmask\big(\vw_{\mathrm{mask}}^*\big)] - \mathcal{L}_\infty^{\mathrm{mask}}\big(\vw_{\mathrm{mask}}^*\big) \Big).
\end{align*}
For each $\vw \in \R^d$ with $\lVert \vw \rVert \leq R$, 
\begin{align*}
    &\quad \left|\mathcal{L}_\infty^{\mathrm{mask}}(\vw)-\mathbb{E}_\infty[\lossmask(\vw)] \right| \\
    &\leq \frac{1}{n} \left|l(\vw^\top \vmu) \right| + \frac{1}{n} \left | \sum_{k=1}^p \left(a_k l\left( \vw^\top \vmu^{(k)}\right) + b_k l \left( - \vw^\top \vmu^{(k)}  \right) + c_k l\left( \vw^\top \vmu \right)  \right) \right|\\
    &\leq \frac{1}{n}\left( l(-R \lvert \vmu \rVert) + \sum_{k=1}^p \left(a_k l\left(-R\left \lVert \vmu^{(k)} \right \rVert \right) + b_k l\left(-R\left \lVert \vmu^{(k)}\right \rVert\right) + c_k l\left( -R \lVert \vmu \rVert\right)  \right)\right)\\
    &= \frac{l(-R\lVert \vmu \rVert)}{n} \left(1 + \sum_{k=1}^p (a_k + b_k + c_k) \right)\\
    &= \frac{2l(-R\lVert \vmu \rVert) }{n}.
\end{align*}
The last two inequalities are due to $\left \lVert \vmu^{(k)}\right \rVert = \lVert \vmu \rVert$ and definition of $a_k,b_k,c_k$'s. Hence, we have
\begin{equation}\label{eqn:norm_converge}
    \frac{\alpha}{2}\lVert \wmask[\infty] - \vw_{\mathrm{mask}}^* \rVert^2 \leq \frac{4l(-R\lVert \vmu \rVert) }{n}.
\end{equation}
From triangular inequality, \Eqref{eqn:mask_uniform_converge}, and \Eqref{eqn:norm_converge}, we have
\begin{align*}
    \lVert \vw_{\mathrm{mask}}^*\rVert &\leq \lVert \wmask \rVert + \lVert \wmask[\infty] - \wmask\rVert + \lVert \vw_{\mathrm{mask}}^*-\wmask[\infty]\rVert\\
    &\leq  \lVert \wmask \rVert + \left(\frac{8l(-R\lVert \vmu \rVert)}{\alpha n}\right)^{1/2} + \left[\frac{4R}{\alpha \kappa^{1/2}} \left( \lVert \mSigma \rVert^{1/2} + \sum_{k=1}^p (a_k+b_k+c_k) \left \lVert \mSigma^{(k)} \right\rVert^{1/2} \right)\right]^{1/2}.
\end{align*}
Thus, if 
\begin{equation} \label{eqn:large_kappa}
n \geq \frac{128l(-R\lVert \vmu \rVert)}{\alpha\lVert  \vw_{\mathrm{mask}}^* \rVert^2} ,\kappa \geq \left(\frac{64R \left( \lVert \mSigma \rVert^{1/2} + \sum_{k=1}^p (a_k+b_k+c_k) \left \lVert \mSigma^{(k)} \right\rVert^{1/2} \right)}{\alpha \lVert  \vw_{\mathrm{mask}}^* \rVert^2}\right)^2,
\end{equation}
then we have $\lVert \wmask \rVert \geq \frac{\lVert \vw_{\mathrm{mask}}^* \rVert}{2}$.
Notice that the lower bounds in \Eqref{eqn:large_kappa} are numerical constants independent of $\kappa$.

\paragraph{Step 5: Sample Complexity for Directional Convergence} \quad

Assume $n$ and $\kappa$ is large enough so that satisfies \Eqref{eqn:large_kappa} and $\frac{\lVert \vw_{\mathrm{mask}}^*\rVert }{2} \epsilon \leq \lVert \wmask \rVert \epsilon < \lVert \wmask \rVert < \alpha_\kappa^{-1/2}$ We also assume the unique existence of $\hat{\vw}_{\mathrm{mask},S}^*$ which occurs almost surely. By Lemma~\ref{lemma:minimizer_dependent}, if
\begin{equation*}
    n \geq  \frac{16 C_1'M_\kappa}{\alpha_\kappa^2 \lVert \vw_{\mathrm{mask}}^* \rVert^4 \epsilon^4} \log \left( \frac{3}{\delta} \max \left\{ 1, \left( \frac{8C_2'd^{1/2}RL_\kappa}{\alpha_\kappa \lVert \vw_{\mathrm{mask}} ^* \rVert \epsilon^2} \right)^d  \right\} \right) = \frac{\Theta(1)}{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right ),
\end{equation*}
then we have $\lVert\wmask - \hat{\vw}_{\mathrm{mask},S}^* \rVert \leq  \frac{\lVert \vw_{\mathrm{mask}}^*\rVert }{2}  \epsilon \leq \lVert \wmask \rVert \epsilon$ with probability at least $1-\delta$.
Furthermore, if $\lVert \wmask - \hat{\vw}_{\mathrm{mask},S}^* \rVert \leq  \lVert \wmask \rVert \epsilon$, then $\hat{\vw}_{\mathrm{mask},S}^*$ belongs to interior of $\mathcal{C}$ therefore, $\hat{\vw}_{\mathrm{mask},S}^*$ is a minimizer of $\lossmask(\vw)$ over the entire $\mathbb{R}^d$. Also, we have
\begin{align*}
    \cosim ( \hat{\vw}_{\mathrm{mask},S}^*,\wmask)&=\bigg (1- \sin^2\Big(\angle\big(\hat{\vw}_{\mathrm{mask},S}^*, \wmask\big)\Big)\bigg)^{1/2}\\
    &\geq 1- \sin\Big(\angle\big(\hat{\vw}_{\mathrm{mask},S}^*, \wmask\big)\Big)\\
    &\geq 1- \epsilon.
\end{align*}
Hence, we conclude that if $n = \frac{\Omega(1)}{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right )$, then with probability at least $1-\delta$,  the masking based Mixup loss $\lossmask(\vw)$ has a unique minimizer $\hat{\vw}_{\mathrm{mask},S}^*$ and $\cosim ( \hat{\vw}_{\mathrm{mask},S}^*, \wmask)\geq 1- \epsilon$. \hfill $\square$