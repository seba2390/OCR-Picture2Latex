\subsection{Proof of Theorem~\ref{thm:expected_loss_mix}}\label{proof:expected_loss_mix}
We first prove the existence and uniqueness of a minimizer of $\mathbb{E}_\kappa[\lossmix(\vw)]$ and characterize its direction in the next part.
\paragraph{Step 1: Existence and Uniqueness of a Minimizer of $\mathbb{E}_\kappa[\lossmix(\vw)]$} \quad

Since $(\tilde{\vx}_{i,i}, \tilde{y}_{i,i}) = (\vx_i, y_i)$ for each $i \in [n]$ and $l(\cdot)$ is non-negative, for each $\vw \in \mathbb{R}^d$, we have
\begin{equation*}
    \mathbb{E}_\kappa[\lossmix(\vw)]  \geq \mathbb{E}_\kappa \left[ \frac{1}{n^2} \sum_{i=1}^n y_i l(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)\right] = \frac{1}{n} \mathbb{E}_\kappa[\loss(\vw)].
\end{equation*}
As we discussed in \Eqref{eqn:ball}, if $\lVert \vw \rVert \geq 2 n m_\kappa^{-1}$, we have
\begin{align*}
\mathbb{E}_\kappa[\lossmix (\vw)] &\geq \frac{1}{n} \mathbb{E}_\kappa[\loss (\vw)] \geq \frac{1}{n} \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma)} \left[- \frac{1}{2} \vw^\top \rvx \cdot \vone_{\vw^\top \rvx <0} \right]\\
&= \frac{1}{n} \lVert \vw \rVert \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1}\mSigma)}\left[- \frac{1}{2} \cdot \frac{\vw^\top \rvx}{\lVert \vw \rVert} \cdot \vone_{\frac{\vw^\top \rvx}{\lVert \vw \rVert}<0} \right]\\
&\geq \frac{m_\kappa}{2n} \lVert \vw \rVert > 1 \geq \log 2 \\
&= \mathbb{E}_\kappa [\lossmix (\vzero)].
\end{align*}
Therefore, a minimizer of $\mathbb{E}_\kappa [ \lossmix (\vw) ]$ necessarily contained in a compact set $\{ \vw \in \mathbb{R}^d :  \lVert \vw \rVert \leq 2 n m_\kappa^{-1} \}$. Since $\mathbb{E}_\kappa [ \lossmix (\vw) ]$ is a continuous function of $\vw$, there must exist a minimizer. The existence part is hence proved.

To show uniqueness, we prove strict convexity of $\mathbb{E}_\kappa[\loss (\vw)]$. From strict convexity of $l(\cdot)$, for any $t \in [0,1]$, $\vw_1, \vw_2 \in \mathbb{R}^d$ with $\vw_1 \neq \vw_2$, and $y \in [0,1]$, we have
\begin{align*}
    &\quad t[yl(\vw_1^\top \vx) + (1-y) l(-\vw_1^\top \vx)] + (1-t) [yl(\vw_2^\top \vx) + (1-y) l(-\vw_2^\top \vx)]\\
    &> y l((t\vw_1+(1-t)\vw_2)^\top \vx) + (1-y) l(-(t\vw_1+(1-t)\vw_2)^\top \vx),
\end{align*}
and any $\vx \in \mathbb{R}^d$ except for a Lebesgue measure zero set (i.e., the set of points $\vx \in \R^d$ satisfying  $\vw_1^\top \vx = \vw_2^\top \vx$).

By taking expectations, we have 
\begin{equation*}
    t \mathbb{E}_\kappa[\lossmix(\vw_1)] + (1-t) \mathbb{E}_\kappa[\lossmix(\vw_2)] >  \mathbb{E}_\kappa[\lossmix(t\vw_1+(1-t)\vw_2)].
\end{equation*}
We conclude $\mathbb{E}_\kappa[\lossmix(\vw)]$ is strictly convex.
Since a strictly convex function has at most one minimizer, we conclude that $\mathbb{E}_\kappa[\lossmix(\vw)]$ has a unique minimizer for any given $\kappa \in (0,\infty)$. 

\paragraph{Step 2: Direction of the Unique Minimizer of $\mathbb{E}_\kappa [\lossmix(\vw)]$} \mbox{}

We express expected losses $\mathbb{E}_\kappa[\lossmix(\vw)]$ as the form
\begin{equation}\label{eqn:mix_form}
 \mathbb{E}_{Z \sim N(0,1)}\left[ \sum_{i=1}^k a_i l\left( b_i \kappa^{-1/2} \left(\vw^\top \mSigma \vw \right)^{1/2} Z + c_i \vw^\top \vmu \right) \right],
\end{equation}
where $a_i,b_i,c_i$'s are real valued random variables depending on $\Lambda$ and $a_i, b_i$'s are positive.
Note that $(\tilde{\vx}_{i,i}, \tilde{y}_{i,i}) = (\vx_i,y_i)$ for each $i\in [n]$ and for each $i,j \in [n]$ with $i \neq j$,
\begin{align}\label{eqn:mix_1}
    \tilde{y}_{i,j} \mid \lambda_{i,j} = 
    \begin{cases}
    1 &\text{with probability } \frac{1}{4},\\
    \lambda_{i,j} &\text{with probability } \frac{1}{4},\\
    1-\lambda_{i,j} &\text{with probability }\frac{1}{4},\\
    0 &\text{with probability } \frac{1}{4}.
    \end{cases}
\end{align}
and the conditional probability distribution of the random variable $\tilde{\vx}_{i,j}$ given $\tilde{y}_{i,j}$ and $\lambda_{i,j}$ can be formulated as the following four cases, depending on the outcome of $\tilde{y}_{i,j}$:
\begin{align}\label{eqn:mix_2}
    \begin{cases}
    \tilde{\vx}_{i,j} \mid \tilde{y}_{i,j}=1, \lambda_{i,j} &\sim  N \left ( \vmu, (g(\lambda_{i,j})^2 +(1-g(\lambda_{i,j}))^2) \kappa^{-1}\mSigma \right ),\\
    \tilde{\vx}_{i,j} \mid \tilde{y}_{i,j}=\lambda_{i,j}, \lambda_{i,j} &\sim  N \left ( (2g(\lambda_{i,j})-1)\vmu, (g(\lambda_{i,j})^2 + (1-g(\lambda_{i,j}))^2) \kappa^{-1} \mSigma \right ),\\
    \tilde{\vx}_{i,j} \mid \tilde{y}_{i,j}=1-\lambda_{i,j}, \lambda_{i,j} &\sim  N \left ( -(2g(\lambda_{i,j})-1)\vmu, (g(\lambda_{i,j})^2 + (1-g(\lambda_{i,j}))^2) \kappa^{-1} \mSigma \right ),\\
    \tilde{\vx}_{i,j} \mid \tilde{y}_{i,j}=0, \lambda_{i,j} &\sim  N \left ( -\vmu, (g(\lambda_{i,j})^2 +(1-g(\lambda_{i,j}))^2) \kappa^{-1} \mSigma \right ).
    \end{cases}
\end{align}
For simplicity, we denote $\mSigma_\lambda = (g(\lambda)^2 + (1-g(\lambda))^2) \mSigma$ for each $\lambda \in [0,1]$. Then, we have 
\begin{align}\label{eqn:mix}
    &\quad \mathbb{E}_\kappa[\lossmix(\vw)] \nonumber \\
    &= \frac{1}{n^2} \sum_{i,j=1}^n\mathbb{E}_\kappa[\tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j})]\nonumber \\
    &= \frac{1}{n^2} \left ( \sum_{\overset{i,j=1}{i \neq j}}^n\mathbb{E}_\kappa \left [\tilde{y}_{i,j} l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j}) \right ] + \sum_{i=1}^n \mathbb{E}_\kappa \left [y_i l(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)\right ]\right)\nonumber\\
    &= \frac{n-1}{2n} \mathbb{E}_{Z \sim N(0,1), \lambda \sim \Lambda } \left [l \left ( (\kappa^{-1} \vw^\top \mSigma_\lambda \vw)^{1/2} Z + \vw^\top \vmu \right ) \right ]\nonumber \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{Z \sim N(0,1), \lambda \sim \Lambda } \left [\lambda l \left ( (\kappa^{-1} \vw^\top \mSigma_\lambda \vw)^{1/2} Z + (2g(\lambda)-1) \vw^\top \vmu \right ) \right ]\nonumber \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{Z \sim N(0,1), \lambda \sim \Lambda } \left [(1-\lambda) l \left ( (\kappa^{-1} \vw^\top \mSigma_\lambda \vw)^{1/2} Z -(2g(\lambda)-1) \vw^\top \vmu \right) \right ]\nonumber \\
    &\quad+ \frac{1}{n} \mathbb{E}_{Z \sim N(0,1) }\left[ l \left ( (\kappa^{-1} \vw^\top \mSigma \vw)^{1/2} Z + \vw^\top \vmu \right )\right].
\end{align}
 This is the form in \Eqref{eqn:mix_form}. Let $C_\kappa = \wmix^\top \vmu$, then by Lemma~\ref{lemma:logistic}, $\wmix$ have to be a solution of the problem $\min_{\vw^\top \vmu = C_\kappa} \frac{1}{2}\vw^\top \mSigma \vw$, and by Lemma~\ref{lemma:opt}, there exists $c_{\mathrm{mix}, n, \kappa}$ such that  $\wmix:= c_{\mathrm{mix}, n, \kappa} \mSigma^{-1} \vmu$ be the unique minimizer of $\mathbb{E}_\kappa[\lossmix(\vw)]$. 
 
 The only remaining part is showing $c_{\mathrm{mix},n,\kappa}>0$. For simplicity, we will omit $\kappa$ and $n$ in $\wmix$. If $c_{\mathrm{mix}, n, \kappa}<0$, 
\begin{align*}
    &\quad \mathbb{E}_\kappa \left[\lossmix\left(\vw_{\mathrm{mix}}^*\right)\right]\\
    &= \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} } \left[l\left( \left (\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^* \right)^{1/2}  Z + c_{\mathrm{mix},n,\kappa} \vmu^\top \mSigma^{-1} \mu \right)\right] }_\text{(a)}\\
    &\quad+ \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} }\left[\lambda l\left( \left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^*\right)^{1/2} Z + (2g(\lambda)-1) c_{\mathrm{mix},n,\kappa} \vmu^\top \mSigma^{-1} \vmu\right)\right]}_\text{(b)} \\
    &\quad+ \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} }\left[(1-\lambda) l\left( \left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^*\right)^{1/2} Z -(2g(\lambda)-1) c_{\mathrm{mix}, n, \kappa} \vmu^\top \mSigma^{-1} \vmu\right)\right]}_\text{(c)} \\
    &\quad+ \frac{1}{n} \underbrace{\mathbb{E}_{Z \sim N(0,1) }\left[ l\left( \left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma \vw_{\mathrm{mix}}^*\right)^{1/2} Z + c_{\mathrm{mix}, n, \kappa} \vmu^\top \mSigma^{-1} \vmu \right) \right]}_\text{(d)}\\
    &> \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} } \left[l\left( \left (\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^* \right)^{1/2}  Z - c_{\mathrm{mix},n,\kappa} \vmu^\top \mSigma^{-1} \vmu \right)\right]}_\text{(a)'} \\
    &\quad+ \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} }\left[\lambda l\left( \left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^*\right)^{1/2} Z - (2g(\lambda)-1) c_{\mathrm{mix},n,\kappa} \vmu^\top \mSigma^{-1} \vmu\right)\right]}_\text{(b)'} \\
    &\quad+ \frac{n-1}{2n} \underbrace{\mathbb{E}_{\overset{Z \sim N(0,1)}{\lambda \sim \Lambda} }\left[(1-\lambda) l\left(\left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma_\lambda \vw_{\mathrm{mix}}^* \right)^{1/2} Z +(2g(\lambda)-1) c_{\mathrm{mix}, n, \kappa} \vmu^\top \mSigma^{-1} \vmu\right)\right]}_\text{(c)'} \\
    &\quad+ \frac{1}{n} \underbrace{\mathbb{E}_{Z \sim N(0,1) }\left[ l\left( \left(\kappa^{-1} {\vw_{\mathrm{mix}}^*}^\top \mSigma \vw_{\mathrm{mix}}^*\right)^{1/2} Z - c_{\mathrm{mix}, n, \kappa} \vmu^\top \mSigma^{-1} \vmu \right) \right]}_\text{(d)'}\\
    &= \mathbb{E}_\kappa[\lossmix(-\vw_{\mathrm{mix}}^*)].
\end{align*}
The inequality holds by comparing (a), (b), (c), and (d) with (a)', (b)', (c)', and (d)' respectively. This contradicts the assumption that $\vw_{\mathrm{mix}}^*$ is a unique minimizer of $\mathbb{E}_\kappa[\lossmix(\vw)]$, letting us to conclude $c_{\mathrm{mix},n, \kappa} \geq 0$. Showing $c_{\mathrm{mix},n, \kappa}$ is strictly positive will be handled in the proof of Lemma~\ref{lemma:mixup_norm} which can be found in Appendix~\ref{proof:lemma:mixup_norm}. \hfill $\square$