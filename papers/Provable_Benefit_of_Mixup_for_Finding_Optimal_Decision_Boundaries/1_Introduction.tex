\section{Introduction}
Mixup~\citep{zhang2018mixup} is a modern technique that augments training data with a random convex combination of a pair of training points and labels. \citet{zhang2018mixup} empirically show that this simple technique has various benefits, such as better generalization, robustness to label corruption and adversarial attack, and stabilization of generative adversarial network training. Inspired by the success of Mixup, several variants of Mixup have appeared in the literature; e.g., Manifold Mixup~\citep{verma2019manifold}, Cutmix~\citep{yun2019cutmix}, Puzzle Mix~\citep{kim2020puzzle}, and Co-Mixup~\citep{kim2021co}. The success of Mixup-style training schemes is not only limited to improved generalization performance in supervised learning; they are known to be helpful in other aspects including model calibration~\citep{thulasidasan2019mixup}, semi-supervised learning~\citep{berthelot2019mixmatch, sohn2020fixmatch}, contrastive learning~\citep{kalantidis2020hard, verma2021towards}, and natural language processing~\citep{guo2019augmenting, sun2020mixup}.

Although Mixup and its variants demonstrate surprising empirical benefits, a concrete theoretical understanding of such benefits still remains mysterious. As a result, a recent line of research~\citep{carratino2020mixup, zhang2020does, zhang2022and, chidambaram2021towards, chidambaram2022provably, parkunified} trying to theoretically understand Mixup and its variants has appeared in the literature. Most results, including this paper, compare the training procedure with Mixup against solving the vanilla empirical risk minimization (ERM) problem without any augmentation. For the remainder of this paper, we refer to the training loss without augmentation as \emph{ERM loss}, and the training loss with Mixup as \emph{Mixup loss}.

The parallel works of \citet{carratino2020mixup} and \citet{zhang2020does} represent the Mixup loss as the ERM loss equipped with additive data-dependent regularizers which penalize the gradient and Hessian of a model with respect to data. Using these regularizers, \citet{zhang2020does} show that Mixup training can yield smaller Rademacher complexity which allows for a smaller uniform generalization bound. Also, \citet{parkunified} extend these results to several Mixup variants such as Cutmix~\cite{yun2019cutmix}. However, the benefits of Mixup-style schemes from the Rademacher complexity view shown by \citet{zhang2020does} and \citet{parkunified} stand out only if the intrinsic dimension of data is small.

\citet{chidambaram2021towards} try to understand Mixup by investigating when Mixup training minimizes the ERM loss. The authors show failure cases of Mixup and also sufficient conditions for its success. \citet{chidambaram2021towards} also show that ERM loss and Mixup loss can have the same optimal classifier by considering linear model and special types of data distribution. Detailed comparisons to our work are provided in Section~\ref{section:setting}.

Another recent work~\citep{chidambaram2022provably} shows that Midpoint Mixup~\citep{guo2021midpoint} can outperform ERM training in terms of feature learning performance in a multi-view data framework proposed by \citet{allen2020towards}. However, their analysis is limited to an extreme case of Mixup  using only the midpoint of two data points. 

\subsection{Our Contributions}

In this paper, we study the problem of finding optimal decision boundaries in a binary linear classification problem with logistic loss (i.e., logistic regression). We consider a data generating distribution whose positive and negative examples come from two symmetric Gaussian distributions. The two distributions have the same covariance matrices inversely proportional to a \emph{separability constant} $\kappa$, which controls the degree to which the two classes are separable.

Our data generating distribution has the advantage that the \emph{Bayes optimal classifier} (i.e., the classifier with the best test accuracy) is given as a closed-form linear classifier. This motivates us to study how well the optimal classifier in terms of training loss is aligned with the Bayes optimal classifier, and how many data points are required to make sure that the two classifiers are close enough. Therefore, we focus on the sample complexity of vanilla ERM training and Mixup-style training for achieving close-to-one cosine similarity between the two classifiers.

Our results demonstrate that Mixup provably requires a much smaller number of samples to achieve the same cosine similarity compared to vanilla ERM training. Our contributions can be summarized as the following:
\begin{itemize}

    \item In Section~\ref{section:ERM}, we investigate the sample complexity of (vanilla) ERM under our data distribution. 
    Theorem~\ref{thm:expected_loss_ERM} shows that the expected value of ERM training loss has a unique optimal solution aligned with the Bayes optimal classifier. 
    We then prove that the sample complexity for making the ERM loss optimum closely aligned with the Bayes optimal classifier grows \emph{exponentially} with the separability constant $\kappa$; we show that the exponential growth is both sufficient (Theorem~\ref{thm:ERM_sufficient}) and necessary (Theorem~\ref{thm:ERM_necessary}). Interestingly, these results demonstrate that \emph{ERM suffers the curse of separability}, where the sample complexity increases as the data distribution becomes more separable.
    
    \item In Section~\ref{section:mix}, we study a unified class of Mixup-style training scheme that includes Mixup~\citep{zhang2018mixup} with various choices of hyperparameters. Theorem~\ref{thm:expected_loss_mix} shows that the expected value of Mixup loss has a unique optimal solution still aligned with the Bayes optimal classifier, which indicates that Mixup-style augmentations do not ``distort'' the training loss in a misleading way, at least in our setting. 
    In Theorem~\ref{thm:mixup_convergence}, we show that the sample complexity for getting a near-Bayes-optimal Mixup loss solution grows only quadratically in $\kappa$. This result indicates that \emph{Mixup provably mitigates the curse of separability}.
    
    \item  In Section~\ref{section:mask}, we analyze how recent masking-based variants of Mixup such as CutMix~\citep{yun2019cutmix} behave in our setting. Unfortunately, Theorem~\ref{thm:mask_minimizer} shows that masking-based augmentation can \emph{distort} the training loss and the expected value of masking-based Mixup loss can have optimal solutions far away from the Bayes optimal classifier. Ironically, Theorem~\ref{thm:mask_convergence} indicates the sample complexity for approaching such a ``wrong'' minimizer does not grow with separability. These results show that, at least in our setting, masking-based techniques have small sample complexity but may not converge to the Bayes optimal classifier.
\end{itemize}