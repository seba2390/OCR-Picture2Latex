\section{ERM Suffers the Curse of Separability}\label{section:ERM}
In this section, we investigate a solution of ERM loss in \Eqref{eq:1}.
The ERM loss function $\loss(\vw)$ is a stochastic function that depends on the random samples in the training set $S = \{(\vx_i, y_i)\}_{i=1}^n \overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$. 
We will first characterize the minimizer of the expected value $\mathbb{E}_\kappa[\loss(\vw)]$ of the ERM loss\footnote{The expected ERM loss is equal to the population loss, which is independent of $n$. However, for Mixup loss, its expected value becomes \emph{dependent} on $n$, as we will see later.} and show that the unique optimum of the expected ERM loss has the same direction as the Bayes optimal classifier. Next, we study the sample complexity for the ERM loss optimum to align closely enough to the Bayes optimal classifier and conclude that ERM without Mixup suffers the \emph{curse of separability}.

Our first theorem below analyzes the optimum of expected ERM loss $\mathbb{E}_\kappa[\loss(\vw)]$.
\begin{theorem}\label{thm:expected_loss_ERM}
For any $\kappa \in (0,\infty)$, the expectation of ERM loss $\mathbb{E}_\kappa[\loss(\vw)]$ has a unique minimizer $\w$. In addition, its direction is the same as the Bayes optimal solution $\mSigma^{-1} \vmu$.
\end{theorem}
\vspace*{-10pt}
\begin{proof}[Proof Sketch]
We will only sketch main proof ideas and full proof can be found in Appendix~\ref{proof:expected_loss_ERM}. We can rewrite the expected ERM loss as
\begin{equation*}
\mathbb{E}_\kappa [\loss (\vw)] = \mathbb{E}\left[ l\left( \kappa^{-1/2} \left(\vw^\top \mSigma \vw\right)^{1/2} Z + \vw^\top \vmu \right)\right],
\end{equation*}
where $Z \sim N(0,1)$.
The following Lemma~\ref{lemma:logistic} implies that for a fixed value of $\vw^\top\vmu$, a smaller value of $\vw^\top \mSigma \vw$ induces a smaller $\mathbb{E}_\kappa[\loss(\vw)]$.
\begin{restatable}{lemma}{logistic}\label{lemma:logistic}
Let $X_1 \sim N(m, \sigma_1^2), X_2 \sim N(m, \sigma_2^2)$, $m \in \mathbb{R}$ and $\sigma_1 > \sigma_2>0$. Then, $\mathbb{E}[l(X_1)] > \mathbb{E}[l(X_2)]$.
\end{restatable}
\vspace*{-10pt}
Lemma~\ref{lemma:opt} below concludes that the unique minimizer should be parallel to $\mSigma^{-1} \vmu$.
\begin{restatable}{lemma}{opt}\label{lemma:opt}
For any constant $C$, a unique solution of $\min_{\vw \in \mathbb{R}^d, \vw^\top \vmu = C} \frac{1}{2} \vw^\top \mSigma \vw$ is a rescaling of $\mSigma^{-1} \vmu$.
\end{restatable}
\vspace*{-10pt}
Any remaining details can be found in Appendix~\ref{proof:expected_loss_ERM}.
\end{proof}
\vspace*{-10pt}
Our Theorem~\ref{thm:expected_loss_ERM} implies that the minimizer $\w$ of expected ERM loss $\mathbb{E}_\kappa [\loss (\vw)]$ induce the Bayes optimal classifier. 
This may look obvious to some readers because $\mathbb{E}_\kappa [\loss (\vw)]$ is in fact equal to $\mathbb{E}_\kappa[y l(\vw^\top \vx) + (1-y) l(-\vw^\top \vx)]$, the population loss. However, notice that the optimal classifier in terms of the population loss is dependent on the loss $l$, and hence is not always equal to the Bayes optimal classifier.
Also, we will see theorems similar to Theorem~\ref{thm:expected_loss_ERM} in later sections; in particular, Theorem~\ref{thm:mask_minimizer} reveals how masking-based augmentation can \emph{distort} the optimal classifier of the corresponding expected training loss; thus, characterizing such optima is of importance.

Notice that the minimizer characterized in Theorem~\ref{thm:expected_loss_ERM} is for \emph{expected} ERM loss $\mathbb{E}_\kappa[\loss(\vw)]$. Since $\mathcal{D}_\kappa$ is not known to us and we only observe training data $ \{ (\vx_i, y_i)\}_{i=1}^n \overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa $, we can only hope to get close to the minimizer by optimizing the training loss $\loss(\vw)$, 
and sufficiently many data samples are required to obtain a ``close enough'' one.
Thus, a natural question arises:
\begin{center}
    \vspace{-5pt}
    \emph{How many data points are required to find \\
    a near Bayes optimal classifier using the ERM loss?}\\
\end{center}
We present two theorems that answer the question above. The first one (Theorem~\ref{thm:ERM_sufficient}) shows that the number of samples growing exponentially with $\kappa^2$ is \emph{sufficient}, and the next one (Theorem~\ref{thm:ERM_necessary}) proves that this exponential growth with $\kappa$ is \iffalse in fact \fi \emph{necessary}.
In other words, as the data distribution becomes more separable, the sample complexity for getting a Bayes optimal classifier grows exponentially. 

\begin{theorem}\label{thm:ERM_sufficient}
Let $\epsilon, \delta \in (0,1)$. Suppose the training set $S = \{(\vx_i, y_i)\}_{i=1}^n\overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ with large enough $\kappa \in (0,\infty)$, and
\begin{equation*}
n = \frac{\exp(\Omega(\kappa^2)) }{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right ).
\end{equation*}
Then, with probability at least $1-\delta$, the unique minimizer $\hat{\vw}_S^*$ of $\loss(\vw)$ exists and  $\cosim (\hat{\vw}_S^*, \mSigma^{-1}\vmu) \geq 1- \epsilon$.
\end{theorem}
\vspace*{-10pt}
\begin{proof}[Proof Sketch]
A useful tool for proving Theorem~\ref{thm:ERM_sufficient} is Lemma~\ref{lemma:minimizer_independent} inspired by the proof techniques used in \citet{dai2000convergence} and \citet{shapiro2008stochastic}.
\begin{lemma}\label{lemma:minimizer_independent}
Let $f(\cdot, \cdot) : \mathbb{R}^k \times \mathbb{R}^m \rightarrow \mathbb{R}$ be a real-valued function. Define functions $F_N :  \mathbb{R}^k \rightarrow \mathbb{R}$ and $\hat{F}_N : \mathbb{R}^k \rightarrow \mathbb{R}$ as
\vspace*{-5pt}
\begin{equation*}
    F (\vtheta) = \mathbb{E}_{\veta \sim \mathcal{P}}[f(\vtheta, \veta)], 
    ~\hat{F}_N  (\vtheta) = \frac{1}{N} \sum\nolimits_{i=1}^N f(\vtheta, \veta_i),
\end{equation*}
where $\mathcal{P}$ is a probability distribution on $\mathbb{R}^m$ and $\{\veta_i\}_{i=1}^N \overset{\mathrm{i.i.d.}}{\sim} \mathcal{P}$. Let $\mathcal{C}$ be a nonempty compact subset of $\mathbb{R}^k$ with diameter $D$. Suppose the following assumptions hold:
\vspace*{-5pt}
\begin{itemize}[leftmargin=3.5mm]
\item The functions $F$ and $\hat{F}_N$ have unique minimizers on $\mathcal{C}$ named $\vtheta^*$ and $\hat{\vtheta}_N^*$, respectively.
\item The function $F$ is $\alpha$-strongly convex on $\mathcal{C}$ ($\alpha>0$).
\item For any $\vtheta \in \mathcal{C}$, $\mathbb{E}_{\veta \sim \mathcal{P}}\left [e^{|f(\vtheta, \veta)- \mathbb{E}_{\veta \sim \mathcal{P}}[f(\vtheta, \veta)] |} \right]<M$.
\item There exists a function $g(\cdot): \R^k \rightarrow \R$ such that for any $\vtheta \in \mathcal{C}$ and $\veta \in \R^k$, it holds that $\lVert \nabla_\vtheta f(\vtheta,\veta) \rVert \leq g(\veta)$ and $\lVert \nabla_\vtheta \mathbb{E}_{\veta \sim \mathcal{P}}[f(\vtheta, \veta)]\rVert \leq \mathbb{E}_{\veta \sim \mathcal{P}}[g(\veta)]$. In addition, $\mathbb{E}_{\veta \sim \mathcal{P}}\left[ e^{g(\veta)}\right]< L$.
\end{itemize}
\vspace*{-5pt}
For each $0<\epsilon< \alpha^{-1/2}$, we have $\lVert \hat{\vtheta}_N^* - \vtheta \rVert < \epsilon$ with probability at least $1- \delta$  if $N$ is greater than
\begin{equation*}
\frac{C_1 M}{ \alpha^2 \epsilon^4} \log\left(\frac{3}{\delta} \max \left \{1, \left(\frac{C_2 k^{1/2} D L}{\alpha \epsilon^2 }\right)^k \right\} \right),
\end{equation*}
where $C_1, C_2>0$ are universal constants.
\end{lemma}

We use Lemma~\ref{lemma:minimizer_independent} by considering $\{ \veta_i\}_{i=1}^n \overset{\mathrm{i.i.d.}}{\sim} \mathcal{P}$ as our training dataset $S = \{(\vx_i,y_i)\}_{i=1}^n \overset{\mathrm{i.i.d.}}{\sim} \mathcal{D}_\kappa$, $\vtheta$ as weight vector $\vw$, and $f(\vtheta, \veta_i)$ as $(\vw, (\vx_i,y_i)) \mapsto y_i l(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)$.
When we apply Lemma~\ref{lemma:minimizer_independent}, several quantities are sensitive to $\lVert \w \rVert$, the $\ell_2$ norm of $\w$ defined in Theorem~\ref{thm:expected_loss_ERM}, which we characterize in the following lemma.
\vspace{-5pt}
\begin{lemma}\label{lemma:ERM_norm} The unique minimizer $\w$ of expected ERM loss $\mathbb{E}_\kappa [\loss (\vw)]$ satisfies
\begin{equation*}
    \lVert \w \rVert = \Theta (\kappa).
\end{equation*}
\end{lemma}
\vspace*{-10pt}
Full proof of Theorem~\ref{thm:ERM_sufficient} appears in Appendix~\ref{proof:ERM_sufficient}.
\end{proof}
\vspace{-10pt}
Our sufficient sample complexity in Theorem~\ref{thm:ERM_sufficient} grows \emph{exponentially} with $\kappa$. The sufficient number of data points for the minimizer $\hat{\vw}_S^*$ of the ERM loss $\loss(\vw)$ to be close to the Bayes optimal classifier becomes exponentially larger for more well-separable data distributions. 

One may think that this exponential dependency may be just an artifact of our analysis and the exponential growth in $\kappa$ is in fact avoidable.
As an answer to this question, we introduce Theorem~\ref{thm:ERM_necessary} indicating that the exponential growth of sample complexity is \emph{inevitable}, dashing hopes for sub-exponential sample complexity bounds.
\begin{theorem}\label{thm:ERM_necessary}
Assume $S = \{(\vx_i, y_i)\}_{i=1}^n\overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ with large enough $\kappa \in (0, \infty)$. If $n = \exp (\mathcal{O}(\kappa))$, then $\{(\vx_i, y_i) \}_{i=1}^n$ is linearly separable and $\cosim(\bar{\vw}_S, \mSigma^{-1}\vmu) < \frac{1+\cosim(\vmu,\mSigma^{-1}\vmu)}{2}$ with probability at least 0.99, where $\bar{\vw}_S$ is the $\ell_2$ max margin solution: 
\begin{equation}
\label{eq:2}
\begin{aligned}
    \bar{\vw}_S = \argmin\nolimits_{\vw \in \mathbb{R}^d} &\quad\lVert \vw \rVert^2 \\
    \mathrm{subject~to }&\quad(2y_i-1)\vw^\top \vx_i \geq 1.
\end{aligned}
\end{equation}
\end{theorem}
\begin{proof}[Proof Sketch]
If $\kappa$ is large, then $(2y-1)\vx$ will be concentrated near $\vmu$ where $(\vx,y) \sim \mathcal{D}_\kappa$. Hence, if $n$ is not large enough, $(2y_i-1)\vx_i$'s will be located inside a small ball centered at $\vmu$, with high probability. Therefore, $\{(\vx_i, y_i)\}_{i=1}^n$ is likely to be linearly separable. From the KKT condition of the $\ell_2$ max margin problem (\Eqref{eq:2}), $\bar{\vw}_S$ is a homogeneous combination of $(2y_i-1)\vx_i$'s. It implies $\bar{\vw}_S$ is directionally close to $\vmu$, not $\mSigma^{-1} \vmu$. The detailed proof is in Appendix~\ref{proof:ERM_necessary}.
We note that the numbers $\frac{1+ \cosim(\vmu, \mSigma^{-1}\vmu)}{2}$ and $0.99$ in the statement are not strictly necessary; they can be replaced by any other feasible constants.
\end{proof}

Theorem~\ref{thm:ERM_necessary} states that if we do not have sufficiently many data points, then the training dataset becomes linearly separable: there exists a direction $\vw$ such that $(2y_i-1) \vw^\top \vx_i > 0$ for all $i \in [n]$. However, this in fact means that there exist infinitely many directions $\vw$ that classify the data perfectly. Then why do we care about the specific $\ell_2$ max margin classifier in Theorem~\ref{thm:ERM_necessary}? \citet{soudry2018implicit, ji2019implicit} study the implicit bias of gradient descent on a linear model with logistic loss and show that this algorithm converges in direction to the $\ell_2$ max margin classifier when training data is linearly separable. In other words, if we run gradient descent on $\loss(\vw)$, then the algorithm will return a linear classifier defined by the direction of $\bar{\vw}_S$. This is why we analyze the $\ell_2$ max margin solution.
\vspace{-5pt}
\paragraph{The Curse of Separability.}Theorem~\ref{thm:ERM_necessary} implies that without the number of samples exponentially growing with $\kappa$, the solution found by gradient descent can be far from the Bayes optimal classifier. Combining Theorems~\ref{thm:ERM_sufficient} and \ref{thm:ERM_necessary}, we have an interesting conclusion that even though it is easier to correctly classify training dataset when $\kappa$ is larger, finding the theoretically optimal model becomes much harder. We refer to this interesting phenomenon as the \emph{curse of separability}; without Mixup, ERM training suffers the curse of separability due to its sample complexity growing exponentially in $\kappa$.
\vspace{-5pt}
\paragraph{Intuitive Explanations.}
What causes this phenomenon? 
When the data distribution is well-separable, limited training data can result in many decision boundaries having high training accuracy. However, among these, there is only one optimal decision boundary in terms of test accuracy, which is difficult to locate due to the scarcity of data points near it; this causes the curse of separability. We believe that this intuition extends beyond our simple setup; see Section~\ref{section:exp}.