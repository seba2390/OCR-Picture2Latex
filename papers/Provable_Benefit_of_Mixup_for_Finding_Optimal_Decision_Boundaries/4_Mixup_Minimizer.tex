\vspace{-5pt}
\section{Mixup Provably Mitigates the Curse of Separability in ERM}\label{section:mix}
In this section, we study a unifying framework of Mixup-style data augmentation techniques and show that Mixup significantly alleviates the curse of separability. 
We will first define the unifying framework along with Mixup loss, study the location of the minimizer of expected Mixup loss, and then study the sample complexity for Mixup training to achieve near Bayes optimal classifier.

We start by defining the Mixup loss in the following framework. From this point on, we assume $n\geq 2$.
\begin{definition}[Mixup Loss] Mixup loss with training set $S = \{(\vx_i,y_i)\}_{i=1}^n$ is defined by
\begin{equation*}
 \lossmix(\vw) \!=\! \frac{1}{n^2}\! \sum_{i,j=1}^n \tilde{y}_{i,j} l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j}),
\end{equation*}
where
\begin{align*}
   \lambda_{i,j} &\overset{\mathrm{i.i.d}}{\sim} \Lambda,\\
   \tilde{\vx}_{i,j} &~= g(\lambda_{i,j}) \vx_i  + (1-g(\lambda_{i,j}))\vx_j, \\
   \tilde{y}_{i,j} &~= \lambda_{i,j} y_i  + (1-\lambda_{i,j})y_j.
\end{align*}
The probability distribution $\Lambda$ satisfies $\support(\Lambda) \subset [0,1]$ and $\mathbb{P}_{\lambda \sim \Lambda}\left [\lambda \notin \{ 0,1 \} \land g(\lambda) \neq \frac{1}{2}\right]>0$. Also, the function $g : [0,1] \rightarrow [0,1]$ satisfies $g(z)>\frac{1}{2}$ if and only if $z >\frac{1}{2}$.
\end{definition}
Our definition is a broad framework that covers the original Mixup by choosing $\Lambda$ as the Beta distribution and $g(\cdot)$ as the identity function.
Similar to ERM loss $\loss ( \vw)$, Mixup loss $\lossmix(\vw)$ is also a stochastic function depending on the training set $S$. Unlike ERM loss, the expectation of Mixup loss \emph{depends} on $n$, as can be checked in Appendix~\ref{proof:expected_loss_mix}. As we did in Section~\ref{section:ERM}, we first characterize the minimizer of expected Mixup loss $\mathbb{E}_\kappa [\lossmix (\vw)]$.
\begin{theorem}\label{thm:expected_loss_mix}
For each $\kappa \in (0,\infty)$ and $n \in \mathbb{N}$, the expectation of Mixup loss $\mathbb{E}_\kappa [\lossmix (\vw)]$ has a unique minimizer $\wmix$. In addition, its direction is the same as the Bayes optimal solution $\mSigma^{-1}\vmu$.
\vspace{-10pt}
\end{theorem}
\begin{proof}[Proof Sketch]
We can rewrite $\mathbb{E}_\kappa[ \lossmix (\vw)]$ as the form
\begin{equation*}
\mathbb{E} \left[ \sum_{i=1}^k a_i l \left( b_i\kappa^{-1/2} (\vw^\top \mSigma \vw)^{1/2} Z   + c_i \vw^\top \vmu \right)\right],
\end{equation*}
where $Z \sim N(0,1)$ and $a_i$, $b_i$, $c_i$'s are real-valued random variables depending on $\Lambda$; in particular, $a_i$, $b_i$'s are positive. Then, the same proof idea of Theorem~\ref{thm:expected_loss_ERM} works.
\vspace{-5pt}
\end{proof}

\paragraph{Mixup Does Not Distort Training Loss.}
Theorem~\ref{thm:expected_loss_mix} shows that the expected Mixup loss also has its unique minimizer pointing to the Bayes optimal direction. In other words, this theorem implies that the pair-wise mixing done in Mixup does not introduce any bias or distortion in the training loss, at least in our setting. This is one benefit that Mixup has compared to other masking-based augmentations, as we will see in Section~\ref{section:mask}.

In order to investigate the sample complexity for achieving a near Bayes optimal classifier when we train with Mixup loss, one could speculate that the same approach using Lemma~\ref{lemma:minimizer_independent} should work. However, this is not the case; analysis of the Mixup loss has to overcome a significant barrier because the mixed data points $\{(\tilde{\vx}_{i,j}, \tilde{y}_{i,j})\}_{i,j=1}^n$ are no longer independent of one another. To overcome this difficulty, we prove the following lemma in Appendix~\ref{proof:minimizer_dependent}, which could be of independent interest:
\begin{lemma}\label{lemma:minimizer_dependent}
Let $f(\cdot, \cdot) : \mathbb{R}^k \times \mathbb{R}^m \rightarrow \mathbb{R}$ be a real-valued function. Define functions $F_N :  \mathbb{R}^k \rightarrow \mathbb{R}$ and $\hat{F}_N : \mathbb{R}^k \rightarrow \mathbb{R}$ as
\begin{equation*}
    F_N(\vtheta) = \frac{1}{N^2}\!\! \sum_{i,j\in [N]}\!\! f(\vtheta, \veta_{i,j}), ~F_N(\vtheta) = \mathbb{E}\left[ \hat{F}_N(\vtheta) \right]
\end{equation*}
where  $\mathcal{P}_{i,j}$ are probability distributions on $\mathbb{R}^m$,$\veta_{i,j} \sim \mathcal{P}_{i,j}$, and expectation is taken over all randomness. Let $\mathcal{C}$ be a nonempty compact subset of $\mathbb{R}^k$ with diameter $D$. Suppose the following assumptions hold:
\begin{itemize}[leftmargin=3.5mm]
\vspace{-5pt}
\item  For $i_1,i_2,j_1,j_2\in [N]$, if $\{i_1\}\cup \{ j_1\}$ and $ \{i_2\}\cup \{j_2\}$ are disjoint, then $\veta_{i_1, j_1}$ and  $\veta_{i_2, j_2}$ are independent.
\vspace{-5pt}
\item  The functions $F_N$ and $\hat{F}_N$ have unique minimizers on $\mathcal{C}$ named $\vtheta_N^*$ and $\hat{\vtheta}_N^*$, respectively.
\vspace{-5pt}
\item The function $F_N$ is $\alpha$-strongly convex on $\mathcal{C}$ ($\alpha>0$).
\vspace{-5pt}
\item For any $\vtheta \in \mathcal{C}$ and $i,j\in[N]$, 
\begin{equation*}
    \mathbb{E}_{\veta \sim \mathcal{P}_{i,j}}\left [e^{|f(\vtheta, \veta)- \mathbb{E}_{\veta \sim \mathcal{P}_{i,j}}[f(\vtheta, \veta)] |} \right]<M.
    \vspace{-10pt}
\end{equation*}
\item There exists a function $g(\cdot): \R^k \rightarrow \R$ such that for any $\vtheta \in \mathcal{C}$ and $\veta \in \R^k$, it holds that  $\lVert \nabla_\vtheta f(\vtheta,\veta) \rVert \leq g(\veta)$. In addition,  $\lVert \nabla_\vtheta \mathbb{E}_{\veta \sim \mathcal{P}_{i,j}}[f(\vtheta, \veta)]\rVert \leq \mathbb{E}_{\veta \sim \mathcal{P}_{i,j}}[g(\veta)]$ and $\mathbb{E}_{\veta \sim \mathcal{P}_{i,j}}\left[ e^{g(\veta)}\right]< L$ for each $\vtheta \in \mathcal{C}$ and  $i,j \in [N]$.
\end{itemize}
For each $0<\epsilon< \alpha^{-1/2}$, we have $\lVert \hat{\vtheta}_N^* - \vtheta \rVert < \epsilon$ with probability at least $1- \delta$  if $N$ is greater than
\begin{equation*}
\frac{C_1' M}{ \alpha^2 \epsilon^4} \log\left(\frac{3}{\delta} \max \left \{1, \left(\frac{C_2' k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\} \right),
\end{equation*}
where $C_1', C_2'>0$ are universal constants.
\end{lemma}
\begin{proof}[Proof Sketch]
When we follow the proof of Lemma~\ref{lemma:minimizer_independent}, the challenging part is that we cannot use the fact that the expected value of a product of independent random variables is equal to a product of expectations of individual random variables. We overcome this by partitioning the $N^2$ random variables into batches such that random variables belonging to the same batch are independent (Lemma~\ref{lemma:partition}) and then applying a generalized Cauchy-Schwartz inequality (Lemma~\ref{lemma:cauchy}) to bound an expectation of a product of dependent random variables (each corresponding to a batch) with a product of expectations of the random variables. A formal proof can be found in Appendix~\ref{proof:minimizer_dependent}. 
\end{proof}

Similar to the proof of Theorem~\ref{thm:ERM_sufficient}, considering $\{ \veta_{i,j} \}_{i,j=1}^n$ as the ``mixed'' dataset $\{ (\tilde{\vx}_{i,j}. \tilde{y}_{i,j})\}_{i.j=1}^n$, $\vtheta$ as weight vector $\vw$, and $f(\vtheta, \veta_{i,j})$ as $(\vw, (\tilde{\vx}_{i,j}, \tilde{y}_{i,j})) \mapsto \tilde{y_i}l(\vw^\top \tilde{\vx}_{i,j} + (1-\tilde{y_i})l(-\vw^\top \tilde{\vx}_{i,j})$ induces the following theorem.
\begin{theorem}\label{thm:mixup_convergence}
Let $\epsilon, \delta \in (0,1)$. Suppose the training set $S = \{(\vx_i, y_i)\}_{i=1}^n\overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ with large enough $\kappa \in (0,\infty)$ and
\begin{equation*}
    n = \frac{\Omega(\kappa^2)}{\epsilon^4}\left (1 + \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right).
\end{equation*}
Then, with probability at least $1-\delta$, the unique minimizer $\hat{\vw}_{\mathrm{mix}, S}^*$ of $\lossmix(\vw)$ exists and $\cosim(\hat{\vw}_{\mathrm{mix}, S}^*, \mSigma^{-1}\vmu) \geq 1-\epsilon$.
\end{theorem}
Theorem~\ref{thm:mixup_convergence} indicates that the sample complexity for finding a near Bayes optimal classifier with Mixup training grows only quadratically in $\kappa$. Compared to the necessity of exponential growth demonstrated in Theorem~\ref{thm:ERM_necessary}, Theorem~\ref{thm:mixup_convergence} shows that there is a provable exponential gap between ERM training and Mixup training.
\paragraph{Intuition on the Smaller Sample Complexity of Mixup.}We would like to provide some intuition on our result before we introduce technical aspects. Unlike ERM training, Mixup training uses mixed training points and these can be located near the optimal decision boundary when we mix two data points having distinct labels. This closeness of mixed points to the Bayes optimal decision boundary makes it easier to correctly locate the boundary.
\vspace*{-5pt}
\paragraph{Different Scaling of $\w$ and $\wmix$.}
On the technical front, the difference in sample complexity from Theorem~\ref{thm:ERM_sufficient} stems from the difference between the norm of expected loss minimizers $\w$ and $\wmix$, which determine several meaningful terms when we apply Lemma~\ref{lemma:minimizer_dependent}.
The following lemma characterizes the $\ell_2$ norm of $\wmix$, defined in Theorem~\ref{thm:expected_loss_mix}:
\begin{lemma}\label{lemma:mixup_norm}
The unique minimizer $\wmix$ of expected Mixup loss $\mathbb{E}_\kappa [\lossmix(\vw)]$ satisfies
\begin{equation*}
    \lVert \wmix \rVert  = \Theta (1).\footnote{We stress that the upper/lower bounds are independent of $n$.}
\end{equation*}
\end{lemma}
\vspace*{-10pt}
Comparison with Lemma~\ref{lemma:ERM_norm} reveals that the minimizer $\wmix$ of expected Mixup loss is much closer to zero compared to that of the expected ERM loss. 
For ERM loss, large scaling of weight leads to smaller loss for correctly classified data points. Also, for larger $\kappa$, larger portion of population data will be correctly classified by $\mSigma^{-1} \vmu$. Hence, $\lVert \w \rVert$ increases as $\kappa$ increases. However, in case of Mixup, mixed labels prevent $\wmix$ from growing with $\kappa$. To illustrate why, consider the case $\tilde y_{i,j} = 0.5$, which leads to a Mixup loss
\begin{equation*}
0.5 l(\vw^\top \tilde \vx_{i,j}) + 0.5 l(-\vw^\top \tilde \vx_{i,j}).
\end{equation*}
Notice here that the loss becomes large whenever $\vw^\top \tilde \vx_{i,j}$ is large in magnitude, no matter the sign is. For this reason, $\wmix$ should not increase with $\kappa$ and this leads to the smaller sample complexity of Mixup.

In this section, we showed that Mixup training does not distort the training loss (Theorem~\ref{thm:expected_loss_mix}) and also that Mixup provides a great remedy to the curse of separability phenomenon (Theorem~\ref{thm:mixup_convergence}), because the sample complexity only grows in $\kappa^2$ while ERM suffers at least exponential growth in $\kappa$. Thus, Mixup provably mitigates the curse of separability and helps us find a model with the best generalization performance.