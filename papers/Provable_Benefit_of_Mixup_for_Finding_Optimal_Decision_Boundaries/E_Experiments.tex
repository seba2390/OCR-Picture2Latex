\section{Detailed Experimental Settings and Additional Results of Section~\ref{exp:gaussian}}\label{additional_reults}

\subsection{Detailed Settings }\label{setting:gaussian}
In Section 6.1, we intentionally selected values for $\vmu$ and $\mSigma$ such that $\vmu$ is not an eigenvector of $\mSigma$. This was done to ensure that $\vmu$ is distinct in a direction from $\mSigma^{-1} \vmu$. Our selected value for $\vmu$ and $\mSigma$ is as follows, but we note that any other general choices would work.
\begin{equation*} \vmu = \begin{pmatrix} -0.1067\\ 0.2572\\ -0.2392\\ 0.4135\\ -0.2179\\ -0.3995\\ -0.1437\\ 0.5950\\ 0.1786\\ -0.2839
\end{pmatrix},
\end{equation*}
\begin{align*}
\mSigma =
\begin{pmatrix} 
0.4481& 0.0904& -0.0128& -0.0245& 0.1082& -0.2444& 0.1817& 0.0881& 0.0308& 0.0450\\ 
0.0904& 0.4727& 0.0578& -0.1620& 0.0481& -0.0629& 0.0509& -0.1300& -0.1013& -0.1706\\
-0.0128& 0.0578& 0.2477& -0.0728& -0.0490& 0.1214& 0.0189& 0.0159& 0.0064& 0.1649\\
-0.0245& -0.1620& -0.0728& 0.4457& 0.0462& -0.1026& 0.1188& -0.0066& -0.0757& 0.1065\\
0.1082& 0.0481& -0.0490& 0.0462& 0.2892& 0.0268& 0.1117& -0.1799& 0.0617& 0.1787\\
-0.2444& -0.0629& 0.1214& -0.1026& 0.0268& 0.4248& -0.0868& 0.0565& 0.0482& 0.2182\\
0.1817& 0.0509& 0.0189& 0.1188& 0.1117& -0.0868& 0.3638& -0.0980& -0.0279& 0.1658\\
0.0881& -0.1300& 0.0159& -0.0066& -0.1799& 0.0565& -0.0980& 0.4999& 0.0010& -0.0318\\
0.0308& -0.1013& 0.0064& -0.0757& 0.0617& 0.0482& -0.0279& 0.0010& 0.1550& 0.1723\\
0.0450& -0.1706& 0.1649& 0.1065& 0.1787& 0.2182& 0.1658& -0.0318& 0.1723& 0.6230
\end{pmatrix}
\end{align*}

\subsection{Addtional Results of Section~\ref{exp:gaussian}}\label{result:gaussian}
We provide additional experimental results of Section~\ref{exp:gaussian}. We follow the same setting described in Section~\ref{exp:gaussian} and Appendix~\ref{setting:gaussian} without fixing initial weights for various choices on the number of samples $n = 50,100,200,500,1000,2000$ and the separability constant $\kappa = 0.1,0.2,0.5,1.0,2.0,5.0$. We plot the average cosine similarity between the Bayes optimal direction and learned weights in Figure~\ref{exp:d=10} and one may check that the experiments align with our theoretical findings.

In addition, we provide results on $d=20$ in Figure~\ref{exp:d=20} in order to demonstrate a dependency of a sample complexity on a data dimension $d$.  For the results with $d=20$, we use additional values on the number of samples $n=5000,10000$ and we choose $\vmu \in \R^{20}$ and $\mSigma \in \R^{20 \times 20}$ as
\begin{equation*}
    \vmu = \left(\vmu_0^\top, \vmu_0^\top\right)^\top,  \mSigma = \begin{pmatrix}
\mSigma_0 & \vzero \\
\vzero &\mSigma_0 
\end{pmatrix}, 
\end{equation*}
where $\vmu_0 \in \R^{10}$ and $\mSigma_0 \in \R^{10 \times 10}$ is choice of $\vmu$ and $\mSigma$ described in Appendix~\ref{setting:gaussian}. This choice makes it easy to compare two cases $d=10$ and $d=20$. Comparison between Figure~\ref{exp:d=10} and Figure~\ref{exp:d=20} shows that sample complexities for finding optimal decision boundary significantly increase in dimension $d$.

\begin{figure}[h]
\centering
    \begin{subfigure}[$d=10$]
    {
    \centering
    \includegraphics[width = \textwidth]{Heat_Map.pdf}
    \label{exp:d=10}
    }
    \end{subfigure}
    \begin{subfigure}[$d=20$]
    {
    \centering
    \includegraphics[width = \textwidth]{Heat_Map_highdim.pdf}
    \label{exp:d=20}
    }
    \end{subfigure}
    \caption{The average cosine similarity between the Bayes optimal direction and learned weights}
\end{figure}

