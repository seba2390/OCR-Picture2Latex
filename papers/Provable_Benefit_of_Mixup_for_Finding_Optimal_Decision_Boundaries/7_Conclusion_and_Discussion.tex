\section{Conclusion}
We analyzed how Mixup-style training influences the sample complexity for getting optimal decision boundaries in logistic regression on data drawn from two symmetric Gaussian distributions with separability constant $\kappa$. Interestingly, we proved that vanilla training suffers the curse of separability. More precisely, ERM requires an exponentially increasing number of data with $\kappa$ for finding a near Bayes optimal classifier. We proved that Mixup mitigates the curse of separability and the sample complexity for finding optimal classifier grows only quadratically with $\kappa$. We further investigated masking-based Mixup methods and showed that they can cause training loss distortion and find a suboptimal decision boundary while having a small sample complexity. One interesting future direction is analyzing when and how state-of-the-art masking-based Mixup works, by considering more sophisticated data distributions capturing image data and complicated models such as neural networks.