Before moving on to proof of the main results of Section~\ref{section:mask}, We would like to represent $\mathbb{E}_\kappa [ \lossmask ( \vw )]$ in a simpler form. Note that $(\tilde{\vx}_{i,i}^\mathrm{mask},\tilde{y}_{i,i}^\mathrm{mask}) = (\vx_i, y_i)$ for each $i \in [n]$. For $i,j \in [n]$ with $i \neq j$, conditioning on $(\mM_{i,j}, \lambda_{i,j}) \sim \mathcal{M}$, we have
\begin{align*}
    \begin{cases}
    \tilde{y}_{i,j} = 1,   &\tilde{x}_{i,j}\sim N(\vmu, \mSigma \odot \left(\mM_{i,j}\mM_{i,j}^\top + (\vone-\mM_{i,j})(\vone-\mM_{i,j})^\top )\right) \hfill\text{with probability }\frac{1}{4}\\
    \tilde{y}_{i,j} = \lambda_{i,j}, &\tilde{x}_{i,j} \sim N((2\mM_{i,j}-1)\odot\vmu, \mSigma \odot \left(\mM_{i,j}\mM_{i,j}^\top + (\vone-\mM_{i,j})(\vone-\mM_{i,j})^\top )\right)\hfill\text{with probability } \frac{1}{4}\\
    \tilde{y}_{i,j} = 1-\lambda_{i,j},  &\tilde{x}_{i,j}\sim N(-(2\mM_{i,j}-1)\vmu, \mSigma \odot \left(\mM_{i,j}\mM_{i,j}^\top + (\vone-\mM_{i,j})(\vone-\mM_{i,j})^\top )\right)\hfill\text{with probability }\frac{1}{4}\\
    \tilde{y}_{i,j} = 0,  &\tilde{x}_{i,j}\sim N(-\vmu, \mSigma \odot \left(\mM_{i,j}\mM_{i,j}^\top + (\vone-\mM_{i,j})(\vone-\mM_{i,j})^\top )\right) \hfill\text{with probability } \frac{1}{4}
    \end{cases}.
\end{align*}
Let $ \support(\mM) = \left \{ \mM^{(1)}, \dots, \mM^{(p)} \right \} $ where $(\mM, \lambda ) \sim \mathcal{M} $. For each $k \in [p]$, define 
\begin{equation*}
\vmu^{(k)} = \vmu \odot (2\mM^{(k)}-1), \mSigma^{(k)} = \mSigma \odot \left(\mM^{(k)}{\mM^{(k)}}^\top + (\vone-\mM^{(k)})(\vone-\mM^{(k)})^\top\right),
\end{equation*}
and
\begin{equation*}
a_k = \frac{1}{2}\mathbb{E}_{(\mM,\lambda) \sim \mathcal{M}}\left[\lambda\vone_{\mM = \mM^{(k)}}\right], b_k = \frac{1}{2}\mathbb{E}_{(\mM,\lambda) \sim \mathcal{M}}\left[(1-\lambda)\vone_{\mM = \mM^{(k)}}\right],c_k = \frac{1}{2} \mathbb{P}_{(\mM, \lambda) \sim \mathcal{M}}\left[\mM = \mM^{(k)} \right].
\end{equation*}
By Assumption~\ref{Assumption:mask}, $a_1, \dots, a_p, b_1, \dots, b_p, c_1, \dots, c_p>0$. One can check that $a_k + b_k = c_k$ for each $k \in [p]$ and $\sum_{k=1}^p (a_k + b_k + c_k) = 1$.  Then, for each $\kappa\in (0,\infty)$, we can rewrite $\mathbb{E}_\kappa [\lossmask (\vw)]$ as the following form:
\begin{align}\label{eqn:mask}
    &\quad \mathbb{E}_\kappa [\lossmask(\vw)]\nonumber \\
    &= \frac{1}{n^2} \left( \sum_{i=1}^n \mathbb{E}_\kappa \left[y_il(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)\right]  + \sum_{\overset{i,j \in [n]}{i \neq j}} \mathbb{E}_\kappa \left[\tilde{y}_{i,j}^\mathrm{mask} l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) + (1-\tilde{y}_{i,j}^\mathrm{mask}) l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\right]\right)\nonumber \\ 
    &=\frac{1}{n}\mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma)} \left[l(\vw^\top \rvx)\right] \nonumber \\
    &\quad + \frac{n-1}{n} \left( \sum_{k=1}^p \mathbb{E}_{\rvx^{(k)} \sim N(\vmu^{(k)}, \kappa^{-1}\mSigma^{(k)})}\left [a_k l\left (\vw^\top \rvx^{(k)}\right) +  b_k l \left(-\vw^\top \rvx^{(k)} \right) \right] + c_k \mathbb{E}_{\rvz^{(k)} \sim N(\vmu, \kappa^{-1} \mSigma^{(k)})}\left[l\left( \vw^\top \rvz^{(k)}\right)\right]\right),
\end{align}
and
\begin{equation}\label{eqn:mask_infty}
    \mathbb{E}_\infty[\lossmask (\vw)] = \frac{1}{n} l(\vw^\top \vmu) + \frac{n-1}{n}\sum_{k=1}^p \left(a_k l\left (\vw^\top \vmu^{(k)}\right) + b_k l\left (- \vw^\top \vmu^{(k)}\right) + c_k l\left (\vw^\top \vmu\right) \right).
\end{equation}
In addition, notice that for each $k \in [p]$, we have
\begin{align}\label{eqn:mask_dist}
\begin{cases}
\tilde{\vx}_{i,j}^\mathrm{mask} \sim N \left (\vmu, \kappa^{-1} \mSigma^{(k)} \right) &\text{with probability $\frac{c_k}{2}$}\\
\tilde{\vx}_{i,j}^\mathrm{mask} \sim N \left (-\vmu, \kappa^{-1} \mSigma^{(k)} \right) &\text{with probability $\frac{c_k}{2}$}\\
\tilde{\vx}_{i,j}^\mathrm{mask} \sim N \left (\vmu^{(k)}, \kappa^{-1} \mSigma^{(k)} \right) &\text{with probability $\frac{c_k}{2}$}\\
\tilde{\vx}_{i,j}^\mathrm{mask} \sim N \left (-\vmu^{(k)}, \kappa^{-1} \mSigma^{(k)} \right) &\text{with probability $\frac{c_k}{2}$}
\end{cases}.
\end{align}

\subsection{Proof of Theorem~\ref{thm:mask_minimizer}}\label{proof:mask}
We first prove the uniqueness of a minimizer $\wmask$ of $\mathbb{E}_\kappa [\lossmask(\vw)]$ for all $\kappa \in (0, \infty]$ and show that they are bounded. Next, we prove that $\wmask$ converges to $\wmask[\infty]$ as $\kappa \rightarrow \infty$.
\paragraph{Step 1: $\mathbb{E}_\kappa [\lossmask(\vw)]$ has a Unique Minimizer $\wmask$ and $\{ \wmask : \kappa \in (0, \infty]\} $ is Bounded}\quad

We prove the strict convexity of $\mathbb{E}_\kappa [\lossmask (\vw)]$ for each $\kappa \in (0, \infty]$. Consider the case $\kappa \neq \infty$ first. For $t \in [0,1]$, and $\vw_1, \vw_2 \in \mathbb{R}^d$ with $\vw_1 \neq \vw_2$,
\begin{equation*}
    t l\left(\vw_1^\top \vx \right) + (1-t) l\left(\vw_2^\top \vx\right) >  l\left((t\vw_1 + (1-t)\vw_2)^\top \vx\right),
\end{equation*}
and
\begin{equation*}
    t l\left(-\vw_1^\top \vx\right) + (1-t) l\left(-\vw_2^\top \vx\right) >  l\left(-(t\vw_1 + (1-t)\vw_2)^\top \vx\right),
\end{equation*}
for any $\vx\in \mathbb{R}^d$ except a Lebesgue measure zero set (hyperplane orthogonal to $\vw_1 - \vw_2$).

By \Eqref{eqn:mask} and taking expectation, we have for any $t \in [0,1]$, and $\vw_1, \vw_2 \in \mathbb{R}^d$,
\begin{equation*}
    t \mathbb{E}_\kappa[\lossmask(\vw_1)] + (1-t) \mathbb{E}_\kappa[\lossmask(\vw_2)] >  \mathbb{E}_\kappa[\lossmask(t\vw_1+(1-t)\vw_2)].
\end{equation*}
For the case $\kappa = \infty$, by Assumption~\ref{Assumption:mask}, there exist $i_1, \dots, i_d \in [m]$ such that $\left \{ \vmu^{(i_1)}, \dots, \vmu^{(i_d)} \right \}$ spans $\mathbb{R}^d$ and for any $t\in [0,1]$ and $\vw_1,\vw_2 \in \mathbb{R}^d$ with $\vw_1 \neq \vw_2$, at least one of $k \in [d]$ satisfies 
\begin{equation*}
    t l\left(\vw_1^\top \vmu^{(i_k)}\right) + (1-t) l\left(\vw_2^\top \vmu^{(i_k)}\right) >  l\left((t\vw_1 + (1-t)\vw_2)^\top \vmu^{(i_k)}\right),
\end{equation*}
and
\begin{equation*}
    t l\left(- \vw_1^\top \vmu^{(i_k)}\right) + (1-t) l\left(-\vw_2^\top \vmu^{(i_k)}\right) >  l\left(-(t\vw_1 + (1-t)\vw_2)^\top \vmu^{(i_k)}\right).
\end{equation*}
From \Eqref{eqn:mask_infty}, we can conclude the strict convexity of $\mathbb{E}_\kappa [\lossmask (\vw)]$. 

In order to complete this step, we prove the existence of a ball containing all minimizers of $\mathbb{E}_\kappa[\lossmask(\vw)]$. For the case $\kappa \neq \infty$, from \Eqref{eqn:mask}, we have
\begin{equation*}
    \mathbb{E}_\kappa[\lossmask(\vw)] \geq \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k}, b_{i_k}\} \mathbb{E}_{\rvx^{(k)} \sim N(\vmu^{(i_k)},\kappa^{-1} \mSigma^{(i_k)})} \left[l\left(\vw^\top \rvx^{(k)}\right) + l\left(-\vw^\top \rvx^{(k)}\right)\right].
\end{equation*}
Since $l(z) + l(-z) \geq |z|$ for each $z \in \mathbb{R}$,
\begin{align*}
    \mathbb{E}_\kappa[\lossmask(\vw)] &\geq \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k},b_{i_k}\} \mathbb{E}_{\rvx^{(k)} \sim N(\vmu^{(i_k)}, \kappa^{-1} \mSigma^{(i_k)})}\left[\left|\vw^\top \rvx^{(k)}\right|\right] \\
    &\geq \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k}, b_{i_k}\} \left|\mathbb{E}_{\rvx^{(k)} \sim N(\vmu^{(i_k)}, \kappa^{-1} \mSigma^{(i_k)})} \left[\vw^\top \rvx^{(k)}\right]\right|\\
    &= \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k}, b_{i_k}\} \left|\vw^\top \vmu^{(i_k)}\right|.
\end{align*}
Also, for the case $\kappa = \infty$, from \Eqref{eqn:mask_infty} and using similar argument, we have
\begin{align*}
    \mathbb{E}_\infty[\lossmask(\vw)] 
    &\geq \frac{1}{2} \sum_{k=1}^d \min\{a_{i_k}, b_{i_k}\} \left(l\left(\vw^\top \vmu^{(k)}\right) + l\left(-\vw^\top \vmu^{(k)}\right)\right)\\
    &\geq \frac{1}{2}\sum_{k=1}^d \min\{a_{i_k},b_{i_k}\} \left|\vw^\top \vmu^{(i_k)}\right|.
\end{align*}
Since $\left\{\mu^{(i_1)}, \dots, \mu^{(i_d)}\right\}$ spans $\mathbb{R}^d$, for any unit vector $\vu \in \mathbb{R}^d$, $\sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vu^\top \vmu^{(i_k)}\right| >0$ and $\vu \mapsto \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vu^\top \vmu^{(i_k)}\right|$ has the minimum value $m>0$ since $\{ \vu \in \mathbb{R}^d : \lVert \vu \rVert =1 \}$ is compact and the mapping is continuous. If $\lVert \vw \rVert > \frac{2\log 2}{m}$, then we have
\begin{align*}
    \mathbb{E}_\kappa[\lossmask(\vw)]  &\geq \frac{1}{2} \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\vw^\top \vmu^{(i_k)}\right|\\ 
    &= \lVert \vw \rVert \sum_{k=1}^d \min\{a_{i_k},  b_{i_k}\} \left|\left(\frac{\vw}{\lVert \vw \rVert}\right)^\top \vmu^{(i_k)}\right|\\ 
    &\geq \frac{1}{2}\lVert \vw \rVert m \geq \log 2\\
    &= \mathbb{E}_\kappa\left[\lossmask(\vzero)\right].
\end{align*}
Hence, for any $\kappa \in (0,\infty]$, the minimizer of $\mathbb{E}_\kappa[\lossmask(\vw)]$ is contained in the ball centered at origin with radius $R:=\frac{2 \log 2}{m}$. Together with the strict convexity of $\mathbb{E}_{\kappa}[\lossmask (\vw)]$, we have our conclusion. \hfill $\square$

\paragraph{Step 2: $\wmask$  Converges to $\wmask[\infty]$ as $\kappa \rightarrow \infty$} \quad

For each $\vw \in \mathbb{R}^d$ with $\lVert \vw \rVert \leq R$ and unit vector $\vv \in \mathbb{R}^d$,  \Eqref{eqn:mask_infty} implies
\begin{align*}
 &\quad \vv^\top \nabla_\vw^2 \mathbb{E}_\infty[\lossmask (\vw) ]\vv \\
 &= \frac{1}{n} l''(\vw^\top \vmu) (\vv^\top \vmu)^2 + \frac{n-1}{n} \sum_{k=1}^p \left[ \left(a_k l''\left(\vw^\top \vmu^{(k)}\right) + b_k l''\left(-\vw^\top \vmu^{(k)}\right)\right) \left(\vv^\top \vmu^{(k)}\right)^2 + c_k l''\left(\vw^\top \vmu \right)(\vv^\top \vmu)^2 \right]\\
 &\geq \frac{1}{2} \sum_{k=1}^p (a_k + b_k) l''\left (R\lVert \vmu^{(k)}\rVert \right) \left(\vv^\top \vmu^{(k)} \right)^2.
\end{align*}
By Assumption~\ref{Assumption:mask}, at least one of $k\in[p]$ satisfies $\vv^\top \vmu^{(k)} \neq 0$ and thus, $\frac{1}{2} \sum_{k=1}^p (a_k + b_k) l''\left ( R\lVert \vmu^{(k)}\rVert \right) \left(\vv^\top \vmu^{(k)} \right)^2 >0$. Since $\vv \mapsto \frac{1}{2} \sum_{k=1}^p (a_k + b_k) l''\left ( R\lVert \vmu^{(k)}\rVert \right) \left(\vv^\top \vmu^{(k)} \right)^2$ is continuous and $\{ \vv \in \mathbb{R}^d:\lVert \vv\rVert=1 \}$ is compact, it has minimum $\alpha>0$ on this set. Hence, $\mathbb{E}_\infty \left[ \lossmask (\vw)\right]$ is $\alpha$-strongly convex on $\left \{\vw\in \mathbb{R}^d : \lVert \vw \rVert \leq R \right \}$ and since $\wmask \in \left \{\vw\in \mathbb{R}^d : \lVert \vw \rVert \leq R \right \}$ for each $\kappa \in (0,\infty)$, we have
\begin{align*}
&\quad \frac{\alpha}{2} \lVert \wmask - \wmask[\infty]\rVert^2\\
&\leq \mathbb{E}_\infty[\lossmask (\wmask)] - \mathbb{E}_\infty[\lossmask (\wmask[\infty])]\\
&\leq \mathbb{E}_\infty[\lossmask (\wmask)] - \mathbb{E}_\kappa[\lossmask (\wmask)]\\
&\quad + \mathbb{E}_\kappa[\lossmask (\wmask)] - \mathbb{E}_\kappa[\lossmask (\wmask[\infty])]\\
&\quad +\mathbb{E}_\kappa[\lossmask (\wmask[\infty])] - \mathbb{E}_\infty[\lossmask (\wmask[\infty])]\\
&\leq \mathbb{E}_\infty[\lossmask (\wmask)] - \mathbb{E}_\kappa[\lossmask (\wmask)]+\mathbb{E}_\kappa[\lossmask (\wmask[\infty])] - \mathbb{E}_\infty[\lossmask (\wmask[\infty])].
\end{align*}
The last inequality holds since $\wmask$ is a minimizer of $\mathbb{E}_\kappa [\lossmask (\vw)]$.
For any $\vw \in \mathbb{R}^d$ with $\lVert \vw \rVert \leq R$, by Lemma~\ref{lemma:ineq5}, we have
\begin{align*}
&\quad\mathbb{E}_\kappa [\lossmask (\vw)] - \mathbb{E}_\infty [\lossmask(\vw)]\\
&= \frac{1}{n} \mathbb{E}_{X \sim N(\vw^\top \vmu, \kappa^{-1} \vw^\top \mSigma \vw)}[l(X) - l(\vw^\top \vmu)] \\
&\quad + \frac{n-1}{n} \Bigg( \sum_{k=1}^p \mathbb{E}_{X^{(k)} \sim N(\vw^\top \vmu^{(k)}, \kappa^{-1} \vw^\top \mSigma^{(k)} \vw)}\left[a_k \left(l\left(X^{(k)}\right)-l\left(\vw^\top \vmu^{(k)}\right) \right) +b_k \left(l\left(-X^{(k)}\right) - l\left(- \vw^\top \vmu^{(k)}\right)\right) \right]\\
& \qquad \qquad \quad+ c_k \mathbb{E}_{X^{(k)}\sim N(\vw^\top \vmu, \kappa^{-1} \vw^\top \mSigma^{(k)}\vw )}\left[l\left(X^{(k)}\right) - l(\vw^\top \vmu)\right] \Bigg)\\
&\leq \kappa^{-1/2} \left( \frac{1}{n} (\vw^\top \mSigma \vw)^{1/2} + \frac{n-1}{n} \sum_{k=1}^p (a_k+ b_k +c_k) \left(\vw^\top \mSigma^{(k)} \vw\right)^{1/2} \right) \\
&\leq  \kappa^{-1/2} \lVert \vw \rVert \left( \frac{\lVert \mSigma \rVert^{1/2}}{n} + \frac{n-1}{n}  \sum_{k=1}^p (a_k + b_k + c_k) \left \lVert \mSigma ^{(k)}\right \rVert^{1/2}\right)\\
&\leq R \kappa^{-1/2} \left( \frac{\lVert \mSigma \rVert^{1/2}}{n} +  \frac{n-1}{n}  \sum_{k=1}^p (a_k + b_k + c_k) \left \lVert \mSigma ^{(k)}\right \rVert^{1/2}\right),\\
&\leq R \kappa^{-1/2} \left( \lVert \mSigma \rVert^{1/2} + \sum_{k=1}^p (a_k + b_k + c_k) \left \lVert \mSigma ^{(k)}\right \rVert^{1/2}\right).
\end{align*}
Therefore, 
\begin{equation}\label{eqn:mask_uniform_converge}
    \frac{\alpha}{2 }\lVert \wmask - \wmask[\infty]\rVert^2 \leq 2 R \kappa^{-1/2} \left( \lVert \mSigma \rVert + \sum_{k=1}^p (a_k + b_k + c_k) \left \lVert \mSigma ^{(k)}\right \rVert^{1/2}\right),
\end{equation}
and we conclude $\lim_{\kappa \rightarrow \infty} \wmask = \wmask[\infty]$. \hfill $\square$