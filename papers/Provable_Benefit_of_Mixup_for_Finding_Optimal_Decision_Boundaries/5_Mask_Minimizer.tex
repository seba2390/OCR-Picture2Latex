\vspace*{-5pt}
\section{Masking-based Mixup Can Distort Training}\label{section:mask}
Recent Mixup variants for image data \citep{yun2019cutmix, kim2020puzzle, kim2021co, liu2022automix} use masking on input data. In this section, we investigate how masking-based augmentation techniques work in our data distribution setting. We consider the class of masking-based Mixup variants formulated as follows.
\begin{definition}(Masking-based Mixup Loss) Masking-based Mixup loss with training set $S = \{ (\vx_i, y_i)\}_{i=1}^n$ is defined as
\vspace{-10pt}
\begin{align*}
   \lossmask(\vw):= \frac{1}{n^2} &\sum_{i,j=1}^n \tilde{y}_{i,j}^\mathrm{mask} l(\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask})\\
   &\qquad+ (1-\tilde{y}_{i,j}^\mathrm{mask}) l(-\vw^\top \tilde{\vx}_{i,j}^\mathrm{mask}) ,
   \vspace{-10pt}
\end{align*}
where
\vspace{-10pt}
\begin{align*}
   (\mM_{i,j},\lambda_{i,j}) &\overset{\mathrm{i.i.d}}{\sim} \mathcal{M},\\
   \tilde{\vx}_{i,j}^\mathrm{mask} &~= \mM_{i,j} \odot  \vx_i  + (\vone -\mM_{i,j}) \odot \vx_j, \\
   \tilde{y}_{i,j}^\mathrm{mask} &~= \lambda_{i,j} y_i  + (1-\lambda_{i,j})y_j.
\end{align*}
Here, $\support (\mathcal{M}) \subset \{0,1\}^d \times[0,1]$. 
\end{definition}
\vspace{-10pt}
In our definition of masking-based mixup loss, we formulate the masking operation on data points by element-wise multiplication with vectors having entries from only $0$ and $1$. This formulation includes CutMix \citep{yun2019cutmix} which is simplest type of masking-based Mixup. State-of-the-art Mixup variants having more complex masking strategies \citep{kim2020puzzle, kim2021co, liu2022automix} are out of the scope of this paper. We also introduce the following assumption on masking.
\begin{assumption} \label{Assumption:mask}
The set $\support(\vmu \odot (2\mM-\vone))$ spans $\mathbb{R}^d$ and $\mathbb{P}[\lambda \vone_{\mM = \mM_0} \not \in \{0,1\}]>0$ for each $\mM_0 \in \support(\mM)$ where $(\mM, \lambda) \sim \mathcal{M}$.
\end{assumption}
\vspace{-5pt}
Before we move on to our main results, we demonstrate why our formulation and assumption hold for CutMix. CutMix samples mixing ratio $\lambda_{i,j}$ from beta distribution and masking vector $\mM_{i,j}$ is uniformly sampled from vectors in which the number of $1$'s is proportional to $\lambda_{i,j}$. 
Since the support of beta distribution is $[0,1]$, support of $\mM_{i,j}$ contains the standard basis of $\mathbb{R}^d$. Hence if all the components of $\vmu$ are nonzero, Assumption~\ref{Assumption:mask} holds.

Recall that we defined $\mathcal{D}_\infty$ as a limit behavior of $\mathcal{D}_\kappa$ as $\kappa \rightarrow \infty$ and it is independent of $\mSigma$. Hence, $\mathbb{E}_\infty [\lossmask(\vw)]$ is independent of $\mSigma$. The following theorem investigates the minimizer of the expected masking-based Mixup loss $\mathbb{E}_\kappa [\lossmask (\vw)]$, focusing on large enough $\kappa$.

\begin{theorem}\label{thm:mask_minimizer}
Suppose Assumption~\ref{Assumption:mask} holds and the training set $S = \{(\vx_i, y_i)\}_{i=1}^n\overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ with $\kappa \in (0, \infty]$. Then, the expected loss $\mathbb{E}_\kappa [\lossmask(\vw)]$ has a unique minimizer $\wmask$. In addition, $\lim_{\kappa \rightarrow \infty} \wmask[\kappa] = \wmask[\infty]$.
\end{theorem}
\vspace*{-10pt}
\begin{proof}[Proof Sketch]
For the uniqueness of the minimizer, we use almost the same strategy as the uniqueness parts of Theorem~\ref{thm:expected_loss_ERM} and Theorem~\ref{thm:expected_loss_mix}. The only part that requires a different strategy is the uniqueness for $\kappa = \infty$; in this case, we exploit Assumption~\ref{Assumption:mask}. Also, from Assumption~\ref{Assumption:mask}, we can show that $\mathbb{E}_\infty \left[ \lossmask (\vw) \right]$ is $\alpha$-strongly convex with some $\alpha>0$.
Using strong convexity constant $\alpha$, we establish upper bound on $\lVert \wmask - \wmask[\infty] \rVert$ represented by $\mathbb{E}_\kappa \left[ \lossmask (\vw) \right]  - \mathbb{E}_\infty \left[ \lossmask (\vw) \right]$ for several values of $\vw$ contained in a bounded set.
We finish up by showing $\mathbb{E}_\kappa \left[ \lossmask (\vw) \right]  - \mathbb{E}_\infty \left[ \lossmask (\vw) \right] \rightarrow 0$ uniformly on the bounded set as $\kappa \rightarrow \infty$.
\vspace{-5pt}
\end{proof} 
\paragraph{Masking-based Mixup Can Distort Training Loss.}
Unlike ERM loss and Mixup loss, characterizing the exact direction of $\wmask$ is challenging since $\mathbb{E}_\kappa \left[ \lossmask (\vw) \right]$ has a more complicated form because of masking. However,
our Theorem~\ref{thm:mask_minimizer} implies that $\wmask$ leads to a solution only depending on $\vmu$ and deviates from the Bayes optimal solution for sufficiently large $\kappa$. Even though Theorem~\ref{thm:mask_minimizer} guarantees deviation of $\wmask$ from the Bayes optimal direction only for large $\kappa$, our experimental results in Section~\ref{exp:gaussian} suggest that our result holds even for moderately sized $\kappa$.

One might be wondering whether the same thing can be said for the minimizer of the expected Mixup loss $\mathbb{E}_\infty[\lossmix (\vw)]$; we illustrate why the proof idea of Theorem~\ref{thm:mask_minimizer} does not work for Mixup. While $\mathbb{E}_\infty[\lossmask (\vw)]$ is strongly convex and has a unique minimizer, minimizers of $\mathbb{E}_\infty[\lossmix (\vw)]$ are not unique since if $\vu \in \mathbb{R}^d$ is a minimizer of  $\mathbb{E}_\infty[\lossmix (\vw)]$, $\vu+\vv$ is also a minimizer of $\mathbb{E}_\infty[\lossmix (\vw)]$ for any $\vv \in \mathbb{R}^d$ orthogonal to $\vmu$. 
Therefore, $\wmix$ maintains its direction even though $\mathbb{E}_\infty[\lossmix (\vw)]$ is independent of $\mSigma$; since there are many minimizers in $\mathbb{E}_\infty[\lossmix (\vw)]$, $\wmix$ converges to the Bayes optimal minimizer (among the many) as $\kappa \to \infty$.

While masking-based Mixup training does not necessarily lead to Bayes optimal classifiers, the sample complexity proof still works. In fact, we can find the solution near the minimizer $\wmask$ of the expected loss $\mathbb{E}_{\kappa}[\lossmask(\vw)]$ with fewer samples.
\begin{theorem}\label{thm:mask_convergence}
Let $\epsilon, \delta \in (0,1)$. Suppose Assumption~\ref{Assumption:mask} holds and the training set $S = \{(\vx_i, y_i)\}_{i=1}^n\overset{\mathrm{i.i.d}}{\sim} \mathcal{D}_\kappa$ with large enough $\kappa \in (0,\infty)$.
If 
\begin{equation*}
    n = \frac{\Omega(1)}{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right ),
    \vspace{-5pt}
\end{equation*}
then with probability at least $1-\delta$, the unique minimizer $\hat{\vw}_{\mathrm{mask}, S}^*$ of $\lossmask(\vw)$ exists and $\cosim(\hat{\vw}_{\mathrm{mask}, S}^*, \wmask) \geq 1-\epsilon$. 
\end{theorem} 
Theorem~\ref{thm:mask_convergence} indicates masking-based Mixup also mitigates the curse of separability (even better than Mixup).
\paragraph{Why Even Smaller Sample Complexity?} We consider a simple case $\vmu = [1,1]^\top$, which is sufficient to convey our intuition. When $\kappa$ is large, most of the data points are likely to be concentrated around $\vmu$ and $-\vmu$. Since Mixup uses linearly interpolated data points, all the mixed training data will be close to a line that passes through the origin and has direction $\vmu$. It means that all raw data points or mixed points are almost orthogonal to $[1,-1]^\top$. Consequently, both ERM and Mixup training loss will be less sensitive to perturbations orthogonal to $\vmu$: i.e., $\loss(\vw ) \approx \loss(\vw + t[1,-1]^\top)$ and $\lossmix(\vw ) \approx \lossmix(\vw + t[1,-1]^\top)$ for any small $t$, which makes it difficult to locate the exact minimizer of the objective loss.
In contrast, masking-based Mixup uses cut-and-pasted data points such as $\vz_1\approx [1,-1]^\top$, constructed by pasting the first coordinate of positive data and the second coordinate of negative data. Masking-based Mixup also uses $\vz_2 \approx [1,1]^\top$, obtained from mixing two positive data points. We observe that ${\vz_1, \vz_2 }$ span the whole space $\mathbb{R}^2$. Therefore, for any perturbation $\vv \in \mathbb{R}^2$, $\vv^\top \vz_1 \not \approx 0$ and/or $\vv^\top \vz_2 \not \approx 0$ should hold, which implies that $\lossmask(\vw ) \not \approx \lossmask(\vw + \vv)$ for any perturbation $\vv$. In other words, the masking-based Mixup loss is sensitive to perturbations in any direction. This makes it easier to locate the exact minimizer of the objective loss, even when $\kappa$ is large.

In this section, we showed that masking-based Mixup mitigates the curse of separability even better than Mixup, but unfortunately, making-based Mixup can find a classifier that is far from being Bayes optimal due to Theorem~\ref{thm:mask_minimizer}. One may think that these results are contradictory to the empirical success of masking-based Mixup such as CutMix~\citep{yun2019cutmix} on image data. However, the regularization effect of Mixup variants is highly dependent on the data, as also noted by \citet{parkunified}. Therefore, our conclusion in Section 5 does not necessarily contradict the success of masking-based Mixup on practical image data. We speculate that the distortion effect of masking-based Mixup on complex image data may be small or even beneficial (e.g., by increasing the chance of co-occurrence of some useful features). In this case, the small sample complexity of masking-based Mixup would be helpful. However, a rigorous theoretical analysis is beyond the scope of our current work and is an essential direction for future research.