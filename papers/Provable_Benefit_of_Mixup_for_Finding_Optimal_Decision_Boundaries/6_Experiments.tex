\section{Experiments} \label{section:exp}
In this section, we present several experimental results to support our findings. 
\subsection{Experiments on Our Setting $\mathcal{D}_\kappa$}\label{exp:gaussian}
First, we provide empirical results on our setting. We properly choose $\vmu$ and $\mSigma$ with $\lVert \vmu \rVert^2 = \lVert \mSigma \rVert$ so that $\vmu$ and the Bayes optimal direction $\mSigma^{-1}\vmu$ have different directions, i.e., $\vmu$ is not an eigenevector of $\mSigma$. 
We provide exact values of our choice of $\vmu$ and $\mSigma$ in Appendix~\ref{setting:gaussian}.  
We compare three training methods ERM, Mixup, and Mask Mixup.
Mask Mixup we considered is a kind of masking-based Mixup with $\mathcal{M}$ defined by the following:
we say $(\mM, \lambda) \sim \mathcal{M}$ when $\lambda$ is drawn from beta distribution $\mathrm{Beta} (\alpha,\alpha)$ and each component of $\mM$ follows $\mathrm{Bernoulli}(\lambda)$.

\begin{figure}[t]
    \centering
\subfigure[$\kappa = 0.5$]{\includegraphics[width= 0.238\textwidth]{GMM_Plot_n=500_kappa=0.5_alpha=1_delta=0.2_lr=1_epochs=1500_num_runs=500.pdf}}
\subfigure[$\kappa = 2.0$]{\includegraphics[width= 0.238\textwidth]{GMM_Plot_n=500_kappa=2.0_alpha=1_delta=0.2_lr=1_epochs=1500_num_runs=500.pdf}}
\vspace{-15pt}
\caption{Cosine similarity between learned weight and the Bayes optimal direction $\mSigma^{-1} \vmu$. ERM successfully finds the Bayes optimal classifier when $\kappa$ is small ($\kappa=0.5$) and fails when $\kappa$ is larger ($\kappa = 2.0$) because of the curse of separability. Meanwhile, Mixup succeeds in both cases since Mixup mitigates the curse of separability. Mask Mixup fails in both cases due to distortion.}
\label{figure:gaussian}
\vspace{-20pt}
\end{figure}

We compare two different values of $\kappa$; $\kappa = 0.5$ and $\kappa = 2.0$. We train for 1500 epochs using randomly sampled 500 training samples from each $\mathcal{D}_\kappa$ and full gradient descent with learning rate $1$ and we choose $\alpha = 1$ for the hyperparameter of Mixup and Mask Mixup. We run 500 times with fixed initial weight but different samples of training sets and we plot cosine similarity between the trained weight and the Bayes optimal direction during training in Figure~\ref{figure:gaussian}. 

For the case $\kappa = 0.5$, ERM and Mixup lead to the Bayes optimal classifier. However, for the case $\kappa = 2.0$, ERM finds a solution deviating from the Bayes optimal solution, while Mixup still finds almost accurate solutions. This result is predicted by our theoretical findings; ERM suffers the curse of separability and Mixup mitigates it. Also, we can check the minimizers of the Mask Mixup loss deviate significantly from the Bayes optimal direction for both cases $\kappa = 0.5$ and $\kappa = 2.0$, even though our theoretical result (Theorem~\ref{thm:mask_minimizer}) focus on large $\kappa$. We provide additional results on more various values of $\kappa$, the number of samples $n$, and the dimension of data $d$ in Appendix~\ref{result:gaussian}.
\vspace{-5pt}
\subsection{2D Classification on Synthetic Data} \label{exp:2d}
\vspace{-5pt}
\begin{figure*}[h]
    \vspace{-10pt}
    \centering
    \subfigure[Large noise, less separable setting]{\includegraphics[width=0.45\textwidth]{sine_data_n_samples_500_kappa_0.5.pdf}\label{figure:2d(a)}}~
    \subfigure[Small noise, well separable setting]{\includegraphics[width=0.45\textwidth]{sine_data_n_samples_500_kappa_1.pdf} \label{figure:2d(b)}}
    \vspace{-10pt}
    \caption{Boundary decision of trained models with ERM loss, Mixup loss and Mask Mixup loss}
    \vspace{-10pt}
    \label{figure:2d}
    \vspace{-5pt}
\end{figure*}
We also provide empirical results supporting that the intuitions gained from our analysis extend beyond our settings. We consider training a two-layer ReLU network with 500 hidden nodes on 2D synthetic data with binary labels having sine-shaped noise\footnote{The noise consists of uniformly sampled $x$-coordinate and $y$-coordinate having sine value with additional Gaussian noise.} from its mean for each class. As a result of the noise, the optimal decision boundary is also sine-shaped.
We consider two settings with different magnitudes of noise while keeping the means the same. 
Using three methods ERM, Mixup, and Mask Mixup (which we introduced in the previous subsection), we train for 1500 epochs using 500 samples of data points and Adam \citep{kingma2014adam} with full batch, learning rate 0.001 and using default hyperparameters of $\beta_1 = 0.9, \beta_2 = 0.999$. We also use $\alpha=1$ for the hyperparameter of Mixup and Mask Mixup. 

Figure~\ref{figure:2d(a)} plots the decision boundaries (red vs.\ blue) of trained models in the setting with larger noise, which corresponds to a less separable setting. We also draw the Bayes optimal boundaries with black solid lines.  All ERM, Mixup, and Mask Mixup find decision boundaries that reflect the sine-shaped optimal decision boundary. Figure~\ref{figure:2d(b)} shows the results with smaller noise, i.e., a more separable setting. The decision boundary of ERM degenerates to a linear boundary, ignoring the sine-shaped noise.
However, even though Mixup slightly distorts training,\footnote{
One may check that the optimal decision boundary becomes similar to a sine-shaped curve with a slightly smaller amplitude.} Mixup finds a nonlinear boundary that captures sine shape even when data is highly separated. 
This result is consistent with our findings that \emph{Mixup mitigates the curse of separability}, even outside our simple settings. 
Also, the decision boundary of models trained by using Mask Mixup is nonlinear, which may come from smaller sample complexity, but it  seems to suffer more distortion compared to Mixup.
\vspace{-5pt}
\subsection{Classification on CIFAR-10}
\vspace{-5pt}
We also conduct experiments on the real-world data CIFAR-10~\citep{krizhevsky2009learning}. 
To compare three methods ERM, Mixup, and CutMix, we train VGG19~\citep{simonyan2014very} and ResNet18~\citep{he2016deep} for 300 epochs on the training set with batch size 256 using SGD with weigh decay $10^{-4}$ and we choose $\alpha = 1$ for the hyperparameter of Mixup and CutMix. Also, we use a learning rate $0.1$ at the beginning and divide it by 10 after 100 and 150 epochs. Unlike linear models and 2D classification tasks, the decision boundaries of deep neural networks trained with complex data are intractable. 
Hence, following the method considered in \citet{nar2019cross,pezeshki2021gradient}, we use the norm of input perturbation\footnote{We apply the projected gradient descent attack implemented by \citet{rauber2017foolbox} to compute the perturbation on input. } required to cross the decision boundary to investigate the complex decision boundary. 

Figure~\ref{fig:CIFAR10} indicates that Mixup tends to find decision boundaries farther from overall data points than decision boundaries obtained by ERM. This is consistent with our intuition on the curse of separability and how Mixup mitigates it. In addition, the plots of CutMix are placed between the plots of ERM and Mixup. As also observed in Figure~\ref{figure:2d(b)}, we believe that the combination of distortion and smaller sample complexity results in such a trend.

\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \subfigure[VGG19]{\includegraphics[width= 0.23\textwidth]{VGG19_CutMix.pdf}}
    \subfigure[ResNet18]{\includegraphics[width= 0.23\textwidth]{ResNet18_CutMix.pdf}}
    \vspace{-10pt}
    \caption{Estimation of decision boundaries of (a) VGG19 and (b) ResNet18 trained with CIFAR-10}
    \label{fig:CIFAR10}
    \vspace{-20pt}
\end{figure}