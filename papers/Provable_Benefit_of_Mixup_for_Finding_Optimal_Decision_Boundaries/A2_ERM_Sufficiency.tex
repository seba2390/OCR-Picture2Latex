\subsection{Proof of Theorem~\ref{thm:ERM_sufficient}} \label{proof:ERM_sufficient}
Since we consider sufficiently large $\kappa$, we may assume $n \geq d$
and let $R_\kappa:=2 \lVert \w \rVert$. By Lemma~\ref{lemma:ERM_norm}, we know that $R_\kappa = \Theta(\kappa)$. Next, define a compact set $\mathcal{C}_\kappa := \{ \vw \in \mathbb{R}^d : \lVert \vw \rVert \leq R_\kappa\}$, which trivially contains $\w$. For any $\vw \in \mathbb{R}^d$ and nonzero $\vv \in \mathbb{R}^d$, we have
\begin{equation*}
    \vv^\top \nabla^2_\vw \loss (\vw) \vv = \frac{1}{n} \sum_{i=1}^n (y_i l''(\vw^\top \vx_i) (\vv^\top \vx_i)^2 + (1-y_i) l''(-\vw^\top \vx_i) (\vv^\top \vx_i)^2 ) >0,
\end{equation*}
almost surely, since $\{\vx_i\}_{i \in [n] }$ spans $\mathbb{R}^d$ almost surely.  Therefore, $\loss(\vw)$ is strictly convex on $\mathbb{R}^d$ almost surely and we conclude that  $\loss (\vw)$ has a unique minimizer $\hat{\vw}_{S}^*$ on $\mathcal{C}_\kappa$ almost surely. Note that, if $\hat{\vw}_S^*$ belongs to interior of $\mathcal{C}$, it is a unique minimizer of $\loss (\vw)$ over the entire $\mathbb{R}^d$. We prove high probability convergence of $\hat{\vw}_{S}^*$ to $\w$ using Lemma~\ref{lemma:minimizer_independent} and convert $\ell_2$ convergence into directional convergence. 
For simplicity, we define
\begin{equation*}
f_i(\vw) := y_il(\vw^\top \vx_i) + (1-y_i) l(\vw^\top \vx_i),
\end{equation*}
for each $i \in [n]$.
We start with the following claim which is useful for estimating quantities described in assumptions of Lemma~\ref{lemma:minimizer_independent} for our setting.
\begin{claim}\label{claim:ERM}
For any $t>0$, we have
\begin{equation*}
    \mathbb{E}_\kappa \left[ e^{t\lVert \vx _i \rVert}\right] \leq  \left( 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert}\right)e^{t\lVert \vmu \rVert},
\end{equation*}
for all $i \in [n] $.
\end{claim}
\begin{proof}[Proof of Claim~\ref{claim:ERM}]
By applying triangular inequality and Lemma~\ref{lemma:ineq4}, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{t \left \lVert \vx _i\right \rVert} \right] &= \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx\right \rVert} \right]\\
&\leq \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx - \vmu \right \rVert} \right] e^{t\lVert \vmu \rVert}\\
&= \mathbb{E}_{\rvz \sim N(\vzero, \kappa^{-1}t^2 \mSigma )} \left[e^{\lVert \rvz \rVert} \right] e^{t\lVert \vmu \rVert} \\
&\leq \left( 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert}\right)e^{t\lVert \vmu \rVert},
\end{align*}
for each $i \in [n]$.
\end{proof}

\paragraph{Step 1: Estimate Upper Bound of $\mathbb{E}_\kappa \left[ e^{\left|f_i(\vw) - \mathbb{E}_\kappa[f_i(\vw)]\right|}\right]$ on $\mathcal{C}_\kappa$}\quad

For any $\vw \in \mathcal{C}_\kappa$ and $i \in [n]$,
\begin{equation*}
    |f_i(\vw)| = |y_il(\vw^\top \vx_i) + (1-y_i)l(-\vw^\top \vx_i)| \leq l(-|\vw^\top \vx_i|) \leq l(-R_\kappa \lVert \vx_i \rVert).
\end{equation*}
Hence, we have $\mathbb{E}_\kappa \left[ e^{|f_i(\vw)|}\right] \leq  \mathbb{E}_\kappa \left[ 1+ e^{R_\kappa\lVert \vx_i \rVert} \right]$. By applying Claim~\ref{claim:ERM} for $t= R_\kappa$, there exists $M_\kappa'$ such that  $\mathbb{E}_\kappa \left[ e^{|f_i(\vw)|}\right] \leq  M_\kappa'$ and $M_\kappa' = \exp(\Theta(\kappa))$ since $R_\kappa = \Theta(\kappa)$ by Lemma~\ref{lemma:ERM_norm}.
By triangular inequality and Jensen's inequality, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{\left|f_i(\vw) - \mathbb{E}_\kappa \left[f_i(\vw)\right]\right|}\right] 
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_i(\vw)\right| + \left|\mathbb{E}_\kappa \left [f_i(\vw)\right]\right|}\right]\\
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_i(\vw)\right| }\right]^2\\
&\leq {M_\kappa'}^2.
\end{align*}
Defining $M_\kappa := {M_\kappa'}^2$, it follows that $M_\kappa = \exp(\Theta(\kappa))$ and $\mathbb{E}_\kappa \left[ e^{\left|f_i(\vw) - \mathbb{E}_\kappa[f_i(\vw)]\right| }\right] \leq M_\kappa$ for any $\vw \in \mathcal{C}_\kappa$.

\paragraph{Step 2: Estimate Upper Bound of $\lVert \nabla_\vw f_i(\vw) \rVert$ and $\lVert \nabla_\vw \mathbb{E}_\kappa[ f_i(\vw) ]\rVert$} \quad

For each $\vw \in \mathcal{C}_\kappa$ and $i \in [n]$, 
\begin{equation*}
\lVert \nabla_\vw f_i(\vw) \rVert = \left \lVert \nabla_\vw (y_il(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)) \right \rVert 
= \left \lVert y_il'(\vw^\top \vx_i)\vx_i - (1-y_i) l'(-\vw^\top \vx_i)\vx_i \right \rVert 
\leq  \lVert \vx_i \rVert.
\end{equation*}
The last inequality holds since $0<l'(z) <1$ for any $z\in \R$. In addition, by Lemma~\ref{lemma:gradient&hessian},
\begin{align*}
\left\lVert \nabla_\vw \mathbb{E}_\kappa [f_i(\vw)]\right \rVert &=  \left \lVert \nabla_\vw \mathbb{E}_\kappa [(y_il(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)) ]\right \rVert\\
&= \left \lVert  \mathbb{E}_\kappa [\nabla_\vw(y_il(\vw^\top \vx_i) + (1-y_i) l(-\vw^\top \vx_i)) ]\right \rVert\\
&= \lVert \mathbb{E}_\kappa [(y_il'(\vw^\top \vx_i) + (1-y_i) l'(-\vw^\top \vx_i))\vx_i ]\rVert\\
&\leq \mathbb{E}_\kappa [\lVert \vx_i \rVert].
\end{align*}
Also, applying Claim~\ref{claim:ERM} with $t = 1$, there exists $L_\kappa$ such that $\mathbb{E}_\kappa \left[ e^{\lVert \vx_i \rVert}\right]< L_\kappa$ and $L_\kappa = \Theta(1)$.


\paragraph{Step 3: Estimate Strong Convexity Constant of $\mathbb{E}_\kappa[\loss (\vw)]$ on $\mathcal{C}_\kappa$} \quad

By Lemma~\ref{lemma:gradient&hessian} and Lemma~\ref{lemma:ineq1}, for any $\vw \in \mathcal{C}_\kappa$ and unit vector $\vv \in \mathbb{R}^{d}$, we have
\begin{equation*}
    \vv^\top \nabla^2_\vw \mathbb{E}_\kappa[\loss(\vw)] \vv = \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma)}[l''(\vw^\top \rvx) (\vv^\top \rvx)^2]
    \geq \frac{1}{4} \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1}\mSigma)}[e^{-(\vw^\top \rvx)^2/2}(\vv^\top \rvx)^2].
\end{equation*}
By Lemma~\ref{lemma:ineq3}, 
\begin{align*}
    &\quad \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1}\mSigma)}[e^{-(\vw^\top \rvx)^2/2}(\vv^\top \rvx)^2]\\
    &\geq \kappa^{d/2}\lVert \mSigma \rVert ^{-d/2} \left (\kappa \lVert\mSigma^{-1}\rVert + \lVert \vw\rVert^2 \right )^{-(d+2)/2} \exp \left (-\lVert \vw\rVert^2 \lVert \mSigma \rVert \lVert \mSigma^{-1} \rVert \lVert \vmu\rVert^2 \right )\\
    &\geq \kappa^{d/2}\lVert \mSigma \rVert ^{-d/2} \left (\kappa \lVert \mSigma^{-1} \rVert + R_\kappa^2 \right )^{-(d+2)/2} \exp \left (-R_\kappa^2 \lVert \mSigma \rVert \lVert \mSigma^{-1} \rVert \lVert \vmu\rVert^2 \right ),
\end{align*}
and substituting $R_\kappa = \Theta (\kappa)$ to the RHS of the inequality above gives
\begin{equation*}
\kappa^{d/2}\lVert \mSigma \rVert ^{-d/2} \left (\kappa \lVert \mSigma^{-1} \rVert + R_\kappa^2 \right )^{-(d+2)/2} \exp \left (-R_\kappa^2 \lVert \mSigma \rVert \lVert \mSigma^{-1} \rVert \lVert \vmu\rVert^2 \right ) 
= \frac{1}{\exp(\Theta(\kappa^2))}.
\end{equation*}
Since this hold for any unit vector $\vv \in \R^d$, $\mathbb{E}_\kappa [\loss(\vw)]$ is $\alpha_\kappa$-strongly convex with $\alpha_\kappa = \frac{1}{\exp(\Theta(\kappa^2))}$ on $\mathcal{C}_\kappa$.


\paragraph{Step 4: Sample Complexity for Directional Convergence}\quad

Since $\lVert \w \rVert = \Theta(\kappa)$ and $\alpha_\kappa = \frac{1}{\exp ( \Theta(\kappa^2))}$, we assume $\kappa$ is large enough so that $\alpha_\kappa \lVert \w \rVert^2 <1$, which is quite easy to satisfy given the rate of decay in $\alpha_\kappa$. Assume the unique existence of $\hat{\vw}_S^*$ which occurs almost surely. By Lemma~\ref{lemma:minimizer_independent}, for each $0<\epsilon<1$, if 
\begin{equation*}
n \geq \frac{C_1M_\kappa}{\alpha_\kappa ^2 \lVert \w \rVert^4 \epsilon^4} \log \left(\frac{3}{\delta} \max \left\{1, \left(\frac{2 C_2d^{1/2}R_\kappa L_\kappa}{\alpha_\kappa \lVert \w\rVert^2 \epsilon^2}\right)^d \right\}   \right) = \frac{\exp(\Theta(\kappa^2)) }{\epsilon^4} \left ( 1+ \log\frac{1}{\epsilon} + \log\frac{1}{\delta} \right ),
\end{equation*}
then we have $\lVert \w- \hat{\vw}_S^* \rVert \leq  \lVert \w\rVert \epsilon$ with probability at least $1-\delta$. Also, if $\lVert \w- \hat{\vw}_S^* \rVert \leq  \lVert \w\rVert \epsilon$, then $\hat{\vw}_S^*$ belongs to interior of $\mathcal{C}_\kappa$. Hence, $\hat{\vw}_S^*$ is a minimizer of $\loss(\vw)$ over the entire $\mathbb{R}^d$. Also, we have
\begin{align*}
    \cosim \left ( \hat{\vw}_S^*, \mSigma^{-1}\vmu \right) 
    &=  \cosim \left( \hat{\vw}_S^*,\w \right)
    = \bigg (1- \sin^2  \Big(\angle\big(\hat{\vw}_S^*, \w\big) \Big ) \bigg )^{1/2}\\
    &\geq 1- \sin \Big (\angle\big(\hat{\vw}_S^*, \w\big) \Big ) = 1-\frac{\lVert \w - \hat{\vw}_S^* \rVert}{\lVert \w \rVert}\\
    &\geq 1- \epsilon.
\end{align*}
Hence, we conclude that if $n = \frac{\exp(\Omega(\kappa^2)) }{\epsilon^4}\left(1+ \log\frac{1}{\epsilon} + \log\frac{1}{\delta}\right)$, then
with probability at least $1-\delta$, the ERM loss $\loss(\vw)$ has a unique minimizer $\hat{\vw}_S^*$ and $\cosim ( \hat{\vw}_S^*, \mSigma^{-1}\vmu)\geq 1- \epsilon$. \hfill $\square$


\subsection{Proof of Lemma~\ref{lemma:minimizer_independent}}
By Lemma~\ref{lemma:independent_concentration}, there exists a universal constant $C_1>0$ such that for any fixed $\vtheta \in \mathcal{C}$ independent of the draws of $\veta_i$'s,
\begin{align}
\label{eq:lemmaminimizer_independent-2}
    &\quad \mathbb{P}\left[ \left| \hat{F}_N(\vtheta) - F(\vtheta)\right| > \frac{\alpha \epsilon^2}{8} \right]\nonumber \\
    &= \mathbb{P}\left[ \frac{1}{N} \sum_{i=1}^N f
(\vtheta, \veta_i)- \mathbb{E}_{\veta \sim \mathcal{P}}[f(\vtheta,\veta)] > \frac{\alpha \epsilon^2}{8} \right] +  \mathbb{P}\left[ \frac{1}{N} \sum_{i=1}^N f
(\vtheta, \veta_i)- \mathbb{E}_{\veta \sim \mathcal{P}}[f(\vtheta,\veta)] < -\frac{\alpha \epsilon^2}{8} \right]\nonumber \\
    &\leq 2\exp \left(-\frac{\alpha^2 \epsilon^4}{C_1 M}N\right).
\end{align}

Notice that from the given condition and Jensen's inequality, 
\begin{equation*}
    \mathbb{E}_{\veta \sim \mathcal{P}}[g(\veta)] \leq \log \mathbb{E}_{\veta \sim \mathcal{P}}\left[e^{g(\veta)}\right] < \log L <L,
\end{equation*}
and from triangular inequality, we have
\begin{align}
\label{eq:lemmaminimizer_independent-1}
 \mathbb{P}\left[ \sup_{\vtheta \in \mathcal{C}}\left\lVert \nabla_\vtheta \left( \hat{F}_N(\vtheta) - F(\vtheta) \right)\right\rVert > 3 L \right] 
 &\leq \mathbb{P}\left[ \frac{1}{N} \sum_{i=1}^N \sup_{\vtheta \in \mathcal{C}}\left\lVert \nabla_\vtheta \left( f(\vtheta, \veta_i) - \mathbb{E}[f(\vtheta, \veta_i)] \right)\right\rVert > 3 L\right]\nonumber \\
 &\leq \mathbb{P} \left[ \frac{1}{N} \sum_{i=1}^N \left( g(\veta_i) + \mathbb{E}[g(\veta_i)] \right)> 3 L \right] \nonumber \\
 &\leq \mathbb{P} \left[ \frac{1}{N} \sum_{i=1}^N \left( g(\veta_i) - \mathbb{E}[g(\veta_i)] \right)> L \right].
\end{align}
Since we have $\mathbb{E}_{\veta \sim \mathcal{P}}\left [e^{ |g(\veta) - \mathbb{E}_{\veta\sim \mathcal{
P}}[g(\veta)]|} \right] \leq \mathbb{E}_{\veta \sim \mathcal{P}}\left [e^{ g(\veta)} \right] \cdot e^{\mathbb{E}_{\veta\sim \mathcal{
P}}[g(\veta)] }<L^2$ by our assumption, we can apply Lemma~\ref{lemma:independent_concentration} to the RHS of \Eqref{eq:lemmaminimizer_independent-1}.

Therefore, we have
\begin{equation}
\label{eq:lemmaminimizer_independent-3}
 \mathbb{P}\left[ \sup_{\vtheta \in \mathcal{C}}\left\lVert \nabla_\vtheta \left( \hat{F}_N(\vtheta) - F(\vtheta) \right)\right\rVert > 3 L \right] 
 \leq \exp \left(-\frac{1}{C_1'}N\right),
\end{equation}
where $C_1'>0$ is a universal constant. Without loss of generality, we can choose $C_1 = C_1'$ that works for both \Eqref{eq:lemmaminimizer_independent-2} and \Eqref{eq:lemmaminimizer_independent-3}.

We choose $\bar{\vtheta}_1, \dots, \bar{\vtheta}_m \in \mathcal{C}$ with $m \leq \max \left\{ 1, \left(\frac{C_2 k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\}$ where $C_2$ is a universal constant and satisfies following: 
For any $\vtheta \in \mathcal{C}$, there exists $i_\vtheta \in [m]$ such that $\left \lVert \vtheta - \bar{\vtheta}_{i_\vtheta} \right \rVert<  \frac{\alpha \epsilon^2}{24  L}$. In other words, $\bar{\vtheta}_1, \dots, \bar{\vtheta}_m$ form an $\frac{\alpha \epsilon^2}{24L}$-cover of $\mathcal {C}$.

Suppose $| \hat{F}_N(\bar{\vtheta}_k) - F(\bar{\vtheta}_k) | < \frac{\alpha \epsilon^2}{8}$  for each $k \in [m]$ and $\sup_{\theta \in \mathcal{C}}\left\lVert \nabla_\vtheta \left( \hat{F}_N(\vtheta) - F(\vtheta) \right)\right\rVert < 3 L$ which implies $\hat{F}_N(\vtheta)-F(\vtheta)$ is $ 3 L$-Lipschitz. Then, for any $\vtheta \in \mathcal{C}$, we have
\begin{align*}
    &\quad \left | \hat{F}_N(\vtheta) - F(\vtheta) \right|\\
    &\leq \left | \hat{F}_N(\bar{\vtheta}_{i_\vtheta}) - F(\bar{\vtheta}_{i_\vtheta}) \right| +  \left |\left( \hat{F}_N(\vtheta) - F(\vtheta) \right) -  \left  (\hat{F}_N(\bar{\vtheta}_{i_\vtheta}) - F(\bar{\vtheta}_{i_\vtheta}) \right) \right|\\
    &\leq \frac{\alpha \epsilon^2}{8} + 3 L \lVert \vtheta - \bar{\vtheta}_{i_\vtheta} \rVert < \frac{\alpha \epsilon^2}{8} + \frac{\alpha \epsilon^2}{8} \\&= \frac{\alpha \epsilon^2}{4}.
\end{align*}
By applying union bound, we conclude
\begin{align*}
    &\quad \mathbb{P}\left[ \sup_{\vtheta \in \mathcal{C}}|\hat{F}_N(\vtheta) - F(\vtheta)| >  \frac{\alpha \epsilon^2}{4} \right] \\&\leq 2 \max \left \{1, \left(\frac{C_2k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\} \exp\left(- \frac{\alpha^2 \epsilon^4}{C_1 M}N \right) + \exp\left(-\frac{1}{C_1}N \right)\\ &\leq 3 \max \left \{1, \left(\frac{C_2 k^{1/2} D L}{\alpha \epsilon^2 }\right)^k \right\} \exp \left( - \min \left\{ \frac{\alpha^2 \epsilon^4}{C_1 M}, \frac{1}{C_1}\right \}N \right)\\
    & =  3\max \left \{1, \left(\frac{C_2 k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\} \exp \left( - \frac{\alpha^2 \epsilon^4}{C_1 M}N \right),
\end{align*}
where the last equality is due to $\alpha \epsilon^2 \leq 1$ and $M\geq 1$ which are implied by given conditions.

Suppose $ \sup_{\vtheta \in \mathcal{C}}|\hat{F}_N(\vtheta) - F(\vtheta)| <  \frac{\alpha \epsilon^2}{4}$ and $\left \lVert \hat{\vtheta}_N^* - \vtheta^* \right \rVert > \epsilon$. Then, from the strong convexity of $F(\vtheta)$, we have
\begin{align*}
    &\quad \hat{F}_N(\hat{\vtheta}_N^*) - \hat{F}_N(\vtheta^*)\\
    &= \left( \hat{F}_N(\hat{\vtheta}_N^*) - F(\hat{\vtheta}_N^*) \right) + \left( F(\vtheta^*) - \hat{F}_N(\vtheta^*)\right) + \left(  F(\hat{\vtheta}_N^*) - F(\vtheta^*) \right)\\
    &\geq -\frac{\alpha \epsilon^2 }{2} + \frac{\alpha}{2} \left \lVert \hat{\vtheta}_N^* - \vtheta^* \right \rVert^2 \\
    &>0.
\end{align*}
This is a contradiction to the fact that $\hat{\vtheta}_N$ is a minimizer of $\hat{F}_N(\vtheta)$. Hence, we have
\begin{align*}
    \mathbb{P} \left[ \left \lVert \hat{\vtheta}_N^* - \vtheta^* \right \rVert >\epsilon \right] &\leq 3 \max \left \{1, \left(\frac{C_2 k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\} \exp \left( - \frac{\alpha^2 \epsilon^4}{C_1 M}N \right),
\end{align*}
and equivalently, if 
\begin{equation*}
    N \geq \frac{C_1 M}{ \alpha^2 \epsilon^4} \log\left(\frac{3}{\delta} \max \left \{1, \left(\frac{C_2 k^{1/2} D  L}{\alpha \epsilon^2 }\right)^k \right\} \right),
\end{equation*}
then $\left \lVert \hat{\vtheta}_N^* - \vtheta^* \right \rVert <\epsilon$ with probability at least $1-\delta$.
\hfill $\square$

\subsection{Proof of Lemma~\ref{lemma:ERM_norm}}\label{proof:lemma:ERM_norm}
By Theorem~\ref{thm:expected_loss_ERM}, there exists $c_\kappa^*\geq 0$ (strict positivity of $c_\kappa^*$ will be proved here) such that $\w= c_\kappa^* \mSigma^{-1}\vmu$. For any $c\geq 0$, we have
\begin{equation*}
\mathbb{E}_\kappa[\loss(c\mSigma^{-1}\vmu)] = \mathbb{E}_{X \sim N \left(\vmu^\top \mSigma^{-1} \vmu, \kappa^{-1} \vmu^\top \mSigma^{-1} \vmu \right)}[l(c X)] = \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s\right)}[l(c X)],
\end{equation*}
where we define $s:= \vmu^\top \mSigma^{-1} \vmu$ for simplicity. By Lemma~\ref{lemma:gradient&hessian}, we have
\begin{align*}
    -\frac{\partial}{\partial c} \mathbb{E}_\kappa[\loss(c\mSigma^{-1}\vmu)] 
    &= - \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s \right)} \left[l'(cX)X \right]\\
    &= \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s \right)} \left[\frac{X}{1+e^{cX}} \right]\\
    &= \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s \right)} \left[\frac{X}{1+e^{cX}}\cdot \vone_{X\geq0} \right]+\mathbb{E}_{X \sim N \left(s, \kappa^{-1}s \right)} \left[\frac{X}{1+e^{cX}} \cdot \vone_{X<0}\right].
\end{align*}
For any $c\geq0$, $1+e^{ct} \geq 2e^{ct/2}$ for all $t \in \mathbb{R}$ by the AM-GM inequality and $1+e^{ct} \leq 2$ if $t<0$. Then, we have
\begin{align*}
    &\quad -\frac{\partial}{\partial c} \mathbb{E}[\loss(c\mSigma^{-1}\vmu)]\\
    &\leq \frac{1}{2} \left (\mathbb{E}_{X \sim N \left(s, \kappa^{-1} s \right)}[X e^{-cX/2} \cdot \vone_{X\geq 0}]+ \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s\right)}[X \cdot \vone_{X<0}]\right)\\
    &= \frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}(\kappa^{-1} s)^{1/2}}x \exp\left(-\frac{\left(x-s \right)^2}{2\kappa^{-1} s}-\frac{cx}{2}\right)dx + \frac{1}{2} \int_{-\infty}^0  \frac{1}{\sqrt{2\pi}(\kappa^{-1} s)^{1/2}}x \exp \left(-\frac{\left(x-s\right)^2}{2\kappa^{-1} s}\right)dx\\
    &=\frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}(\kappa^{-1} s)^{1/2}}x \exp\left(-\frac{\left(x-s \right)^2}{2\kappa^{-1} s}-\frac{cx}{2}\right)dx - \frac{1}{2} \int^\infty_0  \frac{1}{\sqrt{2\pi}(\kappa^{-1} s)^{1/2}}x \exp \left(-\frac{\left(x+s\right)^2}{2\kappa^{-1} s}\right)dx\\
    &= \frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}(\kappa^{-1} s)^{1/2}}x (e^{-cx/2}-e^{-2\kappa x})\exp\left(-\frac{\left(x-s\right)^2}{2\kappa^{-1}s}\right) dx.
\end{align*}
If $c > 4\kappa$, then $e^{-cx/2}-e^{-2\kappa x}<0$ for each $x>0$ and $-\frac{\partial}{\partial c} \mathbb{E}_\kappa \left[ \loss(c\Sigma^{-1}\mu) \right]<0$. Thus, $c_\kappa^* \leq 4\kappa$.

For $c \geq 0$, $1+e^{ct} \leq 2e^{ct}$ if $t\geq 0$ and $1+e^{ct} \geq 2 e^{ct/2}$ for all $t\in \mathbb{R}$ by AM-GM inequality. Therefore, we have
\begin{align*}
    &\quad -\frac{\partial}{\partial c} \mathbb{E}_\kappa[\loss(c\mSigma^{-1}\vmu)] \\
    &\geq \frac{1}{2} \left (\mathbb{E}_{X \sim N \left(s, \kappa^{-1} s\right)}[X e^{-cX} \cdot \vone_{X\geq 0}]+ \mathbb{E}_{X \sim N \left(s, \kappa^{-1} s\right)}[Xe^{-cX/2} \cdot \vone_{X<0}] \right)\\
    &= \frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}\kappa^{-1} s}x \exp\left(-\frac{\left(x-s\right)^2}{2\kappa^{-1} s}-cx\right)dx + \frac{1}{2} \int_{-\infty}^0  \frac{1}{\sqrt{2\pi}\kappa^{-1}s}x \exp\left(-\frac{\left(x-s\right)^2}{2\kappa^{-1} s}-\frac{cx}{2}\right)dx\\
    &= \frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}\kappa^{-1} s}x \exp\left(-\frac{\left(x-s\right)^2}{2\kappa^{-1} s}-cx\right)dx - \frac{1}{2} \int_0^\infty  \frac{1}{\sqrt{2\pi}\kappa^{-1}s}x \exp\left(-\frac{\left(x+s\right)^2}{2\kappa^{-1} s}+\frac{cx}{2}\right)dx\\
    &= \frac{1}{2} \int_0^\infty \frac{1}{\sqrt{2\pi}\kappa^{-1} s}x (e^{-cx}-e^{-(2\kappa-c/2)x})\exp\left(-\frac{\left(x-s\right)^2}{2\kappa^{-1} s}\right) dx.
\end{align*}
If $c < \frac{4}{3}\kappa$, $e^{-cx}-e^{-(2\kappa-c/2) x}<0$ for each $x>0$ and $\frac{\partial}{\partial c} \mathbb{E}_\kappa[\loss(c\mSigma^{-1}\vmu)]<0$. Hence, $c_\kappa^* \geq \frac{4}{3}\kappa$ and we conclude $\lVert \w\rVert = \Theta(\kappa)$.
\hfill $\square$