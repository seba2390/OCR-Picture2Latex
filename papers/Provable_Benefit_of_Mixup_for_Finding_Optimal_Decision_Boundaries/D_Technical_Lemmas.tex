\section{Technical Lemmas}\label{proof:lemmas}
In this section, we introduce several technical lemmas that previously appeared. Before moving on, we define some additional notation which will be used in this section. 
\paragraph{Notation.}
For each (random) vector $\vu \in \mathbb{R}^k$, we use $(\vu)_i$ to refer to the $i$th component of $\vu$ and for each matrix $\mM \in \mathbb{R}^{k \times k}$, we use $(\mM)_i$ to represent $i$th diagonal entry of $\mM$ for $i \in [k]$. In addition, we use $\lVert \vu \rVert_\mM$ for $\left(\vu^\top\mM \vu\right)^{1/2}$.
\subsection{The Bayes Optimal Classifier for $\mathcal{D}_\kappa$}\label{proof:Bayes_optimal}
To get the Bayes optimal classifier for $\mathcal{D}_\kappa$ for $\kappa \in (0,\infty)$, we have to solve the following problem:
\begin{equation}\label{eqn:bayes_optimal}
    \min_{\vw \in \mathbb{R}^d}  \mathbb{P}_{(\vx,y) \sim \mathcal{D}_\kappa }\left[(2y-1)\vw^\top \vx >0 \right].  
\end{equation}
From our definition of $\mathcal{D}_\kappa$ we have
\begin{align*}
\mathbb{P}_{(\vx,y) \sim \mathcal{D}_\kappa }\left[(2y-1)\vw^\top \vx >0 \right] &= \mathbb{P}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) }\left[\vw^\top \rvx>0 \right]\\
&= \mathbb{P}_{Z \sim N(0,1)} \left[\kappa^{-1/2}\lVert \vw \rVert
_\mSigma Z + \vw^\top \vmu >0 \right]\\
&= \mathbb{P} \left[ Z >  - \frac{\kappa^{1/2} \vw^\top \vmu }{\lVert \vw \rVert_\mSigma}\right].
\end{align*}
Without loss of generality, consider the solution of Problem (\ref{eqn:bayes_optimal}) with $\vw^\top \vmu = 1$. We can change Problem (\ref{eqn:bayes_optimal}) to the problem $\min_{\vw^\top \vmu = 1} \vw^\top \mSigma \vw$ and by Lemma~\ref{lemma:opt}, we have our conclusion. \hfill $\square$

\subsection{Interchanging Differentiation and Expectation}\label{appendix:leibniz}
We will introduce technical results related to interchanging differentiation and expectation. The following Lemma \ref{lemma:leibniz} is a slight variant of Leibniz's rule.
\begin{lemma}\label{lemma:leibniz}
Let $f : U \times \mathbb{R}^k \rightarrow \mathbb{R}$ be a function where $U$ is an open subset of $\mathbb{R}$. Suppose a probability distribution $\mathcal{P}$ on $\mathbb{R}^k$  satisfies the following conditions:
\begin{enumerate}
\item $\mathbb{E}_{\veta \sim \mathcal{P}}[f(\theta, \veta)] < \infty$ for all $\theta \in \mathbb{R}$.
\item For any $\theta \in U$, $\frac{\partial}{\partial \theta}f(\theta,\veta)$ exists for every $\veta \in \mathbb{R}$.
\item There is $g: \mathbb{R}^k \rightarrow \mathbb{R}$ such that $|\frac{\partial}{\partial \theta}f(\theta,\veta)| \leq g(\veta)$ for each $\theta \in U$ and $\veta \in \mathbb{R}^k$. In addition, $ \mathbb{E}_{\veta \sim \mathcal{P}}[g(\veta)] < \infty$.
\end{enumerate}
Then, $\frac{d}{d\theta} \mathbb{E}_{\veta \sim \mathcal{P}}[f(\theta, \veta)] = \mathbb{E}_{\veta \sim \mathcal{P}}\left[ \frac{\partial}{\partial \theta}f(\theta,\veta)\right]$ for all $\theta \in U$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:leibniz}]
Fix any $\theta \in U$ and let $\{h_m\}_{m \in \mathbb{N}}$ be any sequence of nonzero real numbers such that $h_m \rightarrow 0$ as $m \to \infty$ and $\theta + h_m \in U$.
Define $f_{\theta,m} : \mathbb{R} \rightarrow \mathbb{R}$ as $f_{\theta,m}(\veta) = \frac{1}{h_m}\left(f\left( \theta + h_m,\veta \right) -f\left( \theta,\veta \right) \right)$. Then, $f_{\theta,m}(\veta) \rightarrow \frac{\partial}{\partial \theta} f(\theta,\veta)$ as $m \to \infty$. 
Therefore, for large enough $m, \left | \frac{\partial}{\partial \theta} f(\theta,\veta) - f_{\theta,m}(\veta)\right|<1$ holds, which implies $|f_{\theta,m}(\veta)| \leq g(\veta)+1$ and $\mathbb{E}_{\veta \sim \mathcal{P}}[g(\veta) +1] = \mathbb{E}_{\veta \sim \mathcal{P}}[g(\veta)] +1 < \infty$. 
Then, by dominated convergence theorem,
\begin{align*}
\lim_{m \rightarrow \infty} \frac{\mathbb{E}_{\veta \sim \mathcal{P}}[f(\theta+h_m,\veta)] - \mathbb{E}_{\veta \sim \mathcal{P}} [f(\theta,\veta)]}{h_m} 
&= \lim_{m \rightarrow \infty} \frac{\mathbb{E}_{\veta \sim \mathcal{P}} [f(\theta+h_m,\veta) - f(\theta,\veta)]}{h_m}\\
&= \lim_{m \rightarrow \infty} \mathbb{E}_{\veta \sim \mathcal{P}} [f_{\theta,m}(\veta)]\\
&= \mathbb{E}_{\veta \sim \mathcal{P}} \left[\frac{\partial}{\partial \theta} f(\theta,\veta)\right].
\end{align*}
Since our choice of $\{h_m\}_{m \in \mathbb{N}}$ is arbitrary, $\frac{d}{d\theta} \mathbb{E}_{\veta \sim \mathcal{P}}[f(\theta, \veta)] = \mathbb{E}_{\veta \sim \mathcal{P}} \left[\frac{\partial}{\partial \theta} f(\theta,\veta)\right]$.
\end{proof} 

By applying Lemma~\ref{lemma:leibniz}, we can obtain the following lemma which makes us possible to investigate stationary points and strong convexity constants of expected losses in the proof of our main theorems.


\begin{lemma}\label{lemma:gradient&hessian}
For any vector $\vu \in \mathbb{R}^k$, positive definite matrix $\mM \in \mathbb{R}^{k \times k}$, functions $f :[0,1] \rightarrow [-1,1], g: [0,1] \rightarrow [0,1]$ and probability distribution $\mathcal{P}$ with support contained in $[0,1]$, we have
\begin{equation*}
    \nabla_\vw \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] \right ]
    = \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l'(\vw^\top \rvx)\rvx \right ],
\end{equation*}
and
\begin{equation*}
    \nabla_\vw^2 \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] \right ]
    = \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l''(\vw^\top \rvx) \rvx \rvx^\top] \right ].
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:gradient&hessian}]
    For each $i \in [k]$ and $\vx \in \mathbb{R}^k$, we have
    \begin{equation*}
        \left| \frac{\partial}{\partial (\vw)_i} l(\vw^\top \vx) \right| = |l'(\vw^\top \vx) (\vx)_i| = \left | \frac{(\vx)_i}{1+e^{\vw^\top \vx}} \right| \leq |(\vx)_i|.
    \end{equation*}
    Since $\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i|]<\infty$ for each $\eta \in [0,1]$, by Lemma~\ref{lemma:leibniz},
    \begin{equation*}
        \frac{\partial}{\partial (\vw)_i} \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] 
        = \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l'(\vw^\top \rvx)(\rvx)_i].
    \end{equation*}
    Also, 
    \begin{align*}
        \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i|] &\leq \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[(\rvx)_i^2]^{1/2} \\
        &= \left(g(\eta) (\mM)_i + f(\eta)^2 (\vu)_i^2 \right)^{1/2}\\
        &\leq \left((\mM)_i + (\vu)_i^2 \right)^{1/2}.
    \end{align*}
    Hence, $\mathbb{E}_{\eta \sim \mathcal{P}}\left[\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i|] \right]< \infty$ because the inner expectation is uniformly bounded for all $\eta \in [0,1]$. Applying Lemma~\ref{lemma:leibniz} again, we have
    \begin{equation*}
        \frac{\partial}{\partial (\vw)_i}\mathbb{E}_{\eta \sim \mathcal{P}}\left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta) \mM)}[l(\vw^\top \rvx)]\right ] 
        = \mathbb{E}_{\eta \sim \mathcal{P}}\left [\mathbb{E}_{\rvx \sim N(f(\eta)\vmu, g(\eta) \kappa^{-1} \mSigma)}[l'(\vw^\top \rvx)(\rvx)_i] \right ],
    \end{equation*}
    and we conclude
    \begin{equation*}
        \nabla_\vw \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\kappa^{-1}\mM)}[l(\vw^\top \rvx)] \right ]
        = \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\kappa^{-1}\mM)}[l'(\vw^\top \rvx)\rvx] \right ].
    \end{equation*}
    For each $i,j \in [k]$ and $\vx \in \mathbb{R}^k$,
    \begin{equation*}
        \left | \frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}l(\vw^\top \vx) \right|  =  \left |l''(\vw^\top \vx) (\vx)_i (\vx)_j \right|\leq \frac{|(\vx)_i(\vx)_j|}{4}.
    \end{equation*}
    Since $\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i(\rvx)_j|]<\infty$ for each $\eta \in [0,1]$, by Lemma~\ref{lemma:leibniz}, we have
    \begin{align*}
         \frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] &= \frac{\partial}{\partial (\vw)_j}\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}\left [\frac{\partial}{\partial (\vw)_i}l(\vw^\top \rvx)\right] \\
         &= \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}\left [\frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}l(\vw^\top \rvx) \right ]\\
         &= \mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l''(\vw^\top \rvx)(\rvx)_i (\rvx)_j].
    \end{align*}
    Also,  $\left| \frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] \right| \leq  \frac{1}{4}\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i(\rvx)_j|]$ and
    \begin{align*}
        \frac{1}{4}\mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[|(\rvx)_i(\rvx)_j|] \right ]
        &\leq \frac{1}{8} \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[(\rvx)_i^2 + (\rvx)_j^2] \right ]\\
        &= \frac{1}{8} \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[g(\eta) ((\mM)_i + (\mM)_j) + f(\eta)^2 ((\vu)_i^2 + (\vu)_j^2)] \right ]\\
        &\leq \frac{1}{8} ( \Tr (\mM) + \lVert \vu \rVert^2) <\infty.
    \end{align*}
By applying Lemma~\ref{lemma:leibniz} again, for each $i,j \in [k]$, 
\begin{equation*}
    \frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}\mathbb{E}_{\eta \sim \mathcal{P}}\left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] \right ] = \mathbb{E}_{\eta \sim \mathcal{P}}\left[\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}\left[\frac{\partial^2}{\partial (\vw)_j \partial (\vw)_i}l(\vw^\top \rvx)\right]\right],
\end{equation*}
and we conclude 
\begin{equation*}
    \nabla_\vw^2 \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l(\vw^\top \rvx)] \right ]
    = \mathbb{E}_{\eta \sim \mathcal{P}} \left [\mathbb{E}_{\rvx \sim N(f(\eta)\vu, g(\eta)\mM)}[l''(\vw^\top \rvx)\rvx \rvx^\top ] \right ].
\end{equation*}
\end{proof}

\subsection{Inequalities}
Let us introduce some inequalities which are used in the proof of main theorems.

The following lemma provides us computable lower bound on the strong convexity constant.
\begin{lemma} \label{lemma:ineq1}
For any $z \in \mathbb{R}$,
\begin{equation*}
    l''(z) = \frac{e^z}{(e^z+1)^2} \geq \frac{1}{4} e^{-z^2/2}.
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:ineq1}]
Define a function $f: \mathbb{R} \rightarrow \mathbb{R}$ as $f(z) = \frac{1}{2} z^2 - 2\log(1+e^z)$ for each $z \in \mathbb{R}$. Then, we have
\begin{equation*}
f'(z) = z - \frac{2 e^z}{1+e^z}, f''(z) = 1- \frac{2e^z}{(e^z+1)^2} > 0,
\end{equation*}
for each $z \in \mathbb{R}$. Hence, $f$ is a convex function and $z \mapsto -z - 2 \log 2$ is a tangent line of the graph of $f(z)$ at $z=0$. Therefore, for each $z \in \mathbb{R}$, we have
\begin{equation*}
     f(z) = \frac{1}{2}z^2 - 2 \log (1+e^z) \geq -z - 2\log 2,
\end{equation*}
which implies 
\begin{equation*}
    \frac{1}{2}z^2 \geq -z +2\log(1+e^z)-2\log 2 = -\log \left(  \frac{4e^z}{(e^z+1)^2}\right) .
\end{equation*}
Thus, we conclude $l''(z) = \frac{e^z}{(e^z+1)^2} \geq \frac{1}{4} e^{-z^2/2}$ for each $z \in \mathbb{R}$.
\end{proof}

When we get lower bounds on $\lVert \w \rVert$ and $\lVert \wmask \rVert$, the following lemma is useful.
\begin{lemma} \label{lemma:ineq2}
For any $z \in \mathbb{R}$,
\begin{equation*}
    \frac{z}{1+e^z} \geq \frac{1}{2} z - \frac{1}{4}z^2.
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:ineq2}]
Define a function $f: \mathbb{R} \rightarrow \mathbb{R}$ as $f(z) = \frac{1}{1+e^z}$ for each $z \in \mathbb{R}$. We have
\begin{equation*}
f'(z) = -\frac{e^z}{(1+e^z)^2}, f''(z) = \frac{e^z (e^z-1)}{(1+e^z)^3}.
\end{equation*}
Therefore, $f$ is convex on $[0, \infty)$ and concave on $(-\infty, 0]$. Since $z \mapsto -\frac{1}{4}z + \frac{1}{2}$ is tangent line of $f(z)$ at $z=0$, we have
\begin{equation*}
f(z) = \frac{1}{1+e^z} \geq -\frac{1}{4}z + \frac{1}{2},
\end{equation*}
for any $z \geq 0$ and
\begin{equation*}
f(z) = \frac{1}{1+e^z} \leq -\frac{1}{4}z + \frac{1}{2},
\end{equation*}
for any $z \leq 0$. By multiplying $z$ to the inequalities above, we have our conclusion.
\end{proof}
Together with Lemma~\ref{lemma:ineq1}, the following lemma provide us lower bounds on strong convexity constant of expected loss $\mathbb{E}_\kappa [\loss (\vw)]$, $\mathbb{E}_\kappa [\lossmix (\vw)]$ and $\mathbb{E}_\kappa [\lossmask (\vw)]$.  
\begin{lemma} \label{lemma:ineq3}
Let $\vu \in \mathbb{R}^k$ be a vector and $\mM \in \mathbb{R}^{k\times k}$ be a positive definite matrix. For any vector $\vw \in \mathbb{R}^k$ and unit vector $\vv \in \mathbb{R}^k$, we have
\begin{align*}
\mathbb{E}_{\rvx \sim N(\vu,\mM)}\left [(\vv^\top \rvx)^2e^{- (\vw^\top \rvx)^2/2} \right]
\geq &\max \left \{ (\lVert \mM^{-1}\rVert +\lVert \vw \rVert^2)^{-1} ,\frac{1}{2} (\vv^\top \vu)^2 - \lVert \mM\rVert^2 \lVert \vw\rVert^4 \lVert \vu \rVert^2  \right \}\\
&\quad \cdot \left (\lVert \mM \rVert (\lVert \mM^{-1}\rVert + \lVert \vw\rVert^2 ) \right )^{-k/2} \exp \left( -\lVert \vw \rVert^2 \lVert \mM^{-1} \rVert \lVert \mM \rVert \lVert \vu\rVert^2\right).
\end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:ineq3}]
By changing expectation to integral form, we have
\begin{equation*}
\mathbb{E}_{\rvx \sim N(\vu,\mM)} \left[ (\vv^\top \rvx)^2 e^{-(\vw^\top \rvx)^2/2}\right]= \int_{\mathbb{R}^k} (2 \pi)^{-k/2} \underbrace{\det(\mM)^{-1/2}}_{\mathrm{(a)}} (\vv^\top \vx)^2 \underbrace{\exp \left(-\frac{1}{2} \left(\lVert \vx-\vu\rVert_{\mM^{-1}}^2 + (\vw^\top \vx)^2 \right)\right)}_{\mathrm{(b)}} d\vx.
\end{equation*}
We can rewrite the term (a) as
\begin{align*}
    \det (\mM)^{-1/2} &= \det(\mM(\mM^{-1} +\vw\vw^\top)(\mM^{-1} +\vw\vw^\top)^{-1})^{-1/2}\\
    &= \det ((\mI+\mM\vw\vw^\top)(\mM^{-1} +\vw\vw^\top)^{-1})^{-1/2} \\
    &= \det(\mI+\mM\vw\vw^\top)^{-1/2} \det (\mM^{-1} + \vw\vw^\top )^{1/2},
\end{align*}
and the term (b) as
\begin{align*}
    &\quad \exp \left(-\frac{1}{2} \left(\lVert \vx-\vu\rVert_{\mM^{-1}}^2 + (\vw^\top \vx)^2 \right)\right)\\
    &= \exp \left(-\frac{1}{2} \left( \lVert \vx \rVert_{\mM^{-1} + \vw\vw^\top} - 2\vu^\top \mM^{-1}\vx + \lVert \vu \rVert_{\mM^{-1}} \right) \right)\\
    &=  \exp \left(-\frac{1}{2} \left( \lVert \vx \rVert_{\mM^{-1} + \vw\vw^\top}^2 - 2\vu^\top \mM^{-1} (\mM^{-1} + \vw\vw^\top)^{-1} (\mM^{-1} + \vw\vw^\top) \vx + \lVert \vu \rVert_{\mM^{-1}}^2 \right) \right)\\
    &= \exp \left(-\frac{1}{2} \left( \lVert \vx \rVert_{\mM^{-1} + \vw\vw^\top}^2 - 2\vu^\top  (\mI + \vw\vw^\top \mM)^{-1} (\mM^{-1} + \vw\vw^\top) \vx + \lVert \vu \rVert_{\mM^{-1}}^2 \right) \right)\\
    &= \exp \left( -\frac{1}{2} \left ( \left \lVert \vx- \left (\mI+\mM\vw\vw^\top \right)^{-1} \vu \right \rVert_{\mM^{-1}+ \vw\vw^\top}^2 
    + \lVert \vu \rVert_{\mM^{-1}}^2 - \left \lVert \left(\mI+\mM\vw\vw^\top \right)^{-1}\vu  \right \rVert_{\mM^{-1} + \vw\vw^\top}^2 \right)\right)\\
    &= \exp \left(-\frac{1}{2} \left \lVert \vx- \left (\mI+\mM\vw\vw^\top \right )^{-1}u \right \rVert_{\mM^{-1} + \vw \vw^\top}^2 \right) \cdot \exp \bigg(-\frac{1}{2}  \underbrace{\left(\lVert \vu \rVert_{\mM^{-1}}^2 - \left \lVert  (\mI+ \mM \vw\vw^\top )^{-1} \vu\right \rVert_{\mM^{-1} + \vw\vw^\top}^2 \right)}_{\mathrm{(c)}}\bigg).
\end{align*}
Also, the term (c) can be simplified as
\begin{align*}
&\quad \lVert \vu \rVert_{\mM^{-1}}^2 - \lVert (\mI+\mM\vw\vw^\top)^{-1}\vu \rVert_{\mM^{-1} + \vw\vw^\top}^2\\
&= \vu^\top \mM^{-1} \vu - ((\mI+\mM\vw\vw^\top)^{-1}\vu)^\top (\mM^{-1} + \vw\vw^\top )((\mI+\mM\vw\vw^\top)^{-1}\vu)\\
&= \vu^\top \mM^{-1} \vu - (\vu^\top (\mI+\vw\vw^\top \mM)^{-1}
) (\mM^{-1} (\mI + \mM \vw\vw^\top ))((\mI+\mM\vw\vw^\top)^{-1}\vu)\\
&= \vu^\top \mM^{-1} \vu - \vu^\top (\mI+\vw\vw^\top \mM)^{-1} \mM^{-1}\vu\\
&= \vu^\top (\mI+\vw\vw^\top \mM) (\mI+\vw\vw^\top \mM)^{-1} \mM^{-1} \vu - \vu^\top (\mI+ \vw\vw^\top \mM)^{-1}\mM^{-1}\vu\\
&= \vu^\top (\vw\vw^\top \mM)(\mI+\vw\vw^\top \mM)^{-1} \mM^{-1}\vu\\
&=  \vu^\top (\vw\vw^\top\mM)(\mM+\mM \vw\vw^\top \mM)^{-1}\vu\\
&= \vu^\top (\vw\vw^\top \mM ) ((\mI + \mM\vw\vw^\top) \mM)^{-1} \vu\\
&= \vu^\top \vw\vw^\top (\mI+ \mM\vw\vw^\top)^{-1} \vu.
\end{align*}
Therefore, we have 
\begin{align*}
&\quad \mathbb{E}_{\rvx \sim N(\vu,\mM)} \left[ (\vv^\top \rvx)^2 e^{-(\vw^\top \rvx)^2/2}\right]\\
&= \underbrace{ \int_{\mathbb{R}^d} (2\pi)^{-d/2} \det(\mM^{-1} + \vw \vw^\top)^{1/2} (\vv^\top \vx)^2 \exp\left( -\frac{1}{2} \left(\lVert \vx-(\mI+\mM\vw\vw^\top)^{-1}\vu\right)\rVert _{\mM^{-1}+ \vw\vw^\top}^2 \right) d\vx}_{\mathrm{(d)}}\\
& \quad \cdot \det (\mI+\mM\vw\vw^\top)^{-1/2} \exp \left( -\frac{1}{2} \vu^\top \vw\vw^\top  (\mI+\mM\vw\vw^\top)^{-1} \vu \right).
\end{align*}
By changing integral into expectation, we have
\begin{align*}
\mathrm{(d)} &= \mathbb{E}_{\rvx \sim N\left( (\mI+\mM\vw\vw^\top)^{-1}\vu, (\mM^{-1}+ \vw\vw^\top)^{-1} \right)}\left[\left(\vv^\top \rvx\right)^2\right]\\
&= \mathbb{E}_{\rz \sim N\left( \vv^\top(\mI+\mM\vw\vw^\top)^{-1}\vu, \vv^\top(\mM^{-1}+ \vw\vw^\top)^{-1}\vv \right)}\left[\rz^2\right].
\end{align*}
From $\mathbb{E}_{\rz \sim N(m, \sigma^2)}[\rz^2] = m^2+\sigma^2$ for each $m \in \mathbb{R}, \sigma >0$, note that
\begin{align*}
&\quad \mathbb{E}_{\rz \sim N\left( \vv^\top(\mI+\mM\vw\vw^\top)^{-1}\vu, \vv^\top(\mM^{-1}+ \vw \vw^\top)^{-1}\vv \right)}[\rz^2]\\
&\geq \vv^\top(\mM^{-1}+ \vw\vw^\top)^{-1}\vv \geq \lVert \mM^{-1} + \vw \vw^\top\rVert^{-1}\\
&\geq \left( \lVert \mM^{-1} \rVert + \lVert \vw\rVert^2 \right)^{-1},
\end{align*}
and
\begin{align*}
&\mathbb{E}_{\rz \sim N\left( \vv^\top(\mI+\mM\vw\vw^\top)^{-1}\vu, \vv^\top(\mM^{-1}+ \vw \vw^\top)^{-1} \vv \right)}[\rz^2]\\
&\geq ( \vv^\top(\mI+\mM\vw\vw^\top)^{-1}\vu)^2 = (\vv^\top \vu - \vv^\top \mM\vw\vw^\top (\mI + \mM \vw \vw^\top)^{-1}\vu)^2\\ 
&\geq \frac{1}{2} (\vv^\top \vu)^2 - (\vv^\top \mM \vw\vw^\top (\mI + \mM \vw \vw^\top)^{-1} \vu)^2 \\
&\geq \frac{1}{2}  (\vv^\top \vu)^2 - \lVert \mM \vw \vw^\top \rVert^2 \lVert(\mI + \mM \vw \vw^\top)^{-1} \rVert \lVert \vu \rVert^2\\
&\geq \frac{1}{2} (\vv^\top \vu)^2 - \lVert \mM\rVert^2 \lVert \vw\rVert^4 \lVert \vu \rVert^2.
\end{align*}
Also,
\begin{equation*}
\det(\mI+\mM \vw \vw^\top) = \det(\mM) \det(\mM^{-1} + \vw \vw^\top) \leq (\lVert \mM\rVert\lVert \mM^{-1} + \vw\vw^\top\rVert)^k \leq (\lVert \mM\rVert (\lVert \mM^{-1}\rVert + \lVert \vw\rVert^2))^k,
\end{equation*}
and
\begin{align*}
    \vu^\top \vw\vw^\top (\mI+\mM\vw\vw^\top )^{-1} \vu  &= \vu^\top \vw\vw^\top  (\mM^{-1}+\vw\vw^\top)^{-1} \mM^{-1} \vu\\
    &\leq \lVert \vw\vw^\top  (\mM^{-1}+\vw\vw^\top)^{-1}\mM^{-1} \rVert \lVert \vu \rVert^2\\
    &\leq \lVert \vw\vw^\top \rVert \lVert (\mM^{-1}+\vw\vw^\top)^{-1} \rVert \lVert \mM^{-1} \rVert  \lVert \vu \rVert^2\\
    &\leq \lVert \vw \rVert^2  \lVert \mM \rVert \lVert \mM^{-1} \rVert\lVert \vu\rVert^2.
\end{align*}
Hence, we have our conclusion.
\end{proof}

The following lemma makes us obtain $M$ value in Lemma~\ref{lemma:minimizer_independent} and Lemma~\ref{lemma:minimizer_dependent} when we prove the sample complexity results.

\begin{lemma} \label{lemma:ineq4}
Let $\mM \in \mathbb{R}^{k\times k}$ be a positive definite matrix. Then, we have
\begin{equation*}
    \mathbb{E}_{\rvz \sim N(\vzero,\mM)} \left[ e^{\lVert \rvz \rVert}\right] \leq e^{4 \lVert \mM\rVert} + 2^{k/2}.
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:ineq4}]
We have
\begin{align*}
\mathbb{E}_{\rvz \sim N(\vzero,\mM)}\left[ e^{\lVert \rvz \rVert}\right] &= \mathbb{E}_{\rvz \sim N(\vzero,\mM)}\left[ e^{\lVert \rvz \rVert} \vone_{\lVert \rvz \rVert \leq 4\lVert \mM \rVert}\right] +  \mathbb{E}_{\rvz \sim N(\vzero,\mM)}\left[ e^{\lVert \rvz \rVert} \vone_{\lVert \rvz \rVert > 4 \lVert \mM \rVert}\right]\\
&\leq e^{4 \lVert \mM \rVert} + \mathbb{E}_{Z \sim N(\vzero,\mM)}\left[ e^{\frac{1}{4} \lVert \mM \rVert^{-1} \lVert \rvz \rVert^2}\right],
\end{align*}
and
\begin{align*}
\mathbb{E}_{\rvz \sim N(\vzero,\mM)}\left[ e^{\frac{1}{4} \lVert \mM\rVert^{-1} \lVert \rvz \rVert^2}\right] &= \int_{\mathbb{R}^k}  (2 \pi)^{-k/2} \det (\mM)^{-1/2} \exp\left(-\frac{1}{2} \vx^\top \left(\mM^{-1}-\frac{1}{2}\lVert \mM\rVert^{-1} \mI\right) \vx\right)d\vx\\
&= \det\left(\mM^{-1}-\frac{1}{2}\lVert \mM \rVert^{-1}\mI\right)^{-1/2}\det (\mM)^{-1/2} = \det\left(\mI - \frac{1}{2} \lVert M \rVert^{-1} \right)^{-1/2} \\
& \leq \left \lVert \left(\mI - \frac{1}{2}\lVert \mM\rVert^{-1}\mM \right)^{-1} \right \rVert^{k/2} \leq 2^{k/2} .
\end{align*}
Hence, we have our conclusion.
\end{proof}

Lastly, we introduce the lemma used in showing uniform convergence of $\mathbb{E}_\kappa[\lossmask(\vw)]$ to $\mathbb{E}_\infty[\lossmask(\vw)]$ as $\kappa \rightarrow \infty$.
\begin{lemma}\label{lemma:ineq5}
For each $m \in \mathbb{R}$ and $\sigma >0$, 
\begin{equation*}
0 \leq \mathbb{E}_{X \sim N(m, \sigma^2)}[l(X)] - l(m) \leq \sigma.
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:ineq5}]
Since $l(\cdot)$ is convex, by Jensen's inequality, we have
\begin{equation*}
    \mathbb{E}_{X \sim N(m, \sigma^2)}[l(X)] \geq l(\mathbb{E}_{X \sim N(m, \sigma^2)}[X])  = l(m).
\end{equation*}
Also,  we have
\begin{equation*}
    l(m) - \mathbb{E}_{X \sim N(m, \sigma^2)}[l(X)] = \mathbb{E}_{X \sim N(m, \sigma^2)}[l(m)-l(X)] \geq \mathbb{E}_{X \sim N(m, \sigma^2)}[l'(X) (m-X)] \geq -\mathbb{E}[|X-m|],
\end{equation*}
where the last inequality used $|l'(z)|\leq 1$ for all $z \in \mathbb{R}$.
By Cauchy-Schwartz inequality, $\mathbb{E}_{X \sim N(m, \sigma^2)}[|X-m|] \leq \mathbb{E}_{X \sim N(m, \sigma^2)}[(X-m)^2]^{1/2} = \sigma$. Thus, we have our conclusion.
\end{proof}

\subsection{Concentration Bounds}
We introduce concentration bounds for i.i.d. random variables which we use in the proof of Lemma~\ref{lemma:minimizer_independent}.
\begin{lemma}\label{lemma:independent_concentration}
Let $X_1, \dots, X_N \overset{\mathrm{i.i.d.}}{\sim} \mathcal{P}$ where $\mathcal{P}$ is a probability distribution on $\mathbb{R}$. Suppose $\mathbb{E}_{X \sim \mathcal{P}}\left[e^{|X - \mathbb{E}_{X \sim \mathcal{P}}[X]|}\right]\leq M$ for some constant $M>0$. Then, for any $0<\epsilon<1$, 
\begin{equation*}
    \mathbb{P} \left [ \frac{1}{N} \sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] > \epsilon \right] 
    \leq \exp\left ( -\frac{C \epsilon^2 N}{M} \right ),
\end{equation*}
and
\begin{equation*}
    \mathbb{P} \left [ \frac{1}{N} \sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] < -\epsilon \right] 
    \leq \exp\left ( -\frac{C \epsilon^2 N}{M} \right ),
\end{equation*}
where $C >0$ is a universal constant.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:independent_concentration}]
From our definition of $M$, we have $M \geq 1$. Choose $t = \frac{\epsilon}{16M}$, then we have $0 < t <\frac{1}{2}$ since $0 < \epsilon < 1$ and $M \geq 1$. From Chernoff bound, we have
\begin{equation*}
\mathbb{P}\left[ \frac{1}{N}\sum_{i=1}^N X_i - \mathbb{E}_{X \sim \mathcal{P}}[X] > \epsilon \right] \leq \mathbb{E}\left[e^{t\left(\sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] \right)}\right]e^{-t N \epsilon} ,
\end{equation*}
and
\begin{equation*}
\mathbb{P}\left[ \frac{1}{N}\sum_{i=1}^N X_i - \mathbb{E}_{X \sim \mathcal{P}}[X] < -\epsilon \right] \leq \mathbb{E}\left[e^{t\left(\sum_{i=1}^N \mathbb{E}_{X\sim \mathcal{P}}[X] - X_i  \right)}\right]e^{-t N \epsilon}.
\end{equation*}
Since $X_1, \dots, X_N \overset{\mathrm{i.i.d.}}{\sim} \mathcal{P}$, we have
\begin{equation*}
\mathbb{E}\left[e^{t\left(\sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] \right)}\right]e^{-t N \epsilon} = \mathbb{E}\left[e^{t\sum_{i=1}^N\left( X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] -\epsilon \right)}\right] = \mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( X - \mathbb{E}_{X\sim \mathcal{P}}[X] -\epsilon \right)}\right]^N,
\end{equation*}
and
\begin{equation*}
\mathbb{E}\left[e^{t\left(\sum_{i=1}^N \mathbb{E}_{X\sim \mathcal{P}}[X] - X_i  \right)}\right]e^{-t N \epsilon} = \mathbb{E}\left[e^{t\sum_{i=1}^N\left(\mathbb{E}_{X\sim \mathcal{P}}[X] - X_i -\epsilon \right)}\right] = \mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( \mathbb{E}_{X\sim \mathcal{P}}[X]-X -\epsilon \right)}\right]^N.
\end{equation*}
For each $x \in \mathbb{R}$, $e^x \leq 1+x + \frac{1}{2}x^2 e^{|x|}$. Thus,
\begin{align*}
 &\quad \mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( X - \mathbb{E}_{X\sim \mathcal{P}}[X] -\epsilon \right)}\right]\\
 &\leq \mathbb{E}_{X \sim \mathcal{P}} \left[ 1 + t\left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right) + \frac{t^2}{2} \left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right)^2 e^{\left|t\left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right) \right|} \right]\\
 &=1 - \epsilon t + \frac{t^2}{2} \mathbb{E}_{X \sim \mathcal{P}} \left [\left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right)^2 e^{\left|t \left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right) \right|}  \right],
\end{align*}
and
\begin{align*}
 &\quad \mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( \mathbb{E}_{X\sim \mathcal{P}}[X]-X -\epsilon \right)}\right]\\
 &\leq \mathbb{E}_{X \sim \mathcal{P}} \left[ 1 + t\left(\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right) + \frac{t^2}{2} \left(\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right)^2 e^{\left|t\left(\mathbb{E}_{X \sim \mathcal{P}}[X]- X-\epsilon \right) \right|} \right]\\
 &=1 - \epsilon t + \frac{t^2}{2} \mathbb{E}_{X \sim \mathcal{P}} \left [\left(\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right)^2 e^{\left|t \left( \mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right) \right|}  \right].
\end{align*}
Also, since $x^2 \leq 4 e^{ |x|/2}$ for each $x \in \mathbb{R}$, we have
\begin{align*}
&\quad \mathbb{E}_{X \sim \mathcal{P}} \left [\left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right)^2 e^{\left|t\left(X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right) \right|}  \right] \\
&\leq 4 \mathbb{E}_{X \sim \mathcal{P}} \left[ e^{\left(t+\frac{1}{2}\right) \left|X - \mathbb{E}_{X \sim \mathcal{P}}[X]-\epsilon \right|}  \right]
\leq 4 \mathbb{E}_{X \sim \mathcal{P}} \left[ e^{ \left|X- \mathbb{E}_{X \sim \mathcal{P}}[X] \right| +\epsilon }  \right] \\
&\leq 16M,
\end{align*}
and
\begin{align*}
&\quad \mathbb{E}_{X \sim \mathcal{P}} \left [\left(\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right)^2 e^{\left|t\left(\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right) \right|}  \right] \\
&\leq 4 \mathbb{E}_{X \sim \mathcal{P}} \left[ e^{\left(t+\frac{1}{2}\right) \left|\mathbb{E}_{X \sim \mathcal{P}}[X]-X-\epsilon \right|}  \right]
\leq 4 \mathbb{E}_{X \sim \mathcal{P}} \left[ e^{ \left|X- \mathbb{E}_{X \sim \mathcal{P}}[X] \right| +\epsilon }  \right] \\
&\leq 16M.
\end{align*}
Therefore, by substituting $t = \frac{\epsilon}{16M}$ we get
\begin{equation*}
\mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( X - \mathbb{E}_{X\sim \mathcal{P}}[X] -\epsilon \right)}\right]  \leq 1- \epsilon t + 8Mt^2 =  1- \frac{\epsilon^2}{32 M} \leq \exp \left (-\frac{\epsilon^2}{32M} \right),
\end{equation*}
and
\begin{equation*}
\mathbb{E}_{X \sim \mathcal{P}}\left[e^{t\left( \mathbb{E}_{X\sim \mathcal{P}}[X]-X -\epsilon \right)}\right]  \leq 1- \epsilon t + 8Mt^2 =  1- \frac{\epsilon^2}{32 M} \leq \exp \left (-\frac{\epsilon^2}{32M} \right).
\end{equation*}
We conclude
\begin{equation*}
    \mathbb{P} \left [ \frac{1}{N} \sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] > \epsilon \right] \leq \exp\left ( -\frac{C \epsilon^2 N}{M} \right ),
\end{equation*}
and
\begin{equation*}
    \mathbb{P} \left [ \frac{1}{N} \sum_{i=1}^N X_i - \mathbb{E}_{X\sim \mathcal{P}}[X] <- \epsilon \right] \leq \exp\left ( -\frac{C \epsilon^2 N}{M} \right ).
\end{equation*}
where $C = \frac{1}{32}$.
\end{proof}

We extend Lemma~\ref{lemma:independent_concentration} to non i.i.d. setting random variables with special types of dependency using the following two technical lemmas.
\begin{lemma}\label{lemma:partition}
There are disjoint sets $P_1, \dots, P_m \subset [N] \times [N]$ such that $[N] \times [N]  = \bigcup_{i=1}^m P_i \cup \{(1,1), \dots, (N,N) \}$ and 
\begin{align*}
    m = 
    \begin{cases}
    2N &\text{if $N$ is odd},\\
    2(N-1) & \text{if $N$ is even},
    \end{cases}
    \quad
    |P_k| = 
    \begin{cases}
    \frac{N-1}{2} & \text{if $N$ is odd},\\
    \frac{N}{2} & \text{if $N$ is even},
    \end{cases}
    \text{ for all } k \in [m].
\end{align*}
That is, $P_1, \dots, P_m$ and $\{(1,1), \dots, (N,N)\}$ together form a partition of $[N] \times [N]$.
Furthermore, for each $k \in [m]$ and for any distinct $(i_1, j_1), (i_2, j_2) \in P_k$, 
$\{i_1\} \cup \{j_2 \}$ and $\{i_2\} \cup \{j_2\}$ are disjoint.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:partition}]\mbox{}\\
\textbf{Case 1: $N$ is odd}\\
For each $k \in [N]$, define
\begin{align*}
    P_{2k-1} &= \{(i,j) \mid i+j \equiv k ~(\bmod ~N), i<j\},\\ 
    P_{2k} &= \{(i,j) \mid i+j \equiv k ~(\bmod ~N), i>j \}.
\end{align*}
It can be easily checked that these $P_1, \dots, P_{2N}$ are what we desired.\\
\textbf{Case 2: $N$ is even}\\
For each $k\in [N-1]$, there is unique $i_k\in [N-1]$ such that $2i_k \equiv k ~(\bmod ~(N-1))$. For each $k \in [N-1]$, define
\begin{align*}
    P_{2k-1} &= \{(i,j) \mid i+j \equiv k ~(\bmod ~(N-1)), i<j\} \cup \{(i_k,N)\},\\
    P_{2k} &= \{(i,j) \mid i+j \equiv k ~(\bmod ~(N-1)), i>j \}\cup \{(N,i_k)\},
\end{align*}
and it can be easily checked that these $P_1, \dots, P_{2(N-1)}$ are what we desired. 
\end{proof}
This is a generalized version of Cauchy-Schwartz inequality.
\begin{lemma}\label{lemma:cauchy}
Suppose $X_1, \dots, X_k$ are nonnegative random variables. Then, 
\begin{equation*}
    \mathbb{E}\left [\prod_{i=1}^k X_i \right ] \leq \left( \prod_{i=1}^k \mathbb{E}\left[X_i^k\right]\right)^{\frac{1}{k}}.
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:cauchy}]
We prove this by using induction on $k$. Note that the case $k=1$ is trivial and $k=2$ is Cauchy-Schwartz inequality. Suppose Lemma~\ref{lemma:cauchy} holds for $k=m$. Let $X_1, \dots, X_{m+1}$ be nonnegative random variables. By H\"older inequality, 
\begin{equation*}
    \mathbb{E}[X_1X_2 \cdots X_{m+1}] \leq \mathbb{E}\left[(X_1 \cdots X_m)^{\frac{m+1}{m}}\right]^\frac{m}{m+1}\mathbb{E}\left[X_{m+1}^{m+1}\right]^{\frac{1}{m+1}}.
\end{equation*}
From the induction hypothesis, we have
\begin{equation*}
    \mathbb{E} \left [(X_1 \cdots X_m)^{\frac{m+1}{m}} \right ] \leq \left( \prod_{i=1}^m \mathbb{E}\left [\left (X_i^\frac{m+1}{m}\right)^m \right] \right)^\frac{1}{m}= \left( \prod_{i=1}^m \mathbb{E}\left [X_i^{m+1} \right] \right)^\frac{1}{m}.
\end{equation*}
Therefore, we have 
\begin{equation*}
    \mathbb{E}[X_1X_2 \cdots X_{m+1}] \leq \left( \prod_{i=1}^{m+1} \mathbb{E}\left [X_i^{m+1} \right] \right)^\frac{1}{m+1}.
\end{equation*}
By the principle of mathematical induction, our conclusion holds for all $k \in \mathbb{N}$.
\end{proof}

Using the two lemmas above, we prove the following lemma which is used in the proof of Lemma~\ref{lemma:minimizer_dependent}.
\begin{lemma}\label{lemma:dependent_concentration}
Let $\{X_{i,j}\}_{i,j\in [N]}$ be real-valued random variables satisfy followings.
\begin{itemize}
\item $\mathbb{E}\left[ e^{|X_{i,j} - \mathbb{E}[X_{i,j}]|} \right] \leq M$ for some $M>0$.
\item If $\{ i_1\} \cup \{ j_1\}$ and $\{i_2\} \cup \{j_1\}$ are disjoint for $i_1, i_2, j_1, j_2 \in [n]$, then $X_{i_1,j_1}$ and $X_{i_2,j_2}$ are independent.
\end{itemize}
Then, for any $0<\epsilon<1$,
\begin{equation*}
    \mathbb{P} \left[ \frac{1}{N^2}\sum_{i,j=1}^N (X_{i,j} - \mathbb{E}[X_{i,j}])  >\epsilon \right] 
    \leq \exp\left (- \frac{C'\epsilon^2 N}{M} \right ),
\end{equation*}
and
\begin{equation*}
    \mathbb{P} \left[ \frac{1}{N^2}\sum_{i,j=1}^N (X_{i,j} -  \mathbb{E}[X_{i,j}]) < -\epsilon \right] 
    \leq \exp \left ( -\frac{C'\epsilon^2 N}{M} \right ),
\end{equation*}
where $C'>0$ is a universal constant.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:dependent_concentration}]\label{proof:dependent_concentration}
From Chernoff bound, we have
\begin{equation*}
\mathbb{P} \left[\frac{1}{N^2} \sum_{i,j=1}^N (X_{i,j}-\mathbb{E}[X_{i,j}]) > \epsilon \right] \leq  \mathbb{E}\left [e^{t \sum_{i,j=1}^N (X_{i,j} - \mathbb{E}[X_{i,j}] - \epsilon)} \right ],
\end{equation*}
and
\begin{equation*}
\mathbb{P} \left [\frac{1}{N^2} \sum_{i,j=1}^N (X_{i,j}-\mathbb{E}[X_{i,j}]) < -\epsilon \right] \leq  \mathbb{E}\left [e^{t \sum_{i,j=1}^N ( \mathbb{E}[X_{i,j}] - X_{i,j} - \epsilon)} \right ],
\end{equation*}
for any $t>0$.
We would like to get an upper bound on $\mathbb{E} \left [e^{t \sum_{i,j=1}^N (X_{i,j} - \mathbb{E}[X_{i,j}] - \epsilon)} \right ]$ and $\mathbb{E} \left [e^{t \sum_{i,j=1}^N ( \mathbb{E}[X_{i,j}]-X_{i,j} - \epsilon)} \right ]$, but the problem is that $\{X_{i,j}\}_{i,j \in [N]}$ are not independent of one another. We overcome this obstacle by applying the two lemmas above.

For our $N$, consider $m$ and $P_1, \dots, P_m \subset [N] \times [N]$ that we can obtain from Lemma~\ref{lemma:partition}. 
From our definition of $M$, we have $M \geq 1$ and then let $t = \frac{\epsilon}{16(m+1)M}$. Since we have $0 < \epsilon < 1$ and $M\geq 1$, we can check that $0 < (m+1)t < \frac{1}{2}$.

By Lemma~\ref{lemma:cauchy}, we have
\begin{align*}
    &\quad \mathbb{E} \left[ e^{t \sum_{i,j=1}^N (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)} \right] \\
    &= \mathbb{E}\left[e^{t \sum_{i=1}^N (X_{i,i} - \mathbb{E}[X_{i,i}]-\epsilon)}\prod_{k=1}^m e^{t \sum_{(i,j)\in P_k} (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)}  \right]\\
    &\leq \left( \mathbb{E}\left[e^{(m+1)t \sum_{i=1}^N (X_{i,i} - \mathbb{E}[X_{i,i}]-\epsilon)} \right] \prod_{k=1}^m \mathbb{E} \left[e^{(m+1)t \sum_{(i,j)\in P_k} (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon))}  \right]\right)^{\frac{1}{m+1}}\\
    &= \left( \prod_{i=1}^N \mathbb{E}\left[e^{(m+1)t(X_{i,i} - \mathbb{E}[X_{i,i}]-\epsilon)} \right] \prod_{k=1}^m \left( \prod_{(i,j) \in P_k} \mathbb{E} \left[e^{(m+1)t (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)}  \right]\right)\right)^{\frac{1}{m+1}}\\
    &= \left(\prod_{i,j=1}^N \mathbb{E}\left[e^{(m+1)t (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)} \right]\right)^{\frac{1}{m+1}},
\end{align*}
and 
\begin{align*}
    &\quad \mathbb{E} \left[ e^{t \sum_{i,j=1}^N ( \mathbb{E}[X_{i,j}]-X_{i,j}-\epsilon)} \right] \\
    &= \mathbb{E}\left[e^{t \sum_{i=1}^N ( \mathbb{E}[X_{i,i}]-X_{i,i}-\epsilon)}\prod_{k=1}^m e^{t \sum_{(i,j)\in P_k} ( \mathbb{E}[X_{i,j}]-X_{i,j}-\epsilon)}  \right]\\
    &\leq \left( \mathbb{E}\left[e^{(m+1)t \sum_{i=1}^N ( \mathbb{E}[X_{i,i}]-X_{i,i}-\epsilon)} \right] \prod_{k=1}^m \mathbb{E} \left[e^{(m+1)t \sum_{(i,j)\in P_k} ( \mathbb{E}[X_{i,j}]-X_{i,j}-\epsilon))}  \right]\right)^{\frac{1}{m+1}}\\
    &= \left( \prod_{i=1}^N \mathbb{E}\left[e^{(m+1)t( \mathbb{E}[X_{i,i}]-X_{i,i}-\epsilon)} \right] \prod_{k=1}^m \left( \prod_{(i,j) \in P_k} \mathbb{E} \left[e^{(m+1)t ( \mathbb{E}[X_{i,j}]-X_{i,j}-\epsilon)}  \right]\right)\right)^{\frac{1}{m+1}}\\
    &= \left(\prod_{i,j=1}^N \mathbb{E}\left[e^{(m+1)t ( \mathbb{E}[X_{i,j}]-X_{i,j}-\epsilon)} \right]\right)^{\frac{1}{m+1}}.
\end{align*}
For each $x \in \mathbb{R}, e^x \leq 1+x+\frac{1}{2}x^2 e^{|x|}$and $x^2 \leq 4 e^{|x|/2}$. Thus, for each $(i,j) \in [N] \times [N]$, we have
\begin{align*}
    &\quad \mathbb{E}\left[e^{(m+1)t (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)} \right]\\
    &\leq 1 -\epsilon (m+1) t+\frac{1}{2}(m+1)^2t^2\mathbb{E}\left[(X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)^2 e^{|(m+1)t (X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon)|}\right]\\
    &\leq 1 -\epsilon(m+1)t+2(m+1)^2t^2\mathbb{E}\left[e^{((m+1)t+\frac{1}{2}) |X_{i,j} - \mathbb{E}[X_{i,j}]-\epsilon|}\right]\\
    &\leq 1-\epsilon (m+1)t + 8M(m+1)^2t^2 = 1-\frac{\epsilon^2}{32M}\\
    &\leq \exp \left(-\frac{ \epsilon^2}{32M} \right).
\end{align*}
and
\begin{align*}
    &\quad \mathbb{E}\left[e^{(m+1)t (\mathbb{E}[X_{i,j}]- X_{i,j}-\epsilon)} \right]\\
    &\leq 1 -\epsilon (m+1) t+\frac{1}{2}(m+1)^2t^2\mathbb{E}\left[(\mathbb{E}[X_{i,j}]- X_{i,j}-\epsilon)^2 e^{|(m+1)t (\mathbb{E}[X_{i,j}]- X_{i,j}-\epsilon)|}\right]\\
    &\leq 1 -\epsilon(m+1)t+2(m+1)^2t^2\mathbb{E}\left[e^{((m+1)t+\frac{1}{2}) |\mathbb{E}[X_{i,j}]- X_{i,j}-\epsilon|}\right]\\
    &\leq 1-\epsilon (m+1)t + 8M(m+1)^2t^2 = 1-\frac{\epsilon^2}{32M} \\
    &\leq \exp \left(-\frac{ \epsilon^2}{32M} \right).
\end{align*}
Thus, we obtain 
\begin{equation*}
\mathbb{E}\left [e^{t \sum_{i,j=1}^N (X_{i,j} - \mathbb{E}[X_{i,j}] - \epsilon)}\right ] \leq \exp \left(- \frac{ \epsilon^2N^2}{32(m+1)M} \right) \leq \exp \left(- \frac{ C'\epsilon^2N}{M} \right),
\end{equation*}
and 
\begin{equation*}
\mathbb{E}\left [e^{t \sum_{i,j=1}^N ( \mathbb{E}[X_{i,j}] -X_{i,j} - \epsilon)}\right ] \leq \exp \left(- \frac{ \epsilon^2N^2}{32(m+1)M} \right) \leq \exp \left(- \frac{ C'\epsilon^2N}{M} \right),
\end{equation*}
where $C'>0$ is a universal constant, and we have our conclusion.
\end{proof}
