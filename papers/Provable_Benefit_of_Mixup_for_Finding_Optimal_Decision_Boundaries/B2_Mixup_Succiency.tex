\subsection{Proof of Theorem~\ref{thm:mixup_convergence}}
Since we consider sufficiently large $\kappa$, we may assume $n \geq d$ and $n\geq 2$.
By Lemma~\ref{lemma:mixup_norm}, we can choose $R_\kappa = \Theta(1)$ such that $\lVert \wmix \rVert < R_\kappa$ for \emph{any} $n\in \mathbb{N}$. Let us define a compact set $\mathcal{C}_\kappa:= \{\vw \in \mathbb{R}^d \mid \lVert \vw \rVert \leq R_\kappa\}$. For any vector $\vw\in \mathbb{R}^d$ and nonzero vector $\vv \in \mathbb{R}^d$, we have
\begin{align*}
    \vv^\top \nabla^2_\vw \lossmix (\vw) \vv &= \frac{1}{n^2} \sum_{i,j=1}^n (\tilde{y}_{i,j} l''(\vw^\top \tilde{\vx}_{i,j})(\vv^\top \tilde{\vx}_{i,j})^2 +  (1-\tilde{y}_{i,j})l''(-\vw^\top \tilde{\vx}_{i,j})(\vv^\top \tilde{\vx}_{i,j})^2)\\
    &\geq \frac{1}{n^2} \sum_{i=1}^n (y_i l''(\vw^\top \vx_i)(\vv^\top \vx_i)^2 +  (1-y_i)l''(-\vw^\top \vx_i)(\vv^\top \vx_i)^2)\\
    &>0,
\end{align*}
almost surely since $\{ \vx_i\}_{i=1}^n $ spans $\mathbb{R}^d$ almost surely. Therefore, $\lossmix(\vw)$ is strictly convex almost surely and we conclude that  $\lossmix(\vw)$ has a unique minimizer $\hat{\vw}_{\mathrm{mix}, S}^*$ on $\mathcal{C}$ almost surely. Also, if $\hat{\vw}_{\mathrm{mix},S}^*$ belongs to the interior of $\mathcal{C}$, then it is a unique minimizer of $\lossmix(\vw)$ on $\mathbb{R}^d$. We prove high probability convergence of $\hat{\vw}_{\mathrm{mix}, S}^*$ to $\wmix$ using Lemma~\ref{lemma:minimizer_dependent} and convert $\ell_2$ convergence into directional convergence.
For simplicity, we define
\begin{equation*}
f_{i,j}(\vw) := \tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(\vw^\top \tilde{\vx}_{i,j}),
\end{equation*}
for each $i,j\in[n]$. We start with the following claim which is useful for estimating quantities described in assumptions of Lemma~\ref{lemma:minimizer_dependent} for our setting.
\begin{claim}\label{claim:mix}
For any $t>0$, we have
\begin{equation*}
\mathbb{E}_\kappa \left[ e^{t \lVert \tilde{\vx}_{i,j} \rVert}\right] \leq \left( 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert}\right)e^{t\lVert \vmu \rVert},
\end{equation*}
for all $i,j \in [n]$.
\end{claim}
\begin{proof}[Proof of Claim~\ref{claim:mix}]
We first consider the case $i = j$. By applying triangular inequality and Lemma~\ref{lemma:ineq4}, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{t \left \lVert \tilde{\vx}_{i,i}\right \rVert} \right] 
&= \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx\right \rVert} \right] \\
&\leq \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma) } \left[ e^{t \left \lVert \rvx - \vmu \right \rVert} \right] e^{t\lVert \vmu \rVert}\\
&= \mathbb{E}_{\rvz \sim N(\vzero, \kappa^{-1}t^2 \mSigma )} \left[e^{\lVert \rvz \rVert} \right] e^{t\lVert \vmu \rVert} \\
&\leq \left( 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma \rVert}\right)e^{t\lVert \vmu \rVert},
\end{align*}
for each $i \in [n]$.

Next, we handle the case $i \neq j$. For simplicity, we denote $\mSigma_\lambda = (g(\lambda)^2 + (1-g(\lambda))^2) \mSigma$ for each $\lambda \in [0,1]$. \Eqref{eqn:mix_1} and \Eqref{eqn:mix_2} imply that for each $i,j\in[n]$ with $i \neq j$,
\begin{align*}
&\quad \mathbb{E}_\kappa\left[ e^{t\lVert \tilde{\vx}_{i,j} \rVert}\right]\\
&= \mathbb{E}_{\lambda \sim \Lambda} \left[ \frac{1}{2} \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma_\lambda)}\left[ e^{t\lVert \rvx\rVert}\right] +\frac{1}{2} \mathbb{E}_{\rvx \sim N((2g(\lambda)-1)\vmu, \kappa^{-1} \mSigma_\lambda)}\left[ e^{t\lVert \rvx\rVert}\right]\right]\\
&\leq \frac{1}{2} \mathbb{E}_{\lambda \sim \Lambda}\left[\mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma_\lambda)}\left[ e^{t\lVert \rvx -\vmu \rVert}\right]e^{t\lVert \vmu \rVert} + \mathbb{E}_{\rvx \sim N((2g(\lambda)-1)\vmu, \kappa^{-1} \mSigma_\lambda)}\left[ e^{t\lVert \rvx - (2g(\lambda)-1)\vmu\rVert}\right] e^{t\lVert (2g(\lambda)-1)\vmu \rVert}\right]\\
&\leq \mathbb{E}_{\lambda \sim \Lambda}\left[ \mathbb{E}_{\rvz \sim N(\vzero, \kappa^{-1} \mSigma_\lambda) }\left[ e^{t\lVert \rvz \rVert}\right]\right] e^{t\lVert \vmu\rVert}.
\end{align*}
By Lemma~\ref{lemma:ineq4} for each $\lambda \in [0,1]$ 
\begin{equation*}
\mathbb{E}_{\rvz \sim N(\vzero, \kappa^{-1} \mSigma_\lambda) }\left[ e^{t\lVert \rvz \rVert}\right] \leq 2^{d/2} + e^{4\kappa^{-1}t^2 \lVert \mSigma_\lambda\rVert} \leq 2^{d/2} + e^{4\kappa
^{-1}t^2 \lVert \mSigma\rVert}.
\end{equation*}
and we have our conclusion.
\end{proof}

\paragraph{Step 1: Estimate Upper Bound of $\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right]$ on $\mathcal{C}_\kappa$ }\quad

For any $\vw \in \mathcal{C}$ and $i,j \in [n]$, we have
\begin{equation*}
    |f_{i,j}(\vw)| = |\tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j})l(-\vw^\top \tilde{\vx}_{i,j})| \leq l(-|\vw^\top \tilde{\vx}_{i,j}|) \leq l(-R_\kappa \lVert \tilde{\vx}_{i,j} \rVert) = \log \left( 1+e^{R_\kappa \lVert \tilde{\vx}_{i,j}\rVert} \right) .
\end{equation*}
By Lemma~\ref{lemma:mixup_norm}, $R_\kappa = \Theta(1)$ and by applying Claim~\ref{claim:mix} for $t = R_\kappa$,  we obtain $M_\kappa'$ such that $\mathbb{E}_\kappa\left[ e^{|f_{i,j}(\vw)|}\right] < M_\kappa'$ and $M_\kappa' = \Theta(1)$.
Also, by triangular inequality and Jensen's inequality, we have
\begin{align*}
\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right]
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw)\right| + \left|\mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right]\\
&\leq \mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw)\right| }\right]^2\\
&\leq {M_\kappa'}^2.
\end{align*}
Letting $M_\kappa  = {M_\kappa'^2}$, we have
\begin{equation*}
\mathbb{E}_\kappa \left[ e^{\left|f_{i,j}(\vw) - \mathbb{E}_\kappa[f_{i,j}(\vw)]\right|}\right] \leq M_\kappa,
\end{equation*}
for any $\vw \in \mathcal{C}_\kappa$ and $M_\kappa = \Theta(1)$.

\paragraph{Step 2: Estimate Upper Bound of $\lVert \nabla_{\vw} f_{i,j}(\vw) \rVert$ and $\lVert \nabla_\vw \mathbb{E}_\kappa[f_{i,j}(\vw)]\rVert$ } \quad

For each $\vw \in \mathcal{C}_\kappa$ and $i,j \in [n]$, 
\begin{align*}
\lVert \nabla_\vw f_{i,j}(\vw) \rVert &= \left \lVert \nabla_\vw (\tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j})) \right \rVert\\
&= \left \lVert \tilde{y}_{i,j}l'(\vw^\top \tilde{\vx}_{i,j})\tilde{\vx}_{i,j} - (1-\tilde{y}_{i,j}) l'(-\vw^\top \tilde{\vx}_{i,j})\tilde{\vx}_{i,j} \right \rVert\\
&\leq  \lVert \tilde{\vx}_{i,j} \rVert.
\end{align*}
In addition, by Lemma~\ref{lemma:gradient&hessian},
\begin{align*}
\left\lVert \nabla_\vw \mathbb{E}_\kappa [f_{i,j}(\vw)]\right \rVert &=  \left \lVert \nabla_\vw \mathbb{E}_\kappa[ (\tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j}))]\right \rVert\\
&= \left   \lVert\mathbb{E}_\kappa[ \nabla_\vw(\tilde{y}_{i,j}l(\vw^\top \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j}) l(-\vw^\top \tilde{\vx}_{i,j}))]\right \rVert\\
&= \left \lVert \mathbb{E}_\kappa[ \tilde{y}_{i,j}l'(\vw^\top \tilde{\vx}_{i,j})\tilde{\vx}_{i,j} - (1-\tilde{y}_{i,j}) l'(-\vw^\top \tilde{\vx}_{i,j})\tilde{\vx}_{i,j} \right] \rVert \\
&\leq \mathbb{E}_\kappa [\lVert \tilde{\vx}_{i,j}\rVert].
\end{align*}
Also, applying Claim~\ref{claim:mix} with $t = 1$, there exists $L_\kappa$ such that $\mathbb{E}_\kappa \left[ e^{\lVert \tilde{\vx}_{i,j} \rVert}\right]< L_\kappa$ and $L_\kappa = \Theta(1)$.


\paragraph{Step 3: Estimate Strong convexity Constant of $\mathbb{E}_\kappa[\lossmix (\vw)]$ on $\mathcal{C}_\kappa$} \quad

For each $\lambda \in [0,1]$, denote $\mSigma_\lambda : = (g(\lambda)^2 + (1-g(\lambda))^2) \mSigma$, for simplicity. By Lemma~\ref{lemma:gradient&hessian}, Lemma \ref{lemma:ineq1} and \Eqref{eqn:mix}, we have 
\begin{align*}
    \vv^\top \nabla^2_\vw \mathbb{E}_\kappa[\lossmix (\vw)] \vv &\geq \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[\mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma_\lambda)}[l''(\vw^\top \rvx)(\vv^\top \rvx)^2]]\\
    &\geq \frac{1}{16} \mathbb{E}_{\lambda \sim \Lambda}[\mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma_\lambda)}[e^{-(\vw^\top \rvx)^2/2}(\vv^\top \rvx)^2]],
\end{align*}
for any vector $\vw\in\mathcal{C}$ and unit vector $\vv \in \mathbb{R}^d$. By Lemma~\ref{lemma:ineq3} and Lemma~\ref{lemma:mixup_norm}, for each $\lambda \in [0,1]$, 
\begin{align*}
&\quad \mathbb{E}_{\rvx \sim N(\vmu, \kappa^{-1} \mSigma_\lambda)}[e^{-(\vw^\top \rvx)^2/2}(\vv^\top \rvx)^2]\\
&\geq \kappa^{d/2} \lVert \mSigma_\lambda \rVert^{-d/2} (\kappa\lVert \mSigma_\lambda^{-1} \rVert + R_\kappa^2)^{-(d+2)/2} \exp(-R_\kappa^2 \lVert \mSigma_\lambda \rVert \lVert \mSigma_\lambda^{-1} \rVert \lVert \vmu\rVert^2)\\
&= \Theta (\kappa^{-1}).
\end{align*}
Hence, $\mathbb{E}_\kappa[\lossmix(\vw)]$ is $\alpha_\kappa $-strongly convex with $\alpha_\kappa = \Theta(\kappa^{-1})$.


\paragraph{Step 4: Sample Complexity for Directional Convergence} \quad

By Lemma~\ref{lemma:mixup_norm}, we can choose $r_\kappa = \Theta(1)$ such that 
 $r_\kappa \leq \lVert \wmix \rVert$ for any $n$. Since $r_\kappa = \Theta(1)$, we assume $\kappa$ is large enough so that 
$r_\kappa \epsilon < r_\kappa <\alpha_\kappa^{-1/2}$.
Assume the unique existence of $\hat{\vw}_{\mathrm{mix},S}^*$ which occurs almost surely. 
By Lemma~\ref{lemma:minimizer_dependent}, if
\begin{equation*}
    n \geq \frac{C_1'M_\kappa}{\alpha_\kappa^2 r_\kappa^4 \epsilon^4} \log \left( \frac{3}{\delta} \max \left\{ 1, \left( \frac{2C_2'd^{1/2}R_\kappa L_\kappa}{\alpha_\kappa r_\kappa ^2 \epsilon^2} \right)^d  \right\} \right) = \frac{\Theta(\kappa^2)}{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right ),
\end{equation*}
then we have $\lVert\wmix - \hat{\vw}_{\mathrm{mix},S}^* \rVert \leq  r_\kappa \epsilon \leq \lVert\wmix \rVert \epsilon$ with probability at least $1-\delta$.
Furthermore, if $\lVert \wmix - \hat{\vw}_{\mathrm{mix},S}^* \rVert \leq  \lVert \wmix \rVert \epsilon$, then $\hat{\vw}_{\mathrm{mix},S}^*$ belongs to interior of $\mathcal{C}_\kappa$. Hence, $\hat{\vw}_{\mathrm{mix},S}^*$ is a minimizer of $\lossmix(\vw)$ over the entire $\mathbb{R}^d$. Also, we have
\begin{align*}
    \cosim ( \hat{\vw}_{\mathrm{mix},S}^*, \mSigma^{-1}\vmu) &=  \cosim ( \hat{\vw}_{\mathrm{mix},S}^*,\wmix)\\
    &= \bigg (1- \sin^2\Big (\angle \big (\hat{\vw}_{\mathrm{mix},S}^*, \wmix \big ) \Big ) \bigg)^{1/2}\\
    &\geq 1- \sin\Big (\angle\big (\hat{\vw}_{\mathrm{mix},S}^*, \wmix \big) \Big)\\
    &\geq 1- \epsilon.
\end{align*}
Hence, we conclude that if $n = \frac{\Omega(\kappa^2)}{\epsilon^4} \left (1+ \log \frac{1}{\epsilon} + \log \frac{1}{\delta} \right)$, then
with probability at least $1-\delta$, the Mixup loss $\lossmix(w)$ has a unique minimizer $\hat{\vw}_{\mathrm{mix},S}^*$ and $\cosim ( \hat{\vw}_{\mathrm{mix},S}^*, \mSigma^{-1}\vmu)\geq 1- \epsilon$. \hfill $\square$

\subsection{Proof of Lemma~\ref{lemma:mixup_norm}}\label{proof:lemma:mixup_norm}
By Theorem~\ref{thm:expected_loss_mix}, let $\wmix = c_{\mathrm{mix}, n, \kappa}^* \mSigma^{-1} \vmu$ where $c_{\mathrm{mix}, n, \kappa }^* \geq 0 $. For any $c \in \R$, from \Eqref{eqn:mix}, we have
\begin{align*}
    \mathbb{E}_\kappa[\lossmix(c \mSigma^{-1} \vmu)]
    &= \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[\mathbb{E}_{X \sim N(s, \kappa^{-1} s_\lambda) } [l(c X)]] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[\lambda \cdot\mathbb{E}_{X \sim N((2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }[ l(cX)]] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[(1-\lambda) \cdot\mathbb{E}_{\rvx \sim N(-(2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }[ l(cX)]] \\
    &\quad+ \frac{1}{n} \mathbb{E}_{X \sim N(s,\kappa^{-1}s) }\left[ l(c X)\right].
\end{align*}
where we denote $s_\lambda = (g(\lambda)^2 + (1-g(\lambda))^2) \vmu^\top \mSigma \vmu$ for each $\lambda \in [0,1]$ and $s = \vmu^\top \mSigma \vmu$, for simplicity.

By Lemma~\ref{lemma:gradient&hessian}, we have
\begin{align*}
    &\quad - \frac{\partial}{\partial c}\mathbb{E}_\kappa[\lossmix(c \mSigma^{-1} \vmu)]\\
    &= -\frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[\mathbb{E}_{X \sim N(s, \kappa^{-1} s_\lambda) } [l'(c X)X]] \\
    &\quad- \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[\lambda \cdot \mathbb{E}_{X \sim N((2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }[ l'(cX)X]] \\
    &\quad- \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}[(1-\lambda)\cdot\mathbb{E}_{X \sim N(-(2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }[ l'(cX)X]] \\
    &\quad- \frac{1}{n} \mathbb{E}_{X \sim N(s, \kappa^{-1}s) }\left[  l'(cX)X\right]\\
    &= \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[\mathbb{E}_{X \sim N(s, \kappa^{-1} s_\lambda) } \left[\frac{X}{1+e^{cX}}\right]\right] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[\lambda \cdot \mathbb{E}_{X \sim N((2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }\left[\frac{X}{1+e^{cX}}\right]\right] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[(1-\lambda) \cdot\mathbb{E}_{X \sim N(-(2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }\left[\frac{X}{1+e^{cX} }\right]\right] \\
    &\quad+ \frac{1}{n} \mathbb{E}_{X \sim N(s,\kappa^{-1}s) }\left[\frac{X}{1+e^{cX} }\right],
\end{align*}
and if $c\geq 0$,  by applying Lemma~\ref{lemma:ineq2} with $z= cX$, we have
\begin{align*}
    &\quad -\frac{\partial}{\partial c } \mathbb{E}[\lossmix(c \mSigma^{-1}\vmu)] \\
    &\geq \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[\mathbb{E}_{X \sim N(s, \kappa^{-1} s_\lambda) } \left[\frac{1}{2}X - \frac{1}{4} c X^2 \right]\right] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[\lambda \cdot \mathbb{E}_{X \sim N((2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }\left[\frac{1}{2}X - \frac{1}{4} c X^2\right]\right] \\
    &\quad+ \frac{n-1}{2n} \mathbb{E}_{\lambda \sim \Lambda}\left[(1-\lambda) \cdot\mathbb{E}_{X \sim N(-(2g(\lambda)-1)s, \kappa^{-1} s_\lambda) }\left[\frac{1}{2}X - \frac{1}{4} c X^2\right]\right] \\
    &\quad+ \frac{1}{n} \mathbb{E}_{X \sim N(s,\kappa^{-1}s) }\left[\frac{1}{2}X - \frac{1}{4} c X^2\right]\\
    &= \frac{s}{4n} \cdot \Big((n-1)\mathbb{E}_{\lambda \sim \Lambda}[1+(2\lambda-1)(2g(\lambda)-1)]+2 \Big) \\
    &\quad - \frac{c}{8n} \cdot \left( \kappa^{-1} \left( 2 (n-1) \mathbb{E}_{\lambda \sim \Lambda}[ s_\lambda]+ 2s \right) +s^2 \left((n-1)\mathbb{E}_{\lambda \sim \Lambda}[(2g(\lambda)-1)^2 +1] +2 \right)  \right) \\
    & \geq \frac{s}{8} \cdot \Bigg( \mathbb{E}_{\lambda \sim \Lambda}[(2\lambda-1)(2 g(\lambda)-1)] - c  \Big(\kappa^{-1} \left(2\mathbb{E}_{\lambda \sim \Lambda}\left[g(\lambda)^2 + (1-g(\lambda))^2 \right]+1\right) + s\left( \mathbb{E}_{\lambda \sim \Lambda}[(2g(\lambda)-1)^2+2]\right)\Big)\Bigg).
\end{align*}

Thus, if $0 \leq c <\frac{\mathbb{E}_{\lambda \sim \Lambda}\left[ (2\lambda-1)(2g(\lambda)-1)\right]}{ \kappa^{-1} \left(2\mathbb{E}_{\lambda \sim \Lambda}\left[g(\lambda)^2 + (1-g(\lambda))^2 \right]+1\right) + s\left( \mathbb{E}_{\lambda \sim \Lambda}[(2g(\lambda)-1)^2+2]\right)}$, then $\mathbb{E}[\lossmix(c \Sigma^{-1}\mu)]$ is decreasing as a function of $c$ and we conclude $c_{\mathrm{mix},n,\kappa }\geq \frac{\mathbb{E}_{\lambda \sim \Lambda}\left[ (2\lambda-1)(2g(\lambda)-1)\right]}{ \kappa^{-1} \left(2\mathbb{E}_{\lambda \sim \Lambda}\left[g(\lambda)^2 + (1-g(\lambda))^2 \right]+1\right) + s\left( \mathbb{E}_{\lambda \sim \Lambda}[(2g(\lambda)-1)^2+2]\right)}$ which implies $\lVert \wmix \rVert = \Omega(1)$, and one can note that this lower bound is independent of $n$.

In order to get the upper bound, we use the following inequality: For each $z \in \mathbb{R}$, we have
\begin{equation*}
    l(z) + l(-z) = \log(1+e^{-z}) + \log(1+e^z) = \log(2+e^z + e^{-z}) > |z|.
\end{equation*}
For each $c \geq 0$, we have
\begin{align*}
    \mathbb{E}_\kappa[\lossmix(c \mSigma^{-1}\vmu)] &= \frac{1}{n^2}  \sum_{i,j=1}^n \mathbb{E}_\kappa\left[\tilde{y}_{i,j} l( c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j})l(- c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j})\right]\\
    &> \frac{1}{n^2} \sum_{\overset{i,j=1}{i \neq j}}^n \mathbb{E}_\kappa\left[ \tilde{y}_{i,j} l( c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}) + (1-\tilde{y}_{i,j})l(- c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j})\right]\\
    &\geq \frac{1}{n^2} \sum_{\overset{i,j=1}{i \neq j}}^n  \mathbb{E}_\kappa\left[\min \{\tilde{y}_{i,j},1-\tilde{y}_{i,j}\} ( l( c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}) + l(- c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}))\right]\\
    &\geq \frac{1}{n^2}  \sum_{\overset{i,j=1}{i \neq j}}^n \mathbb{E}_\kappa\left[ \min \{ \tilde{y}_{i,j},1-\tilde{y}_{i,j}\} | c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}|\right]\\
    &= \frac{1}{n^2}  \sum_{\overset{i,j=1}{i \neq j}}^n \mathbb{E}_\kappa\left[ |\min \{ \tilde{y}_{i,j},1-\tilde{y}_{i,j}\} c \vmu^\top \mSigma^{-1} \tilde{\vx}_{i,j}|\right]\\
    &= \frac{1}{n^2}  \sum_{\overset{i,j=1}{i \neq j}}^n \mathbb{E}_\kappa\left[ |\min \{ \tilde{y}_{i,j},1-\tilde{y}_{i,j}\}(2g(\tilde{y}_{i,j})-1) s|\right] c\\
    &= \frac{n-1}{n}\mathbb{E}_{\lambda \sim \Lambda}[\min \{\lambda, 1-\lambda \}|2g(\lambda)-1|]s c\\
    &\geq \frac{1}{4}\mathbb{E}_{\lambda \sim \Lambda}[\min \{\lambda, 1-\lambda \}|2g(\lambda)-1|] s c.
\end{align*}
From our assumption $\mathbb{P}_{\lambda \sim \Lambda}\left [\lambda \notin \{ 0,1 \} \land g(\lambda) \neq \frac{1}{2}\right]>0$, we have $\mathbb{E}_{\lambda \sim \Lambda}[\min(\lambda, 1-\lambda)]|2g(\lambda)-1| > 0$. Thus, if $c \geq \frac{4\log 2}{\mathbb{E}_{\lambda \sim \Lambda}[\min(\lambda, 1-\lambda)|2g(\lambda)-1|]s}$, then $\mathbb{E}_\lambda[\lossmix (c \Sigma^{-1}\mu)] > \log2 = \mathbb{E}_\lambda[\lossmix (0)]$, so $c\mSigma^{-1}\vmu$ cannot be a minimizer. Hence, $c_{\mathrm{mix},n,\kappa}^* <\frac{4\log 2}{\mathbb{E}_{\lambda \sim \Lambda}[\min(\lambda, 1-\lambda)|2g(\lambda)-1|]s} 
 = \mathcal O(1)$, which is also independent of $n$. Combining the lower and upper bounds, we have our conclusion that $c_{\mathrm{mix},n,\kappa}^* = \Theta(1)$.
\hfill $\square$