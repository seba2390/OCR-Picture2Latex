\section{Introduction}
\label{sec:intro}

Computer graphics has long been concerned with generating photorealistic images at high resolution that allow for direct control over semantic attributes. Until recently, the primary paradigm was to create carefully designed 3D models which are then rendered using realistic camera and illumination models. A parallel line of research approaches the problem from a data-centric perspective. 
In particular, probabilistic generative models ~\cite{Goodfellow2014NEURIPS,Oord2017NEURIPS,Song2021ICLR} have shifted the paradigm from designing assets to designing training procedures and datasets. Style-based GANs (StyleGANs) are a specific instance of these models, and they exhibit many desirable properties. They achieve high image fidelity~\cite{Karras2019CVPR, Karras2020CVPR}, fine-grained semantic control~\cite{Haerkoenen2020NEURIPS, WU2021CVPRa,Ling2021ARXIV}, and recently alias-free generation enabling realistic animation~\cite{Karras2021NEURIPS}. Moreover, they reach impressive photorealism on carefully curated datasets, especially of human faces. However, when trained on large and unstructured datasets like ImageNet~\cite{Deng2009CVPR}, StyleGANs do not achieve satisfactory results yet. One other problem plaguing data-centric methods, in general, is that they become prohibitively more expensive when scaling to higher resolutions as bigger models are required.

Initially, StyleGAN~\cite{Karras2019CVPR} was proposed to explicitly disentangle factors of variations, allowing for better control and interpolation quality. 
However, its architecture is more restrictive than a standard generator network~\cite{Radford2016ICLR, Karras2018ICLR} which seems to come at a price when training on complex and diverse datasets such as ImageNet. Previous attempts at scaling StyleGAN and StyleGAN2 to ImageNet led to sub-par results~\cite{Gwern2020MISC, Grigoryev2022ICLR},  giving reason to believe it might be fundamentally limited for highly diverse datasets~\cite{Gwern2020MISC}.

BigGAN~\cite{Brock2019ICLR} is the state-of-the-art GAN model for image synthesis on ImageNet. The main factors for BigGANs success are larger batch and model sizes.
However, BigGAN has not reached a similar standing as StyleGAN as its performance varies significantly between training runs~\cite{Karras2020NeurIPS} and as it does not employ an intermediate latent space which is essential for GAN-based image editing~\cite{Abdal2021TOG, Patashnik2021ICCV, Collins2020CVPR, WU2021CVPRa}. Recently, BigGAN has been superseded in performance by diffusion models~\cite{Dhariwal2021NEURIPS}. Diffusion models achieve more diverse image synthesis than GANs but are significantly slower during inference and prior work on GAN-based editing is not directly applicable. Following these arguments, successfully training StyleGAN on ImageNet has several advantages over existing methods.

The previously failed attempts at scaling StyleGAN raise the question of whether architectural constraints fundamentally limit style-based generators or if the missing piece is the right training strategy.
Recent work by \cite{Sauer2021NEURIPS} introduced \textit{Projected GANs} which project generated and real samples into a fixed, pretrained feature space. Rephrasing the GAN setup this way leads to significant improvements in training stability, training time, and data efficiency.
Leveraging the benefits of Projected GAN training might enable scaling StyleGAN to ImageNet.
However, as observed by~\cite{Sauer2021NEURIPS}, the advantages of Projected GANs only partially extend to StyleGAN on the unimodal datasets they investigated.
We study this issue and propose architectural changes to address it.
We then design a progressive growing strategy tailored to the latest StyleGAN3. 
These changes in conjunction with Projected GAN already allow surpassing prior attempts of training StyleGAN on ImageNet. 
To further improve results, we analyze the pretrained feature network used for Projected GANs and find that the two standard neural architectures for computer vision, CNNs and ViTs~\cite{Dosovitskiy2021ICLR}, significantly improve performance when used jointly. Lastly, we leverage \textit{classifier guidance}, a technique originally introduced for diffusion models to inject additional class-information~\cite{Dhariwal2021NEURIPS}.

Our contributions culminate in a new state-of-the-art on large-scale image synthesis, pushing the performance beyond existing GAN and diffusion models. 
We showcase inversion and editing for ImageNet classes and find that Pivotal Tuning Inversion (PTI)~\cite{Roich2021ARXIV}, a powerful new inversion paradigm, combines well with our model and even embeds out-of-domain images smoothly into our learned latent space. Our efficient training strategy allows us to triple the parameters of the standard StyleGAN3 while reaching prior state-of-the-art performance of diffusion models~\cite{Dhariwal2021NEURIPS} in a fraction of their training time. It further enables us to be the first to demonstrate image synthesis on ImageNet-scale at a resolution of $1024^2$ pixels. We will open-source our code and models upon publication. 
