\section{Background}
\label{sec:background}
We first introduce the main building blocks of our system:
the StyleGAN3 generator~\cite{Karras2021NEURIPS} and Projected GAN's~\cite{Sauer2021NEURIPS} feature projectors and multi-scale discriminators.

\boldparagraph{StyleGAN.}
This section describes style-based generators in general with a focus on the latest StyleGAN3~\cite{Karras2021NEURIPS}. A StyleGAN generator consists of a mapping network $\bG_m$ and a synthesis network $\bG_s$. First, $\bG_m$ maps a normally distributed latent code $\bz$ to a style code $\bw$. This style code $\bw$ is then used for modulating the convolution kernels of $\bG_s$ to control the synthesis process.
The synthesis network $\bG_s$ of StyleGAN3 starts from a spatial map defined by Fourier features~\cite{Tancik2020NEURIPS, Xu2021CVPR}. This input then passes through $N$ layers of convolutions, non-linearities, and upsampling to generate an image. 
Each non-linearity is wrapped by an upsampling and downsampling operation to prevent aliasing. 
The low-pass filters used for these operations are carefully designed to
balance image quality, antialiasing, and training speed.
Concretely, their cutoff and stopband frequencies grow geometrically with network depth, the transition band half-widths are as wide as possible within the limits of the layer sampling rate, and only the last two layers are critically sampled, i.e., the filter cutoff equals the bandlimit. The number of layers $N$ is $14$, independent of the final output resolution.

Style mixing and path length regularization are methods for regularizing style-based generators. In style mixing, an image is generated by feeding sampled style codes $\bw$ into different layers of $\bG_s$ independently. Path length regularization encourages that a step of fixed size in latent space results in a corresponding fixed change in pixel intensity of the generated image~\cite{Karras2020CVPR}.
This inductive bias leads to a smoother generator mapping and has several advantages including fewer artifacts, more predictable training behavior, and better inversion.

Progressive growing was introduced by~\cite{Karras2018ICLR} for stable training at high resolutions but~\cite{Karras2020CVPR} found that it can impair shift-equivariance.~\cite{Karras2021NEURIPS} observe that texture sticking artifacts are caused by a lack of equivariance and carefully design StyleGAN3 to prevent texture sticking. Hence, in this paper, as we build on StyleGAN3, we can revisit the idea of progressive growing to improve convergence speed and synthesis quality.

\boldparagraph{Projected GAN.}
The original adversarial game between a generator $\bG$ and a discriminator $\bD$ can be extended by a set of feature projectors $\{\bP_l\}$~\cite{Sauer2021NEURIPS}. The projectors map real images $\bx$ and images generated by $\bG$ to the discriminator's input space. 
The Projected GAN objective is formulated as
\begin{equation}
\begin{aligned}
    \min_\bG \max_{\{\bD_l\}} &\sum_{l \in \cL} \Big (
    \nE_{\bx} [\log \bD_l(\bP_l(\bx))]\\ 
    &\quad \; + \nE_{\bz}[ \log( 1- \bD_l(\bP_l(\bG(\bz))))] \Big)
\end{aligned}
\end{equation}
\label{eq:GANobjective2}

where $\{\bD_l\}$ is a set of independent discriminators operating on different feature projections.
The projectors consist of a pretrained feature network $\bF$, cross-channel mixing (CCM) and cross-scale mixing (CSM) layers. 
The purpose of CCM and CSM is to prohibit the discriminators from focusing on only a subset of its input feature space which would result in mode collapse. Both modules employ differentiable random projections that are not optimized during GAN training. CCM mixes features across channels via random 1x1 convolutions, CSM mixes features across scales via residual random 3x3 convolution blocks and bilinear upsampling. The output of CSM is a feature pyramid consisting of four feature maps at different resolutions. Four discriminators operate independently on these feature maps. Each discriminator uses a simple convolutional architecture and spectral normalization~\cite{Miyato2018ICLR}. The depth of the discriminator varies depending on its input resolution, i.e., a spatially larger feature map corresponds to a deeper discriminator. Other than spectral normalization, Projected GANs do not use additional regularization such as gradient penalties~\cite{Mescheder2018ICML}. Lastly, ~\cite{Sauer2021NEURIPS} apply differentiable data-augmentation \cite{Zhao2020NeurIPS} before $\bF$ which improves Projected GAN's performance independent of the dataset size.

~\cite{Sauer2021NEURIPS} evaluate several combinations of $\bF$ and $\bG$ and find an EfficientNet-Lite0~\cite{Tan2019ICML} and a FastGAN generator~\cite{Liu2021ICLR} to work especially well. When using a StyleGAN generator, they observe that the discriminators can quickly overpower the generator for suboptimal learning rates. The authors suspect that the generator might adapt too slowly due to its design which modulates feature maps with styles learned by a mapping network.

