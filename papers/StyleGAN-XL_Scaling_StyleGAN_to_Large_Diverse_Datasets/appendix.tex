In this supplemental document, we elaborate on increasing the resolution of ImageNet to one megapixel, compare to the baseline on a class containing humans, and specify the implementation details of our approach. The supplemental video shows additional samples and interpolations. We use the same mathematical notation as in the paper.

\section{Preprocessing ImageNet}
An initial challenge is the lack of high-resolution data; the mean resolution of ImageNet is $469\times387$.
Similar to the procedure used for generating CelebA-HQ\cite{Karras2018ICLR}, we preprocess the whole dataset with SwinIR-Large~\cite{Liang2021ICCV}, a recent model for real-world image super-resolution.
Of course, a trivial way of achieving good performance on this dataset would be to draw samples from a $256^2$ generative model and passing it through SwinIR. However, SwinIR adds significant computational overhead as it is $60$ times slower than our upsampling stack. Furthermore, this way, StyleGAN-XL's weights can be used for initialization when finetuning on other high-resolution datasets. Lastly, combining StyleGAN-XL and SwinIR would impair translation equivariance.

\section{Classes of unaligned Humans}
We observe that ADM~\cite{Dhariwal2021NEURIPS} generates more convincing human faces than StyleGAN-XL and BigGAN. Both GANs can synthesize realistic faces; however, the main challenge in this setting is that the dataset is unstructured, and the humans are not aligned.~\cite{Brock2019ICLR} remarked the particular challenge of classes containing details to which human observers are more sensitive. We show examples in~\figref{fig:aquamen}.
\aquamen


\section{Inference Speed}
GANs generate samples in a single forward pass, unlike diffusion models that must be applied several hundred or thousand times to generate a sample. \tabref{tab:speedcomparison} compares StyleGAN-XL to ADM. We find that StyleGAN-XL is several orders of magnitude faster. In defense of diffusion models, speeding up their sampling is an active area of research, and novel techniques~\cite{Watson2021ARXIV} may be able to reduce this gap in the future.
\speedcomparison

\section{Results on Unimodal Datasets}
StyleGAN-XL is designed to enable training on large and diverse datasets. However, applying it to big and small unimodal datasets is straightforward. 
In contrast to the configuration for ImageNet, we begin with ten layers at the lowest stage and add two layers per resolution stage. Furthermore, we do not employ classifier guidance.
\tabref{tab:extraresults} reports the results for both datasets at resolution $1024^2$, StyleGAN-XL achieves state-of-the-art performance on both.
\extraresults

\section{Additional Qualitative Results}

In the following, we present additional qualitative results. \figref{fig:interpsupp} shows additional interpolations between samples from different classes. \figref{fig:ffhqsamples} and \figref{fig:pokemonsamples} show samples on FFHQ $1024^2$ and Pokemon $1024^2$ respectively. Lastly, we compare BigGAN, ADM, and StyleGAN-XL on different ImageNet classes. For a  fair comparison, we do not use truncation or classifier guidance. Instead, we show images with the largest logits given by a VGG16 which corresponds to individual image quality.

\section{Implementation details}

\boldparagraph{Inversion.}
Following \cite{Karras2020CVPR}, we use basic latent optimization in $\mathcal{W}$ for inversion. Given a target image, we first compute its average style code $\bar{\bw}$ by running $10000$ random latent codes $\bz$ and target specific class samples $\bc$ through the mapping network. As the class label of the target image is unknown, we pass it to a pretrained classifier. We then use the classifier logits as a multinomial distribution to sample $\bc$. In our experiments, we use Deit-base~\cite{Touvron2021ICML} as a classifier, but other choices are possible.
At the beginning of optimization , we initialize $\bw = \bar{\bw}$. The components of $\bw$ are the only trainable parameters. The optimization runs for 1000 iterations using the Adam optimizer~\cite{Kingma2015ICLR} with default parameters. 
We optimize the LPIPS~\cite{Zhang2018CVPR} distance between the target image and the generated image.
For StyleGAN-XL, the maximum learning rate is $\lambda_{max} = 0.05$. It is ramped up from zero linearly during the first 50 iterations and ramped down to zero using a cosine schedule during the last 250 iterations. 
For BigGAN, we empirically found $\lambda_{max} = 0.001$ and a ramp-down over the last 750 iterations to yield the best results. 
All inversion experiments are performed at resolution $512^2$ and computed on $5k$ images ($10$\% of the validation set). We report the results in \tabref{tab:invresults} and show qualitative results in \figref{fig:inversion}.
\invresults

\boldparagraph{Training StyleGAN3 on ImageNet.}
For training StyleGAN3, we use the official PyTorch implementation\footnote{\url{https://github.com/NVlabs/stylegan3.git}}.
The results in 
\figref{fig:teaser} 
are computed with the StyleGAN3-R configuration on resolution $256^2$ until the discriminator has seen $10$ million images. We find that StyleGAN3-R and StyleGAN3-T converge to similar FID without any changes to their training paradigm. The run with the best FID score was selected from three runs with different random seeds. We use a channel base of $16384$ and train on $8$ GPUs with total batch size $256$, $\gamma=0.256$. The remaining settings are chosen according to the default configuration of the code release.
For the ablation study in  
\tabref{tab:ablation}
, we use the StyleGAN3-T configuration as baseline since StyleGAN-XL builds upon the translational-equivariant layers of StyleGAN3.
We train on $4$ GPUs with total batch size $256$ and batch size $32$ per GPU, $\gamma=0.25$, and disable augmentation.

\boldparagraph{Training \& Evaluation.}
For all our training runs, we do not use data amplification via \textit{x-flips} following~\cite{Karras2020CVPR}. Furthermore, we evaluate all metrics using the official StyleGAN3 codebase. For the baseline values in
\tabref{tab:sotapr}
we report the numbers of~\cite{Dhariwal2021NEURIPS}. The official codebase of ADM\footnote{\url{https://github.com/openai/guided-diffusion}} provides files containing $50$k samples for ADM and BigGAN. We utilize the provided samples to compute rFID. Following ~\cite{Dhariwal2021NEURIPS}, we compute precision and recall between $10$k real samples and $50$k generated samples. \tabref{tab:imagenetlowres} reports the results on ImageNet at lower resolutions.

\imagenetlowres

\boldparagraph{Layer configurations.}
We start progressive growing at resolution $16^2$ using $11$ layers. The layer specifications are computed according to \cite{Karras2021NEURIPS} and remain fixed for the remaining training. For the next stage, at resolution $32^2$, we discard the last $2$ layers and add $7$ new ones. The specifications for the new layers are computed according to \cite{Karras2021NEURIPS} for a model with resolution $32^2$ and $16$ layers. Continuing this strategy up to resolution $1024^2$ yields the flexible layer specification of StyleGAN-XL in \figref{fig:layerspecs}.

\interpsupp
\inversion
\ffhqsamples
\pokemonsamples
\samplesa
\samplesb
\samplesc
\layerspecs