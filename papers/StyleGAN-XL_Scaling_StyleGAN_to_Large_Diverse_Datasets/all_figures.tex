\providecommand{\impath}[1]{}
\providecommand{\impatha}[1]{}
\providecommand{\impathb}[1]{}
\providecommand{\impathc}[1]{}
\providecommand{\impathd}[1]{}
\providecommand{\impathe}[1]{}
\providecommand{\imwidth}{}
\providecommand{\imwidtha}{}
\providecommand{\imwidthb}{}
\providecommand{\smallimwidtha}{}
\providecommand{\smallimwidthb}{}
\providecommand{\subimwidtha}{}
\providecommand{\subimwidthb}{}

\newcommand{\teaser}{
\begin{teaserfigure}
 \includegraphics[width=1.00\textwidth, trim=0em 0em 0em 0em, clip]{gfx/teaser.png}
  \caption{ 
  Class-conditional samples generated by StyleGAN3 (left) and StyleGAN-XL (right) trained on ImageNet at resolution $256^2$.
  }
  \label{fig:teaser}
\end{teaserfigure}
}


\newcommand{\system}{
\begin{figure*}
  \includegraphics[width=\textwidth]{gfx/system.png}
  \caption{\textbf{Training StyleGAN-XL}.
  We feed a latent code $\bz$ and class label $\bc$ to the pretrained embedding and the mapping network $\bG_m$ to generate style codes $\bw$. The codes modulate the convolutions of the synthesis network $\bG_s$. During training, we gradually add layers to double the output resolution for each stage of the progressive growing schedule. We only train the latest layers while keeping the others fixed. $\bG_m$ is only trained for the initial $16^2$ stage and remains fixed for the higher-resolution stages. The synthesized image is upsampled when smaller than $224^2$ and passed through a CNN and a ViT and respective feature mixing blocks (CCM+CSM). At higher resolutions, the CNN receives the unaltered image while the ViT receives a downsampled input to keep memory requirements low but still utilize its global feedback. Finally, we apply eight independent discriminators on the resulting multi-scale feature maps. The image is also fed to classifier CLF for classifier guidance.
  }
  \label{fig:system}
\end{figure*}
}

\newcommand{\highres}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/high_res.png}
  \caption{\textbf{Samples at Different Resolutions Using the Same $\bw$}. The samples are generated by the models obtained during progressive growing. We upsample all images to $1024^2$ using nearest-neighbor interpolation for visualization purposes. Zooming in is recommended.
  }
  \label{fig:highres}
\end{figure*}
}


\newcommand{\inversion}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/inversion.png}
  \caption{\textbf{Inversion of a Given Source Image}.
  For BigGAN, we invert to its latent space $\bz$, for StyleGAN-XL we invert to style codes $\bw$.
  }
  \label{fig:inversion}
\end{figure*}
}

\newcommand{\interpolations}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/interpolations_main.png}
    \caption{\textbf{Interpolations}. StyleGAN-XL generates smooth interpolations between samples of different classes (Row 1 \& Row 2). PTI allows inverting to the latent space with low distortion (outermost image, Row 3 \& Row 4), and consistently embeds out-of-domain inputs, such as the one on the bottom right.
  }
  \label{fig:interpolations}
\end{figure*}
}

\newcommand{\editing}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/editing_main.png}
\caption{\textbf{Image Editing and Style Mixing}. 
Left: First, a given image is inverted via PTI~\cite{Roich2021ARXIV}.
Right: Given two images, we can mix their styles. This methods works for samples of the same or similar classes, and to a certain extent, for distant classes. For this experiment, we utilize random samples instead of inversions.
  }
  \label{fig:editing}
\end{figure*}
}

\newcommand{\layerspecs}{
\begin{figure*}[!p]
  \includegraphics[height=0.9\textheight]{gfx/layer_specs.pdf}
  \caption{\textbf{Flexible Layer Specification of Stylegan-XL}. StyleGAN-XL consists of $39$ layers at resolution $1024^2$. Cutoff (blue) and minimum acceptable stopband frequency (orange) obey geometric progression over the layers; sampling rate (red) and actual stopband (green) are computed according to our design constraints.}
  \label{fig:layerspecs}
\end{figure*}
}

\newcommand{\aquamen}{
\begin{figure}
  \includegraphics[width=1.0\linewidth]{gfx/aquamen.png}
  \caption{\textbf{Imagenet Classes Containing Humans}.   
  Samples for BigGAN and ADM are taken from~\cite{Dhariwal2021NEURIPS}.
  }
  \label{fig:aquamen}
\end{figure}
}

\newcommand{\interpsupp}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/interpolations_supp.png}
  \caption{
  \textbf{Interpolations.} StyleGAN-XL generates smooth interpolations between samples of different classes.
  }
  \label{fig:interpsupp}
\end{figure*}
}

\newcommand{\mixeditingsupp}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/mixing_and_editing_supp.png}
  \caption{\textbf{Extrapolation and Style Mixing}. 
  Left: We use PTI for inversion and extrapolate by providing a bigger input grid.
  Right:  Given two images, we mix their styles. For this experiment, we utilize random samples instead of inversions.
  }
  \label{fig:mixeditingsupp}
\end{figure*}
}

\newcommand{\editingsupp}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/stylemc.png}
  \caption{\textbf{Image Manipulation via Language}. 
 Given a random sample, we manipulate the image by by following semantic directions in latent space found by StyleMC~\cite{Kocasari2021WACV}. The latent space directions from top to bottom are: "smile", "no stripes", and "big eyes".
  }
  \label{fig:editingsupp}
\end{figure*}
}

\newcommand{\ffhqsamples}{
\begin{figure*}[!p]
  \includegraphics[height=0.95\textheight]{gfx/ffhq_samples.png}
  \caption{\textbf{Samples on FFHQ $1024^2$}. 
  }
  \label{fig:ffhqsamples}
\end{figure*}
}

\newcommand{\pokemonsamples}{
\begin{figure*}[!p]
  \includegraphics[height=0.95\textheight]{gfx/pokemon_samples.png}
  \caption{\textbf{Samples on Pokemon $1024^2$}. 
  }
  \label{fig:pokemonsamples}
\end{figure*}
}

\newcommand{\samplesa}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/samples1.png}
  \caption{\textbf{Qualitiative Comparison on ImageNet $256^2$.}. 
  We compare BigGAN (left column), ADM (middle column), and StyleGAN-XL (right column). Classes from top to bottom: pizza, valley, daisy, dough, comic book.
  }
  \label{fig:samplesa}
\end{figure*}
}

\newcommand{\samplesb}{
\begin{figure*}[!p]
  \includegraphics[width=\textwidth]{gfx/samples2.png}
  \caption{\textbf{Qualitiative Comparison on ImageNet $256^2$.}. 
  We compare BigGAN (left column), ADM (middle column), and StyleGAN-XL (right column). Classes from top to bottom: bulbul, nematode, jack-o'-lantern, balloon, crossword puzzle.
  }
  \label{fig:samplesb}
\end{figure*}
}

\newcommand{\samplesc}{
\begin{figure*}[!t]
  \includegraphics[width=\textwidth]{gfx/samples3.png}
  \caption{\textbf{Qualitiative Comparison on ImageNet $256^2$.}. 
  We compare BigGAN (left column), ADM (middle column), and StyleGAN-XL (right column). Classes from top to bottom: agaric, orange, Tibetian mastiff, espresso, paddlewheel.
  }
  \label{fig:samplesc}
\end{figure*}
}