\section{Results}
\label{sec:results}
In this section, we first compare StyleGAN-XL to the state-of-the-art approaches for image synthesis on ImageNet. We then evaluate the inversion and editing capabilities of StyleGAN-XL. As described above, we scale our model to a resolution of $1024^2$ pixels, which no prior work has attempted so far on ImageNet. The resolution of most images in ImageNet is lower. We therefore preprocess the data with a super-resolution network~\cite{Liang2021ICCV}, see supplementary.

\subsection{Image Synthesis}
Both our work and ~\cite{Dhariwal2021NEURIPS} use classifier networks to guide the generator. To ensure the models are not inadvertently optimizing for FID and IS, which also utilize a classifier network, we propose random-FID (rFID). For rFID, we calculate the Fr\'echet distance in the \texttt{pool\_3} layer of a randomly initialized inception network~\cite{Szegedy2015CVPR}.
The efficacy of random features for evaluating generative models has been demonstrated in~\cite{Naeem2020ICML}. Furthermore, we report sFID~\cite{Nash2021ICML} to assess spatial structure.
Lastly, sample fidelity and diversity are evaluated via precision and  recall~\cite{Kynknniemi2019NEURIPS}. 

In \tabref{tab:sotapr}, we compare StyleGAN-XL to the currently strongest GAN model (BigGAN-deep~\cite{Brock2019ICLR}) and diffusion models (CDM~\cite{Ho2022JMLR}, ADM~\cite{Dhariwal2021NEURIPS}) on ImageNet.
The values for ADM are calculated with and without additional methods (Upsampling \textbf{U} and Classifier Guidance \textbf{G}). For StyleGAN2, we report numbers by~\cite{Grigoryev2022ICLR}.
We find that StyleGAN-XL substantially outperforms all baselines across all resolutions in FID, sFID, rFID, and IS. 
An exception is recall, according
to which StyleGAN-XLâ€™s sample diversity lies between BigGAN and
ADM, making progress in closing the gap between these model types.
BigGAN's sample quality is the best among all compared approaches, which comes at the price of significantly lower recall. 
StyleGAN-XL allows for the truncation trick to increase sample fidelity, i.e., we can interpolate a sampled style code $w$ with the class-wise mean style code $\bar{w}$.
We observe that for StyleGAN-XL, truncation does not increase precision, indicating that developing novel truncation methods for high-diversity GANs is an exciting research direction for future work.
Interestingly, StyleGAN-XL attains high diversity across all resolutions, which can be attributed to our progressive growing strategy. Furthermore, this strategy enables to scale to megapixel resolution successfully. Training at $1024^2$ for a single V100-day yields a noteworthy FID~of~$2.8$. At this resolution, we do not compare to baselines 
because of resource constraints as they are prohibitively expensive to train. 
visualizes generated samples at increasing resolutions.
\figref{fig:highres} visualizes generated samples at increasing resolutions. In the supplementary, we show additional  interpolations and qualitative comparisons to BigGAN and ADM.
\sotapr
\highres

\subsection{Inversion and Manipulation}
GAN-editing methods first \textit{invert} a given image into latent space, i.e., find a style code $w$ that reconstructs the image as faithful as possible when passed through $\bG_s$. Then, $w$ can be manipulated to achieve semantically meaningful edits~\cite{Goetschalckx2019ICCV,Shen2020TPAMI}.

\boldparagraph{Inversion.}
Standard approaches for inverting $\bG_s$ use either latent optimization~\cite{Abdal2019ICCV,Creswell2019NEURAL,Karras2020CVPR} or an encoder~\cite{Perarnau2016ARXIV,Alaluf2021ICCV,Tov2021TOG}. A common way to achieve low reconstruction error is to use an extended definition of the latent space: $\mathcal{W}+$. For $\mathcal{W}+$ a separate $\bw$ is chosen for each layer of $\bG_s$. However, as highlighted by~\cite{Zhu2020ECCV,Tov2021TOG}, this extended definition achieves higher reconstruction quality in exchange for lower editability. Therefore, ~\cite{Tov2021TOG} carefully design an encoder to maintain editability by mapping to regions of $\mathcal{W}+$ that are close to the original distribution of $\mathcal{W}$.
We follow~\cite{Karras2020CVPR} and use the original latent space $\mathcal{W}$.
We find that StyleGAN-XL already achieves satisfactory inversion results using basic latent optimization.
For inversion on the ImageNet validation set at $512^2$, StyleGAN-XL yields $\text{PSNR}=13.5$ on average, improving over BigGAN at $\text{PSNR}=10.8$. 
Besides better pixel-wise reconstruction, StyleGAN-XL's inversions are
semantically closer to the target images.
We measure the FID between reconstructions and targets, and StyleGAN-XL attains $\text{FID}=21.7$ while BigGAN reaches $\text{FID}=47.5$. 
For qualitative results, implementation details and additional metrics, we refer to the supplementary.

Given the results above, it is also possible to further refine the obtained reconstructions.~\cite{Roich2021ARXIV} recently introduced pivotal tuning inversion (PTI). PTI uses an initial inverted style code as a pivot point around which the generator is finetuned. Additional regularization prevents altering the generator output far from the pivot. Combining PTI with StyleGAN-XL allows us to invert both in-domain (ImageNet validation set) and out-of-domain images almost precisely. At the same time, the generator output remains perceptually smooth, see~\figref{fig:interpolations}.
\interpolations

\boldparagraph{Image Manipulation.}
Given the inverted images, we can leverage GAN-based editing methods~\cite{Voynov2020ICML,Haerkoenen2020NEURIPS,Shen2021CVPR,Kocasari2021WACV,Spingarn2021ICLR} to manipulate the style code $\bw$. In \figref{fig:editing}~(Left), we first invert a given source image via latent space optimization. 
We can then apply a manipulation directions obtained by, e.g., GANspace~\cite{Haerkoenen2020NEURIPS}.
Prior work~\cite{Jahanian2020ICLR} also investigates in-plane translation. This operation can be directly defined in the input grid of StyleGAN-XL. The input grid also allows performing extrapolation, see  \figref{fig:editing}~(Left).

An inherent property of StyleGAN is the ability of style mixing by supplying the style codes of two samples to different layers of $\bG_s$, generating a hybrid image. This hybrid takes on different semantic properties of both inputs. Style mixing is commonly employed for instances of a single domain, i.e., combining two human portraits. StyleGAN-XL inherits this ability and, to a certain extent, even generates out-of-domain combinations between different classes, akin to counterfactual images~\cite{Sauer2021ICLR}. This technique works best for aligned samples, similar to StyleGAN's originally favored setting, FFHQ. Curated examples are shown in \figref{fig:editing}~(Right).
\editing
\editingsupp
