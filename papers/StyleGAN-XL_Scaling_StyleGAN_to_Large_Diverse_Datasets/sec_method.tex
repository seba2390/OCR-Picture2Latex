\section{Scaling StyleGAN to ImageNet}
\label{sec:method}
As mentioned before, StyleGAN has several advantages over existing approaches that work well on ImageNet.
But a na\"{i}ve training strategy does not yield state-of-the-art performance~\cite{Gwern2020MISC, Grigoryev2022ICLR}.
Our experiments confirm that even the latest StyleGAN3 does not scale well, see \figref{fig:teaser}. Particularly at high resolutions, the training becomes unstable.
Therefore, our goal is to train a StyleGAN3 generator on ImageNet successfully. Success is defined in terms of sample quality primarily measured by inception score (IS)~\cite{Salimans2016NEURIPS} and diversity measured by Fr\'echet Inception Distance (FID)~\cite{Heusel2017NEURIPS}.
Throughout this section, we gradually introduce changes to the StyleGAN3 baseline (\textbf{Config-A}) and track the improvements in \tabref{tab:ablation}. First, we modify the generator and its regularization losses, adapting the latent space to work well with Projected GAN (\textbf{Config-B}) and for the class-conditional setting (\textbf{Config-C}). We then revisit progressive growing to improve training speed and performance (\textbf{Config-D}). Next, we investigate the feature networks used for Projected GAN training to find a well-suited configuration (\textbf{Config-E}). Lastly, we propose classifier guidance for GANs to provide class information via a pretrained classifier (\textbf{Config-F}). Our contributions enable us to train a significantly larger model than previously possible while requiring less computation than prior art. Our model is three times larger in terms of depth and parameter count than a standard StyleGAN3. However, to match the prior state-of-the-art performance of ADM~\cite{Dhariwal2021NEURIPS} at a resolution of $512^2$ pixels, training the models on a single NVIDIA Tesla V100 takes $400$ days compared to the previously required $1914$ V100-days. We refer to our model as \textbf{StyleGAN-XL} (\figref{fig:system}).
\system
\ablation

\subsection{Adapting Regularization and Architectures}
Training on a diverse and class-conditional dataset makes it necessary to introduce several adjustments to the standard StyleGAN configuration. We construct our generator architecture using layers of StyleGAN3-T, the translational-equivariant configuration of StyleGAN3. In initial experiments, we found the rotational-equivariant StyleGAN3-R to generate overly symmetric images on more complex datasets, resulting in kaleidoscope-like patterns. 
 
\boldparagraph{Regularization.}
In GAN training, it is common to use regularization for both, the generator and the discriminator. Regularization improves results on uni-modal datasets like FFHQ~\cite{Karras2019CVPR} or LSUN~\cite{Yu2015ARXIV}, whereas it can be detrimental on multi-modal datasets~\cite{Brock2019ICLR, Gwern2020MISC}. Therefore, we aim to avoid regularization when possible.~\cite{Karras2021NEURIPS} find style mixing to be unnecessary for the latest StyleGAN3; hence, we also disable it. Path length regularization can lead to poor results on complex datasets~\cite{Gwern2020MISC} and is, per default, disabled for StyleGAN3~\cite{Karras2021NEURIPS}. However, path length regularization is attractive as it enables high-quality inversion~\cite{Karras2020CVPR}. 
We also observe unstable behavior and divergence when using path length regularization in practice. We found that this problem can be circumvented by only applying regularization after the model has been sufficiently trained, i.e., after 200k images. For the discriminator, following~\cite{Sauer2021NEURIPS}, we use spectral normalization without gradient penalties. In addition, we blur all images with a Gaussian filter with $\sigma=2$ pixels for the first $200k$ images. Discriminator blurring has been introduced in ~\cite{Karras2021NEURIPS} for StyleGAN3-R. It prevents the discriminator from focusing on high frequencies early on, which we found beneficial across all settings we investigated. 
 
\boldparagraph{Low-Dimensional Latent Space.}
As observed in~\cite{Sauer2021NEURIPS}, Projected GANs work better with FastGAN~\cite{Liu2021ICLR} than with StyleGAN. One main difference between these generators is their latent space, StyleGAN's latent space is comparatively high dimensional 
(FastGAN: $\mathbb{R}^{100}$, BigGAN: $\mathbb{R}^{128}$, StyleGAN: $\mathbb{R}^{512}$). 
Recent findings indicate that the \textit{intrinsic dimension}  of natural image datasets is relatively low~\cite{Pope2021ICLR}, ImageNet's dimension estimate is around $40$. Accordingly, a latent code of size $512$ is highly redundant, making the mapping network's task harder at the beginning of training. Consequently, the generator is slow to adapt and cannot benefit from Projected GAN's speed up. We therefore reduce StyleGAN's latent code $\bz$ to $64$ and now observe stable training in combination with Projected GAN, resulting in lower FID than the baseline (\textbf{Config-B}). We keep the original dimension of the \textit{style code} $\bw \in \mathbb{R}^{512}$ to not restrict the model capacity of the mapping network $\bG_m$.
 
\boldparagraph{Pretrained Class Embeddings.} 
Conditioning the model on class information is essential to control the sample class and improve overall performance. A class-conditional variant of StyleGAN was first proposed in~\cite{Karras2020NeurIPS} for CIFAR10~\cite{Krizhevsky2009CITESEER} where a one-hot encoded label is embedded into a 512-dimensional vector and concatenated with $\bz$. For the discriminator, class information is projected onto the last discriminator layer~\cite{Miyato2018ICLRb}. We observe that \textbf{Config-B} tends to generate similar samples per class resulting in high IS.
To quantify mode coverage, we leverage the recall metric~\cite{Kynknniemi2019NEURIPS} and find that \textbf{Config-B} achieves a low recall of~$0.004$. We hypothesize that the class embeddings collapse when training with Projected GAN. Therefore, to prevent this collapse, we aim to ease optimization of the embeddings via pretraining. We extract and spatially pool the lowest resolution features of an Efficientnet-lite0~\cite{Tan2019ICML} and calculate the mean per ImageNet class. 
The network has a low channel count to keep the embedding dimension small, following the arguments of the previous section. The embedding passes through a linear projection to match the size of $\bz$ to avoid an imbalance. Both $\bG_m$ and $\bD_i$ are conditioned on the embedding.
During GAN training, the embedding and the linear projection are optimized to allow specialization. Using this configuration, we observe that the model generates diverse samples per class, and recall increases to $0.15$ (\textbf{Config-C}). Note that for all configurations in this ablation, we restrict the training time to $15\;V\text{-}100\;days$. Hence, the absolute recall is markedly lower compared to the fully trained models. Conditioning a GAN on pretrained features was also recently investigated by~\cite{Casanova2021NEURIPS}. In contrast to our approach,~\cite{Casanova2021NEURIPS} condition on specific \textit{instances}, instead of learning a general class embedding.

\subsection{Reintroducing Progressive Growing}
Progressively growing the output resolution of a GAN was introduced by~\cite{Karras2018ICLR} for fast and more stable training. The original formulation adds layers during training to both $\bG$ and $\bD$ and gradually fades in their contribution. However, in a later work, it was discarded~\cite{Karras2020CVPR} as it can contribute to texture sticking artifacts. Recent work by~\cite{Karras2021NEURIPS} finds that the primary cause of these artifacts is aliasing, so they redesign each layer of StyleGAN to prevent it. This motivates us to reconsider progressive growing with a carefully crafted strategy that aims to suppress aliasing as best as possible. Training first on very low resolutions, as small as $16^2$ pixels, enables us to break down the daunting task of training on high-resolution ImageNet into smaller subtasks. This idea is in line with the latest work on diffusion models~\cite{Nichol2021ICML, Saharia2021ARXIV, Dhariwal2021NEURIPS, Ho2022JMLR}. They observe considerable improvements in FID on ImageNet when using a two-stage model, i.e., stacking an independent low-resolution model and an upsampling model to generate the final image.

Commonly, GANs follow a rigid sampling rate progression, i.e., at each resolution, there is a fixed amount of layers followed by an upsampling operation using fixed filter parameters. StyleGAN3 does not follow such a progression. Instead, the layer count is set to $14$, independent of the output resolution, and the filter parameters of up- and downsampling operations are carefully designed for antialiasing under the given configuration. The last two layers are critically sampled to generate high-frequency details. When adding layers for the subsequent highest resolution, discarding the previously critically sampled layers is crucial as they would introduce aliasing when used as intermediate layers~\cite{Karras2020CVPR, Karras2021NEURIPS}. Furthermore, we adjust the filter parameters of the added layers to adhere to the flexible layer specification of~\cite{Karras2021NEURIPS}; we refer to the supplementary for details. In contrast to~\cite{Karras2018ICLR} we do not add layers to the discriminator. Instead, to fully utilize the pretrained feature network $\bF$, we upsample both data and synthesized images to $\bF$'s training resolution ($224^2$ pixels) when training on smaller images.

We start progressive growing at a resolution of $16^2$ using $11$ layers. Every time the resolution increases, we cut off $2$ layers and add $7$ new ones. Empirically, fewer layers result in worse performance; adding more leads to increased overhead and diminishing returns. For the final stage at $1024^2$, we add only $5$ layers as the last two are not discarded. This amounts to $39$ layers at the maximum resolution of $1024^2$. Instead of a fixed growing schedule, each stage is trained until FID stops decreasing. We find it beneficial to use a large batch size of $2048$ on lower resolution ($16^2$ and $32^2$), similar to~\cite{Brock2019ICLR}. On higher resolutions, smaller batch sizes suffice ($64^2$ to $256^2$: $256$, $512^2$ to $1024^2$: $128$). Once new layers are added, the lower resolution layers remain fixed to prevent mode collapse. 

In our ablation study, FID improves only slightly (\textbf{Config-D}) compared to \textbf{Config-C}. However, the main advantage can be seen at high resolutions, where progressive growing drastically reduces training time. At resolution $512^2$, we reach the prior state-of-the-art (FID$\;=3.85$) after $2$ V100-days.  This reduction is in contrast to other methods such as ADM, where doubling the resolution from $256^2$ to $512^2$ pixels corresponds to increasing training time from $393$ to $1914$ V100-days to find the best performing model\footnote{Note that these settings are not directly comparable as the stem of our model is pretrained, but the values should give a general sense of the order of magnitude.}. As our aim is not to introduce texture sticking artifacts, we measure $EQ\text{-}T$, a metric for determining translation equivariance~\cite{Karras2021NEURIPS}, where higher is better. 
\textbf{Config-C} yields $EQ\text{-}T=55$, while \textbf{Config-D} attains $EQ\text{-}T=48$.
This only slight reduction in equivariance shows that \textbf{Config-D} restricts aliasing almost as well as a configuration without growing. For context, architectures with aliasing yield $EQ\text{-}T\sim 15$.


\subsection{Exploiting Multiple Feature Networks}
An ablation study conducted in~\cite{Sauer2021NEURIPS} finds that most pretrained feature networks $\bF$ perform similarly in terms of FID when used for Projected GAN training regardless of training data, pretraining objective, or network architecture. However, the study does not answer if combining several $\bF$  is advantageous. Starting from the standard configuration, an EfficientNet-lite0, we add a second $\bF$ to inspect the influence of its pretraining objective (classification or self-supervision) and architecture (CNN or Vision Transformer (ViT)~\cite{Dosovitskiy2021ICLR}). The results in \tabref{tab:ablation} show that an additional CNN leads to slightly lower FID. Combining networks with different pretraining objectives does not offer benefits over using two classifier networks. However, combining an EfficientNet with a ViT improves performance significantly. This result corroborates recent results in neural architecture literature, which find that supervised and self-supervised representations are similar~\cite{Grigg2021ARXIV}, whereas ViTs and CNNs learn different representations~\cite{Raghu2021NEURIPS}. Combining both architectures appears to have complementary effects for Projected GANs. We do not see significant improvements when adding more networks; hence, \textbf{Config-E} uses the combination of EfficientNet~\cite{Tan2019ICML} and DeiT-base~\cite{Touvron2021ICML}.

\subsection{Classifier Guidance for GANs}
\cite{Dhariwal2021NEURIPS} introduced classifier guidance to inject class information into diffusion models. Classifier guidance modifies each diffusion step at time step $t$ by adding gradients of a pretrained classifier $\nabla_{\bx_t}\log p_{\phi}(\bc|x_t, t)$. The best results are obtained by applying guidance on class-conditional models and scaling the classifier gradients by a constant $\lambda>1$.  This combination indicates that our model may also profit from classifier guidance, even though it already receives class information via embeddings.

We first pass the generated image $\bx$ through a pretrained classifier CLF to predict the class label $c_i$. We then add a cross-entropy loss $\mathcal{L}_{CE} = -\sum_{i=0}^{C} c_i \log CLF({x}_i) $ as an additional term to the generator loss and scale this term by a constant $\lambda$. For the classifier, we use DeiT-small~\cite{Touvron2021ICML}, which exhibits strong classification performance while not adding much overhead to the training. Similar to ~\cite{Dhariwal2021NEURIPS}, we observe a significant improvement in IS, indicating an increase in sample quality (\textbf{Config-F}).
We find $\lambda=8$ to work well empirically.
Classifier guidance only works well on higher resolutions ($>32^2$); otherwise, it leads to mode collapse. This is in contrast to~\cite{Dhariwal2021NEURIPS} who exclusively guide their low-resolution model. The difference stems from how guidance is applied: we use it for model training, whereas~\cite{Dhariwal2021NEURIPS} guide the sampling process.