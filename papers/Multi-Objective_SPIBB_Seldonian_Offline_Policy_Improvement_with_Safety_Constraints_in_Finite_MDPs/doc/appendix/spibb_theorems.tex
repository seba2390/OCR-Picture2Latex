\section{SPIBB - Additional details}
\label{app:spibb-additional-details}  
% ----------------------------------------------------
%               Concentration Bounds
% ----------------------------------------------------
\subsection{Concentration Bounds} 
\label{app:error_bounds}

The difference between an estimated parameter and the true one can be bounded using concentration bounds (or equivalently, Hoeffding's inequality) applied to the state-action counts $n_{\mathcal{D}}(x,a)$ in dataset $\mathcal{D}$~\citep{petrik2016safe, laroche2017safe}. Specifically, the following inequalities hold with probability at least $1-\delta = 1 - \delta' - \delta''$ for any state-action pair $(x,a) \in \mathcal{X} \times \mathcal{A}$:
\begin{align}
	\lVert p^\star(\cdot|x,a)-\hat{p}(\cdot|x,a)\rVert_1 &\leq e_p(x,a),\\
	\forall k\in[d], \lvert r^\star_k(x,a)-\hat{r}_k(x,a)\rvert &\leq e_r(x,a)r_{\mytop} \label{eq:bound-Q-2}
\end{align}
where: 
\begin{align}
    e_p(x,a) &:= \sqrt{\cfrac{2}{n_{\mathcal{D}}(x,a)}\log\cfrac{2|\mathcal{X}||\mathcal{A}|2^{|\mathcal{X}|}}{\delta'}} \label{eq:error-function-P-2} \\
    e_r(x,a) &:= \sqrt{\cfrac{2}{n_{\mathcal{D}}(x,a)}\log\cfrac{2|\mathcal{X}||\mathcal{A}|d}{\delta''}}. \label{eq:error-function-Q-2}
\end{align}

    The two inequalities can be proved similarly to \citep[Proposition 9]{petrik2016safe}. We only detail the proof for \eqref{eq:bound-Q-2}: for any $(x,a) \in \mathcal{X} \times \mathcal{A}$, and from the two-sided Hoeffding's inequality, 
    \begin{align*}
        &\mathbb{P} \left( \forall (x,a), \big\lvert r^\star_k(x,a) -\hat{r}_k(x,a) \big\rvert > e_r(x,a)r_{\mytop} \right) 
        \\
        &\qquad\qquad = \mathbb{P} \left( \forall (x,a), \frac{\big\lvert r^\star_k(x,a)-\hat{r}_k(x,a) \big\rvert}{2 V_{max}} > \sqrt{\frac{1}{2 n_\mathcal{D}(x,a)} \log \frac{2 |\mathcal{X}||\mathcal{A}|d}{\delta''}} \right) \\
        &\qquad\qquad \leq 2 \exp \left( -2 n_\mathcal{D}(x,a) \frac{1}{2 n_\mathcal{D}(x,a)} \log \frac{2 |\mathcal{X}||\mathcal{A}|}{\delta''}  \right) \\
        &\qquad\qquad \leq \frac{\delta''}{| \mathcal{X} | | \mathcal{A} | d}
    \end{align*}
    
    By summing all $|\mathcal{X}| |\mathcal{A}|d $ state-action-reward tuples error probabilities lower than $\frac{\delta''}{| \mathcal{X} | | \mathcal{A} |d }$, we obtain \eqref{eq:bound-Q-2}. If we choose $e(x,a)=e_p(x,a)=e_r(x,a)$, we get that:
    \begin{align}
        \cfrac{2}{n_{\mathcal{D}}(x,a)}\log\cfrac{2|\mathcal{X}||\mathcal{A}|2^{|\mathcal{X}|}}{\delta'} &= \cfrac{2}{n_{\mathcal{D}}(x,a)}\log\cfrac{2|\mathcal{X}||\mathcal{A}|d}{\delta''} \\
        \cfrac{2^{|\mathcal{X}|}}{\delta'} &= \cfrac{d}{\delta''} \\
        \delta'' &= d\delta' 2^{-|\mathcal{X}|}
    \end{align}
    
    It means that $\delta = \delta' + \delta'' = \delta'(1+d2^{-|\mathcal{X}|})$. The cost in terms of approximation is therefore linear with a very small slope, inside square root of log, which means that it will basically have an insignificant impact on the concentration bound.


% ----------------------------------------------------
%               Advantage Constraints
% ----------------------------------------------------
\subsection{Need of advantageous constraints}
\label{app:spibb-need-of-advatangeous}



\begin{prop}
\label{prop:mo-sipbb-advantageous}
The advantageous constraints in \ref{eq:s-opt} ensure that performance constraints w.r.t. the individual returns are respected in $\mhat$, i.e., $ \forall k \in [d],\; \J{\pi}{k}{\mhat} - \J{\pib}{k}{\mhat} \ge 0$.
\end{prop}

\begin{proof}
For the $k$\textsuperscript{th} reward function, we can estimate the advantage function in an MDP $m$ as:
\begin{align*}
    \adv{\pib}{k}{m}{x,a} = \qval{\pib}{k}{m}{x,a} - \val{\pib}{k}{m}{x}
\end{align*}
Similarly, let $\rho^{\pi}_{m}(x)$ denote the normalized discounted future state distribution:
\begin{align*}
    \rho^{\pi}_{m}(x) &= (1-\gamma)\sum_{t=0}^{\infty} \gamma^t \mathbb{P}(X_t=x | \pi, X_0=x_0),
\end{align*}
where $X_{t} \sim p(\cdot | X_{t-1}, A_{t-1}), A_{t-1} \sim \pi(\cdot|X_{t-1})$.
From Performance Difference Lemma \citep{kakade2002approximately}, we have the following result:
\begin{align}
    \label{eq:spibb-prop-adv}
    \J{\pi}{k}{\mhat} - \J{\pib}{k}{\mhat} &= \sum_{x \in \X} \rho^{\pi}_{\mhat}(x) \underbrace{\sum_{a \in \A} \pi(a|x) \adv{\pib}{k}{\mhat}{x,a}}_{\text{advantage constraint}}
\end{align}

The first term in the above equation $\rho^{\pi}_{\mhat}(x) \ge 0$ for any $x \in \X$. The second term is the advantage constraint in the construction of \ref{eq:s-opt}.
Therefore, any solution of \ref{eq:s-opt} satisfies $\sum_{a \in \A} \pi(a|x) \adv{\pib}{k}{\mhat}{x,a} \ge 0, \forall x \in \X$.

As both the terms in \Cref{eq:spibb-prop-adv} are $\ge 0 \; \forall x \in \X$, this implies $ \J{\pi}{k}{\mhat} - \J{\pib}{k}{\mhat} \ge 0$.

\end{proof}



% ------- Old prop
% \begin{prop}
% \label{prop:spibb-r-advatangeous}
% If $\pi'$ is the solution to problem \Cref{eq:s-opt} without the $R$-constraints, i.e. without $\sum_a \pi(a|x) \Adv{\pib}{R}{\mhat}(x, a) \geq 0$ term, then $\pi'$ won't necessarily $\pib$-advantageous in $\mhat$ with respect to $\pib$ regarding reward performance $R$:
% \begin{equation*}
%     \sum_a \pi'(a|x) \Adv{\pib}{R}{\mhat}(x, a) \not\geq 0, \forall x \in \X.
% \end{equation*}
% \end{prop}
% \begin{proof}
% \htodo{Write this as a proof by contradiction, assume $\lR, \lC \geq 0$ and advantageous wrt costs are true, and then continue}
% Since $\pi'$ is the solution of \Cref{eq:s-opt} (without $R$-constraints) and $\pib$ also lies in the solution space $\Pi$, we have for any $x \in \X$:
% \begin{align*}
%     &\langle \pi'(\cdot|x) Q_{\lambda}^{\pi_b}(x,\cdot) \rangle \ge \langle \pib(\cdot|x) Q_{\lambda}^{\pib}(x,\cdot) \rangle \\ 
%     &\sum_{a \in \A} \left( \lR \pi'(a|x) \Q{\pib}{R}{\mhat}(x,a ) - \sum_i \left( \lC \pi'(a|x) \Q{\pib}{\ci}{\mhat}(x,a) \right) \right) \ge \\ 
%     &\quad \quad \sum_{a \in \A} \left( \lR \pib(a|x) \Q{\pib}{R}{\mhat}(x,a ) - \sum_i \left( \lC \pib(a|x) \Q{\pib}{\ci}{\mhat}(x,a) \right) \right)
% \intertext{Taking all the terms w.r.t $R$ on left and the rest on R.H.S, we get:}
%     &\lR \left( \sum_{a \in \A}\pi'(a|x) \Q{\pib}{R}{\mhat}(x,a)  -  \underbrace{\sum_{a \in \A} \pib(a|x) \Q{\pib}{R}{\mhat}(x,a)}_{\V{\pib}{R}{\mhat}(x)}  \right) \ge \\ 
%     &\quad \quad \sum_i \lC \left( \sum_{a \in \A} \pi'(a|x) \Q{\pib}{\ci}{\mhat}(x,a)  -  \underbrace{\sum_{a \in \A} \pib(a|x) \Q{\pib}{\ci}{\mhat}(x,a)}_{\V{\pib}{\ci}{\mhat}(x)}  \right)
% \end{align*}
% Using the definition of the advantage function we have:
% \begin{align*}
%     \lR \left( \sum_{a \in \A}\pi'(a|x) \Adv{\pib}{R}{\mhat}(x,a) \right) 
%     &\ge \sum_i \lC \left( \underbrace{\sum_{a \in \A} \pi'(a|x) \Adv{\pib}{\ci}{\mhat}(x,a)}_{\le 0} \right) 
% \end{align*}
% We have $\Adv{\pib}{\ci}{\mhat}(x,a) \le 0 \; \forall i$ by construction, and $\lR \ge 0$ and $\lC \ge 0$. When $\lR > 0,  \lC >0$ and the term on R.H.S. is negative $<0$, then that implies that it might not longer be necessary that $\pib$-advantageous property holds true in this case and the term $\sum_{a \in \A}\pi'(a|x) \Adv{\pib}{R}{\mhat}(x,a)$ can be negative.

% \textit{Remark:} In the case of Soft-SPIBB, $\lC = 0, \lR = 1$, and as such the R.H.S. is always 0 and we have:
% \begin{align*}
% \left( \sum_{a \in \A}\pi'(a|x) \Adv{\pib}{R}{\mhat}(x,a) \right) &\ge 0.
% \end{align*}

% \end{proof}



% ----------------------------------------------------
%               Soft-SPIBB 1-step
% ----------------------------------------------------
\subsection{MO-SPIBB Results}
\label{app:mo-spibb-prop}

Using the results from \Cref{app:error_bounds} and \Cref{app:spibb-need-of-advatangeous}, we can directly apply the Soft-SPIBB theorems to individual objectives in \ref{eq:s-opt}. For instance, we get the following result about 1-step policy improvement guarantees directly from Theorem 1 of Soft-SPIBB:

\begin{prop}
The policy $\pi$ returned from solving the \ref{eq:s-opt} satisfies the following property in every state $x$ with probability at least $(1 - \delta)$:
\begin{align}
    \forall k \in [d], \val{\pi}{k}{\mopt}{x} - \val{\pib}{k}{\mopt}{x} \geq -\frac{\epsilon v_{\text{max}}}{1-\gamma},
\end{align}
where $v_{\text{max}} \le \frac{\rmax}{1-\gamma}$ is the maximum of the value function.
\end{prop}

\begin{proof}
We will show the policy returned by \ref{eq:s-opt} satisfies both the properties required for applying the Theorem 1 of Soft-SPIBB:


\begin{itemize}
    \item $\pi$ is $(\pib, \epsilon, e)$-constrained: This is equivalent to $\sum_{a \in \A} e(x,a)\ |\pi(a|x) - \pib(a|x)| \leq \epsilon$, that is true by construction.
    
    \item $\pib$-advantageous in $\mhat$: For $k$\textsuperscript{th} reward function, this is equivalent to $\J{\pi}{k}{\mhat} - \J{\pib}{k}{\mhat} \ge 0$, which is also true from construction.
\end{itemize}
From there, the exact statement of Theorem 1 can be applied directly to get the above result.
\end{proof}





% \begin{proof}

% \textbf{For rewards:} 
% We first use  \Cref{prop:spibb-r-advatangeous} to show that we need additional constraints for $\pib$-advantageous property to hold true for $R$. Once we have added those to \Cref{eq:s-opt}, we can use the exact same structure of the proof of Theorem 1 of \cite{nadjahi2019safe}. 

% \textbf{For constraints:} 
% From Proposition 1 of \cite{nadjahi2019safe}, we have the following for any constraint $C_i$:
% \begin{align}
%     {Vc}^{\pi}_{i, M^*}(x) - {Vc}^{\pi_b}_{i, M^*}(x)  &= {Qc}^{\pi_b}_{M^*} (\pi - \pi_b) 
%     d^{\pi}_{M^*} \nonumber \\
%     &= \left(Qc^{\pi_b}_{M^*} - Qc^{\pi_b}_{\hat{M}} + Qc^{\pi_b}_{\hat{M}} \right)(\pi - \pi_b) d^{\pi}_{M^*} \nonumber \\ 
%     &= \left(Qc^{\pi_b}_{M^*} - Qc^{\pi_b}_{\hat{M}}\right) (\pi - \pi_b) d^{\pi}_{M^*}  +  Qc^{\pi_b}_{\hat{M}}(\pi - \pi_b)d^{\pi}_{M^*} \label{eq:thm1-proof}.
% \end{align}

% We will now show the first term is bounded by $\frac{\epsilon {Vc}_{i, \max}}{(1-\gamma)}$ using the Holder's inequality.

% \begin{align}
%     \norm{\left(Qc^{\pi_b}_{M^*} - Qc^{\pi_b}_{\hat{M}}\right) (\pi - \pi_b) d^{\pi}_{M^*}}_{\infty} &= \norm{\left(Qc^{\pi_b}_{M^*} - Qc^{\pi_b}_{\hat{M}}\right) (\pi - \pi_b)}_{\infty} \norm{d^{\pi}_{M^*}}_{1} \\ 
%     &= \max_{x} \sum_{a} \left(Qc^{\pi_b}_{M^*} - Qc^{\pi_b}_{\hat{M}}\right) (\pi - \pi_b) \frac{1}{(1 - \gamma)}\\
%     &\leq \frac{\epsilon {Vc}_{\max}}{(1 - \gamma)}.
% \end{align}

% The last line comes from the $(\pi_b, e_Q, \epsilon)$-constrained property and Eq.~\ref{eq:eQc}. The next part is to show the second part of Eq.~\ref{eq:thm1-proof} is negative. All the terms of $d^{\pi}_{M^*}$ are positive so if all the terms of vector $Qc^{\pi_b}_{\hat{M}}(\pi - \pi_b) \leq 0$ we can upper bound the result by the first term. For each $x \in \X$, we have:
% \begin{align*}
%     Qc^{\pi_b}_{\hat{M}}(\pi - \pi_b) &= \sum_a Qc^{\pi_b}_{\hat{M}}(\pi(a|x) - \pi_b(a|x)) \\ 
%     &= \sum_a Qc^{\pi_b}_{\hat{M}}\pi(a|x) - {Vc}^{\pi_b}_{\hat{M}}(x) \\ 
%     &= \sum_{a} {Ac}^{\pi_b}_{i, \hat{M}}(x,a) \pi(a|x) \\ 
%     &\leq 0
% \end{align*}
% The last inequality comes from the construction in the Eq.~\ref{eq:s-opt}. This concludes the proof.

% \end{proof}

% ----- multi-step stuff

% It is possible to search over the class of $(\pib, e_P, \epsilon)$-constrained policies, where $e_p$ is the error bound over the transition function. Using the procedure in \cite{nadjahi2019safe} it is possible to guarantee safety bounds under the Assumption~\ref{assm:eP-bounded}.

% \begin{assumption}
% \label{assm:eP-bounded}
% There exists a constant $\kappa < \frac{1}{\gamma}$ such that, for all state-action pairs $(x,a) \in \X \times \A$ the following inequality holds:
% \begin{equation}
%     \label{eq:assm-eP}
%     \sum_{x', a'} e_P(x', a') \pi_b(a'|x') P^{*}(x'|x,a) \leq \kappa e_P(x,a).
% \end{equation}
% \end{assumption}

% \begin{prop}[Multi-step PI]
% \label{thm:eP-guarantee}
%     Under Assumption~\ref{assm:eP-bounded}, the policy $\pi$ returned by the policy iteration step given by \ref{eq:s-opt} satisfies satisfies the following inequalities in every state $x$ with probability at least $(1 - \delta)$:
%     \begin{align*}
%         \V{\pi}{R}{\mopt}(x) - \V{\pib}{R}{\mopt} (x) &\geq 
%         % (\V{\pi}{R}{\mhat}(x) - \V{\pib}{R}{\mhat}(x)) 
%         - 2 \norm{d^{\pib}_{\mopt}(\cdot|x)  
%         -  d^{\pib}_{\mhat}(\cdot|x) }_1 V^{R}_{\max} 
%         - \frac{1 + \gamma}{(1 - \gamma)^2 (1 - \kappa \gamma)}\epsilon V^R_{\max}, \\ 
%         \V{\pi}{\ci}{\mopt}(x) - \V{\pib}{\ci}{\mopt} (x) &\leq 2 \norm{d^{\pib}_{\mopt}(\cdot|x) + d^{\pib}_{\mhat}(\cdot|x) }_1 V^{\ci}_{\max} 
%         + \frac{1 + \gamma}{(1 - \gamma)^2 (1 - \kappa \gamma)}\epsilon V^{\ci}_{\max}  \tag{$\forall i$}.
%     \end{align*}
% \end{prop}


% The first inequality for $V^R$ follows directly from Theorem 2 of \cite{nadjahi2019safe}. We will show the proof for the second part, i.e, the extension with constraints using the same approach.

% \begin{proof}

% From \citep[Theorem 2]{nadjahi2019safe} we have under the Assumption ~\ref{assm:eP-bounded}, any $(\pib, e_P, \eps)$-constrained policy $\pi$ satisfies the following inequality for every state-action pair $(x, a)$ with probability at least $1-\delta$:
% \begin{align*}
%       \V{\pi}{R}{\mopt}(x) - \V{\pib}{R}{\mopt} (x) &\geq \V{\pi}{R}{\mhat}(x) - \V{\pib}{R}{\mhat} (x) - 2 \norm{d^{\pib}_{\mopt}(\cdot|x) -  d^{\pib}_{\mhat}(\cdot|x) }_1 V^{R}_{\max} \\
%         &\quad - \frac{1 + \gamma}{(1 - \gamma)^2 (1 - \kappa \gamma)}\epsilon V^R_{\max}.
% \end{align*}

% We will show that the first term $(\V{\pi}{R}{\mhat}(x) - \V{\pib}{R}{\mhat}(x))$ is positive, and use that to lower bound the expression above. From \citet[Proposition 1]{nadjahi2019safe} we have: 
% \begin{align*}
%     V^{\pi_1} - V^{\pi_2} &= Q^{\pi_2} (\pi_1 - \pi_2) d^{\pi_1}. 
% \end{align*}
% Substituting $\pi_1 =\pi$ and $\pi_2 =\pi_{b}$, we have:
% \begin{align*}
%     V^{\pi} - V^{\pi_b} &= Q^{\pi_b}(\pi - \pi_b) d^{\pi}
% \end{align*}
% As the term $d^\pi(x) \geq 0$ for any $x$, we need to show the term $Q^{\pi_b}(\pi - \pi_b)$ is positive.  For any $x \in \X$, we have:
% \begin{align*}
%     Q^{\pi_b}_{\hat{M}}(\pi - \pi_b)(x) &= \sum_a Q^{\pi_b}_{\hat{M}}(x,a)(\pi(a|x) - \pi_b(a|x)) \\ 
%     &= \sum_a Q^{\pi_b}_{\hat{M}}(x,a)\pi(a|x) - {V}^{\pi_b}_{\hat{M}}(x) \\ 
%     &= \sum_{a} {A}^{\pi_b}_{\hat{M}}(x,a) \pi(a|x) \\ 
%     &\geq 0
% \end{align*}
% The last inequality comes from \Cref{lemma:spibb-r-advatangeous}. This concludes the proof for the rewards. 

% The proof for the constraints follows the same procedure, but now we will upper bound the difference in performance. From \cite[Theorem 2]{nadjahi2019safe} for constraints we get, for any $i \in \{1, \dots, m\}$:
% \begin{align*}
%       \V{\pi}{\ci}{\mopt}(x) - \V{\pib}{\ci}{\mopt} (x) &\leq - \Big( \V{\pi}{\ci}{\mhat}(x) - \V{\pib}{\ci}{\mhat} (x) \Big) + 2 \norm{d^{\pib}_{\mopt}(\cdot|x) +  d^{\pib}_{\mhat}(\cdot|x) }_1 V^{\ci}_{\max} \\
%         &\quad - \frac{1 + \gamma}{(1 - \gamma)^2 (1 - \kappa \gamma)}\epsilon V^{\ci}_{\max}.
% \end{align*}
% As in the previous case, if we can show the term $(\V{\pi}{\ci}{\mhat}(x) - \V{\pib}{\ci}{\mhat} (x) )$ is negative, and use that to bound the expression. As in Section~\ref{app:proof-thm1}, we have: 
% \begin{align*}
%     \V{\pi}{\ci}{\mhat}(x) - \V{\pib}{\ci}{\mhat} (x) &\leq 0,  
% \end{align*}
% that again comes from using the construction in \ref{eq:s-opt}. This concludes the proof for constraints.
    
    
% \end{proof}
