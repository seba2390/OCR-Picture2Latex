\section{Scalarized safety constraints}
\label{app:naive-construction}

Instead of having constraints of the form in \Cref{eq:general-safety-constraints}, it is possible to define the constraints in terms of the scalarized objective directly, i.e., 
\begin{align*}
    \mathbb{P}\left(    \sum_{k\in[d]} \lambda_i \J{\pi}{i}{\mstar}  - \sum_{k\in[d]} \lambda_i \J{\pib}{i}{\mstar} > - \zeta \Big| \D \right) > 1 -\delta.
\end{align*}
 

Without loss of generality, if we assume there are only two objective ($d=2$), then satisfying the above constraint implies:
\begin{align*}
    \lambda_0 \left(\J{\pi}{0}{\mstar} - \J{\pib}{0}{\mstar} \right) + \lambda_1 \left(\J{\pi}{1}{\mstar} - \J{\pib}{1}{\mstar}  \right) \geq 0 .
\end{align*}
Consider a scenario where the solution policy performs poorly w.r.t. the second objective, i.e, $\lambda_1 (\J{\pi}{1}{\mstar}  - \J{\pib}{1}{\mstar}) < 0$, however the the improvement in the first objective is very large $\lambda_0 (\J{\pi}{0}{\mstar} - \J{\pib}{0}{\mstar}) >> 0$. In this case, even though the linearized cumulative constraint regarding the performance improvement is being satisfied, it fails to guarantee the improvement across each individual objectives.


% As an informal proof, notice that the new constraint in \Cref{eq:general-task-objective} will now be $\lR \J{\pi}{R}{\mstar} - \lC  \J{\pi}{C}{\mstar} \geq \lR \J{\pib}{R}{\mstar} - \lC \J{\pib}{C}{\mstar}$. 

% Satisfying this constraint implies that  $\lR \left( \J{\pi}{R}{\mstar} - \J{\pib}{R}{\mstar} \right) \bm{+} \lC \left( \J{\pib}{C}{\mstar}  - \J{\pi}{C}{\mstar} \right) \geq 0 $.

% Now, assume that if the solution policy actually performs poorly w.r.t. $C$, i.e., $(\J{\pib}{C}{\mstar}  - \J{\pi}{C}{\mstar}) < 0$, but the performance in rewards is very large i.e, $(\J{\pi}{R}{\mstar} - \J{\pib}{R}{\mstar}) >> 0$, then even though the linearized constraint regarding performance is being satisfied, it still violates the individual cost constraint.