\section{HCPI - Additional details}
\label{app:hcpi-details}

\paragraph{Concentration Inequalities:} We experimented with the following concentration inequalities \citep{thomas2015highImprovement}:
\begin{itemize}[leftmargin=*]
    \item Extension of Empirical Bernstein \citep{maurer2009empirical}: This is the extension of
    Maurer \& Pontil's empirical Bernstein (MPeB) inequality. From Theorem 1 of \cite{thomas2015highEvaluation}: Let $X_1, \dots, X_n$ denote $n$ independent real-valued random variables, such that for each $i \in \{1,\dots,n\}$, we have $\pr(0 \le X_i) = 1,  \E[X_i] \le \mu$, and some fixed real-valued threshold $c_i > 0$. Let $\delta > 0$ and $Y_i = \min\{X_i, c_i\}$, then with probability at least $(1-\delta)$:
    \begin{align}
        \mu &\ge \sum_{i=1}^{n}\left(\frac{1}{c_i}\right)^{-1} \sum_{i=1}^{n} \frac{Y_i}{c_i} - \sum_{i=1}^{n}\left(\frac{1}{c_i}\right)^{-1} \frac{7n \ln(2/\delta)}{3n-1} - 
        \sum_{i=1}^{n}\left(\frac{1}{c_i}\right)^{-1} \sqrt{\frac{\ln(2/\delta)}{n-1} \sum_{i,j=1}^{n}\left(\frac{Y_i}{c_i} - \frac{Y_j}{c_j}\right)^2}.
    \end{align}
    In context of this paper, for the $k$\textsuperscript{th} reward function, $X_i$ denotes the $\IS$ estimated return for that trajectory, i.e., $\IS_k(\tau_i,\pi_t, \pib)$. Here, $c_i$ is a hyper-parameter that needs to be tuned. In \cite{thomas2015highImprovement}, a fixed value of $c$ is used for all $c_i$. 
    
    \item Student's t-test \citep{walpole1993probability}: This is an approximate concentration inequality that is based on the assumption that the mean returns are distributed normally. For $k$\textsuperscript{th} reward, the \Cref{eq:hcope-R-lower-bound} can be written as:
    \begin{align}
    \pr \Big( \J{\pi_t}{k}{\mopt} \ge \IS_{k}(\D, \pi_t, \pib) - \frac{\hat{\sigma}_k}{\sqrt{|\D|}}t_{1-\delta/d, |\D|-1} \Big) \ge 1 - \delta/d, 
    \end{align}
    where $\hat{\sigma}_k$ is the sample standard deviation:
    \begin{align}
    \hat{\sigma}_k &= \sqrt{\frac{1}{|\D|-1} \sum_{i=1}^{|\D|}(\IS(\tau_i, \pi_t, \pib) - \overline{\IS} )^2 },
    \end{align}
    and $\overline{\IS} = \frac{1}{|\D|}\sum_{i=1}^{|\D|} \IS(\tau_i,\pi_t,\pib)$ and $t_{1-\delta/d, |\D|-1}$ is the $100(1-\delta/d)$ percentile of the student t-distribution with $|\D|-1$ degrees of freedom. 
    
\end{itemize}

We experimented with both MPeB Extension (with $c=0.5$) and Student's t-test inequalities and found that the solutions returned by the former to be very conservative. Therefore, we use t-test in all of our experiments. Even though the t-test's assumption (normally distributed returns) is technically false, it's a reasonable assumption due to central limit theorem. 
% (it's an assumption used in almost all scientific research when computing p-values)
The consequence is that the failure rate (the chance of deploying an unsafe policy) can, in theory, be higher than desired, though, in practice, that's unlikely.


\paragraph{Regularization:} 
For small problems, \ref{eq:h-opt} can be solved with methods like CMA-ES \citep{hansen2006cma}. 
% In practice, a policy is first derived using the traditional offline RL methods based on $D_{tr}$ and is regularized using $\pib$ to obtain a set of candidate policies. 
% 
For stochastic policies, 
as the optimization problem in \ref{eq:h-opt} is difficult to solve directly, we need to resort to a regularization based heuristic \citep{thomas2015highImprovement, laroche2017safe}. Let $\pi_t$ denote the solution policy found using $\D_{tr}$ using any of the traditional offline RL methods. A set of candidate policies is built using the baseline policy: $\pi_{\text{Cand}} = \{ (1-\alpha)\pi_t + \alpha \pi_b\}$, where $\alpha \in \set{0.0, 0.1, \dots, 0.9}$ is the regularization hyper-parameter. 
The best performing candidate policy that satisfies the safety-test (the performance constraints based on $\D_s$) is then returned.
If none of the candidate policies satisfy the safety-test, the baseline policy is returned.

For finding $\pi_t$, we experimented with both the Linearized and Adv-Linearized baselines in \Cref{sec:synthetic-experiments} and found that Adv-Linearized worked better (higher improvement over $\pib$ while failure rate $<\delta$). Therefore in our experiments, we first find $\pi_t$ using Adv-Linearized and then regularize it using $\pib$ to build the set of candidate policies $\pi_{\text{Cand}}$. 


\paragraph{Safety-guarantees:}
We get the safety guarantees related to \ref{eq:h-opt} directly from \cite{thomas2015highImprovement, Thomas2019}.  The constraints of \ref{eq:h-opt} define the new safety-test that ensures a candidate policy will only be returned if the individual performance guarantees corresponding to each reward function are satisfied. This procedure will only make error in the scenario where the performance constraint related to $k$\textsuperscript{th} is satisfied, i.e, $( \IS_{k}(\D_{s}, \pi, \pib) - \CI_k(\D_s, \delta/d) \geq \mu_k)$,  but in practice the policy is not good enough $(\J{\pi}{k}{\mopt} < \mu_k)$. By transitivity this implies $\J{\pi}{k}{\mopt} < \big( \IS_{k}(\D_{s}, \pi, \pib) - \CI_k(\D_s, \delta/d) \big)$, which from \Cref{eq:hcope-R-lower-bound} we know can only occur with probability at most $\delta/d$. Using the union bound, we know that cumulative probability of the union of any of these $d$ possible scenarios is $\leq \delta$.


\paragraph{Computational cost:} Compared to regular HCPI, there is an increase in computational cost proportional to the number of reward functions $d$. The value and advantage functions estimation cost increases by a factor of $d$: respectively $\mathcal{O}(d|\X|^3)$ and $\mathcal{O}(d|\A||\X|^2)$, the $\IS$ estimation also increases by factor of $d$, and the computational cost for safety-test also increases by $d$: $\mathcal{O}(d|\D|)$. 


% ---------------------------------------------------
%                   Old Stuff 
% ---------------------------------------------------


% \subsection{Chernoff-Hoeffding's Inequality for upper and lower bounds}
% \label{app:hcpi-hoeffding-bounds}


% Let $X_1, \dots, X_n$ be $n$ independent random variables such that $\pr(X_i \in [a_i, b_i])=1$. Let $S_n = \frac{1}{n} \sum_i X_i$ denote the empirical mean and $\E[S_n]$ be the true mean. Then using Chernoff-Hoeffding's Inequality \cite{hagerup1990guided} we have  for any $t>0$, we have:
% \begin{equation*}
%     \pr (|S_n - \E[S_n]| \ge t) \le 2 \exp^{-\frac{2t^2}{\sum_{i=1}^{n}(b_i - a_i)^2 }}
% \end{equation*}

% To invert this bound, set $\delta = 2 \exp^{-\frac{2t^2}{\sum_{i=1}^{n}(b_i - a_i)^2 }} \in (0,1)$. Solving for $t$, we see that with probability at least $1-\delta$,
% \begin{align*}
%     |S_n - \E [S_n]| &\leq \sqrt{\frac{\ln(\frac{1}{\delta}) \sum_{i=1}^n(b_i - a_i)^2}{2n^2}} \\ 
%     \left| \frac{1}{n}\sum{X_i} - \E \left[ \frac{1}{n} \sum_{i} X_i\right] \right| &\le  \sqrt{\frac{\ln(\frac{1}{\delta}) \sum_{i=1}^n(b_i - a_i)^2}{2n^2}}.
% \end{align*}

% From the above inequality we get:
% \begin{align*}
%     \pr \left(\E \left[ \frac{1}{n} \sum_{i} X_i\right] \ge \frac{1}{n}\sum{X_i} -    \sqrt{\frac{\ln(\frac{1}{\delta}) \sum_{i=1}^n(b_i - a_i)^2}{2n^2}} \right) \ge 1 - \delta,
%     \intertext{and}
%     \pr \left(\E \left[ \frac{1}{n} \sum_{i} X_i\right]  \le   \frac{1}{n}\sum{X_i} + \sqrt{\frac{\ln(\frac{1}{\delta}) \sum_{i=1}^n(b_i - a_i)^2}{2n^2}} \right) \ge 1 - \delta.
% \end{align*}



% \harsh{Proof of Hoeffding's from: \url{http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Jan29_Tudor.pdf}
% }

% \subsection{Proof of \Cref{prop:hcpi-safety-guarantee}}
% \label{app:proof-h-opt-safety-guarantee}

% \begin{prop}[Safety guarantees with HCPI]
% \label{prop:hcpi-safety-guarantee}
% When the test set $\D_s$ is big enough to build reliable high-confidence lower bounds, i.e., $\forall i\in [d] \; \IS_{i}(\D_s, \pi_t, \pi_b) \approx \IS_{i}(\D, \pi_t, \pi_b)$, the policy $\pi$ returned by \Cref{eq:h-opt} will only violate the safety guarantees with probability at most $\delta$.
% \end{prop}

% The constraints of \Cref{eq:h-opt} define the new safety-test that ensures a candidate policy will only be returned if the individual performance guarantees are satisfied. This procedure will only make error in either of the following scenarios:
% \begin{itemize} %enumerate?
%     \item The performance constraint related to $R$ is satisfied $( \IS_{R}(\D_{s}, \pi, \pib) - \CI_R(\D_s, \delta) \geq b_R)$,  but in practice the policy is not good enough $(\J{\pi}{R}{\mopt} < b_R)$. By transitivity this implies $\J{\pi}{R}{\mopt} < \big( \IS_{R}(\D_{s}, \pi, \pib) - \CI_R(\D_s, \delta) \big)$, which from \Cref{eq:hcope-R-lower-bound} we know can only occur with probability at most $\delta/2$.
    
%     \item If the cost performance constraint is satisfied $( \IS_{C}(\D_{s}, \pi, \pib) + \CI_C(\D_s, \delta) \leq b_C )$ but the policy ends up violating the constraint in practice $(\J{\pi}{C}{\mopt} > b_{C})$. Again, by transitivity this means that $\J{\pi}{C}{\mopt} > \big( \IS_{C}(\D_{s}, \pi, \pib) + \CI_C(\D_s, \delta) \leq b_C \big)$, and that can only happen with probability at most $\delta/2$ because of \Cref{eq:hcope-C-upper-bound} .
% \end{itemize}

% Using the union bound, we know that cumulative probability of the union of either of these two events is $\leq \delta$.




