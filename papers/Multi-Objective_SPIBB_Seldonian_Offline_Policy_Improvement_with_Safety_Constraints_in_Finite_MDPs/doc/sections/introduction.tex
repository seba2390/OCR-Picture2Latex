\section{Introduction}
\label{sec:introduction}
Reinforcement Learning (RL) as a paradigm for sequential decision-making \citep{sutton1988learning} has shown tremendous success in a variety of simulated domains \citep{mnih2015human, silver2017mastering, OpenAI_dota}.
% However, from a real-world application point of view, there are still quite a few
However, there are still quite a few challenges between the traditional RL research and real-world tasks.
Most of these challenges stem from assumptions that are rarely satisfied in practice \citep{dulac2019challenges}, or the inability of the algorithm's user to specify the desired behavior of the agent without being a domain expert \citep{Thomas2019}.
%
We focus on the real-world application point of view and posit the following requirements:
% We posit the following requirements:
\begin{itemize}[topsep=0pt, leftmargin=*]
% \begin{itemize}[leftmargin=*,]
    \item \textbf{Multiple reward functions:} Traditional RL methods assume a single scalar reward is present in the environment.
    However, most real-world tasks, have multiple (possibly conflicting) objectives or constraints that need to be taken into consideration together, such as the signals related to the safety (physical well-being of the agent or the environment), budget utilization (energy or maintenance costs), etc.
    
    \item \textbf{Stakeholder control of the trade-off:} 
    The ML practitioners should have the ability to control the different trade-offs the agent is making and choose the one they consider best for the task at hand. 
    
    \item \textbf{Offline setting:} 
    In many real-world domains (e.g., healthcare, finance or autonomous vehicles), there is an abundance of data, collected under a sub-optimal policy, but training the agent directly via interactions with the environment is expensive and risky.
    We assume that we only have access to a dataset of past trajectories that can be used for training \citep{lange2012batch}. 
    
    \item \textbf{Preventing unintended behavior:} We want the agent to be robust to 
    both extrapolation errors from offline RL and misaligned objectives that are poor proxy of the user's intentions and algorithm's actual performance \citep{ng1999policy, amodei2016concrete}.
    We consider the case where the user can specify undesirable behavior in the context of the performance observed in the batch. 
    
    \item \textbf{Practical guarantees:}
    We want guarantees about the undesirable behavior that might be caused by the agent in the real-world. We care about the results that can be obtained using the finite amount of samples we have in the batch, and aim to provide some measure of confidence in deploying the agents in the environment. 
\end{itemize}
% \end{enumerate}


To achieve this set of properties, we adopt the Seldonian framework~\citep{Thomas2019}, which is a general algorithm design framework that allows high-confidence guarantees for constraint satisfaction in a multi-objective setting. 
Based on the above specifications, we seek to answer the question:
\textit{if we are given a batch of data collected under some (suboptimal) behavioral policy and some user preference, can we build a policy improvement algorithm that returns a policy with practical high-confidence guarantees on the performance of the policy w.r.t. the behavioral policy?}

% talk about limitations in brief
We acknowledge that there are other important challenges in RL, 
such as partial observability, safe exploration, non-stationary environments and function approximation in high-dimensional spaces, that also stand in the way of making RL a more applicable paradigm. These challenges are beyond the scope of this work, which should rather be thought of as taking a step towards this broader goal.

In \Cref{sec:related-work}, we present our contribution positioned with respect to other related work. 
In \Cref{sec:methodology}, we formalize the setting and then extend traditional SPI algorithms to this setting. 
We then show it is possible to extend the previous work on Safe Policy Iteration (SPI), particularly Safe Policy Iteration with Baseline Bootstrapping~\citep[SPIBB,][]{laroche2017safe},  for the design of agents that satisfy the above requirements. 
% Our main contribution is the extension of the SPIBB algorithm to this setting.
We show that the resulting algorithm is theoretically-grounded and provides practical high-probability guarantees. 
We extensively test our approach on a synthetic safety-gridworld task in \Cref{sec:synthetic-experiments} and show that the proposed algorithm achieves better data efficiency than the existing approaches.
Finally, we show its benefits on a critical-care task in  \Cref{sec:sepsis-experiments}. 
The accompanying codebase is available at \url{https://github.com/hercky/mo-spibb-codebase}.