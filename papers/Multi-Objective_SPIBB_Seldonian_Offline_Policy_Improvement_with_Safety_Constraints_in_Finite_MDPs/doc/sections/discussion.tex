\section{Conclusion}
\label{sec:conclusion}

We present a new Seldonian RL algorithm that takes the user preference based scalarization into account while ensuring the solution policy performs reliably in context to the baseline policy across all objectives. 
On both synthetic and real-world tasks, we show that the proposed approach can improve the policy while ensuring the safety constraints are respected. 

Our setting can accommodate any general form of scalarizations (e.g. non-linear or convex) as well as objectives (such as fairness), making it applicable to a wide variety of real-world tasks. 
% % Limitation/future work
The only assumption we made is regarding the dataset being collected under a single known baseline policy. An exciting line of future work can be to relax this assumption and consider the scenario where the dataset comes from a variety of unknown policies with different qualities. 
We did not make any claims about the optimality of the solutions as often optimality and safety are contradicting objectives. It is not clear how (and if) one can make claims about optimality in the offline setting without bringing in additional unrealistic assumptions (\Cref{sec:related-work}). 
% As majority of real-world data is high-dimensional, another important research direction is to take non-linear function approximation also into account.
% This requires the need for uncertainty estimation techniques for deep RL models that can be used instead of relying on concentration bounds. 
The extension to infinite MDPs and the function-approximation setting is also left for future work. 
It is important to note that when it comes to practical application, it is not unusual for continuous domains to be discretized to enable better interpretability, especially when interactions with humans are necessary. If the Markovian property is valid in the discretized space, SPIBB-based guarantees will also hold true.

