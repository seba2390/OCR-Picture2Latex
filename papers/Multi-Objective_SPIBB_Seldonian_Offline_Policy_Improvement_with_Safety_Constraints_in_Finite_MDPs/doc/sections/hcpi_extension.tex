\subsection{Multi-Objective HCPI (MO-HCPI)}
\label{sec:hcpi-w-constraints}

We briefly recall how the HCPI methodology \citep{thomas2015highImprovement, thomas2015highEvaluation} can be applied directly for solving the objective in \Cref{eq:general-task-objective}.
For a target policy, $\pi_t$, we use $\IS_{k}(\D, \pi_t, \pib)$ to denote the estimated returns for the $k$th reward component ($r_{k}$) using any IS based off-policy estimator \citep{precup2000eligibility}.  A high-confidence lower bound on $\J{\pi_t}{k}{\mopt}$ can be defined as:  
\begin{equation}
    \label{eq:hcope-R-lower-bound}
    \pr \Big( \J{\pi_t}{k}{\mopt} \ge \IS_{k}(\D, \pi_t, \pib) - \CI_{k}(\D, \delta/d) \Big) \ge 1 - \delta/d, 
\end{equation}
where $\CI_{k}(\D, \delta) \geq 0$ denotes the terms associated with the choice of concentration inequality employed (and typically $\lim_{|\D| \rightarrow \infty}\CI_{k}(\D, \delta) = 0$). 
% It can be either an exact inequality like Empirical Bernstein \citep{maurer2009empirical} or an approximate like Studentâ€™s t-test \citep{walpole1993probability}. 


The dataset $\D$ is first split into train ($\D_{tr}$) and test ($\D_{s}$) sets by the user. 
Let $\IS_{\bml}$ denote the IS estimator associated with the user-specified reward scalarization $\bml$. Given the user specified parameters: $\bml, \delta, \CI, \IS, \D_{tr}, \D_{s}$ and $\pib$, the policy improvement problem in \Cref{eq:general-task-objective} is transformed to the following optimization problem:
\begin{align*}
    \label{eq:h-opt}
    \pi_\textsc{h-opt} &= \argmax_{\pi \in \Pi} \IS_{\bml}(\D_{tr}, \pi, \pib) \tag{$\texttt{H-OPT}$}\\
    \text{s.t.} \quad
    &\forall k\in [d], \; \IS_{k}(\D_{s}, \pi, \pib) - \CI_k(\D_s, \delta/d) \ge
    \mu_k, 
\end{align*}
where $\mu_k$ denote the empirical returns for $r_k$ under $\pib$. 
%
% For small problems, \ref{eq:h-opt} can be solved with methods like CMA-ES \citep{hansen2006cma}. 
% In practice, a policy is first derived using the traditional offline RL methods based on $D_{tr}$ and is regularized using $\pib$ to obtain a set of candidate policies. 
% The best performing candidate policy that satisfies the safety-test (the performance constraints based on $\D_s$) is then returned.
% % If none of the candidate policies satisfy the safety-test, the baseline policy is returned.
The policy $\pi$ returned by \ref{eq:h-opt} will only violate the safety guarantees with probability at most $\delta$.
Proof of this claim and additional details are provided in \Cref{app:hcpi-details}.

Although we only focus on finite MDPs in this work, the HCPI based approach relies on IS estimates and therefore it can also be used for infinite MDPs or POMDPs. Unfortunately, the IS estimates are typically known to suffer from high variance \citep{guo2017using}. 
Furthermore, the optimization problem in \ref{eq:h-opt} is more challenging, and we need to resort to regularization based heuristics.
