% -------------------------------------------------------
%               Tabular experiments
% -------------------------------------------------------
\section{Synthetic Experiments}
\label{sec:synthetic-experiments}

The main benefits of working in a synthetic domain are: (i) we can evaluate the performance on the true MDP instead of relying on off-policy evaluation (OPE) methods, (ii) we have control over the quality of the dataset. We test both 
MO-SPIBB (\ref{eq:s-opt}) and MO-HCPI (\ref{eq:h-opt}) on a variety of parameters: the amount of data, quality of baseline and different user reward scalarizations. 

% ------------ Env description ---------------
\textbf{Env details:} 
We take a standard CMDP benchmark \citep{leike2017ai, chow2018lyapunov} which consists of a $10\times10$ grid. From any state, the agent can move to the adjoining cells in the 4 directions using the 4 actions. 
The transitions are stochastic, with some probability $\alpha$ (generated randomly for each state-action for every environment instance) the agent is successfully able to reach the next state, and with $(1-\alpha)$ the agent stays in the current state. 
The agent starts at the bottom-right corner, and the goal is to reach the opposite corner (top-left). The pits are spawned randomly with some uniform probability ($\eta_{pit}=0.3$) for each cell.  
The reward vector consists of two rewards signals. A primary reward $r_0$ that is related the goal and is +1000.0 on reaching the goal and -1.0 at every other time-step. The secondary reward $r_1$ is related to pits, for which the agent gets -1.0 for any action taken in the pit.
The constraint threshold for this CMDP is $-2.0$ and $\gamma = 0.99$. Maximum length of an episode is $200$ steps. Therefore, the task objective is to reach the goal in the least number of steps, such that the agent does not spend more than $2$ time-steps in the pit cells. 

% ------------ Dataset generation ---------------
\textbf{Dataset collection procedure:} 
For every random CMDP generated, we first find the optimal policy $\piopt$ by using the procedure described in \Cref{app:cmdp-solver}. The baseline policy is generated using a convex combination of the optimal policy and a uniform random policy ($\pi_{rand}$), i.e., $\pib = \rho \piopt + (1 - \rho) \pi_{rand}$, where  $\rho$ controls how close $\pib$'s performance is to $\piopt$. Different datasets with varying sizes and $\rho$ are then collected under $\pib$ and given as input to the methods.

% ------------ Baselines ---------------
\textbf{Baselines:} 
We compare against the following baselines:
\begin{itemize}[leftmargin=*, topsep=0pt]
    \item \myuline{Linearized}: This baseline transforms the rewards into a single scalar using $\bml$ and then applies the traditional policy improvement methods on the linearized objective, i.e,  $\argmax_{\pi \in \Pi} \J{\pi}{\bml}{\mhat}$.
    
    % \item \myuline{Adv-Linearized}: This method has the same objective as the Linearized baseline, with the additional constraints based on advantage estimators built from $\mhat$:
    % \begin{align}
    %     \argmax_{\pi \in \Pi} &\langle \pi(\cdot|x) \qval{\pi}{\bml}{\mhat}{x,\cdot} \rangle \quad  \forall x \in \X \\  
    %     \text{s.t.} \quad    
    %     &\forall i\in [d], \; \sum_{a \in \A} \pi(a|x) \adv{\pib}{i}{\mhat}{x, a} \geq 0 \nonumber. 
    % \end{align}
    \item \myuline{Adv-Linearized}: This method has the same objective as the Linearized baseline, with the additional constraints based on advantage estimators built from $\mhat$, i.e. $\forall x \in \X$:
    % \begin{align}
    %     \argmax_{\pi \in \Pi} \langle \pi(\cdot|x) , \qval{\pi}{\bml}{\mhat}{x,\cdot} \rangle \quad   
    %     &\text{s.t.} \quad    
    %     \forall k\in [d], \; \sum_{a \in \A} \pi(a|x) \adv{\pib}{k}{\mhat}{x, a} \geq 0 .
    % \end{align}
    \begin{align}
        \argmax_{\pi \in \Pi} &\langle \pi(\cdot|x) , \qval{\pi}{\bml}{\mhat}{x,\cdot} \rangle \\   
        \text{s.t.} \quad    
        &\forall k\in [d], \; \sum_{a \in \A} \pi(a|x) \adv{\pib}{k}{\mhat}{x, a} \geq 0 . \nonumber
    \end{align}
    
    
    
\end{itemize}


% ------------ Evaluation ---------------
\textbf{Evaluation:} 
Using $\mopt$, we can directly calculate the returns for any solution policy. Only tracking the scalarized objective can be misleading, so we track the following metrics:
\begin{itemize}[leftmargin=*, topsep=0pt,]
    \item \myuline{Improvement over $\pib$}: This denotes the difference between the scalarized return of the solution policy and the baseline policy, i.e., $\J{\pi}{\bml}{\mopt} - \J{\pib}{\bml}{\mopt}$.
    Mean improvement over $\pib$ captures on average improvement over $\pib$ in terms of the scalarized objective.
    
    \item \myuline{Failure-rate:} 
    The failure rate over $n$ runs captures the number of times, on average, the solution policy ends up violating the safety constraints in \Cref{eq:general-safety-constraints}, and thus performs worse than the baseline. In the context of this task, safety constraints are violated if either the agent takes longer to reach the goal, or it steps into more number of pits compared to $\pib$.
\end{itemize}

We test on different combinations of user preference $(\bml)$ and baseline's quality $(\rho)$ on 100 randomly generated CMDPs, where $\lambda_i \in \{0, 1\}$, $\rho \in \{0.1, 0.4, 0.7, 0.9\}$ and %the number of trajectories 
$|D| \in \{ 10, 50, 500, 2000\}$.
We evaluate under two settings: 
(i) we use a fixed set of parameters across different $(\bml, \rho)$ combinations, where we run \ref{eq:s-opt} with $\epsilon \in \{0.01, 0.1, 1.0\}$ and \ref{eq:h-opt} with Doubly Robust IS estimator \citep{jiang2015doubly} and Studentâ€™s t-test concentration inequality; 
(ii) we treat them as hyper-parameters that can be optimized for a particular $(\bml,\rho)$ combination. The best hyper-parameters are tuned in a single environment instance and then they are used to benchmark the results on 100 random CMDPs. 
% More details about the range of hyper-parameters considered are given in \Cref{app:cmdp-best-param-results}.


% ------------ Combined figure ---------------

% \begin{figure}[t]
% \makebox[1\linewidth][c]{%
% \centering
% % \begin{subfigure}[b]{0.48\columnwidth}
% \begin{subfigure}[b]{0.5\textwidth}
%     \includegraphics[width=1\textwidth]{doc/figures/random-mdps/delta_01_latex_nf.pdf}
%     \caption{Fixed hyper-parameters}
%     \label{fig:delta-params-mean} 
% \end{subfigure}
% \hfill
% \begin{subfigure}[b]{0.5\textwidth}
%     \includegraphics[width=1\textwidth]{doc/figures/random-mdps/bench_latex_nf.pdf}
%     \caption{Optimized hyper-parameters.}
%     \label{fig:best-params-mean}
% \end{subfigure}
% }
% \caption[]{
% \small
% Results on 100 random CMDPs for different $\bml$ and $\rho$ combinations with $\delta=0.1$. The different agents are represented by different markers and colored lines. Each point on the plot denotes the mean (with standard error bars) for 12 different $\bml,\rho$ combinations for the 100 randomly generated CMDPs (1200 datapoints). 
% The x-axis denotes the amount of data the agents were trained on. 
% The y-axis for left subplot in each sub-figure represents the improvement over baseline and the right subplot denotes the failure rate. The dotted black line in the right subplots represents the high-confidence parameter $\delta=0.1$.
% \Cref{fig:delta-params-mean} denotes when the hyper-parameters are fixed $\epsilon=\{0.01, 0.1, 1.0\}$ and $\IS=$ Doubly Robust (DR) estimator with student's t-test concentration inequality. 
% \Cref{fig:best-params-mean} is the version with tuned hyper-parameters for each combination.
% \label{fig:cmdp-combined-results}}
% \vskip -0.1in
% \end{figure}

\begin{figure}[t]
\centering
% \begin{subfigure}[b]{0.48\columnwidth}
\begin{subfigure}[b]{0.7\textwidth}
    \includegraphics[width=1\textwidth]{doc/figures/random-mdps/delta_01_latex_nf.pdf}
    \caption{Fixed hyper-parameters}
    \label{fig:delta-params-mean} 
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.7\textwidth}
    \includegraphics[width=1\textwidth]{doc/figures/random-mdps/bench_latex_nf.pdf}
    \caption{Optimized hyper-parameters.}
    \label{fig:best-params-mean}
\end{subfigure}
\caption[]{
\small
Results on 100 random CMDPs for different $\bml$ and $\rho$ combinations with $\delta=0.1$. The different agents are represented by different markers and colored lines. Each point on the plot denotes the mean (with standard error bars) for 12 different $\bml,\rho$ combinations for the 100 randomly generated CMDPs (1200 datapoints). 
The x-axis denotes the amount of data the agents were trained on. 
The y-axis for left subplot in each sub-figure represents the improvement over baseline and the right subplot denotes the failure rate. The dotted black line in the right subplots represents the high-confidence parameter $\delta=0.1$.
\Cref{fig:delta-params-mean} denotes when the hyper-parameters are fixed $\epsilon=\{0.01, 0.1, 1.0\}$ and $\IS=$ Doubly Robust (DR) estimator with student's t-test concentration inequality. 
\Cref{fig:best-params-mean} is the version with tuned hyper-parameters for each combination.
\label{fig:cmdp-combined-results}}
\vskip -0.1in
\end{figure}


% ------------ Results ---------------
\textbf{Results:} 
The mean results with fixed parameters and $\delta=0.1$ can be found in \Cref{fig:delta-params-mean}. 
% Tell changes with data size for the condensed figure
The high failure rate of Linearized baseline, regardless of the size of the dataset, is expected as it optimizes the scalarized reward directly and is agnostic of the individual rewards. Adv-Linearized performs better, but in the low data-regime, we see a high failure rate that eventually decreases as the size of dataset increases. This is expected because with more data, more reliable advantage functions estimates are calculated that are representative of the underlying CMDP. 
Compared to the baselines, both \ref{eq:s-opt} and \ref{eq:h-opt} maintain a failure rate below the required confidence parameter $\delta$, regardless of the amount of data.
Also, as the size of dataset increases, we see an increase in improvement over $\pib$, that makes sense as the methods only deviate from baseline when they are sure of the performance guarantees. We expect \ref{eq:s-opt} to violate the constraints with increasing value of $\epsilon$, as it relaxes the constraint on the policy-class (\Cref{eq:spibb-policy-constraint}) and leads to a looser guarantee on performance. This again is reflected in our experiments where \ref{eq:s-opt} with $\epsilon=1.0$ has a higher failure-rate than $\epsilon=0.1$.
We observed similar trends for different $\delta$ values.
A more detailed plot corresponding to different $\bml$ and $\rho$ combinations as well as results for a riskier value of $\delta=0.9$ are given in \Cref{app:cmdp-fixed-param-results}.


The results with optimized hyper-parameters can be found in \Cref{fig:best-params-mean}.
We notice that when the $\epsilon$ parameter is tuned properly, \ref{eq:s-opt} has better performance in terms of improvement over $\pib$ for the same amount of samples when compared to \ref{eq:h-opt}, while still ensuring the failure rate is less than $\delta$. These observations are consistent with the results in the single-objective setting in the original SPIBB works~\citep{laroche2017safe, nadjahi2019safe}.
The general trends and observations from the fixed-parameter case are also valid here.  Additional details, including results for $\bml, \rho$ combinations, hyper-parameters considered and qualitative analysis can be found in \Cref{app:cmdp-best-param-results}. 

We also compare our methods against \cite{le2019batch} in \Cref{app:lag-baseline}. We show the advantage of our approach over \cite{le2019batch}, particularly in the low-data regime, where our methods can improve over the baseline policy while ensuring a low failure rate. 
% The method in \cite{le2019batch} exhibits similar trends to Adv-Linearized baseline, where in the low data setting it has high-failure rate, which decreases as the size of dataset decreases. 
This makes sense as the method in \cite{le2019batch} relies on the concentrability coefficient which can be arbitrarily high in the low data setting, and therefore their performance guarantees do not hold anymore.
We also provide experiments on the scalability of methods with the number of objectives $d$ in \Cref{app:cmdp-scaling-experiments}.
