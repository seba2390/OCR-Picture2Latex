\section{Methodology}
\label{sec:methodology}

\subsection{Setting}
\label{sec:setting}

We consider the setting where the agent's interactions with the environment can be modelled as a Markov Decision Process \citep[MDP,][]{bellman1957markovian}. 
Let $\X$ and $\A$ respectively be the (finite) state and action spaces. 
Let $\Pstar: \X \times \A \rightarrow \Dist(\X)$ denote the true (unknown) transition probability function, where $\Dist(\X)$ denotes the set of probability distributions on $\X$. 
Without loss of generality, we assume that the process deterministically begins in the state $x_0$.
We define $[N]$ to be the set $\{0,1,\dots,N-1\}$ for any positive integer $N$.
Let there be $d$ different reward signals and
$\Rstar=\left\{r_k\right\}_{k\in[d]}: \X \times \A \rightarrow [- \rmax, \rmax]^d$ be the true (unknown) stochastic multi-reward signal.\footnote{Costs, which are meant to be minimized, can be expressed as negative rewards.}
% \footnote{Undesired or negative reinforcement signals that the agent is interested in minimizing can be expressed as negative rewards.}
% \footnote{The reinforcement signals that the agent is interested in minimizing can be expressed as negative rewards.}
Finally, $\bmg=\left\{\gamma_k\right\}_{k\in[d]} \in [0,1)^d$ is the multi-discount-factor.


% describe the final MDP and then the policy and returns def
The MDP, $\mstar$, can now be defined with the tuple $(\X, \A, \Pstar, \Rstar, \bmg, x_0)$. A policy $\pi: \X \rightarrow \Dist(\A)$ maps a state to a distribution over actions. We denote by $\Pi$ the set of stochastic policies. We consider the infinite horizon discounted return setting.
For any $k\in[d]$, the $k$\textsuperscript{th} reward value function $\val{\pi}{k}{m}{x} : \X \rightarrow \Real$ denotes the expected discounted sum of rewards when when following policy $\pi$ in an MDP $m$ starting from state $x$.
Analogously, we define the state-action value functions for performing action $a$ in state $x$ in MDP $m$ under $\pi$ for rewards as $\qval{\pi}{k}{m}{x,a}$. Let $\adv{\pi}{k}{m}{x,a} = \qval{\pi}{k}{m}{x,a} - \val{\pi}{k}{m}{x}$ denote the corresponding advantage function.
The expected return of policy $\pi$ w.r.t. the $k$\textsuperscript{th} reward in the true MDP $\mstar$ is denoted by $\J{\pi}{k}{\mstar} = \val{\pi}{k}{\mstar}{x_0} = \E_{\pi, \mstar} [ \sum_{t=0}^{\infty} \gamma_k^t R_{k,t} \mid X_0=x_0]$, where action $A_t\sim \pi(\cdot\mid X_t)$, immediate reward $R_{k,t}\sim r^\star_k(\cdot\mid X_t,A_t)$, and state $X_{t+1}\sim \Pstar(\cdot\mid X_t,A_t)$. 



% batch setting
We consider the offline setting, where instead of having access to the environment we have a pre-collected dataset 
of trajectories denoted by $\D = \set{\tau_i}_{i\in[|\D|]}$, where $|\D|$ denotes the number of trajectories in the dataset. A trajectory $\tau$ of length $T$ is an ordered set of transition tuples of the form $\tau = \set{x_i, a_i, x'_i, \bm{r}_i}_{i\in[T]}$, where $x'_i$ denotes the state at the next time-step.
% of transitions denoted by $\D = \set{x_i, a_i, {x'}_i, \bm{r}_i}_{i\in[|\D|]}$, where $x'$ denotes the state at the next time-step and  $|\D|$ denotes the size of the dataset. 
We denote the Maximum Likelihood Estimation (MLE) of the MDP with $\mhat = (\X, \A, \Phat, \Rhat, \bmg, x_0)$, where $\Phat$ and $\Rhat$ denote the transition and reward models estimated from the dataset's statistics.


\begin{assumption}[Baseline policy]
We assume that we have access to the policy that generated the dataset. We call such policy the baseline policy and denote it by $\pib$.~\footnote{\citet{simao2020} proved that SPIBB/Soft-SPIBB bounds may be obtained with an estimate of $\pib$.
% the baseline policy.
}
\end{assumption}



\subsection{Problem formulation}
\label{sec:problem-formulation}

% Let $\Omega$ be the space of all offline RL algorithms, each of which is a function $\omega: \D \rightarrow \pi$. Let $f: \omega \rightarrow \Real$ denote some maximization objective for an offline RL algorithm. Let there be $m$ behavioral constraints, $g_i: \pi \rightarrow \Real$, that depend on the agent's policy learned with the dataset, and let $\delta_i \in [0,1]$ denote the corresponding constraint confidence level. The general Seldonian Optimization Problem (SOP) is then defined as:
% \begin{align}
%     \label{eq:seldonian-optimization-problem}
%     & \argmax_{\omega \in \Omega} f(\omega) \\
%     \text{s.t.} &\, \forall i \in [m], \mathbb{P}\left( g_i(\omega(\D)) \leq 0 \right ) \geq 1 - \delta_i. \nonumber
% \end{align}
% %
% We now describe in more detail the exact form of the Seldonian optimization problem we are interested in this work. 
We consider safe policy improvement with respect to the baseline according to the $d$ dimensions of the multi-objective setting.
Therefore, under a Bayesian approach, we search for target policies such that they perform better (up to a precision error $\zeta$) than the baseline along every objective function with high probability $1 - \delta$, where $\zeta$ and $\delta$ are hyper-parameters controlled by the user, denoting the risk that the practitioner is willing to take. We denote by $\Pi_\textsc{a}$ the set of admissible policies that satisfy:
\begin{align}
    \label{eq:general-safety-constraints}
    \mathbb{P}\left(\forall k\in[d], \J{\pi}{k}{\mstar}-\J{\pi_b}{k}{\mstar}>-\zeta \Big| \D \right)>1-\delta .
\end{align}

In the multi-objective case, there does not exist a single optimal value, but a Pareto frontier of optimal values. One way to evaluate the MORL problems is via the \textit{multiple-policy} approaches \citep{vamplew2011empirical, roijers2013survey} that compute the policies that approximate the true optimal Pareto-frontier. However, note that optimality and safety are contradicting objectives. It is not clear how (and if) one can make claims about optimality in the offline setting without bringing in additional unrealistic assumptions (\Cref{sec:related-work}, MORL). Instead, we take an alternate approach inspired by another category of MORL methods called \textit{single-policy} \citep{roijers2013survey, van2014multi} where the trade-offs between different objectives are explicitly controlled by the user via providing a scalarization or preferences over objectives. 
% We assume the user preference is given as an input to our algorithms, and is used for scalarization $\bml=\left\{\lambda_k\right\}_{k\in[d]} \in \Delta^d$ of the objectives, where $\Delta^d$ is the simplex of dimension $d$. 
We assume the user preference $\bml=\left\{\lambda_k\right\}_{k\in[d]}$ is given as an input to our algorithms, and is used for scalarization of the objectives, where $\lambda_k \in \Real^{+}$. 
Our objective therefore becomes
\begin{align}
\label{eq:general-task-objective}
    \argmax_{\pi \in \Pi_\textsc{a}} &\quad \sum_{k\in[d]} \lambda_k \J{\pi}{k}{\mstar} . 
\end{align}

The above formulation gives freedom to the user in terms of what particular quantity they want to optimize via $\bml$, but still ensures that the solution policy performs as well as the baseline policy across all $d$ objectives.
%
Note that our explicit goal is to maximize the objective specified by the user. However, the user might make mistakes in specifying this objective (\Cref{sec:related-work}, Reward design), and the above formulation offers guarantees that prevent deteriorating the performance of the policy across any of the $d$ objectives. 
This allows the user to to experiment with different reward design strategies in safety-critical settings without worrying about the risks of ill-defined scalarizations.
%
A na\"ive approach would be applying the user scalarization to also define the safety constraints. However, this construction fails to prevent undesirable behavior for the individual objectives (shown in \Cref{app:naive-construction}).
%For instance, one could imagine an iterative process with the user refining its scalarization in the function of the expected trade-offs returned by the algorithm.

% \htodo{Finally, one could imagine an iterative process with the user refining its scalarization in the function of the expected trade-offs returned by the algorithm. Similar approaches are also taken in multiple-policy MORL literature where a single-policy algorithm is called repeatedly with different scalarizations [2,4]. We argue that our work solves this important problem by allowing the user to experiment with different reward design strategies in safety-critical settings without worrying about the risks of ill-defined scalarizations.}



\subsection{Multi-Objective SPIBB (MO-SPIBB)}
\label{sec:spibb-w-constraints}

% \textbf{SPIBB background:}
Robust MDPs~\citep{Iyengar2005,Nilim2005} can be regarded as an approximation of the Bayesian formulation by partitioning the MDP space $\mathcal{M}$ into two subsets: the subset of plausible MDPs $\Xi$  and the subset of implausible MDPs. The plausible set is classically constructed from concentration bounds over the reward and transition function:
\begin{align*}
    \Xi = &\left\{m,
	\textnormal{ s.t. } \forall x,a, 
	\begin{array}{ll}
	\lVert p(\cdot|x,a)-\hat{p}(\cdot|x,a)\rVert_1 \leq e(x,a),\\
	\lVert \bm{r}(x,a)-\hat{\bm{r}}(x,a)\rVert_\infty \leq e(x,a)r_{\mytop} \end{array}\right\},
\end{align*}
where $e$ is an upper bound on the state-action error function of the model that are classically obtained with concentration bounds, such that the true environment $m^{\star}\in\Xi$ with high probability $1-\delta$.
%
In the single objective framework, \citet{laroche2017safe} empirically show that optimising the worst-case performance policy in $\Xi$ provides policies that are too conservative. \citet{petrik2016safe} prove that it is NP-hard to find the policy $\pi$ that maximises the worst-case policy improvement over $\Xi$. 

Instead, the SPIBB methodology \citep{laroche2017safe} consists in searching for a policy that maximizes the safe policy improvement in the MLE MDP, under some policy constraints:
SPIBB and Soft-SPIBB \citep{nadjahi2019safe} policy search constraints both revolve around the idea that we must only consider policies for which the policy improvement may be accurately estimated.   
Using $\pi_b$ as reference, SPIBB allows policy changes only in state-action pairs for which more than $n_\wedge$ samples have been collected. Soft-SPIBB extends this by applying soft constraints that allow slight changes in the policy for the uncertain state-action pairs, which are controlled by an error bound related to model uncertainty. As such, on low-confidence transitions, this class of methods provides a mechanism that prevents the agent from deviating too much from $\pib$. 
%
In this work, we build on Soft-SPIBB because it has yielded better empirical results.
% in every task considered. 
Formally, its constraint on the policy class is defined by:
\begin{equation*}
\label{eq:spibb-policy-constraint}
    \Pi_\textsc{s} = \left\{\pi, \text{ s.t. } \forall x, \sum_a e(x,a)\ |\pi(a|x) - \pib(a|x)| \leq \epsilon \right\},
\end{equation*}
where $\epsilon$ is a hyper-parameter that controls the deviation from the baseline policy.

We define $\qval{\pi}{\bml}{m}{x,a}=\sum_{k\in[d]} \lambda_k \qval{\pi}{k}{m}{x,a}$ to be the state-action value function associated with the linearized $\bml$ parameters. The same notation extension is used for $\val{\pi}{\bml}{m}{x,a}$ and $\J{\pi}{\bml}{m}$. The application of Soft-SPIBB to multi-objective safe policy improvement is therefore direct:
\begin{align}
\label{eq:mo-spibb-objective}
    \argmax_{\pi \in \Pi_\textsc{a}\cap\Pi_\textsc{s}} \J{\pi}{\bml}{\mhat} , 
\end{align}
which is always realizable since $\pib\in\Pi_\textsc{a}\cap\Pi_\textsc{s}$.
% \footnote{Defining the plausible set using the MLE MDP $\mhat$ also allows us to specify the task objective and constraints in \Cref{sec:problem-formulation} using $\mhat$.}
% For future, an alternate way of writing can be the following:
% It needs to be a 3-step process
% Define $\Pi_{\hat{A}}$ for advantage in the $\mhat$
% Rewrite the above equation in terms of \Pi_{\hat{A}}
% Show that Prop 3.1 implies that \pi \in \Pi_A as a result of that.

% First, we need to show that the construction of the plausible set required for the application of SPIBB is technically sound. We do so by deriving the concentration bounds for the multi-objective case.
We show that the construction of the plausible set required for the application of SPIBB is technically sound by deriving the concentration bounds for the multi-objective case. 
In Appendix \ref{app:error_bounds}, we show with Hoeffding's inequality that $e$ grows as the square root of the logarithm of $d$ (the number of reward functions), \textit{i.e.} almost imperceptibly. From there, all the SPIBB theoretical results from \citet{laroche2017safe,nadjahi2019safe,simao2020} may be generalized at a negligible SPI guarantee cost to the multi-objective setting, by applying their theorems separately to every objective function.
% Therefore, we directly get the performance guarantees for each objective function that satisfies 


Now, the problem in \Cref{eq:mo-spibb-objective} can be transformed into a policy improvement procedure that solves for every state $x \in \X$ the following optimization problem\footnote{In practice, we also need to check $\forall x$ that $\pi(\cdot\mid x)$ is a valid probability distribution: positive and sums to 1.}:
\begin{align*}
    \label{eq:s-opt}
    \pi_\textsc{s-opt} &= \argmax_{\pi \in \Pi} \langle \pi(\cdot|x) , \qval{\pi}{\bml}{\mhat}{x,\cdot} \rangle \quad  \tag{$\texttt{S-OPT}$} \\  
    \text{s.t.} \quad    
    &\sum_{a \in \A} e(x,a)\ |\pi(a|x) - \pib(a|x)| \leq \epsilon, \tag{$\pi\in\Pi_\textsc{s}$} \\ 
    &\forall k\in [d], \sum_{a \in \A} \pi(a|x) \adv{\pib}{k}{\mhat}{x, a} \geq 0. \tag{$\pi\in\Pi_\textsc{a}$}
\end{align*}

% \htodo{$\Pi_\textsc{a}$ overloading: We can use $\Pi_{\textsc{a},\mstar}$ and $\Pi_{\textsc{a},\mhat}$ instead of using a single $\Pi_{\textsc{a}}$.}

% The multi-objective setting additionally 
The above procedure requires us to make additional algorithmic modifications that are not present in the original SPIBB algorithms. 
In particular, we need to explicitly incorporate advantage constraints for safety-guarantees for the individual objectives (proof given in \Cref{app:spibb-need-of-advatangeous}). The classic single-objective SPIBB algorithms do
not need to check the advantage conditions because it is automatically guaranteed by the $\argmax$ and the fact that $\pib\in\Pi_\textsc{s}$.

Using the construction above, we directly get the following result on the performance guarantees for each objective function that satisfies the desired property in \Cref{eq:general-safety-constraints}: 

\begin{prop}
The policy $\pi$ returned from solving the \ref{eq:s-opt} satisfies the following property in every state $x \in \X$ with probability at least $(1 - \delta)$:
\begin{align}
    \forall k \in [d], \, \val{\pi}{k}{\mopt}{x} - \val{\pib}{k}{\mopt}{x} \geq -\frac{\epsilon v_{\text{max}}}{1-\gamma},
\end{align}
where $v_{\text{max}} \le \frac{\rmax}{1-\gamma}$ is the maximum of the value function.
\end{prop}
The proof is presented in \Cref{app:mo-spibb-prop}.
%
The solution of \ref{eq:s-opt} is computed by solving the Linear Program using standard solvers, such as 
% SciPy's \texttt{linprog} \citep{SciPy-NMeth} or 
\texttt{cvxpy} \citep{diamond2016cvxpy}. 
There is an increase in the computational cost proportional to the number of reward functions. Compared to Soft-SPIBB, the value and advantage functions estimation cost increases by a factor of $d$: respectively $\mathcal{O}(d|\X|^3)$ and $\mathcal{O}(d|\A||\X|^2)$.
% , and the number of constraints in the linear program also increases by $d$. 
There is $\mathcal{O}(|\mathcal{D}|)$ cost for estimating the error bounds, and we also require solving a Linear Program for each state that approximately amounts to an additional $\mathcal{O}(|\mathcal{X}||\mathcal{A}|^2(|\mathcal{A}|+d))$ steps to the total computational cost \citep{boyd2004convex}. 


\begin{remark}[Extension to Constrained-RL]
The above methodology can also be extended to the Constrained-RL setting for offline policy improvement in general CMDPs. Recall that SPIBB algorithms offer guarantees in the form of: $v_t-v_b \geq \hat{v}_t - \hat{v}_b - \xi$, where $v_t$ and $v_b$ are respectively the true values of the target and baseline policies, $\hat{v}_t$ and $\hat{v}_b$ are their estimates in the MLE MDP, and $\xi$ is an error term due to parametric uncertainty. As a consequence, any constraint $c$ such that $c\leq v_b + \hat{v}_t - \hat{v}_b - \xi$ may be guaranteed ($v_b-\hat{v}_b$ may easily be bounded with Hoeffding's inequality), and when $c$ is larger, we can return no solution found as with other Seldonian algorithms.
\end{remark}


% ------------------------------
% ---------- MO-HCPI -----------
% ------------------------------

\input{doc/sections/hcpi_extension}
