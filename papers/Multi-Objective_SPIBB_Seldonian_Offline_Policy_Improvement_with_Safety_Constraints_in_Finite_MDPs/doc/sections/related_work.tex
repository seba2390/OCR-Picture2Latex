\section{Related work}
\label{sec:related-work}



% -------- MORL -------
\textbf{Multi-Objective RL (MORL):}
Traditional multi-objective approaches \citep{mannor2004geometric, roijers2013survey, liu2014multiobjective} focus on finding the Pareto-frontier of optimal reward functions that gives all the possible trade-offs between different objectives. The user can then select a policy from the solution set according to their arbitrary preferences. In practice, an alternate trial and error based approach of scalarization is used to transform the multiple reward functions into a scalar reward based on preferences across objectives (usually, by taking a linear combination). 
Most traditional MORL approaches have focused on the online, interactive settings where the agent has access to the environment. While some recent approaches are based on off-policy learning methods \citep{lizotte2012linear, van2014multi,yang2019generalized,abdolmaleki2020distributional}, they lack guarantees. In contrast, our work focuses exclusively on learning in the offline setting and gives high-probability guarantees on the performance in the environment.

% -------- CMDPs -------
\textbf{Constrained-RL:} 
RL under constraints frameworks, such as Constrained MDPs~\citep[CMDPs,][]{altman1999constrained}, present an alternative way to define preferences in the form of constraints over policy's returns. 
Here, the user assigns a single reward function to be the primary objective (to maximize) and hard constraints are specified for the others.
The major limitation of this setting is that it assumes the thresholds for the constraints are known a priori. 
% While this is a valid assumption for certain settings, often in practice, the thresholds are hyper-parameters that determine the desired returns and are hard to specify beforehand, e.g., for a traffic control system maximizing throughput and minimizing latency, it is often unclear how should the threshold for latency be set~\citep{roijers2013survey}.
%
\citet{le2019batch} study offline policy learning under constraints
and provide performance guarantees w.r.t. the optimal policy, but their work relies on the concentrability assumption \citep{munos2003error}.
%, an assumption that cannot be verified in practice.
% This is a strong assumption that depends on the distribution of the dataset and the space of policies and their induced state-action distributions. Thus, their performance bounds can be arbitrarily large when the dataset does not satisfy this assumption (which cannot be verified in practice). 
% In this work, we instead focus on the performance guarantees based on returns observed in the dataset, as it does not require making any of the above assumptions.


Concentrability is a strong assumption that upper bounds the ratio between the future state-action distributions of any non-stationary policy and the baseline policy under which the dataset was generated by some constant. From a practical perspective, it is unclear how to get a tractable estimate of this constant, as the space of future state-action distributions of non-stationary policies is vast. Thus, this constant can be arbitrarily huge, potentially even infinite when the baseline policy fails to cover the support of the space of all non-stationary policies (such as in the low-data regime), leading to the performance bounds given by these methods to blow up (and even be unbounded). 
Additionally, the guarantees in \cite{le2019batch} are only valid with respect to the performance of the optimal policy.
In this work, we instead focus on the performance guarantees based on returns observed in the dataset, as it does not require making any of the above assumptions.
% Additionally, these guarantees are only valid with respect to the performance of the optimal policy. From a safety-critical viewpoint, this goes against our motivation as we want practical guarantees on the performance of the policies that we want to eventually deploy. 


% -------- Reward design -------
\textbf{Reward design:} 
Reward-design \citep{sorg2010internal} and reward-modelling approaches \citep{christiano2017deep, littman2017environment, leike2018scalable} focus on designing suitable reward functions that are consistent with the user's intentions. These approaches rely heavily on the human or simulator feedback, and thus do not carry over easily to the offline setting.



% -------- SPI -------
\textbf{Seldonian-RL (and Safe Policy Improvement):} 
The Seldonian framework \citep{Thomas2019} is a general algorithm design framework that allows the user to design ML algorithms that can avoid undesirable behavior with high-probability guarantees. 
In the context of RL, the Seldonian framework allows to design policy optimization problems with multiple constraints,
where the solution policies satisfy the constraints with high-probability. 
In the offline-RL setting, SPI refers to the objective of guaranteeing a performance improvement over the baseline with high-probability guarantees~\citep{thomas2015highImprovement, petrik2016safe, laroche2017safe}. Therefore, SPI algorithms are a specific setting that falls in the general category of Seldonian-RL algorithms. 

We focus on two categories of SPI algorithms that provide practical error bounds on safety: SPIBB \citep{laroche2017safe} that provides Bayesian bounds and HCPI \citep{thomas2015highImprovement, thomas2015highEvaluation} that provides frequentist bounds.
SPIBB methods constrain the change in the policy according to the local model uncertainty. 
SPIBB has been formulated in the context of a single reward function, and as such does not handle multiple rewards and by extension also lacks the ability for the user to specify preferences. 
Our primary focus is to provide a construction for extending the SPIBB methodology to the multi-objective setting that handles user preferences and provides high-probability guarantees.

Instead of relying on model uncertainty, HCPI methods utilize the high-confidence lower bounds on the Importance Sampling (IS) estimates of a target policy's performance to ensure safety guarantees. 
HCPI has been applied to solve Seldonian optimization problems for constrained-RL setting using an enumerable policy class. \citet{Thomas2019} suggested using HCPI for the MORL setting, and we build on that idea. Particularly, we show how HCPI can be implemented with stochastic policies in the context of our setting with user preferences and baseline constraints.

