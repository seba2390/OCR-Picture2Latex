% ------------------------------------------------------------
%               SPIBB with constraints
% ------------------------------------------------------------
\subsection{SPIBB based extension}
\label{sec:spibb-w-constraints}



We will now follow the SPIBB methodology and present how to construct a set of robust MDPs from the data using the concentration inequalities and then solve them respect to the safety constraints on the performance for \Cref{eq:general-task-objective}. 
The difference between the estimated parameters from the dataset and the true can be bounded using concentration inequality based error bounds based on state-action counts \citep{ghavamzadeh2016safe}. The exact formulation The set of admissible MDPs can be defined based on the high-probability confidence parameter $\delta$ as:
\begin{align*}
    \Xi(\hat{M}, e) &\coloneqq \left\{
                M = \langle \X, \A, \gamma, P, R, C \rangle \text{s.t.} \forall (x,a) \in \X \times \A, \right.\\ 
                &\quad \left.
                \begin{array}{lll}
                  \norm{P(\cdot|x,a) - \hat{P}(\cdot|x,a)}_{1} &\leq e(x,a), \\
                  |R(x,a) - \hat{R}(x,a)| &\leq e(x,a) \rmax , \\
                |C(x,a) - \hat{C}(x,a)| &\leq e(x,a) \cmax, 
                \end{array}
                 \right\}
\end{align*}
where $e(x,a): \X \times \A \rightarrow \Real$ is an error function depending on $\D$ and $\delta$.  More details about these error bounds can be found in \Cref{app:err-bounds}. The new policy improvement objective w.r.t. performance in estimated MDP $\hat{M}$ and guaranteeing it be $\zeta$-approximately ($\zeta$ being a precision hyper-parameter) at least as good as $\pi_b$ in the admissible set for \Cref{eq:general-task-objective} can be written as :
% \begin{align}
%     \max_{\pi} &\quad \J{\pi}{R}{\mhat} \label{eq:reward-only-batch-spibb}\\ 
%     \texttt{s.t.} &\quad \J{\pi}{R}{M} \geq \J{\pib}{R}{M} - \zeta, \forall M \in \Xi,
%     \nonumber \\
%     &\quad \J{\pi}{\ci}{M} \leq \J{\pib}{\ci}{M} + \zeta, \forall i, \nonumber, \forall M \in \Xi, \nonumber
% \end{align}
% In the above objective there are 
% additional constraints w.r.t. to the performance of baseline policy for the respective constraints $\ci$.
% Instead we formulate the following multi-objective approach:
\begin{align}
\label{eq:spibb-robust-formulation}
    \argmax_{\pi \in \Pi} &\quad \lR \J{\pi}{R}{\mhat}  - \lC \J{\pi}{\ci}{\mhat}  \\ 
    \texttt{s.t.} &\quad \J{\pi}{R}{M} \geq \J{\pib}{R}{M} - \xi, \forall M \in \Xi, \nonumber \\
    &\quad \J{\pi}{C}{M} \leq \J{\pib}{C}{M} + \zeta,   \forall M \in \Xi, \nonumber,
\end{align}
where $\lR ,\lC \in \Real_{\ge 0}$ denote user specified reward-linearization parameters. 


% Mention to convert it into a tractable optimization problem we
\citet{nadjahi2019safe} propose a computationally tractable formulation for working with admissible MDPs by introducing a constraint that allows slight policy changes for uncertain state-action pairs while remaining safe. They do so by restricting the class of policies w.r.t. model uncertainty captured by $e$ the error function associated with the state-action value function and a local error budget. Formally, this constraint on policy class is defined by:
% to $\EpsPib$-constrained policies defined by: 
\begin{equation}
\label{eq:spibb-policy-constraint}
    \sum_a e(x,a)\ |\pi(a|x) - \pib(a|x)| \leq \epsilon,  \forall x \in \X,
\end{equation}
where $\epsilon$ is a hyper-parameter that controls the deviation from the baseline policy.
% This constraint also called $\EpsPib$-constraint.
We define $Q_{\lambda}$ to be the state-action value function associated with the linearized $\lR$ and $\lC$ parameters, $\qval{\pi}{\lambda}{M}{x,a} \coloneqq \E_{\pi, M} [ \sum_{t=0}^{\infty} \gamma^t ( \lR R(x_t,a_t) - \lC C(x_t,a_t)) \mid  x_0 = x, a_0 = a ]$.

Now, the problem in \Cref{eq:spibb-robust-formulation} can be transformed into a policy improvement procedure that solves for every state $x \in \X$ the following optimization problem:
\begin{align*}
    \label{eq:s-opt}
    \pi_{new} &= \argmax_{\pi \in \Pi} \langle \pi(\cdot|x) \qval{\pib}{\lambda}{\mhat}{x,\cdot} \rangle \quad  \tag{$\texttt{S-OPT}$} \\  
    \text{s.t.} \quad    
    &\sum_{a \in \A} e(x,a)\ |\pi(a|x) - \pib(a|x)| \leq \epsilon, \tag{constraint on policy class} \\ 
    % \adv{\pi}{R}{M}{x,a}
    &\sum_{a \in \A} \pi(a|x) \adv{\pib}{R}{\mhat}{x, a} \geq 0, \tag{advantageous over $\pib$ w.r.t. $R$} \\
    &\sum_{a \in \A} \pi(a|x) \adv{\pib}{C}{\mhat}{x, a} \leq 0, \tag{advantageous over $\pib$ w.r.t. $C$} \\
    &\sum_{a \in \A} \pi(a|x) = 1. \tag{valid probability distribution}
    % &\sum_{a \in \A} \pi(a|x) \Adv{\pib}{R}{\mhat}(x, a) \geq 0, \forall x \in \X \tag{advantageous over $\pib$ w.r.t. $R$} \\
\end{align*}

Intuitively, the constraints defined with the advantage functions w.r.t. $R$ and $C$ ensure that the new policy performs better than the baseline policy's performance based on the estimated MDP $\mhat$. The constraint on the policy class \Cref{eq:spibb-policy-constraint} is required to ensure high confidence guarantees on the performance of the new policy on the true MDP $\mopt$. The last constraint ensures that the new policy is a valid probability distribution.
\harsh{Right now I have both the tags next to constraints as well as intuitive explanation, but maybe they are redundant and one of them can be removed.}
There are two major differences of the above formulation compared to the original SPIBB formulation: (i) There are new constraints related to the additional costs signals $C$, (ii) Now we need to explicitly include the advantageous constraints w.r.t $R$ in the optimization also. Proof of why we need to include reward constraint in the construction is given in \Cref{prop:spibb-r-advatangeous} in the Appendix.

% \harsh{An alternate formulation can also be defined based on constraints on the CMDP thresholds $d_i$ instead:
% \begin{align}
%     \max_{\pi} &\quad \J{\pi}{R}{\mhat} \label{eq:spibb-d0-obj} \\ 
%     \texttt{s.t.} &\quad \J{\pi}{R}{M} \geq \J{\pib}{R}{M} - \zeta, \forall M \in \Xi, \nonumber\\
%     &\quad \J{\pi}{\ci}{M} \leq d_i, \forall i. \nonumber
% \end{align}
% The difference here is the constraints w.r.t. $\ci$ are independent of the behavior of the baseline policies. This direction is left unexplored for now, we will probably revist this later.}



% ------- Guarantees go here 


We now present a result that shows the formulation in \Cref{eq:s-opt} has desirable properties in terms of ensuring the safety guarantees that we originally specified in the \Cref{eq:general-task-objective}. More formally,
\begin{prop}[Safety guarantees with SPIBB]
\label{thm:eQ-guarantee}
    The policy $\pi$ returned by the policy iteration step given by \ref{eq:s-opt} satisfies the following inequalities in every state $x$ with probability at least $(1 - \delta)$:
    \begin{align*}
        \val{\pi}{R}{\mopt}{x} - \val{\pib}{R}{\mopt}{x} &\geq - \frac{\epsilon V_{R}^{\MAX}}{(1 - \gamma)} \\ 
        \val{\pi}{C}{\mopt}{x} -  \val{\pib}{C}{\mopt}{x} &\leq \frac{\epsilon V_{C}^{\MAX}}{(1 - \gamma)},
    \end{align*}
\end{prop}
where $V_{R}^{\MAX} \leq \frac{\rmax}{(1-\gamma)}$ and $V_{C}^{\MAX} \leq \frac{\cmax}{(1-\gamma)}$ are the maximum of value functions w.r.t. $R$ and $C$ respectively. The proof of the above theorem can be found in \Cref{app:proof-thm1}.

It is possible to constrain the policy class using an error bound over the transition function and still provide safety guarantees w.r.t. both $R$ and $C$ for multiple-steps of policy improvement. However, the guarantees associated with using error bound over transition functions comes at the cost of introducing an additional assumption. We mention the additional assumption required as well as the bounds on the performance in \Cref{app:eP-guarantees}.
\harsh{We can move this from appendix here if there is enough space, but for now keeping it in Appendix only.}

% --------- Algorithmic Implementation details 


The solution of the ~\ref{eq:s-opt} by solving the entire Linear Program using a standard LP solvers, such as SciPy's \texttt{linprog} \citep{SciPy-NMeth} or \texttt{cvxpy} \citep{diamond2016cvxpy}. The non-linear constraint on the policy class in \cref{eq:s-opt} is handled by introducing auxiliary variables to bound the sum and replace by corresponding linear constraints.
\harsh{This detail can be moved Appendix also}