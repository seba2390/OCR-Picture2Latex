Consider designing a vision system that solves many tasks. Ideally, any such system should be able to reuse representations for different applications. As the system is trained to solve more problems, its core representations should become more complete and accurate, facilitating the learning of additional tasks. Vision research often focuses on designing good representations for a given task, but what are good core representations to facilitate learning the next?

The application of knowledge learned while solving one task to solve another task
%The phenomenon of reusing knowledge learned while solving one task and applying it to another task
is known as \textit{transfer learning} or \textit{inductive transfer}. Inductive transfer has been demonstrated in recent vision-language tasks in~\cite{li2016learning,lu2016hierarchical,jabri2016revisiting,fukui2016multimodal,shih2016look,ilievski2016focused}, where the hidden or output layers of deep networks learned from pre-training (e.g. on ImageNet~\cite{deng2009imagenet}) or multitask learning serve as the foundation for learning new tasks. However, the relations of features to each new task needs to be re-learned using the new task's data. The goal of our work is to transfer knowledge between related tasks \emph{without} the need to re-learn this mapping. Further, as we are working with vision-language tasks, we aim to transfer knowledge of both vision \emph{and} language across tasks.

\input{teaser_figure_2.tex}

% give high level approach now, talk about parsing details mapping question to noun and adj in the methods section
In this work we propose a Shared Vision-Language Representation (SVLR) module that improves inductive transfer between related vision-language tasks (see Fig.~\ref{fig:short_overview}).  We apply our approach to visual recognition (VR) and attention-based visual question answering (VQA). We formulate VR in terms of
a joint embedding of textual and visual representations computed by the SVLR module. Each region is mapped closest to its correct (textual) class label.  For example, the embedding of ``dog'' should be closer to an embedded region showing a dog than any other object label.  We formulate VQA as predicting an answer from a relevant region, where relevance and answer scores are computed from embedded word-region similarities.  For example, a region will be considered relevant to ``Is the elephant wearing a pink blanket?'' if the embedded ``pink'' \textit{and} either ``elephant'' or ``blanket'' are close to the embedded region. Similarly, the answer score considers embedded similarities, but in a more comprehensive manner. We emphasize that the same word-region embedding is learned for both VR and VQA.  Our experiments show that formulating both tasks in terms of the SVLR module leads to better cross-task transfer than if features are shared through multitask learning but without exploiting the alignment between words and regions. 

In summary, \textbf{our main contribution} is to show that the proposed SVLR module leads to better inductive transfer than unaligned feature sharing through multitask learning. As an added benefit, \textit{attention} in our VQA model is highly interpretable: we can show what words cause the system to score a particular region as relevant. We take a small step towards lifelong-learning vision systems by showing the benefit of an interpretable, flexible, and trainable core representation.  
%Many other tasks can also be formulated in terms of word-region embeddings, so that as each of these tasks are learned, the core representation can continue to improve.

%In the following, we relate how we build on a rich if recent history of lifelong learning, vision-language embeddings, and visual question answering (Sec.~\ref{sec:related}). We then detail our word-region embedding (Sec.~\ref{sec:embedding}), and how the same embedding is made an integral part of object and attribute recognition (Sec.~\ref{sec:vr}) and visual question answering (Sec.~\ref{sec:vqa}). Our experiments (Sec.~\ref{sec:experiments}) compare how much jointly training with VR helps VQA when using SVLR vs. multi-task learning. We also investigate improvements to VR through weakly supervised VQA, interpretability of VQA due to using SVLR as a core representation, and word similarities in the learned word embedding compared to word2vec~\cite{word2vec}.  

%The conclusion (Sec.~\ref{sec:conclusion}) lives up to its name.
%a mapping from a textual representation of the class label
%Attention-based VQA models often learn to project the textual query and image regions into an embedding space such that relevant regions will be closer to the query than irrelevant regions. Similarly, visual recognition is a mapping of images to a set of classes with natural language class names, and thus can be similarly formulated as projecting class labels (as text) and the images into a space where the correct class label is close to the image. Given the similarity between visual recognition and the VQA attention subtask, one can see that VQA and VR can be setup in a hierarchical fashion with VQA being dependent on VR. This allows the VQA model to use VR data as complementary supervision for the attention subtask. To achieve this, our work demonstrates how to construct an SVLR that is shared between VR and the attention mechanism of VQA, allowing for direct knowledge transfer without the need to learn a feature to task mapping. During training, the SVLR module will receive supervision from both VR and VQA tasks, leading to a more refined representation that ultimately benefits both tasks. Further, we demonstrate that the attention in our resulting VQA model is more interpretable than those of previous works, as the image region that the model chooses to focus on can be simultaneously visualized with class-label predictions from the VR model.

%Our main contributions are:
%\vspace{-0.5em}
%\begin{enumerate}
%    \itemsep-0.2em
%    \item learning a shared vision-language representation space through our SVLR module for inductive transfer across tasks
%    \item formulating VQA and VR in terms of inner products of word-region representations to allow them to share an SVLR module
%    \item an interpretable model for VQA with stronger supervision for \textit{attention} using a VR dataset than that provided by QA annotations in a VQA dataset alone.
%\end{enumerate}



%For example, consider an Visual Question Answering (VQA) with visual attention and Visual Recognition (VR) as a set of related tasks. A na{\"i}ve way is to reuse a visual recognition network to obtain the image representation for VQA, but using a completely different network for constructing the QA representation. However, such an approach ignores the fact that class labels in VR are also words appearing in QA annotations for VQA. 

%In the naive approach, the VR model would be a classification network mapping images to class labels. To transfer from VR to VQA, one would simply use the intermediate (eg. fc7) output of the VR network as image features for the VQA model. The problem with this approach is that the interpretation of the intermediate features in the VQA model could vastly differ from its original interpretation in the VR model. However, if both tasks were reformulated as a mapping from image to text in an embedding space, we can force both tasks to use the exact same embedding space, thereby allowing for transfer of knowledge without the need for reinterpretation.

%Both tasks have an image and language component: image and class labels for VR, and image and QA for VQA. A naive approach is to take the intermediate output (eg. fc7) of a VR model as image features for the VQA model, which will have its own

%In other words the VR model does not acknowledge the alignment between image-region and word representations, and the only signal for encouraging such an alignment during training is provided by the VQA task.


%The representation of a car in one task may be used for something completely unrelated after the transformation.  

%We design an Shared Vision Language Representation (SVLR) module which produces image-region and word representations which are reused across domain specific models. In a task hierarchy consisting of VR and VQA, 

%We test our hypothesis on two complementary and related tasks: visual recognition (VR) and visual question answering (VQA). Our key insight for constructing a shared representation is that parts of both VR and VQA models can be posed in terms of a mapping between image regions and natural language. In a shared embedding space, for example, the dot product between representations of an image-region and class label word such (e.g. "dog") can be interpreted as the classification score for that class. This mechanism is already used in VQA models for producing \textit{attention} in which parts of a question are mapped to relevant regions of the image. While \textit{attention} is a latent sub-task in the VQA as annotation is not provided for which regions to map to which parts of the text, VR provides direct supervision for this mapping. Nevertheless, this mapping from text to images has the same interpretation in both settings. We will refer to this shared image to text mapping representation as a Shared Vision-Language Representation (SVLR)
