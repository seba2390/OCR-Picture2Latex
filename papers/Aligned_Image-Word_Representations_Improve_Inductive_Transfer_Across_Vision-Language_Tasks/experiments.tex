\section{Experiments}
\label{sec:experiments}
Our experiments investigate the extent to which using SVLR as a core representation improves transfer in multitask learning.  We first analyze how including the VR task improves VQA (Sec.~\ref{sec:vqa_eval}, Tab.~\ref{Tbl:abltionperf}).  We find that using SVLR doubles the improvement compared to standard multitask learning, and demonstrate performance well above chance in a zero-shot setup (trained only on VR, applied to VQA).  We then analyze improvement to VR due to training with (weakly supervised) VQA (Sec.~\ref{sec:vqa2vr}, Fig.~\ref{fig:obj_change}).  We find moderate overall improvements (1.2$\%$), with the largest improvements for classes that have few VR training examples.  We also quantitatively evaluate how well our attention maps correlate with that of humans using data provided by~\cite{das2016human} in Table~\ref{tbl:vqahat}. We include results of our VQA system trained with ResNet-152 architecture on val, test-dev, test-std, along with state-of-the-art (Tab.~\ref{tab:state_art}). 

\input{main_table.tex}

\subsection{Datasets}
Our model is trained on two separate datasets: one for VQA supervision, one for visual recognition (attributes and object classification). We use the image-question-answer annotation triplets from Antol et al. \cite{antol2015vqa} and bounding box annotations for object and attribute categories from Visual Genome~\cite{krishna2016visual}. The train-val-test splits for the datasets are as follows. 

\boldhead{VQA} We split the \textit{train} set into \textit{train-subset} and \textit{train-held-out} and use the latter for model selection. The \textit{train-subset} consists of 236,277 $(Q,I,A)$ samples whereas \textit{train-held-out} contains 12,072 samples. The \textit{val} and \textit{test} set contain 121,512 and 244,302 samples respectively. There are exactly 3 questions per image. We use VQA val for evaluating on specific question types.

\boldhead{Visual Genome} We use only images from Visual Genome not in VQA (overlaps identified using md5 hashes). The selected images were divided into \textit{train}-\textit{val}-\textit{test} using an 85-5-10 split, yielding 1,565,280, 90,212 and 181,141 annotated regions in each. We use \textit{val} for selecting the model for evaluating recognition performance. 

\subsection{Inductive Transfer from VR to VQA}\label{sec:vqa_eval}\label{sec:vr2vqa}
In Table~\ref{Tbl:abltionperf}, we analyze the role of SVLR module for inductive transfer in both joint training and zero-shot settings.

\boldhead{Joint Training} During joint training, the VR models and VQA model are simultaneously trained using object and attribute annotations from Genome, and Q/A annotations from the VQA dataset. The common approach to joint training is to use a common network for extracting image features (e.g. class logits from ResNet), which feeds into the task-specific networks as input. We refer to this approach in Table~\ref{Tbl:abltionperf} as \textit{Joint Multitask}. This baseline is implemented by replacing $g(y)$ (see Fig.~\ref{fig:features}),  with a fixed set of vectors $h_y$ for each of the predetermined 1000 object and 1000 attribute categories in the VR models. The embedding $g(y)$ is still in the VQA model, but is no longer shared across tasks. Our proposed \textit{Joint SVLR} outperforms VQA-only by $2.4\%$, doubling the $1.2\%$ improvement achieved by \textit{Joint Multitask}.  Our formulation of VR and VQA tasks in terms of shared word-region representations more effectively transfers recognition knowledge from VR than shared features.  The gain is often larger on questions that involve recognition (in bold in Table~\ref{Tbl:abltionperf}).  For example, \textit{what color} questions improve by $8.6\%$ due to SVLR.

Surprisingly, pre-training the visual classifiers on Genome prior to joint training performs worse than the model trained jointly from scratch: 63.7\% versus 65.3\%.

\boldhead{Zero-Shot VQA} We evaluate Zero-shot VQA to further highlight transfer from VR to VQA. We train on only Genome VR annotations but test on VQA val. The model has not seen any Q/A training data, but achieves an overall accuracy of $16.4\%$ where random guessing yields $5.6\%$ (18 choices).  Our zero-shot system does not exploit language priors, which alone can score as high as 54.0$\%$~\cite{shih2016look}. This shows that some knowledge can be directly applied to related tasks using SVLR without additional training.
%Our result demonstrates that given our formulation, knowlege from the VR task can directly contribute to the performance of VQA without the need to learn a feature to task mapping.\\

\begin{figure}
\vspace{-2mm}
\includegraphics[width=0.95\columnwidth]{images/visualize_object_diff_table.pdf}
\vspace{-4mm}
\caption{\textbf{Transfer from VQA to Object Recognition:} Each cell's color reflects the mean change in accuracy for classes within the corresponding frequency ranges of both datasets' training split. Most gains are in nouns rare in Genome but common in VQA (top left), suggesting that the weak supervision provided by training VQA attention augments recognition performance via the SVLR. The numbers in each cell show the Genome-only mean accuracy +/- the change due to SVLR multitask training, followed by the number of classes in the cell in parentheses.}
\vspace{-2mm}
\label{fig:obj_change}
\end{figure}



%As shown in Table~\ref{tab:state_art}, our model achieves competitive results on VQA. Our model offers significantly improved localization performance as implied by \textit{val} set performance on \textit{color} questions. This is because questions about color require precise localization of the relevant area to make an accurate classification.\\ \todo{TODO: update with 152 numbers}

\input{failure_modes_results.tex}

\subsection{Inductive Transfer from VQA to VR}\label{sec:vqa2vr}
We compare the performance of our SVLR based model trained jointly on VQA and VR data with a model trained only on Genome data to analyze transfer from VQA to VR. Genome \textit{test} is used for evaluation. We observe an increase in the overall object recognition accuracy from 43.3\% to 44.5\%, whereas average attribute accuracy remained unchanged at 36.9\%. In Fig.~\ref{fig:obj_change}, we show that nouns that are rare in Genome (left columns) but have 20 or more examples in VQA (upper rows) benefit the most from weak supervision provided by VQA.  On average, we measure improvement from 21$\%$ to 32$\%$ for the 8 classes that have fewer than 125 examples in Genome train but occur more than 160 times in VQA questions. We conducted the same analysis on Genome attributes, but did not observe any notable pattern, possibly due to the inherent difficult in evaluating the multi-label attribute classification problem (the absence of attributes is not annotated in Genome).

%For object recognition, the accuracy for object classes which are neither too rare nor too frequent in the Genome training set improve while performance on most others is preserved. We do not expect to see improvement for classes which are already well represented in the Genome training data, nor for classes too rare for both datasets (bottom left cell). This effect is observed to a lesser extent in case of attributes, but we believe it is due to incomplete attribute annotations in the Genome dataset that prevent accurate multi-label evaluation. But we include attribute recognition as one of the tasks during joint training to enable VQA model to answer attribute based questions.   



\subsection{Interpretable Inference for VQA}
As shown in Fig. \ref{fig:rel_qual}, our VQA model produces interpretable intermediate outputs such as region relevance and visual category predictions, similar to \cite{tommasi2016bmvc}. The answer choice is explained by the object and attribute predictions associated with the most relevant regions. Because relevance is posed as the explicit localization of words in the question and answer, we can qualitatively evaluate the relevance prediction by verifying that the predicted regions match said words. This also provides greater insight into the failure modes as shown in Fig. \ref{fig:fail_modes}. 

We also quantitatively evaluate our attention using collected human attention maps from Das et al.~\cite{das2016human} in Table~\ref{tbl:vqahat} and in Figure~\ref{fig:vqahat_comparison}. Table~\ref{tbl:vqahat} includes the correlation scores between the attention maps from various VQA attention models and human attention on a subset of question-answer pairs on the validation set. Our proposed SVLR model significantly outperforms other models we compare with. However, we note that a strong center-focused heatmap baseline still outperforms all models, signifying that the main topic of a question is very often located in the center of the image. As such, we also evaluate correlations on multiple subsets of the human attention maps is Figure~\ref{fig:vqahat_comparison}, thresholding them based on correlation with the center heatmap. We note that learned attention models appear to have better correlation with human attention at lower thresholds where the human attention correlates poorly with the center-focused heatmap -- a result also demonstrated in ~\cite{das2016human}.

\begin{figure}                                  
  \centering                                                                        
  \includegraphics[width=\columnwidth]{images/vqahat_compare_tbl.pdf}  
  \caption{\textbf{Mean Spearman rank-correlation between model
      predicted and human attention at various thresholds.} Each
      threshold point defines a subset of the dataset for
      which the human attention correlation with the synthetic center
      heatmap is below that threshold value. For example: the first sample
      point of each curve is the mean correlation of each model with
      human attention, measured on a subset in which the human
      attention's correlation with the center heatmap is
      less than or equal to 0. As can
    be seen, the attention maps produced by the proposed SVLR model correlate with human attention significantly more than other models. As the threshold
    approaches 1, the synthetic center heatmap baseline outperforms
    all proposed models, confirming that the majority of the questions
    are about something in the center of the image. Note that due to slight differences in implementations, the subsets at $\le 0$ differ slightly from those used in \cite{das2016human}}
  \label{fig:vqahat_comparison}                                                                                                     
\end{figure}                 
%We also quantitatively evaluate our attention using collected human attentions from Das et al.~\cite{das2016human} in Table~\ref{tbl:vqahat}. The comparison with WTL~\cite{shih2016look} is most informative, as it generates its attention map more similarly to the SVLR than HiCo~\cite{liu2016attention} and SAN~\cite{yang2015stacked}, specifically in the use of weighted Edge Boxes.
%
\begin{table}
\setlength{\tabcolsep}{3 pt}
\small
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|c||c|c|}
        \hline
         & HiCo\cite{lu2016hierarchical} & SAN2\cite{yang2015stacked}& WTL\cite{shih2016look} & SVLR & Center & Human \\
         \hline
         Corr.& 0.27 & 0.26 & 0.38 & \textbf{0.48} & 0.53 &0.62\\
         \hline
    \end{tabular}
    \vspace{-1em}
    \caption{\textbf{Human Attention Comparison:} We compare our attention maps with human attentions collected by Das et al.~\cite{das2016human}. Comparison was done by resizing attention maps to 14$\times$14 and computing the Spearman rank correlation as in ~\cite{das2016human}. We include a strong baseline using a synthetic center-focused heatmap (also used by ~\cite{das2016human}) under the Center column. The Human column represents the inter-annotator agreement. Scores for HiCo and SAN2 were recomputed using released data from~\cite{das2016human}, and differ slightly from originally reported. Our model leads to significantly higher correlation with human annotations than existing models.}
    \label{tbl:vqahat}
    \vspace{-2em}
\end{table}

\subsection{Learned Word Representations}
In Table~\ref{Tbl:embeddingexamples}, we compare the word representations of the SVLR model to that of Word2Vec~\cite{word2vec} by showing several nearest neighbors from both embeddings. We observe a shift from non-visual neighborhoods and meanings (monitor, control) to visual ones (monitor, keyboard). Neighbors were computed using cosine distance after mean centering. 

\input{external_comparisons.tex}
\input{word_vec_table.tex}