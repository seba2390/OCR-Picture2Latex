\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{images/acc_vs_rarity.pdf}
\vspace{-1cm}
\end{center}
  \caption{The effects of jointly training VQA and recognition with and without shared language representations for question types in bold in Table.~\ref{Tbl:abltionperf}. We bin question-answer pairs based on frequency of its least frequent word. We compute each bin's accuracy (primary axis) for each model. We show the percentage reduction in error on the secondary axis. Note that familiarity with $(Q,A)$ words correlates with higher performance. Further, both forms of joint training show larger gains for more frequent question-answers (yellow and green). This is mainly because nouns and adjectives which are more common in VQA also tend to be more common in Visual Genome, resulting in more supervision for the classifiers. Finally, using word embeddings in visual classifiers imposes a structure in the shared representation space that benefits frequent and rare question-answers almost equally (blue).}
\label{fig:acc_vs_rarity}
\end{figure}