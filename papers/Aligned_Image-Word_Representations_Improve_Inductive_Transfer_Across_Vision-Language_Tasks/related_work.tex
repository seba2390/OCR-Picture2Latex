\section{Related Work}
\label{sec:related}
\noindent \textbf{Never-ending learning:}  NEL~\cite{mitchell2010never,carlson2010toward,thrun1998lifelong,silver2013lifelong,chen2013iccv} aims to continuously learn from multiple tasks such that learning to solve newer problems becomes easier. Representation learning~\cite{bengio2013pami}, multitask learning~\cite{caruana1998springer}, and curriculum learning~\cite{pentina2015curriculum} are different aspects of this larger paradigm. Inductive transfer through shared representations is a necessary first step for NEL. Most works focus on building transferable representations within a single modality such as language or vision only. We extend this framework to learn a joint vision-language representation which enables a much larger class of new vision-language tasks to easily build on and contribute to the shared representation. \\
\vspace{-2mm}

\noindent \textbf{VR using Vision-language embeddings:} Traditionally, visual recognition has been posed as multiclass classification over discrete labels~\cite{he2015deep,simonyan2014very,krizhevsky2012imagenet}. Using these recognizers for tasks like VQA and image captioning is challenging because of the open-vocabulary nature of these problems. However, availability of continuous word embeddings (e.g. word2vec~\cite{mikolov2013efficient}) has allowed reformulation of visual recognition as a nearest neighbor search in a learned image-language embedding space~\cite{wang2016learning}. Such embeddings have been successfully applied to a variety of tasks that require recognition such as image captioning~\cite{lin2014microsoft,hodosh2013framing}, phrase localization~\cite{plummer2015flickr30k,krishna2016visual}, referring 
expressions~\cite{KazemzadehOrdonezMattenBergEMNLP14,Mao2016cvpr}, and VQA \cite{antol2015vqa,ren2015nips,yu2015visual}. 

Our recognition model is related to previous open-vocabulary recognition/localization models ~\cite{wang2016learning,rohrbach2016grounding,gong2014improving}, which learn to map visual CNN features to continuous word vector representations. However, we specifically focus on the multitask setting where VR forms a part of a higher-level vision-language task such as VQA. Since the SVLR module is reused in both tasks with inner products in the embedding space forming the basis for both models, during joint training VQA provides a weak supervision for recognition as well. Fang~\etal~\cite{fang2015cvpr} also learn object and attribute classifiers from weak supervision in the form of image-caption pairs using a multiple instance learning (MIL) framework, but do not use a vision-language embedding. Liu~\emph{et al.}~\cite{liu2016attention} similarly use VR annotation from Flickr30K entities~\cite{plummer2015flickr30k} to co-supervise attention in a caption-generation model on the same dataset. Our work goes further by allowing the supervision to come from separate datasets, thereby increasing the amount of training data available for the shared parameters. Additionally, we look at how each task has benefited from jointly training with the other.\\
\vspace{-2mm}


%{While the classifier weights can be interpreted as language embeddings, they are not reused as word representations during caption generation. We note that it is trivial to incorporate their noisy-OR MIL loss in our framework and could yield performance improvements.}

%{But there are two main differences. First, instead of trying to create language embeddings that summarily accommodate words and phrases (eg. by using the hidden state of an RNN), we learn single-word representations and construct more complex phrase representations by averaging constituent word representations (sentence representation is described in \todo{method section}). Second, since object and attribute class labels in the VR task are also part of QA annotations for VQA, we use the same word representations for VQA allowing them to receive additional training signals from the VQA task.}

\noindent \textbf{VQA:} Visual Question Answering (VQA) involves responding to a natural language query about an image. Our VQA model is closely related to attention-based VQA models~\cite{fukui2016multimodal,ilievski2016focused,lu2016hierarchical,xu2016ask,shih2016look,yang2015stacked,andreas2016neural,andreas2016learning, kumar2015ask,tommasi2016bmvc} which attempt to compute a distribution (region relevance or \textit{attention}) over the regions/pixels in an image using inner product of image-region and the \textit{full} query embedding~\cite{xu2016ask, shih2016look, ilievski2016focused,lu2016hierarchical}. Region relevance is used as a weight to pool relevant visual information which is usually combined with the language representation to create a multimodal representation. 
%Finally, this representation is passed through a classifier producing an answer score.
Various methods of pooling such as elementwise-addition, multiplication, and outer-products have been explored~\cite{yang2015stacked, fukui2016multimodal}. Attention models are themselves an active area of research with applications in visual recognition~\cite{mnih2014nips,jaderberg2015nips}, object localization, caption generation~\cite{johnson2015arxiv}, question answering~\cite{weston2014memory,sukhbaatar2015end,kumar2015ask}, machine comprehension~\cite{hermann2015teaching} and translation ~\cite{bahdanau2014arxiv,wu2016arxiv}, and neural turing machines ~\cite{graves2014arxiv}.
  
Our model explicitly formulates attention in VQA as image localization of nouns and adjectives mentioned in a candidate QA pair. Ilievski~\etal~\cite{ilievski2016focused} use a related approach for attention. They use word2vec to map individual words in the question to the class labels of a pre-trained object detector which then generates the attention map by identifying regions for those labels. Tommasi~\etal~\cite{tommasi2016bmvc} similarly use a pre-trainined CCA \cite{gong2014improving} vision-language embedding model to localize noun phrases, then extracts scene, attribute, and object features to answer VQA questions. Our model differs from these methods in two ways: (i) vision-language embeddings for VR allow for end-to-end trainability, and (ii) jointly training on VR provides additional supervision of \textit{attention} through a different (non-VQA) dataset. 


%Another work that relies heavily on the question parse is Andreas \etal~\cite{andreas2016neural,andreas2016learning} which uses the syntactic parse to dynamically arrange a set of parametrized neural modules to form a larger question-specific VQA model. Each module performs a specific function such as localizing a specific word or verifying relative locations such as above and below. In contrast, our model is static and simpler while making use of language parsing to make the model interpretable and modular.  

Andreas~\etal~\cite{andreas2016neural,andreas2016learning} rely heavily
on the syntactic parse to dynamically arrange a set of parametrized neural modules. Each module performs a specific function such as localizing a specific word or verifying relative locations. In contrast, our approach uses a static model but relies on language parse to make it interpretable and modular.  
  
