\begin{figure*}[t]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.6\linewidth}{0pt}}
\includegraphics[width=\linewidth]{images/joint_training.pdf}
\vspace{-0.9cm}
\end{center}
   \caption{\textbf{Joint Training on Visual Recognition(VR) and Visual Question Answering(VQA) with the proposed SVLR Module:} The figure depicts sharing of image and word representations through the SVLR module during joint training on object recognition, attribute recognition, and VQA. The recognition tasks use object and attribute labelled regions from Visual Genome while VQA uses images annotated with questions and answers from the VQA dataset. The benefit of joint training is that while the VQA dataset does not provide region groundings of nouns and adjectives in the QA (e.g. ``fluffy",``dog"), this complementary supervision is provided by the Genome recognition dataset. Models for each task involve image and word embeddings produced by SVLR module or their inner products (See Fig~\ref{fig:system} for VQA model architecture).
   }
   \vspace{-6mm}
\label{fig:features}
\end{figure*}

%  \caption{\textbf{Joint Training on Visual Recognition(VR) and Visual Question Answering(VQA) with the proposed SVLR Module:} The figure depicts sharing of image and word representations through the SVLR module during joint training on object recognition, attribute recognition, and VQA. The recognition tasks use object and attribute labelled regions from Visual Genome while VQA uses images annotated with questions and answers from the VQA dataset. The benefit of joint training is that while the VQA dataset does not provide region groundings of nouns and adjectives in the QA (eg. 'fluffy','dog'), this complementary supervision is provided by the Genome recognition dataset. Models for each task involve image and word embeddings produced by SVLR module or their inner products (See Fig~\ref{fig:system} for VQA model architecture). The parameters of the SVLR module and Answer Scoring module in VQA are trained using stochastic gradient descent. As a baseline, we replace $g(y;\theta_{svlr})$, a function of word2vec representation of $y$, with learnable weight vector $h_y$ in the VR models. This is equivalent to using a 1000 way classification layer like the last layers of AlexNet, VGG, or ResNet instead of using our language embeddings for classification. This enables comparison of inductive transfer due to joint training both with and without sharing language representations.}