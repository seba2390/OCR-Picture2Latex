\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{url}
\usepackage[font=small]{caption}
\newcommand\todo[1]{\textcolor{red}{#1}}
%\usepgfplotslibrary{external} 
%\tikzexternalize
\newcommand*\rot{\rotatebox{90}}
\newcommand\boldhead[1]{\vspace{0.03in}\noindent\textbf{#1: }}
% Include other packages here, before hyperref.

% Using baseline stretch. If possible get rid of it by making more space.
% -Saurabh
\renewcommand{\baselinestretch}{0.99} 

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1997} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks}
%\title{Improving Inductive Transfer across Vision-Language Tasks through Aligned Image-Word Representations}
%\title{Learning Image-Word Representations from Complementary and Hierarchical Vision-Language Tasks}
% Keywords - Shared/rich/generalizable representation, multiple vision-language tasks, inductive transfer, intermediate supervision 
% Sharing Vision-Language representations across multiple tasks for inductive transfer   
% What does sharing Vision-Language representation across tasks give us?
% Learning image and language representations from diverse and complementary experience
% Learning image and language representations from diverse and complementary task hierarchy
%\author{First Author\\
%Institution1\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%}
\author{Tanmay Gupta$^1$ \hspace{0,5cm} Kevin Shih$^1$ \hspace{0,5cm} Saurabh Singh$^2$ \hspace{0,5cm} Derek Hoiem$^1$\\
$^1$University of Illinois, Urbana-Champaign  \hspace{0,5cm} $^2$Google Inc.\\
{\tt\small \{tgupta6, kjshih2, dhoiem\}@illinois.edu} \hspace{0,5cm} {\tt\small saurabhsingh@google.com}}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting.  Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.  
   
  
   %Applying the knowledge learned while solving one task to a different but related task is a challenging problem. We propose to address this challenge for vision-language problems by sharing a region and word representation module across models for different tasks. The domain specific models exploit the alignment between the image-region and word representations produced by this shared vision-language representation (SVLR) module. Using visual recognition and visual question answering as an example of related vision-language tasks, we show how to incorporate the SVLR module into models for these tasks. When trained jointly on all tasks, the SVLR module receives diverse but consistent supervision to produce rich representations, improving the performance of individual models through inductive transfer. Our formulation leads to interpretable predictions for VQA and allows for zero-shot VQA. 
   
   
   %Representation plays a crucial role in many vision-language problems such as visual recognition (VR), visual question answering (VQA), image captioning etc. Models for these tasks learn image and language features via end-to-end training. However, these models often ignore the complementary and hierarchical nature of supervision available across datasets for different tasks. For instance, while VR datasets (such as ImageNet) provide a direct mapping between uncluttered images and class label words, VQA datasets provide an alignment between real-world images and the complex semantics of a question-answer (QA) pair. 
   
   %To learn rich image-region and word representations that generalize across tasks, we propose a shared vision-language representation (SVLR) module that is shared across models for different tasks. Using VR and VQA as an example of a complementary task hierarchy, we show how to incorporate SVLR module into popular models for these tasks. We demonstrate that when trained jointly on all tasks, SVLR accumulates diverse supervision to produce rich representations that can boost performance of individual models through inductive transfer. Our formulation also leads to more interpretable predictions for VQA. 
\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-5mm}

\section{Introduction}

\input{intro.tex}


\input{joint_training.tex}


%-------------------------------------------------------------------------
\input{related_work.tex}



%-------------------------------------------------------------------------


\section{Method}
We propose an SVLR module to facilitate greater inductive transfer across vision-language tasks. As shown in Fig.~\ref{fig:features}, the word and region representations required for object recognition, attribute recognition, and VQA are computed through the SVLR module. By specifically formulating each task in terms of inner products of word and region representations and training on all tasks jointly, we ensure each task provides a consistent, non-conflicting training signal for aligning words and region representations. During training, the joint-task model is fed batches containing training examples from each task's dataset.

\vspace{-1mm}

\subsection{Shared Vision Language Representation}
\label{sec:embedding}
The SVLR module converts words and image-regions into feature representations that are aligned to each other and \textit{shared} across tasks.  \\

\noindent\textbf{Word Representations:} The representation $g(w)$ for a word $w$ is constructed by applying two fully connected layers (with 300 output units each) to pretrained word2vec representation~\cite{word2vec} of $w$ with ReLU after the first layer. \\

\noindent\textbf{Region Representations:} A region $R$ is represented using two $300$ dimensional feature vectors $f_o(R)$ and $f_a(R)$ that separately encode the objects and attributes contained. We used two representations instead of one to encourage disentangling of these two factors of variation. For example, we do not expect ``red" to be similar to ``apple", but we expect $f_o(R)$ and $f_a(R)$ to be similar to $g(``red")$ and $g(``apple")$ if $R$ depicts a red apple. The features are constructed by extracting the average pooled features from Resnet~\cite{he2015deep} pretrained on ImageNet and then passing through separate object and attribute networks. Both networks consist of two fully connected layers (with 2048 and 300 output units) with batch normalization~\cite{batchnorm} and ReLU activations. 

\subsection{Visual Recognition using SVLR}
\label{sec:vr}
\subsubsection{Inference}\label{sec:recog_inference}
The visual recognition task is to classify image regions into one or more object and attribute categories. The classification score for region $R$ and object category $w$ is $f_o^T(R)g(w)$. The classification score for an attribute category $v$ is $f_a^T(R)g(v)$. Attributes may include adjectives and adverbs (e.g., ``standing''). Though our recognition dataset has a limited set of object categories $\mathcal{O}$ and attribute categories $\mathcal{T}$,  our model can produce classification scores for any object or attribute label given its word2vec representation. In experiments, the $\mathcal{O}$ and $\mathcal{T}$ consist of 1000 most frequent object and attribute categories in the Visual Genome dataset~\cite{krishna2016visual}. 

\vspace{-3mm}
\subsubsection{Training}\label{sec:recog_learn}
Our VR model is trained using the Visual Genome dataset which provides image regions annotated with object and attribute labels. VR uses only the parameters for the embedding functions $f_o, f_a$ and $g$ that are part of the SVLR module. The parameters of $f_o$ receive gradients from the object loss while those of $f_a$ receive gradients from the attribute loss. The parameters of word embedding model $g$ receive gradients from both losses.\\

\noindent 
\textbf{Object loss:} %We use a multi-label classification loss as object classes may not be mutually exclusive due to hypernyms (e.g., ``man'' \emph{is a} ``person'') and synonyms.  
We use a multi-label loss as object classes may not be mutually exclusive (e.g., ``man'' \emph{is a} ``person''). For a region $R_j$, we denote the set of annotated object categories and their hypernyms extracted from WordNet \cite{miller1995acm} by $\mathcal{H}_j$. The object loss forces the true labels and their hypernyms to score higher than all other object labels by a margin $\eta_{obj}$. For a batch of $M$ samples $\{(R_j,\mathcal{H}j)\}_{j=1}^{M}$ the object loss is:
\vspace{-5mm}
\begin{multline}
\mathcal{L}_{obj} = \frac{1}{M}\sum_{j=1}^{M}\frac{1}{|\mathcal{H}j|}\sum_{l \in \mathcal{H}j} \frac{1}{|\mathcal{O}|}
\sum_{k \in \mathcal{O}\setminus \mathcal{H}j} \\
\max\{0,\eta_{obj} + f_o^T(R_j)g(k) - f_o^T(R_j)g(l)\}
\end{multline}

\noindent
\textbf{Attribute Loss:} The attribute loss is a multi-label classification loss with two differences from object classification. Attribute labels are even less likely to be mutually exclusive than object labels. As such, we predict each attribute with independent cross entropy losses. We also weigh the samples based on fraction of positive labels in the batch to balance the positive and negative labels in the dataset. For a batch with M samples $\{(R_j,\mathcal{T}_j)\}_{j=1}^{M}$ where $\mathcal{T}_j$ is the set of attributes annotated for region $R_j$, the attribute loss is:
\vspace{-4mm}
\begin{multline}
\mathcal{L}_{atr} = \frac{1}{M}
\sum_{j=1}^{M} 
\sum_{t \in \mathcal{T}} \\
\mathbbm{1}\left[t \in \mathcal{T}_j\right](1-\Gamma(t))\log\left[\sigma(f_a^T(R_j)g(t))\right] + \\
\mathbbm{1}\left[t \notin \mathcal{T}_j\right]\Gamma(t)\log\left[1-\sigma(f_a^T(R_j)g(t))\right]
\end{multline}    
where $\sigma$ is a sigmoid activation function and $\Gamma(t)$ is the fraction of positive samples for attribute $t$ in the batch.

\input{vqa_figure.tex}
\vspace{-1mm}
\subsection{Visual Question Answering using SVLR} \label{sec:vqa}

Our VQA model is illustrated in Fig.~\ref{fig:system}.
%\subsection{Inference} \label{sec:vqa_inference}
The input to our VQA model is an image, a question, and a candidate answer. Regions are extracted from the image using Edge Boxes~\cite{zitnick2014edge}.  The same SVLR module used by VR (Sec.~\ref{sec:vr}) is explicitly applied to VQA for attention and answer scoring.  Our system assigns attention scores to each region according to how well it matches words in the question/answer, then scores each answer based on the question, answer, and attention-weighted scores for all objects ($\mathcal{O}$) and attributes ($\mathcal{T}$).

\boldhead{Attention Scoring}\label{sec:relevance} Unlike other attention models ~\cite{yang2015stacked,lu2016hierarchical} that are free to learn any correlation between regions and question/answers, our attention model encodes an explicit notion of vision-language grounding.  Let $\mathcal{R}$ be the set of region proposals extracted from the image, and $\mathcal{N}$ and $\mathcal{J}$ denote the set of nouns and adjectives in the $(Q,A)$ pair. Each region $R\in \mathcal{R}(I)$ is assigned an attention score $a(R)$ as follows:\\
\vspace{-5mm}
\begin{align}
a'(R) &= \max_{n \in \mathcal{N}} f_o^T(R)g(n) + \max_{j \in \mathcal{J}} f_a^T(R)g(j) \label{eqn:relevance_unnormalized}\\
a(R)&= \frac{\exp(a'(R))}{\sum_{R' \in \mathcal{R}(I)}\exp(a'(R'))}
\label{eqn:relevance}
\end{align}

Thus, a region's attention score is the sum of maximum adjective and noun scores for words mentioned in the question or answer (which need not be in sets $\mathcal{O}$  and $\mathcal{T}$).   

%The image representation is computed by averaging these VR scores using the attention scores as weights. This image representation is combined with the QA representation using bimodal pooling layers, and the resulting encoding of the image-question-answer triplet is scored using a fully connected layer. The answer with the highest score is chosen. \\

\boldhead{Image Representation} To score an answer, the content of region $R$ is encoded using the VR scores for all objects and attributes in $\mathcal{O}$ and $\mathcal{T}$, as presence of unmentioned objects or attributes may help answer the question. The image representation is an attention-weighted average of these scores across all regions: 
%To capture the visual content of a region $R$, we concatenate the object and attribute scores for visual categories $\mathcal{O}$ and $\mathcal{T}$ into vectors $s_o(R) \in \mathbb{R}^{1000}$ and $s_a(R) \in \mathbb{R}^{1000}$. The final 2000 dimensional, $(Q,A)$ specific, attended image representation is constructed by averaging features $s_o$ and $s_a$ across all regions using relevance scores as weights.
\begin{equation}
f(I) = \sum_{R\in\mathcal{R}(I)}a(R)
\begin{bmatrix}
s_o(R) \\
s_a(R) \\
\end{bmatrix}
\label{eqn:weightedvisualfeats}
\end{equation}
where $I$ is the image, $s_o(R)$ are the scores for 1000 objects in $\mathcal{O}$ for each image region $R$, $s_a(R)$ are the scores for 1000 attributes in $\mathcal{T}$, and $a(R)$ is the attention score.

\boldhead{Question/Answer Representation}  To construct representations $q(Q)$ and $a(A)$ for the question and answer, we follow Shih et al.~\cite{shih2016look}, dividing question words into 4 bins, averaging word representations in each bin, and concatenating the bin representations resulting in a 1200 ($=300\times4$) dimensional vector $q(Q)$. The answer representation $a(A)\in\mathbb{R}^{300}$ is obtained by averaging the word representations of all answer words. The word representations used here are produced by the SVLR module.

\boldhead{Answer Scoring} We combine the image and Q/A representations to jointly score the $(Q,I,A)$ triplet. %Since some answers like $\{0,1,2,3,yes,no\}$ may not be well represented using vector representations, we experiment with appending binary features for these answers in $a(A)$.

To ensure equal contribution of language and visual features, we apply batch normalization~\cite{batchnorm} on linear transformations of these features before adding them together to get a bimodal representation $\beta(Q,I,A)\in\mathbb{R}^{2500}$:
\begin{multline}\label{eq:bimodal_pool}
\beta(Q,I,A) = \;\mathcal{B}_1(W_1f(I)) 
+ \;\mathcal{B}_2\left(W_2 
\begin{bmatrix}
q(Q) \\
a(A) \\
\end{bmatrix}
\right)
\end{multline}
Here, $\mathcal{B}_1,\mathcal{B}_2$ denote batch normalization and $W_1\in\mathbb{R}^{2500\times2000}$ and $W_2\in\mathbb{R}^{2500\times1500}$ define the linear transformations.
\noindent
The bimodal representation is: 
\begin{equation}
\mathcal{S}(Q,I,A) = W_3 \; \text{ReLU}(\beta(Q,I,A))
\end{equation}
with  ${W_3\in\mathbb{R}^{1\times2500}}$.


\boldhead{Training}\label{sec:vqa_learn}
We use the VQA dataset~\cite{antol2015vqa} for training parameters of our VQA model: $W_1, W_2, W_3$, and scales and offsets of batch normalization layers. In addition, the VQA loss backpropagates into $f_o, f_a$, and $g$ which are part of the SVLR module. Each sample in the dataset consists of a question $Q$ about an image $I$ with list of answer options including a positive answer $A^{+}$ and $N$ negative answers $\{A^{-}(i) | i=1,\cdots, N\}$. 

The VQA loss encourages the correct answer $A^{+}$ to be scored higher than all incorrect answer options $\{A^{-}(i) | i=1,\cdots, N\}$ by a margin $\eta_{ans}$. Given batch samples $\{(Q_j,I_j,A_j)\}_{j=1}^{P}$, the loss is written as 
\vspace{-2mm}
\begin{multline}
\mathcal{L}_{ans} = \frac{1}{NP}\sum_{j=1}^{P}
\sum_{i=1}^{N} \max\{0,\\\;\eta_{ans} + \mathcal{S}(Q_j,I_j,A_j^{-}(i)) - \mathcal{S}(Q_j,I_j,A_j^{+})\}
\end{multline}

\input{relevance_qual_results.tex}

\subsection{Zero-Shot VQA} 

%In the introduction, we claim the ability to perform a new vision-language task without retraining as evidence that our SVLR transfers knowledge across tasks without the need to re-learn a feature to task mapping.  
The representations produced by SVLR module should be directly usable in related vision-language tasks without any additional learning. To demonstrate this \textit{zero-shot cross-task transfer}, we train the SVLR module using Genome VR data only and apply to VQA. Since bimodal pooling and scoring layers cannot be learned without VQA data, we use a proxy scoring function constructed using region-word scores only.  For each region, we compute $p_q(R)$ as the sum of its scores for the maximally aligned question nouns and question adjectives (Eq.~\ref{eqn:relevance_unnormalized} with only question words). A score $p_a(R)$ is similarly computed using answer nouns and adjectives. The final score for the answer is defined by
\begin{equation}
    S(Q,I,A)=\sum_{R\in\mathcal{R}}a(R)\min(p_q(R),p_a(R))
\end{equation}
where $a$ is the attention score computed using Eq.~\ref{eqn:relevance}. Therefore, the highest score is given to QA pairs where question as well as answer nouns and adjectives can be localized in the image. Note that the since the model is not trained on even a \textit{single question} from VQA, the zero-shot VQA task also shows that our model does use the image to answer questions instead of solely relying on the language prior which is a common concern with most VQA models \cite{agrawal2016analyzing,goyal2016arxiv}. 

\section{Implementation and Training Details}
We use 100 region proposals resized to $224 \times 224$ for all experiments. Resnet-50 was used for image feature extraction in all experiments except those in Tab.~\ref{tab:state_art} which used Resnet-152. The nouns and adjectives are extracted from the $(Q,A)$  and lemmatized using the part-of-speech tagger and WordNet lemmatizer in NLTK \cite{bird2009book}. We use the Stanford Dependency Parser \cite{de2006lrec} to parse the question into bins as detailed in~\cite{shih2016look}.  All models are implemented and trained using TensorFlow \cite{tensorflow2015software}. We train the model jointly for the recognition and VQA tasks by minimizing the following loss function using Adam~\cite{adamoptimizer}:
\begin{equation}
\mathcal{L} = \alpha_{ans}\mathcal{L}_{ans} + \alpha_{obj}\mathcal{L}_{obj} + \alpha_{atr}\mathcal{L}_{atr}
\end{equation}
We observe that values of $\alpha_{obj}$ and $\alpha_{atr}$ relative to $\alpha_{ans}$ can be used to trade-off performance between visual recognition and VQA tasks. For experiments that analyze the effect of transfer from VR to VQA (Sec.~\ref{sec:vr2vqa}), we set ${\alpha_{ans} = 1, \alpha_{obj} = 0.1}$, and ${\alpha_{atr}=0.1}$. For VQA only and Genome only baselines, we set the corresponding $\alpha$ to 1 and others to 0.  For experiments dealing with transfer in the other direction (Sec.~\ref{sec:vqa2vr}), we set ${\alpha_{ans} = 0.1, \alpha_{obj} = 1}$, and ${\alpha_{atr}=1}$.  The margins used for object and answer losses are $\eta_{ans}=\eta_{obj}=1$. The object and attribute losses are computed for the same set of Visual Genome regions with a batch size of $M=200$. The answer loss is computed for a batch size of $P=50$ questions sampled from VQA. We use an exponentially decaying learning rate schedule with an initial learning rate of $10^{-3}$ and decay rate of 0.5 every 24000 iterations. Weight decay is used on all trainable variables with a coefficient of $10^{-5}$. All the variables are Xavier initialized \cite{glorot2010aistats}.


\input{experiments.tex}
\section{Conclusion}
%We propose an effective means of transferring knowledge and representations between related vision-language tasks by formulating each tasks' model around a shared image-to-text embedding space (SVLR). By sharing knowledge across tasks in the form of alignments between visual features and text, the shared representation has the same interpretation across all tasks, allowing supervision in one task to directly benefit the other by means of improving the shared embedding space. 

Humans learn new skills by building upon existing knowledge and experiences. We attempt to apply this behavior to AI models by demonstrating cross-task learning for the class of vision-language problems using VQA and VR. To enhance inductive transfer, we propose sharing core vision and language representations across all tasks in a way that exploits the word-region alignment. We plan to extend our method to larger sets of vision-language tasks. 

%We designed an interpretable Vision-Language representation space that can be reused across multiple tasks. We show learning and application of the shared representation space through visual recognition and the VQA tasks. We also formulate the VQA task in terms of the recognition task which allows more efficient utilization of recognition supervision for VQA and weak supervision of visual recognition through VQA. As future work, we plan to extend the approach to other tasks such as activity recognition and image-caption retrieval. For VQA, we plan to model pairwise relations explicitly to answer questions involving spatial reasoning. 

\section{Acknowledgements}
This work is supported in part by NSF Awards 14-46765 and 10-53768 and ONR MURI N000014-16-1-2007.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
