\begin{figure}[t]

\centering
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{images/short_overview.pdf}
\caption{}
\label{fig:short_overview}
\end{subfigure}

\centering
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{images/yellow_surfboard.pdf}
\caption{}
\label{fig:teaser_interpert}
\end{subfigure}

\caption{(a) We propose a multitask learning model for Vision-Language tasks. Our model projects images and words into a shared representation space. The resulting visual and textual embeddings are then used for each task. We demonstrate the general concept on visual recognition and VQA; (b) We pose VQA in terms of the visual recognition task resulting in a model with interpretable intermediate outputs like region relevance, and object and attribute predictions.}
\end{figure}