\begin{figure*}[ht]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=\linewidth]{images/vqa_model.pdf}
\vspace{-1.1cm}
\end{center}
  \caption{\textbf{Inference in our VQA model:} The image is first broken down into Edge Box region proposals\cite{zitnick2014edge}. Each region $R$ is represented by visual category scores $s(R) = [s_o(R), s_a(R)]$ obtained using the visual recognition model. Using the SVLR module, the regions are also assigned an attention score using the inner products of region features with representations of nouns and adjectives in the question and answer. The region features are then pooled using the relevance scores as weights to construct the \textit{attended} image representation. Finally, the image and question/answer representations are combined and passed through a neural network to produce a score for the input question-image-answer triplet.}
  \vspace{-5mm}
\label{fig:system}
\end{figure*}