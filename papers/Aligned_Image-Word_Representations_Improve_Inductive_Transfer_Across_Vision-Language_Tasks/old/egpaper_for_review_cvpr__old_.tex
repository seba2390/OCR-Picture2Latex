\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{url}
\usepackage[font=small]{caption}
\newcommand\todo[1]{\textcolor{red}{#1}}

\newcommand*\rot{\rotatebox{90}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi




\begin{document}

%%%%%%%%% TITLE
%\title{Learning Image-Text embeddings for Multitask Learning}
%\title{Jointly Supervised Visual Attention for Multiple Vision-Language Tasks}
%\title{Understanding words visually}
%\title{Building shared visual semantic representations across tasks}
\title{Learning Image-Word Representations from Complementary Vision-Language Tasks}
% Building Shared Vision-Language Representations Across Tasks
%\title{Supervising vision through language and vice-versa}
%\title {Exploiting visual recognition subtask structure for jointly learning Vision-Language tasks}
%\title{Exploiting subproblem structure of Vision-Language tasks for multitask learning}
\author{Tanmay Gupta$^1$ \hspace{0,5cm} Kevin Shih$^1$ \hspace{0,5cm} Saurabh Singh$^2$ \hspace{0,5cm} Derek Hoiem$^1$\\
$^1$University of Illinois, Urbana-Champaign  \hspace{0,5cm} $^2$Google Inc.\\
{\tt\small \{tgupta6, kjshih2, dhoiem\}@illinois.edu} \hspace{0,5cm} {\tt\small saurabhsingh@google.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    % moved to intro
   %Representation plays a crucial role in many vision-language problems such as visual recognition (VR), visual question answering (VQA), image captioning etc. For each task, state-of-the-art models employ modern deep neural architectures and massive domain specific datasets to learn image and language features via end-to-end training. However, these models often ignore the complementary and hierarchical nature of supervision available across datasets for different tasks. For instance, while VR datasets (such as ImageNet) provide a direct mapping between uncluttered images and class label words, VQA datasets provide an alignment between real-world images and the complex semantics of a question-answer (QA) pair. 
   Embedding spaces that map language and visual information into a shared space are commonly used internal representation for vision-language tasks. In the following work, we propose a shared vision-language embedding to be used for both visual question answering (VQA) and visual recognition (VR). We further demonstrate that the shared representation allows the two tasks to become mutually beneficial, due to the shared embedding  parameters receiving multiple sources of supervision.
   
   %reworded above ^
   %To learn rich image region and word representations that generalize across tasks, we propose a shared vision-language representation (SVLR) module that is shared across models for different tasks. Using VR and VQA as an example of a complementary task hierarchy, we show how to incorporate SVLR module into popular models for these tasks. We demonstrate that when trained jointly on all tasks, SVLR accumulates diverse supervision to produce rich representations that can boost performance of individual models through inductive transfer. Our formulation also leads to more interpretable predictions for VQA. 
\end{abstract}


%\begin{abstract}
  % Tasks involving vision and language require a mapping between visual and linguistic modalities. In this work, we propose learning such a mapping in a way that is conducive to inductive transfer and reuse across multiple related tasks. As an application of this general concept, we demonstrate how to reuse this shared representation for two tasks: (i) visual recognition of objects and attributes, and (ii) visual question answering (VQA). Our second contribution is posing VQA in terms of the visual recognition task which has two advantages: (i) making the inference procedure transparent by producing human interpretable intermediate outputs, and (ii) allowing us to improve performance on VQA using visual recognition training data. We show that our jointly trained VQA model outperforms the model that does not use visual recognition data, as well as the model that uses visual recognition supervision but without the explicit mapping between images and visual categories. Finally, we confirm inductive transfer in the opposite direction by observing improvements in the classification task after joint training. 
%\end{abstract}


%%%%%%%%% BODY TEXT
\begin{figure}[t]
\centering
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{images/short_overview.pdf}
\caption{}
\label{fig:short_overview}
\end{subfigure}
\centering
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{images/yellow_surfboard.pdf}
\caption{}
\label{fig:teaser_interpert}
\end{subfigure}

\caption{(a) We propose a multitask learning model for Vision-Language tasks. Our model projects images and words into a shared representation space. The resulting visual and textual embeddings are then used for each task. We demonstrate the general concept on visual recognition and VQA; (b) We pose VQA in terms of the visual recognition task resulting in a model with interpretable intermediate outputs like region relevance, and object and attribute predictions.}
\end{figure}

\section{Introduction}

%{ \todo{Derek: Suggested replacement for abstract/intro.  Possibly state contributions after related work.}
%Generalizable vision systems need to learn internal representations that serve as building blocks for multiple tasks.  In this paper, we propose to jointly learn a vision-language representation that serves as an open-vocabulary recognizer and as components of a visual question answerer.  Specifically, we force the recognizer and question answerer to be formulated in terms of the inner products of learned word representations and learned region representations. Compared to the more standard approach of sharing only visual features, the shared vision-language model leads to better transfer learning and more interpretable results for question answering.  Our experiments further investigate the effectiveness of cross-task learning as a function of rarity of question/answers and object/attribute examples.
%}

    Prediction models require meaningful internal representations
    to make accurate inferences. In computer vision, these internal representations can be in the form of learned filter response maps (CNNs), embedding spaces, and bag of words to name a few. Ultimately, we want systems that can solve a wide range of vision tasks, but building such a system would be impractical if we were to train a new independent model for every task. If a set of tasks are similar, a reasonable approach would be to share an internal representation across their respective models. In the following work, we demonstrate such a sharing across the models of two vision-language tasks and further show that the multi-task supervision can be mutually beneficial.
    
    

Consider designing a vision system that learns and performs many tasks.  Along with the performance on already-learned tasks, it is also important to build a core intermediate representation that serves as a foundation for learning new tasks.  As more data and tasks are introduced to the system, the core representation becomes more detailed and accurate, improving the efficiency of future learning.  In recent work (e.g.,~\cite{li2016learning,lu2016hierarchical,jabri2016revisiting,fukui2016multimodal,shih2016look,ilievski2016focused}), the hidden or output layers of deep networks learned from pre-training or multitask learning serve as the foundation for learning new tasks.  Such intermediate representations often improve learning efficiency (higher accuracy with less data), but they tend to be used as a black box representations and their relation to each new task needs to be learned.


%Consider the problem of designing an integrated system for learning to solve multiple visual semantic tasks such as: (i) recognizing objects and their attributes and affordance in an image, (ii) understanding the spatial relations between objects, (iii) identifying human activities, and (iv) answering complex natural language queries about the image. Any system capable of learning multiple such tasks would need to learn representations of image regions and words that are interpretable and can be flexibly reused for inference in each task. A system that can learn such representations jointly would collectively have access to significantly more data than is available for any individual task. Enforcing the representations to be interpretable and reusable across tasks would also regularize the model and enhance generalization to novel examples and tasks. 

We propose to learn an interpretable and reusable shared vision-language representation while learning tasks that involve relating vision to language.  Many such tasks exist, such as captioning, image search, and part localization; we focus on object and attribute recognition and visual question answering (VQA). We show that learning a mapping between regions and words leads to more effective inductive transfer than learning shared visual features.   

Our primary contribution is design and investigation of a vision-language representation that serves an open-vocabulary recognizer as well as a visual question answerer.  Specifically, we force the recognizer and question answerer to be formulated in terms of the inner products of learned word and region representations. This allows each task to provide a consistent training signal for learning the representations and ensure that the vision-language mapping is appropriate for all the tasks.  We show that the language representation learned for our visual tasks differs from the original word2vec~\cite{word2vec} representation learned from text.
%We propose to map image regions and textual labels to a common space and use these visual and textual embeddings for each of the tasks. The mapping functions are learned jointly with other tasks. By using shared representations our method allows for inductive transfer between tasks while training since each task contributes to enriching this representation space. %We use visual recognition and VQA on Antol \textit{et al.} \cite{antol2015vqa} to demonstrate our framewor

Our work is motivated by the challenge of incorporating trained recognizers in a high level task like VQA. Recognition could be posed as classification and responses mapped to words, but this tokenized representation cannot represent new words.  The recognition network hidden or output layers could be used as features, but the original meaning of these features is then obscured and needs to be relearned for the new task.  By posing recognition as a mapping between words and regions in feature space, we create a representation that is easily extensible to new words and maintains semantic structure. In VQA, a region is deemed relevant if it matches the attributes and nouns mentioned in the question and/or answer.  The answer is scored based on how well regions match mentioned words, content of the regions represented by 2000 objects and attributes scores produced by the learned recognizer, and the language representation of the question/answer. Our experiments show that our VQA system consistently identifies relevant regions, even when it can't produce the correct answer.  Our experiments also investigate the inductive transfer effects between VQA and recognition as a function of word rarity in each task.



%The second contribution of this work is to pose VQA in terms of the visual recognition task. Visual recognition is integrated into the VQA model in two ways: (i) the object and attribute labels in the question and answer are grounded in the image to select the relevant regions, and (ii) object and attribute category scores are used as image representation while scoring the question-image-answer. Such an approach allows us to interpret the inference by producing interpretable attention maps with predicted visual categories from each region. In addition, the model can be jointly supervised with both VQA and visual recognition data.


%In particular, we show that by carefully framing each task in terms of the mapping between words and regions, the strongly supervised recognition and loosely supervised VQA tasks build a common representation that leads to better performance in VQA than when a learned language representation is not shared among the tasks.  

%framework for learning multiple tasks using a shared vision-language representations. Specifically, our model is jointly trained to perform visual recognition and answer visual questions. Inner product between region and word representations corresponding to visual categories are used as visual classification scores. Training for visual recognition directly supervises the shared representation space. In addition, by reusing inner products as components of the visual question answering (VQA) model, training for VQA provides weak supervision for learning the shared representation space. 

%Our framework supports multitasks

%Many tasks in AI such as visual recognition, VQA, caption based image retrieval etc. have a common problem structure. All these tasks require: (i) visual recognition to understand image content, (ii) sentence parsing to understand the natural language text, and (iii) a semantic mapping across modalities. However, current methods solve each of these tasks independently and thereby fail to tap into collective knowledge that could be learned from heterogeneous data while solving multiple tasks. 


\begin{figure*}[t]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.6\linewidth}{0pt}}
\includegraphics[width=0.8\linewidth]{images/feature_extraction.pdf}
\vspace{-0.9cm}
\end{center}
   \caption{This figure depicts the construction of vision-language representations that are interpretable and reusable across tasks. The image region is mapped to representations $f_o$ and $f_a$ which encode visual features for objects and attributes. Words are also mapped from word2vec vectors to a comparable language space denoted by mapping $g$. In this framework visual recognition is formulated in terms of inner product of image and word representations. The inner product is also a reusable component for relating an image region to the question or answer (Sec.~\ref{sec:vqa}) in VQA.}
\label{fig:features}
\end{figure*}


\section{Related Work}
\noindent

%\todo{Derek: I think the main sections of related work should be: (1) Multitask/never-ending learning, see Zhizhong's related work; (2) Vision-language embeddings; (3) Visual question answering; (4) recognition. For (1), we differ in trying to create an interpretable shared representation that can serve as components of more complex tasks, rather than simply sharing features.  This enables more direct transfer between tasks, leading to better performance.  For (2), vision-language embeddings are commonly used; our twist is that s  For (3), we make formulate region relevance and answer scoring directly in terms of word-region products, providing more accurate and interpretable results.
%Use a uniform format for bibtex labels, e.g., <author><year><conference>.  This helps prevent duplicate labels and see what is cited.  You should be building up a single bib that you use for all papers.} 

\noindent \textbf{Never-ending learning:} The goals of our work are in line with those of never-ending \cite{mitchell2010never,carlson2010toward,thrun1998lifelong,silver2013lifelong,chen2013iccv} and multitask learning \cite{caruana1998springer}. Most of these works focus on building a representation within a single modality, such as vision, either accumulating classifiers or shared features that can be applied broadly.  We learn a mapping between vision and language representations which enables new vision-language tasks to easily build on and contribute to the shared representation.  

%In never ending learning, the goal is to have a model that accumulates knowledge over time and uses learned knowledge as the foundation for learning newer knowledge. Works such as Mitchell \textit{et al.}~\cite{carlson2010toward} and Chen \textit{et al.}~\cite{chen2013iccv} attempt to construct never ending learning models for learning textual relationships and image relationships respectively. Caruana \textit{et al.} \cite{caruana1998springer} explores multitask learning and observes that backprop nets benefit from receiving supervisory signals from multiple related tasks. Our work also attempts to demonstrate performance improvements due to joint supervisory signals from related vision-language tasks. Our modular representation is conducive to learning additional tasks on top of our learned representation, ultimately creating a usable knowledge foundation for lifelong learning frameworks.

\noindent \textbf{Recognition with Vision-language embeddings:} While traditional visual classification tasks require mapping visual content to a set of mutually exclusive class labels, directly applying these models with natural language is difficult due to the sheer size of a language's vocabulary. Vision-language embeddings have been used to simplify the mapping from vision to language in a variety of tasks such as image captioning and retrieval \cite{lin2014microsoft,hodosh2013framing}, phrase localization \cite{plummer2015flickr30k,krishna2016visual}, referring 
expressions \cite{KazemzadehOrdonezMattenBergEMNLP14,Mao2016cvpr}, and visual question answering (VQA) \cite{antol2015vqa,ren2015nips,yu2015visual}. The embedding approach (e.g. word2vec~\cite{mikolov2013efficient}) ameliorates the vocabulary size problem by assuming the language can be modeled in a continuous space where similar words have similar representations. Therefore by transitivity, visual features that map to a word in this space also lie close to other similar words. 

Our recognition model is related to previous open-vocabulary recognition/localization models \cite{wang2016learning,rohrbach2016grounding,gong2014improving}, which learns to map visual CNN features (e.g. from VGG~\cite{simonyan2014very}, ResNet~\cite{he2015deep} and AlexNet~\cite{krizhevsky2012imagenet}) to continuous word vector representations. But there are two main differences. First, instead of trying to create language embeddings that summarily accommodate words and phrases, we learn single-word representations and construct more complex phrase representations by averaging constituent word representations. Second, we extend these models by incorporating them as a word-region matching component within a larger VQA model, allowing them to simultaneously receive additional training signals from the VQA task. Fang \emph{et al.} \cite{fang2015cvpr} also consider learning object and attribute classifiers from pairs of image and ungrounded natural language using a multiple instance learning (MIL) framework. Their visual classifiers however do not use vision-language embeddings. While the classifier weights can be interpreted as language embeddings, they are not reused as word representations during caption generation. We note that it is trivial to incorporate their noisy-OR MIL loss in our framework and could yield performance improvements.


%The proposed
%models for these tasks often involve learning a joint embedding for image and language features to identify image regions for responding to queries, generating/retrieving captions, or localizing descriptive phrases. In the following work, we create a single-word to image region embedding as opposed to mapping phrases or entire questions to image regions. This allows us to have a single embedding model that can be used for multiple tasks, as opposed to a multi-word question to region mapping which may only be applicable to VQA.

\noindent \textbf{VQA:} Visual Question Answering (VQA) involves responding to a natural language query about an image. Our work uses VQA as a high-level vision-language task which is jointly trained alongside a lower level recognition task. Our work is closely related to attention-based VQA models \cite{fukui2016multimodal,ilievski2016focused,lu2016hierarchical,xu2016ask,shih2016look,yang2015stacked,andreas2016neural,andreas2016learning, kumar2015ask,tommasi2016bmvc} which attempt to compute a distribution (region relevance) over the regions/pixels in an image with a vision-language embedding. Relevance is usually computed as an inner product between image regions and a vector representation of the query \cite{xu2016ask, shih2016look, ilievski2016focused,lu2016hierarchical}. Region relevance is used as weights to pool relevant visual information which is usually combined with the language representation to create a multimodal representation. Finally, this representation is passed through a classifier that produces an answer score. Various methods of pooling such as elementwise-addition, multiplication, and outer-products have been explored \cite{yang2015stacked, fukui2016multimodal}. Attention models are themselves an active area of research with applications in visual recognition \cite{mnih2014nips,jaderberg2015nips}, object localization and caption generation \cite{johnson2015arxiv}, question answering \cite{weston2014memory,sukhbaatar2015end,kumar2015ask}. machine comprehension \cite{hermann2015teaching} and translation \cite{bahdanau2014arxiv,wu2016arxiv}, neural turing machines \cite{graves2014arxiv}.
  
  Our model explicitly formulates attention in VQA as a word-region matching task and trains it jointly with separate recognition supervision. A related approach is seen in Ilievski \textit{et al.} \cite{ilievski2016focused} where individual words in the question are explicitly mapped to the labels of a pre-trained object detector within a certain word2vec distance \cite{mikolov2013efficient}. The detector then generates the attention map by identifying regions for the matched labels. Tommasi \textit{et al.} similarly use a pre-trainined CCA \cite{gong2014improving} vision-language embedding model to localize noun phrases, then extracts features from specialized scene, attribute, and object networks to answer VQA questions. In contrast, we allow the attention task to be jointly trained by sharing the embedding model, allowing it to expand beyond the scope of the original detection/recognition task labels. Andreas \textit{et al.} \cite{andreas2016neural,andreas2016learning} define a set of parametrized neural modules, each with a specific function such as localizing a specific word or checking relative locations such as above and below. These modules are dynamically arranged into a feed-forward model based on a syntactic parse of the question. Like ours, their model features word-region recognizers that are trained in a latent fashion in the VQA framework, however we incorporate additional supervision from a separate classification task.



%\textbf{Baseline Models} Many baseline models have been proposed recently that model VQA as a standard pattern recognition problem. Zhou \textit{et al.} \cite{zhou2015simple} concatenate image features extracted from GoogLeNet \cite{szegedy2015going} with bag of words representation of the question and produce a distribution over 5216 most frequent answers in the VQA dataset \cite{antol2015vqa}. \cite{jabri2016revisiting} is a similar baseline that concatenates Resnet-101 \cite{he2015deep} image features with bag of words representation of the question and answer, and scores it using a multilayer perceptron. This baseline achieves answering accuracies competitive with more complicated models raising interesting questions about dataset biases and the evaluation criterion for VQA. Some other criterion for judging a model for VQA are - inference procedure that is transparent to human interpretation, compositionality, generalization of a trained model to other tasks, and capability of domain expansion on training for a new task.\\

%\noindent
%Some attempts have been made to incorporate human-like sequential inference for answering questions. Andreas \textit{et al.} \cite{andreas2016learning,andreas2016neural} define a set of parametrized neural modules each with a specific function like locating objects, predicting attributes, verifying existence etc. During inference these modules are arranged into a feed-forward directed acyclic graph based on a syntactic parse of the question. Their learning procedure is an example of \textit{learning as a model building activity} as advocated in \cite{lake2016building} where parameters of causally interacting components are learned so as to \textit{explain away} the observed data. 

%\noindent
%\textbf{Attention Mechanism} 

%\subsection*{Memory Networks}

%\subsection*{Question Answering and Text Comprehension}

%\subsection*{Multitask Learning}

\section{Method}
Our goal is to jointly learn a shared vision-language representation (Sec.~\ref{sec:shared_rep}) that can be reused for open-vocabulary visual recognition and also as building blocks of an interpretable VQA model. The inner product of image region and word representations in this space are treated as visual classification scores (Sec.~\ref{sec:recog_inference}) which are reused for predicting region relevance (Sec.~\ref{sec:vqa_inference}) for VQA and also for constructing interpretable representation of the image in terms of objects and attributes contained. The model is jointly trained, allowing the inner products to be directly supervised by recognition task (Sec.~\ref{sec:recog_learn}) and weakly supervised (Sec.~\ref{sec:vqa_learn}) through the VQA task. 

\subsection{Shared Vision-Language Representations} \label{sec:shared_rep}
We represent image regions through features that capture its objects and attributes. We use separate representations for objects and attributes because an object can be \textit{apple} and \textit{red} at the same time, but we do not necessarily want the two words to be close in the language space. The object and attribute representations are constructed by extracting the average pooled features from the 50 layer Resnet \cite{he2015deep} architecture and then applying two fully connected layers with batch normalization~\cite{batchnorm} and ReLU activations. The parameters of Resnet are fixed after pretraining on Imagenet. We construct the word representations by remapping their word2vec~\cite{word2vec} vectors using two fully connected layers with ReLU activations after the first. The size of each layer is shown in Fig.~\ref{fig:features}. Throughout the paper we use $f_o(R)$ and $f_a(R)$ to represent object and attribute representations of region $R$ and $g(w)$ for the representation of word $w$. Inner products of visual and language representations are interpreted as visual classification scores in this shared space. They are strongly supervised by a recognition task and weakly supervised by VQA.

\subsection{Visual Recognition}

\subsubsection{Inference}\label{sec:recog_inference}
The visual recognition task is to classify image regions into one or more object and attribute categories. For a region $R$ the classification score for an object category $w$ is simply given by $f_o^T(R)g(w)$. The classification score for an attribute category $v$ is given by $f_a^T(R)g(v)$. Note that while we train and test the model on a dataset with finite sets of object and attribute categories $\mathcal{O}$ and $\mathcal{T}$, our model can produce classification scores for any object or attribute provided its word2vec representation.

\subsubsection{Learning}\label{sec:recog_learn}
For visual recognition supervision, we use the Visual Genome dataset which provides image regions annotated with object and attribute labels. We select the 1000 most frequent object and attribute categories each to construct sets $\mathcal{O}$ and $\mathcal{T}$. Our classification losses are as follows.\\

\begin{figure*}
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=\linewidth]{images/system.pdf}
\vspace{-1.1cm}
\end{center}
  \caption{Illustration of inference in our VQA model. The image is first broken down into Edge Box region proposals\cite{zitnick2014edge}. Each region is represented using scores $s = [s_o, s_a]$ for visual categories as shown in Fig.~\ref{fig:features}. The regions are also assigned a relevance score using the visual classifiers corresponding to the nouns and adjectives that appear in the question and answer. The region features are then pooled using the relevance scores as weights to construct the image representation. Finally, the image and question/answer representations (also constructed using the shared representations) are jointly used to produce a score for the question-image-answer triplet.}
\label{fig:system}
\end{figure*}

\noindent 
\textbf{Object loss:} As the object classes are not completely mutually exclusive due to hypernyms and synonyms in the labels, we use a multi-label classification loss. For a region $R_j$, we denote the set of annotated object categories and their hypernyms extracted from WordNet \cite{miller1995acm} by $\mathcal{H}_j$. The object loss ensures that the true labels and their hypernyms score more than all other object labels by a margin $\eta_{obj}$. For a batch with $M$ samples $\{(R_j,\mathcal{H}j)\}_{j=1}^{M}$ the object loss is:
\begin{multline}
\mathcal{L}_{obj} = \frac{1}{M}\sum_{j=1}^{M}\frac{1}{|\mathcal{H}j|}\sum_{l \in \mathcal{H}j} \frac{1}{|\mathcal{O}|}
\sum_{k \in \mathcal{O}\setminus \mathcal{H}j} \\
\max\{0,\eta_{obj} + f_o^T(R_j)g(k) - f_o^T(R_j)g(l)\}
\end{multline}

\noindent
\textbf{Attribute Loss:} The attribute loss is a multi-label classification loss with two differences from object classification. Unlike in object classification, labels are rarely mutually exclusive. We account for this by using the independent cross entropy losses for each attribute. We also weight the samples based on fraction of positive labels in the batch to balance the positive and negative labels in the dataset. For a batch with M samples $\{(R_j,\mathcal{T}_j)\}_{j=1}^{M}$ where $\mathcal{T}_j$ is the set of attributes annotated for region $R_j$, the attribute loss is:
\begin{multline}
\mathcal{L}_{atr} = \frac{1}{M}
\sum_{j=1}^{M} 
\sum_{t \in \mathcal{T}} \\
\mathbbm{1}\left[t \in \mathcal{T}_j\right](1-\Gamma(t))\log\left[\sigma(f_a^T(R_j)g(t))\right] + \\
\mathbbm{1}\left[t \notin \mathcal{T}_j\right]\Gamma(t)\log\left[1-\sigma(f_a^T(R_j)g(t))\right]
\end{multline}    
where $\sigma$ is a sigmoid activation function and $\Gamma(t)$ is the fraction of positive samples for attribute $t$ in the batch.


\subsection{Visual Question Answering} \label{sec:vqa}

\subsubsection{Inference} \label{sec:vqa_inference}
We now describe reusing the shared vision-language representation for VQA in an interpretable fashion (see Fig.~\ref{fig:system}). Our key observation is that for most questions, a region is most relevant for scoring a question-answer pair if it contains at least one object and one attribute mentioned in the pair. To verify the answer ``\textit{Pink}" to the question ``\textit{What color is the dog?}", the only region required to confirm visual evidence is the one containing ``\textit{Pink dog}". Our approach is to score relevance using classifiers for the nouns and adjectives in the $(Q,A)$ pair. Finally, we extract and fuse visual content from the regions and score the visual evidence relative to the question and answer.\\

\noindent
\textbf{Relevance prediction:}\label{sec:relevance} A region's relevance score is the sum of max of adjective scores and max of noun scores. Let $\mathcal{R}$ be the set of Edge Boxes \cite{zitnick2014edge} region proposals extracted from the image. Let $\mathcal{N}$ and $\mathcal{J}$ denote the set of nouns and adjectives in the $(Q,A)$ pair. Each region $R\in \mathcal{R}(I)$ is assigned a relevance score $r(R)$ as follows:
\begin{align}
r'(R) &= \max_{n \in \mathcal{N}} f_o^T(R)g(n) + \max_{j \in \mathcal{J}} f_a^T(R)g(j) \\
r(R)&= \frac{\exp(r'(R))}{\sum_{R' \in \mathcal{R}(I)}\exp(r'(R'))}
\label{eqn:relevance}
\end{align}

This is in contrast to other attention models \cite{yang2015stacked,lu2016hierarchical} where there is no explicit notion of grounding of visual categories and the model is free to learn any correlation between $\mathcal{R}(I)$ and $(Q,A)$ from the dataset.\\

\noindent
\textbf{Visual content extraction and feature pooling:} As features, we first extract the scores of objects and attributes present in each region. We concatenate the object and attribute scores for each visual category in sets $\mathcal{O}$ and $\mathcal{T}$ into vectors $s_o(R_j) \in \mathbb{R}^{|\mathcal{O}|}$ and $s_a(R_j) \in \mathbb{R}^{|\mathcal{T}|}$. To construct the final $(Q,A)$ specific image representation, the features $s_o$ and $s_a$ are pooled across all regions using relevance scores as weights
\begin{equation}
f(I) = \sum_{R\in\mathcal{R}(I)}r(R)
\begin{bmatrix}
s_o(R) \\
s_a(R) \\
\end{bmatrix}
\label{eqn:weightedvisualfeats}
\end{equation}

\noindent
\textbf{Answer scoring:} We combine the visual and language representations to jointly score the $(Q,I, A)$ triplet. To construct representations $q(Q)$ and $a(A)$ for the question and answer, we follow \cite{shih2016look}, dividing question words into 4 bins and averaging word representations in each bin. The answer representation $a(A)$ is obtained by averaging the word representations of all answer words. The word representations used here are those constructed as part of the shared Vision-Language representation. %Since some answers like $\{0,1,2,3,yes,no\}$ may not be well represented using vector representations, we experiment with appending binary features for these answers in $a(A)$.

Since the magnitudes and the number of language and visual features are different we first perform batch normalization on linear transformations of these features before adding them to get a bimodal representation $\beta(Q,I,A)$
\begin{multline}\label{eq:bimodal_pool}
\beta(Q,I,A) = \;\mathcal{B}(W_1f(I)) 
+ \;\mathcal{B}\left(W_2 
\begin{bmatrix}
q(Q) \\
a(A) \\
\end{bmatrix}
\right)
\end{multline}
where $\mathcal{B}$ denotes a batch normalization operation.
\noindent
The bimodal representation is then scored with the following function: 
\begin{equation}
\mathcal{S}(Q,I,A) = W_3 \; \text{ReLU}(\beta(Q,I,A))
\end{equation}
\noindent
We pick the multiple choice option with the highest score as the final answer.

\subsubsection{Learning}\label{sec:vqa_learn}
We use the weakly supervised VQA dataset \cite{antol2015vqa} for this task. Each sample in the dataset consists of a question $Q$ about an image $I$ with list of answer options including a positive answer $A^{+}$ and $N$ negative answers $\{A^{-}(i) | i=1,\cdots, N\}$. \\

\noindent
\textbf{Answer Loss}: This enforces the condition that the correct answer $A^{+}$ should score higher than all incorrect answer options $\{A^{-}(i) | i=1,\cdots, N\}$ by a margin $\eta_{ans}$. Given batch samples $\{(Q_j,I_j,A_j)\}_{j=1}^{P}$, the loss can be written as 
\begin{multline}
\mathcal{L}_{ans} = \frac{1}{NP}\sum_{j=1}^{P}
\sum_{i=1}^{N} \max\{0,\\\;\eta_{ans} + \mathcal{S}(Q_j,I_j,A_j^{-}(i)) - \mathcal{S}(Q_j,I_j,A_j^{+})\}
\end{multline}

\subsubsection{Optional features}
To help the model learn associations between words in the $(Q,A)$ pair and visual evidence, we use four additional features for scoring $(Q,I,A)$ tuple. Similar to Eq.~\ref{eqn:weightedvisualfeats}, these features are extracted for each region and pooled using relevance scores to get $\xi(Q,I,A) \in \mathbb{R}^4$. For each region, the four features are the maximum classification scores among all question nouns, question adjectives, answer nouns, and answer adjectives. The features are They are then batch normalized after applying a linear transformation and added to $\beta(Q,I,A)$ in Eq.~\ref{eq:bimodal_pool}. Since answers such as \{\textit{yes,no,0,1,2,3}\} may not be well represented by word vectors, we also try explicitly encoding them with binary features. Our experiments show a slight boost from these features (Sec~\ref{sec:vqa_eval}).

\subsection{Implementation and Training Details}
We use 100 region proposals resized to $224 \times 224$ for all experiments. The nouns and adjectives are extracted from the $(Q,A)$  and lemmatized using the part-of-speech tagger and WordNet lemmatizer in NLTK \cite{bird2009book}. We use the Stanford Dependency Parser \cite{de2006lrec} to parse the question into bins and our models are implemented using TensorFlow \cite{tensorflow2015software}. We train the model jointly for the recognition and VQA tasks by minimizing the following loss function using Adam~\cite{adamoptimizer}:
\begin{equation}
\mathcal{L} = \alpha_{ans}\mathcal{L}_{ans} + \alpha_{obj}\mathcal{L}_{obj} + \alpha_{atr}\mathcal{L}_{atr}
\end{equation}
For the main experiments, we set ${\alpha_{ans} = 1, \alpha_{obj} = 0.1}$, and ${\alpha_{atr}=0.1}$. We observe that values of $\alpha_{obj}$ and $\alpha_{atr}$ relative to $\alpha_{ans}$ can be used to trade-off performance between visual recognition and VQA tasks. The margins used for object and answer losses are $\eta_{ans}=\eta_{obj}=1$. The object and attribute losses are computed for the same set of Visual Genome regions with a batch size of $M=200$. The answer loss is computed for a batch size of $P=50$ questions sampled from VQA. We use an exponentially decaying learning rate schedule with an initial learning rate of $10^{-3}$ and decay rate of 0.5 every 24000 iterations. Weight decay is used on all trainable variables with a coefficient of $10^{-5}$. All the variables are Xavier initialized \cite{glorot2010aistats}.
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{images/relevance_qual_wide_large_font.pdf}
\vspace{-1.1cm}
\end{center}
  \caption{Our model offers a transparent inference procedure by producing outputs of intermediate computation like region relevance, and object and attribute predictions for the most relevant region. Also our region relevance involves explicit grounding of question and answer nouns and adjectives in the image which clearly explains the choice of regions. Above we show these intermediate outputs for a few correctly answered questions. The relevant regions are visualized by multiplying the image with normalized relevance scores. The relevance scores are summed in overlapping regions.}
\label{fig:rel_qual}
\end{figure*}


\section{Experiments}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{images/failure_modes.pdf}
\vspace{-1.4cm}
\end{center}
  \caption{\textbf{Failure modes:} Our model cannot count or read, though it often identifies relevant regions. Being blind to relations the model fails to recognize that \textit{birds} while present in the image are not \textit{drinking water}. Sometimes the model gives a low score to the right answer despite accurate visual recognition. The model observes \textit{asphalt} but predicts \textit{concrete}, likely due to language bias. A clear example of error due to language bias is seen in the top-left image as it believes the lady is holding a \textit{baby} rather than a \textit{dog}, even though visual recognition confirms evidence for dog. Finally, our model fails to answer questions that require more complex reasoning involving comparison of multiple regions.}
\label{fig:fail_modes}
\end{figure*}

\subsection{Datasets}
We use the visual question and answers from VQA and bounding box annotations for object and attribute categories from Visual Genome to train our model. The train-val-test splits for the datasets are as follows. \\

\noindent
\textbf{VQA:} Fine grained analysis by question type is an important part of our experiments. We use the VQA validation set for this analysis. We split the \textit{train} set into \textit{train-subset} and \textit{train-held-out} and use the latter for model selection. The \textit{train-subset} consists of 236277 $(Q,I,A)$ samples whereas \textit{train-held-out} contains 12072 samples. The \textit{val} and \textit{test} set contain 121512 and 244302 samples respectively. Each of these sets contains exactly 3 questions per image. \\

\noindent
\textbf{Visual Genome:} We use only images from Visual Genome which do not occur in VQA (dataset overlap was computed by matching md5 hashes). The selected images were divided into \textit{train}-\textit{val}-\textit{test} using an 85-5-10 split, yielding 1565280, 90212 and 181141 annotated regions in each. We use \textit{train-held-out} for model selection.

\subsection{Interpretable inference for VQA}
As shown in Fig. \ref{fig:rel_qual}, our VQA model produces interpretable intermediate outputs such as region relevance and visual category predictions, similar to \cite{tommasi2016bmvc}. The choice of regions corresponding to \textit{orange fork}, \textit{yellow hydrant}, \textit{blue helmet} are easily explained by the associated predictions. Our model recognizes that the room is a bathroom because it saw a sink. Our model also justifies its reason to believe that the animal in the picture is not a \textit{polar bear}. Our model failure modes are similarly analyzed in Fig. \ref{fig:fail_modes}.

\subsection{Quantitative evaluation on VQA and Genome}\label{sec:vqa_eval}
\begin{table*}
    \centering 
\setlength\tabcolsep{4 pt}.

    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c|c|c||c|}
    \hline
     & 
     \rot{\parbox{3cm}{can/could/\\does/do/has}} & 
     \rot{\parbox{3cm}{what does/\\number/name}}& 
     \rot{what brand}&
     \rot{which/who}& 
     \rot{what is/are}& 
     \rot{\textbf{\parbox{3cm}{what is the\\(wo)man/person}}}& 
     \rot{\textbf{what color}}& 
     \rot{\textbf{what is in/on}}& 
     \rot{why/how}&
     \rot{how many}&
     \rot{\textbf{\parbox{3cm}{what kind/\\type/animal}}}&
     \rot{what time}& 
     \rot{what room/sport}&
     \rot{where}&
     \rot{none of the above}&
     \rot{is/are/was}&
     \rot{other} &
     \rot{number}&
     \rot{yes/no}&
     \rot{overall accuracy}\\ \hline
     VQA Only & 81.8 & 41.9 & 45.9 & 49.0 & 58.3 & 70.5 & 53.5 & 53.6 & \textbf{33.8} & 38.4 & 56.8 & \textbf{53.9} & 89.8 & 45.8 & 56.0 & 80.2 & 54.5 & 39.2 & 82.1 & 62.9 \\
     \parbox{5cm}{\centering VQA + Genome Classifiers} & 81.9 & \textbf{43.8} & 46.4 & 50.8 & 59.2 & 71.8 & 59.4 & 54.6 & 32.3 & \textbf{39.4} & 58.3 & \textbf{53.9} & 91.0 & 47.0 & 57.1 & 80.4 & 56.7 & \textbf{39.8} & 82.2 & 64.1 \\
     \parbox{5cm}{\centering VQA + Genome (Shared)} & \textbf{82.8} & 41.6 & \textbf{52.9} & \textbf{52.0} & \textbf{61.1} & \textbf{74.1} & \textbf{62.1} & \textbf{57.9} & 33.6 & 39.0 & \textbf{60.0} & 51.3 & \textbf{91.1} & \textbf{48.6} & \textbf{58.5} & \textbf{81.4} & \textbf{58.8} & 38.8 & \textbf{83.0} & \textbf{65.3} \\ \hline
    \end{tabular}}
    \caption{The table shows the effect of training for both visual QA (VQA dataset) and recognition tasks (Visual Genome) simultaneously. We also see the benefit of shared vision-language representations in comparison to the standard approach of just sharing visual features. See Sec \ref{sec:vqa_eval} for details.}
    \label{Tbl:abltionperf}
\end{table*}
\begin{table*}[t]
\scriptsize
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    Word & Word2Vec & Ours \\
    \hline
    band & guitar, rock, group, piano, performing & short, wristband, sock, headband, collar \\
    column & magazine, newspaper, book, letter, section & pillar, post, pole, tower, chimney \\
    counter & curb, stem, foil, stop, dispenser & shelf, stove, reflecting, burner, countertop \\
    horn & piano, guitar, brass, pedal, bell & tail, harness, tag, collar, paw \\
    meat & chicken, lamb, sausage, food, uncooked & rice, scrambled, piled, chip, slice \\
    monitor & watch, control, checked, alert, watching & keyboard, computer, notebook, portable, ipod \\
    \hline
    \end{tabular}
    \caption{We compare nearest neighbors (cosine distance) for a set of words using word2vec embedding as well as ours. We observe a shift from non-visual neighborhoods and meanings (monitor, control) to visual ones (monitor, keyboard).}
    \label{Tbl:embeddingexamples}
\end{table*}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{images/acc_vs_rarity.pdf}
\vspace{-1cm}
\end{center}
  \caption{The effects of jointly training VQA and recognition with and without shared language representations for question types in bold in Table.~\ref{Tbl:abltionperf}. We bin question-answer pairs based on frequency of its least frequent word. We compute each bin's accuracy (primary axis) for each model. We show the percentage reduction in error on the secondary axis. Note that familiarity with $(Q,A)$ words correlates with higher performance. Further, both forms of joint training show larger gains for more frequent question-answers (yellow and green). This is mainly because nouns and adjectives which are more common in VQA also tend to be more common in Visual Genome, resulting in more supervision for the classifiers. Finally, using word embeddings in visual classifiers imposes a structure in the shared representation space that benefits frequent and rare question-answers almost equally (blue).}
\label{fig:acc_vs_rarity}
\end{figure} 
\noindent
\textbf{Effect of joint training on VQA:} \todo{explain each row better} In rows 1 and 2 of Table \ref{Tbl:abltionperf}, we see that jointly training object and attribute classifiers and VQA yields an overall gain of 1.2\% over the model trained only on VQA. Note that row 2 is designed to be close to our joint model with shared representation (row 3) which can exploit both the VQA and Genome data. However, it differs from row 3 as the inner products with word representations is replaced with a standard classification layer for computing visual category scores which are then used as features for VQA. The comparison between rows 1 and 2 shows that our joint model is able to effectively train the visual classifiers and use the trained classifiers for achieving good performance on VQA. In row 3, we see that reusing the word representations for computing visual classification scores outperforms the more standard approach used in row 2 by achieving twice the overall gain achieved by row 2. The largest increase is observed in \textit{what color} questions which confirms the effective use of visual classifiers for improving localization in VQA. Other questions that benefit from better visual recognition are -- \textit{what is the man/woman/person}, \textit{what is in/on} and \textit{what kind/type/animal} questions. Fig.~\ref{fig:acc_vs_rarity} shows the performance as a function of how rare is the $(Q,A)$ pair in the VQA \textit{train} set for these selected question types. 

\noindent
\textbf{Effect of optional features:} Adding relevance features $\xi(Q,I,A)$ improves the performance of the \textit{VQA + Genome (Shared)} model from 65.3\% to 65.5\%. Adding the binary features for answers further improves it to 65.7\%. \\

\noindent
\textbf{Effect of pretraining:} Surprisingly, pretraining the visual classifiers on Genome prior to joint training performs worse than the model trained jointly from scratch. The pretrained model had an overall accuracy of 63.7\%, compared to 65.3\% achieved by the jointly trained model. \\

\noindent
\textbf{Comparison to State-of-the-art:} As shown in Table~\ref{tab:state_art}, our model achieves competitive results on VQA while simultaneously solving the recognition task and following a transparent inference procedure. Note that our model is trained only on the VQA train set and offers significantly improved localization performance as can be seen from the \textit{val} set performance on \textit{color} questions. \\
\subsection{Learning object classifiers through VQA}
We train our joint model using ${\alpha_{ans} = 0.1, \alpha_{obj} = 1}$, and $\alpha_{atr} = 1$ and evaluate its object recognition performance on Genome \textit{test} set. In Fig.~\ref{fig:obj_acc} we show that VQA provides weak supervision for learning visual classifiers. The object classes which are neither too rare nor too frequent in the Genome training set improve in recognition accuracy while performance on other classes is preserved. We do not expect to see improvement for classes which are already well represented in the Genome training data. Neither do we observe improvement for rare classes as they do not occur in VQA data either as seen in the secondary axis. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{images/obj_acc_vqa_mentions_large_font.pdf}
\vspace{-1cm}
\end{center}
  \caption{Effect of jointly training for VQA and visual recognition on object classification task. We plot the object classification accuracy after grouping classes into bins based on number of training samples. Joint training performs similarly or better than training for visual recognition alone in all buckets. VQA yields most benefit for object classes which occur in VQA training data but are neither too rare nor too plentiful in the recognition training data.}
\label{fig:obj_acc}
\end{figure} 

\begin{table}[]
\centering
\setlength{\tabcolsep}{2 pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{}                                    & \parbox{2.3cm}{\centering \\[0.1cm] Where To Look \\ \cite{shih2016look} \\[0.1cm]}& \parbox{0.8cm}{\centering FDA \\ \cite{ilievski2016focused}}  & \parbox{1.2cm}{\centering MLP \\ \cite{mallya2016simplevqa,jabri2016revisiting}}  & \parbox{0.8cm}{\centering MCB \\ \cite{fukui2016multimodal}}  & \parbox{2.1cm}{\centering Co-Attention \\ \cite{lu2016hierarchical}} & Ours \\ \hline
\multicolumn{2}{|c|}{Base recognition network}                                                                       & VGG         & Resnet & Resnet-152  & Resnet-152    & Resnet & Resnet-50          \\\hline
\multirow{6}{*}{\rot{\parbox{4cm}{\centering val}}} & What color (9.8\%)                                                            & 54.0          & -    & 51.9  & - & - & 63.4                      \\
                     & What is in/on (1.8\%)                                                         & 54.8          & -    & 61.7 & - & -  & 57.9                      \\
                     & \begin{tabular}[c]{@{}c@{}}What kind/\\ type/animal (23.8\%)\end{tabular}     & 52.9        & -    & 65.8  & - & -  & 60.7                      \\
                     & \begin{tabular}[c]{@{}c@{}}What is the \\ man/woman/person (2\%)\end{tabular} & 70.2        & -    & 78.0    & - & -  & 74.9                      \\
                     & \begin{tabular}[c]{@{}c@{}}Can/could/does/do/has \\ (4.6\%)\end{tabular}      & 75.7       & -    & 51.5 & -  & -  & 82.5                  \\
                     & Overall                                                                       & 58.9         & -    & 63.6  & -  & -  & 65.7 \\ \hline 
\multicolumn{2}{|c|}{test-dev}                                                                       & 62.4           & 64.0 & 65.9  & 69.9 & 65.8 & 64.7                      \\
\multicolumn{2}{|c|}{test-std}                                                                       & 63.5          & 64.2 & -  & -    & 66.1 & 64.8          \\ \hline
\multicolumn{2}{|c|}{Training data used for test}                                                                       & \textit{train+val}          & \textit{-} & \textit{train}  & \textit{train+val}    & \textit{train+val} & \textit{train}          \\\hline
\end{tabular}}

\caption{Comparison to state-of-the-art on VQA. For test accuracies, its unclear whether FDA uses \textit{val} in training. The MLP results were obtained using the implementation provided by \cite{mallya2016simplevqa}. The original MLP implementation \cite{jabri2016revisiting} using Resnet-101  yields 64.9 and 65.2 on \textit{test-dev} and \textit{test-std} respectively. MCB reports only \textit{test-dev} accuracy for the directly comparable model (final without ensemble). The percentage of different question types in the \textit{val} set are taken from \cite{shih2016look}.}
\vspace{-1 cm}
\label{tab:state_art}
\end{table}

\subsection{Learned word representations}
In Table~\ref{Tbl:embeddingexamples} we show how the word representations in our shared vision-language model differ from Word2Vec. For each word we show five nearest neighbors from both the embeddings. We observe a shift towards more visual and spatial neighborhoods. Neighbors were computed using cosine distance after mean centering. 

\section{Conclusion}
We designed an interpretable Vision-Language representation space that can be reused across multiple tasks. We show learning and application of the shared representation space through visual recognition and the VQA tasks. We also formulate the VQA task in terms of the recognition task which allows more efficient utilization of recognition supervision for VQA and weak supervision of visual recognition through VQA. As future work, we plan to extend the approach to other tasks such as activity recognition and image-caption retrieval. For VQA, we plan to model pairwise relations explicitly to answer questions involving spatial reasoning. 
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
