\section{Introduction}
Recommender models vary in depth, width, interaction types and selection of dense/sparse features. These versatile feature interactions exhibit different levels of performance in recommender systems. 
The design of interaction between dense/sparse features is the key driver to optimizing the recommender models. 
The advancement of feature interactions incorporates improved prior knowledge into the recommender systems, enhancing the underlying user-item relationships and benefits personalization benchmarks such as Click-Through Rate (CTR) prediction.
In past practices, there have been several advancements in feature interactions, such as collaborative filtering~\cite{barkan2016item2vec,wang2016structural,he2017neural,zhang2017collaborative}.
With the rise of Deep Learning (DL), factorization machines~\cite{guo2017deepfm,lian2018xdeepfm}, DotProduct~\cite{cheng2016wide,naumov2019deep}, deep crossing~\cite{wang2021dcn}, and self-attention~\cite{song2019autoint}.
The stack (e.g., DHEN~\cite{zhang2022dhen}) and the combination (e.g., Mixture of Experts~\cite{masoudnia2014mixture,balog2006formal}) create versatile interaction types with varying orders, types, and dense/sparse input sources.


\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.85\linewidth]{figs/intro_distdnas_ne.pdf}
    \caption{Model AUC versus FLOPs on Criteo Terabyte. DistDNAS unlocks 0.001 AUC compared to state-of-the-art feature interactions in recommender models.}
    \label{fig:distdnas_sota}
    \end{center}
    \vspace{-1em}
\end{figure}

The recent thriving of Automated Machine Learning (AutoML) democratized the design of feature interactions and exceeded human performance in various domains, such as feature selection~\cite{liu2020autofis} and Neural Architecture Search~\cite{song2020towards,zhang2022nasrec,krishna2021differentiable}.
Remarkably, NASRec~\cite{zhang2022nasrec} employs a supernet to represent the search space for recommender models and achieves state-of-the-art results on small-scale CTR benchmarks.
However, optimizing feature interactions through manual design/search for large-scale CTR prediction has two challenges.
First, designing/searching for the optimal feature interactions requires
extensive wall-clock time, as the design/search sequentially iterates high data volume in production to obtain a good solution in feature interaction.
This raises obstacles in ensuring the freshness of feature interaction on the latest data, thus potentially harming production quality, such as incurring model staleness.
Thus, an efficient search methodology is pressing to explore the selection of the feature interaction and scale with the growth of the data volume.
Second, fusing versatile feature interactions introduces potential conflicts and redundancy into recommender models.
For example, we observe that loss divergence issues can be directly attributed to a complex interaction module (e.g., xDeepInt) with alternative feature interaction modules (e.g., CrossNet, DLRM) when training on a large dataset.
This creates a sub-optimality of performance-efficiency trade-offs in service. 
As the relationship between model size and model performance in recommender systems has not been exploited yet, it is not certain whether fusing multiple orders and types of feature interactions into a single architecture can benefit recommender models or whether it is possible to fully harvest the improvement from different feature interactions. 

In this paper, we address the above challenges and present an efficient AutoML system, \textbf{Dist}ributed \textbf{D}ifferentiable \textbf{N}eural \textbf{A}rchitecture \textbf{S}earch (DistDNAS), to craft efficient feature interactions in a few hours. 
DistDNAS follows the setting of the supernet-based approach in NASRec~\cite{zhang2022nasrec} and applies Differentiable Neural Architecture Search ~\cite{liu2018darts} (DNAS) to learn the structure of a feature interaction. 
In DistDNAS, a feature interaction is built with multiple choice blocks. 
Each choice block represents a linear combination of feature interaction modules (e.g., Linear, CrossNet~\cite{wang2021dcn}, etc.).
DistDNAS presents two strong techniques to improve design efficiency and model efficiency during feature interaction search.
For search efficiency, DistDNAS distributes DNAS on each training day and averages the learned weighting to derive the best combinatorial choice of interaction modules.
Without ad-hoc optimizations such as embedding table sharding and communication between multiple devices, DistDNAS exhibits better scalability with a large volume of training data, achieving $\sim25\times$ speedup on 1TB Criteo Terabyte benchmark and reducing the search cost from 2 days to 2 hours.
For serving efficiency, DistDNAS calculates the cost importance of each interaction module and incorporates a differentiable cost-aware regularization loss to penalize cost-expensive interaction modules.
As cost-aware regularization loss removes unnecessary interaction modules within the supernet, DistDNAS alleviates potential conflict and harvests performance improvement. 

We evaluate DistDNAS on the 1TB Criteo Terabyte dataset using AUC, LogLoss and Normalized Entropy (NE)~\cite{he2014practical} as evaluation metrics.
Without human intervention, DistDNAS removes redundant interaction modules and discovers efficient feature interactions, unlocking 0.001 higher AUC and/or 60\% fewer FLOPs in the discovered models. The optimization in AUC and FLOPs brings state-of-the-art models discovered by DistDNAS, see Figure \ref{fig:distdnas_sota}. We summarize our contributions as follows.
\begin{itemize}[noitemsep,leftmargin=*]
    \item We analyze the challenges in search efficiency and serving efficiency when designing feature interactions on large-scale CTR recommender benchmarks.
    \item We propose DistDNAS, an AutoML system to tackle the efficiency challenge in feature interaction design. DistDNAS distributes the search over multiple data splits and averages the learned architecture on each data split for search efficiency. In addition, DistDNAS incorporates a cost-aware regularization into the search to enhance the serving efficiency of discovered feature interactions.
    \item Our empirical evaluations demonstrate that DistDNAS significantly pushes the Pareto frontier of state-of-the-art CTR models.
\end{itemize}