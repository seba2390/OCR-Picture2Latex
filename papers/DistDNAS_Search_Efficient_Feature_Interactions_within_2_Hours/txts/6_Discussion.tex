\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{figs/dist_vs_vanilla.pdf}
    \vspace{-1em}
    \caption{Comparison of learned architecture weights of dense interaction modules in choice block 1.}
    \vspace{-2em}
    \label{fig:dist_vs_vanilla}    
    \end{center}
\end{figure}

\section{Discussion}
In this section, we conduct ablation studies and analyze various confounding factors within DistDNAS, including the effect of distributed search, the effect of cost-aware regularization, and the analysis of performance on state-of-the-art models under a recurring training setting.

\subsection{DistDNAS Search Strategy}
DistDNAS proposes distributed search and cost-aware regularization as the main contribution. Upon applying DNAS to the recommender systems, we discuss the alternative choices to DistDNAS as follows.

\noindent \textbf{SuperNet} indicates a direct use of the DistDNAS supernet as a feature interaction module. No search is performed.

\noindent \textbf{Distributed DNAS} applies the DistDNAS search process in a distributed manner, but does not involve cost-aware regularization. Both distributed DNAS and DistDNAS take 2 hours to complete on NVIDIA A5000 GPUs.

\noindent \textbf{One-shot DNAS} kicks off DNAS and iteratively over the entire search data set (that is, 22 days in Criteo Terabyte) to obtain the best architecture. Despite the inefficiency of the search discussed in Section \ref{sec4:dist}, a single shot DNAS on multiple days of training data cannot converge to a standalone architecture, with loss divergence during training on a large-scale dataset. Running a one-shot DNAS takes $\sim$ 50 GPU hours on an NVIDIA A5000 GPU.

\noindent \textbf{Fresh DNAS} only covers the most recent data in the training data set (i.e. day 22 on Criteo Terabyte) and performs a search to learn the best architecture. Intuitively, this serves as a strong baseline, as the testing data set is more correlated with the most recent data due to model freshness.
We compare the learning dynamics of the architecture weights for fresh DNAS (Day-22) versus Distributed DNAS.
Thanks to the averaging mechanism, Distributed DNAS benefits from more robust learning dynamics and shows smoother progress toward the learned architecture weights, see Figure \ref{fig:dist_vs_vanilla}. 

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.95\linewidth]{figs/DistDNAS_Flops_Vis.pdf}
    \vspace{-0.5em}
    \caption{Comparison of learned architecture weights under distributed DNAS versus DistDNAS.}
    \vspace{-1em}
    \label{fig:cost_ablation}    
    \end{center}
\end{figure*}

\noindent \textbf{DistDNAS} applies all the techniques proposed in this paper, including distributed search and cost-aware regularization. Figure \ref{fig:cost_ablation} shows a comparison of the learned architecture weights in Distributed DNAS versus DistDNAS.
DistDNAS is more likely to preserve cost-efficient interaction modules, such as EmbedFC/Identity compared to distributed DNAS without cost-aware regularization.


We perform each of the aforementioned searches and evaluate different search strategies based on the following questions:
\begin{itemize}[noitemsep,leftmargin=*]
    \item \textbf{(Search Convergence) }Whether the search converges to a stable architecture weight and produces a feature interaction in discovery?
    \item \textbf{(Training Convergence)} Does the discovered feature interaction converge on a large-scale Criteo Terabyte benchmark with 24 days of training data?
    \item \textbf{(Testing Quality)} What is the quality of the interactions of the features discovered?
\end{itemize}


\begin{table}[t]
\begin{center}
    \caption{Study of different DistDNAS search strategies.}
    \vspace{-1em}
    \begin{scalebox}{0.96}{
        \begin{tabular}{|c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Strategy}} & \textbf{Searching} & \textbf{Training} & \textbf{Testing}  \\
        & \textbf{Converge?} & \textbf{Converge?} & \textbf{FLOPs/NE} \\
        \hline
        \textbf{SuperNet} & N/A & No & N/A \\
        \textbf{One-shot DNAS} & No & No & N/A \\
        \textbf{Freshness DNAS} & Yes & No & N/A \\
        \textbf{Distributed DNAS} & Yes & Yes & 3.56M/0.8460 \\
        \textbf{DistDNAS} & Yes & Yes & 3.11M/0.8444 \\
        \hline
    \end{tabular}
    }    
    \end{scalebox}
    \label{tab:distdnas_strategy}    
\end{center}
\vspace{-1em}
\end{table}


Table \ref{tab:distdnas_strategy} summarizes a study of different search strategies for these questions. We have a few findings regarding the use of distributed search and the use of cost-aware regularization.

\begin{figure}[b]
    \vspace{-1em}
    \begin{center}
    \includegraphics[width=1.0\linewidth]{figs/ablation_daily_ne.pdf}
    \vspace{-1em}
    \caption{Comparison of AUC and Relative NE on Criteo terabyte under recurring training setting.}
    \label{fig:ablation_ne_recurring}    
    \end{center}
\end{figure}

A standalone supernet cannot converge when trained on Criteo Terabyte dataset. This indicates that varying feature interactions may have conflicts with each other, increasing the difficulty of performing search and identify promising feature interactions.

On a large-scale dataset such as Criteo Terabyte, distributing DNAS over multiple-day splits and aggregating the learned weight architectures are critical to the convergence of search and training.
This is because in recommender systems, there might be an abrupt change in different user behaviors across/intra-days; thus, a standalone architecture learned on a single day may not be suitable to capture the knowledge and fit all user-item representations. Additionally, as NAS may overfit the target dataset, a standalone feature interaction searched on day $X$ may not be able to learn day $Y$ well and is likely to collapse due to changes in user behavior.

We also compare distributed DNAS with DistDNAS to demonstrate the importance of cost-aware regularization. 
Experimental evaluation demonstrates that FLOPS-regularization enhances the performance of searched feature interaction, removing the redundancy contained in the supernet. This observation provides another potential direction for recommender models to compress unnecessary characteristics and derive better recommender models, such as the usage of pruning. A more recent study using an instance-guided mask~\cite{wang2021masknet} supports the feasibility of applying model compression to advance performance on recommender models.


\subsection{Performance Analysis under Recurring Training Scenario}
Recurring training~\cite{he2014practical} is a common practice in recommender system applications. In recurring training, practitioners must regularly update the model on the latest data to gain fresh knowledge. Here, we simulate the scenario in recurring training to evaluate top-performing feature interactions (i.e., DistDNAS and DCN-v2) on different training/evaluation splits of Criteo Terabyte. More specifically, we use day 1 to day $t$ as training dates, and day $t+1$ as testing date to report AUC and relative NE. 
In Criteo Terabyte, $t$ can choose from \{2, 3, 4,..., 24\}.

We demonstrate the evaluation of recurring training on DistDNAS, DCN-v2 and DLRM (baseline) in Figure \ref{fig:ablation_ne_recurring}.
Although performing better on the final testing date, DistDNAS consistently outperforms the previous state-of-the-art DCN-v2 on all testing splits under recurring training. This indicates that DistDNAS successfully injects the implicit patterns contained within the large-scale dataset into the searched feature interaction, learning a better prior to gain knowledge from a large amount of training data.
As DistDNAS can be highly paralleled with only a single pass of the dataset, we envision that it can be beneficial for large-scale applications in search efficiency and serving efficiency.  





\iffalse
\subsection{Architecture Transferability Analysis}
\noindent \textcolor{red}{FixME}
We show the transferability of search feature interactions on three alternative small-scale CTR benchmarks:
Criteo Kaggle\footnote{\hyperlink{https://www.kaggle.com/c/criteo-display-ad-challenge}{https://www.kaggle.com/c/criteo-display-ad-challenge}}, Avazu\footnote{\hyperlink{https://www.kaggle.com/c/avazu-ctr-prediction/data}{https://www.kaggle.com/c/avazu-ctr-prediction/data}}, and KDD Cup 2012~\footnote{\hyperlink{https://www.kaggle.com/c/kddcup2012-track2/data}{https://www.kaggle.com/c/kddcup2012-track2/data}}. We demonstrate the evaluation of LogLoss and AUC of different models in Table \ref{tab:ctr_results}. 
We reuse the NASRec training pipeline~\citep{zhang2022nasrec} to ensure a fair comparison.

The three small CTR benchmarks contain different dense/sparse features with unique priors.
Thus, prior state-of-the-art performs well on certain CTR benchmarks (e.g., DCN-v2 on Criteo Kaggle), while performing less competitively when transferred to alternative CTR benchmarks. 
Yet, DistDNAS achieves state-of-the-art results on all three benchmarks without further search or fabrication, thanks to the distribution of search in the DistDNAS that aggregates the feature representation from different days of data.
With similar log loss (i.e. 0.02\% improvement over DCN-v2), DistDNAS saves $\sim$35\% FLOPs, thanks to the joint force of flexible discretization choice within feature interaction supernet and FLOP-aware regularization. \noindent \textcolor{red}{Tunhou: FIXME.}


\begin{table}[t]
    \begin{center}
    \caption{Performance of NASRec on General CTR Predictions Tasks. We use \textbf{bold} to mark the best result one each benchmark and use \underline{underline} to mark the second-best result.}
    \vspace{-0.5em}
    \scalebox{0.78}{
    \begin{tabular}{|c|cc|cc|cc|}
    \hline
     \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{|c|}{\textbf{Criteo}}  &  \multicolumn{2}{|c|}{\textbf{Avazu}} & \multicolumn{2}{|c|}{\textbf{KDD Cup 2012}}  \\
       &  LogLoss  $\downarrow$ & AUC $\uparrow$ & LogLoss $\downarrow$ & AUC $\uparrow$ & LogLoss $\downarrow$ & AUC $\uparrow$ \\
    \hline \hline
    DLRM~\cite{naumov2019deep} & 0.4406 & 0.8107  & 0.3756 & 0.7872 & 0.1494 & 0.8145 \\
    AutoInt~\cite{song2019autoint} & 0.4452 & 0.8055 & 0.3812 & 0.7778  & 0.1498 & 0.8121  \\
    DeepFM~\cite{guo2017deepfm} & 0.4403 & 0.8110 & 0.3797 & 0.7801  & 0.1494 & 0.8141   \\
    xDeepFM~\cite{lian2018xdeepfm} & 0.4408 & 0.8105  & 0.3781 & 0.7830 & 0.1495 & 0.8135    \\
    DCN-v2~\cite{wang2021dcn} & 0.4382 & 0.8132   & 0.3782 & 0.7825 & 0.1498 & 0.8119 \\
    xDeepInt~\cite{yan2023xdeepint} & 0.4393 & 0.8121 & 0.3778 & 0.7833 & 0.1489 & 0.8162  \\
    \hline
    NASRec~\cite{zhang2022nasrec} & 0.4395 & 0.8118 & 0.3748 & 0.7886 & 0.1493 & 0.8151\\
    AutoCTR~\cite{song2020towards} & 0.4397 & 0.8117 & \underline{0.3742} & \underline{0.7895} & \underline{0.1489} & \underline{0.8165} \\
    DistDNAS & \textbf{} & \textbf{} & \textbf{} & \textbf{} & \textbf{} & \textbf{}  \\
    \hline
    \end{tabular}
    \label{tab:ctr_results} 
    }
    \vspace{-2em}
    \end{center}
\end{table}
\fi