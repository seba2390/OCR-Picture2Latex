\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{figs/DistDNAS_Search_Space_v2.pdf}
    \vspace{-1em}
    \caption{Feature interaction search space for each choice block in DistDNAS. Here, a dashed line denotes a searchable feature interaction in DistDNAS, and $\otimes$ denotes the mixing of different feature interaction modules.}
    \label{fig:dnas_search_space}
    \end{center}
    \vspace{-1em}
\end{figure*}

\section{Differentiable Feature Interaction Search Space}
Supernet is a natural fusion that incorporates feature interaction modules. DistDNAS emphasizes search on feature interaction modules and simplifies the search space from NASRec~\cite{zhang2022nasrec}. 
A supernet in DistDNAS contains multiple choice blocks, with a fixed connection between these choice blocks, and fully enabled dense-to-sparse/sparse-to-dense merger within all choice blocks. 
Unlike NASRec~\cite{zhang2022nasrec} which solely selects the optimal interaction module within each choice block, DistDNAS can select an arbitrary number of interaction modules within each choice block and use differentiable bi-level optimization~\cite{liu2018darts} to determine the best selection.
This allows flexibility in fusing varying feature interactions and obtaining the best combination with enhanced search efficiency.
We present the details of feature interaction modules as follows.


\subsection{Feature Interaction Modules}
Feature interaction modules connect dense 2D inputs with sparse 3D inputs to learn useful representations on user modeling.
In a recommender system, a dense input is a 2D tensor from either raw dense features or generated by dense interaction modules, such as a fully connected layer.
A sparse input is a 3D tensor of sparse embeddings either generated by raw sparse/categorical features or by sparse interaction modules such as a self-attention layer.
We define a dense input as $X_{d} \in \mathbb{R}^{B \times dim_d}$ and a sparse input $X_{s} \in \mathbb{R}^{B \times N_s \times dim_s}$. 
Here, B denotes the batch size, $dim_d$/$dim_s$ denotes the dimension of the dense/sparse input, and $N_s$ denotes the number of inputs in the sparse input.

We collect a set of simple feature interaction modules from the existing literature, as demonstrated in Figure \ref{fig:dnas_search_space}.
A dense interaction module produces a dense output given input features, and a sparse interaction module produces a sparse output given input features.
These interaction modules can cover a set of state-of-the-art CTR models, such as DLRM~\cite{naumov2019deep}, DeepFM~\cite{wang2017deep}, xDeepInt~\cite{yan2023xdeepint}, DCN-v2~\cite{wang2021dcn}, and AutoInt~\cite{song2019autoint}.
\begin{itemize}[noitemsep,leftmargin=*]
    \item A \textbf{Identity/Identity3D} layer is a dense/sparse interaction module that carries an identity transformation on dense/sparse input. Through identity layers, one can bypass low-order interaction outputs towards deeper choice blocks and formulate a higher-order feature interaction.

    \item A \textbf{Linear/Linear3D} layer is a dense/sparse interaction module that applies on 2D/3D dense inputs, followed by a ReLU activation.
    Linear/Linear3D is the backbone of recommender systems since Wide \& Deep Learning~\cite{cheng2016wide}.     

    %if the dimension of $x_1$ and $x_2$ does not match.
    \item A \textbf{DotProduct}~\cite{cheng2016wide,naumov2019deep} layer is a dense interaction module that computes pairwise inner products of dense/sparse inputs.
    Given a dense input $X_{d} \in \mathbb{R}^{B \times dim_{d}}$ and a sparse input $X_{s} \in \mathbb{R}^{B \times N_c \times dim_{s}}$, a DotProduct first concatenates them as $X = Concat[X_d, X_s]$, then performs pairwise inner products and collects upper triangular matrices as output: $DP(X_d, X_s)=Triu(XX^{T})$.

    \item A \textbf{CrossNet}~\cite{wang2021dcn} layer is a dense interaction module with gate inputs from various sources. 
    Given a dense input $X_{d}\in \mathbb{R}^{B \times dim_{d}}$, we process the dense input and the raw dense input $X_0$ with $CrossNet(X_{d}) = sigmoid(Linear(X_d)) * X_{0}$.

    \item An \textbf{PIN}~\cite{yan2023xdeepint} layer is a dense interaction module with gate inputs from various sources. 
    Given a dense input $X_{d}\in \mathbb{R}^{B \times dim_{d}}$, xDeepInt interacts the dense input with the raw dense input $X_0$ as follows: $PIN(X_{d}) = sigmoid(Linear(X_{0})) * X_{d}$.
    % If the dimension of two dense inputs does not match, zero padding is applied on the input with a lower dimension. 
    In an xDeepInt layer, we switch the order of left/right input and perform $PIN(X_{d1}, X_{d2}) = X_{d1} * sigmoid(Linear(X_{d2}))$.

    \item A \textbf{Transformer}~\cite{vaswani2017attention} layer is a sparse interaction module that utilizes the multihead attention mechanism to learn the weighting of different sparse inputs. 
    The queries, keys, and values of a Transformer layer are identically the sparse input $X_{s} \in \mathbb{R}^{B\times N_s \times dim_{s}}$.
    Two Linear3D layers are then utilized to process sparse features in the embedding dimension, followed by addition and layer normalization.
    
    \item An \textbf{Embedded Fully-Connected (EmbedFC)} is a sparse interaction module that applies a linear operation along the middle dimension. Specifically, an EmbedFC with weights $W \in \mathbb{R}^{N_{in} \times N_{out}}$ transforms an input $X_s \in \mathbb{R}^{B \times N_{in} \times dim_{s}}$ to $Y_s \in \mathbb{R}^{B \times N_{out} \times dim_{s}}$.



    \item A \textbf{Pooling by Multihead Attention (PMA)}~\cite{lee2019set} layer is a sparse interaction module that forms attention between seed vectors and sparse features.
    This encourages permutation-invariant representations in aggregation. 
    Here, PMA applies multihead attention on a given sparse input $X_{s} \in \mathbb{R}^{B\times N_s \times dim_{s}}$ with seed vector $V \in \mathbb{R}^{1 \times N_{s} \times dim_{s}}$. PMA uses the seed vector $V$ as queries and uses sparse features as keys and values.
\end{itemize}

Within all dense/sparse interaction modules, a proper linear projection (e.g., Linear) will be applied if the dimension of inputs does not match. The feature interaction search space contains versatile dense/sparse interaction modules, providing 


\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{figs/dnas_diagram.pdf}
    \vspace{-1em}
    \caption{Overview of DistDNAS methodology. Here, dashed lines denote searchable interaction modules, and the size of interaction modules indicates the cost penalty applied to each interaction module for serving efficiency.}
    \label{fig:distributed_dnas}
    \end{center}
    \vspace{-1em}
\end{figure*}
\subsection{Differentiable Supernet}
In DistDNAS, a differentiable supernet contains $N$ choice blocks with a rich collection of feature interactions.
Each choice block employs a set of dense/sparse interactions $op^{(i)}=\{op_{d}^{(i)}, op_{s}^{(i)}\}$ to take dense/sparse inputs $X^{(i)}_{d}$/$X^{(i)}_{s}$ and learn useful representations. 
Each choice block contains $|op_{d}|=5$ dense feature interactions and $|op_{s}|=5$ sparse feature interactions. 
Each choice block receives input from previous choice blocks and produces a dense output $Y_d^{(i)}$ and a sparse output $Y_{s}^{(i)}$. 
In block-wise feature aggregation, each choice block concatenates the dense/sparse output from the previous 2/1 choice blocks as dense/sparse block input. 
For dense inputs in previous blocks, the concatenation occurs in the last feature dimension. 
For sparse inputs in previous blocks, concatenation occurs in the middle dimension to aggregate different sparse inputs.
We present the mixing of different feature interaction modules in the following context.


\noindent \textbf{Continuous Relaxation of Feature Interactions.}
Within a single choice block in the differentiable supernet, we depict the mixing of candidate feature interaction modules in Figure \ref{fig:dnas_search_space}.
We parameterize the weighting of each dense/sparse/dense-sparse interaction using architecture weights. 
In choice block $i$, we use $\alpha^{(i)}=\{\alpha_{1}^{(i)}, \alpha_{2}^{(i)}, ..., \alpha_{|op_{d}|}^{(i)}\}$ to represent the weighting of a dense interaction module and parameterize the selection of dense interaction modules with architecture weight $\mathrm{A}=\{\alpha^{(1)}, \alpha^{(2)}, ..., \alpha^{(N)}\}$.
Similarly, we parameterize the selection of sparse interaction modules with architecture weight $\mathrm{B}=\{ \beta^{(1)}, \beta^{(2)}, ..., \beta^{(N)} \}$. We employ the Gumbel Softmax~\cite{jang2016categorical} trick to allow a smoother sampling from categorical distribution on dense/sparse inputs as follows: 
\begin{equation}
Y_{d}^{(i)} = \sum_{j=1}^{|op_d|}\frac{\exp(\frac{\log \alpha_{j}^{(i)} + g_{j}}{\lambda})}{\sum_{k}^{|op_d|}\exp(\frac{\log \alpha_{k}^{(i)} + g_{k}}{\lambda})} op_{j}(X_{d}^{i},X_{d}^{i-1}),
    \label{eq:dense_block_out_dnas}
\end{equation}
\vspace{-0.5em}
\begin{equation}
    Y_{s}^{(i)} = \sum_{j=1}^{|op_s|} \frac{\exp{(\frac{\log \beta_{j}^{(i)} + g_{j}}{\lambda}})}{\sum_{k}^{|op_s|} \exp(\frac{\log {\beta_k}^{(i)} + g_{k}}{\lambda})} op_{j}(X_{s}^{i}, X_{s}^{i-1})
    \label{eq:sparse_block_out_dnas}.
\end{equation}

Here, $g_{j}$ and $g_{k}$ are sampled from the Gumbel distribution, and $\lambda$ is the temperature. As a result, a candidate architecture $\mathcal{C}$ can be represented as a tuple of dense/sparse feature interaction: $\mathcal{C}=(\mathrm{A}, \mathrm{B})$. Within DistDNAS, our goal is to perform a differentiable search and obtain the optimal architecture $C^{*}$ that contains the weighting of dense/sparse interaction modules. 

\noindent \textbf{Discretization.} Discretization converts the weighting in the optimal architecture to standalone models to serve CTR applications. 
Past DNAS practices~\cite{liu2018darts} typically select the top k modules for each choice block, brewing a building cell containing a fixed number of modules in all parts of the network.
In recommender models, we discretize each choice block by a fixed threshold $\theta$ to determine useful interaction modules. For example, in the choice block $i$, we discretize the weight $\alpha^{(i)}$ to obtain the discretized dense interaction module $\hat{\alpha}^{(i)}$ as follows:
$$
\hat{\alpha}^{(i)}_{j} =
\begin{cases}
1, if    \hat{\alpha}^{(i)}_{j} \geq \theta, \\
0, otherwise.\\
\end{cases}
$$
We typically set threshold $\theta=1/|op|$ (i.e., 0.2 for dense/sparse module search) for each choice block, or use a slightly larger value (e.g., 0.25) to remove more redundancy.
There are a few advantages of adopting threshold-based discretization in recommender models. 
First, using a threshold $\theta$ is a clearer criterion to distinguish important/unimportant interaction modules within each choice block.
Second, since a recommender model contains multiple choice blocks with different hierarchies, levels, and dense/sparse input sources, there is a need for varying numbers of dense/sparse interactions to maximize the representation capacity within each module.

