\begin{table*}[t]
    \vspace{-1em}
    \begin{center}
    \caption{Performance of the best discovered DistDNAS model on 1TB Criteo Terabyte.}
    \vspace{-1.0em}
    \scalebox{1.0}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{FLOPS(M)} & \textbf{Params(M)} & \textbf{NE (\%)} $\downarrow$ & \textbf{Relative NE (\%)} $\downarrow$ & \textbf{AUC} $\uparrow$  & \textbf{LogLoss} $\downarrow$   \\
    \hline \hline
    DLRM~\cite{naumov2019deep} & 1.79 & 453.60 & 0.8462 & 0.0 & 0.8017 & 0.12432  \\
    DeepFM~\cite{guo2017deepfm} & 1.81 & 453.64 & 0.845 & -0.14 & 0.8028 & 0.12413 \\
    xDeepFM~\cite{lian2018xdeepfm} & 6.03 & 454.14 & 0.846 & -0.02 & 0.8023 & 0.12429 \\
    AutoInt~\cite{song2019autoint} & 2.16 & 453.80 & 0.8455 & -0.08 & 0.8024 & 0.12421 \\
    DCN-v2~\cite{wang2021dcn} & 8.08 & 459.91 & 0.845 & -0.14 & 0.8031 & 0.12413  \\
    xDeepInt~\cite{yan2023xdeepint} & 8.08 & 459.91 & 0.8455 & -0.08 & 0.8027 & 0.12421 \\
    \hline
    NASRec-tiny~\cite{zhang2022nasrec} & 0.57 & 452.47 & 0.8463 & 0.01 &  0.8014 & 0.12437 \\
    AutoCTR-tiny~\cite{song2020towards} & 1.02 & 452.78 & 0.8460 & -0.02 & 0.8017 & 0.12429 \\
    \multirow{2}{*}{DistDNAS} & 1.97 & 453.62 & \textbf{0.8448} & \textbf{-0.17} & 0.8030 & \textbf{0.12410} \\
    & $3.11^{*}$ & 454.70 & \textbf{0.8444} & \textbf{-0.21} & \textbf{0.8032} & \textbf{0.12405} \\
    DistDNAS (M=2) & 3.94 & 455.48 & \textbf{0.8448} & \textbf{-0.17} & \textbf{0.8033} & \textbf{0.12410} \\
    DistDNAS (M=3) & 5.90 & 457.31 & \textbf{0.8440} & \textbf{-0.26} & \textbf{0.8035} & \textbf{0.12399} \\
    DistDNAS (M=4) & 7.87 & 459.14 & \textbf{0.8438} & \textbf{-0.29} & \textbf{0.8039} & \textbf{0.12395} \\
    \hline
    \end{tabular}
    \label{tab:criteo_terabyte_tab} 
    }
    \end{center}
    \vspace{-1em}
\end{table*}
\section{Experiments}
We thoroughly examine DistDNAS on Criteo Terabyte.
We first introduce the experiment settings of DistDNAS that produce efficient feature interaction in discovery.
Then, we compare the performance of models crafted by DistDNAS versus a series of metrics with strong hand-crafted baselines and AutoML baselines.

% We further evaluate the best architectures found on Criteo Terabyte on small-scale CTR benchmarks, Criteo Kaggle\footnote{\hyperlink{https://www.kaggle.com/c/criteo-display-ad-challenge}{https://www.kaggle.com/c/criteo-display-ad-challenge}}, Avazu\footnote{\hyperlink{https://www.kaggle.com/c/avazu-ctr-prediction/data}{https://www.kaggle.com/c/avazu-ctr-prediction/data}}, and KDD Cup 2012~\footnote{\hyperlink{https://www.kaggle.com/c/kddcup2012-track2/data}{https://www.kaggle.com/c/kddcup2012-track2/data}} to demonstrate the transferability of our searched models.

\subsection{Experiment Setup} 
We illustrate the key components of our experiment setup and elaborate on the detailed configuration.

\noindent \textbf{Training Dataset.}
Criteo Terabyte contains 4B training data on 24 different days. Each data item contains 13 integer features and 26 categorical features.
Each day of data on Criteo Terabyte contains $\sim$ 0.2B data.
During the DistDNAS search, we use data from day 1 to day 22 to learn architecture the optimal architecture weights: $C^{*}=(A^{*}, B^{*})$. During the evaluation, we use the data from day 1 to day 23 as a training dataset and use \textbf{day 24} as a holdout testing dataset. We perform inter-day data shuffling during training, yet iterate over data from day 1 to day 23 in a sequential order.

\noindent \textbf{Data Preprocessing.} We do not apply any special preprocessing to dense features except for normalization. For sparse embedding tables, we cap the maximum embedding table size to 5M, and use an embedding dimension of 16 to obtain each sparse feature. Thus, each model contains $\sim$450M parameters in the embedding table.

\noindent \textbf{Optimization.} 
We train all models from scratch without inheriting knowledge from other sources, such as pre-trained models or knowledge distillation.
We use different optimizers for sparse embedding parameters and dense parameters (e.g., other parameters except for sparse embedding parameters). For sparse parameters, we utilize Adagrad with a learning rate of 0.04. For dense parameters, we use Adam with a learning rate of 0.001. 
No weight decay is performed.
During training, we use a fixed batch size of 8192, with a fixed learning rate schedule after initial warm-up. We enable Auto-Mixed Precision (AMP) to speed up training.

\noindent \textbf{Architecture Search.}
Our supernet contains $N=7$ choice blocks during the search.
We choose $\gamma$=0.004 for cost-aware regularization to balance trade-offs between performance.
During the search, we linearly warm up the learning rate from 0 to maximum with 10K warm-up steps, and use a batch size of 8K to learn the architecture weights while optimizing the DNAS supernet.
Each search takes $\sim$ 2 GPU hours on an NVIDIA A5000 GPU.

\noindent \textbf{Discretization.}
During discretization, we only attempt on 0.25/0.2, and select the discovered feature interaction with a better performance/FLOPs trade-off as the product of the search. 
As most baseline models are larger, we naively stack $M$ copies of feature interactions in parallel to match the FLOPs of large models, such as DCN-v2~\cite{wang2021dcn} and xDeepInt~\cite{yan2023xdeepint}. In Table \ref{tab:criteo_terabyte_tab}, we use $\theta$=0.20 as the discretization threshold for DistDNAS marked with $*$, and use $\theta=0.25$ to other feature interactions created by DistDNAS. We use the discovered feature interaction to connect raw dense/sparse inputs and craft a recommender model as the product of search. 


\iffalse
\noindent  \textbf{Small-scale CTR tasks.} We use Criteo, Avazu, and KDD Cup 2012 as a set of small-scale CTR tasks.
We follow the data processing protocol in NASRec~\cite{zhang2022nasrec}. Specifically, we use a split of 80\% training data, 10\% validation data, and 10\% test data for each benchmark. We use Stratified K-fold to obtain the dataset splits, ensuring a balanced distribution of positive/negative labels. We use training and validation split to train the best discovered model, and report LogLoss/AUC on the test split.
\fi


\noindent  \textbf{Training.} To ensure a fair comparison and better demonstrate the strength of the discovered models, we employ the aforementioned optimization methodologies without hyperparameter tuning.
We linearly warm up the learning rate from 0 to maximum using the first 2 days of training data.
To prevent overfitting, we use single-pass training and iterate the whole training dataset only once.

\noindent \textbf{Baselines.} We select the popular hand-crafted design choice of CTR models from the existing literature to serve as baselines, i.e., DLRM~\cite{naumov2019deep}, DeepFM~\cite{guo2017deepfm}, xDeepFM~\cite{lian2018xdeepfm}, AutoInt~\cite{song2019autoint}, DCN-v2~\cite{wang2021dcn} and xDeepInt~\cite{yan2023xdeepint}. 
We also incorporate the best models from the NAS literature: AutoCTR~\cite{song2020towards} and NASRec~\cite{zhang2022nasrec} to serve as baselines and use the best model discovered for Criteo Kaggle.

Without further specification, 
all hand-crafted or AutoML baselines use $dim_{s}=16$ as the embedding dimension.
All hand-crafted or AutoML baselines use 512 or $dim_{d}=256$ units in the MLP layer, including 1 MLP layer in dense feature processing and 7 MLP layer in aggregating high-level dense/sparse features.
All AutoML models use $N_{s}=16$ for sparse interaction modules.
This ensures a fair comparison between hand-crafted and AutoML models, as the widest part in hand-crafted/AutoML models does not exceed 512.
All hand-crafted feature interactions (e.g., CrossNet) are stacked 7 times to match $N=7$ blocks in the AutoML supernet, as NASRec, AutoCTR, and proposed DistDNAS employ $N=7$ blocks for feature interaction. As a result, the FLOPs cost of AutoCTR~\cite{song2020towards} and NASRec~\cite{zhang2022nasrec} reduces significantly. We name the derived NAS baselines \textbf{NASRec (tiny)} and \textbf{AutoCTR (tiny)}. We implement all of the baseline feature interactions based on open-source code and/or paper demonstration. 

\subsection{Evaluation on Criteo Terabyte}
We use DistDNAS to represent the performance of the best models discovered by DistDNAS and compare performance against a series of cost metrics such as FLOPs and parameters. 
We use AUC, Normalized Entropy (NE)~\cite{he2014practical}, and LogLoss as evaluation metrics to measure model performance.
We also calculate the testing NE of each model relative to DLRM and demonstrate relative performance. Note that relative NE is equivalent to relative LogLoss on the same testing day of data. 
Table \ref{tab:criteo_terabyte_tab} summarizes our evaluation of DistDNAS.
Here, $M$ indicates the number of parallel stackings we apply on DistDNAS to match the FLOPs of baseline models.

Upon transferring to large datasets, previous AutoML models~\cite{song2020towards,zhang2022nasrec} searched on smaller datasets are less competitive when applied to large-scale Criteo Terabyte. This is due to sub-optimal architecture transferability from the source dataset (i.e., Criteo Kaggle) to the target dataset (i.e., Criteo Terabyte). Among all baseline models, DCN-v2 achieves state-of-the-art performance on Criteo Terabyte with the lowest LogLoss/NE and highest AUC. Despite the same parameter count and FLOPs, xDeepInt shows 0.06\% NE degradation, due to the sub-optimal interaction design compared to the cross-net interaction modules.

DistDNAS shows remarkable model efficiency by establishing a new Pareto frontier on AUC/NE versus FLOPs.
With a discretization threshold of 0.25, the 1.97M DistDNAS model outperforms tiny baseline models such as DLRM and DeepFM, unlocking at least 0.02\% AUC/NE with on-par FLOPs complexity.
With a discretization threshold of 0.2, we achieve better AUC/NE as state-of-the-art DCN-v2 models, yet with a reduction of over 60\% FLOPS.
By naively stacking more blocks in parallel, DistDNAS achieves the state-of-the-art AUC/NE and outperforms DCN-v2 by 0.001 AUC.

