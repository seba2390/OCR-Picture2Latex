\section{Conclusion}
In this article, we emphasize search efficiency and serving efficiency in the design of feature interactions through a differentiable supernet. 
We propose DistDNAS to explore the differentiable supernet containing various dense and sparse interaction modules.
We distribute the search on different days of training data to advance search scalability, reducing end-to-end search cost from 2 days to 2 hours with a 25$\times$ speed-up in scalability.
In addition, DistDNAS incorporates cost-aware regularization to remove potential conflicts and redundancies within feature interaction modules, yielding higher serving efficiency in searched architectures.
Our empirical evaluation justifies the search efficiency of DistDNAS on Criteo Terabyte dataset, reducing search cost from 2 days to 2 hours.
Despite search efficiency, DistDNAS discovers an efficient feature interaction to benefit serving, advancing AUC by 0.001 and surpassing existing hand-crafted/AutoML interaction designs.