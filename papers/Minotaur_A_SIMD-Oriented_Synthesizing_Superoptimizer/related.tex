\section{Related Work}

A \emph{superoptimizer} is a program optimizer that meaningfully
relies on search to generate better code, in contrast with traditional
compilers that attempt a fixed (but perhaps very large) sequence of
transformations.
%
The eponymous superoptimizer~\cite{massalin} exhaustively generated
machine instruction sequences, using various strategies to prune the
search space, and using testing to weed out infeasible candidates.
%
Also predating modern solver-based methods, Davidson and Fraser~\cite{peep84}
constructed peephole optimizations from machine description files.
%
In contrast, modern superoptimizers rely on solvers to perform
automated reasoning about program semantics.


Souper~\cite{souper} is a synthesizing superoptimizer that works on
LLVM IR; it is the most directly connected previous work to \tool{}.
%
Souper's slicing strategy is similar to \tool's in that it extracts a
DAG of LLVM instructions that overapproximates how a given SSA value
is computed.
%
However, unlike Souper, \tool{} extracts memory operations and
multiple basic blocks, so it is capable of (we believe) strictly more
transformations than Souper is able to perform.
%
Additionally, Souper's undefined behavior model does not capture all
of the subtleties of undefined behavior in LLVM, whereas we reuse
Alive2's model, which is probably the most widely used formalization
of these semantics, and is generally recognized as being correct.
%
Finally, \tool{} focuses on vector-related transformations, whereas
Souper supports neither LLVM's portable vector instruction set nor its
platform-specific intrinsics.


\tool{} is also strongly inspired by Bansal and Aiken's
work~\cite{Bansal06}; their superoptimizer operated on x86 assembly
code and was able to make interesting use of vector instructions.
%
Starting from unoptimized assembly produced by GCC, it was able to
produce code competitive with higher optimization levels.
%
The overall structure of this superoptimizer, where program fragments
are extracted, canonicalized, checked against a cache, and then
optimized in the case of a cache miss, is very similar to \tool{}, but
there are many differences in the details, particularly in \tool's
slice extractor which allows its synthesis specification to
approximate the original code's effect much more closely.
%
Another assembly superoptimizer, STOKE~\cite{stoke, stoke-fp,
  conditionally}, is not as closely related; it is based on randomly
perturbing assembly-language functions.
%
STOKE can potentially perform transformations that Minotaur cannot,
but we believe that its results are more difficult to translate into
standard peephole optimizations than are Minotaur's.


Several recent projects have focused not on optimizing individual
programs but rather on generating program rewrite rules.
%
OptGen~\cite{optgen} finds scalar peephole optimizations that meet
a specified syntactic form.
%
Even at small rewrite sizes, it was able to find numerous
optimizations that were missing from the 2015 versions of GCC and
LLVM\@.
%
VeGen~\cite{vegen} generates SLP vectorization rules---an SLP
vectorizer~\cite{slp} merges a set of scalar operations into vector
instructions.
%
VeGen parses the Intel Intrinsics Guide~\cite{intelguide} and uses this
to build pattern matchers for x86 vector instructions.
%
VeGen applies the pattern matchers to an input scalar program, and
replaces scalar expressions with vector instructions when it
finds a profitable match.
%
VeGen uses syntactic pattern matching rather than solver-based
equivalence/refinement checking.
%
Diospyros~\cite{diospyros} is another vector rewrite rule generator,
it takes an equality saturation~\cite{equalitysat} approach and uses a translation
validator to reject unsuitable candidates.
As an equality saturation-based tool, Diospyros builds its search space
with existing rewrite rules.


Program synthesis---generating implementations that conform to
a given specification---is intimately related to superoptimization.
%
Rake~\cite{rake} performs instruction selection for vectorized
Halide~\cite{halide} expressions using a two stage synthesis
algorithm.
%
First, Rake synthesizes a data-movement-free sketch~\cite{sketch}, and
then in the second stage it concretizes data movement for the
sketch via another synthesis query.
%
Rake targets Hexagon DSP processors~\cite{hexagon} which share some functionally
similar SIMD instructions with x86\@.
%
Cowan et al.~\cite{ml_syn} synthesized quantized machine learning
kernels.
%
Their work introduces two sketches: a compute sketch, which computes a matrix
multiplication, and a reduction sketch that collects the computation
result to the correct registers.
%
It relies on Rosette~\cite{rosette} to generate an efficient NEON~\cite{neon}
implementation that satisfies the specifications for those two
sketches.
%
Swizzle Inventor~\cite{swizzleinventor} is another tool built on
Rosette; it synthesizes data movement instructions for a GPU compute
kernel, and it requires user-defined sketches describing the
non-swizzle part of the program.
%
MACVETH~\cite{sparse} generates high-performance vector packings of
regular strided-access loops, by searching for a SIMD expression that
is equivalent to a gather specification.
%
All of these works show good performance results, but they focus on
relatively narrow tasks, whereas \tool{} attempts to improve SIMD
programs in general.


Most previous superoptimizers and program synthesizers use simple
cost models.
%
For example, Souper~\cite{souper} assigns each kind of instruction a
weight and uses the weighted sum as the cost of a rewrite.
%
This kind of cost model is not a very good predictor of performance
on a modern out-of-order processor.
%
\tool{} and MACVETH~\cite{sparse} use the LLVM-MCA~\cite{llvmmca}
microarchitectural performance analyzer, which can still lead to
mispredictions, but it is generally more accurate than simple
approaches are.
