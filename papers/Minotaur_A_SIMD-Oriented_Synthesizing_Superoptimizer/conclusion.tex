\section{Conclusion}
\label{sec:conc}

We created \tool{} because we noticed that LLVM appeared to be missing
relatively obvious optimizations in code containing both its portable
vector instructions and also its platform-specific intrinsic
functions that provide direct access to hardware-level primitives.
%
\tool{} slices loop-free DAGs of instructions---including branches and
memory operations---out of LLVM functions and then attempts to
synthesize better implementations for them.
%
When improved code is found, the optimization is performed and also
the synthesis result is cached.
%
On the libYUV test suite, \tool{} gives speedups up to 1.64x,
with an average speedup of 2.2\%.
%
We expect to see impact not by convincing application developers to
use \tool, but rather by convincing compiler developers to implement
useful optimizations that we can discover.




\iffalse
\paragraph{Who is supposed to use \tool{}, and how?}
%
Even with a warm cache, \tool{} is almost certainly too slow for
developers to use during their day-to-day life.
%
It could instead be used to build final software releases, which are
infrequent enough that long compile times might not be a big problem.
%
However, our guess is that the most likely users for \tool{} are
compiler developers, who would use it to discover optimizations and
then cherry-pick the best of these and add them, by hand, to the LLVM
implementation.


\paragraph{Why does \tool{} sometimes make code run slower?}
%
There are three main reasons why our
superoptimizer sometimes performs transformations that make code
slower.
%
First, its cost model takes a myopic view of the code being compiled,
looking only at the (predicted) performance of an extracted fragment.
%
It could be the case that a locally profitable transformation is
unprofitable in a larger context, for example because the optimized
code relies on a processor port that is already highly contended.
%
Second, \tool{} runs in the middle of a long sequence of LLVM
transformation passes, and we do not attempt to model or predict how it
might interact with them.
%
It seems probable that our tool sometimes makes changes that interfere
with LLVM transformations downstream in the pass pipeline.
%
For example, even small changes to the size of functions can cause the
backend to make different decisions, which can lead to performance effects.
%
Third, we fundamentally rely on LLVM-MCA to accurately assess the cost
of running a code fragment on the target microarchitecture.
%
However, LLVM-MCA is known to contain inaccuracies~\cite{ithemal}.


\paragraph{Future work}
%
Several enhancements and future projects suggest themselves:
%
\begin{itemize}
\item
We would love to synthesize floating point optimizations, but our
experiments in that direction have so far not been promising; the
solver is the bottleneck.
\item
In addition to a middle-end superoptimizer like \tool{}, LLVM should
have a backend superoptimizer that runs on, for example, its MIR
(machine intermediate representation).
\item
Instead of synthesizing $1$--$3$ instructions per optimization, we would
like to be synthesizing $5$--$10$ of them.
%
A tool that could do this would seem to be qualitatively more powerful
than a standard peephole optimizer.
%
However, the size of the search space is daunting.
\end{itemize}
%
Generating compiler optimizations by looking into large search spaces
has been a dream for more than 40 years, we look forward to pushing
this idea a little farther.
\fi

