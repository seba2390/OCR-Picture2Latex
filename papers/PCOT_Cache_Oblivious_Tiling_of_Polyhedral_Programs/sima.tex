\section{Schedule Independent Memory Allocation}
\label{sec:sima}

We also address a memory-based limitation of polyhedral compilation
tools.  It is well known that in any parallelization (of any program), it is
essential to respect (only) the \emph{true} or flow dependences.  Other
(memory-based) dependences can be ignored if one can re-allocate memory.  In
practice, this is limited by the fact that the associated memory expansion may
be prohibitively expensive, and there has been work on mitigating this
expansion~\cite{vasilache-impact12, lefebvre-feautrier-pc98, sanjay-europar96,
  sanjay-toplas00}.  We propose a novel yet simple \emph{schedule-independent}
memory allocation strategy.  Our work also generalizes polyhedral compilation
by enabling polyhedral tools to use alternate, \emph{hybrid} schedules
consisting of affine loops for certain parts of the iteration space and
cache-oblivious divide-and-conquer schedules for others.


\subsection{Background}

In this section, we introduce the necessary background of our work. We first
give a brief description of the polyhedral representation of programs, and the
general flow of a polyhedral compiler.  Then, we discuss the legality of
tiling, which is related to the input of our code generator.

\begin{figure*}[tb]
  \centering %\vspace*{6cm}
  \includegraphics[scale=0.6]{PolyCompiler}
  \caption{\small{Polyhedral Compilation: the Polyhedral Reduced Dependence
      (hyper) Graph (PRDG) serves as the intermediate representation.
      Piecewise Quasi-Affine Functions (PQAFs) describe transformations.}}
  \label{fig:compiler}
\end{figure*}

\subsubsection{Polyhedral Compilation and Representation}

Figure~\ref{fig:compiler} shows the flow of polyhedral compilation.  First,
dependence analysis of an input program (or a ``polyhedral section'' thereof)
produces an intermediate representation (IR) in the form of~\cite{DRV-sched00}
a \emph{Polyhedral Reduced Dependence (hyper) Graph} (PRDG).  Various analyses
are performed on the PRDG to choose a number of mappings in the form of
\emph{Piecewise Quasi-Affine Functions} (PQAFs) that specify the schedule as a
set of \emph{multi-dimensional} vectors.  The PQAFs come with annotations to
indicate whether each dimension is sequential or parallel, and also whether it
is part of a \emph{tilable band}, i.e., whether tiling this band of dimensions
is legal.  The transformations may be applied to the PRDG iteratively, and
(eventually) the PRDG and QLAF are provided to a code-generator that produces
code for various targets.

% specify which dimensions are sequential, which dimensions are (and
% implicitly, also the algorithm~\cite{uday-pldi08} to get tiling hyperplanes
% and tilable dimensions for each statement which is called \emph{tilable
% band}.  Then we transform the program using tiling hyperplanes.  This
% results in a program where hyper-rectangular tiles are legal and the
% wavefront parallel execution order is a legal schedule for executing tiles.
% We also provide schedule independent memory allocations for all the
% variables.  Finally we generate code which traverse the iteration space of
% the program in divide and conquer order.  by post processing the AST of
% tiled code generated by DTiler The following section presents the schedule
% independent memory allocation for affine programs.

One of the strengths of the polyhedral model is that a parametric program may
be concisely represented with a PRDG with finite number of nodes (statements)
and edges (dependences).  The potentially unbounded sets of instances of a
statement are represented in abstract forms of integer sets, called
\emph{domains}, and dependences between them as affine functions (or
relations, which are viewed as a set-valued function) over these statement
domains.  Indeed, every edge, $e$ from node $v$ to $w$, in the PRDG is
annotated with two objects: (i) a domain, $D_e$ specifying the (subset of) the
domain, $D_v$ of its source node, where the dependence occurs, and (ii) the
affine function, $f$, such that for any point $z\in D_e$, the (set of)
point(s) in $D_w$ on which it depends is given by $f(z)$.  $D_e$ is called the
context of the edge, and $f$ is its dependence function.  We also use the
notation $f(D_e)$ to denote the set valued image of $D_e$ by $f$.

An affine function $\mathbb{Z}^n \rightarrow \mathbb{Z}^m$ may be expressed as
$f(x) = A\vec{x} + \vec{b}$, where $\vec{x}$, function domain, is an integer
vector of size $n$; $A$, linear part, is an $n\times m$ matrix; and $\vec{b}$,
constant part, is an integer vector of size $m$.  A dependence is said to be
uniform if the dependence function is only a constant offset, i.e., when the
linear part $A$ is the identity.

%  is used to get the  tilable dimensions....  The parallelism that can be
%  explored using tiles is assumed to be wavefront....??  However, we modify
%  the execution order of tiles and schedule them in a divide-and-conquer
%  fashion.... Updating the memory allocation schemes becomes important when
%  we change the order of execution of tiles.... The following Section talks
%  about Memory Allocation...

\subsubsection{Legality of Tiling}

Tiling is a well-known loop transformation for partitioning computations into
smaller, atomic (all inputs to a tile can be computed before its execution),
units called tiles~\cite{irigoin-popl88, Wolf91tiling}.  The natural legality
condition is that the dependences across tiles do not create a cycle.  In
compilers, this condition is typically expressed as fully permutability (i.e.,
dependences are non-negative direction vectors), which is a sufficient
condition.  Our transformation for cache oblivious tiling takes as inputs a
loop nest that is fully permutable.  For polyhedral programs, scheduling
techniques to expose such loop nests are available~\cite{uday-pldi08}.


\subsection{Memory Allocation}

\begin{figure}[tb]
  \centering % \vspace*{4cm}
{\small\begin{lstlisting}
for (i = 0; i < N; i++){
  S0:  X[0,i] = A[i]; } // Initialize
for (t = 1; t <= 2*N; t++){ //Note: ub is even
  for (i = 1; i < N-1; i++){
    S1: X[t%2][i] = f(X[(t-1)%2][i-1],
             X[(t-1)%2][i], X[(t-1)%2][i+1],
             X[(t-1)%2][0]);
    }
  S2: X[t%2][0] = g(X[(t-1)%2][0]); 
  S3: X[t%2][N-1] = g(X[(t-1)%2][N-1]);
}
for (i = 0; i < N; i++){
  S4: Aout[i] = X[0,i]; } // Output copy
\end{lstlisting}
}
\caption{\small{Neither Pochoir nor Autogen can handle the computation
    performed by this simple loop.  Moreover, it has a memory based dependence
    that prevents polyhedral compilers like Pluto from tiling both dimensions.
    However, the true dependences of the program admit a tilable schedule, but
    at the potentical cost of $O(N^2)$ memory.  Our scheme reduces this to
    $O(N)$}}
\label{fig:motiv}
\end{figure}


In this section, we first describe how memory based dependences prevent
tiling, using our motivating example (Fig.~\ref{fig:motiv}), and show that
simply ignoring these (false) dependences would lead to memory explosion.
After formulating our problem, we next propose a simple, schedule independent
memory allocation scheme that resolves it.

Consider the statement $\mathrm{S1}$, and note that its domain, $D_1$ is the
polyhedral set, $\{t,i~|~ 1\leq t\leq 2N \wedge 1\leq i\leq N-1 \}$.
$\mathrm{S1}$ has four true dependences (for points sufficiently far from the
boundaries), three of which are $\mathrm{S1}[t-1, i-1]$, $\mathrm{S1}[t-1, i]$
and $\mathrm{S1}[t-1, i+1]$, the typical, 1D-Jacobi stencil dependences, and
the fourth one is $\mathrm{S2}[t-1, 0]$, which is a truly affine dependence on
the most recent writer to the memory location $\mathtt{X[(t-1)\%2,0]}$ when
the statement $\langle \mathrm{S1}, [t, i]\rangle$ is being executed.  All
these dependences are captured as edges with affine \emph{functions} in the
PRDG.  In addition, there is a memory based dependence, that we must also
respect.  Consider statement $\mathrm{S2}$, whose domain, $D_2 = \{t~|~ 1\leq
t\leq 2N\}$ is just one dimensional.  The $t$-th instance of S2 \emph{(over)
  writes} $\mathtt{X[t\%2, 0]}$, therefore all computations that read the
previous value must be executed before it.  In this sense, $\mathrm{S2}[t]$
``depends on'' the set $\mathrm{S1}[t,i]$, for all $1\leq i\leq N-1$.  This
dependence (which is a \emph{relation} rather than a function) is captured by
another a special edge in the PRDG.

The only schedule that respects all these dependences is the family of lines
parallel to the $t$ axis (provided all iterations of S1 are done first).
Although this has maximal parallelism, it has very poor locality.  Note that
the Pluto scheduler does not seek maximal parallelism, but rather, to maximize
the \emph{number of linearly independent tiling hyperplanes}.  Unfortunately,
the $t=\mathrm{const}$ is the only legal tiling hyperplane for this set of
dependences, and the tilable band obtained by Pluto is only 1-dimensional.

What if we did not have the memory-based dependences, i.e., what if we ignored
the memory allocation of the original program, and stored each computed value
in a distinct memory location?  In this case, there would be no memory based
dependences, and we can indeed find another family of (actually, infinitely
many) legal tiling hyperplanes: say, the lines $i+t=\mathrm{const}$.  As a
result, if we use the mapping $(t,i) \mapsto (t,i+t)$ as our ``schedule,'' the
new loops in the transformed program would be fully permutable, and could be
legally tiled.

Thus, the problem we seek to solve is: \emph{how to avoid memory based
  dependences, but without the cost of memory expansion that it seems to
  imply}.

Memory allocation for polyhedral programs is a well studied problem, and there
are two main approaches.  One either does memory allocation after the schedule is
chosen~\cite{sanjay-europar96, degreef-memory97, lefebvre-feautrier-pc98,
  sanjay-toplas00, darte-lattice05, vasilache-impact12,
  bhaskaracharya-toplas16, bhaskaracharya-popl16} since it often leads to a
smaller memory footprint, or else uses a \emph{schedule independent} memory
allocation, based on the so called \emph{universal occupancy vectors} (UOV).
This problem is solved when the program has \emph{uniform dependences}, i.e.,
when each dependence can be described by a \emph{constant vector}, and for some
simple extensions of this~\cite{strout-etal-asplos98, sanjay-memory-2011}.

It is important to note that tiling actually modifies a schedule: the so
called, ``schedule dimensions'' become fully permutable loops, and indeed,
these loops \emph{are actually permuted} in the generated tiled code.  So,
when a \emph{tiling schedule} specified by a family of $d$ tiling hyperplanes
is finally implemented by the generated code, the actual time-stamps are not
really $d$-dimensional vectors, but rather $2d$-dimensional ones obtained as
some complicated function of these indices.  Furthermore, we will see when we
generate cache-oblivious tiled codes, these tilable loops will actually be
visited in the divide-and-conquer order of execution, as required by COT.  As
a result, finding a memory map that takes into account such a rather
complicated final schedule is a tricky problem.  We therefore seek and propose
schedule-independent memory allocations.

The intuition behind our solution is (deceptively) simple, and we first
illustrate it on our motivating example (Fig.~\ref{fig:motiv}).  Rather than
the so-called ``single assignment'' program for the entire iteration space of
the program (i.e., full memory expansion), could we find lower-dimensional
subsets, such that a single assignment memory for only these subsets is
sufficient?  A careful examination of the code reveals that the memory based
dependences arise due to statement S2, and its domain is only 1-dimensional.
So we store the results of this statement into an auxiliary array, \texttt{Y},
and modify the program so that the fourth dependence simply reads
\texttt{Y[t-1]}, rather than \texttt{X[(t-1)\%2,0]}.  For the variable,
\texttt{X}, we use the old $(t,i) \mapsto (t\%2,i)$ memory allocation that was
used in the original code.  This results in $4N$ memory, which is a polynomial
degree better than quadratic.  Of course, the challenge is how to discover
this automatically.

% PLUTO schedule give a schedule and a tiling band.  A point in a tiling band
% corresponds to a particular tile.  All points within a tile execute
% sequentially.  However, the tiles can be executed in parallel.  Consider the
% modified Jacobi-1D stencil (CHANGE THIS TO BE CONSISTENT WITH RUNNING
% EXAMPLE).  The Figure \textbf{ABC} shows the iteration space of Jacobi-1D
% example (CHANGE THIS TO BE CONSISTENT WITH RUNNING EXAMPLE).  The blue bars
% show the inputs and outputs of the program corresponding to statements
% $S_{0}$ and $S_{4}$ respectively.  The memory allocation scheme used
% initially is modulo memory allocation.  However, this modulo memory
% allocation is not affine and hence PLUTO is unable to find a schedule. We
% make this memory map to not to use modulo but use previous and current
% instead. Even then, PLUTO is unable to find a schedule such that all
% dimensions are tilable.  PLUTO will decide to tile this program with single
% assignment memory.  Identity memory allocation is known to be overkill and
% lead to inefficient codes.  Therefore, we need a schedule independent memory
% allocation scheme which guarantees that the given memory allocation is both
% legal as well as optimal for any given schedule.

% The problem of schedule independent memory allocation for Polyhedral
% programs is a partially solved problem. [Strout et al] presented an
% algorithm for schedule independent memory allocation for a class of programs
% that have only uniform dependences.  A set of programs with uniform
% dependences is a proper subset of the set of programs with affine
% dependences, the class of Polyhedral programs.  Our recursive
% divide-and-conquer code generator is applicable to all Polyhedral programs
% in general, including the ones that have truly affine non-uniform
% dependences. We therefore present a novel scheme for schedule independent
% memory allocation for all affine programs.

We now outline how this is done.  At a high level, our algorithm takes a PRDG
as input, applies some (piecewise) affine transformations to it, and outputs
the transformed PRDG together with a separate memory map for each node in the
transformed PRDG.  More specifically, it works as follows.

\begin{itemize}
\item \emph{Preprocessing.}  For each edge, $e$, in the PRDG, with context
  $D_e$, and function, $f$, we first identify whether $f$ is \emph{uniform in
    context} in the sense that, for all points, $z\in D_e$, the value of
  $z-f(z)$ is a constant vector, independent of $z$.

  For example, consider a dependence function, $(i,j) \mapsto (i-1,i-1)$
  which, maps any point $[i,j]$ in the plane to a point on the diagonal, and
  is clearly not uniform.  However, what if $D_e = \{i,j~|~i=j-1\}$?  With
  this contextual information, the dependence is actually uniform: $(i, j)
  \mapsto (i-1, j-2)$.

  All edges/dependences that are neither uniform to begin with, not uniform in
  context, are marked as \emph{truly affine}.
\item \emph{Affine Split.}  For every node, $v$, in the PRDG that has at least
  one truly affine edge $e$ incident on it, we create a new node, $v'$.  Its
  domain $D_{v'}$ is the union of $f(D_e)$ of all such incident edges.

  The edges in the PRDG are modified as follows.  All the truly affine edges
  that were incident on $v$ are now made incident on $v'$; and $v'$ has a
  single outgoing edge $e'$, annotated with $\langle D_{v'}, I \rangle$ (its
  dependence function is the identity map) and whose destination is $v$.

  It is easy to see that we have not changed the program semantics.  In
  effect, we have simply copied the value of every point in $D_v$ that was the
  target of any truly affine dependence over to a new variable $v'$, and
  ``diverted'' all the truly affine edges that used to be incident on $v$ over
  to $v'$.  Moreover, since the identity function is uniform by definition, all
  edges incident on $v$ are now either uniform, or uniform in context.
\item We now use existing UOV based methods~\cite{strout-etal-asplos98,
    sanjay-memory-2011} to choose a schedule-independent memory allocation for
  all the original nodes in the PRDG, and a \emph{single-assignment} memory
  allocation for all the newly introduced variables.
\end{itemize}

The key insight into why this leads to significant memory savings, is the fact
that in all polyhedral programs that we encountered, truly affine dependences
are almost always \emph{rank deficient}, i.e., are many-to-one mappings from
the consumer index points to the producers.  The only exceptions are either
pathological programs, or programs that do multi-dimensional data
reorganizations via bijections (e.g., matrix transpose, tensor permutations,
etc.) where here is no scope nor need to reduce the total memory footprint.
As a result, $f(D_e)$ is almost always a lower dimensional polyhedron, and
requires significantly less memory, even when stored supposedly inefficiently.


\subsection{Related Work}

Memory allocation for polyhedral programs is a well studied problem for almost
two decades.  DeGreef and Cathoor~\cite{degreef-memory97} tackled the problem
of sharing the memory across multiple arrays in the program.the so called
inet-array memory reuse problem, and proposed an ILP based solution.  Wilde
and Rajopadhye, in dealing with an intrinsically memory-inefficient functional
language Alpha~\cite{mauras1989thesis} (one can think of this as a program after
full expansion) first addressed the memory reuse for points of an iteration
space~\cite{sanjay-europar96}.  They gave necessary and sufficient conditions
for the legality of a memory allocation fucntion, which they allowed to be
``in any direction.''  but they did not provide any insight into how to choose
the mapping.  Lefebvre and Feautrier~\cite{lefebvre-feautrier-pc98} on the
other hand, considered only canonic projections, combined with a modulo
factor, but showed how to choose the mapping optimally.  Later, Quiller\'e and
Rajopadhye~\cite{sanjay-toplas00} revisited multiprojections, extended them to
quasi-affine functions, and proved a tight bound on the number of dimensions
of reuse.  They also showed that cananic projections with modulo factors was
sometimes a constant factor better, and sometimes a constant factor worse.
Darte at al.~\cite{darte-lattice05} took a fresh and elegant approach to the
problem, and formulated the conditions for legal memory allocations by
defining the \emph{conflict set}.  This led to techniques for choosing
provably optimal memory allocations, initially for non-parameterized iteration
spaces, and recently in the context of FPGA acelerators, for parametrically
tiled spaces~\cite{darte2014parametric, darte2016extended}.  Vasilache et
al.~\cite{vasilache-impact12} developed a tool to combine the scheduling and
limiting memory expansion using an ILP formulation, implemented in the
R-Stream compiler.  Recently, Bhaskaracharya et
al.~\cite{bhaskaracharya-toplas16} developed methods to optimally choose
quasi-affine memory allocations, and showed how they are beneficial for tiled
codes, especialy with live-out data.  Furthermore, they also
showed~\cite{bhaskaracharya-popl16} how to combine iner-and intra array reuse
in a unifying framework.

The other \emph{schedule independent} memory allocation was pioneered by
Strout et al.~\cite{strout-etal-asplos98}.  Here, the memory allocation is
chosen based only on the dependences, and is guaranteed to be legal,
regardless of the schedule.  This problem is solved when the program has
\emph{uniform dependences}, i.e., when each dependence can be descibed by a
\emph{constant vector}, and for some simple extensions of
this~\cite{strout-etal-asplos98, sanjay-memory-2011}.

Thies at al.~\cite{thies-pldi02} have also formulated the problem of
simultaneously choosing the schedule and memory allocation as a combined
optimization problem.
% \begin{algorithm}[h]
%   \mbox{} Input : PRDG
  
%     Output : Transformed PRDG and Memory Map
%     \begin{enumerate}
%     \item Recognize truly non-uniform dependence edges
  
%       For each node $X_{i}$ with one or more incoming non-uniform affine
%       edge(s):
%       \begin{itemize}
%       \item Create new node $X_{i}^{new}$ by applying ``Affine Split"
%         transformation
    	
%         Assertion: $X_{i} = X_{i}^{old} + X_{i}^{new}$

%       \end{itemize}
%     \item Find universal occupancy vector $(UOV)$ for all nodes with uniform
%       dependences
%     \item Construct memory mapping function for all nodes except
%       $X_{i}^{new}$ using $UOV$
% 	\item Use identity memory map for $X_{i}^{new}$ nodes
% 	\item \textbf{(SANJAY verify 4 , 5)} Apply change of basis to reduce
%    number of dimensions of the domain of new node
%  \end{enumerate}
%  \caption{SIMA: Schedule Independent Memory Allocation for Polyhedral
%  Programs}
%  \label{alg:sima}
% \end{algorithm}

% Local Variables: ***
% TeX-master: "PACT17.tex" ***
% fill-column: 78 ***
% End: ***
