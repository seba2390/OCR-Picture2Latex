\section{Empirical Study}
\label{sec:experiments}
In this section, we empirically study different tiling strategies. The goal of
the experiments is to characterize SLT and COT, and to show the influence of the
factors discussed in Section~\ref{sec:motivations}. 

%We want to show that 
%\begin{itemize}
%	\item Cache oblivious tiling provide significantly smaller number of off-chip
%memory access as well as variability of number of off-chip accesses. 
%	\item Both cache oblivious tiling and single-level tiling require
%exploration of tile sizes for speed.
%\end{itemize}

%In addition, we show when cache oblivious tiling is beneficial and the cases
%for which its benefits diminish with explanations that are described in the
%Section~\ref{sec:motivations}. Finally, we show that we do not trade-off speed
%for the benefits of the cache oblivious tiling.

\subsection{Experimentation Setup}
For our experiments, we use two CPU architectures: Haswell and Broadwell. The
configuration of these machines are shown in Table~\ref{tab:machine}.  The
benchmarks are compiled with ICC 16.0.2 using the following flags: \texttt{-O3
-xHost -ipo}.  We use \texttt{perf} tool to count off-chip memory accesses
(OCA), which is the total number of LLC misses. The tool counts the number of
OCA for the entire program that consists of kernel, timing functions and reading
program inputs. The number of OCA is dominant in the kernel, therefore we can
safely use \texttt{perf} to measure OCA for the kernels.

\begin{table}
\caption{Machine Configuration
\label{tab:machine}}
\centering
\begin{tabular}{|l|r|r|r|}
\hline\hline
 Architecture Parameters &Haswell & Broadwell \\ \hline\hline
 Processor &  E3-1231 v3 & E5-1650v4 \\ \hline
 Base Frequency & 3.4 GHz & 3.6 GHz \\ \hline
 Turbo Boost Frequency & 3.8 GHz & 4 GHz \\ \hline
 Number of Cores &  4 & 6 \\ \hline
 RAM & 32 GB & 16 GB \\ \hline
 L3 Cache &  8 MB 16-way & 15 MB 20-way\\ \hline
 L2 Cache &  \multicolumn{2}{c|}{256 KB per core 8-way} \\ \hline
 L1 Cache &  \multicolumn{2}{c|}{32 KB per core 8-way} \\ \hline
% Instruction Set Extensions & \multicolumn{2}{c|}{AVX 2.0} \\ \hline
 Max Mem.Bandwidth & 25.6 GB/s & 76.8GB/s \\ \hline
 Compiler & \multicolumn{2}{c|}{icc 16.0.2} \\ \hline
\end{tabular}
\end{table}

\subsubsection{Code Generators}
We experiment with multiple state-of-the-art code generators.  We use
D-Tiling~\cite{sanjay-lcpc2009,sanjay-kim-dtilingTR-2010} implemented in
AlphaZ~\cite{yuki2013alphaz} to generate SLT code with parametric tile sizes.
We compare the performance of parametric SLT and PCOT, which are main subject
of our study. In addition, we compare speed of PCOT with
Pluto~\cite{uday-pldi08} for fixed size SLT code, Pochoir~\cite{Tang2011} for
cache oblivious Jacobi-style stencils, and Autogen~\cite{autogen-ppopp16} for
cache oblivious dynamic programming.


\subsubsection{Benchmark Suite}
We use kernels from stencils, linear algebra, and dynamic programming for our
experiments.  Specifically, we include FDTD-2D, GaussSeidel-2D (GS-2D),
Heat-2D and Heat-3D for stencils; Cholesky decomposition and LU decomposition
(LUD) for linear algebra, and Optimal String Parenthesizing (OSP) from the
dynamic programming.  
%
These kernels are selected to cover the types of kernels handled by existing
tools, as well as additional polyhedral kernels that can now be supported with
the PCOT generator.  All the benchmarks are from PolyBench/C
\footnote{Available online at
\url{https://sourceforge.net/projects/polybench/}} version 4.2 except for OSP.
We include OSP, since it was used to evaluate Autogen~\cite{autogen-ppopp16}
and we obtained OSP code from authors of Autogen.
%
All benchmarks are scheduled using the Pluto algorithm, and use the same
memory allocation as PolyBench kernels.  
%We use the same code structure to
%visit iterations in a tile in parametric SLT code, as well as to execute a
%leaf tile (base function) in PCOT code.
%%Since we use the same compiler infrastructure for both cache-oblivious and
%%single-level tiling code generation, the code for each tile is identical, and
%%Therefore, the compiler is able to apply the same set of low-level
%%optimizations for both parametric SLT and PCOT generated code.. 

Pluto algorithm is unable to find tiling hyperplanes for all three dimensions
for OSP. However, there is a well-known transformation that reorders the
summation described by Guibas et al.~\cite{guibas-kung-thompson} to transform
the dependences to be tileable.  We apply this transformation as a
pre-processing to tile all three dimensions of OSP. Autogen is able to tile all
three dimensions automatically,
making this necessary to make a fair comparison of the tile execution order.

\subsubsection{Problem Size Selection}
We select the problem size of each benchmark such that the memory footprint of
a wavefront of tiles, i.e., the set of tiles that may be executed in parallel
with single-level tiling code, does not fit in LLC. Therefore, during the
execution there will be off-chip memory accesses. We do not use problem sizes
that are  powers of 2 since they exacerbate the occurrences of conflict misses
unless padded.  Table~\ref{tab:bench} shows the chosen problem sizes for all
the benchmarks.


\subsubsection{Consistent Optimization Level Across Codegen Techniques} 
We want to ensure that the performance of each tile is similar across all
techniques to focus on the difference due to tile execution order.  Autogen
and Pochoir apply some low-level optimizations not supported by the PCOT
generator and Pluto. These optimizations were applied manually to SLT, PCOT,
and Pluto generated code.
%
%\begin{inlinelist}
These optimizations are:
\begin{itemize}
\item Copy optimization: copying transpose of a column-major input matrix
inside a base function to a local array, so that it can be accessed in unit
stride, and 
\item Write optimization: if there is an accumulation in the innermost loop,
first perform the accumulation on a local variable and then update the array
at the end.
\item Indexing Simplification: array accesses are simplified by use of pointer arithmetic
instead of accesses to multi-dimensional arrays.
 \end{itemize}
 %\end{inlinelist}
%
%These optimizations are applied manually to PCOT generated code (which is the
%case for Autogen as well). 
%These two optimizations alone provide factor of 2 speedup. Therefore,
%application of these optimizations are critical to compare performance against
%Autogen\footnote{We applied these transformations only to OSP out of our
%benchmarks}.
%
In addition we used the restrict keyword and additional compiler options to
ensure that parallel innermost loops are vectorized by ICC.

\begin{table}
\caption{Benchmarks, problem sizes and achieved performance in GFLOPs per
second for the sequential single-level tiling code on both architectures.
\label{tab:bench}}
\centering
\begin{tabular}{|l|c|S|S|}
\hline\hline
 \multirow{2}{*}{Benchmark} & \multirow{2}{*}{Problem size} &
\multicolumn{2}{c|}{{SLT-seq GFLOPS}} \\  \cline{3-4}
  &  & {Haswell} & {Broadwell} \\ \hline\hline
 Cholesky & $4000^2$ & 12.3 & 11.7 \\ \hline
 LUD & $3000^2$ & 4.1 & 4.1 \\ \hline
 FDTD-2D & $500\times 2000^2$ & 6.4 & 5.6 \\ \hline
 GS-2D & $500\times 2000^2$ & 1.9 & 2.7 \\ \hline
 Heat-2D & $500\times 3000^2$ & 20.5 & 20.4\\ \hline
 Heat-3D & $50\times 350^3$ & 16.5 & 17.6\\ \hline
 OSP\footnotemark & $4000^2$ & 21.0 & 22.8 \\ \hline
% FDTD-2D & $256\times 3072^2$ & 4.2 & 3.6 \\ \hline
% GS-2D & $512\times 3072^2$ & 1.9 & 2.7 \\ \hline
% Heat-3D & $64\times 256^3$ & 9.9 & 10.0\\ \hline
% Cholesky & $5120^2$ & 12.3 & 12.9 \\ \hline
% LTMI & $6144^2$ & 1.4 & 1.9 \\ \hline
% OSP & $6144^2$ & 26.2 GUPDATES & 23.4 GUPDATES \\ \hline

\end{tabular}
\end{table}
\footnotetext{OSP performance numbers are in GUpdateS}

\subsubsection{Tile Size Exploration} 

%
%In the rest of this section, we use the term tile sizes interchangeably with
%base case thresholds for cache-oblivious methods.
In our study, we explore the difference between SLT and COT. An
important aspect of this study is the influence of tile sizes, which have
large impact on both approaches.  We are not interested in the ``best'' tile
size in this paper, but in the characterization of the effect of tile sizes.
Therefore, our main focus is in the statistical property over a same set of
tile sizes for both tiling schemes. 
%TODO, later mention/explain that we are not too far from best or machine
%peak.
%
%if we further explore the neighbours of the good tile sizes, then each tiling
%scheme will end up with exploring different tile sizes. Then we cannot make a
%meaningful statistical comparison between two tiling strategies.    

We explore a set of tile sizes ranges from 10 to 50 in steps of 10, 100 to 500
in steps of 100 in all the dimensions, and 1000 to problem-size in steps of
1000 along innermost dimension of a tile. For Heat-3D, tile sizes 10, 30, 50,
100 and 300 were explored along all the dimensions. These ranges of tile sizes
give a relatively coarse-grained sampling of possible tile sizes that fits in
different levels of cache. Exhaustively searching the entire space of legal
tile sizes is impractical due to time limitations.  For each tile size, we use
the mean over three runs, which had low variance across runs.

We do not explore tile sizes on both Pluto and Pochoir. It is impossible to
automatically explore tile sizes for Pluto since it performs fixed sized
tiling, and we had to manually modify the output to make the performance of
tiles consistent with others. The tile sizes in Pochoir are hardwired into the
code generator, and we could not find an easy way to explore the tile sizes.
%should I remove the comment on Pochoir?

%For sequential runs on both architectures, good tile sizes (i.e. tile sizes
%which has execution time within 10\% of the best execution time) contain both
%small and larger tiles -- making small tiles to fit in L1/L2 cache and larger
%tiles to fit in LLC cache -- for both SLT and PCOT codes except for GS2D.  In
%large tiles, the tile size along the inner most dimension is large or untiled
%for the best performance. We attribute this shape of tiles to the better
%prefetching behavior to improve performance. 
%%The memory footprint of these tiles does not fit in L1/L2 but in LLC. The
%%memory footprint of rest of the sequential runs and parallel runs of the
%%benchmarks fit in either L1/L2.  


\begin{figure*}[t!]
	\centering
	\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{HW_oca_variance}
			%\includegraphics[height=1.2in]{a}
 %     \caption{Lorem ipsum}
	\end{subfigure}%
	 
	\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.988\textwidth]{BW_oca_variance}
	%    \caption{Lorem ipsum, lorem ipsum,Lorem ipsum, lorem ipsum,Lorem ipsum}
	\end{subfigure}
	\caption{Mean and standard deviation of number of OCA across the tile sizes
that fits in LLC for both Haswell and Broadwell. The figures in the left (a,c)
show the OCA behavior when restricted to tile sizes within 50 in all
dimensions and the figures in the right (b,d) show the behavior when all the
tile sizes are used. We have to focus on both relative standard deviation and
mean number of OCA. When the tile sizes are restricted to 50, we see the
expected behavior where the variability is much lower with COT. This confirms
the benefits of the recursive structure of COT as it automatically takes
advantage of multiple levels of memory hierarchy.  When the tile sizes are not
restricted, the number of OCA remains similar for COT. The mean number of OCA
for SLT decreases compared to smaller tiles since the larger tiles fits in LLC
reducing the number of OCA.  The behavior of OCA on both cpu platforms is
similar except that Haswell has relatively more OCA since its LLC size is
smaller compared to Broadwell.
%in many benchmarks, the variability with PCOT significantly increases. When
%the tile size is large, data footprint of a tile already fits in LLC cache
%reducing the benefits of divide-and-conquer. 
} 
	\label{fig:oca_var}
\end{figure*}

%\begin{figure*}[tb]
%  \centering
%    \includegraphics[width=0.98\textwidth]{figures/HW_oca_variance}
%\caption{Mean and standard deviation of number of OCA across the explored tile
%sizes for Haswell. The figure in the left (a) shows the OCA behavior when
%restricted to tile sizes within 50 The OCA behavior when tile sizes are
%restricted within 50 in all dimensions and the figure in the right side (b)
%shows the behavior when data points from all the tile sizes are used. Here we
%have to focus on both relative standard deviation and mean number of OCA. When
%the tile sizes are restricted to 50, we see the expected behavior where the
%variability is much lower with PCOT. This confirms the benefits of the
%recursive structure of COT as it automatically takes advantage of multiple
%levels of memory hierarchy. When the tile sizes are not restricted, in many
%benchmarks, the variability with PCOT significantly increases. When the tile
%size is large, it makes more effective use of prefetcher and reduces the
%benefits of divide-and-conquer. } 
%	\label{fig:hw_oca_var}
%\end{figure*}
%
%\begin{figure*}[tb]
%  \centering
%    \includegraphics[width=0.98\textwidth]{figures/BW_oca_variance}
%\caption{Mean and standard deviation of number of OCA across the explored tile
%sizes for Broadwell. The key observations that can be made from these figures
%are identical to those in Figure~\ref{fig:hw_oca_var}
%}
%	\label{fig:bw_oca_var}
%\end{figure*}

\begin{figure*}[tb]
  \centering
    \includegraphics[width=0.98\textwidth]{ex_avg_all_variance}
\caption{Mean and standard deviation of execution time on (a) Haswell and (b)
Broadwell across all explored tile sizes. Variance is high for both COT and
SLT on both machines. Therefore, tile size exploration/tuning is needed for
speed.}
	\label{fig:ex_avg_all_var}
\end{figure*}





\subsection{Characterization of SLT and COT}
%{\color{red}
%\begin{itemize}
%	\item for small tile sizes, absolute number and the variation of number of OCA
%are smaller for PCOT compared to SLT.  Except for Heat-3D
%	\item when we consider all ranges of tile sizes, absolute number and the
%variability of OCA significantly increases for some of the benchmarks
%	\item the variability of execution time is high for both PCOT and SLT,
%therefore, tile size exploration is needed for speed
%\end{itemize}
%}
Figures~\ref{fig:oca_var}, and \ref{fig:ex_avg_all_var} summarize the result
of our study in the two platforms.

We first discuss the variability of OCA with respect to tile size. COT has
much smaller number of OCA and variability compared to SLT when we restrict to
smaller tile sizes (Figures~\ref{fig:oca_var}a and~\ref{fig:oca_var}c). This
confirms the benefits of COT as it automatically takes advantage of the
multiple levels of memory hierarchy, owing to the recursive structure of COT. 

Heat-3D does not exhibit the expected behavior since COT introduces more
conflict misses for higher dimensional data as explained below.
% as explained in Section~\ref{sec:motivations}. 
%
The theoretical results of COT were derived with the assumption of fully
associative caches where a cache line can be stored at any location in the
cache. Therefore, there will not be any occurrences of conflict misses but
capacity misses. In reality, caches are n-way (n=16 and 20 for the cpus used
in our experiments) associative where there are n candidate locations in the
cache to store a cache line. This leads to conflict misses. 
%
Let us consider sequential execution of a d-dimensional stencil where all d+1
(including time dimension) dimensions are tiled.  A tile uses d of the faces
as inputs and generates d faces of outputs.  Let us assume that the tile sizes
are such that data correspond to all d output faces fit in LLC. 
SLT executes tiles in lexicographic order.  Therefore, lexicographically next
tile will read one of the input faces through LLC. But all other d-1 input
faces were produced long time ago, therefore data is no longer in LLC due to
potential capacity misses. Hence, conflict misses can potentially occur when
the input face is read through LLC and when accessing data while computing
values within the tile. COT visits the neighbouring tiles first. Therefore, a
tile reads more than one (ideally d) input faces through LLC. Hence, conflict
misses can occur when all the input faces are read and when compute values
within the tile. Since COT reads more input faces through LLC, there is a
higher chance of occurring conflict misses compared to SLT.  When the number
of dimensions are increased, the dimensions of the  memory footprint of a tile
increase as well as the number of input faces COT read through LLC. The higher
number of dimensions, the larger the distance between memory footprints of
neighbouring tiles. Therefore, the chance of occurring conflict misses is
further increased. Hence for Heat-3D, the number of conflict misses for COT is
as high as the number of capacity misses for SLT. This behaviour diminish the
benefits of COT for higher dimensional stencils.

%This is also confirmed
%experimentally by analysing the L2 misses for Heat-3D. Most of the L2 misses
%(LLC accesses) become OCA which is a symptom of conflict misses at LLC.


When data points correspond to all the tile sizes are considered
(Figures~\ref{fig:oca_var}b and ~\ref{fig:oca_var}d), the absolute number of
OCA remains similar for COT since all the tile sizes fits in LLC. For SLT, the
mean of OCA is slightly decreased compared to smaller tile sizes since the
larger tiles fit in LLC reducing the number of LLC misses. Heat3D remains an
outlier due to the conflict misses as described in previous paragraph. The
behavior of OCA we discussed so far similar for both sequential and parallel
codes except that parallel codes have more OCA due to the increased number of
conflict misses.

%When data points correspond to all the tile sizes are considered
%(Figures~\ref{fig:oca_var}b and ~\ref{fig:oca_var}d), the absolute number and the variability of
%OCA for COT are significantly increased for some of the benchmarks.
%The memory footprint of large tile sizes already fit in LLC which reduces the
%benefits of COT.  
%For parallel runs, effectiveness of the prefetcher diminishes due to
%bandwidth limitations.  

%We first discuss the variability of OCA with respect to tile size.  When the
%tile sizes are small, COT has much smaller number of OCA and variability
%compared to SLT. This confirms the benefits of COT as it automatically get
%tuned for multiple levels of memory hierarchy. Heat-3D does not exhibit the
%expected behavior since COT introduces more conflict misses for higher
%dimensional data as explained in Section~\ref{sec:motivations}. As we increase
%the tile size (Figure~\ref{fig:bw_oca_var}), the absolute number and
%variability of off-chip accesses increase for COT, specially for sequential
%runs. It can be explained by the  effective use of prefetcher as described
%under \emph{hardware prefetching} in Section~\ref{sec:motivations}. The
%functioning of prefetcher demands more bandwidth. For parallel runs,
%prefetcher become hardly effective due to memory bandwidth limitations.
%Therefore, larger tiles no longer benifit the speed. Hence, for parallel runs,
%COT has significant reduction in number of off-chip memory accesses and
%variability of OCA.

The variability of execution time is high for both COT and SLT on both
platforms. It is related to the recursion overhead and restrictive tile sizes
due to recursive split of COT as described in Section~\ref{sec:motivations}.
This shows that speed of COT is sensitive to tile sizes, therefore, both COT
and SLT need tile size exploration/tuning for speed. For sequential runs,
large tile sizes make more effective use of prefetcher with increased
bandwidth requirement as described under \emph{hardware prefetching} in
Section~\ref{sec:motivations}. We verified this behavior of prefetcher by
counting the L2 prefetcher activities. We noticed that for large tile sizes
the number of prefetcher activities are 3 order of magnitude higher compared
to small tile sizes.  But for parallel runs, the bandwidth limit is reached
faster, therefore, benefits of prefetching diminish.

%
%We first discuss the variability of OCA with respect to tile size.  The message
%from the figures is that for both sequential and parallel runs, PCOT has much
%less number of off-chip accesses and low variance across a set of tile sizes
%compared to SLT. For Heat-3D, the benefits of the COT decay because it
%introduce more conflict misses due to higher dimensional iteration space as
%explained in Section~\ref{sec:motivations}. 
%
%When the all data points are considered, more variance is observed with PCOT.
%This is because large tile sizes make more effective use of prefetcher, which
%in turn increased bandwidth requirement, as well as the number of OCA.For
%parallel runs, prefetcher is hardly effective due to memory bandwidth
%limitations. The larger tile sizes are not beneficial anymore, hence for speed
%tiles get tuned for L1/L2 caches reducing significantly the number of off-chip
%accesses   of COT compared to SLT. The corresponds to the effect of
%\emph{hardware prefetching} in Section~\ref{sec:motivations}.
%
%The variability of execution time in these figures related to the first two factors mentioned in Section~\ref{sec:motivations}. The combination of these factors make PCOT sensitive to tile sizes for speed, which is clearly shown in the figures.
%
%


\begin{figure*}[tb]
  \centering
    \includegraphics[width=0.98\textwidth]{norm_gflops}
\caption{Performance of SLT and PCOT on (a)
Haswell and (b) Broadwell, normalized to sequential parametric SLT
\texttt{SLT-seq} (higher the better). The absolute performance numbers are
shown on top of each bar in GFLOPS. Both the sequential and the parallel
performance of SLT and PCOT closely matches the performance of other code
generators. \texttt{Other} represents Pluto for the first 4 benchmarks, Pochoir
for Heat-2D and 3D, and Autogen for OSP.}
	\label{fig:gflops}
\end{figure*}




\subsection{Comparison of Speed Against State of the Art}
In the following we compare the performance of SLT and PCOT with
state-of-the-art code generators.  The performance is normalized to the GFLOPS
(giga flops per second) achieved by sequential parametric single-level tiling
(\texttt{SLT-seq}) code.  We use the best tile size found in the tile size
exploration described above. For Pluto, we used the best tile size for SLT.
The absolute performance numbers are shown on top each bar in Figure~\ref{fig:gflops}

The parallel variants are executed with the number of threads equal to the
number of physical cores (i.e., hyper-threading is not considered) on each
machine.  Figure~\ref{fig:gflops} shows the normalized performance (GFLOPS) on
both Haswell and Broadwell. The speed of SLT and PCOT closely matches the
speed of state-of-the-art.

PCOT out performs Pochoir on Heat-2D. For this benchmark, Pochoir does not
vectorize the iterations at the boundaries. Pochoir scales better (20\%) with
6 threads for Heat-3D due to the concurrent start of tiles. 
%
For OSP, PCOT is 10-15\% faster than Autogen. This difference is attributed to
the tile shapes. The Autogen generated code is restricted to cubic tile sizes
where as the PCOT generated code have more degrees of freedom, and we found
that using large tile sizes in the innermost dimension is better for
performance in many cases. When we try cubic tile sizes in PCOT, the speed is
similar to Autogen.
%
Performance of SLT, Pluto and PCOT are similar with each other.

Despite the significant reduction in off-chip memory accesses of benchmarks in
PCOT, the absolute speeds of codes from PCOT and other techniques remain
similar. For compute-bound kernels (i.e., after applying SLT), the latency of
OCA are already hidden. Therefore, further reducing OCA does not translate
into savings in execution time. 

Finally we compare the speed (GFLOPS) of the benchmarks to the peak
performance per benchmark. The compiler does clever optimizations for both
Haswell and Broadwell architectures. For example, 5-point Heat-2D stencil has
4 muls, 2 subs and 4 adds and the throughput of vector (4 doubles wide)
instructions is 2, 1 and 1 per cycle respectively \footnote{Data available at
http://software.intel.com/sites/landingpage/IntrinsicsGuide/}. 
%\url{https://software.intel.com/sites/landingpage/IntrinsicsGuide/}} 
The compiler replace some of the operations with fused multiply-add/sub (FMA)
operations where the throughput is 2 per cycle. The resulting assembly code
has only 2 subs, and 4 FMAs.  Hence, it takes only 4 cycles to execute 6
vector instructions which corresponds to 40 scalar operations. Therefore,
achievable peak (ceiling) is 36 GFLOPS on Broadwell. The achieved performance
is 20.5 GFLOPS which is 57\% of the peak. If we account for the cycles for the
non-arithmetic operations in the loop body of the assembly code, then the
achievable ceiling will be further lowered making the achieved performance
much closer to the peak.

%\begin{figure}[tb]
%  \centering
%    \includegraphics[width=0.5\textwidth]{figures/h_norm_gflops}
%\caption{Performance of cache-oblivious and
%single-level tiling code on Haswell, normalized to sequential parametric single-level
%tiling (higher the better). The absolute performance numbers are shown on top of
%each bar in GFLOPS. Both the sequential and the parallel performance of
%PCOT code either slightly better or closely matches the performance of other
%code generators.}
%	\label{fig:gflops_h}
%\end{figure}
%
%\begin{figure}[tb]
%  \centering
%    \includegraphics[width=0.5\textwidth]{figures/b_norm_gflops}
%\caption{Performance of cache-oblivious and
%single-level tiling code on Broadwell, normalized to sequential parametric single-level
%tiling (higher the better). The absolute performance numbers are shown on top of
%each bar in GFLOPS. Both the sequential and the parallel performance of
%PCOT code either slightly better or closely matches the performance of other
%code generators.}
%	\label{fig:gflops_b}
%\end{figure}

%\subsubsection{Comparison with parametric SLT code}
%We compare speed of all the benchmarks with parametric single-level tiling.
%Performance of PCOT is comparable on both machines, except for
%Cholesky-parallel which is only 15\% slow compared to \texttt{SLT-par} on
%Broadwell. The rest of the benchmarks either slightly better or closely match
%the speed of SLT. 
%
%
%
%\subsubsection{Comparison with Pochoir}
%Pochoir can only handle Heat-2D and 3D benchmarks. The other two stencils in
%our benchmark suite cannot be implemented by Pochoir due to dependences along
%time iteration. 
%%Pochoir uses pointer arithmatic instead of index computations
%%therefore, we updated PCOT Heat-2D and Heat-3D code with the same
%%optimization. 
%The performance of PCOT is similar to Pochoir for Heat-3D on both machines.
%But PCOT out performs Pochoir on Heat-2D. For this benchmark, Pochoir has a
%vectorization issue for the iterations on the boundaries. They use a 
%function to define boundary computations that operates only on a single
%iteration and these points are not vectorized.  
%
%%There are more sophisticated tiling strategies for stencils (e.g., hybrid
%%tiling~\cite{grosser2014hybrid}), which have better parallel behavior than
%%standard tiling. Extending the recursive tiling strategy to such tile shapes
%%may help reduce this gap.
%
%
%\subsubsection{Comparison with Autogen}
%
%OSP is the only benchmark -- out of the benchmarks we use -- that have the
%fractal property they seek.  The execution time of PCOT is 15\% faster for
%sequential runs, and 10\% faster for parallel runs than Autogen.  This
%difference is attributed to the tile shapes. The Autogen generated code is
%restricted to cubic tile sizes. 
%%Current code generated by Autogen uses cubic tile sizes, except for the base
%%cases at the edges of the iteration space. 
%PCOT generated code have more degrees of freedom, and we found that using
%large tile sizes in the innermost dimension is better for performance in many
%cases. One of the reason that favor such tile shapes is hardware
%prefteching~\cite{mehta2016turbotiling}. We explored cubic tile sizes for OSP
%code generated using PCOT, it gave similar performance as Autogen.
%
%\subsubsection{Comparison with Pluto}
%We compared performance of PCOT with Pluto for FDTD-2D, GS-2D, LUD and
%Cholesky benchmarks. For all the runs PCOT performance is better or comparable
%to Pluto's performance.  In the worst case, Pluto is 12\% faster for
%Cholesky-parallel on Broadwell.  This result confirms that even though PCOT
%generate parametrically tiled code, performance is comparable to the
%benchmarks we use in this paper. 
%




%\begin{figure}[tb]
%  \centering
%    \includegraphics[width=0.5\textwidth]{figures/h_norm_oca}
%\caption{Off-chip memory accesses for tile size optimized for speed on
%Haswell. Data is normalized to PCOT-seq. Absolute values are printed on top of
%each bar.  
%}
%	\label{fig:norm_oca_h}
%\end{figure}
%
%\begin{figure}[tb]
%  \centering
%    \includegraphics[width=0.5\textwidth]{figures/b_norm_oca}
%\caption{Off-chip memory accesses for tile size optimized for speed on
%Broadwell. Data is normalized to PCOT-seq. Absolute values are printed on top of
%each bar.  
%}
%	\label{fig:norm_oca_b}
%\end{figure}


%\begin{table}
%\caption{Off-chip memory accesses on Haswell
%\label{tab:llch}}
%\centering
%\begin{tabular}{|l|c|c|c|c|c|c|}
%\hline\hline
% Benchmark & \multicolumn{6}{c|}{OCAs in millions} \\ \hline
% & \multicolumn{3}{c|}{sequential} & \multicolumn{3}{c|}{parallel} \\ \hline 
% & SLT & PCOT & SLT/PCOT ratio & SLT & PCOT & SLT/PCOT ratio\\ \hline\hline
%	
% FDTD-2D & 38 & 37 & 1.0 & 121 & 79 & 1.5 \\ \hline
% GS-2D & 25 & 4 & 6.7 & 46 & 5 & 9.7 \\ \hline
% Heat-2D & 13 & 11 & 1.3 & 47 & 39 & 1.2 \\ \hline
% Heat-3D & 49 & 49 & 1.0 & 109 & 83 & 1.3 \\ \hline
% Cholesky & 7  & 8 & 0.9  & 63  & 24 & 2.7 \\ \hline
% LUD & 6 & 9 & 0.7 & 65 & 29 & 2.2\\ \hline
% OSP & 4 & 5 & 0.8 &  29 & 15 & 1.9 \\ \hline
%%



%\subsection{Summary}
%In this section, we presented an evaluation of cache-oblivious codes generated
%using PCOT. For the set of benchmarks used, we are able to mostly confirm the
%hypotheses: the absolute number of off-chip memory accesses and the
%variability of that number remain significantly low for cache oblivious tiling
%compared to single-level tiling. The variability of execution time over a
%range of tile sizes is higher for both cache oblivious and single-level
%tiling. Which confirms that we need to explore tile sizes for speed for both
%techniques on each machine.  Cache oblivious techniques are not always
%beneficial, i.e., for Heat-3D benchmark and for sequential runs which ends up
%with larger tile sizes due to benefits from prefetching. Finally, we show that
%significant savings in off-chip memory accesses do not translate to speed for
%the set of compute intensive benchmarks we use in this study.


%hypotheses: cache-oblivious code is as fast as tuned single-level tiling code,
%but have much less off-chip memory accesses and low variability especially
%when tiles are optimized for L1 and L2 cache.  The fewer number of LLC misses
%of PCOT does not translate into better speed compared to SLT since, tuned SLT
%code is already a compute bound program which hides the latency to the
%off-chip memory accesses.  However, our experiments also show that there is
%little difference in off-chip memory accesses when the tiles are optimized for
%LLC, explained by the importance of hardware prefetching in current
%architectures. Finally, we have successfully, generalized cache-oblivious code
%generator for polyhedral class of programs which performs as fast as state of
%the art code generators and have significantly less number of off-chip memory
%accesses compared to single-level tiling codes.
%%The optimizations for
%%cache-oblivious code explained in section~\ref{sec:codegen} help improve the
%%performance. Even-though the cache-oblivious code is more portable compared to
%%single-level tiling code, the tile size and base size threshold parameters
%%must be tuned for each architecture for each benchmark for the best
%%performance. 



% Local Variables: ***
% TeX-master: "PACT2017.tex" ***
% fill-column: 78 ***
% End: ***
