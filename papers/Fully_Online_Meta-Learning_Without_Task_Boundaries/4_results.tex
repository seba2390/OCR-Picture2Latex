
\section{Experimental Evaluation}

Our experiments focus on online meta-learning of image classification tasks. In these settings, an effective algorithm should adapt to changing tasks as quickly as possible, rapidly identify when the task has changed, and adjust its parameters accordingly. Furthermore, a successful algorithm should make use of past task shifts to meta-learn effectively, thus accelerating the speed with which it adapts to future task changes. In all cases, the algorithm receives one data point at a time, and the task changes periodically. In order to allow for a comparison with prior methods, which generally assume known task boundaries, the data stream is partitioned into discrete tasks, but our algorithm is not aware of which datapoint belongs to which tasks or where the boundaries occur. The prior methods, in contrast, \emph{are} provided with this information, thus giving them an advantage. We first describe the specific task formulations, and then the prior methods that we compare to.

Online meta-learning algorithms should adapt to each task as quickly as possible, and also use data from past tasks to accelerate acquisition of future tasks. Therefore, we report our results as a learning curve, with one axis corresponding to the number of seen tasks, and the other axis corresponding to the cumulative error rate on that task. This error rate is computed using a held-out validation data for each task after the adaptation that task. 

We evaluate prior online meta-learning methods and baselines on three different datasets (Rainbow-MNIST, CIFAR100 and CELEBA). TOE (train on everything), TFS (train from scratch), FTL (follow the leader), FTML (follow the meta-leader)~\citep{finn19a}, LwF~\citep{li2017learning}, iCaRL~\citep{rebuffi2016icarl} and MOCA~\citep{harrison2020continuous} are the baseline methods we compare against our method FOML. See Section 4 for more detailed description of these methods.

\noindent \textbf{Datasets:} We compare TOE, TFS, FTL, FTML, LwF, iCaLR and FOML on three different datasets. Rainbow-MNIST~\citep{finn19a} was created by changing the background color, scale and rotation of the MNIST dataset. It includes 7 different background colors, 2 scales (full and half) and 4 different rotations. This leads to a total of 56 number of tasks. Each individual task is to classify the images into 10 classes. We use the same partition with 900 samples per each task, as in prior work~\citep{finn19a}. However, this task contains relatively simple images, and only 56 tasks. To create a much longer task sequence with significantly more realistic images, we modified the CIFAR-100 and CELEBA datasets to create an online meta-learning benchmark, which we call online-CIFAR100 and online-CELEBA, respectively. Every task is a randomly sampled set of classes, and the goal is to classify whether two images in this set belongs to same class or not. Specifically, each task corresponds to 5 classes, and every datapoint consists of a \emph{pair} of images, each corresponding to one of the 5 classes for that task. The goal is to predict whether the two images belong to the same class or not. Note that different tasks are not mutually exclusive, which \emph{in principle} should favor a TOE-style method, since meta-learning is known to underperform with non-mutually-exclusive tasks~\citep{yin2019meta}. To make sure the data distribution changes smoothly over tasks, we only change a subset of the classes between consecutive tasks. This allow us to create a very large number of tasks (1200 for online-CIFAR100), and evaluate our method and the baselines on much longer and more realistic task sequences.

\begin{figure*}[!t]
\centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figs/exp4x.pdf}
    \end{subfigure} 
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figs/exp_cifar_1_main.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figs/exp_celeba_1_main.pdf}
    \end{subfigure}
    \caption{\footnotesize{ \emph{Comparison between online algorithms:} We compare our method with baselines and prior approaches, including TFS (Train from Scratch), TOE (Train on Everything), FTL (Follow the Leader) and FTML (Follow the Meta Leader). \textbf{a:} Performance relative to the number of tasks seen over the course of online training on the Rainbow-MNIST dataset. As the number of task increases, FOML achieves lower error rates compared to other methods. We also compare our method with continual learning baselines: LwF~\citep{li2017learning}, iCaRL~\citep{rebuffi2016icarl} and MOCA~\citep{harrison2020continuous}. MOCA~\cite{harrison2020continuous} archive similar performance to ours at the end of the learning, but FOML makes significantly faster progress. \textbf{b:} Error rates on the Online-CIFAR100 dataset. Note that FOML not only achieves lower error rates on average, but also reaches the lowest error (of around 17\%) more quickly than the other methods. \textbf{c:} Performance of FOML on the CELEBA dataset. This dataset contains more than 1000 classes, and we follow the same protocol as in Online-CIFAR100 experiments. Our method, FOML, learns more quickly on this task as well.}}
    \label{fig:main_plots}

\end{figure*}





\noindent \textbf{Implementation Details:} We use a simple 4 layer convolutional neural network with 8,16,32,64 filters at each layer for both experiments. However, for the CIFAR-100 experiments, a Siamese version of the same network is used. All the methods were trained via a cross-entropy loss, with their best performing hyper-parameters, on a single NVIDA-2080 GPU machine. Please see Appendix~\ref{sec:A1} for more details on hyper-parameters and network architecture.


\noindent \textbf{Results on Rainbow-MNIST:} As shown in Fig~\ref{fig:main_plots}, FOML attains the lowest error rate on most tasks in Rainbow-MNIST, except a small segment in the very beginning. The performance of TFS is similar performance across all the tasks, and does not improve over time. This is because it resets its weights every time it encounters a new task, and therefore cannot not gain any advantage from past data. TOE has larger error rates at the start, but as we add more data into the buffer, TOE is able to improve. On the other hand, both FTL and FTML start with similar performance, but FTML achieve much lower error rates at the end of the sequence compared to FTL, consistently with prior work~\citep{finn19a}. The final error rates of FOML are around 10\%, and it reaches this performance significantly faster than FTML, after less than 20 tasks. Note that FTML also has access to task boundaries, while FOML does not.

\noindent \textbf{Results on Online-CIFAR100 and Online-CELEBA:} We use a Siamese network for this experiment, where each image is fed into a 7-layer convolutional network, and each branch outputs a 128 dimensional embedding vector. A difference of these vectors are fed into a fully connected layer for the final classification. Each task contains data from 5 classes, and each new task introduces three new classes, and retains two of the classes from the previous task, providing a degree of temporal coherence while still requiring each algorithm to handle persistent shift in the task. Fig~\ref{fig:main_plots} shows the error rates of various online learning methods, where each method is trained over a sequence of 1200 tasks for online-CIFAR100. All the methods start with initial error rates of 50\%. The tasks are not mutually exclusive, so in principle TOE can attain good performance, but it makes the slowest progress among all the methods, suggesting that simple pretraining is not sufficient to \emph{accelerate} learning. FTL uses a similar pre-training strategy as TOE. However it has an adaptation stage where the model is fine-tuned on the new task. This allows it to make slower progress.
As expected from prior work~\citep{finn19a}, the meta-learning procedure used by FTML allows it to make faster progress than FTL. However, FOML makes faster progress on average, and achieves the lowest final error rate ($\sim15\%$) after sequence of 1200 tasks. 

\vspace{-0.2cm}
\subsection{Ablation Studies}
\vspace{-0.2cm}
We perform various ablations by varying the number of online parameters used for the meta-update $K$, importance of meta-model to analysis the properties of our method.

\noindent \textbf{Number of online parameters used for the meta-update:} Our method periodically updates the online weights and meta weights. The meta-updates involves taking $K$ recent online parameters and updating the meta model via MAML gradient. Therefore, meta-updates depend on the trajectory of the online parameters. In this experiment, we investigate how the performance of FOML changes as we vary the number of parameters used for the meta-update ($K$ in Algorithm~\ref{alg:OML}). Fig~\ref{fig:ablations} shows the performance of our method with various values of $K$: $K=[1,2,3,5,10]$. We can see that the performance improves when we update the meta parameters over longer trajectory of online parameters (larger $K$). We speculate that this is due to the longer sequences providing a clearer signal for how the meta-parameters influence online updates over many steps.

\noindent \textbf{Importance of meta update:} FOML keeps track of separate online parameters and meta-parameters, and each of them is updated via corresponding updates. However, only the online parameters $\phi$ are used for evaluation, while the meta-parameters $\theta$ only influence them via the regularizer, and have no direct effect on evaluation performance. This might raise the question: how important is the contribution of the meta-parameters to the performance of the algorithm during online training? We train a model with and without meta-updates, and the performance is shown in Fig~\ref{fig:ablations}. None that, the model without meta-updates is identical to our method, except that the meta-updates themselves are not performed. We can clearly see that the model trained with meta-updates preforms much better than a model trained without meta-updates. The model trained without meta-updates generally does not improve significantly as more tasks are seen, while the model trained with meta-updates improves with each task, and reaches significantly lower final error. This shows that, even though $\theta$ and $\phi$ are decoupled and only connected via a regularization, the meta-learning component of our method really is critical for its good performance.

\begin{figure}[!t]
    \centering
    \begin{minipage}[]{0.45\textwidth}
        \includegraphics[width=0.95\columnwidth]{figs/test5.pdf}
    \end{minipage}
    \begin{minipage}[]{0.45\textwidth}
        \includegraphics[width=0.95\columnwidth]{figs/test7.pdf}
    \end{minipage}
    \caption{\footnotesize{\emph{Ablation experiments:} \textbf{a)} We vary the number of online updates $K$ used before the meta-update, to see how it affects the performance of our method. The performance of FOML improves as the number of online updates is increased. \textbf{b)} This experiment shows how FOML performs with and without meta updates, to confirm that the meta-training is indeed an essential component of our method. With meta-updates, FOML learns more quickly, and performance improves with more tasks.}}
    \label{fig:ablations}
\end{figure}



\vspace{-0.4cm}
\section{Conclusion}
\vspace{-0.4cm}
We presented FOML, a MAML-based algorithm for online meta-learning that does not require ground truth knowledge of task boundaries, and does not require resetting the parameter vector back to the meta-learned parameters for every task. FOML is conceptually simple, maintaining just two parameter vectors over the entire online adaptation process: a vector of online parameters $\phi$, which are updated continually on each new batch of datapoints, and a vector of meta-parameters $\theta$, which are updated correspondingly with meta-updates to accelerate the online adaptation process, and influence the online updates via a regularizer. We find that even a relatively simple task sampling scheme that selects datapoints at random from a buffer of all seen data enables effective meta-training that accelerates the speed with which FOML can adapt to each new task, and we find that FOML reaches a final performance that is comparable to or better than baselines and prior methods, while learning to adapt quickly to new tasks significantly faster. While our work focuses on supervised classification problems, a particularly exciting direction for future work is to extend such online meta-learning methods to other types of online supervision that may be more readily available, including self-supervision and prediction, so that models equipped with online meta-learning can continually improve as they see more of the world.
