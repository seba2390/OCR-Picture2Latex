\newpage

\section{Appendix}

\subsection{Implementation Details-RainbowMNIST}
\label{sec:A1}
All the images from Rainbow-MNIST are in $28 \times 28$ pixel resolution and with 3 channels. We used a 4 layer convolutional neural network for these experiments, with each layer having [32,32,64,64] number of filters. After every convolution ReLU activation and max pooling is applied to the features. At the last layer we take an average pooling. Finally, 64 dimensional features are passed through fully connected layer with a sigmoid activation. Since the Rainbow-MNIST uses MNIST data to classify, the final layer has 10 neurons, and the objective is to correctly classify the digits of different scales and colors into 10 classes (actual numbers irrespective of the rotation and background color). All the models were trained with 50 gradient updates.

\vspace{-0.4cm}
\subsection{Implementation Details-ONLINE-CIFAR100}
\vspace{-0.2cm}
\label{sec:A2}
All the images from CIFAR100 are in $32 \times 32$ pixel resolution and with 3 channels. We used a similar 7 layer convolutional neural network in a siamese network architecture for these experiments, with each layer having [32,32,32,64,64,64,128] number of filters. After every convolution ReLU activation and batchnorm is applied to the features. Every other layers have max-pooling. At the last layer we take an average pooling. The L2 distance between two images are calculated and a fully connected layer is used to classify the two images as same class or not.

\vspace{-0.4cm}
\subsection{Baseline Methods}
\vspace{-0.2cm}
\label{sec:A3}
\textbf{TFS:} Every time this model sees a new task (This needs to know the task boundary), the model resets to new sets of random weights. After that the model is trained only using the data from the new task. At the same time, we also rest the optimizer state to remove any effect of momentum from previous updates. The model is trained with Adam optimizer with a learning rate of 0.001. 

\textbf{TOE:} Although TOE optimize the parameters on the all available task, training the model using all the available data is practically impossible. Therefore, we sample datapoints from the buffer and update the model parameters. However, since the number of data is increasing over time, the model also needs be updated on a good representative data of the online stream. Therefore, we increase the number of gradient updates over time. For ONLINE-CIFAR100 experiment, the number of gradients updates are increased by 10 for every 100 tasks. The model is trained with Adam optimizer with a learning rate of 0.001. 

\textbf{FTL:} This method, first pretrains a model on the past data, and then fine-tune it on the very recent samples. Similar to TOE, we first train a set of weights using the datapoints in the data buffer. After that, the model is fine-tuned on the correct task data. However, after this adaptation and evaluation the fine-tuned weights are simply discarded and the pretraining starts from the initial weights of the adaptation process. Adam optimizer with same configuration is used to train this model. 

\textbf{FTML:} The FTML updates the meta-parameters using MAML style update. For that, we trained the FTML with 5 inner-loop updates (each inner-loop have task-specific data for the inner-loop adaptation). After the meta-update, the meta-parameters are used as the initialization for the inner-loop adaptation, and similar to FTL the adapted weights are simply discarded after evaluation. The inner-loop is trained with SGD with a learning rate of 0.001 and the outer-loop is trained with a learning rate of 0.0005.

\textbf{FOML:} Our method also have two sets of parameter vectors, thus two different optimizers. We use Adam for both online and meta updates with a learning rate of 0.001 for both cases. Since, our online updates share the meta-knowledge via the regularization term, the $\beta_1$ controls how much pull is applied to the meta parameters by the online parameters. We use 0.01 for $\beta_1$. Similarly the pull on meta-parameters by the online parameters is controlled by $\beta_2$, and we use 0.001 in our experiments.

\vspace{-0.4cm}
\subsection{Meta updates}
\vspace{-0.2cm}
\label{sec:A4}

The full meta update we employ in our complete implementation includes an additional regularization term, in addition to the loss, such that the full meta-update is given by the following equation:
\begin{align*}
    \theta = \theta - \alpha_2 \nabla_{\theta} \left\{ \mathcal{L}(\phi^j_{\theta}; \mathcal{D}_{val}^m) + \beta_2 \sum_{k=0}^K\mathcal{R}(\theta, \phi^{j-k}_{\theta}) \right\} 
\end{align*}
Note that the gradient of $\mathcal{L}(\phi^j_{\theta}; \mathcal{D}_{val}^m)$ with respect to $\theta$ also contains gradients (with respect to $\theta$) for each $\mathcal{R}(\theta, \phi^{j-k}_{\theta})$, so the additional second term simply increases the weights on these gradients by an additional $\beta_2$ factor. While we found this to work well in our experiments, we did not ablate this design decision in detail, and we do not believe it is essential to good performance.
