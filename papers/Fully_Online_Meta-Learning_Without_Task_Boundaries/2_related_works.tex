\section{Related Work}

Online meta-learning brings together ideas from online learning, meta learning, and continual learning, with the aim of adapting quickly to each new task while \emph{simultaneously} learning how to adapt even more quickly in the future. We discuss these three sets of approaches next.


\noindent \textbf{Meta Learning:} Meta learning methods try to learn the high-level context of the data, to behave well on new tasks (\emph{Learning to learn}). These methods involve learning a metric space~\citep{koch2015siamese, vinyals2016matching, snell2017prototypical, yang2017learning}, gradient based updates~\citep{finn2017model, li2017meta, park2019meta, nichol2018first, nichol2018reptile}, or some specific architecture designs~\citep{santoro2016meta, munkhdalai2017meta, ravi2016optimization}.
In this work, we are mainly interested in gradient based meta learning methods for online learning. MAML~\citep{finn2017model} and its variants~\citep{nichol2018first, nichol2018reptile, li2017meta, park2019meta, antoniou2018train} first meta train the models in such a way that the meta parameters are close to the optimal task specific parameters (good initialization). This way, adaptation becomes faster when fine tuning from the meta parameters. However, directly adapting this approach into an online setting will require more relaxation on online learning assumptions, such as access to task boundaries and resetting back and froth from meta parameters. Our method does not require knowledge of task boundaries.




\noindent \textbf{Online Learning:} Online learning methods update their models based on the stream of data sequentially. There are various works on online learning using linear models~\citep{cesa2006prediction}, non-linear models with kernels~\citep{kivinen2004online, jin2010online}, and deep neural networks~\citep{zhou2012online}. Online learning algorithms often simply update the model on the new data, and do not consider the past knowledge of the previously seen data to do this online update more efficiently. However, the online meta learning framework, allow us to keep track of previously seen data and with the ``meta'' knowledge we can update the online weights to the new data more faster and efficiently.
\noindent \textbf{Continual Learning:} 
A number of prior works on continual learning have addressed catastrophic forgetting~\citep{mccloskey1989catastrophic,li2017learning,ratcliff1990connectionist, rajasegaran2019random, rajasegaran2020itaml}, removing the need to store all prior data during training. Our method does not address catastrophic forgetting for the meta-training phase, because we must still store all data so as to ``replay'' it for meta-training, though it may be possible to discard or sub-sample old data (which we leave to future work). However, our adaptation process is fully online. A number of works perform meta-learning for better continual learning, i.e. learning good continual learning strategies~\citep{al2017continuous,nagabandi2018deep,javed2019meta,harrison2019continuous,he2019task,beaulieu2020learning}. However, these prior methods still perform batch-mode meta-training, while our method also performs the meta-training itself incrementally online, without task boundaries.


The closest work to ours is the follow the meta-leader (FTML) method~\citep{finn19a} and other online meta-learning methods~\citep{yao2020online}. FTML is a varaint of MAML that finetunes to each new task in turn, resetting to the meta-trained parameters between every task. While this effectively accelerates acquisition of new tasks, it requires ground truth knowledge of task boundaries and, as we show in our experiments, our approach outperforms FTML \emph{even when FTML has access to task boundaries and our method does not}. Note that the memory requirements for such methods increase with the number of adaptation gradient steps, and this limitation is also shared by our approach. Online-within-online meta-learning~\cite{denevi2019online} also aims to accelerate online updates by leveraging prior tasks, but still requires knowledge of task boundaries. MOCA~\cite{harrison2020continuous} instead aims to \emph{infer} the task boundaries. In contrast, our method does not even attempt to find the task boundaries, but directly adapts without them. A number of related works also address continual learning via meta-learning, but with the aim of minimizing catastrophic forgetting~\cite{gupta2020maml, caccia2020online}. Our aim is not to address catastrophic forgetting. Our method also meta-trains from small datasets for thousands of tasks, whereas prior continual learning approaches typically focus on settings with fewer larger tasks (e.g., 10-100 tasks).
