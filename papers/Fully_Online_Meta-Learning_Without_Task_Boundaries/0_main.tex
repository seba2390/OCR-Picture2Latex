
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


% \usepackage[pdftex]{graphicx}          
% \usepackage{comment}
\usepackage{color}
\usepackage{colortbl}
% \usepackage{geometry}
\usepackage[english]{babel}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{mathtools}
\usepackage{adjustbox}
\usepackage{import}
\usepackage[noend]{algpseudocode}
\usepackage{relsize}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{enumitem}
\usepackage[low-sup]{subdepth}
\usepackage{xspace}
\usepackage{bm}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{varwidth}
\newsavebox\tmpbox


\title{Fully Online Meta-Learning Without Task Boundaries}
\iclrfinalcopy

\author{Jathushan Rajasegaran$^{1}$, Chelsea Finn$^{2}$, Sergey Levine$^{1}$\\
$^{1}$UC Berkeley, $^{2}$Stanford University \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ours}{FOML\xspace}
% \newcommand{\rj}[1]{\textcolor{blue}{#1}}
\DeclareMathOperator*{\argminB}{argmin}
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
While deep networks can learn complex functions such as classifiers, detectors, and trackers, many applications require models that continually adapt to changing input distributions, changing tasks, and changing environmental conditions. Indeed, this ability to continuously accrue knowledge and use past experience to learn new tasks quickly in continual settings is one of the key properties of an intelligent system. For complex and high-dimensional problems, simply updating the model continually with standard learning algorithms such as gradient descent may result in slow adaptation. Meta-learning can provide a powerful tool to accelerate adaptation yet is conventionally studied in batch settings. In this paper, we study how meta-learning can be applied to tackle online problems of this nature, simultaneously adapting to changing tasks and input distributions and meta-training the model in order to adapt more quickly in the future. Extending meta-learning into the online setting presents its own challenges, and although several prior methods have studied related problems, they generally require a discrete notion of tasks, with known ground-truth task boundaries. Such methods typically adapt to each task in sequence, resetting the model between tasks, rather than adapting continuously across tasks. In many real-world settings, such discrete boundaries are unavailable, and may not even exist. To address these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and stays fully online without resetting back to pre-trained weights. Our experiments show that FOML was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.
\end{abstract}



\input{1_introduction}
\input{2_related_works}
\input{3_method}
\input{4_results}


\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\appendix
\input{5_appendix}

\end{document}
