\section{Introduction}


Flexibility and rapid adaptation are a hallmark of intelligence: humans can not only solve complex problems, but they can also figure out \emph{how} to solve them very rapidly, as compared to our current machine learning algorithms. Such rapid adaptation is crucial for both humans and computers: for humans, it is crucial for survival in changing natural environments, and it is also crucial for agents that classify photographs on the Internet, interpret text, control autonomous vehicles, and generally make accurate predictions with rapidly changing real-world data. While deep neural networks are remarkably effective for learning and representing \emph{accurate} models~\citep{he2015deep, krizhevsky2012imagenet, simonyan2014very, szegedy2015going}, they are comparatively unimpressive when it comes to adaptability, due to their computational and data requirements. Meta-learning in principle mitigates this problem, by leveraging the generalization power of neural networks to accelerate adaptation to new tasks~\citep{finn19a, li2017meta, nichol2018first, nichol2018reptile, park2019meta, antoniou2018train}. However, standard meta-learning algorithms operate in batch mode, making them poorly suited for continuously evolving environments. More recently, online meta-learning methods have been proposed with the goal of enabling continual adaptation~\citep{finn19a, jerfel2018reconciling, yao2020online, nagabandi2018deep, li2020online}, where a constant stream of data from distinct tasks is used for  \emph{both} adaptation and meta-training. In this scheme, meta-training is used to accelerate how quickly the network can adapt to each new task it sees, and \emph{simultaneously} use that data from each new task for meta-training. This further accelerates how quickly each subsequent task can be acquired. However, current online meta-learning methods fall short of the goal of creating an effective adaptation system for online data in several ways: (1) they typically require task boundaries in the data stream to be known, making them ill-suited to settings where task boundaries are ill-defined and tasks change or evolve gradually, a common tread in real-world; (2) as a result, they typically re-adapt from the meta-trained model on each task, resulting in a very ``discrete'' mode of operation, where the model adapts to a task, then resets, then adapts to a new one. These limitations restrict the applicability of current online meta-learning methods to real-world settings. We argue that task boundary assumption is somewhat artificial in online settings, where the stream of incoming data is cleanly partitioned into discrete and well-separated tasks presented in sequence. In this paper, we instead develop a \emph{fully} online meta-learning approach, which does not assume knowledge of task boundaries and does not re-adapt for every new task from the meta parameters.



\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.75\linewidth}
        \centering
        \includegraphics[trim={0 0 0 0.0cm},clip,width=\textwidth]{figs/oml_1.png}
        %%CF.5.1: This seems like a great start, but it is also somewhat abstract. Perhaps it would be useful to include a little bit more text/notation to connect it with the text? (could potentially be accomplished with a second figure)
        %%CF.5.1: It could also be a good idea to try to get across that OML doesn't require knowledge of task boundaries.
        %%SL.9.18: maybe the sketch in my email could be better? I agree this is a bit abstract right now
    \end{subfigure}
    \caption{
    % \emph{An over view of our method, FOML:}
    %%SL.9.18: make sure the name is introduced somewhere in the introduction text
    % Our method is significantly different from previous works in two aspects. Our updates are \emph{Fully Online}
    %%SL.9.18: would be better to explain this instead of just using the term "fully online"
    % and FOML does not require access to task boundaries. \emph{Left)} Our method updates the online model from last task to new task (Fully Online), without fine tuning from meta parameters and then reset back to meta parameters. This is ideal when the adjacent task optimal parameters stay closer to each other than meta parameters (the online tasks distribution changes smoothly over time). \emph{right)} FOML does not require any additional knowledge about the task boundaries unlike previous works~\citep{finn19a} which assume the task boundaries are given. We assume the online data structure changes smoothly, then a random sampling on the data stream is sufficient enough to have diverse samples to train our meta model.
    \footnotesize{\textbf{Comparison of standard online meta-learning and FOML:} In standard online meta-learning (e.g., FTML~\citep{finn19a}), shown on the left, adaptation is performed on one task a time, and the algorithm ``resets'' the adaptation process at task boundaries. For example, a MAML-based method would reset the current parameters back to the meta-trained parameters. In our approach (right), knowledge of task boundaries is not required, and the algorithm continually keeps track of \emph{online} parameters $\phi$ and \emph{meta}-parameters $\theta$. The online parameters are simply updated on the latest data, and the meta-parameters are updated to ``pull'' the online parameters toward fast-adapting solutions via a MAML-style meta-update.}}
    %%SL.10.4: Here is a recommended caption:
    % \textbf{Comparison of standard online meta-learning and FOML:} In standard online meta-learning (e.g., FTML~\citep{finn19a}), shown on the left, adaptation is performed on one task a time, and the algorithm ``resets'' the adaptation process at task boundaries. For example, a MAML-based method would reset the current parameters back to the meta-trained parameters. In our approach (right), knowledge of task boundaries is not required, and the algorithm continually keeps track of \emph{online} parameters and \emph{meta}-parameters. The online parameters are simply updated on the latest data, and the meta-parameters are updated to ``pull'' the online parameters toward fast-adapting solutions via a MAML-style meta-update.
    \label{fig:oml1}
\end{figure*}

Standard meta-learning methods consist of a meta-training phase, typically done with standard SGD, and an ``inner loop'' adaptation phase, which computes task specific parameter $\phi_i$ for the task $\mathcal{T}_i$ from a \emph{support} set to make accurate predictions on a \emph{query} set. For example, in model-agnostic meta-learning (MAML), adaptation consists of taking a few gradient steps on the support set, starting from the meta-trained parameter vector $\theta$, leading to a set of \emph{post-adaptation} parameters, and meta-training optimizes the meta-trained parameters $\theta$ so that these gradient steps lead to good results. Previous extensions of such approaches into the online setting typically observe one task at a time, adapt to that task (i.e., compute post-adaptation parameters on it), and then \emph{reset} $\phi_i$ back to the meta-trained parameters $\theta$ at the beginning of the next task. Thus, the algorithm repeatedly adapts, resets at the task boundary, adapts again, and repeats. This is illustrated in Figure 1 (left). However, in many realistic settings, the task boundaries are not known, and instead the tasks shift gradually over time. The discrete ``resetting'' procedure is a poor fit in such cases, and we would like to simply continue adapting the weights over time without ever resetting back to the meta-trained parameters, but still benefit from a concurrent meta-training process. For example, a meta-trained image-tagging model on the Internet (e.g., tagging friends in photographs) might gradually adapt to changing patterns and preferences of its users over time, where it would be unnatural to assume discrete shifts in what users want to tag. Similarly, a traffic prediction system might adapt to changing traffic patterns, including periodic changes due to seasons, and unexpected changes due to shifting economic conditions, weather, and traffic accidents. In this spirit, our method does not require any knowledge on the task boundaries as well as stays fully-online through out the learning. 


% Our method (FOML) contains two models: an online model that only sees very recent samples, and a meta-model that learns the context of the online data stream, without any knowledge of task boundaries. Previous online algorithms typically \emph{reset} back to the meta/pretrained model after adoption and \emph{discard} the last task parameters. Because of this, every new task needs to be learned from the generalized weights and could not utilize the previous task weights.
%%SL.9.18: I have this sense that it might be a little tough for readers to fully appreciate what's going on here, because it requires somewhat deeper understanding of basic terminology and concepts in meta-learning. We could write something more like this: Standard meta-learning methods consist of a meta-training phase, typically done with standard SGD, and an ``inner loop'' adaptation phase, which computes some kind of task parameter $\phi$ for the task $\mathcal{T}_i$ from a \emph{support} set to make accurate predictions on a \emph{query} set. For example, in model-agnostic meta-learning (MAML), adaptation consists of taking a few gradient steps on the support set, starting from the meta-trained parameter vector $\theta$, leading to a set of \emph{post-adaptation} parameters, and meta-training optimizes the meta-trained parameters $\theta$ so that these gradient steps lead to good results. Previous extensions of such approaches into the only setting typically observe one task at a time, adapt to that task (i.e., compute post-adaptation parameters on it), and then \emph{reset} $\phi_i$ back to the meta-trained parameters $\theta$ at the beginning of the next task. Thus, the algorithm repeatedly adapts, resets at the task boundary, adapts again, and repeats. This is illustrated in Figure 1 (left). In contrast, our method... [this summary is a bit wordy, but I'm struggling to figure out how to make it clearer/more concise]
% However, in many realistic settings, the task boundaries are not known, and instead the tasks shift gradually over time. The discrete ``resetting'' procedure is a poor fit in such cases, and we would like to simply continue adapting the weights over time without ever resetting back to the meta-trained parameters, but still benefit from a concurrent meta-training process.
%%SL.9.18: This would be a great place for an example, such as: For example, a meta-trained image-tagging model on the Internet (e.g., tagging friends in photographs) might gradually adapt to changing patterns and preferences of its users over time, where it would be unnatural to assume discrete shifts in what users want to tag. Similarly, a traffic prediction system might adapt to changing traffic patterns, including periodic changes due to seasons, and unexpected changes due to shifting economic conditions, weather, and traffic accidents.
% Additionally, these methods share meta knowledge via fine-tuning meta parameters to new tasks, therefore they need to go back to the meta-model after adaptation.
%%SL.9.18: I don't quite get what the above sentence is saying.
% However, our online model and meta-model communicate only via meta-gradients. Hence, our online model always stays online (``Fully online''). Subsequently, our method does not require any knowledge of task boundaries. In the real world online stream of data, it is hard to find a very discrete set of tasks as well as a clear task boundary between tasks. While gradient-based meta-learning methods require diverse tasks to learn good meta parameters, we relax this assumption, by randomly sampling metadata on the online stream. Based on the structure of the online data streams, nearby samples are often similar and samples taken from far are more diverse. Therefore, by randomly sampling data on the online stream, we can get diverse enough samples to train our meta-model. Our assumption is more relaxed and valid compared to task boundary assumptions used in the previous works.
%%SL.9.18: My sense is that we can defer the discussion of why randomly sampling from the past is OK to the methods section -- it's rather hard for the reader to appreciate this here because they don't yet know how our method works. What would however be important to do here is clearly explain how our method differs from the prior online meta-learning methods, with something along these lines: In contrast, our method persistently maintains a set of ``slow'' meta-trained parameters $\theta$ and ``fast'' adapted parameters $\phi$, and simply adapts $\phi$ to each datapoint as it arrives, without the need for task boundaries and without any resetting. A regularizer constrains $\phi$ to stay close to $\theta$, while $\theta$ is meta-trained on randomly sampled batches from all of the data seen so far (i.e., by ``replaying'' random samples from the data stream). [and then maybe reference the figure for an illustration?]




% Our method is partially inspired by psychological studies on two systems of thought~\citep{kahneman2011thinking}. System 1 is fast and pre-attentive while System 2 is slow and forms more logical thinking. Our online model is similar to System 1, it just looks at the current task and tries to solve it quickly. Our meta-model is similar to System 2, which looks at the high-level context of task distributions and guides the online model towards the new task. System 1 can solve an easy task without going back to system 2, however, for a hard task, it would be justifiable to reach System 2. Similarly, our online model can solve an easy or a similar task which it has seen recently, without too much interference from the meta-model. On the other hand, to solve a hard task, both online and meta-models needs to work together and share knowledge between them.
%%SL.9.18: This is a creative analogy, but I suspect that some hostile reviewers will be none too happy with it. Conventionally, System 2 ("knowing-that" according to Heidegger) is a "model based" system that uses an understanding of the rules that govern the world ("knowing that X does Y") to infer, via reasoning, planning, etc., a suitable course of action, while System 1 ("knowing-how") corresponds to prepared skills that don't require reasoning or planning. That's not quite the same as what we're doing -- the meta-trained parameters are not really using reasoning or planning, they are just trying to distill prior knowledge into a fast adaptation mechanism. My sense is that it will be hard to make this analogy fit. But on the other hand, I also don't think we actually have a problem with motivation -- the motivation is pretty good.
%%SL.9.18: On a completely unrelated point, I do think it would be pretty cool to *actually* figure out how to do what the above paragraph is saying, it's a really cool research vision :) but I suspect it would be a whole different paper.

% Several methods have tried to include some additional context of the past by pretraining or meta-training a model. However, both cases needs some additional information about the task boundaries. In addition to this, all these methods always fine-tune the pre-trained weights when a new task is introduced. Therefore, the online parameters do not stay online all the time. To address these issues, in this paper we propose a Fully Online Meta-Learning algorithm, which works without additional knowledge of task boundaries. Our method, has two models: an online model which only sees very recent samples, and a meta model which learns the context of the online data stream. However, unlike other methods, we do not transfer weights from a meta model to online model. 

%%SL.9.18: It's often easier to get a clear message across in the contributions paragraph if it's written as a narrative without bullet points. Bullet points take away your control over how the reader reads the paragraph, since they can jump between bullet points, whereas if you write it as a paragraph you have full control over the narrative and the order in which it is presented.
% Our main contributions in this paper are:
% \begin{itemize}
%     \item We propose a fully online meta-learning algorithm, which can be simply updated on new tasks, without any fine-tuning from the meta trained model. 
%     %%SL.9.18: One thing that can get you in trouble here is the "forgetting" stuff -- the forgetting folks might take issue at excessive use of "fully online" without qualifying that the meta-training requires replay.
%     \item We propose a meta-learning framework that does not require any knowledge about the task boundaries.
%     \item We evaluate our method on two online meta-learning datasets against strong baselines, and show that our method can learn new tasks faster than current state-of-the-art online learning algorithm, even when the prior methods are provided with ground truth knowledge of task boundaries and our method is not.
% \end{itemize}
The main contribution of our paper is FOML (fully online meta-learning), an online meta-learning algorithm that continually updates its online parameters with each new datapoint or batch of datapoints, while simultaneously performing meta-gradient updates on a separate set of meta-parameters using a buffer of previously seen data. FOML does not require ground truth knowledge of task boundaries, and does not reset the online parameters back to the meta-parameters between tasks, instead updating the online parameters continually in a fully online fashion. We compare FOML empirically to strong baselines and a state-of-the-art prior online meta-learning method, showing that FOML learns to adapt more quickly, and achieves lower error rates, both on a simple sequential image classification task from prior work and a more complex benchmark that we propose based on the CIFAR100 dataset, with a sequence of 1200 tasks.

