\section{Foundations}

Prior to diving into online meta learning, we first briefly summarize meta learning, model agnostic meta-learning, and online learning in this section. 

\noindent \textbf{Meta-learning:} Meta-learning address the problem of learning to learn. It uses the knowledge learned from previous tasks to quickly learn new tasks. Meta-learning assumes that the tasks are drawn from a stationary distribution $\mathcal{T}\sim \mathbb{P}(\mathcal{T})$. During the meta-training phase (outer-loop), $N$ tasks are assumed to be drawn from this distribution to produce the meta-training set, and the model is trained in such a way that, when a new task with its own training and test data $\mathcal{T}=\{\mathcal{D}^{tr}_{\mathcal{T}}, \mathcal{D}^{te}_{\mathcal{T}}\}$ is presented to it at meta-test time, the model should be able to adapt to this task quickly (inner-loop). Using $\theta$ to denote the meta-trained parameters, the meta-learning objective is:
\begin{equation}
    \theta^* = \argminB_{\theta} \mathbb{E}_{\mathcal{D}^{tr}_{\mathcal{T}} \ \text{where} \ \mathcal{T} \sim \mathbb{P}(\mathcal{T})}\left[\mathcal{L}(F_{\theta}(\mathcal{D}^{tr}_\mathcal{T}), \mathcal{D}^{te}_\mathcal{T})\right],
\end{equation}
where $F_{\theta}$ is the meta-learned adaptation process that reads in the training set $\mathcal{D}^{tr}_t$ and outputs task-specific parameters, prototypes, or features (depending on the method) for the new task $\mathcal{T}_i$.

\noindent \textbf{Model-agnostic meta-learning:} In MAML~\citep{finn2017model}, the inner-loop function is (stochastic) gradient decent. Hence, during the MAML inner-loop adaptation, $F_{\theta}(\mathcal{D}^{tr}_i)$ becomes $\theta - \alpha \nabla \mathcal{L}_\theta(\theta, \mathcal{D}^{tr}_i)$ (or, more generally, multiple gradient steps). Intuitively, what this means is that meta-training with MAML produces a parameter vector $\theta$ that can quickly adapt to any task from the meta-training distribution via gradient descent on the task loss. One of the principle benefits of this is that, when faced with a new task that differs from those seen during meta-training, the algorithm ``at worst'' adapts with regular gradient descent, and at best is massively accelerated by the meta-training.


\noindent \textbf{Online learning:} In online learning, the model faces a sequence of loss functions $\{\mathcal{L}_t\}_{t=1}^\infty$ and a sequence of data $\{\mathcal{D}_t=\{(x,y)\}\}_{t=1}^\infty$ for every time step $t$. The function $f: x \rightarrow \hat{y}$ maps inputs $x$ to predictions $\hat{y}$. The goal of an online learning algorithm is to find a set of parameters for each time step $\{\phi\}_{t=1}^\infty$, such that the overall loss between the predictions $\hat{y}$ and the ground truth labels $y$ is minimized over the sequence. This is typically quantified in terms of regret:
\begin{equation}
    \text{Regret}_T = \sum_{t=1}^T \mathcal{L}_t(\phi, \mathcal{D}_t) - \sum_{t=1}^T \mathcal{L}_{t}(\phi_t, \mathcal{D}_t). 
\end{equation}
where, $\phi_t = \argminB_{\phi} \mathcal{L}_t(\phi, \mathcal{D}_t)$. The first term measures the loss from the online model, and the second term measures the loss of the best possible model on that task. Various online algorithms tries to minimize the regret as much as possible when introducing new tasks.










\section{Online Meta-Learning: Problem Statement and Methods}

In an online meta-learning setting~\citep{finn19a}, the model $f_{\phi}$ observes datapoints one at a time from a online data stream $\mathcal{S}$. Each datapoint consists of an input $x^t_m$, where $t$ is the task index and $m$ is the index of the datapoint within that task, and a label $y^t_m$. The task changes over time and the model should be able to update the parameters $\phi$ to minimize the loss at each time step. \emph{The goal of online meta-learning is to quickly learn each new task $\mathcal{T}_t$ and perform well as soon as possible according to the specified loss function}.

A simple baseline solution would be to just train the model on the current task $\mathcal{T}_t$. We denote this baseline as TFS (Train from Scratch). For every new task, the model simply trains a new set of parameters using all of the data from the current task $\mathcal{T}_t$ that has been seen so far:
\begin{align*}
    \phi^t_{TFS} = \argminB_{\phi} \frac{1}{M}\sum_{m=1}^M \mathcal{L}_{t}(\phi, (x^t_m, y^t_m)).
\end{align*}
TFS has two issues. First, it requires the task boundaries to be known, which can make it difficult to apply to settings where this information is not available. Second, it does not utilize knowledge from other tasks, which greatly limits its performance even when task boundaries are available.

A straightforward way to utilize knowledge from other tasks of the online data stream is to store all the seen tasks in a large buffer $\mathcal{B}$, and simply keep training the model on all of the seen tasks. We will refer to this baseline method as TOE (Train on Everything):
\begin{align*}
    \phi^t_{TOE} = \argminB_{\phi} \frac{1}{Mt}\sum_{i=1}^t \sum_{m=1}^M  \mathcal{L}_{i}(\phi, (x^i_m, y^i_m)).
\end{align*}
TOE learns a function that fits all of the previously seen samples. However, this function may be far from optimal for the task at hand, because the different tasks may be mutually exclusive.

Therefore, fitting a single model on all of the previously seen tasks might not provide a good task-specific model for the current task. A more sophisticated baseline, which we refer to as FTL (Follow the Leader), pre-trains a model on all of the previous tasks, and then fine-tunes it only on the data from the current task. Note that this is subtly different from FTL in the classic online learning setting, due to the difference in problem formulation. This can be achieved by initializing $\phi$ with a pretrained weights up to the previous task $\phi^{t-1}_{TOE}$.

\begin{align*}
    \phi^t_{FTL} = \argminB_{\phi} \frac{1}{M}\sum_{m=1}^M \mathcal{L}_{t}(\phi, (x^t_m, y^t_m)).
\end{align*}
Here, for the task $t$, we take a model that is pre-trained on all previously seen tasks ($f_{\phi^{t-1}_{TOE}}$) and fine-tune on the current task data. In this way, FTL can use the past knowledge to more quickly adapt to the new task. However, pre-training on past tasks may not necessarily results in an initialization that is conducive to fast adaptation~\citep{finn2017model, nichol2018first, nichol2018reptile, li2017meta}. Finn~\emph{et al.}~\citep{finn19a} proposed a MAML-based online meta-learning approach, where MAML is used to meta-train a ``meta-leader'' model on all previously seen tasks, which is then adapted on all data from the current task. This way, the meta-leader parameters will be much closer to new task optimal parameters, and because of this it is much faster to adapt to new tasks from the online data.

{\small
\begin{align*}
\phi^t_{FTML} = \argminB_{\phi} \mathbb{E}_{(x^t_m, y^t_m) \sim T_t }[\mathcal{L}_{t}(\phi^{t-1}_{MAML}, (x^t_m, y^t_m))] \\
\text{where,} \ \ \phi^{t-1}_{MAML} = \argminB_{\phi} \mathbb{E}_{T_j \sim \mathcal{D}(\mathcal{T})}[\mathcal{L}_{j}(\phi - \nabla\mathcal{L}_{j}(\phi, \mathcal{D}^{tr}_j), \mathcal{D}^{te}_j)]
\end{align*}}
FTL and FTML try to efficiently use knowledge from past tasks to quickly adapt to the new task. However, pre-trained weights from FTL do not guarantee fast adaptation, and both methods require ground-truth knowledge of task boundaries. This knowledge may not be realistic in many real-world settings, where the tasks may change gradually and no external information is available to indicate task transitions. Although FTML can enable fast adaptation, the model needs to be ``reset'' at each task, essentially creating a ``branch'' on each task and maintaining two independent learning processes: an adaptation process, whose result is discarded completely at the end of the task, and a meta-training process, which does not influence the current task at all, and is only used for forward transfer into future tasks. For this reason, we argue that FTL and FTML are not fully online. In this work, our aim is to develop a \emph{fully} online meta-learning method that continually performs both ``fast'' updates and ``slow'' meta-updates, does not need to periodically ``reset'' the adapted parameters back to the meta-parameters, and does not require any ground truth knowledge of task boundaries.

\section{Fully Online Meta-Learning Without Task Boundaries}


We first discuss the intuition behind how our approach handles online meta-learning without task boundaries. In many real-world tasks, we might expect the tasks in the online data stream to change gradually. This makes it very hard to draw a clear boundary between the tasks. Therefore, it is necessary to relax task boundary assumption if we want a robust online learner that can work on a real-world data stream. Additionally, since nearby data points are most likely to belong to the same or similar task, we would expect adaptation to each new data point to be much faster from a model that has already been adapted to other recent data points.

\ours maintains two separate parameter vectors for the online updates ($\phi$) and the meta updates ($\theta$). Both parameterize the same architecture, such that $f_\phi$ and $f_\theta$ represent the same neural network, but with different weights. The online model continuously reads in the latest datapoints from the online data stream, and updates the parameters $\phi$ in online fashion, without any boundaries or resets. However, simply updating the online model on each data point na\"{i}vely will not meta-train it to adapt more quickly, and may even result in drift, where the model forgets prior data. Therefore, we also incorporate a regularizer into the online update that is determined by a concurrent meta-learning process (see Fig.~\ref{fig:overview}). Note that \ours only incorporates the meta-parameters into the online updates via the meta-learned regularization term, without ever resetting the online parameters back to the meta-parameters (in contrast, e.g., to FTML~\citep{finn19a}).

The meta-updates of previous MAML-based online meta-learning approaches involve sampling data from all of the tasks seen so far, and then updating the meta-parameters $\theta$ based on the derivatives of the MAML objective. This provides a diverse sampling of tasks for the meta update, though it requires storing all of the seen data~\citep{finn19a}. We also use a MAML-style update for the meta parameters, and also require storing the previously seen data. To this end, we will use $\mathcal{B}$ to denote a buffer containing all of the data seen so far. Each new datapoint is added to $\mathcal{B}$ once the label is observed.

However, since we do not assume knowledge of task boundaries, we cannot sample entire tasks from $\mathcal{B}$, but instead must sample individual datapoints. We therefore adopt a different strategy, which we describe in Section~\ref{sec:meta_updates}: as shown in Fig~\ref{fig:overview}, instead of aiming to sample in complete \emph{tasks} from the data buffer, we simply sample random past datapoints, and meta-train the regularizer so that the online updates retain good performance on \emph{all} past data. We find that this strategy is effective at accelerating acquisition of future tasks in online meta-learning settings where the tasks are not mutual exclusive.
We define both types of updates in detail in the next sections.

\begin{figure*}[!h]
    \vspace{-0.1in}
    \centering
    \begin{minipage}[]{0.48\textwidth}
        \includegraphics[width=1\columnwidth]{figs/oml_2.png}
    \end{minipage}
    \hfill
    \begin{minipage}[]{0.5\textwidth}
        \vspace{0.4cm}
        \caption{\textbf{Overview of \ours learning}: \ours updates the online parameters $\phi$ using only the most recent $K$ datapoints from the buffer $\mathcal{B}$. Meta-learning learns a regularizer, parameterized by meta-parameters $\theta$, via second-order MAML-style updates. The goal of meta-learning is to make $\phi$ perform well on randomly sampled prior datapoints \emph{after performing $K$ steps with the meta-trained regularizer}.
        }
        \label{fig:overview}
    \end{minipage}
    \vspace{-0.2in}
\end{figure*}


\subsection{Fully Online Adaptation}


At each time step, \ours observes a data point $x_t$, predicts its label $\hat{y}_t$, then receives the true label $y_t$ and updates the online parameters. In practice, we make updates after observing $N$ new datapoints ($N = 10$ in our experiments), so as to reduce the variance of the gradient updates. We create a small dataset $\mathcal{D}^j_{tr}$ with these $N$ datapoints for the time step $j$. The true label for these datapoints can be from class labels, annotations, rewards, or even self-supervision, though we focus on the supervised classification setting in our experiments. 

However, the online updates are based only on the most recent  samples, and do not make use of any past data. Therefore, we need some mechanism for the (slower) meta-training process to ``transfer'' the knowledge it is distilling from the prior tasks into this online parameter vector. We can instantiate such a mechanism by introducing a regularizer into the online parameter update that depends on the meta-parameters $\theta$, which we denote as $\mathcal{R}(\phi,\theta)$. While a variety of parameterizations could be used for $\mathcal{R}(\phi,\theta)$, we opt for a simple squared error term of the form $\mathcal{R}(\phi,\theta) = (\phi - \theta)^2$, resulting in the following online update at each step $j$:
\begin{align*}
    \phi^j_{\theta} &= \phi^{j-1}_{\theta} - \alpha_1 \nabla_{\phi^{j-1}_{\theta}} \{ \mathcal{L}(\phi^{j-1}_{\theta}; \mathcal{D}^j_{tr}) + \beta_1 \mathcal{R}(\phi^{j-1}_{\theta}, \theta) \} \\
           &= \phi^{j-1}_{\theta} - \underbrace{\alpha_1 \nabla_{\phi^{j-1}_{\theta}}\mathcal{L}(\phi^{j-1}_{\theta}; \mathcal{D}^j_{tr})}_{ \text{task specific update}} + \underbrace{2\alpha_1\beta_1 (\theta - \phi^{j-1}_{\theta})}_{\text{meta directional update}}
\end{align*}
For classification, $\mathcal{L}$ is the cross-entropy loss. $\alpha_1, \beta_1$ are hyperparameters. Note that $\phi^j_{\theta}$ has a dependency on $\theta$ through the the \emph{meta directional update}, which keeps $\phi^j_{\theta}$ close to $\theta$. In the next section, we will present our meta-learning update, which will differentiate through this dependency so as to maximize the effectiveness of the regularizer at accelerating adaption.

\subsection{Meta-Learning Without Task Boundaries}
\label{sec:meta_updates}

As discussed in the previous section, the online updates to $\phi^j$ include a regularizer $\mathcal{R}$ that transfers knowledge from the meta-parameters $\theta$ into each online update. Additionally, our method maintains a buffer $\mathcal{B}$ containing all data seen so far, which is used for the meta-update. In contrast to prior methods, which explicitly draw a training and validation set from the buffer (i.e., a query and support set) and then perform a separate ``inner loop'' update on this training set~\citep{finn19a}, our meta-updates recycle the inner loop adaptation that is already performed via the online updates, and therefore we only draw a validation set $\mathcal{D}_{val}^m$ from the buffer $\mathcal{B}$. Specifically, we sample a set of $N$ datapoints at random from $\mathcal{B}$ to form $\mathcal{D}_{val}^m$. We then update the meta-parameters $\theta$ using the gradient of the meta-loss evaluated at $\phi^j_{\theta}$  with $\mathcal{D}_{val}^m$. In other words, we adjust the meta-parameters in such a way that, if an online update is regularized with this meta-weights, then the loss on the online update will be minimized. This can be expressed via following meta update:
% \emph{and} the regularizer $\mathcal{R}$ after the last $K$ updates on $\phi$
% \begin{align*}
%     \theta = \theta - \alpha_2 \nabla_{\theta} \left\{ \mathcal{L}(\phi^j_{\theta}; \mathcal{D}_{val}^m) + \beta_2 \sum_{k=0}^K\mathcal{R}(\theta, \phi^{j-k}_{\theta}) \right\} 
% \end{align*}
\begin{align*}
    \theta = \theta - \alpha_2 \nabla_{\theta} \mathcal{L}(\phi^j_{\theta}; \mathcal{D}_{val}^m) 
\end{align*}
Here, $\phi^j_\theta$ has a dependence on $\theta$, because $\theta$ appears in the regularizer $\mathcal{R}(\phi^i_{\theta}, \theta)$ in each online update from $\phi^{j-K+1}_\theta$ to $\phi^j_\theta$.
%Here, $\theta$ and $\phi^j_{\theta}$ are only related via regularization terms in the online updates, unlike FTML~\citep{finn19a}, which sets $\phi^0=\theta$ at every task boundary. 


The choice of sampling $\mathcal{D}_{val}^m$ at random from $\mathcal{B}$ has several interpretations. We can interpret this as regularizing $\phi$ to prevent the online parameters from drifting away from solutions that also work well on the entire data buffer. However, this interpretation is incomplete, since the meta-update doesn't simply keep $\phi$ close to a single parameter vector that works well on $\mathcal{D}_{val}^m$, but rather changes the regularizer \emph{so that gradient updates with that regularizer} maximally improve performance on $\mathcal{D}_{val}^m$. This has the effect of actually accelerating how quickly $\phi$ can adapt to new tasks using this regularizer, so long as past tasks are reasonably representative of prior tasks. We experimentally verify this claim in our experiments. Note, however, that this scheme does assume that the tasks are not mutually exclusive. In future work, it would also be interesting to explore more sophisticated strategies for sampling $\mathcal{D}_{val}^m$, for example by leveraging temporal coherence and drawing $N$ sequential points, which are more likely to belong to the same task. 
We summarize the complete algorithm in Algorithm~\ref{alg:OML}. We further discuss the implementation details of the meta updates in the Appendix~\ref{sec:A4}.

% We add the regularizer term shown above in all of our experiments. We do not believe this regularizer is essential, but we did not ablate it in our experiments.

\begin{algorithm}[!h]
\caption{Online Meta Learning with \ours}
\label{alg:OML}
\begin{algorithmic}[1]
\Procedure{Meta training}{}\\
\algorithmicrequire{ $\theta, \mathcal{B}, \phi, \text{Buffer}~\mathcal{B}, \text{Data stream}~\mathcal{S} $}
\State $\phi^0 \gets \phi$  
\While {Data stream $\mathcal{S}$ available}
    \State $\mathcal{D}^{j} \gets \mathcal{S} $  \Comment{get new data from online data-stream}
    \State $\mathcal{B} \gets \mathcal{B} + \mathcal{D} $  \Comment{add new data to the buffer}
    \State $\mathcal{D}^j_{tr}, \mathcal{D}^j_{val} \gets \mathcal{D}$  \Comment{partition the data into train and validation splits}
    \State $\hat{y}_{tr} \gets f_{\phi^{j-1}}(\mathcal{D}^j_{tr})$  \Comment{make predictions on the train set}
    \State $\phi^j_{\theta} \gets \phi^{j-1}_{\theta} - \alpha_1 \nabla_{\phi^{j-1}_{\theta}} \{ \mathcal{L}_{task} (\phi^{j-1}_{\theta}; \mathcal{D}^j_{tr})  + \beta_1 \mathcal{R}(\phi^{j-1}_{\theta}, \theta)\} $
    \State $\hat{y}_{val} \gets \phi^{j}(\mathcal{D}^j_{val})$ \Comment{Evaluate the updated model on the validation set}
    \State $\mathcal{D}^m_{val} \sim \texttt{random-sample}(\mathcal{B}) $ \Comment{sample random batch from buffer}
    \State $\theta \gets \theta - \alpha_2 \nabla_{\theta}  \mathcal{L}_{task} (\phi^j_{\theta}; \mathcal{D}^m_{val})  $
    % \State $\theta \gets \theta - \alpha_2 \nabla_{\theta} \{ \mathcal{L}_{task} (\phi^j_{\theta}; \mathcal{D}^m_{val})  + \beta_2 \sum_{k=1}^K \mathcal{R}(\phi^{j-k}_{\theta}, \theta)\} $
    \State $j \gets j + 1$
    % \EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}