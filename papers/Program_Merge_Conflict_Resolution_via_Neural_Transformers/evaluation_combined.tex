\section{Evaluation}

\subsection{Evaluation Metrics}

We evaluate \thistool{}'s performance of resolution synthesis in terms of precision and accuracy of string match (modulo whitespaces or indentation) to the user resolution extracted from real-world historical merge resolutions. This approach is rather restrictive as a suggested resolution might differ from the actual user resolution by, for instance, only the order of statements, being semantically equivalent otherwise. As such, this evaluation approach gives a lower bound of performance.

We evaluate \thistool{} and compare it to baselines and existing approaches using two metrics, precision at top-k and accuracy at top-k.  
Since \thistool{} is a neural approach, it may provide more than one suggestion, which we rank according to the associated prediction probabilities.
In addition, because we filter out resolution suggestions that are not syntactically valid, it may provide no suggestions in rare cases.  
Accuracy at top-1 indicates the percentage of total conflicts for which \thistool{} produces the correct resolution as its top suggestion. Precision at top-1 indicates how often (as a percentage) the top suggestion is correct when the \thistool{} provides any suggestions at all.  As a concrete example, if a tool produces a resolution suggestion for 50 out of 100 conflicts and 40 of the suggestions matched the actual historical user resolution, then the precision would be 80\% (40/50), but the accuracy would be 40\% (40/100).  Precision at top-k indicates how often the correct resolution is found in the top-k suggestions and Accuracy at top-k is analogous. When ``top-k'' is omitted from the metric name (e.g. just "Precision") then k is 1.

%In addition to the precision and accuracy, we also report the fraction of syntactically correct (or parseable) source code suggestions to filter out merge resolutions with syntax errors. 

%\chris{we only report syntactic correctness in table 3 and no others.  I suggest we remove it, as reviewers will ask why it is so rarely present and it's not critical to the perf. evaluation.}

\subsection{Baseline Models}
\label{sec:baselines}

\subsubsection{Language Model Baseline}

Neural language models (LMs) have shown great performance in natural language generation~\citep{gpt2, sellam-etal-2020-bleurt}, and have been successfully applied to the domain of source code~\citep{10.5555/2337223.2337322, gptc, feng-etal-2020-codebert}. We consider the generative pretrained transformer language model for code (GPT-C) and appeal to the naturalness of software~\citep{naturalness} to construct our baseline approach for the merge resolution synthesis task. We establish the following baseline:
given an unstructured line-level conflict produced by \texttt{diff3}, we take the common source code prefix acting as user intent for program merge. We attempt to generate an entire resolution region token-by-token using beam search. As an ablation experiment, we repeat this for the conflicts produced with the token-level differencing algorithm (Fig.~\ref{fig:word1} shows details about prefix and conflicting regions).

\subsubsection{DeepMerge: Neural Model for Interleavings}

Next, we consider \textsc{DeepMerge}~\citep{Dinella2021}: a sequence-to-sequence model based on the bidirectional GRU summarized in section~\ref{sec:background}. It learns to generate a resolution region by choosing from line segments present in the input (line interleavings) with a pointer mechanism. We retrain the \textsc{DeepMerge} model on our TypeScript dataset.

\subsubsection{JDIME}
Looking for a stronger baseline, we consider \textsc{JDime}, a Java-specific merge tool that automatically tunes the merging process by switching between structured and unstructured merge algorithms \citep{apel2012structured}. Structured merge is abstract syntax tree (AST) aware and leverages syntactic information to improve matching precision of conflicting nodes.  We use the publicly available implementation~\citep{jdime}, and run JDime in semi-structured mode. 

\subsubsection{jsFSTMerge}
\citet{tavares2019javascript} implemented \jsfstmerge{} by adapting an off-the-shelf grammar for JavaScript to address shortcomings of \fstmerge{}~\cite{apel2012fstmerge} and modify its algorithm.
\jsfstmerge{} allows for certain types of nodes to maintain their relative order (\emph{e.g.}, statements) while others may be order independent (\emph{e.g.}, function declarations) even when sharing the same parent node.
For cases where \jsfstmerge{} produces a resolution not matching the user resolution, we manually inspect the output for semantic equivalence (e.g., reordered import statements).

\subsection{Results}
\label{sec:eval}


\noindent \textbf{RQ\scriptsize{1}: }\textbf{\rqOne}

To evaluate \thistool{} We first compare it to other neural approaches and to \texttt{diff3}. 
To be comprehensive, we evaluate at both the token level and the line level.  
We then compare \thistool{} to existing state of the art structured and semi-structured merge language-specific merge approaches.

\begin{table}[htb]
\small
\caption{Evaluation results for \thistool{} and various neural baselines calculated for merge conflicts in TypeScript programming language test set. The table shows top-1 precision and accuracy metrics.}
\centering
\begin{tabular}{lllllllllll} \toprule
\textbf{Approach}  & \textbf{Granularity} & {\textbf{Precision}} & {\textbf{Accuracy}} \\ \midrule
LM   & Line  &3.6 & 3.1 \\      % line 
DeepMerge & Line  & 55.0 & 35.1  \\ % Tyepscript aligned linearized
\midrule
\texttt{diff3} & Token & 82.4 & 36.1  \\
\midrule
LM & Token  & 49.7  & 48.1    \\      % token
DeepMerge & Token  & 64.5 & 42.7 \\ % Tyepscript aligned linearized
\thistool{} & Token  & \textbf{69.1} & \textbf{68.2}  \\  
\bottomrule   
\end{tabular}
\label{tab:baselines_left}
\end{table}




As seen in Tab.~\ref{tab:baselines_left}, language model baselines' performance on merge resolution synthesis is relatively low, suggesting that the naturalness hypothesis is insufficient to capture the developer intent when merging programs. This is perhaps not surprising given the notion of precision that does not tolerate even a single token mismatch. 

\thistool{} is based on two core components: token-level \texttt{diff3} and a multi-input neural transformer model. The token-level differencing algorithm alone gives a high top-1 precision of 82.4\%, with a relatively low accuracy of only 36.1\% (i.e., it doesn't always generate a resolution suggestion, but when it does, it is very often correct). Combined with the neural transformer model, the accuracy is increased to a total of 68.2\%. Note, as a deterministic algorithm token-level \texttt{diff3} can only provide a single suggestion. 

DeepMerge precision of merge resolution synthesis is quite admirable, showing 55.0\% top-1 precision. However, it fails to generate predictions for merge conflicts which are not representable as a line interleaving. This type of merge conflict comprises only roughly one third of the test set, resulting in an accuracy of only 35.1\% which is significantly lower than \thistool{}.

% \negar{should we mention both the token and the line-lvl precision of DeepMerge here?}.
% \alexey{Separated this as an ablation experiment below...}
% \negar{This number (63.8\%) is not in the table, and hence, a bit confusing. We can only mention the low accuracy and its reason, i.e., line interleaving.} 
% \alexey{Negar: I edited the paragraph above per your suggestion. Please take a look and make changes if needed}

As an experiment, we also evaluate the DeepMerge model in combination with the token-level \texttt{diff3}. This enables DeepMerge to overcome the limitation of providing only resolutions comprised of interleavings of lines from the conflict region by interleaving tokens instead. As seen in Tab.~\ref{tab:baselines_left} (DeepMerge with Token granularity) overall accuracy improves from 35.1\% to 42.7\%. However this still falls short of \thistool{} with precision that is 5\% less (64.5\% vs. 69.1\%) and accuracy that is 25\% less (42.7\% vs 68.2\%). 

%Second, note that DeepMerge can only produce resolution suggestions for merge conflicts that are representable as an interleaving of lines from the conflict region. As an experiment, we tried evaluating performance when restricting the test set to this type of merge conflicts only. Using this smaller test set of resolutions that DeepMerge could provide correct resolutions for, \thistool{} has 70.6\%  precision (not shown in Tab.~\ref{tab:baselines_left}) and Deepmerge has 55.0\%.


\begin{table}[htb]
\small
\caption{Comparison of \thistool{} to \jdime{} and \jsfstmerge{} semi-structured merge tools. The table shows the percentage of conflicts in which the tool produces a resolution, the top-1 precision of produced resolutions, and the overall top-1 accuracy of merge resolution synthesis. \jdime{} evaluation is on a Java data set and \jsfstmerge{} is on a JavaScript data set.}
\vspace{-4pt}
\centering
\resizebox{0.98\columnwidth}{!}{%
\begin{tabular}{lllllll} \toprule
\textbf{Approach} & \textbf{Language} & \textbf{\% conf. w/ res.} & \textbf{Precision} & \textbf{Accuracy} \\ %& \textbf{Syn (\%)} \\ 
\midrule
\jdime{} & Java & 82.1 & 26.3 & 21.6 \\ %&  90.9 \\
\thistool{} & Java & \textbf{98.9} & \textbf{63.9} & \textbf{63.2} \\ \midrule % & \textbf{98.3} \\ \midrule
\jsfstmerge & JavaScript & 22.8 & 15.8 & 3.6 \\ %& 94.4 \\
\thistool{} & JavaScript & \textbf{98.1} & \textbf{66.9} & \textbf{65.6} \\ %& \textbf{97.4} \\ % JavaScript
\bottomrule
\end{tabular}%
}
\label{tab:baselines_right}
\vspace{-6pt}
\end{table}

We also compared \thistool{} to state of the art structured and semi-structured merge tools.  Since both \jdime{} and \jsfstmerge{} are language-specific, to compare against \thistool{}, we use our dataset's corresponding language-specific subset of conflicts (leading to slightly different results for \thistool{} on Java and JavaScript).

As can be seen from Tab.~\ref{tab:baselines_right}, \jsfstmerge{} only produces a resolution for 22.8\% of conflicts and when a resolution is produced by \jsfstmerge{}, it is only correct 15.8\% of the time, yielding a total accuracy of 3.6\%. 
This is in line with the conclusions of the creators of \jsfstmerge{} that semi-structured merge approaches may not be as advantageous for dynamic scripting languages~\citep{tavares2019javascript}. Because \jsfstmerge{} may produce reformatted code, we manually examined cases where a resolution was produced but did not match the user resolution (our oracle).  If the produced resolution was semantically equivalent to the user resolution, we classified it as correct.

To compare the accuracy of \textsc{JDime} to that of \thistool{}, we use the Java Test data set introduced previously and complete the following evaluation steps: \textsc{JDime} does not merge all conflicts and generates a resolution for 82.1\% of conflicts. This is in line with related work reporting that as much as 21\% of files cannot be merged~\cite{apel2012structured}. Therefore, first, we identify the set of merge conflict scenarios where \texttt{diff3} reports a conflict and \textsc{JDime} produces a non-conflicted merge. 
% \Shuvendu{Why not cases where JDIME does not resolve the conflict?}, \sarah{This is because JDime only resolves conflicts 35\% of the time. If we were to consider all conflicts, accuracy would be lower than 15\% as reported in the ICSE paper. I believe the numbers here are from ICLR submission, so this decision is persisting from what we decided earlier. In the ICSE paper we did this differently. We reported the percentage of conflicts with predicted resolutions, precision as the number of correctly resolved conflicts over the total number of conflicts for which an approach attempts a resolution. Accuracy is the number of correctly resolved conflicts over the total number of conflicts. } 
When comparing the \textsc{JDime} output to the actual historical user-performed merge conflict resolution, we do not use a simple syntactic match.  As a result of its AST matching approach, code generated by \jdime{} is reformatted, and the original order of statements and other constructs are not always preserved. 
%In addition, source code comments that are part of conflicting code chunks are not merged. This makes a simple syntactic comparison is too restrictive, and \jdime{} merge output can still be semantically correct. 
In an effort to accurately and fairly identify semantically equivalent merges, we use GumTree \cite{FalleriMBMM14}, an AST differencing tool, to identify and ignore semantically equivalent differences between \textsc{JDime} output and the user resolution, such as reordered method declarations. When \textsc{JDime} produces a resolution, it generates a semantically equivalent match 26.3\% of the time, with an accuracy of 21.6\%. 
%we have a more accurate baseline comparison between the number of semantically equivalent merges generated by \jdime{} and \thistool{}.

\noindent \textbf{RQ\scriptsize{2}: }\textbf{\rqTwo}
One goal of our approach is to be able to handle multiple languages with minimal effort.  For \thistool{} to be able to provide merge resolution suggestions for conflicts in a particular language, it needs three things.  First, a tokenizer in that language, which allows us to split the source text into tokens for processing.  Second, a parser in that language, which allows us to filter out syntactically incorrect merge resolution suggestions. Third, a data set of merge conflicts and their user-resolutions to train \thistool{}.  Fortunately, tokenizers and parsers for nearly any language are readily available (e.g., we use GitHub's tree-sitter for this) and repositories that use a particular language can be easily identified (e.g. on GitHub) and mined for conflicts and resolutions.

We incorporated tokenizers and parsers into \thistool{} for JavaScript, TypeScript, Java, and C\# and gathered merge conflict data for these languages as described previously.  
Note that both comments and strings in these languages are represented as single tokens and can be quite long.  
Therefore we further split these tokens on whitespace.
Tab.~\ref{tab:mergebert_summary} shows the detailed evaluation results of \thistool{} broken down by language. The top section of results shows performance when \thistool{} is trained on data for that specific language.  
The bottom section shows performance for each language when \thistool{} is trained on a data set comprising data for all languages (we term this the \emph{multilingual} model).
Note that for the language specific models, performance is fairly consistent across all four languages with Top-1 precision ranging from 63.9\% to 69.1\% and Top-1 Accuracy ranging from 63.2\% to 68.2\%. We also find that over 97\% of \thistool{} suggestions are syntactically correct across all programming languages. 

We had no a priori expectations of the performance of the multilingual model, as it is trained on more data, which could lead to improvement, but it is not language specific, which could lead to poorer results.
%As can be seen, the multilingual variant of \thistool{} yields $63.6-68.5\%$ top-1 and $75.2-76.8\%$ top-3 precision of verbatim match and relatively high recall values.
Overall, the multilingual variant of the model generates results that are just slightly below the monolingual versions.
Thus performance on one language isn't improved by adding more data in other languages.
Thus, from a pragmatic perspective, if one chooses to simplify their use of \thistool{} by training just one model instead of one model per language, then the performance takes only a negligible hit.



\begin{table}
\small
\caption{Detailed evaluation results for (top) monolingual JavaScript, TypeScript, Java, and C\# models, and (bottom) multilingual \thistool{} model trained on all four programming languages. The table shows precision and accuracy of merge resolution synthesis.}
\vspace{-4pt}
\centering
\begin{tabular}{llllllllllll} \toprule
\textbf{Test (Train) Languages} & \multicolumn{2}{c}{\textbf{Precision}} &  \multicolumn{2}{c}{\textbf{Accuracy}}  \\ \cmidrule{2-3} \cmidrule{4-5} 
& Top-1 & Top-3 & Top-1 & Top-3 \\ 
\midrule
JavaScript (JS)  & 66.9 &75.4 & 65.6& 73.9 \\ %& 97.4 \\    % monolingual
TypeScript (TS)  & 69.1 &76.6 & 68.2& 75.6 \\ %& 97.0 \\   % monolingual
Java (Java)  & 63.9 &76.1 & 63.2 &75.2 \\ %& 98.3 \\  % monolingual
C\# (C\#)  & 68.7 &76.4 & 67.3& 74.8 \\ %& 98.3 \\   % monolingual
\midrule
JavaScript (JS, TS, C\#, Java)  & 66.6& 75.2 & 65.3 &73.8 \\ %& 97.4 \\   
TypeScript (JS, TS, C\#, Java)  & 68.5 &76.8 & 67.6 &75.8 \\ %& 96.9 \\   
Java (JS, TS, C\#, Java)  &  63.6 &76.0 & 62.9& 75.1 \\ %& 98.2 \\  
C\# (JS, TS, C\#, Java)  & 66.3 &76.2 & 65.1 &74.8 \\ %& 98.3\\  
\bottomrule
\end{tabular}
\label{tab:mergebert_summary}
\vspace{-4pt}
\end{table}

\noindent \textbf{RQ\scriptsize{3}: }\textbf{\rqThree}

We conduct an ablation study on the edit type embedding to understand the impact of edit-awareness of encoding on the model performance. As shown in Tab.~\ref{tab:edit_ablation}, use of the edit type embedding improves  \thistool{} from 63\% to 68\%.
\begin{table}[htb]
\small
\caption{Evaluation results for \thistool{} and the model variant without edit-type embedding for merge conflicts in TypeScript programming language test set. The table shows top-1 precision and accuracy metrics.}
\centering
\begin{tabular}{lllllll} \toprule
\textbf{Approach} & \textbf{Precision} & \textbf{Accuracy}   \\ 
\midrule
w/o edit type embeddings  & 65.2 & 63.1  \\
\thistool{} w/ edit type embeddings & \textbf{69.1} & \textbf{68.2}  \\ 
\bottomrule
\end{tabular}
\label{tab:edit_ablation}
\end{table}

%\negar{I have a general suggestion: We can move Tab. 4 up to the beginning of the results section. This way, for RQ1, which is about efficiency, we can mention and briefly discuss Tab. 4 as the detailed evaluation results of MergeBERT and continue with the comparison of MergeBERT and other techniques using Tab. 2 and 3. Then for RQ2, we can mention Tab. 4 again and further discuss the monolingual and multilingual results.}