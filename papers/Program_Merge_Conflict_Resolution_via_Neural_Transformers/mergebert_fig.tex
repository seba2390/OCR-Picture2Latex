\begin{figure*}
\begin{center}
    \includegraphics[width=.85\textwidth]{images/mergebert2.pdf}
\caption{An overview of the \thistool{} architecture. From left to right: given conflicting programs $\mathcal{A}$, $\mathcal{B}$ and $\mathcal{O}$ token-level differencing is performed first, next, programs are tokenized and the corresponding sequences are aligned ($a|_o$ and $o|_a$, $b|_o$, and $o|_b$). We extract edit steps for each pair of token sequences ($\Delta_{ao}$ and $\Delta_{bo}$). Four aligned token sequences are fed to the multi-input encoder neural network, while edit sequences are consumed as edit type embeddings. Finally, encoded token sequences are aggregated into a hidden state which serves as input to classification layer.}
\label{fig:mergebert}
\end{center}
\vspace{-8pt}
\end{figure*}
