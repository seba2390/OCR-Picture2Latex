\section{Method Analysis}
\label{sec:crowd}
Due to the absence of pre-defined ``ground truth'' and lack of standard metrics in the nascent area of motion transfer for quantitative assessment, we empirically evaluated our approach by (1) analyzing the impacts introduced by each component, (2) comparing the automatically generated result from different styles of GIFs and typefaces, and (3) conducting questionnaire studies to understand how general people perceive the outputs based on several cases.


\subsection{\rev{Effects of Components}}
\label{sec:ablation}
\rev{We evaluated the effect of each component in the workflow} to analyze how our adaption to FOMM and the introduced human interventions can improve the generated result, including local position optimization, vectorized text representation, key point correction, and glyph refinement.

% 

\paragraph{Local Position Optimization}
The position optimization module is introduced to preserve the local shape of each glyph better.
\autoref{fig: ablation1} demonstrates three motion transfer results with $\alpha$ set to $0, 2, 4$. 
$\alpha$ is the weight of $\mathcal{L}_{\text{glyph}}$, which controls the degree of preservation of local shape. As can be seen, when alpha is set to 0, i.e., without local position optimizing, some local parts of the glyphs are unsatisfactory, such as the ``p'' in ``sleep'', the ``t'' and ``k'' in ``thanks'', and the ``a'' and ``k'' in ``wakey''.
With the increment of $\alpha$, the glyph becomes smoother. However, when alpha is too large, it may cause too much preservation of the original glyph and result in a loss of motion, for example, the ``w'' and ``k'' of ``wakey'' when $\alpha = 4$. Through experiments, we find a suitable default value of 2.
% which is the weight of $\mathcal{L}_{\text{glyph}}$.
% Without position optimization, due to changes in the relative position of points after local affine transformation, adjacent letters may become entangled and compressed together, such as the text ``orte'' shown in \autoref{fig: ablation2}, with excessive deformation of individual letters and narrow separation between them. 
% However, with position optimization, the independence and integrity of the letters are enhanced, the deformation of individual letters is more moderate, and the boundaries between letters are clearer and more distinct, resulting in stronger legibility.


\label{sec:case}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/case-study.pdf}
  \caption{A comparison with the FOMM model~\cite{siarohin2019first}. Our approach operates on the control points of vectorized text, which improves legibility.}
  \label{fig: case}
  \Description{Figure 5 contains three rows of animation keyframes. From top to bottom, they are the driving GIF, the rasterized animated text generated by the FOMM model, and the vectorized kinetic typography generated by our method.}
    % \vspace{-0.1in}
\end{figure}
\paragraph{Vectorized Text Representation.}
% Users can select anchor GIFs with expected semantics to drive the corresponding generations of animated text, such as emotions like anger. 
Instead of directly employing existing image-based motion transfer models, our approach operates on the control points of text glyphs.
As shown in \autoref{fig: case}, the output results based on pixels are not stable enough.
For example, in the pixel-based glyph generated by FOMM, the letter ``y'' of ``angry'' has breaks and extra noisy strokes.
As the anchor GIF is hardly the targeted category of kinetic typography, using FOMM for motion transfer does not produce satisfactory results. 
In contrast, our method better preserves the integrity and legibility of the glyph and produces a more stable frame sequence.



\paragraph{Key Point Correction.}
The key points $\{X_i^f\}$ detected by the model may not always be accurate, which can result in unexpected deformations in the generated animated text that rely on these key points. 
Our approach allows users to interactively correct the key points, thereby obtaining a more desirable generation that aligns with their expectations.
As shown in \autoref{fig: ablation2}, by analyzing the preceding and following frames, we can find that in the fourth frame of the pixel-based animated text image sequence generated by FOMM, the key point marked in red noticeably shifts towards the right. 
This caused an excessive deformation towards the right in the lower right part of the letter ``W'' in the vector-based animated text generated with this key point. 
By dragging the key point towards the left to an area consistent with the preceding and following frames, the deformation of the generated glyph appears more reasonable and smoother.


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/alignment.pdf}
  \caption{Comparison of the output before (top) and after (bottom) key point correction. The automatic mapping fails when the highlighted red key point shifts to another location in two consecutive frames, which yields distortion in the text.}
  \label{fig: ablation2}
  \Description{In Figure 5, the left part shows the key points before and after correction, and the right part shows the corresponding generated vector text.}
    % \vspace{-0.1in}
\end{figure}


\paragraph{Glyph Refinement.}
The control point sequence $\{C_j^f\}$ can be manually updated for fine-grain refinement.
Through the authoring interface, users are supported to drag the control points and preview the result immediately.
As shown in \autoref{fig: ablation3}, the sharp corners inside the first letter ``p'' affect the glyph aesthetics, where the highlighted left line segment is tilted to the left and needs to be adjusted. 
By moving the three control points in the sharp corner area to the right and adjusting the relative positions of the three points, the refinement process is done. 
It is evident that the updated glyph achieves a better effect through simple and immediate dragging.


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/glyph-refinement.pdf}
  \caption{Manual refinement of the text control points. By dragging the distorted control points, one may intuitively refine the output kinetic typography at a fine-grained level.}
  \label{fig: ablation3}
  \Description{In Figure 7, the left and right are the glyphs before and after the glyph refinement, and the center shows the control points that were adjusted.}
    % \vspace{-0.1in}
\end{figure}




\subsection{\rev{Generalizability}}
\label{sec:generalizability}
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/complexity.pdf}
  \caption{\rev{A comparison of results from driving GIFs of different complexities in background and target object(s).}}
  \label{fig: complexity}
  \Description{In Figure 8, there are four blocks displaying keyframes of the generated results of “UIST” from different complexities of GIF inputs.}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/font.pdf}
  \caption{\rev{A comparison of results from typefaces of different categories and average number of control points for the 26 English alphabets (Avg. \#Control). The first column shows four key frames of the driving GIF. Each column in the rest shows the font information and the corresponding motion transfer result.} }
  \label{fig: font}
  \Description{In Figure 9, there are eleven columns. The first column shows four keyframes of the driving GIF. The rest ten columns show corresponding keyframes based on different types of typefaces. Three additional rows in each column present the class of the typeface, the name of the typeface, and the average number of control points for 26 English alphabets in the typeface.}
\end{figure*}


\rev{
Drawing from our experience, we reflect on the generalizability of our approach in terms of the driving GIF and the input typeface.

In general, \tool{} can accommodate input GIFs with a clean background and a simple-shape moving rigid body, such as instances from the Puppy Maltese dataset.
This is because our implementation adopts the pre-trained FOMM model based on the MGif dataset, which features a white background and one cartoon animal. 
Seen from \autoref{fig: complexity}, the automatic key point extraction may fail and cause large distortion when there are multiple moving objects, or the moving object exhibits complex patterns.
A complex background also threatens the reliability of extracted key points in the driving GIFs, such as a clip from natural videos.
While these issues can be addressed by manual correction, we also note that the motion trajectory extraction module can be improved by unsupervised training on a larger dataset with representative cases or using a large universal model.

As for the input fonts, our approach empirically performs well for typefaces with more than 5 control points in a glyph.
The more control points encapsulated in the typeface, the more likely that the position optimization can maintain its legibility.
\autoref{fig: font} showcases the automatic generation results for ten typefaces of common classes.
Typefaces with the most control point number also yield the most smooth results, including Fredericka and Cedarville.
However, there might be strong deformation for handwriting-styled typefaces, potentially due to their high flexibility.

}

\subsection{Questionnaire Study}




We conducted two questionnaire studies to evaluate the effectiveness of our approach.
Specifically, we seek to understand (1) whether our approach convincingly transfers the motion, and (2) to what extent the semantics of the original GIF can be preserved.

\subsubsection{Setup} The questionnaires are distributed on Qualtrics.
\rev{Participants are required to complete Study I before Study II}. And the questions appear in a random order in each study. 
We used meaningless pseudo-words from the Lorem Ipsum corpus~\cite{lorem} as input text in order to minimize the influence of text content.
\rev{For driving GIFs, we used the Puppy Maltese dataset to generate cases. To avoid confounding effects, the driving GIFs are non-repetitive. And we employed the typeface ``Akronim'' for it has over 300 control points, which may lead to satisfying results without human intervention and therefore suitable for our scenario requiring batch generation.}

\paragraph{Study I: Motion Transfer}
The first study aimed to evaluate the overall quality of the output kinetic typography.
As no quantitative metric is available in our task, we obtained subjective assessments by asking the participants to rate the similarity between the driving GIF and the output kinetic typography and their aesthetics.
On the one hand, the similarity between the source and target is the primary goal in motion transfer.
On the other hand, aesthetics is a common pursuit in animation design.

% 丢去结论 First, whether the motion of the anchor GIF is effectively transferred, thus validating the feasibility of the technique; 
% second, whether the animated text is visually appealing, confirming the applicability of the generated results.


% Following the opinion that it is just as valid to believe that an artwork is aesthetically pleasing without providing a specific reason, as it is to attribute its aesthetic appeal to a particular visual quality~\cite{AestheticPrinciples}, we take ``aesthetically pleasing'' as the aesthetic metric, and use "move similarly" to assess the motion.
A sample question is shown in \autoref{fig: crowd} A.
When designing the questionnaires, we tried to familiarize participants with simple and concrete questions.
For instance, we asked whether a GIF is ``aesthetically pleasing'' to align participants' appraisal of the aesthetic property to their feelings~\cite{AestheticPrinciples}\rev{.}
For each question, the animated text and the corresponding anchor GIF are displayed, and participants are asked to rate them on a 7-point Likert scale (0--strongly disagree to 6--strongly agree) for aesthetics and motion similarity, respectively. 
% From the dataset, we selected 6 driving GIFs labeled corresponding to Ekman's six basic emotions~\cite{ekman1999basic}, and the other 14 driving GIFs are selected randomly, making a total of twenty GIFs.
20 driving GIFs were randomly selected.
For each driving GIF, we set two questions, one for the baseline--rasterized kinetic typography generated with FOMM, and one for the experimental group--vectorized kinetic typography with our approach.
Therefore, a questionnaire consists of 40 questions.
Considering the influence of the font, participants are asked to rate the aesthetics of the static font before viewing the main body of the questionnaire.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/crowd.pdf}
  \caption{Example questions and results in two questionnaire studies (N=33). (A) Study I: Subjective ratings for the aesthetic property of the output kinetic typography and similarity between the target and source GIF. (B) Study II: Perception of emotions underlying kinetic typography. (C) The average ratings and standard errors of cases in Study I, where our approach outperforms the baseline FOMM in motion similarity and result aesthetics. (D) The average ratings and standard errors of the pairwise (pleasure, arousal) ratings for sample GIFs, where the driving GIF and corresponding kinetic typography posit in adjacent areas. \rev{Ratings of the same emotion are encoded with colors of a similar hue.}}
  \label{fig: crowd}
  \Description{Figure 10 contains four figures demonstrating the questionnaire design and results. (A) A snapshot of Questionnaire I. The top shows the anchor GIF and the corresponding kinetic typography. The bottom is a 7-point scale for rating motion similarity and aesthetic pleasure. (B) A snapshot of Questionnaire II. The top shows a GIF, and the bottom is a slider scale for rating pleasure and arousal. (C) The results of Questionnaire I show that our method is superior to FOMM in terms of aesthetic appeal and movement similarity. (D) The results of Questionnaire II show that the anchor GIF of a certain emotion and its corresponding generated kinetic typography are located closely on a two-dimensional coordinate system with pleasure and arousal as the x and y axes, respectively.}
    % \vspace{-0.1in}
\end{figure*}
\paragraph{Study II: Semantic Preservation}
% user emotion
% picture emotion
% lorem
% 防止上下题情感的影响，random

The second study aimed to verify whether the learned animation can preserve the semantics of the driving GIF.
We tackled the problem from the perspective of emotion, which is an integral part of semantics.
Specifically, we focused on Ekman's six basic emotions~\cite{ekman1999basic}, \ie~sadness, happiness, fear, anger, surprise, and disgust.

\autoref{fig: crowd} B illustrates a sample question. We adopted the Affective Slider~\cite{betella2016affective} for participants to self-report their emotions, which consist of two dimensions: pleasure and arousal from the extent 1 to 100.
Pleasure means the degree of positivity or negativity of an individual's emotional state.
Arousal corresponds to the level of physiological activation or stimulation in an individual's emotional state.
For each basic emotion, we selected two GIFs as anchors \rev{according to the pre-defined labels in the dataset.}
%resulting in a total of 12 animations used in the questionnaire.
A question comprises one kinetic typography, where we required participants to assess the perceived emotions.
Hence, there were 12 questions.

% The corresponding animated text was generated for each anchor GIF, resulting in twelve questions assessing the emotions conveyed by the twelve animations using the AS scale. 
% The intention was to compare the consistency of the emotion expressed in the source motion pictures and the animated text driven by them.




\subsubsection{Participants}
We recruited participants from a local university by posting advertisements on social media. Each participant is paid £3.5 for completing the questionnaire.
A total of 33 people signed up for the questionnaire study. 
Most participants were between 18--24 years old, with 15 females and 18 males.
%indicating that the study primarily targeted a younger demographic 
% who is more familiar with contemporary digital trends and communication.
% The gender distribution among the participants showed a male-to-female ratio of 4:6, suggesting a fairly balanced representation of both genders, albeit with a slightly higher proportion of female participants.
In addition, all participants reported using emojis frequently in daily communication, where 
14 (42\%) reported to use emojis \textit{very often}.

\subsubsection{Result Analysis} % The results initially validated our approach. 
\paragraph{Study I. Motion Transfer}
Participants spent an average of 10.4 minutes in completing the 20 questions (std=6.2, ranging from 3.5 to 28).
We deemed all the responses valid.
% \rev{The average aesthetic score on the static text was 3.79.}
Seen from \autoref{fig: crowd} C, participants generally recognized the aesthetics and the similarity of movement between the driving GIFs and the animated text in our approach.
\rev{For our approach, t}he average score on the aesthetics was 4.71 (std=1.21), and the motion similarity was 4.85 (std=1.06), where both scores exceeded 4, \ie, somewhat agree, on the 7-point scale. \rev{For the baseline, the average aesthetic score was 2.34 (std=1.60) and the average similarity score was 3.31 (std=1.59).}
Compared to the baseline method, our approach obtained significantly higher scores (significance level $\alpha=0.001$, Student's t--test), which suggests that our method outperforms the FOMM in terms of motion transfer, making the generated animations more visually appealing and more similar to the source motion.
This finding echoes our ablation study on the vectorized text representation, where we identified certain glitches in the pixel-based methods.
% In addition, the average score for the font was 3.788, indicating that the participants found the font aesthetically pleasing. 
Moreover, after adding the dynamic motion, the aesthetics of the text improves compared to static text with an average aesthetics score of 3.79, further validating the effectiveness of our method.
% As for user comments, several participants provided comments stating that our model performed much better than the baseline FOMM. 
% These comments support the quantitative results and emphasize the superiority of our model in terms of motion transfer and visual appeal.

In summary, Study I verified that our model could achieve better motion transfer compared to the baseline. 
The improved aesthetics and motion similarity scores, along with the user comments, demonstrated the effectiveness and applicability of our method in generating visually appealing and motion-consistent animated text.
% \begin{table}[ht]
% \centering 
% \begin{tabular}{ lll } 
% \toprule
%  & aesthetical pleasure & motion similarity\\
% \midrule
% FOMM & 2.34 ($\pm$1.60) & 3.31 ($\pm$1.59)\\
% Ours & \textbf{4.71 ($\pm$1.21)} & \textbf {4.85 ($\pm$1.06)} \\
% \bottomrule
% \end{tabular} 
% \caption{mean and std} %title of the table 
% \label{tab:evaluation} 
% \end{table} 
\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/workshop.pdf}
  \caption{Demonstrations in the workshop. (A) A video opening featuring synchronous animation of textual descriptions and the cartoon character. (B) A browser plugin for the automatic generation of kinetic typography based on our approach.}
  \label{fig: video}
  \Description{Figure 11 shows two usage scenarios labeled (A) and (B). (A) Six frames of the video opening made with kinetic typography generated by our method. (B) The browser plugin interface with the top input area above and the bottom preview and output area.}
    % \vspace{-0.1in}
\end{figure*}
\paragraph{Study II. Emotion Preservation}
Participants spent an average of 8.5 minutes to complete the questionnaire, with a maximum of 30 minutes and a minimum of 2 minutes (std=8.9).
The results of study II are shown in \autoref{fig: crowd} B, where the scatterplot maps the average scores of the (pleasure, arousal) pairs.
The attached error bars indicate the standard error of each dimension with their lengths, where vertical for arousal and horizontal for pleasure.

Inspecting the diagram, we could see that the data points for the same emotion under both the driving GIF and Text are projected on adjacent areas, revealing that the expression of emotion has also been successfully transferred through the motion transfer. 
Furthermore, different emotion classes largely varied.
For example, there is a significant difference between happy and sad emotions, demonstrating the effectiveness of our method in preserving the emotional semantics of the anchor GIFs.
One could see that the emotions of disgust and anger are very close, and the demarcation is not obvious, with only the ordering of pleasure being altered. 
This suggested that the emotional expressions in these two emotions might be similar, making it harder for users to differentiate them clearly.
In addition, the variances in the arousal dimension were generally greater, possibly due to the users' perception of arousal being more ambiguous or subjective, leading to a wider range of responses.
In contrast, the arousal and pleasure scores for the text are more neutral (around 50 points). 
Although the emotional expression has been learned and transferred, it is not as strong as in the source motion pictures. 
It might result from the limitations of the method or the inherent difference between text and the figure-like domain in representing emotions.



In summary, results from Study II showed that our method could effectively preserve the emotional semantics when transferring animations from the driving GIF to a text. However, some emotions may not be as distinct as they are in the original anchor GIFs. And users' perceptions of arousal might be more ambiguous. \rev{While the questionnaire shows the success of emotion transfer on the particular typeface being used, more studies are needed to validate similar mechanisms for other fonts, as we did not eliminate the influence on emotion perception from the typeface.}


% Based on this analysis, the participant group primarily consisted of young adults with diverse gender representation and a high familiarity with emojis. 
% This demographic might have contributed to a better understanding of the emotions conveyed by the animations and the effectiveness of the motion-transfer method in preserving the semantics of the anchor GIFs. 
% However, it would be beneficial to include a broader age range in future studies to ensure the generalizability of the results across different age groups.