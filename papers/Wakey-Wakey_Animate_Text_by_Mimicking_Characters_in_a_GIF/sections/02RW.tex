\section{Background \& Related Work}
In this section, we provide background information on typography and review existing research on kinetic typography, text stylization, and guided animation generation with an anchor.

\subsection{Preliminaries on Digital Typography}
Typography is defined as the art and technique of organizing text in a way that is easy to read, comprehend, and visually pleasing while presented.
In general, the visual appearance of a digital letter is determined by its \textit{font}, which is a particular size, weight, and style of a \textit{typeface}.
The typeface is a set of designed characters or letters, named glyphs, such as \rev{\texttt{Courier New}, {\fontfamily{ptm}\selectfont Times New Roman}, and {\fontfamily{pbk}\selectfont Bookman Old}}.
Internally, a typeface is represented in the raster domain or vector domain.
As bitmap fonts may become distorted or blurred with mosaic-like jagged edges at high resolution, we chose to adopt a vector-based typeface--the TrueType~\cite{penney1996truetype} font, which describes glyphs with quadratic bezier curves.


\subsection{Kinetic Typography}
Kinetic typography enriches animated user interface~\cite{chang1993animation} and digital media, which has received scholarly interest since the 1990s~\cite{shannon1998kinetic}. Most recently, Xie~\ea~\cite{xie2023emordle} summarized a design space of kinetic typography concerning changes in style, shape, position, and scale.
Compared with static text, kinetic typography is more competent in guiding attention~\cite{borzyskowski2004animated, minakuchi2008kinetic} and communicating emotions or semantics with the paralinguistic clues underlying animation~\cite{malik2009communicating, Lee07EmotiveCaptioning}.
Accordingly, there has been a series of works seeking to lower the burden of creating kinetic typography.
Kinetic Typography Engine~\cite{lee2002engine} set the basis of modern animation software (\eg, Adobe After Effects~\cite{aftereffects} and TypeMonkey~\cite{typemonkey}) with frame-based low-level specifications and a library of common effects.
The specification concerns text properties like position, rotation, \etc~
And the library was composed of functional time filters like oscillation.
TextAlive~\cite{kato2015textalive} featured kinetic typography synchronized with audio signals in video editing.

A stream of work investigated tools for average users rather than professional designers, where reducing efforts in animation configurations is a primary goal.
These works normally predefined a suite of animated effects and support selection or automatic matching under various contexts.
Instant messaging has been most studied,~\eg,~\cite{gaylord2015atim,yeo2008kim,forlizzi2003kinedit, minakuchi2005kinetic}.
For instance, Kinedit~\cite{forlizzi2003kinedit} allowed users to integrate text animation into a line of words.
Minakuchi and Tanaka~\cite{minakuchi2005kinetic} conceptualized an automatic composer that analyzes the semantics of text and queries suitable motions from a static repository to amplify its meanings.
Other scenarios include emotional animation for lyric videos~\cite{vy2008enact} and dynamic display based on viewers' emotions~\cite{lim2022study}.
These works suffered from \rev{the number of animated effects provided}.
For instance, there is hardly any consideration of transforming the text shape, which is common in animation~\cite{thomas1995illusion} yet requires by-frame editing.
In comparison, our work takes advantage of the ubiquitous online memes or stickers and can scrape their animation schemes to a random text with reliable transformation on its outlines.


\subsection{Text Stylization}
Our work closely relates to the task of text style transfer and semantic typography in text stylization, an area widely studied in computer vision/graphics to make a given text visually appealing.

Similar to our workflow, text style transfer concerns transferring the style of a given source (font samples, natural image/video) into text.
Some works explored propagating the design of a few stylized letters to others, such as typeface geometry~\cite{phan2015flexyfont} and glyph decorations~\cite{wang2019typography}.
Other works followed the general workflow of neural style transfer~\cite{gatys2016image} and viewed style as local neural patterns of the input image/video,~\eg, \cite{mao2022intelligent, yang2021shape, men2019dyntypo}.
In contrast, our work deforms the vectorized outline of the text to match the reference GIF.
% 还需要补充说明我们的变形和他们的异同，特别是vector base的，生成式的 
% 有没有使用通用算法或者创新
% ~\cite{berio2022strokestyles}
% Existing motion transfer models are mostly designed specifically for a certain field, requiring prior knowledge of the object. 
We propose to vivify text by animating it in the way of a cartoon character, which diverges from their focus on learning image patch-based features.
Additionally, compared with kinetic typography, text style transfer emphasizes the artistic effect rather than an affective impact and typically produces static output.

Semantic typography amplifies the semantic meanings through visual cues in typography, which is also our goal.
Xu and Kaplan~\cite{xu2007calligraphic} proposed calligraphic packing, which deforms letters in a word to fit a given shape, which was improved by Zou~\ea~\cite{zou2016legible}.
In contrast to the intense deformation in letters, Word-As-Image~\cite{iluz2023word} stroke the balance of transformation on both sides, preserving the original font's style and legibility while ensuring the semantic implication, which was constrained by a pre-trained Stable Diffusion model~\cite{rombach2022high}.
Other approaches operate in the raster domain and leveraged external icons to replace parts of a text~\cite{tendulkar2019trick, zhang2017synthesizing}.
Our work differentiates from these works in that we imply semantics/emotion via animation of the text geometry rather than its static appearance, where the continuity between frames is considered.
To the best of our knowledge, this work is the first attempt to incorporate semantics in generating kinetic typography.

%%%%



\subsection{Guided Animation Generation}
As we aim to produce emotionally or semantically resonant kinetic typography based on a given text, relevant constraints need to be introduced in the animation generation process.
Some works infer motions directly from a given still image, concerning features like texture~\cite{chuang2005animating, kazi2014draco, lai2016data}, status in a motion cycle~\cite{xu2008animating}, periodic patterns~\cite{halperin2021endless}, \etc~
These methods are unsuitable for our goal because a text usually appears with no background and is not equipped with equivalently rich properties for motion inference.

Motion transfer has been a standard task in computer vision, which is to generate a video based on a source image and a driven video by learning the motion from the driving video while preserving the appearance of the source image.
Monkey-Net~\cite{siarohin2019animating} was the first model-free approach to transfer motions of arbitrary objects by aligning key points between the source and target domain.
FOMM~\cite{siarohin2019first} further enhanced it with local affine transformations on the extracted key points.
It is one of the state-of-the-art models and we adapted it to fit the vector-based text.
Specifically, we maintained the text legibility by regularizing motion anchors with the distance change in the Laplacian coordinate.
Our method shares the same idea to preserve the structural information as DAM~\cite{tao2022structure}, which introduced a latent root anchor to model the structure of objects.
Different from our focus on the text, most existing datasets and models concern talking heads and human posture (\eg,~\cite{chan2019everybody, zhou2020makeittalk, siarohin2021motion, hong2022depth, smith2023tog}) and do not yield desired results on texts where legibility matters (see \autoref{sec:method}).
Our exploration of kinetic typography contributes to a unique case of cross-domain motion transfer.


In addition to fully automatic approaches, mixed-initiative interfaces for animation authoring have been investigated.
Users may specify the intended effect with sketch-based demonstration~\cite{kazi2014draco, xing2016energy, kazi2016motionamplifiers, willett2018mixed}, gestures~\cite{arora2019magicalhands}, or examples~\cite{dvoroznak2017example}.
% For instance, Energy Brushes~\cite{xing2016energy} inferred the animation base on the user's sketch gestures that coarsely define the underlying forces, such as water drift or heat ascension.
% Willett~\ea~\cite{willett2018mixed} presented a tool for animating visual elements of a static picture, where users scribbled sample objects and their motion direction, and the system applied the motion to similar objects.
Pose2Pose~\cite{willett2020pose2pose} supports creating cartoon character animation by minimizing the design efforts through clustering postures and automatically matching the stylized postures designed by the artists to the driving video. 
Most similar to our work, Live Sketch~\cite{su2018live} leveraged motion transfer to let novice users create animated sketches, where users are required to define control points in both the source and target domain. 
Our approach also allows users to specify their desired animation effect through a GIF, which is easy to access online.
However, the key points in the driving video are automatically extracted and automatically mapped to the target domain.
For a finer-grain control, users can  adjust the extracted key points and internal parameters.


