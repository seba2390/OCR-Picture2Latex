\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/workflow.pdf}
  \caption{\rev{Overview of our approach. Inputs are a driving GIF and static text. The output is kinetic typography echoing the GIF's animations. The motion trajectory extraction module captures the key points in the driving GIF. The key point alignment module aligns the control points of the vectorized text to the key points. The position optimization module regularizes the text outline. And the User Interaction module allows human intervention on the intermediate key points and final results. }}
  \label{fig: overview}
  \Description{Figure 2 shows a conceptual diagram of the proposed method. The input is a GIF and a static text, and the output is kinetic typography. There are four modules. The motion trajectory extraction module receives the GIF and rasterizes static text as input and outputs the key points. This is input to the key point alignment module along with the control points obtained by vectorizing the static text, and output the deformed control points, which will be input to the position optimization module and output the final result. There is also a user interaction module pointing to key points and final control points for the adjustment.}
    % \vspace{-0.1in}
\end{figure*}
\section{Framework}
\label{sec:method}
In this section, we introduce a general framework for transferring the motion of a given GIF to a static text.
\subsection{Overview}
\autoref{fig: overview} illustrates our framework. 
\rev{The computational pipeline takes in a driving GIF and static text as input and outputs the kinetic typography. Users can tweak the intermediate key points and control points in the generated result (\textbf{C1}).}

Internally, the input text is represented in the TrueType format~\cite{penney1996truetype}.
It is first converted into an image and fed into a FOMM model~\cite{siarohin2019first} together with the anchor GIF to obtain the trajectory of the motion key points at each frame $X_i^f$, where the model identifies $N$ key points, and the GIF consists of $F$ frames, \ie, $i=1, ..., N$, $f=1, ..., F$.
The input text is also parsed to the initial control point set $C_j^0$ of its glyphs, with a total of $M$ control points, \ie, $j=1, ..., M$.
A local affine transformation is applied to both the initial control point set $C^0$ and the key point set trajectory $X^f$ to obtain the motion trajectory of the control point set $C_j^f$.
The updated control point trajectory $C_j^{\prime f}$ is attained through position optimization.
Subsequently, a vectorized glyph sequence is generated, culminating in the creation of animated text in vector form. 
Through the user interaction module, $X_i^f$ and $C_j^{\prime f}$ can be directly manipulated, and users can control some hyperparameters (\textbf{C2}).


\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{figures/interface.pdf}
  \caption{Wakey-Wakey: An authoring interface to interactively create anchor-based kinetic typography. There are three views: input view, correction view, and refinement view. (A) Input and preview the text, where font and color can be specified. (B) Upload a driving GIF. (C) Preview the matching of key points between the text and GIF at each frame. (D) Directly manipulate key points locations. (E) Fine-tune the hyperparameter. (F) Preview result GIF. (G) Refine the text control points at each frame. }
  \label{fig: interface}
  \Description{Figure 3 is a screenshot of the authoring interface for creating kinetic typography decomposed into seven areas. There are an input view, a correction view, and a refinement view. The input view allows for the input and customization of text (A) and the upload and preview of GIF (B). The correction view displays the key points in the animated image (C) and supports drag for correction (D). The refinement view contains parameter input boxes (E), supports control point editing (G), and displays the final result (F).}
    % \vspace{-0.1in}
\end{figure*}
\subsection{Motion Trajectory Extraction}
\label{subsec: fomm}
     We convert the static text to an image, and input it along with the anchor GIF to obtain the trajectory of motion key points. We adopted motion transfer to support the fast and flexible generation of kinetic typography.
As the object in the GIF usually differs from the text in shape, we need to separate the appearance and extract the motion trajectories of key points from the source GIF.
% Most existing motion transfer models are designed for a specific field, requiring prior knowledge of the object. 

FOMM~\cite{siarohin2019first} is applied for key points extraction in our task. It is a self-supervised method using a framework that decouples appearance and motion, which effectively enriches the possible transferable motions to support motion transfer within any object category.
% FOMM features its key point detector, which learns the transformation information of key points and their surroundings in a self-supervised way. 
To address the problem of large differences in key points between the driving frame $\mathbf{D}$ and the source image $\mathbf{S}$, the FOMM model introduces an abstract reference frame $\mathbf{R}$ and obtains $\mathcal{T}_{\mathbf{S}\leftarrow \mathbf{D}}$ by separately calculating $\mathcal{T}_{\mathbf{S}\leftarrow \mathbf{R}}$ and $\mathcal{T}_{\mathbf{D}\leftarrow \mathbf{R}}^{-1}$.
$$
\mathcal{T}_{\mathbf{S} \leftarrow \mathbf{D}}=\mathcal{T}_{\mathbf{S} \leftarrow \mathbf{R}} \circ \mathcal{T}_{\mathbf{R} \leftarrow \mathbf{D}}=\mathcal{T}_{\mathbf{S} \leftarrow \mathbf{R}} \circ \mathcal{T}_{\mathbf{D} \leftarrow \mathbf{R}}^{-1},
$$
where $\mathcal{T}_{\mathbf{A}\leftarrow \mathbf{B}}$ denotes the mapping from the image $B$ to $A$.

In the implementation, $\mathcal{T}_{\mathbf{S}\leftarrow \mathbf{R}}$ and $\mathcal{T}_{\mathbf{D}\leftarrow \mathbf{R}}$ are obtained by key points detection in $\mathbf{S}$ and $\mathbf{D}$, respectively, which supports us to extract the key point trajectories from both the source and generated pixel-based text GIFs. Either of the two trajectories of the key points can be applied to drive the subsequent generation, and we use $X_i^f$ to represent the selected key point trajectory for simplicity. The separate detection mode also supports the relative generation ($\mathcal{T}_{\mathbf{S}_t\leftarrow \mathbf{S}_1}$ to deform from the source image) following a similar mindset, in addition to the absolute way ($\mathcal{T}_{\mathbf{S}_t\leftarrow \mathbf{D}_t}$ to deform from the corresponding frame of the source GIF directly).
%making it easy to switch between absolute (directly deforming from the corresponding frame of the source GIF) and relative (deforming from the source image) generation way.

In our implementation, we utilized the pre-trained FOMM model on the MGif dataset\rev{~\cite{siarohin2019animating}}, which has shown good performance in key point detection. 
\rev{Following the pre-trained model, the features extracted for each frame are estimated independently, and the number of key points is set to 10.}
However, to further enhance the integration of emotion into generation and analysis, we gather and create a dataset of Puppy Maltese~\cite{linedog} with 77 emotional-labeled GIFs and use it to fine-tune the model. This is done to better cater to the needs of the subsequent case studies and user surveys.

Due to the difficulty of FOMM in achieving good performance in motion transfer across different categories of objects, we only extract intermediate results from the key point detection module and redesign the subsequent generation steps based on our task.
We compare our result with the rasterized output of FOMM in a crowdsourcing study introduced in \autoref{sec:crowd}.


\subsection{Key Point Alignment}
\label{sec: keypointalignment}
A local affine transformation is applied to align the motion trajectories of key points to the control points. It introduces non-linearity to preserve local information better and achieve richer deformation. 
In our task, each key point extracted from each frame from the GIF is considered a local region, and the local affine transformation matrix set is obtained by computing the translational transformation of each key point in adjacent frames. The global nonlinear transformation is then calculated using a distance-weighted interpolation-based approach with the matrix set.
$$
\left[\begin{array}{c}
C_{j}^{f+1} \\
1 \\
\end{array}\right]
=\sum_{i=1}^{N} w_{i}(C_{j})\cdot\left[\begin{array}{c}
C_{j}^0 \\
1 \\
\end{array}\right] \left[\begin{array}{cc}
\mathcal{I} & 0 \\
(X_{i}^{f} - X_{i}^1)^{T} & 1
\end{array}\right],
$$
$$
w_i(C_{j})=\frac{1 / \|C_{j}^0 - X_{i}^1\|^{e}}{\sum_i 1 /\|C_{j}^0 - X_{i}^1\|^{e}}.
$$
$C_{j}^f$ and $X_{i}^f$ denote the control point $j$ and the key point $i$ at frame $f$, respectively. The control point's position at each frame is calculated in reference to the key point at the first frame to achieve global stability. $\mathcal{I}$ is a 2nd-order identity matrix. $w_i$ is a weight function for a control point with respect to the key point $i$.

The weight decays according to the inverse of the $e$-th power of the relative distance from $X_i$ to $C_j$, where $e$ controls the locality of the affine transformations, \ie, the degree to which each affine transformation affects the target point.
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/alpha-test.pdf}
  \caption{Comparison of the generation results with $\alpha = 0, 2, 4$. Increasing $\alpha$ enhances the smoothness of the glyph, but an excessive value of $\alpha$ may negatively impact the amplitude of the motion.}
  \label{fig: ablation1}
  \Description{Figure 4 contains three blocks showing cases applying different parameter alpha. The vertical axis represents different values of the parameter alpha (0, 2, 4), and the horizontal axis shows the generated kinetic typography from three words "sleepy", "thanks", and "wakey" arranged by frame.}
    % \vspace{-0.1in}
\end{figure*}
\subsection{Position Optimization}
To alleviate inappropriate deformation of glyphs caused by changes in the relative position of the control points, we optimize the positions of the control points by frame based on the Laplacian coordinate, which generally describes the relative positions on the surface using the neighbor information.
For a control point $j$ at frame $f$, its Laplacian coordinate $L_j^f$ is calculated as
$$
\begin{aligned}
    L_{j}^f &= \sum_{k \in N_{j}}\omega_{jk}^f\left(C_{k}^f-C_{j}^f\right) =\sum_{k \in N_{j}}\omega_{jk}^f C_{k}^f - C_{j}^f,
\end{aligned}
$$
where $C_j^f$ and $C_k^f$ denote the Cartesian coordinate of the control point $j$ and $k$, respectively. $N_{j}$ is the set of $K$-nearest neighboring control points with the smallest Euclidean distance to the control point $j$, which is calculated based on the initial control points set $C^0$, invariant to changes in $f$. $\omega_{jk}^f$ denotes the weight of the neighbor point $k$ in the Laplacian representation of the control point $j$, where
$$
\omega_{jk}^f=\frac{1 / \|C^{f}_{k} - C^{f}_{j}\|^{2}}{\sum_{k \in N_j} 1 /\|C_{k}^f - C_{j}^f\|^{2}}.
$$

Considering the inhomogeneity of the distribution of discrete sampling points, we use the aforementioned distance-based weights to describe the detailed location information better.
Further, we design the following objective function  $\mathcal{L}_{\text{total}}$ to optimize the coordinates of the sequence of control points obtained by frame.
$$
\begin{aligned}
 \mathcal{L}_{\text{total}} &= \alpha \cdot  \mathcal{L}_{\text{glyph}} + \mathcal{L}_{\text{motion}}, \ \alpha\in[0,+\infty),\\
 \mathcal{L}_{\text{glyph}} &= \sum_{j=1}^{M}\left\|L_{j}^f-L_{j}^0\right\|^{e},\  
 \mathcal{L}_{\text{motion}} = \sum_{j=1}^{M}\left\|C_j^f - C_j^{f\prime}\right\|^{e}.\\
 % C1, C2, ..., C_n & = \arg\min\mathcal{L}_{\text{total}}.
\end{aligned}
$$

$L_{j}^f$ and $L_{j}^0$ denote the Laplacian coordinates of the control point $j$ in frame $f$ and $0$, respectively. $C_j^f$ and $C_j^f\prime$ denote the coordinates of the control points before and after optimization.
$\mathcal{L}_{\text{glyph}}$ measures how much the local shape details are preserved, which is computed as the sum of the distance of Laplacian coordinates between the optimized and initial control points. 
$\mathcal{L}_{\text{motion}}$ measures how much of the motions are preserved, as the sum of the distance of the control points before and after optimization, i.e., minimizing edit distance.
$\alpha$ is a hyperparameter representing the trade-off between the two loss functions.
The larger $\alpha$ is, the more details of the initial glyph and the less motion are preserved.
As for the norm $e$, the larger it is, the locality is more regulated, which leads to stronger deformation.
We use a $K$-dimensional tree to accelerate the nearest-neighbor search, where the parameter is empirically set: $K=3$.
\rev{While we employ frame-by-frame optimization, we note it is worth introducing global temporal regularization terms in the loss function to promote smoothness and consistency. 
% such as a penalty term that penalizes abrupt changes or high-frequency fluctuations between consecutive frames.
}

\subsection{User Interaction}
The user interaction module allows direct manipulation of the computed positions of the key points and control points at each frame, \ie, $\{X_i^f\}$ and $\{C_j^f\}$, $\forall i \in [1, N]\cap \mathbb{N}$, $j\in [1,M]\cap\mathbb{N}$, $f\in [1, F]\cap\mathbb{N}$.
In this way, creators of kinetic typography can participate in the motion transfer process and adjust the final results according to their needs.
The hyperparameter $\alpha$ in the position optimization stage can also be adjusted for different texts, as illustrated in \autoref{sec:ablation}.

