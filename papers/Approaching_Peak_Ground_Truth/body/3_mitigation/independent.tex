\subsection{Choosing Annotation Independent Metrics}
Another potential mitigation strategy is the introduction of performance measures that are independent of reference labels.
Due to the lack of reference annotations, the evaluation of \eacp{GAN} is far more developed in this regard.
Borji reviews several quantitative and qualitative measures for evaluating \eacp{GAN} \citep{borji2019pros,BORJI2022103329}.
One of these measures, also employed for measuring the quality of segmentation models, are human expert ratings \citep{kofler2021we}.
Building upon this, Kofler et al.\ \citep{kofler2022deep} demonstrate that surrogate models can approximate human expert ratings.
Further, the developing field of perceptual metrics promises to generate new label-independent metrics.
For instance, Bhardwaj et al.\ \citep{NEURIPS2020_Bhardwaj} propose \emph{PIM}, a perceptual metric grounded in information theory.
Additionally, Corneanu et al.\ \citep{corneanu2020computing} propose persistent topology measures to predict model performance on unseen samples.
Quantifying model performance through a proxy downstream task is another reference-independent strategy.
However, such a pipeline has to be carefully designed to avoid introduction of further evaluation noise.
