\section{Introduction}
Modern \eac{DL} offers countless possibilities for creating models.
Network architecture, optimizer, hyperparameters, and loss function can be arranged in infinite permutations.
Researchers are confronted with the non-trivial task of identifying the best models for their given use case.
A popular way of approaching this, also prominent in classical \eac{ML}, is to compare model outputs with \emph{reference annotations} by computing similarity metrics.
Technical innovations distinguish themselves from established choices by achieving higher similarity scores.
Achieving higher similarity scores is key in marketing innovations and often pivotal for publication decisions.
Several challenges, such as \emph{BraTS} \citep{menze2014multimodal}, \emph{KiTS} \citep{heller2021state}, or \emph{LiTS} \citep{bilic2019liver}, emerged and manifested themselves as a platform for participants to benchmark their algorithms even beyond the bio-medical domain, e.g., \emph{Natural Language Processing} \citep{parra-escartin-etal-2017-ethical}.
Across all these challenges, organizers decorate winners based on their achieved overlap with \emph{reference annotations} \citep{bakas2018identifying}.


\noindent\textbf{Contribution:}
Here, we introduce the theoretical concept of \eac{PGT}.
We define \eac{PGT} as the point beyond which an increase in similarity of a model's output with the annotation will not translate to improved \emph{RWMP}.
Besides proposing means of quantitatively approximating \eac{PGT}, we discuss \eac{PGT}\emph{-aware} strategies to evaluate \eac{ML} model performance.
We illustrate its widespread implications for interpreting \eac{ML} models across and beyond the biomedical domain.