\begin{figure*}[ht]
\centering
\label{fig:peak}
% \includegraphics[width=1.0\linewidth]{body/fig/peakgt.png}
% \includegraphics[width=0.75\linewidth]{body/fig/peakgt.png}
\includegraphics[width=1.0\linewidth]{body/fig/peakgt.png}
\caption{
\eacf{PGT} implications for \eac{ML} training.
When employing a similarity metric in the loss function, the network is incentivized to maximize similarity with the \emph{annotation}.
Frequently, similarity with the annotation does not (fully) represent \eacf{RWMP} either/or due to erroneous annotations or poor selection of metrics.
Consequently, increasing similarity only corresponds to increased model performance up to a certain point, as a hypothetical similarity of 100\% would mean reproducing all (potential) errors in the annotation, leading to \emph{falling off the cliff}.
This point is defined as \eacf{PGT}; in this example sketch, it is located around 0.9 on the \emph{x-axis}.
The real location and shape of the curve are defined by the \eacf{ML} problem at hand.
}
\end{figure*}