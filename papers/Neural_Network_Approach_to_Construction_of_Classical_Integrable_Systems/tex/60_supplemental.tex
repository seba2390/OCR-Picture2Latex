\section{Simple Examples of Action-angle Variables}
The action-angle variables are readily obtained if the Hamiltonian is separable to $N$ decoupled Hamiltonians:
\begin{align}
  H = \sum_{i=1}^{N} H_i(p_i,q_i),
\end{align}
where $H_i$  is the Hamiltonian for each particle.
We consider the case where each motion generated by each partial Hamiltonian $H_i$ is periodic.
In this situation, the action variables are defined by
\begin{align}
  I_i = \frac{1}{2\pi}\oint p_i \, dq_i \qquad{i=1,\ldots,N}, \label{eq:action_variables_circle}
\end{align}
where the contour integration is carried out on the trajectory.
The generating function of canonical transformations of the action variables, denoted as $W$, is given by
\begin{align}
  \begin{split}
    & W(q, I)  = \sum_{i=1}^N W_i(q_i,I_i),
  \end{split}
  \label{eq:generating_function}
\end{align}
where
\begin{align}
  W_i(q_i,I_i) = \int p_i(q_i, I_i) \, dq_i  \qquad{i=1,\ldots,N}.
\end{align}
From the generating function~(\ref{eq:generating_function}), the momenta and the angle variables are obtained as
\begin{align}
  p_i &= \frac{\partial W}{\partial q_i}, \\
  \theta_i &= -\frac{\partial W}{\partial I_i}  \qquad{i=1,\ldots,N},
\end{align}
respectively.

One of the simplest examples is a harmonic oscillator chain of $N$ particles with periodic boundary conditions~\cite{Arnold-text}.
It is well known that the system can be transformed to $N-1$ independent harmonic oscillators and the free motion of the center of mass:
\begin{align}
 H = \frac{1}{2N}\left(\sum_{i=1}^N p_i\right)^2 + \sum_{k=1}^{N-1} \Big[ \frac{P_k^2}{2} + \frac{\omega_k^2 Q_k^2}{2} \Big],
\end{align}
where $\omega_k \defeq \sqrt{4J}\sin (\pi k/N)$ ($k=1,\ldots,N-1$) are the normal mode frequencies with $J$ being the coupling constant, and $\{P_k,Q_k\}_{k=1}^{N-1}$ are the momenta and positions of normal modes.
Here, one can introduce the conserved quantities, which correspond to the ``radii'' of the trajectory in the phase space:
\begin{align}
 \begin{split}
   I_k & = \frac{P_k^2}{2\omega_k} + \frac{\omega_k Q_k^2}{2} \qquad k = 1, \ldots N-1,\\
   I_N & = \frac{1}{2N}\left(\sum_{i=1}^N p_i\right)^2.
 \end{split}\label{eq:harmonic_oscillator_canonical_trans}
\end{align}
The last conserved quantity, $I_N$, always exists when the potential function is translation invariant.
Since the transformation from the original coordinates to the decoupled oscillators is a canonical transformation, it is easy to prove that the quantities are $N$ linearly independent first integrals in involution.
The angular variables are given by
\begin{align}
 \theta_k = \tan^{-1}\left(\frac{P_k}{\omega_k Q_k}\right) \qquad k = 1, \ldots N-1.
\end{align}
The variables $\{I_k,\theta_k\}_{k=1}^{N-1}$ are the canonical coordinates.
Consequently, the harmonic oscillator chain is completely integrable and has the action-angle variables, and $K(I)$ is given by
\begin{align}
  K(I) = \sum_{k=1}^{N-1}\omega_k I_k + I_N \label{eq:harmonic_oscillator_K_I}.
\end{align}
Note that $I_N$ is not the action variable because the corresponding motion is not bounded.
We can also obtain the action variables from the definition, \refeq{eq:action_variables_circle}.
We denote each energy of the periodic motion as
\begin{align}
  E_k = \frac{P_k^2}{2} + \frac{\omega_k^2 Q_k^2}{2} \qquad k = 1, \ldots N-1.
\end{align}
Then, the contour integration~\refeq{eq:action_variables_circle} is evaluated as
\begin{align}
  \begin{split}
    I_k & = \frac{1}{2\pi}\oint P_k \, dQ_k
        = \frac{1}{\pi} \int_{-\frac{\sqrt{2E_k}}{\omega_k}}^{\frac{\sqrt{2E_k}}{\omega_k}} \sqrt{2E_k -\omega_k^2Q_k^2} \, dQ_k
        = \frac{E_k}{\omega_k} \qquad k=1,\ldots,N-1,
  \end{split} \label{eq:action_of_harmonic_derived_from_circle_integration}
\end{align}
which is nothing but the quantities defined by \refeq{eq:harmonic_oscillator_canonical_trans}.

The second example is the square-well potential~\cite{Reichl-1992}.
While $K(I)$ of the harmonic oscillator chain is given by a linear combination of $\{I_i\}_{i=1}^{N-1}$, that for the square-well potential is realized by a linear combination of $\{I_i^2\}_{i=1}^{N-1}$.
Let us introduce the following Hamiltonian:
\begin{align}
  H = \frac{1}{2N}\left(\sum_{i=1}^N p_i\right)^2 + \sum_{k=1}^{N-1} \Big[ \frac{P_k^2}{2} + U(Q_k) \Big], \label{eq:hamiltonian_potential_well}
\end{align}
where $U$ is the square-well potential given by
\begin{align}
  U(r) = \begin{cases}
            0 & -\frac{\pi}{2\sqrt{2}} < r < \frac{\pi}{2\sqrt{2}}, \\
            \infty & \mathrm{otherwise}.
         \end{cases} \label{eq:potential_well}
\end{align}
The system is also separable to $N$ independent motions.
We introduce the energy of each motion as
\begin{align}
  E_k = \frac{P_k^2}{2} + U(Q_k) \qquad k = 1, \ldots N-1.
\end{align}
In the same way as \refeq{eq:action_of_harmonic_derived_from_circle_integration}, the action variables are introduced as
\begin{align}
  \begin{split}
    I_k & = \frac{1}{2\pi}\oint P_k \, dQ_k
        = \frac{1}{\pi} \int_{-\frac{\pi}{2\sqrt{2}}}^{\frac{\pi}{2\sqrt{2}}} \sqrt{2\left(E_k-U(Q_k)\right)} \, dQ_k
        = \sqrt{E_k} \qquad k=1,\ldots,N-1.
  \end{split}
\end{align}
Therefore, $K(I)$ of \refeq{eq:hamiltonian_potential_well} is given by
\begin{align}
 K(I) = \sum_{k=1}^{N-1} I_k^2 + I_N \label{eq:potential_well_K_I}.
\end{align}

For the Toda lattice, the action variables are calculated from the Lax pair~\cite{Flaschka-McLaughlin1976,Flaschka1974}.
We consider the Toda lattice with periodic boundary conditions.
The representative form of the Lax pair of the Toda lattice is~\cite{Flaschka1974-2}
\begin{align}
  L & \defeq
  \left(
    \begin{array}{ccccccccc}
      b_1 & a_1 &       &        &         &         &       &         & a_N     \\
      a_1 & b_2 &       &        &         &         &       &         &         \\
          &     & \ddots &       &         &         & 0     &         &        \\
          &     &       &b_{i-1} & a_{i-1} &         &       &         &         \\
          &     &       &a_{i-1} & b_{i}   & a_i     &       &         &         \\
          &     &       &        & a_{i}   & b_{i+1} &       &         &         \\
          &     &  0    &        &         &         & \ddots &        &        \\
          &     &       &        &         &         &       & b_{N-1} & a_{N-1} \\
      a_N &     &       &        &         &         &       & a_{N-1} & b_{N}
    \end{array}
  \right) \label{eq:toda_lax_matrix}\\
  M & \defeq
  \left(
    \begin{array}{ccccccccc}
      0   &-a_1 &       &        &         &         &       &         & a_N     \\
      a_1 & 0   &       &        &         &         &       &         &         \\
          &     & \ddots &       &         &         & 0     &         &        \\
          &     &       &    0   & -a_{i-1}&         &       &         &         \\
          &     &       &a_{i-1} &   0     & -a_i    &       &         &         \\
          &     &       &        & a_{i}   &   0     &       &         &         \\
          &     &  0    &        &         &         & \ddots&         &        \\
          &     &       &        &         &         &       &    0    & - a_{N-1} \\
    - a_N &     &       &        &         &         &       & a_{N-1} & b_{N}
    \end{array}
  \right),
\end{align}
where $a_n$ and $b_n$ are the Flashka variables defined by~\cite{Flaschka1974}
\begin{align}
  \begin{split}
    a_i & \defeq \frac{1}{2}e^{-(q_{i+1} - q_i)}, \\
    b_i & \defeq \frac{1}{2}p_i \qquad i=1,\ldots,N.
  \end{split}
\end{align}
The matrices, $L$ and $M$, are called the Lax pair and satisfy the following equation~\cite{Lax1968}:
\begin{align}
 \frac{dL}{dt} = [M, L],\label{eq:eom_lax_pair}
\end{align}
where $[\cdot]$ denotes the commutator.
The action variables are calculated by the Lax pair as~\cite{Flaschka-McLaughlin1976,Flaschka1974}
\begin{align}
  I_k = \frac{2}{\pi} \int_{\lambda_{2k}}^{\lambda_{2k+1}} \cosh^{-1}\left[\frac{(-1)^k}{2} \Delta(\lambda)\right] \, d\lambda \qquad k=1,\ldots,N-1, \label{eq:exact_toda_actions}
\end{align}
where $\{\lambda_{2k-1}\}_{k=1}^{N}$ are the eigenvalues of $L$, and $\{\lambda_{2k}\}_{k=1}^{N}$ are those of the matrix obtained from $L$ by replacing $L_{N1}$ and $L_{1N}$ by $-L_{N1}$ and $-L_{1N}$, respectively. We assume that the eigenvalues are sorted in ascending order.
The function $\Delta(\lambda)$ in \refeq{eq:exact_toda_actions} is related to the determinant of $L$ by
\begin{align}
  \mathrm{det}(L - \lambda I) = (-1)^N\left(\prod_{j=1}^{N}a_j\right) \left(\Delta(\lambda) - 2\right), \label{eq:discriminant}
\end{align}
where $I$ is the $N \times N$ identity matrix.
The action variables play the role of the amplitude of the solitons~\cite{Ferguson-Flaschka-McLaughlin1982}.

\section{Structures of Neural Networks}
\begin{figure}[t]
  \centering
  \includegraphics[width=8cm]{figures/residual_and_dense.pdf}
  \caption{
    (Color online) Schematic diagrams of (a) dense neural networks and (b) residual neural networks.
    }
  \label{fig:residual_and_dense}
\end{figure}
In this section, we describe the details of our neural networks.
We consider a particular group of potential functions, which depends on the difference between two positions $q_i - q_j$.
According to the translational invariance, the motion of the center of mass is trivially conserved.
We assume the following potential function:
\begin{align}
    V(q) & = \sum_{i=1}^{N}v(r_{ii+1}),
\end{align}
where $v(r)$ is represented by a neural network, $N$ is the number of particles, and $r_{ij}$ is the difference between the displacements of the adjacent particles $r_{ij} = q_i - q_j$.
In the present study, $N$ was set to $5$.
We consider two different types of neural networks for $v(r)$, a dense neural network and a residual neural network (see \reffig{fig:residual_and_dense}).
These neural networks have two parameters: the number of hidden layers and the dimension of the hidden layers.
In general, each hidden layer can be different from the others.
In the present work, we used the same dimension for all the hidden layers for simplicity and convenience.
We set the dimension of the hidden layers to $128$ and the number of hidden layers to $4$.
We used the sigmoid-weighted linear units as the activation function~\cite{Elfwing-Uchibe-Doya2018}.
For the Toda lattice, from preliminary tests using different combinations of neural networks, we chose a residual neural network without additive biases.

Our canonical transformation is a map from the real space $\{p_i,q_i\}_{i=1}^{N}$ to the auxiliary space $\{\xi_i,\zeta_i\}_{i=1}^{N}$.
The auxiliary variables are then related to the action-angle variables as
\begin{align}
  \begin{split}
    I_i & = \frac{\xi_i^2 + \zeta_i^2}{2}, \\
    \theta_i & = \tan^{-1}\left(\frac{\xi_i}{\zeta_i}\right) \qquad k = 1, \ldots N-1,
  \end{split}
\end{align}
where $I_i$ is the action variable, and we define $(\xi_N,\zeta_N)$ as the variables of the center of mass.
This transformation is a trivial canonical transformation, as discussed in the previous section.
The entire canonical transformation built by the neural network is illustrated in the main text.
Our canonical transformation consists of (i) point transformations~\cite{Shuo-Hui-etal2020}, (ii) symplectic linear transformations~\cite{Bondesan-Lamacraft2019}, and (iii) the discrete Hartley transformations.
The point transformation is implemented by an invertible neural network. In our approach, we use the RealNVP neural network~\cite{RealNVP2017}.
The scale and the translation transformations in RealNVP are represented by dense neural networks.
We here set the number of hidden layers to $5$ and the dimension of the hidden layer to $30$.
We chose the hyperbolic tangent function as the activation function and stacked $4$ RealNVP.
We implemented the symplectic linear transformation using the method proposed in Ref.~\citen{Bondesan-Lamacraft2019}, in which U(1) transformations were represented by $N$ Householder transformations.
The discrete Hartley transformation is defined by~\cite{Hartley1942,Bracewell1983}
\begin{align}
  \mathrm{DHT}(q)_k \defeq
        \sum_{n=1}^N \sqrt{\frac{1}{N}}q_n\biggl(\cos\left(\frac{2\pi nk}{N}\right) + \sin\left(\frac{2\pi nk}{N}\right)\biggr) \qquad k=1,\ldots,N.\label{eq:discrete_hartley_trans_in_chap4}
\end{align}
The transformation extracts the variables of the center of mass as $\mathrm{DHT}(q)_N$.
For the Toda lattice, the whole canonical transformation was constructed by $10$ elemental blocks consisting of (i) and (ii), and one discrete Hartley transformation layer.

\section{Symplectic Integrator with Adjoint Method}
To calculate the gradients of $L_I$, we used the Yoshida fourth-order symplectic integrator with the adjoint method, which is one of the most popular fourth-order symplectic integrators~\cite{Yoshida1990,Candy-Rozmus1991,Forest-Ruth1990,Neri1987}.
We derived a specific form of the adjoint equations that implement this integrator.
Note that our implementation is a special case of the method proposed in Ref.~\citen{Sanz-Serna2016}.

Let us consider time evolution of a $N$-particle system from $t_{\mathrm{init}}$ to $t_{\mathrm{fin}}$ and $\{t_k\}_{k=1}^{N_{\mathrm{time}}}$ be $N_{\mathrm{time}}$ time points obtained by discretization of the duration.
We also assume that the system Hamiltonian takes the following form:
\begin{align}
    H(p,q) = T(p) + V(q,\theta), \; T(p) = \sum_{i=1}^{N}\frac{p_i^2}{2},
\end{align}
where $V(q,\theta)$ is a learnable potential function, and $\theta$ is determined such that the loss function is minimized.
The equations of motion are
\begin{align}
  \begin{split}
    & \frac{dq_i}{dt} = p_i, \\
    & \frac{dp_i}{dt} = -\frac{\partial V(q,\theta)}{\partial q_i}, \qquad i = 1, \ldots N.
  \end{split}
\end{align}
The Yoshida fourth-order symplectic integrator is represented by~\cite{Yoshida1990}
\begin{align}
  \begin{split}
    & q_i(\tau_{n+1}) = q_i(\tau_{n}) + h c_n p_i(\tau_{n}), \\
    & p_i(\tau_{n+1}) = p_i(\tau_{n}) - h d_n \frac{\partial V(q_i(\tau_{n+1}),\theta)}{\partial q_i}, \qquad n = 1, \ldots 4, \\
  \end{split}\label{eq:Yoshida_algorithm}
\end{align}
where
\begin{align}
  \begin{split}
    & c_1 = c_4 = \frac{1}{2(2-2^{1/3})}, \;   c_2 = c_3 = \frac{1-2^{1/3}}{2(2-2^{1/3})},  \\
    & d_1 = d_3 = \frac{1}{2-2^{1/3}}, \;   d_2 = -\frac{2^{1/3}}{2-2^{1/3}}, \;  d_4 = 0,
  \end{split} \label{eq:coefficients_neri}
\end{align}
$h$ is the time step $(t_{\mathrm{fin}} - t_{\mathrm{init}})/N_\mathrm{time}$, and $\tau_n$ describe intermediate times satisfying $t_k = \tau_1 < \tau_2 < \tau_3 < \tau_4 < \tau_5 = t_{k} + h = t_{k+1}$.
We note that the backward evolution produces exactly the same time series with the forward evolution because of the symmetry of the coefficients.
This property is common for the symplectic integrators constructed by the method proposed in Ref.~\citen{Yoshida1990}.
Let $L$ be the loss function whose minimization determines the parameter $\theta$.
To optimize $L$, we need to calculate the gradients $\lambda_i^q \equiv \frac{\partial L}{\partial q_i}$, $\lambda_i^p \equiv \frac{\partial L}{\partial p_i}$, and $\lambda^\theta \equiv \frac{\partial L}{\partial \theta}$ at $t_{\mathrm{init}}$.
It is known that these gradients satisfy the following adjoint equations~\cite{NeuralODE,Sanz-Serna2016}:
\begin{align}
  \begin{split}
    \frac{d\lambda^{q}_i}{dt} & = \sum_{j=1}^{N}\frac{\partial^2 V(q,\theta)}{\partial q_i q_j} \lambda^{p}_j, \\
    \frac{d\lambda^{p}_i}{dt} & = - \lambda^{q}_i,\\
    \frac{d\lambda^{\theta}}{dt} & =  \sum_{j=1}^{N}\lambda^{p}_j\frac{\partial^2V(q,\theta)}{\partial q_i \partial \theta}.
  \end{split} \label{eq:hamilton_dynamics_of_adjoint}
\end{align}
We obtain the values at $t_{\mathrm{init}}$ by integrating the equation from $t_{\mathrm{fin}}$ to $t_{\mathrm{init}}$ backwards.
As seen in \refeq{eq:hamilton_dynamics_of_adjoint}, the relation between $\lambda^q$ and $\lambda^p$ is equivalent to the relation between the momenta and positions of Hamiltonian mechanics.
Therefore, we can use the symplectic integrator to describe the dynamics of $\lambda^q$ and $\lambda^p$.
Furthermore, the discretization induced by the symplectic integrator produces the equations derived from the chain rule of the algorithm~\cite{Sanz-Serna2016}.
For the Yoshida symplectic integrator, we can see the fact by the following procedure.
The update procedure described by \refeq{eq:Yoshida_algorithm} can be split into two steps:
\begin{align}
  \begin{split}
    q_i(\tau_{n+1}) &  = q_i(\tau_{n}) + h c_n p_i(\tau_{n}), \\
    p_i(\tau_{n}) & = p_i(\tau_{n}), \qquad n = 1, \ldots 4.
  \end{split}
\end{align}
and
\begin{align}
  \begin{split}
    q_i(\tau_{n+1}) & = q_i(\tau_{n+1}), \\
    p_i(\tau_{n+1}) & = p_i(\tau_{n}) - h d_n \frac{\partial V(q_i(\tau_{n+1}),\theta)}{\partial q_i},\qquad n = 1, \ldots 4.
  \end{split}
\end{align}
According to the chain rule, $\lambda^q$ and $\lambda^p$ obey
\begin{align}
  \left(
    \begin{array}{c}
      \lambda^q_{n} \\
      \lambda^p_{n}
    \end{array}
  \right)
  =
  \left(
    \begin{array}{cc}
      I &  0  \\
      hc_n &  I
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \lambda^q_{n+1} \\
      \lambda^p_{n}
    \end{array}
  \right),\qquad n = 1, \ldots 4,
\end{align}
and
\begin{align}
  \left(
    \begin{array}{c}
      \lambda^q_{n+1} \\
      \lambda^p_{n}
    \end{array}
  \right)
  =
  \left(
    \begin{array}{cc}
      I &  M(\tau_{n+1})  \\
      0 &  I
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \lambda^q_{n+1} \\
      \lambda^p_{n+1}
    \end{array}
  \right),\qquad n = 1, \ldots 4,
\end{align}
where $M(\tau_{n+1})$ is the matrix whose elements are
\begin{align}
  M_{ij}(\tau_{n+1}) = -hd_n\frac{\partial^2 V(q_i(\tau_{n+1}),\theta)}{\partial q_i q_j}.
\end{align}
The update procedure of $\lambda^q$ and $\lambda^p$ is thus represented by
\begin{align}
  \left(
    \begin{array}{c}
      \lambda^q_{n+1} \\
      \lambda^p_{n+1}
    \end{array}
  \right)
  =
  \left(
    \begin{array}{cc}
      I &  -M(\tau_{n+1})  \\
      0 &  I
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      I &  0  \\
      -hc_n &  I
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \lambda^q_{n} \\
      \lambda^p_{n}
    \end{array}
  \right), \qquad n = 1, \ldots 4.
  \label{eq:algorithm_adjoint_symplectic}
\end{align}
This expression is the same as a specific implementation of the symplectic integrator for \refeq{eq:hamilton_dynamics_of_adjoint}.
Since $M(\tau_n)$ depends on $q(\tau_{n})$, we calculate the backward time evolution of $(q,p)$ simultaneously.
The auxiliary variables and $\lambda^{\theta}$ can be regarded as positions and momenta, respectively.
We applied the symplectic integrator to these variables for calculating the time evolution of $\lambda^{\theta}$.

\section{Brief Analysis of Performances}
\begin{figure}[t]
  \centering
  \includegraphics[width=12cm]{figures/toda_larning_residual_result_performane_data_changing_log.pdf}
  \caption{
    (Color online) (a) Training data size dependence of MRE of the action variables: $I_1$ (blue circle), $I_2$ (orange cross), $I_3$ (green square), $I_4$ (red plus) and (b) $L_I$ (blue circle) and MRE of the total energy $E$ (orange cross).
    $N_D$ is the number of the training data.
    $L_I$ is the loss function of the action variables introduced in the Letter.
    }
  \label{fig:toda_larning_residual_result_performance_data_changing}
\end{figure}
\begin{figure}[tp]
  \centering
  \includegraphics[width=12cm]{figures/toda_larning_residual_result_performane_batch_changing.pdf}
  \caption{
    (Color online) (a) Mini-batch size dependence of MRE of the action variables: $I_1$ (blue circle), $I_2$ (orange cross), $I_3$ (green square), $I_4$ (red plus) and (b) $L_I$ (blue circle) and MRE of the total energy $E$ (orange cross).
    $N_{\mathrm{MB}}$ stands for the number of the mini-batch.
    $L_I$ is the loss function of the action variables introduced in the Letter.
  }
  \label{fig:toda_larning_residual_result_performance_batch_changing}
\end{figure}

We tested the performance of the trained neural networks by changing the training data size and the mini-batch size.
We set the number of time series for calculations of $L_I$ to $10^3$,
the period of the time development to $100$, the interval for the discretization to $0.01$,
and the number of initial states sampled from the Boltzmann distribution to $10^4$.
The MRE and the loss function for different training data sizes are shown in \reffig{fig:toda_larning_residual_result_performance_data_changing}.
Although the increase of training data monotonically improved the performance, the reduction of the error and the loss became slow for large training data sizes.
This result might be due to the limited representation power of the neural networks.
Nevertheless, the larger the number of parameters and layers in the neural networks is, the smaller the error and the loss should become.
As well as the training data size, we checked the dependence of the performance on the mini-batch size.
The performance for different mini-batch sizes is shown in \reffig{fig:toda_larning_residual_result_performance_batch_changing}.
We observed that the error and the loss were smaller for larger mini-batch sizes.
In the present study, taking these performance analyses into account, we set the training data size to $N_D=10^5$ and the mini-batch size to $N_{MB}=2 \times 10^3$, respectively.
