
We demonstrate successful learning, applying our approach to the Toda lattice~\cite{Toda1967-2,Toda1967}, which is a prototype of the classical integrable system.
\begin{figure}[tp]
  \centering
  \includegraphics[width=8.4cm]{figures/toda_larning_residual_result_second_potential_wide.pdf}
  \caption{
    (Color online) Neural network potential function obtained from the dataset of action-angle-variable samples at temperature $T=2J$.
    (a) The learned potential function (crosses) and the true Toda potential function (orange solid line) for $J = 1$ and $\alpha = 1$.
    (b) The absolute error of the learned potential function.
    }
\label{fig:toda_larning_residual_result_second_potential}
\end{figure}
\begin{figure}[tp]
  \centering
  \includegraphics[width=8cm]{figures/toda_larning_residual_result_second_energy_error.pdf}
  \caption{
    (Color online) (a) The scatter plot and (b) the relative error distribution of neural network predictions of the total energy.
    The MRE is 0.0308, averaged over $10^4$ samples.
    }
\label{fig:toda_larning_residual_result_second_energy_error}
\end{figure}
\begin{figure}[tp]
  \centering
  \includegraphics[width=7.7cm]{figures/toda_larning_residual_result_action_variables_rel_err.pdf}
  \caption{
    (Color online) Scatter plots of neural network predictions of the action variables (a) $I_1$, (b) $I_2$, (c) $I_3$, and (d) $I_4$.
    Using the known analytical expression, we calculated the true values from real space variables sampled from the Boltzmann distribution at $T=2J$.
    The MREs were $0.132$, $0.122$, $0.132$, and $0.117$, respectively, averaged over $10^4$ samples.
    }
\label{fig:toda_larning_residual_result_action_variables_rel_err}
\end{figure}
\begin{figure}[tp]
  \centering
  \includegraphics[width=8cm]{figures/toda_larning_residual_result_second_action_variables_linear.pdf}
  \caption{
      (Color online) Time series of action variables predicted by the trained neural networks: $I_1$ (blue), $I_2$ (orange), $I_3$ (green), and $I_4$ (red). The solid lines show the true values.
    }
\label{fig:toda_larning_residual_result_second_action_variables}
\end{figure}
The Hamiltonian is given by
\begin{align}
 H = \sum_{i=1}^{N} \frac{p_i^2}{2m} + \sum_{i=1}^N J\left(e^{-\alpha(q_{i+1} - q_{i})} + \alpha (q_{i+1} - q_{i}) - 1 \right), \label{eq:toda_hamiltonian}
\end{align}
where $J$ and $\alpha$ are the coupling constants, and $q_i$ is the displacement of the $i$-th particle. We set $N=5$, $m=1$, $J = 1$, and $\alpha = 1$ and used the periodic boundary condition.
For the Toda lattice, the function form of $K(I)$ is not known, but the canonical transformation into the action variables is known~\cite{Flaschka-McLaughlin1976,Flaschka1974,Kappelerf-Henricit2008,Henrici2015}.
We first sampled real-space coordinates from the Boltzmann distribution, using the Hamiltonian Monte Carlo (HMC) method for $H(p,q)$~\cite{Duane-etal-1987,mcmc-handbook2011}.
We then obtained samples of the action variables from the real-space coordinates through the known canonical transformation.
Note that we used the exact canonical transformation only for generating input data and testing the final result.
The resulting distribution of the samples is identical to the Boltzmann distribution.
If we had the function form of $K(I)$, we could generate samples from the Boltzmann distribution directly.
The angle variables were sampled from the uniform distribution.
The total momentum, namely $I_5$, was fixed to zero.
The temperature $T$ was set to $2J$.
This temperature is high enough to see the deviation of the Toda lattice potential from the harmonic oscillator and low enough to learn the potential bottom accurately.

We generated $10^5$ samples in total and set the mini-batch size to $2 \times 10^3$~\cite{our_supplemental_material}. We updated the neural network parameters in $160$ epochs.
The learning rate was first set to $10^{-3}$ and reduced to $10^{-4}$ after $80$ epochs.
We calculated the time evolution up to $t = 5$ with $dt = 0.05$ and set $N_\mathrm{time}=5$ at equal intervals for the loss function.

Our neural network successfully reproduced the true potential function, as shown in \reffig{fig:toda_larning_residual_result_second_potential}. It is reasonable that the error becomes larger for higher energy because of the fewer number of samples in the input data.
Note that the learning of the potential function has uncertainty due to periodic boundaries: the total energy is unchanged by adding a linear term as $v'(r) = v(r) + a r$, where $a$ is a constant.
We here plot the potential after removing the linear term: $ \hat{v}(r) - r\left.\frac{\partial \hat{v}(r)}{\partial r}\right|_{r=0}$, where $\hat{v}(r)$ is the learned potential function. See Supplemental Material for details of the neural network parameters~\cite{our_supplemental_material}.
The error of the neural network prediction of the total energy is shown in \reffig{fig:toda_larning_residual_result_second_energy_error}. Here, the mean relative error (MRE) of a quantity $O$ is defined as
\begin{align}
  \mathrm{MRE} = \average{\left|1 - \frac{O^{\mathrm{predict}}}{O^{\mathrm{true}}}\right|}_{D}, \label{eq:mean_relative_error}
\end{align}
where $O^{\mathrm{predict}}$ is the neural network prediction, and $O^{\mathrm{true}}$ is the true value. The average was taken over the dataset.
The MRE of the total energy was only 3\%.

The trained neural network accurately predicted the values of action variables from real space coordinates, as shown in \reffig{fig:toda_larning_residual_result_action_variables_rel_err}.
The true values were calculated from the known analytical expression~\cite{Flaschka-McLaughlin1976,Flaschka1974}.
The MRE of the prediction of the action variables was of order 10\%.
We plot time series of neural network predictions of the action variables in \reffig{fig:toda_larning_residual_result_second_action_variables}.
The time evolution was simulated using the exact potential function in this test.
The predicted action variables do not show any drift with time, fluctuating around the true value.
This result clearly shows that the neural networks learn the conserved quantities without any prior knowledge about the canonical transformation.
