%\section{SUMMARY AND OUTLOOK} \label{sect:summary}

In conclusion, we have proposed a machine learning approach to finding classical integrable systems.
Our approach is distinct from the previous approaches in that we construct a natural Hamiltonian from samples in latent space.
We simultaneously train the neural networks to represent the canonical transformation and the potential functions.
In our approach, input data are samples of the action-angle variables.
The action variables are sampled from the Boltzmann distribution, and the angle variables are sampled from the uniform distribution.
We have also proposed a loss function that consists of the two parts: the loss of the action-variable conservation with time and the loss of the energy equivalence.
Note that we simulated the time evolution using the symplectic integrator with the adjoint method.
We applied our approach to the Toda lattice and demonstrated successful unsupervised learning of the canonical transformation and the potential function.
The relative errors of neural network predictions were of order a few percent for the total energy and of order ten percent for the action variables.
Our neural network learned the conserved quantities with no prior knowledge about the canonical transformation.

We here discuss possible extensions of the present approach.
Although we focused on the two-body potential function given by \refeq{eq:chain}, we can extend the function form to more general forms, such as fully connected potential functions.
We can study multi-dimensional systems by applying the discrete Hartley transformation in each dimension.
The entire learning could be improved by adding a loss function to require the distribution of $\{p_i\}_{i=1}^{N}$ to be the Gaussian distributions.
We transformed the real space variables into the latent space variables through the auxiliary space and assumed the specific transformation~(\ref{eq:latent_space_representation_of_action_angle_variables}) between the latent space and the auxiliary space.
Using the auxiliary space is helpful for the learning in the present study.
Nevertheless, if it was removed, the neural network could seek direct canonical transformations between the latent space and the real space.
Whether using the auxiliary space is beneficial may depend on the structure of the neural network.

We also discuss how to distinguish nonintegrability from learning failure.
The loss may remain finite due to the absence of the corresponding integrable model or the low representability of the neural network we use.
It is not easy to distinguish the two cases in practice, which is a typical problem of machine learning approaches. Nevertheless, we can check integrability by monitoring the loss while increasing the neural network representability.
The representability can be enhanced by increasing the number of neural network parameters, such as the network depth and width.
If an integrable natural Hamiltonian of the assumed form exists, the loss should decrease and eventually reach zero as the representability is enhanced.
On the other hand, we can conclude that the given action-variable Hamiltonian is not transformed into the assumed form of natural Hamiltonians if the loss is not decreased by increasing the number of network parameters.

Finally, it is of great interest to search function forms $K(I)$ that allow the Hamiltonian to take the natural form in real space.
It is known that the harmonic oscillator is transformed to a linear function of $K(I)$, and the Hamiltonian with a box (square-well) potential is transformed to a quadratic function of $K(I)$~\cite{Reichl-1992}.
It is, thus, intriguing to find integrable systems described by higher-order functions of $K(I)$ as future problems.
