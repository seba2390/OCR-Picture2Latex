\section{Evaluation}
\label{evaluation}
This section studies the effectiveness of \tool{} (e.g., whether it can indeed identify interaction inefficiencies) and its overheads. 

We evaluate \tool{} on a 14-core Intel Xeon E7-4830 v4 machine clocked at 2GHz running Linux 3.10. The machine is equipped with 256 GB of memory and four debug registers. \tool is compiled with {\tt  GCC 6.2.0 -O3}, and CPython (version 3.6) is built with {\tt --enable-shared} flag. \tool subscribes hardware event {\tt MEM\_UOPS\_RETIRED\_ALL\_STORES} for redundant stores detection and {\tt MEM\_UOPS\_RETIRED\_ALL\_LOADS} for redundant loads detection, respectively.










 % and the detecting ability of \tool is investigated with more than 100 real-world applications.







% . At last, we perform two typical case studies to present how \tool guides programmers to identify inefficiencies. 





% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{asplos21/paper/Figures/store_overhead_ppt.pdf}
% 	\caption{Runtime slowdown of \tool at the sampling rate of 500K, 1M and 5M respectively, on scikit-learn, numexpr and NumpyDL. \yu{The error bars denote the confidence}}
% 	\label{fig:storeoverhead}
% \end{figure}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{asplos21/paper/Figures/load_overhead_ppt.pdf}
% 	\caption{load overhead~\yu{TODO}}
% 	\label{fig:loadoverhead}
% \end{figure}


% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{asplos21/paper/Figures/store_memory_ppt.pdf}
% 	\caption{store memory bloating~\yu{TODO}}
% 	\label{fig:storememory}
% \end{figure}


% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{asplos21/paper/Figures/load_memory_ppt.pdf}
% 	\caption{load memory bloating~\yu{TODO}}
% 	\label{fig:loadmemory}
% \end{figure}




% \subsection{Fraction of Redundant Native Function Calls}
% It's subtle to measure the fraction of redundant \ffs. Because \tool applies memory-related inefficiency detection patter, we employs wasting memory operations\yu{CITE} to quantify redundant native function calls. We calculate the wasting memory access bytes triggered by {\it Mirror-nfc}/{\it Ghost-nfc} to measure their redundant portions, as following:
% \begin{equation}
% \end{equation}
% \begin{equation}
% \end{equation}


\subsection{Effectiveness}

This section assesses the effectiveness of \tool{}, and the breadth of the interaction inefficiencies problem among influential Python packages. The lack of a public benchmark creates two inter-related challenges: \emph{(i)} determining the codebases to examine inevitably involves human intervention, and \emph{(ii)} most codebases provide a small number of ``hello world'' examples, which have limited test coverage. 

We aim to include all ``reasonably important'' open-source projects and we use only provided sample code for testing. While using only sample code makes inefficiency detection more difficult, this helps us to treat all libraries as uniformly as possible. For each of Numpy, Scikit-learn, and Pytorch, we find all projects in Github that import the library, and sort them by popularity, which gives us three lists of project candidates. Our stopping rule for each list differs and involves human judgement because we find that the popularity of a project may not always reflect its importance (e.g., specialized libraries could be influential, but generally have smaller user bases, and are less popular in Github's rating system). For example, Metaheuristics is important and included in our experiment but it received only 91 ratings at the time we performed evaluation. At the end, we evaluated more than 70 read-world applications, among which there are more projects that import Numpy than the other two libraries. 


Indentifying a total of 19 inefficiencies is quite
surprising because these projects are mostly written by professionals,
and the sample codes usually have quite low codebase coverage,
and are usually ``happy paths'' that are highly optimized. The fact that we identify 18 new performance bugs as reported in Table 2, 
indicates that interaction inefficiencies are quite widespreaded.










% We identified 19 inefficiencies from these projects, which is quite surprising because these projects are mostly written by professionals, and sample code usually have quite low codebase coverage, and are usually ``happy paths'' that are highly optimized. This also suggests that interaction inefficiencies are quite widespreaded. 

% Specifically, of 19 inefficiencies, 18 are newly identified performance bugs (see Table~\ref{table} for a summary). 
% Table~\ref{table} also reports the performance speedup over the whole application ({\tt AS}, denoting application-level speedup) and over the function ({\tt FS}, denoting function-level speedup), respectively, by following \tool's optimization guidance. 
% {\tt AS} is defined as the ratio of the whole application execution time with the original library over the execution time with the optimized library. {\tt FS} is defined as the ratio of the total execution time of inefficient function before optimization over its total execution time after optimization.

\iffalse
choose all projects that import at least one of {\tt numpy}, {\tt Scikit-learn}, and {\tt Pytorch}, and sort them according to their popularity. 
 
We use \tool to study a collection of \jtan{highly ranked
Python applications}, including widely used ones like Scikit-learn, Pytorch, and highly specialized ones like
Meta-heuristics. By running only sample code provided by the applications, we identify actionable interaction inefficiencies from 17
applications with inconsequential overheads. Moderate refactoring
effort leads to significant performance improvement (e.g., half of
them have 100% or more improvement at the functional level). This
is quite unexpected because happy paths (provided sample code) in
high-profile projects usually are carefully optimized and have low
coverage of codebase

{\color{blue} move to somewhere else?}
Selecting Python applications for investigating in this paper follows the following rule: 1) they are open-sourced Python projects on Github; 2) \jtan{they are highly ranked in Github trending system~\cite{trending}}; and 3) they have interactions with native libraries. \yu{We need to reconsider the description here}

To confirm the effectiveness of \tool, \jtan{we investigate more than 100 real-world Python applications}. 
\fi


Table~\ref{table} reports that the optimizations following \tool's optimization guidance lead to 1.02$\times$ to 6.3$\times$ application-level speedup (AS), and 1.05$\times$ to 27.3$\times$ function-level speedup (FS), respectively. According to Amdahl's law, AS approaches FS as the function increasingly dominates the overall execution time.  
%For some application, the application-level and function-level speedups perform differently, depending on the application characteristic {\color{red}this sentence needs to be revised}. For example, Ta has a lower application-level speedup compared with its function-level speedup, whereas Cholesky has comparable speedups between application-level and function-level. 
For the five inefficiency categories we define in Section~\ref{classification} and which are common in real applications, \tool's superior redundant loads/stores detection proves its effectiveness.



% It is worth noting that the five inefficiency categories defined before (see Section~\ref{classification}) widely exist in real-world applications, and \tool can detect with redundant loads/stores detection. It proves \tool's strong effectiveness.





\begin{figure*}[!t]
  \centering
  \subfloat[Redundant Stores Detection]{
  \label{fig:slowdown:store}
  \includegraphics[width=0.492\textwidth]{Figures/store_overhead_ppt.pdf}
    %\graphicsplaceholder{4cm}{1cm}%
  }
  \subfloat[Redundant Loads Detection]{
  \label{fig:slowdown:load}
  \includegraphics[width=0.492\textwidth]{Figures/load_overhead_ppt.pdf}
    %\graphicsplaceholder{4cm}{1cm}%
  }
  \caption{Runtime slowdown of \tool on Scikit-learn, Numexpr, and NumpyDL with sampling rates of 500K, 1M, and 5M. The y-axis denotes slowdown ratio and the x-axis denotes program name.}
  \label{fig:slowdown}
\end{figure*}


\begin{figure*}[!t]
  \centering
  \subfloat[Redundant Stores Detection]{
  \label{fig:memory:store}     
  \includegraphics[width=0.492\textwidth]{Figures/store_memory_ppt.pdf}
    %\graphicsplaceholder{4cm}{1cm}%
  }
  \subfloat[Redundant Loads Detection]{ 
  \label{fig:memory:load} 
  \includegraphics[width=0.492\textwidth]{Figures/load_memory_ppt.pdf}
    %\graphicsplaceholder{4cm}{1cm}%
  }
  \caption{Memory bloating of \tool on Scikit-learn, Numexpr, and NumpyDL with sampling rates of 500K, 1M, and 5M. The y-axis denotes slowdown ratio and the x-axis denotes program name.}
  \label{fig:memory}
\end{figure*}



\subsection{Overhead}

This section reports the runtime slowdown and memory bloating caused by \tool. We measure runtime slowdown by the ratio of program execution time with \tool enabled over its vanilla execution time. Memory bloating shares the same measuring method but with the peak memory usage. %{\color{red}do not start a new paragraph?} 

Since Python does not have standard benchmarks, we evaluate the overhead of \tool on three popular Python applications --- Scikit-learn, Numexpr~\cite{numexpr}, and NumpyDL~\cite{numpydl} which contain benchmark programs from scientific computing, numerical expression and deep learning domains. We report only the first half of the Scikit-learn benchmark due to space limitations, and exclude {\tt varying-expr.py} from Numexpr, {\tt cnn-minist.py} and {\tt mlp-minist.py} from NumpyDL due to large variations in memory consumption, or the runtime errors of vanilla runs {\tt cnn-minist.py} and {\tt mlp-minist.py}.



% \jtan{Only the evaluation result of the first half of scikit-learn benchmark set is reported due to the space limitation}, and three programs ({\tt varying-expr.py} from Numexpr, {\tt cnn-minist.py} and {\tt mlp-minist.py} from NumpyDL) are \jtan{excluded due to either huge memory consumption variances of vanilla runs ({\tt varying-expr.py}), or runtime errors of vanilla runs ({\tt cnn-minist.py}, and {\tt mlp-minist.py}).} 


We run each experiment three times, and report the average overhead. Furthermore, the overhead of \tool is evaluated with three commonly-used sampling rates, 500K, 1M, and 5M.

Figure~\ref{fig:slowdown:store} shows the runtime slowdown of the redundant stores detection. The geo-means are 1.09$\times$, 1.07$\times$, and 1.03$\times$ under the sampling rates of 500K, 1M, and 5M, and the medians are 1.08$\times$, 1.05$\times$, and 1.03$\times$, respectively. Figure~\ref{fig:slowdown:load} shows the runtime slowdown of the redundant loads detection. The geo-means are 1.22$\times$, 1.14$\times$, and 1.05$\times$, under the sampling rates of 500K, 1M, and 5M, and the medians are 1.22$\times$, 1.11$\times$, and 1.04$\times$, respectively. The runtime slowdown drops as sampling rate decreases, because more PMUs samples incur more frequent profiling events, such as inspecting Python runtime, querying the CCT, and arming/disarming watchpoints to/from the debug registers. Redundant loads detection incurs more runtime slowdown compared to redundant stores detection, because programs usually have more loads than stores. Another reason is that \tool sets {\tt RW\_TRAP} for the debug register to monitor memory loads (x86 does not provide trap on read-only facility) which traps on both memory stores and loads. Even though \tool ignores the traps triggered by memory stores, monitoring memory loads still incurs extra overhead.

% \textcolor{red}{\tool sets {\tt RW\_TRAP} for the debug register to monitor memory loads (x86 does not provide trap on read-only facility) which traps on both memory stores and loads. Even though \tool ignores the traps triggered by memory stores, it still incurs extra overhead.}




Figure~\ref{fig:memory:store} shows memory bloating of the redundant stores detection. The geo-means are 1.25$\times$, 1.24$\times$, and 1.23$\times$ under the sampling rates of 500K, 1M, and 5M, and the medians are 1.18$\times$, 1.18$\times$, and 1.16$\times$, respectively. Figure~\ref{fig:memory:load} reports memory bloating of the redundant loads detection. The geo-means are 1.67$\times$, 1.56$\times$, and 1.29$\times$ under the same sampling rates, and the medians are 1.52$\times$, 1.51$\times$, and 1.24$\times$, respectively. Memory bloating shows a similar trend to runtime slowdown with varied sampling rates and between two kinds of inefficiency detection. The extra memory consumption is caused by the larger CCT required for the larger number of unique call paths. {\tt issue36}, {\tt vmltiming2}, and {\tt cnnsentence} suffer the most severe memory bloating due to the small memory required by their vanilla 
runs.  \tool consumes a fixed amount of memory because some static structures are irrelevant to the testing program. Thus, a program has a higher memory bloating ratio if it requires less memory for a vanilla run. {\tt mlpdigits} consumes more memory for redundant loads detection, because {\tt mlpdigits} (a deep learning program) contains a two-level multilayer perceptron (MLP) that has more memory loads than stores. %, because it only needs to update weights of two dense layers during computation.

Although lower sampling rates reduce overhead, the probability of missing some subtle inefficiencies increases. To achieve a better trade-off between overhead and detecting ability, we empirically select 1M as our sampling rate.







% \begin{table*}[t]
%     \centering
%     \caption{Overview of performance improvement guided by \tool.}
%     \label{table}
%     %\vspace{-1em}
%     \scriptsize
%     %\fontsize{10pt}{20pt}
%     \LARGE
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{||c|c|c||c|c||l|l||}
%     %\toprule
%     \hline
%         \multicolumn{3}{||c||}{Program Information} &
%         \multicolumn{2}{c||}{Inefficiency} & 
%         \multicolumn{2}{c||}{Optimization}  \\
%     \hline
%         Applications & Library & Problem Code & Category & Pattern$^\#$  & AS $^+$ & LS$^{*}$ \\
%     \hline
%     \hline
%         \multirow{2}{3em}{Ta~\cite{ta}} & \multirow{2}{2em}{Ta}  & 
%         trend.py(536, 549, 557,  & Slice Underutilization & \multirow{2}{0.5em}{$L$}  & \multirow{2}{2em}{1.1$\times$}  & \multirow{2}{3em}{16.55$\times$} \\
        
%         &   & 
%         571, 579) volatily.py(45) &   &    &   &  \\        
%     \hline
%         NumPyCNN~\cite{numpycnn} & Numpy~\cite{oliphant2006guide, van2011numpy}  & numpycnn.py(161)  & Loop-invariant Computation   & $S$  & 1.83$\times$  & 2.04$\times$    \\ 
%     \hline
%         Census\_main & NumpyWDL~\cite{numpywdl}  & ftrl.py(60)  & Loop-invariant Computation  &  $S$ & 1.03$\times$  & 1.13$\times$   \\ 
%     \hline
%         Lasso & Scikit-learn~\cite{scikit-learn}  & least\_angle.py(456, 458)  & Inefficient algorithm  & $S$ &  1.22$\times$ & 6.10$\times$ \\ 
%     \hline
%         \multirow{2}{5em}{IrisData~\cite{irisdata}} & \multirow{2}{3em}{Numpy}  & nn\_backprop.py(222, 228,   &  \multirow{2}{9em}{Slice Underutilization}  & \multirow{2}{0.5em}{$L$}  & \multirow{2}{2em}{1.96$\times$}  & \multirow{2}{2em}{2.02$\times$}   \\
    
%         &  & 247, 256, 263, 271, 278)  &   &  &  &    \\
%     \hline
%         \multirow{2}{5em}{Network} & Neural-network-  & \multirow{2}{9em}{network.py(103-115)}  & \multirow{2}{10em}{Redundant Computation}   & \multirow{2}{0.5em}{$L$} &  \multirow{2}{2em}{1.03$\times$}  &  \multirow{2}{2em}{1.05$\times$} \\ 
        
%         & from-scratch  &  &   &  &   &   \\ 
%     \hline 
%         Cnn-from-scratch~\cite{cnnscratch} & Numpy  & conv.py(62)  &  Slices Underutilization  & $L$  & 2.52$\times$  & 3.85$\times$    \\ 
%     \hline
%         \multirow{5}{6em}{Metaheuristics~\cite{meta}} & \multirow{5}{3em}{Numpy} & FunctionUtil.py(374)  & Native Function Misuse & $L$  & 1.41$\times$  & 1.86$\times$    \\
%     %\hline
%     \cline{3-7}
%         &  & FunctionUtil.py(270)  &  Slices Underutilization & $L$ &  6.34$\times$  & 27.26$\times$ \\
%     %\hline
%     \cline{3-7}
%         &  & FunctionUtil.py(309, 375)  &  Loop-invariant Computation  & $S$  & 1.04$\times$  & 1.40$\times$    \\
%     %\hline
%     \cline{3-7}
%         &  & FunctionUtil.py(437) &  Redundant Computation & $L$   & 1.02$\times$  & 1.09$\times$ \\
%     \cline{3-7}
%         &  & EPO.py(40)  & Loop-invariant Computation  & $S$    & 1.10$\times$  & 1.13$\times$    \\        
%     \hline
%     %\midrule
%         LinearRegression~\cite{linearregression} & LinearRegression  & LinearRegression.py(49, 50)  & Redundant Computation   & $L$  & 1.39$\times$  & 1.48$\times$    \\
%     \hline
%         Pytorch-examples~\cite{pytorch-example} & PyTorch~\cite{paszke2017automatic}  & adam.py:loop(66)  & Redundant computation   & $L$   &  1.02$\times$ & 1.07$\times$     \\ 
%     \hline
%         Cholesky~\cite{zhou2020harp} & PyTorch  & cholesky.py(76)  & Slices Underutilization  & $L$  & 3.20$\times$  & 3.87$\times$    \\
%     \hline
%         GGNN.pytorch~\cite{ggnn} & PyTorch  & model.py(122, 125)  & Loop-invariant Computation & $S$   & 1.03$\times$  &  1.07$\times$   \\
%     \hline
%         Network-sliming~\cite{Liu_2017_ICCV} & \multirow{2}{*}{Torchvision~\cite{torchvision}}  & \multirow{2}{*}{functional.py(164)}  & \multirow{2}{*}{Slices Underutilization} & \multirow{2}{*}{$L$}   & 1.10$\times$  &  1.68$\times$ \\
%     \cline{1-1}
%     \cline{6-7}
%         Pytorch-sliming~\cite{Liu_2017_ICCV} &   &   &   &    &  1.04$\times$ &  1.68$\times$ \\
%     % \hline
%     %      &   & transforms.py & Identical computation   &  &   & 1.02$\times$  & 2.83$\times$    \\
%     \hline
%         Fourier-Transform~\cite{fourier} & \multirow{3}{*}{Matplotlib~\cite{Hunter:2007}}  & \multirow{3}{*}{transforms.py(1973)} & \multirow{3}{*}{Input-sensitive Identical}   & \multirow{3}{*}{$S$}   & 1.02$\times$  & 2.83$\times$    \\
%     %\hline
%     \cline{1-1}
%     \cline{6-7}
%         Jax~\cite{jax2018github} &   &  &   &   & 1.04$\times$  & 2.83$\times$  \\    
%     %\hline
%     \cline{1-1}
%     \cline{6-7}
%         Autograd~\cite{autograd} &   &    &   &   & 1.05$\times$  & 2.83$\times$  \\  
%     %\bottomrule
%     \hline

% \multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $^\#$: $L$ means {\it redundant loads}, $S$ means {\it redundant stores}} \\

% \multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $^+$: Application level speedup} \\
    
% \multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $^*$: Library level speedup} \\


%     \end{tabular}
%     }
%  %   \vspace{-1em}
%     %\caption{The Overview of \bench{} dataset.}
% \end{table*}



\begin{figure}[t]
\begin{lstlisting}[caption={Interaction inefficiency in CNN-from-Scratch due to slice underutilization.},label=lst:case1_org]
def backprop(self, d_L_d_out, learn_rate):
    d_L_d_filters = np.zeros(self.filters.shape)
    for im_region, i, j in self.iterate_regions(self.last_input):
        for f in range(self.num_filters):
            d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region
\end{lstlisting}
%\vspace{-1em}
\end{figure}



\begin{figure}[t]
\begin{lstlisting}[caption={Optimized code of Listing~\ref{lst:case1_org}, eliminates inefficiencies by performing slice notation.},label=lst:case1_opt]
def backprop(self, d_L_d_out, learn_rate):
    d_L_d_filters = np.zeros(self.filters.shape)
    for im_region, i, j in self.iterate_regions(self.last_input):
        new_im_region = np.repeat(im_region[np.newaxis,:,:], 8, axis = 0)
        tmp = d_L_d_out[i, j, 0:self.num_filters]
        d_L_d_filters[0:self.num_filters] += tmp[:,None,None] * new_im_region
\end{lstlisting}
%\vspace{-1em}
\end{figure}



% Figure 7
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/stack.pdf}
	\caption{The redundant load pair reported by \tool for Listing~\ref{lst:case1_org}.}
	\label{fig:callpath1}
\end{figure}




% It is worth noting that the five inefficiency categories that we defined before (see Section~\ref{classification}) widely exist in these real-world applications\footnote{They also appear in other more than 100 real applications we studied.}, and \tool can detect all of them with redundant loads/stores detection. This proves \tool's strong effectiveness.
%It also proves the \tool's strong detecting ability, which can cover all these inefficiency categories with redundant loads/stores detection {\color{red}Notably, our five inefficiency categories appear in more than 100 if real applications we study}.


\section{Case Studies}
\label{casestudy}

This section discusses our three heuristic case studies. Our primary aim is to demonstrate the superior guidance provided by \tool for inefficiency detection and optimization.

\subsection{CNN-from-Scratch}
CNN-from-Scratch is an educational project that implements a Convolutional Neural Network. The code in Listing~\ref{lst:case1_org} performs tensor computation within a two-level nested loop. {\tt d\_L\_d\_filters} is a 8$\times$3$\times$3 tensor, {\tt d\_L\_d\_out} is a 26$\times$26$\times$8 tensor and {\tt im\_region} is a 3$\times$3 tensor. The inner loop iterates {\tt d\_L\_d\_filters} by its first dimension, iterates {\tt d\_L\_d\_out} by its third dimension. In each iteration of inner loop, {\tt d\_L\_d\_filters[f]} performs as a 3$\times$3 tensor, and {\tt d\_L\_d\_out[i, j, f]} is a number. The computation in line 5 is summarized as a 3$\times$3 vector cumulatively adding the multiplication of a number and a 3$\times$3 vector.







Figure~\ref{fig:callpath1} shows a redundant loads pair reported by \tool. The redundant pair is represented as hybrid call path, and the upper call path is killed by the lower call path. For each native call path, \tool reports the native function name, shared library directory, and the instruction pointer. For each Python call path, it reports the problematic code piece and its location in the source file. In this case, the call path pair reveals that the interaction inefficiency is introduced by line 62 of conv.py (line 5 in Listing~\ref{lst:case1_org}). The call path also shows that the inefficiency caused by \ff \ {\tt prepare\_index({\tt array\_subscript})}, denotes the redundant {\tt []} operations. This inefficiency belongs to the category of slice under-utilization. 

For optimization, we match the dimension of {\tt d\_L\_d\_filters}, {\tt d\_L\_d\_out}, and {\tt im\_region} by expanding the dimension of {\tt im\_region}, and use slice notation to replace the inner loop, as shown in Listing~\ref{lst:case1_opt}. The optimization yields a 3.9$\times$ function-level speedup and 2.5$\times$ application-level speedup.





% \begin{figure}[t]
% \begin{lstlisting}[caption={ta org},label=lst:]
% def _run(self):
%     ...
%     self._trs_initial = np.zeros(self._n-1)
%     self._trs = np.zeros(len(self._close) - (self._n - 1))
%     for i in range(1, len(self._trs)-1):
%         self._trs[i] = self._trs[i-1] - (self._trs[i-1]/float(self._n)) + tr[self._n+i]
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}


% \begin{figure}[t]
% \begin{lstlisting}[caption={ta opt},label=lst:]
% def _run(self):
%     ...
%     self._trs_initial = np.zeros(self._n-1)
%     self._trs = np.zeros(len(self._close) - (self._n - 1))
%     tmp = 1 - (1/float(self._n))
%     for i in range(1, len(self._trs)-1):
%         self._trs[i] = self._trs[i-1] * tmp
%     self._trs[1:(len(self._trs)-1)] += tr[self._n+1:(self._n+len(self._trs)-1)]
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}




% \begin{figure}[t]
% \begin{lstlisting}[caption={ta2 org},label=lst:]
% def adx(self) -> pd.Series:
%     ...
%     adx = np.zeros(len(self._trs))
%     for i in range(self._n+1, len(adx)):
%         adx[i] = ((adx[i-1] * (self._n - 1)) + dx[i-1]) / float(self._n)
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}


% \begin{figure}[t]
% \begin{lstlisting}[caption={ta2 opt},label=lst:]
% def adx(self) -> pd.Series:
%     ...
%     adx = np.zeros(len(self._trs))
%     for i in range(self._n+1, len(adx)):
%         adx[i] = (adx[i-1] * (self._n - 1))/float(self._n)
%     adx[self._n+1:len(adx)] += dx[self._n:(len(adx)-1)] / float(self._n)
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}



% \begin{figure}[t]
% \begin{lstlisting}[caption={NumpyWDL org (loop invariant)},label=lst:]
% def predict_logit(self, feature_ids, feature_values):
%     ...
%     for feat_id, feat_val in zip(feature_ids, feature_values):
%         z = self._z[feat_id]
%         sign_z = -1. if z < 0 else 1.
%         if abs(z) > self._L1:
%             w = (sign_z * self._L1 - z) / ((self._beta + np.sqrt(self._n[feat_id])) / self._alpha + self._L2)
%             ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}


% \begin{figure}[t]
% \begin{lstlisting}[caption={NumpyWDL opt (loop invariant)},label=lst:]
% def predict_logit(self, feature_ids, feature_values):
%     ...
%     tmp1 = self._alpha + self._L2
%     tmp2 = self._beta / tmp1
%     for feat_id, feat_val in zip(feature_ids, feature_values):
%         z = self._z[feat_id]
%         sign_z = -1. if z < 0 else 1.
%         if abs(z) > self._L1:
%             w = (sign_z * self._L1 - z) / (tmp2 + (np.sqrt(self._n[feat_id]) / tmp1))
%             ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}






\begin{figure}[t]
\begin{lstlisting}[caption={Interaction inefficiency in Metaheuristic due to API misuse and loop-invariant computation.},label=lst:case2_org]
def CEC_10(solution=None, problem_size=None, shift=0):
    ...
    for i in range(dim):
        temp = 1
        for j in range(32):
            temp += i * (np.abs(np.power(2, j + 1) * x[i] - round(np.power(2, j + 1) * x[i]))) / np.power(2, j)
        A *= np.power(temp, 10 / np.power(dim, 1.2))
    ...
\end{lstlisting}
\end{figure}

\begin{figure}[t]
\begin{lstlisting}[caption={Optimized code of Listing~\ref{lst:case2_org}, eliminates inefficiencies with an appropriate API and memorization technique.},label=lst:case2_opt]
def CEC_10(solution=None, problem_size=None, shift=0):
    ...
    tmp_dim = 10 / np.power(dim, 1.2)
    for i in range(dim):
        temp = 1
        for j in range(32):
            frac, whole = math.modf(np.power(2, j + 1) * x[i])
            temp += i * np.abs(frac) / np.power(2, j)
        A *= np.power(temp, tmp_dim)
    ...
\end{lstlisting}
\end{figure}

%, eliminates inefficiencies by an appropriate API and memorization technique.












\subsection{Metaheuristics}


\sloppy
Listing~\ref{lst:case2_org} is a code snippet from Metaheuristics. It performs complex numerical computation in a two-level nested loop, where {\tt x} is a Numpy array. \tool reports a redundant loads on line 6, where the code triggers the redundant native function call {\tt array\_multiply} and {\tt LONG\_power}. Guided by this, we observe that {\tt np.abs(np.power(2,j+1)*x[i]} is calculated twice within every iteration, because the code aims to get the computation result's fraction part. To eliminate the redundant computation, we use {\tt math.modf} function to calculate the fraction directly. 

This inefficiency belongs to the category of API misuse in native libraries. \tool also reports redundant stores in line 7 with native function {\tt LONG\_power}. Upon further investigation, we find the result of {\tt np.power(dim, 1.2)} does not change among iterations, which belong to loop-invariant computation. For optimization, we use a local variable to store the result outside the loop and reuse it among iterations. The appropriate usage of API yields 1.4$\times$ application-level speedup and 1.9$\times$ function-level speedup, and eliminating loop invariant computation yields 1.04$\times$ application-level speedup and 1.4$\times$ function-level speedup, respectively.





% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{asplos21/paper/Figures/stack2.pdf}
% 	\caption{call path of Listing~\ref{lst:case2_org}}
% 	\label{fig:path2}
% \end{figure}



% \begin{figure}[t]
% \begin{lstlisting}[caption={},label=lst:]

% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}




% \begin{figure}[t]
% \begin{lstlisting}[caption={ta org},label=lst:]
% def _run(self):
%     ...
%     self._trs_initial = np.zeros(self._n-1)
%     self._trs = np.zeros(len(self._close) - (self._n - 1))
%     for i in range(1, len(self._trs)-1):
%         self._trs[i] = self._trs[i-1] - (self._trs[i-1]/float(self._n)) + tr[self._n+i]
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}


% \begin{figure}[t]
% \begin{lstlisting}[caption={ta opt},label=lst:]
% def _run(self):
%     ...
%     self._trs_initial = np.zeros(self._n-1)
%     self._trs = np.zeros(len(self._close) - (self._n - 1))
%     tmp = 1 - (1/float(self._n))
%     for i in range(1, len(self._trs)-1):
%         self._trs[i] = self._trs[i-1] * tmp
%     self._trs[1:(len(self._trs)-1)] += tr[self._n+1:(self._n+len(self._trs)-1)]
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}





\subsection{Technical Analysis} Technical Analysis (Ta)~\cite{ta} is a technical analysis Python library. Listing~\ref{lst:ta1} is a problematic code region of Ta, where {\tt adx} and {\tt dx} are two multi-dimension Numpy arrays, and a loop iterates them and performs numerical calculations. 

\tool reports redundant loads in line 6 with native function {\tt array\_subscript}, which denotes the code that suffers from the inefficiency of slice underutilization. Unfortunately, we cannot eliminate the loop because {\tt adx} has computing dependency among the iterations. Therefor, we optimize the access to {\tt dx} with slice notation shown in Listing~\ref{lst:ta2}. Eliminating all similar patterns in Ta yields 1.1 $\times$ application-level speedup and 16.6$\times$ function-level speedup.


\begin{figure}[t]
\begin{lstlisting}[caption={Interaction inefficiency in Ta due to slice underutilization.},label=lst:ta1]
def adx(self) -> pd.Series:
    ...
    adx = np.zeros(len(self._trs))
    tmp = (self._n - 1)/float(self._n)
    for i in range(self._n+1, len(adx)):
        adx[i] = adx[i-1] * tmp + dx[i-1] / float(self._n)
    ...
\end{lstlisting}
%\vspace{-1em}
\end{figure}


\begin{figure}[t]
\begin{lstlisting}[caption={Optimized code of Listing~\ref{lst:ta1}, eliminates inefficiencies by performing slice notation.},label=lst:ta2]
def adx(self) -> pd.Series:
    ...
    adx = np.zeros(len(self._trs))
    tmp = (self._n - 1)/float(self._n)
    for i in range(self._n+1, len(adx)):
        adx[i] = adx[i-1] * tmp
    adx[self._n+1:len(adx)] += dx[self._n:(len(adx)-1)] / float(self._n)
    ...
\end{lstlisting}
%\vspace{-1em}
\end{figure}


% \begin{figure}[t]
% \begin{lstlisting}[caption={ta2 org},label=lst:]
% def adx(self) -> pd.Series:
%     ...
%     adx = np.zeros(len(self._trs))
%     tmp = (self._n - 1)/float(self._n)
%     for i in range(self._n+1, len(adx)):
%         adx[i] = ((adx[i-1] * (self._n - 1)) + dx[i-1]) / float(self._n)
%     ...
% \end{lstlisting}
% %\vspace{-1em}
% \end{figure}