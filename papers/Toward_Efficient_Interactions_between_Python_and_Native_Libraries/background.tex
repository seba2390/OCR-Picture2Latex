\section{Background and Related Work}
\label{background}
%Sec.~\ref{sec:prun} describes Python rumtime. Sec.~\ref{sec:related} describes related works. 
%In this section, we first introduce Python runtime and its interaction with native libraries. We then review existing related tools to distinguish \tool{}.

\subsection{Python Runtime}
\label{sec:prun}

\myparabb{\textbf{Background on Python.}} Python is an interpreted language with dynamic features. When running a Python application, the interpreter translates Python source code into stack-based bytecode and executes it on the Python virtual machine (PVM), which varies implementations such as CPython~\cite{cpython}, Jython~\cite{jpython}, Intel Python~\cite{intelpython} and PyPy~\cite{pypy}. This work focuses on CPython because it is \emph{the  reference implementation}~\cite{pyimplementation}, while
% This work considers CPython because it is the reference implementation~\cite{pyimplementation} widely used by the community.
%(CITE https://wiki.python.org/moin/PythonImplementations)
the proposed techniques are generally applicable to other Python implementations as well. %\yu{In this paper, we study CPython based on: 1. CPython is the most popular Python implementation 2. interaction inefficiencies behave similarly on different implementations}. 
The CPython PVM maintains the execution call stack that consists of a chain of {\tt PyFrame} objects known as function frames.  Each {\tt PyFrame} object includes the executing context of corresponding function call, such as local variables, last call instruction, source code file, and current executing code line, which can be leveraged by performance or debugging tools.

%The PVM is a binary executable, it uses a call stack to build the main structure of the Python application. The call stack stores a type of objects called {\tt PyFrame}. Each function call will push a new {\tt PyFrame} object into call stack, and its {\tt PyFrame} object is popped off when the function returns. The {\tt PyFrame} object contains the executing context of corresponding function call, such as local variables, last called instruction, source code file and current executing code line.

\sloppy
Python supports multi-threaded programming, where each Python thread has an individual call stack. Because of the global interpreter lock (GIL)~\cite{gil}, the concurrent execution of Python threads is emulated as regular switching threads by the interpreter, i.e., for one interpreter instance, only one Python thread is allowed to execute at a time.

%It means that, for one interpreter instance, only one Python thread is allowed to execute at any time. 


% \begin{table*}[!htbp]
%     \centering
%     %\vspace{-1em}
%     %\fontsize{10pt}{0}
%     %\large
%     %\scriptsize
%     \scriptsize
%     %\small
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{||c|c|c||c|c||c|c||}
%     %\toprule
%     \hline
%         \multicolumn{3}{||c||}{Program Information} &
%         \multicolumn{2}{c||}{Inefficiency} & 
%         \multicolumn{2}{c||}{Optimization}  \\
%     \hline
%         Applications & Library & Problem Code & Category & Pattern  & AS & FS \\
%     \hline
%     \hline
%         \multirow{2}{3em}{Ta~\cite{ta}} & \multirow{2}{2em}{Ta}  & 
%         volatily.py(45)/trend.py(536,  &  \multirow{2}{*}{Slice Underutilization}  & \multirow{2}{*}{$L$}    & \multirow{2}{2em}{1.1$\times$}  & \multirow{2}{3em}{16.6$\times$} \\
        
%         &   & 549, 557, 571, 579)
%          &    &    &   &  \\        
%     \hline
%         NumPyCNN~\cite{numpycnn} & Numpy~\cite{oliphant2006guide, van2011numpy}  & numpycnn.py(161)  & Loop-invariant Computation  & $S$  & 1.8$\times$  & 2.04$\times$    \\ 
%     \hline
%         Census\_main & NumpyWDL~\cite{numpywdl}  & ftrl.py(60)  & Loop-invariant Computation &  $S$  & 1.03$\times$  & 1.1$\times$   \\ 
%     \hline
%         Lasso & Scikit-learn~\cite{scikit-learn}  & least\_angle.py(456, 458)  & Inefficient Algorithms & $S$  &  1.2$\times$ & 6.1$\times$ \\ 
%     \hline
%         \multirow{2}{5em}{IrisData~\cite{irisdata}} & \multirow{2}{3em}{Numpy}  & nn\_backprop.py(222, 228,    &  Slice Underutilization \& & \multirow{2}{*}{$L$}  & \multirow{2}{2em}{2$\times$}  & \multirow{2}{2em}{2.02$\times$}   \\
    
%         &  & 247, 256, 263, 271, 278)  & API Misuse  &    &  &    \\
%     \hline
%         \multirow{2}{5em}{Network} & Neural-network-  & \multirow{2}{9em}{network.py(103-115)}  & \multirow{2}{8em}{Repeated NFC with the same Augments}   & \multirow{2}{*}{$L$} &  \multirow{2}{2em}{1.03$\times$}  &  \multirow{2}{2em}{1.05$\times$} \\ 
        
%         & from-scratch  &  &  &   &   &   \\ 
%     \hline 
%         Cnn-from-scratch~\cite{cnnscratch} & Numpy  & conv.py(62)  &  Slice Underutilization  & $L$  & 2.5$\times$  & 3.9$\times$    \\ 
%     \hline
%         \multirow{5}{*}{Metaheuristics~\cite{nguyen2019building, nguyen2018resource}} & \multirow{5}{*}{Numpy} & FunctionUtil.py(374)  & API Misuse
%         & $L$  & 1.4$\times$  & 1.9$\times$    \\
%     %\hline
%     \cline{3-7}
%         &  & FunctionUtil.py(270)  &  Slice Underutilization & $L$ &  6.3$\times$  & 27.3$\times$ \\
%     %\hline
%     \cline{3-7}
%         &  & FunctionUtil.py(309, 375)  &  Loop-invariant Computation & $S$  & 1.04$\times$  & 1.4$\times$    \\
%     %\hline
%     \cline{3-7}
%         &  & \multirow{2}{*}{FunctionUtil.py(437)} & \multirow{2}{8em}{Repeated NFC with the same Augments} & \multirow{2}{*}{$L$} &  \multirow{2}{*}{1.02$\times$}  & \multirow{2}{*}{1.1$\times$} \\
        
%         &  &  &  &  &    &  \\
%     \cline{3-7}
%         &  & EPO.py(40)  & Loop-invariant Computation & $S$ & 1.1$\times$  & 1.1$\times$    \\        
%     \hline
%     %\midrule
%         \multirow{2}{*}{LinearRegression~\cite{linearregression}} & \multirow{2}{*}{LinearRegression}   & \multirow{2}{*}{LinearRegression.py(49, 50)}  & \multirow{2}{8em}{Repeated NFC with the same Augments}  & \multirow{2}{*}{$L$} & \multirow{2}{*}{1.4$\times$}  & \multirow{2}{*}{1.5$\times$}    \\
        
%         &  &   &   &  &  &    \\
%     \hline
%         Pytorch-examples~\cite{pytorch-example} & PyTorch~\cite{paszke2017automatic}  & adam.py:loop(66)  & Loop-invariant Computation  & $L$ &  1.02$\times$ & 1.07$\times$     \\ 
%     \hline
%         Cholesky~\cite{zhou2020harp} & PyTorch  & cholesky.py(76)  & Slice Underutilization  & $L$ & 3.2$\times$  & 3.9$\times$    \\
%     \hline
%         GGNN.pytorch~\cite{ggnn} & PyTorch  & model.py(122, 125)  & Loop-invariant & $S$  &  1.03$\times$  &  1.07$\times$   \\
%     \hline
%         Network-sliming~\cite{Liu_2017_ICCV} & \multirow{2}{*}{Torchvision~\cite{torchvision}}  & \multirow{2}{*}{functional.py(164)}  & \multirow{2}{*}{Slice Underutilization} & \multirow{2}{*}{$L$}  & 1.1$\times$  &  1.7$\times$ \\
%     \cline{1-1}
%     \cline{6-7}
%         Pytorch-sliming~\cite{Liu_2017_ICCV} &   &   &   &  & 1.04$\times$ &  1.7$\times$ \\
%     % \hline
%     %      &   & transforms.py & Identical computation   &  &   & 1.02$\times$  & 2.83$\times$    \\
%     \hline
%         Fourier-Transform~\cite{fourier} & \multirow{3}{*}{Matplotlib~\cite{Hunter:2007}}  & \multirow{3}{*}{transforms.py(1973)} & \multirow{3}{8em}{Repeated NFC with the same Augments}   & \multirow{3}{*}{$S$} & 1.02$\times$  & 2.8$\times$    \\
%     %\hline
%     \cline{1-1}
%     \cline{6-7}
%         Jax~\cite{jax2018github} &   &  &   &   & 1.04$\times$  & 2.8$\times$  \\    
%     %\hline
%     \cline{1-1}
%     \cline{6-7}
%         Autograd~\cite{autograd} &   &   &   &   & 1.05$\times$  & 2.8$\times$  \\  
%     %\bottomrule
%     \hline

% %\multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $Pattern^*$: $L$ means {\it redundant loads}, $S$ means {\it redundant stores}} \\

% % \multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $AS^+$: Application level speedup} \\
    
% % \multicolumn{6}{l}{{\vbox to 2ex{\vfil}} $LS^*$: Library level speedup} \\
%     \end{tabular}
%     }
%     \caption{Overview of performance improvement guided by \tool. {\it AS} denotes application-level speedup, {\it FS} denotes function-level speedup. $L$ refers to redundant loads, $S$ refers to redundant stores.}
%     \label{table}
% \end{table*}


%{\tt PyObject} is the super class of all objects. All objects are treated as {\tt PyObject} structure, which is the universal initial segment. {\tt PyObject} structure contains basic information like object type and reference count.

\myparabb{\textbf{Interaction with native libraries.}}
When heavy-lifting computation is needed, Python applications usually integrate native libraries written in C/C++/Fortran for computation kernels, as shown in Figure~\ref{fig:hybridmode}. Such libraries include Numpy~\cite{van2011numpy, harris2020array}, Scikit-learn~\cite{scikit-learn}, Tensorflow~\cite{tensorflow2015-whitepaper}, and PyTorch~\cite{paszke2017automatic}. Therefore, modern software packages enjoy the benefit from the simplicity and flexibility of Python and native library performance. When the Python runtime calls a native function, it passes the {\tt PyObject}\footnote{{\tt PyObject} is the super class of all objects in Python.} or its subclass objects to the native function. The Python runtime treats the native functions as blackboxes --- the Python code is blocked from execution until the native function returns.

Figure~\ref{fig:hybridmode} shows an abstraction across the boundary of Python runtime and native library, which logically splits the entire software stack. On the upper level, Python applications are disjoint from their execution behaviors because Python runtime (e.g., interpreter and GC) hides most of the execution details. On the lower level, the native libraries lose most program semantic information. This knowledge gap leads to interaction inefficiencies.

% Python allows calling native functions for high performance. The Python runtime passes the {\tt PyObject}\footnote{{\tt PyObject} is the super class of all objects in Python.} or its subclass objects to the native function. The Python runtime treats the native functions as blackboxes---the Python code is blocked from execution until the native function returns. 

\iffalse
\yu{(ORIGINAL:) [[[Thus, modern Python applications usually integrate native libraries written in C/C++/Fortran for computation kernels, as shown in Figure~\ref{fig:hybridmode}. Such libraries include Numpy~\cite{van2011numpy, harris2020array}, Scikit-learn~\cite{scikit-learn}, Tensorflow~\cite{tensorflow2015-whitepaper}, and PyTorch~\cite{paszke2017automatic} to name a few. Therefore, modern software packages can enjoy the simplicity and flexibility from Python as well as performance from native library. However, such programming model complicates the software stack, which results in new performance inefficiencies. Figure~\ref{fig:hybridmode} shows an abstraction across the boundary of Python runtime and native library, which logically splits the entire software stack. On the upper level, Python applications are disjoined from their execution behaviors because Python runtime (e.g., interpreter and GC) hides most of the execution details. On the lower level, the native libraries lose most of program semantic information, resulting in a challenge of associating execution behaviors with high-level algorithms. This knowledge gap leads to a new type of performance losses, which we refer to as {\em interaction inefficiencies}.]]]}
\fi

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{Figures/software_stack_v2.pdf}
	\caption{The typical stack of production Python software packages. Python applications usually rely on native libraries for high performance but introduce an abstraction across the boundary of Python runtime and native libraries.}
	\label{fig:hybridmode}
\end{figure}




%{\it Native function call} is a common way to improve Python performance. To interact with Python code, native functions take {\tt PyObject}-wise data as input and output. When an interpreter instance calls a native function, it will give the function full control, and stay blocking until the function returns. Python runtime doesn't poke into native functions and treats each function as blackbox.

\begin{comment}

\myparabb{Python Call Path Profiling} Call path profiling~\cite{hall1992call} is a widely-used profiling technique, it builds the profiling metrics based on different call paths. For Python applications, a call path of an instruction contains both procedure frames (on the call stack) and Python function call chains (in the Python runtime) at the point when the instruction executes. A call path begins at Python runtime/thread's entry and ends at the instruction pointer. When collect a profiling event, the profiler runtime aggregates it based on the call path of instruction (that triggers the event). Flat profiling, which is function level measurement, will introduce ambiguities when a same leaf function is invoked from multiple call paths. Compared with flat profiling, call path profiling is more comprehensive for complex applications.
%Compared to the flat profiling, which is the function level measurement, a deep call path provides more insights to perform a holistic analysis for complex applications, because the flat profiling introduces ambiguities when the same leaf function are invoked from multiple call paths.
\end{comment}

\subsection{Existing Tools vs. \tool}\label{sec:related}

%As the first to thoroughly characterize the interaction inefficiencies, we are not aware any related characterization or benchmarking work in this domain, to the best of our knowledge.

This section compares existing tools that analyze inefficiencies in Python and native codes to distinguish \tool.

\myparabb{\textbf{Python performance analysis tools.}}
PyExZ3~\cite{irlbeck2015deconstructing}, PySym~\cite{pysym}, flake8~\cite{flake8}, and Frosted~\cite{frosted} analyze Python source code and employ multiple heuristics to identify code issues statically~\cite{gulabovska2019survey}. XLA~\cite{xla2017xla} and TVM~\cite{chen2018tvm} apply compiler techniques to optimize deep learning applications. Harp~\cite{zhou2020harp} detects inefficiencies in Tensorflow and PyTorch applications based on computation graphs. All of these approaches, however, ignore Python dynamic behavior, omitting optimization opportunities. 


%These works all focus on machine learning applications. Compared to static analysis, \tool performs dynamic profiling to pinpoint inefficiencies, and diagnoses Python applications without domain constrains.

Dynamic profilers are a complementary approach. cProfile~\cite{cProfile} measures Python code execution, which provides the frequency/time executions of specific code regions. Guppy~\cite{guppy3} employs object-centric profiling, which associates metrics such as allocation frequency, allocation size, and cumulative memory consumption with each Python object. PyInstrument~\cite{pyinstrument} and Austin~\cite{austin} capture Python call stack frames periodically to identify executing/memory hotspots in Python code. PySpy~\cite{py-spy} is able to attach to a Python process and pinpoint function hotspots in real time. Unlike \tool, these profilers mainly focus on Python codes, with no insights into the native libraries.

Closely related to \tool{}, Scalene~\cite{berger2020scalene} separately attributes Python/native executing time and memory consumption. However, it does not distinguish useful/wasteful resources usage as \tool does. %Thus, unlike \tool, all Python profilers showing above are hard to identify interaction inefficiencies in Python software packages.

%PyProf~\cite{pyprof} profiles and analyzes the GPU performance of PyTorch models.



%have limited profiling scope in Python runtime without involving low-level views. Rather than locating hotspots, \tool identifies wasting resource usages. Though \tool is Python based profiler, it analyzes applications with a greater scope than above profilers, because \tool gathers information from whole software stack. 

% While all of these Python based profilers perform profiling limited in Python runtime scope without involving low-level views. Also they only locate application hotspots and fail to identify wasting resource usages.


\myparabb{Native performance analysis tools.} 
While there are many native profiling tools~\cite{reinders2005vtune, de2010new, adhianto2010hpctoolkit}, from which the most related to Python that can identify performance inefficiencies are Toddler~\cite{nistor2013toddler} that identifies redundant memory loads across loop iterations, and LDoctor~\cite{song2017performance} that reduces Toddler's overhead by applying dynamic sampling and static analysis. DeadSpy~\cite{chabbi2012deadspy}, RedSpy~\cite{wen2017redspy}, and LoadSpy~\cite{loadspy} analyze dynamic instructions in the entire program execution to detect useless computations or data movements. Unfortunately, all of them use heavyweight binary instrumentation, which results in high measurement overhead, and they do not work directly on Python programs.

\subsection{Performance Monitoring Units and Hardware Debug Registers}
Hardware performance monitoring units (PMUs) are widely equipped on the modern x86 CPU architectures. Software can use PMUs to count various hardware events like CPU cycles, cache misses, et cetera. Beside the counting mode that counts the total number of events, PMUs can be configured in sampling, which periodically sample a hardware event and record event's detailed information. PMUs trigger an overflow interrupt when the sample number reaches a threshold. The profiler runtime captures interrupts as signals and collects samples with their executing contexts.

For memory-related hardware events such as memory load and store, Precise Event-Based Sampling (PEBS)~\cite{pebs} in Intel processors provides the effective address and the precise instruction pointer for each sample. Instruction-Based Sampling (IBS)~\cite{ibs} in the AMD processors and Marked Events (MRK)~\cite{srinivas2011ibm} in PowerPC support similar functionalities.

Hardware debug registers~\cite{johnson1982some, mclear1982guidelines} trap the CPU execution when the program counter (PC) reaches an address (breakpoint) or an instruction accesses a designated address (watchpoint). One can configure the trap conditions with different accessing addresses, widths and types. The number of hardware debug registers is limited (e.g., the modern x86 processor has four debug registers).







% There exist various native profiling tools~\cite{reinders2005vtune, de2010new, adhianto2010hpctoolkit} and we only review most related ones that can identify performance inefficiencies.
% Toddler~\cite{nistor2013toddler} identifies redundant memory loads across loop iterations. The follow-up tool, LDoctor~\cite{song2017performance} reduces Toddler's overhead by applying dynamic sampling and static analysis. DeadSpy~\cite{chabbi2012deadspy}, RedSpy~\cite{wen2017redspy}, and LoadSpy~\cite{loadspy} analyze dynamic instructions in the entire program execution to detect useless computations or data movements. Unlike \tool, these tools utilize heavyweight binary instrumentation, resulting in high measurement overhead. Moreover, they do not work directly on Python programs.

% \yu{DELETE: 
% Perhaps, the most related work is Witch~\cite{wen2018watching}, which leverages hardware performance monitoring units (PMUs) and debug registers to identify redundant memory loads and stores in consecutive memory accesses to the same memory location. 
% The basic idea is to use PMUs to sample memory loads or stores and use debug registers to trap the next accesses to the same memory location. 
% If the values from the two loads or two stores are the same, the second load or store is redundant. 
% \tool relies on a similar approach to obtain the inefficiencies in native libraries. Unlike Witch, \tool integrates the semantics from Python runtime, which pioneers the analysis in interaction inefficiencies in Python applications.}

%Distinct from them, \tool doesn't involve instrumentation and achieve a much lower overhead. Witch identifies wasting memory operations in native language by sampling memory accesses with hardware PMUs, and intercepting the subsequent accesses with hardware debug registers. \tool employs a similar approach, as a specific profiler for Python, \tool detects inefficiencies by combining Python domain knowledge and system information rather than only detecting redundant memory operations.


% \myparabb{Hardware Performance Monitoring Units} Hardware performance monitoring units (PMUs) are widely equipped on the modern x86 CPU architectures. Software can use PMUs to count various hardware events like CPU cycles, cache misses, et cetera. Beside the counting mode that counts the total number of events, PMUs can be configured in sampling, which periodically samples a hardware event and records event's detailed information. PMUs trigger an overflow interrupt when the sample number reaches a threshold. The profiler runtime captures interrupts as signals and collects samples with their executing contexts.

% For memory-related hardware events such as memory load and store, Precise Event-Based Sampling (PEBS) in Intel processors provides the effective address and the precise instruction pointer for each sample. Instruction-Based Sampling (IBS) in the AMD processors and Marked Events (MRK) in PowerPC support similar functionalities. \yu{CITE}


% \myparabb{Hardware Debug Registers} Hardware debug registers trap the CPU execution when the program counter (PC) reaches an address (breakpoint) or an instruction accesses a designated address (watchpoint). One can configure the trap conditions with different accessing addresses, widths and types. The number of hardware debug registers is limited. For example, the modern x86 processors only have four debug registers.

% \myparabb{Linux perf\_event} Linux offers a standard interface to program PMUs and debug registers via the {\it perf\_event\_open} \yu{CITE}. Both the synchronous CPU trap that caused by hardware debug registers, and the CPU overflow interrupt triggered by PMUs, can be handled by Linux signals with {\it perf\_event} interface. All procedures executed in the signal handler need to be signal-safe. The profiler can {\it mmap} a circular buffer where the kernel keeps appending the PMU data of each sample. The profiler runtime can extract calling context via the signal context, to achieve the call path profiling.


