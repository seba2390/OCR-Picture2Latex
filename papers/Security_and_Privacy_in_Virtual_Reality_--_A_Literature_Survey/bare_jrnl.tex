%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and 
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


% \usepackage{draftwatermark}
% \SetWatermarkText{Confidential}
% \SetWatermarkScale{1}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.

\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{balance}

\usepackage{booktabs}
\usepackage{tabularx} \setlength{\tabcolsep}{4pt}
\usepackage{array,booktabs}
\usepackage{ragged2e}

% define a new column type for convenience
\newcolumntype{L}[1]{>{\hsize=#1\hsize\RaggedRight} X}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\usepackage{adjustbox}
\usepackage{array}

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{40}{1.5em}}}% no optional argument here, please!

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcolumntype{a}{>{\centering\arraybackslash\columncolor{Gray}}X}
\newcolumntype{d}{>{\centering\arraybackslash\columncolor{white}}X}

\newcolumntype{A}{>{\arraybackslash\columncolor{Gray}}X}



\usepackage{siunitx}
\newcolumntype{s}{>{\arraybackslash\columncolor{white}}S}
\newcolumntype{f}{>{\arraybackslash\columncolor{Gray}}S}

%\usepackage{todonotes}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\usepackage{pifont}
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Security and Privacy in Virtual Reality -\\A Literature Survey}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Alberto Giaretta% <-this % stops a space
\thanks{A. Giaretta is with the Department of Science and Technology, Centre for Applied Autonomous Sensor Systems, \"Orebro University, \"Orebro 70281, Sweden e-mail: alberto.giaretta@oru.se}% <-this % stops a space
\thanks{Manuscript received April 29, 2022; revised April 29, 2022.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of }%
{Giaretta: Security and Privacy in Virtual Reality - A Literature Survey}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Virtual Reality (VR) is a multibillionaire market that keeps growing, year after year. As VR is becoming prevalent in households and small businesses, it is critical to address the effects that this technology might have on the privacy and security of its users. In this paper, we explore the state-of-the-art in VR privacy and security, we categorise potential issues and threats, and we analyse causes and effects of the identified threats. Besides, we focus on the research previously conducted in the field of authentication in VR, as it stands as the most investigated area in the topic. We also provide an overview of other interesting uses of VR in the field of cybersecurity, such as the use of VR to teach cybersecurity or evaluate the usability of security solutions.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Virtual Reality, VR, privacy, security, cybersecurity, authentication
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{E}xtended Reality (XR) is a paradigm that refers to different types of human-machine interaction within real and virtual environments, enabled by computer technology and wearable devices. Under the umbrella of XR, Virtual Reality (VR) is a computer-generated simulation of a 3D environment that enables realistic physical interactions by means of technology and wearable devices. The popularity of VR has grown steadily over the years; according to Oberlo~\cite{oberlo}, the number of US consumers that used VR went from 16\% in 2019 to 19\% in 2020. Of those users, shows a report from eMarketer~\cite{emarketer}, 52.1 million people use VR technology at least once per month. Furthermore, the growth is expected to continue in the near future. By 2028, the compound annual growth rate of the VR market is expected to reach 18\%~\cite{grandview}, according to statistics by Grand View Research. In terms of market value, Valuates Reports~\cite{valuatesVR} projects that the global VR market size will reach US\$ 26860 million by 2027, from US\$ 7719.6 million in 2020.

By design, VR is capable of collecting a large amount of non-verbal information, such as users' movements, biometrics, and usage patterns. As shown in \Cref{fig:vr}, VR devices are equipped with different sensors that collect both explicit input and non-verbal information, which is used by the VR engine to model the virtual world according to the user's actions. As noted by Bailenson~\cite{bailenson2018}, non-verbal information is uniquely telling and can be used for different goals, from tailoring advertisements, to determining if users are low or high performers. Hours of personal use, paired with unclear policies regarding data handling and learning algorithms, could allow companies to estimate users' preferences and infer their characteristic behaviour.
Taking into account the consistent and considerable growth of VR, alongside its pervasive nature, it is of paramount importance to evaluate any privacy and security concern that could arise. In this paper, we provide a literature survey on VR threats and issues, in terms of privacy and security, with the aim of highlighting what has been done and what is still open for research.

\begin{figure*}[t]
	\center
	\includegraphics[width=0.98\textwidth,clip]{images/VR_Architecture.pdf}
	\caption{The Virtual Reality (VR) paradigm. Users perceive the VR environment using head-mounted displays (HMDs) and interact with it via controllers or more advanced techniques, such as finger tracking sensors. Every device is equipped with various sensors to provide useful information to the VR engine, which creates the virtual world and modifies it according to the input received from the user.}
	\label{fig:vr}
\end{figure*}

Within the umbrella term XR, besides VR, exist other types of extended reality. Namely, Augmented Reality (AR) and Mixed Reality (MR). In AR, virtual objects are added to the real environment for enriching it;a notable example is pointing a smartphone to an art piece and getting information about the piece itself. MR is a combination of VR and AR, where the interactions do not happen exclusively in the virtual space nor in the real one, but in a hybrid fashion. AR and MR exhibit their own set of challenges and issues, in terms of security and privacy, which are not necessarily similar to those found in VR. In 2014, Roesner et al.~\cite{roesner2014} conducted a survey on AR security and privacy concerns, while King et al.\cite{king2020} focused specifically on the privacy issues of AR applications. Regarding MR, De Guzman et al.~\cite{deguzman2019} conducted an exhaustive survey on the security and privacy issues that affect it. However, to our knowledge, a comprehensive survey on the security and privacy aspects of VR is lacking. In this paper, we aim to fill this gap, providing four main contributions:
\begin{itemize}
    \item An overview of privacy issues that affect VR, organised by typology and causes;
    \item An analysis of threats to VR security, categorised in traditional attacks that also affect VR, such as denial of service (DoS), and attacks specific for VR, such as dizziness inducing ones;
    \item An in-depth exploration on the research done on authentication aspects in VR;
    \item An overview of other uses of VR in the field of cybersecurity and physical security, such as teaching cybersecurity concepts or evaluating the usability of security solutions.
\end{itemize}

\subsection{Outline}
This paper is organised as follows. In \Cref{sec:privacy}, we discuss issues and threats to privacy within the VR domain. Similarly, in \Cref{sec:security} we discuss the threats to the security of VR and we dedicate \Cref{sec:authentication} to the topic of authentication for VR. In \Cref{sec:teach_train_ev} we discuss different uses of VR within the cybersecurity domain, such as teaching security concepts, training physical security, and evaluating the usability of security processes. Finally, in \Cref{sec:conclusion} we provide some final remarks and provide our conclusions.

\section{Privacy Issues in VR}\label{sec:privacy}
\begin{figure*}[t]
	\center
	\includegraphics[width=0.98\textwidth,clip]{images/privacy_issues.pdf}
	\caption{Privacy issues in VR. In purple the three main categories, further specified in subcategories and specific concerns.}
	\label{fig:privacy}
\end{figure*}

VR technology raises a number of concerns about the privacy of its users. In this section, we provide an overview of the different threats to privacy in the context of VR, following the structure shown in \Cref{fig:privacy}. In 2018, Adams et al.~\cite{adams2018} identified to which extent users and developers are concerned about privacy in VR. They conducted interviews with 20 participants, 10 users and 10 developers, showing that 6 users and 6 developers raised concerns on the data collection practises, while 7 participants out of 20 expressed general mistrust in headset producers. The authors also highlighted that developers show consideration for users' privacy, but that there is a marked disconnection between general concerns and concerns about their own products. One of the interviewees expressed concern for the perception capabilities of the VR sensors and cameras. As shown by Kreylos~\cite{kreylos2017}, Oculus Rift sensors are regular webcams, equipped with a firmware and a physical filter that allow infrared light to pass, while removing visible light. However, Kreylos showed that it is possible to operate software manipulations and extract realistic pictures from the Oculus camera, even though it is meant to perceive only the infrared spectrum.

O'Brolch\'ain et al.~\cite{obrolchain2016} discuss the privacy concerns that derive from the combination of VR and social networks. They divide threats to privacy and privacy issues into three categories: 
\begin{itemize}
    \item Associational privacy, in particular the ability to include and exclude people from certain events or the \textit{global village}, in which everyone could potentially learn important and trivial matters about others;
    \item Informational privacy, in particular the increased vulnerability of data or its misuse;
    \item Physical privacy, in particular the prevalence of recording devices or the unintended revelation of physical information (such as physical reactions to ads and the loss of anonymity).
\end{itemize}

%informational privacy (increased vulnerability of data, misuse of data), physical privacy (prevalence of recording devices, unintended revelation of physical information, such as physical reactions to ads, loss of anonymity), and threats to associational privacy (online socialising, intended as the ability of including or excluding people from certain events, the 'global village' problem in which everyone could potentially come to know important and trivial matters). 
Although this categorisation focuses on the threats arising from the combination of VR and social networks, it applies equally well to general privacy threats to VR. In addition to privacy issues, the authors note that when privacy cannot be ensured, the autonomy of individuals (i.e., their ability to form independent opinions, plans, and goals) is also at stake, because autonomy is intertwined with privacy. Human beings need privacy to think through their problems, make mistakes, and explore different possibilities. Without the guarantee of privacy, users might experience a decrease in autonomy or even lose it.


\subsection{Threats to Associational Privacy}
In 2016, Nwaneri~\cite{nwaneri2016} noted that Facebook has a track record of experimenting with its users (e.g., showing that removing negative or positive news from feeds affected their mood). They also noted that the Oculus policies were phrased in such a way that it would allow Facebook to conduct similar experiments. The risk to users' privacy would be even higher than the ones found in social networks, given the kind of data gathered by VR systems. In particular, Nwaneri argues that the policy statement regarding collecting "information about [the users'] physical movements and dimensions when [they] use a virtual reality headset" could facilitate discrimination and mass surveillance. According to the author, failing to address these points could lead companies to face serious consequences, such as accusations of violating state privacy and wiretapping laws. On a side note, Nwaneri highlights that the long-term effects of immersive sensory experience granted by VR are still unknown. VR environments might cause safety risks in their users, such as seizures, post-traumatic stress disorder, or interference with childhood mental development. Therefore, it is in the best interest of VR companies to push for clear policies and legislation that regulate their business.

Tromp et al.~\cite{tromp2018} elaborated more on the issues of combining social networks and VR, to produce multi-user social VR environments. Among various psychological risks, which fall outside the scope of our work, the authors list privacy. Head-mounted displays (HMDs), due to the sensors they are equipped with, record detailed user's physical and psychological responses to stimuli. These responses could be analyzed automatically and used for targeting users with tailored advertising, a method known as \textit{neuromarketing}. In their work, they provided some recommendations to different actors involved with VR applications, such as users, parents, software and hardware companies, regulatory institutions, and researchers. For example, it is still unclear whether VR addiction exhibits similar characteristics to internet use addiction. Therefore, psychologists should establish if the same treatments can be used and VR users should be aware that doctors may not yet know how to treat such addiction.

\subsection{Threats to Informational Privacy}
Regarding the legislative issues, Lake~\cite{lake2020} highlights that the current US legal framework for online identity misappropriation does not adequately protect VR users. In particular, the author notes that the combination of different laws leaves plaintiffs without any party to sue: anonymity laws make it impossible to sue the identity appropriator and ISP immunity also prevents the victim from suing the VR provider. Until these issues are addressed (for example, following some of the suggestions given by Lake), victims of identity theft in VR environments cannot even bring their claims to US courts.

Kumarapeli~\cite{kumarapeli2021} highlights that the major players in VR are now focused on producing realistic avatars, tracking facial expressions and user body movement. On the one hand, this can help to enrich communications in VR. On the other hand, the capability of tracking such information - without allowing users to opt out from undesired tracking - can expose several privacy issues. For example, if the system tracks hints of anger during a VR-mediated collaboration and shows that explicitly on the avatar face, the collaboration could be negatively affected. Therefore, it is important to adopt strategies that allow for enriching avatars' behaviour, while filtering undesired emotions and informing the users about the emotions currently detected. 

In 2020, Miller et al.~\cite{miller2020personal} wrote that a considerable number of works on user authentication in VR assume a positive connotation. In other words, many papers explore the identification of users as a way for providing seamless and non-invasive authentication. However, the potential shown by VR data to provide authentication implies the potential to identify and de-anonymise users against their will. In their work, they showed that VR motion data can be used to identify users without requiring them to perform any specific action, contrary to what VR authentication studies have done before. The authors recruited 511 participants and showed them five 360-degrees videos (randomly picked from a pool of 80 videos), each 20s long. The participants did not have to perform any particular task and were free to look around as they preferred. The results of the study show that users can be identified on the basis of their motion data with an accuracy of 87,5\%, using a simple Random Forest algorithm. The authors also noted that the most important features included height, posture, distance from VR content, focal length, and placement of the hand controllers at rest. In conclusion, just collecting data of VR interactions shows high probabilities for identification, even if such interactions were not designed with identification intentions. Therefore, one of the future challenges for VR will be to design non-identifying interactions. For example, manipulating positional data (e.g., altering the height) could obfuscate some physical characteristics and help to protect users' privacy.

Finally, Maloney et al.~\cite{maloney2020} show that most social VR platforms (such as AltspaceVR, RecRoom, and VRchat) do not clearly inform users about which collected information is private and which public. After conducting a survey among VR users regarding their privacy-related behaviour on social VR platforms, the authors drew some interesting conclusions. First, users showed a similar tendency to share their experiences and emotions in VR, compared to traditional social networks. Second, social VR platforms aim to provide to their users an immersive experience by using resembling avatars. However, this leads to the leakage of some personal information, such as height, race, and appearance in general.



\subsection{Threats to Physical Privacy}
Buck and Bodenheimer~\cite{buck2021} point out that the way that users interact with the surrounding environment can be a critical information itself. The authors highlight that research has neglected the relationship between users and personal space in VR environments. In particular, personal space is plastic and varies depending on the interaction at hand: for example, personal space restricts when users approach objects they enjoy and expands when the interaction is not appreciated. This knowledge could potentially be used by companies to gather information about personal preferences, done by positioning an item in the environment and verifying whether users approach it or walk away from it. In turn, this could allow to identify and profile people without their knowledge nor consent. Moreover, the psychology literature shows that stress increases when interactions happen within personal space, hence personal space should be calculated and taken into account for designing pleasant virtual interactions. In conclusion, personal space is easy to detect and use to infer information about VR users, and this poses the nontrivial problem of how to collect and treat this type of data.

In general, any interaction in VR can lead to private data leakage. Falk et al.~\cite{falk2021} prove feasible a de-anonymization attack on VR which correlates VR avatars to their human users, leveraging human unique movement patterns. Named ReAvatar, this attack uses only the movement information provided by the VR setup, available without any peculiar privilege or malicious code injection. In their experiments, both the attacker and the victim join a VR game (i.e., VRChat), where people are embodied as avatars and can interact with other people's avatars. The goal of the attacker is to discover who is hiding behind a certain avatar by analysing the movement patterns of the observed victim. To do so, the attacker asks the victim to perform common and apparently innocuous movements, such as dancing or picking up objects. The attacker observes the victim's avatar and extracts the features of their movement. Then, the authors implement pose estimation using part affinity fields (PAF), detect keywise pairs of poses, and extract the local coordinates of \textit{(x,y,z)} from the victim avatar. Finally, each joint pair is evaluated against a threshold that determines if the avatar corresponds to any known person. In a limited study of 6 avatars and 5 users, ReAvatar achieved 89,60\% accuracy, proving that the attack is at least feasible. %In their discussion, the authors suggest that combining more advanced deep learning algorithms with more refined clustering algorithms would lead to better results, from the accuracy point of view.

%\todo[inline]{Maybe more privacy issues ideas in~\cite{valluripally2021modeling}}

Although beyond the scope of this review, it is worth noting that eye-tracking technologies are increasingly integrated into modern VR HMDs. Eye-tracking data have the potentiality of revealing more information than intended about the users. For example, Partala et al.~\cite{partala2000} showed that pupil size varies according to emotionally provocative sound stimuli. Their experiments suggested that the size of the pupil discriminates during and after different variations of emotional stimuli. This means that eye-tracking technologies embedded in VR systems might allow to extrapolate emotional information, without explicit knowledge or consent from the users.

\subsection{Solutions for Strengthening Privacy in VR}
Bozkir et al.~\cite{bozkir2019} proposed a method to assess the cognitive load of users during a driving simulation experience, while preserving their privacy. To do so, they used critical and non-critical time frames, trained multiple classifiers (SVM, DTs, RFs, and kNN with k=1,5,10) and validated their proposal with a leave-one-person-out cross-validation (LOOCV) approach. The authors used a small number of short time frames, which allows them to predict cognitive load in real time using minimal data, thus preserving users' privacy.
Their experimental hardware setup included a HTC-Vive, a Logitech G27 Steering Wheel and Pedals, Phillips headphones, and a Pupil-Labs HTC-Vive binocular add-on for eye tracking.

Regarding the use of eye tracking technology, John et al.~\cite{john2019,john2020} note that the trend of equipping VR headsets with eye tracking sensors puts users' privacy in danger. Eye-trackers collect images of the users' iris patterns and iris scans, often used as biometrics for a variety of services, can identify people with high accuracy. 
To counteract this issue, John et al.~\cite{john2019} showed that Gaussian filters can be applied to iris images to blur iris patterns, making them unrecognisable while preserving gaze detection capabilities. Later in 2020, John et al.~\cite{john2020} investigated a different approach, hardware-based. Their idea is to attach the eye tracker to a small telescopic arm, allowing users to manually defocus images of the iris at will. In addition to being effective, their approach has the valuable feature of allowing users to directly control when they want to show their iris (e.g., if needed for authentication purposes) or when they want to hide it.


\section{Threats to VR Security}\label{sec:security}
\begin{figure*}[t]
	\center
	\includegraphics[width=0.98\textwidth,clip]{images/security_issues.pdf}
	\caption{Security issues in VR. In purple the three main categories, further specified in subcategories and specific concerns.}
	\label{fig:security}
\end{figure*}

Adams et al.~\cite{adams2018} interviewed 20 participants, 10 users and 10 developers, investigating their perception of security in VR. Only 2 users raised concerns about malicious applications, 4 participants in total worried about data sniffing, and 3 users said that security "is not there yet", which suggests that not many end-users might have cybersecurity concerns. Given this attitude, the authors are of the opinion that many early VR applications will exhibit different security vulnerabilities, as cyber-security does not hold a high perceived value for end-users. Therefore, it is critical to assess which attacks could be expected and mounted on VR. In this section, we provide an overview of the different attacks that can affect VR, categorized according to the tree shown in \Cref{fig:security}. First, we discuss VR-specific attacks that are made possible by the very nature of VR and the involved sensors. Then, we cover the generic security issues that can affect VR, as they affect various information systems.






\subsection{VR-Specific Security Threats}
Casey et al.~\cite{casey2019} noted that VR, due to its immersive nature, can have a considerable impact on people; therefore, security is of paramount importance. In their paper, the authors formulate four VR-specific attacks, classified as Immersive attacks, as they leverage the unique immersive characteristics delivered by VR systems. These attacks are:
\begin{itemize}
    \item Chaperone attacks manipulate the Virtual Environment (VE) boundaries (i.e., tampers with the walls drawn in the VR scene) to make the collision avoidance fail and put users' safety at risk;
    \item Disorientation attacks aim to cause dizziness and confusion in VR users, causing a condition referred to as cybersickness. Dennison et al.~\cite{dennison2016} define cybersickness as a form of motion and simulation sickness due to physiological factors of the users, in correlation with their immersion and presence in VR environments;
    \item Human Joystick attacks aim to control the physical movement of a user, such that they move to a predefined spot without realizing it;
    \item Overlay attack, where an attacker inserts in the VE unwanted objects, such as images and videos. 
\end{itemize}
%\todo[inline]{Talk about the table!}

%Chaperone attacks manipulate the Virtual Environment (VE) boundaries (i.e., tampers with the walls drawn in the VR scene) to make the collision avoidance fail and put users' safety at risk. Disorientation attacks aim to cause dizziness and confusion in VR users, causing a condition often referred to as cybersickness. Dennison et al.~\cite{dennison2016} define cybersickness as a form of motion and simulation sickness due to physiological factors of the users, in correlation with their immersion and presence in VR environments. Human Joystick attacks aim to control the physical movement of a user, such that they move to a predefined spot without realizing it. The fourth is the Overlay attack, where an attacker inserts in the VE unwanted objects, such as images and videos. 

In addition to the four Immersive attacks, Casey et al.~\cite{casey2019} list two additional possible attacks. The first is the Camera Stream and Tracking Exfiltration attack, where the attacker gains access to the HMD live stream and the HMD front-facing camera stream. The second is the Man-In-The-Room attack, which consists of an attacker joining the target's VR environment while remaining invisible, allowing to extract information regarding the user or maliciously manipulate the environment. These six attacks have different impacts in terms of the Confidentiality, Integrity, Availability, and Safety (CIAS) quartet, as shown in \Cref{tab:specattacks}.

\begin{table*}[t]
\centering
\normalsize
\renewcommand\arraystretch{1.4}
\caption{VR-Specific Attacks and their Impact in terms of the CIAS Quartet}
\label{tab:specattacks}
\begin{tabularx}{1\textwidth}{p{13em}@{\hskip 4em}adad}
 & \cellcolor{white}\textbf{Confidentiality} & \textbf{Integrity} & \cellcolor{white}\textbf{Availability} & \textbf{Safety}\\ \toprule
 Chaperone Attack 
 &           & \xmark            &                  & \xmark  \\
Disorientation Attack                          &                 & \xmark         &              & \xmark      \\
Human Joystic Attack                           &                 & \xmark         & \xmark            & \xmark      \\
Overlay Attack                                 &                 & \xmark         & \xmark            &        \\
Camera Stream and Tracking Exfiltration Attack & \xmark               &           &              &        \\
Man-In-The-Room Attack                         & \xmark               & \xmark         &              & \xmark      \\
Side-channel Attack on Virtual Key Logging     & \xmark               &        &              &     \\
\bottomrule
\end{tabularx}
\end{table*}

The authors analyzed OpenVR, an SDK developed by Valve, to identify how an attacker could perform the attacks that they identified. Using an HTC Vive headset and an Oculus Rift, the authors proved that these attacks could be carried out. They found that Chaperone, Disorientation, and Human Joystick attacks could be struck by modifying a JSON configuration file. For the Chaperone attacks, the authors modified the virtual room setup stored in the JSON file. To strike a Disorientation attack, they modified two parameters that control players' orientation, stored in the same JSON file. The same parameters were manipulated to carry out a Human Joystick attack, even though the sequence and the rate of modifications are different. They also proved possible the Overlay attack, due to the fact that any OpenVR application is allowed to call an overlay and that no mechanism for force closing overlays existed. The authors suggested that this could be exploited to show undesired advertisements or even perform ransomware attacks that prevent users from using their VR system.
Last, the authors were able to extract information from the video buffer of the HTC Vive; the Oculus Rift was not used as it was not equipped with a front camera. In conclusion, every attack was successful and the reason was OpenVR lack of robustness: critical information was stored in plain-text, permissions were not defined or not restrictive enough, and applications were insufficiently isolated without sufficient integrity checks. Casey et al. suggest that Arya~\cite{lebeck2017,lebeck2018}, a policy enforcer created for AR applications, could be adapted to the VR domain.

Regarding Overlay attacks, Ahn et al.~\cite{ahn2018} noted that most of the research conducted for AR and VR security has focused more on input privacy (i.e., minimising the sensor data available to third parties) than visual output security. A notable exception is the aforementioned Arya~\cite{lebeck2017,lebeck2018}, proposed by Lebeck et al. The authors showcase that defining rule-based policies for every conceivable AR/VR scenario is an intractable task, using a notable example that assume an application required to create various virtual objects, which should not obstruct the users' view nor overlap with other important objects (e.g., a virtual avatar). In other words, in this application a new dynamic object should be positioned in the environment in such a way that it is visible but not obstructive. To overcome the complexity of managing this task, the authors proposed Arya, which uses Fog computing and Reinforcement Learning to automatically synthesizes security policies. As suggested by Casey et al.~\cite{casey2019}, their approach can be used to prevent attackers from manipulating a VR application, either to distract and disrupt users or to prevent expected interactions by obstructing important environmental objects. An example of an attack would be a ransomware that obstructs the user's view with an object until the victim pays a ransom.

Last, VR platform might be vulnerable to various specific side-channel attacks. For example, Al Arafat et al.~\cite{arafat2021} investigated keystrokes on virtual keyboards in terms of security. The authors proposed VR-Spy, a side-channel attack that allows to extract gesture patterns from channel state information (CSI) wave-forms of Wi-Fi signals and then, by applying machine learning, recognize keystrokes from such patterns. The experimental setup consists of a person equipped with a commercially available headset, positioned between 2 commercially off-the-shelf (COTS) wireless devices. Their experiments show that VR-Spy achieves an accuracy of 69,75\% when performing virtual keystroke recognition. Referring to the CIAS quartet, this specific side-channel attack (listed in \Cref{tab:specattacks}) breaches communication confidentiality.



% \begin{table*}[t]
% \centering
% \normalsize
% \renewcommand\arraystretch{1.4}
% \caption{Generic Attacks and their Impact on VR in terms of the CIAS Quartet}
% \label{tab:genattacks}
% \begin{tabularx}{1\textwidth}{p{13em}@{\hskip 4em}CCCC}
%  & Confidentiality & Integrity & Availability & Safety\\ \toprule
% Shoulder-surfing Attack                        & \xmark               &           &              &       \\

% \bottomrule
% \end{tabularx}
% \end{table*}

\subsection{Generic Security Threats that Affect VR}
Beyond security threats that affect specifically VR due to its characteristics, VR systems can also be vulnerable to generic security threats
Valluripally et al.~\cite{valluripally2021rule,valluripally2020,valluripally2021modeling} focused on modeling and controlling generic attacks that could cause cybersickness in VR users. In their first paper~\cite{valluripally2021rule}, they introduce a rule-based framework that controls cybersickness by means of 3Qs adaptation. In other words, they propose a system that regulates three aspects:
\begin{itemize}
    \item Quality of Application (QoA), a measure of application performance;
    \item Quality of Service (QoS), a measure of network resources such as bandwidth and jitter;
    \item Quality of Experience (QoE), a measure of user perceived satisfaction.
\end{itemize}

The framework is useful for reducing sickness, both if induced by incidental performance issues (e.g., temporary network bottlenecks), as well as if caused by malicious attacks, such as DoS. Through continuous monitoring of anomaly events, the framework classifies events into predefined categories and selects the appropriate adaptation strategy. For example, assuming that the system is deployed on Amazon Web Services (AWS), if the framework detects an abnormal high CPU utilisation that affects QoA, an adaptation strategy would be to upgrade the server instance to a better instance (e.g., from AWS t2.micro to AWS c4.large). Their experiments show that adaptations for QoA anomalies can reduce cybersickness by up to 26.43\%, while, in the case of QoS anomalies, the proposed adaptation reduces cybersickness by 30.28\%. The framework also has the ability to take into account the associated hourly costs for each adaptation strategy, so that the best cost-effective adaptation can be adopted.

In their subsequent work, Valluripally et al.~\cite{valluripally2020,valluripally2021modeling} expand on their analysis of sickness-inducing attacks. First, the authors conducted an experiment where they simulate the effects of on-going security attacks and/or privacy attacks while users perform some tasks in VR. The experiment was performed by artificially manipulating the traffic rate and it shows that these attacks cause considerable disruption to different factors that contribute to cybersickness, such as nausea and discomfort. 
The authors then proved that cybersickness can be correlated with security and privacy issues, regardless if due to attacks, faults, or a combination of both. In particular, they proposed a framework based on an attack-fault tree (AFT) formalism for producing threat models that describe the possible security and privacy attacks. The resulting AFTs are used to perform a quantitative assessment of the probability of disruption caused by each threat. Evidence shows that the threats that induce higher levels of cybersickness in VR-based Learning Environments (VRLEs) are DoS attacks, data leakage, man-in-the-room attacks, and generic unauthorised accesses.
Based on the quantitative results, the authors also experimented the effect on cybersickness when different design principles are applied, such as hardening and redundancy. For example, applying hardening and the principle of least privilege reduces the probability of cybersickness by 28,96\%, whereas applying redundancy and the principle of least privilege achieves only a small reduction of 3,05\%. When all prescribed principles are applied together, the probability of experiencing cybersickness is reduced by 35,18\% in total.

Gulhane et al.~\cite{gulhane2019} focused on a sub-area of VR, namely, VRLEs. VRLEs are VR spaces designed for delivering virtual learning experiences to their users. For doing so, VRLEs often combine data from emotion tracking sensors with other sensitive information, such as users' learning progresses and personal data. Therefore, VRLEs pose a number of challenges with respect to the security, privacy, and safety of their users. Gulhane et al. proposed a risk assessment framework that uses attack trees to formalise internal and external vulnerabilities for VRLEs, such as DoS, SQL injections, and unauthorised logins. %In particular, attack trees are a structured way for depicting which events would lead to different attacks; 
For example, they use attack trees to formalize the observation that delayed packets (e.g., due to DoS attacks) would lead to compromise the availability of the server, hence its security. The output is a risk score for each analysed property: security, privacy, and safety. Using vSocial~\cite{nuguri2021} (a cloud-based VRLE designed for young people with learning disabilities) as a testbed, the authors evaluated the attack trees produced by their methodology. Each leaf of every attack tree represents a threat and, for each leaf, the authors simulated the corresponding attack. Then, they assigned an impact score for every threat and used the scores for computing complete attack trees.

Finally, it should be noted that VR equipment could facilitate security threats, such as shoulder-surfing attacks where a malicious party spies on the user's actions to steal their password. Chen et al.~\cite{chen2018} described a computer vision-based attack, struck using VR augmented with additional sensors. In their experiments, the authors showed that it is possible to use a HMD, equipped with an additional ZED stereo camera, for recording the surroundings and stealing the passwords input by users on their devices. The paper shows that it is possible to reconstruct a 3D video from the two two side-by-side videos recorded by the ZED camera. Then, the 3D video can be analysed and used for extracting the 3D trajectories of the fingertip movements to recover the passwords tapped by the victim. The paper shows that there is the potential risk of malicious users that pretend playing some videogames on VR while recording unconscious people moving in the surrounding environment. In particular, the experiments show that there is a strong correlation between the success rate of the attack and the distance from the victim, as well as with the number of input attempts. In addition, some authentication systems require to confirm the input password and such requirement increases the success rate of the proposed attack. 

%\todo[color=red]{Maybe this paper should go.}
%Zhang et al.~\cite{zhang2021} focus on the security concerns in virtual reality intelligent interaction systems (VRIISes), which results as a combination of different paradigms: Intelligent Manufacturing Systems (IMSes), VR, and Digital Twins (DTs). On the one hand, VRIISes allow manufactures to produce remotely their products, monitor the entire production chain, and interact seamlessly with products both in the virtual world and in the real world. On the other hand, the authors note that VRIISes are complex combinations of heterogeneous and decentralized subsystems, which produce large amounts of structured, semi-structure, and unstructured information. This makes it difficult to manage VRIISes from a security point of view. Therefore, the authors propose that each intelligent unit in a VRIIS exhibits: an instruction execution module for computational tasks; an information interaction module for communication with the environment, other units, and human beings; a security control module for performing authority checks, authentication routines, encryption and decryption, and so on.

\section{Authentication in VR}\label{sec:authentication}
Among the areas of research within VR security, the topic of authentication deserves a section of its own. In fact, most of the work related to VR security focuses on identifying or authenticating its users. In this section, we provide an in-depth exploration of the literature and we summarise the works in \Cref{tab:auth}.

\begin{table*}[t]
\centering
\normalsize
\renewcommand\arraystretch{1.4}
\caption{Works on Authentication for VR}
\label{tab:auth}
\begin{tabularx}{0.95\textwidth}{L{0.5}A}
\textbf{Authentication Approach} & \cellcolor{white}\textbf{Works in the Literature} \\ \toprule
Traditional Authentication & George et al.~\cite{george2017seamless}, Yu et al.~\cite{yu2016}, Olade et al.~\cite{olade2020swipe}, Funk et al.~\cite{funk2019}, Khamis et al.~\cite{khamis2018}\\
Authentication via VR interactions  & Mathis et al.~\cite{mathis2020,mathis2021}, George et al.~\cite{george2019}, Funk et al.~\cite{funk2019}, \\
Head gaze and movements & Mathis et al.~\cite{mathis2020,mathis2021}, Funk et al.~\cite{funk2019}, Rogers et al.~\cite{rogers2015}, Sivasamy et al.~\cite{sivasamy2020}, Mustafa et al.~\cite{mustafa2018} \\
Eye-Tracking & Olade et al.~\cite{olade2020swipe}, Mathis et al.~\cite{mathis2020,mathis2021}, Liebers et al.~\cite{liebers2021gaze}, Luo et al.~\cite{luo2020}, Khamis et al.~\cite{khamis2018}, Lohr et al.~\cite{lohr2018,lohr2020}, Zhu et al.~\cite{zhu2020}\\
Bio-mechanics & Mustafa et al.~\cite{mustafa2018}, Pfeuffer et al.~\cite{pfeuffer2019}, Kupin et al.~\cite{kupin2019}, Ajit et al.~\cite{ajit2019}, Liebers et al.~\cite{liebers2021}, Miller et al.~\cite{miller2020}, Miller et al.~\cite{miller2021}, Olade et al.~\cite{olade2020biomove}, Shen et al.~\cite{shen2018} \\
\bottomrule
\end{tabularx}
\end{table*}


In their seminal work, George et al.~\cite{george2017seamless} proposed a first step toward integrating traditional authentication approaches into VR, in particular PINs and swipe patterns. They conducted a study with a total of 30 people and showed that both techniques are suitable for transposition into VR. In particular, they showed that the usability in VR matches the usability in the physical world. Furthermore, they showed that the private visual channel granted by VR makes such authentication processes harder to observe and steal.

Similarly, Yu et al.~\cite{yu2016} investigated the implementation of two traditional authentication methods, pattern swiping and PINs, while also making a first attempt to create a 3D password for VR. Although the authors did not introduce sufficient details about the 3D password, they provided some intuition about the relative resilience of the three methods against shoulder-surfing attacks. In their experiments, the participants were recorded while using the three authentication methods and then asked to guess the passwords of other participants, watching the recordings. The results showed that 3D passwords were considerably harder to guess than 2D swipes and PINs, most probably due to the additional complexity added by introducing a third dimension.

Olade et al.~\cite{olade2020swipe} further investigated the portability of swipe patterns from mobile devices to VR, noting that George et al.~\cite{george2017seamless} did not provide a direct performance comparison between mobile devices and VR. %Furthermore, they noted that the related work did not investigate in depth the impact of potential shoulder-surfing attacks.
In their experimental setup, the authors implemented the swiping authentication system in terms of 4 different interaction methods: using the hand-held controller (HHC), the HMD, the LeapMotion (a hand-tracking device), and the aGlass (for eye tracking). While the mobile swiping authentication was the easiest and fastest to use, the HHC and the LeapMotion versions were comparable both in terms of speed and usability. Regarding shoulder-surfing attacks, given the same amount of information, attackers have a considerable lower success rate when swiping is done in VR than when it is done on a mobile phone screen.

Mathis et al.~\cite{mathis2020,mathis2021} proposed RubikAuth, a three-dimensional authentication method, consisting in a five-faced cube (one face is omitted for improved reachability) that exhibits 1 colour and 9 digits per face. For authenticating themselves, the user rotates the cube with the left HHC and selects n digits as their chosen password. The entropy of a RubikAuth password is equal to $45^n$, considerably higher than the $10^n$ entropy granted by traditional numerical pins. The authors evaluated the impact on performance of different input techniques, concluding that controller tapping is faster than head pose and eye gaze, while the study participants did not express any clear preference regarding the usability of each input approach. %As expected, the complexity of the password directly affects memorability, and the higher the number of face switches, the lower the memorability of the passwords. 

The authors also evaluated the security of their proposal, using three realistic attackers with different tools: pen and paper to annotate observations, a 3D replica of the cube, and recordings of users performing authentication. The experiments show that having access to such resources slightly increases the chances of guessing a RubikAuth password (and, reasonably, the same applies to other VR authentication modes): 8 attacks out of 540 attempts succeeded, even though the passwords were simple patterns involving only a single cube face. %It is worth noting that RubikAuth offers some interesting security properties, which contribute to its resilience against shoulder-surfing attacks. For example, 
RubikAuth exhibits strong guarantees against shoulder-surfing attacks, as it is possible to apply fake face switches to deceive bystanders. It is also possible to strategically angle the cube and pick numbers from different faces without applying any rotation.

Funk et al.~\cite{funk2019} proposed LookUnlock, a head-gaze-based solution to enter passwords on HMDs. LookUnlock can use different kind of spatially-aware passwords, namely, spatial passwords, virtual passwords, and hybrid passwords. To set a spatial password, users move the HMD cursor to different positions in the environment and perform enter actions. For the unlocking phase, users insert their password by placing the head-gaze cursor over the spatial targets previously defined at setting phase. After the first object, the users have up to 3000 ms for detecting the next object, before getting a time-out. LookUnlock adds an additional layer of security to the authentication phase by binding spatial passwords to the environment. In practical terms, the same actions performed in a different environment would not authenticate the user, as the framework would identify that the physical environment differs from the one used at enrolment time. 
%When interviewed, the users that acted as the second type of attacker mentioned that it was ``impossible for them to imagine themselves in the correct position in the virtual scene''.


\subsection{Biomechanics-based Authentication}
As pointed out by Miller et al.~\cite{miller2021}, traditional authentication methods, such as PINs and passwords, not only can be stolen by malicious attackers, but they also allow a legitimate user to hand-over control to a third party. This can be particularly problematic for VR applications that must ensure that the user performing the tasks is the same user who authenticated themselves in the first place. For example, a VR scene used to undertake university exams should prevent a student from cheating by logging in and handing over the control to a fellow student. One solution for countering handover threats is to implement implicit and continuous authentication via biometrics, which allow for detecting automatically when the user's characteristics differ from the expected ones. 
Another downside of explicit authentication, as observed by Liebers et al.~\cite{liebers2021gaze}, is that it negatively affects user immersion in VR. Biometrics-based authentication techniques also have an advantage over explicit authentication methods in this regard, as they are implicit and provide a smoother experience for the users. In this section, we describe the state of the art for what concerns the authentication of VR users using biomechanical biometrics.  Jain et al.~\cite{jain2004} noted that body movement can be used for authentication purposes when it is universal, distinctive, repeatable, and collectible. Based on such observations, several works in the literature explored how VR sensors can be used to capture movement patterns. In this section, we explore the state of the art; in \Cref{tab:biomech} we summarise the features used by each analysed work and the accuracy results obtained by the authors.

\begin{table*}[t]
\centering
\normalsize
\renewcommand\arraystretch{1.5}
\caption{Biomechanics-based Authentication in VR}
\label{tab:biomech}
\begin{tabularx}{0.9\textwidth}{p{8em}@{\hskip 2em}adadadadada}
 &
  \rot{Participants} &
  \rot{Accuracy (\%)} &
  \rot{Equal Error Rate (\%)} &
  \rot{Head Position} &
  \rot{Head Orientation} &
  \rot{Controllers Position} &
  \rot{Controllers Orientation} &
  \rot{Distance Between Devices} &
  \rot{Body Normalisation} &
  \rot{Velocity/Acceleration} &
  \rot{Gait Estimation} \\ \toprule
Li et al.~\cite{li2016} & 30 &   & 4,43 & \cmark & \cmark  &  &   &  &   &   &    \\
Sivasamy et al.~\cite{sivasamy2020} & 40~/~48 & 99  &  & \cmark & \cmark  &  &   &  &   &   &    \\
Mustafa et al.~\cite{mustafa2018}  & 23 &  & 7  & \cmark & \cmark  &  &   &  &   & \cmark  &   \\
Pfeuffer et al.~\cite{pfeuffer2019}  & 22 & 40  &  & \cmark &   & \cmark &   & \cmark &   &   &    \\
Kupin et al.~\cite{kupin2019}        & 95 & 92,86 &  & &   & \cmark &   &   &   &   &    \\
Ajit et al.~\cite{ajit2019}          & 33 & 93,03 & & \cmark & \cmark & \cmark & \cmark &   &   &   &    \\
Liebers et al.~\cite{liebers2021}    & 16 &  100 & & \cmark &   & \cmark &   &   & \cmark &   &    \\
Miller et al.~\cite{miller2020}      & 46 &   85 & & \cmark & \cmark & \cmark & \cmark &   &   & \cmark &     \\
Miller et al.~\cite{miller2021}      & 46 & 98,53 & & \cmark & \cmark & \cmark & \cmark &   &   & \cmark &     \\
Shen et al.~\cite{shen2018}          & 20 & 95  &  &   &   &   &   &   &   &   &   \cmark   \\ \bottomrule
\end{tabularx}
\end{table*}


Li et al.~\cite{li2016} noted that adults exposed to fast beat audio tracks exhibit unique patterns of head movement. Their observation resulted in Headbanger, a framework that captures unique head movements that arise as a natural response to fast-tempo audio stimuli, and authenticates the users accordingly. Implemented using a Google Glass device, Headbanger shows an EER of 4,43\% when users are exposed to a 10-second music cue. When tested against imitation attacks, less than 3\% of attempted attacks achieved a false positive.

With VRCAuth, Sivasamy et al.~\cite{sivasamy2020} also investigated the use of head movements to provide continuous authentication in VR settings. VRCAuth performs a binary classification to determine whether the current user is authorised or not. The authors evaluated five different binary classifiers on two different data sets, the \textit{Kaggle VR driving simulator}~\cite{jafarnejad2017} and the \textit{user behaviours in spherical video streaming}~\cite{wu2017}. Both datasets provide three-dimensional information; additionally, the latter provides quaternion information. The results of the experiments show that the PART and LMT classifiers achieved an accuracy of more than 99\% for the first dataset. On the second dataset, every classifier achieved an accuracy of more than 99\%, suggesting that quaternion data can significantly simplify binary classification tasks.

Similarly to Miller et al.~\cite{miller2020personal}, Mustafa et al.~\cite{mustafa2018} found that body movement patterns (freely interacting within a VR environment) exhibit enough information to identify users. In their work, the authors argued that data of head movements can be used for authentication purposes. To prove their claim, they designed an experiment with 23 subjects that interacted with a VR application developed by the authors. The VR application was designed specifically for evaluating head and body biometrics. In the experiment, the users start from a point A and must move towards a random point B where a ball appears, then to a third random point C, and so on, until the user has reached 25 balls. To move towards a ball, users must move a VR pointer towards the target ball by nudging the VR headset in the desired direction. Each subject was evaluated in two sessions that took place, at least one day apart from each other, and the experiments showed an EER of around 7\%, in the best case. Although these results would not suffice to deploy a real-life application, head and body movements hold promise for authentication purposes.

Beyond the raw data of its sensors, VR equipment can provide more interesting information for authentication purposes. In 2019, Pfeuffer et al.~\cite{pfeuffer2019} investigated the use of body motion and behavioural biometrics for identifying users in a VR setting. They conducted their experiments using an HTC Vive (composed of a HMD, two controllers, and an optical tracking), enriched with an additional eye tracker from Pupil Labs. First, they identified four basic tasks that can be found in most VR applications: pointing, grasping, walking, and typing. Then, they designed specific tasks that revolve around these four activities and invited 22 participants to perform them. Based on the collected data, the authors evaluated different motions and distance relations (e.g., the distance between the headset and the joysticks). However, their best result, obtained using a combination of head motion and the distance between every device, achieved only an average of 40\% accuracy. Therefore, further investigations were warranted.

Other works investigated the combination of data from controllers and HMDs for achieving authentication on VR. Several of these works used a virtual ball-throwing task as a case study. In the first instance, Kupin et al.~\cite{kupin2019} used as a feature the position of the dominant hand holding a VR controller, following the intuition that users exhibit identifiable trajectories due to their unique biomechanics. In their experiment, the authors instructed users to pick up a virtual ball and throw it at a target and they compared the trajectory of the dominating hand of each user against a dataset containing the movements of the other users. Using bounding box centering and a nearest-neighbour (NN) matching algorithm on 95 different 3D trajectories, the authentication accuracy reached 92,86\%, proving that trajectories exhibit enough peculiarities to identify a user. %\todo[inline]{However, in their study the authors used only 14 users; it is unclear whether this would work with thousands of users.}

Ajit et al.~\cite{ajit2019} improved the approach proposed by Kupin et al.~\cite{kupin2019}, extending the data analysis to the recessive hand and the head and using the device orientation in addition to the position. In an experiment involving 33 users, the authors improved the results of Kupin et al., obtaining an authentication accuracy of 93,03\%. Their approach is based on a perceptron classifier which learns weights on the matches between position and orientation features on the hand controllers' trajectories, as well as the headset trajectory. In practical terms, trajectories belonging to the same user result in a low classifier score and allow the user to authenticate.

Liebers et al.~\cite{liebers2021} further investigated the effect of body normalisation on user identification in VR. In particular, they analysed the impact of body height normalisation, arm length normalisation, and the combination of both against scenarios where no normalisation is applied.
They evaluated a small pool of 16 participants during the execution of two tasks that can be correlated to actions executed in a typical VR game: a bowling throw task and an arrow shooting task. The research highlights that, while physiological factors play a clear role in behavioural biometrics, they also increase the noise injected into deep learning algorithms. Removing or reducing the features showed a positive impact on the identification accuracy of the deep learning models implemented by the authors, i.e. RNN and MLP. Not only this work suggests that it is possible to achieve user identification without processing some sensitive information, but also that it is feasible to perform continuous authentication during regular gaming sessions.


Miller et al.~\cite{miller2020} noted that authentication approaches based on behavioural biometrics assume that users enrol in the same VR system they will later use. However, in the future users might interact with many different VR headsets in their daily tasks. This exposes a problem, because different VRs exhibit system-specific biases, meaning that every time that users want to use a new headset, they have to engage in new enrolment phases. Therefore, Miller et al. investigated the differences between 3 different VR systems (Oculus Quest, HTC Vive, and HTC Vive Cosmos), collecting the biometric data of 46 users performing a simple task on each system. They instructed the participants to perform a simple (virtual) ball-throwing task on each headset, repeated 10 times, for several consecutive days. Expanding on what has previously been done by Ajit et al.~\cite{ajit2019}, they introduced trigger control information from the dominant controller, as well as linear and angular velocity to account for the differences in speed between users. The results in terms of within-system and cross-system accuracy showed that some features play a prominent role for authentication purposes. In particular, head orientation, left controller position, right controller orientation, and right controller position proved to be the most important features for authentication purposes. Their results confirm the intuition of Ajit et al.~\cite{ajit2019}, about the relative importance of different features when performing authentication.

Improving their previous work, Miller et al.~\cite{miller2021} proposed using siamese neural networks to learn the baseline distance function across HMDs. Knowing this information, when a user wants to use a new VR headset, it suffices to apply the distance function to offset the inherent differences with the original headset used for enrolment. This prevents users from having to go through an additional enrolment phase every time they use new VR equipment. In this paper, they used the same experimental setting they used in their previous study~\cite{miller2020}. Regarding user authentication, their results show that the siamese network achieves at its best a 1,39\% EER, when the Oculus Quest trajectories are used at enrolment time and the HTC Vive at use time. For user identification, their highest accuracy is 98,53\%, with the same setup observed for the EER authentication result.


Shen et al.~\cite{shen2018} note that VR systems often store users' private data, such as social network accounts and credit card information. Although convenient, this method of accessing information exposes significant security and privacy issues. For amending this issue, the authors proposed an authentication system based on the users' gait signatures, named GaitLock, that implements a new gait recognition model, Dynamic-SRC. Dynamic-SRC fuses data information from both the HMD accelerometer and the HMD gyroscope to address the problem of noisy signals coming from head motions. The validation experiments consisted of different walking tasks; among these tasks, the most interesting is the uncontrolled outdoor walking task. The results show that Dynamic-SRC outperforms by 20\% the gait recognition accuracy of the best alternative algorithm (SRC with Sparse Fusion). Furthermore, in the uncontrolled walking task, GaitLock achieves an accuracy of 95\% using only a small dataset of 5 steps. Although the authors used only an AR device that could provide enough computational power (i.e., Google Glasses), they noted that with enough computational power, their system can be transposed to VR. 


\subsection{Authentication with eye-tracking sensors}
Although early VR headsets were not equipped by default with eye trackers, these sensors are becoming more and more common in commercial headsets. One of the main benefits provided by VR HMDs to eye tracking is that HMDs fully cover users' eyes, allowing for controlling lighting conditions and reducing problematic reflexions.

Authentication using eye tracking sensors has been studied long before VR technology took hold. For example, in 1978 Hill~\cite{hill1978} patented an apparatus for recognising individuals by the retinal vasculature in their eyes and, in 2004, Maeder et al.~\cite{maeder2004} investigated the use of eye gaze to authenticate users. 
More recently, in 2016 Sluganovic et al.~\cite{sluganovic2016} observed that gaze-based authentication systems suffer from high error rates or require long authentication times. Based on this premise, they proposed using reflexive saccades, rapid eye movements that reorient the eye to the next focus object. %When the gaze stares at slow moving stimuli, the eye produces a smooth pursuit movement. However, when the speed exceeds a certain threshold, the human eye cannot follow stimuli with smooth pursuits anymore and it must switch to the saccadic movement. The experiments of the authors show that saccades exhibit differences between individuals, enough to be used as biometric information. 
Other works, such as Rigas et al.~\cite{rigas2016} and Holland and Komogortsev~\cite{corey2013}, show the potential of using saccades for identification purposes. In this section, we focus on works that discuss the use of eye-tracking sensors in VR, either alone or alongside other sensors. 

For the scope of this paper, we neglect the research conducted in the vast field of eye-based authentication and recognition, not specifically targeted to VR applications and implementation. In \Cref{tab:eyet}, we summarise the papers discussed in this section and we highlight which features have been used by the authors for performing authentication.

\begin{table*}[t]
\centering
\normalsize
\renewcommand\arraystretch{1.5}
\caption{Authentication with eye-tracking sensors}
\label{tab:eyet}
\begin{tabularx}{0.98\textwidth}{p{8.5em}@{\hskip 2em}adadadadadad}
 &
  \rot{Participants} &
  \rot{Accuracy (\%)} &
  \rot{Equal Error Rate (\%)} &
  \rot{Head Position} &
  \rot{Head Orientation} &
  \rot{Controllers Position} &
  \rot{Controllers Orientation} &
  \rot{Gaze} &
  \rot{Saccades} &
  \rot{Pupil Size} &
  \rot{EOG Signals} &
  \rot{Blinking} \\ \toprule
Rogers et al.~\cite{rogers2015} & 20 & 94 &   & \cmark & \cmark  &  &   &   & &  &  & \cmark \\
Liebers et al.~\cite{liebers2021gaze} & 11 & 100 & & \cmark & \cmark &  &  &     & \cmark &  & & \\
Luo et al.~\cite{luo2020} & 70  &  & 4,97 & &  &  &  &     & \cmark &  & \cmark & \cmark \\
Khamis et al.~\cite{khamis2018} & 26  & 82 &  & &  &  &  & \cmark &  &    & & \\
Lohr et al.~\cite{lohr2018,lohr2020} & 458 &  & 9,98 &  &  &  &  &   \cmark & \cmark & &  &\\
Olade et al.~\cite{olade2020biomove} & 15 & 98,6 & & \cmark & \cmark & \cmark & \cmark &   \cmark & & &  &\\
Zhu et al.~\cite{zhu2020} & 52 &  & 4 &  &  &  &  &  &  & \cmark &  &   \cmark\\
\bottomrule
\end{tabularx}
\end{table*}


Rogers et al.~\cite{rogers2015} showed the possibility of identifying users using their unconscious head movement and blinking, while performing a simple passive task. The authors set up an experiment in which they showed 20 participants a rapid series of numbers and letters. Using the collected data, they achieved an identification accuracy of 94\%, showing the suitability of unconscious movements for unobtrusive user authentication. 

Liebers et al.~\cite{liebers2021gaze} proposed an implicit identification method based on users' saccadic movements and head orientation. Similarly to when eyes switch from slow pursuit to saccadic movement, as previously mentioned, when the stimulus speed exceeds the saccadic speed threshold, human beings must recruit head movement to keep track of the stimulus. The authors show that the characteristics of fixations and saccades, together with head orientation, are complex and unique enough to be used as biometric features. In particular, with a small pool of 12 participants, using a kNN algorithm on every feature they envisioned, their approach achieves an accuracy of 82\%, whereas using deep learning the accuracy reaches the 100\% mark.

Luo et al.~\cite{luo2020} noted that authentication methods based on head and body movements, such as those mentioned above, expose the authentication actions to bystanders. They also noted that eye-gazing alone can exhibit a high EER, too high for trusting it as an authentication method. Therefore, with OcuLock the authors propose to involve the entire human visual system (HVS), as it is composed of different entities, such as eyelids and extraocular muscles, which exhibit features that could be used for authenticating purposes. Tested with 70 participants, OcuLock achieves an EER of 3,55\%  against impersonation attacks and 4,97\% against statistical attacks. However, it should be noted that standard HMS headsets equipped with eye-tracking cameras cannot measure such HVS signals; for example, they cannot be used to implement a framework based on electrooculography (EOG) such as OcuLock. In fact, to capture EOG signals, the authors had to enrich with tiny electrodes the foam face cover of a Lenovo Mirage Solo VR headset.

With VRPursuits, Khamis et al.~\cite{khamis2018} showed that VR equipped with eye tracking is suitable for implementing smooth pursuits that address the Midas touch problem (i.e., the problem of distinguishing deliberate gaze from basic eye perception functions). Although their work does not address user authentication nor security properties, their implementation of a virtual ATM shows the potential feasibility of an authentication method based on smooth pursuits detection. In their proof of concept, participants were asked to enter a 4-digit PIN by intentionally looking at the desired numbers. The numbers, from 0 to 9, were visualised as two groups of cubes rotating clockwise and counterclockwise, respectively. 

Lohr et al.~\cite{lohr2018,lohr2020} explicitly address the goal of using eye movement biometrics to authenticate VR users. During the enrolment phase, the prototype extracts four fixation features and eight saccades features, for a total of twelve eye movement features. To perform authentication, the authors propose to compare the features of two different templates (the user currently examined and a stored template), create a vector of match scores, and finally combine the scores into a single fused score using a weighted mean fusion with equal weights.

Olade et al.~\cite{olade2020biomove} proposed BioMove, a method for recognising biomechanics behaviour patterns and using them to authenticate users. In particular, the authors studied the movements of the head, hands, and eyes of 15 users while performing controlled tasks, such as grab, rotate, and drop. The authors investigated the use of BioMove for the purpose of both user identification and authorization. In the best case scenario, for identification tasks BioMove achieved a classification accuracy 98,6\%. Regarding authentication, the authors' experiments show a 0,0032\% FPR and a 1,3\% FNR, highlighting that BioMove (and behavioural biometrics, in general) could be used as an efficient second-factor authentication.

Zhu et al.~\cite{zhu2020} noted that, by design, VR systems prevent their users from seeing what happens around them in the real world. This means that users have a hard time concealing their actions from bystanders, which could lead them to potentially leak sensitive information. BlinKey, an authentication scheme designed for VR headsets equipped with eye-tracking technology, aims to mitigate the problem by using only users' eyes, naturally covered when using a VR HMD. The core idea of BlinKey is to encode a password as a set of blinks, performed with a rhythm only known to the user (e.g., following a tune that they like). As an additional authentication layer, BlinKey analyses the pupil size variation between blinks, a biological marker that uniquely characterises every person. BlinKey was implemented and tested using a commercially available HTC Vive Pro equipped with a Pupil Labs eye tracker. To evaluate the security resilience of their proposals, the authors tested four different types of attack: zero-effort, statistical, shoulder surfing, and credential-aware. For the first attack, attacking participants tried to guess the target blinking pattern without any information and were incapable of compromising passwords composed of 7 or more blinks. With statistical attacks, attackers had access to a dataset of blinks of their victim, which they could statistically analyse. This leads to a slight advantage for the attacker, with respect to the zero-effort attack, but passwords composed of at least 8 blinks seem to prevent any compromise attempt. Shoulder-surfing attacks were more successful, with an attack success rate of 4.9\%. Last, credential-aware attacks assume that the attacker knows the password and tries to replicate it with their own blinks (i.e., exhibiting their own pupillary biometrics). Although the attack proved to be more successful than the previous ones, when the password consisted of at least 10 blinks the attack success rate was 4,4\%, showing that pupil variation alone cannot be used as an authentication factor.

%In conclusion, shoulder-surfing attacks on BlinKey are considerably harder than guessing PINs or observing swiping patterns, as it is challenging for the attacker to guess blinks when the target's eyes are covered by the VR headset itself.


\subsection{Other works in VR Authentication}
Most papers in the field of VR authentication, at the time of writing, either leverage something users know, such as a password, or some personal characteristics, such as their behavioural biometrics. Combining these different approaches for achieving multi-layered authentication has the potential to produce stronger authentication schemes across the board. 

For example, with RubikBiom, Mathis et al.~\cite{mathis2020biometrics} investigated the use of knowledge-driven behavioural authentication for authentication in the VR domain. The authors developed a proof of concept that uses two different authentication features. A password only known to the user and the movement patterns that arise from the act of inputting such a password in VR. As for previous proposals for behavioural biometrics, the intuition is that every user exhibits peculiar movement patterns due to their specific biomechanics. In particular, an attacker that attempts to infiltrate a victim's account by shoulder-surfing, not only should steal the password, but should also be able to replicate the same movement patterns when inserting the password. In RubikBiom, the password is a 4-digit PIN selected on a 5-faced cube. Each face has a different colour and exhibits 9 digits (1-9), similarly to the setup used by the same authors in their works previously discussed~\cite{mathis2020,mathis2021}. The best accuracy score was 98,91\%, which demonstrates that multilayered approaches in VR hold promise. The result was obtained with a fully convolutional network (FCN) over a multivariate dataset (composed of dominant hand position and rotation), together with non-dominant hand position and rotation.

In a preliminary study, Wang and Gao~\cite{wang2021} show that multi-attribute authentication methods could also be effective in countering Man-in-the-Room (MITR) attacks. The basic intuition behind their authentication scheme is that objects in the real world are defined by a rich set of features (e.g., colour and shape). In their proof-of-concept, the user chooses a number of secret features at enrolment time and, at authentication time, the framework creates random objects which might contain none, some, or every feature. To log into the system, the user must stare at the object that exhibits all the desired features. This approach is particularly interesting because it allows to effectively decouple the input process from the password, increasing the resilience to MITR attacks.

Von Willich et al.~\cite{vonwillich2019} discussed the danger that passersby create to VR users, unaware of their surroundings. HMDs impede visual perception of the surrounding environment, allowing bystanders to physically collide with VR users or observe their actions to infer their authentication credentials. As a mitigation, the authors propose to detect passersby using environmental sensors and depict them in the VR environment, so that the VR user is aware of them, effectively implementing augmented virtuality (AV), a paradigm that involves enriching the virtual world with information from the real world. This depiction can take different forms, such as avatars, 2D pictures, or 3D renderings. While AV was originally meant for avoiding unwanted collisions with dynamic agents in the real world, the authors argue that it can be used for preventing some by-standing attacks. Although AV cannot impede attackers from observing from a distance, it could be used to detect attackers that get close enough to be picked by the sensors and prevent them from having a proper close-up view of VR users' motion, thus reducing their chances of inferring useful information.



\section{VR for teaching, training, and evaluating Security}\label{sec:teach_train_ev}
VR environments have been proposed as a mean for improving the cyber and physical security of sensitive locations, as well as for training employees on routines and policies. In addition, VR has been proposed to assess the usability of authentication processes. In this section, we provide a brief overview of these different applications.

\subsection{Teaching Cyber-Security}
In 2017, Puttawong et al.~\cite{puttawong2017} noted that the abstract nature of network security can make it hard to grasp for students, when taught in traditional lecture-based classes. Therefore, the authors proposed VRFiWall, a VR edutainment game developed with Unity for teaching firewall security. In this game, users interact with the environment using two basic actions, that is, pointing and head gestures. This simple game, which instructs its users about stateless firewalls, revolves around a hero on a secret mission from his kingdom (IP source) to another kingdom (IP destination). To complete his mission, the hero has to match the requirements of Statelessa, a non-playing character that embodies the firewall.

Following the same philosophy, Visoottiviseth et al.~\cite{visoottiviseth2018} proposed Lord of Secure, a serious game for VR that challenges users with different cybersecurity concepts. The game is divided into three chapters, which present various topics: IP spoofing, flooding, TCP covert channels, firewalls, intrusion detection and prevention systems (IDSes and IPSes), and honeypots. The goal of Lord of Secure is to make abstract security topics easier to digest for cybersecurity students and to evaluate the users' understanding of core concepts by means of pre-test and post-test quizzes. Among a group of 28 participants, 90\% claimed that they well understood the concepts covered in the game, and 82\% thought that this approach allowed them to understand better than traditional frontal lessons. Indeed, quizzes that included pre-test and post-test clearly showed that the users improved their knowledge by playing the game. On average, considering Chapters 1 and 3, 70\% of the users improved their scores after playing the game. Taking into account users' enjoyment and their improvements, the authors concluded that edutainment games might be an alternative and more effective way to teach cybersecurity.


Recently, Ulsamer et al.~\cite{ulsamer2021} discussed the use of storytelling in VR to improve user awareness of information security (ISA), in particular, with respect to social engineering. In their experiments, they split the participants into two groups, providing one of the groups with a VR environment and the other (named no-VR) with a traditional e-learning platform. The learning objectives set for the two groups were the same. The no-VR group received theoretical material and interesting examples (but no videos) related to social engineering. The VR group, instead, watched a storytelling 3D video with a coherent and immersive plot that revolves around a hacker which performs social engineering; the hacker is played by an actor and the user follows his steps throughout the story. The ISA analysis show that the VR group achieved better scores in the ISA test, supporting the hypothesis that VR-based learning can improve cybersecurity awareness and material retention. Since that the no-VR group was provided with static material but no traditional videos, it is still unknown whether regular videos could achieve comparable results to VR-based videos.


\subsection{Training Physical Security}
Henrique da Silva et al.~\cite{henriquedasilva2015} proposed a VR-based tool to train staff in a nuclear facility structure and plan its defence strategies. The authors produced (and used) a high-fidelity 3D reproduction of a Brazilian nuclear research centre, showing that their approach can be useful for evaluating the physical security of different facilities. In particular, they proved that inside the VR simulation it is possible to identify strategic points of view and change the environment to evaluate the impact on visibility. For example, their VR implementation allowed them to change weather conditions, such as rain, wind, and snow, as well as natural light conditions (day and night alternation, sun position, presence of stars) and artificial illumination. In conclusion, VR environments can be useful tools, both for evaluating the physical security of critical facilities and for training employees in risk management.

\subsection{Evaluating Cyber-Security Usability}
Mathis et al.~\cite{mathis2021repli,mathis2021virsec} noted that evaluating the usability of new authentication schemes can be expensive, in particular for the cases when special hardware and infrastructures are required. VR can be helpful in addressing this problem. First, it saves researchers time and money by sparing them from building their physical prototypes (e.g., a replica of an ATM machine). Moreover, it enables them to recruit more participants and increase their pools' diversity, by allowing to enrol people from different parts of the world. 

With RepliCueAuth, Mathis et al.~\cite{mathis2021repli} paved the way to usability studies in VR, discerning which security assessments could be transferred from VR to the real world and which ones not. In their work, the authors replicated in VR the experiments of a paper that explored 3 cue-based authentication methods on situated displays. The results showed that some usability studies can be transferred from VR to the real world, such as the accuracy of authentication entry, perceived workload, and perception of security. However, there are some notable differences between the VR experience and the physical study, in terms of results. Users took longer time to authenticate on VR with the touch-based method, while they in the real world the gazing-based approach was the slowest one. Similarly, the security studies conducted on VR and on real world experiments lead to comparable results for some evaluation variables, but results differed in other instances. For example, the attack rates for shoulder-surfing attacks were similar when observing a real human being and a VR avatar, but the accuracy of the attackers' guesses was different. 


%\subsection{Eye-tracking}
%Authentication via eye-tracking hardware on VR can be divided in three main categories:
%\begin{itemize}
%    \item Eye-gazing methods for inputting traditional security primitives, such as PINs and passwords;
%    \item Eye biometrics that leverage the gazing patterns for distinguishing users from each other;
%    \item Retina-based biometrics, based on the unique patterns on human beings' retinas.
%\end{itemize}




% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}\label{sec:conclusion}
Once a technology only used in exhibitions and research laboratories, VR is becoming more and more common among local businesses and families. Therefore, it is of paramount importance to take into consideration the issues that this groundbreaking technology can create. In this work, we have provided a thorough analysis of the privacy and security threats that affect VR. First, we split the privacy issues in three main categories, explaining what could cause these issues and what might be the consequences. Then, we focused on the security aspect of VR. We categorised the threats to security in VR, including both the generic threats that affect VR and the threats that are specific to VR. We have also dedicated a specific section to the topic of authentication in VR, as it appears to be the most common area of research in the field of VR cybersecurity, at the time of writing. Finally, we have covered other interesting areas of research that use VR for cybersecurity goals, such as teaching cybersecurity, training physical security, and evaluating the usability of cybersecurity solutions.



\balance

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.

% use section* for acknowledgment
%\section*{Acknowledgment}


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,./bibliography.bib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}