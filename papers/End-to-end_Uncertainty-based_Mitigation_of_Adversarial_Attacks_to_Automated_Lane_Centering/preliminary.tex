\section{Preliminary}\label{sec:pre}


\subsection{Uncertainty Estimation and Safety Bound\hengyi{Modeling uncertainty}}
Although deep neural networks can reach unprecedented accuracy in many tasks, DNN' predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Measuring the uncertainty of neural networks' output is crucial for safety-critical applications such as  autonomous driving. Prediction uncertainty in deep neural networks generally derives from two sources: data uncertainty and model uncertainty. Data uncertainty is caused by the input noise and intrinsic randomness of the real data while model uncertainty results from a lack of training data in certain areas of the input domain and the test example is out of distribution.  Since the data uncertainty follows a normal distribution, we can use estimate it by using maximum likelihood (ML) approach. In neural networks, the model can take negative log-likelihood function as a loss function to capture the aleatory uncertainty[]. In equation\eqref{log-likelihood}, $\mu$ is the model output and distribution variance $\sigma$ is the estimated data uncertainty. The maximum likelihood approach can give a relatively precise estimate of the data uncertainty along with the mean value. However, the estimator needs to be trained on a large-scale dataset for long time. For already-trained model, data noise can be forward-propagated through the network via Assumed Density Filtering(ABF)[], The ABF replaces each network activation by probability distributions and it can output not only the predictions $\mu$ but also the corresponding variance $\sigma$.  

\begin{equation}
\mathcal{L}(x, y)=-\log \phi(y \mid x)=\frac{\log \hat{\sigma}^{2}_{data}(x)}{2}+\frac{(y-\hat{\mu}(x))^{2}}{2 \tilde{\sigma}^{2}_{data}(x)}\label{log-likelihood}
\end{equation}

For estimate model uncertainty, Similarly to Bayesian approaches, we consider there are distributions over the weights of neural networks and the parameters of the distribution is trained on the training dataset. Therefore, the weight distribution after training can be written as $p(\omega \mid \mathrm{X}, \mathrm{Y})$. We apply the Monte Carlo methods to estimate the distribution by adding dropout layers to sample weights i.e.\eqref{dropout}[]:
\begin{equation}
p(\omega | \mathbf{X}, \mathbf{Y}) \approx \operatorname{Bern}(\omega ; \Phi)\label{dropout}
\end{equation}
Therefore, the model uncertainty estimated as:
\begin{equation}
\sigma_{model}^{2} = \frac{1}{T} \sum_{t=1}^{T} \left(\boldsymbol{\mu}_{t}^{(l)}-\overline{\boldsymbol{\mu}}\right)^{2}\label{monte carlo}
\end{equation}
The total variance can be derived by adding the model variance $\sigma_{model}^{2}$ and data variance $\sigma_{data}^{2}$, i.e.\eqref{total error}
To best of our knowledge, current commercial driving assistance systems doesn't take the perception uncertainty into consideration. 
\begin{equation}
\sigma_{total}^{2} = \sigma_{model}^{2}+\sigma_{data}^{2}\label{total error}
\end{equation}

