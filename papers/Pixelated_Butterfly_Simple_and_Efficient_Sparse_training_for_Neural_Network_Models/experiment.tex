
\section{Experiments}
\label{sec:experiments}


In this section, our goal is to demonstrate that an extremely simple fixed sparsity pattern can actually speed up sparse model training in wall-clock time without degrading model quality. Specifically, we empirically validate three claims that suggest Pixelfly can improve training speed of different model architectures while retaining model quality on a wide range of domains and tasks.
\begin{enumerate}[leftmargin=*,nosep,nolistsep]
  \item Section~\ref{exp:image}: for image classification tasks, we first show the empirical NTK of flat block butterfly + low-rank sparsity pattern is closer to dense NKT than other baselines. Then we demonstrate our superior end-to-end performance. Specifically, we achieve training speed up on both MLP-Mixer and ViT models by up to 2.3$\times$ wall-clock time with no drop in accuracy compared to the dense model and up to 4$\times$ compared to RigL, BigBird and other sparse baselines.
  \item Section~\ref{exp:lm}: for language modeling and text classification tasks, we can speed up GPT-2 small dense model training by 2.1$\times$, achieving a perplexity of 22.5 on wikitext-103. In addition, on Long Range Arena (LRA) benchmark, we maintain the same accuracy but have $5.2\times$ speed-up in training.
  \item Section~\ref{exp:ablation}: we show the necessity of block flat butterfly and low-rank structures, hardware-alignment and wide coverage of most network layers with ablation studies on these three components of Pixelfly.
 \end{enumerate}

\subsection{Image Classification}
\label{exp:image}

\begin{wrapfigure}{}{0.28\textwidth}
\captionsetup{font=small}
  \iftoggle{arxiv}{}{
  \vspace{-3em}
  }
  \centering
  \includegraphics[width=0.9\linewidth]{figs/ntk_cifar100.pdf}
    \vspace{-1em}
  \caption{NTK Comparison with Dense Model.}
  \label{fig:ntk}
  \vspace{-1em}
\end{wrapfigure}

We evaluate the quality and efficiency of Pixelfly through three metrics: (1) distance to training dynamic of the dense model: compare the distance between empirical NTK kernel\footnote{There is an emerging consensus that the NTK is an informative measure of how training and convergence behaviors of two models are similar.} of the models with candidate patterns, including BigBird~\citep{zaheer2020bigbird}, Butterfly~\citep{dao2020kaleidoscope}, and that of the dense model, (2) upstream accuracy: compare the accuracy and training time of the Pixelfly, the dense counterpart, and other baselines on same image classification tasks, (3) downstream accuracy: compare the accuracy of our pretrained Pixelfly and dense model fine-tuned on downstream tasks (\cref{app:throughput}). The empirical NTK of the model with flat block butterfly + low-rank, picked by Pixelfly, is closer to the NTK of the dense model. Pixelfly MLP-mixer and ViT models also retain the same top-1 accuracy of the original dense models while achieving up to 2.3$\times$ speed up.




\textbf{Setup:} We use three popular vision benchmarks, CIFAR-10/100~\citep{krizhevsky2009learning} and ImageNet~\citep{deng2009imagenet}. We choose recent popular Vision Transformer~\citep{dosovitskiy2020image}, T2T-ViT~\citep{yuan2021tokens} and MLP-Mixer~\citep{tolstikhin2021mlp} as representative base models. Their major computation bottlenecks are in different components, e.g. MLP only, attention, or both so we can evaluate the end-to-end applicability of Pixelfly more clearly.

\begin{wrapfigure}{}{0.67\textwidth}
    \vspace{-0.4cm}
\captionsetup{font=small}
    \caption{The performance of Pixelfly and ViT or MLP-Mixer on CIFAR10, CIFAR100 and ImageNet benchmarks. We measure the accuracy and the training time speedup (on ImageNet) compared to the dense model.\vspace{-0.3cm}}
\centering
\resizebox{\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{CIFAR10}&\multicolumn{1}{c}{CIFAR100}&\multicolumn{1}{c}{ImageNet}&\multicolumn{1}{c}{Speedup}\\
\midrule
Mixer-S/16 &86.4& 58.7&72.4& - \\
Pixelfly-Mixer-S/16 &89.8&62.9 &72.6& 1.7$\times$ \\
Mixer-B/16 &87.6& 59.5 &75.6& - \\
Pixelfly-Mixer-B/16 &90.6 &65.4 &76.3& 2.3$\times$ \\
\midrule
ViT-S/16& 89.5 & 65.1 & 77.7 & - \\
Pixelfly-ViT-S/16& 91.3 & 66.8 & 77.5 & 1.9$\times$ \\
ViT-B/16& 89.9 & 61.9 & 78.5 & - \\
Pixelfly-ViT-B/16& 92.2 & 65.1 & 78.6 & 2.0$\times$ \\
\bottomrule
\end{tabular}
}
    \vspace{-0.2cm}
\label{table:imagenet}
\end{wrapfigure}

\textbf{Empirical NTK:}
To characterize the training dynamic of the sparse networks, we compute the empirical NTK kernels for dense Vision Transformer on CIFAR-100. Then, we show the relative differences between kernels of models with different sparsity patterns and that of the dense one in~\cref{fig:ntk}. Specifically, we pick a popular sparsity pattern combination -- Bigbird pattern~\citep{zaheer2020bigbird} for attention layer and random (magnitude-based sparsity at initialization equals to random) for MLP layer, as a representative baseline. The plot indicates that our designed pattern, flat block butterfly + low-rank is the closest one to that of the dense one among all the patterns. Hence, we expect them to enjoy the most benefits of their dense overparameterized counterparts in real tasks. More details on measuring empirical NTK are covered in the~\cref{app:ntk_exp}.


\begin{wrapfigure}{}{0.43\textwidth}
    \vspace{-0.4cm}
\captionsetup{font=small}
    \iftoggle{arxiv}{
      \caption{Comparison with a representative sparse training baseline RigL~\citep{evci2020rigging}.}
    }{
    \caption{Comparison with a representative sparse training baseline RigL~\citep{evci2020rigging}. \vspace{-0.2cm}}
    }
      \centering
          	\resizebox{\linewidth}{!}{
	   %
	\begingroup
	\setlength{\tabcolsep}{10pt}
	\renewcommand{\arraystretch}{1.15}
	   %
	\begin{tabular}{c||c|c}
    \specialrule{.15em}{.05em}{.05em}
    \multirow{1}{*}{ {\bf Model} } & \multicolumn{1}{c|}{\multirow{1}{*}{ImageNet (Acc)}}
                                   & \multicolumn{1}{c}{\multirow{1}{*}{Speedup}}\\
	\hline
    Mixer-S/32 & 58.56 & - \\
	\cline{1-3}
	RigL~\citep{evci2020rigging} &  56.10 & 0.8$\times$ \\
	\cline{1-3}
	Pixelfly (ours)& 59.61  & 2.1$\times$\\
	\specialrule{.15em}{.05em}{.05em}
	\end{tabular}
		\endgroup
	}\label{fig:rigl}
	    \vspace{-0.2cm}
\end{wrapfigure}

\textbf{Training from scratch:} We validate that Pixelfly trains up to 2.3$\times$ and 2.0$\times$ faster than dense MLP-Mixer and ViT models from scratch, with the same accuracy under the same setting (batch size, epochs). Specifically, we sparsify the models with Pixelfly and train them on three commonly used vision benchmarking datasets, CIFAR-10/100 and ImageNet. We measure their Top-1 accuracy wall-clock training time.
To summarize the general trend,~\cref{table:imagenet} highlights that our sparse vision models consistently retain the accuracy of their dense counterparts in terms of accuracy and achieve training-time speed-up.



Furthermore, we have discussed in~\cref{sec:intro} that current sparse training algorithms aim to dynamic search what could be good sparsity for efficient inference but do not speed up training in wall-clock time. But we still present the comparison results in~\cref{fig:rigl} for completeness. For a fair comparison, we conduct the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we aim for both weights \& attention. As expected, RigL does not speed up training (the pioneering work has unstructured sparsity and does not achieve speed up on GPU) but surprisingly Pixelfly outperforms both dense and RigL in terms of accuracy while achieving $2.1\times$ speedup.

\begin{wrapfigure}{}{0.53\textwidth}
  \iftoggle{arxiv}{}{
    \vspace{-0.3cm}
  }
\captionsetup{font=small}
    \caption{Comparison with representative sparse attention baselines.\vspace{-0.3cm}}
      \centering
          	\resizebox{0.95\linewidth}{!}{
	   %
	\begingroup
	\setlength{\tabcolsep}{10pt}
	\renewcommand{\arraystretch}{1.15}
	   %
	\begin{tabular}{c||c|c}
    \specialrule{.15em}{.05em}{.05em}
    \multirow{1}{*}{ {\bf Model} } & \multicolumn{1}{c|}{\multirow{1}{*}{ImageNet (Acc)}}
                                   & \multicolumn{1}{c}{\multirow{1}{*}{Speedup}}\\
	\hline
    T2T-ViT & 81.7 & - \\
	\cline{1-3}
	BigBird &  81.5 & 0.9$\times$ \\
	\cline{1-3}
	Sparse Transformer &  81.4 &  1.3$\times$ \\
	\cline{1-3}
	Pixelfly& 81.7  & 1.4$\times$\\
	\specialrule{.15em}{.05em}{.05em}
	\end{tabular}
		\endgroup
	}\label{fig:bigbird}
	\vspace{-0.3cm}
\end{wrapfigure}


Finally, we compare Pixelfly with BigBird and Sparse Transformer pattern. For a fair comparison, we choose T2T-ViT as the base model because its major bottleneck is on the T2T attention module (our baselines are efficient attention variants). We can see from~\cref{fig:bigbird} that Pixelfly is the only one that can maintain the accuracy and have actual speed up. Further more, Pixelfly speeds up T2T module (large attention) by 1.4$\times$ compare to dense.







\subsection{Language Modeling and Text Classification}
\label{exp:lm}

In this section, we aim to evaluate the effectiveness of Pixelfly in the text domain, on a language modeling task and Long Range Arena (LRA~\citep{tay2020long}) benchmarks. On WikiText-103~\citep{merity2016pointer}, Pixelfly achieves 22.5 perplexity, which is around the same perplexity as GPT-2 small~\citep{radford2019language} but trains 2.1$\times$ faster. On LRA, Pixelfly obtains almost the same accuracy as the full model but gains up to $5.2\times$ speed-up.



\textbf{Setup:} We use WikiText-103 for language modeling and LRA for classification tasks. We use GPT-2 small and vanilla Transformer as the base dense models. The computational bottleneck of GPT-2 small for moderate sequence length, e.g. 512, would be on both attention and MLP layers, while the bottleneck of transformer on LRA task is on attention since the benchmark is designed to evaluate models under long-context scenarios. 

\begin{wrapfigure}{}{0.5\textwidth}
    \vspace{-0.3cm}
\captionsetup{font=small}
    \caption{The performance of Pixelfly, BigBird and GPT-2-Small, Medium on WikiText-103. We measure the perplexity and the training speed up.\vspace{-0.3cm}}
      \centering
          	\resizebox{\linewidth}{!}{
	\centering
	\begingroup
	\setlength{\tabcolsep}{10pt}
	\renewcommand{\arraystretch}{1.15}
	\begin{tabular}{c||c|c}
    \specialrule{.15em}{.05em}{.05em}
    \multirow{1}{*}{ {\bf Model} } & \multicolumn{1}{c|}{\multirow{1}{*}{WikiText-103 (ppl)}}
                                   & \multicolumn{1}{c}{\multirow{1}{*}{Speedup}}\\
	\hline
    GPT-2-Small &  22.2 & - \\
	\cline{1-3}
	BigBird & 23.3  & 0.96$\times$\\
	\cline{1-3}
	Pixelfly& 22.5  & 2.1$\times$\\
	\hline
	    GPT-2-Medium &  20.9 & - \\
	\cline{1-3}
	BigBird & 21.5  & 1.1$\times$\\
	\cline{1-3}
	Pixelfly& 21.0  & 2.5$\times$\\
	\specialrule{.15em}{.05em}{.05em}
	\end{tabular}
		\endgroup
	}
	\vspace{-0.3cm}
	\label{table:gpt}
\end{wrapfigure}

\textbf{GPT-2-Small, Medium on WikiText-103:} We show training GPT-2-Small, Medium and its Pixelfly model from scratch on a commonly used NLP benchmarking dataset, wikiText-103. We measure their perplexity on that dataset, and our training speed up.
All setup and finetuning hyperparameters follow the ones in the original paper~\citep{radford2019language}. We present the results in~\cref{table:gpt}. It is not hard to see that Pixelfly models have great advantages in accuracy-efficiency tradeoffs since it maintains the same perplexity as the dense model but achieve up to 2.5$\times$ speed-up in training.

\begin{wrapfigure}{}{0.7\textwidth}
    \vspace{-0.4cm}
\captionsetup{font=small}
    \caption{The performance of Pixelfly, Reformer and vanilla transformer on Long-Range-Arena benchmarks. We measure the accuracy and training speed.\vspace{-0.3cm}}
      \centering
\resizebox{\linewidth}{!}{
	\centering
	\Huge
	\begingroup
	\setlength{\tabcolsep}{10pt}
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c||c|c|c|c|c|c|c}
    \specialrule{.15em}{.05em}{.05em}
    \multirow{1}{*}{ {\bf Model} }  &
    \multicolumn{1}{c|}{\multirow{1}{*}{ListOps}} &
    \multicolumn{1}{c|}{\multirow{1}{*}{Text}} & 
    \multicolumn{1}{c|}{\multirow{1}{*}{Retrieval}} & 
    \multicolumn{1}{c|}{\multirow{1}{*}{Image}} &
    \multicolumn{1}{c|}{\multirow{1}{*}{Pathfinder}}  & 
    \multicolumn{1}{c|}{\multirow{1}{*}{Avg}} &
    \multicolumn{1}{c}{\multirow{1}{*}{Speedup}}\\
	\hline
	\hline
	Transformer& 36.54& 63.12& 80.33 & 41.56 & \textbf{73.49} & 59.01 & -\\
	\cline{1-8}
	\hline
	Reformer& 36.85 & 58.12 & 78.36 & 28.30 & 67.95 & 53.90 & 0.8$\times$ \\
	Pixelfly& \textbf{37.65} & \textbf{66.78} & \textbf{80.55} & \textbf{42.35} & 72.01 & \textbf{59.86} & 5.2$\times$ \\
	\specialrule{.15em}{.05em}{.05em}
	\end{tabular}
		\endgroup
	}
	\label{table:lra}
	\vspace{-0.3cm}
\end{wrapfigure}
\textbf{Vanilla Transformer on LRA:} We compare vanilla transformer and its Pixelfly models trained from scratch on LRA benchmark. We measure the accuracy, throughput, and training time of both models. Each task has a different sequence length varying between 1024 and 4096.
We follow the implementation and experimental setting in~\citep{xiong2021nystr}. We compare the performance of Pixelfly against the dense transformer and report the results in~\cref{table:lra}. We also include the numbers of other baselines from the same repository in the appendix. We can see Pixelfly cause almost no drop in accuracy while achieving 5.2$\times$ speed-up in time.






\subsection{Ablation Study}
\label{exp:ablation}
We conduct ablation studies on each component of Pixelfly (Details in~\cref{app:study}). Specifically, we present (i) how flat block butterfly and low-rank affect the model quality, (ii) how different block size would affect the training speed, (iii) how budget allocation affects the end-to-end speed up.

\textbf{Necessity of Flat Block Butterfly and Low-rank:} (i) We apply different parameter allocation of flat block butterfly and Low-rank component in Pixelfly Mixer-S model on CIFAR-10 under the different density varying in [0.05, 0.1, 0.2]. We found that similar to what was reported in~\citep{scatterbrain}, using around $\frac{1}{4}$ budget on Low-rank and $\frac{3}{4}$ on flat block butterfly achieves the best accuracy. (ii) We also compare Pixelfly with baseline sparsity patterns and show it is $2.7\times$ faster than dense, $3\times$ faster than Butterfly, $3.2\times$ faster than BigBird under $10\%$ density.

\textbf{Block Size:} We study the accuracy-efficiency trade-off for flat block butterfly and random sparsity pattern with different block sizes from 1-32 (~\cref{table:throughput}). We found that first, under the same density, the same sparsity patterns covered with different block sizes could have a big difference in efficiency. 
Under the same block, the pattern with more locality can be more efficient. 
Last, the density can seem very small, but actually memory access could be up to $100\%$ of the matrix. 
Therefore, we always want to make full utilization of the smallest block size that the hardware (or compiler) supported.   

\textbf{Budget Allocation:} We sparsify different components of ViT-small separately, including attention and MLP. We show that their compute ratio is approximately $1:2$ , so if only sparsify one of them, the other one will be the bottleneck preventing end-to-end speed up. Therefore, it is necessary to have an algorithm that can sparsify all layers. 










  








