


\newpage
\section{Dropout Neural Network and KRR}\label{sec:dropout_KRR}
\label{sec:notation1}
We consider a two layer neural network with ReLU activation function, and write 
\begin{align}
\label{eq:pb2}
f(W, x) := \frac{1}{\sqrt{m}}\sum_{r = 1}^{m}a_r \phi(w_{r}^{\top}x) = \frac{1}{\sqrt{m}}\sum_{r = 1}^{m}a_r w_{r}^{\top}x \mathbf{1}_{w_{r}^{\top}x\geq 0}
\end{align}
where $w_r(0) \sim N(0, I_d) \in \R^d$, $a_{r} \sim \mathrm{unif}(\{-1, +1\})$ and all randomnesses are independent. We will fix $a_r$ during the training process and use $\frac{1}{\sqrt{m}}$ normalization factor, both of which are in the literature of \cite{dzps19,sy19,bpsw21}.

Suppose the training data are $(x_1,y_1), \ldots, (x_n, y_n) \in \R^{d}\times \R$, we define the classical objective function $\hat{L}$ as follows:
\begin{align*}
\hat{L}(W) := \frac{1}{2}\sum_{i = 1}^{n}\left(f(W, x_i) - y_i\right)^2.
\end{align*}

The gradient with respect to loss function $\hat{L}$ is %
\begin{align*}
\frac{\partial \hat{L}}{\partial w_r} = \frac{1}{\sqrt{m}}\sum_{i=1}^{n} (f(W, x_i) - y_i)a_r x_i\mathbf{1}_{w_r^{\top}x_i \geq 0}.
\end{align*}


We consider the effect of dropout on network training. For each $r \in [m]$, we introduce the mask by defining random variable $\sigma_{r}$ as follows:
\begin{align*}
\sigma_r = 
\begin{cases}
0, & \mathrm{with~probability~} 1-q ; \\
1/q, & \mathrm{with~probability~} q .
\end{cases}
\end{align*}

It is easy to see that $\E[ \sigma_r ] = 0 \cdot (1-q) + (1/q) \cdot q = 1$ and $\E[ \sigma_r^2 ] = 0^2 \cdot (1-q) + (1/q)^2 \cdot q = 1/q$. %
We assume $\sigma_i$ and $\sigma_j$ are independent for any $i\neq j$, then $\E[\sigma_i\sigma_j] = \E[\sigma_i]\E[\sigma_j]=1$. Let $\sigma = (\sigma_1,\cdots, \sigma_m)$, we define our \textbf{dropout neural net} as
\begin{align}
\label{eq:pb1}
F(W, x, \sigma) := \frac{1}{\sqrt{m}}\sum_{r = 1}^{m}a_r \sigma_r\phi(w_{r}^{\top}x) = \frac{1}{\sqrt{m}}\sum_{r = 1}^{m}a_r\sigma_r w_{r}^{\top}x \mathbf{1}_{w_{r}^{\top}x\geq 0}.
\end{align}
Dropout explicitly change the target function, since we need to minimize the $\ell_2$ distance between $F(W, x, \sigma)$ and $y$, instead of $f(W, x)$ and $y$. Formally, we define the \textbf{dropout loss} as %
\begin{align}
\label{eq:dropout_loss_def}
L(W) := \frac{1}{2}\E_{\sigma}\left[\sum_{i = 1}^{n}\left(F(W, x_i, \sigma) - y_i\right)^2\right].
\end{align}

We first give an explicit formulation of $L$ which also shows the difference between $L$ and $\hat{L}$.%

\begin{lemma}
\label{lem:explicit-regularization}
The dropout loss defined in Eq.~\eqref{eq:dropout_loss_def} can be expressed as the sum of classical loss $\hat{L}$ and a regularization term as
\begin{align}
\label{eq:explicit-regularization}
L(W) = \hat{L}(W) + \frac{1-q}{2mq}\sum_{i=1}^{n} \sum_{r = 1}^{m}\phi(w_r^\top x_i)^{2}.
\end{align}
\end{lemma}

\begin{proof}
Since $\E[\sigma_r] = 1$, we have
\begin{align}\label{eq:E_F}
    \E_{\sigma}[F(W, x_i, \sigma)] = & ~ \frac{1}{\sqrt{m}}\E_{\sigma}[\sum_{r = 1}^{m}a_r \sigma_r\phi(w_{r}^{\top}x)] =  \frac{1}{\sqrt{m}}\sum_{r=1}^{m}a_r\phi(w_r^{\top}x_i) = f(W, x_i)
\end{align}
holds for any $i \in [n]$. Next, we show the difference between $L$ and $\hat{L}$:
\begin{align}
    & ~ 2( L(W) - \hat{L}(W) ) \notag\\
    = & ~ \E_{\sigma}\left[\sum_{i = 1}^{n}\left(F(W, x_i, \sigma) - y_i\right)^2\right] -  \sum_{i = 1}^{n}\left(f(W, x_i) - y_i\right)^2\notag\\
    = & ~ \sum_{i = 1}^{n}\left(\E_{\sigma}\left[\left(F(W, x_i, \sigma) - y_i\right)^2\right] - \left(f(W, x_i) - y_i\right)^2 \right)\notag\\
    = & ~ \sum_{i = 1}^{n}\left(\E_{\sigma}\left[F(W, x_i, \sigma)^2\right] - f(W, x_i)^2  \right)\notag \\
    = & ~ \sum_{i = 1}^{n}\left(\frac{1}{m}\sum_{r_1, r_2 \in [m]}\E[a_{r_1}a_{r_2}\sigma_{r_1}\sigma_{r_2}\phi(w_{r_1}^{\top}x_i)\phi(w_{r_2}^{\top}x_i)]- \frac{1}{m}\sum_{r_1, r_2 \in [m]}a_{r_1}a_{r_2}\phi(w_{r_1}^{\top}x_i)\phi(w_{r_2}^{\top}x_i)
    \right)\notag\\
    = & ~ \frac{1}{m}\cdot \frac{1-q}{q}\sum_{i = 1}^{n}\sum_{r=1}^{m}a_r^{2}\phi(w_{r}^{\top}x_i)^2 \notag\\
    = & ~ \frac{1}{m}\cdot \frac{1-q}{q}\sum_{i = 1}^{n}\sum_{r=1}^{m}\phi(w_{r}^{\top}x_i)^2 \label{eq:pb3}
\end{align}
where the first step follows from definition, the second step follows from the linearity of expectation, the third step follows from Eq.~\eqref{eq:E_F}, the forth step follows from expansion, the fifth step follows from $\E[\sigma_{r_1}\sigma_{r_2}] = 1$ for $r_1 \neq r_2$ and $\E[\sigma_{r_1}^2] = \frac{1}{q}$, and the last step follows from $a_r^2 = 1$. Thus we have
\begin{align*}
    L(W) = \hat{L}(W) + \frac{1-q}{2mq}\sum_{i = 1}^{n}\sum_{r=1}^{m}\phi(w_{r}^{\top}x_i)^2
\end{align*}
and finish the proof.
\end{proof}



Before we move on, we introduce some extra notations and definitions. We denote%
\begin{align*}
    \overline{W} = \mathrm{vec}(W) = \left[\begin{matrix}
    w_1\\
    w_2\\
    \vdots\\
    w_m
    \end{matrix}
    \right] \in \R^{md}, ~~~and~~~
    Y = \left[\begin{matrix}
    y_1\\
    y_2\\
    \vdots\\
    y_n
    \end{matrix}
    \right] \in \R^{n}.
\end{align*}

\begin{definition}
We define matrix $G^{\infty}\in\R^{n\times n}$ which can be viewed as a Gram matrix from a kernel associated with ReLU function as follows:
\begin{align*}
    G^{\infty}_{ij}(X) = \E_{w\sim\N(0,I)}[x_i^\top x_j \mathbf{1}_{w^\top x_i \geq 0, w^\top x_j\geq 0}],~~~ \forall i, j\in [n]\times [n]
\end{align*}
and assume $\lambda_0 = \lambda_{\min}(G^{\infty}) > 0$\footnote{According to Theorem 3.1 in \cite{dzps19}, the assumption holds when $x_i$ is not parallel with $x_j$ for $i\neq j$, which is reasonable in reality.}.
\end{definition}

\begin{definition}\label{def:Phi_first_time}
We define the masked matrix $\Phi_{W}(X, \sigma)\in \R^{n \times md}$ as %
\begin{align*}
    \Phi_{W}(X,\sigma) := & ~ \frac{1}{\sqrt{m}}\left[
    \begin{matrix}
    \Phi(x_1, \sigma)\\
    \Phi(x_2, \sigma)\\
    \vdots\\
    \Phi(x_n, \sigma)
    \end{matrix}
    \right] \\
    = & ~
    \frac{1}{\sqrt{m}}\left[
    \begin{matrix}
    a_1 \sigma_1  \mathbf{1}_{\langle w_{1}, x_1\rangle \geq 0} x_1^{\top} & a_2 \sigma_2 \mathbf{1}_{\langle w_{2}, x_1\rangle \geq 0} x_1^{\top} &\ldots &a_m \sigma_m \mathbf{1}_{\langle w_{m}, x_1\rangle \geq 0} x_1^{\top}\\
    a_1 \sigma_1 \mathbf{1}_{\langle w_{1}, x_2\rangle \geq 0} x_2^{\top} & a_2 \sigma_2 \mathbf{1}_{\langle w_{2}, x_2\rangle \geq 0} x_2^{\top} &\ldots &a_m \sigma_m \mathbf{1}_{\langle w_{m}, x_2\rangle \geq 0} x_2^{\top}\\
    \vdots &\vdots &\vdots &\vdots\\
    a_1 \sigma_1 \mathbf{1}_{\langle w_{1}, x_n\rangle \geq 0} x_n^{\top} & a_2 \sigma_2 \mathbf{1}_{\langle w_{2}, x_n\rangle \geq 0} x_n^{\top} &\ldots &a_m \sigma_m \mathbf{1}_{\langle w_{m}, x_n\rangle \geq 0} x_n^{\top}\\
    \end{matrix}
    \right]
\end{align*}
and also define the unmasked matrix $\hat{\Phi}_W(X)\in\R^{n\times md}$ as
\begin{align*}
    \hat{\Phi}_W(X) := \frac{1}{\sqrt{m}}\left[
    \begin{matrix}
    a_1  \mathbf{1}_{\langle w_{1}, x_1\rangle \geq 0} x_1^{\top} & a_2 \mathbf{1}_{\langle w_{2}, x_1\rangle \geq 0} x_1^{\top} &\ldots &a_m \mathbf{1}_{\langle w_{m}, x_1\rangle \geq 0} x_1^{\top}\\
    a_1 \mathbf{1}_{\langle w_{1}, x_2\rangle \geq 0} x_2^{\top} & a_2 \mathbf{1}_{\langle w_{2}, x_2\rangle \geq 0} x_2^{\top} &\ldots &a_m \mathbf{1}_{\langle w_{m}, x_2\rangle \geq 0} x_2^{\top}\\
    \vdots &\vdots &\vdots &\vdots\\
    a_1 \mathbf{1}_{\langle w_{1}, x_n\rangle \geq 0} x_n^{\top} & a_2 \mathbf{1}_{\langle w_{2}, x_n\rangle \geq 0} x_n^{\top} &\ldots &a_m \mathbf{1}_{\langle w_{m}, x_n\rangle \geq 0} x_n^{\top}\\
    \end{matrix}
    \right].
\end{align*}
\end{definition}

\begin{definition}\label{def:Psi_first_time}
We  define the masked block diagonal matrix $\Psi_W(X, \sigma) \in\R^{md \times md}$ as
\begin{align*}
    \Psi_{W}(X, \sigma) :=
    \frac{1}{m}\diag \Big( \psi_1, \psi_2, \cdots, \psi_m \Big).
\end{align*}
where $\forall r \in [m]$, $\psi_r \in \R^{d \times d}$ is defined as
\begin{align*}
    \psi_r := a_r^2 \sigma_r^2\sum_{i=1}^{n} x_i x_i^{\top}\cdot \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0}^{2} = \sigma_r^2 \sum_{i=1}^{n} x_i x_i^{\top}\cdot \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0}.
\end{align*}
We also define the unmasked block diagonal matrix $\hat{\Psi}_W(X) \in\R^{md \times md}$ as
\begin{align*}
    \hat{\Psi}_{W}(X) :=
    \frac{1}{m}\diag \Big( \hat{\psi}_1, \hat{\psi}_2, \cdots, \hat{\psi}_m \Big).
\end{align*}
where $\forall r \in [m]$, $\hat{\psi}_r \in \R^{d \times d}$ is defined as
\begin{align*}
    \hat{\psi}_r := \sum_{i=1}^{n} x_i x_i^{\top}\cdot \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0}.
\end{align*}
\end{definition}

\begin{lemma}
It is easy to verify that
\begin{align*}
    \Phi_W(X,\sigma) = \hat{\Phi}_W(X) \cdot D_{\sigma} ~~~ and ~~~\Psi_W(X, \sigma) = \hat{\Psi}_W(X) \cdot D_{\sigma}^2
\end{align*}
where
\begin{align*}
    D_{\sigma} := \diag (\underbrace{\sigma_1,\cdots, \sigma_1}_{d}, \cdots, \underbrace{\sigma_m,\cdots, \sigma_m}_{d})\in\R^{md\times md}.
\end{align*}
\end{lemma}

For convenience, we will simply denote $\Phi_W = \Phi_W(X, \sigma)$ and $\Psi_W = \Psi_W(X,\sigma)$. Then by using the above notations, we can express our dropout loss as $L(W)=\frac{1}{2}\E_{\sigma} [\|\Phi_W\overline{W} - Y\|_2^2]$.


\begin{lemma}\label{lem:explicit_regularization_2}
If we denote $\lambda = \frac{1-q}{q}\geq 0$, then we have
\begin{align*}
    L(W) = \frac{1}{2}\|\hat{\Phi}_{W} \overline{W} - Y\|_{2}^{2} + \frac{\lambda}{2}\overline{W}^{\top}\hat{\Psi}_{W}\overline{W}.
\end{align*}
\end{lemma}
\begin{proof}
As for the first term, we have
\begin{align*}
    \|\hat{\Phi}_W \overline{W} - Y\|_2^2 = & ~ \sum_{i=1}^n (\frac{1}{\sqrt{m}}\sum_{r=1}^m a_r \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0} x_i^\top \cdot w_r - y_i)^2 \\
    = & ~ \sum_{i=1}^n (\frac{1}{\sqrt{m}}\sum_{r=1}^m a_r \phi(w_r^\top x_i) - y_i)^2 \\
    = & ~ \sum_{i=1}^n (f(W,x_i) - y_i)^2 \\
    = & ~ 2\hat{L}(W).
\end{align*}
As for the second term, since $\hat{\Psi}_W$ is a block diagonal matrix, we have
\begin{align*}
    \overline{W}^\top \hat{\Psi}_W \overline{W} = & ~ \frac{1}{m}\sum_{r=1}^m \Big(w_r^\top \cdot \big( a_r^2\sum_{i=1}^n x_i x_i^\top \cdot \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0}^{2}\big) \cdot w_r\Big) \\
    = & ~ \frac{1}{m}\sum_{r=1}^m\sum_{i=1}^n \big((w_r^\top x_i)\cdot(w_r^\top x_i)^\top\cdot \mathbf{1}_{\langle w_{r}, x_i \rangle \geq 0}^{2}\big) \\
    = & ~ \frac{1}{m}\sum_{i=1}^n \sum_{r=1}^m\phi(w_r^\top x_i)^2.
\end{align*}
Thus by using Lemma~\ref{lem:explicit-regularization}, we have
\begin{align*}
    L(W) = & ~ \hat{L}(W) + \frac{1-q}{2mq}\sum_{i=1}^n\sum_{r=1}^m \phi(w_r^\top x_i)^2 \\
    = & ~ \frac{1}{2}\|\hat{\Phi}_{W}\overline{W} - Y\|_{2}^{2} + \frac{\lambda}{2}\overline{W}^{\top}\hat{\Psi}_{W}\overline{W}
\end{align*}
and finish the proof.
\end{proof}

\begin{remark}
A classical kernel ridge regression problem can be defined as
\begin{align*}
    \min_{W} \frac{1}{2} \|\phi(X)^\top W - Y\|_2^2 + \frac{\lambda}{2} \|W\|_2^2
\end{align*}
where $\phi: \R^d \to \mathcal{F}$ is a feature map. Note that Lemma~\ref{lem:explicit_regularization_2} breaks the dropout loss into two parts: the first part is an error term, and the second part can be seen as a regularization term. 
Thus the task of minimizing the dropout loss $L(W)$ is equivalent to a kernel ridge regression (KRR) problem. 
\end{remark}






\newpage
\section{Dynamics of Kernel Methods (Continuous Gradient Flow)}\label{sec:gradient_flow}

The NTK also allows us to analyze the training convergence of sparse networks.
We show that gradient descent converges globally when training wide sparse networks.
This convergence speed is similar to that of dense models~\citep{dzps19,als19_dnn}.

In this section we will discuss the dynamics of kernel method under the mask $\sigma$, which adds sparsity in the output layer. Our problem will be considered in over-parameterized scheme.
First we introduce some additional definitions and notations. We define symmetric Gram matrix $G(W)$ as $G(W) := \hat{\Phi}_W\cdot\hat{\Phi}_W^\top \in \R^{n\times n}$. For all $i, j\in [n] \times [n]$, we have
\begin{align*}
    G(W)_{ij} = \frac{1}{m}\sum_{r=1}^m a_r^2 \mathbf{1}_{\langle w_r, x_i \rangle\geq 0, \langle w_r, x_j \rangle\geq 0} x_i^\top x_j = \frac{1}{m} x_i^\top x_j \sum_{r=1}^m \mathbf{1}_{\langle w_r, x_i \rangle\geq 0, \langle w_r, x_j \rangle\geq 0}.
\end{align*}
We define block symmetric matrix $H(W)$ as $H(W) = \hat{\Phi}_W^\top \cdot \hat{\Phi}_W\in\R^{md\times md}$. Then for all $i, j\in [m] \times [m]$, the $(i,j)$-th block of $H(W)$ is
\begin{align*}
    H(W)_{ij} = \frac{1}{m} a_i a_j \sum_{k=1}^n x_k x_k^\top \cdot \mathbf{1}_{\langle w_i, x_k \rangle\geq 0, \langle w_j, x_k \rangle\geq 0} \in \R^{d\times d}.
\end{align*}


By using Lemma~\ref{lem:explicit_regularization_2}, we consider the corresponding kernel regression problem: 
\begin{align}
\label{eq:sc1}
    \min_{W}L_{k}(W) = \min_{W}\frac{1}{2}\|\hat{\Phi} \overline{W} - Y\|_2^2 + \frac{\lambda}{2}\overline{W}^{\top}\hat{\Psi} \overline{W}
\end{align}
where $\hat{\Phi} \in \R^{n \times md}$, $\overline{W}\in \R^{md\times 1}$, $Y\in \R^{n\times 1}$ and $\hat{\Psi}\in \R^{md\times md}$. The main difference from neural network is that we assume $\hat{\Phi}$ (related to NTK, e.g., see Definition~\ref{def:Phi_first_time}) and $\hat{\Psi}$ (related to regularization term, e.g., see Definition~\ref{def:Psi_first_time}) do not change during the training process. 






The gradient of $L_k$ can be expressed as
\begin{align}
\label{eq:nabla_W}
\nabla_{\overline{W}} L_k(W) = \hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top} Y + \lambda\hat{\Psi} \overline{W}.
\end{align}
We use $\overline{W^{\star}}$ to denote the optimal solution of Eq.~\eqref{eq:sc1}, and it satisfies 
\begin{align}
\label{eq:st2}
    \nabla_{\overline{W}} L_k(W)\big|_{\overline{W} = \overline{W^{\star}}} = (\hat{\Phi}^{\top}\hat{\Phi} + \lambda \hat{\Psi})\overline{W^{\star}} - \hat{\Phi}^{\top}Y = 0.
\end{align}
Since $\hat{\Psi}$ is a positive diagonal matrix, $\hat{\Phi}^{-\frac{1}{2}}$ exists, thus we have
\begin{align*}
    \overline{W^{\star}} = & ~ (\hat{\Phi}^\top \hat{\Phi} + \lambda \hat{\Psi})^{-1} \hat{\Phi}^\top Y.
\end{align*}
Next, we consider the question from a continuous gradient flow aspect. In time $t$, we denote $\overline{W}(t) = \mathrm{vec}(W(t)), \hat{\Phi}(t) = \hat{\Phi}_{W(t)}, \hat{\Psi}(t) = \hat{\Psi}_{W(t)}$. We also denote $G(t) = G(W(t))$ and $H(t) = H(W(t))$. Following the literature of \cite{dzps19}, we consider the ordinary differential equation defined by
\begin{align}
    \label{eq:ode_flow}
    \frac{\d w_r(t)}{\d t} = - \frac{\partial L_k(W(t))}{\partial w_r(t)}.
\end{align}

\begin{lemma}[Lemma 3.1 in \cite{dzps19}]\label{lem:dzps3.1}
If $m = \Omega(\frac{n^2}{\lambda_0^2}\log(\frac{n}{\delta}))$, we have with probability at least $1-\delta$, $\|G(0) - G^{\infty}\|_2 \leq \frac{\lambda_0}{4}$ and $\lambda_{\min}(G(0))\geq \frac{3}{4}\lambda_0$.
\end{lemma}

\begin{lemma}[Lemma 3.2 in \cite{dzps19}]\label{lem:dzps3.2}
If $w_1,\cdots, w_m$ are i.i.d generated from $\mathcal{N}(0,I_d)$, then with probability at least $1-\delta$, the following holds. For any set of weight vectors $w_1,\cdots,w_m \in\R^d$ that satisfy for any $r\in [m], \|w_r - w_r(0)\|_2\leq \frac{c\delta \lambda_0}{n^2}$ for some small positive constant $c$, then matrix $G\in\R^{d\times d}$ satisfies $\|G - G(0)\|_2 < \frac{\lambda_0}{4}$ and $\lambda_{\min}(G) > \frac{\lambda_0}{2}$.
\end{lemma}
The above lemma shows that for $W$ that is close to $W(0)$, the Gram matrix $G$ also stays close to the initial Gram matrix $G(0)$, and its minimal eigenvalue is lower bounded. 

\begin{lemma}[Gradient Flow]\label{lem:gradient_flow}
If we assume $\lambda_{\min}(\hat{\Psi}) \geq \Lambda_0 > 0$, then with probability at least $1 - \delta$, for $w_1,\cdots,w_m \in\R^d$ that satisfy $\forall r\in [m], \|w_r - w_r(0)\|_2\leq \frac{c\delta \lambda_0}{n^2}$, we have
\begin{align*}
    \frac{\d\|\hat{\Phi} \overline{W} -\hat{\Phi} \overline{W^{\star}} \|_{2}^{2}}{\d t} \leq -\gamma \|\hat{\Phi} \overline{W} -\hat{\Phi} \overline{W^{\star}}\|_{2}^{2}
\end{align*}
holds some constant $\gamma > 0$.
\end{lemma}
\begin{proof}
By using Eq.~\eqref{eq:nabla_W} and Eq.~\eqref{eq:ode_flow}, we can express $\frac{\d \overline{W}}{\d t}$ as
\begin{align}
    \label{eq:d_W_bar}
    \frac{\d \overline{W}}{\d t} = - \nabla_{\overline{W}} L_k(W) = -( \hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top} Y + \lambda\hat{\Psi} \overline{W}).
\end{align}
Then we have 
\begin{align}\label{eq:d_Phi_W}
    & ~ \frac{\d \|\hat{\Phi} \overline{W} -\hat{\Phi} \overline{W^{\star}} \|_{2}^{2}}{\d  t} \notag \\
    = & ~ \frac{\d \|\hat{\Phi} \overline{W} -\hat{\Phi} \overline{W^{\star}} \|_{2}^{2}}{\d  \overline{W}}\cdot\frac{\d \overline{W}}{\d t} \notag \\
    = & ~ 2(\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}})^{\top}\hat{\Phi} \cdot (- ( \hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top}Y + \lambda\hat{\Psi} \overline{W})) \notag \\
    = & ~ -2 (\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}})^{\top}\hat{\Phi} ( \hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top}Y + \lambda\hat{\Psi} \overline{W}) \notag \\
    = & ~ -2 (\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}})^{\top}\hat{\Phi} (\hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top}\hat{\Phi} \overline{W^{\star}} - \lambda\hat{\Psi} \overline{W^{\star}} + \lambda\hat{\Psi} \overline{W}) \notag \\
    = & ~ -2 (\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}})^{\top}\hat{\Phi}\hat{\Phi}^{\top}(\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}}) - 2\lambda(\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}})^{\top}\hat{\Phi} (\hat{\Psi} \overline{W} - \hat{\Psi} \overline{W^{\star}}) \notag \\
    \leq & ~ -2 \lambda_0 \|\hat{\Phi} \overline{W} - \hat{\Phi} \overline{W^{\star}}\|_2^2 - 2\lambda (\overline{W} - \overline{W^{\star}})^{\top}\hat{\Phi}^{\top} \hat{\Phi} \hat{\Psi} (\overline{W} - \overline{W^{\star}})
\end{align}
where the second step follows from Eq.~\eqref{eq:d_W_bar}, the fourth step follows from Eq.~\eqref{eq:st2}, and the last step follows from the definition that $\lambda_0 = \lambda_{\min}(G) = \lambda_{\min}(\hat{\Phi} \hat{\Phi}^\top)$.

As for the second term in the Eq.~\eqref{eq:d_Phi_W}, we have
\begin{align}
    \label{eq:second_term}
    & ~ 2\lambda (\overline{W} - \overline{W^{\star}})^{\top}\hat{\Phi}^{\top} \hat{\Phi} \hat{\Psi} (\overline{W} - \overline{W^{\star}}) \notag \\
    = & ~ 2\lambda (\overline{W}\hat{\Phi}^{\top} \hat{\Phi} - \overline{W^{\star}}\hat{\Phi}^{\top} \hat{\Phi})^{\top} \hat{\Psi} (\overline{W} - \overline{W^{\star}}) \notag \\
    \geq & ~ 2\lambda \Lambda_0 (\overline{W} - \overline{W^{\star}})^{\top}\hat{\Phi}^{\top} \hat{\Phi} (\overline{W} - \overline{W^{\star}}) \notag \\
    = & ~ 2\lambda \Lambda_0 \|\hat{\Phi} \overline{W} - \hat{\Phi}\overline{W^{\star}}\|_2^2
\end{align}
Thus by Eq.~\eqref{eq:d_Phi_W} and Eq.~\eqref{eq:second_term} we have
\begin{align*}
     \frac{\d \|\hat{\Phi} \overline{W} -\hat{\Phi} \overline{W^{\star}} \|_{2}^{2}}{\d t} \leq -(2\lambda_0 + 2\lambda \Lambda_0) \|\hat{\Phi} \overline{W} - \hat{\Phi}\overline{W^{\star}}\|_2^2.
\end{align*}
By letting $\gamma = 2\lambda_0 + 2\lambda \Lambda_0$ we finish the proof.







\end{proof}

For convenience, we denote $u(t) = \hat{\Phi}(t) \cdot \overline{W}(t) \in \R^n$. Then it is easy to verify that
\begin{align*}
    u_i(t) = \frac{1}{\sqrt{m}}\sum_{r=1}^m a_r \phi(w_r^\top x_i) = f(W(t), x_i), ~~~\forall i\in [n],
\end{align*}
showing that $u(t)$ is the prediction in time $t$. %

\begin{lemma}[Convergence rate]\label{lem:gradient_flow_2}
If we assume $\lambda_{\min}(G(s))\geq \frac{\lambda_0}{2}$ holds for $0\leq s\leq t$, then we have
\begin{enumerate}
    \item $\|u(t) - Y\|_2^2 \leq e^{-(\lambda_0+2\lambda/m) t} \|u(0) - Y\|_2^2;$
    \item $\forall r\in [m], \|w_r(t) - w_r(0)\|_2 \leq \frac{\sqrt{n}\|u(0) - Y\|_2}{\lambda_0 \sqrt{m}}.$
\end{enumerate}
\end{lemma}
\begin{proof}
From Eq.~\eqref{eq:nabla_W}, we can express the dynamics by using $u(t)$ as
\begin{align}
    \label{eq:d_u}
    \frac{\d u(t)}{\d t} = & ~ - \hat{\Phi} (\hat{\Phi}^{\top}\hat{\Phi} \overline{W} - \hat{\Phi}^{\top} Y + \lambda\hat{\Psi} \overline{W}) \notag \\
    = & ~ G(t) (Y - u(t)) - \lambda \hat{\Phi} \hat{\Psi} \overline{W}.
\end{align}
Thus we have
\begin{align}
    \label{eq:d_y_u}
    \frac{\d \|u(t) - Y\|_2^2}{\d t} = & ~ 2(u(t) - Y)^\top \big(G(t) (Y - u(t)) - \lambda \hat{\Phi} \hat{\Psi} \overline{W} \big) \notag \\
    = & ~ -2 (u(t) - Y)^\top G(t) (u(t) - Y) - 2\lambda (u(t) - Y)^\top\hat{\Phi} \hat{\Psi} \overline{W} \notag \\
    \leq & ~ - \lambda_0 \|u(t) - Y\|_2^2- 2\lambda (u(t) - Y)^\top\hat{\Phi} \hat{\Psi} \overline{W}.
\end{align}
As for the second term, we have
\begin{align}
    \label{eq:phi_psi_W}
    2\lambda (u(t) - Y)^\top\hat{\Phi} \hat{\Psi} \overline{W} = & ~ \frac{2\lambda}{m} (u(t) - Y)^\top\hat{\Phi}\cdot [\hat{\psi}_1\cdot w_1, \cdots, \hat{\psi}_m\cdot w_m]^\top \notag \\
    = & ~ \frac{2\lambda}{m} (u(t) - Y)^\top\hat{\Phi}\cdot [\sum_{i=1}^n x_i \phi(w_1^\top x_i), \cdots, \sum_{i=1}^n x_i \phi(w_m^\top x_i)]^\top \notag \\
    = & ~ \frac{2\lambda}{m} (u(t) - Y)^\top \cdot [U_1(t),\cdots, U_n(t)]^\top
\end{align}
where for $j\in [n]$, $U_j(t)\in\R$ can be expressed as
\begin{align*}
    U_j(t) = & ~ \frac{1}{\sqrt{m}}\sum_{r=1}^m \big(a_r \mathbf{1}_{\langle w_r, x_j \rangle \geq 0}x_j^\top \cdot \sum_{i=1}^n x_i \phi(w_r^\top x_i)\big) \\
    = & ~ \frac{1}{\sqrt{m}}\sum_{r=1}^m \sum_{i=1}^n a_r x_j^\top (x_i x_i^\top) w_r \cdot \mathbf{1}_{\langle w_r, x_i \rangle \geq 0, \langle w_r, x_j \rangle \geq 0} \\
    = & ~ \frac{1}{\sqrt{m}}\sum_{r=1}^m \Big(a_r x_j^\top w_r \cdot \mathbf{1}_{\langle w_r, x_j \rangle \geq 0}\cdot \sum_{i=1}^n \mathbf{1}_{\langle w_r, x_i \rangle \geq 0}\Big). %
\end{align*}
We denote $U(t) = [U_1(t),\cdots, U_n(t)]^\top\in\R^n$ and have
\begin{align}
    \label{eq:U}
    2\lambda (u(t) - Y)^\top\hat{\Phi} \hat{\Psi} \overline{W} = \frac{2\lambda}{m} (u(t) - Y)^\top \cdot U(t)
\end{align}
and our dynamics becomes
\begin{align}
    \label{eq:new_ode}
    \frac{\d \|u(t) - Y \|_2^2}{\d t} \leq & ~ -\lambda_0 \|u(t) - Y\|_2^2 - \frac{2\lambda}{m} (u(t) - Y)^\top \cdot U(t) \notag \\
    \leq & ~ -(\lambda_0 + \frac{2\lambda}{m}) \|u(t) - Y\|_2^2
\end{align}
showing that $\frac{\d}{\d t}\big(e^{(\lambda_0+2\lambda/m) t}\|u(t) - Y\|_2^2 \big) \leq 0$. Thus $e^{(\lambda_0+2\lambda/m) t}\|u(t) - Y\|_2^2$ is a decreasing function with respect to $t$, and we have
\begin{align*}
    \|u(t) - Y\|_2^2 \leq e^{-(\lambda_0+2\lambda/m) t} \|u(0) - Y\|_2^2.
\end{align*}
As for bounding $\|w_r(t) - w_r(0)\|_2$, we use the same method as in Lemma 3.3 of \cite{dzps19}. Thus we complete the proof.
\end{proof}

Finally, by combining Lemma~\ref{lem:dzps3.1}, \ref{lem:dzps3.2}, \ref{lem:gradient_flow} and \ref{lem:gradient_flow_2}, we have the following convergence result.
\begin{theorem}[Convergence of gradient flow]\label{thm:convergence_1}
Suppose $\lambda_0>0$, $m = \poly(n, 1/ \lambda_0, 1/ \delta)$, then with probability at least $1-\delta$ over the randomness of initialization, we have
\begin{align*}
    \|u(t) - Y\|_2^2 \leq e^{-(\lambda_0 + 2\lambda/m)t} \|u(0) - Y\|_2^2.
\end{align*}
\end{theorem}
The above theorem shows that in the over-parameterized setting (when $m$ is large enough), the training loss of the kernel ridge regression problem define in Eq.~\eqref{eq:sc1} converges to $0$ in a linear rate. By comparing our Theorem~\ref{thm:convergence_1} with Theorem 3.2 in \cite{dzps19}, we can find that the introducing of regularization term makes the convergence speed faster, though the improvement is limited. Further notice that in Section~\ref{sec:dropout_KRR} we prove the equivalence between minimizing the dropout loss and the kernel ridge regression problem. So we conclude our results as:
\begin{center}
    \emph{The introducing of sparsity into neural network makes the convergence speed faster, but the improvement is limited due to the over-parameterized scheme.}
\end{center}

