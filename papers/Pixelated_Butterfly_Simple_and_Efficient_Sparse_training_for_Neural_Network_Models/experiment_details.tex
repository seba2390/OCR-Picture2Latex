\section{Experiment Details}
\label{sec:experiment_details}
\subsection{Datasets}
\begin{itemize}
    \item \textbf{Cifar10} \citep{krizhevsky2009learning} consists of 60000 coloured images of resolution 32 $\times$ 32. Each of them belong to one of 10 classes, including airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Among these, 50000 images are allocated to be the training set and 10000 images the testing set.
    \item \textbf{Cifar100} \citep{krizhevsky2009learning} is similar to Cifar10. It also consists of images of resolution 32 $\times$ 32. In total, there are 60000 images, each of which belongs to one of 100 classes. Each of the 100 classes has 500 images in training set and 100 images in testing set.
    \item \textbf{ImageNet1K} \citep{russakovsky2015imagenet} spans 1000 object classes, containing 1,281,167 training images, 50,000 validation images and 100,000 test images. Although images are collected in different resolutions, in practice they are generally reshaped and cropped into 224 $\times$ 224.
    \item \textbf{WikiText-103} \citep{merity2016pointer} contains articles from the wikipedia page. It extracts verified articles from Wikipedia, which add up to over 100 million tokens. Compared to other datasets, such as Penn Treebank (PTB) \citep{taylor2003penn}, WikiText features a larger vocabulary and preserves original upper/lower cases, punctuation and numbers.
    
    
\end{itemize}
\subsection{Model Configurations and Hyperparameter}



We summarize the details required to replicate our experiments below.

\textbf{Baseline Model:} Except for dense model. We choose our baselines for each experiment base on the following. RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models (Mixer). BigBird focuses on attention matrices, so we used it as a baseline in Transformer-based models (ViT, GPT-2).

\subsubsection{Image Classification}
\begin{table}[!htbp]
    
\centering
\caption{Configuration of the Cifar10 experiments.}
\resizebox{0.9\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{Optimizer}&\multicolumn{1}{c}{Weight Decay}&\multicolumn{1}{c}{Learning Rate}&\multicolumn{1}{c}{Drop Path}&\multicolumn{1}{c}{Warmup/Epoch}\\
\midrule
ViT-Small& AdamW & 0.05 & 0.0005 & 0.1& 5/300 \\
Pixelfly-ViT-Small& AdamW & 0.05 & 0.0005 &0& 5/300 \\
ViT-Base& AdamW & 0.05 & 0.0005 &0.1& 5/300 \\
Pixelfly-ViT-Base& AdamW & 0.05 & 0.0005 &0& 5/300 \\
\midrule
Mixer-Small &AdamW& 0.1 &0.0005&0.1& 5/300 \\
Pixelfly-Mixer-Small &AdamW&0.1 &0.0005& 0 & 5/300 \\
Mixer-Base &AdamW& 0.1 &0.0005&0.1& 5/300 \\
Pixelfly-Mixer-Base &AdamW &0.1 &0.0005& 0 & 5/300 \\
\bottomrule
\end{tabular}}
\label{table:}
\end{table}

\begin{table}[!htbp]
    
\centering
\resizebox{0.9\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{Optimizer}&\multicolumn{1}{c}{Weight Decay}&\multicolumn{1}{c}{Learning Rate}&\multicolumn{1}{c}{Drop Path}&\multicolumn{1}{c}{Warmup/Epoch}\\
\midrule
ViT-Small& AdamW & 0.05 & 0.0005 & 0.1& 5/300 \\
Pixelfly-ViT-Small& AdamW & 0.05 & 0.0005 &0& 5/300 \\
ViT-Base& AdamW & 0.05 & 0.0005 &0.1& 5/300 \\
Pixelfly-ViT-Base& AdamW & 0.05 & 0.0005 &0& 5/300 \\
\midrule
Mixer-Small &AdamW& 0.1 &0.0005&0.1& 5/300 \\
Pixelfly-Mixer-Small &AdamW&0.1 &0.0005& 0 & 5/300 \\
Mixer-Base &AdamW& 0.1 &0.0005&0.1& 5/300 \\
Pixelfly-Mixer-Base &AdamW &0.1 &0.0005& 0 & 5/300 \\
\bottomrule
\end{tabular}
}
\caption{Configuration of the Cifar100 experiments}
\label{table:}
\end{table}

\begin{table}[!htbp]
    
\centering
\resizebox{0.9\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{Optimizer}&\multicolumn{1}{c}{Weight Decay}&\multicolumn{1}{c}{Learning Rate}&\multicolumn{1}{c}{Drop Path}&\multicolumn{1}{c}{Warmup/Epoch}\\
\midrule
ViT-Small& AdamW & 0.05 & 0.001 & 0.1& 5/300 \\
Pixelfly-ViT-Small& AdamW & 0.05 & 0.001 &0& 5/300 \\
ViT-Base& AdamW & 0.05 & 0.001 &0.1& 5/300 \\
Pixelfly-ViT-Base& AdamW & 0.05 & 0.001 &0& 5/300 \\
\midrule
Mixer-Small &AdamW& 0.1 &0.001&0.1& 5/300 \\
Pixelfly-Mixer-Small &AdamW&0.1 &0.001& 0 & 5/300 \\
Mixer-Base &AdamW& 0.1 &0.001&0.1& 5/300 \\
Pixelfly-Mixer-Base &AdamW &0.1 &0.001& 0 & 5/300 \\
\bottomrule
\end{tabular}
}
\caption{Configuration of the ImageNet experiment}
\label{table:}
\end{table}

We report more details on the models, including number of parameters and FLOPs, in \cref{table:flops_imagenet}.

We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular, ViT-S and ViT-B refers to the small and base ViT models respectively, and 16 refers to the patch size of 16x16. The MLP-Mixer models follows the same convention.

\begin{table}[h]
\centering
\caption{\label{table:flops_imagenet}The performance of Pixelfly and ViT or MLP-Mixer on the ImageNet benchmarks, including the number of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet) compared to the dense model.}
\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{ImageNet top-1 acc.}&\multicolumn{1}{c}{Speedup} &\multicolumn{1}{c}{Params} & \multicolumn{1}{c}{FLOPs} \\
\midrule
Mixer-S/16& 72.4& - & 18.5M & 3.8G \\
Pixelfly-Mixer-S/16& 72.6& 1.7$\times$ & 5.9M & 1.3G \\
Mixer-B/16& 75.6& - & 59.9M & 12.6G \\
Pixelfly-Mixer-B/16& 76.3& 2.3$\times$ & 17.4M & 4.3G \\
\midrule
ViT-S/16& 77.7 & - & 48.8M & 9.9G \\
Pixelfly-ViT-S/16& 77.5 & 1.9$\times$ & 16.9M & 3.6G \\
ViT-B/16& 78.5 & - & 86.6M  & 17.6G \\
Pixelfly-ViT-B/16& 78.6 & 2.0$\times$ & 28.2M & 6.1G \\
\bottomrule
\end{tabular}
\end{table}



\subsubsection{Language Modeling}
We report more details on the models, including number of parameters and FLOPs, in \cref{table:gpt_flop} and \cref{table:wt103}.

\begin{table}[!h]
\caption{The performance of Pixelfly, BigBird and GPT-2-Small on WikiText-103, including the number of parameters and FLOPs. We measure the perplexity and the training speed up.}
  \centering
      	\resizebox{0.8\linewidth}{!}{
\centering
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{c||c|c|c|c}
\specialrule{.15em}{.05em}{.05em}
\multirow{1}{*}{{\bf Model} } & \multicolumn{1}{c|}{\multirow{1}{*}{WikiText-103 (ppl)}}
                              & \multicolumn{1}{c|}{\multirow{1}{*}{Speedup}}
                              & \multicolumn{1}{c|}{\multirow{1}{*}{Params}}
                              & \multicolumn{1}{c}{\multirow{1}{*}{FLOPS}}\\
\hline
GPT-2-Small &  22.2 & - & 117M& 48.4G\\
\cline{1-5}
BigBird & 23.3  & 0.96$\times$ & 117M& 40.2G\\
\cline{1-5}
Pixelfly& 22.5  & 2.1$\times$ &68M & 18.5G\\
\midrule
\hline
GPT-2-Medium &  20.9 & - & 345 M& 168G\\
\cline{1-5}
BigBird & 21.5  & 1.1$\times$ & 345 M& 134G\\
\cline{1-5}
Pixelfly& 21.0  & 2.5$\times$ &203M & 27G\\
\specialrule{.15em}{.05em}{.05em}
\end{tabular}
}
\label{table:gpt_flop}
\end{table}

\begin{table}[!h]
\centering
\caption{Configuration of the WikiText103 experiments}
\resizebox{0.9\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Model&\multicolumn{1}{c}{Optimizer}&\multicolumn{1}{c}{Weight Decay}&\multicolumn{1}{c}{Learning Rate}&\multicolumn{1}{c}{Dropout}&\multicolumn{1}{c}{Warmup/Epoch}\\
\midrule
GPT-2-Small& Adam & 0.1 & 0.0001 & 0.1& 5/100 \\
Pixelfly& Adam & 0.1 & 0.0001 & 0.1 & 5/100 \\
\bottomrule
\end{tabular}
}
\label{table:wt103}
\end{table}
\pagebreak

    

\subsection{Measuring Empirical NTK}
\label{app:ntk_exp}
The Empirical NTK is a rough estimation of the real NTK, in which the width of the neural net goes to infinity. As the width grows, the kernel gets closer to its infinite-width limit. Fortunately, both our models of interest, MLP-Mixer and Vision Transformer, are wide and overly parameterized. Therefore they are only one step away from the real NTK domain. This allows us to use the Empirical NTK to approximately predict their training behaviors.

As described in equation \ref{eq:empirical_ntk}, we first compute the gradient of each data sample, then we compute pair-wise product to construct the Empirical NTK. Although we use a relatively small dataset, it's still expensive to build a kernel for large models, such as ViTs and MLP-Mixers. In practice, we find that it's sufficient to compute kernels for a subsampled dataset.

MLP-Mixer and Vision Transformer each represent one type of module of interest for our sparsification. In MLP-Mixer, we study the sparse behavior of the Linear module, whereas, in Vision Transformer, we mainly focus on sparsifying attention. All models are first sparsified to around $10 \%$ of the original dense compute. Then we compare their NTK kernels with their original dense kernel. We run three random seeds to eliminate noise, i.e., three different initializations for each pair of configurations. We report the mean relative difference between the kernels with respect to the norm of the dense kernel.

\subsection{Transfer Learning Experiments}
\label{app:throughput}
We conduct extended experiments to test the generalization of our pretrained sparse models on downstream tasks. Specifically, we finetune Pixelfly pretrained model (ImageNet) on CIFAR-10 and show that it get 99.03\% accuracy compared to 98.77\% on our pretrained dense ViT-B/16 model. In addition, we see more than $2\times$ speed up on downstream task fine-tuning process as well.



\subsection{Microbenchmarking}
\label{app:study}
In this section, we perform microbenchmarking on a  4K$\times$ 4K sparse matrix multiplication. We aim to show that Pixelfly patterns are far more hardware friendly than random patterns. For a 4K$\times$4K matrix, expected density is the number of non-zero entries/(4K$\times$4K) ; actual density is the number of accessed entries/(4K$\times$4K), e.g. even if there is only one non-zero, 32$\times$32 entries would be accessed because the hardware block size is 32$\times$32. 

When random patterns are generated with small block size, (e.g $1\times1$, $2\times2$), the resources, such as memory access and computes(denoted by Actual Density), required  to compute a random sparse matrix of density $1.25\%$ are equivalent to computing a dense matrix multiplication. This is further reflected in the latency: As pattern block size shrinks, deviating from the hardware block size of $32\times 32$, the random patterns' latency worsens, whereas the Pixelfly remains efficient.
Vanilla Butterfly is 5$\times$ slower than Pixelfly as expected,  because (1) it does not take advantage of the hardware property -- not structured sparsity(2) it is a series of products. 
\begin{table}[!htbp]
\centering
\resizebox{0.7\linewidth}{!}{
\noindent\begin{tabular}{@{}lccccccc@{}}
Pattern&\multicolumn{1}{c}{Block size}&\multicolumn{1}{c}{Expected Density}&\multicolumn{1}{c}{Actual Density}&\multicolumn{1}{c}{Latency(ms)}\\
\midrule
 & 1$\times$1 & 1.25\% & 100\% & 9.4 \\
 & 2$\times$2& 2.5\% & 99.84\% & 9.3 \\
 & 4$\times$4 & 5\% & 96.24\% & 9.04 \\
Random & 6$\times$6 & 10\% & 93.66\% & 8.8\\
 & 8$\times$8 & 20\%& 81.89\% & 7.7 \\
 & 16$\times$16 & 40\% &34.52\%& 3.3 \\
 & 32$\times$32 & 80\%& 10.15\% & 1.0 \\
\midrule
Butterfly & 1$\times$1 & 10\% & 62.50\% & 5.2 \\
\bottomrule
 & 1$\times$1 & 1.25\% & 4.62\% & 0.48 \\
 & 2$\times$2& 2.5\% & 5.38\% & 0.56 \\
 & 4$\times$4 & 5\% & 6.13\% & 0.63 \\
Pixelfly & 6$\times$6 & 10\% & 9.64\% & 0.96\\
 & 8$\times$8 & 10\%& 10.58\% & 1.05 \\
 & 16$\times$16 & 10\% &11.30\%& 1.12 \\
 & 32$\times$32 & 10\%& 10.58\% & 1.04 \\
 \bottomrule
\end{tabular}
}
\caption{Microbenchmarking of different patterns. Given GPU processes the matrix block by block of size 32 $\times$ 32, random block pattern's latency increases as the block size shrinks, while Pixelfly remains efficient. We measure the latency by averaging 100 runs of batch size 4096 for each configuration.}
\label{table:throughput}
\end{table}

\subsection{Efficient Implementation of Pixelfly}
\label{subsec:efficient_implementation}

We run all of our experiments on V100 GPUs.
We rely on efficient implementation of block sparse matrix multiply and block
sparse attention from the libraries Triton
(\url{https://github.com/openai/triton}) and
\url{https://github.com/huggingface/pytorch_block_sparse}.
For the low-rank part, we rely on efficient (dense) matrix multiplies from cuBLAS.
In particular, to multiply the input $x$ by the low-rank matrix $U V^\top$, we
multiply $U (V^\top x)$.

We keep the same number of training epochs as that of the dense models (e.g.,
on ImageNet, the dense model and the Pixelfly model are trained for 300 epochs).
The training speedup of the Pixelfly models is due to faster time per
epoch.

We do not use 2:4 sparsity (available on Ampere GPUs such as A100).
Such fine-grained sparsity is orthogonal to our approach, and we expect that
future work incorporating both 2:4 sparsity and block sparsity to yield further speedup.

\subsection{Ablation: Speed-Accuracy Tradeoff of Pixelfly}
\label{subsec:speed_accuracy_tradeoff}

We conduct an ablation experiment to examine the speed-accuracy trade of
Pixelfly: on the ImageNet dataset and the Mixer-B/16 model, we replace the dense
matrices with flat block butterfly + low-rank matrices, while varying the
compute / parameter budget.
We plot the speed-accuracy tradeoff in \cref{fig:speed_accuracy_tradeoff}.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/speed_acc_tradeoff_imagenet.pdf}
  \caption{\label{fig:speed_accuracy_tradeoff}Speed-accuracy tradeoff of Pixelfly on
    ImageNet classification, with Mixer-B/16 as the dense model. Pixelfly
    maintains or exceeds the accuracy of the dense model, up to around
    2.3$\times$ speedup (or around 30\% of the number of parameters).
    Performance degrades when the Pixelfly model has fewer than 30\% of the number
    of parameters.}
\end{figure}

\subsection{Comparison Against Original Butterfly}
\label{subsec:original_butterfly}

We compare Pixelfly against original Butterfly
matrices~\citep{dao2020kaleidoscope} on the ImageNet dataset and Mixer-B/16
dense model.
We present the results in \cref{table:original_butterfly}.
We notice another benefit of Pixelfly compared to Butterfly: it trains more stably and requires less careful initialization. Since Butterfly is a product of many factors, it requires careful initialization, otherwise the activation and gradient will be very large or very small.

\begin{table}[h]
  \centering
  \caption{\label{table:original_butterfly}The performance of Pixelfly and
    original Butterfly on MLP-Mixer on the ImageNet benchmarks.}
  \begin{tabular}{@{}lccccccc@{}}
    Model&\multicolumn{1}{c}{ImageNet top-1 acc.}&\multicolumn{1}{c}{Speedup} &\multicolumn{1}{c}{Params} & \multicolumn{1}{c}{FLOPs} \\
    \midrule
    Mixer-B/16& 75.6& - & 59.9M & 12.6G \\
    Butterfly-Mixer-B/16& 76.1& 0.8$\times$ & 17.4M & 4.3G \\
    Pixelfly-Mixer-B/16& 76.3& 2.3$\times$ & 17.4M & 4.3G \\
    \bottomrule
  \end{tabular}
\end{table}


