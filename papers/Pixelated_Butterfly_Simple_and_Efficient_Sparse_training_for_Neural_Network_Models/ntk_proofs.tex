\section{Neural Tangent Kernel, Convergence, and Generalization}
\label{sec:appx_ntk}

Our analysis relies on the neural tangent kernel (NTK)~\citep{jacot2018neural} of the network.
\begin{definition}
  Let $f(\cdot, \theta) \colon \mathbb{R}^{d} \to \mathbb{R}$ be the function specified by a neural network with parameters $\theta \in \mathbb{R}^p$ and input dimension $d$.
  The parameter $\theta$ is initialized randomly from a distribution $P$.
  Then its neural tangent kernel (NTK) \citep{jacot2018neural} is a kernel $K \colon \mathbb{R}^{d} \times \mathbb{R}^{d} \to \mathbb{R}$ defined by:
  \begin{equation*}\label{eq:kernel}
    K(x, y) = \E_{\theta \sim P} \left[ \left\langle \frac{\partial f(x; \theta)}{\partial \theta}, \frac{\partial f(y; \theta) }{\partial \theta} \right\rangle\right].
  \end{equation*}
\end{definition}

We can relate the training and generalization behavior of dense and sparse
models through their NTK.
The standard result~\citep{sy19} implies the following.
\begin{proposition}
  \label{thm:ntk}
  Let $f_\mathrm{dense}$ denote a ReLU neural network with $L$ layers with dense weight matrices $\theta_\mathrm{dense}$ with NTK $K_\mathrm{dense}$, and let $f_\mathrm{sparse}$ be the ReLU neural network with the same architecture and with weight matrices $\theta_\mathrm{sparse}$ whose rows are $k$-sparse, and with NTK $K_\mathrm{sparse}$.
  Let $x_1, \dots, x_N$ be the inputs sampled from some distribution $P_X$.
  Suppose that the empirical NTK matrices $K_d = K_\mathrm{dense}(x_i, x_j)$ and $K_s = K_\mathrm{sparse}(x_i, x_j)$ for $(i, j) \in [N] \times [N]$ satisfy $\| K_d - K_s \| \leq \delta$.

  {\bf Training.}
  We knew the the number of iterations of dense network is $\lambda_{\min}(K_d)^{-2} n^2 \log(1/\epsilon)$ to reach the $\epsilon$ training loss. For sparse network we need $(\lambda_{\min}(K_d) -\delta)^{-2} n^2 \log(1/\epsilon)$.

  {\bf Generalization.}
  We knew the the number of iterations of dense network is $\lambda_{\min}(K_d)^{-2} n^2 \log(1/\epsilon)$ to reach the generalization error $\epsilon$ training loss. For sparse network we need $(\lambda_{\min}(K_d) -\delta)^{-2} n^2 \log(1/\epsilon)$.
\end{proposition}
These results relate the generalization bound of sparse models to that of dense models.