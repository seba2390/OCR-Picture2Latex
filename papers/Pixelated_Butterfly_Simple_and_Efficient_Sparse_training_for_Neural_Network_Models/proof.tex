
\paragraph{Roadmap}
The analysis of sparse networks is organized as follows. In Section~\ref{sec:notations} we list some basic notations that will be used. In Section~\ref{sec:binary_mask_analysis} we consider the problem of adding sparsity on $W$, and we achieve polynomial solving time. In Section~\ref{sec:gradient_compute} we prove that the gradient descent can be done fast under the sparsity assumption. In Section~\ref{sec:dropout_KRR} we consider the problem of adding sparsity on $a$, and we show that minimizing the dropout loss is equivalent with a kernel ridge regression problem. In Section~\ref{sec:gradient_flow} we analyze the dynamics of gradient flow and prove the convergence result.

\section{Notations}\label{sec:notations}

For a vector $x$, we use $\|x\|_p$ to denote its $\ell_p$ norm, and we mainly consider $p = 1, 2$ in this paper. For a matrix $A$, we use $\| A \|_0, \| A \|_1, \| A \|_F$ to denote the $\ell_0$ norm, entry-wise $\ell_1$ norm and Frobenius norm of $A$ respectively. For two matrices $A, B\in\R^{d\times m}$, we use $A\circ B$ to denote their Hadamard product. We use ${\cal T}_{\mathrm{mat}}(n,d,m)$ to denote the time of multiplying $n \times d$ matrix with another $d \times m$ matrix. For a symmetric matrix $A$, we use $\lambda_{\min}(A)$ to denote its minimum eigenvalue. We also let $\mathrm{vec}(A)$ be the vectorization of a matrix $A$ in column first order. We use $\langle \cdot, \cdot \rangle$ to denote standard Euclidean inner product between two vectors.

Moreover, we use $\mathcal{N}(\mu, \Sigma)$ to denote the Gaussian distribution with mean $\mu$ and covariance $\Sigma$. We denote the ReLU function by $\phi(z) = \max \{z, 0\}$. For an event $E$, we use $\mathbf{1} \{E\}$ or $\mathbf{1}_E$ to denote its indicator function.


\section{Sparsity on hidden layer weights}
\label{sec:binary_mask_analysis}
\subsection{Applying masks before multiplication}

Given matrix $A \in \R^{n \times d}$, $B \in \R^{d \times n}$, naively computing $AB$ takes ${\cal T}_{\mathrm{mat}}(n,d,n)$. Note that, we can also consider the case where $A$ and $B$ have different size. For simplicity, let us consider the case where matrix $A$ and matrix $B^\top$ have the same size.

Our goal is to find ``optimal" binary mask matrix $W \in \{0, 1\}^{d \times n}$ such that,
\begin{align*}
     \min_{W} & ~ \| f(A \cdot B) - f(A \cdot (W \circ B)) \|_1 \\
    \text{s.t.} & ~ \| W_{B,i} \|_0 = k , \forall i \in [n]
\end{align*}

\begin{remark}
In the practical applications we care about, the function $f$ is the activation function of neural network, e.g., $\mathrm{ReLU}(z) =\max\{z,0\}$.
\end{remark}



We define a sparse targeted regression problem:
\begin{definition}[Sparse mark regression, $\ell_1$ version]
Given a matrix $B \in \R^{d \times n}$, and a vector $a \in \R^d$, the goal is to find a $k$-sparse binary vector $w \in \{0,1\}^d$ to minimize the following problem:
\begin{align*}
    \min_{w} \| a^\top\cdot B - (a^\top \circ w^\top) \cdot B \|_1.
\end{align*}

\end{definition}

Naively, the above problem can be solved in $n \cdot d^{O(k)}$ via guess all the ${d \choose k}$ choices. 
\begin{lemma}\label{lem:mask_regreesion_vector}
The targeted sparse mask regression problem can be solved in $n \cdot d^{O(k)}$.
\end{lemma}
\begin{proof}
We need to guess ${d \choose k}$ times, which becomes $d^{O(k)}$. Each time it takes $n d$ operations, thus the total time is 
\begin{align*}
    nd \cdot d^{O(k)} = n \cdot d^{O(k)}.
\end{align*}
\end{proof}


\begin{definition}[$\ell_1$ version]\label{def:sparse_mask_factorization_before}
Given three positive integers $m \geq n \geq d \geq k \geq 1$, matrices $A \in \R^{m \times d}$ and $B \in \R^{d\times n}$. We define our  problem as finding the binary matrix $W\in \{0, 1\}^{m \times d}$ that satisfies 
\begin{align*}
     \min_{W} & ~ \| A \cdot B - (A \circ W) \cdot B \|_1 \\
    \mathrm{s.t.} & ~ \| W_{i*} \|_0 = k , \forall i \in [m].
\end{align*}
where $W_{i*}$ is the $i$-th row of $W$.
\end{definition}

\begin{theorem}\label{thm:mask_regression}
The problem being defined as Definition~\ref{def:sparse_mask_factorization_before} can be solved in $m n d^{O(k)}$ time.
\end{theorem}

\begin{proof}
Our problem can be decomposed into $m$ sub-problems as follows:

\begin{align*}
    \| A\cdot B - (A\circ W) \cdot B \|_1 & =  \sum_{i=1}^m \big\|(A\cdot B)_{i*} - ((A\circ W) \cdot B)_{i*}  \big\|_1 \\
    & = \sum_{i=1}^m\big\|A_{i*}\cdot B - (A\circ W)_{i*} \cdot B \big\|_1 \\
    & = \sum_{i=1}^m\big\|A_{i*}\cdot B - (A_{i*}\circ W_{i*}) \cdot B  \big\|_1
\end{align*}
where $A_{i*}$ means the $i$-th row of matrix $A$. By applying Lemma~\ref{lem:mask_regreesion_vector}, each sub-problem
\begin{align*}
    \min_{W_{i*}} \| A_{i*}\cdot B - (A_{i*}\circ W_{i*}) \cdot B \|_1
\end{align*}
can be solved in $n \cdot d^{O(k)}$ time. Then the problem defined in Definition~\ref{def:sparse_mask_factorization_before} can be solved in 
\begin{align*}
    m \cdot n d^{O(k)} = m n d^{O(k)}
\end{align*}
time in total. Thus we finish the proof.
\end{proof}


In the above Theorem, we show that solving the sparse mask regression problem is NP-hard. However, if we add some mild assumptions and consider minimizing $\ell_1$ norm, then we can solve the regression problem in polynomial time, as the following parts show. 

\begin{definition}[$\ell_1$ version]
Given a matrix $B\in\R_{\geq 0}^{d\times n}$, and a vector $a\in\R_{\geq 0}^d$, the goal is to find a $k$-sparse binary vector $w\in\{0,1\}^d$ to solve
\begin{align*}
    \min_{w} \|a^\top \cdot B -  (a^\top \circ w^\top) \cdot B\|_1
\end{align*}
\end{definition}

\begin{lemma}\label{lem:mask_regreesion_vector_ell_1}
The targeted $\ell_1$ version sparse mask regression problem can be solved in 
\begin{align*}
    O(nd + n\log n)
\end{align*}
which is polynomial time.
\end{lemma}
\begin{proof}
We first consider the situation when $a\in\{0,1\}^d$. In this case, we have
\begin{align*}
    \|a^\top \cdot B -  (a^\top \circ w^\top) \cdot B\|_1 + \|(a^\top \circ w^\top) \cdot B\|_1 = \|a^\top \cdot B\|_1
\end{align*}
where $\|a^\top \cdot B\|_1$ is fixed. So we only need to consider the following problem:
\begin{align*}
    \max_{w} \|(a^\top \circ w^\top) \cdot B\|_1.
\end{align*}
For simplicity we assume $a_i = 1, \forall i \in [d]$, and we only need to solve
\begin{align*}
    \max_{w} \|w^\top \cdot B\|_1
\end{align*}
where $w$ has $k$ elements equal to $1$ and $d-k$ elements equal to $0$. For $i\in[d]$, we compute $S_i = \sum_{j=1}^n B_{ij}$ which is the summation of $i$-th row of $B$, and sort them as $S_{(1)} \geq S_{(2)}\geq \cdots \geq S_{(n)}$. Then we only need to let $w_{(i)} = 1$ for $i\in [k]$ and other elements equal to $0$. Computing all $S_i$ takes $O(nd)$ time, sorting $S_i$ takes $O(n\log n)$ time, thus the total time consumption is $O(nd + n\log n)$ in this case.

Next, we consider the general case when $a\in\R_{\geq 0}^d$. We let
\begin{align*}
    \overline{B}_{i*} = a_i B_{i*} ~~~ and ~~~ \overline{a}_i = 
    \begin{cases}
    1, & a_i > 0 \\
    0, & a_i = 0
    \end{cases},
    ~~~ \forall i \in [d]
\end{align*}
where $B_{i*}$ is the $i$-th row of $B$. Then our optimization problem is equivalent to
\begin{align*}
    \min_{w}\|\overline{a}^\top \cdot \overline{B} - (\overline{a}^\top \circ w^\top) \cdot \overline{B}\|_1
\end{align*}
where $\overline{B}\in\R_{\geq 0}^{d\times n}$ and $\overline{a} \in \{0,1\}^d$. Thus we turn this case into the first case. Constructing $\overline{B}$ and $\overline{a}$ takes $O(nd)$ time, thus the total time consumption is also $O(nd +n\log n)$ in this case.
\end{proof}

\begin{definition}[$\ell_1$ version]\label{def:sparse_mask_ell_1}
Given three positive integers $m \geq n \geq d \geq k \geq 1$, matrices $A \in \R_{\geq 0}^{m \times d}$ and $B \in \R_{\geq 0}^{d\times n}$. We define our  problem as finding the binary matrix $W\in \{0, 1\}^{m \times d}$ that satisfies 
\begin{align*}
     \min_{W} & ~ \| A \cdot B - (A \circ W) \cdot B \|_1 \\
    \mathrm{s.t.} & ~ \| W_{i*} \|_0 = k , \forall i \in [m].
\end{align*}
where $W_{i*}$ is the $i$-th row of $W$.
\end{definition}

\begin{theorem}\label{thm:mask_regression}
The problem being defined as Definition~\ref{def:sparse_mask_ell_1} can be solved in
\begin{align*}
    O(mnd + mn\log n)
\end{align*}
time.
\end{theorem}

\begin{proof}
Our problem can be decomposed into $m$ sub-problems as follows:
\begin{align*}
    \| A\cdot B - (A\circ W) \cdot B \|_1 & =  \sum_{i=1}^m \big\|(A\cdot B)_{i*} - ((A\circ W) \cdot B)_{i*}  \big\|_1 \\
    & = \sum_{i=1}^m\big\|A_{i*}\cdot B - (A\circ W)_{i*} \cdot B \big\|_1 \\
    & = \sum_{i=1}^m\big\|A_{i*}\cdot B - (A_{i*}\circ W_{i*}) \cdot B  \big\|_1
\end{align*}
where $A_{i*}$ means the $i$-th row of matrix $A$. By applying Lemma~\ref{lem:mask_regreesion_vector_ell_1}, each sub-problem
\begin{align*}
    \min_{W_{i*}} \| A_{i*}\cdot B - (A_{i*}\circ W_{i*}) \cdot B \|_1
\end{align*}
can be solved in $O(nd + n\log n)$ time. Then the problem defined in Definition~\ref{def:sparse_mask_ell_1} can be solved in
\begin{align*}
    m \cdot O(nd + n\log n) = O(mnd + mn\log n)
\end{align*}
time in total. Thus we finish the proof.
\end{proof}


\subsection{Applying Masks After Multiplication}

\begin{definition}\label{def:sparse_mask_factorization_after}
Given matrix $B \in \R^{d \times n}$, $C \in \R^{m \times n}$.  The goal is to find a mask $W \in \{0,1\}^{m \times d}$ where each column of $W$ is $k$-sparse 
\begin{align*}
  \min_{W \in \{0,1\}^{m \times d} } \| C \cdot B^\top  - ( C \cdot B^\top ) \circ W \|_1
\end{align*}
\end{definition}

\begin{remark}
The $B$ defined in Definition~\ref{def:sparse_mask_factorization_before} is the same as the $B$ defined in Definition~\ref{def:sparse_mask_factorization_after}. $B$ is corresponding to the $X$ in the neural network setting. %
\end{remark}








\section{Gradient computation}\label{sec:gradient_compute}


In this section we consider a neural network with one hidden layer and $m$ neurons in this hidden layer. Suppose $x\in\R^{d}$ is the input, $W = (w_1, \cdots, w_m)\in \R^{d \times m}$ is the weight matrix of the first layer, $a \in \R^m$ is the output weight,  and $M \in \{0,1\}^{d\times m}$ is the mask matrix with each column having at most $k$ non-zero entries. The neural network $f: \R^d \rightarrow \R$ is defined as
\begin{align*}
f(x) = a^{\top} \phi \Big( ( M \circ W )^\top \cdot x  \Big).
\end{align*}
For simplicity, we only optimize $W$ and fix $a$. Consider the mean square loss
\begin{align*}
L(W) = \frac{1}{2} \sum_{i=1}^n ( f(x_i) - y_i )^2 = \frac{1}{2} \sum_{i=1}^n ( a^\top \phi((M\circ W)^\top \cdot x_i) - y_i )^2 .
\end{align*}
In the forward computation, for a batch of data points $x_1, \cdots, x_n \in \R^d$, let $X \in \R^{d \times n}$ denote the input data points matrix.
For convenience, we define
\begin{align*}
    \Delta W(t) = W(t+1) - W(t) = -\eta \frac{\partial L(W(t))}{\partial W(t)}
\end{align*}
where $\eta$ is the step size. We define function $g_t: \R^d \rightarrow \R^m$ as 
\begin{align*}
    g_t(x) = ( f(x) -y )\cdot \diag\{\phi'( (M \circ W(t))^\top\cdot  x)\} \cdot a
\end{align*}
and also denote $g_t(X) = (g_t(x_1), \cdots, g_t(x_n))\in\R^{m\times n}$.
\begin{lemma}\label{lem:Delta_W}
We can express $\Delta W(t)$ as
\begin{align*}
    \Delta W(t) = - \eta (X\cdot g^\top_t(X))\circ M,
\end{align*}
and each column of $\Delta W(t)$ has at most $k$ non-zero entries.
\end{lemma}
\begin{proof}
    From the definition, we know
    \begin{align*}
        \Delta W(t) = & ~ -\eta \frac{\partial L(W(t))}{\partial W(t)} \\
        = & ~ -\eta\Big(\sum_{i=1}^{n}(f(x_i) - y_i)  \underbrace{ \diag\{\phi'( (M \circ W(t))^\top\cdot x_i)\} }_{ m \times m }\underbrace{ a }_{m \times 1}  \underbrace{ x_i^{\top} }_{1 \times d}  \Big)^\top \circ \underbrace{M}_{d\times m} \\
        = & ~ -\eta (\sum_{i=1}^n g_t(x_i) \cdot x_i^\top )^\top \circ M\\
        = & ~ -\eta(\underbrace{X}_{d \times n} \cdot \underbrace{ g^\top_t(X) }_{n \times m} ) \circ \underbrace{M}_{d\times m}.
    \end{align*}
    Since each column of $M$ has at most $k$ non-zero entries, we easily know each column of $\Delta W(t)$ also has at most $k$ non-zero entries.
\end{proof}




\begin{lemma}\label{lem:forward_time}
Suppose that matrices $M \in \R^{d \times m}$, $W(t) \in \R^{d \times m}$ and $\Delta W(t) \in \R^{d \times m}$ are given and pre-computed, then we can compute $f_{t+1}(X)$ in
\begin{align*}
    O(mnk)
\end{align*}
time. (Here $f_{t+1}(X)$ is the evaluation of $f$ at $W(t+1)$.)
\end{lemma}
\begin{proof}
    The goal is to compute
    \begin{align*}
      f_{t+1}(X)  = a^\top \cdot \phi( ( \underbrace{ M }_{ d \times m } \circ \underbrace{W(t+1)}_{ d \times m } )^\top \cdot X ).
    \end{align*}
    By using Lemma~\ref{lem:Delta_W}, we have
    \begin{align*}
    (M\circ W(t+1))^\top\cdot X
    = & ~ (M \circ( W(t) + \Delta W(t) ) )^\top \cdot X \\
    = & ~ ( M \circ W(t) )^\top \cdot X + ( M \circ \Delta W(t)  )^\top \cdot X \\
    = & ~ ( M \circ W(t) )^\top \cdot X - \eta (M\circ (X\cdot g^\top_t(X) ) \circ M)^\top \cdot X \\
    = & ~ ( M \circ W(t) )^\top \cdot X - \eta ((X\cdot g^\top_t(X)) \circ M)^\top\cdot X \\
    = & ~ ( M \circ W(t) )^\top \cdot X + (\Delta W(t))^\top\cdot X.
    \end{align*}
    Notice that we have already computed $( M \circ W(t) )^\top \cdot X \in \R^{m \times d}$ from previous iteration, so we only need to compute $(\Delta W(t))^\top\cdot X$ where $\Delta W(t) \in \R^{d\times m}$ and $X\in\R^{d\times n}$. By using Lemma~\ref{lem:Delta_W}, each row of $(\Delta W(t))^\top$ has at most $k$ non-zero entries, thus we can compute $(\Delta W(t))^\top\cdot X$ in $O(mnk)$ time. 
\end{proof}


\begin{lemma}\label{lem:backward_time}
Suppose that matrices $M \in \R^{d \times m}, W(t) \in \R^{d \times m}$ and $f_t(X)$ are given and pre-computed, then we can compute $\frac{\partial L(W(t))}{ \partial W(t)}$ %
in $O(m n k)$ time. 
\end{lemma}
\begin{proof}
By using Lemma~\ref{lem:Delta_W}, we have
\begin{align*}
    \frac{\partial L(W(t))}{ \partial W(t)} = (X\cdot g^\top_t(X))\circ M
\end{align*}
where $g_t(x) = ( f(x) -y )\cdot \diag\{\phi'( (M \circ W(t))^\top\cdot  x)\} \cdot a \in \R^m$ and $g_t(X) = (g_t(x_1), \cdots, g_t(x_n)) \in \R^{m \times n}$. We first compute $M\circ W(t)$ in $O(m k)$ time, then we can construct $g_t(X)\in\R^{m\times n}$ in $n\cdot O(m k)$ time. Given $g_t(X)$, since we only need to compute $km$ entries of $X\cdot g_t^\top(X)$, where each entry can be computed in $O(n)$ time, thus we can compute $\frac{\partial L(W(t))}{ \partial W(t)}$ in $O(m n k)$ time.
\end{proof}


\begin{algorithm}\caption{The sparse training algorithm
}
\begin{algorithmic}[1]
\Procedure{Sparse Training}{$\{x_i, y_i\}_{i\in[n]}$}
    \State Initialization $a_r, w_r(0)\sim \mathcal{N}(0, I_d)$ for $r\in[m]$.
    \For{$t=1 \to T$}
        \State {/*forward computation*/}
        \State Compute $M\circ W(t)$ \Comment{Takes $O(m k)$ time.}
        \For{$i=1 \to n$}
            \State $f_t(x_i)\leftarrow a^\top \phi((M\circ W(t))^\top \cdot x_i)$ \Comment{Takes $O(m k)$ time.}
            \State $g_t(x_i) \leftarrow (f(x_i)-y_i)\cdot \diag{\phi'((M\circ W(t))^\top \cdot x_i)}\cdot a$ \Comment{Takes $O(m k)$ time.}
        \EndFor
        \State {/*backward computation*/}
        \State $g_t(X) \leftarrow (g_t(x_1),\cdots, g_t(x_n))$.
        \State $\frac{\partial L(W(t))}{ \partial W(t)} = (X\cdot g^\top_t(X))\circ M$ \Comment{Takes $O(m n k)$ time.}
        \State $W(t+1) = W(t) + \Delta W(t)$ \Comment{$\Delta W(t) = -\eta \frac{\partial L(W(t))}{ \partial W(t)}$.}
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}




