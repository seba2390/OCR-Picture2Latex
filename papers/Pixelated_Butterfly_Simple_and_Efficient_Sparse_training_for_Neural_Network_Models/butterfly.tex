\vspace{-0.1cm}
\section{Butterfly matrices and Pixelated Butterfly}
\label{sec:butterfly}



Butterfly matrices~\citep{parker1995random, dao2019learning} are expressive
and theoretically efficient.
As they contain the set of sparse matrices, we choose to search for the sparsity
pattern in this larger class due to their fixed sparsity structure.
However, there are three technical challenges.
We highlight them here along with our approaches to address them:
\begin{enumerate}[leftmargin=*,nosep,nolistsep]
  \item Slow speed: butterfly matrices are not friendly to modern hardware as their
  sparsity patterns are not block-aligned, thus are slow.
  We introduce a variant of butterfly matrices, \emph{block butterfly}, which operate at the block level, yielding
  a block-aligned sparsity pattern.
  \item Difficulty of parallelization: the sequential nature of butterfly matrices as products
  of many factors makes it hard to parallelize the multiplication.
  We propose another class of matrices, \emph{flat butterfly} matrices, that are
  the first-order approximation of butterfly with residual connections.
  Flat butterfly turns the product of factors into a sum, facilitating parallelization.
  \item Reduced expressiveness of flat butterfly: even though flat butterfly
  matrices can approximate butterfly matrices with residual connections, they are
  necessarily high-rank and cannot represent low-rank matrices~\citep{udell2019big}.
  We propose to add a low-rank matrix (that is also block-aligned) to flat
  butterfly to increase their expressiveness.
\end{enumerate}
Combining these three approaches (flat \& block butterfly + low-rank), our
proposal (Pixelated Butterfly) is a very simple method to train sparse networks.

\subsection{Block Butterfly Matrices}
\label{sec:block_butterfly}




We propose a block version of butterfly matrices, which is more
hardware-friendly than the regular butterfly.
The regular butterfly matrices~\citet{dao2019learning, dao2020kaleidoscope} will be a special case of block butterfly with
block size $b = 1$.
We omit $b$ in the notation if $b = 1$.
\begin{definition} \label{def:bfactor}
  A \textbf{block butterfly factor} (denoted as $\vB_{k, b}$) of size $kb$ (where $k \ge 2$) and block size $b$ is a matrix of the form
    \(
        \vB_{k, b} = \begin{bmatrix}
            \vD_1 & \vD_2 \\ \vD_3 & \vD_4
        \end{bmatrix}
    \)
    where each $\vD_i$ is a $\frac{k}{2} \times \frac{k}{2}$ block diagonal
    matrix of block size $b$ of the form
    $\mathrm{diag} \left( D_{i, 1}, \dots, D_{i, k/2} \right)$ where
    $D_{i, j} \in \mathbb{R}^{b \times b}$.
    We restrict $k$ to be a power of 2.
\end{definition}
\begin{definition}
  A \textbf{block butterfly factor matrix} (denoted as $\vB_{k}^{(n, b)}$) of size $nb$ with stride $k$ and
     block size $b$ is a block diagonal matrix
     of $\frac{n}{k}$ (possibly different) butterfly factors of size $kb$ and
     block size $b$:
     \[
        \vB_{k}^{(n, b)} = \mathrm{diag} \left( \left[ \vB_{k, b} \right]_1, \left[ \vB_{k, b} \right]_2, \hdots, \left[ \vB_{k, b} \right]_\frac{n}{k} \right)
     \]
\end{definition}

\begin{definition} \label{def:bmatrix}
    A \textbf{block butterfly matrix} of size $nb$ with block size $b$ (denoted as $\vB^{(n, b)}$) is a matrix that can be expressed as a product of butterfly factor matrices:
    \(
        \vB^{(n, b)} = \vB_n^{(n, b)} \vB_{\frac{n}{2}}^{(n, b)} \hdots \vB_2^{(n, b)}.
    \)
    Define $\B_b$ as the set of all matrices that can be expressed in the form $\vB^{(n, b)}$ (for some $n$).
\end{definition}

\begin{figure}
\vspace{-1.2cm}
	\begin{center}
		\begin{tabular}{c}
			\includegraphics[width=0.87\linewidth]{figs/flat_block_butterfly.pdf}
		\end{tabular}
	\end{center}
	\captionsetup{font=small}
		\vspace{-0.8cm}
	\caption{Visualization of Flat, Block, and Flat Block butterfly.}
	\label{fig:tradeoff}
	\vspace{-0.3cm}
\end{figure}

\subsection{Flat butterfly matrices}
\label{sec:flat_butterfly}
In most applications of butterfly matrices to neural networks, one multiplies
the $O(\log n)$ butterfly factors.
However, this operation is hard to be efficiently implemented on parallel hardware (e.g., GPUs) due to
the sequential nature of the operation\footnote{Even with a very specialized
  CUDA kernel, butterfly matrix multiply ($O(n \log n)$ complexity) is only
faster than dense matrix multiply ($O(n^2)$ complexity) for large values of $n$
(around 1024)~\citep{dao2019learning}.}.
We instead propose to use a sum of butterfly factors that can approximate the
products of the factors.
This sum of factors results in one sparse matrix with a fixed sparsity pattern,
which yields up to 3$\times$ faster multiplication on GPUs (\cref{sec:appx_benchmark}).

Residual connections have been proposed to connect the butterfly
factors~\citep{vahid2020butterfly}.
We show that residual products of butterfly matrices have a first-order
approximation as a sparse matrix with a fixed sparsity.
Let $M$ be a matrix in the set of butterfly matrices $\B$.
In residual form, for some $\lambda \in \mathbb{R}$:
\begin{equation}
  \label{eq:residual_butterfly}
  M = (I + \lambda \vB_n^{(n)}) (I + \lambda \vB_{n/2}^{(n)}) \dots (I + \lambda \vB_2^{(n)}).
\end{equation}
Note that this form can represent the same matrices in the class of butterfly
matrices $\vB$, since any $\vB_k^{(n)}$ contains the identity matrix $I$.

Assuming that $\lambda$ is small, we can expand the residual and collect the
terms\footnote{We make the approximation rigorous in \cref{sec:analysis}.}:
\begin{equation*}
  M = I + \lambda (\vB_2^{(n)} + \vB_{4}^{(n)} + \dots + \vB_n^{(n)}) + \tilde{O}(\lambda^2).
\end{equation*}
\begin{definition}
  \label{def:flat_butterfly}
  \emph{Flat butterfly} matrices of maximum stride $k$ (for $k$ a power of 2)
  are those of the form $I + \lambda (\vB_2^{(n)} + \vB_{4}^{(n)} + \dots + \vB_k^{(n)})$.
\end{definition}
Flat butterfly matrices of maximum stride $n$ are the first-order approximation
of butterfly matrices in residual form (\cref{eq:residual_butterfly}).
Notice that flat butterfly of maximum stride $k$ are sparse matrices with $O(n \log k)$ nonzeros with a fixed
sparsity pattern, as illustrated in~\cref{fig:tradeoff}.
We call this sparsity pattern the \emph{flat butterfly} pattern.

\emph{Flat block butterfly} matrices are block versions of flat butterfly in~\cref{sec:flat_butterfly} (shown in~\cref{fig:tradeoff}).
We empirically validate that flat block butterfly matrices are up to 3$\times$
faster than block butterfly or regular butterfly (\cref{sec:appx_benchmark}).

Since flat butterfly matrices approximate the residual form of butterfly
matrices, they have high rank if $\lambda$ is small (\cref{sec:analysis}).
This is one of the motivations for the addition of the low-rank term in our
method.

\subsection{Pixelated Butterfly: Flat Block Butterfly + Low-rank for Efficient Sparse Training}
\label{sec:method}

We present Pixelated Butterfly, an efficient sparse model with a simple and fixed sparsity
pattern based on butterfly and low-rank matrices.
Our method targets GEMM-based neural networks, which are networks whose computation is dominated by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer.
As a result, we can view the network as a series of matrix multiplies.

Given a model schema (layer type, number of layers, matrix dimension) and a
compute budget, Pixelated Butterfly has three steps: compute budget allocation per layer,
sparsity mask selection from the flat butterfly pattern, and model
sparsification.
We describe these steps in more details:
\begin{enumerate}[leftmargin=*,nosep,nolistsep]
  \item \textbf{Compute budget allocation}: based on our cost model
  (\cref{app:problem_formulation}), given the layer type, number of layers, and
  matrix dimension, we can find the density (fraction of nonzero weights) of
  each layer type to minimize the projected compute cost.
  Continuing our goal for a simple method, we propose to use a simple rule of
  thumb: allocate sparsity compute budget proportional to the compute fraction
  of the layer.
  For example, if the MLP layer and attention layers are projected to takes 60\%
  and 40\% the compute time respectively, then allocate 60\% of the sparsity compute budget
  to MLP and 40\% to attention.
  We verify in \cref{sec:appx_method_details} that this simple rule of thumb
  produces similar results to solving for the density from the cost model.

  \item \textbf{Sparsity mask selection}: given a layer and a sparsity compute budget for
  that layer, we use one-quarter to one-third of the budget for the low-rank
  part as a simple rule of thumb.
  We pick the rank as a multiple of the smallest supported block
  size of the device (e.g., 32) so that the low-rank matrices are also block-aligned.
  The remaining compute budget is used to select the sparsity mask from the flat
  block butterfly sparsity pattern: we choose the butterfly block size as the
  smallest supported block size of the device (e.g., 32), and pick the maximum
  stride of the flat block butterfly (\cref{def:flat_butterfly}) to fill up the
  budget.

  \item \textbf{Model sparsification}: The resulting sparse model is simply a
  model whose weights or attention follow the fixed sparsity mask chosen in
  step 2, with the additional low-rank terms (rank also chosen in step 2).
  In particular, we parameterize each weight matrix\footnote{We describe how
  to add sparse and low-rank for attention in \cref{sec:appx_method_details}} as:
  $W = \gamma B + (1 - \gamma) U V^\top$, where $B$ is a flat block butterfly
  matrix (which is sparse), $U V^\top$ is the low-rank component, and $\gamma$
  is a learnable parameter.
  We train the model from scratch as usual.
\end{enumerate}

Our method is very simple, but competitive with more complicated procedures that
search for the sparsity pattern (\cref{sec:appx_ntk_algorithm}).
We expect more sophisticated techniques (dynamic sparsity, a better approximation
of butterfly) to improve the accuracy of the method.

