\section{Introduction}
\label{sec:intro}



Recent results suggest that overparameterized neural networks generalize well ~\citep{belkin2019reconciling}, but they are expensive to train~\citep{kaplan2020scaling}. An ideal model should use less compute and memory while retaining the generalization benefits of large models. The simplest and most popular direction is to sparsify these models. 
This idea has a long history in machine learning~\citep{lecun1990optimal} and has driven fundamental progress in other fields such as statistics~\citep{tibshirani1996regression}, neuroscience~\citep{foldiak2003sparse}, and signal processing~\citep{candes2006stable}. 
However, despite significant efforts, {\em speeding up sparse training in wall-clock time without degrading accuracy} remains an unresolved problem.




While sparse training is an active research area, it has not seen wide adoption.
First, it is difficult and expensive to find the sparsity pattern (the possible locations of the nonzeros) that could maintain the same level of accuracy of dense models.
Many methods (pruning~\citep{lee2018snip}, lottery tickets~\citep{frankle2018lottery}, hashing~\citep{kitaev2020reformer}) maintain dynamic sparsity masks.
However, the large overhead of evolving the sparsity mask can significantly slow down training and complicate the implementation. Indeed, these methods either require long cycles of pruning and retraining~\citep{frankle2018lottery}\footnote{State-of-the-art sparse training methods require up to 5$\times$ more training epochs compared to dense models~\citep{evci2020rigging}} or maintain expensive hash tables~\citep{chen2019slide}.
Second, most existing methods adopt unstructured sparsity, which may be efficient in theory, but do not take into account the efficiency of training hardware such as GPUs (optimized for dense computation)\footnote{An unstructured sparse model with 1\% nonzero weights can be as slow as a dense model~\citep{hooker2020hardware}}.
Finally, most methods target a single type of operation such as attention~\citep{child2019generating,zaheer2020bigbird}, whereas neural network (NN) models often compose different modules (attention, MLP), and in many applications the MLP layers are the main training bottleneck~\citep{wu2020lite}.


A better sparse training method should (i) be simple yet accurate, ideally with a static sparsity pattern, (ii) be fast by aligning sparsity pattern with available hardware, and (iii) have wide coverage of operators that applies to most NN layers.
There are three technical challenges.
First, we show that given a budget (e.g., total non-zeros in a matrix), it is NP-hard to find the optimal static sparsity pattern for a NN module to minimize the approximation error to the dense model. 
Second, for each sparsity pattern, we need to take into account hardware block-oriented efficiency (accessing each element in memory takes the same time as accessing the block of adjacent elements~\citep{cook2012cuda}, illustrated in~\cref{fig:hardware}). Common theoretical measures of efficiency (e.g., number of non-zeros, FLOPs) do not map well to modern hardware designed for block computation.
Last, every different NN module might require different sparsity patterns, which makes the problem even more complicated.


In our early exploration, we empirically study many sparsity patterns proposed in the literature to find those patterns that can closely approximate the dense model (Details in~\cref{sec:appx_ntk_algorithm}). We found that one sparsity pattern, namely butterfly + low-rank, consistently outperforms the others. This sparsity pattern closely connects to two lines of work in matrix structures: (i) sparse + low-rank matrices, which can capture global and local information~\citep{candes2011robust,udell2019big, scatterbrain}, and (ii) butterfly matrices~\citep{parker1995random,dao2019learning} whose products can tightly represent any sparse matrix~\citep{desa2018two, dao2020kaleidoscope}.
Using the fixed sparsity pattern from butterfly matrices, with the addition of a low-rank term, would address two of the three challenges above and yield a simple way to sparsify most NN layers (that are based on matrix multiply). 


However, butterfly matrices are inefficient on modern hardware: (i) they are difficult to parallelize as they contain sequential products of many factors, and (ii) they are not hardware-friendly because the sparsity patterns are not block-aligned.
We propose two simple changes to make Butterfly efficient while retaining their favorable properties. Our proposal, Pixelated Butterfly (Pixelfly), combines flat block butterfly and low-rank matrices to yield a simple and efficient sparse training method.
\begin{itemize}[leftmargin=*,nosep,nolistsep]
\item We design an extremely simple sparsity pattern inspired by butterfly + low-rank matrices, which takes into account the hardware's block-oriented efficiency.
We propose block butterfly matrices that are efficient as their sparsity patterns align with hardware blocks.
We then introduce flat butterfly, a first-order approximation of butterfly with residual connection, that turns the original product of factors into a sum.
Flat butterfly matrix multiplications are easy to parallelize.
Pixelfly, uses the fixed sparsity pattern from flat \& block butterfly, along with a low-rank term, to produce a sparse network.
\item We prove that block butterfly retains the expressiveness of butterfly matrices and can thus tightly capture sparse matrices.
We show that flat butterfly matrices can closely approximate large classes of matrices that butterfly matrices capture.
Moreover, we demonstrate that flat block butterfly + low-rank matrices are strictly more expressive than sparse or low-rank matrices alone.
Finally, leveraging the recent advance in the neural tangent kernel (NTK), we adapt existing techniques to prove the global convergence of gradient descent on training sparse and wide ReLU networks.
\item Our proposed Pixelfly can be applied to all network modules that rely on matrix multiplication (e.g., linear layer, attention, MLP). To sparsify a full network, we simply need to allocate compute budget for each layer based on matrix and hardware block size.
\end{itemize}

\begin{figure}
\vspace{-1.2cm}
\captionsetup{font=small}
	\begin{center}
	\scriptsize
		\begin{tabular}{c}
			\includegraphics[width=0.89\linewidth]{figs/pixelated_butterfly.pdf}
		\end{tabular}
	\end{center}
		\vspace{-0.4cm}
	\caption{Pixelfly targets GEMM-based networks (networks whose computation is dominated by matrix multiply), which it views as a series of matrix multiplication. For each matrix multiply from Model Schema, it (1) allocates compute budget based on dimension and layer type, (2) the budget decides a mapping (hyper-parameter) to our proposed flat block butterfly sparsity patterns, (3) outputs a hardware-aware sparse mask. Note since the hardware is a block device, one memory access to an element in a block leads to the access to the full block.}
	\label{fig:tradeoff}
	\vspace{-0.5cm}
\end{figure}

We empirically validate that Pixelfly can speed up the training of models (Transformers, ViT, MLP-Mixer) without quality drop compared to baselines on a wide range of domains and tasks.
On CIFAR10/100 \& ImageNet classification, Pixelfly achieve 2.3$\times$ training time speedup compared to dense ViT, MLP-Mixer models, and other sparse training baselines, while preserving the same accuracy. On the WikiText-103 language modeling task, we speed up GPT-2 Medium training by 2.5$\times$ and achieve the same perplexity. On the Long Range Arena benchmark, we maintain the same accuracy as Transformer with $5.2\times$ faster training than a dense model, $2\times$ faster than Sparse transformer, and $6\times$ faster than non-block-aligned sparse methods (Reformer).
Our ablation studies highlight the importance of each of our components:
our butterfly sparsity improves on existing hand-crafted patterns by up to 2\% of accuracy on ImageNet,
our hardware-aware block-sparsity yields up to 5$\times$ speedup, and the balanced compute budget allocation brings 2$\times$ speedup compared to baselines that only sparsify attention.\footnote{Pixelfly code is available at \url{https://github.com/HazyResearch/pixelfly}}




