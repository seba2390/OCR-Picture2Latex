


\section{Extended Related Work}
\label{app:related}
In this section, we extend the related works referenced in the main paper and discuss them in detail.

\subsection{Neural Pruning} 
Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections, pruning has seen great success in compressing complex models.\citet{han2015deep,han2015learning} put forth two naive but effective algorithms to compress models up to 49x and maintain comparable accuracy. \citet{li2016pruning} employ filter pruning to reduce the cost of running convolution models up to 38 $\%$, \citet{NIPS2017_a51fb975} prunes the network at runtime, hence retaining the flexibility of the full model. \citet{dong2017learning} prunes the network locally in a layer by layer manner.  \citet{sanh2020movement} prunes with deterministic first-order information, which is more adaptive to pretrained model weights. \citet{lagunas2021block} prunes transformers models with block sparsity pattern during fine-tuning, which leads to real hardware speed up while maintaining the accuracy. \citet{zhu2017prune} finds large pruned sparse network consistently outperform the small dense networks with the same compute and memory footprints. Although both our and all the pruning methods are aiming to produce sparse models, we differ in our emphasis on the overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost in finding the smaller model.\\

\subsection{Lottery Ticket Hypothesis} 
Models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery tickets \citet{frankle2018lottery} are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization. A huge number of studies are carried out to analyze these tickets both empirically and theoretically: \citet{morcos2019one} proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; \citet{frankle2019stabilizing} improves the stability of the lottery tickets by iterative pruning; \citet{frankle2020linear} found that subnetworks reach full accuracy only if they are stable against SGD noise during training; \citet{orseau2020logarithmic} provides a logarithmic upper bound for the number of parameters it takes for the optimal sub-networks to exist; \citet{pensia2020optimal} suggests a way to construct the lottery ticket by solving the subset sum problem and it's a proof by construction for the strong lottery ticket hypothesis. Furthermore, follow-up works \citep{liu2020finding, wang2020picking, tanaka2020pruning} show that we can find tickets without any training labels.\\

\subsection{Neural Tangent Kernel} 

Our work rely heavily on neural tangent kernel in theoretical analysis. Neural Tangent Kernel \citet{jacot2018neural} is first proposed to analyse the training dynamic of infinitely wide and deep networks. The kernel is deterministic with respect to the initialization as the width and depth go to infinity, which provide an unique mathematical to analyze deep overparameterized networks. Couples of theoretical works are built based upon this: \cite{lee2019wide} extend on the previous idea and prove that finite learning rate is enough for the model to follow NTK dynamic. \citet{arora2019exact} points out that there is still a gap between NTK and the real finite NNs. \citet{cao2020generalization} sheds light on  the good generalization behavior of overparameterized deep neural networks. \citet{arora2019fine} is the first one to show generalization bound independent of the network size. Later, some works reveal the training dynamic of models of finite width, pointing out the importance of width in training: \citet{hayou2019training} analyzes stochastic gradient from the stochastic differential equations' point of view; Based on these results, we formulate and derive our theorems on sparse network training.\\

\subsection{Overparameterized Models} 
Our work mainly targets overparameterized models. In \citet{nakkiran2019deep},  the double descendent phenomenon was observed. Not long after that, \cite{d2020triple} discover the triple descendent phenomenon. It's conjectured in both works that the generalization error improves as the parameter count grows. On top of that, \citet{arora2018optimization} speculates that overparameterization helps model optimization,
and without "enough" width, training can be stuck at local optimum. Given these intuitions, it's not surprising that the practitioning community is racing to break the record of the largest parameter counts: The two large language models, GPT-2 and GPT-3 \citep{radford2019language, brown2020language}, are pushing the boundary on text generation and understanding; Their amazing zero-shot ability earn them the title of foundation models \citep{bommasani2021opportunities}. On the computer vision side, \citet{dosovitskiy2020image, tolstikhin2021mlp, zhai2021scaling} push the top-1 accuracy on various vision benchmarks to new highs after scaling up to 50 times the parameters; \citet{naumov2019deep} shows impressive results on recommendation with a 21 billion large embedding; \citet{jumper2021highly} from DeepMind solve a 50 year old grand challenge in protein research with a 46-layer Evoformer. In our work, we show that there is a more efficient way to scale up model training through sparsification and double descent only implies the behavior of the dense networks.
