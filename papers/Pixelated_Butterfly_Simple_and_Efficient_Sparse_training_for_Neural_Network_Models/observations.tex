\section{Empirical study on efficiency-quality tradeoff of sparse model training}
\label{sec:observations}

We present an empirical study of existing sparse training methods on standard models and tasks: Image Classification(MlpMixer, ViT, T2T-ViT),  Causal Language Modeling (GPT), LRA (Transformer) and Contrastive Language-Image Pre-training (CLIP).

Our focus is on model training time (wallclock), memory, and accuracy, as the feature size, sequence length, and sparsity pattern vary.

Results: TBD

Takeaway message:
\begin{enumerate}
  \item Sparse MLPs (mixture of expert) tend to be more efficient for large feature size, while sparse attention (e.g., Reformer, OpenAI's sparse transformer) has the advantage when the sequence length is large compared to the feature dimension.
  
  This is a function of how much compute / memory is due to the attention layers vs the MLP layers.
  
  \item Could we smoothly trade-off effiency-accuracy as the sequence length / feature size ratio changes, to combine the best of both worlds?
  
\end{enumerate}