\vspace{-0.2cm}
\section{Related Work}
\textbf{Lottery Ticket Hypothesis.} Models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery tickets~\citep{frankle2018lottery} are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks. Many insightful studies~\citep{morcos2019one, orseau2020logarithmic, frankle2019stabilizing, frankle2020linear, malach2020proving, pensia2020optimal} are carried out to analyze these tickets, but it remains difficult to generalize to large models due to training cost. In an attempt, follow-up works~\citep{wang2020picking, tanaka2020pruning} show that one can find tickets without training labels. 
We draw inspiration from one of them, \citet{liu2020finding}, which uses the NTK to avoid using labels in sparsifying networks. Other recent works use specialized hardware to accelerate sparse training~\citep{Goli_2020_CVPR, raihan2020sparse}.

\textbf{Neural Pruning.} Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections, pruning has seen great success in compressing complex models.
Pioneering work~\citep{han2015deep,han2015learning} shows that pruning can produce significantly smaller and faster models for inference.
Subsequent methods~\citep{li2016pruning, NIPS2017_a51fb975, dong2017learning,  sanh2020movement, lagunas2021block, zhu2017prune} improve on the quality of the pruned models. 
While both our and the pruning methods aim to produce sparse models, we target training efficiency, whereas pruning mostly focuses on inference efficiency at the cost of sacrificing training speed.



\textbf{Overparameterized Models and NTK.} Our analysis for sparse model convergence relies heavily on recent advance in neural tangent kernel (NTK)~\citep{jacot2018neural}. NTK is a tool which has been widely used in analyzing overparameterized models' convergence~\citep{ll18,dzps19,als19_dnn,als19_rnn,sy19}, generalization~\citep{all19}, connection to data separability~\citep{os20}, and cost per iteration~\citep{bpsw21}). Deep Double Descent \citep{nakkiran2019deep, d2020triple} conjectures that the generalization error improves as the parameter count grows. It is not surprising that the community is racing to break the record of the largest parameter counts~\citep{radford2019language, brown2020language, dosovitskiy2020image, tolstikhin2021mlp, zhang2021cpm, naumov2019deep, jumper2021highly}. 

We provide extended related work in~\cref{app:related}.


\vspace{-0.2cm}