\begin{figure}[t]
	\begin{center}
	\scriptsize
		\begin{tabular}{c}
			\includegraphics[width=\linewidth]{figs/candidates.pdf}
		\end{tabular}
	\end{center}
	\caption{Sparsity pattern candidate components:  Local corresponds to local interaction of neighboring elements; Global (low-rank) involves the interaction between all elements and a small subset of elements; Butterfly captures the interaction between elements that are some fixed distance apart; Random is common in the pruning literature.}
	\label{fig:block_sparse_visualization} 
\end{figure}

\section{Exhausted Searching Sparsity Patterns for Efficient Sparse Training}
\label{sec:appx_ntk_algorithm}
We describe here our early exploration of searching among different sparsity patterns that has been proposed in the literature.
We use a metric derived from the NTK, which has emerged as one of the standard metric to predict the training and generalization of the model.
We consistently found the butterfly + low-rank pattern to perform among the best.

In~\cref{sec:challenges}, we describe the challenges of selecting sparsity patterns for every model components using the a metric derived from the NTK, followed by our approaches.
Then in , we describe details of empirical NTK computation, which is an important step in our method implementation. 
Last, in~\cref{sec:property}, we highlight important properties of our method -- it rediscovers several classical sparsity patterns, and the sparse models can inherit the training hyperparamters of the dense models, reducing the need for hyperparameters tuning.

\subsection{Challenges and Approaches}
\label{sec:challenges}

\textbf{Challenge 1:} We seek sparsity patterns for each model components that can closely mimic the training dynamics of the dense counterpart. As mentioned in~\cref{thm:mask_regression}, it is NP-hard to find the optimal sparse matrix approximation. Although NTK provides insights and measurement on the ``right'' sparse model, bruteforcely computing NTK for one-layer models with all sparsity patterns is still infeasible.

\textbf{Approach 1: Sparsity Pattern Candidates.}
To address the above challenge, we design our search space to be a limited set of sparsity pattern candidates, each is either a component visualized in \cref{fig:block_sparse_visualization} or the combination of any two of them.
These components encompass the most common types of sparsity pattern used, and can express
We provide the intuition behind these sparsity components:
\begin{itemize}[leftmargin=*,nosep,nolistsep]
  \item Local: this block-diagonal component in the matrix corresponds to local interaction of neighboring elements. This has appeared in classical PDE discretization \citep{collins1971diagonal}, and has been rediscovered in Longformer and BigBird attention patterns.
  \item Global: this component involves interaction between all elements and a small subset of elements (i.e., ``global'' elements).
  This global pattern is low-rank, and this sparse + low-rank structure is common in data science~\citep{udell2019big}, and rediscovered in Longformer and BigBird patterns as well.
  \item Butterfly: this component corresponds to interaction between elements that are some fixed distance apart.
  The many divide-and-conquer algorithms, such as the classical fast Fourier transform~\citep{cooley1965algorithm}, uses this pattern at each step. Butterfly matrices reflects this divide-and-conquer structure, and hence this sparsity component. The sparse transformer~\citep{child2019generating} also found this pattern helpful for attention on image data.
  \item Random: this component is a generalization of sparsity patterns found in one-shot magnitude, gradient, or momentum based pruning~\citep{lee2018snip}. Note that at network initialization, they are equivalent to random sparsity.
\end{itemize}

\textbf{Challenge 2:} Even with a fixed pool of sparsity patterns for each layer, if the model has many layers, the number of possible layer-pattern assignments is exponentially large.

\textbf{Approach 2:} To further reduce the search space,
we constrain each layer type (attention, MLP) to have the same sparsity pattern.
For example, if there are 10 patterns and 2 layer types, the candidate pool is $10^2 = 100$ combinations.

\textbf{Challenge 3:} Computing the empirical NTK on the whole dataset is expensive in time and space, as it scales quadratically in the dataset size.

\textbf{Approach 3:} We compute the empirical NTK on a randomly chosen subset of the data (i.e., a principal submatrix of the empirical NTK matrix).
In our experiments, we verify that increasing the subset size beyond 1000 does not change the choices picked by the NTK heuristic.
The subsampled empirical NTK can be computed within seconds or minutes.

\subsection{Algorithm Description}
\label{sec:algorithm_description}












\begin{algorithm}[t]
{\small
    \begin{algorithmic}[1]
        \State \textbf{Input: model schema $\Omega$, compute budget $B$, dataset subset $X$, sparsity mask candidate set $C$.}
        \State $K_{dense} \leftarrow$ \textsc{NTK}$(f_\theta, X)$. \Comment{\cref{eq:empirical_ntk}}
        \State output sparsity mask assignment $s_\mathrm{out}$, $d_{min} \leftarrow \inf$
        \For {$M_1, \dots, M_{|\Omega|} \in C^{|\Omega|}$} \Comment{Enumerate all sparsity mask candidate combinations}
            \State Let $s$ be the sparsity mask assignment $(t_i, r_i, m_i, n_i) \to M_i$.
            \If {$\text{TotalCompute}(s) < B$} \Comment{\cref{eq:budget}, Check if masks satisfy budget constraint}
                \State Let $M_s$ be the flattened sparse masks
                \State $K_{sparse} \leftarrow \textsc{NTK}(f_{\theta \circ M_s}, X)$
                \State $d_s \leftarrow \textsc{Distance}(K_{dense}, K_{sparse})$ \Comment{\cref{eq:empirical_ntk}}
                \If{$d_{min}>d_s$}
                    \State $d_{min} \leftarrow d_s$, $s_\mathrm{out} \leftarrow s$
                \EndIf
            \EndIf
        \EndFor
        \State\Return $s_\mathrm{out}$ \Comment{Return sparsity mask assignment}
    \end{algorithmic}
    }
    \caption{Model Sparsification}\label{algo:pre}
    \end{algorithm}



Our method targets GEMM-based neural networks, which are networks whose computation is dominated by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer.
As a result, we can view the network as a series of matrix multiplies.
We first define:
\begin{itemize}[leftmargin=*,nosep,nolistsep]
  \item Model schema: a list of layer types $t$ (e.g., attention, linear layers in MLP), number of layers $r$ of that type, and dimension of the matrix multiplies $m \times n$.
  We denote it as $\Omega = \{(t_1, r_1, m_1, n_1), \dots, (t_{|\Omega|}, r_{|\Omega|}, m_{|\Omega|}, n_{|\Omega|})\}$.
  \item A \emph{mask} $M$ of dimension $m \times n$ is a binary matrix $\{0, 1\}^{m \times n}$.
  The compute of a mask is the total number of ones in the matrix: $\mathrm{compute}(M) = \sum_{i, j} M_{ij}$.
  \item A \emph{sparsity pattern} $P_{m \times n}$ for matrix dimension $m \times n$ is a set of masks $\{M_1, ..., M_{|P|}\}$, each of dimension $m \times n$.
  \item A \emph{sparsity mask assignment} is a mapping from a model schema $\Omega$ to masks $M$ belonging to some sparsity pattern $P$: $s \colon (t, r, m, n) \to M$.
  \item Given a set of sparsity patterns $P_1, \dots, P_k$, the set of sparsity mask candidate $C$ is the union of sparsity masks in each of $P_i$: $C = \cup P_i$
  \item A sparsity pattern assignment $s$ satisfies the compute budget $B$ if:
\begin{equation}
\label{eq:budget}
  \mathrm{TotalCompute}(s) := \sum_{\text{layer type } l} \mathrm{compute}(s(t, r, m, n)) \le B.
\end{equation}
  \item Let $\theta$ be the flattened vector containing the model parameters, and let $M_s$ be the flattened vector containing the sparsity mask by the sparsity mask assignment $s$.
  Let $f_\theta(x)$ be the output of the dense network with parameter $\theta$ and input $x$.
  Then the output of the sparse network is $f_{\theta \circ M_s}(x)$.
  \item The empirical NTK of a network $f_\theta$ on a data subset $X = \{x_1, \dots, x_{|X|}\}$ is a matrix of size $|X| \times |X|$:
\begin{equation}
  \label{eq:empirical_ntk}
  \mathrm{NTK}(f_\theta, X)_{i, j} = \left \langle \frac{\partial f_\theta(x_i)}{\partial \theta}, \frac{\partial f_\theta(x_j)}{\partial \theta} \right \rangle.
\end{equation}
\end{itemize}

The formal algorithm to assign the sparsity mask to each layer type is described in \cref{algo:pre}.
The main idea is that, as the set of sparsity mask candidate is finite, we can enumerate all possible sparsity mask assignment satisfying the budget and pick the one with the smallest NTK distance to the dense NTK.
In practice, we can use strategies to avoid explicitly enumerating all possible sparsity mask, e.g. for each sparsity pattern, we can choose the largest sparse mask that fits under the budget.









\subsection{Method Properties: Rediscovering Classical Sparsity Patterns, No Additional Hyperparameter Tuning}
\label{sec:property}
When applied to the Transformer architecture, among the sparsity components described in \cref{sec:challenges}, the NTK-guided heuristic consistently picks the local and global components for \emph{both} the attention and MLP layers.
Moreover, the butterfly component is also consistently picked for image data, reflecting the 2D inductive bias in this component\footnote{Convolution (commonly used in image data) can be written in terms of the fast Fourier transform, which has this same sparse pattern at each step of the algorithm}.
While some of these patterns have been proposed for sparse attention, it is surprising that they are also picked for the MLP layers.
The most popular type of sparsity pattern in MLP layers is top-k (in magnitude or gradient, which at initialization is equivalent to random sparsity).
We have proved that lower NTK difference results in better generalization bound for the sparse model. As expected, we observe that this allows the sparse model to use the same hyperparamters (optimizer, learning rate, scheduler) as the dense model (\cref{sec:experiments}).






