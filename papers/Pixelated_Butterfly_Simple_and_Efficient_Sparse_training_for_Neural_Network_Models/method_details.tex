\section{Method Details}
\label{sec:appx_method_details}

We describe some details of our method.
\subsection{Compute budget allocation}

We describe here a procedure to compute the budget allocation
based on our cost model.
This procedure is more complicated than our simple rule of thumb in
\cref{sec:method}, and tend to produce the same allocation.
For completeness, we include the procedure here for the interested reader.

Given a parameter budget $B$, we find the density of each layer type that
minimize the models' total cost of matrix multiplication.
For example, in Transformers, let $d_a$ and $d_m$ be the density of the
attention and the MLP layers.
Let $s$ be the sequence length and $d$ be the feature size.
The attention layer with density $d_a$ will cost $d_a (n^2 + nd)$, and the fully
connected layers with density $d_m$ will cost $2 d_m nd$.
We then set $d_a$ and $d_m$ to minimize the total cost while maintaining the
parameter budget:
\begin{equation}\label{eq:budget}
  \text{minimize}_{\delta_a, \delta_m} \delta_a (n^2 + nd) + 2 \delta_m n d \quad
  \text{subject to} \quad \text{$\#$ of trainable parameters} \leq B.
\end{equation}
As this is a problem with two variables, we can solve it in closed form.

\subsection{Low-rank in Attention}

In \cref{sec:method}, we describe how to use the sparsity pattern from flat
block butterfly and the low-rank term for weight matrices.
This applies to the linear layer in MLP and the projection steps in the
attention.

We also use the sparse + low-rank structure in the attention step itself.
\citet{scatterbrain} describes a general method to combine sparse and low-rank
attention, where one uses the sparse component to discount the contribution from
the low-rank component to ensure accurate approximation of the attention matrix.

We follow a simpler procedure, which in practice yields similar performance.
We use a restricted version of low-rank of the form a ``global'' sparsity mask
(as shown in \cref{fig:block_sparse_visualization}).
Indeed, a sparse matrix whose sparsity pattern follows the ``global'' pattern is
a sum of two sparse matrices, one containing the ``horizontal'' global components
and one containing the ``vertical'' components.
Let $w$ be the width of each of those components, then each of them has rank at
most $w$.
Therefore, this sparse matrix has rank at most $2w$, and is low-rank (for small $w$).

We also make the global component block-aligned (i.e., set $w$ to be a multiple
of the smallest supported block size such as 32) for hardware efficiency.

\subsection{Comparison to Other Sparsity Patterns for Attention}

In the context of sparse attention, other sparsity patterns such as BigBird and
Longformer also contain a ``global'' component, analogous to our low-rank
component.
Their ``local'' component is contained in the block diagonal part of the flat
block butterfly sparsity pattern.

The main difference that we do not use the random components (e.g., BigBird),
and the diagonal strides from flat block butterfly are not found in BigBird or
Longformer.
Moreover, we apply the same sparsity pattern (+ low-rank) to the linear layers
in the MLP and the projection step in attention as well, allowing our method to
target most neural network layers, not just the attention layer.

\subsection{Sparsity Mask for Rectangular Matrices}

We have described the sparsity masks from flat block butterfly for square
matrices.
For rectangular weight matrices, we simply ``stretch'' the sparsity mask.
The low-rank component applies to both square and rectangular matrices (as shown in~\cref{fig:rec}).
We have found this to work consistently well across tasks.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{figs/rec_butterfly.pdf}
  \caption{\label{fig:rec} Sparsity Mask for Rectangular Matrices.}
\end{figure}


