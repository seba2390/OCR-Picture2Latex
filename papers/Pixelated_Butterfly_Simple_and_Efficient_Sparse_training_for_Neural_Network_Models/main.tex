\documentclass{article} %

\usepackage{etoolbox}
\newtoggle{arxiv}
\toggletrue{arxiv}

\iftoggle{arxiv}{}
{
\usepackage{iclr2022_conference,times}
}



\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage[inline]{enumitem}
\usepackage{caption}

\usepackage{graphicx}
\usepackage{grffile}
\usepackage{natbib}
\usepackage{wrapfig,epsfig}
\usepackage{epstopdf}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{comment}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{color}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{natbib}
\usepackage{wrapfig,epsfig}
\usepackage{epstopdf}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{bbm}
\usepackage{comment}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{afterpage}

\usepackage[capitalize]{cleveref}
\usepackage{capt-of}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{open}[theorem]{Open Problem}
\newtheorem{property}[theorem]{Property}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{process}{Process}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\newcommand{\rank}{\mathrm{rank}}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\newcommand{\ov}{\overline}
\newcommand{\eps}{\epsilon}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\RHS}{\mathrm{RHS}}
\newcommand{\LHS}{\mathrm{LHS}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\i}{\mathbf{i}}
\renewcommand{\varepsilon}{\epsilon}
\renewcommand{\tilde}{\wt}
\renewcommand{\hat}{\wh}
\newcommand{\ReLU}{{$\mathsf{ReLU}$}}
\newcommand{\new}{\mathrm{new}}
\newcommand{\nnz}{\mathrm{nnz}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\norm}[1]{\left\|{#1}\right\|} %
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\defeq}{:=}
\newcommand{\vB}{\mathbf{B}}
\newcommand{\vD}{\mathbf{D}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BS}{\B^*}
\newcommand{\BBS}{\B\B^*}
\newcommand{\BSB}{\B^*\B}

\DeclareMathOperator*{\E}{{\mathbb{E}}}

\newcommand{\Zhao}[1]{{\color{red} [Zhao: {#1}]}}
\newcommand{\Beidi}[1]{{\color{orange} [Beidi: {#1}]}}
\newcommand{\Jiaming}[1]{{\color{blue} [Jiaming: {#1}]}}
\newcommand{\Tri}[1]{{\color{cyan} [Tri: {#1}]}}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\iftoggle{arxiv}{
  \setlength{\textwidth}{6.5in}
  \setlength{\textheight}{9in}
  \setlength{\oddsidemargin}{0in}
  \setlength{\evensidemargin}{0in}
  \setlength{\topmargin}{-0.5in}
  \newlength{\defbaselineskip}
  \setlength{\defbaselineskip}{\baselineskip}
  \setlength{\marginparwidth}{0.8in}
}{
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*1}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}

\usepackage[subtle, mathdisplays=tight, charwidths=normal, leading=normal]{savetrees}

\addtolength\textwidth{0.15in}
\addtolength\textheight{0.15in}
\addtolength\textfloatsep{-0.5em}
\addtolength\intextsep{-0.2em}

\def\setstretch#1{\renewcommand{\baselinestretch}{#1}}
\setstretch{0.99}
\addtolength{\parskip}{-0.3pt}
}


\title{Pixelated Butterfly: Simple and Efficient Sparse Training for Neural Network Models}

\iftoggle{arxiv}{
  \usepackage{authblk}
  \author[$\dagger$]{Tri Dao\thanks{Equal contribution. Order determined by coin flip.}}
  \author[$\dagger$]{Beidi Chen\samethanks}
  \author[$\oplus$]{Kaizhao Liang}
  \author[$\diamond$]{Jiaming Yang}
  \author[$\S$]{Zhao Song}
  \author[$\ddagger$]{Atri Rudra}
  \author[$\dagger$]{Christopher R{\'e}}
  \affil[$\dagger$]{Department of Computer Science, Stanford University}
  \affil[$\oplus$]{SambaNova Systems, Inc}
  \affil[$\diamond$]{Department of Probability and Statistics, Peking University}
  \affil[$\S$]{Adobe Research}
  \affil[$\ddagger$]{Department of Computer Science and Engineering, University at Buffalo, SUNY\vspace{4pt}}
  \affil[ ]{\small{\texttt{\{trid,beidic\}@stanford.edu}, \texttt{kaizhao.liang@sambanovasystems.com}, \texttt{edwinyjmpku@gmail.com}, \texttt{zsong@adobe.com}, \texttt{atri@buffalo.edu}, \texttt{chrismre@cs.stanford.edu}}}
}{

\author{%
  Tri Dao\thanks{Equal contribution. Order determined by coin flip.}\, $^1$, Beidi
  Chen\samethanks\, $^1$, Kaizhao Liang $^2$, Jiaming Yang $^3$, Zhao Song $^4$,
  Atri Rudra $^5$, Christopher R\'{e} $^1$ \\
  $^1$ Department of Computer Science, Stanford University \\
  $^2$ SambaNova Systems, Inc \\
  $^3$ Department of Probability and Statistics, Peking University \\
  $^4$ Adobe Research \\
  $^5$ Department of Computer Science and Engineering, University at Buffalo, The State University of New York\\
  \texttt{\{trid,beidic\}@stanford.edu},
  \texttt{kaizhao.liang@sambanovasystems.com}, \\
  \texttt{edwinyjmpku@gmail.com}, \texttt{zsong@adobe.com}, \\ \texttt{atri@buffalo.edu}, \texttt{chrismre@cs.stanford.edu}
}

}

\newcommand{\fix}{\marginpar{FIX}}

\iftoggle{arxiv}{}{
\iclrfinalcopy %
}
\begin{document}

 
\maketitle
\begin{abstract}


Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components.
The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive.
To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices.
As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware.
Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP).
We empirically validate that Pixelated Butterfly is $3\times$ faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs.
On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5$\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.



\end{abstract}


\input{intro}

\input{theory}
\input{butterfly}
\input{analysis}
\input{experiment}
\input{related_work}
\input{conclusion}

\subsubsection*{Acknowledgments}

We thank Laurel Orr, Xun Huang, Sarah Hooper, Sen Wu, Megan Leszczynski, and Karan Goel for their helpful discussions and feedback on early drafts of the paper.

We gratefully acknowledge the support of NIH under No.\ U54EB020405 (Mobilize), NSF under Nos.\ CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No.\ N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and Bioengineering through Grant P41EB027060. The U.S.\ Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S.\ Government.
Atri Rudraâ€™s research is supported by NSF grant CCF-1763481.


\bibliography{ref}
\bibliographystyle{plainnat}


\appendix
\newpage
\input{formulation_details}
\newpage
\input{butterfly_proofs}
\newpage
\input{proof}
\newpage
\input{ntk_proofs}
\newpage
\input{convergence}
\newpage
\input{method_details}
\newpage
\input{benchmark}
\newpage
\input{algorithm_details}
\newpage
\input{experiment_details}
\newpage
\input{related_work_details}


\end{document}
