\section{Theoretical analysis}
\label{sec:analysis}

We characterize the expressiveness of the matrices used in our method.
In particular, we prove that block butterfly retains the expressiveness
of butterfly, and that flat butterfly can accurately approximate the
residual form of butterfly.
Moreover, flat block butterfly + low-rank (an instance of sparse +
low-rank) is more expressive than sparse or low-rank matrices alone.
Finally, we analyze the training convergence and generalization of networks
with sparse weights.
All proofs are in the Appendix.

\subsection{Expressiveness of Block Butterfly}
We first prove the expressiveness of block butterfly matrices.
\begin{theorem}
  \label{thm:block_butterfly}
  The set $\vB_{2b}$ of $n \times n$ block butterfly matrices with block size $2b$ contains
  the set $\vB_b$ of $n \times n$ block butterfly matrices of block size $b$.
\end{theorem}
By a recursive argument, the set of block butterfly matrices whose block size is
a power of 2 contains the set of regular butterfly matrices.

\citet{dao2020kaleidoscope} show that butterfly matrices can tightly represent
all structured matrices, such as sparse matrices and many fast transforms.
As a result, block butterfly matrices can also represent those structured
matrices.
In particular,
\begin{corollary}
  \label{cor:block_butterfly_contains_sparse}
  For any constant block size $b$ that is a power of 2, any $nb \times nb$ spare
  matrix with $s$ nonzeros can be written as products of block butterfly
  matrices with block size $b$ and their transposes, with $O(s \log n)$
  parameters.
\end{corollary}

\subsection{Expressiveness of Flat Butterfly}

We now characterize how the flat butterfly matrices approximate butterfly
matrices.
In particular, assuming that each butterfly factor has bounded norm,
we show that flat-butterfly matrices can accurately approximate the residual
form of butterfly with error scaling as $\tilde{O}(\lambda^2)$.
\begin{theorem}
  \label{thm:flat_butterfly_approx}
  Let $M$ be a matrix of the form in \cref{def:flat_butterfly} where $k = n$, with
  $B_\mathrm{max} \defeq \max_{i} \norm{\vB_i^{(n)}}_F$ and
  $\abs{\lambda} \leq \frac{c \sqrt{\epsilon}}{\log n B_\mathrm{max}}$ for some
  constant $0 < c \leq \frac{1}{2}$ and some $\epsilon > 0$.
  Then
  \begin{equation*}
    \norm{M - \left( I + \lambda (\vB_2^{(n)} + \vB_{4}^{(n)} + \dots + \vB_n^{(n)}) \right)}_F \leq \epsilon.
  \end{equation*}
\end{theorem}

We show that flat butterfly matrices must have high-rank if $\lambda$ is small.
This is the motivation for the addition of the low-rank term in Pixelfly (\cref{sec:butterfly}).
\begin{theorem}
  \label{thm:flat_butterfly_rank}
  Let $M$ be as in \cref{eq:residual_butterfly}, with
  $B_\mathrm{max} \defeq \max_{i} \norm{\vB_i^{(n)}}_F$ and
  $\abs{\lambda} \leq \frac{c \sqrt{\epsilon}}{\log n B_\mathrm{max}}$ for some
  constant $0 < c \leq \frac{1}{4}$ and some $\epsilon > 0$.
  Let $B^\infty_\mathrm{max} = \max_i \norm{\vB_i}_\infty$.
  Assuming $B^\infty_\mathrm{max} \leq B_\mathrm{max}$.
  Then
  \begin{equation*}
    \mathrm{rank}(I + \lambda (\vB_2^{(n)} + \dots + \vB_n^{(n)})) = \Omega \left( \left( \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \right)^2 \cdot \frac{\log n}{\epsilon \log \left( \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \right)} \right).
  \end{equation*}
\end{theorem}

\subsection{Expressiveness of Flat Block Butterfly + Low-rank}

\citet{scatterbrain} prove that there is a natural class of input sequences (generated
by a clustering process) whose attention matrix can only be approximated well by
sparse + low-rank matrices, and not sparse or low-rank matrices alone.
We adapt their technique to show a similar result for the class of matrices we
use in Pixelfly.

We require an extra assumption on the clustering process compared
to~\citet{scatterbrain}: the elements in the input sequence form clusters with
the same size.
Then their attention matrix will have a large block diagonal component
well-approximated by flat butterfly, while the rest of the attention matrix is
of medium size and is well-approximated by low-rank.
\begin{theorem}[Informal]
  There exists a class of input sequences whose attention matrices are
  well-approximated by flat block butterfly + low-rank (a special case of sparse +
  low-rank) but not by sparse or low-rank alone.
\end{theorem}
The formal theorem statement and proof are in \cref{subsec:flat_butterfly_lr_proofs}.




\subsection{Convergence and Generalization of Sparse Networks}

There are natural questions about the training and generalization of sparse models: do they train similarly to dense models, is their generalization close to that of dense models, and can one successfully train them with gradient descent?
Our analysis theoretically shows that the answers are yes.

Our analysis relies on the neural tangent kernel (NTK)~\citep{jacot2018neural} of the network.
The NTK of two data points $x$ and $y$ measures the similarity between the gradient of the network when evaluated at $x$ compared to the gradient when evaluated at $y$.
This kernel governs the dynamics of the neural network output function $f(\cdot, \theta)$ throughout the training and its generalization.
We build on the great literature of NTK~\citep{ll18,dzps19,als19_dnn}.
The standard result~\citep{sy19} implies the following, if the NTK of the sparse model is close to the NTK of the dense model, then
(i) their training convergence speed is similar, (ii) their generalization
bounds are similar.
For completeness, we state the formal result in \cref{sec:appx_ntk}.


Though this result does not capture the possible regularization effect of sparsity, it shows that sparse models with small NTK difference from dense NTK preserve the generalization ability of dense models, a subject that has been studied more extensively, both from empirical and from theoretical perspectives.
We also show that training wide and sparse networks with gradient descent
converges globally, similar to the result for wide dense networks~\citep{dzps19,
  als19_dnn} in \cref{sec:gradient_flow}.






