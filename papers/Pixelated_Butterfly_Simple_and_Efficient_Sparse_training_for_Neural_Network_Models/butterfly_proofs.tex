\section{Analysis of Butterfly Variants}
\label{sec:butterfly_proofs}

We present formal versions of theorems in~\cref{sec:analysis} regarding variants
of butterfly matrices.
We provide full proofs of the results here.

\subsection{Block Butterfly Analysis}
\label{subsec:block_butterfly_proofs}

\begin{proof}[Proof of \cref{thm:block_butterfly}]
  Let $M$ be an $n \times n$ block butterfly matrix with block size $b$.
  We want to show that $M$ also has a representation as an $n \times n$ block
  butterfly matrix with block size $2b$.

  By \cref{def:bmatrix}, $M$ has the form:
  \begin{equation*}
    M = \vB_{\frac{n}{b}}^{ \left( \frac{n}{b}, b \right)} \vB_{\frac{n}{2b}}^{ \left( \frac{n}{b}, b \right)} \hdots \vB_4^{ \left( \frac{n}{b}, b \right)} \vB_2^{ \left( \frac{n}{b}, b \right)}.
  \end{equation*}
  Notice that we can combine that last two terms to form a matrix of the form
  $\vB_2^{ \left( \frac{n}{2b}, 2b \right)}$ (see \cref{fig:tradeoff}).
  Moreover, other terms in the product of the form
  $\vB_{\frac{n}{2^ib}}^{ \left( \frac{n}{b}, b \right)}$ can also be written as
  $\vB_{\frac{n}{2^{i-1} 2b}}^{ \left( \frac{n}{2b}, 2b \right)}$ (see \cref{fig:tradeoff}).
  Thus $M$ also has the form:
  \begin{equation*}
    M = \vB_{\frac{n}{2b}}^{ \left( \frac{n}{2b}, 2b \right)} \vB_{\frac{n}{4b}}^{ \left( \frac{n}{2b}, 2b \right)} \hdots \vB_2^{ \left( \frac{n}{2b}, 2b \right)}.
  \end{equation*}
  In other words, $M$ is also an $n \times n$ block butterfly matrix with block
  size $2b$.

\end{proof}

\begin{proof}[Proof of \cref{cor:block_butterfly_contains_sparse}]
  \citet[Theorem 3]{dao2020kaleidoscope} states that any $n \times n$ sparse
  matrix with $s$ nonzeros can be represented as products of butterfly matrices
  and their transposes, with $O(s \log n)$ parameters.

  For a constant block size $b$ that is a power of 2, the set of $n \times n$ block butterfly
  matrices of block size $b$ contains the set of regular butterfly matrices by
  \cref{thm:block_butterfly}.
  Therefore any such $n \times n$ sparse matrix also has a representation has
  products of block butterfly matrices of block size $b$ and their transposes,
  with $O(s \log n)$ parameters.
\end{proof}

\subsection{Flat Butterfly Analysis}
\label{subsec:flat_butterfly_proofs}

We prove \cref{thm:flat_butterfly_approx}, which relates the first-order
approximation in the form of a flat butterfly matrix with the original butterfly
matrix.
\begin{proof}[Proof of \cref{thm:flat_butterfly_approx}]
  Let $n = 2^m$ and let $B_1, \dots, B_m \in \mathbb{R}^{n \times n}$ be the $m$
  butterfly factor matrices (we rename them here for simplicity of notation).

  Let
  \begin{equation*}
    E = \prod_{i=1}^m \left(I + \lambda B_i\right) - \left( I + \sum_{i=1}^m \lambda B_i \right).
  \end{equation*}
  Our goal is to show that $\norm{E}_F \leq \epsilon$.

  We first recall some properties of Frobenius norm.
  For any matrices $A$ and $C$, we have
  $\norm{A C}_F \leq \norm{A}_F \norm{C}_F$ and
  $\norm{A + C}_F \leq \norm{A}_F + \norm{C}_F$.

  Expanding the terms of the product in $E$, we have
  \begin{equation*}
    E = \sum_{i=2}^{m} \lambda^i \sum_{s \in [m], \abs{s} = i} \prod_{j \in s} B_j.
  \end{equation*}
  Using the above properties of Frobenius norm, we can bound $E$:
  \begin{align*}
    \norm{E}_F
    &\leq \sum_{i=2}^{m} \lambda^i \sum_{s \in [m], \abs{s} = i} \prod_{j \in s} \norm{B_j}_F \\
    &\leq \sum_{i=2}^{m} \lambda^i \sum_{s \in [m], \abs{s} = i} \prod_{j \in s} B_\mathrm{max} \\
    &= \sum_{i=2}^{m} \lambda^2 m^i \left( B_\mathrm{max} ^i \right) \\
    &= \sum_{i=2}^{m} (\lambda m B_\mathrm{max})^i \\
    &\leq \sum_{i=}^{m} \left(c \sqrt{\epsilon} \right)^i \\
    &\leq c^2 \epsilon \sum_{i=0}^{\infty} (c \sqrt{\epsilon})^i \\
    &\leq \frac{c^2\epsilon}{1 - c\sqrt{\epsilon}} \\
    &\leq \epsilon,
  \end{align*}
  where in the last step we use the assumption that $c \leq \frac{1}{2}$.
\end{proof}

We now bound the rank of the first-order approximation.
\begin{proof}[Proof of \cref{thm:flat_butterfly_rank}]
  Let $M^* = I + \sum_{i=1}^{m} \lambda B_i$.
  Note that any entry in $\sum_{i=1}^{m} \lambda B_i$ has absolute value at most
  \begin{equation*}
    m \lambda B^\infty_\mathrm{max} \leq \frac{c \sqrt{\epsilon} B^\infty_\mathrm{max}}{B_\mathrm{max}} \leq \frac{1}{4},
  \end{equation*}
  where we use the assumption that $B^\infty_\mathrm{max} \leq B_\mathrm{max}$
  and $c \leq \frac{1}{4}$.

  Thus any diagonal entry in $M^*$ has absolute value at least
  $1 - \frac{1}{4} = \frac{3}{4}$ and the off-diagonal entries are at most
  $\frac{c \sqrt{\epsilon}B^\infty_\mathrm{max}}{bm}$.

  \citet[Theorem 1.1]{alon2009perturbed} states that: there exists some $c > 0$
  such that for any real $M \in \mathbb{R}^{n \times n}$, if the diagonal
  elements have absolute values at least $\frac{1}{2}$ and the off-diagonal
  elements have absolute values at most $\epsilon$ where
  $\frac{1}{2 \sqrt{n}} \leq \epsilon \leq \frac{1}{4}$, then
  $\rank(M) \geq \frac{c \log n}{\epsilon^2 \log 1/\epsilon}$.

  Applying this theorem to our setting, we have that
  \begin{equation*}
    \rank(M^*) \geq \Omega \left( \left( \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \right)^2 \cdot \frac{m}{\epsilon \log \left( \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \right) }\right).
  \end{equation*}
  We just need to show that
  $\frac{B^\infty_\mathrm{max}}{B_\mathrm{max}} \geq \frac{1}{2 c \sqrt{\epsilon n}}$
  to satisfy the condition of the theorem.

  Indeed, we have that
  $1 \leq \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \leq \sqrt{2n}$ as each
  $\norm{B_i}_0 \leq 2n$.
  Combining the two conditions on
  $\frac{B_\mathrm{max}}{B^\infty_\mathrm{max}}$, we have shown that
  $1 \leq \frac{B_\mathrm{max}}{B^\infty_\mathrm{max}} \leq 2 c \sqrt{\epsilon n}$.
  This concludes the proof.
\end{proof}


\subsection{Flat Block Butterfly + Low-rank Analysis}
\label{subsec:flat_butterfly_lr_proofs}

We show that flat butterfly + low-rank (an instance) of sparse + low-rank, is
more expressive than either sparse or low-rank alone.
We adapt the argument from~\citet{scatterbrain} to show a generative process
where the attention matrix can be well approximated by a flat butterfly +
low-rank matrix, but not by a sparse or low-rank alone.

We describe here a generative model of an input sequence to attention, parameterized by the inverse temperature $\beta \in \mathbb{R}$ and the intra-cluster distance $\Delta \in \mathbb{R}$.
\begin{process}
  \label{ex:generative}
  Let $Q \in \mathbb{R}^{n \times d}$, where $d\geq\Omega(\log^{3/2}(n))$, with every row of $Q$ generated randomly as follows:
  \begin{enumerate}[leftmargin=*,nosep,nolistsep]
    \item For $C = \Omega(n)$, sample $C$ number of cluster centers $c_1, \dots, c_C \in \mathbb{R}^{d}$ independently from $\mathcal{N}(0, I_d/\sqrt{d})$.
    \item For each cluster around $c_i$, sample $n_i = b$ number of elements around $c_i$, of the form $z_{ij} = c_i + r_{ij}$ for $j = 1, \dots, n_i$ where $r_{ij} \sim \mathcal{N}(0, I_d \Delta/\sqrt{d})$.
    Assume that the total number of elements is $n = c b$ and $\Delta\leq O(1/\log^{1/4} n)$.
  \end{enumerate}
  Let $Q$ be the matrix whose rows are the vectors $z_{ij}$ where $i = 1, \dots, C$ and $j = 1, \dots, n_i$.
  Let $A = Q Q^\top$ and let the attention matrix be $M_\beta = \exp(\beta \cdot A)$.
\end{process}

\begin{theorem}
  \label{thm:temperature}
  Let $M_\beta$, be the attention matrix in~\cref{ex:generative}. Fix $\epsilon\in (0,1)$. Let $R \in \mathbb{R}^{n \times n}$ be a matrix.
  Consider low-rank, sparse, and sparse + low-rank approximations to $M_\beta$.
    Assume $(1 - \Delta^2) \log n  \leq \beta \leq O(\log n)$.
    \begin{enumerate}
        \item \textbf{Flat butterfly + low-rank}: There exists a flat butterfly + low-rank $R$ with $n^{1+o(1)}$ parameters with $\|M_\beta-R\|_F\leq \eps n$.
        \item \textbf{Low-rank}: If $R$ is such that $n-\rank(R)=\Omega(n)$, then $\|M_\beta-R\|_F\geq \Omega(n)$.
        \item \textbf{Sparse}: If $R$ has sparsity $o(n^2)$, then $\|M_\beta - R\|_F\geq \Omega(n)$.
   \end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch]
  As the argument is very similar to that of \citet[Theorem 1]{scatterbrain}, we
  describe here the modifications needed to adapt their proof.

  The main difference between our generative process and that of
  \citet{scatterbrain} is that each cluster has the same number of elements,
  which is the same as the block size.
  The resulting attention matrix will have a large block diagonal component,
  similar to that \citet{scatterbrain}.
  However, all the blocks in the block diagonal component has the same block
  size, which is $b$.
  Moreover, a flat block butterfly of block size $b$ contains a block diagonal
  component of block size $b$.
  Therefore, this flat block butterfly matrix plays the same role as the sparse
  matrix in the proof of \citet{scatterbrain}.
  The rest of the argument follows that of theirs.
\end{proof}

