\section{Problem Setting}
\label{sec:problem_formulation}
We first define the problem as sparse matrix approximation with a simple hardware cost model. Then we briefly introduce butterfly and sparse + low-rank matrices. 

\begin{wrapfigure}{}{0.25\textwidth}
  \captionsetup{font=small}
  \vspace{-2em}
  \centering
  \includegraphics[width=0.7\linewidth]{figs/hardware.pdf}
  \vspace{-1em}
  \caption{Visualization of memory access for a hardware with block size 4: accessing the one (red) location means accessing the full $4\times4$ block (blue).}
  \label{fig:hardware}
  \vspace{-1em}
\end{wrapfigure}
\textbf{Problem Formulation:} We focus on the training of GEMM-based models, which can be viewed as a series of matrix multiplies (Given $A, B \in R^{n \times d}$, compute $C = AB^T$). Speeding up training while maintaining model quality can be mapped to finding an approximation procedure $f$ which reduces the time $T$ of computing $C$ while minimizing error $\mathbf{E}[\Vert f(A, B)-AB^T\Vert^2_F]$. Since the hardware is a block device, accessing any individual element within a block of memory is the same as accessing the full block~\citep{cook2012cuda} (\cref{fig:hardware}). A simple cost model of $T$ on hardware with block size $b$ would depend on the number of $b$-blocks being accessed and compute time
(formal definition in~\cref{app:problem_formulation}).
Our experiment (\cref{app:study}) reveals that when the non-zeros are grouped into blocks, picking the smallest block size supported by hardware can speed up operations by 10$\times$ compared to sparsity patterns that are not block-aligned.


\textbf{Butterfly, Sparse + Low-rank Matrices:} Butterfly matrices have been used in numerical linear algebra~\citep{parker1995random, li2015butterfly} and machine learning~\citep{mathieu2014fast, jing2017tunable, munkhoeva2018quadrature, dao2019learning, choromanski2019unifying}. They encode the recursive divide-and-conquer structure of the fast Fourier transform (FFT) algorithm~\citep{cooley1965algorithm} and provably capture any sparse matrix with near-optimal space and time complexity. Sparse and Low-rank structures have been studied in Robust PCA~\citep{candes2011robust}, graph clustering~\citep{jalali2011clustering}, and co-variance estimation~\citep{luo2011high}. Recently it has been adopted in attention approximation for Transformers~\citep{scatterbrain}. 