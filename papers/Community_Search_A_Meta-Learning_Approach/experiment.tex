\section{Experimental Studies}
\label{sec:exp}


We introduce the experimental setup (\cref{sec:exp:setup}) and report our  substantial results as follows:
\ding{172} compare the effectiveness of CGNP under different task configurations (\cref{sec:exp:effect}),
\ding{173} evaluate the efficiency of CGNP with the baselines, and conduct scalability test for learning-based approaches (\cref{sec:exp:effic}),
\ding{174} investigate the effect of the volume of the ground-truth on the performance of CGNP (\cref{sec:exp:labels}), and 
\ding{175} conduct the ablation studies on the CGNP model regarding the GNN layer and the commutative operation (\cref{sec:exp:ablation}). 

 
\subsection{Experimental Setup}
\label{sec:exp:setup}

\stitle{Datasets:} We use 6 real-world graph datasets, including five
single graphs (\Cora, \Citeseer, \Arxiv, \Reddit, \DBLP) and one
multiple graph (\Facebook).  Table~\ref{tab:dataset} lists the profile
of the 6 datasets.
%
\Cora, \Citeseer and \Arxiv are citation networks whose nodes
represent research papers and edges represent citation
relationships. We use node class labels to simulate the communities
derived from the paper citation, which reveal the research topics that
papers belong to.
%
\DBLP \cite{DBLP} is a co-authorship network where nodes represent
authors and two authors are connected if they collaborate on at least
one paper. A ground-truth community is by the publication venue.
%
% This dataset has ground-truth communities, i.e.,
% the publication venues of the papers.
%
\Reddit is collected from an online discussion forum, where nodes
refer to posts, and an edge between two posts exists if a user
comments on both of the posts.  The ground-truth is the communities
that posts belong to.
%
\Facebook is a dataset containing 10
ego-centric social networks, which have friendship community
ground-truth.
%
\Cora, \Citeseer, and \Facebook have discrete node attributes. The
attributes of \Cora and \Citeseer are the keywords in the papers and
the attributes of \Facebook are the user properties.  For \Cora,
\Citeseer, and \Facebook, we use one-hot representations of the
attributes as the node features, concatenating with the core number
and local cluster coefficient of the node.  We use core number and
local cluster coefficient alone as node features, for \Arxiv, \DBLP
and \Reddit, as they do not have node attributes. 


\begin{table}[t]
	\vspace{-0.4cm}
	\caption{Profile of Datasets}
	\label{tab:dataset}
	\vspace{-0.2cm}
	\centering
	\tiny
	\resizebox{0.4\textwidth}{!}{
	\begin{tabular}{|c|r|r| r| r| r|}
		\hline
		\multicolumn{2}{|c|}{\textbf{Dataset}}   & \textbf{$|V(G)|$} &\textbf{ $|E(G)|$}  & \textbf{$ |\mathcal{A}| $} & \textbf{$| \mathcal{C}(G) |$} \\\hline 
		\multicolumn{2}{|c|}{\Cora}    & 2,708 & 5,429  & 1,433  &7\\         
		\multicolumn{2}{|c|}{\Citeseer} & 3,327 & 4,732 & 3,703 & 6\\
		\multicolumn{2}{|c|}{\Arxiv}&199,343 &1,166,243&N/A&40\\	
		\multicolumn{2}{|c|}{\DBLP} &317,080 &1,049,866  &N/A&5,000\\  
		\multicolumn{2}{|c|}{\Reddit}&232,965&114,615,892&N/A & 50 \\                                \hline
		\multirow{10}{*}{\begin{tabular}[c]{@{}c@{}}\Facebook \end{tabular}} 
		&0   & 348 & 2,867  & 224 & 24 \\ 
		&107 & 1,046    & 27,795  &  576   & 9  \\
		& 348 & 228    &3,420  &   162  &  14   \\
		& 414    & 160    & 1,853  & 105  & 7   \\
		& 686 & 171    &1,827  &   63   &  14  \\
		& 698 & 67    &337  &   48   &  13  \\
		& 1684 & 793    & 14,817  &   319 & 17\\
		& 1912 & 756    &30,781  &   480 & 46\\
		& 3437 & 548    &5,361  & 262  & 32\\
		& 3980 & 60    &206  &  42  & 17\\ \hline
	\end{tabular}
	}
	\vspace{-0.4cm}
\end{table}

\stitle{Tasks \& Queries:} We test our CGNP in different subgraphs of
same graph, different graphs, and different application scenarios,
% we
% use the datasets in Table~\ref{tab:dataset}
%
following the 4 different types of tasks described in \cref{sec:problem}:
%
\ding{172} Single Graph Shared Communities Task (\SGSC), 
\ding{173} Single Graph Disjoint Communities Task (\SGDC), 
\ding{174} Multiple Graphs from One Domain Task (\MGOD), and   
\ding{175} Multiple Graphs from Different Domains Task (\MGDD).
%
% \kfadd{
For \SGSC, \SGDC and \MGDD, one task is generated by sampling a
subgraph of 200 nodes by BFS.
% in \Cora, \Citeseer, \Arxiv, \Reddit, and \DBLP.
The query nodes are randomly drawn from a sampled subgraph,
$G$, where we assign 1 or 5 query nodes to the support set $\support$,
i.e., 1-shot or 5-shot tasks, and assign 30 query nodes to the query
set $\query$ disjointly.  It is worth noting that
%
% the difference
% between \SGSC and \SGDC lies in
%
the query nodes
%
% in $\support$ and $\query$
%
may be from the same ground-truth communities for \SGSC whereas the
query nodes must be from disjoint communities for \SGDC.
%
For each query $q$, we randomly drawn 5 positive samples from the
community of $q$, $\mathcal{C}_{q}(G)$, to construct $l_q^{+}$ and 10
negative samples from $V(G) \setminus \mathcal{C}_{q}(G)$ to construct
$l_q^{-}$.
%
Here, for \SGSC and \SGDC, we
generate 100 training tasks for \Cora, \Citeseer, \Arxiv, \Reddit and
\DBLP, and generate 50 valid tasks and 50 test tasks for the five
datasets, respectively.
%
For \MGOD, we use one \Facebook ego-network as the graph in one task,
and sample the same numbers of queries and labels as discussed
above. Ten tasks are split into 6 for training, 2 for validation, and
2 for testing.
%
For \MGDD, we also generate 100 tasks of \Citeseer for training, 50
tasks of \Cora for validation, and 50 tasks of \Cora for testing,
denoted as \Citeseercora.
% }


\stitle{Baselines:} To comprehensively evaluate the performance of
CGNP framework for CS, we compare with {10} baseline approaches,
including {3} graph algorithms, {4} naive approaches discussed in
\cref{sec:naive}, {3} traditional ML/DL-based approaches.
%
%  We briefly introduce these baselines as below.
%
\ding{182} Attributed Truss Community Search (\ATC)~\cite{ATC}. It is
an attributed community search algorithm given the input of query
nodes and attributes. Firstly, it finds the maximal $(k, d)$-truss
containing the query nodes. Then, the algorithm iteratively removes
unpromising nodes from the truss, which has a small attribute score.
%
% The time and space complexity of \ATC is $O(m|\mathcal{A}_q| +
% m + n \log n)$ and $O(m + \sum_{v \in V(G)} \mathcal{A}(v))$,
% respectively, where $n$ and $m$ are the numbers of nodes and edges of
% the graph, and $|\mathcal{A}_q|$ is the number of given query
% attributes.
%
\ding{183} Attributed Community Query (\ACQ)~\cite{ACQ}. It aims to
find subgraph whose nodes are tightly connected and share common
attributes with the given query node.
%
% The algorithm is impractical for query
% nodes with a large number of attributes due to combinatorial explosion
% during enumerating the shared attribute sets.
%
\ding{184} Closest Truss Community (\CTC)~\cite{CTC}. It is a
$k$-truss based community search framework for non-attributed
graphs. Given a set of query nodes, $Q$, a greedy algorithm finds a
$k$-truss with the largest $k$ that contains $Q$ and has the minimum
diameter among the truss.
%
% \CTC takes $O((|Q|t+\rho)m)$ time and $O(m)$ space, where
% $\rho$ is the arboricity of the graph, $t$ is the number of iterations
% and $m$ is the number of edges for the maximal connected $k$-truss.
%	
\ding{185} Model-Agnostic Meta-Learning (\MAML)~\cite{MAML}. We use 
GNN as the base model. The task-specific parameters of  GNN are
updated in an inner loop as Eq.~(\ref{eq:maml:inner}), and the
task-common parameters are updated in an outer loop as
Eq.~(\ref{eq:maml:outer}) over all training tasks.
%
\ding{186} First-Order Meta-Learning (\Reptile)~\cite{reptile}. As a
first-order alternative of \MAML, \Reptile adopts the same GNN as the
base model. Task-common parameters are updated by
Eq.~(\ref{eq:reptile:outer}) in an outer loop, over all the training
tasks.
%
\ding{187} Feature Transfer (\Featrans). A base GNN model is
pre-trained on all the training tasks. For a test task $\task^*=
(\support^*, \query^*)$, the final layer of the GNN is finetuned on
the support set $\support^*$ by one gradient step, while all the other
parameters are kept intact.
%
\ding{188} Graph Prototypical Network (\PN).  For each query $q$, 3
positive samples and 3 negative samples are randomly drawn from $l_q$
to compute the query-specific prototypes. We use Euclidean distance as
the distance function in Eq.~(\ref{eq:gpn:likelihood}).
%
\ding{189} Supervised GNN (\Supervise). One GNN model is trained for
each test task from scratch by the few-shot data in $\support^*$.
%
\ding{190} ICS-GNN (\ICSGNN) \cite{ICSGNN}. For each query node $q$, a
GNN model is trained by some positive and negative samples and
predicts a score for the remaining nodes.  Then, the algorithm finds a
subgraph connected to $q$, with a fixed number of nodes, aiming to
maximize the summation of the scores predicted by GNN.
%
% \kfadd{
\ding{191} AQD-GNN (\AQDGNN) \cite{AQDGNN}. The setting is similar
to \Supervise. For each test task, \AQDGNN trains the model from
scratch by the few-shot data in $\support^*$ and test in $\query^*$.
%}
It is worth noting that \PN and \ICSGNN are different
from other learning-based approaches, where test query nodes are
required to have ground-truth. \PN uses the ground-truth to compute
the query-specific prototypes while \ICSGNN uses the ground-truth to
train a query-specific model.  These two approaches \emph{cannot fully
  generalize} to query nodes without any prior knowledge of
membership.


%%%%%%%%%%%%%%%%%%%%
\stitle{Implementation and Settings:} 
We give the settings of 8 ML approaches, including our CGNP and 7 baselines, \MAML, \Reptile, \Featrans, \PN, \Supervise, \ICSGNN and \AQDGNN.
For the GNN encoder of CGNP and the base GNN models of the 6 baselines, the number of the GNN layers is 3, where each GNN layer has 128 hidden units and a Dropout probability of 0.2 by default.

We investigate popular GNN layers, including the vanilla Graph Convolutional Network (\GCN)~\cite{GCN}, Graph Attention Network (\GAT)~\cite{GAT} and \SAGE~\cite{SAGE}, and finally choose 
\GAT by default due to its high performance. 
For the MLP decoder of CGNP, we use a two-layer MLP with 512 hidden units. 
For the GNN decoder of CGNP, we use a two-layer GNN which has the same configuration as the encoder. 

The learning framework of CGNP and the 7 ML baselines are built on
PyTorch~\cite{pytorch} with PyTorch Geometric~\cite{torchgeo}.
% \kfadd{
We use Adam optimizer with a learning rate of $5 \times
  10^{-4}$ to train CGNP, \PN, \ICSGNN, \Supervise, and \Featrans by
  200 epochs.  For \MAML and \Reptile, the inner loop performs 10
  gradient steps for training and 20 steps for testing, with a
  learning rate of $5\times 10^{-4}$, and the learning rate for the
  outer loop is $10^{-3}$.
% }
It is worth mentioning that the performance of CGNP is robust in the
range of empirical training hyper-parameters.  By default, the
training and prediction are conducted on a Tesla V100 with 16GB
memory.  \ATC, \ACQ and \CTC are tested on the same Linux server with
32 Intel(R) Silver 4,215 CPUs and 128GB RAM.

\stitle{Evaluation Metrics:} To evaluate the quality of the found result, we use accuracy, precision, recall and \Fone-score between the prediction and the ground-truth. 
\comment{Suppose $\hat{l} \in \{0, 1\}^{n}$ and $l \in \{0, 1\}^{n}$ are the binary representations of the prediction result and the ground-truth of a query $q$. Precision, recall and \Fone-score are defined as below:
\begin{align}
	\nonumber
	\Pre(\hat{l},l)&=\frac{\sum_{v \in V(G)} \hat{l}(v) \& l(v)}{\sum_{v \in V(G)}\hat{l}(v) }, 
	\Rec(\hat{l},l)=\frac{\sum_{v \in V(G)} \hat{l}(v) \& l(v)}{\sum_{v \in V(G)}{l(v)}}  \\
	\nonumber
	\Fone(\hat{l},l)&=\frac{2\cdot \Rec(\hat{l},l)\cdot \Pre(\hat{l},l)}{ \Rec(\hat{l},l) + \Pre(\hat{l},l)}
\end{align}
}
\Fone-score is the harmonic average of precision and recall, which better reflects the overall performance.
\begin{table*}[t]
	
	\centering
	\caption{Performance on \SGSC and \SGDC Tasks (First and Second Best \Fone Scores are Highlighted)}
	\vspace{-0.2cm}
	\label{tab:result1}
	\resizebox{1\textwidth}{!}{
		\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
			\hline
			\multicolumn{1}{|c|}{\multirow{3}{*}{Dataset}} & \multicolumn{1}{c|}{\multirow{2}{*}{Task config.}} & \multicolumn{8}{c|}{Single Graph with Shared Communities}                                                                                                                                                               & \multicolumn{8}{c|}{Single Graph with Disjoint Communities}                                                                                                                                                            \\\cline{3-18}
			\multicolumn{1}{|c|}{}                         & \multicolumn{1}{c|}{}                                & \multicolumn{4}{c|}{1-shot}                                                                               & \multicolumn{4}{c|}{5-shot}                                                                              & \multicolumn{4}{c|}{1-shot}                                                                              & \multicolumn{4}{c|}{5-shot}                                                                              \\\cline{2-18}
			\multicolumn{1}{|c|}{}                         & \multicolumn{1}{c|}{Methods}                         & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{l|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} \\\hline 
			
			\comment{
				\multirow{11}{*}{\rotatebox{90}{\Cora}} 
				&\ATC        & 0.6427 & 0.8246 & 0.1965 & 0.3174 & 0.6313 & 0.8205 & 0.1922 & 0.3114 & 0.6481 & 0.6805 & 0.0714 & 0.1292 & 0.6342 & 0.7125 & 0.0694 & 0.1264 \\
				&\CTC        & 0.5884 & 0.8728 & 0.0305 & 0.0590 & 0.5773 & 0.8726 & 0.0301 & 0.0581 & 0.6446 & 0.8538 & 0.0341 & 0.0656 & 0.6294 & 0.8724 & 0.0339 & 0.0652 \\\cline{2-18}
				&\MAML       & 0.5747 & 0.4962 & 0.4130 & 0.4508 & 0.5941 & 0.5542 & 0.3288 & 0.4128 & 0.6332 & 0.4984 & 0.4458 & 0.4706 & 0.7067 & 0.6337 & 0.5484 & 0.5879 \\
				&\Reptile    & 0.5695 & 0.4894 & 0.4269 & 0.4560 & 0.5770 & 0.5224 & 0.2923 & 0.3749 & 0.6620 & 0.5427 & 0.4822 & 0.5107 &0.7247       & 0.6612       &0.5639        &\cellcolor{LightCyan}{0.6087}        \\
				&\Featrans  & 0.5779 & 0.5022 & 0.1653 & 0.2488 & 0.5686 & 0.5125 & 0.1151 & 0.1880 & 0.6100 & 0.4425 & 0.2551 & 0.3236 & 0.6045 & 0.4402 & 0.1337 & 0.2051 \\
				&\PN        & 0.1541 & 0.1223 & 0.1540 & 0.1363 & 0.1913 & 0.2035 & 0.2378 & 0.2193 & 0.3099 & 0.3823 & 0.4088 & 0.3951 & 0.1314 & 0.1730 & 0.1745 & 0.1738 \\\cline{2-18}
				&\Supervise & 0.5869 & 0.5129 & 0.4506 & 0.4797 & 0.5870 & 0.5287 & 0.4416 & 0.4813 & 0.6799 & 0.5639 & 0.5512 & \cellcolor{LightRed}{0.5575} & 0.7318 & 0.6507 & 0.6417 & \cellcolor{LightRed}{0.6462} \\
				&\ICSGNN    & 0.6151 & 0.6424 & 0.2103 & 0.3169 & 0.6105 & 0.6489 & 0.2093 & 0.3165 & 0.6637 & 0.6052 & 0.2365 & 0.3400 & 0.6539 & 0.6300 & 0.2353 & 0.3427 \\\cline{2-18}
				&\CGNPIP    & 0.6033 & 0.5202 & 0.7891 & \cellcolor{LightRed}{0.6271 } &0.6025 & 0.5281 & 0.7885 & \cellcolor{LightRed}{0.6325} & 0.5115 & 0.4005 & 0.6757 & 0.5029 & 0.5222 & 0.4233 & 0.6957 & 0.5264 \\
				&\CGNPMLP   & 0.6143 & 0.5313 & 0.7425 & 0.6194 & 0.6100 & 0.5360 & 0.7526 & 0.6261 & 0.5223 & 0.4005 & 0.6155 & 0.4852 & 0.5322 & 0.4257 & 0.6469 & 0.5135 \\
				&\CGNPGNN   & 0.5903 & 0.5099 & 0.7914 & \cellcolor{LightCyan}{0.6202} & 0.5844 & 0.5134 & 0.8058 & \cellcolor{LightCyan}{0.6272} & 0.5121 & 0.4041 & 0.7035 & \cellcolor{LightCyan}{0.5134} & 0.5114 & 0.4165 & 0.6998 & 0.5222\\\hline\hline
			}
			
			\multirow{11}{*}{\rotatebox{90}{\Citeseer}} 
			&\ATC       & 0.4759 & 0.8366 & 0.1044 & 0.1856 & 0.4623 & 0.8344 & 0.1005 & 0.1793 & 0.5393 & 0.8288 & 0.1131 & 0.1990 & 0.5373 & 0.8357 & 0.1144 & 0.2013 \\
			&\CTC       & 0.4386 & 0.8585 & 0.0226 & 0.0440 & 0.4264 & 0.8653 & 0.0225 & 0.0439 & 0.5043 & 0.8262 & 0.0262 & 0.0508 & 0.5010 & 0.8293 & 0.0261 & 0.0507 \\\cline{2-18}
			&\MAML      & 0.5293 & 0.6450 & 0.3942 & 0.4894 & 0.5494 & 0.6937 & 0.4108 & 0.5160 & 0.5528 & 0.5835 & 0.4071 & 0.4796 & 0.5738 & 0.6277 & 0.4022 & 0.4903 \\
			&\Reptile   & 0.5474 & 0.6382 & 0.4825 & 0.5495 & 0.5550 & 0.6886 & 0.4363 & 0.5342 & 0.5812 & 0.6038 & 0.5022 & 0.5483 & 0.5970 & 0.6500 & 0.4531 & 0.5340 \\
			&\Featrans  & 0.4719 & 0.6625 & 0.1571 & 0.2540 & 0.4548 & 0.6692 & 0.1337 & 0.2229 & 0.5044 & 0.5346 & 0.1602 & 0.2465 & 0.4925 & 0.5127 & 0.0819 & 0.1413 \\
			&\PN        & 0.1744 & 0.1159 & 0.1564 & 0.1332 & 0.1383 & 0.1208 & 0.1441 & 0.1314 & 0.4498 & 0.4632 & 0.6199 & 0.5302 & 0.2957 & 0.3960 & 0.3263 & 0.3578 \\\cline{2-18}
			&\Supervise & 0.5492 & 0.6574 & 0.4429 & 0.5293 & 0.5688 & 0.6895 & 0.4780 & 0.5646 & 0.5751 & 0.6072 & 0.4544 & 0.5198 & 0.6221 & 0.6692 & 0.5110 & 0.5795 \\
			&\ICSGNN    & 0.4856 & 0.7115 & 0.1783 & 0.2852 & 0.4793 & 0.7094 & 0.1760 & 0.2821 & 0.5424 & 0.6738 & 0.1925 & 0.2994 & 0.5411 & 0.6658 & 0.1905 & 0.2963 \\
			&\AQDGNN    &0.5036	&0.5993	&0.4406	&0.5079  &0.5263	&0.5806	&0.6816	&0.6270 &0.5558	&0.5717	&0.5291	&0.5496 &0.5072	&0.5100	&0.7761	&0.6155\\\cline{2-18}
			&\CGNPIP    & 0.6076 & 0.6429 & 0.7071 & \cellcolor{LightCyan}{0.6734} & 0.6150 & 0.6562 & 0.7176 & \cellcolor{LightCyan}{0.6855} & 0.5611 & 0.5488 & 0.7469 & \cellcolor{LightCyan}{0.6327} & 0.5626 & 0.5515 & 0.7584 & \cellcolor{LightCyan}{0.6386} \\
			&\CGNPMLP   & 0.6041 & 0.6556 & 0.6490 & 0.6523 & 0.6160 & 0.6710 & 0.6735 & 0.6723 & 0.5510 & 0.5427 & 0.7174 & 0.6179 & 0.5773 & 0.5633 & 0.7588 & \cellcolor{LightRed}{0.6466} \\
			&\CGNPGNN   & 0.6133 & 0.6393 & 0.7443 & \cellcolor{LightRed}{0.6878} &  0.6158 & 0.6513 & 0.7367 &\cellcolor{LightRed} {0.6914} & 0.5685 & 0.5527 & 0.7730 & \cellcolor{LightRed}{0.6446} & 0.5765 & 0.5631 & 0.7532 & 0.6444\\\hline\hline
			
			\multirow{11}{*}{\rotatebox{90}{\Arxiv}} 
			&\ATC       & 0.5802 & 0.7253 & 0.0734 & 0.1333 & 0.5850 & 0.7349 & 0.0757 & 0.1373 & 0.5767 & 0.7875 & 0.0542 & 0.1015 & 0.5804 & 0.7930 & 0.0557 & 0.1042 \\
			&\CTC       & 0.5751 & 0.7693 & 0.0484 & 0.0911 & 0.5795 & 0.7783 & 0.0501 & 0.0942 & 0.5733 & 0.8160 & 0.0411 & 0.0782 & 0.5766 & 0.8200 & 0.0415 & 0.0790 \\\cline{2-18}
			&\MAML      & 0.5674 & 0.6512 & 0.0355 & 0.0673 & 0.5903 & 0.8005 & 0.0806 & 0.1465 & 0.5692 & 0.7345 & 0.0355 & 0.0676 & 0.5770 & 0.7221 & 0.0544 & 0.1011 \\
			&\Reptile   & 0.5762 & 0.6409 & 0.0829 & 0.1468 & 0.5888 & 0.8260 & 0.0726 & 0.1334 & 0.5697 & 0.6034 & 0.0693 & 0.1242 & 0.5719 & 0.6804 & 0.0409 & 0.0771 \\
			&\Featrans  & 0.5735 & 0.6527 & 0.0647 & 0.1177 & 0.5762 & 0.6626 & 0.0577 & 0.1062 & 0.5744 & 0.7288 & 0.0546 & 0.1016 & 0.5775 & 0.7066 & 0.0589 & 0.1087 \\
			&\PN        & 0.2588 & 0.2458 & 0.2061 & 0.2242 & 0.2754 & 0.2773 & 0.2453 & 0.2603 & 0.4681 & 0.6257 & 0.5354 & \cellcolor{LightCyan}0.5771 & 0.4195 & 0.4867 & 0.6055 & 0.5397 \\\cline{2-18}
			&\Supervise & 0.5531 & 0.4742 & 0.1488 & 0.2265 & 0.5816 & 0.6512 & 0.0877 & 0.1545 & 0.5461 & 0.4508 & 0.1364 & 0.2094 & 0.5791 & 0.6245 & 0.0957 & 0.1659 \\
			&\ICSGNN    & 0.5904 & 0.6004 & 0.2016 & 0.3019 & 0.5896 & 0.5995 & 0.2011 & 0.3012 & 0.5968 & 0.6224 & 0.2153 & 0.3199 & 0.5999 & 0.6220 & 0.2168 & 0.3215 \\
			&\AQDGNN    & 0.5183 & 0.4622 & 0.5217 & 0.4901 & 0.4821 & 0.4425 & 0.7266 & 0.5501 & 0.5215 & 0.4573 & 0.5105 & 0.4824 & 0.5069 & 0.4619 & 0.7220 & 0.5633 \\\cline{2-18}
			&\CGNPIP    & 0.5172 & 0.4716 & 0.8118 & \cellcolor{LightCyan}{0.5966} & 0.5520 & 0.4915 & 0.7889 & \cellcolor{LightRed}{0.6057} & 0.5699 & 0.5076 & 0.8067 & \cellcolor{LightRed}{0.6231} & 0.5856 & 0.5170 & 0.8083 & \cellcolor{LightRed}{0.6306} \\
			&\CGNPMLP   & 0.5079 & 0.4649 & 0.7870 & 0.5845 & 0.5642 & 0.5003 & 0.7371 & 0.5960 & 0.5365 & 0.4806 & 0.6397 & 0.5489 & 0.5847 & 0.5194 & 0.6841 & 0.5905 \\
			&\CGNPGNN   & 0.4699 & 0.4496 & 0.9161 & \cellcolor{LightRed}{0.6032} &  0.4649 & 0.4449 & 0.9205 & \cellcolor{LightCyan}{0.5998} & 0.4938 & 0.4548 & 0.7464 & 0.5652  &0.5520	&0.4950	&0.8399	&\cellcolor{LightCyan}{0.6229}\\\hline\hline
			
			
			\multirow{11}{*}{\rotatebox{90}{\Reddit}} 
			&\ATC       & 0.6574 & 0.3566 & 0.4286 & 0.3893 & 0.6582 & 0.3553 & 0.4282 & 0.3883 & 0.4784 & 0.9586 & 0.4108 & 0.5752 & 0.4787 & 0.9572 & 0.4136 & 0.5776 \\
			&\CTC       & 0.6614 & 0.3593 & 0.4202 & 0.3874 & 0.6627 & 0.3583 & 0.4190 & 0.3863 & 0.4713 & 0.9593 & 0.4019 & 0.5664 & 0.4722 & 0.9577 & 0.4054 & 0.5697 \\\cline{2-18}
			&\MAML      & 0.7450 & 0.3254 & 0.0007 & 0.0014 & 0.7465 & 0.3812 & 0.0010 & 0.0020 & 0.4679 & 0.9864 & 0.3861 & 0.5550 & 0.5017 & 0.9863 & 0.4277 & 0.5967 \\
			&\Reptile   & 0.7414 & 0.3039 & 0.0116 & 0.0224 & 0.7447 & 0.2874 & 0.0050 & 0.0098 & 0.4051 & 0.9904 & 0.3107 & 0.4730 & 0.4046 & 0.9907 & 0.3121 & 0.4746 \\
			&\Featrans  & 0.7327 & 0.2972 & 0.0361 & 0.0644 & 0.7393 & 0.3224 & 0.0261 & 0.0484 & 0.2784 & 0.9369 & 0.1719 & 0.2906 & 0.2345 & 0.8634 & 0.1328 & 0.2302 \\
			&\PN        & 0.4731 & 0.2285 & 0.5556 & 0.3238 & 0.5051 & 0.2253 & 0.5379 & 0.3175 & 0.6708 &0.9891 &0.6749 &0.8024 & 0.6622 &0.9871 &0.6675 &0.7965 \\\cline{2-18}
			&\Supervise & 0.7079 & 0.2677 & 0.0845 & 0.1284 & 0.7288 & 0.2546 & 0.0364 & 0.0637 & 0.5834 & 0.9736 & 0.5296 & 0.6860 & 0.5536 & 0.9827 & 0.4907 & 0.6545 \\
			&\ICSGNN    & 0.6949 & 0.3331 & 0.1959 & 0.2467 & 0.6975 & 0.3361 & 0.1990 & 0.2500 & 0.2748 & 0.9460 & 0.1652 & 0.2813 & 0.2725 & 0.9499 & 0.1652 & 0.2815 \\
			&\AQDGNN    & 0.5221 & 0.2514 & 0.4427 & 0.3207 & 0.4141 & 0.2616 & 0.7199 & 0.3837 & 0.6476 & 0.8851 & 0.6772 & 0.7673 & 0.7830 & 0.9139 & 0.8250 & 0.8672 \\\cline{2-18}
			&\CGNPIP    & 0.2557 & 0.2549 & 0.9991 & \cellcolor{LightRed}{0.4062} & 0.2543 & 0.2534 & 0.9983 & 0.4042 & 0.7885 & 0.9264 & 0.8184 & 0.8691 & 0.8482 & 0.9110 & 0.9122 & 0.9116 \\
			&\CGNPMLP   & 0.2566 & 0.2544 & 0.9934 & 0.4051 & 0.2774 & 0.2557 & 0.9694 & \cellcolor{LightRed}{0.4047} & 0.8229 & 0.9397 & 0.8479 & \cellcolor{LightCyan}{0.8915} & 0.8697 & 0.9508 & 0.8945 & \cellcolor{LightRed}{0.9218} \\
			&\CGNPGNN   & 0.2548 & 0.2548 & 1.0000 & \cellcolor{LightCyan}{0.4061} & 0.2534 & 0.2534 & 1.0000 & \cellcolor{LightCyan}{0.4043} & 0.8578 & 0.8578 & 1.0000 & \cellcolor{LightRed}{0.9235} & 0.8584 & 0.8584 & 1.0000 & \cellcolor{LightCyan}{0.9238}\\\hline\hline
			
			\multirow{11}{*}{\rotatebox{90}{\DBLP}} 
			&\ATC       & 0.8376 & 0.8749 & 0.1752 & 0.2919 & 0.8230 & 0.8916 & 0.1676 & 0.2822 & 0.7527 & 0.7539 & 0.0922 & 0.1643 & 0.7360 & 0.7849 & 0.1038 & 0.1834 \\
			&\CTC       & 0.8365 & 0.9107 & 0.1599 & 0.2720 & 0.8216 & 0.9214 & 0.1534 & 0.2629 & 0.7512 & 0.7711 & 0.0803 & 0.1454 & 0.7345 & 0.8012 & 0.0931 & 0.1668 \\\cline{2-18}
			&\MAML      & 0.8161 & 0.6395 & 0.0864 & 0.1522 & 0.8029 & 0.6545 & 0.1065 & 0.1832 & 0.7383 & 0.5337 & 0.0581 & 0.1047 & 0.7201 & 0.5713 & 0.0776 & 0.1366 \\
			&\Reptile   & 0.8106 & 0.5135 & 0.1704 & 0.2559 & 0.7993 & 0.5833 & 0.1162 & 0.1938 & 0.7208 & 0.3890 & 0.1033 & 0.1632 & 0.7184 & 0.5508 & 0.0741 & 0.1306 \\
			&\Featrans  & 0.8194 & 0.6339 & 0.1296 & 0.2152 & 0.8057 & 0.6600 & 0.1315 & 0.2193 & 0.7417 & 0.5736 & 0.0796 & 0.1397 & 0.7238 & 0.6301 & 0.0789 & 0.1402 \\
			&\PN        & 0.1819 & 0.0120 & 0.4528 & 0.0235 & 0.1790 & 0.0748 & 0.6846 & 0.1349 & 0.4017 & 0.2292 & 0.5911 & 0.3303 & 0.3581 & 0.2408 & 0.3988 & 0.3003 \\\cline{2-18}
			&\Supervise & 0.7312 & 0.2142 & 0.1523 & 0.1780 & 0.7773 & 0.3987 & 0.1438 & 0.2113 & 0.6805 & 0.3075 & 0.1692 & 0.2183 & 0.7015 & 0.4255 & 0.1307 & 0.2000 \\
			&\ICSGNN    & 0.7997 & 0.4662 & 0.3571 & \cellcolor{LightRed}{0.4044} & 0.7911 & 0.5030 & 0.3519 & \cellcolor{LightCyan}{0.4141} & 0.7366 & 0.4978 & 0.2304 & 0.3150 & 0.7290 & 0.5414 & 0.2373 & 0.3299 \\
			&\AQDGNN    & 0.6129 & 0.2257 & 0.4220 & 0.2941 & 0.5615 & 0.2705 & 0.6556 & 0.3830 & 0.5421 & 0.2990 & 0.5737 & 0.3931 & 0.4567 & 0.2994 & 0.6992 & 0.4192 \\\cline{2-18}
			&\CGNPIP    & 0.3951 & 0.2206 & 0.8548 & \cellcolor{LightCyan}{0.3507} & 0.5320 & 0.2829 & 0.8175 & \cellcolor{LightRed}{0.4203} & 0.3988 & 0.2779 & 0.8288 & \cellcolor{LightRed}{0.4162} & 0.5166 & 0.3404 & 0.7720 & \cellcolor{LightCyan}{0.4725} \\
			&\CGNPMLP   & 0.4926 & 0.2317 & 0.7147 & 0.3499 & 0.5203 & 0.2487 & 0.6488 & 0.3596 & 0.4437 & 0.2814 & 0.7415 & \cellcolor{LightCyan}{0.4080} & 0.5652 & 0.3632 & 0.7304 & \cellcolor{LightRed}{0.4851} \\
			&\CGNPGNN   & 0.4262 & 0.2223 & 0.8018 & 0.3481 & 0.4535 & 0.2463 & 0.7928 & 0.3759 & 0.3999 & 0.2682 & 0.7644 & 0.3971  & 0.3777 & 0.2894 & 0.8377 & 0.4302
			\\\hline
		\end{tabular}
	}
\end{table*}




\comment{\begin{table*}[t]
		
		\centering
		\caption{Performance on \SGSC and \SGDC Tasks (First and Second Best \Fone Scores are Highlighted)}
		\vspace{-0.2cm}
		\label{tab:result1}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
				\hline
				\multicolumn{1}{|c|}{\multirow{3}{*}{Dataset}} & \multicolumn{1}{c|}{\multirow{2}{*}{Task config.}} & \multicolumn{8}{c|}{Single Graph with Shared Communities}                                                                                                                                                               & \multicolumn{8}{c|}{Single Graph with Disjoint Communities}                                                                                                                                                            \\\cline{3-18}
				\multicolumn{1}{|c|}{}                         & \multicolumn{1}{c|}{}                                & \multicolumn{4}{c|}{1-shot}                                                                               & \multicolumn{4}{c|}{5-shot}                                                                              & \multicolumn{4}{c|}{1-shot}                                                                              & \multicolumn{4}{c|}{5-shot}                                                                              \\\cline{2-18}
				\multicolumn{1}{|c|}{}                         & \multicolumn{1}{c|}{Methods}                         & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{l|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} \\\hline 
				
				\comment{
				\multirow{11}{*}{\rotatebox{90}{\Cora}} 
				&\ATC        & 0.6427 & 0.8246 & 0.1965 & 0.3174 & 0.6313 & 0.8205 & 0.1922 & 0.3114 & 0.6481 & 0.6805 & 0.0714 & 0.1292 & 0.6342 & 0.7125 & 0.0694 & 0.1264 \\
				&\CTC        & 0.5884 & 0.8728 & 0.0305 & 0.0590 & 0.5773 & 0.8726 & 0.0301 & 0.0581 & 0.6446 & 0.8538 & 0.0341 & 0.0656 & 0.6294 & 0.8724 & 0.0339 & 0.0652 \\\cline{2-18}
				&\MAML       & 0.5747 & 0.4962 & 0.4130 & 0.4508 & 0.5941 & 0.5542 & 0.3288 & 0.4128 & 0.6332 & 0.4984 & 0.4458 & 0.4706 & 0.7067 & 0.6337 & 0.5484 & 0.5879 \\
				&\Reptile    & 0.5695 & 0.4894 & 0.4269 & 0.4560 & 0.5770 & 0.5224 & 0.2923 & 0.3749 & 0.6620 & 0.5427 & 0.4822 & 0.5107 &0.7247       & 0.6612       &0.5639        &\cellcolor{LightCyan}{0.6087}        \\
				&\Featrans  & 0.5779 & 0.5022 & 0.1653 & 0.2488 & 0.5686 & 0.5125 & 0.1151 & 0.1880 & 0.6100 & 0.4425 & 0.2551 & 0.3236 & 0.6045 & 0.4402 & 0.1337 & 0.2051 \\
				&\PN        & 0.1541 & 0.1223 & 0.1540 & 0.1363 & 0.1913 & 0.2035 & 0.2378 & 0.2193 & 0.3099 & 0.3823 & 0.4088 & 0.3951 & 0.1314 & 0.1730 & 0.1745 & 0.1738 \\\cline{2-18}
				&\Supervise & 0.5869 & 0.5129 & 0.4506 & 0.4797 & 0.5870 & 0.5287 & 0.4416 & 0.4813 & 0.6799 & 0.5639 & 0.5512 & \cellcolor{LightRed}{0.5575} & 0.7318 & 0.6507 & 0.6417 & \cellcolor{LightRed}{0.6462} \\
				&\ICSGNN    & 0.6151 & 0.6424 & 0.2103 & 0.3169 & 0.6105 & 0.6489 & 0.2093 & 0.3165 & 0.6637 & 0.6052 & 0.2365 & 0.3400 & 0.6539 & 0.6300 & 0.2353 & 0.3427 \\\cline{2-18}
				&\CGNPIP    & 0.6033 & 0.5202 & 0.7891 & \cellcolor{LightRed}{0.6271 } &0.6025 & 0.5281 & 0.7885 & \cellcolor{LightRed}{0.6325} & 0.5115 & 0.4005 & 0.6757 & 0.5029 & 0.5222 & 0.4233 & 0.6957 & 0.5264 \\
				&\CGNPMLP   & 0.6143 & 0.5313 & 0.7425 & 0.6194 & 0.6100 & 0.5360 & 0.7526 & 0.6261 & 0.5223 & 0.4005 & 0.6155 & 0.4852 & 0.5322 & 0.4257 & 0.6469 & 0.5135 \\
				&\CGNPGNN   & 0.5903 & 0.5099 & 0.7914 & \cellcolor{LightCyan}{0.6202} & 0.5844 & 0.5134 & 0.8058 & \cellcolor{LightCyan}{0.6272} & 0.5121 & 0.4041 & 0.7035 & \cellcolor{LightCyan}{0.5134} & 0.5114 & 0.4165 & 0.6998 & 0.5222\\\hline\hline
			    }
		
				\multirow{11}{*}{\rotatebox{90}{\Citeseer}} 
				&\ATC       & 0.4759 & \cellcolor{LightCyan}{0.8366} & 0.1044 & 0.1856 & 0.4623 & \cellcolor{LightCyan}{0.8344} & 0.1005 & 0.1793 & 0.5393 & \cellcolor{LightRed}{0.8288} & 0.1131 & 0.1990 & 0.5373 & \cellcolor{LightRed}{0.8357} & 0.1144 & 0.2013 \\
				&\CTC       & 0.4386 & \cellcolor{LightRed}{0.8585} & 0.0226 & 0.0440 & 0.4264 & \cellcolor{LightRed}{0.8653} & 0.0225 & 0.0439 & 0.5043 & \cellcolor{LightCyan}{0.8262} & 0.0262 & 0.0508 & 0.5010 & \cellcolor{LightCyan}{0.8293} & 0.0261 & 0.0507 \\\cline{2-18}
				&\MAML      & 0.5293 & 0.6450 & 0.3942 & 0.4894 & 0.5494 & 0.6937 & 0.4108 & 0.5160 & 0.5528 & 0.5835 & 0.4071 & 0.4796 & 0.5738 & 0.6277 & 0.4022 & 0.4903 \\
				&\Reptile   & 0.5474 & 0.6382 & 0.4825 & 0.5495 & 0.5550 & 0.6886 & 0.4363 & 0.5342 & \cellcolor{LightRed}{0.5812} & 0.6038 & 0.5022 & 0.5483 & 0.5970 & 0.6500 & 0.4531 & 0.5340 \\
				&\Featrans  & 0.4719 & 0.6625 & 0.1571 & 0.2540 & 0.4548 & 0.6692 & 0.1337 & 0.2229 & 0.5044 & 0.5346 & 0.1602 & 0.2465 & 0.4925 & 0.5127 & 0.0819 & 0.1413 \\
				&\PN        & 0.1744 & 0.1159 & 0.1564 & 0.1332 & 0.1383 & 0.1208 & 0.1441 & 0.1314 & 0.4498 & 0.4632 & 0.6199 & 0.5302 & 0.2957 & 0.3960 & 0.3263 & 0.3578 \\\cline{2-18}
				&\Supervise & 0.5492 & 0.6574 & 0.4429 & 0.5293 & 0.5688 & 0.6895 & 0.4780 & 0.5646 & \cellcolor{LightCyan}{0.5751} & 0.6072 & 0.4544 & 0.5198 & \cellcolor{LightRed}{0.6221} & 0.6692 & 0.5110 & 0.5795 \\
				&\ICSGNN    & 0.4856 & 0.7115 & 0.1783 & 0.2852 & 0.4793 & 0.7094 & 0.1760 & 0.2821 & 0.5424 & 0.6738 & 0.1925 & 0.2994 & 0.5411 & 0.6658 & 0.1905 & 0.2963 \\
				&\AQDGNN    &0.5036	&0.5993	&0.4406	&0.5079  &0.5263	&0.5806	&0.6816	&0.6270 &0.5558	&0.5717	&0.5291	&0.5496 &0.5072	&0.5100	&\cellcolor{LightRed}{0.7761}	&0.6155\\\cline{2-18}
				&\CGNPIP    & \cellcolor{LightCyan}{0.6076} & 0.6429 & \cellcolor{LightCyan}{0.7071} & \cellcolor{LightCyan}{0.6734} & 0.6150 & 0.6562 & \cellcolor{LightCyan}{0.7176} & \cellcolor{LightCyan}{0.6855} & 0.5611 & 0.5488 & \cellcolor{LightCyan}{0.7469} & \cellcolor{LightCyan}{0.6327} & 0.5626 & 0.5515 & 0.7584 & \cellcolor{LightCyan}{0.6386} \\
				&\CGNPMLP   & 0.6041 & 0.6556 & 0.6490 & 0.6523 & \cellcolor{LightRed}{0.6160} & 0.6710 & 0.6735 & 0.6723 & 0.5510 & 0.5427 & 0.7174 & 0.6179 & \cellcolor{LightCyan}{0.5773} & 0.5633 & \cellcolor{LightCyan}{0.7588} & \cellcolor{LightRed}{0.6466} \\
				&\CGNPGNN   & \cellcolor{LightRed}{0.6133} & 0.6393 & \cellcolor{LightRed}{0.7443} & \cellcolor{LightRed}{0.6878} &  \cellcolor{LightCyan}{0.6158} & 0.6513 & \cellcolor{LightRed}{0.7367} &\cellcolor{LightRed} {0.6914} & 0.5685 & 0.5527 & \cellcolor{LightRed}{0.7730} & \cellcolor{LightRed}{0.6446} & 0.5765 & 0.5631 & 0.7532 & 0.6444\\\hline\hline
				
				\multirow{11}{*}{\rotatebox{90}{\Arxiv}} 
			     &\ATC       & \cellcolor{LightCyan}{0.5802} & \cellcolor{LightCyan}{0.7253} & 0.0734 & 0.1333 & 0.5850 & 0.7349 & 0.0757 & 0.1373 & \cellcolor{LightCyan}{0.5767} & \cellcolor{LightCyan}{0.7875} & 0.0542 & 0.1015 & 0.5804 & \cellcolor{LightCyan}{0.7930} & 0.0557 & 0.1042 \\
			      &\CTC       & 0.5751 & \cellcolor{LightRed}{0.7693} & 0.0484 & 0.0911 & 0.5795 & 0.7783 & 0.0501 & 0.0942 & 0.5733 & \cellcolor{LightRed}{0.8160} & 0.0411 & 0.0782 & 0.5766 & \cellcolor{LightRed}{0.8200} & 0.0415 & 0.0790 \\\cline{2-18}
			      &\MAML      & 0.5674 & 0.6512 & 0.0355 & 0.0673 & \cellcolor{LightRed}{0.5903} & \cellcolor{LightCyan}{0.8005} & 0.0806 & 0.1465 & 0.5692 & 0.7345 & 0.0355 & 0.0676 & 0.5770 & 0.7221 & 0.0544 & 0.1011 \\
			      &\Reptile   & 0.5762 & 0.6409 & 0.0829 & 0.1468 & 0.5888 & \cellcolor{LightRed}{0.8260} & 0.0726 & 0.1334 & 0.5697 & 0.6034 & 0.0693 & 0.1242 & 0.5719 & 0.6804 & 0.0409 & 0.0771 \\
			      &\Featrans  & 0.5735 & 0.6527 & 0.0647 & 0.1177 & 0.5762 & 0.6626 & 0.0577 & 0.1062 & 0.5744 & 0.7288 & 0.0546 & 0.1016 & 0.5775 & 0.7066 & 0.0589 & 0.1087 \\
			      &\PN        & 0.2588 & 0.2458 & 0.2061 & 0.2242 & 0.2754 & 0.2773 & 0.2453 & 0.2603 & 0.4681 & 0.6257 & 0.5354 & \cellcolor{LightCyan}0.5771 & 0.4195 & 0.4867 & 0.6055 & 0.5397 \\\cline{2-18}
			      &\Supervise & 0.5531 & 0.4742 & 0.1488 & 0.2265 & 0.5816 & 0.6512 & 0.0877 & 0.1545 & 0.5461 & 0.4508 & 0.1364 & 0.2094 & 0.5791 & 0.6245 & 0.0957 & 0.1659 \\
			      &\ICSGNN    & \cellcolor{LightRed}{0.5904} & 0.6004 & 0.2016 & 0.3019 & \cellcolor{LightCyan}{0.5896} & 0.5995 & 0.2011 & 0.3012 & \cellcolor{LightRed}{0.5968} & 0.6224 & 0.2153 & 0.3199 & 0.5999 & 0.6220 & 0.2168 & 0.3215 \\
			      &\AQDGNN    & 0.5183 & 0.4622 & 0.5217 & 0.4901 & 0.4821 & 0.4425 & 0.7266 & 0.5501 & 0.5215 & 0.4573 & 0.5105 & 0.4824 & 0.5069 & 0.4619 & 0.7220 & 0.5633 \\\cline{2-18}
			      &\CGNPIP    & 0.5172 & 0.4716 & \cellcolor{LightCyan}{0.8118} & \cellcolor{LightCyan}{0.5966} & 0.5520 & 0.4915 & \cellcolor{LightCyan}{0.7889} & \cellcolor{LightRed}{0.6057} & 0.5699 & 0.5076 & \cellcolor{LightRed}{0.8067} & \cellcolor{LightRed}{0.6231} & \cellcolor{LightRed}{0.5856} & 0.5170 & \cellcolor{LightCyan}{0.8083} & \cellcolor{LightRed}{0.6306} \\
			      &\CGNPMLP   & 0.5079 & 0.4649 & 0.7870 & 0.5845 & 0.5642 & 0.5003 & 0.7371 & 0.5960 & 0.5365 & 0.4806 & 0.6397 & 0.5489 & \cellcolor{LightCyan}{0.5847} & 0.5194 & 0.6841 & 0.5905 \\
			      &\CGNPGNN   & 0.4699 & 0.4496 & \cellcolor{LightRed}{0.9161} & \cellcolor{LightRed}{0.6032} &  0.4649 & 0.4449 & \cellcolor{LightRed}{0.9205} & \cellcolor{LightCyan}{0.5998} & 0.4938 & 0.4548 & \cellcolor{LightCyan}{0.7464} & 0.5652  &0.5520	&0.4950	&\cellcolor{LightRed}{0.8399}	&\cellcolor{LightCyan}{0.6229}\\\hline\hline
			      
				
				\multirow{11}{*}{\rotatebox{90}{\Reddit}} 
				&\ATC       & 0.6574 & \cellcolor{LightCyan}{0.3566} & 0.4286 & 0.3893 & 0.6582 & 0.3553 & 0.4282 & 0.3883 & 0.4784 & 0.9586 & 0.4108 & 0.5752 & 0.4787 & 0.9572 & 0.4136 & 0.5776 \\
				&\CTC       & 0.6614 & \cellcolor{LightRed}{0.3593} & 0.4202 & 0.3874 & 0.6627 & \cellcolor{LightCyan}{0.3583} & 0.4190 & 0.3863 & 0.4713 & 0.9593 & 0.4019 & 0.5664 & 0.4722 & 0.9577 & 0.4054 & 0.5697 \\\cline{2-18}
				&\MAML      & \cellcolor{LightRed}{0.7450} & 0.3254 & 0.0007 & 0.0014 & \cellcolor{LightRed}{0.7465} & \cellcolor{LightRed}{0.3812} & 0.0010 & 0.0020 & 0.4679 & 0.9864 & 0.3861 & 0.5550 & 0.5017 & \cellcolor{LightCyan}{0.9863} & 0.4277 & 0.5967 \\
				&\Reptile   & \cellcolor{LightCyan}{0.7414} & 0.3039 & 0.0116 & 0.0224 & \cellcolor{LightCyan}{0.7447} & 0.2874 & 0.0050 & 0.0098 & 0.4051 & \cellcolor{LightRed}{0.9904} & 0.3107 & 0.4730 & 0.4046 & \cellcolor{LightRed}{0.9907} & 0.3121 & 0.4746 \\
				&\Featrans  & 0.7327 & 0.2972 & 0.0361 & 0.0644 & 0.7393 & 0.3224 & 0.0261 & 0.0484 & 0.2784 & 0.9369 & 0.1719 & 0.2906 & 0.2345 & 0.8634 & 0.1328 & 0.2302 \\
				&\PN        & 0.4731 & 0.2285 & 0.5556 & 0.3238 & 0.5051 & 0.2253 & 0.5379 & 0.3175 & 0.6708 &\cellcolor{LightCyan}{0.9891} &0.6749 &0.8024 & 0.6622 &0.9871 &0.6675 &0.7965 \\\cline{2-18}
				&\Supervise & 0.7079 & 0.2677 & 0.0845 & 0.1284 & 0.7288 & 0.2546 & 0.0364 & 0.0637 & 0.5834 & 0.9736 & 0.5296 & 0.6860 & 0.5536 & 0.9827 & 0.4907 & 0.6545 \\
				&\ICSGNN    & 0.6949 & 0.3331 & 0.1959 & 0.2467 & 0.6975 & 0.3361 & 0.1990 & 0.2500 & 0.2748 & 0.9460 & 0.1652 & 0.2813 & 0.2725 & 0.9499 & 0.1652 & 0.2815 \\
				&\AQDGNN    & 0.5221 & 0.2514 & 0.4427 & 0.3207 & 0.4141 & 0.2616 & 0.7199 & 0.3837 & 0.6476 & 0.8851 & 0.6772 & 0.7673 & 0.7830 & 0.9139 & 0.8250 & 0.8672 \\\cline{2-18}
				&\CGNPIP    & 0.2557 & 0.2549 & \cellcolor{LightCyan}{0.9991} & \cellcolor{LightRed}{0.4062} & 0.2543 & 0.2534 & \cellcolor{LightCyan}{0.9983} & 0.4042 & 0.7885 & 0.9264 & 0.8184 & 0.8691 & 0.8482 & 0.9110 & \cellcolor{LightCyan}{0.9122} & 0.9116 \\
				&\CGNPMLP   & 0.2566 & 0.2544 & 0.9934 & 0.4051 & 0.2774 & 0.2557 & 0.9694 & \cellcolor{LightRed}{0.4047} & \cellcolor{LightCyan}{0.8229} & 0.9397 & \cellcolor{LightCyan}{0.8479} & \cellcolor{LightCyan}{0.8915} & \cellcolor{LightRed}{0.8697} & 0.9508 & 0.8945 & \cellcolor{LightRed}{0.9218} \\
				&\CGNPGNN   & 0.2548 & 0.2548 & \cellcolor{LightRed}{1.0000} & \cellcolor{LightCyan}{0.4061} & 0.2534 & 0.2534 & \cellcolor{LightRed}{1.0000} & \cellcolor{LightCyan}{0.4043} & \cellcolor{LightRed}{0.8578} & 0.8578 & \cellcolor{LightRed}{1.0000} & \cellcolor{LightRed}{0.9235} & \cellcolor{LightCyan}{0.8584} &0.8584  & \cellcolor{LightRed}{1.0000} & \cellcolor{LightCyan}{0.9238}\\\hline\hline
				
				\multirow{11}{*}{\rotatebox{90}{\DBLP}} 
				&\ATC       & \cellcolor{LightRed}{0.8376} & \cellcolor{LightCyan}{0.8749} & 0.1752 & 0.2919 & \cellcolor{LightRed}{0.8230} & \cellcolor{LightCyan}{0.8916} & 0.1676 & 0.2822 & \cellcolor{LightRed}{0.7527} & \cellcolor{LightCyan}{0.7539} & 0.0922 & 0.1643 & \cellcolor{LightRed}{0.7360} & \cellcolor{LightCyan}{0.7849} & 0.1038 & 0.1834 \\
				&\CTC       & \cellcolor{LightCyan}{0.8365} & \cellcolor{LightRed}{0.9107} & 0.1599 & 0.2720 & \cellcolor{LightCyan}{0.8216} & \cellcolor{LightRed}{0.9214} & 0.1534 & 0.2629 & \cellcolor{LightCyan}{0.7512} & \cellcolor{LightRed}{0.7711} & 0.0803 & 0.1454 & \cellcolor{LightCyan}{0.7345} & \cellcolor{LightRed}{0.8012} & 0.0931 & 0.1668 \\\cline{2-18}
				&\MAML      & 0.8161 & 0.6395 & 0.0864 & 0.1522 & 0.8029 & 0.6545 & 0.1065 & 0.1832 & 0.7383 & 0.5337 & 0.0581 & 0.1047 & 0.7201 & 0.5713 & 0.0776 & 0.1366 \\
				&\Reptile   & 0.8106 & 0.5135 & 0.1704 & 0.2559 & 0.7993 & 0.5833 & 0.1162 & 0.1938 & 0.7208 & 0.3890 & 0.1033 & 0.1632 & 0.7184 & 0.5508 & 0.0741 & 0.1306 \\
				&\Featrans  & 0.8194 & 0.6339 & 0.1296 & 0.2152 & 0.8057 & 0.6600 & 0.1315 & 0.2193 & 0.7417 & 0.5736 & 0.0796 & 0.1397 & 0.7238 & 0.6301 & 0.0789 & 0.1402 \\
				&\PN        & 0.1819 & 0.0120 & 0.4528 & 0.0235 & 0.1790 & 0.0748 & 0.6846 & 0.1349 & 0.4017 & 0.2292 & 0.5911 & 0.3303 & 0.3581 & 0.2408 & 0.3988 & 0.3003 \\\cline{2-18}
				&\Supervise & 0.7312 & 0.2142 & 0.1523 & 0.1780 & 0.7773 & 0.3987 & 0.1438 & 0.2113 & 0.6805 & 0.3075 & 0.1692 & 0.2183 & 0.7015 & 0.4255 & 0.1307 & 0.2000 \\
				&\ICSGNN    & 0.7997 & 0.4662 & 0.3571 & \cellcolor{LightRed}{0.4044} & 0.7911 & 0.5030 & 0.3519 & \cellcolor{LightCyan}{0.4141} & 0.7366 & 0.4978 & 0.2304 & 0.3150 & 0.7290 & 0.5414 & 0.2373 & 0.3299 \\
				&\AQDGNN    & 0.6129 & 0.2257 & 0.4220 & 0.2941 & 0.5615 & 0.2705 & 0.6556 & 0.3830 & 0.5421 & 0.2990 & 0.5737 & 0.3931 & 0.4567 & 0.2994 & 0.6992 & 0.4192 \\\cline{2-18}
				&\CGNPIP    & 0.3951 & 0.2206 & \cellcolor{LightRed}{0.8548} & \cellcolor{LightCyan}{0.3507} & 0.5320 & 0.2829 & \cellcolor{LightRed}{0.8175} & \cellcolor{LightRed}{0.4203} & 0.3988 & 0.2779 & \cellcolor{LightRed}{0.8288} & \cellcolor{LightRed}{0.4162} & 0.5166 & 0.3404 & \cellcolor{LightCyan}{0.7720} & \cellcolor{LightCyan}{0.4725} \\
				&\CGNPMLP   & 0.4926 & 0.2317 & 0.7147 & 0.3499 & 0.5203 & 0.2487 & 0.6488 & 0.3596 & 0.4437 & 0.2814 & 0.7415 & \cellcolor{LightCyan}{0.4080} & 0.5652 & 0.3632 & 0.7304 & \cellcolor{LightRed}{0.4851} \\
				&\CGNPGNN   & 0.4262 & 0.2223 & \cellcolor{LightCyan}{0.8018} & 0.3481 & 0.4535 & 0.2463 & \cellcolor{LightCyan}{0.7928} & 0.3759 & 0.3999 & 0.2682 & \cellcolor{LightCyan}{0.7644} & 0.3971  & 0.3777 & 0.2894 & \cellcolor{LightRed}{0.8377} & 0.4302
				\\\hline
			\end{tabular}

		}
	\end{table*}
	
	\begin{table}[t]
		\centering
		\caption{Performance on \MGOD and \MGDD Tasks}
		\vspace{-0.3cm}	
		\label{tab:result2}
		\resizebox{0.5\textwidth}{!}{
			\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|}
				\hline
				\multirow{2}{*}{Dataset}        & \multicolumn{1}{l|}{Task config.} & \multicolumn{4}{c|}{1-shot}                                                                             & \multicolumn{4}{c|}{5-shot}                                                                             \\ \cline{2-10} 
				& \multicolumn{1}{c|}{Methods}        & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} \\ \hline
				\multirow{12}{*}{\rotatebox{90}{\Facebook}}       
				&\ATC       & 0.5564 & 0.2595 & 0.6305 & 0.3677 & 0.5592 & 0.2611 & 0.6464 & 0.3720 \\
				&\ACQ       & 0.3625 & 0.2190 & 0.8248 & 0.3461 & 0.4109 & 0.2266 & 0.7944 & 0.3526 \\
				&\CTC       & 0.8518 & \cellcolor{LightRed}{0.8734} & 0.3224 & 0.4710 & \cellcolor{LightRed}{0.8540} & \cellcolor{LightRed}{0.8904} & 0.3159 & 0.4664 \\\cline{2-10}
				&\MAML      & 0.6050 & 0.2319 & 0.1692 & 0.1956 & 0.6806 & 0.4091 & 0.2687 & 0.3244 \\
				&\Reptile   & 0.6356 & 0.3049 & 0.2215 & 0.2566 & 0.6680 & 0.4251 & 0.4642 & 0.4438 \\
				&\Featrans  & 0.6105 & 0.2462 & 0.1804 & 0.2082 & 0.5867 & 0.2936 & 0.3192 & 0.3059 \\
				&\PN        & 0.1549 & 0.0648 & 0.1289 & 0.0863 & 0.0938 & 0.0469 & 0.0967 & 0.0631 \\\cline{2-10}
				&\Supervise & 0.6291 & 0.2343 & 0.1350 & 0.1713 & 0.6073 & 0.3421 & 0.4079 & 0.3721 \\
				&\ICSGNN    & \cellcolor{LightRed}{0.7606} & \cellcolor{LightCyan}{0.5722} & 0.5598 & \cellcolor{LightRed}{0.5659} & \cellcolor{LightCyan}{0.7574} & \cellcolor{LightCyan}{0.5906} & 0.5516 & \cellcolor{LightRed}{0.5704} \\
				&\AQDGNN    & \cellcolor{LightCyan}{0.6779} & 0.3548 & 0.1644 & 0.2247 & 0.4239 & 0.3112 & 0.8396 & 0.4540\\\cline{2-10}
				&\CGNPIP    & 0.3727 & 0.3107 & \cellcolor{LightRed}{0.9925} & {0.4733} & 0.5121 & 0.3666 & \cellcolor{LightRed}{0.9756} & {0.5329} \\
				&\CGNPMLP   & 0.4161 & 0.3203 & 0.9418 &\cellcolor{LightCyan} {0.4781} & 0.5659 & 0.3860 & 0.8832 &{0.5372} \\
				&\CGNPGNN   & 0.3077 & 0.2888 & \cellcolor{LightCyan}{0.9835} & 0.4465 & 0.6029	&0.4118	&\cellcolor{LightCyan}{0.9145}	&\cellcolor{LightCyan}{0.5678}\\\hline\hline
				
				\comment{
				\multirow{11}{*}{\rotatebox{90}{\Coraciteseer}} 
				&\ATC       & 0.5421 & 0.8159 & 0.0726 & 0.1333 & 0.5308 & 0.8123 & 0.0676 & 0.1249 \\
				&\CTC       & 0.5231 & 0.8455 & 0.0209 & 0.0407 & 0.5135 & 0.8474 & 0.0207 & 0.0405 \\\cline{2-10}
				&\MAML      & 0.5132 & 0.4979 & 0.4111 & 0.4504 & 0.5271 & 0.5378 & 0.3164 & 0.3984 \\
				&\Reptile   & 0.5282 & 0.5191 & 0.3737 & 0.4346 & 0.5452 & 0.5593 & 0.3823 & 0.4542 \\
				&\Featrans  & 0.5108 & 0.4843 & 0.1293 & 0.2041 & 0.5119 & 0.5213 & 0.1676 & 0.2537 \\
				&\PN        & 0.2983 & 0.3553 & 0.3309 & 0.3427 & 0.3185 & 0.4131 & 0.3936 & 0.4031 \\\cline{2-10}
				&\Supervise & 0.5331 & 0.5280 & 0.3549 & 0.4245 & 0.5506 & 0.5607 & 0.4246 & 0.4833 \\
				&\ICSGNN    & 0.5564 & 0.6708 & 0.1622 & 0.2613 & 0.5492 & 0.6800 & 0.1613 & 0.2608 \\\cline{2-10}
				&\CGNPIP    & 0.5495 & 0.5429 & 0.8674 & \cellcolor{LightCyan}{0.6678} & 0.5475 & 0.5396 & 0.9013 & \cellcolor{LightRed}{0.6750} \\
				&\CGNPMLP   & 0.5464 & 0.5403 & 0.8789 & \cellcolor{LightRed}{0.6692} & 0.5516 & 0.5487 & 0.7876 & 0.6468 \\
				&\CGNPGNN   & 0.5544 & 0.5529 & 0.7649 & 0.6419 & 0.5427 & 0.5398 & 0.8324 & \cellcolor{LightCyan}{0.6549}   \\\hline\hline
				}
		
				\multirow{11}{*}{\rotatebox{90}{\Citeseercora}} 
				&\ATC       & \cellcolor{LightRed}{0.5779} & \cellcolor{LightCyan}{0.8191} & 0.1885 & 0.3064 & \cellcolor{LightCyan}{0.5783} & \cellcolor{LightCyan}{0.8154} & 0.1934 & 0.3127 \\
				&\CTC       & 0.5166 & \cellcolor{LightRed}{0.8714} & 0.0269 & 0.0523 & 0.5156 & \cellcolor{LightRed}{0.8692} & 0.0271 & 0.0526 \\\cline{2-10}
				&\MAML      & 0.5042 & 0.4986 & 0.3630 & 0.4202 & 0.5334 & 0.5544 & 0.3008 & 0.3900 \\
				&\Reptile   & 0.5202 & 0.5219 & 0.3620 & 0.4275 & 0.5658 & 0.6066 & 0.3538 & 0.4469 \\
				&\Featrans  & 0.5289 & 0.5529 & 0.2498 & 0.3442 & 0.5122 & 0.5464 & 0.0960 & 0.1632 \\
				&\PN        & 0.2521 & 0.1716 & 0.3800 & 0.2364 & 0.1766 & 0.1656 & 0.3524 & 0.2254 \\\cline{2-10}
				&\Supervise & 0.5446 & 0.5537 & 0.4099 & 0.4711 & \cellcolor{LightRed}{0.6066} & 0.6206 & 0.5320 & 0.5729 \\
				&\ICSGNN    & \cellcolor{LightCyan}{0.5532} & 0.6642 & 0.1923 & 0.2982 & 0.5538 & 0.6677 & 0.1932 & 0.2996 \\
				&\AQDGNN    & 0.5145 & 0.5040 & 0.5685 & 0.5343 & 0.4652 & 0.4626 & 0.6365 & 0.5358\\\cline{2-10}
				&\CGNPIP    & 0.5351 & 0.5177 & \cellcolor{LightCyan}{0.8822} & 0.6525 & 0.5280 & 0.5134 & \cellcolor{LightRed}{0.9241} & \cellcolor{LightRed}0.6601 \\
				&\CGNPMLP   & 0.5397 & 0.5207 & 0.8781 & \cellcolor{LightCyan}{0.6537} & 0.5476 & 0.5267 & \cellcolor{LightCyan}{0.8654} & \cellcolor{LightCyan}{0.6548} \\
				&\CGNPGNN   & 0.5367 & 0.5179 & \cellcolor{LightRed}{0.9181} & \cellcolor{LightRed}{0.6623} &  0.5456 & 0.5191 & 0.8532 & 0.6455\\\hline
			\end{tabular}
		}
	\end{table}
}


%%%%%%%%%%%%%%%%%%%%
\subsection{Effectiveness}
\label{sec:exp:effect}

We investigate the overall performance of CGNP on the four types of
tasks (\SGSC, \SGDC, \MGOD and \MGDD), for 1-shot and 5-shot
learning. The number of shots is the number of query nodes provided in
the support set.  The three variants of CGNP, CGNP with simple inner
product decoder (\CGNPIP), CGNP with MLP decoder (\CGNPMLP), and CGNP
with GNN decoder (\CGNPGNN) are compared with 10 baseline approaches.

Table \ref{tab:result1} presents the performance for tasks of single
graph with shared/disjoint communities. Here, we
highlight the first (purple) and the second (blue) best \Fone.
% \kfadd{
We observe that CGNP outperforms all the baselines in most cases.
%
The \Fone of CGNP succeeds all the baselines 0.28 on average.
%
The superiority of CGNP is reflected in improving the recall
significantly, while keeping relatively high accuracy and precision.
%A model is easy to achieve high accuracy as long as it predicts more nodes as the negative samples. 
In the testing, we observe that the optimization-based approaches, e.g., MAML, Reptile, predict almost all the nodes as the negative samples.  These approaches are sensitive to the imbalanced label distribution, leading to a higher accuracy but low recall. 
That indicates accuracy is not a suitable metric to evaluate the overall performance for CS task, because most nodes are in the negative class. A model is easy to achieve high accuracy as long as it predicts more nodes as the negative samples.
%Acc can be a misleading metric for imbalanced datasets. Some baselines may predict most samples to be negative, which leads to high True Negative (TN) and False Negative (FN). Since our datasets are imbalanced, they get high accuracy but much low recall. Such prediction results cannot search a community accurately. \Fone can better reflect the effect.}
\ICSGNN performs best in some cases (e.g., \DBLP and \Facebook), as it
is a query-specific model and uses the ground-truth of the test query
nodes additionally.  The naive approaches like \Featrans even fail to
search the community in most cases due to their low \Fone score. As
for ML/DL-based methods, they get comparable scores with naive
approaches. Since these methods are trained from scratch for each new
test task or each new query, ML/DL-based methods can utilize
task-specific knowledge. However, our approach gets higher score than
these methods. It indicates that our method is an efficient
meta-learning approach, which can learn prior knowledge from different
tasks. The prior knowledge is beneficial for predicting the
community. CGNP is the most robust learner due to its metric-based
learning strategy, and this property is similar to KNN and GP, which
fully validates the effectiveness of CGNP for small data.
%}
%Specifically, the performance of \Supervise is influenced by the number of training queries to the largest extent, which even surpasses CGNP on the $5$-shot \Citeseer.


	\begin{table}[t]
	\centering
	\caption{Performance on \MGOD and \MGDD Tasks}
	\vspace{-0.3cm}	
	\label{tab:result2}
	\resizebox{0.5\textwidth}{!}{
		\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|}
			\hline
			\multirow{2}{*}{Dataset}        & \multicolumn{1}{l|}{Task config.} & \multicolumn{4}{c|}{1-shot}                                                                             & \multicolumn{4}{c|}{5-shot}                                                                             \\ \cline{2-10} 
			& \multicolumn{1}{c|}{Methods}        & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone} \\ \hline
			\multirow{12}{*}{\rotatebox{90}{\Facebook}}       
			&\ATC       & 0.5564 & 0.2595 & 0.6305 & 0.3677 & 0.5592 & 0.2611 & 0.6464 & 0.3720 \\
			&\ACQ       & 0.3625 & 0.2190 & 0.8248 & 0.3461 & 0.4109 & 0.2266 & 0.7944 & 0.3526 \\
			&\CTC       & 0.8518 & 0.8734 & 0.3224 & 0.4710 & 0.8540 & 0.8904 & 0.3159 & 0.4664 \\\cline{2-10}
			&\MAML      & 0.6050 & 0.2319 & 0.1692 & 0.1956 & 0.6806 & 0.4091 & 0.2687 & 0.3244 \\
			&\Reptile   & 0.6356 & 0.3049 & 0.2215 & 0.2566 & 0.6680 & 0.4251 & 0.4642 & 0.4438 \\
			&\Featrans  & 0.6105 & 0.2462 & 0.1804 & 0.2082 & 0.5867 & 0.2936 & 0.3192 & 0.3059 \\
			&\PN        & 0.1549 & 0.0648 & 0.1289 & 0.0863 & 0.0938 & 0.0469 & 0.0967 & 0.0631 \\\cline{2-10}
			&\Supervise & 0.6291 & 0.2343 & 0.1350 & 0.1713 & 0.6073 & 0.3421 & 0.4079 & 0.3721 \\
			&\ICSGNN    & 0.7606 & 0.5722 & 0.5598 & \cellcolor{LightRed}{0.5659} & 0.7574 & 0.5906 & 0.5516 & \cellcolor{LightRed}{0.5704} \\
			&\AQDGNN    & 0.6779 & 0.3548 & 0.1644 & 0.2247 & 0.4239 & 0.3112 & 0.8396 & 0.4540\\\cline{2-10}
			&\CGNPIP    & 0.3727 & 0.3107 & 0.9925 & {0.4733} & 0.5121 & 0.3666 & 0.9756 & {0.5329} \\
			&\CGNPMLP   & 0.4161 & 0.3203 & 0.9418 &\cellcolor{LightCyan} {0.4781} & 0.5659 & 0.3860 & 0.8832 &{0.5372} \\
			&\CGNPGNN   & 0.3077 & 0.2888 & 0.9835 & 0.4465 & 0.6029	&0.4118	&0.9145	&\cellcolor{LightCyan}{0.5678}\\\hline\hline
			
			\comment{
				\multirow{11}{*}{\rotatebox{90}{\Coraciteseer}} 
				&\ATC       & 0.5421 & 0.8159 & 0.0726 & 0.1333 & 0.5308 & 0.8123 & 0.0676 & 0.1249 \\
				&\CTC       & 0.5231 & 0.8455 & 0.0209 & 0.0407 & 0.5135 & 0.8474 & 0.0207 & 0.0405 \\\cline{2-10}
				&\MAML      & 0.5132 & 0.4979 & 0.4111 & 0.4504 & 0.5271 & 0.5378 & 0.3164 & 0.3984 \\
				&\Reptile   & 0.5282 & 0.5191 & 0.3737 & 0.4346 & 0.5452 & 0.5593 & 0.3823 & 0.4542 \\
				&\Featrans  & 0.5108 & 0.4843 & 0.1293 & 0.2041 & 0.5119 & 0.5213 & 0.1676 & 0.2537 \\
				&\PN        & 0.2983 & 0.3553 & 0.3309 & 0.3427 & 0.3185 & 0.4131 & 0.3936 & 0.4031 \\\cline{2-10}
				&\Supervise & 0.5331 & 0.5280 & 0.3549 & 0.4245 & 0.5506 & 0.5607 & 0.4246 & 0.4833 \\
				&\ICSGNN    & 0.5564 & 0.6708 & 0.1622 & 0.2613 & 0.5492 & 0.6800 & 0.1613 & 0.2608 \\\cline{2-10}
				&\CGNPIP    & 0.5495 & 0.5429 & 0.8674 & \cellcolor{LightCyan}{0.6678} & 0.5475 & 0.5396 & 0.9013 & \cellcolor{LightRed}{0.6750} \\
				&\CGNPMLP   & 0.5464 & 0.5403 & 0.8789 & \cellcolor{LightRed}{0.6692} & 0.5516 & 0.5487 & 0.7876 & 0.6468 \\
				&\CGNPGNN   & 0.5544 & 0.5529 & 0.7649 & 0.6419 & 0.5427 & 0.5398 & 0.8324 & \cellcolor{LightCyan}{0.6549}   \\\hline\hline
			}
			
			\multirow{11}{*}{\rotatebox{90}{\Citeseercora}} 
			&\ATC       & 0.5779 & 0.8191 & 0.1885 & 0.3064 & 0.5783 & 0.8154 & 0.1934 & 0.3127 \\
			&\CTC       & 0.5166 & 0.8714 & 0.0269 & 0.0523 & 0.5156 & 0.8692 & 0.0271 & 0.0526 \\\cline{2-10}
			&\MAML      & 0.5042 & 0.4986 & 0.3630 & 0.4202 & 0.5334 & 0.5544 & 0.3008 & 0.3900 \\
			&\Reptile   & 0.5202 & 0.5219 & 0.3620 & 0.4275 & 0.5658 & 0.6066 & 0.3538 & 0.4469 \\
			&\Featrans  & 0.5289 & 0.5529 & 0.2498 & 0.3442 & 0.5122 & 0.5464 & 0.0960 & 0.1632 \\
			&\PN        & 0.2521 & 0.1716 & 0.3800 & 0.2364 & 0.1766 & 0.1656 & 0.3524 & 0.2254 \\\cline{2-10}
			&\Supervise & 0.5446 & 0.5537 & 0.4099 & 0.4711 & 0.6066 & 0.6206 & 0.5320 & 0.5729 \\
			&\ICSGNN    & 0.5532 & 0.6642 & 0.1923 & 0.2982 & 0.5538 & 0.6677 & 0.1932 & 0.2996 \\
			&\AQDGNN    & 0.5145 & 0.5040 & 0.5685 & 0.5343 & 0.4652 & 0.4626 & 0.6365 & 0.5358\\\cline{2-10}
			&\CGNPIP    & 0.5351 & 0.5177 & 0.8822 & 0.6525 & 0.5280 & 0.5134 & 0.9241 & \cellcolor{LightRed}0.6601 \\
			&\CGNPMLP   & 0.5397 & 0.5207 & 0.8781 & \cellcolor{LightCyan}{0.6537} & 0.5476 & 0.5267 & 0.8654 & \cellcolor{LightCyan}{0.6548} \\
			&\CGNPGNN   & 0.5367 & 0.5179 & 0.9181 & \cellcolor{LightRed}{0.6623} &  0.5456 & 0.5191 & 0.8532 & 0.6455\\\hline
		\end{tabular}
	}
\end{table}
 


Table \ref{tab:result2} shows the performance for tasks of multiple graphs.
The tasks of multiple graphs are harder than that of the single graph, and the tasks across domains are even harder. 
%We can observe that for the testing \Cora tasks, the \Fone of \Citeseercora in Table~\ref{tab:result2} is lower than that of \Citeseer in Table~\ref{tab:result1} for all the learning-based approaches. 
The \Fone of CGNP surpasses the \Fone of all the baselines 0.25 by average. 
The CGNP variants dominate the top two best models on \Citeseercora while it is overwhelmed by \ICSGNN on \Facebook.
This demonstrates that CGNP can effectively learn prior knowledge from only a few data of one graph and adapt to other graphs even from different domains, and the learned prior is indeed helpful.
In fact, transferring the prior of a shared node embedding function for clustering, as what CGNP does, is much easier than transferring model parameters, as \MAML, {\Reptile} and \Featrans do. 
%


CGNP with different decoders may bring different performance to the
result. The difference between them is subtle i.e. less than $5\%$. 
Both \MAML and \Reptile perform worse than CGNP in general.  The graph
algorithms \ATC, \ACQ and \CTC also fail to outperform learning-based
approaches due to their low recall. 
%\shadd{Since ACQ cannot apply to the datasets without attribute, we did not give results for ACQ in Arxiv, Reddit and DBLP. For the datasets with attributes, ACQ needs to enumerate all the sets of attributes that are shared by the query node and candidates. As Table \ref{tab:dataset} shows, the number of attributes for Cora and Citeseer is large (1433 and 3703, respectively), ACQ fails to return the results for \Citeseercora and \Citeseer in 12 hours. Therefore, we did not give results for ACQ in \Citeseercora and \Citeseer. }
It is worth to mentioning that \ACQ fails to return the results for \Citeseercora and \Citeseer in 12 hours, since \ACQ needs to enumerate all the sets of attributes that are shared by the query node and candidates.
In addition, \ACQ relies on the node attributes and it cannot support graphs
without node attributes, such as \Arxiv, \DBLP and \Reddit.

%




\subsection{Efficiency}
\label{sec:exp:effic}



We compare the efficiency of CGNP and the baselines regarding the
test/training time.  Fig.~\ref{fig:test time} presents the total test
time.  Regarding the prediction efficiency, our CGNP is the best
learning-based approach and the second-best among all the approaches,
which is over one order of magnitude faster than \ATC, \ACQ, \MAML,
\Reptile, \PN, \Supervise, \ICSGNN and \AQDGNN, and slightly faster
than \Featrans.  For one test task, \MAML, \Reptile and \Featrans
apply the backward propagation algorithm to update the parameters
online, and \Supervise and \AQDGNN train the parameters from scratch.
\ICSGNN needs to train a model for each query node on-the-fly.  \PN
not only needs to apply the backward propagation algorithm to update
parameters but also has to compute the distance between each node and
prototypes.  \ACQ needs to enumerate all the sets of
attributes shared by the query node and candidates, so that
it fails to return the results for \Citeseercora and \Citeseer in 12
hours.  For \CTC, the intermediate candidate communities of \Reddit
and \Facebook are large, it takes much longer time to compute the
diameter and maintain the $k$-truss structure.


Fig.~\ref{fig:train time} shows the meta training time of the
learning-based approaches on the training task set, where all the
models are trained by the same epoch of 200.  Note that \ATC, \ACQ,
\CTC, \PN, \Supervise, \ICSGNN and \AQDGNN do not involve this meta
training stage.  Our CGNP is one order of magnitude faster than \MAML
and \Reptile and its training efficiency is close to the simplest
transfer strategy, \Featrans,  \MAML and \Reptile are quite
time-consuming due to their two-level optimization paradigm.  For the
three CGNP variants, due to different model complexities, training
\CGNPGNN is slightly slower than that of \CGNPMLP, which is further
slightly slower than that of \CGNPIP. We observe that these
differences are negligible in the testing stage in Fig.~\ref{fig:test
  time}.


\begin{figure}[t]
	\centering
	%\includegraphics[width=1\columnwidth]{fig/legendtime.pdf}
	\begin{tabular}[h]{c}		
		\subfigure[{Total Test Time}] {\label{fig:test time}
			\includegraphics[ width=0.85\columnwidth]{fig/testtime.pdf}
		} \\
		%\vspace{-0.3cm}
		\subfigure[{Total Training Time}] {\label{fig:train time}
			\includegraphics[ width=0.85\columnwidth]{fig/traintime.pdf}
		}
	\end{tabular}
	\vspace{-0.2cm}
	\caption{Training \& Test Time (s)}
	\vspace{-0.2cm}
	\label{fig:time}
\end{figure}

\begin{figure}[t]
	%\vspace{-0.2cm}
	\centering
	\includegraphics[width=0.75\columnwidth]{fig/scalablelegend.pdf}
	\begin{tabular}[h]{c}
		\hspace{-0.6cm}
		\subfigure[{Total Test Time}] {\label{fig:scalable test time}
			\includegraphics[ width=0.48\columnwidth]{fig/scalabletesttime.pdf}
		} 
		\hspace{-0.2cm}
		\subfigure[{Total Training Time}] {\label{fig:scalable train time}
			\includegraphics[ width=0.48\columnwidth]{fig/scalabletraintime.pdf}
		}
	\end{tabular}
	\vspace{-0.2cm}
	\caption{Scalability of Training \& Test (s)}
	\vspace{-0.6cm}
	\label{fig:scalabletime}
\end{figure}


\stitle{Scalability Test.}  We explore the scalability of the
learning-based approaches. Fig.~\ref{fig:scalabletime} shows the GPU
training time and test time of our CGNP and 6 ML baselines, as the
number of nodes of graph in each task increases from $1,000$ to
$10,000$.
%
%It is worth mentioning that only our CGNP and \Reptile can scale to graphs of $10,000$ nodes in the limited 16GB GPU memory, whereas \Featrans runs out of memory and \MAML even fails for $1,000$ nodes.
%
%\kfadd{
All the methods can scale to graphs of $10,000$ nodes in the limited 16GB GPU memory.
%  
%Although CGNP spends longer time than \Featrans for testing in small
%graph, their gap shrinks as the size of graphs becomes larger and
%CGNP is almost one order of magnitude faster than \Featrans when the
%size becomes $5000$ or $10000$.  
%  
The test time of CGNP costs the least time than other baselines in all
the sizes. Only \Featrans spends close time to CGNP. Other methods
like \MAML, \Reptile, \PN, \AQDGNN show similar results as shown in
Fig.~\ref{fig:test time} due to their training strategy.
%
%That is because the larger the graph, the longer time \Featrans
%spends in the finetuning step, which CGNP does not incur.
%
While the training time of CGNP does not increase significantly with
the size increasing. And it is one or two orders of magnitude faster
than other methods in large graph.
% }






\subsection{Effect of the ground-truth number}
\label{sec:exp:labels}


We further evaluate how the number of ground-truth samples influences
the performance of the learning-based approaches.
%
% \kfadd{
For each query node $q$ in the support set, we vary the number of
positive/negative samples, i.e., $|l_q^{+}|$/$|l_q^{-}|$,
from 2\%/10\% to 20\%/100\% of the total number of the nodes.
Fig.~\ref{fig:mask} shows the \Fone-score of the 3 CGNP variants and 7
ML baselines on the 6 different tasks in 1-shot scenario.  In
Fig.~\ref{fig:mask}, CGNP variants surpass the ML baselines by 30\% on
average, particularly under the circumstances of the scarce
ground-truth. For small training samples, \Supervise suffers from
severe over-fitting, and \Featrans and \PN also face a high risk of
over-fitting in their adaptation step.  As the number of ground-truth
increases, the performance of \PN would degrade, since more samples
may blur the representation of prototypes.  The performance of \MAML, \Reptile and \AQDGNN increase in general with the increasing number of ground
truth. In addition, \Supervise would overtake CGNP as shown in
Fig.~\ref{fig:maskciteseer}, when the number of ground-truth is at a
high level.  Given sufficient training data, a task-specific
\Supervise model can better adapt to the task, compared with a
meta model.
% }
%\PN dominates the best \Fone on \DBLP as shown in
%Fig.~\ref{fig:maskdblp}, while it cannot surpass CGNP for most ratios
%of samples.
%
The \Fone of \ICSGNN tends to be stable on varying ratios of
samples. We conjecture the reason could be its hyper-parameter, the
community size to search, determines the final results in the
post-processing stage.
%
Furthermore, we find that the performance of CGNP is robust to the
number of ground-truth. That is in accordance with the nature of
metric-based learning, where only a few training samples can achieve
high performance for KNN and kernel learning.







%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
	\vspace{-0.2cm}
	\centering
	\includegraphics[width=0.8\columnwidth]{fig/legend.pdf}
	\begin{tabular}[h]{c}
		\hspace{-0.6cm}
		%\vspace{-0.3cm}
		\subfigure[\Citeseer ] {
			\includegraphics[ width=0.4\columnwidth]{fig/citeseer.pdf}
			\label{fig:maskciteseer}
		}
		\subfigure[\Arxiv] {
			\includegraphics[ width=0.4\columnwidth]{fig/arxiv.pdf}
			\label{fig:maskarxiv}
		}\\ 
		
		\hspace{-0.6cm}
		%\vspace{-0.3cm}
		\subfigure[\Reddit] {
			\includegraphics[ width=0.4\columnwidth]{fig/reddit.pdf}
			\label{fig:maskreddit}
		}
		\subfigure[\DBLP ] {
			\includegraphics[ width=0.4\columnwidth]{fig/dblp.pdf}
			\label{fig:maskdblp}
		}\\
		
		\hspace{-0.6cm}
		%\vspace{-0.3cm}
		\subfigure[\Facebook] {
			\includegraphics[ width=0.4\columnwidth]{fig/facebook.pdf}
			\label{fig:maskfacebook}
		}
		\subfigure[\Citeseercora] {
			\includegraphics[ width=0.4\columnwidth]{fig/citeseercora.pdf}
			\label{fig:maskcoraciteseer}
		}\\
	\end{tabular}
	\vspace{-0.2cm}
	\caption{\Fone under Different Ratios of Ground-truth}
	\label{fig:mask}
	\vspace{-0.6cm}
\end{figure}

\subsection{Ablation Study}
\label{sec:exp:ablation}


In this section, we conduct ablation studies to investigate the effect
of different options for the GNN layer and the commutative operation
on the performance of CGNP.  \CGNPMLP, the CGNP with an GNN decoder,
is tested as the base CGNP model and all the model variants are
trained by the same hyper-parameters.
% \kfadd{
These model variants are tested on the $5$-shot \Citeseer~\SGSC,
\Arxiv~\SGSC, \Reddit~\SGDC, \DBLP~\SGDC, \Facebook~\MGOD and
\Citeseercora~\MGDD tasks, respectively.
% }

\stitle{GNN Layer.} We adopt three popular GNN, \GCN~\cite{GCN},
\GAT \cite{GAT} and \SAGE \cite{SAGE} as the encoder,
where the commutative operation is fixed to the average
pooling. Table~\ref{tab:layer} lists the performance of the \CGNPGNN
variants on the 2 tasks.
%
% \kfadd{
In general, the \GAT encoder consistently outperforms \GCN
encoders. This is because \GAT aggregates the node representation
weighted by learnable weights via self-attention, where the importance
of each neighbor are considered regarding its local structure,
possible features and positive/negative labels. The higher \Fone of
\GAT demonstrates the attention mechanism can also contribute to
improving the performance of CGNP in the encoder part. \SAGE encoder
gets highest \Fone scores in \SGDC and \SGSC tasks. This is because
\SAGE uses a generalized aggregation function and this mechanism is
beneficial for encoder of CGNP.
% }

\stitle{Commutative Operation.} We adopt the sum, average pooling and
self-attention, introduced in \cref{sec:CGNP} as the commutative
operation big $\oplus$ of \CGNPGNN, by fixing \GAT as the encoder GNN.
%
% \kfadd{
Table~\ref{tab:layer} shows the corresponding performance of the 3
model variants. In different tasks, the performance of three
commutative operations is different.  However, the differences between
the three variants are relatively slight.  We speculate that different
tasks, graphs or ground-truth distributions may benefit from different
commutative operations, and the effect of the type of commutative
operation is not as remarkable as that of the GNN encoder.
% }

\begin{table}[t]
	\small
	\centering
	\vspace{-0.4cm}
	\caption{Performance with Different Layers and Com. Op.}
	\vspace{-0.2cm}
	\label{tab:layer}
	\resizebox{0.5\textwidth}{!}{
		\begin{tabular}{|l|l|c|c|c|c|l|c|c|c|c|}
			\hline
			Dataset                   & \multicolumn{1}{c|}{Layer} & \multicolumn{1}{c|}{\Acc}    &  \multicolumn{1}{c|}{\Pre}    &  \multicolumn{1}{c|}{\Rec}    &  \multicolumn{1}{c|}{\Fone}  & \multicolumn{1}{c|}{$\oplus$}  & \multicolumn{1}{c|}{\Acc}    &  \multicolumn{1}{c|}{\Pre}    &  \multicolumn{1}{c|}{\Rec}    &  \multicolumn{1}{c|}{\Fone}   \\\hline
			\multirow{3}{*}{\Citeseer}    & GCN   &0.5001	&0.4601	&0.7779	&0.5782 & Att. & 0.5956 & 0.6437 & 0.6893 & 0.6657 \\
			& GAT   & 0.5520	&0.4950	&0.8399	&0.6229 & Sum       & 0.6154 & 0.6526 & 0.7306 & 0.6894 \\
			& SAGE  &0.6348	&0.5555	&0.8553	&\cellcolor{LightYellow}{0.6736}  & Ave.   & 0.6158 & 0.6513 & 0.7367 &\cellcolor{LightYellow} {0.6914} \\\hline
			\multirow{3}{*}{\Arxiv}  & GCN   & 0.4540 & 0.4388 & 0.9076 & 0.5916 & Att. & 0.4816 & 0.4526 & 0.9080 & \cellcolor{LightYellow}{0.6041}\\
			& GAT   & 0.4649 & 0.4449 & 0.9205 & 0.5998 & Sum       & 0.4696 & 0.4405 & 0.8044 & 0.5692 \\
			& SAGE  & 0.6035 & 0.5305 & 0.7800 & \cellcolor{LightYellow}{0.6315} 	& Ave.   & 0.4649 & 0.4449 & 0.9205 & 0.5998 \\\hline
			\multirow{3}{*}{\Reddit} & GCN   & 0.8006 & 0.8596 & 0.9175 & 0.8876 & Att.  & 0.8006 &0.8006 &1.0000 &0.8893\\
			& GAT   & 0.8584 & 0.8584 & 1.0000 & 0.9238 & Sum       &0.7122 &0.7122 &1.0000 &0.8319\\
			& SAGE  & 0.9335 & 0.9553 & 0.9679 & \cellcolor{LightYellow}{0.9615} & Ave.   & 0.8584 & 0.8584 & 1.0000 & \cellcolor{LightYellow}{0.9238} \\\hline
			\multirow{3}{*}{\DBLP}   & GCN   & 0.3189 & 0.2829 & 0.9308 & 0.4339  & Att. & 0.3761 & 0.2866 & 0.8223 & 0.4251 \\
			& GAT   & 0.3777 & 0.2894 & 0.8377 & 0.4302 & Sum       & 0.3742 & 0.2896 & 0.8472 & \cellcolor{LightYellow}{0.4316} \\
			& SAGE  & 0.5480 & 0.3446 & 0.6783 & \cellcolor{LightYellow}{0.4570} & Ave.   & 0.3777 & 0.2894 & 0.8377 & 0.4302 \\\hline
			\multirow{3}{*}{\Facebook} & GCN   & 0.3082	&0.2880	&0.9676	&0.4438 & Att. & 0.5547	&0.3795	&0.8826	&0.5307 \\
			& GAT   & 0.6029	&0.4118	&0.9145	&\cellcolor{LightYellow}{0.5678} & Sum       & 0.5427	&0.3762	&0.9156	&0.5333 \\
			& SAGE  &0.4361	&0.3143	&0.8263	&0.4554  & Ave.   & 0.6029	&0.4118	&0.9145	&\cellcolor{LightYellow}{0.5678} \\\hline
			\multirow{3}{*}{\Citeseercora} & GCN   & 0.5427 & 0.5179 & 0.8224 & 0.6356 & Att. & 0.5626 & 0.5310 & 0.8390 & \cellcolor{LightYellow}{0.6504} \\
			& GAT   & 0.5456 & 0.5191 & 0.8532 & \cellcolor{LightYellow}{0.6455} & Sum       & 0.5341 & 0.5113 & 0.8898 & 0.6494 \\
			& SAGE  & 0.5591 & 0.5306 & 0.7867 & 0.6337 & Ave. 	& 0.5456 & 0.5191 & 0.8532 & 0.6455\\\hline
		\end{tabular}
	}
\end{table}


\comment{
\begin{table}[t]
	\small
	\centering
	\caption{Performance with Different Commutative Op.}
	\label{tab:pool}
	\resizebox{0.35\textwidth}{!}{
		\begin{tabular}{|l|l|r|r|r|r|}
			\hline
			Dataset                    & \multicolumn{1}{c|}{$\oplus$} & \multicolumn{1}{c|}{\Acc} & \multicolumn{1}{c|}{\Pre} & \multicolumn{1}{c|}{\Rec} & \multicolumn{1}{c|}{\Fone}         \\\hline
			\multirow{3}{*}{\Citeseer}    & Attention & 0.5956 & 0.6437 & 0.6893 & 0.6657 \\
			& Sum       & 0.6154 & 0.6526 & 0.7306 & 0.6894 \\
			& Average   & 0.6158 & 0.6513 & 0.7367 &\cellcolor{LightYellow} {0.6914} \\\hline
			\multirow{3}{*}{\Arxiv} & Attention & 0.4816 & 0.4526 & 0.9080 & \cellcolor{LightYellow}{0.6041} \\
			& Sum       & 0.4696 & 0.4405 & 0.8044 & 0.5692 \\
			& Average   & 0.4649 & 0.4449 & 0.9205 & 0.5998  \\\hline
			\multirow{3}{*}{\Reddit} & Attention  & 0.8006 &0.8006 &1.0000 &0.8893 \\
			& Sum       &0.7122 &0.7122 &1.0000 &0.8319\\
			& Average   & 0.8584 & 0.8584 & 1.0000 & \cellcolor{LightYellow}{0.9238}\\\hline
			\multirow{3}{*}{\DBLP}     & Attention & 0.3761 & 0.2866 & 0.8223 & 0.4251 \\
			& Sum       & 0.3742 & 0.2896 & 0.8472 & \cellcolor{LightYellow}{0.4316} \\
			& Average   & 0.3777 & 0.2894 & 0.8377 & 0.4302\\\hline
			\multirow{3}{*}{\Facebook} & Attention & 0.5547	&0.3795	&0.8826	&0.5307 \\
			& Sum       & 0.5427	&0.3762	&0.9156	&0.5333 \\
			& Average   & 0.6029	&0.4118	&0.9145	&\cellcolor{LightYellow}{0.5678} \\\hline   

			 \multirow{3}{*}{\Citeseercora} & Attention & 0.5626 & 0.5310 & 0.8390 & \cellcolor{LightYellow}{0.6504} \\
			 & Sum       & 0.5341 & 0.5113 & 0.8898 & 0.6494 \\
			 & Average 	& 0.5456 & 0.5191 & 0.8532 & 0.6455\\\hline
		\end{tabular}
	}
	\vspace{-0.4cm}
\end{table}
}


