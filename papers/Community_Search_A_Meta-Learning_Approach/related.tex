

\section{Related Work}
\label{sec:related}
%Our model is related to the community search, meta learning and graph meta learning. We review the progress of related works in this section.

\stitle{Community Search.}
%The problem of community search (CS) is to find densely connected subgraphs containing given query nodes. 
A comprehensive survey of CS problems and approaches can be found in~\cite{DBLP:journals/vldb/FangHQZZCL20, DBLP:series/synthesis/2019Huang}. 
%\subsection{Community Search}
In a nutshell, CS problem can be divided into two categories. One is non-attributed community search which only concerns the structural cohesiveness over simple graphs and the other is attributed community search (ACS) which concerns both the structural cohesiveness and content overlapping or similarities over attributed graphs.
Regarding capturing the structural cohesiveness, various community metrics have been proposed, including $k$-core~\cite{cs3,cs4,cs6}, $k$-truss~\cite{cs2,cs7}, $k$-clique \cite{cs1,cs8} and $k$-edge connected component~\cite{cs9,cs10}. 
These metrics are inflexible to adapt to complex real-world graphs and applications. 
%For one thing, the constraint may be either too loose like $k$-core or too tight like $k$-clique.
%For the other thing, choosing a proper $k$ value as well as one metric is not an easy task for diversified graphs.    
%There exists many community search methods, and can be generally divided into two categories, structure-based algorithm and structure combined feature approach. In order to capture graph structure, various of $k$-related measurements model have been proposed, for example, $k$-core \cite{cs3,cs4,cs6}, $k$-truss (CTC) \cite{cs2,cs7}, $k$-clique \cite{cs1,cs8} and $k$-edge connected component (ECC) \cite{cs9,cs10}. These pre-defined metrics are proposed from the number of triangles, degree, the maximum diameter, etc. Thus they are lack of flexibility and it is difficult to have proper topology structure of communities. And set the parameter $k$ is not easy, since too large may cause the searched community not cohesive and too small may fail to find all the candidate node in target community. However, these methods only make use of graph structure and ignore the information of content feature. 

In addition to only exploiting the structural information, ACS leverages both the structural constraint and attributes such as keywords~\cite{ACQ, ATC}, location~\cite{DBLP:conf/icde/WangCLZQ18}, temporal~\cite{DBLP:conf/icde/LiSQYD18}, etc. 
As two representative approaches for ACS, ATC~\cite{ATC} finds $k$-truss community with the maximum pre-defined attribute score. 
And ACQ~\cite{ACQ} finds $k$-core communities whose nodes share the maximum attributes with the query attributes.
Both ATC and ACQ adopt a two-stage process. First, they find the candidate communities based on the structural constraints.
Then, the candidates are verified based on the computed attribute score or the appearance of attribute set.
However, the quality of the found communities of the two approaches are unpromising since the independent two stages fail to capture the correlations between structures and attributes in a joint fashion. 
%Other works combine both of them to measure communities, such as ACQ \cite{ACQ} and ATC \cite{ATC}. Both of them adopt a two-stage process. First a pre-defined structural constraint is imposed to find candidate communities. Then they use feature score function to select the most related communities. However, these models fail to capture the features of other nodes. And the two-stage methods fail to find the relation between structure and features. 

With the development of ML/DL, recently, GNN has been adopted for
CS~\cite{ICSGNN}.  By recasting the community membership determination
to a classification task, a model can learn via its prediction error
feedback given the training samples and can adapt to a specific graph
in an end-to-end way.  Recently, Gao et al. proposed ICS-GNN
~\cite{ICSGNN} for interactive CS, which allows users to provide
ground-truth for online incremental learning.  The model is a
query-specific model that fails to generalize to new query nodes.  
%In other words, the model has to be retrained from scratch for each
%encountered query.
%
\cite{AQDGNN} proposes a graph neural network based model that is
trained by a collection of query nodes with their ground-truth, and
makes predictions for unseen query nodes in a single graph.
%
% For one graph, a large
% volume of query nodes with ground-truth communities are necessary for
%training, which ensure the model well generalizes to other local
% queries.
%
%This design is not only inefficient for online query processing but also is a waste of the generalization capability of neural networks. 
%Recently, ICS-GNN \cite{ICSGNN} was proposed for interactive community search problem. It recast the community membership as a classification problem using GNN, which combining content feature and structural information. Then they introduce a $k$-sized Maximum-GNN-scores community to describe the target community. 
%Although ICS-GNN combine both content feature and structural information, it can only train a query-specific model. That's mean, it has to re-train a model when it comes to a new query, which is less efficient and hard for large graph. In this paper, our model are designed from a meta learning perspective. Our trained CGNP can combine feature and structure information and be utilized in different queries or different tasks without retraining the model.
%cs1 cs8 k-clique ; 
%cs3 cs4 cs6 k-core;
%ACQ ;ATC;
%cs2 cs7 CTC k-truss;
%cs9 k-edge cs10;
%ICSGNN

\comment{\color{red}
\stitle{ML/DL for Graph Analytics.}
A large number of ML/DL models are exploited to perform graph analytics.
 To solve combinatorial optimization NP-hard problem,  ~\cite{Combinatorial} combines deep learning techniques with useful algorithmic elements and the central component is the GNN that can estimate the likelihood. ~\cite{Combinatorial2} proposes a unique combination of Reinforcement Learning and graph embedding to address recurring problems.%Combinatorial optimization for NP-hard problem
Graph similarity is a fundamental and critical problem in graph-based applications and graph edit distance (GED) is one of the most commonly used graph similarity measures. TaGSim~\cite{TaGSim} proposes a type-aware graph similarity learning and computation framework that estimates GED in a fine-grained approach. GHashing~\cite{GHashing} is a novel GNN based semantic hashing for approximate pruning and the GNN learns to generate embeddings and hash codes that preserve GED between graphs. GLSEARCH~\cite{GLSearch} is a general framework for Maximum Common Subgraph detection combining the advantages of search and deep Q-learning into a single framework. %Graph similarity
Subgraph Matching is a problem of enumerating all isomorphic embeddings of a query graph $q$ in a data graph $G$. ~\cite{subiso} address the scalability challenges induced by a stream of subgraph isomorphism queries and they present a novel subgraph index based on graph embeddings that serves as the foundation for efficient stream processing. For the first time, ~\cite{RLbased} apply Reinforcement Learning and Graph Neural Networks techniques to generate the high-quality matching order for subgraph matching algorithms. NeuroMatch~\cite{NeuralSM} proposes an accurate, efficient and robust neural approach to subgraph matching which decomposes query and target graphs into small subgraphs and embeds them using GNN. %Subgraph Matching
Subgraph Counting is to count the number of subgraphs in a data graph that match a given query graph. ~\cite{subgraphcounting} proposes an Active Learned Sketch for Subgraph Counting (ALSS) with a sketch learned and an active learner for Subgraph Counting over a large data graph. ~\cite{NSIC} proposes a learning framework that augments different representation learning architectures and pays attention to pattern and target data graphs to memorize intermediate states of subgraph isomorphism searching for global counting to make it scalable for large-scale graphs and patterns. %Subgraph Counting
Detecting critical entities in social network communities is an important issue. ~\cite{FindingCriticalUsers} presents a learning-based approach for finding critical users to solve the collapsed $k$-core problem. %Community Collapsion
Predict shortest-path distances on road networks have many applications.
~\cite{predict} achieve fast distance predictions without a high space cost by learning an embedding for every vertex that preserves its distance to the other vertices. A multi-layer perceptron is trained to predict the distance between two vertices given their embeddings. %Reachability & shortest path query
Community detection is of great significance in network analysis.  A comprehensive overview of community detection problems with Deep Learning can be found in ~\cite{communitydetection}. This survey proposes different state-of-the-art methods including deep learning-based models which can be further divided into convolutional networks, graph attention networks, generative adversarial networks and autoencoders. %Community detection: Refer to survey
Community Search aims to find communities containing query vertex. ICSGNN~\cite{ICSGNN} is an Interactive Community Search method based on GNN to locate the target community. %CS
}

\stitle{ML/DL for Graph Analytics.} Apart from CS, ML/DL techniques are widely used in various graph analytical tasks, including classical combinatorial optimization problems~\cite{Combinatorial, Combinatorial2}, graph similarity search~\cite{TaGSim, GHashing, GLSearch}, subgraph matching~\cite{subiso, RLbased, NeuralSM}, subgraph counting~\cite{subgraphcounting, NSIC, zhao2023learned}, shortest path query~\cite{predict}, community collapsing~\cite{FindingCriticalUsers} and community detection~\cite{communitydetection}. 
In brief, the main ideas of these approaches contain learning a model-based algorithm heuristics~\cite{Combinatorial, Combinatorial2, GLSearch, RLbased} to replace the traditional predefined heuristics, where Reinforcement Learning algorithms can be used; learning a workload-specific estimator for approximate query processing~\cite{subgraphcounting, NSIC, TaGSim}; constructing a model-based database index for filtering or searching~\cite{subiso, NeuralSM, predict, GHashing, FindingCriticalUsers}, which is node or graph embedding preserving task-related semantics. 
Our approach is in the first category regarding learning a meta heuristic for CS while the subtle difference is that we leverage metric learning to evaluate the community membership.


\comment{
\stitle{Meta-Learning.}
Meta-learning is a learning paradigm that learns the prior knowledge from multiple tasks, which can be swiftly transferred to a new task with only a few observed data. 
In general, the meta-learning approaches fall into three categories, i.e.,  black-box adaption~\cite{NTM, meta1, MANN, meta4}, optimization-based~\cite{MAML, meta2, meta3}, and metric-based~\cite{prototypical, RN, MN} approaches. 
Black-box adaption relies on specific neural network architecture, e.g., recurrent neural network to encode each task sequentially. 
These approaches have powerful expressive capability to model the task priors but are data inefficient and challenging for optimization.
The optimization-based approaches learn the hierarchy by explicit gradient-based backpropagation. These algorithms are usually model-agnostic and effective regarding learning the meta model. However, their computations are time and memory-consuming due to the hierarchical optimization paradigm. 
The metric-based approaches borrow the idea from clustering algorithms and KNN that learn embeddings or distance metrics from the input. The approaches are more effective for small data but only applicable for classification tasks. 
%
Our approach CGNP can be regarded as metric-based since the predictive distribution of CGNP is derived from inner-product similarity in a hidden embedding space. That can be analogized to the predictive distribution of GP or KNN  derived from a learned kernel. The neural network components of CGNP explicitly map the inputs to finite dimensional space in contrast to that kernel function implicitly maps the inputs to infinite dimensional space. 
%\subsection{Meta Learning}
%Meta learning learns from prior knowledge and transfers them to a new task with only a few observed data. Meta learning methods fall into three types. First is model-based method i.e. NTMs \cite{NTM}, Memory Networks \cite{meta1}, MANN \cite{MANN} and Meta Networks \cite{meta4}. These methods rely on specific model architecture, they have strong power to model prior knowledge of model. But they require large amount of data and inefficient. Second category is metric-based approach, there are prototypical network \cite{prototypical}, Relation Network \cite{RN} and Matching Networks \cite{MN}. The key of these metrics-based algorithm is to learn embedding to measure similarity. However, these methods may only apply to classification tasks. Then there are optimization-based meta learning algorithm, like LSTM-based meta-learner \cite{meta2}, MAML \cite{MAML} and  Reptile \cite{meta3}. Optimization-based methods learns model by gradient back propagation, and they are effective in meta learning. However, they are quite time and memory consuming.
%model base: NTM-Neural Turing Machines(NTMs) ;meta1-memory networks ;MANN;meta4-Meta Networks
%metric bsed:prototypical;RN;MN
%optimization based:meta2_LSTM-based meta-learner optimizer; meta3;MAML
}

\stitle{Meta-Learning on Graph.}
%\subsection{Graph Meta Learning}
Meta-learning is a learning paradigm that learns the prior knowledge from multiple tasks, which can be swiftly transferred to a new task with only a few observed data. 
In general, the meta-learning approaches fall into three categories, i.e.,  black-box adaptation~\cite{NTM, meta1, MANN, meta4}, optimization-based~\cite{MAML, meta2, reptile}, and metric-based~\cite{prototypical, RN, MN} approaches. 
%
Meta-learning has been adopted over graph data to deal with various graph learning tasks, including node classification~\cite{metagnn, GMETA}, link prediction~\cite{metagraph, GMETA}, graph classification~\cite{5, DBLP:conf/cikm/MaBYZYYZY20} and graph alignment~\cite{62, 53}. 
A brief survey that summarizes the applications and methods can be found in~\cite{DBLP:journals/corr/abs-2103-00137}. 
Here, GNN is widely used as the base model or core component of these approaches. The optimization-based, metric-based, or hybrid of optimization and metric-based~\cite{GMETA} are used as the meta-learning strategies.
However, all the existing approaches are oriented to graph learning tasks and cannot be directly applied to our CS task, where the input is specified by a personalized query node.
%
Our approach CGNP can be regarded as metric-based since the predictive probability of CGNP is derived from inner-product similarity in a hidden embedding space. %That can be analogized to the predictive distribution of GP or KNN  derived from a learned kernel. The neural network components of CGNP explicitly map the inputs to finite dimensional space in contrast to that kernel function implicitly maps the inputs to infinite dimensional space.
%Recently, meta learning has been used in graph structure data and can solve various of tasks, including link prediction, node classification, etc. For classification tasks, Meta-GNN \cite{metagnn} deploys gradient-based meta learning with GNN for node classification and \cite{5} uses prototypical network to do graph classification. For link prediction tasks, Meta-Graph \cite{metagraph} study few-shot link prediction from multiple graphs data.  And G-Meta \cite{GMETA} focuses on both node classification and link prediction and they have several settings for these tasks, i.e. single graphs with disjoint labels, multiple graphs with shared labels, etc. Other taks like fast network alignment, \cite{62} use graph meat learning to reformulate the network alignment problem as a classification problem. GMatching \cite{53} and MetaR \cite{7} use metrics-based meta learning method to solve problems for knowledge graph. To our best of knowledge, our work is the first one that use meta learning method for community search task.
% 62 fast network alignment;;5 graph classification; metagnn;metagraph;GMETA
% 7 MetaR; 53 GMatching
