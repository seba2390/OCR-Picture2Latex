\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\newcommand{\shadd}[1]{\textcolor{blue}{#1}}
\makeatother
\newtheorem{example}{Example}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{enumitem}
\usepackage{cleveref}
\usepackage{pifont}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{LightRed}{rgb}{1,0.88,1}
\definecolor{LightYellow}{rgb}{1,1,0.88}
\definecolor{LightGray}{gray}{0.8}
%\usepackage[font=scriptsize]{caption}
\input{custom_sytles}
\pagenumbering{gobble}
\input{command}

\begin{document}

\title{Community Search: A Meta-Learning Approach}

\author{%
	% author names are typeset in 11pt, which is the default size in the author block
	{Shuheng Fang\IEEEauthorrefmark{1}, Kangfei Zhao\thanks{\IEEEauthorrefmark{2} Corresponding author.}\IEEEauthorrefmark{2}, Guanghua Li\IEEEauthorrefmark{3}, Jeffrey Xu Yu\IEEEauthorrefmark{1}} 
	% add some space between author names and affils
	\vspace{1.6mm}\\
	\fontsize{10}{10}\selectfont\itshape
	% 20080211 CAUSAL PRODUCTIONS
	% separate superscript on following line from affiliation using narrow space}%
	% add some space between author names and affils
	\IEEEauthorrefmark{1}The Chinese University of Hong Kong,
	\IEEEauthorrefmark{2}Beijing Institute of Technology\\
	\IEEEauthorrefmark{3}The Hong Kong University of Science and Technology (Guangzhou)\\
	
	\fontsize{9}{9}\selectfont\ttfamily\upshape
	
	\IEEEauthorrefmark{1}{\{shfang,yu\}}@se.cuhk.edu.hk, \IEEEauthorrefmark{2}zkf1105@gmail.com,
	\IEEEauthorrefmark{3}gli945@connect.hkust-gz.edu.cn
	\fontsize{9}{9}\selectfont\ttfamily\upshape
	\vspace{-0.6cm}
}
\vspace{-0.6cm}


\maketitle
\thispagestyle{plain}



\comment{
\author{\IEEEauthorblockN{Shuheng Fang}
\IEEEauthorblockA{\textit{The Chinese University of Hong Kong} \\
%Hong Kong, China\\
shfang@se.cuhk.edu.hk}

\and
\IEEEauthorblockN{Kangfei Zhao}
\IEEEauthorblockA{\textit{Beijing Institute of Technology} \\
%Shenzhen, China\\
zkf1105@gmail.com}

\and
\IEEEauthorblockN{Guanghua Li}
\IEEEauthorblockA{\textit{Wuhan University} \\
%Wuhan, China \\
guanghli@whu.edu.cn}


\and
\IEEEauthorblockN{Jeffrey Xu Yu}
\IEEEauthorblockA{\textit{The Chinese University of Hong Kong} \\
%Hong Kong, China \\
yu@se.cuhk.edu.hk}
}
\maketitle}

\begin{abstract}
Community Search (CS) is one of the fundamental graph analysis tasks, which is a building block of various real applications.
Given any query nodes, CS aims to find cohesive subgraphs that query nodes belong to. 
Recently,  a large number of CS algorithms are designed. 
These algorithms adopt pre-defined subgraph patterns to model the communities, which cannot find ground-truth communities that do not have such pre-defined patterns in real-world graphs.
Thereby, machine learning (ML) and deep learning (DL) based approaches are proposed to capture flexible community structures by learning from ground-truth communities in a data-driven fashion.
These approaches rely on sufficient training data to provide enough generalization for ML models, however, the ground-truth cannot be comprehensively collected beforehand.
%

In this paper, we study ML/DL-based approaches for CS, under the
circumstance of small training data.  Instead of directly fitting the
small data, we extract prior knowledge which is shared across multiple
CS tasks via learning a meta model. Each CS task is a graph with
several queries that possess corresponding partial ground-truth.  The
meta model can be swiftly adapted to a task to be predicted by feeding
a few task-specific training data.  We find that trivially applying multiple
classical meta-learning algorithms to CS suffers from problems
regarding prediction effectiveness, generalization capability and
efficiency.  To address such problems, we propose a novel
meta-learning based framework, Conditional Graph Neural Process
(CGNP), to fulfill the prior extraction and adaptation procedure.  A
meta CGNP model is a task-common node embedding function for
clustering, learned by metric-based graph learning, which
fully exploits the characteristics of CS.  We compare CGNP with
CS algorithms and ML baselines on real graphs with
ground-truth communities.
% \kfadd{
Our experiments verify that CGNP outperforms the other native
graph algorithms and ML/DL baselines 0.33 and 0.26 on \Fone score by average.
%}
The source code has been made available at
\textbf{https://github.com/FangShuheng/CGNP}.
\end{abstract}

\begin{IEEEkeywords}
Community search, Meta-learning, Neural process
\end{IEEEkeywords}

\section{Introduction}

Community is a cohesive subgraph that is densely intra-connected and
loosely inter-connected in a graph. Given any query nodes, community
search (CS) aims at finding communities covering the query nodes,
i.e., local query-dependent communities, which has a wide range of
real applications, e.g., friend recommendation, advertisement in
e-commence and protein complex
identification~\cite{DBLP:journals/vldb/FangHQZZCL20,
  DBLP:series/synthesis/2019Huang}.
%
%  Recently, a large number of CS
%  algorithms~\cite{DBLP:journals/vldb/FangHQZZCL20,
%  DBLP:series/synthesis/2019Huang} have been proposed to efficiently
%  search communities from large graphs in real-time.
% 
In the literature, to model structural cohesiveness, various
community models are adopted, including $k$-core~\cite{cs3,cs4,cs6},
$k$-truss~\cite{cs2,cs7}, $k$-clique \cite{cs1,cs8} and $k$-edge
connected component~\cite{cs9,cs10}. Such models can be computed
efficiently by CS algorithms.
%
But such models are designed based on some pre-defined community
patterns which are too rigid to be used to find ground-truth
communities in real applications.  We show a DBLP example in
Fig.~\ref{fig:case} in which nodes represent researchers
%
% in the
% field of computer science
%
and edges represent their collaboration.
% 
% The ground-truth community
% are identified by the researchers' interest and high-frequency
% keywords used in the papers.
%
The ground-truth community of 'Jure Leskovec', i.e., the orange and
white nodes in Fig.~\ref{fig:case} are with the researchers who have
collaborations and share the common interest of 'social
networks'. Such a community cannot be accurately found with any
$k$-related subgraph patterns. For example, in the community, some
nodes (e.g., Michael W. M.) have one neighbor, which can only be found
by $1$-core that may result in accommodating the whole graph.

\comment{
To this end,
it is quite difficult to choose a proper $k$ value as well as one
community metric to pursue high accuracy.

In other words, in real-world
graphs, it is difficult to have some universal community constraints
for us to

which is the overall best. One constraint may be either too loose or
too tight.


Even for one graph, the topology is diverse in different
subgraphs so that a fixed community constraint may not be consistently
applicable for different local queries.
}

\begin{figure}[t] 
	\centering 
	\includegraphics[width=0.34\textwidth]{fig/caseJure.pdf} 
	\vspace{-0.4cm}
	\caption{An Example on DBLP: Query `Jure Leskovec'} 
	\vspace{-0.4cm}
	\label{fig:case} 
\end{figure}


To tackle the structural inflexibility of CS algorithms, ML/DL-based
solutions~\cite{ICSGNN, AQDGNN} are arising as an attractive research
direction.  They build ML/DL models from given ground-truth
communities and expect the models to generalize to unknown
community-member relationships.
% Compared with traditional algorithms,
Such ML/DL-based approaches have achieved success in finding
high-quality solutions due to two reasons.  For one thing, these
data-driven approaches get rid of the inflexible constraints and adapt
to implicit structural patterns from data. For another thing, the
models can learn via error feedback from its predictions on the query
nodes in the ground-truth communities.
%
%It has been shown that compared with algorithmic approaches, the
%learning-based approaches are more successful in finding high-quality
%solutions.
%
But, effective error feedback heavily relies on sufficient
ground-truth communities to train, which are hard to collect and
label. On the one hand, they are labor-intensive, on the other hand, such
ground-truth communities for different query nodes can be very
different.

\comment{
Specifically, \cite{ICSGNN}
collects user feedback to update a model incrementally and
interactively. The model is trained for a specific query node, whose
effectiveness is fully determined by the quality of the given
ground-truth of that query node.
%
%The effectiveness will be degraded if the user cannot provide enough
%high-quality ground-truth for that query node.
%
\cite{AQDGNN} proposes a graph neural network based model that is
trained by a collection of query nodes with their ground-truth, and
makes predictions for unseen query nodes.  For one graph, a large
volume of query nodes with ground-truth communities are necessary for
training, which ensure the model well generalizes to other local
queries.
}


To deal with this problem, an effective solution is to inject prior
knowledge extracted from multiple CS tasks into the ML model,
%
% instead
% of training one model from scratch over the insufficient training data
% of one task.
%
where one CS task is a subgraph with a small number of
query nodes with partial ground-truth community membership.
% Fortunately,
The implicit prior knowledge of the CS tasks is rather intuitive,
i.e., for any query node of an arbitrary graph, its communities are
the nearby densely connected nodes that share similar attributes with
the query node. Such prior is shared by different CS tasks for
different query nodes in any real-world graphs.
%
% and also serves as the design principle of existing CS algorithms,
% in different forms, e.g., different $k$-related subgraph patterns.
%
%
%For example, in Fig.~\ref{fig:case}, the meta-model that predicts for
%`Jure' is trained by 10 tasks (graphs) drawn from DBLP, which are
%ego-network of `Jeff. Dean, xxxx, etc.', involving communities of
%various research interest. For each graph with more than 500 nodes,
%aaa query nodes with bbb positive and ccc negative samples are used
%for training.
%
%
\comment{
For example, the meta-model is constructed by multiple tasks, which
are ego-networks of researchers in different fields, possessing
several query nodes with ground-truth as training data.  Then, given
several query nodes, e.g., `U. Kang', with partial ground-truth for
the adaptation, the model can be adapted to the task of
Fig.~\ref{fig:case} to search communities for new query nodes, e.g.,
`Jure'.
}
%
%
And the prior knowledge is capable of synthesizing similar or
complementary inductive bias across different CS tasks to compensate
the insufficient knowledge from small training data, thus can be
swiftly adapted to a new task to test.
%
In this paper, we concentrate ourselves on learning a meta model to
capture this prior by meta-learning.

There are existing meta-learning algorithms, e.g., simple feature
transfer and model-agnostic meta-learning. However, trivial
adaptations to CS tasks fail to achieve high performance since they do
not exploit the intrinsic characteristic of the CS tasks.  For CS,
what a model needs to justify for each node in a graph is whether or
not it has its community membership with any given query node.  To
facilitate such binary justification, we propose a novel model,
Conditional Graph Neural Process (CGNP), to generate node embeddings
conditioned on the small training data, where the distance between a
node embedding to that of the query node explicitly indicates their
community membership.  Furthermore, as a graph specification of
Conditional Neural Process (CNP)~\cite{CNP}, CGNP inherits the main
ideas of CNP that implicitly learns a kernel function between a
training query node and a query node to be predicted.  \comment{ In a
  nutshell, our learned CGNP is a common embedding function that
  transforms the nodes of every graph into a distance-aware hidden
  space, as well as a common kernel function between query nodes
  across different graphs, which captures the CS prior knowledge.  }
In a nutshell, the learned CGNP is not only a \emph{common embedding
  function} but also a \emph{common kernel function}, shared across
different graphs. The embedding function transforms the nodes of each
graph into a distance-aware hidden space, while the kernel function
memorizes the small training data of each task as a hidden
representation.  Compared with optimization-based meta-learning
approaches whose parameters are easy to overfit, the metric learning
and memorization mechanisms are more suitable for classification tasks
with small training data, especially for imbalanced labels.
%
% Compared
% with graph algorithms for CS, CGNP learns some patterns that are
% adaptive to the graph data via error feedback with given
% ground-truth.
%
%
%Combining with Graph Neural Network, the metric-based learning
%approach, CGNP is particularly suitable for classification tasks over
%graphs with small training data. 

The contributions of this paper are summarized as follows:
%
%\begin{itemize}[noitemsep,topsep=0pt,parsep=5pt,partopsep=0pt,leftmargin=*]
\ding{172} We formulate the problem of learning a meta model to answer CS queries, where the meta model is to absorb the prior knowledge from multiple CS tasks and adapt to a specific task with only a few training data.  We generalize three CS task scenarios that represent comprehensive query cases. To the best of our knowledge, our study is the first attempt at meta model/algorithm for CS.
%
\ding{173} We explore three Graph Neural Network based solutions, i.e., feature transfer, model-agnostic meta-learning and Graph Prototypical Network, which are trivial adaptations of existing transfer/meta-learning algorithms to CS. 
We identify their individual limitations regarding prediction effectiveness, generalization capability and efficiency.
%
\ding{174} We propose a novel framework, \emph{Conditional Graph Neural Process} (CGNP) on the basis of conceptual CNP and learn the meta model in an efficient, metric-based learning perspective. We design and explore model variants with different model complexities and different options for the core components. To the best of our knowledge, we made the first effort to explore how to solve CS problem by meta-learning.
%We propose a novel framework, \emph{Conditional Graph Neural Process} (CGNP) that learns the meta-model in an efficient, metric-based learning perspective. We design and explore three CGNP models with different model complexities and different options for the core components.
%
% \kfadd{
\ding{175} We conduct extensive experiments on 6 real-world datasets
with ground-truth communities for performance evaluation. Compared
with 3 CS algorithms, 4 naive approaches, and 3 supervised
learning validates our CGNP outperforms the others
%algorithmic and
%ML-based approaches 0.33 and 0.26 on the \Fone on average,
with small training and prediction cost.
% }
%\end{itemize}

%1. study ml for community search, meta model. Discuss  2 native solution.. 
%2. propose CNP differnet variants 
%3. extensive experimentï¼Œ compare with xxx, achieve xxx performance



%\textbf{Motivation.} 
%ICS-GNN is a learning-based model proposed for interactive community search. It begins from building a candidate subgraph from the query node. Then it apply a GNN model on subgraph and learns the node representation. Finally, a BFS-based algorithm is deployed to find the $k$-sized maximum GNN scores and locate the target community. 

%However, it has some inevitable drawbacks. The first thing is about the pre-defined $k$-sized community. From the perspective of interactive community search, it is convenient for users to adjust $k$ according to their requirements. However, some applications require us to find entire community in subgraphs. For example, we need to figure out the size of interest groups of a student participated in or to find out the circles that contains a specific users in the online social network. For ICS-GNN, the searched community is dependent of size $k$, and it's difficult for us to set a proper $k$ to find the complete community that contains the query node. Second, a model of ICS-GNN can only applies to a specified query node. Each time we encounter a new graph or even a new query node, it has to retrain a new model. That's lack of efficiency. 

%Consider these challenges above, we come up to a meta learning method for community search without a pre-defined community size. Our model can be applied to different graphs and different query nodes after it trained. 


\stitle{Roadmap:} 
The rest of the paper is organized as follows. 
\cref{sec:related} reviews the relative work. In \cref{sec:problem}, we give the problem statement followed by three naive solutions introduced in \cref{sec:naive}.
We introduce the core idea of our approach, CGNP in \cref{sec:metric}. We elaborate on its architecture design and present the learning algorithms of CGNP in \cref{sec:CGNP}. 
We present our comprehensive experimental studies in \cref{sec:exp} and conclude the paper in \cref{sec:conclusion}.

\input{related}
\input{main}
\input{experiment}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we study leveraging ML/DL approaches for community search (CS), under the circumstance that the training data is scarce. 
We propose a metric-based meta-learning framework, Conditional Graph Neural Process (CGNP) to learn a meta model to capture the prior knowledge of CS.
The meta model is adapted to a new task swiftly to make predictions of the community membership, where a task is a graph with only a few given ground-truth. 
To the best of our knowledge, CGNP is the first meta-learning model for CS that utilizes the generalization ability of neural networks to the greatest extent.
Compared with algorithmic approaches, CGNP supports flexible community structures learned from the data. Compared with general meta-learning algorithms, CGNP further exploits the characteristic of CS. Our extensive experiments demonstrate that CGNP outperforms the two lines of approaches significantly regarding accuracy and efficiency.

\section*{Acknowledgment}
This work was supported by the Research Grants Council of Hong Kong, China, No. 14203618, No. 14202919 and No. 14205520.


\bibliographystyle{IEEEtran}
\bibliography{ref}


\end{document}
\endinput
