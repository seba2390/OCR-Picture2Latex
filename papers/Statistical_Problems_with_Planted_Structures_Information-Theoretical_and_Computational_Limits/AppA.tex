\section{Mutual information-characterization of correlated recovery}
\label{app:MIcorr}

\newcommand{\Unif}{\mathrm{Unif}}


We consider a general setup:  Let the number of communities $k$ be a constant. Denote the membership vector by $\sigma=(\sigma_1,\ldots,\sigma_n) \in [k]^n$ and the observation is $A=(A_{ij}: 1 \leq i < j \leq n)$. Assume the following conditions:
\begin{enumerate}[label=A\arabic*]
	%[{A}1.]
	\item \label{A1}
	For any permutation $\pi\in S_k$, $(\sigma,A)$ and $(\pi(\sigma),A)$ are equal in law, where $\pi(\sigma)\triangleq (\pi(\sigma_1),\ldots,\pi(\sigma_n))$; 
	
	\item \label{A2}
	For any $i \neq j \in [n]$, $I(\sigma_i,\sigma_j;A)= I(\sigma_1,\sigma_2;A)$;
	
	\item \label{A3}
	For any $z_1,z_2 \in [k]$, $\prob{\sigma_1=z_1,\sigma_2=z_2} = \frac{1}{k^2} + o(1)$ as $n\to \infty$.
\end{enumerate}
These assumptions are satisfied for example for $k$-community SBM (where each pair of vertices $i$ and $j$ are connected independently with probability $p$ if $\sigma_i=\sigma_j$ and $q$ otherwise),\index{Stochastic block model (SBM)! $k$ communities}
 and the membership vector
$\sigma$ can be either uniformly distributed on $[k]^n$ or the set of equal-sized $k$-partition of $[n]$. 

Recall that correlated recovery entails the following:
For any $\sigma , \hat{\sigma} \in [k]^n$, define the overlap:
\begin{align}
o\left( \sigma, \hat{\sigma} \right) = \frac{1}{n} \max_{\pi \in S_k} 
\sum_{i \in [n] } \left( \indc{ \pi\left(\sigma_i \right) = \hat{\sigma}_i} - \frac{1}{k} \right).
\end{align}
We say an estimator $\hat{\sigma} = \hat{\sigma}(A)$ achieves correlated recovery if\footnote{For the special case of $k=2$, \prettyref{eq:corro} is equivalent to 
$\frac{1}{n}\Expect[|\Iprod{\sigma}{\hat \sigma}|] = \Omega(1)$, where $\sigma , \hat{\sigma}$ are assumed to be $\{\pm\}^n$-valued.}
\begin{equation}
\expect{o\left( \sigma, \hat{\sigma} \right)}=\Omega(1),
\label{eq:corro}
\end{equation}
 that is, the misclassification rate, up to a global permutation, outperforms random guessing.
Under the above three assumptions, we have the following characterization of correlated recovery:
\begin{lemma}
\label{lmm:MIcorr}	
Correlated recovery is possible if and only if 
$I(\sigma_1, \sigma_2 ; A) = \Omega(1)$.	
\end{lemma}



\begin{proof}
We start by recalling the relation between mutual information and total variation.
For any pair of random variables $(X,Y)$, define the so-called $T$-information \cite{Csiszar96}:
$T(X;Y) \triangleq \TV(P_{XY}, P_XP_Y) = \Expect[\TV(P_{Y|X}, P_Y)]$.
For $X \sim \Bern(p)$, this simply reduces to 
\begin{equation}
T(X;Y) = 2p(1-p) \TV(P_{Y|X=0}, P_{Y|X=1}).
\label{eq:Tbern}
\end{equation}
Furthermore, the mutual information can be bounded by the $T$-information, 
by Pinsker's and Fano's inequality, as follows \cite[Eq.~(84) and Prop.~12]{PW14a}
\begin{equation}
 2 T(X;Y)^2 \leq I(X;Y)  \leq \log (M-1) T(X;Y) + h(T(X;Y))		
	\label{eq:TI}
\end{equation}
where in the upper bound $M$ is the number of possible values of $X$, and $h$ is the binary entropy function in \prettyref{eq:binaryentropy}.


We prove the ``if'' part.
Suppose 
%correlated recovery is impossible and for the sake of contradiction,
$I(\sigma_1, \sigma_2; A) = \Omega(1)$.
We first claim that assumption \ref{A1} implies that 
\begin{equation}
I (\indc{\sigma_1 =\sigma_2}; A)=I(\sigma_1, \sigma_2; A)
\label{eq:ss}
\end{equation}
that is,
 $A$ is independent of $\sigma_1,\sigma_2$ conditional on $\indc{\sigma_1 =\sigma_2}$. 
%\nbr{JX. Okay. I think we need this condition to ensure that 
%correlated recovery is impossible implies $I(\sigma_1, \sigma_2; A) = o(1)$. 
%Otherwise, we could have the situation that $A=a$ for $\sigma_1=+1$ and
%$A=b$ for $\sigma_1=-1$. In this case, correlated recovery of $\sigma$ is certainly 
%impossible, but $I(\sigma_1, \sigma_2;A)=I(\sigma_1; A)=1$.}
Indeed, 
for any $z \neq z'\in [k]$, let $\pi$ be any permutation such that 
%$\pi(z)=z'$ and 
$\pi(z')=z.$
%that interchanges $z$ and $z'$. 
Since $P_{\sigma, A} =  P_{\pi(\sigma), A}$, 
we have $P_{A|\sigma_1=z,  \sigma_2=z} =  P_{A| \pi(\sigma_1)=z,  \pi(\sigma_2)=z }$, i.e., 
$P_{A |\sigma_1=z, \sigma_2=z} =  P_{A| \sigma_1 =z',  \sigma_2 =z' }$. 
Similarly, one can show that 
$P_{A|\sigma_1=z_1, \sigma_2=z_2} =  P_{A| \sigma_1 =z_1',  \sigma_2 =z_2'}$, 
for any $z_1 \neq z_2$ and $z_1'\neq z_2'$, and this proves the claim.
%Since $\calL(\sigma, A) =  \calL(\pi(\sigma), A)$, where $\calL(\cdot)$ denote the law, 
%we have $\calL(A|\sigma_1=z,  \sigma_2=z) =  \calL(A| \pi(\sigma_1)=i,  pi(\sigma_2)=i )$, i.e., 
%$\calL(A|\sigma_1=z, \sigma_2=z) =  L(A| \sigma_1 =z',  \sigma_2 =z' )$. 
%Similarly, one can show that 
%$\calL(A|\sigma_1=z_1, \sigma_2=z_2) =  L(A| \sigma_1 =z_1',  \sigma_2 =z_2')$, for any $z_1 \neq z_2$ and $z_1'\neq z_2'$, and this proves the claim.

Let $x_j=\indc{\sigma_1 =\sigma_j}$.
By the symmetry assumption \ref{A2}, 
$I(x_j; A) = I(x_2; A) = \Omega(1)$ for all $j \neq 1$.
Since $\prob{x_j = 1} = \frac{1}{k} + o(1)$ by assumption \ref{A3}, applying \prettyref{eq:TI} with $M=2$ and in view of \prettyref{eq:Tbern}, we have
$\TV(P_{A|x_j=0},P_{A|x_j=1})=\Omega(1)$.
Thus, there exists an estimator $\widehat{x}_j \in \{0,1\}$ as a function of $A$, such that
\begin{align}
\prob{\widehat{x}_j = 1 \mid x_j =1 } + 
\prob{\widehat{x}_j = 0 \mid x_j =0 } \ge 1+\TV(P_{A|x_j=0},P_{A|x_j=1})
= 1+ \Omega(1).  \label{eq:estimator_x_hat}
\end{align}

Define $\hat\sigma$ as follows: set $\hat\sigma_1= 1 $; for $j \neq 1$, set $\hat \sigma_j = 1 $ if $\widehat{x}_j = 1$ 
and draw $\hat \sigma_j $ from $\{2,\ldots,k\}$ uniformly at random 
if $\widehat{x}_j = 0$.
Next, we show that $\hat\sigma$ achieves correlated recovery. 
Indeed, fix a permutation $\pi \in S_k$ such that $\pi(\sigma_1)=1$. It follows from 
the definition of overlap that
\begin{equation}
\Expect[o\left(\sigma, \hat{\sigma} \right)]
\ge \frac{1}{n} \sum_{j \neq 2} \prob{\pi(\sigma_j) =\hat\sigma_j} - \frac{1}{k}.
\label{eq:olb}
\end{equation}
Furthermore, since $\pi(\sigma_1)=1$, we have, for any $j\neq 1$,
\[
\prob{\pi(\sigma_j) = \hat\sigma_j,x_j=1}=
\prob{\hat x_j=1,x_j=1}
\]
and
\[
\prob{\pi(\sigma_j) = \hat\sigma_j,x_j=0}=
\prob{\pi(\sigma_j) = \hat\sigma_j, \hat x_j=0,x_j=0}=
\frac{1}{k-1} \prob{\hat x_j=0,x_j=0},
\]
where the last step is because conditional on $\hat{x}_j=0$,
$\hat{\sigma}_j$ is chosen  from $\{2,\ldots,k\}$ uniformly and 
independently of everything else.
Since $\prob{x_j = 1} = \frac{1}{k} + o(1)$, we have
\[
\prob{\pi(\sigma_j) =\hat\sigma_j} = \frac{1}{k}(\prob{\widehat{x}_j = 1 \mid x_j =1 } + 
\prob{\widehat{x}_j = 0 \mid x_j =0 }) +o(1) \overset{\prettyref{eq:estimator_x_hat}}{\ge} \frac{1}{k}+ \Omega(1).  
\]
By \prettyref{eq:olb}, we conclude that $\hat{\sigma}$ achieves correlated recovery
of $\sigma$.

Next we prove the ``only if'' part.
Suppose $I(\sigma_1, \sigma_2; A) = o(1)$ and we aim to show 
%the impossibility of correlated recovery, that is, 
$\expect{o\left(\sigma, \hat{\sigma} \right)}=o(1)$ for any estimator $\hat\sigma$.
By the definition of overlap, we have
\begin{align*}
o\left(\sigma, \hat{\sigma} \right) 
\le 
\frac{1}{n} \sum_{\pi \in S_k}
\left| \sum_{i \in [n] }  
\left( \indc{ \pi\left(\sigma_i \right) = \hat{\sigma}_i} - \frac{1}{k}  \right) \right|.
%& \le \frac{1}{n} \sum_{\pi \in S_k}
%\sum_{\ell=1}^k 
%\left| \sum_{i \in [n] } \left( \indc{ \pi\left(\sigma_i \right) =\ell} 
%\indc{ \hat{\sigma}_i = \ell} - \frac{1}{k^2} \right)\right|.
\end{align*}
Since there are $k!=\Omega(1)$ permutations in $S_k$, it suffices to show
for any fixed permutation $\pi$,
$$
\expect{ \left| \sum_{i \in [n] } \left( \indc{ \pi\left(\sigma_i \right) = \hat{\sigma}_i} - \frac{1}{k}  \right) 
\right| } = o(n).
$$
Since $I(\pi(\sigma_i), \pi(\sigma_j); A)=I(\sigma_i, \sigma_j; A)$, without loss of generality, 
we assume $\pi=\text{id}$ in the following. By the Cauchy-Schwarz inequality, it further suffices to show
\begin{equation}
\expect{ \left( \sum_{i \in [n] } \left( \indc{ \sigma_i =\hat{\sigma}_i } - \frac{1}{k} \right)\right)^2 } =o(n^2).
\label{eq:overlap2}
\end{equation}
Note that 
\begin{align*}
& \expect{  \left( \sum_{i \in [n] } \left(
   \indc{ \sigma_i  = \hat{\sigma}_i } - \frac{1}{k} \right)   \right)^2 } \\
 & = \sum_{i, j \in [n] } 
 \expect{ \left( \indc{ \sigma_i = \hat{\sigma}_i }- \frac{1}{k}  \right) 
 \left( \indc{ \sigma_j  = \hat{\sigma}_j }- \frac{1}{k}  \right) }  \\
 & =  \sum_{i, j \in [n] }  \prob{  \sigma_i = \hat{\sigma}_i , \sigma_j  = \hat{\sigma}_j }
 - \frac{2n}{k} \sum_{i \in [n] }\prob{ \sigma_i = \hat{\sigma}_i } + \frac{n^2}{k^2}.
\end{align*}
For the first term in the last displayed equation, 
let $\sigma'$ be identically distributed as $\hat\sigma$ but independent of $\sigma$.
Since $I(\sigma_i,\sigma_j;\hat\sigma_i,\hat\sigma_j) \le I(\sigma_i,\sigma_j;A)=o(1)$ by the data processing inequality, it follows from the lower bound in 
\prettyref{eq:TI} that $\TV(P_{\sigma_i,\sigma_j,\hat\sigma_i,\hat\sigma_j}, P_{\sigma_i,\sigma_j,\sigma_i',\sigma_j'})=o(1)$.
Since 
$\pprob{  \sigma_i = {\sigma}_i', \sigma_j  = {\sigma}_j' } \leq 
\max_{a,b \in [k]} \prob{  \sigma_i = a, \sigma_j  = b} \leq \frac{1}{k^2}+o(1)$ by assumption \ref{A3}, 
we have
$$
\prob{  \sigma_i = \hat{\sigma}_i , \sigma_j  = \hat{\sigma}_j }
\leq \frac{1}{k^2} + o(1),
$$
Similarly, for the second term, we have
$$
\prob{ \sigma_i = \hat{\sigma}_i } = \frac{1}{k} +o(1),
$$
where the last equality holds due to $I(\sigma_i; A) =o(1).$
Combining the last three displayed equations gives \prettyref{eq:overlap2} and completes the proof.
\end{proof}


%The argument below is essentially contained in \cite{PW18}.
%
%We first consider $k=2$, in which case it is convenient to assume $\sigma \in \{\pm\}^n$. Recall that correlated recovery amounts to reconstruct $\sigma$ (up to a global sign flip) better than chance, that is, find $\hat \sigma=\hat \sigma(Y) \in \{\pm1\}^n$, such that
%\begin{equation}
%%\liminf_{n \to\infty} \frac{1}{n}\Expect[|\Iprod{\sigma}{\hat \sigma}|] > 0.
%\frac{1}{n}\Expect[|\Iprod{\sigma}{\hat \sigma}|] = \Omega(1).
%%> \epsilon
%\label{eq:corr}
%\end{equation}
%%for some constant $\epsilon$.
%Note that by symmetry, for any $i\neq j$, $I(\sigma_i,\sigma_j;A) = I(\sigma_i \sigma_j; A) = I(\sigma_1 \sigma_2; A)$. Suppose $I(\sigma_1 \sigma_2; A) = \Omega(1)$, then for any $j \neq 1$, there exists an estimator $\widehat{\sigma_1\sigma_j}$ as a function of $A$, such that
%$\prob{\widehat{\sigma_1\sigma_j} \neq \sigma_1\sigma_j} \leq \frac{1}{2} - \Omega(1)$. Thus, setting $\hat\sigma$ according to $\hat\sigma_1=+$ and $\hat \sigma_j = \hat{\sigma_1\sigma_j}$ achieves \prettyref{eq:corr}. 
%This shows \prettyref{eq:MIcorr2} is necessary for the impossibility of correlated recovery. 
%%To show \prettyref{eq:MIcorr2} implies the impossibility of correlated recovery,
%To prove the sufficiency, 
%for any estimator $\hat \sigma= \hat \sigma(A) \in \{\pm\}^n$, 
%since $I(\sigma_i \sigma_j; A)=o(1)$, which is equivalent to 
%$\TV(P_{A|\sigma_i \sigma_j=+}, P_{A|\sigma_i \sigma_j=-})=o(1)$, we have 
%$\prob{\sigma_i\sigma_j \neq \hat \sigma_i\hat \sigma_j} \geq \frac{1}{2}-o(1)$. 
%On the other hand, we have the equality:
%\begin{align*}
%2n^2 - 2 \eexpect{\iprod{\sigma}{\hat\sigma}^2} 
%= & ~ \Expect\Fnorm{\sigma \sigma^\top - \hat \sigma\hat \sigma^\top}^2 \\
%= & ~ 4 \sum_{i \neq j} \prob{\sigma_i\sigma_j \neq \hat \sigma_i\hat \sigma_j} = 2n^2-o(n^2).
%\end{align*}
%Thus, $\eexpect{\iprod{\sigma}{\hat\sigma}^2} =o(n^2)$, which implies $\eexpect{|\Iprod{\sigma}{\hat\sigma}|} =o(n)$.
%
%Next we consider $k \geq 3$, in which case correlated recovery is achieved if there exists an estimator $\hat \sigma \in [k]^n$ that outperforms random guessing, i.e., 
%\begin{equation}
%\Expect[d(\sigma,\hat \sigma)] \leq \frac{k-1}{k} - \Omega(1).
%\label{eq:corr-sbmk}
%\end{equation}
%Here the loss function $d$ is the fraction of classification errors up to a global permutation of labels, formally defined as follows: for any $\sigma,\hat \sigma \in [k]^n$, 
%\begin{equation}
%d(\sigma,\hat \sigma) \triangleq  \min_{\pi \in S_k} \frac{1}{n}\sum_{i\in[n]} \indc{\sigma_i \neq \pi(\hat \sigma_i)}
%\label{eq:lossd}
%\end{equation}
%where $S_k$ is the collection of permutations on $[k]$.
%
%%By symmetry, \prettyref{eq:MIcorrk} implies that for any fixed $m$, as $n\diverge$,
%%\begin{equation}
%%I(\sigma_S; A) = o(1)
%%\label{eq:IXSY}
%%\end{equation}
%%for any $S \in \binom{[n]}{m}$, where 
%To show the impossibility of correlated recovery on the basis of \prettyref{eq:MIcorrk}, first of all, note that for any fixed $x,\hat x\in[k]^n$ and any $m\in [n]$ we have
%\begin{equation}
%d(x,\hat x)
%\geq \Expect_{S}[d(x_{\sfS},\hat x_{\sfS})] \label{eq:davg}
%\end{equation}
%where ${\sfS}  \sim \Unif(\binom{[n]}{m})$ and recall that for any $S$, we have 
%$d(x_S,\hat x_S) = \frac{1}{|S|}  \min_{\pi \in S_k} \sum_{i\in S} \indc{x_i \neq \pi(\hat x_i)}$ per \prettyref{eq:lossd}. The inequality \prettyref{eq:davg} simply follows from
%\begin{align}
%d(x,\hat x)
%= & ~ \min_{\pi \in S_k} \probs{x_I \neq \hat x_{\pi(I)}}{I \sim \Unif([n])}	\nonumber \\
%= & ~ \min_{\pi \in S_k} \Expect_{\sfS \sim \Unif(\binom{[n]}{m})} \probs{x_I \neq \hat x_{\pi(I)}}{I \sim \Unif({\sfS})}	\nonumber \\
%= & ~ \Expect_{{\sfS}} \min_{\pi \in S_k} \probs{x_I \neq \hat x_{\pi(I)}}{I \sim \Unif({\sfS})}	\nonumber \\
%\geq & ~ \Expect_{{\sfS}} [d(x_{\sfS},\hat x_{\sfS})] \nonumber.
%\end{align}
%%Fix a constant $m$ independent of $n$. 
%For any estimator $\hat \sigma=\hat \sigma(Y) \in [k]^n$, applying \prettyref{eq:davg} yields
%\begin{equation}
%\Expect[d(\sigma_{\sfS},\hat \sigma_{\sfS})] \leq \Expect[d(\sigma,\hat \sigma)], \label{eq:davg2}
%\end{equation}
%where ${\sfS}$ is a random uniform $m$-set independent of $\sigma,\hat \sigma$.
%
%
%By the data processing inequality, we have for any $S \in \binom{[n]}{m}$,
%\[
%I(\sigma_S; \hat \sigma_S) \leq I(\sigma_S; A) = I(\sigma_1,\ldots,\sigma_m; A) \overset{\prettyref{eq:MIcorrk}}{=} o(1),
%\]
%as $n\diverge$.
%By Pinsker's inequality, we have 
%$\TV(P_{\sigma_S, \hat \sigma_S}, P_{\sigma_S} \otimes P_{\hat \sigma_S}) \leq \sqrt{2 I(\sigma_S; \hat \sigma_S)} = o(1)$.
%Since the loss function $d$ defined in \prettyref{eq:lossd} is bounded by one, we have
%\begin{equation}
%\Expect[d(\sigma_S,\hat \sigma_S)] \geq \Expect[d(\sigma_S,\sigma'_S)] - \TV(P_{\sigma_S, \hat \sigma_S}, P_{\sigma_S} \otimes P_{\hat \sigma_S}) 
%= \Expect[d(\sigma_S,\sigma'_S)] + o(1), 
%\label{eq:ddTV}
%\end{equation}
%where $\sigma'_S$ has the same marginal distribution as $\hat \sigma_S$ but independent of $\sigma_S$.
%By \prettyref{lmm:randomguess} below, we have
%\begin{equation}
%\Expect[d(\sigma_S,\sigma'_S)] \geq \pth{\frac{k-1}{k} - m^{-1/3}}(1-k! e^{-2m^{1/3}}).
%\label{eq:randomguess2}
%\end{equation}
%Averaging \prettyref{eq:randomguess2} over $S \in \binom{[n]}{m}$ then combining with \prettyref{eq:davg2} and \prettyref{eq:ddTV}, and finally sending $n\to\infty$ followed by $m \to \infty$, we conclude that $\eexpect{ d(\sigma,\hat \sigma)} \geq \frac{k-1}{k}-o(1)$, hence the impossibility of correlated recovery.
%
%
%\begin{lemma}
%\label{lmm:randomguess}	
	%Let $\sigma$ be uniformly distributed on $[k]^m$ and $\sigma'$ is independent of $\sigma$ with an arbitrary distribution on $[k]^m$. 
%For the loss function in \prettyref{eq:lossd}, we have\footnote{Note that for any fixed $k,m$ and any string $x,z\in [k]^m$, we can always outperform random matching, i.e., $d(x,z) < \frac{k-1}{k}$. The point of \prettyref{eq:randomguess} is that this improvement is negligible for large $m$.}
	%\begin{equation}
	%d(\sigma,\sigma') \geq \frac{k-1}{k} - m^{-1/3}
	%\label{eq:randomguess}
	%\end{equation}
	%with probability at least $1-(k! e^{-2m^{1/3}})$.
%\end{lemma}
%\begin{proof}
	%For each fixed $\pi$, the Hamming distance $d_H(\sigma,\pi(\sigma'))\sim \Binom(m,\frac{k-1}{k})$. From Hoeffding's inequality we have
	%$$ \Prob[d_H(\sigma,\pi(\sigma') < {k-1\over k} - \delta] \le e^{-2m \delta^2}\,,$$
	%and from the union bound
	%$$ \Prob[\min_\pi d_H(\sigma,\pi(\sigma') < {k-1\over k} - \delta] \le k! e^{-2m \delta^2}\,.$$
	%Setting $\delta = m^{-1/3}$ completes the proof.
%\end{proof}



%\section{Proof of \prettyref{eq:second_moment_conditional} $\implies$ \prettyref{eq:MI_TV} and verification in the binary symmetric SBM}
%\label{app:MITV}
%Let $S=[m]$ and denote $\sigma_1,\ldots,\sigma_m$ by $\sigma_S$.
%Recall that $m$ is a constant and $\P_z \triangleq \P_{A|\sigma_S=z}$ for $z\in[k]^m$.
%We first prove the following:
%%\begin{align}
%%I(\sigma_S; A) = o(1) & \Leftrightarrow D \left( \P_{A | \sigma_S} \| \P \right) =o(1), \quad \forall \sigma_S  \nonumber \\
%%& \Leftrightarrow d_{\rm TV} \left( \P_{A | \sigma_S}, \P \right) =o(1), \quad \forall \sigma_S. \nonumber \\
%%& \Leftarrow d_{\rm TV} \left( \P_{A | \sigma_S}, \P_{A | \tsigma_S} \right) =o(1), \quad \forall \sigma_S, \tsigma_S,
%%\label{eq:MI_TV}
%%\end{align}
%\begin{align}
%I(\sigma_S; A) = o(1) & \Leftrightarrow D \left( \P_z \| \P \right) =o(1), \quad \forall z, \label{eq:MI_TV1}\\
%& \Leftrightarrow d_{\rm TV} \left( \P_z, \P \right) =o(1), \quad \forall z, \label{eq:MI_TV2}\\
%& \Leftarrow d_{\rm TV} \left( \P_z, \P_{\tz} \right) =o(1), \quad \forall z, \tz. \label{eq:MI_TV}
%\end{align}
%For \prettyref{eq:MI_TV1}, by definition,
%$$
%I(\sigma_S; A) =  \Expect_{\sigma_S} \qth{ D \left( \P_{A|\sigma_S} \| \P \right) }.
%$$
%Note that the distribution of $\sigma_S$ has a finite support
%and $\Omega(1)$ probability mass on each possible value. 
%Therefore, $I(\sigma_S; A)=o(1)$ if and only if 
%$D \left( \P_{A|\sigma_S=z} \| \P \right) = o(1)$ for all $z$.
%
%For \prettyref{eq:MI_TV2}, by Pinsker's inequality, $D \left( \P_{z} \| \P \right) = o(1)$
%implies $d_{\rm TV} \left( \P_{z}, \P \right) = o(1)$. Conversely, 
%suppose that $d_{\rm TV} \left( \P_{z}, \P \right) = o(1)$. Then
%\begin{align*}
%D \left( \P_{z} \| \P \right) &= \int \P_{z} \log \frac{\P_{z}  }{\P} \\
%& \overset{(a)}{\le} \int  \P_{z}  \frac{\P_{z} - \P }{\P} \\
%&  \le \int  \frac{ \P_{z} }{ \P } \left| \P_{z} - \P  \right| \\
%& \overset{(b)}{=} O(1) \times \int \left| \P_{z} - \P  \right| \\
%& = O\left( d_{TV}  \left( \P_{z}, \P \right) \right) = o(1),
%\end{align*}
%where $(a)$ is due to $\log x \le x-1$, and $(b)$ follows because 
%$\frac{ \P_{z}(a) }{ \P(a) } = \frac{ \P_{A|\sigma_S=z}(a) }{ \P_A(a)} \leq \frac{1}{\prob{\sigma_S=z}} = O(1)$ everywhere.
%%, and  the distribution of $\sigma_S$ has $\Omega(1)$ probability mass on each realization and hence 
%
%
%For \prettyref{eq:MI_TV}, suppose that $d_{\rm TV} \left( \P_{z}, \P_{\tz} \right) =o(1)$
%for all $z$ and $\tz$. By the convexity of $d_{\rm TV}(\cdot,\cdot)$ and Jensen's inequality, it readily
%follows that $d_{\rm TV} \left( \P_{z}, \P \right) =o(1)$.
%
%
%
%Next we prove that for any reference distribution $\Q$,  \prettyref{eq:second_moment_conditional} implies 
%$d_{\rm TV} \left( \P_{z}, \P_{\tz} \right) =o(1)$
%for all $z,\tz$, which further implies \prettyref{eq:MIcorrk} in view of \prettyref{eq:MI_TV}.
 %Indeed, by Cauchy-Schwartz inequality, we have
%\begin{align}
%d_{\rm TV}  \left( \P_{z}, \P_{\tz} \right) & =
%\frac{1}{2} \int  \left| \P_{z} - \P_{ \tz} \right| \nonumber \\
%%& = \frac{1}{2} \int  \left| \P_{z} - \P_{\tz} \right| \frac{\sqrt{\Q}}{\sqrt{\Q}} \nonumber \\
%& \le \frac{1}{2}  \left( \int \Q  \right)^{1/2} \left( \int  \frac{  \left( \P_{z} - \P_{\tz} \right)^2 }{\Q }  \right)^{1/2}
%\nonumber \\
%& = \frac{1}{2} \left(    \int \frac{\P^2_{A | z}}{\Q} +\int \frac{\P^2_{A | \tz}}{\Q} - 2 \int \frac{ 
%\P_{z} \P_{\tz} }{ \Q} \right)^{1/2} \overset{\prettyref{eq:second_moment_conditional}}{=} o(1).
%\end{align}
%%where the last equality holds from the assumption \prettyref{eq:second_moment_conditional}. 
%
%
%
%Finally, we consider the binary symmetric SBM and show that,
%below the correlated recovery threshold $\tau=\frac{(a-b)^2}{2(a+b)}<1$, 
 %\prettyref{eq:second_moment_conditional} is satisfied if the reference distribution $\Q$ is the distribution of $A$ in
%the null (\ER) model.
%Specifically, following the derivations in \prettyref{eq:SBM_second_moment_eq},
%we have
%\begin{align}
%\int \frac{   \P_{z}  \P_{\tz} }{ \Q } 
%&= \Expect \qth{  \prod_{i < j} \left(1 +  \sigma_i \sigma_j \tsigma_i \tsigma_j \rho \right)
%\mathrel{\bigg|} \sigma_S=z, \tsigma_S =\tz}  \nonumber \\
%& =  \left( 1+o(1) \right) e^{ -\tau^2/4 -\tau/2} \times 
%\Expect \qth{ \exp \left( \frac{\rho}{2} \iprod{ \sigma}{\tsigma}^2  \right) \mathrel{\Big|} \sigma_S=z, \tsigma_S =\tz},
%%& = \prod_{(i,j) \in \calE(S) } \left(1 +  \sigma_i \sigma_j \tsigma_i \tsigma_j \rho \right)  \nonumber \\
%%& \times \Expect \qth{ \prod_{i \in S, j \in S^c}  \left(1 +  \sigma_i \sigma_j \tsigma_i \tsigma_j \rho \right) \mid \sigma_S, \tsigma_S } \nonumber \\
%%& \times \Expect \qth{ \prod_{(i,j)\in \calE(S^c)}  \left(1 +  \sigma_i \sigma_j \tsigma_i \tsigma_j \rho \right) \mid \sigma_S, \tsigma_S }.
%\end{align}
%where the last equality holds because $m$ is a constant, $\rho=\tau/n + O(1/n^2)$ and $\log(1+x) = x -x^2/2 +O(x^3)$. 
%%\nbr{
%%I got $e^{ -\tau^2/4 - \tau/2}$, because 
%%$\sum_{i < j} \sigma_i \sigma_j \tsigma_i \tsigma_j = \frac{1}{2}(
%%\iprod{ \sigma}{\tsigma}^2 - n)$.}
%
%Write $\sigma=2\xi-1$ for $\xi\in\{0,1\}^n$ and let 
%$$
%H_1\triangleq \Iprod{\xi_{S}} { \tilde{\xi}_{S} } \quad \text{ and } \quad 
%H_2\triangleq \Iprod{\xi_{S^c}} { \tilde{\xi}_{S^c} }.
%$$ 
%Then $\iprod{\sigma}{\tsigma} = 4 (H_1+H_2) -n$.
%Moreover, conditional  on $\sigma_S$ and $\tsigma_S$, 
%$$
%H_2 \sim \text{Hypergeometric} \left( n-m, n/2 - \| \xi_S\|_1, n/2 - \|\tilde{\xi}_S \|_1 \right).
%$$
%Therefore
%\begin{align*}
%\Expect \qth{ \exp \left(  \frac{\rho}{2} \iprod{ \sigma}{\tsigma}^2  \right) \mathrel{\bigg|} \sigma_S=z, \tsigma_S =\tz}
%& =\expect{ \exp \left(  \frac{n\rho}{2} \left( \frac{ 4 H_1 + 4H_2 - n}{\sqrt{n} } \right)^2 \right) \mathrel{\bigg|} \sigma_S=z, \tsigma_S =\tz}  \\
%& = \frac{1+o(1)}{\sqrt{1-\tau}},
%\end{align*}
%where the last inequality holds because $n \rho = \tau +o(1/n)$ and 
%conditional on $\sigma_S$ and $\tsigma_S$,
%$ \frac{1}{\sqrt{n}} ( 4H_1 + 4H_2 - n )$ converges to $\calN(0,1)$ in distribution 
%as $n \to \infty$ by the central limit theorem for hypergeometric distribution. 
%
%In conclusion, we have shown that 
%$$
%\int \frac{   \P_{z}  \P_{\tz} }{ \Q}  
%= \left( 1+o(1) \right) e^{ -\tau^2/4 - \tau/2}  \frac{1}{\sqrt{1-\tau}}, \quad \forall z,\tz.
%$$
%%Hence, by taking the expectation of $\sigma_S$ and $\tsigma_S$ over the both hand sides of the last displayed equation, we get that
%Averaging both sides over $z,\tz$ yields
%$$
%\int \frac{   \P^2 }{ \Q}  
%= \left( 1+o(1) \right) e^{ -\tau^2/4 - \tau/2}  \frac{1}{\sqrt{1-\tau}}.
%$$
%Thus \prettyref{eq:second_moment_conditional} is satisfied. 



