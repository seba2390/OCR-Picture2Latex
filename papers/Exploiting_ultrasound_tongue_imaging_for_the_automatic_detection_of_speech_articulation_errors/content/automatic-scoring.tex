\section{Automatic speech error detection}
\label{sec:automatic_detection}

In this section we investigate the automatic detection of speech errors in typically developing and disordered Scottish English child speech.
The proposed system is designed as a tool to be used by Speech and Language Therapists on data collected from speech therapy and assessment sessions.
Therefore, we evaluate model scores with the expert scores for velar fronting and gliding of /r/ provided by therapists in Section \ref{sec:expert_detection}.
Our \textbf{goal} is to investigate the proposed system's ability to simulate expert behaviour in the detection of substitution errors.
Additionally, we aim to analyse the contribution of \emph{ultrasound tongue imaging} and \emph{out-of-domain adult data} on the automatic speech error detection.

\subsection{Data preparation}

\begin{table}[]
\centering
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{@{}ccccl@{}}
\toprule
\multicolumn{1}{c}{\textbf{Data set}} & \multicolumn{1}{c}{\textbf{Source}} & \multicolumn{1}{c}{\textbf{Speakers}} & \multicolumn{1}{c}{\textbf{Samples}} & \multicolumn{1}{c}{\textbf{Notes}}                                                                        \\ \midrule
Train                                 & UXTD                                & 45                                    & 8302                                 & Child in-domain train data                                                                                \\
Train                                 & TaL                                 & 81                                    & 81193                                & Adult out-of-domain train data                                                                                  \\
Validation                            & UXTD                                & 5                                     & 534                                  &
Child in-domain validation data                                                                              \\
Test                                  & UXTD                                & 13                                    & 901                                  & Typically developing evaluation set                                                                       \\
Test                                  & UXSSD                               & 8                                     & 768                                  & Disordered speech evaluation set \\ \bottomrule
\end{tabular}%
}
\caption{Datasets used for automatic speech error detection. All sets contain samples from all places of articulation, except the disordered speech evaluation set, which contains the velar (384) and rhotic (384) samples rated by annotators in Section \ref{sec:expert_detection}.}
\label{tab:model-data-sets}
\end{table}


For \textbf{training data}, we use Ultrax Typically Developing (UXTD), which collected data from 58 child speakers.
UXTD includes a subset of utterances with manually-annotated word boundaries for 13 speakers.
We save data from those speakers for evaluation.
From the remaining 45 speakers, we randomly select 40 speakers for training and 5 speakers for validation.
The TaL corpus of adult speech is used as an additional source of training data.
We use the TaL80 dataset, containing data from 81 speakers.

For \textbf{evaluation data}, we use an evaluation set of disordered speech samples and an evaluation set of typically developing speech samples.
The disordered speech samples consist of the \emph{main set} rated by expert SLTs, described in Section \ref{sec:expert_detection}.
This is a set of 768 word instances from the UXSSD dataset.
The typically developing samples are extracted from the UXTD dataset, which includes 220 utterances with manually annotated word boundaries.
These utterances are produced by 13 speakers, disjoint from those in the training and validation sets.
After pre-processing, the typically developing evaluation set consists of 901 phone instances extracted from 866 words with a vocabulary of 153 words.

Because output classes are unbalanced, we control the number of samples per class for the training data.
For classes that are under-represented, we retrieve additional examples.
This is done by perturbing the anchor frame by up to 40ms for under-represented classes.
For classes that are over-represented, we randomly sample 1000 and 10000 examples for the UXTD and TaL sets, respectively.
After balancing and pre-processing, we have a total of 8302 for the UXTD training data.
The TaL corpus is larger than UXTD and has a total of 81193 samples.
Table \ref{tab:model-data-sets} shows the datasets used in this section, and their respective number of speakers and number of samples.

\begin{figure}
\includegraphics[width=\columnwidth]{figures_eps/diagram-sample-creation.eps}
\caption{\label{fig:diagram-sample-build} Sample build process. A sample is constructed around an anchor frame, typically the mid-point of a phone instance. Context windows are fixed at 100ms. Step is set such that 5 context frames are extracted for audio and 4 context frames for ultrasound.}
\end{figure}

The Kaldi speech recognition toolkit \citep{povey2011kaldi} is used to force-align all datasets at the phone level using the reference audio.
For the evaluation data, we constrain the phone alignment to the manually verified word boundaries.
The phone set is a Scottish accent variant of the Combilex lexicon \citep{richmond2009robust, richmond2010generating}.
We discard silence segments and vowels from the phone set and map the remaining phones onto one of nine classes corresponding to place of articulation:
\emph{alveolar, dental, labial, labiovelar, lateral, palatal, postalveolar, rhotic}, and \emph{velar}.
From the training data, we exclude phone instances that do not have parallel audio and ultrasound.
These instances occurred when audio started recording before the ultrasound.
For the UXTD data, there were 91 segments excluded due to early start.

We use Kaldi to extract Mel-frequency cepstral coefficients (MFCCs) for the audio signal.
MFCCs are commonly used for speech recognition, with good results reported for child speech recognition \citep{shivakumar2014improving}.
Waveforms are downsampled to 16KHz and features computed every 10ms over 25ms windows.
We keep 20 cepstral coefficients and append their respective first and second derivatives for a total of 60 features.
A high number of cepstral coefficients is helpful for child speech processing \citep{li2001automatic}.
Ultrasound frames are individually reshaped to $63\times103$ using bi-linear interpolation.
A single sample consists of an anchor frame and a set of context frames.
The anchor frame is fixed at the mid-point of each phone instance and the set of context frames are extracted over a fixed sized window of 100 ms to the left and right of the anchor frame.
Because of the different frame rates, the number of frames in the context window is different for the ultrasound and audio streams.
For the audio, each context window corresponds to 10 frames.
For ultrasound, the context window corresponds to 12 frames for Ultrasuite data and to 8 frames for TaL data.
Over each context window, we extract 5 MFCC frames and 4 ultrasound frames, with the step size set separately to account for the respective frame rates.
Figure \ref{fig:diagram-sample-build} illustrates the sample build process.


\subsection{Model architecture and training}

\begin{figure}
\includegraphics[width=\columnwidth]{figures_eps/diagram-model-architecture.eps}
\caption{\label{fig:diagram-model-architecture}
Convolutional neural network architecture for classifier using ultrasound and audio (MFCCs) inputs.}
\end{figure}


The adopted model architecture, illustrated in Figure \ref{fig:diagram-model-architecture}, largely follows that of earlier work \citep{ribeiro2019speaker, ribeiro2019ultrasound}.
The ultrasound stream is processed by two convolutional layers.
These layers use $5\times5$ kernels with 32 and 64 filters, respectively, and ReLU activation functions.
Each convolutional layer is followed by max-pooling with a $2\times2$ kernel.
The sequence of frames for the audio stream is flattened and processed by a fully-connected layer with rectified linear units.
When using the ultrasound and audio streams, the features are concatenated at this stage.
The batch normalized features are then processed by two fully-connected layers with ReLU activation functions and an output fully-connected layer followed by the softmax function.

Models are optimized via Stochastic Gradient Descent with minibatches of 128 samples and an L2 regularizer with weight 0.1.
We train models on the UXTD data or on the pooled TaL and UXTD data.
When using the UXTD training data, systems are optimized for 200 epochs with a learning rate of 0.1.
With the pooled dataset, systems are optimized for 50 epochs with an identical learning rate of 0.1.
After each epoch, the model is evaluated on the validation data and we keep the best model across all epochs.
We fine-tune systems trained on the pooled data on the UXTD data.
Models that are fine-tuned reduce the learning rate to 0.001 and are optimized for 100 epochs.

\subsection{Scoring}


The output of the classifier is a probability distribution over the nine places of articulation.
To score an input phone instance $x$, we consider an expected class $y$ and a competing class $c$.
The model score $s_m$ is then computed as
%
\begin{equation}
    s_m = \log(p(y|x)) - \log(p(c|x))
\label{eq:model_score}
\end{equation}
%
The expected class may be the canonical phone class, such as a velar or a rhotic.
The competing class is a possible substitution, such as an alveolar or a labiovelar approximant.
If no competing class is given, we can estimate it and compute the phone score with
%
\begin{equation}
c = \underset{q \in \mathcal{Q} - \{ y \}}{\arg\max}\, p(q|x)
\label{eq:model_max_score}
\end{equation}
%
where $\mathcal{Q}$ is the set of places of articulation considered by the model.
This method is related to the Goodness of Pronunciation score \citep{witt2000phone, hu2015improved}.
As with the combined expert score $s_c$ (Section \ref{subsec_expert_method}), 
the magnitude of the model score encodes certainty, whereas the sign encodes preference.
A positive score indicates preference for the expected class, while a negative score indicates preference for the competing class.
We simplify model and combined expert scores onto a binary correct/incorrect label $b(s)$ for error detection according to:
%
\begin{equation}
    b(s) =
    \begin{cases}
        0 & \text{if $s>k$} \\
        1 & \text{if $s\le k$} \\
    \end{cases}
\label{eq:model_binary}
\end{equation}
%
where $s$ is either $s_c$ or $s_m$ and $k$ is a configurable threshold.
Unless otherwise stated, results presented in this work use $k=0$, which treats uncertainty in the model score ($s_m=0$) as an error.
For the purposes of this analysis, uncertainty is not applicable to the combined expert score because we retain only cases that are correct or clear substitutions.

\begin{table*}[t]
\centering
\resizebox{0.89\textwidth}{!}{%
\begin{tabular}{@{}cccccccccc|cc@{}}
\textbf{Training Data} & \textbf{Alveolar} & \textbf{Dental} & \textbf{Labial} & \textbf{Labiovelar} & \textbf{Lateral} & \textbf{Palatal} & \textbf{Postalveolar} & \textbf{Rhotic} & \textbf{Velar} & \textbf{Global} &  \\ \midrule
                       & \multicolumn{10}{c}{\textit{Audio}}    \\ \midrule[.02em]

UXTD                   &  72.36\% &  46.77\%  &  64.00\%  &  52.5\%   &  84.85\%  &  64.52\%  &   68.66\% &   85.44\% &  75.62\%  &  70.81\%  \\
Joint                  &  70.56\%  &  35.14\% &  67.21\%  &  52.11\%  &  74.02\%  &  50.00\%  &   71.64\% &   88.64\% &  70.25\%  &  65.93\%   \\
Joint (+fine-tuning)   &  75.49\%  &  39.73\% &  66.67\%  &  59.42\%  &  80.56\%  &   66.67\% &   70.59\% &   85.71\%  &   77.65\% &  72.03\%   \\ \midrule[.02em]
                       & \multicolumn{10}{c}{\textit{Ultrasound}} \\ \midrule[.02em]
UXTD                   &  78.32\%  &  53.62\%   &  59.21\%   &  74.42\%   &  78.95\%   &  25.93\%  &   50.67\%  &  67.57\%  &  88.27\%   &  69.48\%   \\
Joint                  &  79.38\%  &  60.34\%   &  47.11\%   &  67.35\%   &  84.38\%   &  22.22\%  &   41.00\%  &  73.13\%  &  92.52\%   &  68.37\%   \\
Joint (+fine-tuning)   &  84.05\%  &  61.11\%   &  60.82\%   &  76.00\%   &  84.91\%   &  38.89\%  &   48.81\%  &  79.26\%  &  94.89\%   &  76.14\%   \\ \midrule[.02em]
                       & \multicolumn{10}{c}{\textit{Audio+Ultrasound}}   \\ \midrule[.02em]
UXTD                   &  80.10\%  &  81.08\%   &  83.87\%   &  82.61\%   &  90.18\%   &  65.52\%   &   58.33\%  &  74.83\%  &  89.58\%  &  80.47\%   \\
Joint                  &  83.25\%  &  76.74\%   &  83.33\%   &  63.08\%   &  73.19\%   &  66.67\%   &   76.19\%  &  91.59\%  &  93.45\%  &  81.80\%   \\
Joint (+fine-tuning)   &  87.94\%  &  76.47\%   &  87.78\%   &  72.31\%   &  91.82\%   &  58.82\%   &   75.38\%  &  94.34\%  &  95.58\%  &  \textbf{86.90\%}   \\ \midrule[.02em]
Number of samples      &  196  &  37   &  62   &  46   &  112   &  29   &   84  &  143   &  192  &   901   \\
\bottomrule
\end{tabular}%
}
\caption{Accuracy for the typically developing evaluation set across all places of articulation. Global accuracy is an average of all places of articulation, weighted by the number of samples for each class. The Joint training data denotes the pooled UXTD and TaL data. Highlighted results in bold indicate best overall performance.}
\label{tab:model-results-uxtd}
\end{table*}


\subsection{Results}

We evaluate model performance on the \textbf{typically developing} set.
Table \ref{tab:model-results-uxtd} shows accuracy results, which are computed across examples of all output classes.
Systems trained on the joint UXTD and TaL data underperform when compared with systems trained only on the UXTD data, 
even though there is more training data available.
However, fine-tuning the pre-trained joint model on the UXTD data leads to the best performance.

\begin{table}[t]
\centering
\resizebox{0.90\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Training data} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} \\ \hline
\multicolumn{5}{c}{\textit{Audio}}   \\ \midrule
UXTD                   &  0.384   &  0.524 &  0.443 &  64.1\%   \\
Joint                  &  0.417   &  0.585 &  0.487 &  66.5\%   \\ 
Joint (+fine-tuning)   &  0.393   &  0.537 &  0.454 &  64.9\%   \\ \midrule

\multicolumn{5}{c}{\textit{Ultrasound}}   \\ \midrule
UXTD                   &  0.670   &  0.842  &  0.746 &  84.4\%   \\
Joint                  &  0.702   &  0.805  &  0.750 &  85.4\%   \\ 
Joint (+fine-tuning)   &  0.677   &  0.768  &  0.720 &  83.7\%   \\ \midrule

\multicolumn{5}{c}{\textit{Audio+Ultrasound}}   \\ \midrule
UXTD                   &  0.732   &  0.866   &  \textbf{0.793} &  \textbf{87.7\%}   \\
Joint                  &  0.704   &  0.695   &  0.699 &  83.7\%   \\ 
Joint (+fine-tuning)   &  0.681   &  0.756   &  0.717 &  83.7\%   \\

\bottomrule
\end{tabular}%
}
\caption{Results for velar fronting error detection using the UXSSD velar samples rated by all annotators, except annotator 1.
Scores are computed using \enquote{velar} and \enquote{alveolar} as expected and competing classes respectively.}
\label{tab:model-results-velar}
\end{table}

Comparing systems using only one modality, accuracy results are better for ultrasound when using additional TaL data.
As expected, systems using both audio and ultrasound provide the best results.
Observing accuracy separately for each class, we observe that \emph{labial}, \emph{palatal}, \emph{postalveolar}, or \emph{rhotic} speech sounds have better results when using only audio compared to using only ultrasound.
The remaining speech sounds have better results with ultrasound tongue imaging.
Such differences are expected due to the individual characteristics of speech sounds.
For example, labial sounds do not rely on tongue movement, so they are not expected to benefit much from ultrasound tongue imaging alone.
On the other hand, velar and alveolar sounds have well-defined tongue shapes on the mid-saggital plane, so we would expect ultrasound data to be the primary contributor when identifying them.
We also observe that accuracy improves across all classes when using both modalities as input.
These results meet our expectations that ultrasound and audio complement each other well and that additional out-of-domain training data is beneficial.
Similar findings were reported on related tasks, such as speaker diarisation and word alignment of speech therapy sessions \citep{ribeiro2019ultrasound}.

Table \ref{tab:model-results-velar} shows results for \textbf{velar fronting error detection}.
These are computed over samples identified by annotators as correct velar productions or alveolar substitutions (see Figure \ref{fig:annotation-primary-secondary-matrix}).
We exclude samples rated by annotator 1, due to less than perfect agreement with other annotators.
Results are computed on $b(s)$ using the combined expert score $s_c$ and the model score $s_m$ with an expected \emph{velar} class and a competing \emph{alveolar} class.

We observe that ultrasound is more suited than audio to discriminate between velar and alveolar productions, although systems using both data streams have the best results.
There are no performance improvements to the systems using the joint dataset and fine-tuning when compared to the system using only typically developing child data.
This is an interesting observation, as results on the typically developing dataset indicate that using additional training data and fine-tuning is beneficial.
On the typically developing data, the individual accuracy for the velar and alveolar classes increases with more data and training.
Considering the system using both audio and ultrasound and comparing the UXTD and fine-tuned systems,
velar accuracy increases from 89.58\% to 95.58\% and alveolar accuracy increases from 80.10\% to 87.94\%.
The discrepancy observed between the typically developing set and velar fronting error detection could be attributed to challenges associated with speaker's data.
Speaker performance can vary substantially, particularly when using ultrasound data \citep{ribeiro2019speaker}.
This observation can be further supported by measuring agreement between model and expert binary scores for each of the eight speakers.
Using Cohen's $\kappa$ \citep{cohen1960coefficient}, models and expert scores have near perfect agreement for speakers 1 and 3 ($\kappa > 0.8$), substantial agreement for speakers 4 and 7 ($0.6 < \kappa \leq 0.8$), moderate agreement for speakers 5 and 6 ($0.4 < \kappa \leq 0.6$), and no or slight agreement ($\kappa \leq 0.2$) for speakers 2 and 8.

In Section \ref{subsec:annotator-agreement}, we reported intra- and inter-annotator agreement for the scoring of rhotic productions.
Unlike velars, expert scores for rhotics were shown to have very low inter-annotator agreement.
For this reason, results for \textbf{gliding error detection} should be interpreted carefully.
However, intra-annotator agreement was reliable and consistent for all annotators except annotator 8 (Figure \ref{fig:annotation-cohen-kappa}).
Therefore, we opt to analyse the results for gliding error detection separately for each speaker.

We note from Table \ref{tab:model-results-uxtd} that classification accuracy for rhotics and labiovelars is good across all classifiers on the typically developing evaluation set.
We observe that classification of rhotic instances benefits more from audio (85.71\%) than ultrasound (79.26\%).
The labiovelar class, on the other hand, achieves higher accuracy when using only ultrasound (76.0\%) than when using only audio (59.42\%).
Jointly using audio and ultrasound improves accuracy for rhotics (94.34\%) but not for labiovelars (72.31\%).
The average accuracy for rhotic and labiovelars when using ultrasound and audio is 78.72\% when training only on the UXTD data.
This accuracy slightly decreases when jointly training on the TaL corpus (77.34\%), but improves when fine-tuning on the UXTD data (83.33\%).
These results, however, relate to the typically developing evaluation set.
As observed with the velar case, they may not transfer in the same way to error detection.
Nevertheless, we select the fine-tuned system using both ultrasound and audio to analyse speaker-wise results for gliding error detection.

\begin{table}[t]
\centering
\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{@{}ccccccc@{}}
\toprule
\textbf{Speaker} & \textbf{N}  & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score}    & \textbf{Accuracy} & \textbf{Cohen's $\kappa$} \\ \midrule
1       & 41 & 0.778     & 0.467  & 0.583 & 75.6\%  & 0.426 \\
2       & 29 & 1.000     & 0.071  & 0.133 & 55.2\%  & 0.074 \\
3       & 36 & 0.900     & 0.474  & 0.621 & 69.4\%  & 0.404 \\
4       & 11 & 0.833     & 1.000  & 0.909 & 90.9\%  & 0.820 \\
5       & 42 & 0.964     & 0.675  & 0.794 & 66.7\%  & 0.045 \\
6       & 36 & 1.000     & 0.639  & 0.780 & 63.9\%  & 0.000 \\
7       & 45 & 0.500     & 1.000  & 0.667 & 97.8\%  & 0.656 \\
8       & 35 & 0.692     & 0.783  & 0.735 & 62.9\%  & 0.123 \\ \bottomrule
\end{tabular}%
}
\caption{Speaker-wise results for gliding error detection using the UXSSD rhotic samples identified as correct productions or gliding substitutions. Cohen's $\kappa$ is calculated on expert and model binary scores.
Results are generated by the fine-tuned system using both audio and ultrasound, with
scores computed using \enquote{rhotic} and \enquote{labiovelar} as expected and competing classes, respectively.
}
\label{tab:model-results-rhotic}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=0.80\columnwidth]{figures_eps/rhotic_confusion_matrices.eps}
\caption{\label{fig:rhotic_confusion_matrices}
Confusion matrices for gliding error detection produced by the fine-tuned system using audio and ultrasound for all 8 speakers in the UXSSD evaluation set. Correct rhotic productions are denoted by $0$ and gliding substitutions are denoted by $1$. }
\end{figure}


Table \ref{tab:model-results-rhotic} shows speaker-wise results for gliding error detection and Figure \ref{fig:rhotic_confusion_matrices} shows their respective confusion matrices.
We observe that the scores given by the expert annotators can vary per speaker.
For example, most samples produced by speakers 5 and 6 were marked as gliding cases by their respective annotators.
The limited number of correct productions influences the calculation of Cohen's $\kappa$, leading to poor agreement even though accuracy and F$_1$ are high.
On the other hand, most samples by speaker 7 were marked as correct instances.
The classifier appears to behave similarly with data from speakers 2 and 7, with most samples classified as correct rhotic instances.
This behaviour is in agreement with the expert for speaker 7, but not for speaker 2.
Most errors produced by the model are Type II errors (false negatives).
This might be due to the lower performance of the competing labiovelar class, as observed on the typically developing set.
Because the classifier is more confident when scoring rhotics, there is a limited number of Type I errors (false positives).
Due to the low inter-annotator agreement, it is not clear whether these differences are due to the scores provided by the annotators or due to challenges associated with speaker or recording variability.
