
\section{Introduction}

Speech sound disorders (SSDs) are a common communication impairment in childhood \citep{wren2016prevalence}.
If left untreated, SSDs can have a negative impact on the social and emotional development of children and can lead to poor educational outcomes.
For example, self-awareness of disordered speech contributes to low confidence in social situations when children engage with their peers or educators. In turn, this introduces communication barriers that lead to lower literacy levels  \citep{johnson2010twenty, lewis2011literacy, mccormack2011nationally}.

It is estimated that SSDs affect between 2.3\% and 24.6\% of children \citep{law2000prevalence, wren2016prevalence}.
Speech and language therapy is often recommended, with the majority of interventions heavily reliant on auditory feedback. That is, the speech and language therapist (SLT) relies on their perceptual skills to give the child verbal feedback during intervention; and in turn the child relies on their perceptual skills to modify their articulations. This may also be accompanied by auditory cues describing where and how to place the articulators to produce the target sound.
Interventions are often successful, especially for younger children \citep{mcleod2020waiting}.
However, some children do not respond well and the SSD becomes persistent.
There is growing evidence that including visual biofeedback (VBF) during therapy is beneficial for such children \citep{sugden2019systematic}.
VBF allows the visualization of the vocal tract during the speech production process, enabling children to view articulations in real-time.

With the widespread use of technology, there is increasing interest in automatically processing speech therapy tasks.
This type of automation can be helpful to teachers and parents, who may use screening tools to determine the presence or absence of SSDs \citep{sadeghian2015towards, ward2016automated}; and to clinicians, who can save time on these time-consuming tasks \citep{ribeiro2019ultrasound}.
Clinicians and researchers are trained to use instrumented methods such as spectrograms or ultrasound tongue imaging to assist in the assessment, diagnosis, or quantification of treatment efficacy.
These methods, however, can be laborious and impractical in the speech therapy clinic, as they still rely on manual annotation by the therapist or other trained professionals.
Typical tasks include the identification of utterances spoken by the child, the identification of boundaries of target words, or measurements to determine correctness of speech articulations.

In this paper, we are concerned with the \emph{automatic detection of speech articulation errors for speech therapy using ultrasound visual biofeedback}.
This is intended as a tool for clinicians to automatically process data from ultrasound visual biofeedback assessment and therapy sessions.
This work provides the following contributions:
1) the collection and analysis of pronunciation scores of velar fronting and gliding of /r/ given by experienced speech and language therapists;
2) a method for the automatic detection of speech articulation errors to be used by clinicians when processing data collected after therapy sessions;
3) an investigation of the impact of ultrasound tongue imaging for automatic error detection; and
4) an analysis of the impact of out-of-domain adult speech data for automatic error detection.
Our method is evaluated on typically developing child speech and, more specifically, on cases of velar fronting and gliding of /r/ in Scottish English child speakers.
These two errors are common in children with SSDs and amenable to intervention with ultrasound visual biofeedback \citep{sugden2019systematic}.

Section \ref{sec:background} provides background on speech disorders and recent evidence on the benefits of ultrasound visual biofeedback, as well as a review of recent literature on automatic speech articulation error detection.
The data used throughout this work is described in Section \ref{sec:data}.
Section \ref{sec:expert_detection} describes a perceptual experiment where SLTs were asked to rate the goodness of pronunciation of phone instances, using ultrasound and audio data.
Section \ref{sec:automatic_detection} describes a set of experiments for the automatic scoring of phone pronunciations using ultrasound tongue imaging.
Finally, Sections \ref{sec:discussion} and \ref{sec:conclusion} provide an overall discussion and conclusion for this work, respectively.
