\section{Discussion}
\label{sec:discussion}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.00\textwidth]{figures_eps/threshold_scatter_joint_colors_k.eps}
    \caption{
Velar fronting error detection with Audio and Ultrasound system trained on UXTD data.
The figure on the left shows model scores for velar and alveolar classes with diagonal lines indicating possible thresholds.
Each sample is coloured according to its true label, as given by expert annotators, using $k=0$.
On the right, precision, recall, and F$_1$ score as a function of the threshold $k$.}
    \label{fig:results_scatter_threshold_joint}
\end{figure*}

Considering the scores provided by the expert Speech and Language Therapists, we observed that results for velar samples are consistent and reliable, meeting our initial expectations.
The high inter-annotator agreement, excluding rater 1, for the combined score ($\alpha = 0.922$) suggests that the data provided by the experienced SLTs can be used for the evaluation of automatic methods.
The scores provided on the rhotic samples, however, were less consistent and did not meet our original expectations.
There are various reasons why this might have occurred.
Because children from the Ultrasuite repository were not treated for the production of rhotics, correct and incorrect samples were  unbalanced in the annotation list.
This may have affected the judgements made by the annotators, who expected to encounter some correct productions of /r/.
Including samples from typically developing children could have mitigated this issue and helped anchor the scale for correct productions.
Additionally, /r/ is a less common target for intervention in the UK \citep{wren2016prevalence}.
This might have affected the behaviour of the experienced SLTs, who, although able to discriminate between correct and incorrect productions, have less experience when evaluating this particular speech sound clinically.
Related to this observation, there is a wide range of socially acceptable productions of /r/ in the United Kingdom \citep{scobbie2006r, lawson2011social}, which motivated the choice of gliding for analysis in this work.
Unlike velars, which have a relatively consistent tongue shape between speakers, rhotics can be produced with a wide variety of tongue shapes \citep{boyce2015articulatory}, potentially making it more difficult to judge acceptability using ultrasound.
To account for the wide range of acceptable productions of /r/, annotators could browse through a set of samples drawn from typically developing children.

With respect to automatic scoring of speech articulation errors, our results indicate that expert behaviour can be simulated with an acceptable level of accuracy for velar fronting error detection.
The best performing system correctly detected 86.6\% of all errors identified by SLTs.
Out of all the segments identified as errors, 73.2\% of those are correct.
When evaluating systems, we assumed a threshold $k=0$ to compute the final score according to Equation \ref{eq:model_binary}.
The threshold $k$ is a parameter configured by the user, allowing some control over precision and recall.
Figure \ref{fig:results_scatter_threshold_joint} illustrates model scores and the impact of $k$ on precision and recall.
As expected, most of the uncertainty with respect to the true label occurs around $k=0$.
For the system in Figure \ref{fig:results_scatter_threshold_joint}, the $F_1$ score improves from $0.793$ to $0.817$ when $k=-0.4$.

Even though changing $k$ might result in slight improvements, the ranking of the systems across all conditions remains the same.
Ultrasound tongue imaging has a positive contribution to the overall accuracy of the models, when used by itself or together with audio features.
Considering out-of-domain data, results show that model performance can be improved when pre-trained on adult speech data.
Overall performance increases further when fine-tuning models on in-domain data.
This is observed when evaluating on typically developing speech, but not when detecting errors on disordered speech data.
This discrepancy could be caused by differences in the two datasets.
The typically developing set (UXTD) and disordered speech set (UXSSD) were collected separately, with different purposes and conditions  \citep{eshky2018ultrasuite}.
This might lead to domain mismatches between training (UXTD) and test (UXSSD) data.
Although the model achieves better accuracy on typically developing data, it may not generalise to the different disordered speech domain.
Differences between the two datasets include speaker characteristics, ultrasound probe placement, or acoustic variability due to room conditions or hardware used for data collection.

Results for gliding error detection are harder to interpret due to low inter-annotator agreement.
We do observe reasonable accuracy for some speakers in the evaluation set.
However, further work should investigate primarily the processes used by annotators to score the samples.

Future work for automatic error detection should explore error processes beyond substitutions, such as insertions or deletions.
These cases could be detected using methods similar to those used for mispronunciation detection in language learning (e.g. \cite{witt2000phone, witt2012automatic, hu2015improved}).
There are various child speech corpora which could complement this type of analyses, mostly through the addition of out-of-domain acoustic data.
A recent study has identified probe placement variability in the Ultrasuite data \citep{csapo2020quantification}.
Variable probe placement could be limiting the performance of an error detection classifier.
Future work could leverage the techniques proposed by \cite{csapo2020quantification} to account for such variability at training and test time.
This could also be used to provide real-time probe placement feedback to clinicians and minimise misalignment errors.
Alternatively, to reduce domain mismatch between training and test data, unsupervised domain adversarial training \citep{ganin2016domain} could be helpful, as well as the application of in-domain data augmentation techniques \cite{shorten2019survey}.
A different direction for future work could could leverage the the temporal dependency of therapy sessions.
In this longitudinal online learning scenario \citep{karanasou2015speaker}, the SLT provides feedback in early sessions (e.g. baseline assessment session) by verifying scores given by the model.
Those verified labels are then used to improve model scores on subsequent sessions (e.g. mid-therapy or post-therapy).
This scenario could help account for annotator preferences, as well as variability due to speaker characteristics or hardware configuration.
