\section{Expert speech error detection}
\label{sec:expert_detection}

In this section, our \textbf{goal} is the collection of pronunciation scores for speech segments produced by children with speech sound disorders.
This data is to be used in the evaluation of the automatic error detection systems described in Section \ref{sec:automatic_detection}.
We recruited Speech and Language Therapists with experience using ultrasound visual biofeedback 
and who routinely work with Scottish English-speaking children.
The collection of these scores aimed to simulate the process therapists undergo after collecting data from speech therapy sessions.
We are interested in the processes of velar fronting and gliding of /r/, therefore we focused on productions of /k, g/ (\textit{velars}) and pre-vocalic /r/ (\textit{rhotic}).
We expected SLTs to be able to identify correct and incorrect productions of velars and rhotics with good reliability.

\subsection{Data preparation}

We used data from the eight children available in Ultrasuite's UXSSD dataset.
Children in this dataset were treated for velar fronting, therefore we expected to observe an increasing number of correct velar productions throughout assessment sessions.
Children's response to intervention is reported in \citet{cleland2015using}.
Because intervention for these children did not focus on correcting rhotic productions, this led to an imbalanced set of correct and incorrect rhotic samples.
We pre-selected words containing the target velar and rhotic phones and occurring within prompts of type \enquote{A} (single words) in assessment sessions (baseline, mid-therapy, post-therapy, and maintenance).
We discarded words that contained more than one instance of a target phone (e.g.\ \enquote{cake}) or corrupted word instances (e.g.\ overlapping or unintelligible speech, other background noise, etc).
From the pre-selected word list, we randomly sampled 96 word instances per child.
Samples were balanced across assessment sessions and across velars and rhotics.
For each child, an assessment session contained 24 word instances (12 velars and 12 rhotics).
Where possible, samples were also balanced for the position of the target phone in the word (initial, medial, or final).
The final set of samples consisted of a total of 768 word instances with a vocabulary of 148 words, which
we denote as the \textbf{main set}.

We generated a set of additional samples from one of the speakers available in Ultrasuite's UPX dataset.
We selected speaker 04M, treated for velar fronting and reported to have good improvement after intervention \citep{cleland2019enabling}.
As the speakers in the main set, this speaker was also not treated for gliding.
The sampling process was repeated and a total of 24 word instances were selected (12 velars and 12 rhotics), balanced across assessment sessions.
We denote this set of samples the \textbf{control set}.


\begin{figure}
\includegraphics[width=\columnwidth]{figures_eps/annotation-sample-02M-Post-012A.eps}
\caption{\label{fig:annotation-video-sample} Video frame for the word \enquote{tiger} produced by speaker 02M in a post-therapy assessment session. The annotator is requested to score the target phone /g/ in a word medial position. The vertical red line in the spectrogram indicates the current position of the video.}
\end{figure}


\subsection{Method}
\label{subsec_expert_method}

To annotate the word instances, we recruited 8 annotators.
All annotators were SLTs with more than 4 years of experience and who routinely work with children speaking Scottish English.
Additionally, the annotators had at least 3 years of experience with ultrasound visual biofeedback, with two annotators having more than 10 years of experience.
Each annotator was assigned 96 words from the main set and the 24 words from the control set.
Words taken from the main set were selected such that they were produced by a single child.
Therefore, each SLT annotated data from two children (one main and one control).
For intra-annotator agreement, 20\% of the words (24 samples) were repeated in the annotation list.
Of these, 12 were taken from the control set and 12 from the main set.
Each SLT annotated a total of 144 word instances.

Results were collected via a web interface displaying a video of each word sample separately.
The video contained the spectrogram, ultrasound images of the tongue, and the audio for each word.
Figure \ref{fig:annotation-video-sample} illustrates one video frame extracted from one of the samples.
Annotators were allowed to play videos at normal, half, or quarter speed up to a maximum of 6 total playbacks. 

\begin{figure}
\includegraphics[width=\columnwidth]{figures_eps/annotation_joint_bar.eps}
\caption{\label{fig:annotation-bar-chart} Normalised distribution of primary scores for Velars and Rhotics, ordered chronologically (bottom up) by assessment session: baseline (BL), mid-therapy (Mid), post-therapy (Post), and maintenance (Maint).}
\end{figure}


Annotators were instructed to rate the target phone on a 5-point Likert scale,
where 1 indicates \emph{wrong pronunciation} and 5 indicates \emph{perfect pronunciation}.
The first question requested a score for the target phone (e.g. \enquote{Please rate the velar /k/ in the sample}).
This score is denoted the \textbf{primary score}.
If the annotator scored 3 or lower in the primary score, we requested a \textbf{secondary score}.
The secondary score asked the annotator to rate the target phone with respect to an expected substitution.
(e.g. \enquote{Please rate the target phone for alveolar substitution} or \enquote{Please rate the target phone for gliding substitution}).
An optional field allowed annotators to provide a short comment for each sample.

Given the primary score $s_p$ and the secondary score $s_s$, we determine a \textbf{combined score} $s_c$ defined as $s_c = \log(s_p)-\log(s_s)$.
Because we did not request a secondary score when perfect pronunciation was rated for the target phone, we assumed a value of 1 for $s_s$ when computing the combined score.
The score $s_c$ is positive if there is a preference for the primary class (e.g. velars or rhotics) and negative if there is a preference for the secondary class (e.g. alveolars or glide).
If the annotator gave the same primary and secondary scores, then this uncertainty is represented in $s_c$ with 0.
This preference can be further simplified to produce a \textbf{binary score} $s_b$.
For each sample, positive values of the combined scores are treated as correct productions of the primary class and negative or zero values as incorrect productions.


\subsection{Results}
\label{subsec:annotation_results}

Figure \ref{fig:annotation-bar-chart} shows the normalised distribution of the primary score for all annotated samples, ordered chronologically by session.
The number of incorrect velars across assessment sessions decreases over time, while the distribution for rhotics is more or less stable.
This is expected since intervention for these children focused on velar fronting and production of /r/ was not addressed.

Figure \ref{fig:annotation-primary-secondary-matrix} shows the frequency of primary and secondary scores for velars and rhotics in the main set. 
We remove duplicate samples used for intra-annotator agreement, keeping the score of the first sample to be rated.
For this work, we are primarily interested in the correct production of velars and rhotics and clear cases of substitutions (fronting and gliding).
Cases of correct pronunciations for the expected class are identified by a high primary score (4 or 5).
Cases of velar fronting or gliding are identified by a low primary score (1 or 2) and a high secondary score (4 or 5).
We observe from Figure \ref{fig:annotation-primary-secondary-matrix} that 342 out of the 384 velar samples (89.06\%) fall under one of these two cases.
Of these samples, 248 are correct velars and 94 are alveolar substitutions.
Rhotics include a smaller number of correct productions or gliding (275 out of 384 samples, 71.61\%).
Of these, 122 are marked as a correct production of /r/, while 153 denote cases of gliding.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures_eps/annotation_primary_secondary_joint_nodups.eps}
\caption{\label{fig:annotation-primary-secondary-matrix} Frequency of primary and secondary scores given by annotators for Velars and Rhotics in the main set with duplicate entries removed, where 1 indicates wrong pronunciation and 5 indicates perfect pronunciation.}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=0.95\textwidth]{figures_eps/annotation_binary_cohen.eps}
\caption{\label{fig:annotation-cohen-kappa} Pairwise Cohen's $\kappa$ for the binary score, excluding cases not rated as correct productions or clear substitutions. Diagonal values show $\kappa$ for intra-annotator agreement, while off-diagonal shows pairwise inter-annotator agreement. Each cell is colour-coded according to the levels of agreement proposed by \citet[pp 164-165]{landis1977measurement}.}
\end{figure*}

Some annotators used the optional comment field to elaborate on their score, particularly for incorrect cases that were not instances of velar fronting or gliding.
For velars, some of the cases were reported to be uvular, palatal, or postalveolar realisations, or omitted phones.
For rhotics, most of the non-typical scores indicated deletion of /r/ with some cases reporting a distortion towards a labiodental approximant.

\subsection{Annotator agreement}
\label{subsec:annotator-agreement}

We compute inter-annotator agreement using the control set of samples, rated by all annotators.
Duplicates are removed by choosing the first rating and discarding the second.
Intra-annotator agreement is computed on the 20\% duplicate samples, half from the main set and half from the control set.

To measure global agreement, we use \emph{Krippendorf's $\alpha$} \citep{krippendorff2004content}, which computes annotator 
agreement for multiple annotators and supports several levels of measurements.
We compute $\alpha$ using a difference function for ordinal data for the primary score, a function for interval data for the combined score, and a function for nominal data for the binary score \citep{krippendorff2011computing}.
According to \citet[pp 241-243]{krippendorff2004content}, $\alpha > 0.8$ indicates reliable data, while $0.667 \leqslant \alpha \leqslant 0.8$ indicates moderately reliable data. When $\alpha < 0.667$, the data should be considered unreliable.
Table \ref{tab:annotation-krippendorf} shows $\alpha$ values for primary, combined, and binary scores.
For the binary score, we exclude samples not rated as correct productions or clear substitutions.
Overall annotator agreement appears to be very good for velar samples and poor for rhotic samples.

\begin{table}
\centering
\resizebox{0.65\columnwidth}{!}{%
\begin{tabular}{@{}cccc@{}}
\toprule
            & \textbf{Primary} & \textbf{Combined} & \textbf{Binary} \\ \midrule
All         &  0.601   &  0.578 & 0.579 \\
Velars      &  0.883   &  0.868 & 0.946 \\ 
Rhotics     &  0.210   &  0.117 & 0.050 \\
\bottomrule
\end{tabular}%
}
\caption{Krippendorf's $\alpha$ for primary score $s_p$, combined score $s_c$, and binary score $s_b$ computed for all annotators across all, velar, and rhotic samples. Agreement for binary score excludes samples not rated as correct productions or clear substitutions.}
\label{tab:annotation-krippendorf}
\end{table}


We measure pairwise agreement using Cohen's $\kappa$ \citep{cohen1960coefficient}, which measures agreement between two raters on categorical data.
The $kappa$ statistic is a standardised metric where $\kappa \in [-1, 1]$, with $\kappa = 0$ denoting chance agreement and $\kappa=1$ denoting perfect agreement.
Traditionally, Cohen's $\kappa$ is discussed according to the five agreement levels suggested by \citet[pp 164-165]{landis1977measurement}.
These group values of $\kappa$ into: poor ($\kappa \leq 0.0$), slight ($0.0 < \kappa \leqslant 0.2$), fair ($0.2 < \kappa \leqslant 0.40$), moderate ($0.40 < \kappa \leqslant 0.60$), substantial ($0.60 < \kappa \leqslant 0.80$), and almost perfect ($0.80 < \kappa \leqslant 1.0$) agreement.
Figure \ref{fig:annotation-cohen-kappa} visualises pairwise annotator agreement for the binary score.
Off-diagonal values denote pairwise inter-annotator agreement on the control set, whereas diagonal values denote intra-annotator agreement on the duplicate samples.

According to Figure \ref{fig:annotation-cohen-kappa}, scores provided for the velar samples are very consistent and reliable, with perfect agreement across most raters.
This is observed for inter and intra-annotator scores.
Annotator 1 has substantial agreement with some of the other raters, but not perfect.
Excluding samples rated by annotator 1 leads to improved global inter-annotator agreement for velar samples on the combined score ($\alpha=0.922$).
Results for the rhotic samples, however, indicate a substantial disagreement between annotators.
We observe a perfect agreement for intra-annotator scores across all annotators except annotator 8.
Rhotic agreement between annotators 6, 7, and 8 is higher than other raters, with perfect or moderate agreement.
However, considering only those three raters, global agreement on the combined score is still lower than the moderate reliability threshold for Krippendorf's $\alpha$ ($\alpha=0.631$).
There are various reasons that could explain the agreement discrepancy between velar and rhotic samples.
We provide further insights into these results in Section \ref{sec:discussion}.
However, these results indicate that we should use rhotic scores carefully when evaluating automatic error detection systems in the next section.

