\section{Background}
\label{sec:background}

\subsection{Speech sound disorders}
SSDs occur when children exhibit difficulties in the production of speech sounds in their native language.
\emph{Organic speech sound disorders} denote difficulties that are associated with known causes.
These causes may be motor or neurological (e.g. childhood dysarthria associated with cerebral palsy), structural (e.g. cleft lip and palate), or sensory (e.g. hearing impairments).
\emph{Functional speech sound disorders} are related to difficulties producing intelligible or acceptable speech with unknown causes.
These disorders may be associated with motor production (articulation or motor speech disorders), or related to predictable or rule-based errors (phonological disorders) \citep{asha2020}.


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Substitution} & \textbf{Description}                         & \textbf{Example} \\ \midrule
Fronting              & Alveolars (/t,d/) replace velars (/k,g/)     & \emph{cookie} \textrightarrow{} \emph{tootie}   \\
Backing               & Velars (/k,g/) replace alveolars (/t,d/)     & \emph{dog} \textrightarrow{} \emph{gog}          \\
Gliding               & Glides (/w,j/ replace liquids (/r,l/)        & \emph{rabbit} \textrightarrow{} \emph{wabbit}    \\
Stopping              & Stops (/p, d/) replace fricatives (/f,s/)    & \emph{zoo}\textrightarrow{} \emph{doo}         \\
Labialisation         & Labials (/p,b/) replace non-labials          & \emph{tie} \textrightarrow{} \emph{pie}         \\ 
\bottomrule
\end{tabular}%
}
\caption{Selected examples of substitutions. A phone or group of phones is systematically replaced by another phone or group of phones.}
\label{tab:background-substitutions}
\end{table}


Both types of SSDs can result in a variety of speech patterns.
\textbf{Substitutions} are a common pattern where a phone or a group of phones is replaced by another phone or group of phones.
Table \ref{tab:background-substitutions} summarises common substitutions.
We highlight here the two processes that are relevant to this work.
\emph{Fronting} occurs when phones that are produced towards back of the mouth (velars such as /k, g/) are replaced with phones produced towards the front of the mouth (alveolars such as /t, d/).
This leads to word instances such as \emph{cookie} \textrightarrow{} \emph{tootie} or \emph{gate} \textrightarrow{} \emph{date}.
\emph{Gliding} occurs when liquids (e.g. /r, l/) are replaced with glides (e.g. /w/), originating cases such as \emph{rabbit} \textrightarrow{} \emph{wabbit} or \emph{leg} \textrightarrow{}\emph{weg}. 
Other examples of substitutions not listed in Table \ref{tab:background-substitutions} are affrication, vowelization, depalatalization, or alveolarization \citep{mcleod2017children}.

Beyond substitutions, we may observe \textbf{insertions} and \textbf{deletions} of phones in words (e.g. \emph{black} \textrightarrow{} \emph{buhlack}, \emph{spoon} \textrightarrow{} \emph{poon}).
Alternatively, \textbf{assimilation} denotes cases when specific sounds are transformed due to the influence of those around it.
For example, the process of nasal assimilation occurs when a non-nasal sound becomes nasal due to the presence of a nasal sound in the word (e.g. \emph{bunny}\textrightarrow{} \emph{nunny}).
Similarly, pre-vocalic voicing occurs when a voiceless phone becomes voiced when followed by a vowel (e.g. \emph{comb} \textrightarrow{} \emph{gomb}).
Additional phonological patterns may be influenced by \textbf{syllable structure}.
For example, cluster reduction, where a consonant cluster is reduced (\emph{plane}\textrightarrow{} \emph{pane}, \emph{clean} \textrightarrow{} \emph{keen}); the deletion of a consonant at the beginning (\emph{bunny} \textrightarrow{} \emph{unny}) or end of the syllable (\emph{bus} \textrightarrow{} \emph{bu}); or the deletion of weak or unstressed syllables in words (\emph{banana} \textrightarrow{} \emph{nana}).
Other \textbf{phonetic distortions} may also be observed (for example, a lateral /s/).

Many of these processes are typical stages in the speech development of children.
They are, however, expected to be eliminated as children reach a certain age.
For instance, the typical age of elimination of velar fronting is around three years \citep{mcleod2018children}. 
On the other hand, because /r/ in particular is late acquired, gliding of this phone is typically eliminated around the age of five \citep{mcleod2018children}. 
Children that persist with one or more of these processes beyond their expected age of elimination usually require speech and language therapy.


\subsection{Ultrasound visual biofeedback}

In the context of speech sound disorders, visual biofeedback involves the use of instrumented methods to provide visual information regarding the position, movement, or shape of intra-oral articulators during speech production \citep{sugden2019systematic}.
Common techniques to provide real-time visual biofeedback for speech therapy are electropalatography (EPG, \cite{lee2009electropalatography}), electromagnetic articulography (EMA, \cite{katz2010treating}), and ultrasound tongue imaging (UTI, \cite{sugden2019systematic}).
EPG uses an artificial palate to measure the contact points between the tongue and hard palate.
However, the manufacture of custom-made palates incurs additional costs and may limit the use of this technique to a few patients.
EMA requires the placement of sensor coils on the tongue and other articulators to measure their position over time, which can be both expensive and intrusive for children.
Ultrasound tongue imaging uses diagnostic ultrasound operating in B-mode to visualise the tongue surface during the speech production process.
A real-time B-mode ultrasound transducer is placed under the speaker's chin to generate a mid-saggital or coronal view of the tongue.
This form of ultrasound is  clinically safe, non-invasive, non-intrusive, portable, and relatively cheap \citep{stone2005guide}.
Figure \ref{fig:td_samples} provides examples of ultrasound images of the tongue for a typically developing speaker.

\begin{figure}
\includegraphics[width=\columnwidth]{figures_eps/td_samples_13F.eps}
\caption{Ultrasound tongue images collected from a typically developing speaker (female, aged 11). Each frame is the mid-point of the phone showing a midsaggital view of the oral cavity with the tip of the tongue facing right. Samples extracted from the Ultrasuite repository \citep{eshky2018ultrasuite}.}
\label{fig:td_samples}
\end{figure}


Increasing evidence shows that ultrasound VBF can be beneficial for patients, therapists, and annotators \citep{bernhardt2005ultrasound, cleland2019enabling, cleland2020impact}.
U-VBF is beneficial when used in intervention for a range of speech sound disorders, particularly if used in the initial stages of motor learning \citep{sugden2019systematic}.
Related work suggests that U-VBF can be used as an objective measure of progress in intervention \citep{cleland2020dorsal},
or to complement audio feedback and contribute to positive reinforcement \citep{roxburgh2015articulation}.
U-VBF can also assist annotators in the identification of covert errors and increase inter-annotator agreement scores \citep{cleland2020impact}.
Additionally, U-VBF can contribute to the automatic processing of speech therapy recordings.
Recent work used ultrasound data to develop tongue contour extractors \citep{fabre2015tongue}, animate a tongue model \citep{fabre2017automatic}, automatically synchronise therapy recordings \citep{eshky2019synchronising}, and for speaker diarisation and alignment of therapy sessions \citep{ribeiro2019ultrasound}.
There are, however, several challenges associated with the automatic processing of ultrasound tongue images \citep{stone2005guide, ribeiro2019speaker}.
Ultrasound output tends to be noisy, with unrelated high-contrast edges, speckle noise, or interruptions of the tongue surface.
Image quality may also be affected by speaker characteristics (e.g. age and physiology) or session variability (e.g. incorrect or variable probe placement).


\subsection{Automatic speech error detection}

Automatic speech error detection aims to identify inaccurate productions of words or phones.
These are often described in terms of insertions, deletions, and substitutions.
Most studies adopt techniques from computer assisted pronunciation training, primarily developed for adult speakers using language learning systems (e.g. \cite{witt2000phone, witt2012automatic, hu2015improved}).
This work is often considered part of the broader area of Computer Assisted Language Learning (CALL, \cite{beatty2013teaching}).
In children, speech error, or mispronunciation, detection can be used to assess reading levels (e.g. \cite{black2010automatic}, \cite{proencca2018mispronunciation}), or with disordered speech for Computer Assisted Speech Therapy (CAST, e.g. \cite{saz2009tools, parnandi2015development, ahmed2018speech}).
CALL systems are generally concerned with a global pronunciation score, which may or may not use the speaker's native language, while CAST systems aim to identify error types or underlying phonological processes.

Mispronunciation detection systems often use speech recognition techniques to compute pronunciation scores.
Training data for the acoustic models is L2 speech for language learning, or typically developing speech for therapy applications.
Other sources, if available, may consist of in-domain speech from the learners' native language or disordered speech.
In-domain data can be used to develop extended search lattices accepting non-canonical pronunciation alternatives \citep{harrison2009implementation, ward2016automated, dudy2018automatic}.
The trained acoustic models and extended transducers then decode unseen utterances for which the text is known.
To provide pronunciation scores, \emph{likelihood-based systems} use the log-likelihoods generated by the models.
The Goodness of Pronunciation score (GOP, \cite{witt2000phone}) is a widely-used method for such systems.
In its simplest form, the GOP score is the log-likelihood ratio between a target phone and a competing phone.
Recently, Gaussian mixture models have been replaced by deep neural networks, with GOP-like scores defined over neural network posteriors \citep{hu2015improved}.
Alternatively, \emph{classifier-based systems} use model outputs with supervised classifiers to determine error types or to provide more informed feedback.
However, these methods require supervised in-domain data (e.g. phone-level annotated disordered speech), which are costly to acquire.

When annotated in-domain data is not available, a possible approach is to learn distributions over canonical training examples, such as  typically developing speech.
Unseen samples can then be compared against those distributions.
\cite{shahin2018anomaly} use one-class support vector machines to model the distribution of features describing manner and place of articulation.
\cite{wang2019child} use Siamese recurrent networks, with positive and negative samples drawn from typically developing speech.
In this paper, we adopt a similar strategy.
Because we do not have annotated disordered speech for training, the acoustic model is trained only on typically developing speech.
Scoring is based on a GOP-like score defined over neural network posteriors.
Section \ref{sec:automatic_detection} provides additional details on our model implementation.

