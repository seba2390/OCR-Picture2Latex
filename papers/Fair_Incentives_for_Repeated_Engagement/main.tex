\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage{hyperref}
\hypersetup{colorlinks,breaklinks,
            urlcolor=[RGB]{177,32,41},
            citecolor=[RGB]{103,172,71},
            linkcolor=[RGB]{177,32,41}}
\usepackage[capitalize]{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{booktabs}

\input defs.tex

% \usepackage{color-edits}
\usepackage[suppress]{color-edits}
% \addauthor{kz}{magenta}
\addauthor{df}{blue}
\addauthor{ch}{magenta}
\usepackage{bbm}
\bibliographystyle{alpha}

\title{Fair Incentives for Repeated Engagement}
\author{Daniel Freund \and Chamsi Hssaine}

\begin{document}
\maketitle

\begin{abstract}
We study a decision-maker's problem of finding optimal monetary incentive schemes when faced with agents whose participation decisions {(stochastically) depend on the incentive they receive}. Our focus is on policies constrained to fulfill two fairness properties that {preclude outcomes wherein} different groups of agents experience different treatment on average. {We formulate the problem as a high-dimensional stochastic optimization problem, and study it through the use of a closely related deterministic variant.} We show that the optimal \emph{static} solution to this deterministic variant is asymptotically optimal for the {\it dynamic} problem under fairness constraints. Though solving for the optimal static solution gives rise to a non-convex optimization problem, we uncover a structural property that allows us to design a tractable, fast-converging heuristic policy.
Traditional schemes for stakeholder retention ignore fairness constraints; indeed, the goal in these is to use differentiation to incentivize {repeated engagement with the system}. Our work $(i)$ shows that even in the absence of explicit discrimination, {dynamic policies may unintentionally discriminate between agents of different types by varying the type composition of the system}, and $(ii)$ presents an asymptotically optimal policy to avoid such discriminatory outcomes.
\end{abstract}

\newpage
% \tableofcontents
% \newpage

\section{Introduction}\label{sec:intro}


Stakeholder retention is a fundamental challenge faced by many organizations.
For instance, it was said of former IBM executive Buck Rodgers that he ``behaved as if every IBM customer
were on the verge of leaving and that
[he'd] do anything to keep them from
bolting'' \cite{rodgers1986ibm}. Indeed, though it is conventional wisdom that continued {\it growth}  is necessary to ensure the success of a business, studies have routinely found that increasing customer {\it retention} rates by as little as 5\% could lead to an increase in profits of up to 95\% \cite{gallo2014value}. 

Of the many ways in which an organization can increase retention, one important lever it has at its disposal is that of {\it monetary incentives}. {Such incentives are used in many practical contexts:}
\begin{itemize}
    \item \textbf{Customer-side retention by for-profit corporations}: An oft-used practice by cable providers is the use of discounts for customers looking to cancel their subscription at the end of the cycle, with many companies going so far as to have entire retention departments to which such calls are routed \cite{cable_discounts,cable_discounts_2,cable_discounts_3}.
    \item \textbf{Supply-side retention for gig economy platforms}: A stable supply of flexible workers is crucial to meeting demand on these platforms. As a result, these platforms (ride-hailing platforms being one such example) have turned to bonus incentives to persuade workers to remain active. For instance, in light of the driver shortage due to the Covid-19 pandemic, in 2021 Lyft announced its intention to offer \$800 bonuses to drivers to return to the app, as well as covering the cost of rental cars \cite{lyft_bonuses}.
    \item \textbf{Non-profit organizations}: Lotteries have been found to be extremely effective in incentivizing charitable giving in empirical studies \cite{landry2006toward}. In practice, certain banks have set up programs, referred to in Germany as \emph{Gewinnsparen}, that enroll their clients for recurring donations for local causes in exchange for a chance to win a cash amount \cite{gewinnsparen}. Monetary awards have also been found to effectively combat against volunteer attrition, which plagues non-profit organizations across the board \cite{downs2014modeling,frey2017volunteer}.
\end{itemize}



In implementing these sorts of monetary incentives, one important concern that a decision-maker faces is that of {\it fairness}, especially in light of the abundance of examples in which algorithms deployed in the real world unintentionally discriminate against protected groups~\cite{kleinberg2018algorithmic}. %This is relevant 
Within the context of monetary incentives for retention, %since 
%the optimal policy for 
a decision-maker may want  %to  in wanting 
to maximize her bang-per-buck by paying individuals with different earnings sensitivities different amounts in order to to minimize the cost of retaining them. %However, it is well-known that
Since individuals' earnings sensitivities correlate with protected classes such as gender \cite{heckert2002gender}, such unconstrained optimal policies would discriminate between classes.



Our work is motivated by the goal of better understanding the optimal design of monetary incentives for repeated engagement, with a special focus on the {\it fairness} properties of such incentives. Indeed, despite the pervasive use of such incentives in practice, as well as extensive empirical work on their effectiveness in a variety of settings, to the best of our knowledge there have been few attempts to develop {\it theoretical} insights into their design, let alone approaches from the perspective of algorithmic fairness. We further detail our contributions below.



\subsection{Summary of contributions}

We consider a model wherein agents join a system in each (discrete-time) period, and receive a (possibly random) reward to remain in the system in the next period. At the end of the period, {\it unaware of the underlying distribution} from which rewards are drawn, agents probabilistically make {a} decision based on the reward received in the period to stay in the system, or leave once and for all. Specifically, we assume that agents are partitioned into {\it types} defined by $(i)$ their sensitivity to rewards, formalized via a {\it departure function} {that maps rewards received to the probability of departing}, and $(ii)$ the rate at which they join the system. This model, though simple, gives rise to a wide spectrum of models of agent behavior. In particular, most of our results hold for {\it any} set of departure functions, as long as these  functions are non-increasing in the reward paid out to an agent.

The decision-maker collects some revenue associated with the number of agents in the system in each period, and incurs the cost associated with incentivizing these agents to stay according to the chosen reward distribution (where the support of this distribution is assumed to be {an arbitrary finite} set). The goal of the decision-maker is to determine the optimal policy to maximize her long-run average profit. As is common in classical stochastic control problems, the infinite-horizon Markov Decision Process (MDP) associated with the decision-maker's optimization problem suffers from the {curse of dimensionality}, which motivates the task of finding {\it near-optimal} policies.

One natural approach a decision-maker may want to take to maximize her profit is to {\it learn, then discriminate}: given the history of rewards paid out to each agent, the decision-maker could try to estimate each agent's type, and ``target'' agents whom she believes would stay in the system for lower rewards. Though we show that such a policy can indeed perform arbitrarily well relative to a benchmark fluid-based static policy that draws rewards independently and identically from the same distribution in each period, not only is such {\it explicit} discrimination potentially problematic from a public relations standpoint, but it also runs counter to a fundamental principle of fairness, known as {\it  group fairness}, which at a high level requires that an algorithm treat (reward) individuals belonging to different groups (e.g., demographic groups) similarly.\footnote{Within the field of machine learning, the notion of group fairness is typically defined via statistical parity, wherein a classifier must assign subjects in protected and unprotected groups to a class with equal probability.}
Avoiding this sort of egregious discrimination, the decision-maker can then turn to {\it dynamic} policies that draw rewards i.i.d. from the same distribution {\it in each period}, all the while actively managing the number of agents in the system by varying the distribution {\it across periods}. We show that even such seemingly fair policies can insidiously discriminate between groups by having different agent types experience different reward distributions {on average}, across time. 

In light of this, we impose two fairness constraints on the decision-maker's optimization problem: $(i)$ agents {must} be paid from the same reward distribution {\it in each time period}, and $(ii)$ different agent types must experience the same reward distribution on average, over a long enough time horizon. In one of our main contributions, we identify an asymptotically optimal policy amongst the space of all {\it fair} reward schemes in a large-market regime parametrized by $\theta$. Perhaps surprisingly, this policy is precisely {the fluid-based heuristic}. We moreover show that this policy is {\it fast-converging}: not only does it converge to the value of the optimal static policy in the deterministic relaxation at a rate of $\mathcal{O}(\frac{1}{\sqrt{\theta}})$, as is known to be the case in many stochastic control problems, but under an additional mild technical condition on the decision-maker's revenue function, this convergence rate improves to $\mathcal{O}(\frac{1}{\theta})$. As we later discuss, this is due to the fact that the system is `self-regulating' in a certain sense, {which strengthens} the policy's convergence.

{While our policy satisfies these natural desiderata,} computing it requires us to solve a high-dimensional nonconvex optimization problem which is, \emph{a priori}, nontrivial to optimize. In our second main technical contribution, we show a surprising structural property of the problem that allows us to efficiently compute its optimal solution. In particular, {\it independent of the size of the reward set, the number of agent types, and their departure probabilities}, there exists a fluid-optimal reward distribution that places positive weight on {\it at most} two rewards. This allows for an optimal solution to be found via exhaustive search over pairs of rewards, and then solving a KKT condition consisting of a single equation in one variable for each pair. 
 


\paragraph{Structure of the paper.} In Section~\ref{ssec:related-work}, we survey related literature. We then present the model and formulate the decision-maker's optimization problem in Section~\ref{sec:preliminaries}. We show in Section \ref{sec:main-results} that the fluid-based heuristic is optimal amongst the space of all policies that satisfy a natural fairness property within the context of a deterministic relaxation of the system; while we also show the existence of dynamic policies that outperform the fluid heuristic, these policies are inherently discriminatory. Section~\ref{sec:asymptotic-opt} is devoted to analyzing the fluid-based heuristic and proving its fast convergence to the value of the fluid relaxation in a large-market regime. %We conclude with numerical experiments in Section~\ref{sec:special-cases}. 
The proofs of all results are deferred to the appendix.


\subsection{Related work}\label{ssec:related-work}

\paragraph{Workforce capacity planning.} Our work is related to the topic of {\it workforce capacity planning}, which has a long history in the operations management literature (see, e.g., \cite{de2015workforce} for an excellent survey). Within this line of work, we highlight papers that consider attrition and retention aspects of workforce planning. In contrast to our work, which focuses on the question of issuing monetary incentives throughout an agent's lifetime to retain them, these works are concerned with {hiring}, promotion, and {termination} decisions. For example, motivated by the naval aviation system, early work by \cite{grinold1976manpower} considered optimal accession policies when aviators have a known and deterministic lifetime. More recently, \cite{hu2016strategic} studied optimal hiring and admission and training policies for junior nurses, a profession in which attrition is pervasive, and as a result has been a central focus of much of the workforce planning literature. In their model, a {\it fixed} and exogenous fraction of the population leaves the system in each period, whereas in ours the decision-maker aims to set incentives in order to affect their retention. This work also resembles ours in that agents are {\it homogeneous} from a skills' perspective (though ours are heterogeneous with respect to their departures). 

A subset of the literature on workforce capacity planning is interested in worker {\it heterogeneity}; however, most of these works focus on heterogeneity with respect to skill set, not with respect to attrition. \cite{ahn2005staffing} consider a model in which workers turn over independently of the organization's policy and the state of the system, whereas \cite{gans2002managing} and \cite{arlotto2014optimal} allow workers' departure decisions to depend on the state of the system, but not the decision-maker's policy nor their own history in the system. Most recently, \cite{jaillet2021strategic} considered a more complex model of hiring, dismissing and promoting when workers' resignation decisions depend on their ``time-in-grade,'' or lifetime, in the system. 

\paragraph{Compensation in the gig economy.} We also mention the maturing stream of literature in operations management on matching of {{\it strategic}} supply and demand in on-demand service platforms. {A primary topic of focus has been {the} workers' capacity to {\it self-schedule}, i.e., the ability to decide {\it when} to work, given their beliefs about future earnings~\cite{dong2020managing,cachon2017role,gurvich2019operations,banerjee2015pricing,castillo2017surge,taylor2018demand,yang2018mean,bimpikis2019spatial,lobel2021employees,lian2021larger}.} Many of these works assume that workers know their expected earnings, and make the decision to work on the platform based on this expectation. In contrast, we assume that agents do not \emph{a priori} know the incentive scheme: they join the system according to an exogenous process, and make a {memoryless}, probabilistic decision to remain on or depart given their type and their incentive paid out, which is in line with models proposed for {\it customer} retention below. 

Within the context of wages on gig economy platforms, in an empirical study \cite{barzilay2016platform} find that, although women work for more hours on a platform, women's average hourly rates average about two-thirds of men's rates, with significant gaps persisting even after controlling for various features such as experience, occupational category, and educational attainment. Despite these very real concerns, to the best of our knowledge the design of {\it fair} monetary incentives for retention has yet to be broached in the operations literature.

\paragraph{Customer retention.} We highlight the most closely related works here, as this area has a rich history in the marketing liteature (see, e.g., \cite{ascarza2018pursuit} for a survey). To the best of our knowledge, none of these papers focus on designing {\it fair} customer retention policies. On the contrary, the goal in these latter works is precisely to use differentiation in order to incentivize customers to stay. For example, in a computational study \cite{lemmens2020managing} define a profit-based loss function to predict, for each customer, the financial impact of a retention intervention, ranking customers based on the marginal impact of the intervention on churn, and post-intervention profits. \cite{aflaki2014managing} develop theoretical insights around optimal retention policies, in a setting where customer ``types,'' or sensitivities to interventions, are known by the decision-maker, thus allowing for customer differentiation; in their model, the optimal decision across the population decouples into optimal decisions for each individual customer. 
A separate stream of work investigates how {\it capacity} decisions affect service access quality, and customer retention as a result \cite{afeche2017customer,furman2021customer}. In contrast to the interventions we consider, which occur {\it in each period}, in the settings these latter papers consider, the decision-maker is constrained to make a single decision {\it at the beginning} of the time horizon.

\paragraph{Empirical work on effectiveness of monetary incentives.} The effectiveness of monetary incentives for retention is also well-documented in the medical community. Empirical studies highlight their efficacy within the context of adherence to medication~\cite{volpp2008test,kimmel2012randomized}, weight loss and exercise~\cite{volpp2008financial,meeker2021combining}, postpartum compliance~\cite{stevens1994incentives}, and home-based health monitoring~\cite{sen2014financial}, for instance.

\paragraph{Algorithmic fairness.} Finally, our work adds to the large and growing body of work on the design of {\it fair} algorithms. We note that there is no universally agreed-upon notion of fairness (see, e.g., \cite{mehrabi2021survey} for a comprehensive overview of the different notions of fairness that have been considered in the literature). The notion of fairness that we choose to focus on is that of {\it group fairness} (as opposed to {\it individual fairness} \cite{dwork2012fairness}), which itself has no single definition. The one closest to ours, that arises in the machine learning literature within the context of group-fair classifiers, is {\it statistical parity} \cite{corbett2017algorithmic}. This requires that individuals in both  protected and non-protected groups have equal probabilities of being assigned to the positive predicted class; interpreting the set of rewards as possible predicted classes, our fairness definition can be understood as a multidimensional variant of statistical parity.

Some of the related literature on fair algorithms includes pricing problems with fairness constraints such as those studied by \cite{cohen2021dynamic,salem2021taming} and \cite{cohen2022price}. However, these do not model customer attrition. Further removed from our study are settings in fair (online) resource allocation which have received significant recent attention at the intersection of EconCS and operations research \cite{sinclair2021sequential,bateni2022fair,allouah2022robust, manshadi2021fair}. 
%
\section{Preliminaries}\label{sec:preliminaries}

We consider a discrete-time, infinite-horizon model of an organization{, which we henceforth generically refer to as a {\it system}}. {In each period {agents join the system, receive a reward, and decide whether to stay in the system for future periods or leave.} The {decision-maker} makes a profit, in each period, composed of the revenue from the {number of agents in the system}, net of {the cost of} the rewards {paid out to {agents}}. {For example, within the employment context, an agent corresponds to a worker performing a set of tasks in each period, with the reward corresponding to a bonus incentive; an agent can similarly correspond to a customer who enjoys service from her cable provider in a given period, with the reward corresponding to a discount.} We formalize each component of the model below}, {beginning with some technical notation.} {For clarity of exposition, we defer a lengthy discussion of modeling assumptions to the end of the section.}

\paragraph{Technical notation.} Throughout the paper, $\mathbb{R}^+ = \left\{x \in \mathbb{R} \, | \, x \geq 0\right\}$, $\mathbb{R}^{> 0} = \left\{x \in \mathbb{R} \, | \, x > 0\right\}$, and $\mathbb{N}^+ = \left\{i \in \mathbb{N} \, | \, i \geq 1\right\}$. For $K  \in \mathbb{N}^+$, we let $[K] = \{1,\ldots,K\}$. We use $\supp(f)$ to denote the support of a given probability mass function $f$, i.e., $\supp(f) = \left\{x : f(x) > 0\right\}$, and $|\supp(f)|$ denotes the cardinality of its support. Moreover, given set $\rewardset \subset \mathbb{R}$, we let {$\Delta^{|\rewardset|}=\left\{\xvec\in[0,1]^{|\rewardset|}: \sum_r x_r=1\right\}$} be the standard probability simplex over $\rewardset$.  Finally, $\unitvec_r$ is used to denote the unit vector in the direction of reward $r$.

\subsection{Basic setup}

\paragraph{Agents.}
We assume there are $K \in \mathbb{N}^+$ types of agents, defined by $(i)$ their reward sensitivity, and $(ii)$ the rate at which they join the system. Specifically, for $i \in [K]$, a type $i$ agent is associated with a {\it departure probability function} $\ell_i: \rewardset \mapsto (0,1]$, where $\rewardset \subset \mathbb{R}^+$ is a finite set of rewards from which the decision-maker chooses to compensate its agents. ({{Heterogeneity in departure probability functions has also been considered in \cite{ovchinnikov2014balancing} in a simple model with two types.}}) We assume that~$\ell_i$ is non-increasing and known to the decision-maker. Let $\rmin = \inf\{r \, | \, r \in \rewardset\}$ and $\rmax = \sup\{r \, | \, r \in \rewardset\}$. Moreover, the number of type $i$ arrivals in period $t$, $A_i(t)$, is drawn i.i.d. (across types and periods) from a $\text{Pois}(\lambda_i)$ distribution also known to the decision-maker; let~$\mathbf{A}(t) = \left(A_i(t), i = 1 \in [K]\right)$.


\paragraph{Periods.} Period $t \in \mathbb{N}$ is defined by the following sequence of events: $(i)$ for all $i \in [K]$, $A_i(t)$ type $i$ agents join the system; $(ii)$ each agent $\workerid$ is in the system (e.g., enjoying service, or working, for the duration of the period), and collects a (possibly random) reward $r_{\workerid}$; $(iii)$ having collected reward $r_{\workerid}$, {agent $\workerid$, unaware of the reward distribution, departs from the system with probability $\ell_{i_\workerid}(r_{\workerid})$, where $i_\workerid \in [K]$ denotes the type of agent $\workerid$;
otherwise, she remains in the system and moves onto the next period. We assume that the agent's decision to leave the system is made independently from all other agents, and independently of her prior history of rewards, i.e., agents make a decision based only on {their most recent experience}; moreover}, once an agent leaves, she does not return in a later period.\footnote{This latter assumption --- that the agent is ``lost for good'' --- is standard in the workforce planning and customer retention literature; see, e.g., \cite{aflaki2014managing}, \cite{arlotto2014optimal}.}


We use $D_i(t)$ to denote the number of type $i$ departures at the end of period $t$, with $\mathbf{D}(t) = \left(D_i(t), i \in [K]\right)$. Finally, let $\randomstate_i(t)$ be the number of type $i$ agents in the system in period $t$ (and thus requiring payment), with $\randomstatevector(t) = \left(\randomstate_i(t), i \in [K]\right)$, and $\randomstate(t) = \sum_{i \in [K]} \randomstate_i(t)$. $\randomstatevector(t)$ {is based on the number of agents who were in the system in the previous period and did not depart, as well as the number of new agents who joined {at the beginning of the} current period. Given the described dynamics, the {\it state} of the system is fully characterized by} $\randomstatevector(t)$, which evolves as
\begin{align*}
\randomstatevector(t+1) = \randomstatevector(t) - \mathbf{D}(t) + \mathbf{A}(t+1), \qquad \forall \, t \in \mathbb{N}.
\end{align*}



We next specify the decision-maker's objective and corresponding optimization problem.

\paragraph{Objective.}

Given $\randomstate(t)$, the total number of agents in period $t$, the decision-maker obtains revenue $\rev\left(\randomstate(t)\right)$, where $\rev:\mathbb{R}^+ \mapsto \mathbb{R}^+$. We assume that $\rev$ is time-invariant, and depends only on the {\it total number} of agents in the system, rather than the type composition of the agent pool in each period. $\rev$ is moreover assumed to be $L$-Lipschitz continuous and differentiable over $\mathbb{R}^{>0}$, as well as non-decreasing and concave.

Given $\{r_\workerid\}_{\workerid = 1}^{\randomstate(t)}$, the (possibly random) set of rewards paid out to agents in period $t$, the period-$t$ profit is given by:
$$\Pi(t) = \rev\left(\randomstate(t)\right) - \sum_{\workerid=1}^{\randomstate(t)} r_\workerid.$$


Let $\widehat{\Pi}(t)$ denote the expected profit %at the end of time
{in period} $t$, where the expectation is taken over the randomness in the reward realizations. 

We formulate the decision-maker's optimization problem as a discrete-time, infinite horizon Markov Decision Process (MDP), where the objective is to maximize the long-run average profit. Suppose the initial condition is $\randomstatevector(0) = \mathbf{c}_0$, $\mathbf{c}_0 \in \mathbb{N}^K$. For any policy $\varphi$, let $v({\varphi})$ denote the long-run average profit under $\varphi$ (assuming this limit exists). Formally:
\begin{align*}
v(\varphi) =& \lim_{T\to \infty} \frac1T \mathbb{E}\left[\,\sum_{t=1}^T\widehat{\Pi}(t)\, \bigg{\vert} \, \randomstatevector(0) = \mathbf{c}_0\,\right].
\end{align*}

In complete generality, a policy $\varphi$ maps the set of  agents in the system in a given period, and the history of rewards observed by each agent, to a distribution over rewards for each of these agents in that period. We restrict our attention to the set of policies for which the above limit exists. More importantly, we impose that any policy $\varphi$ satisfy the following two fairness criteria:
\begin{enumerate}
    \item If two agents of different types are in the system {in the same period}, the distribution from which their rewards are drawn is identical. Thus, $\varphi$ must map the set of  agents in the system in a given period, and the history of rewards observed by each agent, to a {\it single} distribution over rewards in every period. We denote the distribution in period $t$ by $\bx(t) = (x_r(t), r \in \rewardset)$. {Then, in a fixed period $t$, we have:
        \begin{align*}
        \widehat{\Pi}(t) = \rev\left(\randomstate(t)\right)-\randomstate(t)\left(\sum_r rx_r(t)\right). 
        % \widehat{\Pi}(t) =& \rev\left(\randomstate(t)\right)-\sum_{\agentid=1}^{\randomstate(t)} \sum_{r} r x_r(t) = \rev\left(\randomstate(t)\right)-\left(\sum_r rx_r(t)\right)\cdot\randomstate(t)
        \end{align*}} 
    \item The {\it average reward distribution} observed by agents of different types, conditional on their types, is ``approximately'' the same over time. We refer to this latter constraint as {\it group fairness}, and provide its mathematical formalization in \cref{sec:main-results}.
\end{enumerate}

%%%%%%%%%%%

\subsection{Large-market regime}

Given the size of the state space, the curse of dimensionality renders the goal of solving the MDP to optimality intractable. As a result, we turn to the more attainable goal of designing {\it asymptotically optimal} policies in a so-called {\it large-market limit}. %We formally define this large-market scaling below.


The regime we consider is defined by a sequence of systems parametrized by $\theta \in \mathbb{N}^+$, with $\scaledlambda_i = \theta\lambda_i$ for all $i \in [K]$, and departure probabilities $\left\{\ell_i(\cdot)\right\}_{i\in[K]}$ held fixed. 
We use~$\scaledrandomstate(t)$ to denote the number of agents in the system in period $t$ in the scaled system, and~$\scaledrandomstate$ the steady-state number of agents in the system. In order to keep the cost and revenue of any given policy on the same order, we define the 
{normalized profit in the scaled system} %\chdelete{scaled profit} 
at time $t$ to be
\begin{align*}
\scaledprofit(t) = \rev\left(\frac{\scaledrandomstate(t)}{\theta}\right)-\frac{\scaledrandomstate(t)}{\theta}\left(\sum_r r x_r(t)\right).
\end{align*}
For $\theta \in \mathbb{N}^+$, we let $\scaledv(\varphi)$ denote the long-run average {normalized} %\chdelete{scaled} 
profit of policy $\varphi$. %, and the random variable $\scaledrandomstate$ denote the steady-state number of agents in the system in the scaled system. 

{We briefly motivate this choice of scaling via a newsvendor-like revenue function. Suppose $\rev(N) = \min\{N,D\}$, for all $N \in \mathbb{N}$ and some $D > 0$. Then, scaling both demand and supply by $\theta$, %for fixed $N^\theta$, 
a more standard scaling in which we simply divide $\rev$ by $\theta$ would give us:
\begin{align*}
    \frac1\theta\rev(N^\theta) = \frac1\theta \min\left\{N^\theta, \theta D\right\} = \min\left\{\frac{N^\theta}{\theta}, D\right\} = \rev\left(\frac{N^\theta}{\theta}\right).
\end{align*}
}
With strictly concave and increasing $R$, {a more standard scaling in which the revenue function is {not normalized} %\chdelete{unscaled} 
(i.e., $\rev{(N^\theta)}$ is not replaced with $\rev(N^\theta/\theta)$), yields a vacuous asymptotically optimal solution $\bx=\unitvec_{\rmin}$, and the objective going to $-\infty$. Scaling the system in this manner overcomes such vacuities.} Finally, we remark that a constant additive loss in $\widehat{\Pi}^\theta$ translates to an $\Omega(\theta)$ loss in the non-normalized profit $\theta\rev(\scaledrandomstate/\theta)-\scaledrandomstate(\sum_r rx_r)$.







\paragraph{Deterministic relaxation of the stochastic system.} 

In order to analyze {\it first-order} {differences between policies}, we consider a deterministic relaxation of the stochastic system, which we formally define below.


Let $(\mathbf{x}(t))_{t \in \mathbb{N}^+}$ denote a sequence of reward distributions. In each period~$t$, for all~$i \in [K]$, we observe~$\lambda_i$ arrivals and~$\fluidn_i(t)\sum_r \ell_i(r)x_r(t)$ departures of type~$i$ agents, where $\fluidn_i(t)$ satisfies the following inductive relation:
\begin{align}\label{eq:inductive-deterministic}
    \fluidn_i(t+1) = \fluidn_i(t) + \lambda_i - \fluidn_i(t)\sum_r\ell_i(r)x_r(t), \quad \forall\, t \in \mathbb{N}^+.
\end{align}
Let $\widetilde{N}(t) = \sum_i \widetilde{N}_i(t)$, for $t \in \mathbb{N}^+$. We use $\widetilde{\varphi}$ to denote the policy which pays out rewards from $(\mathbf{x}(t))_{t \in \mathbb{N}^+}$ in this deterministic system, and let $\detPi(\widetilde{\varphi})$ denote the long-run average profit induced by $\widetilde{\varphi}$ in the deterministic system, i.e.,
\begin{align}
    \detPi(\widetilde{\varphi}) = \lim_{T\to\infty}\frac1T \sum_{t=1}^T \left[R\left(\fluidn(t)\right) - \left(\sum_r r x_r(t)\right)\fluidn(t)\right].
\end{align}
Given policy $\varphi$, when necessary we use $\widetilde{N}^\varphi(t)$ to emphasize the dependence on $\varphi$. (As in the stochastic system, we assume $\widetilde{\varphi}$ is such that this limit exists.)


In order to establish the connection between the two systems, consider the following coupling: for any $T, \theta \in \mathbb{N}^+$, fix payout distributions $\mathbf{x}(1),\ldots,\mathbf{x}(T)$. Moreover, let $\Delta(T)$ be the absolute difference between the expected profit in the stochastic system and the profit in the deterministic system over $T$ periods, i.e.,
$$\Delta(T) := \left\lvert\left(\sum_{t=1}^T\EE\left[R\left(\frac{N^\theta(t)}{\theta}\right)\right] -\sum_r rx_r(t)\EE\left[\frac{N^\theta(t)}{\theta}\right]\right)-\left(\sum_{t=1}^T R(\widetilde{N}(t))-\sum_r rx_r(t)\widetilde{N}(t)\right)\right\rvert.$$ The following proposition states that the long-run average profit in the deterministic system converges to the long-run average expected profit in the large-market regime.


\begin{proposition}\label{prop:det-to-stoch}
Suppose $\widetilde{N}_i(0) = \mathbb{E}\left[\frac{N_i^\theta(0)}{\theta}\right]$ for all $i \in [K]$. Then, there exists a constant $C_0 > 0$ (independent of $T$, $\theta$) such that $\lim_{T\to\infty}\frac{\Delta(T)}{T} \leq C_0/\sqrt{\theta}$.
\end{proposition}



Before proceeding to a discussion of our modeling assumptions, we illustrate the non-triviality of the structure of ``good'' policies, even in the deterministic system. 
 In the example described in \cref{fig:normal-variance}, we consider the policy that draws rewards from a normal distribution with mean $\mu$ and variance~$\sigma^2$, and study the effect of varying both of these parameters on the decision-maker's profit, for each of these settings. 
For a type 1 agent with a convex departure probability function, holding average reward $\mu$ fixed, profit is highly sensitive to the standard deviation of the reward distribution. In particular, around $\mu = 37$, there is a steep drop in profit between $\sigma = 0$ and $\sigma = 20$. On the other hand, when $\ell_2(r)$ is linear, for fixed $\mu$, $\sigma$ has no visible impact on profit. Finally, for a type 3 agent with a concave departure probability function, for fixed average reward, profit is {\it increasing} in the standard deviation of the reward distribution. 

% 

\begin{figure}
     \centering
     \subfloat[\centering $\ell_1(r) = e^{\alpha_1(-r+15)}, \alpha_1 = 7/100$
     %$\ell(r)$ vs. $r$
     \label{fig:normal-variance-type1-types}]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_worker_type1.png}}
     \subfloat[Type 1 agent
     %$\ell(r)$ vs. $r$
     \label{fig:normal-variance-type1}]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_type1_final.png}} \\
     \subfloat[\centering $\ell_2(r) = -\alpha_2r + \beta_2$, $\alpha_2 = 1/45$, $\beta_2 = 4/3$]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_worker_type2.png}}
     \subfloat[Type 2 agent]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_type2_final.png}}\\
     \subfloat[\centering $\ell_3(r) = -\alpha_3r^2 + \beta_3r + \gamma_3$, $\alpha_3 = \frac{1}{2025}, \beta_3 = \frac{2}{135}, \gamma_3=\frac89$.]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_worker_type3.png}}
     \subfloat[Type 3 agent]{\includegraphics[width = 0.5\textwidth]{Figures/normal_variance_type3_final.png}}\\
    \caption{\centering Impact of reward variability on long-run average profit, $K = 1, \lambda = 10, \rev(N) = 100\min\{N,50\}$, with $r \sim \mathcal{N}(\mu,\sigma^2)$.}
    \label{fig:normal-variance}
\end{figure}

Figure \ref{fig:normal-variance} thus demonstrates that, depending on the specific structure of an agent's departure probability function, the decision-maker's profit is driven by the exact reward distribution, not only by its first moment (which only determines the expected cost in a single period). This thereby suggests that solving for (near)-optimal policies requires solving a non-trivial, high-dimensional optimization problem (where the dimension depends on the support of the underlying reward set).


We conclude the section with a discussion of our modeling assumptions.

\subsection{Discussion of modeling assumptions}\label{ssec:modeling-asp} 

\paragraph{{Memoryless agents.}} A key feature of our model is that agents decide to stay or leave based only on the most recent reward. {Such an assumption} is motivated by the fact that {agents typically lack insight into the algorithms that generate the decisions they receive (e.g., why an algorithm paid out a reward in a given period, in our setting). This fact is well documented in noncontractual settings such as gig economy platforms~\cite{no_transparency}.} 
{Moreover, the memoryless assumption follows a long tradition of models that consider agent attrition (also referred to as disengagement, in some settings) in the operations literature. For instance, within the context of recommender systems, \cite{ben2022modeling} and \cite{bastani2021learning} assume that the probability a user disengages with the recommendations depends only on the quality of the most recent recommendation. \cite{afeche2017customer} similarly note that this sort of  ``recency effect'' is typically assumed in the customer retention setting, in models that link demand to past {\it service levels} \cite{hall2000customer, ho2006incorporating, liu2007dynamic}. \cite{lemmens2020managing} also consider memoryless customers in their churn prediction problem. {We note that, in practice, there exist online forums and additional sources of information that agents can turn to in order to improve their priors on incentive schemes used historically by a decision-maker. Papers belonging to this line of work do not capture this reality; our model follows in this tradition. Rather} than our model being an improvement on existing models of agent memory, we leverage these models to gain insights into {\it fairness considerations} for these well-studied systems, which is our main contribution.}

We also highlight that the abstraction of a departure probability function is general enough to allow for arbitrary rules of thumb, as long as they depend only on the reward obtained by the agent in the most recent period, and are non-increasing in this reward. In particular, one special case of this general model of departure probabilities is a setting in which agents are endowed with a {\it reservation value} and leave the system with a probability that's proportional to the difference between the reward received and their reservation value. {The model moreover implicitly allows for a {\it base} departure probability (which could in theory arise from a utility maximization problem solved by the agent, {\it independent} of the reward paid out at the end of the period), to then be modified by the reward received. In addition, if the belief is that the reward has {\it no} impact on the base probability, this too is captured by our model, by setting $\ell_i(r)$ to be a constant, given type $i$.} 


We conclude the discussion of the memoryless assumption by noting that the abstraction of a period {is very general.} For example, in a contractual employment setting, a period could be considered to be the length of the contract. In a noncontractual setting (e.g., gig economy platform), a period could constitute however long agents are believed to consider their earnings before making the decision to leave the system. In the setting where agents are customers, a period would be the duration of the subscription contract.


\paragraph{Time-invariance of the revenue function.} Another assumption upon which our model relies is the fact that the revenue function depends only on the number of agents in the system in a given period. {In an employment setting, for instance,} the time-invariant assumption models a {\it mature} market, with newsvendor-like dynamics. The work performed in the system can be viewed as ``low-skill,'' in the sense that workers arriving to the system are homogeneous, and the decision-maker does not benefit from workers gaining skill specificity with time. In the customer retention setting, on the other hand, stationarity of the revenue function is a fairly reasonable assumption within the context of profit generated from the number of active subscribers (ignoring heterogeneity in subscription plans). We also note that, though the firm's revenue could depend on exogenous variables such as demand, we assume that the environment is stationary, and as such the revenue function we consider can be interpreted as an expected revenue, with the expectation taken over the randomness in all other variables of interest.

\paragraph{Exogenous arrival rate.} Our model assumes that, in each period, agents join the system independently of the incentive scheme (e.g., the bonus incentive) determined by the decision-maker. For instance, this models a new subscriber signing up for a period's worth of service based on the base price of the service, but not on the expectation of a discount in future periods. Similarly, salaried workers may join a company for benefits specified in the contract, rather than unpredictable future bonus incentives. In an extreme setting, where the exogenous arrival rate is very large, it may be true that the decision-maker's optimal policy is to never give out any bonus rewards. Though our model subsumes such an uninteresting case, it also subsumes far more interesting settings, wherein the exogenous arrival rate is small (or, equivalently, the base departure probability is large), and thus bonus incentives are crucial to sustain a supply of agents.

\paragraph{Unconditional versus conditional incentives.}  As in \cite{lemmens2020managing}, we focus on {\it unconditional} incentives, wherein the decision-maker pays out the reward {\it independently} of the agent's decision to stay or leave, as opposed to {\it conditional} incentives. Empirical evidence of the effectiveness of such unconditional incentives has been found in the behavioral sciences, e.g., within the context of physician and patient surveys~\cite{abdulaziz2015national,young2015unconditional,rosoff2005response}, in addition to clinical study enrollment~\cite{young2020unconditional,kumar2022randomized}. {We believe that the analysis of conditional incentives can be similarly approached.}

\paragraph{Fairness constraints.} 
The two fairness constraints respectively enforce distributional envy-freeness {\it within} and {\it across} periods. On the one hand, if one views the reward distribution abstraction as agents playing a lottery for retention, distributional envy-freeness within periods ensures that agents know they are playing the same lottery. Group fairness, on the other hand, addresses the possibility that, {\it in hindsight}, different types of agents may realize that they were rewarded differently on average. This would be particularly problematic in settings, for instance, where departure probabilities are correlated with protected classes such as gender and race~\cite{heckert2002gender}. {For concreteness, suppose all individuals were given the same retention bonuses in a given year. Suppose moreover that, in years where an organization estimated individuals were more likely to remain given smaller amounts, it paid out smaller bonuses to all. If in hindsight it turned out that these years were years in which there were more women in the organization, this would clearly be unfair, as on average women would be paid lower bonuses than those of other genders {\it across time}. Moreover, distributional envy-freeness as a principle is reasonable given the fact that, in the systems we consider, agents are symmetric from a revenue perspective (e.g., there is no ``specialization'' across types). In \cref{sec:pof}, we investigate the effect of relaxing these two constraints on system profit.
}

\section{Optimal policies via the deterministic relaxation}\label{sec:main-results}


{In this section we design and analyze a heuristic policy within the context of the deterministic system.  We begin by formalizing the group fairness constraint, first introduced in \cref{sec:preliminaries}.



}



\begin{definition}[Group-fair policy]\label{def:fair_policy}
A policy $\widetilde{\varphi}$ defined by sequence of reward distributions $(\bx(t))_{t \in \mathbb{N}^+}$ is \emph{group-fair} if, for all $\delta > 0$, there exists $\tau_0 \in \mathbb{N}^+$ such that for all $\tau > \tau_0$:
\begin{align}\label{eq:fair-policy}
    \bigg{\lVert}\frac{1}{\sum_{t=t'}^{t'+\tau}\widetilde{N}^\varphi_i(t)}\sum_{t=t'}^{t'+\tau} \widetilde{N}^\varphi_i(t)\bx(t) - \frac{1}{\sum_{t=t'}^{t'+\tau}\widetilde{N}^\varphi_j(t)}\sum_{t=t'}^{t'+\tau} \widetilde{N}^\varphi_j(t)\bx(t) \bigg{\rVert}_1 < \delta  \qquad \, \forall \, t' \in \mathbb{N}^+, \, \forall \, i,j \in [K]. 
\end{align}

\end{definition}
Informally, a group-fair policy guarantees that, over any long enough time interval, the expected reward distributions respectively observed by different agent types do not differ too greatly.

We first show that, despite the unwieldiness of the group fairness constraint, there exists an exceedingly simple group-fair policy that is optimal in the context of the deterministic system: a policy that pays out the {\it same} distribution in each period.


\subsection{Optimality of the fluid-based heuristic}

 Consider the following optimization problem, termed \fluidopt, which computes the optimal {\it static} policy in the deterministic system described above:
\begin{align}\label{eq:fluid-opt}
    \optfluidprofit := \max_{\xvec \in \simplex^{|\rewardset|}, {\mathbf{\fluidn} \in \mathbb{N}^K}} &\rev\left(\sum_i \fluidn_i\right) - \left(\sum_r r x_r\right)\left(\sum_i \fluidn_i\right)\tag{\fluidopt}\\
    \text{s.t.} \qquad &\lambda_i=\fluidn_i\sum_r\ell_i(r)x_r \quad \forall \, i \in [K]. \notag 
\end{align}
Here, the stability constraint ensures that, for each type, the number of arrivals and departures are equal, and follows from plugging $x_r(t) = x_r$, for all $r \in \rewardset, t \in \mathbb{N}^+$ into \eqref{eq:inductive-deterministic}.  Note moreover that omitting the group fairness constraint \eqref{eq:fair-policy} is without loss of generality, as static policies are necessarily group-fair. We have the following theorem.


\begin{theorem}\label{thm:static-policies-are-opt-for-one-type}
Suppose $\ell_i(\rmin) = 1$ for all $i\in[K]$, and let ${\Phi}$ denote the space of all fair policies. Then, $\sup_{\varphi\in\Phi}\widetilde{\Pi}(\varphi) = \widetilde{\Pi}^*$. That is, there exists an optimal fair policy that is static.
\end{theorem} 

In the remainder of the paper, we refer to this optimal static policy as the \emph{fluid heuristic}. The proof of \cref{thm:static-policies-are-opt-for-one-type} is constructive, and makes use of the notion of {\it cyclic} policies, which we formally define below.

\begin{definition}[Cyclic policy]\label{def:cyclic-pol}
Policy $\varphi$ is \emph{cyclic} if there exists $\tau \in \mathbb{N}^+$ such that $\bx(t+\tau) = \bx(t)$ for all $t \in \mathbb{N}^+$. The smallest $\tau$ for which this holds is the cycle length of policy $\varphi$, which we term $\tau$-\emph{cyclic}.
\end{definition}

When making the distinction between a $\tau$-cyclic policy $\varphi^\tau$ and another policy $\varphi$, we sometimes use $\widetilde{N}_i^{\tau}(t)$, for $i \in [K], t \in \mathbb{N}^+$.


To prove \cref{thm:static-policies-are-opt-for-one-type}, we show that given any fair dynamic policy, for any $\delta > 0$ there exists an {\it approximately} fair {\it cyclic} policy with a long enough cycle length whose profit is within $\delta$ of the dynamic policy. We then show that the performance of any approximately fair cyclic policy can be approximated to arbitrary closeness by an appropriately constructed static policy that ``mixes'' between the cyclic policy's distribution across periods. Putting these facts together, we obtain optimality of the fluid heuristic amongst the space of arbitrary fair policies. We defer a complete proof to Appendix \ref{apx:fairness-thm}. We briefly note that the assumption that $\ell_i(\rmin) = 1$ for all $i$ is for technical simplicity. The proof for the case that $\ell_i(\rmin) < 1$ for some $i \in [K]$ becomes much more cumbersome as reward slashing (see proof in appendix) is then required over not one, but $\Theta\left(\log(\frac{\lambda_i}{\ell_i(\rmax)})\right)$ periods.

As an immediate corollary to \cref{thm:static-policies-are-opt-for-one-type}, we have that, when $K=1$, static policies are optimal amongst the space of {\it all} policies.\footnote{This follows from considering an arbitrary partition of agents when $K=1$, defining them to be of different types, and applying the above theorem.}
\begin{corollary}
Suppose $K = 1$ and that $\ell(\rmin) = 1$. Then, static policies are optimal amongst the space of \emph{all} policies. 
\end{corollary}




\subsection{Impact of discrimination by type}\label{sec:pof}

We next investigate the impact of the two fairness constraints imposed. In particular, when expanding the space of policies beyond fair ones, one approach a decision-maker could take would be in the flavor of \emph{learn, then discriminate}: by deploying machine learning algorithms to learn agents' types, a decision-maker can leverage this additional information to then pay agents of different types different amounts. We say that such policies {\it explicitly}  discriminate. 

{


\cref{prop:explicit-wage-disc} formalizes the intuition described above, that policies that learn agent types and target ``cheaper'' agents can greatly outperform optimal fair policies. 


\begin{proposition}\label{prop:explicit-wage-disc}
Consider the setting with $K = 2$, $\rewardset = \{0,v_1,v_2\}$, $v_1 < v_2$, and the following departure probabilities:
\begin{align*}
    \ell_1(r') = \begin{cases}
    1 \quad &\mbox{if }r' = 0 \\
    0 \quad &\mbox{if }r' \in \{v_1,v_2\} 
    \end{cases} \qquad \text{and} \qquad 
    \ell_2(r') = \begin{cases}
    1 \quad &\mbox{if }r' \in \{0,v_1\}\\
    0 \quad &\mbox{if }r' = v_2.
    \end{cases}
\end{align*}
Moreover, let $R(\widetilde{N}) = \alpha \min\{\widetilde{N}, D\}$, $\alpha > 2v_2$, and {$\lambda_1 = D/4, \lambda_2 = D/2$}.
Then, there exists a policy~$\varphi^b$ that {\it explicitly}  discriminates such that $\widetilde{\Pi}(\varphi^b) - \widetilde{\Pi}(\varphi^s) = \Omega(D)$, where $\varphi^s$ is the optimal static policy. 
\end{proposition}

The policy $\varphi^b$ that we construct is {\it belief-based}, i.e., it targets cheaper type 1 agents by first learning their type, and then keeping them in the system, all the while keeping type 2 agents out of the system. Specifically, $\varphi^b$ learns the type of agents early on by paying all arriving agents~$v_1$. If an agent stays in the system after having been paid~$v_1$, then this agent is necessarily a type 1 agent, who is ``cheaper'' to keep in the system than a type 2 agent. Once enough type 1 agents are in the system, the policy no longer needs to keep arriving agents in the system, and can pay them nothing for the rest of time.

The above policy clearly violates our first fairness desideratum of drawing rewards from the same distribution for all agents {\it within} a given period. Our next result shows that there exist policies that satisfy this first fairness constraint, but fail to be group-fair; moreover, avoiding group-fairness allows this policy to outperform any fair policy by an unbounded amount. 
We refer to this more subtle version of discrimination, which pays agents in the same period according to the same distribution, as \emph{implicit  discrimination}.
}
{
\cref{ex:steady-state-cyclic} shows that such discrimination also successfully outperforms static policies.
}



\begin{proposition}\label{ex:steady-state-cyclic}
Suppose $K = 2$, and $\lambda_2 = \lambda, \lambda_1 = {0.1\lambda}$, $\lambda > 0$. Let $\Xi = \{0,r\}$, for some $r > 0$, with departure probabilities given by:
% with $\lambda_1 = 1, \lambda_2 = 10$. Let $\Xi = \{0,r\}$, for some $r > 0$, with departure probabilities given by:
\begin{align*}
    \ell_1(r') = \begin{cases}
    0 \quad &\mbox{if }r' = r \\
    0.1 \quad &\mbox{if }r' = 0
    \end{cases} \qquad \text{and} \qquad 
    \ell_2(r') = \begin{cases}
    0.5 \quad &\mbox{if }r' = r \\
    1 \quad &\mbox{if }r' = 0.
    \end{cases}
\end{align*}
Suppose moreover that $\rev(\widetilde{N}) = \alpha \widetilde{N}$, $\alpha \in [0.7r, r)$. Consider the cyclic policy $\varphi^c$ of length $2$ which alternates between the two rewards in every period, i.e., the policy defined by $(\bx_r(t), t \in \mathbb{N}^+)$ such that:
\begin{align*}
    x_r(t) = \begin{cases}
    1 \quad &\mbox{if } t \text{ odd} \\
    0 \quad &\mbox{if } t \text{ even}.
    \end{cases}
\end{align*}
Then, $\widetilde{\Pi}(\varphi^c) -\widetilde{\Pi}(\varphi^s) = \Omega(\lambda).$
\end{proposition}

The cyclic policy described above engages in strategic {\it reward slashing}: it induces a large number of type 2 agents to stay in the system every other period, thus benefiting from their presence in the next period. In this next period, however, the decision-maker is able to retain all of its revenue as net profit by not incentivizing agents to stay in the system. %We moreover make the following important observation regarding this reward slashing policy: in expectation, {\it type 1 and type 2 agents experience different reward distributions.} 
It is easy to show that under $\varphi^c$, while a type 1 agent in expectation receives the higher reward approximately 50\% of the time, a type 2 agent is only paid the higher reward 40\% of the time in expectation. Thus, $\varphi^c$ fails to satisfy the group-fairness constraint.\footnote{This instance technically violates the assumption that $\ell(\rmax) > 0$. $\rmin = 0$, $\ell_2(\rmin) = 1$ and $\ell_1(\rmax) = 0$ were chosen for ease of exposition; one can similarly construct instances where $\rmin > 0$, $\ell_2(\rmin) = 1-\epsilon$, and $\ell_1(\rmax) = \epsilon$ for small enough $\epsilon > 0$ such that reward slashing is beneficial.}

{We highlight here that, in both examples constructed above, a reward of 0 and high exogenous arrival rates were chosen for clarity of exposition. One can similarly construct an example with $\rmin > 0$, and significantly smaller arrival rates (e.g., $\tilde{\lambda}_1 + \tilde{\lambda}_2 = 0.01(\lambda_1 + \lambda_2)$), with a significantly longer learning period / period of building up the number of agents in the system. Thus, these insights are not intrinsically tied to the exogenous arrival rate, or the presence of ``free'' agents.} 

{We conclude the section by noting that a natural quantity to consider is the {\it price of fairness}, i.e., the worst-case ratio (across all problem instances) between the decision-maker's optimal profit with all fairness constraints relaxed, and her profit under the optimal fair policy. A slight modification to the instance in \cref{ex:steady-state-cyclic} immediately gives us that the price of fairness in our setting is {\it unbounded}. To see this, let $v^*$ denote the value of the optimal fair solution for this instance. Defining revenue function $\widehat{R}(\widetilde{N}) = R(\widetilde{N}) - v^*$, the optimal fair solution for this new problem instance achieves a profit of zero, whereas the cyclic policy still achieves strictly positive profit, thus resulting in an unbounded price of fairness.}



\section{Structure and analysis of the fluid heuristic}\label{sec:asymptotic-opt}

Leveraging a deterministic relaxation to develop asymptotically optimal policies is a classical approach in many stochastic control problems. 
In these models, not only is the deterministic relaxation a natural upper bound, but it is also an attractive candidate to develop good policies due to its tractability; in particular, the corresponding fluid problem can typically be cast as either a linear or convex program. In this section, we first establish that our problem does not inherit such a convenient structure. {\cref{prop:fluid-non-cvx} formalizes this.}

{
\begin{proposition}\label{prop:fluid-non-cvx}
\ref{eq:fluid-opt} is, in general, non-convex.
\end{proposition}

{Thus, solving~\ref{eq:fluid-opt} efficiently is \emph{a priori} a difficult task given its high-dimensional and non-convex nature. Though the issue of non-convexity is typically circumvented via an exchange of variables~\cite{cao2020dynamic}, this approach fails in our setting due to the nonlinearity in the total cost of employing $\fluidn_i$ agents, $\fluidn_i\left(\sum_r rx_r\right)$, for $i \in [K]$. Further, the arbitrary heterogeneity in agent types (i.e., the lack of assumptions on $\left\{\ell_i(r)\right\}_{i \in [K]}$ stronger than non-increasing in $r$), makes a crisp characterization of the optimal solution seem elusive.}
} Despite this, we derive an algorithm that {\it efficiently} solves this a priori intractable problem. We moreover show that this optimal solution is asymptotically optimal in the original stochastic system.

\subsection{Structure and computation of the fluid optimum}\label{ssec:fluid-structure}
 
{\cref{thm:main-theorem} first formalizes that it is possible to derive tractable optimal solutions based on a surprising structural property.}

\begin{theorem}\label{thm:main-theorem}
For any instance of ~\ref{eq:fluid-opt}, independent of $K$ and $|\rewardset|$, there exists an optimal solution $\nf{\bx}^\star$ such that $|\supp(\nf{\bx}^\star)|\leq 2$.% is independent of the number of types $K$. In particular, $|\supp(\nf{\bx}^\star)| \leq 2$.
\end{theorem}
{Theorem~\ref{thm:main-theorem} presents a {\it structural} characterization of the fluid optimal solution which has far-reaching implications for the {\it analytical} and {\it computational} tractability of~\ref{eq:fluid-opt}.  In particular, suppose we fix $r_1,r_2 \in \rewardset$ and assume $\supp(\bx^\star) = \{r_1, r_2\}$, with $x$ being the weight placed on $r_1$, and $1-x$ the weight placed on $r_2$. Then, \ref{eq:fluid-opt} becomes a {one-dimensional} problem in $x$, and can be solved via the KKT conditions. This then implies that %{\it an optimal solution to~\ref{eq:fluid-opt} 
an {\it optimal solution to the non-convex problem} \ref{eq:fluid-opt} can be found efficiently {by exhaustively searching over $\binom{|\rewardset|}{2}$ possible pairs of rewards}.} %{Importantly, we leverage this property to derive both analytical and numerical insights in Sections \ref{sec:fairness} and \ref{sec:special-cases}.} 
We provide some high-level intuition for the strategy used to prove this key structural result below, and defer the proof of the theorem to Appendix \ref{apx:structure-of-fluid}.

Our proof technique is motivated by the following natural interpretation: a profit-maximizing solution must trade off between recruiting enough agents in order to collect high revenue, all the while keeping costs relatively low. We disentangle these two competing effects by introducing a closely related {\it budgeted supply maximization} problem, which we term~\ref{eq:supply-opt}. {At a high level,~\ref{eq:supply-opt} simplifies~\ref{eq:fluid-opt} by removing a degree of freedom: namely, {\it how much the decision-maker can spend to retain agents}. Given this added constraint, there are no longer two competing effects: the goal of the decision-maker is to simply recruit as many agents as possible.} We then constructively show that, given an optimal solution to \ref{eq:supply-opt} that places positive probability mass on more than two rewards, there exists a ``support reduction'' procedure that takes three appropriately chosen rewards and allocates all of the weight on one reward to the other two. Crucially, this procedure is designed so that both the average reward paid out to agents and the total number of agents are maintained. {We complete the proof by showing that, if an optimal solution to \ref{eq:supply-opt} has a support of size no more than 2, it must be that the same holds for \ref{eq:fluid-opt}.}


\subsection{Asymptotic optimality of the fluid heuristic in the stochastic system}

We now analyze the performance of the fluid-based heuristic in our original stochastic system. Let $\varphi^\star$ be the policy that draws rewards according to $\bx^\star$ in every period. 
\cref{prop:fluid-ub-for-all-theta} first establishes that $\optfluidprofit$ is an upper bound on the profit of the optimal policy in the system parameterized by~$\theta$.


\begin{proposition}\label{prop:fluid-ub-for-all-theta}
Let $\scaledv^\star~=~ \sup_{\varphi\in\Phi}\scaledv(\varphi)$. $v^\star_\theta \leq \optfluidprofit$ for all $\theta \in \mathbb{N}^+$.
\end{proposition}




{
%\paragraph{Performance bound. }
As a corollary of \cref{prop:det-to-stoch} and \cref{prop:fluid-ub-for-all-theta}, we obtain the standard $\mathcal{O}\left(\frac{1}{\sqrt{\theta}}\right)$ additive {loss} bound of the fluid heuristic. {Under a mild additional technical condition, we further prove that~$\scaledv(\varphi^\star)$ actually converges to this upper bound at a {\it linear} rate.}
Let {$\Lambdaub = \sum_i \frac{\lambda_i}{\ell_i(\rmax)}$ and $\Lambdalb = \sum_i \frac{\lambda_i}{\ell_i(r_{\min})}$}. Theorem~\ref{thm:asymptotic-result} characterizes the performance of policy $\varphi^\star$ relative to $\optfluidprofit$. }%, an upper bound on the objective of the} optimal static policy in the $\theta$-scaled regime.


\begin{theorem}\label{thm:asymptotic-result}
Suppose $\rev$ is twice-continuously differentiable over $\mathbb{R}_{> 0}$, and that there exists a constant $\alpha > 0$ such that $\rev''(n) \geq -\theta^{\alpha}$ for all $n \in \left[\frac1\theta,\Lambdaub\right)$. Then, there exists a constant $C > 0$ such that  $\scaledv(\varphi^\star) \geq {\optfluidprofit} - \frac{C}{\theta}.$
\end{theorem}



It is easy to check that ``reasonable'' concave functions such as $\rev(n) = n^{\beta}$, $\beta \in (0,1)$, and $\rev(n) = \log(1+n)$ satisfy the conditions of Theorem~\ref{thm:asymptotic-result}.

At a high level, our system distinguishes itself from {many} systems considered in the classical stochastic control literature due to the fact that it is ``self-adjusting,'' or, more specifically, ``self-draining.'' What we mean by this is that, even though $\varphi^\star$ is a static policy, its effect on the system does in fact depend on the current state: the higher the number of agents in the system, the more agents depart from the system in the next period, on average. This {directly} follows from the fact that the mean of a Binomial distribution is increasing in the number of trials. Such a ``self-draining'' feature acts as self-regulation, preventing the system from being overloaded with agents who yield little marginal revenue relative to the reward paid out. On the other end of the spectrum, the lower bound on $\rev''$ for small values of $x$ precludes exponentially steep functions, for which the revenue loss could be large when there are few agents in the system. A similar ``self-adjustment'' phenomenon leading to $\mathcal{O}\left(\frac1\theta\right)$-convergence was identified in~\cite{cao2020dynamic}, though for an entirely different setting.

\subsubsection{Numerical experiments} We conclude by complementing our theoretical results with numerical experiments. In particular, we illustrate the  performance of $\varphi^\star$ relative to two natural reward schemes a decision-maker may use: a deterministic payout in each period, and a lottery. We first describe the experimental setup.
 %We then investigate the sensitivity of various reward schemes to the convexity structure of agents' departure probability functions.

%\subsection{Performance of the fluid heuristic}


\paragraph{{Reward} schemes.} We assume the revenue function is newsvendor-like: for $x \in \mathbb{R}^+$,  $\rev(x) = 100\min\left\{x, 150\right\}$. We let $\rewardset = \{15, {16,}\ldots,60\}$ and consider three different reward schemes: $(i)$ %the reward distribution $\bx^\star$ given by 
the fluid-based heuristic, $(ii)$ {a lottery with variance $\widehat{\sigma}^2$ such that agents receive $\mu$ in expectation, with $\mu = \sum_r rx_r^\star$ and $\widehat{\sigma} = 10$,} and $(iii)$ a fixed reward $r^{\text{det}}$ (specifically, the optimal fixed reward).


\paragraph{Agent types.} We let $K = 3$, with $\lambda_1 = \lambda_2 = \lambda_3 = 10/3$, and consider the following departure probability functions, depicted in Figure~\ref{fig:fluid-probs}:
\begin{enumerate}
    \item type 1 agents: $\ell_1(r) = \min\left\{1,e^{\alpha_1(-r+15)}\right\}$, $\alpha_1 = 7/100$
    \item type 2 agents: $\ell_2(r) = -\alpha_2r + \beta_2$, $\alpha_2 = 1/45$, $\beta_2 = 4/3$
    \item type 3 agents: $\ell_3(r) = -\alpha_3r^2 + \beta_3r + \gamma_3$, $\alpha_3 = (1/2025), \beta_3 = 2/135, \gamma_3=8/9$.
\end{enumerate}



\paragraph{Performance metric.} 
For each of the three reward schemes $\varphi$ described above, we compute
$\optfluidprofit - \scaledv(\varphi)$, for $\theta\in\{1,\ldots,5\cdot 10^3\}.$
%, however, remains {\it asymptotically optimal} policies for which the additive regret {converges to zero}. 

\paragraph{Results.} We show the results {(for 1,000 replications of simulations)} in Figure~\ref{fig:fluid-regret}. These results illustrate the strong performance of the fluid heuristic for this particular problem instance. In particular, we indeed observe that the fluid heuristic converges to $\optfluidprofit$, %at a linear rate, 
as predicted by our theoretical results. Moreover, the fluid policy %consistently 
outperforms the deterministic and lottery schemes, {both of which have performance loss bounded away from zero, i.e., they are not (asymptotically) optimal.} 


\begin{figure}%\label{fig:numerics-results}
	\captionsetup[subfigure]{justification=centering}
	\centering
	\subfloat[\centering $\ell_i(r)$ vs. $r$, for $i \in \{1,2,3\}$ \label{fig:fluid-probs}]{{\includegraphics[width=0.45\textwidth]{Figures/numerics_worker_types} }}\quad%
	\subfloat[\centering Additive loss vs. $\theta$\label{fig:fluid-regret}]{{\includegraphics[width=0.45\textwidth]{Figures/minrev_3_3_3} }}
	\caption{\centering Additive loss of the fluid (dark blue curve), deterministic (dark orange curve), and lottery (green curve) heuristics.}%
	\label{fig:ogd-regret}%
\end{figure}




\section{Conclusion}\label{sec:discussion}


%\paragraph{Conclusion.}
{In this paper, we studied an organization's problem of designing {\it fair} compensation schemes when agents make stochastic participation decisions based on recent rewards.} 
Despite its stylized nature, our model gives rise to a complex stochastic optimization problem whose natural deterministic relaxation is also \emph{a priori} intractable. A surprising structural property of the deterministic relaxation, however, allowed us to design a tractable, fast-converging fluid-based heuristic policy. 
This policy is not only asymptotically optimal amongst all static ones, but also bounds the objective of all policies that fulfill a natural fairness property that restricts discrimination between different types. On the other hand, fairness comes at a price, as we show an unbounded price of fairness when either of the two fairness properties is relaxed. {This work opens up a number of interesting directions for future work. For instance, it would be interesting to understand to what extent our insights continue to hold under slightly modified fairness definitions (e.g.,  rather than imposing a hard fairness constraint, imposing a penalty in the objective corresponding to the maximum amount by which the average reward distribution seen by two different types differs.)}



\newpage
\bibliography{template}
\newpage


% Paper body
%\input{body.tex}

% In the interest of anonymization, please do not include acknowledgements in your submission.
%
%\begin{acks}
%
%	The authors would like to thank Dr. Maura Turolla of Telecom
%	Italia for providing specifications about the application scenario.
%
%	The work is supported by the \grantsponsor{GS501100001809}{National
%		Natural Science Foundation of
%		China}{http://dx.doi.org/10.13039/501100001809} under Grant
%	No.:~\grantnum{GS501100001809}{61273304\_a}
%	and~\grantnum[http://www.nnsf.cn/youngscientsts]{GS501100001809}{Young
%		Scientsts' Support Program}.
%
%
%\end{acks}




% Appendix
\appendix
\section{Omitted Proofs: \cref{sec:preliminaries}}

\subsection{Proof of \cref{prop:det-to-stoch}}
\begin{proof}
We have:
\begin{align}
    \Delta(T)
    &\leq\sum_{t=1}^T\left\lvert\EE\left[R\left(\frac{N^\theta(t)}{\theta}\right)-R(\widetilde{N}(t))\right]\right\rvert +  r_{\max}\sum_{t=1}^T\left\lvert\EE\left[\frac{N^\theta(t)}{\theta} - \widetilde{N}(t)\right]\right\rvert \notag \\
    &\leq (L+r_{\max})\sum_{t=1}^T \left\lvert\EE\left[\frac{N^\theta(t)}{\theta} - \widetilde{N}(t)\right]\right\rvert \label{eq:lip}\\
    &= (L+r_{\max})\sum_{t=1}^T\sqrt{\left(\EE\left[\frac{N^\theta(t)}{\theta} - \widetilde{N}(t)\right]\right)^2} \notag \\
    &\leq (L+r_{\max})\sum_{t=1}^T\sqrt{\EE\left[\left(\frac{N^\theta(t)}{\theta} - \widetilde{N}(t)\right)^2\right]} \label{eq:jen}.
\end{align}
where \eqref{eq:lip} follows from $L$-Lipschitz continuity of $R$, and \eqref{eq:jen} from Jensen's inequality.

The following lemma establishes the first connection between the number of agents in the deterministic systems and the expected number of agents in the large-market regime. We defer its proof to the end of the section.

\begin{lemma}\label{lem:det-stoch-num}
For all $i \in [K], \theta \in \mathbb{N}^+, t \in \mathbb{N}^+$, $N_i^\theta(t) \sim \text{Poi}(\theta\widetilde{N}_i(t))$.
\end{lemma}

Using the above fact, we then have that
\begin{align*}
\EE\left[\left(\frac{N^\theta(t)}{\theta} - \widetilde{N}(t)\right)^2\right] &= \text{Var}\left(\frac{N^\theta(t)}{\theta}\right) 
= \frac{1}{\theta^2}\sum_{i\in[K]}\EE[N_i^\theta(t)]
= \frac{1}{\theta}\sum_{i\in[K]}\widetilde{N}_i(t),
\end{align*}
where the second equality follows from the fact that the variance of a Poisson distribution is equal to its mean.
Since $\ell_i(\rmax) > 0$ for all $i$, there exists $N_{\max}$ such that $\widetilde{N}_i(t) \leq N_{\max}$ for all $i \in [K], t \in \mathbb{N}^+$. Plugging this back into \eqref{eq:jen}, we have:
\begin{align*}
    \Delta(T) \leq (L+\rmax)T\sqrt{\frac1\theta K N_{\max}}.
\end{align*}
Dividing by $T$ on both sides, we obtain the result.
\end{proof}

\begin{proof}[Proof of \cref{lem:det-stoch-num}.]
By Poisson. thinning, $N_i^\theta(t)$ is Poisson distributed for all $t\in\mathbb{N}^+$, with $\EE[N_i^\theta(t)] = \EE[N_i^\theta(t-1)(1-\sum_r\ell_i(r)x_r(t))] + \theta\lambda_i$. Dividing by $\theta$ on both sides:
\begin{align*}
    % \EE\left[\frac{N_i^\theta(t)}{\theta}\right] &= \EE\left[\frac{N_i^\theta(t-1)(1-\sum_r\ell_i(r)x_r(t))}{\theta}\right] + \lambda_i \\
   \EE\left[\frac{N_i^\theta(t)}{\theta}\right] &= \EE\left[\frac{N_i^\theta(t-1)}{\theta}\right](1-\sum_r\ell_i(r)x_r(t)) + \lambda_i 
\end{align*}
Recall, $\widetilde{N}_i(t) = \widetilde{N}_i(t-1)(1-\sum_r \ell_i(r)x_r(t)) + \lambda_i$ for all $i$. Initializing $\widetilde{N}_i(0) = \mathbb{E}\left[\frac{N_i^\theta(0)}{\theta}\right]$, we obtain the result.
\end{proof}




\section{Omitted Proofs: \cref{sec:main-results}}

In this section, we use the wrap-around convention that, given a cyclic policy, for $t' \in \{-(\tau-1),\ldots,0\}$, $\bx(t') := \bx(\tau+t')$. For a given distribution $\bx$, let $\widehat{\ell}_i(\bx) = \sum_r \ell_i(r) x_r$, for $i \in [K]$.  

We first introduce a useful result upon which we will rely in the remainder of the proofs in this section. In particular, \cref{lem:cyclic-steady-state-equations} derives the number of agents of each type induced by an arbitrary $\tau$-cyclic $\varphi^\tau$, for $\tau \in \mathbb{N}^+$. We defer its proof to Appendix \ref{apx:cyclic-aux-proofs}.

\begin{proposition}\label{lem:cyclic-steady-state-equations}
Fix $\tau \in \mathbb{N}^+$, and consider a $\tau$-cyclic policy defined by the sequence of reward distributions $(\bx(1), \bx(2),\ldots,\bx(\tau))$. The  number of type $i$ agents at time $t$, for $t \in \mathbb{N}^+, i \in [K]$, is given by:
\begin{align}\label{eq:lem-cyclic-steady-state-equations}
    \widetilde{N}_i(t) = \lambda_i \cdot \frac{\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t-1}\left[1-\widehat{\ell}_i(\bx(t'))\right] + 1}{1-\prod_{t'=1}^\tau \left[1-\widehat{\ell}_i(\bx(t'))\right]}.
\end{align}
\end{proposition}

\begin{proof}[Proof of \cref{lem:cyclic-steady-state-equations}.]
For ease of notation, we omit the dependence on the agent type $i$, and normalize the arrival rates to $\lambda_i = 1$ (this is without loss of generality since, given the policy, the types are independent). Moreover, let $Z_t = 1-\widehat{\ell}(\bx(t))$, for $t \in \mathbb{N}^+$. Recall, the dynamics of the deterministic relaxation:
\begin{align}\label{eq:steady-state-to-show}
    \widetilde{N}(t) = \widetilde{N}(t-1)Z_{t-1} + 1 \quad t \in \mathbb{N}^+, 
    \text{and} \quad \widetilde{N}(0) := \widetilde{N}(\tau).
\end{align}
By the cyclic property, it suffices to show the fact for $t \in [\tau]$.

It is easy to see that, if the above system of equations admits a solution, it is unique. Thus, it suffices to show that \eqref{eq:lem-cyclic-steady-state-equations} satisfies the dynamics of the deterministic relaxation.

Plugging \eqref{eq:lem-cyclic-steady-state-equations} into the right-hand side of \eqref{eq:steady-state-to-show}, we have:
\begin{align}\label{eq:check1}
    \widetilde{N}(t-1)Z_{t-1} + 1 &= \frac{\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-1-\tau'}^{t-2}Z_{t'}\right) + 1}{1-\prod_{t'=1}^\tau Z_{t'}}Z_{t-1} + 1 \notag \\
    &= \frac{\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-1-\tau'}^{t-2}Z_{t'}\right)Z_{t-1} + Z_{t-1} + 1-\prod_{t'=1}^\tau Z_{t'}}{1-\prod_{t'=1}^\tau Z_{t'}} \notag \\
    &= \frac{\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-1-\tau'}^{t-1}Z_{t'}\right) + Z_{t-1} + 1-\prod_{t'=1}^\tau Z_{t'}}{1-\prod_{t'=1}^\tau Z_{t'}}.
\end{align}
Observe that, at $\tau' = \tau-1$, $\prod_{t'=t-1-\tau'}^{t-1}Z_{t'}$ has exactly $\tau$ terms. Thus, by the cyclic property we can re-write the first two terms in the numerator as:
\begin{align*}
\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-1-\tau'}^{t-1}Z_{t'}\right) + Z_{t-1} &= \prod_{t'=1}^{\tau} Z_{t'} + \left(\sum_{\tau' = 1}^{\tau-2} \prod_{t'=t-1-\tau'}^{t-1}Z_{t'}\right) + Z_{t-1} \\
&=\prod_{t'=1}^{\tau} Z_{t'} + \sum_{\tau' = 0}^{\tau-2} \prod_{t'=t-1-\tau'}^{t-1}Z_{t'}
= \prod_{t'=1}^{\tau} Z_{t'} + \sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t-1}Z_{t'}
\end{align*}
where the final equality follows from simple re-indexing. Plugging this back into~\eqref{eq:check1} and simplifying, we obtain:
\begin{align*}
    \widetilde{N}(t-1)Z_{t-1} + 1 = \frac{\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t-1}Z_{t'}\right) + 1}{1-\prod_{t'=1}^\tau Z_{t'}} = \widetilde{N}(t).
\end{align*}
\end{proof}




\subsection{Proof of \cref{thm:static-policies-are-opt-for-one-type}}\label{apx:fairness-thm}

{We first introduce the notion of {\it cyclic fairness}.

\begin{definition}[$\delta$-cyclic fairness]
A $\tau$-cyclic policy $\varphi^\tau$ defined by a sequence of reward distributions $(\bx(t), \, t \in [\tau])$ is said to be $\delta$-\emph{cyclic fair} if :
\begin{align}\label{eq:cyclic-fair}
    \bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_i(t)\bx(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_j(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_j(t)\bx(t) \bigg{\rVert}_1 < \delta  \qquad \, \, \forall \, i,j \in [K]. \tag{\textsc{Cyclic Fairness}}
\end{align}
\end{definition}


In contrast to the notion of fairness defined above, cyclic fairness is only required to hold over individual {\it cycles} of length $\tau$, rather than for all intervals of length $\tau$. 
}


We proceed to the proof of the theorem, a consequence of the following two facts:
\begin{enumerate}
    \item (\cref{lem:cyclic-beats-state-dep}) For any fair policy $\varphi$, for all $\delta > 0$ there exists a cyclic policy $\varphi^\delta$ such that
    \begin{enumerate}
    \item the cyclic policy's profit is within $\delta$ of the profit under $\varphi$, and
    \item the cyclic policy is $2\delta$-cyclic fair.
    \end{enumerate}
    \item (\cref{lem:static-beats-cyclic}) For all $\epsilon > 0$, given any $\epsilon$-cyclic fair policy $\varphi^\tau$, there exists a static policy whose profit is within $C_0\epsilon$ of that of $\varphi^\tau$, for some constant $C_0 > 0$ independent of $\epsilon$.
\end{enumerate}



\begin{proof}[Proof of \cref{thm:static-policies-are-opt-for-one-type}.]
Consider an arbitrary fair policy $\varphi$, and fix $\delta > 0$. Consider the cyclic policy $\varphi^\delta$ that satisfies the two conditions of \cref{lem:cyclic-beats-state-dep}. Then, we have: $\widetilde{\Pi}(\varphi^\delta) \geq \widetilde{\Pi}(\varphi)-\delta$. Moreover, by \cref{lem:static-beats-cyclic}, since $\varphi^\delta$ is $2\delta$-cyclic fair, there exists a static policy ${\varphi}^s(\delta)$ that yields profit within $2C_0\delta$ of $\varphi^{\delta}$. Chaining these facts together, we obtain:
\begin{align*}
     \widetilde{\Pi}(\varphi^s(\delta)) \geq \widetilde{\Pi}(\varphi^\delta)-2C_0\delta \geq \widetilde{\Pi}(\varphi)-(1+2C_0)\delta.
\end{align*}
{Since the inequality holds for all $\delta>0$, we find that for any fair policy $\varphi$ we can construct a static policy with objective arbitrarily close to that of $\varphi$. Taking the supremum over all static policies we find that its objective must match that of the supremum over all fair policies $\varphi$.}
\end{proof}



We now prove the two key lemmas, relegating the proofs of auxiliary lemmas to Appendix \ref{apx:aux-lemmas-static-is-opt}.

\begin{lemma}\label{lem:cyclic-beats-state-dep}
Consider an arbitrary fair policy $\varphi$. Then, for all $\delta > 0$, there exists a cyclic policy $\varphi^\tau$, for some $\tau \in \mathbb{N}^+$, such that (1) $\widetilde{\Pi}(\varphi^{\tau}) \geq \widetilde{\Pi}(\varphi)-\delta$, and (2) $\varphi^\tau$ is $2\delta$-cyclic fair.
\end{lemma}

\begin{proof}
For ease of notation, let $\ell(\rmax) = \min_{i\in[K]} \ell_i(\rmax)$. Fix $\delta > 0$, and let $\tau$ be such that \eqref{eq:fair-policy} holds. Given a fixed time horizon $T$, partition the time horizon into $L = \lceil{\frac{T}{\tau}}\rceil$ segments of length $\tau$. Let $\widetilde{\Pi}^{(l)}(\varphi)$ denote the cumulative profit under $\varphi$ during time segment $l$, for $l \in \{1,\ldots,{L}\}$. The long-run average profit is then given by:
\begin{align*}
    \lim_{L\to\infty} \frac{1}{\tau L} \sum_{l=1}^L \widetilde{\Pi}^{(l)}(\varphi) &\leq \frac1\tau \sup_{l \in \mathbb{N}^+} \widetilde{\Pi}^{(l)}(\varphi).
\end{align*}
Suppose first that the supremum on the right-hand side is attained, and let $l^*$ denote the time segment such that $\widetilde{\Pi}^{(l^*)}(\varphi) = \sup_{l} \widetilde{\Pi}^{(l)}(\varphi)$, with $(\bx^{(l^*)}(1),\ldots,\bx^{(l^*)}(\tau))$ the sequence of reward distributions distributed by $\varphi$ during $l^*$.\footnote{Without loss of generality we re-index this truncated policy from $t = 1$ to $\tau$.} 

Now, let ${\varphi}^\tau$ denote the $\tau$-cyclic policy such that
\begin{align}\label{eq:mirror-def}
(\bx^\tau(1),\ldots,\bx^\tau(\tau)) = (\bx^{(l^*)}(1),\ldots,\bx^{(l^*)}(\tau-1),\mathbf{e}_{\rmin}).
\end{align}

That is, ${\varphi}^\tau$ mirrors the policy used in segment $l^*$ for the first $\tau-1$ time periods of a cycle, and pays out the minimum reward in the last period. (Recall, $\ell_i(\rmin) = 1$ for all $i \in [K]$ by assumption.)  For ease of notation, in the remainder of the proof we omit the dependence of $\bx^{(l^*)}$ on $l^*$.

We first show that the profits achieved by these two policies differ by at most $\mathcal{O}(1/\tau)$. Let $\widetilde{N}^*(t)$ and $\widetilde{N}^\tau(t)$ respectively denote the  number of agents under $\varphi$ and $\varphi^\tau$ at time $t \in [\tau]$. The difference in average profits over $\tau$ time periods, then, is:
\begin{align*}
  \frac1\tau \widetilde{\Pi}^{(l^*)}(\varphi)-\widetilde{\Pi}({\varphi}^\tau) &= \frac1\tau \Bigg(\sum_{t=1}^{\tau} \left[\rev(\widetilde{N}^*(t)) - \rev(\widetilde{N}^{\tau}(t))\right] \\&\qquad -
   \sum_{t=1}^{\tau}\left[\widetilde{N}^*(t) \sum_r rx_r (t)\right] + \sum_{t=1}^{\tau}\left[\widetilde{N}^\tau(t) \sum_r rx_r^\tau(t)\right] \Bigg) \\
   &\leq \frac1\tau \Bigg(\sum_{t=1}^{\tau} \left[\rev(\widetilde{N}^*(t)) - \rev(\widetilde{N}^{\tau}(t))\right] -
   \sum_{t=1}^{\tau}\left(\widetilde{N}^*(t)-\widetilde{N}^\tau(t)\right) \sum_r rx_r (t)\Bigg),
\end{align*}
where the inequality follows from the fact that $\sum_r r x_r(t) \geq \sum_r r x_r^\tau(t)$ for all $t$, by \eqref{eq:mirror-def}. Similarly, since ${\varphi}^\tau$ pays out $\rmin$ in the last period of each cycle, the  number of agents under $\varphi^\tau$ is lower at the beginning of a cycle, and consequently for the rest of time. Thus, the cost difference between the two policies $\sum_{t=1}^{\tau}\left(\widetilde{N}^*(t)) - \widetilde{N}^{\tau}(t)\right)\sum_r rx_r(t)$ is nonnegative, and
\begin{align}\label{eq:diff-2}
    \frac1\tau\widetilde{\Pi}^{(l^*)}(\varphi)-\widetilde{\Pi}({\varphi}^\tau) &\leq \frac1\tau \sum_{t=1}^{\tau} \rev(\widetilde{N}^*(t)) - \rev(\widetilde{N}^{\tau}(t)) 
    \leq \frac{L}{\tau}\sum_{t=1}^{\tau} \widetilde{N}^*(t) - \widetilde{N}^{\tau}(t),
\end{align}
{where, recall, $L$ denotes the Lipschitz constant of $R$}. 


For $t \in [\tau]$, the  number of type $i$ agents in the system at time $t$, for any initial number of agents $\widetilde{N}_i(0)$ is:
\begin{align}\label{eq:key-decomposition}
  \widetilde{N}_i(t) = \widetilde{N}_i(0) \prod_{t'=1}^t (1-\widehat{\ell}_i(\bx(t'-1))) + \sum_{t'=1}^t \prod_{t''=t'}^t (1-\widehat{\ell}_i(\bx(t''-1))),
\end{align}
where we use the convention that $\bx(0)$ is such $\widehat{\ell}_i(\bx(0)) = 0$ for all $i \in [K]$.

Let $\widetilde{N}_i^*(0)$ and $\widetilde{N}_i^\tau(0)$ respectively denote the initial number of type $i$ agents under policies $\varphi$ and ${\varphi}^\tau$. By \eqref{eq:key-decomposition}, for all $t\in[\tau]$ we have:
\begin{align*}
    \widetilde{N}^*(t)-\widetilde{N}^\tau(t) &= \left(\widetilde{N}_i^*(0) \prod_{t'=1}^t (1-\widehat{\ell}_i(\bx(t'-1))) + \sum_{t'=1}^t \prod_{t''=t'}^t (1-\widehat{\ell}_i(\bx(t''-1)))\right)\\
    &\quad- \left(\widetilde{N}_i^\tau(0) \prod_{t'=1}^t (1-\widehat{\ell}_i(\bx^\tau(t'-1))) + \sum_{t'=1}^t \prod_{t''=t'}^t (1-\widehat{\ell}_i(\bx^\tau(t''-1)))\right) \\
    &= \left(\widetilde{N}_i^*(0)-\widetilde{N}_i^\tau(0)\right) \prod_{t'=1}^t (1-\widehat{\ell}_i(\bx(t'-1)))
\end{align*}
where the second equality follows from the fact that $\varphi^\tau$ {\it exactly mirrors} $\varphi$ over $\{1,\ldots,\tau-1\}$.



Plugging this into~\eqref{eq:diff-2}, we obtain:
\begin{align*}
    \frac1\tau\widetilde{\Pi}^{(l^*)}(\varphi)-\widetilde{\Pi}({\varphi}^\tau) &\leq \frac{L}{\tau} \sum_{t=1}^{\tau} \sum_{i\in[K]}(\widetilde{N}_i^*(0)-\widetilde{N}_i^\tau(0))\prod_{t'=1}^t (1-\widehat{\ell}_i(\bx(t'-1))) \\
    &\leq \frac{L}{\tau}\sum_{i\in[K]}\widetilde{N}_i^*(0)-\widetilde{N}_i^\tau(0)\sum_{t=1}^{\tau} (1-\ell({\rmax}))^t \\
    &= \frac{L}{\tau}\left(\sum_{i\in[K]}\widetilde{N}_i^*(0)-\widetilde{N}_i^\tau(0)\right)(1-\ell({\rmax}))\frac{1-(1-\ell({\rmax}))^{\tau}}{\ell({\rmax})} \\
    &\leq \frac{L}{\tau}N_{\max}\frac{1-\ell({\rmax})}{\ell({\rmax})}
\end{align*}
where $N_{\max} < \infty$ is the maximum number of agents in any time period, achieved by deterministically paying out $\rmax$ in each timestep. Note that $N_{\max}$ is a finite constant due to the assumption that $\ell({\rmax}) > 0$.

We now show that that, for large enough $\tau$, $\varphi$ is $2\delta$-cyclic fair. 
\cref{lem:cyclic-not-far-from-og} states that the expected reward distribution seen by agents under $\varphi^\tau$ is close to the expected reward distribution under $\varphi$ during segment $l^*$. We defer its proof to Appendix \ref{apx:aux-lemmas-static-is-opt}.

\begin{lemma}\label{lem:cyclic-not-far-from-og}
There exists $C > 0$ such that, for all $i \in [K]$,
\begin{align*}
\bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_i(t)\bx^\tau(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^*_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^*_i(t)\bx(t) \bigg{\rVert}_1 \leq C/\tau.
\end{align*}
\end{lemma}



Thus, by \cref{lem:cyclic-not-far-from-og}, for all $i, j \in [K]$, we have:
\begin{align*}
&\bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_i(t)\bx^\tau(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_j(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_j(t)\bx^\tau(t) \bigg{\rVert}_1  \\  &\qquad \leq \bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_i(t)\bx^\tau(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^*_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^*_i(t)\bx(t) \bigg{\rVert}_1 \\
&\qquad \qquad + \bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^\tau_j(t)}\sum_{t=1}^{\tau} \widetilde{N}^\tau_j(t)\bx^\tau(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^*_j(t)}\sum_{t=1}^{\tau} \widetilde{N}^*_j(t)\bx(t) \bigg{\rVert}_1 \\
&\qquad \qquad \qquad + \bigg{\lVert}\frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^*_i(t)}\sum_{t=1}^{\tau} \widetilde{N}^*_i(t)\bx(t) - \frac{1}{\sum_{t=1}^{\tau}\widetilde{N}^*_j(t)}\sum_{t=1}^{\tau} \widetilde{N}^*_j(t)\bx(t) \bigg{\rVert}_1 \\
&\qquad \leq 2C/\tau + \delta,
\end{align*}
where the first inequality is a simple application of the triangle inequality, and the second inequality follows from \cref{lem:cyclic-not-far-from-og} and the fact that $\tau$ is such that \ref{eq:fair-policy} holds, by construction. Choosing $\tau \geq \frac1\delta \max\left\{2C,L N_{\max}\frac{1-\ell({\rmax})}{\ell(\rmax)}\right\}$ ensures that $\varphi^\tau$ is $2\delta$-cyclic fair and within $\delta$ of the profit under $\varphi$.


We conclude the proof by considering the case where the supremum is not attained. In this case, consider any $l^{(\epsilon)} < \infty$ such that $\widetilde{\Pi}^{(l^{(\epsilon)})}(\varphi) \geq \sup_{l \in \mathbb{N}^+}\widetilde{\Pi}^{(l)}(\varphi) - \epsilon$, for constant $\epsilon > 0$. An identical argument as above proves that the $2\delta$-cyclic fairness guarantee holds for large enough $\tau$. Moreover, in the profit bound we lose at most an additive factor of $\epsilon/\tau$ in profits. Choosing $\tau \geq \frac1\delta \max\left\{2C, L N_{\max}\frac{1-\ell({\rmax})}{\ell(\rmax)} + \epsilon\right\}$ recovers the $\delta$ additive loss.
\end{proof}

 

\begin{lemma}\label{lem:static-beats-cyclic}
Fix $\tau \in \mathbb{N}^+$, and consider any $\tau$-cyclic policy $\varphi^\tau$ that is $\epsilon$-cyclic fair, for $\epsilon > 0$. There exists a static policy $\varphi^s$ such that 
    $\widetilde{\Pi}(\varphi^s) \geq \widetilde{\Pi}(\varphi^\tau) - C_0\epsilon$,
for some constant $C_0 > 0$ that is independent of $\epsilon$.
\end{lemma}

\begin{proof}
The proof is constructive. Let $\varphi^\tau$ be defined by the sequence of $\tau$ reward distributions $\left(\bx(1),\ldots,\bx(\tau)\right)$. Moreover, fix any $i \in [K]$, and let $\varphi^s$ be the static policy defined by the following reward distribution:
$\bx^s = \sum_{t=1}^\tau \frac{\widetilde{N}_i^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_i^\tau(t')} \bx(t).$
Let $\widetilde{N}_j^s$ denote the number of type $j$ agents induced by $\varphi^s$, for $j \in [K]$.  Recall,
   $ \widetilde{N}_j^s = \frac{\lambda_j}{\widehat{\ell}_j(\bx^s)}.$ Let $\bx^{s,j} = \sum_{t=1}^\tau \frac{\widetilde{N}_j^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_j^\tau(t')} \bx(t)$, for $j \in [K]$. Since $f_j(z):= \frac{\lambda_j}{z}$ is $C_f$-Lipschitz over $[\ell(\rmax),1]$ for some $C_f > 0$, we have:
\begin{align}\label{eq:lipschitz-num}
   \bigg{\lvert}\, \widetilde{N}_j^s-\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})}\, \bigg{\rvert} &\leq  C_f\, \bigg{\lvert}\, \widehat{\ell}(\bx^{s,j})-\widehat{\ell}_j(\bx^s)\, \bigg{\rvert} \\ &= C_f\, \bigg{\lvert}\, \sum_r \ell_j(r)\sum_{t=1}^\tau x_r(t)\left(\frac{\widetilde{N}_j^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_j^\tau(t')} - \frac{\widetilde{N}_i^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_i^\tau(t')}\right)\, \bigg{\rvert} \notag \\
   &\leq C_f\,\sum_r \bigg{\lvert}\,\sum_{t=1}^\tau x_r(t)\left(\frac{\widetilde{N}_j^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_j^\tau(t')} - \frac{\widetilde{N}_i^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_i^\tau(t')}\right)\, \bigg{\rvert}\\& \leq C_f\epsilon,
\end{align}
where the final inequality follows from the assumption that $\varphi^\tau$ is $\epsilon$-cyclic fair. We can then leverage \eqref{eq:lipschitz-num} to obtain a bound on the total number of agents in the system, i.e.,
\begin{align}\label{eq:lipschitz-total-num}
     \bigg{\lvert}\, \widetilde{N}^s-\sum_{j \in [K]}\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})}\, \bigg{\rvert} \leq C_fK\epsilon.
\end{align}
We use this fact to obtain a lower bound on the profit under $\varphi^s$:
\begin{align}\label{eq:prof-ub1}
    \widetilde{\Pi}(\varphi^s) &= \rev(\widetilde{N}^s) - \left(\sum_r rx_r^s\right)\widetilde{N}^s \\
    &\geq \rev\left(\sum_{j \in [K]}\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})}\right) - LC_fK\epsilon - \left(\sum_r rx_r^s\right)\left(\sum_{j \in [K]}\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})} + C_fK\epsilon\right),
\end{align}
where the inequality follows by putting \eqref{eq:lipschitz-total-num} together with the assumption that $\rev$ is $L$-Lipschitz.

We can simplify \eqref{eq:prof-ub1} via \cref{lem:weight-avg-conserves-num}, whose proof we defer to Appendix \ref{apx:aux-lemmas-static-is-opt}.
\begin{lemma}\label{lem:weight-avg-conserves-num}
For all $j \in [K]$, $\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})}$ is equal to the average number of agents under the cyclic policy, i.e.,
$$\frac{\lambda_j}{\widehat{\ell}_j(\bx^{s,j})} = \frac1\tau \sum_{t=1}^\tau \widetilde{N}^\tau_j(t).$$
\end{lemma}

Applying this to \eqref{eq:prof-ub1}, we obtain:
\begin{align}\label{eq:prof-ub2}
    \widetilde{\Pi}(\varphi^s) &\geq \rev\left(\frac{1}{\tau}\sum_{j\in[K]}\sum_{t=1}^\tau \widetilde{N}^\tau_j(t)\right) - LC_fK\epsilon - \left(\sum_r rx_r^s\right)\left(\frac1\tau\left(\sum_{j \in [K]}\sum_{t=1}^\tau \widetilde{N}^\tau_j(t)\right) + C_fK\epsilon\right) \notag \\
    &\geq \frac1\tau\sum_{t=1}^\tau \rev\left(\sum_{j\in[K]}\widetilde{N}_j^\tau(t)\right)  - \underbrace{\left(\sum_r rx_r^s\right)\left(\sum_{j \in [K]}\frac1\tau\sum_{t=1}^\tau \widetilde{N}^\tau_j(t)\right)}_{(I)}- LC_fK\epsilon-\rmax C_fK\epsilon,
\end{align}
where the final inequality follows from concavity of $\rev$, and $\sum_r rx_r^s \leq \rmax$. We now focus on $(I)$. By definition of $\bx^s$, for $j \in [K]$, we have:
\begin{align*}
    \sum_{t=1}^\tau \widetilde{N}_j^\tau(t)\sum_r rx_r^s &= \left(\sum_{t=1}^\tau \widetilde{N}_j^\tau(t)\right)\left(\sum_r r \sum_{t=1}^\tau\frac{\widetilde{N}_i^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_i^\tau(t')} x_r(t)\right) \\
    &\leq \left(\sum_{t=1}^\tau \widetilde{N}_j^\tau(t)\right)\,  \left( \left(\sum_r r\sum_{t=1}^\tau\frac{\widetilde{N}_j^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}_j^\tau(t')} x_r(t) \right) + \rmax \epsilon\right),
\end{align*}
where the inequality follows from $\epsilon$-cyclic fairness of $\varphi^\tau$.
Simplifying, we obtain:
\begin{align*}
\sum_{t=1}^\tau \widetilde{N}_j^\tau(t)\sum_r rx_r^s &\leq \sum_{t=1}^\tau \widetilde{N}_j^\tau(t) \sum_r r x_r(t) + \rmax N_{\max}\tau\epsilon \\
\implies \frac1\tau\sum_{j\in[K]}\sum_{t=1}^\tau \widetilde{N}_j^\tau(t)\sum_r rx_r^s &\leq \frac1\tau\sum_{t=1}^\tau\sum_{j\in[K]}\widetilde{N}_j^\tau(t)\sum_r rx_r(t) + \rmax N_{\max} K \epsilon.
\end{align*}

Plugging this back into~\eqref{eq:prof-ub2}:
\begin{align*}
    \widetilde{\Pi}(\varphi^s) &\geq \frac1\tau \sum_{t=1}^\tau \rev\left(\widetilde{N}^\tau(t)\right) - \frac1\tau\sum_{t=1}^\tau\widetilde{N}^\tau(t)\sum_r rx_r(t) - \left(\rmax N_{\max}K + LC_fK + \rmax C_fK\right) \epsilon\\
    &= \widetilde{\Pi}(\varphi^\tau) - C_0\epsilon,
\end{align*}
where $C_0 = \rmax N_{\max}K + LC_fK + \rmax C_fK$.
\end{proof}




\subsubsection{Proofs of auxiliary lemmas}\label{apx:aux-lemmas-static-is-opt}

\begin{proof}[Proof of \cref{lem:cyclic-not-far-from-og}.]
For ease of notation, we omit the dependence on the agent type $i$, and emphasize that all subsequent derivations refer to a {\it single} agent type.

Using the definition of $\bx^\tau$ in \eqref{eq:mirror-def}, we have:
\begin{align}\label{eq:cyclic-ratio}
    \frac{\sum_{t=1}^\tau \widetilde{N}^\tau(t)\bx^\tau(t)}{\sum_{t=1}^\tau\widetilde{N}^\tau(t)}  &= \frac{\left(\sum_{t=1}^{\tau-1} \widetilde{N}^\tau(t)\bx(t)\right) + \widetilde{N}^\tau(\tau) \bx^\tau(\tau)}{\sum_{t=1}^\tau\widetilde{N}^\tau(t)} 
\end{align}

By \eqref{eq:key-decomposition}, the number of agents at time $t$ in the $\tau$-cyclic policy is given by:
\begin{align}\label{eq:key-decomp2}
    \widetilde{N}^\tau(t) &= \widetilde{N}^\tau(0) \prod_{t'=1}^t (1-\widehat{\ell}(\bx(t'-1))) + \sum_{t'=1}^t \prod_{t''=t'}^t (1-\widehat{\ell}(\bx(t''-1)))\notag \\
    &= \widetilde{N}^*(0)\prod_{t'=1}^t (1-\widehat{\ell}(\bx(t'-1))) + \sum_{t'=1}^t \prod_{t''=t'}^t (1-\widehat{\ell}(\bx(t''-1)))\notag \\ &\hspace{2cm}+ (\widetilde{N}^\tau(0) - \widetilde{N}^*(0))\prod_{t'=1}^t (1-\widehat{\ell}(\bx(t'-1))) \notag \\
    &= \widetilde{N}^*(t) + (\widetilde{N}^\tau(0) - \widetilde{N}^*(0))\prod_{t'=1}^t (1-\widehat{\ell}(\bx(t'-1))),
\end{align}
where the final equality also follows from \eqref{eq:key-decomposition}. For ease of notation, let $c_0 = \widetilde{N}^*(0)-\widetilde{N}^\tau(0) < N_{\max}$, and let $Z_t = (1-\widehat{\ell}(\bx(t))$ for all $t \in [\tau]$. Plugging \eqref{eq:key-decomp2} into the numerator of the right-hand side of \eqref{eq:cyclic-ratio}, we have:
\begin{align}\label{eq:numerator}
    \left(\sum_{t=1}^{\tau-1} \widetilde{N}^\tau(t)\bx(t)\right) + \widetilde{N}^\tau(\tau) \bx^\tau(\tau) &= \left(\sum_{t=1}^{\tau-1} \widetilde{N}^*(t)\bx(t)\right) - c_0 \left(\sum_{t=1}^{\tau-1} \bx(t) \prod_{t'=1}^{t} Z_{t'-1} \right) + \widetilde{N}^\tau(\tau) \bx^\tau(\tau)\notag \notag \\
    &= \left(\sum_{t=1}^{\tau} \widetilde{N}^*(t)\bx(t)\right)- c_0 \left(\sum_{t=1}^{\tau-1} \bx(t) \prod_{t'=1}^{t} Z_{t'-1} \right)\notag \\&\qquad - \widetilde{N}^*(\tau)\bx(\tau)   + \widetilde{N}^\tau(\tau) \bx^\tau(\tau) \notag \\
    &= \left(\sum_{t=1}^{\tau} \widetilde{N}^*(t)\bx(t)\right)- c_0 \left(\sum_{t=1}^{\tau-1} \bx(t) \prod_{t'=1}^{t} Z_{t'-1} \right)\notag \\&\qquad - \widetilde{N}^*(\tau)\left(\bx(\tau)- \bx^\tau(\tau)\right) - c_0\bx^\tau(\tau)\prod_{t'=1}^{\tau} Z_{t'-1}, 
\end{align}
where the final equality again follows from \eqref{eq:key-decomp2}. Plugging this back into \eqref{eq:mirror-def}, we have:
\begin{align*}%\label{eq:ratio-full-exp}
     &\frac{\sum_{t=1}^\tau \widetilde{N}^\tau(t)\bx^\tau(t)}{\sum_{t=1}^\tau\widetilde{N}^\tau(t)} = \frac{\sum_{t=1}^{\tau} \widetilde{N}^*(t)\bx(t)}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} - \frac{c_0 \left(\sum_{t=1}^{\tau-1} \bx(t) \prod_{t'=1}^{t} Z_{t'-1} \right)}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \notag \\&\hspace{3cm}- \frac{ \widetilde{N}^*(\tau)\left(\bx(\tau)- \bx^\tau(\tau)\right) + c_0\bx^\tau(\tau)\prod_{t'=1}^{\tau} Z_{t'-1}}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \notag \\
     \implies &\frac{\sum_{t=1}^\tau \widetilde{N}^\tau(t)\bx^\tau(t)}{\sum_{t=1}^\tau\widetilde{N}^\tau(t)} -  \frac{\sum_{t=1}^\tau \widetilde{N}^*(t)\bx(t)}{\sum_{t=1}^\tau\widetilde{N}^*(t)} = \frac{\left(c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}\right)\left(\sum_{t=1}^\tau\widetilde{N}^*(t)\bx(t)\right)}{\left(\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}\right)\left(\sum_{t=1}^\tau \widetilde{N}^*(t)\right)}\notag \\
     &\hspace{5cm} - \frac{c_0 \left(\sum_{t=1}^{\tau-1} \bx(t) \prod_{t'=1}^{t} Z_{t'-1} \right) +c_0 \bx^\tau(\tau) \prod_{t'=1}^\tau Z_{t'-1} }{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \notag \\
     &\hspace{5cm}-\frac{ \widetilde{N}^*(\tau)\left(\bx(\tau)- \bx^\tau(\tau)\right)}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \notag.
    \end{align*}
    
    Taking the $L_1$-norm and applying the triangle inequality, we obtain:
    \begin{align*}
     \bigg{\lVert}\frac{\sum_{t=1}^\tau \widetilde{N}^\tau(t)\bx^\tau(t)}{\sum_{t=1}^\tau\widetilde{N}^\tau(t)} -  \frac{\sum_{t=1}^\tau \widetilde{N}^*(t)\bx(t)}{\sum_{t=1}^\tau\widetilde{N}^*(t)}\bigg{\rVert}_1 &\leq \frac{3c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \notag \\
     &\quad+ \frac{2N_{\max}}{\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}} \\
     &\stackrel{(a)}{\leq} \frac{3c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1}}{\lambda_i\tau} + \frac{2N_{\max}}{\lambda_i\tau} \\
     &\stackrel{(b)}{\leq} \frac{3c_1 + 2N_{\max}}{\lambda_i\tau},
\end{align*}
where $(a)$ uses the fact that  $\sum_{t=1}^\tau \widetilde{N}^*(t) - c_0 \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1} =  \sum_{t=1}^\tau \widetilde{N}^\tau(t) \geq \lambda_i \tau$, and $(b)$ leverages $\ell(\rmax) > 0 \implies \sum_{t=1}^\tau \prod_{t'=1}^t Z_{t'-1} \leq \sum_{t=1}^\tau (1-\ell(\rmax))^t$, which is upper bounded by a constant independent of $\tau$. Defining $C = \max_{i\in [K]} \frac{3c_1 + 2N_{\max}}{\lambda_i}$ completes the proof of the claim.
\end{proof}

\begin{proof}[Proof of \cref{lem:weight-avg-conserves-num}.]
For ease of notation, we omit the dependence of all quantities on the agent type $j$, except for distribution $\bx^{s,j}$, and we normalize the arrival rates to $\lambda_j = 1$. {We again} let $Z_t = 1-\widehat{\ell}(\bx(t))$ for $t \in \mathbb{N}^+$, and define $\beta_t = \frac{\widetilde{N}^\tau(t)}{\sum_{t'=1}^\tau \widetilde{N}^\tau(t')}$. %Without loss of generality, suppose a unit mass of agents enters the platform in each period. 
It is easy to check that, by linearity:
\begin{align}\label{eq:total-static-number}
 \frac{1}{\widehat{\ell}(\bx^{s,j})}{=} \frac{1}{\sum_{t=1}^\tau \beta_t \widehat{\ell}(\bx(t))} {=} \frac{1}{1-\sum_{t=1}^\tau \beta_t Z_t}.
\end{align}

\cref{lem:weighted-avg} leverages \cref{lem:cyclic-steady-state-equations} to allow us to obtain a closed-form expression of $\frac{1}{\widehat{\ell}(\bx^{s,j})}$ as a function of $Z_t$. We defer its proof below.

\begin{lemma}\label{lem:weighted-avg}
\begin{align*}
    \sum_{t=1}^\tau \beta_tZ_t = \frac{\tau\left(\prod_{t=1}^\tau Z_t - 1\right)}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } + 1.
\end{align*}
\end{lemma}

Plugging this into \eqref{eq:total-static-number}, we obtain:
\begin{align}\label{eq:total-static-number2}
\frac{1}{\widehat{\ell}(\bx^{s,j})} = \frac1\tau \cdot \frac{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau }{1-\prod_{t=1}^\tau Z_t} = \frac1\tau\cdot \sum_{t=1}^\tau \frac{\left(\sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'}\right)  + 1}{{1-\prod_{t=1}^\tau Z_t} }.
\end{align}

Recall, by \cref{lem:cyclic-steady-state-equations}, for all {$t \in \mathbb{N}^+$}, we have:
\begin{align}\label{eq:cyclic-steady-state-equations}
    \widetilde{N}^\tau(t) = \frac{\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t-1}\left[1-\widehat{\ell}(\bx(t'))\right] + 1}{1-\prod_{t'=1}^\tau \left[1-\widehat{\ell}(\bx(t'))\right]}.
\end{align}

Noting that the right-hand side of \eqref{eq:total-static-number2} is exactly the average number of agents in the system per period under $\varphi^\tau$, by \eqref{eq:cyclic-steady-state-equations}, we obtain the claim.
\end{proof}

 

\begin{proof}[Proof of \cref{lem:weighted-avg}.]
By definition of $\beta_t$, we have:
\begin{align}\label{eq:step1}
    \sum_{t=1}^\tau \beta_t Z_t = \frac{1}{\sum_{t=1}^\tau \widetilde{N}(t)}\cdot \sum_{t=1}^\tau \widetilde{N}(t)Z_t.
\end{align}
Plugging the expression for $\widetilde{N}(t)$ into \eqref{eq:step1} and factoring terms, we have:
\begin{align}
    \sum_{t=1}^\tau \beta_t Z_t &= \frac{1}{\sum_{t=1}^\tau \widetilde{N}(t)} \cdot \frac{1}{1-\prod_{t=1}^\tau Z_t} \cdot \left[\sum_{t=1}^\tau\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t-1}Z_{t'}Z_t \right)+ Z_t\right] \notag \\
    &= \frac{1}{\sum_{t=1}^\tau \widetilde{N}(t)} \cdot \frac{1}{1-\prod_{t=1}^\tau Z_t} \cdot \left[\sum_{t=1}^\tau\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t}Z_{t'}\right)+ Z_t\right] \notag.
    \end{align}
We further simplify the first two terms of the product by noting that the denominator of $\widetilde{N}(t)$ is exactly $1-\prod_{t=1}^\tau Z_t$, and obtain:
\begin{align}\label{eq:zhat}
    \sum_{t=1}^\tau \beta_t Z_t &= \frac{1}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } \cdot { \left[\sum_{t=1}^\tau\left(\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t-\tau'}^{t}Z_{t'}\right) + Z_t\right]} \notag \\
    &= \frac{1}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } \cdot \left[\left(\sum_{t=1}^\tau\left(\sum_{\tau' = 1}^{\tau-2} \prod_{t'=t-\tau'}^{t}Z_{t'}\right) + Z_t\right) +\left(\sum_{t=1}^\tau \prod_{t'=t-(\tau-1)}^{t}Z_{t'}\right)\right].
\end{align}
Noting that $\prod_{t'=t-(\tau-1)}^t Z_{t'}$ contains $\tau$ terms, by the cyclic property we have $\prod_{t'=t-(\tau-1)}^t Z_{t'} = \prod_{t'=1}^\tau Z_{t'}$ for all $t \in [\tau]$. Plugging this fact into \eqref{eq:zhat}, we obtain:
\begin{align}
    \sum_{t=1}^\tau \beta_t Z_t &= \frac{1}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } \cdot \left[\left(\sum_{t=1}^\tau\left(\sum_{\tau' = 1}^{\tau-2} \prod_{t'=t-\tau'}^{t}Z_{t'}\right) + Z_t\right) +\tau \prod_{t'=1}^{\tau}Z_{t'}\right] \notag \\
    &= \frac{1}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } \cdot \left[\left(\sum_{t=1}^\tau\sum_{\tau' = 0}^{\tau-2} \prod_{t'=t-\tau'}^{t}Z_{t'}\right) +\tau \prod_{t'=1}^{\tau}Z_{t'}\right] \label{eq:zero-indexing} \\
    &= \frac{1}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } \cdot \underbrace{\left[\underbrace{\left(\sum_{t=1}^\tau\sum_{\tau' = 1}^{\tau-1} \prod_{t'=t+1-\tau'}^{t}Z_{t'}\right)}_{(I)} +\tau \prod_{t'=1}^{\tau}Z_{t'}\right]}_{(II)} \label{eq:reindexing}
\end{align}
where \eqref{eq:zero-indexing} follows from the trivial equality $Z_t = \prod_{t'=t}^t Z_{t'}$ for all $t \in [\tau]$, and \eqref{eq:reindexing} follows from simple re-indexing.

We now leverage the cyclic property to simplify $(I)$. Re-indexing the outer sum from $t = 2$ to $\tau + 1$:
\begin{align*}
    \sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\prod_{t'=t+1-\tau'}^t Z_{t'} = \sum_{t=2}^{\tau+1}\sum_{\tau'=1}^{\tau-1}\prod_{t'=t-\tau'}^{t-1} Z_{t'} =  \sum_{t=1}^{\tau}\sum_{\tau'=1}^{\tau-1}\prod_{t'=t-\tau'}^{t-1} Z_{t'},
\end{align*}
where the final equality follows from the cyclicity of $Z_t$. Putting this together and adding and subtracting $\tau$:
\begin{align*}
    (II) &= \left(\tau \prod_{t'=1}^\tau Z_{t'}\right) +\left( \sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\prod_{t'=t-\tau'}^{t-1} Z_{t'}\right) + (\tau-\tau) \\
    &= \tau\left(\left(\prod_{t=1}^\tau Z_t\right) - 1\right) +\left( \left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\prod_{t'=t-\tau'}^{t-1} Z_{t'}\right) + \tau\right)
\end{align*}
Finally plugging this into \eqref{eq:reindexing}, we have:
\begin{align*}
    \sum_{t=1}^\tau \beta_tZ_t = \frac{\tau\left(\prod_{t=1}^\tau Z_t - 1\right)}{\left(\sum_{t=1}^\tau \sum_{\tau'=1}^{\tau-1}\sum_{t'=t-\tau}^{t-1}Z_{t'} \right) + \tau } + 1.
\end{align*}
\end{proof}



%We argue that, in general, non-stationary policies can perform strictly better than the optimal static policy. 
\subsection{Proof of \cref{prop:explicit-wage-disc}}

\begin{proof}
The proof is constructive. Consider the {belief-based} policy $\varphi^b$ defined in \cref{alg:bbp}.% and let $\widetilde{\Pi}(\varphi^b)$ denote the average per-period profit under $\varphi^b$.

\begin{algorithm}
\begin{algorithmic}
%\Require Initial number of agents $\widetilde{N}(0)$
\For{$t\in\mathbb{N}^+$}
\If{$\widetilde{N}(t-1) + \lambda_1 + \lambda_2 < D$}
\State Pay all new arrivals $v_1$.
\State Label any agent that stays in the system a type 1 agent; and pay all such agents $v_1$ 
\State for all $t' > t$.
\EndIf
\State Let $t^* = \inf\left\{t\, : \, \widetilde{N}(t-1) + \lambda_1 + \lambda_2 \geq D\right\}$.
% \If{$t = t^*$}
% \State Pay $D - \left(\widetilde{N}(t^*-1)-\lambda_1 + \lambda_2\right)$ agents a reward of 0.
% \EndIf
\If{$t \geq t^*$}
\State Pay $D-\left(\lambda_1 + \lambda_2\right)$ type 1 agents previously identified a reward $v_1$, and everyone else a \State reward of 0.
\EndIf
\EndFor
\end{algorithmic}
	\caption{Belief-based policy $\varphi^b$\label{alg:bbp}}
\end{algorithm}

We first derive the long-run average profit of $\varphi^b$:
\begin{align*}
    \widetilde{\Pi}(\varphi^b) &= \lim_{T\to\infty} \frac1T \sum_{t=1}^T \alpha\min\{\widetilde{N}(t-1)+\lambda_1 + \lambda_2,D\} - \left(\widetilde{N}(t-1)+\lambda_1 + \lambda_2\right)\sum_r rx_r(t) \\ 
    &= \lim_{T\to\infty}\frac1T \sum_{t=1}^{t^*-1} \left[\left(\alpha-\sum_r rx_r(t)\right) \left(\widetilde{N}(t-1)+\lambda_1 + \lambda_2\right) +\sum_{t=t^*}^T\left[\alpha D - v_1(D-(\lambda_1 + \lambda_2))\right]\right] \\
    &= \alpha D - v_1(D-(\lambda_1 + \lambda_2)),
\end{align*}
where the final equality follows from the fact that $t^* \leq 4$.


The following lemma helps to characterize the optimal objective attained by any static policy. 
\begin{lemma}\label{lem:static-keeps-d-on}
Suppose $\alpha > 2v_2$, and let $\widetilde{N}^s$ denote the number of agents induced by the optimal static solution. Then $\widetilde{N}^s = D$.
\end{lemma}

 Thus, it suffices to show that
    $\min\left\{\widehat{r}(\bx^{s_1}),\widehat{r}(\bx^{s_2})\right\}\cdot D - v_1(D-(\lambda_1 + \lambda_2)) = \Omega(D)$,
where $\bx^{s_i}$ denotes the static policy with support $\{0,v_i\}$ that induces $D$ agents in the system, for $i \in \{1,2\}$. Let $x^{s_i}$ denote the weight placed on $v_i$ by $\bx^{s_i}$, for $i \in \{1,2\}$. (As noted in the proof of \cref{lem:static-keeps-d-on}, it is without loss of generality to consider only these two types of distributions.) 


Consider first $\bx^{s_1}$. $x^{s_1}$ satisfies:
    $\frac{\lambda_1}{1-x^{s_1}} + \lambda_2 = D \iff x^{s_1} = 1 - \frac{\lambda_1}{D-\lambda_2}.$
After some algebra, we have $ \widehat{r}(\bx^{s_1})D-v_1(D-(\lambda_1 + \lambda_2)) = v_1D/4$.
% \begin{align*}
%     \widehat{r}(\bx^{s_1})D-v_1(D-(\lambda_1 + \lambda_2)) = v_1D/4% v_1 \left(1-\frac{\lambda_1}{D-\lambda_2}\right)D-v_1(D-(\lambda_1 + \lambda_2)) \\
%   % &= v_1 \left(\frac{D(D-(\lambda_1 + \lambda_2))-(D-\lambda_2)(D-(\lambda_1 + \lambda_2))}{D-\lambda_2}\right) \\
%   % &= v_1 \frac{\lambda_2(D-(\lambda_1 + \lambda_2))}{D-\lambda_2}  =  v_1D/4.
% \end{align*}

Consider now $\bx^{s_2}$. Similarly, we have:
$    \frac{\lambda_1 + \lambda_2}{1-x^{s_2}} = D \iff x^{s_2} = 1-\frac{\lambda_1 + \lambda_2}{D}.$
Then, algebraic manipulations give us $ \widehat{r}(\bx^{s_2})D-v_1(D-(\lambda_1 + \lambda_2)) = (v_2-v_1)D/4.$
% \begin{align*}
%     \widehat{r}(\bx^{s_2})D-v_1(D-(\lambda_1 + \lambda_2)) &= v_2\left(1-\frac{\lambda_1 + \lambda_2}{D}\right)D - v_1(D-(\lambda_1 + \lambda_2)) 
%     = (v_2-v_1)(D-(\lambda_1 + \lambda_2)) \\& = (v_2-v_1)D/4.
% \end{align*}
\end{proof}

 

\subsubsection{Proofs of auxiliary lemmas}\label{apx:aux-lemmas-belief-based}

\begin{proof}[Proof of \cref{lem:static-keeps-d-on}.]
For $i\in \{1,2\}$, let $\varphi^{(i)}$ denote the reward distribution with support $\{0,v_i\}$ that places weight $x_i$ on $v_i$, and $\widetilde{\Pi}(\varphi^{(i)})$ its associated profit. Note that only considering distributions with these two supports is without loss of generality, as $(i)$ we have established that there exists an optimal solution with a support of size 2, and $(ii)$ $\bx$ such that $\supp(\bx) = \{v_1,v_2\}$ induces $\widetilde{N}_1 = \infty$ agents, with only $\alpha D$ revenue.

It suffices to show that, for $i \in \{1,2\}$, $\frac{d}{dx_i}\widetilde{\Pi}(\varphi^{(i)}) > 0$ for all $x_i$ such that $\widetilde{N}^{\varphi^{(i)}} < D$, where $\widetilde{N}^{\varphi^{(i)}}$ is used to denote the number of agents induced by $\varphi^{(i)}$.

For $i \in \{1,2\}$, we have:
\begin{align*}
    \frac{d}{dx_i}\widetilde{\Pi}(\varphi^{(i)}) &= \frac{d}{dx_i} \left[(\alpha - v_ix_i) \widetilde{N}^{\varphi^{(i)}}\right] 
    = -v_i\widetilde{N}^{\varphi^{(i)}} + (\alpha-v_ix_i)\frac{d}{dx_i}\widetilde{N}^{\varphi^{(i)}}.
\end{align*}

Thus,
\begin{align}\label{eq:static-keeps-d-on}
    \frac{d}{dx_i}\widetilde{\Pi}(\varphi^{(i)}) > 0 \iff \alpha > v_i\left (x_i + \frac{\widetilde{N}^{\varphi^{(i)}}}{\frac{d}{dx_i}\widetilde{N}^{\varphi^{(i)}}}\right)
\end{align}


For $i = 1$, we have:
\begin{align*}
    \widetilde{N}^{\varphi^{(1)}} = \frac{\lambda}{1-x_1} + \lambda \implies & \frac{d}{dx_1}\widetilde{N}^{\varphi^{(1)}} = \frac{\lambda}{(1-x_1)^2} 
    \implies  \frac{\widetilde{N}^{\varphi^{(1)}}}{\frac{d}{dx_1}\widetilde{N}^{\varphi^{(1)}}} = (1-x_1) + (1-x_1)^2
\end{align*}

For $i = 2$:
\begin{align*}
    \widetilde{N}^{\varphi^{(2)}} = \frac{2\lambda}{1-x_2} \implies & \frac{d}{dx_2}\widetilde{N}^{\varphi^{(2)}} = \frac{2\lambda}{(1-x_2)^2} 
    \implies  \frac{\widetilde{N}^{\varphi^{(2)}}}{\frac{d}{dx_2}\widetilde{N}^{\varphi^{(2)}}} = 1-x_2
\end{align*}

Noting that $\alpha > 2v_2$ satisfies \eqref{eq:static-keeps-d-on} for all $x_i$, $i \in \{1,2\}$, we obtain the lemma.
\end{proof}



\subsection{Proof of \cref{ex:steady-state-cyclic}}\label{apx:cyclic-diff-dist}



%We now prove \cref{ex:steady-state-cyclic}.
\begin{proof}
Applying \cref{lem:cyclic-steady-state-equations}, under $\varphi^c$, via some algebra we have that for all $t$ odd:
{
\begin{align}\label{eq:steadystate-cyclic1}
    &\widetilde{N}_1(t) = 1.9\lambda,
    \widetilde{N}_2(t) = \lambda 
    \implies  \widetilde{N}(t) = 2.9\lambda.
\end{align}

Similarly, for all $t$ even we have:
\begin{align}\label{eq:steadystate-cyclic2}
    \widetilde{N}_1(t) = 2\lambda,
    \widetilde{N}_2(t) = 1.5\lambda
    \implies \widetilde{N}(t) = 3.5\lambda.
\end{align}
}

% \begin{align}\label{eq:steadystate-cyclic}
%     &\widetilde{N}_1(t) = 19, \quad \widetilde{N}_2(t) = 10, \implies  \widetilde{N}(t) = 29 \quad \, \forall \, t \text{ odd} \\
%     \text{and }\quad &\widetilde{N}_1(t) = 20, \quad 
%     \widetilde{N}_2(t) = 15 \implies \widetilde{N}(t) = 35 \quad \, \forall \, t \text{ even}. \notag
% \end{align}


We leverage \eqref{eq:steadystate-cyclic1} and \eqref{eq:steadystate-cyclic2} for the following two results. In particular, \cref{lem:exists-rev-counter} characterizes the optimal static solution. \cref{lem:cyclic-beats-zero-counter} establishes that the cyclic policy outperforms the static solution.
 %respectively state that the optimal static policy $\varphi^s$ pays out 0 reward in each time period, and that, for $\alpha \geq 0.7r$, $\varphi^c$ achieves strictly higher profit than the zero-reward policy.
\begin{lemma}\label{lem:exists-rev-counter}
For $\alpha < r$, the optimal static solution $\bx^s$ is such that $x^s = 0$, where $x^s$ is the weight placed on reward $r > 0$.
\end{lemma}

\begin{lemma}\label{lem:cyclic-beats-zero-counter}
For $\alpha \geq 0.7r$, $\widetilde{\Pi}(\varphi^c) - \widetilde{\Pi}(\varphi^0)  = \Omega(\lambda)$, where $\varphi^0$ denotes the policy that deterministically pays out zero reward in each period.
\end{lemma}

 Putting these two lemmas together proves the result.
 \end{proof}
 
 



\subsubsection{Proofs of auxiliary lemmas}\label{apx:cyclic-aux-proofs}


\begin{proof}[Proof of \cref{lem:exists-rev-counter}.]
We abuse notation and let $\widetilde{N}(\bx)$ and $\widetilde{\Pi}(\bx)$ respectively denote the number of agents and profit induced by static solution $\bx$ that places weight $x$ on $r$. It suffices to show that $\frac{d}{dx}\widetilde{\Pi}(\bx) < 0$ for all $x \in (0,1)$. We have:
\begin{align}
    \frac{d}{dx}\widetilde{\Pi}(\bx) &= -r\widetilde{N}(\bx) + (\alpha-rx)\frac{d}{dx}\widetilde{N}(\bx)
    < -r\widetilde{N}(\bx) + r(1-x)\frac{d}{dx}\widetilde{N}(\bx) \label{eq:lemc4-step1}
    \end{align}
where \eqref{eq:lemc4-step1} follows from the assumption that $\alpha < r$ and $\frac{d}{dx}\widetilde{N}(\bx) > 0$. 

Consider now our specific instance. We have:
\begin{align*}
    \widetilde{N}(\bx) &= \frac{0.1\lambda}{0 \cdot x + 0.1 (1-x)} + \frac{\lambda}{0.5x + 1-x} = \lambda \left(\frac{1}{1-x} + \frac{1}{1-0.5x}\right) \\
    \implies \frac{d}{dx}\widetilde{N}(\bx) &= \lambda\left(\frac{1}{(1-x)^2} + \frac{0.5}{(1-0.5x)^2}\right).
\end{align*}

Plugging this into \eqref{eq:lemc4-step1}, we obtain:
\begin{align}
    \frac{d}{dx}\widetilde{\Pi}(\bx) &< \lambda r \left[-\left(\frac{1}{1-x} + \frac{1}{1-0.5x}\right) + (1-x)\left(\frac{1}{(1-x)^2} + \frac{0.5}{(1-0.5x)^2}\right)\right] \notag \\
    &= \lambda r\left[\frac{-\left(1-0.5x\right) + 0.5(1-x)}{(1-0.5x)^2}\right] 
    < 0 \quad \forall \, x \in (0,1). \notag
\end{align}
\end{proof}



\begin{proof}[Proof of \cref{lem:cyclic-beats-zero-counter}.]
We have:
\begin{align*}
    \widetilde{\Pi}(\varphi^c) - \widetilde{\Pi}(\varphi^0) &= \left(\alpha\left(\frac12\widetilde{N}(1) + \frac12\widetilde{N}(2)\right) - \frac12 r \widetilde{N}(1) \right) - \alpha(\lambda_1+\lambda_2) \\& \geq 0.7r\left(3.2\lambda-1.1\lambda\right) - \frac12r \cdot 2.9\lambda 
    = 0.02r\lambda,
\end{align*}
where the inequality follows from plugging in the expressions for $\widetilde{N}(t), t \in \{1,2\}$, as well as $\alpha \geq 0.7r$.
\end{proof}



\section{Omitted Proofs: \cref{sec:asymptotic-opt}}\label{apx:structure-of-fluid}

\subsection{Proof of Proposition~\ref{prop:fluid-non-cvx}}

\begin{proof}
{Suppose} $\rev(\fluidn) = \fluidn$, $\rewardset = \{r, 0\}$ for some $r > 0$, and $K = 1$. Moreover, let $\ell_1(0) = 1$. For a given distribution $\bx$, let $x$ denote the weight placed on $r$. We abuse notation and let $\widehat{\Pi}(\bx)$ denote the profit induced by $\bx$ in the deterministic relaxation. Then, the objective of~\ref{eq:fluid-opt} evaluates to:
\begin{align*}
&\widehat{\Pi}(\bx) = \lambda_1 \, \frac{1-rx}{1 - (1-\ell_1(r))x} \\ \implies &\frac{\partial^2}{\partial x^2}\widehat{\Pi}(\bx) = 2\lambda_1 \, \frac{(1-\ell_1(r))(1-\ell_1(r)-r)}{(1 - (1-\ell_1(r))x)^3} > 0 \  \forall \ r < 1-\ell_1(r).
\end{align*}
\end{proof}

\subsection{Proof of \cref{thm:main-theorem}}

{Consider the following {\it budgeted supply maximization} problem, which we term~\ref{eq:supply-opt}. {For a fixed set of types, an instance of~\ref{eq:supply-opt} is defined by a budget $B \in \mathbb{R}^+$ as follows:}}
\begin{align}\label{eq:supply-opt}
    \max_{{\xvec \in \simplex^{|\rewardset|}, \mathbf{\fluidn} \in \mathbb{N}^K}} \quad & \sum_i \fluidn_i \tag{\supplyopt} \\
    \text{subject to} \quad & \left(\sum_r rx_r\right)\left(\sum_i \fluidn_i\right) \leq B \notag\\
    & \fluidn_i=\frac{\lambda_i}{\sum_r \ell_i(r) x_r}\notag
\end{align}

{Before proceeding with the proof, we note that the introduction of~\ref{eq:supply-opt} is a {\it conceptual} simplification, rather than a computational one. In particular, though the objective is now linear in $\mathbf{\fluidn}$, the budget constraint remains non-convex.}

{Lemma~\ref{prop:profit-reduction} formalizes the idea that~\ref{eq:supply-opt} is a useful proxy via which one can characterize the fluid optimum of~\ref{eq:fluid-opt}. Namely, it establishes that that the optimal solution to \ref{eq:fluid-opt} inherits the same structure as that of~\ref{eq:supply-opt}. We defer its proof -- as well as the proofs of all subsequent lemmas -- to Appendix \ref{apx:main-thm-aux-lemmas}.}
\begin{lemma}\label{prop:profit-reduction}
Suppose there exists $n \in \mathbb{N}^+$ such that, for all instances of~\ref{eq:supply-opt}, there exists an optimal solution $\supplyoptsol$ such that $|\supp(\nf{\supplyoptsol})| \leq n$. Then, there exists an optimal solution $\nf{\bx}^\star$ to~\ref{eq:fluid-opt} such that $|\supp(\nf{\bx}^\star)| \leq n$. 
\end{lemma}

{Thus, to prove the theorem, it suffices to show {that for all instances of~\ref{eq:supply-opt}} there exists {an optimal solution $\supplyoptsol$ with the desired structure, i.e. $|\supp(\nf{\supplyoptsol)}| \leq 2$.}}

Let $\supplyoptsol$ denote an optimal solution to~\ref{eq:supply-opt}. {Based on the second constraint}, we define $\fluidn_i(\bx) = \frac{\lambda_i}{\sum_r \ell_i(r)x_r}$, and $\fluidn(\bx) = \sum_i \fluidn_i(\bx)$. {Further}, $\avgreward(\bx) = \sum_r rx_r$ is used to denote {an agent's} expected reward under $\bx$, and $\avgloss_i(\bx) = \sum_r \ell_i(r)x_r$ the expected departure probability of a type $i$ agent under~$\bx$. {We moreover assume that $B$ is such that $\rmin\fluidn(\unitvec_{\rmin}) < B$ (i.e., the budget is large enough to serve all agents with the minimum possible reward).}

Finally, we introduce the concept of \emph{interlacing rewards}. {Intuitively, two rewards $r<r'$ {interlace} the budget $B$ if a solution $\xvec=\unitvec_r$ spends at most $B$, while $\xvec'=\unitvec_{r'}$ spends more than $B$.}



\begin{definition}[Interlacing rewards]
Consider an instance of~\ref{eq:supply-opt}. Rewards $r$ and $r'$, $r < r'$, are said to \emph{interlace} budget $B$ if the following holds:
$ {r \fluidn(\unitvec_{r}) \leq B < r'\fluidn(\unitvec_{r'}). }$
We also say that three rewards $r, r'$ and $r''$ interlace if there exist two pairs of interlacing rewards amongst $r, r'$ and $r''$.
\end{definition}



{We present a simple, but useful, lemma upon which we repeatedly rely to prove Theorem~\ref{thm:main-theorem}. We leave the easy proof of this fact to the reader.}

\begin{lemma}\label{lem:obvious-fact}
Suppose distribution $\by$ is such that $\supp(\by) = \left\{r, r'\right\}$ for some $r > r' \in \rewardset$, with weights $y$ on $r$ and $1-y$ on $r'$, $y \in (0,1)$. Then, $\avgreward(\by)$ and $\fluidn(\by)$ are {both} non-decreasing in $y$. 
\end{lemma}

With these two lemmas in hand, we prove the theorem.

\begin{proof}[Proof of \cref{thm:main-theorem}.]
We first consider the trivial case in which $\rmax\fluidn(\unitvec_{\rmax}) \leq B$. In this case, {the optimal solution for \ref{eq:supply-opt} is} to give out the maximum reward with probability~1 (i.e., $\bx = \unitvec_{\rmax}$). %and obtain the maximum number of agents. 
Thus, in the remainder of the proof, we assume $\rmax\fluidn(\unitvec_{\rmax}) > B$.

Any optimal solution $\supplyoptsol$ to \ref{eq:supply-opt} satisfies the budget constraint with equality (else, we can strictly improve the solution by moving weight from a low reward to a high reward, contradicting optimality of $\supplyoptsol$). As a result, we can instead consider the following equivalent {\it budgeted reward minimization} problem:
\begin{align}
    \min_{{\xvec \in \simplex^{|\rewardset|}}} \qquad &\sum_r rx_r \tag{\textsc{Reward-OPT}} \label{eq:reward-opt} \\
    \text{subject to} \qquad &\left(\sum_r r x_r\right)\left(\sum_i \fluidn_i\right) {=} B \label{eq:budget-constraint}\\
    & \fluidn_i=\frac{\lambda_i}{\sum_r \ell_i(r) x_r}\notag 
\end{align}

We abuse notation and let $\supplyoptsol$ denote an optimal solution to~\ref{eq:reward-opt}, and suppose $|\supp(\supplyoptsol)| > 2$. Algorithm~\ref{alg:support-reduction} exhibits a procedure which either contradicts optimality of $\supplyoptsol$, or iteratively reduces the size of the support of $\supplyoptsol$, all the while maintaining feasibility  -- i.e., satisfying~\eqref{eq:budget-constraint} -- and never degrading the quality of the solution. Thus, {we only need to show} % it suffices to show 
that Algorithm~\ref{alg:support-reduction} maintains these two invariants and successfully terminates.


\begin{algorithm}
\begin{algorithmic}
\Require {optimal solution $\supplyoptsol$ to~\ref{eq:reward-opt} such that $|\supp(\supplyoptsol)| > 2$}
\Ensure {optimal solution $\algxvec$ such that $|\supp(\algxvec)| = 2$, or that $\supplyoptsol$ is not optimal, a contradiction.}
\State $\algxvec \gets \supplyoptsol$
\While{$|\supp(\algxvec)| > 2$}
\State Choose a set $\mathcal{R} = \{r_1, r_2, r_3\} \subseteq \supp(\algxvec)$ of interlacing rewards.
\State 
Construct a distribution $\by$ such that $\supp(\by) = \supp(\algxvec) \setminus \{r^\star\}, r^\star\in \mathcal{R}$, such that
\State $(i)$ $\avgreward(\by) = \avgreward(\algxvec)$ and $(ii)$ $\fluidn(\by) \geq \fluidn(\algxvec)$.
\If{$\fluidn(\by) > \fluidn(\algxvec)$}
\State Return that $\supplyoptsol$ is not optimal for~\ref{eq:reward-opt}, a contradiction.
% \State Continuously move weight from $\max\{r_j, r_k\}$ to $\min\{r_j, r_k\}$, $j,k \neq j^\star$ until
% \State $\left(\sum_r r y_r\right)\left(\sum_i \lambda_i \left[\frac{1}{\sum_r \ell_i(r)y_r} \right]\right) = B$.
\EndIf
\State $\algxvec \gets \by$
\EndWhile
\end{algorithmic}
	\caption{Support-reduction procedure}\label{alg:support-reduction}
\end{algorithm}

{As stated, the first step of each iteration of the algorithm implicitly relies on the existence of three interlacing rewards. It is, however, \emph{a priori} unclear that such a set necessarily exists. Lemma~\ref{lem:opt-uses-interlacing-rewards} establishes that, as long as the incumbent solution $\algxvec$ is feasible, existence of three interlacing rewards is{,} in fact{,} guaranteed.}
\begin{lemma}\label{lem:opt-uses-interlacing-rewards}
Suppose $\algxvec$ is feasible and $|\supp(\algxvec)| > 2$. Then, there exists a set $\mathcal{R} = \{r_1,r_2,r_3\}\subseteq\supp(\algxvec)$ of interlacing rewards.
\end{lemma}

{{Next, Lemma~\ref{lem:main-lemma}} establishes that, given an incumbent feasible solution $\algxvec$, it is {\it always} possible to (weakly) increase the fluid number of agents by removing a reward from the support of $\algxvec$, all the while holding the expected reward fixed. Moreover, one can do this such that at least two interlacing rewards remain in the support of the newly constructed distribution.}

\begin{lemma}\label{lem:main-lemma}
Consider a set $\mathcal{R} = \{r_1, r_2, r_3\} \subseteq \supp(\algxvec)$ with $r_1 > r_2 > r_3$ interlacing. Then, there exists a distribution $\by$ and $r^\star \in \mathcal{R}$ such that $(i)$ $\supp(\by) = \supp(\algxvec) \setminus \{r^\star\}$, $(ii)$ $\avgreward(\by) = \avgreward(\algxvec)$, $(iii)$ $\fluidn(\by) \geq \fluidn(\algxvec)$, and $(iv)$ $p, q \in \mathcal{R}\setminus\{r^\star\}$ are interlacing.
\end{lemma}

{Finally,} {Lemma~\ref{lem:support-reduction-maintains-invariants}} shows that either this new supply-improving solution $\by$ is optimal, or it implies the existence of a feasible solution which {\it strictly} improves upon $\supplyoptsol$, contradicting the assumption that $\supplyoptsol$ was optimal in the first place.
\begin{lemma}\label{lem:support-reduction-maintains-invariants}
At the end of each iteration, either $\algxvec$ is optimal, or the algorithm has correctly returned that $\supplyoptsol$ was not optimal to begin with.
\end{lemma}

Lemmas~\ref{lem:opt-uses-interlacing-rewards},~\ref{lem:main-lemma}, and~\ref{lem:support-reduction-maintains-invariants} together establish that, in each iteration of the procedure, both invariants are maintained. {It remains to show that Algorithm~\ref{alg:support-reduction} terminates. Suppose first that the algorithm returns that~$\supplyoptsol$ was not optimal to begin with. In this case, the algorithm clearly terminates. In the other case, termination follows since in each iteration of the algorithm, $|\supp(\algxvec)|$ strictly decreases.}
\end{proof}

\subsubsection{Proofs of auxiliary lemmas}\label{apx:main-thm-aux-lemmas}

% \subsubsection{Proof of Lemma~\ref{prop:profit-reduction}}\label{apx:proof-of-profit-red}

\begin{proof}[Proof of Lemma~\ref{prop:profit-reduction}.]
Consider an optimal solution $\bx^\star$ to~\ref{eq:fluid-opt}, and suppose $|\supp(\bx^\star)| > n$. We define $C\left(\fluidn(\bx^\star)\right)$ to be the minimum cost of securing a supply of $\fluidn(\bx^\star)$ agents in the system. Formally, 
$C\left(\fluidn(\bx^\star)\right) = \min_{\bx: \fluidn(\bx) = \fluidn(\bx^\star)} \fluidn(\bx)\avgreward(\bx).$
We moreover let $\widehat{\Pi}(\bx) = \rev(\fluidn(\bx)) - C\left(\fluidn(\bx)\right).$

Consider now an instance of~\ref{eq:supply-opt} with budget $B = C\left(\fluidn(\bx^\star)\right)$. Let $\supplyoptsol$ denote an optimal solution to this instance of~\ref{eq:supply-opt}, with $|\supp(\supplyoptsol)| \leq n$, and let $\fluidn(\supplyoptsol)$ denote the number of agents induced by $\supplyoptsol$. %the optimal solution to this instance of~\ref{eq:supply-opt}, where $|\supp(\supplyoptsol)| \leq n$. 
Clearly, $\bx^\star$ is feasible to this instance, and thus $\fluidn(\supplyoptsol)\geq\fluidn(\bx^\star)$. Since $\rev$ is increasing, we have $\rev(\fluidn(\supplyoptsol)) \geq \rev(\fluidn(\bx^\star))$. Thus,
\begin{align*}
    \widehat{\Pi}(\supplyoptsol) = \rev(\fluidn(\supplyoptsol)) - C\left(\fluidn(\supplyoptsol)\right) \geq \rev(\fluidn(\bx^\star))-C\left(\fluidn(\supplyoptsol)\right) \geq \rev(\fluidn(\bx^\star))-C\left(\fluidn(\bx^\star)\right)\\ = \widehat{\Pi}(\bx^\star)
\end{align*}
where the final inequality follows from the fact that $C\left(\fluidn(\supplyoptsol)\right) \leq B = C\left(\fluidn(\bx^\star)\right)$, by construction. Thus, $\supplyoptsol$ is also optimal for~\ref{eq:fluid-opt}. Since $|\supp(\supplyoptsol)| \leq n$ by assumption, the result then follows.
\end{proof}





% \subsubsection{Proof of \cref{lem:opt-uses-interlacing-rewards}}

\begin{proof}[Proof of \cref{lem:opt-uses-interlacing-rewards}.]
By feasiblity of $\algxvec$ and the assumption that $|\supp(\algxvec)| > 2$, there exists $r \in \supp(\algxvec)$ such that $r\fluidn(\unitvec_{r}) < B$ (else the budget has been exceeded).  By the same argument, there exists $r \in \supp(\algxvec)$ such that $r\fluidn(\unitvec_r) > B$ (else the total reward paid out is {\it strictly} less than the budget, contradicting feasibility to~\ref{eq:reward-opt}). Let $r_1 = \inf\left\{r \in \supp(\algxvec) \big{|} r\fluidn(\unitvec_r) > B \right\}$, and $r_2 = \sup\left\{r \in \supp(\algxvec) \big{|} r\fluidn(\unitvec_r) < B \right\}$. Then, by definition, $(r_1, r_2)$ is an interlacing pair.

Moreover, since $|\supp(\algxvec)| > 2$, there necessarily exists $r_3$ such that, either $r_3\fluidn(\unitvec_{r_3}) \leq B$, or $r_3\fluidn(\unitvec_{r_3})> B$, as argued above. In the former case $(r_1,r_3)$ form an interlacing pair; in the latter case, $(r_3, r_2)$ form an interlacing pair. Having established that $(r_1,r_2)$ is an interlacing pair, in either case we obtain at least 2 interlacing pairs.
\end{proof}



% \subsubsection{Proof of Lemma~\ref{lem:main-lemma}}

\begin{proof}[Proof of Lemma~\ref{lem:main-lemma}.]
The proof is constructive. Fix ${r}^\star \in \mathcal{R}$, and let $\by(p,q)$ be the distribution that distributes the weight placed on $r^\star$ by $\algxvec$ to $p, q \in \mathcal{R} \setminus \{r^\star\}$, $p\neq q$, all the while holding the expected reward constant. Formally, $\by(p,q)$ satisfies:
    (1) $y_{r} = \widetilde{x}_{r}^S$ for $r \not\in \mathcal{R}$.
    (2) $y_{r^\star} = 0$,
    (3) $y_{q} = \sum_{r\in\mathcal{R}}\algx_r - y_{p}$ for $p, q \in \mathcal{R}\setminus \{r^\star\}$, and
    (4) $\avgreward(\by(p,q)) = \avgreward(\algxvec)$.
    
    
Recall, $\mathcal{R} = \{r_1, r_2, r_3\} \subseteq \supp(\algxvec)$, with $r_1 > r_2 > r_3$ interlacing. We claim that one of the following two inequalities necessarily holds:
\begin{align}\label{ineq:main-lemma-2}
\fluidn\left(\by(r_1,r_3)\right) \geq \fluidn(\algxvec) \quad \text{ or } \quad \fluidn\left(\by(r_2,r_3)\right) \geq \fluidn(\algxvec),
% \fluidn\left(\by(r_2)\right) \geq \fluidn(\algxvec) \quad \text{ or } \quad \fluidn\left(\by(r_1)\right) \geq \fluidn(\algxvec),
\end{align}
{\it and} one of the following two inequalities necessarily holds:
\begin{align}\label{ineq:main-lemma-1}
\fluidn\left(\by(r_1,r_2)\right) \geq \fluidn(\algxvec) \quad \text{ or } \quad \fluidn\left(\by(r_1,r_3)\right) \geq \fluidn(\algxvec).
\end{align}
Suppose the claim is true. Then, the proof of the lemma is complete. To see that we have shown existence of $\by$ such that $p, q \in \mathcal{R}\setminus\{r^\star\}$ in the support of $\by$ are interlacing (i.e., Condition $(iv)$ of the lemma holds), we consider the following cases:
\begin{enumerate}
    \item $(r_1, r_3)$ and $(r_2, r_3)$ are the interlacing pairs:~\eqref{ineq:main-lemma-2} covers this case, as $\{r_1, r_3\}\subseteq \supp(\by(r_1,r_3))$, and $\{r_2, r_3\}\subseteq \supp(\by(r_2,r_3))$.
    \item $(r_1, r_3)$ and $(r_1, r_2)$ are the interlacing pairs:~\eqref{ineq:main-lemma-1} covers this case, as $\{r_1, r_2\} \subseteq \supp(\by(r_1,r_2))$, and $\{r_1, r_3\} \subseteq \supp(\by(r_1,r_3))$.
\end{enumerate}

We only show the proof of the first set of alternatives~\eqref{ineq:main-lemma-2}. The proof of~\eqref{ineq:main-lemma-1} is entirely analogous. With slight abuse of notation, we let $y_j = y_{r_j}, \algx_j = \algx_{r_j}$ for $r_j \in |\rewardset|$.  

Consider first distribution $\by(r_1,r_3)$. Solving $(iii)$ and $(iv)$ for $y_1$ and $y_3$, we obtain:
\begin{align*}
y_{1} = \algx_1 + \frac{r_2-r_3}{r_1-r_3}\algx_2 \quad , \quad y_3 = \algx_3 + \frac{r_1-r_2}{r_1-r_3} \algx_2.
\end{align*}

Since $\fluidn_i(\bx)$ is the composition of convex function $f(z) = \frac1z$ with affine mapping $g(x) = \sum_{r\in\rewardset} \ell_i(r)x_r$, we have that $\fluidn_i(\bx)$ is convex. Thus:
\begin{align*}
\fluidn_i(\by(r_1,r_3)) &\geq \fluidn_i(\algxvec) + \nabla \fluidn_i(\algxvec)^T(\by(r_1,r_3) - \algxvec) \\
&= \fluidn_i(\algxvec) - \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\begin{pmatrix}
\ell_{i}(r_1)\\
\ell_{i}(r_2) \\
\ell_i(r_3)
\end{pmatrix}^T	\begin{pmatrix}
\algx_1 + \frac{r_2-r_3}{r_1-r_3}\algx_2 - \algx_1\\
0-\algx_2 \\
\algx_3 + \frac{r_1-r_2}{r_1-r_3} \algx_2 - \algx_3
\end{pmatrix} \\
&= \fluidn_i(\algxvec) - \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\algx_2\left[\frac{r_2-r_3}{r_1-r_3}\ell_{i}(r_1)- \ell_{i}(r_2) + \frac{r_1-r_2}{r_1-r_3}\ell_{i}(r_3)\right].
\end{align*}


Summing over all $i$, we obtain that the following condition suffices for $\fluidn(\by(r_1,r_3)) \geq \fluidn(\algxvec)$ to hold:
\begin{align}\label{inequality-3}
\sum_i \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\left[\frac{r_2-r_3}{r_1-r_3}\ell_{i}(r_1)- \ell_{i}(r_2) + \frac{r_1-r_2}{r_1-r_3}\ell_{i}(r_3)\right] \leq 0.
\end{align}

Suppose~\eqref{inequality-3} fails to hold, i.e. $\sum_i \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\left[\frac{r_2-r_3}{r_1-r_3}\ell_{i}(r_1)- \ell_{i}(r_2) + \frac{r_1-r_2}{r_1-r_3}\ell_{i}(r_3)\right] > 0$. Consider now distribution $\by(r_2,r_3)$. Solving $(iii)$ and $(iv)$ for $y_{2}$ and $y_3$ we obtain:
$$y_2 = \algx_2 + \frac{r_1-r_3}{r_2-r_3}\algx_1 \quad , \quad y_3 = \algx_3 + \frac{r_2-r_1}{r_2-r_3}\algx_1.$$

Again, by convexity of $\fluidn(\bx)$:
\begin{align*}
\fluidn_i(\by(r_2,r_3)) &\geq \fluidn_i(\algxvec) + \nabla \fluidn_i(\algxvec)^T(\by(r_2,r_3) - \algxvec) \\
&= \fluidn_i(\algxvec) - \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\begin{pmatrix}
\ell_{i}(r_1)\\
\ell_{i}(r_2) \\
\ell_i(r_3)
\end{pmatrix}^T	\begin{pmatrix}
0-\algx_1\\
\algx_2 + \frac{r_1-r_3}{r_2-r_3}\algx_1 - \algx_2 \\
x_3^2 + \frac{r_2-r_1}{r_2-r_3}\algx_1 - x_3^2
\end{pmatrix} \\
&= \fluidn_i(\algxvec) - \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\algx_1\left[-\ell_{i}(r_1) + \frac{r_1-r_3}{r_2-r_3}\ell_{i}(r_2) + \frac{r_2-r_1}{r_2-r_3}\ell_i(r_3)\right].
\end{align*}

Summing over all $i$, the following condition suffices for $\fluidn(\by(r_2,r_3)) \geq \fluidn(\algxvec)$ to hold:
\begin{align*}
\frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\algx_1\left[-\ell_{i}(r_1) + \frac{r_1-r_3}{r_2-r_3}\ell_{i}(r_2) + \frac{r_2-r_1}{r_2-r_3}\ell_i(r_3)\right] \leq 0 \\
% &\iff \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\algx_1\left[-\frac{r_2-r_3}{r_1-r_3}\ell_{i}(r_1) + \ell_{i}(r_2) + \frac{r_2-r_1}{r_1-r_3}\ell_i(r_3)\right] \leq 0 \\
\iff \frac{\lambda_i}{\hat{\ell}_i(\algxvec)^2}\left[\frac{r_2-r_3}{r_1-r_3}\ell_{i}(r_1) -\ell_{i}(r_2) + \frac{r_1-r_2}{r_1-r_3}\ell_i(r_3)\right] \geq 0
\end{align*}
which holds by assumption.
\end{proof}


% \subsubsection{Proof of Lemma~\ref{lem:support-reduction-maintains-invariants}}


\begin{proof}[Proof of Lemma~\ref{lem:support-reduction-maintains-invariants}.]
By Lemma~\ref{lem:main-lemma}, there exists $\by$ with strictly smaller support than $\algxvec$ such that $\avgreward(\by) = \avgreward(\algxvec)$ and $\fluidn(\by) \geq \fluidn(\algxvec)$. 

Suppose first that $\fluidn(\by) = \fluidn(\algxvec)$. Then feasibility is clearly maintained, and $\by$ is also optimal. Suppose instead that $\fluidn(\by) > \fluidn(\algxvec)$. In this case, $\by$ is {\it not} feasible, as 
$\avgreward(\by)\fluidn(\by) > \avgreward(\algxvec)\fluidn(\algxvec) = B.$
% $$\left(\sum_r r y_r\right)\left(\sum_i \lambda_i \left[\frac{1}{\sum_r \ell_i(r)y_r} \right]\right) > \left(\sum_r r \algx_r\right)\left(\sum_i \lambda_i \left[\frac{1}{\sum_r \ell_i(r)\algx_r} \right]\right) = B.$$ 
We claim {we} %that one 
can construct a {feasible} distribution $\bz$ such that $(i)$ $\supp(\bz) = \supp(\by)$, $(ii)$ $\avgreward(\bz) < \avgreward(\by)$, and $(iii)$ $\avgreward(\bz)\fluidn(\bz) = B$. Since $\avgreward(\by) = \avgreward(\algxvec)$ by construction, the existence of such a distribution $\bz$ would contradict optimality of $\algxvec$, and consequently that of $\supplyoptsol$. 

To complete the proof, we detail the construction of such a distribution $\bz$. Take interlacing rewards $r_{\max}(\by) = \sup \{r | r \in \supp(\by)\}$, $r_{\min}(\by) = \inf\{r | r \in \supp(\by)\}$ (of which Lemma~\ref{lem:main-lemma} guarantees the existence), and continuously move weight from $r_{\max}(\by)$ to $r_{\min}(\by)$, holding all else fixed. This procedure strictly decreases $\avgreward(\by)$ and weakly decreases $\fluidn(\by)$, by Lemma~\ref{lem:obvious-fact}, and thus strictly decreases $\avgreward(\by)\fluidn(\by)$. Do this until one of two events occurs:
\begin{enumerate}
\item Constraint \eqref{eq:budget-constraint} is satisfied: In this case, we have found $\bz$.
\item The weight on $r_{\max}(\by)$ has been exhausted. Let $\widetilde{\by}$ denote the distribution at the point at which $r_{\max}(\by)$ has been exhausted. For this new distribution $\widetilde{\by}$, define $r_{\max}(\widetilde{\by}) = \sup\{r | r \in \supp(\widetilde{\by})\}$, and repeat the process.
\end{enumerate}
To see that this procedure must terminate and return a feasible solution $\bz$, in the worst case we are left with but one $r$ such that $r\fluidn(\unitvec_r) > B$. Let $\widetilde{y}$ be the weight placed on this reward. Since the left-hand side of~\eqref{eq:budget-constraint} is continuously increasing in $\widetilde{y}$ (again, by Lemma~\ref{lem:obvious-fact}), and at $\widetilde{y} = 0$, $\hat{r}(\widetilde{\by})\fluidn(\widetilde{\by}) \leq B$ (since $\rmin\fluidn(\unitvec_{\rmin}) \leq B$), there must exist $\widetilde{y}$ such that~\eqref{eq:budget-constraint} is satisfied. 
\end{proof}


% \section{Asymptotic optimality of the fluid heuristic: Proofs}\label{app:fluid-results}

{\subsection{Proof of Proposition~\ref{prop:fluid-ub-for-all-theta}}

Given policy $\varphi$ and $\theta \in \mathbb{N}^+$, let $\randomstatevector^\theta:=\randomstatevector^\theta(\infty)$ denote the steady-state number of agents in the system of each type, and let $\pi^\theta(\varphi)$ be the associated steady-state distribution, if it exists. We moreover define $N^\theta = \sum_{i\in[K]} N^\theta_i$. The upper bound follows almost immediately from the following lemma.


\begin{lemma}\label{prop:limiting-dist}
For any static policy $\varphi$ defined by reward distribution $\bx$, a unique limiting distribution exists. Thus, $v_\theta(\varphi) = \EE\left[\rev\left(\frac{\scaledrandomstate}{\theta}\right) - \frac{\scaledrandomstate}{\theta}\left(\sum_r r x_r \right)\right].$ Moreover,
$$\scaledrandomstate \sim\normalfont{Pois}\left(\sum_i\frac{\theta\lambda_i}{\sum_r \ell_i(r)x_r}\right).$$
\end{lemma}

\begin{proof}
Since the Markov chain governing $\scaledrandomstate_i(t)$ is irreducible and aperiodic, if a stationary distribution exists, it is the unique limiting distribution. Moreover, by Poisson thinning, $\scaledrandomstate_i$ is Poisson-distributed.
Since the Poisson distribution is uniquely defined by its mean, a stationary distribution exists if and only if the expected number of agents entering the system in each period is equal to the expected number of agents departing from the system, or equivalently 
\begin{align*}
\EE\left[\scaledrandomstate_i\right] = \EE\left[\scaledrandomstate_i\right]\left(1-\sum_r \ell_i(r)x_r\right) + \theta\lambda_i
\implies \EE\left[\scaledrandomstate_i\right] = \frac{\theta\lambda_i}{\sum_r \ell_i(r)x_r} < \infty
\end{align*}
since $\ell_i(r) > 0$ for all $r \in \rewardset$.
Thus, we obtain $\scaledrandomstate\sim\text{Pois}(\sum_i\frac{\theta\lambda_i}{\sum_r \ell_i(r) x_r})$. 

To prove that $v_\theta(\varphi) = \EE\left[\rev\left(\frac{\scaledrandomstate}{\theta}\right) - \frac{\scaledrandomstate}{\theta}\left(\sum_r r x_r \right)\right]$, it suffices to show that $\widehat{\Pi}(\scaledrandomstate) := \rev\left(\frac{\scaledrandomstate}{\theta}\right) - \frac{\scaledrandomstate}{\theta}\left(\sum_r r x_r \right)$ is integrable with respect to $\pi^\theta(\varphi)$. Let $\pi^\theta_n = \PP(\scaledrandomstate = n).$ We have:
\begin{align}\label{eq:integrable-step1}
    \sum_{n=0}^{\infty} \bigg{|} \rev\left(\frac{n}{\theta}\right) - \frac{n}{\theta}\left(\sum_r rx_r\right)\bigg{|} \, \pi^\theta_n < \sum_{n=0}^{\infty} \rev\left(\frac{n}{\theta}\right)\pi^\theta_n + \frac1\theta\left(\sum_r rx_r\right)\sum_{n=0}^{\infty} n \pi^\theta_n
\end{align}
Let $c \geq 0$ by any constant such that $\rev'(c)$ exists. By concavity of $\rev$, $\rev\left(\frac{n}{\theta}\right) \leq \rev(c) + \rev'(c)(\frac{n}{\theta}-c)$. Thus, 
\begin{align*}
    \sum_{n=0}^{\infty} \rev\left(\frac{n}{\theta}\right)\pi^\theta_n \leq  \sum_{n=0}^{\infty} \left(\rev(c) + \rev'(c)\left(\frac{n}{\theta}-c\right)\right)\pi^\theta_n = \rev(c)-cR'(c) + \frac1\theta\rev'(c)\sum_{n=0}^{\infty} n\pi^\theta_n
\end{align*}
Combining this with~\eqref{eq:integrable-step1}, we obtain:
\begin{align*}
    \sum_{n=0}^{\infty} \bigg{|} \rev(n) - n\left(\sum_r rx_r\right)\bigg{|} \, \pi^\theta_n &< \rev(c) - cR'(c) + \frac1\theta\left(\rev'(c) + \sum_r rx_r\right) \sum_{n=0}^{\infty}n\pi^\theta_n \\
    & = \rev(c) - cR'(c) + \frac1\theta\left(\rev'(c) + \sum_r rx_r\right)\EE\left[\scaledrandomstate\right] \\
    &= \rev(c) - cR'(c) + \left(\rev'(c) + \sum_r rx_r\right)\left(\sum_i \frac{\lambda_i}{\sum_r \ell_i(r)x_r}\right) \\
    &< \infty.
\end{align*}
\end{proof}


% The proof of the proposition follows similar lines as that of Proposition~\cref{prop:fluid-ub}.
\begin{proof}[Proof of Proposition~\ref{prop:fluid-ub-for-all-theta}.]
By \cref{prop:limiting-dist},
\begin{align}
\scaledv(\varphi) &= \EE\left[\rev\left(\frac{\scaledrandomstate}{\theta}\right)\right] - \left(\sum_r rx_r\right)\EE\left[\frac{\scaledrandomstate}{\theta}\right] \\ &\leq \rev\left(\EE\left[\frac{\scaledrandomstate}{\theta}\right]\right)- \left(\sum_r rx_r\right)\EE\left[\frac{\scaledrandomstate}{\theta}\right] \label{eq:jensens2}\\
&= \rev\left(\sum_i \frac{\lambda_i}{\sum_r \ell_i(r) x_r}\right)- \left(\sum_r rx_r\right)\sum_i \frac{\lambda_i}{\sum_r \ell_i(r) x_r} \label{eq:poisson-steady-state2} \\
&\leq \fluidopt \label{eq:opt-def2}
\end{align}
where~\eqref{eq:jensens2} follows from concavity of $\rev$,~\eqref{eq:poisson-steady-state2} from \cref{prop:limiting-dist}, and~\eqref{eq:opt-def2} by taking the supremum over all $\xvec \in \simplex^{|\rewardset|}$. Since the upper bound holds for arbitrary $\varphi \in \Phi$, $v^\star_\theta = \sup_{\varphi \in \Phi} \scaledv(\varphi) \leq \optfluidprofit$.
\end{proof}}



\subsection{Proof of Theorem~\ref{thm:asymptotic-result}}
\begin{proof}
Let $\randomstate^\star$ be the random variable denoting the steady-state number of agents in the system, induced by policy $\varphi^\star$. For ease of notation, let $\Lambda^\star = \sum_i \frac{\lambda_i}{\sum_r \ell_i(r)x_r^\star}$. By \cref{prop:limiting-dist}, $\randomstate^\star \sim$ Poisson$(\theta\Lambda^\star)$.

We first claim that it suffices to establish that 
$\EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right] \geq \rev\left(\frac{\EEn}{\theta}\right)-\frac{C}{{\theta}},$
since 
\begin{align*}
    \scaledv(\varphi^\star)-\optfluidprofit &= \left(\EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right]-\left(\sum_r r x_r^\star\right)\frac{\EEn}{\theta}\right) - \left(\rev\left(\frac{\EEn}{\theta}\right)-\left(\sum_r r x_r^\star\right)\frac{\EEn}{\theta}\right) \\
    &= \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right]-\rev\left(\frac{\EEn}{\theta}\right).
\end{align*}
%The claim then follows by applying Proposition~\ref{prop:fluid-ub}. 


 Consider the following three events:
{
\begin{itemize}
\item $E_1 = \left\{\randomstate^\star \geq \EEn - {c_0}\log\theta\sqrt{\EEn},\,\randomstate^\star \geq 1\right\}$
\item $E_2 = \left\{ \randomstate^\star < \EEn - {c_0}\log\theta\sqrt{\EEn},\,\randomstate^\star \geq 1 \right\}$
\item $E_3 = \left\{\randomstate^\star = 0\right\}$ 
\end{itemize}
where $c_0 = \frac{\Lambdalb-\varepsilon}{\sqrt{\Lambdalb}}\frac{e}{2}$, for some constant $\varepsilon \in (0,\Lambdalb)$. 
}

We first decompose the expected revenue under $\varphi^\star$ according to $E_1, E_2, E_3$:
\begin{align}
    \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right] &= \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\bigg{|} E_1\right]\PP(E_1) + \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\bigg{|} E_2\right]\PP(E_2) \notag \\
    &\hspace{5cm}+ \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\bigg{|} E_3\right]\PP(E_3) \notag \\
    & \geq \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\bigg{|} E_1\right]\PP(E_1) + \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\bigg{|} E_2\right]\PP(E_2). \label{eq:ignore-e3} 
\end{align}
where~\eqref{eq:ignore-e3} follows from non-negativity of $\rev$.

For $i \in \{1,2\}$, let $I_i$ denote the interval corresponding to $E_i$. We leverage the fact that {$\rev$ is twice-continuously differentiable over $\mathbb{R}_{> 0}$}, and apply Taylor's theorem to bound $\rev\left(\frac{\randomstate^\star}{\theta}\right),$ for {$\randomstate^\star \in I_i$, $i \in \{1,2\}$}:
\begin{align}\label{eq:taylor}
\rev\left(\frac{\randomstate^\star}{\theta}\right) = \rev\left(\frac{\EEn}{\theta}\right) + \rev'\left(\frac{\EEn}{\theta}\right)\left(\frac{\randomstate^\star-\EEn}{\theta}\right) + \frac12 \rev''(\eta_i)\left(\frac{\randomstate^\star-\EEn}{\theta}\right)^2
\end{align}
for some {$\eta_i$ between $\randomstate^\star/\theta$ and $\EEn/\theta$, i.e.,} $\eta_i \in \left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right]$.

Plugging the above into~\eqref{eq:ignore-e3}, we obtain:
\begin{align}\label{eq:expected-taylor}
    \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right] &\geq \rev\left(\frac{\EEn}{\theta}\right) \left(\PP\left(E_1\right)+\PP\left(E_2\right)\right) + \rev'\left(\frac{\EEn}{\theta}\right)\sum_{i\in\{1,2\}}\EE\left[\frac{{\randomstate^\star-\EEn}}{\theta}\,\bigg{|}\,E_i\right]\PP\left(E_i\right) \notag \\
    &\qquad + \frac{1}{2}\sum_{i\in\{1,2\}}\rev''(\eta_i) \EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_i\right]\PP(E_i) \notag \\
    &= \rev\left(\frac{\EEn}{\theta}\right) \left(1-\PP(E_3)\right) + \rev'\left(\frac{\EEn}{\theta}\right)\sum_{i\in\{1,2\}}\EE\left[\frac{{\randomstate^\star-\EEn}}{\theta}\,\bigg{|}\,E_i\right]\PP\left(E_i\right) \notag \\
    &\qquad + \frac{1}{2}\sum_{i\in\{1,2\}}\rev''(\eta_i) \EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_i\right]\PP(E_i)
\end{align}

Lemmas~\ref{lem:step2-decomposition}-\ref{lem:prob-e3} allow us to complete the {loss} analysis. We defer their proofs to Appendix~\ref{app:asymp-thm-lemmas}.

\begin{lemma}\label{lem:step2-decomposition}
$\rev'\left(\frac{\EEn}{\theta}\right)\sum_{i\in\{1,2\}}\EE\left[\frac{{\randomstate^\star-\EEn}}{\theta}\,\bigg{|}\,E_i\right]\PP\left(E_i\right) \geq 0 \quad \forall \, \theta.$
% There exists a constant $c_2 > 0$ such that $\sum_{i\in\{1,2\}}\EE\left[\frac{{\randomstate^\star-\EEn}}{\theta}\,\bigg{|}\,E_i\right]\PP\left(E_i\right) \geq -\frac{c_2}{\theta}$.
\end{lemma}




\begin{lemma}\label{lem:e1-analysis}
There exists a constant $c_1 > 0$ such that, conditioned on $E_1$, $$\rev''(x) \EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_1\right]\PP(E_1) \geq -\frac{c_1}{\theta} \quad \forall \, x\in \left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right].$$
\end{lemma}



\begin{lemma}\label{lem:e2-analysis}
There exists a constant $c_2 > 0$ such that, conditioned on $E_2$, $$\rev''(x) \EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_2\right]\PP(E_2) \geq -\frac{c_2}{\theta} \quad \forall \, x\in \left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right].$$
\end{lemma}



\begin{lemma}\label{lem:prob-e3}
$\PP(E_3) \leq \frac{1}{e\,\Lambdalb}\cdot\frac1\theta.$
% There exists a constant $c_1 > 0$ such that $\PP(E_3) \leq \frac{c_1}{\theta}$.
\end{lemma}



Applying the above lemmas to~\eqref{eq:expected-taylor}, we obtain:
\begin{align*}
    \EE\left[\rev\left(\frac{\randomstate^\star}{\theta}\right)\right] &\geq \rev\left(\frac{\EEn}{\theta}\right)\left(1-\frac{1}{e\Lambdalb}\frac{1}{\theta}\right) - \frac12\left(\frac{c_1}{\theta} + \frac{c_2}{\theta}\right).
\end{align*}
Defining $C = \frac{\rev(\Lambdaub)}{e\Lambdalb} + \frac12({c_1+c_2})$, we obtain the theorem.
\end{proof}



\subsubsection{Proofs of auxiliary lemmas}\label{app:asymp-thm-lemmas}

\begin{proof}[Proof of Lemma~\ref{lem:step2-decomposition}.]
We have:
\begin{align}\label{eq:step2-decomposition}
    0 = \EE\left[\randomstate^\star-\EEn\right] &= \sum_{i\in\{1,2\}}\EE\left[{\randomstate^\star-\EEn}\bigg{|}E_i\right]\PP\left(E_i\right) \notag
    \\ &\hspace{3cm}+ \EE\left[{\randomstate^\star-\EEn}\bigg{|}E_3\right]\PP\left(E_3\right) \notag \\
    \implies  \sum_{i\in\{1,2\}}\EE\left[{\randomstate^\star-\EEn}\bigg{|}E_i\right]\PP\left(E_i\right) &= -\EE\left[{\randomstate^\star-\EEn}\bigg{|}E_3\right]\PP\left(E_3\right) = \EEn\PP(E_3) > 0.
\end{align} 

Combining~\eqref{eq:step2-decomposition} with the fact that $\rev'(x) \geq 0$ for all $x$, we obtain the lemma. 
\end{proof}



\begin{proof}[Proof of Lemma~\ref{lem:e1-analysis}.]
For ease of notation, we define $$\mathcal{I}(\randomstate^\star,\theta) := \left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right].$$ Conditioned on $E_1$, $\randomstate^\star \geq \theta\Lambda^\star-c_0\log\theta\sqrt{\EEn}$. We then have that  $$\mathcal{I}(\randomstate^\star,\theta) \subseteq \left[\frac{\theta\Lambda^\star-{c_0}\log\theta\sqrt{\EEn}}{\theta},+\infty\right).$$

By assumption, $c_0 \leq \frac{\Lambdalb-\varepsilon}{\sqrt{\Lambdalb}}\frac{e}{2}$, therefore:
\begin{align}
    \frac{\theta\Lambda^\star-{c_0}\log\theta\sqrt{\EEn}}{\theta} &= \Lambda^\star-c_0\sqrt{\Lambda^\star}\frac{\log\theta}{\sqrt\theta}\notag \\&\geq \Lambda^\star - \left(\frac{\Lambdalb-\varepsilon}{\sqrt{\Lambdalb}}\frac{e}{2}\right)\sqrt{\Lambda^\star}\frac{\log\theta}{\sqrt\theta}\notag\\ &\geq \Lambda^\star - \left(\frac{\Lambdalb-\varepsilon}{\sqrt{\Lambdalb}}\frac{e}{2}\right)\sqrt{\Lambda^\star}\cdot\frac2e \label{eq:ub-on-logxsqrtx} \\
    &\geq\Lambda^\star - \left(\frac{\Lambdalb-\varepsilon}{\sqrt{\Lambdalb}}\right)\sqrt{\Lambdaub} \label{eq:ub2} \\
    &\geq \varepsilon \label{eq:ub3}
\end{align}
where~\eqref{eq:ub-on-logxsqrtx} follows from the upper bound $\frac{\log\theta}{\sqrt{\theta}} \leq \frac2e$ for all $\theta$,~\eqref{eq:ub2} from $\Lambda^\star \leq \Lambdaub$, and~\eqref{eq:ub3} from $\Lambda^\star \geq \Lambdalb$.



Thus, $\mathcal{I}(\randomstate^\star,\theta) \subseteq \left[\varepsilon,+\infty\right)$, for some constant $\varepsilon$. Since $\rev$ is twice-continuously differentiable over $\mathbb{R}_{>0}$, there exists $c_1' > 0$ such that $\rev''(x) \geq - c_1'$ for all $x \in \left[\varepsilon,+\infty\right)$, and consequently for all $x \in \mathcal{I}(\randomstate^\star,\theta)$.

We next bound $\EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\right]\PP(E_1).$
\begin{align*}
     &\text{Var}(\randomstate^\star)= \EE\left[(\randomstate^\star-\EEn)^2\right] = \sum_{i=1}^3 \EE\left[(\randomstate^\star-\EEn)^2\bigg{|}E_i\right]\PP(E_i) \\
     \implies &\EE\left[(\randomstate^\star-\EEn)^2\bigg{|}E_1\right]\PP(E_1)\leq \text{Var}(\randomstate^\star) = \theta\Lambda^\star \leq \theta\Lambdaub \\
     \implies &\EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\bigg{|} E_1
     \right]\PP(E_1) \leq \frac{\Lambdaub}{\theta}.
\end{align*}

Putting this all together, we obtain:
\begin{align*}
\rev''(x)\EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_1\right]\PP(E_1) \geq -\frac{c_1'\Lambdaub}{\theta} \quad \forall \, x \in \mathcal{I}(\randomstate^\star,\theta).
\end{align*}
\end{proof}



\begin{proof}[Proof of Lemma~\ref{lem:e2-analysis}.]
By the Poisson tail bound~\cite{canonne2017short}, 
\begin{align*}\PP(E_2) \leq \exp\left\{-\frac{c_0^2{\log^2\theta}\,\EEn}{2\EEn}\right\} = \theta^{-c_0^2\log\theta/2}.
\end{align*}

Conditioned on $E_2$, $\randomstate^\star \in [1,\EEn)$. Thus:
\begin{align*}
&(\randomstate^\star-\EEn)^2 \leq (1-\theta\Lambda^\star)^2 \leq \left(\theta\Lambda^\star\right)^2 \leq \theta^2\Lambdaub^2
\implies \EE\left[\frac{(\randomstate^\star-\EEn)^2}{\theta^2}\,\bigg{|}\,E_2\right] \leq \Lambdaub^2.
\end{align*}

Moreover, {by assumption there exists $\alpha > 0$ such that $\rev''(x) \geq -\theta^{\alpha}$ for all $x \in \left[\frac1\theta,\Lambdaub\right]$. Thus 
\begin{align}
    \rev''(x) \EE\left[\frac{\left(\randomstate^\star-\EEn\right)^2}{\theta^2}\,\bigg{|}\,E_2\right]\PP(E_2) \geq -\Lambdaub^2 \theta^{-c_0^2\log\theta/2}\cdot \theta^{\alpha} \quad \forall\, x \in  \left[\frac1\theta,\Lambdaub\right].
\end{align}

Let $g(\theta) = \theta^{-c_0^2\log\theta/2 + \alpha}.$ We have that $g(\theta) \leq c_2' \theta^{-1}$ for some constant $c_2' > 0$ if and only if $\theta^{-c_0^2\log\theta/2+\alpha+1} \leq c_2' \iff -c_0^2\log\theta/2+\alpha+1 \leq \log c_2' \iff c_0^2 \geq \frac{2(1+\alpha-\log c_2')}{\log\theta}$. For {$c_2' > e^{1+\alpha}$}, this holds for all $c_0 > 0$, $\theta > 1$.}

We conclude the proof of the lemma by observing that, conditioned on $E_2$, $$\left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right] = \left[\frac{\randomstate^\star}{\theta},\frac{\EEn}{\theta}\right]\subseteq \left[\frac1\theta,\Lambda^\star\right] \subseteq\left[\frac1\theta,\Lambdaub\right].$$  Thus, the bound holds for all $x \in \left[\frac1\theta\min\left\{\randomstate^\star,{\EEn}\right\},\frac1\theta\max\left\{{\randomstate^\star},{\EEn}\right\}\right]$.
\end{proof}



\begin{proof}[Proof of Lemma~\ref{lem:prob-e3}.]
By definition,
$\PP(E_3) = \PP(\randomstate^\star = 0) = e^{-\theta\Lambda^\star} \leq \frac1e \cdot \frac{1}{\theta\Lambda^\star} \leq \frac{1}{e\,\Lambdalb}\cdot\frac1\theta.$
\end{proof}

\end{document}
