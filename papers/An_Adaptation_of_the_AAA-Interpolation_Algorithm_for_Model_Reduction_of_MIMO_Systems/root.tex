\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% Import standard set of classes and commands
\usepackage{defs}

% Custom paper-specific commands
\newcommand{\non}{\nonumber}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\W}{\widehat{\mbb{W}}}

\title{\LARGE \bf
An Adaptation of the AAA-Interpolation Algorithm for Model  Reduction  of MIMO Systems
}

\author{Jared Jonas and Bassam Bamieh%
\thanks{Jared Jonas and Bassam Bamieh are with the Department of Mechanical Engineering, University of California - Santa Barbara, USA 
{\tt\small jjonas@ucsb.edu, bamieh@ucsb.edu}}%
}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
We consider the Adaptive Antoulas-Anderson (AAA) rational interpolation algorithm recently developed by Trefethen and co-authors,  
which can be viewed as a type of moment-matching technique for system realization and approximation. We consider variations 
on this algorithm that are suitable for model reduction of linear time invariant systems while addressing some of the shortcomings 
of the block-AAA variant of the algorithm for MIMO systems. In particular, we develop state-space formulas and keep track of the state-space dimension at every step of the adaptive block-AAA algorithm, showing an unfavorable increase of the state dimension.  We propose  a new low-rank adaptive  interpolation algorithm that addresses this shortcoming. 
Comparative  computational results are included for the algorithms above, together with comparisons to balanced reduction.  
\end{abstract}

\section{Introduction}
Model order reduction is an important tool in the analysis, simulation, and control of large-scale systems~\cite{antoulas2000survey,Baur14}, and is particularly relevant for control applications in, for example, fluid and structural mechanics~\cite{Lassila14,Hetmaniuk12}.  In the context of linear dynamic systems, model reduction algorithms aim to produce a state-space model with fewer states that approximates the dynamics of the original system.  
Amongst several model-reduction techniques, moment matching  constructs a reduced-order model that matches the original model's moments at a given set of points~\cite{ionescu_astolfi_2011}.  This can be interpreted  as creating a rational interpolant whose value (or some derivative) matches the original transfer function at that point. Moment matching and interpolation problems are therefore intimately linked. 

Building on the original rational interpolation results of Antoulas and Anderson~\cite{antoulas1986scalar} 
that uses a 
pre-specified set of interpolation points, Trefethen et. al~\cite{Nakatsukasa_2018} developed an algorithm they termed Adaptive Antoulas-Anderson (AAA). This algorithm uses  a barycentric interpolation formula~\cite{berrut_trefethen_2004}, and ``adaptively'' picks points in the complex plane at which a scalar-valued  function is interpolated based on a maximum error criterion. The algorithm yields a rational approximant to a given complex function. 
Its main advantage is the automated selection of interpolation points, and has several interesting features as discussed in~\cite{Nakatsukasa_2018}. 

Subsequently, a matrix-valued version of the algorithm, termed  
 block-AAA~\cite{gosea2021algorithms} was developed. This algorithm interpolates the {\em matrix value} of a given function at certain points that are also  adaptively selected according to a maximum error criterion. 

Since their introduction, AAA and related algorithms have been used in a systems context for  model-order reduction and also in system identification.  
%System identification is a natural application for AAA because it can produce a rational function that approximates input-output behavior.  
Such ``data-driven'' rational approximations have been used in  parametric dynamical systems~\cite{Rodriguez23}, and in quadratic-output systems~\cite{gosea2021datadriven}.  More recently, they 
have been used in a model-order reduction scheme~\cite{yu2023leveraging} with a two-step method  utilizing both block-AAA on a discrete set of points and Hankel norm approximation.

In this paper, we propose new variants of the AAA algorithm for the purpose of model reduction of high-order LTI systems. We give state-space formulas for realizations of interpolants with real parameters. We also replace the discretized maximum criterion employed in previous algorithms by a bisection algorithm for computing $L^\infty$ errors on the imaginary axis, which in turn guides the adaptive selection of interpolation points. Most importantly, we show that adapting the existing block-AAA algorithm for use on linear systems has undesirable features when used on MIMO systems, especially when the number of outputs is large, in that it leads to a rapid increase in the state dimension of the interpolant compared to other schemes. The requirement of exactly interpolating the full matrix at each point causes this increase in state dimension. We argue that matrix-valued interpolation with lower rank matrices (formed from the significant singular values/vectors at those points) rather than exact interpolation is more effective. With this motivation, we develop an algorithm and demonstrate its effectiveness with numerical examples comparing the proposed algorithms with balanced reduction. We close with a discussion of some open problems in matrix-valued interpolation, and directions for future work. 

%In this paper we propose system-AAA, a modification of block-AAA that directly yields a reduced order system, and we investigate the performace of system-AAA in the context of model order reduction.  From the results in \cite{gosea2021algorithms}, block-AAA yields functions that approximate the user-defined function well, but it has some drawbacks that make it unideal to utilize in a dynamic systems context as-is.  It yields a rational function with complex coefficients even if the user-supplied function has real coefficients, the user has to supply a set of points in the domain to approximate on, and the inverse that appears in the interpolation function may not yield a well-defined state space model.  In the following section, we develop and derive the algorithm that rectifies the above points, and show its performance with computational results.  In addition to this, we introduce a modified algorithm that attempts to system-AAA's performance for multi-input, multi-output systems.  


\subsection{Notation}
We use the notation 
\[
	H(s) = C(sI-A)^{-1} B + D =  \brac{\begin{array}{c|c}A & B \\ \hline C & D\end{array}}
\]
for the transfer function and state space realization of a finite-dimensional Linear Time Invariant (LTI) system. 
%Represent a linear dynamic system \(H\) with dynamics 
%\[\dot{x} = Ax + Bu, \quad y = Cx + Du,\]
%And let the transfer function be denoted \(H(s)\).  
\(\overline{X}\) denotes the complex conjugate (not transpose) of  a matrix \(X\), and  \(X^*\) denotes the complex-conjugate transpose of \(X\).  

\section{System-AAA}
The block-AAA algorithm~\cite{gosea2021algorithms} is an iterative algorithm that starts with a given matrix-valued function $G(.)$ (of possibly high order), 
and builds up a matrix-valued rational function approximation at step $r$  of the form
\begin{align}
        R_r(z) &= \p{\sum_{k=1}^r \frac{W_k}{z-z_k}}\inv \p{\sum_{k=1}^r \frac{W_kG(z_k)}{z-z_k}} 	  \label{blockAAA}	\\
        &=: M^{-1}_r(z) ~N_r(z) . 															\nonumber
\end{align}
This particular form ensures that $R_r$ interpolates $G$ exactly at the so-called support points $\left\{ z_k \right\}$ in the sense
that $R_r(z_k) = G(z_k)$ as matrices. The weight  matrices $\left\{ W_k \right\}$ are free parameters 
 chosen to minimize some measure (usually a least squares criterion) of 
error between $R_r$ and $G$ over (typically a large number of) points in the domain $\Omega$. 
The  next support point $z_{r+1} \in \Omega \subset \mathbb{C}$ is 
chosen where  the following error criterion is maximized 
\begin{equation}
	z_{r+1} = \arg\min_{z\in\Omega} \left\| R_r(z) - G(z) \right\| .
  \label{next_support}
\end{equation} 
The rationale being that since interpolation is exact at the support points, this error will be most reduced by this choice at 
the next iteration. 

The block-AAA algorithm presented in~\cite{gosea2021algorithms} produces approximations that have complex coefficients, and only evaluates the least squares error and solves the problem~(\ref{next_support}) numerically  over a large grid of points in $\Omega$. In this section, we propose a variant we call system-AAA, which works directly with state-space realizations with real matrices, performs the support point selection step~(\ref{next_support}) using a bisection algorithm (similar to those for computing $H^\infty$ norms), and selects the matrix weights $\left\{ W_k \right\}$ using a solution of the least squares problem without gridding. The solution of this last problem involves computing Gramians of systems and finding eigenvectors of matrices related to them. Thus gridding of the domain $\Omega$ is completely avoided. 

% and improves its approximation error successively.  The following equation is the interpolating function for block-AAA; each iteration adds one term to each sum which ensures \(R_r(z)\) interpolates at the ``support point'' \(z_k\).  This support point is chosen at the point in the user-defined domain \(\Omega\) where the approximation error is maximized.  The weight matrices \(W_k\) are then tuned via an optimization that minimizes the overall approximation error in a least-squares sense.  

%System-AAA is a modification of block-AAA that works directly on linear dynamic systems.  Given a system in state-space form, it will produce a reduced order state-space model iteratively, adding a number of states as it goes.  Unlike block-AAA, it always yields a system with real coefficients and does not need a discrete set of points to approximate on.  The user can choose how many iterations to run the algorithm, or can terminate the algorithm once the system performance is acceptable.  Like AAA, each iteration is broken into two main parts, which are the support point selection and the weight optimization.  The following is an outline of the system-AAA algorithm.  

\begin{algorithm}  
\caption{System-AAA}
\label{sys_aaa_alg}
\begin{algorithmic} 
        \Require \(G(s)\) in state space form
        \State \(k \gets 0\)
        \State \(R \gets \mathrm{ss}(G_D)\)
        \State \(NM \gets \mathrm{ss}()\)
        \Repeat
                \State \(\omega_k \gets \mathrm{hinfnorm}(G-R)\)
                % \State \(NM_i \gets \mathrm{createNM}(\omega_i, G(\omega_i))\)
                \State \(G_k = G(\omega_i), \; G_{k, r} = \mathrm{real}(G_k), \; G_{k, i} = \mathrm{imag}(G_k)\)
                \If{\(\omega_k = 0\)}
                        \State \(NM_k \gets \brac{\begin{array}{c|cc}0 & G_i & I \\ \hline I & 0 & 0\end{array}}\)
                \Else 
                        \State \(NM_k \gets \brac{\begin{array}{cc|cc}0 & \omega_k I & G_{k, r} & I \\ -\omega_k I & 0 & -G_{k, i} & 0 \\ \hline I & 0 & 0 & 0 \\ 0 & I & 0 & 0 \end{array}}\)
                \EndIf
                \State \(NM \gets \begin{bsmallmatrix}NM \\ NM_k\end{bsmallmatrix}\)
                \State \(H \gets \mathrm{minreal} \p{NM \begin{bsmallmatrix}I \\ -G\end{bsmallmatrix}}\) 
                \State \(X \gets H_C \p{\mathrm{lyap}(H_A, H_BH_B^*)} H_C^*\)
                \State Construct \(\mbb{W}\) using theorem \ref{opt_thm}
                \State \(\mcal{B}_1 \gets NM_B(:, 1:m)\)
                \State \(\mcal{B}_2 \gets NM_B(:, m+1:\mathrm{end})\)
                \State \(R \gets \brac{\begin{array}{c|c}NM_A - \mcal{B}_2 \widehat{\mbb{W}} & \mcal{B}_2 G_D - \mcal{B}_1 \\ \hline -\widehat{\mbb{W}} & G_D\end{array}}\)
                \State \(i \gets i + 1\)
        \Until{\(R\) approximates \(G\) sufficiently}
        \State \textbf{return} \(R\)
\end{algorithmic}
\end{algorithm}
Algorithm \ref{sys_aaa_alg} loosely follows MATLAB notation.  The subscripts \(A\), \(B\), \(C\), and \(D\) denote the corresponding state-space matrix for the system.  The following subsections detail the derivation of the algorithm and its connections to AAA.  The first subsection uses the block-AAA interpolating function as a basis and derives a new interpolating function that interpolates at \(\omega=j\infty\), guarantees real coefficients, and derives its associated transfer functions.  The second subsection details the transformation of the block-AAA algorithm into a state-space context.  The third details the derivation of the state-space representation of the interpolation function.  Finally the final section shows computational results for the algorithm.  

\subsection{Interpolation function}
Consider the multi-input, multi-output (MIMO) system 
\[G = \brac{\begin{array}{c|c}A & B \\ \hline C & D\end{array}},\]
where \(A\in\real^{n\times n}\), \(B\in\real^{n\times q}\), \(C\in\real^{p\times n}\), and \(D\in\real^{p\times q}\). We choose support points that always lie on the imaginary axis, thus Equation~(\ref{blockAAA}) becomes
\begin{equation}
        R_r(s) = \p{\sum_{k=1}^r \frac{W_k}{s - j\omega_k}}\inv \p{\sum_{k=1}^r \frac{W_k G(j\omega_k)}{s-j\omega_k}}. \label{rr_eq}
\end{equation}
\begin{remark} \label{rem_int}
        The interpolating function (\ref{rr_eq}) guarantees that \(R_r(j\omega_i) = G(j\omega_i)\) for any support point \(\omega_i\), \(1\leq i \leq r\), provided that each \(W_k\) is invertible.    

        \begin{proof}
                Multiplying by \(\frac{s-j\omega_i}{s-j\omega_i}\) yields 
                \begin{align*}
                    R_r(s) =& \p{W_i + \sum_{i\neq k = 1}^r \frac{(s-j\omega_i) W_k}{s-j\omega_k}}\inv \\
                    & \p{W_iG(j\omega_i) + \sum_{i\neq k = 1}^r \frac{(s-j\omega_i) W_kG(j\omega_k)}{s-j\omega_k}} \\
                    \therefore R_r(j\omega_i) =& W_i\inv W_i G(j\omega_i) = G(j\omega_i).
                \end{align*}
        \end{proof}
\end{remark}

From here, we begin to address the issues that were outlined above.  The algorithm needs the ability to interpolate at \(\omega=j\infty\).  We therefore rewrite the interpolation in a more general form, yielding
\begin{subequations} \label{rmn_eq}
        \begin{align}
                R_\ell(s) &= M(s)\inv N(s), \\
                \shortintertext{where}
                M(s) &= \mcal{W}_0 + \sum_{k=1}^\ell \mcal{W}_k M_k(s) \\
                N(s) &= \mcal{W}_0 D + \sum_{k=1}^\ell \mcal{W}_k N_k(s), 
        \end{align}
\end{subequations}
and \(M(s)\in\comp^{p\times p}\), \(N(s)\in\comp^{p\times q}\).  All of the weights in \(M(s)\) and \(N(s)\) can be factored out to the left, meaning \(M\) and \(N\) can be written as
\begin{align}
    N(s) &= \mbb{W}\mcal{N}(s), \quad M(s) = \mbb{W}\mcal{M}(s), \non \\
    \shortintertext{where}
    \mbb{W} &= \begin{bmatrix}\mcal{W}_0 & \mcal{W}_1 & \cdots & \mcal{W}_\ell\end{bmatrix} \non \\
    \mcal{N}(s) &= \begin{bsmallmatrix}D \\ N_1(s) \\ \vdots \\ N_\ell(s)\end{bsmallmatrix}, \qquad \mcal{M}(s) = \begin{bsmallmatrix}I \\ M_1(s) \\ \vdots \\ M_\ell(s)\end{bsmallmatrix}. \non 
\end{align}
Note \(\mbb{W}\in\real^{p\times pr}\).  Depending on the location of the support point, the size of \(M_k(s)\) or \(N_k(s)\) can change.  To ensure the resulting interpolating function has real coefficients, it must be the case that \(G(j\omega) = \overline{G}(-j\omega)\) for any \(\omega\in\real\).  This may be accomplished by adding pairs of complex conjugate support points with conjugate weights.  Starting with \(M\),
\begin{align}
    \mcal{W}_kM_k(s) &= \frac{W_{k,1}}{s-j\omega_k} + \frac{W_{k, 2}}{s+j\omega_k}. \non \\
    \shortintertext{Assuming \(W_{k,1} = W_k\) and \(W_{k, 2} = \overline{W}_k\),}
    \mcal{W}_kM_k(s) &= \frac{2s\Re(W_k)-2\omega_k\Im(W_k)}{s^2 + \omega_k^2}. \non \\
    \shortintertext{Therefore}
    \mcal{W}_k &= 2\begin{bmatrix}\Re(W_k) & \Im(W_k)\end{bmatrix} \non \\ 
    M_k(s) &= \begin{bmatrix}\frac{s}{s^2 + \omega_k^2} \\ -\frac{\omega_k}{s^2 + \omega_k^2}\end{bmatrix}. \label{mk_eq} \\
    \shortintertext{Similarly for \(N\),}
    \mcal{W}_k N_k(s) &= \frac{W_{k, 1}G(j\omega_k)}{s-j\omega_k} + \frac{W_{k, 2}\overline{G}(j\omega_k)}{s+j\omega_k} \non \\
    \therefore N_k(s) &= \begin{bmatrix}\frac{\Re(G(j\omega_k))s - \Im(G(j\omega_k))\omega_k}{s^2+\omega_k^2} \\ -\frac{\Im(G(j\omega_k))s + \Re(G(j\omega_k)\omega_k}{s^2+\omega_k^2} \end{bmatrix}. \label{nk_eq} 
\end{align} 
In this case \(\mcal{W}_k\in\real^{p\times 2p}\), \(M_k(s)\in\comp^{2p\times p}\), and \(N_k(s)\in\comp^{2p\times q}\). When \(\omega_k=0\), the first order system is already real thus there is no need to add an additional complex conjugate support point.  In this case,
\begin{equation}
        M_k(s) = \frac{I}{s}, \quad N_k(s) = \frac{G(0)}{s}, \quad \mcal{W}_k = W_k, \label{nm0_eq}
\end{equation}
and \(\mcal{W}_k\in\real^{p\times p}\), \(M_k(s)\in\comp^{p\times p}\), and \(N_k(s)\in\comp^{p\times q}\).

\subsection{Algorithm reformulation}
Each step of the AAA algorithm is composed of two main parts, the first being the selection of the new support point at the beginning of each iteration. The second is the selection of the weight matrices from an optimization problem that minimizes the approximation error between the interpolating function and the input function.  In this section we show that these parts can be reformulated remove the necessity of a user-defined domain and to better utilize systems machinery.  

The next support point is chosen at the point in the domain where the error between \(R_r(z)\) and \(G(z)\) is largest.  The domain in this case is the imaginary line, so the next support point will be at the frequency where the \(\mcal{H}_\infty\) norm occurs, i.e.
\begin{equation}
        \omega_\ell = \arg\min_{\omega\in\real_{\geq 0}} \norm{G(j\omega) - R_{\ell-1}(j\omega)}_2.
\end{equation}
This can be efficiently calculated with a bisection algorithm \cite{Bruinsma90}.  After a support point is selected, the weights in the interpolating function are selected via an optimization problem.  The optimization problem in block-AAA is the following: 
\[\min_{\mbb{W}} \sum_{z\in\Omega} \norm{N(z)-M(z)G(z)}^2_F \quad \st \norm{\mbb{W}}_F = 1.\]
Since our analysis in in continuous time, the sum will be replaced with an integral over the positive imaginary axis yielding
\begin{align}
    \mbb{W} &= \arg\min_{\mbb{W}} \int_0^\infty \norm{\mbb{W}\p{\mcal{N}(j\omega)-\mcal{M}(j\omega)G(j\omega)}}_F^2 \rmd\omega. \non \\
    \shortintertext{Letting \(H(s) = \mcal{N}(s) - \mcal{M}(s)G(s)\),}
    &= \arg\min_{\mbb{W}} \int_0^\infty \tr\p{\mbb{W}H(j\omega)H^*(j\omega)\mbb{W}^*} \rmd\omega \non \\
    &= \arg\min_{\mbb{W}} \tr\p{\mbb{W}X\mbb{W}^*}, \non \\
    \shortintertext{where}
    X &= \int_0^\infty H(j\omega)H^*(j\omega)\rmd\omega = \hat{C}\hat{G}_C\hat{C}^*, \label{x_eq}
\end{align}
where \(G_C\) is the controllability Gramian for \(H\).  Note that \(H\) can be written as a product of two augmented systems,
\begin{equation}
        H = \begin{bmatrix}\mcal{N} & \mcal{M}\end{bmatrix} \begin{bmatrix}I \\ -G\end{bmatrix}, \label{h_eq}
\end{equation}
and the positive matrix \(G_C\) can be found via the Lyapunov equation \cite[p. 112]{Zhou95} 
\begin{equation}
        \hat{A}G_C + G_C\hat{A}\trans = -\hat{B}\hat{B}^*, \label{x_lyap}
\end{equation}
where \(\hat{A}\), \(\hat{B}\), and \(\hat{C}\) are the corresponding state space matrices of \(H\).
\begin{remark}
        In order to guarantee existence and uniqueness of \(G_c\), the system \(H\) must not have any marginally stable poles, i.e. \(\hat{A}\) must not have any eigenvalues on the imaginary axis.  However, this system has poles at \(\pm j\omega_k\) for all support points \(\omega_k\).  It can be shown that there is a pole-zero cancellation for all of these poles, thus finding a minimal realization of \(H\) will suffice to find \(G_c\).  
\end{remark}
The constraint \(\norm{\mathbb{W}}_F = 1\) is modified to \(\mathbb{W}\mathbb{W}^* = I\) in the new problem to guarantee \(\mbb{W}\) has full row rank.  Therefore the optimization becomes:
\begin{equation}
        \mbb{W} = \arg\min_{\mbb{W}} \tr\p{\mbb{W}X\mbb{W}^*}, \quad \st \mbb{W}\mbb{W}^* = I. \label{opt_eq} 
\end{equation}
The closed form for (\ref{opt_eq}) may be found by finding stationary points.  A necessary condition for optimality is the following:
\begin{equation}
        \mbb{W}X - \Lambda^* \mbb{W} = 0. \label{eig_eq}
\end{equation}
\begin{comment}
        \begin{align}
        L &= \tr \p{\mbb{W}X\mbb{W}^*} + \tr \p{\Lambda^* (I - \mbb{W}\mbb{W}^*)} \non \\
        &= \tr \p{\mbb{W}X\mbb{W}^* - \Lambda^*\mbb{W}\mbb{W}^*} + \tr\Lambda^* \non \\
        \partial_\mbb{W} L(V) &= \tr \p{2\mbb{W}XV^* - 2\Lambda^*\mbb{W}V^*} \non \\
        &= 2\tr \p{(\mbb{W}X - \Lambda^* \mbb{W})V^*} = 0 \non \\
        \therefore 0 &= \mbb{W}X - \Lambda^* \mbb{W} \label{eig_eq}
        \end{align}
\end{comment}
\begin{theorem} \label{opt_thm}
        A solution for equation (\ref{eig_eq}) subject to \(\mbb{W}\mbb{W}^* = I\) is \(\mathbb{W} = QV^*\), where \(Q\) is an arbitrary real unitary matrix and the columns of \(V\) are the eigenvectors corresponding to the \(p\) smallest distinct non-zero eigenvalues of \(X\).  

        \begin{proof}
                Rearranging (\ref{eig_eq}) yields \(X\mbb{W}^* = \mbb{W}^*\Lambda\).  This implies \(\mbb{W}X\mbb{W}^* = \mbb{W}\mbb{W}^*\Lambda = \Lambda\).  Taking the conjugate transpose shows that \(\Lambda = \Lambda^*\), therefore \(\Lambda\) can be diagonalized.  Letting \(\Lambda = UDU^*\) where \(U\) is unitary and \(D\) is a diagonal matrix yields
                \[X\mbb{W}^* = \mbb{W}^* X \implies XV = VD,\]
                where \(V = \mbb{W}^* U\).  Now let \(V = \begin{bsmallmatrix}v_1 & \cdots & v_p\end{bsmallmatrix}\) and \(D = \diag\curly{\begin{bsmallmatrix}d_1 & \cdots & d_p\end{bsmallmatrix}}\).  Then,
                \[Xv_1 = d_1 v_1, \quad \cdots \quad Xv_p = d_p v_p,\]
                This shows that the columns of \(V\) are eigenvectors of \(X\) and the diagonal elements of \(D\) are the corresponding eigenvalues.  
                
                From \(V=\mbb{W}^*U\), clearly \(\mbb{W} = UV^*\), therefore \(\mbb{W}\mbb{W}^* = UV^*VU^* = V^*V = I\).  This demonstrates the columns of \(V\) must be orthonormal, which implies the eigenvectors that are picked must be associated with distinct eigenvalues.  
                
                Recall the optimization problem is \(\min\tr\p{\mbb{W}X\mbb{W}^*} = \min\tr\p{\mbb{W}\mbb{W}^*\Lambda} = \min\tr\Lambda = \min\tr{UDU^*} = \min\tr D\).  This implies the eigenvectors we pick must correspond to the smallest eigenvalues.    
        \end{proof}
\end{theorem}
\subsection{State-space realizations}
In this subsection we show that there exists a state-space representation for the interpolation function.  Equation (\ref{h_eq}) requires a realization for \(\begin{bsmallmatrix}\mcal{N} & \mcal{M}\end{bsmallmatrix}\).  This system is a vertical concatenation of the individual \(\begin{bsmallmatrix}N_k & M_k\end{bsmallmatrix}\) systems.  We can create a state space representation of this combined system by considering equations (\ref{mk_eq}), (\ref{nk_eq}), and (\ref{nm0_eq}).  For \(\omega_k = 0\), clearly
\begin{equation}
        \begin{bmatrix}N_k & M_k\end{bmatrix} = \brac{\begin{array}{c|cc}0 & G_k & I \\ \hline I & 0 & 0\end{array}}.
\end{equation}
When \(\omega_k \neq 0\), the equations are more complicated, so the realization is slightly harder to derive:
\begin{equation}
        \begin{bmatrix}N_k & M_k\end{bmatrix} = \brac{\begin{array}{cc|cc}0 & \omega_k I & G_{k, r} & I \\ -\omega_k I & 0 & -G_{k, i} & 0 \\ \hline I & 0 & 0 & 0 \\ 0 & I & 0 & 0 \end{array}},
\end{equation}
where \(G(j\omega_k) = G_k\), \(\Im(G_k) = G_{k, i}\), and \(\Re(G_k) = G_{k, r}\).  In general, this is written as 
\begin{equation}
        \begin{bmatrix}N_k & M_k\end{bmatrix} = \brac{\begin{array}{c|cc}A_k & B_{k, 1} & B_{k, 2} \\ \hline I & 0 & 0\end{array}}. \label{nkmk_eq}
\end{equation}
Notice that in either case, the size of \(A_k\) is a multiple of the number of outputs \(p\).  Now let \(\mathcal{A}\) be a block diagonal matrix composed of \(A_k\) matrices, and \(\mcal{B}_1\) and \(\mcal{B}_2\) be the block column vectors of \(B_{k, 1}\) and \(B_{k, 2}\) matrices respectively. This leads to the state-space representation of the interpolating function \(R_\ell\).  
\begin{lemma}
        Let 
        \begin{equation}
                N = \brac{\begin{array}{c|c}\mcal{A} & \mcal{B}_1 \\ \hline \mbb{W}_1 & \mcal{W}_0 D\end{array}}, \; M = \brac{\begin{array}{c|c}\mcal{A} & \mcal{B}_2 \\ \hline \mbb{W}_1 & \mcal{W}_0\end{array}}. \label{n_m_eq}
        \end{equation}
        where \(\mbb{W} = \begin{bsmallmatrix}\mcal{W}_0 & \mbb{W}_1\end{bsmallmatrix}\).  Assuming \(\mcal{W}_0\) is invertible,
        \begin{equation}
                R_\ell = M\inv N = \brac{\begin{array}{c|c}\mcal{A}-\mcal{B}_2 \W & \mcal{B}_2 D - \mcal{B}_1 \\ \hline -\W & D \end{array}}. \label{rss_eq}
        \end{equation}

\begin{proof}
Vertically concatenating each \(\begin{bsmallmatrix}N_k & M_k\end{bsmallmatrix}\) to create \(\begin{bsmallmatrix}\mcal{N} & \mcal{M}\end{bsmallmatrix}\) yields
\begin{equation}
        \begin{bmatrix}\mcal{N} & \mcal{M}\end{bmatrix} = \brac{\begin{array}{c|cc}\mcal{A} & \mcal{B}_1 & \mcal{B}_2 \\ \hline 0 & D & I \\ I & 0 & 0\end{array}}. 
\end{equation}    
Multiplying by \(\mbb{W}\) and separating \(N\) and \(M\) yields equation (\ref{n_m_eq}).  Then, 
\[M\inv = \brac{\begin{array}{c|c}\mcal{A}-\mcal{B}_2 \mcal{W}_0\inv \mbb{W}_1 & \mcal{B}_2 \mcal{W}_0\inv \\ \hline -\mcal{W}_0\inv \mbb{W}_1 & \mcal{W}_0\inv\end{array}}.\]
This shows 
\[M\inv N = \brac{\begin{array}{cc|c}\mcal{A}-\mcal{B}_2 \W & \mcal{B}_2 \W & \mcal{B}_2 D \\ 0 & \mcal{A} & \mcal{B}_1 \\ \hline -\W & \W & D\end{array}},\]
where \(\W = \mcal{W}_0\inv \mbb{W}_1\).  Applying a coordinate change with transformation matrix \(\frac{1}{2}\begin{bsmallmatrix}I & I \\ -I & I\end{bsmallmatrix}\), we get 
\[M\inv N = \brac{\begin{array}{cc|c}\mcal{A}-\mcal{B}_2 \W & 0 & \mcal{B}_2 D - \mcal{B}_1 \\ -\mcal{B}_2 \W & \mcal{A} & \mcal{B}_2 D + \mcal{B}_1 \\ \hline -\W & 0 & D\end{array}}.\]
From this it is clear the second block mode is unobservable since it affects neither the output nor the first block state, therefore it may be eliminated to yield the final representation for the approximating system.  
\end{proof}
\end{lemma}

\begin{remark} \label{rem_p}
        One \(A_k\) matrix is appended to \(\mcal{A}\) at every iteration, showing that the system grows by a multiple of \(p\) states each time. 
\end{remark}

\begin{comment}
\subsection{Summary}
\begin{enumerate}
    \item Let \(R_0 = D\), \(\ell = 1\). 
    \item Using an \(\mcal{H}_\infty\) bisection algorithm, find \(\omega_\ell\) such that 
    \[\omega_\ell = \arg\min_{\omega\in\real\geq 0} \norm{G(j\omega)-R_{\ell-1}(j\omega)}_2\]
    \item Construct the state space representation of \(H\) from (\ref{h_eq}) using (\ref{scrnm_eq}). 
    \item Use \(H\) to find \(X\) using (\ref{x_eq}) and (\ref{x_lyap}).
    \item Find the weight matrices \(\mbb{W}\) that solves the optimization problem in (\ref{opt_eq}) using theorem \ref{opt_thm} by choosing the rows of \(\mbb{W}\) to be the eigenvectors corresponding to the \(p\) smallest non-zero distinct eigenvalues of \(X\).  
    \item Create a state space representation of \(R_\ell\) with (\ref{rss_eq}). 
    \item Evaluate the resulting reduced order system and go back to step 2, setting \(i\leftarrow i+1\) if better results are desired.  
\end{enumerate}
\end{comment}

\subsection{Computational results}
In this section, we discuss some numerical results where we compare our algorithm with a baseline, i.e. balanced reduction.  Balanced reduction is a standard algorithm for model reduction on LTI systems~\cite{Zhou95}.  We use as a test case a 270-state, 3-input, 3-output stable dynamic system modeling the dynamics of a module on the International Space Station (ISS) \cite{iss_model}.   The figures each show a plot of the maximum singular value of the frequency response for the reduced-order systems, and the absolute error between the reduced order systems and the full system.  The model used in figure \ref{cont_iss11_n27} is a 28-state approximation of the ISS system's first output only, while figure \ref{cont_iss_n9} shows the approximation on the full system.  The ISS system was reduced using both system-AAA and balanced reduction, and the figures demonstrate the difference in approximation error. 

\begin{figure}[!ht]
        \includegraphics[width=\linewidth]{cont_iss13_n28}
        \caption{ISS single-output \(n=28\) reduction}
        \label{cont_iss11_n27}
\end{figure}

Figure \ref{cont_iss11_n27} shows that the algorithm generated a stable and well-matched approximation to the system with comparable error to that of standard balanced reduction when used on a single-output system.

\begin{figure}[!ht]
        \includegraphics[width=\linewidth]{cont_iss_n9}
        \caption{ISS \(n=9\) reduction}
        \label{cont_iss_n9}
\end{figure}

Figure \ref{cont_iss_n9} shows the result when the algorithm is used on a MIMO system after two iterations.  In this case, the algorithm selected two support points at \(\omega=0\) and \(\omega\approx 0.8\) Hz.  Balanced reduction is able to stably replicate the dynamics at 4 peaks, while system-AAA only mirrors one peak and has unstable poles.  This effect becomes more pronounced as more outputs are added.  As stated in remark \ref{rem_p}, the number of states added per iteration is proportional to the number of outputs, suggesting that an improved algorithm would not have this dependence.  As mentioned in remark \ref{rem_int}, invertible \(W_k\) matrices are required for interpolation.  Numerous numerical simulations demonstrate that these \(W_k\) matrices are well-conditioned.   These observations motivate us to propose a different algorithm as stated in the next section.

\begin{comment}
        \begin{figure}[!ht]
                \includegraphics[width=\linewidth]{cont_iss_n51}
                \caption{ISS \(n=51\) reduction}
                \label{cont_iss_n51}
        \end{figure}
\end{comment}

% When the algorithm is run on a stable multi-input single-output system, we conjecture that the lower order resulting system is always stable.  The algorithm yields unstable poles in the MIMO case.  As mentioned in remark \ref{rem_int}, invertible \(W_k\) matrices are required for interpolation.  Numerous numerical simulations demonstrate that these \(W_k\) matrices are well-conditioned.   

\section{Low-rank approximation}
Though the performance of system-AAA is satisfactory with single-output systems, the results indicate that the performance degrades as the number of outputs increases.  In order to rectify this, we investigated a slight change to remove the system's size dependence on the number of outputs with an algorithm we shall call low-rank approximation.  

\subsection{Interpolation function}
With low-rank approximation, we allow the approximating system to be non-full-rank at the support points.  This ensures that the interpolation function will grow one state when a new support point is added.  Consider the following approximating function
\begin{equation}
        R_r(s) = \p{\sum_{k=1}^r \frac{W_k U_k^*}{s-j\omega_k}}^\dagger \p{\sum_{k=1}^r \frac{W_k \Sigma_k V_k^*}{s-j\omega_k}}, \label{pir_eq}
\end{equation}
where \(U_k\Sigma_k V_k^* = G(j\omega_k)\) is a rank \(r_k\) approximation.  Let \(U_k\in\comp^{p\times r_k}\), \(V\in\comp^{m\times r_k}\), and \(\Sigma_k\in\real^{r_k \times r_k}\), and assume \(U_k\) and \(V_k\) have orthonormal columns, and \(\Sigma_k\) is diagonal.   When \(r_k=1\), \(U_k\), \(V_k\), \(\Sigma_k\), and \(W_k\) are all rank 1, showing that this approximation function will clearly not fully interpolate at the support point.   When \(r_k=p\), the approximation is full rank and fully interpolates the corresponding support point.   

\begin{comment} THE FOLLOWING IS INCORRECT
\begin{theorem}
        If \(U_k\Sigma_k V_k^*\) is a rank \(r_k\) approximation of \(G(j\omega_k)\), then 
        \[\lim_{\omega\to\omega_k}R_r(j\omega) = U_k\Sigma_k V_k^*\]  
        \noindent
        \begin{proof}
                Let \(\omega_i\) be one of the \(\omega_k\)s.  Then,
                \begin{align*}
                    \hat{R}(s) &= \frac{s-j\omega_i}{s-j\omega_i} R_r(s) = \\
                    & \p{W_iU_i^* + \sum_{i\neq k = 1}^r \frac{(s-j\omega_i)W_kU_k^*}{s-j\omega_k}}^\dagger \\
                    & \p{W_i\Sigma_iV_i^* + \sum_{i\neq k = 1}^r \frac{(s-j\omega_i)W_k\Sigma_kV_k^*}{s-j\omega_k}} \\
                    \lim_{s\to j\omega_i} \hat{R}(s) &= \p{W_i U_i^*}^\dagger \p{W_i \Sigma_i V_i^*} \\
                    \shortintertext{Assuming \(W_i\) is full rank,}
                    \p{W_i U_i^*}^\dagger &= U_i \cancel{(U_i^* U_i)\inv} \p{W_i^* W_i}\inv W_i^* \\
                    \shortintertext{Therefore}
                    \lim_{s\to j\omega_i} \hat{R}(s) &= U_i \cancel{(W_i^*W_i)\inv W_i^* W_i} \Sigma_i V_i^* = U_k\Sigma_k V_k^*
                \end{align*}
        \end{proof}
\end{theorem}
\end{comment}

Akin to system-AAA, \(R_r(s)\) can be rewritten to yield a \(M(s)\) and \(N(s)\) that are in the same form as (\ref{rmn_eq}).  Though \(R_r(s)\) contains a pseudoinverse, the system inverse \(M\inv\) is well-defined as long as \(\mcal{W}_0\) is invertible like before, thus the pseudoinverse is replaced with \(M\inv\).  Note that \(U_k \in\comp^{p\times r_k}\) and \(W_k\in\comp^{p\times r_k}\), so their product can only be full rank when \(r_k = p\).  
\begin{remark}
If \(p>q\), then when \(r_k = p\), \(V_k\) is not full rank, so the resulting \(R_\ell(j\omega_k)\) will not be full rank.  Therefore, we may perform model reduction on the dual of \(G(s)\), i.e. 
\[\mathrm{dual}[G](s) := \brac{\begin{array}{c|c}A\trans & C\trans \\ \hline B\trans & D\trans\end{array}}.\]
After the model is satisfactory, then we may return the dual of the reduced system.  
\end{remark}

The transfer functions of \(M_k\) and \(N_k\) are similar to the forms seen in the full interpolation algorithm, except the \(M_k\) systems have an added matrix \(U_k^*\), making their transfer functions more similar to \(N_k\).  The form of \(M_k\) and \(N_k\) are the following when \(\omega_k = 0\),
\[M_k(s) = \frac{U_k\trans}{s}, \quad N_k(s) = \frac{\Sigma_k V_k\trans}{s}, \quad \mcal{W}_k = W_k,\]
and when \(\omega_k \neq 0\),
\begin{align*}
    M_k(s) &= \begin{bmatrix}\frac{U_{k, r}\trans s + U_{k, i}\trans\omega_k}{s^2 + \omega_k^2} \\ \frac{U_{k, i}\trans s - U_{k, r}\trans\omega_k}{s^2 + \omega_k^2}\end{bmatrix} \\
    N_k(s) &= \begin{bmatrix}\frac{\Sigma_k V_{k, r}\trans s + \Sigma_k V_{k, i}\trans\omega_k}{s^2 + \omega_k^2} \\ \frac{\Sigma_k V_{k, i}\trans s - \Sigma_k V_{k, r}\trans\omega_k}{s^2 + \omega_k^2}\end{bmatrix}.
\end{align*}
The state space realizations for \(\begin{bsmallmatrix}N_k & M_k\end{bsmallmatrix}\) for \(\omega_k = 0\) is 
\begin{align*}
    \begin{bmatrix}N_k & M_k\end{bmatrix} &= \brac{\begin{array}{c|cc}0 & \Sigma_k V_k\trans & U_k\trans \\ \hline I & 0 & 0\end{array}}, \\
    \shortintertext{and for \(\omega_k \neq 0\),}
    \begin{bmatrix}N_k & M_k\end{bmatrix} &= \brac{\begin{array}{cc|cc}0 & \omega_k I & \Sigma_k V_{k, r}\trans & U_{k, r}\trans \\ -\omega_k I & 0 & \Sigma_k V_{k, i}\trans & U_{k, i}\trans \\ \hline I & 0 & 0 & 0 \\ 0 & I & 0 & 0 \trans\end{array}}.
\end{align*}
Note that \(U_{k, r}\), \(U_{k, i}\) are the real and imaginary parts of \(U_k\) respectively, and similarly for \(V_{k, r}\) and \(V_{k, i}\).  

\subsection{Algorithm}
The main change between system-AAA and the low-rank approximation algorithm is the modification of the approximating function.  This does not affect the majority of the algorithm.  However, the rank of the approximation at each support point needs to be addressed.  When a new support point is added, it will always start out as a rank 1 approximation, but the algorithm must also consider whether the improvement of the approximation at an existing support point will be more effective.  To do this, after a candidate \(\omega_\ell\) is selected, it will be compared to the previous support points, and if it is close to an existing support point, then it will instead improve said support point.  The minimum distance to a support point then is a tunable parameter.  

\subsection{Computational Results}
The ISS model was used again as a test for the partial approximation algorithm.  Like before, the following figures show the maximum singular value plot and its absolute error for a various number of states in each reduced system.  

\begin{figure}[!ht]
        \includegraphics[width=\linewidth]{part_iss_n8}
        \caption{ISS \(n=8\) reduction}
        \label{part_iss_n8}
\end{figure}

\begin{figure}[!ht]
        \includegraphics[width=\linewidth]{part_iss_n30}
        \caption{ISS \(n=30\) reduction}
        \label{part_iss_n30}
\end{figure}

Figures \ref{part_iss_n8} and \ref{part_iss_n30} show the results for the low-rank approximation algorithm.  From here it is clear the dynamics at more peaks are being incorporated compared to full interpolation.  In general, the results outperform full interpolation and are close to that of balanced reduction.  

\begin{comment}
        \begin{figure}[!ht]
                \includegraphics[width=\linewidth]{part_iss_n26}
                \caption{ISS \(n=26\) reduction}
                \label{part_iss_n26}
        \end{figure}
\end{comment}

Figures \ref{error_siso} and \ref{error_mimo} show the approximation error as the number of states increases for the two algorithms presented in this paper as well as balanced reduction.  The \(\mcal{H}_\infty\) norm indicated is the maximum error over the frequency domain, and the \(\mcal{H}_2\) norm written is the error integrated across the domain.  More precisely, in this context it has been calculated as:
\[\sqrt{\left|\tr\p{\hat{C}P\hat{C}^*}\right|}, \quad \hat{A}P + P\hat{A}^* = \hat{B}\hat{B}^*, \]
where \(\hat{A}\), \(\hat{B}\), and \(\hat{C}\) are the corresponding state space matrices to \(G-R_\ell\), the system representing the error between the input system and the reduced order system.  The presence of an `x' indicates that the resulting reduced order system had unstable poles with that number of states.  The first figure shows the error for the (1, 1) channel of the ISS system, while the second figure shows the error for the entire ISS system.  
\begin{figure}[ht!]
        \includegraphics[width=\linewidth]{error_siso}
        \caption{SISO system error as number of states increases}
        \label{error_siso}
\end{figure}

\begin{figure}[ht!]
        \includegraphics[width=\linewidth]{error_mimo}
        \caption{MIMO system error as number of states increases}
        \label{error_mimo}
\end{figure}

It is clear to see that in the SISO and MISO case, both algorithms perform well and match the performance of balanced reduction, yielding a stable reduced system.  For MIMO systems, the results are much more interesting and indicate a few things.  The error for full interpolation may increase as the number of states increases, and doesn't always yield a great result for some number of states.  In addition to this, most of the resulting systems contain a number of unstable poles.  

In comparison to these observations, Low-rank approximation matches the performace of balanced reduction up until a certain number of states, at which point the error slightly increases.  Low-rank approximation may generate systems with a few unstable poles, but does not always, indicating that the user may stop the algorithm once a satisfactorily-performing stable system is found.  Overall, the low rank approximation algorithm gives better results compared to full interpolation, and can give comparable results to balanced reduction.  

\section{Discussion}
In this paper, we adapted the AAA algorithm for use in the model order reduction of state space systems.  The first algorithm, system-AAA, gives satisfactory results for single-output systems, but does not perform as strongly when compared to balanced reduction with multi-output systems.  We also discussed a second algorithm, low-rank approximation, which removes the state dimension's dependence on the number of outputs.  Low-rank approximation fixes some issues with full interpolation and yields improved results with MIMO systems.  Numerical results show that this new algorithm performs similarly to balanced reduction with MIMO systems, and matches or exceeds its performace otherwise.  For single-output systems, both system-AAA and low-rank approximation are good alternatives to balanced reduction when the user needs a minimum order system.  Starting with a minimum order system and gradually increasing the order allows the user to choose the smallest system that meets certain \(\mcal{H}_\infty\) or \(\mcal{H}_2\) error requirements, which is an advantage over other model reduction techniques.  

In future work, we will investigate why both algorithms can produce unstable poles in the MIMO case.  We would like to find ways to further improve the performace of low-rank system-AAA, namely by ensuring the algorithm yields a stable, well-matched result on MIMO systems.  

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,paper_bib}

\end{document}