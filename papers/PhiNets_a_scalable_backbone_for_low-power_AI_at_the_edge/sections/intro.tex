\section{Introduction}

% Paper outline:
% \begin{enumerate}
% \item intro;
% \item related works (MIT Han lab, EfficientNets (v1-v2), MobileNets (v1-v2));
% \item Background work (why backbone is the place we work on, yolov2 - why not the others?);
% \item On mobilenets... how they scale? what is our contribution here?
% \item Experimental results;
% \item Results (det+class, tracking)
% \item Conclusions
% \end{enumerate}
Distributed intelligence is critical for smart cities since it significantly reduces infrastructure maintenance and is compliant with privacy regulations. In fact, with the considerable technological improvements of artificially intelligent systems, more and more applications need audio or video streams from which to extract meaningful information from complex scenarios. However, this presents a challenge for what concerns the present privacy regulation and infrastructure maintenance since the communication of the stream is very power-hungry and thus requires a power plug or frequent battery swaps.
The main threat from the privacy perspective is the possible leak of sensitive data (as non-anonymised video/audio streams) during the communication stage. The Privacy by Design (PbD) paradigm \cite{schaar2010privacy} demonstrates how edge AI is an asset from this perspective. The main advantage of edge processing consists in sending only non-sensitive and meaningful information extracted on edge. Thus, it is not required that data streams are stored locally,  another asset from the security perspective.
Moreover, this privacy paradigm is in line with recent trends in communicating and processing high-throughput data streams. An example of such an application scenario is video applications (up to 768 KB/s for a 10fps stream of QVGA grayscale images or 2.3MB/s for a 10fps stream QVGA RGB images). A wireless stream of this data is not supported by BLE5.0 (max 3fps grayscale QVGA). Therefore technologies as Ultra Wide Band or Wireless 6 need to be implemented but require one order of magnitude more energy than BLE5.0. 
Two approaches are proposed in the literature to address this problem; the first one exploits on-chip intelligence to compress images by removing redundant data during the acquisition stage \cite{7456200, RGB-PIXEL?, 6032080}. The other approach uses the near-sensor processing paradigm \cite{336244}, thus extracting only information relevant to the specific application directly on edge.
The two approaches are merged in some state-of-the-art methods, as for example \cite{paissan2021enabling, 8791337, 10.1145/3203217.3204463}, where the processing of the compressed frames is performed on edge, and only relevant information is transferred.
To conclude the comparison, it is essential to notice how edge processing affects the communication bandwidth, reducing the transmission bandwidth of a factor linear with the number of tracks in the frame for a Multi-Object Tracking application (128KB/s for 100 objects 10fps stream, regardless of the image resolution and number of channels). Therefore, edge processing allows decoupling the communication efficiency from the image resolution and type and represents an improvement of a factor around seven in data compression.

Despite the many benefits edge processing has, it also presents several challenges since it consists of computing on platforms where flash memory, RAM and computational cost are constrained to a fraction of the usual quantities implied in image processing. Typical IoT-oriented MCUs, such as those based on the ARM Cortex-M7 or ARM Cortex-M4 architectures, only have 100s of KB to 1MB of SRAM and a similar amount of Flash memory. Therefore, running a Deep Neural Network on these devices requires specific network adaptation oriented towards model compression. Many methods as quantization \cite{zhou2017incremental, jacob2018quantization} and pruning proved to work well in reducing the computational cost of neural networks but still require creating a benchmark to reduce.
On the other hand, mobile Neural Architecture Search (NAS) \cite{howard2017mobilenets, sandler2018mobilenetv2, tan2019efficientnet} presented state-of-the-art performance with a low computational cost and therefore is suitable to run on an MCU. Furthermore, models like MobileNets \cite{howard2017mobilenets, sandler2018mobilenetv2} and EfficientNets \cite{tan2019efficientnet, tan2021efficientnetv2} are usually optimised to work well on tasks as ImageNet classification. Hence, despite their computational cost, they can be exploited and finely tuned to work well in real-life scenarios without compromising the application domain.

The main contribution of our work is the design of a novel scalable backbone, \textit{PhiNet}, for detection and multi-object tracking on resource-constrained platforms. \textbf{sono arrivato qui} YOLOv2 \cite{redmon2017yolo9000} and Simple Online Realtime Tracking (SORT) \cite{bewley2016simple} which can run on off-the-shelf MCUs with state-of-the-art consumption (1.3mJ per frame). Our work has a meaningful impact in the fields of:
\begin{itemize}
\item embedded vision processing by proposing a new architecture family, \textit{PhiNets}, which push forward the state-of-the-art in tiny vision;
\item low-power image processing, since our pipeline requires only \textbf{1.3mJ per frame} or \textbf{13mW @ 10 fps};
\end{itemize}


