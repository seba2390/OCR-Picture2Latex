\section{Related works}

\subsection{Scalable backbones}
In our work, we propose an MCU-friendly MOT pipeline based on deep learning. For this, we benchmarked many state-of-the-art approaches.
Our focus is on optimising the backbone, which is the initial part of the neural network in charge of learning and extracting the features needed to localize, classify, detect, and track objects. %We choose to optimise the 
We focused on the backbone optimization since it accounts for the most considerable computational cost in the network inference. Thus, a reduction in the backbone complexity has a significant impact on the computational cost of the network.
Scalable backbones are an excellent solution for embedded processing since MCUs vary in resource availability, and different network architectures can be implemented based on the specific application domains.
 In the MobileNets paper, \cite{howard2017mobilenets, sandler2018mobilenetv2}, the authors presented a scalable backbone with good performance in vision benchmarks. In Howard et al. \cite{howard2017mobilenets}, the architecture is based on depth-wise separable convolutions and the authors presents a width and resolution multiplier. Both parameters are constrained in $[0, 1]$ (where 1 represents the standard architecture) and reduce parameter count and computational complexity quadratically. Instead, in Sandler et al. \cite{sandler2018mobilenetv2} inverted residual structures and linear bottlenecks represent the building blocks of the architecture. Moreover, the same scaling model presented by Howard et al. \cite{howard2017mobilenets} is applied.
Despite the model's ability to scale, the authors did not study any particular scaling model and its relationships with the model's performance in different vision tasks.
In the first EfficientNet paper \cite{tan2019efficientnet}, the authors propose a Neural Architecture Search (NAS) based approach for vision tasks that is also capable of scaling in depth. Moreover, they proved that scaling the neural network architecture one dimension at a time is less efficient than scaling all three dimensions (resolution, width, depth) simultaneously. The well-described compound scaling model, which consists of scaling all three dimensions by a power of the initial coefficients, proved to give a state-of-the-art performance in neural network scaling.
In EfficientNetV2 \cite{tan2021efficientnetv2}, the authors improved the model proposed by Tan et al. \cite{tan2019efficientnet} with training-aware NAS to optimise the model's parameter efficiency.

\subsection{Detection methods}
Object detection is a field of computer vision dealing with the detection of semantic objects in images. Different techniques have been developed in the past decades and will be hereafter compared based on the hardware implementation feasibility and computational complexity.
We can split the main detectors based on how many stages are required to extract the bounding boxes. The two-stage detectors, as for example Faster R-CNN \cite{ren2015faster} or Mask R-CNN \cite{he2017mask} are based on region proposal techniques that are then processed (i) to generate a region of interest or (ii) to perform object classification and bounding box regression. On the other hand, single-stage detectors such as YOLO (You Only Look Once) \cite{redmon2017yolo9000} or SSD (Single Shot Multibox Detectors) \cite{liu2016ssd} solve detection as a regression problem by learning to infer class probability and bounding box coordinates from input images. While two-stage detectors usually have higher accuracy scores with respect to single-stage detectors, they require more computational power and energy to infer a frame. Thus, to address MCU-friendly MOT applications, single-stage detectors are preferable. %Thus, single-stage detectors are the go-to solution for MCU-friendly MOT applications.

% \textbf{Why YoloV2 and single-shot?}

\subsection{Tracking methods}

Many object association techniques can be implemented for MOT, ranging from Intersection over Union (IoU) comparison to unified detection and tracking techniques \cite{zhang2020fairmot}. As it was for detection, there is a trade-off between computational complexity and performance, compared in terms of ID switches and MOTA score.
There are two categories of tracking algorithms, one in which algorithms perform tracking after a detection pipeline (e.g. SORT \cite{bewley2016simple}, DeepSORT \cite{wojke2017simple}, IoU) and the other category composed by algorithms that perform detection and tracking together (e.g. FairMOT \cite{zhang2020fairmot} and FairMOT Lite).

\begin{table}[ht]
\begin{tabular}{lcccc}
\hline
\multicolumn{1}{c}{} & IDs & MOTA \% & MOTP  & Hz (fps) \\ \hline
IoU                  & 287 & 61.6    & 0.116 & 5.33     \\ \hline
SORT                 & 48  & 54.3    & 0.172 & 5.31     \\ \hline
DeepSORT             & 47  & 55.9    & 0.175 & 3.64     \\ \hline
FairMOT              & 49  & 48.9    & 0.192 & 0.2      \\ \hline
FairMOT Lite         & 60  & 46.9    & 0.196 & 3.85     \\ \hline
\end{tabular}
\caption{Results of trackers on a sequence from MOT significant to smart cities environment. IoU, SORT, DeepSORT, and FairMOT lite use YOLOv5S as object detector.
The benchmarking is performed on a Intel(R) Core(TM) i9-10900KF CPU @ 3.70GHz}
\label{table:trackers}
\end{table}

Simple Online and Realtime Tracking (SORT) exploits the Hungarian association algorithm to associate bounding boxes from consecutive frames by maximizing the IoU score. The same approach is performed by Deep SORT, in which the score to be minimised is the distance (e.g. euclidean, cosine, correlation, etc.) between the latent representation of the content of the bounding boxes. This enables the algorithm to be more robust with respect to IoU because also visual attributes of the images are taken into account.

FairMOT instead is based on CenterNet \cite{duan2019centernet} and performs the detection and tracking together. This approach does not require anchors for the detection. In fact, the bounding box shape is inferred from the center of the blob. Some modification of this algorithm in which YOLOv5S is implemented in place of CenterNet is referred to as FairMOT Light here.

In Table \ref{table:trackers}, we quantitatively compared the presented trackers to help understanding trackers performance and computational cost (expressed in terms of frames per second). A more detailed discussion on the trackers is performed in Section \ref{sec:dettrack}.


\subsection{Vision-based MCU applications}
Although tiny vision is an emerging technology, the main focus of the literature is on detection and classification tasks, thus a subset of the tracking pipeline.
Some works explore both neural architectural optimisation and inference optimisation employing custom compilers and operations. In \cite{lin2020mcunet}, an MCU-oriented NAS (TinyNAS) is combined with a lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on MCU.
Industry-oriented tools as STM X-Cube-AI can be exploited to implement artificial neural networks on MCUs \cite{s20092638}, though compromising the inference performance with respect to manual network implementation via CMSIS-NN \cite{lai2018cmsis}.
On the other end, some works \cite{roma} exploit extreme parameter reduction using XNOR Networks. However, the performance of those approaches, and in general of Binary Neural Networks (BNNs) is notably lower than the one achieved by classical CNNs \cite{bulat2019xnor} thus a lot of application scenarios are not addressable with BNNs.
Another way to implement vision intelligence at the edge is by exploiting custom hardware architectures, which use parallel computing to speed up computation \cite{garofalo2020pulp, flamand2018gap}.

In this context, our work is towards the design of a novel scalable backbone, which maximise resource usage by decoupling the computational requirements of the neural network, thus not relying on custom hardware or compilers. %Therefore, it does not exploit custom hardware architectures nor custom compilers to achieve state-of-the-art results for MCU scale inference. 

In the following sections, we describe the proposed network architecture family and how it achieves good performance in object detection and tracking suitable for microcontroller-scale execution.

%\textcolor{red}{MANCA UN PO' UNA CHIUSURA sia di questo sottoparagrafo 2.4 (perchè mi parli di questi altri lavori? cosa hai imparato da essi e quindi come li continui oppure come ti differenzi da essi in questo lavoro?)
% sia forse (ma meno necessario) un gancio alla sezione successiva --
% senza hw-custom, con off the shelf e state of the art -- Thus, our work si inserisce in un'area ancora da esplorare
% scalable network ci spingiamo più giù con più modularità/flessibilità - scalando non solo in performance ma anche RAM, flesh e MACC mentre efficientnet perfomrance x macc
% }