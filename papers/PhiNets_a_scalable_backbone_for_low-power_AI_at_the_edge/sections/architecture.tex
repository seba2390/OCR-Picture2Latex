\section{\textit{PhiNets} Architecture}

When constraining computational cost and memory usage to fit neural networks on an MCU, scaling approaches like the ones presented in EfficientNet highlight the inefficiency of current state-of-the-art architectures. This is proved by the significant performance drop-offs on computer vision tasks when these networks are constrained to lower powered devices, as demonstrated by the authors in \cite{howard2017mobilenets} and also confirmed empirically in Fig. \ref{fig:detresults_params}.

\begin{figure}[h]
  \centering
  \includegraphics[width=10cm]{images/2-PhiNet_body_structure.jpg}
  \caption{An overview of the \textit{PhiNets} family network architecture. The proposed architecture scales with respect to the expansion factor $t_0$, number of convolutional blocks, shape factor $\beta$ and width multiplier as explained in Sec \ref{scaling}.}
  \label{fig:phinet_overview}
\end{figure}

In this work, we present \textit{PhiNets}, an efficient neural network family developed for MCU inference. \textit{PhiNets} aim at solving the main drawbacks of current state-of-the-art scalable backbones for image processing at the edge. They are a family of networks optimized for inference within the one to ten million MACCs range, tuned to minimize the performance's drop while scaling with the architecture specifications.

%An overview of the network architecture is shown in Fig. 

In the following subsections, we will introduce the main network building blocks (\ref{bblocks}), the main constraints when it comes to MCU inference (\ref{has}) and how \textit{PhiNets} solve this problem (\ref{scaling}). In the end, will we present the detection and tracking pipelines selected for the benchmarking (\ref{sec:dettrack}).

\subsection{Network building blocks}
\label{bblocks}
As it is common in parameter-efficient architectures, such as \cite{sandler2018mobilenetv2, tan2021efficientnetv2, he2016deep}, the network is composed of a sequence of inverted residual blocks (by default, there are seven blocks for our architecture), each followed by a swish activation function. Fig. \ref{fig:phinet_overview} shows the network architecture overview.

As illustrated in Fig. \ref{fig:convblock_structure}, the number of filters in the first bottleneck layer is $24 \alpha$, where $\alpha$ is a hyperparameter that works similarly to how it does in the MobilenetV2 architecture, while the multiplication factor gets doubled every time the feature map is downsampled in the network. Squeeze-and-Excitation blocks \cite{hu2018squeeze} are inserted after each convolutional block and skip connections are used between the same-resolution bottleneck layers as in MobileNetV2 \cite{sandler2018mobilenetv2}.
The expansion factor used in the inverted residual block of index $N$, where N equals 0 for the first layer, is determined by two hyperparameters, i.e. $t_0$ (\textit{base expansion factor}) and $\beta$ (\textit{shape hyperparameter}), as 
\begin{equation}
    t=t_0 \bigg( \frac{(\beta-1)N+B}{B} \bigg)
\end{equation}
Five strided convolutions are used for down-sampling the feature maps through the network with a spatial resolution reduction of a factor of $32\times$ between input and output tensors.

As the network has been primarily developed for object detection tasks, we maximized the receptive field of each element in the output grid, also considering the resolution of the output tensor. Reducing the latter too much would affect the spatial information flow through the network and significantly lower the performance \cite{bochkovskiy2020yolov4}. To tackle this issue, we placed a neck composed of a $2\times$ up-sampling layer and a skip connection to the latest convolutional block of the exact resolution after the sequence of convolutional blocks.
Our approach proved to be a computationally efficient method of optimizing the trade-off between high output resolution and output grid receptive field without negatively impacting the computational requirements of the network.

\begin{figure}[h]
  \centering
  \includegraphics[width=14cm]{images/2-PhiNet_block_structure.jpg}
  \caption{An overview of the \textit{PhiNets} convolutional block structure. First, the number of channels is increased with a pointwise convolution, followed by a depthwise convolution and SE block. Finally, a second pointwise convolution connects to the low dimensionality bottleneck block.}
  \label{fig:convblock_structure}
\end{figure}

\subsection{Hardware-constrained and hardware-aware scaling}
\label{has}
When bringing CNNs to edge devices, the general approach consists of (i) architectural space exploration to identify the best performing networks for a given task (ii) selection of the best network fitting in the most restrictive resource constraints. This approach is known as hardware-constrained network architecture search and is the most often used technique for architecture search in constrained devices. While this helps optimise a network for a very resource-limited device such as a microcontroller, this approach results in a network that provides sub-optimal resource utilization of the platform, as only the most stringent requirement is met. But, when working with a resource constrained platform, we usually need to face three different constraints:
\begin{itemize}
    \item the number of operations (MACC) required for network inference. High-performance microcontrollers, coupled with efficient inferencing frameworks, can achieve tens to thousands of MACC per second. For real-time video applications, this means that a top-of-the-line MCU can run a $10MMACC$ detection network at $\approx 10Hz$;
    \item the dynamic memory (working memory - WM). When executing a network's computational graph, at each point, the CPU must compute a matrix multiplication between the output of the previous layer or the input of the network and the following channel filter matrix. The RAM usage is determined by the size of these tensors, plus the memory required to keep the tensors for skip connections for later usage;
    \item the static memory (parameter memory - PM). As FLASH memory is the most expensive part of the microcontroller's die both in terms of cost and area, this can usually contain $100KB$ to $1MB$ of data. Assuming that all network parameters get quantized to 8bit integers, a maximum of $100K$ to $1M$ parameters can be used, based on the selected platform.
\end{itemize}
Our work proposes an optimized architecture family, \textit{PhiNets}, which inverts the hardware network architecture search paradigm, using a \textit{hardware-aware} network scaling pipeline. Resource constraints can be met with minimal performance loss by varying different sets of hyperparameters. Moreover, eventual performance bottlenecks can be easily identified thanks to the structure of the network.


\subsection{Meeting the requirements: decoupling MACC and memory usage efficiently}
\label{scaling}
Resource usage for the three main hardware constraints of the network can be optimized in a decoupled way, i.e., using different hyperparameter combinations to meet different resource constraints and achieve optimal use of the available hardware. This optimization will allow for superior performance with respect to networks generated using hardware-constrained scaling techniques. The following sections will highlight how the network parameters are connected to the resource requirements for \textit{PhiNets}.

\subsubsection{Number of operations}
The number of operations (MACC) for the baseline network depends on network input resolution $w\times h$, on the network width, which scales quadratically with the parameter $\alpha$, and on the network depth (determined by the number of blocks $B$). These parameters can be tuned using a compound scaling methodology as proposed in \cite{tan2021efficientnetv2} to obtain the best performing network for a given operation count or can be determined by other implementation factors (e.g. the resolution of the camera used can set a fixed $w\times h$).

The parameters $w\times h$, $\alpha$ and $B$ determine the number of operations for the base network. This can be defined by real-time requirements for the system, power consumption targets, or accuracy requirements. Fig. \ref{fig:macc_dependency} shows the effects of the three parameters on the complexity of the network.

\begin{figure}[H]
  \centering
  \includegraphics[height=5cm]{images/macc_dependency.pdf}
  \caption{\textit{PhiNets} computational complexity with respect to $\alpha$, $w\times h$ and $B$. The plots highlight an exponential increase in MACC count with the number of filter and input resolution, while a linear trend with respect to the number of convolutional blocks.}
  \label{fig:macc_dependency}
\end{figure}



\subsubsection{Dynamic memory}
The dynamic memory will be determined by the size of the tensors in the expansion layers of the first convolutional block. The tensor size in the later layers increases linearly with the network's depth and decreases quadratically with resolution. Moreover, no tensors need to be kept in memory for the first block as there are no previous layers with residual connections. Varying the base expansion factor $t_0$  scales the dynamic memory required by the network linearly. Note that it is recommended to keep this parameter between $2$ and $8$, using by default $6$ for networks larger than $5MMACC$ and $5$ for networks smaller than that. Fig. \ref{fig:ram_flash_dependency} (left) shows the linear effects of the expansion factor on the working memory requirements of the network.

\subsubsection{Parameter memory}
The parameter memory is determined by the convolutional kernels in the network. In particular, it is determined by the size of the kernels used in the expansion layers of the later network blocks, where the tensors usually have a low spatial resolution, but a high number of filters is used. Given how the expansion factor of later layers is related to $\beta$, the parameter memory required by the network varies with this parameter. In particular, the relationship between the number of parameters in the network and the shape hyperparameter is modelled as $\#Params \approx C\cdot \frac{1}{2}(1+\beta)$ with $C$ number of parameters of the base network ($\beta=1$). The Fig. \ref{fig:ram_flash_dependency} (right) shows the effects of the shape factor on the number of parameters of the network.

\begin{figure}[H]
  \centering
  \includegraphics[height=5cm]{images/ram_flash_dependency.pdf}
  \caption{ Left: \textit{PhiNets} RAM requirements from a default network by varying $t_0$. Right: \textit{PhiNets} FLASH requirements from a default network by varying $\beta$. As shown, the trend is linear in both cases.}
  \label{fig:ram_flash_dependency}
\end{figure}

\subsubsection{Network tuning strategy}


While tuning the network, it is recommended to optimize the number of operations of the base network firstly, then RAM and FLASH usage. In particular, a general network-tuning approach can be as follows:
\begin{enumerate}
    \item Estimation of the available time for an inference operation and estimate of the corresponding MACC count given the platform performance;
    \item Selection of the hyperparameters $w\times h$, $\alpha$ and $B$, based either on a compound scaling technique, from a default architecture, or with a network architecture search algorithm, to achieve the correct number of operations;
    \item Tuning of the $t_0$ hyperparameter knowing input resolution and available WM from the size of input and output tensors for the first convolutional block (assuming network weights and activations get quantized to 8bit integers)
\begin{equation*}
    WM\approx \bigg(\frac{w}{2}\times\frac{h}{2}\cdot 24\alpha + \frac{w}{4}\times\frac{h}{4}\cdot 24\alpha \bigg) t_0
\end{equation*}
    For example, going from $t_0=5$ to $t_0=4$ decreases the needed RAM for the network by $20\%$;
    \item Tuning of the $\beta$ hyperparameter to achieve the desired number of parameters. Knowing the number of parameters of the starting architecture $P_0$ and the target $P_{target}$, we can obtain $\beta$ from:
\begin{equation*}
    \beta \approx 2\frac{P_{target}}{P_0}-1
\end{equation*}
    For example, going from $\beta=1$ to $\beta=0.6$ decreases the number of parameters by $20\%$;
\end{enumerate}
The optimization procedure can be repeated multiple times if higher precision is required, as varying $t_0$ and $\beta$ has second-order effects on the number of operations.

% \begin{figure}[H]
%   \centering
%   \includegraphics[height=8cm]{images/pairplot.pdf}
%   %\caption{\textit{PhiNets} RAM scaling from a default network by varying $t_0$.}
%   %\label{fig:convblock_structure}
% \end{figure}

\subsection{Detection and tracking}
\label{sec:dettrack}
Nowadays, there are many alternatives for both detection \cite{he2017mask, bochkovskiy2020yolov4, liu2016ssd} and tracking \cite{zhang2020fairmot, bewley2016simple, wojke2017simple}. We took into account the algorithms considering the trade-off between computational cost and performance. For the object detection task, we used the YOLOv2 \cite{redmon2017yolo9000} detection head working at a single scale, as this requires only a single convolutional layer for bounding box prediction and class identification of all objects in the frame, leading to minimal operation count networks. Choosing YoloV2 also helps in embedded processing since the computational complexity is affected mainly by the \textit{PhiNet} architecture and does not directly depend on the number of objects in the frame.

For the tracker, we used a tracking-by-detection pipeline based on the proposed object detector. Different alternatives like SORT, DeepSORT and IoU association can be considered to work with our architecture. All of them have a computational cost that scales linearly with the number of objects to be tracked, thus implying a limit in the number of possibly tracked objects in resource-constrained platforms targeting real-time applications.
Moreover, while IoU is the shallowest concerning the operation count, it has many ID switches, thus performing worse than SORT and DeepSORT, considering this critical metric in low-fps applications, where an object might be in the scene a total of 10-20 frames. Between DeepSORT and SORT, we selected SORT since having the embedding extractor as in the DeepSORT architecture implies using a lower complexity object detector. In tracking-by-detection pipelines, it is crucial to have good detection performance since it directly impacts tracking performance. Thus, we are interested in having the best performing detector possible since it will help the shallower tracker achieve the same MOTA score as the more complex one, working with a worse detector. 

