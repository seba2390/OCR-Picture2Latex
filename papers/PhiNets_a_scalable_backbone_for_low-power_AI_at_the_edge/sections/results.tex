\section{Results}

Since, as already described, we used a tracking-by-detection pipeline, and the tracking performance is highly dependent on the detections, we decided to split the benchmarking in detection performance, tracking performance, and power consumption to show how each component of our pipeline was performing.

\subsection{Baseline architectures}
\textit{PhiNets} baseline architectures were tested, with the parameters summarized in the Table \ref{table:params}.

\begin{table}[htbp]
\begin{tabular}{cccccccl}
\textbf{Resolution} & \textbf{$\alpha$} & \textbf{$B$} & \textbf{$\beta$} & \textbf{$t_0$} & \textbf{MACC} & \textbf{Parameters} & \textbf{Task} \\ \hline
$128 \times 128$ & $0.35$ & 7 & 1 & 6 & 9.85 M  & 61.2 K & Detection \\
$128 \times 128$ & $0.25$ & 7 & 1 & 6 & 6.08 M  & 37.9 K & Detection \\
$96 \times 96$   & $0.25$ & 7 & 1 & 5 & 3.01 M  & 31.8 K & Detection \\
$96 \times 96$   & $0.15$ & 7 & 1 & 5 & 1.23 M  & 14.3 K & Detection \\
\hline
$160 \times 160$ & $0.3$  & 7 & 1 & 5 & 10.42 M & 39.9 K & Tracking  \\
$160 \times 160$ & $0.2$  & 7 & 1 & 5 & 4.96 M  & 21.6 K & Tracking  \\
$128 \times 128$ & $0.2$  & 7 & 1 & 5 & 3.18 M  & 21.6 K & Tracking 
\end{tabular}
\caption{Parameters for generating the benchmarked \textit{PhiNets}}
\label{table:params}
\end{table}

Networks have been grouped by task, as we found that a slightly lower resolution but a higher number of filters benefited, at the same MACC count, more the detection task than the tracking one, for which a higher resolution proved to be the better choice.

\begin{figure}[H]
  \centering
  \includegraphics[height=5cm]{images/dataset_adaptation.pdf}
  \caption{Number of objects removed per frame from COCO and VOC2012 datasets based on area constraint. The removal prevents having objects which are represented by only a couple of pixels on the down-sampled image (input of the backbone).}
  \label{fig:data}
  %\label{fig:convblock_structure}
\end{figure}

\subsection{Detection}
To evaluate object detection performance towards tiny multi-object tracking, we trained EfficientNets, MobileNets and \textit{PhiNets} sized between $1M$ and $10M$ MACC on a subset of the MS COCO \cite{lin2014microsoft} and VOC2012 \cite{pascal-voc-2012} object detection benchmarks.

\begin{figure}[h]
  \centering
  \includegraphics[width=14cm]{images/macc_vs_map.pdf}
  \caption{Comparison of scalable backbones applied before a YOLOv2 detection head for MCU-scale object detection considered in terms of  mean Average Precision (mAP) vs MACC count. The best fitting and performing models for MCU inference are the ones in the top-left area of the plot (requiring less operations with better performance).}
  \label{fig:detresults_macc_1}
\end{figure}


Given the application constraints of tiny vision, mainly regarding the input resolution, we reduced the training set by considering only the "person" class and by using only targets whose bounding box was more extensive in area than $1/64$ of the original image size. This pre-processing of the dataset is visualised in Fig. \ref{fig:data}, where we investigated the number of objects per frame in both datasets and the number of removed objects based on the above constraints.

The networks were trained for 60 (COCO) / 120 (VOC) epochs, using the Adam optimizer. The first three epochs for both datasets are used for warm-up, with a $5\times 10^{-3}$ learning rate, while for the remaining epochs, we used cosine decaying on the learning rate, starting from $1\times 10^{-2}$.

As shown in Fig. \ref{fig:detresults_macc_1}, all the \textit{PhiNets} perform better in object detection on the 1-10 MMACC range, given their advanced scalability features. In fact, \textit{PhiNet}'s performance is almost constant with respect to the number of MMACC, while EfficientNets and MobileNets have a performance drop greater that 15\% mAP in the depicted MACC range.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=14cm]{images/params_vs_map.pdf}
  \caption{Comparison of scalable backbones applied before a YOLOv2 detection head for MCU-scale object detection. The plots show a vertical section when we downsized the network by lowering the input resolution, keeping the same number of filters (and thus parameters), in order to avoid drastic performance drop in EfficientNets and MobileNets. The best fitting and performing models for MCU inference are the ones in the top-left area of the plot (requiring less parameters with better performance).}
  \label{fig:detresults_params}
\end{figure}

We achieved the same behaviour also in the analysis with respect to the number of parameters, depicted in Fig. \ref{fig:detresults_params}, proving that \textit{PhiNets} are more efficient in parameter count than previous state-of-the-art architectures. In conclusion, \textit{PhiNets} set a new standard for embedded object detection on MCUs by achieving higher performance with less operations and parameters.

\subsection{Multi-Object Tracking}

After the detection, we perform multi-object tracking using SORT \cite{bewley2016simple}. We benchmarked the proposed backbones performance on the MOT15 dataset \cite{leal2015motchallenge} after augmenting the training of the detectors with 360 epochs on the benchmark data.

The relationship between detection and tracking performance in tracking-by-detection pipelines is intuitively linear. In fact, since the tracking is based on the detection IoU, a bad detector implies low tracking performance. Thus, as expected by analysing the detection results, and as we empirically proved in Fig. \ref{fig:mot_res}, \textit{PhiNets} are the best performing backbone for tracking in the 1-10MMACC range.

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{images/mot15_tracking.pdf}
  \caption{Results for MOT15 tracking showing the relation between Multi Object Tracking Accuracy (MOTA) score and network complexity (in terms of MACC count - left - and parameters count - right). The plots show a vertical section when we downsized the network by lowering the input resolution, keeping the same number of filters (and thus parameters), in order to avoid drastic performance drop in EfficientNets and MobileNets. The best fitting and performing models for MCU inference are the ones in the top-left area of the plot (requiring less parameters and operations with better performance).}
  \label{fig:mot_res}
\end{figure}

\subsection{Power consumption}

In order to test the power consumption of the network on a off-the-shelf MCU, a prototype hardware board (shown in Figure \ref{fig:hw_prototype}) was developed, based around an \texttt{STM32H743} microcontroller, with $2MB$ of internal Flash and $1MB$ of RAM. 
\begin{figure}[h]
  \centering
  \includegraphics[width=12cm]{images/hw_prototype.png}   
  \caption{The system hardware prototype realized. The board includes an \texttt{STM32H743} MCU, switch mode power regulator, a DCMI camera interface and display connection, external FLASH and DRAM and a bluetooth section (FLASH, DRAM and BLE are not needed for this application).  A: Layout of board and functional blocks.  B: Final prototype picture, with soldered components}
  \label{fig:hw_prototype}
\end{figure}
The MCU was powered at $V_{dd}=1.8V$ from a switch-mode power supply, and the internal LDO powering the core was set to output $1.1V$. The microcontroller was run at a constant frequency of $300MHz$, as this allowed the best efficiency in terms of energy requirements per network inference run. This can be seen from the graphs in Fig. \ref{fig:pwr_eff}, where we analyze the effects of different running frequencies and MCU core voltages on the required current (and, consequently, energy consumption).



\begin{figure}[htbp]
  \centering
  \includegraphics[width=12cm]{images/efficiency_pwr.png}
  \caption{Trade-offs between core voltage, running frequency and power consumption for the target MCU.
  Left: current absorbed by the MCU at different running speeds when computing inference.
  Right: estimated energy per inference with the reference 1.23 MMACC network. The platform shows the minimum energy consumption running at 200 or 300MHz when powered at 1.8V. Different colors show the possible frequency ranges that can be achieved with a given core voltage. A higher core voltage (dotted line) increases power consumption without bringing additional benefits.}
  \label{fig:pwr_eff}
\end{figure}


The energy required by the platform for an inference pass was estimated by sampling the current through a shunt resistor at the input of the hardware prototype. Empirically, we sampled the current $I(\tau)$ every $t_s=10\mu s$, and computed the energy required for inference using the relationship  $E=V_{dd} \sum_\tau I(\tau) t_s$.

\begin{figure}[htbp]
  \centering
  \includegraphics[height=5cm]{images/Inference_energy.pdf}
  \caption{Linear relationship between MACC and energy requirements for the realized hardware prototype. As energy required is, in a first approximation, directly dependent on network complexity, we can interpolate data points for different networks to obtain an estimation for all possible working points of the platform (Fig. \ref{fig:average_power_fps})}
  \label{fig:Inference_energy}
\end{figure}

We investigated the relationship between computational complexity and energy required per inference in Fig. \ref{fig:Inference_energy}. After interpolating the data, we found that the relationship between MACC count and energy requirements is, in a first approximation, linear with a coefficient of around 1.2 mJ/MMACC. This, can be exploited to estimate the power consumption at different frame rates, as depicted in Fig. \ref{fig:Inference_energy}. Estimated power is computed using the data from the microcontroller's datasheet for the current in low power (between frames) and the data obtained above for the run mode current, at a certain inference time $t_L=\frac{1}{fps}$


This choice of hardware and software allowed for a state of the art power consumption of under $1.3mJ$ for the $1.2MMACC$ \textit{PhiNet} ($53.7$ / $60.3$ mAP on a subset of the COCO/VOC2012 datasets) and $11.8mJ$ for the $9.8MMACC$ \textit{PhiNet} ($64.1$ / $73.9$ mAP on COCO/VOC2012, $60.8$ MOTA on MOT15).

Figure \ref{fig:average_power_fps} shows the working points that can be reached with the proposed hardware and software with respect to performance, power consumption, and inference speed. The platform is capable of running object detection at over $50$ fps with the proposed hardware, at a power consumption from $1.3mW/fps$ (for networks achieving $53.7$ / $60.3$ mAP on the selected subsets of COCO/VOC datasets) to $11.8mW/fps$ ($64.1$ / $73.9$ mAP on COCO/VOC), or, in other words, $10fps$ tracking at $13mW$ to $118mW$, depending on the performance required by the specific application.

\begin{figure}[ht]
  \centering
  \includegraphics[height=8cm]{images/Average_power_fps.pdf}
  \caption{Possible working points of the platform, with respect to latency, performance and power consumption. Networks with higher precision are in cyan, networks with higher performance and lower power requirements in purple. Smaller networks can achieve either very low power consumption figures, or very high speeds, with, in specific fields, small accuracy losses with respect to larger ones. A usage example will be presented in section \ref{sec:example}}
  \label{fig:average_power_fps}
\end{figure}

\subsection{Case study: monitoring application}
\label{sec:example}
A brief case study is here presented considering a scenario akin the ones presented in the sequences \texttt{TownCentre}  (top-left in Fig \ref{fig:detresults_macc}) and \texttt{PETS09} (bottom-left in Fig \ref{fig:detresults_macc}), where a camera is mounted on an elevated position, overseeing a walkway.

\begin{figure}[h]
  \centering
  \includegraphics[scale=.25]{images/detection_examples.png}
  \caption{Visual example of detection and tracking pipeline's output.}
  \label{fig:detresults_macc}
\end{figure}

Since the camera is mounted on an elevated point, there are no significant variations in the size and pose of the targets, thus allowing good tracking performance using smaller networks, which require lower power consumption. For example, the resulting MOTA score on \texttt{PETS09} is $62.86$ for the smallest \textit{PhiNet} and $68.51$ for the largest one tested. Given the large area seen by the camera, we can use low frame rates since targets will require multiple seconds to cross from one side of the image to the other. We can estimate the required energy usage for always-on tracking from Fig \ref{fig:average_power_fps}.

Given the target experimental results, we can use the smallest tracking network presented in Table \ref{table:params}, at $3.18 MMACC$ complexity. Fig. \ref{fig:Inference_energy} shows the measured energy consumption per inference at $3.21 mJ$, or $16 mW$ at the chosen framerate. A similar result can be extrapolated from the plot in Fig. \ref{fig:average_power_fps}, knowing the target frame rates and noting that the network we are using is on the left side of the mAP bar.

Such low energy requirements are ideal for always-on IoT nodes, and can run from a tiny solar panel. We tested the endnode using a $9cm \times 5cm$ monocrystalline solar panel, producing a peak power of around $913mW$. Assuming a conversion efficiency of $85\%$, a single hour under the sun could allow the system to run for two days uninterrupted.

%\textcolor{blue}{Ho modificato un po' la struttura del paragrafo e anche dell'ultima parte, mettendola piu "abbiamo provato". vedi se puo andare -- il tuo testo in IT è commentato nel latex}

%NB: ho preso i dati dalla mia tesi (il pannello è quello comprato da digikey [ https://www.digikey.it/product-detail/it/anysolar-ltd/SM531K08L/SM531K08L-ND/9990469] e l'efficienza quella risultante dal caricabatterie che avevo progettato). Dobbiamo giustificare i valori in qualche modo? $85\%$ di efficienza non è fuori dal comune ma non saprei come giustificarla (potremmo mettere in ref il datsheet del convertitore utilizzato ma temo non lo vedano di buon occhio, dall'efficienza da datasheet a quella pratica di solito c'è un po' di divario)