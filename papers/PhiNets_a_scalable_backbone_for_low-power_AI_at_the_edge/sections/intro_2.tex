\section{Introduction}
Over the past decade, we have witnessed two parallel trends. On one side, the increasing popularity of the internet of things, i.e., intelligent networked things everywhere, a consequence of the growing capabilities of the embedded systems, enhanced with capable processing units working at always increasing frequencies and offering attractive low-power modes \cite{cortex-m, raspberry, friendlyelec}. 
On the other side, with the advent of deep learning techniques, machine learning algorithms' size grows exponentially, thanks to the improvements in processor speeds and the availability of large training data. However, %as illustrated by Cerutti et. al in \cite{Cerutti2019}, 
embedded systems cannot sustain the resource requirements of standard deep learning techniques, adequate for GP-GPUs \cite{Cerutti2019, Rusci2018, hao2021enabling}.

How then to compose the need for exploiting the opportunity to bring intelligence at the edge with the complexity of deep learning?
In this context, the junction point is TinyML \cite{wang2020convergence, xu2020edge}, a cutting-edge field that brings the machine learning (ML) transformative power to the performance- and power-constrained domain of tiny devices and embedded systems.

Among the several application domains in which exploring this novel trend, computer vision is one of the most popular. In this domain, object detection and tracking should be real-time, reliable, and accurate. Recent best-performing pipelines for multi-object detection and tracking imply using many computational resources, thus substantially limiting the application scenarios in which such techniques can be exploited. The current limitations mainly depend on the high computational cost of state-of-the-art techniques, which require GPUs for real-time inference and thus are not a good candidate for resource-constrained devices.

%In terms of computational complexity - which is the most substantial constraint -, the most efficient solution for tracking pipelines is a tracking-by-detection approach. 
The most efficient solutions for Multi-Object Tracking (MOT) follows the tracking-by-detection paradigm. The tracking algorithm consists of an association algorithm based on the detected bounding boxes. Many pipelines are available for object detection. In particular, the main difference for what concerns object detection is the number of stages required to detect objects in one frame. The detection pipeline can be one-stage (single-shot bounding box regression) \cite{bochkovskiy2020yolov4, liu2016ssd} or two-stage (region proposal, object identification) \cite{he2017mask, ren2015faster}. The one-stage detectors are the most efficient from the computational complexity perspective; thus, they are the go-to solution for lightweight, real-time object detection.

The most popular one-stage detectors are YOLO and SSD. The core of the detection pipeline is the convolutional backbone for latent space representation of the images, which accounts for most of the Multiply-Accumulate operations (MACC). After that, the detection head is applied to the output of the backbone and outputs the bounding box regression.
Since the most expensive part of the pipeline is the backbone, many works have proposed scalable architectures to improve the efficiency in image processing \cite{howard2017mobilenets, tan2021efficientnetv2}. The ability of the networks to change computational requirements is an asset for embedded inferencing since the embedded platforms are diverse in hardware constraints. For example, typical IoT-oriented MCUs, such as the STM32F7 MCU or STM32L4, only have 320kB SRAM/1MB Flash and 32KB SRAM/256KB Flash, respectively, thus they cannot run the same networks. Also, developing a CNN architecture to tackle vision problems efficiently is not a new problem. Networks using minimal resources, such as MobileNets \cite{howard2017mobilenets} and EfficientNets \cite{tan2019efficientnet}, have already been optimized to allow state-of-the-art performance with less than $1B$ Multiply and Accumulate operations (MACCs), but with low performance on MCU scale computational resources.

Our work contributes to the state-of-the-art by proposing a novel scalable backbone,  \textit{PhiNets}, for detection and multi-object tracking on resource-constrained platforms. 
We prove the efficiency of \textit{PhiNets} by comparing them with existing lightweight backbones within a YOLOv2 \cite{redmon2017yolo9000} detection head and Simple Online Real-time Tracking (SORT) tracker \cite{bewley2016simple}.
Moreover, we implemented the tracking inference on off-the-shelf MCUs with state-of-the-art consumption (1.3mJ per frame).
%Moreover, we demonstrate the tracking inference on a prototype based on H7... and... obtaining... quantit√† varie on this task...
Thus, our work has a meaningful impact in the fields of:
\begin{itemize}
\item embedded vision processing by proposing a new architecture family, \textit{PhiNets}, which pushes forward the state-of-the-art in object detection and  on tiny devices;
\item low-power image processing, since our pipeline requires only 1.3mJ per frame or 13mW at 10 fps;
\end{itemize}