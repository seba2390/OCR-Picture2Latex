
\section{Learning with the Mirrored-Human dataset}
\subsection{Mirrored-Human}


Based on our framework, a large-scale Internet dataset can be built for the training of single-view tasks. 
The existing datasets~\cite{h36m_pami, mono-3dhp2017} lack the variety of both appearances and poses, making the training easy to overfit. For multi-person tasks, collecting data is more difficult . 
Therefore, previous methods exploit MuCo~\cite{mehta2018single}, a pseudo multi-person dataset composited from MPI-INF-3DHP~\cite{mono-3dhp2017} by masks, or JTA~\cite{fabbri2018learning}, a synthetic dataset. The gap between these datasets and the real scene may limit the performance of learning-based methods.

To alleviate the training data issue, we provide a large-scale Internet dataset named Mirrored-Human with our framework. Specifically, we collect a large number of videos from the Internet, in which we can see the person and the personâ€™s mirror image. Actions cover dancing, fitness, mirror installation, swing practice, etc. Fig.~\ref{fig:dataset} demonstrates both the appearance and pose diversity of our dataset. 
Table~\ref{tab:datasets} shows a thorough comparison between our dataset and relevant datasets. 
Please refer to our supplementary material for more details of our dataset.


\subsection{Single-person mesh recovery}

\input{tables/sota}
For this task, we choose MeshNet~\cite{Moon_2020_ECCV_I2L-MeshNet}, a state-of-the-art method for single-view 3D pose estimation, for evaluation. Two datasets are used for evaluation. Human3.6M~\cite{h36m_pami} is an indoor benchmark with 3D annotations. 3DPW~\cite{vonMarcard2018} is an outdoor benchmark to test the generalization ability and only its defined test set is used. Following standard protocols, we report both MPJPE and PA-MPJPE. We also test the baseline method that uses the state-of-the-art optimization-based method SMPLify-X~\cite{SMPL-X:2019} to generate pseudo ground-truth to train the same network. Table~\ref{tab:sota_mesh} shows that with our dataset, the performance of MeshNet can be improved significantly, especially when tested on the 3DPW dataset without using training data from 3DPW. We also outperform the baseline, indicating that our framework is more accurate than \cite{SMPL-X:2019}.

\subsection{Multi-person 3D pose estimation.}
\begin{table}[t]
	\begin{center}
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{clccc}
	\hline
 		& Methods & AP$_{root}^{25}\uparrow$ & PCK$_{rel}\uparrow$ & PCK$_{abs}\uparrow$\\ \hline
 		\multirow{7}{*}{TD} & LCRNet~\cite{rogez2017lcr} & - & 53.8 & - \\
 		& LCRNet++~\cite{rogez2019lcr} & - & 70.6 & - \\
 		& Dabral.~\cite{dabral2019multi} & - & 71.3 & - \\
 		& PandaNet~\cite{benzine2020pandanet} & - & 72.0 & -\\
 		& HMOR~\cite{li2020hmor} & - & 82.0 & \textbf{43.8} \\
        & Moon.~\cite{Moon_2019_ICCV_3DMPPE} & 31.0 & 81.8 & 31.5 \\
        \rowcolor{gray!10}
 		&Moon.~\cite{Moon_2019_ICCV_3DMPPE}+MiHu & \textbf{42.2} & \textbf{82.3} & 43.0 \\ \hline
 		\multirow{4}{*}{BU} & Mehta.~\cite{mehta2018single} & - & 65.0 & - \\
 		& Xnect~\cite{mehta2019xnect} & - & 70.4 & - \\
 		&SMAP~\cite{zhen2020smap} & 37.3 & 73.5 & 35.4 \\
 		\rowcolor{gray!10}
 		&SMAP~\cite{zhen2020smap}+MiHu & \textbf{42.3} & \textbf{74.1} & \textbf{38.0} \\ 
  	    \hline
	\end{tabular}
	}
	\end{center}
	\caption{Results on the MuPoTS-3D dataset. The numbers are calculated for all people. `MiHu' is our Mirrored-Human dataset. `TD' and `BU' mean `top-down' and `bottom-up', respectively.}
	\label{tab:sota_multi}
\end{table} 
For this task, previous methods fall into two categories. Top-down methods detect human first and then estimate keypoints with a single-person pose estimator. Bottom-up methods localize all keypoints in the image first and then group them into people. We choose the top-down method~\cite{Moon_2019_ICCV_3DMPPE} and the bottom-up method~\cite{zhen2020smap} for evaluation.

The MuPoTS-3D~\cite{mehta2018single} dataset is used. Following previous methods~\cite{Moon_2019_ICCV_3DMPPE,zhen2020smap}, AP$_{root}^{25}$, PCK$_{rel}$ and PCK$_{abs}$ are measured. AP$_{root}^{25}$ is the average precision of 3D human root location, which treats the prediction as correct if it lies within 25cm from the ground-truth. PCK$_{rel}$ is the percentage of correct keypoints after root alignment. A keypoint is correct if the distance between the prediction and the ground-truth is smaller than 15cm. PCK$_{abs}$ has almost the same definition as PCK$_{rel}$, but without the root alignment it measures the absolute pose accuracy. Note that AP is calculated only for the root and PCK is for all keypoints.

It can be observed from Table~\ref{tab:sota_multi} that with our dataset, AP$_{root}^{25}$ and PCK$_{abs}$ are improved significantly compared with \cite{Moon_2019_ICCV_3DMPPE}. For bottom-up methods, we also improve the performance of \cite{zhen2020smap} apparently.
