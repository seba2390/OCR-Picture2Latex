\section{Method}\label{method}
Our multilingual moment retrieval model \ModelName~is built on top of the Cross-model Moment Localization (XML)~\cite{lei2020tvr} model, which performs efficient video-level retrieval at its shallow layers and accurate moment-level retrieval at its deep layers. 
To adapt the monolingual XML model into the multilingual setting in \DsetName~and improve its efficiency and effectiveness, we apply encoder parameter sharing and neighborhood constraints~\cite{wang2018learning,kim2020mule} which encourages the model to better utilize multilingual data to improve monolingual task performance while maintaining smaller model size.

\paragraph{Query and Context Representations.}
We represent videos using ResNet-152~\cite{he2016deep} and I3D~\cite{carreira2017quo} features extracted every 1.5 seconds. 
We extract language features using pre-trained, then finetuned (on our queries and subtitles) RoBERTa-base~\cite{liu2019roberta}, for English~\cite{liu2019roberta} and Chinese~\cite{cui2020revisiting}, respectively.
For queries, we use token-level features. 
For subtitles, we max-pool the token-level features every 1.5 seconds to align with the video features. 
We then project the extracted features into a low-dimensional space via a linear layer, and add learned positional encoding~\cite{devlin2018bert} after the projection. 
We denote the resulting video features as $E^v \in \mathbb{R}^{l \times d}$, subtitle features as $E^s_{en} \in \mathbb{R}^{l \times d}, E^s_{zh} \in \mathbb{R}^{l \times d}$, and query features as $E^{q}_{en} \in \mathbb{R}^{l_q \times d}, E^{q}_{zh} \in \mathbb{R}^{l_q \times d}$. $l$ is video length, $l_q$ is query length, and $d$ is hidden size. The subscripts $en$ and $zh$ denote English and Chinese text features, respectively. 

\begin{figure}[!t]
\begin{center}
  \includegraphics[width=0.9\columnwidth]{res/tvrm_encoding.pdf}
  \vspace{-3pt}
  \caption{
  Illustration of \ModelName's encoding process.
  Compared to monolingual models, \ModelName~learns from the two languages simultaneously, and allows them to benefit each other via \textit{encoder parameter sharing} and \textit{neighborhood constraints}. 
  We show the detailed encoding process of the model in the appendix (Figure~\ref{fig:mxml_overview}).
  }
  \label{fig:tvrm_encoding}
  \vspace{-8pt}
  \end{center}
\end{figure}



\paragraph{Encoders and Parameter Sharing.}
We follow~\citet{lei2020tvr} to use \textit{Self-Encoder} as our main component for query and context encoding. 
A Self-Encoder consists of a self-attention~\cite{vaswani2017attention} layer, a linear layer, and a residual~\cite{he2016deep} connection followed by layer normalization~\cite{ba2016layer}.
We use a Self-Encoder followed by a modular attention~\cite{lei2020tvr} to encode each query into two modularized query vectors $\boldsymbol{q}^{v}_{lang}, \boldsymbol{q}^{s}_{lang} \in \mathbb{R}^{d}$ ($lang\in \{en, zh\}$) for video and subtitle retrieval, respectively.
For videos, we apply two Self-Encoders instead of a Self-Encoder and a Cross-Encoder as in XML, because we found this modification simplifies the implementation while maintains the performance.
We use the outputs from the first and the second Self-Encoder $H^v_{vr,lang}, H^v_{mr,lang} \in \mathbb{R}^{l \times d}$ for video retrieval and moment retrieval. 
Similarly, we have $H^s_{vr,lang},H^s_{mr,lang} \in \mathbb{R}^{l \times d}$ for subtitles.
All the Self-Encoders are shared across languages, e.g., we use the same Self-Encoder to encode both English and Chinese queries, as illustrated in Figure~\ref{fig:tvrm_encoding}. 
This parameter sharing strategy greatly reduces the model size while maintaining or even improving model performance, as we show in Section~\ref{sec:experiments}.

\paragraph{Language Neighborhood Constraint.}
To facilitate stronger multilingual learning, we add neighborhood constraints~\cite{wang2018learning,kim2020mule,burns2020learning} to the model. 
This encourages sentences that express the same or similar meanings to lie close to each other in the embedding space, via a triplet loss.
Given paired sentence embeddings $\boldsymbol{e}_{en}^{i} \in \mathbb{R}^d$ and $\boldsymbol{e}_{zh}^{i} \in \mathbb{R}^d$, we sample negative sentence embeddings $\boldsymbol{e}_{en}^{j} \in \mathbb{R}^d$ and $\boldsymbol{e}_{zh}^{k} \in \mathbb{R}^d$ from the same mini-batch, where $i \neq j, i \neq k$.
We use cosine similarity function $\mathcal{S}$ to measure the similarity between embeddings.
Our language neighborhood constraint can be formulated as:
\scalebox{0.9}{\parbox{1.1\columnwidth}{%
\begin{align}\label{eq:triplet}
    \mathcal{L}_{nc} &{=} \frac{1}{n}\sum_i[\mathrm{max}(0, \mathcal{S}(\boldsymbol{e}_{en}^{i}, \boldsymbol{e}_{zh}^{k}) - \mathcal{S}(\boldsymbol{e}_{en}^{i}, \boldsymbol{e}_{zh}^{i}) + \Delta) \nonumber \\&+ \mathrm{max}(0, \mathcal{S}(\boldsymbol{e}_{en}^{j}, \boldsymbol{e}_{zh}^{i}) - \mathcal{S}(\boldsymbol{e}_{en}^{i}, \boldsymbol{e}_{zh}^{i}) + \Delta)],
\end{align}
}}

\noindent
where $\Delta{=}0.2$ is the margin.
We apply this constraint on both query and subtitle embeddings, across the two languages, as illustrated in Figure~\ref{fig:tvrm_encoding}. For queries, we directly apply it on the query vectors $\boldsymbol{q}^{v}_{lang}, \boldsymbol{q}^{s}_{lang}$. For the subtitle embeddings, we apply it on the embeddings $H^{s}_{vr,lang}, H^{s}_{mr,lang}$, after max-pooling them in the temporal dimension.



\paragraph{Training and Inference.}
During training, we optimize video retrieval scores with a triplet loss, and moment scores with a cross-entropy loss. 
At inference, these two scores are aggregated together as the final score for video corpus moment retrieval.
See appendix for details.

