\section{Introduction}\label{introuction}
The number of videos available online is growing at an unprecedented speed.
Recent work~\cite{escorcia2019temporal,lei2020tvr} introduced the Video Corpus Moment Retrieval (VCMR) task: given a natural language query, a system needs to retrieve a short moment from a large video corpus. 
Figure~\ref{fig:data_example} shows a VCMR example.
Compared to the standard text-to-video retrieval task~\cite{xu2016msr,yu2018joint}, it allows more fine-grained moment-level retrieval, as it requires the system to not only retrieve the most relevant videos, but also localize the most relevant moments inside these videos. 
Various datasets~\cite{Krishna2017DenseCaptioningEI,anne2017localizing,gao2017tall,lei2020tvr} have been proposed or adapted for the task. 
However, they are all created for a single language (English), though the application could be useful for users speaking other languages as well. 
Besides, it is also unclear whether the progress and findings in one language generalizes to another language~\cite{bender2009linguistically}.
While there are multiple existing multilingual image datasets~\cite{gao2015you,elliott-etal-2016-multi30k,shimizu2018visual,pappas2016multilingual,lan2017fluency,li2019coco}, the availability of multilingual video datasets~\cite{Wang_2019_ICCV,chen2011collecting} is still limited.


\begin{figure}[!t]
\begin{center}
  \includegraphics[width=0.99\columnwidth]{res/example_vcmr.pdf}
  \vspace{-12pt}
  \caption{
  A \DsetName~example in the Video Corpus Moment Retrieval (VCMR) task. Ground truth moment is shown in \textit{\textcolor{green}{green}} box. Colors in the query text indicate whether the words are more related to video (\textcolor{orchid}{orchid}) or  subtitle (\textcolor{salmon}{salmon}) or both (\textcolor{orange}{orange}). 
  The query and the subtitle text are presented in both English and Chinese. 
  The video corpus typically contains thousands of videos, for brevity, we only show 3 videos here.
  }
  \label{fig:data_example}
  \end{center}
\end{figure}


Therefore, we introduce~\DsetName, a large-scale, multilingual moment retrieval dataset, with 218K human-annotated natural language queries in two languages, English and Chinese. 
\DsetName~extends the TVR~\cite{lei2020tvr} dataset by collecting paired Chinese queries and Chinese subtitle text (see Figure~\ref{fig:data_example}).
We choose TVR over other moment retrieval datasets~\cite{Krishna2017DenseCaptioningEI,anne2017localizing,gao2017tall} because TVR is the largest moment retrieval dataset, and also has the advantage of having dialogues (in the form of subtitle text) as additional context for retrieval, in contrast to pure video context in the other datasets.
We further propose \ModelName, a compact, multilingual model that learns jointly from both English and Chinese data for moment retrieval. 
Specifically, on top of the state-of-the-art monolingual moment retrieval model XML~\cite{lei2020tvr}, we enforce encoder parameter sharing~\cite{sachan2018parameter,dong2015multi} where the queries and subtitles from the two languages are encoded using shared encoders. 
We also incorporate a language neighborhood constraint~\cite{wang2018learning,kim2020mule} to the output query and subtitle embeddings. 
It encourages sentences of the same meaning in different languages to lie close to each other in the embedding space.
Compared to separately trained monolingual models, \ModelName~substantially reduces the total model size while improving retrieval performance (over monolingual models) as we show in Section~\ref{sec:experiments}. 
Detailed dataset analyses and model ablations are provided.






