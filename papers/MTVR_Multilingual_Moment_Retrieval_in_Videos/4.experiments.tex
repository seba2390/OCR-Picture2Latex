\section{Experiments and Results}\label{sec:experiments}
We evaluate our proposed \ModelName~model on the newly collected \DsetName~dataset, and compare it with several existing monolingual baselines. We also provide ablation studies evaluating our model design and the importance of each input modality (videos and subtitles).

\noindent\textbf{Data Splits and Evaluation Metrics.}
We follow TVR~\cite{lei2020tvr} to split the data into 80\% \textit{train}, 10\% \textit{val}, 5\% \textit{test-public} and 5\% \textit{test-private}. We report average recall (R@1) on the Video Corpus Moment Retrieval (VCMR) task. A predicted moment is correct if it has high Intersection-over-Union (IoU) with the ground-truth.


\begin{table}[]
\centering
\small
\setlength{\tabcolsep}{1.5pt}
\renewcommand{\arraystretch}{1.05}
\scalebox{1.0}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{ Method } & \multirow{2}{*}{ \#param } & \multicolumn{2}{c}{ English R@1 } & \multicolumn{2}{c}{ Chinese R@1 } \\
\cmidrule(l){3-4} \cmidrule(l){5-6}
& & IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7 \\
\midrule
Chance & - & 0.00 & 0.00 & 0.00 & 0.00 \\
\multicolumn{3}{l}{\textbf{Proposal based}} & & & \\
MCN & 6.4M & 0.02 & 0.00 & 0.13 & 0.02 \\
CAL & 6.4M & 0.09 & 0.04 & 0.11 & 0.04 \\
\multicolumn{3}{l}{\textbf{Retrieval + Re-ranking}} & & & \\
MEE+MCN & 10.4M & 0.92 & 0.42 & 1.43 & 0.64 \\
MEE+CAL & 10.4M & 0.97 & 0.39 & 1.51 & 0.62 \\
MEE+ExCL & 10.0M & 0.92 & 0.33 & 1.43 & 0.72 \\
XML & 6.4M & 7.25 & 3.25 & 5.91 & 2.57 \\
\midrule
\bf \ModelName & \bf 4.5M & \bf 8.30 & \bf 3.82 & \bf 6.76 & \bf 3.20 \\
\bottomrule
\end{tabular}
}
\vspace{-3pt}
\caption{
Baseline comparison on \DsetName~\textit{test-public} split. \ModelName~achieves better retrieval performance on both languages while using fewer parameters.}
\label{tab:vcmr_baseline_comparison}
\vspace{-3pt}
\end{table}




\noindent\textbf{Baseline Comparison.}
In Table~\ref{tab:vcmr_baseline_comparison}, we compare \ModelName~with multiple baseline approaches. 
Given a natural language query, the goal of video corpus moment retrieval is to retrieve relevant moments from a large video corpus.
The methods for this task can be grouped into two categories, ($i$) proposal based approaches (MCN~\cite{anne2017localizing} and CAL~\cite{escorcia2019temporal}), where they perform video retrieval on the pre-segmented moments from the videos; ($ii$) retrieval+re-ranking methods (MEE~\cite{miech2018learning}+MCN, MEE+CAL, MEE+ExCL~\cite{ghosh2019excl} and XML~\cite{lei2020tvr}), where one approach is first used to retrieve a set of videos, then another approach is used to re-rank the moments inside these retrieved videos to get the final moments.
Our method \ModelName~also belongs to the retrieval+re-ranking category.
Across all metrics and both languages, we notice retrieval+re-ranking approaches achieve better performance than proposal based approaches, indicating that retrieval+re-ranking is potentially better suited for the VCMR task.
Meanwhile, our \ModelName~outperforms the strong baseline XML significantly\footnote{Statistically significant with $p{<}0.01$. We use bootstrap test~\cite{efron1994introduction,noreen1989computer}.} while using few parameters.
XML is a monolingual model, where a separate model is trained for each language. 
In contrast, \ModelName~is multilingual, trained on both languages simultaneously, with parameter sharing and language neighborhood constraints to encourage multilingual learning. 
\ModelName~prediction examples are provided in the appendix.



\paragraph{Ablations on Model Design.} 
In Table~\ref{tab:ablation_model_design}, we present our ablation study on~\ModelName. 
We use `\textit{Baseline}' to denote the~\ModelName{} model without parameter sharing and neighborhood constraint. 
Sharing encoder parameter across languages greatly reduces \#parameters while maintaining (Chinese) or even improving (English) model performance.
Adding neighborhood constraint does not introduce any new parameters but brings a notable ($p{<}0.06$) performance gain to both languages.
We hypothesize that this is because the learned information in the embeddings of the two languages are complementary (though the sentences in the two languages express the same meaning, their language encoders~\cite{liu2019roberta,cui2020revisiting}) are pre-trained differently, which may lead to different meanings at the embedding level.
In Table~\ref{tab:ablation_query_type_comparison}, we show a detailed comparison between~\ModelName{} and its baseline version, by query types. 
Overall, we notice the \ModelName~perform similarly with \textit{Baseline} in `\textit{video}' queries, but shows a significant performance gain in `\textit{subtitle}' queries, suggesting the parameter sharing and neighborhood constraint are more useful for queries that need more language understanding.


\begin{table}[]
\centering
\small
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1.05}
\scalebox{0.96}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{ Method } & \multirow{2}{*}{ \#param } & \multicolumn{2}{c}{ English R@1 } & \multicolumn{2}{c}{ Chinese R@1 } \\
\cmidrule(l){3-4} \cmidrule(l){5-6}
& & IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7 \\
\midrule
Baseline & 6.4M & 5.77 & 2.63 & 4.7 & 2.38 \\
\;+ Share Enc. & \bf 4.5M & 6.09 & 2.85 & 4.72 & 2.25 \\
\;\;\;+ NC (\ModelName) & \bf 4.5M & \bf 6.22 & \bf 2.96 & \bf 5.17 & \bf 2.41 \\
\bottomrule
\end{tabular}
}
\vspace{-3pt}
\caption{
\ModelName~ablation study on \DsetName~\textit{val} split. \textit{Share Enc.} {=} encoder parameter sharing, \textit{NC} {=} Neighborhood Constraint. 
Each row adds an extra component to the row above it.
}
\label{tab:ablation_model_design}
\vspace{-6pt}
\end{table}


\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.05}
\scalebox{1.0}{
\begin{tabular}{lcccc}
\toprule
Model Type & \multicolumn{2}{c}{English R@1} & \multicolumn{2}{c}{Chinese R@1} \\  \cmidrule(l){2-3} \cmidrule(l){4-5}
& IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7 \\
\midrule
\multicolumn{2}{l}{\textbf{Query type: video}} & & \\
Baseline & 5.46 & 2.53 & 4.78 & 2.47 \\
mXML & 5.77 & 2.67 & 5.14 & 2.32 \\
\midrule
\multicolumn{2}{l}{\textbf{Query type: subtitle}} & & \\
Baseline & 4.15 & 1.97 & 3.11 & 1.14 \\
mXML & 6.12 & 3.32 & 4.05 & 1.87 \\
\midrule
\multicolumn{2}{l}{\textbf{Query type: video+subtitle}} & & \\
baseline & 8.02 & 3.38 & 5.18 & 2.62 \\
mXML & 8.29 & 4.09 & 5.89 & 3.11 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of \ModelName~and the baseline on \DsetName~\textit{val} set, with breakdown on query types. Both models are trained with video and subtitle as inputs.}
\label{tab:ablation_query_type_comparison}
\end{table}



\paragraph{Ablations on Input Modalities.}
In Table~\ref{tab:ablation_input_modalities_query_type}, we compare \ModelName~variants with different context inputs, i.e., video or subtitle or both.
We report their performance under the three annotated query types, \textit{video}, \textit{sub} and \textit{video+sub}.
Overall, the model with both video and subtitle as inputs perform the best.
The video model performs much better on the \textit{video} queries than on the \textit{sub} queries, while the subtitle model achieves higher scores on the \textit{sub} queries than the \textit{video} queries.

In the appendix, we also present results on `\textit{generalization to unseen TV shows}' setup. 

\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.05}
\scalebox{1.0}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{ QType (percentage) } & \multicolumn{2}{c}{ English R@1 } & \multicolumn{2}{c}{ Chinese R@1 } \\ \cmidrule(l){2-3} \cmidrule(l){4-5}
& IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7 \\
\midrule
\multicolumn{1}{l}{\textbf{Model input: video}} & & & & \\
video (74.32\%) & 4.12 & 1.89 & 3.73 & 1.86 \\
sub (8.85\%) & 1.97 & 1.24 & 1.35 & 1.04 \\
video+sub (16.83\%)  & 2.67 & 1.2 & 2.45 & 1.15 \\
\midrule
\multicolumn{3}{l}{\textbf{Model input: subtitle}} & & \\
video & 1.35 & 0.62 & 1.11 & 0.51 \\
sub & 6.33 & 2.9 & 4.15 & 1.97 \\
video+sub & 6.22 & 2.62 & 4.2 & 2.13 \\
\midrule
\multicolumn{3}{l}{\textbf{Model input: video+subtitle}} & & \\
video & 5.77 & 2.67 & 5.14 & 2.32 \\
sub & 6.12 & 3.32 & 4.05 & 1.87 \\
video+sub & 8.29 & 4.09 & 5.89 & 3.11 \\
\bottomrule
\end{tabular}
}
\caption{\ModelName~performance breakdown on \DsetName~\textit{val} set by query types, with different inputs. }
\label{tab:ablation_input_modalities_query_type}
\end{table}

