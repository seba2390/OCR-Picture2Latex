
\appendix

\section{Appendix}

\paragraph{Data Analysis.}
In Table~\ref{tab:dataset_comparison} we show a comparison of \DsetName~with existing moment retrieval datasets and related video and language datasets. 
Compared to other moment retrieval datasets, \DsetName~is significantly larger in scale, and comes with query type annotations that allows in-depth analyses for the models trained on it.
Besides, it is also the only moment retrieval dataset with multilingual annotations, which is vital in studying the moment retrieval problem under the multilingual context. 
Compared to the existing multilingual video and language datasets, \DsetName~is unique as it has a more diverse set of context and annotations, i.e., dialogue, query type, and timestamps.


\paragraph{Training and Inference Details.}
In Figure~\ref{fig:mxml_overview} we show an overview of the \ModelName~model.
We compute video retrieval score as:
\begin{align}
    s^{vr} = \frac{1}{2}\sum_{m \in \{v, s\}} \mathrm{max}(\frac{H^{m}_{vr}}{\left\Vert H^{m}_{vr}\right\Vert} \frac{\boldsymbol{q}^{m}}{\left\Vert \boldsymbol{q}^{m}\right\Vert}).
\end{align}
The subscript $lang \in \{en, zh\}$ is omitted for simplicity.
It is optimized using a triplet loss similar to main text Equation (1).
For moment retrieval, we first compute the query-clip similarity scores $S^{q,c} \in \mathbb{R}^{l}$ as:
\begin{align}
    S^{q,c} = \frac{1}{2}(H^{s}_{mr}\boldsymbol{q}^{s} + H^{v}_{mr}\boldsymbol{q}^{v}).
\end{align}
Next, we apply Convolutional Start-End Detector (ConvSE module)~\cite{lei2020tvr} to obtain start, end probabilities $P_{st}, P_{ed} \in \mathbb{R}^{l}$. These scores are optimized using a cross-entropy loss. The single video moment retrieval score for moment $[t_{st}, t_{ed}]$ is computed as:
\begin{align}
    s^{mr}(t_{st}, t_{ed}) = P_{st}(t_{st}) P_{ed}(t_{ed}), \, t_{st} \leq t_{ed}.
\end{align}

\noindent
Given a query $q_i$, the retrieval score for moment [$t_{st}$:$t_{ed}$] in video $v_j$ is computed following the aggregation function as in~\cite{lei2020tvr}:
\begin{align}
    s^{vcmr}&(v_j,t_{st}, t_{ed}|q_i) = \nonumber \\ &s^{mr}(t_{st}, t_{ed})\mathrm{exp}(\alpha s^{vr}(v_j|q_i)),
\end{align}


\noindent
where $\alpha{=}20$ is used to assign higher weight to the video retrieval scores.
The overall loss is a simple summation of video and moment retrieval loss across the two languages, and the language neighborhood constraint loss. 








\paragraph{Implementation Details.}
\ModelName~is implemented in PyTorch~\cite{paszke2017automatic}.
We use Adam~\cite{kingma2014adam} with initial learning rate 1e-4, $\beta_1{=}0.9$, $\beta_2{=}0.999$, L2 weight decay 0.01, learning rate warm-up over the first 5 epochs. 
We train \ModelName~for at most 100 epochs at batch size 128, with early stop based on the sum of R@1 (IoU=0.7) scores for English and Chinese.
The experiments are conducted on a NVIDIA RTX 2080Ti GPU. 
Each run takes around 7 hours.



\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{3.5pt}
\renewcommand{\arraystretch}{1.05}
\scalebox{1.0}{
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{English R@1} & \multicolumn{2}{c}{Chinese R@1} \\  \cmidrule(l){2-3} \cmidrule(l){4-5}
Setting & IoU=0.5 & IoU=0.7 & IoU=0.5 & IoU=0.7 \\
\midrule
unseen & 1.68 & 0.79 & 1 & 0.54 \\
seen & 4.82 & 2.79 & 4.18 & 2.32 \\
\bottomrule
\end{tabular}
}
\caption{\ModelName~performance on the \DsetName~val split \textit{Friends} examples, in both \textit{unseen} and \textit{seen} settings. 
}
\label{tab:ablation_unseen}
\end{table}



\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{res/mXML_overview.pdf}
  \caption{
  \ModelName~overview. For brevity, we only show the modeling process for a single language (Chinese). The cross-language modifications, i.e., parameter sharing and neighborhood constraint are illustrated in Figure~\ref{fig:tvrm_encoding}. This figure is edited from the Figure 4 in~\citep{lei2020tvr}. 
  }
  \label{fig:mxml_overview}
\end{figure*}



\begin{table*}[ht]
\centering
\small
\setlength{\tabcolsep}{5pt}
\scalebox{0.96}{
\begin{tabular}{lcccccc}
\toprule
Dataset & Domain & \#Q/\#videos & Multilingual & Dialogue & QType & Timestamp \\
\midrule
\bf QA datasets with temporal annotation &  &  &  &  &  &  \\
TVQA~\cite{Lei2018TVQALC} & TV show & 152.5K/21.8K & - & \checkmark & - & \checkmark \\
How2QA~\cite{li2020hero} & Instructional & 44K/22K & - & \checkmark & - & \checkmark \\
\bf Multilingual video description datasets &  &  &  &  &  &  \\
MSVD~\cite{chen2011collecting} & Open & 70K/2K & \checkmark & - & - & - \\
VATEX~\cite{wang2019vatex} & Activity & 826K/41.3K & \checkmark & - & - & - \\
\bf Moment retrieval datasets &  &  &  &  &  &  \\
TACoS~\cite{regneri2013grounding} & Cooking & 16.2K/0.1K & - & - & - & \checkmark \\
DiDeMo~\cite{anne2017localizing} & Flickr & 41.2K/10.6K & - & - & - & \checkmark \\
ActivityNet Captions~\cite{Krishna2017DenseCaptioningEI} & Activity & 72K/15K & - & - & - & \checkmark \\
CharadesSTA~\cite{gao2017tall} & Activity & 16.1K/6.7K & - & - & - & \checkmark \\
How2R~\cite{li2020hero} & Instructional & 51K/24K & - & \checkmark & - & \checkmark \\
TVR~\cite{lei2020tvr} & TV show & 109K/21.8K & - & \checkmark & \checkmark & \checkmark \\
\midrule
\DsetName & TV show & 218K/21.8K & \checkmark & \checkmark & \checkmark & \checkmark \\ 
\bottomrule
\end{tabular}
}
\caption{
Comparison of~\DsetName~with related video and language datasets.   
}
\label{tab:dataset_comparison}
\end{table*}

 

\paragraph{Generalization to Unseen TV shows.} 
To investigate whether the learned model can be transferred to other TV shows, we conduct an experiment by using the TV show `\textit{Friends}' as an `\textit{unseen}' TV show for testing, and train the model on all the other 5 TV shows. 
For comparison, we also include a model trained on `\textit{seen}' setting, where we use all the 6 TV shows including \textit{Friends} for training. 
To ensure the models on these two settings are trained on the same number of examples, we downsample the examples in the \textit{seen} setting to match the \textit{unseen} setting.
The results are shown in Table~\ref{tab:ablation_unseen}.
We notice our \ModelName~achieves a reasonable performance even though it does see a single example from the TV show \textit{Friends}.
Meanwhile, the gap between \textit{unseen} and \textit{seen} settings are still large, we encourage future work to further explore this direction.


\paragraph{Prediction Examples}
We show \ModelName~prediction examples in Figure~\ref{fig:pred_examples}. 
We show both Chinese (\textit{top}) and English (\textit{bottom}) prediction examples, and correct (\textit{left}) and incorrect (\textit{right}) examples.


\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{res/pred_examples.pdf}
  \caption{
  Qualitative examples of \ModelName. \textit{Top:} examples in Chinese. \textit{Bottom:} examples in English. \textit{Left:} correct predictions. \textit{Right:} incorrect predictions.
  We show top-3 retrieved moments for each query. \textcolor{salmon}{salmon bar} shows the predictions, \textcolor{ForestGreen}{green box} indicates the ground truth.
  }
  \label{fig:pred_examples}
\end{figure*}
