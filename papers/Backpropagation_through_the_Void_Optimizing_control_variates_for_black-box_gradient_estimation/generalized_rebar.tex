\documentclass{article}
\usepackage{iclr2018_conference, times}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{wrapfig} % for side-by
\usepackage{bm}
% rename algorithmic labels
\renewcommand{\algorithmicensure}{\textbf{return}}
\renewcommand{\algorithmicfunction}{\textbf{def}}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}

\newcommand{\vtheta}{\mathbf{\theta}}
\newcommand{\relaxed}{r}
\newcommand{\controlf}{c}  % Control variate for functions
\newcommand{\controlg}{\hat g}  % Control variate for gradients
\newcommand{\discreteDist}{p(b|\theta)}
\newcommand{\loss}{f(b)}
\newcommand{\lossGrad}{\loss{} \frac{\partial}{\partial \theta }\log \discreteDist{}}
\newcommand{\mcGrad}{\hat{g}}
\newcommand{\expectedLoss}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \loss{} \right]}
\newcommand{\expectedLossLogTrick}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \lossGrad{} \right]}
\newcommand{\var}{\mathbb{V}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\LL}[1]{\frac{\partial \log \pi(a_{#1}| s_{#1}, \theta)}{\partial \theta}}
\newcommand{\PT}{\frac{\partial}{\partial \theta}}
\newcommand{\PP}[1]{\frac{\partial}{\partial #1}}
\newcommand{\PPH}{\frac{\partial}{\partial \phi}}
\newcommand{\LP}[1]{\PT \log p(#1)}
\newcommand{\LZ}[1]{\frac{\log \pi(z_{#1}| s_{#1}, \theta)}{\partial \theta}}

\newcommand{\YW}[1]{{\color{red} \bf [[YW: #1]]}}
\newcommand{\LAX}{{\textnormal{LAX}}}
\newcommand{\DLAX}{{\textnormal{DLAX}}}
\newcommand{\RL}{{\textnormal{RL}}}
\newcommand{\RELAX}{{\textnormal{RELAX}}}
\newcommand{\BAR}{{\textnormal{BAR}}}
\newcommand{\REBAR}{{\textnormal{REBAR}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{question}[theorem]{Question}
\newtheorem{observation}[theorem]{Observation}


\title{Backpropagation through the Void:\\
Optimizing control variates for \\ black-box gradient estimation}
% Optimizing gradient control variates\\ for black-box expectations}

\author{Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, David Duvenaud \\
University of Toronto and Vector Institute\\
\texttt{\{wgrathwohl, choidami, ywu, roeder, duvenaud\}@cs.toronto.edu}
}


\iclrfinalcopy 

\begin{document}
\maketitle
\begin{abstract}
Gradient-based optimization is the foundation of deep learning and reinforcement learning, but is difficult to apply when the mechanism being optimized is unknown or not differentiable.
%Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy.
We introduce a general framework for learning low-variance, unbiased gradient estimators, applicable to black-box functions of discrete or continuous random variables.
Our method uses gradients of a surrogate neural network to construct a control variate, which is optimized jointly with the original parameters.
%Our method applies to both continuous and discrete variables.
We demonstrate this framework for training discrete latent-variable models.
We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.
\end{abstract}


\section{Introduction}
Gradient-based optimization has been key to most recent advances in machine learning and reinforcement learning.
The back-propagation algorithm \citep{rumelhart1986learning}, also known as reverse-mode automatic differentiation~\citep{speelpenning1980compiling, rall1981automatic} computes exact gradients of deterministic, differentiable objective functions.
The reparameterization trick \citep{williams1992simple, kingma2013autoencoding, rezende2014stochastic} allows backpropagation to give unbiased, low-variance estimates of gradients of expectations of continuous random variables.
This has allowed effective stochastic optimization of large probabilistic latent-variable models.

%Unfortunately, backpropagation cannot be easily applied to problems involving discrete random variables, or when the function being optimized is a black box~\citep{schulman2015gradient}.
Unfortunately, there are many objective functions relevant to the machine learning community for which backpropagation cannot be applied. In reinforcement learning, for example, the function being optimized is unknown to the agent and is treated as a black box~\citep{schulman2015gradient}. Similarly, when fitting probabilistic models with discrete latent variables, discrete sampling operations create discontinuities giving the objective function zero gradient with respect to its parameters.
%This is the case in most reinforcement learning settings, or when fitting probabilistic models with discrete latent variables.
Much recent work has been devoted to constructing gradient estimators for these situations.
In reinforcement learning, advantage actor-critic methods~\citep{sutton2000policy} give unbiased gradient estimates with reduced variance obtained by jointly optimizing the policy parameters with an estimate of the value function.
In discrete latent-variable models, low-variance but biased gradient estimates can be given by continuous relaxations of discrete variables~\citep{maddison2016concrete, jang2016categorical}.

A recent advance by \citet{tucker2017rebar} used a continuous relaxation of discrete random variables to build an unbiased and lower-variance gradient estimator,
%Low-variance estimates of the expectation of the control variate can be computed using the reparameterization trick to produce an unbiased estimator with lower variance than previous methods.
%Furthermore, \citet{tucker2017rebar} 
and showed how to tune the free parameters of these relaxations to minimize the estimator's variance during training.
%
%\paragraph{Contributions}
We generalize the method of \citet{tucker2017rebar} to learn a free-form control variate parameterized by a neural network.
This gives a lower-variance, unbiased gradient estimator which can be applied to a wider variety of problems.
Most notably, our method is applicable even when no continuous relaxation is available, as in reinforcement learning or black-box function optimization.

%\paragraph{Contributions}
%\begin{itemize}
%\item we keep conditional reparam from rebar while exploring and expanding on gradient signal for minimizing variance of estimator
%\item we generalize the concrete and muprop relaxations for a learnable family of functions that can be optimized via gradient decent to minimize the variance of the estimator
%\item We show that concrete relax is not optimal, and that control variate based on the discrete objective at continuous input is is not the optimal control variate.
%\item we combine these insights into a new family of estimators that generalizes rebar
%\end{itemize}

\begin{figure}[h]
\hspace{-1em}
\centering
\begin{tabular}{c c}
\includegraphics[width=.48\textwidth]{figures/toy_losses_10000_0_499}
&
\includegraphics[width=.48\textwidth]{figures/variance_100_t_499}
\end{tabular}
\vspace{-1em}
%\includegraphics[width=.325\textwidth]{figures/relaxations_t_499_which_2}
\caption{
\emph{Left:} Training curves comparing different gradient estimators on a toy problem: ${\mathcal{L}(\theta) = \E_{p(b|\theta)} [ (b - 0.499)^2 ]}$
\emph{Right:} Log-variance of each estimator's gradient.
}
\label{first figure}
\end{figure}


\section{Background: Gradient estimators}
How can we choose the parameters of a distribution to maximize an expectation?
This problem comes up in reinforcement learning, where we must choose the parameters $\theta$ of a policy distribution $\pi(a|s, \theta)$ to maximize the expected reward $\mathbb{E}_{\tau \sim \pi} \left[ R \right]$ over state-action trajectories $\tau$.
It also comes up in fitting latent-variable models, when we wish to maximize the marginal probability ${p(x|\theta) = \sum_z p(x|z) p(z|\theta) = \mathbb{E}_{p(z|\theta)} \left[ p(x|z) \right]}$.
In this paper, we'll consider the general problem of optimizing
%
\begin{align}
\mathcal{L}(\theta) = \expectedLoss{}.
\end{align}

When the parameters $\theta$ are high-dimensional, gradient-based optimization is appealing because it provides information about how to adjust each parameter individually.
Stochastic optimization is essential for scalablility, but is only guaranteed to converge to a fixed point of the objective when the stochastic gradients $\hat g$ are unbiased, i.e. ${\mathbb{E} \left[ \hat g \right] = \PT \mathcal{L}(\theta)}$~\citep{robbins1951stochastic}.

How can we build unbiased, stochastic gradient estimators?
There are several standard methods:

\paragraph{The score-function gradient estimator}
One of the most generally-applicable gradient estimators is known as the score-function estimator, or REINFORCE~\citep{williams1992simple}:
%
\begin{align}
\hat g_\textnormal{REINFORCE}[f] =  f \left( b \right) \PT \log p(b | \theta), \qquad b \sim p(b | \theta)
\end{align}
%
This estimator is unbiased, but in general has high variance.
Intuitively, this estimator is limited by the fact that it doesn't use any information about how $f$ depends on $b$, only on the final outcome $f(b)$.

\paragraph{The reparameterization trick}
When $f$ is continuous and differentiable, and the latent variables $b$ can be written as a deterministic, differentiable function of a random draw from a fixed distribution, the reparameterization trick \citep{williams1992simple, kingma2013autoencoding, rezende2014stochastic} creates a low-variance, unbiased gradient estimator by making the dependence of $b$ on $\theta$ explicit through a reparameterization function $b=T(\theta, \epsilon)$:
%
\begin{align}
%\hat g_\textnormal{reparam} = \frac{\partial f \left( b(\theta, \epsilon) \right)}{\partial \theta}, \qquad \epsilon \sim p(\epsilon)
\hat g_\textnormal{reparam}[f]
= \PT f \left( b \right)
= \frac{\partial f}{\partial T}\frac{\partial T}{\partial \theta} , 
\qquad \epsilon \sim p(\epsilon) 
\end{align}
%
This gradient estimator is often used when training high-dimensional, continuous latent-variable models, such as variational autoencoders.
% or GANs \citep{goodfellow2014generative}.
One intuition for why this gradient estimator is preferable to REINFORCE is that it depends on ${\partial f} / {\partial b}$, which exposes the dependence of $f$ on $b$.
% NOTE for revision: last sentence is confusing. What is the advantage of exposing the dependence of f on b for a gradient estimator?

%The reparameterization trick, while useful, is only applicable when $f$ is a known, differentiable function of a continuous random variable.
%\paragraph{Concrete relaxation}
%For functions of discrete variables, gradient estimators based on continuous relaxations such as the concrete, or Gumbel-softmax relaxations~\citep{maddison2016concrete, jang2016categorical} are applicable only when $f$ is known.
%Moreover, they require that $f$ be computable at and behave predictably at inputs outside of the domain of the function.
%These assumptions greatly restrict the space of functions for which we can apply these methods. 

%\paragraph{Concrete relaxation}
%When $b$ is discrete and $f$ is known and differentiable, one can obtain biased but low-variance gradients using a continuous relaxation of the discrete random variable $b$.
%For example, if $b$ is a categorical random variable with parameters $\theta$,% and  developed a differentiable relaxation of the categorical distribution, called the concrete distribution:
%
%\begin{align}
%\hat g_\textnormal{concrete} = \PT f \left( \texttt{softmax}_\lambda ( \log \theta - \log(-\log \vu)) \right), \qquad \vu \sim \textnormal{uniform}[0, 1] 
%\end{align}
%
%where $\sigma_\lambda$ is the \texttt{softmax} function with temperature $\lambda$.
%This is called the Gumbel-softmax trick~\citep{jang2016categorical}, or the concrete distribution~\citep{maddison2016concrete}


\paragraph{Control variates}
Control variates are a general method for reducing the variance of a stochastic estimator.
A control variate is a function $\controlf(b)$ with a known mean $\mathbb{E}_{p(b)} [ \controlf(b) ]$.
Given an estimator $\hat g(b)$, subtracting the control variate from this estimator and adding its mean gives us a new estimator:
%
\begin{align}
\hat g_\textnormal{new}(b) = \hat g(b) - \controlf(b) + \mathbb{E}_{p(b)}[\controlf(b)]
\end{align}
%
This new estimator has the same expectation as the old one, 
%
%\begin{align}
%\mathbb{E}_{p(b)}\left[\hat g_\textnormal{new}(b) \right] 
%= \mathbb{E}_{p(b)}\left[\hat g(b) - \controlf(b) + \mathbb{E}_{p(b)} \left[ \controlf(b) \right] \right]
%= \mathbb{E}_{p(b)}\left[ \hat g(b) \right]
%\end{align}
%
but has lower variance if $\controlf(b)$ is positively correlated with $\hat g(b)$.
% NOTE for revision: 
% I think it's important to stick to the standard definition of a control variate here, which includes the scalar constant eta
% c(b) can be positively or negatively correlated with f(b) and still reduce variance if the control variate has a learned scalar like eta
% Eta can be positive or negative. When you learn eta optimally (Cov(g,c) / Var(c), see for example Tucker et al. 2017 sec. 7.1), eta will flip sign so that even if c(b) is negatively correlated, the new estimator will have lower variance .

\section{Constructing and optimizing a differentiable surrogate}
\label{lax section}
In this section, we introduce a gradient estimator for the expectation of a function $\PT \E_{p(b|\theta)}[f(b)]$ that can be applied even when $f$ is unknown, or not differentiable, or when $b$ is discrete.
Our estimator combines the score function estimator, the reparameterization trick, and control variates.
%In the continuous case, we obtain an unbiased estimator whose variance can potentially be as low as the reparameterization-trick estimator.%, even when $f$ is not differentiable or not computable.

First, we consider the case where $b$ is continuous, but that $f$ cannot be differentiated.
Instead of differentiating through $f$, we build a surrogate of $f$ using a neural network $c_\phi$, and differentiate $c_\phi$ instead.
Since the score-function estimator and reparameterization estimator have the same expectation,
we can simply subtract the score-function estimator for $c_\phi$ and add back its reparameterization estimator.
This gives a gradient estimator which we call LAX:
%\begin{center}
%\begin{tabular}{c|c|c|c}
%a & b & c & d
%\end{tabular}
%\end{center}
%
\begin{align}
\label{eq:cont_est}
\hat g_\LAX &= 
\hat{g}_\textnormal{REINFORCE}[f] - \hat{g}_\textnormal{REINFORCE}[c_\phi] + \hat{g}_\textnormal{reparam}[c_\phi] \nonumber\\
&= \left[ f(b) -c_\phi(b) \right] \PT \log p(b|\theta) + \PT c_\phi(b) \qquad b = T(\theta, \epsilon), \epsilon \sim p(\epsilon).
\end{align}
%
This estimator is unbiased for any choice of $c_\phi$.
When $c_\phi = f$, then \LAX{} becomes the reparameterization estimator for $f$.
Thus \LAX{} can have variance at least as low as the reparameterization estimator. An example of the relative bias and variance of each term in this estimator can be seen below.%in Figure \ref{fig:grad hist}.
\begin{figure}[h!]
\includegraphics[width=\columnwidth]{figures/grad_hist.pdf}
\caption{Histograms of samples from the gradient estimators that create LAX. Samples generated from our one-layer VAE experiments (Section \ref{vae section}).}
\label{fig:grad hist}
\end{figure}

%\subsection{Optimizing the gradient control variate with gradients}
\subsection{Gradient-based optimization of the control variate}
Since $\hat g_\LAX$ is unbiased for any choice of the surrogate $c_\phi$, the only remaining problem is to choose a $c_\phi$ that gives low variance to $\hat g_\LAX$.
How can we find a $\phi$ which gives our estimator low variance?
We simply optimize $c_\phi$ using stochastic gradient descent, at the same time as we optimize the parameters $\theta$ of our model or policy.

To optimize $c_\phi$, we require the gradient of the variance of our estimator.
To estimate these gradients, we could simply differentiate through the empirical variance over each mini-batch.
Or, following \cite{ruiz2016overdispersed} and \cite{tucker2017rebar}, we can construct an unbiased, single-sample estimator using the fact that our gradient estimator is unbiased.
For any unbiased gradient estimator $\hat g$ with parameters $\phi$:
%
\begin{align}
\PPH \text{Variance}(\hat g)
= \PPH \E[\hat g^2] - \PPH \E[\hat g]^2
%= \PPH \E[\hat g^2] - \PPH \E_{p(b|\theta)}[f(b)]
= \PPH \E[\hat g^2]
= \E \left[ \PPH \hat g^2 \right].
%= \E \left[ 2 \hat g \frac{\partial \hat g}{\partial \phi} \right].
\label{eq:vargrad}
\end{align}  % Do we need hats on these gs?  Or get rid of them elsewhere.
%
Thus, an unbiased single-sample estimate of the gradient of the variance of $\hat g$ is given by $\partial \hat g^2 / \partial \phi$.
%{$2 \hat g \frac{\partial \hat g}{\partial \phi}$}.

This method of directly minimizing the variance of the gradient estimator stands in contrast to other methods such as Q-Prop~\citep{gu2016q} and advantage actor-critic~\citep{sutton2000policy}, which train the control variate to minimize the squared error $(f(b) - c_\phi(b))^2$.
Our algorithm, which jointly optimizes the parameters $\theta$ and the surrogate $c_\phi$ is given in Algorithm~\ref{lax}.
%[Todo: talk about the variance of the gradient estimator of the variance of the gradient estimators?]

\subsubsection{Optimal surrogate}
What is the form of the variance-minimizing $c_\phi$?
Inspecting the square of \eqref{eq:cont_est}, we can see that this loss encourages $c_\phi(b)$ to approximate $f(b)$, but with a weighting based on $\PT\log p(b|\theta)$.  % Todo: reword this awkward sentence.
Moreover, as $c_\phi \rightarrow f$ then $\hat g_\textnormal{\LAX} \rightarrow \PT c_\phi$.
Thus, this objective encourages a balance between the variance of the reparameterization estimator and the variance of the REINFORCE estimator.
Figure~\ref{learned-relaxations} shows the learned surrogate on a toy problem.


\begin{algorithm}[h]
\begin{algorithmic}
\Require $f(\cdot)$, $\log p(b|\theta)$, reparameterized sampler $b = T(\theta, \epsilon)$, neural network $c_\phi(\cdot)$, \\ \qquad \quad~step sizes $\alpha_1, \alpha_2$ 
\While {not converged} 
	\State $\epsilon \sim p(\epsilon)$ \Comment Sample noise
	\State $b \leftarrow T(\epsilon, \theta)$ \Comment Compute input
	\State  $\hat g_\theta \leftarrow \left[f(b) - c_{\phi}(b) \right] \nabla_\theta \log p(b|\theta) + \nabla_\theta c_\phi(b)$ \Comment Estimate gradient of objective
	\State  $\hat g_\phi \leftarrow \partial \hat g_\theta^2 / \partial \phi$ \Comment Estimate gradient of variance of gradient
	\State $\theta \leftarrow \theta - \alpha_1 \hat{g}_\theta$ \Comment Update parameters
	\State $\phi \leftarrow \phi - \alpha_2 \hat{g}_\phi$ \Comment Update control variate
\EndWhile
\State \textbf{return} $\theta$ 
\end{algorithmic}
\caption{\LAX{}: Optimizing parameters and a gradient control variate simultaneously.}
\label{lax}
\end{algorithm}

\subsection{Discrete random variables and conditional reparameterization}
We can adapt the \LAX{} estimator to the case where $b$ is a discrete random variable by introducing a ``relaxed'' continuous variable $z$.
We require a continuous, reparameterizable distribution $p(z|\theta)$ and a deterministic mapping $H(z)$ such that $H(z) = b \sim p(b|\theta)$ when $z \sim p(z|\theta)$.
In our implementation, we use the Gumbel-softmax trick, the details of which can be found in appendix~\ref{resample}.

The discrete version of the \LAX{} estimator is given by:
%
\begin{align}
\label{eq:discrete lax}
\hat g_\DLAX = f(b) \PT \log p(b|\theta) - c_\phi(z) \PT \log p(z|\theta) + \PT c_\phi(z), \qquad b = H(z), z \sim p(z|\theta).
\end{align}
%
This estimator is simple to implement and general.
However, if we were able to replace the $\PT \log p(z|\theta)$ in the control variate with $\PT \log p(b|\theta)$ we should be able to achieve a more correlated control variate, and therefore a lower variance estimator. This is the motivation behind our next estimator, which we call RELAX.

%when $f = c_\phi$ we do not recover the reparameterization estimator as we do with LAX. To achieve this, we must be able to replace the $\PT \log p(z|\theta)$ in the control variate with $\PT \log p(b|\theta)$. This is the motivation behind our next estimator, which we call RELAX.

To construct a more powerful gradient estimator, we incorporate a further refinement due to~\cite{tucker2017rebar}.
Specifically, we evaluate our control variate both at a relaxed input $z \sim p(z|\theta)$, and also at a relaxed input \emph{conditioned on the discrete variable $b$}, denoted $\tilde z \sim p(z|b, \theta)$. 
Doing so gives us:
%
\begin{align}
\hat g_\textnormal{RELAX} = \left[ f(b) - c_\phi(\tilde{z}) \right] \PT \log p(b|\theta) + \PT c_\phi(z) - \PT c_\phi (\tilde{z}) \\
\qquad b = H(z), z \sim p(z|\theta), \tilde{z} \sim p(z|b, \theta) \nonumber
\end{align}
%
This estimator is unbiased for any $c_\phi$.
A proof and a detailed algorithm can be found in appendix~\ref{relax proof}.
%
We note that the distribution $p(z|b,\theta)$ must also be reparameterizable.
We demonstrate how to perform this conditional reparameterization for Bernoulli and categorical random variables in appendix~\ref{resample}.

\subsection{Choosing the control variate architecture}
The variance-reduction objective introduced above allows us to use any differentiable, parametric function as our control variate $c_\phi$. 
How should we choose the architecture of $c_\phi$?
Ideally, we will take advantage of any known structure in $f$.

In the discrete setting, if $f$ is known and happens to be differentiable, we can use the concrete relaxation~\citep{jang2016categorical, maddison2016concrete} and let $c_\phi(z) = f(\sigma_\lambda(z))$.
In this special case, our estimator is exactly the REBAR estimator.
We are also free to add a learned component to the concrete relaxation and let $c_\phi(z) = f(\sigma_\lambda(z)) + {r}_\rho(z)$ where ${r}_\rho$ is a neural network with parameters~$\rho$ making $\phi = \{\rho, \lambda\}$.
We took this approach in our experiments training discrete variational auto-encoders.
If $f$ is unknown, we can simply let $c_\phi$ be a generic function approximator such as a neural network.
We took this simpler approach in our reinforcement learning experiments.


\subsection{Reinforcement learning}
We now describe how we apply the \LAX{} estimator in the reinforcement learning (RL) setting.
By reinforcement learning, we refer to the problem of optimizing the parameters $\theta$ of a policy distribution $\pi(a | s, \theta)$ to maximize the sum of rewards.
In this setting, the random variable being integrated over is $\tau$, which denotes a series of $T$ actions and states $[(s_1, a_1), (s_2, a_2), ..., (s_T, a_T)]$.
The function whose expectation is being optimized, $R$, maps $\tau$ to the sum of rewards ${R(\tau) = \sum_{t=1}^{T} r_t(s_t, a_t)}$.

Again, we want to estimate the gradient of an expectation of a black-box function: $ \PT \E_{p(\tau|\theta)}[R(\tau)]$.
The \emph{de facto} standard approach is the advantage actor-critic estimator (A2C)~\citep{sutton2000policy}:
%We can view the sum of future rewards as a black-box function of state-action trajectories $\tau$ sampled from our policy.
%Instead, we typically compute
%
%\begin{align}
%\frac{\partial \E_\tau[R]}{\partial \theta} = \E_\tau \left[ \sum_{t=1}^{\infty} \LL{t} \left[ \sum_{t'=t}^{\infty} r_{t'} - b(s_t) \right] \right]
%\end{align}
%
%
\begin{align}
\hat g_{\textnormal{A2C}} = \sum_{t=1}^{T} \LL{t} \left[ \sum_{t'=t}^{T} r_{t'} - c_\phi(s_t) \right], \qquad a_t \sim \pi(a_t | s_t, \theta)
\label{eq:rl_a2c}
\end{align}
%
Where $c_\phi(s_t)$ is an estimate of the state-value function, $c_\phi(s) \approx V^\pi(s) = \E_{\tau}[R|s_1=s].$
This estimator is unbiased when $c_\phi$ does not depend on $a_t$.
The main limitations of A2C are that $c_\phi$ does not depend on $a_t$, and that it's not obvious how to optimize $c_\phi$.
Using the \LAX{} estimator addresses both of these problems.

First, we assume $\pi(a_t|s_t, \theta)$ is reparameterizable, meaning that we can write $a_t = a(\epsilon_t, s_t, \theta)$, where $\epsilon_t$ does not depend on $\theta$.
We again introduce a differentiable surrogate $c_\phi(a,s)$.
Crucially, this surrogate is a function of the action as well as the state.

The extension of LAX to Markov decision processes is: 
%
\begin{align}
\hat g_\LAX^{\RL} = \sum_{t=1}^{T} \LL{t} \left[ \sum_{t'=t}^{T} r_{t'} - c_\phi(a_t,s_t) \right] +\frac{\partial}{\partial\theta} c_\phi(a_t, s_t), \\
a_t = a(\epsilon_t,s_t, \theta) \qquad \epsilon_t \sim p(\epsilon_t)\nonumber.
\label{eq:rl_est}
\end{align}
%
This estimator is unbiased if the true dynamics of the system are Markovian w.r.t. the state $s_t$.
When $T = 1$, we recover the special case ${\hat g_\LAX^{\RL} = \hat g_\LAX}$.
Comparing $\hat g_\LAX^{\RL}$ to the standard advantage actor-critic estimator in~\eqref{eq:rl_a2c}, the main difference is that our baseline $c_\phi(a_t, s_t)$ is action-dependent while still remaining unbiased.

To optimize the parameters $\phi$ of our control variate $c_\phi(a_t, s_t)$, we can again use the single-sample estimator of the gradient of our estimator's variance given in~\eqref{eq:vargrad}.
This approach avoids unstable training dynamics, and doesn't require storage and replay of previous rollouts.

Details of this derivation, as well as the discrete and conditionally reparameterized version of this estimator can be found in appendix~\ref{rl appendix}.


\section{Scope and Limitations}
\label{limitations}
The work most related to ours is the recently-developed REBAR method~\citep{tucker2017rebar}, which greatly inspired our work.
The REBAR estimator is a special case of the \RELAX{} estimator, when the surrogate is set to ${c_\phi(z) = \eta \cdot f(\texttt{softmax}_\lambda(z))}$.
The only free parameters of the REBAR estimator are the scaling factor $\eta$, and the temperature $\lambda$, which gives limited scope to optimize the surrogate.
REBAR can only be applied when $f$ is known and differentiable.
Furthermore, it depends on essentially undefined behavior of the function being optimized, since it evaluates the discrete loss function at continuous inputs.

Because \LAX{} and \RELAX{} can construct a surrogate from scratch, they can be used for optimizing black-box functions, as in reinforcement learning settings where the reward is an unknown function of the environment.
\LAX{} and \RELAX{} only require that we can query the function being optimized, and can sample from and differentiate $p(b|\theta)$.

%In principle one could use \RELAX{} to optimize deterministic black-box functions, but only by introducing stochasticity to the inputs.
%Thus, \RELAX{} is most suitable for problems where one is already optimizing a distribution over inputs, such as in inference or reinforcement learning.
% Note that most optimal policies are deterministic?

\paragraph{Direct dependence on parameters}
Above, we assumed that the function $f$ being optimized does not depend directly on $\theta$, which is usually the case in black-box optimization settings.
However, a dependence on $\theta$ can occur when training probabilistic models, or when we add a regularizer. % to the parameters. %a black-box optimization problem.
In both these settings, if the dependence on $\theta$ is known and differentiable, we can use the fact that
%
\begin{align}
\PT \E_{p(b|\theta)}[f(b, \theta)] = \E_{p(b|\theta)}\left[\PT f(b, \theta) + f(b, \theta)\PT \log p(b|\theta) \right]
\end{align}
%
and simply add $\PT f(b, \theta)$ to any of the gradient estimators above to recover an unbiased estimator.


\section{Related work}
%There has been a great deal of other recent work in the area of gradient estimation.
\citet{miller2017reducing} reduce the variance of reparameterization gradients in an orthogonal way to ours by approximating the gradient-generating procedure with a simple model and using that model as a control variate.
NVIL~\citep{mnih2014neural} and VIMCO~\citep{mnih2016variational} provide reduced variance gradient estimation in the special case of discrete latent variable models and discrete latent variable models with Monte Carlo objectives.
\citet{salimans2017evolution} estimate gradients using a form of finite differences, evaluating hundreds of different parameter values in parallel to construct a gradient estimate.
In contrast, our method is a single-sample estimator.

%As gradient estimators become more complex, checking their unbiasedness numerically becomes difficult.
%The automatic theorem-proving-based unbiasedness checker developed by \citet{selsam2017developing} may become relevant to this line of research.

%Also: \citep{levine2016end}

\citet{staines2012variational} address the general problem of developing gradient estimators for deterministic black-box functions or discrete optimization.
They introduce a sampling distribution, and optimize an objective similar to ours.
\citet{wierstra2014natural} also introduce a sampling distribution to build a gradient estimator, and consider optimizing the sampling distribution.
In the context of general Monte Carlo integration, \citet{oates2017control} introduce a non-parametric control variate that also leverages gradient information to reduce the variance of an estimator.

In parallel to our work, there has been a string of recent developments on action-dependent baselines for policy-gradient methods in reinforcement learning.  Such works include \citet{gu2016q} and \citet{gu2017interpolated} which train an action-dependent baseline which incorporates off-policy data. \cite{liu2017sample} independently develop a method similar to LAX applied to continuous control. \citet{Wu2018Factorized} exploit per-dimension independence of the action distribution in continuous control tasks to produce an action-dependent unbiased baseline. 

%In the reinforcement learning setting, the work most similar to ours is $Q$-prop \citep{gu2016q}.
%Like our method, $Q$-prop reduces the variance of the policy gradient with a learned, action-dependent control variate whose expectation is approximated via a Monte Carlo sample from a Taylor series expansion of the control variate.
%Unlike our method, their control variate is trained off-policy.
%While our method is applicable in both the continuous and discrete action domain, $Q$-prop is only applicable to continuous actions.
%We are interested in the potential of training our control variate off-policy, but we leave that for further work. 

%\citet{asadi2017mean} reduce the variance of actor-critic gradient estimates by simply summing over all possible actions.

%\par{Generalized Reparameterization Gradients}
%REBAR and the generalization in this paper uses a mixture of score function and reparameterization gradients.
%A recent paper by \cite{ruiz2016generalized} unifies these two gradient estimators as the generalized reparameterization gradient (GRG).
%This framework can help disentangle the various components of generalized REBAR.

%REBAR innovation as further decomposition the correction term into secondary reparameterization components
%note this is a recursive application of the principles of GRG
%observe that the GRG suggests this recursive application to components of an estimator
%propose that other estimators could be similarly recursively decomposed?


\section{Applications}
\label{Applications}
%
\begin{wrapfigure}[21]{R}{0.50\textwidth}
\centering
\vspace{-10mm}
\begin{tabular}{c}
REINFORCE\\
\hspace{-3mm}\includegraphics[width=0.50\columnwidth, clip, trim=0.5cm 8cm 0.8cm 0.5cm]{figures/relaxations_t_499_which_2}\\
REBAR\\
\hspace{-3mm}\includegraphics[width=0.50\columnwidth, clip, trim=0.5cm 5cm 0.8cm 3.9cm]{figures/relaxations_t_499_which_2}\\
RELAX\\
\hspace{-3mm}\includegraphics[width=0.50\columnwidth, clip, trim=0.5cm 0cm 0.8cm 7.2cm]{figures/relaxations_t_499_which_2}
\end{tabular}
\vspace*{-6mm}
\caption{The optimal relaxation for a toy loss function, using different gradient estimators.
Because REBAR uses the concrete relaxation of $f$, which happens to be implemented as a quadratic function, the optimal relaxation is constrained to be a warped quadratic.
In contrast, RELAX can choose a free-form relaxation.}
% ${\mathcal{L}(\theta) = \E_{p(b|\theta)} [ (b - 0.499)^2 ]}$ using REINFORCE (blue), REBAR (green), and RELAX (red)} 
\label{learned-relaxations}
\end{wrapfigure}
%
We demonstrate the effectiveness of our estimator on a number of challenging optimization problems. Following~\citet{tucker2017rebar} we begin with a simple toy example to illuminate the potential of our method and then continue to the more relevant problems of optimizing binary VAE's and reinforcement learning.

\subsection{Toy experiment}
As a simple example, we follow \citet{tucker2017rebar} in minimizing $\mathbb{E}_{p(b|\theta)}[(b - t)^2]$ as a function of the parameter $\theta$ where {$p(b|\theta) = \textnormal{Bernoulli}(b|\theta)$}.
\citet{tucker2017rebar} set the target $t = .45$.
We focus on the more challenging case where $t = .499$.
Figures~\ref{first figure}a and \ref{first figure}b show the relative performance and gradient log-variance of REINFORCE, REBAR, and RELAX.

%With this setting of the target, REBAR and competing methods suffer from high variance and are unable to discover the optimal solution $\theta = 0$.
%The fixed concrete relaxation of REBAR is unable to produce a gradient whose signal outweighs the sample noise and is therefore unable to solve this problem noticeably faster than REINFORCE.

Figure~\ref{learned-relaxations} plots the learned surrogate $c_\phi$ for a fixed value of $\theta$. We can see that $c_\phi$ is near $f$ for all $z$, keeping the variance of the REINFORCE part of the estimator small. Moreover the derivative of $c_\phi$ is positive for all $z$ meaning that the reparameterization part of the estimator will produce gradients pointing in the correct direction to optimize the expectation. Conversely, the concrete relaxation of REBAR is close to $f$ only near $0$ and $1$ and its gradient points in the correct direction only for values of $z > \log (\frac{1-t}{t})$. These factors together result in the RELAX estimator achieving the best performance. 

\subsection{Discrete variational autoencoder}
\label{vae section}
Next, we evaluate the \RELAX{} estimator on the task of training a variational autoencoder~\citep{kingma2013autoencoding, rezende2014stochastic} with Bernoulli latent variables.
We reproduced the variational autoencoder experiments from \citet{tucker2017rebar}, training models with one or two layers of 200 Bernoulli random variables with linear or nonlinear mappings between them, on both  the MNIST and Omniglot~\citep{lake2015human} datasets.
Details of these models and our experimental procedure can be found in Appendix~\ref{app_disc_vae}.

To take advantage of the available structure in the loss function, we choose the form of our control variate to be $c_\phi(z) = f(\sigma_\lambda(z))+  \hat{r}_\rho(z)$ where $\hat{r}_\rho$ is a neural network with parameters $\rho$ and $f(\sigma_\lambda(z))$ is the discrete loss function, the evidence lower-bound (ELBO), evaluated at continuously relaxed inputs as in REBAR.  
%
In all experiments, the learned control variate improved the training performance, over the state-of-the-art baseline of REBAR. In both linear models, we achieved improved validation performance as well increased convergence speed. We believe the decrease in validation performance for the nonlinear models was due to overfitting caused by improved optimization of an under-regularized model. We leave exploring this phenomenon to further work. 

\begin{table}[h]
\centering
\begin{tabular}{r l | c c c c c} 
Dataset & Model & Concrete & NVIL & MuProp  & REBAR & RELAX\\\midrule 
               & Nonlinear & $-102.2$ & $-101.5$ & -101.1  &  -81.01 &  \textbf{-78.13} \\
\textbf{MNIST} & linear one-layer  &-111.3 & $-112.5$ & $-111.7$  & -111.6 & \textbf{-111.20} \\ 
               & linear two-layer  &-99.62 & $-99.6$ & $-99.07$   & -98.22 & \textbf{-98.00} \\
\midrule
               & Nonlinear  & $-110.4$  & $-109.58$ & -108.72  & -56.76 & \textbf{-56.12} \\
\textbf{Omniglot} & linear one-layer &-117.23 & $-117.44$ & $-117.09$   & -116.63 & \textbf{-116.57} \\ 
                  & linear two-layer &-109.95 & $-109.98$ & $-109.55$  & -108.71 & \textbf{-108.54}
\end{tabular}
\caption{Highest training ELBO for discrete variational autoencoders.}
\label{tab:vae tr}
\end{table}




%In \citep{tucker2017rebar}, a separate REBAR estimator was used to estimate the gradients of each model parameter (each weight matrix and bias vector).
%To apply our estimator to this formulation, we would need to learn a separate relaxation for each model parameter.
%To get around this, we use our gradient estimator to approximate $g_\phi \approx \PT \E_{q(b|\theta)}[f(b)]$ where $x\cdot W = \theta$ is the parameters of the Bernoulli latent variables, $W$ is our layer's weight matrix. We then obtain an estimate of $\PP{W} \E_{q(b|\theta)}[f(b)] = g_\phi\cdot \frac{\partial \theta}{\partial W}$. We note this gives us unbiased gradients because 
%\begin{align}
%\E_\epsilon[g_\phi(\epsilon) \cdot \frac{\partial \theta}{\partial W}] = \E_\epsilon[g_\phi(\epsilon)] \cdot \frac{\partial \theta}{\partial W} =  \PT \E_{q(b|\theta)}[f(b)] \cdot \frac{\partial \theta}{\partial W} = 
%\frac{\partial}{\partial W} \E_{q(b|\theta)}[f(b)]
%\end{align} 

%To provide a fair comparison, we re-implemented REBAR in this way (denoted REBAR-ours in table~\ref{tab:vae}).
%We believe this explains the large difference in performance between our implementation and that of \citep{tucker2017rebar} for the nonlinear models since there are 3 layers of parameters that all share the same gradient estimator.
%In the linear models, each layer has its own gradient estimator making our implementation closer to that of \citep{tucker2017rebar}.
To obtain training curves we created our own implementation of REBAR, which gave identical or slightly improved performance compared to the implementation of \citet{tucker2017rebar}.

While we obtained a modest improvement in training and validation scores (tables~\ref{tab:vae tr} and \ref{tab:vae val}), the most notable improvement provided by \RELAX{} is in its rate of convergence.
Training curves for all models can be seen in Figure~\ref{fig:vae_curves} and in Appendix~\ref{extra vae results}.
In Table~\ref{tab:vae epochs} we compare the number of training epochs that are required to match the best validation score of REBAR.
In both linear models, RELAX provides an increase in rate of convergence. 

\begin{figure}
\centering
\hspace*{-.5in}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{ccc}
&MNIST & Omniglot \\
\rotatebox{90}{\qquad \qquad \qquad \small -ELBO} & 
\includegraphics[width=.33\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/MNIST_L1} &
%\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 3mm]{figures/MNIST_L2} &
\includegraphics[width=.35\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/OMNIGLOT_L1}
%\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 3mm]{figures/OMNIGLOT_L2}\\
\end{tabular}
\caption{Training curves for the VAE Experiments with the one-layer linear model.
%L1 represents the one-layer linear model and L2 represents the two-layer linear model.
The horizontal dashed line indicates the lowest validation error obtained by REBAR.}
\label{fig:vae_curves}
\end{figure}

%\begin{figure}[h]
%\centering
%\includegraphics[width=.4\textwidth]{figures/OMNIGLOT_L1}
%\includegraphics[width=.4\textwidth]{figures/OMNIGLOT_L2}
%\caption{Training curves on OMNIGLOT. The horizontal dashed line indicates the lowest validation score obtained by REBAR.}
%\label{fig:vae omni}
%\end{figure}




\subsection{Reinforcement learning}
\label{experiments section}

We apply our gradient estimator to a few simple reinforcement learning environments with discrete and continuous actions.
We use the \RELAX{} and \LAX{} estimators for discrete and continuous actions, respectively. We compare with the advantage actor-critic algorithm (A2C)~\citep{sutton2000policy} as a baseline. 

As our control variate does not have the same interpretation as the value function of A2C, it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model. For instance, to do reward bootstrapping, we would need to use the state-value function. In the discrete experiments, due to the simplicity of the tasks, we chose not to use reward bootstrapping, and therefore omitted the use of state-value function. However, with the more complicated continuous tasks, we chose to use the value function to enable bootstrapping. In this case, the control variate takes the form: $c_\phi(a,s) = V(s) + \hat{c}(a,s)$, where $V(s)$ is trained as it would be in A2C. Full details of our experiments can be found in Appendix~\ref{experiment appendix}.

% <Dami: i took it out because it doesn't match with the continuous case, and we dont' have variance plots yet>We are aware that better results could be obtained with bootstrapping and larger batch sizes but we wanted to work in the highest possible variance setting to demonstrate the variance reduction capabilities of our approach.

%We test our alrgorithm on the Cart-Pole and Lunar-Lander environments from the OpenAI Gym~\citep{1606.01540}.
%We run the Cart-Pole and Lunar-Lander environments for 250 and 1000 episodes, respectively and plot reward and the log-variance of the policy gradients in figure~X.

In the discrete action setting, we test our approach on the Cart Pole and Lunar Lander environments as provided by the OpenAI gym~\citep{1606.01540}.
In the continuous action setting, we test on the MuJoCo-simulated~\citep{todorov2012mujoco} environment Inverted Pendulum also found in the OpenAI gym.
% and Inverted Double Pendulum also found in the OpenAI gym.
In all tested environments we observe improved performance and sample efficiency using our method.
The results of our experiments can be seen in Figure~\ref{fig:rl_results}, and Table~\ref{tab:rl_results}.

We found that our estimator produced policy gradients with drastically reduced variance (see Figure~\ref{fig:rl_results}) allowing for larger learning rates to be used while maintaining stable training.
In both discrete environments our estimator achieved greater than a 2-times speedup in convergence over the baseline.

\newcommand{\rlfig}[1]{{\includegraphics[height=0.24\linewidth, clip, trim=3mm 3mm 3mm 3mm]{#1}}}%
\newcommand{\rlfigg}[1]{{\includegraphics[height=0.24\linewidth, clip, trim=2mm 2mm 2mm 2mm]{#1}}}%
\begin{figure}%
\centering
\hspace*{-.1in}
\setlength{\tabcolsep}{0pt}
\begin{tabular}{cccc}%c}%
%\renewcommand{\arraystretch}{0}
& Cart-pole & Lunar lander & Inverted pendulum \\%& Inverted double pendulum\\
\rotatebox{90}{\qquad \qquad \small Reward} & \rlfig{figures/cp_paper} & 
\rlfig{figures/ll_paper} &
\rlfigg{figures/ip_paper_NEW} \\%& \rlfig{figures/idp_paper}\\
\rotatebox{90}{\qquad \qquad \small Log-Variance} & \rlfig{figures/cp_paper_var} & 
\rlfig{figures/ll_paper_var} &
\rlfigg{figures/ip_paper_var_NEW} \\%& \rlfig{figures/idp_paper_var}
\end{tabular}
\caption{\emph{Top row:} Reward curves.
\emph{Bottom row:} Log-variance of policy gradients.
In each curve, the center line indicates the mean reward over 5 random seeds.
The opaque bars in the top row indicate the 25th and 75th percentiles.
The opaque bars in the bottom row indicate 1 standard deviation. Since the gradient estimator is defined at the end of each episode, we display log-variance per episode.
After every 10th training episode 100 episodes were run and the sample log-variance is reported averaged over all policy parameters. }
\label{fig:rl_results}
\end{figure}

\begin{table}%
\centering
\begin{tabular}{l | c c c }%c }%
\textbf{Model} & Cart-pole & Lunar lander & Inverted pendulum \\\midrule%& Inverted double pendulum \\\midrule
A2C             & $1152 \pm 90$ & $162374 \pm 17241$                    & $6243 \pm 164$ \\%& $60186 \pm 3488$  \\
LAX/RELAX & $\bm{472 \pm 114}$ & $\bm{68712 \pm 20668}$ & $\bm{2067 \pm 412}$ \\%& $\bm{60967 \pm 1669}$
\end{tabular}
\caption{Mean episodes to solve tasks.
Definitions of solving each task can be found in Appendix~\ref{experiment appendix}.}
\label{tab:rl_results}
\end{table}

%Code for all experiments can be found at \href{https://github.com/duvenaud/relax}{github.com/duvenaud/relax}.


\section{Conclusions and future work}
\label{conclusion}
In this work we synthesized and generalized several standard approaches for constructing gradient estimators.
We proposed a generic gradient estimator that can be applied to expectations of known or black-box functions of discrete or continuous random variables, and adds little computational overhead.
We also derived a simple extension to reinforcement learning in both discrete and continuous-action domains. 
%This approach is relatively simple to implement and adds little computational overhead. 

%The foundation of our approach is the score function gradient estimator with a control variate whose expectation can be estimated with low variance using the reparameterization trick.
%This control variate is neural network which is trained directly to minimize the variance of the estimated gradients.
%The central result of this paper is that learning the function in the control variate leads to even better convergence properties and lower variance gradient estimates. 

%The generality of this method opens up new possibilities for training non-differentiable models. % which can now be trained via gradient decent.
Future applications of this method could include training models with hard attention or memory indexing~\citep{zaremba2015reinforcement}.
One could also apply our estimators to continuous latent-variable models whose likelihood is non-differentiable, such as a 3D rendering engine.
Extensions to the reparameterization gradient estimator~\citep{ruiz2016generalized, naesseth2017reparameterization} could also be applied to increase the scope of distributions that can be modeled. 
%There is also room to explore other architecture choices for the control variate.% and to better understand the properties of the optimal control variate. 
%

 In the reinforcement learning setting, our method could be combined with other variance-reduction techniques such as generalized advantage estimation~\citep{kimura2000analysis, schulman2015high}, or other optimization methods, such as KFAC~\citep{wu2017scalable}.
One could also train our control variate off-policy, as in $Q$-prop~\citep{gu2016q}.
%We also feel that the relationship between our learned control variate and the action-value function (commonly denoted as $Q$) is worth exploring and understanding in greater detail.

\subsection*{Acknowledgements}  % Uncomment for arxiv
We thank Dougal Maclaurin, Tian Qi Chen, Elliot Creager, and Bowen Xu for helpful discussions.
We also thank Christopher Prohm for pointing out an error in one of our derivations. We would also like to thank George Tucker for pointing out a bug in our initially released reinforcement learning code. 

\bibliography{bibliography}
\bibliographystyle{iclr2018_conference}



% ** Questions to answer: 
% (1) is z-tilde a clever way of using Rao-Blackwellization for a part of the reparameterization gradient? This would mean that the reparameterization z-tilde is related to the sufficient statistic of the estimator...
% (1) also maybe: the Q function has the opportunity to learn an estimator based on the sufficient statistics of the model, which by Rao-Blackwell-Kolmogorov is lower variance
% (2) What's a better notation to keep the dependence of z on $\theta$ in view?
% (3) is the REBAR control variate really using the reparameterization gradient in a meaningful way? Or, is it best viewed as just another f + control variate where control variate is cleverly designed with lower variance? 


%\par{Generalizing the reparameterization trick}

%Write sample from distribution $s(\epsilon)$ as $\epsilon = \mathcal{T}^{-1}(\mathbf{z}; \mathbf{\nu})$ for some invertible transform $\mathcal{T}$ with variational parameters $\nu$.
%write out transformed density
%example: normal with standard normal $s$
%example: inverse CDF of Gaussian with uniform $s$
%write out expected gradient under transformation
%show decomposition of expected gradient into reparameterization and correction terms 

%\par{Applying GRG to REBAR}

%show mapping of terms
%note denser derivation in REBAR appendix

%\par{Interpreting REBAR through GRG}

\clearpage
\section*{Appendices}
\appendix




\section{The RELAX Algorithm}
\label{relax proof}

\begin{proof}
	We show that $\hat g_\textnormal{RELAX}$ is an unbiased estimator of $\frac{\partial}{\partial \theta} \mathbb{E}_{p(b \vert \theta)} \left[ f(b) \right]$. The estimator is
	
	\begin{align*}
	\E_{p(b|\theta)} \! \left[\left[ f(b) - \E_{p(\tilde{z}|b, \theta)} \! \left[c_\phi(\tilde{z}) \right] \right]\PT \log p(b|\theta)  - \PT \E_{p(\tilde{z}|b, \theta)} \! \left[c_\phi(\tilde{z}) \right] \right] + \PT\E_{p(z|\theta)} \! \left[ c_\phi(z) \right]. \span & \nonumber \\
	\end{align*}
	Expanding the expectation for clarity of exposition, we account for each term in the estimator separately:
	\begin{align}
	& \E_{p(b|\theta)} \! \left[ f(b) \PT \log p(b|\theta)  \right] \label{one}\\ 
	& - \E_{p(b|\theta)} \left[ \E_{p(\tilde{z}|b, \theta)} \! \left[c_\phi (\tilde{z}) \right] \PT \log p(b|\theta)  \right] \label{two}\\
	& - \E_{p(b|\theta)}  \left[ \PT \E_{p(\tilde{z}|b, \theta)} \! \left[c_\phi(\tilde{z}) \right] \right] \label{three}\\
	& + \PT\E_{p(z|\theta)} \! \left[ c_\phi(z) \right]. \label{four}
	\end{align}
	Term \eqref{one} is an unbiased score-function estimator of $\frac{\partial}{\partial \theta} \mathbb{E}_{p(b \vert \theta)} \left[ f(b) \right]$. It remains to show that the other three terms are zero in expectation. Following \cite{tucker2017rebar} (see the appendices of that paper for a derivation), we rewrite term \eqref{three} as follows:
	\begin{align}
	-\mathbb{E}_{p(b \vert \theta)} \left[ \frac{\partial}{\partial \theta} \mathbb{E}_{p(\tilde{z} \vert b, \theta)} \left[ c_\phi (\tilde{z}) \right] \right] =~&\mathbb{E}_{p(b \vert \theta)} \left[ \mathbb{E}_{p(\tilde{z} \vert b, \theta)} \left[ c_\phi(\tilde{z}) \right] \frac{\partial}{\partial \theta} \log p(b \vert \theta) \right] \nonumber \\ & -  \mathbb{E}_{p(z \vert \theta)} \left[ c_\phi(z) \frac{\partial}{\partial \theta} \log p(z) \right] . \label{six}
	\end{align}
	Note that the first term on the right-hand side of equation \eqref{six} is equal to term \eqref{two} with opposite sign. The second term on the right-hand side of equation \eqref{six} is the score-function estimator of term \eqref{four}, opposite in sign. The sum of these terms is zero in expectation.
	\\
\end{proof}


\begin{algorithm}[h]
	\begin{algorithmic}
		\Require $f(\cdot)$, $\log p(b|\theta)$, reparameterized samplers $b = H(z)$, $z = S(\epsilon, \theta)$ and $\tilde{z} = S(\epsilon, \theta | b)$, \\ 
		\hspace{3em} neural network $c_\phi(\cdot)$, step sizes $\alpha_1, \alpha_2$  \While {not converged} 
		\State $\epsilon_{i}, \widetilde{\epsilon_i} \sim p(\epsilon)$ \Comment Sample noise
		\State $z_i \leftarrow S(\epsilon_i, \theta)$ \Comment Compute unconditional relaxed input
		\State $b_i \leftarrow H(z_i)$ \Comment Compute input
		\State $\widetilde{z_i} \leftarrow S(\widetilde{\epsilon_i}, \theta | b_i)$ \Comment Compute conditional relaxed input
		\State  $\hat{g}_\theta \leftarrow \left[f(b_i) - c_{\phi}(\widetilde{z_i}) \right] \nabla_\theta \log p + \nabla_\theta c_\phi(z_i) - \nabla_\theta c_\phi(\widetilde{z_i})$ \Comment Estimate gradient
		\State  $\hat{g}_\phi \leftarrow \partial \hat{g}_\theta^2 / \partial \phi$ \Comment Estimate gradient of variance of gradient
		\State $\theta \leftarrow \theta - \alpha_1 \hat{g}_\theta$ \Comment Update parameters
		\State $\phi \leftarrow \phi - \alpha_2 \hat{g}_\phi$ \Comment Update control variate
		\EndWhile
		\State \textbf{return} $\theta$ 
	\end{algorithmic}
	\caption{\RELAX{}: Low-variance control variate optimization for black-box gradient estimation.}
	\label{relax}
\end{algorithm}





\section{Conditional Re-sampling for Discrete Random Variables}
\label{resample}
When applying the RELAX estimator to a function of discrete random variables $b \sim p(b|\theta)$, we require that there exists a distribution $p(z|\theta)$ and a deterministic mapping $H(z)$ such that if $z \sim p(z|\theta)$ then $H(z) = b \sim p(b|\theta)$. Treating both $b$ and $z$ as random, this procedure defines a probabilistic model $p(b, z | \theta) = p(b|z)p(z|\theta)$. The RELAX estimator requires reparameterized samples from $p(z|\theta)$ and $p(z|b,\theta)$. We describe how to sample from these distributions in the common cases of $p(b|\theta) = \text{Bernoulli}(\theta)$ and $p(b|\theta) = \text{Categorical}(\theta)$.

\paragraph{Bernoulli} When $p(b|\theta)$ is Bernoulli distribution we let $H(z) = \mathbb{I}(z>0)$ and we sample from $p(z|\theta)$ with 
$$ z = \log \frac{\theta}{1 - \theta} + \log \frac{u}{1-u}, \qquad u \sim \text{uniform}[0,1].
$$ 
We can sample from $p(z|b, \theta)$ with 
\[
v' =    \left\{
\begin{array}{ll}
      v\cdot(1-\theta) & b = 0 \\
      v\cdot\theta + (1 - \theta) & b = 1 \\
\end{array} 
\right.
\]
$$ \tilde{z} = \log \frac{\theta}{1 - \theta} + \log \frac{v'}{1-v'}, \qquad v \sim \text{uniform}[0, 1].$$

\paragraph{Categorical} When $p(b|\theta)$ is a Categorical distribution where $\theta_i = p(b=i|\theta)$, we let $H(z) = \text{argmax}(z)$ and we sample from $p(z|\theta)$ with 
$$ z = \log\theta -\log(-\log u), \qquad u \sim \text{uniform}[0,1]^k
$$ where $k$ is the number of possible outcomes.


To sample from $p(z|b, \theta)$, we note that the distribution of the largest $\hat z_b$ is independent of $\theta$, and can be sampled as $\hat z_b = -\log (-\log v_b)$ where $v_b\sim \text{uniform}[0, 1]$.
Then, the remaining $v_{i\neq b}$ can be sampled as before but with their underlying noise truncated so $\hat z_{i \neq b} < \hat z_b$. As shown in the appendix of \cite{tucker2017rebar}, we can then sample from $p(z|b, \theta)$ with:
%
\begin{align}
\hat z_i =    \left\{
\begin{array}{ll}
      -\log(-\log v_i )& i = b \\
      -\log \left( -\frac{\log v_i}{\theta_i} - \log v_b \right) & i \neq b \\
\end{array} 
\right.
\end{align}
%
where $v_i \sim \text{uniform}[0, 1]$.


%To sample from $p(z|b, \theta)$ we sample a value $v'$ and compute $\tilde{z} = \log\theta -\log(-\log v')$. We note that in the unconditional case we would have $v'_b \sim \text{uniform}[0, 1]$ but in the conditional case $v'_b \sim \text{Beta}\left[1+\frac{1 - \theta_b}{\theta_b}, 1\right]$. We first sample $v'_b$ in this way. Then we can sample $v'_{i\neq b}$ by finding the point in $[0, 1]$ where $z_b = z_{i\neq b}$ and scaling a uniform random variable $v_i$ to be below that value. 

%Intuitively, to sample from $p(z|b, \theta)$ we should first sample $v\sim \text{uniform}[0, 1]^k$, then compute $g_b = \log\theta_b -\log(-\log(v_b))$. Then we must determine how to scale each $v_{i\neq b}$ such that $g_{i\neq b} < g_b$. We can define $v'$ such that
%Formally,
%\[
%v_i' =    \left\{
%\begin{array}{ll}
%      v'_b & i = b \\
 %     v_i\cdot(v'_b)^{\frac{\theta_i}{\theta_b}} & i \neq b \\
%\end{array} 
%\right.
%\] 
%$$ v'_b \sim \text{Beta}\left[1+\frac{1 - \theta_b}{\theta_b}, 1\right], \qquad v_{i\neq b} \sim \text{uniform}[0, 1]$$ and then $\tilde{z} = \log\theta - \log(-\log v')$ which is our sample from $p(z|b, \theta)$. 

%Let $G_{1:k} = -\log-\log(U_{i:k})$ be samples from the Gumbel distribution, and learnable parameters $(\alpha_1, \dots, \alpha_k)$ be interpreted as some unnormalized parameterization of the discrete distribution under consideration.
%Then, consider the following sampling procedure: for each k, find the k that maximizes $\log \alpha_k - G_k$, and then set $D_k=1$ and $D_{i \neq k} = 0$. The Gumbel-Max trick states that sampling from the discrete distribution is equivalent to taking this argmax, that is, $p(D_k = 1) = \alpha_k / \sum_{i=1}^n \alpha_i$.

%Since taking an argmax is still a discontinuous operation, \cite{maddison2016concrete} and \cite{jang2016categorical} proposed further relaxing the argmax operator through the softmax function with an additional temperature parameter $\lambda$:
%\begin{equation}
%x_k = \frac{\exp\{( \log \alpha_k+ G_k) / \lambda\}}{\sum_{i=1}^n\exp\{( \log \alpha_i+ G_i) / \lambda\}}
%\end{equation}
%This relaxation allows values within the simplex, but in the low temperature limit, it becomes exactly the discrete argmax.
%One limitation of the concrete distribution is that it is a biased estimator except in limiting temperature.
%In other words, a small amount of bias is present for a non-zero temperature.


\section{Derivations of estimators used in Reinforcement learning}
\label{rl appendix}
We give the derivation of the \LAX{} estimator used for continuous RL tasks.
\begin{theorem}
The \LAX{} estimator,
\begin{align}
\hat g_\LAX^{\RL} = \sum_{t=1}^{T} \LL{t} \left[ \sum_{t'=t}^{T} r_{t'} - c_\phi(a_t,s_t) \right] +\frac{\partial}{\partial\theta} c_\phi(a_t, s_t), \\
a_t = a_t(\epsilon_t,s_t,\theta), \quad \epsilon_t \sim p(\epsilon_t)\nonumber,
\end{align}
is unbiased.
\end{theorem}
\begin{proof}
Note that by using the score-function estimator, for all $t$, we have 
%
\begin{align*}
\E_{p(\tau)}\Big[\LL{t} c_\phi(a_t, s_t)\Big] = \E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{\pi(a_t|s_t, \theta)}\Big[c_\phi(a_t, s_t)\Big]\Big].
\end{align*}
Then, by adding and subtracting the same term, we have
\begin{align*}
\PT\E_{p(\tau)}[f(\tau)] &= \E_{p(\tau)}\left[f(\tau)\cdot\LP{\tau;\theta}\right]-\sum_t\E_{p(\tau)}\Big[\LL{t} c_\phi(a_t, s_t)\Big]+\\&\sum_t \E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{\pi(a_t|s_t, \theta)}\Big[c_\phi(a_t,s_t)\Big]\Big]\nonumber\\
&= \E_{p(\tau)}\left[ \sum_{t=1}^{\infty} \LL{t}\left(\sum_{t'=t}^{\infty} r_{t'} - c_\phi(a_t,s_t)\right)\right]\\
& \qquad + \sum_t \E_{p(a_{1:t-1},s_{1:t})}\Big[\E_{p(\epsilon_t)}\Big[\frac{\partial}{\partial\theta}c_\phi(a_t(\epsilon_t,s_t,\theta), s_t)\Big]\Big]\nonumber\\
&= \E_{p(\tau)}\left[ \sum_{t=1}^{\infty} \LL{t}\left(\sum_{t'=t}^{\infty} r_{t'} - c_\phi(a_t,s_t)\right)+\frac{\partial}{\partial\theta}c_\phi(a_t(\epsilon_t,s_t,\theta), s_t)\right]\nonumber
\end{align*}
\end{proof}

In the discrete control setting, our policy parameterizes a soft-max distribution which we use to sample actions. We define $z_t\sim p(z_t|s_t)$, which is equal to $\sigma (\log\pi - \log(-\log(u)))$ where $u\sim \text{uniform}[0, 1]$, $a_t = \text{argmax}(z_t)$, $\sigma$ is the soft-max function. We also define $\tilde{z_t} \sim p(z_t|a_t,s_t)$ and uses the same reparametrization trick for sampling $\tilde{z_t}$ as explicated in Appendix \ref{resample}.
\begin{theorem}
The \RELAX{} estimator,
\begin{align}
\hat g_\RELAX^{\RL} = \sum_{t=1}^{T} \LL{t}\left(\sum_{t'=t}^{T} r_{t'} - c_\phi(\tilde{z}_t, s_t)\right)-\frac{\partial}{\partial\theta}c_\phi(\tilde{z}_t, s_t)+\frac{\partial}{\partial\theta}c_\phi(z_t, s_t), \label{eq:relaxrlproof}\\
\tilde{z}_t \sim p(z_t|a_t,s_t), \qquad z_t \sim p(z_t|s_t)\nonumber, 
\end{align}
is unbiased.
\end{theorem}
\begin{proof}
Note that by using the score-function estimator, for all $t$, we have 
%
\begin{align*}
& \E_{p(a_{1:t},s_{1:t})}\Big[\LL{t} \E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)]\Big]\\
 &= \E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{\pi(a_t|s_t, \theta)}\Big[\E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)]\Big]\Big]\\
&=\E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{p(z_t|s_t)}[c_\phi(z_t, s_t)]\Big]
\end{align*}
Then, by adding and subtracting the same term, we have
\begin{align*}
\PT\E_{p(\tau)}[f(\tau)] &= \E_{p(\tau)}\left[f(\tau)\cdot\LP{\tau;\theta}\right]\\
& \qquad -\sum_t\E_{p(a_{1:t},s_{1:t})}\Big[\LL{t} \E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)]\Big]\\
& \qquad + \sum_t\E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{p(z_t|s_t)}[c_\phi(z_t, s_t)]\Big]\nonumber\\
& = \E_{p(\tau)}\left[ \sum_{t=1}^{\infty} \LL{t}\left(\sum_{t'=t}^{\infty} r_{t'} - \E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)]\right)\right]\\
& \qquad + \sum_t\E_{p(a_{1:t-1},s_{1:t})}\Big[\frac{\partial}{\partial\theta}\E_{p(z_t|s_t)}[c_\phi(z_t, s_t)]\Big]\nonumber\\
& = \E_{p(\tau)}\Big[ \sum_{t=1}^{\infty} \LL{t}\left(\sum_{t'=t}^{\infty} r_{t'} - \E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)\right)\\
& \qquad - \frac{\partial}{\partial\theta}\E_{p(z_t|a_t,s_t)}[c_\phi(z_t, s_t)]+\frac{\partial}{\partial\theta}\E_{p(z_t|s_t)}[c_\phi(z_t, s_t)]\Big]\nonumber
\end{align*}
Since $p(z_t|s_t)$ is reparametrizable, we obtain the estimator in Eq.(\ref{eq:relaxrlproof}).
\end{proof}

\section{Further results on discrete variational autoencoders}
\label{extra vae results}

\begin{table}[h]
\centering
\begin{tabular}{r l | c c} 
  Dataset & Model & REBAR & RELAX \\\midrule
 & one-layer linear  & -114.32 & \textbf{-113.62} \\ 
\textbf{MNIST} & two-layer linear  & -101.20 & \textbf{-100.85}\\
& Nonlinear & \textbf{-111.12} & 119.19 \\ \midrule
 & one-layer linear & -122.44 & \textbf{-122.11} \\ 
\textbf{Omniglot}& two-layer linear & -115.83 & \textbf{-115.42}\\
& Nonlinear& \textbf{-127.51} & 128.20
\end{tabular}
\caption{Highest obtained validation ELBO.}
\label{tab:vae val}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{r l | c c} 
 Dataset & Model  & REBAR & RELAX \\\midrule
 & one-layer  & 857 & \textbf{531} \\ 
\textbf{MNIST} & two-layer  & 900 & \textbf{620} \\
& Nonlinear & \textbf{331} & - \\
\midrule
& one-layer & 2086 & \textbf{566} \\ 
\textbf{Omniglot}  & two-layer & 1027 & \textbf{673}\\
& Nonlinear & \textbf{368} & - 
\end{tabular}
\caption{Epochs needed to achieve REBAR's best validation score. ``-'' indicates that the nonlinear RELAX models achieved lower validation scores than REBAR.}
\label{tab:vae epochs}
\end{table}

\begin{figure}
\centering
\hspace*{-.5in}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{ccc}
& MNIST & Omniglot \\
\rotatebox{90}{\qquad \qquad \qquad \small -ELBO} & 
%\includegraphics[width=.33\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/MNIST_L1} &
\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/MNIST_L2} &
%\includegraphics[width=.35\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/OMNIGLOT_L1}
\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/OMNIGLOT_L2}\\
\end{tabular}
\caption{Training curves for the VAE Experiments with the two-layer linear model.
%L1 represents the one-layer linear model and L2 represents the two-layer linear model.
The horizontal dashed line indicates the lowest validation error obtained by REBAR.}
\label{fig:vae curves2}
\end{figure}

\begin{figure}
\centering
\hspace*{-.5in}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{ccc}
& MNIST & Omniglot \\
\rotatebox{90}{\qquad \qquad \qquad \small -ELBO} & 
\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/MNIST_NL1} &
\includegraphics[width=.31\textwidth, clip, trim=3mm 3mm 3mm 2mm]{figures/OMNIGLOT_NL1}\\
\end{tabular}
\caption{Training curves for the VAE Experiments with the one-layer nonlinear model.
The horizontal dashed line indicates the lowest validation error obtained by REBAR.}
\label{fig:vae curves3}
\end{figure}


\section{Experimental Details}
\label{experiment appendix}

\subsection{Discrete VAE}
We run all models for $2,000,000$ iterations with a batch size of $24$.
For the REBAR models, we tested learning rates in $\{.005, .001, .0005,  .0001, .00005\}$. 

\RELAX{} adds more hyperparameters.
These are the depth of the neural network component of our control variate $r_\rho$, the weight decay placed on the network, and the scaling on the learning rate for the control variate.
We tested neural network models with $l$ layers of 200 units using the ReLU nonlinearity with $l \in \{2, 4\}$.
We trained the control variate with weight decay in $\{.001, .0001\}$.
We trained the control variate with learning rate scaling in $\{1, 10\}$.

To limit the size of hyperparameter search for the RELAX models, we only test the best performing learning rate for the REBAR baseline and the next largest learning rate in our search set.
In many cases, we found that RELAX allowed our model to converge at learning rates which made the REBAR estimators diverge.
We believe further improvement could be achieved by tuning this parameter. It should be noted that in our experiments, we found the RELAX method to be fairly insensitive to all hyperparameters other than learning rate. In general, we found the larger (4 layer) control variate architecture with weight decay of $.001$ and learning rate scaling of $1$ to work best, but only slightly outperformed other configurations.  

All presented results are from the models which achieve the highest ELBO on the validation data.
\label{app_disc_vae}
\subsubsection{One-layer linear model}
In the one-layer linear models we optimize the evidence lower bound (ELBO): $$\log p(x) \geq \mathcal{L}(\theta) = \E_{q(b|x)}[\log p(x|b) + \log p(b) - \log q(b|x)]$$ where $q(b_1|x) = \sigma(x\cdot W_q + \beta_q)$ and $p(x| b_1) = \sigma(b_1\cdot W_p + \beta_p)$ with weight matrices $W_q,W_p$ and bias vectors $\beta_q,\beta_p$.
The parameters of the prior $p(b)$ are also learned.


\subsubsection{Two layer linear model}
In the two layer linear models we optimize the ELBO $$\mathcal{L}(\theta) = \E_{q(b_2|b_1)q(b_1|x)}[\log p(x|b_1) + \log p(b_1|b_2) + \log p(b_2) - \log q(b_1|x) - \log q(b_2|b_1)]$$ where $q(b_1|x) = \sigma(x\cdot W_{q_1} + \beta_{q_1})$, $q(b_2|b_1) = \sigma(b_1\cdot W_{q_2} + \beta_{q_2})$, $p(x| b_1) = \sigma(b_1\cdot W_{p_1} + \beta_{p_1})$, and $p(b_1| b_2) = \sigma(b_2\cdot W_{p_2} + \beta_{p_2})$ with weight matrices $W_{q_1},W_{q_2},W_{p_1},W_{p_2}$ and biases $\beta_{q_1},\beta_{q_2},\beta_{p_1},\beta_{p_2}$. As in the one-layer model, the prior $p(b_2)$ is also learned.

\subsubsection{Nonlinear model}
In the one-layer nonlinear model, the mappings between random variables consist of 2 deterministic layers with 200 units using the hyperbolic-tangent nonlinearity followed by a linear layer with 200 units. 

We run an identical hyperpameter search in all models. 


\subsection{Discrete RL}
In both the baseline A2C and RELAX models, the policy and control variate (value function in the baseline model) were two-layer neural networks with 10 units per layer.
The ReLU non linearity was used on all layers except for the output layer which was linear.

For these tasks we estimate the policy gradient with a single Monte Carlo sample.
We run one episode of the environment to completion, compute the discounted rewards, and run one iteration of gradient descent.
We believe using larger batches will improve performance but would less clearly demonstrate the potential of our method. 

%As our control variate does not have the same interpretation as the value function of A2C, it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model. We leave the task of incorporating these and other variance reduction techniques to future work.  

Both models were trained with the RMSProp~\citep{Tieleman2012} optimizer and a reward discount factor of $.99$ was used. Entropy regularization with a weight of $.01$ was used to  encourage exploration. 

Both models have 2 hyperparameters to tune; the global learning rate and the scaling factor on the learning rate for the control variate (or value function).
We complete a grid search for both parameters in $\{0.01, 0.003, 0.001\}$ and present the model which ``solves'' the task in the fewest number of episodes averaged over 5 random seeds.
``Solving'' the tasks was defined by the creators of the OpenAI gym~\citep{1606.01540}.
The Cart Pole task is considered solved if the agent receives an average reward greater than 195 over 100 consecutive episodes.
The Lunar Lander task is considered solved if the agent receives an average reward greater than 200 over 100 consecutive episodes. 

The Cart Pole experiments were run for 250,000 frames.
The Lunar Lander experiments were run for 5,000,000 frames. 

The results presented for the CartPole and LunarLander environments were obtained using a slightly biased sampler for $p(z|b, \theta)$.

\subsection{Continuous RL}
%The continuous tasks uses both the value function and the control variate to enable bootstrapping, which is needed due to the increased complexity of the problem.
The three models- policy, value, and control variate, are two-layer neural networks with 64 hidden units per layer.
The value and control variate networks are identical, with the ELU~\citep{Clevert2016ELUs} nonlinearity in each hidden layer.
The policy network has \texttt{tanh} nonlinearity.
The policy network, which parameterizes the Gaussian policy comprises of a network (with the architecture mentioned above) that outputs the mean, and a separate, trainable log standard deviation value that is not input dependent.
All three networks have a linear output layer.
We selected the batch size to be 2500, meaning for a fixed timestep (2500) we collect multiple rollouts of a task and update the networks' parameters with the batch of episodes.
Per one policy update, we optimize both the value and control variate network multiple times.
The number of times we train  the value network is fixed to 25, while for the control variate, it was chosen to be a hyperparameter. 
%need to add information on whether we build on top of reinforce, or a2c or a2c+GAE
All models were trained using ADAM~\citep{kingma2015adam}, with $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=1e-08$. 

The baseline A2C case has 2 hyperparameters to tune: the learning rate for the optimizer for the policy and value network.
A grid search was done over the set: $\{0.03, 0.003, 0.0003\}$.
\RELAX{} has 4 hyperparameters to tune: 3 learning rates for the optimizer per network, and the number of training iterations of the control variate per policy gradient update.
Due to the large number of hyperparameters, we restricted the size of the grid search set to $\{0.003, 0.0003\}$ for the learning rates, and $\{1, 5, 25\}$ for the control variate training iteration number.
We chose the hyperparameter setting that yielded the shortest episode-to-completion time averaged over 5 random seeds.
As with the discrete case, we used the definition of completion provided by the OpenAI gym~\citep{1606.01540} for each task. 

The Inverted Pendulum experiments were run for 1,000,000 frames.
%The Inverted Double Pendulum experiments were run for 5,000,000 frames.

\citet{tucker2018mirage} pointed out a bug in our initially released code for the continuous RL experiments. This issue has been fixed in the publicly available code and the results presented in this paper were generated with the corrected code. 


\subsubsection{Implementation Considerations}
For continuous RL tasks, it is convention to employ a batch of a fixed number of timesteps (here, 2500) in which the number of episodes vary. We follow this convention for the sake of providing a fair comparison to the baseline. However, this causes a complication when calculating the variance loss for the control variate because we must compute the variance averaged over completed episodes, which is difficult to obtain when the number of episodes is not fixed. For this reason, in our implementation we compute the gradients for the control variate outside of the Tensorflow computation graph. However, for practical reasons we recommend using a batch of fixed number of episodes when using our method.
\end{document}
