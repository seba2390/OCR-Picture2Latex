\section{A Sustainability Mindset for AI}
\label{sec:optimization-opportunities}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=1.0\linewidth]{mlsys2022greenai/images/Optimization 
%    \vspace{-0.8cm}
%    \caption{Across the ML development cycle, the solution space for designing AI technologies in an environmentally-responsible manner is wide open --- there is a general under-investment in the \textit{AI algorithm and data efficiency} space (Sections~\ref{sec:data} and~\ref{sec:model}) beyond system design and optimization (Section~\ref{sec:systems}).}
%    \vspace{-0.4cm}
%    \label{fig:optimization-opportunity}
%\end{figure}

To tackle the environmental implications of AI's exponential growth (Figure~\ref{fig:data-model-system-growth}), the first key step requires ML practitioners and researchers to develop and adopt an \textit{sustainability mindset}. The solution space is wide open---while there are significant efforts looking at \textit{AI system and infrastructure efficiency} optimization, the \textit{AI data, experimentation, and training algorithm efficiency} space (Sections~\ref{sec:data}  and~\ref{sec:experimentation-training}) beyond system design and optimization (Section~\ref{sec:systems}) is less well explored. 
%Navigating the large experimentation space for AI efficiently can translate into noticeable environmental footprint reduction. 
We cannot optimize what cannot be measured --- telemetry to track the carbon footprint of AI technologies must be adopted by the community (Section~\ref{sec:metrics}). 
We synthesize a number of important directions to scale AI in a \textit{sustainable} manner and to minimize the environmental impact of AI for the next decades.

%\subsection{An Efficiency Mindset for AI}
%\label{sec:efficiiency-mindset}
The field of AI is currently primarily driven by research that seeks to maximize model accuracy --- \textit{progress} is often used synonymously with improved prediction quality. This endless pursuit of higher accuracy over the decade of AI research has significant implications in computational resource requirement and environmental footprint.
% achieved through the use of massive computational power while disregarding any associated resources or environmental footprint.
To develop AI technologies responsibly, \textit{we must achieve competitive model accuracy at a fixed or even reduced computational and environmental cost}. 
Despite the recent calls-to-action~\cite{Strubell:arxiv:2019,Lacoste:arxiv:2019,Henderson:arxiv:2020,Bender:facct:2021,Patterson:arxiv:2021}, the overall community remains under-invested in research that aims at deeply understanding and minimizing the cost of AI. We conjecture the factors that may have contributed to the current state in Appendix~\ref{sec:appendix-efficiiency-mindset}.
% There are several factors that may have contributed to the current state:
%%%%%% APPENDIX %%%%%%
%\begin{itemize}
%\vspace{-0.35cm}
%\setlength\itemsep{0em}
%    \item {\bf Lack of incentives:} Over 90\% of the ML publications only focus on model accuracy improvements at the expense of efficiency~\cite{Schwartz:arxiv:2019}. Challenges\footnote{Efficient Open-Domain Question Answering (\url{https://efficientqa.github.io/}), SustaiNLP: Simple and Efficient Natural Language Processing  (\url{https://sites.google.com/view/sustainlp2020/home}), and WMT: Machine Translation Efficiency Task (\url{http://www.statmt.org/wmt21/efficiency-task.html}).} incentivize investment into efficient approaches.
%    \item {\bf Lack of common tools:} There is no standard telemetry in place to provide accurate, reliable energy and carbon footprint measurement. The measurement methodology is complex --- factors, such as datacenter infrastructures, hardware architectures, energy sources, can perturb the final measure easily.
%    \item {\bf Lack of normalization factors:} Algorithmic progress in ML is often presented in some measure of model accuracy, e.g., BLEU, points, ELO, cross-entropy loss, but without considering resource requirement as a normalization factor, e.g., the number of CPU/GPU/TPU hours used, the overall energy consumption and/or carbon footprint required.   
%    \item {\bf Platform fragmentation:} Implementation details can have a significant impact on real-world efficiency, but best practices remain elusive and platform fragmentation prevents performance and efficiency portability across model development.
%    \vspace{-0.35cm}
%\end{itemize}
%%%%%% APPENDIX %%%%%%
To bend the exponential growth curve of AI and its environmental footprint, we must build a future where efficiency is an evaluation criterion for publishing ML research on computationally-intensive models beyond accuracy-related measures. 

%and have an option to focus on advancing the state of AI through data, algorithmic, and hardware-software design efficiency improvement.

%We need to develop AI research that yields novel results while taking into account their computational (and thus carbon) cost, encouraging a reduction in resources spent. Despite a few calls to action largely coming from academia % [_Str19_ (https://arxiv.org/pdf/1906.02243.pdf),_Sch20_ (https://arxiv.org/pdf/1910.09700.pdf),_Hen20_ (https://jmlr.org/papers/volume21/20-312/20-312.pdf), _Bender21_ (https://dl.acm.org/doi/10.1145/3442188.3445922),_Pat21_ (https://arxiv.org/pdf/2104.10350.pdf)],
%the community remains underinvested in research aimed at deeply understanding and reducing the cost of AI. There are several factors that contribute to this state of affairs including 1) Lack of uniform measure of efficiency  and algorithmic progress within various AI disciplines,  2) Lack of common tools to collect and measure energy and CO2 across heterogeneous hardware and platforms, 3) Lack of incentives to focus on measures of performance other than accuracy %[_Schwartz19_ (https://arxiv.org/pdf/1907.10597.pdf)]. 

%We can build a future where efficiency would be an evaluation criterion for publishing ML research on computationally intensive models besides accuracy and related measures, thus encouraging advances across the board (as the most sustainable energy is the energy you don’t use.)

%There is a huge opportunity for \fb to be a thought leader in advancing AI research in a cost-efficient manner. We can build a future where efficiency would be an evaluation criterion for publishing AI research on computationally intensive models besides accuracy and related measures, thus encouraging advances across the board (as the most sustainable energy is the energy you don’t use).  When measures of efficiency are widely accepted as important evaluation metrics alongside accuracy for AI, then practitioners will have the option of focusing on the efficiency of their models leading to a positive impact on the environment. 


\subsection{Data Utilization Efficiency}
\label{sec:data}

\textbf{Data Scaling and Sampling:} \textit{No data is like more data} --- data scaling is the de-facto approach to increase model quality, where the primary factor for accuracy improvement is driven by the size and quality of training data, instead of algorithmic optimization. However, data scaling has significant environmental footprint implications. 
To keep the model training time manageable, overall system resources must be scaled with the increase in the data set size, resulting in larger embodied carbon footprint and operational carbon footprint from the data storage and ingestion pipeline and model training. 
Alternatively, if training system resources are kept fixed, data scaling increases training time, resulting in a larger operational energy footprint. 

When designed well, however, data scaling, sampling and selection strategies can improve the competitive analysis for ML algorithms, reducing the environmental footprint of the process (Appendix~\ref{sec:appendix-data-efficiency}). For instance, Sachdeva et al. demonstrated that intelligent data sampling with merely 10\% of data sub-samples can effectively preserve the relative ranking performance of different recommendation algorithms~\cite{Sachdeva:arxiv:2021}. This ranking performance is achieved with an average of 5.8 times execution time speedup, leading to significant operating carbon footprint reduction.
% Paul:arxiv:2021,Killamsetty:arxiv:2021

\textbf{Data Perishability:} Understanding key characteristics of data is fundamental to efficient data utilization for AI applications. \textit{Not all data is created equal} and data collected over time loses its predictive value gradually. 
Understanding the rate at which data loses its predictive value has strong implications on the resulting carbon footprint.
For example, natural language data sets can lose half of their predictive value in the time period of less than 7 years (the half-life time of data)~\cite{valavi:hbs:2020}. The exact half-life period is a function of context. If we were able to predict the half-life time of data, we can devise effective sampling strategies to subset data at different rates based on its half-life. 
By doing so, the resource requirement for the data storage and ingestion pipeline can be significantly reduced~\cite{Zhao:arxiv:2021} --- lower training time (operational carbon footprint) as well as storage needs (embodied carbon footprint).  


\subsection{Experimentation and Training Efficiency}
\label{sec:experimentation-training}

The experimentation and training phases are closely coupled (Section~\ref{sec:model-life-cycle-analysis}).   
%, in that experimentation typically revolves around changes to the training pipeline (e.g., model architecture, data pre-processing/augmentation, optimizer and hyperparameter search), which then get rolled out during the training phase. 
There is a natural trade-off between the investment in experimentation and the subsequent training cost (Section~\ref{sec:ai-carbon-footprint}).
\textbf{\emph{Neural architecture search} (NAS) and \emph{hyperparameter optimization} (HPO)} are techniques that automate the design space exploration. Despite their capability to discover higher-performing neural networks, NAS and HPO can be extremely resource-intensive, involving training many models, especially when using simple approaches. Strubell et al. show that grid-search NAS can incur over $3000\times$ environmental footprint overhead~\cite{Strubell:arxiv:2019}.
%However, much more sample-efficient NAS and HPO methods exist~\citep{Turner2021bbox,Ren2021NASsurvey}, and utilizing those can translate directly into carbon footprint reduction.
Utilizing much more sample-efficient NAS and HPO methods~\cite{Turner2021bbox,Ren2021NASsurvey} can translate directly into carbon footprint improvement.
% it is important to not only educate ML researchers and practitioners about these improved optimization tools, but also to incorporate methods into the full ML life cycle as part of standard, easy-to-use tool kits. 
In addition to reducing the number of training experiments, one can also reduce the training time of each experiment.
% \textbf{Early stopping:} F
%For many models / hyperparameter configurations, it is evident early on during training that the resulting model will perform poorly. 
By detecting and \emph{stopping under-performing training workflows early}, unnecessary training cycles can be eliminated. 
% \textbf{Check-pointing:} 
%Furthermore, 
%in practice training jobs often fail, e.g. due to bugs in unrelated pipeline code or infrastructure reliability issues. 
%routinely \emph{check-pointing model state} avoids the large cost of re-training ``from scratch''~\cite{Maeng:arxiv:2021,Eisenman:arxiv:2021}.
    
% \textbf{Multi-Objective Optimization:} 
%There are nontrivial trade-offs between model quality and system resource requirement --- minor accuracy tolerance can lead to significant computational savings. 
%However, these trade-offs are not known a priori, and they are often hard to encode in a single scalar objective. 
%Rather than optimizing a pre-defined cost function,  
\textbf{\emph{Multi-objective optimization}} explores the Pareto frontier of efficient model quality and system resource trade-offs. If used early in the model exploration process, it enables more informed decisions about \textit{which} model to train fully and deploy given certain infrastructure capacity. 
% 
Beyond model accuracy and timing performance~\cite{Song:kdd:2020,Joglekar:kdd:2020,Tan:arxiv:2020,eriksson2021latencyNAS}, energy and carbon footprint can be directly incorporated into the cost function as optimization objectives %into the NAS design space 
to enable discovery of environmentally-friendly models. 
Furthermore, when training is decoupled from NAS, sub-networks tailoring to specialized system hardware can be selected \textit{without additional training}~\cite{cai:arxiv:2020,Stamoulis:arxiv:2019,Chen:arxiv:2021,Mellor:arxiv:2021}. Such approaches can significantly reduce the overall training time, however, at the expense of increased embodied carbon footprint.

Developing \textbf{\textit{resource-efficient model architectures}} fundamentally reduce the overall system capacity need of ML tasks.  
% Feeding the demand for continual accuracy improvements, modern DNNs encompass trillions of parameters and memory capacity requirements exceed terabyte scale. 
% Large-scale training infrastructures are being developed from the ground up~\cite{Jouppi:cacm:2020,Mudigere:scaling-training:2021} in conjunction with a variety of new parallel training approaches~\cite{Rajbhandari:zero:2021} in order to sustain the chase of model accuracy. However, 
%In many large scale training cases, accelerator utilization is low at only 30-50\%; see Figure~\ref{fig:gpu-utilization}. For example, neural recommendation models exhibit orders-of-magnitude lower compute-to-memory ratios~\cite{Gupta:hpca:2020} compared to multi-layer perceptron, convolutional, and recurrent neural networks. Meanwhile, neural recommendation models require significantly higher memory capacity and bandwidth~\cite{Acun:hpca:2021,Ke:isca:2020}. 
From the systems perspective, accelerator memory is scarce. 
However, DNNs, such as neural recommendation models, require significantly higher memory capacity and bandwidth~\cite{Acun:hpca:2021,Ke:isca:2020}. 
%and embodied carbon cost is starting to outweigh the operational footprint. 
This motivates researchers to develop memory-efficient model architectures. For example, the Tensor-Train compression technique (TT-Rec) achieves more than 100$\times$ memory capacity reduction with negligible training time and accuracy trade-off~\cite{yin:mlsys:2021}. Similarly, the design space trade-off between memory capacity requirement, training time, and model accuracy is also explored in Deep Hash Embedding (DHE)~\cite{kang:kdd:2021}. While training time increases lead to higher operational carbon footprint, in the case of TT-Rec and DHE, the memory-efficient model architectures require significantly lower memory capacity while better utilizing the computational capability of training accelerators, resulting in lower embodied carbon footprint. 
% From the sustainability perspective, innovating in the space of resource-efficient algorithms and model architectures is a promising direction to address AI's ever-increasing infrastructure demands.

Developing \textbf{\textit{efficient training algorithms}} is a long-time objective of research in optimization and numerical methods~\cite{nemirovskij1983problem}. 
%Whereas in traditional optimization it is possible, in theory, to properly set hyperparameters based on problem-specific constants (which are either known, or can be estimated online),  hyperparameter tuning is an essential part of life in optimization for ML. 
Evaluations of optimization methods should account for \textit{all} experimentation efforts required to tune optimizer hyperparameters, not just the method performance after tuning~\cite{choi2019empirical,sivaprasad2020optimizer}. 
%When comparisons are performed on a fixed hardware platform, this can also serve as a reasonable proxy for algorithm-based energy efficiency improvement.
In addition, 
%large-scale ML training experiments leverage multiple GPUs in parallel. 
significant research has gone into algorithmic approaches to efficiently scale training~\cite{goyal2017accurate,ott2018scaling} by reducing communication cost via compression~\cite{alistarh2017qsgd,vogels2019powersgd}, pipelining~\cite{huang2019gpipe}, and sharding~\cite{rajbhandari2020zero,rasley2020deepspeed}. The advances have enabled efficient scaling to larger models and larger datasets. 
%by making large-scale training more efficient. 
%There remains much more research for efficient distributed training. 
We expect efficient training methods to continue as an important domain.
While this paper has focused on supervised learning relying labeled data,   
%(e.g., machine translation and recommendation models, where training data is labeled), 
algorithmic efficiency extends to other learning paradigms including self-supervised and semi-supervised learning (Appendix~\ref{sec:ssl}).

% % \textbf{Operations-aware design:} 
% Traditionally, development of ML pipeline  components is rather siloed - feature selection, model design and training, and inference optimization are considered separate steps, often handled by different teams. In such a setting, it is challenging to design the model and associated pipelines in a way that considers both training and operational efficiency of the deployed model from the start. For instance, removing some features may not substantially affect model performance or training costs (e.g., because the data has already been collected), but may save large amounts of storage and data ingestion costs during deployment, resulting in much lower operational costs over the lifetime of the model. In order to fully understand and harness these inter-dependencies, we need to consider the various design stages more holistically going forward.

\subsection{Efficient, Environmentally-Sustainable AI Infrastructure and System Hardware}
\label{sec:systems}

To amortize the embodied carbon footprint, model developers and system architects must \textit{maximize the utilization of accelerator and system resources} when in use and \textit{prolong the lifetime of AI infrastructures}. 
Existing practices such as the move to domain-specific architectures at cloud scale~\cite{Jouppi:isca:2017,AWS-inferentia,Microsoft-graphcore} reduce AI computing’s footprint by consolidating computing resources at scale and by operating the shared infrastructures more environmentally-friendly with carbon free energy\footnote{We discuss additional important directions for building environmentally-sustainable systems in Appendix~\ref{sec:appendix-system-efficiency}, including datacenter infrastructure disaggregation; fault tolerant, resilient AI systems.}. %~\cite{greenest_cloud}.

%%%%%% APPENDIX %%%%%%
%\textcolor{blue}{Appendix} \textbf{Disaggregating Machine Learning Pipeline Stages:} As depicted in Figure~\ref{fig:ml_lifecycle}, the overall training throughput efficiency for large scale ML models depends on the throughput performance of both \textit{data ingestion and pre-processing} and \textit{model training}. Disaggregating the data ingestion and pre-processing stage of the machine learning pipeline from model training is the de-facto approach for industry-scale machine learning model training. This allows training accelerator, network and storage I/O bandwidth utilization to scale independently, thereby increasing the overall model training throughput by 56\%~\cite{Zhao:arxiv:2021}. Disaggregation with well-designed check-pointing support~\cite{Maeng:arxiv:2021,Eisenman:arxiv:2021} improves training fault tolerance as well. By doing so, failure on nodes that are responsible for data ingestion and pre-processing can be recovered efficiently without requiring re-runs of the entire training experiment. From a sustainability perspective, disaggregating the data storage and ingestion stage from model training maximizes infrastructure efficiency by \textit{using less system resources to achieve higher training throughput}, resulting in lower embodied carbon footprint. By increasing fault tolerance, the operational carbon footprint is reduced at the same time.  
%%%%%% APPENDIX %%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/GPU-utilization.pdf}
    %\vspace{-0.8cm}
    \caption{A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50\%, leaving room for utilization and efficiency improvements.}
    \label{fig:gpu-utilization}
    %\vspace{-0.6cm}
\end{figure}

\textbf{Accelerator Virtualization and Multi-Tenancy Support:} Figure~\ref{fig:gpu-utilization} illustrates the utilization of GPU accelerators in \fb's research training infrastructure. A significant portion of machine learning model experimentation utilizes GPUs at only 30-50\%, leaving significant room for improvements to efficiency and overall utilization. Virtualization and workload consolidation technologies can help maximize accelerator utilization~\cite{GPU-vm}. Google's TPUs have also recently started supporting virtualization~\cite{TPU-vm}. Multi-tenancy for AI accelerators is gaining traction as an effective way to improve resource utilization, thereby amortizing the upfront embodied carbon footprint of customized system hardware for AI at the expense of potential operational carbon footprint increase~\cite{Gschwind:jrd:2017,Ghodrati:micro:2020,Kao:arxiv:2021,Jeon:usenix:2019,Yu:arxiv:2019}.

\textbf{Environmental Sustainability as a Key AI System Design Principle:}
Today, servers are designed to optimize performance and power efficiency. 
However, system design with a focus on operational energy efficiency optimization does not always produce the most environmentally-sustainable solution~\cite{jain:mobicom:2002,Chang:hotpower:2010,Gupta:HPCA:2021}.
With the rising embodied carbon cost and the exponential demand growth of AI, system designers and architects must re-think fundamental system hardware design principles to minimize computing’s footprint end-to-end, considering the entire hardware and ML model development life cycle. In addition to the respective performance, power, and cost profiles, the environmental footprint characteristics of processors over the generations of CMOS technologies, DDRx and HBM memory technologies, SSD/NAND-flash/HDD storage technologies can be orders-of-magnitude different~\cite{Bardon:iedm:2020}. Thus, designing AI systems with the least environmental impact requires explicit consideration of environmental footprint characteristics at the design time. 


\textbf{The Implications of General-Purpose Processors, General-Purpose Accelerators, Reconfigurable Systems, and ASICs for AI:} 
There is a wide variety of system hardware choices for AI from general-purpose processors (CPUs), general-purpose accelerators (GPUs or TPUs), field-programmable gate arrays (FPGAs)~\cite{Putnam:ieee-micro-2015}, to application-specific integrated circuit (ASIC), such as Eyeriss~\cite{7551407}. 
The exact system deployment choice can be multifaceted --- 
the cadence of ML algorithm and model architecture evolution, the diversity of ML use cases and the respective system resource requirements, and the maturity of the software stack. 
While ML accelerator deployment brings a step-function improvement in \textit{operational energy efficiency}, it may not necessarily reduce the carbon footprint of AI computing overall. This is because of the upfront embodied carbon footprint associated with the different system hardware choices. 
From the environmental sustainability perspective, the optimal point depends on the compounding factor of operational efficiency improvement over generations of ML algorithms/models, deployment lifetime and embodied carbon footprint of the system hardware. Thus, to design for environmental sustainability, one must strike a careful balance between \textit{efficiency} and \textit{flexibility} and, at the same time, consider environmental impact as a key design dimension for next-generation AI systems.



%%%%%% APPENDIX %%%%%%
%\textbf{Fault-Tolerant AI Systems and Hardware:}
%One way to amortize the rising embodied carbon cost of AI infrastructures is to extend hardware lifetime. However, hardware ages --- depending on the wear-out characteristics, increasingly more errors can surface over time and result in \textit{silent data corruption}, leading to erroneous computation, model accuracy degradation, non-deterministic ML execution, or fatal system failure. In a large fleet of processors, silent data corruption can occur frequently enough to have disruptive impact on service productivity~\cite{Dixit:arxiv:2021,Hochschild:hotos:2021}. Decommissioning an AI system entirely because of hardware faults is expensive from the perspective of resource and environmental footprints. System architects can design differential reliability levels for micro architectural components on an AI system depending on the ML model execution characteristics. Alternatively, algorithmic fault tolerance can be built into deep learning programming frameworks to provide a code execution path that is cognizant of hardware wear-out characteristics.
%%%%%% APPENDIX %%%%%%

%\subsubsection{Increase system/hardware life time
%One way to amortize embodied carbon emissions is to extend hardware lifetimes. Today, servers are maintained in \fb’s data centers for ~3 years before being recycled for newer hardware. This enables \fb services to enjoy the doubling performance from Moore’s Law. However, Moore’s Law is coming to an end so there is much less incentive for \fb to keep up with the 3-year server upgrade cycle. Elixir %(https://www.internalfb.com/intern/wiki/Elixir_FAQ/) is exploring server lifetime extension.
    
%Developing fault-tolerance AI systems can help amortize the upfront embodied carbon emissions from manufacturing and infrastructure capacity. Example: Silent Error - Service Level Mitigation, Cores That Don’t Count %(https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s01-hochschild.pdf).
    
%Fault and failure rate characteristics of CPUs, GPUs, DRAM, HBM, SSD/HDD disks are distinct [Large Scale Studies of Memory, Storage, and Network Failures in a Modern Data Center (https://arxiv.org/abs/1901.03401)]. When system failures occur, monolithic systems require the entire system be de-commissioned while systems designed with modularity in mind can be repaired — reducing e-waste. The Open Compute Project (OCP) specifies standard system interface, enabling customized \fb hardware infrastructures that are efficient, flexible, and scalable. Aligning with \fb’s sustainability mission, OCP kick-started the sustainability pillar to collaborate with industry partners defining sustainability standards, such as Life Cycle Analysis (LCA) metrics % (https://ocp-all.groups.io/g/Sustainability-Metrics-LCA), for computing infrastructures, aiming to build a circular economy around environmentally-sustainable computing. 

%\textbf{Programming Framework Support for Carbon-Efficient Model Architectures:}
%\textcolor{blue}{[Udit] Can we provide an illustrating example for different networks will be selected for a machine learning task when carbon cost is the optimization objective?}
%%\subsection{Optimize deep learning frameworks to scale model efficiency improvement across a wide variety of deep learning use cases}
%% Deep learning frameworks, compilers and toolchains can incorporate environmental footprint costs, such as carbon emission, as an optimization target or as part of a multi-objective optimization function. This enables model developers and architects to discover carbon-efficient model architectures, tailoring to the resource specification of ML systems.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.75\linewidth]{mlsys2022greenai/images/carbon aware scheduling.png}
%    \caption{\textcolor{blue}{Opportunity for 24/7 carbon aware scheduling.}}
%    \label{fig:cas}
%    \vspace{-0.4cm}
%\end{figure}


\textbf{Carbon-Efficient Scheduling for AI Computing At-Scale:}
As the electricity consumption of hyperscale data centers continues to rise, data center operators have devoted significant investment to neutralize operational carbon footprint.
By operating large-scale computing infrastructures with carbon free energy, technology companies
%, such as \fb~\cite{\fb_sustainability}, Google~\cite{google_sustainability}, and Microsoft~\cite{microsoft_sustainability}, 
are taking an important step to address the environmental implications of computing.
%~\cite{facebook_sustainability,google_sustainability,microsoft_sustainability}
\textit{More can be done however}.

As the renewable energy proportion in the electricity grid increases, fluctuations in 
energy generation will increase due to the intermittent nature of renewable energy sources (i.e. wind, solar). Elastic carbon-aware workload scheduling techniques
can be used in and across datacenters to predict and exploit the intermittent energy generation patterns~\cite{radovanovic2021carbon}. However such scheduling algorithms might require server over-provisioning to allow for flexibility of shifting workloads to times when carbon-free energy is available.
Furthermore, any additional server capacity comes with manufacturing carbon cost
which needs to be incorporated into the design space. 
%in carbon aware scheduling algorithm studies.
Alternatively, energy storage (e.g. batteries, pumped hydro, flywheels, molten salt) can be used to store renewable
energy during peak generation times for use during low generation times. 
There is an interesting design space to achieve 24/7 carbon-free AI computing.
%However, energy storage infrastructure is limited and expensive, and trade-offs
%of using storage versus shifting workloads using extra server capacities needs to
%be further studied in order to utilize renewable energy in the most effective way.


%\subsubsection{Enable carbon aware scheduling for AI workloads}
%Carbon aware scheduling considers the availability of renewable energy capacities when determining which data center computation should take place where and when. While Fblearner supports geography-aware, elastic scheduling for ML training by leveraging idle compute resources during off-peak hours, Fblearner does not consider carbon-intensity nor renewable energy availabilities currently. %https://fb.quip.com/BCCAEAnVbL8
    
%To mitigate the intermittent nature of renewable energy availability, scalable and cost-effective energy storage solutions are essential but can come with tradeoff, such as energy conversion loss, infrastructure availability, additional upfront infrastructure cost. Energy storage fuels, such as hydrogen, require complex electrochemical reactions driven by low-cost catalysts. To accelerate the discovery of effective catalysts, the OpenCatalyst %(https://fb.workplace.com/groups/2447831298797573/permalink/2757154297865270/) 
%project enables the community to find new ways to store renewable energy %(https://ai.\fb.com/blog/\fb-and-carnegie-mellon-launch-the-open-catalyst-project-to-find-new-ways-to-store-renewable-energy).
%Furthermore, to enable datacenter-scale computing be sourced with renewable energy in real-time, it requires fundamental changes to how we design systems and software to adopt to the intermittent nature of renewable energy availability — both temporally and spatially. 
    
%Carbon-free, renewable energy introduces interesting system and architectural optimization opportunities. For example, computation sprinting %(http://acg.cis.upenn.edu/sprinting/) 
%with turbo-boosting processor performance envelope or by activating dark silicon 
% (https://ieeexplore.ieee.org/document/6307773) 
%can be realized with more advanced, potentially more power-hungry data center and/or processor spot-cooling technologies. The additional cooling power requirement is carbon free and furthers the amortization of the embodied carbon emission of silicon.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/FL-footprint.pdf}
    %\vspace{-0.5cm}
    \caption{Federated learning and optimization can result in a non-negligible amount of carbon emissions, equivalent to the carbon footprint of training $Transformer_{Big}$~\cite{Patterson:arxiv:2021}. % With the increasing demand for on-device learning over billions of client devices and limited access to renewable energy at the edge, the carbon footprint of on-device learning can add up to a dire amount quickly.
    FL-1 and FL-2 represent two production FL applications.
    P100-Base represents the carbon footprint of $Transformer_{Big}$ training on P100 GPU
    whereas TPU-base is $Transformer_{Big}$ training on TPU. P100-Green and TPU-Green consider renewable energy at the cloud (Methodology detail in Appendix~\ref{sec:appendix-system-efficiency}).}
    %We compare the carbon footprint of FL-1 and FL-2 with the carbon footprint of Transformer Big on P100 GPU (P100-Base) and TPU (TPU-base). Although the models used in FL-1 and FL-2 are small (around 10MB), the carbon footprint of federated learning is comparable with training Transformer Big. We also show the carbon footprint considering renewable energy availability at the cloud. (P100-Green, TPU-Green).}
    \label{fig:fl_carbon}
    %\vspace{-0.25cm}
\end{figure}

\textbf{On-Device Learning}
On-device AI is becoming more ubiquitously adopted to enable model personalization~\cite{tinytl, fl_personalization,Bonawitz:arxiv:2019} while improving data privacy~\cite{gboard_prediction, gboard_ctr, gboard_emoji,huba2021papaya}, yet its impact in terms of carbon emission is often overlooked.
%
On-device learning emits non-negligible carbon. Figure~\ref{fig:fl_carbon} illustrates that the operational carbon footprint for training a small ML task using \emph{federated learning} (FL) is comparable to that of training an orders-of-magnitude larger Transformer-based model in a centralized setting.
%
As FL trains local models on client devices and periodically aggregates the model parameters for a global model, without collecting raw user data~\cite{gboard_prediction},
%
the FL process can emit non-negligible carbon at the edge due to both computation and wireless communication. 
%

%%%%%% APPENDIX %%%%%%
%To estimate the carbon emission, we used a similar methodology to~\cite{flcarbon}. We collected the 90-day log data for federated learning production use cases at \fb, which recorded the time spent on computation, data downloading, and data uploading per client device. We multiplied the computation time with the estimated device power and upload/download time with the estimated router power, and omitted other energy, as in~\cite{flcarbon}. We assumed a device power of 3W and a router power of 7.5W~\cite{phone_ml_energy, flcarbon}.
%Model training on client edge devices is inherently less energy-efficient because of the high wireless communication overheads, sub-optimal training data distribution in individual client devices~\cite{flcarbon}, large degree of system heterogeneity among client edge devices, and highly-fragmented edge device architectures that make system-level optimization significantly more challenging~\cite{wu:hpca:2019}. Note, the wireless communication energy cost takes up a significant portion of the overall energy footprint of federated learning, making energy footprint optimization on communication important.
%%%%%% APPENDIX %%%%%%

It is important to reduce AI's environmental footprint at the edge. With the ever-increasing demand for on-device use cases over billions of client devices, such as teaching AI to understand the physical environment from the first-person perception~\cite{grauman:2021:ego4d} or personalizing AI tasks, the carbon footprint for on-device AI can add up to a dire amount quickly. Also, renewable energy is far more limited for client devices compared to datacenters.
%
Optimizing the overall energy efficiency of FL and on-device AI is an important first step~\cite{kim:micro:2021,kang:asplos:2017,kim:micro:2020,yang:arxiv:2017,Stamoulis:iccad:2018}. Reducing embodied carbon cost for edge devices is also important, as 
%the dominating environmental footprint of client devices is unique, where 
manufacturing carbon cost accounts for 74\% of the total footprint~\cite{Gupta:HPCA:2021} of client devices.
%This is primarily because c
It is particularly challenging to amortize the embodied carbon footprint because client devices are often under-utilized~\cite{gao:ispass:2015}. 
%This distinct usage characteristics expose novel design dimensions for on-device computing. 
%
%
%When training is offloaded to the edge, it is hard to quantify and/or control their carbon emission, as each user's device hardware and their energy source is something that cannot be measured or controlled easily.


%\textcolor{blue}{Appendix} \textbf{Optimizing Efficiency vs. Optimizing Environmental Sustainability:}
%\textcolor{blue}{An example showing how optimizing efficiency may not always lead to environmentally-sustainable AI solutions.}


\section{Call-to-Action}

\subsection{Development of Easy-to-Adopt Telemetry for Assessing AI's Environmental Footprint}
\label{sec:metrics}

While the open source community has started building tools to enable automatic measurement of AI training's environmental footprint~\cite{Lacoste:arxiv:2019,Henderson:arxiv:2020,codecarbon,Lottick:2019} and the ML research community requiring a broader impact statement for the submitted research manuscript, more can be done in order to incorporate efficiency and sustainability into the design process.
Enabling carbon accounting methodologies and telemetry that is easy to adopt is an important step to quantify the significance of our progress in developing AI technologies in an environmentally-responsible manner. While assessing the novelty and quality of ML solutions, it is crucial to consider sustainability metrics including \textit{energy consumption} and \textit{carbon footprint} along with measures of \textit{model quality} and \textit{system performance}. %We must consider these metrics for the entire ML life cycle . 
% starting from \textif{Data} from which important features are extracted, \textit{Experimentation} and \textit{Training} where model explorations --- machine learning algorithms, model architectures, modeling techniques, training algorithms to determine parameters of models --- take place. Trained models are further optimized for \textit{Inference} deployment.

% \subsubsection{Accounting methods and component-level carbon footprint cost}
\textbf{Metrics for AI Model and System Life Cycles:} Standard carbon footprint accounting methods for AI's overall carbon footprint are at a nascent stage. We need simple, easy-to-adopt metrics to make fair and useful comparisons between AI innovations. Many different aspects must be accounted for, including the life cycles of both AI models (\textit{Data}, \textit{Experimentation}, \textit{Training}, \textit{Deployment}) and system hardware (\textit{Manufacturing} and \textit{Use}) (Section~\ref{sec:model-life-cycle-analysis}). 

In addition to incorporating an efficiency measure as part of leader boards for various ML tasks, data~\cite{kiela2021dynabench}, models\footnote{Papers with code: \url{https://paperswithcode.com/sota/image-classification-on-imagenet}}, training algorithms~\cite{hernandez2020efficiency}, environmental impact must also be considered and adopted by AI system hardware developers.
For example, MLPerf~\cite{Mattson:ieee-micro:2020,Reddi:ieee-micro:2021,mlperf:mobile} is the industry standard for ML system performance comparison. 
%Since the MLPerf Training~\cite{mlperf-training} and Inference~\cite{mlperf-inference} benchmark suites in 2018 and 2019, respectively, 
The industry has witnessed significantly higher system performance speedup, outstripping what is enabled by Moore's Law~\cite{mlperf-training,mlperf-inference}. Moreover, 
%the ML Commons Algorithmic Efficiency Working Group is currently 
an algorithm efficiency benchmark is under development\footnote{\url{https://github.com/mlcommons/algorithmic-efficiency/}}. 
%to measure progress in efficiency due to algorithmic advances (e.g., sampling, better optimizers)
 The MLPerf benchmark standards can advance the field of AI in an environmentally-competitive manner by enabling the measurement of energy and/or carbon footprint.
%Furthermore, depending on the carbon intensities of energy sources that fuel the AI model life cycle, the exact carbon emissions can vary. 

%The following metrics are crucial to assess the novelty and quality of machine learning solutions: 
%\begin{itemize}
%\vspace{-0.35cm}
%\setlength\itemsep{0em}
%    \item \textbf{Model quality} determines if a DNN is accurate enough to perform a given machine learning task;
%    \item \textbf{Latency/throughput} determines if the DNN trains fast enough to achieve the service-level objective and/or %can provide inference prediction under a pre-specified latency constraint;
%    \item \textbf{Power/energy consumption} is primarily determined by the AI system hardware; and
%    \item \textbf{Carbon footprint}, including both operational and manufacturing carbon emissions, determines the %environmental cost for a machine learning solution.
%\vspace{-0.35cm}
%\end{itemize}

% renewable energy availabilities. We are starting to tackle this direction by building first-order analytical models for rapid research system exploration undertaking detailed LCAs for \fb infrastructures and customized systems 
% [First-order Analytical Model for Embodied Carbon Footprint %(https://fb.workplace.com/groups/462049494322506/permalink/1047937865733663/)]. https://fb.quip.com/BOXAEAyhakM
% FAIR Cluster dashboard provides AI training energy consumption and estimated carbon emission data %(https://fb.workplace.com/notes/louis-martin/estimating-the-fair-cluster-gpu-co2-emissions/601001143813363). 
% To enable GPU system efficiency optimization, FAIR Cluster dashboard will provide detailed per-GPU SM-level utilization, GPU power consumption and temperature logging in the coming days.% https://fb.quip.com/HDcAEA4lK2B https://fb.quip.com/CVXAEAZreF4
% TCO versus carbon footprint: Many academic research studies often use Thermal Design Power (TDP) as a proxy for data center total cost of ownerships (TCO). As sustainability includes both operational and infrastructure capacity related components, understanding the differences between sustainability and data center TCO will better illuminate how the two can are aligned or differ.  
    
% \textbf{Machine Learning System Performance Benchmarks:}
%% \textcolor{blue}{MLPerf -- DawnBench -- Fathom -- TDB}    


\textbf{Carbon Impact Statements and Model Cards:} 
%An important step in raising awareness of AI's carbon footprint is to require carbon footprint disclosures. 
We believe it is important for all published research papers to disclose the operational \textit{and} embodied carbon footprint of proposed design; we are only at the beginning of this journey\footnote{\url{https://2021.naacl.org/ethics/faq/\#-if-my-paper-reports-on-experiments-that-involve-lots-of-compute-timepower}}. Note, while embodied carbon footprints for AI hardware may not be readily available, describing hardware platforms, the number of machines, total runtime used to produce results presented in a research manuscript is an important first step.
%Examples: Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning %(https://arxiv.org/abs/2002.05651).
%\textbf{Model cards:} %(https://arxiv.org/abs/1810.03993): 
In addition, new models must be associated with a model card that, among other aspects of data sets and models~\cite{Mitchell:fat:2019}, describes the model’s overall carbon footprint to train and conduct inference. 

%\subsubsection{Policy and guideline}
%    As hardware manufacturing and infrastructure account for a large fraction of \fb’s environmental footprint it is crucial to consider the role of future resource planning and provisioning decisions for AI infrastructure. Capacity that is ineffectively used incurs high environmental costs. An external example of this includes Microsoft's internal tax to “hold business divisions financially responsible for reducing their carbon emissions” 
%(https://blogs.microsoft.com/on-the-issues/2019/04/15/were-increasing-our-carbon-fee-as-we-double-down-on-sustainability/). 

