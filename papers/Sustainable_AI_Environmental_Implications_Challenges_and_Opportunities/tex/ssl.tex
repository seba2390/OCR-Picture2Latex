
\emph{Self-supervised learning} (SSL) have received much attention in the research community in recent years. SSL methods train deep neural networks without using explicit supervision in the form of human-annotated labels for each training sample. Having humans annotate data is a time-consuming, expensive, and typically noisy process. SSL methods are typically used to train \emph{foundation models} --- models that can readily be fine-tuned using a small amount of labeled data on a down-stream task~\cite{bommasani2021opportunities}. SSL methods have been extremely successful for pre-training large language models, becoming the de-facto standard, and they have also attracted great interest in computer vision.

When comparing supervised and self-supervised methods, there is a glaring trade-off between having labels and the amount of computational overhead involved in pre-training. For example, Chen et al. report achieving 69.3\% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for 1000 epochs on the ImageNet dataset and using the linear evaluation protocol, freezing the pre-trained feature extractor, and fine-tuning a linear classifier on top for 60 epochs using the full ImageNet dataset with all labels~\cite{chen2020simple}. In contrast, the same model typically achieves at least 76.1\% top-1 accuracy after 90 epochs of fully-supervised training. Thus, in this example, using labels and supervised training is worth a roughly 10$\times$ reduction in training effort, measured in terms of number of passes over the dataset.

Recent work suggests that incorporating even a small amount of labeled data can significantly bridge this gap. Assran et al. describe an approach called \emph{Predicting view Assignments With Support samples} (PAWS) for semi-supervised pre-training inspired by SSL~\cite{assran2021semi}. With access to labels for just 10\% of the training images in ImageNet, a ResNet-50 achieves 75.5\% top-1 accuracy after just 200 epochs of PAWS pre-training. Running on 64 V100 GPUs, this takes roughly 16 hours. Similar observations have recently been made for language model pre-training as well~\cite{dery2021should}.

Self-supervised pre-training potentially has advantages in that a single foundation model can be trained (expensive) but then fine-tuned (inexpensive), amortizing the up front cost across many tasks~\cite{bommasani2021opportunities}. Substantial additional research is needed to better understand the cost-benefit trade-offs for this paradigm.