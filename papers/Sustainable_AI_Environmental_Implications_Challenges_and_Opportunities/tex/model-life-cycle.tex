\section{Model Development Phases and AI System Hardware Life Cycle}
\label{sec:model-life-cycle-analysis}


Figure~\ref{fig:ml_lifecycle} depicts the major development phases for ML --- \textbf{Data Processing}, \textbf{Experimentation}, \textbf{Training}, and \textbf{Inference} (Section~\ref{sec:ml-model-lifecycle}) --- over the life cycle of AI system hardware (Section~\ref{sec:ml-hardware-lifecycle}).
Driven by distinct objectives of AI research and advanced product development, infrastructure is designed and built specifically to maximize data storage and ingestion efficiency for the phase of \textbf{Data Processing}, developer efficiency for the phase of \textbf{Experimentation}, training throughput efficiency for the phase of \textbf{Training}, and tail-latency bounded throughput efficiency for \textbf{Inference}.


\subsection{Machine Learning Model Development Cycle}
\label{sec:ml-model-lifecycle}


% Model development cycle
ML researchers extract features from data during the \textbf {Data Processing} phase and apply weights to individual features based on feature importance to the model optimization objective.
During \textbf{Experimentation}, the researchers design, implement and evaluate the quality of proposed algorithms, model architectures, modeling techniques, and/or training methods for determining model parameters.
This model exploration process is computationally-intensive. A large collection of diverse ML ideas are explored simultaneously at-scale. 
Thus, during this phase, we observe unique system resource requirements from the large pool of training experiments. 
Within \fb's ML research cluster, 50\% (p50) of ML training experiments take up to 1.5 GPU days while 99\% (p99) of the experiments complete within 24 GPU days. There are a number of large-scale, trillion parameter models which require over 500 GPUs days.
% Mean: 1.5 GPU days, P95: 4.3 GPU days, P99: 24 GPU days. 

Once a ML solution is determined as promising, it moves into \textbf{Training} where the ML solution is evaluated using extensive production data --- data that is \textit{more recent}, is \textit{larger in quantity}, and contains \textit{richer features}.
The process often requires additional hyper-parameter tuning. 
Depending on the ML task requirement, the models can be trained/re-trained at different frequencies. For example, models supporting \fb's \textit{Search} service were trained at an hourly cadence whereas the \textit{Language Translation} models were trained weekly~\cite{Hazelwood:hpca:2018}.
A p50 production model training workflow takes 2.96 GPU days while a training workflow at p99 can take up to 125 GPU days.
% XLMG trained on Azure is 94K hours 
% longest training at prod is 42K hours

Finally, for \textbf{Inference}, the best-performing model is deployed, producing trillions of daily predictions to serve billions of users worldwide.
% https://ai.\fb.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-\fb/
%\textbf{Inference}:
%Trained models are further optimized for deployment, where 
The total compute cycles for inference predictions are expected to exceed the corresponding training cycles for the deployed model. 
% to reach the state-of-the-art quality. At \fb, we observe a rough power capacity breakdown of 10:20:70 for AI infrastructures devoted to \textbf{Experimentation}, \textbf{Training}, and \textbf{Inference} [Figure~\ref{fig:ml_lifecycle}(a)].
% To be made available: \footnote{Additional carbon footprint from AI can be incurred from model training in public clouds, such as Google Cloud Platform, AWS, Microsoft Azure.}.
% At \fb, production model inference consumes the largest amount of the total AI power capacity at approximately \textcolor{blue}{70\%} with the remaining \textcolor{blue}{30\%} contributed by \textbf{Experimentation} and \textbf{Training}.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\linewidth]{mlsys2022greenai/images/Power Resource 
%    \caption{\textcolor{blue}{Resource requirement for AI model development cycle}}
%    \label{figure:resource-breakdown}
%    \vspace{-0.4cm}
%\end{figure}

%\textbf{ML Model Life Cycle:} 
%From the perspective of an \textit{individual} machine learning task, we start with \textit{\underline{data}} from which important \textit{\underline{features}} are extracted. Weights are applied to individual features depending on feature importance to the model optimization objective. At the \textit{\underline{training}} stage is when model exploration takes place --- machine learning algorithms, model architectures, modeling techniques, training algorithms are applied to determine model parameters. Further \textit{\underline{evaluation}} and optimization is performed on the trained model for \textit{\underline{deployment}} to produce predictions for the task. Data used for model inference is logged, forming a virtuous cycle of continual improvement for the machine learning pipeline. 


\subsection{Machine Learning System Life Cycle}
\label{sec:ml-hardware-lifecycle}


%\textbf{System Life Cycle:}
Life Cycle Analysis (LCA) is a common methodology to assess the carbon emissions over the product life cycle. There are four major phases: \textit{manufacturing}, \textit{transport}, \textit{product use}, and \textit{recycling}\footnote{Recycling is an important domain, for which the industry is developing a circular economy model to up-cycle system components --- design with recycling in mind.}. From the perspective of AI's carbon footprint analysis, \textit{\underline{manufacturing}} and \textit{\underline{product use}} are the focus. Thus, in this work, we consider the overall carbon footprint of AI by including \textit{manufacturing} --- carbon emissions from building %domain-specific 
infrastructures specifically for AI (i.e., \textit{embodied carbon footprint}) and \textit{product use} --- carbon emissions from the use of AI (i.e., \textit{operational carbon footprint}).

%There are two major sources contributing to the overall carbon footprint of AI computing: \textit{embodied} energy consumption required from the manufacturing of AI system hardware, such as CPUs, GPUs, TPUs, or Application Specific Integrated Circuit (ASIC) specialized hardware, and \textit{operational} energy consumption from the aforementioned model development phases. 
While quantifying the exact breakdown between operational and embodied carbon footprint is a complex process, we estimate the significance of embodied carbon emissions using \fb’s Greenhouse Gas (GHG) emission statistics\footnote{Facebook  Sustainability Data: \url{https://sustainability.fb.com/report/2020-sustainability-report/}.}. 
\textit{In this case, more than 50\% of \fb’s emissions owe to its value chain --- Scope 3 of \fb's GHG emission}. As a result, a significant embodied carbon cost is paid upfront for every system component brought into \fb's fleet of datacenters, where AI is the biggest growth driver.
