\section{AI Computing's Carbon Footprint}
\label{sec:ai-carbon-footprint}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cfcharacterizationv8.pdf}
    %\vspace{-0.5cm}
    \caption{The carbon footprint of the LM model is dominated by Inference whereas, for RM1 -- RM5, the carbon footprint of Training versus Inference is roughly equal. The average carbon footprint for ML training tasks at Facebook is 1.8 times larger than that of Meena used in modern conversational agents and 0.3 times of GPT-3's carbon footprint. Carbon footprint for inference tasks is included for models that are used in production. Note: the operational carbon footprint of AI does not correlate with the number of model parameters. The OSS large-scale ML tasks are based on the vanilla model architectures from~\cite{Patterson:arxiv:2021} and may not be reflective of production use cases.}
    \label{figure:cf-characterization}
    %\vspace{-0.25cm}
\end{figure}



\subsection{Carbon Footprint Analysis for Industry-Scale ML Training and Deployment}


% In this section, we perform a carbon footprint analysis for industry-scale machine learning model training and deployment use cases at \fb.


Figure~\ref{figure:cf-characterization} illustrates the operational carbon emissions for model training and inference across the ML tasks. 
We analyze six representative machine learning models in production at \fb\footnote{In total, the six models account for a vast majority of compute resources for the overall inference predictions at \fb, serving billions of users world wide.}.
\textbf{LM} refers to \fb's Transformer-based Universal Language Model for text translation~\cite{XLM-r}.
\textbf{RM1} -- \textbf{RM5} represent five unique deep learning recommendation and ranking models for various Facebook  products~\cite{Naumov:arxiv:2019,Gupta:hpca:2020}. 

 We compare the carbon footprint of \fb's production ML models with seven large-scale, open-source (OSS) models: BERT-NAS, T5, Meena, GShard-600B, Switch Transformer, and GPT-3. 
 Note, we present the operational carbon footprint of the OSS model training from~\cite{Strubell:arxiv:2019,Patterson:arxiv:2021}. The operational carbon footprint results can vary based on the exact AI systems used and the carbon intensity of the energy mixture. Models with more parameters do not necessarily result in longer training time nor higher carbon emissions. Training the Switch Transformer model equipped with 1.5 trillion parameters~\cite{Fedus:switch-transformer:2021} produces significantly less carbon emission than that of GPT-3 (750 billion parameters)~\cite{brown:arxiv:2020}. This illustrates the carbon footprint advantage of operationally-efficient model architectures.
 %We compare the carbon footprint of important \fb ML tasks with other open-source (OSS) large scale ML models~\cite{Strubell:arxiv:2019,Patterson:arxiv:2021}: BERT-NAS, T5, Meena, GShard-600B, Switch Transformer, and GPT-3. We followed a methodology similar to the ones outlined in the above papers:  We use measured average power for host types to compute overall power consumed, we compute carbon intensity in a location-based manner based on the energy mix, and assume a PUE of 1.1  for \fb data centers\footnote{A significant portion of BERT-NAS carbon footprint is from NAS that may not be considered in other model training. Based on \fb's renewable energy and sustainability programs~\cite{\fb-sustainability-report}, the operational carbon footprint of \fb AI computing has been completely neutralized. The operational carbon footprint for the \textit{OSS Large-Scale ML Models} is from~\cite{Strubell:arxiv:2019,Patterson:arxiv:2021} and can vary depending on specifics of the AI systems.}.
 
 \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/Embodied-Operational-CF-v1.pdf}
    %\vspace{-0.5cm}
    \caption{When considering the overall life cycle of ML models and systems in this analysis, manufacturing carbon cost is roughly 50\% of the (location-based) operational carbon footprint of large-scale ML tasks (Figure~\ref{figure:cf-characterization}). Taking into account carbon-free energy, such as solar, the operational energy consumption can be significantly reduced, leaving the manufacturing carbon cost as the dominating source of AI's carbon footprint.}
    \label{figure:ops-vs-embodied}
    %\vspace{-0.25cm}
\end{figure}

\vspace{+0.2cm}
\noindent{\textit{Both \textbf{Training} and \textbf{Inference} can contribute significantly to the overall carbon footprint of machine learning tasks at \fb.
The exact breakdown between the two phases varies across ML use cases.}}
\vspace{+0.2cm}

The overall operational carbon footprint is categorized into \textit{offline training}, \textit{online training}, and \textit{inference}. 
Offline training encompasses both experimentation and training models with historical data.
Online training is particularly relevant to recommendation models where parameters are continuously updated based on recent data.
The inference footprint represents the emission from serving production traffic.
The online training and inference emissions are considered over the period of offline training.
 For recommendation use cases, we find the carbon footprint is split evenly between training and inference. On the other hand, the carbon footprint of LM is dominated by the inference phase, using much higher inference resources (65\%) as compared to training (35\%).
  %\textcolor{blue}{For recommendation use cases we find the carbon footprint is split evenly between training and inference. On the other hand,  for XLM-R use cases the breakdown varies across the model scales.}



 %We categorize the overall carbon footprint into \textit{offline training} which includes experimentation, \textit{online training} which updates model parameters continuously with latest data (relevant for ranking models), and \textit{inference} where the models are running inference at production-scale traffic. The online training and inference emissions are normalized by the offline training duration  for each model. 
 
 %We compare the carbon footprint of important \fb ML tasks with other open-source (OSS) large scale ML models~\cite{Strubell:arxiv:2019,Patterson:arxiv:2021}: BERT-NAS, T5, Meena, GShard-600B, Switch Transformer, and GPT-3. We followed a methodology similar to the ones outlined in the above papers:  We use measured average power for host types to compute overall power consumed, we compute carbon intensity in a location-based manner based on the energy mix, and assume a PUE of 1.1  for \fb data centers\footnote{A significant portion of BERT-NAS carbon footprint is from NAS that may not be considered in other model training. Based on \fb's renewable energy and sustainability programs~\cite{\fb-sustainability-report}, the operational carbon footprint of \fb AI computing has been completely neutralized. The operational carbon footprint for the \textit{OSS Large-Scale ML Models} is from~\cite{Strubell:arxiv:2019,Patterson:arxiv:2021} and can vary depending on specifics of the AI systems.}.
%\footnote{https://engineering.fb.com/2014/03/14/data-center-engineering/open-sourcing-pue-wue-dashboards/}

\vspace{+0.2cm}
\noindent{\textit{Both \textbf{operational} and \textbf{embodied carbon emissions} can contribute significantly to the overall footprint of ML tasks}.}
\vspace{+0.2cm}

\textbf{Operational Carbon Footprint:} 
Across the life cycle of the Facebook  models shown in Figure~\ref{figure:cf-characterization}, the average carbon footprint is 1.8$\times$ higher than that of the open-source Meena model~\cite{google-meena} and one-third of GPT-3's training footprint.
%The average carbon footprint for large scale machine learning tasks at \fb is 1.6 times larger than that of the open-source large scale ML model training for Meena used in modern conversational agents~\cite{google-meena} and 0.3 times of GPT-3's carbon footprint.
%We consider the overall life cycle of ML models and systems in this analysis.
To quantify the emissions of \fb's models we measure the total energy consumed, assume location-based carbon intensities for energy mixes,\footnote{Renewable energy and sustainability programs of \fb~\cite{facebook-sustainability-report}.} and use a data center Power Usage Effectiveness (PUE) of 1.1. 
%The operational carbon footprint is a product of the operational energy consumption and carbon intensity of the geographical location where data centers are located for model training and deployment.
% DLRM training: 344 g CO2/kWh; DLRM serving: 280 g CO2/kWh
% OSS workloads: US average mix is 0.429 kg of CO 2 e/KWh
In addition to model-level and hardware-level optimizations, \fb's renewable energy procurement~\cite{facebook-sustainability-report} programs mitigates these emissions. 
%based on \fb's renewable energy procurement programs~\cite{\fb-sustainability-report}, this operational carbon footprint has been completely neutralized. 



\textbf{Embodied Carbon Footprint:} To quantify the embodied carbon footprint of AI hardware, we use LCA (Section~\ref{sec:ml-hardware-lifecycle}). 
% with similar compute, memory, and storage capabilities.
We assume GPU-based AI training systems have similar embodied footprint
as the production footprint of Apple's 28-core CPU with dual AMD Radeon GPUs (2000kg CO$_2$)~\cite{appleMacProMax}.
For CPU-only systems, we assume half the embodied emissions.
Based on the characterization of model training and inference at \fb, we assume an average utilization of 30-60\% over the 3- to 5-year lifetime for servers.
%We approximate the embodied carbon footprint from the manufacturing footprint of system hardware based on the life cycle analysis of large-scale Apple desktop with 28-core CPU and dual AMD Radeon GPUs~\cite{}. 
%We assume the server operates at 30\% of the potential maximum utilization for 24 hours a day, 365 days a year, and for 3 years.
Figure~\ref{figure:ops-vs-embodied} presents the overall carbon footprint for the large scale ML tasks at \fb, spanning both operational and embodied carbon footprint. Based on the assumptions of location-based renewable energy availability, the split between the embodied and (location-based) operational carbon footprint is roughly 30\% / 70\% for the large scale ML tasks. Taking into account carbon-free energy, such as solar, the operational carbon footprint can be significantly reduced, leaving the manufacturing carbon cost as the dominating source of AI's carbon footprint.


%\textit{Operational carbon footprint dominates the overall carbon footprint of model training and inference for XLM-R while embodied carbon footprint plays a more significant role for recommendation and ranking tasks (DLRM1 -- DLRM5)}. 
%Transformer model training can more effectively leverage hardware parallelism in GPU accelerators. This translates into higher resource utilization. 
%On the other hand, \textcolor{blue}{DLRM-X} exhibits much lower resource utilization for training. This is when embodied carbon footprint starts dominating the overall carbon footprint. 


%FOr each model, emissions are broken down into offline training (including experimentation), online training (for ranking models that are continuously retrained), and inference costs. For comparison, we show the carbon emissions reported by other OSS machine learning tasks. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/Optimizationbreakdown.pdf}
    %\vspace{-0.5cm}
    %mlsys2022greenai/images/CO2-breakdown.pdf}
    \caption{Optimization is an iterative process — we have achieved an average of 20\% operational energy footprint reduction every 6 months across the machine learning hardware-software stack.}  
    % In total, this has led to 25.8\% operational energy footprint reduction over a two-year time period.
    %Carbon footprint improvement from state-of-the-art machine learning accelerators (M times), at-scale infrastructure optimization (N times), machine learning platform optimization (O times), and algorithmic efficiency improvement (P times).} \textcolor{blue}{add another figure to show the accumulated CO2 reduction benefit from H1-Yr1 to ....}}
    \label{fig:hw-sw-optimization}
    %\vspace{-0.25cm}
\end{figure}


\subsection{Carbon Footprint Optimization from Hardware-Software Co-Design}
\label{sec:hw-sw-optimization}


%In this section, we review optimization techniques that enable the lower carbon footprint at the granularity of individual machine learning models. 

%\subsubsection{Optimization across AI Model Development and System Stack over Time}
%\label{sec:continuous-optimization}

% Figure 7: Overview showing we need optimization across ML, platform, HW, Infrastructure. The amount of benefit from each will be application specific. 

\vspace{+0.2cm}
\noindent{\textit{Optimization is an iterative process --- we reduce the power footprint across the machine learning hardware-software stack by ~20\% every 6 months. But at the same time, AI infrastructure continued to scale out. The net effect, with Jevon's Paradox, is a 28.5\% operational power footprint reduction over two years (Figure~\ref{fig:jevon-paradox}).}}
\vspace{+0.2cm}

\if 0 
\textit{Optimization is an iterative process --- we achieve an average of 20\% operational energy footprint reduction every 6 months across the machine learning hardware-software stack. However, over the same time period, AI infrastructure capacity continued to expand. The net effect,with Jevon's Paradox, led to the 28.5\% operational energy footprint reduction over two years (Figure~\ref{fig:jevon-paradox}).}
\fi

\textbf{Optimization across AI Model Development and System Stack over Time:}
Figure~\ref{fig:hw-sw-optimization} shows the operational power footprint reduction across \fb's AI fleet over two years. 
The improvement come from four areas of optimizations: \textit{model} (e.g., designing resource-efficient models), \textit{platform} (e.g., PyTorch's support for quantization), \textit{infrastructure} (e.g., data center optimization and low-precision hardware), and \textit{hardware} (e.g., domain-specific acceleration).
Each bar illustrates the operational power reduction across \fb's AI fleet over 6-month period from each of the optimization areas.
The optimizations in aggregate provide, on average, a 20\% reduction in operational power consumption every six months. 
The compounded benefits highlight the need for cross-stack optimizations. 
%Figure~\ref{fig:hw-sw-optimization} shows the carbon footprint reduction potential when optimization techniques are pursued for ML use cases in production using model-level optimization (Model), platform level optimization (Platform), such as PyTorch quantization support, at-scale infrastructure optimization (Infrastructure), and more efficient system hardware (Hardware) over the two-year time frame at \fb. 
%Each bar shows the operational carbon footprint reduction obtained across \fb's AI fleet over a 6-month period, broken down by the contributions coming from each layer of the optimization stack. 

% Hardware optimization improvement comes from servers and ML accelerators with higher performance-per-Watt energy efficiency. Infrastructure optimizations refer to techniques that improve utilization of the large scale training and serving infrastructures and at-scale infrastructure resource management. Platform efficiency refers to efficiency improvement from deep learning frameworks, such as high-performance PyTorch operators, memory-friendly data layout optimization. Model efficiency refers to algorithmic optimizations that reduce the overall computation requirement for a machine learning task by using resource-efficient modeling technique or achieving model consolidation in favor of foundational models~\cite{}. Here we use the operational energy footprint as a proxy for carbon footprint improvement achieved at the scale of \fb over the two-year time period. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/HW-SW-TextRay.pdf}
    %\vspace{-0.5cm}
    \caption{For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than $800\times$ using \textit{platform-level caching}, \textit{GPUs}, \textit{low precision data format}, and \textit{additional algorithmic optimization}.}
    %\textcolor{red}{Udit: It may be interesting to add in the impacts of renewable energy procurement as well (going from 300g CO2 per kWh down to around 40 g CO2 per kWh with solar).}}
    \label{fig:textray-optimization}
    %\vspace{-0.25cm}
\end{figure}


%\subsubsection{Case Study: Optimizing XLM-R's Carbon Footprint}
%\label{sec:XLM-R-carbon-footprint}
\textbf{Optimizing the Carbon Footprint of \mbox{LMs}:} 
We dive into a specific machine learning task at \fb: language translation using a Transformer-based architecture (\mbox{LM}).
\mbox{LM} is designed based on the state-of-the-art cross-lingual understanding through self-supervision. 
%using 24 layers and 600 million parameters. 
Figure~\ref{fig:textray-optimization} analyzes the power footprint improvements over a collection of optimization steps for \mbox{LM}: \textit{platform-level caching}, \textit{GPU acceleration}, \textit{low precision format on accelerator}, and \textit{model optimization}. 
In aggregate the optimizations reduce the infrastructure resources required to serve \mbox{LM} at scale by over 800$\times$.
We outline the optimization benefits from each area below.
%\textcolor{blue}{Describe the optimization steps that lead to an overall of more than 800 times resource reduction in carbon footprint.}
%With traditional chip-multiprocessor server based inference, achieving the target latency-bounded queries-per-second throughput would have required a fleet of CPU servers at a power budget --- using $800\times$ more resources than what is required in an optimal setting.
\begin{itemize}
    \item \textbf{Platform-Level Caching.} Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7$\times$. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks. Using DRAM and Flash storage devices as caches, these pre-computed embeddings can be shared across applications and use cases.
    \item \textbf{GPU acceleration.} In addition to caching, deploying LM across GPU-based specialized AI hardware unlocks an additional 10.1$\times$ energy efficiency improvement.

    \item \textbf{Algorithmic optimization.} Finally, algorithmic optimizations provide an additional 12$\times$ energy efficiency reduction. Halving precision (e.g., going from 32-bit to 16-bit operations) provides a 2.4$\times$ energy efficiency improvement on GPUs. Another 5$\times$ energy efficiency gain can be achieved by using custom operators to schedule encoding steps within a single kernel of the Transformer module, such as~\cite{faster-transformer}. 

\end{itemize}


%Pre-computing embeddings by caching frequently 

%By storing computed results in the DRAM and flash memory, we achieve approximately $6.69\times$ higher power efficiency. With an additional L0 cache on each CPU host to cache the most frequently-accessed embedding portions can further boost inference efficiency. Caching at the application level is plausible because our model produces embeddings that are repeatedly consumed by different down-stream applications over the course of several days. By analyzing down-stream traffic, we are able to identify optimal caching strategies and more effectively retain computed embeddings in a storage hierarchy of DRAM and Flash. The computed embeddings can be shared across different applications over time.


%\textit{Platform-Level Caching.} By storing computed results in the DRAM and flash memory, we achieve approximately $6.69\times$ higher power efficiency. With an additional L0 cache on each CPU host to cache the most frequently-accessed embedding portions can further boost inference efficiency. Caching at the application level is plausible because our model produces embeddings that are repeatedly consumed by different down-stream applications over the course of several days. By analyzing down-stream traffic, we are able to identify optimal caching strategies and more effectively retain computed embeddings in a storage hierarchy of DRAM and Flash. The computed embeddings can be shared across different applications over time.



%\textit{GPUs and Algorithmic Optimization.} With GPUs, we can unlock an additional $10.1\times$ and $24.2\times$ times energy efficiency improvement using 32-bit and 16-bit data formats, respectively. Platform-level caching optimization on CPUs and acceleration using GPUs in combination reduces the carbon cost of serving large-scale language models by $162\times$.
%Another 5 times energy efficiency gain can be achieved by 
%optimizing the implementation of the Transformer encoder operator, 
%using a custom operator to schedule the encoding steps performed by the Transformer module in a single kernel, and optimizes padding across batch element boundaries~\cite{faster-transformer}.

% Based on the average miles driven in the US (about 12,500km), this corresponds to the yearly power consumption of about 20,500 Tesla Model 3. 
%Despite the significantly reduced environmental footprint achieved through hardware-software co-design, we see a variety of XLM-R models in production, spanning from commerce to digital assistant use cases. Taking into account the scale, AI's environmental footprint could add up quickly without these optimizations across the application, hardware selection, representation and algorithmic optimizations. 

%This is great but we need to continue optimizing carbon footprint. 
%One route is efficiency at-scale optimizations which \fb has been engaging with very heavily.
%Show utilization case study. 
%This is great! But we need to go beyond efficiency as well… (leads into Challenges, and Optimizaitons Section --- Section 4)


\textbf{Optimizing the Carbon Footprint of \mbox{RMs}:}
The LM analysis is used as an example to highlight the optimization opportunities available with judicious cross-stack, hardware/software optimization. In addition to optimizing the carbon footprint for the language translation task,
%In Appendix~\ref{sec:DLRM-carbon-footprint}, 
we describe additional optimization techniques tailored for ranking and recommendation use cases. 

%We use XLM-R as an example to highlight the potential of carbon footprint reduction that can be achieved through careful cross-stack optimization, AI accelerators, and hardware-software co-design and optimization. In Appendix~\ref{sec:DLRM-carbon-footprint}, we describe additional optimization techniques tailored for the DLRM tasks. 
%When applied to all ML models at scale, we can make step-function carbon footprint reduction.
%, reducing both operational and embodied carbon cost of AI. %In fact, the scale of \fb data center fleets and workloads offers additional at-scale optimization opportunities. 


A major infrastructure challenge faced by deep learning RM training and deployment (\textbf{RM1} -- \textbf{RM5}) is the fast-rising memory capacity and bandwidth demands (Figure~\ref{fig:data-model-system-growth}). There are two primary sub-nets in a RM: the dense fully-connected (FC) network and the sparse embedding-based network. The FC network is constructed with multi-layer perceptions (MLPs), thus computationally-intensive. The embedding network is used to project hundreds of sparse, high-dimensional features to low-dimension vectors. It can easily contribute to over 95\% of the total model size. For a number of important recommendation and ranking use cases, the embedding operation dominates the inference execution time~\cite{Gupta:hpca:2020,Ke:isca:2020}.

To tackle the significant memory capacity and bandwidth requirement, we deploy model quantization for RMs~\cite{deng:ieee-micro-2021}. 
Quantization offers two primary efficiency benefits: the low-precision data representation reduces the amount of computation requirement and, at the same time, lowers the overall memory capacity need. 
By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15\%. This has led to 20.7\% reduction in memory bandwidth consumption.
Furthermore, the memory capacity reduction enabled by quantization unblocks novel systems with lower on-chip memory. For example, for RM1, quantization has enabled RM deployment on highly power-efficient systems with smaller on-chip memory, leading to an end-to-end inference latency improvement of 2.5 times. 


\subsection{Machine Learning Infrastructures at Scale}

\textbf{ML Accelerators:}
GPUs are the de-facto training accelerators at \fb, contributing to significant power capacity investment in the context of \fb's fleet of datacenters. However, GPUs can be severely under-utilized during both the ML Experimentation and Training phases (Figure~\ref{fig:gpu-utilization})~\cite{Wesolowski:ieee-micro:2021}. To amortize the upfront embodied carbon cost of every accelerator deployed into \fb’s datacenters, maximizing accelerator utilization is a must. 
% From the perspective of environmental sustainability, ML accelerators such as GPUs offer significantly higher training throughput performance per unit carbon footprint cost. Taking ResNet-50 as an example, GPUs, such as NVIDIA v100s, offer \textcolor{blue}{M} times training throughput per unit carbon footprint as compared with conventional CPUs, such as \textcolor{blue}{Y}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/CarbonFootprintGrowthofAI.pdf}
    %\vspace{-0.5cm}
    \caption{The iterative optimization process has led to 28.5\% operational energy footprint reduction over the two-year time period (Section~\ref{sec:hw-sw-optimization}). Despite the significant operational power footprint reduction, we continue to see the overall electricity demand for AI to increase over time --- an example of \textit{Jevon's Paradox}, where efficiency improvement stimulates additional novel AI use cases.}
    \label{fig:jevon-paradox}
    %\vspace{-0.25cm}
\end{figure}

\textbf{Efficiency of Scale:} The higher throughput performance density achieved with ML accelerators reduces the total number of processors deployed into datacenter racks. This leads to more effective amortization of shared infrastructure overheads. Furthermore, datacenter capacity is not only limited by physical space but also power capacity --- higher operational power efficiency directly reduces the inherited carbon cost from manufacturing of IT infrastructures and datacenter buildings. 

%In Appendix~\ref{sec:at-scale-optimization-fb-appendix}, we further discuss optimization opportunities for \fb's fleet of datacenters. 

\textbf{At-Scale Efficiency Optimization for Facebook Data Centers:}
Servers in Facebook data center fleets are customized for internal workloads only --- machine learning tasks~\cite{Hazelwood:hpca:2018} or not~\cite{Sriraman:isca:2019,Sriraman:asplos:2020}. Compared to public cloud providers, this puts Facebook at a unique position for at-scale resource management design and optimization. First, \fb customizes server SKUs --- compute, memcached, storage tiers and ML accelerators --- to maximize performance and power efficiency. Achieving a Power Usage Effectiveness (PUE) of about 1.10, \fb's data centers are about 40\% more efficient than small-scale, typical data centers. 
% (\url{https://sustainability.fb.com/report-page/data-centers/}).

Furthermore, the large-scale deployment of servers of different types provides an opportunity to build performance measurement and optimization tools to ensure high utilization of the underlying infrastructure. For data center fleets in different geographical regions where the actual server utilization exhibits a diurnal pattern, Auto-Scaling frees the over-provisioned capacity during off-peak hours, by up to 25\% of the web tier’s machines~\cite{Tang:osdi:2020}. By doing so, it provides opportunistic server capacity for others to use, including offline ML training. Furthermore, static power consumption plays a non-trivial role in the context of the overall data center electricity footprint. This motivates more effective processor idle state management.


\textbf{Carbon-Free Energy:} Finally, over the past years, \fb has invested in carbon free energy sources to neutralize its operational carbon footprint~\cite{facebook-sustainability-report}. 
Reaching net zero emissions entails matching every unit of energy consumed by data centers with 100\% renewable energy purchased by \fb. Remaining emissions are offset with various sustainability programs, further reducing the operational carbon footprint of AI computing at \fb. As Section~\ref{sec:systems} will later show, \textit{more can be done}.  



\subsection{Going Beyond Efficiency Optimization}

Despite the opportunities for optimizing energy efficiency and reducing environmental footprint at scale, there are many reasons why we must care about scaling AI in a more environmentally-sustainable manner. AI growth is multiplicative beyond current industrial use cases. Although domain-specific architectures improve the operational energy footprint of AI model training by more than 90\%~\cite{Patterson:arxiv:2021}, these architectures require more system resources, leading to larger embodied carbon footprints.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/ChasingCarbon-projection.pdf}
    \vspace{-0.5cm}
    \caption{As accelerator utilization improves over time, both operational and embodied carbon footprints of AI improve. Carbon-free energy helps reduce the operational carbon footprint, making embodied carbon cost the dominating factor. 
    To reduce the rising carbon footprint of AI computing at-scale, we must complement efficiency and utilization optimization with novel approaches to reduce the remaining embodied carbon footprint of AI systems.}
    \label{figure:server-utilization}
    \vspace{-0.25cm}
\end{figure}
While shifting model training and inference to data centers with carbon-free energy sources can reduce emissions, the solution may not scale to all AI use cases.
Infrastructure for carbon free energy is limited by rare metals and materials, and takes significant economic resources and time to build. 
Furthermore, the carbon footprint of federated learning and optimization use cases at the edge is estimated to be similar to that of training a Transformer Big model (Figure~\ref{fig:fl_carbon}). As on-device learning becomes more ubiquitously adopted to improve data privacy, we expect to see more computation being shifted away from data centers to the edge, where access to renewable energy may be limited. The edge-cloud space for AI poses interesting design opportunities (Section~\ref{sec:systems}). 

%In addition to carbon reduction potential of further efficiency improvements at-scale, we must also look beyond efficiency optimization to tackle AI's growing carbon footprint.
%In particular, we find solely optimizing efficiency provides limited carbon reduction potential compared to the exponentially rising AI data set size, model size, and infrastructure capacity. 

\textit{The growth of AI in all dimensions outpaces the efficiency improvement at-scale.}
Figure~\ref{figure:server-utilization} illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80\%, the overall carbon footprint decreases by 3$\times$. 
Powering AI services with renewable energy sources 
%as compared with using the average energy mix in the United States 
can further reduce the overall carbon footprint by a factor of 2. Embodied carbon cost becomes the dominating source of AI's overall carbon footprint. To curb the rising carbon footprint of AI computing at-scale (Figure~\ref{fig:jevon-paradox} and Figure~\ref{figure:server-utilization}), \textit{we must look beyond efficiency optimization and complement efficiency and utilization optimization with efforts to tackle the remaining embodied carbon footprint of AI systems.}

% The analysis focuses on GPU-based and CPU-based AI training on top and bottom, respectively. 
% We assume the GPU-based server has comparable embodied emissions as Apple's desktop with 28-core CPU and dual AMD Radeon GPU's~\cite{}; embodied carbon footprint for the CPU-based configuration is scaled based on the hardware compute, memory, and storage capacity.
% Based on characterizations of \fb's GPU-based and CPU-based training workloads we find the baseline utilization to be 33\% and 20\%, respectively.


