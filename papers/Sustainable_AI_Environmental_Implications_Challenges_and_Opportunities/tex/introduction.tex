\section{Introduction}
\label{sec:introduction}

Artificial Intelligence (AI) is one of the fastest growing domains spanning research and product development and significant investment in AI is taking place across nearly every industry, policy, and academic research. This investment in AI has also stimulated novel applications in domains such as science, medicine, finance, and education. Figure~\ref{fig:ai-growth} analyzes the number of papers published within the scientific disciplines, illustrating the growth trend in recent years\footnote{Based on monthly counts, Figure~\ref{fig:ai-growth} estimates the cumulative number of papers published per category on the arXiv database.}.
% is based on monthly counts of the published papers per category and charting the cumulative count 

AI plays an instrumental role to push the boundaries of knowledge and sparks novel, more efficient approaches to conventional tasks.
AI is applied to predict protein structures radically better than previous methods. It has the potential to revolutionize biological sciences by providing in-silico methods for tasks only possible in a physical laboratory setting~\cite{AlphaFold}. 
AI is demonstrated to achieve human-level conversation tasks, such as the Blender Bot~\cite{Komeili:arxiv:2021}, and play games at superhuman levels, such as AlphaZero \cite{AlphaZero}.
AI is used to discover new electrocatalysts for efficient and scalable ways to store and utilize renewable energy~\cite{open-catalyst}, predicting renewable energy availability in advance to improve energy utilization~\cite{AI-load-shaping}, 
operating hyperscale data centers efficiently~\cite{google-cloud}, 
growing plants using less natural resources~\cite{robot-farms}, and, at the same time,
being used to tackle climate changes~\cite{rolnick:arxiv:2019,Nishant:IJIM:2020}.
%AI can be a building block within or replacement for existing systems.
%For instance, AI has been applied to predict protein structures radically better than previous methods, which has the potential to revolutionize biological sciences by providing in-silico methods for tasks only possible in a physical laboratory setting~\cite{AlphaFold}. 
%In addition, AI can be a goal onto itself.
%An example is AI being developed to achieve human-level language tasks, e.g., \cite{}, or play games at superhuman levels, e.g., \cite{}.
%AI can increase the energy and environment efficiency of systems, but also be the source of new systems that consume huge amounts of energy.
%In fact AI can be specifically used to mitigate climate change such as discovering new electrocatalysts for efficient and scalable ways to store and utilize renewable energy~\cite{open-catalyst}, predicting renewable energy availability in advance to improve energy utilization~\cite{AI-load-shaping}, operating hyperscale data centers efficiently~\cite{google-cloud}, growing plants using less natural resources~\cite{robot-farms}, and generally being used to mitigate climate change \cite{}. % https://arxiv.org/abs/1906.05433
%%The number of AI applications in different disciplines has seen significant growth in recent years (Figure~\ref{fig:ai-growth}). 
% Putting this in terms of market size, 
%For the purposes of our discussion AI refers to everything from giant language models pushing the boundaries of machine intelligence and neural network approximations of conventional methods that make those methods more computationally efficient.
%We refer to all of the above collectively as AI because they share a common technological substrate that we can analyze and optimize using similar tools and approaches.
It is projected that, in the next five years, the market for AI will increase by 10$\times$ into hundreds of billions of dollars~\cite{AI-market}. 
All of these investments in research, development, and deployment have led to a super-linear growth in AI data, models, and infrastructure capacity.
With the dramatic growth of AI, it is imperative to understand the environmental implications, challenges, and opportunities of this nascent technology. This is because technologies tend to create a self-accelerating growth cycle, putting new demands on the environment. 
%For example, creation of the jet airplane used very little energy and emitted very little CO2.
%However, it enabled an aviation industry to meet a demand for mobility that produced over 1 billion tons of CO2 in 2020, which was close to 3\% of global emissions in 2020.
%It is important at this early stage to ask the question how AI can be made sustainable.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/arxiv.pdf}
    \caption{The growth of ML is exceeding that of many other scientific disciplines. Significant research growth in machine learning is observed in recent years as illustrated by the increasing cumulative number of papers published in machine learning with respect to other scientific disciplines based on the monthly count
    %The significant research and development investment in AI has stimulated novel applications of AI in many disciplines 
    (y-axis measures the cumulative number of articles on arXiv).
    }
    \vspace{-0.5cm}
    \label{fig:ai-growth}
\end{figure}

This work explores the environmental impact of AI from a \textit{holistic} perspective. 
%\textbf{This Work:} We explore the environmental impact of these trends from a holistic perspective and present challenges and opportunities to designing and building sustainable AI technologies, spanning \textit{Data}, \textit{Algorithm}, and \textit{System Hardware}. 
More specifically, we present the challenges and opportunities to designing sustainable AI computing across the key phases of the machine learning (ML) development process --- \textit{Data}, \textit{Experimentation}, \textit{Training}, and \textit{Inference} --- for a variety of AI use cases at \fb, such as vision, language, speech, recommendation and ranking. The solution space spans across our fleet of datacenters and on-device computing. 
Given particular use cases, we consider the impact of AI \textit{data}, \textit{algorithms}, and \textit{system hardware}.
Finally, we consider emissions across the life cycle of hardware systems, from manufacturing to operational use. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/DataModelSystemGrowth.pdf}
    \caption{Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years.
    (a) The $1000\times$ model size growth has led to higher model accuracy for various ML tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model $1,000\times$ larger in size. 
    (b) At \fb, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2 times increase in the data ingestion bandwidth demand.
    (c) %To learn valuable information from the data, model capacities have also increased to achieve higher model quality. 
    \fb's recommendation and ranking model sizes have increased by 20 times during the same time period~\cite{Mudigere:scaling-training:2021}.
    (d) The explosive growth in AI has driven $2.9\times$ and $2.5\times$ capacity increases for AI training and inference, respectively.
    %\textcolor{orange}{Note on the data is published and updated.}
    %(a) The \textcolor{blue}{M} times model accuracy improvement in the past decade demands \textcolor{blue}{N} times carbon footprint increase. (b) The decade of hardware-software co-design has improved AI computing's carbon footprint by \textcolor{blue}{X} (model optimization) and \textcolor{blue}{Y} (system optimization) times, respectively.
    }
    \vspace{-0.25cm}
    \label{fig:data-model-system-growth}
\end{figure*}


%The past decade has witnessed a 300,000 times increase in the amount of compute for AI~\cite{Open-AI}.
\textbf{AI Data Growth.}
In the past decade, we have seen an exponential increase in AI training data and model capacity.
Figure~\ref{fig:data-model-system-growth}(b) illustrates that the amount of training data at Facebook for two recommendation use cases --- one of the fastest growing areas of ML usage at \fb --- has increased by 2.4$\times$ and 1.9$\times$ in the last two years, reaching exabyte scale.
The increase in data size has led to a 3.2$\times$ increase in data ingestion bandwidth demand.
Given this increase, data storage and the ingestion pipeline accounts for a significant portion of the infrastructure and power capacity compared to ML training and end-to-end machine learning life cycles.
%[CJW: remove for submission for anonymity]~\cite{Zhao:arxiv:2021}. 
%end-to-end machine learning 
%Due to these trends in model size (and the associated increase in training data) the infrastructure and power capacity needed for data storage and the ingestion pipeline can easily exceed that of model training.

%With the training data sets reaching the exabyte scale,
%Figure~\ref{figure:fig-1}(a) illustrates that the amount of data for AI has grown by 2.4$\times$, leading to a 3.2$\times$ increase in data ingestion bandwidth demand at \fb.
%To learn valuable information from the data, model capacities for a variety of machine learning tasks, such as vision, language, and ranking, have also increased by orders-of-magnitude to achieve higher model quality.
%For example, at \fb, for personalization and ranking alone --- one of the fastest growing areas of ML usage at \fb, the size of training data sets grew by 1.75$\times$ and is now at the exabyte-scale~\cite{Zhao:arxiv:2021}.
% This leads to the recommendation and ranking model capacity increase by 2000 times between 2019 -- 2021.
% \textcolor{blue}{The explosive growth in AI use cases has driven 8.3 times and 7 times capacity increases for AI training and inference at \fb over the recent M years, respectively.}
% For instance, at \fb, the data volume used to train ML models has increased threefold between 2019-2020. 
% During this time, for personalization and ranking alone, one of the fastest growing areas of ML usage at \fb, the size of training data sets grew by 1.75$\times$ and is now at the exabyte-scale~\cite{Zhao:arxiv:2021}.



\textbf{AI Model Growth.}
The ever-increasing data volume has also driven a super-linear trend in model size growth. 
%Modern deep neural networks deployed in industry have now reached the trillion parameter scale~\cite{Fedus:switch-transformer:2021,Rajbhandari:zero:2021,Mudigere:scaling-training:2021,Xie:Kraken:2020}.
%For example, since 2017, neural recommendation models have grown from hundreds of gigabytes to the terabyte scale~\cite{Yi2018FactorizedDR,zhao2020distributed,lui2020understanding, Mudigere:scaling-training:2021}. 
% For example, the number of parameters in large Transformer models has increased exponentially from a few hundred million parameters in 2018 to over a trillion parameters in 2021---$>$200$\times$ increase in the last 2 years alone~\cite{Hernandez:arxiv:2020}.
Figure~\ref{fig:data-model-system-growth}(a) depicts the $1000\times$ model size increase for GPT3-based language translation tasks~\cite{Hernandez:arxiv:2020,brown:arxiv:2020}, whereas for Baidu's search engine, the model of $1000\times$ larger in size improves accuracy in AUC by 0.030. Despite small, the accuracy improvement can lead to significantly higher-quality search outcomes~\cite{g-search}. 
% AUC required increasing the web search and image search models by 1000$\times$.
Similarly, Figure~\ref{fig:data-model-system-growth}(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20$\times$~\cite{Yi2018FactorizedDR,zhao2020distributed,lui2020understanding, Mudigere:scaling-training:2021}.
%For model quality improvement of 0.030 in AUC, the overall capacity of the machine learning model supporting Baidu's search engine has increased by 1,000 times.
Despite the large increase in model sizes, the memory capacity of GPU-based AI accelerators, e.g. 32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100, 2021), has increased by $<2\times$ every 2 years.
The resource requirements for strong AI scaling clearly outpaces that of system hardware.


\textbf{AI Infrastructure Growth.}
The strong performance scaling demand for ML motivates a variety of \textit{scale-out} solutions~\cite{Mudigere:scaling-training:2021,Rajbhandari:zero:2021} by leveraging parallelism at scale with a massive collection of training accelerators.
%, training time can be further minimized. 
%The server capacity demand for AI model training has increased by more than 4 times over an 18-month period, corresponding to a 7 times increase in the number of training experiments~\cite{Naumov:arxiv:2020}. 
Figure~\ref{fig:data-model-system-growth}(d) illustrates that the explosive growth in AI use cases at Facebook has driven $2.9\times$ increase in AI training infrastructure capacity over the 1.5 years.
% 8.3 times for 3 years
In addition, we observe trillions of inference per day across \fb's data centers---more than doubling in the past 3 years. 
The increase in inference demands has also led to an $2.5\times$ increase in AI inference infrastructure capacity. 
Last but not least, the carbon footprint of AI goes beyond its \textit{operational} energy consumption. 
The \textit{embodied} carbon footprint of systems is becoming a dominating factor for AI's overall environmental impact (Section~\ref{sec:ai-carbon-footprint})~\cite{Gupta:HPCA:2021}.
%\textcolor{blue}{In fact, in this work we show the ratio between operational and embodied carbon footprint for large model training on CPUs is approximately 65:35 for a modern industry-scale recommendation model (Section~\ref{sec:ai-carbon-footprint}).}
%Finally, we see trillions of machine learning inference instances per day at \fb --- more than doubling over the past three years. 
%Taking all inference across \fb into account, 80\% of inference cycles are executed by neural recommendation models~\cite{Gupta:hpca:2020}. 
%In combination, more than {\bf W} inference predictions are made for each active user each day.

%\textcolor{blue}{\textbf{Environmental Footprint of AI:} 
%The exponential growth of AI comes with significant overhead.
%Despite the positive societal benefits~\cite{ai-social-good}, the scaling trend of AI can incur a large energy and environmental footprint. 
%%the \textcolor{blue}{M} times model accuracy improvement in the past decade. 
%The endless pursuit of model quality improvement has significant environment implications, as what this paper will show.
%%This improved model accuracy has resulted in a \textcolor{blue}{N} times operational carbon footprint increase.
%%Figure~\ref{figure:fig-1} shows that the \textcolor{blue}{M} times model accuracy improvement %in the past decade comes with \textcolor{blue}{N} times carbon footprint increase. 
%This environmental cost of AI computing already accounts for decades of optimization across the hardware /software computing stack that has improved AI computing's carbon footprint. We use XLM-R --- a Transformer-based universal language model -- to show case the 810 times operational carbon footprint reduction that can be achieved by judicious hardware-software co-design and optimization.}

\textbf{The Elephant in the Room.} 
Despite the positive societal benefits~\cite{ai-social-good}, the endless pursuit of achieving higher model quality has led to the exponential scaling of AI with significant energy and environmental footprint implications.
% has significant environment implications, as what this paper will show.
Although recent work shows the carbon footprint of training one large ML model, such as \emph{Meena}~\cite{Patterson:arxiv:2021}, is equivalent to 242,231 miles driven by an average passenger vehicle~\cite{GHG-calculator}, this is only one aspect; to fully understand the real environmental impact
%$Transformer_{big}$~\cite{Patterson:arxiv:2021}, may be small (219 miles driven by an average passenger vehicle~\cite{GHG-calculator}), 
we must consider the AI ecosystem \textit{holistically} going forward --- beyond looking at model training alone and by accounting for both \textit{operational} and \textit{embodied carbon footprint} of AI.
We must look at the ML pipeline end-to-end: data collection, model exploration and experimentation, model training, model optimization and run-time inference. 
The \textit{frequency of training} and \textit{scale} of each stage of the ML development cycle matter. 
%must also be considered to quantify salient bottlenecks for sustainable AI.
From the systems perspective, the life cycle of ML software and system hardware, including manufacturing and operational use, must also be considered. 
%At-scale efficiency optimization is another crucial aspect. 
% to enabling future sustainable AI systems.
%he life cycle of ML tasks over the end-to-end machine learning pipeline (Figure~\ref{fig:ml_lifecycle}) and the entire system stack of domain-specific architectures, from deep learning software to system hardware. 
%Frequency of training matters (Figure~\ref{figure:fig-2}). Efficiency at scale matters (Section~\ref{}). 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/MLmodellifecyclev2.pdf}
    %\vspace{-0.5cm}
    \caption{Model Development Phases over AI System Hardware Life Cycle: (a) At \fb, we observe a rough power capacity breakdown of \textbf{10:20:70} for AI infrastructures devoted to the three key phases --- \textbf{Experimentation}, \textbf{Training}, and \textbf{Inference}; (b) Considering the primary stages of the ML pipeline end-to-end, the energy footprint of RM1 is roughly \textbf{31:29:40} over \textbf{Data},  \textbf{Experimentation/Training}, and \textbf{Inference}; 
    (c) Despite the investment to neutralize the operational footprint with carbon-free energy, the overall data center electricity use continues to grow, demanding over 7.17 million MWh in 2020~\cite{facebook-sustainability-report}.}
    \label{fig:ml_lifecycle}
\end{figure*}

Optimizing across ML pipelines and systems life cycles end-to-end is a complex and challenging task. 
While training large, sparsely-activated neural networks improves model scalability, achieving higher accuracy at lower operational energy footprint~\cite{Patterson:arxiv:2021},
%by more than 90\%~\cite{Patterson:arxiv:2021}, 
it can incur higher embodied carbon footprint from the increase in the system resource requirement.
%require more system resources increasing embodied carbon cost.
Shifting model training and inference to data centers with carbon-free energy can reduce emissions; however, this approach may not scale to a broad set of use cases.
Infrastructure for carbon-free energy is limited by factors such as geography and available materials (e.g. rare metals), and takes significant economic resources and time to build.
In addition, as on-device learning becomes more ubiquitously adopted to improve data privacy, we can see more computation being shifted away from data centers to the edge, where access to renewable energy is limited. 

\textbf{A Holistic Approach.} This paper is the first to take a holistic approach to characterize the environmental footprint of AI computing from \textit{experimentation} and \textit{training} to \textit{inference}. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases at Facebook (Section~\ref{sec:model-life-cycle-analysis}). This is illustrated by the more than 800$\times$ operational carbon footprint reduction achieved through judicious hardware-software co-design for a Transformer-based universal language model.
Taking a step further, we present an end-to-end analysis for both operational \textit{and} embodied carbon footprint for AI training and inference (Section~\ref{sec:ai-carbon-footprint}).
Based on the industry experience and lessons learned, we chart out opportunities and important development directions across the dimensions of AI including --- data, algorithm, systems, metrics, standards, and best practices (Section~\ref{sec:optimization-opportunities}). 
We hope the key messages (Section~\ref{sec:takeaways}) and the insights in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.

%\begin{comment}
%The rapid growth of AI computing comes with significant costs. 
%Over the past 7 years, a 44x compute improvement on ImageNet training time has been enabled by algorithmic improvements while 11x training performance improvement come from hardware improvements [Measuring the Algorithmic Efficiency of Neural Networks %(https://arxiv.org/pdf/2005.04305.pdf)].
%\end{comment}

%To sustain the growth of AI and its environmental impact, using large but sparsely activated deep neural networks (DNNs) can reduce the model training energy consumption by more than 90\% as compared to dense DNNs despite using as many or even more parameters [Carbon Emissions and Large Neural Network Training %(https://arxiv.org/pdf/2104.10350.pdf)]. 
%Shifting computations to data centers with carbon-free energy sources can further reduce ML model training’s carbon emissions (Google %(https://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows/), 
%Microsoft %(https://devblogs.microsoft.com/sustainable-software/carbon-aware-vs-carbon-efficient-applications/)). 
%Finally,  the open source community is starting to build tools to enable automatic measurement of computing’s carbon footprint [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning %(https://research.fb.com/wp-content/uploads/2020/12/Towards-the-Systematic-Reporting-of-the-Energy-and-Carbon-Footprints-of-Machine-Learning.pdf)]
%[Reinforcement Learning Energy Leaderboard %(https://breakend.github.io/RL-Energy-Leaderboard/reinforcement_learning_energy_leaderboard/index.html)]
%[CodeCarbon (https://github.com/mlco2/codecarbon)]. 

%At \fb, we are seeing lots of model scaling (up) efforts, just to name a few [Enabling 10B Parameter Model Training for Computer Vision] %(https://fb.workplace.com/notes/1357728857941884/), 
%[Larger-Scale Transformers for Multilingual Masked Language Modeling %(https://fb.workplace.com/groups/831302610278251/permalink/5412717568803376/)], 
%[Improving ASR Performance on FB Data Using Massively Multilingual Models (https://fb.workplace.com/notes/vineel-pratap/improving-asr-performance-on-fb-data-using-massively-multilingual-models/839333623142024/)], 
%[Recommender Systems Competitive Analysis (https://fb.workplace.com/notes/dhruv-choudhary/recommender-systems-competitive-analysis/784151439174785/)]. 
%Lots of efficiency investment on infrastructure is taking place in order to sustain the model scaling (up) desire while keeping the power capacity growth demand of AI reasonable [Deploying NVIDIA GPU inference in production at scale %(https://fb.workplace.com/groups/capacity.efficiency/permalink/5898894086825824/)]
%[Scaling self-supervision models on FB, FAIR, A100 clusters %(https://fb.workplace.com/notes/1352827801765323)][Understanding Training Efficiency of Deep Learning Recommendation Models at Scale (https://research.fb.com/publications/understanding-training-efficiency-of-deep-learning-recommendation-models-at-scale/)]. 
%However, there is little model efficiency efforts. 

%\begin{itemize}
    %\item \textcolor{blue}{{\bf AI Data:} 3x increase 2019-2020 (30\% to 50\% of all data for ML)}
    %\item \textcolor{blue}{{\bf AI Training:} Server compute demand for AI model training has increased by over 4 times over the 18-month period, corresponding to over 7 times increase in the number of training experiments. (Figure~\ref{figure:fig-1}(a)).} 
    %\item \textcolor{blue}{{\bf AI Inference:} As of June 2021, we see {\bf M} trillion machine learning inferences per day at \fb --- more than two times increase over the past three years. 80\% of AI inference cycles has been devoted to neural recommendation inference. In combination, {\bf W} inference predictions are made for each active user each day for recommender system use cases (Figure~\ref{figure:fig-1}(a))}. 
    %\item \textcolor{blue}{{\bf Frequency of Training Matters:} Training a single Transformer big and BERT base model consumed 200 kWh and 1500 kWh of energy, respectively; in comparison, excluding the hyper-parameter training cost, recommendation model training consumes 2600 kWh of energy — {\bf X} times and {\bf Y} times higher energy consumption than that of Transformer big and BERT base model. [Add training frequencies] (Figure~\ref{figure:fig-2})}.
    %\item \textcolor{blue}{Data storage and ingestion can consume significantly higher power than that of model training.}
    %\item \textcolor{blue}{It is equally important to optimize model training  and inference in order to reduce the overall carbon and environmental footprint of AI computing. {\bf The role of model, framework optimization, and system hardware optimization in AI's environmental footprint.} (Figure~\ref{figure:fig-3})} 
    %\item \textcolor{blue}{{\bf Cloud versus on-device learning.}}
    %\item \textcolor{blue}{{\bf The role of renewable energy} --- market versus location-based --- where AI computation happens matters.}
    
%\end{itemize}



%% Move the following paragraph to model efficiency section
% There is a huge opportunity for the AI community to build a future where model efficiency innovation happens as often as computationally-intensive innovation due to common understanding and metrics. While it is critical we continue to reap benefits from building out an efficient infrastructure, it is not sufficient as growth in AI workloads outpace our infrastructure savings. Our efforts need to match the historical payoffs that the broader industry has seen by investing in algorithmic efficiency: recent analysis by OpenAI showed that, with strong investment in AI technologies in the past decade, ML algorithm innovations have yielded significantly higher efficiency improvement of 44 times as compared to the improvement of 11 times from classical hardware optimization -- 4 times higher improvement from algorithm innovations over hardware optimization!

% In this paper, we start by describing the environmental sustainability crisis faced by the AI community (Section 2). In Section 3, we present the lay of the land and on-going work to improve the environmental footprint of AI computing at \fb, spanning multiple dimensions including — environmental sustainability, the Open Compute Project (OCP) Sustainability Initiative (https://www.opencompute.org/projects/sustainability-initiative), infrastructure demand characterization across the major AI domains, data center infrastructure design. We share the life cycle analysis approach to quantify the carbon footprint of ML models and for AI systems. In Section 4, we chart out on-going work in the space and collaboration opportunities across the dimensions of AI including — data, algorithm, systems, metrics and standards. 


