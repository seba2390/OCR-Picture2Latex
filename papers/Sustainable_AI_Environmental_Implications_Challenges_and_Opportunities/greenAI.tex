%\documentclass{sig-alternate}
%\documentclass[conference,a4paper]{IEEEtran}
\documentclass{IEEEtran}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}
\usepackage{comment}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{xurl}

\newcommand{\fb}{Facebook}
\newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{mlsys2022} with \usepackage[nohyperref]{mlsys2022} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better :
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{authblk}
\newlength{\bibitemsep}\setlength{\bibitemsep}{.2\baselineskip plus .05\baselineskip minus .05\baselineskip}
\newlength{\bibparskip}\setlength{\bibparskip}{0pt}
\let\oldthebibliography\thebibliography
\renewcommand\thebibliography[1]{%
  \oldthebibliography{#1}%
  \setlength{\parskip}{\bibitemsep}%
  \setlength{\itemsep}{\bibparskip}%
}



\title{Sustainable AI: Environmental Implications, Challenges and Opportunities
}


\author{\normalsize{Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\\ Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott,\\ Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee,\\ Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood}}

\affil{Facebook AI}

% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\pagenumbering{arabic}

\begin{document}
\maketitle
%\thispagestyle{firstpage}
\pagestyle{plain}



%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}
This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning \textit{Data}, \textit{Algorithms}, and \textit{System Hardware}. 
We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware.
Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for \textit{what} and \textit{how} hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI.
Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. 
We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.
\end{abstract}


\input{tex/introduction}
\input{tex/model-life-cycle}
\input{tex/cf-characterization}
\input{tex/futureofai}
\input{tex/takeaways}
\input{tex/conclusion}

\section*{Acknowledgement}

We would like to thank 
Nikhil Gupta,
Lei Tian, 
Weiyi Zheng, 
Manisha Jain,
Adnan Aziz,
and Adam Lerer for their feedback on many iterations of this draft, and in-depth technical discussions around building efficient infrastructure and platforms;  
Adina Williams,
Emily Dinan,
Mona Diab,
Ashkan Yousefpour for the valuable discussions and insights on AI and environmental responsibility; 
Mark Zhou,
Niket Agarwal,
Jongsoo Park,
Michael Anderson,
Xiaodong Wang; 
Yatharth Saraf,
Hagay Lupesco, Jigar Desai, Joelle Pineau, 
Ram Valliyappan, Rajesh Mosur,
Ananth Sankarnarayanan and
Eytan Bakshy for their leadership and vision without which this work would not have been possible. 


% the insightful discussions and valuable context for sustainable computing efforts across industry.

% Acknowledgement
% Yatharth Saraf
% Hagay Lupesco
% Nikhil Gupta
% Eytan Bakshy
% Ram Valliyappan
% Ananth Sankarnarayanan
% Adina Williams
% Emily Dinan
% Mark Zhou
% Niket Agarwal
% Jongsoo Park
% Michael Anderson
% Xiaodong Wang
% Ashkan Yousefpour
% Adnan Aziz
% Lei Tian 
% Mona Diab 



%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%


%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ieeetr}
\bibliography{greenai}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix


\section{An Sustainability Mindset for AI}
\label{sec:appendix-efficiiency-mindset}

Despite the recent calls-to-action~\cite{Strubell:arxiv:2019,Lacoste:arxiv:2019,Henderson:arxiv:2020,Bender:facct:2021}, the overall community remains under-invested in research that aims at deeply understanding and minimizing the cost of AI.
There are several factors that may have contributed to the current state of AI:

\begin{itemize}

\setlength\itemsep{0em}
    \item {\bf Lack of incentives:} Over 90\% of the ML publications only focus on model accuracy improvements at the expense of efficiency~\cite{Schwartz:arxiv:2019}. Challenges\footnote{Efficient Open-Domain Question Answering (\url{https://efficientqa.github.io/}), SustaiNLP: Simple and Efficient Natural Language Processing  (\url{https://sites.google.com/view/sustainlp2020/home}), and WMT: Machine Translation Efficiency Task (\url{http://www.statmt.org/wmt21/efficiency-task.html}).} incentivize investment into efficient approaches.
    \item {\bf Lack of common tools:} There is no standard telemetry in place to provide accurate, reliable energy and carbon footprint measurement. The measurement methodology is complex --- factors, such as datacenter infrastructures, hardware architectures, energy sources, can perturb the final measure easily.
    \item {\bf Lack of normalization factors:} Algorithmic progress in ML is often presented in some measure of model accuracy, e.g., BLEU, points, ELO, cross-entropy loss, but without considering resource requirement as a normalization factor, e.g., the number of \\CPU/GPU/TPU hours used, the overall energy consumption and/or carbon footprint required.   
    \item {\bf Platform fragmentation:} Implementation details can have a significant impact on real-world efficiency, but best practices remain elusive and platform fragmentation prevents performance and efficiency portability across model development.

\end{itemize}



\section{Additional Opportunities for AI Research and Development}
\label{sec:additional-opportunities}
\vspace{0.4cm}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/Model-Data-Scaling.pdf}
    %{mlsys2022greenai/images/Data Scaling.png}
    \vspace{-1cm}
    \caption{Model quality of recommendation use cases improves as we scale up the amount of data and/or the number of model parameters (e.g., embedding cardinality or dimension), leading to higher energy and carbon footprint. Maximizing model accuracy for the specific recommendation use case comes with significant energy cost --- Roughly 4$\times$ energy saving can be achieved with only 0.004 model quality degradation (green vs. yellow stars).}
    \label{fig:energy-scaling}
    \vspace{-0.4cm}
\end{figure}

\subsection{Data Utilization Efficiency}
\label{sec:appendix-data-efficiency}

Figure~\ref{fig:energy-scaling} depicts energy footprint reduction potential when data and model scaling is performed in tandem. The x-axis represents the energy footprint required per training step whereas the y-axis represents model error. 
The \textbf{blue} solid lines capture model size scaling (through embedding hash scaling) while the training data set size is kept fixed. 
Each line corresponds to a different data set size, in an increasing order from top to bottom.
The points within each line represent different model (embedding) sizes, in an increasing order from left to right. 
The \textbf{red} dashed lines capture data scaling while the model size is kept fixed. 
Each line corresponds to a different embedding hash size, in an increasing order from left to right.
The points within each line represent different data sizes, in an increasing order from top to bottom. 
The dashed black line captures the performance scaling trend as we scale data and model sizes in tandem. This represents the energy-optimal scaling approach.

%0.79 vs. 0.794
Scaling data sizes or model sizes independently deviates from the energy-optimal trend.
We highlight two energy-optimal settings along the Pareto-frontier curve.
The yellow star uses the scaling setting of \textit{Data scaling 2$\times$} and \textit{Model scaling 2$\times$} whereas the green star adopts the setting of \textit{Data scaling 8$\times$} and \textit{Model scaling 16$\times$}.
The yellow star consumes roughly 4$\times$ lower energy as compared to the green star with only 0.004 model quality degradation in Normalized Entropy. 
%for 0.8\% NE gain) highlights two different energy-optimal solutions at two different accuracy targets and energy cost. Note that while the green point is only 0.5\% better in performance, it requires almost 4x more energy.
Overall model quality performance has a (diminishing) power-law relationship with the corresponding energy consumption and the power of the power law is extremely small (0.002-0.004). This means achieving higher model quality through model-data scaling for recommendation use cases incurs significant energy cost.

%\subsection{Resource-Efficient Modeling Techniques}
%\label{sec:appendix-algo-efficiency}

\subsection{Efficient, Environmentally-Sustainable AI Systems}
\label{sec:appendix-system-efficiency}

\textbf{Disaggregating Machine Learning Pipeline Stages:} As depicted in Figure~\ref{fig:ml_lifecycle}, the overall training throughput efficiency for large-scale ML models depends on the throughput performance of both \textit{data ingestion and pre-processing} and \textit{model training}. Disaggregating the data ingestion and pre-processing stage of the machine learning pipeline from model training is the de-facto approach for industry-scale machine learning model training. This allows training accelerator, network and storage I/O bandwidth utilization to scale independently, thereby increasing the overall model training throughput by 56\%~\cite{Zhao:arxiv:2021}. Disaggregation with well-designed check-pointing support~\cite{Maeng:arxiv:2021,Eisenman:arxiv:2021} improves training fault tolerance as well. By doing so, failure on nodes that are responsible for data ingestion and pre-processing can be recovered efficiently without requiring re-runs of the entire training experiment. From a sustainability perspective, disaggregating the data storage and ingestion stage from model training maximizes infrastructure efficiency by \textit{using less system resources to achieve higher training throughput}, resulting in lower embodied carbon footprint. By increasing fault tolerance, the operational carbon footprint is reduced at the same time.  

\textbf{Fault-Tolerant AI Systems and Hardware:}
One way to amortize the rising embodied carbon cost of AI infrastructures is to extend hardware lifetime. However, hardware ages --- depending on the wear-out characteristics, increasingly more errors can surface over time and result in \textit{silent data corruption}, leading to erroneous computation, model accuracy degradation, non-deterministic ML execution, or fatal system failure. In a large fleet of processors, silent data corruption can occur frequently enough to have disruptive impact on service productivity~\cite{Dixit:arxiv:2021,Hochschild:hotos:2021}. Decommissioning an AI system entirely because of hardware faults is expensive from the perspective of resource and environmental footprints. System architects can design differential reliability levels for micro architectural components on an AI system depending on the ML model execution characteristics. Alternatively, algorithmic fault tolerance can be built into deep learning programming frameworks to provide a code execution path that is cognizant of hardware wear-out characteristics.

%\textbf{AI Computing with Renewable Energy:} There exist a number of challenges and opportunities as data centers increasingly draw on renewable energy sources. Hyperscale data center operators
%---such as \fb~\cite{}, Google~\cite{google_sustainability}, and Microsoft~\cite{microsoft_sustainability}---
%have made significant strides to address the environmental implications of computing. Operators have achieved "net zero" by matching data center energy consumption against renewable energy generation. First, operators collaborate with utility providers, investing in wind or solar generation and securing contracts to purchase the resulting carbon-free energy. Second, as contracted renewable energy is generated, the operator receives renewable energy credits that are matched against data center energy consumption. 

%At present, the generation and consumption of renewable energy is matched annually and more can be done. In future, data centers should schedule computation based on the hourly availability and carbon intensity of its energy sources (\textit{i.e.}, 24/7 carbon-free). As the proportion of renewable energy in the electricity grid increases, data centers must be able to manage increased fluctuations in energy generation due to the intermittent nature of renewable energy sources (\textit{i.e.}, wind, solar). 

%Ensuring renewable energy generation matches data center consumption in each and every hour of the day requires a coordinated, multi-pronged strategy. First, operators must continue to invest in carbon-free energy, ensuring that its contracted mix of solar and wind assets provide significant supply across times of day and seasons of the year. Second, operators should plan to store carbon-free energy, charging and discharging batteries when renewable generation is high and low, respectively. Third, operators should provision additional server capacity so that flexible workloads can be shifted to times when carbon-free energy, whether from the generation or batteries, is more abundant. 

%System architects should explore the rich, multi-dimensional design space to determine the least expensive path to 24/7 matching. The most efficient solution coordinates the provision of renewable generation, battery capacity, and server capacity. Decisions made in one dimension will affect others. A complementary mix of wind and solar may require less battery capacity. Greater battery capacity may reduce demands on workload shifting. The various solutions differ in total cost of ownership as well as the embodied carbon of infrastructure such as batteries or servers. Although there has been significant prior work in data center scheduling based on signals from the power grid \cite{radovanovic2021carbon, liu14sigmetrics, wierman14igcc}, we have yet to see a holistic approach to data center design and management in an era of renewable, carbon-free energy.  

\textbf{On-Device Learning:} 
Federated learning and optimization can result in a non-negligible amount of carbon emissions at the edge, similar to the carbon footprint of training $Transformer_{Big}$~\cite{Patterson:arxiv:2021}.
Figure~\ref{fig:fl_carbon} shows that the federated learning and optimization process emits non-negligible carbon at the edge due to both computation and wireless communication during the process. 
To estimate the carbon emission, we used a similar methodology to~\cite{flcarbon}. We collected the 90-day log data for federated learning production use cases at \fb, which recorded the time spent on computation, data downloading, and data uploading per client device. We multiplied the computation time with the estimated device power and upload/download time with the estimated router power, and omitted other energy. We assumed a device power of 3W and a router power of 7.5W~\cite{phone_ml_energy, flcarbon}.
Model training on client edge devices is inherently less energy-efficient because of the high wireless communication overheads, sub-optimal training data distribution in individual client devices~\cite{flcarbon}, large degree of system heterogeneity among client edge devices, and highly-fragmented edge device architectures that make system-level optimization significantly more challenging~\cite{wu:hpca:2019}. Note, the wireless communication energy cost takes up a significant portion of the overall energy footprint of federated learning, making energy footprint optimization on communication important.


\subsection{Efficiency and Self-Supervised Learning}
\label{sec:ssl}
\input{tex/ssl}

\end{document}
