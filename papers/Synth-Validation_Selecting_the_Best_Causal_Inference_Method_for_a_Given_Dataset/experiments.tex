\section{Evaluation}
\label{sec:eval}

\subsection{Experiments}

We use the set of simulations from \citet{Powers:2017wd} to test various permutations of synth-validation. The simulations are comprised of sixteen different combinations (scenarios) of treatment- and outcome-generating functions. The true average treatment effect in each of these scenarios is standardized to be zero, although each scenario has different levels of effect heterogeneity. The first eight scenarios have randomized treatment assignment (no confounding). The remaining eight scenarios are the same as the first eight, but with biased treatment assignment where subjects most likely to benefit from the treatment are more likely to receive it. We generate ten datasets (repetitions) from each scenario for a total of 160 datasets to test our algorithms on. 

For each dataset, we estimate the treatment effect using five causal inference methods. The first is a baseline method: the difference in mean outcomes between the treatment groups (raw). We also use three popular causal inference methods: linear regression of the outcome on the treatment adjusted for the covariates (adjusted), linear regression of the outcome on the treatment using a 1:1 propensity-matched sample (1:1 matched), and linear regression of the outcome on the treated weighted by the inverse of the propensity score (IPTW). Finally, we test a machine learning approach: taking the mean difference between the potential outcomes predicted by tree boosting models fit on the treated and untreated samples (boosting) \cite{Austin:2012cy}. In all cases, the methods are set up so that the estimand is the average treatment effect (ATE). All propensity scores are calculated using logistic regression of the treatment on all covariates. Matching is done on the logit of the propensity score, with calipers equal to $0.2$ times the standard deviation of the propensity score \cite{Austin:2011dc}. Weighted estimates are calculated by trimming propensities greater than $0.99$ or less than $0.01$ and stabilizing the weights \cite{Sturmer:2014kr}. We use cross-validation to select among boosting models. 

We assess the true estimation error of each method by taking the difference of the estimate and the true effect $e_t = \hat{\tau}_t - \tau$. We calculat the error from using synth-validation as $e_{t^\dagger}$ where $t^\dagger$ is the causal inference method chosen by the synth-validation (equation \ref{eq:best-meth}). We also compare synth-valdiation against an oracle selection algorithm that achieves the lowest possible error of any method selection algorithm. We calculate the error from using the oracle selector as $e_{oracle} = e_{t^{\dagger \dagger}} = \underset{t \in \mathcal{T}}{\text{min}} \ e_t$. The oracle selector is not usable outside of simulations because it uses the true errors $e_t$ to select the causal inference method, which requires knowing the true effect $\tau$ a-priori.

We test various permutations of synth-validation. Each permutation is a combination of an algorithm for fitting the conditional mean models (section \ref{sec:fitting}) and an approach for choosing the synthetic effects (section \ref{sec:synth-effect}).

\subsubsection{Fitting}
\label{sec:fitting-eval}
For fitting, we test fit-plus-constant using linear regression (FPC-linear), fit-plus-constant using gradient boosted trees (FPC-treeboost), and constrained gradient boosted trees (CGB-tree).

\subsubsection{Synthetic Effect Choice}
\label{sec:synth-effect-eval}
To choose the synthetic effects, we use the approach described in section \ref{sec:synth-effect} with $Q=5$ synthetic effects spaced out over a narrow span using $\gamma=2$ (narrow) or a wide span using $\gamma=5$ (wide). We also test using $Q=10$ synthetic effects that are the union of those in the wide and narrow spans (combined). 

%In addition to the sets of synthetic effects chosen by these heuristics, we test the following three sets of synthetic effect sizes: $\tilde\tau \in \{-5,-4,-3,-2,-1\}$ (underestimate), $\tilde\tau \in \{1,2,3,4,5\}$ (overestimate), and $\tilde\tau \in \{-2,-1,0,1,2\}$ (centered). These sets test the effect of systematically (in)correctly specifying the synthetic effects. They do not represent a usable strategy for picking synthetic effects, since in practice the true effect is not known.

\paragraph{} We test every combination of these approaches for fitting and setting the synthetic effects. In each case we use $K=20$ bootstrap samples from each resulting generative distribution (section \ref{sec:run-meth}).

%\subsection{Analysis}
%
%\paragraph{Performance} We quantify the performance differences between our algorithms with random effects models that use the scenario as a random intercept. Let $z_{f}$ and $z_{e}$ denote the choices of fitting algorithm and synthetic-effect-setting heuristic, respectively. For scenarios $a \in \{1,2\dots 16\}$ each with repetitions $s \in \{1,2\dots 10\}$, we model:
%
%\begin{equation}
%e_{a,s} = \alpha_{s} + \beta_{f} z_{f_{a,s}} + \beta_{e} z_{e_{a,s}} + \epsilon_{a,s}
%\end{equation} 
%
%We use a similar approach to quantify differences between our algorithms and the consistent use of each causal inference method, but we treat each combination of fitting algorithm and synthetic-effect-setting heuristic as a single factor $(z_{f}, z_{e}, z_{n}) = t' \in \mathcal{T}'$: 
%
%\begin{equation}
%e_{a,s} = \alpha_{s} + \beta_{t} z_{(t \in \mathcal{T} \cup \mathcal{T}')_{a,s}} + \epsilon_{a,s}
%\end{equation} 
%
%We run these models for each pairwise comparison of fitting algorithms, synthetic-effect-setting heuristics, and causal inference methods, adjusting for the full ranges of all other variables outside the pairwise comparison. For instance, to compare CGB-tree to FPC-treeboost, $z_{f} \in \{\text{CGB-tree}, \text{FPC-treeboost}\}$ and $z_e \in \{\text{narrow}, \text{wide}, \text{combined}\}$ and we use the estimate of $\beta_f$ and its associated p-value to assess the magnitude of the effect and its significance. 
%
%\paragraph{Algorithm biases} To see if our algorithms are biased towards selecting certain causal inference methods, we also qualitatively examine the number of times that each causal inference method is selected by each selection algorithm against how many times it is selected by the oracle in each scenario. 

% What is the most natural way to quantify this? I don't want to run a bunch of chi-sq. tests against the oracle probabilities. Multinomial logistic regression of p = bz_f + ... bz_n ?    