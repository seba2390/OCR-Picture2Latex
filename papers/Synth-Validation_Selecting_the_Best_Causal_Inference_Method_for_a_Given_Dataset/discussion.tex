\section{Discussion}
\label{sec:disc}

Our results demonstrate that synth-validation successfully lowers estimation error by choosing a causal inference method that is appropriate for the observed data.

\subsection{Components of Synth-Validation}

\subsubsection{Fitting}
CGB-tree, the most sophisticated of our fitting approaches, is the best performing. FPC-treeboost also performs well, but since the computational difference between the two is the cost of solving $m$ small quadratic programs (regardless of $n$), the two scale equally well and there is not a good argument to be made for FPC-treeboost besides that it can be implemented with off-the-shelf software.

The difference between our three fitting approaches is how ``similar'' we require the resulting synthetic outcomes to be to the original outcomes, where similarity is measured in terms of the loss in equation \ref{eq:opt}. Because of the more prudent optimization scheme, the minimum value of the loss obtained via CGB-tree should be less than that obtained by FPC-treeboost. There is no general relationship between the minimum loss of FPC-treeboost and FPC-linear, but the only difference between the two is the off-the-shelf learner (boosting vs. linear regression) used to fit $h_0$ and $h_1$. Boosting is generally considered to achieve better fits than linear models on most data, and so we generally expect FPC-treeboost to achieve a lower loss than FPC-linear. 

Based on this theory and our results, we posit that the difference in performance between algorithms that use these different fitting approaches is due to the extent to which they can produce synthetic data that ``looks like'' the real data. In practice, users are free to use cross-validation as we implement it to select not just between parameter settings of CGB-tree, but between different fitting approaches altogether. If our hypothesis holds, then algorithms using the fitting approach that yields the lowest expected MSEs while satisfying the same set of synthetic treatment effects should generally select the causal inference method that has the lowest estimation error on the real data.

The relative differences between using synth-validation vs. single causal inference methods disappear when the treatment is randomized. Primarily this is because it becomes easy to estimate average treatment effects in randomized settings and so all causal inference methods perform reasonably well (coming close to matching the performance of the oracle selector). However, even if this were not the case (e.g. if one of the causal inference methods were a strawman that always returned $\hat\tau=10$), we suspect that each fitting approach in our algorithms would perform equally well given randomized data. On nonrandomized data, CGB-tree preferentially ``moves'' potential outcomes that are in regions of low observed support for their treatment in order to satisfy the constraint while better minimizing the objective. In other words, the residuals for subjects in regions of low covariate support (given their treatment) are higher. FPC-treeboost uses a single constant to shift its initial predictions, and thus we expect the residuals of its final predictions to all be of similar magnitude (figure \ref{fig:didactic}). In randomized data, there are no regions of low support for either treatment, and we expect the predictions from CGB-tree to be very similar to those of FPC-treeboost, and consequently their downstream results should be similar as well. 

Figure \ref{fig:didactic} also illustrates the differences between synthetic datasets generated by synth-validation using CGB-tree and FPC-treeboost. Using CGB-tree, the synthesized outcomes are very close to the observed outcomes for the majority of subjects, but for a minority of them (those in regions of low support), the synthesized outcomes may be very different than the observed outcomes. Using FCP algorithms, the outcomes for all subjects are perturbed by a moderate amount. 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{didactic} 
\caption{Illustration the behavior of different fitting approaches using a toy dataset. Lines represent the conditional mean functions estimated using CGB-tree and FPC-treeboost and points represent the original dataset. The toy dataset has a single confounder $X \sim \text{Uniform}(-\pi, \pi)$, biased treatment assignment $W \sim \text{Binom}((1+e^{-2X})^{-1})$ and an outcome that does not depend on the treatment $Y\sim \text{Normal}(\sin(X), 0.15)$. The synthetic effect is set to either $\tilde\tau=-2$ (underestimated) or $\tilde\tau=2$ (overestimated). Note that CGB-tree stays closer to the data in regions of good support (e.g. $x<0$ for the untreated), but diverges from the data in regions of poor support (e.g. $x<0$ for the untreated).}
\label{fig:didactic}
\end{figure}

% FPC-linear performs more variably across repetitions of the 8 scenarios with biased treatment assignment than the other algorithms. The result suggests that FPC-linear is sensitive to sampling noise in the original data which possibly causes misprediction of relatively important outliers that models based on constants (e.g. trees) might predict with less variance.



It is interesting that CGB-tree is not demonstrably biased towards picking any particular causal inference method. This can be seen in figure \ref{fig:selection}, where biases are visible as horizontal bands of blue or orange. FPC-linear has a clear bias for covariate-adjusted linear regression ("adjusted") and FPC-treebost seems to be biased towards boosting (visible in the right panel). Our hypothesis before performing the evaluation was that each fitting approach would favor the causal inference method with the correct outcome model specification: linear regression for FPC-linear and boosting for FPC-treeboost and CGB-tree. Our results, however, tell a more nuanced story: there are many scenarios in which boosting is not chosen as the causal inference method by CGB-tree or FPC-treeboost, or both. It may be the case that other causal inference methods often perform better on synthetic data generated by boosting models because the final target of inference is the average treatment effect, not the conditional mean of the outcome-generating function per-se. Boosting relies on estimates of unobserved potential outcomes for subjects that are in regions of low support. Even if the model is well-specified, there will be higher variance in the predictions in those regions, which could impact error in treatment effect estimation. This is especially relevant for CGB-tree, where synthetic outcomes for subjects in regions of low support are more likely to vary unexpectedly (figure \ref{fig:didactic}). This, combined with the fact that regions of low support will be less sampled from during bootstrapping, means that boosting is likely to critically misestimate potential outcomes in those regions. This helps explain why synth validation with CGB-tree appears immune to bias from correctly-specified outcome models.

\subsubsection{Synthetic Effect Choice}
The choice of the synthetic effects does not play a large role in the performance of synth-validation. Considering how close synth-validation with CGB-tree gets to the oracle in terms of performance, it seems that our heuristic works well enough to advocate its use. There are other heuristics that may also work. Considering that changing $\gamma=2$ to $\gamma=5$ in our heuristic did not change performance, using other heuristics may not have a large impact either. In practice the choice of heuristic or of $\gamma$ should be based on expert knowledge of the domain: if few previous studies exist, it would be wiser to use a larger value of $\gamma$. % Bayesian approaches to selecting synthetic effects may also be appropriate.

%Underestimating the range for the synthetic effects negatively impacts the performance synth-validation with CGB-tree, but overestimating the range does not have a notable effect. This is likely because, in all of our scenarios, the confounding is such that the treatment effect appears larger than it actually is. Because GCB-tree preferentially increases residuals in regions of low support for each treatment, using underestimated synthetic effects creates potential outcomes surfaces $\hat{f}^{(\tilde\tau)}$ that are more severely warped or disjointed than those for overestimated synthetic effects (figure \ref{fig:didactic}). FPC-treeboost fits the surfaces $h_0$ and $h_1$ and shifts them with constants, so they are identical up to constants for different synthetic effects and do not exhibit the same warping. In a sense, the synthetic data generated by FPC-treeboost in these cases is more ``similar'' to the real data in a way that is not accounted for by the loss alone. This complicates our hypothesis that lower expected loss leads to selection of causal inference methods with lower expected error.

%Despite this, the performance of synth-validation is still relatively unaffected under systemic misspecification of the synthetic effects. 
It is likely that more misspecification of the synthetic treatment effects would degrade performance. However, employing our heuristic, misspecification of the synthetic effects only occurs if no causal inference method comes close to correctly estimating the effect. In such a situation, selection of the best method is largely a moot point: the study is bound to have large bias regardless.

\subsection{Future Work}

\paragraph{Further evaluation} We have focused here on the conceptual and methodological advancements necessary to perform dataset-specific selection of causal inference methods. Further simulation studies using larger sets of scenarios and causal inference methods will be helpful in teasing out and explaining differences in performance. It may also be possible to perform an equivalent evaluation on a set of real observational datasets where the true treatment effect is known to a high degree of confidence from large randomized experiments (e.g. \citet{Madigan2014}). 

\paragraph{Extension to other classes of outcomes} For the sake of brevity, we have limited the current work to algorithms that are formulated for real-valued outcomes and squared-error loss. Alternative losses are easily accommodated: as long as they are convex, the resulting optimization problems (equations \ref{eq:opt-proj-consts}, \ref{eq:first-opt}, and \ref{eq:stage-opt}) will still be convex, if not quadratic. However, many outcomes of interest are binary or time-to-event, which requires more than changing the loss. We have generalized our approach to other kinds of outcomes using link functions (e.g. for binary outcomes, treatment effects are risk differences and we model the log-odds of the outcome in a continuous space). This creates nonlinear constraints, but since each optimization problem is still small, the algorithms remain tractable. In fact, these nonlinear constraints are what motivate the use of a regularizer in equation \ref{eq:stage-opt} instead of a scaling factor: scaling the solution to a nonlinearly constrained optimization problem takes it off of the constraint surface. We reserve further discussion and evaluation of these extensions for a later date.

\paragraph{Impact of unmeasured confounding} No modeling approach can capture the effects of real unobserved confounding. However, some unobserved confounding can be simulated within synth-validation by removing certain covariates from the synthetic datasets. Variable importance metrics for each covariate in the fit models could be used to choose which covariates to remove; removing covariates with high variable importance would simulate high unobserved confounding and vice-versa. However, there remains the possibility that the unobserved confounders have relationships to the outcome that are fundamentally different from those between the measured covariates and the outcome, in which case even this kind of artificial censoring would not result in synthetic datasets that are good proxies for the original. Further study will be necessary to elucidate the impact of different kinds of unmeasured confounding.

\paragraph{Software} We plan to release user-friendly open-source software packages that implement our algorithms for causal inference method selection.

