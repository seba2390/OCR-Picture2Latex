\section{Introduction}

\subsection{Background}
 
The fundamental problem of causal inference is that after a subject receives a treatment and experiences an outcome it is impossible to know what the outcome would have been had the subject received a different treatment (the counterfactual outcome) \cite{Stuart:2013dt}. The difference in outcome between those two treatments is the true causal effect of the treatment on that subject. It is possible to avoid this problem by randomizing which subjects receive the treatment. Randomization ensures that the populations receiving the different treatments are statistically identical and that the difference in average outcomes between the two populations approaches the true average causal effect in expectation \cite{ROSENBAUM1983}.

Subjects in an observational study are not randomized to their treatments, which creates \textsl{confounding}: the resulting treatment populations are different at baseline in ways that affect their corresponding outcomes \cite{King2005}. Consequently, differences in outcomes cannot be solely attributed to differences in treatment. It is well known that observational data analysis is a risky business \cite{Rubin2010, Hannan:2008gh}. 

Causal inference methods exist to alleviate the problems with observational data analysis \cite{Stuart:2013dt}. Many of these methods work by matching together subjects similar at baseline from each treatment population and using only the matched subsample for further analysis. Popular causal inference methods include covariate matching \cite{Iacus}, propensity score matching \cite{ROSENBAUM1983}, and inverse probability weighting \cite{Robins1992}, while less widely-used methods include doubly robust estimators \cite{Robins1992}, prognostic score matching \cite{Leacy:2013fs}, and targeted maximum likelihood estimation \cite{Schuler:2017cq}. Each method has a number of variations and many methods can be combined and used in tandem with other methods \cite{Colson:2016fu}. 

Existing comparative evaluations of causal inference methods rely on handcrafted data-generating distributions that encode specified treatment effects \cite{Setoguchi2008, Colson:2016fu, Shortreed:2017fk, Antonelli:2016ve, Lee2010, Hill2011}. Causal inference methods are applied to datasets sampled from these distributions and the results are compared with the known effects to see how close each method gets on average (figure \ref{fig:bakeoff}). However, dissimilarities between these handcrafted benchmarks and real-world data-generating processes mean that it is difficult to know what methods actually work best with real data in general or in the context of a specific study. Because different causal inference methods rely on assumptions that hold in different cases, is likely that different methods are better suited for different studies. There is little consensus among evaluations that use different handcrafted benchmarks, which suggests that there is no one-size-fits-all best method \cite{Setoguchi2008, Colson:2016fu, Shortreed:2017fk, Antonelli:2016ve, Lee2010, Hill2011}. Deep domain and statistical expertise is necessary to pick an appropriate method, which means that many researchers default to what they learned in their first course in statistics (i.e. linear regression) \cite{Stuart2010}. A data-driven approach could augment expertise and safeguard against na\"{\i}vet\'{e}. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.75\textwidth]{benchmark} 
\caption{A standard evaluation of some causal inference methods $t_A$, $t_B$, $t_C$ using handcrafted generative distributions. The experimenter comes up with several data-generating distributions from which several datasets are sampled. All datasets are fed through each causal inference method, producing treatment effect estimates. Estimates are compared to the corresponding true effects and the errors are averaged over all samples and data-generating distributions. The causal inference method with lowest average error is deemed best.}
\label{fig:bakeoff}
\end{figure}

% As a proxy, researchers might choose the method that retrospectively performs best on similar studies for which the true treatment effects are known from corresponding high-quality randomized studies\cite{Madigan2014}. Unfortunately, the devil is in the details: it is difficult to quantify similarity between studies, define standards of quality, and find precisely-corresponding pairs of observational and randomized trials.

This problem does not exist in a predictive modeling setting, where the goal is to estimate the outcomes for previously-unseen subjects. Predictive modelers can easily ``synthesize'' unseen subjects by hiding a set of subjects (the test set) from their models during fitting. Models are benchmarked by comparing predictions with the previously hidden ground-truth outcomes in the test set to compute a test error. This is most often done in a round-robin fashion (cross-validation). The cross-validation error is an estimate of how well the model predicts outcomes generated by the unknown distribution that the training data is sampled from \cite{Hastie:2009fg}. Depending on the underlying distribution, different models work better, which is reflected in their relative cross-validation errors. The end result is that, given a dataset, predictive modelers are able to determine which model is likely to produce the most accurate prediction of future outcomes.

The same cannot be done for causal inference methods because the estimand (the treatment effect) is a parameter of the entire data-generating distribution, not a variable that can be directly measured \cite{Shmueli:2010ec}. For that reason, it does not suffice to hide a portion of the dataset. Estimating error requires us to know the true treatment effect a-priori which is a catch-22 since the treatment effect is what we would like to estimate in the first place. Because of this, there is no way to get around simulating data from known distributions with known effects. However, we show how the observed data can inform the creation of these distributions to produce study-specific benchmarks. The end result is that, given a dataset, we are able to determine which causal inference method is likely to produce the most accurate estimate of the treatment effect. We call our procedure synth-validation.

The difference between synth-validation and the handcrafted benchmarking process we describe above (figure \ref{fig:bakeoff}) is that the generative distributions we use in synth-validation are derived from the observed data. Our hypothesis, which we confirm experimentally, is that letting the study data shape the generative distributions improves our ability to find the right causal inference method for that study.

Cross-validation and synth-validation are related only insofar as they allow users to compare models or estimators in the context of their own data. Unlike cross-validation, synth-validation is not entirely ``model free'': the process involves modeling generative distributions. The modeling choices we make could bias the procedure towards favoring certain causal inference methods, but we find that these biases are empirically negligible when we use flexible models. 

Synth-validation is inspired by and generalizes the ``plasmode simulations'' of  \citet{Franklin:2014kz}.

\subsection{Outline}

Section \ref{sec:notation} briefly describes some notation. In section \ref{sec:algo} we derive a class of algorithms that use observed data to estimate generative distributions encoding specified treatment effects. Each algorithm is defined by three parts: an algorithm to select what treatment effects to encode, an algorithm that fits a predictive outcomes model that is constrained to encode a specified effect, and a semi-parametric bootstrap that combines a predictive model with a noise model and distribution over predictor variables to create a fully generative model.

To test the efficacy of synth-validation, we use several simulated datasets that we treat as ``real'' for the purposes of the evaluation (section \ref{sec:eval}). We use synth-validation to select the best causal inference method to use for each dataset and use that method to estimate the treatment effect from the real data, which we compare with the known ground-truth effect to calculate the error. We average the errors across all datasets and compare with the average errors obtained by consistently using each single causal inference method.

In section \ref{sec:results} we show the result of the evaluation. Using synth-validation to select different causal inference methods for each dataset is better than using the single best causal inference method for all datasets. In section \ref{sec:disc} we discuss the result, performance differences between variants of synth-validation, limitations, and future directions.