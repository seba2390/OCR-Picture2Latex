\section{Conclusion}


To our knowledge, we present the first formal approach to select causal inference methods in a way that is tailored to a given dataset. We show that the best-performing causal inference method for one dataset is not necessarily the best for another. We present synth-validation, a procedures that estimates how well causal inference methods will estimate an average treatment effect in the context of a given dataset. We evaluate synth-validation using a large number of diverse simulated datasets with known treatment effects. Using synth-validation results in a meaningful and significant decrease in the expected error of estimating the average treatment effect relative to the consistent use of any single causal inference method. We suggest that practitioners of observational studies in healthcare, business, and other policy domains use synth-validation to improve treatment effect estimation and contribute to better decision-making.

% use alongside negative control outcomes and sensitivity analyses?