\section{Results}
\label{sec:results}

\paragraph{Performance} Our results show that all of the synth-validation variants we propose outperform the individual use of each of the causal inference methods that synth-validation selects from (figure \ref{fig:result}). Synth-validation using CBG-tree has the best performance, followed closely by synth-validation with FPC-treeboost,
%($p=0.01$ vs. CBG-tree)
and trailed by synth-validation with FPC-linear
%($p<10^{-6}$ vs. FPC-treeboost) 
only marginally beating the individual use of any of the causal inference methods
%($p>0.14$ vs. Boosting for all synthetic effect-setting heuristics). 
Synth-validation appears robust to the choice of $\gamma$ and $Q$ in our heuristic for choosing the synthetic treatment effects. No settings (narrow, wide, or combined) result in notably better performance.
The individual causal inference methods all perform on par with each other. The baseline method (raw) performs poorly in comparison to other methods.

These results are driven by performance on scenarios with biased treatment assignment. All methods work about equally well when assignment is randomized and the average effect is easy to estimate.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{final_result} 
\caption{Average error across all scenarios in estimating the true treatment effect using each causal inference method across all scenarios or using synth-validation or an oracle to select which method to use in each scenario. Lower error is better. The left plot includes only scenarios with biased treatment assignment, the right includes only those with randomized assignment. Boxplots represent variation across the ten repetitions of the entire evaluation. We evaluate several variants of synth-validation: colors represent different fitting algorithms (sections \ref{sec:fitting-eval} and \ref{sec:fitting}) while different saturations (darkness of the color) represent different heuristics used to choose the synthetic effects (sections  \ref{sec:synth-effect-eval} and \ref{sec:synth-effect}).}
\label{fig:result}
\end{figure}

%\begin{figure}[h!]
%\centering
%\includegraphics[width=\textwidth]{final_result_fake} 
%\caption{Error in estimating the true treatment effect using each method selection algorithm with pre-specified ranges for the synthetic causal effects.}
%\label{fig:result_fake}
%\end{figure}

%The results are similar when using pre-specified ranges for the synthetic causal effects (underestimate, overestimate, or centered), except that synth-validation with CGB-tree performs significantly worse (to the level of FPC-linear) when the synthetic effects are underestimated (figure \ref{fig:result_fake}). 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{selection} 
\caption{
Number of times that synth-validation with different fitting algorithms selected each causal inference method across the ten repetitions of each scenario, relative to the number of times each method was selected by the oracle across the same repetitions. The left panels are scenarios with biased treatment assignment, the right are scenarios with randomized treatment assignment. The top, middle, and bottom panels are the different fitting algorithms used in synth-validation (sections \ref{sec:fitting-eval} and \ref{sec:fitting}). Colors represent the number of times that synth-validation selected each method in each scenario relative to the number of times that method was selected by the oracle in that scenario (a ratio of frequencies). Orange means that synth-validation selected that method in that scenario more often than the oracle did across repetitions of each scenario; blue means that synth-validation selected that method in that scenario fewer times than the oracle did. Lighter colors (closer to white) indicate close agreement with the oracle and thus optimal performance. The size of the circles represents the absolute number of times that the oracle chose each method in each scenario. The pattern of circles is the same in the top, middle, and bottom panels since each panel compares synth-validation using a different fitting algorithm to the same comparator (the oracle).
}
\label{fig:selection}
\end{figure}

\paragraph{Algorithm biases} The oracle chooses different methods with different frequencies in different scenarios (figure \ref{fig:selection}). There are some scenarios where one method dominates (e.g. Boosting in scenario 12), but more frequently there is some variance across repetitions of the evaluation in which is the best method. The distribution of choices with synth-validation using CBG-tree most closely tracks the distribution of the oracle's choices (i.e. their checkerboard patterns are a close match). Synth-validation with FPC-linear seems to have an affinity to the adjusted method of causal inference in the scenarios with biased assignment, whereas synth-validation with FPC-treeboost seems to be biased towards boosting in scenarios with randomized assignment
% What is the most natural way to quantify this? I don't want to run a bunch of chi-sq. tests against the oracle probabilities. Multinomial logistic regression of p = bz_f + ... bz_n ?    