
% 
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[a4]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{algorithmic}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}  % extended mathematics
\usepackage{amsthm} % for the proof environment
\usepackage{amsfonts}
\usepackage{alltt}
\usepackage[margin=1in]{geometry}
\usepackage{authblk}

\usepackage{float}
\usepackage{placeins}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{float}
\newfloat{algorithm}{t}{lop}
\floatstyle{boxed}
\newfloat{mybox}{t}{lop}
% \floatname{box}{Box}

%\usepackage{lineno}
%\linenumbers


% All such use the theorem counter
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{example}[thm]{Example}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{conj}[thm]{Conjecture}


% shortcuts
\newcommand{\curly}[1]{\mathcal{#1}}
\newcommand{\+}[1]{\mathbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}

% commands
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\Tr}{\mathsf{Tr}}
\newcommand{\Vect}{\mathsf{Vec}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\rk}{\operatorname{rk}}
\newcommand{\dist}{\operatorname{Dist}}
  
%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
%for fig:indistinguishability
% \usepackage{tikz}
% \usetikzlibrary{tikzmark}
% \usetikzlibrary{matrix}    
    
\renewcommand\Affilfont{\fontsize{9}{10.8}\itshape}


% end of the preamble, start of the body of the document source.
\begin{document}

\title{CLARITY - Comparing heterogeneous data using dissimiLARITY} 
\author[1,2]{Daniel J. Lawson\thanks{dan.lawson@bristol.ac.uk}}
\author[ ]{Vinesh Solanki}
\author[3]{Igor Yanovich}
\author[3]{Johannes Dellert}
\author[4]{Damian Ruck}
\author[5]{Phillip Endicott}


%%\author{Daniel J. Lawson$^{1,2}$, Vinesh Solanki$^1$, Igor Yanovich$^3$, Johannes Dellert$^3$,  Damian Ruck$^4$ and Phillip Endicott$^5$}
\affil[$^1$]{Institute of Statistical Sciences, School of Mathematics, University of Bristol, Bristol, UK}
\affil[$^2$]{Integrative Epidemiology Unit, Population Health Sciences, University of Bristol, Bristol, UK}
\affil[$^3$]{University of T\"ubingen, Seminar f\"ur Sprachwissenschaft; DFG Center ``Words, Bones, Genes, Tools'', T\"ubingen, Germany}
\affil[$^4$]{Department of Anthropology, University of Tennessee, Knoxville, TN, USA}
\affil[$^5$]{Unité Eco-Anthropologie (EA), Muséum National d’Histoire Naturelle, 17 place du Trocadero, 75016, Paris, France}
\maketitle

\section*{Abstract}
Integrating datasets from different disciplines is hard because the data are often qualitatively different in meaning, scale, and reliability. When two datasets describe the same entities, many scientific questions can be phrased around whether the (dis)similarities between entities are conserved  across such different data. Our method, CLARITY, quantifies consistency across datasets, identifies where inconsistencies arise, and aids in their interpretation. We illustrate this using three diverse comparisons: gene methylation vs expression, evolution of language sounds vs word use, and country-level economic metrics vs cultural beliefs.  The non-parametric approach is robust to noise and differences in scaling, and makes only weak assumptions about how the data were generated. It operates by decomposing similarities into two components: a `structural' component analogous to a clustering, and an underlying `relationship' between those structures. This allows a `structural comparison' between two similarity matrices using their predictability from `structure'. Significance is assessed with the help of re-sampling appropriate for each dataset. The software, CLARITY, is available as an R package from \href{https://github.com/danjlawson/CLARITY}{github.com/danjlawson/CLARITY}.

\section{Introduction}
\label{sec:intro}

The need to compare different sources of information about the same subjects arises in most quantitative sciences. With sufficient effort, it is always possible to construct a model that accounts for data of arbitrary complexity. But without this time-consuming work, can we visualise the data to determine whether the different sources describe the same qualitative phenomena?
 
Many datasets are best expressed in terms of similarities or differences between subjects, and are frequently compared by plotting the resulting matrices side by side. Examples include the co-evolution of language and culture \cite{SokalGeneticgeographiclinguistic1988}, as well as genetics and phenotype \cite{Creanzacomparisonworldwidephonemic2015}, which are all linked through their geographical constraints and shared history. Further uses include identifying brain function using neural activity patterns \cite{KriegeskorteRepresentationalSimilarityAnalysis2008a}, understanding disease through comparing the expression of genes with biomarkers \cite{GrigoriadisMolecularcharacterisationcell2012}, toxicology prediction comparing the activation of biological pathways \cite{RomerCrossplatformtoxicogenomicsprediction2014} and  understanding bacterial function by comparing nucleotide variation to that of amino acids \cite{ZhangCompletemitochondrialrDNA2018}.

We describe a new method that is computationally efficient and can be applied whenever similarities or dissimlarities can be defined. We present three diverse examples to demonstrate the potential utility of this method. The first is comparing gene methylation to expression in simulated data containing anomalies; the second compares lexical change to phonetic change data in linguistics, and the third examines the interaction between culture and economics. Beyond providing a new method with extremely wide applicability, this paper aims to focus attention on the problem area of \emph{structural comparisons} in general.

\subsection{The purpose of CLARITY}

Figure \ref{fig:sim1} is a `graphical abstract' to illustrate what CLARITY is designed to detect. Rather than learning a model, CLARITY identifies features of one dataset that are anomalous in the second - marginalising out structures present in both. It does this from information on subjects, i.e. labelled entities on which we have data such as countries, languages, genes, etc. --- which are matched across datasets. It looks for structures --- that can be loosely thought of as clusters or shared variance components --- that are present in the second dataset, but were not in the first. It reports by identifying subjects with large residuals that `persist', i.e. can't be predicted from the first dataset, for a wide range of complexity of representation. 

CLARITY works with dissimilarities (and equivalently, similarities), which until they are formally defined (see Section \ref{sec:methodsmath}), can be thought of as generalising distances between all pairs of items. 
Similarities can often be defined even when the data does not form a convenient space, e.g. neural spike trains \cite{victor1997metric} or genetic relatedness \cite{lawson_inference_2012}. Similarities are more general than covariances and make a richer representation than a tree --- all trees can be represented as a distance, but the converse is not true. They can be defined on regular feature matrices, or on richer spaces, and are robust to the inherent complexity of the data. However, the better chosen a similarity measure is, the better empirical performance can be expected.

CLARITY should have wide application across many disciplines. The paper is written to allow non-specialists to gain insight into the approach and its correct interpretation. Users of the methodology should read the Results and Discussion section, which include simulated and real examples that should be insightful for specialists and non-specialists of the application area. Further mathematical justification and technical details are available in Methods.

\begin{figure}[!ht]
\begin{center}
 \centerline{\includegraphics[width=1.0\columnwidth]{Fig1}}
 \caption{
   {\bf What is CLARITY For?} CLARITY compares a \emph{reference} dataset (i) to a \emph{target} dataset (ii-iii) to perform a \emph{structural comparison}.
   (a) \emph{Structure} is defined in terms of the similarity between \emph{subjects} (here letters a-p) that in this example fall into \emph{clusters}. These clusters have a \emph{relationship}, here the distance between clusters.
   (b) The data are quantified as a \emph{similarity matrix} between all subjects. We learn the Structure by minimising residuals in the reference (b-i) and predict the target similarity (b-ii,b-iii) by relearning the relationship. In (ii) the Structure is the same as the reference but the relationship changes. In (iii), a subject also changes cluster, leading to a structural change.
   (c) This is captured in Clarity using a residual \emph{persistence chart}. This quantifies how well we predict each subject at a range of representation complexity (here, number of clusters). This allows CLARITY to identify structural change separately to relationship change, for diverse models and data. Box 1 defines the italicised terms and Section \ref{sec:claritydef} discusses this figure.}
   \label{fig:sim1}
\end{center}
\end{figure}
%%% Could we reference the following sections that explain fully the meaning of residuals and complexity? Phil 

\subsection{Overview of comparison approaches}

How different is the information provided about \emph{the same} subjects in two datasets? For what follows, we are interested in the relationship between the subjects, rather than particular features in the datasets, and we assume that we have enough information to build a meaningful similarity matrix between the subjects.

The gold standard approach involves \emph{generative modelling}, in which the joint model for both datasets is specified. Examples include host-parasite coevolution \cite{BrooksTestingContextExtent1979} and comparing linguistic and genetic data \cite{AmorimBayesianApproachGenome2013}. However, each analysis is bespoke, requiring an expert modeller able to specify a joint model for the two datasets. 

If the datasets take a matrix form then \emph{testing} whether two matrices are statistically equivalent is another natural starting point. For this, Mantel's test \cite{Manteldetectiondiseaseclustering1967} and related approaches \cite{SmouseMultipleRegressionCorrelation1986}, can be used. However, for the sort of scientific investigation that we are considering here, the null hypothesis that the two datasets have `the same' distribution, or, in the case of shared historical processes, are 'independent' of each other, can often be rejected \emph{a-priori}. We are thus interested in richer comparisons, able to highlight specific subjects that behave differently in two datasets. 

Data can be directly compared by transforming one to look like the other. When applied to matrices, an important class are \emph{Procrustes transformations} \cite{HurleyprocrustesprogramProducing1962}, which use rotation, translation and scaling \cite{SchneiderMatrixcomparisonPart2007} to perform the maximally achievable matching.
% This technique is widely applied, for example \cite{gower1975generalized} has well over two thousand citations and is just one of several key papers on the method. 
% were widely used in handwriting and image recognition \cite{GoodallProcrustesMethodsStatistical1991} and
Procrustes transformations have been used for testing matrix equality under transformation \cite{JacksonPROTESTPROcrusteanRandomization1995}, and are often combined with initial rank reduction via \emph{Spectral decomposition} for matrix comparison, e.g. \cite{Peres-NetoHowwellmultivariate2001}.

If we are not constructing an explicit model of both datasets, nor testing whether they are identical, then the remaining options revolve around constructing summaries that can be compared. Many methods exist to compare \emph{covariance matrices}. Testing \cite{SteigerTestscomparingelements1980} is again straightforward. Metrics comparing covariance matrices exist \cite{ForstnerMetricCovarianceMatrices2003}, while spectral methods, such as common principal component analysis \cite{FluryCommonPrincipalComponents1988}, allow theoretical statements to be made about the results of a comparison \cite{FluryAsymptoticTheoryCommon1986}.

Another important class of summary are \emph{tree-based methods} that represent each dataset as a tree, which can be compared using standard metrics. These include topological distance \cite{PennyUseTreeComparison1985,Billesurveytreeedit2005}, and tree-space \cite{NyePrincipalcomponentanalysis2017}, and the approach is implemented in popular packages such as `phangorn' \cite{Schliepphangornphylogeneticanalysis2011} in R. The downside is that handling model uncertainty is difficult, with only some types of tree being stable to small changes in the data \cite{carlsson_characterization_2010}. Often the data are not completely hierarchical - for example, tree-based methods can be misleading when the data have a mixture element to them \cite{mossel_phylogenetic_2005}. Conversely, whilst mixtures might be compared using fixed-dimensional mixture based methods \cite{TippingDerivingclusteranalytic1999,MahalanabisApproximatingdistancesmixture2009}, this can be misleading when the data have an hierarchical element to them \cite{lawson2018tutorial}.

With CLARITY we are addressing scientific questions that relate to which similarity structures are present in two datasets. There are other scientific questions that might be asked. For example, Canonical Correlation Analysis (CCA) \cite{hotelling1936relations, seber2009multivariate} and related approaches can be applied on datasets with matched features, as in e.g. ecology \cite{ter1987analysis} and machine learning \cite{hardoon2004canonical, NIPS2017_7188}. CCA addresses the question of which features in one dataset are important for understanding another. Because of this focus on features, CCA cannot be used directly in any of the simulations or real datasets that we consider below. Qualitatively this is because the datasets can match perfectly if the number of features is higher than the number of subjects. 

\section{Results}
\label{sec:results}

\subsection{High level view of CLARITY for comparing data from different sources}
\label{sec:claritydef}

This section contains a high-level mathematical description of the sort of comparison CLARITY is useful for. Technical mathematics is left for the Methods, Section \ref{sec:methodsmath}, but at a high level, the key concepts are given in Box 1.

CLARITY allows comparison of arbitrary datasets for which the same set of $d$ subjects are observed.
It represents the similarity of a \emph{reference} dataset $Y_1$ non-parametrically using increasingly rich representations of \emph{complexity}  $k \le d$.  At each $k$ we learn a \emph{structure} $A_k$, which is a $d \times k$ matrix, and a \emph{relationship} $X^{(k)}$ between the structures, which is a $k \times k$ matrix. Learning is by minimising the squared error of the estimate $\hat{Y}_1$, which is the sum of the squared residuals.

\textbf{Example:} {\it In Figure \ref{fig:sim1}b $Y_1$ is a similarity. Figure \ref{fig:sim1}a treats $A_k$ as a clustering learned from the Reference Similarity $Y_1$ (Figure \ref{fig:sim1}b). The relationship $X^{(k)}$ is then the similarity between clusters, and the complexity is the number of clusters $k$.}

We make a \emph{structural comparison} between the reference $Y_1$ and the target $Y_2$, by keeping the \emph{same structure}  $A_k$ but fitting a new relationship $X_{2}^{(k)}$ to make a prediction $\hat{Y}_{2}^{(k)}$.
The procedure is:
%%%It seems that here it could be useful to explain how a (dis)similarity looks, say on a heat map. Referring to a Figure? This could be the first point that the reader gets confused about what they are looking for; If they are used to Mantel tests and Procrustes etc then they will be pre orientated towards matches, but perhaps new Fig 1 will do this?

\begin{enumerate}
\item[0] Construct (dis)\emph{similarity} matrices: both a reference $Y_1$ and a target $Y_2$.
\item[1] Learn structure: Learn  $\hat{Y}_{1}^{(k)}$ in terms of structure $A_k$ and relationship $X^{(k)}_1$ to best predict $Y_1$ for a range of complexities $k$ by minimising the total error, subject to constraints. %% Learn A and X from Y_1? $= A_k X^{(k)}_1 A_k^T$ 
\item[2] Predict conditional on structure:  Predict $Y_2$ using $\hat{Y}_{2}^{(k)}$ which uses $A_k$ at each complexity $k$. %%  $\hat{Y}_{2,k} = A_k X^{(k)}_2 A_k^T$ 
\item[3] Evaluate prediction: examine the residuals $R_{2}^{(k)} = \left(Y_2 -  \hat{Y}_{2}^{(k)} \right)$ as a function of $k$. Visually report residuals that are present for many $k$.
\end{enumerate}

\textbf{Example, continued:} {\it In Figure \ref{fig:sim1}a $A_k$ are clusterings learned from the reference similarity $Y_1$ with each of $k=1, \ldots, d$ clusters. We also learn the relationship $X^{(k)}_1$ i.e. similarity between clusters in that dataset. We then predict $Y_2$ from each set of clusters $A_k$, but learn a completely new relationship between clusters $X^{(k)}_2$. With few clusters, we have a bad representation of $Y_1$ whilst with many clusters we overfit it. We are therefore interested in subjects that are poorly explained for many intermediate clusterings, i.e. those with persistent residuals.}

The CLARITY model uses a range of \emph{complexities} $k$ to represent data. The full set of structures therefore quantifies a rich range of models. For example, it can be constrained to a hierarchical clustering (i.e. a tree), and in Section \ref{sec:sim} we show that if the data are tree-like, the structure can be interpreted in terms of a `soft tree' (Figure \ref{fig:sim2}), which can capture deviations from a strict tree model.

\textbf{Example, continued:} \emph{In Figure \ref{fig:sim1}c, persistencies at different complexities $k$ are plotted. Observe that at high $k$, residuals and consequently persistences largely perish. In (ii), we have the same clustering (i.e.~structure) but a different relationship, and in c(ii) the large decrease in persistences happens at $k=4$, i.e.~as soon as complexity $k$ reaches the true number of clusters in the data. In contrast, in c(iii), the data are generated under a clustering that differs by one subject from the structure (i.e.~the clustering) from the first dataset. In this condition, persistences remain high until much higher complexity, especially so for the anomalous subject $i$. }

\begin{mybox}
 \label{box:1}
 {\bf Box 1: Important concepts in CLARITY}
    
 {\bf Similarity} $Y$: a $d \times d$ matrix comparing all subjects, for which `closer' subjects have large pairwise values. A typical example is a covariance. CLARITY operates entirely equivalently with dissimilarities for which `close' implies small values; a typical example is the Euclidean distance.
 
 {\bf Structure} $A_k$: A $d \times k$ matrix, providing a representation of each subject in $k$ dimensions.  The choice of \emph{structure} is the most important choice in CLARITY. It is learned subject to constraints on $A$, conditional on the relationship. Three important Structures are a) a clustering, with entries of $A$ being $0$ except when subject $i$ is in cluster $k$ in which case $A_{ik}=1$. b) A Mixture: rows of $A_{ij}$ sum to 1 and $A_{ij}$ are non-negative. c) An unconstrained subspace, in which case $A$ is the top $k$ eigenvectors, as seen in Spectral methods (Singular Value Decomposition, SVD and Principal Components Analysis, PCA).

{\bf Relationship} $X^{(k)}$: The \emph{relationship} between structures depends on the structure. We again learn it by minimizing the error conditional on the structure. In this paper we do not consider constraints. For a clustering, the relationship is the similarity between clusters, or the similarity between latent clusters in a mixture model. It describes the `branch lengths' of a tree. For PCA, the relationship is a rotation, translation and scaling of the matrix of singular values.

{\bf Structural comparison}: The \emph{structure} learned from the reference $Y_1$ at each complexity is used to predict the target $Y_2$. $Y_2$ may be numerically quite different if the relationships are different. However, as long as the datasets can be predicted in this sense then we say that the matrices are defined to be \emph{structurally similar}; this will happen if the same clusters, mixtures or eigenvalues are important in both datasets and describe the same subjects.

{\bf Residual persistence charts}: We use graphical summaries to present useful scientific insights, focussing on structures that persist over a range of model complexity. These are inspired by the concept of persistent homology from Topological Data Analysis \cite{WassermanTopologicalDataAnalysis2018}.  When the complexity $k$ is sufficiently high, every (full rank) dataset can predict every other, so the focus is on which structures in $Y_2$ are explained late in the sequence defined by $Y_1$, i.e. persist. This is captured by the residual persistence $P_{ik}$, a matrix whose entries for data subject $i$ at a complexity $k$, are the sum (over $j$) of the squared residuals. %$\sum_{j=1}^d\left(Y_{ij}-\hat{Y}_{ij}\right)^2$.
\end{mybox}

\begin{figure}[!ht]
\begin{center}
 \centerline{\includegraphics[width=1.0\columnwidth]{Fig2_Hierarchy}}
%% It is important to emphasise that changing some members of a cluster changes the rest of the cluster, and hence every similarity with that cluster. %%% BUT SEE FIG XXX?
 \caption{
 {\bf What is complexity in CLARITY?}
 CLARITY uses a non-parametric representation that captures a wide class of model, which is interpretable if the truth is interpretable. This is demonstrated with a hierarchical simulated dataset (see Section \ref{sec:methodssim}).
   a) Subjects are generated as belonging to a cluster under a `true structure'. Each feature is shared (with noise) with all descendent subjects in the tree, so may represent any branch of the tree, each of which is assigned a colour.
   CLARITY models the Similarity $Y$ between samples. 
   b) In such a hierarchical dataset, an inferred mixture Structure at each complexity $k$ relates to a `soft hierarchy', i.e. a set of mixtures $A_k$ with $k$ components whose latent clusters approximately represent the branches of the tree. Adding components explains different branches, until eventually only noise is explained. (Parameters: $d=100$, true $k=10$, $\sigma=0.005$.)}
\label{fig:sim2}
\end{center}
\end{figure}

{\bf Key decisions when using CLARITY}: There are just two substantive choices to make when running CLARITY. First, how to construct the similarity matrices $Y_1$ and $Y_2$, which may be determined by the data. Second, the choice of structure $A_k$, for which the two main approaches we provide are SVD, which is computationally convenient, or a mixture model, which is more interpretable. We have not found a case for using clusters in practice, as they are not robust. There is one final technical choice, which is the statistical procedure for assessing significance, which we address in Section \ref{sec:pvals}. Good practice for all choices is discussed in Section \ref{sec:bestpractice}. 


\subsection{Learning Structure in CLARITY}
Two similarity matrices are structurally similar if one can be predicted from the other, using the partial representation we have defined as \emph{structure}.  CLARITY is comparing similarity matrices $Y$ which requires a \emph{quadratic} model (in $A$) rather than the more familiar linear model:
$$\hat{Y} = A X A^T,$$
where $A$ and $X$ are intended to be `simpler' (concretely, $k$-dimensional) approximations to $Y$.

\textbf{Example, continued:}  {\it When $A$ is a clustering, then rows of $A$ are subjects and columns are clusters. Row $i$ of $A$ is a vector with value $1$ if subject $i$ is in cluster $j$. We then predict the similarity $Y_{ij}$ with an estimate $\hat{Y}_{ij}$ between subject $i$ and $j$ by finding which cluster $c_i$ and $c_j$ each is in, and replacing it with the similarity between those clusters, $X(c_i,c_j)$.}

We always seek to minimise a loss $L(A,X)$ which is the (squared) error. Setting $R(i,j) = Y(i,j) - \hat{Y}(i,j)$, the loss is:
$$L(A,X) = \sum_{i=1}^d \sum_{j=1}^d R(i,j)^2.$$

If this minimisation is unconstrained, then (see Methods Section \ref{sec:methodsmath}) $A$ is the matrix of the (first $k$) eigenvectors and $X$ the diagonal matrix of (first $k$) singular values of $Y$. $Y_2$ is structurally similar to $Y_1$ if it can be predicted using this learned $A$ and a new $X_2$. Technically, if $Y_2$ is poorly predicted at complexity $k$ then it is not close to the subspace spanned by the first $k$ principal components of variation in $Y_1$.

When $A$ is constrained to be a mixture (that is, its elements are non-negative and sum to one), $X$ describes the similarity between `latent clusters', and the rows of $A$ describe mixtures between these clusters. Similarity matrices are hence `structurally similar' if they can be described by the same mixture.
This mixture model is interpretable, as we demonstrate in the simulation study. Specifically, a `structural difference' at complexity $k$ means that the latent clusters of $Y_2$ are not in the $k$ most important latent clusters of $Y_1$. Further, the subjects that are poorly predicted, i.e. whose cluster membership is not captured, can be read off from the residual matrix.

Persistences and residuals decrease with model complexity and are affected by correlations between similarities.
%We can determine whether an observed dataset is predicted `significantly poorly' by resampling the original data to obtain samples of $Y_2$ under the null hypothesis, and comparing the observed $Y_2$ (See Methods Section \ref{sec:pvals}). The null hypothesis is that both the structure and relationship are the same but as complexity $k$ increases, CLARITY is powered to detect the alternative hypothesis that the structure is different. Whilst significance is important, the true value of the CLARITY approach is to provide an understanding of how the two datasets differ.
Despite the complexities of working with a similarity matrix, in Theorem \ref{thm:SVDperturb} (Methods Section \ref{sec:math}) we prove that the model is stable in the presence of noise, so that if two datasets were resampled then their residuals based on our notion of structure are not expected to change by a large amount. The theoretical and simulation results together demonstrate that the CLARITY paradigm is performing a meaningful comparison.

\subsection{Structure of the Examples}

We now present use cases. First we consider a simulation study for a mixture of trees model in Section \ref{sec:sim}, to demonstrate how CLARITY can be used to identify differences in large-scale structure. In Section \ref{sec:epigenetics} we consider simulated data based on methylation and gene expression data to give an example in which CLARITY can be used for anomaly detection. Section \ref{sec:languagechange}  is a real-data example from linguistics, which is smaller in scale and therefore more subtle in interpretation. Finally, Section \ref{sec:culture} examines the relationship between culture and economics at the country-scale.
% 'I put give' instead of 'given', Phil
The take-home messages from these examples are summarised in Section \ref{sec:bestpractice}.

\subsection{Comparing Simulated Hierarchical Mixtures}
\label{sec:sim}
Hierarchical data is common and naturally interpretable using CLARITY. In this section we simulate subjects related by a tree and insert an interpretable structural difference between two datasets. The relationship between structures includes features such as the branch lengths of the tree. The structure itself is defined by the membership of subjects in the clusters. Both are detectable with CLARITY but changing structure creates a much larger effect in the data.

\subsubsection{Simulation model}

The model creates data that is generated with $N=100$ subjects observed at $L=2000$ features, grouped into $k=10$ clusters related via a tree. This data is used as a reference for a CLARITY model.  In \emph{Scenario A} we construct a target by regenerating the tree with the same topology but altering the branch lengths, and resimulating data. In \emph{Scenario B}  we construct a target with the branch lengths changed as in Scenario A, but additionally change the structure $A$. See Section \ref{sec:methodssim} for details.

\subsubsection{Hierarchical Mixture Inference}

%% Simple simulation, one dataset only
Figure \ref{fig:sim1} shows that CLARITY is insensitive to changes in the heatmaps themselves, but remains sensitive to changes in structure.
Figure \ref{fig:sim2} shows how the CLARITY mixture model infers detailed structure of $A_k$ capturing the clusters present in the data when the tree is `cut' at different heights.
Figure \ref{fig:simexplain} illustrates how this is achieved.
%This is a very nice addition to the text! Should we make Fig 1 showed rather than shows ditto for Fig 2? Phil

When we use the $Y_1$ structure (Figure \ref{fig:simexplain}a) for prediction of the second similarity matrix $Y_2$ (Figure \ref{fig:simexplain}c,e), several situations may occur. In Scenario A-1, the trees share the same split ordering, and any differences will be completely absorbed by differences between $X_1$ and $X_2$. The residuals and persistences of $Y_2$  will be distributed as for  $Y_1$ (see Section \ref{sec:pvals} for how this is estimated). 
In scenario A-2, the trees share the same topology but the split order differs. The structures in $Y_2$ may not appear in exactly the same order as in $Y_1$ but still all appear in the top $k<k_{max}=10$ structures representing the tree.
The residuals and persistences may be larger at lower complexity, as happens in Figure \ref{fig:simexplain}c-d and Figure \ref{fig:sim1}c, but the entire difference can be explained at some complexity threshold. 
Things are different in Scenario B when $Y_2$ has a different topology to $Y_1$ -- perhaps containing mixtures as in Figure \ref{fig:simexplain}e-f or new clusters, such as Figure \ref{fig:sim1}a. Only then will important structure be absent until much higher $k$ and this will result in high \emph{and persistent} residuals for the affected data (Figure \ref{fig:simexplain}c, Figure \ref{fig:sim1}iii).

%% Complex simulation, detection of structural differences

%Figure \ref{fig:simexplain} shows that residual persistence can identify structural differences in a simulation of subjects from 10 clusters ($t1-t10$), related by a tree. Figure \ref{fig:simexplain}a-f) illustrates the residuals and persistence and highlights statistically significant differences. Figure \ref{fig:simexplain}c-d) describes Scenario A in which structure was not changed; the residuals are similar to in the learned dataset (Figure \ref{fig:simexplain}b). Figure \ref{fig:simexplain}e-f) describes Scenario B in which structural changes lead to strong, persistent residuals.

The persistence $P$ in Figure~\ref{fig:simexplain}f identifies the clusters that are affected by the structural change: cluster group t2 has significantly inflated $P$. Examining the residuals themselves at a specific $k$, Figure~\ref{fig:simexplain}e identifies the two clusters affected, which have highest off-diagonal shared residuals. In addition, they show that the `recipient' cluster t2 has consistently high pairwise residuals. The `donor' cluster t9 does not have exceptional residuals overall, but does have the highest pairwise residual with the `recipient' t2.
Of note is that low-dimensional representations ($k=1,2$) are not helpful because there is high intrinsic variability (i.e. these persistences are large but not significant). We must have a `good enough model' of $Y_1$ before it is useful to understand $Y_2$.
%The persistence $P$ identifies the clusters that are affected, whilst examining the residuals themselves shows that the pair of clusters can be read from this: mixing t3 into t7 makes these clusters have the highest off-diagonal shared residual. 
%The within-cluster effect can be separated from the between-cluster effect by reference to the original similarity matrix, Figure \ref{fig:simexplain}a), allowing the clusters to be read directly from the residual matrix.

This interpretation is robustly replicated in simulations, as is shown in Figure \ref{fig:simexplain}g-h for 200 different mixture-of-tree simulations. Specifically, %%% XXXX
\begin{itemize}
\item Persistence is high in `recipient' clusters of $Y_2$ containing a mixture of two different signals of the structure found in $Y_1$.
\item Squared residuals of the recipient cluster are high with all clusters that are topologically close to affected subjects. This happens both under the original structure and the `new' structure in which the `recipient' and `donor' clusters are close.
\item Persistence for `donor' clusters is not exceptional, but they are identifiable from their very high residuals with the recipient cluster.
\end{itemize}
In this way, the residuals for tree-like data can be interpreted topologically by first identifying clusters of subjects that experience a high residual persistence. The source of the mixture can be identified from which clusters (that are dissimilar in the reference) have increased residuals with these subjects.

\begin{figure}[!htbp]
\begin{center}
 \centerline{\includegraphics[width=0.8\columnwidth]{Fig3_SimulationExplanation}}
 \caption{Interpreting residuals and persistences using simulated data, for $d=100$ subjects from $k=10$ clusters. Dissimilarities shown are averaged within clusters, whilst residuals and persistences are summed to create population values. a) Learned tree and dissimilarity matrix. b) Residual persistence chart for the learned data. c) Squared residuals and d) residual persistence, for new simulated data with the same structure as in a). e) Squared residuals and f) residual persistence, for simulated data with a different structure to a), for which some subjects in cluster t2 are a mixture with t9. For c-f) lack of significance at $p=0.01$ is illustrated by drawing a smaller rectangle. %% XXXXX
g-h) Replicated results averaged over 200 simulations. g) `excess persistence' which is the residual persistence of samples in the recipient cluster -- i.e. t2 in the tree in e) -- with the mean residual persistence of the other samples subtracted. h) Summed squared residuals for different parts of the residual matrix. Shown is the `recipient' compared to the `donor' -- t9 in  e) -- as well as the recipient compared to all non-donor samples, and the average residuals for all pairs of samples that were neither recipient nor donor. Simulation settings: $\sigma=0.05$, $\beta=0.5$, for which in `Different Structure', half of the recipient cluster is affected by the mixture.}
\label{fig:simexplain}
\end{center}
\end{figure}

%% Interpretation

Whilst the Mixture model allows interpretation of how structural differences can be detected, both the Mixture model and SVD model make comparable predictions. Supp. Figure 1 shows that the \emph{same} structural similarity information is learned from the SVD model as in the Mixture model. The models predict $Y_1$ with near identical performance. Further, they both agree that the presence of different structure leads to poorer prediction of $Y_2$ from $Y_1$ for a wide range of $k$.

In terms of computational complexity, the SVD method is dominated by the SVD ($O(d^3)$). For reference, it takes 6.75 minutes to run our SVD model for $d=5000$ on a personal laptop, most of which is computing the SVD. The mixture model is dominated by a $d \times d$ matrix inversion ($O(d^3)$ or better) but is in practice slower as the convergence time of the iterative algorithm scales with $d$.

\subsection{Comparing Simulated Gene Methylation to Gene Expression}
\label{sec:epigenetics}

This example focusses on identifying \emph{anomalies}: subjects (here loci) that behave differently in one dataset compared to another. The CLARITY framing using a similarity matrix means that the datasets need not have features in common.

It is very common in epigenetics to wish to compare different measurements on genes. These are often measured on different scales --- methylation is a proportion whilst expression is a positive number --- and writing a formal model is hard. Further, the raw data may not be available to each researcher as they are identifiable and sensitive. It is therefore very helpful if summaries, such as those based on similarities, can be compared.

To emphasise the utility of CLARITY we focus on an epigenetic simulation model from the literature \cite{gu2016complex}, in which Methylation and Expression data are generated from \emph{independent} case-control experiments that describe the same set of genetic loci, i.e. positions in the genome, that correspond to known genes. These loci are our subjects.
We do not claim that this is a realistic model, only that it has received attention.

Methylation is a chemical process that reduces the accessibility of DNA for transcription, and is therefore negatively associated with gene expression. It can be measured using high-throughput arrays (e.g. \cite{bibikova_high_2011,min_genomic_2020}) that target known methylation loci. Similarly, gene expression can be measured using high-throughput arrays by capturing the transcribed RNA \cite{lockhart_genomics_2000}. However, both approaches are subject to a variety of noise, including inter-cell and inter-subject variability, high stochasticity in measurement from the amplification process, and so on. This variability adds to signal from causative factors of interest. The simulation therefore assumes only a -5\% average correlation.

The simulation is for a case-control study in which some people were controls, and others had a tumor. Methylation is simulated in different classes (`hyper' and `hypo' methylated loci) which interact with case-control status. Expression is simulated conditional on methylation, with large and random noise. Finally, anomalous loci are chosen in which the relationship between methylation and expression is reversed. See Methods \ref{sec:methysim} for full details.


The important feature of this setup is that the set of people considered to quantify methylation and expression are independent. The only thing they share in common is the set of loci on which data are reported. Figure \ref{fig:methy} illustrates this dataset and highlights the ability of CLARITY to extract the anomalous loci, despite the considerable structure in this dataset.

\begin{figure}[htb]
\begin{center}
  \centerline{\includegraphics[width=1.0\columnwidth]{Fig4_Methylation}}
  \caption{
    A simulated Methylation/Expression heatmap comparison from \cite{gu2016complex} with added anomalies (see Methods \ref{sec:methysim}).
   a) A reference dataset is used to learn structure; here, simulated Methylation patterns across the genome.
   b) The structure is used to predict the similarities in the target dataset; here, gene expression data at the same loci, with inserted anomalies. The data need not describe the same features (here, samples with Tumor/Control status).
   c) \emph{Persistent residuals} over a range of $l$ indicate which subjects (here, loci) are structurally different between the target data and the reference data, accurately identifying the anomalies.
  }
  \label{fig:methy}
\end{center}
\end{figure}


Figure \ref{fig:methy} shows the two input datasets $Y_1$ and $Y_2$, which contain some clear visual structures, and specifically loci that differ between Tumor and Control. It then shows the \emph{persistent residuals} as estimated by CLARITY. A subset of these are `anomalies', one cluster of which (top) has lower expression than expected in Tumor samples, the other has lower expression in Controls. Both are clearly highlighted via the Persistence chart (Figure \ref{fig:methy}c) over a range of $k$, even though the changes are individually small.
%change to lower case anomalies? To be consistent with earlier usage...and that in following section....

\subsection{Two types of language change}
\label{sec:languagechange}

In this example, we also search for \emph{anomalies}, but in contrast to the previous case, our finding here is both surprising and scientifically significant. It therefore merits explicit hypothesis testing, as well as a number of further checks making sure that the effect we find is not spurious. We compare two different, but non-independent types of language change: phonetic, or sound, change; and lexical, or word-replacement, change. There is no question that sound (i.e. phonetic) change and word-replacement (i.e. lexical) change are correlated because they result from a shared historical process: both types of change are inherent to the transmission of language from one generation of speakers to the next. There are two general sources for both phonetic and lexical change. First, each language changes on its own as time proceeds, even in complete isolation from external influences \cite{Trask-Millar-2015}. Second, languages can influence each other when there are multilingual people, with this process being called \emph{language contact} \cite{Matras-2009}. 


In the linguistic literature, it is often argued qualitatively, that phonetic and lexical change can be unevenly favoured by different social situations of language transmission and contact, see e.g.~\cite{Thomason_Kaufmann_1988}. Here, we use CLARITY to provide a quantitative test, the first such experiment known to us. Figure~\ref{fig:linguistic}a-b, demonstrates that the two change types induce  closely aligned similarities between language. While this is not surprising, given that the same factors influence both types of linguistic change, we show that, despite this high degree of correlation, there are still detectable `structural' differences between lexical and phonetic changes. Two components of our comparison are crucial: (i) we use a large dataset that enables us to apply CLARITY significance testing and thus derive a statistically credible result; (ii) allowing the CLARITY `relationship' to differ for phonetic and lexical, we effectively permit the rates of both types of change to differ considerably without the matrices becoming structurally dissimilar; in other words, the null hypothesis of CLARITY is broad enough that its rejection would be informative. 

The true history of both phonetic and lexical change is unknown, but can be conceptualised as a graph that consists of a ``vertical'' inheritance via a tree that captures independent change, and ``horizontal'' edges that capture change through language contact. Such a graph induces a similarity matrix between languages. Many graphs may induce the same matrix, making direct inference of the history graph impossible in the general case, Figure~\ref{fig:indistinguishability}a-b. But the similarity matrix does allow us to distinguish between different \emph{classes} of graphs, cf.~Figure~\ref{fig:indistinguishability}a-b and Figure~\ref{fig:indistinguishability}c. With CLARITY, we can infer changes to the structure of the underlying graph without explicitly learning that graph. With two matrices representing similarity due to phonetic vs.~lexical change, we can use CLARITY to find out whether phonetic and lexical change go hand in hand. Our null hypothesis is that lexical and phonetic change are aligned, because they ultimately stem from the same interactions between and within speech communities. It is the rejection of this null hypothesis that would be scientifically interesting. This is an appropriate setup for applying CLARITY, which looks for evidence of differences between two (dis)similarity matrices.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Fig5_indistinguishable.pdf}

 \caption{Some history-of-change graphs are not distinguishable from dissimilarity matrices, and others are. Let each of A, B, and C be a feature descending from the root node, with a constant rate of change. Then the probability of a mismatch between X and Y, $p_{X\neq Y}$, is a weighted sum of terms $1-e^{-t}$, where $t$ is the length of a path between $X$ and $Y$ in the graph, and the weights are given by the probabilities a given path was taken, determined by admixture proportions $\alpha$ and $\beta$ in b) and c).  Graphs a) and b) differ, but with specific values for the times of splits and $\alpha$ can lead to exactly the same probabilities of mismatch, and thus the same dissimilarity matrices. Graph c), in contrast, leads to a different dissimilarity matrix over any choice of split times and $\beta$ as long as $\beta$ is not 0. 
 \label{fig:indistinguishability}}
\end{center}
\end{figure}

In the limit of an infinite number of linguistic features, there exists a ``true'' matrix induced by the history-of-change graph. But in practice we have to work with  matrices estimated from a finite amount of data. To achieve a reasonable estimate, we need many individual features, which in practice requires automatic methods for inferring both phonetic and lexical similarity. Automatic methods incur an inherent error at the level of individual features, but processing many features results in reasonably stable estimates of similarity matrices. Specifically, in cross-validation, we show that however we divide our features into two halves, the halves can predict each other with great success, i.e. carry very similar information.

Furthermore, an important technical detail is that our automatic lexical-similarity recognition operates on word-to-word phonetic similarity scores. This dependence amplifies the correlation between the lexical and phonetic similarities, thereby working in favour of the null hypothesis. This way, we can be confident that a rejection of the null hypothesis would correspond to a true real-world difference in `structure'. Finally, automatic cognate detection cannot distinguish between cognates inherited from the last common ancestor and words that have been borrowed between sister branches after divergence. For recovering the true tree part of the language-family history, this is a drawback; for us, this is a benefit, as it captures language contact relations in the lexical-change data, just as it affects the phonetic-change data.

Our data come from one of the largest existing historical-linguistic datasets, NorthEuraLex v0.9 \cite{NorthEuraLex}, which stores phonetic transcriptions of words expressing 1016 different meanings in over a hundred languages. We focus on the 36 Indo-European languages in NorthEuraLex, for which we computed measures of both phonetic and lexical dissimilarity using a state-of-the-art method \cite{Dellert-2018}, as discussed in more detail in Sec.~\ref{sec:methodsling}. 

\begin{figure}[!ht]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{c|ccc|ccc|ccc}
\multicolumn{10}{c}{\bf \scalebox{1.25}{(a)}}\\[6pt]
\bf Meaning	& \bf EN 	&\bf DA 	&\bf DE 	&\bf ph:EN-DA	&\bf ph:EN-DE 	&\bf ph:DA-DE 	&\bf le:EN-DA	&\bf le:EN-DE	&\bf le:DA-DE\\\hline
BEARD	& beard	& sk{\ae}g	& Bart	& 0.27	& 0.523	& 0.27	& no	& yes	& no\\
HEAD	& head	& hoved	& Kopf	& 0.541	& 0.068	& 0.34	& yes	& no	& no\\
GREEN	& green	& gr\o{}n	& gr\"{u}n	& 0.436	& 0.553	& 0.687	& yes	& yes	& yes\\
DOG	& dog	& hund	& Hund	& 0.234	& 0.119	& 0.697	& no	& no	& yes\\
FIRE	& fire	& ild	& Feuer	& 0.179	& 0.539	& 0.166	& no	& yes	& no\\
COME	& come	& komme	& kommen	& 0.725	& 0.585	& 0.712	& yes	& yes	& yes\\
THROW	& throw	& kaste	& werfen	& 0.232	& 0.231	& 0.182	& no	& no	& no\\
SLEEP	& sleep	& sove	& schlafen	& 0.277	& 0.338	& 0.288	& no	& yes	& no\\
DRINK	& drink	& drikke	& trinken	& 0.476	& 0.566	& 0.491	& yes	& yes	& yes\\
TAKE	& take	& tage	& nehmen	& 0.401	& 0.302	& 0.183	& yes	& no	& no\\
\end{tabular}
}
\\[0.2in]
\begin{tabular}{c|ccc}
\multicolumn{4}{c}{\bf (b) phonetic from 10 meanings}\\[4pt]
phon & EN & DA & DE \\\hline
ENG	& 1.000	& 0.377	& 0.382 \\
DAN	& 0.377	& 1.000	& 0.402 \\
DEU	& 0.382	& 0.402	& 1.000\\
\end{tabular}
\hspace{0.4in} 
\begin{tabular}{c|ccc}
\multicolumn{4}{c}{\bf (c) lexical from 10 meanings}\\[4pt]
lex & EN & DA & DE \\\hline
EN &	1.000	& 0.500	& 0.600 \\
DA &	0.500	& 1.000	& 0.400 \\
DE &	0.600	& 0.400	& 1.000 \\
\end{tabular}
\\[0.2in]
\begin{tabular}{c|ccc}
\multicolumn{4}{c}{\bf (d) phonetic from full data}\\[4pt]
phon & EN & DA & DE \\\hline
EN	& 1.000	& 0.312	& 0.318 \\
DA	& 0.312	& 1.000	& 0.458 \\
DE	& 0.318	& 0.458	& 1.000 \\
\end{tabular}
\hspace{0.4in} 
\begin{tabular}{c|ccc}
\multicolumn{4}{c}{\bf (e) lexical from full data}\\[4pt]
lex & EN & DA & DE \\\hline
EN	& 1.000	& 0.399	& 0.415 \\
DA	& 0.399	& 1.000	& 0.593 \\
DE	& 0.415	& 0.593	& 1.000 \\
\end{tabular}
\caption{{\bf (a)}: Phonetic and lexical word-to-word similarities for 10 meanings in English (EN), Danish (DA) and German (DE). Columns \textbf{ph:A-B} list inferred phonetic similarity between languages A and B accounting for regular sound correspondences. Columns \textbf{le:A-B} list word-to-word lexical similarity, namely cognacy (i.e.~descending from the same ancestral word), inferred via clustering based on phonetic similarity scores as in \textbf{ph:A-B}. See Sec.~\ref{sec:methodsling} for details of the measures. {\bf (b),(c)}: The phonetic and lexical similarity matrices induced by the data in (a). {\bf (d), (e)}: Same, induced by the full data.}
\label{fig:ling-ex-tables}
\end{center}
\end{figure}

Tables in Figure~\ref{fig:ling-ex-tables}  illustrate how the phonetic and lexical similarities work in practice, using 10 meanings and words for their expression in English, Danish and German. Historical linguists have established that English and German have a more recent common ancestor than either has with Danish (i.e.~in the vertical `backbone tree'). After their respective divergences, the three languages experienced complex language-contact patterns, which can be conceptualised as horizontal edges in their historical graphs.

We discuss three rows in Figure~\ref{fig:ling-ex-tables} to illustrate the individual-feature diversity behind the general similarities. The meaning BEARD in Figure~\ref{fig:ling-ex-tables}a is straightforward. The English and German words descend from the same ancestral word, experiencing no lexical-replacement events. In linguistic parlance, these words are cognates. This common ancestry means that their phonetic shape descends from one and the same shape of the ancestral word, and thus causes them to be phonetically similar in the real world. Our phonetic-similarity algorithm correctly recovers that information (column ph:EN-DE), and based on this high level of similarity, our lexical cognate-detection algorithm also declares them lexically similar (``yes'', or 1, in le:EN-DE). The Danish word for BEARD is historically unrelated, and is indeed not similar to either German or English word phonetically. For the meaning HEAD, it is the English and Danish words that are true cognates, and are inferred to be both phonetically and lexically similar by the algorithms. But the German word for HEAD is accidentally much closer phonetically to the Danish word than to the English one. The more the phonetic systems of two languages are alike, the more frequent and pronounced such accidental similarities of unrelated words will be on average. Finally, the meaning GREEN illustrates a different property of phonetic similarity. For GREEN, all three languages use cognate words, and are correctly inferred to do so by the lexical algorithm. But their phonetic similarities to each other differ. The English vowel in `green' is closer to that in German `gr\"{u}n'  than Danish `gr\o{}n'. For cognate words, phonetic similarity depends on how sound changes operated on what was initially one and the same word. 

Schematically, our automatically inferred similarity measures $Phon$ and $Lex$ can be characterised as follows. $Lex = TrueLex + \epsilon_{lex}$, where $TrueLex$ is the true lexical similarity, and $\epsilon_{lex}$ is the error from the automatic cognacy-detection algorithm. $Phon = TrueLex*unrelated.phon.sim + (1-TrueLex)*cognate.phon.sim + \epsilon_{phon}$, where we condition on whether the relevant words are related (with $TrueLex$ impressionistically viewed as probability) or not; $unrelated.phon.sim$ and $cognate.phon.sim$ are two types of phonetic word-to-word similarities discussed in the previous paragraph; and $\epsilon_{phon}$ the error of the phonetic algorithm. This informal representation shows how our empirical measures $Phon$ and $Lex$ are even more dependent than the real-world quantities $TrueLex$, $unrelated.phon.sim$ and $cognate.phon.sim$. In turn, the latter three are also dependent because they are the result of language-change processes in the same speaker communities. 

\begin{figure}[!htbp]
\begin{center}
 \centerline{\includegraphics[width=1\columnwidth]{Fig7-ling-example}}
 \caption{(a-b) Phonetic and Lexical similarity matrices. The three large orange-red clusters correspond, left to right, to the Slavic, Germanic, and Romance language subfamilies.
(c-d) persistence diagrams. Statistical significance for cells is visually highlighted by color saturation and by the size of the background rectangle. Compare the columns for \texttt{oss} (Ossetian) and \texttt{slv} (Slovenian) in (c). The column for \texttt{oss} has no significant cells. The column for \texttt{slv} has significant cells up to $k=23$ and non-significant ones higher than that. \label{fig:linguistic}}
\end{center}
\end{figure}

Figure \ref{fig:linguistic}a-b shows our similarity matrices for Phonetic and Lexical, which look very much alike. Figure \ref{fig:linguistic}c-d shows the CLARITY analysis, which uncovers structural differences between the matrices in (a-b). The differences between Phonetic and Lexical are  declared \emph{statistically significant} by the CLARITY hypothesis-testing procedure based on resampling. In the figure, significant persistences are marked by a brighter colour and a larger rectangle. They only concern one direction of prediction, namely Phon from Lex, and only some groups of languages; the most affected ones are the Slavic and Scandinavian subfamilies. The differences are also \emph{scientifically significant}, which we determined by analysing the residuals at individual $k$, shown in Supp. Fig. 2: outstanding residuals remained on the order of 0.1-0.2 s.d. (computed from all similarities in the matrix) in individual cells even at the highest $k$s. We believe this is a moderate, yet considerable difference. Our additional checks also included establishing that CLARITY decomposition captures signal rather than noise up to the maximal $k$ (Supp. Fig. 3); checking whether self-similarity affects inference (Supp. Fig. 4) examining significances at a stricter p=0.01 (Supp. Fig. 5) and examining the persistence curves for the resampled matrices to make sure there were no anomalies (Supp. Fig 6-7). We conclude that the effect CLARITY finds, captured in the visual summary in Figure~\ref{fig:linguistic}c-d, is a real one. 

This result obtained by CLARITY is striking because it is based on very subtle distinctions in the observed data. To the bare eye, the Phonetic and Lexical distance matrices Figure~\ref{fig:linguistic}a-b are quite similar, %, and they return high significance on Mantel's test \cite{Manteldetectiondiseaseclustering1967}, a formal measure of correlation for dissimilarity matrices.
because the two sets of features are not independent of each other. Despite high correlation, CLARITY allowed us to discover a clear difference between the two processes of language change. 

How should we interpret this finding? In terms of the informal representation above, our results about $Lex$ and $Phon$ indicates that the real-world quantities $TrueLex$ on the one hand, and $unrelated.phon.sim$ and $cognate.phon.sim$ on the other, are affected by subtly different historical processes. Given that $Phon$ depends on a superset of real-world quantities that $Lex$ depends on, it is not surprising that we only find the effect in the direction of predicting $Phon$ from $Lex$. The difference between the two is considerable, but is only discovered with statistical significance for groups of languages closely related to each other, such as the Slavic and Scandinavian languages. This might be a real-world phenomenon: perhaps in the long run, the phonetic and lexical change processes average out and start looking very similar). It could also be an artefact of our algorithms for estimating similarity: as all computational-historical-linguistic algorithms, they inherently tend to be more accurate for more closely related languages, so it might be that we  see significant mismatches between Phonetic and Lexical only where our estimates can be sharp enough. We leave solving this question to future research. 

We conclude this case study with the following strong thesis: qualitative linguistic research into small sets of linguistic features should refrain from generalising its results to the overall workings of different types of change. The statistical mismatch between Phonetic and Lexical that we found was subtle and required a large dataset to be discovered. 

\subsection{Predicting Culture from Economics}
\label{sec:culture}

In this example, we use the `structure' from economic properties of countries to predict cultural values. The purpose of this case study is to demonstrate how exploratory analysis with CLARITY can be performed on the level of individual anomalous residuals. 
Overall, country-level economics and culture similarity matrices are significantly different, as is to be expected. But examining the CLARITY residual structure, we identify particularly interesting anomalies. 


\begin{figure}[!htbp]
\begin{center}
 \centerline{\includegraphics[width=1.0\columnwidth]{Fig8_CvsVsWb}}
 \caption{Predicting economic properties  from cultural properties. (a)  The dissimilarity $Y_2$ for Culture. The order of countries is determined by clustering based on the dissimilarity, so countries that are close on the x-axis are, other things being equal, close to each other in the Culture dataset. (b)  Persistence of Culture predicted from Economics. The dashed line indicates the complexity $k$ used in (c). (c) Residuals for Culture predicted from Economics at $k=15$.}
\label{fig:culture}
\end{center}
\end{figure}

For Economics, the World Bank \cite{CIA2018} provides a range of  features primarily relating to wealth, inequality, trade and the like for over 200 countries. The World and European Values Surveys (WEVS) \cite{WVS2017, EVS2011} provides features on Culture - that is, people's attitudes and beliefs regarding topics like religion, prosociality, openness to out-groups, justifiability of homosexuality, political engagement and trust in national institutions. There are 104 countries shared between these data that represent our subjects for this case study. The `Cultural' dissimilarity matrix, Figure~\ref{fig:culture}a is constructed from a dimensionality-reduced dataset of nine cultural factors from WEVS from circa 2000CE, as described by \cite{ruck2018religious}. For `Economics', we retained  the 284 indicators of the World Bank dataset with less than 40\% missingness, standardised to unit variance, capped extreme values at 10 s.d.-s, mean imputed, and computed the `Economic' pairwise distance matrix.  The raw dissimilarity matrices are shown in Supp. Fig. 8.

Cultural values are known to predict economic outcomes such as GDP per capita \cite{Gorodnichenko2016, ruck2018religious}, economic inequality \cite{Nikolaev2017} and the balance of agriculture-industrial-service sectors within the economy \cite{Inglehart2005a}. Conversely, a country's economic performance predicts cultural factors such as religiosity \cite{sinding_bentzen_acts_2019} and book writing \cite{bentley_books_2014}. CLARITY does not presuppose a causal model and therefore the choice of reference and target should not be interpreted as a causal claim without additional information.

Because we have only $9$ features for Culture, it cannot be used to identify persistent residuals in Economics with CLARITY as the maximum complexity is $k_{max}=\mathrm{min}(k,d)=9$. Instead we ask whether Culture (Figure \ref{fig:culture}a) can be predicted from Economic data, in which $k_{max}=d=104$. The persistence chart (Figure \ref{fig:culture}b) makes it clear that Culture is incompletely predicted from Economics, as almost all Persistences are significant. %We therefore focus on the scale of this chart, which identifies the most interesting anomalies, and residuals.
%When interpreting the results, recall that any clustering observed is \emph{not} showing which countries cluster together in Culture or Economics, but which countries cluster together in Culture \emph{after accounting for Economics}. Therefore if we can see factors shared by such countries but not modelled by the Economic data, this identifies putative hypotheses for a confounding factor in the association.
Conversely, not all residuals are significant. The residual matrix at a specific $k$ (Figure~\ref{fig:culture}c) identifies the most important mismatches between Culture and Economics, which may be worthy of further study using other methods and/or data. There are two main classes: anomalies, and clusters.

One anomaly is Andorra, a small European country in the Pyrenees between Spain and France, which belongs culturally, to the cluster of Scandinavian and economically strong Western European countries, Figure~\ref{fig:culture}a. But predicting Culture from Economics in Figure~\ref{fig:culture}c, we see highly negative (blue) residuals between Andorra and its cultural cluster. (Remember that absence of significance simply means we do not have \emph{enough} evidence to reject the null hypothesis based on the available data.)  Examining the raw data in Supp. Fig. 8, shows Andorra to be an Economic outlier, clustering with other territories with complex sovereignty that may influence data gathering: Taiwan, the Turkish region of Cyprus, and Northern Ireland. %%These data are not missing in our data, but are on average biased towards the mean. %%% MAYBE DOWNPLAY THIS!?

Now consider Vietnam and Uzbekistan. Their column of residuals in Figure~\ref{fig:culture}c are all green, meaning they are farther away on Culture than expected from Economics from all (!) other countries. These two countries are the only ones with all-green residual columns in Figure~\ref{fig:culture}c. Examining the raw data in Figure~\ref{fig:culture}a and Supp. Fig 8, as well as the Economic PCs in Supp. Fig. 9, we see that they are not particularly close in either Culture or Economics to any other countries, but that there is little relation between which countries are closer in either. Future research may thus want to first confirm that the cultural unusualness is not spurious (based, e.g.,~on coding errors and the like in the datasets as processed), and second, to study what could be driving these anomalous profiles. 

There are countries that share residual structure. Countries in Latin America, such as Argentina, Uruguay and Puerto Rico, have large European descended populations \cite{CIA2018a, Putterman2010}, so are culturally similar to Europe because cultural values percolate along linguistic and religious pathways \cite{Matthews2013, Matthews2016a, Spolaore2013}. However, Latin American countries have a relatively smaller GDP and have high economic inequality; they are therefore more culturally similar to Europe than Economics predict.

The Cultural data contain associations that are perhaps surprising; for example, Japan is culturally similar to the Czech Republic. Correcting for Economics, Figure~\ref{fig:culture}c, reveals that this cannot be fully explained away by Economic similarity (the relevant cell is light blue, though not significant). In contrast, Poland and the Czech Republic are neighbouring countries that cluster very differently in Culture, and have large persistences. In this case, Economics may be playing an even smaller role than in the comparison Japan-Czech Republic: the residual Poland-Czech Republic is significant and shows they are much farther from each other on Culture than we could expect from Economics.  

To conclude, we have described how CLARITY analysis identifies interesting anomalies in one dataset that stand out when making predictions based on the other dataset. In this case, the prediction is poor, so  considerable follow up analysis would be needed to draw strong conclusions. The assessment of statistical significance was of secondary importance: we concentrated on systematic patterns in the CLARITY residuals that are likely to be of scientific significance, and the presence of statistically significant individual cells only confirmed the insights. This strategy is particularly useful for datasets based on a relatively small number of features, which will often be the case in social sciences: the statistical power of such data would be in the general case limited. 

\subsection{Summary of the examples}
\label{sec:summary-of-cases}

We summarise the lessons from the four case studies above. In Section~\ref{sec:sim}, we show how CLARITY behaves on data from a simulation where we purposefully manipulated the `structure' of the compared datasets. This case study builds intuitions about what CLARITY output would look like in different real-world scenarios. In another simulation study in Section~\ref{sec:epigenetics}, we demonstrate that CLARITY correctly identifies anomalous links between two datasets generated by an existing epigenetic model, that did not conform to our definition of Structure. In the linguistic study on real-world data in Section~\ref{sec:languagechange}, we report the first-of-its-kind quantitative finding that phonetic and lexical language change operates in subtly different ways, though both occur in the same human communities and are subject to similar constraining factors. This example is the only one  in this paper where explicit hypothesis testing is of primary importance. Finally, in the example on culture and economics in Section~\ref{sec:culture}, we illustrated how CLARITY residuals can be explored to detect interesting anomalies that can then be selected for further in-depth study. 

Overall, we intend this set of examples to show how CLARITY can be a versatile tool for both exploratory data analysis and for explicit hypothesis testing, allowing us to make subtle and fine-grained comparisons between paired datasets stemming from the same subjects, but consisting of very different features, sometimes generated within different scientific disciplines. In the next section, we further discuss best practice and provide advice for interpretation of CLARITY results. 

\section{Discussion}

\subsection{Best Practice}
\label{sec:bestpractice}

CLARITY is simple to use and has very few moving parts. There are however a few critical decisions:
\begin{enumerate}
\item designing the input (dis)similarity $Y$, for which the user should take care;
\item choosing a structure representation $A$, for which the defaults provide good performance;
\item whether to use null hypothesis statistical testing, to identify statistically significant deviations;
\item how to interpret the results.
\end{enumerate}

\subsubsection{How to choose a (dis)similarity}

CLARITY can work with rich input data $Y$. However, the simpler the measure, the more reliable the inference will be. Therefore, if you can compute a regular covariance or Euclidean distance, this will be more interpretable.

The \emph{self-similarity} should be correctly quantified. Broadly, this should mean, `if we removed any excess similarity that is unique to the subject, how similar would it be to itself?' We implemented a `diagonal removal' for this purpose, which sets the diagonal to the next-highest value. This is generally recommended, and was applied in the simulations, Methylation, and Culture examples. We also provide a (slower) iterative model in which $\hat{Y}=AX A^T+D$ is iteratively solved for a diagonal matrix $D$. This was used in the Linguistic example. We did not find any qualitative difference between the methods. Sensitivity analysis will confirm for users whether working with the raw similarity, or diagonal-corrected similarity, imapacts inference.

Whether to \emph{centre} the data is an important decision. Centring changes whether any difference in mean is used in the comparison, and is therefore a modelling decision. We only recommend centring if the mean is known apriori to be unimportant or misleading. We only centred the Culture/Economics data, which required feature standardisation. Again, we recommend either careful justification of the choice to centre, or sensitivity analysis to confirm it does not matter. Conversely, \emph{scaling} will typically not affect the inference because any change can be accounted for in $X$.

We note a clear distinction between two classes of data. The first, represented in the language example, is where every subject is distinct, or the total amount of data is `small'.  In this case, self-similarity correction is important as it affects $A$ closely. Further, persistences have relatively small ranges of $k$ to persist over. and therefore inference is subtle. Conversely, the second class has a `large' number of subjects, such as the Methylation example. In this case the effect of the diagonal becomes negligible as the data grows, as can be verified in a sensitivity analysis.

\subsubsection{How to choose the structure}

In general, the unconstrained SVD-based structure is recommended; it scales to large data, and provides the best overall fit. Although the representation of structure $A$ is a linear embedding using only the SVD, it is flexible in prediction of $Y_2$ due to using $k^2$ free parameters in $X_2$.

We only recommend the mixture model when the data are small, and suspected to lie very close to some interpretable model such as a tree containing mixtures. The residuals should be close to identical between the methods, with the important difference being the interpretation of $A$. We note that unlike in many mixture model inference methods, there is nothing in our loss function that encourages $A$ to be close to the boundaries (i.e. some $A_{ij}$ are close to $0$ or $1$), except the initial conditions. This feature should therefore be used with further validation, as part of the hypothesis generation process.

\subsubsection{Whether to perform null hypothesis statistical testing}

CLARITY is in general a hypothesis generating tool and much of the value can be obtained without any use of statistical testing. For many datasets, statistical power will dominate (such as the Culture example) and many persistences are expected to be significant. We then care about practical significance and hypothesis testing is somewhat spurious. Similarly, in the Methylation example, clear persistent residuals were observed and these require no testing to confirm anomalies.

Conversely, especially with small datasets, statistical power is limiting (such as the Language example), in which case we wish to check that a given persistence is significant. The statistical test that we provide compares $Y_2$ to the distribution of left-out data from $Y_1$. It does so by first mean centering and scaling each dataset \emph{to the reference}, and then applying the standard CLARITY correction of learning independent Relationships. This encodes an implicit assumption that the signal and noise are of the same scale, and therefore should only be used if this is plausible.

Null hypothesis testing is a non-trivial process to implement because it requires that the user is able to either create independent features, or able to provide a set of bootstrapped samples generated on pseudo-independent samples, for both the reference and the test dataset. This comes with a computational cost, as of the order of 200 seudo-independent replicates are required to confidently reject at the 0.05 level.

We emphasise that the principal use case is the identification of large persistent residuals which CLARITY will only create where structure has changed, and does not require testing.

\subsubsection{How to interpret persistences and residuals}

We have noted that high residuals at a single complexity $k$ might capture only a small change in the importance of a particular relationship, such as two clusters getting further apart. Conversely, CLARITY is easiest to interpret when residuals are large and persist across a range of complexities. This will typically highlight structural anomalies in certain subjects.

There are two phenomena that require care. The first is that  the sum of squared residuals are not monotonic for a particular subject (but are overall). For example, Figure \ref{fig:sim1}c(iii) shows residuals increasing for subject i up to complexity 3, and then decreasing; similarly, subject l has high residual at complexity 12-13. This occurs when the change to the structure fits other, nearby but different, subjects which can `drag' that subject away from its target.

The second important phenomenon is that residuals need not be largest in the subject that has changed structure. Because the inferred relationship $X_2$ is chosen to minimise the total squared residuals, the model may make nearby subjects have the highest residuals. The important thing to note is that this still creates some residual excess in the target subject, which can be identified by looking at the matrix of residuals (e.g. Figure \ref{fig:culture}c).

\subsection{Conclusion}
\label{sec:discussion}

CLARITY can be applied to any pair of datasets in which subjects are matched and so can be used in a wide variety of situations. We demonstrated this in very different fields: epigenetics, linguistics and bridging sociology and economics. In these examples we have recovered differences, supported by well-documented evidence, and generated new hypotheses.

The software requires very little technical knowledge to employ, there are no tuning parameters, and the output can be presented in a simple, interpretable chart we called the residual persistence. This identifies the clusters and subjects that are poorly predicted, and allows interpretation of which other clusters they may share additional structure with. We suggest that the same approach may yield valuable insights when applied to other fields of interest, and that the results will generate hypotheses for further investigation through the application of additional, statistically robust, methods.

CLARITY is fast, and, for prediction, is limited only by the cost of computing a Singular Value Decomposition. We showed via simulation that the SVD approach is representing the structure in the data very similarly to a mixture model, for which we presented a novel algorithm based on a multiplicative update rule. The mixture model correctly identifies hierarchical structure, clusters and mixtures when these are present in the data and so permits the interrogation of why a particular prediction may have been made. 
%Did we present a multiplicative update rule at this point? Actually not until the Methods. Should we adjust to mention this earlier or do we want to focus people on the SVD? Phil
We were unable to find tools that were able to perform an analogous structural comparison and, therefore, have not performed statistical recall and efficiency benchmarking. Whilst we could have run the models listed in the introduction, they have different null hypotheses and purposes. Some provide qualitatively different information to CLARITY, while others test for equality of the similarities, which is an implausible null hypothesis for our examples. Whilst CLARITY is currently performing a unique function in terms of information extraction from complex data, we anticipate that the problem may be addressed in other ways, and that the insights which can be automatically extracted can be extended.
%"We could not find tools that can perform an analogous structural comparison"? Phil

\section{Methods}

\label{sec:methods}

\subsection{Notation} The notation we use is largely standard. Matrices are denoted by upper case letters. The set of all $d \times k$ matrices with real entries is denoted by $\R^{d \times k}$. If $Y \in \R^{d \times k}$ is a matrix, its $(i,j)$-entry is denoted by $Y_{ij}$. The quantity $\norm{Y}_{F}$ denotes the Frobenius norm of $Y$, i.e.
\[ \norm{Y}_{F} := \left(\sum_{i = 1}^{d} \sum_{j = 1}^{k} Y_{ij}^{2} \right)^{1/2}.\] 

\subsection{Structural Representation}
\label{sec:methodsmath}
In general, CLARITY can be applied to any pairs of matrices. In practice, the utility of the subsequent results does depend on how the matrix was constructed and the `best practice' input is likely to be a distance matrix corresponding to a reasonable metric, such as Euclidean. In limited experimentation, asymmetry does not appear to be a major problem but the class of matrices we can prove that CLARITY is sensible for is the following.

A dissimilarity matrix is defined to be any symmetric matrix $Y \in \R^{d \times d}$ of full rank consisting of non-negative entries. Let $Y_{1}$ and $Y_{2}$ be a pair of dissimilarity matrices in $\R^{d \times d}$. For each natural number $k \leq d$, we initially seek matrices $A_{k} \in \R^{d \times k}$ and $X_{1}^{(k)} \in \R^{k \times k}$ such that the quantity
\[ \norm{Y_{1} - A_{k}X_{1}^{(k)}A_{k}^{T}}_{F} \] is minimised. Note that the squared error discussed in the text is the squared Frobenius norm and is minimised at the same $A$ and $X$.

The product $A_{k} X_{1}^{(k)} A_{k}^{T}$ is to be viewed as the best rank $k$ approximation of $Y_{1}$ in Frobenius norm subject to whatever constraints may be placed on both $A_{k}$ and $X^{(k)}_{1}$ and it affords a structural reduction of $Y_{1}$ at dimension $k$ as motivated by the following proposition.

\begin{prop} 
\label{prop:minproj} Let $Y \in \R^{d \times d}$ be a dissimilarity matrix and let $(A, X)$ be a pair of matrices such that 
\[ \norm{Y - AXA^{T}}_{F} \] is minimised, where $A$ has full column rank. Then 
\[ X = P_{A}YP_{A} \] where $P_{A}$ denotes the orthogonal projection operator onto $\im(A)$. 
\end{prop} 

\begin{proof} Define the objective function 
\[ \curly{L}(A, X) := \frac{1}{2} \norm{Y - AXA^{T}}^{2}_{F} \] Taking matrix derivatives with respect to $X$ gives the condition
\[ A^{T}(AXA^{T} - Y)A  = 0 \] at a critical point $(A, X)$. If $A$ has full column rank, the matrix $A^{T}A$ is invertible and it is possible to solve for $X$ by
\[ X = A^{+}Y(A^{+})^{T} \] where $A^{+} := (A^{T}A)^{-1}A^{T}$ is the generalised (Moore-Penrose) inverse of $A$. Then
\[ AXA^{T} = AA^{+}Y(AA^{+})^{T} = P_{A}YP_{A} \]
\end{proof}

Given the above structural reduction of $Y_{1}$, we seek to find the extent to which it is capable of predicting the matrix $Y_{2}$. To this end, we find a matrix $X_{2}^{(k)}$ such that 
\[ \norm{Y_{2} - A_{k}X_{2}^{(k)}A_{k}^{T}}_{F} \] is minimised and we examine both the residual matrix 
\[ Y_{2} - A_{k}X_{2}^{(k)}A_{k}^{T} \] and element-wise norms of it. 

If $A_{k}$ has full column rank, the argument in Proposition \ref{prop:minproj} gives that $X_{2}^{(k)} = P_{A_{k}}Y_{2}P_{A_{k}}$ where $P_{A_{k}}$ denotes the orthogonal projection onto $\im(P_{A_{k}})$. 

\subsection{Learning structure}
\label{sec:learning}
We consider two methods that differ only in the manner in which the initial optimisation problem stated above is solved. Our \emph{SVD model} uses singular value decomposition to solve analytically for $A_{k}$ and $X^{(k)}_{1}$, and it is possible to do this precisely because these matrices are assumed to be unconstrained. Our \emph{Mixture model} constrains the matrix $A_{k}$ to have rows taken from a probability simplex (but does not constrain $X^{(k)}_{1}$), and an optimum is obtained numerically via an iterative procedure. 

\subsubsection{SVD-based solution}
Suppose that we have singular value decomposition 
\[ Y_{1} = \sum_{j = 1}^{d} \sigma_{j}u_{j}v_{j}^{T} \] where $\sigma_{j}$ denotes the $j$-largest singular value of $Y_{1}$. The matrix product $A_{k}X_{1}^{(k)}A_{k}^{T}$ can have rank at most $k$, and by the Eckart-Young theorem \cite{Eckartapproximationonematrix1936},
\[ \min_{Y': \rk(Y') \leq k} \norm{Y - Y'}_{F} = \norm{Y - \hat{Y}_{1,k}} \] where $\hat{Y}_{1,k}$ is defined to be the truncation of the SVD of $Y_{1}$ to its top $k$ singular values, i.e. 
\[ \hat{Y}_{1, k} := \sum_{j = 1}^{k} \sigma_{j}u_{j}v_{j}^{T}. \]
We set $A_{k} = [u_{1}|u_{2}| \dots| u_{k}]$. The matrix $X_{1}^{(k)}$ is then the top left-hand $k \times k$ block of $\Sigma_{1}$, and 
\[ X_{2}^{(k)} = A_{k}^{T}Y_{2}A_{k}. \] 

\subsubsection{Solution under a simplicial constraint}
We assume that the entries of $A_{k}$ are non-negative and that the rows of $A_{k}$ sum to $1$. This constraint is motivated by mixture modelling. A solution is sought via an iterative gradient descent procedure using multiplicative update rules based on the approach of \cite{lee_algorithms_2001-1}.

Specifically, we derive a multiplicative update rule for $A$ given $X$ and $Y_{1}$ and then solve for $X$ given $A$ and $Y_{1}$. These two steps are applied to convergence. This particular model does not appear to have been solved previously in the literature, and this solution is relatively efficient.

Given $X$ and $Y_{1}$, consider the objective function
\[ \curly{L}(A) := \frac{1}{2}\norm{Y_{1} - AXA^T}^{2}_{F}. \] 
If $A_{t}$ is the current estimate of $A$ at iteration $t$, taking matrix derivatives of $\curly{L}(A)$ leads to the update rule 
\[ (A_{t+1})_{ij} \leftarrow (A_{t})_{ij} \frac{(N^{A}_{t})_{ij}}{(D^{A}_{t})_{ij}} \] where 
\[ N^{A}_{t} = Y_{1}^{T}A_{t}X_{t} + Y_{1}A_{t}X_{t}^{T} \] and
\[ D^{A}_{t} = A_{t} X_{t} A_{t}^T A_{t} X_{t}^T + A_{t} X_{t}^T A_{t}^T A_{t} X_{t} \] and $X_{t}$ denotes the estimate of $X$ at the $t$-th iteration. If $A_{t+1}$ has full column rank, then we solve for $X_{t+1}$ by use of the generalised inverse; i.e., 
\[ X_{t+1} = A_{t+1}^{+}Y_{1}(A_{t+1}^{+})^{T} \] If $A_{t+1}$ does not have full column rank, a multiplicative update rule is used to update $X_{t}$ derived analogously, i.e. 
\[ (X_{t+1})_{ij} = (X_{t})_{ij} \frac{(N^{X}_{t})_{ij}}{(D^{X}_{t})_{ij}} \] where 
\[ N^{X}_{t} := A_{t+1}^{T}Y_{1}A_{t+1} \] and 
\[ D^{X}_{t}:= A_{t+1}^{T}A_{t+1}X_{t}A_{t+1}^{T}A_{t+1} \]  

Empirically, the row-sums are approximately stable in this algorithm, but it does \emph{not} guarantee that the rows sum to $1$. Therefore, at each iteration we renormalise the rows to enforce this property. The row sums are not in general identifiable. In practice, disabling this normalisation does not allow the row sums to drift significantly, except in cases where the model is a very poor approximation to the data. Poor model fit may cause the algorithm to terminate because it cannot find the local optima.

The following algorithm describes this rule, using $\circ$ to denote the entry-wise product of two matrices.

%\begin{algorithm}[h]
%\caption{LearnModelK: Infer $A$ and $X$ at fixed $k$}
%\label{alg:fixedK}
\begin{algorithmic}
\STATE {\bf Algorithm 1}
\STATE Inputs: Data $Y$, initial value of $A_0$; maximum number of iterations $t_{max}$
\FOR{$t=1 \ldots t_{max}$}
\STATE $A_{t} = \mathrm{NormaliseRows} \left(A_{t - 1} \circ \frac{N^{A}_{t-1}}{D^{A}_{t-1}} \right)$
\IF{$A$ has full column rank}
\STATE $X_{t} = A_{t-1}^{+}Y_{1}(A^{+}_{t-1})^{T}$
\ELSE
\STATE $X_{t} = X_{t-1} \circ \frac{N^{X}_{t-1}}{D^{X}_{t-1}}$
\ENDIF
\STATE {\bf if} $\norm{A_{t}-A_{t-1}}< \delta$ {\bf then} Break
\ENDFOR
\STATE Outputs: Estimates $A=A_{t}$ and $X=X_{y}$
\end{algorithmic}
%\end{algorithm}


\subsection{Statistical significance}
\label{sec:pvals}

For simple datasets consisting of $d$ subjects about which we observe $L$ features, significance is measured using a statistical resampling procedure implemented in the CLARITY package.
More complex datasets where similarities are computed in a complex way, and not read straightforwardly off matches between features -- for example, as for our linguistic data -- can still be
quantified via resampling. In such cases the data are bootstrapped externally and provided to the software as a set of matrices.
In this procedure, we sample $L/2$ of the $L$ features (columns) of the data $D_1$, which is a $d \times L$ matrix. We then compute a `sampled reference' (dis)similarity matrix, and from the remaining $L/2$, a `sampled target' (dis)similarity matrix.
We then replicate the downsampling procedure on the target data and obtain a (dis)similarity matrix.
We then mean centre and scale both sampled target and downsampled original target matrices into the sampled reference matrix,
and evaluate test statistics $f$ (squared residuals and persistences). This is repeated $n_{bs}$ times.

We compute a regularised empirical p-value $p(f(Y_2) |f(Y_1)) = \frac{1}{1+n_{bs}}(1 + \sum_{i=1}^{n_{bs}}\mathbb{I}( f(Y_2) \ge f(Y_1))$, formed from the probability that a sample from the true target is as great or geater than the resampled reference. This procedure is necessary because bootstrap resampling  \cite{efron_introduction_1994} is not straightforwardly valid for similarity matrices.

Whilst this procedure correctly estimates which structures of $Y_2$ are not predicted by $Y_1$, it does not distinguish between structures that are generated by signal vs noise. Because we are not interested in predicting noise, we further need to detect it. Estimating values of $k$ associated with structure is straight forward by simple cross-validation, because we have already constructed many random resamples of the data. We can therefore predict fold-2 of $Y_2$ from a CLARITY model learned in fold-1 of $Y_2$ and estimate $\hat{k}$ from the minimum cross-validation error.

Because $\hat{k}$ is a point estimate subject to variation, we obtain $n_{bs}$ samples $\hat{k}_i$ and implement a soft threshold, the `probability that complexity $k$ is describing structure' $p(k) = \frac{1}{n_{bs}} \sum_{i=1}^{n_{bs}}\mathbb{I}(\hat{k}_i \ge k)$, i.e. the proportion of bootstrap samples that have an estimate at least as large as $k$. We then report the complete CLARITY p-value $p_{c}$, quantifying the `probability of observing a test-statistic for $Y_2$, this extreme or more so, under the null hypothesis that $Y_2$ is a mean scaled (dimension $k$ rotation, translation and scaled) version of $Y_1$ with in which Complexity $k$ describes Structure.'. This is:
$$p_{c} = 1-\Big( \big(1-p(k)\big) p(f(Y_2) |f(Y_1)) \Big),$$
which is close to 0 only if both $p(k)$ is close to 1 and  $p(f(Y_2) |f(Y_1))$ is close to 0.

Because the p-values are highly correlated, and there is a multiple testing burden, the p-values should not be used to test for the presence of any difference in structure between $Y_1$ and $Y_2$. In particular, it can be that $Y_1$ and $Y_2$ are substantially different, but this does not result in any particular cell in the persistence diagram having a p-value at the appropriate multiple-testing level. In other words, on the level of $Y_1$ and $Y_2$ viewed globally, testing individual residuals has very low power; testing individual persistences has low power. A more powerful test would be for $f$ being the Frobenius norm of the whole matrix, but this has limited scientific value as it discards the scientific significance of the results.

We provide a formal statistical significance procedure to add weight to the identification of scientifically significant results, but emphasise that these are different concepts.

\subsection{Methylation/Expression Simulated data}
\label{sec:methysim}

For Figure \ref{fig:methy} we use the simulation data from \cite{gu2016complex} (Supplementary Figure S3) which is based on real methylation and expression patterns.

First each locus is assigned a methylation class, 300 are `hypo'methylated (low) and 700 are `hyper'methylated (high). They are then given a mean methylation, $\mu_i \sim U(0.1,0.4)$ and $\mu_i \sim U(0.55,0.85)$ for hypo/hyper respectively. Then `Tumor' cases are assigned methylation $m_{ij}\sim U(\mu_i-\mu_i/2, \mu_i+\mu_i/2)$ or $m_{ij}\sim U(\mu_i-(1-\mu_i)/2, \mu_i+(1-\mu_i)/2)$ for hypo/hyper. Control cases follow the same distribution but shift towards the mean by replacing $\mu_i$ with $\mu_i+0.2$ (if $\mu_i<0.3$), or $\mu_i-0.2$ (if $\mu_i>0.7$) or with equal probability of positive or negative shift $+/-0.2$ otherwise.

To create Expression data, the procedure generates $\sigma_i \sim U(0.5,0.9)$ for each locus and then sets $e_{ij}^\prime\sim N(\mu_{ij},\sigma_i^2)$. The reported expression $e_{ij}$ are then centred and scaled.

We then chose two segments of 10 Loci and moved the two classes towards each other; the top anomaly loci have `tumor' expression altered, and the bottom have `control' altered, by setting $e_{ij}^{anomaly}=-2e_{ij}$ . This simulates loci that behave differently in Methylation data to in Expression data. These parameters induce an average -5\% correlation between methylation and expression, i.e. the association is weak. 


\subsection{Simulation details}
\label{sec:methodssim}


% We generate a tree $\mathcal{T}_1$ relating clusters in the data (see Methods Section \ref{sec:methodssim}). The true $A$ is therefore a clustering, i.e. subject $i$ has $A_{ik}=0$ for all $k$ except its cluster for which $A_{ik}=1$. We then generate data $D_0$ which is a matrix of features for each subject, by sampling each feature from the features of the subjects' cluster. Each of the $L$ features drifts according to this tree using the `ape' \cite{paradis2011package} function `rTraitCont' which generates a brownian motion on a tree. The true $X$ is
% $X_1 = \mathrm{Distance}(D_0(i,\cdot),D_o(j,\cdot))$. The observable data is $D=\mathrm{Normal}(A D_0,\sigma^2)$ which is a matrix of features for each subject, obtained by sampling each feature from the features of the subjects' cluster.

% We then simulate $Y_2$. In \emph{Scenario A} we change the relationship only. We generate $Y_2$ by making a new tree $\mathcal{T}_2$ with different branch lengths from  $\mathcal{T}_1$, by multiplying each branch by a $\mathrm{Uniform}(0.1,10)$ random number and repeat the process of generating data.  In \emph{Scenario B}  we use the \emph{same} changed branch lengths but additionally change $A$. We choose one cluster and assign a proportion $p=0.5$ are assigned as a mixture from a second cluster (with weight $\beta=0.5$, retaining weight $1-\beta$ from the original cluster). The second cluster is chosen uniformly from the clusters that are further away than the median.


For Section \ref{sec:sim} we generate a coalescent tree $\mathcal{T}_1$ using `rcoal' from the package `ape' \cite{paradis2011package} for R \cite{Rproject}. The `true' $A$ is a vector of zeroes except for the cluster membership $k$ of $i$, for which $A_{ik}=1$. We then simulate a matrix $D_0$ consisting of $k$ rows (clusters) and  $L$ columns (features) by allowing features to drift in a correlated manner under a random-walk model using the function `rTraitCont' from the package `ape'. This generates a `true' $X=\dist(D_0)$. To generate a feature $d$ for a sample with mixture $a$, we simulate features $d~N(a^T D_o, \sigma_0^2)$, from which we can compute $Y=\dist(D)$.

In \emph{Scenario A} we make $\mathcal{T}_2$ into a non-ultrametric tree by randomly perturbing the branch lengths of $\mathcal{T}_1$ by multiplying each by a $U(0.1,2)$ variable. We generate $Y^{(2)}$ as above from $\mathcal{T}_2$.

In \emph{Scenario B} we make $\mathcal{T}_2$ as in Scenario A. Then one additional \emph{mixture edge} is added at random. This is done by choosing a tip of the tree $i$, choosing a second tip $j$ at least the median distance from the first tip, and setting $A[,i] \leftarrow (1-\beta) A[,j]$ and $A[,j]= A[,j] + \beta A[,i]$. This edge affects a proportion $r$ of the subjects in cluster $i$. If $r=0$ or $r=1$ this becomes a relationship change rather than a structural change, because all of the samples in the cluster adopt a new relationship with the remaining clusters (though the relationship is no longer a tree). We use $r=0.5$ throughout.

\subsection{Language example details}
\label{sec:methodsling}

%we first compute phonetic similarities for each pair of words expressing the same meaning using using a state-of-the-art method that takes into account global phonetic similarity and language-pair-specific regular correspondences \cite{Dellert-2018}. We estimate overall Phonetic similarity between two languages as the average of their word-to-word similarities. Then for each meaning, we automatically infer which words in different languages stem from the same ancestral word (i.e. which are \emph{cognates}), via clustering based on word-level phonetic similarities. Lexical similarity between two languages is operationalised as sharing cognates for the same meaning, thus tracking a different type of information than Phonetic. 

We compute \textbf{phonetic similarities} using the \emph{Information-Weighted Distance with Sound Correspondences} (IWDSC) method \cite{Dellert-2018}. First, we estimate global sound similarity scores, based on the whole NorthEuraLex 0.9 dataset with 107 languages. This provides us with an idea of which sounds in the data generally tend to be close. Implicitly, the employed inference method makes those sounds close that appear in words that are likely historically related. In other words, global sound similarities are not directly about articulatory or auditory similarities (i.e.~how humans produce and perceive different sounds), but rather estimate ``historical similarity'', thus implicitly tracking processes of language change. 

After obtaining global sound similarities, we compute local sound similarity scores for each language-language pair in our 36-language Indo-European subset of NorthEuraLex. This works similarly to global similarity scores, but now only taking into account data from those two languages. Both global and local sound similarity scores are based on mutual information. In particular, the local scores declare  such sounds similar which are highly predictable from the sounds in the word expressing the same meaning in the other language. 

To obtain overall language-language Phonetic scores, we first build word-to-word similarity scores based on sound-to-sound similarity scores. Crucially, we discount the weight of the sounds in highly regular parts of words,
e.g. the infinitive ending in German verbs such as geb*en* ``to give'' and leb*en* ``to live''  \cite{Dellert-Buch-2018}. This way, we discount the regular grammatical elements: they carry information about the grammar of a language, but not about its individual words. We also normalise by word length. To get aggregate language-to-language similarities out of word-to-word similarities, we simply average. 
 
Language-to-language \textbf{lexical similarity} is defined as cognate overlap: the share of words in the relevant two languages that were inferred to have the same ancestral word. We produce automatic cognacy judgements by applying UPGMA clustering to the word-to-word phonetic similarity scores within each meaning, a method shown to currently produce state-of-the-art automatic cognacy judgements \cite{Dellert-2018}. 

Both Phonetic and Lexical similarities that we compute are based on word-to-word phonetic similarity scores. The cognate clustering step that takes us from word-to-word similarities to cognate overlap aims to uncover, automatically, information about the word-replacement change. Phonetic and Lexical information is bound to be \textbf{highly correlated}. First, the change of two types occurs in the same communities subject to the same historical processes. For example, both Phonetic and Lexical change accumulate with time, so two speech communities that split earlier will be more dissimilar on both Phonetic and Lexical change than two speech communities with a later split, other things being equal. Second, when two languages retain a common ancestral word, simply by virtue of stemming from the same proto-word, the two modern words are going to be more phonetically similar than two randomly selected phonetic sequences from the two languages. Thus higher levels of true cognate overlap will lead to higher levels of phonetic similarities. 

Finally, in addition to these two real-world drivers of correlation, in our computational analysis we infer lexical overlap based on low-level phonetic similarity. It is a common and effective practice in computational historical linguistics, and only slightly inferior to expert-coded information for at least some types of practical inference \cite{Rama-et-al-2018}. But we do expect to miss some true cognates that changed phonetically so much as to be not statistically identifiable from the raw data without additional expert knowledge. This makes our estimated dissimilarity matrices for Phonetic and Lexical still more correlated than the corresponding ground-truth matrices would be. This makes it all the more striking that despite a strong correlation between Phonetic and Lexical, stemming from both natural and analysis-induced sources, we find a robust and convincing effect of mismatch using CLARITY. 

To assess \textbf{significance}, we use the method described in Sec.~\ref{sec:methodssim}, dividing the data into two halves by meaning, and computing independently a similarity matrix from each half. When doing that, we always use the same global similarity scores, which represent the properties of a much larger sample of 107 languages, taken as a proxy for languages of the world in general. 

We use \textbf{cross-validation} to assess whether CLARITY decomposition at higher complexities $k$ still captures signal rather than noise. For the pairs of matrices based on two halves of the data, we predict one based on the other (so Phonetic from Phonetic and Lexical from Lexical). This shows (Supp. Fig. 3) that even at the highest $k$ we do not have overfitting to noise. Therefore the full range of $k$ in the main analysis is interpretable. 

The CLARITY setup in this example implies that the diagonal values in our similarity matrices might not be amenable to successful modelling. In the main text above, we report the results where we discount the diagonal when doing CLARITY decomposition. We checked whether the pattern we found depended on this choice. Supp. Fig 4 shows CLARITY persistences with no special treatment for the diagonal, and those are basically the same as those in Figure~\ref{fig:linguistic}. Similarly, we checked (Supp. Fig 5) that lowering the significance threshold from 0.05 to 0.01 does not change the result. Finally, in Supp. Fig 6-7), we checked that the image representation of persistence did not affect our inference, and that resampling  the persistence curves for the matrices based on half the data, used for significance testing. Taken together, these further checks convince us that our reported result is real and not spurious. 

\subsection{Mathematical validity of structural comparison}
\label{sec:math}

The matrices $Y_{1}$ and $Y_{2}$ are typically observed with non-independent noise and so there is a need for the various quantities of interest to be stable under perturbation - that is, that a small change in the data does not result in a large change to the inference. The following result describes the stability of the residual matrix under perturbation of $Y_{1}$ and $Y_{2}$ for the SVD-based solution.

\begin{thm} \label{thm:SVDperturb} Let $Y_{1}, Y_{2}, Y'_{1}, Y'_{2} \in \R^{d}$ be symmetric matrices such that $\norm{Y_{2} - Y'_{2}}_{F}, \norm{Y_{1} - Y'_{1}}_{F} \leq \epsilon$. Suppose that we have singular value decompositions $Y_{1} = U_{1}\Sigma_{1}V_{1}^{T}$ and $Y'_{1} = U'_{1}\Sigma'_{1}V_{1}^{'T}$. Let $A_{k}$ and $A'_{k}$ be the matrices obtained by taking the first $k$ columns of $U_{1}$ and $U_{1}'$ respectively.
\begin{enumerate}
\item If $X_{2}^{(k)} = A_{k}^{+}Y_{2}(A_{k}^{+})^{T}$ with $X_{2}^{'(k)}$ defined analogously for $Y'_{2}$ then
\[ \norm{Y_{2} - A_{k}X_{2}^{(k)}A_{k}^{T}}_{F} \leq \norm{Y'_{2} - A_{k}X^{'(k)}_{2}A_{k}^{T}}_{F} + 2\epsilon \]
\item Suppose that $Y_{1}$ has eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \dots \geq \lambda_{d}$ and let $\delta_{k} := \lambda_{k} - \lambda_{k + 1}$ for each natural number $k < d$. Then
\[ \norm{Y_{2} - A_{k}X_{2}^{(k)}A_{k}^{T}}_{F} \leq \norm{Y_{2} - A'_{k}X_{2}^{(k)}A_{k}^{'T}}_{F} + \frac{2^{5/2} \epsilon}{\delta_{k}}.\] 
\end{enumerate}
\end{thm}

The proof of Theorem \ref{thm:SVDperturb} may be found in the appendix. Theorem \ref{thm:SVDperturb} can be used for statistical purposes as follows. If $Y'_{1}$ and $Y'_{2}$ are sampled matrices that are believed to be close to their population counterparts $Y_{1}, Y_{2}$ (for example when dealing with covariances), then given suitably sized eigengaps $\delta_{k}$ and $\delta_{k}'$ for $Y_{1}$ and $Y_{1}'$ respectively, the Frobenius norm of the estimated residual matrix is close to that of the true residual matrix. Specifically, simple manipulation of the inequalities established in Theorem \ref{thm:SVDperturb} leads to the deviation inequality
\[ |\norm{Y_{2} - A_{k}X_{2}^{(k)}A_{k}^{T}}_{F} - \norm{Y_{2}' - A'_{k}X_{2}^{'(k)}A_{k}^{'T}}_{F}| \leq \frac{2 + 2^{5/2}}{\min(\delta_{k}, \delta_{k}')}\epsilon \] where $\norm{Y_{2} - Y'_{2}}_{F}, \norm{Y_{1} - Y'_{1}}_{F} \leq \epsilon$. 

%In the SVD case, one can measure the structural difference between the matrix $Y_{1}$ and $Y_{2}$ at dimension $k$ by means of a \emph{structural divergence} 
%\[ D(Y_{1}, Y_{2}; k) := \norm{\hat{Y}_{1, k} - P_{k}Y_{2}P_{k}}_{F} \] where $\hat{Y}_{1,k}$ is defined to be the truncation of the SVD of $Y_{1}$ to its top $k$ singular values and $P_{k}$ is defined to be the orthogonal projection operator onto the first $k$ singular vectors. In general, this divergence is asymmetric in $Y_{1}$ and $Y_{2}$. It is also stable to perturbation in both arguments. 
%
%\begin{thm} \label{thm:SVDdivperturb} Let $Y_{1}, Y_{1}', Y_{2}, Y_{2}' \in \R^{d \times d}$ be symmetric matrices such that $\norm{Y_{2} - Y'_{2}}_{F}, \norm{Y_{1} - Y'_{1}}_{F} \leq \epsilon$. Suppose that we have singular value decompositions $Y_{1} = U_{1}\Sigma_{1}V_{1}^{T}$ and $Y'_{1} = U'_{1}\Sigma'_{1}V_{1}^{'T}$. Let $P_{k}$ and $P'_{k}$ be the orthogonal projections onto the span of the first $k$ columns of $U_{1}$ and $U_{1}'$ respectively. Suppose that $Y_{1}$ has eigenvalues $\lambda_{1} \geq \dots \lambda_{d}$. Put $\delta_{k} := \lambda_{k} - \lambda_{k + 1}$ for each $j \in [d]$. Then 
%\begin{enumerate}
%\item 
%$D(Y_{1}, Y_{2}; k) \leq D(Y_{1}, Y'_{2}; k) + \epsilon$
%\item 
%$D(Y_{1}, Y_{2}; k) \leq D(Y'_{1}, Y_{2}; k) + $\\$\left\{\frac{2^{5/2}}{\delta_{k}}(\norm{Y_{1}}_{F} + \norm{Y_{2}}_{F}) + 1 \right\} \epsilon$
%\end{enumerate}
%\end{thm}
%
%The proofs of both theorems may be found in the appendix.

% 
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{apalike}
\begin{thebibliography}{}

\bibitem[Amorim et~al., 2013]{AmorimBayesianApproachGenome2013}
Amorim, C. E.~G., {Bisso-Machado}, R., Ramallo, V., Bortolini, M.~C., Bonatto,
  S.~L., Salzano, F.~M., and H\"unemeier, T. (2013).
\newblock A {{Bayesian Approach}} to {{Genome}}/{{Linguistic Relationships}} in
  {{Native South Americans}}.
\newblock {\em PLOS ONE}, 8(5):e64099.

\bibitem[Bentley et~al., 2014]{bentley_books_2014}
Bentley, R.~A., Acerbi, A., Ormerod, P., and Lampos, V. (2014).
\newblock Books {Average} {Previous} {Decade} of {Economic} {Misery}.
\newblock {\em PLOS ONE}, 9(1):e83147.
\newblock Publisher: Public Library of Science.

\bibitem[Bibikova et~al., 2011]{bibikova_high_2011}
Bibikova, M., Barnes, B., Tsan, C., Ho, V., Klotzle, B., Le, J.~M., Delano, D.,
  Zhang, L., Schroth, G.~P., Gunderson, K.~L., Fan, J.-B., and Shen, R. (2011).
\newblock High density {DNA} methylation array with single {CpG} site
  resolution.
\newblock {\em Genomics}, 98(4):288--295.

\bibitem[Bille, 2005]{Billesurveytreeedit2005}
Bille, P. (2005).
\newblock A survey on tree edit distance and related problems.
\newblock {\em Theoretical Computer Science}, 337(1):217--239.

\bibitem[Brooks, 1979]{BrooksTestingContextExtent1979}
Brooks, D.~R. (1979).
\newblock Testing the {{Context}} and {{Extent}} of {{Host}}-{{Parasite
  Coevolution}}.
\newblock {\em Systematic Biology}, 28(3):299--307.

\bibitem[Carlsson and M{\'e}moli, 2010]{carlsson_characterization_2010}
Carlsson, G. and M{\'e}moli, F. (2010).
\newblock Characterization, {{Stability}} and {{Convergence}} of {{Hierarchical
  Clustering Methods}}.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1425--1470.

\bibitem[CIA, 2018]{CIA2018a}
CIA (2018).
\newblock {The World Factbook 2018}.

\bibitem[Creanza et~al., 2015]{Creanzacomparisonworldwidephonemic2015}
Creanza, N., Ruhlen, M., Pemberton, T.~J., Rosenberg, N.~A., Feldman, M.~W.,
  and Ramachandran, S. (2015).
\newblock A comparison of worldwide phonemic and genetic variation in human
  populations.
\newblock {\em Proceedings of the National Academy of Sciences},
  112(5):1265--1272.

\bibitem[Dellert, 2018]{Dellert-2018}
Dellert, J. (2018).
\newblock Combining information-weighted sequence alignment and sound
  correspondence models for improved cognate detection.
\newblock In {\em Proceedings of the 27th International Conference on
  Computational Linguistics}, pages 3123--3133.

\bibitem[Dellert and Buch, 2018]{Dellert-Buch-2018}
Dellert, J. and Buch, A. (2018).
\newblock A new approach to concept basicness and stability as a window to the
  robustness of concept list rankings.
\newblock {\em Language Dynamics and Change}, 8(2):157--181.

\bibitem[Dellert and J\"{a}ger, 2017]{NorthEuraLex}
Dellert, J. and J\"{a}ger, G., editors (2017).
\newblock {\em NorthEuraLex (version 0.9)}.

\bibitem[Eckart and Young, 1936]{Eckartapproximationonematrix1936}
Eckart, C. and Young, G. (1936).
\newblock The approximation of one matrix by another of lower rank.
\newblock {\em Psychometrika}, 1(3):211--218.

\bibitem[Efron et~al., 1994]{efron_introduction_1994}
Efron, B., Tibshirani, R.~J., and Tibshirani, R.~J. (1994).
\newblock {\em An {Introduction} to the {Bootstrap}}.
\newblock Chapman and Hall/CRC.

\bibitem[EVS, 2011]{EVS2011}
EVS (2011).
\newblock {European Values Study}.

\bibitem[Flury, 1988]{FluryCommonPrincipalComponents1988}
Flury, B. (1988).
\newblock {\em Common {{Principal Components}} \& {{Related Multivariate
  Models}}}.
\newblock {John Wiley \& Sons, Inc.}, New York, NY, USA.

\bibitem[Flury, 1986]{FluryAsymptoticTheoryCommon1986}
Flury, B.~N. (1986).
\newblock Asymptotic {{Theory}} for {{Common Principal Component Analysis}}.
\newblock {\em The Annals of Statistics}, 14(2):418--430.

\bibitem[F\"orstner and Moonen, 2003]{ForstnerMetricCovarianceMatrices2003}
F\"orstner, W. and Moonen, B. (2003).
\newblock A {{Metric}} for {{Covariance Matrices}}.
\newblock In Grafarend, E.~W., Krumm, F.~W., and Schwarze, V.~S., editors, {\em
  Geodesy-{{The Challenge}} of the 3rd {{Millennium}}}, pages 299--309.
  {Springer Berlin Heidelberg}, Berlin, Heidelberg.

\bibitem[Gorodnichenko and Roland, 2016]{Gorodnichenko2016}
Gorodnichenko, Y. and Roland, G. (2016).
\newblock {Culture, Institutions and the Wealth of Nations}.
\newblock {\em Review of Economics and Statistics}, page REST{\_}a{\_}00599.

\bibitem[Grigoriadis et~al.,
  2012]{GrigoriadisMolecularcharacterisationcell2012}
Grigoriadis, A., Mackay, A., Noel, E., Wu, P.~J., Natrajan, R., Frankum, J.,
  {Reis-Filho}, J.~S., and Tutt, A. (2012).
\newblock Molecular characterisation of cell line models for triple-negative
  breast cancers.
\newblock {\em BMC genomics}, 13:619.

\bibitem[Gu et~al., 2016]{gu2016complex}
Gu, Z., Eils, R., and Schlesner, M. (2016).
\newblock Complex heatmaps reveal patterns and correlations in multidimensional
  genomic data.
\newblock {\em Bioinformatics}, 32(18):2847--2849.

\bibitem[Hardoon et~al., 2004]{hardoon2004canonical}
Hardoon, D.~R., Szedmak, S., and Shawe-Taylor, J. (2004).
\newblock Canonical correlation analysis: An overview with application to
  learning methods.
\newblock {\em Neural computation}, 16(12):2639--2664.

\bibitem[Hotelling, 1936]{hotelling1936relations}
Hotelling, H. (1936).
\newblock Relations between two sets of variates.
\newblock {\em Biometrika}, 28:321--327.

\bibitem[Hurley and Cattell, 1962]{HurleyprocrustesprogramProducing1962}
Hurley, J.~R. and Cattell, R.~B. (1962).
\newblock The procrustes program: {{Producing}} direct rotation to test a
  hypothesized factor structure.
\newblock {\em Behavioral Science}, 7(2):258--262.

\bibitem[Inglehart and Welzel, 2005]{Inglehart2005a}
Inglehart, R. and Welzel, C. (2005).
\newblock {\em {Modernization, cultural change, and democracy : the human
  development sequence}}.
\newblock Cambridge University Press.

\bibitem[Jackson, 1995]{JacksonPROTESTPROcrusteanRandomization1995}
Jackson, D.~A. (1995).
\newblock {{PROTEST}}: {{A PROcrustean Randomization TEST}} of community
  environment concordance.
\newblock {\em \'Ecoscience}, 2(3):297--303.

\bibitem[Kriegeskorte et~al.,
  2008]{KriegeskorteRepresentationalSimilarityAnalysis2008a}
Kriegeskorte, N., Mur, M., and Bandettini, P. (2008).
\newblock Representational {{Similarity Analysis}} \textendash{} {{Connecting}}
  the {{Branches}} of {{Systems Neuroscience}}.
\newblock {\em Frontiers in Systems Neuroscience}, 2.

\bibitem[Lawson et~al., 2012]{lawson_inference_2012}
Lawson, D.~J., Hellenthal, G., Myers, S., and Falush, D. (2012).
\newblock Inference of {{Population Structure}} using {{Dense Haplotype Data}}.
\newblock {\em PLOS Genet}, 8(1):e1002453.

\bibitem[Lawson et~al., 2018]{lawson2018tutorial}
Lawson, D.~J., Van~Dorp, L., and Falush, D. (2018).
\newblock A tutorial on how not to over-interpret {{STRUCTURE}} and
  {{ADMIXTURE}} bar plots.
\newblock {\em Nature communications}, 9(1):3258.

\bibitem[Lee and Seung, 2001]{lee_algorithms_2001-1}
Lee, D.~D. and Seung, H.~S. (2001).
\newblock Algorithms for {{Non}}-negative {{Matrix Factorization}}.
\newblock In Leen, T.~K., Dietterich, T.~G., and Tresp, V., editors, {\em
  Advances in {{Neural Information Processing Systems}} 13}, pages 556--562.
  {MIT Press}.

\bibitem[Lockhart and Winzeler, 2000]{lockhart_genomics_2000}
Lockhart, D.~J. and Winzeler, E.~A. (2000).
\newblock Genomics, gene expression and {DNA} arrays.
\newblock {\em Nature}, 405(6788):827--836.
\newblock Bandiera\_abtest: a Cg\_type: Nature Research Journals Number: 6788
  Primary\_atype: Reviews Publisher: Nature Publishing Group.

\bibitem[Mahalanabis and {\v S}tefankovi{\v c},
  2009]{MahalanabisApproximatingdistancesmixture2009}
Mahalanabis, S. and {\v S}tefankovi{\v c}, D. (2009).
\newblock Approximating {{L}} 1 -distances between mixture distributions using
  random projections.
\newblock In {\em Proceedings of the {{Meeting}} on {{Analytic Algorithmics}}
  and {{Combinatorics}}}, pages 75--84. {Society for Industrial and Applied
  Mathematics}.

\bibitem[Mantel, 1967]{Manteldetectiondiseaseclustering1967}
Mantel, N. (1967).
\newblock The detection of disease clustering and a generalized regression
  approach.
\newblock {\em Cancer Research}, 27(2):209--220.

\bibitem[Matras, 2009]{Matras-2009}
Matras, Y. (2009).
\newblock {\em Contact {L}inguistics}.
\newblock Cambridge University Press.

\bibitem[Matthews et~al., 2013]{Matthews2013}
Matthews, L.~J., Edmonds, J., Wildman, W.~J., and Nunn, C.~L. (2013).
\newblock {Cultural inheritance or cultural diffusion of religious violence? A
  quantitative case study of the Radical Reformation}.
\newblock {\em Religion, Brain {\&} Behavior}, 3(1):3--15.

\bibitem[Matthews et~al., 2016]{Matthews2016a}
Matthews, L.~J., Passmore, S., Richard, P.~M., Gray, R.~D., and Atkinson, Q.~D.
  (2016).
\newblock {Shared Cultural History as a Predictor of Political and Economic
  Changes among Nation States}.
\newblock {\em PLOS ONE}, 11(4).

\bibitem[Min et~al., 2020]{min_genomic_2020}
Min, J.~L., Hemani, G., Hannon, E., and {Lawson, D.J. et al.} (2020).
\newblock Genomic and phenomic insights from an atlas of genetic effects on
  {DNA} methylation.
\newblock {\em medRxiv; Nature Genetics (to appear)}, page 2020.09.01.20180406.

\bibitem[Mossel, 2005]{mossel_phylogenetic_2005}
Mossel, E. (2005).
\newblock Phylogenetic {{MCMC Algorithms Are Misleading}} on {{Mixtures}} of
  {{Trees}}.
\newblock {\em Science}, 309(5744):2207--2209.

\bibitem[Nikolaev et~al., 2017]{Nikolaev2017}
Nikolaev, B., Boudreaux, C., and Salahodjaev, R. (2017).
\newblock {Are individualistic societies less equal? Evidence from the parasite
  stress theory of values}.
\newblock {\em Journal of Economic Behavior {\&} Organization}, 138:30--49.

\bibitem[Nye et~al., 2017]{NyePrincipalcomponentanalysis2017}
Nye, T. M.~W., Tang, X., Weyenberg, G., and Yoshida, R. (2017).
\newblock Principal component analysis and the locus of the {{Fr\'echet}} mean
  in the space of phylogenetic trees.
\newblock {\em Biometrika}, 104(4):901--922.

\bibitem[Paradis et~al., 2011]{paradis2011package}
Paradis, E., Bolker, B., Claude, J., Sien~Cuong, H., Desper, R., Durand, B.,
  Dutheil, J., Gascuel, O., Heilbl, C., Lawson, D., et~al. (2011).
\newblock Package `ape': Analysis of phylogenetics and evolution.
\newblock {\em URL [http://cran. r-project. org/web/packages/ape/ape. pdf]},
  pages 1--222.

\bibitem[Penny and Hendy, 1985]{PennyUseTreeComparison1985}
Penny, D. and Hendy, M.~D. (1985).
\newblock The {{Use}} of {{Tree Comparison Metrics}}.
\newblock {\em Systematic Zoology}, 34(1):75--82.

\bibitem[{Peres-Neto} and Jackson, 2001]{Peres-NetoHowwellmultivariate2001}
{Peres-Neto}, P.~R. and Jackson, D.~A. (2001).
\newblock How well do multivariate data sets match? {{The}} advantages of a
  {{Procrustean}} superimposition approach over the {{Mantel}} test.
\newblock {\em Oecologia}, 129(2):169--178.

\bibitem[Putterman and Weil, 2010]{Putterman2010}
Putterman, L. and Weil, D.~N. (2010).
\newblock {Post-1500 Population Flows and the Long Run Determinants of Economic
  Growth and Inequality.}
\newblock {\em The quarterly journal of economics}, 125(4):1627--1682.

\bibitem[{R Core Team}, 2018]{Rproject}
{R Core Team} (2018).
\newblock {\em R: A Language and Environment for Statistical Computing}.
\newblock R Foundation for Statistical Computing, Vienna, Austria.

\bibitem[Raghu et~al., 2017]{NIPS2017_7188}
Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. (2017).
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30}, pages 6076--6085. Curran Associates, Inc.

\bibitem[Rama et~al., 2018]{Rama-et-al-2018}
Rama, T., List, J.-M., Wahle, J., and J\"{a}ger, G. (2018).
\newblock Are automatic methods for cognate detection good enough for
  phylogenetic reconstruction in historical linguistics?
\newblock In {\em Proceedings of NAACL-HLT 2018}, pages 393--400. Association
  for Computational Linguistics.

\bibitem[R\"omer et~al., 2014]{RomerCrossplatformtoxicogenomicsprediction2014}
R\"omer, M., Eichner, J., Metzger, U., Templin, M.~F., Plummer, S.,
  {Ellinger-Ziegelbauer}, H., and Zell, A. (2014).
\newblock Cross-platform toxicogenomics for the prediction of non-genotoxic
  hepatocarcinogenesis in rat.
\newblock {\em PloS One}, 9(5):e97640.

\bibitem[Ruck et~al., 2018]{ruck2018religious}
Ruck, D.~J., Bentley, R.~A., and Lawson, D.~J. (2018).
\newblock Religious change preceded economic change in the 20th century.
\newblock {\em Science advances}, 4(7):eaar8680.

\bibitem[Schliep, 2011]{Schliepphangornphylogeneticanalysis2011}
Schliep, K.~P. (2011).
\newblock Phangorn: Phylogenetic analysis in {{R}}.
\newblock {\em Bioinformatics}, 27(4):592--593.

\bibitem[Schneider and Borlund, 2007]{SchneiderMatrixcomparisonPart2007}
Schneider, J.~W. and Borlund, P. (2007).
\newblock Matrix comparison, {{Part}} 2: {{Measuring}} the resemblance between
  proximity measures or ordination results by use of the mantel and procrustes
  statistics.
\newblock {\em Journal of the American Society for Information Science and
  Technology}, 58(11):1596--1609.

\bibitem[Seber, 2009]{seber2009multivariate}
Seber, G.~A. (2009).
\newblock {\em Multivariate observations}, volume 252.
\newblock John Wiley \& Sons.

\bibitem[Sinding~Bentzen, 2019]{sinding_bentzen_acts_2019}
Sinding~Bentzen, J. (2019).
\newblock Acts of {God}? {Religiosity} and {Natural} {Disasters} {Across}
  {Subnational} {World} {Districts}*.
\newblock {\em The Economic Journal}, 129(622):2295--2321.

\bibitem[Smouse et~al., 1986]{SmouseMultipleRegressionCorrelation1986}
Smouse, P.~E., Long, J.~C., and Sokal, R.~R. (1986).
\newblock Multiple {{Regression}} and {{Correlation Extensions}} of the
  {{Mantel Test}} of {{Matrix Correspondence}}.
\newblock {\em Systematic Zoology}, 35(4):627--632.

\bibitem[Sokal, 1988]{SokalGeneticgeographiclinguistic1988}
Sokal, R.~R. (1988).
\newblock Genetic, geographic, and linguistic distances in {{Europe}}.
\newblock {\em Proceedings of the National Academy of Sciences},
  85(5):1722--1726.

\bibitem[Spolaore and Wacziarg, 2013]{Spolaore2013}
Spolaore, E. and Wacziarg, R. (2013).
\newblock {How Deep Are the Roots of Economic Development?}
\newblock {\em Journal of Economic Literature}, 51(2):325--369.

\bibitem[Steiger, 1980]{SteigerTestscomparingelements1980}
Steiger, J.~H. (1980).
\newblock Tests for comparing elements of a correlation matrix.
\newblock {\em Psychological Bulletin}, 87(2):245--251.

\bibitem[Ter~Braak, 1987]{ter1987analysis}
Ter~Braak, C.~J. (1987).
\newblock The analysis of vegetation-environment relationships by canonical
  correspondence analysis.
\newblock {\em Vegetatio}, 69(1-3):69--77.

\bibitem[Thomason and Kaufman, 1988]{Thomason_Kaufmann_1988}
Thomason, S.~G. and Kaufman, T. (1988).
\newblock {\em Language contact, creolization and genetic linguistics}.
\newblock University of California Press, Berkeley.

\bibitem[Tipping, 1999]{TippingDerivingclusteranalytic1999}
Tipping, M.~E. (1999).
\newblock Deriving cluster analytic distance functions from {{Gaussian}}
  mixture models.
\newblock {\em icann99}, pages 815--820.

\bibitem[Trask and Millar, 2015]{Trask-Millar-2015}
Trask, L. and Millar, R.~M. (2015).
\newblock {\em Trask's Historical Linguistics}.
\newblock Routledge, 3rd edition.

\bibitem[Victor and Purpura, 1997]{victor1997metric}
Victor, J.~D. and Purpura, K.~P. (1997).
\newblock Metric-space analysis of spike trains: theory, algorithms and
  application.
\newblock {\em Network: computation in neural systems}, 8(2):127--164.

\bibitem[Wasserman, 2018]{WassermanTopologicalDataAnalysis2018}
Wasserman, L. (2018).
\newblock Topological {{Data Analysis}}.
\newblock {\em Annual Review of Statistics and Its Application}, 5(1):501--532.

\bibitem[{World Bank}, 2018]{CIA2018}
{World Bank} (2018).
\newblock {World Development Indicators}.

\bibitem[WVS, 2017]{WVS2017}
WVS (2017).
\newblock {World Value Survey - What We Do.}

\bibitem[Yu et~al., 2015]{YuusefulvariantDavis2015}
Yu, Y., Wang, T., and Samworth, R.~J. (2015).
\newblock A useful variant of the {{Davis}}\textendash{{Kahan}} theorem for
  statisticians.
\newblock {\em Biometrika}, 102(2):315--323.

\bibitem[Zhang et~al., 2018]{ZhangCompletemitochondrialrDNA2018}
Zhang, S.-M., Bu, L., Laidemitt, M.~R., Lu, L., Mutuku, M.~W., Mkoji, G.~M.,
  and Loker, E.~S. (2018).
\newblock Complete mitochondrial and {{rDNA}} complex sequences of important
  vector species of {{Biomphalaria}} , obligatory hosts of the human-infecting
  blood fluke, {{Schistosoma}} mansoni.
\newblock {\em Scientific Reports}, 8(1):7341.

\end{thebibliography}
% 
% If your work has an appendix, this is the place to put it.
\appendix

\section{Proofs}
\label{sec:appendix}

%\subsection{Proofs}

\subsection{Notation} In addition to the notation already introduced, if $A$ is a matrix, its spectral norm is denoted by $\norm{A}_{2}$. The singular values $\sigma_{1}(A) \geq \sigma_{2}(A) \dots$ of $A$ are listed in non-increasing order, and so $\norm{A}_{2} = \sigma_{1}(A)$. If $v$ is a vector, its Euclidean norm is denoted by $\norm{v}$. If $A$ is a matrix, its vectorisation (the vector obtained by stacking the columns of $A$) is denoted by $\Vect(A)$. 

\subsection{Preliminary facts} Recall that for any matrices $A, B$ and $C$ where the product $ABC$ exists, we have the identity $\Vect(ABC) = (C^{T} \otimes A)\Vect(B)$ where $\otimes$ denotes the Kronecker product of two matrices. This identity is useful in what follows.

If $V, V'$ are $d \times k$ matrices with orthonormal columns, we have a vector $(\cos^{-1}(\sigma_{1}), \dots, \cos^{-1}(\sigma_{k}))^{T}$ of principal angles, where the $\sigma_{j}$ are the singular values of $V^{'T}V$. Let $\Theta(V', V)$ denote the $r \times r$ diagonal matrix with the $j$-th diagonal entry given by the $j$-th principal angle. The matrices $\sin \Theta(V', V)$ and $\cos \Theta(V', V)$ are defined entry-wise. The perturbation bounds established rely on the following variant of the Davis-Kahan theorem \cite{YuusefulvariantDavis2015}.

\begin{thm} \label{thm:dk} Let $Y, Y' \in \R^{d \times d}$ be symmetric matrices with eigenvalues $\lambda_{1} \geq \dots \geq \lambda_{d}$ and $\lambda'_{1} \geq \dots \geq \lambda'_{d}$ respectively. Fix $1 \leq r \leq s \leq d$ and suppose that $\delta_{r, s} := \min(\lambda_{r - 1} - \lambda_{r}, \lambda_{s} - \lambda_{s + 1}) > 0$, where $\lambda_{0} := \infty$ and $\lambda_{d + 1} := - \infty$. Put $p = s - r + 1$ and define $V := [v_{r} | v_{r+1} | \dots | v_{s}], V' := [v'_{r} | v'_{r+1} | \dots | v'_{s}]$, both with orthonormal columns, satisfying $Yv_{j} = \lambda_{j} v_{j}$ and $Y'v'_{j} = \lambda'_{j} v'_{j}$ for each $j = r, r+1, \dots, s$. Then
\[ \norm{\sin \Theta(V', V)}_{F} \leq \frac{2 \min(p^{1/2}\norm{Y - Y'}_{2}, \norm{Y - Y'}_{F})}{\delta_{r, s}} \] 
\end{thm}

\subsection{Proof of Theorem \ref{thm:SVDperturb}}
\begin{enumerate}
\item Let $P_{k} = P_{A_{k}}$. Then 
\begin{align*} \norm{A_{k}(X'_{2}- X_{2})A_{k}^{T}}_{F} 
	&= \norm{P_{k}(Y'_{2} - Y_{2})P_{k}}_{F} \\
	&= \norm{(P_{k} \otimes P_{k})\Vect(Y'_{2} - Y_{2})} \\
	&\leq \norm{P_{k} \otimes P_{k}}_{2}\norm{Y'_{2} - Y_{2}}_{F} \\
	&= \epsilon 
\end{align*} and the claim follows by the triangle inequality.
\item Let $P'_{k} = P_{A'_{k}}$. Then,
\begin{align*} \norm{P'_{k}Y_{2}P'_{k} &- P_{k}Y_{2}P_{k}}_{F} \\
	&\leq \norm{(P'_{k} - P_{k})Y_{2}P'_{k}}_{F} + \norm{P_{k}Y_{2}(P'_{k} - P_{k})}_{F} \\
	&\leq \norm{(P'_{k} \otimes (P'_{k} - P_{k}))\Vect(Y_{2})} + \\
                                       & \hspace{1cm}\norm{(P'_{k} - P_{k}) \otimes P_{k})\Vect(Y_{2})} \\
	&\leq 2 \norm{P'_{k} - P_{k}}_{2} \norm{Y_{2}}_{F} 
\end{align*} Moreover, 
\begin{align*} \norm{P'_{k} - P_{k}}_{2}^{2} 
	&\leq \norm{P'_{k} - P_{k}}_{F}^{2} \\
	&= \norm{P'_{k}}_{F}^{2} + \norm{P_{k}}_{F}^{2} - 2\Tr(P_{k}P'_{k}) \\
	&=2(k - \norm{\cos \Theta(U'_{k}, U_{k})}_{F}^{2}) \\
	&=2\norm{\sin \Theta(U'_{k}, U_{k})}_{F}^{2} \\
	&\leq 8 \frac{\norm{Y_{1} - Y_{1}'}^{2}_{F}}{\delta_{k}^{2}} 
\end{align*} where Theorem \ref{thm:dk} has been used to obtain the last inequality. The claim follows by the triangle inequality.
\end{enumerate}

%\subsubsection{Proof of Theorem \ref{thm:SVDdivperturb}} 
%To prove 1:
%\begin{align*} D(Y_{1}, Y_{2}; k) 	
%	&= \norm{P_{k}(Y_{1} - Y_{2})P_{k}}_{F} \\
%	&\leq \norm{P_{k}(Y_{1} - Y'_{2})P_{k}}_{F} \\ 
%	& \qquad + \norm{P_{k}(Y_{2} - Y'_{2})P_{k}}_{F} \\ 
%	&\leq D(Y_{1}, Y_{2}'; k) + \epsilon			
%\end{align*} 
%Then for 2:
%\begin{align*} P_{k}(Y_{1} - Y_{2})P_{k} 
%	&= P'_{k}(Y'_{1} - Y_{2})P'_{k} \\
%	& \qquad + (P'_{k}Y_{2}P'_{k} - P_{k}Y_{2}P_{k}) \\
%	& \qquad + P'_{k}(Y_{1} - Y'_{1})P'_{k} \\
%	& \qquad + (P_{k}Y_{1}P_{k} - P'_{k}Y_{1}P'_{k}) 
%\end{align*} 
%Therefore we have 
%\[ \norm{P'_{k}Y_{2}P'_{k} - P_{k}Y_{2}P_{k}}_{F} \leq \frac{2^{5/2}\epsilon}{\delta_{k}} \] by the same argument as for part 2 of Theorem \ref{thm:SVDperturb} and a similar bound holds for $\norm{P_{k}Y_{1}P_{k} - P'_{k}Y_{1}P'_{k}}_{F}$. The claim follows.

%\newpage

\section{Reproducibility}

Data and relevant code (R packages \emph{CLARITY} and \emph{CLARITYsim}) for this research work are stored in GitHub: \url{github.com/danjlawson/CLARITY} and have been archived within the Zenodo repository: \url{https://doi.org/10.5281/zenodo.5172063}.

% \subsection*{Figure 1}
% The data was simulated with \emph{CLARITYsim} using:
% \begin{alltt}
% set.seed(1)
% sim=simulateCoalescent(N=100,K=10,
%         alpha=rep(0,10),sigma0=0.0001)
% \end{alltt}
% The model was run with \emph{CLARITY} using:
% \begin{alltt}
% multscan = Clarity_Scan(sim$Y,method=,niter=0)
% \end{alltt}

% \subsection*{Figure 2}
% The data was simulated with \emph{CLARITYsim} using:
% \begin{alltt}
% set.seed(1)
% learned=simulateCoalescent(N=20,K=20,A=diag(20))
% set.seed(1)
% samestructure=transformCoalescent(learned)
% set.seed(1)
% differentstructure=mixCoalescent(learned,beta=0.5)
% \end{alltt}
% The multiplicative model was run with \emph{CLARITY} using:
% \begin{alltt}
% mult_learnedmodel<-Clarity_Scan(learned$Y,
%     method="m",niter=0)
% mult_samepred<-Clarity_Predict(samestructure$Y,
%     mult_learnedmodel)
% mult_differentpred<-Clarity_Predict(samestructure$Y,
%     mult_learnedmodel)
% \end{alltt}
% The SVD model was run with \emph{CLARITY} using:
% \begin{alltt}
% svd_learnedmodel<-Clarity_Scan(learned$Y)
% svd_samepred<-Clarity_Predict(samestructure$Y,
%     svd_learnedmodel)
% svd_differentpred<-Clarity_Predict(samestructure$Y,
%     svd_learnedmodel)
% \end{alltt}

% \subsection*{Figure 3}
% The data in Figure \ref{fig:simexplain}a-f) is simulated using \emph{ClarititySim} using:
% \begin{alltt}
% set.seed(2)
% learned=simulateCoalescent(100,10,
%     alpha=rep(0,10),sigma0=0.05)
% samestructure=transformCoalescent(learned)
% differentstructure=mixCoalescent(learned,beta=0.5)
% \end{alltt}
% and the inference run identically to Figure 2. 
% The data generating Figure \ref{fig:simexplain}g-h) is the same with seeds 1--200.

% Persistences are extracted and summed across a cluster \verb!i! with
% \begin{alltt}
% learned_pers=c_Persistence(c_extract(svdscan))[ilist,]
% \end{alltt}
% where $ilist$ is a list of samples in the cluster $i$. Squared residuals Yresid are summed into clusters using:
% \begin{alltt}
% A=learned$A
% Yresidplot=t(A) %*% Yresid^2 %*% A
% \end{alltt}

% \subsection*{Figure 4}
% The Linguistic data were kindly provided to us by \cite{Longobardi-et-al-2016}. Their paper describes in detail the process by which the data were generated. Dissimilarites were produced assuming Euclidean distances to create two matrices, \verb!lexical! and \verb!phonetic!. Each was further processed to replace the diagonal (which is zero) with the next lowest value, to prevent Clarity from assigning structure to the unique, rather than shared, features. This choice does not influence the features discussed, though does reweight some other features that we have not commented on.

% Usage in \emph{CLARITY} is again straightforward, only requiring the change to the \verb!kmax! parameter:
% \begin{alltt}
% lexical_scan= Clarity_Scan(lexical, kmax=21)
% \end{alltt}
% otherwise all usage is as above.

% \subsection*{Figure 5}
% The Cultural Values Survey and World Bank economic dissimilarity data are open source data. Therefore we have provided them \emph{CLARITY} package (\verb!cvs0! and \verb!wb0!). Again this was processed to replace the diagonal with the next lowest value, but in this case the change is minimal because many countries have similar values. Usage in \emph{CLARITY} is:
% \begin{alltt}
% cv_scan0= Clarity_Scan(cvs0, kmax=108)
% \end{alltt}
% otherwise all usage is as above.



\section{Acknowledgements}

D.J.L is funded by the Wellcome Trust and Royal Society Sir Henry Dale Fellowship, grant no. WT104125MA.
D.J.L and P.E. are supported by the OCSEAN grant funded by the EU Research Executive Agency (Horizon 2020 MSCA RISE 2019 number 873207).
J.D. has been supported by the German Research Foundation (DFG) under FOR 2237 "Words, Bones, Genes, Tools" and by the European Research Council (ERC) under the Horizon 2020 research and innovation programme (CrossLingference, grant agreement no. 834050,
I.Y. has been supported by the German Research Foundation (DFG) under Emmy-Noether-NWG 391377018 and under FOR 2237 "Words, Bones, Genes, Tools".


\end{document}
