%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
%\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS

\usepackage{ifthen}
\usepackage{tikz}
\usetikzlibrary{matrix,calc}

\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{subcaption}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\usepackage{times}
\usepackage{latexsym}
\usepackage{array}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand\m[1]{\textcolor{red}{#1}} 

\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

\usepackage{algpseudocode}
\usepackage{algcompatible}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{array,multirow}
% OR \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage{float}

\newcommand{\mehdi}[1]{\textcolor{cyan}{#1}}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{5pt}}m{#1}}
\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{SortedNet, a Place for Every Network and Every Network in its Place: \\ Towards a Generalized Solution for Training Many-in-One Neural Networks}
% \title{SortedNet: Towards Training Search-Free Zero-Shot Many-in-One Neural Networks}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    Mojtaba Valipour\textsuperscript{\rm 1,\rm 2},
    Mehdi Rezagholizadeh\textsuperscript{\rm 2},
    Hossein Rajabzadeh\textsuperscript{\rm 1,\rm 2},\\
    Marzieh Tahaei\textsuperscript{\rm 2},
    Boxing Chen\textsuperscript{\rm 2},
    Ali Ghodsi\textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}University of Waterloo\\
    \textsuperscript{\rm 2}Huawei Noah's Arc Lab\\
    \{mojtaba.valipour, hossein.rajabzadeh, ali.ghodsi\}@uwaterloo.ca, \\
    \{mehdi.rezagholizadeh, marzieh.tahaei, boxing.chen\}@huawei.com
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org
%
% See more examples next
}

% %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name\textsuperscript{\rm 1},
%     Second Author Name\textsuperscript{\rm 2},
%     Third Author Name\textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Affiliation 1\\
%     \textsuperscript{\rm 2}Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi

\newcommand\moji[1]{\textcolor{red}{#1}} 

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry


\ExplSyntaxOn
\NewDocumentCommand{\avercalc}{O{1}+m}{%
  \clist_set:Nn \l_tmpa_clist {#2}%
  \fp_zero:N \l_tmpa_fp
  \clist_map_inline:Nn  \l_tmpa_clist {
    \fp_add:Nn \l_tmpa_fp {##1}
  }
  \fp_eval:n { round(\l_tmpa_fp/\clist_count:N \l_tmpa_clist, #1)}
}
\ExplSyntaxOff

\ExplSyntaxOn
\NewDocumentCommand{\stdcalc}{O{1}+m}{%
  \clist_set:Nn \l_tmpa_clist {#2}%
  \fp_zero:N \l_tmpa_fp
  \fp_zero:N \l_summation_fp
  \clist_map_inline:Nn  \l_tmpa_clist {
    \fp_add:Nn \l_tmpa_fp {##1}
  }
  \clist_map_inline:Nn  \l_tmpa_clist {
    \fp_add:Nn \l_summation_fp {\fp_eval:n {(##1-{\l_tmpa_fp/\clist_count:N \l_tmpa_clist})^2}}
  }
  \fp_eval:n {round(sqrt(\l_summation_fp/\clist_count:N \l_tmpa_clist), #1)}
  
  % \fp_eval:n { round(sqrt((##1-\fp_eval:n { round(\l_tmpa_fp/\clist_count:N \l_tmpa_clist, #1)})^2/\clist_count:N \l_tmpa_clist), #1)} 
}
\ExplSyntaxOff

\begin{document}

\maketitle

\begin{abstract}
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way (i.e. using the sub-networks of a given network after training), their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes, SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions (e.g. width, depth, blocks) for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and train them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during training that combines random sampling of sub-networks with gradient accumulation to improve training efficiency. Furthermore, the sorted nature of our training leads to a search-free sub-network selection at inference time; and the nested architecture of the resulting sub-networks leads to minimal storage requirement and efficient switching between sub-networks at inference. Our general dynamic training approach is demonstrated across various architectures and tasks, including large language models and pre-trained vision models. Experimental results show the efficacy of the proposed approach in achieving efficient sub-networks while outperforming state-of-the-art dynamic training approaches. Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of our proposed method while maintaining 96\% of the model performance. %The codes of the paper will be released soon.

% ####Version 1
% As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. However, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. 

% This paper proposes a generalized solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. By sorting sub-networks based on their computation/accuracy characteristics along different dimensions we are able to scale the number of sub-networks to hundreds using a single round of training.
% We utilize a novel updating scheme during training that combines random sampling of sub-networks with gradient accumulation to improve training efficiency. Furthermore the sorted nature of our training leads to a search-free sub-network selection at inference time thus enabling practical deployment in real-world scenarios.  The nested architecture of the resulting sub-networks leads to minimal storage requirement and efficient switching between sub-networks at inference.

% Our general dynamic training approach is demonstrated across various architectures and tasks, including large pre-trained language models and vision models. Experimental results show the efficacy of the proposed approach in achieving efficient sub-networks.

% Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of out proposed method while maintaining 98\% of the model performance. Furthermore, our technique allows for identifying the best performing sub-models, which can reach to 99\% of the possible performance
% ####End of Version 1


% Pre-trained language models (PLMs) are highly effective at improving the performance of downstream models. As deploying these models can be computationally expensive on devices with limited resources, several methods have been developed in the literature to extract smaller models.
% % While these days language models are trained and deployed in one particular configuration setting, we are curious to see if we can add the concept of fractal language models 
% To achieve state-of-the-art results, most of these methods require students and teachers to be pre-trained. It is, however, costly to train several smaller models with this two-phase training methods. Our study proposes a novel method for obtaining many smaller sub-models from one big pre-trained model without compromising the performance of downstream tasks. 


% Are we fully harnessing the power of neural networks? The answer is likely no. The human brain's adaptability and complexity set a benchmark that trained neural networks currently struggle to meet, particularly in enabling search-free, zero-shot deployment of sub-models. To address this, we have devised an innovative solution called 'Sorted Networks' which is a training technique being aware of the targeted sub-models of a network and tries to sort the information in these submodules during traning in a probabilistic manner. This training algorithm makes the sub-models of a network being fully functional which enhances the flexibility of their deployment. Our approach works with any model dimension, structure, scale, and architecture—ranging from transformers to stable diffusion. Consequently, our method is adaptable to a variety of use-cases, such as Model Compression, Harmony OS, LMaaS, Cloud-BU, and CBG. Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of out proposed method while maintaining 98\% of the model performance. Furthermore, our technique allows for identifying the best performing sub-models, which can reach to 99\% of the possible performance. The idea has already been patented, and published in EACL 2023 and we target one submission in AAAI. Furthermore, in terms of public recognition, our method has been incorporated into Stable Diffusion; it was a trending topic on Hacker News and deployed in git repositories, like HuggingFace PEFT and Facebook DAdaptation. 
\end{abstract}

\section{Introduction}

\textit{"For every minute spent organizing, an hour is earned." - Benjamin Franklin.}

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/SortedNet_diagram-Page-4-Zoom.drawio.pdf}
    \caption{SortedNet: The overall diagram of our proposed method. During training, in each iteration, we sample from a pre-defined random distribution which will help us to optimize the sub-models as well as the original model.}
    \label{fig:DyLoRA}
\end{figure*}


There has been a remarkable growth in the size of deep neural networks. %However optimmal deployments depends on on the specific hardware availability and accuracy/time requirements of applications, whether deployed in the cloud or on edge devices. 
Nevertheless, the computation/memory resources allocated to a model at inference depends on the specific hardware availability and the accuracy/time requirements of applications, whether deployed in the cloud or on edge devices.

% While many of deep neural network architectures are modular in design (using similar layers \cite{vaswani2017attention} or blocks \cite{sandler2018mobilenetv2}) but this modularity is not preserved at the time of training. In this paper, inspired by \cite{valipour-etal-2023-dylora}, we propose a solution to this problem by using a new training approach based on sorting the information learned by the model. 

In particular, the computational burden from concurrent processes and battery limitations can significantly impact the resources allocated to a neural network. Moreover, in the era of gigantic pre-trained models the computational demand can vary from task to task. Therefore there is a growing demand for models that can adapt themselves to such dynamic conditions. Unfortunately, conventional neural network training, with its fixed architecture, falls short in adaptive adjusting of the computational load at inference time.

On the other hand, deep neural networks demonstrate modular architectures along various dimensions, like layers and blocks across depth, and neurons and channels and even attention heads along width. This inherent modularity enables the extraction of sub-networks with the same shape as the original model. 
However, the current training methods fail to effectively leverage this modularity, resulting in limited practical advantages of sub-networks in the final product. Consequently, the performance of these sub-networks falls short compared to the main model, making their deployment during inference impractical.

Hence, the challenge lies in harnessing the full potential of modularity in deep neural networks, allowing for the efficient utilization of sub-networks to enhance performance and enable practical deployment in real-world scenarios.

Recent works have proposed a variety of approaches for training dynamic models. These approaches While effective often use a sophisticated training process combined knowledge distillation \cite{hou2020dynabert}, architecture modification \cite{Nunez_2023_WACV}, and redundant subnetwork optimization \cite{fan2019reducing}. Although not explicitly mentioned, an important ingredient shared by all these methods is the attempt to implicitly sort sub-networks along a specific dimension with respect to computation/accuracy. Limiting dynamicity to one or two dimension while leaving other dimensions intact can lead to suboptimal sub-networks. % TODO: add DyLoRA and Nested Dropout  (\cite{valipour-etal-2023-dylora}, \cite{rippel2014learning})s
Inspired by these works (\cite{valipour-etal-2023-dylora}, \cite{rippel2014learning}), in this paper we explore how sorting generalized to all dimensions can provide many in one efficient dynamic models. 
Our solution take advantage from an intrinsic nested sub-networks which are sorted monotonically from bottom to top (i.e. we have only a single instance from each sub-network). This sorted configuration with shared parameters enforces a regular order and consistency in the knowledge learned by sub-networks.
Sorting these sub-networks based on their computation/accuracy characteristics presents the most optimal solution. By organizing the model in this manner, extracting the desired sub-network becomes a search-free process. The use of a predefined sorting order ensures that each targeted sub-network possesses a unique computation overhead, effectively removing optimization of redundant sub-networks from training.
In the resulting nested architecture with shared parameters in a \textit{sorted} manner, each smaller (shallower/narrower) model is a sub-networks of a larger (deeper/wider) model. This will lead to models with sorted accuracy, latency and importance.


To achieve this sorted architecture, during training, we propose a novel updating scheme that combines random sampling of sub-networks with gradient accumulation in order to further reduce the cost of training. With one single training our proposed method can yield multiple models with different capacities. 

Our general dynamic training approach is applicable to any architecture without necessitating any modifications to the original model. The proposed nested architecture offers several benefits, including minimal storage requirements and efficient switching between various computation budgets during inference. 

Through comprehensive empirical studies across different architectures, tasks and dynamicity along various dimensions ranging from width and depth to attention head and embedding layer we show the superiority and generalizabilty of our proposed methods over state of the art dynamic training methods. 


To summarize, the main contributions of this paper are:
\begin{itemize}
\item Introduction of nested sorted network training, which leverages efficient sub-network training through a combination of gradient accumulation and sub-network sampling.
\item Generalization of dynamicity to multiple dimensions through stochastic sampling of sub-networks during training.
\item Outperforming state-of-the-art methods in dynamic training on CIFAR10 \cite{krizhevsky2009learning}. Furthermore, scaling the number of sub-networks to 160 and showcasing the efficacy of our single round training method. 
\item Demonstrating the effectiveness of the proposed method on Large pre-trained language models by dynamic training of the BERT model. %\todo{need more check}


\end{itemize}


%Deep neural networks are typically trained in a way that provides a single model that estimates the output through hierarchical representations of the inputs. As a result, the trained model can only be applied to a single budget. Additionally, the budget remains constant for all scenarios and samples. In many cases, however, the budget for a given application is dynamic. Moreover in context of large pre-trained models different downstream task may require different capacities and performing a separate pre-training for each task is prohibitively expensive.

%In this paper the aforementioned challenges are addressed by enforcing a nested architecture during training. In this nested architecture each smaller ( shallower/narrower) model is a sub-model of a larger (deeper/wider) model. This will lead to models with sorted accuracy, latency and layer importance. To achieve this nested architecture, during training, at each iteration, the loss for all sub models are minimized through back-propagation. A novel scalable methodology has been developed to effectively combine random sampling of sub-models with gradient accumulation in order to further reduce the cost of training. With one single training our proposed method can yield multiple models with different capacities. This simple strategy can resolve several significant challenges for practical deployment scenarios, providing business value for the company. These scenarios are briefly described in the following section. 

% \subsection{Nested Architectures for Efficient training and Dynamic/Anytime Inference}

% Main Plot: During training at each iteration we will choose a sub-model from a pre-defined distribution. In this approach, layers will be sorted, and it is possible to extract several sub-models instead of a single 12 layer model. 

% \subsubsection{TLDR:} A nested model that provide efficient training and full control over the latency and accuracy. 

% figure 1: 

% \begin{figure}
%     \centering
%     \includegraphics{}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}

%\cite{xiao2019autoprune}
%\cite{casperrobustness}

% \section{Background}


% What: Modularity of Neural Networks are not well deployed during training. 

% Why? The training procedure is unaware of the sub-networks. 

% While the architectures of many deep neural networks are modular (i.e. sub-networks of a model has a similar shape to the entire model e.g. Transformer), this modularity of sub-networks is not reflected anywhere in the training. Consequently, after training deep neural nets, the modularity of sub-networks is not of much use in the final product. So we can summarize our problem statement as sub-networks of a model does not perform very well compared to the main model and at the inference time we cannot deploy these sub-models in practice. 

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Figs/Fig1.png}
%     \caption{Modularity of Neural Networks are not well deployed during training. }
%     \label{fig:modularity}
% \end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.45\textwidth]{Figs/compare.png}
    \caption{Comparing SortedNet and Once For All: on a hypothetical 5-layer network, we show how the sub-network selection strategy of SortedNet differs from the Once-for-All~\cite{cai_once-for-all_2020} approach.}
    \label{fig:comp}
\end{figure}

\section{Related Work}
\label{ref::related}
%Modular Deep Learning: \cite{pfeiffer2023modular}
%Order in the Brain: \cite{konstantinides2022complete}


% \begin{table*}[h]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{|m{4.0cm}| x{2.2cm} c c x{1cm} x{2cm} x{1.5cm}  x{2cm} c x{2cm}| }
% \hline
% \textbf{Method}& \textbf{Sub-Networks: Config.(\#)} &  \textbf{Performance} & \textbf{Anytime}  & \textbf{Search-Free} & \textbf{\# of Training Params} & \textbf{No Re-training} &  \textbf{Training Loss} & \textbf{Target Dim.} & \textbf{Architecture}\\
%  % &  &  &  &  &  &   &  &  \\
% \hline
% \hline\vspace{2pt}
% Early Exit \cite{xin2020deebert} & Sorted (Few) & Low & \checkmark&\checkmark & $|\theta|$ & \xmark  & & Depth & Transformer \\ 
% \hline \vspace{2pt}
% Layer Drop \cite{fan2019reducing} & Random (Many) & Low &\xmark & \xmark &$|\theta|$ & \xmark & & Depth& Transformer \\ 
%  \hline \vspace{2pt}
% DynaBERT \cite{hou2020dynabert} & Sorted \& Random (Few) & High& \xmark &\xmark & $2|\theta|$ & \xmark & & Depth \& Width & Transformer  \\ 
% \hline \vspace{2pt}
% Once for All \cite{cai2019once}& Nested (Many) & High & \xmark &\xmark & $|\theta|$ or $2|\theta|$ &  \xmark & & General & CNN \\ 
% \hline \vspace{2pt}
% LCS \cite{Nunez_2023_WACV} & Arbitrary (Many) & High &  \checkmark& & $|\theta|$ or $2|\theta|$ &  \checkmark & & General & CNN \\ 
% \hline \vspace{2pt}
% Slimmable ~\cite{yu2018slimmable} & Sorted (Few) & Moderate & \checkmark &\checkmark &$|\theta|$ &  \checkmark & & Width & CNN\\ 
% \hline \vspace{2pt}
% \textbf{Sorted Nets (Ours)} & Sorted (Many) & High & \checkmark & \checkmark & $|\theta|$ & \checkmark&  &General & CNN \& Transformer\\ 
% \hline 
% \end{tabular}}
% \caption{Comparison of different existing related work and distinguishing our solution}
% \label{tab:RelatedWork}
% \end{table*}

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{  
\begin{tabular}{|m{4.0cm}| x{2.2cm} c c x{1cm} x{2cm}  x{2cm} c x{2cm}| }
\hline
\textbf{Method}& \textbf{Sub-Networks: Config.(\#)} &  \textbf{Performance} & \textbf{Anytime}  & \textbf{Search-Free} & \textbf{\# of Training Params} & \textbf{No Re-training} & \textbf{Target Dim.} & \textbf{Architecture}\\
 % &  &  &  &  &  &   &  &  \\
\hline
\hline\vspace{2pt}
Early Exit \cite{xin2020deebert} & Sorted (Few) & Low & \checkmark&\checkmark & $|\theta|$ & \xmark  & Depth & Transformer \\ 
\hline \vspace{2pt}
Layer Drop \cite{fan2019reducing} & Random (Many) & Low &\xmark & \xmark &$|\theta|$ & \xmark & Depth& Transformer \\ 
 \hline \vspace{2pt}
DynaBERT \cite{hou2020dynabert} & Sorted \& Random (Few) & High& \xmark &\xmark & $2|\theta|$ & \xmark & Depth \& Width & Transformer  \\ 
\hline \vspace{2pt}
Once for All \cite{cai2019once}& Nested (Many) & High & \xmark &\xmark & $|\theta|$ or $2|\theta|$ &  \xmark & General & CNN \\ 
\hline \vspace{2pt}
LCS \cite{Nunez_2023_WACV} & Arbitrary (Many) & High &  \checkmark& \checkmark & $|\theta|$ or $2|\theta|$ &  \checkmark & General & CNN \\ 
\hline \vspace{2pt}
Slimmable ~\cite{yu2018slimmable} & Sorted (Few) & Moderate & \checkmark &\checkmark &$|\theta|$ &  \checkmark & Width & CNN\\ 
\hline \vspace{2pt}
\textbf{SortedNet (Ours)} & Sorted (Many) & High & \checkmark & \checkmark & $|\theta|$ & \checkmark  &General & CNN \& Transformer\\ 
\hline 
\end{tabular}}
\caption{Comparison of different existing related work and distinguishing our solution}
\label{tab:RelatedWork}
\end{table*}

In this section, we briefly review the most relevant existing works to our SortedNet idea. A summary of these solutions and how they are different from each other can be found in Table~\ref{tab:RelatedWork}. 

\paragraph{ Slimmable Networks~\cite{yu_slimmable_2018}}
Slimmable networks is an idea to train a single neural network in a way that it can be deployed with adjustable width at the inference time. This solution was proposed particularly for CNN architectures and thus, careful consideration of the batch normalization module for various width sizes is necessary. In this regard, they use switchable batch normalization which leads to additional trainable parameters. In contrast to slimmable networks, our SortedNet are architecture agnostic and work in both depth and width dimensions. 
% \moji{It's good to note that Slimmable Networks is the summation loss!}

\paragraph{Early Exit~\cite{xin2020deebert}} Early exit refers to a technique which adds a classifier to intermediate layers of an already trained neural network. While the parameters of the main model are frozen, the parameters of the classifiers are updated in a separate fine-tuning process. In this approach, each of the classifiers and their subsequent network can be treated as an independent sub-model. While this solution is relatively straightforward, the performance of the sub-models lags significantly behind that of the main model.

\paragraph{Dayna-BERT~\cite{hou2020dynabert}}
Dyna-BERT is a dynamic compression method for pre-trained BERT models, allowing flexible adjustments of the size of the model along depth and width at the inference time. While the introduced objective in the DynaBERT paper has some overlap with ours, there are several key differences: first, in DynaBERT,  Only a few subsets of the model are functional (while our SortedNet do not have such an assumption); second, DynaBERT needs an already trained teacher and uses knowledge distillation but our technique does not need KD; third, DynaBERT needs search during both training and inference while our solution is \textit{search-free}; Lastly, DynaBERT's applicability is architecture-dependent, whereas our approach is not.


\paragraph{Layer-drop~\cite{fan2019reducing}} 
Layer-drop is a structured dropout solution at the training time which allows layer pruning at the inference time. Similar to DynaBERT, this solution is applied to pre-trained language models; however, in contrast to DynaBERT, Layer-drop only targets the depth of neural networks and not their width. 
In Layer-drop, there is no fixed training pattern and any layer can be dropped with a certain probability, which is referred to as drop rate. At the inference time, the number of active layers can be adjusted by the drop-rates that are seen during the training time of that network (i.e. to achieve the best performance on any other drop-rate value, the network needs to be re-trained.). Layer-drop works only in depth while our solution works for both depth and width. Moreover, Layer-Drop requires specific search patterns for dropping layers at the inference time and training time, whereas our solution is search free. %does not have such a hyperparameter. It needs search at the training time for the layer dropout strategies and at the inference time to select the best layers.
 

\paragraph{Once-for-All~\cite{cai_once-for-all_2020}}
Once-for-all(OFA) targets efficient inference across different devices by first training an OFA network which supports many sub-networks with varying latency/accuracy characteristics ; it then searches among the feasible sub-networks according to the accuracy and latency requirements of their target device. OFA has a progressive training nature i.e. it goes from the largest model to the smaller sub-networks. OFA is different from our solution from the following aspects: first, it needs teacher and knowledge distillation; second, OFA requires a separate Neural Architecture Search (NAS) at the inference time; third, OFA is not architecture agnostic (their solution is for CNN-based neural networks while our SortedNet works for both CNNs and Transformers). Moreover, OFA is different from our solution in terms of the sub-network selection strategy. While our SortedNet selects sub-networks in a sorted manner, OFA does not have any particular assumption for sorting sub-networks (see Fig.~\ref{fig:comp} for more details).   
% Also the arbitrary choice of procedural training will make it difficult to generalize this solution. 

\paragraph{Learning Compressible Subspace~\cite{Nunez_2023_WACV}}
Learning Compressible Subspace (LCS) is an adaptive compression technique based on training compressible subspace of neural networks (using a linear convex combination of two sets of weights for the network). While LCS does not require any re-training at the inference time,  this solution has some other limitations including: first, it needs double memory at the training time; second, the choices of initial weights and the compression function are unclear and arbitrary (left as a hyper-parameter); third, it is only tried on CNNs; forth, similar to Layer-drop intermediate sub-networks are trained randomly which will make the performance of the target model sub-optimal.

% \paragraph{TIPS: Topologically Important Path Sampling for Anytime Neural Networks
% \cite{li2023tips}}
% % https://arxiv.org/pdf/2305.08021.pdf
% We don't need to search a huge search space as we fixed the architecture search by sorting the information in different sub-models. 

% DynaBERT~\citep{hou2020dynabert}
% \citep{xin2020deebert}
%\citep{fan2019reducing}


\section{Methodology}
\label{sec:method}
\subsection{A Generalized and Scalable View }
In the related work section, we have discussed several approaches concerning the training of many-in-one networks. These approaches differ in terms of their target architecture, training loss, number of training parameters, configuration of the sub-networks (random or sorted), the number of trained sub-models, and reliance on search or re-training before deployment. 
Our SortedNet method can be viewed as a simple general and scalable version of these existing solutions. These generalization and scalability have been mostly resulted from the sorted configuration of sub-networks with their shared parameters and our stochastic training. To the best of our knowledge this is the first work which has scaled training of sorted nested sub-networks to various dimensions.

% \subsection{Batch normalization}

% \subsection{Efficient weight update via gradient accumulation}

% \subsection{Generalization to sub-networks of arbitrary structure}

\subsection{SortedNet: Towards a Generalized Solution for Training Many-in-One Networks}
While many of deep neural network architectures are modular in design (using similar layers such as in Transformers~\cite{vaswani2017attention} or blocks such as in MobileNet~ \cite{sandler2018mobilenetv2}) but this modularity is not preserved during training. 
In this subsection, we introduce our SortedNet solution which aims at training a generalized and scalable many-in-one networks. In order to train many-in-one networks, we need to specify a few design choices: first, how to form the sub-networks and their configurations; second, what are the target architectures; and third, how to train the sub-networks along with the main model. 


\paragraph{Designing the Sub-networks}
SortedNet impose an inductive bias on the training based on the assumption that the parameters of sub-networks across each dimension have a concentric (or onion shape) architecture with shared parameters in a \textit{sorted} manner. 
% Our solution has an intrinsic nested architecture which are sorted in a monotonical order from bottom to top. 
This sorted configuration with shared parameters enforces a regular order and consistency in the knowledge learned by sub-networks (see Fig.~\ref{fig:DyLoRA}).

% \todo{seems not accurate! We are soring the information along these dimensions} 
% We call the model sorted because the sub-networks are ordered in a nested way that always smaller sub-networks are a subset of any larger sub-networks and the sub-networks always start from the input layer (see Fig.~\ref{fig:DyLoRA}).  
Let's consider a many-in-one neural network $f(x;\theta (n))$ with the parameters $\theta(n)$ and the input $x$ which is comprised of $n$ sub-networks $f(x;\theta (i)) |_{i=0}^{n-1}$, where $\theta(i)$ represents the weights of the $i^{\text{th}}$ sub-model. We define a universal set which contains all unique sub-models: $\Theta= \{\theta(0), \theta(1), ..., \theta(n) \}$.  

\paragraph{How to build the sub-models?} Suppose that we would like to target $D = \{ Dim_1, Dim_2, ..., Dim_K\}$ many-in-one dimensions in our model. Then, let's start with $\Theta = \varnothing$ and build the sub-models iteratively. In this regard, at each iteration $t$ during training, we have a sampling and truncation procedures along any of the targeted dimensions: 
\begin{equation}
\begin{split}
        & \theta_t^* = \cap_{j=1}^{|D|} \theta_{Dim_j\downarrow b_j^t}(n)  \\ 
        & \text{where } b_j^t \sim P_{B_j} \\ 
        &\text{IF } \theta_t^*  \notin \Theta \text{ : } \Theta \leftarrow \Theta \cup \{ \theta_t^* \} 
\end{split}
\label{eq:1}
\end{equation}
where $Dim_j\downarrow b_j^t$ indicates that we have truncated $\theta(n)$ along the $Dim_{j}$ dimension up to the index $b_j^t$ at the iteration $t$. $b_j^t$ is sampled from a distribution $P_{B_j}$ with the support set of $B_j = \{1,2, ..., d_j\}$ to form the $i^{\text{th}}$ sub-model. $d_j$ refers to the maximum index of the $j^{\text{th}}$ dimension. This iterative process will be done during training and the set of $n$ unique sub-models $\Theta$ will be built.

To illustrate the process better, let's see a simple case such as $\text{BERT}_{base}$ where we want to make a many-in-one network across the width and depth dimensions, $D = \{ \text{Depth}, \text{Width} \}$. In this case, we have 12 layers and the hidden dimension size of 768. Suppose that $Depth$ corresponds to $j=1$ and $Width$ corresponds to $j=2$ in Eq.~\ref{eq:1}. For simplicity, let's use a discrete uniform distribution for sampling indices across these two dimensions. To create the first sub-network ($i=1$), we need to sample $b_1^1$ uniformly from the set of natural numbers in the range of 1 to 12: $B_1 = \{1,2,..., 12\}$; and we need to sample $b_2^1$ from the range of 1 to 768: $B_2 = \{1,2,3,..., 768\}$. Bear in mind that we can even choose a subset of $B_1$ and $B_2$ as the support set for sampling probability distribution. After these two samplings, we will have two truncated sets of parameters: $\theta_{Depth \downarrow b_1^1 }$ and $\theta_{Width \downarrow b_2^1 }$. The intersection of these two truncated parameters will give us the first sub-network:  
$\theta_1 = \theta_{Depth \downarrow b_1^1} \cap \theta_{Width \downarrow b_2^1} $.   


% Then, our objective is to train the many-in-one network such that its nested sub-networks have the following characteristics: 

% \begin{itemize}
%     \item Nested Property: $\theta_0 \subseteq \theta_1 \subseteq ... \subseteq \theta_n$
%     \item Size: $|f_0(x;\theta_0)| < |f_1(x;\theta_1)| < ... < |f_n(x;\theta_n)|$
%     \item Latency/Computation: \\ $\tau(f_0(x;\theta_0)) < \tau(f_1(x;\theta_1)) < ... < \tau(f_n(x;\theta_n))$
%     \item Performance: $\pi(f_0(x;\theta_0)) < \pi(f_1(x;\theta_1)) < ... < \pi(f_n(x;\theta_n))$
% \end{itemize}
% where $\tau(.)$ is the latency cost, and $\pi(.)$ is the performance of the model.
% In our \todo{need attention}

\paragraph{Training Paradigm}
Regular training of neural networks concerns improving the performance of the whole model and usually this training is not aware of the performance of the sub-networks. In fact, in this scenario, if we extract and deploy the sub-models of the trained large model on a target task, we would experience a significant drop in the performance of these sub-networks compared with the main model. 
However in SortedNet, we propose a training method that allows for training sub-networks together with the main model in a stochastic way. 
The SortedNet paradigm leads to the following benefits: 
\begin{itemize}
    % \item The cost of obtaining sub-models is equal to training the large model only once. This leads to significant reduction to SOTA works that require separate training for each sub-model. 
    % \item Minimize the performance drop of the large model compared to normal training. 
    \item Search-free sub-model extraction: after training, by importance sorting of sub-models the best sub-model for a given budget can be selected without the need for search. 
    
    \item Anytime: Each smaller sub-model is a subset of a larger one which makes switching between different sub-models efficient.
    This leads to an important feature of our SortedNet which is so-called \textit{anytime} that is a network which can produce its output at any stage of its computation. 
    
    \item Memory efficient:  we train a many-in-one network where sub-models are all part of a single checkpoint, which minimizes storage requirement.
\end{itemize}





\begin{algorithm}[htb!]
\caption{SortedNet}\label{alg:two}
\begin{algorithmic}
% \STATE $R=Rank$ 
\REQUIRE \\
Define the target dimensions for sorting $D= \{ Dim_1, Dim_2, ..., Dim_K \}$, for each given dimension we need the set of target indices: $B_j = \{ 1,2, ..., d_j \} $; the discrete sampling distribution $P_{B_j}$ for each dimension needs to be decided;\\ 
$iter$: the number of training iterations; $g_{acc}$: Gradient Accumulation Steps; $\eta$: Learning Rate, $\mathcal{L}$: Loss Function; 
% $W \in \mathbb{R}^{r \times d}$ 
% \REQUIRE $\mathcal{L}^{\mathcal{DY}}$
% \REQUIRE $f$: Update a subset of params
\WHILE {t $<$ $iter$}:
    \STATE {\color{blue} Building Sub-models:}
    \STATE {\color{gray} // sample indices across all target dimensions}
    \STATE $b_j^t \sim P_{B_j}$  for $j \in \{1,2, ..., |D|\}$ 
    \STATE {\color{gray} // truncate the parameters $\theta(n)$ along each dimension}
    \STATE $\theta (t) = \cap_{j=1}^{|D|} \theta_{Dim_j\downarrow b_j^t}(n) $
    \STATE {\color{gray} // alternatively one can provide a list of settings and randomly sample from a single distribution}
    % \STATE {\color{blue} Forward:}
    % \STATE {\color{gray} // calculate the loss}
    \STATE {\color{blue} Forward \& Backward:}
    % \STATE $\Delta W = \nabla_W L$
    \STATE $\underset{\theta_t^*}{\min} ~\mathcal{L} \triangleq \text{L}(y, f(x,\theta_t^*))$
    \IF{$t \mod g_{acc}==0$}
        \STATE {\color{gray} // update the parameters}
        \STATE {$\theta_t^* \leftarrow \theta_t^* - \eta \nabla_{\theta_t^*} \mathcal{L}$}
    \ENDIF
\ENDWHILE

\end{algorithmic}
\label{alg}
\end{algorithm}


For efficiency purposes, in our training, the last layer, e.g. the  classification layer, is shared between all sub-models; alternatively we can add a separate classification layer to each sub-model. For simplicity and efficiency, we chose to use a shared classification layer. %The gain was not significant in our experiments and therefore, we chose to use the more efficient choice.


% \begin{figure}[h]
% \centering  
% \includegraphics[width=0.5\textwidth, trim = 1cm 15cm 0cm 1cm]{Figs/fig_method.pdf}
% \caption{A simple depiction of how sub-models are selected at each iteration during training SortedNet. The highlighted parts of the network (with purple) shows the selected sub-networks for training at each iteration. }
% \label{fig:method}
% \end{figure}


\subsection{SortedNet Algorithm}

In this subsection, we describe our proposed training algorithm. For training a SortedNet with $n$ sub-models, at each iteration during training, a random index needs to be sampled from a pre-defined distribution: $b_j^i \sim P_{B_j}$. 
% where $1 \leq b_i \leq n$ and $\theta_0 \subseteq \theta_1 \subseteq ... \subseteq \theta_n$. 
After finding the target sub-model $\theta_t^*$ at each iteration, we can use one of the following objectives to update the parameters of the selected sub-model: 

% \begin{itemize}
%     \item Only train the sub-model $i$ layers ($SM_{b_i}$): \\    
%     $\underset{\theta_i}{\min} ~\mathcal{L} \triangleq \text{CE}(y, SM_{b_i}(x;\theta_i) )$
%     \item Train the sub-models that are subset of $SM_{b_i}$ i.e. from $SM_1$ to $SM_{b_i}$: \\    
%     $\underset{\bigcup_{i=1}^{b_i} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{b_i} \text{CE}(y, SM_{i}(x;\theta_i) )$
% \end{itemize}

\begin{itemize}
    \item (Stochastic Loss) Only train the selected sub-model $f(x,\theta_t^*)$ : \\$\underset{\theta_t^*}{\min} ~\mathcal{L} \triangleq \text{L}(y, f(x,\theta_t^*))$
    where $L$ is the loss function for training the model on a given task (e.g. $L$ can be a regular cross entropy loss) and $y$ refers to the ground-truth labels.
    \item (Stochastic Summation) Train the sub-model $f(x,\theta_t^*)$ and all its targeted sub-models along each dimension. Let's assume that $\Theta^\perp (\theta_t^*)$ is the universal set for all targeted sub-networks of $\theta_t^*$. Then the loss function can be defined as:  \\
    $\underset{\Theta^\perp (\theta_t^*)}{\min} ~\mathcal{L} \triangleq \sum_{\theta \in \Theta^\perp (\theta_t^*)} \text{L}(y, f(x,\theta) )$
    % \item Train the sub-models $SM_{1,w_i}$ to $SM_{L,w_i}$: \\
    % $\underset{\bigcup_{i=1}^{L} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{L} \text{CE}(y, SM_{i,w_i}(x;\theta_i) )$. 
\end{itemize}

This way, one sub-model or a subset of sub-models are updated in each iteration. Alternatively, one can choose to train all the sub-models at each iteration which is costly in the large scale. %(see Fig. \ref{fig:method})

% Alternatively, we can train all the sub-models at each iteration using the following objective:
% \begin{itemize}
%     \item Train the sub-models $f_1(x,\theta_{1})$ to $f_b{_i}(x,\theta_{b_i})$: $\underset{\bigcup_{i=1}^{L} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{L} \text{L}(y, f_i(x,\theta_{i}))$. 
% \end{itemize}


% for depth-wise sorted training :

 
% \begin{itemize}
%     \item Only train the sub-model with $b_i$ layers ($SM_{b_i}$): \\    
%     $\underset{\theta_i}{\min} ~\mathcal{L} \triangleq \text{CE}(y, SM_{b_i}(x;\theta_i) )$
%     \item Train the sub-models that are subset of $SM_{b_i}$ i.e. from $SM_1$ to $SM_{b_i}$: \\    
%     $\underset{\bigcup_{i=1}^{b_i} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{b_i} \text{CE}(y, SM_{i}(x;\theta_i) )$
% \end{itemize}



% This way one sub-model or a a subset of sub-models are updated in each iteration (see Figure \ref{fig:emp1}). Alternatively, we can train all the sub-models at each iteration using the following objective:
% \begin{itemize}
%     \item Train the sub-models $SM_1$ to $SM_L$: $\underset{\bigcup_{i=1}^{L} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{L} \text{CE}(y, SM_{i}(x;\theta_i) )$. 
% \end{itemize}

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Figs/Fig2.png}
%     \caption{?? }
%     \label{fig:emp1}
% \end{figure}

%\subsubsection{The technical benefit(s) / advantage(s) of Embodiment }
% \subsubsection{Detailed description of Embodiment 2}
% As for Embodiment 2 of this invention, we propose sorted importance training across width. 

% Similar to Embodiment 1,  the last layer is shared between sub-models for efficiency. 
% We sample a random index for the width of the model, i.e. $w_i \sim P_w(.)$ where $1 \leq w_i \leq L$.
% We then update the parameters of the model based on one of the following schemes:
% \begin{itemize}
%    \item We only update $SM_{w_i}$ using: \\ $\underset{\theta_i}{\min} ~\mathcal{L} \triangleq \text{CE}(y, SM_{w_i}(x;\theta_i) )$
%   \item we sample a random index for the width of the model and train the sub-models $SM_{w_1}$ to $SM_{w_i}$: \\
%   $\underset{\bigcup_{i=1}^{b_i} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{j=1}^{i} \text{CE}(y, SM_{w_j}(x;\theta_i) )$
% \end{itemize}

% Alternatively, we can update all the sub-models of the original model i.e. $SM_{w_1}$ to $SM_{w_L}$ using the following objective:
% \begin{itemize}
% \item $\underset{\bigcup_{j=1}^{w_i} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{L} \text{CE}(y, SM_{w_j}(x;\theta_i) )$. 
% \end{itemize}


% \paragragh{Embodiment 2}
% \subsubsection{Detailed description of Embodiment 3}
% As for Embodiment 3 of this invention, we enforce the sorted importance training across width and depth. 

% Similar to Embodiment 1 and 2,  the last layer is shared between sub-models for efficiency.  At each time-step during training, sample a random index for the layers ($b_i \sim P_B(.)$ where $1 \leq b_i \leq L$) and we also sample a random index for the width of the model: $w_i \sim P_w(.)$ where $1 \leq w_i \leq L$. \\  
% We then update the model parameters based on one of the following schemes: \\ 
% \begin{itemize}
%     \item Only train the selected sub-model $SM_{b_i, w_i}$ : \\$\underset{\theta_i}{\min} ~\mathcal{L} \triangleq \text{CE}(y, SM_{b_i, w_i}(x;\theta_i) )$
%     \item Train the sub-models $SM_{1,w_i}$ to $SM_{b_i,w_i}$:\\ 
%     $\underset{\bigcup_{i=1}^{b_i} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{b_i} \text{CE}(y, SM_{i,w_i}(x;\theta_i) )$
%     \item Train the sub-models $SM_{1,w_i}$ to $SM_{L,w_i}$: \\
%     $\underset{\bigcup_{i=1}^{L} \theta_i}{\min} ~\mathcal{L} \triangleq \sum_{i=1}^{L} \text{CE}(y, SM_{i,w_i}(x;\theta_i) )$. 
% \end{itemize}


%\subsubsection{The technical benefit(s) / advantage(s) of Embodiment }



% \begin{figure}[htb!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Figs/Fig3.png}
%     \caption{?? }
%     \label{fig:emb2}
% \end{figure}


% \section{Key Points of Your Solution and What You Want to Protect}

% The key protected point in this patent is the new approach for training neural networks to preserve their modularity. So our algorithm for training neural networks in depth and width needs to be protected.   

% 1- Compared to regular training: 
% Instead of training the entire model, we train sub-models
% In contrast to regular training, our sub-models are also functional and can compete with the main model \\

% 2- Compared to “nested dropout”: 
% Nested dropout is for adding order to data  representations, 
% Not used for sorting neural network modules, and Different training objective \\ 


% By enforcing a specific order over all the sub-models, we can enjoy several advantages including:

% \subsection{Efficient Training:} 

% There is a rapid increase in the size of language models in order to achieve better generalization in both few shot and non-few shot settings. However, the desired memory/computation budget will differ depending on the available hardware and the accuracy/time sensitivity of the application operating in the cloud or on edge devices. Models with different sizes are trained from scratch or compressed using various compression methods in order to meet the needs of different budgets. There is a high computational cost associated with each of these scenarios. In the proposed methodology, a single training is sufficient to generate numerous models without requiring additional computations. Therefore, it is possible to achieve training efficiency, both for pre-training and for fine-tuning. With respect to pre-training, this nested method allows us to provide multiple models for the same cost as one pre-training for large language models. Depending on the downstream task requirements, the appropriate sub-model can be selected and fine-tuned. Our proposed nested architecture fine-tuning is suitable when a single pre-trained checkpoint is available but we need multiple models for a given downstream task (dynamic inference). We use the proposed idea in the fine-tuning stage to provide us with multiple fine-tuned models at the cost of one single fine-tuning. The resulting sub-models can also be fine-tuned individually to further improve accuracy.  

% \subsection{Dynamic Inference:} 

% In dynamic inference scenarios the budget for a given application changes dynamically (Harmony OS). In the proposed training scheme for a L-layer model, we have L sub-models, each with increasing latency/accuracy. The sub-models are all part of a single checkpoint, which minimizes storage requirements. 


% \subsection{Anytime Inference: } 

% Anytime algorithms are algorithms that can return valid solutions to problems, even if their execution is interrupted before they are completed. With time, the algorithm is expected to find better and better solutions. As the budget is unknown at the time of execution, it is possible for the budget to be cut suddenly in an anytime setting. For example, when the server is overwhelmed by a large number of requests, it may delay requests to avoid catastrophic failures, causing dissatisfaction among users. The proposed method, however, allows us to provide models that improve over time in terms of performance. Therefore, if the budget is increased, the output of one model can be refined without wasting resources or recalculating any operations. In this way, a minimum performance can be guaranteed, and if more budget is available, the performance can be refined. 

% \subsection{Sample Efficient Inference:} 

% NLP applications have been significantly improved by large-scale pre-trained language models such as BERT. They are, however, also known for their slow inference speed, which makes deployment in real-time applications difficult. With our approach, samples can exit earlier without passing through the entire model if they meet a minimum performance requirement to conserve time and speed up inference. 

\section{Experiments}

% \begin{itemize}
%     \item Motivation
%     \item with and without the Dynamic approach
%     \item No need for KD 
%     \item No need for pre-training
%     \item Ablation study: stochastic and batch objective function
%     \item Method's comparison: requirement phases
% \end{itemize}

% \subsection{Implicit or Explicit Sorting? How we can sort the learned information?}

% \subsubsection{Cumulative Activation Function?}

% Implicit sorting by changing the training procedure can be advantageous as it won't be necessary to change the network architecture which might affect the performance. Explicit sorting, however, will guarantee the sorted behavior. However, in this experiment we explicitly sort the output of each layer by using a cumulative activation function to better see the effect of sorting. This is not directly comparable to information sorting but as we sort the neuron activation, we basically sort their effectiveness and it might lead to the same behaviour that we want. 

% \subsection{Why not ordinary training?}

% \subsection{Is the proposed method scalable?}

% In this experiment, first we tried to train 160 different sub-models simultaneously, and after we find the most promising models we simply trained the desired sub-models and we reported the results. 

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccccccc}
% \hline
% Depth/Width & 10\% & 20\% & 30\% & 40\%  & 50\%  & 60\% & 70\% & 80\%  & 90\% & 100\% & Depth Only & W/O Our Method\\
% \hline
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 2 Blocks & 22.41 & 24.91 & 25.67 & 31.08  & 35.37  & 32.67 & 35.76 & 36.41  & 34.81 & 33.97 & 38.79 & 9.64 \\
% 1 Blocks & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% 1 Block & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% Width Only & 19.99 & 21.07 & 29.40 & 34.20  & 36.04  & 34.88 & 35.76 & 35.84  & 37.68 & 37.91 & 25.60 & 10.85 \\
% \hline
% \end{tabular}}
% \caption{CIFAR10 Classification Accuracy (Percentage) for SortedNet and without our proposed method}
% \label{tab:scalable}
% \end{table*}

In this section, we discuss a set of experiments that we conducted to show the effectiveness and importance of sorting information and fixing a nested property. The details of the hyperparameters for each experiment can be found in Appendix \ref{ap:hyperparameters}.

% \subsection{Baselines}

% \paragraph{LCS\_p:}

% \paragraph{NS:}

% \paragraph{US:}

% \paragraph{MobileNetV2:}

% \paragraph{Bert:}


%TODO: UNCOMMENT
\begin{figure}
\hspace*{-1cm}
\centering
\resizebox{1.1\columnwidth}{!}{  
\input{cifar10-160.tex}}
\caption{CIFAR10 Classification Accuracy (and Recovery Percentage) for Sorted-Net (160 Models) and the baseline. In each cell, we reported the performance of the sub-network (top) and the recovery percentage of the model (bottom) with respect to the baseline largest model performance. W. Only: Sorting only the widths, D. Only: Sorting only the depth.}
\label{fig:scalable160}
\end{figure}

%TODO: UNCOMMENT
\begin{figure}
\centering
\resizebox{\columnwidth}{!}{  
\input{cifar10-20.tex}
}
\caption{CIFAR10 Classification Performance for the best-performing subset of sub-networks trained by SortedNet from scratch.}
\label{fig:bestsubset}
\end{figure}

\begin{table*}[htb!]
\centering
\resizebox{\textwidth}{!}{  
\begin{tabular}{lcc|ccc|ccc}
\hline
Network & Width & FLOPs & NS-IN & LCS-p-IN  & SortedNet-IN & NS-BN  & LCS-p-BN (aka US) &SortedNet-BN\\
\hline
\multirow{4}{*}{cpreresnet20 \cite{he2015deep} (CIFAR10)} & 100\% & 301M & 88.67 & 87.61 & \textbf{89.14} & 79.84  & 65.87  & \textbf{85.24}\\
& 75\% & 209M & 87.86 & 85.73 & \textbf{88.46} & 78.59  & \textbf{85.67} &85.29\\
& 50\% & 97M & 84.46 & 81.54 &\textbf{85.51} & 69.44  & 65.58  &\textbf{70.98} \\
& 25\% & 59M &75.42 & \textbf{76.17} &75.10 & 10.96  & \textbf{15.78}  &12.59\\
\hline 
avg. & -& - & 84.10 & 82.76 & \textbf{84.55} & 59.70 & 58.22 & \textbf{63.52}\\
\hline
\end{tabular}}
\caption{Comparing the performance of state-of-
the-art methods with Sorted-Net over CIFAR10 in terms of test accuracies.}
\label{tab:CIFAR10-lcs}
\end{table*}

\begin{figure*}[htb!]
    \centering
    \resizebox{1.0\textwidth}{!}{  
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         % \includegraphics[width=1.1\textwidth]{Figs/acc/plot-accPerWidth-0.25.png}
         \includegraphics[width=1.1\textwidth]{Figs/loss/plot-lossPerWidth-0.25.png}
         \caption{}
         \label{fig:accloss0.25}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         % \includegraphics[width=1.1\textwidth]{Figs/acc/plot-accPerWidth-0.5.png}
         \includegraphics[width=1.1\textwidth]{Figs/loss/plot-lossPerWidth-0.5.png}
         \caption{}
         \label{fig:accloss0.5}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         % \includegraphics[width=1.1\textwidth]{Figs/acc/plot-accPerWidth-0.75.png}
         \includegraphics[width=1.1\textwidth]{Figs/loss/plot-lossPerWidth-0.75.png}
         \caption{}
         \label{fig:accloss0.75}
     \end{subfigure}
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         % \includegraphics[width=1.1\textwidth]{Figs/acc/plot-accPerWidth-1.0.png}
         \includegraphics[width=1.1\textwidth]{Figs/loss/plot-lossPerWidth-1.0.png}
         \caption{}
         \label{fig:accloss1.0}
     \end{subfigure}
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         % \includegraphics[width=1.1\textwidth]{Figs/acc/plot-AvgAcc.png}
         \includegraphics[width=1.1\textwidth]{Figs/loss/plot-AvgLoss.png}
         \caption{}
         \label{fig:acclossavg}
     \end{subfigure}}
\caption{Comparing the training loss trajectory of SortedNet on CIFAR10 for different gradient accumulation values with LCS\_p. Each subfigure demonstrates the results in different widths. The rightmost subfigure reports the average across the widths. The underlying network (cPreResNet20) and hyperparameters are fixed.}
\label{fig:acc_loss_combined}
\end{figure*}


\subsection{Is SortedNet scalable?}

To show that our proposed method is scalable, we designed an experiment that try to train 160 different models across multiple dimensions (width and depth) all at once. As baseline, we trained the largest network (a MobileNetV2), and reported the best performance of the model. Because the performance of the model was poor for all the other sub-models (less than 12\%), we trained the classifier layer for 5 more epochs before evaluating each sub-model for the baseline and reported the best performance. As the results suggests in figure \ref{fig:scalable160}, our method was able to capture the maximum performance for many of these sub-models in a zero-shot manner. In each cell, we reported the performance of the sub-network on top and the recovery percentage of the model with respect to the largest model (in this example, 95.45). Despite sharing the weights across all models, sharing the classifier and zero-shot evaluation, the proposed method preserved up to 96\% of the performance of the largest model which is highly encouraging. Further training of the classifier for our proposed method will lead to even better performance as shown in appendix \ref{ap:adjusting-classifier} (between $\sim$2 to 15\% improvement for different sub-models). In addition, we also tried to sort the depth and width uisng proposed method individually which has been reported in the figure \ref{fig:scalable160} as D. Only, and W. Only, respectively. Across width, SortedNet successfully preserved up to 99\% of the largest network's performance.


\subsection{Can we find the best sub-models using SortedNet?}

As shown in figure \ref{fig:bestsubset}, based on the performance of the models in the previous experiment which has been shown in figure \ref{fig:scalable160}, we selected a subset of best-performing networks (width $>$ 60\% and depth $>$ 13 blocks), and retrained the network from scratch using SortedNet to show the success rate of our proposed method. As shown, SortedNet successfully preserved up to 99\% of the performance of the ordinary training of the largest network. %Alternatively, one can continue the training for the chosen subset using SortedNet.

We can also make this selection process fully automated by sorting the performance of all sub-models after evaluation and filtering out a subset of best-performing models that perform better than a desired threshold. As can be seen in figure \ref{fig:elbow}, there is a set of sub-networks which perform better than 80\%. To better understand the pattern, we annotated some of the points using ``$_{W}^{D}$" as template which shows for each model the corresponding width and depth.

\begin{figure}
\centering
\resizebox{\columnwidth}{!}{  
\includegraphics[width=1.2\textwidth]{Figs/plot-Elbow.png}
}
\caption{Finding best sub-models automatically using a desired threshold bar to eliminate the worst performing models.}
\label{fig:elbow}
\end{figure}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{  
\begin{tabular}{lccccccccc|c}
% \hline
% \multicolumn{11}{c}{\textbf{Refine the Performance Over Time}} \\
\hline
& Acc. & Acc. & F1 & Mathews Corr. & Acc. & Acc. & Acc. & Pearson & \\
\textbf{Model} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}& \textbf{QQP}& \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}& \textbf{AVG w/o ours}\\
\hline
Sorted-RoBERTa (1L)&	60.07&	70.76&	81.22&	0.00&	59.64&	77.80&	47.65&	9.36&	\textbf{50.81}& 40.33\\
Sorted-RoBERTa (2L)&	71.98&	80.28&	81.22&	0.00&	81.49&	87.09&	47.29&	70.37&	\textbf{64.97}&40.86\\
Sorted-RoBERTa (4L)&	76.74&	80.50&	81.22&	0.00&	85.21&	88.82&	46.93&	75.07&	\textbf{66.81}&41.06\\
Sorted-RoBERTa (4L)&	79.13&	84.75&	81.22&	44.51&	86.60&	90.11&	49.10&	84.94&	\textbf{75.04}&42.95\\
Sorted-RoBERTa (5L)&	81.14&	89.91&	81.22&	48.41&	87.88&	90.86&	55.96&	88.22&	\textbf{77.95}&43.80\\
Sorted-RoBERTa (6L)&	82.21&	92.09&	86.67&	53.41&	88.83&	91.12&	67.87&	89.09&	\textbf{81.41}&46.13\\
Sorted-RoBERTa (7L)&	82.99&	92.78&	89.13&	56.42&	89.29&	\textbf{91.29}&	73.29&	89.58&	\textbf{83.10}&44.80\\
Sorted-RoBERTa (8L)&	83.33&	93.23&	89.78&	57.22&	89.40&	91.29&	75.09&	89.67&	\textbf{83.63}&55.17\\
Sorted-RoBERTa (9L)&	83.39&	92.66&	89.66&	58.69&	89.40&	91.25&	\textbf{77.26}&	89.72&	\textbf{84.00}&61.36\\
Sorted-RoBERTa (10L)&	\textbf{87.42}&	93.12&	\textbf{91.64}&	\textbf{61.21}&	\textbf{91.87}&	91.19&	74.01&	\textbf{89.74}&	\textbf{85.02}& 54.30\\
Sorted-RoBERTa (11L)&	87.34&	\textbf{93.35}&	91.45&	60.72&	91.74&	91.17&	74.01&	89.72&	\textbf{84.94}&77.48\\
Sorted-RoBERTa (12L)&	83.35&	92.89&	90.81&	59.20&	89.44&	91.28&	76.53&	89.77 & 84.16 & \textbf{86.13}\\
\hline 
avg. & 79.26 & 87.93 & 86.09 & 41.25 & 85.50 & 89.45 & 64.26 & 79.61 & \textbf{76.67} & 52.86 \\
\hline
\end{tabular}}
\caption{A comparison of the performance of different sub-models with and without the SortedNet. The model's performance will improve if we have more budgets and calculate the representation of deeper layers.}
\label{tab:sortedRoBerta}
\end{table*}

\subsection{Can we generalize SortedNet?}

% ref: http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec6.pdf

% \todo{introduce baselines}
In another experiment, as shown in table \ref{tab:CIFAR10-lcs}, we wanted to show our stochastic approach can outperform the state-of-the-art methods such as LCS (shown as $LCS_p$ in table) \cite{Nunez_2023_WACV}, Slimmable Neural Network (NS) \cite{yu2018slimmable}, and Universally Slimmable Networks (US) \cite{yu2019universally}. To make the comparisons fair, we equalized the number of gradient updates for all models. % based on the principle/rule of thumb that a backward pass worth approximately twice the forward pass for all methods \footnote{https://www.alignmentforum.org/posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks} \cite{Kaplan2019NotesOC}. 
We also tried to remove the impact of architecture design such as the choice of the normalization layers. Therefore, we tried to compare methods by different layer normalization techniques such as BatchNorm \cite{ioffe2015batch} and InstanceNorm \cite{ulyanov2016instance}. In addition, we made sure that the orthogonal methods such as Knowledge Distillation will not impact the results as these methods can be applied and improve the results independent of the method. As shown in the table, SortedNet demonstrates a superior average performance compared to other methods, indicating its generalization across various settings such as different norms.



\subsection{Can we extend SortedNet to language models?}

In this experiment, the goal is to apply SortedNet for a pre-trained transformer model and evaluate the performance on the GLUE benchmark \cite{wang2018glue}. As the baseline, we chose RoBERTa \cite{liu2019roberta} to demonstrate the flexibility of our algorithm. In table \ref{tab:sortedRoBerta}, we sorted all the layers of  RoBERTa-base. As the results demonstrate, our proposed method in average perform better than the baseline by a significant margin ($\sim$ 23\%). However, the largest model has a small drop in performance (less than 2\%). It is interesting that the transformer architecture can preserve the performance of sub-models up to some extent without additional training. However, our algorithm improve the performance of these sub-models between 10 to 40\% approximately.

\section{Can we extend SortedNet to complex dimensions?}
\label{ap:extend-more-complex}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccc}
& & & Acc. & F1 & Acc. & Acc. & Matthews Corr. & Spearman Corr. & F1 & Acc. &  \\
Model & Flops & Weights & MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC & RTE & avg. \\ 
\hline
\multicolumn{12}{c}{Random Initialized Networks} \\
\hline
$BERT_{BASE} (3\mathcal{L}_B)$ & 78.96 G  & $W_B$  & 62.13 $\pm$ 0.27 & 68.74 $\pm$ 0.70 & 61.24 $\pm$ 0.45 &  79.89 $\pm$ 0.59 & 0.00 $\pm$ 0.00 & 12.92 $\pm$ 0.63 & 78.67 $\pm$ 0.41 & 54.51 $\pm$ 1.11 & 52.26 $\pm$ 29.75 \\
\hline
\multicolumn{12}{c}{pre-trained Baselines} \\
\hline
$BERT_{BASE} (3\mathcal{L}_B)$* & 22.36 G & $W_B$ & 84.0 & 71.2 & 90.5 & 93.5 & 52.1 & 85.8 & 88.9 & 66.4 & 79.5 \\
$BERT_{LARGE} (3\mathcal{L}_L)$* & 78.96 G  & $W_L$ & 86.3 & 72.1 & 92.7 & 94.9 & 60.5 & 86.5 & 89.3 & 70.1 & 81.55 \\
\hline
\multicolumn{12}{c}{Paper Setting} \\
\hline
$BERT_{BASE} (3\mathcal{L}_B)$ & 22.36 G & $W_B$ & 84.22 $\pm$ 0.32 & 87.37 $\pm$ 0.08 & 91.47 $\pm$  0.21 & 92.61 $\pm$ 0.15 & 54.90 $\pm$ 0.79 & 88.08 $\pm$ 0.49 & 86.91 $\pm$ 0.82 & 62.96 $\pm$ 2.36 & \textbf{81.07} $\pm$ 14.08 \\
$BERT_{LARGE} (3\mathcal{L}_L)$ & 78.96 G  & $W_L$ & 86.32 $\pm$ 0.09 & 88.36 $\pm$ 0.07 & 92.01 $\pm$ 0.29 & 93.21 $\pm$ 0.42 & 59.39 $\pm$ 1.45 & 88.65 $\pm$ 0.33 & 88.67 $\pm$ 0.75 & 68.23 $\pm$ 1.59 & 83.11 $\pm$ 12.33 \\
\hline
\multicolumn{12}{c}{Extracted Networks} \\
\hline
$BERT_{BASE}^{LARGE} (3\mathcal{L}_B)$ & 22.36 G  & $W_B$ & 77.43 $\pm$ 0.08 & 84.88 $\pm$ 0.15 & 84.74 $\pm$ 0.34 & 84.98 $\pm$ 0.47 & 12.17 $\pm$ 1.62 & 78.33 $\pm$ 4.11 & 79.44 $\pm$ 0.93 & 55.23 $\pm$ 1.08 & 69.65 $\pm$ 25.18 \\
% $BERT_{BASE}^{LARGE} (6\mathcal{L}_B)$ & 22.36 G  & $W_B$ &  &  &  & & \\
\hline
\multicolumn{12}{c}{Proposed Methods} \\
\hline
Sorted $BERT_{BASE} (\sim1.5\mathcal{L}_B+1.5\mathcal{L}_L)$ & 22.36 G & $W_B^L$ & 76.20 $\pm$ 0.02 & 83.58 $\pm$ 0.16 & 83.91 $\pm$ 0.18 & 83.26 $\pm$ 0.69 & 0.08 $\pm$ 0.18 & 70.75 $\pm$ 9.25 & 80.75 $\pm$ 1.29 & 52.85 $\pm$ 2.53 & 66.42 $\pm$ 28.76 \\
Sorted $BERT_{LARGE} (\sim1.5\mathcal{L}_B+1.5\mathcal{L}_L)$ & 78.96 G & $W_L^L$ & 85.93 $\pm$ 0.33 & 87.28 $\pm$ 0.14 & 91.58 v 0.33 & 93.17 $\pm$ 0.26 & 57.08 $\pm$ 1.91 & 88.18 $\pm$ 0.68 & 87.06 $\pm$ 1.02 & 65.56 $\pm$ 1.41 & 81.98 $\pm$ 13.17 \\
Sorted $BERT_{BASE} (\sim3\mathcal{L}_B+3\mathcal{L}_L)$ & 22.36 G & $W_B^L$ & 77.48 & 85.16 $\pm$ 0.02 & 84.96 $\pm$ 0.23 & 86.01 $\pm$ 0.62 & 12.58 $\pm$ 2.04 & 79.29 $\pm$ 2.80 & 78.96 $\pm$ 0.44 & 55.81 $\pm$ 1.37 & 70.03 $\pm$ 25.16 \\
Sorted $BERT_{LARGE} (\sim3\mathcal{L}_B+3\mathcal{L}_L)$ & 78.96 G & $W_L^L$ & 86.12 & 88.26 $\pm$ 0.01 & 92.18 $\pm$ 0.28 & 93.49 $\pm$ 0.21 & 59.84 $\pm$ 1.35 & 88.85 $\pm$ 0.44 & 88.88 $\pm$ 1.10 & 68.45 $\pm$ 2.11 & \textbf{83.26} $\pm$ 12.24 \\
\hline
\end{tabular}}
\caption{The performance of BERT-base and Bert-large in the GLUE Benchmark over 5 runs for SortedNet (sharing weights across both models), pre-trained berts and different initialization.}
\label{tab:bert_base_large_ft}
\end{table*}

In this section, we are interested to investigate whether SortedNet is applicable to more complex dimensions other than width and depth. For example, can we utilize the SortedNet for sorting the Attention Heads \cite{vaswani2017attention}? To achieve this, we conducted an experiment over BERT-large \cite{devlin2019bert} which we tried to sort the information across multiple dimensions at once including, number of layers, hidden dimension, and number of attention heads. In other words, we tried to sort information over Bert-large and Bert-base as Bert-base can be a subset of the Bert-large and therefore respect the nested property. As shown in table \ref{tab:bert_base_large_ft}, in addition to the reported performance of Bert-base and Bert-large according to the original paper \cite{devlin2019bert}, we reported the performance of these models in the paper experimental setting. The performance of randomly initialized Bert-base has been reported as well. We also extracted a Bert-base from a Bert-large model, and we reported the performance of such model in the same table. Additionally, we highlighted the number of training updates with respect to each objective function in front of each model. For example, in the last row (Sorted $BERT_{LARGE}$), we approximately trained our Sorted model half of the times ($\sim3 Epochs$) over the objective function of Bert-base ($\mathcal{L}_B$) and the other half of the times over the objective function of Bert-large ($\mathcal{L}_L$) in an iterative random manner as introduced in the section \ref{sec:method}. The learned Bert-base performance with these methods is still around 10\% behind a pre-trained base but we argue that this is the value of pre-training. To investigate the impact, one should apply the SortedNet during pre-training which we will leave for future research. However, the performance of the learned Bert-large is on-par with an individual Bert-large which suggests sharing the weights does not necessarily have a negative impact over learning. It seems, however, the secret sauce to achieve a similar performance is that we should keep the number of updates for each objective the same as the individual training of Bert-large and Bert-base.

% In Appendix \ref{ap:extend-more-complex}, we additionally investigated whether it is possible to extend the SortedNet idea over more complex dimensions such as sorting Multi-Head Attentions across Bert-large and Bert-base.

% This experiment compares the performance of our model with slimmable neural networks on ImageNet dataset. We use the same settings as the reference paper.
% MobileNet v1 (Howard et al., 2017), MobileNet v2 (Sandler et al., 2018), ShuffleNet (Zhang et al.,
% 2017), and one representative large model ResNet-50 (He et al., 2016).

% \subsection{Can we preserve the capacity of sub-models?}

%\subsection{Why not AutoFormer\cite{chen2021autoformer}?}
% Weight entanglement supports our method
% This is about one-shot NAS
% No need for the search pipeline
% using an inductive bias we are tyring to avoid the search cost. 

% \subsection{Find The Most Attractive Sub-Models}

% Based on the previous experience, we chose a subset of all 160 models and reran the same experiments.




% \subsection{Why not ONCE-FOR-ALL\cite{cai2019once}?}

% \subsubsection{No need for progressive shrinking}
% \subsubsection{No need for Mixed Batch Size of Sub-Models}

% \subsection{Why not DynaBERT\cite{hou2020dynabert}?}

% table 3 or table 11 of the Dynabert
% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lccccccccc}
% \hline
% & Acc. & Acc. & Acc. & Acc. & Math. Corr. & Spear. Corr. & Acc. & Acc. & \\
%  & MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC & RTE & avg. \\
% \hline
% Individual Networks$^*$ & 82.2 & 90.3 & 87.8 & 91.0 & 39.9 & 84.6 & 78.8 & 61.6 & 77.0\\ 
% Vanilla DynaBERT$_{W}^*$ & 82.3 & 90.6 & 89.1 & 91.2 & 44.0 & 87.4 & 80.5 & 64.2 & 78.7\\
% \hline
% SortedBERT (Ours) & & & & & & & & & \\
% \hline
% \end{tabular}}
% \caption{Following \cite{hou2020dynabert}, we used BERT$_{base}$ with the same width multiplier $m_w \in [0.25,0.5,0.75,1.0]$ and average Accuracy of all width multipliers on the GLUE dev-set is reported. *: indicated that the results have been reported in the \cite{hou2020dynabert} paper. As our method can also benefit from Network Rewiring and Distillation, we did not include the results of these methods. The details of this experiment can be found in Appendix \ref{apdx:danabert-details}.}
% \label{tab:dynabert}
% \end{table*}

%Hossein
% \subsection{Why not SLIMMABLE Neural Networks\cite{yu2018slimmable,yu2019universally,li2021dynamic}?}
% This experiment compares the performance of our model with slimmable neural networks on ImageNet dataset. We use the same settings as the reference paper.
% MobileNet v1 (Howard et al., 2017), MobileNet v2 (Sandler et al., 2018), ShuffleNet (Zhang et al.,
% 2017), and one representative large model ResNet-50 (He et al., 2016).

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccc}
% \hline
% Network & Width & FLOPs & I-Net* & S-Net* & US-Net* & Sorted-Net (Ours) \\
% \hline
% \multirow{4}{*}{MobileNet v2} & 100\% & 301M & 28.2 & $29.5_{-1.3}$ & $28.5_{-0.3}$ & \\
% & 75\% & 209M & 30.2 & $31.1_{-0.9}$ & $30.3_{-0.1}$ & \\
% & 50\% & 97M & 34.6 & $35.6_{-1.0}$ & $35.0_{-0.4}$ & \\
% & 35\% & 59M & 39.7 & $40.3_{-0.6}$ & $37.8_{1.9}$ & \\
% \hline 
% avg. & & 167M & 33.2 & $34.1_{-0.9}$ & $32.9_{0.3}$ & \\
% \hline
% \end{tabular}}
% \caption{ImageNet Top-1 Classification Error for I-Net \cite{sandler2018mobilenetv2}, S-Net \cite{yu2018slimmable}, US-Net \cite{yu2019universally} and Sorted Nets in equal settings. *: The result of I-Net, S-Net and US-Net have been reported in \cite{yu2018slimmable, yu2019universally}}
% \label{tab:ImageNet}
% \end{table*}




% \subsubsection{No need for Switchable Batch-Norm \cite{yu2018slimmable}}

% \subsubsection{No need for Post-Statistics Calculation of Batch-Norm \cite{yu2019universally}}

%\subsection{Ensemble All Sub-Networks}

%\subsection{Pretraining?}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lccccc|cc}
% & & & & & & Perplexity & Accuracy \\
% Model & Layers & Hidden Size & Attn Heads & Hidden Size/Head & Total GPUs & WikiText103 $\downarrow$ & LAMBADA $\uparrow$ \\ 
% \hline
% 355M & 24 & 1024 & 16 & 64 & 64 & 19.31 & 45.18\% \\
% 2.5B & 54 & 1920 & 20 & 96 & 128 & 12.76 & 61.73\% \\
% 8.3B & 72 & 3072 & 24 & 128 & 512 & 10.81 & 66.51\% \\
% \hline
% Sorted 355M$_{\text{8.3B}}$ & 24 & 1024 & 16 & 64 & 64 & & \\
% Sorted 2.5B$_{\text{8.3B}}$ & 54 & 1920 & 20 & 96 & 128 & & \\
% Sorted 8.3B$_{\text{8.3B}}$ & 72 & 3072 & 24 & 128 & 512 & & \\
% \hline
% \end{tabular}}
% \caption{GPT Megatron-LM \cite{shoeybi2019megatron}}
% \label{tab:pre-training}
% \end{table*}






% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccccc}
% & Acc. & F1 & Acc. & Acc. & Acc. & Spearman Corr. & F1 & Acc. &  \\
% Model & MNLI-(m/mm) & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC & RTE & avg. \\ 
% \hline
% BERT$_{\text{BASE}}$* & 84.6/83.4 & 71.2 & 90.5 & 93.5 & 52.1 & 85.8 & 88.9 & 66.4 & 79.6 \\
% BERT$_{\text{LARGE}}$* & 86.7/85.9 & 72.1 & 92.7 & 94.9 & 60.5 & 86.5 & 89.3 & 70.1 & 82.1 \\
% \hline
% Sorted BERT$_{\text{BASE}}$ &  &  &  &  &  & & \\
% Sorted BERT$_{\text{LARGE}}$ &  &  &  &  &  & & \\
% \hline
% \end{tabular}}
% \caption{GPT Megatron-LM \cite{shoeybi2019megatron}}
% \label{tab:pre-training}
% \end{table*}

\subsection{Ablation Study}

\subsubsection{Convergence (Training Time) Analysis}
\label{convergence-time}
Being sorted and randomly selecting one sub-network at the time from a predefined set of the sub-networks empowers SortedNet with a higher convergence rate and a faster training time. Figure \ref{fig:acc_loss_combined} empirically certifies this claim and compares the training convergence of SortedNet against LCP\_p, which, to the best of our knowledge, LCP\_p stands as the most recent state-of-the-art method. As LCS\_p uses summation loss over four sub-networks in every training steps and to have a fair comparison, we therefore report the performance of SortedNet in different values of gradient accumulation ($g_{acc}$), where $g_{acc}=4$ provides a fair comparison with LCS\_p. As shown in the figure, SortedNet with $g_{acc}=4$ converges either faster or competitive across different sub-networks. Moreover, SortedNet does not require any for-loop in its implementation; thus tailoring parallel computation and resulting in faster running time. We empirically investigate this feature and found that in the same settings, SortedNet runs  at least one third faster than LCS\_p (details in Appendix \ref{apdx:running-time}).

% We just need a plot of epoch/performance for different networks and our proposed method to show the difference in convergence difference. 

\subsubsection{The impact of gradient accumulation}
 
% \begin{table}[h!]

% \resizebox{\columnwidth}{!}{  
% \begin{tabular}{lcccc}
% \hline
% Gradient Acc. & $\sim$ Flops & Epochs & \multicolumn{2}{c}{Extracted Networks} \\
% & & & Width & Accuracy \\
% \hline
% \multirow{5}{*}{$g_{acc}=1$} & \multirow{5}{*}{$e_1+2\times e_1=3e_1$} & \multirow{5}{*}{200} & & \\
% & & & 100\% & 84.94 \\ 
% & & & 75\% & 84.92 \\
% & & & 50\% & 82.54 \\
% & & & 25\% & 71.03 \\
% \hline
% avg. & & & & 80.85 \\
% \hline
% \multirow{5}{*}{$g_{acc}=2$} & \multirow{5}{*}{$e_2+2\times \frac{e_2}{2}=3e_1$} & \multirow{5}{*}{300} & & \\
% & & & 100\% & 86.69 \\ 
% & & & 75\% & 86.68 \\
% & & & 50\% & 84.40 \\
% & & & 25\% & 72.36 \\
% \hline
% avg. & & & & 82.53 \\
% \hline
% \multirow{5}{*}{$g_{acc}=3$} & \multirow{5}{*}{$e_3+2\times \frac{e_3}{3}=3e_1$} & \multirow{5}{*}{360} & & \\
% & & & 100\% & 87.37 \\ 
% & & & 75\% & 87.50 \\
% & & & 50\% & 84.57 \\
% & & & 25\% & 73.00 \\
% \hline
% avg. & & & & 83.11 \\
% \hline
% \multirow{5}{*}{$g_{acc}=4$} & \multirow{5}{*}{$e_4+2\times \frac{e_4}{4}=3e_1$} & \multirow{5}{*}{400} & & \\
% & & & 100\% & 87.93 \\ 
% & & & 75\% &  87.40\\
% & & & 50\% &  84.27\\
% & & & 25\% &  76.23\\
% \hline
% avg. & & & & 83.95 \\
% \hline
% % Width:75\%,Acc=84.92\\Width:50\%,Acc=82.54\\Width:25\%,Acc=71.03\end{tabular}   \\ 
% % \hline
% % $g_{acc}=2$ & $e^f_2+2\times \frac{e_2^b}{2}=3e_1$ & 300 & \begin{tabular}{@{}c@{}c@{}c@{}}Width:100\%,Acc=86.69 \\ Width:75\%,Acc=86.68\\Width:50\%,Acc=84.40\\Width:25\%,Acc=72.36\end{tabular} \\

% % \hline
% % $g_{acc}=3$ & $e^f_3+2\times \frac{e_3^b}{3}=3e_1$ & 360 & \begin{tabular}{@{}c@{}c@{}c@{}}Width:100\%,Acc=87.37 \\ Width:75\%,Acc=87.50\\Width:50\%,Acc=84.57\\Width:25\%,Acc=73.0\end{tabular}  \\
% % \hline
% % $g_{acc}=4$ & $e^f_4+2\times \frac{e_4^b}{4}=3e_1$ & 400 & 87.93 \\
% \hline
% \end{tabular}}

\begin{table}[h!]

\resizebox{\columnwidth}{!}{  
\begin{tabular}{lcccc}
\hline
Gradient Acc. & Num. Updates & Epochs & \multicolumn{2}{c}{Extracted Networks} \\
&aka optimizer.step() & & Width & Accuracy \\
\hline
\multirow{5}{*}{$g_{acc}=1$} & \multirow{5}{*}{200} & \multirow{5}{*}{200} & & \\
& & & 100\% & 84.94 \\ 
& & & 75\% & 84.92 \\
& & & 50\% & 82.54 \\
& & & 25\% & 71.03 \\
\hline
avg. & & & & 80.85 \\
\hline
\multirow{5}{*}{$g_{acc}=2$} & \multirow{5}{*}{200} & \multirow{5}{*}{400} & & \\
& & & 100\% & 86.69 \\ 
& & & 75\% & 86.68 \\
& & & 50\% & 84.40 \\
& & & 25\% & 72.36 \\
\hline
avg. & & & & 82.53 \\
\hline
\multirow{5}{*}{$g_{acc}=3$} & \multirow{5}{*}{200} & \multirow{5}{*}{600} & & \\
& & & 100\% & 87.37 \\ 
& & & 75\% & 87.50 \\
& & & 50\% & 84.57 \\
& & & 25\% & 73.00 \\
\hline
avg. & & & & 83.11 \\
\hline
\multirow{5}{*}{$g_{acc}=4$} & \multirow{5}{*}{200} & \multirow{5}{*}{800} & & \\
& & & 100\% & 87.93 \\
& & & 75\% &  87.40\\
& & & 50\% &  84.27\\
& & & 25\% &  76.23\\
\hline
avg. & & & & 83.95 \\
\hline
\end{tabular}}
\caption{Effect of gradient accumulation on SortedNet-IN performance while fixing the number of parameters' updates. The underlying network and dataset are cPreResNet20 and CIFAR10, respectively.}
\label{tab:gradientaccumulation}
\end{table}

The goal of this experiment is to examine the impact of gradient accumulation ($g_{acc}$) on the performance of SortedNet within an equal number of parameters updates. Table \ref{tab:gradientaccumulation} presents the results obtained in terms of accuracies for 4 different gradient accumulation values. To ensure an equal number of updates, the maximum number of epochs is adjusted for each scenario, e.g. $g_{acc}=k$ receives $k$ times more epochs than $g_{acc}=1$. As the results explains, increasing gradient accumulation values results in a higher performance for SortedNet. This observation can be attributed to the increase in training stochasticity when gradient accumulation is raised. Consequently, each sub-network in SortedNet contributes more equally to the updating of weight parameters, leading to a faster convergence rate. More details are provided in Appendix \ref{apdx:gradient-accumulation}. 

In addition, we highlighted the details of each experiment hyperparameters in appendix \ref{ap:hyperparameters} and further analysis has been provided in appendix \ref{ap:sorting_impact} to better understand the behavior of sortedNet methodology. %further experiments has been provided in the Appendix. 

% The goal of this experiment is to examine the impact of gradient accumulation on the performance of SortedNet within an equal number of parameters updates. Table \ref{tab:gradientaccumulation} presents the results obtained in terms of accuracies for four different gradient accumulation values. To ensure an equal number of updates, the number of forward-backward cycles, i.e. Flops, is calculated for each value, and the number of epochs is adjusted accordingly. More details can be found in the appendix \ref{apdx:appendix}. As the results explains, increasing the gradient accumulation values results in a higher performance.This observation can be attributed to the increase in training stochasticity when gradient accumulation is raised. Consequently, each sub-model contributes more equally to the updating of weight parameters, leading to a faster convergence.

\section*{Conclusion}
In summary, this paper proposes a new approach for training dynamic neural networks that leverages the modularity of deep neural networks to efficiently switch between sub-networks during inference. Our method sorts sub-networks based on their computation/accuracy and trains them using an efficient updating scheme that randomly samples sub-networks while accumulating gradients. The stochastic nature of our proposed method is helping our algorithm to generalize better and avoid greedy choices to robustly optimize many networks at once. We demonstrate through extensive experiments that our method outperforms previous dynamic training methods and yields more accurate sub-networks across various architectures and tasks. 
The sorted architecture of the dynamic model proposed in this work aligns with sample efficient inference by allowing easier samples to exit the inference process at intermediate layers. Exploring this direction could be interesting for future. %research.


\section*{Limitations}
It is good to note that our proposed method might be sensitive to the randomness as the chosen trajectory at the moment is random uniform. Further research is necessary to investigate the effect of choosing more optimal strategies for choosing the next model at each iteration.

% Anytime algorithms are algorithms that can return valid  





% https://openreview.net/forum?id=BJe_z1HFPr
%http://visal.cs.cityu.edu.hk/static/pubs/conf/ijcai20-fn3.pdf


% \section*{Acknowledgements}
% This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
% EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
% EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
% ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
% \bibliographystyle{acl_natbib}

\clearpage
\newpage

\appendix

\section{Appendix}
\label{apdx:appendix}

\subsection{Effect of gradient accumulation on SortedNet performance}
\label{apdx:gradient-accumulation}


It is of interest to explore whether limiting the number of parameters' updates is a suitable approach for investigating the influence of gradient accumulation on SortedNet. One possible way to certify this factor is running SortedNet with different gradient accumulation values while keeping the number of updates fixed. To that end, we consider the same settings as Table \ref{tab:gradientaccumulation} and repeat the experiment while fixing the maximum number of training epochs. By fixing this value and increasing gradient accumulation values, we implicitly decrease the number of parameters' updates. Table \ref{tab:gradientaccumulation-extra} reports the results. Comparing the results of these two tables it is obvious that the number of updates plays a significant role in the model's performance. For instance, when considering $g_{acc}=2$, an average performance drop of approximately 2\% is observed  across all sub-networks. This reduction indicates  that the underlying model needs more training time for higher values of $g_{acc}$.

\begin{table}[h!]

\resizebox{\columnwidth}{!}{  
\begin{tabular}{lcccc}
\hline
Gradient Acc. & Num. Updates & Epochs & \multicolumn{2}{c}{Extracted Networks} \\
&aka optimizer.step() & & Width & Accuracy \\
\hline
\multirow{5}{*}{$g_{acc}=1$} & \multirow{5}{*}{200} & \multirow{5}{*}{200} & & \\
& & & 100\% & 84.94 \\ 
& & & 75\% & 84.92 \\
& & & 50\% & 82.54 \\
& & & 25\% & 71.03 \\
\hline
avg. & & & & 80.85 \\
\hline
\multirow{5}{*}{$g_{acc}=2$} & \multirow{5}{*}{100} & \multirow{5}{*}{200} & & \\
& & & 100\% &  85.01\\ 
& & & 75\% &  85.12\\
& & & 50\% &  82.24\\
& & & 25\% &  70.65\\
\hline
avg. & & & &  80.75\\
\hline
\multirow{5}{*}{$g_{acc}=3$} & \multirow{5}{*}{$\sim$ 66} & \multirow{5}{*}{200} & & \\
& & & 100\% & 85.09\\ 
& & & 75\% &  85.06\\
& & & 50\% &  82.64\\
& & & 25\% &  73.74\\
\hline
avg. & & & &  81.63\\
\hline
\multirow{5}{*}{$g_{acc}=4$} & \multirow{5}{*}{50} & \multirow{5}{*}{200} & & \\
& & & 100\% &  86.05\\ 
& & & 75\% &  86.06\\
& & & 50\% &  83.66\\
& & & 25\% &  73.0\\
\hline
avg. & & & &  82.19\\
\hline
\end{tabular}}
\caption{Exploring the impact of limited number of parameters updates on the effect of gradient accumulation in SortedNet-IN. The underlying network and dataset are cPreResNet20 and CIFAR10, respectively.}
\label{tab:gradientaccumulation-extra}
\end{table}

\subsection{Details of training time comparison}
\label{apdx:running-time}
To empirically compare the training time between SortedNet and LCS\_p, the elapsed time per epoch for five epochs is recorded independently for each method. We then ignore the first epoch to reduce the impact of first-time loading and initialization. Next, for each method we take the average of the remaining elapsed times. We refer to these averaging times (in seconds) by $\bar{T}_{SortedNet}=49.7 \pm 2.06$ and $\bar{T}_{LCS\_p}=292.7 \pm 3.17$ for simplicity. As it is mentioned in Subsection \ref{convergence-time}, SortedNet with $g_{acc}=4$ can be considered as a fair comparison with LCS\_p. As a result, each epoch in LCS\_p holds four times the significance of SortedNet in terms of the total number of parameter updates. Therefore, we can simply multiply $\bar{T}_{SortedNet}$ by a factor of four to equalize their impacts in term of total number of parameter updates. By doing that, we have $\bar{T}_{SortedNet}=198.8$, which is almost one-third less than $\bar{T}_{LCS\_p}$.
% \subsection{Comparison with DynaBERT$_{W}$}
% \label{apdx:danabert-details}
% % table 3 or table 11 of the Dynabert
% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccccc}
% \hline
%  & & Acc. & Acc. & Acc. & Acc. & Math. Corr. & Spear. Corr. & Acc. & Acc. & \\
%  & $m_w$ & MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC & RTE & avg. \\
% \hline
% \multirow{4}{*}{Individual Networks$^*$} & 100\% & 84.8 & 90.9 & 92.0 & 92.9 & 58.1 & 89.8 & 87.7 & 71.1 & \\ 
% & 75\% & 84.1 & 90.6 & 89.7 & 92.9 & 48.0 & 87.2 & 82.8 & 66.1 & \\ 
% & 50\% & 81.7 & 89.7 & 86.0 & 91.4 & 37.2 & 84.5 & 75.5 & 55.2 & \\ 
% & 25\% & 77.9 & 89.9 & 83.7 & 86.7 & 14.7 & 77.4 & 71.3 & 57.4 & \\ 
% \hline
% & avg. & 82.2 & 90.3 & 87.8 & 91.0 & 39.9 & 84.6 & 78.8 & 61.6 & 77.0\\ 
% \hline 
% \hline
% \multirow{4}{*}{Vanilla DynaBERT$_{W}^*$} & 100\% & 84.8 & 91.3 & 91.7 & 92.9 & 58.1 & 89.9 & 83.3 & 69.3 & \\ 
% & 75\% & 83.7 & 91.1 & 90.1 & 91.7 & 54.5 & 88.7 & 82.6 & 65.7 & \\ 
% & 50\% & 82.2 & 90.7 & 88.9 & 91.6 & 46.9 & 87.3 & 83.1 & 61.0 & \\ 
% & 25\% & 78.5 & 89.1 & 85.6 & 88.5 & 16.4 & 83.5 & 72.8 & 60.6 & \\ 
% \hline
% & avg. & 82.3 & 90.6 & 89.1 & 91.2 & 44.0 & 87.4 & 80.5 & 64.2 & 78.7\\
% \hline
% \hline
% \multirow{4}{*}{SortedNet (Ours)} & 100\% &  & &  &  &  &  &  & & \\ 
% & 75\% &  &  &  &  &  &  &  &  & \\ 
% & 50\% &  &  &  &  &  &  &  &  & \\ 
% & 25\% &  &  &  &  &  &  &  &  & \\ 
% \hline
% & avg. &  & &  &  &  &  &  &  & \\
% \hline
% \end{tabular}}
% \caption{Following \cite{hou2020dynabert}, we used BERT$_{base}$ with the same width multiplier $m_w \in [0.25,0.5,0.75,1.0]$ and average Accuracy of all width multipliers on the GLUE dev-set is reported. *: indicated that the results have been reported in the \cite{hou2020dynabert} paper. As our method can also benefit from Network Rewiring and Distillation, we did not include the results of these methods.}
% \label{tab:dynabert}
% \end{table*}

% \section{Flops Calculation - Table \ref{tab:gradientaccumulation}}
% \label{ap:flopscalculation}
% This section explains how we ensure that models with different gradient accumulation value, i.e. $g_{acc}$ (Table \ref{tab:gradientaccumulation}), receive equal training budgets.
% Assuming that the backward pass costs twice more than the forward pass, each epoch in $g_{acc}=1$ approximately costs $2+1=3$. In other words,

% \begin{align*}    
%     Flops_{g_{acc}=1} \approx \big( Cost_{forward}+Cost_{Backward} \big ).e_1 \\
%     \approx \big( 1+2 \big).e_1\\
%     =3e_1
% \end{align*}
% where $e_1$ is maximum epochs value for $g_{acc}=1$. 

% For the case of $g_{acc}=2$, we have one forward pass in each epoch, while the backward pass occurs every two epochs. Therefore,   
   
% \begin{align*}    
%     Flops_{g_{acc}=2} \approx \big( Cost_{forward}+\frac{1}{2}Cost_{Backward} \big ).e_2 \\
%     \approx \big( 1+\frac{1}{2}.2 \big).e_2\\
% \end{align*}
% which should be equal with $3e_1$ to have the same training budgets. Hence, 
% if the maximum epochs for the model with $g_{acc}=1$ is set to 200, the equivalence  epochs for $g_{acc}=2$ is 
% \begin{align*}    
%     2e_2=3e_1 \\
%     e_2=\frac{3}{2}e_1=\frac{3}{2}.200=300.
% \end{align*}

% The equivalent epochs for $g_{acc}=3$ and $g_{acc}=4$ are 360 and 400 respectively.

\subsection{Hyperparameters}
\label{ap:hyperparameters}
This section provides an overview of the hyperparameters and experimental configurations, detailed in Table \ref{tab:hyperparamters-table}.
% We did not use any parameter tuning nor MNLI trick (initializing some down-streams tasks from MNLI checkpoint instead of pre-trained weights). Therefore, we fine-tuned all the datasets from original pre-trained weights. We simply followed a unified hyper-parameters for all different experiments. We reported the mean and standard deviation over 5 random seeds for some of the experiments. See the details in Table \ref{hyperparamters-table}.

\begin{table}[hbt!]
\centering
\resizebox{\columnwidth}{!}{  
\begin{tabular}{l|cc}
\hline
\textbf{Model} & \textbf{Parameter} & \textbf{Value} \\
\hline
\multirow{11}{*}{BERT-Base} & &  \\
& Optimizer & AdamW \\
& Warmup Ratio & 0.06 \\
& Dropout & 0.1 \\
& LR Scheduler & Linear \\
& Batch Size & 32 (RoBertA) / 8 (Bert) \\
& Epochs & 30 (RoBertA) / 3,6 (Bert) \\ 
& Learning Rate (LR) & 2e-5 (RoBertA / 6e-6 (Bert) \\
& Weight Decay & 0.1 \\
& Max Sequence Length & 512 \\
& Seeds & [10, 110, 1010, 42, 4242]\\
& GPU & Tesla V100-PCIE-32GB \\
\hline
\multirow{7}{*}{MobileNetV2} & &  \\
& Model & "google/mobilenet\_v2\_1.4\_224" \\
& Optimizer & AdamW \\
& LR Scheduler & Linear \\
& Batch Size & 128 \\
& Seeds & 4242\\
& Epochs & 60 $\times \#$ Models \\
& GPU & 8 $\times$ Tesla V100-PCIE-32GB \\
\hline
\multirow{9}{*}{cPreResNet20} & &  \\
& Optimizer & SGD \\
& Criterion & Cross Entropy \\
& LR Scheduler & cosine\_lr \\
& Batch Size & 128 \\
& Seed & 40 \\
& Momentum & 0.9 \\
& Weight Decay & 0.0005 \\
& LR & 0.1 \\
& Epochs & [200,400,600,800] \\
& Gradient Accumulation & [1,2,3,4] \\

\hline
\end{tabular}}
\caption{
All the hyperparameters that have been used throughout our study for different experiments. If we didn't mention a parameter specifically, it means we utilized the default value of the HuggingFace Transformers v'4.27.0.dev0'. \footnote{https://huggingface.co/docs/transformers}. Otherwise, we highlighted any exception in the main text.
}
\label{tab:hyperparamters-table}
\end{table}

%TODO: UNCOMMENT
\begin{figure}[htb!]
\hspace*{-1cm}
\centering
\resizebox{1.1\columnwidth}{!}{  
    \input{cifar10-160-adjusted.tex}}
\caption{CIFAR10 Adjusted Classification Accuracy for SortedNet (160 Models) and the baseline. The relative performance gain of each sub-network has been reported at the bottom of each cell with respect to the performance of the same network without adjustment.}
\label{fig:scalable160-adjusted}
\end{figure}

\subsection{Can we improve the performance of SortedNet by adjusting the classifier layer?}
\label{ap:adjusting-classifier}

In figure \ref{fig:scalable160}, as mentioned before, we adjusted the performance of the classifiers for the baseline in the experiment but not for the SortedNet. Therefore, as an additional experiment, we wanted to analyze the impact of adjusting the classifier over the performance of our proposed method as well. Same as the previous experiment, we trained the classifier layer for 5 epochs for each sub-network and reported the performance. As shown in figure \ref{fig:scalable160-adjusted}, the gain is much higher for very smaller networks than the large ones. The SortedNet shared classifier already doing a good job without additional computation overhead for all sub-networks but further adjustments might be beneficial as shown.

\subsection{What is the impact of Sorting?}
\label{ap:sorting_impact}

In order to better understand the impact of sorting information, we designed an experiment that compare the dependency order of all the neurons in a sorted network. To keep the experiment simple, we designed a one layer neural network with 10 (hidden dimension) $\times$ 2 (input dimension) neurons as the hidden layer and a classifier layer on top of that which map the hidden dimension to predict the probabilities of a 4 class problem. The task is to predict whether a 2d point belong to a specific class on a synthetic generated dataset. We trained both Sorted Network and the ordinary one for 10 epochs and optimize the networks using Adam optimizer \cite{kingma2017adam} with the learning rate of 0.01 and batch size of 16.

\begin{figure}[htb!]
% \hspace*{-1cm}
\centering
\resizebox{1.0\columnwidth}{!}{  
    \includegraphics[width=1.0\textwidth]{Figs/orderDep/synthetic_dataset.png}    
}
\caption{Synthetically generated dataset with four classes and with the centers of [[-2, 0], [0, 2], [2, 0], [0, -2]] and cluster standard deviation of [0.5, 1, 0.5, 1]. Seed has been fixed to 42, and 1000 samples has been generated.}
\label{fig:synthetic-dataset}
\end{figure}

\begin{table}[htb!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc}
Active Neurons & Network Accuracy & SortedNet Accuracy \\
\hline
\multicolumn{3}{c}{Baseline} \\
\hline
\shortstack{XXXXXXXXXX \\ XXXXXXXXXX} &  \avercalc[2]{94.40, 94.50, 94.50, 94.30, 94.70} $\pm$ \stdcalc[2]{94.40, 94.50, 94.50, 94.30, 94.70} & \avercalc[2]{93.40, 93.20, 94.30, 93.70, 93.90} $\pm$ \stdcalc[2]{93.40, 93.20, 94.30, 93.70, 93.90}\\
\hline
 \shortstack{OOOOOOOOOO \\ OOOOOOOOOO} &  \avercalc[2]{25.0} $\pm$ \stdcalc[2]{25.0} & \avercalc[2]{25.0} $\pm$ \stdcalc[2]{25.0}\\
\hline
\multicolumn{3}{c}{Target Order $\uparrow$} \\
%%% 1
\hline
 \shortstack{XXXXXXXXXO \\ XXXXXXXXXO} &  \avercalc[2]{94.20, 93.60, 94.10, 93.40, 94.00} $\pm$ \stdcalc[2]{94.20, 93.60, 94.10, 93.40, 94.00} & \avercalc[2]{93.30, 93.00, 94.40, 93.60, 93.90} $\pm$ \stdcalc[2]{93.30, 93.00, 94.40, 93.60, 93.90}\\
%%% 2
\hline
 \shortstack{XXXXXXXXOO \\ XXXXXXXXOO} &  \avercalc[2]{93.60, 94.60, 94.30, 92.40, 93.60} $\pm$ \stdcalc[2]{93.60, 94.60, 94.30, 92.40, 93.60} & \avercalc[2]{93.30, 93.50, 94.30, 93.70, 94.10} $\pm$ \stdcalc[2]{93.30, 93.50, 94.30, 93.70, 94.10}\\
%%% 3
\hline
 \shortstack{XXXXXXXOOO \\ XXXXXXXOOO} &  \avercalc[2]{92.30, 94.0, 93.50, 93.50, 92.20} $\pm$ \stdcalc[2]{92.30, 94.0, 93.50, 93.50, 92.20} & \avercalc[2]{93.30, 92.80, 94.30, 93.70, 94.00} $\pm$ \stdcalc[2]{93.30, 92.80, 94.30, 93.70, 94.00}\\
 %%% 4
\hline
 \shortstack{XXXXXXOOOO \\ XXXXXXOOOO} &  \avercalc[2]{91.60, 92.60, 94.10, 92.10, 91.30} $\pm$ \stdcalc[2]{91.60, 92.60, 94.10, 92.10, 91.30} & \avercalc[2]{94.60, 92.30, 94.10, 93.70, 93.80} $\pm$ \stdcalc[2]{94.60, 92.30, 94.10, 93.70, 93.80}\\
 %%% 5
\hline
 \shortstack{XXXXXOOOOO \\ XXXXXOOOOO} &  \avercalc[2]{91.90, 91.90, 91.20, 89.70, 87.20} $\pm$ \stdcalc[2]{91.90, 91.90, 91.20, 89.70, 87.20}  & \avercalc[2]{94.50, 92.60, 94.10, 94.00, 94.10} $\pm$ \stdcalc[2]{94.50, 92.60, 94.10, 94.00, 94.10}\\
 %%% 6
\hline
 \shortstack{XXXXOOOOOO \\ XXXXOOOOOO} &  \avercalc[2]{92.80, 91.90, 85.50, 92.40, 71.60} $\pm$ \stdcalc[2]{92.80, 91.90, 85.50, 92.40, 71.60}  & \avercalc[2]{94.40, 92.10, 93.70, 94.00, 94.10} $\pm$ \stdcalc[2]{94.40, 92.10, 93.70, 94.00, 94.10}\\
 %%% 7
\hline
 \shortstack{XXXOOOOOOO \\ XXXOOOOOOO} &  \avercalc[2]{93.60, 82.40, 69.40, 89.20, 66.10} $\pm$ \stdcalc[2]{93.60, 82.40, 69.40, 89.20, 66.10}  & \avercalc[2]{94.10, 90.20, 94.50, 93.20, 93.70} $\pm$ \stdcalc[2]{94.10, 90.20, 94.50, 93.20, 93.70}\\
 %%% 8
\hline
 \shortstack{XXOOOOOOOO \\ XXOOOOOOOO} &  \avercalc[2]{50.60, 78.40, 43.00, 56.50, 76.30} $\pm$ \stdcalc[2]{50.60, 78.40, 43.00, 56.20, 76.30} & \avercalc[2]{93.50, 57.80, 91.40, 90.60, 93.00} $\pm$ \stdcalc[2]{93.50, 57.80, 91.40, 90.60, 93.00}\\
 %%% 9
\hline
 \shortstack{XOOOOOOOOO \\ XOOOOOOOOO} &  \avercalc[2]{49.20, 50.90, 46.30, 48.10, 48.80} $\pm$ \stdcalc[2]{49.20, 50.90, 46.30, 48.10, 48.80} & \avercalc[2]{48.30, 64.40, 62.10, 66.70, 71.40} $\pm$ \stdcalc[2]{48.30, 64.40, 62.10, 66.70, 71.40}\\
\hline
avg. & \avercalc[2]{93.86, 93.70, 93.10, 92.34, 90.38, 86.84, 80.14, 60.96, 48.66} $\pm$ \stdcalc[2]{93.86, 93.70, 93.10, 92.34, 90.38, 86.84, 80.14, 60.96, 48.66} & \textbf{\avercalc[2]{93.64, 93.78, 93.62, 93.70, 93.86, 93.66, 93.14, 85.26, 62.58}} $\pm$ \textbf{\stdcalc[2]{93.64, 93.78, 93.62, 93.70, 93.86, 93.66, 93.14, 85.26, 62.58}}\\
\hline
\multicolumn{3}{c}{Reverse Order $\downarrow$} \\
%%% 1
\hline
 \shortstack{OOOOOOOOOX \\ OOOOOOOOOX} &  \avercalc[2]{49.50, 58.80, 48.80, 48.20, 53.20} $\pm$ \stdcalc[2]{49.50, 58.80, 48.80, 48.20, 53.20} & \avercalc[2]{25.0, 50.50, 23.70, 25.00, 25.00} $\pm$ \stdcalc[2]{25.0, 50.50, 23.70, 25.00, 25.00}\\
 %%% 2
\hline
 \shortstack{OOOOOOOOXX \\ OOOOOOOOXX} &  \avercalc[2]{86.00, 92.00, 89.90, 66.60, 93.10} $\pm$ \stdcalc[2]{86.00, 92.00, 89.90, 66.60, 93.10} & \avercalc[2]{25.0, 85.30, 24.90, 15.70, 25.00} $\pm$ \stdcalc[2]{25.0, 85.30, 24.90, 15.70, 25.00}\\
 %%% 3 
\hline
 \shortstack{OOOOOOOXXX \\ OOOOOOOXXX} &  \avercalc[2]{91.90, 88.10, 93.20, 91.30, 89.60} $\pm$ \stdcalc[2]{91.90, 88.10, 93.20, 91.30, 89.60} & \avercalc[2]{25.0, 85.80, 33.50, 16.00, 47.30} $\pm$ \stdcalc[2]{25.0, 85.80, 33.50, 16.00, 47.30}\\
  %%% 4
\hline
 \shortstack{OOOOOOXXXX \\ OOOOOOXXXX} &  \avercalc[2]{89.20, 79.90, 86.40, 86.90, 91.10} $\pm$ \stdcalc[2]{89.20, 79.90, 86.40, 86.90, 91.10} & \avercalc[2]{48.40, 85.40, 61.50, 56.80, 47.30} $\pm$ \stdcalc[2]{48.40, 85.40, 61.50, 56.80, 47.30}\\
   %%% 5
\hline
 \shortstack{OOOOOXXXXX \\ OOOOOXXXXX} &  \avercalc[2]{93.10, 91.40, 93.50, 86.50, 90.10} $\pm$ \stdcalc[2]{93.10, 91.40, 93.50, 86.50, 90.10} & \avercalc[2]{48.50, 89.40, 89.50, 53.80, 49.10} $\pm$ \stdcalc[2]{48.50, 89.40, 89.50, 53.80, 49.10}\\
  %%% 6
\hline
 \shortstack{OOOOXXXXXX \\ OOOOXXXXXX} &  \avercalc[2]{93.60, 93.20, 93.30, 92.40, 92.20} $\pm$ \stdcalc[2]{93.60, 93.20, 93.30, 92.40, 92.20} & \avercalc[2]{48.40, 87.50, 90.80, 51.60, 49.30} $\pm$ \stdcalc[2]{48.40, 87.50, 90.80, 51.60, 49.30}\\
  %%% 7
\hline
\shortstack{OOOXXXXXXX \\ OOOXXXXXXX} &  \avercalc[2]{93.70, 93.70, 93.20, 93.80, 93.20} $\pm$ \stdcalc[2]{93.70, 93.70, 93.20, 93.80, 93.20} & \avercalc[2]{57.00, 90.30, 93.00, 68.80, 51.60} $\pm$ \stdcalc[2]{57.00, 90.30, 93.00, 68.80, 51.60}\\
   %%% 8
\hline
 \shortstack{OOXXXXXXXX \\ OOXXXXXXXX} &  \avercalc[2]{93.70, 94.00, 93.70, 93.30, 94.50} $\pm$ \stdcalc[2]{93.70, 94.00, 93.70, 93.30, 94.50} & \avercalc[2]{90.60, 91.50, 91.60, 68.10, 68.50} $\pm$ \stdcalc[2]{90.60, 91.50, 91.60, 68.10, 68.50}\\
    %%% 9
\hline
 \shortstack{OXXXXXXXXX \\ OXXXXXXXXX} &  \avercalc[2]{93.70, 94.00, 94.50, 94.20, 93.80} $\pm$ \stdcalc[2]{94.0, 94.00, 94.50, 94.20, 93.80} & \avercalc[2]{93.20, 92.80, 92.70, 62.60, 69.70} $\pm$ \stdcalc[2]{93.20, 92.80, 92.70, 62.60, 69.70}\\
\hline
avg. & \avercalc[2]{51.70, 85.52, 90.82, 86.70, 90.92, 92.94, 93.52, 93.84, 94.04} $\pm$ \stdcalc[2]{51.70, 85.52, 90.82, 86.70, 90.92, 92.94, 93.52, 93.84, 94.04} & \textbf{\avercalc[2]{29.84, 35.18, 41.52, 59.88, 66.06, 65.52, 72.14, 82.06, 82.2}} $\pm$ \textbf{\stdcalc[2]{29.84, 35.18, 41.52, 59.88, 66.06, 65.52, 72.14, 82.06, 82.2}}\\
\hline
\end{tabular}}
\caption{Order dependency of all neurons in the network using the proposed method (SortedNet) and the ordinary training across 5 random runs. X means we used the neuron as is, and O means we removed the impact of that neuron by making it 0. $\uparrow$ higher the better, $\downarrow$ the lower the better.}
\label{tab:order_dep}
\end{table}

As can be seen in Table \ref{tab:order_dep}, the performance of different orders in the original neural network training paradigm can be different and unfortunately there is no specific pattern in it. Therefore, if one search the whole space of different orders (from neuron 1 to neuron n, from neuron n to neuron 1, or even select a subset of neurons by different strategies i.e. for the half size network activate every other neurons like XOXOXOXOXO.) might find better alternatives that work even better than the desirable target order. In this example, the reverse order in average perform better than the target order (86.67\% versus 82.22\%). However, with the proposed method, we can clearly see that the target order performance consistently is much better than the reverse order (89.25\% versus 59.38\%). This means, we have been able to enforce the desirable target order as we wanted using our proposed method. For example, neuron 2 is more dependent to neuron 1 in SortedNet in comparison with the ordinary training. In another example, the last 5 neurons are more dependent to the first 5 neurons than other way around. As shown, the performance of the first five neurons is 93.86\% while the performance of the last five neurons is only 66.06\% in SortedNet. In other words, the gain of adding the last five neurons is quite marginal and most probably prunnable, while the first 5 neurons contains most of the valuable information. %For example neuron 2 is more dependent to neuron 1 than neuron 1 to others. TODO: add several other examples that show this in the experiment
It is of interest to further investigate the dependency of neurons to one another and with other metrics which we will leave for future research.

%-------------------------------
%-------------------------------
%-------------------------------
%-------------------------------

% \section{Extra}
% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lccccccccc}
% \hline
% \textbf{Method}& All-in-One & \textbf{Pre-training}  & \textbf{Loss Function}& \textbf{Anytime(Inference Efficiency)}& \textbf{Dynamic}& \textbf{Search-Free} & \textbf{Additional Params}& \textbf{Hyperparams}\\
%  % &  &  &  &  &  &   &  &  \\
% \hline
% % \textbf{Stochastic Multi-Objective Bert}& Optional
% %  & $\mathcal{L}_{\downarrow b}^{\mathcal{DY}} =\frac{1}{N}\sum_{i=1}^N \mathcal{L} (f(x_i;W_{\downarrow b}),y_i)$  & \cmark & \cmark & \cmark  & None & None \\
% \textbf{Sorted-BERT} & Yes & Optional & $\sum_{b=r_{min}}^{r_{max}} p_B(b) \mathcal{L}_{\downarrow b}^{\mathcal{DY}} (\textbf{x},\textbf{y};W_{\downarrow b}) $ & \cmark & \cmark & \cmark & None & pB (b) \\
% \textbf{TinyBert} & No & Required & $\Sigma_{i=1}^{N}\Sigma_{b=r_{min}}^{b=r_{max}}\lambda_l\mathcal{L}_{layer}(f^S_b(x_i),f^T_{g(b)}(x_i))$ &	\xmark & \xmark & \xmark  &	None & $\Lambda=\{\lambda_l\}_{b=0}^{L}$ \\
% \textbf{DistilBert} & No & Required & $\mathcal{L}_{mlm}+\Sigma_{i=1}^{N}f^T(x_i)log(f^S(x_i))+\mathcal{L}_{cos}$ & \xmark & \xmark & \xmark &  None & $p_i=exp(z_i/T)/\Sigma_j exp(z_j/T)$ \\
% \textbf{LayerDrop} & No & Optional  & $\mathcal{L}(\textbf{x},\textbf{y};M \odot W)$ & \cmark & \cmark & \xmark  & None & $p$ \\
% \textbf{Bert of Theseus} & No & Optional & $\mathcal{L}(\textbf{x},\textbf{y};y_{b+1})=r_{b+1}f^{scc}_b(y_b)+(1-r_{b+1})f^{prd}_b(y_b)$ &	\xmark & \cmark & \cmark  &	$F^{scc}=\{f^{scc}_b\}_{b=r_{min}}^{r_{max}}$ & $r_{b+1} \sim Bernouli(p)$ \\
% \textbf{DeeBert} & No & Not Required  & $\Sigma_{b=r_{min}}^{r_{max}}\frac{1}{N}\Sigma_{i=1}^{N}\mathcal{L}(f(x_i;W_{\downarrow b}^{*}),y_i)$ &	\xmark & \cmark & \xmark &	None & ? \\
% \textbf{DynaBert} & Partial & Optional & $\Sigma_{b=r_{min}}^{b=r_{max}}\Sigma_{i=1}^{N}\lambda_l\mathcal{L}_{layer}(f^S_b(x_i),f^T_{g(b)}(x_i))$ & \xmark & \cmark & \cmark  & None & ? \\
% \hline
% \end{tabular}}
% \caption{Comparison}
% \label{tab:comparison}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lccccccccc}
% \hline
% % & 		f1+acc	matthews correlation		F1+acc		pearson+spearman AVG
% &  Acc.(m+mm) &  Acc. & (F1+Acc.) & Matt. & Acc. & (F1+Acc.) & Acc. & (Pear.+Spear.) & \\
% \textbf{Model} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}&  \textbf{QQP} & \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}\\
% & (393K) & (67K) & (3.7K) & (8.5K) & (105K) & (364K) & (2.5K) & (5.7K) &\\  
% \hline
% \textbf{BERT-base*} &	83.5 &	91.5 &	89.5 &	54.3 &	91.2 &	89.8 &	71.1 &	88.9 & ? \\
% \hline
% \multicolumn{10}{c}{\textbf{Average Performance Over all Sub-Models}} \\
% \hline
% \textbf{DistilBERT*} &	79.0 &	90.7&	87.5&	43.6&	85.3&	84.9&	59.9&	81.2 & 76.51\\
% \textbf{Vanilla KD*}& 80.1&	90.5&	86.2&	45.1&	88&	88.1&	64.9&	84.9 & 78.48 \\
% \textbf{LayerDrop*}& 80.7&	90.7&	85.9&	45.4&	88.4&	88.3&	65.2&	85.7& 78.79 \\
% \textbf{BERT of Theseus*} &	82.3&	91.5&	89&	51.1&	89.5&	89.6&	68.2&	88.7 & 81.24 \\
% \hline
% \textbf{Sorted-BERT} & ? &	? &	? &	? &	? &	? &	? &	? & ? \\
% % \textbf{Sorted-BERT} & ? &	? &	? &	? &	? &	? &	? &	? & ? \\
% % \multicolumn{10}{c}{\textbf{6 Layers Models}} \\
% % \hline
% % \textbf{DistilBERT*} &	79.0 &	90.7&	87.5&	43.6&	85.3&	84.9&	59.9&	81.2 & 76.51\\
% % \textbf{Vanilla KD*}& 80.1&	90.5&	86.2&	45.1&	88&	88.1&	64.9&	84.9 & 78.48 \\
% % \textbf{LayerDrop*}& 80.7&	90.7&	85.9&	45.4&	88.4&	88.3&	65.2&	85.7& 78.79 \\
% % \textbf{BERT of Theseus*} &	82.3&	91.5&	89&	51.1&	89.5&	89.6&	68.2&	88.7 & 81.24 \\
% % \hline
% % \textbf{Sorted-BERT} & ? &	? &	? &	? &	? &	? &	? &	? & ? \\
% \hline
% \end{tabular}}
% \caption{In this table, we chose the baselines and the experiment from Bert of Theseus PAPER. Accordingly following their setup, we use the first 6 layers of the fine-tuned BERT-base to initialize the the smaller model since the over-parameterized nature of Transformer (Vaswani et al., 2017) could cause the model unable to converge while training on small datasets. We fine-tune BERT-base as the predecessor model for each task with the batch size of 32, the learning rate of $2\times10-5$, and the number of epochs as 4. As a result, we are able to obtain a predecessor model with comparable performance with that reported in previous studies (Sanh et al., 2019; Sun et al.,2019; Jiao et al., 2019). During module replacing, We fix the batch size as 32 for all evaluated tasks to reduce the search space. All r variables only sample once for a training batch. The maximum sequence length is set to 256 on QNLI and 128 for the other tasks. We perform grid search over the sets of learning rate lr as \{1e-5, 2e-5\} We apply an early stopping mechanism and select the model with the best performance on the development set. We conduct our experiments on a single Nvidia V100 16GB GPU. Median of 5 runs (random seeds=42,1010,2020,2022,4242,8080)}
% % MRPC - BERT-base:
% % 1:0.8489877822967298
% % 20:0.8485697759579214
% % 9090:0.8534517973856209
% % 5050:0.8566677522797876
% % 4040:0.8610160957422018
% % 4242:0.8665858095815646
% % 1010:0.8688594654943829
% % 7070:0.8691314259945886
% % 2022:0.8711185839523523
% % 8080:0.8718300367432437
% % 6060:0.8731083459024636
% % 42:0.8814291101055808
% % 3030: 0.8835909790537375
% % 2020:0.8899042155192571

% \label{tab:accents}
% \end{table*}


% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccc}
% \hline
% \textbf{Method} & \textbf{Anytime}& \textbf{Dynamic}& \textbf{Search-Free} & \textbf{Sample-wise Efficient Inference} \\
% \hline
% \textbf{Nested Bert} & \cmark & \cmark & \cmark &	\cmark \\
% \textbf{TinyBert}  & \xmark & \xmark & \xmark & \xmark \\
% \textbf{LayerDrop} & \xmark & \cmark & \xmark & \xmark \\
% \textbf{DeeBert} & \xmark & \cmark & \xmark & \cmark \\
% \textbf{DynaBert} & \xmark & \cmark & \cmark & \xmark \\
% \hline
% \end{tabular}}
% \caption{In the following table, we present a comparison of NestedBert with the most comparable methods available in the literature}
% \label{tab:comparison}
% \end{table*}



% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccccc}
% \hline
% \textbf{Model} & \textbf{Req. Phases} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}& \textbf{QQP}& \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}\\
% \hline
% \multicolumn{11}{c}{\textbf{Baselines}} \\
% \hline
% \textbf{6L6D} & FT(6L6D) & 60.41&	81.42&	81.15&	16.53&	62.40&	81.78&	55.60&	22.08&	57.67\\
% \textbf{DistilRoBERTa} & PT(6L6D+KD)+FT(6L6D) & 83.83&	92.55&	90.69&	61.64&	91.07&	91.40&	70.04&	88.67&	83.74\\
% \textbf{Vanilla KD*} & PT(6L12D)+PT(12L12D)+FT(6L6D+V. KD) & 84.18 &	92.54&	90.2&	60.97&	91.37&	91.64&	71.11&	88.86&	83.86\\
% \textbf{TA KD*} & PT(6L12D)+PT(12L12D)+FT(6L6D+TA KD) & 83.89&	92.54&	89.91&	61.15&	91.32&	91.7&	71.84&	88.94&	83.91\\
% \textbf{6L12D} & PT(12L12D)+FT(6L12D) & 84.78&	92.09&	90.40&	57.02&	90.77&	91.38&	65.70&	90.32&	82.81 \\
% \hline
% \textbf{Vanilla DynaRoBERTa*} & PT(12L12D)+FT(6L12D) & ?&	?&	?&	?	&?&	?&	?&	?&	? \\
% \textbf{DynaRoBERTa*} & PT(12L12D)+$FT_W$(.+KD)+$FT_{D,W}$(.+KD)+FT(6L12D)+DA & 86&	93.3&	88.5&	59.5	&91.4&	91.7&	72.9&	90&	84.16 \\
% \hline
% \multicolumn{11}{c} {\textbf{Proposed Methods}} \\
% \hline
% \textbf{6L12D+FT} & PT(12L12D)+FT(12L12D)+FT(6L12D) & 85.27&	92.66&	90.20&	57.90&	91.47&	91.50&	72.56&	90.57&	84.02 \\
% \textbf{8L12D+FT} & PT(12L12D)+FT(12L12D)+FT(8L12D) & 87.19&	92.89&	92.23&	59.65& & &	76.17&	90.89&	83.17 \\
% \textbf{AnytimeRoBERTa 6L12D} & PT(12L12D)$\rightarrow $ Dynamic & 84.06&	91.86&	90.47&	53.21&	90.54&	90.75&	66.79&	89.02 & 82.09\\
% \textbf{AnytimeRoBERTa 6L12D ($\Sigma_iL_i$)} & PT(12L12D)$\rightarrow $ Dynamic & 82.21&	92.09&	86.67&	53.41&	88.83&	? &	67.87&	89.09&	80.02\\
% \textbf{AnytimeRoBERTa 6L12D} & PT(12L12D)$\rightarrow $ Dynamic+FT(6L12D) & 84.96&	91.86&	90.97&	53.92&	90.57&	90.75&	71.84&	89.33&	83.02\\
% \textbf{AnytimeRoBERTa 6L12D ($\Sigma_iL_i$)} & PT(12L12D)$\rightarrow $ Dynamic+FT(6L12D) & 85.14&	92.32&	90.85&	59.27&	91.67& ?	&72.92&	89.48&	83.09\\
% \hline

% \textbf{AnytimeRoBERTa 8L12D} & PT(12L12D)$\rightarrow $ Dynamic & 86.07&	91.97&	91.51&	54.17&	90.81&	90.79&	68.95&	89.45&	82.97\\
% \textbf{AnytimeRoBERTa 8L12D ($\Sigma_iL_i$)} & PT(12L12D)$\rightarrow $ Dynamic & 83.33&	93.23&	89.78&	57.22&	89.40&	91.29&	75.09&	89.67&	83.63\\
% \textbf{AnytimeRoBERTa 8L12D} & PT(12L12D)$\rightarrow $ Dynamic+FT(8L12D) & & 91.97&	90.78&	56.27& & &	71.84&	89.72&	80.12 \\
% \textbf{AnytimeRoBERTa 8L12D ($\Sigma_iL_i$)} & PT(12L12D)$\rightarrow $ Dynamic+FT(8L12D) & 	& 93.23&	92.39&	59.83& & & 76.17&	89.91&	82.29 \\
% \hline

% \end{tabular}}
% \caption{Best results of the fine-tuned model}
% \label{tab:accents}
% \end{table*}





% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccc}
% \hline
% \multicolumn{9}{c}{\textbf{6 Layers Sub-Model - BERT}} \\
% \hline
% \textbf{Model} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}& \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}\\
% \hline

% \hline
% \textbf{Proposed (Ind.)}& 84.96&	90.94&	90.97&	53.92&	90.57&	71.84&	89.33&	81.79 \\
% \textbf{Proposed (Sum.)}& 85.14&	92.32&	90.85&	59.27&	91.67&	72.92&	89.48&	83.09 \\
% \hline
% \end{tabular}}
% \caption{Bert 6 Layers with Baselines}
% \label{tab:bert-6-baselines}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccc}
% \hline
% \multicolumn{9}{c}{\textbf{6 Layers RoBertA}} \\
% \hline
% \textbf{Model} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}& \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}\\
% \hline
% \textbf{Vanilla KD}& 84.18&	92.54&	90.2&	60.97&	91.37&	71.11&	88.86&	82.75 \\
% \textbf{TA KD}& 83.89&	92.54&	89.91&	61.15&	91.32&	71.84&	88.94&	82.80 \\
% \textbf{DistilRoBERTa} &	83.83&	92.55&	90.69&	61.64&	91.07&	70.04&	88.67&	82.64 \\
% \hline
% % \textbf{Proposed (Ind.)}& 84.96&	90.94&	90.97&	53.92&	90.57&	71.84&	89.33&	81.79 \\
% \textbf{Sorted-RoBERTa+FT}& 85.14&	92.32&	90.85&	59.27&	91.67&	72.92&	89.48&	83.09 \\
% \hline
% \end{tabular}}
% \caption{Best results of the fine-tuned model}
% \label{tab:accents}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccc}
% \hline
% \multicolumn{9}{c}{\textbf{Ablation Study}} \\
% \hline
% \textbf{Model} & \textbf{MNLI}& \textbf{SST-2}& \textbf{MRPC}& \textbf{CoLA}& \textbf{QNLI}& \textbf{RTE}& \textbf{STS-B}& \textbf{AVG}\\
% \hline
% % \textbf{Proposed (Ind.)}& 84.96&	90.94&	90.97&	53.92&	90.57&	71.84&	89.33&	81.79 \\
% \textbf{Sorted-BERT}& 85.14&	92.32&	90.85&	59.27&	91.67&	72.92&	89.48&	83.09 \\
% \textbf{Sorted-BERT+GA}& 85.14&	92.32&	90.85&	59.27&	91.67&	72.92&	89.48&	83.09 \\
% \textbf{Sorted-BERT+ALL}& 85.14&	92.32&	90.85&	59.27&	91.67&	72.92&	89.48&	83.09 \\

% \hline
% \end{tabular}}
% \caption{Best results of the fine-tuned model}
% \label{tab:accents}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccccccccc}
% \hline
% \multicolumn{11}{c}{\textbf{Arabic Jaber}} \\
% \hline
% & F1 & F1 & Pearson & Jaccard & F1 & F1 & Acc. & F1 & \\
% \textbf{Model} & \textbf{MQ2Q}& \textbf{MDD}& \textbf{SVREG}& \textbf{SEC}& \textbf{FID}& \textbf{OOLD}& \textbf{XNLI}& \textbf{OHSD}& \textbf{AVG}& \textbf{AVG w/o ours}\\
% \hline
% Sorted-Jaber (1L)&	31.09&	28.13&	64.85&	36.17&	79.56&	87.12&	37.83&	62.32&	53.38& 21.71\\
% Sorted-Jaber (2L)&	50.45&	42.51&	76.73&	40.37&	82.38&	87.99&	53.73&	75.68&	63.73& 23.06\\
% Sorted-Jaber (4L)&	48.89&	47.06&	80.25&	41.71&	83.52&	89.83&	55.30&	79.79&	65.79& 29.60\\
% Sorted-Jaber (4L)&	69.57&	51.29&	81.12&	42.76&	84.06&	89.56&	60.92&	80.17&	69.93&40.95\\
% Sorted-Jaber (5L)&	70.77&	53.65&	83.50&	42.91&	84.24&	90.52&	63.21&	88.22&	71.08&50.77\\
% Sorted-Jaber (6L)&	73.10&	53.34&	84.81&	43.12&	84.15&	90.59&	64.66&	79.67&	71.68&60.36\\
% Sorted-Jaber (7L)&	73.33&	52.83&	84.96&	43.46&	84.05&	91.07&	65.86&	79.67&	71.90&62.41\\
% Sorted-Jaber (8L)&	73.48&	53.30&	84.49&	43.30&	84.26&	91.22&	66.75&	80.37&	72.15&65.49\\
% Sorted-Jaber (9L)&	73.59&	54.07&	85.37&	43.39&	84.37&	91.18&	68.47&	79.91&	72.54&70.16\\
% Sorted-Jaber (10L)&	73.48&	52.56&	85.90&	43.28&	84.27&	90.50&	68.63&	80.37&	72.37& 73.54\\
% Sorted-Jaber (11L)&	73.78&	52.10&	85.43&	43.45&	84.27&	90.54&	67.95&	79.67&	72.15&74.42\\
% Sorted-Jaber (12L)&	74.00&	52.46&	85.86&	43.09&	84.27&	91.03&	67.35&	80.37&	72.30&75.20\\
% \hline
% \end{tabular}}
% \caption{A comparison of the performance of different sub-models with and without the dynamic approach. The model's performance will improve if we have more budgets and calculate the representation of deeper layers.}
% \label{tab:accents}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lccccccccc}
% \hline
% \multicolumn{10}{c}{\textbf{Arabic Jaber}} \\
% \hline
% & F1 & F1 & Pearson & Jaccard & F1 & F1 & Acc. & F1 & \\
% \textbf{Model} & \textbf{MQ2Q}& \textbf{MDD}& \textbf{SVREG}& \textbf{SEC}& \textbf{FID}& \textbf{OOLD}& \textbf{XNLI}& \textbf{OHSD}& \textbf{AVG}\\
% \hline
% Pre-trained 150K (6L) + FT &	73.65&	62.38&	85.45&	45.82&	84.24&	90.61&	66.14&	82.99&	73.91\\
% Sorted-Jaber (6L) &	73.10&	53.34&	84.81&	43.12&	84.15&	90.59&	64.66&	79.67&	71.68\\
% TinyBERT 300K (6L) &	72.06& 60.54 &	81.69	& 42.51	& 83.56&	89.22& 61.16&	80.63&	71.42\\
% \hline
% \end{tabular}}
% \caption{A comparison of the performance of different sub-models with and without the dynamic approach. The model's performance will improve if we have more budgets and calculate the representation of deeper layers.}
% \label{tab:accents}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \resizebox{\textwidth}{!}{  
% \begin{tabular}{lcccc}
% \hline
% \multicolumn{5}{c}{\textbf{Arabic Jaber}} \\
% \hline
% & F1 & F1 & Accuracy & \\
% \textbf{Model} & \textbf{FID}& \textbf{OOLD}& \textbf{XNLI} & \textbf{AVG}\\
% \hline
% JABER &	$85.38\pm0.35$ & $92.2\pm0.27$ & $71.82\pm0.54$ & 83.13 \\
% \hline
% JABER + RKD & $84.81\pm0.5$ & $91.79\pm0.51$ & $71.87\pm0.18$ & 82.82 \\
% JABER + (ATT+CE) & $84.59\pm0.57$ & $91.51\pm0.37$ & $72.53\pm0.24$ & 82.88 \\
% \hline
% JABER + PKD & $85.71\pm0.27$ & $92.33\pm0.43$ & $71.99\pm0.48$ & $\textbf{83.34}$ \\
% JABER + PKD + (ATT+CE) & $85.18\pm0.32$ & $92.33\pm0.26$ & $71.91\pm0.21$ & 83.14 \\
% \hline
% JABER + ALP & $84.87\pm0.57$ & $92.23\pm0.25$	 & $71.75\pm0.25$ & 82.95	 \\
% \hline
% \end{tabular}}
% \caption{In this experiment, Saber is the teacher and Jaber is the student!}
% \label{tab:accents}
% \end{table*}

\end{document}
