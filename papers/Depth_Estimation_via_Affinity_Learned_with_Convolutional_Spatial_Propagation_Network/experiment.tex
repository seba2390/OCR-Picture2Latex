\section{Experiments}
\label{sec:exp}
In this section, we describe our implementation details, the datasets and evaluation metrics used in our experiments. Then present comprehensive evaluation of CSPN on both depth refinement and sparse to dense tasks. 

%\addlinespace
\noindent\textbf{Implementation details.}
The weights of ResNet in the encoding layers for depth estimation (\secref{subsec:unet}) are initialized with models pretrained on the
ImageNet dataset~\cite{deng2009imagenet}.  
Our models are trained with SGD optimizer, and we use a small batch size of 24 and train for 40 epochs for all the experiments, and the model performed best on the validation set is used for testing.
The learning rate starts at 0.01, and is reduced to 20$\%$ every 10 epochs. A small weight decay of $10^{-4}$ is applied for regularization.
We implement our networks based on PyTorch~\footnote{http://pytorch.org/} platform, and use its element-wise product and convolution operation for our one step CSPN implementation.

For depth, we show that propagation with hidden representation $\ve{H}$ only achieves marginal improvement over doing propagation within the domain of depth $D$. Therefore, we perform all our experiments direct with $D$ rather than learning an additional embedding layer. For sparse depth samples, we adopt 500 sparse samples as that is used in~\cite{Ma2017SparseToDense}.

\subsection{Datasets and Metrics}
\label{subsec:data_metric}
All our experiments are evaluated on two datasets: NYU v2~\cite{silberman2012indoor} and KITTI ~\cite{geiger2012we}, using commonly used metrics.\\
%\addlinespace
\noindent\textbf{NYU v2.} The NYU-Depth-v2 dataset consists of RGB and depth images collected from 464 different indoor scenes. We use the official split of data, where 249 scenes are used for training and we sample 50K images out of the training set with the same manner as~\cite{Ma2017SparseToDense}. For testing, following the standard setting~\cite{eigen2015predicting,peng2016depth}, the small labeled test set with 654 images is used the final performance. The original image of size $\by{640}{480}$ are first downsampled to half and then center-cropped, producing a network input size of $\by{304}{228}$.\\
%\addlinespace
\noindent\textbf{KITTI odometry dataset.} It includes both camera and LiDAR measurements, and consists of 22 sequences. Half of the sequence is
used for training while the other half is for evaluation. Following~\cite{Ma2017SparseToDense}, we use all 46k images from the training sequences for training, and a random subset of 3200 images from the test sequences for evaluation. Specifically, we take the bottom part $\by{912}{228}$ due to no depth at the top area, and only evaluate the pixels with ground truth.\\
%\addlinespace
\noindent\textbf{Metrics.} We adopt the same metrics and use their implementation in \cite{Ma2017SparseToDense}. Given ground truth depth $D^* = \{d^*\}$ and predicted depth $D = \{d\}$, the metrics include:  (1) RMSE: $\sqrt{\frac{1}{|D|}\sum_{d \in D}||d^* - d||^2}$. (2) Abs Rel: $\frac{1}{|D|}\sum_{d \in D}|d^* - d|/d^*$. (3) $\delta_t$: $\%$ of $d \in D$, s.t. $max(\frac{d^*}{d}, \frac{d}{d^*})<t$, where $t \in \{1.25, 1.25^2, 1.25^3\}$. Nevertheless, for the third metric, we found that the depth accuracy is very high when sparse depth is provided, $t = 1.25$ is already a very loosen criteria where  almost $100\%$ of pixels are judged as correct, which can hardly distinguish different methods as shown in (\tabref{tbl:sota}). Thus we adopt more strict criteria for correctness by choosing $t \in \{1.02, 1.05, 1.10\}$.

% Sq Rel: $\frac{1}{|D|}\sum_{d \in D}||d^* - d||^2/d^*$

% RMSE log: $\sqrt{\frac{1}{|D|}\sum_{d_{pred}\in D}||\log d_{gt} - \log d_{pred}||^2}$
% $
% mean: $\frac{1}{|N|}\sum_{n_{pred}\in N}(n_{gt}\cdot n_{pred})$

% median: $median([(n_{gt}\cdot n_{pred})]_{n_{pred} \in N})$

% degree: $\%$ of $n_{pred} \in N$, s.t. $(n_{gt}\cdot n_{pred}) < degree$ 

\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{fig/Image_size.pdf}
\caption{Ablation study.(a) RMSE (left axis, lower the better) and $\delta < 1.02$ (right axis, higher the better) of CSPN \wrt number of iterations. Horizontal lines show the corresponding results from SPN~\cite{liu2017learning}. (b) RMSE and $\delta < 1.02$ of CSPN \wrt kernel size. (c) Testing times \wrt input image size.}
\label{fig:ab_study}
\end{figure}

\subsection{Parameter Tuning and Speed Study}
\label{subsec:ablation}
We first evaluate various hyper-parameters including kernel size $k$, number of iterations $N$ in \equref{eqn:cspn} using the NYU v2 dataset. Then we provide an empirical evaluation of the running speed with a Titan X GPU on a computer with 16 GB memory.

%\addlinespace
\noindent\textbf{Number of iterations.} We adopt a kernel size of $3$ to validate the effect of iteration number $N$ in CSPN. 
As shown in  \figref{fig:ab_study}(a), our CSPN has outperformed SPN~\cite{liu2017learning} (horizontal line) when iterated only four times. Also, we can get even better performance when more iterations are applied in the model during training. From our experiments, the accuracy is saturated when the number of iterations is increased to $24$. 

%\addlinespace
\noindent\textbf{Size of convolutional kernel.} As shown in  \figref{fig:ab_study}(b), larger convolutional kernel has similar effect with more iterations, due to larger context is considered for propagation at each time step. Here, we hold the iteration number to $N = 12$, and we can see the performance is better when $k$ is larger while saturated at size of $7$. 
We notice that the performance drop slightly when kernel size is set to $9$. This is because we use a fixed number of epoch, \ie 40, for all the experiments, while larger kernel size induces more affinity to learn in propagation, which needs more epoch of data to converge. Later, when we train with more epochs, the model reaches similar performance with kernel size of $7$.
Thus, we can see using kernel size of $7$ with $12$ iterations reaches similar performance of using kernel size of $3$ with $20$ iterations, which shows CSPN has the trade-off between kernel size and iterations. In practice, the two settings run with similar speed, while the latter costs much less memory. Therefore, we adopt kernel size as $3$ and number of iterations as $24$ in our comparisons.

%\addlinespace
\noindent\textbf{Concatenation end-point for mirror connection.} 
As discussed in \secref{subsec:unet}, based on the given metrics, we experimented three concatenation places, \ie after \textit{conv}, after \textit{bn} and after \textit{relu} by fine-tuning with weights initialized from encoder network trained without mirror-connections.
The corresponding RMSE are  $0.531$, $0.158$ and $0.137$ correspondingly. Therefore, we adopt the proposed concatenation end-point.

%\addlinespace
\noindent\textbf{Running speed}
In  \figref{fig:ab_study}(c), we show the running time comparison between the SPN and CSPN with kernel size as $3$. We use the author's PyTorch implementation online. As can be seen, we can get better performance within much less time. For example, four iterations of CSPN on one $1024\times768$ image only takes 3.689~$ms$, while SPN takes 127.902~$ms$. In addition, the time cost of SPN is linearly growing \wrt image size, while the time cost of CSPN is irrelevant to image size and much faster as analyzed in \secref{subsec:time}. In practice, however, when the number of iterations is large, \eg ``CSPN Iter 20'', we found the practical time cost of CSPN also grows \wrt image size. This is because of PyTorch-based implementation, which keeps all the variables for each iteration in memory during the testing phase. Memory paging cost becomes dominant with large images. In principle, we can eliminate such a memory bottleneck by customizing a new operation, which will be our future work. Nevertheless, without coding optimation, even at high iterations with large images, CSPN's speed is still twice as fast as SPN. 

\begin{table}[t]
	\centering
	\fontsize{7.5}{7.5}\selectfont
	\caption{Comparison results on NYU v2 dataset~\cite{silberman2012indoor} between different variants of CSPN and other state-of-the-art strategies. Here, ``Preserve SD'' is short for preserving the depth value at sparse depth samples.}
	\bgroup
	\def\arraystretch{1.3}
	\setlength{\tabcolsep}{4.5pt} % General space between cols (6pt standard)
	\begin{tabular}{lccccccccc}
		\hline
		\multicolumn{1}{l}{\multirow{2}{*}{Method}}  & \multirow{2}{*}{Preserve ``SD''} & \multicolumn{2}{c}{Lower the better}   & \multicolumn{6}{c}{Higher the better} \\ \cline{3-10}
		\multicolumn{1}{l}{} & & RMSE  & \multicolumn{1}{c|}{REL} & $\delta_{1.02}$ & $\delta_{1.05}$ & $\delta_{1.10}$ & $\delta_{1.25}$ & $\delta_{1.25^2}$ & $\delta_{1.25^3}$ \\ \hline
		~~Ma \etal \cite{Ma2017SparseToDense} &   & 0.230  & 0.044    & 52.3            & 82.3            & 92.6            & 97.1            & 99.4              & 99.8              \\ \hline
		+Bilateral~\cite{barron2016fast}&   & 0.479  & 0.084    & 29.9            & 58.0            & 77.3            & 92.4            & 97.6              & 98.9     \\\hline
		+SPN~\cite{liu2016learning}      &  & 0.172          & 0.031                    & 61.1            & 84.9            & 93.5            & 98.3            & 99.7              & 99.9              \\ \hline
	    +CSPN (Ours)     &  & 0.162          & 0.028                    & 64.6            & 87.7            & 94.9            & 98.6            & 99.7              & 99.9              \\\hline
	    +UNet (Ours)     &  & 0.137          & 0.020                    & 78.1       & 91.6 & 96.2 & 98.9 & 99.8 & 100.0              \\ \hline
        \hline
        +ASAP~\cite{igarashi2005rigid} & \checkmark  & 0.232  & 0.037    & 59.7            & 82.5            & 91.3            & 97.0            & 99.2              & 99.7     \\\hline
		+Replacement &\checkmark   & 0.168  & 0.032    & 56.5            & 85.7            & 94.4            & 98.4            & 99.7              & 99.8              \\ \hline
        +SPN~\cite{liu2016learning} &\checkmark & 0.162    & 0.027      & 67.5            & 87.9            & 94.7            & 98.5            & 99.7              & 99.9  \\\hline
	    +UNet(Ours)+SPN      & \checkmark &  0.144 & 0.022 & 75.4 & 90.8 & 95.8 & 98.8 & 99.8 & 100.0   \\ \hline
	    +CSPN (Ours)   & \checkmark & 0.136    & 0.021      & 76.2  & 91.2  & 96.2   & 99.0   &99.8     & 100.0   \\ \hline
        +UNet+CSPN (Ours) & \checkmark & \textbf{0.117} & \textbf{0.016} & \textbf{83.2} & \textbf{93.4} & \textbf{97.1} & \textbf{99.2} & \textbf{99.9} & 100.0   \\ \hline
	\end{tabular}
	\egroup
\label{tbl:sota}
\end{table}

\subsection{Comparisons}
\label{subsec:compare}
We compare our methods against various SOTA baselines in terms of the two proposed tasks. (1) Refine the depth map with the corresponding color image. (2) Refine the depth using both the color image and sparse depth samples. For the baseline methods such as SPN~\cite{liu2016learning} and Sparse-to-Dense~\cite{Ma2017SparseToDense}, we use the released code released online from the authors.

%\addlinespace
\noindent\textbf{NYU v2.} \tabref{tbl:sota} shows the comparison results. Our baseline methods are the depth output from the network of~\cite{Ma2017SparseToDense}, together with the corresponding color image.  
At upper part of \tabref{tbl:sota} we show the results for depth refinement with color only. 
At row ``Bilateral'', we refine the network output from~\cite{Ma2017SparseToDense} using bilateral filtering~\cite{barron2016fast} as a post-processing module with their spatial-color affinity kernel tuned on our validation set. Although the output depths snap to image edges (\figref{fig:example}(c)), the absolute depth accuracy is dropped since the filtering over-smoothed original depths. At  row ``SPN'', we show the results filtered with SPN~\cite{liu2017learning}, using the author provided affinity network. Due to joint training, the depth is improved with the learned affinity, yielding both better depth details and absolute accuracy. Switching SPN to CSPN (row ``CSPN'') yields relative better results.
Finally, at the row ``UNet'', we show the results of just modifying the network with mirror connections as stated in~\secref{subsec:unet}.  The results turn out to be even better than that from SPN and CSPN, demonstrating that by simply adding feature from beginning layers, the depth can be better learned.

At lower part of \tabref{tbl:sota}, we show the results using both color image and sparse depth samples, and all the results preserves the sparse depth value provided. We randomly select 500 depth samples per image from the ground truth depth map. 

\begin{figure}[t]
\centering
\includegraphics[width=0.98\textwidth]{fig/nyudepth.pdf}
\caption{Qualitative comparisons on NYU v2 dataset. (a) Input image; (b) Sparse depth samples(500); (c) Ma \etal \cite{Ma2017SparseToDense}; (d) UNet(Ours)+SPN\cite{liu2016learning}; (e) UNet+CSPN(Ours); (f) Ground Truth. Most significantly improved regions are highlighted with yellow dash boxes (best view in color).}
\label{fig:nyudepth}
\end{figure}

For comparison, we consider a baseline method using as-rigid-as-possible (ASAP)~\cite{igarashi2005rigid} warping. Basically the input depth map is warped with the sparse depth samples as control points. At row ``ASAP'', we show its results, which just marginally improves the estimation over the baseline network. For SPN, we also apply the similar replacement operation in \equref{eqn:cspn_sp} for propagation, and the results are shown at row ``SPN'', which outperforms both the results form ASAP and SPN without propagation of SD due to joint training helps fix the error of warping.  At row ``UNet + SPN'', we use our UNet architecture for learning affinity with SPN, which outperforms ``SPN'', while we did not see any improvements compared with that only using UNet. 
Nevertheless, by replacing SPN with our CSPN, as shown in row ``UNet + CSPN'', the results can be further improved by a large margin and performs best in all cases. We think this is mostly because CSPN updates more efficiently than SPN during the training. 
Some visualizations are shown in \figref{fig:nyudepth}. We found the results from CSPN do capture better structure from images (highlighted with dashed bounding boxes) than that from other state-of-the-art strategies.

%\addlinespace
\noindent\textbf{KITTI.} \tabref{tbl:sota_kitti} shows the depth refinement with both color and sparse depth samples. Ours final model ``UNet + CSPN'' largely outperforms other SOTA strategies, which shows the generalization of the proposed approach. For instance, with a very strict metric $\delta < 1.02$, ours improves the baseline~\cite{Ma2017SparseToDense} from $30\%$ to $70\%$, which is more than $2\times$ better. More importantly, CSPN is running very efficiently, thus can be applied to real applications.
Some visualization results are shown at the bottom in \figref{fig:kitti}. Compared to the network outputs from~\cite{Ma2017SparseToDense} and SPN refinement, CSPN sees much more details and thin structures such as poles near the road (first image (f)), and trunk on the grass (second image (f)). For the third image, we highlight a car under shadow at left, whose depth is difficult to learn. We can see SPN fails to refine such a case in (e) due to globally vast lighting variations, while CSPN learns local contrast and successfully recover the silhouette of the car. Finally, we also submit our results to the new KITTI depth completion challenge~\footnote{\url{http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion}} and show that our results is better than previous SOTA method~\cite{uhrig2017sparsity}. 

\begin{table}[!htpb]
	\centering
	\caption{Comparison results on KITTI dataset~\cite{geiger2012we} 
	%between our proposed UNet and CSPN and other state-of-the-art strategies. Here, ``Preserve SD'' is short for preserving the depth value at sparse depth samples.
	}
	\label{tbl:sota_kitti}
	\fontsize{7.5}{7.5}\selectfont
	\bgroup
	\def\arraystretch{1.3}
	\setlength{\tabcolsep}{4.5pt} % General space between cols (6pt standard)
    \begin{tabular}{lccccccccc}
	\hline
		\multicolumn{1}{l}{\multirow{2}{*}{Method}}  & \multirow{2}{*}{Preserve ``SD''} & \multicolumn{2}{c}{Lower the better}   & \multicolumn{6}{c}{Higher the better} \\ \cline{3-10}
		\multicolumn{1}{l}{} & & RMSE  & \multicolumn{1}{c|}{REL} & $\delta_{1.02}$ & $\delta_{1.05}$ & $\delta_{1.10}$ & $\delta_{1.25}$ & $\delta_{1.25^2}$ & $\delta_{1.25^3}$ \\ \hline
	~~Ma \etal ~\cite{Ma2017SparseToDense}                    &                                 & 3.378                                 & 0.073                    & 30.0            & 65.8            & 85.2            & 93.5            & 97.6              & 98.9              \\ \hline
	+SPN~\cite{liu2016learning}                   & \checkmark                                & 3.243                                 & 0.063                    & 37.6            & 74.8            & 86.0            & 94.3            & 97.8              & 99.1              \\ \hline
	+CSPN(Ours)            & \checkmark                                & 3.029                                 & 0.049                    & 66.6            & 83.9            & 90.7            & 95.5            & 98.0              & 99.0              \\ \hline
	+UNet(Ours)            &                              & 3.049                                 & 0.051                    & 62.6            & 83.2            & 90.2            & 95.3            & 97.9              & 99.0              \\ \hline
	+UNet(Ours)+SPN        & \checkmark                               & 3.248                                 & 0.059                    & 52.1            & 79.0            & 87.9            & 94.4            & 97.7              & 98.9              \\ \hline
	+UNet+CSPN(Ours)       & \checkmark                                & \textbf{2.977}                        & \textbf{0.044}           & \textbf{70.2}   & \textbf{85.7}   & \textbf{91.4}   & \textbf{95.7}   & \textbf{98.0}     & \textbf{99.1}     \\ \hline
    \end{tabular}
	\egroup
\label{tbl:sota_kitti}
\end{table}

