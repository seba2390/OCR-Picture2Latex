\section{Our Approach}
We formulate the problem as an anisotropic diffusion process and the diffusion tensor is learned through a deep CNN directly from the given image, which guides the refinement of the output.

\begin{figure}[t]
\includegraphics[width=1.0\textwidth]{fig/CSPN_SPN2.pdf}
\caption{Comparison between the propagation process in SPN~\cite{liu2017learning} and CPSN in this work.}
\label{fig:compare}
\end{figure}

\subsection{Convolutional Spatial Propagation Network}
% demonstrate the thereom is hold when turns to be convolution.
Given a depth map $D_o \in \spa{R}^{m\times n}$ that is output from a network, and image $\ve{X} \in \spa{R}^{m\times n}$, our task is to update the depth map to a new depth map $D_n$ within $N$ iteration steps, which first reveals more details of the image, and second improves the per-pixel depth estimation results. 

\figref{fig:compare}(b) illustrates our updating operation. Formally, without loss of generality, we can embed the $D_o$ to some hidden space $\ve{H} \in \spa{R}^{m \times n \times c}$. The convolutional transformation functional with a kernel size of $k$ for each time step $t$ could be written as,
\begin{align}
    \ve{H}_{i, j, t + 1} &= \sum\nolimits_{a,b = -(k-1)/2}^{(k-1)/2} \kappa_{i,j}(a, b) \odot \ve{H}_{i-a, j-b, t} \nonumber \\
\mbox{where,~~~~}
    \kappa_{i,j}(a, b) &= \frac{\hat{\kappa}_{i,j}(a, b)}{\sum_{a,b, a, b \neq 0} |\hat{\kappa}_{i,j}(a, b)|}, \nonumber\\
    \kappa_{i,j}(0, 0) &= 1 - \sum\nolimits_{a,b, a, b \neq 0}\kappa_{i,j}(a, b)
\label{eqn:cspn}
\end{align}
where the transformation kernel $\hat{\kappa}_{i,j} \in \spa{R}^{k\times k \times c}$ is the output from an affinity network, which is spatially dependent on the input image. The kernel size $k$ is usually set as an odd number so that the computational context surrounding pixel $(i, j)$ is symmetric.
$\odot$ is element-wise product. Following~\cite{liu2017learning}, we normalize kernel weights between range of $(-1, 1)$ so that the model can be stabilized and converged by satisfying the condition $\sum_{a,b, a,b \neq 0} |\kappa_{i,j}(a, b)| \leq 1$. Finally, we perform this iteration $N$ steps to reach a stationary distribution.

% theorem, it follows diffusion with PDE 
%\addlinespace
\noindent\textbf{Correspondence to diffusion process with a partial differential equation (PDE).} \\
Similar with~\cite{liu2017learning}, here we show that our CSPN holds all the desired properties of SPN.
Formally, we can rewrite the propagation in \equref{eqn:cspn} as a process of diffusion evolution by first doing column-first vectorization of feature map $\ve{H}$ to $\ve{H}_v \in \spa{R}^{\by{mn}{c}}$.
\begin{align}
     \ve{H}_v^{t+1} = 
     \begin{bmatrix}
    1-\lambda_{0, 0}  & \kappa_{0,0}(1,0) & \cdots & 0 \\
    \kappa_{1,0}(-1,0)   & 1-\lambda_{1, 0} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    \vdots & \cdots & \cdots & 1-\lambda_{m,n} \\
\end{bmatrix} = \ve{G}\ve{H}_v^{t}
\label{eqn:vector}
\end{align}
where $\lambda_{i, j} = \sum_{a,b}\kappa_{i,j}(a,b)$ and $\ve{G}$ is a $\by{mn}{mn}$ transformation matrix. The diffusion process expressed with a partial differential equation (PDE) is derived as follows, 
\begin{align}
     \ve{H}_v^{t+1} &= \ve{G}\ve{H}_v^{t} = (\ve{I} - \ve{D} + \ve{A})\ve{H}_v^{t} \nonumber\\
     \ve{H}_v^{t+1} - \ve{H}_v^{t} &= - (\ve{D} - \ve{A}) \ve{H}_v^{t} \nonumber\\
     \partial_t \ve{H}_v^{t+1} &= -\ve{L}\ve{H}_v^{t}
\label{eqn:proof}
\end{align}
where $\ve{L}$ is the Laplacian matrix, $\ve{D}$ is the diagonal matrix containing all the $\lambda_{i, j}$, and $\ve{A}$ is the affinity matrix which is the off diagonal part of $\ve{G}$.

In our formulation, different from~\cite{liu2017learning} which scans the whole image in four directions~(\figref{fig:compare}(a)) sequentially, CSPN propagates a local area towards all directions at each step~(\figref{fig:compare}(b)) simultaneously, \ie with~\by{k}{k} local context, while larger context is observed when recurrent processing is performed, and the context acquiring rate is in an order of $O(kN)$.

In practical, we choose to use convolutional operation due to that it can be efficiently implemented through image vectorization, yielding real-time performance in depth refinement tasks.

Principally, CSPN could also be derived from loopy belief propagation with sum-product algorithm~\cite{kschischang2001factor}. However, since our approach adopts linear propagation, which is efficient while just a special case of pairwise potential with L2 reconstruction loss in graphical models. Therefore, to make it more accurate, we call our strategy convolutional spatial propagation in the field of diffusion process.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{fig/hist.pdf}
\caption {(a) Histogram of RMSE with depth maps from~\cite{Ma2017SparseToDense} at given sparse depth points.  (b) Comparison of gradient error between depth maps with sparse depth replacement (blue bars) and with ours CSPN (green bars), where ours is much smaller. Check~\figref{fig:gradient} for an example. Vertical axis shows the count of pixels.}
\label{fig:hist}
\end{figure}

\subsection{Spatial Propagation with Sparse Depth Samples}
In this application, we have an additional sparse depth map $D_s$ (\figref{fig:gradient}(b)) to help estimate a depth depth map from a RGB image. Specifically, a sparse set of pixels are set with real depth values from some depth sensors, which can be used to guide our propagation process. 

Similarly, we also embed the sparse depth map $D_s = \{d_{i,j}^s\}$ to a hidden representation $\ve{H}^s$,  and we can write the updating equation of $\ve{H}$ by simply adding a replacement step after performing \equref{eqn:cspn}, 
\begin{align}
    \ve{H}_{i, j, t+1} = (1 - m_{i, j}) \ve{H}_{i, j, t+1}  +  m_{i, j} \ve{H}_{i, j}^s 
\label{eqn:cspn_sp}
\end{align}
where $m_{i, j} = \spa{I}(d_{i, j}^s > 0)$ is an indicator for the availability of sparse depth at $(i, j)$. 

In this way, we guarantee that our refined depths have the exact same value at those valid pixels in sparse depth map. Additionally, we propagate the information from those sparse depth to its surrounding pixels such that the smoothness between the sparse depths and their neighbors are maintained. 
Thirdly, thanks to the diffusion process, the final depth map is well aligned with image structures. 
This fully satisfies the desired three properties for this task which is discussed in our introduction (\ref{sec:intro}). 

% it performs a non-symmetric propagation where the information can only be diffused from the given sparse depth to others, while not the other way around.

% still follows PDE
In addition, this process is still following the diffusion process with PDE, where the transformation matrix can be built by simply replacing the rows satisfying $m_{i, j} = 1$ in $\ve{G}$ (\equref{eqn:vector}), which are corresponding to sparse depth samples, by $\ve{e}_{i + j*m}^T$. Here $\ve{e}_{i + j*m}$ is an unit vector with the value at $i + j*m$ as 1.
Therefore, the summation of each row is still $1$, and obviously the stabilization still holds in this case.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig/fig2.pdf}
\caption{Comparison of depth map~\cite{Ma2017SparseToDense} with sparse depth replacement and with our CSPN \wrt smoothness of depth gradient at sparse depth points. (a) Input image. (b) Sparse depth points. (c) Depth map with sparse depth replacement. (d) Depth map with our CSPN with sparse depth points. We highlight the differences in the red box.}
\label{fig:gradient}
\end{figure}

Our strategy has several advantages over the previous state-of-the-art sparse-to-dense methods~\cite{Ma2017SparseToDense,LiaoHWKYL16}.
In \figref{fig:hist}(a), we plot a histogram of depth displacement from ground truth at given sparse depth pixels from the output of Ma \etal~\cite{Ma2017SparseToDense}. It shows the accuracy of sparse depth points cannot preserved, and some pixels could have very large displacement (0.2m), indicating that directly training a CNN for depth prediction does not preserve the value of real sparse depths provided. To acquire such property, 
one may simply replace the depths from the outputs with provided sparse depths at those pixels, however, it yields non-smooth depth gradient \wrt surrounding pixels. 
In~\figref{fig:gradient}(c), we plot such an example, at right of the figure, we compute Sobel gradient~\cite{sobel2014history} of the depth map along x direction, where we can clearly see that the gradients surrounding pixels with replaced depth values are non-smooth.
We statistically verify this in \figref{fig:hist}(b) using 500 sparse samples, the blue bars are the histogram of gradient error  at sparse pixels by comparing the gradient of the depth map with sparse depth replacement and of ground truth depth map. We can see the difference is significant, 2/3 of the sparse pixels has large gradient error.
Our method, on the other hand, as shown with the green bars in \figref{fig:hist}(b), the average gradient error is much smaller, and most pixels have zero error. In\figref{fig:gradient}(d), we show the depth gradients surrounding sparse pixels are smooth and close to ground truth, demonstrating the effectiveness of our propagation scheme. 

% Finally, in our experiments~\ref{sec:exp}, we validate the number of iterations $N$ and kernel size $k$ used for doing the CSPN.


\subsection{Complexity Analysis}
\label{subsec:time}

As formulated in~\equref{eqn:cspn}, our CSPN takes the operation of convolution, where the complexity of using CUDA with GPU for one step CSPN is $O(\log_2(k^2))$, where $k$ is the kernel size. This is because CUDA uses parallel sum reduction, which has logarithmic complexity. In addition,  convolution operation can be performed parallel for all pixels and channels, which has a constant complexity of $O(1)$. Therefore, performing $N$-step propagation, the overall complexity for CSPN is $O(\log_2(k^2)N)$, which is irrelevant to image size $(m, n)$.

SPN~\cite{liu2017learning} adopts scanning row/column-wise propagation in four directions. Using $k$-way connection and running in parallel, the complexity for one step is $O(\log_2(k))$. The propagation needs to scan full image from one side to another, thus the complexity for SPN is $O(\log_2(k)(m + n))$. Though this is already more efficient than the densely connected CRF proposed by~\cite{philipp2012dense}, whose implementation complexity with permutohedral lattice is $O(mnN)$, ours $O(\log_2(k^2)N)$ is more efficient since the number of iterations $N$ is always much smaller than the size of image $m, n$. We show in our experiments (\secref{sec:exp}), with $k=3$ and $N=12$, CSPN already outperforms SPN with a large margin (relative $30\%$), demonstrating both efficiency and effectiveness of the proposed approach.


\subsection{End-to-End Architecture}
\label{subsec:unet}
\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth,height=0.45\textwidth]{fig/framework2.pdf}
\caption{Architecture of our networks with mirror connections for  depth estimation via transformation kernel prediction with CSPN (best view in color). Sparse depth is an optional input, which can be embedded into the CSPN to guide the depth refinement.}
\label{fig:arch}
\end{figure}

We now explain our end-to-end network architecture to predict both the transformation kernel and the depth value, which are the inputs to CSPN for depth refinement.
 As shown in \figref{fig:arch}, our network has some similarity with that from Ma \etal~\cite{Ma2017SparseToDense}, with the final CSPN layer that outputs a dense depth map.  
 
For predicting the transformation kernel $\kappa$ in \equref{eqn:cspn}, 
rather than building a new deep network for learning affinity same as Liu \etal~\cite{liu2017learning}, we branch an additional output from the given network, which shares the same feature extractor with the depth network. This helps us to save memory and time cost for joint learning of both depth estimation and transformation kernels prediction. 

Learning of affinity is dependent on fine grained spatial details of the input image. However, spatial information is weaken or lost with the down sampling operation during the forward process of the ResNet in~\cite{laina2016deeper}. Thus, we add mirror connections similar with the U-shape network~\cite{ronneberger2015u} by directed concatenating the feature from encoder to up-projection layers as illustrated by ``UpProj$\_$Cat'' layer in~\figref{fig:arch}. Notice that it is important to carefully select the end-point of mirror connections. Through experimenting three possible positions to append the connection, \ie after \textit{conv}, after \textit{bn} and after \textit{relu} as shown by the ``UpProj'' layer in~\figref{fig:arch} , we found the last position provides the best results by validating with the NYU v2 dataset (\secref{subsec:ablation}). 
In doing so, we found not only the depth output from the network is better recovered, and the results after the CSPN is additionally refined, which we will show the experiment section~(\secref{sec:exp}).
Finally we adopt the same training loss as~\cite{Ma2017SparseToDense}, yielding an end-to-end learning system.

