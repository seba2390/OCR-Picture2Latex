\section{Introduction}
\label{sec:intro}
% pro
Depth estimation from a single image, \ie, predicting per-pixel distance to the camera,  has many applications from augmented realities (AR), autonomous driving,  to robotics.  
%a must-have ability for visual perception of robots, which plays the key-role in navigation, collision %avoidance~\cite{biswas2011depth,haque2017obstacle} and semantic recognition~\cite{qi20173d,Wang_2015_CVPR}.
Given a single image, recent efforts to estimate per-pixel depths have yielded high-quality outputs by taking advantage of deep fully convolutional neural networks~\cite{eigen2015predicting,laina2016deeper} and large amount of training data from indoor~\cite{silberman2012indoor,xiao2013sun3d,Matterport3D} and outdoor~\cite{geiger2012we,wang2016torontocity,huang2018apolloscape}. 
The improvement lies mostly in more accurate estimation of global scene layout and scales with advanced networks, such as VGG~\cite{simonyan2014very} and ResNet~\cite{HeZRS15}, and better local structure recovery through deconvolution operation~\cite{long2015fully}, skip-connections~\cite{ronneberger2015u} or up-projection~\cite{laina2016deeper}. 
% better details while worse absolute scale, better scale no a
Nevertheless, upon closer inspection of the output from a contemporary approach~\cite{Ma2017SparseToDense} (\figref{fig:example}(b)), the predicted depths is still blurry and do not align well with the given image structure such as object silhouette.

\begin{figure}[!htpb]
\centering
\includegraphics[width=1.02\textwidth]{fig/fig1.pdf}
\caption{(a) Input image; (b) Depth from~\cite{Ma2017SparseToDense}; (c) Depth after bilateral filtering; (d) Refined depth by SPN \cite{liu2017learning}; (e) Refined depth by CSPN; (f) Sparse depth samples (500); (g) Ground Truth; (h) Depth from our network; (i) Refined depth by SPN with depth sample; (j) Refined depth by CSPN with depth sample. The corresponding root mean square error (RMSE) is put at the left-top of each predicted depth map.}
\label{fig:example}
\end{figure}

%We argue that this is mostly due to the supervised learning pipeline that primarily optimizes the per-pixel errors with high-level features, while paying less attention to the neighboring relationship that exhibits local similarities. Commonly used and effective strategies to address such an issue include affinity propagation though non-local densely connected conditional random fields (CRF)~\cite{philipp2012dense}, and bilateral filtering~\cite{barron2016fast}.  Researchers often adopt manually designed affinity between neighboring pixels,  which could depend on Euclidean distances between RGB values~\cite{barron2016fast,chen2016deeplab} or transformed features from image edges~\cite{peng2016depth,chen2016semantic}. %\figref{fig:example}(c) shows the results after applying bilateral filtering as a post-processing step, which recovers detailed edge inside the images, while is over smoothed yielding larger depth error.

Most recently, Liu \etal~\cite{liu2017learning} propose to directly learn the image-dependent affinity through a deep CNN with spatial propagation networks (SPN), yielding better results comparing to the manually designed affinity on image segmentation. 
%In \figref{fig:example}(d), we show the results by applying SPN for depth refinement, where not only the depths are better snapped to object and region boundaries but also have smaller error. 
%\color[red]{I really don't think (d) has better edges, compared to (b), the depth edge is blurry. }
However, its propagation is performed in a scan-line or scan-column fashion, which is serial in nature. For instance, when propagating left-to-right, pixels at right-most column must wait the information from the left-most column to update its value. 
Intuitively, depth refinement commonly just needs a local context rather a global one. 

Here we propose convolutional spatial propagation networks (CSPN), where the depths at all pixels are updated simultaneously within a local convolutional context. The long range context is obtained through a recurrent operation. \figref{fig:example} shows an example, the depth estimated from CSPN (e) is more accurate than that from SPN (d) and Bilateral filtering (c). In our experiments, our parallel update scheme leads to significant performance improvement in both speed and quality over the serial ones such as SPN.

Practically, we show that the proposed strategy can also be easily extended to convert sparse depth samples to a dense depth map given corresponding image~\cite{LiaoHWKYL16,Ma2017SparseToDense}.  This task can be widely applied in robotics and autonomous cars, where depth perception is often acquired through LiDAR, which usually generates sparse but accurate depth measurement. By combining the sparse measurements with images, we could generate a full-frame dense depth map.
%This task currently is interested in real applications like autonomous driving, where depth perception is from LiDAR, \eg Velodyne~\footnote{http://velodynelidar.com/}, which provides only sparse measurements for distant objects and scenes. 
For this task, we consider three important requirements for an algorithm: 
(1) The dense depth map recovered should align with image structures;
(2) The depth value from the sparse samples should be preserved, since they are usually from a reliable sensor;
(3) The transition between sparse depth samples and their neighboring depths should be smooth and unnoticeable.  
In order to satisfy those requirements, we first add mirror connections based on the network from~\cite{Ma2017SparseToDense}, which generates better depths as shown in \figref{fig:example}(h). Then, we tried to embed the propagation into SPN in order to keep the depth value at sparse points. As shown in \figref{fig:example}(i), it generates better details and lower error than SPN without depth samples (\figref{fig:example}(d)). Finally, changing SPN to our CSPN yields the best result (\figref{fig:example}(j)). 
As can be seen, our recovered depth map with just 500 depth samples produces much more accurately estimated scene layouts and scales. 
We experiment our approach over two popular benchmarks for depth estimation, \ie NYU v2~\cite{silberman2012indoor} and KITTI~\cite{geiger2012we}, with standard evaluation criteria. In both datasets, our approach is significantly better (relative $30\%$ improvement in most key measurements) than previous deep learning based state-of-the-art (SOTA) algorithms~\cite{LiaoHWKYL16,Ma2017SparseToDense}. More importantly, it is very efficient yielding 2-5$\times$ acceleration comparing with SPN. In summary, this paper has the following contributions: 
\begin{enumerate}
% CSPN
    \item We propose convolutional spatial propagation networks (CSPN) which is more efficient and accurate for depth estimation than the previous SOTA propagation strategy~\cite{liu2017learning}, without sacrificing the theoretical guarantee.
    
    % application
    \item  We extend CSPN to the task of converting sparse depth samples to dense depth map by using the provided sparse depths into the propagation process. It guarantees that the sparse input depth values are preserved in the final depth map.  
    It runs in real-time, which is well suited for robotics and autonomous driving applications, where sparse depth measurement from LiDAR can be fused with image data.
    
    % result is not a contribution. 
    % results
    %\item We conducted extensive experiments for both indoor and outdoor datasets, and validate multiple variations of CSPN, \eg different amount of neighbors and iterations, which demonstrates the superiority of our method in depth enhancement tasks over the other SOTA algorithms~\cite{Ma2017SparseToDense}.
\end{enumerate}
%We will release all our code and models upon the publication of the paper.
