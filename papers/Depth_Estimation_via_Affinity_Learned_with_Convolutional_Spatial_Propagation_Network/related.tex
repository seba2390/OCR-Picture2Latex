\section{Related Work}
\label{sec:related}
Depth estimating and enhancement/refinement have long been center problems for computer vision and robotics. Here we summarize those works in several aspects without enumerating them all due to space limitation.

%\addlinespace
\noindent\textbf{Single view depth estimation via CNN and CRF.}
Deep neural networks (DCN) developed in recent years
provide strong feature representation for 
per-pixel depth estimation from a single image. Numerous algorithms are developed through supervised methods~\cite{wang2015designing,eigen2015predicting,laina2016deeper,li2017two}, semi-supervised methods~\cite{kuznietsov2017semi} or unsupervised methods~\cite{godard2016unsupervised,zhou2017unsupervised,yang2018aaai,yang2018lego}.
%The learned CNN model shows significant improvement in terms of recovering global scene layouts and recovered detailed structures via taking stronger networks, %\eg ResNet~\cite{HeZRS15} 
and add in skip and mirror connections.
%\eg hourglass network~\cite{newell2016stacked} and UNet~\cite{ronneberger2015u}.
Others tried to improve the estimated details further by appending a conditional random field (CRF)~\cite{DBLP:conf/cvpr/WangSLCPY15,Liu_2015_CVPR,li2015depth} and joint training~\cite{crfasrnn_iccv2015,peng2016depth}. 
%More recently, CNN and CRF are shown to be able to joint learned through backpropagation~\cite{crfasrnn_iccv2015} for image segmentation. 
%For depth estimation, Wang \etal~\cite{peng2016depth} propose to use a densely connected CRF~\cite{philipp2012dense} which can be jointly trained with the network. 
However, the affinity for measuring the coherence of neighboring pixels is manually designed.
%, and not efficient enough for applications requiring real-time performance.

%\addlinespace
\noindent\textbf{Depth Enhancement.}
Traditionally, depth output can be also efficiently enhanced with explicitly designed affinity through image filtering~\cite{barron2016fast,matsuo2015depth}, or data-driven ones through total variation (TV)~\cite{ferstl2013image,ferstl2015variational} and learning to diffuse~\cite{liu2016learning} by incorporating more priors into diffusion partial differential equations (PDEs).
However, due to the lack of an effective learning strategy, they are limited for large-scale complex visual enhancement.

Recently, deep learning based enhancement yields impressive results on super resolution of both images~\cite{dong2014learning,yang2014color} and depths~\cite{song2016deep,hui2016depth,kwon2015data,riegler2016atgv}. The network takes low resolution inputs and output the high-resolution results, and is trained end-to-end where the mapping between input and output is implicitly learned.
However, these methods are only trained and experimented with perfect correspondent ground-truth low-resolution and high-resolution depth maps and often a black-box model. In our scenario, both the input and ground truth depth are non-perfect, \eg depths from a low cost LiDAR or a network, thus an explicit diffusion process to guide the enhancement such as SPN is necessary.

%\addlinespace
\noindent\textbf{Learning affinity for spatial diffusion.}
% Ours is convolutional
Learning affinity matrix with deep CNN for diffusion or spatial propagation receives high interests in recent years due to its theoretical supports and guarantees~\cite{weickert1998anisotropic}.
Maire \etal~\cite{maire2016affinity} trained a deep CNN to directly predict the entities of an affinity matrix, which demonstrated good performance on image segmentation. However, the affinity is followed by an independent non-differentiable solver of spectral embedding, it can not be supervised end-to-end for the prediction task. Bertasius \etal~\cite{bertasius2016convolutional} introduced a random walk network that optimizes the objectives of pixel-wise affinity for semantic segmentation. Nevertheless, their affinity matrix needs additional supervision from ground-truth sparse pixel pairs, which limits the potential connections between pixels. Chen \etal~\cite{chen2016semantic} try to explicit model an edge map for domain transform to improve the output of neural network. 

The most related work with our approach is SPN~\cite{liu2017learning}, where the learning of a large affinity matrix for diffusion is converted to learning a local linear spatial propagation, yielding a simple while effective approach for output enhancement. However, as mentioned in~\secref{sec:intro}, depth enhancement commonly needs local context, it might not be necessary to update a pixel by scanning the whole image. As shown in our experiments, our proposed CSPN is more efficient and provides much better results.

%\addlinespace
\noindent\textbf{Depth estimation with given sparse samples.}
The task of sparse depth to dense depth estimation was introduced in robotics due to its wide application for enhancing 3D perception~\cite{LiaoHWKYL16}. Different from depth enhancement, the provided depths are usually from low-cost LiDAR or one line laser sensors, yielding a map with valid depth in only few hundreds of pixels, as illustrated in \figref{fig:example}(f). 
Most recently, Ma \etal~\cite{Ma2017SparseToDense} propose to treat sparse depth map as additional input to a ResNet~\cite{laina2016deeper} based depth predictor, producing superior results than the depth output from CNN with solely image input. However, the output results are still blurry, and does not satisfy our requirements of depth as discussed in \secref{sec:intro}. In our case, we directly embed the sampled depth in the diffusion process, where all the requirements are held and guaranteed.

Some other works directly convert sparse 3D points to dense ones without image input~\cite{Zimmermann2017Learning,Ladicky_2017_ICCV,uhrig2017sparsity}, whereas the density of sparse points must be high enough to reveal the scene structure, which is not available in our scenario.