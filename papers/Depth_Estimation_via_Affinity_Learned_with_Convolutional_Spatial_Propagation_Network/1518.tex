% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016
\documentclass[runningheads]{llncs}
\pdfoutput=1
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{epsfig}
%\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{hyperref}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{gensymb}

%\cvprfinalcopy % *** Uncomment this line for the final submission
\newcommand{\figref}[1]{Fig\onedot~\ref{#1}}
\newcommand{\equref}[1]{Eq\onedot~\eqref{#1}}
\newcommand{\secref}[1]{Sec\onedot~\ref{#1}}
\newcommand{\tabref}[1]{Tab\onedot~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\algref}[1]{Alg\onedot~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property\onedot~\ref{#1}}

\newcommand{\ve}[1]{{\mathbf #1}} % for displaying a vector or matrix
\newcommand{\hua}[1]{{\mathcal #1}}
\newcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\spa}[1]{{\mathbb #1}}
\newcommand{\by}[2]{\ensuremath{#1 \! \times \! #2}}
\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

%\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g.}} 
\def\Eg{\emph{E.g}\onedot}
\def\any{\forall}
\def\ie{\emph{i.e.}} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} 
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al.}}


\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter

\title{Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network} 
% Replace with your title

\titlerunning{CSPN}
% Replace with a meaningful short version of your title

\authorrunning{X. Cheng, P. Wang and R. Yang}
% Replace with shorter version of the author list. If there are more authors than fits a line, please use A. Author et al.

\author{Xinjing Cheng\thanks{equal contribution}, Peng Wang\printfnsymbol{1} and Ruigang Yang}

%Please write out author names in full in the paper, i.e. full given and family names. 
%If any authors have names that can be parsed into FirstName LastName in multiple ways, please include the correct parsing, in a comment to the volume editors:
%\index{Lastnames, Firstnames}
%(Do not uncomment it, because you may introduce extra index items if you do that, we will use scripts for introducing index entries...)


\institute{Baidu Research, Baidu Inc.\\
	\email{ \{chengxinjing,wangpeng54,yangruigang\}@baidu.com}
}

\maketitle

\begin{abstract}
Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. 
Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). 
We apply the designed CSPN to two depth estimation tasks given a single image:  (1) Refine the depth output from existing state-of-the-art (SOTA)  methods;  (2) Convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LiDAR that provides sparse but accurate depth measurements. We experimented the proposed CSPN over the popular NYU v2~\cite{silberman2012indoor} and KITTI~\cite{geiger2012we} datasets, where we show that our proposed approach improves  not only quality (e.g., 30\% more reduction in depth error), but also speed (e.g., 2 to 5$\times$ faster) of depth maps than previous SOTA methods. %The codes of CSPN are available at: \url{https://github.com/XinJCheng/CSPN}.
\keywords{Depth estimation, Convolutional spatial propagation}
\end{abstract}

\input{intro}
\input{related}
\input{approach}
\input{experiment}
\input{conclusions}

\clearpage

\bibliographystyle{splncs}
\bibliography{egbib}
\end{document}
