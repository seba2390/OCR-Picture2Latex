\section{Systematic Derivation of Matrix Properties}
\label{sec:propertyDerivation}

In order to derive algorithms for CG from its description in matrix form, it is necessary to use properties of matrices and expressions that are not explicitly part of the initial description. Those properties have to be derived from the description by means of algebraic manipulation and deductive reasoning. To automate the process of finding algorithms, an automatic method for the derivation of properties is necessary. Thus, this systematic approach should replicate the steps performed by a human expert, without actually requiring any human guidance. In this section, we describe such a method.


%Thus, to obtain a system that is capable of deriving algorithms without human intervention, it must be able to derive properties in a systematic way. In this chapter, we describe such a method.

%breadth first search-based method. While such a methods has the disadvantage that it does not necessarily terminate, it allows us to 

\subsection{Preliminaries}

To begin, it is useful to formalize the notion of properties and equations. We start with the most basic building blocks, terms and expressions.

\begin{definition}[Terms and Expressions]
\emph{Terms} are inductively defined as follows:
\begin{enumerate}
\item Every matrix, vector and scalar is a term.
\item If $t$ is a term, then $t^T$, $(-t)$ and $t^{-1}$ are terms.
\item If $t_1$ and $t_2$ are terms, then $(t_1 + t_2)$ and $(t_1 \cdot t_2)$ are terms.
\end{enumerate}
Every term is also an \emph{expression}. Furthermore, if $t_1$ and $t_2$ are terms, then $t_1 = t_2$ is an \emph{expression}. \qed
\end{definition}

%\todo{Make this a positive statement: correctness is implicitly assumed}
Note that this is a simplified definition, as it does not include functions,
%Note that this definition is rather limited, as it does not extend to functions,
nor does it make any statement about the validity of terms. A product of two matrices with dimensions that do not match is still a valid term. Nonetheless, this definition is sufficient for our purposes. Furthermore, we usually use a simplified notation, omitting parentheses, if they are unnecessary, as well as the multiplication dot. We can now define equations and properties:

\begin{definition}[Equations]
Let $t_1$, $t_2$ be terms. An \emph{equation} is an expression of the form $t_1 = t_2$. \qed
\end{definition}

\begin{definition}[Properties]
Let $t$ be a term and $\text{\ttfamily P}$ be a boolean predicate. Then $\text{\ttfamily P}[t]$ is a \emph{property}. \qed
\end{definition}

Note that properties are always predicates, even if they can also be expressed as equations. Take the property $\text{\ttfamily Orthogonal}[R]$ as an example. It implies $\text{\ttfamily Diagonal}[R^T R]$. Applying the partitioning that is also applied to the postcondition,
%
$$R \rightarrow \myFlaOneByThree{R_L}{r_M}{R_R}\text{,}$$
%
to $R^T R$, we find out, among other things, that $R_L^T R_L$ is diagonal and $r_M^T R_L$ equals zero. 
%Applying the partitioning that is also applied to the postcondition to $R^T R$, that is
%%
%$$R \rightarrow \myFlaOneByThree{R_L}{r_M}{R_R}\text{,}$$
%%
%we find out, among other things, that $R_L^T R_L$ is diagonal and $r_M^T R_L$ equals zero. 
Instead of considering the equation $r_M^T R_L = 0$ as a property, we use $\text{\ttfamily Zero}[r_M^T R_L]$. This allows us to use a more consistent notation and simplifies the systematic derivation.

%In the following, we make a distinction between \emph{properties} and \emph{equations}, even though properties itself can be equations. For objects that are not partitioned, one can think of properties exclusively as logical predicates, for example $\text{\ttfamily Diagonal}[D]$ or $\text{\ttfamily Orthogonal}[R]$. Everything that is obtained from a property by repartitioning is also a property, for example $\text{\ttfamily Diagonal}[D_{TL}]$ or $r_M^T R_L = 0$. In the initial description of an operation, properties are given in the precondition.
%
%Equations are what is given in the postcondition, and equivalently, everything that is derived from it by repartitioning or algebraic manipulations.

\subsection{Representing Knowledge about Linear Algebra}

A human expert who derives properties of matrices inevitably applies some basic knowledge about linear algebra. To allow a system to replicate the expert reasoning, it needs a knowledgebase that encodes this knowledge. We define five different types of implications that will be included in the knowledgebase.
%
\begin{enumerate}
\item $\text{\ttfamily P}_1[t] \land \ldots \land \text{\ttfamily P}_i[t] \rightarrow \text{\ttfamily P}[t]$

This type of implication allows to reason about the combination of properties of one single term. One example is:
%
\begin{align}
\text{\ttfamily LowerTriangular}[t] \land \text{\ttfamily Symmetric}[t] &\rightarrow \text{\ttfamily Diagonal}[t] 
\end{align}
%%%%%%%%%%%
\item $\text{\ttfamily P}_1[t_1] \land \ldots \land \text{\ttfamily P}_i[t_i] \land \exists t \rightarrow \text{\ttfamily P}[t] $

Here, $t_1,\ldots, t_i$ are subterms of $t$. Thus, it allows the system to infer the properties of a product or sum of multiple quantities with different properties. The $\exists t$ is used to avoid deriving properties for terms that do not occur anywhere. Consider two examples:
%
\begin{align}
\text{\ttfamily Diagonal}[t_1] \land \text{\ttfamily LowerTriangular}[t_2] \land \exists t_1 t_2 &\rightarrow \text{\ttfamily LowerTriangular}[t_1 t_2] \\
\text{\ttfamily StrictlyUpperTriangular}[t] \land \exists I + t &\rightarrow \text{\ttfamily UpperTriangular}[I + t]
\end{align}
%%%%%%%%%%%
\item $\text{\ttfamily P}[t_1] \land t_1 = t_2 \rightarrow \text{\ttfamily P}[t_2]$

This implication enables the system to propagate properties across equalities. $\text{\ttfamily P}$ now is a pattern that matches any property.
%%%%%%%%%%%
\item $\text{\ttfamily P}_1[t] \rightarrow \text{\ttfamily P}_2[f(t)]$

$f$ is a function, for example transposition. Thus, this kind of implication allows to derive properties of transposed or inverted quantities. In addition to that, it is used reason about orthogonal or orthonormal matrices. Let us look at three examples:
%
\begin{align}
\text{\ttfamily LowerTriangular} \left[t\right] &\rightarrow \text{\ttfamily UpperTriangular} \left[ t^T \right] \label{eqn:loTriTransposed}\\
\text{\ttfamily LowerTriangular} \left[t\right] &\rightarrow \text{\ttfamily LowerTriangular} \left[ t^{-1} \right] \\
\text{\ttfamily Orthogonal}[t] &\rightarrow \text{\ttfamily Diagonal}\left[t^T t \right]
\end{align}
%%%%%%%%%%%
\item $t \rightarrow f(t)$

For one of the steps of the method presented in this chapter, it is necessary that properties have a canonical form: The unary operators $^{-1}$ and $^{T}$ will not be applied to products. However, some of the other types of implications may produce such terms. To transform those terms into the canonical form, implications are necessary that distribute those unary operators across products. Those implications are:
%This implication is necessary to deal with the case that any of the terms in the implications above matched more than one single operand. Two examples are:
%
\begin{align}
(t_1 t_2)^T &\rightarrow t_2^T t_1^T \label{eqn:productTransposed} \\
(t_1 t_2)^{-1} &\rightarrow t_2^{-1} t_1^{-1} 
\end{align}
%
\end{enumerate}
%%
%
%%
%\begin{align*}
%\text{\ttfamily P}_1[t_1] \land \ldots \land \text{\ttfamily P}_i[t_i] &\rightarrow \text{\ttfamily P}[f(t_1, \ldots, t_i)] \\
%\text{\ttfamily P}_1[t_1] \land \ldots \land \text{\ttfamily P}_i[t_i] \land t &\rightarrow \text{\ttfamily P}[f(t_1, \ldots, t_i, t)] \\
%t &\rightarrow f(t) \\
%\text{\ttfamily P}[t_1] \land t_1 = t_2 &\rightarrow \text{\ttfamily P}[t_2]
%\end{align*}
%%
%
%Consider a few examples:
%%
%\begin{align}
%\text{\ttfamily LowerTriangular}[A] \land \text{\ttfamily Symmetric}[A] &\rightarrow \text{\ttfamily Diagonal}[A] \\
%\text{\ttfamily Diagonal}[A] \land \text{\ttfamily LowerTriangular}[B] \land AB &\rightarrow \text{\ttfamily LowerTriangular}[AB] \\
%\text{\ttfamily Orthogonal}[A] &\rightarrow \text{\ttfamily Diagonal}\left[A^T A \right] \\
%\text{\ttfamily LowerTriangular} \left[A\right] &\rightarrow \text{\ttfamily UpperTriangular} \left[ A^T \right] \label{eqn:loTriTransposed} \\
%(AB)^T &\rightarrow B^T A^T \label{eqn:productTransposed}
%\end{align}
%%
Note that all the terms in the implications above may match not only a single op\-er\-and, but every term that has the required property. So if the product $AB$ is lower triangular, implication (\ref{eqn:loTriTransposed}) tells us that $(AB)^T$ is upper triangular, and using (\ref{eqn:productTransposed}), we find out that $B^T A^T$ is upper triangular.

%Note that $A$ or $B$ should not only match a single quantity, but also multiple, composed quantities. So if $CD$ is lower triangular, the second to last implication tells us that $(CD)^T$ is upper triangular, and with the last implication, we find out that $D^T C^T$ is upper triangular.

\subsection{Derivation of Properties}
\label{sec:propertyDerivation:Derivation}

In the following, we will assume that all equations have the form
%
$$\prod_{1 \leq i \leq n} t_i =\prod_{1 \leq j \leq m} t^\prime_j \text{.}$$
%
%This is possible because, following from the definition of terms, any term can be written as a product of terms.
In case of a sum $X + Y$, the product consists of just one term $t = X + Y$. This assumption is no restriction as the only sums that appear in the matrix representations of iterative methods are of the form $I - Y$ and are used exclusively to emphasize the structure of those matrices.

\begin{description}
\item[Initialization] We start with two sets, $\mathcal{P}$ and $\mathcal{E}$. $\mathcal{P}$ is a subset of the precondition $P_\text{pre}$ of the description of an operation. It only contains those properties that describe operand types, like $\text{\ttfamily Square}[X]$, $\text{\ttfamily Matrix}[X]$ or $\text{\ttfamily Diagonal}(X)$. Not included are properties that specify what is input and output, as they are superfluous for the derivation. $\mathcal{E}$ is a set of expressions which contains the equations of the corresponding postcondition $P_\text{post}$.

\item[Derivation of Properties] The implications of the knowledgebase $\mathcal{K}$ are used to derive all possible properties at this stage. This is done as follows:

\begin{itemize}
%\item[-] Given $\text{\ttfamily P}_1[t_1] \land \ldots \land \text{\ttfamily P}_i[t_i] \land u_1 \land \ldots \land u_j \rightarrow \text{\ttfamily P}[f(t_1, \ldots, t_i, u_1, \ldots, u_j)] \in K$, the property $\text{\ttfamily P}[\ldots]$ is added to P if and only if there is an equation $e \in E$ that contains all
\item[-] Given $\text{\ttfamily P}_1[t] \land \ldots \land \text{\ttfamily P}_i[t] \rightarrow \text{\ttfamily P}[t] \in \mathcal{K}$, the property $\text{\ttfamily P}[t]$ is added to $\mathcal{P}$ if $\text{\ttfamily P}_k[t] \in \mathcal{P}$ for all $k \in \{1,\ldots, i\}$.

\item[-] Given $\text{\ttfamily P}_1[t] \land \ldots \land \text{\ttfamily P}_i[t] \land \exists t \rightarrow \text{\ttfamily P}[t] \in \mathcal{K}$, the property $\text{\ttfamily P}[t]$ is added to $\mathcal{P}$ if 
\begin{enumerate}
\item there is an equation $e \in \mathcal{E}$ that contains the term $t$, and
\item $\text{\ttfamily P}_k[t] \in \mathcal{P}$ for all $k \in \{1,\ldots, i\}$.
\end{enumerate}

\item[-] Given $\text{\ttfamily P}[t_1] \land t_1 = t_2 \rightarrow \text{\ttfamily P}[t_2] \in \mathcal{K}$, the property $\text{\ttfamily P}[t_2]$ is added to $\mathcal{P}$ if $\text{\ttfamily P}[t_1] \in \mathcal{P}$ and $(t_1 = t_2) \in \mathcal{E}$.

\item[-] Given $\text{\ttfamily P}_1[t] \rightarrow \text{\ttfamily P}_2[f(t)] \in K$, the property $\text{\ttfamily P}_2[f(t)]$ is added to $\mathcal{P}$ if $\text{\ttfamily P}_1[t] \in \mathcal{P}$.

\item[-] Given $t \rightarrow f(t) \in \mathcal{K}$ and $\text{\ttfamily P}[t] \in \mathcal{P}$, the property $\text{\ttfamily P}[f(t)]$ is added to $\mathcal{P}$.

\end{itemize}
%If there is any property in the set of properties that matches the left-hand side of any implication, the corresponding right hand side is added to $P$. In case of implication like the second one, that contains the expression $AB$, a matching (sub)expression has to exist in $E$.

\item[Matrix Inversion] Then, new expressions are added to $\mathcal{E}$.
\begin{enumerate}
\item For every equation $e \in \mathcal{E}$ with $e = ( t_1 \cdots t_i = u_1 \cdots u_j )$, it is checked whether $t_1$, $t_i$, $u_1$ or $u_j$ is invertible. 
\item If $t_1$ or $u_1$ is invertible, $t_1^{-1}$ or $u_1^{-1}$, respectively, is multiplied from the left to $e$, eliminating the invertible term on the one side and adding its inverse on the other. We proceed analogously with $t_i$ and $u_j$, then multiplying the inverse of the term from the right.
\item The resulting, new equation $e'$ is added to $\mathcal{E}$.
\end{enumerate}

Those three steps are repeated until no new expressions are found.

%Every equation is scanned for invertible quantities that appear at the left- or rightmost position in a product. The inverse of this quantity is then multiplied from the left or right side, respectively, and the resulting equation is added to the set. As an example, let us assume we have the equation $AB = C$ and $A$ is invertible. Multiplying $A^{-1}$ form the left provides us with the new equation $B = A^{-1} C$. This step is repeated until no new expression are found.

\item[Application of Properties] In the final step, we apply known properties to expressions to derive new properties. Intuitively, we multiply quantities to both sides of an equation in order to recreate subexpressions that are known to present some property, which are then used to infer new properties.
\begin{enumerate}
\item For every property $\text{\ttfamily P}[t] \in \mathcal{P}$ with $t = t_1 \cdots t_i$, $i > 1$ a set of tuples $$S(t) = \{ (t_1 \cdots t_k, t_{k + 1} \cdots t_i) \mid 1 \leq k < i \} $$ is generated. Intuitively, this set contains the term $t$, split into two parts in all possible ways.
\item Let $(t_L, t_R) \in S(t)$. If there is an equation $e \in \mathcal{E}$ where $t_L$ is the rightmost (sub)term in any term, then $t_R$ is multiplied from the right. $t_L$ is multiplied from the left in the analogous case. Let $e'$ be the resulting equation. As an example, let $e$ be $ABC = D$ and $(BC, F) \in S(t)$. Since $BC$ appears as the rightmost subterm in $e$, $t_R = F$ is multiplied from to right, resulting in $ABCF = DF$.
\item The knowledgebase is used to derive new properties using $e'$ that are added to $\mathcal{P}$ (see step ``Derivation of Properties''). $e'$ is \emph{not} added to $\mathcal{E}$.
\end{enumerate}


%We explain the formal approach using the property $\text{\ttfamily LowerTriangular}[X^T A X]$ as an example.
%
%\begin{enumerate}
%\item A set $S\left( X^T A X \right)$ is constructed. From $X^T A X$, pairs of subexpressions are formed. Each pair satisfies the following conditions:
%%
%\begin{enumerate}
%\item It has the form $(\text{prefix}, \text{suffix})$.
%\item Neither the prefix nor the suffix is empty.
%\item The concatenation of prefix and suffix forms $X^T A X$.
%\end{enumerate}
%%
%The product $X^T A X$ is decomposed into prefix and a suffix, such that neither the prefix nor the suffix is empty, and the concatenation of prefix and suffix forms $X^T A X$.
%
%\end{enumerate}

%First, two sets of subexpressions of $X^T A X$ are generated. One set contains the subexpressions that contain the leftmost quantity of $X^T A X$, the other set the ones that contain the rightmost quantity (alternatively, one could say we take all prefixes and suffixes). This results in the sets $\{ X^T, X^T A\}$ and $\{A X, X\}$. If any element of the first set appears as  at the right-hand side of an expression, we multiply the remaining part of $X^T A X$ from the left, thus recreating $X^T A X$. The resulting expression is then used to derive new properties that are added to the set. For the second set, we proceed analogously. As an example, let us assume we have the expression $D X^T = B^T$, where $D$ is diagonal. We recognize $X^T$ and multiply the remaining part of $X^T A X$, $A X$, from the right, and obtain the equation $D X^T A X = B^T A X$. Hence, the new property $\text{\ttfamily LowerTriangular}[B^T A X]$ is added. This step is repeated for all properties.
\end{description}


\subsection{Design Considerations}

One observes that the presented method is not goal-oriented. The derived properties are mainly used to solve equations, so it might seem more natural to derive properties starting with an equation that has to be solved. Based on this equation, expressions would be selected, and in a second step, the properties of those expression would be derived. Those properties would then be used to solve the equation. Instead, the presented method derives a large number or properties, irrespective of the question whether they might be useful or not.
%This way, the derivation would be performed on partitioned matrices.

The problem with a goal-oriented approach is that quite often, it is not obvious from the equation which property could be used to solve it. This leaves us with the much more challenging task of identifying which expression might have relevant properties. Take the following equation as an example, which appears when deriving algorithms for nonsymmetric CG. $P_L$, $p_M$, $r_M$ and $A$ are known.
%
$$-P_L u_{TM} + p_M = r_M$$
%
It is solved for $u_{TM}$ by using the fact that $P^T A P$ is lower triangular. Thus, $P_L^T A P_L$ is lower triangular as well and $P_L^T A p_M$ is zero. Multiplying $P_L^T A$ from the left to both sides of the equation gives us
%
$$-P_L^T A P_L u_{TM} = P_L^T A r_M \text{.}$$
%
This now is a triangular system that can easily be solved. While it is possible to individually derive that $P_L^T A P_L$ is lower triangular and $P_L^T A p_M$ is zero, the initial equation gives us little to no indication to inspect the properties of those expressions in the first place.

One the other hand, the advantage of a method that is not goal-oriented is that it may find properties that we do not expect to find.

\subsubsection{Orthogonality of the Residual Matrix}

For some iterative methods, for example CG, the residual matrix $R$ is orthogonal. In the postcondition of those methods, $R$ usually appears multiple times, either as a whole, or without the last column ($\underline{R}$). Unfortunately, for the derivation of properties, this poses a problem.

If $R$ is orthogonal, then $R^T R$ and $\underline{R}^T \underline{R}$ are diagonal. $R^T \underline{R}$ and its transpose are rectangular and all entries except for the ones on the main diagonal are zero. Thus, in some sense, they are diagonal as well.

It turns out that for deriving certain properties, $R^T \underline{R}$ is needed. Unfortunately, using the described method, neither $\text{\ttfamily Orthogonal}[R]$ nor $\text{\ttfamily Orthogonal}[\underline{R}]$ implies any property of $R^T \underline{R}$. One way to solve this would be to treat any matrix that appears with and without the last column in a special way, such that properties of $R^T \underline{R}$ and $\underline{R}^T R$ are found as well. This, however, would require significant modifications of the derivation process.

The simpler solution is to consider $R$ and $\underline{R}$ to be two distinct objects, and properties of $R^T \underline{R}$ and its transpose are added to the precondition. 

\subsubsection{Substituting Equations}

Note that we deliberately avoid substituting quantities in one equation by expressions obtained from others. While this might be a very natural approach if deriving properties by hand, doing this systematically is difficult. Plugging in equations quickly leads to arbitrarily large expressions unless some heuristics are applied to terminate this process. Apart from that, it is possible to achieve the same results using the approach presented in this section. Consider a short example to get an intuition why this is the case. Let us assume we have two equations
%
\begin{align*}
t_1 &= t_2 {\color{green!50!black} t_3} \\
{\color{green!50!black}t_4} t_2 &= {\color{green!50!black}t_5}
\end{align*}
%
and we want to derive a property for $t_1$. Properties of $t_3$, $t_4$ and $t_5$ are known (colored green). As the properties of $t_2$ are not known as well, we have to use the second equation to proceed. If $t_4$ is invertible, we can solve to
%
$$t_2 = {\color{green!50!black}t_4^{-1}} {\color{green!50!black}t_5} \text{.}$$
%
Instead of substituting $t_2$ in $t_1 = t_2 t_3$, yielding
%
$$t_1 = {\color{green!50!black}t_4^{-1}} {\color{green!50!black}t_5} {\color{green!50!black} t_3} \text{,}$$
%
and then reason about properties of $t_4^{-1} t_5$, we first derive all properties of $t_4^{-1} t_5$, which are also properties of $t_2$. In the final step, we derive all properties of $t_2 t_3$, obtaining the same properties for $t_1$ we would find by plugging one equation into the other.

\subsubsection{Termination}

The disadvantage of this approach is that it may not terminate either, and increasingly long properties are derived. This is not unexpected, since this approach aims at replicating the process of substituting equations. The difference of the presented method is that the set of equations is finite, its size does not even change anymore after the matrix inversion step.\footnote{This is why we refrain from adding $e'$ to $\mathcal{E}$ in ``Application of Properties'', step 3.} Furthermore, for most iterative methods, no properties of products of  more than three quantities are used. While it might be possible to use significantly longer properties to solve equations, most likely, they result in algorithms that use unnecessarily large expressions to compute certain quantities. This naturally leads to the solution of introducing a maximum length for properties, similar to a recursion limit, with a reasonable default value that can be changed by the user. This way, there is only a finite number of properties that can be derived. 

Unfortunately, if we just refrained from adding properties of products of more than three quantities to $\mathcal{P}$, the derivation process would cease to work in certain cases (this will be explained in the following section). To avoid that, there are multiple options. In both cases, we initially add longer properties to $\mathcal{P}$ as well. Then, one solution is to never use them to derive further properties, that is, we never construct $S(t)$ if $t$ is a product of more than three quantities. Alternatively, those longer properties are removed from $\mathcal{P}$ when $e'$ is discarded. A third option would be to construct an additional set of temporary properties.

\subsection{Example: Nonsymmetric CG}
\label{sec:derivationOfPropertiesExample}

We demonstrate the method presented in this chapter by deriving some properties for nonsymmetric CG. In the interest of brevity, we only derive a small number of selected properties, in addition to limiting properties to a maximum length of three quantities. The knowledgebase $\mathcal{K}$ is not shown here due to its size. Properties used in the following which are not self-explanatory are defined in Appendix \ref{chap:appendixProperties}. The pre- and postcondition of nonsymmetric CG are shown below. For the sake of simplicity, we treat $\underline{I} - \underline{J}$ as one distinct matrix, as $\underline{I}$ and $\underline{J}$ do not appear separately.
%
\begin{align*}
P_\text{pre}: \{ &\text{\ttfamily Input}[A] \land \text{\ttfamily Matrix}[A] \land \text{\ttfamily NonSingular}[A] \land \\
		&\text{\ttfamily Output}[P] \land \text{\ttfamily Matrix}[P] \land \\
		&\text{\ttfamily Output}[D] \land \text{\ttfamily Matrix}[D] \land \text{\ttfamily Diagonal}[D] \land \\
		&\text{\ttfamily FirstColumnInput}[R] \land \text{\ttfamily Matrix}[R] \land \text{\ttfamily Orthogonal}[R] \land \\
		&\text{\ttfamily FirstColumnInput}[\underline{R}] \land \text{\ttfamily Matrix}[\underline{R}] \land \text{\ttfamily Orthogonal}[\underline{R}] \land \\
		&\text{\ttfamily DiagonalR}[R^T \underline{R}] \land \text{\ttfamily DiagonalR}[\underline{R}^T R] \land \\
		&\text{\ttfamily FirstColumnInput}[X] \land \text{\ttfamily Matrix}[X] \land \\
		&\text{\ttfamily Output}[U] \land \text{\ttfamily Matrix}[U] \land \text{\ttfamily StrictlyUpperTriangular}[U] \land \\
		& \text{\ttfamily Matrix}[\underline{I} - \underline{J}] \land \text{\ttfamily LowerTrapezoidal}[\underline{I} - \underline{J}] \}
\end{align*}
%
\begin{align*}
P_\text{post}: \{ &APD = R \left( \underline{I} - \underline{J}  \right) \\
			&P \left( I - U \right) = \underline{R} \\
			&PD = X \left( \underline{I} - \underline{J} \right) \\
			&\| R e_r^T \| < \varepsilon\}
\end{align*}
%
%
\subsubsection{Initialization}

The first step consists of initializing $\mathcal{P}$ and $\mathcal{E}$. The former contains all the properties of the precondition $P_\text{pre}$ that describe operand types.
%
\begin{align*}
\mathcal{P} = \{ & \text{\ttfamily Matrix}[A], \text{\ttfamily NonSingular}[A], \\
		& \text{\ttfamily Matrix}[P], \\
		& \text{\ttfamily Matrix}[D], \text{\ttfamily Diagonal}[D], \\
		& \text{\ttfamily Matrix}[R],  \text{\ttfamily Orthogonal}[R], \\
		& \text{\ttfamily Matrix}[\underline{R}], \text{\ttfamily Orthogonal}[\underline{R}], \\
		&\text{\ttfamily DiagonalR}[R^T \underline{R}], \text{\ttfamily DiagonalR}[\underline{R}^T R] \land \\
		& \text{\ttfamily Matrix}[X], \\
		& \text{\ttfamily Matrix}[U], \text{\ttfamily StrictlyUpperTriangular}[U], \\
		& \text{\ttfamily Matrix}[\underline{I} - \underline{J}], \text{\ttfamily LowerTrapezoidal}[\underline{I} - \underline{J}] \} \\
\end{align*}
%
$\mathcal{E}$ contains the equations of the postcondition $P_\text{post}$.
%
\begin{align*}
\mathcal{E} = \{ &APD = R \left( \underline{I} - \underline{J}  \right), \\
	&P \left( I - U \right) = \underline{R}, \\
	&PD = X \left( \underline{I} - \underline{J} \right) \}
\end{align*}
%
\subsubsection{Derivation of Properties}
%
During this step, only a small number of new properties can be derived. The implication
%
$$\text{\ttfamily StrictlyUpperTriangular}[t_1] \land I + t_1 \rightarrow \text{\ttfamily UpperTriangular}[I + t_1]$$
%
is used to infer that $(I - U)$ is upper triangular. $\text{\ttfamily Orthogonal}[R]$ and $\text{\ttfamily Orthogonal}[\underline{R}]$ imply that $R^T R$ and $\underline{R}^T \underline{R}$ are diagonal. Thus, this step yields
%
\begin{align*}
\mathcal{P} = P \cup \{ &\text{\ttfamily Diagonal}[R^T R], \\
				&\text{\ttfamily Diagonal}[\underline{R}^T \underline{R}], \\
				& \text{\ttfamily UpperTriangular}[I - U] \}\text{.}
\end{align*}
%
\subsubsection{Matrix Inversion}

From the set of properties $\mathcal{P}$ it follows that $A$, $D$ and $(I - U)$ are nonsingular. The equation $APD = R \left( \underline{I} - \underline{J} \right)$ is inspected first. Two invertible objects, namely $A$ and $D$, occur in it. Since $A$ is the leftmost quantity of the product $APD$, it can be eliminated by multiplying its inverse from the left-hand side to both sides of the equation. This yields $PD = A^{-1} R \left( \underline{I} - \underline{J} \right)$, which is added to $\mathcal{E}$. By multiplying $D^{-1}$ from the right to this new equation and the original $APD = R \left( \underline{I} - \underline{J} \right)$, two additional equations are obtained. By applying the same procedure to the two remaining equations, $\mathcal{E}$ becomes the following set:
%$AP = R \left( I -J  \right)D^{-1}$
\begin{align*}
\mathcal{E} = \{ &APD = R \left( \underline{I} - \underline{J} \right), &&AP = R \left( \underline{I} - \underline{J} \right)D^{-1}, \\
	&PD = A^{-1}R \left( \underline{I} - \underline{J} \right), &&P = A^{-1} R \left( \underline{I} - \underline{J} \right) D^{-1}, \\
	&P \left( I - U \right) = \underline{R}, &&P = \underline{R} \left( I - U \right)^{-1}, \\
	&PD = X \left( \underline{I} - \underline{J} \right), &&P = X \left( \underline{I} - \underline{J} \right) D^{-1} \}
\end{align*}
%
At this point, it is not possible to find any new equations by multiplying inverted quantities, so we proceed to the next step.

\subsubsection{Application of Properties}

Initially, the only properties $\text{\ttfamily P}[t] \in \mathcal{P}$ with $t = t_1 \cdots t_i$, $i > 1$ are
%
\begin{align*}
&\text{\ttfamily Diagonal}[R^T R] &&\text{\ttfamily Diagonal}[\underline{R}^T \underline{R}]\\
&\text{\ttfamily DiagonalR}[R^T \underline{R}] && \text{\ttfamily DiagonalR}[\underline{R}^T R] \text{.}
\end{align*}
%
From $\text{\ttfamily Diagonal}[R^T R]$, the set $S(R^T R) = \{(R^T, R)\}$ is obtained. For every $(t_L, t_R) \in S(R^T R)$, it now has to be checked if $t_L$ or $t_R$ appears in any equation contained in $E$. Since $(R^T, R)$ is the only element in $S(R^T R)$, the system just searches for $t_L = R^T$ and $t_R = R$. The following four equations, all containing $R$, are found:
%
\begin{align*}
&APD = R \left( \underline{I} - \underline{J} \right) &&AP = R \left( \underline{I} - \underline{J} \right)D^{-1} \\
	&PD = A^{-1}R \left( \underline{I} - \underline{J} \right) &&P = A^{-1} R \left( \underline{I} - \underline{J} \right) D^{-1}
\end{align*}
%
Only in the first two equations, $t_L = R$ appears at the rightmost position in a product. Hence, $t_R = R^T$ is multiplied just to those two:
%
\begin{align*}
&R^TAPD = R^TR \left( \underline{I} - \underline{J}  \right) &&R^TAP = R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}
\end{align*}
%
The knowledgebase is now used to derive new properties. In this example, we just look at the equation on the right-hand side, and show the steps that the system would perform.
%
\begin{enumerate}
\item $R^TR \left( \underline{I} - \underline{J}  \right)$ is a product of a diagonal and a lower trapezoidal matrix, so it is lower trapezoidal.
\item $D$ is diagonal, so $D^{-1}$ is diagonal as well.
\item $R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}$ is a product of a lower trapezoidal matrix ($R^TR \left( \underline{I} - \underline{J}  \right)$) and a diagonal matrix ($D^{-1}$), so it is lower trapezoidal too.
\item The right-hand side of the equation is lower trapezoidal, so the left-hand side of the equation, $R^TAP$, is lower trapezoidal as well.
\end{enumerate}
%
With every step, the new properties are added to $\mathcal{P}$. Thus, the set becomes:
%
\begin{align*}
\mathcal{P} := \mathcal{P} \cup \{ & \text{\ttfamily LowerTrapezoidal}[R^TR \left( \underline{I} - \underline{J}  \right) ], \\
	& \text{\ttfamily Diagonal}[D^{-1} ], \\
	& \text{\ttfamily LowerTrapezoidal}[R^TR \left( \underline{I} - \underline{J}  \right)D^{-1} ], \\
	& \text{\ttfamily LowerTrapezoidal}[R^T A P ] \}
\end{align*}
%
Now, it becomes apparent why it is not possible to simply set a limit on the length of properties and never add any longer properties to $\mathcal{P}$. $R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}$ is a product of four quantities. To infer that $R^T A P$ is lower trapezoidal, the property $$\text{\ttfamily LowerTrapezoidal}[R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}]$$ has to be derived. If this property is never added to $\mathcal{P}$, it is not possible to derive that $R^T A P$ is lower trapezoidal. Hence, it must be possible to derive properties of any length, even though they are only needed temporarily.

Using the newly added properties, immediately some more are found because of the implications 
%
\begin{align*}
\text{\ttfamily LowerTrapezoidal} \left[t\right] &\rightarrow \text{\ttfamily UpperTrapezoidal} \left[ t^T \right] \\
(t_1 t_2)^T &\rightarrow t_2^T t_1^T \text{.}
\end{align*}
%
The following properties are added to the set:
%
\begin{align*}
\mathcal{P} := \mathcal{P} \cup \{ & \text{\ttfamily UpperTrapezoidal}[ \left( \underline{I} - \underline{J}  \right)^T R^TR ], \\
	& \text{\ttfamily Diagonal} [D^{-T} ], \\
	& \text{\ttfamily UpperTrapezoidal} [ D^{-T} \left( \underline{I} - \underline{J} \right) R^TR ], \\
	& \text{\ttfamily UpperTrapezoidal} [P^T A R ] \}
\end{align*}
%
At this point, we cut this derivation short. In practice, many more properties would be derived. Most of them may not be of any use for the derivation of algorithms, but a few are crucial. For nonsymmetric CG, such an important property is that $P^T A P$ is lower triangular. In \cite{eijkhout:CGderivation}, this property is derived manually. Let us shortly illustrate how it is derived.

\begin{enumerate}
\item Because of $\text{\ttfamily DiagonalR}[\underline{R}^T R]$, $R^T$ is multiplied from the left to $P = \underline{R} \left( I - U \right)^{-1}$, yielding
%
$$R^T P = R^T \underline{R} \left( I - U \right)^{-1}\text{.}$$
%
$\left( I - U \right)^{-1}$ is upper triangular and $R^T \underline{R}$ is rectangular diagonal, so $R^T P$ is upper triangular and rectangular. It follows that its transpose $P^T R$ is lower triangular and rectangular.
%
\item Because of the property $\text{\ttfamily LowerTriangularR}[P^T R]$, $P^T$ is multiplied from the left to $AP D = R \left( \underline{I} - \underline{J}  \right)$. Based on the resulting equation
%
$$P^T A P D = P^T R \left( \underline{I} - \underline{J}  \right)\text{,}$$
%
we derive that $P^T R \left( \underline{I} - \underline{J}  \right)$ is square.
%
\item For the same reason, $P^T$ is multiplied from the left to $AP = R \left( \underline{I} - \underline{J}  \right)D^{-1}$, resulting in
%
$$P^T A P = P^T R \left( \underline{I} - \underline{J}  \right)D^{-1}\text{.}$$
%
$P^T R$ is lower triangular and rectangular and $\underline{I} - \underline{J}$ is lower trapezoidal. $P^T R \left( \underline{I} - \underline{J}  \right)$ is square, so it is lower triangular. Since $D^{-1}$ is diagonal, $P^T A P$ is lower triangular as well. 
\end{enumerate}

%\newpage
%
%From this property, we obtain $S(R^T R) = \{(R^T, R)\}$. For every $(t_L, t_R) \in S(R^T R)$, we now have to check if $t_L$ or $t_R$ appears in any equation contained in $E$. Since $(R^T, R)$ is the only element in $S(R^T R)$, we just need to look for $t_L = R^T$ and $t_R = R$. It is easy to see that $R^T$ does not appear in $\mathcal{E}$ at all. $R$ appears in the following four equations.
%%
%\begin{align*}
%&APD = R \left( \underline{I} - \underline{J}  \right) &&AP = R \left( \underline{I} - \underline{J}  \right)D^{-1} \\
%&P \left( I - U \right) = R &&P = R \left( I - U \right)^{-1}
%\end{align*}
%%
%In all cases, $t_L = R$ is the rightmost term of a product, so we can multiply $t_R = R^T$ to both sides of all equations.
%%
%\begin{align*}
%&R^TAPD = R^TR \left( \underline{I} - \underline{J}  \right) &&R^TAP = R^TR \left( \underline{I} - \underline{J}  \right)D^{-1} \\
%&R^TP \left( I - U \right) = R^TR &&R^TP = R^TR \left( I - U \right)^{-1}
%\end{align*}
%%
%The knowledgebase is now used to derive new properties. Let us inspect the two equations on the right, beginning with $R^TP = R^TR \left( I - U \right)^{-1}$. In a first step, we can infer that $\left( I - U \right)^{-1}$ is upper triangular, because $\left( I - U \right)$ is upper triangular. Then, $R^TR \left( I - U \right)^{-1}$ is a product of a diagonal and an upper triangular matrix, so this expression is upper triangular as well. Thus, $R^T P$ is also upper triangular. All those properties are added to $\mathcal{P}$:
%%
%\begin{align*}
%\mathcal{P} := \mathcal{P} \cup \{ & \text{\ttfamily UpperTriangular}[\left( I - U \right)^{-1}], \\
%	& \text{\ttfamily UpperTriangular}[R^TR \left( I - U \right)^{-1}], \\
%	& \text{\ttfamily UpperTriangular}[R^T P] \}
%\end{align*}
%%
%Using those new properties, we can immediately add three more to $\mathcal{P}$, using the implications
%%
%\begin{align*}
%\text{\ttfamily UpperTriangular} \left[t\right] &\rightarrow \text{\ttfamily LowerTriangular} \left[ t^T \right] \\
%(t_1 t_2)^T &\rightarrow t_2^T t_1^T \text{.}
%\end{align*}
%%
%Thus, we add
%%
%\begin{align*}
%\mathcal{P} := \mathcal{P} \cup \{ & \text{\ttfamily LowerTriangular}[\left( I - U \right)^{-T}], \\
%	& \text{\ttfamily LowerTriangular}[\left( I - U \right)^{-T} R^T R ], \\
%	& \text{\ttfamily LowerTriangular}[P^T R]\}\text{.}
%\end{align*}
%%
%The second equation we inspect is $R^TAP = R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}$. We know that $R^T R$ and $D^{-1}$ are diagonal, while $(\underline{I} - \underline{J})$ is lower triangular. Thus, we can conclude that the expression on the right-hand side of the equation is lower triangular as well. Now, it becomes apparent why it is not possible to simply set a limit on the length of properties and never add any longer properties to $\mathcal{P}$. $R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}$ is a product of four quantities. To infer that $R^T A P$ is lower triangular, the property $\text{\ttfamily LowerTriangular}[R^TR \left( \underline{I} - \underline{J}  \right)D^{-1}]$ has to be derived. If this property is never added to $\mathcal{P}$, it is not possible to derive that $R^T A P$ is lower triangular. Hence, it must be possible to derive properties of any length, even though they are only needed temporarily.
%
%At this point, we stop the derivation. In practice, many more properties would be derived. Most of them may not be of any use for the derivation of algorithms, but a few are crucial. For nonsymmetric CG, such an important property is that $P^T A P$ is lower triangular. It can be derived by applying $\text{\ttfamily LowerTriangular}[P^T R]$ to $AP = R \left( \underline{I} - \underline{J}  \right)D^{-1}$. In \cite{eijkhout:CGderivation}, this property is derived manually.

%Applying $\text{\ttfamily UpperTriangular}(R^T P)$ to $AP = R \left( I -J  \right)D^{-1}$, it can be derived that $P^T A P$ is lower triangular. 

%Additionally, it has to be possible to apply already known properties. If we know for example that $A$ is orthogonal, and we have the expression $A = B$, we can multiply $A^T$ from the left-hand side to obtain a new expression $A^T A = A^T B$, which tells us that $A^T B$ is diagonal.
