\chapter{Derivation of Algorithms for Iterative Methods: Foundations}
%\chapter{Matrix Representation of Iterative Methods}
\label{chap:iterativeMethodsIntro}

% Derivation of Algorithms for Iterative Methods: Foundations

%\todo{Chapter name? ``Towards a Systematic Derivation of Algorithms for Iterative Methods''? ``Preliminaries''?}

To be able to use a FLAME-like methodology to derive algorithms for iterative methods, a matrix representation of those methods is required. Such a representation was used in \cite{eijkhout:CGderivation} for CG and the Krylov sequence, and in \cite{eijkhout:CGvariants2} for some CG variants.

This chapter begins with the introduction of an additional notation. The details of the matrix representation are explained in the first section. In the second part, a systematic method for deriving properties of matrices from this representation is presented. Finally, different possible partitionings are discussed, in addition to their implications for the derivation of algorithms.

\subsubsection{Notation}

In this thesis, we use a notation that deviates slightly from the one used in \cite{eijkhout:CGvariants2} and \cite{eijkhout:CGderivation}. We use $e_0$ to denote the unit vector that is one in the first position and $e_r$ for the unit vector that is one in the last position. Both are column vectors. The matrix $J$ is a square matrix with ones on the lower diagonal:
%
$$
J = \left(
\begin{array}{c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c} 
%\begin{array}{c c c c}
0 & 0 & \hdots \\
1 & 0 & \\
0 & 1 & \ddots \\
\vdots & 0 & \ddots
\end{array} 
\right)
$$
%
 As usual, $I$ is the identity matrix. The dimension of those matrices and vectors will not be given explicitly if they are clear from the context. If $X$ is a matrix, we use $\underline{X}$ to indicate that the right-most column of this matrix is omitted. Thus, $\underline{I}$ and $\underline{J}$ are both lower trapezoidal matrices with one more row than columns.

\section{Matrix Representation}
\label{sec:matrixRepresentationIntroduction}

In this section, we discuss the details of the matrix representation for iterative methods used in this thesis. Note that the representation for CG, which will serve as an example, is a slightly modified version of the one introduced in \cite{eijkhout:CGderivation}. The difference lies in the use of the underline.
%
%As mention above, a matrix representation for some iterative methods was already introduced in \cite{eijkhout:CGvariants2} and \cite{eijkhout:CGderivation}.
%
%Occasionally, a similar representation is  used to explain some iterative methods, for example the Arnoldi method \cite{saad2011numerical} or MINRES \cite{barrett:templates}, but not with the intention to derive algorithms. Furthermore, they employ a mix matrices and indexed quantities. \todo{van der vorst reference}
%
%We will discuss the details of the representation used in this thesis in this section, using CG as an example.
%
The three governing equations are shown below. Deriving those equations is not trivial and beyond the scope of this thesis. 
%
\begin{align}
A P D &= R \left( \underline{I} - \underline{J}  \right) \label{eq:CGrr1} \\
P \left( I - U \right) &= \underline{R} \label{eq:CGrr2} \\
P D &= X \left( \underline{I} - \underline{J} \right) \label{eq:CGrr3}
\end{align}
%
%The first two equations are commonly referred to as the couple recurrence relation.
The operands have the following properties:
%
\begin{itemize}
%
\item[-] $A \in \mathbb{R}^{n \times n}$ is the coefficient matrix of the linear system that is supposed to be solved. It is nonsingular. Depending on whether $A$ is symmetric or not, different algorithms can be derived.
%
\item[-] $P \in \mathbb{R}^{n \times m}$ is the matrix of search directions, that is, each column represents the search direction vector during one iteration. It is initially unknown.
%
\item[-] $D \in \mathbb{R}^{m \times m}$ is an unknown diagonal matrix.
%
\item[-] $R \in \mathbb{R}^{n \times (m + 1)}$ is the residual matrix. Initially, only the first column $r_0$ is known. It is computed as $r_0 = A x_0 - b$, where $x_0$ is an initial guess for the solution. Additionally, it is orthogonal.
%
\item[-] $U \in \mathbb{R}^{m \times m}$ is unknown and upper diagonal\footnote{This property is defined in Appendix \ref{chap:appendixProperties}.} if $A$ is symmetric. Otherwise, it is strictly upper triangular.
%
\item[-] $X \in \mathbb{R}^{n \times (m + 1)}$ is the matrix of approximated solution vectors. Similar to $R$, only the first column is initially known.
%
\end{itemize}

While it might seem unusual that the same matrix ($R$) appears twice in the governing equations with varying sizes, this is necessary to ensure the correctness of the last column of $R$ and $X$.  The formula for computing the residual, in indexed notation, is $r_{i+1} = r_i - A p_i \delta_i$. Without the additional column of $R$ in equation (\ref{eq:CGrr1}), the incorrect equation $A p_i \delta_i = r_i$ would be obtained for the last iteration. Similarly, from equation (\ref{eq:CGrr3}), we would obtain $p_i \delta_i = x_i$, which is not correct either.

One of the fundamental differences compared to direct methods is that the dimensions of some matrices are not fixed. 
%While for direct methods, we never specify concrete sizes either, they are constant.
Usually, all dimensions are determined by the sizes of input operands. In case of the LU-factorization, for example, we know that $L$ and $U$ have the same size as $A$. If not, the equation $LU = A$ is not valid. If $A$ is of size $n \times n$, and the $LU$ factorization of a $k \times k$ block of $A$, with $k < n$, is computed, then the postcondition is not rendered true.

Due to the orthogonality of $R$, $m + 1$ can not be larger than $n$, as the number of $n$-dimensional, orthogonal vectors is at most $n$. However, for every $m < n$, the equations above can be satisfied by performing the corresponding number of iterations.

For the systematic derivation of algorithms, this introduces the problem that it is not possible to derive a loop guard exclusively from the equations. It makes no sense to compare the size of a block of $R$ to $R$ itself, because $R$ grows too. For those iterative methods that are used to find solutions for linear systems, the goal is usually to minimize the residual in some norm \cite{barrett:templates}. %\footnote{Quite often, there is also a limit on the maximum number of iterations, to ensure termination if the algorithm does not converge. It might also be desirable to check if the algorithms makes progress by testing if the norm of the residual decreases \cite{barrett:templates}. For the sake of simplicity, we do not consider those stopping criteria in this thesis. \todo{do we? what about stationary iteration? combination}}
Thus, the loop guard typically is a predicate like ``$\| r_i \| \geq \varepsilon$'', where $\varepsilon$ is a threshold chosen by the user. We make sure that the postcondition of CG correctly represents the situation at the end of the operation by adding $\| R e_r^T \| < \varepsilon$ to it. This also allows us to derive a suitable loop guard from it.

For stationary iterative methods, the stopping criterion quite often is $\| x_i - x_{i-1} \| < \varepsilon$. Translating that in our notation, we obtain $\| X e_r^T - X e_{r-1}^T \| < \varepsilon$. 
%
%This stopping criterion can be incorporated into the postcondition of CG by adding $\| R e_r^T \| < \varepsilon$.
%
For iterative methods where those criteria are not applicable, we will add an expression to the postcondition that fixes the number of columns of a matrix with variable size. Such a predicate could be ``$\text{size}(Y) = n \times k$''.\footnote{It is not uncommon to combine multiple criteria \cite{barrett:templates}. For the sake of simplicity, we do not use more than one at a time.}

