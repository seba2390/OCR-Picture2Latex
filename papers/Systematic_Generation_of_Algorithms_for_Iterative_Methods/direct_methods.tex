\chapter{Derivation of Algorithms for Direct Methods}
\label{chap:directMethods}

The purpose of this chapter is to introduce the reader to the systematic derivation of loop-based, blocked algorithms for direct methods using the FLAME methodology. For the derivation, we will follow the systematic approach and the notation of \cite{Fabregat-Traver:thesis}. We begin with the simple example of a triangular linear system in Section \ref{sec:triLS}. In Section \ref{sec:SPDsystem}, we proceed with a somewhat more elaborate example to go into some details that are not covered in the first one. Next, in Section \ref{sec:generalLS}, a case will be demonstrated where it is not possible to derive algorithms that compute the solution for every input of the operation. Finally, we discuss the equivalence of loop invariants that are obtained with this approach in Section \ref{sec:equivalence}.


%In this section, we will show how the \todo{FLAME methodology?} is used to derive algorithms for direct methods.
%%This is done by filling out a ``worksheet'', a template for a loop-based algorithm \cite{Bientinesi:thesis, Fabregat-Traver:thesis}, shown in Figure \ref{fig:ws:empty}.
%For the derivation, we will follow the systematic approach and the notation of \cite{Fabregat-Traver:thesis}. Since in the following sections, we will mainly focus on iterative methods for solving linear systems, algorithms for computing the solution to a linear system are also the most natural choice for an example.
%
%We start with a very simple example, a lower triangular system, followed by the somewhat more complex example of a symmetric positive definite system. Finally, we will show that it is not possible to derive algorithms to solve general linear systems with the FLAME methodology.

\subsubsection{Notation}

Throughout this thesis, two different notations are used for the indexing of matrix blocks and elements. The first one is the standard notation that uses numerals. The second one uses capital letters, as shown below:
%
\begin{align*}
& \bullet \myFlaOneByTwo{A_L}{A_R} 
&& \bullet \myFlaTwoByOne{A_T}{A_B}
&& \bullet \myFlaTwoByTwo{A_{TL}}{A_{TR}}{A_{BL}}{A_{BR}} \\
& \bullet \myFlaOneByThree{A_L}{A_M}{A_R}
&& \bullet \myFlaThreeByOne{A_T}{A_M}{A_B}
&& \bullet \myFlaThreeByThree	{A_{TL}}	{A_{TM}}	{A_{TR}}
					{A_{ML}}	{A_{MM}}	{A_{MR}}
					{A_{BL}}	{A_{BM}}	{A_{BR}}
\end{align*}

%\begin{center}
%\begin{tabular}{cc}
%& $\myFlaOneByTwo{A_L}{A_R}$ \\
%$\myFlaTwoByOne{A_T}{A_B}$ &
%$\myFlaTwoByTwo{A_{TL}}{A_{TR}}{A_{BL}}{A_{BR}}$
%\end{tabular}
%\end{center}

%\label{default}
%\end{table}%
%
%\begin{gather*}
%\myFlaOneByTwo{A_L}{A_R} \quad \myFlaTwoByOne{A_T}{A_B} \quad \myFlaTwoByTwo{A_{TL}}{A_{TR}}{A_{BL}}{A_{BR}} \\
%\myFlaOneByThree{A_L}{A_M}{A_R} \quad
%\myFlaThreeByOne{A_T}{A_M}{A_B} \quad
%\myFlaThreeByThree	{A_{TL}}	{A_{TM}}	{A_{TR}}
%					{A_{ML}}	{A_{MM}}	{A_{MR}}
%					{A_{BL}}	{A_{BM}}	{A_{BR}}
%\end{gather*}
%
The subscript letters $T$, $B$, $L$, $M$ and $R$ stand for Top, Bottom, Left, Middle and Right, respectively.


\section{Triangular Linear System}
\label{sec:triLS}

We begin with deriving algorithms for the linear system $Ax = b$ where $A$ is triangular.
%For reasons that will become apparent later, we will first consider the case where $A$ is triangular. 
The starting point of the derivation is a complete description of the problem, consisting of two logical predicates, a precondition ($P_\text{pre}$) and a postcondition ($P_\text{post}$). The precondition lists the properties of all quantities that are part of the operation, while the postcondition consists of the equation (or equations) that constitute the operation. The precondition is true before the execution of the algorithm, and our goal is to find an algorithm that makes the postcondition true upon termination. Additionally, a function is introduced which abstracts from the details of the operation, and instead describes the output as a function of the input. For reasons that will become apparent later, we use $\hat{b}$ to denote the initial content of $b$. The description for a linear system with $A$ lower triangular is:
%
$$x:= \Phi ( A, \hat{b}) \equiv
\left\{
\begin{aligned}
P_\text{pre}: \{ &\text{\ttfamily Input}[A] \land \text{\ttfamily Matrix}[A] \land \text{\ttfamily NonSingular}[A] \land \\
		&\text{\ttfamily LowerTriangular}[A] \land \\
		&\text{\ttfamily Input}[\hat{b}] \land \text{\ttfamily Vector}[\hat{b}] \land \\
		&\text{\ttfamily Output}[x] \land \text{\ttfamily Vector}[x] \} \\
P_\text{post}: \{ & Ax = \hat{b} \}
\end{aligned}
\right.$$
%
The actual algorithms are then constructed by filling out a so-called ``worksheet'', a template for a loop-based algorithm \cite{Bientinesi:thesis, Bientinesi2005:460, Fabregat-Traver:thesis}, shown in Figure \ref{fig:ws:empty}.

%
\begin{figure}
\centering
\begin{minipage}[t]{2.35in}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\WSoperation}{ $\ldots$}
	
	\renewcommand{\WSupdate}{\text{Update}}
	
	{
	\worksheetGrayNoNumbersEmpty
	}
\end{minipage}
\caption{The skeleton of a FLAME worksheet.}
\label{fig:ws:empty}
\end{figure}
%

\subsubsection{PME Generation}

As a first step towards an algorithm, the PME for the operation is generated. In order to do so, we partition the postcondition. For direct methods, partitioning operands along each dimension in at most two parts proved to be sufficient (this is further discussed in Section \ref{sec:equivalence}). 
Since $A$ is lower triangular, and to preserve the triangularity of the resulting objects, a $2 \times 2$ partitioning is chosen for it where $A_{TL}$ (and $A_{BR}$) is square.
%Since $A$ is lower triangular, a $2 \times 2$ partitioning is chosen for it. $x$ and $b$ are partitioned accordingly.
%
$$\myFlaTwoByTwo{A_{TL}}{ 0 }{A_{BL}}{A_{BR}} \myFlaTwoByOne{x_T}{x_B} = \myFlaTwoByOne{\hat{b}_T}{\hat{b}_B}$$
%
%As $A$ is lower triangular, we require that $A_{TL}$ is square. It follows that $A_{TL}$ and $A_{BR}$ are lower triangular, too.
Next, in the so called \emph{Matrix Arithmetic} step, we compute the symbolic multiplication and distribute the equality over the partitioned objects, resulting in the following expression:
%
$$\myFlaTwoByOne{A_{TL} x_T = \hat{b}_T}{A_{BL} x_T + A_{BR} x_B = \hat{b}_B}$$
%
In the final \emph{Pattern Matching} step, we try to find ways to solve the given equations by writing them as known operations. We begin with the top part. $A_{TL}$ and $b_T$ are known, while $x_T$ is unknown. Since $A_{TL}$ is lower triangular, we recognize that $A_{TL} x_T = \hat{b}_T$ is a lower triangular system. Thus, we can rewrite this expression as  $x_T := \Phi \left( A_{TL}, \hat{b}_T \right)$, utilizing the function introduced earlier. Doing so, we can from now on consider $x_T$ as known, which is necessary to solve the second equation. $A_{BL} x_T$ is a matrix-vector product of two known quantities, and if we subtract it on both sides of the equation, we identify $A_{BR} x_B = \hat{b}_B - A_{BL} x_T$ as another triangular system. Rewriting this as $x_B := \Phi \left( A_{BR}, \hat{b}_B - A_{BL} x_T \right)$ provides us with the PME
%
$$\myFlaTwoByOne{x_T := \Phi \left( A_{TL}, \hat{b}_T \right)}{x_B := \Phi \left( A_{BR}, \hat{b}_B - A_{BL} x_T \right)} \text{.}$$
%
\subsubsection{Loop Invariant Identification}

The next step consists of finding loop invariants ($P_{\text{inv}}$). A loop invariant is a logical predicate that is true at certain points of a loop in an algorithm. It is true before the loop is entered and after it is left, as well as at the beginning and the end of the loop body. It allows to formally reason about the correctness of the loop \cite{GrSc:92}.

As a first step, the PME is decomposed into basic operations, for example a matrix multiplication, or the function representing the operation we are deriving algorithms for. In general, one can think of those basic operations in terms of BLAS-like functions. For the lower triangular system, the following three operations are obtained:
%
\begin{enumerate}
\item $x_T := \Phi \left( A_{TL}, \hat{b}_T \right)$
\item $b_B := \hat{b}_B - A_{BL} x_T$
\item $x_B := \Phi \left( A_{BR}, b_B \right)$
\end{enumerate}
%
Then, from those operations, a dependency graph is constructed. Every node represents one operation, while the edges of this directed graph represent the data dependencies between those operations. If one operation requires the output of another operation, the former depends on the latter. Thus, in the graph, there are edges from the node that computes a quantity to those which need this quantity as input. Consider Figure \ref{fig:dg:loTriSys} for the resulting dependency graph.
%
%As a first step, a dependency graph is constructed. Every node of the graph represents a basic operation, for example a matrix multiplication, or the function representing the operation we are deriving algorithms for. In general, one can think of those basic operations in terms of BLAS-like functions. %\footnote{Since it is our goal to derive algorithms for solving triangular systems, we ignore that there already is such a BLAS function. \todo{Do we need to point that out?}}
%Decomposing the PME, we obtain the following three operations:
%%
%\begin{enumerate}
%\item $x_T := \Phi \left( A_{TL}, \hat{b}_T \right)$
%\item $b_B := \hat{b}_B - A_{BL} x_T$
%\item $x_B := \Phi \left( A_{BR}, b_B \right)$
%\end{enumerate}
%%
%%For the second operation, we have the choice to either update $b_B$, store $b_B - A_{BL} x_T$ in $x_B$, or introduce an auxiliary variable. In this example, we chose to use $x_B$.\footnote{Careful inspection of the update we derive in the following will reveal that, in order to ensure the correctness of the algorithm, $x$ initially has to contain the contents of $b$.}
%%
%The edges of this directed graph represent the data dependencies between the operations. If one operation requires the output of another operation, the former depends on the latter. Thus, in the graph, there are edges from the node that computes a quantity to those which need this quantity as input. Consider Figure \ref{fig:dg:loTriSys} for the resulting dependency graph.
%%
\begin{figure}[h]
\centering
\begin{tikzpicture}
%\draw[help lines] (0,-3) grid (7,0);

\node[default]	(A)							{$1$};

\node[default]	(B)	[below=of A, yshift=-0.0cm]	{$2$};

\node[default]	(C)	[right=of B, yshift=-0.0cm]		{$3$};

\path[-]	(-0.75cm, -0.75cm) edge [lightgray]  (2.25cm, -0.75cm);

\path[->]	(A)		edge							(B);
		
\path[->]	(B)		edge							(C);

\end{tikzpicture}
\caption{Dependency graph for a lower triangular system.}
\label{fig:dg:loTriSys}
\end{figure}

Subsets of nodes from this graph are now selected as loop invariants. For reasons that will be explained later, those subsets can neither be empty, nor can they contain all nodes. Furthermore, if a node is part of the subset, then all preceding nodes are also in the set. It follows that there are two subsets of nodes of the graph, $\{1\}$ and $\{1, 2\}$, that can be used as loop invariants. 
%
%\begin{align*}
%P_\text{inv} = &\left\{ x_T := \Phi \left( A_{TL}, b_T \right) \right\} \\
%P_\text{inv}' = &\left\{ x_T := \Phi \left( A_{TL}, b_T \right) \land \right. \\
%&\left. b_B := b_B - A_{BL} x_T \right\}
%\end{align*}
%
\begin{align*}
\{1\}: P_\text{inv}^{1} = &\left\{ x_T := \Phi \left( A_{TL}, \hat{b}_T \right) \right\} \\
\{1, 2\}: P_\text{inv}^{2} = &\left\{ x_T := \Phi \left( A_{TL}, \hat{b}_T \right) \land b_B := \hat{b}_B - A_{BL} x_T \right\}
\end{align*}
%
%\begin{table}[htdp]
%\begin{center}
%\begin{tabular}{cc}
%$\{1\}$									& $\{1, 2\}$ \\ \hline
%$\left\{ x_T := \Phi \left( A_{TL}, b_T \right) \right\}$	&
%$\begin{aligned}
%&\left\{ x_T := \Phi \left( A_{TL}, b_T \right) \land \right. \\
%&\left. b_B := b_B - A_{BL} x_T \right\}
%\end{aligned}$
%\end{tabular}
%%\caption{Possible partitionings for a general linear system.}
%%\label{tab:LS:part}
%\end{center}
%\end{table}
%
Now, we can also identify the loop guard $G$. The loop is supposed to terminate when all operands are traversed and the complete solution is computed. Thus, the loop guard depends on how the operands are traversed. The first assignment of the dependency graph operates on $x_T$, $A_{TL}$ and $b_T$, so we can conclude that $x$ and $b$ are traversed from the top to the bottom, and $A$ is traversed from the top-left to the bottom-right corner. In the algorithm, this has the result that initially, $A_{TL}$, $x_T$ and $b_T$ are empty. $A_{TL}$ grows with every iteration until it has the same size as $A$. Similarly, $x_T$ and $b_T$ grow to the size of $x$ and $b$. Hence, one possible loop guard is ``$\text{size}(A_{TL}) < \text{size}(A)$''. How $A_{TL}$, $x_T$ and $b_T$ change their sizes is explained in further detail in the next section.

\subsubsection{Algorithm Construction}

Having found a number of loop invariants, we can proceed to the final step, the derivation of the updates, using the worksheet (Figure \ref{fig:ws:empty}). For each loop invariant, a separate worksheet is filled out. Every loop invariant is rewritten in two different ways, resulting in two predicates per loop invariant. The first predicate specifies the state of all operands before the update ($P_\text{before}$). Analogously, the second predicate $P_\text{after}$ represents the situation after the update. The update is then found by identifying the operations that transform the predicate $P_\text{before}$ into $P_\text{after}$. Intuitively speaking, the difference between both predicates is determined.

The rewritings that are applied to the loop invariant are repartitionings of the already partitioned operands. They are chosen in a way that ensures that the operation makes progress and eventually terminates in combination with a suitable loop guard: Those blocks of the output that are already computed grow, and those parts that are not computed yet shrink. This is done by splitting off parts of some quantities by applying the ``Repartition'' rules and merging them with others with the ``Continue with'' repartitioning.
%
%This is done by deriving two predicates, one that is true before the update ($P_\text{before}$), and a second one that is true after the update ($P_\text{after}$). The difference between $P_\text{before}$ and $P_\text{after}$ is what the update needs to compute.

We begin with repartitioning the partitioned operands. % to make sure that the algorithm actually makes progress.
%
To obtain a blocked algorithm, we use the following ``Repartition'' rules:
\begin{align*}
\myFlaTwoByTwoI{A_{TL}}{ 0 }{A_{BL}}{A_{BR}} &\rightarrow
\myFlaThreeByThreeBRI	{A_{00}}	{0}		{0}
					{A_{10}}	{A_{11}}	{0}
					{A_{20}}	{A_{21}}	{A_{22}} \\
\FlaTwoByOneI{x_T}{x_B} &\rightarrow \FlaThreeByOneBI{x_{0}}{x_1}{x_2} \\
\FlaTwoByOneI{b_T}{b_B} &\rightarrow \FlaThreeByOneBI{b_{0}}{b_1}{b_2}
\end{align*}
%
Doing so, during every iteration, some parts of $A_{BR}$ are split off. To continue after the update, those parts are merged into $A_{TL}$. Thus, $A_{BR}$ shrinks and $A_{TL}$ grows until the entire matrix, and at the same time both vectors $b$ and $x$, are traversed. This ``Continue with'' partitioning is shown below.
%
\begin{align*}
\myFlaTwoByTwoI{A_{TL}}{ 0 }{A_{BL}}{A_{BR}} &\leftarrow
\myFlaThreeByThreeTLI	{A_{00}}	{0}		{0}
					{A_{10}}	{A_{11}}	{0}
					{A_{20}}	{A_{21}}	{A_{22}} \\
\FlaTwoByOneI{x_T}{x_B} &\leftarrow \FlaThreeByOneTI{x_{0}}{x_1}{x_2} \\
\FlaTwoByOneI{b_T}{b_B} &\leftarrow \FlaThreeByOneTI{b_{0}}{b_1}{b_2}
\end{align*}
%
To get to the predicates $P_\text{before}$ and $P_\text{after}$, we have to rewrite the loop invariant using the repartitioned operands. We demonstrate this for the second loop invariant, $P_\text{inv}^{2}$.
%We demonstrate this for the second loop invariant, consisting of the following two expressions:
%
%\begin{align*}
%x_T &:= \Phi \left( A_{TL}, b_T \right) \\
%b_B &:= b_B - A_{BL} x_T
%\end{align*}
%
For $P_\text{before}$, we replace all quantities by their counterparts according to the first partitioning (from $2 \times 2$ to $3 \times 3$), resulting in
%
\begin{align*}
x_0 &:= \Phi \left( A_{00}, \hat{b}_0 \right) \\
\myFlaTwoByOne{b_1}{b_2} &:= \myFlaTwoByOne{\hat{b}_1}{\hat{b}_2} - \myFlaTwoByOne{A_{10}}{A_{20}} x_0 \text{.} %\Leftrightarrow \left\{
%\begin{aligned}
%b_1 &:= b_1 - A_{10} x_0 \\
%b_2 &:= b_2 - A_{20} x_0
%\end{aligned}
%\right.
\end{align*}
%
Then, we flatten the expression, that is, perform all algebraic operations, distribute the assignments and decompose the result into its parts. This yields the following predicate:
%
\begin{align*}
P_\text{before} = \{	x_0 &:= \Phi \left( A_{00}, \hat{b}_0 \right) \land \\
				b_1 &:= \hat{b}_1 - A_{10} x_0 \land \\
				b_2 &:= \hat{b}_2 - A_{20} x_0 \}
\end{align*}
%
The same is done using the ``Continue with'' partitioning. To repartition the function $\Phi$, the PME is used.
%
\begin{align*}
\myFlaTwoByOne{x_0}{x_1} &:= \Phi \left( \myFlaTwoByTwo{A_{00}}{0}{A_{10}}{A_{11}}, \myFlaTwoByOne{\hat{b}_0}{\hat{b}_1} \right) \\
b_2 &:= \hat{b}_2 - \myFlaOneByTwo{A_{20}}{A_{21}} \myFlaTwoByOne{x_0}{x_1}
\end{align*}
%
Thus, we obtain the predicate below:
%
\begin{align*}
P_\text{after} = \{ 	x_0 &:= \Phi \left( A_{00}, \hat{b}_0 \right) \land \\
				x_1 &:= \Phi \left( A_{11}, \hat{b}_1 - A_{10} x_0 \right) \land \\
				b_2 &:= \hat{b}_2 - A_{20} x_0  - A_{21} x_1 \}
\end{align*}
%
Now, by comparing the predicates $P_\text{before}$ and $P_\text{after}$, we determine the update. Highlighted in red are those parts of $P_\text{after}$ that do not appear in $P_\text{before}$.
%
\begin{align*}
P_\text{after} = \{ 	&x_0 := \Phi ( A_{00}, \hat{b}_0 ) \land \\
				&\textcolor{red}{x_1 := \Phi} \mathopen{\textcolor{red}{(}}  \textcolor{red}{A_{11}} \mathpunct{\textcolor{red}{,}} \hat{b}_1 - A_{10} x_0 \mathclose{\textcolor{red}{)}} \land \\
				&b_2 := \hat{b}_2 - A_{20} x_0 \mathbin{\textcolor{red}{-}} \textcolor{red}{A_{21} x_1} \}
\end{align*}
%
This provides us with the following update:
%
\begin{align*}
x_1 &:= \Phi \left( A_{11}, b_1 \right) \\
b_2 &:= b_2 - A_{21} x_1
\end{align*}
%
We have now derived a complete algorithm. Consider Figure \ref{fig:ws:triLS} for the filled out worksheet with updates for both loop invariants (omitting some of the repartitionings and all predicates in the interest of visual clarity).

Using the loop guard and the loop invariants, it is easy to see that the derived algorithms are correct and at the end of the computation, the linear system is solved. The algorithms terminate when the loop guard becomes false. The negation of ``$\text{size}(A_{TL}) < \text{size}(A)$'' implies that $A_{TL}$ has the same size as $A$, as it can not be larger. Since $A_{TL}$ is a part of $A$, this means that $A_{TL}$ is equal to $A$. Similarly, $x_T$ equals $x$. $\hat{b}_T$ and $b_T$ are equal to $\hat{b}$ and $b$, respectively. As we updated $b$, we have to distinguish between its initial and its current content to reason about the correctness of the derived algorithm. All other parts have either size $0 \times n$, $0 \times 0$ or $n \times 0$, so they disappear. Plugging that in the loop invariants, in both cases we only get $x := \Phi \left( A, \hat{b} \right)$, which proves that both algorithms compute the solution to a triangular system.

\begin{figure}
\centering
\begin{minipage}[t]{4.5in}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\ALGroutinename}{ $x := \Phi \left( A, b \right)$}
	
	\renewcommand{\WSprecondition}{
	
	}
	
	\renewcommand{\WSpartition}{
        $
        A \rightarrow
		\myFlaTwoByTwo{A_{TL}}{ 0 }{A_{BL}}{A_{BR}}
	$
        }

	\renewcommand{\WSpartitionsizes}{
		$ A_{TL} $ is $ 0 \times 0 $
	}
	
	\renewcommand{\WSguard}{ $\text{size}(A_{TL}) < \text{size}(A)$ }
	
	
	\renewcommand{\WSrepartition}{
	$\myFlaTwoByTwoI{A_{TL}}{ 0 }{A_{BL}}{A_{BR}} 
	\rightarrow
	\myFlaThreeByThreeBRI	{A_{00}}	{0}		{0}
						{A_{10}}	{A_{11}}	{0}
						{A_{20}}	{A_{21}}	{A_{22}}
	$
	}
	
	\renewcommand{\WSrepartitionsizes}{
	$A$ is $k \times k$
	}

	
	\renewcommand{\WSupdate}{
	\begin{tabular}{c p{0.5cm} c}
	Variant 1 & & Variant 2 \\[4pt]
	$\begin{aligned}
	b_1 &:= \hat{b}_1 - A_{10} x_0 \\
	x_1 &:= \Phi \left( A_{11}, b_1 \right)
	\end{aligned}$
	& &
	$\begin{aligned}
	x_1 &:= \Phi \left( A_{11}, b_1 \right) \\
	b_2 &:= b_2 - A_{21} x_1
	\end{aligned}$
	\end{tabular}
	}
	
	\renewcommand{\WSmoveboundary}{
	$\myFlaTwoByTwoI{A_{TL}}{ 0 }{A_{BL}}{A_{BR}} \leftarrow
	\myFlaThreeByThreeTLI	{A_{00}}	{0}		{0}
					{A_{10}}	{A_{11}}	{0}
					{A_{20}}	{A_{21}}	{A_{22}}$
	}
	
	\renewcommand{\WSpostcondition}{
	
	}
	
	{
	\FlaAlgorithm
	}
\end{minipage}
\caption{Worksheet for a lower triangular linear system.}
\label{fig:ws:triLS}
\end{figure}


%\begin{tabular}{c p{0.5cm} c}
%Variant 1 ($P_\text{inv}$) & & Variant 2 ($P_\text{inv}'$) \\
%$\begin{aligned}
%b_1 &:= b_1 - A_{10} x_0 \\
%x_1 &:= \Phi \left( A_{11}, b_1 \right)
%\end{aligned}$
%& &
%$\begin{aligned}
%x_1 &:= \Phi \left( A_{11}, b_1 \right) \\
%b_2 &:= b_2 - A_{21} x_1
%\end{aligned}$
%\end{tabular}
%
%	$\begin{aligned}
%	&\text{Variant 1 ($P_\text{inv}$)}			& &\text{Variant 2 ($P_\text{inv}'$)} \\
%	b_1 &:= b_1 - A_{10} x_0\qquad			&x_1 &:= \Phi \left( A_{11}, b_1 \right) \\
%	x_1 &:= \Phi \left( A_{11}, b_1 \right)			&b_2 &:= b_2 - A_{21} x_1 \\
%	\end{aligned}$

\section{Symmetric Positive Definite Linear System}
\label{sec:SPDsystem}

As a second example, we derive algorithms for a symmetric positive definite (SPD) linear system to show in greater detail how the feasibility of loop invariants is checked. Again, the starting point is the formal description of the operation, which is shown below: 
%
$$x:= \Sigma \left( A, b\right) \equiv
\left\{
\begin{aligned}
P_\text{pre}: \{ &\text{\ttfamily Input}[A] \land \text{\ttfamily Matrix}[A] \land \text{\ttfamily SPD}[A] \land \\
		&\text{\ttfamily Input}[b] \land \text{\ttfamily Vector}[b] \land \\
		&\text{\ttfamily Output}[x] \land \text{\ttfamily Vector}[x] \} \\
P_\text{post}: \{ & \hat{A}x = \hat{b} \}
\end{aligned}
\right.$$
%
Because of the symmetry of $\hat{A}$, we apply a $2 \times 2$ partitioning where $\hat{A}_{TL}$ (and $\hat{A}_{BR}$) is square. Furthermore, because of the symmetry, it holds that $\hat{A}_{BL} = \hat{A}_{TR}^T$. The following partitioned postcondition is obtained.
%
$$\myFlaTwoByOne{\hat{A}_{TL} x_T + \hat{A}_{TR} x_B  = \hat{b}_T}{\hat{A}_{BL} x_T + \hat{A}_{BR} x_B = \hat{b}_B}$$
%
Unfortunately, none of the two equations matches the function $x:= \Sigma \left( A, b\right)$. If we rewrite $\hat{A}_{TL} x_T + \hat{A}_{TR} x_b  = \hat{b}_T$ as $\hat{A}_{TL} x_T = \hat{b}_T - \hat{A}_{TR} x_b $, the right-hand side of this equation is not completely known, as $x_b$ is not known. For the same reason, the second equation does not match the function either. In order to proceed, we need to perform a number of steps that are not part of the systematic approach presented in \cite{Fabregat-Traver:thesis}. As $\hat{A}$ is SPD and $\hat{A}_{TL}$ is square, we know that $\hat{A}_{TL}$ (and $\hat{A}_{BR}$) is also SPD, so we can rewrite the top part of the partitioned postcondition as:
%
$$x_T = \hat{A}_{TL}^{-1} \left( \hat{b}_T - \hat{A}_{TR} x_B \right)$$
%
By replacing $x_T$ in the second equation with the right-hand side of the equation above and performing further manipulations, it is possible to obtain an equation that matches the pattern of a SPD linear system. Since $\hat{A}$ is SPD, we can infer that $\hat{A}_{BR} - \hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{A}_{TR}$ is SPD too \cite{stew:98a}.
%
$$\left( \hat{A}_{BR} - \hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{A}_{TR} \right) x_B = \hat{b}_B - \hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{b}_T$$
%
Thus, the following PME is derived:
%
$$\myFlaTwoByOne{x_T := \Sigma \left(\hat{A}_{TL}, \hat{b}_T - \hat{A}_{TR} x_B \right)}{x_B := \Sigma \left( \hat{A}_{BR} - \hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{A}_{TR} , \hat{b}_B - \hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{b}_T \right)}$$
%
Libraries for linear algebra operations like BLAS usually do not offer operations for products of more than two quantities. Thus, to decompose this PME into its basic buildings blocks, we need to introduce auxiliary variables. In general, there are mutliple ways to compute expressions like $\hat{A}_{BL} \hat{A}_{TL}^{-1} \hat{A}_{TR}$, for example by solving $\hat{A}_{BL} \hat{A}_{TL}^{-1}$ or $\hat{A}_{TL}^{-1} \hat{A}_{TR}$ first. To ensure that the dependency graph is as general as possible and does not impose an ordering on expressions like these, multiple auxiliary variables would have to be introduced. Furthermore, their sizes would have to be left unspecified. To keep this example simple, we compute $\hat{A}_{BL} \hat{A}_{TL}^{-1}$ first, because it appears twice in the PME. While in that case, we only need one auxiliary variable that has the same size as $\hat{A}_{BL}$, it is useful to formally introduce a complete matrix $Z$ and partition it in the same way we partition $\hat{A}$. Doing so, we can treat the auxiliary variable just like all the other operands. Using $Z_{BL}$, we obtain the following decomposition. The corresponding dependency graph is shown in Figure \ref{fig:dg:SPDSys}.
%
%Note that those auxiliary variables do not have a fixed size. This is important to ensure that the dependency graph is as general as possible and does not impose any ordering on how a product of three quantities has to be computed ($(A B) C$ versus $A (B C)$ \todo{This looks ugly}). This, however, does not mean that the sizes are not known. Once an ordering is chosen, the sizes of the auxiliary variables can easily be inferred from the context \footnote{In an actual implementation, it might be useful or even necessary to introduce additional auxiliary variables with sizes that remain constant for their entire lifetime.}.
%
%\begin{enumerate}
%\item $W := A_{TL}^{-1}$
%\item $Y := W$
%\item $Y := A_{BL} Y$
%\item $Y := Y A_{TR}$
%\item $A_{BR} := A_{BR} - Y$ \todo{Is overwriting possible here?}
%\item $Z := W$
%\item $Z := A_{BL} Z$
%\item $Z := Z b_T$
%\item $b_B := b_B - Z$ \todo{Is overwriting possible here?}
%\item $x_B := \Sigma \left( A_{BR}, b_B \right)$
%\item $b_T := b_T - A_{TR} x_B$ \todo{Is overwriting possible here?}
%\item $x_T :=\Sigma \left( A_{TL}, b_T \right)$
%\end{enumerate}
%
\begin{enumerate}
\item $Z_{BL} := \hat{A}_{BL} \hat{A}_{TL}^{-1}$
\item $A_{BR} := \hat{A}_{BR} - Z_{BL} \hat{A}_{TR}$
\item $b_B := \hat{b}_B - Z_{BL} \hat{b}_T$
\item $x_B := \Sigma \left( A_{BR}, b_B \right)$
\item $b_T := \hat{b}_T - \hat{A}_{TR} x_B$
\item $x_T :=\Sigma \left( \hat{A}_{TL}, b_T \right)$
\end{enumerate}
%
%\begin{enumerate}
%\item $Z := A_{BL} A_{TL}^{-1}$
%\item $A_{BR} := A_{BR} - Z A_{TR}$ \todo{Is overwriting possible here?}
%\item $b_B := b_B - Z b_T$ \todo{Is overwriting possible here?}
%\item $x_B := \Sigma \left( A_{BR}, b_B \right)$
%\item $b_T := b_T - A_{TR} x_B$ \todo{Is overwriting possible here?}
%\item $x_T :=\Sigma \left( A_{TL}, b_T \right)$
%\end{enumerate}
%
%\begin{enumerate}
%\item $x_B := \Sigma \left( A_{BR} - A_{BL} A_{TL}^{-1} A_{TR} , b_B - A_{BL} A_{TL}^{-1} b_T \right)$
%\item $b_T := b_T - A_{TR} x_B$
%\item $x_T :=\Sigma \left( A_{TL}, b_T \right)$
%\end{enumerate}
%
%
\begin{figure}[h]
\centering
\begin{tikzpicture}
%\draw[help lines] (0,-3) grid (7,0);

\node[default]	(A)							{$1$};

\node[default]	(B)	[above=of A, yshift=-0.0cm]	{$2$};

\node[default]	(C)	[right=of A, yshift=-0.0cm]		{$3$};

\node[default]	(D)	[right=of B, yshift=-0.0cm]		{$4$};

\node[default]	(E)	[above=of D, yshift=-0.0cm]	{$5$};

\node[default]	(F)	[above=of E, yshift=-0.0cm]	{$6$};

\path[-]	(-0.75cm, 2.25cm) edge [lightgray]  (2.25cm, 2.25cm);

\path[->]	(A)		edge							(B);
\path[->]	(A)		edge							(C);
\path[->]	(B)		edge							(D);
\path[->]	(C)		edge							(D);
\path[->]	(D)		edge							(E);
\path[->]	(E)		edge							(F);

\end{tikzpicture}
\caption{Dependency graph for a SPD system.}
\label{fig:dg:SPDSys}
\end{figure}
%
The next step is again to select subsets of the dependency graph to use them as loop invariants and determine the loop guard.

Here, $\hat{A}$ is traversed from the bottom right to the top left, and both $x$ and $\hat{b}$ are traversed from the bottom to the top. Thus, the loop guard $G$ is ``$\text{size}(A_{BR}) < \text{size}(A)$''. If we had solved $\hat{A}_{BL} x_T + \hat{A}_{BR} x_B = \hat{b}_B$ to $x_B$ earlier, as opposed to $x_T$, we would have obtained an algorithm that proceeds in the opposite direction.
%By solving $A_{BL} x_T + A_{BR} x_B = b_B$ to $x_B$, it is possible to obtain an algorithms that proceeds in the opposite direction.

For this operation, to select loop invariants, the simplified rule that the subsets can neither be empty nor contain all nodes is not sufficient anymore. We now have to consider the full constraints. In general, it has to be checked whether a loop invariant is feasible, that is, if it leads to an algorithm that actually computes the operation. In \cite{Fabregat-Traver:thesis}, the author lists the following two constraints:
%
\begin{quote}
\itshape
\begin{enumerate}
\item There must exist a basic initialization of the operands, i.e., an initial partitioning, that renders the predicate $P_\text{\textrm{inv}}$ true:
\begin{align*}
&\{P_\text{\textrm{pre}}\} \\
&\text{\textbf{Partition}} \\
&\{P_\text{\textrm{inv}}\}
\end{align*}
\item $P_\text{\textrm{inv}}$ and the negation of the loop guard, $G$, must imply the postcondition, $P_\text{\textrm{post}}$:
$$P_\text{\textrm{inv}} \land \neg G \Rightarrow P_\text{\textrm{post}}$$
\end{enumerate}
\end{quote}
\label{feasibilityConditions}
%
The empty subset always fails to satisfy the second constraint, as it translates to an empty predicate. The empty predicate in conjunction with the negation of the loop guard can never imply the postcondition. Similarly, the set that contains all nodes can not satisfy the first condition. In this case, the solution to the operation would already be computed even before the loop is entered. However, merely partitioning the operands does not render such a loop invariant true. In addition to that, predicates $P_\text{before}$ and $P_\text{after}$ would be identical, so no update would be derived.

For SPD linear systems, there are additional subsets that fail to satisfy the second constraint. Let us look at $\{1\}$ for example. The corresponding loop invariant is $P_\text{inv} = \{Z_{BL} := \hat{A}_{BL} \hat{A}_{TL}^{-1}\}$. The negation of the the loop guard ``$\text{size}(A_{BR}) < \text{size}(A)$'' implies that $A_{BR}$ is equal to $A$. Then, $\hat{A}_{BL}$ and $\hat{A}_{TL}$ are empty, and the empty predicate does not imply the postcondition. If we take the subset $\{1, 2\}$, the loop invariant becomes $\{A := \hat{A} \}$ at the end of the operation, which does not imply the postcondition either.

Doing this for all subsets, two feasible loop invariants are obtained:
%
\begin{enumerate}
\item $\{1, 2, 3, 4\}$ ($P_\text{inv}^1$)
\item $\{1, 2, 3, 4, 5\}$ ($P_\text{inv}^2$)
\end{enumerate}
%
As mentioned before, the algorithms proceed from the bottom right to the top left, so the repartitionings are different compared to the ones used for the lower triangular system. First, parts of $A_{TL}$ are split off, which are then merged with $A_{BR}$ after the update. Thus, the ``Repartition'' statement for $A$ (and $Z$) is the following:
%
$$\myFlaTwoByTwoI{A_{TL}}{ A_{TR} }{A_{BL}}{A_{BR}} \rightarrow
	\myFlaThreeByThreeTLI	{A_{00}}	{A_{01}}		{A_{02}}
					{A_{10}}	{A_{11}}	{A_{12}}
					{A_{20}}	{A_{21}}	{A_{22}}$$
%
This then is the ``Continue with'' partitioning:
%
$$\myFlaTwoByTwoI{A_{TL}}{ A_{TR} }{A_{BL}}{A_{BR}} 
	\leftarrow
	\myFlaThreeByThreeBRI	{A_{00}}	{A_{01}}		{A_{02}}
						{A_{10}}	{A_{11}}	{A_{12}}
						{A_{20}}	{A_{21}}	{A_{22}}$$
%
Similarly, the repartitionings for $b$ are:
%
$$\FlaTwoByOneI{b_T}{b_B} \rightarrow \FlaThreeByOneTI{b_{0}}{b_1}{b_2}$$
%
$$ \FlaTwoByOneI{b_T}{b_B} \leftarrow \FlaThreeByOneBI{b_{0}}{b_1}{b_2} $$
%
$x$ is repartitioned in the same way. In the interest of brevity, the derivation of the updates will not be shown here. There are, however, two important points that should be mentioned. When deriving the predicate $P_\text{before}$, the expression
%
$$\myFlaOneByTwo{Z_{20}}{Z_{21}} := \myFlaOneByTwo{A_{20}}{A_{21}} \myFlaTwoByTwo{A_{00}}{A_{01}}{A_{10}}{A_{11}}^{-1}$$
%
appears. To flatten it, the PME of a different SPD system, namely $XA = B$, has to be used. Then, in order to find out which expressions appear both in $P_\text{before}$ and $P_\text{after}$ and to identify the differences, it is necessary to rewrite the expressions of both predicates first. Doing so, the auxiliary variables are eliminated.

Finally, the following two updates are found:
%
\begin{align*}
P_\text{inv}^1: 	b_1 &:= \hat{b}_1 - A_{12} x_2		&	P_\text{inv}^2: 	x_1 &:= \Sigma ( A_{11}, b_1 ) \\
			x_1 &:= \Sigma ( A_{11}, b_1 )	&				b_0 &:= b_0 - A_{01} x_1 
\end{align*}
%

\section{General Linear System}
\label{sec:generalLS}

In this section, we apply the FLAME methodology to a linear system where $A$ is a general, nonsingular matrix.
%
$$x:= \Lambda \left( A, b\right) \equiv
\left\{
\begin{aligned}
P_\text{pre}: \{ &\text{\ttfamily Input}[A] \land \text{\ttfamily Matrix}[A] \land \text{\ttfamily NonSingular}[A] \land \\
		&\text{\ttfamily Input}[b] \land \text{\ttfamily Vector}[b] \land \\
		&\text{\ttfamily Output}[x] \land \text{\ttfamily Vector}[x] \} \\
P_\text{post}: \{ & \hat{A}x = \hat{b} \}
\end{aligned}
\right.$$
%

Since $\hat{A}$ is not lower triangular, there is more than one possibility for the initial partitioning. The different partitionings are shown in Table \ref{tab:LS:part}.
%
\begin{table}[htp]
\begin{center}
\renewcommand*{\arraystretch}{1.4}
%\begin{tabular}{|c|c|c|}
\begin{tabular}{ccc}
%\multicolumn{2}{test} \\
\toprule
\#	&	Partitioned postcondition	& Flattened expression	\\ \midrule
1	&	$\myFlaOneByTwo{\hat{A}_L}{\hat{A}_R} \myFlaTwoByOne{x_T}{x_B} = \hat{b}$	&	$\hat{A}_L x_T + \hat{A}_R x_B = \hat{b}$	\\
2	&	$\myFlaTwoByOne{\hat{A}_T}{\hat{A}_B} x = \myFlaTwoByOne{\hat{b}_T}{\hat{b}_B}$	&	$\myFlaTwoByOne{\hat{A}_T x = \hat{b}_T}{\hat{A}_B x = \hat{b}_B}$\\
3	&	$\myFlaTwoByTwo{\hat{A}_{TL}}{ \hat{A}_{TR} }{\hat{A}_{BL}}{\hat{A}_{BR}} \myFlaTwoByOne{x_T}{x_B} = \myFlaTwoByOne{\hat{b}_T}{\hat{b}_B}$	& $\myFlaTwoByOne{\hat{A}_{TL} x_T + \hat{A}_{TR} x_B  = \hat{b}_T}{\hat{A}_{BL} x_T + \hat{A}_{BR} x_B = \hat{b}_B}$ \\ \bottomrule
\end{tabular}
\caption{Possible partitionings for a general linear system.}
\label{tab:LS:part}
\end{center}
\end{table}

At this point, we are again not able to make any further progress with the usual approach. No (sub)expression of the three expressions matches the initial operation. In case of the first two, the parts of $\hat{A}$ are not square. Some or all of the four parts of $\hat{A}$ in the third expression could be square, but since the partitioning does not prescribe any sizes, we can in general not assume that this is the case for any part.

If we use the third partitioning and assume that $\hat{A}_{TL}$ (and $\hat{A}_{BR}$) is square, we are in a similar situation as with the SPD system. Recall that to proceed, we had to rewrite
%
$$\hat{A}_{TL} x_T + \hat{A}_{TR} x_B  = \hat{b}_T$$
%
as 
%
$$x_T = \hat{A}_{TL}^{-1} \left( \hat{b}_T - \hat{A}_{TR} x_B \right) \text{.}$$
%
This was possible because $\hat{A}$ is SPD, so we were able to infer that $\hat{A}_{TL}$ is SPD, and thus nonsingular, as well. The difference now is that the $\hat{A}_{TL}$ part of a nonsingular matrix $\hat{A}$ is in general not nonsingular. While we could assume that it is, proceed as we did in the previous section and derive similar algorithms, those algorithms would not solve general linear systems. 

%
%
%Recall that for the SPD systems, those two expressions were SPD as well. This was necessary to obtain a PME, which consisted of two linear systems of $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$:
%%
%$$\myFlaTwoByOne{x_T := \Sigma \left(A_{TL}, b_T - A_{TR} x_B \right)}{x_B := \Sigma \left( A_{BR} - A_{BL} A_{TL}^{-1} A_{TR} , b_B - A_{BL} A_{TL}^{-1} b_T \right)}$$
%%
%The PME for a general linear system would have exactly the same shape. The difference is that $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ are in general not nonsingular.



%Since $A$ is not lower triangular anymore, there is more than one possibility for the initial partitioning, resulting in different PMEs (Table \ref{tab:LS:part}).
%
%At this point, we are again not able to make any further progress with the usual approach. No (sub)expression of the three PMEs matches the initial operation. In case of the first two, the parts of $A$ can never be square.\footnote{Unless we consider one part to be empty, which is equivalent to not partitioning at all.} Some or all of the four parts of $A$ in the third PME could be square, but since the partitioning does not prescribe any sizes, we can in general not assume that this is the case for any part. Even if for example $A_{TL}$ was square, it would still not be possible to apply any pattern. While it is possible to rewrite $A_{TL} x_T + A_{TR} x_b  = b_T$ as $A_{TL} x_T = b_T - A_{TR} x_b $, the right-hand side of this equation is not completely known, as $x_b$ is not known.
%
%To derive any algorithms, we had to impose the restriction on $A_{TL}$ that is has to be square and assume that $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ are nonsingular. The former is a choice we can make if we disregard the conventions of the FLAME methodology for partitioning matrices. Similarly, it is possible to assume the latter. Unfortunately, this assumption is in general not true for nonsingular matrices. So while it would still be possible to proceed now as we did in the previous section and derive similar algorithms, those algorithms would not solve general linear systems.



%
%Additionally, $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ had to be nonsingular, too. Requiring that $A_{TL}$ is square i The non singularity of a matrix $A$, however, does not imply that $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ are nonsingular two.
%
%
%With the assumption that $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ are nonsingular, however, 
%
%For genreal, nonsingular matrices, however, $A_{TL}$ and $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ is not necessarily 
%
%In order to proceed, we could impose the restriction on $A_{TL}$ that is has to be square and nonsingular. While the former is just a choice we make, the latter is not true for general, nonsingular matrices. While it would be possible to proceed now as we did in the previous section and derive similar algorithm, those algorithms would not solve general linear systems.

%Additionally, $A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}$ had to be nonsingular, too. 

%
%In order to proceed, we need to perform some steps that are not part of the FLAME methodology. We begin by imposing the restriction that $A_{TL}$, and thus also $A_{BR}$, is square. Next, we infer from $A$ being nonsingular that $A_{TL}$ is also nonsingular \cite{}\todo{Reference!}. This now allows us to manipulate the equations as follows. The top part of the PME is solved to $x_T$.
%%
%$$x_T = A_{TL}^{-1} \left( b_T - A_{TR} x_B \right)$$
%%
%Plugging that into the second equation and doing some further manipulations, it is possible to obtain an equation that matches the pattern of a general linear system:
%%
%$$\left( A_{BR} - A_{BL} A_{TL}^{-1} A_{TR} \right) x_B = b_B - A_{BL} A_{TL}^{-1} b_T$$
%%
%
%\begin{align*}
%x_T &:= A_{TL}^{-1} \left( b_T - A_{TR} x_B \right) \\
%x_B &:= \Lambda \left( A_{BR} - A_{BL} A_{TL}^{-1} A_{TR}, b_B - A_{BL} A_{TL}^{-1} b_T \right)
%\end{align*}