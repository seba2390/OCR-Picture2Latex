\chapter{Derivation of Algorithms for Iterative Methods}
\label{chap:derivationIM}

After having laid the foundations in the previous chapter, the actual approach for deriving algorithms for iterative methods is presented in this chapter. The approach itself can be found in Section \ref{sec:derivationIterativeMethods}, followed by two examples in Section \ref{sec:derivationOfAlgorithmsExample} and \ref{sec:BiCGexample}. In the final Section \ref{sec:scopeLimitations}, scope and limitations of the approach are discussed.

\section{Derivation of Algorithms}
\label{sec:derivationIterativeMethods}

The derivation of algorithms for iterative methods mainly follows the same three basic steps as for direct methods. First, one or more PMEs are generated. In the second step, dependency graphs are constructed, which are then used to select loop invariants. In the third and final step, from each loop invariant, an algorithm is constructed. In this section, we will present this process for iterative methods. There is, however, a new fourth step. In this step, some postprocessing is applied to the derived algorithms to generate a number of variants that may behave differently in floating point arithmetic or vary in their performance. If applicable, we follow the same structure as \cite{Fabregat-Traver:thesis}.

\subsection{PME Generation}

The first stage towards the generation of algorithms is to find PMEs. The necessary steps are explained in the following. There are a number of differences compared to direct methods: It is necessary to derive properties of matrices, which then enable the system to solve equations. Additionally, different partitionings are needed to deal with new types of operands.

\subsubsection{Derivation of Properties}

The approach presented in Section \ref{sec:propertyDerivation} is used to derive properties.

\subsubsection{Initial Partitioning}

Operands of the postcondition are partitioned depending on their shape and properties. For objects that are completely known or unknown, the applicable partitionings are similar to the ones used for direct methods. The difference lies in the sizes of the resulting objects. Only the top, left, and top left parts are matrices, the remaining ones are either vectors or scalars:
%
\begin{align*}
B &\rightarrow B	&	B &\rightarrow \myFlaOneByTwo{B_L}{b_R} \\
B &\rightarrow \myFlaTwoByOne{B_T}{b_B}	&	B &\rightarrow \myFlaTwoByTwo{B_{TL}}{b_{TR}}{b_{BL}}{\beta_{BR}}
\end{align*}
%\begin{center}
%\begin{tabular}{cc}
%$B \rightarrow B$ & $B \rightarrow \myFlaOneByTwo{B_L}{b_M}$ \\
%$B \rightarrow \myFlaTwoByOne{B_T}{b_M}$ & $B \rightarrow \myFlaTwoByTwo{B_{TL}}{b_{TM}}{b_{ML}}{\beta_{MM}}$
%\end{tabular}
%\end{center}
%
Just as with direct methods, triangular or symmetric matrices are either not partitioned at all, or the $2 \times 2$ partitioning is used. In case of the latter, the top left part is required to be square, such that it inherits the property of the matrix.
%If the original matrix is square, the top left part is square to, so 
%It is not necessary to explicitly impose the restriction that the top left part is square in the latter case
%In case of the latter, the top left part is required to be square, such that it inherits the property of the original matrix. 

Separate partitionings are necessary to deal with matrices where initially, only the first column is known. If the last column of those matrices is omitted, the usual $1 \times 2$ partitioning is applied:
%
$$\underline{B} \rightarrow \myFlaOneByTwo{B_L}{b_R}$$
%
In case of the complete matrix, an additional column is obtained:
%
$$B \rightarrow \myFlaOneByThree{B_L}{b_R}{b_{+}}$$
%
The constant matrices $\underline{J}$ and $\underline{I}$ pose a special case. They have one more row than columns, so $\underline{J}$ is not lower diagonal and $\underline{I}$ is not an identity matrix. To derive algorithms, it is important to utilize their specific structure, so we will provide the partitionings explicitly:
%
\begin{align*}
\underline{J} \rightarrow \myFlaThreeByTwo{J}{0}{e_r^T}{0}{0}{1} \qquad \underline{I} \rightarrow \myFlaThreeByTwo{I}{0}{0}{1}{0}{0}
\end{align*}
%
The same partitioning that is applied to the operands in the postcondition is also applied to the operands in the set of properties. Then, properties of expressions of partitioned operands are derived, similar to how partitioned operands inherit properties. If for example $B$ is partitioned into $\myFlaOneByTwo{B_L}{b_R}$, and $\text{\ttfamily Diagonal} \left[B^T B\right]$ is contained in the set of properties, the system obtains
%
$$\text{\ttfamily Diagonal} \left[\myFlaTwoByOne{B_L^T}{b_R^T} \myFlaOneByTwo{B_L}{b_R} \right] = \text{\ttfamily Diagonal} \left[\myFlaTwoByTwo{B_L^T B_L}{B_L^T b_R}{b_R^T B_L}{b_R^T b_R} \right] \text{.}$$
%
Thus, it is possible to derive that $B_L^T B_L$ is diagonal as well, and $B_L^T b_R$ and $b_R^T B_L$ are zero.

\subsubsection{Finding the PME}

The first part of this step consists of performing symbolic arithmetic and distributing equalities across the partitionings. Consider the Krylov sequence as an example:
%
\begin{align*}
K:= \text{KS} ( A, K e_0) \equiv
\left\{
\begin{aligned}
P_\text{pre}: \{ &\text{\ttfamily Input}(A) \land \text{\ttfamily Matrix}(A) \\
		&\text{\ttfamily Matrix}[\underline{J}] \land \text{\ttfamily LowerDiagonalR}[\underline{J}] \land \\
		&\text{\ttfamily FirstColumnInput}(K) \} \\
P_\text{post}: \{ &A \underline{K} = K \underline{J} \\
			&\text{size}(K) = n \times m \}
\end{aligned}
\right.
\label{eq:KrylovSeq:Description}
\end{align*}
%
The partitioned postcondition looks as follows:
%
$$A \myFlaOneByTwo{K_L}{k_R} = \myFlaOneByThree{K_L}{k_R}{k_{+}} \myFlaThreeByTwo{J}{0}{e_r^T}{0}{0}{1}$$
%
It can be rewritten as the following expression:
%
$$\myFlaOneByTwo{A K_L = K_L J + k_R e_r^T}{ A k_R = k_{+}}$$
%
%we attempt to describe the expression that was obtained in terms of known operations. T
In the second part, the goal is to find a representation of this expression where the value of each unknown quantity is determined by an assignment, using known operations. The quantities on the right-hand side of that assignment either have to be known, or their value is determined by another assignment. Intuitively, one could say the goal is to make this expression computable. This is done by matching patterns of known functions and operations.

Rewriting the equation on the left-hand side as
%
$$A K_L = \myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{J}{e_r^T} \text{,}$$
%
it is easy to see that it describes the computation of a Krylov sequence, so it is possible to use the function from the description of the operation:
%\todo{$\myFlaOneByTwo{K_L}{k_m} \myFlaTwoByOne{e_0}{0} ?$}
%
$$\myFlaOneByTwo{K_L}{k_R} := \text{KS} \left(A, \myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{e_0}{0} \right)$$
%
Now, $K_L$ and $k_R$ can be considered known, so all that remains is to find an assignment for $k_+$. In this example, the equation on the right hand side is already solved to $k_+$, so
%
$$k_+ := A k_R$$
%
is immediately obtained. The PME then is 
%
$$\myFlaOneByTwo{ \myFlaOneByTwo{K_L}{k_R} := \text{KS} \left(A, \myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{e_0}{0} \right) }{ k_+ := A k_R} \text{.}$$
%
In general, finding assignments is not that straightforward. Usually, it is necessary to apply known properties. Similar to the ``Application of Properties'' step in Section \ref{sec:propertyDerivation:Derivation}, parts of expressions with known properties are multiplied to both sides of equations, with the intention to recreate said expressions. Unfortunately, it is not easy to determine beforehand if the application of a property allows to solve an equation. Thus, in the manner of an exhaustive search, all matching properties are applied. If by this means, multiple assignments for the same expression are found, separate PMEs are derived for each variant.

\subsubsection{Remark on Recursive Algorithms}
%\todo{put that at the end of the next chapter?}
%\todo{subsubsection instead of paragraph? after PME introduction}
The PMEs for direct methods immediately lead to recursive algorithms \cite{Bientinesi:thesis, Fabregat-Traver:thesis}. This is also true for iterative methods. The difference is that in case of direct methods, those algorithms are usually divide and conquer algorithms. For iterative methods, the PME naturally leads to a ``head recursive'' implementation, that is, the recursive function call is the first operation in the function body.

\subsection{Loop Invariant Identification}

For the second step, the identification of loop invariants, constraints for the feasibility of loop invariants are introduced that differ from the ones used for direct methods. To understand why they are introduced, it is helpful to get an intuition for what those loop invariants express, and how they differ from the ones for direct methods.

\paragraph{Loop Invariants for Iterative Methods}

For iterative methods, there is usually a (partial) ordering in which parts of different operands can be computed. Let us assume there are three matrices $B$, $C$ and $D$. During the execution of the algorithm, the following sequence is computed, where $b_k$, $c_k$ and $d_K$ are columns of $B$, $C$ and $D$, respectively:
%
$$\ldots b_i \; c_i \; d_i \; b_{i+1} \; c_{i+1} \; d_{i+1} \ldots$$
%
A variant derived from one loop invariant may compute $c_i \; d_i \; b_{i+1}$ in one iteration. In contrast, one derived from another loop invariant may compute $d_i \; b_{i+1} \; c_{i+1}$.
%
%By definition, a loop invariant can not express how many iterations have already been computed, because that number changes with each iteration and is thus not \emph{invariant}. \todo{of course, that's why the full is not a loop invariant} As long as we assume that the number of previous iterations is small enough to not violate any orthogonality constraints, it is always possible to compute an additional iteration. Hence, there is no loop invariant that implies that the entire solution is already computed.
%
Since no quantities are overwritten, a loop invariant only expresses at which point in the sequence above an iteration starts.

In contrast, different loop invariants for direct methods may result in algorithms that compute the solution by row or by column. 



\subsubsection{Graph of Dependencies}

The construction of the dependency graph is even simpler compared to direct methods. The assignments of the PME are not decomposed into basic building blocks. Each assignment is represented by one node in the dependency graph. The dependencies are established as usual.
%There is no difference in how the dependency graph is constructed compared to direct methods. %Each function or assignment is represented by one node.
%\todo{How much detail?}

\subsubsection{Subset Selection}

Just as with direct methods, subsets of nodes of the dependency graph are selected as candidates for loop invariants. Again, for every node that is contained in a subset, all preceding nodes have to be in that set, too. To assess the feasibility of loop invariants, however, slightly modified constraints have to be imposed:
%\cite{Fabregat-Traver:thesis}
%
\begin{enumerate}
\item There must exist a basic initialization of the operands, that is, an initial partitioning, followed by some preprocessing operations, that renders the predicate $P_\text{\textrm{inv}}$ true:
\begin{align*}
&\{P_\text{\textrm{pre}}\} \\
&\text{\textbf{Partition}} \\
&\text{Preprocessing} \\
&\{P_\text{\textrm{inv}}\}
\end{align*}
\item $P_\text{\textrm{inv}}$ and the negation of the loop guard, $G$, must imply the postcondition, $P_\text{\textrm{post}}$:
$$P_\text{\textrm{inv}} \land \neg G \Rightarrow P_\text{\textrm{post}}$$
%\item $P_\text{\textrm{inv}}$ and the negation of the loop guard, $G$, in conjunction with some postprocessing operations, must imply the postcondition, $P_\text{\textrm{post}}$:
%$$P_\text{\textrm{inv}} \land \neg G \land \text{Postprocessing} \Rightarrow P_\text{\textrm{post}}$$
\end{enumerate}
%

We begin with discussing the first condition. How the operands are partitioned was already established in the Section ``Initial Partitioning'', but the initial sizes of the partitioned operands were not specified. The iterative methods covered in this thesis proceed through those matrices where initially, only the first column is known, from the left to the right. As a consequence, the left parts of those matrices are initially empty. The initial block sizes for all partitionings are given in Table \ref{tab:initialBlockDimensions}.
%
\begin{table}[htp]
\begin{center}
\begin{tabular}{l l}
\toprule
Initial Partitioning	&	Dimensions \\ \midrule
$B \rightarrow \myFlaOneByTwo{B_L}{b_R}$	&	$B_L$ is $n \times 0$ \\
$B \rightarrow \myFlaTwoByOne{B_T}{b_R}$	&	$B_T$ is $0 \times n$ \\
$B \rightarrow \myFlaTwoByTwo{B_{TL}}{b_{TR}}{b_{BL}}{\beta_{BR}}$	&	$B_{TL}$ is $0 \times 0$ \\
$\underline{B} \rightarrow \myFlaOneByTwo{B_L}{b_R}$	&	$B_L$ is $n \times 0$ \\
$B \rightarrow \myFlaOneByThree{B_L}{b_R}{b_+}$		&	$B_L$ is $n \times 0$ \\ \bottomrule
\end{tabular}
\end{center}
\caption{Initial sizes of partitioned operands.}
\label{tab:initialBlockDimensions}
\end{table}%

Recall that for direct methods, the full set of nodes of the dependency graph can never be a feasible loop invariant. Formally, the reason is that there is no initial partitioning that renders this loop invariant true. Alternatively, one can say that it implies that the complete solution is already computed even before the loop is entered. For iterative methods, this is different. Due to the initial partitioning, which does not expose blocks that represent the remaining iterations, there is no subset implying that the entire solution is already computed. Thus, the full set is a feasible loop invariant.

However, further loop invariant candidates for iterative methods are not rendered true by the initial partitioning for a different reason. This is the case for some subsets that contain more than the first node. The reason is that the additional nodes may compute quantities that are not empty in the initial partitioning. Consider the Krylov sequence as an example: $k_R$ and $k_+$ remain $n \times 1$ vectors in the initial partitioning. Thus, the assignment $k_+ := A k_R$ computes a non-empty quantity, even if the initial partitioning is applied. However, at the beginning of the operation, only $k_R$ is known, which is the fist column of $K$ in the initial partitioning, while $k_+$ is the second column, which is unknown.

Fortunately, the equations from those additional nodes can be rendered true by computing those quantities with some preprocessing operations. Naturally, the necessary preprocessing operations are obtained by applying the initial partitioning to the operations of those nodes. In this example, the preprocessing operation is $k_+ := A k_R$. Those preprocessing operations can consist of the same type of operations as the actual update operations.

To demonstrate how the first constraint is checked, we return to the example of the Krylov sequence. We start with the following loop invariant candidate:
%
$$\myFlaOneByTwo{ \myFlaOneByTwo{K_L}{k_R} := \text{KS} \left(A, \myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{e_0}{0} \right) }{ \neq}$$
%
If $K_L$ has size $n \times 0$, the expression on the left-hand side becomes
%
$k_R := \text{KS} \left(A, k_R \right)$.
%
Initially, the first column of $K$, which is now $k_R$, is known. Thus, since both sides of this assignment are known, this expression is considered to be true. This implies that the loop invariant satisfies the first constraint.

The second candidate for a loop invariant is 
%
$$\myFlaOneByTwo{ \myFlaOneByTwo{K_L}{k_R} := \text{KS} \left(A, \myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{e_0}{0} \right) }{ k_+ := A k_R} \text{.}$$
%
Note that in the interest of simplicity, we usually just write $K_L e_0$ instead of
%
$$\myFlaOneByTwo{K_L}{k_R} \myFlaTwoByOne{e_0}{0}\text{,}$$
%
if it is sufficient. The following expression is obtained if the initial partitioning is applied, where $K_L$ is empty:
%
$$\myFlaOneByTwo{k_R := \text{KS} \left(A, k_R \right)}{k_+ := A k_R}$$
%
As $k_+$ is not known, the initial partitioning alone does not render this expression true. This, however, can be solved with a preprocessing operation. Here, the operation is $k_+ := A k_R$. It follows that this loop invariant satisfies the first constraint as well.

To check if the second condition for the feasibility of loop invariants is satisfied, we first need to determine the loop guard. As discussed in Section $\ref{sec:matrixRepresentationIntroduction}$, comparing the size of a growing block of a matrix to the size of the entire matrix does not work, as the matrix grows as well. For this reason, the additional predicates were added to the postcondition (by hand), which are now easily translated into loop guards (automatically). How this is done is shown in Table \ref{tab:loopGuards}.
%
\begin{table}[htp]
\begin{center}
\begin{tabular}{lll}
\toprule
 & \multicolumn{2}{c}{Loop guard} \\ \cmidrule{2-3}
Predicate & $\text{\ttfamily FirstColumnInput}[B]$ & $\text{\ttfamily Output}[B]$ \\ \midrule
$\| B e_r^T \| < \varepsilon$ & $\| b_R \| \geq \varepsilon$ & $\| B_L e_r^T \| \geq \varepsilon$ \\ 
$\text{size}(B) = n \times k$ & $\text{size}\left( \myFlaOneByTwo{B_L}{b_R} \right) < n \times k$ & $\text{size}\left( B_L \right) < n \times k$ \\
$\| B e_r^T - B e_{r-1}^T \| < \varepsilon$ & $\| b_R - B_L e_{r}^T \| \geq \varepsilon$ & $\| B_L e_r^T - B_L e_{r-1}^T \| \geq \varepsilon$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{Look-up table for determining loop guards for iterative methods. The row is selected according to the additional predicate in the postcondition. The column is selected depending on the property of the operand that appears in the position of $B$ in that predicate. Example: The predicate is $\| R e_r^T \| < \varepsilon$. The precondition contains the property $\text{\ttfamily FirstColumnInput}[R]$. Thus, the loop guard is $\| r_R \| \geq \varepsilon$. To allow for other loop guards, this table has to be extended manually.}
\label{tab:loopGuards}
\end{table}%

Note that even though $B e_r^T$ in the predicate $\| B e_r^T \| < \varepsilon$ refers to the last column of $B$, $b_R$ in the loop guard $\| b_R \| \geq \varepsilon$ and $B_L e_r^T$ in $\| B_L e_r^T \| \geq \varepsilon$, respectively, are the second to last columns. Similarly, the other loop guards omit the last column as well. To understand why this is necessary, we have to look at the loop invariant candidates again. Just as with direct methods, the empty set can never be a valid loop invariant because it corresponds to the empty predicate. Thus, all remaining candidates contain at least the first node of the dependency graph. For iterative methods, this first node always represents the operation itself, that is, it contains the original function. Which parts of the operands are output of that function depends on the properties of the operands:
%
\begin{itemize}
\item[-] If initially, the first column of $B$ is known ($\text{\ttfamily FirstColumnInput}[B]$), $B_L$ and $b_R$ are output of the function.
\item[-] If $B$ is initially unknown ($\text{\ttfamily Output}[B]$), just $B_L$ is output.
\end{itemize}
%
Consider nonsymmetric CG as an example: The operation of the first node is
%
\begin{gather*}
\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} :=  \text{CG} \left(A, R_L e_0, X_L e_0 \right) \text{.}
\end{gather*}
%
The first columns of $R$ and $X$, here denoted by $R_L e_0$ and $X_L e_0$, are initially known and the function computes $R_L$, $r_R$, $X_L$ and $x_R$. In contrast, the property of $P$ is $\text{\ttfamily Output}[P]$, and only $P_L$ is computed. Intuitively, the reason is that the function always computes the same number of columns of all operands. If initially, one column of an operand is already known, in the end, one additional column is obtained.

Since only the first node is guaranteed to be part of the loop invariant, only those parts computed in the first node are guaranteed to be known at the beginning and at the end of the loop. Thus, for $\text{\ttfamily FirstColumnInput}[B]$, checking the norm of $b_R$ in the loop guard is always possible, but using $b_+$ is not.

While there are variants which compute $b_+$ (or $b_R$ in case of $\text{\ttfamily Output}[B]$), and those would allow different loop guards, we refrain from using those to keep the derivation simple. This, however, means that if $b_+$ (or $b_R$) are computed, they will not be regarded as part of the solution. That is possible because $B$ does not have a fixed size. Unfortunately, this has the effect that some algorithms compute results that are subsequently discarded. If, however, the loop body of those algorithms has a reduced computational complexity, this is an acceptable tradeoff.

We demonstrate how the second condition for the feasibility of loop invariants is checked using the following loop invariant candidate $P_\text{inv}$ as an example:
%
\begin{align*}
\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} &:= \text{CG} \left(A, R_L e_0, X_L e_0 \right) \\
u_{TR} &:= - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R
\end{align*}
%
The loop guard $G$ is $\| r_R \| \geq \varepsilon$, so its negation $\neg G$ is $\| r_R \| < \varepsilon$. Now, it has to be checked whether $P_\text{inv}$ and $\neg G$ imply the postcondition $P_\text{post}$:
%
\begin{align*}
A P D &= R \left( \underline{I} - \underline{J}  \right)\\
P \left( I - U \right) &= \underline{R} \\
P D &= X \left( \underline{I} - \underline{J} \right)\\
\| R e_r^T \| &< \varepsilon
\end{align*}
%
To do that, we rewrite $P_\text{inv}$ as
%
\begin{align*}
A P_L D_{TL} &= \myFlaOneByTwo{R_L}{r_R} \myFlaTwoByTwo{I - J}{0}{-e_r^T}{1}\\
P_L \left( I - U_{TL} \right) &= R_L \\
P_L D_{TL} &= \myFlaOneByTwo{X_L}{x_R} \myFlaTwoByTwo{I - J}{0}{-e_r^T}{1} \\
u_{TR} &= - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \text{.}
\end{align*}
%
Clearly, by rewriting $\myFlaOneByTwo{R_L}{r_R}$ as $R$, $P_L$ as $P$ and so on, one can see that the first three equations above are the same as the equations in the postcondition. Furthermore, the negation of the loop guard, $\| r_R \| < \varepsilon$, refers to the last column of $\myFlaOneByTwo{R_L}{r_R}$, just like $\| R e_r^T \| < \varepsilon$ refers to the last column of $R$. Thus, $P_\text{inv} \land \neg G$ implies the postcondition. While $P_\text{inv} \land \neg G$ also implies the equation
%
$$u_{TR} = - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \text{,}$$
%
this equation is not needed to render the postcondition true. In the algorithm, $u_{TR}$ will be discarded.

Note that the initial partitioning exposed one additional column of each operand. $R$ for example was partitioned into $\myFlaOneByThree{R_L}{r_R}{r_+}$. In the postcondition, $R$ just consists of $R_L$ and $r_R$. As mentioned before, this is possible because the number of columns of $R$ is variable.


%\todo{mention postprocessing? it's not possible at least if norm is checked, because it may increase}

%Even if we used different loop guards for those algorithms, there inevitable are some 

%Some of the algorithms that can be  Since the size of $B$ is fixed, 

%\todo{fictitious example again?}

%To illustrate this, let us consider again the fictitious example of the algorithm that computes $B$, $C$ and $D$. Let us assume the goal is to minimize the norm of $b_k$, and $B$ is initially partially known. Thus, the loop guard is $\| b_R \| \geq \varepsilon$. The sequence below now shows the last two iterations.
%%
%$$\ldots b_i \; c_i \; d_i \; b_{i+1} \; c_{i+1} \; d_{i+1} \; b_{i+2} $$
%%
%The algorithms derived from the loop invariant that only contains the first node computes $c_1 \; d_1 \; b_2$ in each iteration.




%Let us assume that the update derived from the first node starts with computing $b_i$. Then, the update derived from the full set starts with computing $b_{i+1}$. Thinking of the sequence of quantities again, one can say the update is shifted by a full iteration. While this might seem a bit unintuitive, algorithms derived from the full set also require preprocessing.

\subsection{Algorithm Construction}

The algorithm is constructed in the third step. In addition to the update, preprocessing operations have to be determined. Furthermore, in line with different rules for the initial partitioning, the repartitioning is modified. The process of identifying the update operations does not change.

\subsubsection{Preprocessing}

%To understand why some preprocessing might be necessary, one has to know what kind of algorithms are derived with the presented approach. For iterative methods, there is usually a (partial) ordering in which parts of different operands can be computed. Let us assume there are three matrices $B$, $C$ and $D$. During the execution of the algorithm, the following sequence is computed, where $b_k$, $c_k$ and $c_K$ are columns of $B$, $C$ and $D$, respectively:
%%
%$$\ldots b_i \; c_i \; d_i \; b_{i+1} \; c_{i+1} \; d_{i+1} \ldots$$
%%
%A variant derived from one loop invariant may compute $c_i \; d_i \; b_{i+1}$ in one iteration. In contrast, one derived from another loop invariant may compute $d_i \; b_{i+1} \; c_{i+1}$. Initially, however, only $b_0$ is known, but for the computation of $d_0$, $c_0$ is required. Thus, a preprocessing operation is needed to compute $c_0$ before the loop is entered.
%
%Similarly, to render the postcondition true at the end of the operation, some postprocessing operations would be needed. They can be avoided by performing an additional iteration and discarding those quantities that are not needed. Thus, instead of the loop guard ``$\text{size} \left( \myFlaOneByTwo{B_L}{b_R} \right) < \text{size} \left( \underline{B} \right)$'', we use ``$\text{size} \left( B_L \right) < \text{size} \left( \underline{B} \right)$'' \todo{Where is the best place to talk about the loop guard?}
%
%Due to the nature of iterative methods, the left-hand side of the PME is always the operation itself. It is also the first node of the dependency graph. If the subset contains only the first node, no preprocessing is required. Intuitively, this is the case because the update, in some sense, is aligned with the sequence of computed quantities. That is, the first quantity that is computed in the update is also the first quantity that has to be computed in the sequence of quantities. Conversely, subsets that contain more than the initial node require some preprocessing, because the update computes a different quantity first.
%
%This fact is also reflected in how the preprocessing operations are determined. They are the operations from those nodes that are part of the loop invariant, except for the first nodes, which contains the function of the operation itself. Then, the initial partitioning is applied to those operations, as shown in Table \ref{tab:initialBlockDimensions}, and all expression that are empty are eliminated. In some cases, entire equations might disappear.

As mentioned before, the preprocessing operations are obtained by applying the initial partitioning (Table \ref{tab:initialBlockDimensions}) to all but the initial node contained in the loop invariant. The first node is excluded because when the initial partitioning is applied to it, it always reduces to an expression similar to $k_R := \text{KS} \left(A, k_R \right)$ for the Krylov sequence. Since the output of this function is already known, nothing has to be computed. A FLAME worksheet, extended by the preprocessing, is shown in Figure \ref{fig:ws:emptyP}.

\begin{figure}
\centering
\begin{minipage}[t]{2.35in}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\WSoperation}{ $\ldots$}
	
	\renewcommand{\WSupdate}{\text{Update}}
	
	{
	\worksheetGrayNoNumbersEmptyP
	}
\end{minipage}
\caption{The skeleton of a FLAME worksheet, extended by some preprocessing.}
\label{fig:ws:emptyP}
\end{figure}

\subsubsection{Repartitioning the Operands}

To ensure that the resulting algorithms make progress, the operands have to be repartitioned. The sizes of some operands depend on the number of iterations that is computed, so with every iteration, their sizes have to grow. This is done by adding rows and/or columns in the ``Continue with'' repartitioning. The rules are shown in Table \ref{tab:repartitionIterativeMethods}.

%The operands with variable sizes grow by one row and/or column per iteration. Thus, the ``Continue with'' repartitioning adds

%Recall that one difference compared to direct methods is that some matrices do not have fixes sizes. Some of their dimensions depend on the number of iterations that is performed. Thus, with every iteration, they sizes have to grow, which is done by adding rows and/or columns accordingly. The rules are shown in Table \ref{tab:repartitionIterativeMethods}.
%
\begin{table}[htp]
\begin{center}
\begin{tabular}{ccc}
\toprule
Initial Partitioning	&	Repartition	&	Continue with \\ \midrule
%%%%%%%%
$\myFlaOneByTwoI{B_L}{b_R}$	&
$\myFlaOneByTwoI{B_0}{b_1}$	&
$\myFlaOneByThreeLI{B_0}{b_1}{b_2}$ \\
%%%%%%%%
$\myFlaTwoByOneI{B_T}{b_R}$	&
$\myFlaTwoByOneI{B_0}{b_1}$ 	&
$\FlaThreeByOneTI{B_0}{b_1}{b_2}$ \\
%%%%%%%%
$\myFlaTwoByTwoI{B_{TL}}{b_{TR}}{b_{BL}}{\beta_{BR}}$	&
$\myFlaTwoByTwoI{B_{00}}{b_{01}}{b_{10}}{\beta_{11}}$ &
$\myFlaThreeByThreeTLI	{B_{00}}	{b_{01}}		{b_{02}}
					{b_{10}}	{\beta_{11}}	{\beta_{12}}
					{b_{20}}	{\beta_{21}}	{\beta_{22}}$ \\
%%%%%%%%
$\myFlaOneByThreeI{B_L}{b_R}{b_+}$		&
$\myFlaOneByThreeI{B_0}{b_1}{b_2}$ &
$\myFlaOneByFourTFF{B_0}{b_1}{b_2}{b_3}$\\ \bottomrule
\end{tabular}
\end{center}
\caption{``Repartition'' and ``Continue with'' rules for iterative methods.}
\label{tab:repartitionIterativeMethods}
\end{table}%
%

\subsubsection{Predicates $P_\text{\textrm{before}}$ and $P_\text{\textrm{after}}$}

To obtain the predicates $P_\text{\textrm{before}}$ and $P_\text{\textrm{after}}$, the repartitioned operands are plugged into the loop invariant. The resulting expressions are flattened, using the PME if necessary. What is obtained by applying the ``Repartition'' rules to the loop invariant becomes $P_\text{before}$. Applying the ``Continue with'' partitioning result in $P_\text{after}$.

We demonstrate this for the loop invariant
%
$$\myFlaOneByTwo{ \myFlaOneByTwo{K_L}{k_R} := \text{KS} \left(A, K_L e_0 \right) }{ \neq} \text{.}$$
%
The ``Repartition'' and ``Continue with'' rules for $K$ are shown below, introducing the newly added $k_3$:
%
\begin{align*}
\myFlaOneByThreeI{K_L}{k_R}{k_+} &\rightarrow \myFlaOneByThreeI{K_0}{k_1}{k_2} \\
\myFlaOneByThreeI{K_L}{k_R}{k_+} &\leftarrow \myFlaOneByFourTFF{K_0}{k_1}{k_2}{k_3} \text{.}
\end{align*}
%
Applying the ``Repartition'' rules to the loop invariant yields the following predicate $P_\text{before}$:
%
$$ \myFlaOneByTwo{K_0}{k_1} := \text{KS} \left(A, K_0 e_0 \right)$$
%
Using the ``Continue with'' repartitioning, we obtain the expression
%
$$\myFlaOneByThree{K_0}{k_1}{k_2} := \text{KS} \left(A, K_0 e_0 \right)\text{.}$$
%
To flatten this expression, the PME is used. This results in the following $P_\text{after}$:
%
$$\myFlaOneByTwo{ \myFlaOneByTwo{K_0}{k_1} := \text{KS} \left(A, K_0 e_0 \right) }{ k_2 := A k_1}$$
%
\subsubsection{Finding the Updates}

The difference between $P_\text{\textrm{after}}$ and $P_\text{before}$ now is the update. Identifying the differences is much easier compared to direct methods because they are always entire equations, not subexpression. The reason is that no quantities are updated.

For the Krylov sequence, $k_2 := A k_1$ can easily be identified as the difference between $P_\text{\textrm{after}}$ and $P_\text{before}$.

\subsection{Refinement}
\label{sec:postprocessing}

%\todo{Is this section name confusing? This postprocessing has nothing to do with the preprocessing mentioned earlier. The preprocessing is part of the algorithm, the postprocessing is applied to the algorithm.}

In practice, algorithms would not, and depending on the language, can not be implemented exactly like they are derived with the presented approach. Consider the following three assignments as an example. They are part of the update of one nonsymmetric CG algorithm.
%
\begin{align*}
r_2 &:= r_1 - A p_1 \delta_{11} \\
u_{02} &:= \left(- P_0^T A P_0 \right)^{-1} \cdot P_0^T A r_2 \\
\nu_{12} &:= - \frac{p_1^T A r_2 + p_1^T A P_0 u_{02}}{p_1^T A p_1}
\end{align*}
%
To translate the assignments above to C for example, they have to be decomposed into basic operations that are implemented in a library like BLAS. Since such libraries usually do not include functions for general products of more than two quantities, auxiliary variables have to be introduced.

While the assignments could immediately be translated into Matlab code, this would not result in efficient code. Clearly, some subexpressions appear multiple times, so it is preferable to introduce auxiliary variables for those and compute their values just once. This is referred to as common subexpression elimination. While this concept is well known in the domain of compiler construction \cite{steven1997advanced}, to the author's knowledge there is no research on the elimination of \emph{overlapping} common subexpressions. Overlapping common subexpression are common subexpression that can not be eliminated at the same time. Consider the three terms $P_0^T A P_0$, $p_1^T A r_2$ and $p_1^T A P_0$ for a simple example. $P_0^T A P_0$ and $p_1^T A P_0$ have $AP_0$ in common, $p_1^T A r_2$ and $p_1^T A P_0$ share $p_1^T A$. In $p_1^T A P_0$, those two common subexpression overlap since $A$ is part of both.

Finding a replacement of subexpressions that is optimal in the sense that it has the lowest computational cost is not trivial. Inspecting the problem, one observes that a simplified version of it can be mapped to a \emph{maximum weight matching} problem, which is solvable in polynomial time \cite{edmonds1965maximum}. Again, consider the terms $P_0^T A P_0$, $p_1^T A r_2$ and $p_1^T A P_0$ as an example. Every expression becomes a node in a graph. Every possible replacement of a common subexpression is represented by an edge. The resulting graph is shown in Figure \ref{fig:CSE}.
%
\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=2.5cm and 2.5cm]
%\draw[help lines] (0,-3) grid (7,0);

\node[rect]	(A)							{$P_0^T A P_0$};

\node[rect]	(B)	[below left=of A]	{$p_1^T A r_2$};

\node[rect]	(C)	[below right=of A, xshift=0.0cm]		{$p_1^T A P_0$};

\path[-]	(A)		edge 		node {$AP_0$}		(C);
		
\path[-]	(B)		edge			node {$p_1^T A$}		(C);

\end{tikzpicture}
\caption{Elimination of common subexpressions represented as a graph.}
\label{fig:CSE}
\end{figure}
%
The weight of each edge would be the computational cost of the expression that is replaced. The problem then is to find a set of edges, such that each node is attached to at most one of the selected edges. At the same time, the sum of the weights should be minimized. The requirement that each node is attached to at most one of the selected edges represents the fact that multiple replacements are not possible because the expressions overlap.

Clearly, the actual problem is more complex. A common subexpression might be replaced in more than two expressions. A graph representing this is a hypergraph. In addition to that, in longer expressions, some subexpressions do not overlap, so they can both be replaced. 

In practice, trying to solve this problem is probably not necessary. The expressions encountered in most iterative methods are rarely as complex as in the example above. Furthermore, simple heuristics may already produce good results. In products of more than two quantities, matrix-vector products should always be computed first. Then, if one subexpression is replaced, all other occurrences of this expression should be replaced as well.

Alternatively, to generate as many variants as possible, all possible replacements could be constructed.

%\newpage
%
%There are several subexpression that appear multiple times. $A p_1$ for example appears twice, and $p_1^T A$ appears three times. To avoid redundant computations, auxiliary variables are introduces for those expression. Their value is computed once, and every occurrence of that expression is replaced by the auxiliary variable. This procedure is referred to as common subexpression elimination.
%
%Finding such a replacement that is optimal in the sense that the computational complexity is reduced to a minimum is not trivial. The reason is that in many cases, the elimination of two subexpression is mutually exclusive. In the example above, $A r_2$ and $P_0^T A$ appear twice, indicating that auxiliary variables should be introduced for both expression. However, because of $P_0^T A r_2$, if one is replaced, the other does not appear twice anymore. Furthermore, if $A r_2$ is replaced, there is one occurrence less of $p_1^T A$.
%
%\todo{matrix vector products first?}

\section{Example: Nonsymmetric CG}
\label{sec:derivationOfAlgorithmsExample}

In this section, as a more elaborate example, we will show how the derivation of an algorithm for nonsymmetric CG proceeds.

\subsubsection{PME Generation}

The derivation of some properties for nonsymmetric CG was already shown in Section \ref{sec:derivationOfPropertiesExample} and will not be repeated here. The postcondition is shown below:
%
\begin{align*}
A P D &= R \left( \underline{I} - \underline{J}  \right)\\
P \left( I - U \right) &= \underline{R} \\
P D &= X \left( \underline{I} - \underline{J} \right)\\
\| R e_r^T \| &< \varepsilon
\end{align*}
%
The initial partitioning is determined based on the properties of the operands. It is applied to the postcondition as well as all derived properties. The expression that is obtained from the postcondition is flattened, yielding
%
\begin{gather*}
\myFlaOneByTwo{A P_L D_{TL} = R_L \left( I - J \right) - r_R e_r^T}{ A p_R \delta_{BR} = r_R - r_+ } \\
\myFlaOneByTwo{P_L \left( I - U_{TL} \right) = R_L}{ - P_L u_{TR} + p_R = r_R} \\
\myFlaOneByTwo{ P_L D_{TL} = X_L \left( I - J \right) - x_R e_r^T}{ p_R \delta_{BR} = x_R - x_+ } \text{.}
\end{gather*}
%
The left-hand side is now matched by the CG function, so
%
\begin{gather*}
\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} :=  \text{CG} \left(A, R_L e_0, X_L e_0 \right)
\end{gather*}
%
is obtained. Thus, all quantities on the left-hand side of that assignment are considered to be known. To find assignments for the remaining unknown quantities, the three equations on the right have to be solved. One of the derived properties is that $P^T A P$ is lower triangular, so $P_L^T A P_L$ is lower triangular as well and $P_L^T A p_R$ is zero. The system would recognize that both $P_L$ and $p_R$ appear in $- P_L u_{TR} + p_R = r_R$ on the left-hand side of products. Hence, $P_L^T A$ is multiplied from the left to both sides of the equation to recreate those properties. The resulting equation $- P_L^T A P_L u_{TR} = P_L^T A r_R$ contains only one unknown quantity, so it is solvable. Since $P_L^T A P_L$ is lower triangular, a triangular system is identified, which can also be written as 
%
$$u_{TR} := - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \text{.}$$
%
Having found an assignment for $u_{TR}$, it is considered known as well. By instead using that $R^T A P$ is lower triangular, a different equation for $u_{TR}$ would have been found, resulting in a different algorithm. In practice, two separate derivation processes would be executed for both variants; here we continue just with the first one.

Now, there is only one unknown quantity left in $- P_L u_{TR} + p_R = r_R$, so the following formula is determined for $p_R$:
%
$$p_R := r_R + P_L u_{TR}$$
%
Because $r_R^T r_+$ is zero, $r_R^T$ is multiplied from the left to both sides of $A p_R \delta_{BR} = r_R - r_+$. There is only one unknown in the resulting equation $r_R^T A p_R \delta_{BR} = r_R^T r_R$, so another assignment is found:
%
$$\delta_{BR} := \frac{r_R^T r_R}{r_R^T A p_R}$$
%
Alternatively, the fact that $P^T R$ is lower triangular and rectangular could be used. Finally, $A p_R \delta_{BR} = r_R - r_+$ and $p_R \delta_{BR} = x_R - x_+$ can be solved to $r_+$ and $x_+$, respectively, completing the PME:
%
\begin{align*}
\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} &:= \text{CG} \left(A, R_L e_0, X_L e_0 \right) \\
u_{TR} &:= - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \\
p_R &:= r_R + P_L u_{TR} \\
\delta_{BR} &:= \frac{r_R^T r_R}{r_R^T A p_R} \\
r_+ &:= r_R - A p_R \delta_{BR} \\
x_+ &:= x_R - p_R \delta_{BR}
\end{align*}
%
\subsubsection{Loop Invariant Identification}

Based on this PME, the dependency graph is constructed. Since there are six assignments in the PME, there are six nodes:
%
\begin{enumerate}
\item $\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} := \text{CG} \left(A, R_L e_0, X_L e_0 \right)$
\item $u_{TR} := - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R$
\item $p_R := r_R + P_L u_{TR}$
\item $\delta_{BR} := \frac{r_R^T r_R}{r_R^T A p_R}$
\item $r_+ := r_R - A p_R \delta_{BR}$
\item $x_+ := x_R - p_R \delta_{BR}$
\end{enumerate}
%
The corresponding dependency graph is shown in Figure \ref{fig:dg:nonSymCG}.
%
\begin{figure}[]
\centering
\begin{center}
\begin{tikzpicture}
%\draw[help lines] (0,-3) grid (7,0);

\node[default]	(R1)				{1};

\node[default]	(U1)	[below=of R1, yshift=-0.5cm]	{2};

\node[default]	(P1)	[below=of U1, yshift=-0.5cm]	{3};

\node[default]	(D1)	[below=of P1, yshift=-0.5cm]	{4};

\node[default]	(R2)	[below=of D1, yshift=-0.5cm, xshift=-1cm]	{5};

\node[default]	(X2)	[below=of D1, yshift=-0.5cm, xshift=1cm]	{6};


\path[->]	(R1)		edge							(U1)
		(R1)		edge		[bend right=30]			(P1)
		(R1)		edge		[bend right=30]			(D1)
		(R1)		edge		[bend right=30]			(R2);
		
\path[->]	(U1)		edge							(P1);

\path[->]	(P1)		edge							(D1)
		(P1)		edge		[bend right=20]			(R2)
		(P1)		edge		[bend left=20]			(X2);

\path[->]	(D1)		edge							(R2)
		(D1)		edge							(X2);

\end{tikzpicture}
\caption{Dependency graph for nonsymmetric CG.}
\label{fig:dg:nonSymCG}
\end{center}
\end{figure}
%
All nonempty subsets of this graph that respect the dependencies are feasible loop invariants. Thus, the following seven loop invariants are obtained:
%
$$\{1\}, \{1, 2\}, \{1, 2, 3\}, \{1, 2, 3, 4\}, \{1, 2, 3, 4, 5\}, \{1, 2, 3, 4, 6\}, \{1, 2, 3, 4, 5, 6\}$$
%
%\begin{enumerate}
%\item $\{1\}$
%\item $\{1, 2\}$
%\item $\{1, 2, 3\}$
%\item $\{1, 2, 3, 4\}$
%\item $\{1, 2, 3, 4, 5\}$
%\item $\{1, 2, 3, 4, 6\}$
%\item $\{1, 2, 3, 4, 5, 6\}$
%\end{enumerate}
%
To determine the loop guard $G$, the additional predicate in the postcondition is inspected. It is $\| R e_r^T \| < \varepsilon$. Since the precondition contains $\text{\ttfamily FirstColumnInput}[R]$, according to Table \ref{tab:loopGuards}, the loop guard $\| r_R \| \geq \varepsilon$ is selected.

In this example, we show the derivation for the set $\{1, 2, 3\}$, so the loop invariant is
%
\begin{align*}
P_\text{inv} = \big\{ &\left\{ \myFlaOneByTwo{R_L}{r_R}, U_{TL}, P_L, D_{TL}, \myFlaOneByTwo{X_L} {x_R} \right\} := \text{CG} \left(A, R_L e_0, X_L e_0 \right) \land \\
&\:u_{TR} := - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \land \\
&\:p_R := r_R + P_L u_{TR} \big\}\text{.}
\end{align*}
%
\subsubsection{Algorithm Construction}
%
In a first step, the preprocessing operations are determined. The relevant assignments are the ones contained in the loop invariant, except for the one obtained from the first node. They are shown below:
%
\begin{align*}
u_{TR} &:= - \left( P_L^T A P_L \right)^{-1} P_L^T A r_R \\
p_R &:= r_R + P_L u_{TR}
\end{align*}
%
According to the initial partitioning, $P_L$ is empty, that is, it has the size $n \times 0$. Consequently, the right-hand side of the first assignment is empty, too, so it disappears. The second assignment reduces to
%
$$p_R := r_R \text{,}$$
%
which is the only preprocessing operation. Next, the update is derived. The ``Repartition'' rules are determined using Table \ref{tab:repartitionIterativeMethods}:
%
\begin{align*}
\myFlaOneByThreeI{R_L}{r_R}{r_+} &\rightarrow \myFlaOneByThreeI{R_0}{r_1}{r_2} &
\myFlaOneByThreeI{X_L}{x_R}{x_+} &\rightarrow \myFlaOneByThreeI{X_0}{x_1}{x_2}\\
\myFlaTwoByTwoI{U_{TL}}{u_{TR}}{0}{0} &\rightarrow \myFlaTwoByTwoI{U_{00}}{u_{01}}{0}{0} &
\myFlaTwoByTwoI{D_{TL}}{0}{0}{\delta_{BR}} &\rightarrow \myFlaTwoByTwoI{D_{00}}{0}{0}{\delta_{11}} \\
\myFlaOneByTwoI{P_L}{p_R} &\rightarrow \myFlaOneByTwoI{P_0}{p_1} &&
\end{align*}
%
Applying those rules to the loop invariant yields the following predicate $P_\text{before}$:
%
\begin{align*}
P_\text{before} = \big\{ &\left\{ \myFlaOneByTwo{R_0}{r_1}, U_{00}, P_0, D_{00}, \myFlaOneByTwo{X_0} {x_1} \right\} := \text{CG} \left(A, R_0 e_0, X_0 e_0 \right) \land \\
&\:u_{01} := - \left( P_0^T A P_0 \right)^{-1} P_0^T A r_1 \land \\
&\:p_1 := r_1 + P_0 u_{01} \big\}
\end{align*}
%
The ``Continue with'' repartitioning is
%
\begin{align*}
\myFlaOneByThreeI{R_L}{r_R}{r_+} &\leftarrow \myFlaOneByFourTFF{R_0}{r_1}{r_2}{r_3} \\
\myFlaOneByThreeI{X_L}{x_R}{x_+} &\leftarrow \myFlaOneByFourTFF{X_0}{x_1}{x_2}{x_3} \\
\myFlaTwoByTwoI{U_{TL}}{u_{TR}}{0}{0} &\leftarrow
\myFlaThreeByThreeTLI	{U_{00}}	{u_{01}}	{u_{02}}
					{0}		{0}		{\nu_{12}}
					{0}		{0}		{0} \\
\myFlaTwoByTwoI{D_{TL}}{0}{0}{\delta_{BR}} &\leftarrow
\myFlaThreeByThreeTLI	{D_{00}}	{0}			{0}
					{0}		{\delta_{11}}	{0}
					{0}		{0}			{\delta_{22}}\\
\myFlaOneByTwoI{P_L}{p_R} &\leftarrow \myFlaOneByThreeLI{P_0}{p_1}{p_2} \text{.}
\end{align*}
%
Plugging that into the function, the following expression is obtained:
%
\begin{align*}
&\left\{ \myFlaOneByThree{R_0}{r_1}{r_2}, \myFlaTwoByTwoI{U_{00}}{u_{01}}{0}{0}, \myFlaOneByTwo{P_0}{p_1}, \right. \\&
\qquad \qquad \qquad \; \left. \myFlaTwoByTwoI{D_{00}}{0}{0}{\delta_{11}}, \myFlaOneByThree{X_0} {x_1}{x_2} \right\} := \text{CG} \left(A, R_0 e_0, X_0 e_0 \right)
\end{align*}
%
It it flattened by using the PME, resulting in six assignments:
%
\begin{align*}
\left\{ \myFlaOneByTwo{R_0}{r_1}, U_{00}, P_0, D_{00}, \myFlaOneByTwo{X_0}{x_1} \right\} &:= \text{CG} \left(A, R_0 e_0, X_0 e_0 \right) \\
u_{01} &:= - \left( P_0^T A P_0 \right)^{-1} P_0^T A r_1 \\
p_1 &:= r_1 + P_0 u_{01} \\
\delta_{11} &:= \frac{r_1^T r_1}{r_1^T A p_1} \\
r_2 &:= r_1 - A p_1 \delta_{11} \\
x_2 &:= x_1 - p_1 \delta_{11}
\end{align*}
%
For the second assignment of the loop invariant, the PME of a lower triangular system is needed (see Section \ref{sec:triLS}).
%
\begin{align*}
&\myFlaTwoByOne{u_{02}}{\nu_{12}}:= - \myFlaTwoByTwo{P_0^T A P_0}{0}{p_1^T A P_0}{p_1^T A p_1}^{-1} \myFlaTwoByOne{P_0^T A r_2}{p_1^T A r_2} \\
%
&\Rightarrow \left\{
\begin{aligned}
u_{02} &:= \left(- P_0^T A P_0 \right)^{-1} \cdot P_0^T A r_2 \\
\nu_{12} &:= - \frac{p_1^T A r_2 + p_1^T A P_0 u_{02}}{p_1^T A p_1}
\end{aligned} \right.
\end{align*}
%
The third equation, $p_1 := r_1 + P_0 u_{01}$, becomes $p_2 := r_2 + P_0 u_{02} + p_1 \nu_{12}$ after the application of the ``Continue with'' partitioning. Now that the complete predicate $P_\text{after}$ is determined, the update is found by comparing it to $P_\text{before}$. The assignments that are contained in $P_\text{after}$, but not in $P_\text{before}$, constitute the update. They are shown below.
%
\begin{align*}
\delta_{11} &:= \frac{r_1^T r_1}{r_1^T A p_1} \\
r_2 &:= r_1 - A p_1 \delta_{11} \\
x_2 &:= x_1 - p_1 \delta_{11} \\
u_{02} &:= \left(- P_0^T A P_0 \right)^{-1} \cdot P_0^T A r_2 \\
\nu_{12} &:= - \frac{p_1^T A r_2 + p_1^T A P_0 u_{02}}{p_1^T A p_1} \\
p_2 &:= r_2 + P_0 u_{02} + p_1 \nu_{12}
\end{align*}

\subsubsection{Postprocessing}

For this example, we use the heuristics explained in Section \ref{sec:postprocessing} for the elimination of common subexpressions. The first expression that is identified is $A p_1$. The auxiliary variable $t_1 := A p_1$ is introduced and all occurrences of $A p_1$ are replaced with $t_1$. The resulting assignments are shown below.
%
\begin{align*}
t_1 &:= A p_1 \\
\delta_{11} &:= \frac{r_1^T r_1}{r_1^T t_1} \\
r_2 &:= r_1 - t_1 \delta_{11} \\
x_2 &:= x_1 - p_1 \delta_{11} \\
u_{02} &:= \left(- P_0^T A P_0 \right)^{-1} \cdot P_0^T A r_2 \\
\nu_{12} &:= - \frac{p_1^T A r_2 + p_1^T A P_0 u_{02}}{p_1^T t_1} \\
p_2 &:= r_2 + P_0 u_{02} + p_1 \nu_{12}
\end{align*}
%
Further auxiliary variables are introduced for $A r_2$ and $P_0 u_{02}$. The update that is obtained at the end of this step is shown in the filled out worksheet in Figure \ref{fig:ws:nonsymCG}. Some of the partitionings and the loop invariant are omitted in the interest of legibility.
%
\begin{figure}
\centering
\begin{minipage}[t]{4.5in}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\ALGroutinename}{ nonsymmetric CG}
	
	\renewcommand{\WSprecondition}{}
	
	\renewcommand{\WSpartition}{
        $
        R \rightarrow
		\myFlaOneByThreeI{R_L}{r_R}{r_+}
	$
        }

	\renewcommand{\WSpartitionsizes}{
		$ R_{L} $ is $ n \times 0 $
	}
	
	\renewcommand{\WSpreprocessing}{
		$p_R := r_R$
	}
	
	\renewcommand{\WSguard}{ $\| r_R \| \geq \varepsilon$ }
	
	
	\renewcommand{\WSrepartition}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \rightarrow \myFlaOneByThreeI{R_0}{r_1}{r_2}$
	}
	
	\renewcommand{\WSrepartitionsizes}{
	$A$ is $k \times k$
	}

	
	\renewcommand{\WSupdate}{
		$\begin{aligned}
		t_1 &:= A p_1 \\
		\delta_{11} &:= \frac{r_1^T r_1}{r_1^T t_1} \\
		r_2 &:= r_1 - t_1 \delta_{11} \\
		x_2 &:= x_1 - p_1 \delta_{11} \\
		t_2 &:= A r_2 \\
		u_{02} &:= \left(- P_0^T A P_0 \right)^{-1} \cdot P_0^T t_2 \\
		t_3 &:= P_0 u_{02} \\
		\nu_{12} &:= - \frac{p_1^T t_2 + p_1^T A t_3}{p_1^T t_1} \\
		p_2 &:= r_2 + t_3 + p_1 \nu_{12}
		\end{aligned}$
	}
	
	\renewcommand{\WSmoveboundary}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \leftarrow \myFlaOneByFourTFF{R_0}{r_1}{r_2}{r_3}$
	}
	
	\renewcommand{\WSpostcondition}{
	
	}
	
	{
	\FlaAlgorithmIter
	}
\end{minipage}
\caption{Worksheet for a nonsymmetric CG algorithm.}
\label{fig:ws:nonsymCG}
\end{figure}
%
\section{Example: BiCG}
\label{sec:BiCGexample}

As a second example, we show the derivation of two algorithms for the biconjugate gradient method (BiCG). Since we already showed a full example in the previous section, we now proceed at a slightly higher pace. Pre- and postcondition are shown below.
%
\begin{align*}
P_\text{pre}: \{ &\text{\ttfamily Input}[A] \land \text{\ttfamily Matrix}[A] \land \text{\ttfamily NonSingular}[A] \land \\
		&\text{\ttfamily Output}[P] \land \text{\ttfamily Matrix}[P] \land \\
		&\text{\ttfamily Output}[\tilde{P}] \land \text{\ttfamily Matrix}[\tilde{P}] \land \\
		&\text{\ttfamily Output}[D] \land \text{\ttfamily Matrix}[D] \land \text{\ttfamily Diagonal}[D] \land \\
		&\text{\ttfamily FirstColumnInput}[R] \land \text{\ttfamily Matrix}[R] \land \\
		&\text{\ttfamily FirstColumnInput}[\underline{R}] \land \text{\ttfamily Matrix}[\underline{R}]  \land \\
		&\text{\ttfamily FirstColumnInput}[\tilde{R}] \land \text{\ttfamily Matrix}[\tilde{R}] \land  \\
		&\text{\ttfamily FirstColumnInput}[\underline{\tilde{R}}] \land \text{\ttfamily Matrix}[\underline{\tilde{R}}] \land \\
		&\text{\ttfamily Diagonal}[R^T \tilde{R}] \land \text{\ttfamily Diagonal}[\underline{R}^T \underline{\tilde{R}}] \land \\
		&\text{\ttfamily DiagonalR}[\underline{R}^T \tilde{R}] \land \text{\ttfamily DiagonalR}[R^T \underline{\tilde{R}}] \land \\
		&\text{\ttfamily FirstColumnInput}[X] \land \text{\ttfamily Matrix}[X] \land \\
		&\text{\ttfamily Output}[U] \land \text{\ttfamily Matrix}[U] \land \text{\ttfamily UpperDiagonal}[U] \land \\
		& \text{\ttfamily Matrix}[\underline{I} - \underline{J}] \land \text{\ttfamily LowerTrapezoidal}[\underline{I} - \underline{J}] \}
\end{align*}
%
\begin{align*}
P_\text{post}:	\{ 	&APD = R \left( \underline{I} - \underline{J} \right) \\
				&A^T \tilde{P}D = \tilde{R} \left( \underline{I} - \underline{J} \right) \\
				&P \left( I - U \right) = \underline{R} \\
				&\tilde{P} \left( I - U \right) = \underline{\tilde{R}} \\
				&PD = X \left( \underline{I} - \underline{J} \right) \\
				&\| R e_r^T \| < \varepsilon \}
\end{align*}
%
Following from the precondition, the function representing the operation is
%
\begin{align*}
\left\{ R, \tilde{R}, U, P, \tilde{P}, D, X \right\} &:= \text{BiCG} \left(A, R e_0, X e_0 \right) \text{.}
\end{align*}
%

\subsubsection{Derivation of Properties}

For BiCG, neither $R$ nor $\tilde{R}$ is orthogonal. Instead, they are mutually orthogonal, which means that $R^T \tilde{R}$ is diagonal. Except for that, the derivation of properties is mostly the same to the one shown in the example in Section \ref{sec:derivationOfPropertiesExample}. For this reason, it will not be shown here. Instead. we give a short overview of the relevant properties: 
%
\begin{itemize}
\item[-] $\underline{\tilde{R}}^T A P$ and $\underline{R}^T A^T \tilde{P}$ are lower triangular.
\item[-] $\tilde{P}^T A P$ (and thus $P^T A^T \tilde{P}$) is diagonal.
\item[-] $P^T \tilde{R}$ and $\tilde{P}^T R$ are lower triangular and rectangular.
\end{itemize}
%
%
%
%The following six expressions are lower triangular:
%%
%\begin{align*}
%&\underline{R}^T A P					&&\underline{\tilde{R}}^T A P \\
%&\underline{\tilde{R}}^T A^T \tilde{P}		&&\underline{R}^T A^T \tilde{P} \\
%&P^T A P							&&\tilde{P}^T A^T \tilde{P}
%\end{align*}
%%
%$\tilde{P}^T A P$ (and thus $P^T A^T \tilde{P}$) is diagonal. The expressions below are lower triangular and rectangular.
%%
%\begin{align*}
%&P^T R					&&P^T \tilde{R} \\
%&\tilde{P}^T R				&&\tilde{P}^T \tilde{R}
%\end{align*}
%%
\subsubsection{PME Generation}

As a first step towards the PME, the operands are partitioned. Initially, only the fist columns of $R$ and $\tilde{R}$ are known, so they are partitioned into \smash{$\myFlaOneByThree{R_L}{r_R}{r_+}$} and \smash{$\myFlaOneByThree{\tilde{R}_L}{\tilde{r}_R}{\tilde{r}_+}$}, respectively. The remaining operands are partitioned accordingly:
%
\begin{align*}
A \myFlaOneByTwo{P_L}{p_R} \myFlaTwoByTwo{D_{TL}}{0}{0}{\delta_{BR}} &= \myFlaOneByThree{R_L}{r_R}{r_+} \myFlaThreeByTwo{I - J}{0}{-e_r^T}{1}{0}{-1} \\
A^T \myFlaOneByTwo{\tilde{P}_L}{\tilde{p}_R} \myFlaTwoByTwo{D_{TL}}{0}{0}{\delta_{BR}} &= \myFlaOneByThree{\tilde{R}_L}{\tilde{r}_R}{\tilde{r}_+} \myFlaThreeByTwo{I - J}{0}{-e_r^T}{1}{0}{-1} \\
\myFlaOneByTwo{P_L}{p_R}\myFlaTwoByTwo{I- U_{TL}}{- u_{TR}}{0}{1} &= \myFlaOneByTwo{R_L}{r_R} \\
\myFlaOneByTwo{\tilde{P}_L}{\tilde{p}_R}\myFlaTwoByTwo{I- U_{TL}}{- u_{TR}}{0}{1} &= \myFlaOneByTwo{\tilde{R}_L}{\tilde{r}_R} \\
\myFlaOneByTwo{P_L}{p_R} \myFlaTwoByTwo{D_{TL}}{0}{0}{\delta_{BR}} &= \myFlaOneByThree{X_L}{x_R}{x_+} \myFlaThreeByTwo{I - J}{0}{-e_r^T}{1}{0}{-1}
\end{align*}
%
After the execution of the Matrix Arithmetic step, those expressions become
%
\begin{gather*}
\myFlaOneByTwo{A P_L D_{TL} = R_L \left( I - J \right) - r_R e_r^T}{ A p_R \delta_{BR} = r_R - r_+ } \\
\myFlaOneByTwo{A^T \tilde{P}_L D_{TL} = \tilde{R}_L \left( I - J \right) - \tilde{r}_R e_r^T}{ A \tilde{p}_R \delta_{BR} = \tilde{r}_R - \tilde{r}_+ } \\
\myFlaOneByTwo{P_L \left( I - U_{TL} \right) = R_L}{ - P_L u_{TR} + p_R = r_R} \\
\myFlaOneByTwo{\tilde{P}_L \left( I - U_{TL} \right) = \tilde{R}_L}{ - \tilde{P}_L u_{TR} + \tilde{p}_R = \tilde{r}_R} \\
\myFlaOneByTwo{ P_L D_{TL} = X_L \left( I - J \right) - x_R e_r^T}{ p_R \delta_{BR} = x_R - x_+ }\text{.}
\end{gather*}
%
The left-hand sides of those expressions can now be replaced by the BiCG function:
%
\begin{align*}
\left\{ \myFlaOneByTwo{R_L}{r_R}, \myFlaOneByTwo{\tilde{R}_L}{\tilde{R}_R}, U_{TL}, P_L, \tilde{P}_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} &:= \text{BiCG} \left(A, R_L e_0, X_L e_0 \right)
\end{align*}
%
Updates for the remaining unknown quantities are obtained by solving the equations on the right-hand using the derived properties. For this example, to find an assignment for $u_{TR}$, we use that $\tilde{P}^T A P$ is diagonal. Multiplying $\tilde{P}_L^T A$ from the left to both sides of $- P_L u_{TR} + p_R = r_R$ results in $- \tilde{P}_L^T A P_L u_{TR} = \tilde{P}_L^T A r_R$. Since $\tilde{P}_L^T A P_L$ is diagonal, and thus is invertible, the following assignment is obtained:
%
$$u_{TR} := - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R$$
%
Note that it would have been also possible to use $- \tilde{P}_L u_{TR} + \tilde{p}_R = \tilde{r}_R$ to get to an assignment for $u_{TR}$.  Now, the following assignments are obtained for $p_R$ and $\tilde{p}_R$:
%
\begin{align*}
p_R &:= r_R +  P_L u_{TR} \\
\tilde{p}_R &:= \tilde{r}_R + \tilde{P}_L u_{TR}
\end{align*}
%
Similarly to $u_{TR}$, $\delta_{BR}$ can be computed in several different ways. Here, we use that $\tilde{P}^T R$ is lower triangular and rectangular, in combination with the equation $A p_R \delta_{BR} = r_R - r_+$. Multiplying $\tilde{p}_R^T$ from the left and solving to $\delta_{BR}$ yields
%
$$\delta_{BR} := \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R}$$
%
Finally, the following assignments are derived for $r_+$, $\tilde{r}_+$ and $x_+$:
%
\begin{align*}
r_+ &:= r_R -  A p_R \delta_{BR} \\
\tilde{r}_+ &:= \tilde{r}_R - A \tilde{p}_R \delta_{BR} \\
x_+ & := x_R -  p_R \delta_{BR}
\end{align*}
%
The complete PME is shown below (already in form of a list, in anticipation of the construction of the dependency graph):
%
%\begin{align*}
%\left\{ \myFlaOneByTwo{R_L}{r_R}, \myFlaOneByTwo{\tilde{R}_L}{\tilde{R}_R}, U_{TL}, P_L, \tilde{P}_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} &:= \text{BiCG} \left(A, R_L e_0, X_L e_0 \right) \\
%u_{TR} &:= - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R \\
%p_R &:= r_R +  P_L u_{TR} \\
%\tilde{p}_R &:= \tilde{r}_R + \tilde{P}_L u_{TR} \\
%\delta_{BR} &:= \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R} \\
%r_+ &:= r_R -  A p_R \delta_{BR} \\
%\tilde{r}_+ &:= \tilde{r}_R - A \tilde{p}_R \delta_{BR} \\
%x_+ & := x_R -  p_R \delta_{BR}
%\end{align*}

\begin{enumerate}
\item $\left\{ \myFlaOneByTwo{R_L}{r_R}, \myFlaOneByTwo{\tilde{R}_L}{\tilde{R}_R}, U_{TL}, P_L, \tilde{P}_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} := \text{BiCG} \left(A, R_L e_0, X_L e_0 \right)$
%
\item $u_{TR} := - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R$
%
\item $p_R := r_R +  P_L u_{TR}$
%
\item $\tilde{p}_R := \tilde{r}_R + \tilde{P}_L u_{TR}$
%
\item $\delta_{BR} := \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R}$
%
\item $r_+ := r_R -  A p_R \delta_{BR}$
%
\item $\tilde{r}_+ := \tilde{r}_R - A \tilde{p}_R \delta_{BR}$
%
\item $x_+  := x_R -  p_R \delta_{BR}$
\end{enumerate}

\subsubsection{Loop Invariant Identification}

The dependency graph is shown in Figure \ref{fig:dg:BiCG}.
%
\begin{figure}[]
\centering
\begin{center}
\begin{tikzpicture}
%\draw[help lines] (0,-3) grid (7,0);

\node[default]	(R1)				{1};

\node[default]	(U1)	[below=of R1, yshift=-0.3cm]	{2};

\node[default]	(P1)	[below left=of U1, yshift=-0.3cm]	{3};

\node[default]	(PT1)	[below right=of U1, yshift=-0.3cm]	{4};

\node[default]	(D1)	[below right=of P1, yshift=-0.3cm]	{5};

\node[default]	(R2)	[below left=of D1, yshift=-0.3cm, xshift=-0cm]	{6};

\node[default]	(RT2)	[below right=of D1, yshift=-0.3cm, xshift=0cm]	{7};

\node[default]	(X2)	[below=of D1, yshift=-0.3cm, xshift=0cm]	{8};


\path[->]	(R1)		edge							(U1)
		(R1)		edge		[bend right=30]			(P1)
		(R1)		edge		[bend left=30]			(PT1)
		(R1)		edge		[bend right=25]			(D1)
		(R1)		edge		[bend right=40]			(R2)
		(R1)		edge		[bend left=40]			(RT2);
		
\path[->]	(U1)		edge							(P1)
		(U1)		edge							(PT1);

\path[->]	(P1)		edge							(D1)
		(P1)		edge		[bend left=0]			(R2)
		(P1)		edge		[bend right=0]			(X2);

\path[->]	(PT1)	edge							(D1)
		(PT1)	edge							(RT2);

\path[->]	(D1)		edge							(R2)
		(D1)		edge							(RT2)
		(D1)		edge							(X2);

\end{tikzpicture}
\caption{Dependency graph for BiCG.}
\label{fig:dg:BiCG}
\end{center}
\end{figure}
%
%\begin{figure}[]
%\centering
%\begin{center}
%\begin{tikzpicture}
%%\draw[help lines] (0,-3) grid (7,0);
%
%\node[default]	(R1)				{1};
%
%\node[default]	(U1)	[below=of R1, yshift=-0.5cm]	{2};
%
%\node[default]	(P1)	[below left=of U1, yshift=-0.5cm]	{3};
%
%\node[default]	(PT1)	[below right=of U1, yshift=-0.5cm]	{4};
%
%\node[default]	(D1)	[below right=of P1, yshift=-0.5cm]	{5};
%
%
%
%\node[default]	(RT2)	[below right=of PT1, yshift=-0.5cm, xshift=0cm]	{7};
%
%\node[default]	(X2)	[below left=of P1, yshift=-0.5cm, xshift=0cm]	{8};
%
%\node[default]	(R2)	[left=of X2, yshift=-0.0cm, xshift=-0.5cm]	{6};
%
%\path[->]	(R1)		edge							(U1)
%		(R1)		edge		[bend right=30]			(P1)
%		(R1)		edge		[bend left=30]			(PT1)
%		(R1)		edge		[bend right=20]			(D1)
%		(R1)		edge		[bend right=40]			(R2)
%		(R1)		edge		[bend left=40]			(RT2);
%		
%\path[->]	(U1)		edge							(P1)
%		(U1)		edge							(PT1);
%
%\path[->]	(P1)		edge							(D1)
%		(P1)		edge		[bend left=0]			(R2)
%		(P1)		edge		[bend right=0]			(X2);
%
%\path[->]	(PT1)	edge							(D1)
%		(PT1)	edge							(RT2);
%
%\path[->]	(D1)		edge							(R2)
%		(D1)		edge							(RT2)
%		(D1)		edge							(X2);
%
%\end{tikzpicture}
%\caption{Dependency Graph for BiCG.}
%\label{fig:dg:BiCG}
%\end{center}
%\end{figure}
%
Of all the subsets of this graph that respect the dependencies, only the empty one fails to satisfy the conditions for the feasibility of loop invariant. The remaining 13 subsets are feasible loop invariants. We will continue the derivation in this example with the loop invariant that corresponds to the full set.
%
%\begin{align*}
%P_\text{inv} = \big\{ &\left\{ \myFlaOneByTwo{R_L}{r_R}, \myFlaOneByTwo{\tilde{R}_L}{\tilde{R}_R}, U_{TL}, P_L, \tilde{P}_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} := \text{BiCG} \left(A, R_L e_0, X_L e_0 \right) \land \\
%&u_{TR} := - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R \land
%p_R := r_R +  P_L u_{TR} \land
%\tilde{p}_R := \tilde{r}_R + \tilde{P}_L u_{TR} \land \\
%&\delta_{BR} := \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R} \land
%r_+ := r_R -  A p_R \delta_{BR} \land
%\tilde{r}_+ := \tilde{r}_R - A \tilde{p}_R \delta_{BR} \\
%x_+ & := x_R -  p_R \delta_{BR} \big\}
%\end{align*}

Because of the additional predicate in the postcondition, $\| R e_r^T \| < \varepsilon$, and the property $\text{\ttfamily FirstColumnInput}[R]$ in the precondition, the loop guard $\| r_R \| < \varepsilon$ is determined.

\subsubsection{Algorithm Construction}

The first part of this step consists of finding the preprocessing operations. This is done by taking all assignments of the loop invariant except for the one from the first node, and eliminating all expression that are empty in the initial partitioning. The relevant assignments are shown below:
%
\begin{align*}
u_{TR} &:= - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R \\
p_R &:= r_R +  P_L u_{TR} \\
\tilde{p}_R &:= \tilde{r}_R + \tilde{P}_L u_{TR} \\
\delta_{BR} &:= \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R}
\end{align*}
%
\begin{align*}
r_+ &:= r_R -  A p_R \delta_{BR} \\
\tilde{r}_+ &:= \tilde{r}_R - A \tilde{p}_R \delta_{BR} \\
x_+ & := x_R -  p_R \delta_{BR}
\end{align*}
%
Initially, $P_L$ and $\tilde{P}_L$ have the size $n \times 0$ and $u_{TR}$ is of size $0 \times 1$. As a result, the first assignment is eliminated, while the second and third are reduced to
%
\begin{align*}
p_R &:= r_R \\
\tilde{p}_R &:= \tilde{r}_R \text{.}
\end{align*}
%
The remaining four assignments do not change.

To determine the update, the predicates $P_\text{before}$ and $P_\text{after}$ have to be constructed by repartitioning the loop invariant. The ``Repartition'' rules are shown below:
%
\begin{align*}
\myFlaOneByThreeI{R_L}{r_R}{r_+} &\rightarrow \myFlaOneByThreeI{R_0}{r_1}{r_2} &
\myFlaOneByThreeI{\tilde{R}_L}{\tilde{r}_R}{\tilde{r}_+} &\rightarrow \myFlaOneByThreeI{\tilde{R}_0}{\tilde{r}_1}{\tilde{r}_2}\\
\myFlaTwoByTwoI{U_{TL}}{u_{TR}}{0}{0} &\rightarrow \myFlaTwoByTwoI{U_{00}}{u_{01}}{0}{0} &
\myFlaTwoByTwoI{D_{TL}}{0}{0}{\delta_{BR}} &\rightarrow \myFlaTwoByTwoI{D_{00}}{0}{0}{\delta_{11}} \\
\myFlaOneByTwoI{P_L}{p_R} &\rightarrow \myFlaOneByTwoI{P_0}{p_1} & \myFlaOneByTwoI{\tilde{P}_L}{\tilde{p}_R} &\rightarrow \myFlaOneByTwoI{\tilde{P}_0}{\tilde{p}_1} \\ 
\myFlaOneByThreeI{X_L}{x_R}{x_+} &\rightarrow \myFlaOneByThreeI{X_0}{x_1}{x_2} &&
\end{align*}
%
Applying that to the loop invariant yields the following equations which constitute $P_\text{before}$:
%
\begin{align*}
\left\{ \myFlaOneByTwo{R_0}{r_1}, \myFlaOneByTwo{\tilde{R}_0}{\tilde{R}_1}, U_{00}, P_0, \tilde{P}_0, D_{00}, \myFlaOneByTwo{X_0}{x_1} \right\} &:= \text{BiCG} \left(A, R_0 e_0, X_0 e_0 \right) \\
u_{01} &:= - \left( \tilde{P}_0^T A P_0 \right)^{-1} \tilde{P}_0^T A r_1 \\
p_1 &:= r_1 +  P_0 u_{01} \\
\tilde{p}_1 &:= \tilde{r}_1 + \tilde{P}_0 u_{01} \\
\delta_{11} &:= \frac{\tilde{p}_1^T r_1}{\tilde{p}_1^T A p_1} \\
r_2 &:= r_1 -  A p_1 \delta_{11} \\
\tilde{r}_2 &:= \tilde{r}_1 - A \tilde{p}_1 \delta_{11} \\
x_2 & := x_1 -  p_1 \delta_{11} 
\end{align*}
%
For the ``Continue with'' repartitioning it is important to note that $U$ is upper diagonal. Thus, $u_{02}$ is zero:
%
\begin{align*}
\myFlaOneByThreeI{R_L}{r_R}{r_+} &\leftarrow \myFlaOneByFourTFF{R_0}{r_1}{r_2}{r_3} \\
\myFlaOneByThreeI{\tilde{R}_L}{\tilde{r}_R}{\tilde{r}_+} &\leftarrow \myFlaOneByFourTFF{\tilde{R}_0}{\tilde{r}_1}{\tilde{r}_2}{\tilde{r}_3} \\
\myFlaTwoByTwoI{U_{TL}}{u_{TR}}{0}{0} &\leftarrow
\myFlaThreeByThreeTLI	{U_{00}}	{u_{01}}	{0}
					{0}		{0}		{\nu_{12}}
					{0}		{0}		{0} \\
\myFlaTwoByTwoI{D_{TL}}{0}{0}{\delta_{BR}} &\leftarrow
\myFlaThreeByThreeTLI	{D_{00}}	{0}			{0}
					{0}		{\delta_{11}}	{0}
					{0}		{0}			{\delta_{22}}\\
\myFlaOneByTwoI{P_L}{p_R} &\leftarrow \myFlaOneByThreeLI{P_0}{p_1}{p_2} \\
\myFlaOneByTwoI{\tilde{P}_L}{\tilde{p}_R} &\leftarrow \myFlaOneByThreeLI{\tilde{P}_0}{\tilde{p}_1}{\tilde{p}_2} \\
\myFlaOneByThreeI{X_L}{x_R}{x_+} &\leftarrow \myFlaOneByFourTFF{X_0}{x_1}{x_2}{x_3} \\
\end{align*}
%
This repartitioning transforms
%
$$\left\{ \myFlaOneByTwo{R_L}{r_R}, \myFlaOneByTwo{\tilde{R}_L}{\tilde{R}_R}, U_{TL}, P_L, \tilde{P}_L, D_{TL}, \myFlaOneByTwo{X_L}{x_R} \right\} := \text{BiCG} \left(A, R_L e_0, X_L e_0 \right)$$
%
into those assignments that are also contained in the predicate $P_\text{before}$. Applying it to 
%
$$u_{TR} := - \left( \tilde{P}_L^T A P_L \right)^{-1} \tilde{P}_L^T A r_R \text{,}$$
%
yields
%
$$\myFlaTwoByOne{0}{\nu_{12}} := \myFlaTwoByTwo{\tilde{P}_0^T A P_0}{0}{0}{\tilde{p}_1^T A p_1}^{-1} \myFlaTwoByOne{\tilde{P}_0^T A r_2}{\tilde{p}_1^T A r_2} \text{.}$$
%
Flattening this expression results in the assignment
%
$$\nu_{12} := \frac{\tilde{p}_1^T A r_2}{\tilde{p}_1^T A p_1}$$
%
for $\nu_{12}$. Because of 
%
\begin{align*}
p_R &:= r_R +  P_L u_{TR} \\
\tilde{p}_R &:= \tilde{r}_R + \tilde{P}_L u_{TR} \text{,}
\end{align*}
%
the following two expressions are obtained and added to $P_\text{after}$:
%
\begin{align*}
p_2 &:= r_2 +  p_1 \nu_{12} \\
\tilde{p}_2 &:= \tilde{r}_2 + \tilde{p}_1 \nu_{12}
\end{align*}
%
For the remaining assignments of the loop invariant, only the indices change:
%
\begin{align*}
\delta_{22} &:= \frac{\tilde{p}_2^T r_2}{\tilde{p}_2^T A p_2} \\
r_3 &:= r_2 -  A p_2 \delta_{22} \\
\tilde{r}_3 &:= \tilde{r}_2 - A \tilde{p}_2 \delta_{22} \\
x_3 & := x_2 -  p_2 \delta_{22}
\end{align*}
%
Now that the $P_\text{before}$ and $P_\text{after}$ are completely determined, the update is found by identifying those assignments that are contained in $P_\text{after}$, but not in $P_\text{before}$. For this example, no common subexpressions are replaced. The worksheet for this algorithm, together with a second one for the algorithm obtained from the set that only contains the first node, is shown in Figure \ref{fig:ws:BiCG}.

\begin{figure}
\centering
\scalebox{0.9}{
\begin{tabular}{cc}
\begin{minipage}[t]{0.5\textwidth}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\ALGroutinename}{ BiCG}
	
	\renewcommand{\WSprecondition}{}
	
	\renewcommand{\WSpartition}{
        $
        R \rightarrow
		\myFlaOneByThreeI{R_L}{r_R}{r_+}
	$
        }

	\renewcommand{\WSpartitionsizes}{
		$ R_{L} $ is $ n \times 0 $
	}
	
	\renewcommand{\WSpreprocessing}{
		$\begin{aligned}
		p_R &:= r_R \\
		\tilde{p}_R &:= \tilde{r}_R \\
		\delta_{BR} &:= \frac{\tilde{p}_R^T r_R}{\tilde{p}_R^T A p_R} \\
		r_+ &:= r_R -  A p_R \delta_{BR} \\
		\tilde{r}_+ &:= \tilde{r}_R - A \tilde{p}_R \delta_{BR} \\
		x_+ & := x_R -  p_R \delta_{BR}
		\end{aligned}$
	}
	
	\renewcommand{\WSguard}{ $\| r_R \| \geq \varepsilon$ }
	
	
	\renewcommand{\WSrepartition}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \rightarrow \myFlaOneByThreeI{R_0}{r_1}{r_2}$
	}
	
	\renewcommand{\WSrepartitionsizes}{
	$A$ is $k \times k$
	}

	
	\renewcommand{\WSupdate}{
		$\begin{aligned}
		\nu_{12} &:= \frac{\tilde{p}_1^T A r_2}{\tilde{p}_1^T A p_1} \\
		p_2 &:= r_2 +  p_1 \nu_{12} \\
		\tilde{p}_2 &:= \tilde{r}_2 + \tilde{p}_1 \nu_{12} \\
		\delta_{22} &:= \frac{\tilde{p}_2^T r_2}{\tilde{p}_2^T A p_2} \\
		r_3 &:= r_2 -  A p_2 \delta_{22} \\
		\tilde{r}_3 &:= \tilde{r}_2 - A \tilde{p}_2 \delta_{22} \\
		x_3 & := x_2 -  p_2 \delta_{22}
		\end{aligned}$
	}
	
	\renewcommand{\WSmoveboundary}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \leftarrow \myFlaOneByFourTFF{R_0}{r_1}{r_2}{r_3}$
	}
	
	\renewcommand{\WSpostcondition}{
	
	}
	
	{
	\FlaAlgorithmIter
	}
\end{minipage}
\vspace{0.05cm} & \vspace{0.05cm}
\begin{minipage}[t]{0.5\textwidth}
	\resetsteps
	\setboolean{BlockedAlgQ}{false}
	
	\renewcommand{\ALGroutinename}{ BiCG}
	
	\renewcommand{\WSprecondition}{}
	
	\renewcommand{\WSpartition}{
        $
        R \rightarrow
		\myFlaOneByThreeI{R_L}{r_R}{r_+}
	$
        }

	\renewcommand{\WSpartitionsizes}{
		$ R_{L} $ is $ n \times 0 $
	}
	
	\renewcommand{\WSpreprocessing}{
	}
	
	\renewcommand{\WSguard}{ $\| r_R \| \geq \varepsilon$ }
	
	
	\renewcommand{\WSrepartition}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \rightarrow \myFlaOneByThreeI{R_0}{r_1}{r_2}$
	}
	
	\renewcommand{\WSrepartitionsizes}{
	$A$ is $k \times k$
	}

	
	\renewcommand{\WSupdate}{
		$\begin{aligned}
		u_{01} &:= - \left( \tilde{P}_0^T A P_0 \right)^{-1} \tilde{P}_0^T A r_1 \\
		p_1 &:= r_1 +  P_0 u_{01} \\
		\tilde{p}_1 &:= \tilde{r}_1 + \tilde{P}_0 u_{01} \\
		\delta_{11} &:= \frac{\tilde{p}_1^T r_1}{\tilde{p}_1^T A p_1} \\
		r_2 &:= r_1 -  A p_1 \delta_{11} \\
		\tilde{r}_2 &:= \tilde{r}_1 - A \tilde{p}_1 \delta_{11} \\
		x_2 & := x_1 -  p_1 \delta_{11} 
		\end{aligned}$
	}
	
	\renewcommand{\WSmoveboundary}{
	$\myFlaOneByThreeI{R_L}{r_R}{r_+} \leftarrow \myFlaOneByFourTFF{R_0}{r_1}{r_2}{r_3}$
	}
	
	\renewcommand{\WSpostcondition}{
	
	}
	
	{
	\FlaAlgorithmIter
	}
\end{minipage}
\end{tabular}
} % scalebox
\caption{Worksheets for two BiCG algorithms. The one on the left is obtained from the loop invariant that consists of the full set of nodes. The right one is derived from the set that only contains the first node.}
\label{fig:ws:BiCG}
\end{figure}



\subsection{Remark on the Equivalence of Loop Invariants and Algorithms}
\label{sec:RemarkEquivalence}

Comparing those algorithms, one notes that the updates are very similar. In fact, according to the criteria established in Section \ref{sec:equivalence}, those algorithms are considered equivalent. With a suitable replacement, the loop invariant for one can be transformed into the loop invariant of the other. The differences in the shape of some assignments stem from the fact that $u_{01}$ is zero except for the last position. In the algorithm on the left in Figure \ref{fig:ws:BiCG}, this is revealed by the repartitioning, while this is not the case on the right.

In general, one observes that the presented approach always produces two equivalent loop invariants, namely the subset of the dependency graph that only contains the first node, and the full set. For direct methods, the coarsest possible, that is, the standard $2 \times 2$ partitioning, never results in equivalent loop invariants. Finer partitionings result in new loop invariants than can not be found with a coarser one.

For the iterative methods covered in this thesis, this is different. The partitioning that is used for the derivation is the coarsest possible one, as a coarser one would only partition those matrices that are initially partially known. All other matrices would not be partitioned at all, so it would not be possible to derive a PME. On the other hand, for quite a few of the methods presented in Appendix \ref{chap:appendixRepresentations}, a finer partitioning does not result in new loop invariants.

This is the case for those methods where either there is no matrix $U$ or it is upper diagonal. The reason is that for those methods, finer partitionings do not partition quantities that are computed in one iteration in multiple parts. All they do is expose additional quantities that are fully computed in different iterations.

%
%\subsubsection{Inductive PME}
%
%As mentioned in Section \ref{sec:MatrixRepresentationPartitionings}, the PME can be seen as an inductive description of the operation. To ensure that it is correct, the base case has to be correct as well. This can easily be shown by applying the initial partition (see Table \ref{tab:initialBlockDimensions}) to the PME and eliminating all quantities that are empty. For nonsymmetric CG, this yields the following expressions:
%%
%\begin{align*}
%\left\{ r_R, \varnothing, \varnothing, \varnothing, x_R \right\} &:= \text{CG} \left(A, r_R, x_R \right) \\
%p_R &:= r_R \\
%\delta_{BR} &:= \frac{r_R^T r_R}{r_R^T A p_R} \\
%r_+ &:= r_R - A p_R \delta_{BR} \\
%x_+ &:= x_R - p_R \delta_{BR}
%\end{align*}
%%
%$\varnothing$ is used to denote that there is no output in this position. Initially, only the first column of $R$ and $X$ are known. With the initial partitioning, $r_R$ and $x_R$ are those columns, so they are known. Since they are also the only output of $\text{CG} \left(A, r_R, x_R \right)$, this expression can be interpreted as a function that computes zero iterations. \todo{conclusion? can we say it's correct? it's computable.}
%


\section{Scope and Limitations}
\label{sec:scopeLimitations}

The presented method extends to a lot more iterative methods than those shown as examples throughout the thesis. Matrix representations of further methods are shown in Appendix \ref{chap:appendixRepresentations}. Note that this includes stationary iterative methods (\ref{sec:stationaryIterative}). For those, the derivation is even simpler because no properties have to be derived.

With the presented approach, it is possible to derive a large number of algorithms for most iterative methods. In case of CG for example, by using the derived properties to solve equations in different ways, four PMEs are found. From each PME, seven loop invariants are obtained, resulting in $28$ algorithms. Another considerable factor is added by the elimination of common subexpressions. Similar numbers can be expected for other, comparably complex iterative methods.
%
%In case of CG for example, four different PMEs are found by using the derived properties to solve equations in different ways.
%
%With the presented approach, it is possible to derive a large number of algorithms for most iterative methods. For CG for example, $28$ variants are found. There are seven different loop invariants per PME. Using the derived properties to solve equations in different ways, four PMEs are found.
%
%\todo{mention: family of algorithms. CG 28, 7 LIs, for variants. two quantities, two alternatives}
%
%With the presented approach, it is possible to derive a large number of algorithms for the CG method. There are seven different loop invariants. Using the derived properties, for two quantities, two different assignments are found for each one \todo{each? respectively?}. Thus, the number of algorithms is increased by a factor of $2 \cdot 2 = 4$, resulting in $28$ variants. Another considerable factor is added by the elimination of common subexpressions. Similar numbers can be expected for other, comparably complex iterative methods.

There are, however, some limitations. They are explained in the following.

%\paragraph{2 x 2 PMEs} For the derivation, we implicitly assumed that the PME always is of size $1 \times 2$. The CG variants Chronopoulos-Gear and Saad-Meurant yield PMEs of size $2 \times 2$. By itself, this is not a problem. The problem is that the top right, bottom left and bottom right parts each contain an equation that can be solved to the same quantity. This results in three different assignments for the same quantity. That should not be confused with obtaining multiple assignments by solving the same equation in different ways, using different properties. There are three ways to deal with this:
%
%\begin{enumerate}
%\item By far the easiest solution would be to discard those parts of the PME that are not on the main diagonal. The rationale behind this is that only the parts on the diagonal lead to the standard algorithms. The downside is that this reduces the number of derived algorithms.
%%
%\item The second option is to impose the restriction on dependency graphs that there can be only one assignment per quantity. Then, multiple dependency graphs are constructed, each containing one assignment.
%%
%\item Finally, one dependency graph is constructed that contains all assignments, but a similar restriction is imposed on the loop invariants: A loop invariant is only feasible if it contains at most one assignment for a given quantity.
%\end{enumerate}
%
%The latter two have the disadvantage in common that with the repartitioning, multiple assignments show up again. This then makes comparing $P_\text{before}$ and $P_\text{after}$ more complicated.

\subsubsection{Rewriting of Updates}

In some cases, the presented method is not able to generate those assignments that are commonly found in literature. This is for example the case for symmetric CG. The derived updates for $\nu_{12}$ always have a shape like this:
%
\begin{align}
\nu_{12} :=  \frac{p_1^T A r_2}{p_1^T A p_1} \label{eq:nuUpdate}
\end{align}
%
Usually, the following formula is used \cite{barrett:templates, saad2000iterative, vanderVorst:book}:
%
$$\nu_{12} := \frac{r_2^T r_2}{r_1^T r_1}$$
%
The advantage is that the matrix-vector product $p_1^T A$ is eliminated. It is obtained as follows. We begin with rewriting $r_2 = r_1 - A p_1 \delta_{11}$, which is the update for $r_2$, as $A p_1 = (r_1 - r_2) \delta_{11}^{-1}$. Then, both sides of this equation are transposed, resulting in $p_1^T A = \delta_{11}^{-1} (r_1 - r_2)^T$. Now, this equation is used to replace $p_1^T A$ in the numerator of equation (\ref{eq:nuUpdate}):
%
\begin{align*}
\nu_{12} :=  \frac{\delta_{11}^{-1} (r_1 - r_2)^T r_2}{p_1^T A p_1}
\end{align*}
%
Because of the orthogonality or $R$, it simplifies to 
%
\begin{align*}
\nu_{12} :=  \frac{\delta_{11}^{-1} r_2^T r_2}{p_1^T A p_1} \text{.}
\end{align*}
%
$\delta_{11}$ is then replaced with
%
$$\delta_{11} = \frac{r_1^T r_1}{p_1^T A p_1} \text{,}$$
%
finally resulting in
%
\begin{align*}
\nu_{12} :=  \frac{p_1^T A p_1}{r_1^T r_1} \cdot \frac{r_2^T r_2}{p_1^T A p_1} = \frac{r_2^T r_2}{r_1^T r_1} \text{.}
\end{align*}
%

By itself, this transformation is not particularly difficult. Since all steps follow well defined algebraic rules, it is not even difficult to design a system that is able to perform the individual steps of this rewriting. The problem is that based on the initial equation (\ref{eq:nuUpdate}), there is no indication that it is possible to reduce the number of matrix-vector products. Even if that is known, there is no indication which steps to perform. Thus, any system that is supposed to rewrite the assignment has to perform some sort of exhaustive search. Unfortunately, since expressions are substituted, the search space is infinite. Heuristics have to be applied to guarantee the termination of this search. However, they must not limit the capability of the system to find such rewritings.

%
%
%To some extent, it is not even difficult to do it in a systematic way. After all, all those steps 
%
%While this transformation is not very difficult, it is certainly not trivial to recognize that it is possible at all.  
%While a human expert can easily transform the first one into the second by some basic algebraic manipulations, it is difficult to do this in a systematic way.

\subsubsection{Normalized Vectors}

Some iterative methods construct a set of orthonormal vectors. With the presented approach, it is not possible to derive algorithms for those methods. Consider the Arnoldi iteration as an example. The matrix representation is shown below. It is based on the description in \cite{saad2000iterative}:
%
\begin{align*}
\{ Q, H\}:= \text{AI} ( A, Q e_0) \equiv
\left\{
\begin{aligned}
P_\text{pre}: \{ &\text{\ttfamily Input}(A) \land \text{\ttfamily Matrix}(A) \\
		&\text{\ttfamily FirstColumnInput}(Q) \land \text{\ttfamily Matrix}[Q] \land  \\
		&\text{\ttfamily Orthonormal}[Q] \land \\
				&\text{\ttfamily Output}[H] \land \text{\ttfamily Matrix}[H] \land \text{\ttfamily UpperHessenberg}[H]  \} \\
P_\text{post}: \{ &A \underline{Q} = Q H \\
			&\text{size}(Q) = n \times m \}
\end{aligned}
\right.
\end{align*}
%
The postcondition is repartitioned as follows:
%
$$A \myFlaOneByTwo{Q_L}{q_R} = \myFlaOneByThree{Q_L}{q_R}{q_{+}} \myFlaThreeByTwo{H_{TL}}{h_{TR}}{h_{ML}}{\eta_{MR}}{0}{\eta_{BR}}$$
%
After that, the Matrix Arithmetic step yields the following expression:
%
$$\myFlaOneByTwo{AQ_L = Q_L H_{TL} + q_R h_{ML}}{A q_R = Q_L h_{TR} + q_R \eta_{MR} + q_+ \eta_{BR}}$$
%
As usual, the left-hand side is matched by the function:
%
$$\left\{ \myFlaOneByTwo{Q_L}{q_R} , \myFlaTwoByOne{H_{TL}}{h_{ML}} \right\}:= \text{AI} ( A, Q_L e_0)$$
%
Using that $Q$ is orthonormal, the following assignments are obtained for $h_{TR}$ and $\eta_{MR}$:
%
\begin{align*}
h_{TR} &:= Q_L^T A q_R \\
\eta_{MR} &:= q_R^T A q_R 
\end{align*}
%
The problem is now to compute the normalized vector $q_+$ and $\eta_{BR}$. From $A q_R = Q_L h_{TR} + q_R \eta_{MR} + q_+ \eta_{BR}$, the following equation can be obtained, where all quantities on the right-hand side are known:
%
$$q_+ \eta_{BR} = A q_R - Q_L h_{TR} - q_R \eta_{MR}$$
%
Since we know that $q_+$ is normalized, the scalar $\eta_{BR}$ has to be computed as
%
\begin{align}
\eta_{BR} :=  \| A q_R - Q_L h_{TR} - q_R \eta_{MR} \| \text{.}
\label{eq:ArnoldiProblem}
\end{align}
%
Then, $q_+$ is determined by the following assignment:
%
$$q_+ := \frac{A q_R - Q_L h_{TR} - q_R \eta_{MR}}{\eta_{BR}}$$
%
With the presented method of solving equations by applying properties, the assignment (\ref{eq:ArnoldiProblem}) can not be obtained. To ensure that algorithms for such methods can be derived, the presented approach for solving equations must be expanded.

%must be enabled to identify equations that can be used to compute normalized vectors and transform them into suitable assignments.