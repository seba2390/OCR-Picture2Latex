\newif\ifaos
\aosfalse

\ifaos
\documentclass[aos,submission]{imsart}
\else
%\documentclass[aos,preprint]{imsart-arxiv}
\documentclass{article}
\usepackage[margin=1.7in]{geometry}
\usepackage[hang,flushmargin]{footmisc}
\fi

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[authoryear]{natbib}
\RequirePackage{xr-hyper}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}

\RequirePackage{mathtools}
\RequirePackage{array,multirow}
\RequirePackage{verbatim}
\RequirePackage{caption}
\RequirePackage{subcaption}
\RequirePackage{fancyvrb}
\RequirePackage{enumerate}
\RequirePackage{relsize}
\RequirePackage{placeins}

\RequirePackage{xparse}
\RequirePackage{xifthen}
\RequirePackage{enumitem}
\RequirePackage{pdfsync}
\RequirePackage{stefan_tex}
\synctex=1

%%%%%%%%% Theorems
\theoremstyle{plain}
\newtheorem{prop}{Proposition}
\newtheorem{conj}[prop]{Conjecture}
\newtheorem{coro}[prop]{Corollary}
\newtheorem{lemm}[prop]{Lemma}
\newtheorem{theo}[prop]{Theorem}

\theoremstyle{remark}
\newtheorem{exam}{Example}
\newtheorem{defi}{Definition}
\newtheorem{comm}{Comment}
\newtheorem{rema}{Remark}
\newtheorem{assumption}{Assumption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifaos 
\startlocaldefs 
\fi

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator*{\vspan}{span}
\DeclareMathOperator*{\cspan}{\overline{span}}
\DeclareMathOperator*{\starhull}{star}
\DeclareMathOperator*{\absconv}{absconv}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator{\proj}{\Pi}

\newcommand{\ww}{\mathcal{W}}
\newcommand{\treatment}{Z}
\newcommand{\target}{T}
\newcommand{\ntreatment}{n_{\treatment}}
\newcommand{\ntarget}{n_{\target}}
\newcommand{\Fperp}{\F_{\perp}}
\newcommand{\btheta}{\overline{\theta}}
\newcommand{\Xs}{X^{\star}}
\newcommand{\poptreatment}{\P_Z}
\newcommand{\poptarget}{\P_T}
\newcommand{\FK}{\F_K}
\newcommand{\FKspace}{\bar \F_K}
\newcommand{\MM}{\mathcal M}

\newcommand{\regfunc}{R}
\newcommand{\regbound}{\bar{R}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\influence}{\iota}
\newcommand{\dpsi}[1][]{\ifthenelse{\equal{#1}{}}{\dot{\psi}}{\dot{\psi}_{#1}}}
\newcommand{\dchi}[1][]{\ifthenelse{\equal{#1}{}}{\dot{\chi}}{\dot{\chi}_{#1}}}
\newcommand{\edchi}[1][]{\ifthenelse{\equal{#1}{}}{\dot{\chi}}{\dot{\chi}_{#1}}}
\renewcommand{\dh}[1][]{\ifthenelse{\equal{#1}{}}{\dot{h}}{\dot{h}_{#1}}}
\newcommand{\riesz}[1][]{\ifthenelse{\equal{#1}{}}{\gamma}{\gamma_{{#1}}}}
\newcommand{\hriesz}[1][]{\ifthenelse{\equal{#1}{}}{\hgamma}{\hgamma_{{#1}}}}
\newcommand{\ariesz}[1][]{\ifthenelse{\equal{#1}{}}{\tilde{gamma}}{\tilde{\gamma_}{{#1}}}}
\newcommand{\pd}[2]{\frac{\partial}{\partial #1}|_{#1=#2}}


\newcommand{\tangent}{\mathcal{T}}
\newcommand{\gammaipw}{\gamma^{\star}}
\newcommand{\excess}{\mathcal{L}}
\newcommand{\gexcess}{\check{\excess}}
\newcommand{\gF}{\ff^{\star}}
\newcommand{\gH}{\mathcal{H}^{\star}}
\newcommand{\gf}{\check{f}}
\newcommand{\centeredg}{\check{g}}
\renewcommand{\gh}{h_c}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\rff}{\tilde{\ff}}
\renewcommand{\AA}{\mathcal{A}}
\renewcommand{\SS}{\mathcal{S}}
\newcommand{\Cc}{\mathcal{C}_c}
\newcommand{\Cz}{\mathcal{C}_0}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\gstar}{\riesz}
\newcommand{\gapprox}{\tilde{g}}

\newcommand{\trim}{\mathrm{trim}}
\newcommand{\union}{\cup}
\newcommand{\intersection}{\cap}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\OO}{\mathcal{O}}

\DeclarePairedDelimiter\smallabs{\lvert}{\rvert}
\DeclarePairedDelimiter\smallnorm{\lVert}{\rVert}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}
\DeclarePairedDelimiter\inner{\langle}{\rangle}

\newcommand{\textand}{\ \text{and}\ }
\newcommand{\st}{\text{ {\em s.t.} }}
\newcommand{\withprob}{\text{ {\em w.p.} }}
\newcommand{\as}{\text{ {\em a.s.} }}

\renewcommand{\S}{\mathcal{S}}
\newcommand{\R}{\mathbb{R}}
%\newcommand{\Pn}{\mathbb{P}_n}
%\renewcommand{\P}{\mathbb{P}}
\newcommand{\toP}{\rightarrow_p}
\newcommand{\todist}{\rightarrow{\mathcal{L}}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathcal{Z}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\bmu}{\bar{\mu}}
\newcommand{\Op}[1]{\mathcal{O}_P\left(#1\right)}
\newcommand{\op}[1]{o_{\P}\left(#1\right)}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
%\renewcommand{\and}{\text{ and }} % this breaks \and in title :(
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}


\NewDocumentCommand{\Pn}{g}{\IfValueTF{#1}{\P{#1}_n}{\P_n}}
\RenewDocumentCommand{\P}{g}{\IfValueTF{#1}{P^{(#1)}}{P}}
\NewDocumentCommand{\einv}{g}{%
    \ensuremath{e^{-1}\IfValueT{#1}{\p{#1}}}}
\NewDocumentCommand{\E}{g}{%
	\ensuremath{\operatorname{{\mathbb E}}\IfValueT{#1}{ \p{#1} }}}
\RenewDocumentCommand{\L}{mg}{%
        \ensuremath{L_{#1}\IfValueT{#2}{(#2)}}}
\NewDocumentCommand{\M}{g}{\IfValueTF{#1}{\smash{\mathbb{M}^{(#1)}}}{\mathbb{M}}}
\NewDocumentCommand{\Mn}{g}{\IfValueTF{#1}{\smash{\mathbb{M}^{(#1)}_n}}{\mathbb{M}_n}}
\NewDocumentCommand{\I}{mmg}{{\IfValueTF{#3}{I_{#2}^{(#3)}}{I_{#2}}}(#1)}

\ifaos 
\endlocaldefs 
\fi

%%%%%%%%%%%%%%%%%%%%%%

\ifaos
\externaldocument{supplement}
\fi

\begin{document}

\ifaos
\begin{frontmatter}

\title{Augmented Minimax Linear Estimation}
\runtitle{Augmented Minimax Linear Estimation}


\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Only one address is permitted per author. %%
%%Only division, organization and e-mail is %%
%%included in the address.                  %%
%%Additional information can be included in %%
%%the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{David} \snm{Hirshberg}\ead[label=e1]{davidahirshberg@stanford.edu}}
\and
\author[B]{\fnms{Stefan} \snm{Wager}\corref{}\ead[label=e2]{swager@stanford.edu}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Statistics, Stanford University, \printead{e1}}
\address[B]{Stanford Graduate School of Business, \printead{e2}}
\end{aug}
\else
\title{Augmented Minimax Linear Estimation}
\author{David A.~Hirshberg \and Stefan Wager}
\date{Stanford University}
%\def\monthname{\ifcase\month\or
%  January\or February\or March\or April\or May\or June\or July\or
%  August\or September\or October\or November\or December\fi}
%\author{David A.~Hirshberg\thanks{{\small Postdoctoral Fellow, Department of Statistics and Graduate School of Business, Stanford University, \texttt{davidahirshberg@stanford.edu}.}}
%\and Stefan Wager\thanks{{\small Assistant Professor of Operations, Information and Technology,
%Graduate School of Business, and of Statistics (by courtesy), Stanford University, \texttt{swager@stanford.edu}.}}}
\maketitle
\fi

\begin{abstract}
Many statistical estimands can expressed as continuous linear functionals of a conditional expectation function.
This includes the average treatment effect under unconfoundedness and generalizations for continuous-valued and personalized treatments.
In this paper, we discuss a general approach to estimating such quantities: we begin with a simple plug-in estimator based on
an estimate of the conditional expectation function, and then correct the plug-in estimator by subtracting a minimax linear estimate
of its error. We show that our method is semiparametrically efficient under weak conditions and observe promising performance on both real and simulated data.
\end{abstract}

\ifaos
\begin{keyword}[class=MSC2010]
\kwd{62F12}
\end{keyword}

\begin{keyword}
\kwd{causal inference}
\kwd{convex optimization}
\kwd{semiparametric efficiency}
\end{keyword}

\end{frontmatter}
\fi

\section{Introduction}
\label{sec:introduction}
Suppose we observe
$n$ independent and identically distributed samples $(Z_i, Y_i) \sim P$ with support in $\zz \times \RR$,
and we want to estimate a continuous linear functional of the form
\begin{equation}
\label{eq:psi}
\psi(m) = \EE[P]{h(Z_i, \, m)} \ \ \text{ at } \ \ m(z) = \EE[P]{Y_i \cond Z_i = z}. 
\end{equation}
Our main result establishes that we can build 
efficient estimators for a wide variety of such problems simply by subtracting from 
a plugin estimator $\psi(\hm)$ a minimax linear estimate of its error $\psi(\hm)-\psi(m)$. 

The following estimands from the literature on causal inference and missing data are of this type 
and can be estimated efficiently by our approach.

\begin{exam}[Mean with Outcomes Missing at Random]
\label{exam:mar}
We observe covariates $X_i$ and some but not all of the corresponding outcomes $Y_i^{\star}$.
We write $W_i \in \cb{0, \, 1}$ to indicate whether the outcome $Y_i^{\star}$ was observed, 
and define $Z_i=(X_i, W_i)$ and $Y_i=W_i Y_i^{\star}$; we then estimate the 
linear functional $\psi(m) = \EE[P]{m(X_i, \, 1)}$ at $m(x,w) = \EE[P]{Y_i \cond X_i=x, W_i=w}$.
This will be equal to the mean $\EE[P]{Y^{\star}_i}$ if, conditional on covariates $X_i$,
each outcome $Y_i^{\star}$ is independent of its nonmissingness $W_i$ \citep{rosenbaum1983central}. 
\end{exam}
\begin{exam}[Average Partial Effect]
\label{exam:ape}
Letting $Z_i = \p{X_i, \, W_i} \in \xx \times \RR$, we estimate the average of the derivative of
the response surface $m(x,w)$ with respect to $w$,
$\psi(m) = \EE[P]{\frac{\partial}{\partial w} \cb{m(X_i, \, w)}_{w = W_i} }$.
This estimand, and weighted variants of it, quantify the
average effect of a continuous treatment $W_i$ under exogeneity \citep*{powell1989semiparametric}.
\end{exam}
\begin{exam}[Average Partial Effect in the Conditionally Linear Model]
\label{exam:ape-linear}
In the setting of the previous example, we make the additional assumption that the 
regression function $m$ is conditionally linear in $w$, $m(x,w) = \mu(x) + w\tau(x)$.
The average partial effect is then $\psi(m) = \EE[P]{\tau(X_i)}$.
\end{exam}
\begin{exam}[Distribution Shift]
\label{exam:distribution-shift}
We estimate the effect of a shift in the distribution of the
conditioning variable $Z$ from one known distribution, $P_0$, to another, $P_1$, i.e., $\psi(m) =$ \smash{$\int m(z) (d\P_1(z) - d\P_0(z))$} for \smash{$m(z) = \EE[P]{Y_i \mid Z_i=z}$}.
Under exogeneity assumptions, this estimand can be used to compare policies for assigning personalized treatments, and estimators for it form a key building block in methods for estimation of optimal treatment policies.
\end{exam}

Below, we first discuss our estimator in the simple case that 
$h(z, \, m)$ in \eqref{eq:psi} does not depend on $z$, i.e., $h(z,m)=\psi(m)$. In this case, e.g., in Example~\ref{exam:distribution-shift},
we can evaluate
$\psi(m)$ without knowledge of the distribution $\P$ of $z$, and we say that our functional of interest 
$\psi(\cdot)$ is \emph{evaluable}.
From Section~\ref{sec:simple-setting} on, we will address the general case where $h$ also
depends on $z$ and so, even if we knew $m$ a-priori, we could only approximate $\psi(m)$
with a sample average \smash{$n^{-1}\sum_{i=1}^n h(Z_i,\,m)$.}


\subsection{Estimating Evaluable Linear Functionals}
\label{sec:amle}

Consider the estimation of $\psi(m)$ where $\psi(\cdot)$ is an evaluable mean-square-continuous linear functional.
The estimator we propose takes a plugin estimator $\psi(\hm)$, and then subtracts out an estimate of its error
$\psi(\hm)-\psi(m)=\psi(\hm-m)$ obtained as a weighted average of regression residuals,
\begin{equation}
\label{eq:aml-intro}
\hpsi = \psi(\hm) - \frac{1}{n} \sum_{i = 1}^n \hriesz_i \p{\hm(Z_i) - Y_i}.
\end{equation}
Our approach builds on a result of \citet*{chernozhukov2016locally} and \citet*{chernozhukov2018double},
who show that we can use the Riesz representer for $\psi$ to construct efficient estimators of this type.

To motivate this approach recall that, by the Riesz representation theorem,
any continuous linear functional $\psi(\cdot)$ on the square integrable functions from $\zz$ to $\R$
has a Riesz representer $\riesz[\psi](\cdot)$, i.e., a function satisfying \smash{$\int \riesz[\psi](z)f(z) d\P(z) = \psi(f)$} for all square-integrable functions $f$ \citep[e.g.,][Theorem 1.41]{peypouquet2015convex}.
Then, if we set \smash{$\hriesz_i = \riesz[\psi](Z_i)$} in \eqref{eq:aml-intro}, the second term
in the estimator acts as a correction for the error of \smash{$\psi(\hm)$} because
\begin{equation}
\label{eq:derivation}
\begin{split}
\psi(\hm) - \psi(m) &= \int \riesz[\psi](z)(\hm-m)(z) d\P(z)  
\approx \frac{1}{n}\sum_{i=1}^n \riesz[\psi](Z_i) (\hm(Z_i) - m(Z_i))  \\
&= \frac{1}{n}\sum_{i=1}^n \riesz[\psi](Z_i) (\hm(Z_i)-Y_i) + \frac{1}{n}\sum_{i=1}^n \riesz[\psi](Z_i) \p{Y_i - m(Z_i)}. 
\end{split}
\end{equation}
Thus, plugging the above expression into \eqref{eq:aml-intro}, we see that if we could compute
our estimator with the oracle Riesz representer weights $\riesz[\psi](Z_i)$, 
its error would very nearly be a weighted sum of mean-zero noise \smash{$n^{-1}\sum_{i=1}^n \riesz[\psi](Z_i) \varepsilon_i$} where \smash{$\varepsilon_i = Y_i - m(Z_i)$.}
This behavior is asymptotically optimal with a great deal of generality \citep[e.g.,][Proposition 4]{newey1994asymptotic}. 

Our goal will be to imitate the behavior of this oracle estimator without a-priori knowledge of the Riesz representer. 
One possible approach is to determine the form of the Riesz representer $\riesz[\psi](\cdot)$ by solving
analytically the set of equations that define it, 
\begin{equation}
\label{eq:riesz}
\int \riesz[\psi](z) f(z) d\P(z)  = \psi(f)\ \text{ for all $f$ satisfying }\ \int f(z)^2 d\P(z)  < \infty,
\end{equation}
then estimate it and plug the resulting weights $\hriesz_i = \hriesz[\psi](Z_i)$ into \eqref{eq:aml-intro}. 
In the context of our first example, the estimation of a mean with outcomes missing,
the Riesz representer is the inverse probability weight $\riesz[\psi](w,x) = w/e(x)$ where $e(x) = \P[W_i=1 \mid X_i=x]$, 
and this plug-in approach involves first obtaining an estimate \smash{$\he(x)$} of
treatment probabilities and then weighting by its inverse.
This is the well-known Augmented Inverse Probability Weighting (AIPW) estimator of \citet{robins1994estimation}. 
\citet{chernozhukov2016double} provide general results on the efficiency of
such estimators, provided \smash{$\hriesz[\psi](Z_i) - \riesz[\psi](Z_i)$} goes to zero fast enough in squared-error loss.

We take another approach. Considering our regression estimator $\hm$ and the design $Z_1 \ldots Z_n$ to be fixed,\footnotemark\,we
simply choose the weights $\hgamma \in \R^n$ that make our correction term $n^{-1} \sum_{i = 1}^n \hriesz_i \p{\hm(Z_i) - Y_i}$
a minimax linear estimator of what it is intended to correct for, $\psi(\hm-m)$.
To be precise, we first choose an absolutely convex set of functions $\ff$ which we
believe should contain the regression error $\hm - m$.
We then choose weights $\hgamma_i$ that perform best in terms of worst case mean squared error
over possible regression errors $\hm - m \in \ff$
and conditional variances satisfying $\Var[P]{Y_i \mid Z_i} \le \sigma^2$. 
This specifies the weights $\hriesz$ as the solution to a convex optimization problem,
\begin{equation*}
\hriesz = \argmin_{\gamma \in \R^n}\set*{ I_{\psi,\ff}^2(\gamma) + \frac{\sigma^2}{n^2}\norm{\gamma}^2 },
\quad I_{\psi,\ff}(\gamma) = \sup_{f \in \ff}\set*{\frac{1}{n} \sum_{i = 1}^n \gamma_i f(Z_i) - \psi(f)}.
\end{equation*}
The good properties of minimax linear estimators like this one are well known. \citet{donoho1994statistical} and related papers
\citep{armstrong2015optimal,cai2003note,donoho1991geometrizing,ibragimov1985nonparametric,
johnstone2015-gaussian-sequence,juditsky2009nonparametric} show that when a regression function $m$ is in a convex set $\ff$ and $Y_i \cond Z_i \sim N(0,\sigma_i^2)$,
a minimax linear estimator of a linear functional \smash{$\psi(m)$} will come within a factor 1.25 of the minimax risk over all estimators.
In addition to strong conceptual support, estimators of the type have been found to perform well
in practice across several application areas \citep{armstrong2015optimal,imbens2017optimized,zubizarreta2015stable}.

Methodologically, the main difference between our proposal and the references cited above
is that we use the minimax linear approach to debias a plugin estimator \smash{$\psi(\hm)$}
rather than as a stand-alone estimator.
Because we `augment' the minimax linear estimator by applying it after
regression adjustment in the same way that the AIPW estimator
augments the inverse probability weighting estimator, we refer to our
approach as the Augmented Minimax Linear (AML) estimator.
Our main result establishes semiparametric efficiency of the AML estimator
under considerable generality.


\footnotetext{If we estimate $\hm$ on an auxiliary sample, this is the case when we condition on both that sample and on $Z_1 \ldots Z_n$.
While it is not necessary to estimate $\hm$ on an auxiliary sample when estimating linear functionals,
it can buy us some additional flexibility. We discuss this in Section~\ref{sec:sample-splitting}.}

We note that the weights $\hgamma$ that underlie minimax linear estimation can be interpreted as a penalized least-squares solution to a set of estimating equations 
suggested by the definition \eqref{eq:riesz} of the Riesz representer $\riesz[\psi]$,
\begin{equation}
\label{eq:riesz-sample}
\frac{1}{n}\sum_{i=1}^n\riesz_i f(Z_i)  \approx \psi(f) \ \text{ for all }\ f \in \ff. 
\end{equation}
These estimating equations generalize covariate balance conditions
from the literature on the estimation of average treatment effects, and 
when analyzing our estimator we build on approaches used to study 
treatment effect estimators that use balancing weights \citep[e.g.,][]{athey2016approximate,graham1,imai2014covariate,kallus2016generalized,zubizarreta2015stable};
see Section~\ref{sec:balancing} for further discussion.

The restriction of $f$ to a strict subset $\ff$ of the square-integrable functions is 
necessary, as there are infinitely many square-integrable functions 
$f$ that agree on our sample $Z_1 \ldots Z_n$ and they need not even approximately agree in terms of $\psi(f)$. 
Our choice of this subset $\ff$, a set that characterizes our uncertainty about the regression error function $\hm - m$, 
focuses our estimated weights $\hgamma$ on the role they play in ensuring that \eqref{eq:riesz-sample} is satisfied for this function $f=\hm - m$. The size of this subset $\ff$, measured by, e.g., its Rademacher complexity, 
determines the accuracy with which these equations \eqref{eq:riesz-sample} can be simultaneously satisfied.
The smaller we can make $\ff$, i.e., the better the consistency guarantees we have for $\hm$, the more accurately we can solve
\eqref{eq:riesz-sample}. In practice, we may take $\ff$ to be a set of smooth functions, functions that are approximately sparse in some basis, 
functions of bounded variation, etc.

That our weights $\hriesz_i$ approximately solve the estimating equations \eqref{eq:riesz-sample}
does not imply that they estimate the Riesz representer \smash{$\riesz[\psi](\cdot)$} well in the mean-square
sense. 
However, to whatever degree the oracle weights $\riesz_i = \riesz[\psi](Z_i)$ also approximately solve 
\eqref{eq:riesz-sample}, it will imply that $\hriesz$ and $\riesz[\psi](\cdot)$ are close in the sense that
\begin{equation}
\label{eq:bilin-uniform}
\frac{1}{n}\sum_{i=1}^n [\hriesz_i-\riesz[\psi](Z_i)]f(Z_i) \approx 0 \ \text{ for all}\ f \in \ff.
\end{equation}
This property holds if and only if the vector with elements $\hriesz_i - \riesz[\psi](Z_i)$ is small or 
approximately orthogonal to every vector with elements $f(Z_i)$ for $f \in \ff$. 
And it implies that when $\hm - m \in \ff$, our estimator \eqref{eq:aml-intro} approximates the corresponding
oracle estimator, as the difference between them is $n^{-1}\sum_{i=1}^n [\hriesz_i-\riesz[\psi](Z_i)][(\hm-m)(Z_i) - \varepsilon_i]$. 

We state below a simple version of our main result. In essence, 
if an estimator $\hm$ converges to $m$ in mean square and our regression error $\hm - m$ is in a uniformly bounded Donsker class $\ff$ 
or more generally satisfies $(\hm - m)/O_P(1) \in \ff$, then our approach can be used to define an efficient estimator.

\subsection{Definitions}

As a measure of the scale of a function $f$ relative to an absolutely convex set $\ff$, 
we define the \emph{gauge} \smash{$\norm{f}_{\ff}$} $= \inf\set{\alpha > 0 : f \in \alpha\ff}$.
%We will write \smash{$\L{2}{\P}$} to refer to \smash{$\{ f : \EE[P]{ f(Z)^2} \le 1$\}} 
%and \smash{$\L{2}{\Pn}$} for \smash{$\{ f : n^{-1}\sum_{i=1}^n$} \smash{$f(Z_i)^2 \le 1\}$},
%so that the gauges \smash{$\norm{\cdot}_{\L{2}{\P}}$} and \smash{$\norm{\cdot}_{\L{2}{\Pn}}$} have their typical meanings as
%the root mean squared error and empirical root mean squared error.
We will write $\ff_r$ to denote the localized class \smash{$\set{ f \in \ff : \norm{f}_{L_2(\P)} \le r}$},
$g\ff$ to denote the class of products $\set{ gf : f \in \ff}$, and
$h(\cdot, \ff)$ to denote the image class $\set{ h(\cdot, f) : f \in \ff}$.
We will write $\overline{\mm}$ to denote the closure of a subspace $\mm$ of the square-integrable functions
and $\mm_{\perp}$ to denote its orthogonal complement, and will write $\cspan \ff$ to denote the closure of $\vspan \ff$.
We will say that a set of functions $\ff$ from $\zz \to \R$ is pointwise bounded if $\sup_{f \in \ff} \abs{f(z)} < \infty$ for all $z \in \zz$,
uniformly bounded if $\sup_{f \in \ff}\norm{f}_{\infty} < \infty$ where $\norm{f}_{\infty}=\sup_{z \in \zz}\abs{f(z)}$, 
and pointwise closed if $f \in \ff$ whenever it is the limit of a sequence $f_j \in \ff$ in the sense that $\lim_{j \to \infty} f_j(z)=f(z)$ for all $z \in \zz$.

\subsection{Setting}
\label{sec:simple-setting}
We observe $(Y_1,Z_1) \ldots (Y_n,Z_n) \overset{iid}{\sim} P$ with $Y_i \in \R$ and $Z_i$ in an arbitrary set $\zz$.
We assume that $m(z)=\E[Y_i \mid Z_i=z]$ is in a subspace $\mm$ of the square integrable functions
 and that $v(z)=\Var[P]{Y_i \mid Z_i=z}$ is bounded. 
And we let $\ff$ be an absolutely convex set of square integrable functions.

Our estimand is $\psi(m)$ for a continuous linear functional $\psi(\cdot)$ on a subspace $\mm \cup \vspan \ff$
of the square integrable functions, which takes the form $\psi(m)=\E h(Z_i, m)$.
The Riesz representation theorem guarantees the existence and uniqueness of a function $\riesz[\psi] \in \cspan \ff$ 
satisfying the set of equations $\set{\E \riesz[\psi](Z) f(Z) = \psi(f) : f \in \cspan \ff}$.\footnotemark \ 
We call this function the Riesz representer of $\psi$ on the \emph{tangent space} $\cspan \ff$.
This generalizes our prior definition \eqref{eq:riesz}, coinciding when $\cspan\ff$ is the space of square integrable functions.


Our regularity and efficiency claims are relative to the set of all one-dimensional submodels $\P_t$ 
through $\P_0=\P$ for which, letting $(Y_t,Z_t) \sim P_t$, the regression functions \smash{$m_{\P_t}(z)=\E[Y_t \mid Z_t=z]$} are in $\mm$
and satisfy  \smash{$\lim_{t \to 0}\norm{m_{\P_t} - m_{\P}}_{L_2(\P)} = 0$} and the squares of \smash{$\epsilon_t = Y_t - m_{P_t}(Z_t)$} are uniformly integrable.
For these claims, we use the additional assumptions that there is a regular conditional probability $\P[Y_i \in \cdot \mid Z_i=z]$ and 
that $\mm_{\perp}$ has a dense subset of bounded functions.


\footnotetext{In this statement we implicitly work with the unique extension of the continuous functional $\psi(\cdot)$ defined on $\vspan \ff$
to a functional defined on its closure $\cspan \ff$ \citep[e.g.,][Theorem IV.3.1]{lang1993real}.}

\begin{theo}
\label{theo:simple}
In the setting above, choose finite $\sigma > 0$ and consider the estimator 
\begin{align}
\label{eq:aml}
&\hpsi_{AML} = \frac{1}{n} \sum_{i = 1}^n [ h(Z_i, \hm) - \hriesz_i \p{\hm(Z_i) - Y_i}] \quad \text{ where }\  \\
\label{eq:aml-primal}
&\hriesz = \argmin_{\gamma \in \R^n}\set*{I_{h,\ff}^2(\gamma) + \frac{\sigma^2}{n^2}\norm{\gamma}^2},\ \ I_{h,\ff}(\gamma) = \sup_{f \in \ff}\set*{ \frac{1}{n} \sum_{i = 1}^n [\gamma_i f(Z_i) - h(Z_i,f)]}. 
\end{align}
If $\ff$ is uniformly bounded and pointwise closed; $\ff$, $\riesz[\psi]\ff$, and $h(\cdot,\ff)$ are Donsker;
and $h(Z,\cdot)$ is pointwise bounded and mean-square equicontinuous on $\ff$ in the sense that 
$\sup_{f \in \F} \abs{ h(z,f) } < \infty$ for each $z \in \zz$ and 
$\lim_{r \to 0}\sup_{f \in \F_r}\norm{ h(\cdot,f) }_{L_2(P)} = 0$;
then our weights converge to the Riesz representer of $\psi$ on the tangent space $\cspan \ff$, i.e.,
\begin{equation}
\label{eq:gamma-consistency}
\frac{1}{n}\sum_{i=1}^n (\hgamma_i - \riesz[\psi](Z_i))^2 \to_P 0.
\end{equation}
If, in addition, $\hm$ has the tightness and consistency properties 
\begin{equation}
\label{eq:consistency-properties}
\norm{\hm - m}_{\ff} = O_P(1) \ \text{ and }\ \norm{\hm - m}_{\L{2}{\Pn}} = o_P(1) 
\end{equation}
then our estimator $\hpsi_{AML}$ is asymptotically linear, i.e., 
\begin{equation}
\label{eq:asymptotic-linearity}
\begin{split}
&\hpsi_{AML} - \psi(m) = \frac{1}{n}\sum_{i=1}^n \influence(Y_i, \, Z_i) + o_{P}(n^{-1/2}) \ \text{ where }\\
&\influence(y, \, z) = h(z,m) - \riesz[\psi](z)(m(z) - y) - \psi(m),
\end{split}
\end{equation}
and therefore $\sqrt{n}(\hpsi_{AML} - \psi(m))/V^{1/2} \Rightarrow \nn\p{0, \, 1}$ with $V = \EE[P]{\influence(Y, \, Z)^2}$.


Furthermore, an estimator satisfying \eqref{eq:asymptotic-linearity} is 
regular on the model class $\mm$ if $\mm \subseteq \cspan \ff$,
and asymptotically efficient if, in addition, $v(\cdot) \riesz[\psi](\cdot) \in \overline{\mm}$.\footnote{If an estimator satisfies
\eqref{eq:asymptotic-linearity}, 
a combination of two simple conditions implies efficiency: 
$\cspan \ff = \overline \mm$ and $v(\cdot)\overline{\mm} \subseteq \overline{\mm}$. The first says that
we correct for all error functions $\hm - m$ permitted by our assumption that $m \in \mm$, 
and waste no effort on those (in $\mm_{\perp}$) ruled out by it. The second
holds when the conditional variance $v(z)$ is sufficiently simple relative to $\overline{\mm}$,
e.g., when $v(z)$ is constant or when the model class $\mm$ is fully nonparametric in the sense that it contains an approximation to
every square integrable function.} 
\end{theo}

Theorem~\ref{theo:simple} follows from a finite sample result, Theorem~\ref{theo:simple-rate},
that we will discuss in Section~\ref{sec:main-results}. 
We end this section with a few remarks on the statistical behavior of the estimator, 
focusing on the choices of $\hm,\ff,\sigma$ that define a specific estimator $\hpsi$ of this type. 
We defer the discussion of computational issues to 
\ifaos
Appendix~\ref{sec:computation} \citep{amle-supplement}.
\else
Appendix~\ref{sec:computation}.
\fi


\begin{rema}
\label{rema:riesz-assumptions}
Our approach does not require knowledge of the functional form of the Riesz representer $\riesz[\psi](\cdot)$,
sparing us the trouble of solving \eqref{eq:riesz} analytically. 
\end{rema}

\begin{rema}
\label{rema:consistency}
If $\norm{m}_{\ff} < \infty$, the tightness and consistency properties \eqref{eq:consistency-properties}
are satisfied by the penalized least squares estimator $\hm=\argmin$ $n^{-1}\sum_{i=1}^n (Y_i - m(Z_i))^2 + \lambda \norm{m}_{\ff}$
for an appropriate choice of $\lambda$ (see Appendix~\ref{appendix:penalized-least-squares}).
For example, we might choose $\ff$ to be the absolutely convex hull $\set{ \sum_j  \beta_j \phi_j : \norm{\beta}_{\ell_1} \le 1}$
of a sequence of basis functions satisfying \smash{$\sum_{j=1}^{\infty} \E \phi_j^2(Z_i) < \infty$.}
It is Donsker \citep[Section 2.13.2]{vandervaart-wellner1996:weak-convergence}
and the corresponding estimator $\hm$ is $\ell_1$-penalized regression in this basis.
This approach is easy to implement and performs well in simuation when $\lambda$ is chosen by cross-validation.
In our simulations, we use a class of this type defined in terms a basis of scaled Hermite polynomials.
\end{rema}

\begin{rema}
\label{rema:semiparametric-models}
The choices we make for $\hm$ and $\ff$ reflect 
assumptions about the regression function $m$. In addition to nonparametric assumptions like smoothness, we may
make parametric or semiparametric assumptions. A semiparametric assumption 
distinguishes Examples~\ref{exam:ape} and \ref{exam:ape-linear}, which consider the average partial effect for arbitrary functions 
$m(x,w)$ and for functions of the form $m(x,w) = \mu(x) + w\tau(x)$ respectively. 

In the latter case, which we discuss in detail in
Section~\ref{sec:ape}, the tangent space $\cspan\ff$ is smaller than the space of all square integrable functions,
and the Riesz representer $\riesz[\ff]$ for $\psi(\cdot)$ will be the orthogonal projection onto $\cspan \ff$ of the Riesz representer $\riesz[L_2]$ for $\psi(\cdot)$ 
on the tangent space of all square-integrable functions. An important consequence is that,
under our efficiency condition $v\riesz[\psi]\in \overline\mm$,
the optimal asymptotic variance in Example~\ref{exam:ape-linear} is smaller than that in Example~\ref{exam:ape}.\footnotemark\
This reflects the ease of estimating the average partial effect in the conditionally linear model relative to the general case.

Naturally, such an estimator will be considered superefficient if we entertain the possibility that $m(x,w)$ does not have
the form $\mu(x) + w\tau(x)$, i.e., if our regularity condition $\mm \subseteq \cspan \ff$ is not satisfied. In this case,
our weights fail to adjust for the deviation $\hm - m$ for some possible regression function $m \in \mm$ in a neighborhood of $\hm$, 
and any gain in efficiency possible by doing so is, in a local minimax sense, spurious.
Characterization of the behavior of our estimator under this form of misspecification is important but beyond the scope of this paper.

This phenomenon is not unique to our approach; for additional discussion of the choice of tangent space when estimating a Riesz representer, see e.g., 
Remark 2.5 of \citet{chernozhukov2016double} and Section 3 of \citet{robins2007comment}. It pervades the literature on inference in high dimensional statistics,
which typically involves an estimate of the Riesz representer on an appropriate tangent space of high-dimensional parametric functions \citep[e.g.,][]{athey2016approximate, javanmard2014confidence, zhang2014confidence}. For example, when estimating a mean with outcomes missing at random in a high-dimensional linear model $m(x,w) = w x^T \beta$, 
$\riesz[\psi]$ is the best linear-in-$x$ approximation to the inverse propensity weights $w/e(x)$.

\footnotetext{
The difference in asymptotic variance between estimators using weights converging to $\riesz[L_2]$ (Example~\ref{exam:ape}) and weights converging to $\riesz[\ff]$ (Example~\ref{exam:ape-linear})
is $\E v(Z)[ \riesz[L_2]^2(Z) - \riesz[\ff]^2(Z)] = \E v(Z)[\riesz[L_2](Z) - \riesz[\ff](Z)]^2  + 2\E v(Z)\riesz[\ff](Z)[\riesz[L_2](Z) - \riesz[\ff](Z)].$
The first term in this decomposition is positive and the second term is zero \emph{if} $v\riesz[\ff] \in \cspan{\ff}$, as in this case 
$\E \riesz[L_2](Z) [v(Z)\riesz[\ff](Z)] = \psi(v\riesz[\ff]) = \E \riesz[\ff](Z)[v(Z)\riesz[\ff](Z)]$. 
}
\end{rema}

\begin{rema}
\label{rema:overlap}
Our assumption that $\psi(\cdot)$ has a square-integrable Riesz representer $\riesz[\psi]$,
equivalent to its mean-square continuity, 
is necessary in the sense that $\psi(m)$ does not have a regular estimator when it is violated 
\citep[Theorem 2.1][see Section~\ref{sec:pathwise-derivative} here for details]{van1991differentiable}.
 If $\ff$ has a finite uniform entropy integral, it is also sufficient. Theorem~\ref{theo:simple} requires no additional conditions on $\riesz[\psi]$ because
under this condition on $\ff$, the square integrability of $\riesz[\psi]$ implies our condition that $\riesz[\psi]\ff$ is Donsker \citep[Example 2.10.23]{vandervaart-wellner1996:weak-convergence}.

In the context of Example~\ref{exam:mar}, in which $\riesz[\psi](x,w)$ is the inverse probability weight $w/e(x)$ for 
$e(x)=P[W_i=1 \mid X_i=x]$, this means that all we require of $e(x)$ is that
$\E \riesz[\psi]^2(X_i,W_i) = \E 1/e(X_i) < \infty$.
\citet{d2017overlap} highlights the need for a weak condition like this, showing that the usual `strict overlap' condition that $e(x)$ is bounded away from zero
 implies strong constraints on the conditional distribution of $X_i \mid W_i$.
\citet{chen2008semiparametric} discusses the estimation of parameters defined by nonlinear moment conditions  
using overlap assumptions comparable to what we use here.


In simulation settings in which $\riesz[\psi](Z_i)$ has a spiky distribution, our estimator sometimes outperforms a double robust oracle estimator that weights using
the true Riesz representer $\riesz[\psi]$, while a typical double robust estimator performs substantially worse
than this oracle estimator. This suggests that common responses to limited overlap, like changing the estimand \citep[e.g.,][]{crump, li2018balancing} or 
assuming a semiparametric model as in Remark~\ref{rema:semiparametric-models},
may not be needed as frequently with our approach.
\end{rema}

\begin{rema}
\label{rema:riesz-estimation}
Although we assume no regularity conditions on the Riesz representer $\riesz[\psi]$,
 our weights $\hriesz_i$ still estimate it consistently. This is a universal consistency result,
in line with well known results about $k$-nearest neighbors regression and related estimators
\citep{lugosi1995nonparametric,stone1977consistent}. Heuristically, the reason
for this phenomenon is that the Riesz representer \smash{$\riesz[{\psi}]$} is the unique\footnotemark\ weighting function that sets a
population-analogue of $I_{h,\ff}$ to 0; because $\hgamma$ comes close to doing the same, 
it must also approximate \smash{$\riesz[{\psi}]$}. This universal consistency property
is not what controls the bias of our estimator \smash{$\hpsi$}. In fact, the rate of convergence of
\smash{$\hgamma_i$} to $\riesz[{\psi}](X_i)$ is in general too slow for standard arguments for
plugin estimators to apply. However, it plays a key role in understanding why we get efficiency under heteroskedasticity even though
we choose our weights by solving an optimization problem \eqref{eq:aml-primal} that is 
not calibrated to the conditional variance structure of $Y_i$.

\footnotetext{This uniqueness is violated when the tangent space $\cspan \ff$ that $\psi$ acts on is
not the space of all square integrable functions. However, the dual characterization Lemma~\ref{lemma:duality}
shows that our weights must converge to a function in this tangent space, and it follows that they
converge to the unique Riesz representer $\riesz[\psi]$ on this tangent space.}

To understand this phenomenon, observe that under the conditions of Theorem~\ref{theo:simple}, 
the conditional bias term 
$n^{-1}\sum_{i=1}^n h(Z_i, \hm-m) - \hriesz_i (\hm(Z_i)-m(Z_i))$ in our error is $o_P(n^{-1/2})$. It is therefore 
unnecessary to make an optimal bias-variance tradeoff by this sort of calibration
to get efficiency under heteroskedasticity and heteroskedasticity-robust confidence intervals;
the asymptotic behavior of our estimator is determined by the asymptotic behavior of our noise term 
\smash{$n^{-1}\sum_{i=1}^n \hriesz_i \varepsilon_i$} and therefore by the limiting weights $\riesz[\psi](Z_i)$.

For the same reason, it is not necessary to know the error scale $\norm{\hm - m}_{\ff}$ to form asymptotically valid confidence intervals.
We stress that this is an asymptotic statement; in finite samples, there are strong impossibility results
for uniform inference that is adaptive to the scale of an unknown signal \citep{armstrong2015optimal}. %\footnotemark\ 
Furthermore, tuning approaches that estimate and incorporate individual variances $\sigma_i$ into the minimax weighting problem \eqref{eq:aml-primal}
like those discussed in \citet{armstrong2017finite} may offer some finite-sample improvement. 
\end{rema}

\subsection{Comparison with Double-Robust Estimation}
\label{sec:double-robustness}

Perhaps the most popular existing paradigm for building asymptotically efficient estimators 
in our setting is via constructions that first compute stand-alone estimates
\smash{$\hm(\cdot)$} and \smash{$\hriesz[\psi](\cdot)$} for the regression function and the Riesz representer,
and then plug them into the following functional form
\citep{chernozhukov2016locally,newey1994asymptotic,robins1},
\begin{equation}
\label{eq:DR}
\hpsi_{DR} = \frac{1}{n}\sum_{i=1}^n [h(Z_i, \hm) -  \hriesz[\psi](Z_i) \p{\hm(Z_i)-Y_i} ],
\end{equation}
or an asymptotically equivalent expression \citep[e.g.,][]{van2006targeted}.
This estimator has a long history in the context of many specific estimands,
e.g., the aforementioned AIPW estimator for the estimation of a mean with outcomes missing at random \citep*{cassel1976some,robins1994estimation}.
In recent work, \citet*{chernozhukov2018double} describe a general approach of this type,
making use of a novel estimator for the Riesz representer of a functional $\riesz[\psi]$ 
in high dimensions motivated by the Dantzig selector of \citet{candes2007dantzig}.

In considerable generality, this estimator $\hpsi_{DR}$ is efficient when we use sample splitting\footnotemark\ to construct $\hm$ 
and these estimators satisfy \citep{chernozhukov2016double, zheng2011cross}
\begin{equation}
\label{eq:bilinear-error-negligible}
 \frac{1}{n}\sum_{i=1}^n [\hriesz[\psi](Z_i)-\riesz_{\psi}(Z_i)] [ \hm(Z_i)-m(Z_i)] = o_P(n^{-1/2}).
\end{equation}
Taking the Cauchy-Schwarz bound on this bilinear form results in a well-known sufficient condition on
the product of errors, \smash{$\norm{\hriesz[\psi]-\riesz[\psi]}_{\L{2}{\Pn}}\norm{\hm-m}_{\L{2}{\Pn}}$} \smash{$= o_P(n^{-1/2})$}.
This phenomenon, that we can trade off accuracy in how well the two nuisance functions $m$ and $\riesz[\psi]$ are
estimated, is called \emph{double-robustness}. 

\footnotetext{In particular, this result holds if we use the cross-fitting construction of
\citet{schick1986asymptotically}, where separate data folds are used to estimate the nuisance
components \smash{$\hm$} and \smash{$\hriesz[\psi]$} and to compute the expression \eqref{eq:DR} given those estimates.
The three-way sample splitting scheme of \citet{newey2018cross}, discussed below, refines this by using different folds to estimate the two nuisance functions,
and the remaining ones to compute the expression \eqref{eq:DR}.
}

While the estimator $\hpsi_{AML}$ defined in \eqref{eq:aml} shares the form of $\hpsi_{DR}$, it is not designed to be double robust.
The weights $\hgamma$ used in $\hpsi_{AML}$ are optimized for the task of correcting the error of the plugin estimator 
$\psi(\hm)$ when our assumptions on the regression error function $\hm - m$ are correct. When this is the case and the class $\ff$ 
characterizing our uncertainty about this function is sufficiently small (e.g., Donsker), this allows us to be completely robust
to the difficulty of estimating the Riesz representer $\riesz[\psi]$. Our estimator will be efficient essentially because 
the error \smash{$\hriesz-\gamma_{\psi}$} will be sufficiently orthogonal to all functions $f \in \ff$ 
that \eqref{eq:bilinear-error-negligible} will be satisfied uniformly over the class
of possible regression error functions $\hm - m \in \ff$.
As the existence of an estimator \smash{$\hm$}
whose error \smash{$\hm - m$} is tight in the gauge of some Donsker class $\ff$
is equivalent to the existence of an \smash{$o_P(n^{-1/4})$}-consistent estimator of $m$,
relative to the aforementioned sufficient condition on the product of error rates, 
this characterization completely eliminates regularity requirements on the Riesz representer $\riesz[{\psi}]$ 
while requiring the same level of regularity on the regression function \smash{$m$}. 

This type of phenomenon is not unique to our approach. The higher order influence function estimator of \citet{mukherjee2017semiparametric} 
is efficient under the minimal H\"older-type smoothness conditions on $\riesz[\psi]$ and $m$. This includes the case
where either $m$ or $\riesz[\psi]$ admits an \smash{$o_P(n^{-1/4})$}-consistent estimator with no conditions on the other,
as well as possibilities interpolating these in which neither does \citep{robins2009semiparametric}.
Furthermore, \citet{newey2018cross} show that, if $\hm$ and $\hriesz[\psi]$ are 
appropriately tuned series estimators fit using a three-way cross-fitting scheme,  \smash{$\hpsi_{DR}$} is efficient under 
minimal or nearly minimal H\"older-type smoothness conditions. They also show that 
for this $\hm$, a cross-fit plug-in estimator \smash{$n^{-1}\sum_{i=1}^n h(Z_i, \hm)$} will
be efficient if $m$ is H\"older-smooth enough to admit an $o_P(n^{-1/4})$-consistent estimator, and 
beyond this regime exhibits some double robustness --- 
it is also efficient when $m$ is less smooth and $\riesz[\psi]$ is smooth enough.

The use of undersmoothed, i.e., less biased than variable, nuisance estimators 
seems to be an important ingredient in estimators that beat the error rate product bound \citep[see also][]{kennedy2020optimal,van2019efficient}. 
Both here and in \citet{newey2018cross}, $\riesz[\psi]$ is estimated by solving a set of Riesz representer estimating equations \eqref{eq:riesz-sample}
subject to weak regularization or constraints. 
Furthermore, when $\F$ is a ball in a reproducing kernel Hilbert space,
the minimax linear estimator ($\hpsi_{AML}$ with $\hm \equiv 0$) is
equivalently described as a plug-in using a undersmoothed ridge regression estimator $\hm$ \citep[Theorem 22]{kallus2016generalized}.
\citet{hirshberg2019minimax} show that this estimator is efficient essentially whenever $\norm{m}_{\F} < \infty$.


%Thus, while our estimator \eqref{eq:aml} can potentially be seen as an instance of \eqref{eq:DR} because
%our weights $\hgamma_i$ do converge to \smash{$\riesz[{\psi}](Z_i)$},
%the way the two estimators work is very different. Convergence of our weights to the Riesz representer is slow
%and plays only a second-order role in our analysis. The reason our weights succeed in debiasing
%\smash{$\psi(\hm)$} is the form of the optimization problem \eqref{eq:aml-primal}, not our universal
%consistency result. Thus, we often find it more helpful to think of our method in the context of minimax linear
%estimation rather than that of doubly robust methods.%\footnote{It is, however, also possible to use the machinery
%developed here---but tuned differently---to produce an estimator that falls qualitatively in the class of
%doubly robust estimators. Our finite sample bounds show that our weights $\hgamma$ will, if our tuning parameter
%$\sigma$ in \eqref{eq:aml} is allowed to grow with sample size at the correct rate,
%typically give a rate-optimal estimate of the Riesz representer $\hriesz[\psi]$. Thus,
%by varying this parameter $\sigma$ in our estimator \eqref{eq:aml}, we trace out a family
%of estimators including the AMLE and a doubly-robust estimator using a very reasonable estimate of $\hriesz[\psi]$ 
%(see Theorem~\ref{theo:simple-rate}).
%In this paper, we will focus on the AMLE case, deferring the exploration of this continuum and 
%strategies for choosing this tuning parameter $\sigma$ to later work.}

\subsection{Comparison with Minimax Linear and Balancing Estimators}
\label{sec:balancing}

As discussed above, our approach is primarily motivated as a refinement of conditional-on-design minimax linear estimators as
developed and studied by a large community over the past decades
\citep[e.g.,][]{donoho1994statistical, ibragimov1985nonparametric, juditsky2009nonparametric};
however, our focus is on its behavior in a random-design setting,
as in the literature on semiparametrically efficient inference and local asymptotic minimaxity, including results on doubly robust methods
\citep[e.g.,][]{bickel,robins1,van2006targeted}.
The conceptual distinction between these two settings is strong in causal inference and missing data
problems, where in the former we consider an adversary that chooses $m(\cdot)$ having observed the realized covariates and pattern of missing data,
and in the latter we consider an adversary that chooses $m(\cdot)$ having observed no part of the realized data. 

We are aware of three estimators that can be understood as special cases of our augmented
minimax linear estimator \eqref{eq:aml}.
In the case of parameter estimation in high-dimensional linear models, \citet{javanmard2014confidence}
propose a type of debiased lasso that combines a
lasso regression adjustment with weights that debias the $\ell_1$-ball, a convex class known to
capture the error of the lasso; \citet*{athey2016approximate} develop a related
idea for average treatment effect estimation with high-dimensional linear confounding;
and \citet{kallus2016generalized, kallus2018balanced} proposes analogs
for treatment effect estimation and policy evaluation, a special case of Example~\ref{exam:distribution-shift},
that adjust for nonparametric confounding using weights that debias the unit ball of a reproducing kernel Hilbert space.   
The contribution of our paper
relative to this line of work lies in the generality of our results, and also in characterizing the
asymptotic variance of the estimator under heteroskedasticity and proving efficiency
in the fixed-dimensional nonparametric setting. Given heteroskedasticity, the aforementioned papers
prove $\sqrt{n}$-consistency but do not characterize the asymptotic variance directly in terms of the distribution of the data; 
instead, they express the variance in terms of the solution to an optimization problem analogous to \eqref{eq:aml-primal}.


In the special case of mean estimation with outcomes missing at random, the optimization
problem \eqref{eq:aml-primal} takes on a particularly intuitive form, with
\begin{equation}
I_{h,\ff}(\gamma) = \sup_{f \in \ff} \cb{\frac{1}{n} \sum_{i = 1}^n \p{1 - W_i \gamma_i} f(X_i, \, 1)}
\end{equation}
measuring how well the \smash{$\gamma$}-weighted average of $f(x,1)$ over the units with observed outcomes
matches its average over everyone. In other words, the minimax linear weights enforce
``balance'' between these subsamples, which has been emphasized as fundamental to this problem by several authors
including \citet{rosenbaum1983central} and \citet*{hirano2003efficient}. Recently there has been considerable
interest in the use of balancing weights, chosen to control $I_{h,\ff}$ or a variant,
in linear estimators and in augmented linear estimators \eqref{eq:aml} like those we consider here \citep{athey2016approximate,chan2015globally,graham1,graham2,hainmueller,imai2014covariate,kallus2016generalized,ning2017high,wang2017approximate,wong2017kernel,zhao2016covariate,zubizarreta2015stable}. 
In addition to generalizing beyond the missing-at-random problem, our Theorem \ref{theo:simple-rate} provides the sharpest results we are aware of for
balancing-type estimators in this specific problem. 

To do this, we bring together arguments from two strands of the balancing literature.
The first focuses on balancing small finite-dimensional classes, 
and in several instances it has been shown that when tuned so that $I_{h,\ff}(\hgamma)$ is sufficiently small, 
the linear estimator is efficient under strong assumptions on both $m$ and $\riesz[\psi]$ \citep{chan2015globally,fan2016improving,graham1,wang2017approximate}.
The arguments used to establish these results rely on the convergence of $\hriesz$ to $\riesz[\psi]$ at sufficient rate, much like those
used with the estimators discussed in the previous section. 
The second focuses on balancing high or infinite-dimensional classes, 
and in several instances it has been shown that when tuned so that \smash{$I_{h,\ff}(\hgamma) = O_P(n^{-1/2})$},
a level of balance that is attainable under assumptions comparable to ours, the linear estimator is $\sqrt{n}$-consistent and 
the augmented linear estimator is $\sqrt{n}$-consistent and asymptotically unbiased \citep{athey2016approximate,kallus2016generalized,wong2017kernel}. 
The arguments used to establish these results fundamentally rely on balance to bound the estimator's bias, 
and do not fully characterize the estimator's asymptotic distribution. Our argument is a refinement of this one,
using balance to do the bulk of the work, but relying on the convergence of the balancing weights $\hgamma$ to $\riesz[\psi]$ 
to characterize the asymptotic distribution of our estimator and to establish asymptotic unbiasedness under weaker conditions.

\section{Estimating Linear Functionals}
\label{sec:estimating-linear-functionals}

In this section, we give a more general characterization of the behavior of our estimator. We begin by sketching our argument, 
which is based on a decomposition of our estimator's error into a bias-like term and a noise-like term. 
We consider error relative to a sample-average version of our estimand, $\tilde{\psi}(m) = n^{-1}\sum_{i=1}^n h(Z_i,m)$,
as the difference $\psi(m)-\tpsi(m)$ is out of our hands:
\begin{equation}
 \label{eq:error-decomp}
\begin{split}
&\hpsi_{AML} - \tilde{\psi}(m)  
= \frac{1}{n}\sum_{i=1}^n h(Z_i, \hm) - \hgamma_i \p{\hm(Z_i) - Y_i} - h(Z_i, m)  \\
&\quad = \frac{1}{n}\sum_{i=1}^n \underbrace{h(Z_i,\hm-m) - \hgamma_i (\hm-m)(Z_i)}_{\text{bias}} 
  + \underbrace{\hgamma_i \p{Y_i - m(Z_i)}}_{\text{noise}}.
\end{split}
\end{equation}
In Appendix~\ref{sec:finite-sample-proofs}, we prove finite sample bounds on the bias term and 
the difference between the noise term and that of the oracle estimator with weights $\riesz[\psi](Z_i)$.
Our estimator will be asymptotically linear, with the influence function of the oracle estimator,
if both of these quantities are $o_p(n^{-1/2})$. We establish these bounds in three steps.

\paragraph*{Step 1}
We bound \smash{$n^{-1}\sum_{i=1}^n (\hgamma_i - \gamma^{\star}_i)^2$} for $\gamma^{\star}_i = \riesz[\psi](Z_i)$.
To do this,  we work with a dual characterization of our weights $\hgamma_i$ as evaluations $\hriesz[\psi](Z_i)$ of a penalized least squares estimate
of the Riesz representer $\riesz[\psi]$. 
\begin{equation}
\label{eq:dual}
\begin{split}
\hriesz[\psi] &= \argmin_{g} \set*{ \norm{g}_{L_2(\Pn)}^2 - \frac{2}{n}\sum_{i=1}^n h(Z_i, g) + \frac{\sigma^2}{n}\norm{g}_{\ff}^2 } \\
	      &= \argmin_{g} \set*{  \norm{g - \riesz[\psi]}_{L_2(\Pn)}^2 - \frac{2}{n}\sum_{i=1}^n h_{\riesz[\psi]}(Z_i, g) + \frac{\sigma^2}{n}\norm{g}_{\ff}^2}
\end{split}
\end{equation}
where $h_{\riesz}(z,f) = h(z,f) - \riesz(z)f(z)$. Here the term involving $h_{\riesz[\psi]}$ plays the role of `noise' in our least squares problem, 
as it has mean zero for any function $f \in \mm$. The first characterization is established using strong duality in Lemma~\ref{lemma:duality}
and the second is derived by completing the square.

\paragraph*{Step 2} We bound the difference between our noise term and that of the oracle estimator,
$n^{-1}\sum_{i=1}^n (\hgamma_i - \gamma^{\star}_i)(Y_i - m(Z_i))$, using the result of Step 1.


\paragraph*{Step 3} We bound our bias term by $\norm{\hm - m}_{\ff}I_{h, \ff}(\hgamma)$,
where as a consequence of the definition of our weights $\hgamma$ in \eqref{eq:aml-primal}, 
\begin{equation}
\label{eq:gamma-optimality-cond}
I_{h, \ff}^2(\hgamma) \le I_{h,\ff}^2(\gamma^{\star}) + \frac{\sigma^2}{n^2} \sum_{i=1}^n \p{{\gamma^{\star}_i}^2 - \hgamma_i^2}. 
\end{equation}
The first term on the right side can be characterized using empirical process techniques, as $I_{h,\ff}(\gamma^{\star})$ 
is the supremum of the empirical measure indexed by the class of mean-zero functions 
$h_{\riesz[\psi]}(\cdot,\ff)$. And the second term can be shown, using 
some simple arithmetic, to be $o_p(n^{-1})$ when $\hriesz$ is consistent. 
Thus, our bias term will be bounded by $\norm{\hm - m}_{\ff}[I_{\ff}(\gamma^{\star}) + o_p(n^{-1/2})]$.

\paragraph*{Step 3'}
We refine this bound to take advantage of the consistency of $\hm$. 
To do this, we show that our estimator behaves essentially the same way as an oracle
that knows a sharp bound $\norm{\hm - m}_{L_2(\Pn)} \le \rho$ on our regression error
and uses a refined model class \smash{$\F_{\rho}' = \{ f : \norm{f}_{\F}^2 +$} \smash{$ \rho^{-2}\norm{f}_{L_2(\Pn)}^2$ $\le 1\}$}
in place of $\F$. The key insight is that this substitution changes the dual \eqref{eq:dual} and its solution $\hgamma$ very little,
so replacing $\F$ with $\F_{\rho}'$ in our bound \eqref{eq:gamma-optimality-cond} yields an inequality that is approximately satisfied.
Given the assumptions of Theorem~\ref{theo:simple}, 
the resulting refined bias term bound will be \smash{$o_p(n^{-1/2})$}, as 
\smash{$\norm{\hm - m}_{\F_{\rho}'} = O_p(1)$} for $\rho \to 0$ given our
tightness and consistency assumptions \eqref{eq:consistency-properties} and
\smash{$I_{h, \F_{\rho}'}(\gammaipw) = o_p(n^{-1/2})$} when $\rho \to 0$
given our Donskerity and equicontinuity assumptions.\\

Following a few definitions, we will state our main result.
Due to space constraints, all proofs are given in the appendix.

\subsection{Finite sample results}
\label{sec:main-results}
To characterize the size of a set $\GG$, we will use its \emph{Rademacher complexity}, $R_n(\GG) = \E \sup_{g \in \GG}\abs{n^{-1}\sum_{i=1}^n \epsilon_i g(Z_i)}$ where $\epsilon_i = \pm 1$ each with probability $1/2$
independently and independently of the sequence $Z_1 \ldots Z_n$, as well as the uniform bound \smash{$M_{\infty}(\GG)$} \smash{$= \sup_{g \in \GG} \norm{g}_{\infty}$}.
Letting $h_{\gamma}(z,f)=h(z,f)-\gamma(z)f(z)$, our bound depends
on the Rademacher complexity of the classes $\ff_r$,
$h_{\riesz[\psi]}(\cdot,\ff_r)$, and $h_{\tilde \riesz}(\cdot,\ff_r)$ 
for a regularized approximation $\tilde\riesz$ to $\riesz[\psi]$. The regularity of that approximation, 
and therefore the regularity of $\riesz[\psi]$ itself, will be a factor in a higher order term.
Without loss of generality, we will write our weights as function evaluations $\hriesz_i=\hriesz(Z_i)$,
and we will write $a \vee b$ and $a \wedge b$ respectively for the maximum and minimum of $a$ and $b$
and $a \lesssim b$ and $a \ll b$ meaning $a = O(b)$ and $a = o(b)$.

\begin{theo}
\label{theo:simple-rate}
In the setting described in Section~\ref{sec:simple-setting}, 
consider the estimator $\hpsi_{AML}$ defined in \eqref{eq:aml} with $\sigma > 0$ and $\ff$ 
a uniformly bounded absolutely convex set of functions for which $h(\cdot,\ff)$ is pointwise bounded.
Let $\riesz[\psi]$ be the Riesz representer of $\psi$ on the tangent space $\cspan \ff$ and 
$\tilde \riesz$ minimize \smash{$\norm{\riesz[\psi] - \riesz}_{L_2(Q)}^2 + (\sigma^2/n)\norm{\riesz}_{\F}^2\ $} for $Q=\P$ or $Q=\Pn$.
If $\F$ is \smash{$\norm{\cdot}_{L_2(Q)}$}-closed,
this argmin exists and is unique, and for any positive $\delta$, on the intersection of an event of probability \smash{$1-4\delta-3\exp(-c_2nr_Q^2/M_{\infty}^2(\F))$}
and one on which $\norm{\hm - m}_{\F} \le s_{\F}$ and $\norm{\hm - m}_{L_2(\Pn)} \le s_{L_2(\Pn)}$,
\begin{equation}
\label{eq:sample-riesz-rate} 
\begin{aligned}
&\norm{\hriesz - \tilde\riesz}_{L_2(\Pn)}^2 
\le 6\p{nr^4/\sigma^2 + \norm{\tilde\riesz}_{\ff}r^2} \vee 8 r^2 \quad \text{ for } \quad  r = r_Q \vee r_M, \\
r_Q &= \inf\set{r > 0 : R_n(\F_{c_0r}) \le c_1 r^2/M_{\infty}(\F)},\\
r_M &= \begin{cases} \inf\set{ r > 0 : R_n(h_{\tilde\riesz}(\cdot,\F_r))  \ \le \ \delta r^2/2} & \text{ for } \quad Q=\P,  \\
  		     \inf\set{ r > 0 : R_n(h_{\riesz[\psi]}(\cdot,\F_r)) \le \delta r^2/2} & \text{ for } \quad Q=\Pn, \end{cases}  \\
\end{aligned}
\end{equation}
and for $\influence_{\riesz}(y,z) = h(z,m) - \riesz(z)(m(z) - y) - \psi(m)$
and any positive $\epsilon \le 9/16$,
\begin{equation}
\label{eq:remainder-rate} 
\begin{aligned}
&\sqrt{n}\abs{\hpsi_{AML} - \psi(m) - n^{-1}\sum_{i=1}^n \influence_{\tilde\riesz}(Y_i,Z_i)} 
\le (1/\sqrt{\delta}) \norm{v}_{\infty} \norm{\hriesz - \tilde\riesz}_{L_2(\Pn)} \\ 
&\quad +  \sqrt{2n}s_{\F}\ \phi\p{\frac{s_{L_2(\Pn)}}{s_{\F}} \vee c_0 r \vee \frac{6 \sigma}{\epsilon \sqrt{n}}} \p{1+2\epsilon\ /\ \sqrt{1-\epsilon^2/36}}\\
&\quad +  \sqrt{2}\sigma s_{\F} \p{\norm{\riesz[\psi]}_{L_2(\Pn)} \wedge \norm{\riesz[\psi]}_{L_2(\Pn)}^{1/2} \norm{\hgamma-\riesz[\psi]}_{L_2(\Pn)}^{1/2}}\ /\ \sqrt{1-\epsilon^2/36}. 
\end{aligned}
\end{equation}
Here $c_0 \ldots c_2$ are universal constants and 
\begin{align*} 
\phi(\rho) &= \frac{2R_n(h_{\riesz[\psi]}(\cdot, \F_{\sqrt{2}\rho}))}{\delta} 
	   \vee \frac{216}{\epsilon^2}\p{r^2 + \frac{\sigma^2 \norm{\tilde\riesz}_{\ff}}{n}}
	   \vee \frac{ 36\sigma^2 \norm{\riesz[\psi]}_{L_2(\P)}}{\epsilon^2\sqrt{\delta} c_0 n r} \vee \frac{288\sigma^2}{\epsilon^2 n}.
\end{align*}
\end{theo}
Generalization to classes $\F$ that are not uniformly bounded is discussed in Appendix~\ref{sec:not-uniformly-bounded}.
We will briefly interpret this result by considering several asymptotic settings. 
Throughout, we will use the bounds above for $Q=\Pn$ and
the bound $\norm{\tilde\riesz}_{\F} \le (\sqrt{n}/\sigma)\norm{\riesz[\psi]}_{L_2(\Pn)}$.\footnote{
This bound holds because \smash{$\norm{\riesz - \riesz[\psi]}_{L_2(\Pn)}^2 + (\sigma^2/n)\norm{\riesz}_\F^2$}
is smaller at its minimizer than at $\riesz=0$.} 

\subsection{Nonparametric asymptotics}
\label{sec:nonparametric-asymptotics}
In the asymptotic setting we considered in the introduction,
in which the distribution $\P$, the class $\F$, and the tuning parameter $\sigma$ are fixed,
this result implies Theorem~\ref{theo:simple}.
The key steps of the proof are as follows. 
\begin{enumerate}
\item 
As $\riesz[\psi]$ is fixed, the regularized approximation $\tilde\riesz$ converges to $\riesz[\psi]$
in \smash{$\norm{\cdot}_{L_2(\Pn)}$} as the weight of regularization $\sigma^2/n \to 0$, so our `influence function' $\influence_{\tilde\riesz}$ converges to 
the limit $\influence_{\riesz[\psi]}$. 

\item
Given our tightness and consistency assumptions
\eqref{eq:consistency-properties}, we can take $s_{\F} \ge \norm{\hm-m}_{\F}$ to be of constant order and \smash{$s_{L_2(\Pn)} \ge \norm{\hm - m}_{L_2(\Pn)}$} to be converging to zero on a high probability event.
Thus, our remainder bound \eqref{eq:remainder-rate} goes to zero if 
$\sqrt{n}\phi(s_n) \to 0$ for any sequence $s_n$ converging to zero and \smash{$r \ll n^{-1/4}$} and therefore $\norm{\hriesz - \tilde\riesz}_{L_2(\Pn)} \to 0$ (via \ref{eq:sample-riesz-rate}).

\item
Both of these conditions hold if
$\lim_{t \to 0}\sqrt{n}R_n(\F_{t}) = \lim_{t \to 0}\sqrt{n}R_n(h_{\riesz[\psi]}(\cdot,\F_{t})) = 0$.
The first limit is zero because $\F$ is Donsker. 
And the second is zero for the same reason, as
$h_{\riesz[\psi]}(\cdot,\F_t) \subseteq \HH_{\omega(t)}$ where $\HH = h_{\riesz[\psi]}(\cdot,\F)$
is Donsker and $\omega(t)=\sup_{f \in \F_t}\norm{h_{\riesz[\psi]}(\cdot, f)}_{L_2(\P)}$ 
satisfies $\lim_{t \to 0}\omega(t) = 0$ under our equicontinuity and uniform boundedness assumptions. 
\end{enumerate}

\subsection{High dimensional asymptotics}
\label{sec:high-dim-asymptotics}
Now we consider estimation of the mean with outcomes missing at random (Example~\ref{exam:mar}) in the high dimensional linear model,
i.e., with $m(x,w)=wx^T\beta$ for $\beta \in \R^p$. In this setting, $\hpsi_{AML}$ for the class $\F = \set{m(x,w)=wx^T\beta : \norm{\beta}_{1} \le 1}$
is the ``approximate residual balancing'' estimator proposed in \citet{athey2016approximate}. We can derive from 
Theorem~\ref{theo:simple-rate} the main result from that paper: that this estimator is $\sqrt{n}$-consistent 
and an associated $t$-statistic is asymptotically standard normal.
Furthermore, Theorem~\ref{theo:simple-rate} also characterizes the limit of the weights $\hriesz$, and therefore the asymptotic variance of the estimator,
as a simple function of the distribution $\P$.


Specifically, suppose the coordinates of the covariates are bounded, 
$\norm{\riesz[\psi]}_{L_2(\P)}$ is bounded  (an overlap assumption), 
and \smash{$\norm{\hat{\beta} - \beta}_1 = O_P(s_\F)$} for \smash{$s_\F \ll 1/\sqrt{\log(p)}$.}
As discussed in \citet{athey2016approximate}, when \smash{$\hat \beta$} is estimated via the lasso, 
the third holds under standard sparsity and restricted eigenvalue conditions.
Then for any choice of tuning parameter $\sigma$ satisfying \smash{$\sqrt{\log(p)} \ll \sigma \ll 1/s_\F$},
 \smash{$\hat{\psi}_{AML} - \psi(m)$} is first-order equivalent to $n^{-1}\sum_{i=1}^n \influence_{\tilde\riesz}(Y_i,Z_i)$,
as our remainder bound \eqref{eq:remainder-rate} is vanishingly small.


To check this, note that by the finite class lemma of \citet[Lemma 5.2]{massart2000some},
\[ R_n(\F) \vee R_n(h_{\riesz[\psi]}(\cdot,\F)) \lesssim (1 \vee \norm{\riesz[\psi]}_{L_2(\P)})\sqrt{\log(p)/n}. \]
Thus, \eqref{eq:sample-riesz-rate} implies the convergence of $\hriesz$ to $\tilde\riesz$,
as $r^2 \lesssim R_n(\F) \vee R_n(h_{\riesz[\psi]}(\cdot,\F))$. 
It follows that the first term in our remainder bound \eqref{eq:remainder-rate} vanishes.
The second term vanishes as well,
as it is proportional to \smash{$\sqrt{n} s_\F \cdot \phi(x) \ll \sqrt{n/\log(p)} \cdot \phi(x)$} for some $x$ 
and \smash{$\phi(x) \lesssim R_n(h_{\riesz[\psi]}(\cdot,\F)) \lesssim \sqrt{\log(p)/n}$.}
So does the third term, as $\sigma s_\F \ll (1/s_\F) s_\F$.



\subsection{Sieve asymptotics}
\label{sec:sieve-asymptotics}
In the sieve asymptotics often considered \citep[e.g.,][]{newey2018cross, wang2017approximate},
we do not characterize the regression function $m$ by membership in a set $\F$ directly,
but instead by the existence of an element $\tilde m \in \F$ that approximates 
it with a certain degree of accuracy.
Our argument requires modification for this asymptotic setting,
as our bound $\norm{\hm - m}_{\ff}I_{h, \ff}(\hgamma)$ on the `bias term' 
in our error decomposition \eqref{eq:error-decomp} 
will tend to be vacuous: when $\hm - m \not \in \vspan \F$, $\norm{\hm - m}_{\F}=\infty$. 
We can modify our error decomposition as follows.
\begin{equation*}
\begin{split}
&\hpsi_{AML} - \tilde{\psi}(m)  
= \frac{1}{n}\sum_{i=1}^n h(Z_i,\hm-\tilde m) - \hgamma_i (\hm-\tilde m)(Z_i) 
+ \frac{1}{n}\sum_{i=1}^n \hgamma_i \p{Y_i - m(Z_i)} \\
&\quad 
+ \frac{1}{n}\sum_{i=1}^n h_{\riesz[\psi]}(Z_i,\tilde m-m)
+ \frac{1}{n}\sum_{i=1}^n (\riesz[\psi] - \tilde\riesz)(\tilde m-m)(Z_i)
+ \frac{1}{n}\sum_{i=1}^n (\tilde\riesz - \hriesz)(\tilde m-m)(Z_i).
\end{split}
\end{equation*}
The sum of the first two terms tends to converge to the influence function average $n^{-1}\sum_{i=1}^n \influence_{\tilde\riesz}(Y_i,Z_i)$.
The proof of 
Theorem~\ref{theo:simple-rate} implies the remainder satisfies the bound \eqref{eq:remainder-rate} for $s_\F \ge \norm{\hm-\tilde m}_\F$ and $s_{L_2(\Pn)} \ge \norm{\hm - \tilde m}_{L_2(\Pn)}$. We briefly discuss the remaining terms.

The third term is the sample average of 
a deterministic function with mean zero. 
It is negligible if our approximation is consistent in 
the sense that $\E[h_{\riesz[\psi]}^2(Z_i, \tilde m - m)] \to 0$. 


The fourth term is the  the empirical inner product of two approximation errors.
It is comparable to the corresponding population inner product 
$\E[(\riesz[\psi]-\tilde\riesz)(\tilde m-m)(Z_i)]$, 
which can be analyzed deterministically using properties of the approximations.

The fifth term is the empirical inner product between the approximation error $\tilde m - m$
and $\tilde\riesz - \hriesz$, which satisfies  
$\norm{\tilde\riesz - \hriesz}_{\F} = O_P( nr^2/\sigma^2 + \norm{\tilde\riesz}_{\F})$ for $r$ as in \eqref{eq:sample-riesz-rate}
(see Appendix~\ref{sec:consistency}). We can sometimes get a useful bound on this inner product
based on the approximate orthogonality of $\tilde m - m$ to functions in $\F$.
This is natural when $\F$ is a subspace and $\tilde m$ is the $L_2(\P)$ 
orthogonal projection of $m$ onto it, as in that case $\tilde m - m$ is orthogonal to any element of $\vspan \F$.

\citet{newey2018cross}, working with subspaces $\F$ of finite sample-size-dependent dimension,
used techniques along these lines to characterize a cross-fit variant of the estimator we discuss, 
showing efficiency under near-minimal assumptions. The extension of their argument is a
promising area for future work \citep[see also][]{kennedy2020optimal}.  
\ \\

We conclude the section with a few practical considerations.

\subsection{The role of the tuning parameter $\sigma$}
\label{sec:tuning-sigma}
We generally recommend that the tuning parameter $\sigma$ be chosen without consideration of sample size. 
The simple heuristic $\sigma^2 \approx \max_{i \le n}\Var{Y_i \mid Z_i}$ arises 
from the minimax interpretation of our estimator, in which $\sigma^2$ is a bound on the conditional variance.\footnotemark\ 
However, \smash{$\hpsi_{AML}$} is fairly robust to our choice of $\sigma$, and Theorem~\ref{theo:simple-rate} 
justifies a wide range of choices.

\footnotetext{In our minimax framework in Section~\ref{sec:amle}, we also assume that $\norm{\hm - m}_{\ff} \le 1$.
If we instead believe that $\norm{\hm - m}_{\ff} \approx \alpha$, our heuristic suggests $\sigma^2 \approx \alpha^{-2} \max_{i \le n}\Var{Y_i \mid Z_i}$.}

To consider the impact of $\sigma$, we look at the role it plays in the dual characterization \eqref{eq:dual} of our weights.
As discussed above, this is a penalized least squares problem for estimating $\riesz[\psi]$.
From this perspective, taking $\sigma$ to be of constant order is regularizing very weakly, 
and we can improve the rate of convergence of $\hriesz$ to our regularized approximation $\tilde\riesz$ by increasing $\sigma$. On the other hand, 
consideration of the primal \eqref{eq:aml-primal} shows that this comes at a cost in terms of the 
maximal conditional bias $I_{h,\ff}(\hriesz)$, and if we have confidence that $\hm-m$ is in a small class \smash{$\ff$},
we can decrease $\sigma$ so that \smash{$I_{h,\ff}(\hriesz)$} and therefore our bias is zero or nearly zero.  
Recalling our discussion in Section~\ref{sec:double-robustness}, our choice of $\sigma$ essentially trades off between two properties of 
the error $\hriesz[\psi]-\riesz[\psi]$: its degree of orthogonality to the specific functions in $\ff$, and its degree of `orthogonality' to all 
square integrable functions, i.e., its magnitude $\norm{\hriesz - \riesz[\psi]}_{L_2(\P)}$.


When we choose $\sigma$ proportional to $\sqrt{n}r$, $\hpsi_{AML}$ is essentially a standard doubly robust estimator.
Our estimate of $\riesz[\psi]$ is not undersmoothed as discussed in Section~\ref{sec:double-robustness};
with this tuning, if $\norm{\riesz[\psi]}_{\ff} < \infty$, our weights converge to $\riesz[\psi]$ in empirical mean square at the rate $r$,
typically the minimax rate for estimating $\riesz[\psi]$ satisfying $\norm{\riesz[\psi]}_{\ff} < \infty$ 
(see Appendix~\ref{sec:estimating-riesz-optimal}).
The asymptotic linearity of \smash{$\hpsi_{AML}$} may then follow from the rate-product condition \smash{$\norm{\hriesz[\psi] - \riesz[\psi]}_{L_2(\Pn)}$} \smash{$\norm{\hm - m}_{L_2(\Pn)} = o_P(n^{-1/2})$},
 which is a sufficient condition when we use sample splitting to fit $\hm$.\footnotemark\ 
However, to improve our rate of convergence, we sacrifice orthogonality of $\hriesz[\psi] - \riesz[\psi]$ to possible realizations of $\hm - m$ in $\ff$. 
This makes our estimator sensitive to the rate of convergence of $\hm-m$.
We see this in our bound \eqref{eq:remainder-rate}; the term proportional to $\sigma$ will be large. 

\footnotetext{It is common to use sample splitting to fit $\hriesz[\psi]$ as well. Our bound \eqref{eq:sample-riesz-rate}
does not justify this, as it concerns empirical mean squared error on the sample used to estimate $\hriesz[\psi]$.
However, in the course of our proof in Appendix~\ref{sec:finite-sample-proofs}, we show that with this tuning,
$\hriesz[\psi]$ converges to $\riesz[\psi]$ in population mean square at the rate $r$, which is sufficient.} 
\subsection{Flexible regression adjustments and cross-fitting}
\label{sec:sample-splitting}
In some applications, we may want to base our regression adjustment on flexible, adaptive methods like boosting, random forests, or neural networks.
In this case, it may be hard to argue that $\norm{\hm - m}_{\ff} = O_P(1)$ because $\hm$ itself is irregular.
And the violation of this assumption may result in bias. For example, when we take $\ff$ to be a class of smooth functions, 
the weights $\hgamma$ that we use in $\hpsi_{AML}$ will control its bias only when $\hm-m$ is smooth. In this sense,
a nonsmooth estimator $\hm$ is incompatible with this smooth class $\ff$. This problem is easy to fix,
as we can ensure compatibility for any estimator $\hm$ simply by including it in $\ff$. A natural approach 
is to choose a class $\GG$ intended to capture $m$, and let $\ff$ be the absolutely convex hull of $\hm - \GG$. 
For this class, $\norm{\hm - m}_{\ff} \le \norm{m}_{\GG}$.

This set $\ff$ is random, presumably depending on $Y_1 \ldots Y_n$ through $\hm$,
and a problem arises because of the dependence this induces between $\hgamma_i$ and $Y_i$:
the `noise term' in \eqref{eq:error-decomp} can have nonzero mean. We can sidestep this problem by cross-fitting \citep{schick1986asymptotically}, i.e.,
fitting $\hm$ using a subsample of our observations, and defining $\hpsi_{AML}$ in terms of it on the remaining observations.
We will call the former sample the \emph{auxiliary sample} and the latter the \emph{estimation sample}. 
Asymptotic linearity can be established by Theorem~\ref{theo:simple-rate}, applied conditionally on the auxiliary sample.
We get efficiency, under the conditions stated in Theorem~\ref{theo:simple}, by averaging over multiple splits of the sample.

We can generalize this construction by training multiple candidate estimators $\hm_1 \ldots \hm_K$ on the auxiliary sample
and taking $\ff$ to be the absolutely convex hull of $\set{\hm_1 \ldots \hm_K} - \GG$.
We then define $\hpsi_{AML}$ using an estimator $\hm$ chosen from $\hm_1 \ldots \hm_K$ 
or their absolutely convex hull,
e.g., by minimizing empirical mean squared error or a targeted 
loss function \citep[see e.g.,][]{juditsky2000functional, van2003unified}. In addition 
to allowing irregular regression estimators $\hm$, this approach offers robustness to the irregularity 
of the regression function $m$ itself; recalling Section~\ref{sec:sieve-asymptotics}, 
$\norm{\hm - \tilde m}_{\ff}$ and $\norm{\tilde m - m}_{L_2(\Pn)}$
will be small for some $\tilde m$ when $m$ is approximated well by a function in $\GG$ or in $\hm_1 \ldots \hm_K$. 
In ideal conditions, the theorem below justifies the use of up to 
$K = o(n^{1/(2+\alpha)})$ candidates when $\HH \in \set{ \GG,\ \riesz[\psi]\GG,\ h(\cdot,\GG)}$
satisfy the metric entropy bound \smash{$\log \hat N(\HH,\tau) \le \tau^{-\alpha}$} for $\alpha < 2$. 

\begin{theo}
\label{theo:simple-hull}
In the setting of Theorem~\ref{theo:simple},
let $\GG \subseteq \mm $ be an absolutely convex and pointwise closed set,
and let $\ff_n$ be the absolutely convex hull of $\set{m_1 \ldots m_{K_n}} - \GG$
for $m_1 \ldots m_{K_n} \in \cspan \GG$.
Define \smash{$\hpsi_{AML}$} as in \eqref{eq:aml} with $\ff =\ff_n$.
It is asymptotically linear, satisfying \eqref{eq:asymptotic-linearity} with $\riesz[\psi]$ denoting the Riesz representer of $\psi(\cdot)$ on the tangent space $\cspan\GG$, if
\begin{enumerate}
\item $\norm{\hm - m}_{\ff_n} = O_P(1)$ and $\norm{\hm - m}_{L_2(\Pn)} = O_P(s_n)$ \ for \ $s_n \to 0$ 
\item for all $\chi \in \set{ f \to f,\ f \to \riesz[\psi]f,\ f \to h(\cdot,f)}$,
\begin{enumerate}
\item $\chi(\GG)$ is Donsker, 
\item $\sup_{f \in \ff_n}\norm{\chi(f)}_{L_p(\P)}$ is bounded uniformly in $n$ for some $p \in (2,\infty]$, 
\item when $a_n \to 0$ sufficiently slowly,
\[ (K_n+1)\hat N(\chi(\GG),\ a_n \log(K_n+1)^{-1/2}) = o_P(\omega'_{\chi,\ff_n}(n^{-1/4} \vee s_n)^{-2}). \]
\end{enumerate}
\end{enumerate}
Here $\hat N(\HH, \tau)$ is the minimal number of $\norm{\cdot}_{L_2(\Pn)}$-balls of radius $\tau$ covering $\HH$ and
\[ \omega_{\chi,\ff_n}'(r) = \omega_{\chi,\ff_n}(r) \vee r^{(p-2)/(p-1)} \ \text{ where }\
 \omega_{\chi,\ff_n}(r) =\sup_{f \in \ff_n : \norm{f}_{L_2(\P)} \le r} \norm{\chi(f)}_{L_2(\P)}. \]

\end{theo}

Candidates $\hm_1 \ldots \hm_K$ need not be good estimators of $m$ individually. We may benefit, for example, 
from including indicators for strata of estimates of $\riesz[\psi]$ and $m$, motivated 
by the ideas of propensity score and prognostic score stratification in causal inference \citep{rosenbaum1984reducing}.


\begin{rema}
\label{rema:hull-k-constant}
In the case most similar to that of Theorem~\ref{theo:simple}, in which $K_n = O(1)$
and \smash{$\sup_{f \in \ff_n}\norm{f}_{\infty}$} is bounded uniformly in $n$, the assumptions of Theorem~\ref{theo:simple-hull} essentially 
reduce to those of Theorem~\ref{theo:simple} and additional $L_p$ boundedness assumptions on $\riesz[\psi]\ff_n$ and $h(\cdot,\ff_n)$
from (2b). In particular, for any $s_n \to 0$, (2c) is implied by the equicontinuity of $h(Z,\cdot)$ on $\ff_n$ in the sense that $\lim_{r \to 0}\sup_{n}\omega_{\chi,\ff_n}(r)=0$ for $\chi(f)=h(\cdot,f)$. 
\end{rema}


\section{Estimating the Average Partial Effect in a Conditionally Linear Outcome Model}
\label{sec:ape}

As a concrete instance of our approach, we consider the problem
of estimating an average partial effect, assuming a conditionally linear treatment effect model.
A statistician observes features $X \in \xx$, a treatment dose $W \in \RR$, and an outcome
$Y \in \RR$ and wants to estimate $\psi$, where
\begin{equation}
\label{eq:ape_again}
\psi = \EE{\tau(X)} \ \text{ assuming } \ \EE{Y \cond X = x, \, W = w} = \mu(x) + w \,\tau(x).
\end{equation}
By Theorem~\ref{theo:simple}, our AML estimator will be efficient for
$\psi$ under regularity conditions when $\Var{Y_i \cond X_i, \, W_i} = v(X_i)$ is only a function of $X_i$.

In the classical case of an unconfounded binary
treatment, the model \eqref{eq:ape_again} is general and the estimand $\psi$ corresponds
to the average treatment effect \citep{rosenbaum1983central,imbens2015causal}. At the other
extreme, if $W$ is real valued but $\tau(x) = \tau$ is constrained not to depend on $x$, then
\eqref{eq:ape_again} reduces to the partially linear model as studied by \citet{robinson1988root}.
The specific model \eqref{eq:ape_again} has recently been studied by \citet*{athey2016generalized}, \citet*{graham2018semiparametrically},
and \citet*{zhao2017selective}.
We consider the motivation for \eqref{eq:ape_again} in Section \ref{sec:application} in the
context a real-world application; here, we focus on estimating $\psi$ in this model.

Both $\mu(\cdot)$ and $\tau(\cdot)$ in the model \eqref{eq:ape_again} are assumed to have
finite gauge with respect to an absolutely convex class $\HH$, and we define
\begin{equation}
\label{eq:F_ape}
\F_{\HH} = \cb{m : m(x, \, w) = \mu(x) + w \tau(x), \ \Norm{\mu}_\HH^2 + \Norm{\tau}_\HH^2 \leq 1}.
\end{equation}
We can simplify the definition \eqref{eq:aml-primal} of the minimax weights for this class. 
\begin{equation}
\label{eq:ape_gamma}
\begin{split}
\hgamma = \argmin_{\gamma \in \R^n}
\sup_{\mu \in \HH}\sqb{\frac{1}{n} \sum_{i = 1}^n \gamma_i \mu(X_i)}^2 
+ \sup_{\tau \in \HH}\sqb{\frac{1}{n} \sum_{i = 1}^n \p{W_i\gamma_i - 1} \tau(X_i)}^2 + \frac{\sigma^2 \norm{\gamma}^2}{n^2}.
\end{split}
\end{equation}
Given these weights, the augmented minimax linear estimator is  
\begin{equation}
\label{eq:ape_aml}
\hpsi_{AML} = \frac{1}{n} \sum_{i = 1}^n \p{\htau(X_i) - \hgamma_i \p{\hmu(X_i) + W_i \htau(X_i) - Y_i}}.
\end{equation}
Our formal results above give conditions under which it is asymptotically efficient.
In this section, our goal is to explore the behavior of this estimator empirically.
For comparison, we introduce some alternatives.
The first is the minimax linear estimator $\hpsi_{MLIN} = n^{-1}\sum_{i=1}^n \hgamma_i Y_i$,
i.e., $\hpsi_{AML}$ with $\hm \equiv 0$.
The others are variants of the doubly robust estimator \smash{$\hpsi_{DR}$.}
In this setting, the Riesz representer has the form $\riesz[\psi](x, w) = (w - e(x)) / v_w(x)$ with
$e(x) = \EE{W \cond X = x}$ and $v_w(x) = \Var{W \cond X = x}$, so we consider 
a natural doubly robust estimator based on plug-in estimates of these quantities,\footnote{For example,
a random forest version of this estimator is available in the \texttt{grf} package of
\citet*{athey2016generalized}. In the binary treatment assignment case $W_i \in \cb{0, \, 1}$,
we know that $v_w(x) = e(x)(1 - e(x))$; and if we set \smash{$\hv_w(x) = \he(x)(1 - \he(x))$}, then the estimator
in \eqref{eq:ape_dr} is equivalent to the augmented inverse-propensity weighted estimator of
\citet*{robins1994estimation}. For more general $W_i$, however, $v_w(x)$ is not necessarily determined by $e(x)$
and so we need to estimate it separately.}
\begin{equation}
\label{eq:ape_dr}
\hpsi_{DR} = \frac{1}{n} \sum_{i = 1}^n \p{\htau(X_i) - \p{\frac{W_i - \he(X_i)}{\hv_w(X_i)}} \p{\hmu(X_i) + W_i \htau(X_i) - Y_i}}.
\end{equation}
Below, we numerically compare the relative merits of minimax linear, augmented
minimax linear, and plug-in doubly robust estimation of the average partial effect.

\subsection{A Simulation Study}
\label{sec:simu}

To better understand the merits of different approaches to average partial effect estimation, we
conduct a simulation study. 
As baselines, we consider the {\bf plug-in doubly robust estimator} defined in \eqref{eq:ape_dr}, 
where $\he(\cdot)$ and $\hv_w(\cdot)$ are fit separately,
and an {\bf oracle doubly robust estimator} that uses the same
functional form \eqref{eq:ape_dr} but with oracle values of $e(X_i)$ and $v_w(X_i)$.
We compare these baselines to an {\bf augmented minimax linear estimator} (AML) 
that uses minimax linear weights for a class $\ff_{\hh}$ as described in \eqref{eq:ape_aml}, as well as an 
{\bf augmented minimax linear estimator over an extended class} (AML+), a variant 
that uses the same functional form but with the minimax linear weights for an extended class $\ff_{\hh_+}$
that includes a set of estimated functions. We also consider the simpler {\bf minimax linear estimator}
for each class. We provide further implementation details below.


%\subsubsection{Methods under Comparison}
%\label{sec:methods}

\subsubsection{Construction of Augmented Minimax Linear Estimators}

We first describe how we implement our approach,
an augmented minimax linear estimator for the class $\ff_{\hh}$ described in the section above \eqref{eq:F_ape}.
We take $\hh$ to be the absolutely convex hull of a mean-square summable set of basis functions as described in Remark~\ref{rema:consistency}.
Specifically, we use a basis sequence $\phi_j=a_j\phi_j'$, where $\phi_j'$ are $d$-dimensional
interactions of Hermite polynomials that are orthonormal with respect to the standard normal
distribution. The sequence of weights $\cb{a_j}$ varies with order $k$ of the polynomial $\phi_j$;
\smash{$a_j =1/(k\sqrt{n_{k,d}})$} where $n_{k,d}$ is the number of terms of order $k$.
Observe that \smash{$\sum_{j=1}^{\infty} a_j^2$} \smash{$=\sum_{k=1}^{\infty}1/k^2 < \infty$} and therefore \smash{$\sum_{j=1}^{\infty}\E \phi_j^2(X) < \infty$} for standard normal $X$ or $X$ with bounded density with respect to the standard normal.


Following our discussion in Remark \ref{rema:consistency}, we take
an $\ell_1$-penalized least squares approach to estimating the regression function $m$.
Rather than using a fully nonparametric estimate $\hat m(x,w)$,
which would not be in our class $\ff_{\hh}$, 
we fit a conditionally linear model $\hmu(x) + w\htau(x)$
using the $R$-lasso method proposed by \citet{nie2017learning}.
To do this, we first estimate the marginal response function \smash{$r(x)=\EE{Y_i \cond X_i = x}$} and \smash{$e(x)$}
via a cross-validated lasso \citep{tibshirani1996regression}
on the basis $\phi(x)$.\footnotemark\ We then fit $\tau_{\beta}(x)= \phi(x)^T\beta$ 
by minimizing the $\ell_1$-penalized R-loss
$n^{-1}\sum_{i=1}^n [Y_i - \hat r(X_i) - (W-\hat e(X_i))\tau_{\beta}(X_i)]^2 + \lambda\norm{\beta}_{\ell_1}$,
with $\lambda$ chosen by cross-validation.
Finally, we set \smash{$\hmu(x) = \hat r(x)- \htau(x) \he(x)$}.
As discussed in \citet{nie2017learning}, this method is appropriate when the treatment effect function $\tau(x)$
is simpler than \smash{$r(x)$} and \smash{$e(x)$},
and allows for faster rates of convergence on $\tau(x)$ than the other regression components
whenever the nuisance components can be estimated at $o_p(n^{-1/4})$ rates in root-mean squared error.

\footnotetext{We emphasize that, although we use lasso software for fitting $\beta$, we do not follow the default
practice of standardizing the basis functions before applying the $\ell_1$-penalty. Rather, we estimate coefficients $\beta$ for the square-summable
basis \smash{$\phi_1,\phi_2,\ldots$} using a penalty proportional to $\norm{\beta}_{\ell_1}$. 
As discussed in Remark~\ref{rema:consistency}, this is penalized least squares estimation of the functions $r$ and $e$ (and $v_{w}$, which we discuss later) with a penalty proportional to the gauge of a Donsker class, where that Donsker class is the absolutely convex hull of $\phi_1,\phi_2,\ldots$.}

We consider two options for the bias-correcting weights $\hgamma$. The simpler option is to use
the minimax weights for the class $\ff_{\hh}$ described in \eqref{eq:F_ape}.
This choice is directly motivated by our formal results given in Theorem \ref{theo:simple}.
As an alternative, motivated by popular idea of propensity-stratified estimation in the causal inference
literature \citep{rosenbaum1984reducing}, we use minimax weights for an extended class $\ff_{\hh_+}$
where $\hh_+$ extends $\hh$ by adding to our basis expansion $\phi(x)$ the following random basis functions:
\begin{itemize}
\item Multi-scale strata of the estimated average treatment intensity \smash{$\he(X_i)$}
(we balanced over histogram bins of width 0.05, 0.1, and 0.2),
\item Basis elements obtained by depth-3 recursive dyadic partitioning (i.e., pick a feature, split along its median, and recurse), and
\item Leaves generated by a regression tree on the $W_i$ \citep{breiman1984classification}.
\end{itemize}
The underlying idea is that we may be able to improve the practical performance of the method by opportunistically adding
a small number of basis functions that help mitigate bias in case of misspecification (i.e., when $\mu$ and
$\tau$ do not have finite gauge \smash{$\norm{\cdot}_\hh$}). The motivation for focusing on transformations of
\smash{$\he(X_i)$} is that accurately stratifying on \smash{$e(X_i)$} would
suffice to eliminate all confounding in the model \eqref{eq:ape_again}.\footnote{In the case of binary
treatments $W_i$, this corresponds to the classical result of \citet{rosenbaum1983central}, who showed
that the propensity score is a balancing score. With non-binary treatments, \smash{$\EE{W_i \cond X_i}$}
is not in general a balancing score \citep{imbens2000role}; however, it is 
a balancing score for our specific model \eqref{eq:ape_again}.} 
Because $\F_{\hh+}$ is a function of $Z_1 \ldots Z_n$ for $Z_i=(X_i,W_i)$,
it is not necessary to cross-fit as described in 
Section~\ref{sec:sample-splitting} to avoid bias from the `noise term'. 
With both $\ff_{\hh}$ and $\ff_{\hh+}$, we take $\sigma^2=1$ in \eqref{eq:ape_gamma}.

\subsubsection{Baselines and Software Details}

The baselines we consider combine the aforementioned 
regression $\hmu(x) + w\htau(x)$ with various weighting schemes. 
The weights used in the plug-in double robust estimator \eqref{eq:ape_dr} 
involve $\he$ as estimated above and an estimate of $v_w(x)=\Var{W \mid X=x}$,
which we fit by cross-validated lasso regressing \smash{$(W_i-\he_{f_i,\hlambda_e}(X_i))^2$} on $\phi(X_i)$. 
The weights used in the double-robust oracle substitute the true values of $e(x)$ and $v_w(w)$ in our simulated design.



Ten-fold cross-fitting is used throughout: where $\htau(X_i)$ and $\hmu(X_i)$ appear in
\eqref{eq:ape_aml} and \eqref{eq:ape_dr}, we use estimators $\htau^{(-i)}$ and $\hmu^{(-i)}$
trained on the folds that do not include unit $i$. This 
reduces dependence on $(Y_i,X_i,W_i)$ and therefore mitigates potential own-observation bias 
in $\hpsi_{DR}$ \citep[see e.g.,][]{chernozhukov2016double}. However, we do get some dependence
through the estimates of $\hat r$ and $\he$ used to train $\htau$ and
through lasso tuning parameters, which are chosen once for all $i$ by cross-validation.
While this dependence could be eliminated using a computationally demanding nested sample splitting scheme,
we here follow the approach taken in the \texttt{grf} package of \citet*{athey2016generalized} and use a simplified scheme
described in Appendix~\ref{sec:simu_details}.
 Our theoretical results for $\hpsi_{AML}$ do not formally justify the use of this cross-fitting scheme,
as $\hm^{(-i)}(x,w) = \hmu^{(-i)}(x) + w\htau^{(-i)}(x)$ is a function of the fold indicator $f_i$ as
well as $x,w$, and for this reason $\norm{\hm - m}_{\ff_{\hh}} = \infty$;
however, this does not seem to cause problems in our simulations.


All methods are implemented in the \texttt{R} package \texttt{amlinear}, and replication files are
available at \url{https://github.com/davidahirshberg/amlinear}.
We computed minimax linear weights via the cone solver \texttt{ECOS} \citep*{domahidi2013ecos},
available in \texttt{R} via the package \texttt{CVXR} \citep{CVXR}.
When needed, we run penalized regression using the \texttt{R} package
\texttt{glmnet} \citep*{friedman2010regularization}.


\subsubsection{Simulation Design}
\label{sec:spec}

We considered data-generating distributions of the form
\begin{equation*}
\begin{split}
X_i \sim \nn\p{0, \, I_{d \times d}}, \ \ W_i  \cond X_i \sim \law_{X_i}, \ \
Y_i \cond X_i, \, W_i = \nn\p{b(X_i) + W_i \tau(X_i) , \, 1},
\end{split}
\end{equation*}
for different choices of
dimension $d$,
treatment assignment distribution $\law_{X_i}$,
baseline main effect $\mu(\cdot)$ and
treatment effect function $\tau(\cdot)$.
We considered the following 4 setups, each of which depends on a sparsity level
$k$ that controls the complexity of the signal.
\begin{enumerate}
\item Beta-distributed treatment,
\smash{$W_i \cond X_i \sim B(\alpha(X_i), \, 1-\alpha(X_i))$}, with
$\zeta(x) = \sum_{j = 1}^k x_{j}/\sqrt{k}$,
$\eta(x) = \sign(\zeta(x)) \zeta^2(x)$,
$\alpha(x) = \max\{0.05, \, \min\{0.95, $ $ 1/(1 + \exp[-\eta(x)]) \}\}$,
$\mu(x) = \eta(x) + 0.2 (\alpha(x) - 0.5)$, and
$\tau(x) = -0.2$.
\item Scaled Gaussian treatment,
\smash{$W_i \cond X_i \sim \nn\p{\lambda(X_i), \, \lambda^2(X_i)}$}, with
$\eta(x) = 2^{k-1} \prod_{j = 1}^k x_j$,
$\mu(x) = \sign(\eta(x)) \sqrt{\abs{\eta(x)}}$,
$\lambda(x) = 0.1 \, \text{sign}(\mu(x)) + \mu(x)$, and
$\tau(x) = \max\cb{x_{1} + x_{2}, \, 0}/2$.
\item Poisson treatment,
\smash{$W_i \cond X_i \sim \text{Poisson}(\lambda(X_i))$}, with
$\tau(x) = k^{-1} \sum_{j =1}^k$ $ \cos\p{\pi x_j /3}$,
$\lambda(x) = 0.2 + \tau^2(x)$, and
$\mu(x) = 4d^{-1}\sum_{j = 1}^d x_{j} + 2\lambda(x)$.
\item Log-normal treatment,
\smash{$\log(W_i) \cond X_i \sim \nn\p{\lambda(X_i), \, 1/3^2}$}, with
$\zeta(x) = \sum_{j = 1}^k$ $x_{j}/\sqrt{k}$,
$\mu(x) = \max\cb{0, \, 2\zeta(x)}$,
$\lambda(x) = 1 / (1 + \exp[-\sign(\zeta(x))\zeta^2(x)])$, and
$\tau(x) = \sin\p{2\pi x_{1}}$.
\end{enumerate}

\subsection{Results}

We first compare our augmented minimax linear estimators with the corresponding minimax linear estimators.
Figure \ref{fig:augment} compares the resulting mean-squared errors for $\psi$ across
several variants of the simulation design (the exact parameters used are the same as those used in Table \ref{tab:simu_results}).
The left panel shows results where the weights are minimax over $\ff_{\hh}$, while the right panel has minimax weights over $\ff_{\hh_+}$.
 
Overall, we see that the augmented minimax linear estimator is sometimes comparable to the
minimax linear one and sometimes substantially better. Thus, while results of \citet{donoho1994statistical} and \citet{armstrong2015optimal} 
imply that the augmented estimator can be little better than the minimax linear estimator for a convex signal class $\ff$ 
in terms of its behavior at a few specific signals $m \in \ff$,
this does not appear representative of behavior in general.
Furthermore, as the bias of our augmented estimator is bounded as a proportion of \smash{$\norm{\hm-m}_{\ff}$}
rather than \smash{$\norm{m}_{\ff}$}, our approach offers a natural way to accomodate signals in some non-convex signal classes: 
those for which, for some choice of $\hm$, the regression error function $\hm - m$ is well-characterized in terms of some strong norm $\norm{\cdot}_{\ff}$.
This can be the case, for example, when estimating a vector of regression coefficients $\beta$ by the lasso: 
\smash{$\norm{\hbeta - \beta}_{\ell_1}$} will be small
either if \smash{$\norm{\beta}_{\ell_1}$} is small or,
to a degree determined by incoherence properties of $\phi(X)$, if $\beta$ is sparse \citep[e.g.][]{lecue2018regularization}.
This phenomenon offers some explanation for the good behavior we observe empirically, as 
the functions $\mu(x)=\phi(x)^T \beta_{\mu}$ and $\tau(x) = \phi(x)^T \beta_{\tau}$ defining our signal $m(x,w)=\mu(x)+w\tau(x)$ have some degree of sparsity 
and \smash{$\norm{\hm - m}_{\ff_{\hh}}^2=\norm{\hbeta_{\mu} - \beta_{\mu}}_{\ell_1}^2 + \norm{\hbeta_{\tau} - \beta_{\tau}}_{\ell_1}^2$}. 


\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{augmented_cmp.pdf} &
\includegraphics[width=0.49\textwidth]{augmented_plus_cmp.pdf}
\end{tabular}
\caption[Comparing augmented minimax linear estimation with linear estimation.]{Comparing augmented minimax linear estimation with minimax linear estimation.
The solid line $y=x$ indicates equivalent performance and the dotted lines indicate improvements of 50\%, 100\%, 150\%, etc. in root mean squared error.}
\label{fig:augment}
\end{center}
\end{figure}

In Table \ref{tab:simu_results}, we compare augmented minimax linear estimation
with doubly robust estimators, both using an estimated and an oracle Riesz representer.
In terms of mean-squared error, our simple AML estimator already performs well relative to
the main baseline (i.e., plug-in doubly robust estimation), and the AML+ estimator does better yet. Perhaps more surprisingly, our methods sometimes also
beat the doubly robust oracle, achieving comparable control of bias with a substantial decrease in variance. 
This reduction in variance arises from shrinkage due to the penalty term in \eqref{eq:aml-primal}.
It costs us little bias then because, although the oracle weights must be large to control bias
for all square integrable regression errors $\hm - m$ (i.e., to solve \ref{eq:riesz}), 
large weights are not necessary to control bias for $\hm - m$ in $\ff$ (i.e., to solve \ref{eq:riesz-sample}).


In terms of coverage, some of our simulation designs are extremely difficult and all non-oracle estimators
have substantial relative bias. However, in settings 1 and 4, the asymptotics
appear to kick in and our estimators get close to nominal coverage.

\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\centering
\makebox[\textwidth]{
\input{simulation_results.tex}
}
\caption[Performance of AMLE and baselines in simulation.]{Performance of 4 methods described in Section \ref{sec:simu} on
the simulation designs from Section \ref{sec:spec}. We report root-mean squared error,
bias, and coverage of 95\% confidence intervals averaged over 200 simulation replications.}
\label{tab:simu_results}
\end{table}
\setlength{\tabcolsep}{6pt}


\section{The Effect of Lottery Winnings on Earnings}
\label{sec:application}

To test the behavior of our method in practice, we revisit a study of
\citet*{imbens2001estimating} on the effect of lottery winnings on
long-term earnings. It is of considerably policy interest to understand
how people react to reliable sources of unearned income; such questions
come up, for example, in discussing how universal basic income would affect employment.
In an attempt to get some insight about this effect, \citet*{imbens2001estimating}
study a sample of people who won a major lottery whose prize is paid out in installments
over 20 years. The authors then ask how \$1 in yearly lottery
income affects the earnings of the winner.

To do so, the authors consider $n = 194$ people who all won the lottery,
but got prizes of different sizes (\$1,000--\$100,000 per year).\footnote{The
paper also considers some people who won very large prizes (more than \$100k per year) and
some who won smaller prizes (not paid in installments); however, we restrict our analysis
to the smaller sample of people who won prizes paid out in installments worth \$1k--\$100k per year.}
They effectively use a causal model $\E[Y_i(w) \mid X_i=x] = m(x) + \tau w$
for observations $Y_i=Y_i(W_i)$ of the average yearly earnings in the 6 years following winning
$W_i$ in yearly lottery payoff, where $X_i$ denotes a set of $p = 12$ pre-win covariates
(year won, number of tickets bought, age at win, gender, education, whether
employed at time of win, earnings in 6 years prior to win). Here $Y_i(w)$
represents the average yearly earnings that would have occurred had, possibly contrary to fact,
unit $i$ won a prize paying $w$ dollars annually \citep[e.g.,][]{imbens2015causal}. 
The authors also consider several other model specifications.

As discussed at length by \citet*{imbens2001estimating}, although the lottery winnings
were presumably randomly assigned, we cannot assume exogeneity of the form
$W_i \indep \set{ Y_i(w) : w \in \R}$ because of survey non-response. The data was collected by
mailing out surveys to lottery winners asking about their earnings, etc., so
there may have been selection effects in who responded to the survey. 
A response rate of 42\% was observed, and older people with big 
winnings appear to have been relatively more likely to respond than young people with
big winnings. For this reason, the authors only assume exogeneity
conditionally on the covariates, i.e., $W_i \indep \set{ Y_i(w) : w \in \R} \cond X_i$,
which suffices to establish that the aforementioned causal model is identified 
as a regression model $m(x) + \tau w = \E[Y_i \mid X_i=x, W_i=w]$.

Here, we examine the robustness of the conclusions of \citet*{imbens2001estimating}
to potential effect heterogeneity. Instead of assuming that the slope $\tau$ in this model
 is a constant, we let it vary with $x$ and seek to estimate $\psi = \EE{\tau(X)}$;
this corresponds exactly to an average partial effect in the conditionally linear model, which we
studied in Section \ref{sec:ape}. In our comparison, we consider 3 estimators that implicitly
assume constant slope and estimate $\tau$, and 6 that allow
$\tau(x)$ to vary and estimate $\EE{\tau(X)}$.

Among methods that assume constant slope, the first runs ordinary least squares for $Y_i$ on $W_i$, ignoring potential confounding due to
non-response. The second, which most closely resembles the method used by \citet*{imbens2001estimating},
controls for the $X_i$ using ordinary least squares, i.e., it regresses $Y_i$ on $(X_i, W_i)$ and considers the coefficient on $W_i$.
The third uses the method of \citet{robinson1988root} with cross-fitting as in \citet{chernozhukov2016double}:
it first estimates the marginal effect of $X_i$ on $W_i$ and $Y_i$ via a non-parametric adjustment and then
regresses residuals \smash{$Y_i - \hEE{Y_i \cond X_i}$} on \smash{$W_i - \hEE{W_i \cond X_i}$}.
In each case, we report robust standard errors obtained via the \texttt{R}-package \texttt{sandwich} \citep{sandwich}.

The 6 methods that allow for treatment effect heterogeneity correspond to the 5 methods discussed
in Section \ref{sec:ape}, along with a pure weighting estimator using the estimated Riesz representer,
\smash{$\hpsi = n^{-1} \sum_{i = 1}^n \hriesz[\psi](X_i) Y_i$}, with the same choice of
\smash{$\hriesz[\psi](\cdot)$} as used in \eqref{eq:ape_dr}.
For all non-parametric regression adjustments, we run penalized regression as in Section \ref{sec:ape},
on a basis obtained by taking order-3 Hermite interactions of the 10 continuous features, and then
creating full interactions with the two binary variables (gender and employment), resulting in a total
of 1140 basis elements. For AML+, we include propensity
strata of widths $0.05$, $0.1$, and $0.2$ in the class $\hh_+$ .


\begin{table}[t]
\begin{center}
\begin{tabular}{|ll|cc|}
  \hline
  estimand & estimator & estimate & std.~err \\ 
  \hline
  partial effect & OLS without controls & -0.176 & 0.039 \\ 
  partial effect & OLS with controls & -0.106 & 0.032 \\ 
  partial effect & residual-on-residual OLS & -0.110 & 0.032 \\ 
  avg.~partial effect & plugin Riesz weighting & -0.175 & --- \\ 
  avg.~partial effect & doubly robust plugin & -0.108 & 0.042 \\ 
  avg.~partial effect & minimax linear weighting & -0.074 & --- \\ 
  avg.~partial effect & augm.~minimax linear & -0.091 & 0.044 \\ 
  avg.~partial effect & minimax linear+ weighting & -0.083 & --- \\ 
  avg.~partial effect & augm.~minimax linear+ & -0.097 & 0.045 \\ 
   \hline
\end{tabular}
\caption[Estimates for the effect of unearned income on earnings using data from \citet*{imbens2001estimating}.]{Various estimates, estimators, and estimands for the effect of unearned income on earnings,
using the dataset of \citet*{imbens2001estimating}.
The first 3 methods are justified under the assumption of no heterogeneity in $\tau(x)$ (i.e., $\tau(x) = \tau$),
and estimate $\tau$, while the latter 6 allow for heterogeneity and estimate $\EE{\tau(X)}$.}
\label{tab:IRS}
\end{center}
\end{table}

Table \ref{tab:IRS} reports results using the 9 estimators described above, along with standard
error estimates. We do not report standard errors for the 3 pure weighting methods, as these may not be
asymptotically unbiased and so confidence intervals should also account for bias.
The reported estimates are unitless; in other words, the majority of the estimators suggest that
survey respondents on average respond to a \$1 increase in unearned yearly income by reducing
their yearly earnings by roughly \$0.10.

Substantively, it appears reassuring that most point estimates are consistent
with each other, whether or not they allow for heterogeneity in $\tau(x)$. The only two divergent estimators
are the one that doesn't control for confounding at all,
and the one that uses pure plug-in weighting (which may simply be unstable here).
From a methodological perspective, it is encouraging that our method (and here, also the plug-in doubly robust method)
can rigorously account for potential heterogeneity in $\tau(x)$ without excessively inflating uncertainty.

\section*{Acknowledgments}

We are grateful for stimulating
discussions with Timothy Armstrong, Vitor Hadad, Guido Imbens, Whitney Newey, Jamie Robins, Florian Stebegg, and Jos\'e Zubizarreta,
as well as for comments from seminar participants at several venues.
We also thank Guido Imbens for sharing the lottery data with us.
We initiated this research while D.H.~was a Ph.D. candidate at Columbia University and S.W.~was visiting Columbia as a postdoctoral research scientist.

\ifaos
\begin{supplement}
\stitle{Appendices}
\sdescription{We provide complete proofs for the results in the main text, details about our simulation study, and a discussion of computational issues.}
\end{supplement}
\fi

\ifaos
\bibliographystyle{imsart-nameyear}
\else
\bibliographystyle{plainnat-abbrev}
\fi
\bibliography{references}

% include appendix unless AoS [in which case we put the appendix in the supplement]
\ifaos
\else

\newpage

\begin{appendix}
\input{appendix-amle}
\end{appendix}

\fi

\end{document}
